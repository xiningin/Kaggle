{"cell_type":{"ce1ba86e":"code","6bf9c7b3":"code","bacd3660":"code","17c38bec":"code","1350c0a5":"code","a1f3e769":"code","4bbe2465":"code","7c81467b":"code","501c831b":"code","7c308abd":"code","4a6a58f6":"code","ef060c02":"code","315f43e4":"code","706e57a2":"code","6c337832":"code","b06536dc":"code","95ea025f":"code","f1b1c725":"code","cf093f7e":"code","b9c08367":"code","f2b6a493":"code","7b92c9e6":"code","0f30d150":"code","f811b34b":"code","37b87dc2":"code","03c381d2":"code","abea4975":"code","08b11dac":"code","ffc9b00a":"code","c8f0970c":"code","965a8d87":"code","cee98b6d":"code","f25a15a2":"code","5cec5258":"code","4edfdc17":"code","0c225580":"code","95c5e004":"code","8e334bb8":"code","4de0fc47":"code","b430bb7d":"code","9b3de775":"code","40b14d7f":"code","67070879":"code","df83f752":"code","af9231b1":"code","5681d264":"code","fd20f26d":"code","83c45fc1":"code","86e617cd":"code","2efbfb2f":"code","c44ec4b6":"code","2a43222b":"code","99c2efe6":"code","57f2a211":"code","f80b7d24":"code","d649aa2c":"code","b51f85de":"code","a0fc8ca4":"code","8878071d":"code","8fc0e10b":"code","6176ce2f":"code","96ceda83":"code","d4a6830c":"code","1ff3a4b3":"code","252b5630":"code","a497fb58":"code","52d39568":"code","0842897d":"code","6de6aec0":"code","4017ba2b":"code","4f70fcd5":"code","910c7bd9":"code","0d1231ae":"code","0974ed66":"code","f5ed90c6":"code","a767ded5":"code","36258d9f":"code","ca05469d":"code","68b29b3c":"code","5bce0f3b":"code","15dbfaf6":"code","8e82f577":"code","86bb6e97":"code","5e2c28fb":"code","a920d2ca":"code","51711a3a":"code","553310cb":"code","08d5c64e":"code","5eca193a":"code","ed37842c":"code","af76f6f9":"code","a091e5bd":"code","def2d5f3":"code","46701e9c":"code","6fea9d48":"code","80f3f4c5":"code","70a09c81":"code","4d0011c7":"code","9ebcb995":"code","f1dd11c6":"code","8e78e8bd":"code","3cabb7de":"code","0eb3ea95":"code","2b40eab2":"code","bd6d63f4":"code","dd02993c":"code","3b744057":"code","4ef13a36":"code","bfec5378":"code","fff0a4b5":"code","3bddad3a":"code","674d75df":"code","097386f8":"code","79cdcacc":"code","32fee653":"code","ca7ff47d":"code","93e14c48":"code","3967f085":"code","13729bc1":"code","f283db4e":"code","8eb1b7f6":"code","c85e602b":"code","ec47938c":"code","d1450f14":"code","37b6580d":"code","1c2eae0d":"code","8776db01":"code","22cba55b":"code","e5efb99c":"code","28fc1429":"markdown","3fd55b17":"markdown","c243faaf":"markdown","13e2d56d":"markdown","35e30169":"markdown","1b1de9e0":"markdown","8833929d":"markdown","abefc699":"markdown","435d0922":"markdown","b98a4f2f":"markdown","ccd7d286":"markdown","fc435b2d":"markdown","a5af4840":"markdown","d8a82e82":"markdown","9a86cf15":"markdown","afd93044":"markdown","4f97fa48":"markdown","6c33c099":"markdown","fc6b1be3":"markdown","56109f92":"markdown","0b25b408":"markdown","5793ea58":"markdown","289b0b0e":"markdown","69285b12":"markdown","86a295aa":"markdown","59c25f8c":"markdown","7f99fbe3":"markdown","49b834cb":"markdown","918377bd":"markdown","bc5e41b5":"markdown","8769e7a7":"markdown","ac136b94":"markdown","495def33":"markdown","80e91656":"markdown","ee36d585":"markdown","e36254a6":"markdown","b42cb260":"markdown","3d969713":"markdown","3f219a8e":"markdown","3b702541":"markdown","16cd0453":"markdown","dec472fc":"markdown","2b8b12d5":"markdown","8879e2e6":"markdown","b79269be":"markdown","d2d8fdd8":"markdown","d4017b50":"markdown","f492d717":"markdown","96a81fed":"markdown","00f8b8b9":"markdown","1ee8a37e":"markdown","c0dbdc0e":"markdown","760e84d6":"markdown","f849ef64":"markdown","62fc5e3b":"markdown","612ca99a":"markdown","55b4c1ac":"markdown","f06e6ba9":"markdown","1bcd0630":"markdown","f2d6df11":"markdown","da3bf4a9":"markdown","b03fe087":"markdown","33c7660e":"markdown","515afd16":"markdown","f3a5e2be":"markdown","464717e7":"markdown","a356ff8c":"markdown","28588439":"markdown","c3602088":"markdown","47d29802":"markdown","ccdae3b0":"markdown","bf0546d6":"markdown","187f74e2":"markdown","7f09ea37":"markdown","ff83f52c":"markdown","cfbec535":"markdown","510228f8":"markdown","ee5ad4a7":"markdown","bb1375aa":"markdown","3ec62dc0":"markdown","3294eabe":"markdown","a2afa5e3":"markdown","b34d8ac6":"markdown","0ddc47f9":"markdown","e76818df":"markdown","d051b39d":"markdown","399a9803":"markdown","e9f0dfe1":"markdown","4d88e0f4":"markdown","97c92f04":"markdown","4b686943":"markdown","0b9e10e7":"markdown","51c4042d":"markdown","1e85932a":"markdown","646cc820":"markdown","fd369668":"markdown","cd7fcc48":"markdown","cb26c522":"markdown","e298fdb7":"markdown","5d84c265":"markdown","dd68b129":"markdown","96124dcf":"markdown","55c82a34":"markdown","926fbd43":"markdown","4f4ab798":"markdown","e9779eb2":"markdown","9ca57e03":"markdown","c5d3c926":"markdown","2dadb36e":"markdown","abd685f5":"markdown","041e8a7f":"markdown","6b7d7b72":"markdown","a76752cc":"markdown","9f37b69c":"markdown","2a93d4b2":"markdown","2b11828c":"markdown","f1c7f5db":"markdown","469cb4b3":"markdown","a352a709":"markdown","b214511d":"markdown","fc10b155":"markdown","74bd142f":"markdown","ae2a5bc4":"markdown","086f87ce":"markdown","58d2fa76":"markdown","e23ad42c":"markdown","6ec39f5b":"markdown","2924bb77":"markdown","4279b4e3":"markdown","cdbfddf8":"markdown","e88b2205":"markdown","44fcdab8":"markdown","04034481":"markdown","02f5987a":"markdown"},"source":{"ce1ba86e":"import os\nfrom pathlib import Path\nimport subprocess\n\n# Create the input directory if it doesn't exist\nif not os.path.exists('..\/input'):\n    os.makedirs('..\/input')\n\nfile_on_disk = True\n\n# Check if the files are on disk before download\nfor file in os.listdir('..\/input'):\n    if not Path('..\/input\/' + file).is_file():\n        # The file is not on disk\n        file_on_disk = False\n        break\n        \nif not file_on_disk:\n    # Download the files with your API token in ~\/.kaggle\n    error = subprocess.call('kaggle competitions download -c titanic -p ..\/input'.split())\n    if not error:\n        print('Files downloaded successfully.')\n    else:\n        print('An error occurred during donwload, check your API token.')\nelse:\n    print('Files are already on disk.')","6bf9c7b3":"# Load packages\nprint('Python packages:')\nprint('-'*15)\n\nimport sys\nprint('Python version: {}'. format(sys.version))\n\nimport pandas as pd\nprint('pandas version: {}'. format(pd.__version__))\n\nimport matplotlib\nprint('matplotlib version: {}'. format(matplotlib.__version__))\n\nimport numpy as np\nprint('NumPy version: {}'. format(np.__version__))\n\nimport scipy as sp\nprint('SciPy version: {}'. format(sp.__version__)) \n\nimport IPython\nfrom IPython import display\nprint('IPython version: {}'. format(IPython.__version__)) \n\nimport sklearn\nprint('scikit-learn version: {}'. format(sklearn.__version__))\n\n# Miscsellaneous libraries\nimport random\nimport time\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('')\n\n# Check the input directory\nprint('Input directory: ')\nprint('-'*15)\nfrom subprocess import check_output\nprint(check_output(['ls', '..\/input']).decode('utf8'))","bacd3660":"# Common model algorithms\nfrom sklearn import neighbors, ensemble\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\n\n# Common model helpers\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn import model_selection\n\n# Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nfrom matplotlib.ticker import PercentFormatter\nimport seaborn as sns\n\n# Configure visualization defaults\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npalette = sns.color_palette('Set2', 10)\npylab.rcParams['figure.figsize'] = 18,4","17c38bec":"train_df = pd.read_csv('..\/input\/train.csv').set_index(keys='PassengerId', drop=True)\ntest_df  = pd.read_csv('..\/input\/test.csv').set_index(keys='PassengerId', drop=True)\n\n# Useful for more accurate feature engineering\ndata_df = train_df.append(test_df)","1350c0a5":"train_df.sample(10)","a1f3e769":"train_df.describe(include = 'all')","4bbe2465":"test_df.describe(include = 'all')","7c81467b":"def plot_missing_values(dataset):\n    \"\"\"\n        Plots the proportion of missing values per feature of a dataset.\n        \n        :param dataset: pandas DataFrame\n    \"\"\"\n    missing_data_percent = [x \/ len(dataset) for x in dataset.isnull().sum()]\n    data_percent = [1 - x for x in missing_data_percent]\n\n    fig, axs = plt.subplots(1,1,figsize=(18,4))\n    plt.bar(dataset.columns.values, data_percent, color='#84B044', linewidth=0)\n    plt.bar(dataset.columns.values, missing_data_percent, bottom=data_percent, color='#E76C5D', linewidth=0)\n\n    axs.yaxis.set_major_formatter(PercentFormatter(xmax=1))","501c831b":"train_df.isnull().sum().to_frame('Missing values').transpose()","7c308abd":"plot_missing_values(train_df)","4a6a58f6":"test_df.isnull().sum().to_frame('Missing values').transpose()","ef060c02":"plot_missing_values(test_df)","315f43e4":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,6))\n\ncorr_train = train_df[['Age', 'Fare', 'Parch', 'SibSp', 'Survived']].corr()\ncorr_test = test_df[['Age', 'Fare', 'Parch', 'SibSp']].corr()\n\n# Generate masks for the upper triangles\nmask_train = np.zeros_like(corr_train, dtype=np.bool)\nmask_train[np.triu_indices_from(mask_train)] = True\n\nmask_test = np.zeros_like(corr_test, dtype=np.bool)\nmask_test[np.triu_indices_from(mask_test)] = True\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the train set heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_train, ax=ax1, mask=mask_train, cmap=cmap, vmax=.5, center=0, square=True, \n            linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True, fmt='.2f')\nax1.set_title('Pearson\\'s correlation matrix of train set')\n\n# Draw the test heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_test, ax=ax2, mask=mask_test, cmap=cmap, vmax=.5, center=0, square=True, \n            linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True, fmt='.2f')\nax2.set_title('Pearson\\'s correlation matrix of test set')","706e57a2":"from scipy.stats import entropy\nfrom numpy.linalg import norm\n\ndef JSD(P, Q, n_iter=1000):\n    \"\"\"\n        Computes the Jensen-Shannon divergence between two probability distributions of different sizes.\n        \n        :param P: distribution P\n        :param Q: distribution Q\n        :param n_iter: number of iterations\n        :return: Jensen-Shannon divergence\n    \"\"\"\n    size = min(len(P),len(Q))\n    \n    results = []\n    for _ in range(n_iter):\n        P = np.random.choice(P, size=size, replace=False)\n        Q = np.random.choice(Q, size=size, replace=False)\n\n        _P = P \/ norm(P, ord=1)\n        _Q = Q \/ norm(Q, ord=1)\n        _M = 0.5 * (_P + _Q)\n\n        results.append(0.5 * (entropy(_P, _M) + entropy(_Q, _M)))\n\n    return results","6c337832":"# Age vs Survived\ng = sns.FacetGrid(train_df, col='Survived', size=4, aspect=2)\ng = g.map(sns.distplot, 'Age', color='#D66A84')","b06536dc":"# Train set vs Test set\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.distplot(train_df['Age'].dropna(), ax=ax1, color='#D66A84')\nax1.set_title('Train set')\n\nsns.distplot(test_df['Age'].dropna(), ax=ax2, color='#D66A84')\nax2.set_title('Test set')","95ea025f":"age_jsd = JSD(train_df['Age'].dropna().values, test_df['Age'].dropna().values)\nprint('Jensen-Shannon divergence of Age:', np.mean(age_jsd))\nprint('Standard deviation:', np.std(age_jsd))","f1b1c725":"# Fare vs Survived\ng = sns.FacetGrid(train_df, col='Survived', palette=palette, size=4, aspect=2)\ng = g.map(sns.distplot, 'Fare', color='#25627D')","cf093f7e":"fig, ax = plt.subplots(figsize=(18,4))\n\ng = sns.distplot(train_df['Fare'], ax=ax, color='#25627D', label='Skewness : %.2f'%(train_df['Fare'].skew()))\ng = g.legend(loc='best')","b9c08367":"# Train set vs Test set\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.distplot(train_df['Fare'].dropna(), ax=ax1, color='#25627D')\nax1.set_title('Train set')\n\nsns.distplot(test_df['Fare'].dropna(), ax=ax2, color='#25627D')\nax2.set_title('Test set')","f2b6a493":"fare_jsd = JSD(train_df['Fare'].dropna().values, test_df['Fare'].dropna().values)\nprint('Jensen-Shannon divergence of Fare:', np.mean(fare_jsd))\nprint('Standard deviation:', np.std(fare_jsd))","7b92c9e6":"palette6 = [\"#F6B5A4\", \"#EB7590\", \"#C8488A\", \"#872E93\", \"#581D7F\", \"#3A1353\"]\n# Parch vs Survived\ng  = sns.catplot(x='Parch', y='Survived', saturation=5, height=4, aspect=4, data=train_df, kind='bar', palette=palette6)\ng.despine(left=True)\ng = g.set_ylabels(\"Survival probability\")","0f30d150":"# Train set vs Test set\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.distplot(train_df['Parch'], ax=ax1, color='#84B044')\nax1.set_title('Train set')\n\nsns.distplot(test_df['Parch'], ax=ax2, color='#84B044')\nax2.set_title('Test set')","f811b34b":"parch_jsd = JSD(train_df['Parch'].values, test_df['Parch'].values)\nprint('Jensen-Shannon divergence of Parch:', np.mean(parch_jsd))\nprint('Standard deviation:', np.std(parch_jsd))","37b87dc2":"palette7 = [\"#F7BBA6\", \"#ED8495\", \"#E05286\", \"#A73B8F\", \"#6F2597\", \"#511B75\", \"#37114E\"]\n# SibSp feature vs Survived\ng = sns.catplot(x='SibSp', y='Survived', saturation=5, height=4, aspect=4, data=train_df, kind='bar', palette=palette7)\ng.despine(left=True)\ng = g.set_ylabels(\"Survival probability\")","03c381d2":"# Train set vs Test set\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.distplot(train_df['SibSp'], ax=ax1, color='#E76C5D')\nax1.set_title('Train set')\n\nsns.distplot(test_df['SibSp'], ax=ax2, color='#E76C5D')\nax2.set_title('Test set')","abea4975":"sibsp_jsd = JSD(train_df['SibSp'].values, test_df['SibSp'].values)\nprint('Jensen-Shannon divergence of SibSp:', np.mean(sibsp_jsd))\nprint('Standard deviation:', np.std(sibsp_jsd))","08b11dac":"palette4 = [\"#F19A9B\", \"#D54D88\", \"#7B2A95\", \"#461765\"]\nfig, ax = plt.subplots(figsize=(18,4))\njsd = pd.DataFrame(np.column_stack([age_jsd, fare_jsd, parch_jsd, sibsp_jsd]), columns=['Age', 'Fare', 'Parch', 'SibSp'])\nsns.boxplot(data=jsd, ax=ax, orient=\"h\", linewidth=1, saturation=5, palette=palette4)\nax.set_title('Jensen-Shannon divergences of numerical features')","ffc9b00a":"plt.figure(figsize=(18, 4))\nplt.scatter(train_df['Age'], train_df['Fare'], c=train_df['Survived'].values, cmap='cool')\nplt.xlabel('Age')\nplt.ylabel('Fare')\nplt.title('Age vs Fare')","c8f0970c":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.regplot(x='Age', y='Fare', ax=ax1, data=train_df)\nax1.set_title('Train set')\nsns.regplot(x='Age', y='Fare', ax=ax2, data=test_df)\nax2.set_title('Test set')","965a8d87":"print('PCC for the train set: ', corr_train['Age']['Fare'])\nprint('PCC for the test set: ', corr_test['Age']['Fare'])","cee98b6d":"plt.figure(figsize=(18, 4))\nplt.scatter(train_df['Age'], train_df['Parch'], c=train_df['Survived'].values, cmap='cool')\nplt.xlabel('Age')\nplt.ylabel('Parch')\nplt.title('Age vs Parch')","f25a15a2":"palette8 = [\"#F8C1A8\", \"#EF9198\", \"#E8608A\", \"#C0458A\", \"#8F3192\", \"#63218F\", \"#4B186C\", \"#33104A\"]\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Age', x='Parch', ax=ax1, data=train_df, linewidth=1, saturation=5, palette=palette7)\nax1.set_title('Train set')\nsns.boxplot(y='Age', x='Parch', ax=ax2, data=test_df, linewidth=1, saturation=5, palette=palette8)\nax2.set_title('Test set')","5cec5258":"print('PCC for the train set: ', corr_train['Age']['Parch'])\nprint('PCC for the test set: ', corr_test['Age']['Parch'])","4edfdc17":"plt.figure(figsize=(18, 4))\nplt.scatter(train_df['Fare'], train_df['Parch'], c=train_df['Survived'].values, cmap='cool')\nplt.xlabel('Fare')\nplt.ylabel('Parch')\nplt.title('Fare vs Parch')","0c225580":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Fare', x='Parch', ax=ax1, data=train_df, linewidth=1, saturation=5, palette=palette7)\nax1.set_title('Train set')\nsns.boxplot(y='Fare', x='Parch', ax=ax2, data=test_df, linewidth=1, saturation=5, palette=palette8)\nax2.set_title('Test set')","95c5e004":"print('PCC for the train set: ', corr_train['Fare']['Parch'])\nprint('PCC for the test set: ', corr_test['Fare']['Parch'])","8e334bb8":"plt.figure(figsize=(18, 4))\nplt.scatter(train_df['Fare'], train_df['SibSp'], c=train_df['Survived'].values, cmap='cool')\nplt.xlabel('Fare')\nplt.ylabel('SibSp')\nplt.title('Fare vs SibSp')","4de0fc47":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Fare', x='SibSp', ax=ax1, data=train_df, linewidth=1, saturation=5, palette=palette7)\nax1.set_title('Train set')\nsns.boxplot(y='Fare', x='SibSp', ax=ax2, data=test_df, linewidth=1, saturation=5, palette=palette8)\nax2.set_title('Test set')","b430bb7d":"print('PCC for the train set: ', corr_train['Fare']['SibSp'])\nprint('PCC for the test set: ', corr_test['Fare']['SibSp'])","9b3de775":"plt.figure(figsize=(18, 4))\nplt.scatter(train_df['Parch'], train_df['SibSp'], c=train_df['Survived'].values, cmap='cool')\nplt.xlabel('Parch')\nplt.ylabel('SibSp')\nplt.title('Parch vs SibSp')","40b14d7f":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Parch', x='SibSp', ax=ax1, data=train_df, linewidth=1, saturation=5, palette=palette7)\nax1.set_title('Train set')\nsns.boxplot(y='Parch', x='SibSp', ax=ax2, data=test_df, linewidth=1, saturation=5, palette=palette8)\nax2.set_title('Test set')","67070879":"print('PCC for the train set: ', corr_train['Parch']['SibSp'])\nprint('PCC for the test set: ', corr_test['Parch']['SibSp'])","df83f752":"palette3 = [\"#EE8695\", \"#A73B8F\", \"#501B73\"]\n# Embarked feature vs Survived\ng  = sns.catplot(x='Embarked', y='Survived', saturation=5, height=4, aspect=4, data=train_df, \n                    kind='bar', palette=palette3)\ng.despine(left=True)\ng = g.set_ylabels('Survival probability')","af9231b1":"# Train set vs Test set\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\ntrain_df['Embarked'].value_counts().plot(kind='barh', ax=ax1)\nax1.set_title('Train set')\n\ntest_df['Embarked'].value_counts().plot(kind='barh', ax=ax2)\nax2.set_title('Test set')","5681d264":"palette2 = [\"#EE8695\", \"#A73B8F\"]\n# Sex feature vs Survived\ng  = sns.catplot(x='Sex', y='Survived', saturation=5, height=4, aspect=4, data=train_df, \n                    kind='bar', palette=palette2)\ng.despine(left=True)\ng = g.set_ylabels('Survival probability')","fd20f26d":"# Train set vs Test set\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\ntrain_df['Sex'].value_counts().plot(kind='barh', ax=ax1)\nax1.set_title('Train set')\n\ntest_df['Sex'].value_counts().plot(kind='barh', ax=ax2)\nax2.set_title('Test set')","83c45fc1":"# Pclass feature vs Survived\ng  = sns.catplot(x='Pclass', y='Survived', saturation=5, height=4, aspect=4, data=train_df, \n                    kind='bar', palette=palette3)\ng.despine(left=True)\ng = g.set_ylabels('Survival probability')","86e617cd":"# Train set vs Test set\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\ntrain_df['Pclass'].value_counts().plot(kind='barh', ax=ax1)\nax1.set_title('Train set')\n\ntest_df['Pclass'].value_counts().plot(kind='barh', ax=ax2)\nax2.set_title('Test set')","2efbfb2f":"import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\ndef compute_anova(dataset, group, weight):\n    \"\"\"\n        Computes the effect size through ANOVA.\n        \n        :param dataset: pandas DataFrame\n        :param group: categorical feature\n        :param weight: continuous feature\n        :return: effect size\n    \"\"\"\n    mod = ols(weight + ' ~ ' + group, data=dataset).fit()\n    aov_table = sm.stats.anova_lm(mod, typ=2)\n    esq_sm = aov_table['sum_sq'][0]\/(aov_table['sum_sq'][0]+aov_table['sum_sq'][1])\n    \n    return esq_sm","c44ec4b6":"from scipy.stats import chi2_contingency\n\ndef chisq(dataset, c1, c2):\n    \"\"\"\n        Performs the Chi squared independence test.\n        \n        :param dataset: pandas DataFrame\n        :param c1: continuous feature 1\n        :param c2: continuous feature 2\n        :return: array with [Chi^2, p-value]\n    \"\"\"\n    groupsizes = dataset.groupby([c1, c2]).size()\n    ctsum = groupsizes.unstack(c1)\n\n    result = chi2_contingency(ctsum.fillna(0))\n    \n    print('Chi^2:', result[0])\n    print('p-value:', result[1])\n    print('Degrees of freedom:', result[2])","2a43222b":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Age', x='Embarked', ax=ax1, data=train_df, linewidth=1, saturation=5, order=['S', 'C', 'Q'], palette=palette3)\nax1.set_title('Train set')\nsns.boxplot(y='Age', x='Embarked', ax=ax2, data=test_df, linewidth=1, saturation=5, order=['S', 'C', 'Q'], palette=palette3)\nax2.set_title('Test set')","99c2efe6":"train_esq_sm = compute_anova(train_df, 'Embarked', 'Age')\ntest_esq_sm = compute_anova(test_df, 'Embarked', 'Age')\n\nprint('ANOVA 1-way for the train set: ', train_esq_sm)\nprint('ANOVA 1-way for the test set: ', test_esq_sm)","57f2a211":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Fare', x='Embarked', ax=ax1, data=train_df, linewidth=1, saturation=5, order=['S', 'C', 'Q'], palette=palette3)\nax1.set_title('Train set')\nsns.boxplot(y='Fare', x='Embarked', ax=ax2, data=test_df, linewidth=1, saturation=5, order=['S', 'C', 'Q'], palette=palette3)\nax2.set_title('Test set')","f80b7d24":"train_esq_sm = compute_anova(train_df, 'Embarked', 'Fare')\ntest_esq_sm = compute_anova(test_df, 'Embarked', 'Fare')\n\nprint('ANOVA 1-way for the train set: ', train_esq_sm)\nprint('ANOVA 1-way for the test set: ', test_esq_sm)","d649aa2c":"def plot_embarked_variable(dataset, variable):\n    \"\"\"\n        Plots the proportion of variable values per Embarked value of a dataset.\n        \n        :param dataset: pandas DataFrame\n        :param variable: variable to plot\n    \"\"\"\n    s_variable_index = dataset.groupby(['Embarked', variable]).size()['S'].index.values\n    c_variable_index = dataset.groupby(['Embarked', variable]).size()['C'].index.values\n    q_variable_index = dataset.groupby(['Embarked', variable]).size()['Q'].index.values\n\n    index = list(set().union(s_variable_index,c_variable_index,q_variable_index))\n\n    raw_s_variable = dataset.groupby(['Embarked', variable]).size()['S']\n    raw_c_variable = dataset.groupby(['Embarked', variable]).size()['C']\n    raw_q_variable = dataset.groupby(['Embarked', variable]).size()['Q']\n\n    s_variable = []\n    c_variable = []\n    q_variable = []\n\n    for i in range(max(index) + 1):\n        s_variable.append(raw_s_variable[i] if i in s_variable_index else 0)\n        c_variable.append(raw_c_variable[i] if i in c_variable_index else 0)\n        q_variable.append(raw_q_variable[i] if i in q_variable_index else 0)\n\n    percent_s_variable = [s_variable[i]\/(s_variable[i] + c_variable[i] + q_variable[i]) if i in index else 0 for i in range(max(index) + 1)]\n    percent_c_variable = [c_variable[i]\/(s_variable[i] + c_variable[i] + q_variable[i]) if i in index else 0 for i in range(max(index) + 1)]\n    percent_q_variable = [q_variable[i]\/(s_variable[i] + c_variable[i] + q_variable[i]) if i in index else 0 for i in range(max(index) + 1)]\n\n    r = list(range(max(index) + 1))\n    bars = [sum(x) for x in zip(percent_s_variable, percent_c_variable)]\n\n    fig, axs = plt.subplots(1,1,figsize=(18,4))\n    plt.bar(r, percent_s_variable, color='#08c299')\n    plt.bar(r, percent_c_variable, bottom=percent_s_variable, linewidth=0, color='#97de95')\n    plt.bar(r, percent_q_variable, bottom=bars, linewidth=0, color='#fce8aa')\n    plt.xticks(r, r)\n    plt.title('Proportion of Embarked values by ' + variable)\n    axs.legend(labels=['S', 'C', 'Q'])\n    axs.yaxis.set_major_formatter(PercentFormatter(xmax=1))","b51f85de":"plot_embarked_variable(train_df, 'Parch')\nchisq(train_df, 'Embarked', 'Parch')","a0fc8ca4":"plot_embarked_variable(test_df, 'Parch')\nchisq(test_df, 'Embarked', 'Parch')","8878071d":"plot_embarked_variable(train_df, 'SibSp')\nchisq(train_df, 'Embarked', 'SibSp')","8fc0e10b":"plot_embarked_variable(test_df, 'SibSp')\nchisq(test_df, 'Embarked', 'SibSp')","6176ce2f":"tmp_train_df = train_df.copy(deep=True)\ntmp_train_df['Sex'].replace(['male', 'female'], [0,1], inplace=True)\nplot_embarked_variable(tmp_train_df, 'Sex')\nchisq(tmp_train_df, 'Embarked', 'Sex')","96ceda83":"tmp_test_df = test_df.copy(deep=True)\ntmp_test_df['Sex'].replace(['male', 'female'], [0,1], inplace=True)\nplot_embarked_variable(tmp_test_df, 'Sex')\nchisq(tmp_test_df, 'Embarked', 'Sex')","d4a6830c":"plot_embarked_variable(train_df, 'Pclass')\nchisq(train_df, 'Embarked', 'Pclass')","1ff3a4b3":"plot_embarked_variable(test_df, 'Pclass')\nchisq(test_df, 'Embarked', 'Pclass')","252b5630":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Age', x='Sex', ax=ax1, data=train_df, linewidth=1, saturation=5, palette=palette2)\nax1.set_title('Train set')\nsns.boxplot(y='Age', x='Sex', ax=ax2, data=test_df, linewidth=1, saturation=5, palette=palette2)\nax2.set_title('Test set')","a497fb58":"train_esq_sm = compute_anova(train_df, 'Sex', 'Age')\ntest_esq_sm = compute_anova(test_df, 'Sex', 'Age')\n\nprint('ANOVA 1-way for the train set: ', train_esq_sm)\nprint('ANOVA 1-way for the test set: ', test_esq_sm)","52d39568":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Fare', x='Sex', ax=ax1, data=train_df, linewidth=1, saturation=5, palette=palette2)\nax1.set_title('Train set')\nsns.boxplot(y='Fare', x='Sex', ax=ax2, data=test_df, linewidth=1, saturation=5, palette=palette2)\nax2.set_title('Test set')","0842897d":"train_esq_sm = compute_anova(train_df, 'Sex', 'Fare')\ntest_esq_sm = compute_anova(test_df, 'Sex', 'Fare')\n\nprint('ANOVA 1-way for the train set: ', train_esq_sm)\nprint('ANOVA 1-way for the test set: ', test_esq_sm)","6de6aec0":"def plot_sex_variable(dataset, variable):\n    \"\"\"\n        Plots the proportion of variable values per Sex value of a dataset.\n        \n        :param dataset: pandas DataFrame\n        :param variable: variable to plot\n    \"\"\"\n    male_variable_index = dataset.groupby(['Sex', variable]).size()['male'].index.values\n    female_variable_index = dataset.groupby(['Sex', variable]).size()['female'].index.values\n\n    index = list(set().union(male_variable_index, female_variable_index))\n\n    raw_male_variable = dataset.groupby(['Sex', variable]).size()['male']\n    raw_female_variable = dataset.groupby(['Sex', variable]).size()['female']\n\n    male_variable = []\n    female_variable = []\n\n    for i in range(max(index) + 1):\n        male_variable.append(raw_male_variable[i] if i in male_variable_index else 0)\n        female_variable.append(raw_female_variable[i] if i in female_variable_index else 0)\n\n    percent_male_variable = [male_variable[i]\/(male_variable[i] + female_variable[i]) if i in index else 0 for i in range(max(index) + 1)]\n    percent_female_variable = [female_variable[i]\/(male_variable[i] + female_variable[i]) if i in index else 0 for i in range(max(index) + 1)]\n\n    r = list(range(max(index) + 1))\n\n    fig, axs = plt.subplots(1,1,figsize=(18,4))\n    plt.bar(r, percent_male_variable, color='#ce2525')\n    plt.bar(r, percent_female_variable, bottom=percent_male_variable, linewidth=0, color='#ff6600')\n    plt.xticks(r, r)\n    plt.title('Proportion of Sex values by ' + variable)\n    axs.legend(labels=['male', 'female'])\n    axs.yaxis.set_major_formatter(PercentFormatter(xmax=1))","4017ba2b":"plot_sex_variable(train_df, 'Parch')\nchisq(train_df, 'Sex', 'Parch')","4f70fcd5":"plot_sex_variable(test_df, 'Parch')\nchisq(test_df, 'Sex', 'Parch')","910c7bd9":"plot_sex_variable(train_df, 'SibSp')\nchisq(train_df, 'Sex', 'SibSp')","0d1231ae":"plot_sex_variable(test_df, 'SibSp')\nchisq(test_df, 'Sex', 'SibSp')","0974ed66":"plot_sex_variable(train_df, 'Pclass')\nchisq(train_df, 'Sex', 'Pclass')","f5ed90c6":"plot_sex_variable(test_df, 'Pclass')\nchisq(test_df, 'Sex', 'Pclass')","a767ded5":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Age', x='Pclass', ax=ax1, data=train_df, linewidth=1, saturation=5, palette=palette3)\nax1.set_title('Train set')\nsns.boxplot(y='Age', x='Pclass', ax=ax2, data=test_df, linewidth=1, saturation=5, palette=palette3)\nax2.set_title('Test set')","36258d9f":"train_esq_sm = compute_anova(train_df, 'Age', 'Pclass')\ntest_esq_sm = compute_anova(test_df, 'Age', 'Pclass')\n\nprint('ANOVA 1-way for the train set: ', train_esq_sm)\nprint('ANOVA 1-way for the test set: ', test_esq_sm)","ca05469d":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Fare', x='Pclass', ax=ax1, data=train_df, linewidth=1, saturation=5, palette=palette3)\nax1.set_title('Train set')\nsns.boxplot(y='Fare', x='Pclass', ax=ax2, data=test_df, linewidth=1, saturation=5, palette=palette3)\nax2.set_title('Test set')","68b29b3c":"train_esq_sm = compute_anova(train_df, 'Fare', 'Pclass')\ntest_esq_sm = compute_anova(test_df, 'Fare', 'Pclass')\n\nprint('ANOVA 1-way for the train set: ', train_esq_sm)\nprint('ANOVA 1-way for the test set: ', test_esq_sm)","5bce0f3b":"def plot_pclass_variable(dataset, variable):\n    \"\"\"\n        Plots the proportion of variable values per Pclass value of a dataset.\n        \n        :param dataset: pandas DataFrame\n        :param variable: variable to plot\n    \"\"\"\n    first_variable_index = dataset.groupby(['Pclass', variable]).size()[1].index.values\n    second_variable_index = dataset.groupby(['Pclass', variable]).size()[2].index.values\n    third_variable_index = dataset.groupby(['Pclass', variable]).size()[3].index.values\n\n    index = list(set().union(first_variable_index, second_variable_index, third_variable_index))\n\n    raw_first_variable = dataset.groupby(['Pclass', variable]).size()[1]\n    raw_second_variable = dataset.groupby(['Pclass', variable]).size()[2]\n    raw_third_variable = dataset.groupby(['Pclass', variable]).size()[3]\n\n    first_variable = []\n    second_variable = []\n    third_variable = []\n\n    for i in range(max(index) + 1):\n        first_variable.append(raw_first_variable[i] if i in first_variable_index else 0)\n        second_variable.append(raw_second_variable[i] if i in second_variable_index else 0)\n        third_variable.append(raw_third_variable[i] if i in third_variable_index else 0)\n\n    percent_first_variable = [first_variable[i]\/(first_variable[i] + second_variable[i] + third_variable[i]) if i in index else 0 for i in range(max(index) + 1)]\n    percent_second_variable = [second_variable[i]\/(first_variable[i] + second_variable[i] + third_variable[i]) if i in index else 0 for i in range(max(index) + 1)]\n    percent_third_variable = [third_variable[i]\/(first_variable[i] + second_variable[i] + third_variable[i]) if i in index else 0 for i in range(max(index) + 1)]\n\n    r = list(range(max(index) + 1))\n\n    fig, axs = plt.subplots(1,1,figsize=(18,4))\n    plt.bar(r, percent_first_variable, color='#264e86')\n    plt.bar(r, percent_second_variable, bottom=percent_first_variable, linewidth=0, color='#0074e4')\n    plt.bar(r, percent_third_variable, bottom=percent_second_variable, linewidth=0, color='#74dbef')\n    plt.xticks(r, r)\n    plt.title('Proportion of Pclass values by ' + variable)\n    axs.legend(labels=['1', '2', '3'])\n    axs.yaxis.set_major_formatter(PercentFormatter(xmax=1))","15dbfaf6":"plot_pclass_variable(train_df, 'Parch')\nchisq(train_df, 'Pclass', 'Parch')","8e82f577":"plot_pclass_variable(test_df, 'Parch')\nchisq(test_df, 'Pclass', 'Parch')","86bb6e97":"plot_pclass_variable(train_df, 'SibSp')\nchisq(train_df, 'Pclass', 'SibSp')","5e2c28fb":"plot_pclass_variable(test_df, 'SibSp')\nchisq(test_df, 'Pclass', 'SibSp')","a920d2ca":"X_train = train_df[['Age', 'Fare', 'Parch', 'SibSp']].copy(deep=True).dropna()\n\nstd_scaler = StandardScaler()\nX_scaled = std_scaler.fit_transform(X_train)\n\nclf = ensemble.IsolationForest(contamination=0.01)\nclf.fit(X_scaled)\ny_pred = clf.predict(X_scaled)\n\nX_train['isOutlier'] = y_pred\n\noutliers_list = X_train.index[X_train['isOutlier'] == -1].tolist()\n\ndata_df.drop(outliers_list, inplace=True)\ntrain_df.drop(outliers_list, inplace=True)\n\nTRAINING_LENGTH = len(train_df)","51711a3a":"X_train[X_train['isOutlier'] == -1]","553310cb":"data_df['Sex'].replace(['male', 'female'], [0,1], inplace=True)","08d5c64e":"data_df['Fare'].fillna(data_df['Fare'].median(), inplace=True)","5eca193a":"# Apply log to Fare to reduce skewness distribution\ndata_df[\"Fare\"] = data_df[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n\nfig, ax = plt.subplots(figsize=(16,4))\ng = sns.distplot(data_df[\"Fare\"], ax=ax, color='#25627D', label=\"Skewness : %.2f\"%(data_df[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","ed37842c":"data_df['FareBin'] = pd.qcut(data_df['Fare'], 6)\n\nlabel = LabelEncoder()\ndata_df['FareBin_Code'] = label.fit_transform(data_df['FareBin'])\n\ndata_df.drop(['Fare'], 1, inplace=True)\ndata_df.drop(['FareBin'], 1, inplace=True)","af76f6f9":"train_df = data_df[:TRAINING_LENGTH]\ntest_df = data_df[TRAINING_LENGTH:]\n\nlogfare_jsd = JSD(train_df['FareBin_Code'].dropna().values, test_df['FareBin_Code'].dropna().values)\nprint('Jensen-Shannon divergence of Fare:', np.mean(logfare_jsd))\nprint('Standard deviation:', np.std(logfare_jsd))","a091e5bd":"fig, ax = plt.subplots(figsize=(16,4))\njsd = pd.DataFrame(np.column_stack([fare_jsd, logfare_jsd]), columns=['Fare', 'LogFare'])\nsns.boxplot(data=jsd, ax=ax, orient=\"h\", linewidth=1, saturation=5, palette=palette2)\nax.set_title('Jensen-Shannon divergences of Fare and LogFare')","def2d5f3":"Ticket = []\nfor i in data_df['Ticket'].values:\n    if not i.isdigit() :\n        Ticket.append(i.replace('.', '').replace('', '').strip().split()[0])\n    else:\n        Ticket.append('X')\n        \ndata_df['Ticket'] = Ticket","46701e9c":"data_df = pd.get_dummies(data_df, columns=['Ticket'], drop_first=True)","6fea9d48":"# Get Title from Name\ntitles = [i.split(',')[1].split('.')[0].strip() for i in data_df['Name']]\ndata_df['Title'] = pd.Series(titles, index=data_df.index)\n\nrare_titles = pd.Series(titles).value_counts()\nrare_titles = rare_titles[rare_titles < 10].index\n\ndata_df['Title'] = data_df['Title'].replace(rare_titles, 'Rare')\ndata_df['Title'] = data_df['Title'].map({'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Rare': 4})\ndata_df['Title'] = data_df['Title'].astype(int)","80f3f4c5":"data_df = pd.get_dummies(data_df, columns=['Title'], drop_first=True)","70a09c81":"data_df['Family_Size'] = data_df['Parch'] + data_df['SibSp'] + 1","4d0011c7":"tmp_train_df = data_df[:TRAINING_LENGTH].copy(deep=True)\ntmp_test_df = data_df[TRAINING_LENGTH:].copy(deep=True)\n\nfs_jsd = JSD(tmp_train_df['Family_Size'].dropna().values, tmp_test_df['Family_Size'].dropna().values)\nprint('Jensen-Shannon divergence of Family_Size:', np.mean(fs_jsd))\nprint('Standard deviation:', np.std(fs_jsd))","9ebcb995":"fig, ax = plt.subplots(figsize=(16,4))\njsd = pd.DataFrame(np.column_stack([parch_jsd, sibsp_jsd, fs_jsd]), columns=['Fare', 'FareBin', 'Family_Size'])\nsns.boxplot(data=jsd, ax=ax, orient=\"h\", linewidth=1, saturation=5, palette=palette3)\nax.set_title('Jensen-Shannon divergences of Parch, SibSp and Family_Size')","f1dd11c6":"print('Train dataset:')\ntrain_df.isnull().sum().to_frame('Missing values').transpose()","8e78e8bd":"print('Test\/Validation dataset:')\ntest_df.isnull().sum().to_frame('Missing values').transpose()","3cabb7de":"data_df.drop(['Name', 'Parch', 'SibSp'], axis = 1, inplace = True)","0eb3ea95":"data_df['Embarked'].fillna(data_df['Embarked'].mode()[0], inplace=True)","2b40eab2":"data_df = pd.get_dummies(data_df, columns=['Embarked'], drop_first=True)","bd6d63f4":"data_df['Cabin'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in data_df['Cabin'] ])","dd02993c":"palette9 = [\"#F8C7AA\", \"#F19B9C\", \"#EA708E\", \"#D54D88\", \"#A73B8F\", \"#7A2995\", \"#5B1F84\", \"#451764\", \"#300F45\"]\ng = sns.catplot(x='Cabin', y='Survived',saturation=5, aspect=2.5, data=data_df, kind='bar', order=['A','B','C','D','E','F','G','T','X'], palette=palette9)","3b744057":"data_df = pd.get_dummies(data_df, columns=['Cabin'], prefix='Deck', drop_first=True)","4ef13a36":"tmp_data_df = data_df.copy(deep = True)[['Age']]\n\nimp = SimpleImputer(missing_values=np.nan, strategy='median')\ntmp_data_df = pd.DataFrame(data=imp.fit_transform(tmp_data_df),index=tmp_data_df.index.values,columns=tmp_data_df.columns.values)","bfec5378":"tmp_data_df['AgeBin'] = pd.qcut(tmp_data_df['Age'], 5, duplicates='drop')\ntmp_data_df['AgeBin'].replace(np.NaN, -1, inplace = True)\n\nlabel = LabelEncoder()\ntmp_data_df['AgeBin_Code'] = label.fit_transform(tmp_data_df['AgeBin'])\ntmp_data_df.drop(['Age', 'AgeBin'], axis=1, inplace=True)\n\ndata_df['AgeBin_Code'] = tmp_data_df['AgeBin_Code']\ndata_df.drop(['Age'], 1, inplace=True)","fff0a4b5":"# Histogram comparison of Sex, Pclass, and Age by Survival\nh = sns.FacetGrid(data_df, row='Sex', col='Pclass', hue='Survived')\nh.map(plt.hist, 'AgeBin_Code', alpha=.75)\nh.add_legend()","3bddad3a":"train_df = data_df[:TRAINING_LENGTH]\ntrain_df.Survived = train_df.Survived.astype(int)\ntest_df = data_df[TRAINING_LENGTH:]","674d75df":"train_df.sample(5)","097386f8":"X = train_df.drop('Survived', 1)\ny = train_df['Survived']\nX_test = test_df.copy().drop(columns=['Survived'], axis=1)","79cdcacc":"std_scaler = StandardScaler()\nX = std_scaler.fit_transform(X)\nX_test = std_scaler.transform(X_test)","32fee653":"class CatBoostClassifierCorrected(CatBoostClassifier):\n    def fit(self, X, y=None, cat_features=None, sample_weight=None, baseline=None, use_best_model=None, \n            eval_set=None, verbose=None, logging_level=None, plot=False, column_description=None, verbose_eval=None, \n            metric_period=None, silent=None, early_stopping_rounds=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None):\n        # Handle different types of label\n        self.le_ = LabelEncoder().fit(y)\n        transformed_y = self.le_.transform(y)\n\n        self._fit(X=X, y=transformed_y, cat_features=cat_features, pairs=None, sample_weight=sample_weight, group_id=None,\n                  group_weight=None, subgroup_id=None, pairs_weight=None, baseline=baseline, use_best_model=use_best_model, \n                  eval_set=eval_set, verbose=verbose, logging_level=logging_level, plot=plot, column_description=column_description,\n                  verbose_eval=verbose_eval, metric_period=metric_period, silent=silent, early_stopping_rounds=early_stopping_rounds,\n                  save_snapshot=save_snapshot, snapshot_file=snapshot_file, snapshot_interval=None)\n        return self\n        \n    def predict(self, data, prediction_type='Class', ntree_start=0, ntree_end=0, thread_count=1, verbose=None):\n        predictions = self._predict(data, prediction_type, ntree_start, ntree_end, thread_count, verbose)\n\n        # Return same type as input\n        return self.le_.inverse_transform(predictions.astype(np.int64))","ca7ff47d":"# Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    # Ensemble Methods\n    ensemble.RandomForestClassifier(),\n    \n    # Nearest Neighbors\n    neighbors.KNeighborsClassifier(),\n    \n    # XGBoost\n    XGBClassifier(),\n    \n    # LightGBM\n    lgb.LGBMClassifier(),\n    \n    # CatBoost\n    CatBoostClassifierCorrected(iterations=100, logging_level='Silent')\n    ]\n\n# Split dataset in cross-validation with this splitter class\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0)\n\n# Create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n# Create table to compare MLA predictions\nMLA_predict = pd.Series()\n\n# Index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    # Set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    # Score model with cross validation\n    cv_results = model_selection.cross_validate(alg, X, y, cv  = cv_split)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    # If this is a non-bias random sample, then +\/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n    # Save MLA predictions - see section 6 for usage\n    alg.fit(X, y)\n    MLA_predict[MLA_name] = alg.predict(X)\n    \n    row_index+=1\n    \n# Print and sort table\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare","93e14c48":"fig, ax = plt.subplots(figsize=(16,6))\n\n# Barplot\nsns.barplot(x='MLA Test Accuracy Mean', y='MLA Name', ax=ax, data=MLA_compare, palette=sns.color_palette(\"coolwarm_r\", 5))\n\n# Prettify\nplt.title('Machine Learning Algorithm Accuracy Score')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","3967f085":"# Removed models w\/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    # Ensemble Methods: \n    ('rfc', ensemble.RandomForestClassifier()),\n    \n    # Nearest Neighbors:\n    ('knn', neighbors.KNeighborsClassifier()),\n    \n    # XGBoost:\n    ('xgb', XGBClassifier()),\n    \n    # LightGBM:\n    ('lgb', lgb.LGBMClassifier()),\n    \n    # CatBoost:\n    ('cat', CatBoostClassifierCorrected(iterations=100, logging_level='Silent'))\n]\n\n# Hard vote or majority rules\nvote_hard = ensemble.VotingClassifier(estimators = vote_est, voting = 'hard')\nvote_hard_cv = model_selection.cross_validate(vote_hard, X, y, cv = cv_split)\nvote_hard.fit(X, y)\n\nprint(\"Hard Voting Training w\/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting Test w\/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting Test w\/bin score 3*std: +\/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\nprint('-'*15)\n\n# Soft vote or weighted probabilities\nvote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\nvote_soft_cv = model_selection.cross_validate(vote_soft, X, y, cv  = cv_split)\nvote_soft.fit(X, y)\n\nprint(\"Soft Voting Training w\/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting Test w\/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting Test w\/bin score 3*std: +\/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\nprint('-'*15)","13729bc1":"# Hyper-parameter tuning with GridSearchCV:\ngrid_param = [\n            [{\n            # RandomForestClassifier\n            'criterion': ['gini'], #['gini', 'entropy'],\n            'max_depth': [8], #[2, 4, 6, 8, 10, None],\n            'n_estimators': [100], #[10, 50, 100, 300],\n            'oob_score': [False] #[True, False]\n             }],\n    \n            [{\n            # KNeighborsClassifier\n            'algorithm': ['auto'], #['auto', 'ball_tree', 'kd_tree', 'brute'],\n            'n_neighbors': [7], #[1,2,3,4,5,6,7],\n            'weights': ['distance'] #['uniform', 'distance']\n            }],\n    \n            [{\n            # XGBClassifier\n            'learning_rate': [0.05], #[0.05, 0.1,0.16],\n            'max_depth': [10], #[10,30,50],\n            'min_child_weight' : [6], #[1,3,6]\n            'n_estimators': [200]\n             }],\n    \n            [{\n            # LightGBMClassifier\n            'learning_rate': [0.01], #[0.01,0.05,0.1],\n            'n_estimators': [200],\n            'num_leaves': [300], #[300,900,1200],\n            'max_depth': [25], #[25,50,75],\n             }],\n    \n            [{\n            # CatBoostClassifier\n            'depth': [4],\n            'learning_rate' : [0.03],\n            'l2_leaf_reg': [4],\n            'iterations': [300],\n            'thread_count': [4]\n            }]\n        ]\n\nstart_total = time.perf_counter()\nfor clf, param in zip (vote_est, grid_param):\n    start = time.perf_counter()        \n    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n    best_search.fit(X, y)\n    run = time.perf_counter() - start\n\n    best_param = best_search.best_params_\n    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n    clf[1].set_params(**best_param) \n\nrun_total = time.perf_counter() - start_total\nprint('Total optimization time was {:.2f} minutes.'.format(run_total\/60))\n\nprint('-'*15)","f283db4e":"# Hard vote or majority rules w\/Tuned Hyperparameters\ngrid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\ngrid_hard_cv = model_selection.cross_validate(grid_hard, X, y, cv  = cv_split)\ngrid_hard.fit(X, y)\n\nprint(\"Hard Voting w\/Tuned Hyperparameters Training w\/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting w\/Tuned Hyperparameters Test w\/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting w\/Tuned Hyperparameters Test w\/bin score 3*std: +\/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\nprint('-'*15)\n\n# Soft vote or weighted probabilities w\/Tuned Hyperparameters\ngrid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\ngrid_soft_cv = model_selection.cross_validate(grid_soft, X, y, cv  = cv_split)\ngrid_soft.fit(X, y)\n\nprint(\"Soft Voting w\/Tuned Hyperparameters Training w\/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting w\/Tuned Hyperparameters Test w\/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting w\/Tuned Hyperparameters Test w\/bin score 3*std: +\/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\nprint('-'*15)","8eb1b7f6":"test_df['Survived'] = grid_soft.predict(X_test)\ntest_df['Survived'] = test_df['Survived'].astype(int)\nprint('Validation Data Distribution: \\n', test_df['Survived'].value_counts(normalize = True))\nsubmit = test_df[['Survived']]\n#submit.to_csv(\"..\/output\/submission.csv\", index=True)","c85e602b":"columns = [c for c in data_df.columns if 'Deck' in c or 'Embarked' in c or 'Ticket' in c]\n\nsimple_data_df = data_df.copy(deep=True)\nsimple_data_df.drop(columns=columns, axis=1, inplace=True)","ec47938c":"simple_data_df['Young'] = np.where((simple_data_df['AgeBin_Code']<2), 1, 0)\nsimple_data_df.drop(columns=['AgeBin_Code'], axis=1, inplace=True)","d1450f14":"simple_data_df['P1_Male'] = np.where((simple_data_df['Sex']==0) & (simple_data_df['Pclass']==1), 1, 0)\nsimple_data_df['P2_Male'] = np.where((simple_data_df['Sex']==0) & (simple_data_df['Pclass']==2), 1, 0)\nsimple_data_df['P3_Male'] = np.where((simple_data_df['Sex']==0) & (simple_data_df['Pclass']==3), 1, 0)\nsimple_data_df['P1_Female'] = np.where((simple_data_df['Sex']==1) & (simple_data_df['Pclass']==1), 1, 0)\nsimple_data_df['P2_Female'] = np.where((simple_data_df['Sex']==1) & (simple_data_df['Pclass']==2), 1, 0)\nsimple_data_df['P3_Female'] = np.where((simple_data_df['Sex']==1) & (simple_data_df['Pclass']==3), 1, 0)\n\nsimple_data_df.drop(columns=['Pclass', 'Sex'], axis=1, inplace=True)","37b6580d":"simple_train_df = simple_data_df[:TRAINING_LENGTH]\nsimple_test_df = simple_data_df[TRAINING_LENGTH:]\n\nsimple_data_df.sample(5)","1c2eae0d":"X = simple_train_df.drop('Survived', 1)\ny = simple_train_df['Survived']\nX_test = simple_test_df.copy().drop(columns=['Survived'], axis=1)\n\nstd_scaler = StandardScaler()\nX = std_scaler.fit_transform(X)\nX_test = std_scaler.transform(X_test)","8776db01":"# Hyper-parameter tuning with GridSearchCV:\ngrid_param = [\n            [{\n            # RandomForestClassifier\n            'criterion': ['gini'], #['gini', 'entropy'],\n            'max_depth': [8], #[2, 4, 6, 8, 10, None],\n            'n_estimators': [100], #[10, 50, 100, 300],\n            'oob_score': [False] #[True, False]\n             }],\n    \n            [{\n            # KNeighborsClassifier\n            'algorithm': ['auto'], #['auto', 'ball_tree', 'kd_tree', 'brute'],\n            'n_neighbors': [7], #[1,2,3,4,5,6,7],\n            'weights': ['distance'] #['uniform', 'distance']\n            }],\n    \n            [{\n            # XGBClassifier\n            'learning_rate': [0.05], #[0.05, 0.1,0.16],\n            'max_depth': [10], #[10,30,50],\n            'min_child_weight' : [6], #[1,3,6]\n            'n_estimators': [200]\n             }],\n    \n            [{\n            # LightGBMClassifier\n            'learning_rate': [0.01], #[0.01,0.05,0.1],\n            'n_estimators': [200],\n            'num_leaves': [300], #[300,900,1200],\n            'max_depth': [25], #[25,50,75],\n             }],\n    \n            [{\n            # CatBoostClassifier\n            'depth': [4],\n            'learning_rate' : [0.03],\n            'l2_leaf_reg': [4],\n            'iterations': [300],\n            'thread_count': [4]\n            }]\n        ]\n\nstart_total = time.perf_counter()\nfor clf, param in zip (vote_est, grid_param):\n    start = time.perf_counter()        \n    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n    best_search.fit(X, y)\n    run = time.perf_counter() - start\n\n    best_param = best_search.best_params_\n    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n    clf[1].set_params(**best_param) \n\nrun_total = time.perf_counter() - start_total\nprint('Total optimization time was {:.2f} minutes.'.format(run_total\/60))\n\nprint('-'*15)","22cba55b":"# Hard vote or majority rules w\/Tuned Hyperparameters\ngrid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\ngrid_hard_cv = model_selection.cross_validate(grid_hard, X, y, cv  = cv_split)\ngrid_hard.fit(X, y)\n\nprint(\"Hard Voting w\/Tuned Hyperparameters Training w\/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting w\/Tuned Hyperparameters Test w\/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting w\/Tuned Hyperparameters Test w\/bin score 3*std: +\/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\nprint('-'*15)\n\n# Soft vote or weighted probabilities w\/Tuned Hyperparameters\ngrid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\ngrid_soft_cv = model_selection.cross_validate(grid_soft, X, y, cv  = cv_split)\ngrid_soft.fit(X, y)\n\nprint(\"Soft Voting w\/Tuned Hyperparameters Training w\/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting w\/Tuned Hyperparameters Test w\/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting w\/Tuned Hyperparameters Test w\/bin score 3*std: +\/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\nprint('-'*15)","e5efb99c":"simple_test_df['Survived'] = grid_soft.predict(X_test)\nsimple_test_df['Survived'] = test_df['Survived'].astype(int)\nprint('Validation Data Distribution: \\n', simple_test_df['Survived'].value_counts(normalize = True))\nsubmit = simple_test_df[['Survived']]\n#submit.to_csv(\"..\/output\/submission.csv\", index=True)","28fc1429":"Great, we **reduced the divergence** between the train set and the test set a bit, making it a more consistent feature.","3fd55b17":"Test set:","c243faaf":"#### Dropping Name, Parch, SibSp\nLet's drop those features:\n* `Name`: was used to create the `Title` feature\n* `Parch`: was used to create the `Family_Size` feature\n* `SibSp`: was used to create the `Family_Size` feature","13e2d56d":"**Conclusion:** `Sex` and `Pclass` are considered **strongly dependent** both on the train set and test set.","35e30169":"**Conclusion:** contrary to `Parch` which is strongly linearly correlated to `SibSp`, `Pclass` and `SibSp` **are not** considered dependent both on the train set and test set.","1b1de9e0":"## Step 1: Defining the problem <a id=\"step1\"><\/a>\n\n### Kaggle description (as is)\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\n### Objective\nPredict who survived and who did not during the Titanic disaster, based on the features collected for us in the dataset: **BINARY CLASSIFICATION PROBLEM**.\n\n#### Dataset\n\nWe denote our *dataset* by $(X,Y) \\in \\chi^m \\times \\{0,1\\}^m$ where :\n* $\\chi$ is an abstract space of feature vectors\n* $X = (x_1, ..., x_m)$ is our vector of $m$ *feature vectors* where $x_i = (x_1^{(i)},...,x_n^{(i)})$\n* $Y = (y_1, ..., y_m)$ is our vector of labels\n\n#### Goal\n\nWe wish to find a good *classifier* $h$ mapping a vector in the abstract feature space to a binary output:\n$$\\begin{align*}\n  h \\colon \\chi &\\to \\{0,1\\}\\\\\n  x &\\mapsto y\n\\end{align*}$$\n\n*\"good\"* means we want to have a low *classification error (risk)* $\\mathcal{R}(h) = \\mathrm{P}(h(x) \\neq y)$.\n\n#### Hidden goal\n\n$y$ is distributed according to a *Bernoulli distribution* ($y \\in \\{0,1\\}$), so we write $y|x \\sim \\mathrm{Bernoulli}(\\eta(x))$, where $\\eta(x) = \\mathrm{P}(y=1|x) = \\mathrm{E}(y|x)$.\n\nThe problem is we don't have access to the distribution of $y|x$ which makes it hard to find the perfect classifier $\\eta$. Our goal is then not only to find a good classifier, but eventually to transform $x$ such that $y|x$ has a more predictable distribution for a potentially good classifier. In other words, we want our model to be able to have good generalization capabilities, as such we will apply a combination of multiple transformations on our dataset $X$. \n\n$X$ will then be mapped to a dataset $\\widetilde{X}$ in a different feature space $\\widetilde{\\chi} \\simeq [0,1]^n$.\n\nFor a more in-depth look at binary classification, feel free to read those notes: https:\/\/ocw.mit.edu\/courses\/mathematics\/18-657-mathematics-of-machine-learning-fall-2015\/lecture-notes\/MIT18_657F15_L2.pdf.","8833929d":"#### Pclass vs SibSp\nTrain set:","abefc699":"#### Sex vs Age","435d0922":"#### Parch","b98a4f2f":"## Our data science workflow\n* [**Step 1:** Defining the problem (description and objective)](#step1)\n* [**Step 2:** Gathering the data (automatic downloading)](#step2)\n* [**Step 3:** Performing exploratory data analysis (visualizing data, getting intuition)](#step3)\n* [**Step 4:** Preparing the data for consumption (data cleaning, feature engineering)](#step4)\n* [**Step 5:** Modeling the data (machine learning algorithms, optimizations)](#step5)\n* [**Step 6:** Drawing conclusions](#step6)","ccd7d286":"### 3.5 Exploring numerical features\nLet's plot the **Pearson's correlation matrix** of the raw numerical features to get a sense of linear correlations between them.\n\nThe coefficients of the matrix for variables $X$ and $Y$ are computed as follows:\n\n$$\\rho _{X,Y}={\\frac {\\operatorname {cov} (X,Y)}{\\sigma _{X}\\sigma _{Y}}}={\\frac {\\operatorname {E} [(X-\\mu _{X})(Y-\\mu _{Y})]}{\\sigma _{X}\\sigma _{Y}}}$$\n\nIt means that variables show a strong linear correlation if the absolute value of the coefficient is close to one.","fc435b2d":"#### Pclass vs Parch\nLet's first write a quick function to plot the proportion of `Pclass` by another discrete variable.","a5af4840":"#### Sex vs Fare","d8a82e82":"It seems that single passengers or with two other persons had more chance to survive.\n\nLet's now see how the test set is distributed compared to the training set.","9a86cf15":"#### Simplifying Age\nLet's create the `Young`boolean feature telling us if the passenger is young (< 2nd bin).","afd93044":"Train set:","4f97fa48":"`Age` and `Cabin` have quite a lot of missing values in both datasets, we will have to deal with those later.","6c33c099":"### 4.2. Feature engineering\nLet's now create and transform existing features to have stable distributions between both sets.\n### Sex\n#### Mapping Sex\nThe `Sex` feature can't be used as is, it has to be mapped to a boolean feature. Let's quickly map the values.","fc6b1be3":"#### Samples","56109f92":"It's curious how an embarkment has an influence on `Survival`, this must be related to another feature and we'll dive in with bivariate analysis.","0b25b408":"#### Reducing Fare skewness","5793ea58":"The `Fare` feature is right-skewed, if we want to make discriminant bins we'll have to address this concern later.\n\nThe skewness of a random variable $X$ is the third standardized moment $\\gamma _{1}$, defined as:\n\n$${\\displaystyle \\gamma _{1}=\\operatorname {E} \\left[\\left({\\frac {X-\\mu }{\\sigma }}\\right)^{3}\\right]={\\frac {\\mu _{3}}{\\sigma ^{3}}}={\\frac {\\operatorname {E} \\left[(X-\\mu )^{3}\\right]}{\\ \\ \\ (\\operatorname {E} \\left[(X-\\mu )^{2}\\right])^{3\/2}}}={\\frac {\\kappa _{3}}{\\kappa _{2}^{3\/2}}}}$$\n\nLet's now see how the test set is distributed compared to the train set.","289b0b0e":"Test set:","69285b12":"For the **train set**, the effect of `Pclass` on `Fare` is **high** (0.3019).\n\nFor the **test set**, the effect of `Pclass` on `Fare` is **high** (0.3331).\n\n**Conclusion:** the effect of `Pclass` on `Age` differs for about **3%** between the two sets.","86a295aa":"#### Simple statistics from the train set\n891 samples.","59c25f8c":"We can witness very different distributions when `Parch` is equal to 3 between both sets.\n\n**Conclusion**: `Pclass` and `Parch` are considered **strongly dependent** both on the train set and test set.","7f99fbe3":"## Step 6: Drawing conclusions <a id=\"step6\"><\/a>\nIt appears that our models capture some distributions from the engineered training dataset that differ slightly in the testing dataset: this is a sign of **overfitting**.\n\nWe did use a lot of features from our datasets and generally, if you want to avoid overfitting, **less is better**.  Remember the mapping of $X$ to a different feature space? We discovered through EDA that **the train set and the test set are not equally distributed**. We need a mapping that simplifies the distributions of features that have an influence on survival. And that's why this problem is hard.\n\n### 6.1. Simplifying our datasets\n\n#### Dropping Deck, Embarked and Ticket\nLet's drop the most ambiguous features.","49b834cb":"891 samples to predict the outcome of 418 samples is a pretty bad ratio (2.14), there is a high risk of overfitting the train set.","918377bd":"**Conclusion:** although they have similar correlation coefficients, distributions differ between both sets.","bc5e41b5":"`SibSp` looks like it is evenly distributed between both sets but it is quite not the case.\n\nLet's compute the JS divergence for `SibSp`, we will compare this value later with other features.","8769e7a7":"**Conclusion:** `Embarked` and `SibSp` are not considered independent on the train set but they are on the test set.","ac136b94":"Quite similar distributions between the train set and the test set as we can see.\n\n**Conclusion:** we can use `Sex` as is or mix it with other features.","495def33":"#### Train set","80e91656":"### Deck\n#### Creating Deck\nThere are a lot of missing values for the `Cabin` feature. This can be explained as some passengers didn't even have a cabin. Let's fill `NaN` values with `X` and extract the deck (letter) as it could indicate the location of the passenger's cabin on the boat.","ee36d585":"Let's see if we reduced the divergence between both sets.","e36254a6":"`Fare` looks almost evenly distributed between the train set and the test set.\n\nLet's compute the JS divergence for `Fare`, we will compare this value later with other features.","b42cb260":"#### Parch vs SibSp","3d969713":"### 3.4 Missing data\nLet's have a quick look at missing data on both sets.","3f219a8e":"### 3.6 Exploring categorical features\n\n### Univariate analysis\n\nLet's first analyze features individually.\n\n#### Embarked","3b702541":"**Conclusion:** to use `Age`, we will have to impute 20% missing data (not that easy), create bins to avoid overfitting and\/or mix it with other features.","16cd0453":"### Embarked\n#### Filling Embarked\nFor 2 missing values, let's fill `Embarked` with the most frequent value.","dec472fc":"It appears that on both sets, the proportion of male is higher from Southampton (S), thus influencing `Survival`.\n\n**Conclusion:** `Embarked` and `Sex` **are not** considered independent both on the train set and test set.","2b8b12d5":"For the **train set**, the effect of `Sex` on `Age` is **low** (0.0086).\n\nFor the **test set**, the effect of `Sex` on `Age` is **low** (1.6084e-10).\n\n**Conclusion:** the effect of `Sex` on `Age` differs for less than **1%** between the two sets.","8879e2e6":"###  5.2. Submission\nGood scores overall, let's prepare the data for submission.","b79269be":"For the **train set**, the effect of `Sex` on `Fare` is **low\/medium** (0.0332).\n\nFor the **test set**, the effect of `Sex` on `Fare` is **low\/medium** (0.0367).\n\n**Conclusion:** the effect of `Sex` on `Fare` differs for less than **1%** between the two sets.","d2d8fdd8":"#### Embarked vs SibSp\nTrain set:","d4017b50":"Great, we **reduced the divergence** between the train set and the test set a bit, making it a more consistent feature. We didn't lose much information as we are adding two linearly correlated features.","f492d717":"### 6.2. Data formatting\nLet's create our `X` and `y` and scale them.","96a81fed":"#### Pclass vs Fare","00f8b8b9":"#### Making Age bins","1ee8a37e":"#### Fare vs Parch","c0dbdc0e":"## Step 2: Gathering the data <a id=\"step2\"><\/a>\nThe data is available online as 3 CSV files at [https:\/\/www.kaggle.com\/c\/titanic\/data](https:\/\/www.kaggle.com\/c\/titanic\/data).\n\nLet's download them automatically.","760e84d6":"Wait a minute, **0.79904** accuracy after submission? (still top 14% though)","f849ef64":"### Bivariate analysis\nLet's then see if there is an impact of a feature on another.\n\n#### Continuous and categorical variables\n\nWhen dealing with continuous and categorical variables, we can look at statistical significance through variance analysis (**ANOVA**).\n\nIf we denote by $k_i$ the ith value for the continuous variable in the group, $n$ the number of passengers in each group, $T$ the sum of the continuous variable's values for all passengers and $N$ the number of passengers ; we can define $SS_{between}$ the *Sum of Squares Between*:\n\n$$SS_{between} = \\frac{\\sum(\\sum k_i)\u00b2}{n} - \\frac{T\u00b2}{N}$$\n\nIf we denote by $Y$ a value of the continuous variable ; we can define $SS_{total}$ the *Sum of Squares Total*:\n\n$$SS_{total} = \\sum Y\u00b2 - \\frac{T\u00b2}{N}$$\n\nWe then have access to the *effect size* $\\eta\u00b2$ which tells us how much the group has influenced the variable:\n\n$$\\eta\u00b2 = \\frac{SS_{between}}{SS_{total}}$$\n\nFor the value of $\\eta\u00b2$, we will refer to *Cohen's d* guidelines which are as follows:\n* Small effect: 0.01\n* Medium effect: 0.059\n* Large effect: 0.138","62fc5e3b":"Test set:","612ca99a":"### 6.4. Submission\nOk, let's prepare the data for submission.","55b4c1ac":"### Univariate analysis\nLet's first analyze features individually.\n\n#### Age","f06e6ba9":"#### Embarked vs Sex","1bcd0630":"### A glance at our dataset","f2d6df11":"#### Embarked vs Pclass\nTrain set:","da3bf4a9":"Wealthier passengers had more influence on the Titanic, it appears that they were more likely to find a place on a lifeboat.","b03fe087":"It appears that the proportion of whealthy people is higher from Cherbourg (C), thus influencing `Survival`.\n\n**Conclusion:** `Embarked` and `Pclass` are considered **strongly dependent** both on the train set and test set.","33c7660e":"**Conclusion:** `Age` and `Fare` tend to be much more linearly correlated on the test set than on the train set. (remember that the `Fare` distribution is skewed though.","515afd16":"For the **train set**, the effect of `Pclass` on `Age` is **medium\/high** (0.1363).\n\nFor the **test set**, the effect of `Pclass` on `Age` is **high** (0.2422).\n\n**Conclusion:** the effect of `Pclass` on `Age` differs for about **11%** between the two sets.","f3a5e2be":"#### Sex vs SibSp\nTrain set:","464717e7":"Train set:","a356ff8c":"### 5.2. Tune the model with ensemble methods\n\nLet's try to leverage ensemble methods to maximize accuracy on the test set.\n\nLet's try two ensemble methods:\n* **hard voting**: classification is the most frequent answer\n* **soft voting**: classification is based on the argmax of the sums of the predicted probabilities","28588439":"#### Sex vs Parch\nLet's first write a quick function to plot the proportion of `Sex` by another discrete variable.","c3602088":"#### Making Fare bins\nTo help our model better generalize, it often helps to use bins rather than raw values. Let's make `Fare` bins.","47d29802":"#### Simple statistics from the test set\n418 samples.","ccdae3b0":"Let's then compare the 3 most important features.","bf0546d6":"Even though it just looks like sums of Gaussian distributions, we can clearly observe the impact of `Age` on `Survival` with *very young passengers* and *probably parents passengers* having more chance to survive. (remember that about 20% of the data is missing)\n\nLet's now see how the test set is distributed compared to the train set.","187f74e2":"**Conclusion:** we can use `SibSp` as is or mix it with other features.","7f09ea37":"### 6.3. Hyper-parameter tuning and ensemble methods\nLet's try to leverage ensemble methods to maximize accuracy on the test set.","ff83f52c":"#### Deleted outliers (1% ratio)","cfbec535":"#### Differences between the distributions of the train set and the test set\nBy looking at the JS divergence, we can tell how the distributions of invidual features differ. Keep in mind that it is ok to observe some divergence.","510228f8":"## Changelog\nv1: initial submission.","ee5ad4a7":"# Titanic: A Pragmatic Approach\n> Not intended to be read by the absolute beginner.\n\nOverview of the problem: https:\/\/www.kaggle.com\/c\/titanic\n\n![https:\/\/pivotsprites.deviantart.com](https:\/\/imgur.com\/download\/vTprxLc)\n\n### Acknowledgments\nThis notebook has been heavily influenced by those *great* contributions:\n* [A Data Science Framework: To Achieve 99% Accuracy](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\/code) by LD Freeman\n* [Titanic Top 4% with ensemble modeling](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling) by Yassine Ghouzam\n* [Titanic: 2nd degree families and majority voting](https:\/\/www.kaggle.com\/erikbruin\/titanic-2nd-degree-families-and-majority-voting) by Erik Bruin\n* [Pytanic](https:\/\/www.kaggle.com\/headsortails\/pytanic\/code) by Heads or Tails\n* [Divide and Conquer [0.82296]](https:\/\/www.kaggle.com\/pliptor\/divide-and-conquer-0-82296) by Oscar Takeshita\n* [Titanic [0.82] - [0.83]](https:\/\/www.kaggle.com\/konstantinmasich\/titanic-0-82-0-83) by Konstantin","bb1375aa":"**Conclusion:** distributions look quite the same with strong correlation coefficients, we will combine them later.","3ec62dc0":"### 3.3 Meet the data\n\nThe dataset is briefly described here: [https:\/\/www.kaggle.com\/c\/titanic\/data](https:\/\/www.kaggle.com\/c\/titanic\/data)\n\nIt is composed of **11 independent variables** and **1 dependent variable**.\n\n**Variable description**\n\n|Variable|Definition|Key|Type|\n|--------|----------|---|----|\n|**Survived**|Survival|0 = No, 1 = Yes|**CATEGORICAL**|\n|**Pclass**|Ticket class|1 = 1st, 2 = 2nd, 3 = 3rd|**ORDINAL**|\n|**Name**|Passenger's name|N\/A|**MIXED**|\n|**Sex**|Passenger's sex|N\/A|**CATEGORICAL**|\n|**Age**|Passenger's age|N\/A|**CONTINUOUS**|\n|**SibSp**|# of siblings \/ spouses aboard the Titanic|N\/A|**DISCRETE**|\n|**Parch**|# of parents \/ children aboard the Titanic|N\/A|**DISCRETE**|\n|**Ticket**|Ticket number|N\/A|**MIXED**|\n|**Fare**|Passenger fare|N\/A|**CONTINUOUS**|\n|**Cabin**|Cabin number|N\/A|**MIXED**|\n|**Embarked**|Port of embarkation|C = Cherbourg, Q = Queenstown, S = Southampton|**CATEGORICAL**|","3294eabe":"### Fare\n#### Guessing Fare\nWe will see this below in greater detail but there is a missing value for `Fare`.\nLet's quickly fill this value with the median.","a2afa5e3":"#### SibSp","b34d8ac6":"Train set:","0ddc47f9":"## Step 5: Modeling the data <a id=\"step5\"><\/a>\n### 5.1. Model performance with Cross-Validation (CV)\nLet's quickly compare several classification algorithms with default parameters from `scikit-learn`, `xgboost`, `lightgbm` and `catboost` through cross-validation.","e76818df":"## Step 4: Preparing the data for consumption <a id=\"step4\"><\/a>\nWe will proceed with **outliers elimination** and **feature engineering**.\n### 4.1 Outliers elimination\nOutliers are usually bad for model generalization, let's drop a 1% ratio with the **Isolation Forest** algorithm on `Age`, `Fare`, `Parch`, `SibSp`. For more details on the algorithm, feel free to read the original paper: https:\/\/cs.nju.edu.cn\/zhouzh\/zhouzh.files\/publication\/icdm08b.pdf.","d051b39d":"**Conclusion:** although they have similar correlation coefficients, distributions differ between both sets.","399a9803":"*This is a work in progress. Comments and critical feedback are always welcome.*","e9f0dfe1":"Quite similar distributions between the train set and the test set as we can see.\n\n**Conclusion:** we can use `Pclass` as is or mix it with other features.","4d88e0f4":"### 3.2. Load the data modelling libraries","97c92f04":"#### Fare vs SibSp","4b686943":"### Age\n#### Guessing the Age\nThe `Age` feature has quite a lot of missing values but it is still manageable. It can easily be completed with the median value (remember, not the mean) but we will rather predict those values with a **MICE imputer** so that it better fits the distributions of the other features.\n\n**Side note:** for some reason it appears that the MICE imputer was removed from `sklearn 0.20`, weird.","0b9e10e7":"Test set:","51c4042d":"Three remarks:\n* it seems that `Fare` has the strongest linear correlation with `Survived`, making it a strong feature ;\n* `Parch` and `SibSp` show a potentially strong linear correlation, it might be a good idea to combine those features ;\n* except with `Fare`, the `Age` feature shows different correlation coefficients between the train set and the test set.\n\nBecause of that last remark, we will try to get more insights by computing the **Jensen-Shannon divergence** between the distributions of the train set and the test set. It is a measure of similarity between two probability distributions based on the **Kullback-Leibler divergence** well-known in information theory.\n\nIt is defined as:\n\n$${{\\rm {JSD}}}(P\\parallel Q)={\\frac  {1}{2}}D_{\\mathrm {KL}}(P\\parallel M)+{\\frac  {1}{2}}D_{\\mathrm {KL}}(Q\\parallel M)$$\n\nwhere $M={\\frac  {1}{2}}(P+Q)$ and $D_{\\mathrm {KL}}$ is the KL divergence.","1e85932a":"Ok so good results, but there is room for improvement. The reason is we didn't touch any of the hyperparameters of the voting models.\n\nLet's perform grid search on the different classifiers. **(careful, this will take A LOT OF time)**\n\n(everything is set though)","646cc820":"`Parch` looks like it is evenly distributed between both sets but it is quite not the case.\n\nLet's compute the JS divergence for `Parch`, we will compare this value later with other features.","fd369668":"#### Pclass","cd7fcc48":"### Bivariate analysis\nLet's then see if there is an impact of a feature on another.\n\n#### Age vs Fare","cb26c522":"#### Embarked vs Parch\nLet's first write a quick function to plot the proportion of `Embarked` by another discrete variable.","e298fdb7":"#### Embarked vs Fare","5d84c265":"#### Getting Deck dummy variables","dd68b129":"#### Pclass vs Age","96124dcf":"For the **train set**, the effect of `Embarked` on `Age` is **low** (0.0019).\n\nFor the **test set**, the effect of `Embarked` on `Age` is **low\/medium** (0.0327).\n\n**Conclusion:** the effect of `Embarked` on `Age` differs for about **3%** between the two sets.","55c82a34":"### Title\n#### Creating Title\nA feature that often helps categorize in this problem is `Title` derived from `Name`.","926fbd43":"### 4.2. Data formatting\n\nLet's create our `X` and `y` and scale them.","4f4ab798":"**Conclusion:** `Sex` and `Parch` are considered **strongly dependent** both on the train set and test set.","e9779eb2":"#### Merging Pclass and Sex\nLet's merge `Pclass` and `Sex`.","9ca57e03":"At first glance, we can say that if passengers happened to have a relatively small family on the Titanic, they were more likely to survive. We have to stay careful though because 3 and 5 have high standard deviations.\n\nLet's now see how the test set is distributed compared to the training set.","c5d3c926":"For the **train set**, the effect of `Embarked` on `Fare` is **medium\/high** (0.0823).\n\nFor the **test set**, the effect of `Embarked` on `Fare` is **medium\/high** (0.1064).\n\n**Conclusion:** the effect of `Embarked` on `Fare` differs for about **2.4%** between the two sets.","2dadb36e":"#### Sex\nEveryone watched *Titanic*, we all know that women were more likely to survive this disaster.","abd685f5":"### Ticket\n#### Extracting the prefix\nTicket's numbers may have a prefix that could be an indicator of the booking process (tied to wealth) and\/or location on the boat. Let's extract it.","041e8a7f":"So as we can see, the first models are pretty similar in terms of **accuracy**.","6b7d7b72":"### Family_Size\n#### Creating Family_Size\nFinally we are combining `Parch` and `SibSp`: `Family_Size = Parch + SibSp + 1`.","a76752cc":"#### Fare","9f37b69c":"**Conclusion:** there are noticeable differences on the distributions of those features between the train set and the test set. It can be stabilized by making age bins though.","2a93d4b2":"Test set:","2b11828c":"#### Getting Ticket dummy variables","f1c7f5db":"**Conclusion:** we can use `Parch` as is or mix it with other features.","469cb4b3":"**Conclusion:** `Sex` and `SibSp` are considered **strongly dependent** both on the train set and test set.","a352a709":"We see that the `Age` feature alone won't be of great help predicting survival on the test set since most of it is composed of 20-30 years passengers which is a range 50\/50 chance of survival.\n\nLet's compute the JS divergence for `Age`, we will compare this value later with other features.","b214511d":"#### Continuous and continuous variables\nWhen dealing with two continuous variables, we can look at statistical independence through the $\\chi^2$ test. \n\nIn its general statement, if there are $r$ rows and $c$ columns in the dataset, the *theoretical frequency* for a value, given the hypothesis of independence, is:\n\n$$E_{{i,j}}=Np_{{i\\cdot }}p_{{\\cdot j}}$$\n\nwhere $N$ is the total sample size, and:\n\n$$p_{{i\\cdot }}={\\frac  {O_{{i\\cdot }}}{N}}=\\sum _{{j=1}}^{c}{\\frac  {O_{{i,j}}}{N}}$$\n\nis the fraction of observations of type $i$ ignoring the column attribute, and:\n\n$${\\displaystyle p_{\\cdot j}={\\frac {O_{\\cdot j}}{N}}=\\sum _{i=1}^{r}{\\frac {O_{i,j}}{N}}}$$\n\nis the fraction of observations of type $j$ ignoring the row attribute. The term *frequencies* refers to absolute numbers rather than already normalised values.\n\nThe value of the test-statistic is:\n\n$$\\chi ^{2}=\\sum _{{i=1}}^{{r}}\\sum _{{j=1}}^{{c}}{(O_{{i,j}}-E_{{i,j}})^{2} \\over E_{{i,j}}} = N\\sum _{{i,j}}p_{{i\\cdot }}p_{{\\cdot j}}\\left({\\frac  {(O_{{i,j}}\/N)-p_{{i\\cdot }}p_{{\\cdot j}}}{p_{{i\\cdot }}p_{{\\cdot j}}}}\\right)^{2}$$\n\nThe null hypothesis $H_0$ is that the two variables are independent. We will then also look at the *p-value*. ($H_0$ rejected if $p \\leq 0.05$)","fc10b155":"#### Sex vs Pclass\nTrain set:","74bd142f":"**Conclusion:** It is worth noticing that `Embarked` and `Parch` **are not** considered independent on the test set but they **are** on the train set.","ae2a5bc4":"Test set:","086f87ce":"#### Embarked vs Age","58d2fa76":"Test set:","e23ad42c":"### Missing values\nLet's first have a quick look at missing values in the datasets:","6ec39f5b":"**Conclusion:** to use `Fare`, we will have to impute 1 missing value, fix the tailed distribution and create bins to avoid overfitting and\/or mix it with other features.","2924bb77":"## Step 3: Performing exploratory data analysis <a id=\"step3\"><\/a>\n\nKaggle is providing both **train** and **test** sets, we will perform EDA for each one of them.\n\n### 3.1. Import libraries\n\n**Visualization** is `matplotlib`\/`seaborn` based, **data preprocessing** is essentially `pandas` based,  and **modelling** is mostly `scikit-learn` based.","4279b4e3":"#### Test set","cdbfddf8":"#### Getting Embarked dummy variables","e88b2205":"Test set:","44fcdab8":"Quite similar distributions between the train set and the test set as we can see.\n\n**Conclusion:** we can use `Embarked` as is or mix it with other features.","04034481":"#### Getting Title dummy variables\nGreat, the feature shows some discrimination. It is not ordinal though, lets create dummy variables out of it. (we only need `k-1` columns)","02f5987a":"#### Age vs Parch"}}