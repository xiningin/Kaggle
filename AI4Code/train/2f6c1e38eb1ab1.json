{"cell_type":{"43b40d4a":"code","74c8fb50":"code","1d61b9a4":"code","074ee8d3":"code","cedff9fb":"code","0ec86797":"code","ccb02ca4":"code","5ca0b59b":"code","9b0868a4":"code","545331f4":"code","9fcc9433":"code","5d4f4b41":"code","a090706d":"code","ebd29725":"code","8a59b0a1":"code","52ac2472":"code","920cf8d0":"code","5d963fba":"code","7dd2a7ac":"code","a907e6fa":"code","7184edcf":"code","d902dda3":"code","dcbf58dc":"code","00442d46":"code","f8e7f29d":"markdown","a314c927":"markdown","73e2583d":"markdown","b21fc551":"markdown","f11f733c":"markdown","d4a90e33":"markdown","ea500454":"markdown","bb6beaae":"markdown","4b660744":"markdown","fc76c319":"markdown","81745e00":"markdown","5d811cf1":"markdown","fde2b7d5":"markdown","f9dfdfe1":"markdown","df5d0a0c":"markdown","cb1675dc":"markdown","ca824853":"markdown","12f4cf6e":"markdown","c4e90691":"markdown","7dca8aa5":"markdown"},"source":{"43b40d4a":"# importing packages\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, learning_curve\nfrom sklearn.metrics import average_precision_score, f1_score,confusion_matrix\nfrom xgboost.sklearn import XGBClassifier\nimport lightgbm as lgb \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom xgboost import plot_importance, to_graphviz\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report","74c8fb50":"# Loading dataset\ndf = pd.read_csv('..\/input\/paysim1\/PS_20174392719_1491204439457_log.csv')","1d61b9a4":"# Shape of dataset and top 5 rows\nprint(df.shape)\ndf.head()","074ee8d3":"# Checking is there any null values\ndf.isnull().sum()","cedff9fb":"# features which have dtypes object\ndf.select_dtypes(include = ['object']).head(3)","0ec86797":"# Features which have dtypes int and float\ndf.select_dtypes(include = ['int64','float64']).head(3)","ccb02ca4":"# Statistics description of each features\ndf.describe().T","5ca0b59b":"# count number of fraud and not fraud data\nprint(df.isFraud.value_counts())\nsns.countplot(data=df, x='isFraud')\nplt.ylabel('Count')\nplt.show()","9b0868a4":"# Count number of data point in each type of transaction\nprint(df.type.value_counts())\ndf.type.value_counts().plot(kind='bar')\nplt.show()","545331f4":"# Investigate variable \"isFlaggedFraud\"\npd.crosstab(df.isFraud,df.isFlaggedFraud)","9fcc9433":"#Groupby type\ndf.groupby('type')['isFraud','isFlaggedFraud'].sum()","5d4f4b41":"# Amount Vs number of transaction\nfraud = df[df['isFraud']==1]\nnormal = df[df['isFraud']==0]\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Amount per transaction by class')\nbins = 25\nax1.hist(fraud.amount, bins = bins)\nax1.set_title('Fraud')\nax2.hist(normal.amount, bins = bins)\nax2.set_title('Non Fraud')\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\n# plt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","a090706d":"%%time\n# Copy from Arjun Joshua\n# Feature extraction\ndata = df.copy()\n\n# Merchant flag for source and dist\ndata['OrigC']=data['nameOrig'].apply(lambda x: 1 if str(x).find('C')==0 else 0)\ndata['DestC']=data['nameDest'].apply(lambda x: 1 if str(x).find('C')==0 else 0)\n\n# flag for transfer and cashout from type feature\ndata['TRANSFER']=data['type'].apply(lambda x: 1 if x=='TRANSFER' else 0)\ndata['CASH_OUT']=data['type'].apply(lambda x: 1 if x=='CASH_OUT' else 0)","ebd29725":"# Calculating Amount error\ndata['OrigAmntErr']=(abs(data.oldbalanceOrg-data.newbalanceOrig)-data.amount)","8a59b0a1":"# drop list \n# droping those feature which are id type category and those which used for feature extraction\ndroplist=['isFlaggedFraud','type','nameDest','nameOrig']","52ac2472":"#print result\ndef model_result(clf,x_test,y_test):\n    y_prob=clf.predict_proba(x_test)\n    y_pred=clf.predict(x_test)\n    print('AUPRC :', (average_precision_score(y_test, y_prob[:, 1])))\n    print('F1 - score :',(f1_score(y_test,y_pred)))\n    print('Confusion_matrix : ')\n    print(confusion_matrix(y_test,y_pred))\n    print(\"accuracy_score\")\n    print(accuracy_score(y_test,y_pred))\n    print(\"classification_report\")\n    print(classification_report(y_test,y_pred))\n","920cf8d0":"# droping feature which is in droplist\n# Creating X and y for spliting dataset into test and train\nMLData=data.drop(labels=droplist,axis=1)\nX=MLData.drop('isFraud',axis=1)\nY=MLData.isFraud","5d963fba":"# splinting data into X_train, X_test, y_train, y_test\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3,random_state=42, shuffle=False)","7dd2a7ac":"%%time\n\nweights = (Y == 0).sum() \/ (1.0 * (Y == 1).sum()) # giving class weight\nclf = XGBClassifier( scale_pos_weight = weights, n_jobs = 4, random_state=42)\nclf.fit(X_train, y_train)\nprint ('Test')\nmodel_result(clf,X_test,y_test)","a907e6fa":"%%time\nclf2 = XGBClassifier()\nclf2.fit(X_train, y_train)\nprint ('Test')\nmodel_result(clf2,X_test,y_test)","7184edcf":"#split with 40 % of test size\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.4,random_state=42, shuffle=False)","d902dda3":"%%time\nclf3 = XGBClassifier()\nclf3.fit(X_train, y_train)\nprint ('Test')\nmodel_result(clf3,X_test,y_test)","dcbf58dc":"#split with 30 % of test size\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3,random_state=42, shuffle=False)","00442d46":"%%time\nfrom sklearn.ensemble import RandomForestClassifier\nr_clf=RandomForestClassifier()\nr_clf.fit(X_train,y_train)\nprint ('Test')\nmodel_result(r_clf,X_test,y_test)","f8e7f29d":"**Summary:**\n\nIn above model we are giving weight for fraud transaction label.\n\nSpliting data into 60-40 train-test\n\nAll performance matrix is giving excellent results.\n\nSo, the main focus parameter is FN & FP due to skew data.\n- Number of FP point is 0\n- Number of FN point is 2","a314c927":"1. Fraud occurs only in 2 type of transactions: \n  - TRANSFER and \n  - CASH_OUT\n\n\n2. The number of fraudulent TRANSFERs = 4097\n\n3. The number of fraudulent CASH_OUTs = 4116\n\n4. The type of transactions in which isFlaggedFraud is set : TRANSFER","73e2583d":"**Summary:**\n\nIn above model we are giving weight for fraud transaction label.\n\nIf we see all performance matrix is excellent.\n\nSo, the main focus parameter is FN & FP due to skew data.\n- Number of FP point is 1\n- Number of FN point is 2","b21fc551":"**Summary:**\n\nIn above model we are using as it as dataset.\n\nAll performance matrix is giving excellent results.\n\nSo, the main focus parameter is FN & FP due to skew data.\n- Number of FP point is 0\n- Number of FN point is 1","f11f733c":"**ML algorithm selection:** \n\n1. A first approach to deal with imbalanced data is to balance it by discarding the majority class before applying an ML algorithm. The disadvantage of undersampling is that a model trained in this way will not perform well on real-world skewed test data since almost all the information was discarded. \n\n2. A second approach might be to oversample the minority class, say by the synthetic minority oversampling technique (SMOTE) contained in the 'imblearn' library.\n\n3. I find, however, that the best result is obtained on the original dataset by using a ML algorithm based on ensembles of decision trees that intrinsically performs well on imbalanced data. Such algorithms not only allow for constructing a model that can cope with the missing values in our data, but they naturally allow for speedup via parallel-processing. Among these algorithms, XGBoost and random-forest algorithm used below slightly outperforms. Finally, XGBoost, RF, like several other ML algorithms, allows for weighting the positive class more compared to the negative class.","d4a90e33":"### ML Model","ea500454":"**Performance Metric selection:** \n\n- Since the data is highly skewed, I am using the area under the precision-recall curve (AUPRC) rather than the conventional area under the receiver operating characteristic (AUROC).[refer](http:\/\/pages.cs.wisc.edu\/~jdavis\/davisgoadrichcamera2.pdf)\n- F1 score is the harmonic mean of the precision and recall. The highest possible value of F1 is 1, indicating perfect precision and recall, and the lowest possible value is 0, if either the precision or the recall is zero.[wiki](https:\/\/en.wikipedia.org\/wiki\/F1_score)\n- We should do more focus on FP & FN.","bb6beaae":"### EDA","4b660744":"#### XGBoost without class weights","fc76c319":"#### XGBoost with class weights","81745e00":"**Summary:**\n\nIn above model we are not giving any weight for fraud transaction label.\n\nAll performance matrix is giving excellent results.\n\nSo, the main focus parameter is FN & FP due to skew data.\n- Number of FP point is 1\n- Number of FN point is 2","5d811cf1":"According to above info there are no such feature which have null value","fde2b7d5":"### Feature Engineering and Preprocessing","f9dfdfe1":"## Conclusion\n\nThoroughly interrogated the data with the help of EDA and go insight the data.\n\nGenerated new features with the help of existing features\n\nTo deal with the large skew in the data, we chose an appropriate metric and used an ML algorithm based on an ensemble of decision trees which works best with strongly imbalanced classes.\n\nTried different different ML model with upsampling and downsampling of data with the help of **imblearn** and **SMOTE** but\n\nRandom Forest with full dataset is giving best result comapring to all ML model.","df5d0a0c":"Less number of transaction amount in fraud comapare to non fraud data","cb1675dc":"From above info we can get different type of stats description corresponding each features","ca824853":"#### XGBoost without class weights and test size is 40 perc","12f4cf6e":"## Transaction is fraud or not?","c4e90691":"#### Random Forest","7dca8aa5":"Total number of data point is 6362620 in which 8213 is fraud and 6354407 is not fraud data point.\n\nWhich means this is imbalanced datatset."}}