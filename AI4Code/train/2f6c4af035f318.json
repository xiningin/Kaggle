{"cell_type":{"9d5e524f":"code","6666ceed":"code","6398a8b3":"code","c084de42":"code","96ca7ee6":"code","eb3bbac3":"code","b9fd3528":"code","47a63435":"code","4bd2540b":"code","5126632a":"code","85ff3ee6":"code","6d18d619":"code","0d84e7b9":"code","12158f57":"code","91279d9d":"code","8074c3dd":"code","5bbfcc2e":"code","5df6393d":"code","f1510e53":"code","35a26859":"code","ab20e5da":"code","1629141b":"code","2d6b5884":"code","2adb3af0":"code","fd5694fd":"code","7f47c5a9":"code","3be3341a":"code","32e57b3c":"code","2bdc6eee":"code","5fccc5c2":"code","49361d50":"code","81f189bc":"code","2c5edcb9":"code","23e9b608":"code","829fb27d":"code","0db528fc":"markdown","ebb3e50b":"markdown","2912bfd8":"markdown","9e016f6f":"markdown","ea449ebf":"markdown","4e3a57e1":"markdown","98c29fb9":"markdown","ceea9bcb":"markdown","8ead65fb":"markdown","e53c50f0":"markdown","e818c323":"markdown","e8f80a63":"markdown","557425d9":"markdown","35b6cf01":"markdown","8ea539cd":"markdown","e72fed5b":"markdown","97871f61":"markdown","9202389d":"markdown","68d0e383":"markdown","da968ac2":"markdown","e2d0945a":"markdown","6df5d26c":"markdown","2e844fff":"markdown","fc3400f5":"markdown","f939fa05":"markdown","7021cc71":"markdown","8681eca7":"markdown","57f0a46f":"markdown","2beb1286":"markdown","e075bdd1":"markdown","7136d399":"markdown","2c2171dd":"markdown","be32ef7d":"markdown","a0e7a32f":"markdown","5b0945ce":"markdown","282062a4":"markdown"},"source":{"9d5e524f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\nfrom sklearn.metrics import confusion_matrix, classification_report,accuracy_score,roc_curve, auc, precision_recall_curve, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier","6666ceed":"df_train = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/test.csv\")","6398a8b3":"df_train.head()","c084de42":"df_test.head()","96ca7ee6":"df_train.info()","eb3bbac3":"df_train.columns","b9fd3528":"numeric=['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndf_num=df_train.select_dtypes(include=numeric)\ndf_num.head(3)","47a63435":"df_cat=df_train.select_dtypes(include='object')\ndf_cat.head(3)","4bd2540b":"all_data = pd.concat([df_train,df_test])","5126632a":"all_data.drop('id',axis=1,inplace=True)","85ff3ee6":"null=pd.DataFrame(all_data.isnull().sum(),columns=[\"Null Values\"])\nnull[\"% Missing Values\"]=(all_data.isna().sum()\/len(all_data)*100)\nnull = null[null[\"% Missing Values\"] > 0]\nnull.style.background_gradient(cmap='viridis',low =0.2,high=0.1)","6d18d619":"describeNum = df_train.describe(include =['float64', 'int64', 'float', 'int'])\ndescribeNum.T.style.background_gradient(cmap='viridis',low=0.2,high=0.1)","0d84e7b9":"describeNumCat = df_train.describe(include=[\"O\"])\ndescribeNumCat.T.style.background_gradient(cmap='viridis',low=0.2,high=0.1)","12158f57":"cats = df_train.describe(include=[\"O\"])\nfor col in cats:\n    print(f'''Value count colunm {col}:''')\n    print(df_train[col].value_counts())\n    print()","91279d9d":"numeric = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4','cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10']","8074c3dd":"df_train[numeric].corr()","5bbfcc2e":"plt.figure(figsize=(30,20))\nax = sns.heatmap(data = df_train[numeric].corr(),cmap='YlGnBu',annot=True)\n\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5,top - 0.5)","5df6393d":"fig, ax =plt.subplots(1,2)\n\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(10,12))\nsns.set_context(\"paper\", font_scale=1)                                                  \nsns.countplot('target',data=all_data, ax=ax[0])\nall_data['target'].value_counts().plot.pie(explode=[0,0.2],autopct='%1.2f%%',ax=ax[1])\nfig.show()","f1510e53":"features_cat = ['cat0', 'cat1', 'cat2', 'cat3',\n                'cat4', 'cat5', 'cat6', 'cat7',\n                'cat8', 'cat9', 'cat10', 'cat11',\n                'cat12', 'cat13', 'cat14', 'cat15',\n                'cat16', 'cat17', 'cat18']","35a26859":"# plot distribution of categorical features\nfor f in features_cat:\n    plt.figure(figsize=(14,4))\n    df_train[f].value_counts().plot(kind='bar')\n    plt.title(f)\n    plt.grid()\n    plt.show()","ab20e5da":"featuresNum = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4','cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10']\n\nplt.figure(figsize=(15, 7))\nfor i in range(0, len(featuresNum)):\n    plt.subplot(1, len(featuresNum), i+1)\n    sns.boxplot(y=df_train[featuresNum[i]], color='green', orient='v')\n    plt.tight_layout()","1629141b":"all_data.cat0 = all_data.cat0.replace({'A':0,'B':1})\nall_data.cat1 = all_data.cat1.replace({'A':0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'H':7,'I':8,'J':9,'K':10,'L':11,'M':12,'N':13,'O':14})\nall_data.cat2 = all_data.cat2.replace({'A':0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'H':7,'I':8,'J':9,'K':10,'L':11,'M':12,'N':13,'O':14,'Q':15,'R':16,'S':17,'U':18,})\nall_data.cat3 = all_data.cat3.replace({'A':0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'H':7,'I':8,'J':9,'K':10,'L':11,'N':12})\nall_data.cat4 = all_data.cat4.replace({'A':0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'H':7,'I':8,'J':9,'K':10,'L':11,'M':12,'N':13,'O':14,'P':15,'Q':16,'R':17,'S':18,'T':19})\nall_data.cat6 = all_data.cat6.replace({'A':0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'I':7,'K':8,'M':9,'O':10,'Q':11,'S':12,'U':13,'W':14,'Y':15})\nall_data.cat9 = all_data.cat9.replace({'A':0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'I':7,'J':8,'L':9,'N':10,'O':11,'Q':12,'R':13,'S':14,'U':15,'V':16,'W':17,'X':18})\nall_data.cat11 = all_data.cat11.replace({'A':0,'B':1})\nall_data.cat12 = all_data.cat12.replace({'A':0,'B':1})\nall_data.cat13 = all_data.cat13.replace({'A':0,'B':1})\nall_data.cat14 = all_data.cat14.replace({'A':0,'B':1})\nall_data.cat15 = all_data.cat15.replace({'A':0,'B':1,'C':2,'D':3})\nall_data.cat16 = all_data.cat16.replace({'A':0,'B':1,'C':2,'D':3})\nall_data.cat17 = all_data.cat17.replace({'A':0,'B':1,'C':2,'D':3})\nall_data.cat18 = all_data.cat18.replace({'A':0,'B':1,'C':2,'D':3})","2d6b5884":"#Drop Column\nall_data = all_data.drop(['cat5'],axis=1)\nall_data = all_data.drop(['cat7'],axis=1)\nall_data = all_data.drop(['cat8'],axis=1)\nall_data = all_data.drop(['cat10'],axis=1)","2adb3af0":"all_data.head()","fd5694fd":"# ## Get all the numeric features in out dataset\n# numeric_features = all_data.skew().index\n\n# ## Getting all the skewed features (skew > 0.5 or skew < -0.5)\n# skewed_features = all_data[numeric_features].skew()[np.abs(all_data[numeric_features].skew()) > 0.5].index\n\n# ## Performing log(1+x) transformation\n# all_data[skewed_features] = np.log1p(all_data[skewed_features])","7f47c5a9":"df_train = all_data.iloc[:300000][:]\n\ndf_test = all_data[300000:][:]\ndf_test.drop('target',axis=1,inplace=True)","3be3341a":"df_train['target'] = df_train['target'].astype(int)","32e57b3c":"c = df_train.select_dtypes(exclude = [\"object\"]).columns\n\nfor a in range(len(c)):\n    print(\"Is there any negative value in '{}' column  : {} \".format(c[a],(df_train[c[a]]<0).any()))","2bdc6eee":"c = df_test.select_dtypes(exclude = [\"object\"]).columns\n\nfor a in range(len(c)):\n    print(\"Is there any negative value in '{}' column  : {} \".format(c[a],(df_test[c[a]]<0).any()))","5fccc5c2":"df_train=df_train.drop(df_train[df_train.cont0<0].index)\ndf_train=df_train.drop(df_train[df_train.cont3<0].index)\ndf_train=df_train.drop(df_train[df_train.cont5<0].index)\ndf_test=df_test.drop(df_test[df_test.cont0<0].index)\ndf_test=df_test.drop(df_test[df_test.cont3<0].index)\ndf_test=df_test.drop(df_test[df_test.cont5<0].index)\ndf_test=df_test.drop(df_test[df_test.cont6<0].index)","49361d50":"X = df_train.drop('target', axis = 1)\ny  = df_train['target']","81f189bc":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","2c5edcb9":"RFC = RandomForestClassifier()\n\n\nRFC.fit(X_train,y_train)\ny_pred_rf = RFC.predict(X_test)\n\nprint(\"Training Accuracy :\", RFC.score(X_train, y_train))\nprint(\"Testing Accuracy :\", RFC.score(X_test, y_test))\n\ncm = confusion_matrix(y_test, y_pred_rf)\nplt.rcParams['figure.figsize'] = (3, 3)\nsns.heatmap(cm, annot = True, cmap = 'YlGnBu', fmt = '.8g')\nplt.show()\n\ncr = classification_report(y_test, y_pred_rf)\nprint(cr)\n\n\nprint(\"------------------------------------------\")\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_pred_rf)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint(\"ROC Curves              =\",roc_auc)","23e9b608":"GBC = GradientBoostingClassifier(loss=\"deviance\",\n                                 n_estimators=100, \n                                 learning_rate=0.1,\n                                 max_depth=8,\n                                 min_samples_leaf=100,\n                                 max_features=0.1)\n\nGBC.fit(X_train,y_train)\n\ny_pred_rf = GBC.predict(X_test)\n\nprint(\"Training Accuracy :\", GBC.score(X_train, y_train))\nprint(\"Testing Accuracy :\", GBC.score(X_test, y_test))\n\ncm = confusion_matrix(y_test, y_pred_rf)\nplt.rcParams['figure.figsize'] = (3, 3)\nsns.heatmap(cm, annot = True, cmap = 'YlGnBu', fmt = '.8g')\nplt.show()\n\ncr = classification_report(y_test, y_pred_rf)\nprint(cr)\n\n\nprint(\"------------------------------------------\")\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_pred_rf)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint(\"ROC Curves              =\",roc_auc)","829fb27d":"ExtC = ExtraTreesClassifier()\n\nExtC.fit(X_train,y_train)\n\ny_pred_rf = ExtC.predict(X_test)\n\nprint(\"Training Accuracy :\", ExtC.score(X_train, y_train))\nprint(\"Testing Accuracy :\", ExtC.score(X_test, y_test))\n\ncm = confusion_matrix(y_test, y_pred_rf)\nplt.rcParams['figure.figsize'] = (3, 3)\nsns.heatmap(cm, annot = True, cmap = 'YlGnBu', fmt = '.8g')\nplt.show()\n\ncr = classification_report(y_test, y_pred_rf)\nprint(cr)\n\n\nprint(\"------------------------------------------\")\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_pred_rf)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint(\"ROC Curves              =\",roc_auc)","0db528fc":"For handling categorical data. We modtly use these 2 path:\n - OneHotEncoder\n - LabelEncoder\nWhere OneHotEncoder is used where data are not in any order and LabelEncoder when data is in order.","ebb3e50b":"### Statistical Summary","2912bfd8":"We will try to detect outliers for the numeric features and then remove them , But we will only remove those outliers which are a part of the train data i.e. having index within ntrain (defined earlier)","9e016f6f":"### Gradient boosting","ea449ebf":"## Getting the new train and test sets","4e3a57e1":"### Random Forest Classifier","98c29fb9":"The purpose of combine the dataframe are to avoid repeating all the operations (such as transformations, imputations, etc) done on the train set for the test set and to get more data for our analysis (because more data we get, the BETTER it is)","ceea9bcb":"## Combining Train and Test Dataframes","8ead65fb":"# Feature Engineering","e53c50f0":"# Exploratory Data Analysis","e818c323":"## Load Dataset","e8f80a63":"# Data Processing","557425d9":"### Correlation heatmap","35b6cf01":"## Description","8ea539cd":"### Categorical Features","e72fed5b":"### Target distribution analysis","97871f61":"## Dataset Description\nFor this competition, you will be predicting a continuous target based on a number of feature columns given in the data. All of the feature columns, cat0 - cat9 are categorical, and the feature columns cont0 - cont13 are continuous.","9202389d":"## Missing Value ","68d0e383":"## Data numeric","da968ac2":"## Graphic Approach","e2d0945a":"## Import Libraries","6df5d26c":"##### Store the number of rows or indexes for train and test dataset to separate them while performing modeling and prediction.","2e844fff":"# Tabular Playground Series - Feb 2021","fc3400f5":"## Label Encoding","f939fa05":"# Early Features Engineering","7021cc71":"# Data Exploration","8681eca7":"Now how to correlate between data variables. \n\nCorrelation is represented as a value between -1 and +1 where +1 indicates the highest positive correlation, -1 indicates the highest negative correlation, and 0 indicates no correlation.","57f0a46f":"### Categorical Value Counting","2beb1286":"# Modeling","e075bdd1":"## Outliers","7136d399":"## Data type identification","2c2171dd":"## Data categorical","be32ef7d":"### ExtraTrees ","a0e7a32f":"# Data Preparation","5b0945ce":"## Log Transforming all the Highly Skewed Features.","282062a4":"## Numerical Approach"}}