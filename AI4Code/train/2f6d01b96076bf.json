{"cell_type":{"dc3d16de":"code","44b9aea7":"code","788ad394":"code","12740208":"code","3d8fa9df":"code","167cbba8":"code","3b89e74e":"code","8d040388":"code","696cec05":"code","da0b48d2":"code","d0cbeed2":"code","b1c20335":"code","aa6e733e":"code","920c09f2":"code","92d51dd9":"code","2f24d6c5":"code","48b35b1f":"code","7b4e8c37":"code","0efe372b":"code","9aa223ad":"code","4a2b25d3":"code","c7a0a192":"code","09938f74":"code","7cafa1c5":"code","5910320a":"code","26a0443f":"code","e73f9898":"code","5d8efdfc":"markdown","a6c8fa96":"markdown","5607eb61":"markdown","de8fef35":"markdown"},"source":{"dc3d16de":"# Convert email into feature vector\n# Create Test & Training Set\n# Add Hyperparameters to:\n# - Strip email headers\n# - Convert to lowercase\n# - Remove punctuation\n# - Replace urls with \"URL\"\n# - Replace numbers with \"NUMBER\"\n# - Perform Stemming (trim word endings with library)","44b9aea7":"import pandas as pd\nimport numpy as np\nimport os\nimport email\nimport email.policy\nfrom bs4 import BeautifulSoup\n\nos.listdir('..\/input\/hamnspam\/')","788ad394":"ham_filenames = [name for name in sorted(os.listdir('..\/input\/hamnspam\/ham')) if len(name) > 20]\nspam_filenames = [name for name in sorted(os.listdir('..\/input\/hamnspam\/spam')) if len(name) > 20]","12740208":"print('Amount of ham files:', len(ham_filenames))\nprint('Amount of spam files:', len(spam_filenames))    \nprint('Spam to Ham Ratio:',len(spam_filenames)\/len(ham_filenames))","3d8fa9df":"def load_email(is_spam, filename):\n    directory = \"..\/input\/hamnspam\/spam\" if is_spam else \"..\/input\/hamnspam\/ham\"\n    with open(os.path.join(directory, filename), \"rb\") as f:\n        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n    \nham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\nspam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]\n    \n    \ntestEmail = ham_emails[0]","167cbba8":"print('Header Field Names:',testEmail.keys())\nprint('\\n\\n')\nprint('Message Field Values:',testEmail.values())\nprint('\\n\\n')\nprint('Message Content:',testEmail.get_content())","3b89e74e":"testEmailContent = testEmail.get_content()\ntype(testEmailContent)","8d040388":"testEmail['Subject']","696cec05":"print(spam_emails[6].get_content())","da0b48d2":"from collections import Counter\n\ndef get_email_structure(email):\n    if isinstance(email, str):\n        return email\n    payload = email.get_payload()\n    if isinstance(payload, list):\n        return \"multipart({})\".format(\", \".join([\n            get_email_structure(sub_email)\n            for sub_email in payload\n        ]))\n    else:\n        return email.get_content_type()\n\ndef structures_counter(emails):\n    structures = Counter()\n    for email in emails:\n        structure = get_email_structure(email)\n        structures[structure] += 1\n    return structures\n\nham_structure = structures_counter(ham_emails)\nspam_structure = structures_counter(spam_emails)","d0cbeed2":"ham_structure.most_common()","b1c20335":"spam_structure.most_common()","aa6e733e":"for email in spam_emails:\n    if get_email_structure(email) == 'text\/html':\n        testEmail = email\n        break\n\nprint(testEmail.get_content())","920c09f2":"def html_to_plain(email):\n    try:\n        soup = BeautifulSoup(email.get_content(), 'html.parser')\n        return soup.text.replace('\\n\\n','')\n    except:\n        return \"empty\"\n\nprint(html_to_plain(testEmail))","92d51dd9":"def email_to_plain(email):\n    struct = get_email_structure(email)\n    for part in email.walk():\n        partContentType = part.get_content_type()\n        if partContentType not in ['text\/plain','text\/html']:\n            continue\n        try:\n            partContent = part.get_content()\n        except: # in case of encoding issues\n            partContent = str(part.get_payload())\n        if partContentType == 'text\/plain':\n            return partContent\n        else:\n            return html_to_plain(part)\n\nprint(email_to_plain(ham_emails[42]))\nprint(email_to_plain(spam_emails[42]))","2f24d6c5":"import nltk\n\nstemmer = nltk.PorterStemmer()\nfor word in (\"Working\", \"Work\", \"Works\", \"Worked\"):\n        print(word, \"=>\", stemmer.stem(word))","48b35b1f":"#import url_extractor\n#url_extractor = urlextract.URLExtract()\n#print(url_extractor.find_urls(\"Will it detect github.com and https:\/\/youtu.be\/7Pqs\"))\n","7b4e8c37":"from sklearn.base import BaseEstimator, TransformerMixin\n# - Strip email headers\n# - Convert to lowercase\n# - Remove punctuation\n# - Replace urls with \"URL\"\n# - Replace numbers with \"NUMBER\"\n# - Perform Stemming (trim word endings with library)\nclass EmailToWords(BaseEstimator, TransformerMixin):\n    def __init__(self, stripHeaders=True, lowercaseConversion = True, punctuationRemoval = True, \n                 urlReplacement = True, numberReplacement = True, stemming = True):\n        self.stripHeaders = stripHeaders\n        self.lowercaseConversion = lowercaseConversion\n        self.punctuationRemoval = punctuationRemoval\n        self.urlReplacement = urlReplacement\n        #self.url_extractor = urlextract.URLExtract()\n        self.numberReplacement = numberReplacement\n        self.stemming = stemming\n        self.stemmer = nltk.PorterStemmer()\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X_to_words = []\n        for email in X:\n            text = email_to_plain(email)\n            if text is None:\n                text = 'empty'\n            if self.lowercaseConversion:\n                text = text.lower()\n                \n            #if self.urlReplacement:\n                #urls = self.url_extractor.find_urls(text)\n                #for url in urls:\n                #    text = text.replace(url, 'URL')   \n                    \n            if self.punctuationRemoval:\n                text = text.replace('.','')\n                text = text.replace(',','')\n                text = text.replace('!','')\n                text = text.replace('?','')\n                \n            word_counts = Counter(text.split())\n            if self.stemming:\n                stemmed_word_count = Counter()\n                for word, count in word_counts.items():\n                    stemmed_word = self.stemmer.stem(word)\n                    stemmed_word_count[stemmed_word] += count\n                word_counts = stemmed_word_count\n            X_to_words.append(word_counts)\n        return np.array(X_to_words)","0efe372b":"X_few = ham_emails[:3]\nXwordcounts = EmailToWords().fit_transform(X_few)\nXwordcounts","9aa223ad":"from scipy.sparse import csr_matrix\n\nclass WordCountToVector(BaseEstimator, TransformerMixin):\n    def __init__(self, vocabulary_size=1000):\n        self.vocabulary_size = vocabulary_size\n    def fit(self, X, y=None):\n        total_word_count = Counter()\n        for word_count in X:\n            for word, count in word_count.items():\n                total_word_count[word] += min(count, 10)\n        self.most_common = total_word_count.most_common()[:self.vocabulary_size]\n        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(self.most_common)}\n        return self\n    def transform(self, X, y=None):\n        rows = []\n        cols = []\n        data = []\n        for row, word_count in enumerate(X):\n            for word, count in word_count.items():\n                rows.append(row)\n                cols.append(self.vocabulary_.get(word, 0))\n                data.append(count)\n        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))","4a2b25d3":"vocab_transformer = WordCountToVector(vocabulary_size=10)\nX_few_vectors = vocab_transformer.fit_transform(Xwordcounts)\nX_few_vectors.toarray()","c7a0a192":"vocab_transformer.vocabulary_","09938f74":"from sklearn.pipeline import Pipeline\n\nemail_pipeline = Pipeline([\n    (\"Email to Words\", EmailToWords()),\n    (\"Wordcount to Vector\", WordCountToVector()),\n])","7cafa1c5":"from sklearn.model_selection import cross_val_score, train_test_split\n\nX = np.array(ham_emails + spam_emails)\ny = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","5910320a":"X_augmented_train = email_pipeline.fit_transform(X_train)","26a0443f":"from sklearn.linear_model import LogisticRegression\n\nlog_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\nscore = cross_val_score(log_clf, X_augmented_train, y_train, cv=3)\nscore.mean()\n","e73f9898":"from sklearn.metrics import precision_score, recall_score\n\nX_augmented_test = email_pipeline.transform(X_test)\n\nlog_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\nlog_clf.fit(X_augmented_train, y_train)\n\ny_pred = log_clf.predict(X_augmented_test)\n\nprint(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\nprint(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))","5d8efdfc":"## Turning Emails into plaintext\n","a6c8fa96":"## Creating a Pipeline","5607eb61":"## Data Exploration","de8fef35":"## Training a Model"}}