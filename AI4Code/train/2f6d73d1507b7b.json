{"cell_type":{"adc05c8b":"code","7f682a11":"code","cd3b961d":"code","b99b5d1c":"code","1c048624":"code","15a72b3b":"code","58599350":"code","8cf3b851":"code","009487c9":"code","60627710":"code","a9b781b2":"code","834416ce":"code","3f80a832":"code","6b518792":"code","c5829655":"code","92d040a3":"code","71438137":"code","c345f978":"code","b9dd64c7":"code","c40fbb19":"code","caeaa322":"code","6d1c4261":"code","ef1c04c2":"code","cf3582b0":"code","54f8926a":"code","83ba800d":"code","56a4eb8b":"code","b0a9522d":"code","de046786":"code","b9549a0c":"code","e233e0ce":"code","5fa7b705":"code","d25c3fe7":"code","e37aba23":"code","49dba62d":"code","5912a960":"code","019b3cbb":"code","ee4db230":"code","b2c713ee":"code","f5f9a5b9":"code","62d046b5":"code","8412853a":"code","01038c0c":"code","d8e539a7":"code","aec61d8c":"code","92908d74":"code","71711910":"code","7018f933":"code","573c0562":"code","b69c2dda":"code","ba48a227":"code","892ff168":"code","642fadf6":"code","dd323f59":"code","26386a0e":"code","3481f1f2":"code","53bfd202":"code","e98c705f":"code","e1269a2c":"code","bdbce15b":"code","6d55a67e":"code","4b2493b7":"code","d0001b15":"code","dfd03baa":"code","de021c0c":"code","48c71de6":"code","e6dbf0b6":"code","03c6a69a":"code","2ae6f5e5":"code","2998ca9f":"code","f57b603a":"code","fcb81d39":"code","b984d9c0":"code","a1cbda70":"code","3b92258b":"code","0fa08ec8":"code","6885134a":"code","6a5546bb":"code","3dbe26a6":"code","00ae71cf":"code","4127555d":"code","ebcdfb41":"code","fe4aa928":"code","69c4b5bd":"code","c027ea86":"code","0920ee72":"code","6203cbfa":"code","19eefb7b":"code","37518850":"code","a68cd4f8":"code","5bdab2ae":"markdown","7df7530e":"markdown","5f9973a7":"markdown","97956e87":"markdown","323de5f8":"markdown","1f7d96f0":"markdown","f46ca7b1":"markdown","2370e3e8":"markdown","9d3402c1":"markdown","2813f2a5":"markdown","a8c996c9":"markdown","b0fbf0e8":"markdown","99307e84":"markdown","d234fe36":"markdown","d1124085":"markdown","e68d5afd":"markdown","bdae65ec":"markdown","1e1848a2":"markdown","76c0e488":"markdown","1dee754a":"markdown","d50d366b":"markdown","15e2de06":"markdown","c239d75a":"markdown","14d7259f":"markdown","44d36206":"markdown","e4aa242c":"markdown","7685b0d7":"markdown","482178de":"markdown","7b7ae0b9":"markdown","141b46c1":"markdown","ea5f1309":"markdown","d043357d":"markdown","a25653b3":"markdown","2dfe4098":"markdown","3de81e98":"markdown","8cd92e00":"markdown","64b60efa":"markdown","69ccb35c":"markdown","4ed41710":"markdown","77f40673":"markdown","33e3a71e":"markdown","6988505e":"markdown","5cc32789":"markdown"},"source":{"adc05c8b":"import warnings\nfrom sklearn.exceptions import DataConversionWarning, ConvergenceWarning\nwarnings.filterwarnings(action='ignore')\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\nwarnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n","7f682a11":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()","cd3b961d":"import pandas as pd\nimport numpy as np\nimport os\nTRAIN_PATH = '..\/input\/house-prices-advanced-regression-techniques'\nTEST_PATH = '..\/input\/house-prices-advanced-regression-techniques'\ndef load_houses_data(TRAIN_PATH=TRAIN_PATH, TEST_PATH=TEST_PATH):\n    train_csv = os.path.join(TRAIN_PATH, 'train.csv')\n    test_csv = os.path.join(TEST_PATH, 'test.csv')\n    return pd.read_csv(train_csv), pd.read_csv(test_csv)","b99b5d1c":"X_train, X_test = load_houses_data()\ny_train = X_train['SalePrice']\ny_train_first = y_train","1c048624":"X_train.head()","15a72b3b":"X_train.info()","58599350":"X_train.std()","8cf3b851":"X_train.hist(figsize=(20, 20), bins=20)\nplt.show()","009487c9":"X_train['3SsnPorch'].describe()","60627710":"np.unique(X_train['BedroomAbvGr'].values)","a9b781b2":"X_train.groupby('BedroomAbvGr').count()['Id']","834416ce":"X_train.groupby('BsmtFullBath').count()['Id']","3f80a832":"X_train[['TotalBsmtSF', 'BsmtFinSF2', 'BsmtFinSF1', 'BsmtUnfSF']].head()","6b518792":"X_train[['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch']].describe()","c5829655":"X_train[['GarageType']].info()","92d040a3":"X_train.groupby('GarageFinish').count()['Id'] # Let it be in our dataset","71438137":"X_train.corr()['GarageArea']['GarageCars']","c345f978":"sns.distplot(X_train['LotArea'], bins=100)","b9dd64c7":"X_train.corr()['SalePrice'].sort_values()","c40fbb19":"X_train[['MasVnrArea']].hist(bins=100)","caeaa322":"X_train['YrSold'] = X_train['YrSold'].astype(str)\nX_train['MoSold'] = X_train['MoSold'].astype(str)","6d1c4261":"label_attrs = X_train.select_dtypes([object]).columns.values\nnum_attrs = X_train.select_dtypes([np.int64, np.float64]).columns.values\nnum_attrs = num_attrs[~(num_attrs == 'SalePrice')]","ef1c04c2":"from sklearn.base import TransformerMixin, BaseEstimator\nimport seaborn as sns\nclass Normalize(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        dataset = y_train.copy()\n        dataset = np.log1p(dataset)\n        return dataset\ny_train = Normalize().transform(y_train)\nsns.distplot(y_train)\n","cf3582b0":"X_train_label = X_train[label_attrs]\nX_train_num = X_train[num_attrs]","54f8926a":"from sklearn.preprocessing import StandardScaler\nX_train_num_std = pd.DataFrame(StandardScaler().fit_transform(X_train_num), columns=X_train_num.columns)","83ba800d":"cols = X_train_num_std.columns","56a4eb8b":"corr = X_train_num_std.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, vmax=1, center=0,vmin=-1 , \n            square=True, linewidths=.005)","b0a9522d":"corr = corr.iloc[1:, 1:]\ncorr = corr.applymap(lambda x : 1 if x > 0.75 else -1 if x < -0.75 else 0)\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, vmax=1, center=0,vmin=-1 , \n            square=True, linewidths=.005)","de046786":"X_train.corr()['SalePrice'].sort_values()","b9549a0c":"num_colinear_drop_attrs = ['GarageArea', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageYrBlt', '1stFlrSF']","e233e0ce":"class MergeBath(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self;\n    def transform(self, X, y=None):\n        X = X.copy()\n        X['Bath'] = X['HalfBath'] * X['FullBath']\n        X['HalfBath2'] = X['HalfBath'] ** 2\n        X['FullBath2'] = X['FullBath'] ** 2\n        X['BsmtBath'] = X['BsmtHalfBath'] * X['BsmtFullBath']\n        X['BsmtHalfBath2'] = X['BsmtHalfBath'] ** 2\n        X['BsmtFullBath2'] = X['BsmtFullBath'] ** 2\n        return X\nX_num_merged = MergeBath().transform(X_train_num)","5fa7b705":"class MergeBsmntFs(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self;\n    def transform(self, X, y=None):\n        X = X.copy()\n        X['BsmtFinSF'] = X['BsmtFinSF1'] * X['BsmtFinSF2']\n        X['BsmtFinSF12'] = X['BsmtFinSF1'] ** 2\n        X['BsmtFinSF22'] = X['BsmtFinSF2'] ** 2\n        return X\nX_num_bsmnt_proved = MergeBsmntFs().transform(X_num_merged)","d25c3fe7":"# PolynomialFeatures?\nclass MergePorches(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self;\n    def transform(self, X, y=None):\n        X = X.copy()\n        X['Porch'] = X['OpenPorchSF'] + X['EnclosedPorch'] + X['3SsnPorch'] + X['ScreenPorch']\n        X.drop(['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch'], axis=1, inplace=True)\n        return X\nX_num_porch_merged = MergePorches().transform(X_num_bsmnt_proved)","e37aba23":"class FilterLotAreaAndRooms(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self;\n    def transform(self, X, y=None):\n        X = X.copy()\n        X['LotArea'] = X['LotArea'].apply(lambda l: 50001 if l > 50000 else l)\n        X['BedroomAbvGr'] = X['BedroomAbvGr'].apply(lambda l: 5 if l > 5 else l)\n        return X\nX_num_lot_filtered = FilterLotAreaAndRooms().transform(X_num_porch_merged)","49dba62d":"class MergeLots(BaseEstimator, TransformerMixin) :\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X = X.copy()\n        X['Lot'] = X['LotArea'] + X['LotFrontage']\n        X.drop(['LotArea', 'LotFrontage'], axis=1, inplace=True)\n        X['Lot'] = X['Lot'].apply(lambda l: 30000 if l > 30000 else l)\n        return X\nX_num_lots_merged = MergeLots().transform(X_num_lot_filtered)\nsns.regplot(x='Lot', y=y_train, data = X_num_lots_merged)","5912a960":"i = 1;\nplt.figure(figsize=(20, 35))\nfor col in X_num_lots_merged.drop(num_colinear_drop_attrs, axis=1):\n    if col is not 'Id' and col is not 'SalePrice':\n        plt.subplot(10, 4, i)\n        sns.regplot(x=col, y=y_train, data=X_num_lots_merged)\n        i = i+1","019b3cbb":"num_scatter_drop = ['MSSubClass', 'LowQualFinSF']","ee4db230":"to_delete_outlires = ['GrLivArea', 'OverallCond', 'BsmtFinSF1', 'GarageCars',\n                      '2ndFlrSF', 'YearBuilt', 'YearRemodAdd'] #Think about garage cars","b2c713ee":"from sklearn.impute import SimpleImputer\nX_train_num_std_imputed = pd.DataFrame(SimpleImputer().fit_transform(X_train_num_std), \n                                       columns= X_train_num_std.columns)","f5f9a5b9":"i = 1;\nplt.figure(figsize=(20, 35))\nfor col in X_train_num.drop(num_colinear_drop_attrs, axis=1):\n    if col is not 'Id' and col is not 'SalePrice':\n        plt.subplot(10, 4, i)\n        sns.boxplot(x=X_train[col])\n        i = i+1","62d046b5":"from scipy import stats\nimport numpy as np\nz = pd.DataFrame(np.abs(stats.zscore(X_train_num)), columns=X_train_num.columns)","8412853a":"X_train.shape","01038c0c":"X_train_without_outlier = X_train[(z[to_delete_outlires] < 3).all(axis=1)]","d8e539a7":"y_train_without_outlier = y_train[(z[to_delete_outlires] < 3).all(axis=1)]","aec61d8c":"X_train_without_outlier.shape","92908d74":"# sns.distplot(X_train_num_std_imputed.LotFrontage)","71711910":"class DataFrameDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, drop_attrs=[]):\n        self.drop_attrs = drop_attrs\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X = X.copy()\n        X.drop(self.drop_attrs, axis=1, inplace=True, errors='ignore')\n        return X","7018f933":"num_drop_attrs = num_scatter_drop + num_colinear_drop_attrs\nX_num_dropped = DataFrameDropper(num_drop_attrs).transform(X_num_lot_filtered)","573c0562":"X_train_label.info()","b69c2dda":"label_drop_attrs = ['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'] # Think about FireplaceQu\nX_label_dropped = DataFrameDropper(label_drop_attrs).transform(X_train_label)","ba48a227":"from sklearn.impute import SimpleImputer\nX_label_imputed = pd.DataFrame(SimpleImputer(strategy=\"most_frequent\").fit_transform(X_label_dropped.values),\n                               columns=X_label_dropped.columns)","892ff168":"from sklearn.preprocessing import OneHotEncoder\nclass OneHotGoodEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.encoder = OneHotEncoder()\n    def fit(self, X, y=None): \n        self.encoder.fit(X)\n    def transform(self, X, y=None):\n        columns = X.columns\n        X_transformed = self.encoder.transform(X).toarray()\n        cats = self.encoder.categories_\n        i = 0\n        labels = []\n        for cat in cats:\n            for c in cat:\n                labels.append(columns[i] + ' : ' + c)\n            i = i+1\n        return pd.DataFrame(X_transformed, columns=labels)\n            ","642fadf6":"encoder = OneHotGoodEncoder()\nencoder.fit(X_label_imputed)\nX_label_encoded = encoder.transform(X_label_imputed)","dd323f59":"from sklearn.feature_selection import f_regression\nF, p_value = f_regression(X_label_encoded, y_train)\nnp.array(X_label_encoded.columns) + \" = \" + (p_value < 0.05).astype(str) ","26386a0e":"from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, OneHotEncoder, OrdinalEncoder\nencoder = OrdinalEncoder()\nX_label_encoded = pd.DataFrame(OrdinalEncoder().fit_transform(X_label_imputed), columns=X_label_imputed.columns)","3481f1f2":"X_label_analys = X_label_encoded.copy()\nX_label_analys['PriceSale'] = y_train.values","53bfd202":"\nlabel_new_drop_attrs = ['Utilities', 'LandSlope', 'YrSold', 'MoSold']\nX_label_new_analys = DataFrameDropper(label_new_drop_attrs).transform(X_label_analys)","e98c705f":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attrs):\n        self.attrs = attrs\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        return X.loc[:, self.attrs]\nclass LabelBinarizerPipelineFriendly(OneHotEncoder):\n    def fit(self, X, y=None):\n        \"\"\"this would allow us to fit the model based on the X input.\"\"\"\n        super(LabelBinarizerPipelineFriendly,self).fit(X)\n    def transform(self, X, y=None):\n        return super(LabelBinarizerPipelineFriendly, self).transform(X).toarray()\n    def fit_transform(self, X, y=None):\n        return super(LabelBinarizerPipelineFriendly, self).fit(X).transform(X)\n\n","e1269a2c":"from sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n\nnum_pipeline = Pipeline([\n    ('selection', DataFrameSelector(num_attrs)),\n    ('merge_bath', MergeBath()),\n    ('merge_bsmnt', MergeBsmntFs()),\n    ('merge_porch', MergePorches()),\n    ('filter', FilterLotAreaAndRooms()),\n    ('drop', DataFrameDropper(num_drop_attrs)),\n    ('impute', SimpleImputer()),\n    ('std_scale', StandardScaler()),\n])\n\nlabel_pipeline = Pipeline([\n    ('selection', DataFrameSelector(label_attrs)),\n    ('drop', DataFrameDropper(label_new_drop_attrs)),\n    ('impute', SimpleImputer(strategy=\"most_frequent\")),\n#     ('encode', OrdinalEncoder()), # one hot is  better \n    ('encode', OneHotEncoder(sparse=False, handle_unknown='ignore')),\n    ('std_scale', StandardScaler()),\n])\n\nfull_pipeline = FeatureUnion([\n    ('num_pipline', num_pipeline),\n    ('label_pipeline', label_pipeline),\n])\n\n\nX_train_cleaned = pd.DataFrame(full_pipeline.fit_transform(X_train_without_outlier))","bdbce15b":"y_train = y_train_without_outlier","6d55a67e":"X_train_cleaned.head()","4b2493b7":"from sklearn.linear_model import LinearRegression\nlinear_model = LinearRegression()\nlinear_model.fit(X_train_cleaned, y_train)","d0001b15":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_validate\n\ndef analys_model(model):\n    some_data = X_train.iloc[:5]\n    some_label = y_train.iloc[:5]\n    some_data_prepared = full_pipeline.transform(some_data)\n    print(f\"\\x1b[31mPredictions are \\033[92m{model.predict(some_data_prepared)}\")\n    print(f\"\\x1b[31mLables are \\033[92m{list(some_label)}\")\n    housing_prediction = model.predict(X_train_cleaned)\n    scores = cross_validate(model, X_train_cleaned, y_train, scoring=\"neg_mean_squared_error\", cv=3)\n    rmse_scores = np.sqrt(-scores['test_score'])\n    print(f\"\\x1b[31mScores : \\033[92m{rmse_scores}\")\n    print(f\"\\x1b[31mMean : \\033[92m{rmse_scores.mean()}\")\n    print(f\"\\x1b[31mStandard Deviation : \\033[92m{rmse_scores.std()}\")","dfd03baa":"analys_model(linear_model)","de021c0c":"from sklearn.linear_model import SGDRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nsgd_grid = {\n    'n_iter_no_change': [10, 20, 30, 40, 50, 60, 80, 100, 130, 140],\n    'eta0': [0.4, 0.2, 0.1, 0.05, 0.03, 0.01, 0.009, 0.004],\n}\nsgd_model = SGDRegressor()\nsgd_best = RandomizedSearchCV(sgd_model, sgd_grid, verbose=2, cv=3,n_jobs=-1, \n                              scoring=\"neg_mean_squared_error\").fit(X_train_cleaned, y_train).best_estimator_\n\nsgd_best\nsgd_best.fit(X_train_cleaned, y_train)\nanalys_model(sgd_best)","48c71de6":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_model = Pipeline([\n    ('poly_feature', PolynomialFeatures(degree=2, include_bias=False)),\n    ('std_scale', StandardScaler()),\n    ('lin_reg', LinearRegression())\n])\npoly_model.fit(X_train_cleaned, y_train)\nanalys_model(poly_model)\nplt.plot(y_train[:100])\nplt.plot(poly_model.predict(X_train_cleaned[:100]), 'r')","e6dbf0b6":"from sklearn.linear_model import Ridge \nridge_grid = {\n    'alpha': np.linspace(0, 1, num=500),\n    'solver' : ['cholesky'],\n}\nridge_model = Ridge()\nridge_best =  RandomizedSearchCV(ridge_model, ridge_grid, verbose=2, cv=3,n_jobs=-1, \n                              scoring=\"neg_mean_squared_error\").fit(X_train_cleaned, y_train).best_estimator_\nridge_best.fit(X_train_cleaned, y_train)\nanalys_model(ridge_best)","03c6a69a":"from sklearn.linear_model import Lasso\nlasso_grid = {\n    'alpha': np.linspace(0, 1e-3, num=1000),\n}\nlasso_model =  Lasso()\nlasso_best =  RandomizedSearchCV(lasso_model, lasso_grid, verbose=2, cv=3,n_jobs=-1, \n                              scoring=\"neg_mean_squared_error\").fit(X_train_cleaned, y_train).best_estimator_\nlasso_best.fit(X_train_cleaned, y_train)\nanalys_model(lasso_best)","2ae6f5e5":"from sklearn.linear_model import LassoLars\nlasso_lars_grid = {\n    'alpha': np.linspace(0, 1e-3, num=10000),\n    'max_iter' : [int(x) for x in np.linspace(1, 110, num = 100)]\n}\nlasso_lars_model = LassoLars()\nlasso_lars_best =  RandomizedSearchCV(lasso_lars_model, lasso_lars_grid, verbose=2, cv=3,n_jobs=-1, \n                              scoring=\"neg_mean_squared_error\").fit(X_train_cleaned, y_train).best_estimator_\nlasso_lars_best.fit(X_train_cleaned, y_train)\nanalys_model(lasso_lars_best)","2998ca9f":"from sklearn.base import clone\nclass GradientBoostingOtherRegressor(TransformerMixin, BaseEstimator):\n    def __init__(self, estimator, n_estimates = 3):\n        self.estimator = estimator\n        self.estimators = []\n        self.n_estimates = n_estimates\n    def fit(self, X, y_train=None):\n        last_estimator = self.estimator\n        last_estimator.fit(X, y_train)\n        y = y_train.values\n        self.estimators.append(last_estimator)\n        for i in range(self.n_estimates):\n            y = y - last_estimator.predict(X)\n            new_estimator = clone(self.estimator)\n            new_estimator.fit(X, y)\n            last_estimator = new_estimator\n            self.estimators.append(last_estimator)\n        return self\n    def predict(self, X_test):\n        y_pred = sum(tree.predict(X_test) for tree in self.estimators)\n        return y_pred","f57b603a":"gbor = GradientBoostingOtherRegressor(ridge_best, n_estimates=4)\ngbor.fit(X_train_cleaned, y_train)\nanalys_model(gbor)","fcb81d39":"from sklearn.linear_model import ElasticNet\nelastic_grid = {\n    'alpha': np.linspace(0, 1e-2, num=10000),\n    'l1_ratio' : np.linspace(0, 1, num=10)\n}\nelastic_model = ElasticNet()\nelastic_best =  RandomizedSearchCV(elastic_model, elastic_grid, verbose=2, cv=3,n_jobs=-1, \n                              scoring=\"neg_mean_squared_error\").fit(X_train_cleaned, y_train).best_estimator_\nelastic_best.fit(X_train_cleaned, y_train)\nanalys_model(elastic_best)","b984d9c0":"from sklearn.svm import SVR","a1cbda70":"from sklearn.model_selection import GridSearchCV\nsvm_linear_grid = {\n    'epsilon' : np.linspace(0, 0.5, num=200),\n}\nsvm_linear_model = SVR(kernel='linear')\nsvm_linear_best = RandomizedSearchCV(svm_linear_model, svm_linear_grid, verbose=2, cv=3, n_jobs=-1, \n                              scoring='neg_mean_squared_error').fit(X_train_cleaned, y_train).best_estimator_\nsvm_linear_best.fit(X_train_cleaned, y_train)\nanalys_model(svm_linear_best)\n","3b92258b":"svm_poly_grid = {\n    'epsilon' : np.linspace(0, 0.5, num=200),\n}\nsvm_poly_model = SVR(kernel='poly')\nsvm_poly_best = RandomizedSearchCV(svm_poly_model, svm_poly_grid, verbose=2, cv=3, n_jobs=-1, \n                              scoring='neg_mean_squared_error').fit(X_train_cleaned, y_train).best_estimator_\nsvm_poly_best.fit(X_train_cleaned, y_train)\nanalys_model(svm_poly_best)","0fa08ec8":"from sklearn.tree import DecisionTreeRegressor\ndt_model = DecisionTreeRegressor()\ndt_model.fit(X_train_cleaned, y_train)\n\nmax_features = [int(x) for x in np.linspace(1, 270, num = 30)]\nmax_depth = [1, 2, 4, 5, 6, 9, 10, 12 , None]\nmin_samples_leaf = [int(x) for x in np.linspace(1, 10, num = 5)]\nrandom_grid = {'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_leaf': min_samples_leaf}\n\nrandom_search =  RandomizedSearchCV(estimator = dt_model, param_distributions = random_grid,\n                                    n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1, scoring=\"neg_mean_squared_error\")\nrandom_search.fit(X_train_cleaned, y_train) \ndt_best = random_search.best_estimator_\ndt_best.fit(X_train_cleaned, y_train)\nanalys_model(dt_best)","6885134a":"from sklearn.ensemble import AdaBoostRegressor\ndt_ada_model = AdaBoostRegressor(dt_best, n_estimators=200, learning_rate=0.5)\ndt_ada_model.fit(X_train_cleaned, y_train)\nanalys_model(dt_ada_model)","6a5546bb":"from sklearn.ensemble import RandomForestRegressor\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 5)]\nmax_features = ['auto', 10, 20, 40, 90, 140, 200, 250]\nmax_depth = [int(x) for x in np.linspace(1, 1000, num = 20)]\nmax_depth.append(None)\nmin_samples_leaf = [5, 10, 15]\nbootstrap = [True, False]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nrf = RandomForestRegressor()","3dbe26a6":"random_search =  RandomizedSearchCV(estimator = rf, param_distributions = random_grid,\n                                    n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1, scoring=\"neg_mean_squared_error\")\nrandom_search.fit(X_train_cleaned, y_train) ","00ae71cf":"rf_best = random_search.best_estimator_\nrf_best.fit(X_train_cleaned, y_train)\nanalys_model(rf_best)","4127555d":"from sklearn.ensemble import AdaBoostRegressor\ndt_ada_model = AdaBoostRegressor(rf_best, n_estimators=10, learning_rate=0.5)\ndt_ada_model.fit(X_train_cleaned, y_train)\nanalys_model(dt_ada_model)","ebcdfb41":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nX_t, X_v, y_t, y_v = train_test_split(X_train_cleaned, y_train, random_state=42, test_size=0.2)\n\ngradient_reg = GradientBoostingRegressor(\n    n_estimators=1000, \n    random_state=42, \n    learning_rate=0.1, \n    min_samples_split=10,\n    max_features='sqrt',\n    max_depth=5\n)\ngradient_reg.fit(X_t, y_t)\n\nerrors = [mean_squared_error(y_v, y_pred) for y_pred in gradient_reg.staged_predict(X_v)]\nbest_n_estimators = np.argmin(errors)\n\nplt.plot(errors)\n\ngradient_best = GradientBoostingRegressor(\n    n_estimators=best_n_estimators, \n    random_state=42, \n    learning_rate=0.1, \n    min_samples_split=10,\n    max_features='sqrt',\n    max_depth=5\n)\ngradient_best.fit(X_train_cleaned, y_train)\n\nanalys_model(gradient_best)\n\nprint(f'min estimator {best_n_estimators}')\n","fe4aa928":"from sklearn.ensemble import VotingRegressor\nvoting_model = VotingRegressor(\n    estimators=[('ridge', ridge_best), ('lasso', lasso_best), ('elastic', elastic_best), ('svm', svm_linear_best),\n               ('rf', gradient_best), ('dt', dt_ada_model)],\n    n_jobs=-1\n)\nvoting_model.fit(X_train_cleaned, y_train)\nanalys_model(voting_model)","69c4b5bd":"from sklearn.model_selection import KFold\nfrom sklearn.base import clone, RegressorMixin\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n    def fit(self, X, y=None):\n        X = X.values\n        y = y.values\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n    def get_metafeatures(self, X):\n        return np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","c027ea86":"stacked_averaged_models = StackingAveragedModels(base_models = [gradient_best, dt_ada_model, elastic_best, lasso_lars_best, svm_linear_best],\n                                                 meta_model = LinearRegression())\nstacked_averaged_models.fit(X_train_cleaned, y_train)","0920ee72":"meta_features = pd.DataFrame(stacked_averaged_models.get_metafeatures(X_train_cleaned))\n","6203cbfa":"i = 0;\nplt.figure(figsize=(20, 15))\nfor col in meta_features:\n    plt.subplot(3, 3, i+1)\n    sns.regplot(x=meta_features[i], y=y_train)\n    i = i+1","19eefb7b":"analys_model(stacked_averaged_models)","37518850":"X_test_clean = full_pipeline.transform(X_test)\npredictions = stacked_averaged_models.predict(X_test_clean)\nfinal_prediction = pd.DataFrame({'Id': X_test['Id'],\n                                'SalePrice': np.expm1(predictions)})","a68cd4f8":"final_prediction.to_csv('prediction.csv', index=False)","5bdab2ae":"* First we use ada boost with our random","7df7530e":"* We should drop from orange blocks\n* `GarageCars` should remain, and `GarageArea` should be deleted.\n* `GrLivArea` should remain, and `BedroomAbvGr` and `TotRmsAbvGrd` should be deleted.\n* `TotalBsmtSF` should remain, and `1stFlrSF` should be deleted.\n* `YearBuilt` should remain, and `GarageYrBlt` should be deleted.","5f9973a7":"## Colinearity","97956e87":"## Create Pipeline ","323de5f8":"If all classes of a category was false we will delete it.","1f7d96f0":"### Linearing And Removing Outliers","f46ca7b1":"### SVM - Poly","2370e3e8":"### Merge FullBath and HalfBath","9d3402c1":"### Encoding Label","2813f2a5":"### Merge Lots\n","a8c996c9":"Its hard to select from them by eye. so we filter it!","b0fbf0e8":"It seems polynomial regression overfitted. With more degrees also it doesnt get better and get very slow. ","99307e84":"## Train SVM Regression","d234fe36":"### Merge BsmntFS and add Unfinished Fraction","d1124085":"## Train Decision Tree Regression","e68d5afd":"## Train Random Forest Regression\n","bdae65ec":"### Boost Decison Tree\n","1e1848a2":"## Train Linear Regression","76c0e488":"### Impute","1dee754a":"## Work With Numbers","d50d366b":"# Ensemble Methods\n\n## Voting ","15e2de06":"## Work With Labels","c239d75a":"### Boost Random Forest","14d7259f":"### Analys Labels","44d36206":"### Filter Lot Area above 50000 and Room above 5","e4aa242c":"## Train Elastic Net","7685b0d7":"* should drop id.\n* almost all 3SsnPorch are zero and we can delete it.\n* we can group rooms 5 or more into one single column.\n* can merge full and half bathrooms into one filed name bathroom\n* we can drop two type of finished and have one finished basement feet\n* we can change basement unfinished into a fraction of finished\/unfinished (It got bad correlation)\n* can merge porchs into one field.\n* garage year built can be droped and have a single year built for house.\n* we could have just garage area between cars and area.\n* we cad drop kitchen also. becasue a bunch of them are one.\n* can filter Lot Area above 50000 to 50001\n* we **can** drop MSSubClass, OverallCond, YrSold, LowQualFinSF, Id, MiscVal, BsmtHalfBath, BsmtFinSF2, 3SsnPorch, MoSold, PoolArea because of their corrolation\n* It's not important when it was sold. So we drop MoSold and YrSold.\n* Overall Qual is important but Overall cond not!\n* We can add a luxury style field to show having pool or not and other fantasy features.\n* we can have just one of YearRemodAdd or YearBuilt.\n","482178de":"### SVM - Linear","7b7ae0b9":"* Then try a gradient boost with a weak random forest ( max_depth = 5 ); also, we try to find number of estimator with early stoping. ","141b46c1":"### Dropping very null attributes","ea5f1309":"### Analys labels using p-value\n","d043357d":"### Dropping ","a25653b3":"## Train SGD Regressor","2dfe4098":"## Stacking Our Model\n","3de81e98":"### Normalize Sale Price\n","8cd92e00":"## Train Lasso Regression","64b60efa":"## Train Polynomial Regression\n","69ccb35c":"# House Prices \n<img src=\"http:\/\/bridgingandcommercial.co.uk\/cms\/upload\/image\/dab1369294471_213_126864066.jpg\">\nData set is [here](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data).","4ed41710":"# Regularized Linear Models\n## TrainRidge Regression","77f40673":"Because sgd is also a linear model but with selection of theta using stochastic gradient it should not go very better than linear regression.","33e3a71e":"# Sumbit Test Set","6988505e":"### Merge Porchs ","5cc32789":"### Try to boost with gradient method"}}