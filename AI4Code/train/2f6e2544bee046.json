{"cell_type":{"1e76de2b":"code","7130fb00":"code","a95b95d8":"code","5509b8fc":"code","4aa5e5dd":"code","a376ee2c":"code","4b3c71de":"code","b9b02402":"code","efaca8c9":"code","41f1dc47":"code","8cb672c6":"code","39ed9a73":"code","909b13b1":"code","92868433":"code","bfe32c74":"code","a732fac6":"code","b67b13d3":"code","0fb97b69":"code","595deb4e":"code","04b21c16":"code","9b8a8b2c":"code","386a472d":"code","382b95a8":"code","36ba36cb":"code","c0efbede":"code","4ca6a849":"code","f069346e":"code","c803c89f":"markdown","5a8f0e25":"markdown","e421c5d2":"markdown","4ebf3ac8":"markdown","c68f8950":"markdown","297788e8":"markdown","a5a026e3":"markdown","a0435cea":"markdown","2d8fc3fa":"markdown","fa305df2":"markdown","5a1eb2a6":"markdown","56ca8b22":"markdown","f6a4ae48":"markdown","c4d6498e":"markdown","a81efc8f":"markdown","1e09549e":"markdown","ef4d986d":"markdown","f8c86e88":"markdown","707b6bbd":"markdown","9a5e21fc":"markdown"},"source":{"1e76de2b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport time\nimport random\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences \nfrom tensorflow.compat.v1.keras.layers import CuDNNGRU\n\nimport warnings as wrn\nwrn.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7130fb00":"train_set = pd.read_csv(\"\/kaggle\/input\/game-review-dataset\/train_gr\/train.csv\")\ntest_set = pd.read_csv('\/kaggle\/input\/game-review-dataset\/test_gr\/test.csv')\ngame_ov = pd.read_csv('\/kaggle\/input\/game-review-dataset\/train_gr\/game_overview.csv')","a95b95d8":"train_set.head()","5509b8fc":"test_set.head()","4aa5e5dd":"game_ov.head()","a376ee2c":"train_set.info()","4b3c71de":"sns.countplot(train_set[\"user_suggestion\"])\nplt.show()","b9b02402":"# Dropping unrelevant features\nx = train_set[\"user_review\"]\ny = train_set[\"user_suggestion\"]\n\n","efaca8c9":"def cleanTexts(texts):\n    cleaned = []\n    pattern = \"[^a-zA-Z0-9]\"\n    for text in texts:\n        clrd = re.sub(pattern,\" \",text).lower().strip()\n        cleaned.append(clrd)\n    return cleaned\n\n","41f1dc47":"cleanTexts([\"If it works great, it will remove something  ()}12451235\"])","8cb672c6":"x_cleaned = cleanTexts(x)\nx_cleaned[0]","39ed9a73":"# Tokenizer \ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(x_cleaned)\nx_tokens = tokenizer.texts_to_sequences(x_cleaned)","909b13b1":"print(x_tokens[0])\nprint()\n\nprint(len(x_tokens[0]))\nprint(len(x_tokens[1]))\nprint(len(x_tokens[2]))","92868433":"len_arr = [len(s) for s in x_tokens]\nMAX_LEN = int(np.percentile(len_arr,.75))","bfe32c74":"import json\nwith open(\"maxlen.json\",mode=\"w\") as F:\n    json.dump({\"maxlen\":MAX_LEN},F)\n    ","a732fac6":"print(MAX_LEN)","b67b13d3":"x_tokens_pad = pad_sequences(x_tokens,maxlen=MAX_LEN)\nx_tokens_pad.shape\n","0fb97b69":"x_train,x_test,y_train,y_test = train_test_split(x_tokens_pad,np.asarray(y),test_size=0.2,random_state=42)","595deb4e":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","04b21c16":"VOCAB_SIZE = len(tokenizer.word_index) + 1\n# We've added 1 because of padding\n\n# Each world will be 100D vector.\nVECTOR_SIZE = 100\n\ndef buildModel(MAX_LEN,embedding_weights=None):\n    \n    model = keras.Sequential()\n    if embedding_weights is not None:\n        model.add(layers.Embedding(input_dim=VOCAB_SIZE,\n                                   output_dim=VECTOR_SIZE,\n                                   input_length=MAX_LEN,\n                                   weights=[embedding_weights],\n                                   trainable=True\n                              ))\n        \n    else:\n        model.add(layers.Embedding(input_dim=VOCAB_SIZE,\n                                   output_dim=VECTOR_SIZE,\n                                   input_length=MAX_LEN\n                                  ))\n    \n    model.add(CuDNNGRU(512,return_sequences=True))\n    model.add(CuDNNGRU(1024,return_sequences=True))\n    model.add(CuDNNGRU(1024,return_sequences=False))\n    model.add(layers.Dense(1,activation=\"sigmoid\"))\n    \n    model.compile(optimizer=\"Adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n    return model","9b8a8b2c":"model = buildModel(MAX_LEN)\nmodel.summary()","386a472d":"word2vec = {} # Trained glove model \nwith open(\"..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt\",encoding=\"UTF-8\") as f:\n    for line in f:\n        values = line.split() \n        word = values[0]\n        vec = np.asarray(values[1:],dtype=\"float32\")\n        word2vec[word] = vec\n        ","382b95a8":"# initializing as uniform\nembedding_matrix = np.random.uniform(-1,1,(VOCAB_SIZE,100))\n\nfor word,i in tokenizer.word_index.items():\n    if i<VOCAB_SIZE: \n        embedding_vector = word2vec.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n","36ba36cb":"model = buildModel(MAX_LEN,embedding_matrix)","c0efbede":"model.summary()","4ca6a849":"model.fit(x_train,y_train,epochs=3,validation_split=0.2)","f069346e":"y_pred = model.predict_classes(x_test)\n\naccuracy_sc = round(accuracy_score(y_pred=y_pred,y_true=y_test)*100,2)\nconf_matrix = confusion_matrix(y_pred=y_pred,y_true=y_test)\n\n\nprint(\"Accuracy score is {}% \".format(accuracy_sc))\n\nplt.subplots()\nsns.heatmap(conf_matrix,annot=True,linewidths=1.5,fmt=\".1f\")\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Actual\")\nplt.show()","c803c89f":"* Let's check our function.","5a8f0e25":"* As you can see sequences has different shapes but neural networks works with the data that has a constant shape. Let's solve this problem by padding.\n* First we'll make an array that includes lengths of sequences and then we'll find the third quartile's value.\n* Our new sequences will have length third quartile.","e421c5d2":"### Step 2: Tokenizing and Padding Data\nIn this section we'll convert our texts into sequences by matching each word with an integer. Then we'll make sure that each sequence has same length by adding 0's to the short ones and trimming long ones.","4ebf3ac8":"# Final Test\nIn this section I am going to test our model with test set.","c68f8950":"* We can consider our set balanced, great news guys!!!\n\nNow let's start to process our dataset, we'll follow steps below:\n\n1. Cleaning and Lowering Data\n1. Tokenizing and Padding\n1. Train Test Splitting","297788e8":"* As you can see, test set is useless because there are no labels in it.","a5a026e3":"# Loading Pre-trained Word Embeddings \n\nIn this section we'll load pre-trained word embeddings.","a0435cea":"* And let's check our sequences.","2d8fc3fa":"# Building Model\nIn this section I am going to build deep neural network using Tensorflow. But before start to implementation I wanna talk about GRU a bit.\n\nIn deep learning when we work with sequences (such as music is a sequence of notes and texts are sequences of words) we use Recurrent Neural Networks, because they have memories, they evaluate every part of sequences.\n\nBut when we use Recurrent Neural Networks (I'll call them RNN after this) we encounter with a big problem: **vanishing gradient**. Because of the backpropagation of neural networks we encounter with this problem.\n\nBut if we use LSTMs (Long Short Term Memories, developed version of Simple RNNs) or GRU (Gated Recurrent Units) we don't encounter with this problem, because these networks have some data filters that we named **forget gates**.","fa305df2":"# Data Overview and Data Preprocessing\nIn this section we'll take a look at the data and then we'll process it to train a deep neural network.","5a1eb2a6":"* We have 17k sample.\n* Let's check class distribution.","56ca8b22":"* Then we've created our embedding matrix and if a value in our set is in the pre trained vector we changed the value of it.","f6a4ae48":"### Step 1: Cleaning and Lowering The Data\nIn this section we'll drop redundant features from the data and define a function that clean the data.","c4d6498e":"* Also we'll save this value to a json file in order to use in the future.","a81efc8f":"# Introduction\nHello people, welcome to this kernel. In this kernel I am going to classify game reviews collected from Steam. I will use deep learning based approach and tensorflow.\n\n# Table of Content\n1. Preparing Environment\n1. Data Overview and Preprocessing\n1. Building Model\n1. Loading Pre-trained Word Embeddings\n1. Training Model | Displaying Results\n1. Final Test\n1. Conclusion","1e09549e":"### Step 3: Train Test Splitting\nIn this section we'll split our set into train and test, we won't use test set in training.","ef4d986d":"# Training Model | Displaying Results\nIn this section I am going to train our model.","f8c86e88":"# Preparing Environment\n* We'll prepare our environment in this section, we'll import libraries and the data we'll use.","707b6bbd":"* Model does not have balance problem, so we can say not bad for %73.","9a5e21fc":"* First we've read word vectors from text file and created a python dictionary."}}