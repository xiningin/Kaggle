{"cell_type":{"9eeb79e4":"code","44367379":"code","e0186cc7":"code","8a6256a2":"code","f04c761b":"code","10a1ead3":"code","a01bd3a2":"code","729e77c3":"code","a09e7e30":"code","e577f8d0":"code","5bbbdf7d":"code","61790859":"code","1eea622c":"code","e9731a24":"code","8f0179a1":"code","40e48aa4":"code","49f2d720":"code","bdb1e7cf":"code","a0a3847b":"code","aba53a67":"code","9c7ae7f3":"code","87a70a9d":"markdown","24e497c1":"markdown","b6414f41":"markdown","294c6597":"markdown","d4bdc0fc":"markdown","adc060a6":"markdown","e7746142":"markdown","3e975cd8":"markdown","e698daa9":"markdown","21f368a8":"markdown","4da6a331":"markdown","4ae87937":"markdown","46401211":"markdown","1c859569":"markdown","a95bd92b":"markdown","454285c1":"markdown","e17cdaa1":"markdown","be0afed5":"markdown"},"source":{"9eeb79e4":"from PIL import Image\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom keras.applications import vgg16\nfrom keras.preprocessing.image import load_img,img_to_array\nfrom keras.models import Model\nfrom keras.applications.imagenet_utils import preprocess_input\n\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport pandas as pd","44367379":"imgs_path = \"\/kaggle\/input\/fashion-product-images-dataset\/fashion-dataset\/fashion-dataset\/images\/\"\nimgs_model_width, imgs_model_height = 224, 224\nnb_closest_images = 5","e0186cc7":"files = [imgs_path + x for x in os.listdir(imgs_path) if \"jpg\" in x]\n\nprint(\"Total number of images:\",len(files))","8a6256a2":"#For reducing compilation time of the algorithm, we reduce the data to 5000 images or the system crashes!\nfiles=files[0:5000]","f04c761b":"original = load_img(files[9], target_size=(imgs_model_width, imgs_model_height))\nplt.imshow(original)\nplt.show()\nprint(\"Image loaded successfully!\")","10a1ead3":"# load the model\nvgg_model = vgg16.VGG16(weights='imagenet')\n\n# remove the last layers in order to get features instead of predictions\nfeat_extractor = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer(\"fc2\").output)\n\n# print the layers of the CNN\nfeat_extractor.summary()","a01bd3a2":"numpy_image = img_to_array(original)\n# convert the image \/ images into batch format\n# expand_dims will add an extra dimension to the data at a particular axis\n# we want the input matrix to the network to be of the form (batchsize, height, width, channels)\n# thus we add the extra dimension to the axis 0.\nimage_batch = np.expand_dims(numpy_image, axis=0)\nprint('Image Batch size', image_batch.shape)\n\n# prepare the image for the VGG model\nprocessed_image = preprocess_input(image_batch.copy())","729e77c3":"img_features = feat_extractor.predict(processed_image)\n\nprint(\"Features successfully extracted for one image!\")\nprint(\"Number of image features:\",img_features.size)\nimg_features","a09e7e30":"importedImages = []\n\nfor f in files:\n    filename = f\n    original = load_img(filename, target_size=(224, 224))\n    numpy_image = img_to_array(original)\n    image_batch = np.expand_dims(numpy_image, axis=0)\n    \n    importedImages.append(image_batch)\n    \nimages = np.vstack(importedImages)\n\nprocessed_imgs = preprocess_input(images.copy())","e577f8d0":"imgs_features = feat_extractor.predict(processed_imgs)\n\nprint(\"features successfully extracted!\")\nimgs_features.shape","5bbbdf7d":"cosSimilarities = cosine_similarity(imgs_features)\n\n# store the results into a pandas dataframe\n\ncos_similarities_df = pd.DataFrame(cosSimilarities, columns=files, index=files)\ncos_similarities_df.head()","61790859":"# function to retrieve the most similar products for a given one\n\ndef retrieve_most_similar_products(given_img):\n\n    print(\"-----------------------------------------------------------------------\")\n    print(\"original product:\")\n\n    original = load_img(given_img, target_size=(imgs_model_width, imgs_model_height))\n    plt.imshow(original)\n    plt.show()\n\n    print(\"-----------------------------------------------------------------------\")\n    print(\"most similar products:\")\n\n    closest_imgs = cos_similarities_df[given_img].sort_values(ascending=False)[1:nb_closest_images+1].index\n    closest_imgs_scores = cos_similarities_df[given_img].sort_values(ascending=False)[1:nb_closest_images+1]\n\n    for i in range(0,len(closest_imgs)):\n        original = load_img(closest_imgs[i], target_size=(imgs_model_width, imgs_model_height))\n        plt.imshow(original)\n        plt.show()\n        print(\"similarity score : \",closest_imgs_scores[i])","1eea622c":"retrieve_most_similar_products(files[465])","e9731a24":"retrieve_most_similar_products(files[50])","8f0179a1":"import pandas as pd\n\nstyles=pd.read_csv(\"..\/input\/fashion-product-images-dataset\/fashion-dataset\/styles.csv\", error_bad_lines=False)\n","40e48aa4":"import plotly.express as px\nfig = px.pie(styles, styles['gender'],color_discrete_sequence=px.colors.sequential.RdBu)\nfig.show()","49f2d720":"catcounts=pd.value_counts(styles['masterCategory'])","bdb1e7cf":"import plotly.graph_objects as go\n\nfig = go.Figure([go.Bar(x=catcounts.index, y=catcounts.values , text=catcounts.values,marker_color='darkblue')])\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.show()","a0a3847b":"seasons=pd.value_counts(styles['season'])","aba53a67":"import plotly.graph_objects as go\n\nfig = go.Figure(data=[go.Scatter(\n    x=seasons.index, y=seasons.values,\n    mode='markers',\n    marker=dict(\n        color=['rgb(93, 164, 214)', 'rgb(255, 144, 14)',\n               'rgb(44, 160, 101)', 'rgb(255, 65, 54)'],\n        opacity=[1, 0.8, 0.6, 0.4],\n        size=[40, 60, 80, 100])\n)]\n               )\n\nfig.show()","9c7ae7f3":"articles=pd.value_counts(styles['articleType'])\nfig = go.Figure([go.Bar(x=articles.index, y=articles.values , text=articles.values,marker_color='indianred')])\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.show()","87a70a9d":"## Further Work:\n\nTo obtain a more accurate set of predictions, there can be some improvements made to the algorithm. Content based filtering according to the type of item can be done before feature extraction to avoid getting similar items but from different categories. Also, removing and reassembling the images such that no human figures or any such miscellaneous data so as to avoid getting similarities based on such irregularity.","24e497c1":"Online shopping websites always suggest items similar to the ones you have browsed and sometimes even items that can go with it. This notebook is a basic replication of the product recommendations using cosine image similarity. This topic falls majorly in the domain of Recommendation sytems, and I have a whole notebook dedicated to the topic. [Check it out!](https:\/\/www.kaggle.com\/niharika41298\/recommendation-systems-in-a-nutshell)\n\n![Image](https:\/\/1030z2bnst92zo6j523feq9e-wpengine.netdna-ssl.com\/wp-content\/uploads\/2020\/02\/product-recommendations-hero.png)","b6414f41":"## Top Categories","294c6597":"Apparel has a large amount of products followed by 11k accessories!","d4bdc0fc":"Thence, the algorithm is providing pretty good recommendations, even with a small dataset of 5000 images.","adc060a6":"## Testing feature extraction with one image","e7746142":"# Dash User Interface\n\n* Dash is basically a platform by the renowned Plotly library,which many data scientists use as a User Interface.\n* Dash uses Flask as the base framework and creates a web application where data scientists and analysts can display visualizations, text or any such asset. Dash is completely in Python so you do not have to worry about learning JavaScript. It is the nearest equivalent of the R Shiny platform.\n* Here is the static application for product recommendations that uses the vgg16 model to get recommendations.\n\n\n(Click on the below image)\n\n[![image.png](attachment:image.png)](https:\/\/productrecom-n.herokuapp.com\/)\n\n\n","3e975cd8":"Tshirts and Watches have a large amount of products, which is understandable and Shoe laces are the least desired.","e698daa9":"## Which season has the most amount products?","21f368a8":"## Because the algorithm works for one image, it should work for a batch of images, let us try!","4da6a331":"Summer season has a lot products\ud83c\udf1e.Spring though, is at the bottom with only 2983 products.","4ae87937":"# Data analysis and visualization","46401211":"## Number of men's items vs Number of women's items","1c859569":"# Image similarity for product recommendation.","a95bd92b":"# Transfer learning model VGG16 for the above User Interface.","454285c1":"**Getting images and setting height and weight.** Note that here VGG16 Transfer learning model is being used and the architecture of the model informs that the first input layer accepts input in the shape of (none, 224,224,3). Therefore, the shape of the images is 224,224.","e17cdaa1":"First step, importing all necessary libraries.","be0afed5":"# About the VGG16 Model\n\n![image.png](attachment:image.png)\n\nVGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper \u201cVery Deep Convolutional Networks for Large-Scale Image Recognition\u201d. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes.VGG16 was trained for weeks and was using NVIDIA Titan Black GPU\u2019s.It always uses 3 x 3 filters with stride of 1 in convolution layer and uses SAME padding in pooling layers 2 x 2 with stride of 2.\n\nHere, the last softmax layer is not required as classification is not performed. Therefore, for feature extraction the outputs are from the fc2 layer."}}