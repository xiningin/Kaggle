{"cell_type":{"008d7155":"code","0a812439":"code","67ff116e":"code","e531827e":"code","7889a78c":"code","cef31c36":"code","1e88667b":"code","29806b78":"code","c056baed":"code","02fd0513":"code","dfafa13f":"code","e343e8fe":"code","4edda742":"code","d27671d8":"code","986fa795":"code","c952de76":"code","b894da0d":"code","3a1bb98a":"code","4945a483":"code","a4dbd347":"code","2d334f00":"code","5cab0f5d":"code","f0bfbdd6":"code","580e57c3":"code","51966804":"code","4336b10d":"code","c8f356d7":"code","a2fecc95":"code","911ebeb7":"code","6d47eb98":"code","874df363":"code","db9e6198":"code","0370c827":"code","5dd64310":"code","3548a98f":"code","7978be69":"code","704729dd":"code","681b16ad":"code","6c3c89eb":"code","410d73a7":"markdown","061d7fba":"markdown","d5db8498":"markdown","662f5065":"markdown","f2fb5a83":"markdown","bf35821d":"markdown","12a5daa6":"markdown","3757bfe8":"markdown","4eb64449":"markdown","3ed35d8b":"markdown","be178865":"markdown","143e7136":"markdown","4d1922f7":"markdown","6809e690":"markdown","ed707f10":"markdown","5d5e7800":"markdown","2aa22a87":"markdown","4e5f1de7":"markdown","5701bd12":"markdown","caab5c7b":"markdown","a6aa9ae5":"markdown","db11fbf8":"markdown","39132279":"markdown","a3b602be":"markdown","15c5b799":"markdown","2b10006f":"markdown","8bae0a7d":"markdown","240044e6":"markdown","252a0e5d":"markdown"},"source":{"008d7155":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings # current version of seaborn generates a bunch of warnings that we'll ignore\nwarnings.filterwarnings(\"ignore\")\n\nsns.set(style=\"white\", color_codes=True, rc={'figure.figsize':(11.7,8.27)})","0a812439":"cd \/kaggle\/input\/iris","67ff116e":"!ls","e531827e":"iris = pd.read_csv('Iris.csv')","7889a78c":"iris.dtypes","cef31c36":"iris.describe()","1e88667b":"iris.head()","29806b78":"print('Dataset has {n} instances.'.format(n = iris.shape[0]))\nprint('Dataset has {n} columns.'.format(n = iris.shape[1]))\ncols = ', '.join(iris.columns) + '.'\nprint('Dataset has the following columns:',cols)","c056baed":"iris.drop(['Id'], axis=1, inplace=True)","02fd0513":"iris['Species'].value_counts()","dfafa13f":"iris.plot(kind=\"scatter\", x=\"SepalLengthCm\", y=\"SepalWidthCm\")","e343e8fe":"iris.plot(kind=\"scatter\", x=\"PetalLengthCm\", y=\"PetalWidthCm\")","4edda742":"sns.FacetGrid(iris, hue=\"Species\", size=7).map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\").add_legend()","d27671d8":"sns.FacetGrid(iris, hue=\"Species\", size=7).map(plt.scatter, \"PetalLengthCm\", \"PetalWidthCm\").add_legend()","986fa795":"sns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=iris)","c952de76":"sns.boxplot(x=\"Species\", y=\"PetalWidthCm\", data=iris)","b894da0d":"sns.boxplot(x=\"Species\", y=\"SepalLengthCm\", data=iris)","3a1bb98a":"sns.boxplot(x=\"Species\", y=\"SepalWidthCm\", data=iris)","4945a483":"sns.FacetGrid(iris, hue=\"Species\", size=6).map(sns.kdeplot, \"PetalLengthCm\").add_legend()","a4dbd347":"sns.FacetGrid(iris, hue=\"Species\", size=6).map(sns.kdeplot, \"PetalWidthCm\").add_legend()","2d334f00":"sns.FacetGrid(iris, hue=\"Species\", size=6).map(sns.kdeplot, \"SepalLengthCm\").add_legend()","5cab0f5d":"sns.FacetGrid(iris, hue=\"Species\", size=6).map(sns.kdeplot, \"SepalWidthCm\").add_legend()","f0bfbdd6":"sns.pairplot(iris, hue=\"Species\", size=3.5)","580e57c3":"X = iris.drop(['Species'], axis=1)\nY = iris['Species']","51966804":"from sklearn.preprocessing import LabelEncoder \n  \nle = LabelEncoder() \n  \nY= le.fit_transform(Y) ","4336b10d":"print(Y.shape)\nY","c8f356d7":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)","a2fecc95":"print(X.shape)\nprint('\\nMinMax Scaled Inputs:\\n', X)","911ebeb7":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)","6d47eb98":"train_acc = []\ntest_acc = []\ni = 0","874df363":"from sklearn.linear_model import LogisticRegression\nlogR = LogisticRegression(random_state = 0).fit(X_train, y_train)\n\ntrain_acc.append(np.mean(logR.predict(X_train) == y_train)*100)\ntest_acc.append(np.mean(logR.predict(X_test) == y_test)*100)\nprint('\\nTrain Accuracy: {}\\nTest Accuracy: {}'.format(train_acc[i], test_acc[i]))\ni += 1\ny_pred = logR.predict(X_test)","db9e6198":"#F1 score\nfrom sklearn.metrics import f1_score, confusion_matrix\nprint('\\nTrain Confusion Matrix\\n', confusion_matrix(y_train, logR.predict(X_train)))\nprint('\\nTest Confusion Matrix\\n', confusion_matrix(y_test, y_pred))\nprint('\\nF1-score\\n', f1_score(y_test, y_pred, average=None))\n","0370c827":"from sklearn import tree\ndTree = tree.DecisionTreeClassifier(random_state=0).fit(X_train, y_train)\n\ntrain_acc.append(np.mean(dTree.predict(X_train) == y_train)*100)\ntest_acc.append(np.mean(dTree.predict(X_test) == y_test)*100)\nprint('\\nTrain Accuracy: {}\\nTest Accuracy: {}'.format(train_acc[i], test_acc[i]))\ni += 1\ny_pred = dTree.predict(X_test)","5dd64310":"import graphviz \ndot_data = tree.export_graphviz(dTree, out_file=None) \ngraph = graphviz.Source(dot_data) \ndot_data = tree.export_graphviz(dTree, out_file=None,feature_names=iris.columns[:-1], \n                                class_names=['Iris-setosa', 'Iris-vesicolor', 'Iris-viriginica'],\n                                filled=True, rounded=True, special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph ","3548a98f":"#F1 score\nfrom sklearn.metrics import f1_score, confusion_matrix\nprint('\\nTrain Confusion Matrix\\n', confusion_matrix(y_train, dTree.predict(X_train)))\nprint('\\nTest Confusion Matrix\\n', confusion_matrix(y_test, y_pred))\nprint('\\nF1-score\\n', f1_score(y_test, y_pred, average=None))","7978be69":"from sklearn.ensemble import RandomForestClassifier\nranF = RandomForestClassifier(max_depth=2, random_state=0).fit(X_train, y_train)\n\ntrain_acc.append(np.mean(ranF.predict(X_train) == y_train)*100)\ntest_acc.append(np.mean(ranF.predict(X_test) == y_test)*100)\nprint('\\nTrain Accuracy: {}\\nTest Accuracy: {}'.format(train_acc[i], test_acc[i]))\ni += 1\ny_pred = ranF.predict(X_test)","704729dd":"#F1 score\nfrom sklearn.metrics import f1_score, confusion_matrix\nprint('\\nTrain Confusion Matrix\\n', confusion_matrix(y_train, ranF.predict(X_train)))\nprint('\\nTest Confusion Matrix\\n', confusion_matrix(y_test, y_pred))\nprint('\\nF1-score\\n', f1_score(y_test, y_pred, average=None))\n","681b16ad":"from sklearn import svm\nsvmPre = svm.SVC().fit(X_train, y_train)\n\ntrain_acc.append(np.mean(svmPre.predict(X_train) == y_train)*100)\ntest_acc.append(np.mean(svmPre.predict(X_test) == y_test)*100)\nprint('\\nTrain Accuracy: {}\\nTest Accuracy: {}'.format(train_acc[i], test_acc[i]))\ni += 1\ny_pred = svmPre.predict(X_test)","6c3c89eb":"#F1 score\nfrom sklearn.metrics import f1_score, confusion_matrix\nprint('\\nTrain Confusion Matrix\\n', confusion_matrix(y_train, svmPre.predict(X_train)))\nprint('\\nTest Confusion Matrix\\n', confusion_matrix(y_test, y_pred))\nprint('\\nF1-score\\n', f1_score(y_test, y_pred, average=None))","410d73a7":"### Decision Tree","061d7fba":"The random forest classifier also has the same results as the decision tree. Even it is predicting one vesicolor as virginica","d5db8498":"Since the output column is Species. Lets check for the frequency of the different classes. This will help us to know whether the data is properly balanced or not.","662f5065":"Loading the dataset","f2fb5a83":"Next we have to convert our labels to numerical format i.e. we have to represent the classes as numbers, for example setosa as 0, vesicolor as 1 and virginica as 2.","bf35821d":"Here the clusters are well seperated from each other. But again there are 2 species which have kind of same petal width and length","12a5daa6":"### Support Vector Machines","3757bfe8":"### Logistic Regression","4eb64449":"Inspecting the data types of attributes","3ed35d8b":"From the above graphs it was evident that iris setosa has comparatively lesser petal length, petal width, sepal length and greater sepal width. So it should be easy to distinguish between iris setosa and oher species. But the problem arises while distinguishing iris vesicolor and iris virginica.","be178865":"Looking at the data it can be said that the ID variable is of no use for further analysis. As it is the unique integer value alotted to the respective flower species. So dropping that from our dataframe","143e7136":"But shall we rely only on the accuracy as a metric for classifiaction problem?\n\nNo, as some times the accuracy becomes misleading, for example when the target variables are not balanced.\n\nSo we should also consider the metrics like confusion matrix, f1 score, roc score. \n\nConfusion matrix informs us about: -  \n1. True postives : - These are cases in which we predicted yes (they have the disease), and they do have the disease.\n2. True negatives : - We predicted no, and they don't have the disease.\n3. False positives : - We predicted yes, but they don't actually have the disease. (Also known as a \"Type I error.\")\n4. False negatives : - We predicted no, but they actually do have the disease. (Also known as a \"Type II error.\")","4d1922f7":"The above plot is a mixture of the plots that we have done till now.","6809e690":"After encoding the target we have to scale the input features for better and fast training.\n\nThere various types of scalers available. For reference visit: https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_all_scaling.html ","ed707f10":"## Data Preprocessing","5d5e7800":"From the above plot it can be observed that there are 2 prominent clusters for the Sepal Length and Width. But to see the flower species which are related to the above data points we can do a following plot ","2aa22a87":"The logistic regression model is able to achieve a decent accuracy of 91.42% on train set and 88.89% on test set","4e5f1de7":"# Iris Plant Dataset\n\nThis is probably the most common dataset which a data science enthusiast would come across.\nThis dataset is simplest for applying Exploratory Data Analysis (EDA) and various classification models.\n\nAttribute Information:\n\n1. sepal length in cm\n2. sepal width in cm\n3. petal length in cm\n4. petal width in cm\n5. class: \n\n-- Iris Setosa\n\n-- Iris Versicolour\n\n-- Iris Virginica","5701bd12":"Since we are having the input and targets in the same variable firstly we have to seperate them.","caab5c7b":"So there was one wrong prediction where vesicolor was predicted virginica and because of this prediction our f1 scores were affected.","a6aa9ae5":"## Importing libraries required to do EDA","db11fbf8":"Again from the above plots we get to know the denisty of the attributes w.r.t. the species.\n\nFrom these plots also it is clear that petal characteristics are a good distingisher as compared to the sepal characteristics.","39132279":"## Model Selection\n\nNow we will start with model training and selection for classification task.","a3b602be":"We also have to split the data into training and testing set","15c5b799":"### Random Forest","2b10006f":"Above are the class wise f1 score. \n\nHere we can see that as per our analysis it was easy to distinguish between setosa and other species and it is reflecting here. As f1 score of setosa class is 1 meaning that we are able to classify setosa with good precision and recall i.e. every predicted setosa was a setosa.\n\nTo see that we can observe the confusion matrix","8bae0a7d":"Now we have a better f1 score for vesicolor and virginica.\n\nBut again we have to look at the confusion matrix so we can come to know what predictions are going","240044e6":"Again the above boxplots give us the same results as observed from the scatter plots. \n\nBut here we can also see the range that different attributes cover like Sepal Width has the minimum of around 2.3 cm and maximum value of around 4.4 cm for the specie iris setosa.\n\nOne thing to notice here is that the Petal width and Petal length are good distinguishers for the species vesicolor and virginica. ","252a0e5d":"So every class has 50 instances. That means data is well balanced. \n\nNow lets do some data visualization to understand the relation between the variables."}}