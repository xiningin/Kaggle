{"cell_type":{"b81587d9":"code","b12ce50e":"code","c6efb104":"code","cb80afda":"code","127e7b29":"code","84fb10af":"code","98f21ca4":"code","d7d90b85":"code","6486ca6a":"code","cd875fb4":"code","786b275a":"code","95d2281e":"code","a1e4b9b1":"code","169701a8":"code","4f7df4ae":"code","19f48a8d":"code","302e9d93":"code","82385c9b":"code","8c4cfafa":"code","b84b2cf0":"code","922f2c86":"code","70a42696":"code","3aa2388e":"code","301a917c":"code","b1f1ef55":"code","e6f7ba00":"code","697787fa":"code","33f4df19":"code","b527b9e5":"code","9883111c":"markdown","01bc6423":"markdown","a693fee7":"markdown","46ce7aca":"markdown","437d460c":"markdown","b3902260":"markdown","93244aba":"markdown","9228d8a0":"markdown","9c589ebf":"markdown","be3a4bd9":"markdown"},"source":{"b81587d9":"import numpy as np\nimport pandas as pd\n\n# displays 300 columns of pandas dataframe\npd.options.display.max_columns = 300\n\n# model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\n# metrics\nfrom sklearn.metrics import mean_absolute_error\n\n# models\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\ntrain_name = '\/kaggle\/input\/home-data-for-ml-course\/train.csv'\ntest_name = '\/kaggle\/input\/home-data-for-ml-course\/test.csv'","b12ce50e":"def read_csvs(csv):\n    return pd.read_csv(csv)\ntrain = read_csvs(train_name)\ntest = read_csvs(test_name)","c6efb104":"train.head()","cb80afda":"train.describe()","127e7b29":"train.groupby(['OverallQual'])['SalePrice'].mean()","84fb10af":"# Example plot\ntrain.plot(x='OverallQual', y='SalePrice', style='o')","98f21ca4":"def find_nan(df):\n    \"\"\" \n    Finds the columns that have NaN entries\n    \"\"\"\n    null_series = df.isnull().sum()\n    return [col_name for col_name in null_series.index if null_series[col_name] != 0]","d7d90b85":"# Number of NaN entries in each column that has NaN entries\nfor col in find_nan(train):\n    print(col, str(train[col].isnull().sum()))","6486ca6a":"def fill_nan(df):\n    \"\"\" \n    Fills columns containing NaN\n    \"\"\"\n    # Series of columns with NaN entries and each column's datatype\n    dtypes = df[find_nan(df)].dtypes\n    \n    # Fills NaN entries in Object columns with 'None'\n    none_cols = [col_name for col_name in dtypes.index if dtypes[col_name] == 'O']\n    for col in none_cols:\n        df[col] = df[col].fillna('None')\n    \n    # Fills most numerical columns with 0\n    zero_cols = ['LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', \n                 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', \n                 'GarageCars', 'GarageArea']\n    for col in zero_cols:\n        df[col] = df[col].fillna(0)\n        \n    # Fills year garage built with year house built, the closest approximation\n    df.GarageYrBlt = df.GarageYrBlt.fillna(df.YearBuilt)\n    \n    return df\ntrain = fill_nan(train)\ntest = fill_nan(test)","cd875fb4":"find_nan(train), find_nan(test)","786b275a":"train.FireplaceQu","95d2281e":"train.MasVnrArea","a1e4b9b1":"def fill_ord_cols(df):\n    \"\"\" \n    This method sets ordinal columns to numerical rankings\n\n    For example, BsmtCond (Basement Condition) looks like this at the start:\n    Ex - Excellent\n    Gd - Good\n    TA - Typical\/Average\n    Fa - Fair\n    Po - Poor\n    None - No Basement\n\n    Let's instead change to be:\n    5 - Excellent\n    4 - Good\n    3 - Typical\/Average\n    2 - Fair\n    1 - Poor\n    0 - No Basement\n    \"\"\"\n    # Creates dictionaries for conversion and lists of certain columns to convert\n    grading_dict = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0}\n    grading_list = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', \n                    'HeatingQC', 'KitchenQual', 'GarageQual', 'GarageCond', \n                    'PoolQC', 'FireplaceQu']\n    bsmtFinType_dict = {'GLQ': 2, 'ALQ': 2, 'Unf': 2, 'LwQ': 1,\n                        'BLQ': 1, 'Rec': 1, 'None': 0}\n    BsmtFinType_list = ['BsmtFinType1', 'BsmtFinType2']\n    \n    utilities_dict = {'AllPub': 4, 'NoSewr': 3, 'NoSeWa': 2, 'ELO': 1}\n    bsmtExposure_dict = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'None': 0}\n    centralAir_dict = {'Y': 1, 'N': 0}\n    street_dict = {'Pave': 1, 'Grvl': 0}\n    fence_dict = {'None': 5, 'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1}\n    pavedDrive_dict = {'Y': 2, 'P': 1, 'N': 0}\n    garage_dict = {'Fin':3, 'RFn': 2, 'Unf': 1, 'None': 0}\n    \n    # Coverts the columns\n    for col in grading_list:\n        df[col] = df[col].map(grading_dict)\n    for col in BsmtFinType_list:\n        df[col] = df[col].map(bsmtFinType_dict)\n        \n    df.BsmtExposure = df.BsmtExposure.map(bsmtExposure_dict)\n    df.CentralAir = df.CentralAir.map(centralAir_dict)\n    df.PavedDrive = df.PavedDrive.map(pavedDrive_dict)\n    df.Street = df.Street.map(street_dict)\n    df.GarageFinish = df.GarageFinish.map(garage_dict)\n    df.Fence = df.Fence.map(fence_dict)\n    df.Utilities = df.Utilities.map(utilities_dict)\n    return df\n\ntrain = fill_ord_cols(train)\ntest = fill_ord_cols(test)","169701a8":"train.BsmtExposure","4f7df4ae":"def make_dummies(df):\n    \"\"\"\n    Returns a dataframe with remaining Object columns into dummy columns\n    \"\"\"\n    return pd.get_dummies(df)\ntrain = make_dummies(train)\ntest = make_dummies(test)","19f48a8d":"train.head()","302e9d93":"train.shape","82385c9b":"def get_best_cols(num_cols):\n    \"\"\"\n    Returns a list of the best columns ranking from highest to lowest\n    in terms of their correlation to SalePrice using the train dataframe\n    \"\"\"\n    correlations = train[train.columns[1:]].corr()['SalePrice'].abs()\n    correlations = correlations.nlargest(num_cols)\n    return list(correlations.index)\nbest_cols = get_best_cols(125)[1:]","8c4cfafa":"# Top 10 columns most correlated to SalePrice and the correlations\nfor col in best_cols[:10]:\n    print (col, train[col].corr(train.SalePrice))","b84b2cf0":"X = train[best_cols]\ny = train.SalePrice\ntest = test[best_cols]","922f2c86":"X","70a42696":"train_features, test_features, train_labels, test_labels = train_test_split(X, y)","3aa2388e":"#Gradient Boost trial\ngrad_params = {\n    'n_estimators': [5000],\n    'learning_rate': [0.035, 0.05, 0.065],\n    'loss': ['ls'],\n    'random_state': [0]\n}\n\ngrad_model = GridSearchCV(\n    GradientBoostingRegressor(),\n    scoring='neg_mean_absolute_error',\n    param_grid=grad_params,\n    cv=5,\n    n_jobs=-1,\n    verbose=1,\n).fit(train_features, train_labels)\n\nprint(grad_model.best_estimator_)\n\nmean_absolute_error(test_labels, grad_model.predict(test_features))","301a917c":"#XGBoost Regressor\nxgb_model = XGBRegressor(\n    n_estimators=10000,\n    max_depth=5,\n    min_child_weight=0,\n    learning_rate=0.0031,\n    subsample=0.2,\n    random_state=0).fit(train_features, train_labels)\n\nmean_absolute_error(test_labels, xgb_model.predict(test_features))","b1f1ef55":"model = XGBRegressor(\n    n_estimators=10000,\n    max_depth=5,\n    min_child_weight=0,\n    learning_rate=0.0031,\n    subsample=0.2,\n    random_state=0\n).fit(X, y)","e6f7ba00":"predictions = model.predict(test)","697787fa":"id_series = read_csvs(test_name).Id\nid_series","33f4df19":"df = pd.DataFrame({'Id': id_series, 'SalePrice': predictions})","b527b9e5":"df.to_csv('submission.csv', index=False) ","9883111c":"# Finishing up","01bc6423":"# Data exploration","a693fee7":"# Top 2% Approach\nIn this notebook, I show how I got in the top 2% for the Housing Prices Competition using basic data pre-processing and the XGBRegressor model.","46ce7aca":"Let's make sure all nan entries have been filled","437d460c":"# Converts categorical variables into dummy variables","b3902260":"For an example, let's look at how the SalePrice correlates to the OverallQual - a rating of a house's material and finish","93244aba":"# Fill NaN entries\nAppropriately filling NaN entries is an important part of data pre-processing. For this dataframe, most NaN entries in numerical columns correspond to 0.","9228d8a0":"# Modelling","9c589ebf":"# Finds the columns most correlated to SalePrice\nWe'll only keep roughly half of the columns that are strongly or at least somewhat correlated to SalePrice and discard the remaining noise.","be3a4bd9":"# Change ordinal features to numerical representations\nSeveral columns are ordinal in nature. While it is possible to one-hot encode such columns, it is better practice to quantify the data. To figure out which columns are ordinal, I looked through each column with the Object datatype in the provided data description txt file. Roughly half of the columns share a similar rating system, so I made one list of those columns and one dictionary to later map appropriate entries. Then for most of the other columns it's easy enough to create custom dictionaries for mapping."}}