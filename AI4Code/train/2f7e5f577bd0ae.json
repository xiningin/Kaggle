{"cell_type":{"5a0a791e":"code","c9b4c1f0":"code","06adfe7f":"code","9024e422":"code","5adecf24":"code","f930b660":"code","6c4cba7d":"code","865ad8df":"code","ee8a9aa4":"code","d0e0e39f":"code","706d9a29":"code","3c9ce747":"code","796b57ac":"code","1e242a26":"code","5776e3a8":"code","dfa939cf":"code","8afbdc66":"code","52db762f":"code","26cacf36":"code","d0e701c5":"code","1134b912":"code","666fdfba":"code","5831b9a8":"code","ac952cd0":"code","2689ba24":"code","5de99b74":"code","971feb95":"code","648e8510":"code","8e72a141":"code","e4b50e0e":"code","11885aca":"code","54b7da32":"code","e7ae509b":"code","ea06f867":"code","4ed0cded":"code","141d0ded":"code","5f3e41d8":"code","1ce9a235":"code","04b85e35":"code","4b342921":"code","ac811e1d":"markdown","d5cdbb90":"markdown","f9432c0d":"markdown","538ecf83":"markdown","dafe67fe":"markdown","623e3fa5":"markdown","f31722b8":"markdown","184576d8":"markdown","a7a8b46a":"markdown","191988a2":"markdown","cc62aac3":"markdown","ee5c28ba":"markdown","b0580888":"markdown","3124e459":"markdown","84dd84f6":"markdown","861914a1":"markdown"},"source":{"5a0a791e":"import os\nimport gc\nimport sys\nimport time\nimport json\nimport glob\nimport random\nfrom pathlib import Path\nimport pandas as pd\n\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom imgaug import augmenters as iaa\n\nimport itertools\nfrom tqdm import tqdm\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")","c9b4c1f0":"training_folder = \"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train\/\"\ndf = pd.read_csv(\"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train.csv\")\ndf = df.query(\"class_id<14\")\ndf = df.query(\"rad_id=='R9'\")","06adfe7f":"df[\"several_issues\"] = df.duplicated(subset=['image_id'])\ndf[\"box_size\"] = [(row.y_max-row.y_min)*(row.x_max-row.x_min) for idx, row in df.iterrows()]","9024e422":"df.class_name.unique()","5adecf24":"df.groupby(\"class_id\")[\"box_size\"].mean()","f930b660":"df.groupby(\"class_id\")[\"box_size\"].std()","6c4cba7d":"df.groupby(\"class_id\").image_id.count()","865ad8df":"selected_classes = [0,3,5,7,10]\ncategory_list = [\"Aortic enlargement\", \"Cardiomegaly\", \"ILD\", \"Lung Opacity\", \"Pleural effusion\"]\nfiltered_df = df.query(\"class_id in @selected_classes\")","ee8a9aa4":"selected_classes_dict = {\"0\":0,\"3\":1,\"5\":2,\"7\":3,\"10\":4}\nfiltered_df[\"reclass_id\"] = [selected_classes_dict[str(row.class_id)] for idx, row in filtered_df.iterrows()]","d0e0e39f":"filtered_df","706d9a29":"def get_mask(img_dimensions, x_min, y_min, x_max, y_max):\n    img_height, img_width = img_dimensions\n    img_mask = np.full((img_height,img_width),0)\n    img_mask[y_min:y_max,x_min:x_max] = 255\n    \n    return img_mask.astype(np.float32)\n\n\ndef rle_encoding(x):\n    dots = np.where(x.T.flatten() == 255)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return ' '.join([str(x) for x in run_lengths])","3c9ce747":"def read_xray(path, voi_lut = True, fix_monochrome = True):\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data","796b57ac":"resized_folder = \"..\/working\/resized_train\/\"\nos.mkdir(resized_folder)","1e242a26":"filtered_df.groupby(\"class_id\").image_id.count()","5776e3a8":"balanced_filtered_df = pd.DataFrame()\nsamples_per_class = 500\nfor class_name in filtered_df.class_name.unique():\n    balanced_filtered_df = balanced_filtered_df.append(filtered_df.query(\"class_name==@class_name\")[:samples_per_class], \n                                                       ignore_index=True)","dfa939cf":"balanced_filtered_df","8afbdc66":"diagnostic_per_image = []\n\nimage_size=512\nwith tqdm(total=len(balanced_filtered_df)) as pbar:\n    for idx,row in balanced_filtered_df.iterrows():\n        image_id = row.image_id\n        image_df = balanced_filtered_df.query(\"image_id==@image_id\")\n        class_list = []\n        RLE_list = []\n        \n        for diagnostic_id, diagnostic in image_df.iterrows():\n            class_list.append(diagnostic.reclass_id)\n\n            dicom_image = read_xray(training_folder+image_id+\".dicom\")\n            image_dimensions = dicom_image.shape\n            \n            resized_img = cv2.resize(dicom_image, (image_size,image_size), interpolation = cv2.INTER_AREA)\n            cv2.imwrite(resized_folder+image_id+\".jpg\", resized_img) \n            \n            mask = get_mask(image_dimensions, int(diagnostic.x_min), int(diagnostic.y_min), int(diagnostic.x_max), int(diagnostic.y_max))\n            resized_mask = cv2.resize(mask, (image_size,image_size))\n            RLE_list.append(rle_encoding(resized_mask))\n        diagnostic_per_image.append({\"image_id\":image_id,\n                                     \"CategoryId\":class_list,\n                                     \"EncodedPixels\":RLE_list})\n        pbar.update(1)","52db762f":"samples_df = pd.DataFrame(diagnostic_per_image)\nsamples_df[\"Height\"] = image_size\nsamples_df[\"Width\"] = image_size","26cacf36":"samples_df","d0e701c5":"!cp -r ..\/input\/maskrcnn-tf2-keras ..\/working\/maskrcnn-tf2-keras","1134b912":"DATA_DIR = Path('..\/working\/')\nROOT_DIR = \"..\/..\/working\"\n\nNUM_CATS = len(selected_classes)\nIMAGE_SIZE = 512\nos.chdir('..\/working\/maskrcnn-tf2-keras')\nsys.path.append(ROOT_DIR+'\/maskrcnn-tf2-keras')\nfrom mrcnn.config import Config\n\nfrom mrcnn import utils\nimport mrcnn.model as modellib\nfrom mrcnn import visualize\nfrom mrcnn.model import log","666fdfba":"COCO_WEIGHTS_PATH = '..\/..\/input\/coco-weights\/mask_rcnn_coco.h5'\n\nclass DiagnosticConfig(Config):\n    NAME = \"Diagnostic\"\n    NUM_CLASSES = NUM_CATS + 1 # +1 for the background class\n    \n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 2 #That is the maximum with the memory available on kernels\n    \n    BACKBONE = 'resnet50'\n    \n    IMAGE_MIN_DIM = IMAGE_SIZE\n    IMAGE_MAX_DIM = IMAGE_SIZE    \n    IMAGE_RESIZE_MODE = 'none'\n\n    POST_NMS_ROIS_TRAINING = 250\n    POST_NMS_ROIS_INFERENCE = 150\n    MAX_GROUNDTRUTH_INSTANCES = 5\n    BACKBONE_STRIDES = [4, 8, 16, 32, 64]\n    BACKBONESHAPE = (8, 16, 24, 32, 48)\n    RPN_ANCHOR_SCALES = (8,16,24,32,48)\n    ROI_POSITIVE_RATIO = 0.33\n    DETECTION_MAX_INSTANCES = 300\n    DETECTION_MIN_CONFIDENCE = 0.7    \n    # STEPS_PER_EPOCH should be the number of instances \n    # divided by (GPU_COUNT*IMAGES_PER_GPU), and so should VALIDATION_STEPS;\n    # however, due to the time limit, I set them so that this kernel can be run in 9 hours\n    STEPS_PER_EPOCH = int(len(samples_df)*0.9\/IMAGES_PER_GPU)\n    VALIDATION_STEPS = len(samples_df)-int(len(samples_df)*0.9\/IMAGES_PER_GPU)\n    \nconfig = DiagnosticConfig()\nconfig.display()","5831b9a8":"class DiagnosticDataset(utils.Dataset):\n    def __init__(self, df):\n        super().__init__(self)\n        \n        # Add classes\n        for i, name in enumerate(category_list):\n            self.add_class(\"diagnostic\", i+1, name)\n        \n        # Add images \n        for i, row in df.iterrows():\n            self.add_image(\"diagnostic\", \n                           image_id=row.name,\n                           path=\"..\/\"+resized_folder+str(row.image_id)+\".jpg\", \n                           labels=row['CategoryId'],\n                           annotations=row['EncodedPixels'], \n                           height=row['Height'], width=row['Width'])\n\n    def image_reference(self, image_id):\n        info = self.image_info[image_id]\n        return info['path'], [category_list[int(x)] for x in info['labels']]\n    \n    def load_image(self, image_id):\n        return cv2.imread(self.image_info[image_id]['path'])\n\n    def load_mask(self, image_id):\n        info = self.image_info[image_id]\n                \n        mask = np.zeros((IMAGE_SIZE, IMAGE_SIZE, len(info['annotations'])), dtype=np.uint8)\n        labels = []\n        \n        for m, (annotation, label) in enumerate(zip(info['annotations'], info['labels'])):\n            sub_mask = np.full(info['height']*info['width'], 0, dtype=np.uint8)\n            annotation = [int(x) for x in annotation.split(' ')]\n            \n            for i, start_pixel in enumerate(annotation[::2]):\n                sub_mask[start_pixel: start_pixel+annotation[2*i+1]] = 1\n\n            sub_mask = sub_mask.reshape((info['height'], info['width']), order='F')\n            sub_mask = cv2.resize(sub_mask, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_NEAREST)\n            \n            mask[:, :, m] = sub_mask\n            labels.append(int(label)+1)\n        return mask, np.array(labels)","ac952cd0":"training_percentage = 0.9\n\ntraining_set_size = int(training_percentage*len(samples_df))\nvalidation_set_size = int((1-training_percentage)*len(samples_df))\n\ntrain_dataset = DiagnosticDataset(samples_df[:training_set_size])\ntrain_dataset.prepare()\n\nvalid_dataset = DiagnosticDataset(samples_df[training_set_size:training_set_size+validation_set_size])\nvalid_dataset.prepare()\n\nfor i in range(10):\n    image_id = random.choice(train_dataset.image_ids)\n    image = train_dataset.load_image(image_id)\n    mask, class_ids = train_dataset.load_mask(image_id)\n    \n    visualize.display_top_masks(image, mask, class_ids, train_dataset.class_names, limit=5)","2689ba24":"LR = 1e-4\nEPOCHS = [1,18]\n\nmodel = modellib.MaskRCNN(mode='training', config=config, model_dir=\"\")\nmodel.load_weights(COCO_WEIGHTS_PATH, by_name=True, exclude=['mrcnn_class_logits', 'mrcnn_bbox_fc', 'mrcnn_bbox', 'mrcnn_mask'])","5de99b74":"%%time\nmodel.train(train_dataset, valid_dataset,\n            learning_rate=LR,\n            epochs=EPOCHS[0],\n            layers='heads')\n\nhistory = model.keras_model.history.history","971feb95":"%%time\nmodel.train(train_dataset, valid_dataset,\n            learning_rate=LR\/10,\n            epochs=EPOCHS[1],\n            layers='all')\n\nnew_history = model.keras_model.history.history\nfor k in new_history: history[k] = history[k] + new_history[k]","648e8510":"epochs = range(EPOCHS[-1])\n\nplt.figure(figsize=(18, 6))\n\nplt.subplot(131)\nplt.plot(epochs, history['loss'], label=\"train loss\")\nplt.plot(epochs, history['val_loss'], label=\"valid loss\")\nplt.legend()\nplt.subplot(132)\nplt.plot(epochs, history['mrcnn_class_loss'], label=\"train class loss\")\nplt.plot(epochs, history['val_mrcnn_class_loss'], label=\"valid class loss\")\nplt.legend()\nplt.subplot(133)\nplt.plot(epochs, history['mrcnn_mask_loss'], label=\"train mask loss\")\nplt.plot(epochs, history['val_mrcnn_mask_loss'], label=\"valid mask loss\")\nplt.legend()\n\nplt.show()","8e72a141":"best_epoch = np.argmin(history[\"val_loss\"][1:]) + 1\nprint(\"Best epoch: \", best_epoch)\nprint(\"Valid loss: \", history[\"val_loss\"][1:][best_epoch-1])","e4b50e0e":"resized_test_folder = \"..\/..\/working\/resized_test\/\"\nos.mkdir(resized_test_folder)","11885aca":"class InferenceConfig(DiagnosticConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n    IMAGE_MIN_DIM = IMAGE_SIZE\n    IMAGE_MAX_DIM = IMAGE_SIZE    \n    IMAGE_RESIZE_MODE = 'none'\n    DETECTION_MIN_CONFIDENCE = 0.8\n    DETECTION_NMS_THRESHOLD = 0.5\n\ninference_config = InferenceConfig()\n\nmodel = modellib.MaskRCNN(mode='inference', \n                          config=inference_config,\n                          model_dir=\"\")","54b7da32":"glob_list = glob.glob(f'diagnostic*\/mask_rcnn_diagnostic_{best_epoch:04d}.h5')\nmodel_path = glob_list[0] if glob_list else ''\nmodel.load_weights(model_path, by_name=True)","e7ae509b":"from skimage.measure import find_contours\nfrom matplotlib.patches import Polygon\n\n\n# Fix overlapping masks\ndef refine_masks(masks, rois):\n    areas = np.sum(masks.reshape(-1, masks.shape[-1]), axis=0)\n    mask_index = np.argsort(areas)\n    union_mask = np.zeros(masks.shape[:-1], dtype=bool)\n    for m in mask_index:\n        masks[:, :, m] = np.logical_and(masks[:, :, m], np.logical_not(union_mask))\n        union_mask = np.logical_or(masks[:, :, m], union_mask)\n    for m in range(masks.shape[-1]):\n        mask_pos = np.where(masks[:, :, m]==True)\n        if np.any(mask_pos):\n            y1, x1 = np.min(mask_pos, axis=1)\n            y2, x2 = np.max(mask_pos, axis=1)\n            rois[m, :] = [y1, x1, y2, x2]\n    return masks, rois\n\ndef decode_rle(rle, height, width):\n    s = rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(height*width, dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape((height, width)).T\n\ndef annotations_to_mask(annotations, height, width):\n    if isinstance(annotations, list):\n        # The annotation consists in a list of RLE codes\n        mask = np.zeros((height, width, len(annotations)))\n        for i, rle_code in enumerate(annotations):\n            mask[:, :, i] = decode_rle(rle_code, height, width)\n    else:\n        error_message = \"{} is expected to be a list or str but received {}\".format(annotation, type(annotation))\n        raise TypeError(error_message)\n    return mask\n\ndef find_anomalies(dicom_image, display=False):\n\n    image_dimensions = dicom_image.shape\n\n    resized_img = cv2.resize(dicom_image, (image_size,image_size), interpolation = cv2.INTER_AREA)\n    saved_filename = resized_test_folder+\"temp_image.jpg\"\n    cv2.imwrite(saved_filename, resized_img) \n    img = cv2.imread(saved_filename)\n\n    result = model.detect([img])\n    r = result[0]\n    \n    if r['masks'].size > 0:\n        masks = np.zeros((img.shape[0], img.shape[1], r['masks'].shape[-1]), dtype=np.uint8)\n        for m in range(r['masks'].shape[-1]):\n            masks[:, :, m] = cv2.resize(r['masks'][:, :, m].astype('uint8'), \n                                        (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n        \n        y_scale = image_dimensions[0]\/IMAGE_SIZE\n        x_scale = image_dimensions[1]\/IMAGE_SIZE\n        rois = (r['rois'] * [y_scale, x_scale, y_scale, x_scale]).astype(int)\n        \n        masks, rois = refine_masks(masks, rois)\n    else:\n        masks, rois = r['masks'], r['rois']\n        \n    if display:\n        visualize.display_instances(img, rois, masks, r['class_ids'], \n                                    ['bg']+category_list, r['scores'],\n                                    title=\"prediction\", figsize=(12, 12))\n    return rois, r['class_ids'], r['scores']","ea06f867":"test_folder = \"..\/..\/input\/vinbigdata-chest-xray-abnormalities-detection\/test\/\"\ntest_file_list = os.listdir(test_folder)[:5]\n\nfor test_file in test_file_list:\n    dicom_image = read_xray(test_folder+test_file)\n    find_anomalies(dicom_image, display=True)","4ed0cded":"def keep_best_cardiomegaly_box(bbox_list, class_list, confidence_list):\n    '''\n    go through the boxes and keep only one box for \n    cardiomegaly with the highest confidence score\n    '''\n    best_cardiomegaly_score = -1\n    best_cardiomegaly_bbox = []\n    clean_bbox_list, clean_class_list, clean_confidence_list = [],[],[]\n    \n    for bbox, class_id, confidence in zip(bbox_list, class_list, confidence_list):\n        #While the class number if 3 in the dataset, it is 2 in the maskrcnn training process\n        # as I have excluded some classes\n        if class_id==2:\n            if confidence>best_cardiomegaly_score:\n                best_cardiomegaly_score = confidence\n                best_cardiomegaly_bbox = bbox\n        else:\n            clean_bbox_list.append(bbox)\n            clean_class_list.append(class_id)\n            clean_confidence_list.append(confidence)\n            \n    if best_cardiomegaly_score>0:\n        clean_bbox_list.append(best_cardiomegaly_bbox)\n        clean_class_list.append(2)\n        clean_confidence_list.append(best_cardiomegaly_score)\n        \n    return clean_bbox_list, clean_class_list, clean_confidence_list","141d0ded":"results = []\ntest_file_list = os.listdir(test_folder)\nwith tqdm(total=len(test_file_list)) as pbar:\n    for image_filename in test_file_list:\n        dicom_image = read_xray(test_folder+image_filename)\n        image_dimensions = dicom_image.shape\n        bbox_list, class_list, confidence_list = find_anomalies(dicom_image, display=False)\n        prediction_string = \"\"\n        \n        if len(bbox_list)>0:\n                    \n            bbox_list, class_list, confidence_list = keep_best_cardiomegaly_box(bbox_list, class_list, confidence_list)\n            \n            for bbox, class_id, confidence in zip(bbox_list, class_list, confidence_list):\n                class_id = next(key for key, value in selected_classes_dict.items() if value == int(class_id)-1)\n                confidence_score = str(round(confidence,3))\n\n                #HACK: I had to rescale the bounding box here. For some reason,\n                #It did not do it in the prediction function.\n                y_scale = image_dimensions[0]\/image_size\n                x_scale = image_dimensions[1]\/image_size\n                rescaled_bbox = (bbox * [y_scale, x_scale, y_scale, x_scale]).astype(int)\n\n                #organise the bbox into xmin, ymin, xmax, ymax\n                ymin = image_dimensions[0]-rescaled_bbox[2]\n                ymax = image_dimensions[0]-rescaled_bbox[0]\n                xmin = rescaled_bbox[1]\n                xmax = rescaled_bbox[3]\n\n                prediction_string += \"{} {} {} {} {} {} \".format(class_id, confidence_score, xmin, ymin, xmax, ymax)\n            results.append({\"image_id\":image_filename.replace(\".dicom\",\"\"), \"PredictionString\":prediction_string.strip()})\n        else:\n            results.append({\"image_id\":image_filename.replace(\".dicom\",\"\"), \"PredictionString\":\"14 1.0 0 0 1 1\"})\n        pbar.update(1)","5f3e41d8":"submission_df = pd.DataFrame(results)","1ce9a235":"submission_df","04b85e35":"submission_df.to_csv('..\/submission.csv', index=False)","4b342921":"#clear all the images from the working directory\n!rm -rf ..\/..\/working\/resized_train\/\n!rm -rf ..\/..\/working\/resized_test\/","ac811e1d":"First, we run a test on several images and will display the results.","d5cdbb90":"Below, we run the prediction on the entire test set and format the results into a dataframe that will then be saved for submission. As it appeared that the model mistakenly gave several bounding boxes for Cardiomegaly, I am adding a function to keep only the best bounding box.","f9432c0d":"# Mask-RCNN for Chest X-ray Diagnostic - Starter","538ecf83":"The goal of this notebook is to help everyone getting started with the VinBigData chest x-ray abnormalities detection competition. This is essentially an object detection problem and there is several tricky bits to it:\n* Groundtruth is provided by different experts and diagnostics can vary from one expert to another.\n* Images are massive\n* There is a significant amount of metadata\n\nTo start in a simple way, I will resize all the images and only selected groundtruth from one expert and from the diseases with the largest bounding boxes. This data will then be fed to a MaskRCNN which, hopefully, will show some preliminary results.","dafe67fe":"### Thanks for reading this notebook! If you found this notebook helpful, please give it an upvote. It is always greatly appreciated","623e3fa5":"# Predict on test images","f31722b8":"Please note that there seems to be a bug which resets the loss displayed after each of the training blocks below. It did not happen in the original Matterport repo.","184576d8":"The following function is taken from raddar's [notebook](https:\/\/www.kaggle.com\/raddar\/convert-dicom-to-np-array-the-correct-way) and helps to load the images in the cleanest way possible.","a7a8b46a":"After checking the box sizes per abnormality, their standard deviation, and the number of examples, I decided to pick the 5 abnormalities below. I am carefully picking abnormalities with bounding boxes large enough as I will be significantly downsizing the images.","191988a2":"I am not including the loss from the training of the heads when picking the best loss because of the bug mentioned earlier.","cc62aac3":"The 2 functions below allows to go from bounding boxes to the right format for a MaskRCNN.","ee5c28ba":"# Data preparation","b0580888":"# Train a Mask-RCNN model","3124e459":"As I am not going to use the full dataset in this notebook due to the processing time allowed on Kaggle Notebooks, I am sampling an even number of images per class (depending on the number of images available) to ensure I don't end up with a highly imbalanced training set.","84dd84f6":"I struggled a bit to make the Matterport MaskRCNN repo work with Tensorflow 2.3. In the end, I used a [PR created by tomgross](https:\/\/github.com\/matterport\/Mask_RCNN\/pull\/1896\/commits\/a3be0c2c8654628f10736c4dd88060440fab3968) as a base and only had a couple of fixes left to get everything running!","861914a1":"# Prediction and generating the submission file"}}