{"cell_type":{"912fa10d":"code","b80210e4":"code","8d28d611":"code","537cd0bc":"code","f92ae3e4":"code","65ed864e":"code","ae63e893":"code","35c00077":"code","06460267":"code","3177ebc2":"code","e468cc99":"code","d9094cc0":"code","11042abf":"code","efe3986c":"code","c76495e8":"code","a3bb3950":"code","c641f35d":"code","1e706b9e":"code","340270c0":"code","6a6f74ed":"code","aea737c6":"code","e7cb6c77":"code","6fc60d69":"code","958b0601":"code","c29d3753":"code","e942019f":"code","1f02f250":"code","c7489591":"code","a182e349":"code","b3648a50":"code","d99d0f64":"markdown"},"source":{"912fa10d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b80210e4":"import gc\nimport re\nimport os\nimport sys\nimport time\nimport pickle\nimport random\nimport unidecode\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom scipy.stats import spearmanr\nfrom gensim.models import Word2Vec\nfrom flashtext import KeywordProcessor\nfrom keras.preprocessing import text, sequence\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.model_selection import StratifiedKFold, KFold\n","8d28d611":"train = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/test.csv')","537cd0bc":"sub = pd.read_csv('\/kaggle\/input\/google-quest-challenge\/sample_submission.csv')","f92ae3e4":"MODEL = pickle.load(open('\/kaggle\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl', 'rb'))","65ed864e":"PUNCTS = {\n            '\u300b', '\u301e', '\u00a2', '\u2039', '\u2566', '\u2551', '\u266a', '\u00d8', '\u2569', '\\\\', '\u2605', '\uff0b', '\u00ef', '<', '?', '\uff05', '+', '\u201e', '\u03b1', '*', '\u3030', '\uff5f', '\u00b9', '\u25cf', '\u3017', ']', '\u25be', '\u25a0', '\u3019', '\u2193', '\u00b4', '\u3010', '\u1d35',\n            '\"', '\uff09', '\uff40', '\u2502', '\u00a4', '\u00b2', '\u2021', '\u00bf', '\u2013', '\u300d', '\u2554', '\u303e', '%', '\u00be', '\u2190', '\u3014', '\uff3f', '\u2019', '-', ':', '\u2027', '\uff5b', '\u03b2', '\uff08', '\u2500', '\u00e0', '\u00e2', '\uff64', '\u2022', '\uff1b', '\u2606', '\uff0f', '\u03c0',\n            '\u00e9', '\u2557', '\uff3e', '\u25aa', ',', '\u25ba', '\/', '\u301a', '\u00b6', '\u2666', '\u2122', '}', '\u2033', '\uff02', '\u300e', '\u25ac', '\u00b1', '\u00ab', '\u201c', '\u00f7', '\u00d7', '^', '!', '\u2563', '\u25b2', '\u30fb', '\u2591', '\u2032', '\u301d', '\u201b', '\u221a', ';', '\u3011', '\u25bc',\n            '.', '~', '`', '\u3002', '\u0259', '\uff3d', '\uff0c', '{', '\uff5e', '\uff01', '\u2020', '\u2018', '\ufe4f', '\u2550', '\uff63', '\u3015', '\u301c', '\uff3c', '\u2592', '\uff04', '\u2665', '\u301b', '\u2264', '\u221e', '_', '[', '\uff06', '\u2192', '\u00bb', '\uff0d', '\uff1d', '\u00a7', '\u22c5', \n            '\u2593', '&', '\u00c2', '\uff1e', '\u3003', '|', '\u00a6', '\u2014', '\u255a', '\u3016', '\u2015', '\u00b8', '\u00b3', '\u00ae', '\uff60', '\u00a8', '\u201f', '\uff0a', '\u00a3', '#', '\u00c3', \"'\", '\u2580', '\u00b7', '\uff1f', '\u3001', '\u2588', '\u201d', '\uff03', '\u2295', '=', '\u301f', '\u00bd', '\u300f',\n            '\uff3b', '$', ')', '\u03b8', '@', '\u203a', '\uff20', '\uff5d', '\u00ac', '\u2026', '\u00bc', '\uff1a', '\u00a5', '\u2764', '\u20ac', '\u2212', '\uff1c', '(', '\u3018', '\u2584', '\uff07', '>', '\u20a4', '\u20b9', '\u2205', '\u00e8', '\u303f', '\u300c', '\u00a9', '\uff62', '\u2219', '\u00b0', '\uff5c', '\u00a1', \n            '\u2191', '\u00ba', '\u00af', '\u266b', '#'\n          }\n\n\nmispell_dict = {\"aren't\" : \"are not\", \"can't\" : \"cannot\", \"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\", \"didn't\" : \"did not\", \"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\", \"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\",\n\"haven't\" : \"have not\", \"havent\" : \"have not\", \"he'd\" : \"he would\", \"he'll\" : \"he will\", \"he's\" : \"he is\", \"i'd\" : \"I would\",\n\"i'd\" : \"I had\", \"i'll\" : \"I will\", \"i'm\" : \"I am\", \"isn't\" : \"is not\", \"it's\" : \"it is\",\n\"it'll\":\"it will\", \"i've\" : \"I have\", \"let's\" : \"let us\", \"mightn't\" : \"might not\", \"mustn't\" : \"must not\", \n\"shan't\" : \"shall not\", \"she'd\" : \"she would\", \"she'll\" : \"she will\", \"she's\" : \"she is\", \"shouldn't\" : \"should not\", \"shouldnt\" : \"should not\",\n\"that's\" : \"that is\", \"thats\" : \"that is\", \"there's\" : \"there is\", \"theres\" : \"there is\", \"they'd\" : \"they would\", \"they'll\" : \"they will\",\n\"they're\" : \"they are\", \"theyre\":  \"they are\", \"they've\" : \"they have\", \"we'd\" : \"we would\", \"we're\" : \"we are\", \"weren't\" : \"were not\",\n\"we've\" : \"we have\", \"what'll\" : \"what will\", \"what're\" : \"what are\", \"what's\" : \"what is\", \"what've\" : \"what have\", \"where's\" : \"where is\",\n\"who'd\" : \"who would\", \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\", \"who've\" : \"who have\", \"won't\" : \"will not\", \"wouldn't\" : \"would not\", \"you'd\" : \"you would\",\n\"you'll\" : \"you will\", \"you're\" : \"you are\", \"you've\" : \"you have\", \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\":\" will\", \"didn't\": \"did not\", \"tryin'\":\"trying\"}\n\n\ndef clean_punct(text):\n  text = str(text)\n  for punct in PUNCTS:\n    text = text.replace(punct, ' {} '.format(punct))\n  \n  return text\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n","ae63e893":"kp = KeywordProcessor(case_sensitive=True)","35c00077":"for k, v in mispell_dict.items():\n    kp.add_keyword(k, v)","06460267":"def preprocessing(text):\n    text = kp.replace_keywords(text)\n    text = clean_punct(text)\n    text = re.sub(r'\\n\\r', ' ', text)\n    text = re.sub(r'\\s{2,}', ' ', text)\n    \n    return text.split()","3177ebc2":"train['clean_title'] = train['question_title'].apply(lambda x : preprocessing(x))\ntrain['clean_body'] = train['question_body'].apply(lambda x : preprocessing(x))\ntrain['clean_answer'] = train['answer'].apply(lambda x : preprocessing(x))\n\ntest['clean_title'] = test['question_title'].apply(lambda x : preprocessing(x))\ntest['clean_body'] = test['question_body'].apply(lambda x : preprocessing(x))\ntest['clean_answer'] = test['answer'].apply(lambda x : preprocessing(x))","e468cc99":"y_columns = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']","d9094cc0":"def build_matrix(word_index):\n\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    \n    unknown_words = []\n    unk = {}\n    known_count = 0\n    unk_count = 0\n    for word, i in word_index.items():\n        if word in MODEL:\n            embedding_matrix[i] = MODEL[word]\n            known_count += 1\n            continue\n        if word.lower() in MODEL:\n            embedding_matrix[i] = MODEL[word.lower()]\n            known_count += 1\n            continue    \n        if word.upper() in MODEL:\n            embedding_matrix[i] = MODEL[word.upper()]\n            known_count += 1\n            continue\n        if word.capitalize() in MODEL:\n            embedding_matrix[i] = MODEL[word.capitalize()]\n            known_count += 1\n            continue\n        if unidecode.unidecode(word) in MODEL:\n            embedding_matrix[i] = MODEL[unidecode.unidecode(word)]\n            known_count += 1\n            continue\n        try:\n            unk[word] += 1 \n        except:\n            unk[word] = 1\n        \n        unk_count += 1\n    \n    \n    print('all token in embedding percentage : {:.2f}%'.format( known_count\/(unk_count+known_count)  * 100))\n#     print('token in embedding percentage : {}'.format( known_count\/(unk_count+known_count)))\n    return embedding_matrix, unk","11042abf":"tokenizer = text.Tokenizer(filters='', lower=False)\n\n\ntokenizer.fit_on_texts(list(train['clean_title']) + list(train['clean_body']) + list(train['clean_answer']) \\\n                        + list(test['clean_title']) + list(test['clean_body']) + list(test['clean_answer']))\n\n","efe3986c":"TITLE_MAX_LEN = 50\nBODY_MAX_LEN = 500\nANSWER_MAX_LEN = 500\n","c76495e8":"train['clean_title_len'] = train['clean_title'].apply(lambda x : len(x))\ntrain['clean_body_len'] = train['clean_body'].apply(lambda x : len(x))\ntrain['clean_answer_len'] = train['clean_answer'].apply(lambda x : len(x))\n\n\ntest['clean_title_len'] = test['clean_title'].apply(lambda x : len(x))\ntest['clean_body_len'] = test['clean_body'].apply(lambda x : len(x))\ntest['clean_answer_len'] = test['clean_answer'].apply(lambda x : len(x))\n\n# train title max 58 test 48\n\n# train body max 4924 test body max 1894\n\n# train answer max 8194 test max 2224","a3bb3950":"x_train_title = tokenizer.texts_to_sequences(train['clean_title'])\nx_test_title = tokenizer.texts_to_sequences(test['clean_title'])\n\nx_train_body = tokenizer.texts_to_sequences(train['clean_body'])\nx_test_body = tokenizer.texts_to_sequences(test['clean_body'])\n\nx_train_answer = tokenizer.texts_to_sequences(train['clean_answer'])\nx_test_answer = tokenizer.texts_to_sequences(test['clean_answer'])\n\n\nx_train_title = sequence.pad_sequences(x_train_title, maxlen=TITLE_MAX_LEN,padding='post')\nx_test_title = sequence.pad_sequences(x_test_title, maxlen=TITLE_MAX_LEN,padding='post')\n\nx_train_body = sequence.pad_sequences(x_train_body, maxlen=BODY_MAX_LEN,padding='post')\nx_test_body = sequence.pad_sequences(x_test_body, maxlen=BODY_MAX_LEN,padding='post')\n\nx_train_answer = sequence.pad_sequences(x_train_answer, maxlen=ANSWER_MAX_LEN,padding='post')\nx_test_answer = sequence.pad_sequences(x_test_answer, maxlen=ANSWER_MAX_LEN,padding='post')\n","c641f35d":"from sklearn.preprocessing import OneHotEncoder\n\n\nc = 'host'\nonehotencoder = OneHotEncoder(sparse=False, categories='auto').fit(np.concatenate((train[c].values.reshape(-1, 1).astype('str'), test[c].values.reshape(-1, 1).astype('str'))))\ntrain_trans = onehotencoder.transform(train[c].values.reshape(-1, 1).astype('str'))\ntest_trans = onehotencoder.transform(test[c].values.reshape(-1, 1).astype('str'))\nfor i in range(train_trans.shape[1]):\n    train['{}_{}'.format(c, i)] = train_trans[:, i]\n    test['{}_{}'.format(c, i)] = test_trans[:, i]\nprint('remove origin column : {}'.format(c))\ntrain = train.drop(columns=c)\ntest = test.drop(columns=c)\ngc.collect()\n\n","1e706b9e":"# ## additional feature\n# def get_set_char_len(content):\n#     set_char = set()\n#     for char in ' '.join(content):\n#         set_char.add(char)\n#     return len(set_char)\n\n# train['title_set_char_len'] = train['clean_title'].apply(lambda x : get_set_char_len(x)) \n# train['body_set_char_len'] = train['clean_body'].apply(lambda x : get_set_char_len(x)) \n# train['answer_set_char_len'] = train['clean_answer'].apply(lambda x : get_set_char_len(x)) \n\n\n# test['title_set_char_len'] = test['clean_title'].apply(lambda x : get_set_char_len(x)) \n# test['body_set_char_len'] = test['clean_body'].apply(lambda x : get_set_char_len(x)) \n# test['answer_set_char_len'] = test['clean_answer'].apply(lambda x : get_set_char_len(x)) \n\n\n# train_title_len = train['clean_title_len'] \/ max(train['clean_title_len'])\n# train_body_len = train['clean_body_len'] \/ max(train['clean_body_len'])\n# train_answer_len = train['clean_answer_len'] \/ max(train['clean_answer_len'])\n\n# test_title_len = test['clean_title_len'] \/ max(test['clean_title_len'])\n# test_body_len = test['clean_body_len'] \/ max(test['clean_body_len'])\n# test_answer_len = test['clean_answer_len'] \/ max(test['clean_answer_len'])\n\n# train_title_set_char_len = train['title_set_char_len'] \/ max(train['title_set_char_len'])\n# train_body_set_char_len = train['body_set_char_len'] \/ max(train['body_set_char_len'])\n# train_answer_set_char_len = train['answer_set_char_len'] \/ max(train['answer_set_char_len'])\n\n# test_title_set_char_len = test['title_set_char_len'] \/ max(test['title_set_char_len'])\n# test_body_set_char_len = test['body_set_char_len'] \/ max(test['body_set_char_len'])\n# test_answer_set_char_len = test['answer_set_char_len'] \/ max(test['answer_set_char_len'])\n","340270c0":"\ntrain_category = pd.get_dummies(train['category'].values).values\ntest_category = pd.get_dummies(test['category'].values).values\n\nhosts = ['host_{}'.format(i) for i in range(64)]\ntrain_host = train.loc[:, hosts].values\ntest_host = test.loc[:, hosts].values","6a6f74ed":"word2vec_matrix, unk = build_matrix(tokenizer.word_index)\n\n","aea737c6":"class Attention(nn.Module):\n    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n\n        self.supports_masking = True\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.step_dim = step_dim\n        self.features_dim = 0\n\n        weight = torch.zeros(feature_dim, 1)\n        nn.init.xavier_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n\n        if bias:\n            self.b = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x, mask=None):\n\n        feature_dim = self.feature_dim\n        step_dim = self.step_dim\n\n        eij = torch.mm(\n            x.contiguous().view(-1, feature_dim), \n            self.weight\n        ).view(-1, step_dim)\n\n        if self.bias:\n            eij = eij + self.b\n\n        eij = torch.tanh(eij)\n        a = torch.exp(eij)\n\n        if mask is not None:\n            a = a * mask\n\n        a = a \/ torch.sum(a, 1, keepdim=True) + 1e-10\n\n        weighted_input = x * torch.unsqueeze(a, -1)\n        return torch.sum(weighted_input, 1)\n\nclass SpatialDropout(nn.Module):\n    def __init__(self,p):\n        super(SpatialDropout, self).__init__()\n        self.dropout = nn.Dropout2d(p)\n        \n    def forward(self, x):\n        x = x.permute(0, 2, 1)   # convert to [batch, feature, timestep]\n        x = self.dropout(x)\n        x = x.permute(0, 2, 1)   # back to [batch, timestep, feature]\n        return x\n\nclass LSTM_Model(nn.Module):\n    def __init__(self, embedding_matrix, hidden_unit, num_layer=1):\n        super(LSTM_Model, self).__init__()\n        self.max_feature = embedding_matrix.shape[0]\n        self.embedding_size = embedding_matrix.shape[1]\n      \n        self.embedding_body = nn.Embedding(self.max_feature, self.embedding_size)\n        self.embedding_body.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding_body.weight.required_grad = False\n        \n        self.embedding_answer = nn.Embedding(self.max_feature, self.embedding_size)\n        self.embedding_answer.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding_answer.weight.required_grad = False\n        \n        self.embedding_title = nn.Embedding(self.max_feature, self.embedding_size)\n        self.embedding_title.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding_title.weight.required_grad = False\n        \n        self.embedding_dropout = SpatialDropout(0.4)\n        \n        self.lstm1_body = nn.LSTM(self.embedding_size, hidden_unit, num_layers=num_layer, bidirectional=True, batch_first=True)\n        self.lstm2_body = nn.LSTM(hidden_unit*2, int(hidden_unit\/2), num_layers=num_layer, bidirectional=True, batch_first=True)\n        \n        self.lstm1_answer = nn.LSTM(self.embedding_size, hidden_unit, num_layers=num_layer, bidirectional=True, batch_first=True)\n        self.lstm2_answer = nn.LSTM(hidden_unit*2, int(hidden_unit\/2), num_layers=num_layer, bidirectional=True, batch_first=True)\n        \n        self.lstm1_title = nn.LSTM(self.embedding_size, hidden_unit, num_layers=num_layer, bidirectional=True, batch_first=True)\n        self.lstm2_title = nn.LSTM(hidden_unit*2, int(hidden_unit\/2), num_layers=num_layer, bidirectional=True, batch_first=True)\n        \n        self.attention_body = Attention(hidden_unit, BODY_MAX_LEN)\n        self.attention_answer = Attention(hidden_unit, ANSWER_MAX_LEN)\n        self.attention_title = Attention(hidden_unit, TITLE_MAX_LEN)\n        \n#         self.category = nn.Embedding(5, 10)\n#         self.host = nn.Embedding(64, 128)\n        \n        self.linear_title = nn.Linear(hidden_unit*3, hidden_unit)\n        self.linear_body = nn.Linear(hidden_unit*3, hidden_unit)\n        self.linear_answer = nn.Linear(hidden_unit*3, hidden_unit)\n        \n#         self.linear_out = nn.Linear(hidden_unit, 30)\n        self.additional_category = nn.Linear(5, 5)\n        self.additional_host = nn.Linear(64, 32)\n        \n        self.linear_q = nn.Linear(hidden_unit*2+37, hidden_unit)\n        self.linear_a = nn.Linear(hidden_unit+37, hidden_unit)\n        self.linear_q_out = nn.Linear(hidden_unit, 21)\n        self.linear_a_out = nn.Linear(hidden_unit, 9)\n        \n    def forward(self, body, answer, title, category, host):\n        \n        x_body = self.embedding_dropout(self.embedding_body(body))\n        h_lstm1_body, _ = self.lstm1_body(x_body)\n        h_lstm2_body, _ = self.lstm2_body(h_lstm1_body)\n        \n        x_answer = self.embedding_dropout(self.embedding_answer(answer))\n        h_lstm1_answer, _ = self.lstm1_answer(x_answer)\n        h_lstm2_answer, _ = self.lstm2_answer(h_lstm1_answer)\n        \n        x_title = self.embedding_dropout(self.embedding_title(title))\n        h_lstm1_title, _ = self.lstm1_title(x_title)\n        h_lstm2_title, _ = self.lstm2_title(h_lstm1_title)\n        \n#         print(h_lstm2_body.size())\n        att_body = self.attention_body(h_lstm2_body)\n        att_answer = self.attention_answer(h_lstm2_answer)\n        att_title = self.attention_title(h_lstm2_title)\n        \n        avg_pool_body = torch.mean(h_lstm2_body, 1)\n        max_pool_body, _ = torch.max(h_lstm2_body, 1)\n        \n        avg_pool_answer = torch.mean(h_lstm2_answer, 1)\n        max_pool_answer, _ = torch.max(h_lstm2_answer, 1)\n        \n        avg_pool_title = torch.mean(h_lstm2_title, 1)\n        max_pool_title, _ = torch.max(h_lstm2_title, 1)\n        \n        body_cat = torch.cat((att_body, avg_pool_body, max_pool_body), 1)\n        answer_cat = torch.cat((att_answer, avg_pool_answer, max_pool_answer), 1)\n        title_cat = torch.cat((att_title, avg_pool_title, max_pool_title), 1)\n        \n#         additional_feature = self.addtional_linear()\n\n#         category = self.category(category)\n#         host = self.category(host)\n        \n        \n        body_cat = torch.relu(self.linear_body(body_cat))\n        answer_cat = torch.relu(self.linear_answer(answer_cat))\n        title_cat = torch.relu(self.linear_title(title_cat))\n\n        category = self.additional_category(category)\n        host = self.additional_host(host)\n        \n        hidden_q = self.linear_q(torch.cat((title_cat, body_cat, category, host), 1))\n        hidden_a = self.linear_a(torch.cat((answer_cat, category, host), 1))\n                                          \n        q_result = self.linear_q_out(hidden_q)\n        a_result = self.linear_a_out(hidden_a)\n        \n        out = torch.cat([q_result, a_result], 1)\n        return out","e7cb6c77":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","6fc60d69":"SEED = 2020\nNFOLDS = 4\nBATCH_SIZE = 32\nEPOCHS = 6\nLR = 0.001\nhidden_unit = 256\nseed_everything(SEED)","958b0601":"kf = list(KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED).split(x_train_title))\n","c29d3753":"class TextDataset(torch.utils.data.TensorDataset):\n\n    def __init__(self, body_data, answer_data, title_data, category_data, host_data, idxs, targets=None):\n        self.body_data = body_data[idxs]\n        self.answer_data = answer_data[idxs]\n        self.title_data = title_data[idxs]\n        self.category_data = category_data[idxs]\n        self.host_data = host_data[idxs]\n        self.targets = targets[idxs] if targets is not None else np.zeros((self.body_data.shape[0], 30))\n\n    def __getitem__(self, idx):\n        body = self.body_data[idx]\n        answer = self.answer_data[idx]\n        title = self.title_data[idx]\n        category = self.category_data[idx]\n        host = self.host_data[idx]\n        target = self.targets[idx]\n\n        return body, answer, title, category, host, target\n\n    def __len__(self):\n        return len(self.body_data)","e942019f":"test_loader = torch.utils.data.DataLoader(TextDataset(x_test_body, x_test_answer, x_test_title, test_category, test_host, test.index),\n                          batch_size=BATCH_SIZE, shuffle=False)","1f02f250":"gc.collect()","c7489591":"y = train.loc[:, y_columns].values\n\noof = np.zeros((len(train), 30))\ntest_pred = np.zeros((len(test), 30))\n\n# del train, hosts, onehotencoder\n# gc.collect()\nfor i, (train_idx, valid_idx) in enumerate(kf):\n    print(f'fold {i+1}')\n    gc.collect()\n    train_loader = torch.utils.data.DataLoader(TextDataset(x_train_body, x_train_answer, x_train_title, train_category, train_host, train_idx, y),\n                          batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n    \n    val_loader = torch.utils.data.DataLoader(TextDataset(x_train_body, x_train_answer, x_train_title, train_category, train_host, valid_idx, y),\n                          batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n    \n    net = LSTM_Model(word2vec_matrix, hidden_unit)\n    net.cuda()\n    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean').cuda()\n    optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n\n    for epoch in range(EPOCHS):  \n        start_time = time.time()\n        avg_loss = 0.0\n        net.train()\n        for data in train_loader:\n\n            # get the inputs\n            body, answer, title, category, host, labels = data\n            pred = net(body.long().cuda(), answer.long().cuda(), title.long().cuda(), category.float().cuda(), host.float().cuda())\n\n            loss = loss_fn(pred, labels.cuda())\n            # Before the backward pass, use the optimizer object to zero all of the\n            # gradients for the Tensors it will update (which are the learnable weights\n            # of the model)\n            optimizer.zero_grad()\n\n            # Backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # Calling the step function on an Optimizer makes an update to its parameters\n            optimizer.step()\n\n            avg_loss += loss.item()\n        \n        avg_val_loss = 0.0\n        net.eval()\n\n        valid_preds = np.zeros((len(valid_idx), 30))\n        true_label = np.zeros((len(valid_idx), 30))\n        for j, data in enumerate(val_loader):\n\n            # get the inputs\n            body, answer, title, category, host, labels = data\n\n            ## forward + backward + optimize\n            pred = net(body.long().cuda(), answer.long().cuda(), title.long().cuda(), category.float().cuda(), host.float().cuda())\n\n            loss_val = loss_fn(pred, labels.cuda())\n            avg_val_loss += loss_val.item()\n\n            valid_preds[j * BATCH_SIZE:(j+1) * BATCH_SIZE] = torch.sigmoid(pred).cpu().detach().numpy()\n            true_label[j * BATCH_SIZE:(j+1) * BATCH_SIZE]  = labels\n            \n        score = 0\n        for i in range(30):\n            score += np.nan_to_num(\n                    spearmanr(true_label[:, i], valid_preds[:, i]).correlation \/ 30)\n        oof[valid_idx] = valid_preds\n        elapsed_time = time.time() - start_time \n        print('Epoch {}\/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t spearman={:.2f} \\t time={:.2f}s'.format(\n            epoch + 1, EPOCHS, avg_loss \/ len(train_loader), avg_val_loss \/ len(val_loader), score, elapsed_time))\n        \n    test_pred_fold = np.zeros((len(test), 30))\n        \n    with torch.no_grad():\n        for q, data in enumerate(test_loader):\n            body, answer, title, category, host, _ = data\n            y_pred = net(body.long().cuda(), answer.long().cuda(), title.long().cuda(), category.float().cuda(), host.float().cuda())\n            test_pred_fold[q * BATCH_SIZE:(q+1) * BATCH_SIZE] = torch.sigmoid(y_pred).cpu().detach().numpy()\n    test_pred += test_pred_fold\/NFOLDS\n        \n        ","a182e349":"sub.loc[:, y_columns] = test_pred\nsub.to_csv('submission.csv', index=False)","b3648a50":"sub.head()","d99d0f64":"## This is my first public kernel. \n\n### I try using some simple preprocessing and create LSTM neural network with pytorch \n\n### score : 0.339 in version 6\n\n\nThanks for Andrew Lukyanenko sharing, Some of the code can be seen in the kernel:\n\nhttps:\/\/www.kaggle.com\/artgor\/pytorch-approach\/notebook\n\n### next step, I want to adding some additional feature, and try using BERT.\n\n"}}