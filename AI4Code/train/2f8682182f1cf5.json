{"cell_type":{"c8a2827a":"code","783ec676":"code","f1977427":"code","5474fde9":"code","a4363fa8":"code","b94aca6c":"code","e92a8bb9":"code","25c1a4f8":"code","10dff316":"code","25836972":"code","a035a2d6":"code","3fa6f6c4":"code","e6a7a4df":"code","76d081e1":"code","5fc86794":"code","41138354":"code","53af2c02":"code","919de63f":"code","d103402e":"code","0d66599a":"code","c981bb57":"code","0f88b546":"code","1101581f":"code","e4f76009":"code","b6f437c0":"code","d3cc7b4c":"code","f1295c14":"code","4de78189":"code","0ce18528":"code","18663205":"code","897b7f59":"code","b3229bcc":"code","4585d7f2":"code","97d5a62f":"code","03da4914":"code","3d639f09":"code","f0cb6f05":"code","c1a22365":"code","7af162b3":"code","6e59ffe7":"code","5ba9bb47":"code","b391b044":"code","c39d8811":"code","d0ad48af":"code","93755da7":"code","3c02247b":"code","f111805b":"code","b6af4480":"markdown","8522ece1":"markdown","0012b730":"markdown","4c391589":"markdown","625b61d0":"markdown","a19ba1ba":"markdown","87cd12f9":"markdown","7f6d799a":"markdown","a91b03c1":"markdown","e5e9ad6a":"markdown","ff6d70a2":"markdown","bd063903":"markdown"},"source":{"c8a2827a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n#Variable\tDefinition\tKey\n#survival \tSurvival \t0 = No, 1 = Yes\n#pclass \tTicket class \t1 = 1st, 2 = 2nd, 3 = 3rd\n#sex \tSex \t\n#Age \tAge in years \t\n#sibsp \t# of siblings \/ spouses aboard the Titanic \t\n#parch \t# of parents \/ children aboard the Titanic \t\n#ticket \tTicket number \t\n#fare \tPassenger fare \t\n#cabin \tCabin number \t\n#embarked \tPort of Embarkation \tC = Cherbourg, Q = Queenstown, S = Southampton","783ec676":"import os\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport lightgbm as lgbm\nimport catboost as cb\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_absolute_error","f1977427":"X_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest_data.head(5)","5474fde9":"# Remove rows with missing target\nX_train.dropna(axis=0, subset=['Survived'], inplace=True)","a4363fa8":"print(test_data.shape)\nprint(X_train.shape)\nX_train.info()","b94aca6c":"# Count of missing values in each column\nX_train.isnull().sum()","e92a8bb9":"# Count of unique values per column\nX_train.nunique()","25c1a4f8":"# Age, Cabin, Embarked have null values. Age and cabin have so many, they can be dropped.\nX_train = X_train.drop(['Cabin', 'Age'], axis=1)\nfull_test_data = test_data.drop(['Cabin', 'Age'], axis=1)","10dff316":"# NEXT TIME, instead of dropping 'Age':\n# we will use binning for Age and Fare features, hopefully will produce better results\n# func to process values\n#def encodeAgeFare(train):\n#    train.loc[train['Age'] <= 16, 'Age'] = 0\n#    train.loc[(train['Age'] > 16) & (train['Age'] <= 32), 'Age'] = 1\n#    train.loc[(train['Age'] > 32) & (train['Age'] <= 48), 'Age'] = 2\n#    train.loc[(train['Age'] > 48) & (train['Age'] <= 64), 'Age'] = 3\n#    train.loc[ (train['Age'] > 48) & (train['Age'] <= 80), 'Age'] = 4\n#    \n#    train.loc[train['Fare'] <= 7.91, 'Fare'] = 0\n#    train.loc[(train['Fare'] > 7.91) & (train['Fare'] <= 14.454), 'Fare'] = 1\n#    train.loc[(train['Fare'] > 14.454) & (train['Fare'] <= 31.0), 'Fare'] = 2\n#    train.loc[(train['Fare'] > 31.0) & (train['Fare'] <= 512.329), 'Fare'] = 3\n\n#encodeAgeFare(train_df)\n#encodeAgeFare(test_df)","25836972":"plt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.hist(X_train[\"Fare\"], bins=20)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.title(\"Fare distribution\", fontsize=16)\n\nplt.subplot(1, 2, 2)\nembarked_info = X_train[\"Embarked\"].value_counts()\nplt.bar(embarked_info.index, embarked_info.values)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.title(\"Embarked distribution\", fontsize=16);","a035a2d6":"# Show histograms of all (numerical) columns\nX_train.hist(figsize=(15,12),bins = 20, color=\"#107009AA\")\nplt.show()","3fa6f6c4":"!pip install sweetviz","e6a7a4df":"import sweetviz as sv\ndata_report = sv.analyze(X_train)\ndata_report.show_html('Analysis.html')","76d081e1":"from IPython.display import IFrame\nIFrame(src = 'Analysis.html',width=1000,height=600)","5fc86794":"X_train.dtypes","41138354":"# Show correlation between column 'Pclass' and target 'Survived'\n#X_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n# Automated for all columns (you need to drop the target )\n#a = features.drop('Survived').copy()\n#for col in a:\n    #display(X_train[[col, 'Survived']].groupby(col, as_index=False).mean().sort_values(by='Survived', ascending=False))","53af2c02":"# Separate target from features\ny = X_train.Survived\nX_train = X_train.drop('Survived', axis=1)\nfeatures = X_train.columns","919de63f":"X_train = X_train.drop(['Name', 'PassengerId'], axis=1)\ntest_data = full_test_data.drop(['Name', 'PassengerId'], axis=1).copy()\nprint(X_train.shape)\nprint(test_data.shape)","d103402e":"X_train.sample(6)","0d66599a":"X_train.isnull().sum()","c981bb57":"# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)","0f88b546":"print(X_train.shape)\nprint(X_valid.shape)\nprint(y_train.shape)\nprint(y_valid.shape)","1101581f":"## Imputing the missing values with the Mode because mode fills the values with the most accurate values and best for the categorical features\nX_train[\"Embarked\"] = X_train[\"Embarked\"].transform(lambda x: x.fillna(x.mode()[0]))\nX_valid[\"Embarked\"] = X_valid[\"Embarked\"].transform(lambda x: x.fillna(x.mode()[0]))\ntest_data[\"Embarked\"] = test_data[\"Embarked\"].transform(lambda x: x.fillna(x.mode()[0]))\n","e4f76009":"categorical_cols = [cname for cname in X_train.columns if X_train[cname].dtype == \"object\"]\ngood_label_cols = [col for col in categorical_cols if \n                   set(X_train[col]) == set(X_valid[col])]\n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(categorical_cols)-set(good_label_cols))","b6f437c0":"bad_label_cols","d3cc7b4c":"X_train.sample(5)","f1295c14":"# Drop categorical columns that will not be encoded\nlabel_X_train = X_train.drop(bad_label_cols, axis=1)\nlabel_X_valid = X_valid.drop(bad_label_cols, axis=1)\nlabel_test_data = test_data.drop(bad_label_cols, axis=1)\n\n\n# Apply label encoder\nlabel_encoder = LabelEncoder()\nfor col in set(good_label_cols):\n    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n    label_X_valid[col] = label_encoder.transform(X_valid[col])\n    label_test_data[col] = label_encoder.transform(test_data[col])","4de78189":"label_test_data.isnull().sum()","0ce18528":"label_test_data[\"Fare\"] = label_test_data[\"Fare\"].transform(lambda x: x.fillna(x.mode()[0]))","18663205":"label_test_data.isnull().sum()","897b7f59":"label_X_train.sample(5)","b3229bcc":"label_X_train.dtypes","4585d7f2":"## all columns are numerical. Dropped columns with too many null values, dropped irrelevant(?) columns","97d5a62f":"label_test_data.head()","03da4914":"# get baseline\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier()\nmodel.fit(label_X_train, y_train)\npreds = model.predict(label_X_valid)\nprint(f'accuracy score = {accuracy_score(y_valid, preds)}')\nprint(f'MAE = {mean_absolute_error(y_valid, preds)}')","3d639f09":"#from sklearn.model_selection import RandomizedSearchCV\n#param_grid_xgb = {'n_estimators':[400, 600],\n#                   'learning_rate':[0.01, 0.03, 0.05],\n#                   'max_depth':[3, 4],\n#                   'subsample':[0.5, 0.7],\n#                   'colsample_bylevel':[0.5, 0.7],\n#                   'reg_lambda':[15, None],\n#                  }\n\n#clf = xgb.XGBClassifier()\n#grid = RandomizedSearchCV(clf, cv=3, param_distributions = param_grid_xgb,scoring='accuracy', verbose=2, n_jobs=-1)\n#grid.fit(label_X_train, y_train, eval_set = [(label_X_train,y_train),(label_X_valid,y_valid)], early_stopping_rounds = 10)\n#grid.best_params_\n#model = grid.best_estimator_\n# Runs about 10 mins","f0cb6f05":"from xgboost import XGBClassifier\nmodel = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.7, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints='', learning_rate=0.05, max_delta_step=0, max_depth=3, min_child_weight=1, missing=np.nan, monotone_constraints='()', n_estimators=400, n_jobs=4, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=15, scale_pos_weight=1, subsample=0.5, tree_method='exact', validate_parameters=1, verbosity=None)","c1a22365":"from sklearn.metrics import mean_absolute_error\nmodel.fit(label_X_train, y_train)\npreds = model.predict(label_X_valid)\n\nprint(f'accuracy score = {accuracy_score(y_valid, preds)}')\nprint(f'MAE = {mean_absolute_error(preds, y_valid)}')","7af162b3":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier","6e59ffe7":"KNN = KNeighborsClassifier()\nNB = GaussianNB()\nVector = svm.SVC()\nRandomforest = RandomForestClassifier()\nmodels = [KNN, NB, Vector, Randomforest]","5ba9bb47":"models","b391b044":"def get_scores():\n    for i in models:\n        i.fit(label_X_train, y_train)\n        preds = i.predict(label_X_valid)\n        print(f'accuracy score {i} = {accuracy_score(y_valid, preds)}')\n        print(f'MAE {i} = {mean_absolute_error(preds, y_valid)}')","c39d8811":"get_scores()","d0ad48af":"Randomforest.get_params().keys()","93755da7":"from sklearn.model_selection import RandomizedSearchCV\nparams = {'n_estimators':[1, 2, 4, 8, 16, 32, 64, 100, 200],\n          'max_depth':[1, 10, 20, 30],\n          'min_samples_split':[0.1, 0.4, 0.7, 1.0],\n          'max_features':[1, 2, 3, 4, 5, 6, 7]\n         }\n\nRSearch = RandomizedSearchCV(Randomforest, cv=5, param_distributions = params,scoring='accuracy', verbose=1, n_jobs=-1)\nRSearch.fit(label_X_train, y_train)\nmodel = RSearch.best_estimator_\nRSearch.best_params_","3c02247b":"model.fit(label_X_train, y_train)\npreds = model.predict(label_X_valid)\n\nprint(f'accuracy score = {accuracy_score(y_valid, preds)}')\nprint(f'MAE = {mean_absolute_error(preds, y_valid)}')","f111805b":"model3 = DecisionTreeClassifier()\n\nmodel3.fit(label_X_train, y_train)\npreds3 = model3.predict(label_test_data)\n\nfinal3 = pd.DataFrame({'PassengerId':full_test_data['PassengerId'],'Survived':preds3})\nfinal3.to_csv('DecisionTree.csv',index=False)\n\n","b6af4480":"# 1- Read data","8522ece1":"Check test dataset","0012b730":"# Build Model","4c391589":"# Evaluating other models","625b61d0":"Get Tuned parameters for RandomForest","a19ba1ba":"# Create Output","87cd12f9":"# 2- Exploration & feature selection","7f6d799a":"**We can see that the correlation of pclass with survived is more than 0.5 among Pclass=1 so we are going to use this feature in training (repeat for other columns)**","a91b03c1":"Easy plot:\nx-axis values  \nx = [1,2,3]  \n#corresponding y axis values  \ny = [2,4,1]  \n    \n#plotting the points   \nplt.plot(x, y)  \n    \n#naming the x axis  \nplt.xlabel('x - axis')  \n#naming the y axis  \nplt.ylabel('y - axis')  \n    \n#giving a title to my graph  \nplt.title('My first graph!')  \n    \n#function to show the plot  \nplt.show()  ","e5e9ad6a":"XGBoost, RandomForest and DecisionTree performed best ","ff6d70a2":"# best parameters for XGBoost: \nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.7,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.05, max_delta_step=0, max_depth=3,\n              min_child_weight=1, missing=nan, monotone_constraints='()',\n              n_estimators=400, n_jobs=4, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=15, scale_pos_weight=1, subsample=0.5,\n              tree_method='exact', validate_parameters=1, verbosity=None)","bd063903":"# 3- Feature engineering and preparation"}}