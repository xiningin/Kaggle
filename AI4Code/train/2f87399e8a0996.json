{"cell_type":{"889d670d":"code","efb476d1":"code","007ec199":"code","45fc5b29":"code","31699c7c":"code","75f2e173":"code","3d9e9fb4":"code","45c048ab":"code","43714d69":"code","b0100c10":"code","5fd17fca":"code","66749b5f":"code","b749d614":"code","9c41e85e":"code","91685a37":"code","9f1890f7":"code","d1244fcc":"code","0bac01c3":"code","71bf982f":"code","c731585d":"code","336337a9":"code","2eb271fe":"code","fc1a5463":"code","5fa25c73":"code","69cda978":"code","17b72b58":"code","9f30fa16":"code","ec1a81bb":"code","2324daa2":"code","1af5e41a":"code","4a84c1e4":"code","d296d72c":"code","e7cb3f8c":"code","cb332cdb":"code","895cbb6b":"code","62639828":"code","25089089":"code","431eec3e":"code","9e17d4ca":"code","bbcb6a6c":"code","d3fb6596":"code","d60667f1":"code","ac818c2a":"code","27b79130":"code","511feebc":"code","3e777bee":"code","19f42b81":"code","cbdd60d1":"code","7bd93beb":"code","e63b8912":"code","6b1a9c4a":"code","b79bb59a":"code","912397b5":"code","020674f6":"code","25c207a6":"code","4aff074d":"code","9c83d2ab":"code","a67b3ef3":"code","eea2cbab":"code","eba61e17":"code","12f8254d":"code","786ba57f":"code","2b4e4e0c":"code","6fc04091":"code","42c7d6e7":"code","9f42eb5f":"code","83d0c6b8":"code","e1be47e7":"code","30029b99":"code","7eae7af1":"markdown","f1300cca":"markdown","b9c5c5c7":"markdown","c511bb7f":"markdown","ac651f38":"markdown","e1021709":"markdown","401aadbe":"markdown","53ead8bd":"markdown","aadc23d5":"markdown","72aba917":"markdown","48e7de6e":"markdown","798b3f46":"markdown","ce9d1526":"markdown","24b1298e":"markdown","e0ffb8af":"markdown","8d607758":"markdown","58653931":"markdown","ae12ed80":"markdown","e693f73b":"markdown","f9b77d85":"markdown"},"source":{"889d670d":"\nimport numpy as np \nimport pandas as pd \nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport emoji\nimport os\nimport time\nimport sys\nimport fasttext\nimport re\nimport nltk \nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nimport tensorflow as tf\ntf.keras.backend.clear_session()\n\nimport itertools\nimport collections\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n#load data \nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","efb476d1":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntrain.head()","007ec199":"test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ntest.head()","45fc5b29":"train=train[['text','target']]\n#null_data = train[train.isnull().any(axis=1)] there is no null value in test set after removing the id, keyword, location column\ntest=test[['id','text']]","31699c7c":"# check class distribution in train dataset\nfrom scipy import stats\ntrain.groupby(['target']).size()","75f2e173":"colors = list('rgb') #rgbkm\ncount_classes = pd.value_counts(train['target'], sort = True)\ncount_classes.plot(kind = 'bar', rot=0, color=colors, figsize=(6,4))\nplt.title(\"Class Distribution\")\nplt.xlabel(\"class\")\nplt.ylabel(\"Count\");","3d9e9fb4":"train.head()\ntrain['text'].head()","45c048ab":"#import contractions list and remove it in the next step\n# A list of contractions from http:\/\/stackoverflow.com\/questions\/19790188\/expanding-english-language-contractions-in-python\ncontractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'll\": \"i will\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'll\": \"it will\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"needn't\": \"need not\",\n\"oughtn't\": \"ought not\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"she'd\": \"she would\",\n\"she'll\": \"she will\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"that'd\": \"that would\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'll\": \"they will\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'll\": \"we will\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"who'll\": \"who will\",\n\"who's\": \"who is\",\n\"won't\": \"will not\",\n\"wouldn't\": \"would not\",\n\"you'd\": \"you would\",\n\"you'll\": \"you will\",\n\"you're\": \"you are\",\n\"thx\"   : \"thanks\"\n}","43714d69":"def remove_contractions(text):\n    return contractions[text.lower()] if text.lower() in contractions.keys() else text","b0100c10":"train['text']=train['text'].apply(remove_contractions)\ntrain.tail()","5fd17fca":"test['text']=test['text'].apply(remove_contractions)\nlen(test['text'])","66749b5f":"# clean dataset\ndef clean_dataset(text):\n    # To lowercase\n    text = text.lower()\n    # Remove hashtag while keeping hashtag text\n    text = re.sub(r'#','', text)\n    # Remove HTML special entities (e.g. &amp;)\n    text = re.sub(r'\\&\\w*;', '', text)\n    # Remove tickers\n    text = re.sub(r'\\$\\w*', '', text)\n    # Remove hyperlinks\n    text = re.sub(r'https?:\\\/\\\/.*\\\/\\w*', '', text)\n    # Remove whitespace (including new line characters)\n    text = re.sub(r'\\s\\s+','', text)\n    text = re.sub(r'[ ]{2, }',' ',text)\n    # Remove URL, RT, mention(@)\n    text=  re.sub(r'http(\\S)+', '',text)\n    text=  re.sub(r'http ...', '',text)\n    text=  re.sub(r'(RT|rt)[ ]*@[ ]*[\\S]+','',text)\n    text=  re.sub(r'RT[ ]?@','',text)\n    text = re.sub(r'@[\\S]+','',text)\n    # Remove words with 2 or fewer letters\n    text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n    #&, < and >\n    text = re.sub(r'&amp;?', 'and',text)\n    text = re.sub(r'&lt;','<',text)\n    text = re.sub(r'&gt;','>',text)\n    # Insert space between words and punctuation marks\n    text = re.sub(r'([\\w\\d]+)([^\\w\\d ]+)', '\\1 \\2',text)\n    text = re.sub(r'([^\\w\\d ]+)([\\w\\d]+)', '\\1 \\2',text)\n    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n    text= ''.join(c for c in text if c <= '\\uFFFF') \n    text = text.strip()\n    # Remove misspelling words\n    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n    # Remove punctuation\n    text = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=\\\/\\|\\'\\(\\']\", \" \", text).split())\n    # Remove emoji\n    text = emoji.demojize(text)\n    text = text.replace(\":\",\" \")\n    text = ' '.join(text.split()) \n    text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n    # Remove Mojibake (also extra spaces)\n    text = ' '.join(re.sub(\"[^\\u4e00-\\u9fa5\\u0030-\\u0039\\u0041-\\u005a\\u0061-\\u007a]\", \" \", text).split())\n    return text","b749d614":"train['text'] =train['text'].apply(clean_dataset)\ntrain.tail()","9c41e85e":"test['text'] =test['text'].apply(clean_dataset)","91685a37":"#remove stop word i.e. the most frequently appeared words \n# I ended up with building my own stop word lists because NLTP simply removed all negation words which totally changed the meaning of the sentence. \nmyOwnStopList=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'what','how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'do', 'should', \"should\", 'now', 'd', 'm', 'o', 're', 've', 'y', 'ain', \"are\", 'could', \"was\",\n'would','have','get','got','getting','one','two','still','going']","9f1890f7":"#stop = stopwords.words('english')\n#stop+=['get','got','getting','one','two','would','still','could','going']#customized stop word list\n#stop = [e for e in stop if e not in (\"n't\", \"not\", \"no\")]\n\n# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n\ntrain['text'] = train['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (myOwnStopList)]))\ntrain.head()","d1244fcc":"test['text'] = test['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (myOwnStopList)]))\npred=test['text']","0bac01c3":"# most common words in twitter dataset \nall_texts = []\nfor line in list(train['text']):\n    texts = line.split()\n    for text in texts:\n        all_texts.append(text)\n# plot word frequency distribution of first few words\nplt.figure(figsize=(12,5))\nplt.xticks(fontsize=13, rotation=90)\nfd = nltk.FreqDist(all_texts)\nfd.plot(25,cumulative=False)\n# log-log of all words \nword_counts = sorted(Counter(all_texts).values(), reverse=True)","71bf982f":"# create a word frequency dictionary\nwordfreq = Counter(all_texts)\n# draw a Word Cloud with word frequencies\nwordcloud = WordCloud(width=900,\n                      height=500,\n                      max_words=200,\n                      max_font_size=100,\n                      relative_scaling=0.5,\n                      colormap='cubehelix_r',\n                      normalize_plurals=True).generate_from_frequencies(wordfreq)\nplt.figure(figsize=(17,14))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","c731585d":"print(all_texts[:30])","336337a9":"toBeCleanedNew='[%s]' % ' '.join(map(str, all_texts))#remove all the quation marks and commas. \n#print(toBeCleanedNew)","2eb271fe":"len(wordfreq)#the whole vocabulary size in train dataset","fc1a5463":"#You need to install version==0.4.2 to run hugging face tokenizer on kaggle\n!pip --no-cache-dir install tokenizers==0.4.2 #you get only a warning, just ignore it","5fa25c73":"#!pip install tokenizers #hugging face tokenizer\nfrom tokenizers import (ByteLevelBPETokenizer,\n                            CharBPETokenizer,\n                            SentencePieceBPETokenizer,\n                            BertWordPieceTokenizer)\ntokenizer = SentencePieceBPETokenizer()\n#Kaggle has a restriction to write file directly to the server, I download the cleaned corpus and uploaded it back.\npath=\"..\/input\/tobetrainedtxt\/tobeTrained.txt\"\n#set vocab_size to 15000 as the len(train_set)was something like 12500 \ntokenizer.train(files=path, vocab_size=15_000, min_frequency=2, special_tokens=[\n   \"<pad>\",\n   \"<s>\",\n   \"<\/s>\",\n   \"<unk>\",\n   \"<mask>\",\n])\n","69cda978":"# Now I got my trained corpus and it saved to my working directory.\ntokenizer.save(\".\", \"\/kaggle\/working\/tokenedText\")","17b72b58":"#let's try to tokenize the first sentence\noutput = tokenizer.encode(\"deeds reason earthquake may allah forgive\")\n#print(output)\nprint(output.ids,output.tokens, output.offsets)","9f30fa16":"#convert it back :)\ndecode=tokenizer.decode([119, 2281, 1940, 1002, 352, 3542, 9179])\nprint(decode)","ec1a81bb":"# As we don't have label for test dataset, I will split train dataset as trainsub and testsub dataset.\nfrom sklearn.model_selection import train_test_split\nX_subtrain, X_subtest, y_subtrain, y_subtest = train_test_split(\n  train['text'],train['target'], test_size=0.2, random_state=42)","2324daa2":"X_subtrain.head()","1af5e41a":"X_subtest.head()","4a84c1e4":"y_subtrain.head()","d296d72c":"# import trained corpus to tokenize our dataframe\n\nfrom tokenizers.implementations import SentencePieceBPETokenizer\nfrom tokenizers.processors import BertProcessing\n\ntokenizer =SentencePieceBPETokenizer(\n    '\/kaggle\/working\/tokenedText-vocab.json',\n    '\/kaggle\/working\/tokenedText-merges.txt', \n)\ntokenizer._tokenizer.post_processor = BertProcessing(\n   (\"<\/s>\", tokenizer.token_to_id(\"<\/s>\")),\n  (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n)\ntokenizer.enable_truncation(max_length=512)\n\nX_subtrain_tokened=X_subtrain.apply(lambda x:tokenizer.encode(x).ids)\nX_subtrain_tokened.head()","e7cb3f8c":"X_subtest_tokened=X_subtest.apply(lambda x:tokenizer.encode(x).ids)\nX_subtest_tokened.head()","cb332cdb":"y_subtrain.head()","895cbb6b":"y_subtest.head()","62639828":"# the original test dataset (without label)\ntest[\"Tokened_Text\"]=test[\"text\"].apply(lambda x:tokenizer.encode(x).ids)\ntest.head()","25089089":"#check Vocabulary (VOCAB_SIZE) in test dataset\nfrom collections import Counter\nsubtrain_tokened=[]\nfor i in X_subtrain_tokened:\n    subtrain_tokened+=i\nprint(\"Total amount of tokens in train dataset is:\", len(subtrain_tokened))\ndistinct_list= (Counter(subtrain_tokened).keys())\nprint(\"The vocabulary size in subtrain dataset is :\",len(distinct_list))","431eec3e":"#print(distinct_list)\nprint(max(distinct_list))","9e17d4ca":"#check Vocabulary (VOCAB_SIZE) in test dataset\nsubtest_tokened=[]\nfor i in X_subtest_tokened:\n    subtest_tokened+=i\nprint(\"Total amount of tokens in train dataset is:\", len(subtest_tokened))\ndistinct_list_2= (Counter(subtest_tokened).keys())\nprint(\"The vocabulary size in subtrain dataset is :\",len(distinct_list_2))","bbcb6a6c":"# length distribution in tokened subtrain dataset\nsubtrain_length_dist=[]\ni=0\nfor l in X_subtrain_tokened:\n    subtrain_length_dist+=[len(l)]\ny = np.array(subtrain_length_dist)\nsns.distplot(y);","d3fb6596":"# length distribution in tokened subtest dataset\nsubtest_length_dist=[]\ni=0\nfor l in X_subtest_tokened:\n    subtest_length_dist+=[len(l)]\ny = np.array(subtest_length_dist)\nsns.distplot(y);","d60667f1":"X_subtrain.head()\nX_subtrain_tokened.head()","ac818c2a":"import keras\nmax_len=30\nx_train = keras.preprocessing.sequence.pad_sequences(X_subtrain_tokened, maxlen=max_len)\nx_test = keras.preprocessing.sequence.pad_sequences(X_subtest_tokened, maxlen=max_len)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)","27b79130":"import tensorflow as tf\nembedding_dim=128\nvocab_size=9940\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()#","511feebc":"y_train=y_subtrain\ny_test=y_subtest","3e777bee":"from keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.callbacks import TensorBoard\nimport datetime\nfrom tensorflow.python.keras.callbacks import TensorBoard\n\nlogs_base_dir = \".\/logs\"\nlogdir = os.path.join(logs_base_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = tf.keras.callbacks.TensorBoard(logdir)\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=8)\nhistory=model.fit(x_train, y_train,\n          batch_size=12,\n          epochs=20,\n          validation_data=[x_test, y_test]) ","19f42b81":"submission=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","cbdd60d1":"test=keras.preprocessing.sequence.pad_sequences(test['Tokened_Text'], maxlen=max_len)\ntest.shape","7bd93beb":"y_pre=model.predict(test)\ny_pre=np.round(y_pre).astype(int).reshape(3263)\nsubmission=pd.DataFrame({'id':submission['id'].values.tolist(),'target':y_pre})\nsubmission.to_csv('submission.csv',index=False)","e63b8912":"submission.head()","6b1a9c4a":"#!pip install tokenizers #hugging face tokenizer\nfrom tokenizers import (ByteLevelBPETokenizer,\n                            CharBPETokenizer,\n                            SentencePieceBPETokenizer,\n                            BertWordPieceTokenizer)\ntokenizer = BertWordPieceTokenizer()\n\npath=\"..\/input\/forbert\/tobeTrainedForBert.txt\"\n#set vocab_size to 15000 as the len(train_set)was something like 12500 \ntokenizer.train(files=path, vocab_size=15_000, min_frequency=2)\n#tokenizer.train(files=path, vocab_size=15_000, min_frequency=2,special_tokens=[\n   # \"<s>\",\n    #\"<pad>\",\n    #\"<\/s>\",\n    #\"<unk>\",\n    #\"<mask>\"\n#])","b79bb59a":"# Now I got my trained corpus and it saved to my working directory.\ntokenizer.save(\".\", \"\/kaggle\/working\/newBert\")","912397b5":"#token.ids  PAD-->0, UNK-->1, CLS-->2 (start of sentence),SEP-->3 (end of sentence),MASK-->4\ntokenizer = BertWordPieceTokenizer(\n    \"\/kaggle\/working\/newBert-vocab.txt\", \n     lowercase=True, \n   # unk_token=\"<unk>\",\n    #sep_token=\"<\/s>\",\n   # cls_token=\"<s>\"\n)\n#tokenizer.add_special_tokens([\"<pad>\", \"<mask>\"])","020674f6":"print(tokenizer)","25c207a6":"output = tokenizer.encode(\"Hello, y'all! \ud83d\ude42 How are you  ?\")\nprint(output.tokens)\nprint(output.ids)","4aff074d":"tokenizer.enable_truncation(max_length=30)","9c83d2ab":"outputLongerThan30=tokenizer.encode(\"BBC News is an operational business division of the British Broadcasting Corporation responsible for the gathering and broadcasting of news and current affairs. \")\nprint(outputLongerThan30.tokens)#cut off length > 30 parts ==\"and current affairs. \"","a67b3ef3":"print(outputLongerThan30.ids)","eea2cbab":"convertback=tokenizer.decode([2, 2884, 355, 634, 168, 3821, 87, 1177, 3577, 1050, 7393, 1103, 2135, 9171, 83, 2587, 4406, 151, 6960, 303, 1103, 21, 520, 83, 2875, 9171, 83, 7393, 355, 3])\nprint(convertback)","eba61e17":"def bert_token(texts,max_len=128): \n    all_input_ids=[]\n    all_mask_ids=[]\n    all_seg_ids=[]\n    for token in texts: \n    \n        input_ids=tokenizer.encode(token).ids\n        mask_ids = [1] * len(input_ids)\n        seg_ids = [0] * len(input_ids)\n        padding = [0] * (max_len - len(input_ids))\n        input_ids += padding\n        mask_ids += padding\n        seg_ids += padding\n        all_input_ids.append(input_ids)\n        all_mask_ids.append(mask_ids)\n        all_seg_ids.append(seg_ids)\n\n    \n    return np.array(all_input_ids), np.array(all_mask_ids), np.array(all_seg_ids)","12f8254d":"train_input=bert_token(X_subtrain,max_len=32)\ntest_input=bert_token(X_subtest,max_len=32)\npred=bert_token(pred,max_len=32)","786ba57f":"def build_model(bert_layer, max_len=128):\n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n    mask_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"mask_ids\")\n    seg_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"seg_ids\")\n\n    pooled_output, sequence_output = bert_layer([input_ids,mask_ids,  seg_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_ids,  mask_ids,  seg_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","2b4e4e0c":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_wwm_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","6fc04091":"model = build_model(bert_layer, max_len=32)\nmodel.summary()","42c7d6e7":"#Can I use TensorBoard with Google Colab?\n#Tensorboard: ValueError: Duplicate plugins for name projector #22676\nlogs_base_dir = \".\/logs\"\nlogdir = os.path.join(logs_base_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = tf.keras.callbacks.TensorBoard(logdir)","9f42eb5f":"model.fit(\n    train_input, \n    y_subtrain,\n    callbacks=[tensorboard_callback],\n    validation_data=(test_input, y_subtest),\n    epochs=15,\n    batch_size=12 # In this case, the accuracy rate will be improved if you reduce the batch_size e.g., from 32 to 12. \n)","83d0c6b8":"model.save('BertModel.h5')\npred = model.predict(pred)\nmodel.load_weights('BertModel.h5')","e1be47e7":"submission['target'] =pred.round().astype(int)\nsubmission2=submission.to_csv('submission2.csv',index=False)","30029b99":"#for some reason, the tensorboard does not show here. But if you run the code somewhere else e.g., Colab, then you can visualize\n#learning rate and its architecture. \n# Load the extension and start TensorBoard\n%load_ext tensorboard\n%tensorboard --logdir logs","7eae7af1":"The BERT layer requires three input sequences:\n\n* Token ids: for every token in the sentence. In my code, I call it input_ids.\n* Mask ids : BERT replaces 15% of the words in each sequence with a mask token. The model then try to predict the original   value of the masked words, based on the context provided by the other, non-masked, words in the sequence. \n* Segment ids: 0 for one-sentence sequence, 1 for the second sentences in the sequence. In this experimental, we only have\n  one sentence, so the sequence id is 0.\n","f1300cca":"**3.3 Building Bert model **","b9c5c5c7":"The following picture illustrates how to implement the tokenization process in Bert model. ","c511bb7f":"**1. Cleaning dataset**\n\n**1.1 Importing libraries and loading data**","ac651f38":"**2. LSTM model (baseline)**\n\n**2.1 Traing corpus using sentencepiece(BPE) method**","e1021709":"**1.2 Data preprocessing**","401aadbe":"**2.3 Predicting result**","53ead8bd":"**3.4 Predicting result**","aadc23d5":"**3. BERT model**","72aba917":"**3.1 Training corpus using BertWordPieceTokenizer method**\n","48e7de6e":"![test](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/524341\/962346\/Group.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1583153167&Signature=i%2F6udkh5Z6NJb4cUCld04jsFsOZX%2FVx9Z0aD0ofe4Wrm19qS5cOfPXc6qzJqG7ecIBWa9MNyjDjfuqdLqeTSlL%2Bli4QC4G%2FfAI5kX5FmjMRKkHYQp03jQjFVrRZGPLpHNNAi1C35eP8v5KOOt%2FKz1bN4D%2BskPNAXfLLuX9aHjaHdLXd6%2Bq00Xb49CB%2BPxILhWdN6CgFA9JCP7zNKJ4SFNkIZsaDM7Zn9%2BTqtIzs3TWTfrVBilEGYvI1yyRgE49O6tjJrXTc88cw1IHTG8bDkDqvFZpNLiBHzU59CS446aCvVUBdyBERld4K7I%2B3EpNitV6wBLYyjnFkBkHtfD1YDyA%3D%3D)","798b3f46":"In this step, we are going to train the cleaned corpus (only training dataset) using the SentencePieceBPE model under the Huggingface library. Note: sometimes I got run-time errors even without changing my code. I am not sure; however, this is caused by huggingface or Kaggle. Either way, I found that by turning on\/off the accelerator in the settings could solve the problem.  Each user could enjoy 30 hours per week (GPU\/TPU) running time for free.","ce9d1526":"![](data:image\/png;base64,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)","24b1298e":"It seems that I break the Bert performance record, but in the wrong direction. The accuracy rates in Bert model ranged from 0.55 to 0.70. The low accuracy rate might be caused by the small corpus size( less than 10,000 words) I trained. (ref [1](https:\/\/www.researchgate.net\/post\/What_is_the_minimum_corpus_size_for_keyword_and_collocate_testing) & [2](https:\/\/corpus.byu.edu\/size.asp)) Please feel free to let me know your thoughts. \n\nThe unique part of this notebook was the custom language models. Currently, the most common corpus for NLP projects is collected from social media or news in English. When you are able to build custom language models, you can go beyond those limits. In other words, you are free to do a study in any field in different languages (as long as you have enough data :D). For more details, please visit [Huggingface](https:\/\/huggingface.co\/transformers\/main_classes\/tokenizer.html) website. \n\nThanks for Kaggle offering us this learning opportunity. I completed this work in a hard way, but I 've managed seamlessly to connect those tokens to LSTM and Bert models. And I am happy with it. ","e0ffb8af":"**1.4 Collecting texts for training corpus**","8d607758":"This notebook contains three parts:\n1. Data cleaning\n2. Training the corpus from scratch using [Byte Pair Encoding](https:\/\/en.wikipedia.org\/wiki\/Byte_pair_encoding)(bpe) method. Training a customized corpus is recommended, particularly when your texts belong to low-resource language or if you want to extract information from a specialized field such as clinical texts. After that, I trained the data using the [LSTM ](https:\/\/towardsdatascience.com\/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)model. \n3. Training the corpus from scratch using the [Transformer BertWordpiece ](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html)method. Then I trained the data using the KerasBert model. \n","58653931":"**2.2 Splitting train dataset into subtrain and subtest. Training data with LSTM model**","ae12ed80":"![process](http:\/\/jalammar.github.io\/images\/distilBERT\/bert-distilbert-tokenization-2-token-ids.png)\n [source](http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/)","e693f73b":"**1.3 Visualizing dataset** ","f9b77d85":"**3.2 Applying tokenization process**"}}