{"cell_type":{"1a9aed6f":"code","358a46f2":"code","37a38d5e":"code","2d5f87c8":"code","dd5b7c3e":"code","11d55927":"code","0519eec6":"code","76b4a43d":"code","b38910d8":"code","60e3fe87":"code","5ee9b574":"code","91a36cad":"code","612fb0d7":"code","4b387fbf":"code","212c7ab0":"code","c4bfd6bc":"code","f2a0d4cf":"code","47712896":"code","ebdc0df4":"code","3dcf51c2":"code","7c6f7fda":"code","d6ae42a9":"code","2ccb2a41":"code","90627cdd":"code","df66d57d":"code","51620138":"code","a79ca7b2":"code","eaaadf41":"code","d7e6a504":"code","dfeda0e4":"code","9d3dad25":"code","9e95fd93":"code","c51ae637":"code","85a7f738":"code","60ad66db":"code","cd3996fe":"code","d3f3f3d0":"code","cbfccdda":"code","67d220a2":"code","9626fa02":"code","a1d806a4":"code","d3c903d4":"code","e81840f0":"code","d1022145":"code","414dbad0":"code","ea5d4205":"code","dd47638c":"markdown","28e7b1fb":"markdown","7200c700":"markdown","045a2c93":"markdown","286b815b":"markdown","defca0fc":"markdown","38722ed6":"markdown","bdb38e57":"markdown","c95ff9d1":"markdown","4283975d":"markdown","1c5231c2":"markdown","a5a3c939":"markdown","b12c4cab":"markdown","9b97103e":"markdown","14a6f94b":"markdown","74fdd848":"markdown","227361ae":"markdown","cff83f8f":"markdown","4b39b0d9":"markdown","f6d20432":"markdown","e34de9c0":"markdown","d2bab930":"markdown","bc509292":"markdown","9548ee25":"markdown"},"source":{"1a9aed6f":"import pandas as pd\n\ndad_jokes_df=pd.read_csv('..\/input\/reddit-dad-jokes\/reddit_dadjokes.csv')","358a46f2":"dad_jokes_df.head()","37a38d5e":"dad_jokes_df.shape","2d5f87c8":"dad_jokes_df.date=pd.to_datetime(dad_jokes_df.date)\ndad_jokes_df=dad_jokes_df.sort_values('date',ascending=True)\ndad_jokes_df","dd5b7c3e":"dad_jokes_df.drop_duplicates(subset=\"joke\", inplace=True)\ndad_jokes_df=dad_jokes_df.reset_index(drop=True)\ndad_jokes_df.shape","11d55927":"authors=dad_jokes_df.author.value_counts()\nauthors","0519eec6":"authors.drop(\"[deleted]\", inplace=True)","76b4a43d":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\nworldcloud_authors=WordCloud(width=1600, height=800,background_color=\"white\", max_words=100).generate_from_frequencies(authors)\n\nplt.figure(figsize = (20,20), dpi=500)\nplt.imshow(worldcloud_authors, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title(\"100 All Time posters on \/r\/dadjokes\", fontdict={'size': 20,'verticalalignment': 'bottom'})\nplt.show()","b38910d8":"import seaborn as sns\n\nfig, ax = plt.subplots(1,1, figsize=(12,12))\n\nv=sns.scatterplot(x=dad_jokes_df.score.value_counts().index, y=dad_jokes_df.score.value_counts(),size=dad_jokes_df.score.value_counts(),legend=False , sizes=(1, 250),ax=ax)\n\n#v=sns.jointplot(x=dad_jokes_df.score.value_counts().index,y=dad_jokes_df.score.value_counts(),ax=ax)\n\n#v.set_yscale(\"log\")\nv.set_xscale(\"log\")\nv.ticklabel_format(axis=\"y\", style=\"plain\")\nv.set_xlabel(\"Post Karma Points\")\nv.set_ylabel(\"Number of Posts\")\n#v.set_axis_labels(\"Karma Points\",\"Number of Posts\")\nv.set(title=\"Karma Distribution by post\")\n\ndad_jokes_df.score.value_counts()","60e3fe87":"dad_jokes_df.joke.head()","5ee9b574":"dad_jokes_df.joke=dad_jokes_df.joke.str.lower()\ndad_jokes_df.joke=dad_jokes_df.joke.apply(lambda x: \" \".join(x.split()))\ndad_jokes_df.head()","91a36cad":"import nltk\nnltk.download('punkt')","612fb0d7":"from nltk.tokenize import word_tokenize\ndad_jokes_df[\"joke_tokens\"]=dad_jokes_df.joke.apply(lambda x: word_tokenize(x))\n\njokes=dad_jokes_df[[\"joke\",\"joke_tokens\"]].copy()\n\njokes=jokes.explode(\"joke_tokens\")\njokes.head()","4b387fbf":"#Number of Unique Jokes\nprint(len(dad_jokes_df))\n\n#Number of joke tokens(special chars included)\nprint(len(jokes.joke_tokens))\n\ntokens=jokes[[\"joke_tokens\"]].copy()\n\n#Number of unique joke tokens (special chars included)\nprint(len(set(jokes.joke_tokens)))\ntokens.head()","212c7ab0":"import string\npunctuation_list=list(string.punctuation)\n\ntokens=tokens[~tokens.joke_tokens.isin(punctuation_list)]\ntokens=tokens[~tokens.joke_tokens.str.contains(\"http*|\\'s\")]\ntokens=tokens[~tokens.joke_tokens.str.contains(\"\/\/|\\\\_|v=|st=|.*\/\")]\ntokens=tokens[~tokens.joke_tokens.str.contains(\"[\\.|\\'*|\\\"|\u00b4|`|\u201c|\u201d|\u2018|\u2019|\u201a|\u201b|-]+\")]\n\n\nprint(len(tokens))\n#Number wf unique joke tokens without punctuation and without possessive\nprint(len(set(tokens.joke_tokens)))\ntokens.head()","c4bfd6bc":"nltk.download('stopwords')","f2a0d4cf":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\ntokens=tokens[~tokens.joke_tokens.isin(stop_words)]\n\nprint(len(tokens))\n#Number of unique joke tokens without punctuation and without possessive\nprint(len(set(tokens.joke_tokens)))","47712896":"v=tokens.joke_tokens.value_counts()\nv=v[v.index.str.isalpha()].copy()\nv","ebdc0df4":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\n\nworldcloud_authors=WordCloud(width=1600, height=800,background_color=\"white\", max_words=200).generate_from_frequencies(v)\n\nplt.figure(figsize = (20,20), dpi=200)\nplt.imshow(worldcloud_authors, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title(\"200 Most Common words on \/r\/dadjokes\", fontdict={'size': 20,'verticalalignment': 'bottom'})\nplt.show()","3dcf51c2":"words=tokens[tokens.joke_tokens.str.isalpha()].copy()\nprint(len(words))\n#Number of unique joke tokens without punctuation and without possessive\nprint(len(set(words.joke_tokens)))","7c6f7fda":"from nltk.stem import SnowballStemmer,PorterStemmer\n\nsnowball = SnowballStemmer(language='english')\n#porter = PorterStemmer()\n\nsnowball_words=porter_words=words.copy()\nsnowball_words[\"joke_tokens\"]=words.joke_tokens.apply(lambda x: snowball.stem(x))\n#porter_words[\"joke_tokens\"]=words.joke_tokens.apply(lambda x: porter.stem(x))","d6ae42a9":"s=snowball_words.joke_tokens.value_counts()\n#s","2ccb2a41":"#p=porter_words.joke_tokens.value_counts()\n#p","90627cdd":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\n\nworldcloud_authors=WordCloud(width=1600, height=800,background_color=\"white\", max_words=100).generate_from_frequencies(s)\n\nplt.figure(figsize = (20,20), dpi=200)\nplt.imshow(worldcloud_authors, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title(\"Snowball Steaming\", fontdict={'size': 20,'verticalalignment': 'bottom'})\nplt.show()","df66d57d":"nltk.download('wordnet')\nnltk.download('omw-1.4')\nfrom nltk.stem import WordNetLemmatizer\n\nword_lema = WordNetLemmatizer()\n\nlemma_words=words.copy()\nlemma_words[\"joke_tokens\"]=words.joke_tokens.apply(lambda x: word_lema.lemmatize(x))\n\nl=lemma_words.joke_tokens.value_counts()\nl","51620138":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\n\nworldcloud_authors=WordCloud(width=1600, height=800,background_color=\"white\", max_words=100).generate_from_frequencies(l)\n\nplt.figure(figsize = (20,20), dpi=200)\nplt.imshow(worldcloud_authors, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title(\"100th most frequent terms via Word Lemmatization\", fontdict={'size': 20,'verticalalignment': 'bottom'})\nplt.show()","a79ca7b2":"#Tokens as they were before the cleasing and treatment\n#original_tokenization=dad_jokes_df[[\"joke\",\"joke_tokens\"]].copy()\nnltk.download('averaged_perceptron_tagger')\n\n#### The original tokenization was being for comparison purposes. Given the existence of little computing power, this comparison was avoided enabling faster run times\n#Tagging using nltk post_tag function\n#original_tokenization[\"tags\"]=original_tokenization.joke_tokens.apply(lambda x: nltk.pos_tag(x))\n#original_tokenization.head()","eaaadf41":"important_words=words[[\"joke_tokens\"]].copy()\nimportant_words=important_words.groupby(important_words.index).joke_tokens.agg(list).to_frame()\nimportant_words[\"tags\"]=important_words.joke_tokens.apply(lambda x: nltk.pos_tag(x))\n#important_words.head()","d7e6a504":"nltk.download('maxent_ne_chunker')\nnltk.download('words')","dfeda0e4":"important_words[\"chunks\"]=important_words.tags.apply(lambda x: nltk.ne_chunk(x))\n#important_words.head()","9d3dad25":"#### The original tokenization was being for comparison purposes. Given the existence of little computing power, this comparison was avoided enabling faster run times\n#original_tokenization[\"chunks\"]=original_tokenization.tags.apply(lambda x: nltk.ne_chunk(x))\n#original_tokenization.head()","9e95fd93":"#Name chunking \n\n## More information about the grammatic elements: https:\/\/towardsdatascience.com\/text-cleaning-methods-for-natural-language-processing-f2fc1796e8c7\ngrammar = \"NP: {<DT>?<JJ>*<NN>}\"\ncp = nltk.RegexpParser(grammar)\nimportant_words[\"Noun_Phrase_Chunker\"]=important_words[\"tags\"].apply(lambda x: cp.parse(x))","c51ae637":"\n#original_tokenization[\"Noun_Phrase_Chunker\"]=original_tokenization[\"tags\"].apply(lambda x: cp.parse(x))","85a7f738":"result=important_words.Noun_Phrase_Chunker[0]\ntype(result)\n\n#https:\/\/stackoverflow.com\/questions\/49478228\/tclerror-no-display-name-and-no-display-environment-variable-in-google-colab\n\n### CREATE VIRTUAL DISPLAY ###\n!apt-get install -y xvfb # Install X Virtual Frame Buffer\nimport os\nos.system('Xvfb :1 -screen 0 1600x1200x16  &')    # create virtual display with size 1600x1200 and 16 bit color. Color can be changed to 24 or 8\nos.environ['DISPLAY']=':1.0'\n\n### INSTALL GHOSTSCRIPT (Required to display NLTK trees) ###\n!apt install -y ghostscript python3-tk\n\n\nimport matplotlib\nmatplotlib.use('Agg')\n%matplotlib inline\nimport IPython\nIPython.core.display.display(result)","60ad66db":"#### The original tokenization was being for comparison purposes. Given the existence of little computing power, this comparison was avoided enabling faster run times\n\n#result=original_tokenization.Noun_Phrase_Chunker[1]\n#IPython.core.display.display(result)","cd3996fe":"important_words[\"chunk_labels\"]=important_words[\"chunks\"].apply(lambda x: x.label() if hasattr(x, 'label') else [])\nimportant_words[\"chunk_labels\"].value_counts()","d3f3f3d0":"#### The original tokenization was being for comparison purposes. Given the existence of little computing power, this comparison was avoided enabling faster run times\n\n#original_tokenization[\"chunk_labels\"]=original_tokenization[\"chunks\"].apply(lambda x: x.label() if hasattr(x, 'label') else [])\n#original_tokenization[\"chunk_labels\"].value_counts()","cbfccdda":"#import spacy\n#nlp = spacy.load(\"en_core_web_sm\")#_core_sci_lg\n#jokes[\"keyword\"]=jokes.joke.apply(lambda x: nlp(x))","67d220a2":"import gensim\n#from nltk.corpus import brown\n#model = gensim.models.Word2Vec(brown.sents())\n\n#model.save('brown.embedding')\n#new_model = gensim.models.Word2Vec.load('brown.embedding')\nimportant_words.reset_index(drop=True)\n#len(new_model.wv['university'])\n#print(new_model.wv['university'])","9626fa02":"data=[]\nfor x in range(0,4927): #important_words.shape[0]\n    data.append(important_words.joke_tokens[x])\n#print(data)\n#Countinuous Bag of words\nmodel = gensim.models.Word2Vec(data, vector_size=200, window=20, min_count=200, workers=4)\nmodel.save('reddit.embedding')\nmodel_reddit = gensim.models.Word2Vec.load('reddit.embedding')\n\n#Skip-gram\nmodel1 = gensim.models.Word2Vec(data, vector_size=200, window=20, min_count=200, workers=4,sg=1)\nmodel1.save('redditsg.embedding')\nmodel_reddit_sg = gensim.models.Word2Vec.load('redditsg.embedding')\n","a1d806a4":"#print(model.wv.key_to_index)\n#print(\"\\n\")\n#print(model1.wv.key_to_index)\n\nX = model.wv.get_normed_vectors()\nX1= model1.wv.get_normed_vectors()\n\nprint(model.wv.most_similar('joke'))\nprint(model.wv.most_similar('dad'))\nprint(\"\\n\")\nprint(model1.wv.most_similar('joke'))\nprint(model1.wv.most_similar('dad'))","d3c903d4":"##### Similarity Comparison(Most similar by word)","e81840f0":"print(model.wv.similar_by_word('say'))\nprint(model1.wv.similar_by_word('say'))","d1022145":"#For the visualization of the word embeddings i decided to reuse the code by jeffd23 on `https:\/\/www.kaggle.com\/jeffd23\/visualizing-word-vectors-with-t-sne as it was a resource that brought me some ideas on this subject`\n# adapted from https:\/\/www.kaggle.com\/jeffd23\/visualizing-word-vectors-with-t-sne?scriptVersionId=1018109&cellId=7\nimport numpy as np\nfrom sklearn.manifold import TSNE\n\ndef tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    tokens = np.asarray(model.wv.vectors)\n    labels = np.asarray(model.wv.index_to_key)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","414dbad0":"#Countinuous Bag of words\ntsne_plot(model)\n","ea5d4205":"#Skip-gram\ntsne_plot(model1)","dd47638c":"#### Stemming\n\nAfter cleaning most of the irrelevant tokens, next will reduce the complexity of the tokens transforming the words into their root forms.\nThe decision took in account the information provided by:\n`https:\/\/machinelearningknowledge.ai\/beginners-guide-to-stemming-in-python-nltk\/`","28e7b1fb":"##### Similarity Comparison(Most similar)","7200c700":"#### Natural Language Toolkit\n\nWe will now make use of several Natural Language processing tools with focus on the `nltk` package to get more information about the jokes present in this dataset.\n`https:\/\/www.nltk.org\/`\n","045a2c93":"As can be seen, it has been confirmed the dataframe is composed of 5 different collumns: author, url, joke, score and date.\nLet's explore this data a bit more.","286b815b":"We can see 216328 rows which means the dataframe  has 216328 different posts from this subreddit. Let's try to find if there are any duplicate posts.","defca0fc":"The first step is to transform all words to be written in lowercase, followed by the cleaning up of extra spaces.","38722ed6":"### Representation Models\nThis part of the work was the most challenging given the existicence of named entities or events in the dataset. Therefore a simple study of word embeddings was done in this part.\n\n#### Word Embedding\n- https:\/\/radimrehurek.com\/gensim\/auto_examples\/tutorials\/run_word2vec.html\n- https:\/\/medium.com\/@keisukeumezawa\/word2vec-obtain-word-embeddings-885716a56270\n- https:\/\/machinelearningmastery.com\/what-are-word-embeddings\/\n- https:\/\/www.analyticsvidhya.com\/blog\/2021\/07\/word2vec-for-word-embeddings-a-beginners-guide\/","bdb38e57":"#### Lemmatization\nIt will be now tried another way of reducing the complexity of the words tokenized beforehand  looking for a lemma. For that it will be still be used the `nltk` lib. Following the advice from:\n`https:\/\/towardsdatascience.com\/text-cleaning-methods-for-natural-language-processing-f2fc1796e8c7`","c95ff9d1":"#### Named Entity Recognition\n\nUnfortunately the dataset that was chosen has not elements that can be understood as named entities, as can be seen below. This fact was only perceived now.","4283975d":"## Exploring the data\n\nThe first step we should do is to load up the data to a dataframe and check the type of data available by looking at the first 5 rows of the Dataframe\n","1c5231c2":"Now let's look to the actual jokes karma score:","a5a3c939":"##### Plotting the models","b12c4cab":"More information about the grammatic elements:\n- https:\/\/towardsdatascience.com\/text-cleaning-methods-for-natural-language-processing-f2fc1796e8c7\n- https:\/\/medium.com\/@jeffysam02\/making-sense-of-grammar-using-nltk-part-7-81afcccfc5a4","9b97103e":"As can be seen we have several users which deleted their accounts after posting and as such the number of deleted users is impressive. A visualisation of this list was generated using a Wordcloud visual representation and removing these `[deleted]` users","14a6f94b":"#### Chunking\nLet's now focus on the chunking of the data. A more detailed process on how different types of chunking can be applied can be found at(`https:\/\/www.nltk.org\/book\/ch07.html`)","74fdd848":"#### Clean up the data\n\nWe will now filter the tokens for the existence of punctuation, possessive case and other random values that don't add up information into this job","227361ae":"# Reddit Dad Jokes\n\n## Information\n\n### Warning \nThis is my first attempt of text mininig. Feel free to comment and give me some tips :D. \n\n### Dataset Context\n\nThis dataset was obtained on `Kaggle`. It was uploaded by [Oktay Ozturk(oktayozturk010)](https:\/\/www.kaggle.com\/oktayozturk010) and it used data scrapped from [Reddit r\/dadjokes subreddit](https:\/\/reddit.com\/r\/dadjokes).\n\nThe dataset was available in `29-01-2022` at `https:\/\/www.kaggle.com\/oktayozturk010\/reddit-dad-jokes`\n\nThe raw data from this dataset as presented by the author:\n\n>The dataset contains four columns which are;\n>\n> - author: The username of the person who made the joke\n> - url: The Post\/Joke's URL\n> - joke: The joke\n> - score: As a scoring result of Reddit's number of upvotes minus the number of downvotes\n> - date: The date of the post\n\n","cff83f8f":"#### Filter out Stop Words\nLet's now remove the english stop words from the current dataframe","4b39b0d9":"### Identification and Extraction\n\nAfter tagging the tokens it's now time to identify the elements in the jokes and extract some features out of them","f6d20432":"##### Initialization","e34de9c0":"The graphic above shows that normally the \/r\/dadjokes community has being awarding little karma to most of the posts on the sub.\nLet's now focus on the actual dad jokes column of the Dataframe and apply the Text Mining process to study the data.\n","d2bab930":"As can be seen the amount of records was reduced by 5371 jokes as they were repeated. Let's find more statistics about the data.\n\nAfter cleaning the filtered posts, we can find out which are the users with more posts in this community.\n","bc509292":"#### Part of speech tagging (POS)\n\nNow that with Stemming and Lemmatization done we can carry on to POS for that it will be used the nltk libray again.\n","9548ee25":"## Text Mining Process\n\n### Pre-processing of the data\n\nThe first step to take is to analyse the sentences and cleaning them up for being used in the future."}}