{"cell_type":{"2f6cc86c":"code","9194d779":"code","5e5f8727":"code","da0ce041":"code","0a22edbc":"code","347fbf86":"code","0358ef4d":"code","0382a96a":"code","1c1cdce5":"code","4cb41fe9":"code","965ba1e0":"code","45886879":"code","33fa77db":"code","e8585790":"code","fb751e44":"code","ce98e966":"code","baa97b58":"code","64305d87":"code","1920e374":"code","505f873c":"code","f4ce7415":"code","c870efa8":"code","aec50547":"code","eba7a449":"code","410993df":"code","871a4e5f":"code","59bf83da":"code","25e0b65e":"code","de6cfa6b":"code","6b8c2652":"code","51e1838d":"code","f6f5831d":"code","1b7e10bf":"code","93ec8786":"code","e5c871a3":"code","ffcbefc0":"code","7a77d301":"code","e81a3ae1":"code","7dc9f8d3":"code","25765f9b":"code","6e120721":"code","6527942e":"code","2998c089":"code","9a761dc7":"code","533e2f9b":"code","3a8cc288":"code","8ab90379":"code","a9c08f8c":"markdown","1607354d":"markdown","c3efe4da":"markdown","90b53fd4":"markdown","c3607b99":"markdown","74fafd9e":"markdown","4d31dd58":"markdown","c13ed892":"markdown","b838224a":"markdown","c96ca875":"markdown","ee35913c":"markdown","66712bb6":"markdown","99295a61":"markdown","b0547cf1":"markdown","b630807b":"markdown","57df2892":"markdown","1d3fdd7a":"markdown","c6dd0cf2":"markdown","938fac3c":"markdown","d8708e96":"markdown","8b7fdc61":"markdown","861cb446":"markdown","660e6ca7":"markdown","570077be":"markdown","2932b231":"markdown","34f9f4a3":"markdown","c9439b1b":"markdown","793409c1":"markdown","329c643d":"markdown","1033c1e9":"markdown","d0f4e333":"markdown","ff695dd5":"markdown","0af20d34":"markdown","90559b38":"markdown","4e491a9a":"markdown","6df6e19b":"markdown","98135a64":"markdown","0b6d0491":"markdown","aeb94583":"markdown","d6ae46b5":"markdown","fc44758f":"markdown"},"source":{"2f6cc86c":"import pandas as pd\nimport datetime as dt\nimport numpy as np\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom  scipy.stats import skew, kurtosis, shapiro","9194d779":"path = \"\/kaggle\/input\/spain_energy_market.csv\"\ndata = pd.read_csv(path, sep=\",\", parse_dates=[\"datetime\"])\ndata = data[data[\"name\"]==\"Demanda programada PBF total\"]#.set_index(\"datetime\")\ndata[\"date\"] = data[\"datetime\"].dt.date\ndata.set_index(\"date\", inplace=True)\ndata = data[[\"value\"]]\ndata = data.asfreq(\"D\")\ndata = data.rename(columns={\"value\": \"energy\"})\ndata.info()          ","5e5f8727":"data[:5]","da0ce041":"data.plot(title=\"Energy Demand\")\nplt.ylabel(\"MWh\")\nplt.show()","0a22edbc":"len(pd.date_range(start=\"2014-01-01\", end=\"2018-12-31\"))","347fbf86":"data[\"year\"] = data.index.year\ndata[\"qtr\"] = data.index.quarter\ndata[\"mon\"] = data.index.month\ndata[\"week\"] = data.index.week\ndata[\"day\"] = data.index.weekday\ndata[\"ix\"] = range(0,len(data))\ndata[[\"movave_7\", \"movstd_7\"]] = data.energy.rolling(7).agg([np.mean, np.std])\ndata[[\"movave_30\", \"movstd_30\"]] = data.energy.rolling(30).agg([np.mean, np.std])\ndata[[\"movave_90\", \"movstd_90\"]] = data.energy.rolling(90).agg([np.mean, np.std])\ndata[[\"movave_365\", \"movstd_365\"]] = data.energy.rolling(365).agg([np.mean, np.std])\n\nplt.figure(figsize=(20,16))\ndata[[\"energy\", \"movave_7\"]].plot(title=\"Daily Energy Demand in Spain (MWh)\")\nplt.ylabel(\"(MWh)\")\nplt.show()","0358ef4d":"mean = np.mean(data.energy.values)\nstd = np.std(data.energy.values)\nskew = skew(data.energy.values)\nex_kurt = kurtosis(data.energy)\nprint(\"Skewness: {} \\nKurtosis: {}\".format(skew, ex_kurt+3))","0382a96a":"def shapiro_test(data, alpha=0.05):\n    stat, pval = shapiro(data)\n    print(\"H0: Data was drawn from a Normal Ditribution\")\n    if (pval<alpha):\n        print(\"pval {} is lower than significance level: {}, therefore null hypothesis is rejected\".format(pval, alpha))\n    else:\n        print(\"pval {} is higher than significance level: {}, therefore null hypothesis cannot be rejected\".format(pval, alpha))\n        \nshapiro_test(data.energy, alpha=0.05)","1c1cdce5":"sns.distplot(data.energy)\nplt.title(\"Target Analysis\")\nplt.xticks(rotation=45)\nplt.xlabel(\"(MWh)\")\nplt.axvline(x=mean, color='r', linestyle='-', label=\"\\mu: {0:.2f}%\".format(mean))\nplt.axvline(x=mean+2*std, color='orange', linestyle='-')\nplt.axvline(x=mean-2*std, color='orange', linestyle='-')\nplt.show()","4cb41fe9":"# Insert the rolling quantiles to the monthly returns\ndata_rolling = data.energy.rolling(window=90)\ndata['q10'] = data_rolling.quantile(0.1).to_frame(\"q10\")\ndata['q50'] = data_rolling.quantile(0.5).to_frame(\"q50\")\ndata['q90'] = data_rolling.quantile(0.9).to_frame(\"q90\")\n\ndata[[\"q10\", \"q50\", \"q90\"]].plot(title=\"Volatility Analysis: 90-rolling percentiles\")\nplt.ylabel(\"(MWh)\")\nplt.show()","965ba1e0":"data.groupby(\"qtr\")[\"energy\"].std().divide(data.groupby(\"qtr\")[\"energy\"].mean()).plot(kind=\"bar\")\nplt.title(\"Coefficient of Variation (CV) by qtr\")\nplt.show()","45886879":"data.groupby(\"mon\")[\"energy\"].std().divide(data.groupby(\"mon\")[\"energy\"].mean()).plot(kind=\"bar\")\nplt.title(\"Coefficient of Variation (CV) by month\")\nplt.show()","33fa77db":"data[[\"movstd_30\", \"movstd_365\"]].plot(title=\"Heteroscedasticity analysis\")\nplt.ylabel(\"(MWh)\")\nplt.show()","e8585790":"data[[\"movave_30\", \"movave_90\"]].plot(title=\"Seasonal Analysis: Moving Averages\")\nplt.ylabel(\"(MWh)\")\nplt.show()","fb751e44":"sns.boxplot(data=data, x=\"qtr\", y=\"energy\")\nplt.title(\"Seasonality analysis: Distribution over quaters\")\nplt.ylabel(\"(MWh)\")\nplt.show()","ce98e966":"sns.boxplot(data=data, x=\"day\", y=\"energy\")\nplt.title(\"Seasonality analysis: Distribution over weekdays\")\nplt.ylabel(\"(MWh)\")\nplt.show()","baa97b58":"data_mon = data.energy.resample(\"M\").agg(sum).to_frame(\"energy\")\ndata_mon[\"ix\"] = range(0, len(data_mon))\ndata_mon[:5]","64305d87":"sns.regplot(data=data_mon,x=\"ix\", y=\"energy\")\nplt.title(\"Trend analysis: Regression\")\nplt.ylabel(\"(MWh)\")\nplt.xlabel(\"\")\nplt.show()","1920e374":"sns.boxplot(data=data[\"2014\":\"2017\"], x=\"year\", y=\"energy\")\nplt.title(\"Trend Analysis: Annual Box-plot Distribution\")\nplt.ylabel(\"(MWh)\")\nplt.show()","505f873c":"data[\"target\"] = data.energy.add(-mean).div(std)\nsns.distplot(data[\"target\"])\nplt.show()","f4ce7415":"features = []\ncorr_features=[]\ntargets = []\ntau = 30 #forecasting periods\n\nfor t in range(1, tau+1):\n    data[\"target_t\" + str(t)] = data.target.shift(-t)\n    targets.append(\"target_t\" + str(t))\n    \nfor t in range(1,31):\n    data[\"feat_ar\" + str(t)] = data.target.shift(t)\n    #data[\"feat_ar\" + str(t) + \"_lag1y\"] = data.target.shift(350)\n    features.append(\"feat_ar\" + str(t))\n    #corr_features.append(\"feat_ar\" + str(t))\n    #features.append(\"feat_ar\" + str(t) + \"_lag1y\")\n        \n    \nfor t in [7, 14, 30]:\n    data[[\"feat_movave\" + str(t), \"feat_movstd\" + str(t), \"feat_movmin\" + str(t) ,\"feat_movmax\" + str(t)]] = data.energy.rolling(t).agg([np.mean, np.std, np.max, np.min])\n    features.append(\"feat_movave\" + str(t))\n    #corr_features.append(\"feat_movave\" + str(t))\n    features.append(\"feat_movstd\" + str(t))\n    features.append(\"feat_movmin\" + str(t))\n    features.append(\"feat_movmax\" + str(t))\n    \nmonths = pd.get_dummies(data.mon,\n                              prefix=\"mon\",\n                              drop_first=True)\nmonths.index = data.index\ndata = pd.concat([data, months], axis=1)\n\ndays = pd.get_dummies(data.day,\n                              prefix=\"day\",\n                              drop_first=True)\ndays.index = data.index\ndata = pd.concat([data, days], axis=1)\n\n\nfeatures = features + months.columns.values.tolist() + days.columns.values.tolist()","c870efa8":"corr_features = [\"feat_ar1\", \"feat_ar2\", \"feat_ar3\", \"feat_ar4\", \"feat_ar5\", \"feat_ar6\", \"feat_ar7\", \"feat_movave7\", \"feat_movave14\", \"feat_movave30\"]","aec50547":"# Calculate correlation matrix\ncorr = data[[\"target_t1\"] + corr_features].corr()\n\ntop5_mostCorrFeats = corr[\"target_t1\"].apply(abs).sort_values(ascending=False).index.values[:6]\n\n\n# Plot heatmap of correlation matrix\nsns.heatmap(corr, annot=True)\nplt.title(\"Pearson Correlation with 1 period target\")\nplt.yticks(rotation=0); plt.xticks(rotation=90)  # fix ticklabel directions\nplt.tight_layout()  # fits plot area to the plot, \"tightly\"\nplt.show()  # show the plot","eba7a449":"sns.pairplot(data=data[top5_mostCorrFeats].dropna(), kind=\"reg\")\nplt.title(\"Most important features Matrix Scatter Plot\")\nplt.show()","410993df":"data_feateng = data[features + targets].dropna()\nnobs= len(data_feateng)\nprint(\"Number of observations: \", nobs)","871a4e5f":"\nX_train = data_feateng.loc[\"2014\":\"2017\"][features]\ny_train = data_feateng.loc[\"2014\":\"2017\"][targets]\n\nX_test = data_feateng.loc[\"2018\"][features]\ny_test = data_feateng.loc[\"2018\"][targets]\n\nn, k = X_train.shape\nprint(\"Total number of observations: \", nobs)\nprint(\"Train: {}{}, \\nTest: {}{}\".format(X_train.shape, y_train.shape,\n                                              X_test.shape, y_test.shape))\n\nplt.plot(y_train.index, y_train.target_t1.values, label=\"train\")\nplt.plot(y_test.index, y_test.target_t1.values, label=\"test\")\nplt.title(\"Train\/Test split\")\nplt.xticks(rotation=45)\nplt.show()","59bf83da":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nreg = LinearRegression().fit(X_train, y_train[\"target_t1\"])\np_train = reg.predict(X_train)\np_test = reg.predict(X_test)\n\nRMSE_train = np.sqrt(mean_squared_error(y_train[\"target_t1\"], p_train))\nRMSE_test = np.sqrt(mean_squared_error(y_test[\"target_t1\"], p_test))\n\nprint(\"Train RMSE: {}\\nTest RMSE: {}\".format(RMSE_train, RMSE_test) )","25e0b65e":"from sklearn.model_selection import TimeSeriesSplit, ParameterGrid\n\nsplits = TimeSeriesSplit(n_splits=3, max_train_size=365*2)\nfor train_index, val_index in splits.split(X_train):\n    print(\"TRAIN:\", len(train_index), \"TEST:\", len(val_index))\n    y_train[\"target_t1\"][train_index].plot()\n    y_train[\"target_t1\"][val_index].plot()\n    plt.show()\n   ","de6cfa6b":"from sklearn.ensemble import RandomForestRegressor\n\nsplits = TimeSeriesSplit(n_splits=3, max_train_size=365*2)\nrfr = RandomForestRegressor()\n# Create a dictionary of hyperparameters to search\nrfr_grid = {\"n_estimators\": [500], \n        'max_depth': [3, 5, 10, 20, 30], \n        'max_features': [4, 8, 16, 32, 59], \n        'random_state': [123]}\nrfr_paramGrid = ParameterGrid(rfr_grid)\n\ndef TimeSplit_ModBuild(model, paramGrid, splits, X, y):\n    from sklearn.model_selection import TimeSeriesSplit\n    from sklearn.metrics import mean_squared_error\n\n    #Loop over each time split and for each\n    for train_index, val_index in splits.split(X_train):\n        _X_train_ = X.iloc[train_index]\n        _y_train_ = y.iloc[train_index]\n        _X_val_ = X.iloc[val_index]\n        _y_val_ = y.iloc[val_index]\n\n        train_scores = []\n        val_scores = []\n        #models = []\n        \n        # Loop through the parameter grid, set the hyperparameters, and save the scores\n        for g in paramGrid:\n            model.set_params(**g)\n            model.fit(_X_train_, _y_train_)\n            p_train = model.predict(_X_train_)\n            p_val = model.predict(_X_val_)\n            score_train = np.mean(mean_squared_error(_y_train_, p_train))\n            score_val = np.mean(mean_squared_error(_y_val_, p_val))\n            train_scores.append(score_train)\n            val_scores.append(score_val)\n            #models.append(model)\n            best_idx = np.argmin(val_scores)\n            \n        print(\"Best-Fold HyperParams:: \", paramGrid[best_idx])\n        print(\"Best-Fold Train RMSE: \", train_scores[best_idx])\n        print(\"Best-Fold Val RMSE: \",val_scores[best_idx])\n        print(\"\\n\")\n        \n    #Return most recent model\n    return train_scores, val_scores, best_idx\n\n\nCV_rfr_tup = TimeSplit_ModBuild(rfr, rfr_paramGrid, splits, X_train, y_train[\"target_t1\"])","6b8c2652":"best_rfr_idx = CV_rfr_tup[2]\nbest_rfr_grid = rfr_paramGrid[best_rfr_idx]\nbest_rfr = RandomForestRegressor().set_params(**best_rfr_grid).\\\n    fit(X_train.loc[\"2016\":\"2017\"], y_train.loc[\"2016\":\"2017\", \"target_t1\"])\n","51e1838d":"# Get feature importances from our random forest model\nimportances = best_rfr.feature_importances_\n\n# Get the index of importances from greatest importance to least\nsorted_index = np.argsort(importances)[::-1]\nsorted_index_top = sorted_index[:10]\nx = range(len(sorted_index_top))\n\n# Create tick labels \nlabels = np.array(features)[sorted_index_top]\nplt.bar(x, importances[sorted_index_top], tick_label=labels)\nplt.title(\"Feature importance analyisis\")\n# Rotate tick labels to vertical\nplt.xticks(rotation=45)\nplt.show()","f6f5831d":"p_train = best_rfr.predict(X_train)\ntrain_resid_1step = y_train[\"target_t1\"]- p_train\n\np_test = best_rfr.predict(X_test)\ntest_resid_1step = y_test[\"target_t1\"]- p_test","1b7e10bf":"test_df = y_test[[\"target_t1\"]]*std+mean\ntest_df[\"pred_t1\"] = p_test*std+mean\ntest_df[\"resid_t1\"] = test_df[\"target_t1\"].add(-test_df[\"pred_t1\"])\ntest_df[\"abs_resid_t1\"] = abs(test_df[\"resid_t1\"])\ntest_df[\"ape_t1\"] = test_df[\"resid_t1\"].div(test_df[\"target_t1\"])\n\ntest_MAPE = test_df[\"ape_t1\"].mean()*100\nprint(\"1-period ahead forecasting MAPE: \", test_MAPE)","93ec8786":"test_df[[\"target_t1\", \"pred_t1\"]].plot()\n\nplt.title(\"1-period ahead Forecasting\")\nplt.ylabel(\"(MWh)\")\nplt.legend()\nplt.show()","e5c871a3":"plt.scatter(y=y_train[\"target_t1\"],x=p_train, label=\"train\")\nplt.scatter(y=y_test[\"target_t1\"],x=p_test, label=\"test\")\nplt.title(\"1-period ahead Actual vs forecasting \")\nplt.ylabel(\"Actual\")\nplt.xlabel(\"Forecast\")\nplt.legend()\nplt.show()","ffcbefc0":"test_resid_1step.plot.hist(bins=10, title=\"Test 1-step ahead residuals distribution\")\nplt.xlabel(\"Residuals\")\nplt.show()","7a77d301":"test_resid_1step.plot(title=\"Test 1-step ahead residuals time series\")\nplt.ylabel(\"Residuals\")\nplt.show()","e81a3ae1":"plt.scatter(x=y_test[\"target_t1\"].values, y=test_resid_1step.values)\nplt.title(\"Test 1-step ahead residuals vs Actual values\")\nplt.ylabel(\"Residuals\")\nplt.xlabel(\"Actual values\")\nplt.show()","7dc9f8d3":"multi_rfr = RandomForestRegressor().set_params(**best_rfr_grid).\\\n    fit(X_train.loc[\"2016\":\"2017\"], y_train.loc[\"2016\":\"2017\"])","25765f9b":"p_train = multi_rfr.predict(X_train)\ntrain_resid_1step = y_train- p_train\n\np_test = multi_rfr.predict(X_test)\ntest_resid_1step = y_test- p_test","6e120721":"periods = [1, 7, 14, 30]\n\nytest_df = y_test*std+mean\nptest_df = pd.DataFrame(data=p_test*std+mean, index=test_df.index, columns=[\"pred_t\" + str(i) for i in range(1, 31)])\ntest_df = pd.concat([ytest_df, ptest_df], axis=1)\n\ntest_MAPE = []\n\nfor t in periods:\n    test_df[\"resid_t\" + str(t)] = test_df[\"target_t\" + str(t)].add(-test_df[\"pred_t\" + str(t)])\n    test_df[\"abs_resid_t\" + str(t)] = abs(test_df[\"resid_t\" + str(t)])\n    test_df[\"ape_t\" + str(t)] = test_df[\"abs_resid_t\" + str(t)].div(test_df[\"target_t\" + str(t)])\n    test_MAPE.append(round(test_df[\"ape_t\" + str(t)].mean(), 4)*100)\n\nprint(\"MAPE test: \", test_MAPE)","6527942e":"mape_df = pd.DataFrame(index=periods, data={\"test_MAPE\": test_MAPE})\nmape_df.plot(kind=\"bar\", legend=False)\nplt.title(\"Mean Absolute Percent Error in Test\")\nplt.xlabel(\"Forecasting Period\")\nplt.ylabel(\"%\")\nplt.xticks(rotation=0)\nplt.show()","2998c089":"#f, ax = plt.subplots(nrows=3,ncols=2)\nfor t in periods:\n    test_df[[\"target_t\" + str(t), \"pred_t\" + str(t)]].plot(x=\"pred_t\" + str(t), y=\"target_t\" + str(t) ,kind=\"scatter\")\n    plt.title(\"{}-period(s) ahead forecasting\".format(t))\n    plt.xlabel(\"Forecasted (MWh)\")\n    plt.ylabel(\"Actual values (MWh)\")\n    plt.xticks(rotation=45)\n    plt.show()","9a761dc7":"test_df.index = test_df.index.date","533e2f9b":"forecast_range = pd.date_range(start=np.max(test_df.index.values), periods=tau, freq=\"D\")\nlen(forecast_range)","3a8cc288":"forecast = []\nfor t in range(0, tau):\n    #print(-(t+1), (t))\n    forecast = p_test[-(t+1):,(t)]*std+mean","8ab90379":"test_df[\"target_t1\"].plot()\nplt.scatter(x=test_df.index, y=test_df[\"pred_t1\"], c=\"r\", alpha=0.2, label=\"test preds\")\nplt.plot(forecast_range, forecast, c=\"r\", alpha=0.5, label=\"forecasting\")\nplt.ylabel(\"(MWh)\")\nplt.xticks(rotation=45)\nplt.title(\"Forecasting Daily Electricity Consumption (MWh) in Spanish Market (2018)\")\nplt.show()","a9c08f8c":"### Split Data\nData points in 2018 are used as a holdout data to perform a honest assessment of model performance and carry out some residual analysis (6 months)","1607354d":"### Residual Analysis","c3efe4da":"## Model Building","90b53fd4":"### Baseline Model: Linear Regression","c3607b99":"In this step, two candidates models are build using an nice feature in Scikit-Learn such us MultiOutput Regression, it provides a framework to automatically and easily fit models to predict several target variables.\n(https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.multioutput.MultiOutputRegressor.html)\n\nFirst a baseline model (linear regression) will be fit and compared to a more advanced model, like Random Forest. \nA linear model does not need hyperparamenter tunning, and there is some correlation in data, so it is a strongh foundation, but there are several caveats:\n* Target variable is not perfectly normally distributed with constant variance\n* There are a lot of multicollinearity among predictors\n* Observations are not independent\n\nOn the other hand an advanced model, like Random Forest, needs to perform hyperparamenter tunning, tipically it is solved by using GridSearch and Cross Validation, but time series data is not suitable to be used in CV, because **data is shuffled in order to build k-folds**. On the other hand, Scikit-Learng provide us with a nice solution: TimeSeries Splits, that respect time structure of date and iterativelly allow us to perform GridSearch\n","74fafd9e":"MAPE is slightly over 10%, considering that electricity demand is highly dependend on weather, and February was exceptionally cold, it is an astonishing result. However, can it be improved? Keep reading :)","4d31dd58":"* Negative Skewness: Data distribution is not completelly simetric and has a left tail\n* Kurtosis below 3: It means that tails are slightly thinner than in a Normal distribution. It is said that the distribution is platykurtic and the chance of finding extre values is lower than in a normal distribution.","c13ed892":"Broadly speaking, data does not look like a normal distribution, because it has a small left tail and the chance of observing extreme values is smaller, comparing to normally distributed data","b838224a":"Plotting actual vs forecasted provides a glance on how good model can fit train data and generalize to test data","c96ca875":"## Forecasting","ee35913c":"### Import data","66712bb6":"### Performance Metrics: MAPE (Mean Absolute Percent Error)\n\nLastly, it is necesary to test forecasting accuracy in several periods, and assess whether it is stable.","99295a61":"### Train a Random Forest with Time Series Split to tune Hyperparameters","b0547cf1":"Results are not de same as the ones yielded by correlation analysis, showing that complex relationships and interactions may impact model performance. This is a very important point to take into consideration when working with models like ARIMA:","b630807b":"In the short term (quaterly and monthly) volatility is changeable, but in the long term (year window) it is quite stable, therefore, candidate predictors will have to deal with seasonal pattern in variance.","57df2892":"### Performance Metrics: MAPE (Mean Absolute Percent Error)\n\nEven though RMSE is a very common performance metric, MAPE is very suitable to use, and much easer to understand and communicate. Let's use one period ahead model to compute MAPE in test period","1d3fdd7a":"Not surprisingly, forecasting accuacy on 1 period **INCREASES**, more data, not always is better. Moreover, MAPE increases with forecasting horizon, but it seems quite stable.","c6dd0cf2":"Using Random Forest provides an important improvement compared to Linear Regression. A word of caution, as RF are built bootstrapping data, so some time structure may be lost.\n\nhttps:\/\/en.wikipedia.org\/wiki\/Bootstrap_aggregating","938fac3c":"## Model Assessment:","d8708e96":"EDA Objectives:\n* Is target variable normal with constant volatility\n* Is there a seasonal pattern\n* Is there a trend pattern\n\nModel building objectives:\n* Train a Machine Learning model being able to forecast (predict on future data) 30 periods ahead\n* Feature Engineering process is automatic and no assumptions on data statistical properties is required \n* Assessment will be carried out computing MAPE (Mean Absolute Percent Error) for each forecating horizong on a holdout dataset\n\n$$ MAPE =\\frac{1}{N} \\sum_{i=1}^{N}\\frac{|y-\\hat{y}|}{y}$$\n","8b7fdc61":"## EDA: Exploratory Data Analysis\n* Target Analysis\n* Seasonality and Trend\n\nThe goal is to graphically characterize time series data, withouth relying on any (almost) any statistical method like decomposing.","861cb446":"I hope that this has been an interesting LAB, and that you have enjoyed the exercise. Let's point out futher topics:\n* Use a MLP (maybe a Keras NN with several neurons in the output layer and a custom perfomance metric)\n* Use advanced feature engineering and independent variables\n* Implement calendar","660e6ca7":"### Muti-period ahead model building\n\nOnce we know the best hiperparam set, a new instance of the RF model can be fit on the most recent (and relevant) data. Usually, it is considered that at least 2 years are needed in order to produce a long term daily forecast. Let's re-train a family of RF models using MultiOutput Regressor feature.","570077be":"In order to perform a comprehensive exercise, a residual analysis is carried out, in order to verify:\n\n* Is there some skewness in forecasting (tendendy to over or under forecast)?\n    * Verify that residuals are normally distributed\n* Does outliers impact on performance? Is there any information remaining on residuals?\n    * Residual over time\n    * Plot Residual vs Actual","2932b231":"There is a positive linear (or sligthly damped) trend in energy demand, beacause of a steady economic grownth due to recession recovery.","34f9f4a3":"## Feature Engineering\n\nThe challenge now is to create some features in a very automated way that can deal with seasonality, trend and changes in  volatility. The most basic strategy is to use lagged features and rolling window stats, but consider other advanced techniques for further research:\n* Momentum and Mean reversion, like RSI in financial markets (https:\/\/en.wikipedia.org\/wiki\/Relative_strength_index)\n* Sequence minning","c9439b1b":"# Machine Learning Applied to Forecasting: Daily Electricity Production in Spain 2014-2018","793409c1":"The main goal of this lab is to test whether a general and simple approach based on Machine Learning models, can yield good enough results in a complex forecasting problem.\n\nOur objective is to test this methodology to perform 30-ahead daily forecast in electricity demand (Spanish market). It is a very complex time series given that the country is not in a stationary economic cycle (recovering from a economic recession) and industrial production is shifting towards tourism and a flourishing but yet quite small, industrial equipment and machine exportation sector. Despite this positive circumstances, both GDP increase and tourism gronwth rate have stopped going up.\n\nThis comes with changes in electricity consumption patterns that are also affected by energy efficiency improvements and changes in household consumption behavior.\n\nIn addition, some very deep regulation changes where carried out in the last few years (from 2013), that further impact in electricity market, but this impact is more notorious in price time series. Nonetheless, some changes in regulation have triggered a steady switch from regulated tariff to free-market tariff, where a smarter daily consumption profile is encouraged.\n\nThe methodology explained is aimed to be an experiment to demonstrate that even with a very simple, easy to implement approach, (but advanced in Machine Learning expertise) we can success in an advanced forecasting problem (long term daily forecasting, only two years of training data and a complex time series). The main profit to be achieved is to no longer rely on methods that require deep time series analysis and heavy statistical assumptions on data, such as ARIMA.","329c643d":"There are some features that are quite strongly linearly correlated with target, like AR_6 and MOVAVE_7, let's build some models and check this assumption","1033c1e9":"Data is Spanish daily electricity market demand in MWh","d0f4e333":"### Time Series Analysis: Seasonality and Trend","ff695dd5":"No missing values, and 4 years of data to enjoy :)\n\nLet's compute some date features and start the fun part","0af20d34":"### Forecasting 30-days ahead","90559b38":"### Actual vs Forecasted\nAs previously stated, a quick way to assess model fit is to plot actual vs forecasted and take a glance at data scattering","4e491a9a":"### Volatility Analysis","6df6e19b":"### Feature Importance","98135a64":"## Target Analyisis: Normality","0b6d0491":"It can be seen that, as forecasting period is longer, scattering is greater, specially for extreme values","aeb94583":"In this example, TimeSeriesSplit framework is shown, on each fold, train data is closer to the beginning of forecasting period","d6ae46b5":"Data is standardized in order to allow application of models that are sensitive to scale, like neural networks or svm. Remember that distribution shape is maintained, it only changes first and second momentum (mean and standard deviation)","fc44758f":"As expected, there are two clear seasonal pattern in quaterters and weekdays (0 is Monday)"}}