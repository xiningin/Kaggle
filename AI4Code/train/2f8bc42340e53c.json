{"cell_type":{"b93da394":"code","e65e227f":"code","f35ab185":"code","60d92fe7":"code","4541d362":"code","33e0b6ac":"code","5600f06a":"code","c27dae80":"code","1af7c007":"code","adfed544":"code","3c3fd08d":"code","044a9556":"code","52761055":"code","158205b6":"code","5ca092ab":"code","19370b8a":"code","94d64fae":"code","0ef81485":"code","0d1aad31":"code","9602f38d":"code","e7ad27c0":"code","411c710a":"code","fd10560b":"code","7c2b1337":"code","87f0bb42":"code","49404898":"code","0e8266c0":"code","679b71d9":"code","08ceda54":"code","e2f2a641":"code","bb620e88":"code","8b033a22":"markdown","c5b4b6d7":"markdown","e51c7006":"markdown","1ff9408c":"markdown"},"source":{"b93da394":"!pip install -q tensorflow-gpu==2.0.0-beta1 ","e65e227f":"import os\nimport sys\nimport numpy as np \nimport pandas as pd \nimport cv2\nfrom PIL import Image \nimport pathlib\nimport matplotlib.pyplot as plt\nimport xml.etree.ElementTree as ET\nimport tensorflow as tf\nimport multiprocessing as mp\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nassert sys.version_info >= (3, 5), 'Python \u22653.5 required'\nassert tf.__version__ >= '2.0', 'TensorFlow \u22652.0 required' \n\nRANDOM_SEED = 12345\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\nTEST_SIZE = 0.2\nINPUT_IMAGE_SIZE = 224\n\nBATCH_SIZE = 24\nEPOCH = 5","f35ab185":"print(os.listdir(\"..\/input\"))","60d92fe7":"ANNOTATION_DIR = pathlib.Path('..\/input\/annotations\/Annotation\/')\nIMAGES_DIR = pathlib.Path('..\/input\/images\/Images\/')\n\nBREED_DIR = [path for path in IMAGES_DIR.iterdir()]\nBREED_DIR_NAME = [path.name for path in BREED_DIR]\n\nBREED_CODE_TO_NAME = {breed.split('-')[0]: breed.split('-')[1] for breed in BREED_DIR_NAME}\nBREED_NAME_TO_CODE = {v: k for k, v in BREED_CODE_TO_NAME.items()}\n\nBREED_LABEL_TO_CODE = {i: code for i, code in enumerate(BREED_CODE_TO_NAME)}\nBREED_CODE_TO_LABEL = {v: k for k, v in BREED_LABEL_TO_CODE.items()}\n\nBREED_LABEL_TO_NAME = {i: BREED_CODE_TO_NAME[code] for i, code in BREED_LABEL_TO_CODE.items()}\nBREED_NAME_TO_LABEL = {v: k for k, v in BREED_LABEL_TO_NAME.items()}","4541d362":"def path_to_label(path):\n    code = path.stem.split('_')[0]\n    return BREED_CODE_TO_LABEL[code]\n\n\ndef get_all_file_path(directory, file_pattern=''):\n    paths = list(f for f in directory.rglob('**\/*{}'.format(file_pattern)) if f.is_file())\n    return sorted(paths, key=str) \n    \n    \nall_image_paths = get_all_file_path(IMAGES_DIR, '.jpg') # PosixPath\nall_image_labels = [path_to_label(path) for path in all_image_paths] # [0,1,2,...]\n\nassert len(all_image_paths)==len(all_image_labels), 'Numbers of images and labels not match! {}!={}'.format(len(all_image_paths), len(all_image_labels))\n\n# Write labels to file \nwith open('labels.txt', 'w') as f:\n    f.write('\\n'.join(BREED_NAME_TO_LABEL))","33e0b6ac":"# Crop and save images according to boundings\nIMAGES_CROPPED_DIR = pathlib.Path('\/tmp\/images_cropped\/')\nIMAGES_CROPPED_DIR.mkdir(parents=True, exist_ok=True) \n\n# Gets object boundings\ndef parse_bounding(path):\n    # Get annotation path from image path\n    path = ANNOTATION_DIR \/ path.parent.name \/ path.stem\n    \n    # Parse boundings\n    tree = ET.parse(path)\n    bndbox = tree.getroot().findall('object')[0].find('bndbox')\n    left = int(bndbox.find('xmin').text)\n    right = int(bndbox.find('xmax').text) \n    upper = int(bndbox.find('ymin').text)\n    lower = int(bndbox.find('ymax').text) \n    \n    return (left, upper, right, lower)\n\n\ndef crop_and_save_image(path, save_dir=IMAGES_CROPPED_DIR):\n    box = parse_bounding(path)\n    \n    image = Image.open(path)\n    image_cropped = image.crop(box)\n    image_cropped = image_cropped.convert('RGB')\n    image_cropped.save(save_dir \/ path.name)","5600f06a":"# Crop images according to bounding boxes\ntry:\n    pool = mp.Pool(processes=mp.cpu_count())\n    pool.map(crop_and_save_image, all_image_paths)\nexcept Exception as e:\n    print(e)\nfinally:\n    pool.close()\n\n    \nall_image_cropped_paths = get_all_file_path(IMAGES_CROPPED_DIR, '.jpg') # PosixPath\nall_image_cropped_labels = [path_to_label(path) for path in all_image_cropped_paths] # [0,1,2,...]\n\nassert len(all_image_paths)==len(all_image_cropped_paths), 'Numbers of images and cropped images not match! {}!={}'.format(len(all_image_paths), len(all_image_cropped_paths))","c27dae80":"# IMAGE_PATH = all_image_paths\n# LABEL = all_image_labels\nIMAGE_PATH = all_image_cropped_paths\nLABEL = all_image_cropped_labels","1af7c007":"IMAGE_PATH[:5]","adfed544":"LABEL[:5]","3c3fd08d":"# Label distribution\n_ = plt.hist(LABEL, bins=120)\nplt.xlabel('Label index')\nplt.ylabel('Count')\nplt.title('Label Distribution')\nplt.show()","044a9556":"# Ramdomly check a dog image from the dataset\ndog = np.random.choice(IMAGE_PATH)\nprint('Random dog: ', BREED_LABEL_TO_NAME[path_to_label(dog)])\nImage.open(dog)","52761055":"x_train, x_test, y_train, y_test = train_test_split(IMAGE_PATH, \n                                                    LABEL,\n                                                    test_size=TEST_SIZE, \n                                                    random_state=RANDOM_SEED,\n                                                    shuffle=True,\n                                                    stratify=LABEL)\n\nprint('Train data: ', len(x_train))\nprint('Test data: ', len(x_test))","158205b6":"def augmentation(image, label=None):\n    image = tf.image.random_flip_left_right(image, seed=RANDOM_SEED)\n    image = tf.image.random_brightness(image, max_delta=0.1, seed=RANDOM_SEED)\n    image = tf.image.random_contrast(image, lower=0.9, upper=1.1, seed=RANDOM_SEED)\n    if label is None:\n        return image\n    return image, label\n\n\ndef preprocess_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE])\n    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n    return image\n\n\ndef load_and_preprocess_image(path):\n    image = tf.io.read_file(path)\n    return preprocess_image(image)\n\n\ndef load_and_preprocess_from_path_label(path, label):\n    return load_and_preprocess_image(path), label","5ca092ab":"ds_train = tf.data.Dataset.from_tensor_slices(([str(path) for path in x_train], y_train))\nds_test = tf.data.Dataset.from_tensor_slices(([str(path) for path in x_test], y_test))\n\n# Apply shuffle and repeat on training data\nds_train = ds_train.apply(\n    tf.data.experimental.shuffle_and_repeat(buffer_size=len(x_train), seed=RANDOM_SEED))\n\n# Preprocessing\nds_train = ds_train.map(load_and_preprocess_from_path_label, num_parallel_calls=AUTOTUNE)\nds_test = ds_test.map(load_and_preprocess_from_path_label, num_parallel_calls=AUTOTUNE)\n\n# Augmentation\n# ds_train = ds_train.map(augmentation, num_parallel_calls=AUTOTUNE)\n\nds_train = ds_train.batch(BATCH_SIZE)\nds_test = ds_test.batch(BATCH_SIZE)\n\n# `prefetch` lets the dataset fetch batches in the background while the model is training.\nds_train = ds_train.prefetch(buffer_size=1)","19370b8a":"# Fine tuning based on MobileNetV2\nbase_model = tf.keras.applications.MobileNetV2(input_shape=(INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3), include_top=False)\nbase_model.trainable = False","94d64fae":"model = tf.keras.Sequential([\n    base_model,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(len(BREED_NAME_TO_LABEL), \n                          activation='softmax', \n                          kernel_initializer=tf.keras.initializers.glorot_normal(seed=RANDOM_SEED),\n                          bias_initializer='zeros',\n                          name='predictions')\n])\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n              loss='sparse_categorical_crossentropy',\n              metrics=[\"accuracy\"]\n)\n\nmodel.summary()","0ef81485":"# Start training the model\nsteps_per_epoch = len(x_train)\/\/BATCH_SIZE\nhistory = model.fit(ds_train, epochs=EPOCH, validation_data=ds_test, steps_per_epoch=steps_per_epoch)","0d1aad31":"# Plot training & validation metrics\ndef plot_model_history(history):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n    fig.suptitle('Model Training Metrics')\n\n    ax1.plot(history.history['accuracy'])\n    ax1.plot(history.history['val_accuracy'])\n    ax1.title.set_text('Accuracy')\n    ax1.set_ylabel('Accuracy')\n    ax1.set_xlabel('Epoch')\n    ax1.legend(['Train', 'Valid'], loc='upper left')\n\n    ax2.plot(history.history['loss'])\n    ax2.plot(history.history['val_loss'])\n    ax2.title.set_text('Loss')\n    ax2.set_ylabel('Loss')\n    ax2.set_xlabel('Epoch')\n    ax2.legend(['Train', 'Valid'], loc='upper left')\n\n    fig.show()\n    \n    \nplot_model_history(history)","9602f38d":"# # Fine tune layers of the MobileNet base model\n# base_model.trainable = True\n# fine_tune_at = 152\n\n# # Freeze all the layers before the `fine_tune_at` layer\n# for layer in base_model.layers[:fine_tune_at]:\n#      layer.trainable = False\n        \n\n# model.compile(optimizer=tf.keras.optimizers.Adam(1e-8),\n#               loss='sparse_categorical_crossentropy',\n#               metrics=[\"accuracy\"]\n# )\n\n# model.summary()\n\n# history_finetune = model.fit(ds_train, epochs=EPOCH, validation_data=ds_test, steps_per_epoch=steps_per_epoch)\n\n# plot_model_history(history_finetune)","e7ad27c0":"model_version = 'mobilenet_v2_1.0_224_stanford_dogbreeds'","411c710a":"# Save the keras model\nmodel.save('{}.h5'.format(model_version))","fd10560b":"# Convert keras model to .tflite\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n# converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\ntflite_model = converter.convert()\nwith open('{}.tflite'.format(model_version), 'wb') as f:\n    f.write(tflite_model)","7c2b1337":"# Inference \ndef decode_prediction(preds, top=3):\n    top_indices = preds.argsort()[-top:][::-1]\n    result = [(BREED_LABEL_TO_NAME[i], preds[i]) for i in top_indices] # (labels, scores)\n    result.sort(key=lambda x: x[1], reverse=True)\n    return '\\n'.join(['{}: {:.4f}'.format(*item) for item in result])\n\n\ndef inference(image, model, decode=False):\n    preds = model.predict(image)[0]\n    if decode:\n        result = decode_prediction(preds)\n        return result\n    return preds\n\n\n# Inference with .tflite model\ndef tflite_inference(image, model_file, decode=False):\n    interpreter = tf.lite.Interpreter(model_file)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    interpreter.allocate_tensors()\n    interpreter.set_tensor(input_details[0]['index'], tf.cast(image, input_details[0]['dtype']))\n    interpreter.invoke()\n    preds = interpreter.get_tensor(output_details[0]['index'])[0]\n    if decode:\n        result = decode_prediction(preds)\n        return result    \n    return preds\n\n\nimage_example_path = np.random.choice(x_test)\nimage_example = load_and_preprocess_image(str(image_example_path))\nimage_example = tf.expand_dims(image_example, axis=0)\nlabel = BREED_LABEL_TO_NAME[path_to_label(image_example_path)]\nprint('Label: {}'.format(label))\n\nprint('Prediction (keras):')\npreds = inference(image_example, model, decode=True)\nprint(preds)\n\nprint('Prediction (tflite):')\npreds = tflite_inference(image_example, '{}.tflite'.format(model_version), decode=True)\nprint(preds)","87f0bb42":"fig, axes = plt.subplots(5, 4, figsize=(20, 16))\naxes = axes.ravel()\n\nfor i, ax in enumerate(axes):\n    # Randomly test a sample\n    image_example_path = np.random.choice(x_test)\n    image_example = load_and_preprocess_image(str(image_example_path))\n    image_example = tf.expand_dims(image_example, axis=0)\n    \n    label = BREED_LABEL_TO_NAME[path_to_label(image_example_path)]\n    preds = inference(image_example, model, decode=True)\n\n    image = Image.open(image_example_path)\n    ax.imshow(image)\n    ax.set_title(preds)\n    ax.set_xlabel(label)\n    ax.grid(False)\n        \nplt.tight_layout()","49404898":"preds_test = model.predict(ds_test)\ny_preds_test = preds_test.argmax(axis=1)","0e8266c0":"# Get confusion matrix\nconf_mat = confusion_matrix(y_test, y_preds_test)\n# np.fill_diagonal(conf_mat, 0)\nplt.figure(figsize=(8, 8))\nplt.plot(figsize=())\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.imshow(conf_mat)\nplt.axis('scaled')\nplt.show()","679b71d9":"# Get mismatched predictions\nrow_idx, col_idx = conf_mat.nonzero()\nvalue_count = conf_mat[row_idx, col_idx]\ndf_conf_mat = pd.DataFrame({'label': row_idx, 'pred': col_idx, 'count': value_count})\n\ndf_conf_mat = df_conf_mat.sort_values('count', ascending=False)\ndf_label_count = df_conf_mat.groupby('label')['count'].sum().to_frame().reset_index()\ndf_label_count = df_label_count.rename(columns={'count': 'total'})\n\ndf_conf_mat = df_conf_mat.merge(df_label_count, how='left', on='label')\ndf_conf_mat['ratio'] = df_conf_mat['count'] \/ df_conf_mat['total']\n\ndf_conf_mat = df_conf_mat[df_conf_mat['label']!=df_conf_mat['pred']]\ndf_conf_mat[:10]","08ceda54":"def plot_random_image_of_a_breed(image_paths, breed_label, image_num=8):\n    breed_code = BREED_LABEL_TO_CODE[breed_label]\n    sample_image_of_a_breed_path = [path for path in image_paths if breed_code in str(path)][:image_num]\n\n    fig, axes = plt.subplots(image_num\/\/4, 4, figsize=(14, 8))\n    axes = axes.ravel()\n\n    for i, (ax, image_path) in enumerate(zip(axes, sample_image_of_a_breed_path)):\n        image = Image.open(image_path)\n        ax.imshow(image)\n        ax.set_title('{}'.format(image_path.stem))\n        ax.grid(False)\n        ax.axis('off')\n\n    fig.suptitle(BREED_LABEL_TO_NAME[breed_label])\n    fig.tight_layout()","e2f2a641":"# The `label` in first row\nplot_random_image_of_a_breed(x_test, df_conf_mat['label'].iloc[0])","bb620e88":"# The `pred` in first row\nplot_random_image_of_a_breed(x_test, df_conf_mat['pred'].iloc[0])","8b033a22":"## Let's randomly check some other samples","c5b4b6d7":"## Model setup","e51c7006":"## Confusion matrix analysis","1ff9408c":"## Input pipeline"}}