{"cell_type":{"f075e791":"code","7ca2a76e":"code","63703460":"code","e3eee43f":"code","383104fe":"code","084ae772":"code","823ec06b":"code","e12c90c5":"code","2b3d369f":"code","6e6e5e00":"code","55b4fc52":"code","385b1ce7":"code","71780fdb":"code","1aa16da3":"code","e96a8213":"code","3e3046f1":"code","85911302":"code","eba4b104":"code","4901770a":"code","a6dedf31":"code","db0e070d":"code","681ba53f":"code","01855007":"code","2f187418":"code","dbd93aae":"code","0a68b8f8":"code","72fabe26":"code","52caa2f3":"code","089d4a78":"code","15873158":"code","3d882d3d":"code","7847f667":"code","db8d77d6":"code","f4a7aef7":"code","6a311b08":"code","26d71420":"code","bcd00d5f":"code","8ebb8e3a":"code","351e847f":"code","2fe65a71":"code","14950d0a":"code","aa41d12c":"code","6bae776c":"code","2f90ba7f":"code","0e8493cc":"code","e06f7e08":"code","2540c9d2":"code","88495028":"code","4b43d7d2":"code","b4c8b6a9":"code","60c7146e":"code","53459eaa":"code","3cfc3e81":"code","dcfb1b6e":"code","03004d3f":"code","66476836":"code","807ea71c":"markdown","ac8a379d":"markdown","b5d3cb20":"markdown","3fb838d6":"markdown","de3c136a":"markdown","b2349d8e":"markdown","6615130b":"markdown"},"source":{"f075e791":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7ca2a76e":"import pandas as pd\nhousing = pd.read_csv('..\/input\/california-housing-prices\/housing.csv')\nhousing.head()","63703460":"housing.describe()","e3eee43f":"housing.isnull().sum()","383104fe":"housing = housing.dropna(axis=0)","084ae772":"housing","823ec06b":"housing = housing.drop(['ocean_proximity'], axis=1)","e12c90c5":"housing","2b3d369f":"x = housing.iloc[:,:-1]\ny = housing[['median_house_value']]","6e6e5e00":"y.shape","55b4fc52":"from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()\nsc1 = MinMaxScaler()\nx_nor = sc.fit_transform(x)\ny_nor = sc1.fit_transform(y)","385b1ce7":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x_nor, y_nor, test_size=0.25, shuffle=False, random_state=1004)","71780fdb":"X_train.shape","1aa16da3":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, TimeDistributed, LSTM, ConvLSTM2D\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D, MaxPooling1D\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\nfrom keras import regularizers\n\nNNinput = X_train.shape[1]\nact = 'relu'\nopt = 'Adam'\nlos = 'mean_squared_error'\n\nmodel = Sequential()\nmodel.add(Dense(128, activation = act, input_shape = [NNinput,]))\nmodel.add(Dense(128, activation = act))\nmodel.add(Dense(128, activation = act))\nmodel.add(Dense(1, activation = act))\nmodel.compile(optimizer = opt, loss = los, metrics = ['mse'])\n#model.summary()","e96a8213":"batch_size = 128\nepoch = 10\nhistory = model.fit(X_train, y_train, epochs = epoch, batch_size = batch_size, verbose = 1, validation_data=(X_test, y_test))","3e3046f1":"pre = model.predict(X_test)","85911302":"pre","eba4b104":"pre_1=sc1.inverse_transform(pre)\ntest_1=sc1.inverse_transform(y_test)","4901770a":"pre_1","a6dedf31":"test_1","db0e070d":"abs(pre_1\/test_1-1).mean()","681ba53f":"housing.info()","01855007":"housing[\"ocean_proximity\"].value_counts()","2f187418":"housing.describe().T","dbd93aae":"from matplotlib import pyplot as plt\nimport numpy as np","0a68b8f8":"housing.hist(bins=50, figsize=(20,15))\nplt.show();","72fabe26":"fig = plt.figure(dpi = 80, figsize = (6,4))\nax = fig.add_axes([1,1,1,1])\nax.set(xlabel = 'Median Income Class',ylabel = 'Frequency',title = 'Distribution of Median Income')\nhousing[\"median_income\"].hist(color='blue',ax = ax)\nplt.show()","52caa2f3":"fig = plt.figure(dpi = 80, figsize = (6,4))\nax = fig.add_axes([1,1,1,1])\nax.set(xlabel = 'Median House Value Class',ylabel = 'Frequency',title = 'Distribution of House Value ')\nhousing[\"median_house_value\"].hist(color='blue',ax = ax)\nplt.show()","089d4a78":"housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])","15873158":"housing[\"income_cat\"].value_counts()","3d882d3d":"housing[\"house_value_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])","7847f667":"housing[\"house_value_cat\"] = pd.cut(housing[\"median_house_value\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])\nhousing[\"house_value_cat\"].value_counts()","db8d77d6":"fig = plt.figure(dpi = 80, figsize = (6,4))\nax = fig.add_axes([1,1,1,1])\nax.set(xlabel = 'Median Income Category',ylabel = 'Frequency',title = 'Distribution of Median Income Category')\nhousing[\"income_cat\"].hist(color = 'purple',ax=ax)\nplt.show()","f4a7aef7":"fig = plt.figure(dpi = 80, figsize = (6,4))\nax = fig.add_axes([1,1,1,1])\nax.set(xlabel = 'Median house value Category',ylabel = 'Frequency',title = 'Distribution of Median house value Category')\nhousing[\"house_value_cat\"].hist(color = 'purple',ax=ax)\nplt.show()","6a311b08":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","26d71420":"housing = strat_train_set.copy()","bcd00d5f":"fig = plt.figure(dpi = 100,figsize = (4,4))\nax = fig.add_axes([1,1,1,1])\n\nimport matplotlib.image as mpimg\ncalifornia_img=mpimg.imread(\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/1\/1b\/California_Locator_Map.PNG\/280px-California_Locator_Map.PNG\")\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),ax=ax,\n                       s=housing['population']\/100, label=\"Population\",\n                       c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n                       colorbar=False, alpha=0.4,\n                      )\nplt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n           cmap=plt.get_cmap(\"jet\"))\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\n\nprices = housing[\"median_house_value\"]\ntick_values = np.linspace(prices.min(), prices.max(), 11)\ncbar = plt.colorbar()\ncbar.ax.set_yticklabels([\"$%dk\"%(round(v\/1000)) for v in tick_values], fontsize=14)\ncbar.set_label('Median House Value', fontsize=16)\n\nplt.legend(fontsize=16)\nplt.show();","8ebb8e3a":"# from pandas.tools.plotting import scatter_matrix # For older versions of Pandas\nfrom pandas.plotting import scatter_matrix\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n              \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))\nplt.show()","351e847f":"fig = plt.figure(dpi = 80, figsize = (6,4))\nax = fig.add_axes([1,1,1,1])\n\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n             alpha=0.1,color = 'blue',ax=ax)\nplt.axis([0, 16, 0, 550000])\nplt.show()","2fe65a71":"housing[\"rooms_per_household\"] = housing[\"total_rooms\"]\/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]\/housing[\"households\"]","14950d0a":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np","aa41d12c":"corr = housing.corr()\nmask = np.triu(np.ones_like(corr,dtype = bool))\n\nplt.figure(dpi=100)\nplt.title('Correlation Analysis')\nsns.heatmap(corr,mask=mask,annot=False,lw=0,linecolor='white',cmap='magma',fmt = \"0.2f\")\nplt.xticks(rotation=90)\nplt.yticks(rotation = 0)\nplt.show()","6bae776c":"housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","2f90ba7f":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder","0e8493cc":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# column index\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] \/ X[:, households_ix]\n        population_per_household = X[:, population_ix] \/ X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n","e06f7e08":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])","2540c9d2":"from sklearn.compose import ColumnTransformer\n\nhousing_num = housing.drop(\"ocean_proximity\", axis=1)\n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])","88495028":"housing_prepared = full_pipeline.fit_transform(housing)\nhousing_prepared","4b43d7d2":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=5, random_state=42)\nforest_reg.fit(housing_prepared, housing_labels)","b4c8b6a9":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\nhousing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nprint(\"RMSE ==> \", forest_rmse)","60c7146e":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())","53459eaa":"from sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=5)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","3cfc3e81":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # try 12 (3\u00d74) combinations of hyperparameters\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    # then try 6 (2\u00d73) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)","dcfb1b6e":"grid_search.best_params_\n","03004d3f":"grid_search.best_estimator_","66476836":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","807ea71c":"\uc544\ub798 \ucf54\ub4dc\uac00 \uc5b4\ub5a4 \uc758\ubbf8\uc778\uc9c0 \ucc3e\uc544\ubcf4\uc790","ac8a379d":"# * **\uad6c\ubd84\uc744 \uc5b4\ub5a4 \uae30\uc900\uc73c\ub85c \ud558\ub294 \uac74\uc9c0?? **","b5d3cb20":"# \uc77c\ub2e8 \uc544\ub798 \ub9c1\ud06c \ubcf4\uace0 \ub530\ub77c\ud574 \ubd04\nhttps:\/\/www.kaggle.com\/ravichaubey1506\/end-to-end-machine-learning","3fb838d6":"Model Tuning\n\nFeel free to change CV and do some experimentation !!","de3c136a":"Split the Data in Train and Test set","b2349d8e":"# Data Pipeline","6615130b":"Model Training\n\nI am not focusing myself to achieve accuracy here. So i will use simpler models. Feel free to fork and tweak params accordingly :)"}}