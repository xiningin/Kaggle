{"cell_type":{"40b1a9fe":"code","7181fa43":"code","6148ed6f":"code","113dfb0b":"code","740d9a5c":"code","cb277bbd":"code","c32e7c12":"code","161e2a58":"code","f6c614c3":"code","5d6ba03d":"code","9960f80f":"code","9ace255c":"code","cb400f1b":"code","7aa8854f":"code","1f02e929":"code","ab76e627":"code","537d838f":"code","60a7bf9d":"code","91989447":"code","32a7e201":"code","2bf0bf49":"code","8fd994c0":"code","5ad26d33":"code","f71cf7e3":"code","492728eb":"code","bba73e5b":"code","5bb8f2f4":"code","bfefbecc":"code","3be5ceb9":"code","1e9e3032":"code","a1813814":"code","6b78d1cf":"code","70ff9cba":"code","3dcbe818":"code","473acc2a":"code","5577e593":"code","02369d21":"code","bbb14b36":"markdown","3f97514c":"markdown","8ad8fd83":"markdown","ca2495db":"markdown","c6442ce2":"markdown","0b8b80bf":"markdown","b14d5213":"markdown","bad5d1f0":"markdown","d52542f0":"markdown","510191f9":"markdown","46e3b1e8":"markdown","959f6554":"markdown","46f5a4d6":"markdown","60545e0a":"markdown","1851cfc5":"markdown","80148261":"markdown","990ee7f6":"markdown","14945c41":"markdown","eb036710":"markdown","9abc147a":"markdown","66d67636":"markdown","59486944":"markdown","acd7a387":"markdown","31a47e1f":"markdown","5c228c40":"markdown","d4d0cff9":"markdown","63e0ad3b":"markdown","bde63875":"markdown","c61f6b55":"markdown","abd46dd2":"markdown","0eb55a61":"markdown","94efda94":"markdown","2a60f25a":"markdown","44565308":"markdown"},"source":{"40b1a9fe":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.preprocessing import LabelEncoder\n\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n","7181fa43":"data = pd.read_csv('\/kaggle\/input\/pizza-price-prediction\/pizza_v2.csv')","6148ed6f":"data","113dfb0b":"data.isnull().mean()","740d9a5c":"data.info()","cb277bbd":"data.nunique()","c32e7c12":"data['price_rupiah'].unique()","161e2a58":"data['price_rupiah'] = data.price_rupiah.str.extract(r'(\\d+[.\\d]*)').astype(int)","f6c614c3":"data['price_rupiah'].unique()","5d6ba03d":"data['diameter'].unique()","9960f80f":"data['diameter'] = data.diameter.str.extract(r'(\\d+[.\\d]*)').astype(float)","9ace255c":"data['diameter'].unique()","cb400f1b":"data.extra_mushrooms.unique()","7aa8854f":"cat = ['extra_sauce', 'extra_cheese', 'extra_mushrooms']\nlabelencoder = LabelEncoder()\nfor i in cat:\n    data[i] = labelencoder.fit_transform(data[i])","1f02e929":"data.extra_mushrooms.unique()","ab76e627":"for i in data.company.unique():\n    print('company', i, list(data[data['company'] == i]['topping'].unique()))","537d838f":"for i in data.variant.unique():\n    print('variant', i, data[data['variant'] == i]['topping'].unique(), end='\\n\\n')","60a7bf9d":"data['variant'].replace('spicy_tuna', 'spicy tuna', inplace=True)","91989447":"pd.DataFrame(data[data['company'] == 'C'].groupby(['variant', 'topping']).size())","32a7e201":"pd.DataFrame(data[data['company'] == 'C'].groupby(['topping', 'variant'])[['price_rupiah']].mean())","2bf0bf49":"data.drop('topping', axis=1, inplace=True)","8fd994c0":"data.head()","5ad26d33":"data['size'].unique()","f71cf7e3":"for i in data.company.unique():\n    print('company',i, list(data[data['company'] == i]['size'].unique()))","492728eb":"data[(data['company'] == 'A') & (data['size'] == 'reguler')]['diameter'].unique()","bba73e5b":"data[(data['company'] == 'A') & (data['size'] == 'jumbo')]['diameter'].unique()","5bb8f2f4":"data.drop('size', axis=1, inplace=True)","bfefbecc":"data.head()","3be5ceb9":"data.company.value_counts()","1e9e3032":"plt.figure(figsize=(8,5))\nsns.boxplot(x='company', y='price_rupiah', data=data)","a1813814":"data = pd.get_dummies(data)","6b78d1cf":"data.head()","70ff9cba":"data.corr()['price_rupiah'].abs().sort_values(ascending=False)","3dcbe818":"pd.DataFrame(data.corr()['price_rupiah'].abs().sort_values(ascending=False)).T.columns","473acc2a":"X = data.drop(['price_rupiah'], axis=1)\ny = data.price_rupiah\nprint(sm.OLS(y,X).fit().summary())","5577e593":"def model_training(X, y):\n    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n    models = []\n    models.append(('LR', LinearRegression()))\n    models.append(('R', Ridge()))\n    models.append(('L', Lasso()))\n    models.append(('BR', BayesianRidge(n_iter=1000)))\n    models.append(('KNR', KNeighborsRegressor()))\n    models.append(('DTR', DecisionTreeRegressor()))\n    models.append(('SVR', SVR()))\n    models.append(('ABR', AdaBoostRegressor(n_estimators=100)))\n    models.append(('BR', BaggingRegressor(n_estimators=100)))\n    models.append(('ETR', ExtraTreesRegressor(n_estimators=100)))\n    models.append(('GBR', GradientBoostingRegressor(n_estimators=100)))\n    models.append(('RFR', RandomForestRegressor(n_estimators=100)))\n    scores = []\n    names = []\n    results = []\n    predictions = []\n    msg_row = []\n    for name, model in models:\n        kfold = KFold(n_splits=5)\n        cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='r2')\n        names.append(name)\n        results.append(cv_results)\n        m_fit = model.fit(X_train, Y_train)\n        m_predict = model.predict(X_test)\n        predictions.append(m_predict)\n        m_score = r2_score(Y_test, m_predict)\n        scores.append(m_score)\n        msg = \"%s: train = %.3f (%.3f) \/ test = %.3f\" % (name, cv_results.mean(), cv_results.std(), m_score)\n        msg_row.append(msg)\n        print(msg)","02369d21":"model_training(X, y)","bbb14b36":"**List of unique size variables in each company**","3f97514c":"**size - the size of the pizza, which correlates with the diameter. I don't quite understand why it is needed, since there is a detailed variable diameter, which has 12 exact values, when the variable size has 6 values. The values of small, reguler, medium, large, XL, jumbo are approximate, they are different for each company, for example:**\n**<li>company A - reguler, jumbo**\n**<li>company B - small, medium, large, XL**  \n","8ad8fd83":"# Data preparation and analysis","ca2495db":"**Let's take the module and sort it in descending order to see which variables correlate least with the target variable. A large negative correlation is also a correlation.**","c6442ce2":"**To begin with, let's build a correlation matrix to see how the features interact with the target feature - price_rupiah**","0b8b80bf":"# Reading the Dataset","b14d5213":"### diameter","bad5d1f0":"**We change these variables yes|no >>>>  1 | 0**","d52542f0":"### company","510191f9":"**company** - categorical variable   \n**price_rupiah** - continuous variable   \n**diameter** - categorical variable    \n**topping** - categorical variable   \n**variant** - categorical variable    \n**size** - categorical variable   \n**extra_sauce** - binary variable    \n**extra_cheese** - binary variable   \n**extra_mushrooms** - binary variable  ","46e3b1e8":"**When preparing the project, I divided the dataset by companies in order to train separately, but only company A and C had a relationship. The data is artificial so much that it turns out to build a model only if we consider all the companies together, in one dataset.**","959f6554":"**Transpose the dataframe so that our signs become columns and output a list of them. This is so that it is convenient to copy and delete columns before training. But we will leave everything.**","46f5a4d6":"**price_rupiah shows the price for pizza in rupees, we remove the currency designation and leave only the price**","60545e0a":"### extra_sauce, extra_mushrooms, extra_sauce","1851cfc5":"**<li>129 elements and 29 features**  \n**<li>We have removed the size and topping signs**  \n**<li>We applied labelcoder to the extra_safe extra_cheese, extra_mushrooms attributes because they are binary**  \n**<li>For the price_rupiah and diameter signs, we left only the numbers using .str.extract**  \n**<li>We applied get_dummies to the company and variant attributes, because these are categorical attributes and they cannot be numbered within a single attribute.**","80148261":"**Best result: GradientBoostingRegressor: R2 score - 0.94**","990ee7f6":"**The spread of the diameter value with size == reguler is from 16 to 22**","14945c41":"**Pizza name and topping options**","eb036710":"**Some toppings and types of pizzas are highly correlated, for example, tuna and neptune_tuna, spice_tuna, or vegetables and italian_veggie, thai_veggie. In order not to 'feed' 129 pizzas - 32 columns (20 variant and 12 topping using the get_dummies method), let's remove one feature. I suggest leaving variant, since there is more variability in this feature, hence more opportunities for better training of the model.**","9abc147a":"### size","66d67636":"**company - 5 pizza companies** ","59486944":"**topping - pizza filler**  \n**variant - pizza name on the menu**","acd7a387":"**Unique toppings by company**","31a47e1f":"**Regression analysis using sm.OLD.**  \n  \n**<li> R-squared: 0.901 - shows how much our model explains the dependent variable - price_rupiah.**  \n      \n**<li> Adj. R-squared: 0.876 - adjusted R-squared. In simple words, this is the same indicator, but with a penalty for a large number of features in our model.**  \n      \n**<li> AIC and BIC are the Akaic criterion and the Schwartz criterion. Our model is penalized for a large amount of balances and a large number of regressors (signs). The lower the values of the criteria, the better.**  \n      \n**<li> The 'coef' attribute shows the coefficient of change of the price_rupiah variable. For example, increasing diameter by 1 would result in an increase in price_rupiah by 8.9101.**  \n      \n**<li> t and P>|t| shows the significance of the attribute for predicting the target variable price_rupiah. The more the first criterion is different from 0, the better for our model, the more the second criterion tends to 1, the worse for our model.**","5c228c40":"# Importing Necessary Libraries","d4d0cff9":"# Training the model","63e0ad3b":"### topping \/ variant","bde63875":"**diameter - the diameter of the pizza in inches, similarly with the price - we make it a numerical sign**","c61f6b55":"**In both cases, diameter == 16 in both jumbo and reguler, even within the same company A. Therefore, we will delete the size variable so that there is no unnecessary noise in the data.**","abd46dd2":"**The spread of the diameter value with size == reguler is from 8 to 16.5**","0eb55a61":"**The values of the company and variant attributes are categorical, so we can't just take and replace them with numbers. Let's split all the values into columns using get_dummies. In this form, it is automatically applied to all non-numeric features (columns).**","94efda94":"### price_rupiah","2a60f25a":"**Let's make a boxplot on the target variable price_rupiah and company, it is clearly visible that the prices are higher in A and D. The average price in company A is above the 75% quantile for 3 out of 4 companies - B, C, E.**","44565308":"**How many pizzas are in the table from each company**"}}