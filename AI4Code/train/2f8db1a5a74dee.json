{"cell_type":{"6447c02c":"code","4a3677a0":"code","ebfbf0be":"code","12ae4f4e":"code","f22c166d":"code","5bd80c1c":"code","be6a6205":"code","39656ee9":"code","99c2a14f":"code","63c2ae18":"code","75c1c663":"code","87173855":"code","7a66fee6":"code","5477ad8b":"code","6d259702":"code","97e4d967":"code","82ba8571":"code","46d74039":"code","926fef99":"code","0b938dd0":"code","ddab6487":"code","696e1909":"code","abbc72dc":"code","00035c29":"code","f69722dc":"code","9437aeb9":"code","16b240e0":"code","3686654e":"code","932cdfdd":"code","efda1bcb":"code","0796cd03":"code","b4fb1931":"code","1ff44956":"code","ed3f6079":"code","5f6717c1":"code","ea5d440d":"code","c8769621":"code","43c48210":"code","54725ee9":"code","0e3a8afd":"code","8fc78351":"code","ffd811e1":"code","5cf6fc4b":"code","988c246a":"code","8d0da7e6":"code","cbde27b0":"code","d69558a5":"code","9b62863e":"code","6c4994e4":"code","eb4620d4":"code","8ab7213b":"code","8e41315a":"code","6b64ef95":"code","d86db113":"code","f6bf0c0c":"code","6d494325":"code","a1df1532":"markdown","ad52fa2e":"markdown","b925dac9":"markdown","f412a4da":"markdown","ed5ae2d7":"markdown","a036511e":"markdown","00d65795":"markdown","67f52a4a":"markdown","dff9f782":"markdown","c9dfca01":"markdown","034338e1":"markdown","c26ddeb8":"markdown","55a54e21":"markdown","9ecf3871":"markdown","96256505":"markdown","3490995c":"markdown","964a523c":"markdown","54f90b40":"markdown","03aa229b":"markdown","9be5a6b4":"markdown","ddc7dff0":"markdown","7bf583bc":"markdown"},"source":{"6447c02c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4a3677a0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","ebfbf0be":"train=pd.read_csv('\/kaggle\/input\/Train.csv')\ntest=pd.read_csv('\/kaggle\/input\/Test.csv')\nsample=pd.read_csv('\/kaggle\/input\/Sample.csv')\ntrain.head()","12ae4f4e":"train.describe()","f22c166d":"len(set(test.ID)-set(train.ID)),test.shape #295 test case contains the new values.","5bd80c1c":"np.intersect1d(train.ID,test.ID)","be6a6205":"train.loc[train.ID==458996]","39656ee9":"test.loc[test.ID==458996]","99c2a14f":"train.describe(exclude='number')","63c2ae18":"train.Segmentation.value_counts()\/len(train)*100 #Balanced","75c1c663":"train.isnull().sum() ##for the shake of simplicity fill by convention.","87173855":"sns.heatmap(train.corr(),annot=True)","7a66fee6":"#Filled based on intution\ntrain.Graduated.fillna('No',inplace=True)\ntrain.Profession.fillna('unk',inplace=True)#unk means unknown.\ntrain.Work_Experience.fillna(0,inplace=True)\ntrain.Family_Size.fillna(5,inplace=True)\ntrain.Var_1.fillna('unk',inplace=True)","5477ad8b":"_,(ax1,ax2)=plt.subplots(nrows=1,ncols=2,figsize=(10,5))\n\nsns.distplot(train.loc[(train.Ever_Married=='Yes')&(train.Graduated=='Yes'),'Age'],label='MG',ax=ax1)\nsns.distplot(train.loc[(train.Ever_Married=='No')&(train.Graduated=='Yes'),'Age'],label='UMG',ax=ax1)\n\nsns.distplot(train.loc[(train.Ever_Married=='Yes')&(train.Graduated=='No'),'Age'],label='MUG',ax=ax2)\nsns.distplot(train.loc[(train.Ever_Married=='No')&(train.Graduated=='No'),'Age'],label='UMUG',ax=ax2)\nplt.legend()","6d259702":"sns.distplot(train.loc[(train.Ever_Married.isnull())&(train.Graduated=='Yes'),'Age'],label='G')\nsns.distplot(train.loc[(train.Ever_Married.isnull())&(train.Graduated=='No'),'Age'],label='UG')\nplt.legend()","97e4d967":"# Filling married or not\ntrain.loc[(train.Ever_Married.isnull())&(train.Graduated=='Yes')&(train.Age<35),'Ever_Married']=train.loc[(train.Ever_Married.isnull())&(train.Graduated=='Yes')&(train.Age<35),'Ever_Married'].fillna('No')\ntrain.loc[(train.Ever_Married.isnull())&(train.Graduated=='Yes')&(train.Age>=35),'Ever_Married']=train.loc[(train.Ever_Married.isnull())&(train.Graduated=='Yes')&(train.Age>=35),'Ever_Married'].fillna('Yes')\n\ntrain.loc[(train.Ever_Married.isnull())&(train.Graduated=='No')&(train.Age<30),'Ever_Married']=train.loc[(train.Ever_Married.isnull())&(train.Graduated=='No')&(train.Age<30),'Ever_Married'].fillna('No')\ntrain.loc[(train.Ever_Married.isnull())&(train.Graduated=='No')&(train.Age>=30),'Ever_Married']=train.loc[(train.Ever_Married.isnull())&(train.Graduated=='No')&(train.Age>=30),'Ever_Married'].fillna('Yes')","82ba8571":"train.isnull().sum()","46d74039":"test.isnull().sum()","926fef99":"test.Graduated.fillna('No',inplace=True)\ntest.Profession.fillna('unk',inplace=True)\ntest.Work_Experience.fillna(0,inplace=True)\ntest.Family_Size.fillna(3,inplace=True)\ntest.Var_1.fillna('unk',inplace=True)","0b938dd0":"test.loc[(test.Ever_Married.isnull())&(test.Graduated=='Yes')&(test.Age<35),'Ever_Married']=test.loc[(test.Ever_Married.isnull())&(test.Graduated=='Yes')&(test.Age<35),'Ever_Married'].fillna('No')\ntest.loc[(test.Ever_Married.isnull())&(test.Graduated=='Yes')&(test.Age>=35),'Ever_Married']=test.loc[(test.Ever_Married.isnull())&(test.Graduated=='Yes')&(test.Age>=35),'Ever_Married'].fillna('Yes')\n\ntest.loc[(test.Ever_Married.isnull())&(test.Graduated=='No')&(test.Age<30),'Ever_Married']=test.loc[(test.Ever_Married.isnull())&(test.Graduated=='No')&(test.Age<30),'Ever_Married'].fillna('No')\ntest.loc[(test.Ever_Married.isnull())&(test.Graduated=='No')&(test.Age>=30),'Ever_Married']=test.loc[(test.Ever_Married.isnull())&(test.Graduated=='No')&(test.Age>=30),'Ever_Married'].fillna('Yes')","ddab6487":"cat_features=train.columns[train.dtypes==object].drop('Segmentation')","696e1909":"import itertools\nnew=pd.DataFrame(index=train.index)\nfor col1,col2 in itertools.combinations(cat_features,2):\n    new[col1+'_'+col2]=train[col1]+train[col2]","abbc72dc":"import itertools\nnewt=pd.DataFrame(index=test.index)\nfor col1,col2 in itertools.combinations(cat_features,2):\n    newt[col1+'_'+col2]=test[col1]+test[col2]","00035c29":"cat_features_=train.columns[train.dtypes==object].drop('Segmentation')","f69722dc":"label=train.Segmentation.copy()\nlabel.replace(dict(zip(['A','B','C','D'],[0,1,2,3])),inplace=True)\nimport category_encoders\nte=category_encoders.TargetEncoder()\nte.fit(train[cat_features],label)\nencoded=te.transform(train[cat_features])\nencodedt=te.transform(test[cat_features])","9437aeb9":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nfor col in cat_features:\n    train[col]=le.fit_transform(train[col])\n    test[col]=le.transform(test[col])","16b240e0":"train.head()","3686654e":"train.shape","932cdfdd":"feature,label=train.drop(['Segmentation'],axis=1),train.Segmentation","efda1bcb":"feature=feature.join(encoded.add_suffix('enc')) #adding Target Encoded attributes","0796cd03":"from lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nxgb=XGBClassifier()\nlgb=LGBMClassifier()","b4fb1931":"from sklearn.model_selection import GridSearchCV\nxgb=XGBClassifier(n_jobs=-1)\nparams={'max_depth':[3,5],'subsample':[.8],'clsample_bytree':[.6],'n_estimators':[50,100,200,250,300]}\ngs=GridSearchCV(param_grid=params,estimator=xgb,n_jobs=-1)\ngs.fit(feature,label)\ngs.best_params_","1ff44956":"from sklearn.model_selection import GridSearchCV\nlgb=LGBMClassifier(n_jobs=-1)\nparams={'max_depth':[3,5],'subsample':[.8,1],'clsample_bytree':[.6,1],'n_estimators':[50,100,200,250,300],'num_leaves':[20,30,40],'subsample_for_bin':[5000,7000,8000]}\ngs=GridSearchCV(param_grid=params,estimator=lgb,n_jobs=-1)\ngs.fit(feature,label)\ngs.best_params_","ed3f6079":"from sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_selection import SelectKBest\nskb=SelectKBest(mutual_info_classif,k=10)\nselected_data=skb.fit_transform(feature,label)","5f6717c1":"selected_data=pd.DataFrame(skb.inverse_transform(selected_data),columns=feature.columns)\nselected_col=selected_data.columns[selected_data.var()!=0]","ea5d440d":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n#{'clsample_bytree': 0.6,'max_depth': 3,'n_estimators': 250,'num_leaves': 20,'subsample': 0.8,'subsample_for_bin': 5000}\nlgb=LGBMClassifier(clsample_bytree=0.6,max_depth=3,n_estimators=250,num_leaves=20,subsample=0.8,subsample_for_bin=5000)\n\n#{'clsample_bytree': 0.6, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.8}\nxgb=XGBClassifier(max_depth=3,subsample=.8,colsample_bytree=.6,n_estimators=50)\n\nrf=RandomForestClassifier()\nknn=KNeighborsClassifier()\nsvm=SVC(probability=True)\n\nmodels=(('xgb',xgb),('lgb',lgb),('svm',svm),('knn',knn),('rf',rf))\nvc=VotingClassifier(estimators=models,n_jobs=-1)","c8769621":"label=train.Segmentation\nskf=StratifiedKFold(n_splits=5,random_state=123)\nperformance={}\nfor model,name in zip([lgb,xgb,rf,knn,svm],['lgm','xgb','rf','knn','svm']):\n    accuracy=np.array([])\n    for trainind,testind in skf.split(feature,label):\n        X_train,X_test,y_train,y_test=feature.loc[trainind],feature.loc[testind],label[trainind],label[testind]\n        model.fit(X_train,y_train)\n        accuracy=np.append(accuracy,accuracy_score(y_test,model.predict(X_test)))\n    performance[name] = accuracy.mean()  ","43c48210":"pd.Series(performance)","54725ee9":"model=xgb\nmodel.fit(feature,label)","0e3a8afd":"sns.barplot(x=model.feature_importances_,y=feature.columns)","8fc78351":"def fitpredict(estimator):\n    estimator.fit(feature,label)\n    return estimator.predict_proba(feature)\ndf=pd.DataFrame(index=list(range(len(feature))))","ffd811e1":"df=df.join(pd.DataFrame(fitpredict(lgb)).add_suffix('xgb'))\ndf=df.join(pd.DataFrame(fitpredict(rf)).add_suffix('rf'))\ndf=df.join(pd.DataFrame(fitpredict(svm)).add_suffix('svm'))\ndf=df.join(pd.DataFrame(fitpredict(knn)).add_suffix('knn'))","5cf6fc4b":"df.shape","988c246a":"df.head()","8d0da7e6":"modelblend=XGBClassifier(max_depth=3,subsample=.8,colsample_bytree=.6,n_estimators=100)\nmodelblend.fit(df,label)","cbde27b0":"def fitpredict(estimator):\n    estimator.fit(feature,label)\n    return estimator.predict_proba(test.join(encodedt.add_suffix('enc'))[feature.columns])\ndft=pd.DataFrame(index=list(range(len(test))))","d69558a5":"dft=dft.join(pd.DataFrame(fitpredict(lgb)).add_suffix('xgb'))\ndft=dft.join(pd.DataFrame(fitpredict(rf)).add_suffix('rf'))\ndft=dft.join(pd.DataFrame(fitpredict(svm)).add_suffix('svm'))\ndft=dft.join(pd.DataFrame(fitpredict(knn)).add_suffix('knn'))","9b62863e":"dft.shape,test.shape","6c4994e4":"final_pred=model.predict(test.join(encodedt.add_suffix('enc'))[feature.columns])\n#final_pred=modelblend.predict(dft)","eb4620d4":"pd.Series(final_pred).value_counts()\/len(final_pred)*100","8ab7213b":"t=pd.read_csv('\/kaggle\/input\/Test.csv')\nsubmission=t[['ID']]\nsubmission['Segmentation']=final_pred","8e41315a":"common_id=np.intersect1d(train.ID,test.ID)","6b64ef95":"submission.set_index('ID',inplace=True)","d86db113":"submission.loc[common_id,'Segmentation']=np.array(train.set_index('ID').loc[common_id,'Segmentation'])","f6bf0c0c":"submission.reset_index(inplace=True)","6d494325":"submission.to_csv('Targetinc.csv',index=False)","a1df1532":"# If notebook is useful to you, Please UPVOTE.","ad52fa2e":"### Final model training.","b925dac9":"## Encoding.","f412a4da":"#### Lightgbm Tunning.","ed5ae2d7":"### Model validation","a036511e":"#### Xgboost tunning","00d65795":"feature=feature[selected_col]","67f52a4a":"## Dealing with missing values","dff9f782":"## Extracting features for models training","c9dfca01":"## Feature selection","034338e1":"## Grid Search for hyperparameter tunning","c26ddeb8":"## Final prediction","55a54e21":"# Problem statement and columns description.\n![Screenshot%20%2876%29.png](attachment:Screenshot%20%2876%29.png)","9ecf3871":"## Blending\nGiving less public score than the xgb without blending","96256505":"### Importing classes of model and instantiate","3490995c":"### Taking segmentation from trainset for common IDS","964a523c":"#### Target Encoding.","54f90b40":"## Filling test missing values.","03aa229b":"## Generating new features based on combinations.","9be5a6b4":"## Preliminary EDA to check common IDS in train and test set.\nAnd found that lots of ID in testset common to training set,this is the indication of duplicates of trainingset in testset.","ddc7dff0":"#### Label Encoding.","7bf583bc":"## Reading dataset"}}