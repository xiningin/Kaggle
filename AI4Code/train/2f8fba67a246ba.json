{"cell_type":{"9ecee93f":"code","fafa75b4":"code","b3d51208":"code","122312be":"code","056895ef":"code","c802d2d7":"code","7f56e1f6":"code","f7d9aefb":"code","f5b220f5":"code","33268d2b":"code","dbb32597":"markdown","4c0a5639":"markdown","c9bbf310":"markdown","d05fb53b":"markdown","b8414100":"markdown","0f1f0c25":"markdown","fd73f0bb":"markdown","26d4b5ba":"markdown"},"source":{"9ecee93f":"!pip install autoviml --no-cache-dir --upgrade\n### You can get the latest version from the Github\n#!pip install git+https:\/\/github.com\/AutoViML\/Auto_ViML.git","fafa75b4":"from autoviml.Auto_ViML import Auto_ViML","b3d51208":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","122312be":"# read data\ndf = pd.read_csv('\/kaggle\/input\/framingham-heart-study-dataset\/framingham.csv')\n\n# first glimpse at data\ndf.head(20)\n\n# data shape\ndf.shape\n\n# data types\ndf.dtypes","056895ef":"target = 'TenYearCHD'","c802d2d7":"# clarify what is y and what is x label\ny = df[target]\nX = df.drop([target], axis = 1)\nfrom sklearn.model_selection import train_test_split\n# divide train test: 80 % - 20 %\ntrain, test = train_test_split(df, test_size = 0.2, random_state=29)","7f56e1f6":"print(len(train))\nlen(test)","f7d9aefb":"m, feats, trainm, testm = Auto_ViML(train, target, test,\n                            sample_submission='',\n                            scoring_parameter='', KMeans_Featurizer=False,\n                            hyper_param='RS',feature_reduction=True,\n                             Boosting_Flag=False, Binning_Flag=False,\n                            Add_Poly=2, Stacking_Flag=True,Imbalanced_Flag=True,\n                            verbose=1)","f5b220f5":"### Test the Auto_ViML Model\nfrom sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\nmodel_name = 'Auto_ViML'\n# prediction = knn.predict(x_test)\nnormalized_df_knn_pred = testm[target+'_predictions']\ny_test = test[target].values\n\n# check accuracy: Accuracy:\u00a0Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)\/total\nacc = accuracy_score(y_test, normalized_df_knn_pred)\nprint(f\"The accuracy score for {model_name} is: {round(acc,3)*100}%\")\n\n# f1 score: The\u00a0F1 score\u00a0can be interpreted as a weighted average of the precision and recall, where an\u00a0F1 score\u00a0reaches its best value at 1 and worst score at 0.\nf1 = f1_score(y_test, normalized_df_knn_pred)\nprint(f\"The f1 score for {model_name} is: {round(f1,3)*100}%\")\n\n# Precision score:\u00a0When it predicts yes, how often is it correct? Precision=True Positive\/predicted yes\nprecision = precision_score(y_test, normalized_df_knn_pred)\nprint(f\"The precision score for {model_name} is: {round(precision,3)*100}%\")\n\n# recall score: True Positive Rate(Sensitivity or Recall):\u00a0When it\u2019s actually yes, how often does it predict yes? True Positive Rate =\u00a0True Positive\/actual yes\nrecall = recall_score(y_test, normalized_df_knn_pred)\nprint(f\"The recall score for {model_name} is: {round(recall,3)*100}%\")","33268d2b":"# Check overfit of the Auto_ViML Random Forests model\n# accuracy test and train\nX_test = testm[feats].values\ny_train = trainm[target].values\nX_train = trainm[feats].values\nacc_test = m.score(X_test, y_test)\nprint(\"The accuracy score of the test data is: \",acc_test*100,\"%\")\nacc_train = m.score(X_train, y_train)\nprint(\"The accuracy score of the training data is: \",round(acc_train*100,2),\"%\")\n\n","dbb32597":"# Heart Disease Prediction\n### Will a patient have a 10 year risk of developing a cardio vascular diseases?\n\n\n#### Target variable to predict: \n* 10 year risk of coronary heart disease (CHD) - (binary: \u201c1\u201d, means \u201cYes\u201d, \u201c0\u201d means \u201cNo\u201d)\n","4c0a5639":"## Test - Train Split <a name=\"paragraph3\"><\/a>","c9bbf310":"# This Notebook is to show how Auto_ViML can be used to get a better score\n## Please see Himanshu Sharma's Article on Medium to understand this dataset better\nhttps:\/\/towardsdatascience.com\/autoviml-automating-machine-learning-4792fee6ae1e","d05fb53b":"## Exploratory Data Analysis *<a name=\"paragraph1\"><\/a>*","b8414100":"### 1. Auto_ViML Model's Predictions are saved in the modified Test dataframe it returns","0f1f0c25":"# This notebook is a modification of this: Please review to compare Results\nhttps:\/\/www.kaggle.com\/lauriandwu\/machine-learning-heart-disease-framingham","fd73f0bb":"### Result: F-1 Score of Auto_ViML model using multiple settings are as follows:\n1. XGBoost = 38.7%\n2. CatBoost = 32.6%\n3. LogisticRegression = 35.8%\n4. Random Forest = 40.2%\n\n## Random Forests has the highest accuracy score on held out (Test) data","26d4b5ba":"# Let's Run Auto_ViML on Train and Test data here\n### Let's set the model to be Catboost and the Imbalanced Flag to be True"}}