{"cell_type":{"6d5b242a":"code","38498f80":"code","efacb656":"code","1f234361":"code","de63d9aa":"code","6c650c8a":"code","607f6163":"code","534bd4d1":"code","1f97fab5":"code","8c8a9df2":"code","bc7e8616":"code","ac74ed12":"code","c354cac7":"code","3c01eb77":"code","a2fc796b":"code","59b0cce7":"code","83bb88c3":"code","6cd69fe1":"code","fb37d407":"code","60140dc5":"code","932dc601":"code","de447e3b":"code","bf539844":"code","4d5a7916":"code","86f60495":"code","68ac42e6":"code","0c78118b":"code","7ab8498a":"code","972fca08":"code","e52dce0a":"code","9d253e39":"code","86db10ce":"code","f78638d8":"code","0f9f5382":"code","3b39a759":"code","14258191":"code","3269463b":"code","88a2c866":"code","270e2a44":"code","8f726d6b":"code","25e083ed":"code","ad8770fa":"code","eff474bf":"code","d5d7c555":"code","0a5a3736":"code","d7e5fdda":"code","dae7704d":"code","ff8ec326":"code","2a77be76":"code","e926aebd":"code","605e0ac3":"code","398457c9":"code","33754642":"code","4607d5df":"code","96b17ae1":"code","6f3cff56":"code","6295d8e6":"code","7e4ff169":"code","b6747d21":"code","5738a11c":"code","55b0052f":"code","7e902272":"code","92371f78":"code","74eb3a55":"code","bd5f831d":"code","63da3dff":"markdown","9bc1a71c":"markdown","ef44b75e":"markdown","11426115":"markdown","b731b721":"markdown","a99fb033":"markdown","360f7ee9":"markdown","44a32f40":"markdown","b29b47d3":"markdown","9ae264e2":"markdown","90ef1051":"markdown","a7fc276f":"markdown","4de681c4":"markdown","a0ebf415":"markdown","e5958eb5":"markdown","94fb6faa":"markdown","81cee0db":"markdown","b977ea24":"markdown","7b7c0ccc":"markdown","afb38324":"markdown","7292a9b7":"markdown","92f0ad5c":"markdown"},"source":{"6d5b242a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualizations\nimport seaborn as sns # data visualizations\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","38498f80":"import matplotlib.pyplot as plt\nfrom IPython.display import display, HTML, Markdown # provides markdown and display functionalities\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","efacb656":"plt.style.use('bmh')","1f234361":"# Configure notebook display settings to only use 2 decimal places, makes the notebook looks better...\ndef set_notebbok_properties():\n    \"\"\"\n    Set the notebooks parameters decimal places, number of rows and columns to display \n    for easy visualization of the data.\n    Args: \n        None\n    Return: \n        None    \n    \"\"\"\n    pd.options.display.float_format = '{:,.2f}'.format\n    pd.set_option('display.max_columns', 15) \n    pd.set_option('display.max_rows', 50) \n\n# Invoke the set_notebbok_properties function\nset_notebbok_properties()","de63d9aa":"# Set notebook parameters...\nTRAIN_PATH = '\/kaggle\/input\/titanic\/train.csv'\nTEST_PATH  = '\/kaggle\/input\/titanic\/test.csv'\nSUB_PATH   = '\/kaggle\/input\/titanic\/gender_submission.csv'","6c650c8a":"# Load the train and test dataset into a Pandas dataframe.\ntrain = pd.read_csv(TRAIN_PATH)\ntest  = pd.read_csv(TEST_PATH)\nsub   = pd.read_csv(SUB_PATH)","607f6163":"# Display dataframe information as the number of variables and data types.\ntrain.info()","534bd4d1":"# Understand some of the data loaded...\n# Display the first few rows of the dataframe.\ntrain.head()","1f97fab5":"# Understand some of the data loaded...\n# Display the first few rows of the dataframe.\ntest.head()","8c8a9df2":"# Provide a summary of the numerical information...\ntrain.describe()","bc7e8616":"# Provide a summary of the categorical information...\ncateg_variables = []\nfields_to_display = 5\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ntmp = train.select_dtypes(exclude = numerics)\nfor col in tmp.columns:\n    categ_variables.append(col)\n    pct_valid_data = 1 - (tmp[col].isnull().sum() \/ tmp.shape[0])\n    unique_fields = list(set(tmp[col]))\n    print('Variable Name: {:<18} % Data: {:0.2f} # Unique Fields:{:<6} Samples: {}'.format(col,pct_valid_data, len(unique_fields), unique_fields[:fields_to_display]))","ac74ed12":"# Scan the dataset for missing values...\ntrain.isnull().sum()","c354cac7":"# Mapping the data to Dictionary descriptions...\ntrain['Survived'] = train['Survived'].map({0:'No', 1: 'Yes'})\ntrain['Pclass']   = train['Pclass'].map({1:'1st', 2:'2nd', 3:'3rd'})\ntrain['Embarked'] = train['Embarked'].map({'C':'Cherbourg', 'Q':'Queenstown', 'S':'Southampton'})\n\ntest['Pclass']    = test['Pclass'].map({1:'1st', 2:'2nd', 3:'3rd'})\ntest['Embarked']  = test['Embarked'].map({'C':'Cherbourg', 'Q':'Queenstown', 'S':'Southampton'})","3c01eb77":"# Replacing NaN by defaults\ndef fill_with(df, feature_list = ['Embarked'], default_value = ['Other']):\n    for idx, feat in enumerate(feature_list):\n        df[feat] =  df[feat].fillna(default_value[idx])\n    return df\n\ntrain = fill_with(train, feature_list=['Embarked'], default_value = ['Other'])","a2fc796b":"# Verified that the function worked properly in the Dataframe...\ntrain.head()","59b0cce7":"# Understanding data balance.\nnot_survive_pct = train[train['Survived'] == 'No'].shape[0] \/ train.shape[0]\nsurvive_pct = train[train['Survived'] == 'Yes'].shape[0] \/ train.shape[0]\nprint(f'{not_survive_pct * 100: .1f} % Not Survived')\nprint(f'{survive_pct * 100: .1f} % Survived')","83bb88c3":"# Constructing a simple plot function\ndef simple_plt(df, variable = 'Survived'):\n    fig, axs = plt.subplots(1,2,figsize = (11,3))\n    values = (df.groupby(variable)['PassengerId'].count().reset_index(0))\n    survived = (df[df['Survived'] == 'Yes'].groupby(variable)['PassengerId'].count().reset_index(0))\n    print(survived['PassengerId'] \/ values['PassengerId'])\n    axs[0].bar(values[variable], values['PassengerId'])\n    axs[1].bar(survived[variable], survived['PassengerId'] \/ values['PassengerId'])\n    fig.suptitle('Total Passengers By: ' + str(variable))\n    axs[0].tick_params(labelrotation=-45)\n    plt.show()\n\n\nsimple_plt(train, variable='Survived')\nsimple_plt(train, variable='Pclass')\nsimple_plt(train, variable='Embarked')","6cd69fe1":"def calc_survival_ratio(df, category = 'Sex'):\n    for group in list(df[category].unique()):\n        print(group)\n        total_by_category = df[(df[category] == group)].shape[0]\n        print('total pasengers by category:', df[(df[category] == group)].shape[0])\n        if total_by_category != 0:\n            survive_pct = df[(df[category] == group) & (df['Survived'] == 1)].shape[0] \/ total_by_category\n            not_survive_pct = df[(df[category] == group) & (df['Survived'] == 0)].shape[0] \/ total_by_category\n        else:\n            survive_pct = -0.99\n            not_survive_pct = -0.99\n        \n        print(f'{survive_pct * 100: .1f} % Survived')\n        print(f'{not_survive_pct * 100: .1f} % Not Survived')\n        print('.'*10)\n\ncalc_survival_ratio(train, category = 'Sex')","fb37d407":"# There is to many Cabins so before analyzing we will creare a Cabin sector feature.\ntrain['Cabin_Group'] = train['Cabin'].str[:1]\ncalc_survival_ratio(train, category = 'Cabin_Group')","60140dc5":"# Understanding survival rate by Enbarked\ncalc_survival_ratio(train, category = 'Embarked')","932dc601":"# Understanding the amount of NaNs\ntrain.isnull().sum()","de447e3b":"# Populating the Age field vased on other fields, visualizations\ntrain[train['Age'].isnull()].head(5)","bf539844":"# Creating a function to fill NaNs with the mean or the mode of the distribution of the variable\ndef numeric_fill_nan(df, variable = 'Age', group = 'Sex'):\n    df[variable] = df[variable].fillna(df.groupby(group)[variable].transform('mean'))\n    return df\n\ndef categ_fill_nan(df, variable = 'Cabin', group = 'Embarked'):\n    df[variable] = df.groupby(group)[variable].apply(lambda x: x.fillna(x.mode()[0]))\n    return df\n\n# Apply both of the fill functions to complete the missing informations....\ntrain = numeric_fill_nan(train, variable = 'Age', group = 'Sex') # fill the missing ages with average by sex group...\ntrain = categ_fill_nan(train, variable = 'Cabin', group = 'Embarked') # fill the missing cabin by embarked group...\n\n# Apply both of the fill functions to complete the missing informations....\ntest = numeric_fill_nan(test, variable = 'Age', group = 'Sex') # fill the missing ages with average by sex group...\ntest = categ_fill_nan(test, variable = 'Cabin', group = 'Embarked') # fill the missing cabin by embarked group...\n\n# Recalculated the cabin group for updated missing data...\ntrain['Cabin_Group'] = train['Cabin'].str[:1]\ntest['Cabin_Group'] = test['Cabin'].str[:1]","4d5a7916":"# All the NaNs has been filled with means and median values based on groups\ntrain.isnull().sum()","86f60495":"# Looking for outliers using the train dataset describe pandas function...\ntrain.describe()","68ac42e6":"def create_hist(df, variable = 'Fare', bins = 10):\n    plt.hist(train[variable], bins = 10)\n    plt.title('Histogram Analysis:: ' + variable)\n    plt.show()\n    \ncreate_hist(train, variable = 'Fare', bins = 12)\ncreate_hist(train, variable = 'Age', bins = 12)","0c78118b":"def remove_by_IQR(df, variables = ['Age']):\n    for var in variables:\n        print('Analysing: ', var, '...')\n        pct_25, pct_75 = df[var].quantile([0.25,0.75]) \n        IQR = pct_75 - pct_25\n        mean = df[var].mean()\n        upper = mean + 1.5 * IQR\n        lower = mean - 1.5 * IQR\n        tmp = df[(df[var] >= lower) & (df[var] <= upper)]\n        outliers_removed = df.shape[0] - tmp.shape[0]\n        print('Lower Limit: ', lower)\n        print('Upper Limit: ', upper)\n        print('Outliers Removed: ', outliers_removed)\n        print('---')\n    return tmp\n\n#train = remove_by_IQR(train, variables=['Age', 'Fare'])","7ab8498a":"train.describe()","972fca08":"# Quick visulaization of the dataset...\ntrain.sample(10)","e52dce0a":"# Cretaing simple categorical features...\n# Cabin group\ntrain['Cabin_Group'] = train['Cabin'].str[:1]\ntest['Cabin_Group'] = test['Cabin'].str[:1]","9d253e39":"# Creating a Ticket number feature...\ntrain['TicketNumber'] = train['Ticket'].str.split()\ntrain['TicketNumber'] = train['TicketNumber'].str[-1]\n\ntest['TicketNumber'] = test['Ticket'].str.split()\ntest['TicketNumber'] = test['TicketNumber'].str[-1]","86db10ce":"# Creating family size...\ntrain['FamilySize'] = train['SibSp'] + train['Parch'] + 1\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1","f78638d8":"# Is alone...\ntrain['IsAlone'] = train['FamilySize'].apply(lambda x: 1 if x == 1 else 0)\ntest['IsAlone'] = test['FamilySize'].apply(lambda x: 1 if x == 1 else 0)","0f9f5382":"# Extract the title...\ntrain['Title'] = train['Name'].str.extract(' ([A-Za-z]+)\\.')\ntrain['Title'] = train['Title'].replace(['Ms', 'Mlle'], 'Miss')\ntrain['Title'] = train['Title'].replace(['Mme', 'Countess', 'Lady', 'Dona'], 'Mrs')\ntrain['Title'] = train['Title'].replace(['Dr', 'Major', 'Col', 'Sir', 'Rev', 'Jonkheer', 'Capt', 'Don'], 'Mr')\n\ntest['Title'] = test['Name'].str.extract(' ([A-Za-z]+)\\.')\ntest['Title'] = test['Title'].replace(['Ms', 'Mlle'], 'Miss')\ntest['Title'] = test['Title'].replace(['Mme', 'Countess', 'Lady', 'Dona'], 'Mrs')\ntest['Title'] = test['Title'].replace(['Dr', 'Major', 'Col', 'Sir', 'Rev', 'Jonkheer', 'Capt', 'Don'], 'Mr')","3b39a759":"# Mapping Age\ntrain.loc[ train['Age'] <= 16, 'Age_Class'] = 0\ntrain.loc[(train['Age'] > 16) & (train['Age'] <= 32), 'Age_Group'] = 1\ntrain.loc[(train['Age'] > 32) & (train['Age'] <= 48), 'Age_Group'] = 2\ntrain.loc[(train['Age'] > 48) & (train['Age'] <= 64), 'Age_Group'] = 3\ntrain.loc[ train['Age'] > 64, 'Age_Group'] = 4 ;\n\n# Mapping Age\ntest.loc[ test['Age'] <= 16, 'Age_Class'] = 0\ntest.loc[(test['Age'] > 16) & (test['Age'] <= 32), 'Age_Group'] = 1\ntest.loc[(test['Age'] > 32) & (test['Age'] <= 48), 'Age_Group'] = 2\ntest.loc[(test['Age'] > 48) & (test['Age'] <= 64), 'Age_Group'] = 3\ntest.loc[ test['Age'] > 64, 'Age_Group'] = 4 ;","14258191":"# Categorical Fare\ntrain['Fare_Group'] = pd.qcut(train['Fare'], 6, [0,1,2,3,4,5])\ntest['Fare_Group'] = pd.qcut(test['Fare'], 6, [0,1,2,3,4,5])","3269463b":"# Quick visulaization of the dataset...\ntrain.head(10)","88a2c866":"import pandas\nfrom sklearn import preprocessing","270e2a44":"# Before encoding the variables we are going to combine the train and test datasets\n# this will ensure to generate better encodes in case variables are present in the test and not the train\nprint('Train Size:', train.shape[0])\nprint('Test Size:', test.shape[0])\ntrain['is_train'] = 'Yes'\ntest['is_train'] = 'No'\n\ndataset = train.append(test, ignore_index=True)\nprint('Train + Test Size:', dataset.shape[0])\n\ndataset[dataset['is_train'] == 'No'].sample(10)","8f726d6b":"def encode_variables(df, categ_cols = ['Survived']):\n    label_object = {}\n    for col in categ_cols:\n        labelencoder = preprocessing.LabelEncoder()\n        labelencoder.fit(df[col])\n        df[col + '_Enc'] = labelencoder.fit_transform(df[col])\n        label_object[col] = labelencoder\n    return df\n\n# label_object['Product'].inverse_transform(df['Product'])","25e083ed":"categ_cols = ['Pclass', \n              'Sex', \n              'Cabin', \n              'Embarked', \n              'Cabin_Group',\n              'Title',\n              'TicketNumber'\n             ]\ndataset = encode_variables(dataset, categ_cols = categ_cols)","ad8770fa":"dataset.head()","eff474bf":"from sklearn.model_selection import train_test_split\nencoded_categ_features = [feat for feat in dataset.columns if 'Enc' in feat]\nnumerical_feat = ['Age', \n                  'Fare', \n                  'SibSp', \n                  'Parch', \n                  'FamilySize', \n                  #'Age_Group',\n                  #'Fare_Group',\n                  'IsAlone']\n\n\nfeatures = encoded_categ_features + numerical_feat\nX = dataset[dataset['is_train'] == 'Yes']\nX_test = dataset[dataset['is_train'] == 'No']\n\n\ndataset['Survived'] = dataset['Survived'].map({'No':0, 'Yes':1})\nX['Survived'] = X['Survived'].map({'No':0, 'Yes':1})\ny = dataset[dataset['is_train'] == 'Yes']['Survived']\n\nX_train, X_val, y_train, y_val = train_test_split(X[features], y, test_size=0.25, random_state=42)","d5d7c555":"# Model Features\nprint(features)","0a5a3736":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nlogreg = LogisticRegression(random_state=42, max_iter=10000)\nlogreg.fit(X_train, y_train)\ny_pred=logreg.predict(X_val)\n\n\ncnf_matrix = metrics.confusion_matrix(y_val, y_pred)\ncnf_matrix","d7e5fdda":"class_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","dae7704d":"print(\"Accuracy:\",metrics.accuracy_score(y_val, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_val, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_val, y_pred))","ff8ec326":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(max_depth=6, random_state=42)\nclf = clf.fit(X_train,y_train)\ny_pred = clf.predict(X_val)\n\ncnf_matrix = metrics.confusion_matrix(y_val, y_pred)\ncnf_matrix","2a77be76":"class_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","e926aebd":"print(\"Accuracy:\",metrics.accuracy_score(y_val, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_val, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_val, y_pred))","605e0ac3":"from sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.model_selection import KFold\nfrom lightgbm import LGBMClassifier\nimport gc","398457c9":"# Modeling\nfolds = KFold(n_splits=10, shuffle=True, random_state=42)\noof_preds = np.zeros(X.shape[0])\nsub_preds = np.zeros(test.shape[0])\n\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(X)):\n    trn_x, trn_y = X[features].iloc[trn_idx], y.iloc[trn_idx]\n    val_x, val_y = X[features].iloc[val_idx], y.iloc[val_idx]\n    \n    clf = LGBMClassifier(\n        n_estimators     = 5000,\n        learning_rate    = 0.05,\n        num_leaves       = 128,\n        colsample_bytree = 0.8,\n        subsample        = 0.9,\n        max_depth        = 25,\n        reg_alpha        = 0.1,\n        reg_lambda       = 0.1,\n        min_split_gain   = 0.01,\n        min_child_weight = 2\n    )\n    \n    clf.fit(trn_x, trn_y, \n            eval_set= [(trn_x, trn_y), (val_x, val_y)], \n            eval_metric='auc', verbose=50, early_stopping_rounds=250\n           )\n    \n    oof_preds[val_idx] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)[:,1]\n    sub_preds += clf.predict_proba(X_test[features], num_iteration=clf.best_iteration_)[:,1] \/ folds.n_splits\n    \n    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n    del clf, trn_x, trn_y, val_x, val_y\n    gc.collect()\nprint('\\n')    \nprint('Full Accuracy score %.6f' % roc_auc_score(y, oof_preds))   ","33754642":"from bayes_opt import BayesianOptimization\nimport lightgbm as lgb\nRandom_Seed = 4520","4607d5df":"#cut tr and val\nbayesian_tr_idx, bayesian_val_idx = train_test_split(X, test_size = 0.25, random_state = 42, stratify = X['Survived'])\nbayesian_tr_idx = bayesian_tr_idx.index\nbayesian_val_idx = bayesian_val_idx.index","96b17ae1":"# Take the hyper parameters you want to consider\nparamsLGB = {\n    'learning_rate': (0.01,0.1),\n    'num_leaves': (50, 5000), \n    'bagging_fraction' : (0.1, 0.9),\n    'feature_fraction' : (0.1, 0.9),\n    'min_child_weight': (0.00001, 0.01),   \n    'min_data_in_leaf': (20, 140),\n    'max_depth':(-1,200),\n    'reg_alpha': (1, 2), \n    'reg_lambda': (1, 2) \n}","6f3cff56":"def LGB_bayesian(learning_rate, num_leaves, bagging_fraction, feature_fraction, min_child_weight, min_data_in_leaf, max_depth, reg_alpha, reg_lambda):\n    # LightGBM expects next three parameters need to be integer. \n    num_leaves       = int(num_leaves)\n    min_data_in_leaf = int(min_data_in_leaf)\n    max_depth        = int(max_depth)\n    \n    # Catch the errors \n    assert type(num_leaves) == int\n    assert type(min_data_in_leaf) == int\n    assert type(max_depth) == int\n    \n    param = {\n              'num_leaves': num_leaves, \n              'min_data_in_leaf': min_data_in_leaf,\n              'min_child_weight': min_child_weight,\n              'bagging_fraction' : bagging_fraction,\n              'feature_fraction' : feature_fraction,\n              'learning_rate' : learning_rate,\n              'max_depth': max_depth,\n              'reg_alpha': reg_alpha,\n              'reg_lambda': reg_lambda,\n              'objective': 'binary',\n              'save_binary': True,\n              'seed': Random_Seed,\n              'feature_fraction_seed': Random_Seed,\n              'bagging_seed': Random_Seed,\n              'drop_seed': Random_Seed,\n              'data_random_seed': Random_Seed,\n              'boosting_type': 'gbdt',\n              'verbose': 1,\n              'is_unbalance': False,\n              'boost_from_average': True,\n              'metric':'auc'\n    }    \n    \n    oof = np.zeros(len(X))\n    trn_data= lgb.Dataset(X.iloc[bayesian_tr_idx][features].values, label=X.iloc[bayesian_tr_idx]['Survived'].values)\n    val_data= lgb.Dataset(X.iloc[bayesian_val_idx][features].values, label=X.iloc[bayesian_val_idx]['Survived'].values)\n    \n    clf = lgb.train(param, trn_data,  num_boost_round=50, valid_sets = [trn_data, val_data], verbose_eval=0, early_stopping_rounds = 50)\n    oof[bayesian_val_idx]  = clf.predict(X.iloc[bayesian_val_idx][features].values, num_iteration=clf.best_iteration)  \n    score = roc_auc_score(X.iloc[bayesian_val_idx]['Survived'].values, oof[bayesian_val_idx])\n\n    return score","6295d8e6":"LGB_BO = BayesianOptimization(LGB_bayesian, paramsLGB, random_state=42)","7e4ff169":"print(LGB_BO.space.keys)","b6747d21":"init_points = 15\nn_iter = 30\n\nprint('-' * 130)\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)","5738a11c":"LGB_BO.max['target']","55b0052f":"LGB_BO.max['params']","7e902272":"param_lgb = {\n        'min_data_in_leaf': int(LGB_BO.max['params']['min_data_in_leaf']), \n        'num_leaves': int(LGB_BO.max['params']['num_leaves']), \n        'learning_rate': LGB_BO.max['params']['learning_rate'],\n        'min_child_weight': LGB_BO.max['params']['min_child_weight'],\n        'bagging_fraction': LGB_BO.max['params']['bagging_fraction'], \n        'feature_fraction': LGB_BO.max['params']['feature_fraction'],\n        'reg_lambda': LGB_BO.max['params']['reg_lambda'],\n        'reg_alpha': LGB_BO.max['params']['reg_alpha'],\n        'max_depth': int(LGB_BO.max['params']['max_depth']), \n        'objective': 'binary',\n        'save_binary': True,\n        'seed': Random_Seed,\n        'feature_fraction_seed': Random_Seed,\n        'bagging_seed': Random_Seed,\n        'drop_seed': Random_Seed,\n        'data_random_seed': Random_Seed,\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'is_unbalance': False,\n        'boost_from_average': True,\n        'metric':'auc'\n    }","92371f78":"# Modeling\nfolds = KFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = np.zeros(X.shape[0])\nsub_preds = np.zeros(test.shape[0])\n\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(X)):\n    trn_x, trn_y = X[features].iloc[trn_idx], y.iloc[trn_idx]\n    val_x, val_y = X[features].iloc[val_idx], y.iloc[val_idx]\n    \n    clf = LGBMClassifier(\n        n_estimators     = param_lgb['num_leaves'],\n        learning_rate    = param_lgb['learning_rate'],\n        num_leaves       = param_lgb['num_leaves'],\n        colsample_bytree = param_lgb['bagging_fraction'],\n        subsample        = param_lgb['feature_fraction'],\n        max_depth        = param_lgb['max_depth'],\n        reg_alpha        = param_lgb['reg_alpha'],\n        reg_lambda       = param_lgb['reg_lambda'],\n        min_split_gain   = param_lgb['min_data_in_leaf'],\n        min_child_weight = param_lgb['min_child_weight']\n    )\n    clf.fit(trn_x, trn_y, \n            eval_set= [(trn_x, trn_y), (val_x, val_y)], \n            eval_metric='auc', verbose=50, early_stopping_rounds=250\n           )\n    \n    oof_preds[val_idx] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)[:,1]\n    sub_preds += clf.predict_proba(X_test[features], num_iteration=clf.best_iteration_)[:,1] \/ folds.n_splits\n    \n    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n    del clf, trn_x, trn_y, val_x, val_y\n    gc.collect()\nprint('\\n')    \nprint('Full Accuracy score %.6f' % roc_auc_score(y, oof_preds))  ","74eb3a55":"sub['Survived'] = np.round(sub_preds).astype(int)\nsub.sample(10)","bd5f831d":"sub.to_csv('submission.csv',index= False)","63da3dff":"---","9bc1a71c":"---","ef44b75e":"### 6.2 - Hyper-Parameter Optimization...","11426115":"### 5.3 Training a Desicion Tree...","b731b721":"### 3.2 - Removing Outliers\nIn this section I will try to remove some of the outliers by looking some of the data statistical distributions...","a99fb033":"### 4.2 - Label Encoding Categorical variables","360f7ee9":"From a simple data that I observed the only variable that called my attection was the Fare, I will use some Hist to understand better the variable behavior","44a32f40":"### 6.1 - Light GBM Model with Cross Validation","b29b47d3":"#### Variable Notes\n**pclass:** A proxy for socio-economic status (SES)\n* 1st = Upper\n* 2nd = Middle\n* 3rd = Lower\n\n**age:** Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n**sibsp:** The dataset defines family relations in this way...\n\n* **Sibling =** brother, sister, stepbrother, stepsister\n\n* **Spouse =** husband, wife (mistresses and fianc\u00e9s were ignored)\n\n**parch:** The dataset defines family relations in this way...\n\n* **Parent =** mother, father\n\n* **Child =** daughter, son, stepdaughter, stepson\n\n* Some children travelled only with a nanny, therefore parch=0 for them.","9ae264e2":"### 5.2 Training a Logistic Regression...","90ef1051":"#### Data Dictionary\n\n\n|Variable|Definition|Key|\n| ----------- | ----------- | ----------- |\n|survival|Survival|0 = No, 1 = Yes|\n|pclass|Ticket class|1 = 1st, 2 = 2nd, 3 = 3rd|\n|sex|Sex||\n|Age|Age in years||\n|sibsp|# of siblings \/ spouses aboard the Titanic||\n|parch|# of parents \/ children aboard the Titanic||\n|ticket|Ticket number||\n|fare|Passenger fare||\n|cabin|Cabin number||\n|embarked|Port of Embarkation|C = Cherbourg, Q = Queenstown, S = Southampton|","a7fc276f":"## 2.0 - Exploratory Data Analysis\nThe following descriptions will be quite usefull once looking at the data, I will use the Dictionaries provided in the Dataset Descriptions to Map the variables","4de681c4":"## 3.0 - Data Preprocessing\n### 3.1 - Filling NaN\nIn this section we will be filling some of the NaNs values with logical information from mean values or the most popular values in the field...","a0ebf415":"# Exploring Survival Probabilities \/ Titanic + Model and Hyper Parameter Optimization\n\n**August 22, 2021**\n\n\nHi! Kaggle <br> \nIn this notebook I try to separate the majority of my code in functions this will allow to modularize the code for easy modification and overal organization <br> I find this type of strategy helpfull so I decide to share it...\n\n\n**Notebook \/ Analysis Components:**\n1. Loading the train and test datasets.\n    * Visualize the loaded information.\n2. Exploratory data analysis. \n    * Plotting some of the variables\n    * Understanding the Target distribution\n3. Data pre-processing (oulier identification, missing values, label encoding).\n    * Filling NaNs.\n    * Removing outliers.\n4. Feature Engineering (Numeric variables, Categorical variables (ordinal and numerical), Text variables).\n    * Simple features.\n    * Label encoding\n5. Baseline model.\n    * Building a simple logistic regression model.\n6. Advance Model.\n    * Building a cross validation LGBM model.\n    * Hyper-Parameter optimization\n7. Submission.\n    * Creating a submission file and score to the leaderboard.","e5958eb5":"---","94fb6faa":"## 5.0 - Simple Model and Test Strategy...\n### 5.1 - Separating the data in Train and Test...","81cee0db":"---","b977ea24":"%%html\n<style>\ntable {float:left}\n<\/style>","7b7c0ccc":"## 7.0 - Model Submission","afb38324":"## 6.0 - Advanced LGBM + Cross Validation...","7292a9b7":"## 4 - Feature Engineering...\n### 4.1 - Simple Feature Construction...","92f0ad5c":"## 1 - Loading the train and test datasets."}}