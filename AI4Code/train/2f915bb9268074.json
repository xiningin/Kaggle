{"cell_type":{"4f7ef982":"code","d6ace34a":"code","cd1ad637":"code","6bcfb244":"code","c6a66a40":"code","2f634c50":"code","3b654af7":"code","928ca9c6":"code","cd4d6a24":"code","cfba5d36":"code","717feee5":"code","438cd520":"code","7b4ea62b":"markdown","c3b7482c":"markdown","50755535":"markdown","4a9a7559":"markdown","89300607":"markdown","be2c8dbf":"markdown"},"source":{"4f7ef982":"# Import libraries necessary for this project\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import ShuffleSplit\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n%matplotlib inline\n\n","d6ace34a":"# Load the Boston housing dataset\ndata = pd.read_csv('..\/input\/housing.csv')\nprices = data['MEDV']\nfeatures = data.drop('MEDV', axis = 1)\n    \n# Success\nprint(\"Boston housing dataset has {} data points with {} variables each.\".format(*data.shape))","cd1ad637":"data.head()","6bcfb244":"data.info()","c6a66a40":"data.describe().transpose()","2f634c50":"# TODO: Minimum price of the data\nminimum_price = prices.min()\n\n# TODO: Maximum price of the data\nmaximum_price = prices.max()\n\n# TODO: Mean price of the data\nmean_price = prices.mean()\n\n# TODO: Median price of the data\nmedian_price = prices.median()\n\n# TODO: Standard deviation of prices of the data\nstd_price = prices.std()\n\n# Show the calculated statistics\nprint(\"Statistics for Boston housing dataset:\\n\")\nprint(\"Minimum price: ${}\".format(minimum_price)) \nprint(\"Maximum price: ${}\".format(maximum_price))\nprint(\"Mean price: ${}\".format(mean_price))\nprint(\"Median price ${}\".format(median_price))\nprint(\"Standard deviation of prices: ${}\".format(std_price))","3b654af7":"corr= data.corr()\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\nsns.heatmap(corr  , cmap = cmap , vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","928ca9c6":"# TODO: Import 'r2_score'\nfrom sklearn.metrics import r2_score \ndef performance_metric(y_true, y_predict):\n    \"\"\" Calculates and returns the performance score between \n        true and predicted values based on the metric chosen. \"\"\"\n    \n    # TODO: Calculate the performance score between 'y_true' and 'y_predict'\n    score = r2_score(y_true, y_predict)\n    \n    # Return the score\n    return score","cd4d6a24":"# Calculate the performance of this model\nscore = performance_metric([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\nprint(\"Model has a coefficient of determination, R^2, of {:.3f}.\".format(score))","cfba5d36":"# TODO: Import 'train_test_split'\nfrom sklearn.model_selection import train_test_split\n# TODO: Shuffle and split the data into training and testing subsets\nX_train, X_test, y_train, y_test = train_test_split(features, prices, test_size = 0.2 , random_state = 1111)\n\n# Success\nprint(\"Training and testing split was successful.\")","717feee5":"# TODO: Import 'make_scorer', 'DecisionTreeRegressor', and 'GridSearchCV'\nfrom sklearn.metrics import make_scorer\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\n\ndef fit_model(X, y):\n    \"\"\" Performs grid search over the 'max_depth' parameter for a \n        decision tree regressor trained on the input data [X, y]. \"\"\"\n    \n    # Create cross-validation sets from the training data\n    cv_sets = ShuffleSplit(n_splits = 10, test_size = 0.20, random_state = 0)\n\n    # TODO: Create a decision tree regressor object\n    regressor = DecisionTreeRegressor(random_state=0)\n\n    # TODO: Create a dictionary for the parameter 'max_depth' with a range from 1 to 10\n    params = {'max_depth':[1,10]}\n\n    # TODO: Transform 'performance_metric' into a scoring function using 'make_scorer' \n    scoring_fnc = make_scorer(performance_metric)\n\n    # TODO: Create the grid search cv object --> GridSearchCV()\n    # Make sure to include the right parameters in the object:\n    # (estimator, param_grid, scoring, cv) which have values 'regressor', 'params', 'scoring_fnc', and 'cv_sets' respectively.\n    grid = GridSearchCV(estimator = regressor , param_grid = params , scoring = scoring_fnc , cv = cv_sets)\n\n    # Fit the grid search object to the data to compute the optimal model\n    grid = grid.fit(features, prices)\n\n    # Return the optimal model after fitting the data\n    return grid.best_estimator_","438cd520":"# Fit the training data to the model using grid search\nreg = fit_model(X_train, y_train)\n\n# Produce the value for 'max_depth'\nprint(\"Parameter 'max_depth' is {} for the optimal model.\".format(reg.get_params()['max_depth']))","7b4ea62b":"### Learning Curves\n","c3b7482c":"## Some Statistics","50755535":"## Checking Correlation with Visualization","4a9a7559":"Note: template and datset are from Machine Learning Nanodegree Project","89300607":"No Need to check for missing data","be2c8dbf":"# My Submission for Boston Housing Prices Project\n\n"}}