{"cell_type":{"abdcb316":"code","80cf7c68":"code","b171c678":"code","614835ac":"markdown","5b912c30":"markdown","3937e517":"markdown","da65aa11":"markdown","a20dbb9e":"markdown","2258d818":"markdown"},"source":{"abdcb316":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom pathlib import Path\nimport os\n\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoTokenizer,\n    AutoModel,\n    TFAutoModel\n)\n\nWORKING_DIR = Path(\"\/kaggle\/working\")\n\nprint('Transformers version',transformers.__version__) # Current version: 2.3.0","80cf7c68":"def transformers_model_dowloader(pretrained_model_name_list = ['bert-base-uncased'], is_tf = True):\n    model_class = AutoModel\n    \n    if is_tf:\n        model_class = TFAutoModel\n\n    for i, pretrained_model_name in enumerate(pretrained_model_name_list):\n        print(i,'\/',len(pretrained_model_name_list))\n        print(\"Download model and tokenizer\", pretrained_model_name)\n        transformer_model = model_class.from_pretrained(pretrained_model_name)\n        transformer_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n\n        NEW_DIR = WORKING_DIR \/ pretrained_model_name\n\n        try:\n            os.mkdir(NEW_DIR)\n        except OSError:\n            print (\"Creation of directory %s failed\" % NEW_DIR)\n        else:\n            print (\"Successfully created directory %s \" % NEW_DIR)\n\n        print(\"Save model and tokenizer\", pretrained_model_name, 'in directory', NEW_DIR)\n        transformer_model.save_pretrained(NEW_DIR)\n        transformer_tokenizer.save_pretrained(NEW_DIR)","b171c678":"pretrained_model_name_list = ['roberta-base', 'roberta-large', 'roberta-large-mnli', 'distilroberta-base']\n#pretrained_model_name_list = ['roberta-base']\ntransformers_model_dowloader(pretrained_model_name_list, is_tf = False)","614835ac":"**If you found this kernel interesting, please consider upvoting it! **\n\n**If you have any questions, other ideas or suggestions, please share it in the comments!**","5b912c30":"# transformers_model_dowloader\n`pretrained_model_name_list`: List of shortcut names of the pretrained models (available [here](https:\/\/huggingface.co\/transformers\/pretrained_models.html) and [here](https:\/\/huggingface.co\/models)).\n\n`is_tf` : `True`\/`False` to choose between TF2.0\/Pytorch version of the models.","3937e517":"# Example\n!!! Be careful that you have max 4.9GB of memory on Disk. Therefore, don't add too many pretrained models if you don't want to have memory errors !!!","da65aa11":"# Easy and fast way to switch between transformers models in \"Notebook-only competition\".\n\n**N.B: Remember the upvote button is next to the fork button, and it's free too! \ud83d\ude09**\n\nThe amazing [Hugging Face transformers library](https:\/\/huggingface.co\/transformers\/v2.3.0\/) proposes over 32+ pretrained models (list available [here](https:\/\/huggingface.co\/transformers\/pretrained_models.html) or [here](https:\/\/huggingface.co\/models)). Therefore, it becomes sometimes hard to choose between all these models without testing and comparing them.\nOf course, transformers propose an easy way to switch from one model to another but without internet connection activated - like in \"Notebook-only competition\" - the operations become less straightforward. Indeed, if you want to see your score on LB, you will have to upload the pretrained models as a Kaggle dataset.\n\n-----------\n\nIn this notebook, I will show you an easy and fast way to:\n1. Chooses your favorite models, \n2. Download and store them,\n3. Download them - in another notebook(with internet Off) - as Kaggle dataset for **train\/fine-tune** and **inference**.\n\n-----------\n\nIt is worth noting that these presented methods can be used for the \"final phase\" where you want to use the notebook exclusively to perform inference(to save GPU time). In this case, the steps should be :\n1. Chooses your favorite models,\n2. Download them,\n3. **Train\/Fine-tune** them,\n4. Store them,\n5. Download them - in another notebook(with internet Off) - as Kaggle dataset only for **inference**.\n-----------","a20dbb9e":"# Imports and cst definition","2258d818":"# How to Download the models in another notebook?\nIn this section, I describe 3 options to download the saved models as a dataset in another notebook (that has internet Off). For more clarity, I will call this kernel, the kernel_On and the other the kernel_Off (that has internet Off). Also, I made another kernel representing the kernel_Off available [here](https:\/\/www.kaggle.com\/maroberti\/load-test-transformers-model-downloader).\n## Option 1: Through \"Kernel Output Files\"\nThis is the fastest way to get your models. To do so, you just have to:\n1. Set the parameters `pretrained_model_name_list` and `is_tf` to values of your choice.\n2. Commit.\n3. After the commit is finished and successful | 3.1. Go to [kernel_Off](https:\/\/www.kaggle.com\/maroberti\/load-test-transformers-model-downloader) |\u00a03.2. Click on \"Add Data\" | 3.3. Click on \"Kernel Output Files\" | 3.4. Click on \"Your Work\" | 3.5. Next to this notebook name, click on \"Add\".\n\n![](https:\/\/i.ibb.co\/svS7mxH\/Kernel-output-files-V1.png)\n\nIt is worth noting that if you commit this kernel_On again with another model list, it will update the models on the kernel_Off as well (after restarting session of kernel_Off).\n\nThis option is fast but still have some flows. For example, due to the 4.9GB disk limit, you are constrained to a certain amount of pretrained models. Also, as I just said, every time you will commit with another list of models, it automatically updates the models on the other linked kernel_Off. The next sections, Option 2 and Option 3 will show you how to avoids these problems.\n\n## Option 2: Through Kaggle Datasets < 4.9GB  \nTo avoid the models to be automatically updated on kernel_Off after every commits on kernel_On, you can simply create your own private\/public Kaggle dataset from the kernel_On output files. \nTo do so, you have to:\n1. Set the parameters `pretrained_model_name_list` and `is_tf` to values of your choice.\n2. Commit.\n3. After the commit is finished and successful | 3.1. Go to the commit version of your choice | 3.2. In the section \"Output\", click on the button \"New Dataset\". \n4. You follow the steps to create your dataset. \n5. 5.1. Go to [kernel_Off](https:\/\/www.kaggle.com\/maroberti\/load-test-transformers-model-downloader)\u00a0| 5.2. Click on \"Add Data\" | 5.3. Click on \"Your Datasets\" | 5.4. Next to your previously created dataset, click on \"Add\".\n\n![](https:\/\/i.ibb.co\/10wBgw0\/Datasets-V1.png)\n\n## Option 3: Through Kaggle Datasets > 4.9GB\nIf you want to create a dataset bigger than 4.9GB, you will have to:\n1. Set the parameters `pretrained_model_name_list` and `is_tf` to values of your choice.\n2. Commit.\n3. Download the model locally on your computer. To do so: 3.1. Go to the commit version of your choice | 3.2. In the section \"Output\", click on the button \"Download All\".\n4. Repeat steps 1., 2. and 3. with different `pretrained_model_name_list` values until you have all your models.\n5. Upload the models from your computer to a Kaggle dataset. To do so: 5.1. Go to the [Kaggle dataset page](https:\/\/www.kaggle.com\/datasets) | 5.2. Click on \"New Dataset\" | 5.3. Drag and drop the files containing the models | 4.4. Follow the steps to finish the creation of the dataset.\n6. 6.1. Go to [kernel_Off](https:\/\/www.kaggle.com\/maroberti\/load-test-transformers-model-downloader)\u00a0| 6.2. Click on \"Add Data\" | 6.3. Click on \"Your Datasets\" | 6.4. Next to your previously created dataset, click on \"Add\".\n\nTake care that the dataset is created respecting the Kaggle technical specification defined [here](https:\/\/www.kaggle.com\/docs\/datasets#technical-specifications).\n\nBecause you have to pass on local, this operation can be particularly painful and I feel that it could exist another way to charge the models to Kaggle without passing by this local download\/upload. If you have any solution, please tell us in the comment and I will update this notebook in consequences. If I have time, I will certainly explore a way to upload models from Colab to a Kaggle dataset using the Kaggle API. It can be useful, especially for training the models without using all the 30 precious hours of Kaggle GPU time.\n\nAnother way to get around the constraint of the 4.9GB max limit is to make multiple times the **Option 1** or **Option 2** but each time on a different kernel_On. Therefore, on kernel_Off, you will be able to download multiple packs of models on separated datasets and consequently get more than 4.9GB.\n\n----------\n\nLast remark, don't forget that some transformers models are directly available on the public datasets [here](https:\/\/www.kaggle.com\/datasets). If you don't find what you want, create your own dataset following **Option 2** or **3** and share it in the comment! For this example, I made this [dataset](http:\/\/) available.\n\n----------\n\n"}}