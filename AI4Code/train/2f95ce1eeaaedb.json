{"cell_type":{"d6fd91b0":"code","21ff910c":"code","ca0fcbdb":"code","47bdb91a":"code","5173d2d0":"code","9dce517a":"code","da8a44a3":"code","0770ada8":"code","372e75b4":"code","8d9ce21f":"code","fa6bbd3b":"code","eeac7b8e":"code","645793f1":"code","ad69845c":"code","e73cbaf8":"code","09d4a789":"code","212df2a2":"code","0b62dae3":"code","cbcf40a4":"code","29be2186":"code","1ce559cd":"code","42ebcfb3":"code","fb843510":"code","8df4c75b":"code","73f037eb":"code","47c13c99":"code","90e97046":"code","79568fea":"code","1650bba5":"code","48e23cab":"code","f3da0e80":"code","635b0ae2":"code","9ea5a42a":"code","552f047e":"code","a0eeeb11":"code","0a8dacae":"code","8e713fb7":"code","bf3a358b":"code","aa8168fe":"code","a3126ccc":"code","937d7449":"code","c36a15d7":"code","66e1cecd":"code","faabf73e":"code","f4f2e912":"code","462da5a8":"code","7add54e6":"code","4fe363ab":"code","94a0aeec":"code","84231148":"code","9bcc52f6":"code","531a14fc":"code","9b1c6e0f":"code","0f81534e":"code","80e0d8c7":"code","adfeba8b":"code","ee421ad6":"code","77a60c5f":"code","3252045e":"code","f42c8b65":"code","5f369dce":"code","8ff3547f":"code","cbbc330a":"code","5bf4729e":"code","cd425ea3":"code","0b3fd8e0":"code","bad449c9":"code","39adaef9":"code","09884bf1":"code","527af229":"code","06bfd976":"code","f6b6cea9":"code","fa6bd5c1":"code","472b52cd":"code","93c266f6":"code","24f0b8f8":"markdown","eb4b3b23":"markdown","e8ce5c76":"markdown","331b8a61":"markdown","52941808":"markdown","d9fe40f0":"markdown","af62a107":"markdown","33521680":"markdown","7a0e96cd":"markdown","0423f73e":"markdown","f836bd0d":"markdown","97b6e37a":"markdown","26f10d79":"markdown","01ca6d0a":"markdown","468c123f":"markdown","bb8e3ef8":"markdown","35f61ac4":"markdown","2fa01d86":"markdown","234a9654":"markdown","574979c3":"markdown","36ff3a48":"markdown","89078c7d":"markdown","cb1148f0":"markdown","5866c01e":"markdown","d55f5763":"markdown","a241eb6b":"markdown","3ee1b25e":"markdown","82e1a006":"markdown","1bc6a148":"markdown","d14c023d":"markdown","24b707e2":"markdown","8754a70f":"markdown","0b1eb9d9":"markdown","c5526068":"markdown","91e26bf6":"markdown","e70aadec":"markdown","3c0151b4":"markdown","d6d99a44":"markdown","84579cb5":"markdown","8ea0132c":"markdown","adc488be":"markdown","4e773df9":"markdown","06d5980d":"markdown","663ae75d":"markdown","b393af32":"markdown","86d3ef1a":"markdown","9ae98ce2":"markdown","5db8e787":"markdown","98afb9f9":"markdown","c01859c9":"markdown","44c31be2":"markdown","fde237e3":"markdown","69d4a236":"markdown"},"source":{"d6fd91b0":"from collections import Counter\n\nimport seaborn as sns\nimport numpy as np \nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport spacy\nfrom tqdm import tqdm\nfrom spacy.lang.en import English\nimport torch\nfrom torch import nn\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader\nimport pytorch_lightning as pl\n\nfrom sklearn.model_selection import train_test_split, cross_validate, cross_val_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizerFast","21ff910c":"np.random.seed(42)\n\n# prettier graphs!\nplt.style.use('ggplot')","ca0fcbdb":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","47bdb91a":"train.head()","5173d2d0":"target_counts = train.target.value_counts()\nsns.barplot(y=target_counts, x=target_counts.index)\nplt.ylabel('Samples')\nplt.title('Target')\nplt.show()","9dce517a":"has_kw = ~train.keyword.isna()\nfig, ax = plt.subplots(1, 2, sharey=True)\ntrain[has_kw]\nsns.countplot(data=train[has_kw], x='target', ax=ax[0])\nax[0].set_title('With keyword')\nsns.countplot(data=train[~has_kw], x='target', ax=ax[1])\nax[1].set_title('Without keyword')\nplt.show()","da8a44a3":"has_loc = ~train.location.isna()\nsns.countplot(x=has_loc)\nplt.xlabel('Has location')","0770ada8":"loc_count = train.location.value_counts()\ntop_loc = loc_count.iloc[:50]\nplt.subplots(figsize=(20, 8))\nplt.xticks(rotation=80)\nsns.barplot(x=top_loc.index, y=top_loc)","372e75b4":"min_freq = 5\nabove_threshold = train.location.value_counts() > min_freq\nfrequent_places = above_threshold.index[above_threshold]\ndata = train[train.location.isin(frequent_places)].location\nprint(f'{data.nunique()} unique locations with more than {min_freq} occurrences')","8d9ce21f":"train.drop(['location', 'keyword'], axis=1, inplace=True)\ntest.drop(['location', 'keyword'], axis=1, inplace=True)","fa6bbd3b":"train.text.isna().sum(), test.text.isna().sum()","eeac7b8e":"nlp = English()\ntokenizer = nlp.tokenizer\ntokens = tokenizer('This is a test!')\nprint(tokens)\nprint(type(tokens))\nprint([t.text for t in tokens])","645793f1":"text = \"Don't split #hashtags!\"\nprint('Before:', [t for t in tokenizer(text)])\n\nprefixes = list(nlp.Defaults.prefixes)\nprefixes.remove('#')\nprefix_regex = spacy.util.compile_prefix_regex(prefixes)\ntokenizer.prefix_search = prefix_regex.search\n\nprint('After:', [t for t in tokenizer(text)])","ad69845c":"text = 'This is  a test\\n , ok?'\nprint('All tokens:', [t.text for t in tokenizer(text)])\n\nprint('Check for is_space():', [t.text for t in tokenizer(text) if not t.is_space])","e73cbaf8":"train['tokens'] = train['text'].apply(lambda row: [t.text.lower() for t in tokenizer(row) if not t.is_space])\ntest['tokens'] = test['text'].apply(lambda row: [t.text.lower() for t in tokenizer(row) if not t.is_space])","09d4a789":"train.sample(10)","212df2a2":"train['num_tokens'] = train.tokens.apply(len)\nplt.hist(train.num_tokens, bins=20)\nplt.show()","0b62dae3":"inds40 = train.num_tokens <= 40\nfig, ax = plt.subplots(figsize=(16, 8))\nplt.hist(train[inds40 & train.target].num_tokens, bins=20, alpha=0.5, label='Positive', density=True)\nplt.hist(train[inds40 & ~train.target].num_tokens, bins=20, alpha=0.5, label='Negative', density=True)\nplt.legend()\nplt.title('Tweet length distribution')\nplt.show()","cbcf40a4":"from sklearn.feature_extraction.text import CountVectorizer\n\n# min and max document frequency (ratio of documents containing that token)\nmin_df = 5\nmax_df = 0.6\n\n# limit vocabulary size as a function of the training data\nmax_features = len(train) * 2\n\nvectorizer = CountVectorizer(lowercase=False, tokenizer=lambda x: x, min_df=min_df, max_df=max_df, max_features=max_features, binary=True)\ntrain_bow = vectorizer.fit_transform(train.tokens)\ntrain_bow","29be2186":"def plot_top_values(data, k, names, xlabel=None, ylabel=None, use_abs=False):\n    \"\"\"\n    Function to plot a barplot with counts of the top k items in data and their corresponding names.\n    \n    Args:\n        data: a numpy array\n        k: int\n        names: list of strings corresponding to the positions in data\n        use_abs: if True, take the highest absolute values\n    \"\"\"\n    if use_abs:\n        inds = np.abs(data).argsort()\n    else:\n        inds = data.argsort()\n            \n    # inverted argsort and top k\n    top_inds = inds[::-1][:k]\n    top_values = data[top_inds]\n    top_names = [names[i] for i in top_inds]\n    \n    fig, ax = plt.subplots(figsize=(16, 8))\n    plt.bar(np.arange(k), top_values)\n    if ylabel:\n        ax.set_ylabel(ylabel)\n    if xlabel:\n        ax.set_xlabel(xlabel)\n    ax.set_xticks(np.arange(k))\n    ax.set_xticklabels(top_names, rotation=80)\n    fig.tight_layout()","1ce559cd":"k = 50\n\nvocab = vectorizer.get_feature_names()\nword_count = train_bow.toarray().sum(0)\n\nplot_top_values(word_count, k, vocab, 'Count', 'Type')","42ebcfb3":"x = train_bow\ny = train['target']","fb843510":"majority = y.mode()[0] == y\nprint(f'Majority class baseline: {majority.mean()}')","8df4c75b":"classifier = LogisticRegression()\ncv_scores = cross_val_score(classifier, x, y, scoring='f1', cv=10, n_jobs=-1)\nprint(f'Mean F1: {cv_scores.mean()}')","73f037eb":"k = 50\nclassifier = LogisticRegression(max_iter=500)\nclassifier.fit(x, y)\nplot_top_values(classifier.coef_[0], k, vocab, 'Type', 'Weight', use_abs=True)","47c13c99":"def get_rows_containing(data, term):\n    \"\"\"Return rows containing a term\"\"\"\n    has_term = data.tokens.apply(lambda row: term in row)\n    return data[has_term]\n\nterms = ['bags', 'australia']\nfor term in terms:\n    rows = get_rows_containing(train, term)\n    print(f'Distribution containing {term}:')\n    print(rows.target.value_counts())\n    for i, row in rows.sample(5).iterrows():\n        print(row.target, row.text)\n    print()","90e97046":"from sklearn.feature_selection import chi2, SelectKBest\n\nnum_features = [1000, 500, 250, 100, 50]\nf1 = []\nfor k in num_features:\n    selector = SelectKBest(chi2, k=k)\n    x_selected = selector.fit_transform(x, y)\n    scores = cross_val_score(classifier, x_selected, y, scoring='f1', cv=10, n_jobs=-1)\n    f1.append(scores.mean())\n","79568fea":"ticks = np.arange(len(f1))\nplt.plot(ticks, f1)\nplt.xticks(ticks, [str(k) for k in num_features])\nplt.title('F1 per number of features (chi2 selector)')\nplt.show()","1650bba5":"selector = SelectKBest(chi2, k=250)\nx_selected = selector.fit_transform(x, y)\nvocab = [vocab[i] for i, selected in enumerate(selector.get_support()) if selected]\nclassifier.fit(x_selected, y)\nplot_top_values(classifier.coef_[0], k, vocab, 'Type', 'Weight', use_abs=True)","48e23cab":"rows = get_rows_containing(train, 'ebay')\nsns.countplot(x='target', data=rows)\nplt.title('Target distribution containing \"ebay\"')\nplt.show()","f3da0e80":"regularization = [1, 0.1, 0.01, 0.001, 0.0001]\nl1_scores = []\nl2_scores = []\nl1_std = []\nl2_std = []\n\nfor value in regularization:\n    log_reg = LogisticRegression(C=value)\n    results = cross_val_score(log_reg, x_selected, y, scoring='f1', cv=10, n_jobs=-1)\n    l2_scores.append(results.mean())\n    l2_std.append(results.std())\n    \n    alpha = 1 \/ (2 * value)  # as defined in sklearn\n    ridge = RidgeClassifier(alpha=alpha)\n    results = cross_val_score(ridge, x_selected, y, scoring='f1', cv=10, n_jobs=-1)\n    l1_scores.append(results.mean())\n    l1_std.append(results.std())","635b0ae2":"n = np.arange(len(regularization)) + 1\nfig, ax = plt.subplots(figsize=(14, 6))\nwidth = 0.4\n\nax.bar(n, l1_scores, width, label='L1 reg', yerr=l1_std)\nax.bar(n + width, l2_scores, width, label='L2 reg', yerr=l2_std)\nax.set_xlabel('Regularization (lower is stronger)')\nax.set_ylabel('Mean F1')\nax.set_xticks(n + width \/ 2)\nax.set_xticklabels([str(val) for val in regularization])\nax.legend(loc='best')\n","9ea5a42a":"print(f'Best baseline F1: {l2_scores[1]}')","552f047e":"# min and max document frequency (ratio of documents containing that token)\nmin_df = 10\nmax_df = 0.6\n\n# limit vocabulary size as a function of the training data\nmax_features = len(train) * 2\n\n# single words to 3-grams\nngram_range = (1, 3)\n\nvectorizer = CountVectorizer(lowercase=False, tokenizer=lambda x: x, min_df=min_df, max_df=max_df, max_features=max_features, binary=True, ngram_range=ngram_range)\nx = train_bow = vectorizer.fit_transform(train.tokens)\n\nvocab = vectorizer.get_feature_names()\nword_count = train_bow.toarray().sum(0)\n\nplot_top_values(word_count, k, vocab, 'Count', 'Type')","a0eeeb11":"x.shape","0a8dacae":"classifier = LogisticRegression(C=0.1)\nselector = SelectKBest(chi2, k=500)\nx = selector.fit_transform(x, y)\ncv_scores = cross_validate(classifier, x, y, scoring='f1', cv=10, n_jobs=-1, return_train_score=True)\nmean_f1 = cv_scores['test_score'].mean()\nprint(f'Mean F1: {mean_f1}')","8e713fb7":"def plot_model_score(train_scores, valid_scores):\n    \"\"\"Plot train and validation score for comparison and checking overfitting\"\"\"\n    mean_train = train_scores.mean()\n    mean_valid = valid_scores.mean()\n    fig, ax = plt.subplots()\n    plt.bar(0, mean_train, yerr=train_scores.std())\n    plt.bar(1, mean_valid, yerr=valid_scores.std())\n    ax.text(0, mean_train + 0.01, f'{mean_train:.4f}')\n    ax.text(1, mean_valid + 0.01, f'{mean_valid:.4f}')\n    plt.title('Model F1 and standard deviation')\n    plt.xticks([0, 1], ['Train', 'Validation'])\n    ymin = np.min([mean_train, mean_valid]) * 0.8\n    plt.ylim(bottom=ymin)\n    plt.show()","bf3a358b":"plot_model_score(cv_scores['train_score'], cv_scores['test_score'])","aa8168fe":"c = RandomForestClassifier(n_estimators=100, n_jobs=-1)\ncv_scores = cross_validate(c, x, y, scoring='f1', cv=8, n_jobs=-1, return_train_score=True)\nplot_model_score(cv_scores['train_score'], cv_scores['test_score'])","a3126ccc":"c = RandomForestClassifier(n_estimators=100, min_samples_leaf=3)\ncv_scores = cross_validate(c, x, y, scoring='f1', cv=8, n_jobs=-1, return_train_score=True)\nplot_model_score(cv_scores['train_score'], cv_scores['test_score'])","937d7449":"c = RandomForestClassifier(n_estimators=500, min_samples_split=10)\ncv_scores = cross_validate(c, x, y, scoring='f1', cv=8, n_jobs=-1, return_train_score=True)\nplot_model_score(cv_scores['train_score'], cv_scores['test_score'])","c36a15d7":"c = RandomForestClassifier(n_estimators=200, min_samples_split=5, max_depth=50)\ncv_scores = cross_validate(c, x, y, scoring='f1', cv=8, n_jobs=-1, return_train_score=True)\nplot_model_score(cv_scores['train_score'], cv_scores['test_score'])","66e1cecd":"c = XGBClassifier()\ncv_scores = cross_validate(c, x, y, scoring='f1', cv=8, n_jobs=-1, return_train_score=True)\nplot_model_score(cv_scores['train_score'], cv_scores['test_score'])","faabf73e":"test_bow = vectorizer.transform(test.tokens)\ntest_bow = selector.transform(test_bow)\nclassifier = LogisticRegression(C=0.1)\n\n# use the whole training dataset now\nclassifier.fit(x, y)\npredicted = classifier.predict(test_bow)\nsubmission = pd.DataFrame({'id': test.id, 'target': predicted})\nsubmission.to_csv('bow-linear.csv', index=False)","f4f2e912":"filename = '\/kaggle\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt'\nword_dict = {}\nembeddings = []\nwith open(filename, 'r') as f:\n    for line in tqdm(f, total=400000):\n        word, vector_string = line.split(' ', 1)\n        vector = [float(value) for value in vector_string.split()]\n        embeddings.append(vector)\n        word_dict[word] = len(word_dict)\n\nembeddings = torch.tensor(embeddings)","462da5a8":"print(embeddings.shape)\nprint(len(word_dict))","7add54e6":"oov_count = Counter()\nall_tokens = []\n\nfor row in train.tokens:\n    tokens = [t[1:] if t.startswith('#') else t for t in row]\n    all_tokens.append(tokens)\n    oov_count.update(set(t for t in tokens if t not in word_dict))","4fe363ab":"test_tokens = []\nfor row in test.tokens:\n    tokens = [t[1:] if t.startswith('#') else t for t in row]\n    test_tokens.append(tokens)","94a0aeec":"oov_count.most_common(10)","84231148":"words_to_add = [w for w in oov_count if oov_count[w] > 2]\nfor word in words_to_add:\n    word_dict[word] = len(word_dict)\n\nnew_vectors = torch.zeros((len(words_to_add), embeddings.shape[1]))\nembeddings = torch.cat([embeddings, new_vectors], dim=0)\nprint(len(word_dict), embeddings.shape)","9bcc52f6":"len(oov_count)","531a14fc":"def convert_to_indices(all_tokens):\n    word_indices = []\n\n    for tokens in all_tokens:\n        tweet_inds = torch.tensor([word_dict[t] for t in tokens if t in word_dict], dtype=torch.long)\n        word_indices.append(tweet_inds)\n    \n    return word_indices\n\nword_indices = convert_to_indices(all_tokens)\ntest_word_indices = convert_to_indices(test_tokens)","9b1c6e0f":"class BagOfEmbeddingsClassifier(pl.LightningModule):\n    def __init__(self, embeddings, learning_rate=0.001, l2=0.001):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.l2 = l2\n        \n        vocab_size, embedding_dim = embeddings.shape\n        self.embedding_bag = nn.EmbeddingBag.from_pretrained(embeddings, freeze=False)\n        \n        # a single output value determines the probability of class 1 with a sigmoid function\n        self.linear = nn.Linear(embedding_dim, 1, bias=True)\n    \n    def forward(self, x):\n        \"\"\"x is a list of tensors with any shape\"\"\"\n        # embedding bag operates with a single tensor of concatenated inputs and another of offsets\n        lengths = torch.tensor([0] + [len(sample) for sample in x[:-1]])\n        offsets = lengths.cumsum(0).to(x[0].device)\n        x = torch.cat(x)\n        embedded = self.embedding_bag(x, offsets)\n        logits = self.linear(embedded).squeeze(-1)\n        return logits\n    \n    def _get_loss_and_acc(self, logits, y):\n        \"\"\"Internal function\"\"\"\n        predicted = logits > 0\n        acc = (predicted == y).float().mean()\n        loss = F.binary_cross_entropy_with_logits(logits, y.float())\n        \n        return loss, acc\n    \n    def on_fit_start(self):        \n        self.train_losses = []\n        self.train_accs = []\n        self.valid_losses = []\n        self.valid_accs = []\n        \n        self.reset_metrics()\n    \n    def reset_metrics(self):\n        self.partial_train_losses = []\n        self.partial_train_accs = []\n        self.partial_valid_losses = []\n        self.partial_valid_accs = []\n    \n    def on_validation_end(self):\n        self.train_losses.append(np.array(self.partial_train_losses).mean())\n        self.train_accs.append(np.array(self.partial_train_accs).mean())\n        self.valid_losses.append(np.array(self.partial_valid_losses).mean())\n        self.valid_accs.append(np.array(self.partial_valid_accs).mean())\n        self.reset_metrics()\n    \n    def training_step(self, batch, batch_idx):\n        \"\"\"\n        batch is a tuple (x, y)\n        x is a list of tensors as in forward\n        y is a tensor with the classes\n        \"\"\"\n        x, y = batch\n        logits = self(x)\n        loss, acc = self._get_loss_and_acc(logits, y)\n        \n        # ideally we'd use tensorboard to see the graphs, but currently it is disabled in Kaggle\n        # so we resort to manually plotting\n#         self.log('train_loss', loss)\n#         self.log('train_acc', acc)\n        self.partial_train_losses.append(loss.detach().cpu().numpy())\n        self.partial_train_accs.append(acc.detach().cpu().numpy())\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss, acc = self._get_loss_and_acc(logits, y)\n        \n#         self.log('valid_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n#         self.log('valid_acc', acc)\n        self.partial_valid_losses.append(loss.detach().cpu().numpy())\n        self.partial_valid_accs.append(acc.detach().cpu().numpy())\n        \n        return loss\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.l2)\n        return optimizer","0f81534e":"def plot_model_performance(model):\n    fig, ax = plt.subplots(2, 1, figsize=(16, 8))\n    ax[0].set_title('Loss')\n    ax[1].set_title('Accuracy')\n\n    n = np.arange(len(model.train_losses))\n    ax[0].plot(n, model.train_losses, 'bo', label='Train', linestyle='--')\n    ax[1].plot(n, model.train_accs, 'bo', linestyle='--')\n    ax[0].plot(n, model.valid_losses, 'ro', label='Validation', linestyle='--')\n    ax[1].plot(n, model.valid_accs, 'ro', linestyle='--')\n    ax[0].legend()\n    plt.show()","80e0d8c7":"def collate_as_list(samples):\n    \"\"\"Function for the DataLoader to combine samples in a batch. Each sample is a (x, y) pair.\"\"\"\n    x, y = list(zip(*samples))\n    if y[0] is None:\n        return x\n    return x, torch.tensor(y).float()\n\n\nclass WordIndexDataset(Dataset):\n    def __init__(self, x, y=None):\n        self.x = x\n        self.y = y\n    \n    def __getitem__(self, i):\n        if self.y is not None:\n            return self.x[i], self.y[i]\n        else:\n            return self.x[i], None\n    \n    def __len__(self):\n        return len(self.x)\n","adfeba8b":"validation_size = int(0.1 * len(train))\nvalidation_inds = np.random.choice(np.arange(len(train)), size=validation_size, replace=False)\nis_train = np.ones(len(train), dtype=np.bool)\nis_train[validation_inds] = False\n\n# use an object array since we have varied size tensors\ntweets = np.array(word_indices, dtype=object)\ntarget = train.target.to_numpy()\n# train_tweets, valid_tweets, train_target, valid_target = train_test_split(tweets, target, test_size=0.1, stratify=target)\ntrain_tweets = tweets[is_train].tolist()\ntrain_target = target[is_train]\nvalid_tweets = tweets[~is_train].tolist()\nvalid_target = target[~is_train]\n\ntrain_data = WordIndexDataset(train_tweets, train_target)\nvalid_data = WordIndexDataset(valid_tweets, valid_target)\ntest_data = WordIndexDataset(test_word_indices)\ntrain_loader = DataLoader(train_data, batch_size=32, collate_fn=collate_as_list)\nvalid_loader = DataLoader(valid_data, batch_size=256, collate_fn=collate_as_list)\ntest_loader = DataLoader(test_data, batch_size=256, collate_fn=collate_as_list)","ee421ad6":"model = BagOfEmbeddingsClassifier(embeddings, 0.001, l2=0)\nbatch = next(iter(train_loader))\n\n# batch is x, y\nlogits = model(batch[0])\nprint(logits)","77a60c5f":"trainer = pl.Trainer(gpus=1, max_epochs=5, val_check_interval=0.5)\ntrainer.fit(model, train_loader, valid_loader)","3252045e":"plot_model_performance(model)","f42c8b65":"model = BagOfEmbeddingsClassifier(embeddings, 0.001, l2=0.0001)\ntrainer = pl.Trainer(gpus=1, max_epochs=6, val_check_interval=0.5)\ntrainer.fit(model, train_loader, valid_loader)","5f369dce":"plot_model_performance(model)","8ff3547f":"# trainer.predict returns a list with batch results\nlogits = np.concatenate(trainer.predict(model, test_loader))","cbbc330a":"predicted = logits > 0\nsubmission = pd.DataFrame({'id': test.id, 'target': predicted.astype(np.int)})\nsubmission.to_csv('embeddings.csv', index=False)","5bf4729e":"pretrained_name = 'distilroberta-base'\ntokenizer = RobertaTokenizerFast.from_pretrained(pretrained_name)\nroberta = RobertaForSequenceClassification.from_pretrained(pretrained_name, num_labels=2)","cd425ea3":"# create tensors of variable sizes\n# note that the tokenizer returns a tensor with shape [1, num_tokens]\ntrain_tokens = train.text[is_train].apply(lambda s: tokenizer.encode(s, return_tensors='pt')[0]).tolist()\nvalid_tokens = train.text[~is_train].apply(lambda s: tokenizer.encode(s, return_tensors='pt')[0]).tolist()\ntest_tokens = test.text.apply(lambda s: tokenizer.encode(s, return_tensors='pt')[0]).tolist()\n\n# add padding to have a fixed size matrix. With bigger datasets we should be careful about memory usage, but this is small enough to skip this kind of optimization\npadding = tokenizer.pad_token_id\nx_train = pad_sequence(train_tokens, batch_first=True, padding_value=padding)\nx_valid = pad_sequence(valid_tokens, batch_first=True, padding_value=padding)\nx_test = pad_sequence(test_tokens, batch_first=True, padding_value=padding)\n\nx_train_mask = x_train != padding\nx_valid_mask = x_valid != padding\nx_test_mask = x_test != padding\nprint(f'x_train shape: {x_train.shape}, x_valid shape: {x_valid.shape}, x_test shape: {x_test.shape}')","0b3fd8e0":"train_data = TensorDataset(x_train, x_train_mask, torch.tensor(train_target))\nvalid_data = TensorDataset(x_valid, x_valid_mask, torch.tensor(valid_target))\ntest_data = TensorDataset(x_test, x_test_mask)\n\ntrain_loader = DataLoader(train_data, batch_size=32)\nvalid_loader = DataLoader(valid_data, batch_size=256)\ntest_loader = DataLoader(test_data, batch_size=256)","bad449c9":"class TransformerWrapper(pl.LightningModule):\n    def __init__(self, transformer, learning_rate=0.001, l2=0.0001):\n        super().__init__()\n        self.model = transformer\n        self.learning_rate = learning_rate\n        self.l2 = l2\n    \n    def forward(self, batch):\n        x, mask = batch\n        output = self.model(x, mask)\n        return output.logits\n    \n    def training_step(self, batch, batch_idx):\n        loss, acc = self._get_loss_and_acc(batch)\n        self.partial_train_losses.append(loss.detach().cpu().numpy())\n        self.partial_train_accs.append(acc.detach().cpu().numpy())\n        \n        return loss\n    \n    def _get_loss_and_acc(self, batch):\n        x, mask, y = batch\n        output = self.model(x, mask, labels=y)\n        loss = output.loss\n        logits = output.logits\n        \n        predicted = logits.argmax(1)\n        acc = (predicted == y).float().mean()\n        \n        return loss, acc\n    \n    # these functions are copied from the BagOfWords class to allow ploting without tensorboard\n    # ideally, we'd inherit from a common base class. well, ideally we'd have access to tensorboard and none of this would exist :)\n    def on_fit_start(self):        \n        self.train_losses = []\n        self.train_accs = []\n        self.valid_losses = []\n        self.valid_accs = []\n        \n        self.reset_metrics()\n    \n    def reset_metrics(self):\n        self.partial_train_losses = []\n        self.partial_train_accs = []\n        self.partial_valid_losses = []\n        self.partial_valid_accs = []\n    \n    def on_validation_end(self):\n        self.train_losses.append(np.array(self.partial_train_losses).mean())\n        self.train_accs.append(np.array(self.partial_train_accs).mean())\n        self.valid_losses.append(np.array(self.partial_valid_losses).mean())\n        self.valid_accs.append(np.array(self.partial_valid_accs).mean())\n        self.reset_metrics()\n        \n    def validation_step(self, batch, batch_idx):\n        loss, acc = self._get_loss_and_acc(batch)\n        \n#         self.log('valid_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n#         self.log('valid_acc', acc)\n        self.partial_valid_losses.append(loss.cpu().numpy())\n        self.partial_valid_accs.append(acc.cpu().numpy())\n        \n        return loss\n    \n    def configure_optimizers(self):\n        # to make it lighter, fine tune only the classifier on top of the language model\n        parameters = [p[1] for p in self.model.named_parameters() if p[0].startswith('classifier')]\n        optimizer = torch.optim.AdamW(parameters, lr=self.learning_rate, weight_decay=self.l2)\n        return optimizer","39adaef9":"model = TransformerWrapper(roberta, 0.001, l2=0)\ntrainer = pl.Trainer(gpus=1, max_epochs=6, val_check_interval=0.5)\ntrainer.fit(model, train_loader, valid_loader)","09884bf1":"plot_model_performance(model)","527af229":"roberta = RobertaForSequenceClassification.from_pretrained(pretrained_name, num_labels=2)\nmodel = TransformerWrapper(roberta, 0.01, l2=0)\ntrainer = pl.Trainer(gpus=1, max_epochs=4, val_check_interval=0.5)\ntrainer.fit(model, train_loader, valid_loader)","06bfd976":"plot_model_performance(model)","f6b6cea9":"roberta = RobertaForSequenceClassification.from_pretrained(pretrained_name, num_labels=2)\n\ntrain_loader = DataLoader(train_data, batch_size=128)\n\nmodel = TransformerWrapper(roberta, 0.005, l2=0)\ntrainer = pl.Trainer(gpus=1, max_epochs=4, val_check_interval=0.5)\ntrainer.fit(model, train_loader, valid_loader)","fa6bd5c1":"plot_model_performance(model)","472b52cd":"# trainer.predict returns a list with batch results\nlogits = np.concatenate(trainer.predict(model, test_loader), axis=0)\npredicted = logits.argmax(1)\nsubmission = pd.DataFrame({'id': test.id, 'target': predicted})\nsubmission.to_csv('roberta.csv', index=False)","93c266f6":"!head *.csv","24f0b8f8":"# Create a vocabulary and count tokens\n\nThe good old bag of words, or BOW, is super easy to implement and will serve as a baseline. Vocabulary extraction will be useful for more stuff, too. Spacy will help with tokenization.","eb4b3b23":"Whoa, validation performance is even better than in training! Maybe this is because of dropout, or just some fortuitous split. But frankly, I'm tired of doing in-depth analysis here and it's not really a big deal. Let's just give it one more try with a bit of a higher learning rate.","e8ce5c76":"The `EmbeddingBag` receives a 1d tensor (with all inputs concatenated) and a offsets tensor telling where each sample begins. This way, it can deal with variable-length inputs.","331b8a61":"That makes sense, considering the data. Still, we don't want any terms having very high weights (notice they range from below -1.5 to over 2). Our next step is to regularize the classifier.","52941808":"### Token counts","d9fe40f0":"# Create a BOW and run baseline classifiers\n\nThe BOW representation we saw above is far from perfect. But again, this is a baseline. Let's try a bunch of off-the-shelf classifiers and see what they give us.","af62a107":"That's already a lot better than the simple BOW, with a validation accuracy around 0.8. After a few iterations, though, it starts to overfit. Let's experiment with some L2 regularization.","33521680":"That's bad, not much different from a majority baseline. Let's inspect the model weights:","7a0e96cd":"The warnings above are perfectly fine: we loaded the weights of a pretrained language model, and now have to train the classification weights.","0423f73e":"What about the keywords?","f836bd0d":"So that's a very long tail distribution. Location also doesn't seem great. We'll use only text, then. NLP at its best!","97b6e37a":"So the vocabulary size did not come even close to the maximum. Let's look at the most common words","26f10d79":"# Load the data and have a look around","01ca6d0a":"## Vectorization and stopwords\n\nIt's important to look at the number of occurrences of each token; Tokens that appear a single time in the training data don't tell much. Some people prefer to use a hard list of stopwords to filter out some of them. This may be a good idea, but I'd rather filter by frequency and then select relevant features.\n\nIn fact, a preposition as \"besides\" may be in a stopword list, but since it is somewhat rare and indicates some formality, it may actually be a good feature for our model. At the same time, a word like \"shocking\" may be absent from stopwords lists, but be not helpful to our task.\n","468c123f":"That was definitely too much. A middle term, with a larger batch size, maybe?","bb8e3ef8":"A bit of regularization helps, but won't change much. At the strongest regularization, the Logistic Regressor has 0 F1 (weights get so low that all instances are assigned to the same class). For the record, let's keep our BOW baseline as the Logistic Regressor with an L2 regularization of 0.1, which reduces the variance in comparison with 1 (no regularization) at a very small accuracy cost.","35f61ac4":"Not so bad, most tweets have location. But what about their distribution?","2fa01d86":"# Introduction\n\nThis notebook does some EDA of the dataset and tries different methods for classifying the tweets, starting from simple baselines and gradually using more complex techniques.\n\n## What to expect in here:\n\n* Analysis to understand what is in the data and any unexpected stuff in there\n* Comparison of different ML and NLP techniques, starting from the very simple to the complex ones\n* ...by extension, an illustration of the payoff of using fancier algorithms\n* Code supposed to be easily understandable and reproducible\n\n## What not to expect:\n\n* Jumping straight down to the most efficient or trendiest NLP\/ML models. \n  * It may be a good strategy to go straight to them when you know in advance they are needed for a problem. But this notebook serves more as a learning and practice tool.\n* The quickest path to training a classifier\n  * I try to analyze some aspect of the data after each step in order to motivate my decisions.\n  * In a practical scenario, you'll often decide what to do based on your expertise (and\/or gut feeling!), without the need to look at the intermediate results so often.","234a9654":"0.9 train F1 vs 0.6 validation F1? This is textbook overfitting. Let's try some techniques to reduce bias in the forest. \n\nI tried a bunch of different hyperparameters and left here some of them. None were much better than the Logistic Regressor, though.","574979c3":"That's very close to our first try, it'll be hard to get any better. Validation performance being higher than training is rules out overfitting. We'll use this trained model as the final submission!","36ff3a48":"Let's now build a wrapper with Pytorch Lightning for our Roberta model","89078c7d":"## Random forest and XGBoost","cb1148f0":"Some imports and definitions","5866c01e":"Ok, that's good. But one minor thing we gotta remember: there are no hashtags in the Glove vocabulary. \nWe may instead assume that a hashtag can convey essentially the same meaning as its content with the leading `#` removed.\n\nAlso, we may add OOV (out-of-vocabulary) words and give them a vector of zeros. This will include Twitter usernames; most of them tend to be rare or useless, but maybe some important accounts are linked to real events. But let's restrict this to words appearing at least twice in our data.","d55f5763":"Let's also measure the standard deviation for each fold of the validation set","a241eb6b":"As expected, a simple linear classifier is capable of detecting relevant words even if we don't filter out stopwords. But the negative weights are a bit funny, though: apparently, mentioning `australia` or `bags` makes the tweet less likely to be about a true incident? Let's check that:","3ee1b25e":"Just to be sure...","82e1a006":"As expected, the training set has higher accuracy and lower variance, but that 0.7 is pretty much an upper bound of what we can achieve with this classifier and this representation.\n\nI tried playing around with a random forest and XGBoost, and some hyperparameters in the next cell, but could not get significantly better than the logistic regressor.","1bc6a148":"Let's build a model that averages embeddings and runs a linear layer on top.","d14c023d":"Notice the plot has distribution densities, not absolute values, to make comparison easier (remember the negative class is a bit more common).\nThe shapes are slightly different, but it doesn't look like tweet length is a great indicator of the target.","24b707e2":"We should take care with texts containing multiple whitespaces in sequence; spacy keeps them as extra tokens:","8754a70f":"We may lowercase tokens as well:","0b1eb9d9":"Using 250-500 word types gives a reasonable improvement in comparison to all the ~1500, well above the majority baseline. Let's stick with those, then, and inspect weights again.","c5526068":"Validation performance didn't really increase, but it's now closer to training performance and thus we can expect less variance in the classifier.\n\nThis will be our second submission.","91e26bf6":"They are extremely rare and appear in both classes, so we will just ignore them. Let's look at location...","e70aadec":"Notice that the tokenizer returns a Doc object.","3c0151b4":"Increasing the number of trees (estimators) helps a bit","d6d99a44":"# n-grams\n\nBesides a simple BOW, we might try n-grams as well. The same vectorizer from sklearn can do the trick for us. \n\nNote that the possible numbers of n-grams grows exponentially (it's `||vocabulary||^n`), but if we only use sequences seen in training data, it won't be a problem.","84579cb5":"## Pre-embedding submission\n\nLet's submit our logistic regressor + BOW model as a first try!","8ea0132c":"That seems about ok. Now let's explore the relation between tokens counts and the target.\n\n\n","adc488be":"Ok, so a few outliers are getting in the way of a better graph. Let's just ignore lengths > 40 for plotting:","4e773df9":"Interestingly, words like `the` and `in` appear in less than 60% of the tweets (our filter for maximum frequency). Well, this is Twitter. Most of these words do not have any relation with the target, so we can filter them out based on their correlation value.","06d5980d":"## Regularization\n\nBefore jumping to smarter representations and models, let's just try some simple regularization to reduce variance; it could help with the example above.","663ae75d":"Minimum leaf samples reduced training performance but barely improved validation","b393af32":"# Word embeddings\n\nThe results above suggest that the information in the dataset itself will hardly allow us past 0.65 F1.\n\nThe advantage of using **pretrained** word embeddings is that they give us additional knowledge, learned beforehand from large volumes of text.\n\nLet's now try simple word embeddings to get better representations. We'll use GloVe vectors (available as a dataset in Kaggle, be sure to go to `File\/Add or upload data` to make it avaiable).\n\nWe'll load the embedding matrix in torch and create a `word_dict` that maps a word type to its position in the matrix.","86d3ef1a":"That kinda explains it. Still, it is not reasonable that the model should give a heavy negative weight to `australia`. By the way, notice that the model took care of ignoring stopwords by itself, as expected.\n\nLet's now filter feature by correlation with the target.","9ae98ce2":"Ok, so let's proceed to training with Pytorch Lightning! \u26a1","5db8e787":"Curiously, the most frequent 2 or 3-grams are made up of punctuation. \nLet's run our regularized logistic regressor after feature selection again. Since there are so many more features, let's allow for 500. Let's also do a better comparison of train and test performance to see if we spot overfitting.","98afb9f9":"What's special about ebay?","c01859c9":"By default, spacy splits the leading `#` from the rest of a token. But this is Twitter, and we'd better adapt the tokenizer to handle `#hashtags`! cf. [spacy documentation](https:\/\/spacy.io\/usage\/linguistic-features#native-tokenizer-additions) on modifying defaults for tokenization.","44c31be2":"With neural models, training times may take longer and it's more practical to set up a separate validation set instead of cross-fold validation.","fde237e3":"# Bringing in the Big Guns: Pretrained Transformers\n\nLet's now take a step further and try a pretrained Transformer, like BERT or RoBERTa, in our task. While word embeddings are lightweight, but static representations of words, a Transformer analyses the whole text, taking context into consideration.\n\n## They are BIG\n\nHere, we'll try the distilled version of RoBERTa available via HuggingFace's `transformers`; it's already pretty big at 6 layers of size 768! \n\nThis can take quite a bit to train even with GPUs, but this dataset is small.\n\n## Internal tokenization\n\nWhat's more, these models have tokenizers of their own, using the BPE strategy for tokenization. They are even better equipped to deal with usernames and hashtags!","69d4a236":"Let's do a quick test to see if everything is in order"}}