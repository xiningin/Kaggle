{"cell_type":{"bbe5be71":"code","80842b2d":"code","9b56d122":"code","ac146763":"code","7f26087d":"code","76f6f0f4":"code","16e4d32b":"code","e6bdd98d":"code","29bda458":"code","5446acb8":"code","5bc77506":"code","280618e8":"code","c3e11127":"code","bff95de0":"code","b1816b0f":"code","f3366f3d":"code","bc1f5b8e":"code","463efd4c":"code","96a48f88":"code","00707ea0":"code","f0971bf6":"code","82eae512":"code","24fd7598":"code","de5ea6ea":"code","8c1f226a":"code","2719a459":"code","46e76c87":"code","60c4c752":"code","7978a3f5":"code","29e17005":"code","64239f0c":"code","278fb1d4":"code","ce991db6":"code","fd2fd68d":"code","80cafa74":"code","264a1338":"code","3098b289":"code","5cd9cc61":"code","0a745903":"code","fd5797b7":"code","0a3454ce":"code","9af2a8af":"code","1f39cabc":"code","4f0b9a86":"code","fb577d90":"code","77482f3f":"code","f024b66b":"code","930f1455":"markdown","d083d6d7":"markdown","1fb673dd":"markdown","88b8a421":"markdown","3b9cece9":"markdown","3287807b":"markdown","98d78794":"markdown","0ad237b1":"markdown","1b105991":"markdown","f9da83ae":"markdown","adf595b5":"markdown","eff5d02b":"markdown","c3225f59":"markdown","57567dd4":"markdown"},"source":{"bbe5be71":"# Libraries needed \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nimport math\nimport warnings\nimport re\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n#random_state = 1\nplt.rcParams['figure.figsize']=8,8\nimport os\nprint(os.listdir(\"..\/input\"))","80842b2d":"# load wine dataset into notebook\ndata_path = \"..\/input\/winemag-data-130k-v2.csv\"\nwine_data = pd.read_csv(data_path)","9b56d122":"print(\"Wine Data Dimension:\",wine_data.shape)\nwine_data.head(3) #disply first 5 elements in dataset","ac146763":"wine_data.tail(3) #displays last 3 elements in dataset","7f26087d":"# check to see if there are spaces in column names\nwine_data.columns","76f6f0f4":"wine_data.describe()","16e4d32b":"# Useful information about the data\nwine_data.info()","e6bdd98d":"# Disttribution of points for initial dataset\nsns.distplot(wine_data.points) \nplt.xlabel(\"Points\",size=15)\nplt.title(\"Fig.1: Points Distribution\", size=20)","29bda458":"# Wine Tasters and the Amount of Wines Evaluated\nwine_data['taster_name'].value_counts().plot(kind='bar')\nplt.xticks(fontsize=11)\nplt.xlabel(\"Taster Names\",size=15)\nplt.ylabel('No. of wines tasted', size=15)\nplt.title(\"Fig. 2: Wine Tasters and the Amount of Wines Evaluated\", size=20)","5446acb8":"# Selecting sample of the data that will be used (i.e. the wines evaluated by Roger Voss)\nwine_data = wine_data[(wine_data['taster_name']=='Roger Voss') | (wine_data['taster_twitter_handle']=='@vossroger')]\nprint(\"Wine Data Sample Dimension:\",wine_data.shape,\"(i.e. Wines evaluated by Roger Voss)\") #dataset dimension\nwine_data.head()","5bc77506":"# Measures of the numerical data (i.e. wines tasted by Roger Voss)\nwine_data.describe()","280618e8":"wine_data.info()","c3e11127":"# Distribution of points for wines Roger Voss evaluated\nsns.distplot(wine_data.points) \nplt.xlabel(\"Points\",size=15)\nplt.title(\"Fig.3: Points Distribution for Wines Evaluated by Roger Voss \", size=20)","bff95de0":"sns.countplot(x='country',data=wine_data, orient=\"h\")\nplt.ylabel('Country Count',size=12)\nplt.xlabel(\"Country\",size=12)\nplt.xticks(rotation=45)\nplt.suptitle(\"Fig.4: Countries per bottle of Wine Evaluted by Roger Voss \", size=20)","b1816b0f":"# Rename column 'serial' to 'wine_id'\nwine_data.rename(columns={'serial':'wine_Id'}, inplace=True)\nwine_data.head(1)","f3366f3d":"#check for duplicates in dataset and remove if any\nprint(wine_data.duplicated(subset=None, keep='first').sum(),\"duplicate record(s)\")","bc1f5b8e":"# Perform feature extraction to impute the year of each wine\nwine_data['year'] = wine_data['title'].str.extract('(\\d\\d\\d\\d)', expand=True)","463efd4c":"# Check to see if there are any null years\nwine_data['year'].isnull().value_counts()","96a48f88":"# wines that does not have a year in the title\n#Wines without a year are classified as Non-Vintage wines\nwine_data.title[wine_data['year'].isnull()].head()","00707ea0":"# convert year to int so as to make searches for preprocessing easier\nwine_data.year = pd.to_numeric(wine_data.year, errors='coerce').fillna(0).astype(np.int64)","f0971bf6":"# check fo erroneous years (NB: its year 2018, any year above this is invalid)\nprint((wine_data['year']>2018).sum(),\"invalid year(s)\")","82eae512":"# Applying feature engineering to create type of wine (Vintage\/n\\Non-Vintage)\nwine_data['type']= None\nwine_data.type[wine_data['year']!=0] = 'Vintage'\nwine_data.type[wine_data['year']==0] = 'Non-Vintage'","24fd7598":"# Create loation by feature extraction from title\nno_location = wine_data['title'].str.split('(', expand=True, n=1)\n#wine_data['location'] = no_location.str.extract('(', expand=True)\n#wine_data\nno_location=no_location[1].str.split(')', expand=True, n=1)\nwine_data['location']=no_location[0]\n\n#wine_data[wine_data['location'].isnull()==True]","de5ea6ea":"# impute location from region_2,region_1,province \nwine_data['location'].fillna(wine_data.region_2, inplace = True) \nwine_data['location'].fillna(wine_data.region_1, inplace = True)\nwine_data['location'].fillna(wine_data.province, inplace = True)","8c1f226a":"# look for missing locations and country\nprint(wine_data['location'].isnull().sum(),\"missing location(s) and\",wine_data['country'].isnull().sum(),\"missing countries\") #check for null locations\n\n#impute missing missing location and country from title research\nwine_data.location.fillna('Bordeaux',inplace=True)\nwine_data.country.fillna('France',inplace=True)\n\nprint(\"are attributed to 'Bordeaux' region in 'France' based on research of wine titles\")","2719a459":"# look for missing prices\nprint(wine_data['price'].isnull().sum(),\"missing price(s)\") #check for null prices\n\n#impute missing prices with the median price\nwine_data.price.fillna(wine_data['price'].median(),inplace=True)\nprint(\"imputed from median price\")","46e76c87":"# Drop columns that are not needed\nwine_data_2 = wine_data.drop(['designation','region_1','region_2','taster_twitter_handle','description','province','taster_name'],axis=1)\nwine_data_2.head()","60c4c752":"wine_data_2.info() # confirm that there are no missing values","7978a3f5":"# Transformation\n# Label encoder transforms nominal features into numerical labels which algorithms can make sense of\ndef create_label_encoder_dict(df):\n    from sklearn.preprocessing import LabelEncoder\n    \n    label_encoder_dict = {}\n    \n    for column in df.columns:\n        if not np.issubdtype(df[column].dtype, np.number) and column != 'year':\n            label_encoder_dict[column]= LabelEncoder().fit(df[column])\n    return label_encoder_dict","29e17005":"\nlabel_encoders = create_label_encoder_dict(wine_data_2)\n#print(\"Encoded Values for each Label\")\n#print(\"=\"*32)\n#for column in label_encoders:\n #   print(\"=\"*32)\n #   print('Encoder(%s) = %s' % (column, label_encoders[column].classes_ ))\n  #  print(pd.DataFrame([range(0,len(label_encoders[column].classes_))], columns=label_encoders[column].classes_, index=['Encoded Values']  ).T)\n    ","64239f0c":"### Apply each encoder to the data set to obtain transformed values\nwd3 = wine_data_2.copy() # create copy of initial data set\nfor column in wd3.columns:\n    if column in label_encoders:\n        wd3[column] = label_encoders[column].transform(wd3[column])\n\nprint(\"Transformed data set\")\nprint(\"=\"*32)\nwd3.head()\n","278fb1d4":"# Function to do K-Fold Cross Validation\ndef cross_validate(x,y,kf_split):\n    from sklearn.model_selection import KFold\n    \n    #K-Fold Cross Validation\n    kf =KFold(n_splits=kf_split,shuffle=True,random_state=1)\n    \n    for train_index, test_index in kf.split(x):\n        X_train, X_test = x.iloc[train_index], x.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        \n    return [X_train,y_train,X_test,y_test]","ce991db6":"# Algorithms without Hyper Parameter Tuning\ndef pred_techniques(x,y,kf_split): \n    from sklearn.tree import DecisionTreeRegressor\n    from sklearn.neural_network import MLPRegressor\n\n    train_test = cross_validate(x,y,kf_split) #perform kfold cross validation\n\n    # Decision Tree Regressor\n    reg = DecisionTreeRegressor(random_state=1) \n    reg.fit(train_test[0], train_test[1])\n    \n    \n    # Multi-Layer Perceptron Regressor\n    clf = MLPRegressor(solver='adam', alpha=1e-5, activation='relu',learning_rate_init =0.01,shuffle=True,\n                    hidden_layer_sizes=(7, 4),random_state=1)\n    clf.fit(train_test[0],train_test[1])\n    \n\n    return [reg,clf,train_test[2],train_test[3],train_test[0],train_test[1]]","fd2fd68d":"# separate data into dependent (Y) and independent(X) variables\nfeature_cols =  ['variety','winery','location', 'year']\nx_data = wd3[feature_cols]\ny_data = wd3['points']\n\nl = pred_techniques(x_data,y_data,100)\nprint(\"Fig.5: Feature Significance\") \npd.DataFrame([ \"%.2f%%\" % perc for perc in (l[0].feature_importances_ * 100) ], index = x_data.columns, columns = ['Feature Significance in Decision Tree'])    ","80cafa74":"# Accuracy Test Scores of both techniques \nr2_tree = l[0].score(l[2],l[3])\nr2_nn = l[1].score(l[2],l[3])\n\nprint(\"Decision Tree Regressor\")\nprint(\"=\"*32)\nprint(\"R Square:\",r2_tree )\n\n\nprint(\"\\nMulti-Layer Perceptron Regressor\")\nprint(\"=\"*32)\nprint(\"R Square:\",r2_nn)\n","264a1338":"# Actual points and Predicted points by both models plus 4 predictor variables on left\nresults= l[2].copy()\nresults['Actual Points']=l[3]\npred_tree=l[0].predict(l[2])\npred_mlp=l[1].predict(l[2])\nresults['Dec_Tree_Reg']=pred_tree\nresults['MLP_Regressor']=pred_mlp\nprint(\"Fig.6: Actual Points and Predicted Points yielded from both Models\")\nresults.head()\n","3098b289":"# Calculate Variance in both models\nmse_treg = mean_squared_error(l[3],pred_tree)\nmse_nn = mean_squared_error(l[3],pred_mlp)\n\n# Calculate Standard Deviation in both models\nrmse_treg = math.sqrt(mean_squared_error(l[3],pred_tree))\nrmse_nn = math.sqrt(mean_squared_error(l[3],pred_mlp))\n\n# Calcualte Mean Absolute Error in both models\nmae_treg = mean_absolute_error(l[3],pred_tree)\nmae_nn = mean_absolute_error(l[3],pred_mlp)\n\n# Print evaluation metrics of both models\nprint(\"Decision Tree Regressor\")\nprint(\"=\"*32)\nprint(\"MSE:\",mse_treg)\nprint(\"RMSE:\",rmse_treg)\nprint(\"MAE:\",mae_treg)\n\nprint(\"\\nMulti-Layer Perceptron Regressor\")\nprint(\"=\"*32)\nprint(\"MSE:\",mse_nn)\nprint(\"RMSE:\",rmse_nn)\nprint(\"MAE:\",mae_nn)","5cd9cc61":"print(\"Decision Tree Number of Perfect Predictions:\")\nresults[results['Dec_Tree_Reg']==results['Actual Points']].Dec_Tree_Reg.count()","0a745903":"print(\"Neural Network Number of Perfect Predictions:\")\nresults[results['MLP_Regressor']==results['Actual Points']].MLP_Regressor.count()","fd5797b7":"sns.distplot( results[\"Actual Points\"] , color=\"skyblue\", label=\"Actual Points\")\nsns.distplot( results[\"Dec_Tree_Reg\"] , color=\"orange\", label=\"Decision Tree Predicted Points\")\nplt.legend()\nplt.xlabel(\"Points\",size=15)\nplt.title(\"Fig.7: Actual Points vs Decision Tree Predicted Points\", size=20)","0a3454ce":"sns.distplot( results[\"Actual Points\"] , color=\"skyblue\", label=\"Actual Points\")\nsns.distplot( results[\"MLP_Regressor\"] , color=\"red\", label=\"NN Predicted Points\")\nplt.legend()\nplt.xlabel(\"Points\",size=15)\nplt.title(\"Fig.8: Actual Points vs Neural Network Predicted Points\", size=20)","9af2a8af":"print(\"Fig.9: Summary of Evaluation Metrics\")\npd.DataFrame(dict(R_Square= [r2_tree,r2_nn],\n                  MSE=[mse_treg,mse_nn], RMSE=[rmse_treg,rmse_nn],MAE=[mae_treg,mae_nn]),\n                index=['Dec Tree Reg','MLP Reg'])\n","1f39cabc":"results.describe()","4f0b9a86":"sns.regplot(x=\"price\", y=\"points\", data=wine_data, fit_reg = False)\nplt.xlabel(\"Price\",size=12)\nplt.ylabel(\"Points\",size=12)\nplt.title(\"Fig.10: Correlation between Price and Points\",size=20)","fb577d90":"# Coverage of Vintage vs. Non-Vintage\nwine_data['type'].value_counts().plot(kind=\"pie\",autopct='%1.0f%%')\nlabels = 'Vintage', 'Non-Vintage'\nplt.legend(labels)\nplt.suptitle(\"Fig.11: Vintage vs. Non-Vintage Wine\", size=20)\nplt.ylabel('')","77482f3f":"# Non-Vintage Wine Points Distribution\n#wine_data[wine_data['year']== 0]\nno_year= wine_data[wine_data['year']==0]\nsns.boxplot(x=no_year.points)\nplt.xlabel(\"Points\",size=15)\nplt.title(\"Fig.12: Non-Vintage Wine Points Distribution  \", size=20)","f024b66b":"plt.figure(figsize=(10,7))\nsns.heatmap(wd3.corr(),cmap=plt.cm.Reds,)\nplt.xticks(size=12,rotation=45)\nplt.yticks(size=12)\nplt.title('Fig.13: Correlation between Transformed Data Columns ',size=20)","930f1455":"## Conclusion\n\nThe wine retailer is now able to know what wines to purchase and where to purchase them.","d083d6d7":"\n#### ====JUSTIFICATION OF USE====\n\nA cross validataion was done to help reduce the liklihood of selection bias and overfitting\nand to give insights as to how the predictive models will generalise to an unknown dataset.\n\nApart from popularity, there is no specific reason for choosing the K-Fold Cross Validation.\n","1fb673dd":"### Sample Dataset (i.e. wines evaluated by Roger Voss)","88b8a421":"## Appendices","3b9cece9":"## Preprocessing of Data","3287807b":"## Scenario:\nRoger Voss is  a famous master sommelier,  european editor at Wine Enthusiast and author of many books about wine.  **Wine Merchants** is a local wine retailer in France. They realized a trend in their wine sales. This trend shows that the higher the wine scores given by Mr. Roger Voss, the higher the sales of the wine. Wine Merchants wants to expand their wine offerings but are unsure of how these other wines would perform (in sales). Wine Merchant now wants to predict score Mr. Roger Voss would give these newer wine offerings inorder to select which wines will substantially increase their bottom line. \n\n\n### Food for thought\nBased on research, the taste, smell, texture and look of a bottle of wine is what affects the point\/rating a sommelier (wine expert) gives the wine. If these characteristics of the wine drives the points given to it, what are the factors that drive these characteristics to be of a certain quality.  \n","98d78794":"# Wine Points Prediction\nPrepared By: <\/br>\n    Latoya Clarke ,\n    <\/br>\n    Daniella Mcalla ,\n    Mardon Bailey ","0ad237b1":"## Selection of Data","1b105991":"## Mining of Data","f9da83ae":"## Transformation of Data","adf595b5":"#### Description of Dataset:\nThe Wine dataset consists of data about wine tasting reviews scraped from the Wine Enthusiast Magazine https:\/\/www.winemag.com\/?s=&drink_type=wine&page=12466on  . The dataset consists of only wines that have received a point between 80 and 100 inclusive. The dataset consists of 129, 971 observations (rows) and  14 attributes (columns) of which only a subset of these observations were used in the actual analysis. The sample of the dataset used in the analysis has a dimension of (25514, 14) and is exclusive to wines evaluated by Roger Voss.  From the 14 features in the sample dataset, only a few were selected to predict the points Roger Voss alloted to each wine.  Some of these features include the wine's designation, regions in which the grapes used to make the wine are grown and the type of grapes used to make the wine (variety) etc. Other features such as the vintage year were created via feature engineering in aid of developing models to predict the points ratings Roger Voss would give a wine. \n\nSince the aim is to create predictive models that will predict points Roger Voss is likely to give a wine, the sample dataset was not randomly selected. The selection was rather deliberate. \n\n* ###### Metadata:\n\nMore information on the dataset  such as the metadata may be accessed via this link https:\/\/www.kaggle.com\/zynicide\/wine-reviews\n\n#### Definition of Terms: \n\n* ###### Sommelier :\nA wine expert\/specialist. A knowledgeable wine professional \n\n* ###### Wine Tasting Review :\nAn event where sommeliers perform a sensory examination and evaluation of a wine. (i.e. taste, smell, feel & look of wine) \n\n* ###### Vintage Year : \nThe year on a bottle of  wine which denotes that most if not all the grapes used to make that bottle of wine were harvested in that pecified year\n\n\n\n","eff5d02b":"## Interpretation\/Evaluation","c3225f59":"\nThe two techniques used to predict the points Roger Voss would give a wine are Decision Tree Regressor (DTR) and Multi-Layer Perceptron Regressor (MLPR).  Both techniques were used because the response variable (points) is of numeric datatype and both are able to do multiple regression.\n\nDecision Tree\n\nThe Decision Tree Algorithm builds a tree like structure as a model which uses a top-down, greedy search through the space of possible branches with no backtracking.  The model breaks down the dataset into smaller and smaller subsets while the associated decision tree is created incrementally. The Decision Tree algorithm is simple to understand and interpret, has value even with little hard data, helps determine worst, best and expected values for different scenarios and can be combined with other decision techniques.\n\nMulti-Layer Perceptron (MLP) \n\nThe Multi-Layer Perceptron is the sum of several perceptions together. The input layer reads in the data and the output layer creates the resulting output. The Multi-Layer Perceptron model trains using backpropagation with no activation function in the output layer, which can also be seen as using the identity function as activation function. The Multi-Layer Perceptron uses a parameter alpha for regularization (L2 regularization) term which helps in avoiding overfitting by penalizing weights with large magnitudes. The Multi-layer Perceptron algorithm is capable of learning non-linear models in real-time and requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations. Multi-Layer Perception algorithm is also sensitive to feature scaling.\n\n\n The Decision Tree Regressor outperformed the MLP regressor  in this experiment. The R2 score  or \"coefficient of determination\" which depicts how well the data fits the model had a value of 0.278 for the Decision Tree Regressor and 0.020657 for the MLP Regressor.  These values are relatively low which suggest that the models did not fit the data very well. However, the standard deviations or \"RMSE\" of both models were relatively low.  This is an indication that the observations are not spread out but rather closer to the actual points which further suggests that the model did not perform as bad.  The Decision Tree Regressor had a standard deviation of 2.551697 and the MLP Regressor had a standard deviation of 2.972425. ","57567dd4":"## References\n\nAttending a Wine Tasting Event - dummies. (2018). Retrieved from https:\/\/www.dummies.com\/food-drink\/drinks\/wine\/attending-a-wine-tasting-event\/\n\nMetadata \nhttps:\/\/www.kaggle.com\/zynicide\/wine-reviews\n\nThe Different Types of Wine (Infographic) | Wine Folly. (2018). Retrieved from https:\/\/winefolly.com\/review\/different-types-of-wine\/"}}