{"cell_type":{"e71c973b":"code","035ef63f":"code","fa6ddaa0":"code","de43480f":"code","0efca967":"code","a681bff7":"code","401b63d0":"code","d440dbbd":"code","112c6433":"code","39c8747f":"code","681cda7b":"code","e96d8761":"code","660b814d":"code","0a54e683":"code","61e434ea":"code","f6a7dc81":"code","f46c7e0f":"code","b4d5f6e5":"code","cecbe311":"code","ad880010":"code","2acadb5b":"code","3be90baa":"code","d1cffe05":"code","9b1e3bc5":"code","3b740223":"code","d85e5e4f":"code","47c57032":"code","3a8177bb":"code","459937a3":"code","329cbf70":"code","b75aa7a7":"code","698eb3bc":"code","80d212cf":"code","a73109ae":"code","7ad8e857":"code","d9437abf":"code","90ae719b":"code","3e3198d9":"code","35c97c6e":"code","9632fc8d":"code","24136ce5":"code","57938bbb":"code","6b38fd05":"code","cb21218f":"code","f635c577":"code","0a65d490":"code","ebe41ff6":"code","a3a8476f":"code","e185ce38":"code","ea8edaaa":"code","d0c49b40":"code","50357541":"code","e8f5f61b":"code","23becac4":"code","a7776d32":"code","ea88cb10":"code","4f2aa634":"code","e3d65c13":"code","8836fc13":"code","d16d4804":"code","513199d3":"code","a6919e1c":"code","ecba1006":"code","08d37136":"code","1b086ef4":"code","9d560451":"code","3a52454a":"code","775ea000":"code","a0eb4bef":"code","8599de87":"code","cfde50cd":"markdown","838de01c":"markdown","b183f68e":"markdown","8b21638e":"markdown","132a739e":"markdown","21d3ab60":"markdown","b82d3a74":"markdown","a0d591b1":"markdown","f98f99b6":"markdown","473a5dc6":"markdown","a067d945":"markdown","a2b41322":"markdown","32a83938":"markdown","6367186a":"markdown","c1b70fc3":"markdown","cb477c41":"markdown","64009d1d":"markdown","cc305732":"markdown","d979ea4b":"markdown","b62d4db4":"markdown"},"source":{"e71c973b":"import pandas as pd\nimport numpy as np\nimport nltk","035ef63f":"train_tweets = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_tweets = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","fa6ddaa0":"train_tweets.head()","de43480f":"test_tweets.head()","0efca967":"import seaborn as sns\n\nsns.heatmap(train_tweets.isnull(),cbar=False)\nprint(train_tweets.isnull().sum())\nprint('-'*50)\nprint(test_tweets.isnull().sum())","a681bff7":"#Let's take a look to the tail\ntrain_tweets.tail()","401b63d0":"#What's my volumme of 0 & 1 in the target \ntrain_tweets['target'].value_counts()","d440dbbd":"x = ['No Emergency','Emergency']\ny = [4342,3271]\nimport plotly.graph_objects as go\n\nfig = go.Figure(data=[go.Bar(x=x, y=y,\n            hovertext=['No', 'Yes'])])\n\nfig.update_traces(marker_color='rgb(255,202,225)', marker_line_color='rgb(8,48,107)',\n                  marker_line_width=1.5, opacity=0.6)\nfig.update_layout(title_text='EMERGENCIES')\nfig.show()","112c6433":"import string\nstring.punctuation","39c8747f":"def remove_punctuation(text):\n    no_punct=[words for words in text if words not in string.punctuation]\n    words_wno_punct=''.join(no_punct)\n    return words_wno_punct","681cda7b":"#At this point I created another column withouth #,.,\/ etc...\ntrain_tweets['text_wno_punct'] = train_tweets['text'].apply(lambda x: remove_punctuation(x))\ntrain_tweets.head()","e96d8761":"# TEST DF\ntest_tweets['text_wno_punct'] = test_tweets['text'].apply(lambda x: remove_punctuation(x))\ntest_tweets.head()","660b814d":"#text = re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\nimport re\ndef removing_url(text):\n    return re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n\ntrain_tweets['text_wno_punct'].apply(lambda x: removing_url(x))\ntrain_tweets.head()","0a54e683":"# TEST DF\ntest_tweets['text_wno_punct'].apply(lambda x: removing_url(x))\ntest_tweets.head()","61e434ea":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               u\"\\U0001f926-\\U0001f937\"\n                               u\"\\U00010000-\\U0010ffff\"\n                               u\"\\u2640-\\u2642\"\n                               u\"\\u2600-\\u2B55\"\n                               u\"\\u200d\"\n                               u\"\\u23cf\"\n                               u\"\\u23e9\"\n                               u\"\\u231a\"\n                               u\"\\ufe0f\"  # dingbats\n                               u\"\\u3030\"\n                               \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","f6a7dc81":"train_tweets['text_wno_punct'].apply(lambda x: remove_emoji(x))\ntrain_tweets.head()","f46c7e0f":"# TEST DF\ntest_tweets['text_wno_punct'].apply(lambda x: remove_emoji(x))\ntest_tweets.head()","b4d5f6e5":"import unicodedata\ndef remove_non_ascii(words):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        new_words.append(new_word)\n    return new_words\n","cecbe311":"train_tweets['text_wno_punct'].apply(lambda x: remove_non_ascii(x))\ntrain_tweets.head()","ad880010":"# TEST DF\ntest_tweets['text_wno_punct'].apply(lambda x: remove_non_ascii(x))\ntest_tweets.head()","2acadb5b":"from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()","3be90baa":"train_tweets['text_wno_punct'].apply(lambda x: lemmatizer.lemmatize(x))\ntrain_tweets.head()","d1cffe05":"test_tweets['text_wno_punct'].apply(lambda x: lemmatizer.lemmatize(x))\ntest_tweets.head()","9b1e3bc5":"#texting\n#import re\n#def tokenize(text):\n#    split=re.split(\"\\W+\",text) \n#    return split","3b740223":"train_tweets['text_tokenization'] = train_tweets['text_wno_punct'].apply(lambda x: nltk.word_tokenize(x))\ntrain_tweets.head()","d85e5e4f":"test_tweets['text_tokenization'] = test_tweets['text_wno_punct'].apply(lambda x: nltk.word_tokenize(x))\ntest_tweets.head()","47c57032":"#testing code\n#train_tweets['text_wno_punct_split']= train_tweets['text_wno_emoji'].apply(lambda x: tokenize(x.lower()))\n#train_tweets.head()","3a8177bb":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize","459937a3":"## This line of code it's important, with this we avoid to get an error \"that says stopwords it's not iterable\" argument of type 'WordListCorpusReader' is not iterable\nstopword = nltk.corpus.stopwords.words('english')\nprint(stopword[:11])","329cbf70":"print(stopwords.words('english'))","b75aa7a7":"#testing code\ndef remove_stopwords(text):\n    text=[word.lower() for word in text if word not in stopword]\n    return text","698eb3bc":"train_tweets['text_wno_punct_split_wno_stopwords'] = train_tweets['text_tokenization'].apply(lambda x: remove_stopwords(x))\ntrain_tweets.head()","80d212cf":"test_tweets['text_wno_punct_split_wno_stopwords'] = test_tweets['text_tokenization'].apply(lambda x: remove_stopwords(x))\ntest_tweets.head()","a73109ae":"train_tweets['pos_tags'] = train_tweets['text_wno_punct_split_wno_stopwords'].apply(lambda x:[t for w, t in nltk.pos_tag(x)])\ntrain_tweets.head()","7ad8e857":"test_tweets['pos_tags'] = test_tweets['text_wno_punct_split_wno_stopwords'].apply(lambda x:[t for w, t in nltk.pos_tag(x)])\ntest_tweets.head()","d9437abf":"train_tweets['pos_tag_sentence'] = train_tweets['pos_tags'].apply(lambda x: ' '.join(x))\ntrain_tweets.head()","90ae719b":"test_tweets['pos_tag_sentence'] = test_tweets['pos_tags'].apply(lambda x: ' '.join(x))\ntest_tweets.head()","3e3198d9":"from nltk.probability import FreqDist\n\ndef frequency_distribution(text):\n    fdist=FreqDist(text)\n    most_common_w=fdist.most_common(10)\n    #print(most_common_w)  #\/\/ Will display all the words with their frequency in each line \n    return most_common_w","35c97c6e":"train_tweets['text_wno_punct_split_wno_stopwords'].apply(lambda x: frequency_distribution(x))","9632fc8d":"import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud","24136ce5":"# Define a function to plot word cloud\ndef plot_cloud(wordcloud):\n    # Set figure size\n    plt.figure(figsize=(30, 20))\n    # Display image\n    plt.imshow(wordcloud) \n    # No axis details\n    plt.axis(\"off\");","57938bbb":"wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='salmon', colormap='Pastel1', collocations=False).generate(' '.join(train_tweets['text_wno_punct']))\n# Plot\nplot_cloud(wordcloud)","6b38fd05":"#test df\nwordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='white', colormap='magma_r', collocations=False).generate(' '.join(test_tweets['text_wno_punct']))\n# Plot\nplot_cloud(wordcloud)","cb21218f":"from sklearn.feature_extraction.text import TfidfVectorizer","f635c577":"X = train_tweets['pos_tag_sentence']\ny = train_tweets['target']\n\nX_submission = test_tweets['pos_tag_sentence']","0a65d490":"vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n\nX = vectorizer.fit_transform(X)\nX_submission = vectorizer.transform(X_submission)","ebe41ff6":"print(vectorizer.get_feature_names())","a3a8476f":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","e185ce38":"X_train.todense()[0]","ea8edaaa":"X_train","d0c49b40":"XX = pd.DataFrame(\n    X_train.todense(),\n    columns=vectorizer.get_feature_names()\n)\n\nXX.head()","50357541":"from sklearn.linear_model import LogisticRegression","e8f5f61b":"lr_model = LogisticRegression(solver='newton-cg',penalty='l2', C=1.0)","23becac4":"lr_model.fit(X_train.todense(),y_train)\nlr_model.fit(X_test.todense(),y_test)","a7776d32":"acc_train = lr_model.score(X_train, y_train)\nacc_test = lr_model.score(X_test, y_test)\nprint(\"Prediction (Train):\", acc_train)\nprint(\"Prediction (Test):\", acc_test)","ea88cb10":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(lr_model, X_train, y_train, cv=5)\nlr_model.fit(X_train, y_train)\n\nfinal_score = lr_model.score(X_train, y_train)\nprint(f\"Scores: {scores} \\nMean: {scores.mean()} \\nFinal Score: {final_score}\")","4f2aa634":"from sklearn.naive_bayes import GaussianNB","e3d65c13":"nb_model = GaussianNB(var_smoothing = 2e-9)","8836fc13":"nb_model.fit(X_train.todense(),y_train)\nnb_model.fit(X_test.todense(),y_test)","d16d4804":"accnb_train = nb_model.score(X_train.toarray(), y_train)\naccnb_test = nb_model.score(X_test.toarray(), y_test)\nprint(\"Prediction (Train):\", accnb_train)\nprint(\"Prediction (Test):\", accnb_test)","513199d3":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\n\nscores = cross_val_score(nb_model, X_train.toarray(), y_train, cv=5)\nnb_model.fit(X_train.toarray(), y_train)\n\nfinal_score = nb_model.score(X_train.toarray(), y_train)\nprint(f\"Scores: {scores} \\nMean: {scores.mean()} \\nFinal Score: {final_score}\")","a6919e1c":"from sklearn.ensemble import RandomForestRegressor","ecba1006":"rfrmodel = RandomForestRegressor(n_estimators=500)","08d37136":"rfrmodel.fit(X_train.todense(),y_train)\nrfrmodel.fit(X_test.todense(),y_test)","1b086ef4":"accrfr_train = rfrmodel.score(X_train.toarray(), y_train)\naccrfr_test = rfrmodel.score(X_test.toarray(), y_test)\nprint(\"Prediction (Train):\", accrfr_train)\nprint(\"Prediction (Test):\", accrfr_test)","9d560451":"#X_submission = test_tweets['pos_tag_sentence']\n#X_submission = vectorizer.transform(X_submission)","3a52454a":"y_submission = lr_model.predict(X_submission.todense())","775ea000":"submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission.head()","a0eb4bef":"submission['target'] = y_submission","8599de87":"submission.to_csv(\"my_submission.csv\", index=False)","cfde50cd":"#### *RandomForestRegressor*","838de01c":"### Let's delete unuseful words ","b183f68e":"### Usar regresion logistica","8b21638e":"### **Tokenization**\nMy idea it's to slplit the words so I can classify or analize better each single word ","132a739e":"### **Lemmatization**","21d3ab60":"### **Stopwords**","b82d3a74":"### **Post-Taging**","a0d591b1":"### **Punctuation**","f98f99b6":"I want to know my most frequent words so I can analize which ones could be modify or fixed. But first let's apply stopwords so it can be a little bit more clean!!","473a5dc6":"### **Removing URL**","a067d945":"#### *Naive Bayes*\n\nSame case, working with probabilities","a2b41322":"### **Models**","32a83938":"## TfidfVectorizer\n\nfind the most used words in the dataset ","6367186a":"### **WORD CLOUD**","c1b70fc3":"#### *Logistic Regresion*\n\nWhy?\nThis model could fit very well because can classify between TRUE\/FALSE ","cb477c41":"### *Frequency Distribution*\nWhich one are the most commons words?","64009d1d":"### **Removing non ascii**","cc305732":"### Cleaning data\nHow many nulls do we have?\nIs it necessary to do some transformation?","d979ea4b":"## Final Prediction using Logistic Regresion","b62d4db4":"### **Removing Emoticons**"}}