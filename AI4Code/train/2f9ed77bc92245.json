{"cell_type":{"03cb2c3e":"code","127e5ce2":"code","bc4bb203":"code","064f507c":"code","8f66f5b8":"code","2af5859c":"code","349a3b19":"code","f2361191":"code","6cd34fb3":"code","c7987fae":"code","1c6ecca2":"code","46165586":"code","c50c9474":"code","71b07cbb":"code","9c1e7670":"code","c42ab447":"code","c9b46030":"code","4fd5f378":"code","a4961b30":"code","7705504f":"code","7bf17b46":"code","b8282356":"code","e62ae959":"code","2afbcef6":"code","1ae5184d":"code","cd44b01e":"code","874a599c":"code","8a5d359a":"code","e90e9836":"code","3e76bc44":"code","c3d12910":"code","c68d6032":"markdown","cd03dfc2":"markdown","22bcc7b0":"markdown","836dc75a":"markdown","a7372d64":"markdown","49893ec4":"markdown","18eae831":"markdown"},"source":{"03cb2c3e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","127e5ce2":"import numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom typing import *\n\nimport torch\nimport torch.optim as optim","bc4bb203":"from fastai import *\nfrom fastai.vision import *\nfrom fastai.text import *\nfrom fastai.callbacks import *","064f507c":"%%bash\npip install pytorch-pretrained-bert","8f66f5b8":"class Config(dict):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n    \n    def set(self, key, val):\n        self[key] = val\n        setattr(self, key, val)\n\nconfig = Config(\n    testing=False,\n    bert_model_name=\"bert-large-uncased\",\n    max_lr=3e-5,\n    epochs=4,\n    use_fp16=True,\n    bs=16,\n    discriminative=False,\n    max_seq_len=256,\n)","2af5859c":"from pytorch_pretrained_bert import BertTokenizer\nbert_tok = BertTokenizer.from_pretrained(\n    config.bert_model_name,\n)","349a3b19":"def _join_texts(texts:Collection[str], mark_fields:bool=False, sos_token:Optional[str]=BOS):\n    \"\"\"Borrowed from fast.ai source\"\"\"\n    if not isinstance(texts, np.ndarray): texts = np.array(texts)\n    if is1d(texts): texts = texts[:,None]\n    df = pd.DataFrame({i:texts[:,i] for i in range(texts.shape[1])})\n    text_col = f'{FLD} {1} ' + df[0].astype(str) if mark_fields else df[0].astype(str)\n    if sos_token is not None: text_col = f\"{sos_token} \" + text_col\n    for i in range(1,len(df.columns)):\n        #text_col += (f' {FLD} {i+1} ' if mark_fields else ' ') + df[i]\n        text_col += (f' {FLD} {i+1} ' if mark_fields else ' ') + df[i].astype(str)\n    return text_col.values","f2361191":"class FastAiBertTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n\n    def __call__(self, *args, **kwargs):\n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length\"\"\"\n        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]\n","6cd34fb3":"from sklearn.model_selection import train_test_split\n\nDATA_ROOT = Path(\"..\") \/ \"input\/janatahack-independence-day-2020-ml-hackathon\"\n\ntrain, test = [pd.read_csv(DATA_ROOT \/ fname) for fname in [\"train.csv\", \"test.csv\"]]\n\ntrain['comment_text'] = train['TITLE'] + train['ABSTRACT']\ntest['comment_text'] = test['TITLE'] + test['ABSTRACT']\n\ntrain, val = train_test_split(train, shuffle=True, test_size=0.2, random_state=42)\n\n# val = train # we won't be using a validation set but you can easily create one using train_test_split","c7987fae":"train.shape, test.shape","1c6ecca2":"if config.testing:\n    train = train.head(1024)\n    val = val.head(1024)\n    test = test.head(1024)","46165586":"fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))","c50c9474":"fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=config.max_seq_len), pre_rules=[], post_rules=[])","71b07cbb":"label_cols = ['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']\n\n","9c1e7670":"class BertTokenizeProcessor(TokenizeProcessor):\n    def __init__(self, tokenizer):\n        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n\nclass BertNumericalizeProcessor(NumericalizeProcessor):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n\ndef get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n    \"\"\"\n    Constructing preprocessors for BERT\n    We remove sos\/eos tokens since we add that ourselves in the tokenizer.\n    We also use a custom vocabulary to match the numericalization with the original BERT model.\n    \"\"\"\n    return [BertTokenizeProcessor(tokenizer=tokenizer),\n            NumericalizeProcessor(vocab=vocab)]","c42ab447":"class BertDataBunch(TextDataBunch):\n    @classmethod\n    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n                tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n                label_cols:IntsOrStrs=0, label_delim:str=None, **kwargs) -> DataBunch:\n        \"Create a `TextDataBunch` from DataFrames.\"\n        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n        # use our custom processors while taking tokenizer and vocab as kwargs\n        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n                        TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n        return src.databunch(**kwargs)\n","c9b46030":"# this will produce a virtually identical databunch to the code above\ndatabunch = BertDataBunch.from_df(\".\", train, val, test,\n                  tokenizer=fastai_tokenizer,\n                  vocab=fastai_bert_vocab,\n                  text_cols=\"comment_text\",\n                  label_cols=label_cols,\n                  bs=config.bs,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )","4fd5f378":"from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification\nbert_model = BertForSequenceClassification.from_pretrained(config.bert_model_name, num_labels=6)","a4961b30":"loss_func = nn.BCEWithLogitsLoss()","7705504f":"acc_02 = partial(accuracy_thresh, thresh=0.5)\nf_score = partial(fbeta, thresh=0.5)","7bf17b46":"from fastai.callbacks import *\n\nlearner = Learner(\n    databunch, bert_model,\n    loss_func=loss_func, metrics=[acc_02,f_score]\n)\nif config.use_fp16: learner = learner.to_fp16()","b8282356":"learner.lr_find()","e62ae959":"learner.recorder.plot()","2afbcef6":"learner.fit_one_cycle(config.epochs, max_lr=config.max_lr)","1ae5184d":"def get_preds_as_nparray(ds_type) -> np.ndarray:\n    \"\"\"\n    the get_preds method does not yield the elements in order by default\n    we borrow the code from the RNNLearner to resort the elements into their correct order\n    \"\"\"\n    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n    sampler = [i for i in databunch.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    return preds[reverse_sampler, :]","cd44b01e":"test_preds = get_preds_as_nparray(DatasetType.Test)","874a599c":"sample_submission = pd.read_csv(DATA_ROOT \/ \"sample_submission_UVKGLZE.csv\")\nif config.testing: sample_submission = sample_submission.head(test.shape[0])\nsample_submission[label_cols] = test_preds\nsample_submission.to_csv(\"predictions.csv\", index=False)","8a5d359a":"sample_submission.head()","e90e9836":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\nname = \"sample_submission_bert-large-cased.csv\"\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = name):  \n    csv = sample_submission.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n\n# create a link to download the dataframe\ncreate_download_link(sample_submission)","3e76bc44":"submission_binary = sample_submission.copy()\n\ncols_target = ['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']\nfor col in cols_target:\n    submission_binary[col] = submission_binary[col].apply(lambda x: 1 if x >= 0.40 else 0)","c3d12910":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\nname = \"submission_binary.csv\"\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = name):  \n    csv = submission_binary.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n\n# create a link to download the dataframe\ncreate_download_link(submission_binary)","c68d6032":"![image.png](attachment:image.png)","cd03dfc2":"### References\/Credits","22bcc7b0":"### Model","836dc75a":"![image.png](attachment:image.png)","a7372d64":"As this hackathon is similar to the contest conducted in Kaggle - I referred to these kernels.\n\nhttps:\/\/www.kaggle.com\/clinma\/eda-toxic-comment-classification-challenge\n\nhttps:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/discussion\/100661\n\nhttps:\/\/www.kaggle.com\/abhikjha\/jigsaw-toxicity-bert-with-fastai-and-fastai\/notebook\n\nhttps:\/\/www.kaggle.com\/rhodiumbeng\/classifying-multi-label-comments-0-9741-lb\n\n","49893ec4":"## 84.71% Public LB Score- Rank 11 ( Bert Large Uncased - FAST AI) - Solution \n\n## 84.81% Private LB Score - Rank 10 ","18eae831":"![image.png](attachment:image.png)"}}