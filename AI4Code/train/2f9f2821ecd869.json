{"cell_type":{"f65a3280":"code","0c126699":"code","902672e1":"code","b5f5dd5e":"code","f8fe34fb":"code","7dc48a96":"code","6fe7d91b":"code","b294f21c":"code","17b97d62":"code","8aed03f4":"code","21150232":"code","dab09c70":"code","9900a09c":"code","18149ae1":"code","f896729f":"code","99c465b6":"code","682f9433":"code","bc5fe056":"code","162b0e8c":"code","75d5e94a":"code","eab329f2":"code","99d84646":"code","a8c94d04":"code","db62fa87":"code","6909f53c":"code","b0d39a14":"code","e536a3cc":"code","36927b48":"code","758b01de":"code","92e6f212":"code","58750bca":"code","3de1f14c":"code","946093fe":"code","73a6a54a":"code","c8433290":"code","fbcec845":"code","566303b3":"code","868322b3":"code","e89b312b":"code","01de31be":"code","07f87a95":"code","78280176":"code","36fbc021":"code","889af9d3":"code","e1c9cc2e":"code","efc07542":"code","b0888831":"code","1e7b7886":"markdown","456aa0f1":"markdown","df7dba98":"markdown"},"source":{"f65a3280":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0c126699":"#Importing required libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","902672e1":"import os\n\nprint(os.listdir('\/kaggle\/input\/flowers-recognition'))\nprint(os.listdir('\/kaggle\/input\/flowers-recognition\/flowers'))\n","b5f5dd5e":"#Assigning directory for easy access\nroot_dir = '\/kaggle\/input\/flowers-recognition\/flowers'\ndaisy_dir = os.path.join(root_dir,'daisy')\ndaisy_fnames = os.listdir(daisy_dir)\ndandelion_dir = os.path.join(root_dir,'dandelion')\ndandelion_fnames = os.listdir(dandelion_dir)\nsunflower_dir = os.path.join(root_dir,'sunflower')\nsunflower_fnames = os.listdir(sunflower_dir)\ntulip_dir = os.path.join(root_dir,'tulip')\ntulip_fnames = os.listdir(tulip_dir)\nrose_dir = os.path.join(root_dir,'rose')\nrose_fnames = os.listdir(rose_dir)","f8fe34fb":"print(f'Daisy Files: {daisy_fnames[:10]}')\nprint(f'Sunflower Files: {sunflower_fnames[:10]}')","7dc48a96":"#Checking for class imbalance\nnum_daisy = len(daisy_fnames)\nnum_dandelion = len(dandelion_fnames)\nnum_sunflower = len(sunflower_fnames)\nnum_tulip = len(tulip_fnames)\nnum_rose = len(rose_fnames)\n\ndf = pd.DataFrame(data = np.array([[num_daisy,num_dandelion,num_sunflower,num_tulip,num_rose]]),columns=['num_daisy','num_dandelion','num_sunflower','num_tulip','num_rose'])\nplt.figure(figsize=(12,6))\nsns.barplot(data=df)\nplt.title('Number of Flowers in Dataset');","6fe7d91b":"#Importing Libraries to visualize data\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nfrom PIL import Image\nimport matplotlib.image as mpimg","b294f21c":"# img = image.load_img(os.path.join(daisy_dir,daisy_fnames[4])) #Using Tensorflow\n# img =  Image.open(os.path.join(daisy_dir,daisy_fnames[4])) # Using PIL library\n# img = np.array(img)\n\nimg = mpimg.imread(os.path.join(daisy_dir,daisy_fnames[4]))\nplt.imshow(img);\nprint(img.shape)\n","17b97d62":"import cv2\n\nimg = cv2.resize(img,(150,150))\nplt.imshow(img)","8aed03f4":"X = []\ny = []","21150232":"def make_set(flower_name):\n    for i in range(len(os.listdir(os.path.join(root_dir,flower_name)))):\n        img = mpimg.imread(os.path.join(root_dir,flower_name,os.listdir(os.path.join(root_dir,flower_name))[i]))\n        img = cv2.resize(img,(150,150))\n        X.append(img)\n        y.append(flower_name)\n    return X,y\n        \n        ","dab09c70":"X,y = make_set('daisy')","9900a09c":"X,y = make_set('dandelion')","18149ae1":"X,y = make_set('sunflower')\nX,y = make_set('tulip')\nX,y = make_set('rose')","f896729f":"y.count('sunflower')","99c465b6":"X = np.array(X)\ny = np.array(y)\n","682f9433":"print(X.shape)\nprint(y.shape)","bc5fe056":"#Visualizaing Dataset\nplt.figure(figsize=(16,10))\nfor i in range(25):\n    plt.subplot(5,5,1+i)\n    ran_idx = np.random.choice(len(X))\n    plt.imshow(X[ran_idx])\n    plt.title(y[ran_idx])\nplt.tight_layout()\nplt.show()","162b0e8c":"X = X\/255\ny[y=='daisy']=0\ny[y=='dandelion']=1\ny[y=='sunflower']=2\ny[y=='tulip']=3\ny[y=='rose']=4\n# print(y[10],y[25],y[999])\ny= np.array(y,dtype=int)\ny = y.reshape(-1,1)\n# print(y[:10])\n","75d5e94a":"from sklearn.model_selection import train_test_split","eab329f2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","99d84646":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","a8c94d04":"import tensorflow as tf\nfrom tensorflow import keras","db62fa87":"model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(64,(3,3),input_shape=(150,150,3,),activation='relu'),\n                                    tf.keras.layers.MaxPooling2D((2,2)),\n                                    tf.keras.layers.Dropout(0.5),\n                                    \n                                    \n                                    tf.keras.layers.Conv2D(128,(3,3),activation='relu'),\n                                    tf.keras.layers.MaxPooling2D((2,2)),\n                                    tf.keras.layers.Dropout(0.4),\n                                    \n                                    \n                                    tf.keras.layers.Conv2D(128,(3,3),activation='relu'),\n                                    tf.keras.layers.MaxPooling2D((2,2)),\n                                    tf.keras.layers.Dropout(0.4),\n                                    \n                                    \n                                   \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.Dropout(0.4),\n                                    tf.keras.layers.Dense(256,activation='relu'),\n                                    tf.keras.layers.Dropout(0.4),\n                                    tf.keras.layers.Dense(5,activation='softmax')])\nmodel.compile(optimizer='adam',\n             loss='sparse_categorical_crossentropy',\n             metrics=['accuracy'])","6909f53c":"model.summary()","b0d39a14":"# from keras.preprocessing.image import ImageDataGenerator\n\n# datagen = ImageDataGenerator(zoom_range=0.1,\n#                              rotation_range=10,\n#                              width_shift_range=0.2,\n#                              height_shift_range=0.2,\n#                              horizontal_flip=True)\n","e536a3cc":"history = model.fit(x=X_train,y=y_train, batch_size=128,epochs=50,validation_data=(X_test,y_test),steps_per_epoch=len(X_train)\/\/128,verbose=1)","36927b48":"# from keras.preprocessing.image import ImageDataGenerator\n\n# datagen = ImageDataGenerator(zoom_range=0.1,\n#                              width_shift_range=0.2,\n#                              height_shift_range=0.2,\n#                              horizontal_flip=True)","758b01de":"# history = model.fit_generator(datagen.flow(X_train,y_train, batch_size=128),epochs=50,validation_data=(X_test,y_test),steps_per_epoch=len(X_train)\/\/128,verbose=1)","92e6f212":"pred = model.predict(X_train[4].reshape(1,150,150,3)).argmax()\npred","58750bca":"print(y_train[4])\nplt.imshow(X_train[4]);","3de1f14c":"pred = model.predict(X_test[10].reshape(1,150,150,3)).argmax()\npred","946093fe":"print(y_test[10])\nplt.imshow(X_test[10]);","73a6a54a":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']","c8433290":"plt.plot(range(50),acc,label='Training Accuracy')\nplt.plot(range(50),val_acc,label='Test Accuracy')\nplt.legend()","fbcec845":"plt.plot(range(50),loss,label='Training Loss')\nplt.plot(range(50),val_loss,label='Test Loss')\nplt.legend()","566303b3":"from tensorflow.keras.applications.vgg16 import VGG16","868322b3":"pre_trained_model = VGG16(input_shape = (150, 150, 3), \n                                include_top = False, \n                                weights = None,\n                                pooling='avg')","e89b312b":"print(os.listdir('\/kaggle\/input\/vgg16'))\nweights='\/kaggle\/input\/vgg16\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\npre_trained_model.load_weights(weights)","01de31be":"pre_trained_model.summary()","07f87a95":"\n\nmodel2= keras.models.Sequential()\nmodel2.add(pre_trained_model)\nmodel2.add(keras.layers.Dropout(0.2))\nmodel2.add(keras.layers.Dense(512,activation='relu'))\nmodel2.add(keras.layers.Dropout(0.5))\nmodel2.add(keras.layers.Dense(64,activation='relu'))\nmodel2.add(keras.layers.Dropout(0.4))\nmodel2.add(keras.layers.Dense(5,activation='softmax'))\n\nmodel2.summary()","78280176":"pre_trained_model.trainable=False","36fbc021":"model2.compile(optimizer='adam',\n             loss='sparse_categorical_crossentropy',\n             metrics=['accuracy'])","889af9d3":"history2 = model2.fit(x=X_train,y=y_train, batch_size=128,epochs=40,validation_data=(X_test,y_test),steps_per_epoch=len(X_train)\/\/128,verbose=1)","e1c9cc2e":"acc = history2.history['accuracy']\nval_acc = history2.history['val_accuracy']\nloss = history2.history['loss']\nval_loss = history2.history['val_loss']","efc07542":"plt.plot(range(40),acc,label='Training Accuracy')\nplt.plot(range(40),val_acc,label='Test Accuracy')\nplt.legend()","b0888831":"plt.plot(range(40),loss,label='Training Loss')\nplt.plot(range(40),val_loss,label='Test Loss')\nplt.legend()","1e7b7886":"There is no seperate train and test dataset. So we would need to first import entire dataset and then seperate it into train and test dataset manually ","456aa0f1":"# Making Training and Test Set","df7dba98":"# Creating Model"}}