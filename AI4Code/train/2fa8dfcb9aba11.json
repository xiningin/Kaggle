{"cell_type":{"8d5dda63":"code","8c5a3ad3":"code","9a8acbb7":"code","e04bccce":"code","0c5a7f7b":"code","1ebd77bb":"code","625d9864":"code","ce799c5d":"code","ca4a5792":"code","1ff461df":"code","9625dfdc":"code","05a92bb8":"code","504c849b":"code","9083d409":"code","dd1d1b95":"code","58ff9fcb":"code","077b5d85":"code","57e7e46d":"code","b6d6ec47":"code","d4bb21af":"code","46d60d84":"code","30b5beff":"code","596eef5b":"code","8bb5355c":"code","190b948c":"code","5ab99d83":"code","70136e78":"code","f5f7a13f":"code","aa031a2b":"code","50f61621":"code","fc098207":"code","db4209fb":"code","fde10446":"code","f16bcd4e":"code","8cd3e7ee":"code","5eace8a5":"code","05c3caa5":"code","0472790f":"markdown","d23feaa2":"markdown","56b949fa":"markdown","32eb25e4":"markdown","13633808":"markdown","be9ac08a":"markdown","81a29b60":"markdown"},"source":{"8d5dda63":"!pip install transformers --quiet","8c5a3ad3":"# Importing the required Libraries\nimport transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nimport torch\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nimport re\nfrom textblob import TextBlob\nfrom collections import Counter\nfrom nltk import word_tokenize, ngrams\nfrom tqdm import tqdm\nfrom string import punctuation\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom matplotlib import rc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score\nfrom collections import defaultdict\nfrom textwrap import wrap\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.utils import resample\nimport warnings\nfrom nltk import word_tokenize, ngrams\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\nwarnings.filterwarnings('ignore')\n\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize'] = 12, 8\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Stopwords\nstop_words = set(stopwords.words('english'))\n\n# Punctuations\npunctuation = punctuation + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'","9a8acbb7":"# Importing the dataset\ndf = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/train.csv')\ndf.head()","e04bccce":"df['target'].value_counts()","0c5a7f7b":"# Checing for the null values\ndf.isnull().sum(axis = 0)","1ebd77bb":"# Changing the target values for the EDA for now\ndf['target'] = df['target'].map({1 : 'Insincere', 0 : 'Sincere'})","625d9864":"# Distribution of the Insincere and Sincere Questions\nplt.rcParams['figure.figsize'] = [10, 20]\n# fig, ax = plt.subplots(2, 1)\n# sns.set(font_scale = 1.4, style = 'whitegrid')\n# distribution = sns.countplot(df['target'], palette = ['darkcyan', 'crimson'], orient = 'h', ax = ax[0])\n# distribution.set(title = 'Distribution of the Question Types', xlabel = 'Question type', ylabel = 'Count');\n\n# # Creating the labels for the piechart\ntypes = df['target'].value_counts()\nlabels = list(types.index)\naggregate = list(types.values)\npercentage = [(x*100)\/sum(aggregate) for x in aggregate]\nprint (\"The percentages of Sincere and Insincere Questions are : \", percentage)\n\n# Plotting the Piechart to see the percentage distribution of the questions\nplt.rcParams.update({'font.size': 16})\nexplode = (0, 0.1)\n# ax[1].pie(aggregate, labels = labels, autopct='%1.2f%%', shadow=True, colors = ['darkcyan', 'crimson'])\nplt.pie(aggregate, labels = labels, autopct='%1.2f%%', shadow=True, colors = ['darkcyan', 'crimson'])\nplt.legend(labels, loc = 'best')\n#plt.axis('equal')\nplt.tight_layout()\nplt.show()","ce799c5d":"# Copying the dataset for the ngrams so that it won't effect the further processes\ndf_ = df.copy()\ndf_['question_text'] = df_['question_text'].str.replace(r'[^\\w\\d\\s]',' ')\n\n# Segregating the questions\ndf_insincere = \" \".join(df_.loc[df_.target == 'Insincere', 'question_text'])\ndf_sincere = \" \".join(df_.loc[df_.target == 'Sincere', 'question_text'])\n\n# Tokenizing the Sentences\ntokenized_insincere = word_tokenize(df_insincere)\ntokenized_sincere = word_tokenize(df_sincere)","ca4a5792":"# Unigrams\nsns.set(style = 'whitegrid', font_scale = 1.5)\nunigram_insincere = ngrams(tokenized_insincere, 1)\nunigram_sincere = ngrams(tokenized_sincere, 1)\n\n# Making the Frequency chart for the Unigrams\nfrequency_insincere = Counter(unigram_insincere) \nfrequency_sincere = Counter(unigram_sincere)\n\ndf_freq_insincere = pd.DataFrame(frequency_insincere.most_common(20))\ndf_freq_sincere = pd.DataFrame(frequency_sincere.most_common(20))\n\n# Barplot that shows the top 20 Unigrams\nplt.rcParams['figure.figsize'] = [20, 12]\nfig, ax = plt.subplots(1, 2)\nsns.set(font_scale = 1.3, style = 'darkgrid')\n\nsns_sincere = sns.barplot(x = df_freq_sincere[1], y = df_freq_sincere[0], color = 'darkslateblue', ax = ax[0])\nsns_insincere = sns.barplot(x = df_freq_insincere[1], y = df_freq_insincere[0], color = 'cadetblue', ax = ax[1])\n\n# Setting axes\nsns_sincere.set(title = \"Top 20 Unigrams of the Sincere Questions\", ylabel = \"Unigrams\", xlabel = \"Frequency\")\nsns_insincere.set(title = \"Top 20 Unigrams of the Insincere Questions\", xlabel = \"Frequency\", ylabel = \"\");","1ff461df":"# Bigrams\nsns.set(style = 'whitegrid', font_scale = 1.3)\nbigram_insincere = ngrams(tokenized_insincere, 2)\nbigram_sincere = ngrams(tokenized_sincere, 2)\n\n# Making the Frequency chart for the Bigrams\nfrequency_insincere = Counter(bigram_insincere) \nfrequency_sincere = Counter(bigram_sincere)\n\ndf_freq_insincere = pd.DataFrame(frequency_insincere.most_common(20))\ndf_freq_sincere = pd.DataFrame(frequency_sincere.most_common(20))\n\n# Barplot that shows the top 20 Bigrams\nplt.rcParams['figure.figsize'] = [20, 12]\nfig, ax = plt.subplots(1, 2)\nsns.set(font_scale = 1.3, style = 'darkgrid')\n\nsns_sincere = sns.barplot(x = df_freq_sincere[1], y = df_freq_sincere[0], color = 'darkslateblue', ax = ax[0])\nsns_insincere = sns.barplot(x = df_freq_insincere[1], y = df_freq_insincere[0], color = 'cadetblue', ax = ax[1])\n\n# Setting axes\nsns_sincere.set(title = \"Top 20 Bigrams of the Sincere Questions\", ylabel = \"Bigrams\", xlabel = \"Frequency\")\nsns_insincere.set(title = \"Top 20 Bigrams of the Insincere Questions\", xlabel = \"Frequency\", ylabel = \"\");","9625dfdc":"# Polarity of the questions \ndef sentiment_polarity(questions):\n    # Sentiment polarity of the questions\n    pol = []\n    for i in questions:\n        analysis = TextBlob(i)\n        pol.append(analysis.sentiment.polarity)\n    return pol\n\n# Subjectivity of the questions\ndef sentiment_subjectivity(questions):\n    # Sentiment subjectivity of the questions\n    sub = []\n    for i in questions:\n        analysis = TextBlob(i)\n        sub.append(analysis.sentiment.subjectivity)\n    return sub\n\n# Appeding the polarity and subjectivity of the text in the dataframe\ndf['polarity'] = sentiment_polarity(df['question_text'])\ndf['subjectivity'] = sentiment_subjectivity(df['question_text'])","05a92bb8":"# Distribution plot\nplt.rcParams['figure.figsize'] = [20, 10]\nsns.set(style = 'white', font_scale = 1.5)\n\ndist_ = sns.distplot(df['polarity'], kde = False, color = 'deepskyblue')\n# Setting the axes\ndist_.set(title = 'Distribution of the Polarity', xlabel = 'Polarity', ylabel = 'Frequency');","504c849b":"# Distribution plot\nplt.rcParams['figure.figsize'] = [20, 10]\nsns.set(style = 'white', font_scale = 1.5)\ndist_ = sns.distplot(df['subjectivity'], kde = False, color = 'red')\n# Setting the axes\ndist_.set(title = 'Distribution of the Subjectivity', xlabel = 'Subjectivity', ylabel = 'Frequency');","9083d409":"# Changing the target values for BERT\ndf['target'] = df['target'].map({'Insincere' : 1, 'Sincere' : 0})\n\n# Dropping polarity and subjectivity as it's not going to be used in training BERT\ndf.drop(columns = ['polarity', 'subjectivity'], inplace = True)","dd1d1b95":"# Contraction Dictionary for the expansion\ncontractions_dict = {\n    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\",\n    \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n    \"doesn\u2019t\": \"does not\", \"don't\": \"do not\", \"don\u2019t\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\",\n    \"haven't\": \"have not\", \"he'd\": \"he had\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\",\n    \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n    \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\"might've\": \"might have\",\n    \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n    \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n    \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n    \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n    \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y\u2019all\": \"you all\", \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\",\n    \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", \"ain\u2019t\": \"am not\", \"aren\u2019t\": \"are not\",\n    \"can\u2019t\": \"cannot\", \"can\u2019t\u2019ve\": \"cannot have\", \"\u2019cause\": \"because\", \"could\u2019ve\": \"could have\", \"couldn\u2019t\": \"could not\", \"couldn\u2019t\u2019ve\": \"could not have\",\n    \"didn\u2019t\": \"did not\", \"doesn\u2019t\": \"does not\", \"don\u2019t\": \"do not\", \"don\u2019t\": \"do not\", \"hadn\u2019t\": \"had not\", \"hadn\u2019t\u2019ve\": \"had not have\",\n    \"hasn\u2019t\": \"has not\", \"haven\u2019t\": \"have not\", \"he\u2019d\": \"he had\", \"he\u2019d\u2019ve\": \"he would have\", \"he\u2019ll\": \"he will\", \"he\u2019ll\u2019ve\": \"he will have\",\n    \"he\u2019s\": \"he is\", \"how\u2019d\": \"how did\", \"how\u2019d\u2019y\": \"how do you\", \"how\u2019ll\": \"how will\", \"how\u2019s\": \"how is\", \"i\u2019d\": \"i would\", \"i\u2019d\u2019ve\": \"i would have\",\n    \"i\u2019ll\": \"i will\", \"i\u2019ll\u2019ve\": \"i will have\", \"i\u2019m\": \"i am\", \"i\u2019ve\": \"i have\", \"isn\u2019t\": \"is not\", \"it\u2019d\": \"it would\", \"it\u2019d\u2019ve\": \"it would have\",\n    \"it\u2019ll\": \"it will\", \"it\u2019ll\u2019ve\": \"it will have\", \"it\u2019s\": \"it is\", \"let\u2019s\": \"let us\", \"ma\u2019am\": \"madam\", \"mayn\u2019t\": \"may not\",\n    \"might\u2019ve\": \"might have\", \"mightn\u2019t\": \"might not\", \"mightn\u2019t\u2019ve\": \"might not have\", \"must\u2019ve\": \"must have\", \"mustn\u2019t\": \"must not\",\n    \"mustn\u2019t\u2019ve\": \"must not have\", \"needn\u2019t\": \"need not\", \"needn\u2019t\u2019ve\": \"need not have\", \"o\u2019clock\": \"of the clock\",\n    \"oughtn\u2019t\": \"ought not\", \"oughtn\u2019t\u2019ve\": \"ought not have\", \"shan\u2019t\": \"shall not\", \"sha\u2019n\u2019t\": \"shall not\", \"shan\u2019t\u2019ve\": \"shall not have\",\n    \"she\u2019d\": \"she would\", \"she\u2019d\u2019ve\": \"she would have\", \"she\u2019ll\": \"she will\", \"she\u2019ll\u2019ve\": \"she will have\", \"she\u2019s\": \"she is\",\n    \"should\u2019ve\": \"should have\", \"shouldn\u2019t\": \"should not\", \"shouldn\u2019t\u2019ve\": \"should not have\", \"so\u2019ve\": \"so have\", \"so\u2019s\": \"so is\",\n    \"that\u2019d\": \"that would\", \"that\u2019d\u2019ve\": \"that would have\", \"that\u2019s\": \"that is\", \"there\u2019d\": \"there would\", \"there\u2019d\u2019ve\": \"there would have\",\n    \"there\u2019s\": \"there is\", \"they\u2019d\": \"they would\", \"they\u2019d\u2019ve\": \"they would have\", \"they\u2019ll\": \"they will\", \"they\u2019ll\u2019ve\": \"they will have\",\n    \"they\u2019re\": \"they are\", \"they\u2019ve\": \"they have\", \"to\u2019ve\": \"to have\", \"wasn\u2019t\": \"was not\", \"we\u2019d\": \"we would\", \"we\u2019d\u2019ve\": \"we would have\",\n    \"we\u2019ll\": \"we will\", \"we\u2019ll\u2019ve\": \"we will have\", \"we\u2019re\": \"we are\", \"we\u2019ve\": \"we have\", \"weren\u2019t\": \"were not\", \"what\u2019ll\": \"what will\",\n    \"what\u2019ll\u2019ve\": \"what will have\", \"what\u2019re\": \"what are\", \"what\u2019s\": \"what is\", \"what\u2019ve\": \"what have\", \"when\u2019s\": \"when is\",\n    \"when\u2019ve\": \"when have\", \"where\u2019d\": \"where did\", \"where\u2019s\": \"where is\", \"where\u2019ve\": \"where have\", \"who\u2019ll\": \"who will\",\n    \"who\u2019ll\u2019ve\": \"who will have\", \"who\u2019s\": \"who is\", \"who\u2019ve\": \"who have\",\"why\u2019s\": \"why is\", \"why\u2019ve\": \"why have\", \"will\u2019ve\": \"will have\",\n    \"won\u2019t\": \"will not\", \"won\u2019t\u2019ve\": \"will not have\", \"would\u2019ve\": \"would have\", \"wouldn\u2019t\": \"would not\", \"wouldn\u2019t\u2019ve\": \"would not have\",\n    \"y\u2019all\": \"you all\", \"y\u2019all\": \"you all\", \"y\u2019all\u2019d\": \"you all would\", \"y\u2019all\u2019d\u2019ve\": \"you all would have\", \"y\u2019all\u2019re\": \"you all are\",\n    \"y\u2019all\u2019ve\": \"you all have\", \"you\u2019d\": \"you would\", \"you\u2019d\u2019ve\": \"you would have\", \"you\u2019ll\": \"you will\", \"you\u2019ll\u2019ve\": \"you will have\",\n    \"you\u2019re\": \"you are\", \"you\u2019re\": \"you are\", \"you\u2019ve\": \"you have\"\n}\n\ncontractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))","58ff9fcb":"# Function to clean the html from the Questions\ndef cleanhtml(raw_html):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', raw_html)\n    return cleantext\n\n# Function expand the contractions if there's any\ndef expand_contractions(s, contractions_dict=contractions_dict):\n    def replace(match):\n        return contractions_dict[match.group(0)]\n    return contractions_re.sub(replace, s)\n\n# Function to preprocess the questions\ndef preprocessing(question):\n    global question_sent\n    \n    # Converting to lowercase\n#     question = question.str.lower()\n    \n    # Removing the HTML\n    question = question.apply(lambda x: cleanhtml(x))\n    \n    # Removing the email ids\n    question = question.apply(lambda x: re.sub('\\S+@\\S+','', x))\n    \n    # Removing The URLS\n    question = question.apply(lambda x: re.sub(\"((http\\:\/\/|https\\:\/\/|ftp\\:\/\/)|(www.))+(([a-zA-Z0-9\\.-]+\\.[a-zA-Z]{2,4})|([0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}))(\/[a-zA-Z0-9%:\/-_\\?\\.'~]*)?\",'', x))\n    \n    # Mapping the contractions\n    question = question.apply(lambda x: expand_contractions(x))\n    \n    # Stripping the possessives\n    question = question.apply(lambda x: x.replace(\"'s\", ''))\n    question = question.apply(lambda x: x.replace('\u2019s', ''))\n    question = question.apply(lambda x: x.replace(\"\\'s\", ''))\n    question = question.apply(lambda x: x.replace(\"\\\u2019s\", ''))\n    \n    # Removing the Trailing and leading whitespace and double spaces\n    question = question.apply(lambda x: re.sub(' +', ' ',x))\n    \n    # Removing punctuations from the question\n    question = question.apply(lambda x: ''.join(word for word in x if word not in punctuation))\n    \n    # Removing the Trailing and leading whitespace and double spaces again as removing punctuation might lead to a white space\n    question = question.apply(lambda x: re.sub(' +', ' ',x))\n    \n    # Removing the Stopwords\n    # question = question.apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n    \n    return question","077b5d85":"df['processed_text'] = preprocessing(df['question_text'])\n\n# Lemmatization using WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\ndf['processed_text'] = df['processed_text'].apply(lambda x: \" \".join(lemmatizer.lemmatize(word) for word in x.split()))\n\ndf['processed_text'] = df['processed_text'].astype('str')\ndf = resample(df, random_state = RANDOM_SEED)\n\ndf.head(20)","57e7e46d":"# Bert Parameters\n\nPRE_TRAINED_MODEL_NAME = 'bert-base-cased'\nMAX_LEN = 160\nBATCH_SIZE = 16\nEPOCHS = 1","b6d6ec47":"# Bert Tokenizer\ntokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n\n# Bert Tokenizer with example\nsample_txt = 'Corona Sucks so bad,my final year is kinda ruined, man!!'\n\ntokens = tokenizer.tokenize(sample_txt)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(f' Sentence: {sample_txt}')\nprint(f'   Tokens: {tokens}')\nprint(f'Token IDs: {token_ids}')\n\nencoding = tokenizer.encode_plus(\n  sample_txt,\n  max_length=32,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=False,\n  pad_to_max_length=True,\n  return_attention_mask=True,\n  return_tensors='pt',  # Return PyTorch tensors\n)\nencoding.keys()","d4bb21af":"# Using the Bert tokenizer for encoding the questions\ntoken_lens = []\nfor txt in tqdm(df.processed_text):\n    tokens = tokenizer.encode(txt, max_length=512, truncation = True)\n    token_lens.append(len(tokens))","46d60d84":"# Distribution of tokens\nsns.distplot(token_lens)\nplt.xlim([0, 256]);\nplt.xlabel('Token count');","30b5beff":"# Preparing the dataset with input ids and attention_masks\n\nclass GPquestionDataset(Dataset):\n    \n    def __init__(self, questions, targets, tokenizer, max_len):\n        self.questions = questions\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    def __len__(self):\n        return len(self.questions)\n    def __getitem__(self, item):\n        question = str(self.questions[item])\n        target = self.targets[item]\n        encoding = self.tokenizer.encode_plus(\n          question,\n          add_special_tokens=True,\n          max_length=self.max_len,\n          return_token_type_ids=False,\n          pad_to_max_length=True,\n          return_attention_mask=True,\n          return_tensors='pt',\n          truncation=True  \n        )\n        return {\n          'question_text': question,\n          'input_ids': encoding['input_ids'].flatten(),\n          'attention_mask': encoding['attention_mask'].flatten(),\n          'targets': torch.tensor(target, dtype=torch.long)\n      }","596eef5b":"# Splitting the dataset for training, validation and testing\n\ndf_train, df_test = train_test_split(\n  df,\n  test_size = 0.4,\n  random_state = RANDOM_SEED\n)\ndf_val, df_test = train_test_split(\n  df_test,\n  test_size = 0.6,\n  random_state = RANDOM_SEED\n)\nprint (\"The shape of the training dataset : \", df_train.shape)\nprint (\"The shape of the validation dataset : \", df_val.shape)\nprint (\"The shape of the testing dataset : \", df_test.shape)","8bb5355c":"# Creating the data loader for training, validation and testing using the pytorch DataLoader\n\ndef create_data_loader(df, tokenizer, max_len, batch_size):\n    ds = GPquestionDataset(\n      questions = df.processed_text.to_numpy(),\n      targets = df.target.to_numpy(),\n      tokenizer = tokenizer,\n      max_len = max_len\n    )\n    return DataLoader(\n      ds,\n      batch_size = batch_size,\n      num_workers = 0\n    )\n\ntrain_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\ntest_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n\ndata = next(iter(train_data_loader))\ndata.keys()\n\nprint (len(train_data_loader))\nprint (len(val_data_loader))\nprint (len(test_data_loader))","190b948c":"# Shape of the torch\nprint(data['input_ids'].shape)\nprint(data['attention_mask'].shape)\nprint(data['targets'].shape)","5ab99d83":"# Using the Bert Model 'bert-base-cased'\n\nbert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n\nlast_hidden_state, pooled_output = bert_model(\n  input_ids=encoding['input_ids'],\n  attention_mask=encoding['attention_mask']\n)\n\nprint (last_hidden_state.shape)","70136e78":"# Model with BERT layer and Dropout\n\nclass QuestionClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super(QuestionClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n    def forward(self, input_ids, attention_mask):\n        _, pooled_output = self.bert(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n        )\n        output = self.drop(pooled_output)\n        return self.out(output)\n\nmodel = QuestionClassifier(2)\nmodel = model.to(device)","f5f7a13f":"input_ids = data['input_ids'].to(device)\nattention_mask = data['attention_mask'].to(device)\n\nprint(input_ids.shape) # batch size x seq length\nprint(attention_mask.shape) # batch size x seq length\n\nmodel(input_ids, attention_mask)\n\noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\ntotal_steps = len(train_data_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)\nloss_fn = nn.CrossEntropyLoss().to(device)","aa031a2b":"# Training function\n\ndef train_epoch(\n    model,\n    data_loader,\n    loss_fn,\n    optimizer,\n    device,\n    scheduler,\n    n_examples\n):  \n    # Putting the model in the training mode\n    model = model.train()\n    \n    losses = []\n    correct_predictions = 0\n    \n    for d in data_loader:\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n        outputs = model(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n        )\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        \n    return correct_predictions.double() \/ n_examples, np.mean(losses)","50f61621":"# Evaluation Functions\ndef eval_model(model, data_loader, loss_fn, device, n_examples):\n    \n    # Putting the model in the Evaluation mode\n    model = model.eval()\n    \n    losses = []\n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n            outputs = model(\n              input_ids=input_ids,\n              attention_mask=attention_mask\n            )\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n            \n    return correct_predictions.double() \/ n_examples, np.mean(losses)","fc098207":"%%time\nhistory = defaultdict(list)\nbest_accuracy = 0\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}\/{EPOCHS}')\n    print('-' * 70)\n    train_acc, train_loss = train_epoch(\n      model,\n      train_data_loader,\n      loss_fn,\n      optimizer,\n      device,\n      scheduler,\n      len(df_train)\n    )\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    val_acc, val_loss = eval_model(\n      model,\n      val_data_loader,\n      loss_fn,\n      device,\n      len(df_val)\n    )\n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    if val_acc > best_accuracy:\n        torch.save(model.state_dict(), 'best_model_state.bin')\n        best_accuracy = val_acc","db4209fb":"# The accuracy of the model\ntest_acc, _ = eval_model(\n  model,\n  test_data_loader,\n  loss_fn,\n  device,\n  len(df_test)\n)\n\nprint (test_acc)","fde10446":"# Predictions\n\ndef get_predictions(model, data_loader):\n    model = model.eval()\n    question_texts = []\n    predictions = []\n    prediction_probs = []\n    real_values = []\n    with torch.no_grad():\n        for d in data_loader:\n            texts = d[\"question_text\"]\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n            outputs = model(\n              input_ids=input_ids,\n              attention_mask=attention_mask\n            )\n            _, preds = torch.max(outputs, dim=1)\n            question_texts.extend(texts)\n            predictions.extend(preds)\n            prediction_probs.extend(outputs)\n            real_values.extend(targets)\n    predictions = torch.stack(predictions).cpu()\n    prediction_probs = torch.stack(prediction_probs).cpu()\n    real_values = torch.stack(real_values).cpu()\n    return question_texts, predictions, prediction_probs, real_values\n\ny_question_texts, y_pred, y_pred_probs, y_test = get_predictions(\n  model,\n  test_data_loader\n)","f16bcd4e":"# First 10 predictions and text\ni = 0\nfor t, pred, prob in zip(y_question_texts, y_pred, y_pred_probs):\n    print (t, end = \"   \")\n    print (pred, end = \"   \")\n    print (prob)\n    i+=1\n    if i == 10:\n        break","8cd3e7ee":"# Classification_report\nprint(classification_report(y_test, y_pred))","5eace8a5":"# Confusion matrix\n\nconf = confusion_matrix(y_test, y_pred)\nprint (conf)","05c3caa5":"# f1_score\nprint (f1_score(y_test, y_pred))","0472790f":"# BERT using Pytorch","d23feaa2":"## Preprocessing","56b949fa":"#### Distribution of Insincere and Sincere Questions","32eb25e4":"## BERT Modeling","13633808":"#### Distribution of Polarity and Subjectivity of the questions","be9ac08a":"#### Ngrams (Unigrams and Bigrams)","81a29b60":"## Exploratory Data Analysis"}}