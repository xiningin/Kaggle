{"cell_type":{"9d29f71b":"code","d7b949d1":"code","93fa7192":"code","89e1514d":"markdown"},"source":{"9d29f71b":"!pip install sentence-transformers\n!pip install protobuf\n!pip install deep_translator\n!pip install pandas","d7b949d1":"import time\n\nimport pandas\nimport torch\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom torch import nn, optim, tensor\nfrom torch.nn import functional as F","93fa7192":"class Classifier(nn.Module):\n    def __init__(self, embedding_dim, num_labels, dropout):\n        super(Classifier, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.num_labels = num_labels\n        self.dropout = dropout\n\n        self.dp = nn.Dropout(self.dropout)\n        self.ff = nn.Linear(self.embedding_dim, self.num_labels)\n\n    def forward(self, input_embeddings):\n        tensor = self.dp(input_embeddings)\n        tensor = self.ff(tensor)\n        return tensor, F.softmax(tensor, dim=-1)\n\n\nclass Batcher(object):\n    def __init__(self, data_x, data_y, batch_size):\n        self.data_x = data_x\n        self.data_y = data_y\n        self.batch_size = batch_size\n        self.n_samples = data_x.shape[0]\n        self.indices = torch.randperm(self.n_samples)\n        self.ptr = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.ptr > self.n_samples:\n            self.ptr = 0\n            self.indices = torch.randperm(self.n_samples)\n            raise StopIteration\n        else:\n            batch_indices = self.indices[self.ptr:self.ptr+self.batch_size]\n            self.ptr += self.batch_size\n            return self.data_x[batch_indices], self.data_y[batch_indices]\n\n\ndata_df = pandas.read_csv(\"..\/input\/multilingual-spam-data\/data-en-hi-de-fr.csv\")\n     \ndata_df.dropna(inplace=True)\ndata_df.drop_duplicates(inplace=True)\ndata_df.rename(columns={\n    \"Category\": \"labels\",\n    \"Message\": \"text\"\n}, inplace=True)\n\nle = LabelEncoder()\nle.fit(data_df.labels)\ndata_df[\"labels\"] = le.transform(data_df.labels)\n\ntrain_x, test_x, train_y, test_y = \\\n    train_test_split(data_df.text, data_df.labels, stratify=data_df.labels, test_size=0.15,\n                     random_state=123)\n\ntrain_x_de, test_x_de, train_y_de, test_y_de = \\\n    train_test_split(data_df.text_hi, data_df.labels, stratify=data_df.labels, test_size=0.15,\n                     random_state=123)\n\nsentences = train_x.tolist()\ntest_sentences = test_x_de.tolist()\n\nlabels = torch.tensor(train_y.tolist())\ntest_labels = torch.tensor(test_y_de.tolist())\n\n# encoder = SentenceTransformer('distilbert-base-nli-mean-tokens')\nencoder = SentenceTransformer('quora-distilbert-multilingual')\nprint('Encoding segments...')\nstart = time.time()\nembedding = encoder.encode(sentences, convert_to_tensor=True)\ntest_sentences_embedding = encoder.encode(test_sentences, convert_to_tensor=True)\nprint(f\"Encoding completed in {time.time() - start} seconds.\")\n\ntrain_batcher = Batcher(embedding, labels, batch_size=16)\n\nnum_samples, embeddings_dim = embedding.size()\nn_labels = labels.unique().shape[0]\n\nclassifier = Classifier(embeddings_dim, n_labels, dropout=0.01)\n\noptimizer = optim.Adam(classifier.parameters())\nloss_fn = nn.CrossEntropyLoss()\n\nfor e in range(10):\n    total_loss = 0\n    for batch in train_batcher:\n        x, y = batch\n        optimizer.zero_grad()\n        model_output, prob = classifier(x)\n        loss = loss_fn(model_output, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f'epoch:{e}, total_loss:{total_loss}')\n\nwith torch.no_grad():\n    model_output, prob = classifier(test_sentences_embedding)\n    predictions = torch.argmax(prob, dim=-1)\n    results = classification_report(predictions, test_labels)\n    print(results)\n","89e1514d":"<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/QKMGFCHZH7Y\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>"}}