{"cell_type":{"02422ca7":"code","b88ba5c2":"code","65d77295":"code","0bba6245":"code","de5fb3f7":"code","6422e59d":"code","fa4e0ff4":"code","23693d7a":"code","c9f1e596":"code","05144fe6":"code","7464bc63":"code","f6ebe8ef":"code","610e0827":"code","20204dbf":"code","1dca6180":"code","2fd13f35":"code","394917a2":"code","e004fb3a":"code","3cd7a2dc":"markdown","8b02a912":"markdown","1affa1f7":"markdown"},"source":{"02422ca7":"!pip install ..\/input\/sacremoses\/sacremoses-master\/ > \/dev\/null\n\nimport os\nimport sys\nimport glob\nimport torch\n\nsys.path.insert(0, \"..\/input\/transformers\/transformers-master\/\")\nimport transformers\nimport numpy as np\nimport pandas as pd\nimport math","b88ba5c2":"!ls ..\/input\/nlpaug0011\/nlpaug-master","65d77295":"!pip install ..\/input\/nlpaug0011\/nlpaug-master #> \/dev\/null","0bba6245":"print(transformers.__version__)\nprint(torch.__version__)","de5fb3f7":"import os\nimport re\nimport gc\nimport pickle  \nimport random\nimport tensorflow.keras as keras\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow.keras.backend as K\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import Callback\nfrom scipy.stats import spearmanr, rankdata, entropy\nfrom os.path import join as path_join\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom sklearn.linear_model import MultiTaskElasticNet\n\nfrom tqdm import tqdm_notebook\n\n\nSEED = 42\n\nseed(SEED)\ntf.random.set_seed(SEED )\nrandom.seed(SEED )","6422e59d":"\nimport nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.sentence as nas\nimport nlpaug.flow as naf\n\nfrom nlpaug.util import Action","fa4e0ff4":"text = 'The quick brown fox jumps over the lazy dog .'\n# !ls -a ","23693d7a":"aug_syn = naw.SynonymAug(aug_src='wordnet')\n# augmented_text = aug_syn.augment(text)\nprint(\"Original:\")\nprint(text)\nprint(\"Augmented Synonym Text:\")\nfor ii in range(5):\n    augmented_text = aug_syn.augment(text)\n    print(augmented_text)\n\naug = naw.AntonymAug()\n_text = 'Good boy is very good'\n\nprint(\"Original:\")\nprint(_text)\nprint(\"Augmented Antonym Text:\")\nfor ii in range(5):\n    augmented_text = aug.augment(_text)\n    print(augmented_text)","c9f1e596":"!ls ..\/input\/nlpword2vecembeddingspretrained -l","05144fe6":"# model_type: word2vec, glove or fasttext\naug_w2v = naw.WordEmbsAug(\n#     model_type='word2vec', model_path='..\/input\/nlpword2vecembeddingspretrained\/GoogleNews-vectors-negative300.bin',\n    model_type='glove', model_path='..\/input\/nlpword2vecembeddingspretrained\/glove.6B.300d.txt',\n    action=\"substitute\")\nprint(\"Original:\")\nprint(text)\n","7464bc63":"aug_w2v.aug_p=0.1","f6ebe8ef":"print(\"Augmented Text:\")\nfor ii in range(5):\n    augmented_text = aug_w2v.augment(text)\n    print(augmented_text)","610e0827":"#BERT Augmentator\nTOPK=20 #default=100\nACT = 'insert' #\"substitute\"\n\naug_bert = naw.ContextualWordEmbsAug(\n    model_path='distilbert-base-uncased', \n    #device='cuda',\n    action=ACT, top_k=TOPK)\nprint(\"Original:\")\nprint(text)\nprint(\"Augmented Text:\")\nfor ii in range(5):\n    augmented_text = aug_bert.augment(text)\n    print(augmented_text)","20204dbf":"aug = nas.ContextualWordEmbsForSentenceAug(\n#     model_path='gpt2'\n    model_path='xlnet-base-cased',\n#     model_path='distilgpt2', \n    top_k=TOPK\n)\n\nprint(\"Original:\")\nprint(text)\nprint(\"Augmented Text:\")\nfor ii in range(5):\n    augmented_text = aug.augment(text)\n    print(augmented_text)","1dca6180":"# try Compose & Sometimes\n# make offline augmentation + pseudo label from my best ensemble\n# re-train\n\ntext = \"I have a question about programming language. Which is the best between python and R?\"\ntext = \"What is your recommended book on Bayesian Statistics?\"\n# text = \"How do you make a binary image in Photoshop?\"\n# text = \"Can an affidavit be used in Beit Din?\"\n\naug = naf.Sequential([\n    aug_bert,aug_w2v\n])\n\naug.augment(text, n=10)","2fd13f35":"aug2 = naf.Sometimes([\n    aug_bert,aug_w2v\n],aug_p=0.5, pipeline_p=0.5)\n\naug2.augment(text, n=10) # seems Sometimes has a bug, it still EveryTime, but results look better than sequential\n# However, in the manual aug_p pipeline_p are not clearly defined (have to look at the source)","394917a2":"%%time\n# around 3-4\/5-7 mins for Distil\/BertBase [300-350words to 512subwords] respectively\ntrain = pd.read_csv(\"..\/input\/google-quest-challenge\/train.csv\").fillna(\"none\")\ntest = pd.read_csv(\"..\/input\/google-quest-challenge\/test.csv\").fillna(\"none\")\n\nsample = pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")\ntarget_cols = list(sample.drop(\"qa_id\", axis=1).columns)\ntargets = target_cols\ninput_columns = ['question_title', 'question_body', 'answer']","e004fb3a":"texts = train['question_title'].values\n\nfor ii in tqdm_notebook(range(-7,-1)):\n    print(texts[ii])\n    print(aug.augment(texts[ii],n=1),'\\n')","3cd7a2dc":"# Test NLPAUG","8b02a912":"# Try augmentation for training data","1affa1f7":"# Short Introduction\nHi everybody, in the past few days, I played around with this great text augmentation repo : https:\/\/github.com\/makcedward\/nlpaug .\nI think that it has a potential, so I would love to share how to use it here. Please give the repo a star, and also please consider upvote the corresponding Kaggle dataset : https:\/\/www.kaggle.com\/ratthachat\/nlpaug0011\n\nNote that we can use wordnet-based or glove-based word augmentation offline. But I still cound't find how to use bert-family-based offline yet. So I turn on the internet for this kernel.\nNote also that this is only an early development version, so we shall see more and more very nice features soon!\n\nI am sorry I have not much time to write a good kernel. I will have to go for a family trip soon! Hope you all a happy long new year holliday!!!"}}