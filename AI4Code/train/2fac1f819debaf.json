{"cell_type":{"1716f973":"code","e0482ca5":"code","6ced1f1d":"code","9b27074a":"code","569082c8":"code","280ae39f":"code","5a0cd376":"code","106a7bc3":"code","4a336035":"code","0c8283c7":"code","a7ef0250":"code","f87afe52":"code","14055941":"code","406a1d41":"code","fd45c07a":"code","a178fcba":"code","5d49d4ad":"code","8cd7d1b2":"code","2ff3d8ee":"code","ded21293":"code","6a588459":"code","d77df61d":"markdown","d770a70d":"markdown","9f60a52b":"markdown","8304603f":"markdown","f396b0aa":"markdown","c2da1250":"markdown","63aa8da4":"markdown","0b47e778":"markdown","64ce1d5f":"markdown","fc9a2a60":"markdown","9d66de60":"markdown","d8377084":"markdown","2410e8a8":"markdown","9cbb5480":"markdown"},"source":{"1716f973":"# Data Import on Kaggle\nimport os\nimport time\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Importing processing libraries\nimport numpy as np\nimport pandas as pd\n\n# Importing Visualisation libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Importing libraries for the metrics\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\n\n# Importing libraries for the model\nimport xgboost as xgb \nimport lightgbm as lgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom collections import Counter\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting \nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\n# sklearn imports for analysis\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom scipy.stats import randint","e0482ca5":"data = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest_data = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')","6ced1f1d":"data = data.drop('id', axis=1)","9b27074a":"memory_usage = data.memory_usage(deep=True) \/ 1024 ** 2\nprint('memory usage of features: \\n', memory_usage.head(7))\nprint('memory usage sum: ',memory_usage.sum())","569082c8":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df\n\ndata = reduce_memory_usage(data, verbose=True)\ntest_data = reduce_memory_usage(test_data, verbose=True)","280ae39f":"data.describe()","5a0cd376":"sample_df = data.sample(int(len(data) * 0.2))\nsample_df.shape","106a7bc3":"# Let's confirm if the sampling is retaining the feature distributions\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nsns.histplot(\n    data=data, x=\"f6\", label=\"Original data\", color=\"red\", alpha=0.3, bins=15\n)\nsns.histplot(\n    data=sample_df, x=\"f6\", label=\"Sample data\", color=\"green\", alpha=0.3, bins=15\n)\n\nplt.legend()\nplt.show();","4a336035":"sample_df","0c8283c7":"# Check na values\nprint('Amount of existing NaN values', sample_df.isna().sum())\n\nprint('---------')\n# Target Class Distribution\ntarget_dist = sample_df.target.value_counts()\nprint('Distribution of Target Class \\n',target_dist)\nprint(target_dist[0]\/(target_dist[0] + target_dist[1]))","a7ef0250":"f, ax = plt.subplots(figsize=(8, 6))\ncorr = sample_df.iloc[:,:20].corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)\nplt.show()","f87afe52":"cat_variables = []\n\nfor column in sample_df.columns:\n    if len(sample_df[column].unique()) < 10:\n        cat_variables.append(column)\nprint(cat_variables)","14055941":"fig = plt.figure(figsize = (18, 50))\n\nfor i in range(len(sample_df.columns.tolist()[:100])):\n    plt.subplot(20,5,i+1)\n    sns.set_style(\"white\")\n    plt.title(sample_df.columns.tolist()[:100][i], size = 12, fontname = 'monospace')\n    a = sns.kdeplot(sample_df[sample_df.columns.tolist()[:100][i]], color = '#1a5d57', shade = True, alpha = 0.9, linewidth = 1.5, edgecolor = 'black')\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname = 'monospace')\n    plt.yticks([])\n    for j in ['right', 'left', 'top']:\n        a.spines[j].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n        \nfig.tight_layout(h_pad = 3)\n\nplt.show()","406a1d41":"features = data.columns\nscale = MinMaxScaler()\nsample_df[features]=scale.fit_transform(sample_df[features])\nsample_df[features]= scale.transform(sample_df[features])  \n\nprint('Data scaled using : ', scale)","fd45c07a":"X = sample_df.drop('target', axis=1)\ny = sample_df.target\n\nX_train, X_test, y_train, y_test = train_test_split( X, y, train_size=0.7, random_state=42)\n\ndel sample_df # we do this to remove sample_df from memory","a178fcba":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","5d49d4ad":"id_test_submission = test_data.id\nX_test_submission = test_data.drop('id', axis=1)\n\ndel test_data","8cd7d1b2":"model_dict = {\n    'ADABoost': AdaBoostClassifier(),\n    'Light GBM': lgb.LGBMClassifier(random_state=0, verbose=-1),\n    'Logistic Reg': LogisticRegression(random_state=0, max_iter=350, solver='lbfgs'),\n    'Naive Bayes': GaussianNB(), \n#     'K Nearest Classifier': KNeighborsClassifier(),\n            }\nmodel_list = []\ntrain_acc_list = []\ntest_acc_list = []\ncounter_list = []\n\nfor model, clf in model_dict.items():\n    start_time = time.time()\n\n    clf.fit(X_train, y_train)\n    \n    # test results\n    test_pred = clf.predict(X_test)\n    test_acc = roc_auc_score(y_test, test_pred)\n    \n    # train results\n    train_pred =  clf.predict(np.float32(X_train))\n    train_acc = roc_auc_score(y_train, train_pred)\n\n    print(model, 'Model')\n    print('Classification Report \\n',classification_report(y_test, test_pred))\n    print('Confusion Matrix \\n',confusion_matrix(y_test,test_pred))\n    print('Train Accuracy: ', train_acc)\n    print('Test Accuracy: ', test_acc)\n    print(\"\\n Ran in %s seconds\" % (time.time() - start_time))\n    print('--------------------------------')\n    \n    model_list.append(model)\n    train_acc_list.append(train_acc)\n    test_acc_list.append(test_acc)   \n    \n\nresults = pd.DataFrame({\"model\": model_list, \"train_accuracy\": train_acc_list, \"test_acc\": test_acc_list})","2ff3d8ee":"results","ded21293":"X_final = data.drop('target', axis=1)\ny_final = data.target\n\nmodel = lgb.LGBMClassifier(random_state=0, verbose=-1)\nmodel.fit(X_final, y_final)\ntest_pred = model.predict(X_test_submission)","6a588459":"new_df = pd.DataFrame({'id': id_test_submission, 'target': test_pred})\nnew_df.to_csv('.\/lgbm_submission.csv', index=False)","d77df61d":"## Model Selection\n\nFinally, now that we're done with the preprocessing and EDA, we are going to take a look at how some basic models perform on a subset of the data (20%) without any parameter tuning. We can then retrain and evaluate the top performing models with a bigger dataset and tuned parameters.\n\nYou can take a look at the model_dict and add any models that you think might perform well. Or leave a comment and I'll add them asap!\n\n\n*P.S: Kaggle has a time-out error if the run time of a notebook exceeds a certain time limit. So, I'll comment some of these models out. However, I'll keep the top performing models uncommented.*","d770a70d":"Before we look at distributions, we need to split the data into continuous and categorical variables.","9f60a52b":"# Conclusion\n\nThis notebook was an introduction on how to perform EDA, Feature Scaling and build some models on the default parameters. We also went over how to deal with large datasets and limit the use of RAM using memroy reduction techniques and sampling to reduce the training time. With these skills, you can add to the EDA and integrate unique visualiations that you find might be useful. \n\n### Next Steps\n\nIn the future, I will be adding more models to this notebook, but more importantly, I will be working on Feature Engineering and Hyperparameter Tuning in order to improve the predicitive performance of these models.\n\nP.S: Feature Engineering is the process of extracting useful features and attributes from the data to imrpove our model while Hyperparameter Tuning is the process of picking a model and changing each parameter to see which ones perform the best (either manually or using GridSearchCV).","8304603f":"### Train-Test Split\n\nLet's split our sampled data into train and test sets","f396b0aa":"There doesn't seem to be any nan values in the data. Also, the target class is split evenly between the two groups","c2da1250":"## Submission\n\nKeep in mind this submission will not yield great results since its trained on the basic model with default parameters. We will need to work on the models further and try to improve the submission results.","63aa8da4":"## EDA\n\nLet's start looking at any correlations that might exist among the features.\nWe will also be looking at the densities of every feature.","0b47e778":"## Memory Reduction\n\nIf you don't have any issues with memory, you can go ahead and skip this step. \nHere, we will take a look at the memory consumption by the current data and each feature following which we will try to reduce it to some extent. \n\nThere are several other methods to save RAM - you can refer to this article on [14 tips to save RAM memory](https:\/\/www.kaggle.com\/pavansanagapati\/14-simple-tips-to-save-ram-memory-for-1-gb-dataset). ","64ce1d5f":"So, we have no  categorical features in this dataset. Let's find the  distributions of all the features using kdeplot.","fc9a2a60":"# Tabular Playground Series - Nov 21\n\nFor the Playground Series of November '21, we aim to build a model to identify spam emails via various extracted features from the email. Our data consists of 100 feature variables and our target variable is binary classification. We will first perform some basic EDA to take a better look at this data following which we will start working on our models. \n\n## Plan\n\nMoving forward this is the plan we are going to be following. Keep in mind, this is not a concrete plan and I might change it as we move through the notebook. This will show you my process on how I approach these datasets.\n\n- *Memory Reduction*\n- *Sampling to Reduce Training Time*\n- *Basic EDA*\n- *Model Development*\n- *Hyperparameter Tuning*\n- *Feature Importance from top models*\n- *Selecting the best Model*","9d66de60":"## Sampling Data\n\nNow that we have reduced the memory usage by over 70%, let's sample the data. \n\nWhy are we doing this? Well, you don't have to. But if you're like me and own a Macbook Air that can't handle a dataset bigger than 100mb, this might be a good idea.\n\nWhen we are performing model selection and hyperparameter tuning later, we can't afford to let the notebook run for hours on end testing every model. Doing this, preserves the distributions of each feature while taking only 20% of the entire dataset and we can reduce the training time by using this sampled data.\n\nWe can then perform EDA, modelling, hyperparameter tuning and other steps on this sampled data. Once we decide on the model we want to use and improve its performance, we can train the final model on the entire dataset again.","d8377084":"## Data Preparation\n\nIn this section, we will do some preprocessing. This part involves Feature Scaling and Splitting the data into Train and Test sets.","2410e8a8":"## Imports \n\nLet's import some of the libraries we will be using throughout the notebook","9cbb5480":"### Scaling \n\nWhile most of the models I plan to use in the 'model selection' section will not require any form of feature scaling (like, XGBoost, Random Forest, etc.), some of them (like, KNN and SVM) need it to work. \n\n##### Why\n\nIn general, algorithms that exploit distances or similarities (e.g. in the form of scalar product) between data samples, such as K-NN and Support Vector Machines, are sensitive to feature transformations.\n\nGraphical-model based classifiers, such as Fisher LDA or Naive Bayes, as well as Decision trees and Tree-based ensemble methods (Random Forests, XGBoost) are invariant to feature scaling, but still, it might be a good idea to rescale\/standardize your data."}}