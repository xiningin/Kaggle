{"cell_type":{"b729772f":"code","23fc1770":"code","797a7061":"code","0ffc69ef":"code","af3b0432":"code","2349c533":"code","8e808d51":"code","f713fc47":"code","e4ad8cd0":"code","6a096c60":"code","86ce8e80":"code","48dc9f1c":"code","1b2295c6":"code","cd7da875":"code","329692d5":"code","88193a4a":"code","4bfcbeb2":"code","e9343739":"code","dec46047":"code","3ba34422":"code","003d2b8c":"code","6dc92cef":"code","739e8c0a":"code","a4534052":"code","a17de036":"code","a010eb84":"code","e8a6736c":"code","f22af040":"code","604518df":"code","82dd7636":"code","0e737de8":"code","7262f0b6":"code","b1bbb12f":"code","09bc6c3f":"code","7215cc69":"code","4dc52cfc":"code","35ac91ab":"code","f299bd08":"code","e54b98b7":"code","043ef424":"code","dfce2d57":"code","6d0bc1b2":"code","b0ed5cce":"code","aba105f8":"code","f8c4f51c":"code","746a7309":"code","033a476a":"code","6b8b7f15":"code","858f0ef0":"code","7ca34e2a":"code","3dfe211c":"code","3104f1e7":"code","e109dbeb":"code","e11c8cb4":"code","459d7b73":"code","de30fb3f":"code","1cec12c8":"code","5aa9f83e":"code","31da6a58":"code","5fed232e":"code","eb4bee0f":"code","2e90f446":"code","d213547c":"code","0d89072b":"code","27421122":"code","3d43e5ef":"code","f25ec143":"code","e7ade425":"code","1a4c56e3":"code","d5c1f9f3":"code","ca534d5f":"code","c905887f":"code","14121a95":"code","8fc8580b":"code","d926e392":"markdown","ef57c010":"markdown","6795299f":"markdown","a195eb4c":"markdown","e06786b4":"markdown","8e3585dc":"markdown","926e0eb7":"markdown","94618460":"markdown","071a8495":"markdown"},"source":{"b729772f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\ninfile = \"..\/input\/\"\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew","23fc1770":"train = pd.read_csv(infile+\"train.csv\")\ntest = pd.read_csv(infile+\"test.csv\")\nsubmit = pd.read_csv(infile+\"sample_submission.csv\")","797a7061":"train.head(5)","0ffc69ef":"test.head(5)","af3b0432":"print(\"train \uc14b\uc758 (\ud589,\uc5f4) : {}\".format(train.shape))\nprint(\"test \uc14b\uc758 (\ud589,\uc5f4) : {}\".format(test.shape))\n\ntrain = train.drop(['id'],axis=1)\ntest = test.drop(['id'],axis=1)\n\nprint(\"\uc218\uc815\ub41c train \uc14b\uc758 (\ud589,\uc5f4) : {}\".format(train.shape))\nprint(\"\uc218\uc815\ub41c test \uc14b\uc758 (\ud589,\uc5f4) : {}\".format(test.shape))","2349c533":"train.corr(method='pearson')","8e808d51":"plt.figure(figsize = (15,10))\nsns.heatmap(train.corr(), annot=True, cmap='Blues',fmt='.2f',linewidths=.5)","f713fc47":"#corr \uac12\uc774 \ub192\uc740 10\uac1c\uc758 \ub370\uc774\ud130\ub97c heatmap \ud558\ub294 \uacfc\uc815\n\n#saleprice correlation matrix\ncorrmat = train.corr()\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'price')['price'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f',cmap='Blues', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","e4ad8cd0":"train.info()","6a096c60":"sns.distplot(train['price'],fit=norm)","86ce8e80":"(mu, sigma) = norm.fit(train['price'])\nprint(\"\\n mu = {:.2f} and sigma = {:.2f}\".format(mu,sigma))","48dc9f1c":"fig = plt.figure()\nres = stats.probplot(train['price'], plot= plt)\nplt.show()","1b2295c6":"train['price'] = np.log1p(train['price'])\n\nsns.distplot(train['price'],fit=norm)\n\n(mu, sigma) = norm.fit(train['price'])\nprint(\"\\n mu = {:.2f} and sigma = {:.2f}\".format(mu,sigma))\n\nfig = plt.figure()\nres = stats.probplot(train['price'],plot=plt)\nplt.show()","cd7da875":"ntrain = train.shape[0]\nntest = train.shape[0]\ny_train = train['price']","329692d5":"train = train.drop(['price'],axis=1)","88193a4a":"df = pd.concat([train, test],axis=0)","4bfcbeb2":"df.info()","e9343739":"df.date.head()","dec46047":"year = df.date.apply(lambda x:x[0:4]).astype(int)\nmonth = df.date.apply(lambda x:x[4:6]).astype(int)\nday = df.date.apply(lambda x:x[6:8]).astype(int)","3ba34422":"df['year'] = year\ndf['month'] = month\ndf['day'] = day","003d2b8c":"df = df.drop(['date'],axis=1)","6dc92cef":"df.describe()","739e8c0a":"print(\"zipcode\uc758 \uc778\ub371\uc2a4 \uac1c\uc218 : {}\".format(len(df.zipcode.value_counts().index))) #zipcode \uc778\ub371\uc2a4\uc758 \uac1c\uc218\nprint(\"zipcode\uc758 \uc778\ub371\uc2a4 \uc911 \ucd5c\uc19f\uac12 : {}\".format(df.zipcode.value_counts().min())) #zipcode \uc778\ub371\uc2a4 \uc911 \ucd5c\uc18c\uac12\nprint(\"zipcode\uc758 \uc778\ub371\uc2a4 \uc911 \ucd5c\ub313\uac12 : {}\".format(df.zipcode.value_counts().max())) #zipcode \uc778\ub371\uc2a4 \uc911 \ucd5c\ub300\uac12","a4534052":"from sklearn.preprocessing import LabelEncoder","a17de036":"le = LabelEncoder()\nle.fit(df.zipcode)\n\ndf['zipcode'] = le.transform(df.zipcode)\ndf.zipcode.value_counts()","a010eb84":"plt.figure(figsize = (10,10))\nsns.boxplot(x=df.zipcode, y=df.sqft_living)","e8a6736c":"df.columns","f22af040":"sns.countplot(df.bedrooms)","604518df":"sns.countplot(df.bathrooms)","82dd7636":"df['room_sum'] = df.bedrooms + df.bathrooms","0e737de8":"sns.countplot(df.room_sum)","7262f0b6":"print(\"\uc7ac\uac74\ucd95\ud55c \uac74\ubb3c \uac1c\uc218 :\",df[df.sqft_living == df.sqft_living15].shape[0])\nprint(\"\uc7ac\uac74\ucd95\ud558\uc9c0 \uc54a\uc740 \uac74\ubb3c \uac1c\uc218 :\",df[df.sqft_living != df.sqft_living15].shape[0])","b1bbb12f":"df.room_sum = df.room_sum+1 #0\uac12\uc774 \uc788\uae30 \ub54c\ubb38\uc5d0 \ub098\ub220\uc904\ub54c \uc624\ub958\uac00 \ubc1c\uc0dd\ud558\ub294 \uac83\uc744 \ub9c9\uc544\uc8fc\uae30 \uc704\ud574 1\uc744 \ub354\ud55c\ub2e4.","09bc6c3f":"sns.countplot(df.room_sum)","7215cc69":"df = df.reset_index(drop=True) #concat\uc73c\ub85c \uc778\ud574 \uc778\ub371\uc2a4 \uc624\ub958\uac00 \ubc1c\uc0dd\ud568","4dc52cfc":"row = df.shape[0]\nsqft_per_rooms = []\nfor i in range(row):\n    if df.sqft_living[i] == df.sqft_living15[i]:\n        sqft_per_rooms.append(df.sqft_living[i]\/df.room_sum[i])\n    else:\n        sqft_per_rooms.append(df.sqft_living15[i]\/df.room_sum[i])\ndf['sqft_per_rooms'] = sqft_per_rooms","35ac91ab":"sns.distplot(df.sqft_per_rooms,fit=norm)","f299bd08":"print(\"\uc7ac\uac74\ucd95 \ud558\uc9c0 \uc54a\uc740 \uc9d1\uc758 \uac2f\uc218 :\",df[df.yr_renovated==0].shape[0])\nprint(\"\uc7ac\uac74\ucd95 \ub41c \uc9d1\uc758 \uac2f\uc218 :\",df[df.yr_renovated>0].shape[0])","e54b98b7":"during_yr = []\nfor i in range(row):\n    if df.yr_renovated[i]==0:\n        during_yr.append(df.year[i]-df.yr_built[i])\n    else:\n        during_yr.append(df.year[i]-df.yr_renovated[i])\n\ndf['during_yr'] = during_yr","043ef424":"sns.distplot(df.during_yr,fit=norm)","dfce2d57":"df.floors.value_counts()","6d0bc1b2":"sqft_per_floor = []\nfor i in range(row):\n    if df.sqft_living[i]==df.sqft_living15[i]:\n        sqft_per_floor.append(df.sqft_living[i]\/df.floors[i])\n    else:\n        sqft_per_floor.append(df.sqft_living15[i]\/df.floors[i])\n\ndf['sqft_per_floor'] = sqft_per_floor","b0ed5cce":"sns.distplot(df.sqft_per_floor,fit=norm)","aba105f8":"df['sqft_total'] = df['sqft_above'] + df['sqft_basement']","f8c4f51c":"df.columns","746a7309":"use_col = ['sqft_living', 'sqft_lot','sqft_above','sqft_basement','sqft_living15','sqft_lot15','sqft_total','sqft_per_rooms','sqft_per_floor']","033a476a":"skewed_feats = df[use_col].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({\"Skew\":skewed_feats})\nskewness\n\n#skewness\uc5d0 \ub300\ud55c \uac04\ub2e8\ud55c \uc124\uba85\n#\ub370\uc774\ud130\uc758 \ubd84\ud3ec\uac00 \ud55c\ucabd\uc73c\ub85c \uce58\uc6b0\uce5c \uc815\ub3c4\ub97c \uc758\ubbf8\ud55c\ub2e4.\n#\uc67c\ucabd\uc73c\ub85c \uce58\uc6b0\uccd0\uc838 \uc788\uc744 \ub54c\ub294 skewness\uac00 \uc74c\uc218, \uc624\ub978\ucabd\uc73c\ub85c \uce58\uc6b0\uccd0\uc838 \uc788\uc744 \ub54c\ub294 skewness \uc591\uc218.","6b8b7f15":"skewness = skewness[abs(skewness) > 0.75]\n#\uc808\ub313\uac12\uc774 0.75\ubcf4\ub2e4 \ub192\uc740 skewness\ub97c \uac00\uc9c0\ub294 \ud589\uc744 Box Cox transform \ud574\uc900\ub2e4\nprint(\"\ud589 \uac1c\uc218 :\",skewness.shape[0])","858f0ef0":"#Box Cox transform\uc744 \ud574\uc90c\uc73c\ub85c\uc368 \ud55c\ucabd\uc73c\ub85c \uae38\uc5b4\uc9c4 \uaf2c\ub9ac\uc758 \ubaa8\uc591\uc744 \uc7a1\uc544\uc904 \uc218 \uc788\ub2e4.\n#\ud3b8\ud5a5\uc744 \uc7a1\uc544\uc8fc\ub294\ub370 \ub3c4\uc6c0\uc774\ub41c\ub2e4.\n#np.log1p\ub97c \uc0ac\uc6a9\ud574\ub3c4 \ube44\uc2b7\ud55c \ud6a8\uacfc\ub97c \uc5bb\uc744 \uc218 \uc788\ub2e4.\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    df[feat] = boxcox1p(df[feat],lam)","7ca34e2a":"fig, ax = plt.subplots(3,4,figsize=(20,20))\nn=0\nfor r in range(3):\n    for c in range(4):\n        sns.distplot(df[use_col[n]],fit=norm,ax=ax[r][c])\n        ax[r][c].set_title(use_col[n],fontsize=20)\n        n+=1\n        if n==len(use_col):\n            break","3dfe211c":"train = df.iloc[:ntrain,:]\ntest = df.iloc[ntrain:,:]","3104f1e7":"from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","e109dbeb":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","e11c8cb4":"lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9,random_state=3))\nKRR = KernelRidge(alpha=0.6,kernel='polynomial', degree=2, coef0=2.5)\n","459d7b73":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","de30fb3f":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\n","1cec12c8":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","5aa9f83e":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","31da6a58":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","5fed232e":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","eb4bee0f":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","2e90f446":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","d213547c":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","0d89072b":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","27421122":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","3d43e5ef":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","f25ec143":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\n#score = rmsle_cv(stacked_averaged_models)\n#print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","e7ade425":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","1a4c56e3":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","d5c1f9f3":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","ca534d5f":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","c905887f":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","14121a95":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","8fc8580b":"submit['price'] = ensemble\nsubmit.to_csv(\"stacked_suwon_study.csv\",index=False)","d926e392":"#### price\uc640 \uac00\uc7a5 \uc0c1\uad00\uc774 \ub192\uc740 sqft_living\uc744 zipcode \uae30\uc900\uc73c\ub85c boxplot\uc744 \uadf8\ub824\ubcf8\ub2e4.","ef57c010":"#### \ucd1d \ubc29\uac1c\uc218 \ud569\uce58\uae30","6795299f":"#### zipcode\ub97c \ud30c\uc545\ud558\uc5ec label encoding\uc744 \uc9c4\ud589\ud558\ub3c4\ub85d \ud55c\ub2e4.","a195eb4c":"### Averaged base models class","e06786b4":"### \uc0dd\uac01\ub098\ub294\ub300\ub85c \uc9c4\ud589\ud558\uae30 \ub54c\ubb38\uc5d0 \uc21c\uc11c\uc5d0 \ubb38\uc81c\ub294 \uc788\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","8e3585dc":"#### model \ub9cc\ub4e4\uae30","926e0eb7":"#### price \uac12\uc744 \uc815\uaddc\ud654\ud55c\ub2e4.","94618460":"#### train \ub370\uc774\ud130\ub85c price\uc640\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \ubbf8\ub9ac \ud30c\uc545\ud574\ubcf4\uc790.","071a8495":"#### date\uc758 \ub370\uc774\ud130\ub97c \uc774\uc6a9\ud558\uc5ec \uc0c8\ub85c\uc6b4 year,month,day\uc758 \uc5f4\uc744 \ucd94\uac00\ud55c\ub2e4."}}