{"cell_type":{"5cc81a2d":"code","78a18967":"code","fc286163":"code","3c89e245":"code","27389f30":"code","631c3022":"code","e3be350f":"code","90424351":"code","c53205fd":"code","d0839d74":"code","6337e86f":"code","9ee96b40":"code","5076598a":"code","c40c1ae0":"code","bf2dd795":"code","fa0de9e3":"code","ab77c3cb":"code","e3512f9f":"code","d0324e18":"code","e892a724":"code","548c6da8":"code","772e057c":"code","4da1cf14":"code","cb772047":"code","cbbb73b0":"code","cb322a48":"code","c44802c3":"code","2cf3b624":"markdown","0d5cae92":"markdown","d27019e0":"markdown","f8a00839":"markdown","23992941":"markdown","1c9df6ba":"markdown","865a75e0":"markdown","180da80b":"markdown","2149bcfe":"markdown","98b12206":"markdown","88165ae3":"markdown"},"source":{"5cc81a2d":"%pwd","78a18967":"%ls","fc286163":"# uncomment if you want to create directory checkpoint, best_model\n%mkdir checkpoint best_model","3c89e245":"# uncomment if you want to remove folder best_model and checkpoint\n# %rm -r best_model\/ checkpoint\/","27389f30":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport numpy as np\n","631c3022":"# check if CUDA is available\nuse_cuda = torch.cuda.is_available()","e3be350f":"# Define your network ( Simple Example )\nclass FashionClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        input_size = 784\n        self.fc1 = nn.Linear(input_size, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 128)\n        self.fc4 = nn.Linear(128, 64)\n        self.fc5 = nn.Linear(64,10)\n        self.dropout = nn.Dropout(p=0.2)\n        \n    def forward(self, x):\n        x = x.view(x.shape[0], -1)\n        x = self.dropout(F.relu(self.fc1(x)))\n        x = self.dropout(F.relu(self.fc2(x)))\n        x = self.dropout(F.relu(self.fc3(x)))\n        x = self.dropout(F.relu(self.fc4(x)))\n        x = F.log_softmax(self.fc5(x), dim=1)\n        return x","90424351":"# Define a transform to normalize the data\ntransform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n# Download and load the training data\ntrainset = datasets.FashionMNIST('F_MNIST_data\/', download=True, train=True, transform=transform)\n\n# Download and load the test data\ntestset = datasets.FashionMNIST('F_MNIST_data\/', download=True, train=False, transform=transform)\n\nloaders = {\n    'train' : torch.utils.data.DataLoader(trainset,batch_size = 64,shuffle=True),\n    'test'  : torch.utils.data.DataLoader(testset,batch_size = 64,shuffle=True),\n}","c53205fd":"# Create the network, define the criterion and optimizer\nmodel = FashionClassifier()\n# move model to GPU if CUDA is available\nif use_cuda:\n    model = model.cuda()\n\nprint(model)","d0839d74":"#define loss function and optimizer\ncriterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","6337e86f":"import torch\nimport shutil\ndef save_ckp(state, is_best, checkpoint_path, best_model_path):\n    f_path = checkpoint_path\n    torch.save(state, f_path)\n    if is_best:\n        best_fpath = best_model_path\n        shutil.copyfile(f_path, best_fpath)","9ee96b40":"def train(start_epochs, n_epochs, valid_loss_min_input, loaders, model, optimizer, criterion, use_cuda, checkpoint_path, best_model_path):\n    \"\"\"\n    Keyword arguments:\n    start_epochs -- the real part (default 0.0)\n    n_epochs -- the imaginary part (default 0.0)\n    valid_loss_min_input\n    loaders\n    model\n    optimizer\n    criterion\n    use_cuda\n    checkpoint_path\n    best_model_path\n    \n    returns trained model\n    \"\"\"\n    # initialize tracker for minimum validation loss\n    valid_loss_min = valid_loss_min_input \n    \n    for epoch in range(start_epochs, n_epochs+1):\n        # initialize variables to monitor training and validation loss\n        train_loss = 0.0\n        valid_loss = 0.0\n        \n        ###################\n        # train the model #\n        ###################\n        model.train()\n        for batch_idx, (data, target) in enumerate(loaders['train']):\n            # move to GPU\n            if use_cuda:\n                data, target = data.cuda(), target.cuda()\n            ## find the loss and update the model parameters accordingly\n            # clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data)\n            # calculate the batch loss\n            loss = criterion(output, target)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            ## record the average training loss, using something like\n            ## train_loss = train_loss + ((1 \/ (batch_idx + 1)) * (loss.data - train_loss))\n            train_loss = train_loss + ((1 \/ (batch_idx + 1)) * (loss.data - train_loss))\n        \n        ######################    \n        # validate the model #\n        ######################\n        model.eval()\n        for batch_idx, (data, target) in enumerate(loaders['test']):\n            # move to GPU\n            if use_cuda:\n                data, target = data.cuda(), target.cuda()\n            ## update the average validation loss\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data)\n            # calculate the batch loss\n            loss = criterion(output, target)\n            # update average validation loss \n            valid_loss = valid_loss + ((1 \/ (batch_idx + 1)) * (loss.data - valid_loss))\n            \n        # calculate average losses\n        train_loss = train_loss\/len(loaders['train'].dataset)\n        valid_loss = valid_loss\/len(loaders['test'].dataset)\n\n        # print training\/validation statistics \n        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n            epoch, \n            train_loss,\n            valid_loss\n            ))\n        \n        checkpoint = {\n            'epoch': epoch + 1,\n            'valid_loss_min': valid_loss,\n            'state_dict': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n        }\n        \n        save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n        ## TODO: save the model if validation loss has decreased\n        if valid_loss <= valid_loss_min:\n            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n            save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n            valid_loss_min = valid_loss\n            \n    # return trained model\n    return model","5076598a":"trained_model = train(1, 3, np.Inf, loaders, model, optimizer, criterion, use_cuda, \".\/checkpoint\/current_checkpoint.pt\", \".\/best_model\/best_model.pt\")","c40c1ae0":"%ls .\/best_model\/","bf2dd795":"%ls .\/checkpoint\/","fa0de9e3":"#torch.save(checkpoint, 'checkpoint.pth')","ab77c3cb":"\"\"\"\ncheckpoint = {\n            'epoch': epoch + 1,\n            'valid_loss_min': valid_loss,\n            'state_dict': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n        }\n\"\"\"","e3512f9f":"# in train method, above we use this to save checkpoint file\n# save_ckp(checkpoint, False, checkpoint_path, best_model_path)","d0324e18":"# in train method, above we use this to save best_model file\n# save_ckp(checkpoint, False, checkpoint_path, best_model_path)","e892a724":"def load_ckp(checkpoint_fpath, model, optimizer):\n    checkpoint = torch.load(checkpoint_fpath)\n    model.load_state_dict(checkpoint['state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    valid_loss_min = checkpoint['valid_loss_min']\n    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()","548c6da8":"%pwd\n%ls","772e057c":"model = FashionClassifier()\n# move model to GPU if CUDA is available\nif use_cuda:\n    model = model.cuda()\n\nprint(model)","4da1cf14":"optimizer = optim.Adam(model.parameters(), lr=0.001)\nckp_path = \".\/checkpoint\/current_checkpoint.pt\"\nmodel, optimizer, start_epoch, valid_loss_min = load_ckp(ckp_path, model, optimizer)","cb772047":"print(\"model = \", model)\nprint(\"optimizer = \", optimizer)\nprint(\"start_epoch = \", start_epoch)\nprint(\"valid_loss_min = \", valid_loss_min)\nprint(\"valid_loss_min = {:.6f}\".format(valid_loss_min))","cbbb73b0":"trained_model = train(start_epoch, 6, valid_loss_min, loaders, model, optimizer, criterion, use_cuda, \".\/checkpoint\/current_checkpoint.pt\", \".\/best_model\/best_model.pt\")","cb322a48":"trained_model.eval()","c44802c3":"test_acc = 0.0\nfor samples, labels in loaders['test']:\n    with torch.no_grad():\n        samples, labels = samples.cuda(), labels.cuda()\n        output = trained_model(samples)\n        # calculate accuracy\n        pred = torch.argmax(output, dim=1)\n        correct = pred.eq(labels)\n        test_acc += torch.mean(correct.float())\n\nprint('Accuracy of the network on {} test images: {}%'.format(len(testset), round(test_acc.item()*100.0\/len(loaders['test']), 2)))","2cf3b624":"# Prepare the dataset ","0d5cae92":"After we load all the information needed, we can continue training, start_epoch = 4; previously we train the model from 1 to 3","d27019e0":"# Saving and Loading Models in PyTorch\n\nIn this notebook, I'll show you how to save, load and continue to train models with PyTorch.\n\n[1] and [2] provide a good start for me to expands on this topic.\n\nImagine a case when you have been training your model for hours and suddenly the machine crashes or you lose connection to the remote GPU that you have been training your model on. Disaster right? Consider another case that you trained your model for certain epochs which already took a considerable amount of time, but you are not satisfied with the performance and you wish you had trained it for more epochs[2].\n\n* If you follow along please, make sure checkpoint and best_model directories are created in your work directory:\n    * I am going to save latest checkpoint in checkpoint directory\n    * I am going to save best model\/checkpoint in best_model directory\n\nReference: \n- [[1]](https:\/\/www.kaggle.com\/davidashraf\/saving-and-loading-models-in-pytorch) https:\/\/www.kaggle.com\/davidashraf\/saving-and-loading-models-in-pytorch\n- [[2]](https:\/\/medium.com\/analytics-vidhya\/saving-and-loading-your-model-to-resume-training-in-pytorch-cb687352fa61) https:\/\/medium.com\/analytics-vidhya\/saving-and-loading-your-model-to-resume-training-in-pytorch-cb687352fa61\n- [[3]](https:\/\/pytorch.org\/tutorials\/beginner\/saving_loading_models.html) https:\/\/pytorch.org\/tutorials\/beginner\/saving_loading_models.html","f8a00839":"---\n## Saving the model\nThe parameters for PyTorch networks are stored in a model's `state_dict`. It includes the parameters matrices for each of the layers. If we just want to save the model, we can just save the `state_dict`","23992941":"In our case, we want to save a checkpoint that allow us to use these information continue our model traning. Here are the information needed:\n* epoch: a measure of the number of times all of the training vectors are used once to update the weights.\n* valid_loss_min: the minium validation loss, this is needed so that when we continue the training, we can start with this rather than np.Inf value\n* state_dict: model archiecture information, it includes the parameters matrices for each of the layers.\n* optimizer: You need to save optimizer parameters especially when you are using Adam as your optimizer. Adam is an adaptive learning rate method, which means, it computes individual learning rates for different parameters which we would need if we would like to continue your training from where we left off [2].\n","1c9df6ba":"* Noticed, epoch now start from 4 to 6. \n* The validation loss value from epoch 4 is the validation loss minimum we have in the previous training that we get from the saved checkpoint. ","865a75e0":"---\n# Loading the model","180da80b":"Loading is as simple as Saving.\n* Reconstruct the model\n* Load the state dict to the model\n* Freeze the parameters and enter evaluation mode if you're loading for inference\n","2149bcfe":"# Train the network","98b12206":"# Define your model","88165ae3":"# Inference\nRemember that you must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results [3]."}}