{"cell_type":{"b689adc4":"code","d87d1576":"code","c46f61c3":"code","0fa25037":"code","55f91714":"code","c7dedbd7":"code","65b3cd90":"code","379e4120":"code","5d0e2d33":"code","658e37df":"code","8fe0abe2":"code","ebf71317":"code","c2ab2fca":"code","9c0a6022":"code","3ef04982":"code","e44722fe":"code","9b81d805":"code","8ebd58ff":"code","20b85d9b":"code","e02e4be0":"markdown","e24681fc":"markdown","5aeab40d":"markdown","070357dc":"markdown","6c45c5a0":"markdown","6335854b":"markdown","3932fcc9":"markdown","d7b7c29a":"markdown","ec257736":"markdown","c54d65c6":"markdown","9411b811":"markdown"},"source":{"b689adc4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d87d1576":"import numpy\nimport pandas\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom keras.callbacks import History","c46f61c3":"directory = r\"\/kaggle\/input\/bank-marketing-campaign\/bank-full - Copy.csv\"\ndataset = pandas.read_csv(directory, sep=\";\")\nprint(dataset.info())","0fa25037":"print(dataset.groupby('y').size())\ntotal_customer = len(dataset)\n\nlabels = ['Target yes', 'Target No']\npic_sizes = [round((len(dataset[dataset['y'] == 'yes'])\/total_customer)*100, 2), 100-round((len(dataset[dataset['y'] == 'yes'])\/total_customer)*100, 2)]\npic_colors = ['#99ff99','#ff9999']\nexplode = (0.05,0.05)\n\nplt.title('Target Yes vs No')\nplt.pie(pic_sizes, colors = pic_colors, labels=labels, autopct='%1.1f%%', startangle=90, pctdistance=0.85, explode = explode)#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle) \nplt.tight_layout()\nplt.show()\n\ndataset.groupby('y').size().plot(kind='bar')","55f91714":"labels = ['divorced', 'married', 'single']\nsizes = [len(dataset[dataset['marital'] == 'divorced'])\/total_customer,\n         len(dataset[dataset['marital'] == 'married'])\/total_customer,\n         len(dataset[dataset['marital'] == 'single'])\/total_customer\n        ]\n\nfig1, ax1 = plt.subplots(1,1)\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True)\nax1.axis('equal')\nplt.show()\n\ndataset.groupby('marital').size().plot(kind='bar')","c7dedbd7":"print(dataset.groupby('marital').size())\ndataset.groupby(['marital', 'y']).size().plot(kind='bar')\n\n\nlabels = ['single', 'married', 'divorced']\nmarital_yes = [len(dataset[(dataset['marital'] == 'single') & (dataset['y'] == 'yes')]), \n               len(dataset[(dataset['marital'] == 'married') & (dataset['y'] == 'yes')]),\n               len(dataset[(dataset['marital'] == 'divorced') & (dataset['y'] == 'yes')])]\nmarital_no = [len(dataset[(dataset['marital'] == 'single') & (dataset['y'] == 'no')]), \n              len(dataset[(dataset['marital'] == 'married') & (dataset['y'] == 'no')]), \n              len(dataset[(dataset['marital'] == 'divorced') & (dataset['y'] == 'no')])]\nwidth = 0.35\n\nfig, ax = plt.subplots()\nax.bar(labels, marital_no, width, label='No', color='C5')\nax.bar(labels, marital_yes, width, bottom=men_means, label='Yes')\n\nax.set_ylabel('customer base')\nax.set_title('Marketing success')\nax.legend()\n\nplt.show()","65b3cd90":"print(dataset.groupby('education').size())\nplt.figure(figsize=(10, 8))\ndataset.groupby(['education', 'y']).size().plot(kind='bar')\n\n\nlabels = ['Yes', 'No']\nprimary_sizes = [round((len(dataset[(dataset['education'] == 'primary') & (dataset['y'] == 'yes')])\/total_customer)*100, 2), \n                    100-round((len(dataset[(dataset['education'] == 'primary') & (dataset['y'] == 'yes')])\/total_customer)*100, 2)]\nsecondary_sizes = [round((len(dataset[(dataset['education'] == 'secondary') & (dataset['y'] == 'yes')])\/total_customer)*100, 2), \n                    100-round((len(dataset[(dataset['education'] == 'secondary') & (dataset['y'] == 'yes')])\/total_customer)*100, 2)]\ntertiary_sizes = [round((len(dataset[(dataset['education'] == 'tertiary') & (dataset['y'] == 'yes')])\/total_customer)*100, 2), \n                    100-round((len(dataset[(dataset['education'] == 'tertiary') & (dataset['y'] == 'yes')])\/total_customer)*100, 2)]\nunknown_sizes = [round((len(dataset[(dataset['education'] == 'unknown') & (dataset['y'] == 'yes')])\/total_customer)*100, 2), \n                    100-round((len(dataset[(dataset['education'] == 'unknown') & (dataset['y'] == 'yes')])\/total_customer)*100, 2)]\nprimary_colors = ['lime', 'violet']\nsecondary_colors = ['darkred','aqua']\ntertiary_colors = ['#ff9999','#99ff99']\nunknown_colors = ['tan', 'orange']\nexplode = (0.05,0.05)\n\nfig = plt.figure(figsize=(8, 8))\nplt.subplot(2, 2, 1)\nplt.title('Primary education')\nplt.pie(primary_sizes, colors = primary_colors, labels=labels, autopct='%1.1f%%', startangle=90, pctdistance=0.85, explode = explode)#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle) \nplt.tight_layout()\nplt.subplot(2, 2, 2)\nplt.title('Secondary education')\nplt.pie(secondary_sizes, colors = secondary_colors, labels=labels, autopct='%1.1f%%', startangle=90, pctdistance=0.85, explode = explode)#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle) \nplt.tight_layout()\nplt.subplot(2, 2, 3)\nplt.title('Tertiary education')\nplt.pie(tertiary_sizes, colors = tertiary_colors, labels=labels, autopct='%1.1f%%', startangle=90, pctdistance=0.85, explode = explode)#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle) \nplt.tight_layout()\nplt.subplot(2, 2, 4)\nplt.title('education data unknown')\nplt.pie(unknown_sizes, colors = unknown_colors, labels=labels, autopct='%1.1f%%', startangle=90, pctdistance=0.85, explode = explode)#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle) \nplt.tight_layout()\nplt.show()","379e4120":"print(dataset.groupby('housing').size())\ndataset.groupby(['housing', 'y']).size().plot(kind='bar')","5d0e2d33":"print(dataset.groupby('job').size())\nplt.figure(figsize=(20, 12))\ndataset.groupby(['job', 'y']).size().plot(kind='bar')","658e37df":"y = dataset.pop(\"y\")\ny = y.map({\"yes\": 1, \"no\": 0})","8fe0abe2":"dataset_info = {}\nfor name, column in dataset.items():\n    if column.dtype == object:\n        datatype = tf.string\n    else:\n        datatype = tf.float32\n    dataset_info[name] = tf.keras.Input(shape=(1,), dtype=datatype, name=name)\n\nnumeric_inputs = {name: value for name, value in dataset_info.items() if value.dtype == tf.float32}\nprint(numeric_inputs.values())","ebf71317":"normalise = preprocessing.Normalization()\nnormalise.adapt(numpy.array(dataset[numeric_inputs.keys()]))\nconcat_input = tf.keras.layers.Concatenate()(numeric_inputs.values())\nnumeric_input_preprocessed = normalise(concat_input)\n\npreprocessed_data = [numeric_input_preprocessed]\nprint(preprocessed_data)","c2ab2fca":"for name, value in dataset_info.items():\n    if value.dtype == tf.float32:\n        continue\n\n    lookup = preprocessing.StringLookup(vocabulary=dataset[name].unique())\n    encoding = preprocessing.CategoryEncoding(max_tokens=lookup.vocab_size())\n\n    lookup_val = lookup(value)\n    encoded = encoding(lookup_val)\n\n    preprocessed_data.append(encoded)\n\nprint(preprocessed_data)\npreprocessed_data_concat = tf.keras.layers.Concatenate()(preprocessed_data)","9c0a6022":"learning_rate = 1e-4\nbody = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.keras.activations.relu, kernel_regularizer='l1'),\n    tf.keras.layers.Dense(128, activation=tf.keras.activations.tanh), \n    tf.keras.layers.Dense(128, activation=tf.keras.activations.relu, kernel_regularizer='l1'),\n    tf.keras.layers.Dense(1)\n])\n\npreprocessed_input = tf.keras.Model(dataset_info, preprocessed_data_concat)\nresult = body(preprocessed_input(dataset_info))\nbank_model = tf.keras.Model(dataset_info, result)\nbank_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=\"accuracy\")","3ef04982":"print(bank_model.summary())","e44722fe":"tf.keras.utils.plot_model(model=preprocessed_input, rankdir=\"LR\", dpi=72, show_shapes=True)","9b81d805":"bank_features_dict = {name: numpy.array(value) for name, value in dataset.items()}\none_row_dict = {name: value[:1] for name, value in bank_features_dict.items()}\nprint(one_row_dict)\n\nprint(preprocessed_input(one_row_dict))","8ebd58ff":"learning_rate = 1e-4\nbody = tf.keras.Sequential([\n    tf.keras.layers.Dense(64, activation=tf.keras.activations.tanh),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(64, activation=tf.keras.activations.tanh),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(64, activation=tf.keras.activations.relu),\n    tf.keras.layers.Dense(1)\n])\n\npreprocessed_input = tf.keras.Model(dataset_info, preprocessed_data_concat)\nresult = body(preprocessed_input(dataset_info))\nbank_model = tf.keras.Model(dataset_info, result)\nbank_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=\"accuracy\")\nbank_model.fit(x=bank_features_dict, y=y, epochs=5, batch_size=5, validation_split=0.4, callbacks=[history], use_multiprocessing=True)","20b85d9b":"plt.style.use('ggplot')\nplt.plot(history.history['loss'], label = 'loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt.title(\"loss vs val_Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('accuracy vs val_accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","e02e4be0":"**The below graph shows nearly half the data contains customer with \"Married\" status**","e24681fc":"# Prediction model using TensorFlow v2\n**Didn't add fine tuning to improve the accuracy of the model, below code setup is just the demonstration on how to use TensorFlow for data preprocssing and neural network creation\nYou can fine tune neural netork by optimizing hyper params, hidden layers or try models like Decision tree, KNN or gradient boosing models or ensemble model like Random Forest if your main focus is on accuracy**","5aeab40d":"**Below grah shows how the data is preprocessed grpahically**","070357dc":"**Normalise the numeric inputs to avoid initial incorrect feature weights**","6c45c5a0":"**Below graph implies that we got better postive response from customers who don't own house**","6335854b":"# Data Analysis:\n**From the chart below, the success to failure response ratio is 1:8**","3932fcc9":"**Below graphs shows the customer with \"single\" relationship type have best postive response rate**","d7b7c29a":"**Metrics representation for both train and test set**","ec257736":"****","c54d65c6":"# Import dataset","9411b811":"**Encode the categorical fields, object type featues to numeric type as Neural network doesnt process String based input data**"}}