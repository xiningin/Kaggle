{"cell_type":{"b5dd03f0":"code","eaf926c0":"code","3d795d88":"code","a5198670":"code","0a459efe":"code","60830fd9":"code","b09b5dcb":"code","f0ae6437":"code","80e65b40":"code","2f1e22fa":"code","daac85bd":"code","5b559d1a":"code","7c05073f":"markdown","98a33ef5":"markdown","33070fa1":"markdown","ba9e5f7d":"markdown","c5f3ba4b":"markdown","ac41b594":"markdown","a6499e87":"markdown","b31227ff":"markdown","b7f2e7c5":"markdown","361a3fe0":"markdown","f61f193b":"markdown"},"source":{"b5dd03f0":"!pip install -q git+git:\/\/github.com\/oke-aditya\/pytorch_cnn_trainer.git","eaf926c0":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport torchvision.transforms as T\nfrom pytorch_cnn_trainer import dataset\nfrom pytorch_cnn_trainer import model_factory\nfrom pytorch_cnn_trainer import utils\nfrom pytorch_cnn_trainer import engine","3d795d88":"MODEL_NAME = \"resnet18\"\nNUM_ClASSES = 10\nIN_CHANNELS = 3\nPRETRAINED = True  # If True -> Fine Tuning else Scratch Training\nEPOCHS = 5\nEARLY_STOPPING = True  # If you need early stoppoing for validation loss\nSAVE_PATH = \"{}.pt\".format(MODEL_NAME)\nSEED = 42","a5198670":"# Train and validation Transforms which you would like\ntrain_transforms = T.Compose([T.ToTensor(), T.Normalize((0.5,), (0.5,))])\nvalid_transforms = T.Compose([T.ToTensor(), T.Normalize((0.5,), (0.5,))])","0a459efe":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","60830fd9":"# Sets seed for your entire run\nutils.seed_everything(SEED)\nprint(\"Setting Seed for the run, seed = {}\".format(SEED))","b09b5dcb":"# For example I provide ready to use CIFAR10, you can create your own dataset too.\nprint(\"Creating Train and Validation Dataset\")\ntrain_set, valid_set = dataset.create_cifar10_dataset(train_transforms, valid_transforms)\nprint(\"Train and Validation Datasets Created\")","f0ae6437":"print(\"Creating DataLoaders\")\ntrain_loader, valid_loader = dataset.create_loaders(train_set, train_set)\nprint(\"Train and Validation Dataloaders Created\")","80e65b40":"print(\"Creating Model\")\nmodel = model_factory.create_torchvision_model(MODEL_NAME, num_classes=NUM_ClASSES, pretrained=True)\nif torch.cuda.is_available():\n    print(\"Model Created. Moving it to CUDA\")\nelse:\n    print(\"Model Created. Training on CPU only\")\nmodel.to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)","2f1e22fa":"criterion = nn.CrossEntropyLoss()  # All classification problems usually need Cross entropy loss\nearly_stopper = utils.EarlyStopping(patience=7, verbose=True, path=SAVE_PATH)","daac85bd":"history = engine.fit(\n    epochs=EPOCHS,\n    model=model,\n    train_loader=train_loader,\n    valid_loader=valid_loader,\n    criterion=criterion,\n    device=device,\n    optimizer=optimizer,\n    early_stopper=early_stopper,\n)\nprint(\"Done !!\")","5b559d1a":"for epoch in tqdm(range(EPOCHS)):\n    print()\n    print(\"Training Epoch = {}\".format(epoch))\n    train_metrics = engine.train_step(model, train_loader, criterion, device, optimizer)\n    print()\n    print(\"Validating Epoch = {}\".format(epoch))\n    valid_metrics = engine.val_step(model, valid_loader, criterion, device)\n    validation_loss = valid_metrics[\"loss\"]\n    early_stopper(validation_loss, model=model)\n\n    if early_stopper.early_stop:\n        print(\"Saving Model and Early Stopping\")\n        print(\"Early Stopping. Ran out of Patience for validation loss\")\n        break\n\n    print(\"Done Training, Model Saved to Disk\")","7c05073f":"- If you like this kernel or package please do upvote and share with others.\n- Also checkout on Github as well and let me know what you think in comments !! \n\nPackage Link  \nhttps:\/\/github.com\/oke-aditya\/pytorch_cnn_trainer","98a33ef5":"## Easily Fine Tune Torchvision and Timm models","33070fa1":"- The best part. We can simply do a .fit() method as in Keras. It trains the model !!!","ba9e5f7d":"## Time to Use it !!!","c5f3ba4b":"- For hacky people, you simply use train_step and val_step to do it and customize the training too !!","ac41b594":"- Model Factory Simplifies the model making, directly creates model you need","a6499e87":"You can view the source code of this on GitHub here\nhttps:\/\/github.com\/oke-aditya\/pytorch_cnn_trainer","b31227ff":"- Simple Pip install from github.","b7f2e7c5":"## Install it","361a3fe0":"- Introducing PyTorch CNN Trainer.\n- A simple yet powerful trainer, which allows you to easily train over datasets.\n- It is very annoying to write training loop and training code for CNN training. Also to support all the training features it takes massive time.\n\n- Usually we don't need distributed training and it is very uncomfortable to use argparse and get the job done.\n\n- This simplifies the training. It provide you a powerful engine.py which can do lot of training functionalities. Also a dataset.py to load dataset in common scenarios.\n\n- Works for both torchvision and Ross Wightman's models [timm](https:\/\/github.com\/rwightman\/pytorch-image-models\/tree\/master\/timm)\n\n- Note: - Pytorch Trainer is not a distributed training script.\n\n- It will work good for single GPU machine for Google Colab \/ Kaggle.","f61f193b":"That's it !!! Training done.\n- It automates the stuff that you shouldn't worry about. Written in Pure PyTorch it has no extra library requirements too.\n- Train_step is useful for experienced people who want to do something hacky in their loop.\n- The library has only 4 Files !!!. Model Factory, Engine, dataset and utils. \n- I will add more features and support soon.\n"}}