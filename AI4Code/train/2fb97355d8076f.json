{"cell_type":{"a937b0ae":"code","fdbee49c":"code","10669e9a":"code","b36aa7ee":"code","897f500e":"code","9816e530":"code","495037d4":"code","0835701c":"code","99b69473":"code","ec750104":"code","8641f069":"code","426be84f":"code","57b2fcea":"code","7a5410d5":"code","bcc26fe8":"code","540a685c":"code","9f83df7e":"code","44c2e121":"code","8dcbe011":"code","4890dfdd":"code","40fc2fc5":"code","1ecaed14":"code","74ab770f":"code","d1491004":"code","a130e0d7":"code","adfc8500":"code","57307c95":"code","a9be466a":"code","ac73d08e":"code","57e5656b":"code","0d281851":"code","fef97f63":"code","1360ab4b":"code","99fadfa1":"code","349e698e":"code","bb890f46":"code","70ff6c3f":"code","429ea9ea":"code","7eadb811":"code","017f346b":"code","8ff5ac31":"code","00b797d3":"code","57eac5aa":"code","71466f7c":"code","8cc0fff0":"code","4f6237c8":"code","c9a7e161":"code","a4454020":"code","c4ade29f":"code","aff15316":"code","18b7912d":"code","62d87c40":"code","462a5f88":"markdown","ab55bfd3":"markdown","7a3737ce":"markdown","eb69f25c":"markdown","e4d10bba":"markdown","2fb3011a":"markdown","e7f79d7b":"markdown","2d7241f4":"markdown","a711c106":"markdown","9374644b":"markdown","911b0f23":"markdown"},"source":{"a937b0ae":"import pandas as pd\nimport numpy as np","fdbee49c":"df = pd.read_csv('..\/input\/leaf-classification\/train.csv.zip')","10669e9a":"df.head()","b36aa7ee":"df.shape","897f500e":"df.info()","9816e530":"df.describe()","495037d4":"# types\ndf.dtypes","0835701c":"set((df.dtypes).to_list())","99b69473":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\n\n# Image manipulation.\nimport PIL.Image\nfrom IPython.display import display","ec750104":"len(df.id)","8641f069":"# from zipfile import ZipFile\n# image_dir = '..\/input\/leaf-classification\/images.zip'\n# image_folder = ZipFile(image_dir, 'r')\n# image_folder.namelist()[0:5]","426be84f":"# image_folder.namelist()[1:2]","57b2fcea":"# image_folder","7a5410d5":"# # importing required modules \n# from zipfile import ZipFile \n  \n# # specifying the zip file name \n# images_zip = '..\/input\/leaf-classification\/images.zip'\n  \n# # opening the zip file in READ mode \n# with ZipFile(images_zip, 'r') as zip: \n#     # printing all the contents of the zip file \n# #     zip.printdir() \n  \n#     # extracting all the files \n#     print('Extracting all the files now...') \n#     zip.extractall() \n#     print('Done!') ","bcc26fe8":"image_dir = '.\/images'","540a685c":"# img = image_dir + '\/' + str(100) + '.jpg'\n# img","9f83df7e":"# img = mpimg.imread(img)\n# imgplot = plt.imshow(img)\n# plt.show()\n\n# img_lbl = df['species'].loc[100]\n# print(img_lbl)","44c2e121":"# type(imgplot)\n# # imgplot.shape\n\n","8dcbe011":"# type(img), img.shape","4890dfdd":"# img = img.resize((160, 240), mpimg.ANTIALIAS)\n\n# type(img), img.shape","40fc2fc5":"# randTrainInd = np.random.randint(len(df.id))\n# print(randTrainInd)\n\n# randomID = df.iloc[randTrainInd].id\n# print(randomID)\n\n# # df.loc[randTrainInd]\n# df[df['id']==randomID]['species']\n# # df.loc[(df[id] == randTrainInd)]\n\n# # df[df['id']==randomID]['species']","1ecaed14":"# # show some random images\n# plt.figure(figsize=(12,12))\n\n# for k in range(28):\n#     randTrainInd = np.random.randint(len(df.id))\n    \n#     randomID = df.iloc[randTrainInd].id\n    \n#     imageFilename = image_dir + '\/' + str(randomID) + '.jpg' \n    \n#     plt.subplot(4,7,k+1); \n    \n#     plt.imshow(mpimg.imread(imageFilename), cmap='gray')\n    \n# #     plt.title(df['species'].loc[randTrainInd], fontsize=8); \n#     plt.title(df['species'].loc[randTrainInd] + '; ' + str(randomID), fontsize=8); \n#     plt.axis('off')","74ab770f":"# X = df.drop(['id', 'species'], axis=1).values\n# X","d1491004":"X = df.drop(['id', 'species'], axis=1)\ny = df['species']\n\nprint(X.shape, y.shape)","a130e0d7":"from sklearn.preprocessing import LabelEncoder","adfc8500":"# df['species'].value_counts()\ndf['species'].unique()","57307c95":"classEncoder = LabelEncoder()\ntrainLabels = classEncoder.fit_transform(df.loc[:,'species'])","a9be466a":"trainLabels[:5] ","ac73d08e":"classEncoder.classes_[trainLabels[:5]]","57e5656b":"y.head()","0d281851":"y = classEncoder.fit_transform(y)\ny[:5]","fef97f63":"from sklearn.preprocessing import StandardScaler","1360ab4b":"scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","99fadfa1":"X_scaled[:5]","349e698e":"# Split the data into training and validation\nfrom sklearn.model_selection import train_test_split","bb890f46":"# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 99)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.3, random_state = 99)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","70ff6c3f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV","429ea9ea":"# Lets define the parameters for GridSearchCV.\nparams = {'C':[1, 10, 50, 100, 500, 1000, 2000],                   # Inverse of regularization strength; must be a positive float.\n          'tol': [0.001, 0.0001, 0.005]                           # olerance for stopping criteria.\n#           'penalty' : ['l1', 'l2', 'elasticnet', 'none'] ,          # Used to specify the norm used in the penalization.\n#           'solver' : ['lbfgs','newton-cg','sag','saga']            # Algorithm to use in the optimization problem.\n         }","7eadb811":"# Initiate the Logistic Regression Model.\nlr = LogisticRegression(solver='newton-cg', multi_class='multinomial')\n\n# lr = LogisticRegression(multi_class='multinomial')\n\n# Here we are taking solver as `newton-cg` we can also have other solvers such as `lbfgs`.","017f346b":"# For evluation we have to use log_loss, and for this we have to make a callable from GridSearchCV\nfrom sklearn.metrics import log_loss, make_scorer\nLogLoss = make_scorer(log_loss, greater_is_better=False, needs_proba=True)","8ff5ac31":"# lr1 = LogisticRegression(multi_class='multinomial')","00b797d3":"clf = GridSearchCV(lr, params, scoring = LogLoss, refit = 'True', n_jobs = 1, cv = 5)\n# Refer https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html to get more details.","57eac5aa":"lr.fit(X_train, y_train)","71466f7c":"clf_fit = clf.fit(X_scaled, y)","8cc0fff0":"print('Logistic Regression Score {}'.format( lr.score(X_test, y_test)))","4f6237c8":"print('Grid Search Score {}'.format( clf_fit.score(X_scaled, y)))","c9a7e161":"print(\"best params: \" + str(clf.best_params_))\n\nprint(\"best estimator: \" + str(clf.best_estimator_))\n\nclf.cv_results_['mean_test_score']\n\n# for i in ['mean_test_score', 'std_test_score', 'param_n_estimators']:\n#     print(i,\" : \",grid.cv_results_[i])\n        \n# for params, mean_score, scores in clf.cv_results_:\n#     print(\"%0.3f (+\/-%0.03f) for %r\" % (mean_score, scores.std(), params))\n#     print(scores)","a4454020":"test = pd.read_csv('..\/input\/leaf-classification\/test.csv.zip')\n\ntest_ids = test.pop('id')\n\nx_test = test.values\n\nscaler = StandardScaler()\n\nx_test = scaler.fit_transform(x_test)\n\ny_pred = clf.predict_proba(x_test)","c4ade29f":"y_pred","aff15316":"sample_sub = pd.read_csv('..\/input\/leaf-classification\/sample_submission.csv.zip', nrows = 5)\nsample_sub","18b7912d":"subm = pd.DataFrame(y_pred, index=test_ids, columns = classEncoder.classes_)\nsubm.to_csv('Submission_LogisticReg_v1.csv')","62d87c40":"subm.head()","462a5f88":"* So there are 990 data points \/ observations \/ records.\n* There are 194 variables in the train data set. Out of which one `species` is Target \/ Class \/ Dependent Variable, and rest 193 are Independent variables \/ Features \/ Attributes.\n* For images, there is a separate folder in .zip format. For this we will visualize the images.\n(each image is named with its corresponding id)\n* Data fields \n        id - an anonymous id unique to an image\n        margin_1, margin_2, margin_3, ..., margin_64 --> each of the 64 attribute vectors for the margin feature\n        shape_1, shape_2, shape_3, ..., shape_64 --> each of the 64 attribute vectors for the shape feature\n        texture_1, texture_2, texture_3, ..., texture_64 --> each of the 64 attribute vectors for the texture feature\n        ","ab55bfd3":"As we are using Logistic Regression, we have to scale the data first.","7a3737ce":"* First thing all images are in differnt sizes... we have to make it in one common shape \/ size.\n* Second we have to do some kind of encoding on our Target column.","eb69f25c":"# Submission","e4d10bba":"# Visualize\nLets visualize top 5 or 10 leafs.","2fb3011a":"# Import Libraries\nHere will import the required libraries,and will keep on importing other libraries as and when needed. Below are the main which needed to start the work with.","e7f79d7b":"# Load Data","2d7241f4":"So we have float and int as the 2 different data types.","a711c106":"As there are 194 features.. it is not showing the data type fr all features.\n\nThis can either be solve using set_options, so as to display all.\n\nor\n\nWe get the unique data types.","9374644b":"# Training the model on Data set\nFor now we start with the data set to train our first model which is using Logistic Regression. And later we will dig further with various other algorithms.\n\nAlso `species` is of Categorical type, we have to perform LabelEncoder to it.","911b0f23":"# Leaf Classification\nhttps:\/\/www.kaggle.com\/c\/leaf-classification\n\nThis is a kaggle challenge for leaf recognition.\n\nIn-here will understand the data set, and visualize it."}}