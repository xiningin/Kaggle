{"cell_type":{"f7645901":"code","866643f1":"code","04c51831":"code","6c933120":"code","205646ea":"code","2eb12a16":"code","ec0bc22c":"code","2a8d1ce9":"code","3663bd24":"code","402d933b":"code","fec2fa48":"code","7e9a82cc":"code","1b736529":"code","0250bb67":"code","14e97ae7":"code","0c42e4c0":"code","94c00547":"code","8cd2cd2c":"code","d9cae324":"code","15e90644":"code","8af3bd8e":"code","3496e6a7":"code","88b217e6":"code","f67d61de":"code","5e181469":"code","199932ec":"code","0b3226a4":"code","6d611579":"code","c88a6dfb":"code","6d867fc3":"code","3a172a56":"code","daca86d4":"code","fd0fd342":"code","db75ee9c":"code","b591ad02":"code","26c75eae":"code","ba67b35a":"code","c28723b1":"code","01c8b22a":"code","cd3aa621":"code","977280da":"code","13ac502b":"code","1c491f8e":"code","a11895b9":"code","5367bf5d":"code","4c2395a1":"code","92800ae3":"code","5767b64d":"code","780ab07f":"code","5e6bfd11":"code","4fca70c9":"code","c91c4414":"code","8804b455":"code","f0ffd4df":"code","ecbd9bc3":"code","945b4cdf":"code","4308d815":"code","1a04bb92":"code","2fa31abd":"markdown","acd0b189":"markdown","6fefd285":"markdown","11ac655e":"markdown","e3b88ed7":"markdown","1c9f6cb5":"markdown","6dbdc51e":"markdown","d49c0f1b":"markdown","e80866f8":"markdown","c4fbf74f":"markdown","e4ef0305":"markdown","2d0a854c":"markdown","43d07813":"markdown","0f134e28":"markdown","8e5b72a7":"markdown","ab636fcc":"markdown","bb1c7cc7":"markdown","5c5bae16":"markdown","2216d187":"markdown","0e4149ab":"markdown","3e32b9ef":"markdown","a09ad993":"markdown"},"source":{"f7645901":"# installation of keras self attention layer\n!pip install keras-self-attention\n\nimport os\nfrom collections import Counter\nfrom random import choice\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport requests\n\nfrom IPython.display import Image\n\nimport keras\nfrom keras_self_attention import SeqSelfAttention\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import models\n\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import plot_convergence\nfrom skopt.utils import use_named_args","866643f1":"# Tensorflow version checking\nprint(\"Tensorflow version \" + tf.__version__)","04c51831":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","6c933120":"# define some parameters\n\n## size of the train set\ntrain_percent_split = 0.9\n\n# first hyperparameters of the RNN model\nepochs = 1\ndropout = 0\nembedding_size = 128\nhidden_size_lstm = 128\nlearning_rate = 0.00276\nattention_width = 20\nadd_dense_layer = True\nhidden_size_dense = 128\n\nvalidation_split = 0.1\nn_reco = 20\nn_min_interactions = 20\nmax_length_seq = 20\nnb_last_item = 20\nmax_users = 5000\n\n# iterations number for optimization with scikit-optimize lib\nn_random_starts = 2\nn_calls = 15","205646ea":"# paths definition\npath = '\/kaggle\/input\/bookcrossing-dataset\/Book reviews\/'\nbook_fp = os.path.join(path, 'BX-Books.csv')\nuser_fp = os.path.join(path, 'BX-Users.csv')\nrating_fp = os.path.join(path, 'BX-Book-Ratings.csv')","2eb12a16":"# files reading\ndef read_file(fp):\n    data = pd.read_csv(fp,\n                       sep=';',\n                       encoding='latin-1',\n                       low_memory=False,\n                       header=0,\n                       error_bad_lines=False)\n    return data\n    \nuser_df = read_file(user_fp)\nitem_df = read_file(book_fp)\nrating_df = read_file(rating_fp)","ec0bc22c":"print(user_df.shape)\nprint(user_df.head())","2a8d1ce9":"print(item_df.shape)\nprint(item_df.head())","3663bd24":"print(rating_df.shape)\nprint(rating_df.head())","402d933b":"rating_df = rating_df.sample(frac=1).reset_index(drop=True)","fec2fa48":"rating_df.isna().sum()","7e9a82cc":"colname_mapping = {\n    'ISBN': 'item',\n    'User-ID': 'user',\n    'Book-Rating': 'rating',\n    'Book-Title': 'name',\n    'Book-Author': 'author',\n    'Image-URL-M': 'image',\n    'Publisher': 'publisher',\n    'Year-Of-Publication': 'year'\n}\nrating_df = rating_df.rename(columns=colname_mapping)\nitem_df = item_df.rename(columns=colname_mapping)\nuser_df = user_df.rename(columns=colname_mapping)","1b736529":"items = list(set(item_df.item.unique().tolist() + rating_df.item.unique().tolist()))\nusers = user_df.user.unique() \nprint(f'number of unique items: {len(items)}\\nnumber of unique users: {len(users)}')","0250bb67":"rating_df = rating_df[rating_df.item.isin(items)]\nitem_df = item_df[item_df.item.isin(items)]\nrating_df.shape, item_df.shape","14e97ae7":"user_to_token = {user: int(token) for token, user in enumerate(users)}\ntoken_to_item = {token: user for user, token in user_to_token.items()}\nitem_to_token = {item: int(token) for token, item in enumerate(items)}\ntoken_to_item = {token: item for item, token in item_to_token.items()}\n\nrating_df['user_id'] = rating_df['user'].map(user_to_token)\nrating_df['item_id'] = rating_df['item'].map(item_to_token).dropna().astype(int)\n\nitem_df['item_id'] = item_df['item'].map(item_to_token)\nuser_df['user_id'] = user_df['user'].map(user_to_token)","0c42e4c0":"#%%timeit\n#diff = set(item_df.item_id).difference(set(rating_df.item_id))\n#len(diff), diff","94c00547":"item_df.item_id.nunique(), rating_df.item_id.nunique()","8cd2cd2c":"item_df.head()","d9cae324":"def print_single_item_characteristics(item_id=None, item_df=item_df):\n    if not item_id:\n        item_id = choice(item_df.item_id)\n    if item_id not in set(item_df.item_id):\n        print(f'item_id {item_id} not in df')\n        return None\n\n    item_df = item_df[item_df['item_id'] == item_id]\n    url = item_df.image.values[0]\n    response = requests.get(url)\n\n    print(f'item_id: {item_id}; '\n          f'name: {item_df.name.values[0]}; '\n          f'author: {item_df.author.values[0]} '\n          f'publisher: {item_df.publisher.values[0]} '\n          f'year: {item_df.year.values[0]}')\n    return Image(url)\n\ndef print_items_characteristics(item_id_list):\n    for item_id in item_id_list:\n        display(print_single_item_characteristics(item_id=item_id))","15e90644":"n_rating_by_user = rating_df.user_id.value_counts()","8af3bd8e":"n_rating_by_user.describe()","3496e6a7":"top3_item_id = list(rating_df.item_id.value_counts().index.values[:3])","88b217e6":"top3_item_id","f67d61de":"print_items_characteristics(item_id_list=top3_item_id)","5e181469":"user_occurence = Counter(rating_df.user).most_common()\nprint(user_occurence[:20])","199932ec":"def user_with_n_interaction(data, n):\n    print(f'length before filtering: {len(data)}.')\n    user_occurence = Counter(data.user)\n\n    user_to_keep = [\n        user\n        for user, occ in user_occurence.items()\n        if occ >= n\n    ]\n\n    data_filtered = data[data['user'].isin(user_to_keep)]\n    print(f'length after filtering: {len(data_filtered)}.')\n    return data_filtered\n\nrating_df = user_with_n_interaction(data=rating_df, n=n_min_interactions)","0b3226a4":"split_ind = int(len(rating_df) * train_percent_split)\ntrain, test = rating_df[:split_ind], rating_df[split_ind:]\nprint(f'shape of train: {train.shape}\\nshape of test: {test.shape}')","6d611579":"#function to prepare sequence data\ndef prepare_sequences(data, users, item_to_token, max_length=20, \n                      one_hot_encoding=False):\n    \n    print('preparing sequences')\n    \n    #generate sequences - see https:\/\/stackoverflow.com\/questions\/36864699\/pandas-pivot-dataframe-with-unequal-columns        \n    data = pd.concat([\n        pd.DataFrame(\n            {\n                g:[0] * (max_length+1-len(d['item_id'].tolist()[-max_length-1:])) + d['item_id'].tolist()[-max_length-1:]\n            }\n        )\n        for g,d in data.groupby('user_id')], axis=1)\n \n    \n    #from pandas dataframe to numpy array\n    data = data.transpose().values\n        \n    #transpose and build the arrays\n    x = np.array([i[:-1] for i in data])\n    y = np.array([i[1:] for i in data])\n            \n    #build the one-hot encoding, if we want\n    if one_hot_encoding:\n        y = np_utils.to_categorical(y, len(item_to_token)+1)\n    else:\n        y = np.expand_dims(y, -1)\n    \n    print('sequences prepared')\n        \n    return (x, y)\n\n#function to extract prediction from keras model at last timestep\ndef predict_last_timestep(model, data):\n    #calculate the model output\n    prediction = model.predict(data)\n    #keep only the prediction at the final timestep\n    return prediction[-1]","c88a6dfb":"x, y = prepare_sequences(data=train, users=users, item_to_token=item_to_token, max_length=max_length_seq)","6d867fc3":"def keras_model(hidden_size_lstm=hidden_size_lstm, \n                learning_rate=learning_rate, \n                dropout=dropout,\n                attention_width=attention_width,\n                embedding_size=embedding_size,\n                add_dense_layer=add_dense_layer,\n                hidden_size_dense=hidden_size_dense,\n                embedding_matrix=None,\n                item_to_token=item_to_token):\n    \n    with strategy.scope():\n        embedding_layer = Embedding(len(item_to_token)+1,\n                                    embedding_size,\n                                    weights=embedding_matrix,\n                                    mask_zero=True)\n\n        model = Sequential()\n        model.add(embedding_layer)\n        model.add(LSTM(units=hidden_size_lstm,\n                       activation='tanh', dropout=dropout,\n                       return_sequences=True))\n        model.add(SeqSelfAttention(attention_activation='sigmoid',\n                                   attention_width=attention_width,\n                                   history_only=True))\n        if add_dense_layer:\n            model.add(Dense(units=hidden_size_dense, activation='relu'))\n        model.add(Dense(units=len(item_to_token)+1, activation='softmax'))\n        optimizer = Adam(lr=learning_rate)\n    # Compile model\n    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer,\n                  metrics=['sparse_categorical_accuracy'])\n\n    return model","3a172a56":"#prepare the model\nmodel = keras_model(hidden_size_lstm=hidden_size_lstm, \n                    learning_rate=learning_rate, \n                    dropout=dropout, \n                    embedding_size=embedding_size)  \n\nhistory = model.fit(x, y,\n                    epochs=epochs,\n                    validation_split=validation_split,\n                    batch_size=64,\n                    verbose=1)\n\nprint(history.history['val_sparse_categorical_accuracy'][-1])\n","daca86d4":"common_user = list(set(train.user_id).intersection(test.user_id))\ncommon_user[:10]","fd0fd342":"print(f'There are {len(common_user)} common users between the train and test set')","db75ee9c":"def reco_from_item_id_interacted(item_id_interacted, model=model, n_reco=10):\n    predictions = predict_last_timestep(model=model,\n                                        data=[item_id_interacted]).argsort()[0][:n_reco]\n    return list(predictions)\n\ndef build_user_to_interacted(user_id, train=train):\n    return train[train['user_id'] == user_id]['item_id'].dropna().unique().tolist()\n\n#item_id_interacted = build_user_to_interacted(user_id=choice(user_train))\n#len(item_id_interacted)","b591ad02":"def user_to_last_visited_item_id_dict(train: pd.DataFrame,\n                                      user_list: list,\n                                      nb_last_item: int=None) -> dict:\n    \"\"\"\n    Return a dictionary mapping user to last visited items id.\n    input:\n            :train: pd.DataFrame, training set\n            :user_list: list, head_visitor_id\n            :nb_last_item: int, number of last interacted items\n                           to use to predict recommendation\n    output:\n            :: dict, mapping the user (head_visitor_id) to the last\n               visited items\n    \"\"\"\n    if nb_last_item:\n        return train.groupby('user_id')['item_id'].apply(lambda g: g.values\n                                                      .tolist()[-nb_last_item:]).to_dict()\n    else:\n        return train.groupby('user_id')['item_id'].apply(lambda g: g.values\n                                                      .tolist()).to_dict()\n\nuser_to_last_visited_item_id_dict = user_to_last_visited_item_id_dict(train=train,\n                                                                      user_list=train.user_id.unique().tolist(),\n                                                                      nb_last_item=nb_last_item)","26c75eae":"list(user_to_last_visited_item_id_dict.items())[:5]","ba67b35a":"def predict_for_one(\n    model,\n    user_id,\n    n_reco,\n    user_to_last_visited_item_id_dict=user_to_last_visited_item_id_dict\n):\n    item_id_interacted = user_to_last_visited_item_id_dict[user_id]\n    reco_id = reco_from_item_id_interacted(item_id_interacted,\n                                           model=model,\n                                           n_reco=n_reco)\n    return reco_id","c28723b1":"def predict(\n    model,\n    user_id_list,\n    n_reco,\n    user_to_last_visited_item_id_dict=user_to_last_visited_item_id_dict,\n    max_users=None, # reduce computation time\n):\n    start = time.time()\n    print(f'recommendation computation for {len(user_id_list[:max_users])} users.')\n    print(f'n_reco={n_reco}')\n    reco_dict = {\n        user_id: predict_for_one(\n            model=model,\n            user_id=user_id,\n            n_reco=n_reco,\n            user_to_last_visited_item_id_dict=user_to_last_visited_item_id_dict\n        )\n        for user_id in user_id_list[:max_users]\n    }\n    print(f'predict for {len(user_id_list[:max_users])} spent {round(time.time()-start, 2)} s.')\n    return reco_dict\n#max_users=100\n#n_reco=50\nreco_dict = predict(\n    model=model,\n    user_id_list=common_user,\n    user_to_last_visited_item_id_dict=user_to_last_visited_item_id_dict,\n    max_users=max_users,\n    n_reco=n_reco)","01c8b22a":"list(reco_dict.items())[:3]","cd3aa621":"#Function to calculate, precision, recall and coverage\ndef statistics_at_k(reco_dict,\n                    test_df,\n                    train_df,\n                    calculate_precision=True,\n                    calculate_recall=True, \n                    calculate_coverage=True): \n    '''\n    reco_dict: dictionary with the uid as key, list of items recommended as attribute\n    test_df: dataframe of user-item interactions\n    '''     \n    #calculate precision\n    if calculate_precision:\n        k_relevant = 0\n        k_total = 0\n        for uid, iid in reco_dict.items():\n            iid_test = set(test_df[test_df['user_id'] == uid]['item_id'])\n            for j in iid:\n                k_total += 1\n                if j in iid_test:\n                    k_relevant += 1\n        if not k_total:\n            precision = 0\n        else:\n            precision = k_relevant\/k_total\n        print(f'precision={precision}')    \n    else:\n        precision = None\n        \n    #calculate precision\n    if calculate_recall:\n        k_relevant = 0\n        k_total = 0\n        for uid, iid in reco_dict.items():\n            for j in list(test_df[test_df['user_id'] == uid]['item_id']):\n                k_total += 1\n                if j in set(iid):\n                    k_relevant += 1\n        \n        if not k_total:\n            recall = 0\n        else:\n            recall = k_relevant\/k_total\n        print(f'recall={recall}')\n    else:\n        recall = None\n        \n    #calculate coverage\n    if calculate_coverage:\n        nb_recommended = len(set(sum(reco_dict.values(), [])))\n        nb_total = len(train_df['item_id'].unique())\n        coverage = nb_recommended\/nb_total\n        print(f'coverage={coverage}')\n    else:\n        coverage = None\n    \n    return precision, recall, coverage","977280da":"# statistics with RNN recommendation\nstatistics_at_k(reco_dict=reco_dict,\n                test_df=test,\n                train_df=train,\n                calculate_precision=True,\n                calculate_recall=True, \n                calculate_coverage=True)","13ac502b":"topn = list(train.item_id.value_counts().index.values[:n_reco])\ntopn","1c491f8e":"reco_topn_dict = {\n    user_id: topn\n    for user_id in reco_dict.keys() \n}","a11895b9":"list(reco_topn_dict.items())[:3]","5367bf5d":"# statistics with top n recommendation\nstatistics_at_k(reco_dict=reco_topn_dict,\n                test_df=test,\n                train_df=train,\n                calculate_precision=True,\n                calculate_recall=True, \n                calculate_coverage=True)","4c2395a1":"item_id_interacted = [197383]\nitem_id_pred = predict_last_timestep(\n    model=model,data=[item_id_interacted]\n).argsort()[0][:20]","92800ae3":"# past interactions\nitem_df[item_df['item_id'].isin(item_id_interacted)]","5767b64d":"# visualization of the pas\nprint_items_characteristics(item_id_list=item_id_interacted)","780ab07f":"# predictions\nitem_df[item_df['item_id'].isin(item_id_pred)]","5e6bfd11":"item_id_pred","4fca70c9":"print_items_characteristics(item_id_list=item_id_pred)","c91c4414":"del history\ndel model","8804b455":"# define the dimension to search\ndim_epochs = Integer(low=1, high=15, name='epochs')\ndim_hidden_size_lstm = Categorical(categories=[32, 64, 128, 256], name='hidden_size_lstm')\ndim_learning_rate = Real(low=1e-4, high=5e-1, prior='log-uniform',\n                         name='learning_rate')\ndim_attention_width = Integer(low=1, high=50, name='attention_width')\ndim_dropout = Real(low=0, high=0.9, name='dropout')\ndim_embedding_size = Categorical(categories=[64, 128, 256, 512], name='embedding_size')\ndim_add_dense_layer = Categorical(categories=[True, False], name='add_dense_layer')\ndim_hidden_size_dense = Categorical(categories=[32, 64, 128, 256], name='hidden_size_dense')\n\ndimensions = [dim_epochs,\n              dim_hidden_size_lstm,\n              dim_learning_rate,\n              dim_attention_width,\n              dim_dropout,\n              dim_embedding_size,\n              dim_add_dense_layer,\n              dim_hidden_size_dense]\n\nepochs = 3\ndropout = 0\nembedding_size = 256\nhidden_size_lstm = 64\nlearning_rate = 0.00276","f0ffd4df":"def optimize(dimensions=dimensions, n_calls=15, n_random_starts=3, verbose=1, x0=None):\n    print(dimensions)\n    @use_named_args(dimensions=dimensions)\n    def fitness(**params):\n        print(f'params={params}')\n        model = keras_model(hidden_size_lstm=params['hidden_size_lstm'],\n                            learning_rate=params['learning_rate'],\n                            attention_width=params['attention_width'],\n                            dropout=params['dropout'],\n                            embedding_size=params['embedding_size'],\n                            add_dense_layer=params['add_dense_layer'],\n                            hidden_size_dense=params['hidden_size_dense'])  \n\n        history = model.fit(x, y,\n                            epochs=params['epochs'],\n                            validation_split=validation_split,\n                            batch_size=64,\n                            verbose=verbose)\n        sca = history.history['val_sparse_categorical_accuracy'][-1]\n        print(f'##sca={sca}## with params={params}')\n        del history\n        del model\n        return -1.0 * sca\n    \n    res = gp_minimize(func=fitness,\n                      dimensions=dimensions,\n                      acq_func='EI', # Expected Improvement.\n                      n_calls=n_calls,\n                      n_random_starts=n_random_starts,\n                      x0=x0)\n    print(f'best accuracy={-1.0 * res.fun} with {res.x}')\n    return res","ecbd9bc3":"res = optimize(dimensions=dimensions,\n               n_calls=n_calls,\n               n_random_starts=n_random_starts,\n               x0=[3, 256, 0.001, 40, 0, 512, True, 128])","945b4cdf":"res","4308d815":"y","1a04bb92":"x","2fa31abd":"Defining functions to print books characteristics such as name, author, publisher, year of publication and display the cover image for vizualisation","acd0b189":"# 7. Recommendations prediction","6fefd285":"## Libraries importation and setup","11ac655e":"Count the number of \"interactions\" per user (i.e. ratings per user)","e3b88ed7":"# 8. Performance metrics computation","1c9f6cb5":"Rows seem sorted by User-ID, let's shuffle the rows","6dbdc51e":"We make a list of users which are both present in train and test set. We will use these users to compute the performance metrics of our model.","d49c0f1b":"There is a huge number of different books!","e80866f8":"## RNN books recommendation system with TPU training","c4fbf74f":"# 4. Sequences preparation for the RNN model","e4ef0305":"# 5. Keras model definition","2d0a854c":"# 10. RNN model hyperparameters optimization","43d07813":"# 6. Model training","0f134e28":"We don't have NA on rating_df! Good point.\n\n# 3. Data preparation\n\nLet's rename the columns for simplicity.","8e5b72a7":"Tokenization: converting books and users \"real\" ids to integer for efficiency of manipulation","ab636fcc":"Keep users with at least n interactions, to reduce the data size and probably expect better \"qualitative users\"","bb1c7cc7":"# 9. Visualization example","5c5bae16":"# 1. Notebook setup","2216d187":"Datasets inspection","0e4149ab":"# 2. Data importation","3e32b9ef":"Let's display some popular books characteristics for testing","a09ad993":"Welcome to this notebook\n\nWe assume that there is an interaction between an item (book) if there is a rating on the BX-Book-Ratings.csv file. We don't take into account the rate the user gives to the book.\n\nNote: \"items\" in this notebook will refer to books\n\nYou want to improve your knowledge in recommendation systems (and the possible application for ecommerce)? You can check it out this [Medium blog post](https:\/\/medium.com\/decathlondevelopers\/personalization-strategy-and-recommendation-systems-at-d%C3%A9cathlon-canada-d9cb3d37f675) from [D\u00e9cathlon Canada](https:\/\/www.decathlon.ca\/en\/) (world's largest sporting good retailer)\n\nNote2: this notebook is in progress, if you like it or you would like the analysis to be deeper, please upvote it!"}}