{"cell_type":{"6929976c":"code","a8eb825d":"code","e82b7955":"code","8c39afa3":"code","5fc168d4":"code","1061832a":"code","3314d7ba":"code","a020dc0e":"code","bf81dda2":"code","72849eaa":"code","f8727fb4":"code","fe735cc3":"code","6864b53a":"code","37cf5390":"code","a4a4fb60":"code","23d17cf5":"code","88c1e136":"code","05c7e095":"code","54eb467e":"code","9752a874":"code","fa540686":"code","5418826d":"code","bf7df57f":"code","a72b4247":"code","235f2a2a":"code","a253adb0":"code","eea667d7":"code","95a73a36":"code","a6c97de5":"code","00a0876e":"code","97337058":"code","1f1b674a":"code","70015200":"code","82793acc":"code","631af8a5":"code","c6bb2dfe":"code","0d253106":"code","4ee438a8":"code","a2063370":"code","066924be":"code","1a4e66e6":"code","7b5f1752":"code","e8eff64b":"code","042e7ae9":"code","71ac365b":"code","f2416db7":"code","6f112045":"code","6f96542b":"code","97b041bb":"code","38ef4a2f":"markdown","e94c4944":"markdown","16c842f3":"markdown","8e8fc46b":"markdown","4fc8e863":"markdown","21e72f4f":"markdown","616fcaf3":"markdown","1b965395":"markdown","79806055":"markdown"},"source":{"6929976c":"import os\nimport sys \nimport json\nimport glob\nimport random\nimport collections\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport pydicom\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nfrom torch import nn\nfrom torch.utils import data as torch_data\nfrom sklearn import model_selection as sk_model_selection\nfrom torch.nn import functional as torch_functional\nimport torch.nn.functional as F\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score","a8eb825d":"\ndata_directory = '..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification'\npytorch3dpath = \"..\/input\/efficientnetpyttorch3d\/EfficientNet-PyTorch-3D\"\n\n    \nmri_types = ['FLAIR','T1w','T1wCE','T2w']\nSIZE = 256\nNUM_IMAGES = 64\n\nsys.path.append(pytorch3dpath)\nfrom efficientnet_pytorch_3d import EfficientNet3D","e82b7955":"def load_dicom_image(path, img_size=SIZE):\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    if np.min(data)==np.max(data):\n        data = np.zeros((img_size,img_size))\n        return data\n    data = data - np.min(data)\n    if np.max(data) != 0:\n        data = data \/ np.max(data)\n    \n    #data = (data * 255).astype(np.uint8)\n    data = cv2.resize(data, (img_size, img_size))\n    return data\n\ndef load_dicom_images_3d(scan_id, num_imgs=NUM_IMAGES, img_size=SIZE, mri_type=\"FLAIR\", split=\"train\"):\n\n    files = sorted(glob.glob(f\"{data_directory}\/{split}\/{scan_id}\/{mri_type}\/*.dcm\"))\n    \n    middle = len(files)\/\/2\n    num_imgs2 = num_imgs\/\/2\n    p1 = max(0, middle - num_imgs2)\n    p2 = min(len(files), middle + num_imgs2)\n    img3d = np.stack([load_dicom_image(f) for f in files[p1:p2]]).T \n    if img3d.shape[-1] < num_imgs:\n        n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-1]))\n        img3d = np.concatenate((img3d,  n_zero), axis = -1)\n            \n    return np.expand_dims(img3d,0)\n\n#load_dicom_images_3d(\"00000\").shape","8c39afa3":"def set_seed(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n\nset_seed(42)","5fc168d4":"class Dataset(torch_data.Dataset):\n    def __init__(self, paths, targets=None, mri_type=None, label_smoothing=0.01, split=\"train\"):\n        self.paths = paths\n        self.targets = targets\n        self.mri_type = mri_type\n        self.label_smoothing = label_smoothing\n        self.split = split\n          \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, index):\n        scan_id = self.paths[index]\n        if self.targets is None:\n            data = load_dicom_images_3d(str(scan_id).zfill(5), mri_type=self.mri_type[index], split=self.split)\n        else:\n            data = load_dicom_images_3d(str(scan_id).zfill(5), mri_type=self.mri_type[index], split=\"train\")\n\n        if self.targets is None:\n            return {\"X\": torch.tensor(data).float(), \"id\": scan_id}\n        else:\n            y = torch.tensor(abs(self.targets[index]-self.label_smoothing), dtype=torch.float)\n            return {\"X\": torch.tensor(data).float(), \"y\": y}\n","1061832a":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = EfficientNet3D.from_name(\"efficientnet-b0\", override_params={'num_classes': 2}, in_channels=1)\n        n_features = self.net._fc.in_features\n        self.net._fc = nn.Linear(in_features=n_features, out_features=1, bias=True)\n    \n    def forward(self, x):\n        out = self.net(x)\n        return out\n    ","3314d7ba":"class Dataset(torch_data.Dataset):\n    def __init__(self, paths, targets=None, mri_type=None, label_smoothing=0.01, split=\"train\"):\n        self.paths = paths\n        self.targets = targets\n        self.mri_type = mri_type\n        self.label_smoothing = label_smoothing\n        self.split = split\n          \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, index):\n        scan_id = self.paths[index]\n        if self.targets is None:\n            data = load_dicom_images_3d(str(scan_id).zfill(5), mri_type=self.mri_type[index], split=self.split)\n        else:\n            data = load_dicom_images_3d(str(scan_id).zfill(5), mri_type=self.mri_type[index], split=\"train\")\n\n        if self.targets is None:\n            return {\"X\": torch.tensor(data).float(), \"id\": scan_id}\n        else:\n            y = torch.tensor(abs(self.targets[index]-self.label_smoothing), dtype=torch.float)\n            return {\"X\": torch.tensor(data).float(), \"y\": y}","a020dc0e":"modelfiles=['..\/input\/3deffmodels\/FLAIR-e2-loss0.696-auc0.605.pth','..\/input\/3deffmodels\/T1w-e2-loss0.718-auc0.579.pth','..\/input\/3deffmodels\/T1wCE-e6-loss0.683-auc0.633.pth','..\/input\/3deffmodels\/T2w-e8-loss0.658-auc0.677.pth']","bf81dda2":"def predict(modelfile, df, mri_type, split):\n    print(\"Predict:\", modelfile, mri_type, df.shape)\n    df.loc[:,\"MRI_Type\"] = mri_type\n    data_retriever = Dataset(\n        df.index.values, \n        mri_type=df[\"MRI_Type\"].values,\n        split=split\n    )\n\n    data_loader = torch_data.DataLoader(\n        data_retriever,\n        batch_size=1,\n        shuffle=False,\n        num_workers=8,\n    )\n   \n    model = Model()\n    model.to(device)\n    \n    checkpoint = torch.load(modelfile)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    model.eval()\n    \n    y_pred = []\n    ids = []\n\n    for e, batch in enumerate(data_loader,1):\n        print(f\"{e}\/{len(data_loader)}\", end=\"\\r\")\n        with torch.no_grad():\n            tmp_pred = torch.sigmoid(model(batch[\"X\"].to(device))).cpu().numpy().squeeze()\n            if tmp_pred.size == 1:\n                y_pred.append(tmp_pred)\n            else:\n                y_pred.extend(tmp_pred.tolist())\n            ids.extend(batch[\"id\"].numpy().tolist())\n            \n    preddf = pd.DataFrame({\"BraTS21ID\": ids, \"MGMT_value\": y_pred}) \n    preddf = preddf.set_index(\"BraTS21ID\")\n    return preddf","72849eaa":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nsubmission = pd.read_csv(f\"{data_directory}\/sample_submission.csv\", index_col=\"BraTS21ID\")\n\nsubmission[\"MGMT_value\"] = 0\nfor m, mtype in zip(modelfiles, mri_types):\n    pred = predict(m, submission, mtype, split=\"test\")\n    submission[\"MGMT_value\"] += pred[\"MGMT_value\"]\n\n#submission[\"MGMT_value\"] \/= len(modelfiles)\n#submission[\"MGMT_value\"].to_csv(\"submission.csv\")","f8727fb4":"submission","fe735cc3":"mgmt=np.array(submission[\"MGMT_value\"])","6864b53a":"sns.displot(submission[\"MGMT_value\"])","37cf5390":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random as rd\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nfrom tensorflow import keras\nimport tensorflow as tf\nprint(\"Tensorflow version \" + tf.__version__)\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a4a4fb60":"IM_SIZE = 256","23d17cf5":"model = keras.models.load_model('..\/input\/emodels\/model (7).h5 (1)\/model (7).h5')","88c1e136":"model.summary()","05c7e095":"def mid_crop(img,c=25,c2=30):\n    c21=int(img.shape[0]\/\/2)\n    c22=int(img.shape[1]\/\/2)\n    return img[c22-c2:c22+c2,c21-c:c21+c]\ndef read_xray(path, voi_lut = False, fix_monochrome = True):\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    #data = (data * 255).astype(np.uint8)\n        \n    return mid_crop(cv2.resize(data*255,(100,100)))\ndef pad_images(imgs,img_shape=(28,28)):\n    padded=np.zeros((imgs.shape[0],max([len(x) for x in imgs]+[356]),img_shape[0],img_shape[1]))\n    for i in range(imgs.shape[0]):\n        for j in range(len(imgs[i])):\n            try:\n                padded[i,j]=imgs[i][j]\n            except:\n                break\n    return padded[:,:356,...,np.newaxis]\ndef get_prediction_per_case(patient):\n    \n    path = f'..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/test\/{patient}\/FLAIR\/'\n\n    list_subfolders_with_paths = [f for f in os.listdir(path)]\n    \n    prediction = []\n    \n    imagess=[]\n    \n    for images in list_subfolders_with_paths:\n        \n       \n        img = read_xray(path+images)\n        if np.max(img) > 0 and np.mean(img)>= 0.015:\n             \n             \n       \n             img=mid_crop(cv2.resize(img,(100,100)))\n        \n             img=cv2.resize(img,(50,50))\n             img=cv2.merge((img, img, img)) \n             imagess.append(img\/255)\n    \n    #print(pad_images(np.array([imagess]),(28,28)).shape)\n  \n    \n    return float(np.mean(model.predict(np.array(imagess)), axis=0))","54eb467e":"get_prediction_per_case('00047')","9752a874":"df = pd.read_csv('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/sample_submission.csv',dtype=\"string\")","fa540686":"mgmt = (mgmt+df['BraTS21ID'].apply(get_prediction_per_case))","5418826d":"mgmt=np.array(mgmt)","bf7df57f":"mgmt","a72b4247":"sns.histplot(mgmt)","235f2a2a":"model = keras.models.load_model('..\/input\/emodels\/T1w - model.h5') ","a253adb0":"def mid_crop(img,c=25,c2=30):\n    c21=int(img.shape[0]\/\/2)\n    c22=int(img.shape[1]\/\/2)\n    return img[c22-c2:c22+c2,c21-c:c21+c]\ndef read_xray(path, voi_lut = False, fix_monochrome = True):\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    #data = (data * 255).astype(np.uint8)\n        \n    return mid_crop(cv2.resize(data*255,(100,100)))\ndef pad_images(imgs,img_shape=(28,28)):\n    padded=np.zeros((imgs.shape[0],max([len(x) for x in imgs]+[356]),img_shape[0],img_shape[1]))\n    for i in range(imgs.shape[0]):\n        for j in range(len(imgs[i])):\n            try:\n                padded[i,j]=imgs[i][j]\n            except:\n                break\n    return padded[:,:356,...,np.newaxis]\ndef get_prediction_per_case(patient):\n    \n    path = f'..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/test\/{patient}\/T1w\/'\n\n    list_subfolders_with_paths = [f for f in os.listdir(path)]\n    \n    prediction = []\n    \n    imagess=[]\n    \n    for images in list_subfolders_with_paths:\n        \n       \n        img = read_xray(path+images)\n        if np.max(img) > 0 and np.mean(img)>= 0.015:\n             \n             \n       \n             img=mid_crop(cv2.resize(img,(100,100)))\n        \n             img=cv2.resize(img,(50,50))\n             img=cv2.merge((img, img, img)) \n             imagess.append(img\/255)\n    \n    #print(pad_images(np.array([imagess]),(28,28)).shape)\n  \n    \n    return float(np.mean(model.predict(np.array(imagess)), axis=0))","eea667d7":"get_prediction_per_case('00047')","95a73a36":"mgmt = (mgmt+df['BraTS21ID'].apply(get_prediction_per_case))","a6c97de5":"mgmt=np.array(mgmt)","00a0876e":"model = keras.models.load_model('..\/input\/emodels\/T1wCE - model.h5') ","97337058":"def mid_crop(img,c=25,c2=30):\n    c21=int(img.shape[0]\/\/2)\n    c22=int(img.shape[1]\/\/2)\n    return img[c22-c2:c22+c2,c21-c:c21+c]\ndef read_xray(path, voi_lut = False, fix_monochrome = True):\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    #data = (data * 255).astype(np.uint8)\n        \n    return mid_crop(cv2.resize(data*255,(100,100)))\ndef pad_images(imgs,img_shape=(28,28)):\n    padded=np.zeros((imgs.shape[0],max([len(x) for x in imgs]+[356]),img_shape[0],img_shape[1]))\n    for i in range(imgs.shape[0]):\n        for j in range(len(imgs[i])):\n            try:\n                padded[i,j]=imgs[i][j]\n            except:\n                break\n    return padded[:,:356,...,np.newaxis]\ndef get_prediction_per_case(patient):\n    \n    path = f'..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/test\/{patient}\/T1wCE\/'\n\n    list_subfolders_with_paths = [f for f in os.listdir(path)]\n    \n    prediction = []\n    \n    imagess=[]\n    \n    for images in list_subfolders_with_paths:\n        \n       \n        img = read_xray(path+images)\n        if np.max(img) > 0 and np.mean(img)>= 0.015:\n             \n             \n       \n             img=mid_crop(cv2.resize(img,(100,100)))\n        \n             img=cv2.resize(img,(50,50))\n             img=cv2.merge((img, img, img)) \n             imagess.append(img\/255)\n    \n    #print(pad_images(np.array([imagess]),(28,28)).shape)\n  \n    \n    return float(np.mean(model.predict(np.array(imagess)), axis=0))","1f1b674a":"get_prediction_per_case('00047')","70015200":"mgmt = (mgmt+df['BraTS21ID'].apply(get_prediction_per_case))","82793acc":"mgmt=np.array(mgmt)","631af8a5":"model = keras.models.load_model('..\/input\/emodels\/T2w - model.h5') ","c6bb2dfe":"def mid_crop(img,c=25,c2=30):\n    c21=int(img.shape[0]\/\/2)\n    c22=int(img.shape[1]\/\/2)\n    return img[c22-c2:c22+c2,c21-c:c21+c]\ndef read_xray(path, voi_lut = False, fix_monochrome = True):\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    #data = (data * 255).astype(np.uint8)\n        \n    return mid_crop(cv2.resize(data*255,(100,100)))\ndef pad_images(imgs,img_shape=(28,28)):\n    padded=np.zeros((imgs.shape[0],max([len(x) for x in imgs]+[356]),img_shape[0],img_shape[1]))\n    for i in range(imgs.shape[0]):\n        for j in range(len(imgs[i])):\n            try:\n                padded[i,j]=imgs[i][j]\n            except:\n                break\n    return padded[:,:356,...,np.newaxis]\ndef get_prediction_per_case(patient):\n    \n    path = f'..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/test\/{patient}\/T2w\/'\n\n    list_subfolders_with_paths = [f for f in os.listdir(path)]\n    \n    prediction = []\n    \n    imagess=[]\n    \n    for images in list_subfolders_with_paths:\n        \n       \n        img = read_xray(path+images)\n        if np.max(img) > 0 and np.mean(img)>= 0.015:\n             \n             \n       \n             img=mid_crop(cv2.resize(img,(100,100)))\n        \n             img=cv2.resize(img,(50,50))\n             img=cv2.merge((img, img, img)) \n             imagess.append(img\/255)\n    \n    #print(pad_images(np.array([imagess]),(28,28)).shape)\n  \n    \n    return float(np.mean(model.predict(np.array(imagess)), axis=0))","0d253106":"get_prediction_per_case('00047')","4ee438a8":"\nmgmt = (mgmt+df['BraTS21ID'].apply(get_prediction_per_case))","a2063370":"mgmt=np.array(mgmt)","066924be":"sns.distplot(mgmt)","1a4e66e6":"model = keras.models.load_model('..\/input\/effect0-brain\/Brain_flair_model_effect.h5',custom_objects={\"FixedDropout\": keras.layers.Dropout})\ndef read_xray(path, voi_lut = True, fix_monochrome = True):\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    #data = (data * 255).astype(np.uint8)\n        \n    return data\n\ndef get_prediction_per_case(patient):\n    \n    path = f'..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/test\/{patient}\/FLAIR\/'\n\n    list_subfolders_with_paths = [f for f in os.listdir(path)]\n    \n    prediction = []\n    \n    for images in list_subfolders_with_paths:\n        \n            \n        img = read_xray(path+images)\n                       \n        if np.max(img) > 0 and np.mean(img)>= 0.015:\n                \n                \n            img =  cv2.resize(img,(IM_SIZE,IM_SIZE))\n            \n            img = cv2.merge((img,img,img))\n            img = tf.reshape(img, (-1, IM_SIZE, IM_SIZE, 3))\n            \n            pred = model.predict(img)\n            \n            prediction.append(pred)\n    \n    return np.mean(prediction,axis=0)[0][0]","7b5f1752":"get_prediction_per_case('00047')","e8eff64b":"\nmgmt = (mgmt+df['BraTS21ID'].apply(get_prediction_per_case))","042e7ae9":"mgmt=np.array(mgmt)","71ac365b":"submission['MGMT_value']=mgmt\/(5+len(modelfiles))","f2416db7":"import seaborn as sns\nsns.histplot([x[0:4] for x in submission['MGMT_value'].astype(str)])","6f112045":"sns.distplot(submission['MGMT_value'])","6f96542b":"submission","97b041bb":"submission['MGMT_value'].to_csv('submission.csv')","38ef4a2f":"## Model and training classes","e94c4944":"## Ensemble for submission","16c842f3":"## Use stacked images (3D) and Efficientnet3D model\n\nAcknowledgements:\n\n- https:\/\/www.kaggle.com\/ihelon\/brain-tumor-eda-with-animations-and-modeling\n- https:\/\/www.kaggle.com\/furcifer\/torch-efficientnet3d-for-mri-no-train\n- https:\/\/github.com\/shijianjian\/EfficientNet-PyTorch-3D\n    \n    \nUse models with only one MRI type, then ensemble the 4 models \n","8e8fc46b":"## Some function","4fc8e863":"## Functions to load images","21e72f4f":"## Model","616fcaf3":"## Predict function","1b965395":"## Parameters","79806055":"## train models"}}