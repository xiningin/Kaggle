{"cell_type":{"46a86a98":"code","35cba5b0":"code","bc29901d":"code","5849f4db":"code","ae53dd9e":"code","eea9a057":"code","a7b690fb":"code","3f7597b9":"code","dbc29a7b":"code","ef8aeb6e":"code","e5d02c51":"code","b8d930fa":"code","c09feb91":"code","8e047506":"code","c1990942":"code","6115c825":"code","e9c4076f":"code","957a479a":"code","33ebfc31":"code","c3117791":"code","e0c57ec5":"code","64ea00e0":"code","1ffd6955":"code","9d98b03a":"code","9f06344b":"markdown","653b8b4a":"markdown","990acb23":"markdown","4eac87f2":"markdown","99096dd8":"markdown","1e500f21":"markdown","b6e1755d":"markdown","3f5517b7":"markdown","f021db25":"markdown","d001047f":"markdown","c29d5f25":"markdown","d2c8a2de":"markdown","e5eaac81":"markdown"},"source":{"46a86a98":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","35cba5b0":"datapath = \"\/kaggle\/input\/commonlitreadabilityprize\/\"\nsub_df = pd.read_csv(f\"{datapath}\/sample_submission.csv\")\ntrain_df = pd.read_csv(f\"{datapath}\/train.csv\")\ntest_df = pd.read_csv(f\"{datapath}\/test.csv\")\ntrain_df.shape, test_df.shape, sub_df.shape","bc29901d":"train_df.head()","5849f4db":"test_df.head()","ae53dd9e":"sub_df.head()","eea9a057":"train_df.info()","a7b690fb":"test_df.info()","3f7597b9":"train_df.describe()","dbc29a7b":"train_df['excerpt'].head()","ef8aeb6e":"train_df['excerpt'][0]","e5d02c51":"train_df['target'][0]","b8d930fa":"train_df['word_count'] = train_df['excerpt'].apply(lambda x:len(x.split()))","c09feb91":"train_df['word_count'].min(), train_df['word_count'].max()","8e047506":"train_df['target'].min(), train_df['target'].max()","c1990942":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.linear_model import LinearRegression","6115c825":"vectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(train_df['excerpt'])","e9c4076f":"X_train = vectorizer.fit_transform(train_df.excerpt)\nX_test = vectorizer.transform(test_df['excerpt'])\ny = train_df['target']","957a479a":"LR_tfidf = LinearRegression().fit(X_train, y)\nLR_tfidf.score(X, y)","33ebfc31":"LR_tfidf.coef_, LR_tfidf.intercept_","c3117791":"y_train_pred = LR_tfidf.predict(X_train)","e0c57ec5":"from sklearn import metrics\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y, y_train_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y, y_train_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y, y_train_pred)))","64ea00e0":"y_test_pred = LR_tfidf.predict(X_test)\nsub_df['target'] = y_test_pred","1ffd6955":"sub_df.head()","9d98b03a":"sub_df.to_csv('submission.csv', index=False)","9f06344b":"### In this competition we are going to find the readability score for each and every excerpt from test set. \nI generally like to ask why, where the solution is used. So to uderstand the **why** part, we need to understand the problem statement. Here, kaggle wants us to find out the ease of reading score for each and every text so that it becomes easy for non native speakers, non scientific people, business decision makers who are non technical etc. \n\nThe traditional methods like **Flesh-kincaid-score** is based on weak proxies of text decoding and other metircs like **Lexile** is paid one.\n\nHere i am going to provide EDA and baseline solution using traditional Machine Learning methods.","653b8b4a":"1. TFIDF + Libear Regression\n2. TFIDF + Libear Regression + stopwords removal","990acb23":"### understanding target","4eac87f2":"Here the target is nothing but reading ease, we need to predict the score for each exceprt in the train and test dataset, the reading ease. Here the target variable show the readability lavel. The higher the score the easier to read and lower the score the difficult it becomes.","99096dd8":"### Check the train data ('excerpt') column","1e500f21":"### Machine Learning Model Building","b6e1755d":"### Import Libraries","3f5517b7":"So here we have **excerpts\" with a minimum of 135 tokens and a maximum of 205 tokens","f021db25":"### Read Data","d001047f":"#### check the training and predict scores","c29d5f25":"### Lets check the excerpt","d2c8a2de":"### Submission","e5eaac81":"### Details about the dataset"}}