{"cell_type":{"57b96914":"code","35bb5dfc":"code","2cdb09c2":"code","58205231":"code","4e27e237":"code","5a4298ce":"code","1a5942a1":"code","8af08038":"code","6901908e":"code","b1d9a998":"code","ebd24081":"code","919003a7":"code","7890fc79":"code","2ca74352":"code","c7d2ffba":"code","dfe6edbe":"code","9f96a5b3":"code","48aaf512":"code","cbd746c1":"code","b5885920":"code","cddfa5ce":"code","20796836":"code","38deff23":"code","9599974b":"code","a657acbb":"code","25bd6a17":"code","5845f933":"code","3e78f184":"code","f638cca0":"code","c2c53f7a":"code","5e23822f":"code","9b2d090c":"code","61465492":"code","abebe365":"code","6a872e73":"code","5321d566":"code","255b5b8c":"code","24029a1b":"code","f3ae39b7":"code","9472c915":"code","4fd418f8":"code","467e1da1":"code","7ee408ec":"code","f1fc54d2":"code","a9b912f9":"code","683aa31d":"code","cf70a440":"code","8f67dff2":"code","67a3dbee":"code","91d217a2":"code","22b5c018":"code","1266f4cc":"code","f089079a":"code","25a0bf89":"code","d0ba25b1":"code","bcf6f83f":"code","052283a3":"code","a699c97d":"code","bc6abc64":"code","82d7f1da":"code","48f71c54":"code","6847bf2f":"code","f9389725":"code","6a5bba24":"code","29477afd":"code","183348f9":"code","128851b2":"code","3da8fd28":"code","397e636b":"code","6adfcd2d":"code","07134c2e":"code","f46fa1dd":"code","43bfc3ef":"code","a4235718":"code","52308ba0":"code","cee8bd94":"code","1d56110d":"code","5a10960c":"code","209f10b3":"code","b731122a":"code","e3493260":"code","55389454":"code","b4702d53":"code","38046dd1":"code","bc743acf":"code","09fa1541":"code","228b02b8":"code","bef44e66":"code","c06fbd3f":"code","612eb9e4":"code","e0c3f346":"code","18153a17":"code","89c5aa37":"code","4250c51c":"code","0d884a44":"code","5098e44d":"code","d902cb0d":"code","dabaf1e8":"code","cf5c25a4":"code","90b62444":"code","4414a628":"code","8dc88b02":"code","5ce095d6":"code","26740b7f":"code","f45bd088":"code","af641eba":"code","353c5f0a":"code","d4b1137b":"code","6343414d":"code","2105a6d9":"code","32675539":"markdown","250a09e2":"markdown","6a4d84f3":"markdown","cb5ba0c3":"markdown","a6348568":"markdown","e24da592":"markdown","57ebc9b3":"markdown","f864c5d9":"markdown","14834375":"markdown","4828d9d3":"markdown","5dd04cce":"markdown","975e8879":"markdown","9d1cb55e":"markdown","21472665":"markdown","01c9a2e8":"markdown","8082c65d":"markdown","3fb40b86":"markdown","0b74d683":"markdown","eaa4d1e6":"markdown","a9774cc4":"markdown","18113bde":"markdown","00aad5bd":"markdown","f96ce956":"markdown","dfa2afbb":"markdown","1261375c":"markdown","ecdd2fc2":"markdown","b0734759":"markdown","91f63bed":"markdown","61abe2f2":"markdown","14bc46a1":"markdown","2aa00b3f":"markdown","62f5af0c":"markdown","2342b344":"markdown","2bfc54aa":"markdown","e8f482bf":"markdown","43e117de":"markdown","a5006b70":"markdown","8c75ca33":"markdown","0c5b4d8a":"markdown","311a2f0a":"markdown","e249d614":"markdown","68af8b93":"markdown","92da6193":"markdown","66994abd":"markdown","eb2b0135":"markdown","e7acb32a":"markdown","0b703811":"markdown","5c4ad8d8":"markdown","790383a8":"markdown","3c0d6f63":"markdown","a51d3ac7":"markdown","bb01a82c":"markdown","d48f4447":"markdown","b7f4d454":"markdown","f5d3b0d2":"markdown"},"source":{"57b96914":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.feature_selection import VarianceThreshold, SelectPercentile, f_classif, chi2, mutual_info_classif\n\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\n\nfrom sklearn.metrics import make_scorer, fbeta_score, confusion_matrix, classification_report, roc_curve, auc\n\nfrom lime import lime_tabular\nimport random","35bb5dfc":"df = pd.read_csv(\"..\/input\/mushroom-classification\/mushrooms.csv\")","2cdb09c2":"df.head()","58205231":"df.shape","4e27e237":"# Lets see the unique values in the datafrmame\nprint(\"Unique values:\")\nfor col in df.columns:\n    print(col + \":\", df[col].unique())","5a4298ce":"df.info()","1a5942a1":"# Checking for missing values\ndf.isnull().sum()","8af08038":"df.describe()","6901908e":"# Checking for duplicate samples\ndf[df.duplicated()].shape[0]","b1d9a998":"# Count-plots all of the features in the dataframe\ndef features_countplot(df):\n    fig, axs = plt.subplots(8, 3, figsize=(14,30), constrained_layout=True)\n    for i, f in enumerate(df.columns):\n        sb.countplot(x=f, data=df, ax=axs[i\/\/3][i%3])\n    plt.show()","ebd24081":"features_countplot(df)","919003a7":"# Prints a frequency of values under a threshold\ndef values_freq(df, n):\n    print(\"Number of values with frequency of less than\", str(n) + \":\")\n    for col in df.columns:\n        for val in list(df[col].unique()):\n            if df[col][df[col] == val].count() < n:\n                print(col, \"of type\", val + \":\", df[col][df[col] == val].count())","7890fc79":"values_freq(df, 10)","2ca74352":"# As we've noticed above, there's only 1 veil-type so we can drop the whole feature out\ndf.drop(\"veil-type\", axis=1, inplace=True)","c7d2ffba":"# Gets a OneHotEncoder matrix and returns it as a dataframe with the dummy columns of df\ndef ohe_to_df(ohe, df):\n    columns = pd.get_dummies(df, drop_first=True).columns\n    ohe_df = pd.DataFrame(ohe, columns=columns[1:]).astype(\"int\")\n    ohe_df.insert(0, \"class\", df[\"class\"])\n    ohe_df[\"class\"].replace({\"e\":1, \"p\":0}, inplace=True)\n    return ohe_df","dfe6edbe":"df2_encoder = OneHotEncoder(sparse=False, drop=\"first\").fit(df.drop(\"class\", axis=1))\ndf2 = ohe_to_df(df2_encoder.transform(df.drop(\"class\", axis=1)), df)\ndf2.head()","9f96a5b3":"plt.figure(figsize=(22,22))\nsb.heatmap(df2.corr(), cmap='coolwarm')","48aaf512":"# Prints pairs of features with a correlation that is greater than or equal to the given coefficient (in absolute value)\ndef df_corr_coeff(df, coeff):\n    table = []\n    upper = df.corr().where(np.triu(np.ones(df.corr().shape), k=1).astype(np.bool))\n    for col in df.corr().columns:\n        for i, val in enumerate(list(upper[col].dropna().values)):\n            if abs(val) >= coeff:\n                table.append((upper[col].dropna().name, df.corr().columns[i], val))\n    table = pd.DataFrame(table, columns=[\"Feature 1\", \"Feature 2\", \"Correlation\"])\n    return table","cbd746c1":"df_corr_coeff(df2, 0.9)","b5885920":"# Deletes a feature for every pair of perfectly correlated features (disregarding the target feature)\ndef del_perfect_corr(df):\n    \n    features = df_corr_coeff(df, 1)\n    if features.shape[0] == 0:\n        return df\n    features_groups = [{features[\"Feature 1\"][0], features[\"Feature 2\"][0]}]\n    for i in range(1, features.shape[0]):\n        if features[\"Feature 1\"][i] not in set.union(*features_groups) and features[\"Feature 2\"][i] not in set.union(*features_groups):\n            features_groups.append({features[\"Feature 1\"][i], features[\"Feature 2\"][i]})\n        elif features[\"Feature 1\"][i] in set.union(*features_groups) and features[\"Feature 2\"][i] in set.union(*features_groups):\n            continue\n        else:\n            for group in features_groups:\n                if features[\"Feature 1\"][i] in group or features[\"Feature 2\"][i] in group:\n                    group.add(features[\"Feature 1\"][i])\n                    group.add(features[\"Feature 2\"][i])\n                    break\n    for group in features_groups:\n        group.pop()\n        df = df.drop(group, axis=1)        \n    return df","cddfa5ce":"# In every group of perfectly correlated features we want to drop from the dataframe all features but one\ndf2 = del_perfect_corr(df2)","20796836":"X = df2.drop(\"class\", axis=1)\ny = df2[\"class\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)","38deff23":"# Runs classification models using GridSearchCV or RandomizedSearhCV and outputs their best estimators and a table with their best scores and parameters\ndef run_models(X, y, searchCV):\n    \n    grid_params = {\"Logistic Regression\": {\"model\": LogisticRegression(), \"params\": {\"C\": [.1, 1, 8, 9, 10], \"max_iter\": [300], \"solver\": [\"lbfgs\", \"liblinear\"]}},\n                     \"SGD\": {\"model\": SGDClassifier(), \"params\": {\"loss\": [\"modified_huber\"]}},\n                     \"KNN\": {\"model\": KNeighborsClassifier(), \"params\": {\"n_neighbors\": range(1,10), \"weights\": [\"uniform\", \"distance\"]}},\n                     \"SVM\": {\"model\": SVC(), \"params\": {\"C\": [.01, .1, .5, 1, 5, 10], \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]}},\n                     \"Naive Bayes\": {\"model\": GaussianNB(), \"params\": {}},\n                     \"Decision Tree\": {\"model\": DecisionTreeClassifier(), \"params\": {\"criterion\": [\"gini\", \"entropy\"]}},\n                     \"Random Forest\": {\"model\": RandomForestClassifier(), \"params\": {\"n_estimators\": [50, 100, 200], \"criterion\": [\"gini\", \"entropy\"]}}}\n    scores = []\n    estimators = {}\n    for model, mp in grid_params.items():\n        clf = searchCV(mp[\"model\"], mp[\"params\"], scoring=make_scorer(fbeta_score, beta=.5), n_jobs=-1, cv=5)\n        clf.fit(X, y)\n        scores.append({\"Model\": model, \"Best Score\": clf.best_score_, \"Best Parameters\": clf.best_params_})\n        estimators[model] = clf.best_estimator_\n    print(pd.DataFrame(scores, columns=[\"Model\", \"Best Score\", \"Best Parameters\"]).to_string())\n    return estimators","9599974b":"estimators = run_models(X_train, y_train, GridSearchCV)","a657acbb":"# Gets a dictionary of estimators and returns a corresponding dictionary of predictions over a test set\ndef pred_dict(estimators, X_test):\n    return dict(zip(estimators.keys(), [x.predict(X_test) for x in estimators.values()]))","25bd6a17":"predictions = pred_dict(estimators, X_test)","5845f933":"# Plots confusion matrices for a dictionary of estimators\ndef confusion_mat(y_test, predictions):\n    plt.figure(figsize=(18,12))\n    plt.suptitle(\"Confusion Matrices\",fontsize=30)\n    plt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n    \n    for i, model in enumerate(predictions.keys()):\n        plt.subplot(3,3,i + 1)\n        plt.title(model, fontsize=15, pad=10)\n        sb.heatmap(confusion_matrix(y_test, predictions[model]), annot=True, cmap=\"Blues\", \n                   fmt=\"d\",cbar=False, annot_kws={\"size\": 25})\n    plt.show()","3e78f184":"# Prints classification reports for a dictionary of estimators\ndef classification_rep(y_test, predictions):\n    print(\"Classification Reports\".center(125, \"=\"))\n    for i, model in enumerate(predictions.keys()):\n        print(model.center(55, \" \"))\n        print(classification_report(y_test, predictions[model]))\n        print(\"*\" * 54)","f638cca0":"# Plots the distribution of the features in the target\ndef feature_target_dist(df):\n    fig, axs = plt.subplots(7, 3, figsize=(14,30), constrained_layout=True)\n    for i, f in enumerate(df.drop(\"class\", axis=1).columns):\n        sb.countplot(x=f, hue=\"class\", data=df, ax=axs[i\/\/3][i%3])\n    plt.show()","c2c53f7a":"feature_target_dist(df)","5e23822f":"# Prints the frequency of a given feature of a dataframe when it's grouped by the target\ndef feature_freq(df, feature):\n    df = pd.get_dummies(df[[\"class\", feature]], columns=[feature])\n    for f in df.drop(\"class\", axis=1).columns:\n        fd = pd.DataFrame(df[[\"class\", f]][df[f] == 1].groupby(\"class\").count())\n        fd[\"percent\"] = fd[f] \/ fd[f].sum() * 100\n        print(fd)\n        print(\"*************************\")","9b2d090c":"feature_freq(df, \"odor\")","61465492":"# Plots statistical tests between the feature matrix X and the target vector y for n features with the highest scores\ndef stat_scores(X, y, n):\n    \n    fc = f_classif(X, y)\n    chi = chi2(X, y)\n    mi = mutual_info_classif(X, y, random_state=10)\n    rfi = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=10).fit(X, y).feature_importances_\n    eti = ExtraTreesClassifier(n_estimators=100, random_state=10).fit(X, y).feature_importances_\n    \n    plt.title(\"ANOVA F-value stats\")\n    plot_feature_rank(X, fc[0], n)\n    \n    plt.title(\"Chi-squared stats\")\n    plot_feature_rank(X, chi[0], n)\n    \n    plt.title(\"Mutual Information stats\")\n    plot_feature_rank(X, mi, n)\n    \n    plt.title(\"Random Forest Importance\")\n    plot_feature_rank(X, rfi, n)\n    \n    plt.title(\"Extra Trees Importance\")\n    plot_feature_rank(X, eti, n)","abebe365":"# Plots a chart of n important features from X using stat test\ndef plot_feature_rank(X, stat, n):\n    pd.Series(stat).sort_values(ascending=False).nlargest(n).plot.bar(figsize = (16,4))\n    plt.xticks(range(0,n), X.columns[pd.Series(stat).sort_values(ascending=False).nlargest(n).index], rotation=20)\n    plt.show()","6a872e73":"stat_scores(X_train, y_train, 10)","5321d566":"# Explaining the model with LIME by locally choosing a random test sample (DOES NOT WORK FOR SVM)\ndef lime_exp(X_train, X_test, model, n=10):\n    explainer = lime_tabular.LimeTabularExplainer(X_train.values, \n                                                  feature_names=X_train.columns)\n    exp = explainer.explain_instance(X_test.iloc[random.randint(0, len(X_test) - 1)], \n                                     model.predict_proba, num_features=n)\n    exp.show_in_notebook()","255b5b8c":"lime_exp(X_train, X_test, estimators[\"KNN\"])","24029a1b":"original_df3 = df.drop(\"odor\", axis=1)\n\ndf3_encoder = OneHotEncoder(sparse=False, drop=\"first\").fit(original_df3.drop(\"class\", axis=1))\ndf3 = ohe_to_df(df3_encoder.transform(original_df3.drop(\"class\", axis=1)), original_df3)\n\ndf3 = del_perfect_corr(df3)\n\nX2 = df3.drop(\"class\", axis=1)\ny2 = df3[\"class\"]\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, random_state=10)\n\nestimators2 = run_models(X_train2, y_train2, GridSearchCV)","f3ae39b7":"original_df4 = df[[\"class\", \"odor\"]]\n\ndf4_encoder = OneHotEncoder(sparse=False, drop=\"first\").fit(original_df4.drop(\"class\", axis=1))\ndf4 = ohe_to_df(df4_encoder.transform(original_df4.drop(\"class\", axis=1)), original_df4)\n\ndf4 = del_perfect_corr(df4)","9472c915":"plt.figure(figsize=(15,15))\nsb.heatmap(df4.corr(), cmap='coolwarm', annot=True, vmin=-1, annot_kws={\"fontsize\":15})","4fd418f8":"X3 = df4.drop(\"class\", axis=1)\ny3 = df4[\"class\"]\nX_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3, test_size=0.3, random_state=10)\n\nestimators3 = run_models(X_train3, y_train3, GridSearchCV)","467e1da1":"original_df5 = df[[\"class\"] + [i for i in df.columns if \"color\" in i]]\n\ndf5_encoder = OneHotEncoder(sparse=False, drop=\"first\").fit(original_df5.drop(\"class\", axis=1))\ndf5 = ohe_to_df(df5_encoder.transform(original_df5.drop(\"class\", axis=1)), original_df5)\n\ndf5 = del_perfect_corr(df5)","7ee408ec":"plt.figure(figsize=(22,22))\nsb.heatmap(df5.corr(), cmap='coolwarm')","f1fc54d2":"X4 = df5.drop(\"class\", axis=1)\ny4 = df5[\"class\"]\nX_train4, X_test4, y_train4, y_test4 = train_test_split(X4, y4, test_size=0.3, random_state=10)\n\nestimators4 = run_models(X_train4, y_train4, GridSearchCV)","a9b912f9":"# Runs a variance threshold algorithm and returns the features with a variance that is lower than the threshold\ndef var_threshold(X, threshold):\n    vt = VarianceThreshold(threshold)\n    vt.fit(X)\n    low_var_features = [x for x in X.columns if x not in X.columns[vt.get_support()]]\n    print(\"Feaures with variance of less than \" + str(threshold) + \":\", \", \".join(low_var_features))\n    print(\"Number of remaining features:\", sum(vt.get_support()))\n    return low_var_features","683aa31d":"# Runs the KNN model and plots a chart of K values from 1 to n with their scores\ndef best_knn(X, y, n):\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n    scoreList = []\n    for i in range(1,n + 1):\n        knn = GridSearchCV(KNeighborsClassifier(n_neighbors = i, n_jobs=-1), {\"weights\": [\"uniform\", \"distance\"]}, scoring=make_scorer(fbeta_score, beta=.5), cv=5)\n        knn.fit(X_train, y_train)\n        scoreList.append(knn.best_score_)\n    plt.plot(range(1,n + 1), scoreList)\n    plt.xticks(np.arange(1,n + 1,1))\n    plt.xlabel(\"K value\")\n    plt.ylabel(\"Score\")\n    plt.show()\n    print(\"Maximum KNN Score:\", max(scoreList), \"for K =\", scoreList.index(max(scoreList)) + 1)","cf70a440":"# Fixes the multicolinearity of a dataframe by dropping features with high Variance Inflation Factor (VIF) until all features have a VIF of less than n. Returns the updated dataframe and the VIF dataframe with VIF values of the remaining features\ndef fix_multicolinearity(df, n):\n    \n    vif = pd.DataFrame()\n    vif[\"features\"] = df.columns\n    vif[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n    while any(i > n for i in vif[\"VIF\"].values):\n        for f in range(1, df.shape[1]):\n            if vif.iloc[f][1] > n:\n                df = df.drop(vif.iloc[f][0], axis=1)\n                break\n        vif = pd.DataFrame()\n        vif[\"features\"] = df.columns\n        vif[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n    return df, vif","8f67dff2":"X4_1 = X4.drop(var_threshold(X4, 0.005), axis=1)\nX_train4_1, X_test4_1, y_train4_1, y_test4_1 = train_test_split(X4_1, y4, test_size=0.3, random_state=10)\nestimators4_1 = run_models(X_train4_1, y_train4_1, GridSearchCV)","67a3dbee":"X4_2 = SelectPercentile(chi2, percentile=90).fit_transform(X4, y4)\nX_train4_2, X_test4_2, y_train4_2, y_test4_2 = train_test_split(X4_2, y4, test_size=0.3, random_state=10)\nestimators4_2 = run_models(X_train4_2, y_train4_2, GridSearchCV)","91d217a2":"best_knn(X4, y4, 30)","22b5c018":"new_df5, df5_vif = fix_multicolinearity(df5, 10)\nX4_3 = new_df5.drop(\"class\", axis=1)\ny4_3 = new_df5[\"class\"]\nX_train4_3, X_test4_3, y_train4_3, y_test4_3 = train_test_split(X4_3, y4_3, test_size=0.3, random_state=10)\n\nestimators4_3 = run_models(X_train4_3, y_train4_3, GridSearchCV)","1266f4cc":"predictions4 = pred_dict(estimators4, X_test4)","f089079a":"confusion_mat(y_test4, predictions4)","25a0bf89":"predictions4_3 = pred_dict(estimators4_3, X_test4_3)","d0ba25b1":"confusion_mat(y_test4_3, predictions4_3)","bcf6f83f":"# Trains a model using KFold cross-validation on X and y and returns a dataframe with the labels and concatinated predict_probas for each split. The idea is to use it to find an optimal threshold without an overfitting risk on the test set\ndef kfold_predict_proba(X, y, model):\n    kf = KFold(n_splits=5, shuffle=True, random_state=10)\n    clf = model\n    pp_df = pd.DataFrame(columns=[\"Label\", \"Predict_proba\"])\n    for train_idx, test_idx in kf.split(X):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n        clf.fit(X_train, y_train)\n        pp_df = pd.concat([pp_df, pd.DataFrame({\"Label\": y_test, \"Predict_proba\": clf.predict_proba(X_test)[:,1]})], axis=0)\n    return pp_df","052283a3":"# Plots ROC curves of estimators using K-fold predcit_proba\ndef roc_curves(X, y, estimators):\n    \n    estimators2 = dict(estimators)\n    estimators2.pop(\"SVM\")\n    fig, axs = plt.subplots(2, 3, figsize=(15,9), constrained_layout=True)\n    for i, clf in enumerate(estimators2.values()):\n        clf_pp = kfold_predict_proba(X, y, clf)\n        label = clf_pp[\"Label\"].tolist()\n        pp = clf_pp[\"Predict_proba\"].tolist()\n        fpr, tpr, thresholds = roc_curve(label, pp)\n        area = auc(fpr, tpr)\n        axs[i\/\/3][i%3].plot(fpr, tpr, linestyle=\"-\", label='ROC curve (area = %0.2f)' % area)\n        axs[i\/\/3][i%3].set(xlabel=\"False Positive Rate\", ylabel=\"True Positive Rate\", title=list(estimators2.keys())[i])\n        axs[i\/\/3][i%3].legend()\n    fig.suptitle(\"ROC Curves\", fontsize=20)\n    plt.show()","a699c97d":"# Gets a dictionary of trained estimators and returns a dictionary of the same estimators but untrained and with the same parameters\ndef untrain_models(estimators):\n    new_clfs = []\n    for clf in estimators.values():\n        new_clfs.append(eval(type(clf).__name__)().set_params(**clf.get_params()))\n    return dict(zip(estimators.keys(), new_clfs))","bc6abc64":"# Given lists of predict_proba and labels, returns the best threshold under f_0.5 scoring\ndef best_threshold(pred_df):\n    label = pred_df[\"Label\"].tolist()\n    pred_proba = pred_df[\"Predict_proba\"].tolist()\n    fpr, tpr, thresholds = roc_curve(label, pred_proba)\n    scores = np.array([fbeta_score(label, pred_proba >= thr, beta=.5) for thr in thresholds])\n    return thresholds[scores.argmax()]","82d7f1da":"# Gets a dictionary of optimal thresholds for given estimators and X, y train sets\ndef best_thresholds_dict(X, y, estimators):\n    thresholds = []\n    for clf in untrain_models(estimators).values():\n        if type(clf).__name__ == \"SVC\":\n            thresholds.append(0.5)\n        else:\n            thresholds.append(best_threshold(kfold_predict_proba(X, y, clf)))\n    return dict(zip(estimators.keys(), thresholds))","48f71c54":"# Model prediction by a given decision threshold\ndef predict_by_threshold(model, threshold, test):\n    if type(model).__name__ == \"SVC\":\n        return model.predict(test)\n    return (model.predict_proba(test)[:,1] >= threshold).astype(\"int\")","6847bf2f":"# Returns a dictionary of models predictions by the corresponding decision thresholds\ndef pred_by_threshold_dict(estimators, thresholds, test):\n    preds = []\n    for i, clf in enumerate(estimators.values()):\n        preds.append(predict_by_threshold(clf, thresholds[list(thresholds.keys())[i]], test))\n    return dict(zip(estimators.keys(), preds))","f9389725":"thresholds4 = best_thresholds_dict(X_train4, y_train4, estimators4)","6a5bba24":"predictions4 = pred_by_threshold_dict(estimators4, thresholds4, X_test4)","29477afd":"confusion_mat(y_test4, predictions4)","183348f9":"classification_rep(y_test4, predictions4)","128851b2":"colors_clf = estimators4[\"Random Forest\"]\ncolors_threshold = thresholds4[\"Random Forest\"]","3da8fd28":"# Returns the indices of the removed columns during the preprocessing stage\ndef removed_indices(original_df, df):\n    cols = pd.get_dummies(original_df.drop(\"class\", axis=1), drop_first=True).columns\n    return [cols.tolist().index(x) for x in cols if x not in df.columns]","397e636b":"# Gets a dataframe and its deleted indices during the preprocessing and returns the updated unique values as a matrix\ndef unique_values2(df, del_idx):\n    \n    unique_mat = []\n    del_col = pd.get_dummies(df.drop(\"class\", axis=1), drop_first=True).columns[del_idx]\n    for f in df.drop(\"class\", axis=1).columns:\n        values = df[f].unique().tolist()\n        for c in del_col:\n            if f == c[:-2]:\n                values.remove(c[-1])\n        unique_mat.append(values)\n    return unique_mat","6adfcd2d":"# Class for the final model we achieved after all the analysis. Gets the estimator, its best decision threshold, the original dataframe, the OHE fitted object of the original dataframe and the indices of the removed columns during the preprocessing\nclass MushroomModel:\n    \n    def __init__(self, estimator, threshold, df, encoder, del_idx):\n        self.estimator = estimator\n        self.threshold = threshold\n        self.df = df\n        self.encoder = encoder\n        self.del_idx = del_idx\n        self.uniques = unique_values2(df, del_idx)\n        \n    def random_predict(self):\n        sample = []\n        for i, col in enumerate(self.df.drop(\"class\", axis=1).columns):\n            sample.append(random.choice(self.uniques[i]))\n            print(col + \":\", sample[-1])\n        return self.predict(sample)\n        \n    def sample_fit(self, sample):\n        sample = self.encoder.transform(np.array(sample).reshape(1,-1))[0]\n        sum_check = sample.sum()\n        sample = np.array([x for i, x in enumerate(sample) if i not in self.del_idx]).reshape(1,-1)\n        if sum_check != sample.sum():\n            raise ValueError(\"Invalid input\")\n        return sample\n    \n    def predict(self, sample):\n        sample = self.sample_fit(sample)\n        est_name = type(self.estimator).__name__\n        if  est_name == \"SVC\" or est_name == \"KMeans\":\n            return self.estimator.predict(sample)\n        return (self.estimator.predict_proba(sample)[:,1] >= self.threshold).astype(\"int\")","07134c2e":"colors_model = MushroomModel(colors_clf, colors_threshold, original_df5, df5_encoder, removed_indices(original_df5, df5))","f46fa1dd":"sample = ['n','k','n','n','w','k']","43bfc3ef":"colors_model.predict(sample)","a4235718":"lime_exp(X_train4, pd.DataFrame(colors_model.sample_fit(sample)), colors_clf)","52308ba0":"colors_model.random_predict()","cee8bd94":"lime_exp(X_train4, X_test4, colors_clf)","1d56110d":"original_df6 = df[[\"class\", \"population\", \"habitat\"]]\n\ndf6_encoder = OneHotEncoder(sparse=False, drop=\"first\").fit(original_df6.drop(\"class\", axis=1))\ndf6 = ohe_to_df(df6_encoder.transform(original_df6.drop(\"class\", axis=1)), original_df6)\n\ndf6 = del_perfect_corr(df6)","5a10960c":"plt.figure(figsize=(15,15))\nsb.heatmap(df6.corr(), cmap='coolwarm', annot=True, vmin=-1, annot_kws={\"fontsize\":15})","209f10b3":"X5 = df6.drop(\"class\", axis=1)\ny5 = df6[\"class\"]\nX_train5, X_test5, y_train5, y_test5 = train_test_split(X5, y5, test_size=0.3, random_state=10)\n\nestimators5 = run_models(X_train5, y_train5, GridSearchCV)","b731122a":"X5_1 = X5.drop(var_threshold(X5, 0.05), axis=1)\nX_train5_1, X_test5_1, y_train5_1, y_test5_1 = train_test_split(X5_1, y5, test_size=0.3, random_state=10)\nestimators5_1 = run_models(X_train5_1, y_train5_1, GridSearchCV)","e3493260":"X5_2 = SelectPercentile(chi2, percentile=90).fit_transform(X5_1, y5)\nX_train5_2, X_test5_2, y_train5_2, y_test5_2 = train_test_split(X5_2, y5, test_size=0.3, random_state=10)\nestimators5_2 = run_models(X_train5_2, y_train5_2, GridSearchCV)","55389454":"best_knn(X5_1, y5, 30)","b4702d53":"new_df6, df6_vif = fix_multicolinearity(df6, 10)\nX5_3 = new_df6.drop(\"class\", axis=1).drop(var_threshold(X5, 0.05), axis=1)\ny5_3 = new_df6[\"class\"]\nX_train5_3, X_test5_3, y_train5_3, y_test5_3 = train_test_split(X5_3, y5_3, test_size=0.3, random_state=10)\n\nestimators5_3 = run_models(X_train5_3, y_train5_3, GridSearchCV)","38046dd1":"predictions5 = pred_dict(estimators5, X_test5)","bc743acf":"predictions5_1 = pred_dict(estimators5_1, X_test5_1)","09fa1541":"confusion_mat(y_test5, predictions5)","228b02b8":"confusion_mat(y_test5_1, predictions5_1)","bef44e66":"confusion_mat(y_train5, pred_dict(estimators5, X_train5))","c06fbd3f":"roc_curves(X_train5, y_train5, estimators5)","612eb9e4":"thresholds5 = best_thresholds_dict(X_train5, y_train5, estimators5)","e0c3f346":"predictions5 = pred_by_threshold_dict(estimators5, thresholds5, X_test5)","18153a17":"confusion_mat(y_test5, predictions5)","89c5aa37":"classification_rep(y_test5, predictions5)","4250c51c":"area_clf = estimators5[\"KNN\"]\narea_threshold = thresholds5[\"KNN\"]","0d884a44":"area_model = MushroomModel(area_clf, area_threshold, original_df6, df6_encoder, removed_indices(original_df6, df6))","5098e44d":"area_model.random_predict()","d902cb0d":"sample2 = ['c', 'l']","dabaf1e8":"lime_exp(X_train5, pd.DataFrame(area_model.sample_fit(sample2)), area_clf)","cf5c25a4":"lime_exp(X_train5, X_test5, area_clf)","90b62444":"def xgboost_model(X, y, searchCV):\n    \n    params = {\n        'min_child_weight': [0.1, 0.5, 1, 5],\n        'gamma': [0.01, 0.1, 0.5, 1, 1.5, 2],\n        'subsample': [0.7, 0.8, 0.9],\n        'colsample_bytree': [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 2],\n        'max_depth': [2, 3, 4, 10]\n        }\n    xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic', silent=True, nthread=1)\n    clf = searchCV(xgb, params, scoring=make_scorer(fbeta_score, beta=.5), n_jobs=-1, cv=5)\n    clf.fit(X, y)\n    print(\"XGBoost best score:\", clf.best_score_)\n    print(\"XGBoost best parameters:\", clf.best_params_)\n    return clf.best_estimator_","4414a628":"xgb = xgboost_model(X_train5, y_train5, RandomizedSearchCV)","8dc88b02":"confusion_matrix(y_test5, xgb.predict(X_test5))","5ce095d6":"# Calculates an f_0.5 score between the original labels feature and a given clustering labels vector\ndef clustering_score(labels, cl_labels):\n    cm = confusion_matrix(labels, cl_labels)\n    tp = cm[1][1]\n    fp = cm[0][1]\n    fn = cm[1][0]\n    return (1.25 * tp) \/ (1.25 * tp + 0.25 * fn + fp)","26740b7f":"# Creates clustering models for the dataframe by using 2 clusters that would substitute the original binary target and outputs the clustering labels vectors and a table with the models scores\ndef clustering_model(df):\n    \n    kmeans = KMeans(n_clusters=2, random_state=0).fit(df.drop(\"class\", axis=1))\n    gmm = GaussianMixture(n_components=2, random_state=0).fit(df.drop(\"class\", axis=1))\n    kmeans_pred = kmeans.predict(df.drop(\"class\", axis=1))\n    gmm_pred = gmm.predict(df.drop(\"class\", axis=1))\n    kmeans_score = clustering_score(df[\"class\"], kmeans_pred)\n    gmm_score = clustering_score(df[\"class\"], gmm_pred)\n    estimators = {\"KMeans\": kmeans, \"Gaussian Mixture\": gmm}\n    print(pd.DataFrame([[\"KMeans\", kmeans_score],[\"Gaussian Mixture\", gmm_score]], columns=[\"Model\", \"Score\"]))\n    return estimators","f45bd088":"estimators6 = clustering_model(df2)","af641eba":"predictions6 = pred_dict(estimators6, df2.drop(\"class\", axis=1))","353c5f0a":"confusion_mat(df2[\"class\"], predictions6)","d4b1137b":"classification_rep(df2[\"class\"], predictions6)","6343414d":"clusters_model = MushroomModel(estimators6[\"KMeans\"], 0.5, df, df2_encoder, removed_indices(df, df2))","2105a6d9":"clusters_model.random_predict()","32675539":"Optimizing the decision threshold didn't yield a better model than the KNN but made the Random Forest and Decision Tree match its score, and hence the KNN is the model we end up with.","250a09e2":"# Creating models","6a4d84f3":"We optimized our thresholds by the f_0.5 scoring function to increase the precision in order to decrease the FP, and we got the following results on the test set:\n- The FP of the Logistic Regression, SGD, KNN, Decision Tree and Random Forest were decreased.\n- The FP of the Naive Bayes was increased (apparently because it previously had 100% precision and now we forced some FP).\n- The confusion matrix of the SVM remained the same (because SVM is not a probabilistic algorithm and hence it has no decision threshold).\n\nWe can draw the following conclusions:\n- If we were to seek for the safest model, the previous Naive Bayes has no FP at all, but on the other hand it does have 871 FN, which eventually granted it a very low score. A good candidate for a very safe model would be the current Logistic Regression which has only a few more FP than the previous Naive Bayes but more than 500 FN less.\n- Even though the KNN got the highest score, the Decision Tree and Random Forest were previously very close to it with only a 0.002 difference. Now we can see their current \"thresholded\" versions have 12 more FN but 14 less FP, which to me grants them to be the final model to end up with.","cb5ba0c3":"We can see that removing the 5 features with a variance threshold of less than 0.05 decreases a bit the scores of the Logistic Regression and SGD but improves the KNN, SVM, Decision Tree and Random Forest models to the same score, so as for now each one of them is a candidate to be the model to go with (tested with a variety of thresholds and picked the one that yielded the best score). I think the fact we removed almost half of the features and yet received better models might imply the data set is not very balanced...","a6348568":"# Preprocessing","e24da592":"## Analysis","57ebc9b3":"Fixing the multicolinearity using VIF doesn't improve the models.","f864c5d9":"#### Pairs of highly correlated features:","14834375":"The class distribution is balanced; and since all of the features are categoric there's no need to scale the data.","4828d9d3":"We can see that by removing some features with low variance still none of the models is better than our previous KNN, and therefore removing low variance features is not a good idea here (tested with a variety of thresholds).","5dd04cce":"### Confusion matrices and Classification reports","975e8879":"### Data encoding:","9d1cb55e":"Increasing the n neighbors for the KNN doesn't yield a better KNN.","21472665":"## Take 0: Initial evaluation\n\n### Train-test split","01c9a2e8":"### Attempts to improve the models:\n\nWe will use several ways to try improving the models.","8082c65d":"We get the same confusion matrix as the KNN.","3fb40b86":"Putting a variance threshold like we did increased the FP of the Logistic Regression, SGD, KNN and Naive Bayes but decreased it in the SVM, Decision Tree and Random Forest. On the other hand, the FN of the last ones have been significantly increased. Like before, we can take the previous Naive Bayes as the safest model with only 21 FP. But if we want to choose the model with the least FP and yet with a decent amount of FN we should choose the previous KNN. Lets see how the confusion matrix on the train test looks like.","0b74d683":"# Postprocessing\n\n### Threshold optimization:\n\nIn this section we would want to take our current estimators and optimize their decision thresholds in order to decrease the FP rate. We would do that in 3 steps:\n\n- Creating predict-proba sets by splitting them over the train set using KFold (to avoid over-fitting).\n- Using these sets to calculate f_0.5 scores on all of the possible thresholds of the corresponding untrained estimators (with the same parameters).\n- For each estimator, taking the threshold that yields the highest score.","eaa4d1e6":"## Take 1 (anosmic): Data without the odor feature","a9774cc4":"## Exploratory analysis","18113bde":"# Feature Selection and Distribution\n\nLet's see how the features from the original dataframe are distributed in the target.","00aad5bd":"The models are initialized with a specific random_state to make sure the clustering labels correspond to the original labels (1 to 1 and 0 to 0), and also because it gives them the best score.","f96ce956":"To wrap everything up, I made a class that takes the final model and uses it to make predictions.","dfa2afbb":"## Importing libraries","1261375c":"The confusion matrices of both the fixed multicolinearity KNN and the original KNN are the same, so we'll continue with the previous KNN for now.","ecdd2fc2":"### Attempts to improve the models:","b0734759":"#### Inspecting rare values:","91f63bed":"Well, that's an interesting variation. The KNN model got the highest score, so as for now that's the model to go with.","61abe2f2":"We take the f_beta function with beta = 0.5 as the scoring function to lend more weight to precision because in this dataset it's important to keep the FP low (poisonous mushrooms that were falsely predicted to be edible).","14bc46a1":"### Ending remark\n\nOne could mess with the beta value of the f_beta function a little more to try to get better precision results.","2aa00b3f":"Selecting 90% of the best features doesn't improve our models (tested with a variety of functions and percentiles).","62f5af0c":"We get an interesting observation from the confusion matrices. FP in these models means that a poisonous mushroom was predicted to be edible, so ideally we would want to minimize the FP as much as possible. KNN got the best score overall but its FP is not the lowest. Naive Bayes got the worst score with a huge amount of FN but it has no FP at all, so it's basically the safest model!","2342b344":"We still get a perfect score in most of the models, and that's probably because of other features with high importance. Now, instead of shaving a few more features with high importance or low variance off of the dataframe, we should make it interesting and establish thinner variations of the data set, from which we shall focus on those that don't yield a prefect score but yet a decent one.","2bfc54aa":"### Threshold optimization:","e8f482bf":"Just for the sake of experimenting I also created XGBoost model. ","43e117de":"We can see the 'none odor' has significant dominance compared to the rest of the values, and hence the next step would be to drop the whole feature out and see where it leads us. Before that we just add a small section of LIME interpretability that we will use later on.","a5006b70":"### Interpretability:","8c75ca33":"We see that all of the mushrooms with almond or anise odor are edible; all of the mushrooms with creosote, fishy, foul, musty, pungent and spicy odors are poisonous; and among the mushrooms without an odor almost 97% are edible. So the distribution of the  \"odor\" feature is strongly compatible with the distribution of the target, and apparently it's one of the causes that our models have perfect score. This observation actually draws a simple rule to determine the edibility of a mushroom, as opposed to the aforementioned Guide's statement. Let's run some features importance tests to see it more clearly.","0c5b4d8a":"## Introduction\n\nThis notebook is about a classification of mushrooms into edible and poisonous categories. The data set was taken form the UCI repository, the origin of its records is The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf, and it was donated by Jeff Schlimmer.\n\nThis data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family (pp. 500-525). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like \"leaflets three, let it be\" for Poisonous Oak and Ivy. (from UCI)\n\nThe goal of this notebook is partly to refute the very Guide's statement above by showing how almost all of the classification models get a perfect score. We will see how the reason for the perfect score yields a very simple rule of how to easily classify mushrooms without any need of statistical tools. As a consequence, the primary idea of this notebook is to do some features reduction in order to seek for interesting variations that would allow us to construct and analyze models with imperfect but yet good scores. This notebook describes with comments a step-by-step process of selecting 3 variations of models.","311a2f0a":"The models separate the data almost the same, and unfortunately they both have a high FP rate. The score of the KMeans is a little better so that's the model we end up with.","e249d614":"## Loading data set","68af8b93":"Considering what we've seen above it's not very surprising that we get almost a perfect score (that is also identical across all of the models), and therefore we have no use for this variation.","92da6193":"We can see several features that are distributed in the target unequally, maybe the most prominent is odor.","66994abd":"## Take 5: Unsupervised (clustering)\n\nNow we are back to the original dataframe that we encoded with a OneHotEncoder (df2) but leaving the target feature out this time, and we shall see if dividing the data into 2 clusters using KMeans and Gaussian Mixture (separately) yields a good approximation to the original target.","eb2b0135":"We can see the KNN keeps its high precision on the train set with almost an opposite proportion of FP\/FN compared to the rest of the good models. I also tested it with the variance threshold but it yielded worse results, so meanwhile the original KNN is the best model.","e7acb32a":"Selecting 90% of the best features doesn't improve our models either (tested with a variety of functions and percentiles).","0b703811":"We have no need to use these functions for now because we got a perfect score, so we should further analyse the data to see what causes it.","5c4ad8d8":"### Attribute information (from the UCI page):\n1. cap-shape: bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s\n2. cap-surface: fibrous=f, grooves=g, scaly=y, smooth=s\n3. cap-color: brown=n, buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y\n4. bruises?: bruises=t, no=f\n5. odor: almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m, none=n, pungent=p, spicy=s\n6. gill-attachment: attached=a, descending=d, free=f, notched=n\n7. gill-spacing: close=c, crowded=w, distant=d\n8. gill-size: broad=b, narrow=n\n9. gill-color: black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pink=p, purple=u, red=e, white=w, yellow=y\n10. stalk-shape: enlarging=e, tapering=t\n11. stalk-root: bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r, missing=?\n12. stalk-surface-above-ring: fibrous=f, scaly=y, silky=k, smooth=s\n13. stalk-surface-below-ring: fibrous=f, scaly=y, silky=k, smooth=s\n14. stalk-color-above-ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y\n15. stalk-color-below-ring: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y\n16. veil-type: partial=p, universal=u\n17. veil-color: brown=n, orange=o, white=w, yellow=y\n18. ring-number: none=n, one=o, two=t\n19. ring-type: cobwebby=c, evanescent=e, flaring=f, large=l, none=n, pendant=p, sheathing=s, zone=z\n20. spore-print-color: black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, purple=u, white=w, yellow=y\n21. population: abundant=a, clustered=c, numerous=n, scattered=s, several=v, solitary=y\n22. habitat: grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w, woods=d","790383a8":"Increasing the n neighbors for the KNN doesn't improve it.","3c0d6f63":"## Take 4: Area (population + habitat)","a51d3ac7":"## Take 2 (anti-anosmic): Data with only the odor feature","bb01a82c":"## Take 3: Colors only","d48f4447":"Another interseting variation that is very thin in terms of features and yet yields not very bad scores. We shall go through the whole process like we did in the last variation, and meanwhile the candidate for the best model is the SVM.","b7f4d454":"Fixing the multicolinearity using VIF does improve the KNN by a bit.","f5d3b0d2":"#### Missing values (compared to the UCI attribute information):\n\n- gill-attachment: d, n.\n- gill-spacing: d.\n- stalk-root: u, z.\n- veil-type: u.\n- ring-type: c, s, z."}}