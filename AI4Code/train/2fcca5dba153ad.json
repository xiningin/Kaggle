{"cell_type":{"7dfb50d2":"code","1ded2ade":"code","feab52d9":"code","f3377f63":"code","2a8f7128":"code","e8d15cdb":"code","48e091a7":"code","821cebb0":"code","65b640ea":"code","0d8df627":"code","c356483d":"code","17289ce3":"code","4854e96d":"code","d14eba8e":"code","3259b353":"code","832e79e8":"code","248e098b":"code","0e427089":"code","8560a34f":"code","3d495a02":"code","b1030004":"markdown","fbf5931f":"markdown","ec1f6923":"markdown","0ceb6c9f":"markdown","81fa19f9":"markdown","ef2f48fb":"markdown","4704ac78":"markdown","1dadac4c":"markdown","accebb5a":"markdown","1dbc46d2":"markdown","3af89536":"markdown","6e0bba5a":"markdown","5822b6b5":"markdown","a73f87f5":"markdown","cf5e925e":"markdown","5d558709":"markdown"},"source":{"7dfb50d2":"import numpy as np\nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier\nimport matplotlib.pyplot as plt","1ded2ade":"!ls ..\/input","feab52d9":"df_train = pd.read_csv('..\/input\/train.csv',index_col='Id')\ndf_test = pd.read_csv('..\/input\/testX.csv',index_col='Id')\ndf_train.head()","f3377f63":"from sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=20)","2a8f7128":"clf.fit(df_train[['x1','x2']],df_train.y)","e8d15cdb":"vorhersage = clf.predict(df_test)","48e091a7":"df_out = pd.Series(vorhersage,name='y')\ndf_out.index.name='Id'\ndf_out.to_csv('submission.csv',header=True)","821cebb0":"def visualize_class_boundaries(Xtrain,ytrain,k=20,figsize=(5,5),fig=None,title=''):\n    \"\"\"\n    Diese Funktion visualisiert die Vorhersagegrenzen f\u00fcr einen k-NN-Klassifikator auf \n    diesem Datensatz. Es wird ein k-NN-Klassifikator auf den Daten Xtrain, ytrain trainiert und anschliessend \n    die Trainingsdatenpunkte sowie die Klassengrenzen visualisiert. Voraussetzung ist, dass der Datensatz \n    2-dimensional ist, d.h. Xtrain.shape[1] muss den Wert 2 haben.\n    \"\"\"\n    #Generiere das Gitter: Alle Datenpunkte {(x,y) f\u00fcr 0<x,y<1}\n    h = .002  # Maschenabstand\n    x_min = 0\n    y_min = 0\n    x_max = 1\n    y_max = 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    \n    if not fig:\n        fig = plt.figure(1,figsize=figsize)\n    clf = KNeighborsClassifier(n_neighbors=k)\n    clf.fit(Xtrain,ytrain);\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,cmap='bwr');\n    plt.contourf(xx, yy, Z,alpha=0.1,cmap='bwr');\n    plt.plot(Xtrain[ytrain==0,0],Xtrain[ytrain==0,1],\n             marker='.',ls='none',c='b');\n    plt.plot(Xtrain[ytrain==1,0],Xtrain[ytrain==1,1],\n             marker='+',ls='none',c='r');\n    plt.xlabel('x')\n    plt.ylabel('y');\n    if not title:\n        plt.title(f'k={k} n\u00e4chste Nachbarn')\n    else:\n        plt.title(title)","65b640ea":"Xtrain = df_train[['x1','x2']].values\nytrain=df_train['y'].values\n#Visualisiere die Klassengrenzen auf den Trainingsdaten:\nvisualize_class_boundaries(Xtrain,ytrain,k=2)  # <-- HIER DEN WERT VON k (durch Ausprobieren) OPTIMIEREN","0d8df627":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nclf = KNeighborsClassifier()\ngs = GridSearchCV(clf,{'n_neighbors':[1,2,3,4,5,6,7,8,9,10,20,50]},#die m\u00f6glichen Werte die durchprobiert werden\n                  cv=10, #Teile den Datensatz (10 mal) in 10 Subsets. Trainiere auf 9, validiere auf dem letzen.\n                  error_score=np.nan #Sollte ein fit fehlschlagen, bewerte Ihn mit Nan. Dies unterdr\u00fcckt l\u00e4stige eine Warnung.\n                 )","c356483d":"#In Scikit-Learn lassen sich leicht andere Klassifikatoren ausprobieren. F\u00fcr einen Entscheidungsbaum einfach \n#die folgenden zwei Zeilen entkommentieren und (als \u00dcbung) die Fehlermeldungen korrigieren, die entstehen, weil\n#der Hyperparameter nun nicht mehr `n_neighbors`, sondern `max_depth` heisst. \n#clf = DecisionTreeClassifier()\n#gs = GridSearchCV(clf,{'max_depth':[1,2,3,4,5,6,7,20]},cv=10,error_score=np.nan)","17289ce3":"gs.fit(df_train[['x1','x2']],df_train.y)","4854e96d":"df = pd.DataFrame(gs.cv_results_)\ndf.head()","d14eba8e":"ax = df.plot(x='param_n_neighbors',y='mean_train_score')\ndf.plot(x='param_n_neighbors',y='mean_test_score',ax=ax,title='Parameterkurven f\u00fcr kNN',xlim=[0,20]);","3259b353":"#Extrahieren den besten im Grid-Search gefundenen Wert f\u00fcr den Hyperparameter n_neighbours\nk_best = gs.best_params_['n_neighbors']\nk_best","832e79e8":"visualize_class_boundaries(Xtrain,ytrain,k=k_best)  # <-- HIER DEN WERT VON k (durch Ausprobieren) OPTIMIEREN","248e098b":"#Entkommentieren f\u00fcr Entscheidungsbaum: (L\u00f6sung der \u00dcbung):\n\"\"\"\nclf = DecisionTreeClassifier()\ngs = GridSearchCV(clf,{'max_depth':[1,2,3,4,5,6,7,20]},cv=10,error_score=np.nan)\ngs.fit(df_train[['x1','x2']],df_train.y)\ndf = pd.DataFrame(gs.cv_results_)\nfig,axes = plt.subplots(1,2,figsize=(15,5))\ndf.plot(x='param_max_depth',y='mean_train_score',title='Parameterkurven f\u00fcr DecisionTree',xlim=[0,20],ax=axes[0])\ndf.plot(x='param_max_depth',y='mean_test_score',title='Parameterkurven f\u00fcr DecisionTree',xlim=[0,20],ax=axes[0]);\nmax_depth_best =gs.best_params_['max_depth']\nvisualize_class_boundaries(Xtrain,ytrain,k=max_depth_best,title=f'DecisionTree (max_depth={max_depth_best})')  \n\"\"\";","0e427089":"clf = gs.best_estimator_","8560a34f":"yhat = clf.predict(df_test)\nsubmission = pd.Series(yhat,name='y')\nsubmission.index.name='Id'\nsubmission.to_csv('submission.csv',header=True)\nsubmission.head()","3d495a02":"!head submission.csv","b1030004":"# Teilnahme an Kaggle Competition\nDie folgenden Zeilen zeigen, wie mit dem weiter oben trainierten Klassifikator an der Kaggle Competition teilgenommen werden kann.  \nGrunds\u00e4tzlich muss einfach eine Textdatei mit dem Namen `submission.csv` erstellt werden. Diese kann dann direkt an die zum Datensatz geh\u00f6rende Competition weitergeleitet werden. ","fbf5931f":"Nehmen wir also an dieser [Demo-Kaggle-Competition](https:\/\/www.kaggle.com\/c\/oed19classification\/overview) teil. Wir brauchen einige Bibliotheken, die auf Kaggle bereits verf\u00fcgbar sind:","ec1f6923":"Diese Datei k\u00f6nnte man einfach auf der [Submissionsseite](https:\/\/www.kaggle.com\/c\/oed19classification\/submit) unserer Kaggle-Competition hochladen. Noch besser ist es, den Code (Kernel) gleich mitzuver\u00f6ffentlichen: Dazu klicken wir auf \"Commit\". Anschliessend auf \"Open Version\", um den hochgeladenen Kernel anzuschauen. Im Abschnitt \"Output\" gibt es einen Knopf \"Submit to Competition\".  \n\nDies ergibt eine Genauigkeit von 87%. Gewiss finden Sie eine bessere L\u00f6sung!  \n(Tipp: Die Zeile `clf = KNeighborsClassifier(n_neighbors=20)` ist suboptimal.)","0ceb6c9f":"Dieses Beispiel f\u00fcr maschinelles Lernen ist sehr einfach, vielleicht zu einfach. Meine Gr\u00fcnde f\u00fcr diese Wahl: \n* KNN ist ein intuitiv schnell zu erfassender Machine Learning Algorithmus: Gebe dem Testdatenpunkt das selbe Label wie dem n\u00e4chsten Punkt im Trainingsdatensatz\n* Ein 2D-Datensatz l\u00e4sst sich leicht visualisieren, wie auch seine Entscheidungsgrenzen. \n* Komplexere Ans\u00e4tze wie Neuronale Netze funktionieren nicht wesentlich anders.","81fa19f9":"Klicken Sie nun oben rechts auf \"Commit\". ","ef2f48fb":"Laden wir die Daten:","4704ac78":"Die Vorhersage speichern wir in der Datei `submission.csv`. Damit kann man an der Kaggle-Competition direkt mit dem Kernel teilnehmen:","1dadac4c":"Wir trainieren ihn:","accebb5a":"# Hyperparameter Tuning\nWie w\u00e4hlen wir einen guten Wert f\u00fcr die Anzahl Nachbarn $k$ bzw. `n_neighbors`? Wir tun dies hier \"von Auge\", indem wir die Klassengrenzen visualisieren und schauen, dass sie plausibel sind (vermeiden von Overfitting und Underfitting). Professioneller w\u00e4re die Benutzung von [cross_val_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_val_score.html), [Parameterkurven](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_validation_curve.html#sphx-glr-auto-examples-model-selection-plot-validation-curve-py) oder [Grid Searches](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html).  \nDazu gibt's viele Tutorials auf Kaggle! Z.B. [dieses hier](https:\/\/www.kaggle.com\/dansbecker\/underfitting-and-overfitting) auf der [Machine Learning Micro-Course Home Page](https:\/\/www.kaggle.com\/learn\/machine-learning).","1dbc46d2":"Um den optimalen Wert von $k$ zu finden, probieren wir mehrere Werte aus und messen, wie gut der Klassifikator auf einem Validierungsdatensatz ist. Kreuzvalidierung (CV) wiederholt dies mehrfach und mittelt die Ergebnisse.","3af89536":"# Fortgeschrittene L\u00f6sung","6e0bba5a":"# Demo-Notebook zur Teilnahme an einer Kaggle Competition\n### Benutzt im Workshop \"Maschinelles Lernen mit Scikit-learn und Kaggle\"  \nim Rahmen der Open Education Days 2019","5822b6b5":"und machen Vorhersagen auf dem Testdatensatz:","a73f87f5":"Laden wir einen Klassifikator aus [Scikit-Learn](https:\/\/scikit-learn.org\/stable\/#). ","cf5e925e":"# Daten laden\nWo sind die Daten?","5d558709":"Hier ist offenbar der Wert $k=5$ gut, denn der Test-Score, der ja die Verallgemeinerungsf\u00e4higkeit misst, ist hier gross. Man beachte, dass diese Kurven zuf\u00e4llig etwas schwanken k\u00f6nnen und daher  der maximale Wert nicht zwingend optimal ist. "}}