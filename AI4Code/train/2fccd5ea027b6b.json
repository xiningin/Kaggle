{"cell_type":{"22d042f4":"code","d8b873cc":"code","ea0f374b":"code","c38acf4f":"code","59167895":"code","d50a24fc":"code","8d058c05":"code","b434a5ca":"code","a0b1ae60":"code","df27edeb":"markdown","bab75146":"markdown","15c4c69e":"markdown","c3bf4585":"markdown","510e07ed":"markdown","ac181461":"markdown"},"source":{"22d042f4":"# import libraries\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport optuna\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom optuna.samplers import TPESampler\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold, RepeatedKFold\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import cv\nimport xgboost as xgb\nfrom scipy.optimize import minimize\n","d8b873cc":"# import data\ntrain_df = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\n\n# separate data\nX = train_df.drop(['loss', 'id'], axis=1)\ny = train_df['loss']\nX_test = test_df.drop(['id'], axis=1)\n\n# Tune for new hyperparameters or use custom values\nXGB_OPTUNA = False\nPSEUDO = True\n\nEARLY_OPTUNA = 100\nEARLY_FIT = 150\n","ea0f374b":"# scale data\nscaler = StandardScaler()\nscaler.fit(pd.concat([X, X_test]))\nX = scaler.transform(X)\nX_test = scaler.transform(X_test)","c38acf4f":"def xgb_objective(trial,data=X,target=y):\n    X_train, X_valid, y_train, y_valid = train_test_split(data, target, test_size=0.4,random_state=42)\n    \n    param_grid = {'tweedie_variance_power': trial.suggest_float('tweedie_variance_power', 1.035, 1.06),\n                  'n_estimators': trial.suggest_int('n_estimators', 3800, 5000), \n                  'max_depth': trial.suggest_int('max_depth', 5, 9),\n                  'eta': trial.suggest_float('eta', 0.005, 0.011),\n                  'subsample': trial.suggest_discrete_uniform('subsample', 0.3, 0.6, 0.01),\n                  'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.8),\n                  'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.6, 0.8),\n                  'colsample_bynode': trial.suggest_float('colsample_bynode', 0.6, 1.0),\n                  'min_child_weight': trial.suggest_float('min_child_weight', 1e-3, 0.1),\n                  'reg_alpha': trial.suggest_float('reg_alpha', 1, 100),\n                  'reg_lambda': trial.suggest_float('reg_lambda', 800, 8000),\n                  'max_delta_step': trial.suggest_float('max_delta_step', 1, 8000),\n                  'gamma': trial.suggest_float('gamma', 0.1, 1),\n                  'base_score': trial.suggest_float('base_score', 0.42, 0.46)} \n    \n    model = xgb.XGBRegressor(objective='reg:tweedie',\n                             tree_method='gpu_hist',\n                             predictor='gpu_predictor',\n                             sampling_method='gradient_based',\n                             n_jobs=-1,\n                             max_bin=256,\n                             single_precision_histogram='true',\n                             **param_grid)\n    \n    model.fit(X_train, y_train,\n              eval_set=[(X_valid, y_valid)],\n              eval_metric='rmse',\n              early_stopping_rounds=EARLY_OPTUNA,\n              verbose=False)\n\n    return mean_squared_error(y_valid, model.predict(X_valid), squared=False)","59167895":"def create_optuna_study(objective, study_name, train_time):\n    study = optuna.create_study(direction='minimize', \n                                sampler=TPESampler(), \n                                study_name=study_name)\n    study.optimize(objective, \n                   timeout=train_time)\n    trial = study.best_trial\n    \n    print('Number of finished trials: ', len(study.trials))\n    print('Best trial:')\n    print('\\tValue: {}'.format(trial.value))\n    print('\\tParams: ')\n    for key, value in trial.params.items():\n        print(\"\\t\\t'{}': {},\".format(key, value))\n    \n    return trial, study","d50a24fc":"train_time = 1 * 60 * 60\n\n# XGB Optimize\nif XGB_OPTUNA:\n    xgb_trial, study = create_optuna_study(xgb_objective, 'XGBRegressor', train_time)\n    xgb_params = xgb_trial.params\nelse:\n    # \tValue: 7.810990920261464\n    xgb_params = {'tweedie_variance_power': 1.0467,\n                    'n_estimators': 4200,\n                    'max_depth': 6,\n                    'eta': 0.010168813765699104,\n                    'subsample': 0.32999999999999996,\n                    'colsample_bytree': 0.72,\n                    'colsample_bylevel': 0.77,\n                    'colsample_bynode': 0.4,\n                    'min_child_weight': 0.0015983397006165201,\n                    'reg_alpha': 5.089297744468109,\n                    'reg_lambda': 5614.706936183112,\n                    'max_delta_step': 12.488093623290982,\n                    'gamma': 0.002944897792984669,\n                    'base_score': 0.4534214581239122}\nxgb_params['objective']='reg:tweedie'\nxgb_params['tree_method'] = 'gpu_hist'\nxgb_params['predictor'] = 'gpu_predictor'\nxgb_params['n_jobs'] = -1\nxgb_params['max_bin'] = 256\n\npseudo_xgb_params = {'n_estimators': 4874,\n                    'max_depth': 8,\n                    'eta': 0.006269949203588203,\n                    'subsample': 0.52,\n                    'colsample_bytree': 0.5341415254987654,\n                    'colsample_bylevel': 0.7053444074403165,\n                    'colsample_bynode': 0.6195508609737396,\n                    'min_child_weight': 0.07942691380323752,\n                    'reg_alpha': 75.63328698050019,\n                    'reg_lambda': 2045.4576615756023,\n                    'max_delta_step': 4636.2914334780635,\n                    'gamma': 0.652157845901367,\n                    'base_score': 0.42312328086044243}\npseudo_xgb_params['objective'] = 'reg:squarederror'\npseudo_xgb_params['tree_method'] = 'gpu_hist'\npseudo_xgb_params['predictor'] = 'gpu_predictor'\npseudo_xgb_params['n_jobs'] = -1\npseudo_xgb_params['max_bin'] = 256\n","8d058c05":"test_preds = np.zeros(X_test.shape[0])\n\nxgb_rmse = []\n\nn_splits = 7\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X, y), 1):\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_valid, y_valid = X[valid_idx], y[valid_idx]\n    \n    pre_xgb_model = xgb.XGBRegressor(**xgb_params)\n    pre_xgb_model.fit(X_train, y_train,\n                  eval_set=[(X_valid, y_valid)],\n                  verbose=False,\n                  callbacks = [xgb.callback.EarlyStopping(\n                      rounds=EARLY_FIT,\n                      save_best=True)]) \n\n    post_xgb_model = xgb.XGBRegressor(**xgb_params)\n    post_xgb_model.fit(X_train, y_train,\n                      eval_set=[(X_valid, y_valid)],\n                      verbose=False,\n                      callbacks = [xgb.callback.EarlyStopping(\n                          rounds=EARLY_FIT*2,\n                          save_best=True)],\n                      xgb_model=pre_xgb_model)\n    \n    \n    test_preds += post_xgb_model.predict(X_test) \/ n_splits \n    xgb_rmse.append(mean_squared_error(y_valid, post_xgb_model.predict(X_valid), squared=False))\n    \n    print(f'Fold {fold}\\n\\txgb: {xgb_rmse[fold-1]}')\n    \n\nprint(f'\\nAverage xgb rmse: {np.array(xgb_rmse).mean()}')\n\nsubmission['loss'] = test_preds\nsubmission.to_csv('submission.csv', index=False)","b434a5ca":"def pseudolabel(X, y, X_test, test_preds, xgb_params, pseudo_xgb_params, early_fit, n_splits=7, n_repeats=2):\n    rmse = []\n    preds = []\n    best_rmse = 0\n    test_preds2 = np.zeros(len(X_test))\n\n    kf = RepeatedKFold(n_splits = n_splits, n_repeats = n_repeats, random_state=0)\n    kf2 = RepeatedKFold(n_splits = n_splits, n_repeats = n_repeats, random_state=0)\n\n    for fold, ((train_idx, valid_idx),(pseudo_idx, pseudo2_idx)) in enumerate(zip(kf.split(X, y), kf2.split(X_test, test_preds)), 1):\n        X_train, y_train = X[train_idx], y[train_idx]\n        X_valid, y_valid = X[valid_idx], y[valid_idx]\n        \n        X_pseudo, y_pseudo = X_test[pseudo_idx], test_preds[pseudo_idx]\n        X_pseudo2, y_pseudo2 = X_test[pseudo2_idx], test_preds[pseudo2_idx]\n        \n        # Run the model on a smaller pseudolabel dataset\n        pre_xgb_model = xgb.XGBRegressor(**pseudo_xgb_params)\n        pre_xgb_model.fit(np.concatenate([X_train, X_pseudo2]),\n                          np.concatenate([y_train, y_pseudo2]),\n                          eval_set=[(X_valid, y_valid)],\n                          verbose=False,\n                          callbacks=[xgb.callback.EarlyStopping(\n                              rounds=early_fit,\n                              save_best=True)])        \n        \n        # Finetune the model using the larger pseudo dataset and a more complex model\n        # Feed the previous model weights into this new model\n        # The evaluation dataset must be the grountruth data\n        post_xgb_model = xgb.XGBRegressor(**pseudo_xgb_params)\n        post_xgb_model.fit(np.concatenate([X_train, X_pseudo]),\n                          np.concatenate([y_train, y_pseudo]),\n                          eval_set=[(X_valid, y_valid)],\n                          verbose=False,\n                          callbacks=[xgb.callback.EarlyStopping(\n                              rounds=early_fit*2,\n                              save_best=True)],\n                          xgb_model=pre_xgb_model)\n\n        preds.append(post_xgb_model.predict(X_test))\n\n        xgb_rmse = mean_squared_error(y_valid, post_xgb_model.predict(X_valid), squared=False)\n        rmse.append(xgb_rmse)\n        \n        print(f'Fold {fold}\\n\\txgb: {xgb_rmse}')\n    \n    for n in sorted(range(n_splits*n_repeats), key=lambda k: rmse[k])[:n_splits]:\n        test_preds2 += preds[n] \/ n_splits\n        best_rmse += rmse[n] \/ n_splits\n    \n    print(f'\\nAverage total rmse: {np.array(rmse).mean()}')\n    print(f'\\nAverage best rmse: {best_rmse}')\n    \n    return test_preds2\n","a0b1ae60":"if PSEUDO:\n    test_preds2 = pseudolabel(X, y, X_test, test_preds, xgb_params, pseudo_xgb_params, EARLY_FIT)\n    \nsubmission2 = submission\nsubmission2['loss'] = test_preds2\nsubmission2.to_csv('submission2.csv', index=False)","df27edeb":"## Pseudolabel implementation\nWhat is pseudolabelling? Read here:\nhttps:\/\/www.kaggle.com\/c\/tabular-playground-series-apr-2021\/discussion\/231738\n\nTo use this implementation, the following variables need to be declared before you just pop it onto the end of your code\n\n1. X: training dataset\n2. y: training dataset labels\n3. X_test: testing dataset\n4. test_preds: testing dataset predictions\n5. xgb_params: dictionary parameters for XGB model\n6. pseudo_xgb_params: dictionary parameters for fine-tune XGB model\n7. early_fit: number of early stopping rounds\n8. n_splits: number of cross-validation splits\n9. n_repeats: number of split and train repeats","bab75146":"## Initial Label","15c4c69e":"## Contributions made by this notebook\n1. **Pseudolabelled dataset training**: the last cell in this notebook is annotated and compartmentalized so that it can be easily integrated if someone else would like to try it out\n2. **Choose best from repeated Kfolds**: also in the last cell of this notebook. Kfolds is run a repeated number of times and then only the lowest loss runs are chosen to be averaged. To turn it off, simply replace RepeatedKfold with Kfold and remove instances of the variable 'n_repeats'. *Warning* this is highly prone to overfit so regularize the model as necessary or turn it off\n3. **Use a tweedie variance power between [1.035, 1.06]**: see older versions of this notebook to see optuna optimization runs that show this is a better fit than the suggested 1.1 that other notebooks use\n\n(4). Explored a few parameters in the XGB model to tune for Optuna that I didn't see in other notebooks. This may be unnecessary but someone else may be able to explain whether they are helpful or not","c3bf4585":"## What can be improved\n1. XGB model paramters, pseudo XGB model parameters\n2. Ensembling\n3. N_split and N_repeat values","510e07ed":"A lot of what I have based my notebook on is creditted to \n 1. https:\/\/www.kaggle.com\/michael127001\/xgbregressor-with-optuna-tuning\n 2. https:\/\/www.kaggle.com\/pranjalverma08\/tps-08-cb-lgbm-xgb-starter\n 3. https:\/\/www.kaggle.com\/dmitryuarov\/falling-below-7-87-voting-cb-xgb-lgbm\n 4. https:\/\/www.kaggle.com\/hiro5299834\/tps-aug-2021-xgb\/","ac181461":"A foreword, although my overall implementation doesn't have too great of a score, I hope that my contribution will be able to slightly boost someone's with a better notebook."}}