{"cell_type":{"eaa0558d":"code","c4f71ece":"code","2522b19a":"code","b2a89c7b":"code","0b624d76":"code","8f7f20c0":"code","6236586b":"code","104880e0":"code","8f988d10":"code","03ca860c":"code","351b601e":"code","54cf1175":"code","3d622589":"markdown","a6103b2e":"markdown","69cdac9e":"markdown","57b0bf15":"markdown","879580f0":"markdown","875d65de":"markdown","f8f3d4f7":"markdown","c9466715":"markdown","38caabd9":"markdown","4d8d9228":"markdown","510617fb":"markdown","a0dc64ef":"markdown","9f5edd03":"markdown","c7287699":"markdown","ecb20d97":"markdown","28fbd305":"markdown","d6b22319":"markdown","97ccf202":"markdown","218c0770":"markdown","89bafa42":"markdown","fdc11cb8":"markdown","47ecb7e9":"markdown","17815cf5":"markdown"},"source":{"eaa0558d":"!pip install -q SemTorch\n!pip install -q pytorch_toolbelt ","c4f71ece":"# Some useful imports.\n\nfrom fastai.basics import *\nfrom fastai.vision import models\nfrom fastai.vision.all import *\nfrom fastai.metrics import *\nfrom fastai.data.all import *\nfrom fastai.callback import *\nfrom albumentations import *\n# SemTorch\nfrom semtorch import get_segmentation_learner\nfrom torch.utils.data import Dataset\nfrom pathlib import Path\nimport random\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport cv2\n\n\n%matplotlib inline\n\n\n# Some constants.\n\nSEED = 314\nBASE_FOLDER = Path(\"..\/input\/hubmap-256x256\")\nMASKS_FOLDER = BASE_FOLDER \/ \"masks\"\nIMG_FOLDER = BASE_FOLDER \/ \"train\"\nLABELS_PATH = '..\/input\/hubmap-kidney-segmentation\/train.csv'\nN_FOLDS = 5\nNUM_WORKERS = 4\nBATCH_SIZE = 4 # was 64 but too much for Kaggle.\nN_EPOCHS = 1 #\u00a0only one epoch to make things faster.\n\n\n# Before we start, we need to seed!\n\nrandom.seed(SEED)\nset_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n\n","2522b19a":"train_img_mean = np.array([0.65459856,0.48386562,0.69428385])\ntrain_img_std = np.array([0.15167958,0.23584107,0.13146145])\n\n\n\ndef img2tensor(img, dtype: np.dtype=np.float32):\n    if img.ndim==2 : \n        img = np.expand_dims(img,2)\n    img = np.transpose(img,(2,0,1))\n    return torch.from_numpy(img.astype(dtype, copy=False))\n\nclass HuBMAPDataset(Dataset):\n    def __init__(self, fold=0, train=True, tfms=None):\n        ids = pd.read_csv(LABELS_PATH).id.values\n        kf = KFold(n_splits=N_FOLDS, random_state=SEED,shuffle=True)\n        ids = set(ids[list(kf.split(ids))[fold][0 if train else 1]])\n        self.fnames = [fname for fname in os.listdir(IMG_FOLDER) if fname.split('_')[0] in ids]\n        self.train = train\n        self.tfms = tfms\n        \n    def __len__(self):\n        return len(self.fnames)\n    \n    def __getitem__(self, idx):\n        fname = self.fnames[idx]\n        img = cv2.cvtColor(cv2.imread(os.path.join(IMG_FOLDER, fname)), cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(os.path.join(MASKS_FOLDER, fname),cv2.IMREAD_GRAYSCALE)\n        if self.tfms is not None:\n            augmented = self.tfms(image=img, mask=mask)\n            img,mask = augmented['image'],augmented['mask']\n        return img2tensor((img\/255.0 - train_img_mean)\/train_img_std),img2tensor(mask)\n    \ndef get_aug(p=1.0):\n    return Compose([\n        HorizontalFlip(),\n        VerticalFlip(),\n        RandomRotate90(),\n        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.9, \n                         border_mode=cv2.BORDER_REFLECT),\n        OneOf([\n            OpticalDistortion(p=0.3),\n            GridDistortion(p=.1),\n            IAAPiecewiseAffine(p=0.3),\n        ], p=0.3),\n        OneOf([\n            HueSaturationValue(10,15,10),\n            CLAHE(clip_limit=2),\n            RandomBrightnessContrast(),            \n        ], p=0.3),\n    ], p=p)\n","b2a89c7b":"ds = HuBMAPDataset(tfms=get_aug())\ndl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\nimgs, masks = next(iter(dl))\n\nplt.figure(figsize=(12, 12))\nfor i,(img,mask) in enumerate(zip(imgs,masks)):\n    img = ((img.permute(1,2,0)*train_img_std + train_img_mean)*255.0).numpy().astype(np.uint8)\n    plt.subplot(2, 2, i+1)\n    plt.imshow(img, vmin=0, vmax=255)\n    plt.imshow(mask.squeeze().numpy(), alpha=0.2)\n    plt.axis('off')\n    plt.subplots_adjust(wspace=None, hspace=None)","0b624d76":"# TODO: More folds. Myabe not necessary for the example.\n\n\nds_t = HuBMAPDataset(fold=0, train=True, tfms=get_aug())\nds_v = HuBMAPDataset(fold=0, train=False)\ndata = ImageDataLoaders.from_dsets(ds_t, ds_v,bs=BATCH_SIZE, num_workers=NUM_WORKERS, \n                                   pin_memory=True).cuda()","8f7f20c0":"import pytorch_toolbelt.losses as L\n\n\nlovasz = L.LovaszLoss()\n\ndef symmetric_lovasz(outputs, targets):\n    return 0.5*(lovasz(outputs, targets) + lovasz(-outputs, 1.0 - targets))","6236586b":"\n\nfrom semtorch import get_segmentation_learner\n\nlearn = get_segmentation_learner(dls=data, number_classes=2, \n                                 segmentation_type=\"Semantic Segmentation\",\n                                 architecture_name=\"deeplabv3+\", \n                                 backbone_name=\"resnet50\",\n                                 loss_func=symmetric_lovasz,\n                                 image_size=256,\n                                 metrics=[Dice(), JaccardCoeff()], \n                                 wd=1e-2,\n                                 pretrained=True, normalize=True).to_fp16()","104880e0":"#\u00a0learn.lr_find() \n# learn.recorder ","8f988d10":"monitor_training=\"valid_loss\"\ncomp_training=np.less\n\nmonitor_evaluating=\"dice\"\ncomp_evaluating=np.greater\n\npatience=2\n\n\nfname=\"segmentation\"\n\ncallbacksFitBeforeUnfreeze = [\n    ShowGraphCallback(),\n    EarlyStoppingCallback(monitor=monitor_training,comp=comp_training, patience=patience),\n    SaveModelCallback(monitor=monitor_training,comp=comp_training,every_epoch=False,fname=fname)  \n]\n\n#\u00a0learn.fit_one_cycle(N_EPOCHS, slice(1e-5,1e-4), cbs=callbacksFitBeforeUnfreeze)","03ca860c":"import cv2\nimport matplotlib.pylab as plt\n# Change the path to try on another image\nimg_path = \"..\/input\/hubmap-256x256\/train\/0486052bb_0121.png\"\nmask_path = img_path.replace(\"train\", \"masks\")\nimg = cv2.imread(img_path)\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\n\nax.imshow(img)","351b601e":"#\u00a0Load the different models, predict, and take the average.\nimport glob\n\nPATHS = \"..\/input\/hubmap-model-weights\/*\"\npredictions = []\n\n#\u00a0img = data.train_ds[0][0]\nimg = np.moveaxis(img, -1, 0)\nimg = img.astype(np.float32)\nfor path in glob.glob(PATHS):\n    learn.path = Path(\"\")\n    learn.model_dir = Path(\"\")\n    path = path.replace(\".pth\", \"\")\n    learn.load(path)\n    predictions_ = learn.predict(img[None, ...])[0].argmax(dim=0)\n    predictions.append(predictions_)","54cf1175":"predicted_mask = torch.stack(predictions).sum(axis=0) \/ 5\noriginal_mask = cv2.imread(mask_path)\nimg = cv2.imread(img_path)\n\nfig, axes = plt.subplots(1, 3, figsize=(20, 10))\naxes[0].imshow(predicted_mask)\naxes[1].imshow(original_mask[:, :, 0])\naxes[2].imshow(img)\nfig.suptitle(\"Predicted mask |\u00a0True mask |\u00a0Image\", fontsize=16);","3d622589":"# Training ","a6103b2e":"![Webp.net-resizeimage.png](attachment:bce29e69-3ac6-4af9-88c6-58d511a825c2.png)","69cdac9e":"**fastai** is a deep learning library based on [PyTorch](https:\/\/pytorch.org\/). \n\nI have done a previous notebook about fastai [here](https:\/\/www.kaggle.com\/yassinealouini\/learning-fastai-part-i) so this paragraph could be considered the long awaited part II. ;) \n\nFor those that haven't checked part I, it introduces the notion of dataset in fastai: dataloaders.\n\nhttps:\/\/docs.fast.ai\/data.core.html\n\nHere I will talk more specifically about the **learner** object.","57b0bf15":"Next step, we install the **SemTorch** library and an additional one **pytorch_toolbelt** for the metrics.","879580f0":"Now that we have seen how to train a model, let's make some predictions and display these. \n\nFor the sake of brevity, I will load weights that I have trained locally using the same pipeline.\n\nYou can find these in the **hubmap-model-weights** [dataset](https:\/\/www.kaggle.com\/yassinealouini\/hubmap-model-weights): 5 models, one per fold.\n\nWe will also need one image as an input.","875d65de":"\nThis will be a short notebook on how to train a **SemTorch** model using the competition's data. \n\nIn the process, I will try to explain the different steps for those new to fastai in general and SemTorch in\nparticular.\n\nBefore we start, if you are new to **segmentation** in general, consider the following [notebook](https:\/\/www.kaggle.com\/yassinealouini\/all-the-segmentation-metrics) as well.\n\nLet's go!\n","f8f3d4f7":"# Resources","c9466715":"Uncomment the next cell if you want to check how the learning rate finder works.","38caabd9":"For the loss, we will use the **Lovasz loss** from pytorch toolbelt library.","4d8d9228":"# SemTorch: what is it?","510617fb":"This is a class available in the `learner.py` file.\n\nAs stated in the documentation: \n\n\n> Group together a model, some dls and a loss_func to handle training\n\nThe `learner` is used as the training loop. It needs a **dls** (short for a **data loader**) and a **loss function**.\n\nFor more details, check the following [doc](https:\/\/docs.fast.ai\/learner.html).","a0dc64ef":"As you haved guessed it by now, **SemTorch** is a **segmentation library** based on **fastai**. \n\nIt is made by **David Lacalle Castillo**. \n\n\nThe main function that we will use later is `get_segmentation_learner`. To get it, you can \nimport it using the following snippet:\n\n`from semtorch import get_segmentation_learner`\n\nThis fucntion uses some if statements depending on the architecture chosen and then, mainly the fastai `Learner` class.\n\n\nIf you are curious, you can check the code source [here](https:\/\/github.com\/WaterKnight1998\/SemTorch\/blob\/develop\/semtorch\/learner.py#L81)\n\n\nSo in order to understand how it works, we first need to understand (a bit) how [fastai](https:\/\/github.com\/fastai\/fastai) works.","9f5edd03":"## A fastai detour","c7287699":"# Inference","ecb20d97":"Some resources as usual for those that want to dive deeper:\n\n\n\n- Check the following [forum post](https:\/\/forums.fast.ai\/t\/semtorch-a-semantic-segmentation-library-build-above-fastai\/79194) to have more details about SemTorch.\n- Fastai introduction course [link](https:\/\/course.fast.ai\/)\n- My previous fastai notebook [here](https:\/\/www.kaggle.com\/yassinealouini\/learning-fastai-part-i)","28fbd305":"Similarly, uncomment the next cell if you want to run one cycle of training. As you will see\nin the next section, we will instead load some weights.","d6b22319":"As you can see, the model hasn't yet converged (even though we see that the center of the gloumerli is starting to appear) so will need more training (running the training pipeline for more epochs). \n\nThat's it for today, see you later in another notebook.","97ccf202":"<img src=\"https:\/\/www.fast.ai\/images\/Fast.ai.png\" width=480>","218c0770":"Now that everything is set, we can finally define the **learner**. Here are the important parameters:\n\n\n* **dls**: the dataloader\n* **number_classes**: the number of classes, here 2 (one for the globumeli and one for the background)\n* **segmentation_type**: the type of the segmentation task, here semantic segmentation.\n* **architecture_name**: the name of the architecture, here deeplabv3. More about it [here](https:\/\/arxiv.org\/pdf\/1706.05587.pdf) for example\n* **backbone_name**: the backbone model, here resnet50. More about it here for example.\n* **loss_func**: the loss function, here a symmertic Lovasz loss. More about it here for example.\n* **image_size**: the size of the image that will be fed to the model, here it is 256.\n* **metrics**: the metrics to monitor during the training loop.\n* **wd**: weight decay, here it is set to 1e-2\n* **pretrained**: wether to use pretrained weights or not, here it is set to True.\n* **normalize**: wether to normalize the images or not, here it is set to True.","89bafa42":"## The learner object","fdc11cb8":"For this notebook, we will only defined the dataset and data loader for one fold but you can \neasily extend this to different folds of course.","47ecb7e9":"## Usage","17815cf5":"Finally, let's display the average prediction from the 5 models,\nthe original image, and the original mask."}}