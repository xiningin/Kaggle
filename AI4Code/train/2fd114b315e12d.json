{"cell_type":{"e4967dbe":"code","475f25dd":"code","fea25d7d":"code","a7c287d8":"code","1e7a67c9":"code","afdc61dd":"code","39165b2f":"code","bfd4ae8d":"code","c75ddeb6":"code","90b87bcc":"code","84952741":"code","dad0479e":"code","6f99c52a":"code","292ea0fe":"code","a1c91fd6":"code","4e949f6b":"code","53c9e7e8":"code","99282ea5":"code","3823b85b":"code","6829fc59":"code","5a3b8d46":"code","4d731bad":"code","b34a8a29":"code","b239efe9":"code","22e5a93b":"code","07dd348d":"code","0e3c7880":"code","38b2a4bc":"code","22900df9":"code","82d92690":"code","b10a624b":"code","a0eb8ba2":"code","ebb01f0b":"code","b1a598a0":"code","690cca59":"code","96c44f25":"code","dcd11d6b":"code","1ee164cd":"code","6a9ef388":"code","6d62590b":"code","e1c7f0a0":"code","4e9be5de":"code","33c2f513":"markdown","47b7bd38":"markdown","9c3b8f86":"markdown","e3ed392b":"markdown","f170bfae":"markdown","10bd26f2":"markdown","a947d88b":"markdown","eaa33b25":"markdown","3cecd997":"markdown","b1ec7ca5":"markdown","97447e5d":"markdown","febcf432":"markdown","002c1be7":"markdown","849ff8b3":"markdown","3267d4ce":"markdown","97441063":"markdown","386bb35b":"markdown","9b906cdc":"markdown","c9fd7738":"markdown"},"source":{"e4967dbe":"#  Libraries\nimport numpy as np \nimport pandas as pd \n# Data processing, metrics and modeling\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold,KFold\nfrom datetime import datetime\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\nfrom sklearn import metrics\nfrom sklearn import preprocessing\n# Suppr warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport itertools\nfrom scipy import interp\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import rcParams","475f25dd":"%%time\ntrain_transaction = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv', index_col='TransactionID')\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv', index_col='TransactionID')\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')","fea25d7d":"# merge \ntrain_df = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest_df = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(\"Train shape : \"+str(train_df.shape))\nprint(\"Test shape  : \"+str(test_df.shape))","a7c287d8":"train_df = train_df.drop([\"TransactionDT\"], axis = 1)\ntest_df = test_df.drop([\"TransactionDT\"], axis = 1)","1e7a67c9":"train_df = train_df.reset_index()\ntest_df = test_df.reset_index()","afdc61dd":"train_df['nulls1'] = train_df.isna().sum(axis=1)\ntest_df['nulls1'] = test_df.isna().sum(axis=1)","39165b2f":"train_df = train_df.iloc[:, :55]\ntest_df = test_df.iloc[:, :54]","bfd4ae8d":"del train_transaction, train_identity, test_transaction, test_identity","c75ddeb6":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\nus_emails = ['gmail', 'net', 'edu']\n#https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100499#latest_df-579654\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train_df[c + '_bin'] = train_df[c].map(emails)\n    test_df[c + '_bin'] = test_df[c].map(emails)\n    \n    train_df[c + '_suffix'] = train_df[c].map(lambda x: str(x).split('.')[-1])\n    test_df[c + '_suffix'] = test_df[c].map(lambda x: str(x).split('.')[-1])\n    \n    train_df[c + '_suffix'] = train_df[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test_df[c + '_suffix'] = test_df[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","90b87bcc":"for c1, c2 in train_df.dtypes.reset_index().values:\n    if c2=='O':\n        train_df[c1] = train_df[c1].map(lambda x: str(x).lower())\n        test_df[c1] = test_df[c1].map(lambda x: str(x).lower())","84952741":"numerical = [\"TransactionAmt\", \"nulls1\", \"dist1\", \"dist2\"] + [\"C\" + str(i) for i in range(1, 15)] + \\\n            [\"D\" + str(i) for i in range(1, 16)] + \\\n            [\"V\" + str(i) for i in range(1, 340)]\ncategorical = [\"ProductCD\", \"card1\", \"card2\", \"card3\", \"card4\", \"card5\", \"card6\", \"addr1\", \"addr2\",\n               \"P_emaildomain_bin\", \"P_emaildomain_suffix\", \"R_emaildomain_bin\", \"R_emaildomain_suffix\",\n               \"P_emaildomain\", \"R_emaildomain\",\n              \"DeviceInfo\", \"DeviceType\"] + [\"id_0\" + str(i) for i in range(1, 10)] +\\\n                [\"id_\" + str(i) for i in range(10, 39)] + \\\n                 [\"M\" + str(i) for i in range(1, 10)]\n","dad0479e":"numerical = [col for col in numerical if col in train_df.columns]\ncategorical = [col for col in categorical if col in train_df.columns]","6f99c52a":"def nan2mean(df):\n    for x in list(df.columns.values):\n        if x in numerical:\n            #print(\"___________________\"+x)\n            #print(df[x].isna().sum())\n            df[x] = df[x].fillna(0)\n           #print(\"Mean-\"+str(df[x].mean()))\n    return df\ntrain_df=nan2mean(train_df)\ntest_df=nan2mean(test_df)","292ea0fe":"# Label Encoding\ncategory_counts = {}\nfor f in categorical:\n    train_df[f] = train_df[f].replace(\"nan\", \"other\")\n    train_df[f] = train_df[f].replace(np.nan, \"other\")\n    test_df[f] = test_df[f].replace(\"nan\", \"other\")\n    test_df[f] = test_df[f].replace(np.nan, \"other\")\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n    train_df[f] = lbl.transform(list(train_df[f].values))\n    test_df[f] = lbl.transform(list(test_df[f].values))\n    category_counts[f] = len(list(lbl.classes_)) + 1\n# train_df = train_df.reset_index()\n# test_df = test_df.reset_index()","a1c91fd6":"from sklearn.preprocessing import StandardScaler","4e949f6b":"for column in numerical:\n    scaler = StandardScaler()\n    if train_df[column].max() > 100 and train_df[column].min() >= 0:\n        train_df[column] = np.log1p(train_df[column])\n        test_df[column] = np.log1p(test_df[column])\n    scaler.fit(np.concatenate([train_df[column].values.reshape(-1,1), test_df[column].values.reshape(-1,1)]))\n    train_df[column] = scaler.transform(train_df[column].values.reshape(-1,1))\n    test_df[column] = scaler.transform(test_df[column].values.reshape(-1,1))","53c9e7e8":"target = 'isFraud'","99282ea5":"#cut tr and val\ntr_df, val_df = train_test_split(train_df, test_size = 0.2, random_state = 42, shuffle = False)","3823b85b":"def get_input_features(df):\n    X = {'numerical':np.array(df[numerical])}\n    for cat in categorical:\n        X[cat] = np.array(df[cat])\n    return X","6829fc59":"category_counts","5a3b8d46":"from keras.layers import Concatenate, Input, Dense, Embedding, Flatten, Dropout, BatchNormalization, SpatialDropout1D\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import Model\nfrom keras.optimizers import  Adam\nimport keras.backend as k\ndef make_model():\n    k.clear_session()\n\n    categorical_inputs = []\n    for cat in categorical:\n        categorical_inputs.append(Input(shape=[1], name=cat))\n\n    categorical_embeddings = []\n    for i, cat in enumerate(categorical):\n        categorical_embeddings.append(\n            Embedding(category_counts[cat], int(np.log1p(category_counts[cat]) + 1), name = cat + \"_embed\")(categorical_inputs[i]))\n\n    categorical_logits = Concatenate(name = \"categorical_conc\")([Flatten()(SpatialDropout1D(.1)(cat_emb)) for cat_emb in categorical_embeddings])\n#     categorical_logits = Dropout(.5)(categorical_logits)\n\n    numerical_inputs = Input(shape=[tr_df[numerical].shape[1]], name = 'numerical')\n    numerical_logits = Dropout(.1)(numerical_inputs)\n  \n    x = Concatenate()([\n        categorical_logits, \n        numerical_logits,\n    ])\n#     x = categorical_logits\n#     x = BatchNormalization()(x)\n    x = Dense(200, activation = 'relu')(x)\n    x = Dropout(.2)(x)\n    x = Dense(100, activation = 'relu')(x)\n    x = Dropout(.2)(x)\n    out = Dense(1, activation = 'sigmoid')(x)\n    \n\n    model = Model(inputs=categorical_inputs + [numerical_inputs],outputs=out)\n    loss = \"binary_crossentropy\"\n    model.compile(optimizer=Adam(lr = 0.01), loss = loss)\n    return model\n","4d731bad":"from sklearn.metrics import roc_auc_score","b34a8a29":"from copy import deepcopy","b239efe9":"categorical_save = deepcopy(categorical)\nnumerical_save = deepcopy(numerical)","22e5a93b":"#in case we want to restart the search. uncomment and rerun the below cells\n# categorical = deepcopy(categorical_save)\n# numerical = deepcopy(numerical_save)","07dd348d":"X_train = get_input_features(tr_df)\nX_valid = get_input_features(val_df)\nX_test = get_input_features(test_df)\ny_train = tr_df[target]\ny_valid = val_df[target]\nmodel = make_model()\nbest_score = 0\npatience = 0\nfor i in range(100):\n    if patience < 3:\n        hist = model.fit(X_train, y_train, validation_data = (X_valid,y_valid), batch_size = 8000, epochs = 1, verbose = 1)\n        valid_preds = model.predict(X_valid, batch_size = 8000, verbose = True)\n        score = roc_auc_score(y_valid, valid_preds)\n        print(score)\n        if score > best_score:\n            model.save_weights(\"model.h5\")\n            best_score = score\n            patience = 0\n        else:\n            patience += 1","0e3c7880":"drop_cats = []","38b2a4bc":"for category_test in categorical_save:\n    categorical = [cat for cat in categorical_save if cat not in drop_cats]\n    categorical.remove(category_test)\n    print(categorical)\n    X_train = get_input_features(tr_df)\n    X_valid = get_input_features(val_df)\n    X_test = get_input_features(test_df)\n    y_train = tr_df[target]\n    y_valid = val_df[target]\n    model = make_model()\n    local_score = 0\n    patience = 0\n    for i in range(100):\n        if patience < 3:\n            hist = model.fit(X_train, y_train, validation_data = (X_valid,y_valid), batch_size = 8000, epochs = 1, verbose = 0)\n            valid_preds = model.predict(X_valid, batch_size = 8000, verbose = False)\n            score = roc_auc_score(y_valid, valid_preds)\n            if score > local_score:\n                model.save_weights(\"model.h5\")\n                local_score = score\n                patience = 0\n            else:\n                patience += 1\n        else:\n            if local_score < best_score:\n                print(\"performance reduced when\", category_test, \"dropped to\", local_score, \"from\", best_score)\n                break\n            else:\n                drop_cats.append(category_test)\n                print(\"performance increased when\", category_test, \"dropped to\", local_score, \"from\", best_score)\n                best_score = local_score\n                model.save_weights(\"best_model.h5\")\n                break","22900df9":"drop_cats","82d92690":"categorical = [cat for cat in categorical_save if cat not in drop_cats]","b10a624b":"drop_nums = []","a0eb8ba2":"for numerical_test in numerical_save:\n    numerical = [num for num in numerical_save if num not in drop_nums]\n    numerical.remove(numerical_test)\n    print(numerical)\n    X_train = get_input_features(tr_df)\n    X_valid = get_input_features(val_df)\n    X_test = get_input_features(test_df)\n    y_train = tr_df[target]\n    y_valid = val_df[target]\n    model = make_model()\n    local_score = 0\n    patience = 0\n    for i in range(100):\n        if patience < 4:\n            hist = model.fit(X_train, y_train, validation_data = (X_valid,y_valid), batch_size = 8000, epochs = 1, verbose = 0)\n            valid_preds = model.predict(X_valid, batch_size = 8000, verbose = False)\n            score = roc_auc_score(y_valid, valid_preds)\n            if score > local_score:\n                model.save_weights(\"model.h5\")\n                local_score = score\n                patience = 0\n            else:\n                patience += 1\n        else:\n            if local_score < best_score:\n                print(\"performance reduced when\", numerical_test, \"dropped to\", local_score, \"from\", best_score)\n                break\n            else:\n                drop_nums.append(numerical_test)\n                print(\"performance increased when\", numerical_test, \"dropped to\", local_score, \"from\", best_score)\n                best_score = local_score\n                model.save_weights(\"best_model.h5\")\n                break","ebb01f0b":"print(drop_nums)","b1a598a0":"numerical = [num for num in numerical_save if num not in drop_nums]","690cca59":"model = make_model()","96c44f25":"model.load_weights(\"best_model.h5\")","dcd11d6b":"X_valid = get_input_features(val_df)\nX_test = get_input_features(test_df)\nvalid_preds = model.predict(X_valid, batch_size = 500, verbose = True)\nscore = roc_auc_score(y_valid, valid_preds)\nprint(score)","1ee164cd":"hist = model.fit(X_valid,y_valid, batch_size = 8000, epochs = 3, verbose = 1)","6a9ef388":"predictions = model.predict(X_test, batch_size = 2000, verbose = True)","6d62590b":"pd.DataFrame(valid_preds).describe()","e1c7f0a0":"pd.DataFrame(predictions).describe()","4e9be5de":"sample_submission['isFraud'] = predictions\nsample_submission.to_csv('submission_IEEE.csv')","33c2f513":"Grabbing the features we want to pass into the neural network","47b7bd38":"We will then extract the features we actually want to pass to the NN","9c3b8f86":"## Label Encoding\nWe will take our categorical features fill the nans and assign them an integer ID per category and write down the number of total categories per column. We'll use this later in an embedding layer of the NN","e3ed392b":"NN doesn't like nans so we will fill numerical columns with 0's. Previous people have tried plugging in the column means, but upon inspection this didn't seem very reliable because the train and test means for a given column could sometimes be drastically different. Plugging in zeros likely isnt the best. Might be better to plug in the train mean to the test df, but for simplicity I will stick with 0's for now. ","f170bfae":"Selecting just the first set of columns and excluding the synthetic \"v\" features and other very sparse categoricals like deviceinfo and deviceid","10bd26f2":"Let's fine-tune on the validation data now since that is the most recent and will likely help generalization to the test set","a947d88b":"## Numerical Scaling\n\nNow we will do some scaling of the data so that it will be in a more NN friendly format. First we will do log1p for any values that are above 100 and not below 0. This is in order to scale down any numerical variables that might have some extremely high values that screws up the statistics of the standard scaler \n\nAfter that we will pass them through the standard scaler so that the values have a normal mean and std. This makes the NN converge signficantly faster and prevents any blowouts. Feel free to try for yourself by commenting out this cell of code","eaa33b25":"## Data Loading\nJust the standard loading of the data used in most other kernels. ","3cecd997":"Dropping categoricals","b1ec7ca5":"## Sanity Checks\nNow we will make sure we loaded in the correction  model that scored favorably and then we will compare prediction statistics between validation and test. These should be relatively close or else we know something might have been off between how we prepared the train and test data for inference","97447e5d":"## Keras NN Starter Kernel","febcf432":"Dropping numeric","002c1be7":"## Neural Network Model Details\n\nOur neural network will be fairly standard. We will use the embedding layer for categoricals and the numericals will go through feed forward dense layers. \n\nWe create our embedding layers such that we have as many rows as we had categories and the dimension of the embedding is the log1p + 1 of the number of categories. So this means that categorical variables with very high cardinality will have more dimensions but not signficantly more so the information will still be compressed down to only about 13 dimensions and the smaller number of categories will be only 2-3.\n\nWe will then pass the embeddings through a spatial dropout layer which will drop dimensions within the embedding across batches and then flatten and concatenate. Then we will concatenate this to the numerical features and apply batch norm and then add some more dense layers after. ","849ff8b3":"Dropping time since this likely isnt something we want our model to directly learn from","3267d4ce":"## Numerical and Categorical\nListing off and categorizing the various variables available to us. We have numerical and categoricals. We will treat both of these slightly differently later","97441063":"We already dropped a lot of these features because in some trial and error it was shown that these caused rapid overfitting for some reason or otherwise introduced unnecessary noise into the data. We will make sure we only list the features we actually have still in the df now","386bb35b":"We will iterate through epochs of the model and save the model weights if the score is an improvement upon previous best roc_auc_scores since this is competition metric. If the NN does not improve upon previous best after 4 epochs we will skip the rest of the training steps to save time. ","9b906cdc":"Not looking to seriously compete in this competition so figured I'd at least share the small experiments I was toying with. This kernel serves as just a starter to look at how to handle the various numerical and categorical variables in a NN with Keras. \n\nCore purpose of this kernel:\n* How to handle categorical and numerical variables in neural networks\n* Methods to normalize skewed numerical variables\n* Greedy feature selection via exclusion\n* Splitting based on time","c9fd7738":"## Greedy Feature Selection\n\nFirst we will train the NN with all categorical and numerical features in order to make a baseline\n\nAfter that we will greedily drop one feature at a time and see if it increases or decreases performance. If it increases upon dropping the feature then we will drop the feature. If it decreases then it will stay. "}}