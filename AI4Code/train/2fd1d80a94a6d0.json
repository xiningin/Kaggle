{"cell_type":{"4b1d71b1":"code","8454115d":"code","ef105f47":"code","b0851712":"code","c7b1bad6":"code","1f040f87":"code","f0177aaf":"code","39de3fdf":"code","03023c86":"code","9495c0f6":"code","bc293fc5":"code","e98bc56a":"code","488e691a":"code","d1ef0c2b":"code","ce9535c9":"code","94cdd57b":"code","6de8e831":"code","98166e34":"code","914e3679":"code","b92b6e03":"code","015831a2":"code","0fb9ce11":"code","ac059b7c":"code","40444e8f":"code","158a818b":"code","35742927":"code","1b851568":"code","2d83d31b":"markdown","c2dbd00a":"markdown","98246d1f":"markdown","40b306f0":"markdown","a7d63e4b":"markdown","b37d89a2":"markdown"},"source":{"4b1d71b1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8454115d":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")\ndf = pd.concat([df_train, df_test],axis=0, sort='False', ignore_index = True)","ef105f47":"df = df[df.columns.difference(['Id'])]","b0851712":"df_train.head()","c7b1bad6":"df_test[\"Id\"].head()","1f040f87":"ids = df_test[\"Id\"]","f0177aaf":"df = df.fillna(0)","39de3fdf":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten","03023c86":"#Encoding categorical data\ndf = pd.get_dummies(df)","9495c0f6":"print(\"Shape of our dataset is {}\".format(df.shape))","bc293fc5":"df.tail(3)","e98bc56a":"df_train = df.iloc[:1460,:]\ndf_test = df.iloc[1460:,:]\nX_train = df_train[df_train.columns.difference(['SalePrice'])].values\ny_train = df_train[['SalePrice']].values\nX_test = df_test[df_test.columns.difference(['SalePrice'])].values","488e691a":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer\npt_X = PowerTransformer(method='yeo-johnson', standardize=False)\nsc_y = StandardScaler()\nsc_X = StandardScaler()\ny_train = sc_y.fit_transform(y_train)\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\n","d1ef0c2b":"y_train[:5]","ce9535c9":"y_t = y_train.flatten()\ny_t.shape","94cdd57b":"import lightgbm as lgb\n# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train, y_t)\n# specify your configurations as a dict\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': {'l2', 'l1'},\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': 0\n}\n\nprint('Starting training...')\n# train\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=250)\n# predict\nlgbm_prediction_tr = gbm.predict(X_train, num_iteration=gbm.best_iteration)\nlgbm_prediction_te = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n","6de8e831":"df_train[\"lgb\"] = lgbm_prediction_tr\ndf_test[\"lgb\"] = lgbm_prediction_te","98166e34":"df_train.head()","914e3679":"df = pd.concat([df_train, df_test],axis=0, sort='False', ignore_index = True)\ndf = df[df.columns.difference(['Id'])]\ndf_train = df.iloc[:1460,:]\ndf_test = df.iloc[1460:,:]\nX_train = df_train[df_train.columns.difference(['SalePrice'])].values\ny_train = df_train[['SalePrice']].values\nX_test = df_test[df_test.columns.difference(['SalePrice'])].values\npt_X = PowerTransformer(method='yeo-johnson', standardize=False)\nsc_y = StandardScaler()\nsc_X = StandardScaler()\ny_train = sc_y.fit_transform(y_train)\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)","b92b6e03":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom keras.callbacks import ModelCheckpoint","015831a2":"#In\u0131tialising the ANN\nmodel = Sequential()\n#Adding the input layer and first hidden layer\nmodel.add(Dense(units =480, kernel_initializer='random_uniform', activation= 'tanh', \n                input_dim=X_train.shape[1]))\n#Add the second hidden layer\nmodel.add(Dense(units =480, kernel_initializer='random_uniform', activation= 'tanh'))\n#Add the second hidden layer\n\nmodel.add(Dense(units =10, kernel_initializer='random_uniform', activation= 'relu'))\n#The output layer\nmodel.add(Dense(units =1, kernel_initializer='random_uniform', activation= 'elu'))\n\n#Compiling the ANN\nopt = keras.optimizers.Adam(lr=0.0015, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\nmodel.compile(optimizer=opt, loss='mean_squared_logarithmic_error', metrics=['mse'])\n#Fitting the ANN to the training set\nmodel_filepath = 'min_vl_model.h5'\ncheckpoint = ModelCheckpoint(model_filepath, monitor = 'val_loss', verbose=1, save_best_only = True, mode='min' )\nmodel.fit(X_train,y_train, validation_split=0.07, batch_size=32, nb_epoch=3000, callbacks=[checkpoint])\nmodel.load_weights(model_filepath)","0fb9ce11":"y_pred = model.predict(X_test)","ac059b7c":"y_pred = sc_y.inverse_transform(y_pred)","40444e8f":"y_pred","158a818b":"y_pred = pd.DataFrame(y_pred)\ny_pred[\"Id\"] = ids","35742927":"y_pred = y_pred.rename(columns={0: \"SalePrice\"})\ny_pred = y_pred[[\"Id\",\"SalePrice\"]]\ny_pred.to_csv(\"Submission.csv\", index=False)","1b851568":"y_pred.head()","2d83d31b":"Percantage of the empty values for each column is shown above. I feel like we can fill na's with zeros instead of dropping them","c2dbd00a":"Now the result of lightgbm will be added to dataset","98246d1f":"**Hello,**\n\n**In this kernel, I will try maybe some kind of weird thing. Firstly I will imply a tree based model and will use the output of this model as it was a column in the dataset and try to implement a basic neural network.**\n\n**I am hoping to see that this strategy will give me an acceptable outcome**","40b306f0":"SO, let's repeat it all","a7d63e4b":"# Lenght of train is 1460\n\nNow the current dataframe is like:","b37d89a2":"I couldnt find a way to hide output of the cell below."}}