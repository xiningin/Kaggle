{"cell_type":{"c42cc61a":"code","b2f78a76":"code","b6ff0ecb":"code","32359ca0":"code","f7632c34":"code","05672cfb":"code","c61d0520":"code","c1304727":"code","c193a461":"code","7c5d4813":"code","bbe7df75":"code","36c5b53e":"code","890262e9":"code","65bad416":"code","7d3abe46":"code","42b4a547":"code","598165ac":"code","594641cb":"code","23c26978":"code","62dddd0f":"code","bc962833":"code","c4b4d315":"code","28d86305":"code","02abe3fe":"code","5ee496e3":"code","2dc334e6":"code","e50883e3":"code","91c3ed88":"code","c697636e":"code","708cddf3":"code","f7092cc9":"code","8582f674":"code","1fddf772":"code","b230893e":"code","48710ee9":"code","47a5a537":"code","7336ea26":"code","697da5f9":"code","a233879b":"code","13ceda32":"code","be366972":"code","ba61b613":"code","f0ba014d":"code","70afe52e":"code","63d2c2e5":"code","510eb08f":"code","ade9ec9d":"code","1a1be829":"code","10806ae8":"code","35b1f2a2":"code","8ccb995e":"code","18b952d6":"code","198f921b":"code","e8fbf05a":"code","370e98a1":"code","d113189f":"code","e09b3b01":"markdown","c442af6d":"markdown","72517c58":"markdown","9ab1d5d9":"markdown","cc7553aa":"markdown","90ebd909":"markdown","04890db1":"markdown","c8dfc72d":"markdown"},"source":{"c42cc61a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport nltk\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport scikitplot as skplt\n\nfrom keras import callbacks\nfrom keras.layers import Bidirectional\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import LSTM\nfrom keras.models import Sequential\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', 20)\npd.set_option('display.max_rows', 10)\n\nplt.rc('figure', figsize=(10, 7))\n\nnum_epoch = 5","b2f78a76":"data = pd.read_csv('..\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv')\ndata.drop(columns=\"Unnamed: 0\", axis=1, inplace=True)\ndata","b6ff0ecb":"department_list = data['Department Name'].dropna().unique()\ndepartment_list = [x.lower() for x in department_list]\ndepartment_list","32359ca0":"class_list = data['Class Name'].dropna().unique()\nclass_list = [x.lower() for x in class_list]\nclass_list","f7632c34":"department_and_class = np.concatenate((department_list, class_list, ['dress', 'petite', 'petit', 'skirt', 'shirt', 'jacket', 'intimate', 'blouse', 'coat', 'sweater']), axis=0)\ndepartment_and_class","05672cfb":"review_data = data[['Review Text','Recommended IND']]\nreview_data","c61d0520":"review_data.isnull().sum().sort_values()","c1304727":"review_data.dropna(axis=0,inplace=True)","c193a461":"review_data","7c5d4813":"#import for test train split and vect\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef tfidf(data):\n    tfidf_vectorizer =TfidfVectorizer(min_df=3,  max_features=None, \n             analyzer='word', use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\n\n    train = tfidf_vectorizer.fit_transform(data)\n\n    return train, tfidf_vectorizer","bbe7df75":"from sklearn.decomposition import  TruncatedSVD\nimport matplotlib\nimport matplotlib.patches as mpatches\n\n\ndef plot_LSA(test_data, test_labels):\n        #reduce into 2 dimensions using svd \n        lsa = TruncatedSVD(n_components=2)\n        #fits to the train data\n        lsa.fit(test_data)\n        lsa_scores = lsa.transform(test_data)\n        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        colors = ['orange','blue','blue']\n        if plt:\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            red_patch = mpatches.Patch(color='orange', label='Recommended IND = 0')\n            blue_patch = mpatches.Patch(color='blue', label='Recommended IND = 1')\n            plt.legend(handles=[red_patch, blue_patch], prop={'size': 12})","36c5b53e":"X = review_data[\"Review Text\"]\ny = review_data[\"Recommended IND\"]\n\n# Create sequence\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(review_data['Review Text'])\nvocabulary_size = len(tokenizer.word_index) + 1\nprint(vocabulary_size)\n\n# \u9650\u5236\u6700\u957f\u957f\u5ea6\u4e3a70\uff0c\u8fc7\u957f\u622a\u65ad\uff0c\u8fc7\u77ed\u5c31\u5728\u540e\u65b9\uff08post\uff09\u8865\u9f50\nmax_length = 70\n\nsequences = tokenizer.texts_to_sequences(X)\nfeatures = pad_sequences(sequences, maxlen=max_length, padding='post')","890262e9":"from string import punctuation\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport re\n\n# if you don't have stopwords and have some error, please use the download code bollow!\n# nltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n### Text Normalizing function. Part of the following function was taken from this link. \ndef clean_text(text):\n    \n    ## Remove puncuation\n    text = text.translate(punctuation)\n    \n    ## Convert words to lower case and split them\n    text = text.lower().split()\n    \n    ## Remove stop words\n    text = [w for w in text if not w in stop_words]\n    \n    text = \" \".join(text)\n    ## Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n#     text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    ## Stemming\n    text = text.split()\n    stemmer = PorterStemmer()\n    stemmed_words = [stemmer.stem(word) for word in text]\n    text = \" \".join(stemmed_words)\n    return text","65bad416":"review_data['Review Text'] = review_data['Review Text'].map(lambda x: clean_text(x))","7d3abe46":"review_data","42b4a547":"from keras.utils import to_categorical\n\nX = review_data[\"Review Text\"]\ny = review_data[\"Recommended IND\"]\n\n# Create sequence\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(review_data['Review Text'])\nvocabulary_size = len(tokenizer.word_index) + 1\nprint(vocabulary_size)","598165ac":"sequences = tokenizer.texts_to_sequences(review_data['Review Text'])\nnp.max([len(x) for x in sequences])","594641cb":"# \u9650\u5236\u6700\u957f\u957f\u5ea6\u4e3a70\uff0c\u8fc7\u957f\u622a\u65ad\uff0c\u8fc7\u77ed\u5c31\u5728\u540e\u65b9\uff08post\uff09\u8865\u9f50\nmax_length = 70\npadded_features = pad_sequences(sequences, maxlen=max_length, padding='post')","23c26978":"plot_LSA(padded_features, y)\nplt.show()","62dddd0f":"from scipy import interp\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\nfrom sklearn.metrics import roc_curve, auc\n\ndef plot_roc(n_classes, y_test, y_score, title, class_name_list):\n    # Plot linewidth.\n    lw = 2\n\n    y_test = sentiment_test[1]\n    y_score = test_score\n    # Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n    # Compute macro-average ROC curve and ROC area\n\n    # First aggregate all false positive rates\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n    # Then interpolate all ROC curves at this points\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(n_classes):\n        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n    # Finally average it and compute AUC\n    mean_tpr \/= n_classes\n\n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n    # Plot all ROC curves\n    plt.figure(1)\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n             label='micro-average ROC curve (area = {0:0.2f})'\n                   ''.format(roc_auc[\"micro\"]),\n             color='deeppink', linestyle=':', linewidth=4)\n\n    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n             label='macro-average ROC curve (area = {0:0.2f})'\n                   ''.format(roc_auc[\"macro\"]),\n             color='navy', linestyle=':', linewidth=4)\n\n    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n    for i, color in zip(range(n_classes), colors):\n        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n                 label='ROC curve of class {0} (area = {1:0.2f})'\n                 ''.format(class_name_list[i], roc_auc[i]))\n\n    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(title)\n    plt.legend(loc=\"lower right\")\n    plt.show()","bc962833":"from sklearn import model_selection\n\nX_train, X_val, y_train, y_val = model_selection.train_test_split(review_data['Review Text'], review_data['Recommended IND'], test_size=0.2, random_state=666)\nX_test, X_val, y_test, y_val = model_selection.train_test_split(X_val, y_val, test_size=0.5, random_state=888)","c4b4d315":"print(len(X_train))\nprint(len(X_val))\nprint(len(X_test))","28d86305":"from imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler()\nros = RandomOverSampler()","02abe3fe":"X_over, X_cat, y_over, y_cat = model_selection.train_test_split(X_train, y_train, test_size=0.2, random_state=888)\nX_over, X_under, y_over, y_under = model_selection.train_test_split(X_over, y_over, test_size=0.5, random_state=888)","5ee496e3":"print(X_under.shape, X_over.shape, y_under.shape, y_over.shape, X_cat.shape, y_cat.shape)","2dc334e6":"ros_sequences = tokenizer.texts_to_sequences(X_over)\nros_features = pad_sequences(ros_sequences, maxlen=max_length, padding='post')\ntrain_X_ros, train_y_ros = ros.fit_sample(ros_features, y_over)","e50883e3":"print(train_X_ros.shape, train_y_ros.shape)\ntrain_y_ros.value_counts()","91c3ed88":"rus_sequences = tokenizer.texts_to_sequences(X_under)\nrus_features = pad_sequences(rus_sequences, maxlen=max_length, padding='post')\ntrain_X_rus, train_y_rus = rus.fit_sample(rus_features, y_under)","c697636e":"print(train_X_rus.shape, train_y_rus.shape)\ntrain_y_rus.value_counts()","708cddf3":"print(X_cat.shape, y_cat.shape)\ny_cat.value_counts()","f7092cc9":"y_cat.value_counts()","8582f674":"cat_0_idx = y_cat[y_cat == 0]\ncat_0_idx = list(cat_0_idx.keys())\ncat_1_idx = y_cat[y_cat == 1]\ncat_1_idx = list(cat_1_idx.keys())","1fddf772":"X_cat_0 = X_cat[cat_0_idx]\nX_cat_1 = X_cat[cat_1_idx]","b230893e":"count_0 = len(cat_0_idx)","48710ee9":"import random\nnew_X_0 = []\nfor idx in cat_0_idx:\n    cur = X_cat_0[idx]\n    p = random.randint(0, 1)\n    cur_idx = len(cur) \/\/ 2\n    cur = cur[:cur_idx] if p == 0 else cur[cur_idx:]\n    new_X_0.append(cur)\nnew_X_0.extend(list(X_cat_0.values))\n\nnew_X_1 = []\nfor idx in cat_1_idx:\n    cur = X_cat_1[idx]\n    p = random.randint(0, 1)\n    cur_idx = len(cur) \/\/ 2\n    cur = cur[:cur_idx] if p == 0 else cur[cur_idx:]\n    new_X_1.append(cur)\nnew_X_1 = random.sample(new_X_1, count_0)\nnew_X_1.extend(random.sample(list(X_cat_1.values), count_0))","47a5a537":"print(len(new_X_0))\nprint(len(new_X_1))","7336ea26":"len(new_X_0 + new_X_1)","697da5f9":"X_cat = pd.Series(new_X_0 + new_X_1)\ny_cat = pd.Series([0] * count_0 * 2 + [1] * count_0 * 2)","a233879b":"cat_sequences = tokenizer.texts_to_sequences(X_cat)\ncat_features = pad_sequences(cat_sequences, maxlen=max_length, padding='post')","13ceda32":"cat_features.shape","be366972":"cat_features","ba61b613":"train_X_rus","f0ba014d":"train_X_ros","70afe52e":"features_all = np.concatenate((cat_features, train_X_rus, train_X_ros))","63d2c2e5":"features_all.shape","510eb08f":"y_all = y_cat.append(train_y_rus).append(train_y_ros)","ade9ec9d":"labels_all = to_categorical(y_all)\nlabels_all[0]\nprint(labels_all.shape)","1a1be829":"plot_LSA(features_all, y_all)\nplt.show()","10806ae8":"val_sequences = tokenizer.texts_to_sequences(X_val)\nval_features = pad_sequences(val_sequences, maxlen=max_length, padding='post')\nval_labels = to_categorical(y_val)\nprint(val_features.shape, val_labels.shape)\n\ntest_sequences = tokenizer.texts_to_sequences(X_test)\ntest_features = pad_sequences(test_sequences, maxlen=max_length, padding='post')\ntest_labels = to_categorical(y_test)\nprint(test_features.shape, test_labels.shape)","35b1f2a2":"model = Sequential()\n\nclass_weight = {0: 10, 1: 1}\ne = Embedding(vocabulary_size, 100, input_length=max_length, trainable=True)\nmodel.add(e)\nmodel.add(Bidirectional(LSTM(128, dropout=0.5, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(256, dropout=0.5)))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.fit(features_all, labels_all, epochs=num_epoch, batch_size=256, verbose=1,\n          validation_data=(val_features, val_labels), class_weight=class_weight, shuffle=True)\n\nscore = model.evaluate(test_features, test_labels, verbose=1)\n\nprint('loss : {}, acc : {}'.format(score[0], score[1]))","8ccb995e":"test_score = model.predict(test_features)\ntest_predictions = np.argmax(test_score, axis=1)\n\nclass_names = ['(0) Not recommended class', '(1) Recommended class']\nreport = classification_report(np.argmax(test_labels, axis=1), test_predictions, target_names=class_names)\nmatrix = pd.DataFrame(confusion_matrix(y_true=np.argmax(test_labels, axis=1), y_pred=test_predictions), \n                                        index=class_names, columns=class_names)\nprint(matrix)\nprint(report)\nf1_score(np.argmax(test_labels, axis=1), test_predictions, average='micro')   ","18b952d6":"skplt.metrics.plot_roc(np.argmax(test_labels, axis=1), model.predict_proba(test_features),\n                      title='ROC Curves - hyper') ","198f921b":"model = Sequential()\n\n# \u56e0\u4e3a\u6570\u636e\u6bd4\u4f8b\u5df2\u7ecf\u662f1\uff1a1\u4e86\uff0c\u6240\u4ee5\u4e0d\u9700\u8981\u518d\u8c03\u63a7\nclass_weight = {0: 3, 1: 1}\ne = Embedding(vocabulary_size, 100, input_length=max_length, trainable=True)\nmodel.add(e)\nmodel.add(Bidirectional(LSTM(128, dropout=0.5, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(256, dropout=0.5)))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.fit(features_all, labels_all, epochs=num_epoch, batch_size=256, verbose=1,\n          validation_data=(val_features, val_labels), class_weight=class_weight, shuffle=True)\n\nscore = model.evaluate(test_features, test_labels, verbose=1)","e8fbf05a":"score = model.evaluate(test_features, test_labels, verbose=1)\n\nprint('loss : {}, acc : {}'.format(score[0], score[1]))","370e98a1":"test_score = model.predict(test_features)\ntest_predictions = np.argmax(test_score, axis=1)\n\nclass_names = ['(0) Not recommended class', '(1) Recommended class']\nreport = classification_report(np.argmax(test_labels, axis=1), test_predictions, target_names=class_names)\nmatrix = pd.DataFrame(confusion_matrix(y_true=np.argmax(test_labels, axis=1), y_pred=test_predictions), \n                                        index=class_names, columns=class_names)\nprint(matrix)\nprint(report)\nf1_score(np.argmax(test_labels, axis=1), test_predictions, average='micro')   ","d113189f":"skplt.metrics.plot_roc(np.argmax(test_labels, axis=1), model.predict_proba(test_features),\n                      title='ROC Curves - hyper') ","e09b3b01":"# Reference\n1. https:\/\/medium.com\/@sabber\/classifying-yelp-review-comments-using-lstm-and-word-embeddings-part-1-eb2275e4066b\n2. https:\/\/www.kaggle.com\/rajmehra03\/a-detailed-explanation-of-keras-embedding-layer\n3. https:\/\/machinelearningmastery.com\/develop-bidirectional-lstm-sequence-classification-python-keras\/","c442af6d":"# Review Text Feature Transformation","72517c58":"# Basic Visualization","9ab1d5d9":"\u5f31\u667a\u7b97\u6cd51\uff1a0.4undersampling\uff0c0.4oversamplling\uff0c0.2\u662f\u53d6\u968f\u673a\u53d6\u53e5\u5b50\u7684\u524d\u534a\u53e5\u6216\u8005\u540e\u534a\u53e5\u751f\u6210\u65b0\u7684\u6570\u636e\u3002\u8fd9\u6837\u7684\u7406\u7531\u662f\uff0c\u6709\u90e8\u5206\u7528\u6237\u4f1a\u5728\u8bc4\u8bba\u4e00\u5f00\u59cb\u6216\u8005\u6700\u540e\u9762\u5f3a\u70c8\u8868\u8fbe\u81ea\u5df1\u7684\u60c5\u611f\u3002\u6240\u4ee5\u4f7f\u7528\u8fd9\u6837\u7684\u65b9\u5f0f\u6765\u505a\u6570\u636e\u589e\u5f3a\u6709\u5229\u4e8e\u5206\u7c7b\u3002","cc7553aa":"# Recommended IND Classification","90ebd909":"Tokenizer\u662f\u4e00\u4e2a\u7528\u4e8e\u5411\u91cf\u5316\u6587\u672c\uff0c\u6216\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u5e8f\u5217\uff08\u5373\u5355\u8bcd\u5728\u5b57\u5178\u4e2d\u7684\u4e0b\u6807\u6784\u6210\u7684\u5217\u8868\uff0c\u4ece1\u7b97\u8d77\uff09\u7684\u7c7b\u3002\n\nword_index: \u5b57\u5178\uff0c\u5c06\u5355\u8bcd\uff08\u5b57\u7b26\u4e32\uff09\u6620\u5c04\u4e3a\u5b83\u4eec\u7684\u6392\u540d\u6216\u8005\u7d22\u5f15\u3002\u4ec5\u5728\u8c03\u7528fit_on_texts\u4e4b\u540e\u8bbe\u7f6e\u3002\n\ntexts_to_sequences(texts)\n\ntexts\uff1a\u5f85\u8f6c\u4e3a\u5e8f\u5217\u7684\u6587\u672c\u5217\u8868\n\n\u8fd4\u56de\u503c\uff1a\u5e8f\u5217\u7684\u5217\u8868\uff0c\u5217\u8868\u4e2d\u6bcf\u4e2a\u5e8f\u5217\u5bf9\u5e94\u4e8e\u4e00\u6bb5\u8f93\u5165\u6587\u672c\n\npad_sequences \u5c06\u591a\u4e2a\u5e8f\u5217\u622a\u65ad\u6216\u8865\u9f50\u4e3a\u76f8\u540c\u957f\u5ea6\u3002\n\n\u8be5\u51fd\u6570\u5c06\u4e00\u4e2a num_samples \u7684\u5e8f\u5217\uff08\u6574\u6570\u5217\u8868\uff09\u8f6c\u5316\u4e3a\u4e00\u4e2a 2D Numpy \u77e9\u9635\uff0c\u5176\u5c3a\u5bf8\u4e3a (num_samples, num_timesteps)\u3002 num_timesteps \u8981\u4e48\u662f\u7ed9\u5b9a\u7684 maxlen \u53c2\u6570\uff0c\u8981\u4e48\u662f\u6700\u957f\u5e8f\u5217\u7684\u957f\u5ea6\u3002\n\n\u6bd4 num_timesteps \u77ed\u7684\u5e8f\u5217\u5c06\u5728\u672b\u7aef\u4ee5 value \u503c\u8865\u9f50\u3002\n\n\u6bd4 num_timesteps \u957f\u7684\u5e8f\u5217\u5c06\u4f1a\u88ab\u622a\u65ad\u4ee5\u6ee1\u8db3\u6240\u9700\u8981\u7684\u957f\u5ea6\u3002\u8865\u9f50\u6216\u622a\u65ad\u53d1\u751f\u7684\u4f4d\u7f6e\u5206\u522b\u7531\u53c2\u6570 pading \u548c truncating \u51b3\u5b9a\u3002\n\n\u5411\u524d\u8865\u9f50\u4e3a\u9ed8\u8ba4\u64cd\u4f5c\u3002","04890db1":"\u867d\u7136\u597d\u8bc4\u7684recall\u548c\u5dee\u8bc4\u7684prec\u90fd\u4e0b\u964d\u4e86\uff0c\u597d\u8bc4\u7684prec\u548c\u5dee\u8bc4\u7684recall\u90fd\u5f88\u9ad8\uff0c\u4f46\u662f\u6211\u89c9\u5f97\u662f\u5408\u7406\u7684\uff1a\u5982\u679c\u8981\u505a\u8bc4\u8bba\u7cbe\u9009\uff0c\u6bd4\u5982\u628a\u597d\u8bc4\u653e\u5728\u524d\u9762\uff0c\u90a3\u4e48\u597d\u8bc4prec\u9ad8\u662f\u5408\u7406\u7684\uff0c\u8bf4\u660e\u7ed9\u7528\u6237\u770b\u7684\u8bc4\u8bba\u57fa\u672c\u90fd\u662f\u597d\u8bc4\uff1b\u5982\u679c\u5e97\u5bb6\u60f3\u770b\u8206\u60c5\u5206\u6790\uff0c\u5c31\u662f\u60f3\u770b\u81ea\u5df1\u5e97\u94fa\u7684\u5dee\u8bc4\uff0c\u90a3\u4e48\u5dee\u8bc4recall\u9ad8\u662f\u5408\u7406\u7684\uff0c\u8bf4\u660e\u6b64\u65f6\u7ed9\u5e97\u5bb6\u770b\u7684\u57fa\u672c\u90fd\u662f\u5dee\u8bc4\u3002\n\u540e\u671f\u4e0d\u77e5\u9053\u591a\u8bad\u7ec3\u591a\u51e0\u6b21\uff0c\u8fd8\u6709\u8c03\u6574\u4e09\u79cdresample\u65b9\u5f0f\u7684\u6bd4\u91cd\uff0c\u4e0d\u77e5\u9053\u6548\u679c\u5982\u4f55\u3002","c8dfc72d":"# Data Structure"}}