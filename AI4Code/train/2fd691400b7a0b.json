{"cell_type":{"a52a76bb":"code","03a680be":"code","3e2a8e66":"code","dad92e2e":"code","cc9794be":"code","e5cdd151":"code","1da7e5d8":"code","2e3d3484":"code","9a1429a2":"code","e597f8fc":"markdown","223e12d4":"markdown","95ca5a1d":"markdown","6b15e167":"markdown","b1a13b6b":"markdown","0c69bebb":"markdown","373f664c":"markdown","ef08e387":"markdown","9d62490a":"markdown"},"source":{"a52a76bb":"# Loading required packages\n# Environment setup -------------------------------------------------------\n# importing general purpose libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nimport dfply as dp\nimport random\nimport warnings\nfrom sklearn import datasets\nimport seaborn as sb\n\n# importing model selection and evaluation libraries\n\n# train-test-validation dataset creation\nfrom sklearn.model_selection import train_test_split\n\n# data normalization\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\n# Pipeline\nfrom sklearn.pipeline import Pipeline\n\n# feature selection\nfrom mlxtend.feature_selection import SequentialFeatureSelector\nfrom mlxtend.plotting import plot_sequential_feature_selection\n\n# hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n# crossvalidation\nfrom sklearn.model_selection import cross_val_score, KFold\n\n# Linear classifiers\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.svm import SVC\n\n# Non-parametric classifiers\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n#\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn.metrics.classification import precision_score, recall_score\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.metrics import precision_recall_curve, confusion_matrix\n\nfrom keras import Sequential\nfrom sklearn.ensemble import voting_classifier\n","03a680be":"def create_pipeline(norm, model):\n    if norm == 1:\n        scale = StandardScaler()\n        pipe = Pipeline([('norm', scale), ('reg', model)])\n    elif norm == 2:\n        scale = MinMaxScaler()\n        pipe = Pipeline([('norm', scale), ('reg', model)])\n    else:\n        pipe = Pipeline([('reg', model)])\n    return pipe","3e2a8e66":"\ndef select_features(model, X_train, Y_train, selection,\n                    score_criteria, see_details, norm=0):\n    pipe = create_pipeline(norm, model)\n    sfs = SequentialFeatureSelector(pipe,\n                                    forward=selection,\n                                    k_features='best',\n                                    scoring=score_criteria,\n                                    verbose=see_details)\n    sfs = sfs.fit(X_train, Y_train)\n    return list(sfs.k_feature_idx_)","dad92e2e":"def run_model(model, param_grid, X_train, Y_train,\n              X, Y, score_criteria, folds,\n              see_details, norm=0):\n    pipe = create_pipeline(norm, model)\n    model_grid = GridSearchCV(pipe,\n                              param_grid,\n                              cv=folds,\n                              scoring=score_criteria,\n                              verbose=see_details)\n    model_grid.fit(X_train, Y_train)\n\n    pipe = create_pipeline(norm, model_grid.best_estimator_)\n    return model_grid.best_estimator_\n","cc9794be":"def get_model_eval(model, X_train, Y_train, X_test, Y_test):\n    cm = confusion_matrix(Y_test, model.predict(X_test))\n    t1, f1, t0, f0 = cm[1, 1], cm[1, 0], cm[0, 0], cm[0, 1]\n    precision = precision_score(Y_test, model.predict(X_test))\n    recall = recall_score(Y_test, model.predict(X_test))\n    return pd.Series([model,\n                      (t1 + t0) \/ (t1 + t0 + f1 + f0),\n                      precision,\n                      recall,\n                      2 * precision * recall \/ (precision + recall),\n                      -1 if type(model.steps[1][1]) == RidgeClassifier else roc_auc_score(Y_test, model.predict_proba(X_test)[:, 1])])\n","e5cdd151":"# Global model paramater grid dictionary------------------------------------\n# Change your hyperparameter ranges for grid search in this section\nPARAM_DICT = {\n  LogisticRegression: {\n    'reg__tol': [1e-2, 1e-4, 1e-6],\n    'reg__fit_intercept': [True, False],\n    'reg__penalty': ['l1', 'l2']\n  },\n  RidgeClassifier: {\n    'reg__alpha': [0.1, 1, 100],\n    'reg__copy_X': [True, False],\n    'reg__fit_intercept': [True, False],\n    'reg__tol': [0.1, 1],\n    'reg__solver': ['auto', 'svd', 'cholesky', 'lsqr',\n      'sparse_cg', 'sag', 'saga'\n    ]\n  },\n\n  KNeighborsClassifier: {\n    'reg__n_neighbors': [5, 30, 100]\n  },\n  GaussianNB: {\n  },\n\n  DecisionTreeClassifier: {\n    'reg__max_depth': [5, 10, 20],\n    'reg__max_features': [0.3, 0.7, 1.0],\n    'reg__max_leaf_nodes': [10, 50, 100],\n    'reg__splitter': ['best', 'random']\n  },\n\n  BaggingClassifier: {\n    'reg__bootstrap': [True, False],\n    'reg__bootstrap_features': [True, False],\n    'reg__max_features': [0.3, 0.7, 1.0],\n    'reg__max_samples': [0.3, 0.7, 1.0],\n    'reg__n_estimators': [10, 50, 100]\n  },\n  RandomForestClassifier: {\n    'reg__bootstrap': [True, False],\n    'reg__max_depth': [5, 10, 20],\n    'reg__max_features': [0.3, 0.7, 1.0],\n    'reg__max_leaf_nodes': [10, 50, 100],\n    'reg__min_impurity_decrease': [0, 0.1, 0.2],\n    'reg__n_estimators': [10, 50, 100]\n  },\n\n  SVC: {\n    'reg__C': [10 ** -3, 1, 1000],\n    'reg__kernel': ['linear', 'poly', 'rbf'],\n    'reg__shrinking': [True, False],\n    'reg__probability': [True]\n  },\n\n  GradientBoostingClassifier: {\n    'reg__learning_rate': [0.1, 0.2, 0.5],\n    # 'reg__loss': ['ls', 'lad', 'huber', 'quantile'],\n    'reg__max_depth': [10, 20, 50],\n    'reg__max_features': [0.5, 0.8, 1.0],\n    'reg__max_leaf_nodes': [10, 50, 100],\n    'reg__min_impurity_decrease': [0, 0.1, 0.2],\n    'reg__min_samples_leaf': [5, 10, 20],\n    'reg__min_samples_split': [5, 10, 20],\n    'reg__n_estimators': [10, 50, 100]\n  },\n  XGBClassifier: {\n    'reg__booster': ['gbtree', 'gblinear', 'dart'],\n    'reg__learning_rate': [0.2, 0.5, 0.8],\n    'reg__max_depth': [5, 10, 20],\n    'reg__n_estimators': [10, 50, 100],\n    'reg__reg_alpha': [0.1, 1, 10],\n    'reg__reg_lambda': [0.1, 1, 10],\n    'reg__subsample': [0.3, 0.5, 0.8],\n    'reg__probability': [True]\n  }\n}","1da7e5d8":"# --------------------------------------------------------------------------\n# USER CONTROL PANEL, CHANGE THE VARIABLES, MODEL FORMS ETC. HERE\n\n# Read data here, define X (features) and Y (Target variable)\ndata = datasets.load_breast_cancer()\nX = pd.DataFrame(data['data'])\nX.columns = data['feature_names']\nY = data['target']\n\n# Specify size of test data (%)\nsize = 0.3\n\n# Set random seed for sampling consistency\nrandom.seed(100)\n\n# Set type of normalization you want to perform\n# 0 - No Normalization, 1 - Min-max scaling, 2 - Zscore scaling\nnorm = 1\n\n# Mention all model forms you want to run\nto_run = [DecisionTreeClassifier,\n          BaggingClassifier,\n          RandomForestClassifier,\n          GradientBoostingClassifier,\n          XGBClassifier,\n          SVC,\n          KNeighborsClassifier,\n          RidgeClassifier,\n          GaussianNB,\n          LogisticRegression]\n\n# Specify number of crossvalidation folds\nfolds = 2\n\n# Specify model selection criteria\n# Possible values are:\n# 'accuracy'\n# 'precision'\n# 'recall'\n# 'f1'\n# 'roc_auc'\n\nscore_criteria = 'accuracy'\n\n# Specify details of terminal output you'd like to see\n# 0 - No output, 1 - All details, 2 - Progress bar\n# Outputs might vary based on individual functions\nsee_details = 0\n\n","2e3d3484":"# --------------------------------------------------------------------------\n\n# Model execution part, results will be stored in the dataframe 'results'\n# Best model can be selected based on these criteria\n\nresults = pd.DataFrame(columns=['model', 'Accuracy', 'PrecisionLab1', 'RecallLab1',\n                                'FMeasureLab1', 'AUC'])\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=size)\n\nfor model in to_run:\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n        best_feat = select_features(model(), X_train, Y_train, True,\n                                    score_criteria, see_details, norm)\n        model = run_model(model(), PARAM_DICT[model],\n                          X_train.iloc[:, best_feat],\n                          Y_train,\n                          X.iloc[:, best_feat], Y,\n                          score_criteria, folds, see_details, norm)\n        stats = get_model_eval(model, X_train.iloc[:, best_feat], Y_train,\n                               X_test.iloc[:, best_feat], Y_test)\n        stats.index = results.columns\n        results = results.append(stats, ignore_index=True)","9a1429a2":"results['Form'] = [str(i).split()[-1].split('.')[-1] for i in to_run]\nsb.lmplot('RecallLab1', 'Accuracy', hue='Form', data=results, fit_reg=False)","e597f8fc":"# Analytics Building Blocks: Binary Classification\n### A modularized notebook to tune and compare 9 classification algorithms with minimal coding in a control panel fashion\n##### Author: Himanshu Kulkarni\n##### LinkedIn: https:\/\/www.linkedin.com\/in\/himanshu-kulkarni94\/\n##### GitHub: https:\/\/github.com\/himanshu0394\n\n### Introduction\n\nSimilar to the regression problems classification is one of the classic problems in analytics. Any classification can be converted to a regression problem by predicting the probability of belonging to a certain class for the data, but we will not get into that for the time being. Here goes my standard motive for sharing creating these notebooks: Many times while performing an analytics task, it is good to fail fast by testing different model forms to determine the most suitable model which provides a good balance of accuracy, complexity and execution efficiency based on the problem at hand. Some of the software such as RapidMiner provide this functionality. However, using a software product for this purpose results in a black-box approach in terms of tuning the model and exploring some of the intricacies. Hence I decided to create a simple python script with just-enough modularization and parameterization to enable testing and tuning many of the widely used classification algorithms with minimal changes in the code.\nThe summary of this notebook is as follows:\n\n### Objective:\n\nTo test, tune and compare various classification models with minimal manual intervention in Python.\nThe models included in this module are:\n\n* Logistic Regression\n* Ridge Classifier\n* K Nearest Neighbors\n* Decision Tree Classifier\n* Random Forest\n* Bagging (Using decision tree by default)\n* Gradient boosting\n* XGBoost\n* Support Vector Machines\n\n### User Proficiency:\n\nThe user should have an intuitive understanding of how each of these algorithms works along with a good understanding of how changing a particular hyper-parameter might impact the outcome. Basic understanding of python is required to be able to effectively utilize the code and further customize it based on requirements.\n\n### Key Modifiable Inputs:\n\nBelow are the key inputs (More details are provided for each input in the inline comments). These sections have been highlighted in the code with a note 'MAKE MODIFICATIONS HERE':\n\n* Input dataset for classification analysis: In this example, I have used 'breast cancer' dataset from pandas default datasets\n* Test data proportion: Between 0 to 1, default 0.3 (or 30%)\n* Normalization: 0\u200a-\u200aNo Normalization, 1\u200a-\u200aMin-max scaling, 2\u200a-\u200aZ-score scaling\n* List of model objects to test\n* Number of folds for grid-search (hyper-parameter tuning)\n* Scoring criteria to determine the best model (e.g. accuracy)\u200a-\u200amore details are provided in the code comments\n* Flag to see the level of detail on the terminal during model fit: 0\u200a-\u200aNo output, 1\u200a-\u200aAll details, 2\u200a-\u200aProgress bar\n* Hyper-parameter library: A global dictionary in the code that provides a set of hyper-parameters for each model form to tune on\n\n### General Execution Steps:\n\nAfter taking these inputs, the following actions are performed for each model form under consideration:\n* Forward feature selection\n* Normalization\n* Grid search for hyper-parameter tuning\n* Metric calculation for the best model\n\n\n### Output:\nA pandas data frame 'results' is created which provides the following metrics for each of the models you are testing for label 1. You might need to adjust your input data based on which class is more important to your analysis based on context.\n\n* Model details with most optimum hyper-parameters\n* Model accuracy\n* Precision and Recall\n* F-measure\n* The area under the receiver operating characteristics curve (AUC)\n\n### Important Note:\n\nThis module in no way deals with feature engineering and only performs feature selection based on the input data. It is highly important to perform effective feature engineering in order to improve results with any model. A user might observe one of the model forms giving better results than the other however overall performance of any model can be improved significantly with improvement in predictor variables.","223e12d4":"### Modules for various\u00a0tasks\n\nThe first function creates the pipeline for normalization and grid search based on conditions specified by the user in the control panel.","95ca5a1d":"The second function performs forward feature selection and returns the indices of best features.","6b15e167":"### User Control Panel For Key Inputs (MAKE MODIFICATIONS HERE)\n\nThe inputs to the modules can be changed here. This is the control panel for this script where all the variables mentioned in the introduction can be altered to test various scenarios. Please refer to the comments to understand the variables.","b1a13b6b":"### Conclusion\n\nIn the case of the cancer patient detection, it would be very important to detect all the patients, in formal terms, to have the recall as high as possible for our model. From the above graph, we can observe that there are 3 models which are able to achieve perfect recall: Ridge classifier, logistic regression and support vector machine. In this scenario, the best way forward would be to pick the model with the highest accuracy among these models which is SVM.\nSimilar to the example in regression notebook, the simpler models such as logistic regression and ridge classifier perform better than ensemble models.\u00a0\nI hope this module enables faster experimentation and provides an opportunity to build further customizations on top of it based on your needs!","0c69bebb":"The last function calculates all the relevant metrics for the best hyper-parameter combination and returns a pandas series of these metrics.","373f664c":"### Global Hyper-parameter Dictionary (MAKE MODIFICATIONS HERE)\n\nThis is the global dictionary for various model parameters for all the models in this module. Some default set of values have been populated in the code for typical ranges based on the cancer dataset. This dictionary contains some of the key hyper-parameters for each model and it is not exhaustive. Users are encouraged to visit scikit-learn documentation to get a list of all the parameters and add to the below dictionary according to their requirements.","ef08e387":"This function performs grid search for the provided parameter grid and returns the best model object.","9d62490a":"### Model Execution\n\nThis section iteratively finds the best set of the hyperparameters for each of the model specified by the user, calculates the metrics and populates results table for further analysis\/experimentation."}}