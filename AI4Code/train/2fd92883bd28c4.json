{"cell_type":{"74f9e94f":"code","72e72807":"code","968be287":"code","b0ee7bb5":"code","250793d8":"code","2593df19":"code","c62c46c4":"code","2ccd4b29":"code","ce4d0fd7":"code","eb4cc1d2":"code","7b78f71c":"code","9f4132fc":"code","5d1aeda7":"code","33552981":"code","796d85e6":"code","327a1a2f":"code","bf239f24":"code","58530dee":"code","feb86317":"code","2f3f4c40":"code","0ae6a762":"code","674568b9":"code","9bfb5ba0":"code","ca6ae9e4":"code","0a86eecd":"code","c1b08a17":"code","b1ba23bf":"code","1431db6b":"code","672bfa7e":"code","fd36b012":"code","9380e6ad":"code","ae69f99a":"code","f5f2e5ec":"code","5e0e3628":"code","a1d21007":"code","a7db1a94":"code","bf8e1d79":"code","d4faa3ad":"code","27361048":"code","042bd5da":"code","4e7e409b":"code","34ad75ca":"code","4b277fb3":"code","c50e342a":"code","b41f4548":"code","1900d683":"code","b0196a6b":"code","8e2f93bd":"code","42a8eb8c":"code","209c81ca":"code","a13b3183":"code","5d0a461a":"code","7231c1db":"code","089e2dd4":"code","fe44adb1":"code","13122cf0":"code","44522f9e":"code","38f4ea41":"code","d652702f":"code","f13f3edc":"code","5eddeef7":"code","6d7ae3cd":"markdown","1ab6f38c":"markdown","06bbe987":"markdown","b22df2e6":"markdown","6122e593":"markdown","962faeaf":"markdown","39445208":"markdown","ee68b2f8":"markdown","f6ed6474":"markdown","70fc0f6b":"markdown","7debaf9f":"markdown","63a466df":"markdown"},"source":{"74f9e94f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","72e72807":"tweets_test_df = pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/test.csv')\ntweets_train_df = pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv')","968be287":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","b0ee7bb5":"tweets_train_df.head()","250793d8":"tweets_train_df.info()","2593df19":"tweets_train_df.describe()","c62c46c4":"tweets_train_df['tweet']","2ccd4b29":"tweets_train_df.drop(['id'], axis=1, inplace=True)","ce4d0fd7":"tweets_train_df.head()","eb4cc1d2":"sns.heatmap(tweets_train_df.isnull(), yticklabels = False, cbar = False, cmap = 'Blues')","7b78f71c":"tweets_train_df.hist(bins= 30, figsize = (12,5), color = 'b')","9f4132fc":"# These plots clearly shows that its a complete unbalanced data.\nsns.countplot(x=tweets_train_df['label'] ,data=tweets_train_df)","5d1aeda7":"tweets_test_df.head()","33552981":"tweets_train_df['lengths'] = tweets_train_df['tweet'].apply(len)","796d85e6":"tweets_train_df.head()","327a1a2f":"#distribution of tweets\ntweets_train_df['lengths'].plot(bins=100, kind = 'hist')","bf239f24":"tweets_train_df.describe()","58530dee":"#Min length is 11, so let's see it\ntweets_train_df[tweets_train_df['lengths']==11]['tweet'].iloc[0]","feb86317":"# lets viwe the meesage with average length\ntweets_train_df[tweets_train_df['lengths']==85]","2f3f4c40":"# Now separting positive and negative tweets\npositive = tweets_train_df[tweets_train_df['label']==0]\npositive","0ae6a762":"negative = tweets_train_df[tweets_train_df['label']==1]\nnegative","674568b9":"# Plot the word cloud\nfrom wordcloud import WordCloud","9bfb5ba0":"sentences = tweets_train_df['tweet'].tolist()","ca6ae9e4":"#All tweets has been converted to a list\n#sentences","0a86eecd":"len(sentences)","c1b08a17":"#Joining sentences (combining all the sentences that we have)\nsentences_as_single_string = \" \".join(sentences)","b1ba23bf":"plt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(sentences_as_single_string))","1431db6b":"# Lets plot wordcloud of negative words.\nnegative_sentences = negative['tweet'].tolist()\nnegative_string = \" \".join(negative_sentences)","672bfa7e":"plt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(negative_string))","fd36b012":"import string\nstring.punctuation","9380e6ad":"sample = 'Hi! everyone :) ; enjoy learning real world example of NLP !.....'","ae69f99a":"sample_punc_removed = [char   for char in sample if char not in string.punctuation]","f5f2e5ec":"sample_punc_removed","5e0e3628":"#Now join again\ntest_punc_removed_string = ''.join(sample_punc_removed)\ntest_punc_removed_string","a1d21007":"# Second and efficient method\nout = sample.translate(str.maketrans('', '', string.punctuation))\nout","a7db1a94":"# Third and basic method\npunc_removed = []\nfor char in sample:\n    if char not in string.punctuation:\n        punc_removed.append(char)\n        \npunc_removed_join = ''.join(punc_removed)\npunc_removed_join","bf8e1d79":"# The Question is what are stopwords, so lets download and plot them using Natural languae toolkit\nimport nltk #Natural language toolkit\nnltk.download('stopwords')","d4faa3ad":"#Lets import stopword and see the common words stored there. These are words that don't convey any specific information\nfrom nltk.corpus import stopwords\nstopwords.words('english')","27361048":"# Lets remove common words and retain only unique words\ntest_punc_removed_string_clean = [word for word in test_punc_removed_string.split() if word.lower() not in stopwords.words('english')]","042bd5da":"test_punc_removed_string_clean","4e7e409b":"# Lets try Pipeline approach to accomplish removal of punctuation and stopwords\ntest_sample = 'A sample to learn,; that how can we remove punctuations and stopwords in a pipeline fashion!!!'\n","34ad75ca":"pipe_punc_removed_cleaned = [char for char in test_sample if char not in string.punctuation]\npipe_punc_removed_cleaned = ''.join(pipe_punc_removed_cleaned)\npipe_punc_removed_cleaned = [word for word in pipe_punc_removed_cleaned.split() if word.lower() not in stopwords.words('english')]\npipe_punc_removed_cleaned","4b277fb3":"# This will take unique words utilized in text as features, and then count that how many time each word is utilized in that sentence. \nfrom sklearn.feature_extraction.text import CountVectorizer\nsample_new = ['This is first method.', 'This method is the second method.', 'This new one is the third one.' ]","c50e342a":"vectorizer = CountVectorizer()\nX = vectorizer.fit_transform(sample_new)","b41f4548":"#Lets see the extracted feature names (unique words)\nprint (vectorizer.get_feature_names())","1900d683":"X","b0196a6b":"#We can see that in first sentence, only four features (unique words) are present there (first three and last feature).\n#In second sentence of sample_new, word method is repeated two times, so we can see 2 at corresponding feature position\nprint(X.toarray())","8e2f93bd":"# We can see with following example that Countvectroizer always convert each character to lower case before transforming.\nsecond_sample = ['Hello World.', 'Hello Hello World', 'Hello World world world']\nXX = vectorizer.fit_transform(second_sample)\nprint(XX.toarray())","42a8eb8c":"def text_cleaning(text):\n    remv_punc = [char for char in text.lower() if char not in string.punctuation]\n    remv_punc_join = ''.join(remv_punc)\n    remv_punc_clean = [word for word in remv_punc_join.split() if word.lower() not in stopwords.words('english')]\n    return remv_punc_clean","209c81ca":"#Lets visualize the newly created function\ntweets_df_clean = tweets_train_df['tweet'].apply(text_cleaning)\nprint(tweets_df_clean[5])","a13b3183":"#Actual version of selected tweet, we can see that we have removed all punctuations and stopwords using a single user defined function\ntweets_train_df['tweet'][5]","5d0a461a":"# Now we will use \"analyser\" to apply countvectorization. \n#In other words, analyzer is an preprocess step before applying countVectorization step.\nvectorizer_analyzer = CountVectorizer(analyzer = text_cleaning)\ncountvectorizer_tweets = CountVectorizer(analyzer= text_cleaning, dtype= 'uint8').fit_transform(tweets_train_df['tweet']).toarray()","7231c1db":"countvectorizer_tweets.shape","089e2dd4":"X_features = countvectorizer_tweets\ny_label = tweets_train_df['label']","fe44adb1":"X_features.shape","13122cf0":"y_label.shape","44522f9e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_features, y_label, test_size = 0.2, random_state = 1)","38f4ea41":"from sklearn.naive_bayes import MultinomialNB\nNaiveBclassifier = MultinomialNB()\nNaiveBclassifier.fit(X_train,y_train)","d652702f":"from sklearn.metrics import classification_report, confusion_matrix","f13f3edc":"# Predicting test cases\ny_pred_test = NaiveBclassifier.predict(X_test)","5eddeef7":"# Confusion matrix\ncm = confusion_matrix(y_test, y_pred_test)\nsns.heatmap(cm, annot= True)","6d7ae3cd":"## Data cleaning (Remove StopWords from text)","1ab6f38c":"## Exploring Dataset","06bbe987":"## Analyzing the model performance","b22df2e6":"## Now we will train a Naive Bayes Classifier Model","6122e593":"## Now we will perform all operations in a pipeline, (1) Remove punctuations (2) Remove stopwords (3) Tockenization","962faeaf":"## Count Vectorization (Tokenization)","39445208":"I am really thankful to [Coursera](https:\/\/www.coursera.org\/learn\/twitter-sentiment-analysis) and Adjunct Professor [Ryan Ahmad](https:\/\/www.coursera.org\/instructor\/~48777395) for providing such a nice platform to start learning Natural Language Processing.","ee68b2f8":"# As we have a complete unbalanced data, with almost 30K positive and 2.5K negative tweets. Hence,we can see that positive words are more often used in this string.","f6ed6474":"Note: This notebook is not the final version. With the passage of time, i'll update it. Any suggestions would be highly appreciated. Please upvote, if you find it helpful. It will keep me motivated.","70fc0f6b":"## In this project, sentiments in tweets are analyzed. In brief, the goal is to classify the tweets based on positive sentiments using words (such as love, happy) and negative sentiments using words (racist, hate). The tasks performed are:\n1. Exploratory Data Analysis\n2. Plot WordCloud\n3. Data cleaning (Removing Punctuations)\n4. Data cleaning (Removing Stopwords)\n5. Countvectorization (Tocknization)\n6. Create pipeline to perform Task 3,4,5\n7. Train Naive Bayes Classifier\n8. Annalyze Model Performance","7debaf9f":"## Data cleaning (Remove punctuations from text)","63a466df":"## Acknowledgement "}}