{"cell_type":{"246d8d1c":"code","5bc41dd7":"code","0bd8fb1c":"code","8d66d979":"code","44a9d485":"code","df2b065e":"code","29331680":"code","6220f506":"code","66d46c8b":"code","2d0383a0":"code","e2217896":"code","490e8040":"code","6d388e08":"code","535ac66a":"code","343b28c4":"code","708fb407":"code","5fd9e01d":"code","8c69e285":"code","c2dc270d":"code","98ea6190":"code","4354d2cd":"code","1ac4716e":"code","ee70d055":"code","34425010":"code","56d8a081":"code","bd796b0f":"code","21b9cc36":"code","5c60f17f":"code","665f5d74":"code","2a3bfece":"code","0e0a532a":"code","fdc78e6d":"code","14c1792c":"code","2f04fc2c":"code","7f1d216d":"code","b72b5821":"code","7a33b90e":"code","9fd9458d":"code","37ec17e4":"code","7cce312d":"code","5d9f57d0":"code","aad40cff":"code","e084a194":"code","fcaf9afd":"code","9633b3bc":"code","03bf35d6":"code","bf1f3407":"code","fda60266":"code","fb00d24c":"code","9344e3f3":"code","24fc3ed5":"code","a7b9e809":"code","67dda1e8":"code","076bde96":"code","3be644fc":"code","7f402767":"code","0bb47385":"code","8c7334a9":"code","9771ca62":"code","1ecff182":"code","b2fc9a8b":"code","fc8d152d":"code","e8fb7e34":"code","c5a58cd6":"code","8d11df1e":"code","b83b924b":"code","7de77f5a":"code","b1fc4c1c":"code","5e031881":"code","93e3c240":"code","f29945ed":"code","e4b6bd30":"code","ed1fc219":"code","966945a4":"code","d60157b7":"code","52208bef":"code","ff635dbd":"code","2d81810d":"code","c749c921":"code","16bd87b6":"code","5d2fc13c":"code","1c030f15":"code","2c5204ce":"code","7f646da6":"code","d8d8e489":"code","be6fcd77":"code","6147216d":"code","26b5f88c":"code","034603b0":"markdown","9e39ed90":"markdown","08b6e1fe":"markdown","e00fa4a0":"markdown","64832909":"markdown","7d3d573b":"markdown","a6e995a7":"markdown","fe738f9f":"markdown","d7653d2f":"markdown","eba0cfe8":"markdown","fba9587f":"markdown","4a319bfe":"markdown","2814404d":"markdown","955b6535":"markdown","4c67b03d":"markdown","e7eddadf":"markdown","4e0273fd":"markdown","f264beab":"markdown","3df027a0":"markdown","eca77584":"markdown","92da1de3":"markdown","4c6a9de3":"markdown","136581df":"markdown","6d865754":"markdown","db83ea8c":"markdown","184b34a9":"markdown","df0ab994":"markdown","6590f384":"markdown","20c655d2":"markdown","f56fd1fe":"markdown","4977472a":"markdown","abbf0d94":"markdown","31715056":"markdown","fe594d5e":"markdown","30fcd2f7":"markdown","f439c883":"markdown","4203243b":"markdown","65f2cf0b":"markdown","b8f9b545":"markdown","70cec468":"markdown","670b8e69":"markdown","4879351b":"markdown","418e4a8f":"markdown","55ac4cab":"markdown","c2f9a141":"markdown","231a0401":"markdown","17d6e3d0":"markdown","ec68adeb":"markdown","a9eac078":"markdown","d1bb0384":"markdown","aeccf630":"markdown","955209b2":"markdown","28556fec":"markdown","c107195d":"markdown","0d9a8452":"markdown","bd3d657b":"markdown","520556cf":"markdown","9d406ca1":"markdown"},"source":{"246d8d1c":"import random\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport math\nimport types\nimport inspect\nimport plotly.plotly as py\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport warnings\n\nfrom matplotlib import cm\nfrom sklearn import linear_model\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler  \nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\npy.init_notebook_mode(connected=False)\n\nnp.random.seed(201807002)\n\npd_concat_argspec = inspect.getfullargspec(pd.concat)\npd_concat_has_sort = 'sort' in pd_concat_argspec.args\n\ndef pd_concat(frames):\n    # Due to Pandas versioning issue\n    new_frame = pd.concat(frames, sort=False) if pd_concat_has_sort else pd.concat(frames)\n    new_frame.reset_index(inplace=True, drop=True)\n    return new_frame\n    \ndef plt_hist(x, bins=30):\n    # plt.hist() can be very slow.\n    histo, edges = np.histogram(x, bins=bins)\n    plt.bar(0.5 * edges[1:] + 0.5 * edges[:-1], histo, width=(edges[-1] - edges[0])\/(len(edges) + 1))","5bc41dd7":"data = pd.read_csv('..\/input\/gaia-dr2-rave-35.csv', dtype={'source_id': str})","0bd8fb1c":"len(data)","8d66d979":"train_mask = np.random.rand(len(data)) < 0.9\nwork_data = data[train_mask]\nwork_data.reset_index(inplace=True, drop=True)\ntest_data = data[~train_mask]\ntest_data.reset_index(inplace=True, drop=True)","44a9d485":"data.columns","df2b065e":"def get_cv_model_transform(data_frame, label_extractor, var_extractor, trainer, response_column='Response', id_column='source_id', n_splits=2, scale = False):\n    shuffled_frame = data_frame.sample(frac=1).reset_index(drop=True)\n    nrow = len(data_frame)\n    kf = KFold(n_splits=n_splits)\n    response_map = dict()\n    default_model = None\n    default_scaler = None\n    split_idx = 0\n    for train_idx, test_idx in kf.split(shuffled_frame):\n        train_frame = shuffled_frame.iloc[train_idx]\n        test_frame = shuffled_frame.iloc[test_idx]\n        train_labels = label_extractor(train_frame) if isinstance(label_extractor, types.FunctionType) else train_frame[label_extractor]\n        train_vars = var_extractor(train_frame)\n        test_vars = var_extractor(test_frame)\n        if scale:\n            default_scaler = StandardScaler()  \n            default_scaler.fit(train_vars)\n            train_vars = default_scaler.transform(train_vars)  \n            test_vars = default_scaler.transform(test_vars) \n        default_model = trainer.fit(train_vars, train_labels)\n        test_frame.reset_index(inplace=True, drop=True)\n        test_responses = default_model.predict(test_vars)\n        test_id = test_frame[id_column]\n        assert len(test_id) == len(test_responses)\n        for i in range(len(test_id)):\n            response = test_responses[i]\n            key = str(test_id[i])\n            response_map[key] = response\n        split_idx += 1\n    response_id_set = set(response_map)\n        \n    def _transform(_frame):\n        _in_trained_set = _frame[id_column].astype(str).isin(response_id_set)\n        _trained_frame = _frame[_in_trained_set].copy()\n        _trained_frame.reset_index(inplace=True, drop=True)\n        if len(_trained_frame) > 0:\n            _trained_id = _trained_frame[id_column]\n            _tn = len(_trained_id)\n            _response = pd.Series([None] * _tn)\n            for i in range(_tn):\n                _response[i] = response_map[str(_trained_id[i])]\n            _trained_frame[response_column] = _response\n        _remain_frame = _frame[~_in_trained_set].copy()\n        if len(_remain_frame) > 0:\n            _vars = var_extractor(_remain_frame)\n            if default_scaler is not None:\n                _vars = default_scaler.transform(_vars)\n            _response = default_model.predict(_vars)\n            _remain_frame[response_column] = _response\n        _frames_list = [_trained_frame, _remain_frame]\n        _concat_frame = pd_concat(_frames_list)\n        _concat_frame.reset_index(inplace=True, drop=True)\n        return _concat_frame\n    return _transform","29331680":"def print_evaluation(data_frame, label_column, response_column):\n    _response = response_column(data_frame) if isinstance(response_column, types.FunctionType) else data_frame[response_column]\n    _label = label_column(data_frame) if isinstance(label_column, types.FunctionType) else data_frame[label_column]\n    _error = _response - _label\n    _rmse = math.sqrt(sum(_error ** 2) \/ len(data_frame))\n    _correl = stats.pearsonr(_response, _label)[0]\n    print('RMSE: %.4f | Correlation: %.4f' % (_rmse, _correl,), flush=True)\n    ","6220f506":"MIN_PARALLAX = 2.5\nMAX_PARALLAX_ERROR = 0.2","66d46c8b":"def transform_init(data_frame):    \n    parallax = data_frame['parallax']\n    parallax_error = data_frame['parallax_error']\n    new_frame = data_frame[(parallax >= MIN_PARALLAX) & (parallax_error <= MAX_PARALLAX_ERROR)].copy()\n    new_frame.reset_index(inplace=True, drop=True)\n    distance = 1000.0 \/ new_frame['parallax']\n    new_frame['distance'] = distance\n    new_frame['abs_mag_ne'] = new_frame['phot_g_mean_mag'] - 5 * (np.log10(distance) - 1)\n    return new_frame","2d0383a0":"work_data = transform_init(work_data)\nlen(work_data)","e2217896":"wd_distance_mod = 5 * (np.log10(work_data['distance']) - 1)\nwd_photo_distance_mod = 5 * (np.log10(1000.0 \/ work_data['r_parallax']) - 1)","490e8040":"np.sqrt(mean_squared_error(wd_distance_mod, wd_photo_distance_mod))","6d388e08":"GR = 100","535ac66a":"def extract_model_vars(data_frame):\n    distance = data_frame['distance'].values\n    log_distance = np.log(distance)\n    g_mag = data_frame['phot_g_mean_mag']\n    bp_mag = data_frame['phot_bp_mean_mag']\n    rp_mag = data_frame['phot_rp_mean_mag']\n    longitude_raw = data_frame['l'].values\n    longitude = [(lng if lng <= 180 else lng - 360) for lng in longitude_raw]\n    latitude = data_frame['b'].values\n    sin_lat = np.sin(np.deg2rad(latitude))\n    lat_ext_metric_prelim = np.abs(GR \/ sin_lat)\n    lat_ext_metric = [min(distance[i], lat_ext_metric_prelim[i]) for i in range(len(data_frame))]\n    metallicity = data_frame['r_metallicity']\n    radial_velocity = data_frame['r_hrv']\n    mg = data_frame['r_mg']\n    si = data_frame['r_si']\n    fe = data_frame['r_fe']\n    jmag = data_frame['r_jmag_2mass']\n    hmag = data_frame['r_hmag_2mass']\n    kmag = data_frame['r_kmag_2mass']\n    aw_m1 = data_frame['r_w1mag_allwise']\n    aw_m2 = data_frame['r_w2mag_allwise']\n    aw_m3 = data_frame['r_w3mag_allwise']\n    aw_m4 = data_frame['r_w4mag_allwise']\n    denis_imag = data_frame['r_imag_denis']\n    denis_jmag = data_frame['r_jmag_denis']\n    denis_kmag = data_frame['r_kmag_denis']    \n    apass_bmag = data_frame['r_bmag_apassdr9']\n    apass_vmag = data_frame['r_vmag_apassdr9']\n    apass_rpmag = data_frame['r_rpmag_apassdr9']\n    apass_ipmag = data_frame['r_ipmag_apassdr9']\n    \n    color1 = hmag - jmag\n    color2 = kmag - hmag\n    color3 = rp_mag - kmag\n    color4 = g_mag - rp_mag\n    color5 = bp_mag - g_mag\n    color6 = aw_m2 - aw_m1\n    color7 = aw_m3 - aw_m2\n    color8 = aw_m4 - aw_m3\n    color9 = rp_mag - aw_m4\n    color10 = g_mag - denis_imag\n    color11 = denis_imag - denis_jmag\n    color12 = denis_jmag - denis_kmag    \n    color13 = apass_rpmag - apass_ipmag\n    color14 = apass_vmag - apass_rpmag\n    color15 = apass_bmag - apass_vmag\n    color16 = g_mag - apass_bmag\n    \n    return np.transpose([log_distance, distance,\n            color1, color2, color3, color4, color5,\n            color6, color7, color8, color9,\n            color10, color11, color12,\n            color13, color14, color15, color16,\n            mg, si, fe, metallicity,\n            latitude, longitude, lat_ext_metric\n            ])    ","343b28c4":"LABEL_COLUMN = 'phot_g_mean_mag'","708fb407":"transform_linear = get_cv_model_transform(work_data, LABEL_COLUMN, extract_model_vars, linear_model.LinearRegression(), n_splits=2, response_column='linear_' + LABEL_COLUMN, scale=True)\nwork_data = transform_linear(work_data)","5fd9e01d":"print_evaluation(work_data, LABEL_COLUMN, 'linear_' + LABEL_COLUMN)","8c69e285":"def get_gbm_trainer():\n    return xgb.XGBRegressor(n_estimators=550, learning_rate=0.05, gamma=0.01, subsample=0.75,\n                           colsample_bytree=1.0, max_depth=8, random_state=np.random.randint(1,10000))","c2dc270d":"def get_gbm_transform(label_column):\n    return get_cv_model_transform(work_data, label_column, extract_model_vars, \n                get_gbm_trainer(), \n                n_splits=2, response_column='gbm_' + label_column)","98ea6190":"transform_gbm = get_gbm_transform(LABEL_COLUMN)\nwork_data = transform_gbm(work_data)","4354d2cd":"print_evaluation(work_data, LABEL_COLUMN, 'gbm_' + LABEL_COLUMN)","1ac4716e":"def get_gbm2_trainer():\n    return xgb.XGBRegressor(n_estimators=500, learning_rate=0.07, gamma=0.003, subsample=0.80,\n                           colsample_bytree=1.0, max_depth=7, random_state=np.random.randint(1,10000))","ee70d055":"def get_gbm2_transform(label_column):\n    return get_cv_model_transform(work_data, label_column, extract_model_vars, \n                get_gbm2_trainer(), \n                n_splits=2, response_column='gbm2_' + label_column)","34425010":"transform_gbm2 = get_gbm2_transform(LABEL_COLUMN)\nwork_data = transform_gbm2(work_data)","56d8a081":"print_evaluation(work_data, LABEL_COLUMN, 'gbm2_' + LABEL_COLUMN)","bd796b0f":"nn_seed = np.random.randint(1,10000)\ndef get_nn_trainer():\n    return MLPRegressor(hidden_layer_sizes=(30, 10), max_iter=500, alpha=0.1, random_state=nn_seed)","21b9cc36":"def get_nn_transform(label_column):\n    return get_cv_model_transform(work_data, label_column, extract_model_vars, get_nn_trainer(), \n        n_splits=3, response_column='nn_' + label_column, scale=True)","5c60f17f":"transform_nn = get_nn_transform(LABEL_COLUMN)\nwork_data = transform_nn(work_data)","665f5d74":"print_evaluation(work_data, LABEL_COLUMN, 'nn_' + LABEL_COLUMN)","2a3bfece":"def extract_blend_vars(data_frame):\n    gbm_responses = data_frame['gbm_' + LABEL_COLUMN].values\n    gbm2_responses = data_frame['gbm2_' + LABEL_COLUMN].values\n    nn_responses = data_frame['nn_' + LABEL_COLUMN].values\n    linear_responses = data_frame['linear_' + LABEL_COLUMN].values\n    return np.transpose([gbm_responses, gbm2_responses, nn_responses, linear_responses])","0e0a532a":"def get_blend_trainer():\n    return linear_model.LinearRegression()","fdc78e6d":"def get_blend_transform(label_column):\n    return get_cv_model_transform(work_data, label_column, extract_blend_vars, get_blend_trainer(), \n                n_splits=5, response_column='blend_' + label_column)","14c1792c":"transform_blend = get_blend_transform(LABEL_COLUMN)\nwork_data = transform_blend(work_data)","2f04fc2c":"print_evaluation(work_data, LABEL_COLUMN, 'blend_' + LABEL_COLUMN)","7f1d216d":"MODEL_PREFIX = 'blend_'","b72b5821":"def error(data_frame, label_column):\n    return data_frame[label_column] - data_frame[MODEL_PREFIX + label_column]","7a33b90e":"def transform_error(data_frame):\n    new_frame = data_frame.copy()\n    new_frame['error_' + LABEL_COLUMN] = error(data_frame, LABEL_COLUMN)\n    return new_frame","9fd9458d":"work_data = transform_error(work_data)","37ec17e4":"def get_abs_error_label(data_frame):\n    return np.abs(data_frame['error_' + LABEL_COLUMN])","7cce312d":"def extract_error_vars(data_frame):\n    parallax = data_frame['parallax']\n    parallax_error = data_frame['parallax_error']\n    parallax_high = parallax + parallax_error\n    parallax_low = parallax - parallax_error\n    var_error_diff = np.log(parallax_high) - np.log(parallax_low)\n    distance = data_frame['distance']\n    longitude = data_frame['l']\n    latitude = data_frame['b']\n    radial_velocity = data_frame['r_hrv']\n    return np.transpose([\n        var_error_diff,\n        distance,\n        longitude,\n        latitude,\n        radial_velocity\n    ])","5d9f57d0":"def get_error_trainer():\n    return RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_split=2, min_samples_leaf=2)","aad40cff":"transform_expected_error = get_cv_model_transform(work_data, get_abs_error_label, extract_error_vars, get_error_trainer(), \n                n_splits=3, response_column='expected_error_' + LABEL_COLUMN)","e084a194":"work_data = transform_expected_error(work_data)","fcaf9afd":"print_evaluation(work_data, get_abs_error_label, 'expected_error_' + LABEL_COLUMN)","9633b3bc":"def transform_anomaly(data_frame):\n    new_frame = data_frame.copy()\n    new_frame['anomaly_' + LABEL_COLUMN] = new_frame['error_' + LABEL_COLUMN] \/ new_frame['expected_error_' + LABEL_COLUMN]\n    return new_frame","03bf35d6":"work_data = transform_anomaly(work_data)","bf1f3407":"transform_list = [transform_init, \n                  transform_linear, transform_gbm, transform_gbm2, transform_nn, \n                  transform_blend,\n                  transform_error, transform_expected_error, transform_anomaly]","fda60266":"def combined_transform(data_frame):\n    _frame = data_frame\n    for t in transform_list:\n        _frame = t(_frame)\n    return _frame","fb00d24c":"test_data = combined_transform(test_data)","9344e3f3":"np.std(test_data['error_' + LABEL_COLUMN])","24fc3ed5":"data = combined_transform(data)","a7b9e809":"CAND_SD_THRESHOLD = 3.0","67dda1e8":"data_anomalies = data['anomaly_' + LABEL_COLUMN]","076bde96":"anomaly_std = np.std(data_anomalies)","3be644fc":"cand_threshold = anomaly_std * CAND_SD_THRESHOLD\ncandidates = data[data_anomalies >= cand_threshold]\nlen(candidates)","7f402767":"bright_control_group = data.sort_values('anomaly_' + LABEL_COLUMN, ascending=True).head(len(candidates))","0bb47385":"normal_control_group = data[(data_anomalies < anomaly_std) & (data_anomalies > -anomaly_std)].sample(len(candidates))","8c7334a9":"data_sample = data.sample(1000)","9771ca62":"def abs_mag_value(data_frame, mag_column):\n    _distance_mod = 5 * np.log10(data_frame['distance']) - 5\n    return data_frame[mag_column] - _distance_mod","1ecff182":"MODEL_RESPONSE_COLUMN = MODEL_PREFIX + LABEL_COLUMN\n\nplt.rcParams['figure.figsize'] = (10, 5)\nplt.scatter(abs_mag_value(data_sample, MODEL_RESPONSE_COLUMN), abs_mag_value(data_sample, LABEL_COLUMN), color=(0.5,0.5,0.5,0.5,), s=1)\nplt.scatter(abs_mag_value(candidates, MODEL_RESPONSE_COLUMN), abs_mag_value(candidates, LABEL_COLUMN), color='green', s=6)\nplt.scatter(abs_mag_value(bright_control_group, MODEL_RESPONSE_COLUMN), abs_mag_value(bright_control_group, LABEL_COLUMN), color='orange', s=2)\nplt.ylim(-1, 11)\nplt.gca().invert_yaxis()\nplt.title('Model Diagram')\nplt.xlabel('Modeled absolute magnitude')\nplt.ylabel('Observed absolute magnitude')\nplt.show()","b2fc9a8b":"def color_value(data_frame):\n    return data_frame['phot_bp_mean_mag'] - data_frame['phot_rp_mean_mag']","fc8d152d":"def separation_y(color_index):\n    return -0.1 + 4.6 * color_index","e8fb7e34":"plt.rcParams['figure.figsize'] = (10, 5)\nplt.scatter(color_value(data_sample), abs_mag_value(data_sample, LABEL_COLUMN), color=(0.5,0.5,0.5,0.5,), s=1)\nplt.scatter(color_value(bright_control_group), abs_mag_value(bright_control_group, LABEL_COLUMN), color='orange', s=2)\nplt.scatter(color_value(candidates), abs_mag_value(candidates, LABEL_COLUMN), color='green', s=6)\nsep_x = np.linspace(0.5, 2.1, 100)\nsep_y = separation_y(sep_x)\nplt.plot(sep_x, sep_y, '--', color='blue')\nplt.ylim(-1, 11)\nplt.xlim(0.5, 2.1)\nplt.gca().invert_yaxis()\nplt.title('H-R Diagram')\nplt.xlabel('BP - RP color index')\nplt.ylabel('Absolute magnitude')\nplt.show()","c5a58cd6":"mainseq_mask = abs_mag_value(candidates, LABEL_COLUMN) > separation_y(color_value(candidates))\ncandidates_mainseq = candidates[mainseq_mask]\ncandidates_bright = candidates[~mainseq_mask]","8d11df1e":"len(candidates_mainseq)","b83b924b":"len(candidates_bright)","7de77f5a":"def get_position_frame(data_frame):\n    new_frame = pd.DataFrame()\n    new_frame['source_id'] = data_frame['source_id'].values\n    distance = data_frame['distance'].values\n    latitude = np.deg2rad(data_frame['b'].values)\n    longitude = np.deg2rad(data_frame['l'].values)\n    new_frame['z'] = distance * np.sin(latitude)\n    projection = distance * np.cos(latitude)\n    new_frame['x'] = projection * np.cos(longitude)\n    new_frame['y'] = projection * np.sin(longitude)\n    new_frame['is_mainseq'] = (abs_mag_value(candidates, LABEL_COLUMN) > separation_y(color_value(candidates))).values\n    return new_frame","b1fc4c1c":"candidates_pos_frame = get_position_frame(candidates)","5e031881":"def plot_pos_frame(pos_frame, mainseq_color = 'red', other_color= 'red'):\n    star_color = [(mainseq_color if v else other_color) for v in pos_frame['is_mainseq'].values]\n    trace1 = go.Scatter3d(\n        x=pos_frame['x'],\n        y=pos_frame['y'],\n        z=pos_frame['z'],\n        mode='markers',\n        text=candidates['source_id'],\n        marker=dict(\n            size=4,\n            color=star_color,\n            opacity=0.67\n        )\n    )\n    scatter_data = [trace1]\n    layout = go.Layout(\n        margin=dict(\n            l=0,\n            r=0,\n            b=0,\n            t=0\n        )\n    )\n    fig = go.Figure(data=scatter_data, layout=layout)\n    py.iplot(fig)","93e3c240":"plot_pos_frame(candidates_pos_frame, 'blue', 'green')","f29945ed":"bright_control_group_pos_frame = get_position_frame(bright_control_group)\nplot_pos_frame(bright_control_group_pos_frame, 'red', 'red')","e4b6bd30":"candidates['distance'].describe()","ed1fc219":"bright_control_group['distance'].describe()","966945a4":"anomalous_pos_frame = pd_concat([candidates_pos_frame, bright_control_group_pos_frame])","d60157b7":"apf_len = len(anomalous_pos_frame)\napf_source_id_idx = anomalous_pos_frame.columns.get_loc('source_id')\napf_x_idx = anomalous_pos_frame.columns.get_loc('x')\napf_y_idx = anomalous_pos_frame.columns.get_loc('y')\napf_z_idx = anomalous_pos_frame.columns.get_loc('z')\nnew_row_list = []\nfor i in range(apf_len):\n    row1 = anomalous_pos_frame.iloc[i]\n    source1 = row1[apf_source_id_idx]\n    x1 = row1[apf_x_idx]\n    y1 = row1[apf_y_idx]\n    z1 = row1[apf_z_idx]\n    for j in range(i + 1, apf_len):\n        row2 = anomalous_pos_frame.iloc[j]\n        source2 = row2[apf_source_id_idx]\n        x2 = row2[apf_x_idx]\n        y2 = row2[apf_y_idx]\n        z2 = row2[apf_z_idx]\n        distance_sq = (x2 - x1) ** 2 + (y2 - y1) ** 2 + (z2 - z1) ** 2\n        new_row_list.append([source1, source2, distance_sq])\ncross_distance_frame = pd.DataFrame(new_row_list, columns=['source1', 'source2', 'distance_sq'])\ncross_distance_frame.sort_values('distance_sq', inplace=True)","52208bef":"candidate_source_set = set(candidates['source_id'])\ncross_distance_frame['source1_dim'] = cross_distance_frame['source1'].isin(candidate_source_set)\ncross_distance_frame['source2_dim'] = cross_distance_frame['source2'].isin(candidate_source_set)","ff635dbd":"cross_distance_frame.head(5)","2d81810d":"len(cross_distance_frame[cross_distance_frame['source1_dim'] & cross_distance_frame['source2_dim']]) \/ len(cross_distance_frame)","c749c921":"dim_match_frequency = pd.DataFrame(columns=['count', 'frequency'])\nfor ss in range(5, 2000, 10):\n    sub_frame = cross_distance_frame.iloc[:ss]\n    count = len(sub_frame)\n    freq = len(sub_frame[sub_frame['source1_dim'] & sub_frame['source2_dim']]) \/ count\n    dim_match_frequency.loc[len(dim_match_frequency)] = [count, freq]","16bd87b6":"plt.rcParams['figure.figsize'] = (10, 5)\nexp_line_x = [0, 2000]\nexp_line_y = [0.25, 0.25]\nhs_counts = dim_match_frequency['count']\ntt_freqs = dim_match_frequency['frequency']\n# standard margin of error\nstd_moe = np.sqrt(0.25 * (1 - 0.25) \/ hs_counts)\ntt_freqs_low_95 = tt_freqs - 1.96 * std_moe\ntt_freqs_low_99 = tt_freqs - 2.575 * std_moe\nplt.plot(exp_line_x, exp_line_y, '--', color='orange')\nplt.plot(hs_counts, tt_freqs, color='black', linewidth=5)\nplt.plot(hs_counts, tt_freqs_low_95, color='red', linewidth=1)\nplt.plot(hs_counts, tt_freqs_low_99, color='blue', linewidth=1)\nplt.yticks(np.linspace(0, 1, 11))\nplt.ylim(0, 1.0)\nplt.xlim(0, 2000)\n#plt.grid(color=(0.9, 0.9, 0.9,))\nplt.title('Results of clustering test')\nplt.xlabel('Size of headset of ordered cross-distance frame')\nplt.ylabel('Frequency of True-True rows')\nplt.show()","5d2fc13c":"SAVED_COLUMNS = ['source_id', 'ra', 'dec', 'pmra', 'pmdec', 'l', 'b', 'distance', 'abs_mag_ne', \n                 'error_' + LABEL_COLUMN, 'anomaly_' + LABEL_COLUMN]","1c030f15":"def save_data(data_frame, file_name):\n    data_frame[SAVED_COLUMNS].to_csv(file_name)","2c5204ce":"save_data(data, 'all-sources.csv')\nsave_data(candidates, 'dim-candidates.csv')\nsave_data(candidates_mainseq, 'dim-candidates-mainseq.csv')\nsave_data(bright_control_group, 'bright-controls.csv')\nsave_data(normal_control_group, 'normal-controls.csv')","7f646da6":"def sc(data_frame):\n    new_frame = data_frame[['source_id', 'ra', 'dec', 'pmra', 'pmdec', 'distance', 'abs_mag_ne', 'error_phot_g_mean_mag', 'anomaly_phot_g_mean_mag']]\n    new_frame.reset_index(inplace=True, drop=True)\n    return new_frame","d8d8e489":"sc(candidates[abs_mag_value(candidates, LABEL_COLUMN) >= 1.45 + separation_y(color_value(candidates))])","be6fcd77":"closest_cand = candidates.sort_values('distance').head(15)\nsc(closest_cand[closest_cand['ra'] >= 300])","6147216d":"data_close = data[data['distance'] < 75]\nnp.mean(np.abs(data_close['pmra']))","26b5f88c":"np.mean(np.abs(data_close['pmdec']))","034603b0":"## Acknowledgments\n\nThis work has made use of data from the European Space Agency (ESA) mission Gaia (https:\/\/www.cosmos.esa.int\/gaia), processed by the Gaia Data Processing and Analysis Consortium (DPAC, https:\/\/www.cosmos.esa.int\/web\/gaia\/dpac\/consortium). Funding for the DPAC has been provided by national institutions, in particular the institutions participating in the Gaia Multilateral Agreement.","9e39ed90":"## First GBM model\nNext, we will build a Gradient Boosting Machine (GBM). We will use the TreeBoost implementation from [xgboost](https:\/\/xgboost.readthedocs.io\/en\/latest\/model.html).","08b6e1fe":"Additionally, in the interactive 3D scatter chart, we find a group of a few stars that are close to one another and relatively close to Earth.","e00fa4a0":"## Discussion\nWe have produced a model of stellar magnitude as a function of Gaia DR2 parallax distance and spectrophotometric features from a number of databases. Model RMSE is ~0.23 magnitudes, when we select only stars with parallax of 2.5 or more and parallax error of 0.2 or less. This RMSE is nearly a third of the discrepancy between Gaia parallax and RAVE spectrophotometric distance moduli.\n\nIn a Gaia-only H-R diagram, anomalously bright sources are typically giants. Anomalously dim sources are sometimes giants as well, but most occur in the main sequence.\n\nWe find statistically significant stellar clustering in the group of anomalously dim candidates. Clustering is more evident among candidates that are in the main sequence or below.\n\nClustering could be explained as (1) a star type that exhibits clustering and can't be pinned down mathematically with the spectrophotometric features available to us, or (2) a model or data artifact.\n\nOne idea is that, since the model uses galactic latitude and longitude as variables, it might have introduced random regional irregularities in model responses. This hypothesis is easily discarded by removing latitude and longitude from the set of variables.\n\nClustering is, in fact, more pronounced if we remove latitude and longitude features. This should be taken into account going forward: The model seems to smooth out anomalies (though not entirely) as it detects region-dependent patterns. If there's clustering of star types of interest, that's somewhat of a problem. We could be discarding valid candidates.\n\nRegional error that is not reflected in the *parallax_error* field should have been discovered by the absolute residual meta-model. In any case, we did not find evidence of significant regional error patterns. The reason why galactic latitude and longitude are variables of the residual meta-model is precisely to try to address observed clustering. The variables were kept because of their slight contribution to meta-model performance.\n\nThere are pairs of candidates and bright controls that are very close to one another in the sky. The possibility of contamination or incorrect cross-identification should be studied further. However, we don't believe such a systematic explains the observed clustering.","64832909":"So we have over a hundred anomalously dim candidates, at the 3-sigma level. Let's also get an equal number of anomalously bright controls.","7d3d573b":"Our blend is only slightly better than our best individual model. A blend is a combination of multiple models, so it should generally be more stable (less long-tailed) than the individual models.","a6e995a7":"Let's plot the results of this test. The dashed orange line shows the expected (null hypothesis) frequency. Two additional lines show the lower bound of the 95% (red) and 99% (blue) confidence intervals.","fe738f9f":"## Blend\nWe will now blend the 4 individual models we previously produced. First, we need a function that extracts the \"variables\" of the blend model, which are simply the responses of the individual models.","d7653d2f":"## Model and H-R diagrams\n\nWe will now look at some visualizations of model results. First, let's get a random sample of 1000 stars from the data.","eba0cfe8":"This is consistent with the RMSE observed in *work_data*. We can apply the full transform to the entire dataset.","fba9587f":"## Final transformation and validation\nLet's put all our transform functions into a list called *transform_list*.","4a319bfe":"It's valid to use the response of a model as a variable of another model, so long as the response is out-of-bag. Our responses are cross-validated, so they should work. If they didn't, that would be evident once we look at the 'test' data we set aside.\n\nWe will use linear regression to produce the blend model. A blend does not have to be linear, but in this case that appears to be the best choice.","2814404d":"## Model variables\nLet's define the variables we will use to train regression models. Primarily, we will use \"color index\" features, which are simply differences between magnitudes observed with different photometric filters. RAVE DR5 also provides metal abundance and metallicity features that seem to help.\n\nTheoretically, the difference between the magnitude of a star and its absolute magnitude is a linear function of the logarithm of the star's distance. But there's an additional extinction component that is roughly dependent on distance. That's why we include both the distance and the logarithm of the distance as variables.\n\nSince extinction is likely irregular in our region of space, we include galactic latitude and longitude as variables. We additionally include a special transformation of the latitude based on the assumption that a region close to latitude zero has increased extinction. *GR*, defined below, is a hyperpameter of this special variable.","955b6535":"~~The bright controls are only slightly farther away, by ~5% in average.~~\n\n[Edit: Distance distributions can change if the random seed changes, and affect this clustering analysis. A more robust clustering test is presented in a cloned kernel: *[Dysonian SETI With ML, Squared Residual Meta-Model](https:\/\/www.kaggle.com\/solorzano\/dysonian-seti-with-ml-squared-residual-meta-model)*]\n\nWe will now concatenate the position frames of the candidates and bright controls.","4c67b03d":"Any subsample of the cross-distance frame should also have a frequency of True-True rows of ~25%, within statistical uncertainty, if there's no clustering that depends on the modeled anomaly.\n\nThe following code tests incremental headsets of the ordered cross-distance frame, starting with the first 5 rows.","e7eddadf":"Further, we will add two boolean columns to the cross-distance frame indicating whether each of the sources in a pair are anomalously dim.","4e0273fd":"First, let's look at a chart of modeled absolute magnitude vs. observed absolute magnitude. Our 1000 random stars are shown in gray. The set of anomalously dim stars is shown in green. The set of bright controls is shown in orange.","f264beab":"Anomalously dim candidates (green dots) occur both in the main sequence, and above it. We will split the *candidates* frame into two subgroups called *candidates_bright* and *candidates_mainseq*, using the dashed blue line.","3df027a0":"## Space distribution of candidates\nLet's define a function that takes a data frame and converts distance, galactic longitude and latitude to x-y-z coordinates.","eca77584":"Note that the 3D scatter is interactive.\n\nLet's produce a second 3D scatter containing only the bright controls.","92da1de3":"## Neural Network","4c6a9de3":"The cross-distance frame looks as follows.","136581df":"## Boilerplate code","6d865754":"We will also add a couple decorator columns that are useful in visualizations: the \"distance\" to the star in parsecs, and the absolute magnitude (\"abs_mag_ne\") of the star, based on the G magnitude from Gaia DR2. Interstellar extinction is neglected in the absolute magnitude estimate.","db83ea8c":"## Linear modelling\nOur first regressor will be linear. Note that we will be modeling the G magnitude of a star, as provided by Gaia DR2. The reader can change the *LABEL_COLUMN* variable below to model, say, RP or BP magnitudes. There is just one function later in the analysis, called *separation_y*, that is visually derived and would need to be adjusted accordingly.","184b34a9":"## Decorators & data selection\n\nWe will start by limiting the data we will use to train models and obtain candidates. In preliminary analysis we found that many potential candidates appear to be below the main sequence in an H-R diagram, but upon further examination, it was clear these are stars with high parallax error. Additionally, model error is distance-dependent.\n\nNoisy data affects model precision, and also increases the likelihood that candidates will be spurious.\n\nDefining a threshold is a subjective exercise. We want to minimize the error, but we also want a reasonable number of candidates for follow-up analyses. We will use a minimum parallax of 2.5 (400 parsecs) and a maximum parallax error of 0.2.\n\nThese thresholds can be changed below.","df0ab994":"The *abs_mag_value* function below will convert a magnitude column in a data frame into an absolute magnitude.","6590f384":"It should be noted that all of the above are high proper motion stars. A high proper motion is not unusual, however, for stars that are close to Earth. Let's check what the mean absolute proper motion values are for stars that are at most 75 parsecs away.","20c655d2":"Per the methodology previously outlined, we will use this first transform function to alter our *work_data* frame.","f56fd1fe":"## Modeling strategy\n\nOur strategy will consist of transforming the *work_data* frame, typically by adding columns to it at each step. In order to do this, we will create transform functions that take a frame and produce a new frame with altered data.\n\nIn particular, we need a function that can produce a transform from an arbitrary regression model. The transform will add model responses as a new column of the frame. Model responses are cross-validated, so they can be used as inputs of other models. The transform is also able to produce output for data that was not used in training.","4977472a":"Columns found in the data frame follow.","abbf0d94":"The next chart will be a Hertzsprung\u2013Russell (H-R) diagram. We will use a color index in the X axis defined as the difference between the Gaia-provided BP and RP magnitudes.","31715056":"## Spectrophotometic distance error\n\nLet's calculate a baseline Root Mean Squared Error (RMSE) for reference. We will compare the Rave DR5 spectrophotometric distance modulus with the Gaia DR2 parallax-based distance modulus. Distance modulus errors should be equivalent to magnitude errors in terms of scaling.","fe594d5e":"We'll also get an equally sized list of ordinary controls.","30fcd2f7":"## Second GBM Model\nGBMs are stochastic, meaning that their responses include randomness that varies from one run to the next. For this reason, we will train a second GBM, with slightly different hyperparameters, so that the final model blend is more stable.","f439c883":"We will also define a function that compares two frame columns, e.g. a model response and a label, for purposes of model evaluation.","4203243b":"With this position frame, we can now produce an 3D scatter chart showing the candidates in our region of space. Earth is at (0, 0, 0). Candidates in the main sequence or below are shown in blue. The rest are shown in green.","65f2cf0b":"We will add a dashed blue line to the H-R diagram, intended to separate main sequence stars from giants. The separation line is defined by the *separation_y* function below, which is visually derived.","b8f9b545":"Note that we don't use magnitude features. That could be explored, but it seems it would allow the meta-model to discover brightness- and color-based patterns, which would cause us to discard potentially valid candidates.\n\nThe regressor we will use in this case is a Random Forest. There is a technical reason for this. Absolute residuals are always positive. We want a model that always produces positive responses. A Random Forest essentially produces averages of labels over some training examples, which is intuitively in line with what we want to accomplish.","70cec468":"Let's check the 'error_' column that the 'test' data should now have.","670b8e69":"## Model error\nLet's create a transform that will add an 'error_' column with the residuals of the blend.","4879351b":"## Candidate selection\nIt's not obvious how candidates should be selected. One possibility is to check how the error distribution compares to a normal distribution. The problem with this approach is that real-world data does not have a normal distribution. Instead, it's typically long-tailed. Outliers are to be expected and they are not indicative that we've found anything \"unnatural\", nor are non-outliers necessarily \"natural\".\n\nFor these reasons we will simply use a 3-sigma threshold to select our list of anomalously dim candidates.","418e4a8f":"## Clustering analysis\nAnomalously dim candidates, particularly those in the main sequence or below (the blue ones in the 3D chart), appear to cluster. We can mathematically test this conjecture.\n\nLet's compare anomalously dim candidates with bright controls. Both of these subsets should have a distance bias, given that we control for the expected absolute residuals of the model, and those residuals are distance-dependent. Let's start by checking summary statistics of each of the subsets.","55ac4cab":"## Introduction\n\nDysonian SETI is a proposed approach in the search for extraterrestrial civilizations (Bradbury et al. 2011). Instead of performing radio searches, the approach involves looking for stars that host artificial megastructures.\n\nZackrisson et al. (2018) suggested that stars with Dyson Swarms would have spectrophotometric distances that exceed their parallax distances. In other words, Dysonian stars should be dimmer than their spectral characteristics would indicate. \n\nIt's possible to approximate the absolute magnitude of a star, and therefore its probable distance, given the star's color. With additional spectral features, a better approximation of the absolute magnitude can be derived.\n\nIn this kernel we will model the magnitude of stars rather than their distance modulus. There is a practical reason for this. We would like to use the Gaia DR2 parallax distance (not just its logarithm) as a variable of the model, in order to account for interstellar extinction. We won't be using the RAVE spectrophotometric distance as a variable, given that we expect it to leak information about magnitude. But we will take advantage of photometric features provided by RAVE DR5 from various other databases, in addition to features found in Gaia DR2.\n\nWe will test machine learning algorithms that are able to deal with complex nonlinearities. Specifically, we will blend Gradient Boosting Machines (GBMs) and a Neural Network. Then we will build a meta-model of the model's absolute residuals, given that each star's measurements have errors that depend on its distance and other features. All model responses will be out-of-bag, as we rely on k-fold cross-validation.","c2f9a141":"## Data\n\nData is obtained from the [RAVE project](https:\/\/www.rave-survey.org\/project\/) and made available in a [Kaggle dataset](https:\/\/www.kaggle.com\/solorzano\/rave-dr5-gaia-dr2-consolidated\/home). In addition to RAVE DR5 measurements, the dataset  includes cross-identified features from Gaia DR2, APASS DR9, ALLWISE and DENIS.","231a0401":"Next, we will define a function that obtains the variables of the absolute residual meta-model.","17d6e3d0":"## Output files\n\nThe following files are made available in the output tab of this kernel.\n\n* **all-sources.csv** All sources that were used in training and candidate selection.\n* **dim-candidates.csv** The list of anomalously dim candidates.\n* **dim-candidates-mainseq.csv** Anomalously dim candidates that are in the main sequence or below.\n* **bright-controls.csv** A list of anomalously bright controls.\n* **normal-controls.csv** An equal number of ordinary stars.","ec68adeb":"We're interested in rows where both sources are anomalously dim. The expectation is that True-True rows occur 25% of the time. Indeed, this is what we find if we look at the entire cross-distance frame.","a9eac078":"## Synopsis\nWe test a variant of the methodology outlined in Zackrisson et al. (2018) by modeling the g magnitude of 64K stars as a function of Gaia DR2 parallax and spectrophotometric features available in RAVE DR5. In order to account for interstellar extinction irregularities, galactic latitude and longitude are variables of the model. A blend of GBMs and a Neural Network produces an RMSE of ~0.23 magnitudes in cross-validation, which is nearly a third of the error between Gaia parallax and RAVE spectrophotometric distance moduli. We also train a meta-model of the model's absolute residuals, and define \"anomaly\" as a residual divided by the expected absolute residual. Anomalously dim candidates are selected at a cutoff of 3-sigma in the anomaly distribution, and made available in the output tab of this kernel. We note that anomalously dim candidates, particularly those in the main sequence or below it, exhibit stellar clustering.\n\n**Follow-ups:**\n* [Dysonian SETI with ML, Squared Residual Meta-Model](https:\/\/www.kaggle.com\/solorzano\/dysonian-seti-with-ml-squared-residual-meta-model)\n* [New Stellar Magnitude Model](https:\/\/www.kaggle.com\/solorzano\/new-stellar-magnitude-model-dysonian-seti)\n* [Multi-Stellar SETI Candidate Selection](https:\/\/www.kaggle.com\/solorzano\/multi-stellar-seti-candidate-selection-part-1)","d1bb0384":"Let's set aside a 'test' data frame we will double-check only after all of the cross-validated models and transformations are tested using a 'work' data frame.","aeccf630":"## Other subsets\nIn the H-R diagram there are a few anomalously dim stars that are slightly below the main sequence. They can be picked out as follows.","955209b2":"Let's calculate squared distances between all pairs of sources in the concatenated frame. We will create a new frame, where each row contains a pair of sources and their squared distance. The new frame will be ordered by distance.","28556fec":"## References\n\nBradbury et al. (2011). _Dysonian Approach to SETI: A Fruitful Middle Ground?_ Journal of the British Interplanetary Society, vol. 64, p. 156-165\n\nKunder et al. (2016). _The Radial Velocity Experiment (RAVE): Fifth Data Release_. arXiv:1609.03210\n\nZackrisson et al. (2018). _SETI with Gaia: The observational signatures of nearly complete Dyson spheres_. arXiv:1804.08351 ","c107195d":"We can therefore reject the null hypothesis.","0d9a8452":"The *extract_model_vars* function takes a data frame and returns a variable matrix that is ready to be used with *scikit-learn*-style regressors with a *fit* function.","bd3d657b":"## Residual modeling\n\nWe have distance-based and parallax error-based exclusion criteria, which helps improve model performance. However, within included data there are still differences in model error that are characterizable. It seems sensible to make adjustments based on each star's expected error.\n\nAdditionally, in preliminary analysis we found that anomalously dim candidates cluster, so we are making sure that any regional error patters are taken into account, by using distance, longitude and latitude as variables of the residual model.\n\nWe will define a function that extracts the *absolute* residual array (i.e. the regression label) from a frame.","520556cf":"Now we'll create a function that runs the whole transform sequence on a frame, and apply it to the 'test' data we had initially set aside.","9d406ca1":"The following transform function adds an 'anomaly_' column to a frame, defined as the ratio between the model's residual and the expected absolute residual."}}