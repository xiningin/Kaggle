{"cell_type":{"51c672a8":"code","be315e29":"code","4e3b5228":"code","3cdd0d2b":"code","2fda51c1":"code","ddd880a9":"code","ba6879dc":"code","eec7c094":"code","f0d5cae3":"code","bf91a0d6":"code","ee9883f3":"code","bf04b4e8":"code","b6495dac":"code","88361fcb":"code","44f448ed":"code","b182a952":"code","a6c0ae30":"code","02e3ac42":"code","308e9440":"code","ec98e40c":"code","513fbc0f":"code","b9d32c23":"code","52f28685":"code","e17fcbf3":"code","4f4b8270":"code","03d2d05a":"code","b732415b":"code","f35771ac":"code","6f7f9a17":"code","1b06630d":"code","22eef243":"code","1cae9245":"code","c486079e":"code","846a494c":"code","7992d3b8":"code","f9dac7d1":"code","3a402b54":"code","1ad173e5":"code","f018068c":"markdown","423dbf7d":"markdown","15611d4d":"markdown","d8b8293d":"markdown"},"source":{"51c672a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import precision_score,recall_score\npd.options.display.float_format = '{:,.2f}'.format\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","be315e29":"def calculate_statistics_and_make_a_plot(input_data):\n    '''this function calculates descriptive statistics of a numeric variable and makes a plot'''\n    print(input_data.describe())                                                   #prints descriptive statistiscs\n    draw_graph(input_data)                                                         #draw a graph\n    \n    \ndef draw_graph(dt):\n    ''' this function makes a histogram plot'''\n    row_size=22                                                                    #setting row size\n    col_size=6                                                                     # setting column size\n    fig=plt.figure(figsize=(row_size,col_size))                                    #creating a figure\n    ax1=fig.add_subplot(1,2,1)                                                     #adding a subplot\n    ax1=plt.hist(dt)                                                               #creating a histogram\n    plt.title('{} Distribution'.format(dt.name))                                   #setting a title \n    plt.xlabel(dt.name)                                                            # setting x-axis label\n    plt.ylabel('Count')                                                            # setting y-axis label\n    \ndef calculate_statistics_and_make_a_plot_without_extremes(input_data,lower_threshold=0.01,upper_threshold=0.99):\n    '''this function calculates descriptive statistics of a numeric variable and makes a plot without top and bottom %1 data'''\n    mask=np.logical_and(input_data>input_data.quantile(lower_threshold),           #creating a list for spliting extreme values from the rest\n                        input_data<input_data.quantile(upper_threshold))\n    calculate_statistics_and_make_a_plot(input_data.loc[mask])                     #calculating descriptive statistics and making plot\n            \ndef mark_extreme_values(input_data,col_list,lower_threshold=0.01,upper_threshold=0.99):\n    '''this function marks extreme values.'''\n    for col in col_list:                                                           #creating a for loop runs over the col list\n        new_col_name=col+'_Is_Extreme'                                             #creating new name for new feature\n        input_data[new_col_name]=np.logical_or(input_data[col]<input_data[col].quantile(lower_threshold),  #marking extreme values on a new column\n                                                input_data[col]>input_data[col].quantile(upper_threshold))\n        input_data[new_col_name]=input_data[new_col_name]*1                        #converting boolean values to numeric\n        \ndef replace_extremes(input_data,col_list,lower_threshold=0.01,upper_threshold=0.99):\n    ''' this function replaces extreme values'''\n    for col in col_list:                                                           #creating a for loop runs over the col list\n        new_value=input_data[col].median()                                         #calculating the median value\n        is_extreme=np.logical_or(input_data[col]<input_data[col].quantile(lower_threshold),            #creating a list for spliting extreme values from the rest\n                             input_data[col]>input_data[col].quantile(upper_threshold))\n        input_data[col][is_extreme]=new_value                                      #replacing extreme values with the median value\n    return input_data\n        \ndef remove_extremes(input_data,col_list,lower_threshold=0.01,upper_threshold=0.99):\n    '''this function removes extreme values'''\n    for col in col_list:                                                           #creating a for loop runs over the col list\n        is_extreme=np.logical_or(input_data[col]<input_data[col].quantile(lower_threshold),            #creating a list for spliting extreme values from the rest\n                             input_data[col]>input_data[col].quantile(upper_threshold))\n        input_data.drop(input_data[is_extreme].index, axis=0,inplace=True)         #removing rows with extreme values\n    return input_data\n        \ndef standardize_a_column(data,column):\n    '''this function transforms a feature to a standardized feature'''\n    data[column]=StandardScaler().fit_transform(data[column].values.reshape(-1,1))  #standardizing a column\n    return None\n\ndef split_dataset(input_df,col_name):\n    '''this function splits the dataset as X and Y'''\n    X=input_df.drop(columns=[col_name]).to_numpy()                                  #extracting X\n    y=input_df[col_name].to_numpy()                                                 #extracting X\n    return X,y\n\ndef precision_recall_score(classifier,X,y):\n    '''this function calculates precision and recall score'''\n    from sklearn.metrics import precision_score,recall_score\n    y_pred=classifier.predict(X)                                                    #predict y values\n    return precision_score(y_true=y,y_pred=y_pred),recall_score(y_true=y,y_pred=y_pred)               #calculating precision and recall scores","4e3b5228":"#Reading data\ndataset=pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","3cdd0d2b":"#Time\ncalculate_statistics_and_make_a_plot(dataset['Time'])                           #calculating descriptive statistics and making graph","2fda51c1":"#Converting time feature to hours \ndataset['Time-Hour']=[i%24 for i in dataset['Time']\/3600]                           #converting seconds to hours\ndataset['Time-Hour']=dataset['Time-Hour'].astype(int)                               #converting it to integer\ndataset.drop(columns=['Time'],inplace=True)                                         #dropping the old time feature. After having time-hour feature, no need to keep this feature any more.","ddd880a9":"#V1\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V1'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V1'])","ba6879dc":"#V2\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V2'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V2'])","eec7c094":"#V3\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V3'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V3'])","f0d5cae3":"#V4\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V4'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V4'])","bf91a0d6":"#V5\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V5'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V5'])","ee9883f3":"#V6\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V6'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V6'])","bf04b4e8":"#V7\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V7'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V7'])","b6495dac":"#V8\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V8'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V8'])","88361fcb":"#V9\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V9'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V9'])","44f448ed":"#V10\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V10'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V10'])","b182a952":"#V11\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V11'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V11'])","a6c0ae30":"#V12\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V12'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V12'])","02e3ac42":"#V13\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V13'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V13'])","308e9440":"#V14\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V14'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V14'])","ec98e40c":"#V15\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V15'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V15'])","513fbc0f":"#V16\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V16'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V16'])","b9d32c23":"#V17\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V17'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V18'])","52f28685":"#V18\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V18'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V18'])","e17fcbf3":"#V19\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V19'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V19'])","4f4b8270":"#V20\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V20'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V20'])","03d2d05a":"#V21\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V21'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V21'])","b732415b":"#V22\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V22'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V22'])","f35771ac":"#V23\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V23'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V23'])","6f7f9a17":"#V24\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V24'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V24'])","1b06630d":"#V25\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V25'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V25'])","22eef243":"#V26\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V26'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V26'])","1cae9245":"#V27\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V27'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V27'])","c486079e":"#V28\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['V28'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['V28'])","846a494c":"#Amount\nprint('Descriptive Statistics\\n')\nprint('Original Data\\n')\ncalculate_statistics_and_make_a_plot(dataset['Amount'])\nprint('\\nData Without Extremes\\n')\ncalculate_statistics_and_make_a_plot_without_extremes(dataset['Amount'],lower_threshold=0,upper_threshold=0.9)","7992d3b8":"#Target Variable\nprint('Value Counts')\nprint(dataset['Class'].value_counts())\nprint('\\nFraund Ratio:{:.3f}'.format(dataset['Class'].mean()))","f9dac7d1":"features_with_extreme_values=['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11','V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n                              'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n#dataset_v1:Amount feature is standardized.\ndataset_v1=dataset.copy()                                                       # creating a copy of dataset.\nstandardize_a_column(data=dataset_v1,column='Amount')                           # standardizing \"Amount\" feature.\n\n#dataset_v2\n#Amount feature is standardized+Extreme values are flagged\ndataset_v2=dataset.copy()                                                       # creating a copy of dataset.\nstandardize_a_column(data=dataset_v2,column='Amount')                           # standardizing \"Amount\" feature.\nmark_extreme_values(input_data=dataset_v2,                                      # marking extreme values\n                    col_list=features_with_extreme_values,\n                    lower_threshold=0,upper_threshold=0.99)\n#dataset_v3\n#Amount feature is standardized+Extreme values are removed\ndataset_v3=dataset.copy()                                                       # creating a copy of dataset.\nstandardize_a_column(data=dataset_v3,column='Amount')                           # standardizing \"Amount\" feature.\ndataset_v3_refined=remove_extremes(dataset_v3,features_with_extreme_values)     # removing extreme values\n\n#dataset_v4\n#Amount feature is standardized+Extreme values are replaced with median.\ndataset_v4=dataset_v2.copy()                                                    # creating a copy of dataset.\nstandardize_a_column(data=dataset_v4,column='Amount')                           # standardizing \"Amount\" feature.\nreplace_extremes(dataset_v4,features_with_extreme_values)                       # replacing extreme values with median","3a402b54":"#in this section, I work on 4 different datasets and find classifiers to predict fraud cases. \n#As I am doing that, I also compare effect of different extreme value replacement techniques on predictive modelling process.\n#I have run and found parameters of logistic regression and decision tree on my own with grid search cv and use them directly below.\n\nclassifier_dict=pd.DataFrame(columns=['Classifier','Precision_score','Recall_score'])   #creating a dataframe to store models' results\n\n#1st dataset: dataset_v1 --> dataset + Amount feature standardizated and no extreme values modification\nX,y=split_dataset(dataset_v1,'Class')                                                     #splitting the features as dependent(y) and independent variables(X).\nlogistic_regression_1=LogisticRegression(C=0.1,class_weight=None, dual=False,             #creating a logistic regression classifier\n                                         fit_intercept=True,intercept_scaling=1,l1_ratio=0.1, \n                                         max_iter=10,multi_class='warn', n_jobs=None, \n                                         penalty='elasticnet',random_state=None, solver='saga', \n                                         tol=0.0001,verbose=0,warm_start=False)\nlogistic_regression_1.fit(X,y)                                                             #fitting the logistic regression classifier\nprecision_score,recall_score=precision_recall_score(logistic_regression_1,X,y)             #calculating precision (hit ratio) and recall (get ratio) scores\nclassifier_dict.loc['logistic_regression_1']=[logistic_regression_1,precision_score,recall_score] #saving the results in the dataframe\ndecision_tree_1=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,   #creating a decision tree classifier\n                                       max_features=0.8, max_leaf_nodes=None,\n                                       min_impurity_decrease=0.0, min_impurity_split=None,\n                                       min_samples_leaf=1, min_samples_split=0.001,\n                                       min_weight_fraction_leaf=0.0, presort=False,\n                                       random_state=123, splitter='best')\ndecision_tree_1.fit(X,y)                                                                          #fitting the decision tree classifier\nprecision_score,recall_score=precision_recall_score(decision_tree_1,X,y)                          #calculating precision (hit ratio) and recall (get ratio) scores\nclassifier_dict.loc['decision_tree_1']=[decision_tree_1,precision_score,recall_score]             #saving results in the dataframe\n\n#2nd dataset: dataset_v2  --> dataset + Amount feature standardized + Extreme values flagged\nX,y=split_dataset(dataset_v2,'Class')                                                             #splitting the features as dependent(y) and independent variables(X).\nlogistic_regression_2=LogisticRegression(C=100, class_weight=None, dual=False,                    #creating a logistic regression classifier \n                                       fit_intercept=True, intercept_scaling=1, l1_ratio=0.7, \n                                       max_iter=10, multi_class='warn', n_jobs=None, \n                                       penalty='elasticnet', random_state=None, solver='saga', \n                                       tol=0.0001, verbose=0, warm_start=False)\nlogistic_regression_2.fit(X,y)                                                                    #fitting the logistic regression classifier\nprecision_score,recall_score=precision_recall_score(logistic_regression_2,X,y)                    #calculating precision (hit ratio) and recall (get ratio) scores\nclassifier_dict.loc['logistic_regression_2']=[logistic_regression_2,precision_score,recall_score] #saving the results in the dataframe\ndecision_tree_2=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,          #creating a decision tree classifier\n                                       max_features=0.5, max_leaf_nodes=None,\n                                       min_impurity_decrease=0.0, min_impurity_split=None,\n                                       min_samples_leaf=1, min_samples_split=0.001,\n                                       min_weight_fraction_leaf=0.0, presort=False,\n                                       random_state=123, splitter='best')\ndecision_tree_2.fit(X,y)                                                                          #fitting the decision tree classifier\nprecision_score,recall_score=precision_recall_score(decision_tree_2,X,y)                          #calculating precision (hit ratio) and recall (get ratio) scores\nclassifier_dict.loc['decision_tree_2']=[decision_tree_2,precision_score,recall_score]             #saving results in the dataframe\n\n#3rd dataset: dataset_v3 --> dataset+Amount feature standardized + Extreme values are removed\nX,y=split_dataset(dataset_v3,'Class')                                                             #splitting the features as dependent(y) and independent variables(X).\nlogistic_regression_3=LogisticRegression(C=0.01, class_weight=None, dual=False,                   #creating a logistic regression classifier  \n                                       fit_intercept=True, intercept_scaling=1, l1_ratio=0.1, \n                                       max_iter=10, multi_class='warn', n_jobs=None, \n                                       penalty='elasticnet', random_state=None, solver='saga', \n                                       tol=0.0001, verbose=0, warm_start=False)\n\nlogistic_regression_3.fit(X,y)                                                                    #fitting the logistic regression classifier\nprecision_score,recall_score=precision_recall_score(logistic_regression_3,X,y)                    #calculating precision (hit ratio) and recall (get ratio) scores\nclassifier_dict.loc['logistic_regression_3']=[logistic_regression_3,precision_score,recall_score] #saving the results in the dataframe\ndecision_tree_3=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,          #creating a decision tree classifier\n                                       max_features=0.1, max_leaf_nodes=None,\n                                       min_impurity_decrease=0.0, min_impurity_split=None,\n                                       min_samples_leaf=1, min_samples_split=0.1,\n                                       min_weight_fraction_leaf=0.0, presort=False,\n                                       random_state=123, splitter='best')\ndecision_tree_3.fit(X,y)                                                                          #fitting the decision tree classifier\nprecision_score,recall_score=precision_recall_score(decision_tree_3,X,y)                          #calculating precision (hit ratio) and recall (get ratio) scores\nclassifier_dict.loc['decision_tree_3']=[decision_tree_3,precision_score,recall_score]             #saving results in the dataframe\n\n\n\n#4rd dataset: dataset_v4--> dataset+Amount feature standardized + Extreme values are replaced with median.\nX,y=split_dataset(dataset_v4,'Class')                                                             #splitting the features as dependent(y) and independent variables(X).\nlogistic_regression=LogisticRegression(C=10, class_weight=None, dual=False,                       #creating a logistic regression classifier  \n                                       fit_intercept=True, intercept_scaling=1, l1_ratio=0.7, \n                                       max_iter=10, multi_class='warn', n_jobs=None, \n                                       penalty='elasticnet', random_state=None, solver='saga', \n                                       tol=0.0001, verbose=0, warm_start=False)\nlogistic_regression.fit(X,y)                                                                      #fitting the logistic regression classifier\nprecision_score,recall_score=precision_recall_score(logistic_regression,X,y)                      #calculating precision (hit ratio) and recall (get ratio) scores\nclassifier_dict.loc['logistic_regression_4']=[logistic_regression,precision_score,recall_score]   #saving the results in the dataframe\ndecision_tree_4=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=6,          #creating a decision tree classifier\n                                       max_features=0.8, max_leaf_nodes=None,\n                                       min_impurity_decrease=0.0, min_impurity_split=None,\n                                       min_samples_leaf=1, min_samples_split=0.001,\n                                       min_weight_fraction_leaf=0.0, presort=False,\n                                       random_state=123, splitter='best')\ndecision_tree_4.fit(X,y)                                                                          #fitting the decision tree classifier\nprecision_score,recall_score=precision_recall_score(decision_tree_4,X,y)                          #calculating precision (hit ratio) and recall (get ratio) scores\nclassifier_dict.loc['decision_tree_4']=[decision_tree_4,precision_score,recall_score]             #saving results in the dataframe","1ad173e5":"#creating a plot to compare results easier\n#creating a new feature for dataset information.\nclassifier_dict.loc['logistic_regression_1','Data_Process_Summary']='Amount feature is standardized'\nclassifier_dict.loc['decision_tree_1','Data_Process_Summary']='Amount feature is standardized'\nclassifier_dict.loc['logistic_regression_2','Data_Process_Summary']='Amount feature is standardized+Extreme values are flagged'\nclassifier_dict.loc['decision_tree_2','Data_Process_Summary']='Amount feature is standardized+Extreme values are flagged'\nclassifier_dict.loc['logistic_regression_3','Data_Process_Summary']='Amount feature is standardized+Extreme values are removed'\nclassifier_dict.loc['decision_tree_3','Data_Process_Summary']='Amount feature is standardized are removed'\nclassifier_dict.loc['logistic_regression_4','Data_Process_Summary']='Amount feature is standardized+Extreme values are flagged and replaced'\nclassifier_dict.loc['decision_tree_4','Data_Process_Summary']='Amount feature is standardized+Extreme values are flagged and replaced'\n#make a plot\nplt.figure(figsize=(12,8))\nsns.scatterplot(x='Precision_score',y='Recall_score',data=classifier_dict,style='Data_Process_Summary',s=100)\nplt.title('Classifiers Precision-Recall Plot')\nplt.xlabel('Precision')\nplt.ylabel('Recall');","f018068c":"# 3- Predictive Modelling","423dbf7d":"# 1-Explanatory Data Analysis","15611d4d":"User Defined Functions","d8b8293d":"# 2- Preparing Data for Predictive Modelling"}}