{"cell_type":{"b4701e05":"code","08b8715f":"code","1d1a3239":"code","3c7f76e4":"code","bc692dae":"code","0b4a80b9":"code","512292a6":"code","ee7f642f":"code","6b1c9ad5":"code","2979c01b":"code","f602649e":"code","a9b272c4":"code","afe98ce6":"markdown","21e834d8":"markdown","7d75c444":"markdown","a8eaf876":"markdown","a0099af9":"markdown","b4374385":"markdown","76acd321":"markdown","a605a8f9":"markdown","c92b6185":"markdown","b1060940":"markdown","633c3cc7":"markdown"},"source":{"b4701e05":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport spacy\nfrom spacy import displacy\nfrom sklearn.decomposition import PCA\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","08b8715f":"train_data = pd.read_csv(\"..\/input\/train.csv\")","1d1a3239":"model = spacy.load(\"en_core_web_lg\")","3c7f76e4":"train_data.info()","bc692dae":"comments = train_data[['comment_text','target']].copy()\ncomments_sample = comments.sample(n=1000)\ncomments_sample.head()","0b4a80b9":"docs = []\nfor row in comments_sample.iterrows():\n    doc = model(row[1][0])\n    docs.append((doc, row[1][1], row[0]))","512292a6":"tokens = []\n\nfor (doc, target, _id) in docs:\n    tokens.extend([(_id,\\\n                    target,\\\n                    token.text,\\\n                    token.lemma_,\\\n                    token.pos_,\\\n                    token.tag_,\\\n                    token.dep_,\\\n                    token.shape_,\\\n                    token.is_alpha,\\\n                    token.is_stop\\\n                   ) for token in doc])\n\ntokens_df = pd.DataFrame(data = tokens, columns=['id','target','text','lemma','pos','tag','dep','shape','is_alpha', 'is_stop'])\nprint('Tokens Shape: {}'.format(tokens_df.shape))","ee7f642f":"tokens_df.sample(n=20)","6b1c9ad5":"noun_tokens_df = tokens_df[(tokens_df['pos'] == 'NOUN') & (tokens_df['is_alpha'] == True) & (tokens_df['is_stop'] == False)]\nnoun_tokens_df_counts = noun_tokens_df.groupby(by='lemma').agg({ 'target': 'mean', 'text': 'count'})","2979c01b":"vectors = []\nwords = []\ntargets = []\ncounts = []\n\n\nfor doc, target, _id in docs:\n    for token in doc:\n        if token.has_vector and token.is_stop == False and token.pos_ == 'NOUN':\n            product_count = tokens_df[tokens_df['text'] == token.text]['target'].unique().size            \n            counts.append(product_count)\n            targets.append(target)\n            words.append(token.text)\n            vectors.append(token.vector)\n        \nvec_df = pd.DataFrame(data = vectors, index=words)\n\npca = PCA(n_components=2)\nred_vec = pca.fit_transform(vec_df.values)\n\nvec_df['PC1'] = red_vec[:,0]\nvec_df['PC2'] = red_vec[:,1]\nvec_df['target'] = targets\nvec_df['counts'] = counts\nvec_df.head()","f602649e":"vec_df_pos = vec_df[vec_df['target'] >= 0.5]\nplt.figure(figsize=(25,25))\np1 = sns.scatterplot(x='PC1', y='PC2', size='counts', hue='target', data=vec_df_pos)\n\nfor line in range(0,vec_df_pos.shape[0]):\n    if vec_df_pos.counts[line] > 5: \n        p1.text(vec_df_pos.PC1[line]+0.2, vec_df_pos.PC2[line], vec_df_pos.index[line], horizontalalignment='left', size='large', color='black')","a9b272c4":"vec_df_neg = vec_df[vec_df['target'] < 0.5]\nplt.figure(figsize=(25,25))\np1 = sns.scatterplot(x='PC1', y='PC2', size='counts', hue='target', data=vec_df_neg)\n\nfor line in range(0,vec_df_neg.shape[0]):\n    if vec_df_neg.counts[line] > 5: \n        p1.text(vec_df_neg.PC1[line]+0.2, vec_df_neg.PC2[line], vec_df_neg.index[line], horizontalalignment='left', size='large', color='black')","afe98ce6":"## Prepare the tokens for plotting\n\n- For each token I'm fetching the correspondig vector representation.\n- To plot the vector I have to reduce the dimensionality. For this, PCA with two components is applied","21e834d8":"## Extract the tokens\n\n- For each document, extract the tokens and some further info like POS...\n- Save each token (with its correpsonding target and id) in a data frame","7d75c444":"## Plot nouns with target mean >= 0.5","a8eaf876":"## Plot nouns with target < 0.5","a0099af9":"## Select relevant columns and some samples for further analysis\n\n- First of all, in this notebook I'm focusing on the comment text and the target variable\n- To avoid long runtimes, I'm choosing a subset of the data","b4374385":"# Inspect Nouns\n\n- Now I'm only interested in nouns. So lets filter the token data frame and compute some aggregations\n- The aggregations are computed over groups of 'lemma'.\n- The count, just counts the occurences of the lemma in the samples\n- The mean, computes the mean of the target variable. One token (lemma) can of course occur with a postive and\/or negative variable. So the mean may distorting the truth.","76acd321":"## Load the english spacy model","a605a8f9":"## Apply the spcay model to each comment text\n\n- The spacy model computes some metrics for the comment text\n- To not loose the target and the id, both is saved with the comment metrics in a tuple","c92b6185":"## Load the data","b1060940":"## First look","633c3cc7":"## Findings\n\n- After some runs, it seems, that negative nouns seem to be more present in targets >= 0.5\n- But that's just a first impression and a very subjective too :-)\n- In further analysis I want to look at verbs and adjectives in the same way. Also I want to compute the TF\/IDF to get other insights\n- ....."}}