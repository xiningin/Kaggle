{"cell_type":{"645b7f12":"code","9c105df0":"code","d9515407":"code","8301127a":"code","4e96d625":"code","2c5c7609":"code","e6caaf37":"code","c40b5f3c":"code","cd57971f":"code","d46adc54":"code","e4860390":"code","6be27012":"code","7ed4b7f4":"code","11ac920a":"code","d5e45fb5":"code","c8d26b4a":"code","9f23e48b":"code","7cbb8074":"code","27ea6069":"code","ed2bfc47":"code","a9b3eb15":"code","020576a3":"code","2462ac93":"code","50c24894":"code","36fe8bc6":"code","4e0332d7":"code","fe22a68a":"code","d9b6739f":"code","9edf4374":"code","8e64fd0b":"code","e805e027":"code","bb441109":"code","8854c423":"code","52b0648a":"markdown","a673a7d8":"markdown","60109490":"markdown","2261458c":"markdown","48cb92c3":"markdown","26fd4ffe":"markdown","f06f1a33":"markdown","2eebf722":"markdown","dd898d76":"markdown","32c8f6c8":"markdown","c437aea5":"markdown","30f0e645":"markdown","b45755a1":"markdown","ccdbeccf":"markdown","c6d42864":"markdown","877ad92f":"markdown","9ba33a22":"markdown","6ee3ebcd":"markdown","877140ae":"markdown","b37a7e4a":"markdown","04bd0ba3":"markdown","033729b1":"markdown","9830e19c":"markdown","690d68e6":"markdown","5d327b4b":"markdown","e763a0ab":"markdown","11bc6661":"markdown","1b55e490":"markdown","b3cbefc7":"markdown","c9e12518":"markdown","eddea00d":"markdown","cddb56f2":"markdown","733bab84":"markdown","b34bb06a":"markdown","40cf2575":"markdown"},"source":{"645b7f12":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# thu vien in h\u00ecnh anh \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# thu vien tinh toan \nfrom scipy.stats import norm\nfrom scipy import stats\nimport warnings\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import preprocessing\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n# thu vien chia Kfold\nfrom sklearn.model_selection import KFold\n# thu vien chia tinh toan mean_square_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split","9c105df0":"train = pd.read_csv('..\/input\/massp-housing-prices-in-melbourne\/train.csv')\ntest = pd.read_csv('..\/input\/massp-housing-prices-in-melbourne\/test.csv')\n","d9515407":"train.head(10)","8301127a":"test.head(10)","4e96d625":"ntrain = train.shape[0]\nntest = test.shape[0]\ntarget = train.Price.values\ndata = pd.concat((train, test)).reset_index(drop=True)\ndata.drop(['Price'], axis=1, inplace=True)\nprint(\"data size is : {}\".format(data.shape))","2c5c7609":"def missing_values_table(df):\n    # Total missing values\n    mis_val = df.isnull().sum()       \n    # Percentage of missing values\n    mis_val_percent = 100 * df.isnull().sum() \/ len(df)       \n    # Make a table with the results\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)        \n    # Rename the columns\n    mis_val_table_ren_columns = mis_val_table.rename(\n    columns = {0 : 'Missing Values', 1 : '% of Total Values'})        \n    # Sort the table by percentage of missing descending\n    mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n    '% of Total Values', ascending=False).round(1)\n    return mis_val_table_ren_columns","e6caaf37":"missing_values_table(data)","c40b5f3c":"rate_nan_columns = ['CouncilArea', 'Regionname', 'Propertycount','Distance', 'Postcode']\nfor feature in rate_nan_columns:\n    data[feature] = data[feature].fillna(data[feature].mode()[0])","cd57971f":"create_nan_columns = [\"BuildingArea\", \"YearBuilt\", \"Landsize\", \"Car\", \"Bathroom\", \"Bedroom2\", \"Lattitude\"]\nfor feature in create_nan_columns: \n    data[feature+\"_NAN\"] = data[feature].isnull()*1","d46adc54":"fill_mean_nan = ['BuildingArea', 'Landsize', 'Lattitude', 'Longtitude']\nfor feature in fill_mean_nan:\n    data[feature]= data[feature].fillna(0)","e4860390":"fill_min_nan = ['Car', 'Bedroom2', 'Bathroom', 'YearBuilt']\nfor feature in fill_min_nan:\n    data[feature]= data[feature].fillna(0)","6be27012":"missing_values_table(data)","7ed4b7f4":"data.head()","11ac920a":"data['Date'].dtype\n","d5e45fb5":"# Convert Datetime string to Datetime Format\ndata['Date'] = pd.to_datetime(data['Date'], format=\"%d\/%m\/%Y\")","c8d26b4a":"data['Month'] = data['Date'].dt.month\ndata['Year'] = data['Date'].dt.year","9f23e48b":"categorical_features = data.select_dtypes(exclude=[np.number])\ncategorical_features.columns","7cbb8074":"\ndata = pd.get_dummies(data, columns = ['CouncilArea', 'Method', 'Type'])","27ea6069":"\nfor col in ['Suburb', 'SellerG', 'Regionname']: \n    data[col+ \"_encoding\"] = preprocessing.LabelEncoder().fit_transform(data[col])","ed2bfc47":"\ndata = data.drop(columns = ['Suburb','Address', 'SellerG', 'Date',\n       'Regionname', \"id\" ])","a9b3eb15":"data.head()","020576a3":"data.head()","2462ac93":"train_set = data[:ntrain]\ntest_set = data[ntrain:]","50c24894":"train_set","36fe8bc6":"target = np.log1p(target)","4e0332d7":"X_train, X_valid, y_train, y_valid = train_test_split(train_set, target, test_size=0.2) ","fe22a68a":"params = {  'bootstrap': True,\n            'criterion': 'mse',\n            'max_depth': 4,\n            'max_features': 'auto',\n            'max_leaf_nodes': None,\n            'min_impurity_decrease': 0.0,\n            'min_impurity_split': None,\n            'min_samples_leaf': 1,\n            'min_samples_split': 2,\n            'min_weight_fraction_leaf': 0.0,\n            'n_estimators': 100,\n            'n_jobs': 1,\n            'oob_score': False,\n            'random_state': 42,\n            'verbose': 0,\n            'warm_start': False}\n\ndef run_RFR(train_X, train_y, valid_X, valid_y, params):\n    \"\"\"\n    args: \n    + train_X : training dataset\n    + valid_X : validataion dataset\n    + train_y : target of training set\n    + valid_y : target of validation set\n    + params : parameters of RandomForestRegressor\n    \n    output: \n    \n    + model   : The RandomForestRegressor model which is trained by the train_X, train_y\n    + y_predict : The prediction of valid_X by using the above model\n    \"\"\"\n    model = RandomForestRegressor( max_depth = params['max_depth'], random_state=0)\n    model = model.fit(train_X, train_y)\n    y_predict =  model.predict(X_valid)\n    rmse = mean_squared_error(y_valid, y_predict, squared = False)\n    print(rmse)\n    return  model, y_predict","d9b6739f":"RFR, _  = run_RFR(X_train, y_train, X_valid, y_valid, params)","9edf4374":"predicted_X_test = RFR.predict(test_set)","8e64fd0b":"predicted_X_test = np.expm1(predicted_X_test)","e805e027":"test  = pd.read_csv('..\/input\/massp-housing-prices-in-melbourne\/test.csv')","bb441109":"my_submission = pd.DataFrame({'id': test.id, 'Price': predicted_X_test})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('.\/submission.csv', index=False)","8854c423":"my_submission.head(10)","52b0648a":"### S\u1eed d\u1ee5ng ph\u00e9p  bi\u1ebfn \u0111\u1ed5i ng\u01b0\u1ee3c exp1m c\u1ee7a ph\u00e9p bi\u1ebfn \u0111\u1ed5i log1p tr\u00ean prediction t\u1eadp test","a673a7d8":"# <a id='1.1'>1.1. X\u1eed l\u00fd missing data<\/a>","60109490":"## Label Encoding columns: ['Suburb', 'SellerG', 'Regionname']","2261458c":"### T\u1ea1o ra c\u00e1c tr\u01b0\u1eddng m\u1edbi v\u1ec1 th\u00f4ng tin c\u00f3 NaN hay kh\u00f4ng, \u0111\u1ec3 \u0111\u00e1nh d\u1ea5u b\u1ea5t th\u01b0\u1eddng c\u1ee7a d\u1eef li\u1ec7u ","48cb92c3":"#### T\u1ea1o c\u00e1c c\u1ed9t m\u1edbi t\u1eeb tr\u01b0\u1eddng d\u1eef li\u1ec7u **Date**","26fd4ffe":"# <a id='1'>1. Feature engineering<\/a>","f06f1a33":"### S\u1eed d\u1ee5ng ph\u00e9p bi\u1ebfn \u0111\u1ed5i log1p cho target","2eebf722":"Xem \u0111\u1ecbnh d\u1ea1ng c\u1ee7a c\u1ed9t Date","dd898d76":"#### G\u1ed9p 2 d\u1eef li\u1ec7u train v\u00e0 test, l\u01b0u tr\u1eef d\u1eef li\u1ec7u target, \u0111\u1ec3 x\u1eed l\u00fd d\u1eef li\u1ec7u","32c8f6c8":"\n### X\u1eed l\u00fd v\u1edbi c\u00e1c c\u1ed9t c\u00f3 t\u1ef7 l\u1ec7n NaN \u00edt","c437aea5":"# Summary:\n\u1ede notebook ch\u00fang ta \u0111\u00e3 th\u1ef1c hi\u1ec7n: \n+ T\u1ea1o c\u00e1c c\u1ed9t d\u1eef li\u1ec7u m\u1edbi t\u1eeb th\u00f4ng tin missing data\n+ \u0110i\u1ec1n d\u1eef li\u1ec7u thi\u1ebft b\u1eb1ng c\u00e1c ph\u01b0\u01a1ng ph\u00e1p kh\u00e1c nhau\n+ X\u1eed l\u00fd d\u1eef li\u1ec7u ph\u00e2n lo\u1ea1i b\u1eb1ng 2 c\u00e1ch c\u01a1 b\u1ea3n: One hot encoding v\u00e0 label encoding (xem [link](https:\/\/towardsdatascience.com\/all-about-categorical-variable-encoding-305f3361fd02))\n+ Chia d\u1eef li\u1ec7u train th\u00e0nh 2 t\u1eadp train v\u00e0 valid \u0111\u1ec3 \u0111o \u0111\u1ea1c \u0111\u1ed9 ch\u00ednh x\u00e1c tr\u00ean local \n+ Hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh Random Forest cho d\u1eef li\u00eau \n+ S\u1eed d\u1ee5ng m\u00f4 h\u00ecnh hu\u1ea5n luy\u1ec7n \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n tr\u00ean t\u1eadp test\n+ T\u1ea1o file submission \n\n\n","30f0e645":"### B\u1ea3ng d\u1eef li\u1ec7u thi\u1ebfu ","b45755a1":"# <a id='2.=2'>2.2. X\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh hu\u1ea5n luy\u1ec7n<\/a> ","ccdbeccf":"## \u0110\u1ecdc file d\u1eef li\u1ec7u ","c6d42864":"### Fill NaN c\u00e1c c\u1ed9t d\u1eef li\u1ec7u: ","877ad92f":"#### Quan s\u00e1t d\u1eef li\u1ec7u train ","9ba33a22":"## Outline\n\n- <a href='#1'>1. X\u1eed l\u00fd d\u1eef li\u1ec7u  <\/a>\n    - <a id='#1-1'>1.1. X\u1eed l\u00fd d\u1eef li\u1ec7u thi\u1ebfu  <\/a>\n    - <a id='#1-2'>1.2. X\u1eed l\u00fd d\u1eef li\u1ec7u ph\u00e2n lo\u1ea1i <\/a> \n- <a href='#2'>4. X\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh <\/a>\n    - <a id='#2-1'>2.1. X\u00e2y d\u1ef1ng validation <\/a>\n    - <a id='#2-2'>2.2. X\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh<\/a> ","6ee3ebcd":"#### K\u1ebft qu\u1ea3 sau khi fill NaN","877140ae":"Sau khi x\u1eed l\u00fd d\u1eef li\u1ec7u, ch\u00fang ta chia l\u1ea1i t\u1eadp train v\u00e0 test \u0111\u1ec3 ti\u1ebfn h\u00e0nh x\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh","b37a7e4a":"#### Fill NaN b\u1eb1ng **mean**","04bd0ba3":"# <a id='2'>2. X\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh<\/a>","033729b1":"## X\u00f3a c\u00e1c c\u1ed9t **kh\u00f4ng c\u1ea7n thi\u1ebft**","9830e19c":"### One hot encoding columns: ['CouncilArea', 'Method', 'Type']","690d68e6":"### X\u1eed l\u00fd d\u1eef li\u1ec7u ng\u00e0y th\u00e1ng","5d327b4b":"#### Chuy\u1ec3n \u0111\u1ecbnh d\u1ea1ng c\u1ee7a c\u1ed9t **Date** sang d\u1ea1ng Datatime","e763a0ab":"Ch\u00fang ta th\u1ea5y c\u1ed9t **Date** c\u00f3 \u0111\u1ecbnh d\u1ea1ng string","11bc6661":"# M\u1ed9t s\u1ed1 h\u01b0\u1edbng c\u1ea3i thi\u1ec7n m\u00f4 h\u00ecnh \n\n+ X\u1eed l\u00fd d\u1eef li\u1ec7u thi\u1ebfu k\u0129 h\u01a1n, t\u1ea1o ta nh\u1eefng c\u1ed9t t\u1eeb d\u1eef li\u1ec7u missing c\u00f3 \u00fd ngh\u0129a h\u01a1n \n+ S\u1eed d\u1ee5ng c\u00e1c encoding kh\u00e1c ph\u00f9 h\u1ee3p v\u1edbi c\u00e1c c\u1ed9t d\u1eef li\u1ec7u, (ex: Frequency Encoding, Mean Encoding, ...)\n+ Extract th\u00f4ng tin h\u1eefu \u00edch t\u1eeb c\u1ed9t address (ex: ex track th\u00f4ng tin Ave, Rd, St, Ln, Dr, Way, Pl, Blvd etc t\u1eeb tr\u01b0\u1eddng \u0111\u1ecba ch\u1ec9 nh\u00e0)\n+ S\u1eed d\u1ee5ng th\u00f4ng tin t\u1ecda \u0111\u1ed9 \n+ S\u1eed d\u1ee5ng c\u00e1c transform cho d\u1eef li\u1ec7u s\u1ed1\n+ T\u00ecm c\u00e1c d\u1eef li\u1ec7u outlier \n+ S\u1eed d\u1ee5ng K-fold thay v\u00ec ch\u1ec9 chia train-valid set\n+ Tinh ch\u1ec9nh c\u00e1c parameter c\u1ee7a Random Forest\n+ S\u1eed d\u1ee5ng c\u00e1c thu\u1eadt to\u00e1n kh\u00e1c m\u1ea1nh h\u01a1n, \n+ ...\n","1b55e490":"# M\u00f4 t\u1ea3: \nM\u1ee5c ti\u00eau c\u1ee7a notebook nh\u1eb1m gi\u00fap c\u00e1c b\u1ea1n h\u1ecdc vi\u00ean MaSSP hi\u1ec3u \u0111\u01b0\u1ee3c baseline x\u00e2y d\u1ef1ng Feature Engineering  and Modeling trong cu\u1ed9c thi House Pricing.  M\u1ee5c ti\u00eau c\u1ee5 th\u1ec3 nh\u01b0 sau: \n+ X\u1eed l\u00fd m\u1ed9t s\u1ed1 c\u1ed9t d\u1eef li\u1ec7u \n+ X\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh\n+ T\u1ea1o file submission\n+ Th\u1ea3o lu\u1eadn c\u00e1c h\u01b0\u1edbng ph\u00e1t tri\u1ec3n m\u00f4 h\u00ecnh ","b3cbefc7":"#### Fill NaN b\u1eb1ng **min**","c9e12518":"### D\u1ef1 \u0111o\u00e1n k\u1ebft qu\u1ea3 tr\u00ean t\u1eadp test, ghi d\u1ef1 \u0111o\u00e1n tr\u00ean file csv","eddea00d":"# <a id='2.1'>2.1. Chia d\u1eef li\u1ec7u th\u00e0nh training v\u00e0 validation<\/a> ","cddb56f2":"\u0110\u1ecdc l\u1ea1i file test \u0111\u1ec3 l\u1ea5y c\u1ed9t id ","733bab84":"# <a id='1.2'>1.2. X\u1eed l\u00fd d\u1eef li\u1ec7u d\u1ea1ng ph\u00e2n lo\u1ea1i <\/a>","b34bb06a":"#### Quan s\u00e1t d\u1eef li\u1ec7u test","40cf2575":"#### Ti\u1ebfp t\u1ee5c x\u1eed l\u00fd d\u1eef li\u1ec7u ph\u00e2n lo\u1ea1i "}}