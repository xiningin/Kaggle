{"cell_type":{"d93fe8f8":"code","f2e78b2b":"code","0eff6479":"code","63619acf":"code","41b86fcb":"code","2b68add1":"code","d54c3e2f":"code","deea6af2":"code","3082dbcd":"code","bc3412ed":"code","32d8d8a4":"code","754731b3":"code","8eae8bff":"code","3d4d2ee4":"code","e9b23976":"code","35817e7d":"code","4ef61dc4":"code","f95c0abe":"code","10a2a23a":"code","5756a8b3":"code","fab6b5fd":"code","cbfedaed":"code","ec81623b":"code","a295be09":"code","d7b80772":"code","3867cbee":"code","d764b34b":"code","00db3ede":"code","b659be04":"code","c03fd646":"code","614ef5b1":"code","868433e0":"markdown","73986e23":"markdown","e3de78c3":"markdown","8c59e6e2":"markdown","88148e8d":"markdown","d4cd1ea0":"markdown","db8247b0":"markdown"},"source":{"d93fe8f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2e78b2b":"import matplotlib.pyplot  as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","0eff6479":"data = pd.read_csv(\"..\/input\/knn-project-data\/KNN_Project_Data\")\ndata.head()","63619acf":"data.shape","41b86fcb":"data.columns","2b68add1":"data.describe()","d54c3e2f":"data_new = data.copy()\ny = data[['TARGET CLASS']]\nx = data.drop('TARGET CLASS', axis=1)","deea6af2":"x.shape","3082dbcd":"y.shape","bc3412ed":"data.info()","32d8d8a4":"data.describe()","754731b3":"data.isnull().sum()","8eae8bff":"data.skew().plot()","3d4d2ee4":"data.skew()","e9b23976":"sns.pairplot(data, hue='TARGET CLASS', palette='viridis')","35817e7d":"data['TARGET CLASS'].value_counts()","4ef61dc4":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx = scaler.fit_transform(x)","f95c0abe":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)","10a2a23a":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nk_fold = KFold(n_splits=3, shuffle=True, random_state=0)","5756a8b3":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression() \nlogmodel.fit(x_train,y_train)\nlogpred = logmodel.predict(x_test)\n\n\nprint(confusion_matrix(y_test, logpred))\nprint(round(accuracy_score(y_test, logpred),2)*100)\nLOGCV = (cross_val_score(logmodel, x_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","fab6b5fd":"from sklearn import model_selection\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Neighbors\nneighbors = np.arange(0,10)\n\n#Create empty list that will hold cv scores\ncv_scores = []\n\n#Perform 10-fold cross validation on training set for odd values of k:\nfor k in neighbors:\n    k_value = k+1\n    knn = KNeighborsClassifier(n_neighbors = k_value, weights='uniform', p=2, metric='euclidean')\n    kfold = model_selection.KFold(n_splits=10, random_state=123)\n    scores = model_selection.cross_val_score(knn, x_train, y_train, cv=kfold, scoring='accuracy')\n    cv_scores.append(scores.mean()*100)\n    print(\"k=%d %0.2f (+\/- %0.2f)\" % (k_value, scores.mean()*100, scores.std()*100))\noptimal_k = neighbors[cv_scores.index(max(cv_scores))]\nprint (\"The optimal number of neighbors is %d with %0.1f%%\" % (optimal_k, cv_scores[optimal_k]))\n\nplt.plot(neighbors, cv_scores)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Train Accuracy')\nplt.show()","cbfedaed":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=9)\nknn.fit(x_train, y_train)\nknnpred = knn.predict(x_test)\n\nprint(confusion_matrix(y_test, knnpred))\nprint(round(accuracy_score(y_test, knnpred),2)*100)\nKNNCV = (cross_val_score(knn, x_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","ec81623b":"from sklearn.svm import SVC\nsvc= SVC(kernel = 'sigmoid')\nsvc.fit(x_train, y_train)\nsvcpred = svc.predict(x_test)\nprint(confusion_matrix(y_test, svcpred))\nprint(round(accuracy_score(y_test, svcpred),2)*100)\nSVCCV = (cross_val_score(svc, x_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","a295be09":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(criterion='gini') #criterion = entopy, gini\ndtree.fit(x_train, y_train)\ndtreepred = dtree.predict(x_test)\n\nprint(confusion_matrix(y_test, dtreepred))\nprint(round(accuracy_score(y_test, dtreepred),2)*100)\nDTREECV = (cross_val_score(dtree, x_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","d7b80772":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators = 200)#criterion = entopy,gini\nrfc.fit(x_train, y_train)\nrfcpred = rfc.predict(x_test)\n\nprint(confusion_matrix(y_test, rfcpred ))\nprint(round(accuracy_score(y_test, rfcpred),2)*100)\nRFCCV = (cross_val_score(rfc, x_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","3867cbee":"from sklearn.naive_bayes import GaussianNB\ngaussiannb= GaussianNB()\ngaussiannb.fit(x_train, y_train)\ngaussiannbpred = gaussiannb.predict(x_test)\nprobs = gaussiannb.predict(x_test)\n\nprint(confusion_matrix(y_test, gaussiannbpred ))\nprint(round(accuracy_score(y_test, gaussiannbpred),2)*100)\nGAUSIAN = (cross_val_score(gaussiannb, x_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","d764b34b":"from xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(x_train, y_train)\nxgbprd = xgb.predict(x_test)\n\nprint(confusion_matrix(y_test, xgbprd ))\nprint(round(accuracy_score(y_test, xgbprd),2)*100)\nXGB = (cross_val_score(estimator = xgb, X = x_train, y = y_train, cv = 10).mean())","00db3ede":"\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ngbkpred = gbk.predict(x_test)\nprint(confusion_matrix(y_test, gbkpred ))\nprint(round(accuracy_score(y_test, gbkpred),2)*100)\nGBKCV = (cross_val_score(gbk, x_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","b659be04":"models = pd.DataFrame({'Models': ['Random Forest Classifier', 'Decision Tree Classifier', 'Support Vector Machine',\n                           'K-Near Neighbors', 'Logistic Model', 'Gausian NB', 'XGBoost', 'Gradient Boosting'],\n                'Score':  [RFCCV, DTREECV, SVCCV, KNNCV, LOGCV, GAUSIAN, XGB, GBKCV]})\n\nmodels.sort_values(by='Score', ascending=False)","c03fd646":"# XGBOOST ROC\/ AUC , BEST MODEL\nfrom sklearn import metrics\nfig, (ax, ax1) = plt.subplots(nrows = 1, ncols = 2, figsize = (15,5))\nprobs = xgb.predict_proba(x_test)\npreds = probs[:,1]\nfprxgb, tprxgb, thresholdxgb = metrics.roc_curve(y_test, preds)\nroc_aucxgb = metrics.auc(fprxgb, tprxgb)\n\nax.plot(fprxgb, tprxgb, 'b', label = 'AUC = %0.2f' % roc_aucxgb)\nax.plot([0, 1], [0, 1],'r--')\nax.set_title('Receiver Operating Characteristic XGBOOST ',fontsize=10)\nax.set_ylabel('True Positive Rate',fontsize=20)\nax.set_xlabel('False Positive Rate',fontsize=15)\nax.legend(loc = 'lower right', prop={'size': 16})\n\n#Gradient\nprobs = gbk.predict_proba(x_test)\npreds = probs[:,1]\nfprgbk, tprgbk, thresholdgbk = metrics.roc_curve(y_test, preds)\nroc_aucgbk = metrics.auc(fprgbk, tprgbk)\n\nax1.plot(fprgbk, tprgbk, 'b', label = 'AUC = %0.2f' % roc_aucgbk)\nax1.plot([0, 1], [0, 1],'r--')\nax1.set_title('Receiver Operating Characteristic GRADIENT BOOST ',fontsize=10)\nax1.set_ylabel('True Positive Rate',fontsize=20)\nax1.set_xlabel('False Positive Rate',fontsize=15)\nax1.legend(loc = 'lower right', prop={'size': 16})\n\nplt.subplots_adjust(wspace=1)","614ef5b1":"from sklearn.metrics import classification_report\nprint('KNN Reports\\n',classification_report(y_test, knnpred))","868433e0":"# **Decision Tree Classifier**","73986e23":"Similarly you can generate the classification Reports for other Models also","e3de78c3":"# **Support Vector Classification**","8c59e6e2":"# **Random Forest**","88148e8d":"# **Gaussian**","d4cd1ea0":"# **K Nearest Neighbors**","db8247b0":"# **Logistic Regression**"}}