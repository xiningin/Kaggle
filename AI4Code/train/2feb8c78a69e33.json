{"cell_type":{"86ec3be2":"code","dc829f01":"code","67eb09ac":"code","60662fe4":"code","d90ccd62":"code","85f4ea2f":"code","a34ae86c":"code","6254eaaa":"code","e9bc39c1":"code","dc77cfd2":"markdown"},"source":{"86ec3be2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dc829f01":"!nvcc --version","67eb09ac":"!conda install -y -c rapidsai dask-cuda","60662fe4":"from dask.distributed import Client, wait\n\nfrom dask.utils import parse_bytes\nimport cudf\nimport dask_cudf","d90ccd62":"from dask_cuda import LocalCUDACluster","85f4ea2f":"!nvidia-smi","a34ae86c":"cluster = LocalCUDACluster(\n    CUDA_VISIBLE_DEVICES=\"0\",\n    rmm_pool_size=parse_bytes(\"14GB\"), # This GPU has 16GB of memory\n    device_memory_limit=parse_bytes(\"8GB\"),\n)\nclient = Client(cluster)\nclient","6254eaaa":"%%time\ntrain_ddf = dask_cudf.read_csv(\"\/kaggle\/input\/riiid-test-answer-prediction\/train.csv\", chunksize=\"500 MB\")\nprint(train_ddf.npartitions)\nlen(train_ddf)","e9bc39c1":"train_ddf.columns","dc77cfd2":"### What I am trying to achieve with this notebook\n\nThe notebook is inspired by the following Medium blogpost by Nvidia: \n\n\"[Reading Larger than Memory CSVs with RAPIDS and Dask](https:\/\/medium.com\/rapids-ai\/reading-larger-than-memory-csvs-with-rapids-and-dask-e6e27dfa6c0f)\"\n\nWe will see how we can load the dask-cuda library (its a conda install) and then read a large dataset\n\n"}}