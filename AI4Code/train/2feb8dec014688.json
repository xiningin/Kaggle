{"cell_type":{"9c8ec5f7":"code","62a6926e":"code","8087c32e":"code","ad7c88c2":"code","6efa36b4":"code","db9b13a6":"code","3644fe31":"code","aef5c2c4":"code","efa97c82":"code","45e06048":"code","4ccd0119":"code","b5082dd2":"code","4825c712":"code","2e2ea7bf":"code","4eda8b39":"code","ec2f2a0c":"code","cbac7807":"code","b2f95a1b":"code","f85b3574":"code","4e1865ca":"code","68ea8d7e":"code","75905489":"code","1267cdbd":"markdown","01f4c793":"markdown","cb744972":"markdown","f6045104":"markdown","aff9676d":"markdown","ebe3a784":"markdown","4b4676af":"markdown","020cfc48":"markdown","4831c82b":"markdown","988b47e9":"markdown","6441efd5":"markdown","af4ad822":"markdown","03242e63":"markdown","7fc64dc5":"markdown","71ba4a81":"markdown","4107eee4":"markdown","5c909dcb":"markdown","7e4d6cb2":"markdown"},"source":{"9c8ec5f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","62a6926e":"data = pd.read_csv(\"\/kaggle\/input\/world-happiness\/2019.csv\")\ndata.head()","8087c32e":"data.columns","ad7c88c2":"data.tail()","6efa36b4":"data.info()","db9b13a6":"data.drop([\"Overall rank\",\"Country or region\"],axis=1,inplace=True)","3644fe31":"#done\ndata.head()","aef5c2c4":"x_data = data.iloc[:,1:]\ny_data = data.loc[:,[\"Score\"]]","efa97c82":"x = (x_data - np.min(x_data))\/(np.max(x_data) - np.min(x_data)).values\ny = (y_data - np.min(y_data))\/(np.max(y_data) - np.min(y_data)).values","45e06048":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.25, random_state=1)","4ccd0119":"# LinearRegression\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\nreg.fit(x_train,y_train)","b5082dd2":"#Predict Score\nprint(\"print accuracy of dt algo:\",reg.score(x_test,y_test))","4825c712":"# CV\nfrom sklearn.model_selection import cross_val_score\nreg = LinearRegression()\nk = 3\ncv_result = cross_val_score(reg,x_train,y_train,cv=k) # uses R^2 as score \nprint('CV Scores: ',cv_result)\nprint('CV scores average: ',np.sum(cv_result)\/k)","2e2ea7bf":"from sklearn.neighbors import KNeighborsRegressor\nknn = KNeighborsRegressor(n_neighbors=7) #n_neighbors=k\nknn.fit(x,y)\nprediction = knn.predict(x_test) #prediction test ile alakalidir.\n\nprint(\"{} nn score: {}\".format(3,knn.score(x,y)))","4eda8b39":"# grid search cross validation with 1 hyperparameter\nfrom sklearn.model_selection import GridSearchCV\ngrid = {'n_neighbors': np.arange(1,50)}\nknn = KNeighborsRegressor()\nknn_cv = GridSearchCV(knn, grid, cv=3) # GridSearchCV\nknn_cv.fit(x_train,y_train)# Fit\n\n# Print hyperparameter\nprint(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \nprint(\"Best score: {}\".format(knn_cv.best_score_))","ec2f2a0c":"from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor()\ndt.fit(x_train,y_train) #x_train e gore fit ettik\n","cbac7807":"#Predict\nprint(\"print accuracy of dt algo:\",dt.score(x_test,y_test))","b2f95a1b":"# grid search cross validation with 1 hyperparameter\nfrom sklearn.model_selection import GridSearchCV\ngrid = {'min_samples_leaf': np.arange(1,25),\"criterion\":[\"mse\",\"friedman_mse\",\"mae\"]}\ndet = DecisionTreeRegressor()\ndet_cv = GridSearchCV(det, grid, cv=3) # GridSearchCV\ndet_cv.fit(x_train,y_train)# Fit\n\n# Print hyperparameter\nprint(\"Tuned hyperparameter k: {}\".format(det_cv.best_params_)) \nprint(\"Best score: {}\".format(det_cv.best_score_))","f85b3574":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(x_train,y_train)\nprint(\"print accuracy of rf algo:\",rf.score(x_test,y_test))","4e1865ca":"from sklearn.linear_model import SGDRegressor \n\nsvm = SGDRegressor()\nsvm.fit(x_train,y_train)\n\n#test\nprint(\"print accuracy of svm algo:\",svm.score(x_test,y_test))","68ea8d7e":"# grid search cross validation with 1 hyperparameter\nfrom sklearn.model_selection import GridSearchCV\ngrid = {'learning_rate':[\"constant\",\"optimal\",\"invscaling\",\"adaptive\"]}\nsvm = SGDRegressor()\nsvm_cv = GridSearchCV(svm, grid, cv=3) # GridSearchCV\nsvm_cv.fit(x_train,y_train)# Fit\n\n# Print hyperparameter\nprint(\"Tuned hyperparameter k: {}\".format(svm_cv.best_params_)) \nprint(\"Best score: {}\".format(svm_cv.best_score_))","75905489":"from sklearn.linear_model import Ridge\nridge = Ridge(alpha = 0.1, normalize = True)\nridge.fit(x_train,y_train)\nprint('Ridge score: ',ridge.score(x_test,y_test))","1267cdbd":"### Ridge Regression","01f4c793":"### Decision Tree Regression","cb744972":"* Droping unnecessary variables","f6045104":"* The important thing we need to do, making Regression intstead of Classifier because y consisting of numerical values is not classification class (for example 1 or 0). Actually, our variables are continious variables","aff9676d":"### Linear Regression","ebe3a784":"### Normalization","4b4676af":"### KNN","020cfc48":"### Train-Test Split","4831c82b":"# Prepearing the Data","988b47e9":"-there are not nan values","6441efd5":"# Loading Data","af4ad822":"* actually decision tree regression prediction score is not enough. Let's change parameters with grid search cross validation to observe more high prediction score than attained one","03242e63":"* Slicing the data (according to x,y)","7fc64dc5":"### SVM","71ba4a81":"* min_sample_leaf: The minimum number of samples required to be at a leaf node.\n* criterion: The function to measure the quality of a split. ","4107eee4":"# Precisions","5c909dcb":"### Random Forest","7e4d6cb2":"* Scaling values between 0-1"}}