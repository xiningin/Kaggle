{"cell_type":{"7fe47480":"code","ac149676":"code","41fc0ebf":"code","605eb70d":"code","06e007b8":"code","7cc1d212":"code","1f6cc21f":"code","0c2f0b20":"code","99363c90":"code","f718b785":"code","15bea4a3":"code","e2ad3974":"code","3edba9e6":"code","19a172fb":"code","54e5bec3":"markdown","82b79b60":"markdown","163fa238":"markdown","47b7c4c9":"markdown","654266f8":"markdown","02ae6b4b":"markdown","0eeb356e":"markdown","28977bd1":"markdown"},"source":{"7fe47480":"#importig libraries\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense , Dropout , LSTM , Embedding\nfrom keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nimport keras.utils as ku\n\n#set seed for reproducability\nfrom tensorflow.compat.v1 import set_random_seed\nfrom numpy.random import seed\n\nset_random_seed(2)\nseed(1)\n\nimport pandas as pd\nimport numpy as np\nimport string , os\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter(action='ignore' , category= FutureWarning)\n\n\nfrom glob import glob","ac149676":"filenames = glob('..\/input\/nyt-comments\/Articles*.csv')\ndataframe = [pd.read_csv(f) for f in filenames]\n\ndataset = []\nfor i in range(len(dataframe)):\n    dataset.append(dataframe[i].headline.values)\n    \ndataset = pd.DataFrame(dataset)\nprint(\"dataset shape before transpose = \" , dataset.shape)\n\ndataset = dataset.transpose()\nprint(\"dataset shape after transpose = \" , dataset.shape)","41fc0ebf":"#concatenating all datasets columns in one column\ndataframe = dataset[0].append(dataset[1]).append(dataset[2]).append(dataset[3]).append(dataset[4]).append(dataset[5]).append(dataset[6]).append(dataset[7]).append(dataset[8]).reset_index(drop = True)\ndataframe.shape","605eb70d":"#remove null values\ndataframe = dataframe.dropna()\ndataframe.shape","06e007b8":"#show the head of the data\ndataframe.head()","7cc1d212":"def clean_text(text):\n    text = ''.join(v for v in text if v not in string.punctuation).lower()\n    text = text.encode('utf8').decode('ascii' , 'ignore')\n    \n    return text\n\ncorpus = [clean_text(x) for x in dataframe]\ncorpus[:10]","1f6cc21f":"tokenizer = Tokenizer()\n\ndef texts_to_sequences(corpus):\n    tokenizer.fit_on_texts(corpus)\n    total_words = len(tokenizer.word_index) + 1\n    print(\"Number of total words = \" , total_words)\n    #convert data to sequence of words\n    input_sequences = []\n    for line in corpus:\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        \n        for i in range(1 , len(token_list)):\n            n_gram_sequences = token_list[:i]\n            input_sequences.append(n_gram_sequences)\n            \n    return input_sequences , total_words\n\ninput_sequences , total_words = texts_to_sequences(corpus)\ninput_sequences[:20]","0c2f0b20":"#padding the sequences and obtain variables : predictors , label\ndef generate_padded_sequences(input_sequences):\n    max_seq_length = max([len(x) for x in input_sequences])\n    input_sequences = np.array(pad_sequences(input_sequences , maxlen= max_seq_length , padding='pre'))\n    \n    predictors = input_sequences[: , :-1]\n    label = input_sequences[: , -1]\n    label = ku.to_categorical(label , num_classes=total_words)\n    \n    return predictors , label , max_seq_length\n\npredictors , label , max_seq_length = generate_padded_sequences(input_sequences)","99363c90":"predictors[:10]","f718b785":"label[:10]","15bea4a3":"#building the model\ndef create_model(max_sequence_len , total_words):\n    input_len = max_seq_length - 1\n    model = Sequential()\n    model.add(Embedding(total_words , 10 , input_length = input_len))\n    \n    model.add(LSTM(256))\n    model.add(Dropout(0.2))\n    \n    model.add(Dense(total_words , activation = 'softmax'))\n    \n    model.compile(loss ='categorical_crossentropy' , optimizer = 'adam')\n    \n    return model\n\nmodel = create_model(max_seq_length , total_words)\nmodel.summary()","e2ad3974":"model.fit(predictors , label , epochs=100)","3edba9e6":"def generate_text(seed_text , num_next_words , model , max_sequence_len):\n    for i in range(num_next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list] , maxlen= max_sequence_len - 1 , padding='pre')\n        \n        predicted_word = model.predict_classes(token_list , verbose = 0)\n        output_word = \"\"\n        \n        for word , index in tokenizer.word_index.items():\n            if index == predicted_word:\n                output_word = word\n                break\n                \n        seed_text += \" \"+output_word\n        \n    return seed_text","19a172fb":"print(generate_text(\"donald trump\", 8, model, max_seq_length))\nprint(generate_text(\"united states\", 9, model, max_seq_length))\nprint (generate_text(\"new york\", 9, model, max_seq_length))\nprint (generate_text(\"science and technology\", 5, model, max_seq_length))","54e5bec3":"Now that we have generated a data-set which contains sequence of tokens, it is possible that different sequences have different lengths. Before starting training the model, we need to pad the sequences and make their lengths equal. ","82b79b60":"Text Generation is a type of Language Modelling problem. Language Modelling is the core problem for a number of of natural language processing tasks such as speech to text, conversational system, and text summarization.\nA trained language model learns the likelihood of occurrence of a word based on the previous sequence of words used in the text. Language models can be operated at character level, n-gram level, sentence level or even paragraph level.\nIn this notebook, I will explain how to create a language model for generating natural language text by implement and training state-of-the-art Recurrent Neural Networ using character level.","163fa238":"![](http:\/\/)Example of Ngram\n![](http:\/\/)\n![](http:\/\/)Headline: i stand with the shedevils\n![](http:\/\/)Ngrams: | Sequence of Tokens\n![](http:\/\/)\n![](http:\/\/)![](http:\/\/)Ngram\t                    Sequence of Tokens\n![](http:\/\/)![](http:\/\/)i stand\t                    [30, 507]\n![](http:\/\/)![](http:\/\/)i stand with\t            [30, 507, 11]\n![](http:\/\/)![](http:\/\/)i stand with the\t        [30, 507, 11, 1]\n![](http:\/\/)![](http:\/\/)i stand with the shedevils\t[30, 507, 11, 1, 975]","47b7c4c9":"**loading the dataset**","654266f8":"In dataset preparation step, we will first perform text cleaning of the data which includes removal of punctuations and lower casing all the words.","02ae6b4b":"Language modelling requires a sequence input data, as given a sequence (of words\/tokens) the aim is the predict next word\/token.\nThe next step is Tokenization. Tokenization is a process of extracting tokens (terms \/ words) from a corpus.","0eeb356e":"**generating sequence of NGrams Tokens**","28977bd1":"Generating the text"}}