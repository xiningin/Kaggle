{"cell_type":{"21e01b4f":"code","23372300":"code","ee7f6eca":"code","17ae2d1c":"code","ffd8158e":"code","93a97b90":"code","71b9fc9b":"code","3b6cb02f":"code","a9fac569":"code","44857f15":"code","4e755a5f":"code","143cac0a":"code","b190bec5":"code","3083373c":"code","cd937a5d":"code","c997df0d":"code","e6926f58":"code","7e1931f3":"code","55f2fd92":"code","5d2b32c4":"code","4af2cfce":"code","d4d8fa96":"code","190ea9a5":"code","f86c7259":"code","2fbc47ca":"markdown","789fc222":"markdown","2a8de85c":"markdown","88480ed0":"markdown","2c79a399":"markdown"},"source":{"21e01b4f":"#This kernel is implementation of the file: https:\/\/github.com\/vaibhavs4424\/POS-Tagger-using-HMM\/blob\/master\/POSTaggingUsingHMM.py\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","23372300":"import nltk\nfrom nltk.corpus import brown","ee7f6eca":"brown.tagged_sents()","17ae2d1c":"# list of all the unique tags from the corpus\n\nbrown_word_tags=[]\n\n#Manually adding the start and the end tag\nfor brown_sent in brown.tagged_sents():\n    brown_word_tags.append(('START','START'))\n    \n    for words,tag in brown_sent:\n        brown_word_tags.extend([(tag[:2],words)])\n        \n    brown_word_tags.append(('END','END'))\n    ","ffd8158e":"#Getting the continuous frequency distribution for the words which are tagged\ncfd_tag_words=nltk.ConditionalFreqDist(brown_word_tags)","93a97b90":"#Getting the conditional probability distribution\ncpd_tag_words=nltk.ConditionalProbDist(cfd_tag_words,nltk.MLEProbDist)","71b9fc9b":"print(\"The probability of an adjective (JJ) being 'smart' is\", cpd_tag_words[\"JJ\"].prob(\"smart\"))\nprint(\"The probability of a verb (VB) being 'try' is\", cpd_tag_words[\"VB\"].prob(\"try\"))","3b6cb02f":"brown_tags=[]\nfor tag, words in brown_word_tags:\n    brown_tags.append(tag)","a9fac569":"#make conditional frequency distribution: count(t{i-1} ti)\ncfd_tags=nltk.ConditionalFreqDist(nltk.bigrams(brown_tags))","44857f15":"# make conditional probability distribution, using maximum likelihood estimate: P(ti | t{i-1})\ncpd_tags=nltk.ConditionalProbDist(cfd_tags,nltk.MLEProbDist)","4e755a5f":"print('The probability of DT occuring after NN is : ', cpd_tags[\"NN\"].prob(\"DT\"))\nprint('The probability of VB occuring after NN is : ', cpd_tags[\"NN\"].prob(\"VB\"))","143cac0a":"prob_tagsequence = cpd_tags[\"START\"].prob(\"PP\") * cpd_tag_words[\"PP\"].prob(\"I\") * \\\n                   cpd_tags[\"PP\"].prob(\"VB\") * cpd_tag_words[\"VB\"].prob(\"love\") * \\\n                   cpd_tags[\"VB\"].prob(\"NN\") * cpd_tag_words[\"PP\"].prob(\"food\") * \\\n                   cpd_tags[\"NN\"].prob(\"END\")","b190bec5":"print(\"The probability of sentence 'I love food' having the tag sequence 'START PP VB PP END' is : \", prob_tagsequence)","3083373c":"# Viterbi:\n# If we have a word sequence, what is the best tag sequence?\n#\n# The method above lets us determine the probability for a single tag sequence.\n# But in order to find the best tag sequence, we need the probability\n# for _all_ tag sequence.\n# What Viterbi gives us is just a good way of computing all those many probabilities\n# as fast as possible.\n\n\ndistinct_brown_tags=set(brown_tags)","cd937a5d":"sample_sentences=[\"I\",\"love\",\"spicy\",\"food\"]\nlen_sample_sentence=len(sample_sentences)","c997df0d":"# for each step i in 1 .. sentlen,\n# store a dictionary\n# that maps each tag X\n# to the probability of the best tag sequence of length i that ends in X\n\n\nviterbi_tags={}\nviterbi_backpointer={}\n\nfor tag in distinct_brown_tags:\n    if tag==\"START\":\n        continue\n    viterbi_tags[tag]=cpd_tags[\"START\"].prob(tag)*cpd_tag_words[tag].prob(sample_sentences[0])\n    viterbi_backpointer[tag]=\"START\"","e6926f58":"# for each step i in 1 .. sentlen,\n# store a dictionary\n# that maps each tag X\n# to the probability of the best tag sequence of length i that ends in X\n\n\n\nviterbi_main=[]\nbackpointer_main=[]\n\nviterbi_main.append(viterbi_tags)\nbackpointer_main.append(viterbi_backpointer)\n\ncurrent_best=max(viterbi_tags.keys(),key=lambda tag: viterbi_tags[tag])\n\n","7e1931f3":"print(\"Word\", \"'\" + sample_sentences[0] + \"'\", \"current best two-tag sequence:\", viterbi_backpointer[current_best], current_best)\n","55f2fd92":"\n\nfor index in range(1,len_sample_sentence):\n    curr_viterbi={}\n    curr_backpointer={}\n    prev_viterbi=viterbi_main[-1]\n    \n    for brown_tag in distinct_brown_tags:\n        \n        if brown_tag != \"START\":\n            # if this tag is X and the current word is w, then\n            # find the previous tag Y such that\n            # the best tag sequence that ends in X\n            # actually ends in Y X\n            # that is, the Y that maximizes\n            # prev_viterbi[ Y ] * P(X | Y) * P( w | X)\n            # The following command has the same notation\n            # that you saw in the sorted() command.\n            prev_best = max(prev_viterbi.keys(),\n                                key=lambda prevtag: \\\n                                    prev_viterbi[prevtag] * cpd_tags[prevtag].prob(brown_tag) * cpd_tag_words[brown_tag].prob(\n                                        sample_sentences[index]))\n\n            curr_viterbi[brown_tag] = prev_viterbi[prev_best] * \\\n                                cpd_tags[prev_best].prob(brown_tag) * cpd_tag_words[brown_tag].prob(sample_sentences[index])\n            curr_backpointer[brown_tag] = prev_best\n\n    current_best = max(curr_viterbi.keys(), key=lambda tag: curr_viterbi[tag])\n    print(\"Word\", \"'\" + sample_sentences[index] + \"'\", \"current best two-tag sequence:\", curr_backpointer[current_best], current_best)\n\n\n    viterbi_main.append(curr_viterbi)\n    backpointer_main.append(curr_backpointer)\n","5d2b32c4":"# now find the probability of each tag\n# to have \"END\" as the next tag,\n# and use that to find the overall best sequence\n\n\n\nprev_viterbi = viterbi_main[-1]\nprev_best = max(prev_viterbi.keys(),\n                    key=lambda prev_tag: prev_viterbi[prev_tag] * cpd_tags[prev_tag].prob(\"END\"))\n\nprob_tag_sequence = prev_viterbi[prev_best] * cpd_tags[prev_best].prob(\"END\")\n\n\nbest_tag_sequence = [\"END\", prev_best]\n# invert the list of backpointers\nbackpointer_main.reverse()\n\n# go backwards through the list of backpointers\n# (or in this case forward, because we have inverter the backpointer list)\n# in each case:\n# the following best tag is the one listed under\n# the backpointer for the current best tag\ncurrent_best_tag = prev_best\nfor backpointer in backpointer_main:\n    best_tag_sequence.append(backpointer[current_best_tag])\n    current_best_tag = backpointer[current_best_tag]\n","4af2cfce":"best_tag_sequence.reverse()","d4d8fa96":"print(\"The sentence given is :\")\nfor word in sample_sentences:\n    print (word,\"\",)","190ea9a5":"print(\"The best tag sequence using HMM for the given sentence is : \")\n\n\nfor best_tag in best_tag_sequence:\n    print (best_tag, \"\",)","f86c7259":"print(\"The probability of the best tag sequence printed above is given by : \", prob_tag_sequence)","2fbc47ca":"## Say words = w1....wN and tags = t1..tN ,then,\n## P(tags | words) is_proportional_to  product P(ti| t{i-1}) * P(wi | ti)\n## To find the best tag sequence for a given sequence of words,we want to find the tag sequence that has the maximum P(tags | words)\n","789fc222":"## This notebook implements parts of speech (POS) tagging using Hidden Markov Model (HMM). Implementation of Viterbi Algorithm is also shown later.","2a8de85c":"\n## The probability of the tag sequence \"PP VB NN\" for the word sequence \"I love food\"?\n## P(START) * P(PP|START) * P(I | PP) * P(VB | PP) * P(love | VB) * P(TO | VB) * P(food | NN) * P(END | VB)","88480ed0":"# Estimating P(ti | t{i-1}) from corpus data using Maximum Likelihood Estimation (MLE):\n# P(ti | t{i-1}) = count(t{i-1}, ti) \/ count(t{i-1})","2c79a399":"# Now, we will estimate P(wi|ti) from corpus data by calculating Maximum Likelihood Estimation MLE: P(wi | ti)=count(wi,ti)\/count(ti)"}}