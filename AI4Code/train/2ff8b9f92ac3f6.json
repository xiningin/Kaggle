{"cell_type":{"bffb268d":"code","9c4ba50c":"code","698839e0":"code","c8460e0a":"code","f8f7e3ad":"code","a629895e":"code","80aa5441":"code","916f874d":"code","aa15ac79":"code","82fac77f":"code","51c246d4":"code","c01f1bb7":"code","8b8c2493":"code","21e983cd":"markdown","c4c2a597":"markdown","33971ce7":"markdown","b8e761e3":"markdown","33f391c1":"markdown","472ea72c":"markdown","646e0bc3":"markdown","e7d52439":"markdown","f093f4e9":"markdown","9be5bca2":"markdown","48703dd8":"markdown","5babf86f":"markdown","9ca764ea":"markdown","d7c06130":"markdown","81f57c74":"markdown","8134f6af":"markdown","21465c3a":"markdown","a259cc15":"markdown","98f39aa6":"markdown","1f95c5d3":"markdown","0c68f37d":"markdown"},"source":{"bffb268d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.metrics import mean_squared_error","9c4ba50c":"import os # Need this library to access files\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","698839e0":"train_data = pd.read_csv('..\/input\/30-days-of-ml\/train.csv') # Read the training file into a pandas dataframe called train_data\n\npd.set_option('display.max_columns', None) # When I do not set this option, the next line of code was only displaying some columns and not all.\n\ntrain_data.head() # This will display the first 5 rows of the training dataset. Run these two lines first and then go to the next line.\n","c8460e0a":"# Import ProfileReport from pandas_profiling library\n#from pandas_profiling import ProfileReport \n\n# Run the profiling on the RAW training data\n#dqr_report = ProfileReport(train_data, title=\"Data quality report\", explorative=True)\n\n# Present the report as a widget\n#dqr_report.to_widgets()\n","f8f7e3ad":"# Import the train_test_split function and uncomment\nfrom sklearn.model_selection import train_test_split\n\ny = train_data.target\n\nX = train_data.drop(['target'], axis=1) # This will remove the column named target from the training data\n\n# Remember X = features and y= target\n# Our models job is to guess 'y' when we provide it X. So the X_train data should have all columns except target.\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2)\n\n# train_test_split is a pandas function which helps us automatically split the data.\n# In this case I have asked pandas to split the data in 80:20 ration (0.8 and 0.2)\n# If you recollect we were using random_state = 1 for our excercises, this was just to help kaggle automatically evaluate our results.\n# Here we do not use random_state variable and hence the dataset will be split completely randomly.\n","a629895e":"object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n\n# Columns that will be one-hot encoded\nhigh_cardinality_cols = [col for col in object_cols if X_train[col].nunique() > 3 ]\n\n\n# Columns that will be dropped from the dataset\nlow_cardinality_cols = list(set(object_cols)-set(high_cardinality_cols))\n\nprint('Categorical columns that will be one-hot encoded:', high_cardinality_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', low_cardinality_cols)\n","80aa5441":"from sklearn.preprocessing import OneHotEncoder\n\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[high_cardinality_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[high_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n\nOH_X_train.head()","916f874d":"#from hyperopt import Trials, STATUS_OK, tpe, hp, fmin, space_eval\n#from math import sqrt\n#from xgboost import XGBRegressor\n\n#space ={\n#        'max_depth': hp.choice('max_depth', np.arange(10, 30, dtype=int)),\n#        'min_child_weight': hp.quniform ('min_child', 1, 20, 1),\n#        'subsample': hp.uniform ('subsample', 0.8, 1),\n#        'n_estimators' : hp.choice('n_estimators', np.arange(1000, 10000, 100, dtype=int)),\n#        'learning_rate' : hp.quniform('learning_rate', 0.025, 0.5, 0.025),\n#        'gamma' : hp.quniform('gamma', 0.5, 1, 0.05),\n#        'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1, 0.05)\n#    }\n\n\n#def score(params):\n#    model = XGBRegressor(**params)\n#    \n#    model.fit(OH_X_train, y_train, eval_set=[(OH_X_train, y_train), (OH_X_valid, y_valid)],\n#              verbose=False, early_stopping_rounds=10)\n#    y_pred = model.predict(OH_X_valid).clip(0, 20)\n#    score = sqrt(mean_squared_error(y_valid, y_pred))\n#    print(score)\n#    return {'loss': score, 'status': STATUS_OK}    \n    \n#def optimize(trials, space):\n    \n#    best = fmin(score, space, algo=tpe.suggest, max_evals=3)\n#    return best\n\n#trials = Trials()\n#best_params = optimize(trials, space)\n\n# Return the best parameters\n\n#space_eval(space, best_params)\n","aa15ac79":"from xgboost import XGBRegressor\n\n#Define the model with algorithm (XGBRegressor) and parameters \n\n\nmodel = XGBRegressor(n_estimators= 2400,\n learning_rate = 0.1,\n colsample_bytree= 0.85,\n gamma= 0.85,\n eval_metric= 'rmse',\n max_depth= 12,\n min_child_weight= 14.0,\n subsample= 0.81,\n tree_method= 'hist')\n\n# Train fit the model on training dataset once again\n\nmodel.fit(OH_X_train, y_train, eval_set=[(OH_X_train, y_train), (OH_X_valid, y_valid)],verbose=False, early_stopping_rounds=200)","82fac77f":"preds_valid = model.predict(OH_X_valid)\n\nscore = mean_squared_error(preds_valid,y_valid,squared=False)\n\nprint('RMSE:', score)","51c246d4":"submission_sample_data = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')\nsubmission_sample_data.tail()","c01f1bb7":"# First we read the test data into a variable\nX_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\n\nOH_cols_test = pd.DataFrame(OH_encoder.fit_transform(X_test[high_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_test.index = X_test.index\n\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_test = X_test.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)\n\nOH_X_test.tail()","8b8c2493":"# Fill in the line below: preprocess test data\nfinal_X_test = pd.DataFrame(OH_X_test)\n\n\n# Fill in the line below: get test predictions\npreds_test = model.predict(final_X_test)\n\n\n# Save test predictions to file with only two columns, id and the prediction values under the target column\noutput = pd.DataFrame({'id': final_X_test.id,\n                       'target': preds_test})\noutput.to_csv('submission.csv', index=False)\n\nprint(output.tail())","21e983cd":"# Data quality profiling\n\n<span style=\"font-family:verdana;\">  Now in our 30 day ML journey we used pd.describe() to quickly check some statistics about the data. **In [30 Days of ML] Day 12 Lesson 2** we saw how dropping missing values or imputing them changed the outcome of our predictions.\n\n<span style=\"font-family:verdana;\"> I decided to take the data quality asesment a bit further by incorporating the Profile Report feature that is available in the pandas_profiling pandas module.\n\n<span style=\"font-family:verdana;\"> To read more about Pandas profiling click [here](https:\/\/pandas-profiling.github.io\/pandas-profiling\/docs\/master\/rtd\/pages\/introduction.html).","c4c2a597":"# Spliting & Preprocessing of data\n\n<span style=\"font-family:verdana;\"> Lets first split the training data into training and validation data. We will use the technique explained on [30 Days of ML] Day 9 Lesson 4 to achieve a 80:20 split of data. That is 80% of the data will be used for training and 20% will be used to validate our model.\n\n<span style=\"font-family:verdana;\"> NOTE: The target column here is \"target\" and hence it should be dropped from the training input X.","33971ce7":"Test the trained model on the validation dataset.","b8e761e3":"<span style=\"font-family:verdana;\"> Now lets use these parameters in training the model and check the RMSE for the validation dataset once again.","33f391c1":"# Trial and error section ends","472ea72c":"<span style=\"font-family:verdana;\"> At the end of 1000 iterations, Hyperopt found the best RMSE to be 0.723677119530914\n\n<span style=\"font-family:courier;\"> 0.7243151914067743                                                                    \n0.7250544892483871                                                                    \n0.7249625470714105                                                                    \n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000\/1000 [3:15:53<00:00, 11.75s\/trial, best loss: 0.723677119530914\n                                                                        \n\n<span style=\"font-family:verdana;\"> It also found the parameters used to train the model when the RMSE was 0.7236\n    \n<span style=\"font-family:courier;\"> {'colsample_bytree': 1.0,\n 'gamma': 0.55,\n 'learning_rate': 0.05,\n 'max_depth': 23,\n 'min_child_weight': 12.0,\n 'n_estimators': 9300,\n 'subsample': 0.9563076541671783}","646e0bc3":"# Trial and error section begins","e7d52439":"# EDA\n\n<span style=\"font-family:verdana;\"> So lets do EDA (*Exploratory Data Analysis*) on the train file. EDA is just a fancy word for lets look into the data. <\/span>","f093f4e9":"Lets check out the data available to us from Kaggle.","9be5bca2":"<span style=\"font-family:verdana;\"> Hi Friends,<\/span>\n\n<span style=\"font-family:verdana;\"> I'm a beginner in AI\/ML just like everyone who is likely participating in this **30 days of ML challenge**. In my submissions, I'll try to be more descriptive about the process of what I'm learning during this challenge rather than focus on submitting a beautiful notebook.<\/span>\n\n<span style=\"font-family:verdana;\"> Typical data science workflow can be outlined as follows <\/span>\n\n![image.png](attachment:2bf7b8a6-28bf-4d84-a83a-9b4054fb66b4.png)\n\n<span style=\"font-family:verdana;\"> Data science or Machine learning is not about getting it right in the first attempt, its about iterating the same process and achieving perfection over time. Hence Kaggle advises us to make one submission everyday and keep improving the submission as time evolves. This means you need to implement each of these steps every day, improving it as you go along.\n\n<span style=\"font-family:verdana;\"> For example if you choose XGBoost to train your data, in the first iteration you will test it without any hyper parameters. In the next iteration you will introduce n_estimators, in the next early_stopping_rounds and so on.\n(See [30 Days of ML] Day 14). \nThese variations should be introduced at every step (EDA, preprocessing etc) in an attempt to improve the prediction accuracy. <\/span>\n\n<span style=\"font-family:verdana;\"> So join me in this journey and help me stay motivated and perhaps also motivate you to learn some new aspects about data science. \n<\/span>","48703dd8":"<span style=\"font-family:verdana;\"> In the next lines of code we will explicitly deal with categorical data. Algorithms cannot be input categorical data as is, they need to be converted to numerical data. XGBoost can be made to directly train and predict on categorical data but I decided to perform the one hot encoding prior to training the model.\n\nSee **[30 Days of ML] Day 12 Lesson 3**","5babf86f":"<span style=\"font-family:verdana;\"> **Hyperopt** \n\n<span style=\"font-family:verdana;\"> Hyperopt is a python library that can be used to test the best combination of hyper parameters for your model.\nUncomment the code below and edit as necessary to find out the best parameters for your model.\nI trained the model on 1000 combinations of various parameters and it took approximately 3 hours. I think even 50-100 iterations would have found an appropriate combination. If you use GPU this would be faster but I decided to stick to normal CPU.","9ca764ea":"<span style=\"font-family:verdana;\"> Now is the time to run the XGBoost algorithm on the test data and made predictions.\n\n<span style=\"font-family:verdana;\">Once the code execution is successful, click the Save Version button. Once the full execution is over. Open the executed version of the notebook, scroll down to the output section, and click on the submit button to submit your prediction results. Download a copy of the submission csv and have a look for yourself as well as to what format and contents it has.","d7c06130":"<span style=\"font-family:verdana;\"> The test data needs the same treatment as the training data before it can be used for prediction using XGBoost. So we need to encode the categorical columns to numerical values.","81f57c74":"<span style=\"font-family:verdana;\"> So what do we find in the EDA?\nThis data set does not tell us anything about the business problem, its a dataset designed to test our skills to train models and predict the target. Yet one quick look at the column names and the data below it provides us some information for us to select an appropriate model to train the data on.\nThe datatypes in the train.csv file can be described as follows\n\n<span style=\"font-family:verdana;\"> **id** - This is a unique id given to each row. The id is useful in establishing correlation between predictions and actual outcomes.\n\n<span style=\"font-family:verdana;\"> **cat0 - cat9** These columns contain categorical values. This means that these have fixed set of values, in this case some columns have A & B and some have D & E and some have other letters. What is important to note is that categorical values need to be converted to numbers before using them for training the model. This is called preprocessing.\nThis was covered in ***[30 Days of ML] Day 13 - Lesson 4***.\n\n<span style=\"font-family:verdana;\"> **cont0 - cont13** These columns have numerical value and can be directly used for training the model. Of course we should check if there are missing values and in which case we should use some imputation technique to fill those values or drop the column entirely. Imputation was covered in ***[30 Days of ML] Day 13 - Lesson 4*** as well.\n\n<span style=\"font-family:verdana;\"> **target** And then we come to price posession, the column that has the outcome or the target. Based on the categorical columns and the continous columns, the algorithm will try to learn if there is some correlation with the target.\nIn the Intro to machine learning course that we have been through during this challenge the target column was the sale price of the houses. \n\n<span style=\"font-family:verdana;\"> Remember that the input columns are collectively represented by <span style=\"color:blue;\"> <b> X <\/b> <\/span> (in caps) and the target is represented by <span style=\"color:blue;\"> <b> y <\/b> <\/span> (smallcase y). This is just a common notation and not a hard rule.\n\n<span style=\"font-family:verdana;\"> When we say we are training the model, it effectively means that we are asking the algorithm to learn the correlation between\n    <span style=\"color:crimson;\"> <b> X -> y <\/b> <\/span> <\/span>\n","8134f6af":"# Dataset\n\n<span style=\"font-family:verdana;\"> So we find 3 files in the dataset. \n\n\n<span style=\"font-family:verdana;\"> **sample_submission.csv**: This helps us understand how our prediction submission should look like for Kaggle to generate out score. Dont worry about it now.\n\n<span style=\"font-family:verdana;\"> **train.csv** : This is the data with which we will train our model. The EDA will be done on this file. EDA just means we will explore this data and see what is inside it and based on the EDA, we can decide which model is more apt for this data. The training data needs to be split into training data and validation data, generally 25% is kept aside for validation. Read more about this in **[30 Days of ML] Day 9 Lesson 4**.\n    \n<span style=\"font-family:verdana;\"> **test.csv** : The test file has all the columns similar to the train file except the target column. \n    \n <\/span>","21465c3a":"<span style=\"font-family:verdana;\"> Now for our final submission, we use the test dataset to fit the model.\nBefore we do that lets have a quick look at the submission.csv to make sure we are going to submit the predictions to kaggle in the right format.","a259cc15":"<span style=\"font-family:verdana;\"> This section is for me to test some of my hypothesis on what works and what does not. I will then use the model to predict on the validation dataset that we seperated to see the effect of the changes.","98f39aa6":"# Thoughts on preprocessing and model selection\n\n<span style=\"font-family:verdana;\"> Analysis of the data shows that there are no missing values in any of the columns.\nHence our focus for preprocessing should be to convert categorical data to some form of numerical data. See \n**[30 Days of ML] Day 12**\n\n\n<span style=\"font-family:verdana;\"> We also know that the data is tabular in nature with both categorical and continous variables.\nIn [30 Days of ML] Day 14 Lesson 6 we have learnt that XGBoost is a good software library for training models with the gradient boosting technique.\nA good explanation of XGBoost can be found on this [website](https:\/\/towardsdatascience.com\/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d).","1f95c5d3":"# Prepare the playground\n\n<span style=\"font-family:verdana;\"> Lets begin by first preparing our notebook environment. This is also an evolving process. Start with the absolute basics and keep importing libraries as and when required.<\/span>","0c68f37d":"<span style=\"font-family:verdana;\"> During the EDA I found several categorical columns had very low cardinality. This means that some columns only have say two possibilities A or B. Such columns will have very little impact on the prediction quality and hence are best dropped from the dataset prior to encoding. So only cat3-cat9 which have more than 3 different values are retained for training."}}