{"cell_type":{"2b4b1c8b":"code","bae29eee":"code","ddf1b09e":"code","0483fa50":"code","e3a31631":"code","ea021c1c":"code","f915f502":"code","95dfcb10":"code","977a1275":"code","e46823ae":"code","8d95ae6a":"code","802622f6":"code","c3d7e281":"code","664b5b28":"code","7aa64510":"code","59a747d5":"code","988c2487":"code","b5b324d5":"code","e3295c00":"code","269f3e5a":"code","0c911586":"code","02c6b5c8":"code","f27b6b10":"code","2b04298f":"code","27d1c154":"code","ea68193f":"code","04eb7f29":"code","2e6372ea":"code","64e38677":"code","38ecce77":"code","3f484f80":"code","bdf9c573":"code","0802d514":"code","c732e8df":"code","2890dad7":"code","2a38b7ba":"code","689aa38f":"code","4f77e928":"code","2a4ce1d8":"code","61bad682":"code","7aec4078":"code","c7d2853f":"code","521a7ef4":"code","7e7553ee":"code","f2d0ffc3":"code","d502e296":"code","85dce70c":"code","afb28c6f":"code","145f6fec":"code","31a6a83a":"code","efa11534":"code","030fc906":"code","3813181a":"markdown","64641ab5":"markdown","e65ef0e4":"markdown","c31f6e6c":"markdown","8f9bf768":"markdown","f0ba9560":"markdown","ab47be5b":"markdown","65b68f60":"markdown","653d0a39":"markdown","6a7e0817":"markdown","5509ccbc":"markdown","5959e48f":"markdown","887ca35c":"markdown","68f13b8c":"markdown","b06c0377":"markdown","9ee1da16":"markdown","67448096":"markdown","3125f1b4":"markdown","12b44b38":"markdown","61c9a0d3":"markdown","0fb19b21":"markdown","1a23e2b6":"markdown","2f618dba":"markdown","3b450ea8":"markdown"},"source":{"2b4b1c8b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nimport seaborn as sns\n%matplotlib inline","bae29eee":"#from numpy.random import seed\n#seed(1)\n#import tensorflow\n#tensorflow.random.set_seed(2)","ddf1b09e":"df_test= pd.read_csv(\"..\/input\/usp-pj01\/test_Iris.csv\")\ndf_train = pd.read_csv(\"..\/input\/usp-pj01\/train_Iris.csv\")","0483fa50":"df_train.head()","e3a31631":"id = df_train['Id']\ndf_train = df_train.iloc[:,1:] #retirando a coluna de id","ea021c1c":"df_train.corr()","f915f502":"#df = sns.load_dataset('iris')\nsns_plot = sns.pairplot(df_train, hue='Species', height=2.5)","95dfcb10":"df_train['Species'].value_counts()","977a1275":"y = df_train['Species'].value_counts()\/df_train['Species'].value_counts().sum() #frequencia absoluta\nplt.bar(['Iris-virginica','Iris-setosa','Iris-versicolor'],y)\nplt.title(\"Frequencia absoluta das especies\")","e46823ae":"# armazena o nome das classes\n#Virginica = 1\n#Setosa = 2\n#Versicolor = 3\nclasses = np.unique(df_train[df_train.columns[-1]])\n\ndf_train['Species'].replace(['Iris-virginica','Iris-setosa','Iris-versicolor'], ['virginica', 'setosa', 'versicolor'],inplace=True)","8d95ae6a":"#boxplot para verificar se ha outliers\nfig, axes = plt.subplots(nrows=1, ncols=4)\nfor i in range(0, len(df_train.columns[:-1])):\n  sns.boxplot(x = \"Species\", y = df_train.columns[:-1][i], data = df_train, ax=axes[i])\n  axes[i].set_title(df_train.columns[:-1][i].replace(\"_\",\" \").title(), fontsize=14)\n  axes[i].tick_params(axis='y', labelsize=14)\n  axes[i].set_ylabel(\"\")\n  axes[i].set_xlabel(\"Esp\u00e9cie\")\nfig.suptitle(\"\")\nfig.set_figwidth(15)\nplt.show();","802622f6":"# Retorna a qtd de linhas duplicadas\ndf_train.duplicated().sum()","c3d7e281":"# Retorna a qtd de linhas com NaN\ndf_train.isna().sum()","664b5b28":"# Remove as linhas duplicadas\ndf_train = df_train.drop_duplicates()","7aa64510":"df_train.head()","59a747d5":"#Encontrando os quartis e o IQR\nQ1 = df_train.quantile(0.25)\nQ3 = df_train.quantile(0.75)\nIQR = Q3 - Q1","988c2487":"linhas_outliers = ((df_train < (Q1 - 1.5 * IQR)) | (df_train > (Q3 + 1.5 * IQR))).any(axis = 1)\ndf_train.index[list(linhas_outliers)]","b5b324d5":"new_data = df_train.drop(df_train.index[list(linhas_outliers)])\nnew_data.shape","e3295c00":"df_train = new_data","269f3e5a":"y = df_train.iloc[:,-1]\nX = df_train.iloc[:,:-1]","0c911586":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n#scaler = StandardScaler().fit(X)\nscaler = MinMaxScaler().fit(X)\nX = scaler.transform(X)","02c6b5c8":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 202104)","f27b6b10":"print(X_train.shape)\nprint(X_test.shape)","2b04298f":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB # 1. choose model class\nmodel_NB = GaussianNB()                       # 2. instantiate model\nmodel_NB.fit(X_train, y_train)                # 3. fit model to data\ny_predNB = model_NB.predict(X_test)            # 4. predict on new data\n\n# calcula a acuracia\n\nprint('Acuracia Naivy bayes: {:.3f}'.format(accuracy_score(y_predNB, y_test)))\nprint(\"F1 score Naivy bayes: {:.3f}\".format(f1_score(y_test, y_predNB, average = \"weighted\")))\nprint(\"Precision Naivy bayes: {:.3f}\".format(precision_score(y_test, y_predNB, average = \"weighted\")))","27d1c154":"from sklearn.model_selection import cross_val_score\n\ncv_scores = cross_val_score(model_NB, X_train, y_train , cv=10)\n\nprint(cv_scores)\nprint(\"Media Cross-val accuracy: %f\" % cv_scores.mean())\nprint(\"Vari\u00e2ncia: %f\" % cv_scores.var())","ea68193f":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\n#KNN\n#Hiper par\u00e2metros para otimizacao\nn_neighbors = np.arange(1,30)\nweights = [\"uniform\", \"distance\"]\nmetric = ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\nk_fold = 10\n\n#GridSearch para achar a melhor combina\u00e7\u00e3o de valores dos hiper par\u00e2metros.\n#   aplicando ainda uma valida\u00e7\u00e3o cruzada com 10 folds.\nmodel_knn = GridSearchCV(KNeighborsClassifier(), cv = k_fold,\n                     param_grid={\"n_neighbors\": n_neighbors, 'weights': weights, \"metric\":metric})\nmodel_knn.fit(X_train, y_train)\ny_pred = model_knn.predict(X_test)\n\n#Mensurar a qualidade do modelo ajustado\nprint(\"Acur\u00e1cia KNN: {:.3f}\".format(accuracy_score(y_test, y_pred)))\nprint(\"F1 score KNN: {:.3f}\".format(f1_score(y_test, y_pred, average = \"weighted\")))\nprint(\"Precision KNN: {:.3f}\".format(precision_score(y_test, y_pred, average = \"weighted\")))","04eb7f29":"from sklearn.model_selection import cross_val_score\n\n\ncv_scores = cross_val_score(model_knn.best_estimator_, X_train, y_train , cv=10)\n\nprint(cv_scores)\nprint(\"Media Cross-val accuracy: %f\" % cv_scores.mean())\nprint(\"Vari\u00e2ncia: %f\" % cv_scores.var())","2e6372ea":"from sklearn import tree\n#Hiper par\u00e2metros para otimizacao\ncriterion = [\"gini\", \"entropy\"]\nk_fold = 10\n#GridSearch para achar a melhor combina\u00e7\u00e3o de valores dos hiper par\u00e2metros.\n#   aplicando ainda uma valida\u00e7\u00e3o cruzada com 10 folds.\nmodel_dt = GridSearchCV(tree.DecisionTreeClassifier(), cv = k_fold,\n                     param_grid={\"criterion\": criterion})\nmodel_dt.fit(X_train, y_train)\ny_pred = model_dt.predict(X_test)\n\n\n#Mensurar a qualidade do modelo ajustado\nprint(\"Acur\u00e1cia Arvore_decisao: {:.3f}\".format(accuracy_score(y_test, y_pred)))\nprint(\"F1 score Arvore_decisao: {:.3f}\".format(f1_score(y_test, y_pred, average = \"weighted\")))\nprint(\"Precision Arvore_decisao: {:.3f}\".format(precision_score(y_test, y_pred, average = \"weighted\")))","64e38677":"from sklearn.model_selection import cross_val_score\n\n\ncv_scores = cross_val_score(model_dt.best_estimator_, X_train, y_train , cv=10)\n\nprint(cv_scores)\nprint(\"Media Cross-val accuracy: %f\" % cv_scores.mean())\nprint(\"Vari\u00e2ncia: %f\" % cv_scores.var())","38ecce77":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\n#Hiper par\u00e2metros para otimizacao\nC = np.arange(1,30)\ngamma = [\"scale\", \"auto\"]\ndecision_function_shape = [\"ovo\", \"ovr\"]\nk_fold = 10\n#GridSearch para achar a melhor combina\u00e7\u00e3o de valores dos hiper par\u00e2metros.\n#   aplicando ainda uma valida\u00e7\u00e3o cruzada com 10 folds.\nmodel_svm = GridSearchCV(SVC(), cv = k_fold,\n                     param_grid={\"C\": C, \"gamma\": gamma, \"decision_function_shape\": decision_function_shape})\nmodel_svm.fit(X_train, y_train)\ny_pred = model_svm.predict(X_test)\n\n\n#Mensurar a qualidade do modelo ajustado\nprint(\"Acur\u00e1cia SVM: {:.3f}\".format(accuracy_score(y_test, y_pred)))\nprint(\"F1 score SVM: {:.3f}\".format(f1_score(y_test, y_pred, average = \"weighted\")))\nprint(\"Precision SVM: {:.3f}\".format(precision_score(y_test, y_pred, average = \"weighted\")))","3f484f80":"from sklearn.model_selection import cross_val_score\n\n\ncv_scores = cross_val_score(model_svm.best_estimator_, X_train, y_train , cv=10)\n\nprint(cv_scores)\nprint(\"Media Cross-val accuracy: %f\" % cv_scores.mean())\nprint(\"Vari\u00e2ncia: %f\" % cv_scores.var())","bdf9c573":"#SVM \nfrom sklearn.model_selection import cross_validate\ncross_validate(model_svm.best_estimator_, X, y, return_train_score=True)","0802d514":"#KNN \nfrom sklearn.model_selection import cross_validate\ncross_validate(model_knn.best_estimator_, X, y, return_train_score=True)","c732e8df":"df_test.head()","2890dad7":"test = df_test.copy()","2a38b7ba":"#Separa o Id das plantas do teste para futuro envio no Kaggle.\ntest_id = df_test.Id\ntest.drop(\"Id\", axis = 1, inplace = True)","689aa38f":"\ndf1 = df_train.copy()\n\n#Mudar o nome das categorias para ficar do jeito que \u00e9 pedido na competi\u00e7\u00e3o\ndf1.replace({\"versicolor\": \"Iris-versicolor\", \"setosa\": \"Iris-setosa\", \"virginica\":\"Iris-virginica\"}, inplace = True)\ny_train = df1.iloc[:, -1]\nX_train = df1.iloc[:, :-1]\n\nX_test = test.copy()\n","4f77e928":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n#scaler_train = StandardScaler()\nscaler_train = MinMaxScaler()\nX_train = scaler_train.fit_transform(X_train)\n\n#scaler_test =StandardScaler()\nscaler_test = MinMaxScaler()\nX_test = scaler_test.fit_transform(X_test)","2a4ce1d8":"#melhor modelo para aplicar no conjunto de teste\n#model = model_knn.best_estimator_\n\nmodel = model_svm.best_estimator_\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)","61bad682":"#Gera um csv com as predi\u00e7\u00f5es feitas pelo modelo para o conjunto de teste\ntest_id = pd.DataFrame(data = {\"Id\": test_id, \"Category\": y_pred})\ntest_id.to_csv(\"resposta_svm.csv\", index = False)","7aec4078":"test_id.head()","c7d2853f":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nscaler = StandardScaler().fit(X)\nX = scaler.transform(X)\n\nscaler_test = MinMaxScaler()\nX_test = scaler_test.fit_transform(X_test)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 202104)\n\n\n#Nosso modelo para redes neurais precisa ter uma coluna para cada classe \n#Indicando seu pertencimento, para isso usamos o OneHotEncoder\ny_train2 = y_train.values.reshape(-1, 1)\n\n\n#Onehotencoder no treino\nencoder = OneHotEncoder(sparse=False)\ny_train2 = encoder.fit_transform(y_train2)\n\nprint(encoder.categories_)\n\ny_test2 = y_test.values.reshape(-1, 1)\n\n#Onehotencoder no teste\nencoder = OneHotEncoder(sparse=False)\ny_test2 = encoder.fit_transform(y_test2)\nprint(encoder.categories_)","521a7ef4":"from keras.layers import Input, Flatten, Dense\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\n\nmodel = Sequential()\n\nmodel.add(Dense(10, input_shape=(4,), activation='relu', name='h1'))\nmodel.add(Dense(10, activation='relu', name='h2'))\nmodel.add(Dense(3, activation='softmax', name='output'))\n\n# Adam optimizer with learning rate of 0.001\noptimizer = Adam(lr=0.001)\nmodel.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\nprint('Neural Network Model Summary: ')\nprint(model.summary())\n\n#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n\nhistory = model.fit(X_train, y_train2, validation_data=(X_test, y_test2), epochs=500, verbose=0)","7e7553ee":"from matplotlib import pyplot\n# evaluate the model\n_, train_acc = model.evaluate(X_train, y_train2, verbose=0)\n_, test_acc = model.evaluate(X_test, y_test2, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n# plot training history\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.xlabel(\"Epochs\")\npyplot.ylabel(\"Custo\")\npyplot.show()","f2d0ffc3":"from keras.layers import Input, Flatten, Dense\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nfrom matplotlib import pyplot\n\nmodel = Sequential()\n\nmodel.add(Dense(10, input_shape=(4,), activation='relu', name='h1'))\nmodel.add(Dense(10, activation='relu', name='h2'))\nmodel.add(Dense(3, activation='softmax', name='output'))\n\n# Adam optimizer with learning rate of 0.001\noptimizer = Adam(lr=0.001)\nmodel.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n#print('Neural Network Model Summary: ')\n#print(model.summary())\n\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n\nhistory = model.fit(X_train, y_train2, validation_data=(X_test, y_test2), epochs=150, verbose=0) #ajustamos epoch\n\n\n# evaluate the model\n_, train_acc = model.evaluate(X_train, y_train2, verbose=0)\n_, test_acc = model.evaluate(X_test, y_test2, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n# plot training history\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.xlabel(\"Epochs\")\npyplot.show()\n","d502e296":"results = model.evaluate(X_train, y_train2)\ntest_acc = model.evaluate(X_test, y_test2, verbose=0)\n\nprint('Final train set loss: {:4f}'.format(results[0]))\nprint('Final train set accuracy: {:4f}'.format(results[1]))\n\nprint('Final test set loss: {:4f}'.format(results[0]))\nprint('Final test set accuracy: {:4f}'.format(results[1]))\n\n","85dce70c":"test = df_test.copy()\n\n#Separa o Id das plantas do teste para futuro envio no Kaggle.\ntest_id = df_test.Id\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#normalizado o conjunto de teste\nscaler = StandardScaler().fit(test)\n#scaler = MinMaxScaler().fit(test)\ntest = scaler.transform(test)","afb28c6f":"y_pred = model.predict(test)","145f6fec":"y_pred = np.argmax(y_pred,axis=1)","31a6a83a":"resp = []\n\nfor i in range(len(y_pred)):\n    if y_pred[i] == 0:\n        resp.append('Iris-setosa')\n    else:\n        if y_pred[i] == 1:\n            resp.append('Iris-versicolor')\n        else:\n            resp.append('Iris-virginica')","efa11534":"#Gera um csv com as predi\u00e7\u00f5es feitas pelo modelo para o conjunto de teste\ntest_id = pd.DataFrame(data = {\"Id\": test_id, \"Category\": resp})\ntest_id.to_csv(\"resposta_redesneurais.csv\", index = False)","030fc906":"test_id.head()","3813181a":"# Rodando redes neurais com ajuste","64641ab5":"De acordo com os resultados acima, o modelo que melhor performou foi o SVM","e65ef0e4":"## Validacao KNN","c31f6e6c":"## Validacao Arvore de decisao","8f9bf768":"Eliminando outliers","f0ba9560":"# KNN","ab47be5b":"# Redes Neurais com valida\u00e7\u00e3o","65b68f60":"https:\/\/machinelearningmastery.com\/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping\/#:~:text=Early%20stopping%20is%20a%20method,deep%20learning%20neural%20network%20models.","653d0a39":"## Pr\u00e9 - processamento","6a7e0817":"# SVM","5509ccbc":"# Compara\u00e7\u00e3o Resultados com epoch ajustado","5959e48f":"Faz a predicao","887ca35c":"# An\u00e1lise Explorat\u00f3ria de Dados","68f13b8c":"Frequencia das especies","b06c0377":"Vamos pegar a classe com maior probabilidade","9ee1da16":"###### Verificando se houve overfitting\n","67448096":"## Validacao SVM","3125f1b4":"## Analisando qual o melhor momento para parar as itera\u00e7\u00f5es e assim evitar um overffiting","12b44b38":"# Abordagem por redes neurais","61c9a0d3":"### Modelagem\n","0fb19b21":"# Naive Bayes","1a23e2b6":"### Predi\u00e7\u00e3o da base teste","2f618dba":"## Validacao Naive Bayes","3b450ea8":"# \u00c1rvore de decis\u00e3o"}}