{"cell_type":{"bed78377":"code","49c6c479":"code","23ab4de5":"code","f01ed514":"code","4116026d":"code","43869c07":"code","fe3a6e3c":"code","48ee4be1":"code","8e7c8387":"code","edaec720":"code","c0646426":"code","948ec361":"code","07dbca5d":"code","04408b5a":"code","435d93ac":"code","8f4ead32":"code","5126e430":"code","5b4ac7dd":"code","b652dd39":"code","4f25a171":"code","bb7013dd":"code","ddd3c35e":"code","51ec7944":"code","c2a2a595":"code","463d6c60":"code","8a223922":"code","73245e96":"code","269a0731":"code","d07dcf1c":"code","792260cd":"code","27175536":"code","09cba08a":"code","c5cea12c":"code","1ce0472d":"code","a1b68f9b":"code","6572c4fa":"code","3b4a1df5":"code","29b90b6d":"code","546be3de":"code","7c5c6890":"code","15cee3ee":"code","e6862964":"markdown","c7e67e97":"markdown","ec486283":"markdown","528d1bf7":"markdown","e7f39cf1":"markdown","cbef5c4a":"markdown","751a8697":"markdown","c4be6439":"markdown","4c881fbc":"markdown","6d336aa6":"markdown","0b359c70":"markdown","1864d769":"markdown","cde6e3d8":"markdown","327c8fe3":"markdown","2d36f1d9":"markdown","01c097ec":"markdown","c966dc74":"markdown","3126f6f4":"markdown","085464ab":"markdown","696cc6ad":"markdown","45c401ab":"markdown","eb0d26f7":"markdown","e2941045":"markdown","d0c365cb":"markdown","7e9e9172":"markdown"},"source":{"bed78377":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","49c6c479":"#loading dataset\nwine=pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","23ab4de5":"#no null values. Let's check for duplicates\n\nsum(wine.duplicated())","f01ed514":"plt.figure(figsize=(7,5))\nax=sns.countplot(data=wine,x='quality',palette='flare')\nfor p in ax.patches:\n    ax.annotate(f'\\n{p.get_height()}\\n{(p.get_height()\/len(wine)*100).round(0)}%', (p.get_x()+0.3, p.get_height()+10), color='black', size=7)\n\n    \nax.spines['top'].set_color('white') #setting right and top edge to invisible\nax.spines['right'].set_color('white')\n\n\nplt.show()\n","4116026d":"wine[wine.duplicated()]['quality'].value_counts()","43869c07":"wine.drop_duplicates(inplace=True)","fe3a6e3c":"#let's check on our dataframe again\nwine.info()","48ee4be1":"#all looks good and now let's look into features\nwine.head()","8e7c8387":"plt.figure(figsize=(10,6))\nplt.title('Feature correlation')\nshape=np.triu(wine.corr())\nsns.heatmap(wine.corr(),annot=wine.corr().round(2),mask=shape,cmap='RdBu_r')\nplt.tight_layout()\nplt.show()","edaec720":"fig,((ax1,ax2,ax3,ax4,ax5),(ax6,ax7,ax8,ax9,ax10),(ax11,ax12,ax13,ax14,ax15))=plt.subplots(3,5,sharex=False,sharey=True,figsize=(10,8))\nplt.suptitle('Feature distributions',size=14)\naxs=[ax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8,ax9,ax10,ax11,ax12,ax13,ax14,ax15]\ncolumns=wine.columns\n\nfor n in range(0,11):\n    axs[n].hist(wine[columns[n]],color='darkred',bins=15,edgecolor=\"white\")\n    axs[n].spines['bottom'].set_color('lightgrey')\n    axs[n].spines['top'].set_color('white')\n    axs[n].spines['left'].set_color('lightgrey')\n    axs[n].spines['right'].set_color('white')\n    \nax1.set_title('Fixed acidity',size=10)\nax2.set_title('Volatile acidity',size=10)\nax3.set_title('Citric acid',size=10)\nax4.set_title(\"Residual sugar\",size=10)\nax5.set_title('Chlorides',size=10)\nax6.set_title('Free sulfur dioxide',size=10)\nax7.set_title('Total sulfur dioxid',size=10)\nax8.set_title('Density',size=10)\nax9.set_title('pH',size=10)\nax10.set_title('Sulphates',size=10)\nax11.set_title('Alcohol',size=10)\n\n\n\nfor n in range(11,15):\n    axs[n].spines['bottom'].set_color('white')\n    axs[n].spines['top'].set_color('white')\n    axs[n].spines['left'].set_color('white')\n    axs[n].spines['right'].set_color('white')\n    axs[n].xaxis.set_visible(False)\n    axs[n].yaxis.set_visible(False)\n\nplt.tight_layout()\nplt.show()","c0646426":"#use box plots to check for possibble outliers\n\nfig,((ax1,ax2,ax3,ax4,ax5),(ax6,ax7,ax8,ax9,ax10),(ax11,ax12,ax13,ax14,ax15))=plt.subplots(3,5,sharex=False,sharey=False,figsize=(10,8))\nplt.suptitle('Feature boxplots',size=14)\naxs=[ax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8,ax9,ax10,ax11,ax12,ax13,ax14,ax15]\ncolumns=wine.columns\n\nfor n in range(0,11):\n    axs[n].boxplot(wine[columns[n]],manage_ticks=True)\n    axs[n].spines['bottom'].set_color('lightgrey')\n    axs[n].spines['top'].set_color('white')\n    axs[n].spines['left'].set_color('lightgrey')\n    axs[n].spines['right'].set_color('white')\n    \nax1.set_title('Fixed acidity',size=10)\nax2.set_title('Volatile acidity',size=10)\nax3.set_title('Citric acid',size=10)\nax4.set_title(\"Residual sugar\",size=10)\nax5.set_title('Chlorides',size=10)\nax6.set_title('Free sulfur dioxide',size=10)\nax7.set_title('Total sulfur dioxid',size=10)\nax8.set_title('Density',size=10)\nax9.set_title('pH',size=10)\nax10.set_title('Sulphates',size=10)\nax11.set_title('Alcohol',size=10)\n\n\n\nfor n in range(11,15):\n    axs[n].spines['bottom'].set_color('white')\n    axs[n].spines['top'].set_color('white')\n    axs[n].spines['left'].set_color('white')\n    axs[n].spines['right'].set_color('white')\n    axs[n].xaxis.set_visible(False)\n    axs[n].yaxis.set_visible(False)\n\nplt.tight_layout()\nplt.show()","948ec361":"#checking for potential outliers using IQR\n\ndef outlier(column):\n    s=0\n    for x in wine[column]:\n        iqr=wine[column].quantile(0.75)-wine[column].quantile(0.25)\n        if (x > wine[column].quantile(0.75)+ 3*iqr ) | (x < wine[column].quantile(0.25)- 3*iqr ):\n            s+=1\n    print(f'Column {column} has {s} outliers')","07dbca5d":"for c in wine.columns:\n    outlier(c)\n    ","04408b5a":"X = wine.iloc[:, 0:11].values\ny = wine.iloc[:, -1].values\nprint(f'Number of data before removing outliers is {X.shape}')\n\n#cleaning outliers\nfrom sklearn.neighbors import LocalOutlierFactor\nlof = LocalOutlierFactor()\nyhat = lof.fit_predict(X)\n# select all rows that are not outliers\nmask = yhat != -1\nX,y = X[mask, :], y[mask]\n# summarize the shape of the updated dataset\nprint(f'Number of data after removing outliers is {X.shape}');","435d93ac":"fig,((ax1,ax2,ax3,ax4,ax5),(ax6,ax7,ax8,ax9,ax10),(ax11,ax12,ax13,ax14,ax15))=plt.subplots(3,5,sharex=False,sharey=False,figsize=(10,8))\nplt.suptitle('Feature boxplots',size=14)\naxs=[ax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8,ax9,ax10,ax11,ax12,ax13,ax14,ax15]\n\n\nfor n in range(0,11):\n    axs[n].boxplot(X[:,n],manage_ticks=True)\n    axs[n].spines['bottom'].set_color('lightgrey')\n    axs[n].spines['top'].set_color('white')\n    axs[n].spines['left'].set_color('lightgrey')\n    axs[n].spines['right'].set_color('white')\n    \nax1.set_title('Fixed acidity',size=10)\nax2.set_title('Volatile acidity',size=10)\nax3.set_title('Citric acid',size=10)\nax4.set_title(\"Residual sugar\",size=10)\nax5.set_title('Chlorides',size=10)\nax6.set_title('Free sulfur dioxide',size=10)\nax7.set_title('Total sulfur dioxid',size=10)\nax8.set_title('Density',size=10)\nax9.set_title('pH',size=10)\nax10.set_title('Sulphates',size=10)\nax11.set_title('Alcohol',size=10)\n\n\n\nfor n in range(11,15):\n    axs[n].spines['bottom'].set_color('white')\n    axs[n].spines['top'].set_color('white')\n    axs[n].spines['left'].set_color('white')\n    axs[n].spines['right'].set_color('white')\n    axs[n].xaxis.set_visible(False)\n    axs[n].yaxis.set_visible(False)\n\nplt.tight_layout()\nplt.show()","8f4ead32":"wine['alcohol'].max()","5126e430":"#splitting inot train and test set\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","5b4ac7dd":"#scaling the data before feature selection\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\nimport statsmodels.api as sm\n#adding array of ones for OLS\nx0=np.ones(1001).astype(int)\nX_opt =np.insert(X_train, 0, x0, axis=1)","b652dd39":"X_opt=X_opt[:,[0,1,2,3,4,5,6,7,8,9,10,11]]\nregressor_OLS = sm.OLS(endog = y_train, exog = X_opt).fit()\nregressor_OLS.summary();","4f25a171":"#removing x8\nX_opt=X_opt[:, [0,1,2,3,4,5,6,7,9,10,11]]\nregressor_OLS = sm.OLS(endog = y_train, exog =X_opt).fit()\nregressor_OLS.summary();","bb7013dd":"#removing x4\nX_opt=X_opt[:, [0,1,2,3,5,6,7,8,9,10]]\nregressor_OLS = sm.OLS(endog = y_train, exog =X_opt).fit()\nregressor_OLS.summary();","ddd3c35e":"#removing x5\nX_opt=X_opt[:, [0,1,2,3,4,6,7,8,9]]\nregressor_OLS = sm.OLS(endog = y_train, exog =X_opt).fit()\nregressor_OLS.summary();","51ec7944":"#removing x1\nX_opt=X_opt[:, [0,2,3,4,5,6,7,8]]\nregressor_OLS = sm.OLS(endog = y_train, exog =X_opt).fit()\nregressor_OLS.summary();","c2a2a595":"#removing x2\nX_opt=X_opt[:, [0,1,3,4,5,6,7]]\nregressor_OLS = sm.OLS(endog = y_train, exog =X_opt).fit()\nregressor_OLS.summary();","463d6c60":"#defining new train set\nX_train_2=X_train[:,[1,4,6,8,9,10]]\nX_test_2=X_test[:,[1,4,6,8,9,10]]","8a223922":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score\nnb_class=GaussianNB()\nnb_class.fit(X_train_2,y_train)\n\ny_pred=nb_class.predict(X_test_2)\n\nprint(f'\\nAccuracy score is {accuracy_score(y_test,y_pred)} and balanced accuracy score is \\\n{balanced_accuracy_score(y_test,y_pred)}\\n')\n\nfrom sklearn.metrics import precision_recall_fscore_support\nimport warnings\nwarnings.filterwarnings('ignore')\np, r, f, s = precision_recall_fscore_support(\n    y_test,\n    y_pred,\n    labels=[3,4,5,6,7,8], # the labels for which we want the metrics determined\n    average=None, # when None, returns a metric per label\n)\nresults_nb=pd.DataFrame(data=[p.round(2),r.round(2),f.round(2),s],\n                         index=[['Precision','Recall','f score','Support']],\n                         columns=[['class 3','class 4','class 5','class 6','class 7','class 8']])\n\nresults_nb","73245e96":"#defining class weights\nw={3:0.1,4:0.4,5:4.2,6:3.9,7:1.2,8:0.1}","269a0731":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score,GridSearchCV\nimport warnings\nwarnings.filterwarnings('ignore')\nlog_class = LogisticRegression(class_weight=w,multi_class='multinomial',random_state = 0,max_iter=500)\nlog_class.fit(X_train_2, y_train)\n\ncross_val_score(log_class,X_train_2,y_train,cv=5,scoring='accuracy',n_jobs=-1);","d07dcf1c":"#model after tuning hyperparameters\nlog_class = LogisticRegression(class_weight=w,C=0.08,penalty='l2',solver='newton-cg',multi_class='multinomial',random_state = 0)\nlog_class.fit(X_train_2, y_train)\n\ny_pred=log_class.predict(X_test_2)\n\n#presenting results in dataframe\nprint(f'\\nAccuracy score is {accuracy_score(y_test,y_pred)} and balanced accuracy score is {balanced_accuracy_score(y_test,y_pred)}\\n')\np, r, f, s = precision_recall_fscore_support(\n    y_test,\n    y_pred,\n    labels=[3,4,5,6,7,8], # the labels for which we want the metrics determined\n    average=None, # when None, returns a metric per label\n)\nresults_lr=pd.DataFrame(data=[p.round(2),r.round(2),f.round(2),s],\n                        index=[['Precision','Recall','f score','Support']],\n                        columns=[['class 3','class 4','class 5','class 6','class 7','class 8']])\n\nresults_lr","792260cd":"#for KNN I will plot results and choose the optimum number of neighbours\nfrom sklearn.neighbors import KNeighborsClassifier\nacc=[1]*20\nfor n in range (1,21):\n    knn_class = KNeighborsClassifier(n_neighbors = n, metric = 'minkowski', p = 2)\n    knn_class.fit(X_train_2, y_train)\n    y_pred = knn_class.predict(X_test_2)\n    acc[n-1]=accuracy_score(y_test,y_pred)\n\nn=list(np.linspace(1,20,20)) \nplt.figure()\nplt.plot(n,acc,color='red')\n\nplt.xticks(ticks=list(np.linspace(1,20,20)))\nplt.title(f'KNN : No. of neighbours vs. accuracy')\nplt.xlabel('No. of neighbours')\nplt.ylabel('Accuracy)')\n\nax=plt.gca() #setting right and top edge as invisible\nax.spines['top'].set_color('white')\nax.spines['right'].set_color('white')\n\nplt.show()","27175536":"knn_class = KNeighborsClassifier(n_neighbors = 18, metric = 'minkowski', p = 2)\nknn_class.fit(X_train_2, y_train)\ny_pred = knn_class.predict(X_test_2)\n\nprint(f'\\nAccuracy score is {accuracy_score(y_test,y_pred)} and balanced accuracy score is {balanced_accuracy_score(y_test,y_pred)}\\n')\n\n\n#presenting results in dataframe\nfrom sklearn.metrics import precision_recall_fscore_support\nimport warnings\nwarnings.filterwarnings('ignore')\np, r, f, s = precision_recall_fscore_support(\n    y_test,\n    y_pred,\n    labels=[3,4,5,6,7,8], # the labels for which we want the metrics determined\n    average=None, # when None, returns a metric per label\n)\nresults_knn=pd.DataFrame(data=[p.round(2),r.round(2),f.round(2),s],\n                         index=[['Precision','Recall','f score','Support']],\n                         columns=[['class 3','class 4','class 5','class 6','class 7','class 8']])\n\nresults_knn","09cba08a":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\nsv_class = SVC(class_weight=w,kernel = 'rbf',C=2, random_state = 0,probability=True)\nsv_class.fit(X_train_2, y_train)\n\ncross_val_score(sv_class,X_train_2,y_train,cv=5,scoring='accuracy',n_jobs=-1);","c5cea12c":"#model after tuning hyperparameters\nsv_class = SVC(class_weight=w,kernel = 'rbf',C=100, gamma=0.001, random_state = 0)\nsv_class.fit(X_train_2, y_train)\n\ny_pred=sv_class.predict(X_test_2)\n\nprint(f'\\nAccuracy score is {accuracy_score(y_test,y_pred)} and balanced accuracy score is {balanced_accuracy_score(y_test,y_pred)}\\n')\n\n#presenting results in dataframe\np, r, f, s = precision_recall_fscore_support(\n    y_test,\n    y_pred,\n    labels=[3,4,5,6,7,8], # the labels for which we want the metrics determined\n    average=None, # when None, returns a metric per label\n)\nresults_svc=pd.DataFrame(data=[p.round(2),r.round(2),f.round(2),s],\n                         index=[['Precision','Recall','f score','Support']],\n                         columns=[['class 3','class 4','class 5','class 6','class 7','class 8']])\n\nresults_svc","1ce0472d":"from sklearn.tree import DecisionTreeClassifier\ntree_class = DecisionTreeClassifier(class_weight=w,random_state = 0)\ntree_class.fit(X_train_2, y_train)\n\ncross_val_score(sv_class,X_train_2,y_train,cv=5,scoring='accuracy',n_jobs=-1);","a1b68f9b":"#model after tuning hyperparameters\ntree_class = DecisionTreeClassifier(class_weight=w,criterion='entropy',max_depth=5,max_features='sqrt', \n                                    min_samples_leaf=8, min_samples_split=4,random_state=0)\ntree_class.fit(X_train_2, y_train)\n\ny_pred=tree_class.predict(X_test_2)\n\nprint(f'\\nAccuracy score is {accuracy_score(y_test,y_pred)} and balanced accuracy score is {balanced_accuracy_score(y_test,y_pred)}\\n')\n\n#presenting results in dataframe\np, r, f, s = precision_recall_fscore_support(\n    y_test,\n    y_pred,\n    labels=[3,4,5,6,7,8], # the labels for which we want the metrics determined\n    average=None, # when None, returns a metric per label\n)\n\nresults_tree=pd.DataFrame(data=[p.round(2),r.round(2),f.round(2),s],\n                         index=[['Precision','Recall','f score','Support']],\n                         columns=[['class 3','class 4','class 5','class 6','class 7','class 8']])\n\nresults_tree","6572c4fa":"from sklearn.ensemble import RandomForestClassifier\nforest_class = RandomForestClassifier(class_weight=w,n_estimators = 10, random_state = 0)\nforest_class.fit(X_train_2, y_train)\n\ncross_val_score(sv_class,X_train_2,y_train,cv=5,scoring='accuracy',n_jobs=-1);","3b4a1df5":"#model after tuning hyperparameters\nforest_class = RandomForestClassifier(class_weight=w,bootstrap=True, max_depth=10, max_features='sqrt', min_samples_leaf=10,\n                                      min_samples_split=4 ,n_estimators = 8, random_state = 0)\nforest_class.fit(X_train_2, y_train)\n\ny_pred=forest_class.predict(X_test_2)\n\nprint(f'Accuracy score is {accuracy_score(y_test,y_pred)} and balanced accuracy score is {balanced_accuracy_score(y_test,y_pred)}')\n\np, r, f, s = precision_recall_fscore_support(\n    y_test,\n    y_pred,\n    labels=[3,4,5,6,7,8], # the labels for which we want the metrics determined\n    average=None, # when None, returns a metric per label\n)\n\nresults_forest=pd.DataFrame(data=[p.round(2),r.round(2),f.round(2),s],\n                         index=[['Precision','Recall','f score','Support']],\n                         columns=[['class 3','class 4','class 5','class 6','class 7','class 8']])\n\nresults_forest","29b90b6d":"print('Naive Bayes\\n',results_nb,'\\nLogistic classification\\n',results_lr,'\\nKNN\\n',results_knn,'\\nRandom Forest\\n',results_forest)","546be3de":"from yellowbrick.classifier import PrecisionRecallCurve \n\nclasses=wine['quality'].sort_values().unique()\n\nvisualizer = PrecisionRecallCurve(nb_class,classes=classes)\nvisualizer.fit(X_train_2, y_train) # Fit the training data to the visualizer\nvisualizer.score(X_test_2, y_test)        # Evaluate the model on the test data\nvisualizer.show()                       # Finalize and show the figure\nplt.show()\n\nvisualizer = PrecisionRecallCurve(knn_class,classes=classes)\nvisualizer.fit(X_train_2, y_train) # Fit the training data to the visualizer\nvisualizer.score(X_test_2, y_test)        # Evaluate the model on the test data\nvisualizer.show()                       # Finalize and show the figure\nplt.show()","7c5c6890":"models=[log_class,forest_class]\n\nfor classifier in models:\n    p, r, f, s = precision_recall_fscore_support(y_test,classifier.predict(X_test_2),\n                                             labels=[3,4,5,6,7,8],average='micro')\n    \n    print(f'Average micro precision for {classifier}\\nfor all classes is {p.round(2)}\\n')\n","15cee3ee":"from yellowbrick.classifier import ROCAUC\nclasses=wine['quality'].sort_values().unique()\n    \nvisualizer = ROCAUC(nb_class, classes=classes,encoder={3:3,4:4,5:5,6:6,7:7,8:8})\nvisualizer.fit(X_train_2, y_train)\nvisualizer.score(X_test_2, y_test)\nvisualizer.show()\n\nvisualizer = ROCAUC(log_class, classes=classes,encoder={3:3,4:4,5:5,6:6,7:7,8:8})\nvisualizer.fit(X_train_2, y_train)\nvisualizer.score(X_test_2, y_test)\nvisualizer.show()\n\nvisualizer = ROCAUC(knn_class, classes=classes,encoder={3:3,4:4,5:5,6:6,7:7,8:8})\nvisualizer.fit(X_train_2, y_train)\nvisualizer.score(X_test_2, y_test)\nvisualizer.show()\n\nvisualizer = ROCAUC(forest_class, classes=classes,encoder={3:3,4:4,5:5,6:6,7:7,8:8})\nvisualizer.fit(X_train_2, y_train)\nvisualizer.score(X_test_2, y_test)\nvisualizer.show()","e6862964":"# Building classification models and tuning hyperparameters","c7e67e97":"Those models predicted the 3 majority classess with avearge precision-recall scores, but the lower classes predictions were not good. The lower calsses account for only 6% of data. Wines are rerely graded that low so in oiur case we are not interested very much oin precision\/recall of minority class. \n\nIn that case the micro-precision\/recall is the appropriate measure. ","ec486283":"Naive Bayes picked some of the class 7 and 8 but the percentage for classes with the majority data are around 60%","528d1bf7":"For rest of the classifiers I will reppeat the same procedure:\n-run cross validation to check on \"possible\" accuracy\n-run GridSearch and cross validation in order to tune hyperparameters.","e7f39cf1":"# Part 1 \n\n# Duplicates and correlation and data distribution","cbef5c4a":"Note: Features will have to be scaled for certain models","751a8697":"# Part 3\n# PRC or ROC-AUC?","c4be6439":"# Visualizing features distribution","4c881fbc":"1. fixed acidity- most acids involved with wine or fixed or nonvolatile (do not evaporate readily)\n2. volatile acidity- the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste\n3. citric acid-found in small quantities, citric acid can add 'freshness' and flavor to wines\n4. residual sugar - the amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram\/liter and wines with greater than 45 grams\/liter are considered sweet\n5. chlorides-the amount of salt in the wine\n6. free sulfur dioxide-the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine\n7. total sulfur dioxide-amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2 concentrations over 50 ppm, SO2 becomes evident in the nose and taste of wine\n8. density- the density of water is close to that of water depending on the percent alcohol and sugar content\n9. pH- describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale\n10. sulphates- a wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial and antioxidant\n\n","6d336aa6":"Based on precision results KNN and Logistic Regression are performing the best. HOwever our score is still affected by our imbalanced data. \n\nOne way to ov3rcome this would be to redefine wine quality and make two classes : \"average\": for wines with quality <6 and \"good\": for wines with quality bigger than 5. ","0b359c70":"I will first try Naive Bayes","1864d769":"# Discusssion and approach to outliers\n\nFrom box-plots above one could conclude that features have significant number of outliers. But if we think of the data we have, they don't have to be necessarily outliers.\nSince wine making is a process it could be that those wines were not produced up to standards, or that the grape variety was not of good quality, or that the year was not very good year for that grape variety etc... \nIf we research a bit more those aparent outlier values could actually be valid.\n\nFor example :\n\" For wines containing more than 35 g\/L of sugars, the total sulfur dioxide content can be up to 300 mg\/L\"\n\"Typically, the pH level of a wine ranges from 3 to 4\"\n\n\nSo I will try to apply LocalOutlierFactor. The local outlier actually sounds like a much more appropriate solution opver here. \n\nThe local outlier factor uses distance to neighbours to calculate density.  As it's stated in sklearn,by  comparing the local density of a sample to the local densities of its neighbors, one can identify samples that have a substantially lower density than their neighbors. Which menas we actually asses how isolated is the sample and based on that we potentially reject is as an outlier. \n\nBased on the definition of this approach I see some \"distanced\" samples (scale for distance is important to observe) in volatile acidity, residual sugars, chlorides,total sulphor dioxide, pH, sulphates and alcochol. ","cde6e3d8":"From above correaltion heatmap I can see that there is a relatively high correlation between the fixed acidity and volatile acidity, fixed acidit and density and fixed acidity and Ph value. Citric acdi and density also have realtively high correlation woth some other features. This things should one have in mind when doing feature selection. For example, people sense fixed acids with their tongue, and volatile with their nose. But, the volatile acidity can affect the quality of wine a lot and therefore I would rather keep the volatile acidity and drop the fixed acidity.That would also resolve a problem of high correlation between the fixed acidity and other above mentioned features. \n\nLet's cehck the features distribution.","327c8fe3":"In an example of imbalanced dataset like here we have to ask ourselves: do we want our classifier to perform better for majority or minority classes? \n\nClasses 3,4 and 8 account to only 6% of data. In this case data oversampling would not be approrpiate as majority of wines on the market are greaded as 5,6,7. Smaller number of wines are of very low or very high quality. \n\nSo, when building our models we have to have this in mind. \n\n I will look more into majority classes as I rather have model predicting thise classes with high accuracy than model that predicts well for only 5% of data. \n \n \nLet's check on quality category of duplicates","2d36f1d9":"24 points were dropped which is less than 2% of the original data.  Let's now visualize our data again. ","01c097ec":"#checking for null values\nwine.info()","c966dc74":"In the part  2 I will perform feature selection using OLS. \n\nAfter choosing the most important features I will divide my set into test and train and try few classification models. I will use cross validation,accuracy_score, GridSearch, precission\/recall and roc-auc score. ","3126f6f4":"Interestingly,the IQR approach to outliers did not detect outlier in \"alcochol\" feature but the LocalOutlierFactor approach did. And if we look the value removed is value of around 15% alcochol and that is probably right decision. Vino verdhe has usually less than 11.5% of alcochol , so value  around 15% does seem a bit too much. \n\nAlso, as I suspected  some samples with high residual sugars,free and total sulphor dioxid and sulphates were removed. \n","085464ab":"We can see that fixed  and volatile acidity have similar distributions. Same thing with free and total sulfur.We can also notice soem potential outliers. Let's check them with box-and-whisker plot.","696cc6ad":"Since there are 240 duplicatred entries we will remove them as they may cause modle overfitting. Before I delete them I will quickly check on my 'quality' column distribution because that is my target column","45c401ab":"# Part 2 \n\n# Feature selection with OLS","eb0d26f7":"Removing those duplicates will decrease the number ofdata within the categories witth the majority data\nso it is actually good","e2941045":"# Why ROC-AUC score is not a good indicator of model performance for imbalanced data ?\n\nThe next visualization will help us see how the roc-auc score can be misleading for imbalanced data","d0c365cb":"We eliminated 5 variables : fixed acidity, citric acid, residual sugar, free sulphor and pH. All these were values that had highest correlation with other features. ","7e9e9172":"We can see from above theat the highest accuracy is for n=18 or n=19. Since it is very small difference I will chose 18 as it is less computationally expensive."}}