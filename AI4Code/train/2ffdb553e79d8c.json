{"cell_type":{"8a89a039":"code","a71ef09b":"code","7e016aca":"code","76136b48":"code","36cf13b2":"code","fed60a27":"code","872b82e6":"code","3f84b40c":"code","7fe3dfa4":"code","53282971":"code","b571c29e":"code","e3719b8f":"code","5c5c8234":"code","be27ee28":"code","982d5ddf":"code","13a7b600":"code","6b08b2a2":"code","3bb01b75":"code","4d75b90a":"code","4ce355e8":"code","6848ce32":"code","dc3ca87e":"code","c45cc0b1":"code","8ea3c1a8":"markdown","17dbb8cd":"markdown","a74e2b28":"markdown","79876e94":"markdown","01d888e3":"markdown","4ebdb2f3":"markdown","201e2db1":"markdown","c7ad1625":"markdown"},"source":{"8a89a039":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a71ef09b":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","7e016aca":"oov_tok = '<OOV>'\npad_conf = 'post'\ntrunc_conf= 'post'\nmax_length= 40\n# stop_words = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n","76136b48":"from nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))","36cf13b2":"train_data = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')\ntrain_data.head()","fed60a27":"sentences= []\ncounter=0\nfor comment in train_data.comment_text:\n    sentence= comment.replace('\\n', ' the ')\n    sentence= comment.replace('\\\\', ' the ')\n    sentence= comment.replace(\"\\'\", ' the ')\n    sentence= comment.replace('\\\"', ' the ')\n    sentence= comment.replace('\\a', ' the ')\n    sentence= comment.replace('\\b', ' the ')\n    sentence= comment.replace('\\f', ' the ')\n    sentence= comment.replace('\\r', ' the ')\n    sentence= comment.replace('\\t', ' the ')\n    sentence= sentence.lower()\n    sentence= ' '.join([word for word in sentence.split(' ') if word not in stop_words])\n    sentences.append(sentence)","872b82e6":"vocab_size= 3000\ntokenizer = Tokenizer(num_words=vocab_size, oov_token= oov_tok)\ntokenizer.fit_on_texts(sentences)\nword_index = tokenizer.word_index\n# len(word_index)","3f84b40c":"sequences= tokenizer.texts_to_sequences(sentences)\nsequences = pad_sequences(sequences, maxlen=max_length, padding=pad_conf, truncating=trunc_conf)\n\n# average length is 39.1345\n# I choos 40 as the maxlen for truncating\n\n# This is how I calculated the average length\n# counter=0\n# counter2=0\n# for seq in sequences:\n#     counter+=len(seq)\n#     counter2 +=1\n# print('average length is : ' + str(counter\/counter2))\n\n","7fe3dfa4":"#  train and validation split:\n# print(len(sequences))\nsplit = round(0.8 * len(sequences))\nvalid_seq= sequences[split:]\ntrain_seq= sequences[:split]\n# print(len(train_seq))","53282971":"train_labels= train_data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].head(split)\nvalid_labels=  train_data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].tail(len(sequences)-split)\n\ntrain_labels= np.array(train_labels).astype(np.uint8)\nvalid_labels= np.array(valid_labels).astype(np.uint8)","b571c29e":"# we can always delete what we don't need to make some room in the RAM\ndel train_data\ndel sequences","e3719b8f":"# num_epochs = 10\nembeding_dim= 32\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embeding_dim, input_length= max_length ),\n    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dense(6, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","5c5c8234":"history = model.fit(train_seq, train_labels, epochs=4, validation_data=(valid_seq, valid_labels), verbose=2)\n\nprint(\"Training Complete\")","be27ee28":"test_data = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip')\ntest_labels= pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv.zip')\ntest_labels = test_labels[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]","982d5ddf":"# indexNames = test_labels[ test_labels['toxic'] == -1 ].index\n# test_labels.drop(indexNames , inplace=True)\n# test_data.drop(indexNames, inplace=True)\n\nsentences= []\nfor comment in test_data.comment_text:\n    sentence= comment.replace('\\n', ' the ')\n    sentence= comment.replace('\\\\', ' the ')\n    sentence= comment.replace(\"\\'\", ' the ')\n    sentence= comment.replace('\\\"', ' the ')\n    sentence= comment.replace('\\a', ' the ')\n    sentence= comment.replace('\\b', ' the ')\n    sentence= comment.replace('\\f', ' the ')\n    sentence= comment.replace('\\r', ' the ')\n    sentence= comment.replace('\\t', ' the ')\n    sentence= sentence.lower()\n    sentence= ' '.join([word for word in sentence.split(' ') if word not in stop_words])\n    sentences.append(sentence)\n    \ntest_sequences= tokenizer.texts_to_sequences(sentences)\ntest_sequences = pad_sequences(test_sequences, maxlen=max_length, padding=pad_conf, truncating=trunc_conf)","13a7b600":"test_labels=np.array(test_labels).astype(np.uint8)","6b08b2a2":"predictions = model.predict(test_sequences)","3bb01b75":"predictions= pd.DataFrame(predictions, columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])","4d75b90a":"predictions","4ce355e8":"test_id= pd.Series(test_data['id'].values, name='id')\nresults= pd.concat([test_id, predictions], axis=1)","6848ce32":"results.head(5)","dc3ca87e":"len(test_data['id'])","c45cc0b1":"results.to_csv('results.csv', index=False)","8ea3c1a8":"# Tokenizing preprations\nNow, in order to tokenize our texts, convert them to sequences and pad\/truncate them into equal sizes we are defining some parameters as the following.\n**max_length** determines the size of sequences. if a sequence size is less than 40, by padding we are going to add some 0's so that the number of elements in the sequence sum up to 40. And if it is more than 40, we are going to delete some elements of the sequence. The value **'post'** means while we are padding, the 0s will be added to the end of the sequence not to the begining of it; or while we are truncating the elements will be deleted from the end.\n\nthe **OOV** stands for **out of vocabulary**. we are going to tokenize the first 3000 words (chosen arbitrarily), and any new word that we encounter afterwards is categorized as OOV.****","17dbb8cd":"aside from the pandas and numpy we are gioing to use TensorFlow and its childs. So we do some imports here:","a74e2b28":"# Prediction\nAfter training the model, we take the test data and we go through the same preprocessing of training data all over again. The one difference is that we are now using the pre-trained tokenizer on the new body of text. \nAfter that we can use the **model.predict(.)** method to get the final results.","79876e94":"# The DNN part\n\nBefore creating our NN model, we might want to split our training data into train and validation sets. we keep 80% of the samples for training and the remaining 20% for validation.\nwe get the 6 lables, and turn them to numpy arrays so we can feed them to the NN later on.","01d888e3":"# Sentences to Sequences\nWe get our tokenizer object to some use now; defining the vocab_size we fit the tokenizer on our edited corpus, we get sequences out of it and we then use ***pad_sequences*** to make them equal in size. The padded sequences will have a length of max_length which we have defined earlier to be 40.","4ebdb2f3":"# The model\nIt is how we define the model, the embeding with a dimension of 32, following with a convolution layer and a 1D max pooling one after that.\nnext, is a LSTM layer to consider the previous words (context) in the meaning.\nand finally, a dense layer with 6 nodes (1 node per label) to determine the output.\nSince the multilable problem means that a sample can have multiple lables, we choose sigmoid over softmax (the sum of output can easily be greater than 1 in this case) and we used binarry_crossentropy loss function.","201e2db1":"Now, we are going to read the data and show the top 5 rows.\nthen we separate and refine our body of texts by removing special characters (for example '\\n') and converting to lowercase format.\nwe also remove the stopwords.","c7ad1625":"# Stop Words\nSome words do not contribute to the meaning of the sentence but they are used too often; we don't want to spend our precious time and computation power for them. we call them stop words, and we remove them from the our body of text. there are different sources for stop words, one of the famous ones is provided by the NLTK library which we can download as follows. "}}