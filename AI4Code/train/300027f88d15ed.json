{"cell_type":{"8497a3ba":"code","00e4004b":"code","58c56dd5":"code","b8cd1bee":"code","c32efa1c":"code","efcc0c6c":"code","f679b47b":"code","c34cab61":"code","a6eed854":"code","c28c1478":"code","d5cb006f":"code","f9259c12":"code","06a29546":"code","0cc95aa3":"code","3739ce00":"code","75bde4be":"code","c1106e60":"code","2be51d47":"code","b73fe61a":"code","44e00d88":"code","39cb671f":"code","20cbfb18":"code","341247e8":"code","4faaad41":"code","e60b5313":"code","c8686ec5":"code","b818f6b3":"code","053bb5e0":"code","aa17ae6e":"code","7ed6a3b0":"code","011da849":"code","7dc37ea0":"markdown","ec39e3bb":"markdown","5bf0557b":"markdown","28add63f":"markdown","152b734d":"markdown","7f842ebf":"markdown","bafe28fa":"markdown","9236c3b3":"markdown","b247c612":"markdown","7e1379d0":"markdown","4095bf49":"markdown","7e4d264f":"markdown","d73c1495":"markdown","4c6c93e3":"markdown","588ee462":"markdown","2629f6e4":"markdown","8b4cf116":"markdown","e3b779e0":"markdown","3f09a150":"markdown","0e2ea186":"markdown","0b949c1b":"markdown","7932ad8a":"markdown","8fc24004":"markdown","84313627":"markdown"},"source":{"8497a3ba":"import os\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nfrom collections import Counter","00e4004b":"files = os.listdir(\"..\/input\")\nfiles","58c56dd5":"data_parts = {}\nfor file_name in files:\n    file_id = file_name.split(\".\")[0]\n    data_parts[file_id] = pd.read_csv(\"..\/input\/\" + file_name)\n    print(file_id)\n    print(data_parts[file_id].shape)","b8cd1bee":"def add_nodes(G, df, col, type_name):\n    \"\"\"Add entities to G from the 'col' column of the 'df' DataFrame. The new nodes are annotated with 'type_name' label.\"\"\"\n    nodes = list(df[~df[col].isnull()][col].unique())\n    G.add_nodes_from([(n,dict(type=type_name)) for n in nodes])\n    print(\"Nodes (%s,%s) were added\" % (col, type_name))\n    \ndef add_links(G, df, col1, col2, type_name):\n    \"\"\"Add links to G from the 'df' DataFrame. The new edges are annotated with 'type_name' label.\"\"\"\n    df_tmp = df[(~df[col1].isnull()) & (~df[col2].isnull())]\n    links = list(zip(df_tmp[col1],df_tmp[col2]))\n    G.add_edges_from([(src, trg, dict(type=type_name)) for src, trg in links])\n    print(\"Edges (%s->%s,%s) were added\" % (col1, col2, type_name))","c32efa1c":"G = nx.DiGraph()","efcc0c6c":"add_nodes(G, data_parts[\"answers\"], \"answers_id\", \"answer\")\nadd_nodes(G, data_parts[\"comments\"], \"comments_id\", \"comment\")\nadd_nodes(G, data_parts[\"groups\"], \"groups_id\", \"group\")\nadd_nodes(G, data_parts[\"groups\"], \"groups_group_type\", \"group_type\")\nadd_nodes(G, data_parts[\"professionals\"], \"professionals_id\", \"professional\")\nadd_nodes(G, data_parts[\"professionals\"], \"professionals_industry\", \"industry\")\nadd_nodes(G, data_parts[\"questions\"], \"questions_id\", \"question\")\nadd_nodes(G, data_parts[\"school_memberships\"], \"school_memberships_school_id\", \"school\")\nadd_nodes(G, data_parts[\"students\"], \"students_id\", \"student\")\nadd_nodes(G, data_parts[\"tags\"], \"tags_tag_id\", \"tag\")","f679b47b":"add_links(G, data_parts[\"answers\"], \"answers_id\", \"answers_question_id\", \"question\")\nadd_links(G, data_parts[\"answers\"], \"answers_id\", \"answers_author_id\", \"author\")\nadd_links(G, data_parts[\"comments\"], \"comments_id\", \"comments_parent_content_id\", \"parent_content\")\nadd_links(G, data_parts[\"comments\"], \"comments_id\", \"comments_author_id\", \"author\")\nadd_links(G, data_parts[\"group_memberships\"], \"group_memberships_user_id\", \"group_memberships_group_id\", \"member\")\nadd_links(G, data_parts[\"groups\"], \"groups_id\", \"groups_group_type\", \"type\")\nadd_links(G, data_parts[\"professionals\"], \"professionals_id\", \"professionals_industry\", \"type\")\nadd_links(G, data_parts[\"questions\"], \"questions_id\", \"questions_author_id\", \"author\")\nadd_links(G, data_parts[\"school_memberships\"], \"school_memberships_user_id\", \"school_memberships_school_id\", \"member\")\nadd_links(G, data_parts[\"tag_questions\"], \"tag_questions_question_id\", \"tag_questions_tag_id\", \"tag\")\nadd_links(G, data_parts[\"tag_users\"], \"tag_users_user_id\", \"tag_users_tag_id\", \"follow\")","c34cab61":"students = data_parts[\"students\"]\nprofs = data_parts[\"professionals\"]\nstudents = students[~students[\"students_location\"].isnull()]\nprofs = profs[~profs[\"professionals_location\"].isnull()]","a6eed854":"locs1 = list(students[\"students_location\"])\nlocs2 = list(profs[\"professionals_location\"])\nlocs = [loc.lower() for loc in locs1+locs2]\nlocs_unique = list(set(locs))","c28c1478":"cnt = Counter(locs)\ncnt.most_common()[:10]","d5cb006f":"new_edges = []\nnew_nodes = []\nfor loc in locs_unique:\n    loc_hierarchy = loc.split(\", \")\n    loc_nodes = [] # due to city name duplicates in the world\n    k = len(loc_hierarchy)\n    for i in range(k):\n        loc_nodes.append('_'.join(loc_hierarchy[i:]))\n    new_nodes += loc_nodes\n    loc_links = [(loc_nodes[i],loc_nodes[i+1], dict(type=\"location\"))  for i in range(k-1)]\n    new_edges += loc_links\nnew_nodes = list(set(new_nodes))\nnew_nodes = [(n, dict(type=\"location\")) for n in new_nodes]","f9259c12":"G.add_nodes_from(new_nodes)\nG.add_edges_from(new_edges)\nprint(len(new_edges), len(new_nodes))","06a29546":"list(G.in_edges(\"united kingdom\"))[:5]","0cc95aa3":"list(G.in_edges(\"england_united kingdom\"))[:5]","3739ce00":"students[\"students_location\"] = students[\"students_location\"].apply(lambda x: \"_\".join(x.lower().split(\", \")))\nprofs[\"professionals_location\"] = profs[\"professionals_location\"].apply(lambda x: \"_\".join(x.lower().split(\", \")))","75bde4be":"add_links(G, students, \"students_id\", \"students_location\", \"location\")\nadd_links(G, profs, \"professionals_id\", \"professionals_location\", \"location\")","c1106e60":"def encode_graph(G):\n    \"\"\"Encode the nodes of the network into integers\"\"\"\n    nodes = [(n,d.get(\"type\",None)) for n, d in G.nodes(data=True)]\n    nodes_df = pd.DataFrame(nodes, columns=[\"id\",\"type\"]).reset_index()\n    node2idx = dict(zip(nodes_df[\"id\"],nodes_df[\"index\"]))\n    edges = [(node2idx[src], node2idx[trg], d.get(\"type\",None)) for src, trg, d in G.edges(data=True)]\n    edges_df = pd.DataFrame(edges, columns=[\"src\",\"trg\",\"type\"])\n    return nodes_df, edges_df","2be51d47":"print(G.number_of_nodes(), G.number_of_edges())\nG.remove_nodes_from(list(nx.isolates(G)))\nprint(G.number_of_nodes(), G.number_of_edges())","b73fe61a":"nodes_df, edges_df = encode_graph(G)\nlen(nodes_df), len(edges_df)","44e00d88":"print(nodes_df.head())\nprint(nodes_df[\"type\"].value_counts())\nnodes_df.to_csv(\"knowledge_graph_nodes.csv\", index=False)","39cb671f":"print(edges_df.head())\nprint(edges_df[\"type\"].value_counts())\nedges_df[[\"src\",\"trg\"]].to_csv(\"knowledge_graph_edges.csv\", index=False, header=False, sep=\" \")","20cbfb18":"edge_list = list(zip(edges_df[\"src\"],edges_df[\"trg\"]))\nedge_list[:5]","341247e8":"KG = nx.Graph(edge_list)\nKG.number_of_nodes(), KG.number_of_edges()","4faaad41":"largest_cc = max(nx.connected_components(KG), key=len)\nKG = nx.subgraph(KG, largest_cc)\nKG.number_of_nodes(), KG.number_of_edges()","e60b5313":"%%time\nfrom node2vec import Node2Vec\nn2v_obj = Node2Vec(KG, dimensions=10, walk_length=5, num_walks=10, p=1, q=1, workers=1)","c8686ec5":"%%time\nn2v_model = n2v_obj.fit(window=3, min_count=1, batch_words=4)","b818f6b3":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\ndef get_embeddings(model, nodes):\n    \"\"\"Extract representations from the node2vec model\"\"\"\n    embeddings = [list(model.wv.get_vector(n)) for n in nodes]\n    embeddings = np.array(embeddings)\n    print(embeddings.shape)\n    return embeddings\n\ndef dim_reduction(embeddings, labels, frac=None, tsne_obj=TSNE(n_components=2)):\n    \"\"\"Dimensionality reduction with t-SNE. Sampling random instances is supported.\"\"\"\n    N = len(embeddings)\n    print(N)\n    if frac != None:\n        idx = np.random.randint(N, size=int(N*frac))\n        X = embeddings[idx,:]\n        X_labels = [labels[i] for i in idx]\n    else:\n        X = embeddings\n        X_labels = labels\n    X_embedded = tsne_obj.fit_transform(X)\n    print(\"t-SNE object was trained on %i records!\" % X.shape[0])\n    print(X_embedded.shape)\n    return X_embedded, X_labels\n\ndef visu_embeddings(X_embedded, X_labels=None, colors = ['r','b']):\n    if X_labels != None:\n        label_map = {}\n        for i, l in enumerate(usr_tsne_lab):\n            if not l in label_map:\n                label_map[l] = []\n            label_map[l].append(i)\n        fig, ax = plt.subplots(figsize=(15,15))\n        for i, lab in enumerate(label_map.keys()):\n            print(lab)\n            idx = label_map[lab]\n            x = list(X_embedded[idx,0])\n            y = list(X_embedded[idx,1])\n            #print(len(x),len(y))\n            ax.scatter(x, y, c=colors[i], label=lab, alpha=0.5, edgecolors='none')\n        plt.legend()\n    else:\n        plt.figure(figsize=(15,15))\n        x = list(X_embedded[:,0])\n        y = list(X_embedded[:,1])\n        plt.scatter(x, y, alpha=0.5)","053bb5e0":"stud_users = list(nodes_df[nodes_df[\"type\"] == \"student\"][\"index\"])\nprof_users = list(nodes_df[nodes_df[\"type\"] == \"professional\"][\"index\"])\nprint(len(stud_users), len(prof_users))\nstud_users = list(set(stud_users).intersection(set(KG.nodes())))\nprof_users = list(set(prof_users).intersection(set(KG.nodes())))\nprint(len(stud_users), len(prof_users))\nstud_users = [str(item) for item in stud_users]\nprof_users = [str(item) for item in prof_users]","aa17ae6e":"users = stud_users + prof_users\nusr_emb = get_embeddings(n2v_model, users)\nusr_labs = ['student'] * len(stud_users) +  ['professional'] * len(prof_users)","7ed6a3b0":"%%time\nusr_tsne_emb, usr_tsne_lab = dim_reduction(usr_emb, usr_labs, frac=0.5)","011da849":"visu_embeddings(usr_tsne_emb, usr_tsne_lab)","7dc37ea0":"## iv.) Clean and encode knowledge graph","ec39e3bb":"## i.) Nodes\n\nThe vertices of the knowledge graph consists of the following entities:\n- answers\n- questions\n- comments\n- students\n- professionals\n- industries\n- schools\n- tags\n- user groups\n- group types","5bf0557b":"#### Node information summary","28add63f":"# CareerVillage knowledge graph analysis with node embeddings\n\n# 1. Introduction\n\n## a.) What is a knowledge graph?\n\nIt is a network that represents multiple types of entities (nodes) and relations (edges) in the same graph. Each link of the network represents an (entity,relation,value) triplet. For example I give you some examples from the small knowledge graph below:\n\n- Eiffel Tower **(entity)** is located in **(relation)** in Paris **(value)**\n- Paris **(entity)** is an instance of **(relation)** city **(value)**\n- Alice **(entity)** is an instance of **(relation)** person **(value)**\n\n![Example knowledge graph](https:\/\/geomarketing.com\/wp-content\/uploads\/2017\/11\/Amazon-Neptune-Knowledge-Graph.jpg)\n\nThe CareerVillage data set contains several different entities (persons, groups, schools, locations etc.). Thus one possible approach to analyse this data is to build a large knowledge graph representing as many relations as possible.\n\n## b.) What are node embeddings?\n\nEmbeddings, especially word representations (e.g. Word2Vec), is a hot research topic nowadays. The basic idea behind node embedding algorithms (e.g. node2vec, Line, DeepWalk etc.) is that if you generate node sequences originating from a given vertex and feed them to Word2Vec (like sentences from a text) you can map the nodes of the network into a low dimensional vector space. Usually these methods are optimized for the criteria to preserve the global\/local role of each vertex in the graph.\n\n![Example node embeding](https:\/\/www.sentiance.com\/wp-content\/uploads\/2018\/01\/deepwalk1-2.png)\n\n## c.) My work\n\nIn this work I use the [node2vec](https:\/\/cs.stanford.edu\/people\/jure\/pubs\/node2vec-kdd16.pdf) algorithm to generate low dimensional representation for CareerVillage users based on the knowledge graph that I extracted from the data. My ultimate goal is to discover interesting user groups \/ clusters (e.g. popular professionals, satisfied students etc.) using only the available network structure.","152b734d":"# 3. Build knowledge graph from CareerVillage data","7f842ebf":"#### Encode the nodes to have integer identifiers","bafe28fa":"## iii.) Location information\n\nLocation information of users and professionals are preprocessed before I add it to the knowledge graph. I tried to extract **city \/ state \/ country** hierarchy from locations were it was provided. In this case I created different levels for locations: cities, states\/regions and countries.","9236c3b3":"# 4. Node2Vec","b247c612":"#### Remove isolated nodes","7e1379d0":"# 5. Visualization of node embeddings\n\nI trained Node2Vec to map every node of the knowledge graph into a 10 dimensional Euclidean space. In order to visualize the results in 2 dimension I applied the [t-SNE](https:\/\/en.wikipedia.org\/wiki\/T-distributed_stochastic_neighbor_embedding) dimensionality reduction algorithm. ","4095bf49":"## ii.) Edges","7e4d264f":"#### Add location nodes to the graph\n\n- the 3 level of nodes are added\n- connections between cities -> regions, regions -> contires are added","d73c1495":"#### Most common locations:","4c6c93e3":"### I visualize only a randomly selected 50% of the users for clarity","588ee462":"#### Link users to the first level of locations:","2629f6e4":"**In this work I use the Python3 implementation of [node2vec](https:\/\/github.com\/eliorc\/node2vec).**\n\n- First we generate random walks starting from each node of the knowledge graph by initializing the *Node2Vec* object\n- Then we optimize node representations with the *fit()* method","8b4cf116":"#### Edge information summary","e3b779e0":"> # 2. Load data","3f09a150":"## a.) User embeddings with t-SNE dimensionality reduction","0e2ea186":"#### Examples:\n\n- Locations that are immediate in-neighbors of entity United Kingdom (e.g.: England, Scotland etc.).","0b949c1b":"#### NOTE: We will analyse only the greatest (weakly) connected component of our knowledge graph. ","7932ad8a":"#### Preprocess location information:","8fc24004":"# 6. Summary\n\nThe CareerVillage data contains several entities and different type of connections between them. My ultimate goal was to analyze this data only from the aspect of network structure.\n\nI managed to separate the users into **students** and **professionals** using only the underlying knowledge graph. First I mapped the nodes of the network to the 10 dimensional Euclidean space with **node2vec**. Then I applied t-SNE dimensionality reduction to visualize the results.\n\n**Feel free to reach out to [me](https:\/\/github.com\/ferencberes) if you have any questions related to this topic or this notebook! :)**","84313627":"- Locations that are in-neighbors of entity England"}}