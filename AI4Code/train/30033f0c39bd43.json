{"cell_type":{"783914ad":"code","4566382e":"code","e4c63870":"code","de946c87":"code","d5a59fb0":"code","dcc19e08":"code","b79d94b4":"code","ce5d8e2e":"code","a3f616df":"code","38f7ef40":"code","fdc04751":"code","12ad0f0a":"code","9c41fb51":"code","29fe5acc":"code","6e2e1ecc":"code","871c85d1":"code","4460f38d":"code","ebd69a93":"code","e8591308":"code","889db67f":"code","0b3697bd":"code","9816a6b6":"code","18b09dcf":"code","6ee3b83e":"code","de91f829":"code","d3cf9f47":"code","5fe3f60f":"code","b44ef480":"code","7b3453e2":"code","a512ab72":"code","716a6637":"code","a73c1aca":"code","3c18b044":"code","edae937b":"code","8e8f182b":"code","0aab096f":"code","c04856a1":"code","8a7d372f":"code","8a594d09":"code","0dfb04c2":"code","aaefae1e":"code","70f16f30":"code","84e0e750":"code","a0a21397":"code","a88de563":"code","9bca065f":"markdown","23c2ee34":"markdown","5a165679":"markdown","b4a34c43":"markdown","14f50a5c":"markdown","ede6be14":"markdown","f62bae17":"markdown","8803a035":"markdown","5542c517":"markdown","29481af9":"markdown","d5248593":"markdown","fe512a93":"markdown","e807fa20":"markdown"},"source":{"783914ad":"import pandas as pd\nimport os","4566382e":"print(os.listdir('..\/input\/nlpglovedataset'))\nPATH = '..\/input\/nlpglovedataset\/'\n\n# Data is review of amazon , yelp and imdb reviews\n# glove.6B.*.txt are pre-trained weights","e4c63870":"# Create one datafarme havinf data of all source\n# New column created to distiguish data source\n\nreviewDataPath = {'yelp': PATH +'yelp_labelled.txt',\n                 'amazon': PATH +'amazon_cells_labelled.txt',\n                 'imdb': PATH +'imdb_labelled.txt'}\nreviewList = []\n\nfor source, filepath in reviewDataPath.items():\n    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n    # Add another column filled with the source name\n    df['source'] = source \n    reviewList.append(df)\n\ndf = pd.concat(reviewList)","de946c87":"review_imdb = df[df['source'] == 'amazon']\nreview_imdb.info()","d5a59fb0":"# Just a look at data\nprint(df.iloc[:10])","dcc19e08":"from sklearn.model_selection import train_test_split","b79d94b4":"# Creating model of YELP data onl\n\nreview_yelp = df[df['source'] == 'yelp']\n\nsentences = review_yelp['sentence'].values\n\ny = review_yelp['label'].values\n\nsentences_train, sentences_test, y_train, y_test = train_test_split(\n    sentences, y, test_size=0.25, random_state=1000)","ce5d8e2e":"#sentences_train.size","a3f616df":"from keras.preprocessing.text import Tokenizer","38f7ef40":"tokenizer = Tokenizer(num_words=1000)\n\ntokenizer.fit_on_texts(sentences_train)\n\nX_train = tokenizer.texts_to_sequences(sentences_train)\nX_test = tokenizer.texts_to_sequences(sentences_test)\n","fdc04751":"# Adding 1 because of reserved 0 index\n# The indexing is ordered after the most common words in the text, \n# which you can see by the word the having the index 1. \n# It is important to note that the index 0 is reserved \n# and is not assigned to any word. This zero index is used for padding,\n# because every statement is not of same size\n\nvocab_size = len(tokenizer.word_index) + 1 \n\nprint(vocab_size)","12ad0f0a":"# Increasing vocab size by 1 as need to make room for '0' index\nvocab_size = len(tokenizer.word_index) + 1","9c41fb51":"# Lets look at top 5 sentence \nprint(sentences_train[:6])","29fe5acc":"# Lets look at top 5 sentence toeknized \nprint(X_train[0])\nprint(X_train[1])\nprint(X_train[2])\nprint(X_train[3])\nprint(X_train[4])\nprint(X_train[5])","6e2e1ecc":"from keras.preprocessing.sequence import pad_sequences","871c85d1":"# maxlen parameter to specify how long the sequences should be. \n#This cuts sequences that exceed that number.\n\nmaxlen = 100","4460f38d":"X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n","ebd69a93":"print(X_train[1])","e8591308":"print(X_train[4])","889db67f":"from keras.models import Sequential\nfrom keras import layers","0b3697bd":"model = Sequential()","9816a6b6":"# vocab size is 1750 \n# input_length is size of review text after tokenization and pad sequance\nembedding_dim = 50\n\n\nmodel.add(layers.Embedding(input_dim=vocab_size,\n                           output_dim=embedding_dim,\n                           input_length=maxlen))\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(6, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\n\nmodel.summary()","18b09dcf":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n","6ee3b83e":"history = model.fit(X_train, y_train,\n                    epochs=25,verbose=True,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)","de91f829":"loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\n","d3cf9f47":"loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy: {:.4f}\".format(accuracy))\n","5fe3f60f":"import numpy as np\nphrase = \"good food ,will come again\"\n#phrase = \"bad service\"\n#phrase = \"asdasd\"\n\n# ------------ALERT-------------------------\n#  Need to use same tokenizer object \n#  which was used to tokenize training data\n# ------------------------------------------\ntokens = tokenizer.texts_to_sequences([phrase])\npad_tokens = pad_sequences(tokens, padding='post', maxlen=maxlen)\n\nprint(tokens)\nprint(pad_tokens)","b44ef480":"val = model.predict_classes(pad_tokens)   \nprint(val)","7b3453e2":"def predictSentiments ( indexvalue):\n    \n    reviewSentiment = ''\n    \n    if (val[0][0] == 0):\n        reviewSentiment = 'Customer is gone forever,'\n    else:\n        reviewSentiment = 'you got back your customer'\n\n    return reviewSentiment;","a512ab72":"print(predictSentiments(val[0][0]))","716a6637":"from keras.models import load_model\nimport pickle\n\n# Creates a HDF5 file 'my_model.h5'\nmodel.save('my_model.h5')\n\n# Deletes the existing model\n#del model  \n\n\n# saving tokenizer \nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n","a73c1aca":"# loading\nwith open('tokenizer.pickle', 'rb') as handle:\n    tokenizer_saved = pickle.load(handle)\n\n# Returns a compiled model identical to the previous one\nmodel_saved = load_model('my_model.h5')","3c18b044":"#review_sen = \"good food ,will come again\"\nreview_sen = \"bad service\"\n\ntokens_sen = tokenizer_saved.texts_to_sequences([review_sen])\npad_tokens_sen = pad_sequences(tokens_sen, padding='post', maxlen=maxlen)\n\nprint(tokens_sen)\nprint(pad_tokens_sen)","edae937b":"val = model_saved.predict_classes(pad_tokens_sen)\nprint(predictSentiments(val[0][0]))","8e8f182b":"model2 = Sequential()\n\nmodel2.add(layers.Embedding(input_dim=vocab_size,output_dim=embedding_dim,input_length=maxlen))\n\n# remove flatten and use global max pool\nmodel2.add(layers.GlobalMaxPool1D())\n\nmodel2.add(layers.Dense(6, activation='relu'))\nmodel2.add(layers.Dense(1, activation='sigmoid'))\nmodel2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nmodel2.summary()","0aab096f":"history2 = model2.fit(X_train, y_train,\n                    epochs=25,verbose=True,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)","c04856a1":"loss, accuracy = model2.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\n\n","8a7d372f":"loss, accuracy = model2.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy: {:.4f}\".format(accuracy))\n\n","8a594d09":"# Creating embedding from pre-trained GloVe \n# GloVe is trained on millions of senetece\n# We need to take weights of wrods which \n# exist in our training data only\n# This method takes words from training data and\n# copy weight of word from GloVe to a new array\n# which we will use as word embeding\n\ndef create_embedding_matrix(filepath, word_index, embedding_dim):\n    \n    vocab_size = len(word_index) + 1 \n    # Adding again 1 because of reserved 0 index\n    \n    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n\n    with open(filepath) as file:\n        for line in file:\n            word, *vector = line.split()\n            if word in word_index:\n                idx = word_index[word]\n                #print(\"{} {} \".format(word,idx))\n                embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n    return embedding_matrix","0dfb04c2":"# Lets take 50-dimesional weights\n\nembedding_dim = 50\n\nfilePath = PATH + 'glove.6B.50d.txt'\n\nembedding_matrix = create_embedding_matrix(filePath,\n                                           tokenizer.word_index, \n                                           embedding_dim)","aaefae1e":"print(embedding_matrix[0:2])","70f16f30":"model3 = Sequential()\n\nmodel3.add(layers.Embedding(vocab_size, \n                            embedding_dim,\n                            weights=[embedding_matrix], # Change is here\n                            input_length=maxlen,\n                            trainable=True)) # Make it False to check model perfromance\n#model3.add(layers.Conv1D(128, 5, activation='relu'))\nmodel3.add(layers.GlobalMaxPool1D())\n\nmodel3.add(layers.Dense(5, activation='relu'))\nmodel3.add(layers.Dense(1, activation='sigmoid'))\nmodel3.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nmodel3.summary()","84e0e750":"history3 = model3.fit(X_train, y_train,\n                    epochs=50,verbose=True,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)","a0a21397":"loss, accuracy = model3.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))","a88de563":"loss, accuracy = model3.evaluate(X_test, y_test, verbose=False)\nprint(\"Test Accuracy: {:.4f}\".format(accuracy))\n","9bca065f":"# Using Pre-Trained GloVe vector","23c2ee34":"# Split data in train and test","5a165679":"# Tokenize data","b4a34c43":"**NLP Starter**\n* It is basic starter for NLP task\n\n*What will be done*\n* Sentiment analysis of review (  binary classification )\n* Tokenization of text\n* Keras Embeddging Model\n* Using Glove pre-trained embedding \n* Do prediction\n* save the mode \/ load mode and do prediction\n* Fine tuning model","14f50a5c":"Each text sequence has in most cases different length of words. \nTo counter this, pad_sequence() is used ,which simply pads the sequence of words with zeros. \nBy default, it prepends zeros but we want to append them.\nTypically it does not matter whether you prepend or append zeros.","ede6be14":"# Save the model to re-use later","f62bae17":"# Load Model and tokenizer","8803a035":"# Read data from review files ","5542c517":"# Let's do the prediction","29481af9":"With Tokenizer, the resulting vectors equal the length of each text, and the numbers don\u2019t denote counts,\nbut rather correspond to the word values from the dictionary tokenizer.word_index.","d5248593":"# Another model\n    Fine tuning","fe512a93":"# PAD Sequance","e807fa20":"# Model Creation"}}