{"cell_type":{"b54b5d8a":"code","a572218f":"code","b96149d4":"code","7df23cb0":"code","f7c6dc77":"code","d606b2cb":"code","966239a6":"code","b42e8bc4":"code","3f1a3c14":"code","5f0cc116":"code","a8ffc92e":"code","84203579":"code","090e847b":"code","b6446af6":"code","8d1b45ab":"code","0a3879b6":"code","ed11f759":"code","70354199":"code","685fa78e":"code","e46dd89c":"code","47eac6a1":"code","ee0048c5":"code","e87a4bf9":"code","62d26e38":"code","e0267e1f":"code","d5b3c4e7":"code","772db24b":"code","1b9dbd23":"code","0c744059":"code","38596bca":"code","5acd3808":"code","ea00ba18":"code","56779ad2":"code","b99af871":"code","344c11b6":"code","99d9ef12":"code","cd6e0f08":"code","30da23b8":"code","4ad42c96":"code","5b7c22b8":"code","42372a84":"code","14ab0a1a":"code","11ad0a41":"code","238ca0db":"code","ba72b42d":"code","fec33dea":"code","5f2b1caf":"code","fce7a74d":"code","cf84fd31":"code","a5e8bf09":"code","8d13b507":"code","1f0deee8":"code","79b77cb4":"code","1add1b9b":"code","e3c09c15":"code","f624f849":"code","086fa66a":"markdown","7c14414a":"markdown","008bc6e6":"markdown","4836c909":"markdown","bf4cffe2":"markdown","dfd83e86":"markdown","a1640191":"markdown","711df9b2":"markdown","05ce8322":"markdown","64a48b01":"markdown","61619709":"markdown","90ad94b0":"markdown","41b3a631":"markdown","f493befa":"markdown","7c49b658":"markdown","afdfaa6d":"markdown","37f8203f":"markdown","666545e3":"markdown","02fbe339":"markdown","201e206e":"markdown","86da434d":"markdown","979f21b0":"markdown","93f02267":"markdown","1d546377":"markdown","cbc03500":"markdown","50b93f55":"markdown","b03dbb2b":"markdown","08a8bd23":"markdown","7927f929":"markdown"},"source":{"b54b5d8a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import ExtraTreesClassifier","a572218f":"from sklearn import model_selection\nfrom sklearn import preprocessing","b96149d4":"#df_treino = pd.read_csv('data\/dataset_treino.csv')\n#df_teste = pd.read_csv('data\/dataset_teste.csv')\n#df_loja = pd.read_csv('data\/lojas.csv')\n\ndf_treino = pd.read_csv('..\/input\/dataset_treino.csv')\ndf_teste = pd.read_csv('..\/input\/dataset_teste.csv')\ndf_loja = pd.read_csv('..\/input\/lojas.csv')","7df23cb0":"df_treino.head(5)","f7c6dc77":"df_teste.head(5)","d606b2cb":"df_loja.head(5)","966239a6":"print(df_treino.shape, df_teste.shape)","b42e8bc4":"df_treino.isnull().sum()","3f1a3c14":"# Ajusta os valores aprensentados no describe()\npd.options.display.float_format = '{:.2f}'.format","5f0cc116":"df_treino.describe()","a8ffc92e":"fig = plt.figure(figsize =(9,7))\ncorrelations = df_treino.corr()\nsns.heatmap(correlations, cmap=sns.diverging_palette(141,20,as_cmap=True), annot=True)","84203579":"#sns.pairplot(df_treino)\n#pd.scatter_matrix(df_treino, figsize=(6,6))","090e847b":"fig = plt.figure(figsize=(15,6))\nprint(sns.distplot(df_treino[df_treino['Open'] == 1]['Sales'], bins=80, color='darkblue'))","b6446af6":"df_treino.dtypes","8d1b45ab":"df_treino['StateHoliday'].unique()\ndf_treino['StateHoliday'].replace([0,'0'])\ncolumns = {'StateHoliday':{'0':0, 'a':1, 'b':2, 'c':2}}\ndf_treino.replace(columns, inplace = True)\nprint(df_treino['StateHoliday'].unique())","0a3879b6":"df_loja['PromoInterval'].fillna('None', inplace = True)","ed11f759":"df_loja.head(4)","70354199":"df_loja_nulos = df_loja.isnull().any()[df_loja.isnull().any() == True].index\ndf_treino_nulos = df_treino.isnull().any()[df_treino.isnull().any() == True].index\ndf_teste_nulos = df_teste.isnull().any()[df_teste.isnull().any() == True].index","685fa78e":"print('{}:\\n {}\\n\\n'.format('df_loja',df_loja[df_loja_nulos].isnull().sum()));\nprint('{}:\\n {}\\n\\n'.format('df_treino',df_treino[df_treino_nulos].isnull().sum()));\nprint('{}:\\n {}\\n'.format('df_teste',df_teste[df_teste_nulos].isnull().sum()));","e46dd89c":"print(df_teste['Open'].mode()[0])\nprint(df_treino['Open'].mode()[0])\n# O valor mais frequente em ambos os datasets \u00e9 0. Esse ser\u00e1 o valor de interpola\u00e7\u00e3o","47eac6a1":"df_teste.loc[:,'Open'].fillna(0, inplace = True)\nprint('Total NA {}'.format(df_teste.loc[:,'Open'].isnull().sum()))","ee0048c5":"print(round(df_loja.loc[:,'CompetitionDistance'].mean(), 2))\nprint(df_loja.loc[:,'CompetitionOpenSinceMonth'].mode()[0])\nprint(df_loja.loc[:,'CompetitionOpenSinceYear'].mode()[0])\nprint(df_loja.loc[:,'Promo2SinceWeek'].mode()[0])\nprint(df_loja.loc[:,'Promo2SinceYear'].mode()[0])","e87a4bf9":"df_loja.loc[:,'CompetitionDistance'].fillna(5404.9, inplace = True)\ndf_loja.loc[:,'CompetitionOpenSinceMonth'].fillna(9.0, inplace = True)\ndf_loja.loc[:,'CompetitionOpenSinceYear'].fillna(2013.0, inplace = True)\ndf_loja.loc[:,'Promo2SinceWeek'].fillna(14.0, inplace = True)\ndf_loja.loc[:,'Promo2SinceYear'].fillna(2011.0, inplace = True)","62d26e38":"print('Total NA {}'.format(df_loja.loc[:,'CompetitionOpenSinceMonth'].isnull().sum()))\nprint('Total NA {}'.format(df_loja.loc[:,'CompetitionOpenSinceYear'].isnull().sum()))\nprint('Total NA {}'.format(df_loja.loc[:,'Promo2SinceWeek'].isnull().sum()))\nprint('Total NA {}'.format(df_loja.loc[:,'Promo2SinceYear'].isnull().sum()))","e0267e1f":"df_treino = df_treino[df_treino['Sales']!=0]\nprint(df_treino.shape)","d5b3c4e7":"print('{}:\\n {}\\n\\n'.format('df_loja',df_loja[df_loja_nulos].isnull().sum()));\nprint('{}:\\n {}\\n\\n'.format('df_treino',df_treino[df_treino_nulos].isnull().sum()));\nprint('{}:\\n {}\\n\\n'.format('df_teste',df_teste[df_teste_nulos].isnull().sum()));","772db24b":"df_treino['Date_year'] = pd.DatetimeIndex(df_treino['Date']).year\ndf_treino['Date_month'] = pd.DatetimeIndex(df_treino['Date']).month\ndf_treino['Date_day'] = pd.DatetimeIndex(df_treino['Date']).day\n\ndf_teste['Date_year'] = pd.DatetimeIndex(df_teste['Date']).year\ndf_teste['Date_month'] = pd.DatetimeIndex(df_teste['Date']).month\ndf_teste['Date_day'] = pd.DatetimeIndex(df_teste['Date']).day","1b9dbd23":"subset_month = pd.DataFrame()\nsubset_year = pd.DataFrame()","0c744059":"subset_df_treino = df_treino.head(0)","38596bca":"print(df_treino.groupby('Date_year').size())\nprint(df_treino.groupby('Date_month').size())\nprint(df_treino.groupby('Date_day').size())\nprint(df_treino.shape)","5acd3808":"df_treino = pd.merge(df_treino, df_loja, on='Store')\ndf_teste = pd.merge(df_teste, df_loja, on='Store')","ea00ba18":"def encoder(df):\n    for column in df.loc[:,df.dtypes == object].columns:\n        df[column] = preprocessing.LabelEncoder().fit(df[column]).transform(df[column])","56779ad2":"encoder(df_treino)\nencoder(df_teste)","b99af871":"df_treino.head(3)","344c11b6":"df_teste.head(3)","99d9ef12":"print('Teste: {} - Treino: {}'.format(df_teste.shape, df_treino.shape))","cd6e0f08":"df_treino.describe()","30da23b8":"df_teste_previsoes = pd.Series()","4ad42c96":"df_treino.head(7)","5b7c22b8":"df_teste.head(7)","42372a84":"# Agrupando dados por loja para testar o comportamento do modelo treinado para cada loja\ndf_treino_agrupado = dict(list(df_treino.groupby('Store'))) # Transforma o Store (campo agrupado) em um Id na key\ndf_teste_agrupado = dict(list(df_teste.groupby('Store')))","14ab0a1a":"stores_id = df_teste_agrupado[1].loc[:,'Id'].unique()","11ad0a41":"def get_treino(id):\n    df_treino_features_label = df_treino_agrupado[id]\n    df_treino_label = df_treino_features_label.loc[:, 'Sales'] \n    df_treino_features = df_treino_features_label.drop(['Customers','Sales'], axis = 1)    \n    return df_treino_features, df_treino_label\n\ndef get_teste(id):\n    df_teste_features_id = df_teste_agrupado[id]\n    df_teste_id = df_teste_features_id.loc[:, 'Id'] \n    df_teste_features = df_teste_features_id.drop(['Id'], axis = 1)\n    return df_teste_features[df_treino_features.columns], df_teste_id","238ca0db":"len(df_teste_agrupado.keys())","ba72b42d":"print(len(df_treino_agrupado))\nprint(len(df_teste_agrupado))","fec33dea":"def rmspe(real, previsto):\n    return np.sqrt(np.mean(np.square((real-previsto)\/real)))","5f2b1caf":"from sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor","fce7a74d":"df_treino.head(3)","cf84fd31":"def optmize(x_treino, y_treino):\n    modelo = XGBRegressor()\n    \n    params = {'n_estimators':[300], 'max_depth': [4], 'subsample': [0.2, 0.5], 'colsample_bytree': [0.5, 0.9]}\n    \n    grid = model_selection.GridSearchCV(estimator=modelo, param_grid=params)\n    grid.fit(x_treino, y_treino.ravel())\n    \n    return grid.best_estimator_","a5e8bf09":"df_teste_previsoes = pd.Series()","8d13b507":"def append_predictions(store, y_previsto):\n    df_teste_previsoes.append(pd.Series(y_previsto, index=store))","1f0deee8":"%%time\nfrom sklearn.preprocessing import StandardScaler\n\nfor store in df_teste_agrupado:\n    \n    modelo = GradientBoostingRegressor()\n    modelo = GradientBoostingRegressor(n_estimators=241)\n    \n    # Obtendo as lojas com o store 'x'\n    df_treino_features, df_treino_label = get_treino(store)\n    df_teste_features, df_teste_id = get_teste(store)\n    \n    # Divis\u00e3o dos dados em treino e teste\n    x_treino, x_teste, y_treino, y_teste = model_selection.train_test_split(df_treino_features, df_treino_label, test_size =0.20) #1\/5\n\n    # Fit do modelo\n    #fit = modelo.fit(x_treino, y_treino)  # Treinamos o mode com os dados de teste (x e y, respectivamente)\n    #y_previsto = fit.predict(x_teste)\n    \n    # Avalia\u00e7\u00e3o\n    #print_rmspe(y_teste, y_previsto)\n    print(\"\\n>>>>>> Training Store: {}\".format(store))\n    #cv = model_selection.cross_val_score(estimator=modelo, X=x_treino, y=y_treino, scoring='r2')\n    # Otimizando o modelo\n    modelo = optmize(x_treino, y_treino)\n    fit = modelo.fit(x_treino, y_treino)\n    y_previsto_op = fit.predict(x_teste)\n    cv_op = model_selection.cross_val_score(estimator=modelo, X=x_treino, y=y_treino, scoring='r2')\n    print(\"r2: {}% - rmspe: {}\".format(round(cv_op.mean() * 100, 2), round(rmspe(y_teste, y_previsto_op), 6)))\n    \n    #print(\"r2: {}% - {}% \\n\".format(round(cv.mean() * 100, 2), round(cv_op.mean() * 100, 2)))\n    #print(\"rmspe: {} - {} \\n\".format(round(rmspe(y_teste, y_previsto), 2),rmspe(y_teste, y_previsto_op)))\n    \n    #print(previsto)\n    #print(df_teste_id)\n    \n    # Anexando resultados\n    df_teste_previsoes = df_teste_previsoes.append(pd.Series(modelo.predict(df_teste_features), index=df_teste_id))   \n    #print(modelo)\n    #print()","79b77cb4":"%%time\n\ndf_treino = df_treino[df_treino_features.columns]\ndf_teste = df_teste[df_treino_features.columns]\n\nparameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n              'max_depth': [5, 6, 7],\n              'min_child_weight': [2,4],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7],\n              'n_estimators': [400]}\n\nmodelo = XGBRegressor()\n\nx_treino, x_teste, y_treino, y_teste = model_selection.train_test_split(df_treino[df_treino_features.columns], df_treino.loc[:,'Sales'], test_size =0.20)\n\ngrid = model_selection.GridSearchCV(estimator=modelo, param_grid=parameters)\ngrid.fit(x_treino, y_treino.ravel())\n\nprevisoes = pd.Series(grid.best_estimator_.predict(df_teste))\n\nprint(grid.best_estimator_)\n\"\"\"\nXGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=0.7, gamma=0, importance_type='gain',\n       learning_rate=0.07, max_delta_step=0, max_depth=7,\n       min_child_weight=4, missing=None, n_estimators=400, n_jobs=1,\n       nthread=4, objective='reg:linear', random_state=0, reg_alpha=0,\n       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n       subsample=0.7)\n\"\"\"\n\nprevisoes = pd.DataFrame({'real':y_teste, 'previsto':grid.best_estimator_.predict(x_teste)}) \n\nprevisoes.head(4)\n\nrmspe(previsoes.loc[:,'real'], previsoes.loc[:,'previsto']) # 0.19298122496957015","1add1b9b":"# Gravando as predi\u00e7\u00f5es do modelo\ndf_teste_previsoes_EXPORT = pd.DataFrame({'Id':df_teste_previsoes.index,'Sales': list(df_teste_previsoes.values)})","e3c09c15":"df_teste_previsoes_EXPORT.head(5)","f624f849":"# Exportando o resultado \ndf_teste_previsoes_EXPORT.to_csv('df_teste_previsoes_EXPORT.csv', index=False)","086fa66a":"O Desvio Padr\u00e3o de Sales, Store e Customers s\u00e3o consideravelmente altos","7c14414a":"Verificando o shape dos dados","008bc6e6":"##### Encoder StateHoliday","4836c909":"# <a>Preparar os dados<\/a>  \n\n## Carregando os pacotes","bf4cffe2":"## Importando os dados","dfd83e86":"# Prepara\u00e7\u00e3o dos dados","a1640191":"#### Distribui\u00e7\u00e3o das vendas (com a loja aberta)","711df9b2":"### M\u00e9trica de Avalia\u00e7\u00e3o","05ce8322":"##### Aplica encoder","64a48b01":"#### Correla\u00e7\u00f5es","61619709":"## Predi\u00e7\u00f5es com todas as lojas","90ad94b0":"## Predi\u00e7\u00f5es por loja","41b3a631":"#### Pr\u00e9-Processamento","f493befa":"##### Ajustando valores de df_teste","7c49b658":"##### Substitiundo valores faltantes por 'None'","afdfaa6d":"##### Verificandos os valores substituidos","37f8203f":"### Otimiza\u00e7ao Hiperparametrica","666545e3":"##### Encoder das variaveis n\u00e3o num\u00e9ricas (convertendo para representa\u00e7\u00f5es num\u00e9ricas)","02fbe339":"#### Nota-se que conseguimos uma melhor performance realizando as predi\u00e7\u00f5es por loja. O rmspe foi menor realizando as predi\u00e7\u00f5es por agrupamento das lojas","201e206e":"##### Ajustando valores de df_loja","86da434d":"# Modelagem","979f21b0":"Visualizando as primeiras linhas","93f02267":"##### Unificando os datasets","1d546377":"#### Sum\u00e1rio dos dados","cbc03500":"##### Definindo as fun\u00e7\u00f5es para obter os dados de treino e teste de cada loja","50b93f55":"#### Verificando valores Faltantes (Missing Values)","b03dbb2b":"N\u00e3o possuimos valores ausentes","08a8bd23":"##### Tratando valores Nan para None","7927f929":"## Explorando os dados"}}