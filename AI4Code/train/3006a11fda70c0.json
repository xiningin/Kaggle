{"cell_type":{"db01a482":"code","f89f913a":"code","47f93657":"code","c4a0919d":"code","9f4aa57c":"code","1ab9001b":"code","1d654318":"code","1bc9b837":"code","b0ac1bff":"code","e56c3e99":"code","03a496a7":"code","ab9b4ea7":"markdown","abd5a387":"markdown","77cdeeaa":"markdown","a910febd":"markdown","a8421130":"markdown","7624be24":"markdown","4e7a510d":"markdown","a64fd2a1":"markdown","09f245e9":"markdown","079a265f":"markdown","c850de90":"markdown","8e9a9627":"markdown","7c4e5a46":"markdown"},"source":{"db01a482":"import pandas as pd \nimport seaborn as sns\nimport re\nimport gc\nimport os\nimport numpy as np\nimport operator\nfrom wordcloud import WordCloud, STOPWORDS\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\npd_ctx = pd.option_context('display.max_colwidth', 100)\n\nimport nltk\nfrom nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n\nfrom gensim.models import KeyedVectors\n\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as metrics\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D,GRU\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, SpatialDropout1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.models import Model,load_model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ndf_train = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/test.csv')\nquora_data = df_train['question_text'].append(df_test['question_text'])\n\n# KH\u1ea2O S\u00c1T D\u1eee LI\u1ec6U TRONG T\u1eacP TRAIN\n\nprint(\"\\033[1mTrain set info\\033[0m\")\nprint(df_train.info())\n\n# Ki\u1ec3m tra c\u00e1c tr\u01b0\u1eddng d\u1eef li\u1ec7u c\u1ee7a c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c \u0111\u00e1nh sincere\nprint(\"\\033[1mSincere Questions: \\033[0m\")\ndisplay(df_train[df_train['target']==0].head())\n# Ki\u1ec3m tra c\u00e1c tr\u01b0\u1eddng d\u1eef li\u1ec7u c\u1ee7a c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c \u0111\u00e1nh sincere\nprint(\"\\033[1mInsincere Questions: \\033[0m\")\ndisplay(df_train[df_train['target']==1].head())\n\ndf_train.target.value_counts()\n\npos_len = len(df_train[df_train['target'] == 1])\nneg_len = len(df_train[df_train['target'] == 0])\ntotal = len(df_train)\nprint(\"\\033[1mTotal = \\033[0m\", total)\nprint(\"\\033[1mSincere questions:\\033[0m {neg} ({percent: .2f}% )\".format(neg = neg_len, percent = neg_len \/ total * 100))\nprint(\"\\033[1mInsincere questions:\\033[0m {pos} ({percent: .2f}% )\".format(pos = pos_len, percent = pos_len \/ total * 100))\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.bar(['sincere', 'insincere'], df_train.target.value_counts())\nplt.show()\n\n#KH\u1ea2O S\u00c1T D\u1eee LI\u1ec6U TRONG T\u1eacP TEST\n\ndf_test.info()\n# Shuffle t\u1eadp train \u0111\u1ec3 ki\u1ec3m tra nh\u1eefng gi\u00e1 tr\u1ecb ng\u1eabu nhi\u00ean\ntrain = df_train.sample(frac=1).reset_index(drop=True)\ndisplay(train.sample(n=10, random_state=344))","f89f913a":"# C\u00c1C H\u00c0M X\u1eec L\u00dd\n\ndef clean_tag(x):\n  if '[math]' in x:\n    x = re.sub('\\[math\\].*?math\\]', 'MATH EQUATION', x) #replacing with [MATH EQUATION]\n    \n  if 'http' in x or 'www' in x:\n    x = re.sub('(?:(?:https?|ftp):\\\/\\\/)?[\\w\/\\-?=%.]+\\.[\\w\/\\-?=%.]+', 'URL', x) #replacing with [url]\n  return x\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022', '~', '@', '\u00a3', '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`', '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a', '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032',  '\u2588', '\u2026', '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u25ba', '\u2212', '\u00a2', '\u00ac', '\u2591', '\u00a1', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\u2592', '\uff1a', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00b8', '\u22c5', '\u2018', '\u221e',  '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u2264', '\u2021', '\u221a', '\u25c4', '\u2501', '\u21d2', '\u25b6', '\u2265', '\u255d', '\u2661', '\u25ca', '\u3002', '\u2708', '\u2261', '\u263a', '\u2714', '\u21b5', '\u2248', '\u2713', '\u2663', '\u260e', '\u2103', '\u25e6', '\u2514', '\u201f', '\uff5e', '\uff01', '\u25cb',  '\u25c6', '\u2116', '\u2660', '\u258c', '\u273f', '\u25b8', '\u2044', '\u25a1', '\u2756', '\u2726', '\uff0e', '\u00f7', '\uff5c', '\u2503', '\uff0f', '\uffe5', '\u2560', '\u21a9', '\u272d', '\u2590', '\u263c', '\u263b', '\u2510', '\u251c', '\u00ab', '\u223c', '\u250c', '\u2109', '\u262e', '\u0e3f', '\u2266', '\u266c', '\u2727', '\u232a', '\uff0d', '\u2302', '\u2716', '\uff65', '\u25d5', '\u203b', '\u2016', '\u25c0', '\u2030', '\\x97', '\u21ba', '\u2206', '\u2518', '\u252c', '\u256c', '\u060c', '\u2318', '\u2282', '\uff1e', '\u2329', '\u2399', '\uff1f', '\u2620', '\u21d0', '\u25ab', '\u2217', '\u2208', '\u2260', '\u2640', '\u2654', '\u02da', '\u2117', '\u2517', '\uff0a', '\u253c', '\u2740', '\uff06', '\u2229', '\u2642', '\u203f', '\u2211', '\u2023', '\u279c', '\u251b', '\u21d3', '\u262f', '\u2296', '\u2600', '\u2533', '\uff1b', '\u2207', '\u21d1', '\u2730', '\u25c7', '\u266f', '\u261e', '\u00b4', '\u2194', '\u250f', '\uff61', '\u25d8', '\u2202', '\u270c', '\u266d', '\u2523', '\u2534', '\u2513', '\u2728', '\\xa0', '\u02dc', '\u2765', '\u252b', '\u2120', '\u2712', '\uff3b', '\u222b', '\\x93', '\u2267', '\uff3d',  '\\x94', '\u2200', '\u265b', '\\x96', '\u2228', '\u25ce', '\u21bb', '\u21e9', '\uff1c', '\u226b', '\u2729', '\u272a', '\u2655', '\u061f', '\u20a4', '\u261b', '\u256e', '\u240a', '\uff0b', '\u2508', '\uff05',  '\u254b', '\u25bd', '\u21e8', '\u253b', '\u2297', '\uffe1', '\u0964', '\u2582', '\u272f', '\u2587', '\uff3f', '\u27a4', '\u271e', '\uff1d', '\u25b7', '\u25b3', '\u25d9', '\u2585', '\u271d', '\u2227', '\u2409', '\u262d', '\u250a', '\u256f', '\u263e', '\u2794', '\u2234', '\\x92', '\u2583', '\u21b3', '\uff3e', '\u05f3', '\u27a2', '\u256d', '\u27a1', '\uff20', '\u2299', '\u2622', '\u02dd', '\u220f', '\u201e', '\u2225', '\u275d', '\u2610',  '\u2586', '\u2571', '\u22d9', '\u0e4f', '\u2601', '\u21d4', '\u2594', '\\x91', '\u279a', '\u25e1', '\u2570', '\\x85', '\u2662', '\u02d9', '\u06de', '\u2718', '\u272e', '\u2611', '\u22c6', '\u24d8', '\u2752', '\u2623', '\u2709', '\u230a', '\u27a0', '\u2223', '\u2751', '\u25e2', '\u24d2', '\\x80', '\u3012', '\u2215', '\u25ae', '\u29bf', '\u272b', '\u271a', '\u22ef', '\u2669', '\u2602', '\u275e', '\u2017', '\u0702', '\u261c','\u203e', '\u271c', '\u2572', '\u2218', '\u27e9', '\uff3c', '\u27e8', '\u0387', '\u2717', '\u265a', '\u2205', '\u24d4', '\u25e3', '\u0361', '\u201b', '\u2766', '\u25e0', '\u2704', '\u2744', '\u2203', '\u2423', '\u226a', '\uff62',  '\u2245', '\u25ef', '\u263d', '\u220e', '\uff63', '\u2767', '\u0305', '\u24d0', '\u2198', '\u2693', '\u25a3', '\u02d8', '\u222a', '\u21e2', '\u270d', '\u22a5', '\uff03', '\u23af', '\u21a0', '\u06e9', '\u2630', '\u25e5', '\u2286', '\u273d', '\u26a1', '\u21aa', '\u2741', '\u2639', '\u25fc', '\u2603', '\u25e4', '\u274f', '\u24e2', '\u22b1', '\u279d', '\u0323', '\u2721', '\u2220', '\uff40', '\u25b4', '\u2524', '\u221d', '\u264f', '\u24d0',  '\u270e', '\u037e', '\u2424', '\uff07', '\u2763', '\u2702', '\u2724', '\u24de', '\u262a', '\u2734', '\u2312', '\u02db', '\u2652', '\uff04', '\u2736', '\u25bb', '\u24d4', '\u25cc', '\u25c8', '\u275a', '\u2742', '\uffe6', '\u25c9', '\u255c', '\u0303', '\u2731', '\u2556', '\u2749', '\u24e1', '\u2197', '\u24e3', '\u267b', '\u27bd', '\u05c0', '\u2732', '\u272c', '\u2609', '\u2589', '\u2252', '\u2625', '\u2310', '\u2668', '\u2715', '\u24dd',   '\u22b0', '\u2758', '\uff02', '\u21e7', '\u0335', '\u27aa', '\u2581', '\u258f', '\u2283', '\u24db', '\u201a', '\u2670', '\u0301', '\u270f', '\u23d1', '\u0336', '\u24e2', '\u2a7e', '\uffe0', '\u274d', '\u2243', '\u22f0', '\u264b',  '\uff64', '\u0302', '\u274b', '\u2733', '\u24e4', '\u2564', '\u2595', '\u2323', '\u2738', '\u212e', '\u207a', '\u25a8', '\u2568', '\u24e5', '\u2648', '\u2743', '\u261d', '\u273b', '\u2287', '\u227b', '\u2658', '\u265e',  '\u25c2', '\u271f', '\u2320', '\u2720', '\u261a', '\u2725', '\u274a', '\u24d2', '\u2308', '\u2745', '\u24e1', '\u2667', '\u24de', '\u25ad', '\u2771', '\u24e3', '\u221f', '\u2615', '\u267a', '\u2235', '\u235d', '\u24d1',  '\u2735', '\u2723', '\u066d', '\u2646', '\u24d8', '\u2236', '\u269c', '\u25de', '\u0bcd', '\u2739', '\u27a5', '\u2195', '\u0333', '\u2237', '\u270b', '\u27a7', '\u220b', '\u033f', '\u0367', '\u2505', '\u2964', '\u2b06', '\u22f1', '\u2604', '\u2196', '\u22ee', '\u06d4', '\u264c', '\u24db', '\u2555', '\u2653', '\u276f', '\u264d', '\u258b', '\u273a', '\u2b50', '\u273e', '\u264a', '\u27a3', '\u25bf', '\u24d1', '\u2649', '\u23e0', '\u25fe', '\u25b9',  '\u2a7d', '\u21a6', '\u2565', '\u2375', '\u230b', '\u0589', '\u27a8', '\u222e', '\u21e5', '\u24d7', '\u24d3', '\u207b', '\u239d', '\u2325', '\u2309', '\u25d4', '\u25d1', '\u273c', '\u264e', '\u2650', '\u256a', '\u229a',  '\u2612', '\u21e4', '\u24dc', '\u23a0', '\u25d0', '\u26a0', '\u255e', '\u25d7', '\u2395', '\u24e8', '\u261f', '\u24df', '\u265f', '\u2748', '\u21ac', '\u24d3', '\u25fb', '\u266e', '\u2759', '\u2664', '\u2209', '\u061b',  '\u2042', '\u24dd', '\u05be', '\u2651', '\u256b', '\u2553', '\u2573', '\u2b05', '\u2614', '\u2638', '\u2504', '\u2567', '\u05c3', '\u23a2', '\u2746', '\u22c4', '\u26ab', '\u030f', '\u260f', '\u279e', '\u0342', '\u2419', '\u24e4', '\u25df', '\u030a', '\u2690', '\u2719', '\u2199', '\u033e', '\u2118', '\u2737', '\u237a', '\u274c', '\u22a2', '\u25b5', '\u2705', '\u24d6', '\u2628', '\u25b0', '\u2561', '\u24dc', '\u2624', '\u223d', '\u2558', '\u02f9', '\u21a8', '\u2659', '\u2b07', '\u2671', '\u2321', '\u2800', '\u255b', '\u2755', '\u2509', '\u24df', '\u0300', '\u2656', '\u24da', '\u2506', '\u239c', '\u25dc', '\u26be', '\u2934', '\u2707', '\u255f', '\u239b',    '\u2629', '\u27b2', '\u279f', '\u24e5', '\u24d7', '\u23dd', '\u25c3', '\u2562', '\u21af', '\u2706', '\u02c3', '\u2374', '\u2747', '\u26bd', '\u2552', '\u0338', '\u265c', '\u2613', '\u27b3', '\u21c4', '\u262c', '\u2691',   '\u2710', '\u2303', '\u25c5', '\u25a2', '\u2750', '\u220a', '\u2608', '\u0965', '\u23ae', '\u25a9', '\u0bc1', '\u22b9', '\u2035', '\u2414', '\u260a', '\u27b8', '\u030c', '\u263f', '\u21c9', '\u22b3', '\u2559', '\u24e6',    '\u21e3', '\uff5b', '\u0304', '\u219d', '\u239f', '\u258d', '\u2757', '\u05f4', '\u0384', '\u259e', '\u25c1', '\u26c4', '\u21dd', '\u23aa', '\u2641', '\u21e0', '\u2607', '\u270a', '\u0bbf', '\uff5d', '\u2b55', '\u2798',   '\u2040', '\u2619', '\u275b', '\u2753', '\u27f2', '\u21c0', '\u2272', '\u24d5', '\u23a5', '\\u06dd', '\u0364', '\u208b', '\u0331', '\u030e', '\u265d', '\u2273', '\u2599', '\u27ad', '\u0700', '\u24d6', '\u21db', '\u258a',  '\u21d7', '\u0337', '\u21f1', '\u2105', '\u24e7', '\u269b', '\u0310', '\u0315', '\u21cc', '\u2400', '\u224c', '\u24e6', '\u22a4', '\u0313', '\u2626', '\u24d5', '\u259c', '\u2799', '\u24e8', '\u2328', '\u25ee', '\u2637',    '\u25cd', '\u24da', '\u2254', '\u23e9', '\u2373', '\u211e', '\u250b', '\u02fb', '\u259a', '\u227a', '\u0652', '\u259f', '\u27bb', '\u032a', '\u23ea', '\u0309', '\u239e', '\u2507', '\u235f', '\u21ea', '\u258e', '\u21e6', '\u241d', '\u2937', '\u2256', '\u27f6', '\u2657', '\u0334', '\u2644', '\u0368', '\u0308', '\u275c', '\u0321', '\u259b', '\u2701', '\u27a9', '\u0bbe', '\u02c2', '\u21a5', '\u23ce', '\u23b7', '\u0332', '\u2796', '\u21b2', '\u2a75', '\u0317', '\u2762', \n        '\u224e', '\u2694', '\u21c7', '\u0311', '\u22bf', '\u0316', '\u260d', '\u27b9', '\u294a', '\u2041', '\u2722']\n\ndef clean_punct(x):\n  x = str(x)\n  for punct in puncts:\n    if punct in x:\n      x = x.replace(punct, ' ')\n    return x\n\nmispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'bitcoin', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', \n                'electroneum':'bitcoin','nanodegree':'degree','hotstar':'star','dream11':'dream','ftre':'fire','tensorflow':'framework','unocoin':'bitcoin',\n                'lnmiit':'limit','unacademy':'academy','altcoin':'bitcoin','altcoins':'bitcoin','litecoin':'bitcoin','coinbase':'bitcoin','cryptocurency':'cryptocurrency',\n                'simpliv':'simple','quoras':'quora','schizoids':'psychopath','remainers':'remainder','twinflame':'soulmate','quorans':'quora','brexit':'demonetized', 'iiest':'institute','dceu':'comics','pessat':'exam','uceed':'college','bhakts':'devotee','boruto':'anime','cryptocoin':'bitcoin','blockchains':'blockchain','fiancee':'fiance','redmi':'smartphone','oneplus':'smartphone','qoura':'quora','deepmind':'framework','ryzen':'cpu','whattsapp':'whatsapp',\n                'undertale':'adventure','zenfone':'smartphone','cryptocurencies':'cryptocurrencies','koinex':'bitcoin','zebpay':'bitcoin','binance':'bitcoin','whtsapp':'whatsapp',\n                'reactjs':'framework','bittrex':'bitcoin','bitconnect':'bitcoin','bitfinex':'bitcoin','yourquote':'your quote','whyis':'why is','jiophone':'smartphone',\n                'dogecoin':'bitcoin','onecoin':'bitcoin','poloniex':'bitcoin','7700k':'cpu','angular2':'framework','segwit2x':'bitcoin','hashflare':'bitcoin','940mx':'gpu',\n                'openai':'framework','hashflare':'bitcoin','1050ti':'gpu','nearbuy':'near buy','freebitco':'bitcoin','antminer':'bitcoin','filecoin':'bitcoin','whatapp':'whatsapp',\n                'empowr':'empower','1080ti':'gpu','crytocurrency':'cryptocurrency','8700k':'cpu','whatsaap':'whatsapp','g4560':'cpu','payymoney':'pay money',\n                'fuckboys':'fuck boys','intenship':'internship','zcash':'bitcoin','demonatisation':'demonetization','narcicist':'narcissist','mastuburation':'masturbation',\n                'trignometric':'trigonometric','cryptocurreny':'cryptocurrency','howdid':'how did','crytocurrencies':'cryptocurrencies','phycopath':'psychopath',\n                'bytecoin':'bitcoin','possesiveness':'possessiveness','scollege':'college','humanties':'humanities','altacoin':'bitcoin','demonitised':'demonetized',\n                'bras\u00edlia':'brazilia','accolite':'accolyte','econimics':'economics','varrier':'warrier','quroa':'quora','statergy':'strategy','langague':'language','splatoon':'game','7600k':'cpu','gate2018':'gate 2018','in2018':'in 2018','narcassist':'narcissist','jiocoin':'bitcoin','hnlu':'hulu','7300hq':'cpu','weatern':'western','interledger':'blockchain','deplation':'deflation', 'cryptocurrencies':'cryptocurrency', 'bitcoin':'blockchain cryptocurrency',}\n\ndef correct_mispell(x):\n  words = x.split()\n  for i in range(0, len(words)):\n    if mispell_dict.get(words[i]) is not None:\n      words[i] = mispell_dict.get(words[i])\n    elif mispell_dict.get(words[i].lower()) is not None:\n      words[i] = mispell_dict.get(words[i].lower())\n        \n  words = \" \".join(words)\n  return words\n\ndef remove_stopwords(x):\n  x = [word for word in x.split() if word not in STOPWORDS]\n  x = ' '.join(x)\n  return x\n\ncontraction_mapping = {\n \"I'm\": 'I am', \"I'm'a\": 'I am about to', \"I'm'o\": 'I am going to',\"I've\": 'I have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'd\": 'I would', \"I'd've\": 'I would have', \"i'm\": 'i am', \"i'm'a\": 'i am about to', \"i'm'o\": 'i am going to', \"i've\": 'i have', \"i'll\": 'i will', \"i'll've\": 'i will have', \"i'd\": 'i would', \"i'd've\": 'i would have', 'Whatcha': 'What are you', 'whatcha': 'what are you', \"amn't\": 'am not', \"ain't\": 'are not', \"aren't\": 'are not', \"'cause\": 'because', \"can't\": 'can not', \"can't've\": 'can not have', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"daren't\": 'dare not', \"daresn't\": 'dare not', \"dasn't\": 'dare not', \"didn't\": 'did not', 'didn\u2019t': 'did not', \"don't\": 'do not', 'don\u2019t': 'do not', \"doesn't\": 'does not', \"e'er\": 'ever', \"everyone's\": 'everyone is', 'finna': 'fixing to', 'gimme': 'give me', \"gon't\": 'go not', 'gonna': 'going to', 'gotta': 'got to', \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"he've\": 'he have', \"he's\": 'he is', \"he'll\": 'he will', \"he'll've\": 'he will have', \"he'd\": 'he would', \"he'd've\": 'he would have', \"here's\": 'here is', \"how're\": 'how are', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how's\": 'how is', \"how'll\": 'how will', \"isn't\": 'is not', \"it's\": 'it is', \"'tis\": 'it is', \"'twas\": 'it was', \"it'll\": 'it will', \"it'll've\": 'it will have', \"it'd\": 'it would', \"it'd've\": 'it would have', 'kinda': 'kind of', \"let's\": 'let us', 'luv': 'love', \"ma'am\": 'madam', \"may've\": 'may have', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"ne'er\": 'never', \"o'\": 'of', \"o'clock\": 'of the clock', \"ol'\": 'old', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"o'er\": 'over', \"shan't\": 'shall not', \"sha'n't\": 'shall not', \"shalln't\": 'shall not', \"shan't've\": 'shall not have', \"she's\": 'she is', \"she'll\": 'she will', \"she'd\": 'she would', \"she'd've\": 'she would have', \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so is', \"somebody's\": 'somebody is', \"someone's\": 'someone is', \"something's\": 'something is', 'sux': 'sucks', \"that're\": 'that are', \"that's\": 'that is', \"that'll\": 'that will', \"that'd\": 'that would', \"that'd've\": 'that would have', 'em': 'them', \"there're\": 'there are', \"there's\": 'there is', \"there'll\": 'there will', \"there'd\": 'there would', \"there'd've\": 'there would have', \"these're\": 'these are', \"they're\": 'they are', \"they've\": 'they have', \"they'll\": 'they will', \"they'll've\": 'they will have', \"they'd\": 'they would', \"they'd've\": 'they would have', \"this's\": 'this is', \"those're\": 'those are', \"to've\": 'to have', 'wanna': 'want to', \"wasn't\": 'was not', \"we're\": 'we are', \"we've\": 'we have', \"we'll\": 'we will', \"we'll've\": 'we will have', \"we'd\": 'we would', \"we'd've\": 'we would have', \"weren't\": 'were not', \"what're\": 'what are', \"what'd\": 'what did', \"what've\": 'what have', \"what's\": 'what is', \"what'll\": 'what will', \"what'll've\": 'what will have', \"when've\": 'when have', \"when's\": 'when is', \"where're\": 'where are', \"where'd\": 'where did', \"where've\": 'where have', \"where's\": 'where is', \"which's\": 'which is', \"who're\": 'who are', \"who've\": 'who have', \"who's\": 'who is', \"who'll\": 'who will', \"who'll've\": 'who will have', \"who'd\": 'who would', \"who'd've\": 'who would have', \"why're\": 'why are', \"why'd\": 'why did', \"why've\": 'why have', \"why's\": 'why is', \"will've\": 'will have', \"won't\": 'will not', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have',\n \"y'all\": 'you all', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"you're\": 'you are', \"you've\": 'you have', \"you'll've\": 'you shall have', \"you'll\": 'you will', \"you'd\": 'you would', \"you'd've\": 'you would have', 'jan.': 'january', 'feb.': 'february', 'mar.': 'march', 'apr.': 'april', 'jun.': 'june', 'jul.': 'july', 'aug.': 'august', 'sep.': 'september', 'oct.': 'october', 'nov.': 'november', 'dec.': 'december', 'I\u2019m': 'I am', 'I\u2019m\u2019a': 'I am about to', 'I\u2019m\u2019o': 'I am going to', 'I\u2019ve': 'I have', 'I\u2019ll': 'I will', 'I\u2019ll\u2019ve': 'I will have', 'I\u2019d': 'I would', 'I\u2019d\u2019ve': 'I would have', 'i\u2019m': 'i am', 'i\u2019m\u2019a': 'i am about to', 'i\u2019m\u2019o': 'i am going to', 'i\u2019ve': 'i have', 'i\u2019ll': 'i will', 'i\u2019ll\u2019ve': 'i will have', 'i\u2019d': 'i would', 'i\u2019d\u2019ve': 'i would have', 'amn\u2019t': 'am not', 'ain\u2019t': 'are not','aren\u2019t': 'are not','\u2019cause': 'because','can\u2019t': 'can not','can\u2019t\u2019ve': 'can not have','could\u2019ve': 'could have','couldn\u2019t': 'could not','couldn\u2019t\u2019ve': 'could not have','daren\u2019t': 'dare not','daresn\u2019t': 'dare not','dasn\u2019t': 'dare not','doesn\u2019t': 'does not','e\u2019er': 'ever','everyone\u2019s': 'everyone is','gon\u2019t': 'go not','hadn\u2019t': 'had not','hadn\u2019t\u2019ve': 'had not have','hasn\u2019t': 'has not','haven\u2019t': 'have not','he\u2019ve': 'he have','he\u2019s': 'he is','he\u2019ll': 'he will','he\u2019ll\u2019ve': 'he will have','he\u2019d': 'he would','he\u2019d\u2019ve': 'he would have','here\u2019s': 'here is','how\u2019re': 'how are','how\u2019d': 'how did','how\u2019d\u2019y': 'how do you','how\u2019s': 'how is','how\u2019ll': 'how will',\n 'isn\u2019t': 'is not','it\u2019s': 'it is','\u2019tis': 'it is','\u2019twas': 'it was','it\u2019ll': 'it will','it\u2019ll\u2019ve': 'it will have','it\u2019d': 'it would','it\u2019d\u2019ve': 'it would have','let\u2019s': 'let us','ma\u2019am': 'madam','may\u2019ve': 'may have','mayn\u2019t': 'may not','might\u2019ve': 'might have','mightn\u2019t': 'might not','mightn\u2019t\u2019ve': 'might not have','must\u2019ve': 'must have','mustn\u2019t': 'must not','mustn\u2019t\u2019ve': 'must not have','needn\u2019t': 'need not','needn\u2019t\u2019ve': 'need not have','ne\u2019er': 'never','o\u2019': 'of','o\u2019clock': 'of the clock','ol\u2019': 'old','oughtn\u2019t': 'ought not','oughtn\u2019t\u2019ve': 'ought not have','o\u2019er': 'over',\n 'shan\u2019t': 'shall not','sha\u2019n\u2019t': 'shall not','shalln\u2019t': 'shall not','shan\u2019t\u2019ve': 'shall not have','she\u2019s': 'she is','she\u2019ll': 'she will','she\u2019d': 'she would','she\u2019d\u2019ve': 'she would have','should\u2019ve': 'should have','shouldn\u2019t': 'should not','shouldn\u2019t\u2019ve': 'should not have','so\u2019ve': 'so have','so\u2019s': 'so is','somebody\u2019s': 'somebody is','someone\u2019s': 'someone is','something\u2019s': 'something is','that\u2019re': 'that are','that\u2019s': 'that is','that\u2019ll': 'that will','that\u2019d': 'that would','that\u2019d\u2019ve': 'that would have','there\u2019re': 'there are','there\u2019s': 'there is','there\u2019ll': 'there will','there\u2019d': 'there would','there\u2019d\u2019ve': 'there would have','these\u2019re': 'these are','they\u2019re': 'they are','they\u2019ve': 'they have','they\u2019ll': 'they will','they\u2019ll\u2019ve': 'they will have','they\u2019d': 'they would','they\u2019d\u2019ve': 'they would have','this\u2019s': 'this is','those\u2019re': 'those are','to\u2019ve': 'to have','wasn\u2019t': 'was not','we\u2019re': 'we are','we\u2019ve': 'we have','we\u2019ll': 'we will','we\u2019ll\u2019ve': 'we will have','we\u2019d': 'we would','we\u2019d\u2019ve': 'we would have','weren\u2019t': 'were not','what\u2019re': 'what are','what\u2019d': 'what did','what\u2019ve': 'what have','what\u2019s': 'what is','what\u2019ll': 'what will','what\u2019ll\u2019ve': 'what will have','when\u2019ve': 'when have','when\u2019s': 'when is','where\u2019re': 'where are','where\u2019d': 'where did','where\u2019ve': 'where have','where\u2019s': 'where is','which\u2019s': 'which is','who\u2019re': 'who are','who\u2019ve': 'who have','who\u2019s': 'who is','who\u2019ll': 'who will','who\u2019ll\u2019ve': 'who will have','who\u2019d': 'who would','who\u2019d\u2019ve': 'who would have','why\u2019re': 'why are','why\u2019d': 'why did','why\u2019ve': 'why have','why\u2019s': 'why is','will\u2019ve': 'will have','won\u2019t': 'will not','won\u2019t\u2019ve': 'will not have','would\u2019ve': 'would have','wouldn\u2019t': 'would not','wouldn\u2019t\u2019ve': 'would not have','y\u2019all': 'you all','y\u2019all\u2019re': 'you all are','y\u2019all\u2019ve': 'you all have','y\u2019all\u2019d': 'you all would','y\u2019all\u2019d\u2019ve': 'you all would have','you\u2019re': 'you are','you\u2019ve': 'you have','you\u2019ll\u2019ve': 'you shall have','you\u2019ll': 'you will','you\u2019d': 'you would','you\u2019d\u2019ve': 'you would have'}\n\ndef clean_contractions(text):\n#     text = text.lower()\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    \n    text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])\n    return text\n\nlemmatizer = WordNetLemmatizer()\ndef lemma_text(x):\n    x = x.split()\n    x = [lemmatizer.lemmatize(word) for word in x]\n    x = ' '.join(x)\n    return x\n\ndef data_cleaning(x):\n  x = clean_tag(x)\n  x = clean_punct(x)\n  x = correct_mispell(x)\n  x = remove_stopwords(x)\n  x = clean_contractions(x)\n  x = lemma_text(x)\n  return x\n\ndef Preprocess(doc):\n    corpus=[]\n    for text in tqdm(doc):\n        text=clean_contractions(text)\n        text=correct_mispell(text)\n        text=re.sub(r'[^a-z0-9A-Z]',\" \",text)\n        text=re.sub(r'[0-9]{1}',\"#\",text)\n        text=re.sub(r'[0-9]{2}','##',text)\n        text=re.sub(r'[0-9]{3}','###',text)\n        text=re.sub(r'[0-9]{4}','####',text)\n        text=re.sub(r'[0-9]{5,}','#####',text)\n        corpus.append(text)\n    return corpus\n","47f93657":"# TI\u1ec0N X\u1eec L\u00dd C\u00c1C T\u1eacP D\u1eee LI\u1ec6U\n\ntqdm.pandas(desc=\"progress-bar\")\ndf_test['question_text_cleaned'] = df_test['question_text'].progress_map(lambda x: data_cleaning(x))\ndf_train['question_text_cleaned'] = df_train['question_text'].progress_map(lambda x: data_cleaning(x))\n\ndf_train.head(5)","c4a0919d":"### unzipping all the pretrained embeddings\n!unzip ..\/input\/quora-insincere-questions-classification\/embeddings.zip\n\n!du -h .\/\n\n### Loading t\u1eadp Google News Pretrained Embeddings v\u00e0o b\u1ed9 nh\u1edb\nfile_name=\".\/GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin\"\nmodel_embed=KeyedVectors.load_word2vec_format(file_name,binary=True)\n# model_embed = load_embed1('.\/glove.840B.300d\/glove.840B.300d.txt')\n\n### X\u00e2y d\u1ef1ng t\u1eadp t\u1eeb v\u1ef1ng d\u1ef1a tr\u00ean d\u1eef li\u1ec7u c\u1ee7a t\u1eadp Google News\ndef vocab_build(corpus):\n    vocab={}\n    for text in tqdm(corpus):\n        for word in text.split():\n            try:\n                vocab[word]+=1\n            except KeyError:\n                vocab[word]=1\n    return vocab\n\n\n### Ki\u1ec3m tra t\u1eadp Vocabulary xem t\u1eadp vocab \u0111\u00f3 bao ph\u1ee7 bao nhi\u00eau ph\u1ea7n tr\u0103m t\u1eadp d\u1eef li\u1ec7u c\u1ee7a m\u00ecnh\ndef check_voc(vocab,model):\n    embed_words=[]\n    out_vocab={}\n    total_words=0\n    total_text=0\n    for i in tqdm(vocab):\n        try:\n            vec=model[i]\n            embed_words.append(vec)\n            total_words+=vocab[i]\n        except KeyError:\n            out_vocab[i]=vocab[i]\n            total_text+=vocab[i]\n    print(\"The {:.2f}% of vocabularies have Covered of corpus\".format(100*len(embed_words)\/len(vocab)))\n    print(\"The {:.2f}% of total text had coverded \".format((100*total_words\/(total_words+total_text))))\n    return out_vocab\n\n### x\u00e2y t\u1eadp vocab v\u00e0 ki\u1ec3m tra \u0111\u1ed9 ph\u1ee7 c\u1ee7a t\u1eadp vocab v\u1edbi d\u1eef li\u1ec7u \u0111\u00e3 \u0111\u01b0\u1ee3c x\u1eed l\u00fd\ntotal_text=pd.concat([df_train.question_text_cleaned,df_test.question_text_cleaned])\nvocabulary=vocab_build(total_text)\noov=check_voc(vocabulary,model_embed) #oov: out of vocab\n\ndf_test['question_text_cleaned_2'] = Preprocess(df_test['question_text'])\ndf_train['question_text_cleaned_2'] = Preprocess(df_train['question_text'])\ntotal_text_2=pd.concat([df_train.question_text_cleaned_2,df_test.question_text_cleaned_2])\nvocabulary2=vocab_build(total_text_2)\noov2=check_voc(vocabulary2, model_embed)\n\nsort_oov=dict(sorted(oov2.items(), key=operator.itemgetter(1),reverse=True))\ndict(list(sort_oov.items())[:50])\n\ndel oov, oov2,sort_oov,total_text,total_text_2\ngc.collect()\n\ndef get_word_index(vocab):\n    word_index=dict((w,i+1) for i,w in enumerate(vocab.keys()))\n    return word_index\ndef fit_one_hot(word_index,corpus):\n    sent=[]\n    for text in tqdm(corpus):\n        li=[]\n        for word in text.split():\n            try:\n                li.append(word_index[word])\n            except KeyError:\n                li.append(0)\n        sent.append(li)\n    return sent\n\ntrain,val=train_test_split(df_train,test_size=0.2,stratify=df_train.target,random_state=123)\nvocab_size=len(vocabulary2)+1\nmax_len=40\n\nword_index=get_word_index(vocabulary2)\n### Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u \u0111\u00e3 \u0111\u01b0\u1ee3c x\u1eed l\u00fd\ntrain_text=train['question_text_cleaned_2']\nval_text=val['question_text_cleaned_2']\ntest_text=df_test['question_text_cleaned_2']\n\n### m\u00e3 h\u00f3a c\u00e2u trong t\u1eadp train sang d\u1ea1ng onehot cho d\u1ec5 x\u1eed l\u00fd\nencodes=fit_one_hot(word_index,train_text)\ntrain_padded=pad_sequences(encodes,maxlen=max_len,padding=\"post\")\n\n### m\u00e3 h\u00f3a c\u00e2u trong t\u1eadp validation sang d\u1ea1ng onehot cho d\u1ec5 x\u1eed l\u00fd\nencodes_=fit_one_hot(word_index,val_text)\nval_padded=pad_sequences(encodes_,maxlen=max_len,padding=\"post\")\n\n### m\u00e3 h\u00f3a c\u00e2u trong t\u1eadp test sang d\u1ea1ng onehot cho d\u1ec5 x\u1eed l\u00fd\nencodes__=fit_one_hot(word_index,test_text)\ntest_padded=pad_sequences(encodes__,maxlen=max_len,padding=\"post\")\n\ncount=0\n\nembedding_mat=np.zeros((vocab_size,300))\nfor word,i in tqdm(word_index.items()):\n    try:\n        vec=model_embed[word]\n        embedding_mat[i]=vec\n    except KeyError:\n        count+=1\n        continue\n\nprint(\"Number of Out of Vocabulary\",count)\n","9f4aa57c":"def get_model_origin(matrix):\n    inp = Input(shape=(max_len,))\n    x = Embedding(vocab_size,300,weights=[matrix],input_length=max_len,trainable=False)(inp)\n    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n    x = Conv1D(64,3,activation=\"relu\")(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dense(128, activation=\"relu\")(x)\n    x = Dropout(0.2)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    return model\n\nopt=Adam(learning_rate=0.001)\nBATCH_SIZE = 1024\nbin_loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0, name='binary_crossentropy')\n\n### X\u00e1c \u0111\u1ecbnh \u0111i\u1ec3m callback \u0111\u1ec3 gi\u1ea3m learning rate, v\u00e0 restore l\u1ea1i tr\u1ecdng s\u1ed1 t\u1ed1t nh\u1ea5t k\u1ec1 tr\u01b0\u1edbc \nearly_stopping=tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=3,mode=\"min\",restore_best_weights=True)\n\n### Gi\u1ea3m learning rate khi model kh\u00f4ng \u0111\u01b0\u1ee3c c\u1ea3i thi\u00ean (c\u00e0ng h\u1ecdc c\u00e0ng ngu)\nreduce_lr=tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", factor=0.2, patience=2, verbose=1, mode=\"auto\")\n\nmy_callbacks=[early_stopping,reduce_lr]\n\nstrategy = None\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('Use TPU')\nexcept ValueError:\n    if len(tf.config.list_physical_devices('GPU')) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print('Use GPU')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Use CPU')","1ab9001b":"# TH\u1eec NGHI\u1ec6M MODEL 1\n\nwith strategy.scope():\n    google_model = get_model_origin(embedding_mat)\n    google_model.compile(loss=bin_loss, optimizer=opt, metrics=['accuracy'])\nhistory=google_model.fit(train_padded, train.target, batch_size=BATCH_SIZE, epochs=30, validation_data=(val_padded, val.target),callbacks=my_callbacks)\n\n# summarize history for loss\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n# plt.show()\n# summarize history for accuracy\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 2)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\ngoogle_y_pre=google_model.predict(val_padded, verbose=1)\nbest_score = 0\nbest_thresh = 0\nfor thresh in np.arange(0.1,0.5,0.01):\n    if(best_score < metrics.f1_score(val.target,(google_y_pre>thresh).astype(int))):\n        best_score = metrics.f1_score(val.target,(google_y_pre>thresh))\n        best_thresh = round(thresh, 2)\n    print(\"threshold {0:2.2f} f1 score:{1:2.3f}\".format(thresh,metrics.f1_score(val.target,(google_y_pre>thresh).astype(int))))\nprint(\"\\033[1mBest result {0:2.3f} in thresh_hold {1:2.2f}\\033[0m\".format(best_score, best_thresh))\n\ndel model_embed, history, best_score, best_thresh, google_model, google_y_pre\ngc.collect()","1d654318":"# TH\u1eec NGHI\u1ec6M MODEL 2\n\ndef get_model(matrix):\n    inp = Input(shape=(max_len,))\n    x = Embedding(vocab_size, 300, weights=[matrix], trainable=False)(inp)\n    x = SpatialDropout1D(0.3)(x)\n    x1 = Bidirectional(LSTM(256, return_sequences=True))(x)\n    x2 = Bidirectional(GRU(128, return_sequences=True))(x1)\n    max_pool1 = GlobalMaxPooling1D()(x1)\n    max_pool2 = GlobalMaxPooling1D()(x2)\n    conc = Concatenate()([max_pool1, max_pool2])\n    predictions = Dense(1, activation='sigmoid')(conc)\n    model = Model(inputs=inp, outputs=predictions)\n    adam = optimizers.Adam(lr=0.001)\n    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\nstrategy = None\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('Use TPU')\nexcept ValueError:\n    if len(tf.config.list_physical_devices('GPU')) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print('Use GPU')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Use CPU')\n        \nwith strategy.scope():\n    google_model = get_model(embedding_mat)\n    google_model.compile(loss=bin_loss, optimizer=Adam(lr=0.001), metrics=['accuracy'])\nhistory=google_model.fit(train_padded, train.target, batch_size=BATCH_SIZE, epochs=30, validation_data=(val_padded, val.target),callbacks=my_callbacks)\n\n# summarize history for loss\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n# plt.show()\n# summarize history for accuracy\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 2)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\ngoogle_y_pre=google_model.predict(val_padded, verbose=1)\nbest_score = 0\nbest_thresh = 0\nfor thresh in np.arange(0.1,0.5,0.01):\n    if(best_score < metrics.f1_score(val.target,(google_y_pre>thresh).astype(int))):\n        best_score = metrics.f1_score(val.target,(google_y_pre>thresh))\n        best_thresh = round(thresh, 2)\n    print(\"threshold {0:2.2f} f1 score:{1:2.3f}\".format(thresh,metrics.f1_score(val.target,(google_y_pre>thresh).astype(int))))\nprint(\"\\033[1mBest result {0:2.3f} in thresh_hold {1:2.2f}\\033[0m\".format(best_score, best_thresh))\n\nthreshold=best_thresh\ngoogle_y_test_pre=google_model.predict(test_padded, batch_size=BATCH_SIZE, verbose=1)\ngoogle_y_test_pre=(google_y_test_pre>thresh).astype(int)\n\ndel embedding_mat, history, best_score, best_thresh\ngc.collect()","1bc9b837":"def load_embed(file):\n    def get_coefs(word,*arr):\n        return word, np.asarray(arr, dtype='float32')\n\n    if file == '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec':\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n    else:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n\n    return embeddings_index\n\n!tree -h .\/\n\nglove_path = '.\/glove.840B.300d\/glove.840B.300d.txt'\nparagram_path = '.\/paragram_300_sl999\/paragram_300_sl999.txt'\nwiki_path = '.\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'\n\nglove_embed = load_embed(glove_path)\nprint(\"\\033[1mGlove Coverage: \\033[0m]\")\noov_glove = check_voc(vocabulary2, glove_embed)","b0ac1bff":"# GLOVE\n\nstrategy = None\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('Use TPU')\nexcept ValueError:\n    if len(tf.config.list_physical_devices('GPU')) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print('Use GPU')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Use CPU')\n        \ncount=0\nglove_embedding_mat=np.zeros((vocab_size,300))\nfor word,i in tqdm(word_index.items()):\n    try:\n        vec=glove_embed[word]\n        glove_embedding_mat[i]=vec\n    except KeyError:\n        count+=1\n        continue\n\nprint(\"Number of Out of Vocabulary\",count)\n\nwith strategy.scope():\n    glove_model = get_model(glove_embedding_mat)\n    glove_model.compile(loss=bin_loss, optimizer=Adam(lr=0.001), metrics=['accuracy'])\nglove_history=glove_model.fit(train_padded, train.target, batch_size=BATCH_SIZE, epochs=30, validation_data=(val_padded, val.target),callbacks=my_callbacks)\n\n# summarize history for loss\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 1)\nplt.plot(glove_history.history['loss'])\nplt.plot(glove_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n# plt.show()\n# summarize history for accuracy\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 2)\nplt.plot(glove_history.history['accuracy'])\nplt.plot(glove_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\nglove_y_pre=glove_model.predict(val_padded, verbose=1)\nbest_score = 0\nbest_thresh = 0\nfor thresh in np.arange(0.1,0.5,0.01):\n    if(best_score < metrics.f1_score(val.target,(glove_y_pre>thresh).astype(int))):\n        best_score = metrics.f1_score(val.target,(glove_y_pre>thresh))\n        best_thresh = round(thresh, 2)\n    print(\"threshold {0:2.2f} f1 score:{1:2.3f}\".format(thresh,metrics.f1_score(val.target,(glove_y_pre>thresh).astype(int))))\nprint(\"\\033[1mBest result {0:2.3f} in thresh_hold {1:2.2f}\\033[0m\".format(best_score, best_thresh))\n\nthreshold=best_thresh\nglove_y_test_pre=glove_model.predict(test_padded, batch_size=BATCH_SIZE, verbose=1)\nglove_y_test_pre=(glove_y_test_pre>thresh).astype(int)\n\ndel glove_embed, glove_embedding_mat, glove_history, best_score, best_thresh\ngc.collect()","e56c3e99":"# PARAGRAM\nstrategy = None\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('Use TPU')\nexcept ValueError:\n    if len(tf.config.list_physical_devices('GPU')) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print('Use GPU')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Use CPU')\n        \n\nparagram_embed = load_embed(paragram_path)\nprint(\"\\033[1mParagram Coverage: \\033[0m]\")\noov_paragram = check_voc(vocabulary2, paragram_embed)\n\ncount=0\npara_embedding_mat=np.zeros((vocab_size,300))\nfor word,i in tqdm(word_index.items()):\n    try:\n        vec=paragram_embed[word.lower()]\n        para_embedding_mat[i]=vec\n    except KeyError:\n        count+=1\n        continue\n\nprint(\"Number of Out of Vocabulary\",count)\n\nwith strategy.scope():\n    para_model = get_model(para_embedding_mat)\n    para_model.compile(loss=bin_loss, optimizer=Adam(lr=0.001), metrics=['accuracy'])\npara_history=para_model.fit(train_padded, train.target, batch_size=BATCH_SIZE, epochs=30, validation_data=(val_padded, val.target),callbacks=my_callbacks)\n\n# summarize history for loss\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 1)\nplt.plot(para_history.history['loss'])\nplt.plot(para_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n# plt.show()\n# summarize history for accuracy\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 2)\nplt.plot(para_history.history['accuracy'])\nplt.plot(para_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\npara_y_pre=para_model.predict(val_padded, verbose=1)\nbest_score = 0\nbest_thresh = 0\nfor thresh in np.arange(0.1,0.5,0.01):\n    if(best_score < metrics.f1_score(val.target,(para_y_pre>thresh).astype(int))):\n        best_score = metrics.f1_score(val.target,(para_y_pre>thresh))\n        best_thresh = round(thresh, 2)\n    print(\"threshold {0:2.2f} f1 score:{1:2.3f}\".format(thresh,metrics.f1_score(val.target,(para_y_pre>thresh).astype(int))))\nprint(\"\\033[1mBest result {0:2.3f} in thresh_hold {1:2.2f}\\033[0m\".format(best_score, best_thresh))\n\nthreshold=best_thresh\npara_y_test_pre=para_model.predict(test_padded, batch_size=BATCH_SIZE, verbose=1)\npara_y_test_pre=(para_y_test_pre>thresh).astype(int)\n\ndel paragram_embed, para_embedding_mat, para_history, best_score, best_thresh\ngc.collect()","03a496a7":"# WIKI \nstrategy = None\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('Use TPU')\nexcept ValueError:\n    if len(tf.config.list_physical_devices('GPU')) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print('Use GPU')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Use CPU')\n        \n\nwiki_embed = load_embed(wiki_path)\nprint(\"\\033[1mWiki Coverage: \\033[0m]\")\noov_wiki = check_voc(vocabulary2, wiki_embed)\n\ncount=0\nwiki_embedding_mat=np.zeros((vocab_size,300))\nfor word,i in tqdm(word_index.items()):\n    try:\n        vec=wiki_embed[word]\n        wiki_embedding_mat[i]=vec\n    except KeyError:\n        count+=1\n        continue\n\nprint(\"Number of Out of Vocabulary\",count)\n\nwith strategy.scope():\n    wiki_model = get_model(wiki_embedding_mat)\n    wiki_model.compile(loss=bin_loss, optimizer=Adam(lr=0.001), metrics=['accuracy'])\nwiki_history=wiki_model.fit(train_padded, train.target, batch_size=BATCH_SIZE, epochs=30, validation_data=(val_padded, val.target),callbacks=my_callbacks)\n\n# summarize history for loss\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 1)\nplt.plot(wiki_history.history['loss'])\nplt.plot(wiki_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n# plt.show()\n# summarize history for accuracy\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 2)\nplt.plot(wiki_history.history['accuracy'])\nplt.plot(wiki_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\nwiki_y_pre=wiki_model.predict(val_padded, verbose=1)\nbest_score = 0\nbest_thresh = 0\nfor thresh in np.arange(0.1,0.5,0.01):\n    if(best_score < metrics.f1_score(val.target,(wiki_y_pre>thresh).astype(int))):\n        best_score = metrics.f1_score(val.target,(wiki_y_pre>thresh))\n        best_thresh = round(thresh, 2)\n    print(\"threshold {0:2.2f} f1 score:{1:2.3f}\".format(thresh,metrics.f1_score(val.target,(wiki_y_pre>thresh).astype(int))))\nprint(\"\\033[1mBest result {0:2.3f} in thresh_hold {1:2.2f}\\033[0m\".format(best_score, best_thresh))\n\nthreshold=best_thresh\nwiki_y_test_pre=wiki_model.predict(test_padded, batch_size=BATCH_SIZE, verbose=1)\nwiki_y_test_pre=(wiki_y_test_pre>thresh).astype(int)\n\ndel wiki_embed, wiki_embedding_mat, wiki_history, best_score, best_thresh\ngc.collect()\n\ny_pre=0.25*(google_y_pre + glove_y_pre + para_y_pre + wiki_y_pre)\n# y_pre=0.20*google_y_pre + 0.35*glove_y_pre + 0.15*para_y_pre + 0.30*wiki_y_pre\nbest_score = 0\nbest_thresh = 0\nfor thresh in np.arange(0.1,0.5,0.01):\n    if(best_score < metrics.f1_score(val.target,(y_pre>thresh).astype(int))):\n        best_score = metrics.f1_score(val.target,(y_pre>thresh))\n        best_thresh = round(thresh, 2)\n    print(\"threshold {0:2.2f} f1 score:{1:2.3f}\".format(thresh,metrics.f1_score(val.target,(y_pre>thresh).astype(int))))\nprint(\"\\033[1mBest result {0:2.3f} in thresh_hold {1:2.2f}\\033[0m\".format(best_score, best_thresh))\n\n# y_test_pre = 0.25 * (google_y_test_pre + glove_y_test_pre + para_y_test_pre + wiki_y_test_pre)\ny_test_pre = 0.2*google_y_test_pre + 0.3*glove_y_test_pre + 0.2*para_y_test_pre + 0.3*wiki_y_test_pre\ny_test_pre=(y_test_pre>thresh).astype(int)\n### T\u1ea1o File submission\nsubmit=pd.DataFrame()\nsubmit[\"qid\"]=df_test.qid\nsubmit[\"prediction\"]=y_test_pre\nsubmit.to_csv(\"submission.csv\",index=False)\nprint(\"ok\")","ab9b4ea7":"V\u1edbi b\u00e0i to\u00e1n li\u00ean quan \u0111\u1ebfn ng\u00f4n ng\u1eef th\u00ec hi\u1ec7u qu\u1ea3 c\u1ee7a m\u00f4 h\u00ecnh ph\u1ee5 thu\u1ed9c l\u1edbn v\u00e0o vi\u1ec7c ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u, t\u1ee9c lo\u1ea1i b\u1ecf \u0111i nhi\u1ec5u \u1edf trong b\u1ed9 d\u1eef li\u1ec7u.<br>\nV\u00e0 ph\u01b0\u01a1ng ph\u00e1p ph\u1ed5 bi\u1ebfn \u0111\u1ed1i v\u1edbi c\u00e1c b\u00e0i to\u00e1n li\u00ean quan \u0111\u1ebfn ng\u00f4n ng\u1eef \u0111\u01b0\u1ee3c tr\u00ecnh b\u00e0y \u1edf d\u01b0\u1edbi nh\u01b0 sau\n\n**C\u00e1ch l\u00e0m:**<br>\nS\u1eed d\u1ee5ng th\u01b0 vi\u1ec7n x\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean nltk trong vi\u1ec7c preprocess data<br>\n\u1ede \u0111\u00e2y y\u00eau c\u1ea7n ph\u1ea3i s\u1eed d\u1ee5ng wordnet, m\u1ed9t th\u01b0 vi\u1ec7n t\u1eeb \u0111\u1ed3ng ngh\u0129a, tr\u00e1i ngh\u0129a, nltk punkt \u0111\u01b0\u1ee3c y\u00eau c\u1ea7u \u0111\u1ec3 tokenize words<br>\n- B\u01b0\u1edbc 1: ```clean_tag()```: lo\u1ea1i b\u1ecf \u0111i c\u00e1c bi\u1ec3u th\u1ee9c to\u00e1n h\u1ecdc, c\u00e1c \u0111\u1ecba ch\u1ec9 li\u00ean k\u1ebft\n- B\u01b0\u1edbc 2: ```clean_puncts()```: lo\u1ea1i b\u1ecf \u0111i c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t c\u00f3 trong c\u00e2u\n- B\u01b0\u1edbc 3: ```correct_misspell()```: trong b\u1ed9 d\u1eef li\u1ec7u c\u1ee7a **Quora** do \u0111\u00e2y l\u00e0 c\u00e1c c\u00e2u h\u1ecfi c\u1ee7a ng\u01b0\u1eddi d\u00f9ng n\u00ean kh\u00f4ng tr\u00e1nh kh\u1ecfi vi\u1ec7c g\u00f5 sai, hay l\u00e0 c\u00f3 nh\u1eefng t\u1eeb kh\u00f4ng chu\u1ea9n v\u00ed d\u1ee5 l\u00e0 l\u1eabn l\u1ed9n gi\u1eefa ti\u1ebfng Anh-Anh v\u1edbi ti\u1ebfng Anh-M\u1ef9 cho n\u00ean c\u1ea7n ph\u1ea3i fix nh\u1eefng t\u1eeb n\u00e0y v\u00e0 \u0111\u01b0a n\u00f3 v\u1ec1 d\u1ea1ng chu\u1ea9n\n- B\u01b0\u1edbc 4: ```remove_stopwords()```: lo\u1ea1i b\u1ecf \u0111i c\u00e1c stopwords c\u00f3 trong c\u00e2u( nh\u1eefng t\u1eeb, ch\u1eef d\u1ea1ng *'do'*, *'does'*, *'did'*, *'should'*, ...)\n- B\u01b0\u1edbc 5: ```clean_contractions()```: chuy\u1ec3n nh\u1eefng t\u1eeb vi\u1ebft t\u1eaft v\u1ec1 d\u1ea1ng \u0111\u1ea7y \u0111\u1ee7 v\u1ed1n c\u00f3\n- B\u01b0\u1edbc 6: ```lemming_words()```: lemming c\u00e1c t\u1eeb v\u1ec1 d\u1ea1ng nguy\u00ean b\u1ea3n c\u1ee7a n\u00f3\n\n\n**V\u1ea5n \u0111\u1ec1:**<br>\nTuy nhi\u00ean \u0111\u1eddi kh\u00f4ng nh\u01b0 l\u00e0 m\u01a1, cu\u1ed9c thi n\u00e0y c\u1ee7a **Quora** kh\u00f4ng cho ph\u00e9p s\u1eed d\u1ee5ng Internet n\u00ean vi\u1ec7c lemming word b\u1eb1ng c\u00e1ch lookup t\u1eeb \u0111i\u1ec3n n\u00e0y s\u1ebd kh\u00f4ng ho\u1ea1t \u0111\u1ed9ng<br>\n**Gi\u1ea3i ph\u00e1p:**<br>\nC\u1ea7n t\u00ecm ra m\u1ed9t ph\u01b0\u01a1ng ph\u00e1p lemming words m\u1edbi","abd5a387":"# **6. So s\u00e1nh, m\u1edf r\u1ed9ng**\nDo s\u1eed d\u1ee5ng 1 pretrained l\u00e0 **Google News** n\u00ean k\u1ebft qu\u1ea3 thu \u0111\u01b0\u1ee3c \u1edf m\u1ee9c l\u00e0 **```0.68x``` (x<=3)** f1_score<br>\n&nbsp;<br>\nB\u1edfi v\u00ec 3 th\u1eb1ng c\u00f2n l\u1ea1i (kh\u00f4ng ph\u1ea3i **Google News**) \u0111\u1ec1u kh\u00f4ng \u1edf d\u1ea1ng binary v\u00e0 kh\u00f4ng ph\u1ea3i \u1edf d\u1ea1ng chu\u1ea9n word_vector n\u00ean kh\u00f4ng \u0111\u1ecdc \u0111\u01b0\u1ee3c b\u1eb1ng KeyedVectors, em s\u1ebd \u0111\u1ecbnh ngh\u0129a m\u1ed9t h\u00e0m \u0111\u1ec3 \u0111\u1ecdc c\u00e1c file embeddings c\u00f2n l\u1ea1i <br>\nTuy nhi\u00ean do t\u1ef1 \u0111\u1ecbnh ngh\u0129a n\u00ean th\u1eddi gian load d\u1eef li\u1ec7u v\u00e0o bi\u1ebfn kh\u00e1 l\u00e0 l\u00e2u","77cdeeaa":"X\u00e1c \u0111\u1ecbnh c\u00e1c t\u1eadp ```train```, ```test``` v\u00e0 kh\u1edfi t\u1ea1o m\u1ed9t t\u1eadp ch\u1ee9a d\u1eef li\u1ec7u c\u1ee7a c\u1ea3 2 \u0111\u1ec3 v\u1ec1 sau check \u0111\u1ed9 bao ph\u1ee7 c\u1ee7a vocab v\u1edbi t\u1eadp d\u1eef li\u1ec7u \u0111\u00e3 sinh\n\n**T\u1eadp ```train```: **\n\u1ede trong b\u1ed9 d\u1eef li\u1ec7u c\u00f3 3 tr\u01b0\u1eddng, v\u00e0 kh\u1ea3 n\u0103ng d\u1eef li\u1ec7u train n\u1eb1m \u1edf tr\u01b0\u1eddng **```question_text```** <br>\nV\u1eady ta s\u1ebd th\u1eed kh\u1ea3o s\u00e1t d\u1eef li\u1ec7u trong tr\u01b0\u1eddng **```question_text```**\nC\u00f3 th\u1ec3 th\u1ea5y r\u1eb1ng t\u1eadp train c\u00f3 3 tr\u01b0\u1eddng d\u1eef li\u1ec7u\n- **```qid```**: \u0111\u1ecbnh danh id cho c\u00e2u h\u1ecfi\n- **```question_text```**: n\u1ed9i dung c\u00e2u h\u1ecfi\n- **```target```**: ph\u00e2n l\u1edbp c\u00e2u h\u1ecfi\n    - 0: c\u00e2u h\u1ecfi mang t\u00ednh ch\u1ea5t ch\u00e2n th\u00e0nh (sincere)\n    - 1: c\u00e2u h\u1ecfi kh\u00f4ng mang t\u00ednh ch\u1ea5t ch\u00e2n th\u00e0nh (insincere)\n    \nKh\u00e1 may m\u1eafn khi t\u1eadp d\u1eef li\u1ec7u **Quora** cung c\u1ea5p kh\u00f4ng c\u00f3 b\u1ea5t k\u00ec tr\u01b0\u1eddng d\u1eef li\u1ec7u n\u00e0o c\u00f3 gi\u00e1 tr\u1ecb b\u1ea5t th\u01b0\u1eddng (```null```, ```none```, ```missing```)\n    \nT\u1eadp d\u1eef li\u1ec7u train bao g\u1ed3m **1306122** d\u00f2ng d\u1eef li\u1ec7u, g\u1ea7n **1.31 tri\u1ec7u d\u00f2ng**, kh\u00e1 l\u1edbn. \n\nC\u00f3 th\u1ec3 th\u1ea5y trong tr\u01b0\u1eddng target c\u00f3 2 gi\u00e1 tr\u1ecb l\u00e0 ```0``` v\u00e0 ```1```<br>\nD\u1ef1 \u0111o\u00e1n th\u00ec nh\u1eefng c\u00e2u h\u1ecfi \u0111\u00e1nh d\u1ea5u ```0``` l\u00e0 nh\u1eefng c\u00e2u h\u1ecfi sincere, t\u01b0\u01a1ng t\u1ef1 ```1``` l\u00e0 insincere<br>\nV\u1eady c\u00f3 th\u1ec3 xem nh\u01b0 \u0111\u00e2y l\u00e0 b\u00e0i to\u00e1n ph\u00e2n l\u1edbp nh\u1ecb ph\u00e2n<br>\nNh\u01b0ng v\u1eabn c\u1ea7n ph\u1ea3i c\u00f3 \u0111\u00e1nh gi\u00e1 s\u00e2u h\u01a1n v\u1ec1 b\u1ed9 d\u1eef li\u1ec7u n\u00e0y, nh\u00ecn nhanh qua c\u00f3 th\u1ec3 th\u1ea5y b\u1ed9 d\u1eef li\u1ec7u train \u0111\u01b0\u1ee3c cho kh\u00e1 l\u00e0 m\u1ea5t c\u00e2n b\u1eb1ng\n\nQua bi\u1ec3u \u0111\u1ed3 c\u00f3 th\u1ec3 d\u1ec5 d\u00e0ng nh\u1eadn th\u1ea5y t\u1eadp d\u1eef li\u1ec7u train b\u1ecb m\u1ea5t c\u00e2n b\u1eb1ng nhi\u1ec1u:\n- L\u1edbp c\u00e2u h\u1ecfi sincere c\u00f3 **1225312** (chi\u1ebfm **93.81%**) s\u1ed1 l\u01b0\u1ee3ng d\u1eef li\u1ec7u\n- L\u1edbp c\u00e2u h\u1ecfi insincere c\u00f3 **80810** (chi\u1ebfm **6.19%**) s\u1ed1 l\u01b0\u1ee3ng d\u1eef li\u1ec7u\n\nC\u00f3 th\u1ec3 th\u1ea5y t\u1eadp d\u1eef li\u1ec7u b\u1ecb l\u1ec7ch nhi\u1ec1u (l\u00ean t\u1edbi **15 l\u1ea7n**) cho n\u00ean c\u1ea7n ph\u1ea3i s\u1eed d\u1ee5ng metrics f1_score \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 \u0111\u1ed9 hi\u1ec7u qu\u1ea3 c\u1ee7a m\u00f4 h\u00ecnh h\u01a1n l\u00e0 so **accuracy**<br>\nDo **f1_score** quan t\u00e2m \u0111\u1ebfn ph\u00e2n b\u1ed1 c\u1ee7a d\u1eef li\u1ec7u n\u00ean trong b\u00e0i n\u00e0y n\u00f3 s\u1ebd mang nhi\u1ec1u \u00fd ngh\u0129a h\u01a1n l\u00e0 **accuracy**.<br>\nV\u00e0 trong cu\u1ed9c thi **Kaggle** c\u0169ng b\u1ea3o l\u00e0 s\u1eed d\u1ee5ng **f1_score** thay v\u00ec **accuracy**\n\n**T\u1eadp ```test```**:\n\u1ede trong b\u1ed9 d\u1eef li\u1ec7u test th\u00ec ch\u1ec9 c\u00f3 2 tr\u01b0\u1eddng d\u1eef li\u1ec7u l\u00e0 qid \u0111\u1ea1i di\u1ec7n cho id c\u1ee7a c\u00e2u h\u1ecfi\nV\u00e0 question_text \u0111\u1ea1i di\u1ec7n cho d\u1eef li\u1ec7u test\nC\u0169ng may m\u1eafn khi m\u00e0 \u1edf trong t\u1eadp df_test l\u1ea1i kh\u00f4ng c\u00f3 d\u1eef li\u1ec7u k\u00ec l\u1ea1 n\u00e0o c\u1ea3 (null, none, missing)\n\nM\u1ed7i l\u1ea7n shuffle l\u1ea1i cho ra m\u1ed9t k\u1ebft qu\u1ea3 kh\u00e1c, nh\u01b0ng nh\u00ecn chung qua v\u00e0i l\u1ea7n th\u1ef1c nghi\u1ec7m quan s\u00e1t, em nh\u1eadn th\u1ea5y trong t\u1eadp d\u1eef li\u1ec7u \u1edf tr\u00ean:\n\nC\u00f3 nhi\u1ec1u c\u00e2u m\u00e0 c\u00e1c t\u1eeb trong \u0111\u00f3 vi\u1ebft sai ch\u00ednh t\u1ea3, l\u1eabn l\u1ed9n gi\u1eefa ti\u1ebfng Anh-Anh v\u00e0 ti\u1ebfng Anh-M\u1ef9\nNhi\u1ec1u c\u00e2u s\u1eed d\u1ee5ng c\u00e1c t\u1eeb vi\u1ebft t\u1eaft: He's m\u00e0 \u0111\u00e1ng nh\u1ebd n\u00ean l\u00e0 He is.\nC\u00f3 m\u1ed9t s\u1ed1 t\u1eeb vi\u1ebft t\u1eaft cho t\u00ean c\u00e1c t\u1ed5 ch\u1ee9c, hay c\u00e1c chu\u1ea9n m\u00e3 ho\u00e1 ki\u1ec3u RSA, DES\/3DES, ...\nNhi\u1ec1u c\u00e2u s\u1eed d\u1ee5ng c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t cho n\u00ean s\u1ebd g\u00e2y \u1ea3nh h\u01b0\u1edfng l\u1edbn t\u1edbi m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n\nV\u1ec1 m\u1eb7t n\u1ed9i dung h\u00e0m \u00fd th\u00ec c\u00f3 th\u1ec3 kh\u00f4ng \u1ea3nh h\u01b0\u1edfng nh\u01b0ng s\u1ebd \u1ea3nh h\u01b0\u1edfng khi x\u00e2p t\u1eadp t\u1eeb \u0111i\u1ec3n, c\u00f9ng m\u1ed9t \u00fd ngh\u0129a nh\u01b0ng c\u00e1c t\u1eeb l\u1ea1i \u0111\u01b0\u1ee3c t\u00ednh nhi\u1ec1u l\u1ea7n gi\u00e1 tr\u1ecb\n\nTh\u1ea5y r\u1eb1ng, b\u1ed9 d\u1eef li\u1ec7u \u0111\u00e3 cho c\u00f3 kh\u00e1 nhi\u1ec1u nhi\u1ec5u => c\u1ea7n lo\u1ea1i b\u1ecf nhi\u1ec1u nhi\u1ec1u nh\u1ea5t c\u00f3 th\u1ec3\n\nC\u00e1ch x\u1eed l\u00fd\n\nLo\u1ea1i b\u1ecf stopwords c\u00f3 trong c\u00e2u\nStem c\u00e1c t\u1eeb c\u00f3 trong c\u00e2u (driver, driving, driven, drove -> drive)\nChu\u1ea9n ho\u00e1 c\u00e1c t\u1eeb v\u1ec1 d\u1ea1ng lowercase\nLo\u1ea1i b\u1ecf c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t\nLo\u1ea1i b\u1ecf c\u00e1c \u0111\u01b0\u1eddng d\u1eabn li\u00ean k\u1ebft, lo\u1ea1i b\u1ecf c\u00e1c c\u00f4ng th\u1ee9c to\u00e1n h\u1ecdc, ...\n\n### **Nh\u1eadn x\u00e9t**\nNh\u01b0ng c\u00f3 th\u1ec3 th\u1ea5y m\u1ed9t \u0111i\u1ec1u l\u00e0 b\u1ed9 d\u1eef li\u1ec7u **train** v\u00e0 **test** d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o \u0111\u1ec1u \u1edf d\u1ea1ng plain text v\u00e0 m\u00f4 h\u00ecnh c\u1ee7a ch\u00fang ta s\u1ebd kh\u00f4ng th\u1ec3 hi\u1ec3u n\u1ed5i<br>\nV\u1eady h\u01b0\u1edbng gi\u1ea3i quy\u1ebft \u1edf \u0111\u00e2y em ngh\u0129 l\u00e0 l\u00e0 s\u1eed d\u1ee5ng m\u00e3 ho\u00e1 **one_hot** \u0111\u01b0a d\u1eef li\u1ec7u v\u1ec1 d\u1ea1ng binary \u0111\u1ec3 cho m\u00f4 h\u00ecnh c\u00f3 th\u1ec3 hi\u1ec3u \u0111\u01b0\u1ee3c","a910febd":"# **1. Kh\u1ea3o s\u00e1t d\u1eef li\u1ec7u**","a8421130":"**M\u00f4 h\u00ecnh LSTM**\n![lstm](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-chain.png)\n\n> LSTM c\u0169ng c\u00f3 ki\u1ebfn tr\u00fac d\u1ea1ng chu\u1ed7i nh\u01b0 v\u1eady, nh\u01b0ng c\u00e1c m\u00f4-\u0111un trong n\u00f3 c\u00f3 c\u1ea5u tr\u00fac kh\u00e1c v\u1edbi m\u1ea1ng RNN chu\u1ea9n. Thay v\u00ec ch\u1ec9 c\u00f3 m\u1ed9t t\u1ea7ng m\u1ea1ng n\u01a1-ron, ch\u00fang c\u00f3 t\u1edbi 4 t\u1ea7ng t\u01b0\u01a1ng t\u00e1c v\u1edbi nhau m\u1ed9t c\u00e1ch r\u1ea5t \u0111\u1eb7c bi\u1ec7t\n\nS\u1eed d\u1ee5ng embedding layer, m\u1ee5c \u0111\u00edch l\u00e0 \u0111\u1ec3 embedding sang m\u1ed9t kh\u00f4ng gian m\u1edbi c\u00f3 chi\u1ec1u nh\u1ecf h\u01a1n, gi\u1ea3m chi\u1ec1u d\u1eef li\u1ec7u<br>\nBidirectional(LSTM) \u0111\u1ec3 x\u00e2y model LSTM<br>\nLSTM c\u0169ng l\u00e0 m\u1ea1ng CNN n\u00ean c\u1ea7n qua 2 l\u1edbp l\u00e0 Convo1D v\u00e0 Pool1D (convolution v\u00e0 pooling)","7624be24":"# **2. Ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u**","4e7a510d":"\u1ede \u0111\u00e2y em s\u1ebd c\u1ed1 \u0111\u1ecbnh s\u1eed d\u1ee5ng th\u1eb1ng **Google News** l\u00e0m ti\u00eau chu\u1ea9n<br>\nKi\u1ec3m tra word embeddings \u0111\u00e3 gi\u1ea3i n\u00e9n v\u1edbi **Google News** \u0111\u00ednh k\u00e8m trong file embeddings.zip<br>\n\u0110\u1ecbnh ngh\u0129a c\u00e1c h\u00e0m \u0111\u1ec3 build vocab t\u1eeb file **Google News** v\u00e0 h\u00e0m ki\u1ec3m tra \u0111\u1ed9 bao ph\u1ee7 c\u1ee7a vocab \u0111\u1ed1i v\u1edbi t\u1eadp d\u1eef li\u1ec7u \u0111\u00e3 \u0111\u01b0\u1ee3c preprocess<br>\n&nbsp;<br>\nTrong b\u00e0i n\u00e0y, em s\u1ebd gi\u1ea3i n\u00e9n t\u1ec7p embedding \u0111\u1ec3 s\u1eed d\u1ee5ng t\u1eadp c\u00e1c vector v\u1edbi c\u00e1c t\u1eeb cho s\u1eb5n<br>\nC\u00e1c t\u1eeb trong file embeddings \u0111\u1ec1u \u0111\u00ednh k\u00e8m m\u1ed9t vector, v\u00e0 em s\u1ebd x\u00e2y ma tr\u1eadn embeddings d\u1ef1a v\u00e0o file embeddings \u0111\u00f3\n\nFile **Google News** c\u00f3 k\u00edch th\u01b0\u1edbc kh\u00e1 nh\u1ecf: ```3.4G``` nh\u01b0ng n\u00f3 l\u1ea1i \u1edf d\u1ea1ng binary c\u1ee7a word embeddings n\u00ean \u0111\u00e3 c\u00f3 th\u01b0 vi\u1ec7n KeyedVector h\u1ed7 tr\u1ee3 v\u00e0 vi\u1ec7c load kh\u00e1 nhanh n\u00ean em s\u1ebd s\u1eed d\u1ee5ng th\u1eb1ng **Google News** n\u00e0y l\u00e0m tham chi\u1ebfu cho c\u00e1c embeddings kh\u00e1c trong t\u01b0\u01a1ng lai<br>\n\u1ede d\u01b0\u1edbi em s\u1ebd \u0111\u1ecbnh ngh\u0129a c\u00e1c h\u00e0m build ```vocab``` v\u00e0 check \u0111\u1ed9 ph\u1ee7<br>\nDo embeddings l\u00e0 file c\u00f3 word \u0111\u00ednh k\u00e8m vector m\u00f4 t\u1ea3 cho n\u00f3 n\u00ean em s\u1ebd lookup c\u00e1c t\u1eeb c\u00f3 trong c\u00e2u v\u1edbi file embeddings v\u00e0 t\u00ecm vector c\u1ee7a ch\u00fang\n\nX\u00e2y d\u1ef1ng t\u1eadp vocab v\u00e0 ki\u1ec3m tra \u0111\u1ed9 bao ph\u1ee7 c\u1ee7a t\u1eadp vocab v\u1edbi t\u1eadp vocab c\u1ee7a pretrained model t\u1eeb Google News<br>\nTrong block code n\u00e0y em s\u1ebd x\u00e2y t\u1eadp vocab d\u1ef1a tr\u00ean b\u1ed9 d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c x\u1eed l\u00fd v\u1edbi h\u00e0m preprocess th\u1ee9 nh\u1ea5t l\u00e0 ```data_clean()```\n\nV\u1edbi h\u00e0m preprocess th\u1ee9 nh\u1ea5t l\u00e0 ```data_clean()``` th\u00ec th\u1ea7y c\u00f3 th\u1ec3 th\u1ea5y k\u1ebft qu\u1ea3 \u0111\u1ed9 bao ph\u1ee7 \u0111\u1ea1t **81.04%**, h\u01a1i th\u1ea5p m\u1ed9t ch\u00fat d\u00f9 d\u1eef li\u1ec7u \u0111\u00e3 \u0111\u01b0\u1ee3c x\u1eed l\u00fd t\u1eeb tr\u01b0\u1edbc<br>\nNh\u01b0ng t\u1ec7 h\u1ea1i l\u00e0 t\u1eadp vocab ch\u1ec9 bao ph\u1ee7 **25.40%** corpus<br>\n**Nh\u1eadn x\u00e9t:**<br>\nC\u00f3 th\u1ec3 l\u00e0 do vi\u1ec7c \u0111\u1ee5ng ch\u1ea1m t\u1edbi nhi\u1ec1u t\u1eeb trong c\u00e2u khi lo\u1ea1i b\u1ecf n\u00f3, v\u00e0 s\u1eeda l\u1ed7i sai ch\u00ednh t\u1ea3. \u0110i\u1ec1u n\u00e0y \u0111\u00e3 khi\u1ebfn gi\u1ea3m \u0111i \u0111\u00e1ng k\u1ec3 s\u1ed1 l\u01b0\u1ee3ng c\u00e1c t\u1eeb \u0111\u01b0\u1ee3c t\u00ecm th\u1ea5y trong file pretrained\\\n**Gi\u1ea3i ph\u00e1p:**<br>\nS\u1eed d\u1ee5ng m\u1ed9t h\u00e0m Preprocess m\u1edbi m\u00e0 \u00edt \u0111\u1ee5ng ch\u1ea1m t\u1edbi c\u00e1c t\u1eeb, ch\u1ec9 x\u1eed l\u00fd m\u1ed9t c\u00e1ch \u0111\u01a1n gi\u1ea3n c\u00e1c c\u00e2u\n\nTrong block code n\u00e0y, em s\u1ebd s\u1eed d\u1ee5ng h\u00e0m Preprocess th\u1ee9 2 (h\u00e0m process \u0111\u01a1n gi\u1ea3n h\u01a1n)\nNh\u01b0 th\u1ea7y c\u00f3 th\u1ec3 th\u1ea5y hi\u1ec7u qu\u1ea3 \u0111\u01b0\u1ee3c c\u1ea3i thi\u1ec7n m\u1ed9t c\u00e1ch r\u00f5 r\u1ec7t:\n- \u0110\u1ed9 ph\u1ee7 c\u1ee7a vocab v\u1edbi corpus t\u0103ng g\u1ea5p **2.5 l\u1ea7n** l\u00ean **61.37%**\n- \u0110\u1ed9 ph\u1ee7 t\u0103ng l\u00ean **90.81%**, m\u1ed9t con s\u1ed1 kh\u00e1 \u1ea5n t\u01b0\u1ee3ng\n\nN\u1ebfu nh\u01b0 chuy\u1ec3n c\u00e1c t\u1eeb v\u1ec1 d\u1ea1ng ```lowercase``` th\u00ec 2 con s\u1ed1 t\u01b0\u01a1ng \u0111\u01b0\u01a1ng l\u00e0 **38.85%** v\u1edbi corpus v\u00e0 **89.47%** v\u1edbi vocab.<br>\nNg\u01b0\u1eddi Anh \u0111\u1eb7t c\u00e1i t\u00f4i c\u1ee7a h\u1ecd r\u1ea5t cao, n\u00ean trong t\u1ea5t c\u1ea3 c\u00e1c c\u00e2u v\u0103n c\u1ee7a h\u1ecd th\u00ec t\u1eeb 'I' (ch\u1ec9 b\u1ea3n th\u00e2n ng\u01b0\u1eddi n\u00f3i) lu\u00f4n \u0111\u01b0\u1ee3c \u0111\u1eb7t \u1edf tr\u1ea1ng th\u00e1i in hoa ```uppercase```n\u00ean n\u1ebfu ch\u00fang ta chu\u1ea9n ho\u00e1 c\u00e1c t\u1eeb v\u1ec1 ```lowercase``` th\u00ec s\u1ebd kh\u00f4ng lookup \u0111\u01b0\u1ee3c t\u1eeb \u0111\u00f3 trong file pretrained embeddings v\u00e0 s\u1ebd g\u00e2y ra hi\u1ec7n t\u01b0\u1ee3ng thi\u1ebfu s\u00f3t trong ma tr\u1eadn embeddings m\u00e0 ch\u00fang ta chu\u1ea9n b\u1ecb x\u00e2y d\u1ef1ng\n\nT\u1eadp pretrained **Google News** d\u00f9 \u0111\u1ed9 ph\u1ee7 kh\u00e1 cao nh\u01b0ng s\u1ed1 l\u01b0\u1ee3ng t\u1eeb n\u1eb1m ngo\u00e0i t\u1eadp embeddings kh\u00e1 l\u00e0 l\u1edbn khi l\u00ean t\u1edbi 99931 t\u1eeb<br>\nV\u00e0 c\u1ea7n ph\u1ea3i c\u00f3 s\u1ef1 c\u1ea3i thi\u1ec7n v\u1ec1 s\u1ed1 l\u01b0\u1ee3ng t\u1eeb ```out_of_vocab``` n\u1ebfu kh\u00f4ng th\u00ec hi\u1ec7u qu\u1ea3 c\u1ee7a model s\u1ebd k\u00e9m.","a64fd2a1":"# **B\u00e1o c\u00e1o b\u00e0i t\u1eadp l\u1edbn m\u00f4n H\u1ecdc m\u00e1y: Quora Insincere Questions Classification**\n**Sinh vi\u00ean:** Phan H\u1ea3i Anh<br>\n**MSSV:** 19021213<br>","09f245e9":"# **M\u00f4 t\u1ea3 b\u00e0i to\u00e1n**\n\n**Quora Insincere Question Classification** l\u00e0 m\u1ed9t b\u00e0i to\u00e1n c\u1ee7a **Quora** \u0111\u1eb7t ra, s\u1eed d\u1ee5ng s\u1ef1 tr\u1ee3 gi\u00fap t\u1eeb c\u1ed9ng \u0111\u1ed3ng, gi\u00fap h\u1ecd ph\u00e2n lo\u1ea1i nh\u1eefng c\u00e2u h\u1ecfi kh\u00f4ng ch\u00e2n th\u00e0nh. Nhi\u1ec7m v\u1ee5 c\u1ee7a b\u00e0i to\u00e1n l\u00e0 s\u1eed d\u1ee5ng t\u1eadp d\u1eef li\u1ec7u m\u00e0 **Quora** cung c\u1ea5p \u0111\u1ec3 ph\u00e2n lo\u1ea1i ra nh\u1eefng c\u00e2u h\u1ecfi mang h\u00e0m \u00fd kh\u00f4ng ch\u00e2n th\u00e0nh, mang n\u1ed9i dung x\u1ea5u \u0111\u1ed9c, g\u00e2y hi\u1ec3u l\u1ea7m.\n\n- **\u0110\u1ea7u v\u00e0o**: C\u00e2u h\u1ecfi d\u01b0\u1edbi d\u1ea1ng v\u0103n b\u1ea3n (plain text)\n- **\u0110\u1ea7u ra**: Yes\/No (insincere (1) or not (0))\n\n- Ti\u1ec1n x\u1eed l\u00fd b\u1ed9 d\u1eef li\u1ec7u\n    - B\u1ecf \u0111i c\u00e1c bi\u1ec3u th\u1ee9c to\u00e1n h\u1ecdc v\u00e0 c\u00e1c \u0111\u01b0\u1eddng d\u1eabn li\u00ean k\u1ebft\n    - B\u1ecf \u0111i c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t, c\u00e1c ch\u1eef s\u1ed1\n    - S\u1eeda nh\u1eefng t\u1eeb sai ch\u00ednh t\u1ea3 v\u00e0 extend c\u00e1c t\u1eeb vi\u1ebft t\u1eaft\n    - B\u1ecf \u0111i stop_words trong c\u00e2u\n    - Lemming c\u00e1c t\u1eeb c\u00f3 trong c\u00e2u (c\u00f3 th\u1ec3 hi\u1ec3u l\u00e0 chuy\u1ec3n c\u00e1c t\u1eeb v\u1ec1 d\u1ea1ng nguy\u00ean th\u1ec3)\n    \n> **M\u1ee5c \u0111\u00edch**: Lo\u1ea1i b\u1ecf \u0111i c\u00e1c t\u1eeb g\u00e2y nhi\u1ec5u cho b\u1ed9 d\u1eef li\u1ec7u v\u00e0 m\u1edf r\u1ed9ng \u0111\u1ed9 bao ph\u1ee7 c\u1ee7a t\u1eadp Vocab v\u1edbi b\u1ed9 d\u1eef li\u1ec7u\n- X\u00e2y d\u1ef1ng t\u1eadp vocab: \n    - S\u1eed d\u1ee5ng c\u00e1c file trong embeddings.zip m\u00e0 quora cung c\u1ea5p \u0111\u1ec3 x\u00e2y d\u1ef1ng t\u1eadp vocab \n    - C\u00e1c file embeddings c\u00f3 d\u1ea1ng text, m\u1ed7i d\u00f2ng ch\u1ee9a 1 word v\u00e0 \u0111i k\u00e8m \u0111\u00f3 l\u00e0 vector c\u1ee7a n\u00f3 \u0111\u00e3 \u0111\u01b0\u1ee3c pretrained\n    - X\u00e2y t\u1eadp vocab t\u1eeb c\u00e1c word c\u00f3 trong t\u1eeb \u0111i\u1ec3n v\u00e0 vector \u0111i k\u00e8m v\u1edbi ch\u00fang\n    - Ki\u1ec3m tra \u0111\u1ed9 ph\u1ee7 c\u1ee7a vocab x\u00e2y d\u1ef1ng d\u1ef1a tr\u00ean t\u1eadp embeddings v\u1edbi d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c cho\n- X\u00e2y d\u1ef1ng ma tr\u1eadn embeddings:\n    - S\u1eed d\u1ee5ng file pretrained \u0111\u1ec3 t\u1ea1o ra ma tr\u1eadn embeddings d\u1ef1a v\u00e0o c\u00e1c t\u1eeb c\u00f3 trong t\u1eadp vocab\n    - L\u1ea5y c\u00e1c t\u1eeb \u0111\u1ec3 lookup vector descriptor c\u1ee7a c\u00e1c t\u1eeb \u0111\u00f3 v\u00e0 x\u00e2y ma tr\u1eadn\n- T\u1ea1o model v\u00e0 hu\u1ea5n luy\u1ec7n d\u1ef1 \u0111o\u00e1n\n- K\u1ebft h\u1ee3p k\u1ebft qu\u1ea3 c\u1ee7a nhi\u1ec1u model v\u1edbi c\u00e1c ma tr\u1eadn embeddings \u0111\u00e3 x\u00e2y d\u1ef1ng v\u00e0 cho ra k\u1ebft qu\u1ea3 t\u1ed1t nh\u1ea5t","079a265f":"# **3. Build t\u1eadp vocab**","c850de90":"# **4. Chu\u1ea9n b\u1ecb Model**","8e9a9627":"### **a. Th\u1eed nghi\u1ec7m model 1**\nCompile model v\u1edbi 30 epoch cho m\u00e1u, batch_size \u0111\u1ec3 \u1edf m\u1ee9c cao \u0111\u1ec3 t\u1eadn d\u1ee5ng s\u1ee9c m\u1ea1nh t\u00ednh to\u00e1n c\u1ee7a TPU v\u00e0 s\u1eed d\u1ee5ng v\u1edbi t\u1eadp validation \u0111\u1ec3 validate d\u1eef li\u1ec7u, truy\u1ec1n v\u00e0o tham s\u1ed1 callback \u0111\u1ec3 m\u00f4 h\u00ecnh c\u00e0ng h\u1ecdc c\u00e0ng kh\u00f4n ch\u1ee9 kh\u00f4ng \u0111\u01b0\u1ee3c ngu \u0111i<br>\nM\u1ed9t ph\u1ea7n do em c\u00f3 c\u1eadu b\u1ea1n c\u0169ng train v\u1edbi b\u1ecdn Keras nh\u01b0ng khi f1 score \u0111\u1ea1t \u0111\u01b0\u1ee3c t\u1ed1t r\u1ed3i th\u00ec n\u00f3 b\u1eaft \u0111\u1ea7u tri\u1ec7u ch\u1ee9ng ngu \u0111i khi m\u00e0 loss t\u1eb1ng (overfitting)\n\nIn ra f1 score, ```f1_score``` t\u1ed1t nh\u1ea5t \u1edf \u0111\u00e2y l\u00e0 ```0.673``` t\u1ee9c ```67.3%``` \u1ee9ng v\u1edbi h\u00e0m Preprocess th\u1ee9 2 t\u1ee9c l\u00e0 kh\u00f4ng \u0111\u1ee5ng ch\u1ea1m nhi\u1ec1u \u0111\u1ebfn c\u00e2u h\u1ecfi<br>\nT\u01b0\u01a1ng \u1ee9ng l\u00e0 f1_score = ```0.633``` t\u1ee9c ```63.3%``` \u0111\u1ed1i v\u1edbi h\u00e0m Preprocess \u0111\u1ea7u ti\u00ean khi lo\u1ea1i b\u1ecf ```stop_words```, ```lemma_words```, ...\nNh\u1eadn x\u00e9t:\n- V\u1edbi c\u00e1ch ti\u1ebfp c\u1eadn d\u1ef1a tr\u00ean pretrained th\u00ec vi\u1ec7c ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u qu\u00e1 nhi\u1ec1u th\u1ef1c s\u1ef1 c\u00f3 \u1ea3nh h\u01b0\u1edfng l\u1edbn t\u1edbi hi\u1ec7u n\u0103ng c\u1ee7a model khi m\u00e0 n\u00f3 l\u00e0m m\u1ea5t \u0111i ph\u1ea7n nhi\u1ec1u \u00fd ngh\u0129a c\u00e1c c\u00e1c t\u1eeb c\u00f3 trong c\u00e2u h\u1ecfi\n- C\u00f3 th\u1ec3 do c\u00e1ch c\u00e0i \u0111\u1eb7t kh\u00e1c nhau nh\u01b0ng v\u1ec1 c\u01a1 b\u1ea3n v\u1edbi c\u00e1ch ti\u1ebfp c\u1eadn d\u1ef1a v\u00e0o pretrained th\u00ec ta kh\u00f4ng n\u00ean x\u1eed l\u00fd nhi\u1ec5u c\u1ee7a d\u1eef li\u1ec7u nhi\u1ec1u, c\u00f3 ch\u0103ng l\u00e0 lo\u1ea1i b\u1ecf \u0111i c\u00e1c digits, c\u00e1c c\u00f4ng th\u1ee9c to\u00e1n v\u00e0 c\u00e1c \u0111\u01b0\u1eddng d\u1eabn li\u00ean k\u1ebft \n\nC\u00f3 th\u1ec3 th\u1ea5y \u0111\u01b0\u1ee3c r\u1eb1ng m\u00f4 h\u00ecnh ho\u1ea1t \u0111\u1ed9ng kh\u00e1 hi\u1ec7u qu\u1ea3 v\u1edbi t\u1eadp **```train```** khi m\u00e0 ```loss``` gi\u1ea3m \u0111\u1ec1u v\u00e0 ```accuracy``` t\u0103ng \u0111\u1ec1u qua c\u00e1c epoch<br>\nTuy nhi\u00ean th\u00ec v\u1edbi t\u1eadp **```test```** l\u1ea1i ho\u1ea1t \u0111\u1ed9ng kh\u00f4ng \u0111\u01b0\u1ee3c hi\u1ec7u qu\u1ea3 khi ```loss``` v\u00e0 ```accuracy``` c\u00f3 h\u00ecnh d\u1ea1ng tr\u1ed3i s\u1ee5t nh\u00ecn nh\u01b0 s\u00f3ng bi\u1ec3n. V\u00e0 t\u1ec7 h\u01a1n l\u00e0 v\u1edbi t\u1eadp **```test```** th\u00ec ```loss``` v\u1ec1 c\u00e1c epoch cu\u1ed1i l\u1ea1i t\u0103ng kh\u00e1 cao.<br>\nV\u1eady th\u00ec c\u00f3 th\u1ec3 k\u1ebft lu\u1eadn l\u00e0 do **model** \u0111\u00e3 x\u00e2y d\u1ef1ng **ho\u1ea1t \u0111\u1ed9ng ch\u01b0a hi\u1ec7u qu\u1ea3**, c\u1ea7n ph\u1ea3i xem x\u00e9t l\u1ea1i<br>","7c4e5a46":"# **5. Hu\u1ea5n luy\u1ec7n v\u00e0 D\u1ef1 \u0111o\u00e1n**"}}