{"cell_type":{"e8826bfb":"code","18d1cc16":"code","03ad9f2c":"code","1ea44558":"code","16d5788d":"code","ec332808":"code","661c2eeb":"code","45676edd":"code","bdfb87a1":"code","7d7e74b1":"code","9de75910":"code","8420014b":"code","efa55e93":"code","72dce9ca":"code","df7cec2e":"code","90cc06b7":"code","f174e994":"code","d7a39cc9":"code","706653a1":"code","51696cd0":"code","a3092b61":"code","71a04009":"code","7d0203e0":"code","18a40f08":"code","5b6f8e1c":"code","cdf7e3df":"code","e9a9fc81":"code","661df169":"code","2b82672d":"code","9344d173":"code","9abaf6e1":"code","c971c598":"code","c4a89f90":"code","ecd0c159":"code","64799bc5":"code","df7cfbc5":"code","bf9faeed":"code","ad5adc15":"code","14fe7112":"code","b7fa6e11":"code","eefd5e0e":"markdown","80f5a6af":"markdown","838e11ac":"markdown","1eb58cab":"markdown","e9c633b3":"markdown","f966ab44":"markdown","ab5c4114":"markdown","29db4ab6":"markdown","2810240d":"markdown"},"source":{"e8826bfb":"import numpy as np\nfrom numpy.random import seed\nfrom numpy.random import randn\n\nimport pandas as pd\n\nfrom sklearn import linear_model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.graphics.gofplots import qqplot\nimport statsmodels.api as sm\n\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\nplt.style.use('seaborn-whitegrid')\n\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\nimport math\nget_ipython().run_line_magic('config', 'IPCompleter.greedy=True')\n\nimport csv\nimport time\n\nimport scipy.stats as stats\n\n\n\n\nitems = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nsales_train = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\nshops = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\ntest=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")\nitems_categories=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\nsample_submission=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")\n\n\nitems.head(10)","18d1cc16":"sales_train.info()","03ad9f2c":"test.info()","1ea44558":"shops.info()","16d5788d":"sales_train[sales_train.isnull().any(axis=1)].head() ","ec332808":"test[test.isnull().any(axis=1)].head(10)","661c2eeb":"sample_submission[sample_submission.isnull().any(axis=1)].head()","45676edd":"plt.plot(sales_train['item_id'], sales_train['item_price'], 'o', color='blue');","bdfb87a1":"sales_train[sales_train.item_price > 250000]","7d7e74b1":"items_categories[items_categories.item_category_id == 65]","9de75910":"shops[shops.shop_id == 12]","8420014b":"sales_train_sub = sales_train\nsales_train_sub['date'] =  pd.to_datetime(sales_train_sub['date'],format= '%d.%m.%Y')\nsales_train_sub['month'] = pd.DatetimeIndex(sales_train_sub['date']).month\nsales_train_sub['year'] = pd.DatetimeIndex(sales_train_sub['date']).year\nsales_train_sub = sales_train_sub.iloc[:,1:8]\nsales_train_sub.head(10)\n","efa55e93":"items","72dce9ca":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport time\nimport sys\nimport gc\nimport pickle\nsys.version_info","df7cec2e":"sales_train.head()","90cc06b7":"print('train size, item in train, shop in train', sales_train.shape[0], sales_train.item_id.nunique(), sales_train.shop_id.nunique())\nprint('train size, item in train, shop in train', test.shape[0], test.item_id.nunique(),test.shop_id.nunique())\nprint('new items:', len(list(set(test.item_id) - set(test.item_id).intersection(set(sales_train.item_id)))), len(list(set(test.item_id))), len(test))","f174e994":"sales_train.isnull().sum()","d7a39cc9":"sale_by_month = sales_train.groupby('date_block_num')['item_cnt_day'].sum()\nsale_by_month.plot()","706653a1":"block_item_shop_sale = sales_train.groupby(['date_block_num','item_id','shop_id'])['item_cnt_day'].sum()\nblock_item_shop_sale.clip(0,20).plot.hist(bins=20)","51696cd0":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=sales_train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(sales_train.item_price.min(), sales_train.item_price.max()*1.1)\nsns.boxplot(x=sales_train.item_price)\n\nsales_train = sales_train[sales_train.item_price<100000]\nsales_train = sales_train[sales_train.item_cnt_day<1001]","a3092b61":"median = sales_train[(sales_train.shop_id==32)&(sales_train.item_id==2973)&(sales_train.date_block_num==4)&(sales_train.item_price>0)].item_price.median()\nsales_train.loc[sales_train.item_price<0, 'item_price'] = median","71a04009":"\nsales_train.loc[sales_train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n\nsales_train.loc[sales_train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n\nsales_train.loc[sales_train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","7d0203e0":"shops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]\n\nitems_categories['split'] = items_categories['item_category_name'].str.split('-')\nitems_categories['type'] = items_categories['split'].map(lambda x: x[0].strip())\nitems_categories['type_code'] = LabelEncoder().fit_transform(items_categories['type'])\n# if subtype is nan then type\nitems_categories['subtype'] = items_categories['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\nitems_categories['subtype_code'] = LabelEncoder().fit_transform(items_categories['subtype'])\nitems_categories =items_categories[['item_category_id','type_code', 'subtype_code']]\n\nitems.drop(['item_name'], axis=1, inplace=True)","18a40f08":"ts = time.time()\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = sales_train[sales_train.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)\ntime.time() - ts","5b6f8e1c":"sales_train['revenue'] = sales_train['item_price'] *  sales_train['item_cnt_day']","cdf7e3df":"ts = time.time()\ngroup = sales_train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float32))\ntime.time() - ts","e9a9fc81":"test['date_block_num'] = 36\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","661df169":"ts = time.time()\nmatrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 36 ay\ntime.time() - ts","2b82672d":"ts = time.time()\nmatrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, items_categories, on=['item_category_id'], how='left')\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\ntime.time() - ts","9344d173":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","9abaf6e1":"ts = time.time()\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')\ntime.time() - ts","c971c598":"\ndef add_group_stats(matrix_, groupby_feats, target, enc_feat, last_periods):\n    if not 'date_block_num' in groupby_feats:\n        print ('date_block_num must in groupby_feats')\n        return matrix_\n    \n    group = matrix_.groupby(groupby_feats)[target].sum().reset_index()\n    max_lags = np.max(last_periods)\n    for i in range(1,max_lags+1):\n        shifted = group[groupby_feats+[target]].copy(deep=True)\n        shifted['date_block_num'] += i\n        shifted.rename({target:target+'_lag_'+str(i)},axis=1,inplace=True)\n        group = group.merge(shifted, on=groupby_feats, how='left')\n    group.fillna(0,inplace=True)\n    for period in last_periods:\n        lag_feats = [target+'_lag_'+str(lag) for lag in np.arange(1,period+1)]\n        # we do not use mean and svd directly because we want to include months with sales = 0\n        mean = group[lag_feats].sum(axis=1)\/float(period)\n        mean2 = (group[lag_feats]**2).sum(axis=1)\/float(period)\n        group[enc_feat+'_avg_sale_last_'+str(period)] = mean\n        group[enc_feat+'_std_sale_last_'+str(period)] = (mean2 - mean**2).apply(np.sqrt)\n        group[enc_feat+'_std_sale_last_'+str(period)].replace(np.inf,0,inplace=True)\n        # divide by mean, this scales the features for NN\n        group[enc_feat+'_avg_sale_last_'+str(period)] \/= group[enc_feat+'_avg_sale_last_'+str(period)].mean()\n        group[enc_feat+'_std_sale_last_'+str(period)] \/= group[enc_feat+'_std_sale_last_'+str(period)].mean()\n    cols = groupby_feats + [f_ for f_ in group.columns.values if f_.find('_sale_last_')>=0]\n    matrix = matrix_.merge(group[cols], on=groupby_feats, how='left')\n    return matrix","c4a89f90":"ts = time.time()\nmatrix = add_group_stats(matrix, ['date_block_num', 'item_id'], 'item_cnt_month', 'item', [6,12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'shop_id'], 'item_cnt_month', 'shop', [6,12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'item_category_id'], 'item_cnt_month', 'category', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'city_code'], 'item_cnt_month', 'city', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'type_code'], 'item_cnt_month', 'type', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'subtype_code'], 'item_cnt_month', 'subtype', [12])\ntime.time() - ts","ecd0c159":"#first use target encoding each group, then shift month to creat lag features\ndef target_encoding(matrix_, groupby_feats, target, enc_feat, lags):\n    print ('target encoding for',groupby_feats)\n    group = matrix_.groupby(groupby_feats).agg({target:'mean'})\n    group.columns = [enc_feat]\n    group.reset_index(inplace=True)\n    matrix = matrix_.merge(group, on=groupby_feats, how='left')\n    matrix[enc_feat] = matrix[enc_feat].astype(np.float16)\n    matrix = lag_feature(matrix, lags, enc_feat)\n    matrix.drop(enc_feat, axis=1, inplace=True)\n    return matrix","64799bc5":"ts = time.time()\nmatrix = target_encoding(matrix, ['date_block_num'], 'item_cnt_month', 'date_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_id'], 'item_cnt_month', 'date_item_avg_item_cnt', [1,2,3,6,12])\nmatrix = target_encoding(matrix, ['date_block_num', 'shop_id'], 'item_cnt_month', 'date_shop_avg_item_cnt', [1,2,3,6,12])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_category_id'], 'item_cnt_month', 'date_cat_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'shop_id', 'item_category_id'], 'item_cnt_month', 'date_shop_cat_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'city_code'], 'item_cnt_month', 'date_city_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_id', 'city_code'], 'item_cnt_month', 'date_item_city_avg_item_cnt', [1])\ntime.time() - ts","df7cfbc5":"ts = time.time()\ngroup = sales_train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) \/ matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'delta_revenue')\n\nmatrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\ntime.time() - ts","bf9faeed":"matrix['month'] = matrix['date_block_num'] % 12\nmatrix['year'] = (matrix['date_block_num'] \/ 12).astype(np.int8)","ad5adc15":"#Month since last sale for each shop\/item pair.\nts = time.time()\nlast_sale = pd.DataFrame()\nfor month in range(1,35):    \n    last_month = matrix.loc[(matrix['date_block_num']<month)&(matrix['item_cnt_month']>0)].groupby(['item_id','shop_id'])['date_block_num'].max()\n    df = pd.DataFrame({'date_block_num':np.ones([last_month.shape[0],])*month,\n                       'item_id': last_month.index.get_level_values(0).values,\n                       'shop_id': last_month.index.get_level_values(1).values,\n                       'item_shop_last_sale': last_month.values})\n    last_sale = last_sale.append(df)\nlast_sale['date_block_num'] = last_sale['date_block_num'].astype(np.int8)\n\nmatrix = matrix.merge(last_sale, on=['date_block_num','item_id','shop_id'], how='left')\ntime.time() - ts","14fe7112":"ts = time.time()\nlast_sale = pd.DataFrame()\nfor month in range(1,35):    \n    last_month = matrix.loc[(matrix['date_block_num']<month)&(matrix['item_cnt_month']>0)].groupby('item_id')['date_block_num'].max()\n    df = pd.DataFrame({'date_block_num':np.ones([last_month.shape[0],])*month,\n                       'item_id': last_month.index.values,\n                       'item_last_sale': last_month.values})\n    last_sale = last_sale.append(df)\nlast_sale['date_block_num'] = last_sale['date_block_num'].astype(np.int8)\n\nmatrix = matrix.merge(last_sale, on=['date_block_num','item_id'], how='left')\ntime.time() - ts","b7fa6e11":"ts = time.time()\nmatrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmatrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\ntime.time() - ts","eefd5e0e":"# e\u011fitim ve test setini d\u00fczenle","80f5a6af":"# Deneme k\u00fcmesi","838e11ac":"# Son ve ilk sat\u0131\u015ftan bu zamana kadar ay ekle","1eb58cab":"# Son 12 ay i\u00e7inde belirli gruplar\u0131n sat\u0131\u015flar\u0131n\u0131n istatistiklerini olu\u015fturma","e9c633b3":" Her ma\u011faza\/\u00fcr\u00fcn \u00e7ifti ve sadece \u00fcr\u00fcn i\u00e7in ilk sat\u0131\u015ftan bu yana ge\u00e7en ay","f966ab44":"# Ayl\u0131k sat\u0131\u015f","ab5c4114":"# Ge\u00e7en ay ma\u011faza gelir e\u011filimi","29db4ab6":"# ayk\u0131r\u0131 de\u011ferleri silindi\n","2810240d":"# Her bir \u00fcr\u00fcn i\u00e7in son sat\u0131\u015ftan bu zamana ayl\u0131k tahmin"}}