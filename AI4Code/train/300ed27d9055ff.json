{"cell_type":{"e3fcac62":"code","d33551ac":"code","f8da95e7":"code","4d2048ec":"code","32a4b827":"code","f8fd6e97":"code","3ca301c5":"code","6d07ab78":"code","4321ded0":"code","eac705a4":"code","f2582d7f":"code","e471f96f":"code","7d92b58c":"code","e3046563":"code","bc81efa1":"code","e97cd705":"code","e2cfabcf":"code","d7057553":"code","296c1b7a":"code","910c6aa3":"code","91110e38":"code","61e78de4":"code","448288ec":"markdown","60f2a7d8":"markdown","2cf84ba0":"markdown","aa184a73":"markdown","4e28dff2":"markdown","5ec87165":"markdown","3b643455":"markdown","ce86d060":"markdown"},"source":{"e3fcac62":"import numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline","d33551ac":"main = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\nmain.head(10)","f8da95e7":"print(main.dtypes)\nprint('-' * 50)\nprint(main.isnull().sum())","4d2048ec":"import missingno as msn\nmsn.matrix(main)","32a4b827":"sns.countplot(x = main['quality'], palette= 'muted')","f8fd6e97":"#correlating vaiables with quality\nmain.corr()['quality'].sort_values(ascending = False)","3ca301c5":"sns.heatmap(main.corr(), cmap = 'Reds', linewidth = 1, linecolor = 'Black')","6d07ab78":"main['quality']=[1 if i > 6.5 else 0 for i in main['quality']]\nmain['quality'].value_counts()","4321ded0":"sns.histplot(main['alcohol'], palette = 'muted')","eac705a4":"plt.subplot(2,2,1)\nsns.boxplot(x = main['alcohol'])\nplt.subplot(2,2,2)\nsns.boxplot(x = main['sulphates'])\nplt.subplot(2,2,3)\nsns.boxplot(x = main['citric acid'])\nplt.subplot(2,2,4)\nsns.boxplot(x = main['fixed acidity'])","f2582d7f":"plt.subplot(2,2,1)\nsns.boxplot(x = main['volatile acidity'])\nplt.subplot(2,2,2)\nsns.boxplot(x = main['total sulfur dioxide'])\nplt.subplot(2,2,3)\nsns.boxplot(x = main['density'])\nplt.subplot(2,2,4)\nsns.boxplot(x = main['chlorides'])","e471f96f":"main['zscore'] = ((main['citric acid'] - main['citric acid'].mean())\/main['citric acid'].std())\nfltr = np.abs(main['zscore'])<3 \nmain = main[fltr]\nmain.head()","7d92b58c":"main['zscore2'] = ((main['alcohol'] - main['alcohol'].mean())\/main['alcohol'].std())\nfltr = np.abs(main['zscore2'])<3\nmain = main[fltr]\n\nmain['zscore3'] = ((main['total sulfur dioxide'] - main['total sulfur dioxide'].mean())\/main['total sulfur dioxide'].std())\nfltr = np.abs(main['zscore2'])<3\nmain = main[fltr]","e3046563":"x = main.drop(['quality', 'zscore', 'zscore2', 'zscore3', 'free sulfur dioxide'], axis = 1)\ny = main['quality']\n","bc81efa1":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = .20, random_state = 41)","e97cd705":"from sklearn.preprocessing import StandardScaler\nmodel = StandardScaler()\nx_train = model.fit_transform(x_train)\nx_test = model.fit_transform(x_test)","e2cfabcf":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier","d7057553":"model = LogisticRegression()\nmodel.fit(x_train, y_train)\nmodel.score(x_test, y_test)","296c1b7a":"md = tree.DecisionTreeClassifier()\nmd.fit(x_train, y_train)\nmd.score(x_test, y_test)\n","910c6aa3":"md = SVC()\nmd.fit(x_train, y_train)\nmd.score(x_test, y_test)","91110e38":"for i in range(1,20):\n    model=KNeighborsClassifier( n_neighbors=i,metric='manhattan')\n    model.fit(x_train,y_train)\n    print(str(i)+') '+str(model.score(x_test,y_test)))","61e78de4":"model=KNeighborsClassifier( n_neighbors=14,metric='manhattan')\nmodel.fit(x_train,y_train)\nmodel.score(x_test, y_test)","448288ec":"# KNeighborsClassifier","60f2a7d8":"# Box Plots for the important variables","2cf84ba0":"# Decision Tree Classifier","aa184a73":"# Support Vector Machine","4e28dff2":"**I get the best scores in:**\n* 20% test data and when random_state is 41\n* The score improves iof I take less data for test but it is better to keep atleast 20% data in test\n* The best fitted Algorithms in this case were Support Vector Machine and KNeighborsClassifier.\n* Both of the algorithms gave an accuracy score of 91.8%","5ec87165":"# Applying Standard Scaler","3b643455":"**n_neighbors = 14 gave the most accurate score**","ce86d060":"# Logistic Regression"}}