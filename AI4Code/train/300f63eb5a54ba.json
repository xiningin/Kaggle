{"cell_type":{"89fb60b2":"code","779fe1d0":"code","28c7ae84":"code","8f1a0cbf":"code","5d39ec60":"code","c91858c2":"code","5364d496":"code","526d7352":"code","a58ffb97":"code","ab646ea1":"code","63cc9595":"code","e70c16b4":"code","c3667e8a":"code","6bfd5391":"code","292fb118":"code","8fbac6a8":"code","6d7c252a":"code","3a8b65c8":"code","c93e5fe5":"code","4924b84a":"code","820defa2":"code","38f5eedc":"code","28387fdb":"code","abe33485":"code","e5e4db9f":"code","0f82aec8":"code","8eaf54e7":"code","d6efd1b0":"code","eff70241":"code","e00f89c1":"code","a231a5cf":"code","c0e8e2fd":"code","8a20dec4":"code","3c9e3f81":"code","3e54fa44":"markdown","fb020ce7":"markdown","a11cbdcb":"markdown","d3fb0423":"markdown","1b531945":"markdown","abec412c":"markdown","cb65e0fa":"markdown","0f80197f":"markdown","b1bda46a":"markdown","cbbbcab3":"markdown","993b2ed5":"markdown","2446202d":"markdown","f7d31ec5":"markdown","c33c6867":"markdown","43626585":"markdown","6befb01a":"markdown","b30f568d":"markdown","9ff18569":"markdown","3095ef60":"markdown","d15ec7f5":"markdown","88abdb31":"markdown","5494e0e7":"markdown","a0742bfb":"markdown","230e8cfb":"markdown","6668d06d":"markdown","cade3916":"markdown"},"source":{"89fb60b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\n# Layers for our neural networks\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n# A pretrained model for transfer learning\nfrom keras.models import Model\nfrom keras.applications import vgg19\n\n# Our normal python data science stack you've come to know and love\n\n\nimport sys\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\n# Helper fuctions to evaluate our model.\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n\nfrom sklearn import tree\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.model_selection import GridSearchCV\n\nimport statsmodels.api as sm\n\nimport xgboost as xgb\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","779fe1d0":"df_o2sat = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/o2Saturation.csv')\ndf_heart = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","28c7ae84":"print('Shape of o2 Saturation dataset ' + str(df_o2sat.shape))\nprint('Shape of heart attack dataset ' + str(df_heart.shape))\ndf_heart","8f1a0cbf":"df_heart","5d39ec60":"df_heart.output.value_counts().plot(kind ='bar')\nplt.title('Heart attack frequency')","c91858c2":"df_heart.isna().sum()\n#no missing values","5364d496":"fig, ax = plt.subplots(figsize=(20,20))     \nsns.heatmap(df_heart.corr(),annot=True,cmap='coolwarm')\nplt.title('Correlation Plot ')","526d7352":"categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exng','slp', 'caa','thall'] # 8\ncontinous_cols = ['age', 'trtbps', 'chol','thalachh', 'oldpeak'] # 5\nlabel_col = ['output']\n#X = df['']","a58ffb97":"cnt = 0\nmax_in_row = 3\nfor x in continous_cols:\n    data = df_heart[x]\n    plt.figure(cnt\/\/max_in_row, figsize=(40,4))\n    plt.subplot(1, max_in_row, (cnt)%max_in_row + 1)\n    plt.title(x)\n    sns.histplot(data, bins = 50, kde=50);\n    cnt += 1","ab646ea1":"max_in_row = 3\nfor x in continous_cols:\n    plt.figure(cnt\/\/max_in_row, figsize=(25,4))\n    plt.subplot(1, max_in_row, (cnt)%max_in_row + 1)\n    plt.title(x)\n    sns.kdeplot(data=df_heart, x=x, hue=\"output\", fill=True, common_norm=1, alpha=.5, linewidth=0);\n    cnt += 1","63cc9595":"max_in_row = 3\nfor x in categorical_cols:\n    plt.figure(cnt\/\/max_in_row, figsize=(25,4))\n    plt.subplot(1, max_in_row, (cnt)%max_in_row + 1)\n    plt.title(x)\n    sns.kdeplot(data=df_heart, x=x, hue=\"output\", fill=True, common_norm=False, alpha=.5, linewidth=0,);\n    cnt += 1","e70c16b4":"#some statistical libraries\nimport statsmodels.api as sm\nfrom scipy.stats import shapiro\nimport scipy.stats as stats\nfrom scipy.stats import anderson\nfrom scipy.stats import norm, skew","c3667e8a":"max_in_row = 3\ncnt=0\nfor x in continous_cols:\n    plt.figure(cnt\/\/max_in_row, figsize=(25,4))\n    plt.subplot(1, max_in_row, (cnt)%max_in_row + 1)\n    plt.title(x)\n    sns.boxplot(df_heart[x],orient='v')\n    cnt += 1","6bfd5391":"Q1 = df_heart.thalachh.quantile(.25)\nQ3= df_heart.thalachh.quantile(.75)\nIQR = Q3 - Q1 # the 50% between .25 & .75\nfilter = (df_heart.thalachh >= Q1 - 1.5 * IQR) & (df_heart.thalachh <= Q3 + 1.5 *IQR)\nsns.boxplot(df_heart.loc[filter].thalachh,orient='v')\nplt.title('Thalachh boxplot after removing outliers ')\nshapiro(df_heart.loc[filter].thalachh)\nfig = plt.figure()\nres = stats.probplot(df_heart.loc[filter].thalachh, plot=plt)\nplt.show()\n","292fb118":"Q1 = df_heart.oldpeak.quantile(.25)\nQ3= df_heart.oldpeak.quantile(.75)\nIQR = Q3 - Q1 # the 50% between .25 & .75\nfilter = (df_heart.oldpeak >= Q1 - 1.5 * IQR) & (df_heart.oldpeak <= Q3 + 1.5 *IQR)\nsns.boxplot(df_heart.loc[filter].oldpeak,orient='v')\nplt.title('Oldpeak boxplot after removing outliers ')\nshapiro(df_heart.loc[filter].oldpeak)\nfig = plt.figure()\nres = stats.probplot(df_heart.loc[filter].oldpeak, plot=plt)\nplt.show()","8fbac6a8":"Q1 = df_heart.chol.quantile(.25)\nQ3= df_heart.chol.quantile(.75)\nIQR = Q3 - Q1 # the 50% between .25 & .75\nfilter = (df_heart.chol >= Q1 - 1.5 * IQR) & (df_heart.chol <= Q3 + 1.5 *IQR)\nsns.boxplot(df_heart.loc[filter].chol,orient='v')\nplt.title('Chol boxplot after removing outliers ')\nprint(shapiro(df_heart.loc[filter].chol))\n\nfig = plt.figure()\nres = stats.probplot(df_heart.loc[filter].chol, plot=plt)\nplt.show()","6d7c252a":"Q1 = df_heart.trtbps.quantile(.25)\nQ3= df_heart.trtbps.quantile(.75)\nIQR = Q3 - Q1 # the 50% between .25 & .75\nfilter = (df_heart.trtbps >= Q1 - 1.5 * IQR) & (df_heart.trtbps <= Q3 + 1.5 *IQR)\nsns.boxplot(df_heart.loc[filter].trtbps,orient='v')\nplt.title('Trtbps boxplot after removing outliers ')\nprint(shapiro(df_heart.loc[filter].trtbps))\n\nfig = plt.figure()\nres = stats.probplot(df_heart.loc[filter].trtbps, plot=plt)\nplt.show()","3a8b65c8":"Q1 = df_heart.chol.quantile(.25)\nQ3= df_heart.chol.quantile(.75)\nIQR = Q3 - Q1 # the 50% between .25 & .75\nfilter0 = (df_heart.chol >= Q1 - 1.5 * IQR) & (df_heart.chol <= Q3 + 1.5 *IQR) & (df_heart.output == 1)\nsm.stats.DescrStatsW(df_heart[filter]['chol']).zconfint_mean()","c93e5fe5":"Q1 = df_heart.chol.quantile(.25)\nQ3= df_heart.chol.quantile(.75)\nIQR = Q3 - Q1 # the 50% between .25 & .75\nfilter1 = (df_heart.chol >= Q1 - 1.5 * IQR) & (df_heart.chol <= Q3 + 1.5 *IQR) & (df_heart.output == 0)\nsm.stats.DescrStatsW(df_heart[filter]['chol']).zconfint_mean()","4924b84a":"leveneTest = stats.levene(df_heart[filter0].chol, df_heart[filter1].chol)\nleveneTest","820defa2":"ttest = stats.ttest_ind(df_heart[filter0]['chol'], df_heart[filter1]['chol'], equal_var=1)\nttest\n","38f5eedc":"df_heart = pd.get_dummies(df_heart, columns = categorical_cols, drop_first = True)\n\nX = df_heart.drop(['output'],axis=1) \n\ny = df_heart['output']\nscalerX = MinMaxScaler(feature_range=(0, 1))\nX[X.columns] = scalerX.fit_transform(X[X.columns])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n","28387fdb":"model = LogisticRegression(max_iter=550)\n# Train our model using our training data.\n\nmodel.fit(X_train, y_train)\n#model.predict(X_test,y_test)","abe33485":"y_pred = model.predict(X_test)","e5e4db9f":"\n\n# Calculate our accuracy\naccuracy  = accuracy_score(y_test, y_pred)\n\n# Calculate our precision score\nprecision = precision_score(y_test, y_pred)\n\n# Calculate our recall score\nrecall = recall_score(y_test, y_pred)\n\nf1 = f1_score(y_test, y_pred)\n\n# Print each of our scores to inspect performance.\nprint(\"Accuracy Score: %f\" % accuracy)\nprint(\"Precision Score: %f\" % precision)\nprint(\"Recall Score: %f\" % recall)\nprint('F1 Score %f' % f1)","0f82aec8":"model = tf.keras.models.Sequential([                \n  tf.keras.layers.Dense(256, activation='relu'),\ntf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['acc'])\nepochs = 100\nmodel.fit(X_train, y_train, epochs=epochs, validation_split=0.1,verbose=0)\nmodel.evaluate(X_test, y_test)","8eaf54e7":"#model = DecisionTreeClassifier(max_depth=)\nparams = {\n    'criterion' : ['gini', 'entropy'],\n    'max_depth': [2, 4, 8, 16,32,64,128], \n    'min_samples_split': [2, 4, 8, 16,32,64,128],\n    'min_samples_leaf': [2, 4, 8, 16,32,64,128],\n         }\ngrid_search_cv =  GridSearchCV( \n    estimator = DecisionTreeClassifier(), \n    param_grid = params, \n    scoring = 'accuracy')\ngrid_search_cv.fit(X_train, y_train)\n","d6efd1b0":"model = DecisionTreeClassifier(grid_search_cv.best_estimator_)","eff70241":"y_pred = grid_search_cv.predict(X_test)","e00f89c1":"#y_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\nprint(\"Accuracy Score: %f\" % accuracy)","a231a5cf":"\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n\npred = model.predict(X_test)\nprint(accuracy_score(pred,y_test))","c0e8e2fd":"model =  KNeighborsClassifier(n_neighbors=9)\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\nprint(accuracy_score(y_pred,y_test))","8a20dec4":"params = {\n            'max_iter' : [5,7,9,10,12,-1],\n            'degree' : [2,3,4,5,6],\n            'kernel' : [ 'poly','sigmoid','rbf','linear'],\n            'gamma' : ['scale','auto'],\n        \n         }\ngrid_search_cv =  GridSearchCV( \n    estimator = SVC(), \n    param_grid = params, \n    scoring = 'accuracy')\ngrid_search_cv.fit(X_train, y_train)\ny_pred = grid_search_cv.predict(X_test)\nprint(grid_search_cv.best_estimator_)\naccuracy_score(y_pred=y_pred,y_true=y_test)","3c9e3f81":"model = SVC(max_iter=10,degree=3,kernel='poly')\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\nprint(accuracy_score(y_pred,y_test))","3e54fa44":"# Logistic Regression model gives accuracy of 90%","fb020ce7":"## Decision Tree Classifier not so good","a11cbdcb":"*  Null hypothesis - the two populations have the same variance\n*  Alternate hypothesis- the two populations have different variances","d3fb0423":"## Neural Network accuracy: 87%","1b531945":"## These population averages are rather similiar, is there a statistically significant difference between the the population that is more likely to have a heart attack and the population that is less likely to have a heart attack?\n* Null Hypothesis- There is no statistically significant difference between the two populations.\n* Alternate Hypothesis - There does exist a statiscally significant difference betweent he two populations\n## We will use t-test's to see if there is a difference.\n### Do the 2 populations share the same variance? Levenne test will tell us.","abec412c":"# Some statistical analysis","cb65e0fa":"# Random Forrest 82% accuracy","0f80197f":"## What about the population average of cholesterol for people that are less likely to have a heart attack?","b1bda46a":"### Oldpeak does not have a pvalue > .05. so we reject the null hypothesis & it does not fit the QQ plot","cbbbcab3":"### What does this mean? This means that the cholesterol population average of people that are more likely to have heart attacks have a mean cholesterole between 241 - 256.","993b2ed5":"# Support Vector Machine Classifier 85%","2446202d":"### Are any of the continous variables in the dataset normal?\nLets use graphs and normality tests to verify normality","f7d31ec5":"### What does this mean? This means that the cholesterol population average of people that are more likely to have heart attacks have a mean cholesterole between 231 - 245.","c33c6867":"# Neural Network Model","43626585":"### Trtbps  does not have a pvalue > .05. so we reject the null hypothesis & it does not fit the QQ plot","6befb01a":"### What is the average cholesterol for people that are more likely to have heart attack?","b30f568d":"### Thalachh does not have a pvalue > .05. so we reject the null hypothesis & it does not fit the QQ plot","9ff18569":"### The p-value is greater than .05, so we don't accept the alternate hypothesis, and can proceed with a t-test","3095ef60":"## Let's remove the outliers from thalachh, oldpeak, chol, trtbps & test for normality","d15ec7f5":"### Cholesterol is normal, pvalue > .05 so we accept the null hypothesis","88abdb31":"# Start of machine learning models","5494e0e7":"## Cholesterol is normal so let's analyze it a bit more","a0742bfb":"## Since the p-value is less than .05, we can safely infer that the population of people that have heart attacks come from a different distribution than the population that does not have heart attacks.","230e8cfb":"# K Neighbors classifier 89% accuracy","6668d06d":"# Logistic Regression Model","cade3916":"***Assuming 1 is a heart attack and 0 is no heart attack, 165 heart attacks in the dataset****"}}