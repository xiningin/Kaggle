{"cell_type":{"cf4518f8":"code","0a045760":"code","de92c7b2":"code","4c93cdad":"code","76e818e6":"code","7e4202fb":"code","4dd07585":"code","40dcf2ea":"code","08de192d":"code","ce45ede5":"code","fbdcb1f6":"code","0ab80616":"code","97736ce3":"code","08195ca4":"code","8ac33ac7":"code","3c8ccdfd":"markdown","c3cfc4f1":"markdown","7df0cd3b":"markdown","eb8de825":"markdown","227a89f8":"markdown","fe17e514":"markdown","5b73ed43":"markdown"},"source":{"cf4518f8":"import os\nimport re\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport tensorflow as tf\nimport tqdm.notebook as tqdm\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Dense, Input, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam","0a045760":"def encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts,\n        return_attention_masks=False,\n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    return np.array(enc_di['input_ids'])","de92c7b2":"print(os.listdir('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification'))","4c93cdad":"eng = '\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/'\nnon_eng = '\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/'\n\nes = pd.read_csv(f'{non_eng}jigsaw-toxic-comment-train-google-es-cleaned.csv')\nfr = pd.read_csv(f'{non_eng}jigsaw-toxic-comment-train-google-fr-cleaned.csv')\nit = pd.read_csv(f'{non_eng}jigsaw-toxic-comment-train-google-it-cleaned.csv')\npt = pd.read_csv(f'{non_eng}jigsaw-toxic-comment-train-google-pt-cleaned.csv')\nru = pd.read_csv(f'{non_eng}jigsaw-toxic-comment-train-google-ru-cleaned.csv')\ntr = pd.read_csv(f'{non_eng}jigsaw-toxic-comment-train-google-tr-cleaned.csv')\n\nfor df in [es, fr, it, pt, ru, tr]:\n    cols = list(df.columns)[3:]\n    df.toxic = df[cols].sum(axis=1)\n    df.toxic.apply(lambda x: 1 if x >= 1 else 0)\n\neg2 = pd.read_csv(f'{eng}jigsaw-unintended-bias-train.csv')\neg2['toxic'] = eg2.toxic.round().astype(int)\n\nvalid = pd.read_csv(f'{eng}validation.csv')","76e818e6":"bias_augment = '\/kaggle\/input\/translated-train-bias-all-langs\/All languages'\nbias_es = pd.read_csv(f'{bias_augment}\/train-bias-toxic-google-api-es-cleaned.csv')\nbias_fr = pd.read_csv(f'{bias_augment}\/train-bias-toxic-google-api-fr-cleaned.csv')\nbias_it = pd.read_csv(f'{bias_augment}\/train-bias-toxic-google-api-it-cleaned.csv')\nbias_pt = pd.read_csv(f'{bias_augment}\/train-bias-toxic-google-api-pt-cleaned.csv')\nbias_ru = pd.read_csv(f'{bias_augment}\/train-bias-toxic-google-api-ru-cleaned.csv')\nbias_tr = pd.read_csv(f'{bias_augment}\/train-bias-toxic-google-api-tr-cleaned.csv')","7e4202fb":"test_ex = '\/kaggle\/input\/toxic-comment-detection-multilingual-extended\/english\/english\/'\ntest1 = pd.read_csv(f'{eng}test.csv')\ntesten2 = pd.read_csv(f'{test_ex}test_en.csv')\ntesten3 = pd.read_csv(f'{test_ex}jigsaw_miltilingual_test_translated.csv')","4dd07585":"external = '\/kaggle\/input\/toxic-comment-detection-multilingual-extended\/archive\/'\ne_ru = pd.read_csv(f'{external}russian\/labeled.csv')\n\ne_tr = pd.read_csv(f'{external}turkish\/troff-v1.0.tsv', sep='\\t', header=0)\ne_tr.label = e_tr.label.apply(lambda x: 1 if x not in ['non', 'prof'] else 0)\n\ne_it = pd.concat([\n    pd.read_csv(f'{external}italian\/haspeede_FB-train.tsv', sep='\\t', header=0),\n    pd.read_csv(f'{external}italian\/haspeede_TW-train.tsv', sep='\\t', header=0)\n])","40dcf2ea":"e_ru.rename(columns={'comment':'comment_text'}, inplace=True)\ne_tr.rename(columns={'text':'comment_text', 'label':'toxic'}, inplace=True)\ne_it.rename(columns={'comment':'comment_text'}, inplace=True)","08de192d":"train = pd.concat([\n    es[['comment_text', 'toxic']].query('toxic==0').sample(n=20000, random_state=0),\n    es[['comment_text', 'toxic']].query('toxic==1'),\n    fr[['comment_text', 'toxic']].query('toxic==0').sample(n=20000, random_state=0),\n    fr[['comment_text', 'toxic']].query('toxic==1'),\n    it[['comment_text', 'toxic']].query('toxic==0').sample(n=20000, random_state=0),\n    it[['comment_text', 'toxic']].query('toxic==1'),\n    pt[['comment_text', 'toxic']].query('toxic==0').sample(n=20000, random_state=0),\n    pt[['comment_text', 'toxic']].query('toxic==1'),\n    ru[['comment_text', 'toxic']].query('toxic==0').sample(n=20000, random_state=0),\n    ru[['comment_text', 'toxic']].query('toxic==1'),\n    tr[['comment_text', 'toxic']].query('toxic==0').sample(n=20000, random_state=0),\n    tr[['comment_text', 'toxic']].query('toxic==1'),\n    eg2[['comment_text', 'toxic']].query('toxic==1'),\n    eg2[['comment_text', 'toxic']].query('toxic==0').sample(n=200000, random_state=0),\n    bias_es[['comment_text', 'toxic']].query('toxic==1'),\n    bias_fr[['comment_text', 'toxic']].query('toxic==1'),\n    bias_it[['comment_text', 'toxic']].query('toxic==1'),\n    bias_pt[['comment_text', 'toxic']].query('toxic==1'),\n    bias_ru[['comment_text', 'toxic']].query('toxic==1'),\n    bias_tr[['comment_text', 'toxic']].query('toxic==1')\n])","ce45ede5":"train.toxic = train.toxic.round().astype(int)","fbdcb1f6":"fig = plt.figure()\nax = fig.add_axes([0, 0, 1, 1])\nax.set_title(f'Count of Toxic and Non-Toxic Training Datapoints: {len(train)}')\nc_toxic, c_nontoxic = len(train[train['toxic']==1]), len(train[train['toxic']==0])\nlabels = [f'Toxic: {c_toxic}', f'Non-Toxic {c_nontoxic}']\nvalues = [c_toxic, c_nontoxic]\nax.bar(labels, values)\nplt.show()","0ab80616":"tokenizer = transformers.AutoTokenizer.from_pretrained('jplu\/tf-xlm-roberta-large')","97736ce3":"%%time\n\nx_train = encode(train.comment_text.values, tokenizer, maxlen=192)\nx_valid = encode(valid.comment_text.values, tokenizer, maxlen=192)\nx_test1 = encode(test1.content.values, tokenizer, maxlen=192)\nx_test2 = encode(testen2.content_en, tokenizer, maxlen=192)\nx_test3 = encode(testen3.translated, tokenizer, maxlen=192)","08195ca4":"np.save('x_train', x_train)\nnp.save('x_valid', x_valid)\nnp.save('x_test1', x_test1)\nnp.save('x_test2', x_test2)\nnp.save('x_test3', x_test3)","8ac33ac7":"np.save('y_train', train.toxic.values)\nnp.save('y_valid', valid.toxic.values)","3c8ccdfd":"# Tokenization\nUsing XLM-Roberta (Large)","c3cfc4f1":"# Data Visualizations","7df0cd3b":"I split training and tokenization into two seperate notebooks and later chained them together. This conserved TPU time and it also left more resources for training.","eb8de825":"# XLM-Roberta Tokenizer [Jigsaw Toxic Comment]","227a89f8":"### External Data","fe17e514":"## Loading Data","5b73ed43":"# Helper Functions\nThrough experiments, I found that cleaning the data (getting rid of usernames, ip addresses, removing symbols) does not improve model score. In some cases, it even diminished it."}}