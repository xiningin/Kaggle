{"cell_type":{"063f2e27":"code","3ea58440":"code","b53c62af":"code","fec156cb":"code","d4b0636c":"code","4f27209f":"code","9a9183ac":"code","30449592":"code","9214bf8c":"code","ffa44bae":"code","31ca2559":"code","4884b58e":"code","2912dc32":"code","e5e03eef":"code","38a24619":"code","3f99acbc":"code","eb7d3472":"code","845e1d79":"code","4d24edf9":"code","2297d47c":"code","679c50fd":"code","2d0fd66c":"code","a160e475":"code","78aa35d4":"code","2b3370e0":"code","e359e81b":"code","e20c579c":"code","90a13c1c":"code","34f866ac":"markdown","a60e03ae":"markdown","f9d7f41a":"markdown","49b26be8":"markdown","43e8c73d":"markdown","a52fce15":"markdown","5629f4bf":"markdown","e0f3b375":"markdown","18ae9c6c":"markdown","4ad9536e":"markdown","00c83e13":"markdown","9d5fac46":"markdown","52601e23":"markdown","d427f83a":"markdown","2b855ce8":"markdown","7a98d040":"markdown","0cbbd4f1":"markdown","fcf804e4":"markdown","3127b728":"markdown"},"source":{"063f2e27":"import pandas as pd       \nimport matplotlib as mat\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\n\nimport missingno\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","3ea58440":"seed = 0","b53c62af":"df_train = pd.read_csv('..\/input\/porto-seguro-data-challenge\/train.csv', index_col = 'id')\nX_test = pd.read_csv('..\/input\/porto-seguro-data-challenge\/test.csv', index_col = 'id')\nsubmission = pd.read_csv('..\/input\/porto-seguro-data-challenge\/submission_sample.csv')\nmetadata = pd.read_csv('..\/input\/porto-seguro-data-challenge\/metadata.csv')","fec156cb":"quali_nomi = metadata.loc[metadata['Variavel tipo'] == 'Qualitativo nominal', 'Variavel cod']\nquali_nomi_list = [x for x in quali_nomi]\nquali_nomi_list.remove('id')\n\nquali_ord = metadata.loc[metadata['Variavel tipo'] == 'Qualitativo ordinal', 'Variavel cod']\nquali_ord_list = [x for x in quali_ord]\n\nquant_disc = metadata.loc[metadata['Variavel tipo'] == 'Quantitativo discreto', 'Variavel cod']\nquant_disc_list = [x for x in quant_disc]\nquant_disc_list.remove('y')\n\nquant_cont = metadata.loc[metadata['Variavel tipo'] == 'Quantitativo continua', 'Variavel cod']\nquant_cont_list = [x for x in quant_cont]","d4b0636c":"for col in quali_nomi_list:\n    df_train[col+'_nan'] = df_train[col].apply(lambda x: 1 if x == -999 else 0)\n    X_test[col+'_nan'] = X_test[col].apply(lambda x: 1 if x == -999 else 0)","4f27209f":"df_train['missing_nomi_pos'] = df_train['var2_nan'] + df_train['var3_nan'] + df_train['var8_nan']\ndf_train['missing_nomi_neg'] = df_train['var4_nan'] + df_train['var6_nan'] + df_train['var7_nan']\ndf_train['missing_nomi_neg2'] = df_train['var9_nan'] + df_train['var10_nan'] + df_train['var11_nan'] + df_train['var12_nan']\ndf_train['missing_nomi_neg3'] = df_train['var15_nan'] + df_train['var16_nan'] + df_train['var17_nan'] + df_train['var18_nan']\n\nX_test['missing_nomi_pos'] = X_test['var2_nan'] + X_test['var3_nan'] + X_test['var8_nan']\nX_test['missing_nomi_neg'] = X_test['var4_nan'] + X_test['var6_nan'] + X_test['var7_nan']\nX_test['missing_nomi_neg2'] = X_test['var9_nan'] + X_test['var10_nan'] + X_test['var11_nan'] + X_test['var12_nan']\nX_test['missing_nomi_neg3'] = X_test['var15_nan'] + X_test['var16_nan'] + X_test['var17_nan'] + X_test['var18_nan']","9a9183ac":"df_train = df_train.mask(df_train <= -999.0)\nX_test = X_test.mask(X_test <= -999.0)","30449592":"X_train = df_train.copy().drop(['y'], axis = 1)\nY_train = df_train['y'].copy()","9214bf8c":"num_imput = SimpleImputer(strategy = 'median')\n#nomi_imput = SimpleImputer(strategy = 'constant', fill_value = -999)\nord_imput = SimpleImputer(strategy = 'most_frequent')\n\nnum_features = [x for x in quant_cont_list]\nnum_features.extend([x for x in quant_disc_list])\n\nX_train[num_features] = num_imput.fit_transform(X_train[num_features])\nX_test[num_features] = num_imput.transform(X_test[num_features])\n\nX_train[quali_ord_list] = ord_imput.fit_transform(X_train[quali_ord_list])\nX_test[quali_ord_list] = ord_imput.transform(X_test[quali_ord_list])\n\n#X_train[quali_nomi_list] = nomi_imput.fit_transform(X_train[quali_nomi_list])\n#X_test[quali_nomi_list] = nomi_imput.transform(X_test[quali_nomi_list])","ffa44bae":"X_train['comb50_51_27'] = X_train['var51'] + X_train['var27'] - X_train['var50']\nX_train['var4_over25k'] = X_train['var4'].apply(lambda x: 1 if x > 25000 else 0)\n\nX_test['comb50_51_27'] = X_test['var51'] + X_test['var27'] - X_test['var50']\nX_test['var4_over25k'] = X_test['var4'].apply(lambda x: 1 if x > 25000 else 0)","31ca2559":"X_train['var53equal0'] = X_train['var53'].apply(lambda x: 1 if x == 0 else 0)\nX_train['var54equal0'] = X_train['var54'].apply(lambda x: 1 if x == 0 else 0)\n\nX_train['var53_54equal0_sum'] = X_train['var53equal0'] + X_train['var54equal0']\n\nX_train['var53equal1'] = X_train['var53'].apply(lambda x: 1 if x == 1 else 0)\nX_train['var54equal1'] = X_train['var54'].apply(lambda x: 1 if x == 1 else 0)\n\nX_train['var53_54equal1_sum'] = X_train['var53equal1'] +X_train['var54equal1']\n\n\nX_test['var53equal0'] = X_test['var53'].apply(lambda x: 1 if x == 0 else 0)\nX_test['var54equal0'] = X_test['var54'].apply(lambda x: 1 if x == 0 else 0)\n\nX_test['var53_54equal0_sum'] = X_test['var53equal0'] + X_train['var54equal0']\n\nX_test['var53equal1'] = X_test['var53'].apply(lambda x: 1 if x == 1 else 0)\nX_test['var54equal1'] = X_test['var54'].apply(lambda x: 1 if x == 1 else 0)\n\nX_test['var53_54equal1_sum'] = X_test['var53equal1'] +X_test['var54equal1']","4884b58e":"#list to drop\nquali_nomi_list_nan = [(x+'_nan') for x in quali_nomi_list]\n\nX_train = X_train.drop(quali_nomi_list_nan, axis = 1)\n\nX_test = X_test.drop(quali_nomi_list_nan, axis = 1)","2912dc32":"X_train = X_train.drop(['var53equal0', 'var54equal0', 'var53equal1', 'var54equal1', 'var11', 'var44', 'var65', 'var66'], axis = 1)\nX_test = X_test.drop(['var53equal0', 'var54equal0', 'var53equal1', 'var54equal1', 'var11', 'var44', 'var65', 'var66'], axis = 1)","e5e03eef":"X_train.info()","38a24619":"xgb_model = XGBClassifier(n_estimators = 3000, learning_rate = 0.02, random_state = seed, eval_metric = 'aucpr',\n                           subsample = 0.95, colsample_bytree = 0.80, reg_alpha = 0.5, reg_lambda = 0.5, \n                           max_depth = 6, gamma = 0.0)","3f99acbc":"def prediction (X_train, Y_train, model, X_test, seed = seed):\n    \n    print(30*'-')\n    \n    print('seed:', seed)\n    random_state = {'random_state' : seed}\n    model = model.set_params(**random_state)\n    \n    kfold = StratifiedKFold(n_splits = 10, shuffle=True, random_state = seed)\n\n    y_pred = np.zeros(len(X_test))\n    train_oof = np.zeros(len(X_train))\n    train_oof_prob = np.zeros(len(X_train))\n    \n    for idx in kfold.split(X=X_train, y=Y_train):\n        train_idx, val_idx = idx[0], idx[1]\n        xtrain = X_train.iloc[train_idx]\n        ytrain = Y_train.iloc[train_idx]\n        xval = X_train.iloc[val_idx]\n        yval = Y_train.iloc[val_idx]\n        \n        # fit model for current fold\n        model.fit(xtrain, ytrain, \n            early_stopping_rounds = 200, eval_set = [(xval,yval)], verbose = False)\n\n        #create predictions\n        y_pred += model.predict_proba(X_test)[:, 1]\/kfold.n_splits\n        print(y_pred)\n               \n        val_pred = model.predict(xval)\n        # getting out-of-fold predictions on training set\n        train_oof[val_idx] = val_pred\n        \n        val_prob = model.predict_proba(xval)\n        # getting out-of-fold probabilities on training set\n        train_oof_prob[val_idx] = val_prob[:, 1]\n        \n        fold_f1_score = metrics.f1_score(yval,val_pred)\n        fold_logloss = metrics.log_loss(yval,val_prob[:, 1])\n        fold_auc = metrics.roc_auc_score(yval,val_prob[:, 1])\n\n        print(\"F1_score: {0:0.5f}\". format(fold_f1_score))\n        print(\"LogLoss: {0:0.5f}\". format(fold_logloss))\n        print(\"AUC: {0:0.5f}\\n\". format(fold_auc))\n        \n        print(30*'-')\n\n  \n    return y_pred, train_oof, train_oof_prob","eb7d3472":"xgb_pred_0, train_oof_0, train_oof_prob_0 = prediction (X_train, Y_train, xgb_model, X_test, seed = 0)\nxgb_pred_1, train_oof_1, train_oof_prob_1 = prediction (X_train, Y_train, xgb_model, X_test, seed = 1)\nxgb_pred_2, train_oof_2, train_oof_prob_2 = prediction (X_train, Y_train, xgb_model, X_test, seed = 2)\nxgb_pred_3, train_oof_3, train_oof_prob_3 = prediction (X_train, Y_train, xgb_model, X_test, seed = 3)\nxgb_pred_4, train_oof_4, train_oof_prob_4 = prediction (X_train, Y_train, xgb_model, X_test, seed = 4)\nxgb_pred_5, train_oof_5, train_oof_prob_5 = prediction (X_train, Y_train, xgb_model, X_test, seed = 5)\nxgb_pred_6, train_oof_6, train_oof_prob_6 = prediction (X_train, Y_train, xgb_model, X_test, seed = 6)\nxgb_pred_7, train_oof_7, train_oof_prob_7 = prediction (X_train, Y_train, xgb_model, X_test, seed = 7)\nxgb_pred_8, train_oof_8, train_oof_prob_8 = prediction (X_train, Y_train, xgb_model, X_test, seed = 8)\nxgb_pred_9, train_oof_9, train_oof_prob_9 = prediction (X_train, Y_train, xgb_model, X_test, seed = 9)","845e1d79":"train_oof_list = [train_oof_0, train_oof_1, train_oof_2, train_oof_3, train_oof_4\n                      , train_oof_5, train_oof_6, train_oof_7, train_oof_8, train_oof_9]\ntrain_oof_list","4d24edf9":"#Saving out-of-fold hard predictions\ntrain_oof= pd.DataFrame(index = df_train.index)\n\nfor i,col in enumerate(train_oof_list):\n    train_oof[i] = [x for x in col]\n\ntrain_oof.to_csv(\"train_oof.csv\", index=False)\n\ntrain_oof","2297d47c":"train_oof_prob_list = [train_oof_prob_0, train_oof_prob_1, train_oof_prob_2, train_oof_prob_3, train_oof_prob_4\n                      , train_oof_prob_5, train_oof_prob_6, train_oof_prob_7, train_oof_prob_8, train_oof_prob_9]\ntrain_oof_prob_list","679c50fd":"#Saving out-of-fold predicted probabilities\ntrain_oof_prob= pd.DataFrame(index = df_train.index)\n\nfor i,col in enumerate(train_oof_prob_list):\n    train_oof_prob[i] = [x for x in col]\n\ntrain_oof_prob['average'] = train_oof_prob.mean(axis=1)\ntrain_oof_prob.to_csv(\"train_oof_prob.csv\", index=False)\n\ntrain_oof_prob","2d0fd66c":"avg_logloss = metrics.log_loss(Y_train,train_oof_prob['average'])\navg_auc = metrics.roc_auc_score(Y_train,train_oof_prob['average'])\n\nprint(\"Avg Logloss: {0:0.5f}\". format(avg_logloss))\nprint(\"Avg AUC: {0:0.5f}\\n\". format(avg_auc))","a160e475":"xgb_cvprob_threshold= np.where(train_oof_prob['average']>0.345, 1, 0)\nprint(metrics.classification_report(Y_train, xgb_cvprob_threshold, labels = [0, 1], digits = 4))","78aa35d4":"xgb_pred_list = [xgb_pred_0, xgb_pred_1, xgb_pred_2, xgb_pred_3, xgb_pred_4\n                      , xgb_pred_5, xgb_pred_6, xgb_pred_7, xgb_pred_8, xgb_pred_9]\nxgb_pred_list","2b3370e0":"#Saving test set predictions\nxgb_pred = pd.DataFrame(index = X_test.index)\n\nfor i,col in enumerate(xgb_pred_list):\n    xgb_pred[i] = [x for x in col]\n\nxgb_pred['average'] = xgb_pred.mean(axis=1)\nxgb_pred.to_csv(\"xgb_pred.csv\", index=False)\n\nxgb_pred","e359e81b":"xgb_finalpred= np.where(xgb_pred['average']>0.345, 1, 0)\nxgb_finalpred","e20c579c":"submission['predicted'] = xgb_finalpred\nsubmission.to_csv(\"submission.csv\", index=False)\n\nsubmission","90a13c1c":"submission['predicted'].value_counts(normalize = True)","34f866ac":"<br>    \n    \n![30days](https:\/\/i.imgur.com\/Ifwonwn.png)\n <center>Ilustra\u00e7\u00e3o da Solu\u00e7\u00e3o<center>\n<br>","a60e03ae":"Criando listas para cada tipo de vari\u00e1vel com base nas informa\u00e7\u00f5es de metadados","f9d7f41a":"Ent\u00e3o, \u00e9 criada a lista 'xgb_finalpred', convertendo a m\u00e9dia das probabilidades previstas para as previs\u00f5es finais com base no threshold definido. Por fim, os valores s\u00e3o passados para o arquivo utilizado na submiss\u00e3o.","49b26be8":"Criando novas vari\u00e1veis com base em observa\u00e7\u00f5es feitas durante a etapa de EDA.","43e8c73d":"A priori, n\u00e3o parecia haver valores missing nos datasets de treino e teste. Por\u00e9m, foi poss\u00edvel perceber que estes eram representados pelos valores -999.","a52fce15":"## Defini\u00e7\u00e3o e Execu\u00e7\u00e3o do Modelo","5629f4bf":"## Feature Engineering","e0f3b375":"Imputando valores missing. \n\nObs: Ap\u00f3s testes, decidi por n\u00e3o imputar os valores missing de vari\u00e1veis nominais. o XGBoost possui um m\u00e9todo pr\u00f3prio para lidar com valores missing e os resultados foram melhores desta forma.","18ae9c6c":"## Defini\u00e7\u00e3o do Threshold e Gera\u00e7\u00e3o do Submission File.","4ad9536e":"A escolha do threshold foi feita com base no F1-score obtido na m\u00e9dia das previs\u00f5es out-of-fold.\n\n\u00c9 poss\u00edvel criar uma fun\u00e7\u00e3o para otimizar o threshold, tendo como objetivo minimizar o F1-score. Entretanto, existem pontos em que uma pequena varia\u00e7\u00e3o deste pode impactar o F1 de maneira consider\u00e1vel. Por isso, optei por definir o threshold \u2018manualmente\u2019, escolhendo um ponto bom o suficiente, no qual o F1 se manteria pr\u00f3ximo com m\u00ednimas altera\u00e7\u00f5es para mais ou para menos.","00c83e13":"A seguinte fun\u00e7\u00e3o 'prediction' realiza uma valida\u00e7\u00e3o cruzada estratificada com 10 folds. A cada itera\u00e7\u00e3o, o modelo XGBoost \u00e9 treinado em 90% dos dados de treino e utilizado para prever os 10% restantes. Al\u00e9m disso, cada um dos \u2018modelos\u2019 treinados em cada itera\u00e7\u00e3o \u00e9 usado para prever os dados de teste.\n\nOs outputs da fun\u00e7\u00e3o \u2018prediction\u2019 s\u00e3o:\n- Previs\u00f5es (0 ou 1) e probabilidades previstas para cada fold dos dados de treino (out-of-fold predictions);\n- Probabilidades previstas para os dados de teste (m\u00e9dia das 10 itera\u00e7\u00f5es).\n","9d5fac46":"Separando X_train (features) de Y_train (target)","52601e23":"Dropando as vari\u00e1veis bin\u00e1rias (missing ou n\u00e3o) que foram utilizadas para gerar as vari\u00e1veis combinadas ('missing_nomi')","d427f83a":"Como explicado no in\u00edcio, a solu\u00e7\u00e3o apresentada neste notebook consiste em utilizar a m\u00e9dia dos resultados obtidos pelo modelo com 10 seeds. Aqui, a fun\u00e7\u00e3o \u2018prediction\u2019 \u00e9 chamada 10 vezes, cada uma com uma seed diferente. ","2b855ce8":"Observa\u00e7\u00e3o feita durante o EDA: novas vari\u00e1veis bin\u00e1rias, indicando se h\u00e1 ou n\u00e3o um valor missing em uma vari\u00e1vel qualitativa nominal, poderiam ter poder preditivo.\n\nEx: Na vari\u00e1vel 8, por exemplo, 54% das amostras com valores missing adquiriram o produto (y=1), enquanto apenas 14% das amostras sem valores missing apresentavam y=1.\n\nIdeia: \n- Criar vari\u00e1veis bin\u00e1rias (missing ou n\u00e3o) a partir das vari\u00e1veis nominas antes de converter os valores missing para np.nan;\n- Criar novas vari\u00e1veis combinando os valores de algumas destas vari\u00e1veis bin\u00e1rias;\n- Ap\u00f3s testes, decidi por manter apenas estas combina\u00e7\u00f5es e exclu\u00ed as vari\u00e1veis bin\u00e1rias.","7a98d040":"# <center>Porto Seguro Data Challenge<center>\n## <center>XGBoost - 10 Seeds Average (6th place solution)<center>\n---\n\nNova vers\u00e3o criada para incluir uma certa estrutura\u00e7\u00e3o e algumas explica\u00e7\u00f5es sobre a minha solu\u00e7\u00e3o.\n    \n\n    \nObs: Este notebook cont\u00e9m apenas a solu\u00e7\u00e3o submetida. As etapas de an\u00e1lise explorat\u00f3ria dos dados, otimiza\u00e7\u00e3o de hiperpar\u00e2metros, al\u00e9m de eventuais testes, foram realizadas em outros notebooks n\u00e3o divulgados.\n    \n    \nP.S. for Non-Portuguese speakers: This notebook was submitted to the Porto Seguro Data Challenge, a competition which its main audience is formed mainly by Brazilians. Therefore, I made the decision to write all explanations in Portuguese. If you ran into this notebook and found it interesting, but have any sort of doubts, feel free to contact me in the comment section.    \n    \n---\n\nA solu\u00e7\u00e3o consiste em utilizar um \u00fanico tipo de modelo (XGBoost) com um \u00fanico conjunto de hiperpar\u00e2metros, executando-o, entretanto, com 10 seeds diferentes. Assim, s\u00e3o geradas 10 \u2018solu\u00e7\u00f5es\u2019 distintas, contendo as probabilidades previstas para cada seed utilizada. Em seguida, \u00e9 gerada uma nova lista contendo a m\u00e9dia destas probabilidades. \n    \nPara criar a solu\u00e7\u00e3o final a ser submetida, as probabilidades s\u00e3o convertidas para as suas classes (0 ou 1). Entretanto, visto que os datasets s\u00e3o desbalanceados entre as classes, ao inv\u00e9s de se utilizar o threshold padr\u00e3o (50%), \u00e9 escolhido um novo valor de threshold (34,5%), o que garante um aumento do F1-score. \n    \nF1-Scores:\n- Public test set: 0.69615\n- Private test set: 0.69267 (6th place)","0cbbd4f1":"Convertendo os valores -999 (missing) para np.nan","fcf804e4":"## Importando Pacotes e Datasets","3127b728":"Outras vari\u00e1veis dropadas:\n- var65 e var66: Alto n\u00famero de valores missing;\n- var11 e var44: Pouco poder preditivo e melhores resultados nos testes ap\u00f3s suas retiradas;\n- Demais vari\u00e1veis: Vari\u00e1veis auxiliares para criar 'var53_54equal0_sum' e 'var53_54equal1_sum'."}}