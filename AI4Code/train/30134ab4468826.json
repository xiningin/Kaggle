{"cell_type":{"7f48afa8":"code","b783398e":"code","3501a8fc":"code","6c8199dd":"code","b791a0ac":"code","783f2e82":"code","e0d264a4":"code","d33c743c":"code","c1a490e2":"code","c288b622":"code","a7653a9e":"code","c6abc080":"code","775c3a11":"code","a18c34d7":"code","814ecd71":"code","d319d131":"code","0e8a4d0e":"code","3cbe4be1":"code","2feb5599":"code","4a637fd2":"code","c5824df7":"code","87065af1":"code","f61d360b":"code","81a4ef16":"code","f64082e7":"code","2f72887e":"code","fa6dfcce":"code","e94718f0":"code","fdab0687":"code","c8727bc6":"code","f2ea9ec8":"code","1c47e0a5":"code","1e9b724c":"code","c705ffab":"code","478eeccc":"code","131ce344":"code","5cc461b1":"code","47aea56f":"code","ac781040":"code","e911bada":"code","9f488c59":"code","038ebaa7":"code","86383cb5":"code","53e01198":"code","cfe9e301":"code","ee7ede98":"markdown","406e9949":"markdown","ffa19554":"markdown","7e38970a":"markdown","680e5a7b":"markdown","f7ff7973":"markdown","2f467336":"markdown","86922789":"markdown","db3e4fa2":"markdown","976eb1c3":"markdown","12f7d3ad":"markdown","fe8693ed":"markdown","8aa6425c":"markdown","c1a1d00c":"markdown","e006caa3":"markdown","7469d9d7":"markdown","218a8804":"markdown","a00fa0e4":"markdown","4b5f142d":"markdown","7a202e83":"markdown","bb29db23":"markdown","324bbd9e":"markdown","24bbc9da":"markdown","ada9a673":"markdown","648873a3":"markdown","d451d81e":"markdown","2f45e540":"markdown","3dd42ede":"markdown","fd1f523b":"markdown","6e3c3071":"markdown","ae6baa05":"markdown","b52395a9":"markdown","b5523868":"markdown","557d7685":"markdown","2470164f":"markdown"},"source":{"7f48afa8":"# Import all libraries to be used\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split, learning_curve, cross_val_score, cross_validate, validation_curve, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score, roc_curve, plot_roc_curve\n","b783398e":"#data = pd.read_csv('\/kaggle\/input\/students-performance-in-exams\/StudentsPerformance.csv')\ndata = pd.read_csv('\/kaggle\/input\/exams6k\/exams.csv')\ndata","3501a8fc":"#import os\n\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\ndata.describe()","6c8199dd":"# NOTE: # dev_null is used to produce no unnecessary output. Also it will not be used anywhere because it is just a variable to put anything meaningless. like \/dev\/null in linux\ndev_null = sns.distplot(data[\"math score\"])\ndev_null.set(xlabel=\"Math Score\", ylabel=\"Frequency\")\ndev_null = dev_null.set_title(\"Math Scores Distributions\")","b791a0ac":"dev_null = sns.distplot(data[\"reading score\"])\ndev_null.set(xlabel=\"Reading Score\", ylabel=\"Frequency\")\ndev_null = dev_null.set_title(\"Reading Scores Distributions\")","783f2e82":"dev_null = sns.distplot(data[\"writing score\"])\ndev_null.set(xlabel=\"Writing Score\", ylabel=\"Frequency\")\ndev_null = dev_null.set_title(\"Writing Scores Distributions\")","e0d264a4":"data.select_dtypes('object').nunique()","d33c743c":"data.isnull().sum()","c1a490e2":"dev_null = sns.barplot(x=\"race\/ethnicity\", y=\"math score\", hue=\"gender\", data=data)\ndev_null.set(xlabel=\"Group\", ylabel=\"Math Score\")\ndev_null = dev_null.set_title(\"Math Scores By Group\")","c288b622":"dev_null = sns.barplot(x=\"race\/ethnicity\", y=\"reading score\", hue=\"gender\", data=data)\ndev_null.set(xlabel=\"Reading Score\", ylabel=\"Frequency\")\ndev_null = dev_null.set_title(\"Reading Scores Distributions\")","a7653a9e":"%matplotlib inline\n\nplt.figure(figsize=(25,6))\nplt.subplot(1, 3, 1)\nsns.distplot(data['math score'])\n\nplt.subplot(1, 3, 2)\nsns.distplot(data['reading score'])\n\nplt.subplot(1, 3, 3)\nsns.distplot(data['writing score'])\n\nplt.suptitle('Checking for Skewness', fontsize = 15)\nplt.show()","c6abc080":"dev_null = sns.heatmap(data.corr(), annot=True, fmt=\".2f\")\ndev_null = dev_null.set_title(\"Frequency Distributions comparing scores\")","775c3a11":"data.info()","a18c34d7":"dev_null = sns.countplot(x=\"race\/ethnicity\", data=data)\ndev_null.set(xlabel=\"Group\", ylabel=\"Count\")\ndev_null = dev_null.set_title(\"Count of Group members\")","814ecd71":"dev_null = sns.barplot(x=\"race\/ethnicity\", y=\"writing score\", hue=\"gender\", data=data)\ndev_null.set(xlabel=\"Group\", ylabel=\"Writing Score\")\ndev_null = dev_null.set_title(\"Writing Scores by Group\")","d319d131":"countplot = sns.countplot(x=\"parental level of education\", data=data)\ncountplot.set_xticklabels(countplot.get_xticklabels(), rotation=40, ha=\"right\")\ncountplot.set(xlabel=\"Parental Education Lvl\", ylabel=\"Count\")\ndev_null = countplot.set_title(\"Count of Students by Education attained by their parents\")","0e8a4d0e":"dev_null = sns.boxplot(x=\"race\/ethnicity\", y=\"math score\", hue=\"gender\", data=data)\ndev_null.set(xlabel=\"Group\", ylabel=\"Math Score\")\ndev_null = dev_null.set_title(\"Math Scores by Group\")","3cbe4be1":"# check counts for relationship between race\/ethnicity and parental education level. \nparent_edu_vs_eth_race = pd.crosstab(index=data[\"race\/ethnicity\"], columns=data[\"parental level of education\"])\ndev_null = sns.heatmap(parent_edu_vs_eth_race)\ndev_null.set(xlabel=\"Parent Education Lvl\", ylabel=\"Group\")\ndev_null = dev_null.set_title(\"Correlation between Groups and Parent Educ. Lvl\")","2feb5599":"parent_edu_vs_eth_race","4a637fd2":"bar = sns.barplot(x=\"parental level of education\", y=\"math score\", hue=\"race\/ethnicity\", data=data)\nbar.set_xticklabels(bar.get_xticklabels(), rotation=40, ha=\"right\")\nbar.set(xlabel=\"Parental Educ. Lvl\", ylabel=\"Math Score\")\ndev_null = bar.set_title(\"Relating Parental Educ. Lvl to Math Score\")","c5824df7":"bar = sns.boxplot(x=\"parental level of education\", y=\"math score\", hue=\"race\/ethnicity\", data=data)\nbar.set_xticklabels(bar.get_xticklabels(), rotation=40, ha=\"right\")\nbar.set(xlabel=\"Parent Educ. Lvl\", ylabel=\"Math Score\")\ndev_null = bar.set_title(\"Math Score and Parent Educ. Lvl\")","87065af1":"data[\"Pass\"] = data.apply(lambda x: 1 if x[\"math score\"] >= 65 and x[\"reading score\"] >= 65 and x[\"writing score\"] >= 65 else 0, axis=1)\ndata = data.drop([\"math score\", \"reading score\", \"writing score\"], axis=1)\ndata.select_dtypes(include=\"object\")\ndata\n\nX = data.drop([\"Pass\"], axis=1)\ny = data[\"Pass\"]\nX,y","f61d360b":"# Add using different parameters.","81a4ef16":"# Encoding categorical inputs\nencoder = OneHotEncoder(handle_unknown=\"ignore\")\nencoder.fit(X)\nX = encoder.transform(X)\n\n# 80\/20 train split ratio\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=1)\n\nmlp = MLPClassifier(\n    max_iter=3000,\n    hidden_layer_sizes=[17, 13, 7], \n    solver=\"sgd\", \n    random_state=1,\n    verbose=False\n).fit(X_train, y_train)\n\ny_predicted = mlp.predict(X_test)\n\ny_predicted, y_test.to_numpy() # Todo compare \n","f64082e7":"def format_scores_as_dataframe(labels, train_scores, test_scores):\n    learning_data = {\"labels\": [], \"type\": [], \"score\": []}\n\n    for i in range(len(train_sizes)):\n        for j in range(len(train_scores)):\n            learning_data[\"labels\"].append(labels[i])\n            learning_data[\"type\"].append(\"train\")\n            learning_data[\"score\"].append(train_scores[i][j])\n            learning_data[\"labels\"].append(labels[i])\n            learning_data[\"type\"].append(\"test\")\n            learning_data[\"score\"].append(test_scores[i][j])\n            \n    return pd.DataFrame.from_dict(learning_data)","2f72887e":"# Learning Curve | Complexity Curve\ntrain_sizes, train_scores, test_scores = learning_curve(mlp, X, y)\n\nlearning_curve_df = format_scores_as_dataframe(train_sizes, train_scores, test_scores)\n\n# train and test learning scores results\nax = sns.lineplot(x=\"labels\", y=\"score\", hue=\"type\", data=learning_curve_df, marker=\"o\", ci=None)\nax.set_title(\"Learning Curve for MLP Algorithm\")\ndev_null = ax.set(xlabel=\"Samples\", ylabel=\"Error\")","fa6dfcce":"scores = cross_val_score(mlp, X, y)\n\nscores, scores.mean(), scores.std()\n\ndev_null = sns.lineplot(x=[1,2,3,4,5], y=scores)\ndev_null.set_title(\"Cross Score Distribution\")\ndev_null = dev_null.set(xlabel=\"# of runs\", ylabel=\"Accuracy\")","e94718f0":"cross_val_result = cross_validate(mlp, X, y, return_train_score=True)\n\n#validation_curve(mlp, X, y, param_name=\"alpha\", param_range=[0.0001, 0.001, 0.05])\ntrain_scores, test_scores = validation_curve(mlp, X, y, param_name=\"hidden_layer_sizes\", param_range=([5], [10], [10,5], [15, 10], [25,10,5]))\n\nval_curve_data = {\"labels\": [], \"type\": [], \"scores\": []}\nparam_ranges = [\"[5]\", \"[10]\", \"[10,5]\", \"[15,10]\", \"[25,10,5]\"]\n\nfor i in range(len(train_scores)):\n    for j in range(len(train_scores[i])):\n        val_curve_data[\"labels\"].append(param_ranges[i])\n        val_curve_data[\"type\"].append(\"train\")\n        val_curve_data[\"scores\"].append(train_scores[i][j])\n        val_curve_data[\"labels\"].append(param_ranges[i])\n        val_curve_data[\"type\"].append(\"test\")\n        val_curve_data[\"scores\"].append(test_scores[i][j])\n        \nval_curve_df = pd.DataFrame.from_dict(val_curve_data)\n\nax = sns.lineplot(x=\"labels\", y=\"scores\", hue=\"type\", data = val_curve_df, marker=\"o\", ci=None)\nax.set_title(\"Validation Curve for our MLP model\")\ndev_null = ax.set(xlabel=\"Layers\/Neurons\", ylabel=\"Accuracy Score\")\n\n\n","fdab0687":"confusion_mtrx = confusion_matrix(y_test, y_predicted)\nclassification_rprt = classification_report(y_test, y_predicted)\naccuracy_scr = accuracy_score(y_test, y_predicted)\n# TN FP\n# FN TP\nprint(\"Confusion Matrix\")\nprint(confusion_mtrx)\nprint(\"Classification Report\")\nprint(classification_rprt)\nprint(\"Accuracy\")\nprint(accuracy_scr)","c8727bc6":"# Computing AUC score\nroc = roc_auc_score(y_test, y_predicted)\ndev_null = plot_roc_curve(mlp, X_test, y_test, name=\"AUC\/ROC Curve for MLP\")","f2ea9ec8":"parameters = { # parameters commented to make running time shorter\n    \"hidden_layer_sizes\": [[8], [5], [2]],#, [8,8], [8,5], [5,8], [5,2], [2,2], [8,5,2], [8,5,5], [13,8,4], [17,13,7]],\n    \"activation\": [\"identity\", \"logistic\"],#, \"tanh\", \"relu\"], \n    \"solver\": [\"lbfgs\", \"sgd\"],#, \"adam\"], \n    \"max_iter\": [200, 500],#, 1000, 2000, 3000, 5000]\n}\n\n# Brace yourself, this will take a while\nmlp = MLPClassifier()\ngs = GridSearchCV(mlp, parameters)\ngs.fit(X_train, y_train)\ngs.predict(X_test)\ngs.best_estimator_","1c47e0a5":"X = data.drop([\"Pass\"], axis=1)\ny = data[\"Pass\"]\nX,y","1e9b724c":"# Encoding categorical inputs\nencoder = OneHotEncoder(handle_unknown=\"ignore\")\nencoder.fit(X)\nX = encoder.transform(X)\n\n# 80\/20 train split ratio\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=1)\n\nmlp = MLPClassifier(\n    max_iter=10000,\n    hidden_layer_sizes=[100], \n    activation=\"logistic\",\n    random_state=1,\n    verbose=False\n).fit(X_train, y_train)\n\ny_predicted = mlp.predict(X_test)\n\ny_predicted, y_test.to_numpy() # Todo compare ","c705ffab":"# Learning Curve | Complexity Curve\n\ntrain_sizes, train_scores, test_scores = learning_curve(mlp, X, y)\n\nlearning_curve_df = format_scores_as_dataframe(train_sizes, train_scores, test_scores)\n\n# train and test learning scores results\nax = sns.lineplot(x=\"labels\", y=\"score\", hue=\"type\", data=learning_curve_df, marker=\"o\", ci=None)\nax.set_title(\"Learning Curve for MLP Algorithm\")\ndev_null = ax.set(xlabel=\"Samples\", ylabel=\"Error\")","478eeccc":"scores = cross_val_score(mlp, X, y)\n\nscores, scores.mean(), scores.std()\n\ndev_null = sns.lineplot(x=[1,2,3,4,5], y=scores)\ndev_null.set_title(\"Cross Score Distribution\")\ndev_null = dev_null.set(xlabel=\"# of runs\", ylabel=\"Accuracy\")","131ce344":"cross_val_result = cross_validate(mlp, X, y, return_train_score=True)\n\n#validation_curve(mlp, X, y, param_name=\"alpha\", param_range=[0.0001, 0.001, 0.05])\ntrain_scores, test_scores = validation_curve(mlp, X, y, param_name=\"hidden_layer_sizes\", param_range=([5], [10], [10,5], [15, 10], [25,10,5]))\n\nval_curve_data = {\"labels\": [], \"type\": [], \"scores\": []}\nparam_ranges = [\"[5]\", \"[10]\", \"[10,5]\", \"[15,10]\", \"[25,10,5]\"]\n\nfor i in range(len(train_scores)):\n    for j in range(len(train_scores[i])):\n        val_curve_data[\"labels\"].append(param_ranges[i])\n        val_curve_data[\"type\"].append(\"train\")\n        val_curve_data[\"scores\"].append(train_scores[i][j])\n        val_curve_data[\"labels\"].append(param_ranges[i])\n        val_curve_data[\"type\"].append(\"test\")\n        val_curve_data[\"scores\"].append(test_scores[i][j])\n        \nval_curve_df = pd.DataFrame.from_dict(val_curve_data)\n\nax = sns.lineplot(x=\"labels\", y=\"scores\", hue=\"type\", data = val_curve_df, marker=\"o\", ci=None)\nax.set_title(\"Validation Curve for our MLP model\")\ndev_null = ax.set(xlabel=\"Layers\/Neurons\", ylabel=\"Accuracy Score\")","5cc461b1":"confusion_mtrx = confusion_matrix(y_test, y_predicted)\nclassification_rprt = classification_report(y_test, y_predicted)\naccuracy_scr = accuracy_score(y_test, y_predicted)\n# TN FP\n# FN TP\nprint(\"Confusion Matrix\")\nprint(confusion_mtrx)\nprint(\"Classification Report\")\nprint(classification_rprt)\nprint(\"Accuracy\")\nprint(accuracy_scr)","47aea56f":"# Computing AUC score\nroc = roc_auc_score(y_test, y_predicted)\ndev_null = plot_roc_curve(mlp, X_test, y_test, name=\"AUC\/ROC Curve for MLP\")","ac781040":"X = data.drop([\"Pass\"], axis=1)\ny = data[\"Pass\"]\nX,y","e911bada":"# Encoding categorical inputs\nencoder = OneHotEncoder(handle_unknown=\"ignore\")\nencoder.fit(X)\nX = encoder.transform(X)\n\n# 80\/20 train split ratio\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=1)\n\nmlp = MLPClassifier(\n    max_iter=3000,\n    hidden_layer_sizes=[2], \n    solver=\"sgd\",\n    activation=\"identity\",\n    random_state=1,\n    verbose=False\n).fit(X_train, y_train)\n\ny_predicted = mlp.predict(X_test)\n\ny_predicted, y_test.to_numpy() # Todo compare ","9f488c59":"# Learning Curve | Complexity Curve\n\ntrain_sizes, train_scores, test_scores = learning_curve(mlp, X, y)\n\nlearning_curve_df = format_scores_as_dataframe(train_sizes, train_scores, test_scores)\n\n# train and test learning scores results\nax = sns.lineplot(x=\"labels\", y=\"score\", hue=\"type\", data=learning_curve_df, marker=\"o\", ci=None)\nax.set_title(\"Learning Curve for MLP Algorithm\")\ndev_null = ax.set(xlabel=\"Samples\", ylabel=\"Error\")","038ebaa7":"scores = cross_val_score(mlp, X, y)\n\nscores, scores.mean(), scores.std()\n\ndev_null = sns.lineplot(x=[1,2,3,4,5], y=scores)\ndev_null.set_title(\"Cross Score Distribution\")\ndev_null = dev_null.set(xlabel=\"# of runs\", ylabel=\"Accuracy\")","86383cb5":"cross_val_result = cross_validate(mlp, X, y, return_train_score=True)\n\ntrain_scores, test_scores = validation_curve(mlp, X, y, param_name=\"alpha\", param_range=[0.1, 5, 10])\n#train_scores, test_scores = validation_curve(mlp, X, y, param_name=\"hidden_layer_sizes\", param_range=([2], [7], [2,2], [7, 2], [10,7,2]))\n\nval_curve_data = {\"labels\": [], \"type\": [], \"scores\": []}\nparam_ranges = [\"[2]\", \"[7]\", \"[2,2]\", \"[7,2]\", \"[10,7,2]\"]\n\nfor i in range(len(train_scores)):\n    for j in range(len(train_scores[i])):\n        val_curve_data[\"labels\"].append(param_ranges[i])\n        val_curve_data[\"type\"].append(\"train\")\n        val_curve_data[\"scores\"].append(train_scores[i][j])\n        val_curve_data[\"labels\"].append(param_ranges[i])\n        val_curve_data[\"type\"].append(\"test\")\n        val_curve_data[\"scores\"].append(test_scores[i][j])\n        \nval_curve_df = pd.DataFrame.from_dict(val_curve_data)\n\nax = sns.lineplot(x=\"labels\", y=\"scores\", hue=\"type\", data = val_curve_df, marker=\"o\", ci=None)\nax.set_title(\"Validation Curve for our MLP model\")\ndev_null = ax.set(xlabel=\"Layers\/Neurons\", ylabel=\"Accuracy Score\")","53e01198":"confusion_mtrx = confusion_matrix(y_test, y_predicted)\nclassification_rprt = classification_report(y_test, y_predicted)\naccuracy_scr = accuracy_score(y_test, y_predicted)\n# TN FP\n# FN TP\nprint(\"Confusion Matrix\")\nprint(confusion_mtrx)\nprint(\"Classification Report\")\nprint(classification_rprt)\nprint(\"Accuracy\")\nprint(accuracy_scr)","cfe9e301":"# Computing AUC score\nroc = roc_auc_score(y_test, y_predicted)\ndev_null = plot_roc_curve(mlp, X_test, y_test, name=\"AUC\/ROC Curve for MLP\")","ee7ede98":"Reading scores are more skewed towards 70%. ","406e9949":"Running this overnight yielded the following configuration as the best one: MLPClassifier(activation='identity', hidden_layer_sizes=[2], max_iter=3000). We will therefore run a model and analysis for this configuration as well. ","ffa19554":"### GridSearchCV","7e38970a":"### Implement MLP Classifier","680e5a7b":"Check data for empty values","f7ff7973":"### Reading Data","2f467336":"oneHotEncoder is used to encode categiorical columns into values that can be digested by the used algorithm implementation, in our case it.","86922789":"### Math test scores for students by group. Male and Female labeled.\nAs we can see members of group E seem to do the best in the test. Males seem to do slightly better on these tests. ","db3e4fa2":"### Writing test scores for students by group. Male and Female labeled.\nAgain members on the group E did best, followed by group D. In this case however females seem to do better. Overall scores seem to be sligthly lower than reading tests","976eb1c3":"### Compute cross-validation curve","12f7d3ad":"Preparing data","fe8693ed":"As we can see there is some variation among all the groups, and even some outliers. The goal then, is to find what causes these variations with respect to the other fields in the data. \n\nThose are the categories that we should put emphasis on for further analysis and modeling. ","8aa6425c":"It seems, even if by a small degree, that parental level of education has an impact on the student core, improving it slightly when parents of the student attain a higher level of education.","c1a1d00c":"The heatmap above illustrates the relationship between the groups in the data vs parental education.","e006caa3":"This yields similar results as the reading exams, but seems to be more skewed towards the median at 72%.","7469d9d7":"What if we check the relationship between race\/ethnicity and parental education level. Will this answer the question of what causes group E to do better than any other group.","218a8804":"### Compute learning curve","a00fa0e4":"The curve above shows the cross-validation scores for the default 5 runs in the cross-validation process for the MLP model.","4b5f142d":"Our data seems to be clean of missing values. \ud83d\udc4d","7a202e83":"## Exploring MLP with different characteristcics\nLet us check the MLP using logistic as activation function.","bb29db23":"The higher the area under the curve for this graph the better the model is in predicting values for a specific domain. In this graph seveal runs are made and accuracy measured.","324bbd9e":"As we can see in this graph, there is a normal distribution skewed towards ~63% for math test scores.","24bbc9da":"Learning curve is a measurement to check how well the model learns. This is measured by taking a reading of the accuracy of the algorithm as it trains and also while it is testing. This are plotting to see the convergence.","ada9a673":"# MLP Model To Predict P\/F of Students","648873a3":"A grid search will help us determine the optimal configurations to run our models. ","d451d81e":"Cross validation is a measure of how well our model can generalize from what it learns. How well will it perform with data it has neven seen before. This is done by saving part of the data to later predict and measure the accuracy. The training data is split with differing testing folds to be used. Default in this case is k=5 folds.","2f45e540":"The MLP configured above will iterate 3000 times, use hidden layers and 17, 13, 7, solver stochastic gradient descent. Our data was devided into a 80\/20 train\/set splits to train and evaluate your classifier. ","3dd42ede":"The confusion matrix shows the frequency for True Positives, True Negatives, False Positives, and False Negative. Also a summary of the different properties can be presented here, along with the accuracy for predicted values.","fd1f523b":"### Compute confusion matrix","6e3c3071":"Overall it doesn't seem that fail or passing scores in any of the tests can be completely predicted from any of the categories grouped above. Unless there is some variation. Let's see if we can catch that using boxplot:","ae6baa05":"### AUC curve","b52395a9":"Compiling number of categories in the data.","b5523868":"A boxplot graph can help see more clearly the distribution of students of different group.","557d7685":"### Reading test scores for students by group. Male and Female labeled.\nAgain members on the group E did best, followed by group D. In this case however females seem to do better. ","2470164f":"### Running Grid Search suggested model.\nMLPClassifier(activation='identity', hidden_layer_sizes=[2], max_iter=3000), very simplistic as you can see."}}