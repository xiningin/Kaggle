{"cell_type":{"33570c20":"code","859791c9":"code","559dd04d":"code","094d6468":"code","6b099670":"code","02e3dace":"code","18d02329":"code","9b9092b6":"code","e34dbe40":"code","b4cfa981":"code","eaafb03e":"code","36d11623":"code","84b9d074":"code","58c4c523":"code","457474c0":"code","81355be9":"code","faa52872":"code","1d7357ff":"code","73f09478":"code","9575516a":"code","f6c1f037":"code","71f88dc3":"code","20cddfb3":"code","2b0c50ef":"code","ced5416e":"code","fdf410ff":"code","852f9a26":"code","03d93aab":"code","465d2ca6":"code","f0819ff2":"code","87ae8077":"code","1371924a":"code","e65268f6":"code","7b06a61e":"code","0187cea8":"code","f61a06ae":"code","2e944135":"code","a1acd5c5":"code","f8f51c18":"code","a8db7d5f":"code","e6511a71":"code","45b9a5e0":"code","882d344a":"code","03e6b286":"code","fc8d2e69":"code","cd9edfa4":"code","cd815fdf":"code","3088b524":"code","0a5d1bd9":"code","88d9a362":"code","abe48215":"code","b4983b1f":"code","a7546b7d":"code","57a9748a":"code","92620ff7":"code","93ffaa13":"code","3173291f":"code","cb94b2c3":"code","94f13916":"code","c499dc36":"markdown","363b780c":"markdown","5a7d3a46":"markdown","e73520b6":"markdown","99b77287":"markdown","430ddfb0":"markdown","ac535cee":"markdown","a48857b7":"markdown","265ec9d7":"markdown","9de7eb69":"markdown","7cc8ca2d":"markdown","98f42dd0":"markdown","1346bb61":"markdown","9f195427":"markdown","ea7b8737":"markdown","4ba1cb8d":"markdown","757d73d9":"markdown","60a94bbd":"markdown","de60bbb7":"markdown","66cd1483":"markdown","2aac0e47":"markdown","8c55a492":"markdown","c1f2a421":"markdown","816a928c":"markdown"},"source":{"33570c20":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.express as px\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nsns.set_style('darkgrid')\nmatplotlib.rcParams['font.size'] = 14\nmatplotlib.rcParams['figure.figsize'] = (10, 6)\nmatplotlib.rcParams['figure.facecolor'] = '#00000000'\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","859791c9":"import warnings\n\nwarnings.filterwarnings('ignore')","559dd04d":"raw_df = pd.read_csv('\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv')","094d6468":"raw_df","6b099670":"raw_df.info()","02e3dace":"raw_df.describe()","18d02329":"raw_df.dropna(subset=['RainToday', 'RainTomorrow'], inplace=True)","9b9092b6":"px.histogram(raw_df, x='Location', title='Location vs. Rainy Days', color='RainToday')","e34dbe40":"px.histogram(raw_df, \n             x='RainTomorrow', \n             color='RainToday', \n             title='Rain Tomorrow vs. Rain Today')","b4cfa981":"#setting target column\ny = raw_df.RainTomorrow\n\n#removing targets from predictors\n\nX = raw_df.drop(['RainTomorrow'], axis=1)","eaafb03e":"year = pd.to_datetime(raw_df.Date).dt.year\n\ntrain_inputs = X[year < 2015]\nval_inputs = X[year == 2015]\ntest_inputs = X[year > 2015]","36d11623":"train_targets = y[year < 2015]\nval_targets = y[year == 2015]\ntest_targets = y[year > 2015]","84b9d074":"numeric_cols = train_inputs.select_dtypes(include = 'float64').columns.to_list()\ncat_cols = train_inputs.select_dtypes(include = 'object').columns.to_list()[1:]","58c4c523":"# checking number of missing values\nX[numeric_cols].isna().sum()","457474c0":"from sklearn.impute import SimpleImputer","81355be9":"#create an imputer\nimputer = SimpleImputer (strategy = 'mean')\n\n#fit the imputer in data\nimputer.fit(X[numeric_cols])","faa52872":"# filling missing values in train, val and test data\ntrain_inputs[numeric_cols] = imputer.transform(train_inputs[numeric_cols])\nval_inputs[numeric_cols] = imputer.transform(val_inputs[numeric_cols])\ntest_inputs[numeric_cols] = imputer.transform(test_inputs[numeric_cols])","1d7357ff":"train_inputs[numeric_cols].isna().sum()","73f09478":"X[numeric_cols].describe()","9575516a":"from sklearn.preprocessing import MinMaxScaler","f6c1f037":"scaler = MinMaxScaler()\nscaler.fit(X[numeric_cols])","71f88dc3":"train_inputs[numeric_cols] = scaler.transform(train_inputs[numeric_cols])\nval_inputs[numeric_cols] = scaler.transform(val_inputs[numeric_cols])\ntest_inputs[numeric_cols] = scaler.transform(test_inputs[numeric_cols])","20cddfb3":"train_inputs[numeric_cols].describe()","2b0c50ef":"X[cat_cols].nunique()","ced5416e":"from sklearn.preprocessing import OneHotEncoder","fdf410ff":"encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')","852f9a26":"X[cat_cols].isna().sum()","03d93aab":"X2 = X[cat_cols].fillna('None')","465d2ca6":"encoder.fit(X2)","f0819ff2":"encoder.categories_","87ae8077":"encoded_cols = list(encoder.get_feature_names(cat_cols))","1371924a":"train_inputs[encoded_cols] = encoder.transform(train_inputs[cat_cols].fillna('None'))\nval_inputs[encoded_cols] = encoder.transform(val_inputs[cat_cols].fillna('None'))\ntest_inputs[encoded_cols] = encoder.transform(test_inputs[cat_cols].fillna('None'))","e65268f6":"from sklearn.linear_model import LogisticRegression","7b06a61e":"model = LogisticRegression(solver ='liblinear', C=15, fit_intercept= False)","0187cea8":"model.fit(train_inputs[numeric_cols + encoded_cols], train_targets)","f61a06ae":"pd.DataFrame({'feature': numeric_cols+encoded_cols,\n            'weight': model.coef_.tolist()[0]})","2e944135":"print(model.intercept_)","a1acd5c5":"X_train = train_inputs[numeric_cols + encoded_cols]\nX_val = val_inputs[numeric_cols + encoded_cols]\nX_test = test_inputs[numeric_cols + encoded_cols]","f8f51c18":"from sklearn.metrics import accuracy_score","a8db7d5f":"from sklearn.metrics import confusion_matrix","e6511a71":"def predict_and_plot(inputs, targets, name=''):\n    preds = model.predict(inputs)\n    \n    accuracy = accuracy_score(targets, preds)\n    print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n    \n    cf = confusion_matrix(targets, preds, normalize='true')\n    plt.figure()\n    sns.heatmap(cf, annot=True)\n    plt.xlabel('Prediction')\n    plt.ylabel('Target')\n    plt.title('{} Confusion Matrix'.format(name));\n    \n    return preds","45b9a5e0":"train_preds = predict_and_plot(X_train, train_targets, 'Training')","882d344a":"val_preds = predict_and_plot(X_val, val_targets, 'Validatiaon')","03e6b286":"test_preds = predict_and_plot(X_test, test_targets, 'Test')","fc8d2e69":"from sklearn.linear_model import RidgeClassifier","cd9edfa4":"model2 = RidgeClassifier(alpha = 10, random_state= 10)","cd815fdf":"model2.fit(X_train, train_targets)","3088b524":"#validation score\nmodel2.score(X_val, val_targets)","0a5d1bd9":"#test score\nmodel2.score(X_test, test_targets)","88d9a362":"from sklearn.tree import DecisionTreeClassifier","abe48215":"model3 = DecisionTreeClassifier( random_state=42, max_leaf_nodes=128, max_depth=10, )","b4983b1f":"model3.fit(X_train, train_targets)","a7546b7d":"model3.score(X_val, val_targets)","57a9748a":"model3.score(X_test, test_targets)","92620ff7":"from sklearn.ensemble import RandomForestClassifier","93ffaa13":"model4 = RandomForestClassifier(n_jobs=-1, \n                               random_state=42, \n                               n_estimators=500,\n                               max_features=7,\n                               max_depth=30, \n                               class_weight={'No': 1, 'Yes': 1.5})","3173291f":"model4.fit(X_train, train_targets)","cb94b2c3":"#validation score\nmodel4.score(X_val, val_targets)","94f13916":"#Test score\nmodel4.score(X_test, test_targets)","c499dc36":"# 2. Problem Statement\nIn this notebook we will learn how to implement how to apply logistic regression in a real world data. We are using open kaglle data set named, \"Rain in Australia\". It is a 10 year historical rainfall data that comes in csv file format. After doing primary data exploration, we will try to predict next day rainfall for different regions in australia. ","363b780c":"We can see that this dataset contains over 145,000 rows and 23 columns including date, numeric and categorical columns. We will create a model which will predict the RainTomorrow by using other features.\n\n","5a7d3a46":"# Training Model with Decision Tree Classifier","e73520b6":"# Training model with Random forest classifier","99b77287":"# Scaling Numeric Features","430ddfb0":"# Splitting data into train,validation & test data","ac535cee":"## Imputing Missing Values","a48857b7":"# Finding Numerical & Categorical Columns","265ec9d7":"The stats seem reasonable. So, no data cleaning is required at this time. ","9de7eb69":"# Dealing with Missing numerical data","7cc8ca2d":"It is clear that either it will rain tomorrow is greatly dependent on the rainfall today","98f42dd0":"# 1. Import Necessary Libraries","1346bb61":"# Training Model with Ridge Classifier","9f195427":"# Training Model with Logistic Regression","ea7b8737":"# Handling Categorical Variables","4ba1cb8d":"# 3. Loading Dataset\nWe will first load and import the dataset to work with.","757d73d9":"## Confusion Metrics","60a94bbd":"We will now drop rows of missing values in Raintoday and Raintomorrwo columns. ","de60bbb7":"# Prediction & Model Evaluation","66cd1483":"There are significatn amount of missing values in the dataset especially in columns like \"sunshine\". \"Evaporation\",\"Cloud3pm\", \"cloud9pm\". It is also observed that there are different types of data type like object, float64, int64.\n\nLets look at the basic statistics of the data","2aac0e47":"We will use one hot encoding to the categorical variables","8c55a492":"We can see that values in each column lie is in the (0,1) range \n","c1f2a421":"# 4. Exploratory Data Analysis with Visualisation\n\nIn this step, we would analyze the data set, learn the statistics and correlation between different features","816a928c":"Now lets visulize the dataset and correlations by plotting some graphs"}}