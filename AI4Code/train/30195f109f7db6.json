{"cell_type":{"c17416fb":"code","2b55c96f":"code","b5898016":"code","d0adfbc2":"code","f36ad745":"code","27ebf6d7":"code","b738f9ed":"code","b8b84da8":"code","c2bcd303":"code","9b9bbfa1":"code","1dea3d34":"code","7ef7767c":"code","63bc9319":"code","44b35d20":"code","9334c1b1":"code","44731f07":"code","aa303665":"code","71b75fb8":"code","5c8b086c":"code","e2dc4bd1":"code","952222e4":"code","2dcd9d23":"code","671f30b4":"code","a6c9cf4c":"code","339e25d6":"code","7cebcc4a":"code","dca2eb37":"code","21e47c43":"code","f7affd51":"code","8244c7e7":"code","b6caab29":"code","39878c43":"code","a4d0550b":"code","f59f441a":"code","6984e2eb":"code","45b5934e":"code","b799e04d":"code","95f77d4e":"code","1e2d04f6":"code","c981491f":"code","92ade59e":"code","63a50045":"markdown","fb2187cb":"markdown","72e3c2e4":"markdown","b7f6f492":"markdown","1ade3fba":"markdown","4c2c649b":"markdown","0b7181ae":"markdown","37058156":"markdown"},"source":{"c17416fb":"import math\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F \nimport torchtext\nimport torch.optim as optim","2b55c96f":"# Setup seeds\ntorch.manual_seed(1234)\nnp.random.seed(1234)\nrandom.seed(1234)","b5898016":"class PositionalEncoder(nn.Module):\n    '''\uc785\ub825\ub41c \ub2e8\uc5b4\uc758 \uc704\uce58\ub97c \uc6d0\ubc31\ud130\uc815\ubcf4\uc5d0 \ub354\ud55c\ub2e4'''\n    def __init__(self, position, d_model):\n        super().__init__()\n\n        self.d_model = d_model  # \ub2e8\uc5b4 \ubc31\ud130\uc758 \uc6d0\ub798 \ucc28\uc6d0 \uc218\n\n        # \uc785\ub825 \ubb38\uc7a5\uc5d0\uc11c\uc758 \uc784\ubca0\ub529 \ubca1\ud130\uc758 \uc704\uce58\uff08pos\uff09\uc784\ubca0\ub529 \ubca1\ud130 \ub0b4\uc758 \ucc28\uc6d0\uc758 \uc778\ub371\uc2a4\uff08i\uff09\n        pe = torch.zeros(position, d_model)\n\n        # \ud559\uc2b5\uc2dc\uc5d0\ub294 GPU \uc0ac\uc6a9\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        pe = pe.to(device)\n\n        for pos in range(position):\n            for i in range(0, d_model, 2):\n                pe[pos, i] = math.sin(pos \/ (10000 ** ((2 * i)\/d_model)))\n                pe[pos, i + 1] = math.cos(pos \/\n                                          (10000 ** ((2 * i)\/d_model)))\n\n        # pe\uc758 \uc120\ub450\uc5d0 \ubbf8\ub2c8\ubc30\uce58 \ucc28\uc6d0\uc744 \ucd94\uac00\ud55c\ub2e4\n        self.pe = pe.unsqueeze(0)\n\n        self.pe.requires_grad = False\n\n    def forward(self, x):\n        # \uc785\ub825x\uc640 Positonal Encoding\uc744 \ub354\ud55c\ub2e4\n        ret = math.sqrt(self.d_model)*x + self.pe[:, :x.size(1)]\n        return ret\n","d0adfbc2":"def scaled_dot_product_attention(query, key, value, mask):\n  # query \ud06c\uae30 : (batch_size, num_heads, query\uc758 \ubb38\uc7a5 \uae38\uc774, d_model\/num_heads)\n  # key \ud06c\uae30 : (batch_size, num_heads, key\uc758 \ubb38\uc7a5 \uae38\uc774, d_model\/num_heads)\n  # value \ud06c\uae30 : (batch_size, num_heads, value\uc758 \ubb38\uc7a5 \uae38\uc774, d_model\/num_heads)\n  # padding_mask : (batch_size, 1, 1, key\uc758 \ubb38\uc7a5 \uae38\uc774)\n\n  # Q\uc640 K\uc758 \uacf1. \uc5b4\ud150\uc158 \uc2a4\ucf54\uc5b4 \ud589\ub82c.\n  matmul_qk = torch.matmul(query, torch.transpose(key,2,3))\n\n  # \uc2a4\ucf00\uc77c\ub9c1\n  # dk\uc758 \ub8e8\ud2b8\uac12\uc73c\ub85c \ub098\ub220\uc900\ub2e4.\n  depth = key.shape[-1]\n  logits = matmul_qk \/ math.sqrt(depth)\n\n  # \ub9c8\uc2a4\ud0b9. \uc5b4\ud150\uc158 \uc2a4\ucf54\uc5b4 \ud589\ub82c\uc758 \ub9c8\uc2a4\ud0b9 \ud560 \uc704\uce58\uc5d0 \ub9e4\uc6b0 \uc791\uc740 \uc74c\uc218\uac12\uc744 \ub123\ub294\ub2e4.\n  # \ub9e4\uc6b0 \uc791\uc740 \uac12\uc774\ubbc0\ub85c \uc18c\ud504\ud2b8\ub9e5\uc2a4 \ud568\uc218\ub97c \uc9c0\ub098\uba74 \ud589\ub82c\uc758 \ud574\ub2f9 \uc704\uce58\uc758 \uac12\uc740 0\uc774 \ub41c\ub2e4.\n  if mask is not None:\n    logits += (mask * -1e9)\n\n  # \uc18c\ud504\ud2b8\ub9e5\uc2a4 \ud568\uc218\ub294 \ub9c8\uc9c0\ub9c9 \ucc28\uc6d0\uc778 key\uc758 \ubb38\uc7a5 \uae38\uc774 \ubc29\ud5a5\uc73c\ub85c \uc218\ud589\ub41c\ub2e4.\n  # attention weight : (batch_size, num_heads, query\uc758 \ubb38\uc7a5 \uae38\uc774, key\uc758 \ubb38\uc7a5 \uae38\uc774)\n  attention_weights = F.softmax(logits, dim=-1)\n\n  # output : (batch_size, num_heads, query\uc758 \ubb38\uc7a5 \uae38\uc774, d_model\/num_heads)\n  output = torch.matmul(attention_weights, value)\n\n  return output, attention_weights","f36ad745":"class MultiheadAttention(nn.Module):\n\n    def __init__(self, d_model, num_heads):\n        super(MultiheadAttention, self).__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n\n        assert d_model % self.num_heads == 0\n        # d_model\uc744 num_heads\ub85c \ub098\ub208 \uac12.\n        self.depth = int(d_model\/self.num_heads)\n\n        # WQ, WK, WV\uc5d0 \ud574\ub2f9\ud558\ub294 \ubc00\uc9d1\uce35 \uc815\uc758\n        self.q_linear = nn.Linear(d_model, d_model)\n        self.v_linear = nn.Linear(d_model, d_model)\n        self.k_linear = nn.Linear(d_model, d_model)\n        # WO\uc5d0 \ud574\ub2f9\ud558\ub294 \ubc00\uc9d1\uce35 \uc815\uc758\n        self.out = nn.Linear(d_model, d_model)\n\n\n    # num_heads \uac1c\uc218\ub9cc\ud07c q, k, v\ub97c split\ud558\ub294 \ud568\uc218\n    def split_heads(self, inputs, batch_size):\n      inputs = torch.reshape(\n          inputs, (batch_size, -1, self.num_heads, self.depth))\n      return torch.transpose(inputs, 1,2)\n\n    def forward(self, inputs):\n        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n        batch_size = query.shape[0]\n        # 1. WQ, WK, WV\uc5d0 \ud574\ub2f9\ud558\ub294 \ubc00\uc9d1\uce35 \uc9c0\ub098\uae30\n        # q : (batch_size, query\uc758 \ubb38\uc7a5 \uae38\uc774, d_model)\n        # k : (batch_size, key\uc758 \ubb38\uc7a5 \uae38\uc774, d_model)\n        # v : (batch_size, value\uc758 \ubb38\uc7a5 \uae38\uc774, d_model)\n        query = self.q_linear(query)\n        key = self.k_linear(key)\n        value = self.v_linear(value)\n\n\n        # 2. \ud5e4\ub4dc \ub098\ub204\uae30\n        # q : (batch_size, num_heads, query\uc758 \ubb38\uc7a5 \uae38\uc774, d_model\/num_heads)\n        # k : (batch_size, num_heads, key\uc758 \ubb38\uc7a5 \uae38\uc774, d_model\/num_heads)\n        # v : (batch_size, num_heads, value\uc758 \ubb38\uc7a5 \uae38\uc774, d_model\/num_heads)\n        query = self.split_heads(query, batch_size)\n        key = self.split_heads(key, batch_size)\n        value = self.split_heads(value, batch_size)\n\n\n        # 3. \uc2a4\ucf00\uc77c\ub4dc \ub2f7 \ud504\ub85c\ub355\ud2b8 \uc5b4\ud150\uc158. \uc55e\uc11c \uad6c\ud604\ud55c \ud568\uc218 \uc0ac\uc6a9.\n        # (batch_size, num_heads, query\uc758 \ubb38\uc7a5 \uae38\uc774, d_model\/num_heads)\n        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n        # (batch_size, query\uc758 \ubb38\uc7a5 \uae38\uc774, num_heads, d_model\/num_heads)\n        scaled_attention = torch.transpose(scaled_attention, 1,2)\n\n        # 4. \ud5e4\ub4dc \uc5f0\uacb0(concatenate)\ud558\uae30\n        # (batch_size, query\uc758 \ubb38\uc7a5 \uae38\uc774, d_model)\n        concat_attention = torch.reshape(scaled_attention,\n                                      (batch_size, -1, self.d_model))\n\n        # 5. WO\uc5d0 \ud574\ub2f9\ud558\ub294 \ubc00\uc9d1\uce35 \uc9c0\ub098\uae30\n        # (batch_size, query\uc758 \ubb38\uc7a5 \uae38\uc774, d_model)\n        outputs = self.out(concat_attention)\n        return outputs","27ebf6d7":"class FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(FeedForward, self).__init__()\n        self.linear_1 = nn.Linear(d_model, d_ff)\n        self.linear_2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, attention):\n        outputs = self.linear_1(attention)\n        outputs = F.relu(outputs)\n        outputs = self.linear_2(outputs)\n        return outputs","b738f9ed":"class EncoderBlock(nn.Module):\n  def __init__(self, d_ff, d_model, num_heads, dropout):\n    super(EncoderBlock, self).__init__()\n    \n    self.attn = MultiheadAttention(d_model, num_heads)\n    self.dropout_1 = nn.Dropout(dropout)\n    self.norm_1 = nn.LayerNorm(d_model)\n    self.ff = FeedForward(d_model, d_ff)\n    self.dropout_2 = nn.Dropout(dropout)\n    self.norm_2 = nn.LayerNorm(d_model)\n\n  def forward(self, inputs, padding_mask):\n    attention = self.attn({'query': inputs, 'key': inputs, 'value': inputs, 'mask': padding_mask})\n    attention = self.dropout_1(attention)\n    attention = self.norm_1(inputs + attention)\n    outputs = self.ff(attention)\n    outputs = self.dropout_2(outputs)\n    outputs = self.norm_2(attention + outputs)\n\n    return outputs\n","b8b84da8":"class Encoder(nn.Module):\n  def __init__(self,text_embedding_vectors, vocab_size, num_layers, d_ff, d_model, num_heads, dropout):\n    super(Encoder, self).__init__()\n    self.vocab_size = vocab_size\n    self.d_model = d_model\n    self.num_layers = num_layers\n    self.embb = nn.Embedding(text_embedding_vectors, d_model)\n    self.dropout_1 = nn.Dropout(dropout)\n    self.PE = PositionalEncoder(vocab_size, d_model)\n    self.encoder_block = EncoderBlock(d_ff, d_model, num_heads, dropout)\n  def forward(self, x, padding_mask):\n    emb = self.embb(x)\n    emb *= math.sqrt(self.d_model)\n    emb = self.PE(emb)\n    output = self.dropout_1(emb)\n\n    for i in range(self.num_layers):\n      output = self.encoder_block(output, padding_mask)\n\n    return output\n","c2bcd303":"class DecoderBlock(nn.Module):\n  def __init__(self, d_ff, d_model, num_heads, dropout):\n    super(DecoderBlock, self).__init__()\n    \n    self.attn = MultiheadAttention(d_model, num_heads)\n    self.attn_2 = MultiheadAttention(d_model, num_heads)\n    self.dropout_1 = nn.Dropout(dropout)\n    self.norm_1 = nn.LayerNorm(d_model)\n    self.ff = FeedForward(d_model, d_ff)\n    self.dropout_2 = nn.Dropout(dropout)\n    self.dropout_3 = nn.Dropout(dropout)\n    self.norm_2 = nn.LayerNorm(d_model)\n    self.norm_3 = nn.LayerNorm(d_model)\n\n  def forward(self, inputs, enc_outputs, padding_mask, look_ahead_mask):\n    attention1 = self.attn({'query': inputs, 'key': inputs, 'value': inputs, 'mask': look_ahead_mask})\n    attention1 = self.norm_1(inputs + attention1)\n    attention2 = self.attn_2({'query': attention1, 'key': enc_outputs, 'value': enc_outputs, 'mask': padding_mask})\n    attention2 = self.dropout_1(attention2)\n    attention2 = self.norm_2(attention1 + attention2)\n\n    outputs = self.ff(attention2)\n    outputs = self.dropout_3(outputs)\n    outputs = self.norm_3(attention2 + outputs)\n\n    return outputs  ","9b9bbfa1":"class Decoder(nn.Module):\n  def __init__(self,text_embedding_vectors,  vocab_size, num_layers, d_ff, d_model, num_heads, dropout):\n    super(Decoder, self).__init__()\n    self.vocab_size = vocab_size\n    self.d_model = d_model\n    self.num_layers = num_layers\n    self.embb = nn.Embedding(text_embedding_vectors, d_model)\n    self.dropout_1 = nn.Dropout(dropout)\n    self.PE = PositionalEncoder(vocab_size, d_model)\n    self.decoder_block = DecoderBlock(d_ff, d_model, num_heads, dropout)\n  def forward(self, enc_output, dec_input, padding_mask, look_ahead_mask):\n    emb = self.embb(dec_input)\n    emb *= math.sqrt(self.d_model)\n    emb = self.PE(emb)\n    output = self.dropout_1(emb)\n    for i in range(self.num_layers):\n      output = self.decoder_block(output, enc_output, padding_mask, look_ahead_mask)\n\n    return output\n","1dea3d34":"class transformer(nn.Module):\n    def __init__(self, text_embedding_vectors, vocab_size, num_layers, d_ff, d_model, num_heads, dropout):\n        self.vocab_size = vocab_size\n        super(transformer, self).__init__()\n        self.enc_outputs = Encoder(text_embedding_vectors, vocab_size, num_layers, d_ff, d_model, num_heads, dropout)\n        self.dec_outputs = Decoder(text_embedding_vectors, vocab_size, num_layers, d_ff, d_model, num_heads, dropout)\n        self.output = nn.Linear(d_model, text_embedding_vectors)\n        self.softmax = nn.LogSoftmax(dim=-1)\n\n    def forward(self, input, dec_input):\n        enc_input = input\n        dec_input = dec_input\n        enc_padding_mask = create_padding_mask(enc_input)\n        dec_padding_mask = create_padding_mask(enc_input)\n        look_ahead_mask = create_look_ahead_mask(dec_input)\n    \n        enc_output = self.enc_outputs(enc_input, enc_padding_mask)\n        dec_output = self.dec_outputs(enc_output, dec_input, dec_padding_mask, look_ahead_mask)\n        output = self.output(dec_output)\n        return output","7ef7767c":"import pandas as pd\nimport re\nimport urllib.request\nimport time","63bc9319":"#urllib.request.urlretrieve(\"https:\/\/raw.githubusercontent.com\/songys\/Chatbot_data\/master\/ChatbotData%20.csv\", filename=\"ChatBotData.csv\")\nimport os\nfile_list = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        file_list.append(os.path.join(dirname, filename))\ntrain_data = pd.read_csv(file_list[0])\ntrain_data.head()","44b35d20":"from torchtext import data, datasets\nimport os","9334c1b1":"#https:\/\/github.com\/lovit\/soynlp\n!pip install soynlp","44731f07":"from soynlp.tokenizer import LTokenizer","aa303665":"tokenizer = LTokenizer()","71b75fb8":"tokenizer(\"\ub0b4\uc77c \uc5ed \uc55e\uc758 \uc2dd\ub2f9\uc5d0\uc11c \ubc25 \uba39\uc73c\ub7ec \ub098\uac08\ub798 ?\")","5c8b086c":"VOCAB_SIZE = 40","e2dc4bd1":"Q = data.Field(\n    sequential=True,\n    use_vocab=True,\n    lower=True,\n    tokenize=tokenizer,\n    batch_first=True,\n    init_token=\"<SOS>\",\n    eos_token=\"<EOS>\",\n    fix_length=VOCAB_SIZE\n)\n\nA = data.Field(\n    sequential=True,\n    use_vocab=True,\n    lower=True,\n    tokenize=tokenizer,\n    batch_first=True,\n    init_token=\"<SOS>\",\n    eos_token=\"<EOS>\",\n    fix_length=VOCAB_SIZE\n)","952222e4":"trainset = data.TabularDataset(\n        path=file_list[0], format='csv', skip_header=False,\n        fields=[('Q', Q),('A', A)])","2dcd9d23":"print(vars(trainset[2]))","671f30b4":"print('\ud6c8\ub828 \uc0d8\ud50c\uc758 \uac1c\uc218 : {}'.format(len(trainset)))","a6c9cf4c":"Q.build_vocab(trainset.Q, trainset.A, min_freq = 2) # \ub2e8\uc5b4 \uc9d1\ud569 \uc0dd\uc131\nA.vocab = Q.vocab# \ub2e8\uc5b4 \uc9d1\ud569 \uc0dd\uc131","339e25d6":"PAD_TOKEN, START_TOKEN, END_TOKEN, UNK_TOKEN = Q.vocab.stoi['<pad>'], Q.vocab.stoi['<SOS>'], Q.vocab.stoi['<EOS>'], Q.vocab.stoi['<unk>']","7cebcc4a":"#Difine HyperParameter\nVOCAB_SIZE = VOCAB_SIZE\ntext_embedding_vectors = len(Q.vocab)\nNUM_LAYERS = 4\nD_FF = 512\nD_MODEL = 128\nNUM_HEADS = 4\nDROPOUT = 0.3\nBATCH_SIZE=64","dca2eb37":"# Define Iterator\n# train_iter batch has text and target item\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iter = data.BucketIterator(\n        trainset, batch_size=BATCH_SIZE,\n        shuffle=True, repeat=False, sort=False, device = device)","21e47c43":"# \ubaa8\ub378 \uad6c\ucd95\nprint(text_embedding_vectors)\nnet = transformer(text_embedding_vectors = text_embedding_vectors, \n                  vocab_size=VOCAB_SIZE, num_layers=NUM_LAYERS, d_ff=D_FF, d_model=D_MODEL, \n                  num_heads=NUM_HEADS, dropout=DROPOUT)\n\n# \ub124\ud2b8\uc6cc\ud06c \ucd08\uae30\ud654\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        # Liner\uce35\uc758 \ucd08\uae30\ud654\n        nn.init.kaiming_normal_(m.weight)\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0.0)\n\n# \ud6c8\ub828 \ubaa8\ub4dc \uc124\uc815\nnet.train()\n\n# TransformerBlock\ubaa8\ub4c8\uc758 \ucd08\uae30\ud654 \uc124\uc815\nnet.apply(weights_init)\n\n\nprint('\ub124\ud2b8\uc6cc\ud06c \ucd08\uae30\ud654 \uc644\ub8cc')","f7affd51":"# \uc190\uc2e4 \ud568\uc218\uc758 \uc815\uc758\ncriterion = nn.CrossEntropyLoss()\n\n# \ucd5c\uc801\ud654 \uc124\uc815\nlearning_rate = 2e-4\noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)","8244c7e7":"def create_padding_mask(x):\n  input_pad = 0\n  mask = (x == input_pad).float()\n  mask = mask.unsqueeze(1).unsqueeze(1)\n  # (batch_size, 1, 1, key\uc758 \ubb38\uc7a5 \uae38\uc774)\n  return mask","b6caab29":"def create_look_ahead_mask(x):\n  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n  seq_len = x.shape[1]\n  look_ahead_mask = torch.ones(seq_len, seq_len)\n  look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1).to(device)\n\n  padding_mask = create_padding_mask(x).to(device) # \ud328\ub529 \ub9c8\uc2a4\ud06c\ub3c4 \ud3ec\ud568\n  return torch.maximum(look_ahead_mask, padding_mask)","39878c43":"from IPython.display import clear_output\nimport datetime\n# \ud559\uc2b5 \uc815\uc758\ndef train_model(net, train_iter, criterion, optimizer, num_epochs):\n    start_time = time.time()\n\n    ntokens = len(Q.vocab.stoi)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"\uc0ac\uc6a9 \ub514\ubc14\uc774\uc2a4:\", device)\n    print('-----start-------')\n    net.to(device)\n    epoch_ = []\n    epoch_train_loss = []\n    # \ub124\ud2b8\uc6cc\ud06c\uac00 \uc5b4\ub290\uc815\ub3c4 \uace0\uc815\ub418\uba74 \uace0\uc18d\ud654\n    torch.backends.cudnn.benchmark = True\n    \n    net.train()\n    # epoch \ub8e8\ud504\n    best_epoch_loss = float(\"inf\")\n    for epoch in range(num_epochs):\n      epoch_loss = 0.0\n      cnt= 0\n      for batch in train_iter:\n          questions = batch.Q.to(device)\n          answers = batch.A.to(device)\n          with torch.set_grad_enabled(True):\n            # Transformer\uc5d0 \uc785\ub825\n            preds = net(questions, answers)\n            pad = torch.LongTensor(answers.size(0), 1).fill_(PAD_TOKEN).to(device)\n            preds_id = torch.transpose(preds,1,2)\n            outputs = torch.cat((answers[:, 1:], pad), -1)\n            optimizer.zero_grad()\n            loss = criterion(preds_id, outputs)  # loss \uacc4\uc0b0\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(net.parameters(), 0.5)\n            optimizer.step()\n            epoch_loss +=loss.item()\n            cnt += 1\n      epoch_loss = epoch_loss \/ cnt\n      if not best_epoch_loss or epoch_loss < best_epoch_loss:\n        if not os.path.isdir(\"snapshot\"):\n            os.makedirs(\"snapshot\")\n        torch.save(net.state_dict(), '.\/snapshot\/transformermodel.pt')\n      \n      epoch_.append(epoch)\n      epoch_train_loss.append(epoch_loss)\n      print('Epoch {0}\/{1} Average Loss: {2}'.format(epoch+1, num_epochs, epoch_loss))\n      clear_output(wait = True)\n    \n    \n    fig = plt.figure(figsize=(8,8))\n    fig.set_facecolor('white')\n    ax = fig.add_subplot()\n\n    ax.plot(epoch_,epoch_train_loss, label='Average loss')\n\n\n    ax.legend()\n    ax.set_xlabel('epoch')\n    ax.set_ylabel('loss')\n\n    plt.show()\n    end_time = time.time() - start_time\n    times = str(datetime.timedelta(seconds=end_time)).split(\".\")\n    print('Finished in {0}'.format(times[0]))","a4d0550b":"num_epochs = 100\ntrain_model(net, train_iter, criterion, optimizer, num_epochs=num_epochs)","f59f441a":"net_trained = transformer(text_embedding_vectors = text_embedding_vectors, vocab_size=VOCAB_SIZE, num_layers=NUM_LAYERS, d_ff=D_FF, d_model=D_MODEL, num_heads=NUM_HEADS, dropout=DROPOUT).to(device)\nnet_trained.load_state_dict(torch.load('.\/snapshot\/transformermodel.pt'))","6984e2eb":"def stoi(vocab, token, max_len):\n  #\n  indices=[]\n  token.extend(['<pad>'] * (max_len - len(token)))\n  for string in token:\n    if string in vocab:\n      i = vocab.index(string)\n    else:\n      i = 0\n    indices.append(i)\n  return torch.LongTensor(indices).unsqueeze(0)\n\ndef itos(vocab, indices):\n  text = []\n  for i in indices.cpu()[0]:\n    if i==1:\n      break\n    else:\n      if i not in [PAD_TOKEN, START_TOKEN, END_TOKEN]:\n          if i != UNK_TOKEN:\n              text.append(vocab[i])\n          else:\n              text.append('??')\n  return \" \".join(text)","45b5934e":"def evaluate(input_sentence):\n    VOCAB_SIZE = 40\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    tokenizer = LTokenizer()\n    token = tokenizer(input_sentence)\n    input = stoi(Q.vocab.itos, token, VOCAB_SIZE).to(device)\n    output = torch.LongTensor(1, 1).fill_(START_TOKEN).to(device)\n    for i in range(VOCAB_SIZE):\n        predictions = net_trained(input, output)\n        predictions = predictions[:, -1:, :]\n                            \n        #                                      PAD, UNK, START \ud1a0\ud070 \uc81c\uc678\n        predicted_id = torch.argmax(predictions[:,:,3:], axis=-1) + 3\n        if predicted_id == END_TOKEN:\n            predicted_id = predicted_id\n            break\n        output = torch.cat((output, predicted_id),-1)\n    return output","b799e04d":"def predict(sentence):\n  out = evaluate(sentence)\n  out_text = itos(Q.vocab.itos, out)\n  print('input = [{0}]'.format(sentence))\n  print('output = [{0}]'.format(out_text))\n  return out_text\n\nout = predict('\uc6b0\ub9ac \ub0b4\uc77c \uac19\uc774 \uc601\ud654 \ubcfc\ub798?')","95f77d4e":"out = predict('\uadf8 \uc601\ud654 \ub108\ubb34 \ubcc4\ub85c\ub354\ub77c')","1e2d04f6":"out = predict('\uc81c\ub300\ub85c \uc791\ub3d9\ud558\ub294\uac70 \ub9de\uc544?')","c981491f":"out = predict('\ub0b4\uc77c \ubb50 \ubb51\uace0 \uc2f6\uc5b4?')","92ade59e":"out = predict('\uc624\ub294\ub370 \uba87\uc2dc\uac04 \uac78\ub824?')","63a50045":"# Train Transformer model","fb2187cb":"# Run Transformer chatbot for real sentence","72e3c2e4":"# \ub370\uc774\ud130 \ud544\ub4dc \uc124\uc815","b7f6f492":"\ud2b8\ub79c\uc2a4\ud3ec\uba38 \ucc57\ubd07 \ubaa8\ub378 \uad6c\ud604\n\n\ud2b8\ub79c\uc2a4\ud3ec\uba38: Seq2seq\uc758 \uad6c\uc870\ub97c \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998 \ub9cc\uc73c\ub85c \uad6c\ud604\ud55c \ubaa8\ub378.\n\n - \uae30\uc874\uc758 Cell\uc744 \uc21c\ucc28\uc801\uc73c\ub85c \ucc98\ub9ac\ud574 \ub098\uac00\uc57c\ud588\ub358 \uae30\uc874 \ubaa8\ub378\ubcf4\ub2e4 \ube60\ub974\uba70, \uc131\ub2a5\ub3c4 \uae30\uc874 \ubaa8\ub378\uc5d0 \ube44\ud574\uc11c \uc88b\uc558\ub2e4\n \n - \ud2b8\ub79c\uc2a4\ud3ec\uba38\uc758 \uc778\ucf54\ub354\uc640 \ub514\ucf54\ub354\ub294 \ub2e4\uc218\uc758 \uc140\ud504\uc5b4\ud150\uc158, \uc5b4\ud150\uc158, \ud53c\ub4dc\ud3ec\uc6cc\ub4dc \uacc4\uce35\uc73c\ub85c \uad6c\uc131\ub418\uc5b4\uc788\ub2e4.\n \n  - \uc140\ud504 \uc5b4\ud150\uc158 :  \uc774\uc804 \uacc4\uce35\uc758 \ucd9c\ub825\uc5d0 \ub300\ud574 \uc5b4\ud150\uc158 \uc5f0\uc0b0\uc744 \uc218\ud589\n  \n  - \uc5b4\ud150\uc158 : \uc778\ucf54\ub354\uc758 \uacb0\uacfc\uc5d0 \ub300\ud574 \uc5b4\ud150\uc158 \uc5f0\uc0b0\uc744 \uc218\ud589\n  \n  - \ud53c\ub4dc \ud3ec\uc6cc\ub4dc : \uc5b4\ud150\uc158 \uacc4\uce35\uc73c\ub85c \ubd80\ud130\uc758 \uacb0\uacfc\ubb3c\uc744 \uc815\ub9ac\n\n\ucc57\ubd07 \ub370\uc774\ud130 : \uc778\uc704\uc801\uc73c\ub85c \ub9cc\ub4e4\uc5b4\uc9c4 \uac04\ub2e8\ud55c \uc9c8\ubb38(Q)\uacfc \ub300\ub2f5(A)\uc758 \uc30d\uc73c\ub85c \uc774\ub8e8\uc5b4\uc9c4 \ub370\uc774\ud130. \uc0d8\ud50c \uc218 11,823\uac1c\n\nTokenizer : soynlp\nTorchtext \uc758 data.field\n\uc0ac\uc6a9\n\n100 epoch \ud6c8\ub828 \uacb0\uacfc :  Answer \ubb38\uc7a5\uc758 \uad6c\uc131\uc740 \ub098\uc058\uc9c0 \uc54a\uc9c0\ub9cc \uc9c8\ubb38\uc5d0 \ub300\ud55c \uc801\uc808\ud55c \ub2f5\uc774 \uc798 \ub098\uc624\uc9c0 \uc54a\uc74c. \n\npredict \ud568\uc218 \ud3c9\uac00 : Answer \ubb38\uc7a5\uc758 \ud55c \ub2e8\uc5b4 \uc529 \uc0dd\uc131\ud560 \ub54c\ub9c8\ub2e4 \uc804\uccb4 Transformer \ubaa8\ub378\uc744 \uc6c0\uc9c1\uc5ec\uc57c \ud558\uae30 \ub54c\ubb38\uc5d0 \ube44\ud6a8\uc728\uc801\uc784","1ade3fba":"# soynlp tokenizer","4c2c649b":"# \ub370\uc774\ud130\uc14b \uc124\uc815","0b7181ae":"# \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30","37058156":"# \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378 \uad6c\ud604"}}