{"cell_type":{"72151d29":"code","a03293c5":"code","d4194304":"code","9797feef":"code","83988592":"code","f5d77fb7":"code","ef66904e":"code","c59eaefd":"code","61681956":"code","45e5b8b5":"code","c75a4ab0":"code","43426791":"code","295b153b":"code","18bd661d":"code","ec68807f":"code","497bab27":"code","c020ab95":"code","c89f2852":"code","1580d511":"code","87bba1d1":"code","df6710ee":"code","6ba9e0e3":"code","a24bc933":"markdown","c8d1583f":"markdown","b77e9953":"markdown","405cf728":"markdown","7d0756a5":"markdown","b5a5cfc0":"markdown","54dec600":"markdown","e1d1aa4c":"markdown","b1a58759":"markdown"},"source":{"72151d29":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # visualizing data\nimport seaborn as sns # visualizing data with stunning default theme\nimport sklearn # contain algorithms\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# load dataset from input directory\ndf = pd.read_csv(\"..\/input\/common-voice\/cv-valid-train.csv\") \ndf[df['age'].notna()].head()","a03293c5":"sns.set(rc={'figure.figsize':(15, 5)})\nsns.countplot(x=\"age\", \n        data=df[df['age'].notna()], \n        order=['teens', 'twenties', 'thirties', 'fourties', 'fifties', 'sixties', 'seventies', 'eighties'])\n\nplt.show()\n","d4194304":"sns.countplot(x=\"age\", \n        hue='gender',\n        data=df[df['age'].notna()],\n        order=['teens', 'twenties', 'thirties', 'fourties', 'fifties', 'sixties', 'seventies', 'eighties'])\n\nplt.show()","9797feef":"sns.displot(x=\"accent\", \n        data=df[df['accent'].notna()], \n        hue='gender',\n        multiple='stack',\n        height=5, aspect=18\/5)\n\nplt.show()","83988592":"#del df['duration']\nstart=df.shape\n#df.isna().sum()\nend = df[df['age'].notna()& df['gender'].notna() & df['accent'].notna()].shape\nprint(\"initial: {} final: {}\".format(start, end))\nsns.countplot(x=\"age\", \n        hue='gender',\n        data=df[df['age'].notna()& df['gender'].notna() & df['accent'].notna()],\n        order=['teens', 'twenties', 'thirties', 'fourties', 'fifties', 'sixties', 'seventies', 'eighties'])\n\nplt.show()","f5d77fb7":"#we extract the columns that we think useful are\ndf = df[['filename','age','gender']]\n#To clean the data we remove the sample with NaN attribute values.\ndata = df[df['age'].notna() & df['gender'].notna()]\ndata.reset_index(inplace=True, drop=True)\ndata.head()","ef66904e":"#data['gender'] = pd.to_numeric(data['gender'],errors = 'coerce')\n#data.dtypes\n#if the below code fail to covert gender datatype and values then uncomment above code\ncleanup_nums = {\"age\": {\"teens\":0.0,\"twenties\":1.0,\"thirties\":2.0,\"fourties\":3.0,\"fifties\":4.0,\"sixties\":5.0,\"seventies\":6.0,\"twenties\":7.0,\"eighties\":8.0}}\ndata = data.replace(cleanup_nums)\ndata = data[:1000]\ndata.head()","c59eaefd":"import librosa\nds_path = \"\/kaggle\/input\/common-voice\/cv-valid-train\/\"\n\n#this function is used to extract audio frequency features\ndef feature_extraction(filename, sampling_rate=48000):\n    path = \"{}{}\".format(ds_path, filename)\n    features = list()\n    audio, _ = librosa.load(path, sr=sampling_rate)\n    \n    gender = data[data['filename'] == filename].gender.values[0]\n    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sampling_rate))\n    spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=sampling_rate))\n    spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sampling_rate))\n    features.append(gender)\n    features.append(spectral_centroid)\n    features.append(spectral_bandwidth)\n    features.append(spectral_rolloff)\n    \n    mfcc = librosa.feature.mfcc(y=audio, sr=sampling_rate)\n    for el in mfcc:\n        features.append(np.mean(el))\n    \n    return features\n    \n        \nfeatures = feature_extraction(data.iloc[0]['filename'])\nprint(\"features: \", features)","61681956":"#the function create dataframe to store the feature and label related to each other\ndef create_df_features(orig):\n    new_rows = list()\n    tot_rows = len(orig)-1\n    stop_counter = 55001\n    \n    for idx, row in orig.iterrows():\n        if idx >= stop_counter: break\n        print(\"\\r\", end=\"\")\n        print(\"{}\/{}\".format(idx, tot_rows), end=\"\", flush=True)\n        features = feature_extraction(row['filename'])\n        features.append(row['age'])\n        new_rows.append(features)\n\n    return pd.DataFrame(new_rows, columns=[\"label\", \"spectral_centroid\", \"spectral_bandwidth\", \"spectral_rolloff\",\n                                    \"mfcc1\", \"mfcc2\", \"mfcc3\", \"mfcc4\", \"mfcc5\", \"mfcc6\", \"mfcc7\", \"mfcc8\",\n                                   \"mfcc9\", \"mfcc10\", \"mfcc11\", \"mfcc12\", \"mfcc13\", \"mfcc14\", \"mfcc15\", \"mfcc16\",\n                                   \"mfcc17\", \"mfcc18\", \"mfcc19\", \"mfcc20\", \"age\"])\n\ndf_features = create_df_features(data)\ndf_features.head()","45e5b8b5":"data.iloc[:, 1:]","c75a4ab0":"data['age'].unique()","43426791":"from sklearn.preprocessing import StandardScaler\n\ndef scale_features(data):\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(np.array(data.iloc[:, 1:], dtype = float))\n    # with data.iloc[:, 0:-1] we don't consider the label column\n        \n    return scaled_data, scaler\n\nx, scaler = scale_features(df_features)","295b153b":"print(\"Before scaling:\", df_features.iloc[0].values[:-1])\nprint(\"\\nAfter scaling:\", x[0])","18bd661d":"df_features['label'].unique()","ec68807f":"df_features.iloc[:, 0]","497bab27":"from sklearn.preprocessing import LabelEncoder\n\ndef get_labels(data):\n    labels = data.iloc[:, 0]\n    encoder = LabelEncoder()\n    labels = encoder.fit_transform(labels)\n    return labels, encoder\n\ny, encoder = get_labels(df_features)\nclasses = encoder.classes_\nprint(\"Before encoding:\", df_features.iloc[0].values[0])\nprint(\"\\nAfter encoding:\", y[0])\nprint(\"\\nClasses:\", classes)","c020ab95":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\nn_features = 22\n\nf_selector = SelectKBest(f_classif, k=n_features).fit(x, y)\nX_new = f_selector.transform(x)\nscores = f_selector.scores_\n\nindices = np.argsort(scores)[::-1]\n\nfeatures = []\nfor i in range(n_features):\n    features.append(df_features.columns[indices[i]])\n    \nplt.figure(figsize=(22, 5))\nplt.bar(features, scores[indices[range(n_features)]], color='g')\nplt.xticks(fontsize=8)\nplt.show()\n","c89f2852":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\n\nclassifiers_and_params = [\n    (SVC(), {'C': [200, 150, 100], 'gamma': ['auto', 'scale']}),\n    (RandomForestClassifier(), {'n_estimators': [100, 150, 200]})\n]\n\nfor tup in classifiers_and_params:\n    print(\"{}\".format(tup[0].__class__.__name__))\n    \n    # the main CV process\n    outer_cv = KFold(n_splits=3, shuffle=True, random_state=0)\n    fold_counter = 0\n\n    results = list()\n    for train_idx, test_idx in outer_cv.split(X_new):\n        fold_counter += 1\n        \n        # split data in training and test sets\n        X_train, X_test = X_new[train_idx], X_new[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        # the CV process used for the Grid Search\n        inner_cv = KFold(n_splits=2, shuffle=True, random_state=0)\n\n        # define and run the Grid Search CV process\n        gs = GridSearchCV(tup[0], tup[1], scoring='f1_macro', cv=inner_cv, refit=True)\n        res = gs.fit(X_train, y_train)\n\n        # get the best model, re-fit on the whole training set\n        best_model = res.best_estimator_\n\n        # evaluation on the test set\n        pred = best_model.predict(X_test)\n        score = f1_score(y_test, pred, average='macro')\n        results.append(score)\n        \n        print(\"\\tFold {}, Best Params {} with F1 Score {:.3f}, F1 Score on Test data {:.3f}\"\n              .format(fold_counter, res.best_params_, res.best_score_, score))\n\n    print('\\tAverage F1 Score on Test Set: {:.3f}\\n'.format(np.mean(results)))","1580d511":"import itertools\nimport matplotlib.pyplot as plt\n\ndef my_plot_confusion_matrix(cm, classes, normalize=False):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        title = \"Normalized Confusion Matrix\"\n    else:\n        title = \"Confusion Matrix (without normalization)\"\n    \n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    plt.title(title)\n\n    thresh = cm.max() \/ 2.\n    fmt = \"{:0.2f}\" if normalize else \"{:d}\"\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, fmt.format(cm[i, j]),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n","87bba1d1":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nX_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=0)\n\nmodel = SVC(C=100, gamma='scale')\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure()\nmy_plot_confusion_matrix(cm, classes=classes)\n\nplt.grid(False)\nplt.show()","df6710ee":"# Plot normalized confusion matrix\nplt.figure()\nmy_plot_confusion_matrix(cm, classes=classes, normalize=True)\n\nplt.grid(False)\nplt.show()\n\n","6ba9e0e3":"import joblib\n# save the model to disk\nfilename = 'finalized_model.sav'\njoblib.dump(model, filename)","a24bc933":"### **Model Selection**  \n\nHere, we consider two classifiers:\n\n    Support Vector Machine\n    Random Forest\n\nWe evaluate them with the K-Fold Cross-Validation technique. At each iteration of this outer cross-validation process, we tune the hyper-parameters of the classifiers with another (inner) Cross-Validation process , that will further divide the training set into training and validation data.\n\nAt each iteration of the outer CV process, we print the F1-Score obtained by the tuned classifier on the validation data, but also the F1-Score computed on the test data. Finally, we print the average F1-Scores computed on the test data at each step of the outer CV process.","c8d1583f":"### **Feature Selection**  \n\nWe use the ANOVA (ANalysis Of VAriance) statistical technique (f_classif) to select the best 22 features.\n","b77e9953":"### **Feature Transformation**  \n\nScaling the features with the scikit-learn StandardScaler.","405cf728":"#### **First-step findings**\nWe understood the dataset and also found that the dataset is imbalance. To refine the dataset for ml-model, we need to perform pre-processing in next-step.","7d0756a5":"### **Step-01 Data Collection**  \nIn this step the data is loaded and explored. The data is understood with basic graphs and to check if the dataset is balanced.","b5a5cfc0":"## **ML-Model to detect gender age based on his\/her voice**   \nIn this notebook, I am using a common-voice dataset to train a model for detecting age of \na person based on his\/her voice","54dec600":"### **Stepwise ML-model development**  \nThis notebook is created for beginners to develop age detection ml-model with stepwise approach using common-voice dataset.\nThe notebook include following steps:\n* Data Collection\n* Data Pre-processing \n* Feature Engineering\n* Model Selection\n* Model Analysis\n* Model Deployment","e1d1aa4c":"### Data Pre-processing  \n#### Data Cleaning  \nIn this step we drop entries(samples) with NaN values. The columns that doesn't contribute(unnecessary) to the model are removed. The attributes are checked for its datatypes and changed to an approapriate type.","b1a58759":"### **Feature Engineering**\n#### **Feature Extraction**\n\nWe extract the following features:  \nThe following features are related to audio quality through which the model will learn more effectively. In this project it is not necessary to have good knowledge about the given audio features.\n**Gender**\n* **Spectral Centroid**: each frame of a magnitude spectrogram is normalized and treated as a distribution over frequency bins, from which the mean (centroid) is extracted per frame\n* **Spectral Bandwidth**: compute 2nd-order spectral bandwidth\n* **Spectral Rolloff**: the center frequency for a spectrogram bin such that at least roll_percent (0.85 by default) of the energy of the spectrum in this frame is contained in this bin and the bins below\n* **Mel Frequency Cepstral Coefficients (MFCCs)**: a small set of 20 features that describe the overall shape of a spectral envelope\n\n**Librosa package**  \nLibrosa is a Python package for music and audio analysis. It provides the building blocks necessary to create the music information retrieval systems. Librosa helps to visualize the audio signals and also do the feature extractions in it using different signal processing techniques."}}