{"cell_type":{"b7b7ccbe":"code","fc5deb65":"code","71f1280a":"code","626721b9":"code","6c0e0021":"code","7fb40134":"code","8a633b29":"code","14936efa":"code","04e33d55":"code","95853ca3":"code","f43b47b6":"markdown","2cf33524":"markdown"},"source":{"b7b7ccbe":"from __future__ import print_function\nimport keras\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import Conv2D, MaxPooling2D\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline","fc5deb65":"# Defining the parameters\nbatch_size = 32\nnum_classes = 10\nepochs = 75","71f1280a":"# Splitting the data between train and test\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')","626721b9":"# plotting some random 10 images\n\nclass_names = ['airplane','automobile','bird','cat','deer',\n               'dog','frog','horse','ship','truck']\n\nfig = plt.figure(figsize=(8,3))\nfor i in range(num_classes):\n    ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[])\n    idx = np.where(y_train[:]==i)[0]\n    features_idx = x_train[idx,::]\n    img_num = np.random.randint(features_idx.shape[0])\n    im = (features_idx[img_num,::])\n    ax.set_title(class_names[i])\n    plt.imshow(im)\nplt.show()","6c0e0021":"# Convert class vectors to binary class matrices.\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)","7fb40134":"# Printing sample data\nprint(y_train[:10])","8a633b29":"model = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))","14936efa":"# summary of the model\nprint(model.summary())","04e33d55":"# compile\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='sgd',\n              metrics=['accuracy'])\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\n\n# Normalizing the input image\nx_train \/= 255\nx_test \/= 255\n","95853ca3":"# Training the model\nmodel.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=(x_test, y_test),\n              shuffle=True)","f43b47b6":"# **Experiment - II:**\n\nRemove the dropouts after the convolutional layers (but retain them in the FC layer). Also, use batch normalization after every convolutional layer.","2cf33524":"Result: The training accuracy is increasing because of BN but the difference between train accuracy and validation accuracy is low."}}