{"cell_type":{"38ae7322":"code","778b6f8d":"code","b7638a61":"code","9ec1c260":"code","cb1ab0c4":"code","2a62afe4":"code","14aaeebb":"code","f202e9be":"code","1b181c38":"code","448dedca":"code","95cc98fe":"code","b6323507":"code","22d1a20b":"code","f1cbd0ca":"code","71f5d79f":"code","87443186":"code","028e8959":"code","dd8718f2":"code","0b9b464d":"code","3112a85c":"code","1ae8efd3":"code","10bb3ff3":"code","2ef2408a":"code","a2421732":"code","15afe1cc":"code","40cee627":"code","23720198":"code","e8ca9bfd":"code","cb701737":"code","c83bec95":"code","7a88ff3d":"code","4aa853e6":"code","43e688a4":"code","5a838a41":"code","90c447d7":"code","c6ff505c":"code","d19bd0f2":"code","3b92f95d":"code","d20ff4bf":"code","7fe5ac9a":"code","1e47052d":"code","0666cc01":"code","17f8252c":"code","a1c1bf36":"code","f1e493b8":"code","85dd37cb":"code","59857d5d":"code","504f5a97":"markdown","e82a8e77":"markdown","ed846699":"markdown","6e1bdd4b":"markdown","ac23d577":"markdown","f10e7975":"markdown","8f2582cc":"markdown","13c70fbc":"markdown","87cf4410":"markdown","b7dd988c":"markdown","dfde0204":"markdown","df0147e5":"markdown","7947d9fb":"markdown","770ee956":"markdown"},"source":{"38ae7322":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","778b6f8d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","b7638a61":"#Load the dataset\ndata = pd.read_csv(\"..\/input\/brain-tumor\/data.csv\")","9ec1c260":"data.head()","cb1ab0c4":"data.columns","2a62afe4":"data.info()","14aaeebb":"data.dtypes","f202e9be":"data.isnull().sum()","1b181c38":"data.duplicated().sum()","448dedca":"data.describe()","95cc98fe":"# sns.pairplot(data,hue = data['y'])","b6323507":"data['y'].value_counts()","22d1a20b":"get_dummies = pd.get_dummies(data['y'])","f1cbd0ca":"get_dummies","71f5d79f":"#add the dummies columns into original dataset\ndata = pd.concat([data,get_dummies],axis=1)","87443186":"data.head()","028e8959":"#drop the y and Normal columns \ndata = data.drop(['y','Normal'],axis=1)","dd8718f2":"data.head()","0b9b464d":"data = data.drop('Unnamed: 0',axis=1)","3112a85c":"x = data.drop('tumor',axis=1)\ny = data['tumor']","1ae8efd3":"print(x.shape)","10bb3ff3":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel=ExtraTreesClassifier()\nmodel.fit(x,y)","2ef2408a":"print(model.feature_importances_)","a2421732":"ranked_features=pd.Series(model.feature_importances_,index=x.columns)\nranked_features.nlargest(20).plot(kind='barh')\nplt.show()","15afe1cc":"corr=data.iloc[:,:-1].corr()\n# top_features=corr.index\n# plt.figure(figsize=(20,20))\n# sns.heatmap(data[top_features].corr(),annot=True)\n# plt.show()","40cee627":"# threshold=0.8\n# # find and remove correlated features\n# def correlation(dataset, threshold):\n#     col_corr = set()  # Set of all the names of correlated columns\n#     corr_matrix = dataset.corr()\n#     for i in range(len(corr_matrix.columns)):\n#         for j in range(i):\n#             if (corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n#                 colname = corr_matrix.columns[i]  # getting the name of column\n#                 col_corr.add(colname)\n#     return col_corr","23720198":"# correlation(data.iloc[:,:-1],threshold)","e8ca9bfd":"from sklearn.feature_selection import mutual_info_classif\nmutual_info=mutual_info_classif(x,y)\nmutual_data=pd.Series(mutual_info,index=x.columns)\ncol = mutual_data.sort_values(ascending=False)","cb701737":"col.head(10)","c83bec95":"data = data[['M97496','R36977','T96548','Z50753','M83670','M77836','J03037.2','T64297','U17077','tumor']]","7a88ff3d":"data.head()","4aa853e6":"data.size","43e688a4":"data.columns","5a838a41":"data.isnull().sum()","90c447d7":"data.duplicated().sum()","c6ff505c":"X = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values","d19bd0f2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","3b92f95d":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","d20ff4bf":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)","7fe5ac9a":"y_pred = classifier.predict(X_test)","1e47052d":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nacc1 = accuracy_score(y_test, y_pred)","0666cc01":"print(acc1)","17f8252c":"from sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier(n_neighbors=9)\nKNN.fit(X_train,y_train)\npred = KNN.predict(X_test)","a1c1bf36":"cm_knn = confusion_matrix(y_test, y_pred)\nprint(cm_knn)","f1e493b8":"acc2 = accuracy_score(y_test, pred)\nprint(\"Accuracy: \",acc2)","85dd37cb":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","59857d5d":"mylist=[]\nmylist2=[]\nmylist.append(acc1)\nmylist2.append(\"Logistic Regression\")\nmylist.append(acc2)\nmylist2.append(\"KNN\")\nplt.rcParams['figure.figsize']=8,6\nsns.set_style(\"darkgrid\")\nax = sns.barplot(x=mylist2, y=mylist, palette = \"rocket\", saturation =1.5)\nplt.xlabel(\"Classification Models\", fontsize = 20 )\nplt.ylabel(\"Accuracy\", fontsize = 20)\nplt.title(\"Accuracy of different Classification Models\", fontsize = 20)\nplt.xticks(fontsize = 11, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width\/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","504f5a97":"# correlation","e82a8e77":"**Here is display the top 20  columns those are correlated with the dependent variable**","ed846699":"# Encoding the categorical variable -- using get dummies function","6e1bdd4b":"# Logistic Regression","ac23d577":"# Splitting the dataset into the Training set and Test set","f10e7975":"# Information Gain","8f2582cc":"# Feature Selection","13c70fbc":"# Applying k-Fold Cross Validation","87cf4410":"# Feature Importance","b7dd988c":"# create visualization of models with their accuracy score:","dfde0204":"# import the libraries","df0147e5":"# KNN","7947d9fb":"# Feature scaling","770ee956":"# Univariate selection"}}