{"cell_type":{"00658fc0":"code","3c2ed053":"code","66e48fae":"code","e0163a3a":"code","209b001b":"code","b7d7de26":"code","eb0e8755":"code","119a01e6":"code","f83de310":"code","39c06978":"code","1c52bdff":"code","efbadd9d":"code","11abacf7":"code","9ddb7dc4":"code","8be78579":"code","a35e3389":"code","441f5e5a":"code","367f792a":"code","2a4a1ced":"code","35f02574":"code","d5cb5594":"code","050b4362":"code","cf2483d6":"code","14a1a542":"code","da087803":"code","0b8ec767":"code","6b172aed":"code","50eb80dd":"code","1c8fb9b9":"code","d48ce254":"code","9f0be61b":"code","4b0ce688":"code","0f70a958":"code","0455838c":"code","4bbc9193":"code","a73fb2c5":"code","16e4e0f4":"code","cc7a51e4":"code","3dc1f0da":"code","741a1168":"code","e3b53bdb":"code","884a5d86":"code","0b5585d8":"code","712a1769":"code","95f8c535":"code","6c6f83ec":"code","1ddbaa26":"code","116f5b84":"code","d448791d":"code","86001803":"code","df56bd5d":"code","42b51b4b":"code","23e9747f":"code","5d677466":"code","4943aa27":"code","13ed496a":"code","cb323e77":"code","69ffd9da":"code","563204d5":"code","658d7695":"code","12c08963":"code","19e948b1":"code","fd880fa3":"code","95672609":"code","6fbe1555":"code","af112ad3":"code","6a793141":"code","94e9d405":"code","61217481":"code","a2f2c9a1":"code","933357f1":"code","ab10aef6":"code","da6c842a":"code","4278d307":"code","a777e48d":"code","d2134ece":"code","faaf6ff2":"code","28c60182":"code","5b63988f":"code","d4a3559d":"code","d61e7330":"code","0c4a355d":"code","d5362af8":"code","ebc70e7a":"code","0f4d0958":"code","0ec111f7":"code","3edc38a3":"code","515cf46b":"code","4b1bd29b":"code","1633deb9":"markdown","4f57fecb":"markdown","027a31a2":"markdown","14960555":"markdown"},"source":{"00658fc0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3c2ed053":"# read the dataset\ndf = pd.read_csv('..\/input\/superstore-data\/superstore_dataset2011-2015.csv')","66e48fae":"# without encoding error\ndf = pd.read_csv('..\/input\/superstore-data\/superstore_dataset2011-2015.csv',encoding = \"ISO-8859-1\")","e0163a3a":"df.columns","209b001b":"# Let's fetch the customer level details :\n\ndf_customer = df[['Customer ID','Customer Name', 'Segment']].drop_duplicates()","b7d7de26":"df_customer.head()","eb0e8755":"# Let's fetch details at Customer and Order Level : \n\ndf_customer_order = df[['Customer ID','Order ID','Order Date', 'Ship Date', 'Ship Mode',]].drop_duplicates()","119a01e6":"df_customer_order.head()","f83de310":"# Creating a new feature describing the time taken between placing of an order and \n# shipment of the same\n\ndf_customer_order['Order_to_Ship_Days'] = (pd.to_datetime(df_customer_order['Ship Date']) \n                                           - pd.to_datetime(df_customer_order['Order Date'])).dt.days","39c06978":"df_customer_order.head()","1c52bdff":"# Calculating average order-to-ship time(in days) at Customer level\n\ndf_customer_days = df_customer_order.groupby('Customer ID')['Order_to_Ship_Days'].mean()","efbadd9d":"# Saving this information as a Dataframe\n\ndf_customer_days = df_customer_days.to_frame()","11abacf7":"# Set the column name correctly\n\ndf_customer_days.columns = ['Avg_Order_to_Ship_Days']","9ddb7dc4":"df_customer_days.head()","8be78579":"grp_customer = df.groupby(['Customer ID'])\ndf1 = grp_customer['Order ID','Sales'].agg({'Order ID':np.size,'Sales':np.sum})","a35e3389":"# Creating a function which will do all the aggregation of metrics at Customer level : \n\ndef agg_customer(x):\n    d = []\n    d.append(x['Order ID'].nunique())\n    d.append(x['Sales'].sum())\n    d.append(x['Shipping Cost'].sum())\n    d.append(pd.to_datetime(x['Order Date']).min())\n    d.append(pd.to_datetime(x['Order Date']).max())\n    d.append(x['City'].nunique())\n    return pd.Series(d, index=['#Purchases','Total_Sales','Total_Cost','First_Purchase_Date','Latest_Purchase_Date','Location_Count'])\n\ndf_agg = df.groupby('Customer ID').apply(agg_customer)","441f5e5a":"# Checking the names of the new aggregated dataframe\n\ndf_agg.columns","367f792a":"# Creating new features on top of the aggregated dataframe already created above : \n\nfrom datetime import datetime\ndf_agg['Duration'] = (df_agg['Latest_Purchase_Date'] - df_agg['First_Purchase_Date']).dt.days\ndf_agg['Frequency'] = df_agg['Duration']\/df_agg['#Purchases']\ndf_agg['Days_Since_Last_Purchase'] = df_agg['Latest_Purchase_Date'].apply(lambda x: datetime.strptime('2016-01-01', \"%Y-%m-%d\") - x).dt.days\ndf_agg['Sales_Contribution'] = (df_agg['Total_Sales']\/df_agg['Total_Sales'].sum())\ndf_agg['Average_Basket_Value'] = df_agg['Total_Sales']\/df_agg['#Purchases']\ndf_agg['CLTV'] = df_agg['Total_Sales'] - df_agg['Total_Cost']\ndf_agg.sort_values(by=\"Latest_Purchase_Date\",ascending = False).head()","2a4a1ced":"df['Product ID'].nunique()","35f02574":"#  We would like to understand which products are high priced ones, medium and low priced ones \n\ndf_prod = df.groupby(['Product ID'])['Quantity','Sales'].agg(np.sum)\ndf_prod['Average_Price_Point'] = df_prod['Sales']\/df_prod['Quantity']\ndf_prod['Price_Point_Perc_Rank'] = df_prod['Average_Price_Point'].rank(pct=True)\ndf_prod['Ticket_Type'] = df_prod['Price_Point_Perc_Rank'].apply(lambda x: 'High' if x>0.7 else ('Medium' if (x>0.4 and x<=0.7) else 'Low'))\ndf_prod.sort_values(by='Price_Point_Perc_Rank',ascending=False).head()","d5cb5594":"# Check count of rows by Ticket Type :\n\ndf_prod['Ticket_Type'].value_counts()","050b4362":"# Here we join the base table with the Product level table to bring in\n# the Ticket_Type column as a part of the base table\n\ndf_with_ticket = pd.merge(df,df_prod,left_on='Product ID',right_on=df_prod.index,how='inner')","cf2483d6":"df_with_ticket.shape","14a1a542":"# Here we create a pivot table with Customer ID in rows\n# and Ticket type in columns\n# and count of rows as values\ndf_tickettype_pivot = pd.pivot_table(df_with_ticket[['Customer ID','Ticket_Type']],index=[\"Customer ID\"],\n               columns=[\"Ticket_Type\"],aggfunc=[np.size])","da087803":"# Fetch column names from level 2 of the Multi-Index\n\ndf_tickettype_pivot_columns = df_tickettype_pivot.columns.get_level_values(1).tolist()","0b8ec767":"df_tickettype_pivot.head()\n","6b172aed":"# Merge customer aggregated data with Ticket type data \ndf_customer_tickettype = pd.merge(df_agg,df_tickettype_pivot,on = 'Customer ID',how='inner')","50eb80dd":"df_agg.columns.tolist()","1c8fb9b9":"df_customer_tickettype.columns = df_agg.columns.tolist() + df_tickettype_pivot_columns","d48ce254":"df_customer_tickettype.head()","9f0be61b":"# bringing in customer meta-data\ndf_total_customer = pd.merge(df_customer_tickettype,df_customer,on='Customer ID',how='inner')","4b0ce688":"df_total_customer.head()","0f70a958":"# Add average Order-to-Ship days to the dataset\n\ndf_tot_cust_order_final = pd.merge(df_total_customer,df_customer_days,on='Customer ID',how='inner')","0455838c":"df_tot_cust_order_final.head()","4bbc9193":"# Read returns data\ndf_returns = pd.read_csv('..\/input\/product-returns\/Returned orders.csv')","a73fb2c5":"df_returns.columns","16e4e0f4":"# connect with product table to categorize the returned products : \ndf_returns_tickettype = pd.merge(df_returns,df_prod,on='Product ID',how='inner')","cc7a51e4":"df_returns_tickettype.columns","3dc1f0da":"df_cust_prd_tktype = pd.pivot_table(df_returns_tickettype[['Customer ID','Ticket_Type']],index=[\"Customer ID\"],\n               columns=[\"Ticket_Type\"],aggfunc=[np.size])","741a1168":"df_cust_prd_tktype.head()","e3b53bdb":"df_cust_prd_tktype.columns = ['High_Returns','Medium_Returns','Low_Returns']","884a5d86":"df_cust_prd_tktype.head()","0b5585d8":"df_cust_prd_tktype.fillna(0,inplace = True)","712a1769":"df_cust_prd_tktype.head()","95f8c535":"# Merge with the final aggregated table created before to develop the final dataset to work on : \n\ndata_final = pd.merge(df_tot_cust_order_final,df_cust_prd_tktype,on='Customer ID',how='left')","6c6f83ec":"data_final.shape","1ddbaa26":"data_final.head()","116f5b84":"data_final = data_final.fillna({'High':0,\n                                'Medium':0,\n                                'Low':0,\n                                'High_Returns':0,\n                                'Medium_Returns':0,\n                                'Low_Returns':0})","d448791d":"data_final.info()","86001803":"# one hot encode the segment column\n\ndf_segment_ohe = pd.get_dummies(data_final['Segment'])","df56bd5d":"df_segment_ohe.head()","42b51b4b":"df_clean = pd.concat([data_final,df_segment_ohe],axis = 1)","23e9747f":"df_clean.head()","5d677466":"# drop columns which are of text type or of no use\n\ndf_clean.drop(['Customer ID','Customer Name','First_Purchase_Date','Latest_Purchase_Date','Segment'\n               ,'Total_Sales','Total_Cost']\n                , axis=1,inplace = True)","4943aa27":"df_clean.head()","13ed496a":"df_clean.info()","cb323e77":"# Let's split the feature and response variables : \ny = df_clean['CLTV']\nX = df_clean[df_clean.columns.difference(['CLTV'])]","69ffd9da":"# Let's split the data now\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)","563204d5":"# Let's scale the data now\n\n#Standardization\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX_train_std=sc.fit_transform(X_train)\nX_test_std = sc.transform(X_test)","658d7695":"corr = X.corr()\ncorr.style.background_gradient(cmap='coolwarm')","12c08963":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nf, ax = plt.subplots(figsize=(16, 12))\nplt.title('Pearson Correlation of features')\n# Draw the heatmap using seaborn\n#sns.heatmap(house_num.astype(float).corr(),linewidths=0.25,vmax=1.0, square=True, cmap=\"PuBuGn\", linecolor='k', annot=True)\nsns.heatmap(X.corr(),linewidths=0.25,vmax=1.0, square=True, cmap=\"coolwarm\", linecolor='k', annot=True)","19e948b1":"# Wow! That was big! It is better to find the most correlated features\nmost_corr_features = corr.index[abs(corr[\"#Purchases\"])>0.6]\nplt.figure(figsize=(15,15))\nsns.heatmap(X[most_corr_features].corr(),annot=True)","fd880fa3":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train_std,y_train)","95672609":"lr.coef_","6fbe1555":"from sklearn.metrics import r2_score,mean_squared_error","af112ad3":"y_pred = lr.predict(X_test_std)","6a793141":"r2_score(y_pred,y_test)","94e9d405":"mean_squared_error(y_pred,y_test)","61217481":"np.sqrt(mean_squared_error(y_pred,y_test))","a2f2c9a1":"X_train.columns","933357f1":"from sklearn.linear_model import Lasso","ab10aef6":"# create a lasso regressor\nlasso = Lasso(alpha=0.2, normalize=True)\n\n# Fit the regressor to the data\nlasso.fit(X_train_std,y_train)\n\n# Compute and print the coefficients\nlasso_coef = lasso.coef_\nprint(lasso_coef)","da6c842a":"df_coeffs = pd.DataFrame()\ndf_coeffs['feature_names'] = X_train.columns","4278d307":"df_coeffs['values'] = lasso_coef","a777e48d":"df_coeffs","d2134ece":"from sklearn.feature_selection import f_regression\nffs = f_regression(X_train,y_train)","faaf6ff2":"from sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nlreg = LinearRegression()\nrfe = RFE(lreg,5)\nrfe = rfe.fit(X_train_std,y_train)","28c60182":"rfe.ranking_","5b63988f":"df_coeffs['ranking'] = rfe.ranking_","d4a3559d":"df_coeffs","d61e7330":"new_cols = df_coeffs[df_coeffs['ranking'] == 1]['feature_names']","0c4a355d":"X_new = X[new_cols]","d5362af8":"# Let's split the data now\nfrom sklearn.model_selection import train_test_split\nX_new_train,X_new_test,y_new_train,y_new_test=train_test_split(X_new,y,test_size=0.2)","ebc70e7a":"# Let's scale the data now\n\n#Standardization\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX_new_train_std=sc.fit_transform(X_new_train)\nX_new_test_std = sc.transform(X_new_test)","0f4d0958":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_new_train_std,y_new_train)","0ec111f7":"y_new_pred = lr.predict(X_new_test_std)","3edc38a3":"r2_score(y_new_pred,y_new_test)","515cf46b":"mean_squared_error(y_new_pred,y_new_test)","4b1bd29b":"np.sqrt(mean_squared_error(y_new_pred,y_new_test))","1633deb9":"## Brainstorm on what metrics can be created to build an appealing storyline\n* Sales value($)\n* Sales Volume\n* Sales CAGR\n* Footfalls\n* Transactions\n* Profit Margin(%)\n","4f57fecb":"# Let's do some product analysis","027a31a2":"## Now, the modelling journey commences!","14960555":"## What all business questions shall we consider here?\n* Analysis of various business entities like :\n    * Customer Analysis\n    * Sales Analysis\n    * Order Analysis\n    * Product Analysis etc.\n* Analysis of the above entities across dimensions like : \n    * Time Hierarchy\n    * Geographical Hierarchy\n    * Product Hierarchy etc.\n* Analysis of KPIs (Key Performance Indicators) like :\n    * Sales\n    * Profits\n    * Customer Retention Rate\n    * On-Time Delivery\n    * Return Rate\n    * Inventory Turns\n    * Days in Inventory etc.\n    "}}