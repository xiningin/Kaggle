{"cell_type":{"bd008ed6":"code","7f4f53bf":"code","50f9aa9d":"code","84c1b71d":"code","a809dbb8":"code","1eaab935":"code","54aa30bf":"code","9b24fbe8":"code","0d7dc572":"code","ab08c000":"code","0cda4dc8":"code","b0737cb2":"code","e667e4b5":"code","b4bfe541":"code","467a8b24":"code","bbb70607":"code","6e90d7a4":"code","f34c30eb":"code","0cd4e04b":"code","b9209937":"code","71afceff":"code","199ba25f":"code","cdb55727":"code","d39c186e":"code","866b1a4a":"code","687e133f":"code","bc0d0c87":"code","106fb4f2":"code","a9e57896":"code","1f6ae6af":"code","2f4ef3ae":"code","3b9f1895":"code","15983b3f":"code","ee813a20":"code","fa27f636":"code","fe5360fe":"code","4bc6799a":"code","6475258d":"code","6f6c5e76":"code","9bb1d4c9":"code","7dc6b81f":"code","00a5b107":"code","08b5b44d":"code","2a8956ca":"code","1cea6d26":"code","94f3d80c":"code","042ce58b":"code","a5569105":"markdown","45ad6479":"markdown","8e0ccdc8":"markdown","00cff56a":"markdown","a98de9c5":"markdown","57418310":"markdown","6afa76e4":"markdown","d8d28b96":"markdown","9dfbdab6":"markdown","757c715a":"markdown","44cf9fa4":"markdown","c2529d96":"markdown","9cf9faa8":"markdown","a8d3cc8d":"markdown","4c69a858":"markdown","a6026020":"markdown","e0fb1f75":"markdown","3dfa488f":"markdown","d750a03c":"markdown"},"source":{"bd008ed6":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport requests\nimport io\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.distributions.empirical_distribution import ECDF\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nimport geopy\nfrom geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter\nimport tqdm\nfrom tqdm._tqdm_notebook import tqdm_notebook","7f4f53bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","50f9aa9d":"df = pd.read_csv('\/kaggle\/input\/marine-litter-watch-19502021\/MLW_Data.csv')\ndf_meta = pd.read_csv('\/kaggle\/input\/marine-litter-watch-19502021\/MLW_Meta.csv')","84c1b71d":"print(f'dataset has {df.shape[0]} rows and {df.shape[1]} columns.')\nprint(f'metadata dataset has {df_meta.shape[0]} rows and {df_meta.shape[1]} columns.')","a809dbb8":"df.head() ","1eaab935":"df_meta.head()","54aa30bf":"# create a dictionary to keep the category for each G column\ng_category_dict = {}\nfor cat in df_meta.category.unique():\n    g_category_dict[cat] = df_meta[df_meta['category']==cat]['generalcode'].tolist()\ng_category_dict['Cloth\/textile'] = g_category_dict['Cloth\/textile'][:-2] #remove G301, G302 as those two are not in MLW_Data.csv","9b24fbe8":"# inspect whether there exist any other non-float g column\nif len([col for col in df.columns if 'G' in col and df[col].dtype!=float])==0:\n    print('no other G related column')\nelse:\n    print('there exist other G columns as well')","0d7dc572":"# have a look at non-g columns and their types\nnon_g_cols = [col for col in df.columns if 'G' not in col] \ng_cols = list(set(df.columns)-set(non_g_cols))\n[(col, df[col].dtype) for col in non_g_cols]","ab08c000":"# inspect object columns\nobject_cols = [col for col in non_g_cols if df[col].dtype==object]\nfor col in object_cols:\n    print(\"Column %s has %4d unique values, and %4d missing values\" % (col, df[col].nunique(), df[col].isna().sum()))","0cda4dc8":"# have a look at unique values\nfor col in [col for col in object_cols if col!='BeachName']:\n    print(col, df[col].unique().tolist())\n    print(\"\\n\")","b0737cb2":"df[g_cols].describe()","e667e4b5":"# test whether all g columns take integer values \nfor g in g_cols:\n    if not df[g].dropna().apply(float.is_integer).all():\n        print(g)","b4bfe541":"colours = ['#02274f', '#627ea4', '#c4e2ff', '#6dacfb', '#1a71ef', '#5dbcfb']\nf,ax=plt.subplots(2,2,figsize=(16,9))\nsns.countplot(x= df.fillna('Missing')[\"BeachType\"],ax=ax[0,0],palette = colours)\nax[0,0].set_title('Beach Types')\nsns.countplot(x= df.fillna('Missing')[\"BeachLocation\"],ax=ax[0,1],palette = colours)\nax[0,1].set_title('Beach Locations')\nsns.countplot(x= df.fillna('Missing')[\"BeachRegionalSea\"],ax=ax[1,0],palette = colours)\nax[1,0].tick_params(labelrotation=60)\nax[1,0].set_title('Beach Regional sea')\nsns.countplot(x= df.fillna('Missing')[\"EventType\"],ax=ax[1,1],palette = colours)\nax[1,1].set_title('Event Types')  \nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","467a8b24":"f,ax=plt.subplots(2,2,figsize=(16,9))\nsns.countplot(y=\"BeachCountrycode\", hue=\"EventType\", data=df, order=df.BeachCountrycode.value_counts().iloc[:10].index, ax=ax[0,0], palette = colours)\nax[0,0].set_title('Actions by Event Type and Country')\ndf.set_index(pd.to_datetime(df.EventDate.astype(str), format='%Y%m%d')).sort_index()[g_cols].sum(axis=1).cumsum().plot(kind='area', colormap='Blues_r', ax=ax[0,1])\nax[0,1].set_title('Sum of litter gathered by Date')\nsns.violinplot(x=df.BeachType, y=df[g_cols].sum(axis=1)[df[g_cols].sum(axis=1)<2500], palette = colours, ax=ax[1,0])\nax[1,0].set_title('Sum of litter gathered by BeachType')\nsns.swarmplot(x=df.BeachLocation, y=df[g_cols].sum(axis=1)[df[g_cols].sum(axis=1)<2500],palette = colours, ax=ax[1,1])\nax[1,1].set_title('Sum of litter gathered by BeachLocation')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","bbb70607":"f,ax=plt.subplots(2,2,figsize=(16,16))\nsns.heatmap(df[g_category_dict['Rubber']].corr(),cmap= \"Blues\",annot=True, square=True, ax=ax[0,0])\nax[0,0].set_title('Rubber category heatmap')\nsns.heatmap(df[g_category_dict['Cloth\/textile']].corr(),cmap= \"Blues\",annot=True, square=True, ax=ax[0,1])\nax[0,1].set_title('Cloth\/textile category heatmap')\nsns.heatmap(df[g_category_dict['Processed\/worked wood']].corr(),cmap= \"Blues\",annot=True, square=True, ax=ax[1,0])\nax[1,0].set_title('Processed\/worked wood category heatmap')\nsns.heatmap(df[g_category_dict['Glass\/ceramics']].corr(),cmap= \"Blues\",annot=True, square=True, ax=ax[1,1])\nax[1,1].set_title('Glass\/ceramics category heatmap')\nplt.show()","6e90d7a4":"corrmat = df[g_category_dict['Metal']].corr()\ncmap=sns.color_palette(\"Blues\", as_cmap=True)\nplt.subplots(figsize=(18,18))\nsns.heatmap(corrmat,cmap= cmap,annot=True, square=True)\nplt.title('Metal category columns heatmap')\nplt.show()","f34c30eb":"plt.figure(figsize=(29,5))\nsns.boxenplot(data = df[g_category_dict['Plastic']],palette = colours).set_facecolor('#c4e2ff')\nplt.xticks(rotation=90)\nplt.title('Distribution of litter across different plastics')\nplt.show()","0cd4e04b":"ax=plt.figure()\ndf[g_category_dict['Plastic']].groupby(pd.to_datetime(df.EventDate.astype(str), format='%Y%m%d').dt.year).sum().sum(axis=1).plot(title='Quantity of Plastic gathered per year')\nax.set_facecolor('#c4e2ff')\nplt.show()","b9209937":"plt.figure(figsize=(10,4))\nsns.distplot(df[g_category_dict['Processed\/worked wood']]).set_facecolor('#c4e2ff')\nplt.show()","71afceff":"ecdf = ECDF(df[g_category_dict['Plastic']].set_index(pd.to_datetime(df.EventDate.astype(str), format='%Y%m%d')).sort_index().sum(axis=1))\nax=plt.figure(figsize=(10,4))\nplt.plot(ecdf.x, ecdf.y, marker='.', linestyle=None)\nplt.title('Empirical Cumulative Distribution Function of Plastic gathered')\nax.set_facecolor('#c4e2ff')\nplt.show()","199ba25f":"x=int(df[g_category_dict['Plastic']].sum().sum())\nprint('Total liters of Plastic gathered is: '   f'{x:,}')","cdb55727":"plt.figure(figsize=(19,5))\ndf[df.EventType=='Cleanup'][g_cols].set_index(pd.to_datetime(df[df.EventType=='Cleanup']    \\\n                                   .EventDate.astype(str), format='%Y%m%d')).sort_index().sum(axis=1)    \\\n                                   .plot(title='Volume of litter gathered over Event dates').set_facecolor('#c4e2ff')\nplt.show()","d39c186e":"df_cleaned = df.copy()","866b1a4a":"df_cleaned.head()","687e133f":"url=\"https:\/\/gist.githubusercontent.com\/radcliff\/f09c0f88344a7fcef373\/raw\/2753c482ad091c54b1822288ad2e4811c021d8ec\/wikipedia-iso-country-codes.csv\"\ns=requests.get(url).content\nc=pd.read_csv(io.StringIO(s.decode('utf-8')))\nc.head(3)","bc0d0c87":"# obtain country names from codes\nc['CountryName'] = c[c.columns[0]]\nc['CountryCode'] = c[c.columns[1]]\nc = c[['CountryCode', 'CountryName']]\ndictio = c.set_index('CountryCode').to_dict()['CountryName']\ndel dictio[np.nan]\ndf_cleaned['BeachCountry']= df_cleaned.BeachCountrycode.map(dictio)\ndf_cleaned.drop('BeachCountrycode', axis=1, inplace=True)","106fb4f2":"df_cleaned['EventDate'] = pd.to_datetime(df_cleaned.EventDate.astype(str), format='%Y%m%d')\ndf_cleaned['EventDate'].head()","a9e57896":"# keep only one lat and lon column\ndf_cleaned['lat'] = (df_cleaned.lat_y1 + df_cleaned.lat_y2)\/2\ndf_cleaned['lon'] = (df_cleaned.lon_x1 + df_cleaned.lon_x2)\/2\ndf_cleaned.drop(['lat_y1', 'lon_x1', 'lat_y2', 'lon_x2'], axis=1, inplace=True)","1f6ae6af":"d = {'Maroc \/ \u2d4d\u2d4e\u2d56\u2d54\u2d49\u2d31 \/ \u0627\u0644\u0645\u063a\u0631\u0628': 'Morocco', \n     '\u039a\u03cd\u03c0\u03c1\u03bf\u03c2 - K\u0131br\u0131s': 'Cyprus', \n     'Deutschland': 'Germany', \n     'Danmark': 'Denmark', \n     'Espa\u00f1a': 'Spain', \n     'L\u00ebtzebuerg': 'Luxembourg', \n     'Schweiz\/Suisse\/Svizzera\/Svizra' : 'Switzerland', \n     'Madagasikara': 'Madagascar', \n     'Alg\u00e9rie \/ \u2d4d\u2d63\u2d63\u2d30\u2d62\u2d3b\u2d54 \/ \u0627\u0644\u062c\u0632\u0627\u0626\u0631': 'Algeria', \n     '\u10e1\u10d0\u10e5\u10d0\u10e0\u10d7\u10d5\u10d4\u10da\u10dd': 'Georgia', \n     '\u0395\u03bb\u03bb\u03ac\u03c2': 'Greece', \n     'Belgi\u00eb \/ Belgique \/ Belgien': 'Belgium', \n     'M\u00e9xico': 'Mexico', \n     '\u0645\u0648\u0631\u064a\u062a\u0627\u0646\u064a\u0627': 'Mauritania', \n     'Kalaallit Nunaat': 'Greenland', \n     '\u0420\u043e\u0441\u0441\u0438\u044f': 'Russian Federation'}","2f4ef3ae":"# impute BeachCountry column\ntqdm_notebook().pandas()\ndf_cleaned['geom'] = df_cleaned['lat'].map(str) + ',' + df_cleaned['lon'].map(str)\nlocator = Nominatim(user_agent='myGeocoder', timeout=10)\nrgeocode = RateLimiter(locator.reverse, min_delay_seconds=0.001)\ndf_cleaned['address'] = df_cleaned[df_cleaned.BeachCountry.isna()]['geom'].progress_apply(rgeocode)\ndf_cleaned['address'] = df_cleaned['address'].apply(lambda x: x[0].split(',')[-1].strip() if type(x)!=float and x is not None else np.nan)\ndf_cleaned['address'] = df_cleaned['address'].replace(d)\ndf_cleaned['BeachCountry'] = df_cleaned.address.fillna('') + df_cleaned.BeachCountry.fillna('')\ndf_cleaned.drop(['address', 'geom'], axis=1, inplace=True)","3b9f1895":"df_cleaned[df_cleaned['BeachCountry']=='']","15983b3f":"df_cleaned.BeachCountry[df_cleaned.BeachRegionalSea=='Unknown'].value_counts()[:5]","ee813a20":"df_cleaned.BeachCountry[df_cleaned.BeachRegionalSea.isna()].value_counts()[:5]","fa27f636":"df_cleaned.BeachRegionalSea[(df_cleaned.BeachRegionalSea=='Unknown') & (df_cleaned.BeachCountry=='France') & (df_cleaned.lat<43.8) & (df_cleaned.lon<7.5) & (df_cleaned.lon>2.6)] = 'Mediterranean Sea'\ndf_cleaned.BeachRegionalSea[(df_cleaned.BeachRegionalSea=='Unknown') & (df_cleaned.BeachCountry=='France') & (df_cleaned.lat<49) \n                            & (df_cleaned.lat>44) & (df_cleaned.lon<5) & (df_cleaned.lon>-0.7) ] = 'No see near'\ndf_cleaned.BeachRegionalSea[(df_cleaned.BeachRegionalSea=='Unknown') & (df_cleaned.BeachCountry=='France')] = 'North-east Atlantic Ocean' # or 'Atlantic Ocean'\ndf_cleaned.BeachRegionalSea[df_cleaned.BeachCountry=='Italy'][df_cleaned.BeachRegionalSea=='Unknown']= 'Mediterranean Sea'\ndf_cleaned.BeachRegionalSea[df_cleaned.BeachCountry=='Switzerland'] = 'No see near'","fe5360fe":"print(df_cleaned.BeachLocation.value_counts(dropna=False))\nprint('\\n')\nprint(df_cleaned.BeachType.value_counts(dropna=False))","4bc6799a":"# impute with most frequent value\ndf_cleaned['BeachLocation'].fillna(df_cleaned['BeachLocation'].mode()[0], inplace=True)\ndf_cleaned['BeachType'].fillna(df_cleaned['BeachType'].mode()[0], inplace=True)","6475258d":"# set threshold and determine g columns to be removed\nnans = pd.DataFrame(df_cleaned.isna().sum())\nnans.columns=['count']\nnans.sort_values(by='count', ascending=False, inplace=True)\nthres = 0.95\nnans['thres']= nans['count'] >= int(thres*df_cleaned.shape[0])\ng_toremove = nans[nans['thres']].index.tolist()\ng_tokeep = [col for col in df_cleaned.columns if 'G' in col and col not in g_toremove]","6f6c5e76":"df_cleaned.drop(g_toremove + ['CommunityName', 'BeachName', 'NatRef'], axis=1, inplace=True)","9bb1d4c9":"df_cleaned.drop_duplicates(inplace=True, ignore_index=True)","7dc6b81f":"print(\"Old df shape: %s and cleaned df shape: %s\" % (df.shape, df_cleaned.shape))","00a5b107":"non_g_cols = [col for col in df_cleaned.columns if 'G' not in col] \ndf_cleaned[non_g_cols].head(10)","08b5b44d":"g_category_dict.keys()","2a8956ca":"all([g in df.columns for g in g_category_dict['Plastic']])","1cea6d26":"g_cols = [col for col in df_cleaned.columns if 'G' in col] \npipe = Pipeline ([\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler()),\n    ('reducer', PCA(n_components=0.9)) #set variance thrshold for no of components\n])\npipe.fit(df_cleaned[g_cols])","94f3d80c":"print(\"Number of G related columns: %s and after PCA: %s\" % (df_cleaned[g_cols].shape[1], len(pipe.steps[2][1].components_)))","042ce58b":"plt.plot(pipe.steps[2][1].explained_variance_ratio_.cumsum())\nplt.xlabel('Principal component index')\nplt.ylabel('Explained variance ratio')\nplt.title('Variance explained by the first i+1 dimensions')\nplt.show()","a5569105":"#### <a id=\"5\"><\/a>\n<h1 style='background:#1c4ab1; border:0; color:white'><center>DATA VISUALISATION<\/center>","45ad6479":"Reduce null and 'Unknown' values on BeachRegionalSea column.","8e0ccdc8":"Fill None values on BeachLocation, and BeachType with most frequent value","00cff56a":"Impute BeachCountrycode column based on beach's lat, lon","a98de9c5":"#### <a id=\"2\"><\/a>\n<h1 style='background:#1c4ab1; border:0; color:white'><center>LOADING DATA<\/center>","57418310":"Transform country code to country name","6afa76e4":"Handle 'Unknown' and nan values for France, Italy, and Switzerland. Set heuristic squares and assign country name to an observation if beach's coordinates lie inside that square.","d8d28b96":"Cast EventDate to Datetime","9dfbdab6":"<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 style='background:#1c4ab1; border:0; color:white'><center>TABLE OF CONTENTS<\/center><\/h1>\n    \n[1. IMPORTING LIBRARIES](#1)\n    \n[2. LOADING DATA](#2)    \n\n[3. EXPLORE DATA](#3)     \n\n[4. DATA PREPROCESSING](#4)   \n    \n#### <a id=\"1\"><\/a>\n<h1 style='background:#1c4ab1; border:0; color:white'><center>IMPORTING LIBRARIES<\/center>    ","757c715a":"#### <a id=\"4\"><\/a>\n<h1 style='background:#1c4ab1; border:0; color:white'><center>DATA PREPROCESSING<\/center>","44cf9fa4":"Almost 90% of the cleaning acts contain at most 2,500 lts of plastic which is 0.14% of the total amount presented in the dataset.","c2529d96":"There exist high correlated features as expected, so in a supervised problem senario the space of the dataframe could be reduced.","9cf9faa8":"Take average of lat and lon","a8d3cc8d":"We see that the first 50 components explain around 80% of the data variance. So we could reduce the number of G columns to almost 1\/3 of the original data dimensionality.","4c69a858":"An extra step to perform scaling and dimensionality reduction.","a6026020":"#### <a id=\"3\"><\/a>\n<h1 style='background:#1c4ab1; border:0; color:white'><center>EXPLORE DATA<\/center>\n","e0fb1f75":"It is legit to omit columns CommunityName, BeachName, NatRef from the modeling part of the analysis as well as the 'G..' related columns for which 95% of total examples are missing.","3dfa488f":"![a-set-of-beach-cleaning-vector.jpg](attachment:6b7069ef-018a-408a-96db-f7a720464b76.jpg)\n","d750a03c":"We observe that probably due to the pandemic during 2020 just a few litter was gathered "}}