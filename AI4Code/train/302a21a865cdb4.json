{"cell_type":{"ea91d640":"code","d3ecda31":"code","0ca97b1d":"code","e4eb43f2":"code","2ecb1cb6":"code","f608f9d4":"code","66d4afce":"code","4c9360a9":"code","4d6899c7":"code","af27a67d":"code","6cb1ee6e":"code","16cf4994":"code","b4fa265f":"code","5309c8ca":"code","4f4f3c50":"code","ca09fa8c":"code","0cd6fffc":"code","aba72b7b":"code","5732ecd2":"code","e7f06515":"code","d2e622c2":"code","c37bfbcb":"code","f7a7e2c5":"markdown","6624911b":"markdown","0ee2877f":"markdown","1e4df953":"markdown","050badcb":"markdown","dda75a06":"markdown","171057b8":"markdown","8b5ef853":"markdown","3f643454":"markdown"},"source":{"ea91d640":"# loading dataset\n\nimport pandas as pd\nimport numpy as np\n\n# visualisation\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# EDA\nfrom collections import Counter\n\n# data preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\n# data splitting\nfrom sklearn.model_selection import train_test_split\n\n# data modeling\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report\n\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\n# tuning\nfrom sklearn.model_selection import GridSearchCV\n\n# ensembling\nfrom sklearn.ensemble import StackingClassifier","d3ecda31":"df = pd.read_csv('..\/input\/health-care-data-set-on-heart-attack-possibility\/heart.csv')\ndf.head()","0ca97b1d":"df.shape","e4eb43f2":"df.isna().sum()","2ecb1cb6":"df.describe()","f608f9d4":"X = df.drop('target',axis=1)\ny = df[\"target\"]","66d4afce":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)","4c9360a9":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","4d6899c7":"print(y_test.unique())\nCounter(y_train)","af27a67d":"parameters = {'penalty': ['l1', 'l2'], \n              'C': [0.1, 0.4, 0.8, 1, 2, 5,10,20,30]}    \n\ngrid_search=GridSearchCV(estimator=LogisticRegression() ,param_grid=parameters,cv=10,n_jobs=-1,verbose=2)\ngrid_search.fit(X_train,y_train)\n\nlog_reg = grid_search.best_estimator_\n\ngrid_search.best_params_","6cb1ee6e":"y_pred=log_reg.predict(X_test)\n\nprint(\"\\n\",confusion_matrix(y_test,y_pred))\nlog_reg_acc = accuracy_score(y_test,y_pred)\n\nprint(\"\\nAccuracy Score {}\".format(log_reg_acc))\nprint(\"Classification report: \\n{}\".format(classification_report(y_test,y_pred)))","16cf4994":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\n\nparameters = {'criterion' : ['gini', 'entropy'],\n              'max_depth': [2, 4, 5, 7, 9, 10],\n              'n_estimators' : [10,20,50,100,200,500,1000]}\n\n\ngrid_search=GridSearchCV(estimator=rf ,param_grid=parameters,cv=10,n_jobs=-1,verbose=2)\ngrid_search.fit(X_train,y_train)\n\nrf = grid_search.best_estimator_\n\ngrid_search.best_params_","b4fa265f":"y_pred=rf.predict(X_test)\n\nprint(\"\\n\",confusion_matrix(y_test,y_pred))\nrf_acc = accuracy_score(y_test,y_pred)\nprint(\"\\nAccuracy Score {}\".format(rf_acc))\nprint(\"Classification report: \\n{}\".format(classification_report(y_test,y_pred)))","5309c8ca":"from sklearn.neighbors import KNeighborsClassifier\n\nkn = KNeighborsClassifier()\n\n\nparameters = {\n    'n_neighbors' : np.arange(1,40),\n    'algorithm' :['auto', 'ball_tree', 'kd_tree', 'brute'],\n    'leaf_size' : np.arange(1,40)\n}\n\ngrid_search=GridSearchCV(estimator=kn ,param_grid=parameters,cv=10,n_jobs=-1,verbose=2)\ngrid_search.fit(X_train,y_train)\n\nkn = grid_search.best_estimator_\n\ngrid_search.best_params_","4f4f3c50":"y_pred=kn.predict(X_test)\n\nprint(\"\\n\",confusion_matrix(y_test,y_pred))\nkn_acc = accuracy_score(y_test,y_pred)\nprint(\"\\nAccuracy Score {}\".format(kn_acc))\nprint(\"Classification report: \\n{}\".format(classification_report(y_test,y_pred)))","ca09fa8c":"from sklearn.svm import LinearSVC\n\nsvc = LinearSVC()\n\nparameters = {\n      'penalty':['l1', 'l2'],\n      'max_iter': [10,20,50,100,1000], \n      'C': [0.1, 0.4, 0.8, 1, 2, 5,10,20,30],          \n              }\n\ngrid_search=GridSearchCV(estimator=svc ,param_grid=parameters,cv=10,n_jobs=-1,verbose=2)\ngrid_search.fit(X_train,y_train)\n\nsvc = grid_search.best_estimator_\n\ngrid_search.best_params_","0cd6fffc":"y_pred=svc.predict(X_test)\n\nprint(\"\\n\",confusion_matrix(y_test,y_pred))\nsvc_acc = accuracy_score(y_test,y_pred)\nprint(\"\\nAccuracy Score {}\".format(svc_acc))\nprint(\"Classification report: \\n{}\".format(classification_report(y_test,y_pred)))","aba72b7b":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier()\n\nparameters = {'min_child_weight' : np.arange(0,20),\n              'max_depth': [2, 4, 5, 7, 9, 10]}\n\ngrid_search=GridSearchCV(estimator=xgb ,param_grid=parameters,cv=10,n_jobs=-1,verbose=2)\ngrid_search.fit(X_train,y_train)\n\nxgb = grid_search.best_estimator_\n\ngrid_search.best_params_","5732ecd2":"y_pred=xgb.predict(X_test)\n\nprint(\"\\n\",confusion_matrix(y_test,y_pred))\nxgb_acc = accuracy_score(y_test,y_pred)\nprint(\"\\nAccuracy Score {}\".format(xgb_acc))\nprint(\"Classification report: \\n{}\".format(classification_report(y_test,y_pred)))","e7f06515":"model_ev = pd.DataFrame({\n                         'Model': ['Logistic Regression','Random Forest','K Neighbors Classifier','Support Vector Clasifier','XGBoost'], \n                         'Accuracy': [log_reg_acc*100, rf_acc*100, kn_acc*100,svc_acc*100,xgb_acc*100]\n                         })\nmodel_ev","d2e622c2":"colors = ['red','green','blue','yellow','orange',]\n\nplt.figure(figsize=(12,5))\nplt.title(\"Accuracy Graph\")\nplt.xlabel(\"Accuracy %\")\nplt.ylabel(\"Algorithms\")\nplt.bar(model_ev['Model'],model_ev['Accuracy'],color = colors)\nplt.show()","c37bfbcb":"estimators = [ ('xgb', xgb ), \n              ('kn', kn ),\n              #('svc',svc ),\n              #('log_Reg', log_reg),\n              ('rf',rf)]\n\n\nstack =StackingClassifier(estimators=estimators ,final_estimator= rf)\n\nstack.fit(X_train,y_train)\nstack_predicted = stack.predict(X_test)\n\nstack_conf_matrix = confusion_matrix(y_test, stack_predicted)\nstack_acc_score = accuracy_score(y_test, stack_predicted)\n\nprint(\"confussion matrix\")\nprint(stack_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Stacking Classifier:\",stack_acc_score*100,'\\n')\nprint(classification_report(y_test,stack_predicted))","f7a7e2c5":"# K Neighbors Classifier","6624911b":"# Heart Disease Detection\n### About the Data set\n\nThis dataset gives the information realated to heart disease. Dataset contain 13 columns, target is the class variable which is affected by other 12 columns. Here the aim is to classify the target variable to (disease\\non disease) using different machine learning algorithm and findout which algorithm suitable for this dataset.\n\n### Features Information:\n\n* Age : age in years\n* Sex : 1 = male; 0 = female\n* CP  : chest pain type\n* TRESTBPS : resting blood pressure (in mm * Hg on admission to the hospital)\n* CHOL : serum cholestoral in mg\/dl\n* FPS : fasting blood sugar > 120 mg\/dl (1 = true; 0 = false)\n* RESTECH : resting electrocardiographic results\n* THALACH : maximum heart rate achieved\n* EXANG : exercise induced angina (1 = yes; 0 = no)\n* OLDPEAK : ST depression induced by exercise relative to rest\n* SLOPE : the slope of the peak exercise ST segment\n* CA : number of major vessels (0-3) colored by flourosopy \n* THAL : 3 = normal; 6 = fixed defect; 7 = reversable defect\n* TARGET : 1 or 0\n\n","0ee2877f":"# Stacking ","1e4df953":"# Extreme Gradient Boosting","050badcb":"### Algorithms used: \nAn Ensemble stack of: \n* Logistic Regression\n* Random Forest Classifier\n* Extreme Gradient Boost\n* K-Nearest Neighbour\n* Support Vector Machine","dda75a06":"# Logistic Regression","171057b8":"# Random Forest","8b5ef853":"# Model Eval","3f643454":"# SVC"}}