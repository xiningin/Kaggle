{"cell_type":{"5b255ad3":"code","7f210d12":"code","67086ea7":"code","0b607e68":"code","7a356e99":"code","53d62801":"code","d744b772":"code","43552113":"code","6d1339b2":"code","b2638fa5":"code","9f1765cd":"code","eb68e833":"code","5fa51d64":"code","ad64e08e":"code","203555d3":"code","df4c2f3e":"code","9d85bca8":"code","febfae07":"markdown","03501f17":"markdown","7ae1959e":"markdown","a0243bbe":"markdown","a10c66c8":"markdown","6166e7d4":"markdown","4ce2504d":"markdown","5e515714":"markdown","f5beb25a":"markdown","4c5c71b8":"markdown","4f2f5535":"markdown","9ac0700f":"markdown","6036002e":"markdown","51e1cb53":"markdown","dea448b6":"markdown","270c9ba9":"markdown","2e017ede":"markdown","30966675":"markdown"},"source":{"5b255ad3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","7f210d12":"train.info(max_cols=250)","67086ea7":"test.info(max_cols=250)","0b607e68":"f, ax = plt.subplots(figsize=(25,5))\n\ntrain.drop(['target', 'ID_code'], axis=1).plot.box(ax=ax, rot=90)","7a356e99":"len(train.loc[train.target == 1])\/len(train)","53d62801":"r2 = pd.concat([train.drop(['target', 'ID_code'], axis=1), test.drop('ID_code', axis=1)]).corr()**2\nr2 = np.tril(r2, k=-1)  # remove upper triangle and diagonal\nr2[r2 == 0] = np.nan # replace 0 with nan","d744b772":"f, ax = plt.subplots(figsize=(20,20))\nsns.heatmap(np.sqrt(r2), annot=False,cmap='viridis', ax=ax)","43552113":"target_r2 = train.drop(['ID_code', 'target'], axis=1).corrwith(train.target).agg('square')\n\nf, ax = plt.subplots(figsize=(25,5))\ntarget_r2.agg('sqrt').plot.bar(ax=ax)","6d1339b2":"top = target_r2.loc[np.sqrt(target_r2) > 0.048].index\ntop","b2638fa5":"from sklearn.preprocessing import PolynomialFeatures\n\npolyfeat_train = pd.DataFrame(PolynomialFeatures(2).fit_transform(train[top]))\npolyfeat_test = pd.DataFrame(PolynomialFeatures(2).fit_transform(test[top]))","9f1765cd":"from imblearn.over_sampling import RandomOverSampler","eb68e833":"# additional imports\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import RobustScaler","5fa51d64":"from imblearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nimport lightgbm as lgb\n\nlgbpipe = Pipeline([('resample', RandomOverSampler(random_state=42)), ('model', lgb.LGBMClassifier(random_state=42, objective='binary', metric='auc', \n                                                                                                   boosting='gbdt', verbosity=1,\n                                                                                                   tree_learner='serial'))])\n\nparams = {    \n    \"model__max_depth\" : [20],\n    \"model__num_leaves\" : [30],\n    \"model__learning_rate\" : [0.1],\n    \"model__subsample_freq\": [5],\n    \"model__subsample\" : [0.3],\n    \"model__colsample_bytree\" : [0.05],\n    \"model__min_child_samples\": [100],\n    \"model__min_child_weight\": [10],\n    \"model__reg_alpha\" : [0.12],\n    \"model__reg_lambda\" : [15.5],\n    \"model__n_estimators\" : [600]\n    }\n\n# previous best-fit gridsearch parameters and results\n# {'model__colsample_bytree': 0.05, 'model__learning_rate': 0.1, 'model__max_depth': 20, 'model__min_child_samples': 100, 'model__min_child_weight': 10, 'model__n_estimators': 100, 'model__num_leaves': 30, 'model__reg_alpha': 0.1, 'model__reg_lambda': 10, 'model__subsample': 0.3, 'model__subsample_freq': 5}\n# 0.8735588789424164\n# {'model__colsample_bytree': 0.05, 'model__learning_rate': 0.1, 'model__max_depth': 20, 'model__min_child_samples': 100, 'model__min_child_weight': 10, 'model__n_estimators': 400, 'model__num_leaves': 30, 'model__reg_alpha': 0.12, 'model__reg_lambda': 0.2, 'model__subsample': 0.3, 'model__subsample_freq': 5}\n# 0.8915905852982839\n# {'model__colsample_bytree': 0.05, 'model__learning_rate': 0.1, 'model__max_depth': 20, 'model__min_child_samples': 100, 'model__min_child_weight': 10, 'model__n_estimators': 500, 'model__num_leaves': 30, 'model__reg_alpha': 0.12, 'model__reg_lambda': 0.2, 'model__subsample': 0.3, 'model__subsample_freq': 5}\n# 0.8923071245054173\n# {'model__colsample_bytree': 0.05, 'model__learning_rate': 0.1, 'model__max_depth': 20, 'model__min_child_samples': 100, 'model__min_child_weight': 10, 'model__n_estimators': 600, 'model__num_leaves': 30, 'model__reg_alpha': 0.12, 'model__reg_lambda': 0.2, 'model__subsample': 0.3, 'model__subsample_freq': 5}\n# 0.8925518240005254\n# {'model__colsample_bytree': 0.05, 'model__learning_rate': 0.1, 'model__max_depth': 20, 'model__min_child_samples': 100, 'model__min_child_weight': 10, 'model__n_estimators': 550, 'model__num_leaves': 30, 'model__reg_alpha': 0.12, 'model__reg_lambda': 0.2, 'model__subsample': 0.3, 'model__subsample_freq': 5}\n# 0.8924978701504809\n# {'model__colsample_bytree': 0.05, 'model__learning_rate': 0.1, 'model__max_depth': 20, 'model__min_child_samples': 100, 'model__min_child_weight': 10, 'model__n_estimators': 600, 'model__num_leaves': 30, 'model__reg_alpha': 0.12, 'model__reg_lambda': 15, 'model__subsample': 0.3, 'model__subsample_freq': 5}\n# 0.8941148812638564\n# {'model__colsample_bytree': 0.05, 'model__learning_rate': 0.1, 'model__max_depth': 20, 'model__min_child_samples': 100, 'model__min_child_weight': 10, 'model__n_estimators': 600, 'model__num_leaves': 30, 'model__reg_alpha': 0.5, 'model__reg_lambda': 12, 'model__subsample': 0.3, 'model__subsample_freq': 5}\n# 0.8938169988416745\n# {'model__colsample_bytree': 0.05, 'model__learning_rate': 0.1, 'model__max_depth': 20, 'model__min_child_samples': 100, 'model__min_child_weight': 10, 'model__n_estimators': 600, 'model__num_leaves': 30, 'model__reg_alpha': 0.3, 'model__reg_lambda': 15, 'model__subsample': 0.3, 'model__subsample_freq': 5}\n# 0.8941407236592286\n# {'model__colsample_bytree': 0.05, 'model__learning_rate': 0.1, 'model__max_depth': 20, 'model__min_child_samples': 100, 'model__min_child_weight': 10, 'model__n_estimators': 600, 'model__num_leaves': 30, 'model__reg_alpha': 0.2, 'model__reg_lambda': 15, 'model__subsample': 0.3, 'model__subsample_freq': 5}\n# 0.8938875270813017\n# {'model__colsample_bytree': 0.05, 'model__learning_rate': 0.1, 'model__max_depth': 20, 'model__min_child_samples': 100, 'model__min_child_weight': 10, 'model__n_estimators': 600, 'model__num_leaves': 30, 'model__reg_alpha': 0.2, 'model__reg_lambda': 15, 'model__subsample': 0.3, 'model__subsample_freq': 5}\n# 0.8938875270813017\n# {'model__colsample_bytree': 0.05, 'model__learning_rate': 0.1, 'model__max_depth': 20, 'model__min_child_samples': 100, 'model__min_child_weight': 10, 'model__n_estimators': 600, 'model__num_leaves': 30, 'model__reg_alpha': 0.12, 'model__reg_lambda': 15.5, 'model__subsample': 0.3, 'model__subsample_freq': 5}\n# 0.8943001048082946\n# {'model__colsample_bytree': 0.05, 'model__learning_rate': 0.1, 'model__max_depth': 20, 'model__min_child_samples': 100, 'model__min_child_weight': 10, 'model__n_estimators': 600, 'model__num_leaves': 30, 'model__reg_alpha': 0.12, 'model__reg_lambda': 15.2, 'model__subsample': 0.3, 'model__subsample_freq': 5}\n# 0.8939732044413886\n\nlgbgrid = GridSearchCV(lgbpipe, param_grid=params, cv=10, scoring='roc_auc')\nlgbgrid.fit(train.drop(['ID_code', 'target'], axis=1), train.target)\n\nprint(lgbgrid.best_params_)\nprint(lgbgrid.best_score_)","ad64e08e":"from sklearn.linear_model import RidgeClassifier\n\nridgepipe = Pipeline([('resample', RandomOverSampler(random_state=42)), ('scaler', RobustScaler()), ('model', RidgeClassifier(random_state=42))])\n\nparams = {'model__alpha': [1.0]} # between 0.5 and 2; best-fit so far: 1\n \nridgegrid = GridSearchCV(ridgepipe, param_grid=params, cv=3, scoring='roc_auc')\nridgegrid.fit(pd.concat([train.drop(['ID_code', 'target'], axis=1), polyfeat_train], axis=1, join='inner'), train.target)\n\nprint(ridgegrid.best_params_)\nprint(ridgegrid.best_score_)","203555d3":"pred = pd.DataFrame(lgbgrid.predict_proba(test.drop(['ID_code'], axis=1))[:, -1], columns=['target'], index=test.loc[:, 'ID_code'])\npred.to_csv('submission.csv', index=True)","df4c2f3e":"!head submission.csv","9d85bca8":"test.head()","febfae07":"Comparing the results between `lightgbm` and the ridge regression classifier, the former clearly wins despite the additional interaction features that are used in the ridge regression model. ","03501f17":"Inventory:\n* `train`: 200 features with continuous data + binary `target` feature + `ID_code` column; all features complete, no missing data\n* `test`: 200 features with continuous data + `ID_code` column; all features complete, no missing data\n\nFeature labels are **anonymized**.","7ae1959e":"## lightgbm","a0243bbe":"What is the distribution of the data in each feature?","a10c66c8":"# Modeling\n\nWe use `lightgbm` and a simple ridge regression as representatives of tree-based and linear models, respectively.\n\nGiven the highly imbalanced nature of the training data set, we apply a resampler for balancing. We tested `RandomUnderSampler` and `RandomOverSampler` from the `imblearn` module and found that the latter performs better with both models.**","6166e7d4":"In general, all features are **well-behaved**, most box-plot bars are symmetric, so **distributions are more or less symmetric**, as well. ","4ce2504d":"## Feature Engineering\n\nNot much is known about the features, except for what is listed above. This makes feature engineering hard. In a long (and blind) shot, we try the following:\n\nExtract the top $n$ features with $\\sqrt{r^2} \\geq 0.048$ and create second-degree interaction features based on those. These polynomial features will be added in the modeling step.\n\nThe value of 0.048 was derived by maximizing the `roc-auc` for the ridge regression model below. Interestingly, adding these interaction features to the `lightgbm` model does not improve its predictive power. ","5e515714":"# Submission","f5beb25a":"Explained absolute variation ($\\sqrt{r^2}$) up to 8%. Few features seem to stick out in terms of correlation with `target`.","4c5c71b8":"## Correlation analysis\n\nCheck for correlations between individual features:","4f2f5535":"Explained **absolute variation ($\\sqrt{r^2}$) between individual features is small** (< 1%). All features seem to be highly independent from each other.\n\nWhat about correlations between features and `target`?","9ac0700f":"# Changelog\n1. initial commit: cv score: 0.89, public score: 0.80\n2. explored a larger parameter space for `lightgbm` and force higher regularization parameters to prevent overfitting; submit probabilities instead of classes: cv score: 0.873, public score: 0.870\n3. some more fine-tuning of the model: cv score: 0.894 -- final submission","6036002e":"Is the training data set balanced?","51e1cb53":"## Ridge regression\n\nUsing a `RobustScaler`, which is probably not even necessary since the features are rather well-behaved.","dea448b6":"No, only about 10% of the training set have `target == 1` - the training **data set is highly imbalanced**. Will have to use resampling in the modeling!","270c9ba9":"## Feature completeness and data types","2e017ede":"# Data Exploration","30966675":"# Santander \n\nThis is a very simple kernel for the Santander competition featuring the following:\n* interaction features based on those features that show correlations with `target`;\n* re-sampling of the data to compensate for the imbalanced nature of the training data set using `imblearn`;\n* comparison of and modeling with `lightgbm` (using the `sklearn` interface) and a simple ridge classifier;\n* use of `GridSearchCV` and `sklearn.pipeline` to find the best-fit parameters;\n* no magic.\n"}}