{"cell_type":{"3da61a13":"code","21360fd1":"code","c65d33db":"code","558decdb":"code","4be1ced8":"code","3e4ba5bf":"code","13ca914c":"code","6029e00a":"code","8035e386":"code","2395e9d0":"code","6a6dae4e":"code","c2507201":"code","41f1adc3":"code","30d3c8e1":"code","838e9dfe":"code","6aa148c8":"code","074913b1":"code","2a5fc459":"code","c75188f5":"code","b6ec7cce":"code","f454c5e3":"code","daccaacc":"code","f7b17377":"code","27ae7b0d":"code","f2d49b66":"code","5ac2b3af":"code","768aec88":"code","83571e40":"code","fffadc69":"code","d758da38":"code","56fb9023":"code","12ab6445":"code","3f87ae2c":"code","7bbce23e":"code","bbb8aaa6":"code","76310865":"code","ef5ac8a9":"markdown","8d328df4":"markdown","7ee9b2d6":"markdown"},"source":{"3da61a13":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in\nget_ipython().run_line_magic('matplotlib', 'inline')\nimport os\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom scipy.stats import skew\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n","21360fd1":"raw_train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\nsample_sub_df = pd.read_csv('..\/input\/sample_submission.csv')\ndesc_lines = []\nwith open('..\/input\/data_description.txt', 'r') as f:\n    desc_lines = f.read().splitlines()\n\n    \n    ","c65d33db":"# sns.pairplot(raw_train_df[['SalePrice','GrLivArea']], height=3.5)","558decdb":"print(raw_train_df[(raw_train_df['OverallQual']<5) & (raw_train_df['SalePrice']>200000)].shape)\nprint(raw_train_df[(raw_train_df['GrLivArea']>4000) & (raw_train_df['SalePrice']<300000)].shape)","4be1ced8":"# As suggested by many participants, we remove several outliers\nraw_train_df.drop(raw_train_df[(raw_train_df['OverallQual']<5) & (raw_train_df['SalePrice']>200000)].index, inplace=True)\nraw_train_df.drop(raw_train_df[(raw_train_df['GrLivArea']>4000) & (raw_train_df['SalePrice']<300000)].index, inplace=True)\n\n# Some of the non-numeric predictors are stored as numbers; we convert them into strings \nraw_train_df['MSSubClass'] = raw_train_df['MSSubClass'].apply(str)\nraw_train_df['YrSold'] = raw_train_df['YrSold'].astype(str)\nraw_train_df['MoSold'] = raw_train_df['MoSold'].astype(str)","3e4ba5bf":"print('\\n'.join(desc_lines))\n","13ca914c":"target_col = 'SalePrice'\n# TODO: handle nan in real valued columns\nskip_cols = ['Id']#,'GarageYrBlt', 'MasVnrArea', 'LotFrontage']\n","6029e00a":"\nraw_categorical_features = {\n    'Utilities': {\n        'AllPub': 4,\n        'NoSewr': 3,\n        'NoSeWa': 2,\n        'ELO': 1\n    },\n    'ExterQual': {\n        'Ex': 5,\n        'Gd': 4,\n        'TA': 3,\n        'Fa': 2,\n        'Po': 1\n    },\n    'ExterCond': {\n        'Ex': 5,\n        'Gd': 4,\n        'TA': 3,\n        'Fa': 2,\n        'Po': 1\n    },\n    'BsmtQual': {\n        'Ex': 5,\n        'Gd': 4,\n        'TA': 3,\n        'Fa': 2,\n        'Po': 1,\n        'NA': float('nan')\n    },\n    'BsmtCond': {\n        'Ex': 5,\n        'Gd': 4,\n        'TA': 3,\n        'Fa': 2,\n        'Po': 1,\n        'NA': float('nan')\n    },\n    'BsmtExposure': {\n        'Gd': 5,\n        'Av': 4,\n        'Mn': 3,\n        'No': 2,\n        'NA': float('nan'),\n    },\n    'BsmtFinType1': {\n        'GLQ': 5,\n        'ALQ': 4,\n        'BLQ': 3,\n        'Rec': 2,\n        'LwQ': 1,\n        'Unf': float('nan'),\n        'NA': float('nan')\n    },\n    'HeatingQC': {\n        'Ex': 5,\n        'Gd': 4,\n        'TA': 3,\n        'Fa': 2,\n        'Po': 1,\n    },\n    'Electrical': {\n        'SBrkr': 5,\n        'FuseA': 4,\n        'FuseF': 3,\n        'FuseP': 2,\n        'Mix': 1\n    },\n    'KitchenQual': {\n        'Ex': 5,\n        'Gd': 4,\n        'TA': 3,\n        'Fa': 2,\n        'Po': 1,\n    },\n    'Functional': {\n        'Typ': 8,\n        'Min1': 7,\n        'Min2': 6,\n        'Mod': 5,\n        'Maj1': 4,\n        'Maj2': 3,\n        'Sev': 2,\n        'Sal': 1,\n    },\n    'FireplaceQu': {\n        'Ex': 5,\n        'Gd': 4,\n        'TA': 3,\n        'Fa': 2,\n        'Po': 1,\n        'NA': float('nan')\n    },\n    'GarageFinish': {\n        'Fin': 4,\n        'RFn': 3,\n        'Unf': 2,\n        'NA': float('nan'),\n    },\n    'GarageQual': {\n        'Ex': 5,\n        'Gd': 4,\n        'TA': 3,\n        'Fa': 2,\n        'Po': 1,\n        'NA': float('nan')\n    },\n    'GarageCond': {\n        'Ex': 5,\n        'Gd': 4,\n        'TA': 3,\n        'Fa': 2,\n        'Po': 1,\n        'NA': float('nan')\n    },\n    'PavedDrive': {\n        'Y': 3,\n        'P': 2,\n        'N': 1\n    },\n    'PoolQC': {\n        'Ex': 5,\n        'Gd': 4,\n        'TA': 3,\n        'Fa': 2,\n        'NA': float('nan')\n    },\n}\n\ndef get_skew_reduction_columns(train_df):\n    \"\"\"\n    Since we are dealing with large numbers, taking log reduces the skew and therefore makes it more \n    like normal distribution\n    \"\"\"\n    train_df = train_df[value_cols + [target_col]].fillna(train_df[value_cols].mean())\n    raw_skew = train_df.apply(skew)\n    raw_skew = raw_skew[raw_skew.abs() > 0.5]\n    large_skew_cols = raw_skew.index.tolist()\n    train_df = train_df[large_skew_cols]\n    mask = train_df !=0\n    train_df[mask] = train_df[mask].applymap(np.log)\n    improved_skew = train_df.apply(skew)\n    raw_skew = raw_skew[improved_skew.index]\n    diff = (raw_skew - improved_skew)\/raw_skew.abs()\n#     NOTE: this 0.5 is pretty important hyperparameter. It should have been better if it was picked after\n# k fold validation rather than manual selection.\n    return diff[diff > 0.3].index.tolist()\n    \ndef preprocess_categorical_features(df, categorical_features):\n    \"\"\"\n    There are several features which are categorical and whose values have\n    a predefined performance bias.\n    Excellent basement will have higher sale price than poor basement.\n    For such variables we need not go with one hot encoding and can use an integer.\n    \"\"\"\n    df = df.copy()\n    for column in categorical_features:\n        df.loc[:, column] = df[column].map(categorical_features[column])\n    #test\n    columns = list(categorical_features.keys())\n    assert not df[columns].applymap(lambda x: isinstance(x, str)).any().any()\n    \n    return df\n\ndef replace_nan_in_categorical_features(df, categorical_features):\n    \"\"\"\n    It computes the average value of SalePrice when the category is absent. It then finds a category such that\n    SalePrice is closest to it. nan is replaced by the value of the closest category found.\n    \"\"\"\n    categorical_features=categorical_features.copy()\n    for column in categorical_features:\n        nan_categories = []\n        for category, value in categorical_features[column].items():\n            if np.isnan(value):\n                nan_categories.append(category)\n#         print('Attempting to replace NA values in', nan_categories)\n        if not nan_categories:\n            continue\n        \n        avg_price = df[df[column]=='NA'][target_col].mean()\n        print('nan column count for ', column, ': ', df[df[column]=='NA'].shape, avg_price)\n        category_means = df[df[column] != 'NA'].groupby(column)[target_col].mean()\n        print(category_means)\n        closest_category = (\n            category_means - avg_price).abs().sort_values().index[0]\n        \n        for nan_category in nan_categories:\n            categorical_features[column][nan_category] = categorical_features[column][closest_category]\n        # test\n    for column in categorical_features:\n        for category, value in categorical_features[column].items():\n            assert not np.isnan(value)\n    \n    return categorical_features\n","8035e386":"trainable_cols = list(set(raw_train_df.columns) - set(skip_cols + [target_col]))","2395e9d0":"categorical_cols = []\nvalue_cols = []\nfor col in trainable_cols:\n    val =raw_train_df[col].dropna().iloc[0]\n    if isinstance(val, np.int64) or isinstance(val, float) or isinstance(val, np.int64):\n        value_cols.append(col)\n    elif isinstance(val, str):\n        categorical_cols.append(col)\n    else:\n        raise Exception('Unhandled type', type(val))\n    \n","6a6dae4e":"import seaborn as sns\nvar = 'FullBath'\ndata = pd.concat([raw_train_df['SalePrice'], raw_train_df[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis();","c2507201":"log_cols = get_skew_reduction_columns(raw_train_df)\nprint('Features which needs to be in log scale', log_cols)\nassert target_col in log_cols\ntrain_log_df = raw_train_df[log_cols].copy()\nnon_zero_mask_train = train_log_df > 0\ntrain_log_df[non_zero_mask_train] = train_log_df[non_zero_mask_train].applymap(np.log)\nraw_train_df.loc[:, log_cols] = train_log_df[log_cols].copy()\ndel train_log_df\n\ntest_log_cols = list(set(log_cols) - set([target_col]))\ntest_log_df = test_df[test_log_cols].copy()\nnon_zero_mask_test = test_log_df > 0\ntest_log_df[non_zero_mask_test]=test_log_df[non_zero_mask_test].applymap(np.log)\ntest_df.loc[:,test_log_cols] = test_log_df[test_log_cols].copy()","41f1adc3":"categorical_value_cols = list(raw_categorical_features.keys())\none_hot_enc_cols = list(set(categorical_cols) - set(categorical_value_cols))\nvalue_cols = list(set(value_cols + list(categorical_value_cols)))\n\nraw_train_df.loc[:,categorical_cols]= raw_train_df[categorical_cols].fillna('NA')\ntest_df.loc[:,categorical_cols]= test_df[categorical_cols].fillna('NA')\n\nraw_train_df = raw_train_df[trainable_cols + [target_col]]\ntest_df = test_df[trainable_cols + ['Id']]","30d3c8e1":"# filtr =np.random.rand(raw_train_df.shape[0]) > 0.2\n# train_df = raw_train_df[filtr].copy()\n# val_df = raw_train_df[~filtr].copy()\n# print('Train size', train_df.shape, 'Validation size', val_df.shape)\n# # this makes sure we do it on train_df and not the whole of data.\n# replace_nan_in_categorical_features(train_df)\n","838e9dfe":"enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\nenc.fit(raw_train_df[one_hot_enc_cols])\n\n\ndef get_X(df, encoder, categorical_features):\n    df = preprocess_categorical_features(df, categorical_features)    \n    encoded_df = encoder.transform(df[one_hot_enc_cols])\n    X = np.c_[encoded_df, df[value_cols].fillna(method='ffill').fillna(\n        method='bfill').values]\n    return X\n\n\n","6aa148c8":"# X_train = get_X(train_df.copy(), enc)\n# y_train = train_df[target_col].values\n\n# X_val = get_X(val_df.copy(), enc)\n# y_val = val_df[target_col].values\n\n","074913b1":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n","2a5fc459":"def _stat(pred, actual):\n    return np.round(sqrt(mean_squared_error(pred, actual)), 2)\n\n\ndef get_stats(pred_train, pred_val):\n\n    train_stat = _stat(pred_train, y_train)\n    val_stat = _stat(pred_val, y_val)\n    return (train_stat, val_stat)\n\n","c75188f5":"from sklearn.model_selection import KFold\nnF = 15\nkf = KFold(n_splits=nF, random_state=955, shuffle=True)\nkfold_train= []\nkfold_val= []\nifold = 0\nfor train_index, test_index in kf.split(raw_train_df.values):\n    print('Kfold ',ifold)\n    ifold = ifold + 1\n    train_df = raw_train_df.iloc[train_index].copy()   \n    categorical_features_kfold = replace_nan_in_categorical_features(train_df,\n                                                               raw_categorical_features)\n    X_train = get_X(train_df, enc, categorical_features_kfold)\n    X_val = get_X(raw_train_df.iloc[test_index].copy(), enc, categorical_features_kfold)\n    y_train=raw_train_df.iloc[train_index][target_col].values\n    y_val =raw_train_df.iloc[test_index][target_col].values\n    print ('Train', X_train.shape)\n    print ('Test', X_val.shape)\n\n    train_perf = []\n    val_perf = []\n    alphas = [0,0.000001, 0.000005, 0.00001,0.00004, 0.00006,0.00008, .0001,.0002]# 0.0001,0.00001, 0]\n    for alpha in alphas:\n        lr = Lasso(alpha=alpha, normalize=True, copy_X =True)\n        lr.fit(X_train, y_train)\n    #     print(lr.coef_)\n        prediction_train = lr.predict(X_train)\n        prediction_val = lr.predict(X_val)\n\n        (train_corr, val_corr) = get_stats(prediction_train, prediction_val)\n        train_perf.append(train_corr)\n        val_perf.append(val_corr)\n    kfold_train.append(train_perf)\n    kfold_val.append(val_perf)\n\ntrain_perf = np.mean(np.array(kfold_train), axis=0)\nval_perf = np.mean(np.array(kfold_val), axis=0)","b6ec7cce":"plt.figure(figsize=(10,10))\n\nax = plt.gca()\nax.plot(alphas, train_perf, label='train')\nax.plot(alphas, val_perf, label='validation')\nax.legend()\n","f454c5e3":"best_alpha = 0.00005\nmodels = []\nprediction_actual_test = 0\nifold=0\nfor train_index, test_index in kf.split(raw_train_df.values):\n    print('Kfold ',ifold)\n    ifold = ifold + 1\n    train_df = raw_train_df.iloc[train_index].copy()   \n    categorical_features_kfold = replace_nan_in_categorical_features(train_df,\n                                                               raw_categorical_features)\n    X_train = get_X(train_df, enc, categorical_features_kfold)    \n    y_train=raw_train_df.iloc[train_index][target_col].values\n\n\n    lr = Lasso(alpha=best_alpha, normalize=True, copy_X=True)\n    #lr = LinearRegression(normalize=True)\n    lr.fit(X_train, y_train)\n    models.append(lr)\n    X_actual_test = get_X(test_df, enc, categorical_features_kfold)\n    \n    prediction_actual_test = prediction_actual_test + lr.predict(X_actual_test)\n\nprediction_actual_test = prediction_actual_test \/len(models)\n","daccaacc":"column_names = []\nfor col in one_hot_enc_cols:\n    num_col_types = raw_train_df[col].unique()\n    column_names += [col + '_{}'.format(i) for i in num_col_types]\ncolumn_names += value_cols\nfinal_train_df = pd.DataFrame(X_train, columns=column_names)\nfinal_train_df['Error'] = y_train - lr.predict(X_train)","f7b17377":"final_train_df[final_train_df['Error'].abs() < 0.1].shape","27ae7b0d":"final_train_df[final_train_df['Error'].abs() > 0.15].shape","f2d49b66":"max_err_df = pd.DataFrame(index=final_train_df.columns, columns=['category', 'error', 'count'])\nmax_err_df.index.name = 'features'\nmax_err_df.name='error'\nfor col in final_train_df:\n    if col == 'Error':\n        continue\n    if len(final_train_df[col].unique())< 10:\n        mn =final_train_df[[col,'Error']].groupby(col).agg(['mean', 'count']).abs()['Error']\n        \n        max_err_df.loc[col,:] = mn.reset_index().sort_values('mean').values[-1:]\n# max_err_df.plot()","5ac2b3af":"max_err_df[max_err_df['count']> 10].sort_values('error').tail(10)","768aec88":"max_err_df[max_err_df['count']> 10].sort_values('error').tail(10)","83571e40":"# tmp_df= max_err_df.dropna().sort_values().tail(15)\n# fig, ax = plt.subplots()\n# tmp_df.plot( ax=ax) \n# print(tmp_df.tail(10))\n# del tmp_df","fffadc69":"# data_df.head()","d758da38":"# sns.set()\n# # 'RoofMatl',\n# bad_cols= ['BedroomAbvGr','PoolQC','Condition2','OverallCond','Utilities','BsmtFullBath','RoofMatl','Functional',\n#            'PoolArea',  target_col] \n# data_df = raw_train_df[bad_cols].copy()\n# # bad_cat_cols = list(set(bad_cols).intersection(set(one_hot_enc_cols)))\n# # data_df[bad_cat_cols]=data_df[bad_cat_cols].astype('category')\n# # data_df.loc[:,bad_cat_cols] = data_df[bad_cat_cols].apply(lambda x: x.cat.codes)\n# sns.pairplot(data_df, height=3.5,x_vars=target_col, y_vars=bad_cols[:-1])","56fb9023":"# from xgboost import XGBRegressor\n# xgb_train_perf = []\n# xgb_val_perf = []\n# max_depths=[1,2, 3,4,5]\n# # lrs = [0.1, 0.15, 0.2, 0.3, 0.4,0.5]\n# # reg_alpha (float (xgb's alpha)) \u2013 L1 regularization term on weights\n# # reg_lambda (float (xgb's lambda)) \u2013 L2 regularization term on weights\n# reg_alphas= [10, 50, 80, 100, 120, 150]\n# for max_depth in max_depths:\n#     for reg_alpha in reg_alphas:\n#         my_model = XGBRegressor(learning_rate=0.15, max_depth=max_depth, \n#                                 reg_lambda=0, reg_alpha=0.1, n_estimators=reg_alpha)\n#         # Add silent=True to avoid printing out updates with each cycle\n#         my_model.fit(X_train, y_train, verbose=True)\n#         xgb_val_predict = my_model.predict(X_val)\n#         xgb_train_predict = my_model.predict(X_train)\n#         (train_corr, val_corr) = get_stats(xgb_train_predict, xgb_val_predict)\n#         xgb_train_perf.append(train_corr)\n#         xgb_val_perf.append(val_corr)\n# # from sklearn.preprocessing import Normalizer\n# # final_train_df.loc[:,:] = Normalizer().fit_transform(final_train_df)\n# # final_train_df =final_train_df.sort_values('Error')\n# # # plt.figure(figsize=(20,10))\n# # err_df=final_train_df.corr()['Error'].sort_values().dropna().drop('Error')","12ab6445":"# xgb_val_df = pd.DataFrame(np.array(xgb_val_perf).reshape(len(max_depths),len(reg_alphas)),\n#                           columns=reg_alphas, index=max_depths)\n\n# xgb_train_df = pd.DataFrame(np.array(xgb_train_perf).reshape(len(max_depths),len(reg_alphas)),\n#                           columns=reg_alphas, index=max_depths)\n","3f87ae2c":"# for lr in reg_alphas:\n#     pd.concat([xgb_val_df[lr].to_frame('val'), xgb_train_df[lr].to_frame('train')],axis=1).plot(title='Lr: ' + str(lr))","7bbce23e":"# my_model = XGBRegressor(learning_rate=0.15, max_depth=3, \n#                                 reg_lambda=0, reg_alpha=0.1, n_estimators=100)\n# #lr = LinearRegression(normalize=True)\n# my_model.fit(X_train, y_train)\n# X_actual_test = get_X(test_df, enc)\n# prediction_actual_test = my_model.predict(X_actual_test)\n","bbb8aaa6":"output_df = pd.DataFrame(\n    np.c_[test_df.Id.values, prediction_actual_test],\n    columns=['Id', 'SalePrice'])\noutput_df.loc[:, target_col] = output_df[target_col].apply(np.exp)","76310865":"output_df.loc[:, 'Id'] = output_df.Id.apply(int)\noutput_df.to_csv('housing_prices_output.csv', index=False)\n","ef5ac8a9":"**https:\/\/www.kaggle.com\/agehsbarg\/top-10-0-10943-stacking-mice-and-brutal-force**","8d328df4":"**I learnt visualization from https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\nI got the confidence to do partial log transformation from the same (skipping 0 values ;) ).**","7ee9b2d6":" ***XGBoost does not improve performance.***"}}