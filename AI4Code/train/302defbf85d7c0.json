{"cell_type":{"4e2bf5de":"code","f73c5552":"code","f0028ff1":"code","23fbed09":"code","04dbaac6":"code","ac531f2f":"code","6071d709":"code","ea13e6bf":"code","a2db49e5":"code","c69e1abf":"code","6ba46d34":"code","acd07365":"code","9a913e7e":"code","51b24425":"code","8bc01abd":"code","6c4a73dd":"code","fdebb936":"code","9ba2f670":"code","4e0ece69":"code","2a031039":"code","3bc26ad2":"code","3b520e9a":"code","d37b0b70":"code","776d7e57":"code","073b9f33":"code","fcc5c3d2":"code","ad4f6692":"markdown","61c89caa":"markdown","22296ee1":"markdown","0f8b18fb":"markdown","88106ffd":"markdown","08dfab54":"markdown"},"source":{"4e2bf5de":"import os\nimport seaborn as sn ;sn.set(font_scale=1.2)\nimport matplotlib.pyplot as plt             \nimport cv2                                 \nimport tensorflow as tf\nimport numpy as np \nimport pandas as pd \n\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing import image\nfrom sklearn import metrics\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score\nfrom tqdm import tqdm","f73c5552":"class_names = [\"p1-327-chai-armc\",\"p1-327-chai-dini\",\"p1-327-chai-stoo\",\"p1-327-chai-work\",\"p1-328-sofa\",\"p2-80-arti\",\"p2-341-mode\",\"p3-3-curt\",\"p4-261-tabl\",\"p4-266-chan\",\"p6-108-tile\",\"p6-240-carp\",\"p6-348-wood\",\"p7-34-wood\",\"p7-109-magi\",\"p7-146-vene\",\"p7-175-tile\",\"p7-187-ston\",\"p7-214-wall\",\"p7-228-film\",\"p8-8-gyps\",\"p9-36-glas\",\"p11-353-door-door\",\"p12-365-rail\",]\nclass_names_label = {class_name:i \n                     for i, class_name in enumerate(class_names)\n                    }\n\nnb_classes = len(class_names)\n\nIMAGE_SIZE = (150, 150)\n\nBatchSize = 64","f0028ff1":"def load_data():\n\n    datasets = ['..\/input\/wazzadu-24classes\/wazzadu_24classes\/train','..\/input\/wazzadu-24classes\/wazzadu_24classes\/test']\n    output = []\n    \n    # Iterate through training and test sets\n    for dataset in datasets:\n        \n        images = []\n        labels = []\n        \n        print(\"Loading {}\".format(dataset))\n        \n        # Iterate through each folder corresponding to a category\n        for folder in os.listdir(dataset):\n            label = class_names_label[folder]\n            \n            # Iterate through each image in our folder\n            for file in tqdm(os.listdir(os.path.join(dataset, folder))):\n                \n                # Get the path name of the image\n                img_path = os.path.join(os.path.join(dataset, folder), file)\n                \n                # Open and resize the img\n                image = cv2.imread(img_path)\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                image = cv2.resize(image, IMAGE_SIZE) \n                \n                # Append the image and its corresponding label to the output\n                images.append(image)\n                labels.append(label)\n                \n        images = np.array(images, dtype = 'float32')\n        labels = np.array(labels, dtype = 'int32')   \n        \n        output.append((images, labels))\n\n    return output","23fbed09":"(train_images, train_labels),(test_images, test_labels) = load_data()","04dbaac6":"# shuffle data\n\ntrain_images, train_labels = shuffle(train_images, train_labels, random_state=42)\n\nn_train = train_labels.shape[0]\nn_test = test_labels.shape[0]\n\nprint (\"Number of training examples: {}\".format(n_train))\nprint (\"Number of testing examples: {}\".format(n_test))\nprint (\"Each image is of size: {}\".format(IMAGE_SIZE))","ac531f2f":"_, train_counts = np.unique(train_labels, return_counts=True)\n_, test_counts = np.unique(test_labels, return_counts=True)\npd.DataFrame({'train': train_counts,\n                    'test': test_counts}, \n             index=class_names\n            ).plot.bar()\nplt.show()\n\nplt.pie(train_counts,\n        explode=(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0), \n        labels=class_names,\n        autopct='%1.1f%%')\n\nplt.axis('equal')\n# plt.title('\u0e2a\u0e31\u0e14\u0e2a\u0e48\u0e27\u0e19\u0e02\u0e2d\u0e07\u0e41\u0e15\u0e48\u0e25\u0e30 Class \u0e17\u0e35\u0e48\u0e19\u0e33\u0e21\u0e32\u0e43\u0e0a\u0e49\u0e1d\u0e36\u0e01')\nplt.show()","6071d709":"train_images = train_images \/ 255.0 \ntest_images = test_images \/ 255.0","ea13e6bf":"def display_random_image(class_names, images, labels):\n    \n    index = np.random.randint(images.shape[0])\n    plt.figure()\n    plt.imshow(images[index])\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(True)\n    plt.title('Image #{} : '.format(index) + class_names[labels[index]])\n    plt.show()","a2db49e5":"display_random_image(class_names, train_images, train_labels)","c69e1abf":"def display_examples(class_names, images, labels):\n\n    fig = plt.figure(figsize=(10,10))\n    fig.suptitle(\"Some examples of Wazzadu 12 Classes\", fontsize=20)\n    for i in range(121):\n        plt.subplot(11,11,i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(True)\n        plt.imshow(images[i], cmap=plt.cm.binary)\n        plt.xlabel(class_names[labels[i]])\n    plt.show()","6ba46d34":"display_examples(class_names, train_images, train_labels)","acd07365":"x_train = train_images\ny_train = train_labels\n\nx_test = test_images\ny_test = test_labels","9a913e7e":"# reorganize by groups\ntrain_groups = [x_train[np.where(y_train==i)[0]] for i in np.unique(y_train)]\ntest_groups = [x_test[np.where(y_test==i)[0]] for i in np.unique(y_train)]\nprint('train groups:', [x.shape[0] for x in train_groups])\nprint('test groups:', [x.shape[0] for x in test_groups])","51b24425":"def gen_random_batch(in_groups, batch_halfsize = 8):\n    out_img_a, out_img_b, out_score = [], [], []\n    all_groups = list(range(len(in_groups)))\n    for match_group in [True, False]:\n        group_idx = np.random.choice(all_groups, size = batch_halfsize)\n        out_img_a += [in_groups[c_idx][np.random.choice(range(in_groups[c_idx].shape[0]))] for c_idx in group_idx]\n        if match_group:\n            b_group_idx = group_idx\n            out_score += [1]*batch_halfsize\n        else:\n            # anything but the same group\n            non_group_idx = [np.random.choice([i for i in all_groups if i!=c_idx]) for c_idx in group_idx] \n            b_group_idx = non_group_idx\n            out_score += [0]*batch_halfsize\n            \n        out_img_b += [in_groups[c_idx][np.random.choice(range(in_groups[c_idx].shape[0]))] for c_idx in b_group_idx]\n            \n    return np.stack(out_img_a,0), np.stack(out_img_b,0), np.stack(out_score,0)","8bc01abd":"pv_a, pv_b, pv_sim = gen_random_batch(train_groups, 3)\nfig, m_axs = plt.subplots(2, pv_a.shape[0], figsize = (12, 6))\nfor c_a, c_b, c_d, (ax1, ax2) in zip(pv_a, pv_b, pv_sim, m_axs.T):\n    ax1.imshow(c_a[:,:,0])\n    ax1.set_title('Image A')\n    ax1.axis('off')\n    ax2.imshow(c_b[:,:,0])\n    ax2.set_title('Image B\\n Similarity: %3.0f%%' % (100*c_d))\n    ax2.axis('off')","6c4a73dd":"from keras.models import Model\nfrom keras.layers import Input, Conv2D, BatchNormalization, MaxPool2D, Activation, Flatten, Dense, Dropout\n\nimg_in = Input(shape = x_train.shape[1:], name = 'FeatureNet_ImageInput')\nn_layer = img_in\n\nfor i in range(2):\n    n_layer = Conv2D(8*2**i, kernel_size = (3,3), activation = 'linear')(n_layer)\n    n_layer = BatchNormalization()(n_layer)\n    n_layer = Activation('relu')(n_layer)\n    n_layer = Conv2D(16*2**i, kernel_size = (3,3), activation = 'linear')(n_layer)\n    n_layer = BatchNormalization()(n_layer)\n    n_layer = Activation('relu')(n_layer)\n    n_layer = MaxPool2D((2,2))(n_layer)\n    \nn_layer = Flatten()(n_layer)\nn_layer = Dense(32, activation = 'linear')(n_layer)\nn_layer = Dropout(0.5)(n_layer)\nn_layer = BatchNormalization()(n_layer)\nn_layer = Activation('relu')(n_layer)\n\nfeature_model = Model(inputs = [img_in], outputs = [n_layer], name = 'FeatureGenerationModel')\n\nfeature_model.summary()","fdebb936":"from keras.layers import concatenate\n\nimg_a_in = Input(shape = x_train.shape[1:], name = 'ImageA_Input')\nimg_b_in = Input(shape = x_train.shape[1:], name = 'ImageB_Input')\n\nimg_a_feat = feature_model(img_a_in)\nimg_b_feat = feature_model(img_b_in)\n\ncombined_features = concatenate([img_a_feat, img_b_feat], name = 'merge_features')\ncombined_features = Dense(16, activation = 'linear')(combined_features)\ncombined_features = BatchNormalization()(combined_features)\ncombined_features = Activation('relu')(combined_features)\ncombined_features = Dense(4, activation = 'linear')(combined_features)\ncombined_features = BatchNormalization()(combined_features)\ncombined_features = Activation('relu')(combined_features)\ncombined_features = Dense(1, activation = 'sigmoid')(combined_features)\nsimilarity_model = Model(inputs = [img_a_in, img_b_in], outputs = [combined_features], name = 'Similarity_Model')\n\nsimilarity_model.summary()","9ba2f670":"# setup the optimization process\n\nsimilarity_model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['mae'])","4e0ece69":"# make a generator out of the data\n\ndef siam_gen(in_groups, batch_size = 32):\n    while True:\n        pv_a, pv_b, pv_sim = gen_random_batch(train_groups, batch_size\/\/2)\n        yield [pv_a, pv_b], pv_sim\n        \n# we want a constant validation group to have a frame of reference for model performance\n\nvalid_a, valid_b, valid_sim = gen_random_batch(test_groups, 1024)\n\nloss_history = similarity_model.fit_generator(siam_gen(train_groups), \n                               steps_per_epoch = 500,\n                               validation_data=([valid_a, valid_b], valid_sim),\n                                              epochs = 10,\n                                             verbose = True)","2a031039":"def show_model_output(nb_examples = 3):\n    pv_a, pv_b, pv_sim = gen_random_batch(test_groups, nb_examples)\n    pred_sim = similarity_model.predict([pv_a, pv_b])\n    fig, m_axs = plt.subplots(2, pv_a.shape[0], figsize = (12, 6))\n    for c_a, c_b, c_d, p_d, (ax1, ax2) in zip(pv_a, pv_b, pv_sim, pred_sim, m_axs.T):\n        ax1.imshow(c_a[:,:,0])\n        ax1.set_title('Image A\\n Actual: %3.0f%%' % (100*c_d))\n        ax1.axis('off')\n        ax2.imshow(c_b[:,:,0])\n        ax2.set_title('Image B\\n Predicted: %3.0f%%' % (100*p_d))\n        ax2.axis('off')\n    return fig\n\n# a completely untrained model\n\n_ = show_model_output()","3bc26ad2":"# make a generator out of the data\n\ndef siam_gen(in_groups, batch_size = 32):\n    while True:\n        pv_a, pv_b, pv_sim = gen_random_batch(train_groups, batch_size\/\/2)\n        yield [pv_a, pv_b], pv_sim\n        \n# we want a constant validation group to have a frame of reference for model performance\n\nvalid_a, valid_b, valid_sim = gen_random_batch(test_groups, 1024)\nloss_history = similarity_model.fit_generator(siam_gen(train_groups), \n                               steps_per_epoch = 500,\n                               validation_data=([valid_a, valid_b], valid_sim),\n                                              epochs = 10,\n                                             verbose = True)\n","3b520e9a":"_ = show_model_output()","d37b0b70":"t_shirt_vec = np.stack([train_groups[0][0]]*x_test.shape[0],0)\nt_shirt_score = similarity_model.predict([t_shirt_vec, x_test], verbose = True, batch_size = 128)\nankle_boot_vec = np.stack([train_groups[-1][0]]*x_test.shape[0],0)\nankle_boot_score = similarity_model.predict([ankle_boot_vec, x_test], verbose = True, batch_size = 128)","776d7e57":"x_test_features = feature_model.predict(x_test, verbose = True, batch_size=128)","073b9f33":"%%time\nfrom sklearn.manifold import TSNE\ntsne_obj = TSNE(n_components=2,\n                         init='pca',\n                         random_state=101,\n                         method='barnes_hut',\n                         n_iter=500,\n                         verbose=2)\ntsne_features = tsne_obj.fit_transform(x_test_features)","fcc5c3d2":"obj_categories = class_names\ncolors = plt.cm.rainbow(np.linspace(0, 1, 10))\nplt.figure(figsize=(10, 10))\n\nfor c_group, (c_color, c_label) in enumerate(zip(colors, obj_categories)):\n    plt.scatter(tsne_features[np.where(y_test == c_group), 0],\n                tsne_features[np.where(y_test == c_group), 1],\n                marker='o',\n                color=c_color,\n                linewidth='1',\n                alpha=0.8,\n                label=c_label)\n    \nplt.xlabel('Dimension 1')\nplt.ylabel('Dimension 2')\nplt.title('t-SNE on Testing Samples')\nplt.legend(loc='best')\nplt.savefig('armchair-and-lounge-chair_676.jpg')\nplt.show(block=False)","ad4f6692":"# Feature Generation","61c89caa":"# Siamese Model","22296ee1":"# Prediction","0f8b18fb":"# Validate Data","88106ffd":"# Visual Model Feedback","08dfab54":"# Batch Generation"}}