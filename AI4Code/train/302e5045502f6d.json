{"cell_type":{"21b53313":"code","9ca11291":"code","551286e1":"code","c2195b4e":"code","98b4d2d9":"code","813c6fa4":"code","d18bacc1":"code","1689ffeb":"code","babd768d":"code","7933cf2f":"code","2e3e0e83":"code","da1fc321":"code","3e861b1b":"code","2e781110":"code","0e242c2c":"code","1aabce7d":"code","73afcae3":"code","0ad0d144":"code","c0827fbb":"code","3375c64f":"code","612c92de":"code","7b48327a":"code","4ddb4b13":"code","cde02c3a":"code","46e6bb16":"code","d8874cc9":"code","29b7de03":"code","5405f506":"code","8c95901f":"code","db8b94ad":"code","3766b4f4":"code","aa572512":"code","edf50f91":"code","87172ef7":"code","c49d1fa4":"code","4a15209e":"code","c9d2d138":"code","b18b6a29":"code","86c5c101":"code","58206609":"code","8c107fb6":"markdown","41a0d3b3":"markdown","5a2245b9":"markdown","380ee5ee":"markdown","c04b8c52":"markdown","35bd6dca":"markdown","f48cc296":"markdown","dfe31d2a":"markdown","e5dc85d5":"markdown","7216ebc5":"markdown"},"source":{"21b53313":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9ca11291":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","551286e1":"df_train = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")","c2195b4e":"df_train.head()","98b4d2d9":"df_train.info()","813c6fa4":"df_train.describe()","d18bacc1":"df_train.isnull().sum()","1689ffeb":"five = df_train.iloc[df_train[df_train[\"label\"]==5].index[0], 1:].values.reshape(28, 28)\nplt.imshow(five, cmap='gray')","babd768d":"drop_col = []\nfor col in list(df_train.columns):\n    if len(df_train[col].value_counts())==1:\n        drop_col.append(col)","7933cf2f":"df_train = df_train.drop(drop_col, axis = 1)\ndf_test = df_test.drop(drop_col, axis = 1)","2e3e0e83":"sns.countplot(x= df_train[\"label\"]) # Almost balanced dataset","da1fc321":"df_train.describe()","3e861b1b":"df_train.head()","2e781110":"y = df_train.pop('label')\nX = df_train","0e242c2c":"# Normalizing the remaining variables\ndf_train[X.columns] = df_train[X.columns].apply(lambda x: x\/255)","1aabce7d":"df_train.describe()","73afcae3":"from sklearn.model_selection import train_test_split","0ad0d144":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=100)","c0827fbb":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score","3375c64f":"svc_linear = SVC(kernel='linear')\nsvc_linear.fit(X_train, y_train)","612c92de":"print(accuracy_score(y_train, svc_linear.predict(X_train)))\nprint(accuracy_score(y_test, svc_linear.predict(X_test)))","7b48327a":"svc = SVC()\nsvc.fit(X_train, y_train)","4ddb4b13":"print(accuracy_score(y_train, svc.predict(X_train)))\nprint(accuracy_score(y_test, svc.predict(X_test)))","cde02c3a":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV","46e6bb16":"parameters = {'C':[1], \n             'gamma': [1e-3]}\n\nsvc_grid_search = SVC(kernel=\"rbf\")\n\nclf = GridSearchCV(svc_grid_search, param_grid=parameters, scoring='accuracy')\n\nclf.fit(X_train, y_train)","d8874cc9":"import xgboost as xgb","29b7de03":"paramters_xgb =  {\n        'min_child_weight': [10],\n        'gamma': [0.5],\n        'max_depth': [10]\n        }","5405f506":"xgb_grid_search = xgb.XGBClassifier()\n\nclf = RandomizedSearchCV(xgb_grid_search, param_distributions=paramters_xgb, scoring='accuracy', cv=4, verbose=3, random_state=100)\n\nclf.fit(X_train, y_train)","8c95901f":"print(clf.best_estimator_)\nprint(clf.best_score_ )\nprint(clf.best_params_)","db8b94ad":"print(accuracy_score(y_train, clf.predict(X_train)))\nprint(accuracy_score(y_test, clf.predict(X_test)))","3766b4f4":"xg = xgb.XGBClassifier(min_child_weight=10, max_depth=5, gamma=0.5)\nxg.fit(X_train, y_train)","aa572512":"print(accuracy_score(y_train, xg.predict(X_train)))\nprint(accuracy_score(y_test, xg.predict(X_test)))","edf50f91":"test_pred = xg.predict(df_test)","87172ef7":"sample_submission = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')\nsample_submission.head()","c49d1fa4":"df_test[X.columns] = df_test[X.columns].apply(lambda x: x\/255)","4a15209e":"test_pred = xg.predict(df_test)","c9d2d138":"index_list = []\nfor i in list(df_test.index):\n    index_list.append(i+1)","b18b6a29":"submission_df = pd.DataFrame({\n    \"ImageId\": index_list,\n    \"Label\": test_pred\n})","86c5c101":"submission_df.head()","58206609":"submission_df.to_csv('submission_xgb.csv', index=False)","8c107fb6":"# Data Preparation for Model Building","41a0d3b3":"The accuracy achieved with a non-linear kernel is slightly higher than a linear one. Let's now do a grid search CV to tune the hyperparameters C and gamma.\n\n### Grid Search Cross-Validation","5a2245b9":"# RBF kernel SVM\n\nLet's first try building a linear SVM model (i.e. a RBF kernel). ","380ee5ee":"# Submission File","c04b8c52":"# Model Building\n\nLet's now build the model and tune the hyperparameters. Let's start with a **linear model** first.\n\n# Linear SVM\n\nLet's first try building a linear SVM model (i.e. a linear kernel). ","35bd6dca":"# Loading Dataset","f48cc296":"# Xgboost","dfe31d2a":"# Loading Libraries","e5dc85d5":"# MNIST Digits - Classification Using SVM\n\nIn this notebook, we'll explore the popular MNIST dataset and build an SVM model to classify handwritten digits. <a href='http:\/\/yann.lecun.com\/exdb\/mnist\/'>Here is a detailed description of the dataset.<\/a>\n\nWe'll divide the analysis into the following parts:\n- Data understanding and cleaning\n- Data preparation for model building\n- Building an SVM model - hyperparameter tuning, model evaluation etc.","7216ebc5":"# Spliting data into train and test"}}