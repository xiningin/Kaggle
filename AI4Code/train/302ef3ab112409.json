{"cell_type":{"031c5ab9":"code","81be800f":"code","923ff9ae":"code","59bc4045":"code","4f091f98":"code","f2d97106":"code","6890e180":"code","4a799b13":"code","22a1ada6":"code","b908d00a":"code","c6d6ba53":"code","d53ffc58":"code","7b3feee3":"code","bcae4e59":"code","3fa93eda":"code","7c340beb":"code","9a006ea8":"code","9df30824":"code","f35ecfb0":"code","5ce482fe":"code","9ac2b70f":"code","b6307de8":"code","6f0edbdf":"code","cc625293":"code","2c88cecc":"code","38f0699e":"code","4c80dd4f":"code","7c3e2eac":"code","05acc3aa":"code","4d714462":"code","51a05d9a":"code","259f798a":"code","b130372f":"code","f4cca372":"code","7ed90f44":"code","864a1c4e":"code","b5ea6750":"code","f75e38b5":"markdown","5e92beeb":"markdown","73c6af88":"markdown","1295bbe6":"markdown","c248aa8e":"markdown","9e3d38de":"markdown","10eef72d":"markdown","59cedeb9":"markdown","62f5a38a":"markdown"},"source":{"031c5ab9":"import numpy as np \nimport pandas as pd \nimport cudf,cuml,cupy\n#from cuml.feature_extraction.text import CountVectorizer\n#from cuml.cluster import KMeans\n#from cuml.manifold import TSNE\nimport matplotlib.pyplot as plt \nimport os \nimport tensorflow as tf \nimport cv2 as cv \nfrom PIL import Image\nimport seaborn as sns\nimport plotly.express as px\nimport string\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import NearestNeighbors\nimport warnings\nfrom tqdm import tqdm\nimport gc\nwarnings.filterwarnings(\"ignore\")\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.preprocessing.image import load_img , img_to_array\nfrom sklearn.metrics.pairwise import cosine_distances\nfrom collections import Counter\nimport nltk","81be800f":"# Load data for training sets\ntraining_csv =pd.read_csv(\"..\/input\/shopee-product-matching\/train.csv\")\ntraining_img = \"..\/input\/shopee-product-matching\/train_images\"","923ff9ae":"# Load data for test sets\ntest_csv = pd.read_csv(\"..\/input\/shopee-product-matching\/test.csv\")\ntest_img = \"..\/input\/shopee-product-matching\/test_images\"","59bc4045":"# Get look at the datas.\ntraining_csv.head()","4f091f98":"# explorate number duplicates per label_group\n# And visualize the most 100 duplicated label_group\nser= training_csv[\"label_group\"].value_counts()\nser = pd.DataFrame(ser).reset_index(drop=False).rename(columns={\"label_group\":\"Occurences\",\"index\":\"label_group\"})\nser[\"label_group\"] = ser[\"label_group\"].astype(\"str\")\nfig = px.bar(ser[:100],x=\"label_group\",y=\"Occurences\",color= \"Occurences\",\\\n             title=\"TOP 100 most duplicated label_group \")\nfig.update_layout(title ={\"x\":0.475,\"y\":0.9,\"xanchor\":\"center\",\"yanchor\":\"top\"})\nfig.show()","f2d97106":"# Create new feature who summarize the list of posting_id associated to each label_group value. \nlab=training_csv.groupby(\"label_group\")[\"posting_id\"].agg(\"unique\")\ntraining_csv[\"target\"]= training_csv.label_group.map(lab)","6890e180":"def display_related_products(feature_value,df=training_csv,feature=\"label_group\",title=\"\"):\n    \"\"\" Display related photos based on the introduced feature criterion and the value of this\n        feature introduced as parametr.\n      @ args :\n      feature_value(int) : \n      df(DataFrame) : the dataframe that we will use in our function (default = training_csv)\n      feature(str) : the name of feature, used to group items in the same collection.\n      title (str) : The title to assigne to the whole displayed images \n        \n    \"\"\"\n    \n    related_photos = df.loc[df[feature] ==feature_value,[\"image\",\"title\"]]\n    l = len(related_photos)\n    range_k = [2,3,4,5,6,7]\n    k = 0\n    for j in range_k :\n        if l%j == 0 :\n            k=j\n            break\n    if k == 0 :\n        k = 5\n    nb_l = l \/\/k\n    nb_l += int((l%k) !=0)\n         \n    fig,ax = plt.subplots(nb_l,k,figsize=(k*10,nb_l*10))\n    i = 0\n    for row in related_photos.iterrows() :\n        \n        chemin = os.path.join(training_img,row[1][\"image\"])\n        image = Image.open(chemin)\n        image = np.array(image)\n        if nb_l == 1 :\n            ax[i%k].imshow(image)\n            ax[i%k].set_title(row[1][\"title\"],fontsize=10,fontweight=\"bold\")\n        else :\n            ax[i\/\/k,i%k].imshow(image)\n            ax[i\/\/k,i%k].set_title(row[1][\"title\"],fontsize=12,fontweight=\"bold\")\n        i += 1\n    plt.suptitle(title,fontsize =36,\\\n                 size=32,color=\"red\",fontweight=\"bold\")\n    plt.show()","4a799b13":"# Display related images for the most duplicated label_group items.\nmost_label_group = int(ser.iloc[0].label_group)\ndisplay_related_products(most_label_group,title=\"RELATED IMAGES FOR THE MOST DUPLICATED LABEL_GROUP ITEMS\")","22a1ada6":"# Display related images for the smallest duplicated label_group items.\nsmallest_label_group = int (ser.iloc[-1].label_group)\ndisplay_related_products(smallest_label_group,title=\"SMALLEST DUPLICATED LABEL_GROUP ITEMS\")","b908d00a":"def clean(title):\n    \"\"\"This function, allows to clean title from useless characters and symbols.\n    \n    @ params :\n    title(str) : the title text that the function will clean up.\n    \n    @ returns :\n    title(str) : cleaned title\n\n    \n    \"\"\"\n    title = title.lower()\n    title = re.sub(r\"\\-\",\" \",title)\n    title = re.sub(r\"\\+\",\" \",title)\n    title = re.sub (r\"&\",\"and\",title)\n    title = re.sub(r\"\\|\",\" \",title)\n    title = re.sub(r\"\\\\\",\" \",title)\n    title = re.sub(r\"\\W\",\" \",title)\n    for p in string.punctuation :\n        title = re.sub(r\"f{p}\",\" \",title)\n    \n    title = re.sub(r\"\\s+\",\" \",title)\n    \n    return title","c6d6ba53":"test_cdf = cudf.read_csv(\"..\/input\/shopee-product-matching\/train.csv\")\ntest = training_csv\ntest[\"cleaned_title\"] = test[\"title\"].map(clean)\ntest_cdf[\"cleaned_title\"] = test[\"cleaned_title\"]\n#test_cdf = cudf.concat([test_cdf,test_cdf,test_cdf[:],axis=0,ignore_index=False)\n#test = pd.concat([test,test,test[:4000]],axis=0,ignore_index=False)\nsubmission = True \nimages = training_img\nif len(test_csv) > 3 :\n   test_cdf = cudf.read_csv(\"..\/input\/shopee-product-matching\/test.csv\")\n   test = pd.read_csv(\"..\/input\/shopee-product-matching\/test.csv\")\n   test[\"cleaned_title\"] = test[\"title\"].map(clean)\n   test_cdf[\"cleaned_title\"] = test[\"cleaned_title\"]\n   images = test_img\n   submission = False","d53ffc58":"def getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n \/ (len(row.target)+len(row[col]))\n    return f1score","7b3feee3":"def sub_matches(row):\n    return \" \".join(row.pred_tf)","bcae4e59":"corpus = []\nfor tx in test[\"cleaned_title\"].values:\n    text = tx.lower()\n    corpus.extend(text.split())\nwords = set(corpus)","3fa93eda":"nuniques_words = len(words)","7c340beb":"counter = Counter(corpus)","9a006ea8":"seuil = [0.01,0.025,0.05,0.1,0.2 ,0.4 ,0.6 ]","9df30824":"from cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\nif submission :\n bseuil = 0\n bscore = 0\n bd = 0\n for sl in tqdm(seuil) : \n    seuil = int (sl * nuniques_words)\n    stop_words = list(zip(*counter.most_common(seuil)))[0]\n    sw = set()\n    sw.update(stop_words)\n    sw.update(nltk.corpus.stopwords.words(\"english\"))\n    tf_idf = TfidfVectorizer(stop_words=stop_words,max_features=25000,binary=True)\n    embedding = tf_idf.fit_transform(test_cdf[\"cleaned_title\"]).toarray()\n    tf_distance = NearestNeighbors(n_neighbors=50,metric=\"cosine\")\n    tf_distance.fit(embedding)\n    chunk = 4 * 1024\n    cls = len(test)\/\/chunk\n    cls += int((len(test)% chunk) != 0)\n    d = [0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6]\n    best_d = 0\n    best_score = 0\n    for di in d :\n     prediction = []\n     for i in range(cls) :\n       a = i * chunk \n       b = (i+1) * chunk \n       b = min(b,len(test))\n       distances , indices = tf_distance.kneighbors(embedding[a:b,])\n       for j in range(b-a) :\n         distance = distances[j,:]\n         ind = np.where(distance < di )[0]\n         ind = indices[j,ind]\n         ind = cupy.asnumpy(ind)\n         prediction.append(test.iloc[ind].posting_id.values)\n     test[\"pred_tfidf\"] = prediction \n     test[\"f5\"] = test.apply(getMetric(\"pred_tfidf\"),axis=1)\n     sc = test.f5.mean()\n     if sc > best_score :\n            best_score = sc \n            best_d = di \n    if best_score >  bscore :\n     bseuil =  sl \n     bscore = best_score\n     bd = best_d\n            \n            ","f35ecfb0":"if submission : \n    print('CV score for tf_idf embedding text = ',bscore)\n    print(\"best threshold to use to define our stops words= \",bseuil)\n    print(\"best distance to use to define similarity = \",bd)","5ce482fe":"stop_words = list(zip(*counter.most_common(int(0.01 * nuniques_words))))[0]\nsw = set()\nsw.update(stop_words)\nsw.update(nltk.corpus.stopwords.words(\"english\"))\ntf_idf = TfidfVectorizer(stop_words=sw,max_features=25000,binary=True)\nembedding = tf_idf.fit_transform(test_cdf[\"cleaned_title\"]).toarray()","9ac2b70f":"kn = NearestNeighbors(n_neighbors=50,metric=\"cosine\")\nkn.fit(embedding)","b6307de8":"prediction = []\nchunk = 4 *1024 \ncls = len(test) \/\/ chunk \ncls += int((len(test) % chunk) !=0)\nfor i in tqdm(range(cls)):\n    a = i * chunk \n    b = (i+1) * chunk \n    b = min (b,len(test))\n    distances , indices = kn.kneighbors(embedding[a:b,])\n    for j in range(b-a):\n        distance = distances[j,:]\n        ind = np.where(distance < 0.45)[0]\n        ind = indices[j,ind]\n        ind = cupy.asnumpy(ind)\n        prediction.append(test.iloc[ind].posting_id.values)\n    ","6f0edbdf":"test[\"pred_tfidf\"] = prediction \nif submission : \n    test[\"f5\"] = test.apply(getMetric(\"pred_tfidf\"),axis=1)\n    \n    print('CV score for tf embedding text =',test.f5.mean())","cc625293":"LIMIT = 1\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus :\n    try :\n       tf.config.experimental.set_virtual_device_configuration(gpus[0],\\\n                                                           [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n       logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n    \n    except RuntimeError as e :\n       print(e)\nprint('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\nprint('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))","2c88cecc":"class DataGenerator(Sequence):\n    \n    def __init__(self,df,img_size=224,path = images,batch_size = 32):\n        \n        self.df = df \n        self.img_size = img_size\n        self.path = path \n        self.batch_size = batch_size\n        self.indexes = np.arange(len(self.df))\n    def __len__(self) :\n        \n        cl = len(self.df) \/\/ self.batch_size\n        cl += int((len(self.df) % self.batch_size) !=0)\n        return cl\n    def __getitem__(self,index):\n        \n        indices = self.indexes[index * self.batch_size :(index + 1) * self.batch_size]\n        X = self.__data_generation(indices)\n        return X\n    def __data_generation(self,indices) :\n        \n        images = np.zeros((len(indices),self.img_size,self.img_size,3),dtype = \"float32\")\n        ddf = self.df.iloc[indices]\n        for i , (j,row) in enumerate(ddf.iterrows()):\n            img = cv.imread(os.path.join(self.path,row.image))\n            #img = load_img(os.path.join(self.path,row.image),target_size = (self.img_size,self.img_size))\n            #img = img_to_array(img)\n            img = cv.resize(img,(self.img_size,self.img_size))\n            images[i,] = img\n        return images \n            \n        ","38f0699e":"WGT = \"..\/input\/effnetb0\/efficientnetb0_notop.h5\"\nmodel = EfficientNetB0(weights=WGT,input_shape=None,include_top = False,pooling=\"avg\")\n","4c80dd4f":"chunk = 1024 * 4 \ncls = len(test) \/\/ chunk \ncls += int (len(test) % chunk != 0)\nimage_embedding = []\nfor i in tqdm(range(cls)) :\n    \n    a = i * chunk \n    b = (i+1) * chunk \n    b = min(b,len(test))\n    data = DataGenerator(test.iloc[a:b])\n    emb = model.predict(data,use_multiprocessing=True,workers = 4)\n    image_embedding.append(emb)\n\ndel(model)\nimage_embedding = np.concatenate(image_embedding,axis=0)\ngc.collect()","7c3e2eac":"#from numpy.linalg.linalg import norm\n#Norm = norm(image_embedding,axis=1)","05acc3aa":"#Normed_embedding = image_embedding\/Norm.reshape(-1,1)","4d714462":"model = NearestNeighbors(n_neighbors=50,metric=\"cosine\")\nmodel.fit(image_embedding)","51a05d9a":"chunk = 4 *1024 \ncl = len(test) \/\/ chunk \ncl += int((len(test) % chunk) !=0)\npred_img = []\nfor i in tqdm(range(cl)) :\n    a = i * chunk\n    b = (i+1) * chunk\n    b = min(len(test),b)\n    distances,indices = model.kneighbors(image_embedding[a:b,])\n    for j in range(b-a):\n        distance = distances[j,:]\n        #d = distance[distance !=0]\n        #minim = float(np.min(d)) * 10\n        ind = np.where(distance < 0.2)[0]\n        IND = indices[j,ind]\n        pred_img.append(test.iloc[IND].posting_id.values)\ntest[\"pred_img\"] = pred_img","259f798a":"if submission :\n    \n    test[\"f2\"] = test.apply(getMetric(\"pred_img\"),axis=1)\n    \n    print('CV score for tf embedding image =',test.f2.mean())","b130372f":"image_phash = test.groupby(\"image_phash\").posting_id.unique()\ntest[\"pred_phash\"] = test.image_phash.map(image_phash)","f4cca372":"def combine(row):\n    x = np.concatenate([row.pred_img,row.pred_tfidf,row.pred_phash])\n   \n    return np.unique(x)\ndef combine_matches(row):\n    return \" \".join(row.pred)","7ed90f44":"if submission :\n    \n    test[\"f3\"] = test.apply(getMetric(\"pred_phash\"),axis=1)\n    \n    print('CV score for tf image phash related image =',test.f3.mean())","864a1c4e":"test[\"pred\"] = test.apply(combine,axis=1)\nif submission :\n    \n    test[\"f\"] = test.apply(getMetric(\"pred\"),axis=1)\n    \n    print('CV score for baseline =',test.f.mean())\ntest[\"matches\"] = test.apply(combine_matches,axis=1)","b5ea6750":"test[[\"posting_id\",\"matches\"]].to_csv(\"submission.csv\",index = False)\nsub = pd.read_csv('submission.csv')\nsub.head()","f75e38b5":"## 2.2 tf_idf :","5e92beeb":"## Resnet0:","73c6af88":"# <font color=redred> <b><center> Shopee - Price Match Guarantee <\/font>\n<font><center>![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/24286\/logos\/thumb76_76.png?t=2020-11-20-21-03-50)","1295bbe6":"# <font color=red> We came back soon , please upvote if you like it !","c248aa8e":"We can notice from images above :\n- Many related  items have same photos and approximately same title.\n- Many differents images of the same product , but with approximately the same title.\n- Some images are idtentiques or very similar with differents title.\n\n==> This lead us to conclude , that the photos and title should used to determine duplicated products.","9e3d38de":"# 2.Modelisation","10eef72d":"# 0. Preparation:","59cedeb9":"## image_phash related images:","62f5a38a":"# 1. EDA:"}}