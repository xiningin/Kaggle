{"cell_type":{"50dafb4a":"code","38d2cbb5":"code","942299b0":"code","b2b06ef5":"code","7a9752d1":"code","f146baad":"code","1599afd3":"code","db567411":"code","86c75272":"code","849e3dde":"code","68665d0d":"code","c3402265":"code","33917aba":"code","57290556":"code","432d0336":"code","7428c9d5":"code","e295c052":"code","439363d9":"code","990752a9":"code","e5fb3ef9":"code","030cb6ca":"code","6278dd9b":"code","dce0d9c1":"code","3c4889b0":"code","559a0485":"code","3e68037c":"code","4190341e":"code","11e4c174":"code","fd45fce4":"code","a15a2ad2":"code","0ca4322e":"code","4cc97b98":"code","cacc2ef9":"code","082489a3":"code","f101e519":"code","4e099ece":"code","d181ae16":"code","bf283d4a":"code","e46a61b0":"code","e8a3b1c1":"code","2c744198":"code","60693451":"code","e92364b1":"code","e7cdbb59":"code","b2e1d5da":"code","90738f45":"code","08feb64e":"code","e52835ea":"code","c04cb033":"code","7cfe6110":"code","5c2dbeaa":"code","6d7a243d":"code","5278bed3":"code","5bd4a074":"code","874b1bfb":"code","974652a3":"code","9ef2415c":"code","6bf9b42a":"code","dfd1a23a":"code","5ed6dc51":"code","76696acb":"code","490bf1dc":"code","31fee9a2":"code","34fd3723":"code","0c499d19":"code","8557142a":"markdown","a179a40b":"markdown","73b228cb":"markdown","3c4c03bb":"markdown","ee09d88e":"markdown","427be17d":"markdown","5efe2e08":"markdown","3eb89a8e":"markdown","87bbfcf7":"markdown","e07364a1":"markdown","c36b8586":"markdown","20ad4b37":"markdown","3ff24b87":"markdown","ccb395c3":"markdown","6ef80f14":"markdown","2ab6c16c":"markdown","9dac785e":"markdown","b9503199":"markdown","5b7ac23b":"markdown","3ecf3910":"markdown","bd60100d":"markdown","4a4a8c3b":"markdown","8136c8b1":"markdown","710bc424":"markdown","aef5308d":"markdown","fa26e42d":"markdown","c4180041":"markdown","ee00eeec":"markdown","030aee63":"markdown","d74cf55f":"markdown","755849d8":"markdown","39b6c07d":"markdown","e9460f14":"markdown","e0ee8353":"markdown","180a8d2f":"markdown","e4235b9a":"markdown","796de75c":"markdown","a91d2c45":"markdown","7171368e":"markdown","206f2474":"markdown","fb42f60c":"markdown","99f1de7c":"markdown","e3cf2b2a":"markdown","c96c7a8b":"markdown","2802b33d":"markdown","54550674":"markdown","12b44fd0":"markdown","4fc7dbd0":"markdown"},"source":{"50dafb4a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","38d2cbb5":"def warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn #remove spooky warning when working on this spooky notebook","942299b0":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nnp.set_printoptions(suppress=True)\n\nauthor_data = pd.read_csv('..\/input\/spooky-author-identification\/train.zip')\nauthor_data.head()","b2b06ef5":"print(author_data['author'].value_counts())\nauthor_data.info()","7a9752d1":"author = author_data.copy()\nauthor['text_length'] = author['text'].apply(lambda text: len(text))\nauthor.head()","f146baad":"print(author['text_length'].describe())\nax = author['text_length'].hist(bins=100);\nplt.axis([0, 1000, 0, 5000])\nplt.xlabel('text length', fontsize=14)\nplt.title('Text length distribution', fontsize=18)","1599afd3":"author.groupby('author').mean()","db567411":"X = author_data.drop(['author'], axis=1)\ny = author_data.copy()['author']","86c75272":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y ,test_size=0.2, random_state=42)","849e3dde":"len(X_train), len(X_test)","68665d0d":"import nltk\nnltk.word_tokenize('We don\\'t do that here')","c3402265":"stemmer = nltk.PorterStemmer()\n\nfor word in ('computing','computed','compulsive'):\n    print(word,'=>',stemmer.stem(word))","33917aba":"from sklearn.feature_extraction import stop_words\nstop_words.ENGLISH_STOP_WORDS","57290556":"text_list = 'i myself believe it was beautiful'.split()\nless_than_zero = list(filter(lambda x: not x in stop_words.ENGLISH_STOP_WORDS, text_list))\nprint(less_than_zero)","432d0336":"import re\nfrom collections import Counter\n\nsentence = 'I love real madrid for real but I dont like real betis'\nsentence = re.sub(r'\\W+', ' ', sentence, flags=re.M) #remove punctuation from string\n\nc = Counter(sentence.split())\nprint(c.most_common())","7428c9d5":"c['love'] += 3  \nprint(c.most_common())","e295c052":"import string\n\ndef derive_new_features(data):\n    data = data[['text']].copy()\n\n    #Unique words Count\n    data['unique_words_count'] = data.text.apply(lambda x: len(set(str(x).split())))     \n\n    #Punctuation count\n    data['punctuation_count'] = data.text.apply(lambda x: len([x for x in x.lower().split() if x in string.punctuation]))\n\n    #Upper case words count\n    data['uppercase_words_count'] = data.text.apply(lambda x: sum([x.isupper() for x in x.split()]))\n\n    #Title words count\n    data['title_words_count'] = data.text.apply(lambda x: sum([x.istitle() for x in x.split()]))\n\n    return data.drop(['text'], axis=1)","439363d9":"derive_new_features(X_train).head()","990752a9":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass TextToWordCounterTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, remove_punctuation=True, lower_case=True, stemming=True, replace_numbers=True, remove_stopwords=True):\n        self.remove_punctuation = remove_punctuation\n        self.lower_case = lower_case\n        self.stemming = stemming\n        self.replace_numbers = replace_numbers\n        self.remove_stopwords = remove_stopwords\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        word_counters = []\n        for text in X['text']:\n            if self.lower_case:\n                text = text.lower()\n            if self.replace_numbers:\n                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text) #replace any numerical character with 'NUMBER'\n            if self.remove_punctuation:\n                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n            if self.remove_stopwords:\n                words = [word for word in text.split() if not word in stop_words.ENGLISH_STOP_WORDS]\n                text = ' '.join(words)\n            word_list = nltk.word_tokenize(text)\n            word_count = Counter(word_list)\n            if self.stemming:\n                stemmed_word_count = Counter()\n                for word in word_list:\n                    stemmed_word = stemmer.stem(word)\n                    stemmed_word_count[stemmed_word] += 1\n                word_count = stemmed_word_count\n            word_counters.append(word_count)\n        return np.array(word_counters)","e5fb3ef9":"X_word_count = TextToWordCounterTransformer().fit_transform(X_train)\nX_word_count","030cb6ca":"from scipy.sparse import csr_matrix\n\nclass WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, vocabulary_size=1000):\n        self.vocabulary_size = vocabulary_size\n    def fit(self, X, y=None):\n        total_count = Counter()\n        for word_count in X:\n            for word, count in word_count.items():\n                total_count[word] += min(count, 10) \n        most_common = total_count.most_common()[:self.vocabulary_size]\n        self.most_common_ = most_common\n        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}  #spare an index for excluded words\n        return self\n    def transform(self, X, y=None):\n        data = []\n        rows = []\n        cols = []\n        for row, word_count in enumerate(X):\n            for word, count in word_count.items():\n                data.append(count)\n                rows.append(row)\n                cols.append(self.vocabulary_.get(word, 0))\n        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))","6278dd9b":"vectorizer = WordCounterToVectorTransformer(vocabulary_size=1000)\nvectorizer.fit_transform(X_word_count).toarray()","dce0d9c1":"vectorizer.most_common_[:10]","3c4889b0":"from sklearn.pipeline import Pipeline\n\npreprocess_pipeline = Pipeline([\n    ('text_to_word_count', TextToWordCounterTransformer()),\n    ('word_count_to_vector', WordCounterToVectorTransformer(vocabulary_size=14000)), \n])","559a0485":"class NewFeaturesAdderTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        return np.array(derive_new_features(X))","3e68037c":"NewFeaturesAdderTransformer().fit_transform(X_train)","4190341e":"from sklearn.compose import ColumnTransformer\n\nfull_pipeline = ColumnTransformer([\n    ('feature_adder', NewFeaturesAdderTransformer(), ['text']),\n    ('text_pipeline', preprocess_pipeline, ['text']),\n])","11e4c174":"y_train.head()","fd45fce4":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()","a15a2ad2":"X_train_transformed = full_pipeline.fit_transform(X_train)\ny_train_transformed = label_encoder.fit_transform(y_train.values)","0ca4322e":"label_encoder.classes_    #submission format: id, EAP,HPL,MWS","4cc97b98":"from sklearn.linear_model import SGDClassifier\nsgd_clf = SGDClassifier(random_state=42, loss='hinge')","cacc2ef9":"from sklearn.model_selection import cross_val_score\n\ncv_score = cross_val_score(\n        sgd_clf, \n        X_train_transformed, y_train_transformed, \n        cv=5,\n        verbose=3,\n    )\nprint('mean score :', cv_score.mean())","082489a3":"from sklearn.linear_model import LogisticRegression\n\nlog_clf = LogisticRegression(random_state=42)\ncv_score = cross_val_score(\n        log_clf, \n        X_train_transformed, y_train_transformed, \n        cv=5,\n        verbose=3,\n    )\nprint('mean score :', cv_score.mean())","f101e519":"from sklearn.naive_bayes import MultinomialNB\n\nmnb_clf = MultinomialNB()\ncv_score = cross_val_score(\n        mnb_clf, \n        X_train_transformed, y_train_transformed, \n        cv=5,\n        verbose=3,\n    )\nprint('mean score :', cv_score.mean())","4e099ece":"def multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 \/ rows * vsota","d181ae16":"from sklearn.model_selection import cross_val_predict\n\ny_scores = cross_val_predict(\n        log_clf, \n        X_train_transformed, y_train_transformed, \n        cv=5,\n        verbose=3,\n        method='predict_proba',\n    )","bf283d4a":"pd.DataFrame(columns=label_encoder.classes_, data=y_scores).head()","e46a61b0":"multiclass_logloss(y_train_transformed, y_scores)","e8a3b1c1":"y_scores = cross_val_predict(\n        mnb_clf, \n        X_train_transformed, y_train_transformed, \n        cv=5,\n        verbose=3,\n        method='predict_proba',\n    )\n\nmulticlass_logloss(y_train_transformed, y_scores)","2c744198":"import itertools\n\nstopword_params = [True, False]\nvocabulary_params = [7000, 8000, 9000, 10000]\ndata_scores = []\n\nfor stopword, vocabulary in list(itertools.product(stopword_params, vocabulary_params)):\n   \n    full_pipeline.set_params(\n            text_pipeline__text_to_word_count__remove_stopwords=stopword,\n            text_pipeline__word_count_to_vector__vocabulary_size=vocabulary,\n        )\n    X_train_processed = full_pipeline.fit_transform(X_train)\n    y_scores = cross_val_predict(\n            log_clf, \n            X_train_processed, y_train_transformed, \n            cv=3,\n            method='predict_proba',\n        )\n    data_scores += [(stopword, vocabulary, multiclass_logloss(y_train_transformed, y_scores))]","60693451":"for stopword, vocabulary, logloss in data_scores:\n    print('remove_stopwords:', stopword, ',', 'vocabulary_size:',vocabulary)\n    print('logloss: ', logloss)","e92364b1":"full_pipeline.set_params(\n            text_pipeline__text_to_word_count__remove_stopwords=False,\n            text_pipeline__word_count_to_vector__vocabulary_size=8_000,\n        )\nX_train_transformed = full_pipeline.fit_transform(X_train)","e7cdbb59":"%%time\nfrom sklearn.model_selection import GridSearchCV\n\nlog_grid_params = {\n    'penalty': ['L1', 'l2'],\n    'dual': [False],\n    'tol':[1e-4, 1e-5],\n    'class_weight': ['balanced', None],\n    'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n    'multi_class': ['ovr', 'auto', 'multinomial'],\n}\nLogisticRegression #\n\nlog_grid_search = GridSearchCV(\n    estimator=LogisticRegression(random_state=42),\n    param_grid=log_grid_params,\n    scoring='neg_log_loss',\n    cv=3,\n    verbose=2,\n)\n\nlog_grid_search.fit(X_train_transformed, y_train_transformed)","b2e1d5da":"print(log_grid_search.best_params_)","90738f45":"import joblib\n\njoblib.dump(log_grid_search, 'log_grid_best.pkl')\nlog_clf_best = joblib.load('log_grid_best.pkl').best_estimator_\nprint(log_clf_best.get_params())","08feb64e":"%%time\nfrom sklearn.model_selection import GridSearchCV\n\nsgd_grid_params = {\n    'loss': ['log'],\n    'penalty' : ['l2'],\n    'eta0':[0.1],\n    'alpha': [1e-4, 1e-5],\n    'tol': [1e-3, 1e-4],\n    'epsilon': [0.3, 0.5, 1],\n    'learning_rate': ['adaptive'],\n    'class_weight': ['balanced', None],\n    'average':[True, False],\n}\n\nsgd_grid_search = GridSearchCV(\n    estimator=SGDClassifier(random_state=42),\n    param_grid=sgd_grid_params,\n    scoring='neg_log_loss',\n    cv=3,\n    verbose=2,\n)\n\nsgd_grid_search.fit(X_train_transformed, y_train_transformed)","e52835ea":"print(sgd_grid_search.best_params_)","c04cb033":"joblib.dump(sgd_grid_search, 'sgd_grid_best.pkl')\nsgd_clf_best = joblib.load('sgd_grid_best.pkl').best_estimator_\nprint(sgd_clf_best.get_params())","7cfe6110":"%%time\nfrom sklearn.model_selection import GridSearchCV\n\nmnb_grid_params = {\n    'alpha': [0, 0.25, 0.5, 0.75, 1],\n    'fit_prior': [False, True],\n}\n\nGridSearchCV\n\nmnb_grid_search = GridSearchCV(\n    estimator=MultinomialNB(),\n    param_grid=mnb_grid_params,\n    scoring='neg_log_loss',\n    cv=3,\n    verbose=2,\n)\n\nmnb_grid_search.fit(X_train_transformed, y_train_transformed)","5c2dbeaa":"print(mnb_grid_search.best_params_)","6d7a243d":"joblib.dump(mnb_grid_search, 'mnb_grid_best.pkl')\nmnb_clf_best = joblib.load('mnb_grid_best.pkl').best_estimator_\nprint(mnb_clf_best.get_params())","5278bed3":"from sklearn.ensemble import VotingClassifier\n\nsgd_clf_best.set_params(loss='log') #need log soft voting classifier\nestimators=[('log_clf', log_clf_best), ('sgd_clf',sgd_clf_best), ('mnb_clf',mnb_clf_best)]\nvot_clf = VotingClassifier(\n    estimators=estimators,\n    voting='soft',\n)","5bd4a074":"y_scores = cross_val_predict(\n        vot_clf, \n        X_train_transformed, y_train_transformed, \n        cv=5,\n        verbose=3,\n        method='predict_proba',\n    )\n\nmulticlass_logloss(y_train_transformed, y_scores)","874b1bfb":"sgd_clf_best.set_params(loss='hinge')","974652a3":"\nfrom sklearn.ensemble import StackingClassifier\n\nestimators=[('sgd_clf',sgd_clf_best), ('log_clf', log_clf_best),  ('mnb_clf',mnb_clf_best)]\nstk_clf = StackingClassifier(\n    estimators=estimators,\n)","9ef2415c":"y_scores = cross_val_predict(\n        stk_clf, \n        X_train_transformed, y_train_transformed, \n        cv=5,\n        verbose=3,\n        method='predict_proba',\n    )\n\nmulticlass_logloss(y_train_transformed, y_scores)","6bf9b42a":"final_model = stk_clf\nfinal_model.fit(X_train_transformed, y_train_transformed)","dfd1a23a":"X_test_transformed = full_pipeline.transform(X_test)\ny_test_transformed = label_encoder.transform(y_test)\n\ny_scores = final_model.predict_proba(X_test_transformed)\ny_scores","5ed6dc51":"multiclass_logloss(y_test_transformed, y_scores)","76696acb":"X_transformed = full_pipeline.transform(X)\ny_transformed = label_encoder.transform(y)\nfinal_model.fit(X_transformed, y_transformed)","490bf1dc":"test_data = pd.read_csv('..\/input\/spooky-author-identification\/test.zip')\ntest_data.head()","31fee9a2":"test_data_prepared = full_pipeline.transform(test_data)\ntest_scores = final_model.predict_proba(test_data_prepared)\ntest_scores","34fd3723":"submission_file = pd.DataFrame({\n    'id': test_data['id'].values,\n    'EAP': test_scores[:,0],\n    'HPL': test_scores[:,1],\n    'MWS': test_scores[:,2],\n}) \nsubmission_file.head()","0c499d19":"submission_file.to_csv('submission.csv', index=False)","8557142a":"These are 10 most common words:","a179a40b":"### Create Pipelines","73b228cb":"The algorithms we used are not on their best hyperparameters. Tuning their parameters manually would waste our energy. Let's just use Scikit-learn GridSearchCV to do the job for us.","3c4c03bb":"### Grid Search","ee09d88e":"#### Word Counter","427be17d":"Hello! I'm Manfred Michael, a beginner Machine Learning enthusiast and this is my first Kaggle NLP notebook. How did I end up here? I joined a challange called #66DaysData initiated by [Ken jee](https:\/\/www.youtube.com\/channel\/UCiT9RITQ9PW6BhXK0y2jaeg), a Data Scientist Youtuber. From there, i found a minigroup where we discussed about how to tackle this notebook.","5efe2e08":"Let's change our final pipeline parameters with its best parameters","3eb89a8e":"Amazing score! It is almost less than 0.4","87bbfcf7":"Fantastic! We have never touch the test set, but we have pretty similar score from the train set score.","e07364a1":"# Testing Model","c36b8586":"### Try different preprocessing parameters","20ad4b37":"Remember that we have derive_new_features() function. In order to put it in a pipeline, we need to turn it into custom transformer.","3ff24b87":"Be aware that the submission format requires the categories to be in this order: EAP,HPL,MWS","ccb395c3":"Now, we put all of our feature extractor in pipelines. The goal is to automate our data preparation, so we can tune its parameters later. We are using custom transformers to use feature extractor modules we have imported.","6ef80f14":"This notebook will contain scikit-learn pipeline, nltk modules, and custom transformers. This is the highest score i could achieve so far.","2ab6c16c":"There are 3 authors: \n* EAP: Edgar Allan Poe\n* HPL: HP Lovecraft\n* MWS: Mary Wollstonecraft Shelley\n\nEach data has author tag and a chunk of text from on of the author's books","9dac785e":"Credit: https:\/\/github.com\/ageron\/handson-ml2\/blob\/master\/03_classification.ipynb (Aurelien Geron's Hands-On ML Chapter 3 Exercise)","b9503199":"Finally, combining the last transformer with previous pipeline gives us the full pipeline","5b7ac23b":"We used log_loss on our SGD model GridSearch. Because of that, we couldn't use several hyperparameters like loss='hinge'. I later found out that changing our SGD model 'loss' hyperparameter to 'hinge' gives us better score.","3ecf3910":"### Ensemble","bd60100d":"### Split data","4a4a8c3b":"Now that we are satisfied with our score, I will end this notebook here. I would appreciate any critics and suggestion. Thanks for your time going through this notebook!","8136c8b1":"Credit: https:\/\/www.kaggle.com\/sudalairajkumar\/simple-feature-engg-notebook-spooky-author (sudalairajkumar's spooky notebook)","710bc424":"Seems like the dataset is free of missing value. Now, let's see each text length.","aef5308d":"### Evaluate Model","fa26e42d":"And here's the plan for feature Engineering:\n* Tokenize text\n* Stem text\n* Remove stopwords\n* Use word count to vectorize text\n* Use additional features","c4180041":"#### Tokenizer","ee00eeec":"Now, WordCounterToVectorTransformer will transform the Counter objects from previous transformer into sparse metric. The words used as vector is limited too (this time only 1000 most common words).","030aee63":"### Extra Feature","d74cf55f":"# Train Model","755849d8":"#### Stopwords","39b6c07d":"Now, putting these 2 transformers in a single pipeline","e9460f14":"Now, let's train several algorithms.","e0ee8353":"This is the our best score! Now that we have got the score less than 0.4, we could test it on the test set.","180a8d2f":"For now, let's split our data. Here's our plan:\n1. We split the data to create test set\n2. Play with the train set\n3. Don't touch the test set\n4. Build our model and measure its performance with only train set\n5. Still don't touch the test set\n6. Only when we are confident with our model, we could try it with the test set\n\n\nBut why not using the test set to develop our model? Because we don't want to make a model which performs well on the test set, but we want it to perform well on any general dataset.\n\n**NOTE:** This test set is the one we created, not the test.zip","e4235b9a":"Using TextToWordCounterTransformer, we transformed the text dataframe into a Counter objects. These Counter objects count how many times each words(on a single text) occured on the text.","796de75c":"# Prepare data","a91d2c45":"This competition use logloss as scoring. I took this code from abishek's notebook.","7171368e":"# Feature Engineering","206f2474":"We have automated our data preparation. Now, let's try several parameters for our pipeline in hoping to find better score","fb42f60c":"credit: https:\/\/www.kaggle.com\/abhishek\/approaching-almost-any-nlp-problem-on-kaggle (Abhishek Thakur's spooky notebook)","99f1de7c":"Ensembling some models often gives you better score. Let's try some combination of ensemble models!","e3cf2b2a":"Kinda disappointing, there is no obvious difference between authors text length. Which probably makes sense, the provider of this dataset might have taken similar length of texts chunk from authors book. Having to much variance in text length will give us trouble.","c96c7a8b":"### Create Submission File","2802b33d":"### Importing text feature exctraction modules","54550674":"The y_train is in object type (string). We need to transform it into 3 categories. We will use Scikit-learn LabelEncoder transformer","12b44fd0":"### Introduction","4fc7dbd0":"#### Stemmer"}}