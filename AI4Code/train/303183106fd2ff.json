{"cell_type":{"b81e74c2":"code","a5a0c3b8":"code","373d4342":"code","fb81a32f":"code","316f293e":"code","0cb0479d":"code","ff7594dd":"code","4ea8eb7a":"code","31285fdf":"code","9b1df720":"code","93f7c485":"code","628a415d":"code","cbe24e54":"code","0e9c2156":"code","61c0b0e2":"code","695f83c7":"code","061d3f33":"code","91c971c0":"code","1e677e6b":"code","7e9840a4":"code","8f0bcd58":"code","7ef0cd6e":"code","ec7d1930":"code","8bd5b866":"code","46140769":"code","9f6491ad":"code","460e8454":"code","4ddb14ee":"code","1b88e2ad":"code","990843bf":"code","f5e9108d":"code","0406a5ad":"code","f19cb52b":"code","724c7cd5":"code","abd974d3":"code","c2c68e2b":"code","ffa0a366":"code","7a62f9d2":"code","c952ac98":"code","63f23c39":"code","59afbb39":"code","be70d089":"markdown","4be85ccf":"markdown","4e13194a":"markdown","7d1f7eb1":"markdown","ddd5e5b2":"markdown","4932b591":"markdown","d3127387":"markdown","5b1e9843":"markdown","2d3e13b1":"markdown","9c9d5079":"markdown","19397c7b":"markdown","346b2f1d":"markdown","638ce839":"markdown","7d67e41e":"markdown"},"source":{"b81e74c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a5a0c3b8":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB, ComplementNB, CategoricalNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nfrom sklearn.feature_extraction.text import TfidfVectorizer","373d4342":"columns_name = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\ndf = pd.read_csv(\"..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv\", encoding = 'ISO-8859-1', names = columns_name)","fb81a32f":"df.head()","316f293e":"df.info()","0cb0479d":"df['flag'].value_counts()","ff7594dd":"del df['flag']\ndel df['ids']\ndel df['date']","4ea8eb7a":"df.head()","31285fdf":"df['target'].value_counts()","9b1df720":"df['target'] = df['target'].map({0:0, 4:1})","93f7c485":"print(df[\"target\"].value_counts())\nsns.barplot(df[\"target\"].value_counts().index, df[\"target\"].value_counts().values)\nprint('0 = negative, 1 = positive')","628a415d":"emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}","cbe24e54":"import string\n# realization preprocessing\ndef preprocess(doc):\n    # lower the text\n    doc = doc.lower()\n    # remove punctuation, spaces, etc.\n    for p in string.punctuation + string.whitespace:\n        doc = doc.replace(p, ' ')\n    # remove extra spaces, merge back\n    doc = doc.strip()\n    doc = ' '.join([w for w in doc.split(' ') if w != ''])\n    for emoji in emojis.keys():\n        doc = doc.replace(emoji, \"EMOJI\" + emojis[emoji])\n    return doc","0e9c2156":"for colname in df.select_dtypes(include = np.object).columns:\n    df[colname] = df[colname].map(preprocess)\ndf.head()","61c0b0e2":"df = df.sample(frac=1).reset_index(drop=True)","695f83c7":"df.head()","061d3f33":"negative_df = df[df['target'] == 0][:150000]\nnegative_df","91c971c0":"positive_df = df[df['target'] == 1][:150000]\npositive_df","1e677e6b":"df_limited = pd.DataFrame","7e9840a4":"df_limited = negative_df.append(positive_df, ignore_index=True)\ndf_limited = df_limited.sample(frac=1).reset_index(drop=True)\ndf_limited.head()","8f0bcd58":"df_limited['target'].value_counts()","7ef0cd6e":"y = df_limited['target'].map({True: 1, False: 0}).values\ny","ec7d1930":"df_limited.drop(['target'], axis = 1, inplace=True)\ndf_limited.head()","8bd5b866":"X = df_limited","46140769":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify = y)","9f6491ad":"X_train.shape, X_test.shape","460e8454":"y_train.shape, y_test.shape","4ddb14ee":"vectorizer = TfidfVectorizer(stop_words = ENGLISH_STOP_WORDS, ngram_range=(1, 2)).fit(df['text'])\n\nX_train_vectors = vectorizer.transform(X_train['text'])\nX_test_vectors = vectorizer.transform(X_test['text'])","1b88e2ad":"X_train_vectors.shape, X_test_vectors.shape","990843bf":"num = 65\nX_train_vectors[num].data","f5e9108d":"vectorizer.inverse_transform(X_train_vectors[num])[0][np.argsort(X_train_vectors[num].data)]","0406a5ad":"knn = KNeighborsClassifier(n_neighbors = 3).fit(X_train_vectors, y_train)","f19cb52b":"predicts = knn.predict((X_test_vectors))\nprint(classification_report(y_test, predicts))","724c7cd5":"lr = LogisticRegression(penalty = 'l2', C = 2, max_iter = 1000, n_jobs=-1).fit(X_train_vectors, y_train)","abd974d3":"predicts = lr.predict((X_test_vectors))\nprint(classification_report(y_test, predicts))","c2c68e2b":"clf = MultinomialNB(alpha = 2.0689655172413794).fit(X_train_vectors, y_train)","ffa0a366":"predicts = clf.predict((X_test_vectors))\nprint(classification_report(y_test, predicts))","7a62f9d2":"clf = BernoulliNB(alpha = 2.0689655172413794).fit(X_train_vectors, y_train)","c952ac98":"predicts = clf.predict((X_test_vectors))\nprint(classification_report(y_test, predicts))","63f23c39":"clf = ComplementNB(alpha = 2.0689655172413794).fit(X_train_vectors, y_train)","59afbb39":"predicts = clf.predict((X_test_vectors))\nprint(classification_report(y_test, predicts))","be70d089":"### kNN","4be85ccf":"### Train-test split","4e13194a":"#### Take 50\/50 data for balance negative and positive","7d1f7eb1":"### Preprocessing ","ddd5e5b2":"#### ComplementNB","4932b591":"#### Multinomial NB","d3127387":"### Using TF-IDF","5b1e9843":"#### Train-test split with sklearn","2d3e13b1":"#### Bernoulli NB","9c9d5079":"#### Delete unnecessary data","19397c7b":"### Naive Bayes","346b2f1d":"#### Lower the text and replace emojis","638ce839":"### Logistic regression","7d67e41e":"### Download dataframe"}}