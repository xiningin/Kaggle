{"cell_type":{"41004397":"code","d8717645":"code","6f292001":"code","e269d2ef":"code","d8a5965d":"code","bf84f294":"code","6377a96e":"code","0c205d89":"code","2f783b77":"code","ed1d6ecf":"code","3892c7b9":"code","b10ffe5b":"code","71f73d01":"code","55da5930":"code","57f78343":"code","e654efd7":"code","6f1ced09":"code","9a800bd8":"code","c3cd0f4d":"code","e2b8fc82":"code","58b6c47d":"code","37ec3fd8":"code","a9a58353":"code","e2cb47c1":"code","b64ef4c5":"code","dc1b61e5":"code","803b54a0":"code","26bc2631":"code","b1a5b18c":"code","a02021d6":"code","00428caf":"code","77689db9":"code","983ea19e":"code","19da7f50":"code","39c503ca":"code","c1a7a3df":"code","6969c65b":"code","00b5c570":"code","32aa8d71":"code","3b787c59":"code","a915627d":"code","909c0dd1":"code","cdd45a8c":"code","d5c57298":"code","c3f4982f":"code","794ad731":"code","4d297443":"code","08c665e3":"code","3b069b93":"code","31959dba":"code","34d598bf":"code","31a6c7f4":"code","9298cdf2":"code","a869ec10":"code","67afcfd6":"code","09eb2a51":"markdown","05e3791b":"markdown","e5600868":"markdown","6f8207c1":"markdown","f0ed6299":"markdown","b3c256d6":"markdown","4cdb263f":"markdown","43d6acce":"markdown","826e833c":"markdown","fc4dfec8":"markdown","2d217256":"markdown","7381d9e4":"markdown","863a870f":"markdown","25c737e6":"markdown","59d94e0a":"markdown","1cf14bfd":"markdown","ac631c7c":"markdown","feabda6f":"markdown","0c6d1bd2":"markdown","55a6fae5":"markdown","6e0d6d39":"markdown","b4dddba2":"markdown","19fb0a64":"markdown","f85bf9b2":"markdown","cb36849d":"markdown","ece60942":"markdown","7bdf3b0f":"markdown","b8118ae6":"markdown","eb0f1b6f":"markdown","84d20874":"markdown","7126542e":"markdown","f361a508":"markdown","8d64ab2e":"markdown","f60d91c1":"markdown","7b6c8503":"markdown","4ea1c4bc":"markdown","363805d1":"markdown","e60d62ff":"markdown","8088805f":"markdown","9be8c0d0":"markdown"},"source":{"41004397":"# this may need to be installed separately with\n# !pip install category-encoders\nimport category_encoders as ce\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\n\n# python general\nimport pandas as pd\nimport numpy as np\nfrom collections import OrderedDict\n\n#scikit learn\n\nimport sklearn\nfrom sklearn.base import clone\n\n# model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import RFE\n\n# ML models\nfrom sklearn import tree\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\n\n# error metrics\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, median_absolute_error\nfrom sklearn.model_selection import cross_val_score\n\n# plotting and display\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib as mpl\n\nfrom IPython.display import display\npd.options.display.max_columns = None\n\n# widgets and widgets based libraries\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interactive","d8717645":"def rmse(y_true, y_pred):\n    res = np.sqrt(((y_true - y_pred) ** 2).mean())\n    return res\n\ndef mape(y_true, y_pred):\n    y_val = np.maximum(np.array(y_true), 1e-8)\n    return (np.abs(y_true -y_pred)\/y_val).mean()","6f292001":"metrics_dict_res = OrderedDict([\n            ('mean_absolute_error', mean_absolute_error),\n            ('median_absolute_error', median_absolute_error),\n            ('root_mean_squared_error', rmse),\n            ('mean abs perc error', mape)\n            ])","e269d2ef":"def regression_metrics_yin(y_train, y_train_pred, y_test, y_test_pred,\n                           metrics_dict, format_digits=None):\n    df_results = pd.DataFrame()\n    for metric, v in metrics_dict.items():\n        df_results.at[metric, 'train'] = v(y_train, y_train_pred)\n        df_results.at[metric, 'test'] = v(y_test, y_test_pred)\n\n    if format_digits is not None:\n        df_results = df_results.applymap(('{:,.%df}' % format_digits).format)\n\n    return df_results","d8a5965d":"def describe_col(df, col):\n    display(df[col].describe())\n\ndef val_count(df, col):\n    display(df[col].value_counts())\n\ndef show_values(df, col):\n    print(\"Number of unique values:\", len(df[col].unique()))\n    return display(df[col].value_counts(dropna=False))","bf84f294":"def plot_distribution(df, col, bins=100, figsize=None, xlim=None, font=None, histtype='step'):\n    if font is not None:\n        mpl.rc('font', **font)\n\n    if figsize is not None:\n        plt.figure(figsize=figsize)\n    else:\n        plt.figure(figsize=(10, 6))\n    dev = df[col]    \n    dev.plot(kind='hist', bins=bins, density=True, histtype=histtype, color='b', lw=2,alpha=0.99)\n    print('mean:', dev.mean())\n    print('median:', dev.median())\n    if xlim is not None:\n        plt.xlim(xlim)\n    return plt.gca()","6377a96e":"def plot_feature_importances(model, feature_names=None, n_features=20):\n    if feature_names is None:\n        feature_names = range(n_features)\n    \n    importances = model.feature_importances_\n    importances_rescaled = 100 * (importances \/ importances.max())\n    xlabel = \"Relative importance\"\n\n    sorted_idx = np.argsort(-importances_rescaled)\n\n    names_sorted = [feature_names[k] for k in sorted_idx]\n    importances_sorted = [importances_rescaled[k] for k in sorted_idx]\n\n    pos = np.arange(n_features) + 0.5\n    plt.barh(pos, importances_sorted[:n_features], align='center')\n\n    plt.yticks(pos, names_sorted[:n_features])\n    plt.xlabel(xlabel)\n\n    plt.title(\"Feature importances\")\n\n    return plt.gca()","0c205d89":"def plot_act_vs_pred(y_act, y_pred, scale=1, act_label='actual', pred_label='predicted', figsize=None, xlim=None,\n                     ylim=None, font=None):\n    \n    if font is not None:\n        mpl.rc('font', **font)\n\n    if figsize is not None:\n        plt.figure(figsize=figsize)\n    else:\n        plt.figure(figsize=(10, 6))\n    plt.scatter(y_act\/scale, y_pred\/scale)\n    x = np.linspace(0, y_act.max()\/scale, 10)\n    plt.plot(x, x)\n    plt.xlabel(act_label)\n    plt.ylabel(pred_label)\n    if xlim is not None:\n        plt.xlim(xlim)\n    else:\n        plt.xlim([0, 1e2])\n    if ylim is not None:\n        plt.ylim(ylim)\n    else:\n        plt.ylim([0, 1e2])\n    return plt.gca()","2f783b77":"def compute_perc_deviation(y_act, y_pred, absolute=False):\n    dev = (y_pred - y_act)\/y_act * 100\n    if absolute:\n        dev = np.abs(dev)\n        dev.name = 'abs % error'\n    else:\n        dev.name = '% error'\n    return dev\n\ndef plot_dev_distribution(y_act, y_pred, absolute=False, bins=100, figsize=None, xlim=None, font=None):\n    if font is not None:\n        mpl.rc('font', **font)\n\n    if figsize is not None:\n        plt.figure(figsize=figsize)\n    else:\n        plt.figure(figsize=(10, 6))\n    dev = compute_perc_deviation(y_act, y_pred, absolute=absolute)\n    dev.plot(kind='hist', bins=bins, density=True)\n    print('mean % dev:', dev.mean())\n    print('median % dev:', dev.median())\n    # plt.vlines(dev.mean(), 0, 0.05)\n    plt.title('Distribution of errors')\n    plt.xlabel('% deviation')\n    if xlim is not None:\n        plt.xlim(xlim)\n    else:\n        plt.xlim([-1e2, 1e2])\n    return plt.gca()","ed1d6ecf":"categorical_features = [\n    'Body_Type',\n    'Driven_Wheels',\n    'Global_Sales_Sub-Segment',\n    'Brand',\n    'Nameplate',\n    'Transmission',\n    'Turbo',\n    'Fuel_Type',\n    'PropSysDesign',\n    'Plugin',\n    'Registration_Type',\n    'country_name'\n]\n\nnumeric_features = [\n    'Generation_Year',\n    'Length',\n    'Height',\n    'Width',\n    'Engine_KW',\n    'No_of_Gears',\n    'Curb_Weight',\n    'CO2',\n    'Fuel_cons_combined',\n    'year'\n]\n\nall_numeric_features = list(numeric_features)\nall_categorical_features = list(categorical_features)\n\ntarget = [\n    'Price_USD'\n]\n\ntarget_name = 'Price_USD'","3892c7b9":"#ml_model_type = 'Linear Regression'\n#ml_model_type = 'Decision Tree'\n#ml_model_type = 'Random Forest'\n#ml_model_type = 'Gradient Boosting Regressor'\n#ml_model_type = 'AdaBoost'\n#ml_model_type = 'XGBoost'\nml_model_type = 'CatBoost'\n\nregression_metric = 'mean abs perc error'\n\ndo_grid_search_cv = False\nscoring_greater_is_better = False  # THIS NEEDS TO BE SET CORRECTLY FOR CV GRID SEARCH\n\ndo_retrain_total = True\nwrite_predictions_file = True\n\n# relative size of test set\ntest_size = 0.2\nrandom_state = 33","b10ffe5b":"df = pd.read_csv('\/kaggle\/input\/ihsmarkit-hackathon-june2020\/train_data.csv',index_col='vehicle_id')\ndf['date'] = pd.to_datetime(df['date'])\n#g = df['Brand'].value_counts()\n#df['Brand'] = np.where(df['Brand'].isin(g.index[g >= 200]), df['Brand'], 'Other')","71f73d01":"# basic commands on a dataframe\n# df.info()\ndf.head(5)\n# df.shape\n# df.head()\n# df.tail()","55da5930":"df['country_name'].value_counts()","57f78343":"df.groupby(['year', 'country_name'])['date'].count()","e654efd7":"df_oos = pd.read_csv('\/kaggle\/input\/ihsmarkit-hackathon-june2020\/oos_data.csv', index_col='vehicle_id')\ndf_oos['date'] = pd.to_datetime(df_oos['date'])\ndf_oos['year'] = df_oos['date'].map(lambda d: d.year)\n#g_oos = df_oos['Brand'].value_counts()\n#df_oos['Brand'] = np.where(df_oos['Brand'].isin(g.index[g >= 200]), df_oos['Brand'], 'Other')","6f1ced09":"# df_oos.shape\ndf_oos.head()","9a800bd8":"df_oos.groupby(['year', 'country_name'])['date'].count()","c3cd0f4d":"# unique values, categorical variables\nfor col in all_categorical_features:\n    print(col, len(df[col].unique()))","e2b8fc82":"interactive(lambda col: show_values(df, col), col=all_categorical_features)","58b6c47d":"# summary statistics\ndf[numeric_features + target].describe()","37ec3fd8":"figsize = (16,12)\nsns.set(style='whitegrid', font_scale=2)\n\nbins = 1000\nbins = 40\n#xlim = [0,100000]\nxlim = None\nprice_mask = df['Price_USD'] < 100000\ninteractive(lambda col: plot_distribution(df[price_mask], col, bins=bins, xlim=xlim), col=sorted(all_numeric_features + target))\n#interactive(lambda col: plot_distribution(df, col, bins=bins, xlim=xlim), col=sorted(all_numeric_features + target))","a9a58353":"# this is quite slow\nsns.set(style='whitegrid', font_scale=1)\n# sns.pairplot(df[numeric_features[:6] + target].iloc[:10000])\n#sns.pairplot(df[['Engine_KW'] + target].iloc[:10000])\nprice_mask = df['Price_USD'] < 100000\ndf_temp = df[price_mask].copy()\nsns.pairplot(df_temp[['Engine_KW'] + target])","e2cb47c1":"additional_numeric_features = []","b64ef4c5":"features_drop = []\n\nif ml_model_type == 'Linear Regression':\n    features_drop = categorical_features + numeric_features\n    features_to_use = ['Engine_KW']\n    # features_to_use = ['country_name', 'Engine_KW']\n    for feature in features_to_use:\n        features_drop.remove(feature)\n#else:\n    #features_drop = categorical_features + numeric_features\n    #features_to_use = ['Brand', 'country_name', 'Engine_KW']\n    #features_to_use = ['country_name', 'Engine_KW']\n    #for feature in features_to_use:\n        #features_drop.remove(feature)\n    #features_drop = ['Nameplate']\n    \n\ncategorical_features = list(filter(lambda f: f not in features_drop, categorical_features))\nnumeric_features = list(filter(lambda f: f not in features_drop, numeric_features))","dc1b61e5":"features = categorical_features + numeric_features + additional_numeric_features\nmodel_columns = features + [target_name]\nlen(model_columns)","803b54a0":"#dataframe for further processing\ndf_proc = df[model_columns].copy()\ndf_proc.shape","26bc2631":"# One-hot encoding\n#encoder = ce.OneHotEncoder(cols=categorical_features, handle_unknown='value', \n                           #use_cat_names=True)\n#encoder.fit(df_proc)\n#df_comb_ext = encoder.transform(df_proc)\n#features_ext = list(df_comb_ext.columns)\n#features_ext.remove(target_name)","b1a5b18c":"#del df_proc\n#df_comb_ext.head()","a02021d6":"# df_comb_ext.memory_usage(deep=True).sum()\/1e9\n#features_model\n#df_comb_ext.shape","00428caf":"#X_train, X_test, y_train, y_test = train_test_split(df_comb_ext[features_ext], df_comb_ext[target_name], \n                                                    #test_size=test_size, random_state=random_state)\nX_train, X_test, y_train, y_test = train_test_split(df_proc[features], df_proc[target_name], \n                                                    test_size=test_size, random_state=random_state)\nprint(X_train.shape)\nprint(X_test.shape)","77689db9":"#scaler = MinMaxScaler()\n#X_train_scaled = scaler.fit_transform(X_train)\n#X_test_scaled = scaler.fit_transform(X_test)","983ea19e":"#pipelines = []\n#pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',LinearRegression())])))\n#pipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\n#pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\n#pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\n#pipelines.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\n\n#results = []\n#names = []\n#for name, model in pipelines:\n    #cv_results = cross_val_score(model, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n    #results.append(cv_results)\n    #names.append(name)\n    #msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    #print(msg)","19da7f50":"if ml_model_type == 'Linear Regression':\n    model_hyper_parameters_dict = OrderedDict(fit_intercept=True, normalize=False)\n    regressor =  LinearRegression(**model_hyper_parameters_dict)\n\nif ml_model_type == 'Decision Tree':\n    model_hyper_parameters_dict = OrderedDict(max_depth=3, random_state=random_state)\n    regressor =  DecisionTreeRegressor(**model_hyper_parameters_dict)\n      \nif ml_model_type == 'Random Forest':\n\n    model_hyper_parameters_dict = OrderedDict(n_estimators=10, \n                                              max_depth=4, \n                                              min_samples_split=2, \n                                              max_features='sqrt',\n                                              min_samples_leaf=1, \n                                              random_state=random_state, \n                                              n_jobs=4)\n   \n    regressor = RandomForestRegressor(**model_hyper_parameters_dict)\n\nif ml_model_type == 'Gradient Boosting Regressor':\n    model_hyper_parameters_dict = OrderedDict(learning_rate=0.1,\n                                              max_depth=6,\n                                              subsample=0.8,\n                                              max_features=0.2,\n                                             n_estimators=200,\n                                             random_state=random_state)\n    regressor = GradientBoostingRegressor(**model_hyper_parameters_dict)\n    \n\nif ml_model_type == 'AdaBoost':\n    model_hyper_parameters_dict = OrderedDict(n_estimators=180,\n                                             random_state=random_state)\n    regressor = AdaBoostRegressor(**model_hyper_parameters_dict)\n    \n    \n    \nif ml_model_type == 'XGBoost':\n    model_hyper_parameters_dict = OrderedDict(learning_rate=0.01,\n                                              colsample_bytree=0.3,\n                                              max_depth=3,\n                                              subsample=0.8,\n                                              n_estimators=1000,\n                                              seed=random_state)\n    \n    regressor = xgb.XGBRegressor(**model_hyper_parameters_dict)\n    \n\nif ml_model_type == 'CatBoost':\n    model_hyper_parameters_dict = OrderedDict(iterations=4000,\n                                              early_stopping_rounds=50,\n                                              learning_rate=0.05,\n                                              depth=12,\n                                              one_hot_max_size=40,\n                                              colsample_bylevel=0.5,\n                                              bagging_temperature=12,\n                                              random_strength=0.7,\n                                              reg_lambda=1.0,\n                                              eval_metric='RMSE',\n                                              logging_level='Silent',\n                                              random_seed = random_state)\n    \n    regressor = CatBoostRegressor(**model_hyper_parameters_dict)    \n    \nbase_regressor = clone(regressor)\n    \nif do_grid_search_cv:\n    \n    scoring = make_scorer(metrics_dict_res[regression_metric], greater_is_better=scoring_greater_is_better)\n    \n    if ml_model_type == 'Random Forest':\n        \n\n        grid_parameters = [{'n_estimators': [10], 'max_depth': [3, 5, 10], \n                             'min_samples_split': [2,4], 'min_samples_leaf': [1]} ]\n        \n    if ml_model_type == 'XGBoost':\n        \n        grid_parameters = [{'colsample_bytree':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]}]\n        \n    if ml_model_type == 'CatBoost':\n        \n        grid_parameters = [{'learning_rate': [0.1, 0.3, 0.5, 0.8]}]\n        \n    n_splits = 10\n    n_jobs = 4\n    cv_regressor = GridSearchCV(regressor, grid_parameters, cv=n_splits, scoring=scoring, return_train_score=True,\n                                refit=True, n_jobs=n_jobs)    ","39c503ca":"# Use XGBoost API Learner. Comment the whole cell out if you want to use other models\n#DM_train = xgb.DMatrix(data=X_train,label=y_train)\n#DM_test =  xgb.DMatrix(data=X_test,label=y_test)\n#params = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"}\n#xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5)","c1a7a3df":"if do_grid_search_cv:\n    cv_regressor.fit(X_train, y_train, cat_features=categorical_features)\n    regressor_best = cv_regressor.best_estimator_\n    model_hyper_parameters_dict = cv_regressor.best_params_\n    train_scores = cv_regressor.cv_results_['mean_train_score']\n    test_scores = cv_regressor.cv_results_['mean_test_score']\n    test_scores_std = cv_regressor.cv_results_['std_test_score']\n    cv_results = cv_regressor.cv_results_\n\nelif ml_model_type == 'CatBoost':\n    regressor.fit(X_train, y_train, cat_features=categorical_features)\n    \nelse:\n    regressor.fit(X_train, y_train)","6969c65b":"if do_grid_search_cv:\n    # print(cv_results)\n    print(model_hyper_parameters_dict)\n    plt.plot(-train_scores, label='train')\n    plt.plot(-test_scores, label='test')\n    plt.xlabel('Parameter set #')\n    plt.legend()\n    regressor = regressor_best","00b5c570":"y_train_pred = regressor.predict(X_train)\ny_test_pred = regressor.predict(X_test)\n#y_train_pred = xg_reg.predict(DM_train)\n#y_test_pred = xg_reg.predict(DM_test)","32aa8d71":"if ml_model_type == 'Linear Regression':\n    df_reg_coef = (pd.DataFrame(zip(['intercept'] + list(X_train.columns), \n                               [regressor.intercept_] + list(regressor.coef_)))\n                 .rename({0: 'feature', 1: 'coefficient value'}, axis=1))\n    display(df_reg_coef)","3b787c59":"if hasattr(regressor, 'feature_importances_'):\n    sns.set(style='whitegrid', font_scale=1.5)\n    plt.figure(figsize=(12,10))\n    plot_feature_importances(regressor, features_ext, n_features=np.minimum(20, X_train.shape[1]))","a915627d":"df_regression_metrics = regression_metrics_yin(y_train, y_train_pred, y_test, y_test_pred,\n                                               metrics_dict_res, format_digits=3)\n\ndf_output = df_regression_metrics.copy()\ndf_output.loc['Counts','train'] = len(y_train)\ndf_output.loc['Counts','test'] = len(y_test)\ndf_output","909c0dd1":"figsize = (16,10)\nxlim = [0, 250]\nfont={'size': 20}\nsns.set(style='whitegrid', font_scale=2.5)\nact_label = 'actual price [k$]'\npred_label='predicted price [k$]'\nplot_act_vs_pred(y_test, y_test_pred, scale=1000, act_label=act_label, pred_label=pred_label, \n                 figsize=figsize, xlim=xlim, ylim=xlim, font=font)\nprint()","cdd45a8c":"figsize = (14,8)\nxlim = [0, 100]\n#xlim = [-100, 100]\n# xlim = [-50, 50]\n#xlim = [-20, 20]\n\nfont={'size': 20}\nsns.set(style='whitegrid', font_scale=1.5)\n\np_error = (y_test_pred - y_test)\/y_test *100\ndf_p_error = pd.DataFrame(p_error.values, columns=['percent_error'])\n#display(df_p_error['percent_error'].describe().to_frame())\n\nbins=1000\nbins=500\n#bins=100\nabsolute = True\n#absolute = False\nplot_dev_distribution(y_test, y_test_pred, absolute=absolute, figsize=figsize, \n                      xlim=xlim, bins=bins, font=font)\nprint()","d5c57298":"if do_retrain_total:\n    cv_opt_model = clone(base_regressor.set_params(**model_hyper_parameters_dict))\n    # train on complete data set\n    #X_train_full = df_comb_ext[features_ext].copy()\n    #y_train_full = df_comb_ext[target_name].values\n    X_train_full = df_proc[features].copy()\n    y_train_full = df_proc[target_name].values\n    #cv_opt_model.fit(X_train_full, y_train_full) \n    cv_opt_model.fit(X_train_full, y_train_full, cat_features=categorical_features) \n    regressor = cv_opt_model","c3f4982f":"# df_oos.head()","794ad731":"df_proc_oos = df_oos[model_columns[:-1]].copy()\ndf_proc_oos[target_name] = 1","4d297443":"df_proc_oos","08c665e3":"df_proc_oos.drop(target_name, axis=1, inplace=True)","3b069b93":"#df_comb_ext_oos = encoder.transform(df_proc_oos)","31959dba":"#df_comb_ext_oos.drop(target_name, axis=1, inplace=True)\n#df_comb_ext_oos = scaler.fit_transform(df_comb_ext_oos)","34d598bf":"#y_oos_pred = regressor.predict(df_comb_ext_oos)\ny_oos_pred = regressor.predict(df_proc_oos)","31a6c7f4":"id_col = 'vehicle_id'\ndf_out = (pd.DataFrame(y_oos_pred, columns=[target_name], index=df_proc_oos.index)\n            .reset_index()\n            .rename({'index': id_col}, axis=1))","9298cdf2":"df_out.head()","a869ec10":"df_out.shape","67afcfd6":"if write_predictions_file:\n    df_out.to_csv('submission.csv', index=False)","09eb2a51":"## Categorical feature encoding","05e3791b":"Again, not needed as I eventually go with Catboost.","e5600868":"## Regression coefficients\/Feature importance","6f8207c1":"## Metrics","f0ed6299":"#  ML data preparation","b3c256d6":"## Categorical features","4cdb263f":"## Apply categorical encoding","43d6acce":"## Drop features (optional)","826e833c":"## Metrics","fc4dfec8":"## Define features","2d217256":"## Train test split","7381d9e4":"## Conclusion","863a870f":"# Load  data\n","25c737e6":"## ML model training","59d94e0a":"## Training data","1cf14bfd":"# Machine learning model\n\nSupervised learning\n\nhttps:\/\/scikit-learn.org\/stable\/supervised_learning.html\n\nEnsemble methods in scikit learn\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\n\n\nDecision trees\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/tree.html\n","ac631c7c":"##  Model Performance plots","feabda6f":"# Import libraries","0c6d1bd2":"## Subset to relevant columns","55a6fae5":"I did not use one hot encoding, and instead let Catboost take care of the categorical variables.","6e0d6d39":"All in all, I was very impressed with CatBoost's performance. I did not have a chance to try out LightGBM, but comparing to XGBoost and Stochastic Gradient Boosting, CatBoost gave superior results with pretty consistent results between training and testing data, and performance is comparable (albeit I did not track closely). \n\nNext step would be to play around with the hyperparameters tuning -- With the time constraints, I did not have time to try to tinker all the parameters. My three cents:\n- CatBoost deals with categorical features pretty well and I encourage you to make use of that. The one_hot_max_size parameter also allows you to define what is the maximum number of categories. Too many categories will decrease performance of the algorithm.\n- Strike a balance between iterations and learning rate. My suggestion would be to start with a high number of iterations (so you could make use of early stopping) and small learning rate, and then slowly increase learning rate while reducing iterations.","b4dddba2":"##  Model definition","19fb0a64":"## Display functions","f85bf9b2":"# Feature generation","cb36849d":"# Global options","ece60942":"The dataset is fairly clean with a moderate number of features available. For a start, I did not have to worry too much about missing values in the dataset. A few things I tried out:\n- I tried dropping Nameplate because it has too many categories and when combined with one hot encoding, could make your dataset really sparse. Dropping Nameplate certainly results in superior results when paired with Gradient Boosting algorithms, though in the end I add the feature back in since I decided to use Catboost, which takes care of categorical features really well.\n- I tried setting year and generation year to be categorical instead of numerical, as they should be. However, I did not notice any improvement. \n- There are many highly correlated features which should\/could be dropped to reduce the number of dimensions. I have not tried going down the PCA path but would expect an improvement at least for Linear Regression.","7bdf3b0f":"## Apply model and produce output","b8118ae6":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-libraries\" data-toc-modified-id=\"Import-libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Import libraries<\/a><\/span><\/li><li><span><a href=\"#Locally-defined-functions\" data-toc-modified-id=\"Locally-defined-functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Locally defined functions<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Metrics\" data-toc-modified-id=\"Metrics-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Metrics<\/a><\/span><\/li><li><span><a href=\"#Display-functions\" data-toc-modified-id=\"Display-functions-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>Display functions<\/a><\/span><\/li><li><span><a href=\"#Define-features\" data-toc-modified-id=\"Define-features-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;<\/span>Define features<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Global-options\" data-toc-modified-id=\"Global-options-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Global options<\/a><\/span><\/li><li><span><a href=\"#Load--data\" data-toc-modified-id=\"Load--data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Load  data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Training-data\" data-toc-modified-id=\"Training-data-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Training data<\/a><\/span><\/li><li><span><a href=\"#Out-of-sample-data-(to-predict)\" data-toc-modified-id=\"Out-of-sample-data-(to-predict)-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Out of sample data (to predict)<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Feature-exploration\" data-toc-modified-id=\"Feature-exploration-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Feature exploration<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Categorical-features\" data-toc-modified-id=\"Categorical-features-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Categorical features<\/a><\/span><\/li><li><span><a href=\"#Numerical-features\" data-toc-modified-id=\"Numerical-features-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Numerical features<\/a><\/span><\/li><li><span><a href=\"#Pair-plot\" data-toc-modified-id=\"Pair-plot-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Pair plot<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Feature-generation\" data-toc-modified-id=\"Feature-generation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Feature generation<\/a><\/span><\/li><li><span><a href=\"#Feature-selection\" data-toc-modified-id=\"Feature-selection-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Feature selection<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Drop-features-(optional)\" data-toc-modified-id=\"Drop-features-(optional)-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;<\/span>Drop features (optional)<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#ML-data-preparation\" data-toc-modified-id=\"ML-data-preparation-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>ML data preparation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Categorical-feature-encoding\" data-toc-modified-id=\"Categorical-feature-encoding-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;<\/span>Categorical feature encoding<\/a><\/span><\/li><li><span><a href=\"#Train-test-split\" data-toc-modified-id=\"Train-test-split-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;<\/span>Train test split<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Machine-learning-model\" data-toc-modified-id=\"Machine-learning-model-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;<\/span>Machine learning model<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-definition\" data-toc-modified-id=\"Model-definition-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;<\/span>Model definition<\/a><\/span><\/li><li><span><a href=\"#ML-model-training\" data-toc-modified-id=\"ML-model-training-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;<\/span>ML model training<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-evaluation\" data-toc-modified-id=\"Model-evaluation-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;<\/span>Model evaluation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Train,-test-predictions\" data-toc-modified-id=\"Train,-test-predictions-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;<\/span>Train, test predictions<\/a><\/span><\/li><li><span><a href=\"#Regression-coefficients\/Feature-importance\" data-toc-modified-id=\"Regression-coefficients\/Feature-importance-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;<\/span>Regression coefficients\/Feature importance<\/a><\/span><\/li><li><span><a href=\"#Metrics\" data-toc-modified-id=\"Metrics-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;<\/span>Metrics<\/a><\/span><\/li><li><span><a href=\"#Model-Performance-plots\" data-toc-modified-id=\"Model-Performance-plots-10.4\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;<\/span>Model Performance plots<\/a><\/span><\/li><li><span><a href=\"#Optionally-retrain-on-the-whole-data-set\" data-toc-modified-id=\"Optionally-retrain-on-the-whole-data-set-10.5\"><span class=\"toc-item-num\">10.5&nbsp;&nbsp;<\/span>Optionally retrain on the whole data set<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Apply-model-to-OOS-data\" data-toc-modified-id=\"Apply-model-to-OOS-data-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;<\/span>Apply model to OOS data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Subset-to-relevant-columns\" data-toc-modified-id=\"Subset-to-relevant-columns-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;<\/span>Subset to relevant columns<\/a><\/span><\/li><li><span><a href=\"#Apply-categorical-encoding\" data-toc-modified-id=\"Apply-categorical-encoding-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;<\/span>Apply categorical encoding<\/a><\/span><\/li><li><span><a href=\"#Apply-model-and-produce-output\" data-toc-modified-id=\"Apply-model-and-produce-output-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;<\/span>Apply model and produce output<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","eb0f1b6f":"# Apply model to OOS data","84d20874":"# Feature selection\n\nYou can read about feature selection here\nhttps:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html#","7126542e":"Scaling did not seem to improve any performance, which I think could be because I was using a tree-based model, where scaling isn't exactly a deal breaker.","f361a508":"# Locally defined functions","8d64ab2e":"Before deciding on a model to use, I try running the dataset through a handful of out of the box sklearn algorithms. Uncomment this cell if you want to check out the results. Gradient Boosting gave me the best result i.e. lowest MSE.","f60d91c1":"# Model evaluation","7b6c8503":"## Out of sample data (to predict)","4ea1c4bc":"# Feature exploration","363805d1":"## Pair plot","e60d62ff":"## Train, test predictions","8088805f":"## Numerical features","9be8c0d0":"## Optionally retrain on the whole data set"}}