{"cell_type":{"d5249677":"code","9e091efd":"code","1e4877a4":"code","1240cc5c":"code","38ec9af5":"code","aa057e47":"code","e3aef9ae":"code","e8f55bae":"code","01c7e393":"code","a50080f1":"code","37c8b502":"code","69da34e1":"code","77144b6b":"code","d0128ad7":"code","12608a32":"code","366bcf7e":"code","ccecab07":"markdown","d2305d12":"markdown","946927c6":"markdown","999b5f2d":"markdown","147a2cb2":"markdown","55e1b5d5":"markdown","6b0e92e8":"markdown","f713f125":"markdown","e25de3a1":"markdown","a313053c":"markdown","cb13f58a":"markdown","e0b31d4d":"markdown"},"source":{"d5249677":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","9e091efd":"!ls ..\/input\/flowers-recognition\/flowers\/flowers","1e4877a4":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ',tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n    \nprint('Replicas: ',strategy.num_replicas_in_sync)","1240cc5c":"dir_ = '..\/input\/flowers-recognition\/flowers\/flowers'\nimage_size = (180,180)\nbatch_size = 32\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    dir_,\n    validation_split = 0.2,\n    subset = 'training',\n    seed = 1337,\n    image_size = image_size,\n    batch_size = batch_size\n)\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    dir_,\n    validation_split = 0.2,\n    subset = 'validation',\n    seed = 1337,\n    image_size = image_size,\n    batch_size = batch_size\n)","38ec9af5":"labels = train_ds.class_names\nprint(labels)","aa057e47":"plt.figure(figsize = (10,15))\nfor images,label in train_ds.take(1):\n    for i in range(9):\n        ax = plt.subplot(3,3,i+1)\n        plt.imshow(images[i].numpy().astype('uint8'))\n        plt.title(int(label[i]))\n        plt.axis('off')","e3aef9ae":"data_augmentation = keras.Sequential([\n    layers.experimental.preprocessing.RandomFlip('horizontal'),\n    layers.experimental.preprocessing.RandomRotation(0.1),\n])","e8f55bae":"plt.figure(figsize=(10,10))\nfor images, _ in train_ds.take(1):\n    for i in range(9):\n        aug_images = data_augmentation(images)\n        ax = plt.subplot(3,3,i+1)\n        plt.imshow(aug_images[0].numpy().astype('uint8'))\n        plt.axis('off')","01c7e393":"epochs = 35\n\ndef learning_rate(epoch):\n    lr_start = 0.00001\n    lr_max = 0.0004\n    lr_min = 0.00001\n    lr_rampup_epochs = 8\n    lr_sustain_epochs = 0\n    lr_exp_decay = 0.8\n    \n    if epoch < lr_rampup_epochs:\n        lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n    elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n        lr = lr_max\n    else:\n        lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n\n    return lr\n\nrng = [i for i in range(epochs)]\ny = [learning_rate(x) for x in rng]\nplt.plot(rng,y)\nprint(f'Learning Rate : {y[0]} to {max(y)} to {y[-1]}')","a50080f1":"train_ds = train_ds.prefetch(32)\nval_ds = val_ds.prefetch(32)","37c8b502":"lr_schedule = tf.keras.callbacks.LearningRateScheduler(learning_rate,verbose=1)\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor= 'val_loss',patience=0, verbose=1, restore_best_weights=True)\n\nwith strategy.scope():\n    conv_base = tf.keras.applications.InceptionResNetV2(weights='imagenet',include_top=False,input_shape= image_size+(3,))\n    conv_base.trainable = True\n    \n    model = tf.keras.Sequential([\n        tf.keras.layers.Input(shape= image_size + (3,)),\n        tf.keras.Sequential([\n            tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n            tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n            tf.keras.layers.experimental.preprocessing.Rescaling(1.0\/255)\n        ]),\n        conv_base,\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(5,activation='softmax')\n    ])\n    \n    model.summary()\n    model.compile(optimizer='adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])","69da34e1":"history = model.fit(train_ds,epochs = epochs,validation_data= val_ds,callbacks= [lr_schedule, early_stopping])","77144b6b":"# epochs = 50\n\nhis = pd.DataFrame(history.history)\ntrain_loss = his[\"loss\"]\ntrain_accuracy = his[\"accuracy\"]\nval_loss = his[\"val_loss\"]\nval_accuracy = his[\"val_accuracy\"]\n\n# figure 1\nplt.figure()\nplt.plot(range(epochs), train_loss, label='train_loss')\nplt.plot(range(epochs), val_loss, label='val_loss')\nplt.legend()\nplt.xlabel('epochs')\nplt.ylabel('loss')\n\n# figure 2\nplt.figure()\nplt.plot(range(epochs), train_accuracy, label='train_accuracy')\nplt.plot(range(epochs), val_accuracy, label='val_accuracy')\nplt.legend()\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.show()","d0128ad7":"accuracy = model.evaluate(val_ds,verbose=1)\nprint(f'Loss: {accuracy[0]}')\nprint(f'Accuracy: {(accuracy[1] * 100):.2f} %')","12608a32":"def predict(loc,index):\n    img = keras.preprocessing.image.load_img(loc, target_size=image_size)\n    img_array = keras.preprocessing.image.img_to_array(img)\n    img_array = tf.expand_dims(img_array,0)\n\n    pred = model.predict(img_array)\n    prediction = labels[np.argmax(pred[0])]\n    \n    print(f'The given image is a {prediction}')\n    plt.imshow(plt.imread(val_ds.file_paths[index]))\n    plt.axis('off')\n    plt.show()","366bcf7e":"for i in range(3):\n    index = np.random.randint(1,863)\n    predict(val_ds.file_paths[index],index)","ccecab07":"## Importing the required Packages","d2305d12":"#### Visualizing the augmented images","946927c6":"## Prediction on the Model","999b5f2d":"The pictures in the dataset are divided into five classes: *daisy, dandelion, rose, sunflower, tulip*.\nFor each class there are about 800 photos.I have tried diffrent data augmentation techniques and other techniques to overcome overfitting.\n\n* Pre-trained models tried:\n`Xception Network`\n`InceptionResNetV2`\n\n\n* The model has been trained with a learning rate schedule and earlystopping.\n`Learning Rate : 1e-05 to 0.0004 to 1.1178702674124267e-05 \nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor= 'val_loss',patience=10, verbose=1, restore_best_weights=True)`\n\n\n* Data Augmentation techniques:\n`RandomFlip`\n`RandomRotation`\n\n\n* I have tried diffrent epochs:\n`100`\n`50`\n`30`\n`35`\n\nHence, through these methods, I was able to achieve **93% accuracy**","147a2cb2":"## Training the model","55e1b5d5":"## Flowers Dataset Task- Accuracy 93%\n\n<img src=\"https:\/\/hgtvhome.sndimg.com\/content\/dam\/images\/hgtv\/fullset\/2015\/11\/10\/0\/CI_Costa-Farms-Ballad-aster.jpg.rend.hgtvcom.966.644.suffix\/1447169929799.jpeg\" alt=\"Drawing\" align=\"center\" style=\"width: 400px;\" \/> ","6b0e92e8":"## Detect Hardware","f713f125":"> The model jumps a bit in the start, in order to find the learning rate initially then after 15 epochs the model starts to stabilize.","e25de3a1":"## Visualizing the Data with the labels","a313053c":"## Data Directories ","cb13f58a":"*We list the directories, and check the folder for availiable types of flowers*","e0b31d4d":"## Prefetching and preprocessing"}}