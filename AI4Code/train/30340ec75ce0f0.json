{"cell_type":{"141cdefa":"code","bcb44771":"code","fb747c59":"code","85f11c6d":"code","2eb70a67":"code","a7072e9d":"code","49bff6dc":"code","8e90278c":"code","4f0ab0fa":"code","128e3915":"code","dd98d0c1":"code","088c8db2":"code","180d00cc":"code","684303ea":"code","ddb3c037":"code","f8fba68e":"code","39eb55bf":"code","ffe98c24":"code","7fe15938":"markdown","46246879":"markdown","1f780fb5":"markdown","c640d420":"markdown","f7a2b550":"markdown","ed4aa842":"markdown","ed922eba":"markdown","0a35a71b":"markdown","755f6ebe":"markdown","ddb4e05c":"markdown","c3b5791e":"markdown","68793134":"markdown"},"source":{"141cdefa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to build Naive classifiero load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","bcb44771":"import torch\nimport torchvision\nfrom torchvision import transforms\nimport torch.nn as nn","fb747c59":"class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.densenet121=torchvision.models.densenet121(pretrained=True)\n        num_ftrs = self.densenet121.classifier.in_features\n        self.densenet121.classifier=nn.Sequential(\n                nn.Linear(num_ftrs, 15), #don't forget to change the output\n                nn.Sigmoid()\n        )\n    def forward(self, x):\n        x=self.densenet121(x)\n        return x\n        ","85f11c6d":"df=pd.read_csv(\"..\/input\/Data_Entry_2017.csv\")\ndf.head()","2eb70a67":"import random\nwith open('..\/input\/train_val_list.txt') as f:\n    content = f.readlines() #reading from the file\ntraining_set = [x.strip() for x in content] #stripping away the newline characters\nvalidation_set = random.sample(training_set, 17600) \ntraining_set = [i for i in training_set if i not in validation_set]\ni1=training_set\nprint(len(training_set))\nprint(len(validation_set))","a7072e9d":"with open('..\/input\/test_list.txt') as f:\n    content = f.readlines()\ntest_set = [x.strip() for x in content]\nprint(len(test_set))","49bff6dc":"idxs=df['Image Index']\ncnts=df['Finding Labels']\npathology_list = cnts.tolist()\npathology_list = set(pathology_list)\npathology_list = list(pathology_list)\ncnts.index=idxs\nprint(cnts) #getting a view of what my dataset looks like","8e90278c":"pathology_list = ['Cardiomegaly','Emphysema','Effusion','Hernia','Nodule','Pneumothorax','Atelectasis','Pleural_Thickening','Mass','No Finding','Edema','Consolidation','Infiltration','Fibrosis','Pneumonia']\n#Converting string classes to numbers, which can be used as labels for training\nlabels = df['Finding Labels'].tolist()\ntemp = []\nfor i, element in enumerate(labels):\n    temp=len(pathology_list)*[0]\n    for j, pathology in enumerate(pathology_list):\n        if pathology in element:\n            temp[j]=1\n    labels[i] = temp\ndata_labels = pd.Series(labels, index=idxs)\ndata_labels.head()","4f0ab0fa":"training_labels = data_labels[training_set]\nvalidation_labels = data_labels[validation_set]\ntest_labels = data_labels[test_set]\nprint(len(training_set))\nprint(len(validation_set))\nprint(len(test_set))","128e3915":"import glob\nimport random\nimport cv2\nimport matplotlib.pyplot as plt\n\npaths = []\nnidxs1 = []\nnidxs2=[]\nnidxs3=[]\n\nfor filename in glob.iglob('..\/input\/**\/*.png', recursive=True): #worst algorithm ever\n    paths.append(filename)\n    for i in i1:\n        if i in filename:\n            nidxs1.append(filename)\n    for i in validation_set:\n        if i in filename:\n            nidxs2.append(filename)\n    for i in test_set:\n        if i in filename:\n            nidxs3.append(filename)\n            \ntraining_labels.index = nidxs1\nvalidation_labels.index = nidxs2\ntest_labels.index=nidxs3\n\nprint(nidxs1[0])\n\ns = random.sample(paths, 3)\n\n\nplt.figure(figsize=(16,16))\nplt.subplot(131)\nplt.imshow(cv2.imread(s[0]))\n\nplt.subplot(132)\nplt.imshow(cv2.imread(s[1]))\n\nplt.subplot(133)\nplt.imshow(cv2.imread(s[2]));","dd98d0c1":"#Custom dataset that will be used to load the data into a Dataloader\nfrom PIL import Image\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, indices, labels, transforms=None):\n        self.labels = labels\n        self.indices = indices\n        self.transforms = transforms\n    def __len__(self):\n        return len(self.indices)\n    def __getitem__(self, index):\n        address = self.indices[index]\n        x = Image.open(address).convert('RGB')\n        y = torch.FloatTensor(self.labels[address])\n        if self.transforms:\n            x = self.transforms(x)\n        return x, y","088c8db2":"transform = transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n)","180d00cc":"tl = training_labels.to_dict()\nvl = validation_labels.to_dict()\ntrl = training_labels.to_dict()","684303ea":"dsetTrain = CustomDataset(nidxs1, tl, transform)\ndsetVal = CustomDataset(nidxs2, vl, transform)\ndsetTest = CustomDataset(nidxs3, trl, transform)","ddb3c037":"trainloader = torch.utils.data.DataLoader(\n    dataset = dsetTrain, batch_size = 10,\n    shuffle = True, num_workers = 2\n)\n\nvalloader = torch.utils.data.DataLoader(\n    dataset = dsetVal, batch_size = 10,\n    shuffle = True, num_workers = 2\n)\n\ntestloader = torch.utils.data.DataLoader(\n    dataset = dsetTest, batch_size = 10,\n    shuffle = False, num_workers = 2\n)","f8fba68e":"# The function that will train the model\nimport copy\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    i=0\n    for epoch in range(num_epochs):\n        for x, y in trainloader:\n            x = torch.autograd.Variable(x).cuda()\n            y = torch.autograd.Variable(y).cuda()\n            scheduler.step()\n            model.train()\n            optimizer.zero_grad()\n            outputs = model(x)\n            loss = criterion(outputs, y)\n            loss.backward()\n            optimizer.step()\n            i+=1\n            if(i==100):\n                print('.')\n                break #remove this line in real life\n        print('Loss at the end of '+str(epoch+1)+': '+str(loss)+' '+str(i))\n        break #remove this line in real life\n        total = 0\n        correct = 0\n        for x, y in valloader:\n            model.eval()\n            x = torch.autograd.Variable(x).cuda()\n            y = torch.autograd.Variable(y).cuda()\n            with torch.no_grad():\n                outputs = model(x)\n                _, preds = torch.max(outputs, 1)\n                loss = criterion(outputs, y)\n                total += y.size(0)\n                correct += (pred==y).sum()\n        epoch_acc = (correct\/total)\n        print('Accuracy at the end of '+str(epoch+1)+': '+str(epoch_acc*100)+'%')\n        if phase == 'val' and epoch_acc > best_acc:\n            best_acc = epoch_acc\n            best_model_wts = copy.deepcopy(model.state_dict())\n        print('-'*5)\n    print('Training finished...')\n    print()\n    model.load_state_dict(best_model_wts)\n    return model","39eb55bf":"myNet = Net().cuda\ncriterion = nn.BCELoss().cuda()\noptimizer_ft = torch.optim.Adam (myNet().parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\nexp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\nmyNet = train_model(myNet(), criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)","ffe98c24":"#If you want to know the accuracy over the training set, uncomment the region below\n#print('Checking accuracy over the test set')\n#correct=0\n#total=0\n#with torch.no_grad():\n#    for data in testloader:\n#        x, y = data\n#        scores=model_ft(x)\n#        _, predicted=scores.data.max(1)\n#        total+=y.size(0)\n#        correct+=(predicted==y).sum()\n#print('Accuracy over the test set is:  ', str((int(correct)\/int(total))*100), ' %')","7fe15938":"I obtain a pandas series with the labels of each set and the name of the picture files as the index.","46246879":"I will use torch and torchvision to model my neural network and to take advantage of the ready-made optimization functions.","1f780fb5":"All the labels of the data are placed in the csv file, that is where we have to read from.","c640d420":"**In this kernel, I try to implement a Chest X-Ray classifier by using a pretrained version of DenseNet. **","f7a2b550":" **Conclusion**\n \n Whenever the image had two or more labels, I used that as a separate class, which I think is not a good idea in the limited training set. I am very new to the field, so I am quite open to suggestions and corrections. Of course, the predicted loss is not real since we stop it after 100 iterations","ed4aa842":"This will determine the number of classes, which I have used to implement the linear layer of the neural network above.","ed922eba":"Now, I build my custom dataset and load the training data","0a35a71b":"In the 'train_val_list.txt' file, the creator of the dataset specifies which of the files will be used for training and validation. So I obtain a list of the names of the scans which will be used for training and validation purposes. ","755f6ebe":"Now the dataloaders are ready to be used in the training process.","ddb4e05c":"Here I try to modify the index of the datasets in order for them to show the full path of the path of the picture files for each set.","c3b5791e":"This model was inspired by Rajpurkar et al., CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays\nwith Deep Learning, 2017","68793134":"Same reasoning to obtain the scans which will be part of the test set."}}