{"cell_type":{"53df1101":"code","e0d8c895":"code","11c54b9a":"code","2c85778a":"code","160a9f27":"code","6539c35d":"code","66bcd3a8":"code","13fc129e":"code","c0298fa2":"code","5e1731e6":"code","82cedd77":"code","6f862585":"code","6acd2c3a":"code","3626dad1":"code","abeeb883":"code","e6d0679b":"code","29c27e79":"code","1a78d4b2":"code","8aad4b89":"code","5c717aaf":"code","1ea4a001":"code","58e9c3ad":"code","96891b48":"code","7a8533ac":"code","888a841d":"code","fcbcec31":"code","ea3b528e":"code","1dff0ab6":"code","dfe33dd8":"code","38e0c24c":"code","a1219f92":"code","92412445":"code","5f0adbc5":"code","ad03eed0":"code","7a52ca9a":"code","42e35c69":"code","ad0ef5f0":"code","0f84fbc9":"code","35c4261e":"code","485691dc":"code","9665f1c4":"code","9668a3bc":"code","420b60c5":"code","10fa620e":"code","1aaebca7":"code","dba09382":"code","9d883ad1":"code","e539b317":"code","c97a8107":"code","9654f66b":"code","11444582":"code","d544c131":"code","c2784863":"code","76ed378b":"code","8ae30688":"code","f32b8e82":"code","7de3066c":"code","ed492571":"code","5a19d783":"code","90797d2f":"code","d742f186":"code","73e99b84":"code","c8d40218":"code","42a9cb0e":"code","2c4da708":"code","4f65ae77":"code","e5dbc5e7":"code","88d296ca":"code","0884df75":"code","3c687575":"code","98c725d7":"code","b92a3689":"code","2a4091a2":"code","c8d92040":"code","4f73e6e5":"code","ffbd0862":"code","6d779bbb":"code","dd0ffad2":"code","1599b0a0":"markdown","d66ed064":"markdown","d9ac2ef8":"markdown","6e77de92":"markdown","b1df6729":"markdown","b2e346a9":"markdown","892146ee":"markdown","56e3a9d6":"markdown","cbb7f67b":"markdown","164422ad":"markdown"},"source":{"53df1101":"#Imports:\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport math\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom matplotlib import rcParams\nfrom wordcloud import WordCloud","e0d8c895":"pd.set_option('display.max_columns', None) \npd.set_option('display.max_rows', None)  \npd.set_option('display.max_colwidth', -1) \n","11c54b9a":"#Code:\n# reading the csv file into pandas dataframes\ndf=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\n","2c85778a":"df.head()","160a9f27":"df['target'].value_counts()","6539c35d":"#creating a new column- length \n# this gives the length of the post\ndf['length'] = np.NaN\nfor i in range(0,len(df['text'])):\n    df['length'][i]=(len(df['text'][i]))\ndf.length = df.length.astype(int)","66bcd3a8":"#creating subplots to see distribution of length of tweet\nsns.set_style(\"darkgrid\");\nf, (ax1, ax2) = plt.subplots(figsize=(12,6),nrows=1, ncols=2,tight_layout=True);\nsns.distplot(df[df['target']==1][\"length\"],bins=30,ax=ax1);\nsns.distplot(df[df['target']==0][\"length\"],bins=30,ax=ax2);\nax1.set_title('\\n Distribution of length of tweet labelled Disaster\\n');\nax2.set_title('\\nDistribution of length of tweet labelled No Disaster\\n ');\nax1.set_ylabel('Frequency');","13fc129e":"# word cloud for words related to Disaster \ntext=\" \".join(post for post in df[df['target']==1].text)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\nFrequntly occuring words related to Disaster \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","c0298fa2":"# word cloud for words related to No Disaster \ntext=\" \".join(post for post in df[df['target']==0].text)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\nFrequntly occuring words related to No Disaster \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","5e1731e6":"#calculating basline accuracy\ndf['target'].value_counts(normalize=True)","82cedd77":"# Import Tokenizer\nfrom nltk.tokenize import RegexpTokenizer","6f862585":"# Instantiate Tokenizer\ntokenizer = RegexpTokenizer(r'\\w+') \n","6acd2c3a":"#changing the contents of selftext to lowercase\ndf.loc[:,'text'] = df.text.apply(lambda x : str.lower(x))","3626dad1":"#removing hyper link, latin characters and digits\ndf['text']=df['text'].str.replace('http.*.*', '',regex = True)\ndf['text']=df['text'].str.replace('\u00fb.*.*', '',regex = True)\ndf['text']=df['text'].str.replace(r'\\d+','',regex= True)","abeeb883":"# \"Run\" Tokenizer\ndf['tokens'] = df['text'].map(tokenizer.tokenize)","e6d0679b":"#displaying first 5 rows of dataframe\ndf.head()","29c27e79":"# Printing English stopwords\nprint(stopwords.words(\"english\"))","1a78d4b2":"#assigning stopwords to a variable\nstop = stopwords.words(\"english\")","8aad4b89":"# adding this stop word to list of stopwords as it appears on frequently occuring word\nitem=['amp'] #'https','co','http','\u00fb','\u00fb\u00f2','\u00fb\u00f3','\u00fb_'","5c717aaf":"stop.extend(item)","1ea4a001":"#removing stopwords from tokens\ndf['tokens']=df['tokens'].apply(lambda x: [item for item in x if item not in stop])","58e9c3ad":"# Importing lemmatizer \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\n# Instantiating lemmatizer \nlemmatizer = WordNetLemmatizer()\n","96891b48":"lemmatize_words=[]\nfor i in range (len(df['tokens'])):\n    word=''\n    for j in range(len(df['tokens'][i])):\n        lemm_word=lemmatizer.lemmatize(df['tokens'][i][j])#lemmatize\n        \n        word=word + ' '+lemm_word # joining tokens into sentence    \n    lemmatize_words.append(word) # store in list\n   ","7a8533ac":"#creating a new column to store the result\ndf['lemmatized']=lemmatize_words","888a841d":"#displaying first 5 rows of dataframe\ndf.head()","fcbcec31":"#imports\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression","ea3b528e":"#defining X and y for the model\nX = df['lemmatized']\ny = df['target']","1dff0ab6":"# Spliting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)","dfe33dd8":"#ensuring that the value counts are quite evenly distributed\ny_train.value_counts()","38e0c24c":"y_test.shape","a1219f92":"# pipeline will consist of two stages:\n# 1.Instantiating countVectorizer\n# 2.Instantiating logistic regression model\n\npipe = Pipeline([\n    ('cvec', CountVectorizer()),  \n    ('lr', LogisticRegression()) \n])","92412445":"tuned_params = {\n    'cvec__max_features': [2500, 3000, 3500],\n    'cvec__min_df': [2,3],\n    'cvec__max_df': [.9, .95],\n    'cvec__ngram_range': [(1,1), (1,2)]\n}\ngs = GridSearchCV(pipe, param_grid=tuned_params, cv=3) # Evaluating model on unseen data\n\nmodel_lr=gs.fit(X_train, y_train) # Fitting model\n\n# This is the average of all cv folds for a single \n#combination of the parameters specified in the tuned_params\nprint(gs.best_score_) \n\n#displaying the best values of parameters\ngs.best_params_","5f0adbc5":"# Test score\ngs.score(X_train, y_train)","ad03eed0":"# Test score\ngs.score(X_test, y_test)","7a52ca9a":"# Generating predictions!\npredictions_lr = model_lr.predict(X_test)","42e35c69":"# Importing the confusion matrix function\nfrom sklearn.metrics import confusion_matrix","ad0ef5f0":"# Generating confusion matrix\nconfusion_matrix(y_test, predictions_lr)","0f84fbc9":"#interpreting confusion matrix\ntn, fp, fn, tp = confusion_matrix(y_test, predictions_lr).ravel()","35c4261e":"#values with coreesponding labels\nprint(\"True Negatives: %s\" % tn)\nprint(\"False Positives: %s\" % fp)\nprint(\"False Negatives: %s\" % fn)\nprint(\"True Positives: %s\" % tp)","485691dc":"# Importing model\nfrom sklearn.naive_bayes import MultinomialNB","9665f1c4":"# Instantiating model\nnb = MultinomialNB()","9668a3bc":"# Instantiating CountVectorizer.\ncvec = CountVectorizer(max_features = 500)","420b60c5":"# fit_transform() fits the model and transforms training data into feature vectors\nX_train_cvec = cvec.fit_transform(X_train, y_train).todense()","10fa620e":"#tranform test data and convert into array\nX_test_cvec = cvec.transform(X_test).todense()","1aaebca7":"# Fitting model\nmodel_nb=nb.fit(X_train_cvec, y_train)","dba09382":"# Generating predictions\npredictions_nb = model_nb.predict(X_test_cvec)","9d883ad1":"# Training score\nmodel_nb.score(X_train_cvec, y_train)","e539b317":"# Test score\nmodel_nb.score(X_test_cvec, y_test)","c97a8107":"# Generating confusion matrix\nconfusion_matrix(y_test, predictions_nb)","9654f66b":"#interpreting confusion matrix\ntn, fp, fn, tp = confusion_matrix(y_test, predictions_nb).ravel()","11444582":"#values with coreesponding labels\nprint(\"True Negatives: %s\" % tn)\nprint(\"False Positives: %s\" % fp)\nprint(\"False Negatives: %s\" % fn)\nprint(\"True Positives: %s\" % tp)","d544c131":"# word cloud for Frequntly occuring words related to Disaster\ntext=\" \".join(post for post in df[df['target']==1].lemmatized)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\nFrequntly occuring words related to Disaster \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","c2784863":"# word cloud for Frequntly occuring words related to No Disaster\ntext=\" \".join(post for post in df[df['target']==0].lemmatized)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\nFrequntly occuring words related to No Disaster \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","76ed378b":"#reading the test data\ntest=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","8ae30688":"test.head()","f32b8e82":"#creating a new column- length \n# this gives the length of the post\ntest['length'] = np.NaN\nfor i in range(0,len(test['text'])):\n    test['length'][i]=(len(test['text'][i]))\ntest.length = test.length.astype(int)","7de3066c":"# word cloud for Frequntly occuring words in test dataframe\ntext=\" \".join(post for post in df.text)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\nFrequntly occuring words in test dataframe \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","ed492571":"# Instantiate Tokenizer\ntokenizer = RegexpTokenizer(r'\\w+')","5a19d783":"#changing the contents of selftext to lowercase\ntest.loc[:,'text'] = test.text.apply(lambda x : str.lower(x))","90797d2f":"#removing hyper link and latin characters\ntest['text']=test['text'].str.replace('http.*.*', '',regex = True)\ntest['text']=test['text'].str.replace('\u00fb.*.*', '',regex = True)\ntest['text']=test['text'].str.replace(r'\\d+','',regex= True)","d742f186":"# \"Run\" Tokenizer\ntest['tokens'] = test['text'].map(tokenizer.tokenize)","73e99b84":"#displaying first 5 rows of dataframe\ntest.head()","c8d40218":"#removing stopwords from tokens\ntest['tokens']=test['tokens'].apply(lambda x: [item for item in x if item not in stop])","42a9cb0e":"lemmatize_words=[]\nfor i in range (len(test['tokens'])):\n    word=''\n    for j in range(len(test['tokens'][i])):\n        lemm_word=lemmatizer.lemmatize(test['tokens'][i][j])#lemmatize\n        \n        word=word + ' '+lemm_word # joining tokens into sentence    \n    lemmatize_words.append(word) # store in list\n   ","2c4da708":"#creating a new column to store the result\ntest['lemmatized']=lemmatize_words","4f65ae77":"#displaying first 5 rows of dataframe\ntest.head()","e5dbc5e7":"# word cloud for Frequntly occuring words in test dataframe after lemmatizing\ntext=\" \".join(post for post in df.lemmatized)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\nFrequntly occuring words in test dataframe \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","88d296ca":"predictions_kaggle = model_lr.predict(test['lemmatized'])","0884df75":"#tranform test data and convert into array\nkaggle_cvec = cvec.transform(test['lemmatized']).todense()","3c687575":"predictions_kaggle_nb=model_nb.predict(kaggle_cvec)","98c725d7":"# Creating an empty data frame\nsubmission_kaggle = pd.DataFrame()","b92a3689":"# Assigning values to the data frame-submission_kaggle\nsubmission_kaggle['Id'] = test.id\nsubmission_kaggle['target'] = predictions_kaggle","2a4091a2":"# Head of submission_kaggle\nsubmission_kaggle.head()","c8d92040":"# saving data as  final_kaggle.csv\nsubmission_kaggle.loc[ :].to_csv('final_kaggle.csv',index=False)","4f73e6e5":"# Creating an empty data frame\nsubmission_kaggle_nb = pd.DataFrame()","ffbd0862":"# Assigning values to the data frame-submission_kaggle\nsubmission_kaggle_nb['Id'] = test.id\nsubmission_kaggle_nb['target'] = predictions_kaggle_nb","6d779bbb":"# Head of submission_kaggle\nsubmission_kaggle_nb.head()","dd0ffad2":"# saving data as  final_kaggle.csv\nsubmission_kaggle_nb.loc[ :].to_csv('final_kaggle_nb.csv',index=False)","1599b0a0":"### Lemmatizing \nWhen we \"lemmatize\" data, we take words and attempt to return their *lemma*, or the base\/dictionary form of a word.<br>\n","d66ed064":"### Logistic Regression Model","d9ac2ef8":"## Modelling \n---\n This step creates two models \n\n>1.Logistic Regression Model<br>\n>2.Naive Bayes Model<br>\n","6e77de92":"# TEST DATA","b1df6729":"# NAIVE BAYES PREDICTION","b2e346a9":"### Naive Bayes Model","892146ee":"### Tokenizing\n\nWhen we \"tokenize\" data, we take it and split it up into distinct chunks based on some pattern.","56e3a9d6":"Disaster Tweet Classification -Logistic Regression and Naive Bayes score of Logistic Regression - 0.7932 ","cbb7f67b":"### Removing Stop Words","164422ad":"# Creating  .csv file"}}