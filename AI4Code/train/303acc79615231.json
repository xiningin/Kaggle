{"cell_type":{"d62378c9":"code","b3c982fc":"code","be7b2c0f":"code","b9f97c10":"code","952f9321":"code","24042721":"code","d06eda54":"markdown","ea7f22bb":"markdown","52b6227f":"markdown","987e202f":"markdown","7ee1b81e":"markdown","81864b66":"markdown"},"source":{"d62378c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b3c982fc":"df = pd.read_csv('\/kaggle\/input\/heart-failure-prediction\/heart.csv')\nlabel = df['HeartDisease']\nx_df = df.drop('HeartDisease', axis = 1)\ndf.head()","be7b2c0f":"import matplotlib.pyplot as plt\nimport seaborn as sns\n#Dataframe information\nprint('Dataframe information\\n')\nprint(x_df.info())\nprint('*************************************************') \n\n#Null data\nprint('Null Data\\n')\nprint(x_df.isnull().sum()) #No null data\nprint('*************************************************')\n\n#Split categorical \/ numeric data (for pipeline)\ncolumns = x_df.columns\ncat = [col for col in columns if x_df[col].dtype == 'object']\nnum = [col for col in columns if (x_df[col].dtype == 'int') | (df[col].dtype == 'float')]\nprint('Columns shape')\nprint('categorical columns : ', len(cat), 'numeric columns : ', len(num))\nprint('*************************************************')\n\n#Research nunique\nprint('Research nunique for categorical data\\n')\nprint(x_df[cat].nunique())\nprint('*************************************************')\n\nprint('Research nunique for numeric data\\n')\nprint(x_df[num].nunique())\nprint(x_df['FastingBS'].unique())\nprint('*************************************************')\n\n#Label balance\nprint('Label balance')\nprint(label.value_counts())\nprint('*************************************************')\n\n#Outlier Detection\n#IQR * 1.5\ndef outlier_detector(df):\n    Q1, Q3 = df.quantile([0.25, 0.75])\n    IQR = Q3 - Q1\n    min_bound, max_bound = Q1 - IQR * 1.5, Q3 + IQR * 1.5\n    df_idx = df.loc[(df < min_bound) | (df > max_bound)].index\n    return df_idx\nprint('# Outlier based on IQR boundry')\nfor col_ in num:\n    print(col_,':', len(outlier_detector(x_df[col_])))\n\n\n#Box-plot\nplt.figure(figsize = (10,10))\nfor n, col_ in enumerate(num):\n    plt.subplot(3,2,n+1)\n    plt.title(col_)\n    plt.boxplot(x_df[col_])\nplt.show()\nprint('*************************************************')\n\n#Correlation plot\nprint('Correlation : Numerical Varialbe corralation (Pearson)')\nplt.title('Correation Heatmap')\nsns.heatmap(x_df.corr())\nplt.show()\nprint('*************************************************')\n\n#Age - HeartDisease\nprint('Age - HeartDisease')\nAge_df = pd.DataFrame(df['HeartDisease'].groupby(df['Age']).count())\nplt.title('Age-HeartDisease')\nplt.bar(Age_df.index, Age_df['HeartDisease'])\nplt.show()\nprint('*************************************************')\n\n","b9f97c10":"#Data Preprocessing\n#FastingBS (to categorical variable)\ncat.append('FastingBS')\nnum.remove('FastingBS')\nprint('categorical : ', cat, 'numerical : ', num)\n\n#Outlier drop\ndef outlier(df):\n    Q1, Q3 = df.quantile([0.25, 0.75])\n    IQR = Q3 - Q1\n    min_bound, max_bound = Q1 - IQR * 1.5, Q3 + IQR * 1.5\n    return min_bound, max_bound\nmin_b, max_b = outlier(df['RestingBP'])\ndf.iloc[df.loc[df['RestingBP'] < min_b].index]\ndf.drop(index = [449], axis = 0, inplace = True)\nx_df = df.drop('HeartDisease', axis = 1)\nlabel = df['HeartDisease']\n\n#Age Groupping\nprint(x_df['Age'].min(), x_df['Age'].max())\nx, bins = np.histogram(x_df['Age'], bins = 7)\nlabel_names = [10,20,30,40,50,60,70]\nx_df['Age'] = pd.cut(x_df['Age'], bins = bins, labels = label_names, include_lowest = True)\n\n#one-hot-encoding & Min-Max-scaling\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nencoder = OneHotEncoder()\nscaler = MinMaxScaler()\npreprocessor = ColumnTransformer([\n    ('onehotencoder', encoder, cat),\n    ('minmaxscaler', scaler, num)])\nprep_x_data = preprocessor.fit_transform(x_df)\n\n#Data split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(prep_x_data, label, test_size = 0.33, random_state = 26)\nprint('Data Shape')\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","952f9321":"#Modeling\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\n#Randomforest\nrf_clf = RandomForestClassifier()\nrf_clf.fit(X_train, y_train)\nrf_proba = rf_clf.predict_proba(X_test)[:,1]\nrf_preds = rf_clf.predict(X_test)\n\n#Support Vector machine\nsvm_clf = SVC(probability=True)\nsvm_clf.fit(X_train, y_train)\nsvm_proba = svm_clf.predict_proba(X_test)[:,1]\nsvm_preds = svm_clf.predict(X_test)\n\n#Xgboost\nxgb_clf = XGBClassifier(learning_rate = 0.01)\nxgb_clf.fit(X_train, y_train, eval_metric = 'logloss')\nxgb_proba = xgb_clf.predict_proba(X_test)[:,1]\nxgb_preds = xgb_clf.predict(X_test)","24042721":"#Evaluation\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, precision_score, recall_score\nrf_score = roc_auc_score(y_test, rf_proba)\nsvm_score = roc_auc_score(y_test, svm_proba)\nxgb_score = roc_auc_score(y_test, xgb_proba)\nprint('AUC score\\nRandomForestClassifier score : {:.4f}\\nSupport Vector Classifier score : {:.4f}\\nXGB Classifier score : {:.4f}'.format(rf_score, svm_score, xgb_score))\nprint('\\n*************************************************')\n\nprint('RandomForest Confussion Matrix')\nprint(confusion_matrix(y_test, rf_preds))\nprint('Accuracy score : {:.4f}'.format(accuracy_score(y_test,rf_preds)))\nprint('Precision score : {:.4f}'.format(precision_score(y_test,rf_preds)))\nprint('Recall score : {:.4f}'.format(recall_score(y_test,rf_preds)))\nprint('\\n*************************************************')\n\nprint('SVC Confussion Matrix')\nprint(confusion_matrix(y_test, svm_preds))\nprint('Accuracy score : {:.4f}'.format(accuracy_score(y_test,svm_preds)))\nprint('Precision score : {:.4f}'.format(precision_score(y_test,svm_preds)))\nprint('Recall score : {:.4f}'.format(recall_score(y_test,svm_preds)))\nprint('\\n*************************************************')\n\nprint('XGB Confussion Matrix')\nprint(confusion_matrix(y_test, xgb_preds))\nprint('Accuracy score : {:.4f}'.format(accuracy_score(y_test,xgb_preds)))\nprint('Precision score : {:.4f}'.format(precision_score(y_test,xgb_preds)))\nprint('Recall score : {:.4f}'.format(recall_score(y_test,xgb_preds)))\nprint('\\n*************************************************')","d06eda54":"> # **3. Preprocessing**","ea7f22bb":"> # **1. Data Load**","52b6227f":"> # **2. EDA**","987e202f":"# **Result**\n*most model I used has good performance for the Dataset. there is possible to make better performance by refining hyper-parameter with grid-search model but for now, The most performed model is Randomforest Model.*\n","7ee1b81e":"> # **4. Modeling**","81864b66":"> # **5. Evaluation**"}}