{"cell_type":{"37d0246e":"code","34d90672":"code","9170f756":"code","2dc4b4ae":"code","ce0cd6de":"code","16e5a605":"code","70e1f9b1":"code","925182f9":"code","2e8b284f":"code","bb8cc7ec":"code","3df9c56c":"markdown","26f061bc":"markdown","dd3d5da6":"markdown","f1c35243":"markdown","7ac42d5f":"markdown"},"source":{"37d0246e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","34d90672":"import gensim\nmodel = gensim.models.KeyedVectors.load_word2vec_format('..\/input\/embeddings\/GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin', \n                                                        binary=True)\n\nwords = model.index2word\n\nw_rank = {}\nfor i,word in enumerate(words):\n    w_rank[word] = i\n\nWORDS = w_rank","9170f756":"import re\nfrom collections import Counter\n\ndef words(text): return re.findall(r'\\w+', text.lower())\n\ndef P(word): \n    \"Probability of `word`.\"\n    # use inverse of rank as proxy\n    # returns 0 if the word isn't in the dictionary\n    return - WORDS.get(word, 0)\n\ndef correction(word): \n    \"Most probable spelling correction for word.\"\n    return max(candidates(word), key=P)\n\ndef candidates(word): \n    \"Generate possible spelling corrections for word.\"\n    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n\ndef known(words): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in WORDS)\n\ndef edits1(word):\n    \"All edits that are one edit away from `word`.\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef edits2(word): \n    \"All edits that are two edits away from `word`.\"\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))","2dc4b4ae":"train = pd.read_csv(\"..\/input\/train.csv\")","ce0cd6de":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        if punct in x:\n            x = x.replace(punct, f' {punct} ')\n    return x\n\ntrain[\"question_text\"] = train[\"question_text\"].apply(lambda x: x.lower())\ntrain[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_text(x))","16e5a605":"def build_vocab(texts):\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\nvocab = build_vocab(train.question_text)","70e1f9b1":"import heapq\nfrom operator import itemgetter\n\ntop_90k_words = dict(heapq.nlargest(90000, vocab.items(), key=itemgetter(1)))","925182f9":"from multiprocessing import Pool\npool = Pool(4)","2e8b284f":"corrected_words = pool.map(correction,list(top_90k_words.keys()))","bb8cc7ec":"for word,corrected_word in zip(top_90k_words,corrected_words):\n    if word!=corrected_word:\n        print(word,\":\",corrected_word)","3df9c56c":"If you like this notebook then please upvote (button at the top right).","26f061bc":"It is not perfect but helps to understand the dataset much better. \nFor example:\n'baelish' is converted to Bullish\n'sansa' to salsa\n'tarly' to early\n'elon' to felon\n\nApparently doesn't understand the Game of thrones characters. Does that mean a lot of GOT characters are there?\n\nAlso you can try to run something like this after your usual text cleaning pipeline. Hope that helps to remove some spelling errors. \n","dd3d5da6":"This comes from CPMP script in the Quora questions similarity challenge. With some modifications for this challenge.\n\n>> I have been struggling for a while on how to spell check questions while only using allowed data\/software.  Here is the solution I am using now.\n>> It is an adaptation of Peter Norvig's spell checker.  It uses word2vec ordering of words to approximate word probabilities.  Indeed, Google word2vec apparently orders words >> in decreasing order of frequency in the training corpus.","f1c35243":"The rest of the code is a simple modification of Peter Norvig's code. Instead of computing the frequency of each word we use the above rank.","7ac42d5f":"That's it. If you have downloaded word2vec then you can start using this code.  Here are few examples of what it does.\n\ncorrection('quikly') returns quickly\n\ncorrection('israil') returns israel\n\ncorrection('neighbour') returns neighbor"}}