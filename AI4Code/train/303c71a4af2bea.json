{"cell_type":{"1d3ef1a9":"code","3f8c808c":"code","d87d5991":"code","235b25ba":"code","95d89a8a":"code","0a431f8b":"code","f114ca33":"code","222a5933":"code","3172f05a":"code","0687711e":"code","1f2f0f99":"code","0b570486":"code","e90d6abe":"code","bd57ae3e":"code","440bfaa2":"code","f1a26ec5":"code","3c993111":"code","a88b224a":"code","3c18361c":"code","93a3d4dc":"code","111d5fe2":"code","e416e66c":"code","27ffd618":"code","e939f64f":"code","4192b674":"code","3ed8d483":"markdown"},"source":{"1d3ef1a9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (6,6)\n\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras import initializers, regularizers, constraints\n\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU, TimeDistributed\nfrom tensorflow.keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D, Add, Flatten, SpatialDropout1D\nfrom tensorflow.keras.layers import GlobalAveragePooling1D, BatchNormalization, concatenate\nfrom tensorflow.keras.layers import Reshape,  Concatenate, Lambda, Average\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.layers import Add\n\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\nimport tensorflow.keras.utils as utils\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/news-category-dataset\"))\n","3f8c808c":"df = pd.read_json('..\/input\/news-category-dataset\/News_Category_Dataset_v2.json', lines=True)\ndf.head()","d87d5991":"cates = df.groupby('category')\nprint(\"total categories:\", cates.ngroups)\nprint(cates.size())","235b25ba":"df.category = df.category.map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)\n","95d89a8a":"df['text'] = df.headline + \" \" + df.short_description\n\n# tokenizing\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df.text)\nX = tokenizer.texts_to_sequences(df.text)\ndf['words'] = X\n\n# delete some empty and short data\n\ndf['word_length'] = df.words.apply(lambda i: len(i))\ndf = df[df.word_length >= 5]\n\ndf.head()","0a431f8b":"maxlen = 50\nX = list(sequence.pad_sequences(df.words, maxlen=maxlen))","f114ca33":"categories = df.groupby('category').size().index.tolist()\ncategory_int = {}\nint_category = {}\nfor i, k in enumerate(categories):\n    category_int.update({k:i})\n    int_category.update({i:k})\n\ndf['c2id'] = df['category'].apply(lambda x: category_int[x])","222a5933":"word_index = tokenizer.word_index\n\nEMBEDDING_DIM = 100\n\nembeddings_index = {}\nf = open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s unique tokens.' % len(word_index))\nprint('Total %s word vectors.' % len(embeddings_index))","3172f05a":"embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\nembedding_layer = Embedding(len(word_index)+1,\n                            EMBEDDING_DIM,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)","0687711e":"X = np.array(X)\nY = utils.to_categorical(list(df.c2id))\n\n# and split to training set and validation set\n\nseed = 29\nx_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=seed)","1f2f0f99":"inp = Input(shape=(maxlen,), dtype='int32')\nembedding = embedding_layer(inp)\nstacks = []\nfor kernel_size in [2, 3, 4]:\n    conv = Conv1D(64, kernel_size, padding='same', activation='relu', strides=1)(embedding)\n    pool = MaxPooling1D(pool_size=3)(conv)\n    drop = Dropout(0.5)(pool)\n    stacks.append(drop)\n\nmerged = Concatenate()(stacks)\nflatten = Flatten()(merged)\ndrop = Dropout(0.5)(flatten)\noutp = Dense(len(int_category), activation='softmax')(drop)\n\nTextCNN = Model(inputs=inp, outputs=outp)\nTextCNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nTextCNN.summary()","0b570486":"textcnn_history = TextCNN.fit(x_train, \n                              y_train, \n                              batch_size=128, \n                              epochs=20, \n                              validation_data=(x_val, y_val))","e90d6abe":"acc = textcnn_history.history['accuracy']\nval_acc = textcnn_history.history['val_accuracy']\nloss = textcnn_history.history['loss']\nval_loss = textcnn_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\nplt.legend()\n\nplt.show()","bd57ae3e":"inp = Input(shape=(maxlen,), dtype='int32')\nx = embedding_layer(inp)\nx = SpatialDropout1D(0.2)(x)\nx = Bidirectional(GRU(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\nx = Conv1D(64, kernel_size=3)(x)\navg_pool = GlobalAveragePooling1D()(x)\nmax_pool = GlobalMaxPooling1D()(x)\nx = concatenate([avg_pool, max_pool])\noutp = Dense(len(int_category), activation=\"softmax\")(x)\n\nBiGRU = Model(inp, outp)\nBiGRU.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n\nBiGRU.summary()","440bfaa2":"bigru_history = BiGRU.fit(x_train, \n                          y_train, \n                          batch_size=128, \n                          epochs=20, \n                          validation_data=(x_val, y_val))","f1a26ec5":"plt.rcParams['figure.figsize'] = (6,6)\n\nacc = bigru_history.history['acc']\nval_acc = bigru_history.history['val_acc']\nloss = bigru_history.history['loss']\nval_loss = bigru_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\nplt.legend()\n\nplt.show()\n","3c993111":"class Attention(keras.layers.Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n        self.W = self.add_weight(shape=(input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n        if self.bias:\n            self.b = self.add_weight(shape=(input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n        if self.bias:\n            eij += self.b\n        eij = K.tanh(eij)\n        a = K.exp(eij)\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim\n    \n\nlstm_layer = LSTM(300, dropout=0.25, recurrent_dropout=0.25, return_sequences=True)\n\ninp = Input(shape=(maxlen,), dtype='int32')\nembedding= embedding_layer(inp)\nx = lstm_layer(embedding)\nx = Dropout(0.25)(x)\nmerged = Attention(maxlen)(x)\nmerged = Dense(256, activation='relu')(merged)\nmerged = Dropout(0.25)(merged)\nmerged = BatchNormalization()(merged)\noutp = Dense(len(int_category), activation='softmax')(merged)\n\nAttentionLSTM = Model(inputs=inp, outputs=outp)\nAttentionLSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n\nAttentionLSTM.summary()","a88b224a":"attlstm_history = AttentionLSTM.fit(x_train, \n                                    y_train, \n                                    batch_size=128, \n                                    epochs=20, \n                                    validation_data=(x_val, y_val))","3c18361c":"acc = attlstm_history.history['acc']\nval_acc = attlstm_history.history['val_acc']\nloss = attlstm_history.history['loss']\nval_loss = attlstm_history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\nplt.legend()\n\nplt.show()","93a3d4dc":"predicted = AttentionLSTM.predict(x_val)\ncm = pd.DataFrame(confusion_matrix(y_val.argmax(axis=1), predicted.argmax(axis=1)))","111d5fe2":"from IPython.display import display\npd.options.display.max_columns = None\ndisplay(cm)","e416e66c":"def evaluate_accuracy(model):\n    predicted = model.predict(x_val)\n    diff = y_val.argmax(axis=-1) - predicted.argmax(axis=-1)\n    corrects = np.where(diff == 0)[0].shape[0]\n    total = y_val.shape[0]\n    return float(corrects\/total)","27ffd618":"print(\"model TextCNN accuracy:          %.6f\" % evaluate_accuracy(TextCNN))\nprint(\"model Bidirectional GRU + Conv:  %.6f\" % evaluate_accuracy(BiGRU))\nprint(\"model LSTM with Attention:       %.6f\" % evaluate_accuracy(AttentionLSTM))","e939f64f":"def evaluate_accuracy_ensemble(models):\n    res = np.zeros(shape=y_val.shape)\n    for model in models:\n        predicted = model.predict(x_val)\n        res += predicted\n    res \/= len(models)\n    diff = y_val.argmax(axis=-1) - res.argmax(axis=-1)\n    corrects = np.where(diff == 0)[0].shape[0]\n    total = y_val.shape[0]\n    return float(corrects\/total)","4192b674":"print(evaluate_accuracy_ensemble([TextCNN, BiGRU, AttentionLSTM]))","3ed8d483":"This code if forked from https:\/\/www.kaggle.com\/hengzheng\/news-category-classifier-val-acc-0-65. I will be making changes to all of the models pretty soon"}}