{"cell_type":{"41057098":"code","d52c4925":"code","52642ddf":"code","26e1ac42":"code","089050ab":"code","87a5d65e":"code","d1c86820":"code","88ecb281":"code","209ceaf4":"code","18739e07":"code","5def0ffb":"code","e63681ca":"code","ae722bb7":"markdown","081ad0b2":"markdown","2bcc43cb":"markdown","6d9b6ebc":"markdown","d1e0e65d":"markdown","46dbedf5":"markdown","23e2bca4":"markdown"},"source":{"41057098":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd","d52c4925":"mnist=tf.keras.datasets.mnist\n(x_train,y_train),(x_test,y_test)=mnist.load_data()\nx_train,x_test=x_train\/255.0,x_test\/255.0\nprint(\"shape of x_train\",x_train.shape)","52642ddf":"model=tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28,28)),\n    tf.keras.layers.Dense(130,activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(10,activation='softmax')\n])","26e1ac42":"model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',\n             metrics=['accuracy'])","089050ab":"mnist_model=model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=10)","87a5d65e":"plt.plot(mnist_model.history['loss'],label='loss')\nplt.plot(mnist_model.history['val_loss'],label='val_loss')\nplt.legend()","d1c86820":"plt.plot(mnist_model.history['accuracy'],label='acc')\nplt.plot(mnist_model.history['val_accuracy'],label='val_acc')\nplt.legend()","88ecb281":"print(model.evaluate(x_test,y_test))","209ceaf4":"from sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n  if normalize:\n      cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n      print(\"Normalized confusion matrix\")\n  else:\n      print('Confusion matrix, without normalization')\n\n  print(cm)\n\n  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n  plt.title(title)\n  plt.colorbar()\n  tick_marks = np.arange(len(classes))\n  plt.xticks(tick_marks, classes, rotation=45)\n  plt.yticks(tick_marks, classes)\n\n  fmt = '.2f' if normalize else 'd'\n  thresh = cm.max() \/ 2.\n  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n      plt.text(j, i, format(cm[i, j], fmt),\n               horizontalalignment=\"center\",\n               color=\"white\" if cm[i, j] > thresh else \"black\")\n\n  plt.tight_layout()\n  plt.ylabel('True label')\n  plt.xlabel('Predicted label')\n  plt.show()\n\n\np_test = model.predict(x_test).argmax(axis=1)\ncm = confusion_matrix(y_test, p_test)\nplot_confusion_matrix(cm, list(range(10)))","18739e07":"misclassified_idx = np.where(p_test != y_test)[0]\ni = np.random.choice(misclassified_idx)\nplt.imshow(x_test[i], cmap='gray')\nplt.title(\"True label: %s Predicted: %s\" % (y_test[i], p_test[i]));","5def0ffb":"misclassified_idx = np.where(p_test != y_test)[0]\ni = np.random.choice(misclassified_idx)\nplt.imshow(x_test[i], cmap='gray')\nplt.title(\"True label: %s Predicted: %s\" % (y_test[i], p_test[i]));","e63681ca":"misclassified_idx = np.where(p_test != y_test)[0]\ni = np.random.choice(misclassified_idx)\nplt.imshow(x_test[i], cmap='gray')\nplt.title(\"True label: %s Predicted: %s\" % (y_test[i], p_test[i]));","ae722bb7":"As you are able to see below that some digits like 7 and 9 ,4 and 9 ,5 and 3 can be misunderstood by the machine which is very obvious considering the examples shown below","081ad0b2":"Hello guys this is my first kaggle notebook and  consider myself to be a complete beginner in data science an machine learning\n\n\n\nI have stepped into image classification with the very famous **MNIST dataset**,this is a multiclass classifation and my model has accuracy **97%**\n\n\n\n\nMost of you all are aware of this dataset, it consists of handwritten digits from 0-9\nand the aim of this model is to predict which digit it corresponds to.\n\n\n\n\nThe dataset is already present in the tensorflow library , so there is no need of uploading the file.\n\n\nSteps involved are:\n\n\n1.Load in the data\n*mnist data\n*10 digits 0-9\n*it is already included in tensorflow\n\n2.Build the model\n*sequential dense layers ending with multiclass logistic regression\n\n3.\n*train the model\n![image.png](attachment:0051cbb8-29e1-46ad-99c4-d498dd381018.png)![image.png](attachment:b9cd36c2-c92b-4784-9c08-40dec396b918.png)\n\n","2bcc43cb":"**PLEASE UPVOTE GUYS AND RECOMMEND THAT SHOULD IMPLEMENT**","6d9b6ebc":"As you know that a main problem that one faces is overfitting which can be resolved using cross validation.","d1e0e65d":"Now the most interesting step comes which is bulding the model I used the softmax activation function to predict the output because it predicts a multinomial probability distribution ,it also maps the output such that the sum of all outputs is 1, which is very obvious as we have to predict the probability.![image.png](attachment:a4f85ce8-de46-43ed-9d36-fb830f833cd4.png)![image.png](attachment:fb4b5e2f-2976-4a7a-ae5d-31e05a6e4683.png)\n","46dbedf5":"If you think why the flattening of the dataset is being done just notice that the mnist data is of the form (28*28*w) therefore i tried to flatten it using flatten() to make it one vector.","23e2bca4":"You just have to writw tf.keras.datasets.mnist and your data is loaded not only this the data automatically splits into train and test by just using load_data()"}}