{"cell_type":{"25f2135f":"code","9e38a81c":"code","173e0b08":"code","48d9ea48":"code","cc312927":"code","51e2236c":"code","99ce2972":"code","a79bc68b":"code","666b0240":"code","50221379":"markdown","85e7e8e7":"markdown","5c16a3e9":"markdown","7bd82f94":"markdown","65af7f90":"markdown","f0e9614f":"markdown"},"source":{"25f2135f":"import pandas as pd\nimport lightgbm as lgb\nimport numpy as np\nimport matplotlib.pyplot as plt","9e38a81c":"import os\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\ntrain = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv', index_col='Id')\ndataX = train[ train.columns[train.columns.str.contains('Cm$')] ]\ndatay = OrdinalEncoder().fit_transform(train[['Species']]).flatten().astype(int)\ntrainSet = lgb.Dataset(dataX, datay)\n\nparam = {'objective'       : 'multiclass',\n         'metric'          : 'multi_logloss',\n         'num_class'       : train['Species'].nunique(),\n         'num_leaves'      : 50,   # set ridiculously high for demo purpose\n         'min_data_in_leaf': 2,    # set dangerously low for demo purpose\n         'learning_rate'   : .15,\n         'num_boost_round' : 30}\nmodel = lgb.train(param, trainSet)","173e0b08":"def grabdict(tisdict, tree_index, split_index, depth, splits, leaves):\n# recursive function to unravel nested dictionaries\n    depth += 1\n    if 'split_index' in tisdict.keys():\n        tis = tisdict.copy()\n        del tis['left_child']\n        del tis['right_child']\n        tis['tree_index'] = tree_index\n        split_index = tis['split_index']\n        splits = pd.concat([splits, pd.DataFrame(tis, index=[len(splits)])])\n        splits, leaves = grabdict(tisdict['left_child'], tree_index, split_index, depth, splits, leaves)\n        splits, leaves = grabdict(tisdict['right_child'], tree_index, split_index, depth, splits, leaves)\n    else:\n        tis = tisdict.copy()\n        tis['tree_index'] = tree_index\n        tis['split_index'] = split_index\n        tis['depth'] = depth\n        leaves = pd.concat([leaves, pd.DataFrame(tis, index=[len(leaves)])])\n    return splits, leaves\n\ndef grabtrees(model):\n# wrapper function to call grabdict\n    splits, leaves = pd.DataFrame(), pd.DataFrame()\n    tree_info = model.dump_model()['tree_info']\n    for tisdict in tree_info:\n        splits, leaves = grabdict(tisdict['tree_structure'], tisdict['tree_index'], 0, 0, splits, leaves)\n    leaves = leaves.merge(splits, left_on=['tree_index', 'split_index'], right_on=['tree_index', 'split_index'], how='left')\n    return tree_info, leaves\n\ntree_info, leaves = grabtrees(model)\nleaves   # all leaves from all trees","48d9ea48":"# plot the final tree\nlgb.plot_tree(model, tree_index=len(tree_info)-1, figsize=(15, 10))","cc312927":"# find the minimum leaf weight tree in existing model\nmin_leaf_weight = leaves['leaf_weight'].min()\nmin_leaf_weight","51e2236c":"# now set min_sum_hessian_in_leaf to .016, which is lower than the value we got from the previous cell\n# no effect expected\nparam.update({'min_sum_hessian_in_leaf': .016})\nmodel_test = lgb.train(param, trainSet)\n_, leaves_test = grabtrees(model_test)\nmin_leaf_weight = leaves_test['leaf_weight'].min()\nmin_leaf_weight\n# output confirms no change indeed","99ce2972":"# next set min_sum_hessian_in_leaf to .018\n# we do expect some clipping effects\nparam.update({'min_sum_hessian_in_leaf': .018})\nmodel_test = lgb.train(param, trainSet)\ntree_info_test, leaves_test = grabtrees(model_test)\nmin_leaf_weight = leaves_test['leaf_weight'].min()\nmin_leaf_weight\n# output shows that min_leaf_weight is now higher as requested (more conservative)","a79bc68b":"# next, try chopping more aggressively\nparam.update({'min_sum_hessian_in_leaf': .5})\nmodel_test = lgb.train(param, trainSet)\ntree_info_test, leaves_test = grabtrees(model_test)\nmin_leaf_weight = leaves_test['leaf_weight'].min()\nmin_leaf_weight\n# voila, output as expected: this is how parameter *min_sum_hessian_in_leaf* gives us the full handle","666b0240":"# plot and see\n# we expect a tree far smaller than the original\nlgb.plot_tree(model_test, tree_index=len(tree_info_test)-1, figsize=(15, 10))","50221379":"# 4. Sister notebooks: the Leaf-by-leaf series\nDecision trees: a leaf-by-leaf demo\n\nhttps:\/\/www.kaggle.com\/marychin\/decision-trees-a-leaf-by-leaf-demo\n\n**num_leaves** and **min_data_in_leaf**: a LightGBM demo\n\nhttps:\/\/www.kaggle.com\/marychin\/num-leaves-min-data-in-leaf-a-lightgbm-demo\n\nmin_sum_hessian: a LightGBM demo (we are here)\n\nhttps:\/\/www.kaggle.com\/marychin\/min-sum-hessian-a-lightgbm-demo\n\nfeature_importances split vs gain: a demo\n\nhttps:\/\/www.kaggle.com\/marychin\/feature-importances-split-vs-gain-a-demo\n\n# 5. Cheers, Kagglers & Kaggle!\nTogether we democratise learning and skills.","85e7e8e7":"# 2. Toy data & toy model \nBorrowing data from https:\/\/www.kaggle.com\/uciml\/iris.\n*Replace next cell with your own data.*","5c16a3e9":"# 3. min_sum_hessian_in_leaf","7bd82f94":"# 1. Initiation rite\nInvocations we can't go on without.","65af7f90":"# Put trees in Pandas DataFrame\nSkip the next cell unless you are particularly interested! The original notebook where I wrote these[](http:\/\/) functions is available from https:\/\/www.kaggle.com\/marychin\/lightgbm-trees-to-pandas-dataframe.","f0e9614f":"**min_sum_hessian_in_leaf** is a tunable parameter in boosters. This notebook will demo its effect at leaf-by-leaf level. **min_sum_hessian_in_leaf** is also understood by LightGBM as\n* min_sum_hessian_per_leaf;\n* min_sum_hessian;\n* min_hessian;\n* min_child_weight.\n\nThis notebook draws connections between:\n* user-defined parameter: **min_sum_hessian_in_leaf**;\n* attribute from *booster.dump_model()['tree_info']*: **leaf_weight**;\n* *lgb.plot_tree* output."}}