{"cell_type":{"10ded94b":"code","40a76db7":"code","85457a50":"code","e16941d7":"code","8df7ee2f":"code","92d30c4d":"code","60c2ee6d":"code","4f1a0761":"code","e54fed9a":"code","707379b2":"code","4b368929":"code","bb163647":"code","591fb987":"code","0d3112e4":"code","166620e0":"code","7fb7bb93":"code","2c32e208":"markdown","6e921ae7":"markdown","7fc7d43b":"markdown","f6b394b2":"markdown","baccd159":"markdown","cdca384c":"markdown"},"source":{"10ded94b":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression","40a76db7":"data = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","85457a50":"data.head()","e16941d7":"X = data.drop(['id', 'diagnosis', 'Unnamed: 32'], axis = 1)\n\ny = data['diagnosis']","8df7ee2f":"# X.isnull().sum()","92d30c4d":"X.shape, y.shape","60c2ee6d":"# lr = LogisticRegression()\nlr = LogisticRegression(max_iter=5000)","4f1a0761":"lr.fit(X,y)","e54fed9a":"lr.score(X,y)","707379b2":"params = [    \n    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],   # Used to specify the norm used in the penalization.\n    'C' : np.logspace(-4, 4, 20),                      # Inverse of regularization strength; must be a positive float.\n    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],  # Algorithm to use in the optimization problem.\n    'max_iter' : [100, 1000,2500, 5000]                # Maximum number of iterations taken for the solvers to converge.\n    }\n]\n\n# There are many other parameters that we could use... but for nw will start with this.","4b368929":"from sklearn.model_selection import GridSearchCV","bb163647":"clf = GridSearchCV(estimator = lr, param_grid = params, scoring = 'accuracy', cv = 3, verbose=True, n_jobs=-1)\n# cv --> Determines the cross-validation splitting strategy\n# verbose --> Controls the verbosity. Verbose is a general programming term for produce lots of logging output. You can think of it as asking the program to \"tell me everything about what you are doing all the time\". \n# n_jobs --> Number of jobs to run in parallel. `-1` means using all processors. ","591fb987":"clf_fit = clf.fit(X,y)","0d3112e4":"# Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data.\nclf_fit.best_estimator_","166620e0":"clf_fit.score(X,y)\n# Returns the score on the given data.\n# This uses the score defined by scoring where provided, and the best_estimator_.score method otherwise.","7fb7bb93":"# Mean cross-validated score of the best_estimator\nclf_fit.best_score_","2c32e208":"# Build Logistic Regression with Hyperparameter\nNow lets build the Logistic Regression model with Hyperparameter, and will be using GridSearchCV to achive this.","6e921ae7":"Defining the hyper-parameters.\n\nThe details on these parameters can be checked from https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html ","7fc7d43b":"As we will be using GridSearchCV we have to import it first.\n\nFor more details on GridSearchCV, refer https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV","f6b394b2":"# Check Accuracy","baccd159":"# Build Logistic Regression\nLets first Build Logistic Regression Model.","cdca384c":"So we have seen that the Logistic Regression has resulted as ~95% but with the Hyper-Parameter it has scored as ~98%."}}