{"cell_type":{"a51d6474":"code","867b4172":"code","049b40b3":"code","fd9d6587":"code","2386503d":"code","3f71d56a":"code","f744ff9a":"code","e32e90ba":"code","4313c05f":"code","ef4763c5":"code","6697dab5":"code","3a6344e5":"code","ba1e2ebf":"code","4f2deac7":"code","c2e297a1":"code","869e910f":"code","47fc6f83":"code","6069ef80":"code","06d50c65":"code","58e28d83":"code","0d00f544":"code","588237d1":"code","79c45793":"code","04dc18a2":"code","73f33d8a":"code","a17116dd":"code","c79bcf15":"code","0aacd8e1":"code","8151aed2":"code","82781e1a":"code","d1dbb8cb":"code","46c46532":"markdown","c999a830":"markdown","6cc27d70":"markdown","a7e36100":"markdown","977f92c8":"markdown","f64c3781":"markdown"},"source":{"a51d6474":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","867b4172":"#IMPORTED STUFF\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n%matplotlib inline","049b40b3":"#IMPORTED ML ALGORITHMS\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#IMPORTING MAE\nfrom sklearn.metrics import mean_absolute_error","fd9d6587":"#Reading the Data\ndf = pd.read_csv('..\/input\/passenger-list-for-the-estonia-ferry-disaster\/estonia-passenger-list.csv')","2386503d":"#Basic Data Exploration\n\n#Checking first few rows of data\ndf.head()","3f71d56a":"#We don't require the columns PassengerId, Firstname and Lastname so we drop those columns.\ndf = df.drop(['PassengerId', 'Firstname', 'Lastname'], axis=1)","f744ff9a":"df.shape","e32e90ba":"#Checking Data Types and Null Counts\ndf.info()","4313c05f":"#Checking how many null values we have\ndf.isnull().sum()","ef4763c5":"df.describe()","6697dab5":"#Checking Value Counts\n\n#Country Value Counts\ndf.Country.value_counts()","3a6344e5":"#Sex Value Counts\ndf.Sex.value_counts()","ba1e2ebf":"#Category Value Counts\ndf.Category.value_counts()","4f2deac7":"#Plotting Histogram For Age\nplt.hist(df.Age)\nplt.title(\"Age\")\nplt.show()","c2e297a1":"#How does Country affect survival\nsns.set(rc={'figure.figsize':(10,8.27)})\nsns.countplot(x = df['Survived'] , hue = 'Country', data = df)","869e910f":"#How does Sex affect Survival\nsns.countplot(x = df['Survived'], hue = 'Sex' , data = df)","47fc6f83":"#How does Category affect survival\nsns.countplot(x = df['Survived'], hue = 'Category', data = df)","6069ef80":"#Converting the categorical (Country, Sex, Category) data into numeric data\nencoder = LabelEncoder()\nfor i in list(df.columns):\n    if df[i].dtype=='object':\n        df[i]=encoder.fit_transform(df[i])","06d50c65":"df.head()","58e28d83":"#Setting Target Variables and Independant Variables\nX = df.loc[:, ['Country', 'Sex', 'Age', 'Category']]\ny = df.loc[:, 'Survived']","0d00f544":"#Splitting data into training data and testing data\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.2)","588237d1":"#Test Using Random Forest Classifier\n#With help of https:\/\/www.kaggle.com\/nelsonmalgro\/newbie-titanic\nmodel = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=1)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)","79c45793":"print(\"Accuracy:\", model.score(X_train, y_train))\nprint(\"Mean Absolute Error:\", mean_absolute_error(y_test, preds))","04dc18a2":"#Test Using Logistic Regression\nmodel = LogisticRegression(random_state = 1)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)","73f33d8a":"print(\"Accuracy:\", model.score(X_train, y_train))\nprint(\"Mean Absolute Error:\", mean_absolute_error(y_test, preds))","a17116dd":"#Test Using K Neighbours Classifier\nmodel = KNeighborsClassifier(n_neighbors = 5)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)","c79bcf15":"print(\"Accuracy:\", model.score(X_train, y_train))\nprint(\"Mean Absolute Error:\", mean_absolute_error(y_test, preds))","0aacd8e1":"#Test Using Decision Trees\nmodel = DecisionTreeClassifier(random_state=1)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)","8151aed2":"print(\"Accuracy:\", model.score(X_train, y_train))\nprint(\"Mean Absolute Error:\", mean_absolute_error(y_test, preds))","82781e1a":"#Test Using Gradient Boosting Classifier\nmodel = GradientBoostingClassifier(learning_rate = 0.0001)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)","d1dbb8cb":"print(\"Accuracy:\", model.score(X_train, y_train))\nprint(\"Mean Absolute Error:\", mean_absolute_error(y_test, preds))","46c46532":"M = Male, \nF = Female","c999a830":"There are no null values in this dataset.","6cc27d70":"P = Passenger, C = Crew","a7e36100":"This is one of my first ML programs so feedback would be appreciated, I'm always looking to improve :)","977f92c8":"# Basic Data Visualisation ","f64c3781":"#### Algorithms Used: \n##### Random Forest Classifier (Acc ~88%)\n##### Logistic Regression (Acc ~86%)\n##### K Neighbours Classifier (Acc ~87%)\n##### Decision Trees Classifier (Acc ~92%)\n##### Gradient Boosting Classifier (Acc ~86%)\n\nDecision Trees gave the best accuracy score of around 92%"}}