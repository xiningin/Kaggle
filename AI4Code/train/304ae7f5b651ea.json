{"cell_type":{"5395cf4b":"code","8625208b":"code","996cdd3f":"code","7d7dc455":"code","14cfd3f6":"code","d924a649":"code","ba526581":"code","ecd38271":"code","9120655d":"code","918ff45b":"code","1ea0e185":"code","bd8599fe":"code","b92420a9":"code","bea1cb17":"code","a1d686e1":"code","cac5a219":"code","1f7bb6b7":"code","3d9126fd":"code","05724ff2":"code","75e4e3aa":"code","742b2b85":"code","a069e16d":"code","45a89fa2":"code","236d1816":"code","d8ce4cde":"code","c97f95dc":"code","3d00d48a":"code","542330a8":"code","2abeb306":"code","1ebd1eec":"code","7d1ce790":"code","ce297f7f":"code","1e16fb6a":"code","0b4ee17f":"code","17e0095d":"code","67c33ffd":"code","d5ad4f36":"code","3c7ad736":"code","1259985a":"code","bc136601":"code","dcad9b0e":"code","8fe9195b":"code","735d75ca":"code","c08158b9":"code","bcc89f01":"code","1e58b60a":"code","bf1f18bd":"code","edb050fa":"code","147e7ee6":"code","b731d00a":"code","123b2692":"code","6c534f9b":"code","0bf79cd9":"markdown","162012a3":"markdown","155b693f":"markdown","f60d2d71":"markdown","87424413":"markdown","27e687d4":"markdown","3d127fa4":"markdown","177f0715":"markdown","96f84bf6":"markdown","939511c8":"markdown","fd27aeb7":"markdown","eef0575b":"markdown","fdaa0ede":"markdown","67b77483":"markdown","3966a61a":"markdown","3a0e2a2e":"markdown","b0845f61":"markdown","c4447a3a":"markdown","6cca7b97":"markdown","4f9ecdf9":"markdown","f62ce31f":"markdown","d6db0d9d":"markdown","bd021880":"markdown","bf74c225":"markdown","0691388d":"markdown","e7f850fe":"markdown","861402f2":"markdown","fb597169":"markdown","acc41145":"markdown","87ca2a14":"markdown","48b9d56a":"markdown","1bfe0981":"markdown","c4d4e40a":"markdown","93c58f82":"markdown","9871948a":"markdown","fb2ad00b":"markdown","ad4fbaa9":"markdown"},"source":{"5395cf4b":"!nvidia-smi","8625208b":"import tensorflow as tf\n\ndevice_name = tf.test.gpu_device_name()\nif device_name == '\/device:GPU:0':\n    print('Found GPU at: {}'.format(device_name))\nelse:\n    raise SystemError('GPU device not found')","996cdd3f":"import torch\n\nif torch.cuda.is_available():    \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","7d7dc455":"!pip install transformers","14cfd3f6":"import pandas as pd\n\ndf = pd.read_csv(\"train.csv\")\nprint('Number of training sentences: ', len(df))\ndf.sample(5)","d924a649":"# Print some negative sample tweets\nfor txt in df[df.target==0].text.sample(5).values:\n  print(txt)","ba526581":"# Print some positive sample tweets\nfor txt in df[df.target==1].text.sample(5).values:\n  print(txt)","ecd38271":"df.text.isna().sum()","9120655d":"print(\"Positive data: {:.2f}%\".format(len(df[df.target==1])*100\/len(df)))","918ff45b":"tweets = df.text.values\nlabels = df.target.values","1ea0e185":"print(\"{} out of {} tweets have a http:\/\/... link within itself. ({:.2f}%)\".format(len([t for t in tweets if \"http:\/\/\" in t]), len(df), len([t for t in tweets if \"http:\/\/\" in t])*100\/len(df)))","bd8599fe":"[t for t in tweets if \"http:\/\/\" in t][:5]","b92420a9":"# Print some tweets with URL that does NOT have URL at the end\n[t for t in [t for t in tweets if \"http:\/\/\" in t] if \"http:\/\/\" not in t.split()[-1]][:5]","bea1cb17":"print(\"percentage of POSITIVE samples containing http URLs at the end: {:.2f}%\".format(len([t for t in df[df['target']==1]['text'] if \"http:\/\/\" in t])*100\/len(df[df['target']==1])))\nprint(\"percentage of NEGATIVE samples containing http URLs at the end: {:.2f}%\".format(len([t for t in df[df['target']==0]['text'] if \"http:\/\/\" in t])*100\/len(df[df['target']==0])))","a1d686e1":"print(\"{} out of {} tweets have a @user_id tag within itself. ({:.2f}%)\".format(len([t for t in tweets if \"@\" in t]), len(df), len([t for t in tweets if \"@\" in t])*100\/len(df)))","cac5a219":"[t for t in tweets if \"@\" in t][:5]","1f7bb6b7":"print(\"percentage of POSITIVE samples containing @user_id tag: {:.2f}%\".format(len([t for t in df[df['target']==1]['text'] if \"@\" in t])*100\/len(df[df['target']==1])))\nprint(\"percentage of NEGATIVE samples containing @user_id tag: {:.2f}%\".format(len([t for t in df[df['target']==0]['text'] if \"@\" in t])*100\/len(df[df['target']==0])))","3d9126fd":"print(\"{} out of {} tweets have a # tag within itself. ({:.2f}%)\".format(len([t for t in tweets if \"#\" in t]), len(df), len([t for t in tweets if \"#\" in t])*100\/len(df)))","05724ff2":"print(\"percentage of POSITIVE samples containing # tag: {:.2f}%\".format(len([t for t in df[df['target']==1]['text'] if \"#\" in t])*100\/len(df[df['target']==1])))\nprint(\"percentage of NEGATIVE samples containing # tag: {:.2f}%\".format(len([t for t in df[df['target']==0]['text'] if \"#\" in t])*100\/len(df[df['target']==0])))","75e4e3aa":"from transformers import BertTokenizer\n\nprint('Loading BERT tokenizer...')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","742b2b85":"print(' Original: ', tweets[1], labels[1])\nprint('Tokenized: ', tokenizer.tokenize(tweets[1]))\nprint('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweets[1])))","a069e16d":"print(' Original: ', tweets[-1]) # a tweet with http URL\nprint('   Target: ', labels[-1])\nprint('Tokenized: ', tokenizer.tokenize(tweets[-1]))\nprint('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweets[-1])))","45a89fa2":"tweets = [\" \".join([word if 'http:\/\/' not in word else \"http\" for word in t.split()]) for t in tweets]\ntweets[-1]","236d1816":"print(' Original: ', tweets[-4])\nprint('   Target: ', labels[-4])\nprint('Tokenized: ', tokenizer.tokenize(tweets[-4]))","d8ce4cde":"print(' Original: ', tweets[-17])\nprint('   Target: ', labels[-17])\nprint('Tokenized: ', tokenizer.tokenize(tweets[-17]))","c97f95dc":"tokenizer.tokenize(\"Living safely\")","3d00d48a":"tweets = [\" \".join([word if '@' not in word else \"@\" for word in t.split()]) for t in tweets]\ntweets[-4]","542330a8":"import numpy as np\n\nencoded_tweets = [tokenizer.encode(t) for t in tweets]\nlens = np.array([len(t) for t in encoded_tweets])\n\nprint('# of sentences:', len(tweets))\nprint('Max sentence length: ', max(lens))\nprint('Avg sentence length: ', np.mean(lens))\nprint('Median sentence length: ', np.median(lens))","2abeb306":"import matplotlib.pyplot as plt\n\nunique = list(set(lens))\nunique.sort()\ncnt = [sum([1 if l==u else 0 for l in lens]) for u in unique]\nplt.bar(unique, cnt)","1ebd1eec":"# `encode_plus` will:\n#   (1) Tokenize the sentence.\n#   (2) Prepend the `[CLS]` token to the start.\n#   (3) Append the `[SEP]` token to the end.\n#   (4) Map tokens to their IDs.\n#   (5) Pad or truncate the sentence to `max_length`\n#   (6) Create attention masks for [PAD] tokens.\n\ndef encode(sentences, labels, tokenizer, max_len):\n    encoded_dicts = [tokenizer.encode_plus(\n                            sent,                      # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            max_length = max_len,           # Pad & truncate all sentences.\n                            pad_to_max_length = True,\n                            return_attention_mask = True,   # Construct attn. masks.\n                            return_tensors = 'pt',     # Return pytorch tensors.\n                     ) for sent in sentences]\n    input_ids = [d['input_ids'] for d in encoded_dicts]  \n    attention_masks = [d['attention_mask'] for d in encoded_dicts]  \n\n    # Convert the lists into tensors.\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    labels = torch.tensor(labels)\n\n    return input_ids, attention_masks, labels","7d1ce790":"input_ids, attention_masks, labels = encode(tweets, labels, tokenizer, max_len=100)\nprint('Original: ', tweets[0])\nprint('\\nToken IDs:', input_ids[0])","ce297f7f":"print(len(input_ids[0]))\ntokenizer.convert_ids_to_tokens(input_ids[0][:20])","1e16fb6a":"from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n\ndef make_dataloader(input_ids, attention_masks, labels, split=1):  \n    dataset = TensorDataset(input_ids, attention_masks, labels)\n\n    if split:\n        train_size = int(0.9 * len(dataset))\n        val_size = len(dataset) - train_size\n        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n        print('{} training samples'.format(train_size))\n        print('{} validation samples'.format(val_size))\n    else: \n        train_dataset = dataset\n        print(print('{} training samples (no validation)'.format(len(dataset))))\n\n    # For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n    batch_size = 32\n\n    train_dataloader = DataLoader(\n                          train_dataset,  # training samples.\n                          sampler = RandomSampler(train_dataset), # Select batches randomly\n                          batch_size = batch_size # Trains with this batch size.\n                      )\n\n    if split:\n        # For validation the order doesn't matter, so just read them sequentially.\n        validation_dataloader = DataLoader(\n                                    val_dataset, # The validation samples.\n                                    sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n                                    batch_size = batch_size # Evaluate with this batch size.\n                                )\n    \n        return train_dataloader, validation_dataloader\n\n    return train_dataloader","0b4ee17f":"train_dataloader, validation_dataloader = make_dataloader(input_ids, attention_masks, labels)","17e0095d":"from transformers import BertForSequenceClassification, BertConfig\n\n# Load BertForSequenceClassification (pretrained BERT model + a single linear classification layer on top) \nmodel = BertForSequenceClassification.from_pretrained(\n              \"bert-base-uncased\",          # 12-layer BERT base model w\/ uncased vocab\n              num_labels = 2,               # number of output labels (2 for binary classification)  \n              output_attentions = False,    # returns attentions weights?\n              output_hidden_states = False, # return all hidden-states?\n        )\nmodel.cuda()","67c33ffd":"!nvidia-smi","d5ad4f36":"next(model.parameters()).is_cuda","3c7ad736":"from transformers import AdamW \n\n# Note: AdamW is a class from the huggingface library (not pytorch)- 'W'= 'Weight Decay fix\"\noptimizer = AdamW(\n                    model.parameters(),\n                    lr = 5e-5,         # default \n                    eps = 1e-8         # default \n                )","1259985a":"import numpy as np\nimport time, datetime\n\n# Helper functions\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)\n\ndef format_time(elapsed):\n    '''Takes time in seconds and returns a string hh:mm:ss'''\n\n    elapsed_rounded = int(round((elapsed)))  # Round to the nearest second\n    return str(datetime.timedelta(seconds=elapsed_rounded))  # Format as hh:mm:ss\n\ndef set_random_seed(seed):\n    seed_val = 42\n    random.seed(seed_val)\n    np.random.seed(seed_val)\n    torch.manual_seed(seed_val)\n    torch.cuda.manual_seed_all(seed_val)","bc136601":"len(train_dataloader), len(validation_dataloader)","dcad9b0e":"import random\nfrom transformers import get_linear_schedule_with_warmup\n\ndef train_BERT(train_dataloader, validation_dataloader, model, optimizer, n_epochs, output_hidden=0):   \n    set_random_seed(seed=42)  # Set seed to make this reproducible.\n    \n    total_t0 = time.time()   # Measure the total training time for the whole run.\n    training_stats = []   # Store training and valid loss, valid accuracy, and timings.\n    hidden_states = []\n\n    # lr scheduler\n    n_batches_train = len(train_dataloader)\n    scheduler = get_linear_schedule_with_warmup(  optimizer, \n                                                  num_warmup_steps = 0, # Default value in run_glue.py\n                                                  num_training_steps = n_batches_train * n_epochs  )\n\n    for epoch_i in range(n_epochs):   \n        # =================== Training =================== #       \n        t0 = time.time()   \n        total_train_loss, total_train_accuracy = 0, 0\n        model.train()\n\n        for step, batch in enumerate(train_dataloader):\n            input_ids, att_mask, labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n            model.zero_grad()        \n            if output_hidden:\n                loss, logits, h = model(input_ids, \n                                        token_type_ids=None, \n                                        attention_mask=att_mask, \n                                        labels=labels)\n                \n                h = [layer.detach().cpu().numpy() for layer in h]\n                if epoch_i == n_epochs - 1: # store the last epoch's hidden states\n                    hidden_states.append(h[-1]) # only save last layer's h           \n            else:\n                loss, logits = model(input_ids, \n                                    token_type_ids=None, # Not required since training on a SINGLE sentence, not a pair\n                                    attention_mask=att_mask, \n                                    labels=labels)\n                \n            total_train_loss += loss.item()  \n            total_train_accuracy += flat_accuracy(logits.detach().cpu().numpy(), labels.detach().cpu().numpy())\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()      \n            scheduler.step()   # Update learning rate\n        \n        print(\"Epoch: {}\/{}\".format((epoch_i+1), n_epochs),\n              \"  Train loss: {0:.4f}\".format(total_train_loss\/n_batches_train),\n              \"  Train Acc: {0:.4f}\".format(total_train_accuracy\/n_batches_train),\n              \"  ({:})\".format(format_time(time.time() - t0)))\n        \n        training_stats.append( {'epoch':           epoch_i + 1,\n                                'Training Loss':   total_train_loss\/n_batches_train,\n                                'Training Acc' :   total_train_accuracy\/n_batches_train,\n                                'Training Time':   format_time(time.time() - t0)} )\n\n        if validation_dataloader is not None:\n            # =================== Validation =================== #\n            n_batches_valid = len(validation_dataloader)\n            t0 = time.time()\n            model.eval()\n\n            total_eval_accuracy, total_eval_loss = 0, 0\n            for batch in validation_dataloader:\n                v_input_ids, v_att_mask, v_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n                with torch.no_grad(): \n                    if output_hidden:       \n                        loss, logits, val_h = model(v_input_ids, \n                                                    token_type_ids=None, \n                                                    attention_mask=v_att_mask,\n                                                    labels=v_labels)\n                    \n                        val_h = [layer.detach().cpu().numpy() for layer in val_h] # save GPU memory\n                    else:\n                        loss, logits = model(v_input_ids, \n                                             token_type_ids=None, \n                                             attention_mask=v_att_mask,\n                                             labels=v_labels)\n                total_eval_loss += loss.item()\n                logits = logits.detach().cpu().numpy()\n                label_ids = v_labels.detach().cpu().numpy()\n                total_eval_accuracy += flat_accuracy(logits, label_ids)\n\n            print(\"  Valid Loss: {0:.4f}\".format(total_eval_loss\/n_batches_valid),\n                  \"  Valid Acc: {0:.4f}\".format(total_eval_accuracy\/n_batches_valid),\n                  \"  ({:})\".format(format_time(time.time()-t0)))\n\n            training_stats.append( {'            Valid. Loss':     total_eval_loss\/n_batches_valid,\n                                    'Valid. Acc':   total_eval_accuracy\/n_batches_valid,\n                                    'Validation Time': format_time(time.time()-t0)} )\n\n    print(\"\\nTraining complete.\")\n    print(\"Duration: {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n\n    if output_hidden:\n        return training_stats, hidden_states\n    else:\n        return training_stats","8fe9195b":"training_stats = train_BERT(train_dataloader, validation_dataloader, \n                            model=model, optimizer=optimizer, \n                            n_epochs=2)","735d75ca":"train_dataloader = make_dataloader(input_ids, attention_masks, labels, split=0)","c08158b9":"model = BertForSequenceClassification.from_pretrained(\n              \"bert-base-uncased\",          # 12-layer BERT base model, w\/ uncased vocab\n              num_labels = 2,               # number of output labels (2 for binary classification)  \n              output_attentions = False,    # Whether the model returns attentions weights.\n              output_hidden_states = False, # Whether the model returns all hidden-states.\n        )\nmodel.cuda()","bcc89f01":"optimizer = AdamW(  model.parameters(), lr = 5e-5, eps = 1e-8)","1e58b60a":"training_stats = train_BERT(train_dataloader, None,\n                            model=model, optimizer=optimizer, \n                            n_epochs=2)","bf1f18bd":"test_df = pd.read_csv(\"test.csv\")\ntest_sentences = test_df.text.values\ntest_sentences = [\" \".join([word if 'http:\/\/' not in word else \"http\" for word in t.split()]) for t in test_sentences]\ntest_sentences = [\" \".join([word for word in t.split() if '@' not in word]) for t in test_sentences]\ntest_encoded_sentences = [tokenizer.encode(s) for s in test_sentences]\ntest_sent_lens = np.array([len(s) for s in test_encoded_sentences])\n\nprint('# of sentences:', len(test_sentences))\nprint('Max sentence length: ', max(test_sent_lens))\nprint('Avg sentence length: ', np.mean(test_sent_lens))\nprint('Median sentence length: ', np.median(test_sent_lens))","edb050fa":"encoded_dicts = [tokenizer.encode_plus(  sent,                      \n                                         add_special_tokens = True, \n                                         max_length = 100,          \n                                         pad_to_max_length = True,\n                                         return_attention_mask = True,   \n                                         return_tensors = 'pt'  ) for sent in test_sentences]\ninput_ids = [d['input_ids'] for d in encoded_dicts]  \ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = [d['attention_mask'] for d in encoded_dicts]  \nattention_masks = torch.cat(attention_masks, dim=0)\n\nprediction_data = TensorDataset(input_ids, attention_masks)\nprediction_dataloader = DataLoader(dataset = prediction_data, \n                                   sampler = SequentialSampler(prediction_data), # doesn't need to be sampled randomly\n                                   batch_size = 32)","147e7ee6":"len(prediction_dataloader)","b731d00a":"print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\nmodel.eval()\npredictions, true_labels = [], []\nfor batch in prediction_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask = batch\n    with torch.no_grad():\n        logits = model(  b_input_ids, \n                         token_type_ids=None, \n                         attention_mask=b_input_mask  ) # no loss, since \"labels\" not provided\n\n    logits = logits[0].detach().cpu().numpy() # extract x from [[x]]\n    predictions.append(logits)\nprint('    DONE.')","123b2692":"flat_predictions = np.concatenate(predictions, axis=0)\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()","6c534f9b":"submission = pd.DataFrame(test_df.id)\nsubmission['target'] = flat_predictions\nsubmission.to_csv('submission_6_23_17_10.csv', index=False)","0bf79cd9":"# 3. Tokenize\n","162012a3":"## 1.1. Check for Colab GPU \n","155b693f":"# 1. Setup","f60d2d71":"There is no empty cell for text.","87424413":"## 2.3 @user_id tags in tweets","27e687d4":"Let's see if having id tag(s) makes the tweet more probable to be a disaster tweet (target=1).","3d127fa4":"## 3.4 Tokenize Dataset","177f0715":"## 4.4 Train again w\/ ENTIRE data (no validation)","96f84bf6":"# 2. Load Disaster Tweets Dataset (from Kaggle)\n","939511c8":"Let's see if having a URL makes the tweet more probable to be a disaster tweet (target=1).","fd27aeb7":"## 5.1 Test Data Preparation\n","eef0575b":"Ok, seems like positive samples have higher probablity of having a URL. Maybe it's because to share\/announce a disaster, one might share a news\/youtube link as a source. ","fdaa0ede":"## 4.2. Define Optimizer & Learning Rate Scheduler","67b77483":"Now that we observed that 2 epochs are enough in order to prevent overfitting, let's train w\/ the entire dataset.","3966a61a":"## 3.3 @user_id mentions","3a0e2a2e":"# 4. Train","b0845f61":"## 5.2 Predict Test Set\n","c4447a3a":"## 2.3 Hashtags in tweets","6cca7b97":"## BERT for Disaster Tweets Classification (Kaggle)\n\nby Luky","4f9ecdf9":"## 3.5 Build Train & Validation DatatLoaders\n","f62ce31f":"\nWe'll need to apply all of the same steps that we did for the training data to prepare our test data set.","d6db0d9d":"## 1.2. Install transformers Library\n","bd021880":"# 5 Predict & Submit","bf74c225":"Let's keep \"@\" token only.","0691388d":"## 3.2 http:\/\/ URLs","e7f850fe":"## 5.3 Make Submission File","861402f2":"Ok, we have a pretty balanced dataset.","fb597169":"## 4.3. Train","acc41145":"Random alphabetical tokens from URLs are useless. We shold just keep \"http\" token.","87ca2a14":"For fine-tuning, the authors recommend choosing from following values (from Appendix A.3 of the [BERT paper](https:\/\/arxiv.org\/pdf\/1810.04805.pdf)):\n\n>- **Batch size:** 16, **32**  \n- **Learning rate (Adam):** 5e-5, 3e-5, **2e-5**\n- **Number of epochs:** **2**, 3, 4 \n\n*The epsilon parameter `eps = 1e-8` is \"a very small number to prevent any division by zero in the implementation\" (from [here](https:\/\/machinelearningmastery.com\/adam-optimization-algorithm-for-deep-learning\/)).","48b9d56a":"## 2.2  http:\/\/... URLs in tweets","1bfe0981":"## 2.1 Load csv file","c4d4e40a":"Looks like many of positive tweets are coming from a more formal source.","93c58f82":"## 4.1. Define Model: BertForSequenceClassification","9871948a":"## 3.1. BERT Tokenizer","fb2ad00b":"Just in case there are some longer test sentences, I'll set the maximum length to 100.\n","ad4fbaa9":"Even with user_id w\/ common nouns like above instead of proper nouns like names, resulting tokens seem not reasonable due to spacing."}}