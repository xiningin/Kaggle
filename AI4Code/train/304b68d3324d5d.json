{"cell_type":{"7399ea2a":"code","bdd22cff":"code","0ec6390b":"code","c21eba01":"code","918caee2":"code","d94c1499":"code","bef3d5f6":"code","690d30a8":"code","6f6fff14":"code","d070b776":"code","2ce52d4b":"code","10319b9e":"code","963cb090":"code","6a5c8b2b":"code","75149b40":"code","b7626b9e":"code","7bf178b9":"code","e30bb302":"code","7daf4357":"code","a50e5a38":"code","b6f755d2":"code","786922fe":"code","87e3df14":"code","d961b15c":"code","64ccf4c4":"code","9bf6fc6f":"markdown","b91f90a2":"markdown","26cbc49d":"markdown","e671af49":"markdown","6b724a8d":"markdown","0cf07876":"markdown","86287a54":"markdown","45f1742b":"markdown","80560bf7":"markdown","e493d50b":"markdown","50e6fb1b":"markdown","9e5349d0":"markdown"},"source":{"7399ea2a":"!pip install tensorflow_hub\n!pip install bert-for-tf2\n!pip install tensorflow\n!pip install sentencepiece\n!pip install transformers\n!pip install torchvision ","bdd22cff":"try:\n    %tensorflow_version 2.x\nexcept Exception:\n    pass\nimport tensorflow as tf\nimport torch\nimport transformers as ppb # pytorch transformers\nfrom sklearn.linear_model import LogisticRegression\nimport tensorflow_hub as hub\nimport bert\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer, TFBertModel, BertConfig\nfrom transformers import DistilBertModel,DistilBertTokenizer\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tensorflow.keras.layers import Dense, Input\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras import backend as K\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import train_test_split\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0ec6390b":"#model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\nfrom transformers import AutoModel, AutoTokenizer\n\n# Load pretrained model\/tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"dbmdz\/distilbert-base-turkish-cased\")\nmodel = AutoModel.from_pretrained(\"dbmdz\/distilbert-base-turkish-cased\")","c21eba01":"dataset = pd.read_csv(\"\/kaggle\/input\/turkish-sentiment-analysis-data-beyazperdecom\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/turkish-sentiment-analysis-data-beyazperdecom\/test.csv\")\ndataset.info()","918caee2":"#number of labels\nlen(set(dataset[\"Label\"].values))","d94c1499":"import re\nTAG_RE = re.compile(r'<[^>]+>')\n\ndef remove_tags(text):\n    return TAG_RE.sub('', text)\n\ndef preprocess_text(sen):\n    # Removing html tags\n    sentence = remove_tags(sen)\n\n    # Remove punctuations and numbers\n    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n\n    # Single character removal\n    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n\n    # Removing newlines\n    sentence = re.sub(r'\\n', ' ', sentence)\n    \n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n\n    return sentence\ndataset[\"comment\"] = dataset[\"comment\"].apply(lambda x : preprocess_text(str(x)))\ndataset[\"comment\"] = dataset[\"comment\"].apply(lambda x : ' '.join([w.strip() for w in x.split() if w!=' ']))\n\ntest[\"comment\"] = test[\"comment\"].apply(lambda x : preprocess_text(str(x)))\ntest[\"comment\"] = test[\"comment\"].apply(lambda x : ' '.join([w.strip() for w in x.split() if w!=' ']))","bef3d5f6":"def summarize_model(history):\n    pyplot.subplot(211)\n    pyplot.title('Loss')\n    pyplot.plot(history.history['loss'], label='train')\n    pyplot.plot(history.history['val_loss'], label='test')\n    pyplot.legend()\n    # plot accuracy during training\n    pyplot.subplot(212)\n    pyplot.title('Accuracy')\n    pyplot.plot(history.history['accuracy'], label='train')\n    pyplot.plot(history.history['val_accuracy'], label='test')\n    pyplot.legend()\n    pyplot.show()","690d30a8":"train_x,eval_x,train_y,eval_y = train_test_split(dataset[\"comment\"],dataset[\"Label\"],test_size=0.35)","6f6fff14":"print(\"length of train set:\",len(train_x))\nprint(\"length of validate set:\",len(eval_x))\nprint(\"length of test set:\",len(test))","d070b776":"tokenized_tr=tokenizer.batch_encode_plus(train_x.tolist(),max_length =100,return_attention_mask=True,padding=True)","2ce52d4b":"tokenized_vl=tokenizer.batch_encode_plus(eval_x.tolist(),max_length =100,return_attention_mask=True,padding=True)","10319b9e":"tokenized_ts=tokenizer.batch_encode_plus(test[\"comment\"].tolist(),max_length =100,return_attention_mask=True,padding=True)","963cb090":"def get_features(padded,attention):\n    input_ids = torch.tensor(padded,dtype=torch.int64)  \n    attention_mask = torch.tensor(attention,dtype=torch.int64)\n    with torch.no_grad():\n        last_hidden_states = model(input_ids,attention_mask)\n    features = last_hidden_states[0][:,0,:].numpy()\n    return features","6a5c8b2b":"features_tr = get_features(tokenized_tr['input_ids'],tokenized_tr['attention_mask'])","75149b40":"features_vl = get_features(tokenized_vl['input_ids'],tokenized_vl['attention_mask'])","b7626b9e":"features_ts = get_features(tokenized_ts['input_ids'],tokenized_ts['attention_mask'])","7bf178b9":"#logistic regression\nfrom sklearn.linear_model import LogisticRegression\nlr_clf = LogisticRegression()\nlr_clf.fit(features_tr, train_y)\nprint('train score:',lr_clf.score(features_tr, train_y))\nprint('validation score:',lr_clf.score(features_vl, eval_y))\nprint('test score:',lr_clf.score(features_ts, test['Label']))","e30bb302":"#naive bayes\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(features_tr, train_y)\nprint('train score:',gnb.score(features_tr, train_y))\nprint('validation score:',gnb.score(features_vl, eval_y))\nprint('test score:',gnb.score(features_ts, test['Label']))","7daf4357":"from tensorflow.keras.layers import Input,concatenate,Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf \n\ninput1 = Input(shape=(features_tr.shape[1],))\ndense1 = Dense(128,activation='relu')(input1)\ndense2 = Dense(1,activation='sigmoid')(dense1)\ntfmodel = Model(inputs=input1,outputs=dense2)","a50e5a38":"tf.keras.utils.plot_model(tfmodel, show_shapes=True, dpi=48)","b6f755d2":"loss = tf.keras.losses.BinaryCrossentropy (from_logits=False)\noptimizer = keras.optimizers.Adam(lr=1e-3,decay=1e-3\/32)\ntfmodel.compile(optimizer=optimizer, loss=[loss, loss],metrics=[\"accuracy\"])\ncheckpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True)\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy')\nfine_history = tfmodel.fit(features_tr, train_y, validation_data=(features_vl,eval_y),\n                          epochs=3, callbacks=[checkpoint, earlystopping],batch_size=32,verbose=1)","786922fe":"summarize_model(fine_history)","87e3df14":"#training set\ny_pred=tfmodel.predict(features_tr)\ny_pred = y_pred.round()\nprint(classification_report(train_y,y_pred))","d961b15c":"#validation set\ny_pred=tfmodel.predict(features_vl)\ny_pred = y_pred.round()\nprint(classification_report(eval_y,y_pred))","64ccf4c4":"#testing set\ny_pred=tfmodel.predict(features_ts)\ny_pred = y_pred.round()\nprint(classification_report(test['Label'],y_pred))","9bf6fc6f":"# Import Needed Libraries","b91f90a2":"# Helper Functions","26cbc49d":"# Data Statistics","e671af49":"# load Bert tokenizer","6b724a8d":"# Load Dataset","0cf07876":"**Evaluation**","86287a54":"# Build Model","45f1742b":"# Prepare Data for Bert ","80560bf7":"**Traditional Machine Learning Algorithms**","e493d50b":"# Install Needed libraries","50e6fb1b":"Transfer learning is when a model developed for one task is reused to work on a second task. \nFine tuning is one approach to transfer learning.\n\nBERT (Bidirectional Encoder Representations from Transformers) is a big neural network architecture, with a huge number of parameters, that can range from 100 million to over 300 million. So, training a BERT model from scratch on a small dataset would result in overfitting.\n\nSo, it is better to use a pre-trained BERT model that was trained on a huge dataset, as a starting point. We can then further train the model on our relatively smaller dataset and this process is known as model fine-tuning.\n\u201cBERT stands for Bidirectional Encoder Representations from Transformers. It is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of NLP tasks.\u201d\nDifferent Fine-Tuning Techniques\nTrain the entire architecture \u2013 We can further train the entire pre-trained model on our dataset and feed the output to a softmax layer. In this case, the error is back-propagated through the entire architecture and the pre-trained weights of the model are updated based on the new dataset.\nTrain some layers while freezing others \u2013 Another way to use a pre-trained model is to train it partially. What we can do is keep the weights of initial layers of the model frozen while we retrain only the higher layers. We can try and test as to how many layers to be frozen and how many to be trained.\nFreeze the entire architecture \u2013 We can even freeze all the layers of the model and attach a few neural network layers of our own and train this new model. Note that the weights of only the attached layers will be updated during model training.\n","9e5349d0":"**Deep Learning Models**"}}