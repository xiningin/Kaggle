{"cell_type":{"9ec2c8d0":"code","e816fcfe":"code","d61d45ec":"code","f9f3d1a5":"code","7c0c996d":"code","660b18fe":"code","64b75c8b":"code","0c4b9185":"code","6538d510":"code","a4796c6d":"code","090afa21":"code","717a3cba":"code","3c5ae8ad":"markdown","329d4c1a":"markdown","be81d6a9":"markdown","6d4723da":"markdown","ce9e729b":"markdown","adee8d6e":"markdown","7c970f4e":"markdown","87c410aa":"markdown"},"source":{"9ec2c8d0":"!pip install --upgrade transformers","e816fcfe":"import transformers\nprint(transformers.__version__)","d61d45ec":"import librosa\nimport torch\nimport IPython.display as display\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\nimport numpy as np","f9f3d1a5":"#load pre-trained model and tokenizer\ntokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook\/wav2vec2-base-960h\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook\/wav2vec2-base-960h\")","7c0c996d":"#load audio file \naudio, sampling_rate = librosa.load(\"..\/input\/audio-dataset\/Voice 002.m4a\",sr=16000)","660b18fe":"audio,sampling_rate","64b75c8b":"# audio\ndisplay.Audio(\"..\/input\/audio-dataset\/Voice 002.m4a\", autoplay=True)","0c4b9185":"input_values = tokenizer(audio, return_tensors = 'pt').input_values\ninput_values","6538d510":"# store logits (non-normalized predictions)\nlogits = model(input_values).logits\nlogits","a4796c6d":"# store predicted id's\n# pass the logit values to softmax to get the predicted values\npredicted_ids = torch.argmax(logits, dim =-1)","090afa21":"# pass the prediction to the tokenzer decode to get the transcription\ntranscriptions = tokenizer.decode(predicted_ids[0])","717a3cba":"transcriptions","3c5ae8ad":"### Load pre-trained Wav2Vec model","329d4c1a":"First of all tokenize the input values,take the maximum prediction from the logit and then extraxt the text","be81d6a9":"In this notebook we are going to see how to convert speech into text using Facebook Wav2Vec 2.0 model.Wav2Vec2 is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded using Wav2Vec2Tokenizer.For learning more about it click on this [link](https:\/\/huggingface.co\/transformers\/model_doc\/wav2vec2.html)","6d4723da":"### Import Libraries","ce9e729b":"# Play the Audio","adee8d6e":"### Speech to Text","7c970f4e":"If you don't see at least 4.3.0 version,then upgrade it","87c410aa":"### Load Audio file"}}