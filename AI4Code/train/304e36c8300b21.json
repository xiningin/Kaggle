{"cell_type":{"a848a102":"code","bda4aad7":"code","a89dc430":"code","1e61ab30":"code","52d3e821":"code","699c3e72":"code","7904093e":"code","a3180c0e":"code","724bdbf4":"code","ba005b64":"code","9d286f71":"code","cd0835e0":"code","46860358":"code","727592e5":"code","a1bab033":"code","abd7a5e3":"code","3617cd85":"code","b9844eb0":"code","17d1a1ca":"code","8eaa22c1":"code","73dfb4c9":"code","21868328":"code","158f9170":"code","3d5e1ef9":"code","66b18025":"code","7f3ce8e5":"code","422a5a11":"code","d34f16be":"markdown","57804156":"markdown","0719b3ff":"markdown","4c35ec76":"markdown","87b84e57":"markdown","9fa9c2de":"markdown","957184d7":"markdown","5ad5141e":"markdown","ec597eaf":"markdown","a60fc8ee":"markdown","bd822d9f":"markdown","165da80f":"markdown","1e67671c":"markdown","b555a670":"markdown","84fa472b":"markdown","b7378b0c":"markdown","c506129f":"markdown","463cb55f":"markdown","7357e310":"markdown","cd7d01af":"markdown","43744899":"markdown","36c0d91a":"markdown","83bc2249":"markdown"},"source":{"a848a102":"import pandas as pd, numpy as np\nimport os\nimport math\nfrom math import ceil, floor, log\nimport random\n\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, classification_report \nfrom sklearn.model_selection import train_test_split\nimport sklearn\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport seaborn as sns\n\nfrom yellowbrick.classifier import ClassificationReport\nimport scikitplot as skplt\n\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\nimport catboost\nprint(catboost.__version__)\nfrom catboost import *\nfrom catboost import datasets\nfrom catboost import CatBoostClassifier\n\nimport scikitplot as skplt","bda4aad7":"SEED = 1970\nrandom.seed(SEED)\n\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\npath = '..\/input\/health-insurance-cross-sell-prediction\/'","a89dc430":"df_train = pd.read_csv(path + \"train.csv\")\ndf_test = pd.read_csv(path + \"test.csv\")\nprint(df_train.isnull().sum())\nprint(df_test.isnull().sum())","1e61ab30":"col_list = df_train.columns.to_list()[1:]\ndf_train_corr = df_train.copy().set_index('id')\ndf_train_ones = df_train_corr.loc[df_train_corr.Response == 1].copy()\n\ncategorical_features = ['Gender', 'Driving_License', 'Region_Code', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage','Policy_Sales_Channel']\ntext_features = ['Gender', 'Vehicle_Age', 'Vehicle_Damage']\n\n# code text categorical features\nle = preprocessing.LabelEncoder()\nfor f in text_features :\n    df_train_corr[f] = le.fit_transform(df_train_corr[f])\n# change digital categorical datatype so CatBoost can deal with them\ndf_train_corr.Region_Code = df_train_corr.Region_Code.astype('int32')\ndf_train_corr.Policy_Sales_Channel = df_train_corr.Policy_Sales_Channel.astype('int32')","52d3e821":"corr = df_train_corr.loc[:,:'Vintage'].corr()\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","699c3e72":"plt.figure(figsize = (8, 8))\nsns.scatterplot(df_train_corr['Policy_Sales_Channel'],df_train_corr['age_bin_cat'])\nplt.title('Binned Age vs Policy_Sales_Channel', fontsize = 15)\nplt.show()","7904093e":"def plot_ROC(fpr, tpr, m_name):\n    roc_auc = sklearn.metrics.auc(fpr, tpr)\n    plt.figure(figsize=(6, 6))\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc, alpha=0.5)\n    \n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--', alpha=0.5)\n    \n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.grid(True)\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.title('Receiver operating characteristic for %s'%m_name, fontsize=20)\n    plt.legend(loc=\"lower right\", fontsize=16)\n    plt.show()","a3180c0e":"def upsample(df, u_feature, n_upsampling):\n    ones = df.copy()\n    for n in range(n_upsampling):\n        if u_feature == 'Annual_Premium':\n            df[u_feature] = ones[u_feature].apply(lambda x: x + random.randint(-1,1)* x *0.05) # change Annual_premiun in the range of 5%\n        else:\n            df[u_feature] = ones[u_feature].apply(lambda x: x + random.randint(-5,5)) # change Age in the range of 5 years\n                \n        if n == 0:\n            df_new = df.copy()\n        else:\n            df_new = pd.concat([df_new, df])\n    return df_new\n\ntry:\n    df_train_corr.drop(columns = ['bin_age'], inplace = True)\nexcept:\n    print('already deleted')        \n\ndf_train_mod = df_train_corr.copy()\ndf_train_mod['old_damaged'] = df_train_mod.apply(lambda x: pow(2,x.Vehicle_Age)+pow(2,x.Vehicle_Damage), axis =1)\n\n# we shall preserve validation set without augmentation\/over-sampling\ndf_temp, X_valid, _, y_valid = train_test_split(df_train_mod, df_train_mod['Response'], train_size=0.8, random_state = SEED)\nX_valid = X_valid.drop(columns = ['Response'])\n\n# upsampling Positive Response class only\ndf_train_up_a = upsample(df_temp.loc[df_temp['Response'] == 1], 'Age', 1)\ndf_train_up_v = upsample(df_temp.loc[df_temp['Response'] == 1], 'Vintage', 1)","724bdbf4":"df_train_mod.head()","ba005b64":"df_ext = pd.concat([df_train_mod,df_train_up_a])\ndf_ext = pd.concat([df_ext,df_train_up_v])\nX_train = df_ext.drop(columns = ['Response'])\ny_train = df_ext.Response\nprint('Train set target class count with over-sampling:')\nprint(y_train.value_counts())\nprint('Validation set target class count: ')\nprint(y_valid.value_counts())\nX_train.head()","9d286f71":"XGB_model_u = XGBClassifier(random_state = SEED, max_depth = 8, \n                            n_estimators = 3000, reg_lambda = 1.2, reg_alpha = 1.2, \n                            min_child_weight = 1, \n                            objective = 'binary:logistic',\n                            learning_rate = 0.15, gamma = 0.3, colsample_bytree = 0.5, eval_metric = 'auc')\n\nXGB_model_u.fit(X_train, y_train)\nXGB_preds_u = XGB_model_u.predict_proba(X_valid)\nXGB_score_u = roc_auc_score(y_valid, XGB_preds_u[:,1])\nXGB_class_u = XGB_model_u.predict(X_valid)\n\n(fpr, tpr, thresholds) = roc_curve(y_valid, XGB_preds_u[:,1])\nplot_ROC(fpr, tpr,'XGBoost')","cd0835e0":"print('ROC AUC score for XGBoost model with over-sampling + 2 new features: %.4f'%XGB_score_u)\nprint('F1 score: %0.4f'%f1_score(y_valid, XGB_class_u))\nskplt.metrics.plot_confusion_matrix(y_valid, XGB_class_u,\n        figsize=(8,8))","46860358":"xgb.plot_importance(XGB_model_u)","727592e5":"# X_train.drop(columns = ['Previously_Insured', 'Driving_License','Vehicle_Age','Vehicle_Damage'], inplace = True)\n# X_valid.drop(columns = ['Previously_Insured', 'Driving_License','Vehicle_Age','Vehicle_Damage'], inplace = True)\n# X_train.drop(columns = ['Previously_Insured'], inplace = True)\n# X_valid.drop(columns = ['Previously_Insured'], inplace = True)","a1bab033":"# XGB_model_ud = XGBClassifier(random_state = SEED, max_depth = 8, n_estimators = 3000, reg_lambda = 1.2, reg_alpha = 1.2, \n#                           min_child_weight = 1, \n#                           objective = 'binary:logistic',\n#                           learning_rate = 0.15, gamma = 0.3, colsample_bytree = 0.5, eval_metric = 'auc')\n\n# XGB_model_ud.fit(X_train, y_train)\n# XGB_preds_ud = XGB_model_ud.predict_proba(X_valid)\n# XGB_score_ud = roc_auc_score(y_valid, XGB_preds_ud[:,1])\n# XGB_class_ud = XGB_model_ud.predict(X_valid)\n\n# (fpr, tpr, thresholds) = roc_curve(y_valid, XGB_preds_ud[:,1])\n# plot_ROC(fpr, tpr,'XGBoost')","abd7a5e3":"# print('ROC AUC score for XGBoost model with over-sampling, and 4 features removed: %.4f'%XGB_score_ud)\n# print('F1 score: %0.4f'%f1_score(y_valid, XGB_class_ud))\n# skplt.metrics.plot_confusion_matrix(y_valid, XGB_class_ud,\n#         figsize=(8,8))","3617cd85":"# xgb.plot_importance(XGB_model_ud)","b9844eb0":"rf_params = {'max_depth': 20, 'n_estimators': 3000, 'min_samples_leaf': 1}\n# rf_params = {'max_depth': 20, 'n_estimators': 300, 'min_samples_leaf': 1}\nrf_params['random_state'] = SEED\nrf = RandomForestClassifier(**rf_params)\nrf.fit(X_train, y_train)\nrf_preds = rf.predict(X_valid)\nrf_preds_prob = rf.predict_proba(X_valid)[:,1]\n\nreg_score_uc = roc_auc_score(y_valid, rf_preds_prob, average = 'weighted')\nprint('ROC AUC score for RandomForest model with over-sampling: %.4f'%reg_score_uc)\nprint('Optimized RF f1-score', f1_score(y_valid, rf_preds))\nskplt.metrics.plot_confusion_matrix(y_valid, rf_preds,figsize=(8,8))\n\n(fpr, tpr, thresholds) = roc_curve(y_valid, rf_preds_prob)\nplot_ROC(fpr, tpr,'RandomForest')","17d1a1ca":"title=\"Feature Importances Random Forest\"\nfeat_imp = pd.DataFrame({'importance':rf.feature_importances_}) \nfeat_imp['feature'] = X_train.columns\nfeat_imp.sort_values(by='importance', ascending=True, inplace=True)\nfeat_imp = feat_imp.set_index('feature', drop=True)\nfeat_imp.plot.barh(title=title, figsize=(8,8))\nplt.xlabel('Feature Importance Score')\nplt.show()\n# rf.get_params()","8eaa22c1":"categorical_features1 = ['Gender',\n 'age_bin_cat',\n 'Region_Code',\n 'old_damaged',\n 'Policy_Sales_Channel']\n\nX_train.old_damaged = X_train.old_damaged.astype('int32')\nX_valid.old_damaged = X_valid.old_damaged.astype('int32')","73dfb4c9":"Cat_model1 = CatBoostClassifier( iterations = 30000, \n                                random_seed = SEED, \n#                                 task_type = 'GPU',\n                                task_type = 'CPU',\n                                learning_rate=0.15,\n                                random_strength=0.1,\n                                depth=8,\n                                loss_function='Logloss',\n                                eval_metric='Logloss',\n                                leaf_estimation_method='Newton',\n                                subsample = 0.9,\n                                rsm = 0.8,\n                                custom_loss = ['AUC'] )\nCat_model1.fit(X_train, y_train, cat_features = categorical_features1, eval_set = (X_valid, y_valid), plot = False,\n              early_stopping_rounds=50,verbose = 1000)","21868328":"Cat_preds1 = Cat_model1.predict_proba(X_valid)\nCat_class1 = Cat_model1.predict(X_valid)\nCat_score1 = roc_auc_score(y_valid, Cat_preds1[:,1])\n\n(fpr, tpr, thresholds) = roc_curve(y_valid, Cat_preds1[:,1])\nplot_ROC(fpr, tpr, 'CatBoost')","158f9170":"print('ROC AUC score for CatBoost model with over-sampling: %.4f'%Cat_score1)\nprint('CatBoost f1-score', f1_score(y_valid, Cat_class1))\nskplt.metrics.plot_confusion_matrix(y_valid, Cat_class1,figsize=(8,8))","3d5e1ef9":"X_train.head()","66b18025":"XGB_model_l = XGBClassifier(random_state = SEED, max_depth = 8, \n                            n_estimators = 30000, \n                            reg_lambda = 1.2, reg_alpha = 1.2, \n                            min_child_weight = 1, \n                            objective = 'binary:logistic',\n                            learning_rate = 0.15, gamma = 0.3, colsample_bytree = 0.5, eval_metric = 'auc')\n\nXGB_model_l.fit(X_train, y_train,\n                eval_set = [(X_valid, y_valid)],\n                early_stopping_rounds=50,verbose = 1000)","7f3ce8e5":"XGB_preds_l = XGB_model_l.predict_proba(X_valid)\nXGB_score_l = roc_auc_score(y_valid, XGB_preds_l[:,1])\nXGB_class_l = XGB_model_l.predict(X_valid)","422a5a11":"(fpr, tpr, thresholds) = roc_curve(y_valid, XGB_preds_l[:,1])\nplot_ROC(fpr, tpr,'XGBoost')\n\nprint('ROC AUC score for XGBoost model with over-sampling + 2 new features: %.4f'%XGB_score_l)\nprint('F1 score: %0.4f'%f1_score(y_valid, XGB_class_l))\nskplt.metrics.plot_confusion_matrix(y_valid, XGB_class_l,\n        figsize=(8,8))\n\nxgb.plot_importance(XGB_model_l)","d34f16be":"# Oversampling and Feature Engineering\nBrute-force oversampling of Positive Response class, and engineering 2 new synthetic features","57804156":"Now we are going to check how XGBoost with the same parameters as baseline will predict using upsampled data with new features.","0719b3ff":"# 2nd place\n","4c35ec76":"**9\/28\/2020** <br>\nI decided to give another chance for XGBoost, as I've tried it on my local machine with full features set + 2 new features and upsampling and the result was almost phenomenal...","87b84e57":"<p style = 'font-size : medium; font-weight: bold; color : red'>I have to admit it seems I've missed something crucial here :( <br>\nMost likely it is a target leak that I can't catch.<br>\nI would really appreciate any advice\/hint.<br><p>\nI will do my best to simplify the notebook to find where I went wrong as soon as possible.","9fa9c2de":"<strike>And it seems that dropping these features that were seemingly unimportant actually worsens the prediction...<\/strike>","957184d7":"# Import libraries","5ad5141e":"<strike>**Big question** - was AUC the right metric for this problem, or would F1 be better?<\/strike> <br>\n<p style = 'font-size : medium; font-weight: bold; color : brown'> I'm quite sure now that the metric must be F1 for this task to produce usable predictions<\/p>","ec597eaf":"# Data loading and setup","a60fc8ee":"# And the winner is...","bd822d9f":"Looks like we have pretty strong negative correlation between 'Vehicle_Damage' and 'Previously_Insured' features, worth exploring it later.","165da80f":"<strike>  Since we saw that 2 features were correlated: 'Previously_Insured' and 'Vehicle_Damage', and the former is less significant for the decision making we'd drop it.<\/strike>","1e67671c":"# TO-DO\n* <strike>Try to improve XGBoost and CatBoost through fine-tuning, and try to beat Random Forest on augmented data.    <br>\n We got CatBoost beating RandomForest and XGBoost successfully! Now we shall try to improve it even further :) <br>\n* KFold croos-validation might help, but I don't expect it to give a breakthrough improvement, so it is a low priority for now. <br> <\/strike>\n* And think of more feature engineering...<br>\n\n<p style = 'font-size : medium; font-weight: bold; color : green'> More experiments coming. <\/p>","b555a670":"Lets check feature correlations.","84fa472b":"History of changes in parameters reflects how the scoring changes:\n1. When parameters chosen as: ( iterations = 10000, random_seed = SEED, task_type = 'GPU', task_type = 'CPU', learning_rate=0.05, random_strength=0.1, depth=8,                                loss_function='Logloss', eval_metric='Logloss', leaf_estimation_method='Newton', subsample = 0.9, rsm = 0.8, custom_loss = ['AUC'] ) <br>\nthen we got: <br>\nROC AUC score for CatBoost model with over-sampling: 0.9111 <br>\nCatBoost f1-score 0.5562159484893512\n2. When I changed learning to 0.15 and limited iterations to 20000 I got: <br>\nROC AUC score for CatBoost model with over-sampling: 0.9659<br>\nCatBoost f1-score 0.7399318133390156\n3. When iterations were limited to 30000 it came to: <br>\nROC AUC score for CatBoost model with over-sampling: 0.9757 <br>\nCatBoost f1-score 0.7840309910685461\n4. When data set retained all features as opposed to dropping 4 less important ones in all previous attempts:<br>\nROC AUC score for CatBoost model with over-sampling: 0.9791<br>\nCatBoost f1-score 0.8014059753954305","b7378b0c":"# Ex-Winner\nRF gave it's top spot in the last 3 days, so it's not even the 2nd nowadays. Also the result is different\/worse now because RF did better on data set with some of the features dropped. New winners do better with extended data set instead.\n> Random Forest applied to the augmented set with 2 engineered features got us the most remarkable score to date (9\/25\/2020):  AUC 92%, f1 56%, and I'm quite sure there is some room for improvement.","c506129f":"bins = [10,20, 30, 40, 50, 60, 70, 80, 90, 100]\ndf_train_corr['bin_age'] = pd.cut(df_train_corr['Age'], bins)\ndf_train_corr['bin_age']\ndf_train_corr['age_bin_cat'] = le.fit_transform(df_train_corr['bin_age'])\ndf_train_corr['age_bin_cat'].value_counts()","463cb55f":"<p style = 'font-size : medium; font-weight: bold; color : brown'> Please Upvote this notebook if you like my work. <\/p>","7357e310":"Now we got much better True Positives, and quite acceptable AUC and f1 scores.","cd7d01af":"Easy check proves that there is no empty or NaN data.","43744899":"As of 9\/27\/2020 **CatBoost** is top contender.<br>\nAnd now lets try to improve CatBoost predictions. We will use it's built-in capabilities to deal with categorecal features. Hopefully it will work better than XGBoost with OneHot encoded features.<br>\nI tried it starting from 3000 iterations when it gave worse result than RF, but looking at the confusion matrix I saw potential for more precise prediction. So I started slightly tunining the parameters to get better results.","36c0d91a":"I'm not doing Exploratory Data Analysis (EDA) in this notebook, since there are quite a few notebooks with EDA already created. Instead my main goal is to achieve the highest F1 score (and through that the best AUC score obviously too). While analyzing the models predictions I realized from the very beginning that despite getting decent AUC scores all of them failed to provide sustainable precision. Their F1 score falls under 10%, which is unaccaptable in general. <br>\nUnfortunately most of the published notebooks do not address this issue. <br>\nThis notebook provides comparison of leading models used for binary classification on tabular data. I also analyze how well these algorithms deal with categorical data being OneHot encoded vs being just encoded as labels.<br>\nI also apply basic feature engineering to make a couple of synthetic features to see if that might help our models.<br>\n\n<span style = 'font-size : medium; font-weight: bold; color : red'> Spoiler alert <\/span> - you can scroll down to find the curent winning solution at the bottom!<br>\n<br>\nPlease note that for the simplicity's sake I removed baseline models comparison, and quite a few fruitless experiments\/approaches. If you'd like to see it please check version 15 and below of this notebook.","83bc2249":"Data loading and preprocessing (feature engineering)..."}}