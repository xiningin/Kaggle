{"cell_type":{"8fa2f1a7":"code","53d61a0f":"code","a6cffba5":"code","5bff2cf2":"code","a4082b4c":"code","4f743e92":"code","6f878f00":"code","dc6aec08":"code","33eded67":"code","2d1bc8c1":"code","6daa5678":"code","49bb1bc5":"code","9416ccde":"code","f7da1b7c":"code","7aea800a":"code","b3a5b17b":"code","8c73454c":"code","b3e2fb69":"code","640cf1ec":"code","6d49ee98":"code","d4dca231":"code","ec9ff2f8":"code","87032734":"markdown","c6188a9b":"markdown","fcceff49":"markdown","67c18fb9":"markdown","bcfb088e":"markdown","6ee39995":"markdown","9e68150b":"markdown","4af83f7e":"markdown","d15f77c6":"markdown","f3a09a8a":"markdown","d08c2af9":"markdown","f6681d81":"markdown","d761d1a4":"markdown","bc7d1176":"markdown","8a77a5ae":"markdown","d308a783":"markdown","659a2401":"markdown","43d11372":"markdown","94ad0358":"markdown","b281209f":"markdown","6c49ba72":"markdown","ae3d6b46":"markdown","4fa746bb":"markdown","d05b36cc":"markdown","2f74fdba":"markdown","228f8d0e":"markdown","2b5d32f5":"markdown","114d4423":"markdown","329dc26e":"markdown","2b54eacc":"markdown","2a86b8ec":"markdown","8a7b949f":"markdown","3edac105":"markdown","3921b062":"markdown","4fd107f5":"markdown","612a8f7b":"markdown","0949e695":"markdown","ffd6c98d":"markdown","d552c31a":"markdown","51b9f6ee":"markdown","54654d1a":"markdown","376b4971":"markdown","2068ffab":"markdown"},"source":{"8fa2f1a7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nimport math\nimport warnings\nfrom matplotlib.lines import Line2D\nfrom bokeh.layouts import row, layout\nfrom bokeh.transform import transform\nfrom bokeh.transform import factor_cmap\nfrom bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook, output_file\nfrom bokeh.palettes import Cividis256\nfrom bokeh.models import LabelSet, ColumnDataSource, LinearColorMapper, ColorBar, BasicTicker, FactorRange\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.models.widgets import DataTable, TableColumn\n\nwarnings.filterwarnings('ignore')\noutput_notebook()","53d61a0f":"multiplechoice_17 = pd.read_csv('..\/input\/kaggle-survey-2017\/multipleChoiceResponses.csv', low_memory=False, encoding='ISO-8859-1')\nmultiplechoice_18 = pd.read_csv('..\/input\/kaggle-survey-2018\/multipleChoiceResponses.csv', low_memory= False)\nmultiplechoice_19 = pd.read_csv('..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv', low_memory= False)\nmultiplechoice_20 = pd.read_csv('..\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv', low_memory= False)\n\nmultiplechoice_18.columns = [x.split('_')[0] for x in list(multiplechoice_18.columns)]\nmultiplechoice_19.columns = [x.split('_')[0] for x in list(multiplechoice_19.columns)]\nmultiplechoice_20.columns = [x.split('_')[0] for x in list(multiplechoice_20.columns)]\n\nmultiplechoice_18.columns = multiplechoice_18.columns + '_' + multiplechoice_18.iloc[0]\nmultiplechoice_19.columns = multiplechoice_19.columns + '_' + multiplechoice_19.iloc[0]\nmultiplechoice_20.columns = multiplechoice_20.columns + '_' + multiplechoice_20.iloc[0]\nmultiplechoice_18 = multiplechoice_18.drop([0])\nmultiplechoice_19 = multiplechoice_19.drop([0])\nmultiplechoice_20 = multiplechoice_20.drop([0])\n\nmultiplechoice_18['Time from Start to Finish (seconds)_Duration (in seconds)'] = multiplechoice_18['Time from Start to Finish (seconds)_Duration (in seconds)'].astype('float')\nmultiplechoice_19['Time from Start to Finish (seconds)_Duration (in seconds)'] = multiplechoice_19['Time from Start to Finish (seconds)_Duration (in seconds)'].astype('float')\nmultiplechoice_20['Time from Start to Finish (seconds)_Duration (in seconds)'] = multiplechoice_20['Time from Start to Finish (seconds)_Duration (in seconds)'].astype('float')\n\nmultiplechoice_18['Time from Start to Finish (seconds)_Duration (in seconds)'] = multiplechoice_18['Time from Start to Finish (seconds)_Duration (in seconds)'].apply(lambda x:x\/3600)\nmultiplechoice_19['Time from Start to Finish (seconds)_Duration (in seconds)'] = multiplechoice_19['Time from Start to Finish (seconds)_Duration (in seconds)'].apply(lambda x:x\/3600)\nmultiplechoice_20['Time from Start to Finish (seconds)_Duration (in seconds)'] = multiplechoice_20['Time from Start to Finish (seconds)_Duration (in seconds)'].apply(lambda x:x\/3600)\n\ntime_18 = str(round(multiplechoice_18['Time from Start to Finish (seconds)_Duration (in seconds)'].median()*60, 1)) + ' min'\ntime_19 = str(round(multiplechoice_19['Time from Start to Finish (seconds)_Duration (in seconds)'].median()*60, 2)) + ' min'\ntime_20 = str(round(multiplechoice_20['Time from Start to Finish (seconds)_Duration (in seconds)'].median()*60, 2)) + ' min'\n\nTOOLS=\"pan,wheel_zoom,zoom_in,zoom_out,undo,redo,reset,tap,save\"","a6cffba5":"survey_data_dict = {'Year': ['2017', '2018', '2019', '2020'],\n                    'Number of Respondents' : [len(multiplechoice_17), len(multiplechoice_18), len(multiplechoice_19),\n                                               len(multiplechoice_20)],\n                    'Number of Questions' :  ['64', '50', '34', '39'],\n                    'Median Response Time' : ['16.4 min', time_18, time_19, time_20]}\nsurvey_data = pd.DataFrame(survey_data_dict).set_index('Year')\n                            \nsource = ColumnDataSource(survey_data_dict)\n\ncolumns = [TableColumn(field=\"Year\", title=\"Year\"),\n           TableColumn(field=\"Number of Respondents\", title=\"Number of Respondents\"),\n           TableColumn(field=\"Number of Questions\", title=\"Number of Questions\"),\n           TableColumn(field=\"Median Response Time\", title=\"Median Response Time\")]\n\ndata_table = DataTable(source=source, columns=columns, width=695, height=130, index_position=None)\n\np0 = figure(x_range = survey_data.index.values, y_range = (0,26000), plot_width = 350, plot_height = 400, tools = TOOLS, title = \"Number Of Respondents\")\nsource0 = ColumnDataSource(dict(x=survey_data.index.values, y=survey_data.iloc[:,0].values.reshape(len(survey_data))))\nlabels0 = LabelSet(x='x', y='y', text='y', level='glyph', x_offset=-22, y_offset=0, source=source0, render_mode='canvas')\n\np0.vbar(survey_data.index.values, width = 0.9, top = survey_data.iloc[:,0].values.reshape(len(survey_data)), color='mediumseagreen')\np0.xaxis.axis_label = 'Year'\np0.yaxis.axis_label = 'Number of Respondents'\np0.yaxis.axis_label_text_font = 'times'\np0.yaxis.axis_label_text_font_size = '12pt'\np0.xaxis.axis_label_text_font = 'times'\np0.xaxis.axis_label_text_font_size = '12pt'\np0.ygrid.grid_line_color = None\np0.xgrid.grid_line_color = None\np0.toolbar.logo = None\np0.toolbar_location = None\np0.outline_line_color = None\np0.add_layout(labels0)\n\nyears = ['2017', '2018', '2019', '2020']\ncol_name = ['No of Questions', 'Median Response Time']\n\nplot_data = [(year, analysis_type) for year in years for analysis_type in col_name]\nsurvey_data['Median Response Time'] = survey_data['Median Response Time'].apply(lambda x:x.replace(' min', ''))\ncounts = list(survey_data.iloc[0,1:].values) + list(survey_data.iloc[1,1:].values) + list(survey_data.iloc[2,1:].values) + list(survey_data.iloc[3,1:].values)\n\nsource = ColumnDataSource(data=dict(x=plot_data, counts=counts))\nsource1 = ColumnDataSource(dict(x=survey_data.index.values, y=survey_data.iloc[:,1].values.reshape(len(survey_data))))\nsource2 = ColumnDataSource(dict(x=survey_data.index.values, y=survey_data.iloc[:,2].values.reshape(len(survey_data))))\n\nlabels1 = LabelSet(x='x', y='y', text='y', level='glyph', x_offset=-30, y_offset=0, source=source1, render_mode='canvas')\nlabels2 = LabelSet(x='x', y='y', text='y', level='glyph', x_offset=5, y_offset=0, source=source2, render_mode='canvas')\n\np1 = figure(x_range = FactorRange(*plot_data), y_range = (0,70), plot_width = 350, plot_height = 400, tools = TOOLS, title = \"Number Of Questions v\/s Mean Response time\")\np1.vbar(x='x', top='counts', width=0.9, source=source, fill_color=factor_cmap('x', palette=['mediumslateblue', 'burlywood'], factors=col_name, start=1, end=2))\np1.xaxis.axis_label = 'Year'\np1.yaxis.axis_label_text_font = 'times'\np1.yaxis.axis_label_text_font_size = '12pt'\np1.xaxis.axis_label_text_font = 'times'\np1.xaxis.axis_label_text_font_size = '12pt'\np1.ygrid.grid_line_color = None\np1.xgrid.grid_line_color = None\np1.xaxis.major_label_orientation = math.pi\/2\np1.toolbar.logo = None\np1.toolbar_location = None\np1.outline_line_color = None\np1.add_layout(labels1)\np1.add_layout(labels2)\n\nshow(layout([[data_table], [p0, p1]]))","5bff2cf2":"sns.set_style('darkgrid')\nplt.figure(figsize= (16,8))\n\n### Histogram plot\nplt.subplot(231)\nplt.hist(multiplechoice_18['Time from Start to Finish (seconds)_Duration (in seconds)'], bins = 50, color= 'indianred')\nplt.yscale('log')\n# plt.xlabel('Duration (in hrs)', fontsize = 'large')\nplt.ylabel('Number of Respondents', fontsize = 'large')\nplt.title('2018', fontsize = 'x-large', fontweight = 'roman')\n\n### Density plot\nplt.subplot(234)\nax = sns.kdeplot(multiplechoice_18['Time from Start to Finish (seconds)_Duration (in seconds)'], color= 'indianred')\nax.legend_.remove()\nplt.xlabel('Duration (in hrs)', fontsize = 'large')\nplt.ylabel('Density', fontsize = 'large')\n\n### Histogram plot\nplt.subplot(232)\nplt.hist(multiplechoice_19['Time from Start to Finish (seconds)_Duration (in seconds)'], bins = 50, color= 'darkslateblue')\nplt.yscale('log')\n# plt.xlabel('Duration (in hrs)', fontsize = 'large')\nplt.ylabel('Number of Respondents', fontsize = 'large')\nplt.title('2019', fontsize = 'x-large', fontweight = 'roman')\n\n### Density plot\nplt.subplot(235)\nax = sns.kdeplot(multiplechoice_19['Time from Start to Finish (seconds)_Duration (in seconds)'], color= 'darkslateblue')\nax.legend_.remove()\nplt.xlabel('Duration (in hrs)', fontsize = 'large')\nplt.ylabel('Density', fontsize = 'large')\n\n### Histogram plot\nplt.subplot(233)\nplt.hist(multiplechoice_20['Time from Start to Finish (seconds)_Duration (in seconds)'], bins = 50, color= 'seagreen')\nplt.yscale('log')\n# plt.xlabel('Duration (in hrs)', fontsize = 'large')\nplt.ylabel('Number of Respondents', fontsize = 'large')\nplt.title('2020', fontsize = 'x-large', fontweight = 'roman')\n\n### Density plot\nplt.subplot(236)\nax = sns.kdeplot(multiplechoice_20['Time from Start to Finish (seconds)_Duration (in seconds)'], color= 'seagreen')\nax.legend_.remove()\nplt.xlabel('Duration (in hrs)', fontsize = 'large')\nplt.ylabel('Density', fontsize = 'large')\n\nplt.show()","a4082b4c":"gender_17 = multiplechoice_17['GenderSelect'].value_counts().to_frame().iloc[:-1]\ngender_18 = multiplechoice_18['Q1_What is your gender? - Selected Choice'].value_counts().to_frame().iloc[:-1]\ngender_19 = multiplechoice_19['Q2_What is your gender? - Selected Choice'].value_counts().to_frame().iloc[:-1]\ngender_20 = multiplechoice_20['Q2_What is your gender? - Selected Choice'].value_counts().to_frame().iloc[:-1]\ngender_17.index = gender_18.index.values\n\ngender_17 = round(gender_17\/gender_17.sum(), 2)*100\ngender_18 = round(gender_18\/gender_18.sum(), 2)*100\ngender_19 = round(gender_19\/gender_19.sum(), 2)*100\ngender_20 = round(gender_20\/gender_20.sum(), 2)*100\ngender_20 = gender_20.iloc[:-1]\ngender_20.index = gender_17.index\n\n# Combine all genders from 2017 - 2020 to form a single table\ngender_comb = pd.concat([gender_17, gender_18, gender_19, gender_20], axis=1).T\ngender_comb.index = ['2017', '2018', '2019', '2020']\n\nfig, ax = plt.subplots(figsize=(20,6))\nplt.subplot(121)\nplt.bar(x=gender_comb.index.values, height=[x-78 for x in gender_comb.Male.values], bottom=78, color='darkslateblue')\nplt.ylabel('Percentage', fontsize = 'large')\nplt.xlabel('Year', fontsize = 'large')\nplt.title('Male population in Data Science', fontsize = 'large')\n\nplt.subplot(122)\nplt.bar(x=gender_comb.index.values, height=[x-15 for x in gender_comb.Female.values], bottom=15, color='seagreen')\nplt.ylabel('Percentage', fontsize = 'large')\nplt.xlabel('Year', fontsize = 'large')\nplt.title('Female population in Data Science', fontsize = 'large')\nplt.show()","4f743e92":"country = ['France', 'Canada','UK', 'Germany', 'Brazil', 'Russia', 'China', 'India', 'USA', 'Japan']\n\nmultiplechoice_17['Country'] = multiplechoice_17['Country'].replace(\"People 's Republic of China\", 'China').replace('United Kingdom', 'UK').replace('United States', 'USA')\nmultiplechoice_18['Q3_In which country do you currently reside?'] = multiplechoice_18['Q3_In which country do you currently reside?'].replace('United Kingdom of Great Britain and Northern Ireland', 'UK').replace('United States of America', 'USA')\nmultiplechoice_19['Q3_In which country do you currently reside?'] = multiplechoice_19['Q3_In which country do you currently reside?'].replace('United Kingdom of Great Britain and Northern Ireland', 'UK').replace('United States of America', 'USA')\nmultiplechoice_20['Q3_In which country do you currently reside?'] = multiplechoice_20['Q3_In which country do you currently reside?'].replace('United Kingdom of Great Britain and Northern Ireland', 'UK').replace('United States of America', 'USA')\n\ntop_countries_17 = multiplechoice_17['Country'].value_counts().to_frame().loc[country].sort_values('Country')\ntop_countries_18 = multiplechoice_18['Q3_In which country do you currently reside?'].value_counts().to_frame().loc[country].sort_values('Q3_In which country do you currently reside?')\ntop_countries_19 = multiplechoice_19['Q3_In which country do you currently reside?'].value_counts().to_frame().loc[country].sort_values('Q3_In which country do you currently reside?')\ntop_countries_20 = multiplechoice_20['Q3_In which country do you currently reside?'].value_counts().to_frame().loc[country].sort_values('Q3_In which country do you currently reside?')\n\ntop_countries_17 = round(top_countries_17\/top_countries_17.sum(), 2)*100\ntop_countries_18 = round(top_countries_18\/top_countries_18.sum(), 2)*100\ntop_countries_19 = round(top_countries_19\/top_countries_19.sum(), 2)*100\ntop_countries_20 = round(top_countries_20\/top_countries_20.sum(), 2)*100\n\ntop_countries_18 = top_countries_18.reindex(list(top_countries_17.index))\ntop_countries_19 = top_countries_19.reindex(list(top_countries_17.index))\ntop_countries_20 = top_countries_20.reindex(list(top_countries_17.index))\n\ncountry_list = list(top_countries_17.index)\ntop_con_17 = list(top_countries_17['Country'])\ntop_con_18 = list(top_countries_18['Q3_In which country do you currently reside?'])\ntop_con_19 = list(top_countries_19['Q3_In which country do you currently reside?'])\ntop_con_20 = list(top_countries_20['Q3_In which country do you currently reside?'])\n\ndot = figure(title=\"Participants by Country\", tools=TOOLS, plot_width = 750, plot_height = 400,\n             y_range=country_list, x_range=[0,60])\n\ndot.segment(0, country_list, top_con_17, country_list, line_width=2, line_color=\"darkslategrey\", legend='2017')\ndot.circle(top_con_17, country_list, size=15, fill_color=\"tan\", line_color=\"darkslategrey\", line_width=1, legend='2017')\ndot.segment(0, country_list, top_con_18, country_list, line_width=2, line_color=\"darkslategrey\", legend='2018')\ndot.circle(top_con_18, country_list, size=15, fill_color=\"indianred\", line_color=\"darkslategrey\", line_width=1, legend='2018')\ndot.segment(0, country_list, top_con_19, country_list, line_width=2, line_color=\"darkslategrey\", legend='2019')\ndot.circle(top_con_19, country_list, size=15, fill_color=\"darkslateblue\", line_color=\"darkslategrey\", line_width=1, legend='2019')\ndot.segment(0, country_list, top_con_20, country_list, line_width=2, line_color=\"darkslategrey\", legend='2020')\ndot.circle(top_con_20, country_list, size=15, fill_color=\"seagreen\", line_color=\"darkslategrey\", line_width=1, legend='2020')\n\ndot.xaxis.axis_label = 'Percentage of Respondents'\ndot.yaxis.axis_label = 'Country'\ndot.yaxis.axis_label_text_font = 'times'\ndot.yaxis.axis_label_text_font_size = '12pt'\ndot.xaxis.axis_label_text_font = 'times'\ndot.xaxis.axis_label_text_font_size = '12pt'\ndot.ygrid.grid_line_color = None\ndot.xgrid.grid_line_color = None\ndot.legend.location = \"bottom_right\"\ndot.legend.click_policy=\"hide\"\nshow(dot)","6f878f00":"ind_17 = multiplechoice_17[multiplechoice_17['Country'] == 'India']['GenderSelect'].value_counts().iloc[:2]\nind_18 = multiplechoice_18[multiplechoice_18['Q3_In which country do you currently reside?'] == 'India']['Q1_What is your gender? - Selected Choice'].value_counts().iloc[:2]\nind_19 = multiplechoice_19[multiplechoice_19['Q3_In which country do you currently reside?'] == 'India']['Q2_What is your gender? - Selected Choice'].value_counts().iloc[:2]\nind_20 = multiplechoice_20[multiplechoice_20['Q3_In which country do you currently reside?'] == 'India']['Q2_What is your gender? - Selected Choice'].value_counts().iloc[:2]\nind_20.index = ['Male', 'Female']\n\nind_17 = round(ind_17\/ind_17.sum(), 2)*100\nind_18 = round(ind_18\/ind_18.sum(), 2)*100\nind_19 = round(ind_19\/ind_19.sum(), 2)*100\nind_20 = round(ind_20\/ind_20.sum(), 2)*100\n\nind_gend = pd.concat([ind_17, ind_18, ind_19, ind_20], axis=1)\nind_gend.columns = [2017, 2018, 2019, 2020]\n\nusa_17 = multiplechoice_17[multiplechoice_17['Country'] == 'USA']['GenderSelect'].value_counts().iloc[:2]\nusa_18 = multiplechoice_18[multiplechoice_18['Q3_In which country do you currently reside?'] == 'USA']['Q1_What is your gender? - Selected Choice'].value_counts().iloc[:2]\nusa_19 = multiplechoice_19[multiplechoice_19['Q3_In which country do you currently reside?'] == 'USA']['Q2_What is your gender? - Selected Choice'].value_counts().iloc[:2]\nusa_20 = multiplechoice_20[multiplechoice_20['Q3_In which country do you currently reside?'] == 'USA']['Q2_What is your gender? - Selected Choice'].value_counts().iloc[:2]\nusa_20.index = ['Male', 'Female']\n\nusa_17 = round(usa_17\/usa_17.sum(), 2)*100\nusa_18 = round(usa_18\/usa_18.sum(), 2)*100\nusa_19 = round(usa_19\/usa_19.sum(), 2)*100\nusa_20 = round(usa_20\/usa_20.sum(), 2)*100\n\nusa_gend = pd.concat([usa_17, usa_18, usa_19, usa_20], axis=1)\nusa_gend.columns = [2017, 2018, 2019, 2020]\n\nind_gend.T.plot(kind='line', figsize=(12,5), colormap='seismic', title='India', subplots=True, legend=True)\nplt.legend(loc='right')\nplt.xticks([2017, 2018, 2019, 2020])\n\nusa_gend.T.plot(kind='line', figsize=(12,5), colormap='seismic', title='USA', subplots=True, legend=True)\nplt.legend(loc='upper right')\nplt.xticks([2017, 2018, 2019, 2020])\nplt.show()","dc6aec08":"age_17_dict = {\n'18-21' : len(multiplechoice_17[(multiplechoice_17['Age'] > 18) & (multiplechoice_17['Age'] < 21)]['Age']),\n'22-24' : len(multiplechoice_17[(multiplechoice_17['Age'] > 21) & (multiplechoice_17['Age'] < 25)]['Age']),\n'25-29' : len(multiplechoice_17[(multiplechoice_17['Age'] > 24) & (multiplechoice_17['Age'] < 30)]['Age']),\n'30-34' : len(multiplechoice_17[(multiplechoice_17['Age'] > 29) & (multiplechoice_17['Age'] < 35)]['Age']),\n'35-39' : len(multiplechoice_17[(multiplechoice_17['Age'] > 34) & (multiplechoice_17['Age'] < 40)]['Age']),\n'40-44' : len(multiplechoice_17[(multiplechoice_17['Age'] > 39) & (multiplechoice_17['Age'] < 45)]['Age']),\n'45-49' : len(multiplechoice_17[(multiplechoice_17['Age'] > 44) & (multiplechoice_17['Age'] < 50)]['Age']),\n'50-54' : len(multiplechoice_17[(multiplechoice_17['Age'] > 49) & (multiplechoice_17['Age'] < 55)]['Age']),\n'55-59' : len(multiplechoice_17[(multiplechoice_17['Age'] > 54) & (multiplechoice_17['Age'] < 60)]['Age']),\n'60-69' : len(multiplechoice_17[(multiplechoice_17['Age'] > 59) & (multiplechoice_17['Age'] < 70)]['Age']),\n'70+' : len(multiplechoice_17[(multiplechoice_17['Age'] > 70)])}\n\nylab_18 = multiplechoice_18['Q2_What is your age (# years)?'].sort_values().unique()\nylab_19 = multiplechoice_19['Q1_What is your age (# years)?'].sort_values().unique()\nylab_20 = multiplechoice_20['Q1_What is your age (# years)?'].sort_values().unique()\n\nage_df_17 = pd.DataFrame(age_17_dict, index = range(12)).T[0]\nage_df_18 = multiplechoice_18['Q2_What is your age (# years)?'].value_counts().to_frame().loc[ylab_18]\nage_df_19 = multiplechoice_19['Q1_What is your age (# years)?'].value_counts().to_frame().loc[ylab_19]\nage_df_20 = multiplechoice_20['Q1_What is your age (# years)?'].value_counts().to_frame().loc[ylab_20]\n\nage_df_18_last_row = age_df_18.loc['70-79'] + age_df_18.loc['80+']\nage_df_18 = age_df_18.drop(['70-79','80+'])\nage_df_18 = age_df_18.append(pd.DataFrame([age_df_18_last_row], columns=['Q2_What is your age (# years)?'], index=['70+']))\n\nage_df_17 = round(age_df_17\/age_df_17.sum(), 2)*100\nage_df_18 = round(age_df_18\/age_df_18.sum(), 2)*100\nage_df_19 = round(age_df_19\/age_df_19.sum(), 2)*100\nage_df_20 = round(age_df_20\/age_df_20.sum(), 2)*100\n\n\nage_list = list(age_df_17.index)[::-1]\nage_17 = list(age_df_17)[::-1]\nage_18 = list(age_df_18['Q2_What is your age (# years)?'])[::-1]\nage_19 = list(age_df_19['Q1_What is your age (# years)?'])[::-1]\nage_20 = list(age_df_20['Q1_What is your age (# years)?'])[::-1]\n\ndot = figure(title=\"Age Group of Respondents\", tools=TOOLS, plot_width = 750, plot_height = 400,\n             y_range=age_list, x_range=[0,30])\n\ndot.segment(0, age_list, age_17, age_list, line_width=2, line_color=\"darkslategrey\", legend='2017')\ndot.circle(age_17, age_list, size=15, fill_color=\"tan\", line_color=\"darkslategrey\", line_width=1, legend='2017')\ndot.segment(0, age_list, age_18, age_list, line_width=2, line_color=\"darkslategrey\", legend='2018')\ndot.circle(age_18, age_list, size=15, fill_color=\"indianred\", line_color=\"darkslategrey\", line_width=1, legend='2018')\ndot.segment(0, age_list, age_19, age_list, line_width=2, line_color=\"darkslategrey\", legend='2019')\ndot.circle(age_19, age_list, size=15, fill_color=\"darkslateblue\", line_color=\"darkslategrey\", line_width=1, legend='2019')\ndot.segment(0, age_list, age_20, age_list, line_width=2, line_color=\"darkslategrey\", legend='2020')\ndot.circle(age_20, age_list, size=15, fill_color=\"seagreen\", line_color=\"darkslategrey\", line_width=1, legend='2020')\n\ndot.xaxis.axis_label = 'Percentage of Respondents'\ndot.yaxis.axis_label = 'Age Group'\ndot.yaxis.axis_label_text_font = 'times'\ndot.yaxis.axis_label_text_font_size = '12pt'\ndot.xaxis.axis_label_text_font = 'times'\ndot.xaxis.axis_label_text_font_size = '12pt'\ndot.ygrid.grid_line_color = None\ndot.xgrid.grid_line_color = None\ndot.legend.location = \"bottom_right\"\ndot.legend.click_policy=\"hide\"\nshow(dot)","33eded67":"male_17 = multiplechoice_17[multiplechoice_17['GenderSelect'] == 'Male']['Age']\nmale_18 = multiplechoice_18[multiplechoice_18['Q1_What is your gender? - Selected Choice'] == 'Male']['Q2_What is your age (# years)?'].value_counts()\nmale_19 = multiplechoice_19[multiplechoice_19['Q2_What is your gender? - Selected Choice'] == 'Male']['Q1_What is your age (# years)?'].value_counts()\nmale_20 = multiplechoice_20[multiplechoice_20['Q2_What is your gender? - Selected Choice'] == 'Man']['Q1_What is your age (# years)?'].value_counts()\n\nfemale_17 = multiplechoice_17[multiplechoice_17['GenderSelect'] == 'Female']['Age']\nfemale_18 = multiplechoice_18[multiplechoice_18['Q1_What is your gender? - Selected Choice'] == 'Female']['Q2_What is your age (# years)?'].value_counts()\nfemale_19 = multiplechoice_19[multiplechoice_19['Q2_What is your gender? - Selected Choice'] == 'Female']['Q1_What is your age (# years)?'].value_counts()\nfemale_20 = multiplechoice_20[multiplechoice_20['Q2_What is your gender? - Selected Choice'] == 'Woman']['Q1_What is your age (# years)?'].value_counts()\n\nmale_17_dict = {\n'18-21' : len(male_17[(male_17 > 18) & (male_17 < 21)]),\n'22-24' : len(male_17[(male_17 > 21) & (male_17 < 25)]),\n'25-29' : len(male_17[(male_17 > 24) & (male_17 < 30)]),\n'30-34' : len(male_17[(male_17 > 29) & (male_17 < 35)]),\n'35-39' : len(male_17[(male_17 > 34) & (male_17 < 40)]),\n'40-44' : len(male_17[(male_17 > 39) & (male_17 < 45)]),\n'45-49' : len(male_17[(male_17 > 44) & (male_17 < 50)]),\n'50-54' : len(male_17[(male_17 > 49) & (male_17 < 55)]),\n'55-59' : len(male_17[(male_17 > 54) & (male_17 < 60)]),\n'60-69' : len(male_17[(male_17 > 59) & (male_17 < 70)]),\n'70+' : len(male_17[(male_17 > 70)])}\n\nfemale_17_dict = {\n'18-21' : len(female_17[(female_17 > 18) & (female_17 < 21)]),\n'22-24' : len(female_17[(female_17 > 21) & (female_17 < 25)]),\n'25-29' : len(female_17[(female_17 > 24) & (female_17 < 30)]),\n'30-34' : len(female_17[(female_17 > 29) & (female_17 < 35)]),\n'35-39' : len(female_17[(female_17 > 34) & (female_17 < 40)]),\n'40-44' : len(female_17[(female_17 > 39) & (female_17 < 45)]),\n'45-49' : len(female_17[(female_17 > 44) & (female_17 < 50)]),\n'50-54' : len(female_17[(female_17 > 49) & (female_17 < 55)]),\n'55-59' : len(female_17[(female_17 > 54) & (female_17 < 60)]),\n'60-69' : len(female_17[(female_17 > 59) & (female_17 < 70)]),\n'70+' : len(female_17[(female_17 > 70)])}\n\n\nmale_17 = pd.DataFrame(male_17_dict, index = range(12)).T[0]\nmale_18_last_row = male_18.loc['70-79'] + male_18.loc['80+']\nmale_18 = male_18.drop(['70-79','80+'])\nmale_18 = male_18.append(pd.Series([male_18_last_row], index=['70+']))\n\nmale_17 = round(male_17\/male_17.sum(), 2)*100\nmale_18 = round(male_18\/male_18.sum(), 2)*100\nmale_19 = round(male_19\/male_19.sum(), 2)*100\nmale_20 = round(male_20\/male_20.sum(), 2)*100\n\nmale_age = pd.concat([male_17, male_18, male_19, male_20], axis=1)\nmale_age.columns = [2017, 2018, 2019, 2020]\n\nfemale_17 = pd.DataFrame(male_17_dict, index = range(12)).T[0]\nfemale_18_last_row = female_18.loc['70-79'] + female_18.loc['80+']\nfemale_18 = female_18.drop(['70-79','80+'])\nfemale_18 = female_18.append(pd.Series([female_18_last_row], index=['70+']))\n\nfemale_17 = round(female_17\/female_17.sum(), 4)*100\nfemale_18 = round(female_18\/female_18.sum(), 4)*100\nfemale_19 = round(female_19\/female_19.sum(), 4)*100\nfemale_20 = round(female_20\/female_20.sum(), 4)*100\n\nfemale_age = pd.concat([female_17, female_18, female_19, female_20], axis=1)\nfemale_age.columns = [2017, 2018, 2019, 2020]\n\n\nmale_age_dict = {'18-30': list(male_age.iloc[:3,:].sum()), '30-45': list(male_age.iloc[3:6,:].sum()),\n                  '45+': list(male_age.iloc[6:,:].sum())}\n\nfemale_age_dict = {'18-30': list(female_age.iloc[:10,:].sum()), '30-45': list(female_age.iloc[3:6,:].sum()),\n                    '45+': list(female_age.iloc[6:,:].sum())}\nmale_age_df = pd.DataFrame(male_age_dict, index=[2017, 2018, 2019, 2020]).T\nfemale_age_df = pd.DataFrame(female_age_dict, index=[2017, 2018, 2019, 2020]).T\n\n\nfig, ax = plt.subplots(figsize=(20, 6))\nexplode = (0, 0, 0.1)\nstart_angle = 60\ncolors = ['indianred', 'darkslateblue', 'seagreen']\n\npatch1 = mpatches.Patch(color='indianred', label='18-30')\npatch2 = mpatches.Patch(color='darkslateblue', label='30-45')\npatch3 = mpatches.Patch(color='seagreen', label='45+')\n\nplt.subplot(121)\ncomp_range = list(male_age_df.index.values)\ncomp_values = list(male_age_df[2020].values)\nplt.pie(comp_values, wedgeprops=dict(width=0.3), startangle=start_angle, autopct='%1.1f%%', colors=colors,\n        pctdistance=0.925, textprops=dict(color=\"black\", fontsize='large'), radius=2)\n\ncomp_values = list(male_age_df[2019].values)\nplt.pie(comp_values, wedgeprops=dict(width=0.3), startangle=start_angle, autopct='%1.1f%%', colors=colors,\n        pctdistance=0.915, textprops=dict(color=\"black\", fontsize='large'), radius=1.7)\n\ncomp_values = list(male_age_df[2018].values)\nplt.pie(comp_values, wedgeprops=dict(width=0.3), startangle=start_angle, autopct='%1.1f%%', colors=colors,\n        pctdistance=0.89, textprops=dict(color=\"black\", fontsize='large'), radius=1.4)\n\ncomp_values = list(male_age_df[2017].values)\nplt.pie(comp_values, wedgeprops=dict(width=0.3), startangle=start_angle, autopct='%1.1f%%', colors=colors,\n        pctdistance=0.85, textprops=dict(color=\"black\", fontsize='large'), radius=1.1)\n\nplt.legend(title = \"Age group\", handles=[patch1, patch2, patch3], loc=(1.3,1))\nplt.title(\"Male\", fontsize='x-large', pad=-165)\n\nplt.subplot(122)\ncomp_range = list(female_age_df.index.values)\ncomp_values = list(female_age_df[2020].values)\nplt.pie(comp_values, wedgeprops=dict(width=0.3), startangle=start_angle, autopct='%1.1f%%', colors=colors,\n        pctdistance=0.925, textprops=dict(color=\"black\", fontsize='large'), radius=2)\n\ncomp_values = list(female_age_df[2019].values)\nplt.pie(comp_values, wedgeprops=dict(width=0.3), startangle=start_angle, autopct='%1.1f%%', colors=colors,\n        pctdistance=0.915, textprops=dict(color=\"black\", fontsize='large'), radius=1.7)\n\ncomp_values = list(female_age_df[2018].values)\nplt.pie(comp_values, wedgeprops=dict(width=0.3), startangle=start_angle, autopct='%1.1f%%', colors=colors,\n        pctdistance=0.89, textprops=dict(color=\"black\", fontsize='large'), radius=1.4)\n\ncomp_values = list(female_age_df[2017].values)\nplt.pie(comp_values, wedgeprops=dict(width=0.3), startangle=start_angle, autopct='%1.1f%%', colors=colors,\n        pctdistance=0.85, textprops=dict(color=\"black\", fontsize='large'), radius=1.1)\n\nplt.title(\"Female\", fontsize='x-large', pad=-165)\n\n# Add year text to male donut charts\nplt.text(-4.85, 0.9, '2017', fontsize='x-large')\nplt.text(-4.85, 1.2, '2018', fontsize='x-large')\nplt.text(-4.85, 1.52, '2019', fontsize='x-large')\nplt.text(-4.85, 1.8, '2020', fontsize='x-large')\n\n# Add year text to female donut charts\nplt.text(-0.15, 0.9, '2017', fontsize='x-large')\nplt.text(-0.15, 1.2, '2018', fontsize='x-large')\nplt.text(-0.15, 1.52, '2019', fontsize='x-large')\nplt.text(-0.15, 1.8, '2020', fontsize='x-large')\n\nplt.show()","2d1bc8c1":"educ_lvl_17 = multiplechoice_17['FormalEducation'].value_counts().to_frame()\neduc_lvl_18 = multiplechoice_18['Q4_What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'].value_counts().to_frame()\neduc_lvl_19 = multiplechoice_19['Q4_What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'].value_counts().to_frame()\neduc_lvl_20 = multiplechoice_20['Q4_What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'].value_counts().to_frame()\neduc_lvl_17.index = ['Master\u2019s degree', 'Bachelor\u2019s degree', 'Doctoral degree','Some college\/university study without earning a bachelor\u2019s degree',\n                      'Professional degree', 'No formal education past high school', 'I prefer not to answer']\n\neduc_lvl_17 = educ_lvl_17.drop('I prefer not to answer')\neduc_lvl_18 = educ_lvl_18.drop('I prefer not to answer')\neduc_lvl_19 = educ_lvl_19.drop('I prefer not to answer')\neduc_lvl_20 = educ_lvl_20.drop('I prefer not to answer')\n\neduc_lvl_17 = round(educ_lvl_17\/educ_lvl_17.sum(), 4)*100\neduc_lvl_18 = round(educ_lvl_18\/educ_lvl_18.sum(), 4)*100\neduc_lvl_19 = round(educ_lvl_19\/educ_lvl_19.sum(), 4)*100\neduc_lvl_20 = round(educ_lvl_20\/educ_lvl_20.sum(), 4)*100\n\neduc_lvl_17 = educ_lvl_17.reindex(list(educ_lvl_18.index))\n\n\neduc_list =  ['Master\u2019s degree', 'Bachelor\u2019s degree', 'Doctoral degree']\nedu_17 = list(educ_lvl_17['FormalEducation'])[:3]\nedu_18 = list(educ_lvl_18['Q4_What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'])[:3]\nedu_19 = list(educ_lvl_19['Q4_What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'])[:3]\nedu_20 = list(educ_lvl_20['Q4_What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'])[:3]\n\nedu_dict = {'2017': edu_17, '2018': edu_18, '2019': edu_19, '2020': edu_20}\nedu_df = pd.DataFrame(edu_dict, index=educ_list).T\n\nfig, ax = plt.subplots(figsize=(20,6))\nplt.subplot(131)\nplt.bar(x=edu_df.index.values, height=[x-30 for x in edu_df['Bachelor\u2019s degree'].values], bottom=30, color='seagreen')\nplt.ylabel('Percentage', fontsize = 'large')\nplt.xlabel('Year', fontsize = 'large')\nplt.title('Respondent population with Bachelor\u2019s degree', fontsize = 'large')\n\nplt.subplot(132)\nplt.bar(x=edu_df.index.values, height=[x-40 for x in edu_df['Master\u2019s degree'].values], bottom=40, color='darkslateblue')\nplt.ylabel('Percentage', fontsize = 'large')\nplt.xlabel('Year', fontsize = 'large')\nplt.title('Respondent population with Master\u2019s degree', fontsize = 'large')\n\nplt.subplot(133)\nplt.bar(x=edu_df.index.values, height=[x-10 for x in edu_df['Doctoral degree'].values], bottom=11, color='seagreen')\nplt.ylabel('Percentage', fontsize = 'large')\nplt.xlabel('Year', fontsize = 'large')\nplt.title('Respondent population with Doctoral degree', fontsize = 'large')\n\nplt.show()","6daa5678":"yearly_comp_18 = multiplechoice_18['Q9_What is your current yearly compensation (approximate $USD)?'].value_counts().to_frame()[1:]\nyearly_comp_19 = multiplechoice_19['Q10_What is your current yearly compensation (approximate $USD)?'].value_counts().to_frame()[1:]\nyearly_comp_20 = multiplechoice_20['Q24_What is your current yearly compensation (approximate $USD)?'].value_counts().to_frame()[1:]\n\nyearly_comp_18 = round(yearly_comp_18\/yearly_comp_18.sum(), 2)*100\nyearly_comp_19 = round(yearly_comp_19\/yearly_comp_19.sum(), 2)*100\nyearly_comp_20 = round(yearly_comp_20\/yearly_comp_20.sum(), 2)*100\n\n\nold_idx_sort = ['0-10,000', '10-20,000', '20-30,000', '30-40,000', '40-50,000','50-60,000', '60-70,000', '70-80,000', '80-90,000', '90-100,000',\\\n'100-125,000', '125-150,000',  '150-200,000', '200-250,000', '250-300,000', '300-400,000', '400-500,000', '500,000+']\n\nnew_idx_sort = ['1,000-1,999', '2,000-2,999', '3,000-3,999', '4,000-4,999', '5,000-7,499', '7,500-9,999', '10,000-14,999', '15,000-19,999',\n'20,000-24,999', '25,000-29,999', '30,000-39,999', '40,000-49,999','50,000-59,999', '60,000-69,999', '70,000-79,999',\n'80,000-89,999', '90,000-99,999', '100,000-124,999', '125,000-149,999', '150,000-199,999', '200,000-249,999', '250,000-299,999',\n'300,000-500,000', '> $500,000']\n\nyearly_comp_18 = yearly_comp_18.reindex(index = old_idx_sort)\nyearly_comp_19 = yearly_comp_19.reindex(index = new_idx_sort)\nyearly_comp_20 = yearly_comp_20.reindex(index = new_idx_sort)\n\ncomb_idx = ['0-10,000', '10-20,000', '20-30,000', '30-40,000', '40-50,000','50-60,000', '60-70,000', '70-80,000', '80-90,000', '90-100,000',\\\n'100-125,000', '125-150,000',  '150-200,000', '200-250,000', '250-300,000', '300-500,000', '500,000+']\n\n\nyearly_comp_18 = pd.DataFrame({'salary': comb_idx,\n                                'count': [x[0] for x in yearly_comp_18[:-3].values] + [yearly_comp_18.iloc[-3:-1].values.sum()] +\\\n                                [yearly_comp_18.iloc[-1][0]]}).set_index('salary')\n\nyearly_comp_19 = pd.DataFrame({'salary': comb_idx,\n                                'count': [yearly_comp_19.iloc[:6].values.sum(), yearly_comp_19.iloc[6:8].values.sum(),\n                                 yearly_comp_19.iloc[8:10].values.sum()] + [x[0] for x in yearly_comp_19[10:].values]}).set_index('salary')\n\nyearly_comp_20 = pd.DataFrame({'salary': comb_idx,\n                                'count': [yearly_comp_20.iloc[:6].values.sum(), yearly_comp_20.iloc[6:8].values.sum(),\n                                 yearly_comp_20.iloc[8:10].values.sum()] + [x[0] for x in yearly_comp_20[10:].values]}).set_index('salary')\n\n\n\nyearly_comp_list = list(yearly_comp_18.index)[::-1]\ncomp_18 = list(yearly_comp_18['count'])[::-1]\ncomp_19 = list(yearly_comp_19['count'])[::-1]\ncomp_20 = list(yearly_comp_20['count'])[::-1]\n\ndot = figure(title=\"Yearly Compensation\", tools=TOOLS, plot_width = 750, plot_height = 400, y_range=yearly_comp_list, x_range=[0,31])\n\ndot.segment(0, yearly_comp_list, comp_18, yearly_comp_list, line_width=2, line_color=\"darkslategrey\", legend='2018')\ndot.circle(comp_18, yearly_comp_list, size=15, fill_color=\"indianred\", line_color=\"darkslategrey\", line_width=1, legend='2018')\ndot.segment(0, yearly_comp_list, comp_19, yearly_comp_list, line_width=2, line_color=\"darkslategrey\", legend='2019')\ndot.circle(comp_19, yearly_comp_list, size=15, fill_color=\"darkslateblue\", line_color=\"darkslategrey\", line_width=1, legend='2019')\ndot.segment(0, yearly_comp_list, comp_20, yearly_comp_list, line_width=2, line_color=\"darkslategrey\", legend='2020')\ndot.circle(comp_20, yearly_comp_list, size=15, fill_color=\"seagreen\", line_color=\"darkslategrey\", line_width=1, legend='2020')\n\ndot.xaxis.axis_label = 'Percentage of Respondents'\ndot.yaxis.axis_label = 'Yearly Compensation (approx $USD)'\ndot.yaxis.axis_label_text_font = 'times'\ndot.yaxis.axis_label_text_font_size = '12pt'\ndot.xaxis.axis_label_text_font = 'times'\ndot.xaxis.axis_label_text_font_size = '12pt'\ndot.ygrid.grid_line_color = None\ndot.xgrid.grid_line_color = None\ndot.legend.location = \"bottom_right\"\ndot.legend.click_policy=\"hide\"\nshow(dot)","49bb1bc5":"male_18 = multiplechoice_18[multiplechoice_18['Q1_What is your gender? - Selected Choice'] == 'Male']['Q9_What is your current yearly compensation (approximate $USD)?'].value_counts()\nmale_19 = multiplechoice_19[multiplechoice_19['Q2_What is your gender? - Selected Choice'] == 'Male']['Q10_What is your current yearly compensation (approximate $USD)?'].value_counts()\nmale_20 = multiplechoice_20[multiplechoice_20['Q2_What is your gender? - Selected Choice'] == 'Man']['Q24_What is your current yearly compensation (approximate $USD)?'].value_counts()\n\nfemale_18 = multiplechoice_18[multiplechoice_18['Q1_What is your gender? - Selected Choice'] == 'Female']['Q9_What is your current yearly compensation (approximate $USD)?'].value_counts()\nfemale_19 = multiplechoice_19[multiplechoice_19['Q2_What is your gender? - Selected Choice'] == 'Female']['Q10_What is your current yearly compensation (approximate $USD)?'].value_counts()\nfemale_20 = multiplechoice_20[multiplechoice_20['Q2_What is your gender? - Selected Choice'] == 'Woman']['Q24_What is your current yearly compensation (approximate $USD)?'].value_counts()\n\nmale_18 = male_18.reindex(index = old_idx_sort)\nmale_19 = male_19.reindex(index = new_idx_sort)\nmale_20 = male_20.reindex(index = new_idx_sort)\n\nfemale_18 = female_18.reindex(index = old_idx_sort)\nfemale_19 = female_19.reindex(index = new_idx_sort)\nfemale_20 = female_20.reindex(index = new_idx_sort)\n\n\nmale_comp_18 = pd.DataFrame({'salary': comb_idx,\n                                'count': [x for x in male_18[:-3].values] + [male_18.iloc[-3:-1].values.sum()] +\\\n                                [male_18.iloc[-1]]}).set_index('salary')\n\nmale_comp_19 = pd.DataFrame({'salary': comb_idx,\n                                'count': [male_19.iloc[:6].values.sum(), male_19.iloc[6:8].values.sum(),\n                                 male_19.iloc[8:10].values.sum()] + [x for x in male_19[10:].values]}).set_index('salary')\n\nmale_comp_20 = pd.DataFrame({'salary': comb_idx,\n                                'count': [male_20.iloc[:6].values.sum(), male_20.iloc[6:8].values.sum(),\n                                 male_20.iloc[8:10].values.sum()] + [x for x in male_20[10:].values]}).set_index('salary')\n\nfemale_comp_18 = pd.DataFrame({'salary': comb_idx,\n                                'count': [x for x in female_18[:-3].values] + [female_18.iloc[-3:-1].values.sum()] +\\\n                                [female_18.iloc[-1]]}).set_index('salary')\n\nfemale_comp_19 = pd.DataFrame({'salary': comb_idx,\n                                'count': [female_19.iloc[:6].values.sum(), female_19.iloc[6:8].values.sum(),\n                                 female_19.iloc[8:10].values.sum()] + [x for x in female_19[10:].values]}).set_index('salary')\n\nfemale_comp_20 = pd.DataFrame({'salary': comb_idx,\n                                'count': [female_20.iloc[:6].values.sum(), female_20.iloc[6:8].values.sum(),\n                                 female_20.iloc[8:10].values.sum()] + [x for x in female_20[10:].values]}).set_index('salary')\n\n\nmale_comp_18 = round(male_comp_18\/male_comp_18.sum(), 4)*100\nmale_comp_19 = round(male_comp_19\/male_comp_19.sum(), 4)*100\nmale_comp_20 = round(male_comp_20\/male_comp_20.sum(), 4)*100\n\nmale_comp = pd.concat([male_comp_18, male_comp_19, male_comp_20], axis=1)\nmale_comp.columns = [2018, 2019, 2020]\n\nfemale_comp_18 = round(female_comp_18\/female_comp_18.sum(), 4)*100\nfemale_comp_19 = round(female_comp_19\/female_comp_19.sum(), 4)*100\nfemale_comp_20 = round(female_comp_20\/female_comp_20.sum(), 4)*100\n\nfemale_comp = pd.concat([female_comp_18, female_comp_19, female_comp_20], axis=1)\nfemale_comp.columns = [2018, 2019, 2020]\n\n\nmale_comp_dict = {'0-100,000': list(male_comp.iloc[:10,:].sum()), '100,000-250,000': list(male_comp.iloc[10:14,:].sum()),\n                  '250,000+': list(male_comp.iloc[14:,:].sum())}\n\n\nfemale_comp_dict = {'0-100,000': list(female_comp.iloc[:10,:].sum()),\n                    '100,000-250,000': list(female_comp.iloc[10:14,:].sum()),\n                    '250,000+': list(female_comp.iloc[14:,:].sum())}\nmale_comp_df = pd.DataFrame(male_comp_dict, index=[2018, 2019, 2020]).T\nfemale_comp_df = pd.DataFrame(female_comp_dict, index=[2018, 2019, 2020]).T\n\n\n\nfig, ax = plt.subplots(figsize=(20, 6))\nexplode = (0, 0, 0.1)\nstart_angle = 60\ncolors = ['indianred', 'darkslateblue', 'seagreen']\n\npatch1 = mpatches.Patch(color='indianred', label='0-100,000')\npatch2 = mpatches.Patch(color='darkslateblue', label='100-250,000')\npatch3 = mpatches.Patch(color='seagreen', label='250,000+')\n\nplt.subplot(121)\ncomp_range = list(male_comp_df.index.values)\ncomp_values = list(male_comp_df[2020].values)\nplt.pie(comp_values, wedgeprops=dict(width=0.5), startangle=start_angle, autopct='%1.1f%%', colors=colors,\n        pctdistance=0.87, textprops=dict(color=\"black\", fontsize='large'), radius=2)\n\ncomp_values = list(male_comp_df[2019].values)\nplt.pie(comp_values, wedgeprops=dict(width=0.5), startangle=start_angle, autopct='%1.1f%%', colors=colors,\n        pctdistance=0.82, textprops=dict(color=\"black\", fontsize='large'), radius=1.5)\n\ncomp_values = list(male_comp_df[2018].values)\nplt.pie(comp_values, wedgeprops=dict(width=0.5), startangle=start_angle, autopct='%1.1f%%', colors=colors,\n        pctdistance=0.73, textprops=dict(color=\"black\", fontsize='large'), radius=1)\n\nplt.legend(title = \"Yearly compensation\", handles=[patch1, patch2, patch3], loc=(1.3,1))\nplt.title(\"Male\", fontsize='x-large', pad=-165)\n\nplt.subplot(122)\ncomp_range = list(female_comp_df.index.values)\ncomp_values = list(female_comp_df[2020].values)\nplt.pie(comp_values, wedgeprops=dict(width=0.5), startangle=start_angle, autopct='%1.1f%%', colors=colors,\n        pctdistance=0.87, textprops=dict(color=\"black\", fontsize='large'), radius=2)\n\ncomp_values = list(female_comp_df[2019].values)\nplt.pie(comp_values, wedgeprops=dict(width=0.5), startangle=start_angle, autopct='%1.1f%%', colors=colors,\n        pctdistance=0.82, textprops=dict(color=\"black\", fontsize='large'), radius=1.5)\n\ncomp_values = list(female_comp_df[2018].values)\nplt.pie(comp_values, wedgeprops=dict(width=0.5), startangle=start_angle, autopct='%1.1f%%', colors=colors,\n        pctdistance=0.73, textprops=dict(color=\"black\", fontsize='large'), radius=1)\nplt.title(\"Female\", fontsize='x-large', pad=-165)\n\n# Add year text to male donut charts\nplt.text(-4.85, 0.75, '2018', fontsize='x-large')\nplt.text(-4.85, 1.25, '2019', fontsize='x-large')\nplt.text(-4.85, 1.7, '2020', fontsize='x-large')\n\n# Add year text to female donut charts\nplt.text(-0.15, 0.75, '2018', fontsize='x-large')\nplt.text(-0.15, 1.25, '2019', fontsize='x-large')\nplt.text(-0.15, 1.7, '2020', fontsize='x-large')\n\nplt.show()","9416ccde":"comb_df_19 = multiplechoice_19.groupby(['Q6_What is the size of the company where you are employed?', 'Q7_Approximately how many individuals are responsible for data science workloads at your place of business?']).count().iloc[:,0]\ncomb_df_20 = multiplechoice_20.groupby(['Q20_What is the size of the company where you are employed?', 'Q21_Approximately how many individuals are responsible for data science workloads at your place of business?']).count().iloc[:,0]\nindex_19 = ['0-49 employees', '50-249 employees', '250-999 employees', '1000-9,999 employees', '> 10,000 employees']\nindex_20 = ['0-49 employees', '50-249 employees', '250-999 employees', '1000-9,999 employees', '10,000 or more employees']\nindex = ['0-49', '50-249', '250-999', '1000-9,999', '> 10,000']\ncomb_df_19 = comb_df_19.unstack().reindex(index_19)\ncomb_df_20 = comb_df_20.unstack().reindex(index_20)\n\ncomb_df_19.columns = ['0', '1-2', '3-4', '5-9', '10-14', '15-19', '20+']\ncomb_df_19['1-10'] = comb_df_19['1-2'] + comb_df_19['3-4'] + comb_df_19['5-9']\ncomb_df_19['10+'] = comb_df_19['10-14'] + comb_df_19['15-19'] + comb_df_19['20+']\ncomb_df_19 = comb_df_19[['0', '1-10', '10+']].reset_index()\n\n# Convert numbers to percentages\ncomb_df_19 = comb_df_19.loc[:,\"0\":\"10+\"].div(comb_df_19.sum(axis=1), axis=0)\ncomb_df_19 = round(comb_df_19*100, 2)\ncomb_df_19.index = index\n\ncomb_df_20.columns = ['0', '1-2', '3-4', '5-9', '10-14', '15-19', '20+']\ncomb_df_20['1-10'] = comb_df_20['1-2'] + comb_df_20['3-4'] + comb_df_20['5-9']\ncomb_df_20['10+'] = comb_df_20['10-14'] + comb_df_20['15-19'] + comb_df_20['20+']\ncomb_df_20 = comb_df_20[['0', '1-10', '10+']].reset_index()\n\n# Convert numbers to percentages\ncomb_df_20 = comb_df_20.loc[:,\"0\":\"10+\"].div(comb_df_20.sum(axis=1), axis=0)\ncomb_df_20 = round(comb_df_20*100, 2)\ncomb_df_20.index = index\n\npatch1 = mpatches.Patch(color='sienna', label='0')\npatch2 = mpatches.Patch(color='olive', label='1-10')\npatch3 = mpatches.Patch(color='slategrey', label='10+')\n\nfig, ax = plt.subplots(figsize=(20,6))\nplt.subplot(121)\nsns.pointplot(x=index, y=\"0\", data=comb_df_19, color= 'sienna')\nsns.pointplot(x=index, y=\"1-10\", data=comb_df_19, color= 'olive')\nsns.pointplot(x=index, y=\"10+\", data=comb_df_19, color= 'slategrey')\nplt.xticks(rotation=45)\nplt.ylabel('Percentage', fontsize = 'large')\nplt.xlabel('Company Size', fontsize = 'large')\nplt.legend(title = \"Data Science team size\", handles=[patch1, patch2, patch3])\nplt.title('Company size vs Data Science team size - 2019', fontsize = 'large')\n\nplt.subplot(122)\nsns.pointplot(x=index, y=\"0\", data=comb_df_20, color= 'sienna')\nsns.pointplot(x=index, y=\"1-10\", data=comb_df_20, color= 'olive')\nsns.pointplot(x=index, y=\"10+\", data=comb_df_20, color= 'slategrey')\nplt.xticks(rotation=45)\nplt.ylabel('Percentage', fontsize = 'large')\nplt.xlabel('Company Size', fontsize = 'large')\nplt.legend(title = \"Data Science team size\", handles=[patch1, patch2, patch3])\nplt.title('Company size vs Data Science team size - 2020', fontsize = 'large')\nplt.show()","f7da1b7c":"source = multiplechoice_20.iloc[:,244:254]\nfor col in source.columns:\n    source[col] = source[col].value_counts()[0]\nsrc_name = [col.split('Choice - ')[1].split(' (')[0] for col in source.columns]\nsource.columns = src_name\nsource = source.drop_duplicates().T\nsource = source.sort_values(by=1, ascending=False)\n\nplatform =  multiplechoice_20.iloc[:,231:241]\nfor col in platform.columns:\n    platform[col] = platform[col].value_counts()[0]\nplt_name = [col.split('Choice - ')[1].split(' (')[0] for col in platform.columns]\nplatform.columns = plt_name\nplatform = platform.drop_duplicates().T\nplatform = platform.sort_values(by=1, ascending=False)\n\nfig, ax = plt.subplots(figsize=(20,6))\nplt.subplot(121)\nbar1 = plt.bar(x=source.index.values, height=[x[0] for x in source.values], color='chocolate')\nplt.xticks(rotation=90)\nplt.ylabel('Count', fontsize = 'large')\nplt.xlabel('Sources followed', fontsize = 'large')\nplt.title('Sources followed for learning Data Science', fontsize = 'large')\nfor rect in bar1:\n    height = rect.get_height()\n    plt.text(rect.get_x() + rect.get_width()\/2.0, height, f'{height}', ha='center', va='bottom')\n\nplt.subplot(122)\nbar2 = plt.bar(x=platform.index.values, height=[x[0] for x in platform.values], color='chocolate')\nplt.xticks(rotation=90)\nplt.ylabel('Count', fontsize = 'large')\nplt.xlabel('Platforms used', fontsize = 'large')\nplt.title('Platforms used for learning Data Science', fontsize = 'large')\nfor rect in bar2:\n    height = rect.get_height()\n    plt.text(rect.get_x() + rect.get_width()\/2.0, height, f'{height}', ha='center', va='bottom')\n\nplt.show()","7aea800a":"prog_lang = multiplechoice_20.iloc[:,6:18]\nprog_lang.columns = ['Coding exp'] + [x.split('Choice -')[1].split(' (')[0] for x in prog_lang.columns[1:]]\nprog_lang = prog_lang.reindex(list(prog_lang['Coding exp'].dropna().index))\nprog_lang = prog_lang.groupby('Coding exp').count().iloc[:-1].reindex(['< 1 years', '1-2 years', '3-5 years', '5-10 years', '10-20 years', '20+ years'])\nprog_lang = round(prog_lang.div(prog_lang.sum(axis=1), axis=0)*100, 2)\n\nfig, ax = plt.subplots(figsize=(16,8))\nsns.heatmap(prog_lang, annot= True, fmt=\".2f\", linewidths=.5, cmap='YlOrBr')\nplt.yticks(rotation=0)\nplt.xlabel('Programming Languages', fontsize = 'large')\nplt.ylabel('Coding Experience', fontsize = 'large')\nplt.title('Programming Languages used v\/s Coding Experience', fontsize = 'large')\nplt.show()","b3a5b17b":"lang_recom = multiplechoice_20.iloc[:,20].value_counts().to_frame().drop(index=['Other','None'])\nlang_recom.columns = ['Language Recommended']\nlang_recom['Language Recommended'] = round((lang_recom['Language Recommended']\/lang_recom['Language Recommended'].sum())*100, 2)\n\nplt.figure(figsize=(16,8))\nbar = plt.bar(x=lang_recom.index, height=[x[0] for x in lang_recom.values], color = 'chocolate')    \nplt.yscale('log')\nplt.xlabel('Programming Languages', fontsize = 'large')\nplt.ylabel('Percentage', fontsize = 'large')\nplt.title('Languages Recommended', fontsize = 'x-large', fontweight = 'roman')\nfor rect in bar:\n    height = rect.get_height()\n    plt.text(rect.get_x() + rect.get_width()\/2.0, height, f'{height}%', ha='center', va='bottom')\n    \nplt.show()","8c73454c":"ide = multiplechoice_20.iloc[:,[6] + list(range(22,31))]\nide.columns = ['Coding exp'] + [x.split('Choice -')[1].split(' (')[0] for x in ide.columns[1:]]\nide = ide.reindex(list(ide['Coding exp'].dropna().index))\nide = ide.groupby('Coding exp').count().iloc[:-1].reindex(['< 1 years', '1-2 years', '3-5 years', '5-10 years', '10-20 years', '20+ years'])\nide = round(ide.div(ide.sum(axis=1), axis=0)*100, 2)\nide.columns = ['RStudio', 'Visual Studio \/ VS Code', 'Jupyter', 'PyCharm', 'Spyder', 'Notepad++', 'Sublime Text', 'Vim \/ Emacs', 'MATLAB']\n\nfig, ax = plt.subplots(figsize=(16,8))\nsns.heatmap(ide, annot= True, fmt=\".2f\", linewidths=.5, cmap='YlOrBr')\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.xlabel('IDE', fontsize = 'large')\nplt.ylabel('Coding Experience', fontsize = 'large')\nplt.title('IDE used v\/s Coding Experience', fontsize = 'large')\nplt.show()","b3e2fb69":"db = multiplechoice_20.iloc[:,[6] + list(range(155,171))]\ndb.columns = ['Coding exp'] + [x.split('Choice -')[1].split(' (')[0] for x in db.columns[1:]]\ndb = db.reindex(list(db['Coding exp'].dropna().index))\ndb = db.groupby('Coding exp').count().iloc[:-1].reindex(['< 1 years', '1-2 years', '3-5 years', '5-10 years', '10-20 years', '20+ years'])\ndb = round(db.div(db.sum(axis=1), axis=0)*100, 2)\ndb.columns = ['MySQL', 'PostgresSQL', 'SQLite', 'Oracle Database', 'MongoDB', 'Snowflake', 'IBM Db2',\n'MS SQL Server', 'MS Access', 'Azure Data Lake Storage', 'Amazon Redshift', 'Amazon Athena', 'Amazon DynamoDB',\n'Google Cloud BigQuery', 'Google Cloud SQL', 'Google Cloud Firestore']\n\nfig, ax = plt.subplots(figsize=(16,8))\nsns.heatmap(db, annot= True, fmt=\".2f\", linewidths=.5, cmap='YlOrBr')\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.xlabel('Database', fontsize = 'large')\nplt.ylabel('Coding Experience', fontsize = 'large')\nplt.title('Database used v\/s Coding Experience', fontsize = 'large')\nplt.show()","640cf1ec":"vis_lib_19 = multiplechoice_19.iloc[:,97:107]\nvis_lib_19.columns = [x.split('Choice -  ')[1].split(' (')[0] for x in vis_lib_19.columns]\nfor col in vis_lib_19.columns:\n    vis_lib_19[col] = vis_lib_19[col].value_counts()[0]\nvis_lib_19 = vis_lib_19.drop_duplicates().T\nvis_lib_19 = vis_lib_19.sort_values(by=1)[::-1]\nvis_lib_19[1] = round(vis_lib_19[1]\/vis_lib_19[1].sum()*100, 2)\n\nvis_lib_20 = multiplechoice_20.iloc[:,53:63]\nvis_lib_20.columns = [x.split('Choice -  ')[1].split(' (')[0] for x in vis_lib_20.columns]\nfor col in vis_lib_20.columns:\n    vis_lib_20[col] = vis_lib_20[col].value_counts()[0]\nvis_lib_20 = vis_lib_20.drop_duplicates().T\nvis_lib_20 = vis_lib_20.sort_values(by=1)[::-1]\nvis_lib_20[1] = round(vis_lib_20[1]\/vis_lib_20[1].sum()*100, 2)\n\nplt.figure(figsize=(20,6))\nplt.subplot(121)\ncolor_map_19 = ['seagreen']*4 + ['indianred'] + ['darkslateblue'] + ['seagreen'] + ['darkslateblue'] + ['seagreen']*2\nbar1 = plt.bar(x=vis_lib_19.index, height=[x[0] for x in vis_lib_19.values], color=tuple(color_map_19[::-1]))\ncustom_lines_19 = [Line2D([0], [0], color='seagreen', lw=4, label='Python'), Line2D([0], [0], color='indianred',\n                                                                                  lw=4, label='Javascript'),\n                Line2D([0], [0], color='darkslateblue', lw=4, label='R')]\nplt.legend(['Python', 'Javascript', 'R'], handles = custom_lines_19, title = 'Programming Language', title_fontsize = 'large')\nplt.xticks(rotation=90)\nplt.xlabel('Visualization Libraries', fontsize = 'large')\nplt.ylabel('Percentage', fontsize = 'large')\nplt.title('Visualization Libraries used - 2019', fontsize = 'large')\nfor rect in bar1:\n    height = rect.get_height()\n    plt.text(rect.get_x() + rect.get_width()\/2.0, height, f'{height}%', ha='center', va='bottom')\n\n\nplt.subplot(122)\ncolor_map_20 = ['seagreen']*2 + ['indianred'] + ['seagreen']*2 + ['darkslateblue']*2 + ['seagreen']*3\nbar2 = plt.bar(x=vis_lib_20.index, height=[x[0] for x in vis_lib_20.values], color=tuple(color_map_20[::-1]))\ncustom_lines_20 = [Line2D([0], [0], color='seagreen', lw=4, label='Python'), Line2D([0], [0], color='indianred',\n                                                                                  lw=4, label='Javascript'),\n                Line2D([0], [0], color='darkslateblue', lw=4, label='R')]\nplt.legend(['Python', 'Javascript', 'R'], handles = custom_lines_20, title = 'Programming Language', title_fontsize = 'large')\nplt.xticks(rotation=90)\nplt.xlabel('Visualization Libraries', fontsize = 'large')\nplt.ylabel('Percentage', fontsize = 'large')\nplt.title('Visualization Libraries used - 2020', fontsize = 'large')\nfor rect in bar2:\n    height = rect.get_height()\n    plt.text(rect.get_x() + rect.get_width()\/2.0, height, f'{height}%', ha='center', va='bottom')\n    \nplt.show()","6d49ee98":"ml_alg = multiplechoice_20.iloc[:,[65] + list(range(82,92))]\nml_alg.columns = ['ML exp'] + [x.split('Choice -')[1].split(' (')[0] for x in ml_alg.columns[1:]]\nml_alg = ml_alg.reindex(list(ml_alg['ML exp'].dropna().index))\nml_alg = ml_alg.groupby('ML exp').count().iloc[:-1].reindex(['< 1 years', '1-2 years', '2-3 years', '3-4 years', '4-5 years',\n                                                             '5-10 years', '10-20 years', '20 or more years'])\nml_alg = ml_alg.fillna(0).astype('int').iloc[1:]\nml_alg = round(ml_alg.div(ml_alg.sum(axis=1), axis=0)*100, 2)\n\nfig, ax = plt.subplots(figsize=(16,8))\nsns.heatmap(ml_alg, annot= True, fmt=\".2f\", linewidths=.5, cmap='YlOrBr')\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.xlabel('ML Algorithms', fontsize = 'large')\nplt.ylabel('ML Experience', fontsize = 'large')\nplt.title('ML Algorithms used v\/s ML Experience', fontsize = 'large')\nplt.show()","d4dca231":"ml_fw = multiplechoice_20.iloc[:,65:80]\nml_fw.columns = ['ML exp'] + [x.split('Choice -')[1].split(' (')[0] for x in ml_fw.columns[1:]]\nml_fw = ml_fw.reindex(list(ml_fw['ML exp'].dropna().index))\nml_fw = ml_fw.groupby('ML exp').count().iloc[:-1].reindex(['< 1 years', '1-2 years', '2-3 years', '3-4 years', '4-5 years',\n                                                           '5-10 years', '10-20 years', '20 or more years'])\nml_fw = ml_fw.fillna(0).astype('int').iloc[1:]\nml_fw = round(ml_fw.div(ml_fw.sum(axis=1), axis=0)*100, 2)\n\nfig, ax = plt.subplots(figsize=(16,8))\nsns.heatmap(ml_fw, annot= True, fmt=\".2f\", linewidths=.5, cmap='YlOrBr')\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.xlabel('ML Frameworks', fontsize = 'large')\nplt.ylabel('ML Experience', fontsize = 'large')\nplt.title('ML Frameworks used v\/s ML Experience', fontsize = 'large')\nplt.show()","ec9ff2f8":"cloud_plat = multiplechoice_20.iloc[:,[6] + list(range(120,130))]\ncloud_plat.columns = ['Coding exp'] + [x.split('Choice -')[1].split(' (')[0] for x in cloud_plat.columns[1:]]\ncloud_plat = cloud_plat.reindex(list(cloud_plat['Coding exp'].dropna().index))\ncloud_plat = cloud_plat.groupby('Coding exp').count().iloc[:-1].reindex(['< 1 years', '1-2 years', '3-5 years', '5-10 years', '10-20 years', '20+ years'])\ncloud_plat = round(cloud_plat.div(cloud_plat.sum(axis=1), axis=0)*100, 2)\n\nfig, ax = plt.subplots(figsize=(16,8))\nsns.heatmap(cloud_plat, annot= True, fmt=\".2f\", linewidths=.5, cmap='YlOrBr')\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.xlabel('Cloud Platforms', fontsize = 'large')\nplt.ylabel('Coding Experience', fontsize = 'large')\nplt.title('Cloud Platforms used v\/s Coding Experience', fontsize = 'large')\nplt.show()","87032734":"#### **f. Can Data Science be moved into the cloud?**\n\nCloud services is a rapidly growing market. Modern technologies like big data analytics, IoT, artificial intelligence and even web and mobile app hosting requires heavy computing power. Cloud computing offers enterprises an alternative to building their in-house infrastructure. **With cloud computing, anybody using the internet can enjoy scalable computing power on a plug and play basis**. This saves organizations from the need to invest and maintain costly infrastructure, and hence has become a very popular solution. \n\nApart from just the oraganizational point of view, cloud resources are also useful for us developers. With new hardwares coming out every few months, it is difficult to keep our local machines updated. The cloud solutions saves you time and money from investing on these modern hardwares. Many of us frequently use cloud resources such as Google Colab or Kaggle notebooks for conducting various studies or analyses and that too at free of cost. This again highlights the importance of cloud resources at present. Let us now find out the common cloud services used within data science.","c6188a9b":"### 1. A look at the survey data over the years  \n\nAn easy way to understand the demand for data science is to look at the number of respondents in the survey. We can easily say that, more the number of respondents, more the demand will be. \n\nThe basic survey stats are shown below:","fcceff49":"From 2017, **Kaggle**, one of the biggest online community of data scientists and machine learning practioners, have been conducting an industry-wide survey that presents a truly comprehensive view of the state of data science and machine learning. While not all Data Scientists took part in Kaggle survey, and not all survey participants do work in data science, it is reasonable to assume a large overlap. The survey is conducted yearly, usually within a time period of about 3 weeks, and later on, the survey data is made publicly available. The survey data from 2017 to 2020 is used within this study. \n\n> On two occasions I have been asked, \"Pray, Mr. Babbage, if you put into the machine wrong figures, will the right answers come out?\". I am not able rightly to apprehend the kind of confusion of ideas that could provoke such a question.       \n-\u2009**Charles Babbage, Passages from the Life of a Philosopher**\n\nWhat **Charles Babbage** said, refers to **GIGO (garbage in, garbage out)**. GIGO is an important concept in computer science and mathematics which implies that *the quality of output is determined by the quality of input*. The same applies to this study, where all the discoveries made here are based on the survey data only and the accuracy of the results are greatly affected by the accuracy of the survey responese. So let us begin by taking a look at the basic stats of the survey.","67c18fb9":"Now, that you have seen the languages you should learn to have a good start, the next task is to find you some IDEs (Integrated development environment), using which you can keep practising and improving your coding skills. A good IDE helps you save time when writing long lines of codes and also helps you to properly manage projects and its associated files.","bcfb088e":"### **8. Data Science - Where and What to Learn?**\n\nThe demand for Data Science is on the rise and as evident from the survey data, more and more people as well as companies are taking a step forward towards this domain. The increasing demand encourages even more people to be a part of this. The following section is for those people who are interested to learn about the present trends in data science. Not all trends are covered here but the most important ones are :)\n\n![much_to_learn](https:\/\/media0.giphy.com\/media\/3ohuAxV0DfcLTxVh6w\/giphy.gif)","6ee39995":"* Among the top 10 visualization libraries common among the survey respondents, *seven of them belong to Python, two of them belong to R and the one remaining is that of Javascript*. **Matplotlib** and **Seaborn** (which is a high level interface for drawing attractive and informative statistical graphics based on matplotlib) are the most common libraries used. If you are into R, then **ggplot** is what you need to go for and, for those who like to do visualizations in javascript, **D3.js** is most used javascript library, atleast from the survey data.\n\n\n* 2019 and 2020 had similar number of respondents and so when comparing the data in two different years the number of matplotlib and seaborn users have increased a lot. This once again highlights the increasing demand for python as a programming language for data science. Similar trend can be seen with the number of Plotly users overtaking the ggplot library users and also the python libraries bokeh and geoplotlib being used more than D3.js.\n\n> Visualization is a key part of data science. A good visualization makes it easy to retrieve more information out of any data and this can eventually lead to better decision making. For those interested in visualization, the above mentioned libraries will be beneficial to create good quality charts. As seen from the visualization chart and previous sections, it appears that Python is a good place to start learning to code.","9e68150b":"Note - The values are in percentages\n\n* **Scikit-learn** is the most used ML framework. This ML framework contains a lot of the commonly used ML algorithms discussed above including Linear and Logistic regression, Decision trees, random forests and many more. As seen from the ML alogrithms chart, most inexperienced people start working with simple algorithms and that explains the greater use of scikit-learn and, as experience increases they move on to more complex methods and the usage of scikit-learn starts declining.\n\n\n* We have also observed from ML alogrithms chart that as people stay more in the ML field, more they start to use complex methods such as boosting algorithms. This can again be observed here from the increasing usage of Xgboost and LightGBM with increasing experience.\n\n\n* Deep learning has always been one of most dominant area in ML. Tensorflow, Keras and PyTorch are the most used DL frameworks. Even people with low levels of experience use these framewroks in plenty. These frameworks helps implement complex neural network architectures such as Convolutional Neural Networks, Recurrent Neural Networks, Deep Neural Networks etc. Even the state of art models such as transformers and GANs can be implemented with the help of these DL frameworks.\n\n> As previously mentioned, it is better off to start with something simple and built a strong foundation. The above chart guides you in finding that framework that you can start with. Similar to the ML algorithms, we can observe that for ML frameworks that, as experience increase, the variance of the percentage values decrease. Apart from just learning the ML algorithms, it is also important to learn how to use all these frameworks (atleast the most used ones by the community) to bring these algorithms on to your project or application.","4af83f7e":"# **Does Data Science have a Future?**","d15f77c6":"**Note** - *The number of respondents are in log scale*\n\n* **Python is the most used and recommended language**. Python has now become one of the most popular coding languages in the world. The differentiating factor that Python brings to the table is that it enables programmers to flesh out concepts by writing less and readable code. The developers can further take advantage of several Python frameworks to mitigate the time and effort required for building large and complex software applications. According to [GitHub\u2019s 2020 State of the Octoverse](https:\/\/octoverse.github.com\/), in 2019, for the first time, Python outranked Java as the second most popular language on GitHub and the rank still remains intact even today.\n\n\n* SQL ranks second among the languages used by the community (in the heatmap shown above) but, when you look at the list of recommended languages (bar chart), you find that people recommend learning R more. The R language is widely used among statisticians and data miners for developing statistical software and data analysis. SQL is another very important language you must learn to succeed in this field. SQL helps you to efficiently manage and query different databases to fetch and process data. The common databases used by the community can be found in the next section (Data Storage)\n\n\n> The heatmap and the bar chart provides you a list of programming languages used and recommended by the Kaggle community. It is important to keep up with these present trends and also attain enough skills in them to have a good future in data science.","f3a09a8a":"Once you learn atleast some of the ML algorithm mentioned above how do you implement these algorithms in your code? Should you keep writing long lines of code everytime to implement these algorithm in your work or are there some libraries or frameworks that you can use to incorporate ML in your project? \n\nYes, there do exist some Machine Learning frameworks that are easy to learn and use and helps bring intelligence to your application. ML Framework offer building blocks for designing, training and validating ML as well as deep neural networks, through a high level programming interface. Using these frameworks, the developers can more easily and quickly build and deploy ML models, without getting into the details of the underlying algorithms. Let us explore the commonly used frameworks.","d08c2af9":"**Note** - *The analysis ignores those who do not wish to disclose their yearly compensation*.\n\n* The analysis compares the yearly compensation of the respondents in 2018, 2019 and 2020. The graph is a pleasent sight for all those who are interested in the pay.\n\n\n* The *percentage of respondents is either the same or increased in all compensation brackets except in a few from 2019 - 2020*. Amidst the worldwide pandemic and the economy affected badly, it is relaxing to see that the data science field remains ever stronger.\n\n\n* It is delightful to take a look at the percentage of respondents with high pay, especially 100k and above yearly. The percentage of them have increased a lot from around 12% in 2018 to around 20% in 2020, that is, **every 1 out of 5 respondents earn over 100k yearly!!**\n\n\n* The percentage of those who earn less than 10k yearly dropped from 30% in 2018 to less than 28% in 2020. As a summary, percentage of those who earn less went down and, the percentage of those who earn more rocketed. **Data Science is indeed the sexiest job in terms of yearly compensation** and the demand for people in DS is still high!!\n\n\n> In 2020, the year affected by pandemic, the salary compensation has went down in few salary groups, but has stayed constant in most of the salary groups, especially in the higher salary groups of 100k+. This again shows that the demand for skillfull data scientist are still high and also they are getting paid sufficiently even when the world economy has been affected badly. This is yet another confirmation for the demand for the field.\n\n\n**Another interesting observation is to find out whether males and females are paid equally within the data science realm. Lets find out**","f6681d81":"The chart shows the percentage of people belonging to different age groups, for both males and females from 2017 to 2020. Here we have divided the age-groups into three:  \n>    18-30 -> young population  \n    30-45 -> middle-age population  \n    45+ -> experienced population\n\n\n* The age group 18-30 makes up over 50% of the total population. In males, the young population rose from 45 in 2017 to 55.6% in 2020 and in females, it rose from 64.2% to 73.8%. Most of the female population (over 73%) are young and this will definetly improve the sex-ratio in the coming years. \n\n\n* The middle-age population dropped a lot for both males and females. The experienced population kind of remains constant for males while it declines a lot (from 9.1 to 5.4%) for females. We might see a change in the trend in future with more young people joining.","d761d1a4":"* **Young people (less than 30 year olds) makes up over 50% of the respondents**. The age group 18-21 represents the student population. Currently, they make up around 15% of the respondents. The *youth population in DS have seen a massive surge* from under 5% in 2017 to around 15% in 2018 and 2019 and further upto around 18% in 2020. Youth represent the future and this is indeed a great sign.\n\n\n* Now, move a bit down along the y-axis and have a look at the older population of respondents that belong to the age group of 45+. They represent the **experts** in their respective fields with over 15+ years of experience. The percentage of respondents in this age group mostly remain the same in the past year (2020). This is great news, if the experienced are to switch to DS or stay with it, then they might definetly see a great future here. It is also good to find that more older people are staying healthy enough to participate in the survey even in midst of the pandemic.\n\n\n* The percentage of respondents dropped a lot between the age of 25 and 45 from 2017 to 2020. The increase in student population makes the middle age population appears small (the total percentage can only add up to be 100). Also, it is most likely that the people in this age group are either in search for better job oppurtunities or are starting a family life and are either too busy with their lives not having enough time to spend on Kaggle or are struggling to find ground in DS domain, the former looks the better reason though.\n\n\n> Comparing the participation of people belonging to different age groups in the survey, it is great to find that more young people (especially students) are into data science. This is the best sign for its increasing demand. Also the experienced population remains quite intact. Thus we can say that data science is here to stay for quite long\n\n**Like mentioned, the youth population is the one for the future. We have also seen that the women population has been on a rise, so does young women have their share in data science? How does gender affect the different age groups? Lets explore them.**","bc7d1176":"### **5. What qualification do you need?**\n\n![education](https:\/\/s3.ap-southeast-1.amazonaws.com\/images.deccanchronicle.com\/dc-Cover-bsnudco08r3igtj44duecnr7m4-20180630063055.Medi.jpeg)\n\n**Education is a weapon to improve one\u2019s life. It is probably the most important tool to change one\u2019s life**. Education is very important to land in a good job and to have excellent compensation. Education qualification is a measure of what you have learned and, the more qualified you are more will be the oppurtunities.\n\nDo you need higher degree of qualification to do Data Science or can you start early? Generally, the people having higher education should be able to contribute more to the field (there are exceptions though) and so, more educated the person is, better the future data science has. The survey responses helps to understand that.","8a77a5ae":"The figure highlights the growth of data over the past few years. It was no longer possible to manually look at the data and predict the trend or learn about it. As the data grew at an exponential pace, the demand for tools to process these huge piles of data surged.\n\nThe large volume of unstructured data required more complex and advanced analytical tools and algorithms for processing, analyzing and drawing meaningful insights out of it. **Data Science was the answer to this problem!** The idea of Data Science came into existence before 2000s, but it is only recently that, with the discovery of new algorithms and analytical tools and huge data volumes that, Data Science has gained all the popularity.\n\nMarked as the **highest paying job** in the year 2016 by **Glassdoor**, the field of Data science has witnessed an immense growth in recent years. Employers are in the search of data scientists more than ever. A report by **Indeed** indicated a 29% increase in the demand of data scientists in a year.\n\nIs the situation the same today, or was the demand for Data Science short-lived? This study is to find more about that from the Kaggle survey data as well as to learn the present trends seen in the field now.\n\n![DS_Demand](https:\/\/www.thebluediamondgallery.com\/wooden-tile\/images\/demand.jpg)","d308a783":"### **6. What do DS people earn??**\n\n![salary](http:\/\/laschoolreport.com\/wp-content\/uploads\/2014\/09\/Teacher-salary-LAUSD.jpg)\n\nThis is perhaps the most important question. Data Science is viewed as the sexiest job of the 21st century. The growing demand of Data Science is what earned it that title. As the demand for it increases, so should the job openings and pay. Good pay is what attracts the people the most.\n\nIs Data Science the sexiest job in terms of pay? As more and more people entered this domain, what impact did that had on the pay scale?","659a2401":"From the above analysis, we can conclude the following:\n\n* The number of respondents in the ML & DS survey **increased by about 43%** from 16716 in 2017 to 23859 in 2018. 2018 was the year when the first competition on the kaggle survey data was conducted. That might be one of the reasons for around 43% increase in survey participants. This is indeed an indicator of the increasing interest towards Data Science.\n\n\n* The number of participants then dropped to 19717 in 2019 but it was up slightly to 20036 in 2020. After the first competition, maybe the demand went down a bit leading to lower participation in survey. The increase in the participation in 2020, even in the hard time of pandemic is yet another indicator for the increasing demand for DS.\n\n\n* The median response time is *16.4 min* in 2017 survey, *17 min* in 2018 survey, *9 min* in 2019 and *10.4 min* in 2020 survey. Looking at the number of questions, it is clear that the response time is almost proportional to the number of questions. More the number of questions, more the response time.\n\n\n* Larger median survey time means that most respondents have spent time reading and understanding the survey questions before answering them. **Thus we can consider the survey data and hence the insights derived from it to be genuine**.","43d11372":"![beg_and_end](https:\/\/y.yarn.co\/96b749d9-3268-4274-96ae-62c5ff3470b8_text.gif)\n\n**\"Everything that has a beginning, has an end\"** - Everyday people come up with new ideas and innovations that shape our future. Every discoveries and inventions are for the sole purpose of making our lives easier and better. With smart devices such as Alexa, you don't even need to get up to switch off the lights or to turn on the music. As new inventions are born, the old ones slowly fades away from existence.\n\nThe discovery of new scientific methods, algorithms and the invention of new and powerful hardwares have increased the popularity of Data Science. At present, Data Science(DS) is one among the most used words. So, *what is Data Science and what is the need for such a field of study?* \n\nIn simple words, **Data Science** is a multi-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data. As the world progressed into an era of big data, data started into pour in at tremendous rates and a lot of focus turned to storing these huge data volumes. Also, it was impossible to properly label or structure all these data due to their immense volume. So, today, most of stored data are kept as either unstructured or semistructured as shown in the following figure.\n\n![data](https:\/\/miro.medium.com\/max\/893\/1*JeIC6PreHjgh06w3WqkXMA.jpeg)","94ad0358":"Note - The values are in percentages\n\n* RDBMS databases such as MySQL, PostgresSQL etc are the most commonly used ones among the community. RDBMS is a database management system designed for relational databases. NoSQL databases such as MongoDB are also popular. MongoDB is a non-relational, document oriented database management system and works on document based database. The usage of MongoDB is almost the same among all experience groups.\n\n\n* **MySQL is the most used DBMS** which is then followed by PostgresSQL and MS SQL Server. According to the [survey](https:\/\/insights.stackoverflow.com\/survey\/2020\/#technology) conducted by stack overflow, the top three DBMS among developers are MySQL, PostgresSQL and MS SQL Server. PostgreSQL is gaining lots of traction in the last few years. Developers working with Postgres are very pleased with the product, both in terms of capabilities and performance.\n\n\n* As experience rises, the dependency on MySQL drops. The usage of Postgres and MS SQL Server are found to be increasing with the increase in experience. Among cloud services, Google Cloud BigQuery seems to be the dominant database.\n\n\n> In short, databases are not just a medium for data storage, but are also a useful tool to efficiently query and process the data. So, acquiring a good database knowledge is important to have a good growth in data science.","b281209f":"* The good news is that, most of the companies (even smaller ones) have a Data Science team within them. Every company aspires to be a part of this ever growing domain. As can be observed from the chart, bigger the company, bigger the probability of finding a DS team and larger the team size.\n\n\n* There is about 90% chance of finding a data science team and more than 50% chance of finding a big team of over 10 employees, within a company that has atleast 50 employees. \n\n\n* Even for smaller companies, having less than 50 employees, the chance of finding a DS team is high (more than 60%). These companies mostly have a smaller DS team, usually with a team strength of 1-10 employees.\n\n\n* When comparing the data in 2019 and 2020, it is sad to find that the number of respondents working have reduced in the present year, especially in the small companies. For small companies with less than 50 employees, the number of companies not having a DS team increased from over 20% to 30%. This can be due to the pandemic which is responsible for a lot of job loss in the year. Also, the team size has reduced for smaller companies in 2020.\n\n\n* Large companies with over 250 employees seems mostly unaffected by the pandemic with the data points looking identical for both 2019 as well as 2020. Almost 60% of these large companies have a good data science team strength within them.\n\n\n> So, it can inferred that data science is a higly demanding field that is attracting more and more companies towards it. The pandemic has had an impact on smaller companies but with large corporates remaining intact, it again highlights the importance of data scientists in the present world.","6c49ba72":"#### **a. Learn Data Science concepts**\n\nNobody ever talks about motivation in learning. Data science is a broad and fuzzy field, which makes it difficult to learn. Without proper motivation, you\u2019ll end up stopping halfway through and believing you can\u2019t do it. **So, how do people survive here despite all these troubles and what keeps them motivated to go even further? And most importantly, how do you get started?**","ae3d6b46":"**Note** -*The y-axis of the histograms are in the log scale.*\n  \n  \n* The histograms and kde plots shows the distribution of the response time for the survey. It is quite surprising to find that there a few people that spend many hours on the survey.\n\n\n* As expected, the number of respondents decrease exponentially as we move right, along the survey time axes but unexpectedly, there is a small spike around 50hrs in 2018 and 2020 survey and around 150hrs in 2019 survey!!! The cause is unknown. \n\n**Note** - *As you have seen from the above analyses, the number of respondents is different for all the four time periods the survey was coducted. So, for all the following studies involving comparison of the survey data belonging to these time periods, the normalized values (here percentage is used) of the data is used rather than count, for meaningful analysis.*","4fa746bb":"The chart shows the percentage of people belonging to different salary groups, for both males and females from 2018 to 2020. Here we have divided the salary-groups into three:  \n>    0-100,000 -> low salary group  \n    100,000-250,000 -> medium salary group  \n    250,000+ -> high salary group  \n\n* It can easily be observed that there are a lot more women in the lower salary group than in the higher salary group, when compared to men. It can either be due to the fact that women are paid less or it may also be because most of the women repsondents belonged to the young age group as highlighted under the *Age section* above. Anyway the difference isn't too big as expected and we can expect that both charts might possibly look identical in near future\n\n\n* When observing the population that earns over 100k (blue and the green slice), the increase in their number is clear from 2018 to 2020. Also looking at the 2019 and 2020 data, for the same group, the percentage of men dropped from 19.1% to 17.8% and for females the percentage went down to 14.4% from 15.5%. Hence we can say that both the genders were equally affected by the pandemic.\n\n\n> In general, the people who earn more has increased and those who earn less has decreased which is exactly what a field in demand looks like. The share of income is similar for both men and women, eventhough there is small difference, and it might get resolved in the near future.","d05b36cc":"Note - The values are in percentages\n\n\n* **Amazon Web Services (AWS)** is most used cloud platform followed by Google Cloud Platform (GCP) and Microsoft Azure. Most of the other cloud platforms are hardly used. When observing the cloud platforms apart from Big 3 of AWS, GCP and Azure, you can find that as people have larger coding experience, they seldom prefer other cloud platforms apart from the Big 3. These platform are cheaper compared to the Big 3 and also provides a lot of features similar to that found in Big 3. That might be one of the reasons behind why they are chosen by the people having lesser experience (more likely the ones with lesser annual compensation as well).\n\n\n* AWS seems to be the best in the market and is a lot more popular among the experienced compared to its competitors GCP and Azure. Under AWS, Amazon provides on-demand cloud computing platforms like storage, data analysis, etc and also allows their subscribers to enjoy a full-fledged virtual cluster of computers, at any time, based on their requirements. The entire service is enabled through the internet. AWS's virtual cloud platform comes with most of the attributes of an actual computer including hardware (CPU(s) & GPU(s) for processing, hard-disk\/SSD for storage & local\/RAM for memory), an operating system to choose from and pre-loaded apps like web servers, databases, CRM, etc.\n\n\n* GCP seems to be equally popular among people with all sorts of experience. GCP offers services in all major spheres including compute, networking, storage, machine learning (ML) and the internet of things (IoT). It also includes tools for cloud management, security, and development. The Google Cloud Storage is a highly dynamic storage solution that supports both SQL (Cloud SQL) and NoSQL (Cloud Datastore) database storage.\n\n\n* Similar to AWS, the popualrity of Azure also increases with experience. Azure is used to deploy code on Microsoft's servers. At its core, Azure replaces or supplements your on-premise infrastructure. However, it also delivers a vast range of other services such as Azure Analytics, AI services, IoT etc that improve the functioning of several departments in your organization and help you resolve critical business problems. It also supports data, development, compute and network services.\n\n\n> Cloud platforms are one of the most influential factors that has lead to the growth in demand for data science. These platforms has enabled users all over the world access to latest hardware, bigger data storage and ofcousrse massive compute power. Services like AWS, GCP and Azure requires that you pay only depending on your usage and as a result of that cloud services are a lot cheaper than setting up the required infrastructure on-site. As mentioned at the start of this section, there are also few cloud resources such as Google Colab, Kaggle notebook etc that provides access to decent hardware free of cost. To conclude this, as the volume of data increases, so does the need for a better storage and powerful machines and hence it is important to understand the popular cloud platforms available.","2f74fdba":"## **Conclusion**\n\nWe started with a quote **\"Everything that has a beginning, has an end\"** and, as the saying goes, this study has reached its end. We have used the Kaggle survey data from 2017 to present to solve the uncertainity regarding the future of data science. From the study, we could perceive the increasing demand for data science and it is certain that this field will be around for many more years. \n\nWe have also looked into some of the most trending topics within data science which provides insight into how, what and when one should start learning these topics. So, better start from the basic components, build a strong foundation and then push for more as you progress in the field. Data science is a field that is constantly changing everyday and so stay updated with the present trends and work hard so that you can be one among the many leaders in this domain.\n\n![work_hard](https:\/\/quotefancy.com\/media\/wallpaper\/1600x900\/4674533-Urijah-Faber-Quote-Dream-big-stay-positive-work-hard-and-enjoy-the.jpg)","228f8d0e":"#### **e. Machine Learning**\n\nMachine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It is one of most integral part of data science and also the one that attracts the audience towards data science. Thus it is one of the key reasons behind the rise in the demand for Data Science. \n\nMachine Learning (ML) technology had certainly made your life easier and comfortable. Everyday we use tools such as mobile assistants, audio\/video streaming services, shopping sites etc that incorporate ML tools to provide us a better experience. With new and powerful hardware and ML algorithms, the demand for this field in data science is higher than ever before. As discussed back, nowadays almost every companies are investing money and resources to setup their own data science team. So, let us analyze what ML algorithms and frameworks we need to be skilled in so that we could also be a part of this vast growing community.","2b5d32f5":"There is now much evidence from the study that Data Science do indeed have a bright future ahead and it is never too late to be a part of it.\n\n> The Best Way To Predict The Future Is To Create It  \n       **- Peter Drucker**\n       \n\nAs Peter Drucker said, **the best way to know the future, is to be among those creating it**. So, be a part of this highly demanding field to learn and tackle new challenges and finally create and enjoy a good future.\n\nIf you are new to this data science realm, there is always an uncertainity on where to start or what you should learn. The technology changes everyday and keeping up with and learning the present trends is quite important. This can help you become successfull in any domain (not just data science). So, the final section walks you through the present trends in the field at present (in 2020) so that you have a perfect start.","114d4423":"### **9. References**\n\nAll the charts used in the notebook were developed using [Matplotlib](https:\/\/matplotlib.org\/) and [Bokeh](https:\/\/docs.bokeh.org\/en\/latest\/index.html). The other sources followed include:\n\n1. https:\/\/www.edureka.co\/blog\/what-is-data-science\/\n2. https:\/\/dare2compete.com\/bites\/the-rise-of-data-science\/\n3. https:\/\/www.weforum.org\/agenda\/2019\/03\/gender-equality-in-stem-is-possible\/\n4. https:\/\/www.itproportal.com\/features\/a-snapshot-of-data-scientist-jobs-around-the-world\/\n5. https:\/\/www.census.gov\/library\/stories\/2019\/02\/number-of-people-with-masters-and-phd-degrees-double-since-2000.html\n6. https:\/\/www.kdnuggets.com\/2018\/09\/how-many-data-scientists-are-there.html\n7. https:\/\/octoverse.github.com\/\n8. https:\/\/insights.stackoverflow.com\/survey\/2020\/#technology\n9. https:\/\/www.newgenapps.com\/blog\/top-5-cloud-platforms-and-solutions-to-choose-from","329dc26e":"#### **c. Data Storage**\n\nThe most important part of Data Science is the data itself! As more and more data started to pour in, the need for better data storage became important. Databases support this requirement and provides an interface where the data can be stored in a structured manner(in case of RDBMS). \n\nIn simple terms, a database is an organized collection of data. To work with database, you need a DBMS(Database Management System) which is a software system that enables users to define, create, maintain and control access to the database. Like found in the above section, SQL is one among the most used and recommended languages and learning such a database manipulation language is a necessary and an important skill required for data scientists. Apart from the storing large volumes of data, databases also helps to efficiently query and fetch data from them. Let us explore the popular databases used by the communtiy for their daily work.","2b54eacc":"**Note -** *The legends are interactive. Click on it to enable or disable the values associated with that legend*\n\n\n* Data Science is not the same across the globe. **The opportunities you get with DS depends a lot on where you live**.\n\n\n* India is way ahead in the survey, especially in the year 2020. Out of all the respondents, almost 50% of them live in India. Eventhough India has been one of the worst affected countries by the pandemic, data science did manage to thrive there. Apart from the survey, Indians are comparitively more active in Kaggle competitions as well (even in this one). \n\n\n* USA is one of the countries that shows the declining demand for the field. The number of survey respondents has dropped a lot, with the count going down even further every year, from 2017 to 2020. The same trend can also be seen in case of UK and France, even though the decline is not as much as in the United States.\n\n\n* On the other hand, the survey responses from India rocketed up throughout the years. The number of respondents has went up from around 25% in 2017 to 50% in 2020 which is very large compared to other countries. This higlights the increasing demand for DS in India. Japan and Brazil also follow a similar trend with the number of respondents rising every year.\n\n\n* The demand for DS has stayed constant in Canada during 2017 - 2019, but went down in 2020. We can assume pandemic might be the one to blame for the decline in 2020.\n\n\n* The other interesting case is that of China. China had a huge surge of people getting into DS between 2017 and 2018 but surprisingly that number has been on decline since then.\n\n\n> Data science is present globally and that can be seen from the survey participants from over 50 countries. It is hard to jump to a conclusion about the demand for data science globally. In certain countries like India, the field seems to blossom, while in USA and China it doesn't look that good. Even the pandemic might have a say on this, especially in the year 2020.\n\n\n\nIndia and USA are the countries with most number of respondents (around 70%). Covering such a large population, these countries can shed some light into the how easy is it for women to get into data science, especially in countries where the demand for the field is high. So how does the gender distribution look in the two countries?","2a86b8ec":"### **4. Do people start young?**\n\n![age](https:\/\/cdn.psychologytoday.com\/sites\/default\/files\/styles\/image-article_inline_full\/public\/field_blog_entry_images\/Longevity%20Cartoon_1.jpg?itok=X89Hn_1J)\n\nThe future of DS is of course dependent on the people in it. **Youths represent the future, and it is only through their engagement that the field can have a good future**. The percentage of young people in Data Science is a great indicator of where the field is headed. More the percentage, better will be the future.\n\nWhat is the impact of DS on youth population and how do non-youths respond to it?","8a7b949f":"### **3. What is the impact of Data Science across the globe?**\n\n![country](https:\/\/knowledge.wharton.upenn.edu\/wp-content\/uploads\/2019\/01\/country-flags-rankings.jpg)\n\nThe 2020 survey had **respondents from more than 50 countries and territories**. Kaggle, being one of the largest community of data science and machine learning practitioners attracts all data science enthusiasts. Kaggle and along with it, Data Science is turning out into a new revolution. \n\nDoes Data Science have the same impact everywhere across the globe or is it limited to just a few countries? This section helps to uncover that. To keep the analysis simple, only the top 10 countries, with respect to the number of survey respondents are considered.","3edac105":"* In both India and USA, females contribute to almost 22% of the total survey population. This is well above the global average that we saw in the above section.\n\n\n* In India, the number of women in data science is increasing right from 2017 to 2020, especially in 2020. It appears that the pandemic has provided women enough time to get into data science stream. The percentage of women increased by over 5% in 2020.\n\n\n* In USA, the number of women in data science is kind of constant throughout the years from 2017 to 2020. The percentage of women in the field doesn't follow any pattern here. It first increases by almost 2%, then drops and finally rises again. Atleast there was and still are more than 20% females from USA in data science domain, which is almost similar to that in India.\n\n>  We can conclude from the analysis that, as the number of people or the demand in DS increase in a country, so will the percentage of women in the field. US, eventhough having a decline in the total survey participation, the presence of women has went up from 21% in 2017 to 22% in 2020. Similary, India was one of the places that had a rise in kagglers every year from 2017 to 2020 and so did its share of female population ","3921b062":"* Over 40% of the respondents have a Master's degree. Most of the other respondents are those with Bachelor's degree. They together make up over 75% of the total respondents, which is huge!!!. So it is **important to have atleast a Bachelor's degree** to easily become a part of DS community\n\n\n* As seen in the analysis of age group of respondents, over 50% of the them are below the age of 30. That might be so because, people start to enter DS after completing either the Bachelor's or Master's. Also the bachelor's community have seen a good growth especially in 2020 while the master's community suffered a loss in its share of percentages. \n\n\n* This change in percentages of bachelor's and master's group can be inferred as because of more people under 25 getting into data science (as seen from above section) in 2020 and the also due to lesser number of people above 25 and in middle-age group in the same year as compared to 2019.\n\n\n* There is a good percentage (about 15%) of respondents that have a Doctoral degree but the drop in the chart in 2020 isn't a good sign. According to the [census](https:\/\/www.census.gov\/library\/stories\/2019\/02\/number-of-people-with-masters-and-phd-degrees-double-since-2000.html) by U.S. Census Bureau, there are only 4.5 million people with Doctoal degree in US in 2018 comapred to around 70 million of them with either Bachelors or Masters. **The 15% percentage of respondents with Doctoral degree is definitely an indicator that it is, a lot more easier to land in a DS job with a Doctoral degree**.\n\n\n* For those people without a degree, the change for getting into data science is quite small. So, generally the higher the degree you have, more likely you are to end up in DS job.\n\n\n> The key takeaway is that you can start a career with just a Bachelor's degree and learn more as you work. It is always better to learn from real world experiences at your workplace. Pursuing a higher degree is also great and will eventually make it easier to get better jobs and pay. It is clear from the previous section that more young people are into kaggle and DS and hence the large number of Bachelor's community. Similarly as the demand for the field increases, more will be the percentage of people pursuing higher education","4fd107f5":"#### **b. Start practising coding**\n\nCoding is another important skill that you need to acquire to have a good data science career. Once you have learned the key concepts in data science from the above mentioned sources, coding knowledge is required to use whatever you have learned to design and build products useful for the community as well as for the betterment of world. So let us explore the coding skills common within the community at present.","612a8f7b":"* The analysis of the survey data reveals that Data Science has been and is still a male dominated domian with more than 78% male population.\n\n\n* The percentage of Male and Female respondents are almost the same from 2017 to 2019, but a variation in the trend can be observed in 2020. May be the pandemic has provided females time and oppurtunity to pursue data science. Many of them have started to find a carrer here(almost 3% increase).\n\n\n* There are **almost five Males for every Female**, indicating Male dominance in the field. There has been a slight increase in the number of women kagglers in 2020 and it might be an indication of a something better. Women, always have played an important role in many important scientific breakthroughs and thus, more females should step forward to be a part of Data Science.\n\n> A field having good demand should have a good share of women population. The male population remains constant for some time and drops in 2020, while there is an overall rise in the female population (17% in 2017 to 19% in 2020). This is a good indicator of the demand for data science and we can hope the trend continues and women too have an equal say in the field.","0949e695":"Note - The values are in percentages\n\n* Simpler algorithms such as **Linear or Logistic regression and Decision trees or random forests** are the most used ML algorithms by the community. These algorithms are easier to learn and gives reasonably good results making them the most used ones regardless of the experience you have in ML. ML experience only takes into account the experience working with ML algorithms or frameworks.\n\n\n* As people gain more knowledge about the different ML techniques, they start to go for more complex alogrithms such as Gradient Boosting Machines, Evolutionary Approaches, Bayesian approaches etc. Using more complex algorithms increases the possibilty of getting better outcomes.\n\n\n* Deep learning is a subsection of ML that uses neural networks to learn non-linar and complex features in data. Deep learning methods such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) are also used often. The usage of CNNs reduce as experience increases while RNN and Dense Neural Networks are used equally by people having both low and high ML experience. \n\n\n* Transformers and GANs are powerful algorithms that can solve a lot of complex tasks once considered impossible. They are gaining a lot of traction recently. As expected, it is the people with higher experience that uses these advanced algorithm but again it is good to see that atleast a small portion of newbies are also into these algorithms.\n\n\n> You can start with ML by learning all the basic and simple algorithms and can later move on to complex ones. Having a good foundation is the most important part. It can be observed from the chart that as experience increases, the variance of the percentage values decreases (the percentage values are getting closer to each other). This means that, as you grow old in the ML domain, it is important to learn as many algorithms as you can. Every algorithm has its own importance in solving complex real world problems.","ffd6c98d":"* The most popular source for learning DS is **Kaggle** which is followed closely by YouTube and Blogs. Kaggle is an online platform for data science and machine learning practioners. It claims to be the world\u2019s largest community of active data scientists and Kaggle allows you to collaborate and work with people all around the world. Kaggle also hosts competitions frequently which agains helps you to learn, stay motivated and also prepare for the world out there.\n\n\n* YouTube and Blogs are among the top sources used for learning DS. A lot of blogs and YouTube videos can be found now which provides simple explanation to most complex thing out there in the field. Moreover, they also provide an interactive atmosphere where you can discuss and exchange ideas with the author or other viewers. \n\n\n* Details on some of the top blog sites can be found [here](https:\/\/www.tableau.com\/learn\/articles\/data-science-blogs). Some of the most followed YouTube channels for learning DS include Datacamp, Sentdex and many others where you can visualize and learn, which is more effective than reading a book or a journal.\n\n\n* Looking at the chart of the common platforms used by survey participants, there are more than twice the number of users following **Coursera** than those attending university courses for learning data science. Coursera has tons of courses related to Data Science supported by universities and organisations around the globe with expert faculties that helps you to learn at lesser expense (there are tons of free courses as well). Kaggle learn courses and Udemy are also popular among people in the data science industry.\n\n\n> At present, thanks to internet, you have all the information you need at your fingertips. Internet has revolutionized our lifestyle and that includes the platform for education as well. **The way people learn is slowly moving away from attending University sessions to joining online courses**. New platforms and online courses keep popping up everyday and the value of the certification they provide are also considered important now. Earlier, it was difficult getting admission in to your favourite colleges but now anyone can login to and attend their favourite courses. It has made access to education a lot easier and is one of the key factors behind the growing demand for data science.","d552c31a":"Note - The values are in percentages\n\n* As seen from the survey data, Python and SQL are the most used languages by the Kaggle community regardless of the experience. The number of python users reduces as experience increases, while the share of SQL remains almost constant. R is also a language used equally by the community regardless of their experience in their day to day life.\n\n\n* The experieced people also use Java, Javascript and Bash in their daily coding life. This can be noticed from the increase in their percentage share as experience increases. Julia and Swift remains the least popular languages used within the community.\n\n> We have now seen the most commonly used programming language by the survey respondents. This gives you some idea on what to learn to start coding. Now, let us find out what languages the community recommend you learn so that you can stay strong in the field.","51b9f6ee":"Note - The values are in percentages\n\n* **Jupyter is the most used IDE among the respondents**. It is a common tool used by people regardless of their experience. Jupyter IDE supports coding languages including Python and R, which are among the most commonly used languages and that explains its popularity. Apart from just writing codes, Jupyter also supports markdown language, widgets etc, which makes your work more presentable.\n\n\n* PyCharm and Spyder are some of the powerful python IDEs that helps you to create and easily manage projects, virtual environments and ofcourse the different files associated with the project. PyCharm has add-ons that helps you push your code to any repository straight from the IDE interface, provide intellisense and automate manual processes such as writing docstrings etc. Even though both PyCharm and Spyder provides a lot more features than Jupyter, they aren't preferred much by the experienced community. The usage of these two IDEs decrease as experience increases.\n\n\n* RStudio is another powerful IDE for developing codes in R programming language. Similar to jupyter, RStudio is also a common IDE seen among people regardless of their experience.\n\n\n* Source code editors such as **VS Code, Notepad++ and Vim\/Emacs** are also popular among the community, especially among the experienced group. These IDEs features a lightning fast source code editor, perfect for day-to-day use. With support for hundreds of languages, these IDEs helps you to be instantly productive with syntax highlighting, bracket-matching, auto-indentation, box-selection, snippets, and more which is exactly what every programmer wants.\n\n\n> Now that you have found the most popular programming language as well as the IDE to start coding, it is now time to choose an ideal one depending on your experience level and definetly the use case and keep practising it to be a leader in the data sciece domain.","54654d1a":"### 2. **The effect of Gender on Data Science**\n\n![gender](https:\/\/www.ft.com\/paidpost\/CBS\/gender_differences\/img\/gender.jpg)\n\n**There should be no shortage of inspirational role models for young girls dreaming of a career in science**. Women have been responsible for some of the most important scientific breakthroughs that shaped the modern world, from Marie Curie\u2019s discoveries about radiation, to Grace Hopper\u2019s groundbreaking work on computer programming, and Barbara McClintock\u2019s pioneering approach to genetics. The presence of women can even be found within 2020 Nobel prize awards in science.\n\nBut too often their stories aren\u2019t just about the difficulties they faced in cracking some of the toughest problems in science, but also about overcoming social and professional obstacles just because of their gender. And many of those obstacles still exist today. \n\nIs Data Science yet another field filled with such obstacles or do women have a clear path ahead?","376b4971":"#### **d. Data Visualization**\n\nData visualization is another important aspect of data science. It is a perfect way to generate insights from data. Data, as such has no value until some useful information can be generated from it. Visualization is the tool we need for that. In short data visualization can be defined as the presentation of data in a pictorial or graphical format. This enables decision makers to much easily unravel the hidden patterns present in the data and make wiser decisions. \n\nBecause of the way the human brain processes information, using charts or graphs to visualize large amounts of complex data is easier than poring over the data over spreadsheets or reports. Data visualization is a quick, easy way to convey concepts in a universal manner. Even in this study of the demand of the data science, it is the visualizations that helps you reach conclusions a lot faster. Now, we can explore the popular visualization tools used by the survey respondents as well as how the popularity has changed in the past year.","2068ffab":"### **7. How do companies respond to Data Science?**\n\n![ds_comp](https:\/\/builtin.com\/sites\/default\/files\/styles\/og\/public\/2019-05\/data-science-companies.jpg)\n\nFrom the above studies done, it is evident that the demand for Data Science appears to be on a rise, so how do companies respond to it? Is the rise in demand associated only with Kaggle or has it got to do with the work places as well? Has data science got so popular that even the small companies started investing on setting up a DS team or should you join large corporates to work in Data Science?"}}