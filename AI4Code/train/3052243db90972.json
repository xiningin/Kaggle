{"cell_type":{"3fff5d4a":"code","d06f6977":"code","6a5e80c6":"code","3857af1e":"code","68592f30":"code","38120937":"code","995afac4":"markdown","9ca97670":"markdown","abc703f2":"markdown","9722834e":"markdown","a6845a62":"markdown","0a5fe7a0":"markdown","273135e9":"markdown"},"source":{"3fff5d4a":"import numpy as np\nfrom matplotlib.image import imread\nimport matplotlib.pyplot as plt\n\nimage_raw = imread(\"\/kaggle\/input\/images-svd\/PP.jpeg\")\nprint(image_raw.shape)\n\n# Displaying the image\nplt.figure(figsize=[12,8])\nplt.imshow(image_raw)","d06f6977":"image_sum = image_raw.sum(axis=2)\nprint(image_sum.shape)\n\nimage_bw = image_sum\/image_sum.max()\nprint(image_bw.max())\n\nplt.figure(figsize=[12,8])\nplt.imshow(image_bw, cmap=plt.cm.gray)","6a5e80c6":"from sklearn.decomposition import PCA, IncrementalPCA\npca = PCA()\npca.fit(image_bw)\n\n# Getting the cumulative variance\n\nvar_cumu = np.cumsum(pca.explained_variance_ratio_)*100\n\n# How many PCs explain 95% of the variance?\nk = np.argmax(var_cumu>95)\nprint(\"Number of components explaining 95% variance: \"+ str(k))\n#print(\"\\n\")\n\nplt.figure(figsize=[10,5])\nplt.title('Cumulative Explained Variance explained by the components')\nplt.ylabel('Cumulative Explained variance')\nplt.xlabel('Principal components')\nplt.axvline(x=k, color=\"k\", linestyle=\"--\")\nplt.axhline(y=95, color=\"r\", linestyle=\"--\")\nax = plt.plot(var_cumu)","3857af1e":"ipca = IncrementalPCA(n_components=k)\nimage_recon = ipca.inverse_transform(ipca.fit_transform(image_bw))\n\n# Plotting the reconstructed image\nplt.figure(figsize=[12,8])\nplt.imshow(image_recon,cmap = plt.cm.gray)","68592f30":"# Function to reconstruct and plot image for a given number of components\n\ndef plot_at_k(k):\n    ipca = IncrementalPCA(n_components=k)\n    image_recon = ipca.inverse_transform(ipca.fit_transform(image_bw))\n    plt.imshow(image_recon,cmap = plt.cm.gray)\n    \n\nk = 150\nplt.figure(figsize=[12,8])\nplot_at_k(100)","38120937":"ks = [10, 25, 50, 100, 150, 250]\n\nplt.figure(figsize=[15,9])\n\nfor i in range(6):\n    plt.subplot(2,3,i+1)\n    plot_at_k(ks[i])\n    plt.title(\"Components: \"+str(ks[i]))\n\nplt.subplots_adjust(wspace=0.2, hspace=0.0)\nplt.show()","995afac4":"## Principal Component Analysis\n\nPCA is a very powerful tool that should be present in the arsenal of every serious Data Scientist. Sooner or later, you WILL run into a situation where PCA could make your life easier. It is one of those elegant ideas from Linear Algebra that have a host of applications in -\n\n1. Dimensionality reduction\/data compression\n2. Data visualization and Exploratory Data Analysis\n3. Create uncorrelated features\/variables that can be an input to a prediction model\n4. Uncovering latent variables\/themes\/concepts\n5. Noise reduction in the dataset\n\n**Example - PCA on MNIST dataset - reduced to 2 dimensions for visualization**  \nThe MNIST dataset has 784 dimensions for each example - 784 pixels; of course, you can't visualize 784D plots as is.  \nWith PCA, you can already see the digits forming natural groups in 2D space, although with some overlap. But the fact that you can reduce a 784 dimensional data to 2D and be able to visualize, is amazing in itself.\n\n<img src=\"https:\/\/i.stack.imgur.com\/9wmJ2.png\" width=\"350px\">\nImage soure: Stack overflow\n\n\nIn this notebook, we'll learn how to use PCA for Data compression\/ Dimensionality reduction. Specifically, using PCA for image compression.\n\n### Defining Principal Component Analysis in simple terms\nPCA is a statistical procedure to convert observations of possibly correlated variables\/features into \u2018Principal Components\u2019 that are \u2013 \n- Uncorrelated with\/independent of each other\n- Constructed to capture maximum information\/variance in the data\n- Linear combinations of the original variables\n\n**PCA is an unsupervised technique: there is no \u2018Y\u2019 or dependent\/response variable.**\n\n### How does PCA work?\nOr, how are the Principal components found?  \nThe first Pricipal Component is found such that it explains the maximum variance in the data.  \nThe second attempts to find variance incremental to the first, while being **orthogonal** to the first. \nAnd so on ... this process continues to find all the principal components (min of #rows, #columns).  \n\n<img src=\"https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/321157\/675759\/PCA%20working.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1568281657&Signature=OkSTjLSPaxBl92qjFEB6p6u%2BZQ3bHPERjVXI5cQOJuLchfw7T223TflxK2abMICNjFNmXt7TtfF2P2q29I30n0l7LLoSYqyuiXACxyxquf95T%2BLFk8GGzrQRdPgEJzPyDEUtenvPV88ZWdbgUWDHta27YnbA%2FsMgkoD65FRnpxR6ec0AtKpiQNir5KFTzmtCRAMvPeru1UcnpFqNl6FHTaEeP%2BmgpjDmgF%2Bp6%2FLuBp4wd8BtBcYmhUCdDFD7jgogd%2BN9ntb7bKDeYtxQ1Wdzedu0BKQejo8%2B86wwgpvl95yNN802n64Ogv1qUW16BtVnHxbuWpfSILKJj8hqEV0Xow%3D%3D\">\n\nThis is a very brief, very simplified, one of the many formulations of PCA. For more details and more math on how PCA works, you could refer to the [Wikipedia page here](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis), it has pretty good explanation.  \n\n\n### Using PCA for image compression\nAn image can be treated as a matrix - a grid of pixels, with values being the pixel intensities.   \nThe basic steps we'll follow:\n- Apply PCA on the image matrix to reduce the dimensionality to a smaller number of principal components (PCs). This is lossy compression, as we are discarding some of the information.\n- To assess how much visual information we retained, we'll reconstruct the image from the limited number of PC.\n- We'll see how good the reconstructed images are for different number of selecte components.\n\n#### With that, let's get our hands dirty!\n\nImporting the required libraries, importing the image as a matrix, and displaying the image.","9ca97670":"We'll looks like after 150, most of the reconstructed images are indistinguishable from the original greyscale. Even 150 components instead of 784 columns is significant compression.  \n\nWell, so that was it for a brief introduction to PCA. We saw how we can - \n1. Implement PCA in python\n2. Choose the number of components using a scree plot\n3. Reconstruct the original data from the components   \n\nNow, PCA is not the only dimensionality reduction technique - there are **plenty** more. Some notable ones being - \n- ICA (Independent Component Analysis)\n- Manifold Learning (IsoMap, LLE)\n- t-SNE\n\nThese overcome some of the shortcomings of PCA, but there are tradeoffs.    \n\nPCA should be one technique that should be in the arsenal of every Data Scientist. While it does have some drawbacks, the idea of PCA is pretty powerful, and PCA is pretty much a \u2018default\u2019 method for several applications owing to its efficiency, elegant geometrical interpretation, and simplicity.\n\n\n**Happy learning!**","abc703f2":"**Looks like 38 components, instead of 784 pixels, can explain 95% of the variance in the image!**  \n38 instead of 784 - whoah!    \n\nLet's reconstruct the image using only 38 components and see if the reconstructed image is visually very different from the original.\n\n### Reconstructing the b\/w image with the limited number of components (38)\n1. First, we'll use the `fit_transform` method from the IncrementalPCA module to first find the 38 PCs and transform and represent the data in those 38 new components\/columns.  \n2. Next, we'll reconstruct the original matrix from these 38 components using the `inverse_transform` method.  \n\nWe'll then plot the image to visually assess the quality of it.","9722834e":"### Performing PCA on the image\n - We'll perform PCA on the matrix with all the components\n - We'll then look at the scree-plot to assess how many components we could retain and how much cumulative variance they capture\n - We'll pick a suitable number of components to represent the image for compression","a6845a62":"Well, for 95% variance, we expected a clearer image, didn't we? Note that we got all the major elements captured for sure - you can still very well identify objects.\n\nWhat's missing is the clarity - well, maybe it's the finer details in the visuals that make an image appealing and clear.\n\n#### Let's try out a different value of k - 150 components  ","0a5fe7a0":"A beautiful image of a handsome man, enjoying the view at an exotic location. Just a pic from my vacation ;)\n\n**About the image - **  \nThe image is a colour image i.e. has data in 3 channels- Red, Green, Blue.   \nHence the shape of the data - 780 x 1040 x 3  - it is essentially is 780 x 1040 matrix for each channel.\n\nWhile we could work with 3 channels, simply working with 3 matrices, for the purpose of demonstrating and for avoiding distraction from the core lesson, I'll convert the image to Black & White (greyscale, actually).  \n\n**Converting to greyscale** - \n1. summing RGBs channel values for each pixel\n2. capping values to 1\n\nWe'll plot the matrix as a greyscale image.","273135e9":"Much better! Just a little bit grainy, but the details are all there. On a smaller resolution, you probably won't be able to detect the differences from the original greyscale image very easily.\n\n### Reconstructing and plotting for different number of components\n- we'll try out different number of components, begining from 10, ending at 250\n- we'll reconstruct the image at each `k` and plot the images"}}