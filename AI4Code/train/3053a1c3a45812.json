{"cell_type":{"4b213bd7":"code","10e06334":"code","f81aed8a":"code","30afb14f":"code","9103034c":"code","de38ce14":"code","0e62e892":"code","e4a06974":"code","d9f2446b":"code","abf9d2dc":"code","5a602914":"code","a1c07ead":"code","f83395cd":"code","001c5406":"code","85be7cda":"code","83071000":"code","01d40b3a":"code","19181171":"code","d920e495":"code","db230b3f":"code","df7828de":"code","a9f3a15f":"markdown"},"source":{"4b213bd7":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport pandas as pd\nimport os\nimport math\nimport h5py\nimport random\nimport nilearn as nl\nfrom nilearn import image, datasets, plotting\nfrom nilearn.image import get_data\nfrom random import randint\nfrom tqdm.notebook import tqdm\nfrom sklearn.impute import KNNImputer\nfrom tqdm.notebook import tqdm","10e06334":"source = \"..\/input\/trends-assessment-prediction\"","f81aed8a":"img_train = []\nfor name in os.listdir(source+'\/fMRI_train'):\n    img_train.append(int(name[:5]))\nimg_train.sort()\ntrain = {}\nfor i, name in enumerate(img_train):\n    train[i] = name","30afb14f":"impute = KNNImputer(n_neighbors=40)","9103034c":"y_train = pd.read_csv(\"\/kaggle\/input\/trends-assessment-prediction\/train_scores.csv\")","de38ce14":"y_train = impute.fit_transform(y_train)","0e62e892":"y_train = pd.DataFrame(y_train)","e4a06974":"y_train","d9f2446b":"y = {}\nfor i in range(len(img_train)):\n    y[img_train[i]] = y_train[y_train[0] == img_train[i]].drop([0], axis=1).to_numpy()","abf9d2dc":"def get_batch(k, size):\n    image = img_train[k:k+size]\n    batch = np.zeros((size,53, 52, 63, 53))\n    y_train = np.zeros((size,5))\n    for i in range(len(image)):\n        batch[i] = h5py.File(source+f'\/fMRI_train\/{image[i]}.mat', 'r')['SM_feature'][()]\n        y_train[i] = y[image[i]]\n    \n    return batch, y_train","5a602914":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv3D, Dense, Dropout, Flatten, BatchNormalization, PReLU, Reshape , MaxPooling3D,MaxPool3D\nimport tensorflow.keras.backend as K","a1c07ead":"def weighted_NAE(yTrue,yPred):\n    weights = K.constant([.3, .175, .175, .175, .175])\n\n    return K.sum(weights*K.sum(K.abs(yTrue-yPred))\/K.sum(yPred))","f83395cd":"model = Sequential()\nmodel.add(Conv3D(200, (3,3,3), input_shape = (53, 52, 63, 53), activation='relu', data_format='channels_first'))\nmodel.add(MaxPool3D((2,2,2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv3D(200, (3,3,3), activation='relu', data_format='channels_first'))\nmodel.add(MaxPool3D((2,2,2)))\nmodel.add(BatchNormalization()) \n\nmodel.add(Conv3D(200, (3,3,3), activation ='relu', data_format='channels_first'))\nmodel.add(MaxPool3D((2,2,2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv3D(200, (3,3,3), activation='relu', data_format='channels_first'))\nmodel.add(MaxPool3D((2,2,2)))\n\nmodel.add(Flatten())\nmodel.add(BatchNormalization()) \nmodel.add(Dense(8192, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(4096, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2048, activation='relu'))\nmodel.add(BatchNormalization()) \nmodel.add(Dropout(0.2))\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(5, activation='relu'))\nmodel.compile(loss='mse',\n              optimizer='adam',\n              metrics='mse')","001c5406":"batch_size = 10\nnb_batchs = 10\n\nfor e in range(150):\n    print(f'epoch {e}\/150')\n    loss = 0.\n    acc = 0.\n    for batch in range(nb_batchs):\n        train_x, train_y = get_batch(batch, batch_size)\n        loss_batch, acc_batch = model.train_on_batch(train_x, train_y)\n        loss += loss_batch\n        acc += acc_batch\n    print(f'Loss: {loss \/ nb_batchs}')\n    print(f'Acc: {acc \/ nb_batchs}')","85be7cda":"model.save('model.h5')","83071000":"img_test = []\nfor name in os.listdir(source+'\/fMRI_test'):\n    img_test.append(int(name[:5]))\nimg_test.sort()","01d40b3a":"t = h5py.File(source+f'\/fMRI_test\/{img_test[0]}.mat', 'r')['SM_feature'][()]","19181171":"model.predict(np.array([t])).flatten()","d920e495":"pred_model = np.array([])\nfor img in tqdm(img_test):\n    t = h5py.File(source+f'\/fMRI_test\/{img}.mat', 'r')['SM_feature'][()]\n    if len(pred_model) == 0:\n        pred_model = model.predict(np.array([t])).flatten()\n    else:\n        pred_model = np.append(pred_model, model.predict(np.array([t])).flatten())","db230b3f":"sample_submission = pd.read_csv(\"\/kaggle\/input\/trends-assessment-prediction\/sample_submission.csv\")","df7828de":"pred=pd.DataFrame()\npred[\"Id\"]=sample_submission.Id\npred[\"Predicted\"]=pred_model\npred.to_csv('out.csv', index=False)","a9f3a15f":"# Due to limited computing power, network training was only in 20 pictures. I think that if you train model on the entire dataset, then the quality will be high. Good luck"}}