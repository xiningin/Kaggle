{"cell_type":{"6875177a":"code","3d467c5c":"code","a64eec03":"code","9a2ef6e4":"code","f70aaf68":"code","85664504":"code","e3a97f41":"code","bc11b587":"code","5ffea4a9":"code","d79fb008":"code","50558c47":"code","f6132539":"code","72844b1b":"code","244da79d":"code","48bec9f1":"markdown","30105461":"markdown","b1aaa8e9":"markdown","ed0d89f6":"markdown","196f6222":"markdown","55f5b635":"markdown","e8eff557":"markdown","bd5a9e7b":"markdown","41324ac0":"markdown","df70888b":"markdown","5ceb2ff1":"markdown","87a1abac":"markdown"},"source":{"6875177a":"import os\nimport sys\nimport time\nimport random\nimport logging\nimport typing as tp\nfrom pathlib import Path\nfrom contextlib import contextmanager\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoost, Pool\n\n%matplotlib inline","3d467c5c":"ROOT = Path.cwd().parent\nINPUT = ROOT \/ \"input\"\nDATA = INPUT \/ \"tabular-playground-series-jan-2021\"\nWORK = ROOT \/ \"working\"\n\nfor path in DATA.iterdir():\n    print(path.name)","a64eec03":"train = pd.read_csv(DATA \/ \"train.csv\")\ntest = pd.read_csv(DATA \/ \"test.csv\")\nsmpl_sub = pd.read_csv(DATA \/ \"sample_submission.csv\")\nprint(\"train: {}, test: {}, sample sub: {}\".format(\n    train.shape, test.shape, smpl_sub.shape\n))","9a2ef6e4":"train.head().T","f70aaf68":"@contextmanager\ndef timer(logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None):\n    if prefix: format_str = str(prefix) + format_str\n    if suffix: format_str = format_str + str(suffix)\n    start = time.time()\n    yield\n    d = time.time() - start\n    out_str = format_str.format(d)\n    if logger:\n        logger.info(out_str)\n    else:\n        print(out_str)","85664504":"class TreeModel:\n    \"\"\"Wrapper for LightGBM\/XGBoost\/CATBoost\"\"\"\n    def __init__(self, model_type: str):\n        self.model_type = model_type\n        self.trn_data = None\n        self.val_data = None\n        self.model = None\n\n    def train(self,\n              params: dict,\n              X_train: pd.DataFrame, y_train: np.ndarray,\n              X_val: pd.DataFrame, y_val: np.ndarray,\n              train_weight: tp.Optional[np.ndarray] = None,\n              val_weight: tp.Optional[np.ndarray] = None,\n              train_params: dict = {}):\n        if self.model_type == \"lgb\":\n            self.trn_data = lgb.Dataset(X_train, label=y_train, weight=train_weight)\n            self.val_data = lgb.Dataset(X_val, label=y_val, weight=val_weight)\n            self.model = lgb.train(params=params,\n                                   train_set=self.trn_data,\n                                   valid_sets=[self.trn_data, self.val_data],\n                                   **train_params)\n        elif self.model_type == \"xgb\":\n            self.trn_data = xgb.DMatrix(X_train, y_train, weight=train_weight)\n            self.val_data = xgb.DMatrix(X_val, y_val, weight=val_weight)\n            self.model = xgb.train(params=params,\n                                   dtrain=self.trn_data,\n                                   evals=[(self.trn_data, \"train\"), (self.val_data, \"val\")],\n                                   **train_params)\n        elif self.model_type == \"cat\":\n            self.trn_data = Pool(X_train, label=y_train, group_id=[0] * len(X_train))\n            self.val_data =  Pool(X_val, label=y_val, group_id=[0] * len(X_val))\n            self.model = CatBoost(params)\n            self.model.fit(\n                self.trn_data, eval_set=[self.val_data], use_best_model=True, **train_params)\n        else:\n            raise NotImplementedError\n\n    def predict(self, X: pd.DataFrame):\n        if self.model_type == \"lgb\":\n            return self.model.predict(\n                X, num_iteration=self.model.best_iteration)  # type: ignore\n        elif self.model_type == \"xgb\":\n            X_DM = xgb.DMatrix(X)\n            return self.model.predict(\n                X_DM, ntree_limit=self.model.best_ntree_limit)  # type: ignore\n        elif self.model_type == \"cat\":\n            return self.model.predict(X)\n        else:\n            raise NotImplementedError\n\n    @property\n    def feature_names_(self):\n        if self.model_type == \"lgb\":\n            return self.model.feature_name()\n        elif self.model_type == \"xgb\":\n            return list(self.model.get_score(importance_type=\"gain\").keys())\n        elif self.model_type == \"cat\":\n             return self.model.feature_names_\n        else:\n            raise NotImplementedError\n\n    @property\n    def feature_importances_(self):\n        if self.model_type == \"lgb\":\n            return self.model.feature_importance(importance_type=\"gain\")\n        elif self.model_type == \"xgb\":\n            return list(self.model.get_score(importance_type=\"gain\").values())\n        elif self.model_type == \"cat\":\n            return self.model.feature_importances_\n        else:\n            raise NotImplementedError","e3a97f41":"ID_COL = \"id\"\nFEAT_COLS = [f\"cont{i}\" for i in range(1, 15)]\nTGT_COL = \"target\"\n\nN_SPLITS =10# 5\nRANDOM_SEED = 2021#42\nUSE_MODEL = \"lgb\"\n\nMODEL_PARAMS = {\n    \"objective\": \"root_mean_squared_error\",\n    \"boosting\": \"gbdt\",\n    \"learning_rate\": 0.01,\n    \"seed\": RANDOM_SEED,\n    'max_depth': -1,\n    'colsample_bytree': .85,\n    \"subsample\": .85,\n    \"n_jobs\": 2,\n}\nTRAIN_PARAMS = {\n    \"num_boost_round\": 20000,\n    \"early_stopping_rounds\": 300,\n    \"verbose_eval\": 100,\n}","bc11b587":"X = train[FEAT_COLS]\nX_test = test[FEAT_COLS]\n\ny = train[TGT_COL].values\n\nkf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\ntrn_val_indexs = list(kf.split(X, y))","5ffea4a9":"oof_pred_arr = np.zeros(len(X))\ntest_preds_arr = np.zeros((N_SPLITS, len(X_test)))\nfeature_importances = pd.DataFrame()\nscore_list = []","d79fb008":"for fold, (trn_idx, val_idx) in enumerate(trn_val_indexs):\n    print(\"*\" * 100)\n    print(f\"Fold: {fold}\")\n\n    X_trn = X.loc[trn_idx].reset_index(drop=True)\n    X_val = X.loc[val_idx].reset_index(drop=True)\n    y_trn = y[trn_idx]\n    y_val = y[val_idx]\n\n    model = TreeModel(model_type=USE_MODEL)\n    with timer(prefix=\"Model training\"):\n        model.train(\n            params=MODEL_PARAMS, X_train=X_trn, y_train=y_trn,\n            X_val=X_val, y_val=y_val, train_params=TRAIN_PARAMS)\n    fi_tmp = pd.DataFrame()\n    fi_tmp[\"feature\"] = model.feature_names_\n    fi_tmp[\"importance\"] = model.feature_importances_\n    fi_tmp[\"fold\"] = fold\n    feature_importances = feature_importances.append(fi_tmp)\n\n    val_pred = model.predict(X_val)\n    score = mean_squared_error(y_val, val_pred, squared=False)\n\n    print(f\"score: {score:.5f}\")\n    score_list.append([fold, score])\n    oof_pred_arr[val_idx] = val_pred\n    test_pred = model.predict(X_test)\n    test_preds_arr[fold] = test_pred","50558c47":"oof_score = mean_squared_error(y, oof_pred_arr, squared=False)\nscore_list.append([\"oof\", oof_score])\npd.DataFrame(\n    score_list, columns=[\"fold\", \"rmse score\"])","f6132539":"order = list(feature_importances.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\nplt.figure(figsize=(10, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importances, order=order)\nplt.title(\"{} importance\".format(USE_MODEL))\nplt.tight_layout()","72844b1b":"smpl_sub.head()","244da79d":"sub = smpl_sub.copy()\nsub[TGT_COL] = test_preds_arr.mean(axis=0)\n\nsub.to_csv(\"submission.csv\", index=False)\n\nsub.head()","48bec9f1":"### feature importance","30105461":"## read data","b1aaa8e9":"## Config ","ed0d89f6":"### score of each fold and oof","196f6222":"## Prepare Data","55f5b635":"## Training","e8eff557":"## Check Result","bd5a9e7b":"## Definition","41324ac0":"## import libraries","df70888b":"## Make submission","5ceb2ff1":"# Prepare","87a1abac":"# Training & Inference"}}