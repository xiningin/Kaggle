{"cell_type":{"1321699b":"code","7eb9a3a5":"code","65b5c69b":"code","ec9f9d3f":"code","47117016":"code","a6722561":"code","65a82819":"code","c947c285":"code","c7fb96b5":"code","d34c8814":"code","f4c784d6":"code","b1ce4e8d":"code","92f12b9c":"code","eefa5709":"code","b7da56d9":"code","2fc8b277":"code","fba15ab2":"code","9ccf269b":"code","9880df38":"code","c558bee7":"code","3dd9844f":"code","af613f38":"code","e6971be8":"code","d9b8db82":"code","a261b090":"code","71acd624":"markdown","1c6337b8":"markdown","e5d24067":"markdown","6075a475":"markdown","c92bfd36":"markdown","5b6b3a75":"markdown","78a6684f":"markdown","cbbee584":"markdown","689130d8":"markdown","4531f91c":"markdown","dfeed81c":"markdown","aff8e3ca":"markdown","e4710d41":"markdown","321f310f":"markdown","2fb60c47":"markdown","6371db72":"markdown","b9ba2fec":"markdown","5efbea8f":"markdown","43015707":"markdown","2f65cf7e":"markdown","f95fda9e":"markdown","6d68a15e":"markdown","84ba5eed":"markdown","e8637c83":"markdown","ecd27a4a":"markdown","9238d80d":"markdown","631198e3":"markdown"},"source":{"1321699b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","7eb9a3a5":"import pandas as pd\nimport numpy as np\nfrom pandas_profiling import ProfileReport\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom itertools import cycle\npd.set_option('max_columns', 50)\nplt.style.use('bmh')\ncolor_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])","65b5c69b":"INPUT_DIR_PATH = '..\/input\/m5-forecasting-accuracy\/'\n\nsell_prices_df = pd.read_csv(INPUT_DIR_PATH + 'sell_prices.csv')\ncalendar_df = pd.read_csv(INPUT_DIR_PATH + 'calendar.csv')\nsales_train_validation_df = pd.read_csv(INPUT_DIR_PATH + 'sales_train_validation.csv')\nsubmission_df = pd.read_csv(INPUT_DIR_PATH + 'sample_submission.csv')","ec9f9d3f":"print(f'Shape of sell_prices_df is: {sell_prices_df.shape}')\nprint(f'Shape of calendar_df is: {calendar_df.shape}')\nprint(f'Shape of sales_train_validation_df is: {sales_train_validation_df.shape}')\nprint(f'Shape of submission_df is: {submission_df.shape}')","47117016":"sell_prices_df.head()","a6722561":"calendar_df.head()","65a82819":"sales_train_validation_df.head()","c947c285":"submission_df.head()","c7fb96b5":"spd_profile = ProfileReport(sell_prices_df, title='sell_prices_df Profiling Report', html={'style':{'full_width':True}})","d34c8814":"spd_profile.to_file(output_file=\"spd_profile.html\")\nspd_profile.to_notebook_iframe()\n","f4c784d6":"cd_profile = ProfileReport(calendar_df, title='calendar_df Profiling Report', html={'style':{'full_width':True}})","b1ce4e8d":"cd_profile.to_file(output_file=\"cd_profile.html\")\ncd_profile.to_notebook_iframe()","92f12b9c":"# using minimal=True to avoid heavy computation\n# stvd_profile = ProfileReport(sales_train_validation_df, title='sales_train_validation_df Profiling Report', html={'style':{'full_width':True}}, minimal=True)\n","eefa5709":"# stvd_profile.to_file(output_file=\"stvd_profile.html\")\n# stvd_profile.to_notebook_iframe()","b7da56d9":"# selecting 10 random rows from dataframe\nstvd10 = sales_train_validation_df.sample(n = 10)\nd_cols = [c for c in stvd10.columns if 'd_' in c] # sales data columns\nstvd10 = stvd10.set_index('id')[d_cols].T","2fc8b277":"plt.figure(figsize=(40, 40))\nplt.subplots_adjust(top=1.2, hspace = 0.8)\nfor i,item_id in enumerate(list(stvd10.columns)):\n    plt.subplot(5, 2, i + 1)\n    stvd10[item_id].plot(figsize=(20, 12),\n          title=f'{item_id} sales by \"d\" number',\n          color=next(color_cycle))\n    plt.grid(False)\n","fba15ab2":"cal = calendar_df[['d','date','event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n       'snap_CA', 'snap_TX', 'snap_WI']]","9ccf269b":"stvd_1 = sales_train_validation_df.set_index('id')[d_cols].T\n# rename id column to 'd', to perform merge operation\nstvd_1 = stvd_1.reset_index().rename(columns={'index': 'd'})\n# merging df cal and sales_train_validation_df on 'd'\nstvd_merged = stvd_1.merge(cal, how='left', validate='1:1')","9880df38":"stvd10.head()","c558bee7":"# rename id column to 'd', to perform merge operation\nstvd10 = stvd10.reset_index().rename(columns={'index': 'd'})\nstvd10 = stvd10.merge(cal, how='left', validate='1:1')\nstvd10_date = stvd10.set_index('date')","3dd9844f":"stvd10_date.head()","af613f38":"plt.figure(figsize=(40, 40))\nplt.subplots_adjust(top=1.2, hspace = 0.8)\nfor i,item_id in enumerate(list(stvd10_date.columns[1:11])):\n    plt.subplot(5, 2, i + 1)\n    stvd10_date[item_id].plot(figsize=(20, 12),\n          title=f'{item_id} sales by \"d\" number',\n          color=next(color_cycle))\n    plt.tight_layout()\n    plt.grid(False)","e6971be8":"last_thirty_day_avg_sales = sales_train_validation_df.set_index('id')[d_cols[-30:]].mean(axis=1).to_dict()\nfcols = [f for f in submission_df.columns if 'F' in f]\nfor f in fcols:\n    submission_df[f] = submission_df['id'].map(last_thirty_day_avg_sales).fillna(0)\n    \nsubmission_df.to_csv('submission.csv', index=False)","d9b8db82":"submission_df.head()","a261b090":"## TODO:\n# 1. Analyze sales of items by item_types i.e `Hobbies`, `Household`, `Foods`.\n# 2. Analyze store wise sale of an item datewise\n# 3. Analyze weekly trends or may be monthly.\n# 4. Create new features\n# 5. Try different models","71acd624":"* **calendar_df**","1c6337b8":"### 6.2. Head of data files\n\n* **sell_prices_df**","e5d24067":"> Note: sell_prices_df contains following interesting information:\n    * store_id: there are 10 different store_id, which shows data is collected from 10 different stores\n    * item_id: there are 3049 different item_ids, which shows 3049 different items are collected for forecasting. (Also item_id has lots of unique values that's why it has `HIGH CARDINALITY`)\n    * There are no missing or null values in this dataframe\n    * The corelation heat map shows there is no or 0 corelation between variables.","6075a475":"## 2. Data Overview\n\nIn the challenge, you are predicting item sales at stores in various locations for two 28-day time periods. Information about the data is found in the M5 Participants Guide.\n\n### Files\n* `calendar.csv` - Contains information about the dates on which the products are sold.\n* `sales_train_validation.csv` - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n* `sample_submission.csv` - The correct format for submissions. Reference the Evaluation tab for more info.\n* `sell_prices.csv` - Contains information about the price of the products sold per store and date.\n\n> Note: `sales_train_evaluation.csv` not available yet\n* `sales_train_evaluation.csv` - Available once month before competition deadline. Will include sales [d_1 - d_1941]\n\n\nhttps:\/\/storage.googleapis.com\/kaggle-forum-message-attachments\/772349\/15032\/M5-Competitors-Guide-Final-10-March-2020.pdf","c92bfd36":"- Observations:\n    - It is common to see an item unavailable for a period of time.\n    - Some items only sell 1 or less in a day, making it very hard to predict.\n    - Other items show spikes in their demand (super bowl sunday?) possibly the \"events\" provided to us could help with these.","5b6b3a75":"### 6.4. Plotting sales of 10 random items","78a6684f":"## 1. Introduction\n\nThe goal of this notebook is to give a brief overview of M5 Forecasting competition.\n\n> Note: This is one of the two complementary competitions that together comprise the M5 forecasting challenge.\n\n1. **This Competition:** The objective of this competition is to estimate as precisely as possible, the point forecasts of the unit sales of various products sold in the USA by Walmart? \n\n    * Metric Used for evalueation: **Weighted Root Mean Squared Scaled Error** (RMSSE)\n\n\n2. **Second Competition:** The objective of this competition in to estimate the uncertainty distribution of the realized values of the above competition.\n\n    * Metric Used for evalueation: **Weighted Scaled Pinball Loss** (WSPL)","cbbee584":"## 6. Peek of the data\n\n### 6.1. Shape of data files","689130d8":"> * **sales_train_validation_df profile**","4531f91c":"## 4. Importing important packages and libraries","dfeed81c":"> Note: We are given historic sales data in the `sales_train_validation` dataset.\n    * rows exist in this dataset for days d_1 to d_1913. We are given the department, category, state, and store id of the item.\n    * d_1914 - d_1941 represents the `validation` rows which we will predict in stage 1\n    * d_1942 - d_1969 represents the `evaluation` rows which we will predict for the final competition standings.","aff8e3ca":"> Note: sell_prices_df contains information about the price of the products sold per store and date.","e4710d41":"There are 4 data files available for now in this competition","321f310f":"## 7. Merging DataFrames \n\n#### 7.1. Merging Calendar df with sales_train_validation_df\n\nMerging only few important columns","2fb60c47":"# M5 Forecasting Challenge\n","6371db72":"> Note: calender_df contains information about the dates on which the products are sold.","b9ba2fec":"## 9. Simple Submission\n\nsimply setting last 30 days sales.","5efbea8f":"> Note: Brief overview of submission file\n    * The submission file has 29 columns, col1 for id and the remaining 28 columns represent the 28 forecast days.\n    * Each represent a specific item. This id tells us the item type, state, and store. We don't know what these items are exactly.","43015707":"## 8. Plotting sales of above 10 items on actual dates.","2f65cf7e":"## 5. Loading Data","f95fda9e":"The above plot shows that there is lots of variation in item sales, this effect can be seasonal or due to some particular events on high sale days.","6d68a15e":"### 6.3. Profiling of each DataFrame\n\nFor profiling i have used [pandas-profiling](https:\/\/github.com\/pandas-profiling\/pandas-profiling) library\n\n* **sell_prices_df profile**","84ba5eed":"> Note: calendar_df contains following interesting information:\n    * date: there are 1969 different dates, which shows this df has data for 1969 different dates.\n    * d: there are 1969 different d values (Also d has lots of unique values that's why it has `HIGH CARDINALITY`)\n    * event_name and event_type columns denotes special promotional events thus event_name_1, event_type_1, event_name_2, event_type_2 contains a lot of missing values.\n    * date features has very corelation and features snap_CA, snap_TX and snap_WI also show some corelation between them.\n    ","e8637c83":"* **sales_train_validation_df**","ecd27a4a":"> * **calendar_df profile**","9238d80d":"## 3. Peek of the input data folder","631198e3":"* **submission_df**"}}