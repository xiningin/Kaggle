{"cell_type":{"b29bc665":"code","871e8ea9":"code","e892f923":"code","eb5aa5c0":"code","fa7cfee5":"code","4baabf32":"code","bb0f83c9":"code","4e94dc0d":"code","40263d9c":"code","28b10754":"code","94f69a35":"code","2e507f8f":"code","d2e8f711":"code","a15f25f2":"code","ace7ec9b":"code","d4b935b8":"code","8f65f6b6":"code","06d3873b":"code","0116d7bc":"code","bd21d6a5":"code","bbab1069":"code","289dbe24":"markdown"},"source":{"b29bc665":"from  datetime import datetime, timedelta\nimport gc\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom tqdm.notebook import tqdm\n\nimport os, sys, gc, time, warnings, pickle, psutil, random\nfrom math import ceil\nfrom sklearn.preprocessing import LabelEncoder\nwarnings.filterwarnings('ignore')","871e8ea9":"input_path = \"..\/input\/m5-forecasting-accuracy\"\n\ncalendar = pd.read_csv(os.path.join(input_path, \"calendar.csv\"))\nsell_prices = pd.read_csv(os.path.join(input_path, \"sell_prices.csv\"))\nsample_submission = pd.read_csv(os.path.join(input_path, \"sample_submission.csv\"))\nsales = pd.read_csv(os.path.join(input_path, \"sales_train_validation.csv\"))","e892f923":"h = 28 \nmax_lags = 57\ntr_last = 1913\nfday = datetime(2016,4, 25) \nfday","eb5aa5c0":"CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\nPRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }","fa7cfee5":"pd.options.display.max_columns = 50","4baabf32":"def create_dt(is_train = True, nrows = None, first_day = 1200):\n    prices = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sell_prices.csv\", dtype = PRICE_DTYPES)\n    for col, col_dtype in PRICE_DTYPES.items():\n        if col_dtype == \"category\":\n            prices[col] = prices[col].cat.codes.astype(\"int16\")\n            prices[col] -= prices[col].min()\n            \n    cal = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/calendar.csv\", dtype = CAL_DTYPES)\n    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n    for col, col_dtype in CAL_DTYPES.items():\n        if col_dtype == \"category\":\n            cal[col] = cal[col].cat.codes.astype(\"int16\")\n            cal[col] -= cal[col].min()\n    \n    start_day = max(1 if is_train  else tr_last-max_lags, first_day)\n    numcols = [f\"d_{day}\" for day in range(start_day,tr_last+1)]\n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    dtype = {numcol:\"float32\" for numcol in numcols} \n    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n    dt = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv\", \n                     nrows = nrows, usecols = catcols + numcols, dtype = dtype)\n    \n    for col in catcols:\n        if col != \"id\":\n            dt[col] = dt[col].cat.codes.astype(\"int16\")\n            dt[col] -= dt[col].min()\n    \n    if not is_train:\n        for day in range(tr_last+1, tr_last+ 28 +1):\n            dt[f\"d_{day}\"] = np.nan\n    \n    dt = pd.melt(dt,\n                  id_vars = catcols,\n                  value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n                  var_name = \"d\",\n                  value_name = \"sales\")\n    \n    dt = dt.merge(cal, on= \"d\", copy = False)\n    dt = dt.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n    \n    return dt","bb0f83c9":"def create_fea(dt):\n    lags = [7, 28]\n    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n    for lag, lag_col in zip(lags, lag_cols):\n        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n\n    wins = [7, 28]\n    for win in wins :\n        for lag,lag_col in zip(lags, lag_cols):\n            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n\n    \n    \n    date_features = {\n        \n        \"wday\": \"weekday\",\n        \"week\": \"weekofyear\",\n        \"month\": \"month\",\n        \"quarter\": \"quarter\",\n        \"year\": \"year\",\n        \"mday\": \"day\",\n        #\"ime\": \"is_month_end\",\n        #\"ims\": \"is_month_start\",\n    }\n    \n#     dt.drop([\"d\", \"wm_yr_wk\", \"weekday\"], axis=1, inplace = True)\n    \n    for date_feat_name, date_feat_func in date_features.items():\n        if date_feat_name in dt.columns:\n            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n        else:\n            dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")","4e94dc0d":"FIRST_DAY = 365 # If you want to load all the data set it to '1' -->  Great  memory overflow  risk !","40263d9c":"%time\n\ndf = create_dt(is_train=True, first_day= FIRST_DAY)\ndf.shape","28b10754":"create_fea(df)\n","94f69a35":"df.head()","2e507f8f":"df.dropna(inplace = True)\ndf.shape","d2e8f711":"cat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\nuseless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\"]\ntrain_cols = df.columns[~df.columns.isin(useless_cols)]\n","a15f25f2":"#X_train = df[train_cols]\n#sy_train = df[\"sales\"]","ace7ec9b":"#del df","d4b935b8":"#train_data = lgb.Dataset(X_train , label = y_train, categorical_feature=cat_feats, free_raw_data=False)\n\n#del X_train, y_train; gc.collect()\n","8f65f6b6":"params = {\n        \"objective\" : \"poisson\",\n        \"metric\" :\"rmse\",\n        \"force_row_wise\" : True,\n        \"learning_rate\" : 0.075,#0.03,\n#         \"sub_feature\" : 0.8,\n        \"sub_row\" : 0.75,\n        \"bagging_freq\" : 1,\n        \"lambda_l2\" : 0.1,\n#         \"nthread\" : 4\n        \"metric\": [\"rmse\"],\n        'verbosity': 1,\n        'num_iterations' : 1200, #2400\n        'num_leaves': 2**11-1,# main parameter to control the complexity of the tree model\n        # 2**11-1 = 1024\n        # maxdepth = 11 --> 2048\n        # in between: 1536\n        \"min_data_in_leaf\":  2**12-1,# to prevent overfitting, \n        # depends on the nr of training samples and num_leaves \n}","06d3873b":"#m_lgb = lgb.train(params, train_data, verbose_eval=15) ","0116d7bc":"#m_lgb.save_model(\"model_reducedlr_higheriterations.lgb\")#","bd21d6a5":"m_lgb = lgb.Booster(model_file = '\/kaggle\/input\/model-lgbm\/model.lgb') \n","bbab1069":"alphas = [1.028, 1.023, 1.018]\nweights = [1\/len(alphas)]*len(alphas)\nsub = 0.\n\nfor icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n\n    te = create_dt(False)\n    cols = [f\"F{i}\" for i in range(1,29)]\n\n    for tdelta in range(0, 28):\n        day = fday + timedelta(days=tdelta)\n        print(tdelta, day)\n        tst = te[(te.date >= day - timedelta(days=max_lags)) & (te.date <= day)].copy()\n        create_fea(tst)\n        tst = tst.loc[tst.date == day , train_cols]\n        te.loc[te.date == day, \"sales\"] = alpha*m_lgb.predict(tst) # magic multiplier by kyakovlev\n\n\n\n    te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n#     te_sub.loc[te.date >= fday+ timedelta(days=h), \"id\"] = te_sub.loc[te.date >= fday+timedelta(days=h), \n#                                                                           \"id\"].str.replace(\"validation$\", \"evaluation\")\n    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n    te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n    te_sub.fillna(0., inplace = True)\n    te_sub.sort_values(\"id\", inplace = True)\n    te_sub.reset_index(drop=True, inplace = True)\n    te_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n    if icount == 0 :\n        sub = te_sub\n        sub[cols] *= weight\n    else:\n        sub[cols] += te_sub[cols]*weight\n    print(icount, alpha, weight)\n\n\nsub2 = sub.copy()\nsub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\nsub = pd.concat([sub, sub2], axis=0, sort=False)\nsub.to_csv(\"submission.csv\",index=False)","289dbe24":"## Load data"}}