{"cell_type":{"f9cf36c5":"code","945970a1":"code","1567779a":"code","356bef2c":"code","52d98665":"code","7533ab5a":"code","fa9e9b5c":"code","517b19c5":"code","f433cecd":"code","b2067e94":"code","98422015":"code","4e115b8f":"code","91e7d29d":"code","ed3935b1":"code","b3b306ee":"code","66fc8fb6":"code","54e8dd36":"code","d6caac6e":"markdown","af6bac64":"markdown","a5cd2626":"markdown","340e6c35":"markdown","75775399":"markdown","6725bf36":"markdown","23d4ec29":"markdown","2b930280":"markdown","e3f1d869":"markdown","b1b54fa6":"markdown","a820cc54":"markdown","c632cec8":"markdown","2ec736fc":"markdown","feebfad5":"markdown","99c8eb22":"markdown","984c0f98":"markdown","047089b7":"markdown","a6ada0b5":"markdown"},"source":{"f9cf36c5":"# Define and register a kaggle renderer for Altair\n# Source for this: https:\/\/www.kaggle.com\/jakevdp\/altair-kaggle-renderer\n\nimport altair as alt\nimport json\nfrom IPython.display import HTML\n\nKAGGLE_HTML_TEMPLATE = \"\"\"\n<style>\n.vega-actions a {{\n    margin-right: 12px;\n    color: #757575;\n    font-weight: normal;\n    font-size: 13px;\n}}\n.error {{\n    color: red;\n}}\n<\/style>\n<div id=\"{output_div}\"><\/div>\n<script>\nrequirejs.config({{\n    \"paths\": {{\n        \"vega\": \"{base_url}\/vega@{vega_version}?noext\",\n        \"vega-lib\": \"{base_url}\/vega-lib?noext\",\n        \"vega-lite\": \"{base_url}\/vega-lite@{vegalite_version}?noext\",\n        \"vega-embed\": \"{base_url}\/vega-embed@{vegaembed_version}?noext\",\n    }}\n}});\nfunction showError(el, error){{\n    el.innerHTML = ('<div class=\"error\">'\n                    + '<p>JavaScript Error: ' + error.message + '<\/p>'\n                    + \"<p>This usually means there's a typo in your chart specification. \"\n                    + \"See the javascript console for the full traceback.<\/p>\"\n                    + '<\/div>');\n    throw error;\n}}\nrequire([\"vega-embed\"], function(vegaEmbed) {{\n    const spec = {spec};\n    const embed_opt = {embed_opt};\n    const el = document.getElementById('{output_div}');\n    vegaEmbed(\"#{output_div}\", spec, embed_opt)\n      .catch(error => showError(el, error));\n}});\n<\/script>\n\"\"\"\n\nclass KaggleHtml(object):\n    def __init__(self, base_url='https:\/\/cdn.jsdelivr.net\/npm'):\n        self.chart_count = 0\n        self.base_url = base_url\n        \n    @property\n    def output_div(self):\n        return \"vega-chart-{}\".format(self.chart_count)\n        \n    def __call__(self, spec, embed_options=None, json_kwds=None):\n        # we need to increment the div, because all charts live in the same document\n        self.chart_count += 1\n        embed_options = embed_options or {}\n        json_kwds = json_kwds or {}\n        html = KAGGLE_HTML_TEMPLATE.format(\n            spec=json.dumps(spec, **json_kwds),\n            embed_opt=json.dumps(embed_options),\n            output_div=self.output_div,\n            base_url=self.base_url,\n            vega_version=alt.VEGA_VERSION,\n            vegalite_version=alt.VEGALITE_VERSION,\n            vegaembed_version=alt.VEGAEMBED_VERSION\n        )\n        return {\"text\/html\": html}\n    \nalt.renderers.register('kaggle', KaggleHtml())\nprint(\"Define and register the kaggle renderer. Enable with\\n\\n\"\n      \"    alt.renderers.enable('kaggle')\")","945970a1":"#collapse_hide\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nnp.random.seed(13)\nimport tensorflow as tf\nimport keras as k\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, Activation, Multiply, Lambda, Concatenate, Subtract, Flatten\nfrom keras.layers.embeddings import Embedding\nfrom keras.initializers import glorot_uniform, glorot_normal\nfrom keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.manifold.t_sne import TSNE\nimport altair as alt\n\nalt.renderers.enable('kaggle')\nnp.random.seed(13)","1567779a":"#collapse_hide\ndataLoc=Path('..\/input\/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament\/MDataFiles_Stage2\/')\n\ndf_teams = pd.read_csv(dataLoc\/'MTeams.csv')\nteams_dict = df_teams[['TeamID','TeamName']].set_index('TeamID').to_dict()['TeamName']\n\ndf_regSeason_data = pd.read_csv(dataLoc\/'MRegularSeasonCompactResults.csv')\ndf_regSeason_data.head() # cols = Season,DayNum,WTeamID,WScore,LTeamID,LScore,WLoc,NumOT","356bef2c":"#collapse_hide\ndf_otherTourney_data = pd.read_csv(dataLoc\/'MSecondaryTourneyCompactResults.csv').drop(columns='SecondaryTourney')\ndf_otherTourney_data.head() # cols = Season,DayNum,WTeamID,WScore,LTeamID,LScore,WLoc,NumOT","52d98665":"#collapse_hide\n# Create team encoding that differentiates teams by year and school\ndef newTeamID(df):\n    # df = df.sample(frac=1).reset_index(drop=True)\n    df['Wnewid'] = df['Season'].astype(str) + df['WTeamID'].astype(str)\n    df['Lnewid'] = df['Season'].astype(str) + df['LTeamID'].astype(str)\n    return df\n\ndf_regSeason_data = newTeamID(df_regSeason_data)\ndf_otherTourney_data = newTeamID(df_otherTourney_data)\n\ndef idDicts(df):\n    newid_W = list(df['Wnewid'].unique())\n    newid_L = list(df['Lnewid'].unique())\n    ids = list(set().union(newid_W,newid_L))\n    ids.sort()\n    oh_to_id = {}\n    id_to_oh = {}\n    for i in range(len(ids)):\n        id_to_oh[ids[i]] = i \n        oh_to_id[i] = ids[i]\n\n    return oh_to_id, id_to_oh\n\noh_to_id, id_to_oh = idDicts(df_regSeason_data)    \n\n# add training data in swapped format so network sees both wins and losses\ndef swapConcat_data(df):\n\n    df['Wnewid'] = df['Wnewid'].apply(lambda x: id_to_oh[x])\n    df['Lnewid'] = df['Lnewid'].apply(lambda x: id_to_oh[x])\n\n    loc_dict = {'A':-1,'N':0,'H':1}\n    df['WLoc'] = df['WLoc'].apply(lambda x: loc_dict[x])\n\n    swap_cols = ['Season', 'DayNum', 'LTeamID', 'LScore', 'WTeamID', 'WScore', 'WLoc', 'NumOT', 'Lnewid', 'Wnewid']\n\n    df_swap = df[swap_cols].copy()\n\n    df_swap['WLoc'] = df_swap['WLoc']*-1\n\n    df.columns = [x.replace('WLoc','T1_Court')\n                   .replace('W','T1_')\n                   .replace('L','T2_') for x in list(df.columns)]\n\n    df_swap.columns = df.columns\n\n    df = pd.concat([df,df_swap])\n\n    df['Win'] = (df['T1_Score']>df['T2_Score']).astype(int)\n    df['Close_Game']= abs(df['T1_Score']-df['T2_Score']) <3\n    df['Score_diff'] = df['T1_Score'] - df['T2_Score']\n    df['T2_Court'] = df['T1_Court']*-1\n    df[['T1_Court','T2_Court']] = df[['T1_Court','T2_Court']] + 1\n\n    cols = df.columns.to_list()\n\n    df = df[cols].sort_index()\n    df.reset_index(drop=True,inplace=True)\n\n\n    return df\n\ndf_regSeason_full = swapConcat_data(df_regSeason_data.copy().sort_values(by='DayNum'))\ndf_otherTourney_full = swapConcat_data(df_otherTourney_data.copy())\n\n# Convert to numpy arrays in correct format\ndef prep_inputs(df,id_to_oh, col_outputs):\n    Xteams = np.stack([df['T1_newid'].values,df['T2_newid'].values]).T\n    Xloc = np.stack([df['T1_Court'].values,df['T2_Court'].values]).T\n\n    if len(col_outputs) <2:\n        Y_outputs = df[col_outputs].values\n        Y_outputs = Y_outputs.reshape(len(Y_outputs),1)\n    else:\n        Y_outputs = np.stack([df[x].values for x in col_outputs])\n\n    return [Xteams, Xloc], Y_outputs\n\nX_train, Y_train = prep_inputs(df_regSeason_full, id_to_oh, ['Win','Score_diff'])\nX_test, Y_test = prep_inputs(df_otherTourney_full, id_to_oh, ['Win','Score_diff'])\n\n# Normalize point outputs - Win\/loss unchanged\ndef normalize_outputs(Y_outputs, stats_cache=None):\n    if stats_cache == None:\n        stats_cache = {}\n        stats_cache['mean'] = np.mean(Y_outputs,axis=1)\n        stats_cache['var'] = np.var(Y_outputs,axis=1)\n    else: pass\n    \n    numOut = Y_outputs.shape[0]\n    Y_normout = (Y_outputs-stats_cache['mean'].reshape((numOut,1)))\/stats_cache['var'].reshape((numOut,1))\n\n    return Y_normout, stats_cache\n\nY_norm_train, stats_cache_train = normalize_outputs(Y_train,None)\nY_norm_test, _ = normalize_outputs(Y_test,stats_cache_train)\nY_norm_train[0,:] = Y_train[0,:]\nY_norm_test[0,:] = Y_test[0,:]\n","7533ab5a":"#collapse_show\n# build model\n\ntf.keras.backend.clear_session()\n\ndef NCAA_Embeddings_Joint(nteams,teamEmb_size):\n    team_input = Input(shape=[2,],dtype='int32', name='team_input')\n    X_team = Embedding(input_dim=nteams, output_dim=teamEmb_size, input_length=2, embeddings_initializer=glorot_uniform(), name='team_encoding')(team_input)\n\n    loc_input = Input(shape=[2,],dtype='int32', name='loc_input')\n    X_loc = Embedding(input_dim=3, output_dim=1, input_length=2, embeddings_initializer=glorot_uniform(), name='loc_encoding')(loc_input)\n    X_loc = Lambda(lambda z: k.backend.repeat_elements(z, rep=teamEmb_size, axis=-1))(X_loc)\n    \n    X = Multiply()([X_team,X_loc])\n    X = Dropout(rate=.5)(X)\n    X1 = Lambda(lambda z: z[:,0,:])(X)\n    X2 = Lambda(lambda z: z[:,1,:])(X)\n\n    D1 = Dense(units = 20, use_bias=True, activation='tanh')\n    DO1 = Dropout(rate=.5)\n\n    D2 = Dense(units = 10, use_bias=True, activation='tanh')\n    DO2 = Dropout(rate=.5)\n\n    X1 = D1(X1)\n    X1 = DO1(X1)\n\n    X1 = D2(X1)\n    X1 = DO2(X1)\n\n    X2 = D1(X2)\n    X2 = DO1(X2)\n\n    X2 = D2(X2)\n    X2 = DO2(X2)\n\n    X_sub = Subtract()([X1,X2])\n\n    output_w= Dense(units = 1, use_bias=False, activation='sigmoid', name='win_output')(X_sub)\n    output_p= Dense(units = 1, use_bias=False, activation=None, name='point_output')(X_sub)\n\n\n    model = Model(inputs=[team_input, loc_input],outputs=[output_w,output_p],name='ncaa_embeddings_joint')\n\n    return model\n\nmymodel = NCAA_Embeddings_Joint(len(id_to_oh),15)\nmymodel.summary()","fa9e9b5c":"#collapse_show\n# Joint model\noptimizer = Adam(learning_rate=.01, beta_1=0.9, beta_2=0.999, amsgrad=False)\nmymodel.compile(loss=['binary_crossentropy','logcosh'],\n                loss_weights=[.5,400],\n                optimizer=optimizer,\n                metrics = ['accuracy'])\nnumBatch = round(X_train[0].shape[0]\/50)\nresults = mymodel.fit(X_train, [*Y_norm_train], validation_data=(X_test, [*Y_norm_test]), epochs = 20, batch_size = numBatch,shuffle=True, verbose=True)","517b19c5":"#collapse_hide\naccuracy = results.history['win_output_accuracy']\nval_accuracy = results.history['val_win_output_accuracy']\nloss = results.history['win_output_loss']\nval_loss = results.history['val_win_output_loss']\n# summarize history for accuracy\nplt.plot(accuracy)\nplt.plot(val_accuracy)\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(loss)\nplt.plot(val_loss)\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","f433cecd":"#collapse_hide\ndef transform_y(preds,stats_cache):\n    preds = stats_cache['var'][1] * preds + stats_cache['mean'][1]\n    return preds\n\npreds = mymodel.predict(X_test)\n\ntmp=0\n\nx = transform_y(preds[1],stats_cache_train).reshape(-1)\ny = transform_y(Y_norm_test[1],stats_cache_train).reshape(-1)\n\n\nprint('Pearson coefficient: ', round(stats.pearsonr(x, y)[0]*100)\/100)\nplt.scatter(x, y, alpha=0.08)\n# plt.title('Scatter plot pythonspot.com')\nplt.xlabel('Predicted point difference')\nplt.ylabel('Actual point difference')\nplt.show()","b2067e94":"#collapse_hide\nx = preds[0].reshape(-1)\n\nplt.hist(x,bins=100)\n# plt.title('Scatter plot pythonspot.com')\nplt.xlabel('Predicted Win Probability')\nplt.ylabel('Count')\nplt.show()","98422015":"#collapse_hide\nembeddings = mymodel.layers[3].get_weights()[0]\n\nt = TSNE(n_components=2)\nembed_tsne = t.fit_transform(embeddings)\n\ndf_regSeason_full['T1_TeamName'] = df_regSeason_full['T1_TeamID'].apply(lambda x: teams_dict[x]) + '-' + df_regSeason_full['Season'].astype(str)\ndf_agg=df_regSeason_full.groupby('T1_TeamName').mean()\ndf_agg.reset_index(inplace=True,drop=False)\ndf_agg['Score_diff'] = -df_agg['Score_diff'] \ndf_agg['Win'] = -df_agg['Win']\ndf_agg[['T1_TeamName','Win','Score_diff']]\ndf_agg.drop(columns='Season',inplace=True)\n\ndf_tourney_data = pd.read_csv(dataLoc\/'MNCAATourneyCompactResults.csv')\ndf_tourney_data['WTeamName'] = df_tourney_data['WTeamID'].apply(lambda x: teams_dict[x]) + '-' + df_tourney_data['Season'].astype(str)\ndf_tourney_data['Wins'] = 0\ndf_wins = df_tourney_data[['WTeamName','Wins']].groupby('WTeamName').count()\ntourneyWinners = [df_tourney_data.loc[df_tourney_data['Season']==s,'WTeamName'].values[-1] for s in df_tourney_data['Season'].unique()]\n\ndf_seeds = pd.read_csv(dataLoc\/'MNCAATourneySeeds.csv')\ndf_seeds['TeamName'] = df_seeds['TeamID'].apply(lambda x: teams_dict[x]) + '-' + df_seeds['Season'].astype(str)\ndf_seeds['Seed'] = df_seeds['Seed'].str.extract(r'(\\d+)')\ndf_seeds['WonTourney'] = df_seeds['TeamName'].apply(lambda x: True if x in tourneyWinners else False)\ndf_seeds = df_seeds[['TeamName','Seed','WonTourney']]\n\ndf_upsets = pd.read_csv('..\/input\/ncaa-biggest-opening-weekend-upsets-from-cbs\/Upsets.csv') # link to article: https:\/\/www.cbssports.com\/college-basketball\/news\/march-madness-2019-the-10-biggest-upsets-ever-in-the-opening-weekend-of-the-tournament\/\ndf_upsets['David']=df_upsets['David']+'-'+df_upsets['Season'].astype(str)\ndf_upsets['Goliath']=df_upsets['Goliath']+'-'+df_upsets['Season'].astype(str)\nupsets = {}\nfor ii in df_upsets['David'].unique():\n    upsets[ii] = 'Surprise'\nfor ii in df_upsets['Goliath'].unique():\n    upsets[ii] = 'Bust'\ndf_seeds = pd.merge(left=df_seeds, right=df_wins, how='left', left_on='TeamName',right_index=True)\ndf_seeds['Wins'].fillna(0,inplace=True)\n\ndef upset(x):\n    try:\n        y = upsets[x]\n    except:\n        y = None\n    return y\ndf_seeds['Upset'] = df_seeds['TeamName'].apply(lambda x: upset(x))\n\ndf = pd.DataFrame(embed_tsne,columns=['factor1','factor2'])\ndf['TeamName'] = [str(teams_dict[int(oh_to_id[x][-4:])]) + '-' + oh_to_id[x][:4] for x in df.index]\ndf['Season'] = [int(oh_to_id[x][:4])for x in df.index]\n\ndf = pd.merge(left=df, right=df_seeds, how='left', on='TeamName')\ndf = pd.merge(left=df, right=df_agg, how='left', left_on='TeamName',right_on='T1_TeamName')\n\ndf = df[['TeamName','Season','factor1','factor2','Win','Score_diff','Seed','Wins','Upset','WonTourney']]\ndf.columns = ['TeamName','Season','factor1','factor2','RegWins','RegPoint_diff','Seed','TourneyWins','Upset','WonTourney']\n\ndf2020 = df[df['Season']==2020].copy()\n\ndf.dropna(inplace=True,subset=['Seed'])\n\ndf['TourneyWinsScaled'] = df['TourneyWins']\/df['TourneyWins'].max()\ndf['SeedScaled'] = df['Seed'].astype(int)\/df['Seed'].astype(int).max()\n\ndf.head()","4e115b8f":"#collapse_hide\nselector = alt.selection_single(empty='all', fields=['TeamName'])\n\nbase = alt.Chart(df).mark_point(filled=True,size=50).encode(\n    color=alt.condition(selector,\n                        alt.Color('WonTourney:N', scale=alt.Scale(scheme='tableau10')),\n                        alt.value('lightgray') ),\n    order=alt.Order('WonTourney:N', sort='ascending'),\n    tooltip=['TeamName','Seed']\n).properties(\n    width=250,\n    height=250\n).add_selection(selector).interactive()\n\nbase.encode(alt.X('factor1:Q', scale=alt.Scale(domain=[-70,60])), alt.Y('factor2:Q', scale=alt.Scale(domain=[-75,50])) )  | base.encode( alt.X('RegPoint_diff:Q', scale=alt.Scale(domain=[-32,11])),alt.Y('RegWins:Q',scale=alt.Scale(domain=[-1.05,-.3])) )","91e7d29d":"#collapse_hide\nselector = alt.selection_single(empty='all', fields=['TeamName'])\n\nbase = alt.Chart(df).mark_point(filled=True,size=35).encode(\n    color=alt.condition(selector,\n                        alt.Color('Seed:Q', scale=alt.Scale(scheme='viridis',reverse=True)),\n                        alt.value('lightgray') ),\n    order=alt.Order('Seed:Q', sort='descending'),\n    tooltip=['TeamName','Seed']\n).properties(\n    width=250,\n    height=250\n).add_selection(selector).interactive()\n\nbase.encode( alt.X('factor1:Q', scale=alt.Scale(domain=[-70,60])), alt.Y('factor2:Q', scale=alt.Scale(domain=[-75,50])) )  | base.encode( alt.X('RegPoint_diff:Q', scale=alt.Scale(domain=[-32,11])),alt.Y('RegWins:Q',scale=alt.Scale(domain=[-1.05,-.3])) )","ed3935b1":"#collapse_hide\nselector = alt.selection_single(empty='all', fields=['TeamName'])\n\nbase = alt.Chart(df).mark_point(filled=True,size=35).encode(\n    color=alt.condition(selector,\n                        alt.Color('TourneyWins:Q', scale=alt.Scale(scheme='viridis',reverse=False)),\n                        alt.value('lightgray') ),\n    order=alt.Order('TourneyWins:Q', sort='ascending'),\n    tooltip=['TeamName','Seed']\n).properties(\n    width=250,\n    height=250\n).add_selection(selector).interactive()\n\nbase.encode( alt.X('factor1:Q', scale=alt.Scale(domain=[-70,60])), alt.Y('factor2:Q', scale=alt.Scale(domain=[-75,50])) )  | base.encode( alt.X('RegPoint_diff:Q', scale=alt.Scale(domain=[-32,11])),alt.Y('RegWins:Q',scale=alt.Scale(domain=[-1.05,-.3])) )","b3b306ee":"#collapse_hide\nselector = alt.selection_single(empty='all', fields=['TeamName'])\n\nbase = alt.Chart(df).mark_point(filled=True,size=50).encode(\n    color=alt.condition(selector,\n                        alt.Color('Upset:N', scale=alt.Scale(scheme='tableau10')),\n                        alt.value('lightgray') ),\n    order=alt.Order('Upset:N', sort='ascending'),\n    tooltip=['TeamName','Seed']\n).properties(\n    width=250,\n    height=250\n).add_selection(selector).interactive()\n\nbase.encode( alt.X('factor1:Q', scale=alt.Scale(domain=[-70,60])), alt.Y('factor2:Q', scale=alt.Scale(domain=[-75,50])) )  | base.encode( alt.X('RegPoint_diff:Q', scale=alt.Scale(domain=[-32,11])),alt.Y('RegWins:Q',scale=alt.Scale(domain=[-1.05,-.3])) )","66fc8fb6":"#collapse_hide\nselect_year = alt.selection_single(\n    name='select', fields=['Season'], init={'Season': 1985},\n    bind=alt.binding_range(min=1985, max=2019, step=1))\n\nselector = alt.selection_single(empty='all', fields=['TeamName'])\n\n##\nbase = alt.Chart(df).mark_point(filled=True,size=50).encode(\n    color=alt.condition(selector,\n                        alt.Color('TourneyWins:Q', scale=alt.Scale(scheme='viridis',reverse=False)),\n                        alt.value('lightgray') ),\n    order=alt.Order('Seed:Q', sort='descending'),\n    tooltip=['TeamName','Seed']\n).properties(\n    width=250,\n    height=250\n).add_selection(select_year).transform_filter(select_year).add_selection(selector).interactive()\n\nbase.encode( alt.X('factor1:Q', scale=alt.Scale(domain=[-70,60])), alt.Y('factor2:Q', scale=alt.Scale(domain=[-75,50])) )  | base.encode( alt.X('RegPoint_diff:Q', scale=alt.Scale(domain=[-32,11])),alt.Y('RegWins:Q',scale=alt.Scale(domain=[-1.05,-.3])) )","54e8dd36":"#collapse_hide\n## 2020 plot\nselector = alt.selection_single(empty='all', fields=['TeamName'])\n\nbase = alt.Chart(df2020).mark_point(filled=True,size=50).encode(\n    color=alt.condition(selector,\n                        alt.Color('TeamName:N'),\n                        alt.value('lightgray') ),\n    order=alt.Order('RegWins:Q', sort='ascending'),\n    tooltip=['TeamName']\n).properties(\n    width=250,\n    height=250\n).add_selection(selector).interactive()\n\nbase.encode( alt.X('factor1:Q', scale=alt.Scale(domain=[-70,60])), alt.Y('factor2:Q', scale=alt.Scale(domain=[-65,75])) )  | base.encode( alt.X('RegPoint_diff:Q', scale=alt.Scale(domain=[-21,26])),alt.Y('RegWins:Q',scale=alt.Scale(domain=[-1,0])) )","d6caac6e":" > Important: For the following plots T-SNE representations of trained embeddings will be on the left and mean regular season statistics will be on the right. The aggregated statistics have also been multiplied by -1 to have a more similar graphical representation to the embeddings - Altair appears to not allow reversing both axes! :(","af6bac64":"## Building the model\nThis model is built with two input types - home\/away flags and team IDs. Each input is repeated for each team and is fed through a location embedding layer and a team embedding layer. A school's embeddings are separate season to season. It would nice to be able to cary some dependency from year to year, but that is completely disregarded here for simplicity. The location embedding is 1-dimensional and multiplied by each team's embedding vector element by element. The team embeddings are separately fed through the same two-layers before being subtracted. This subtracted layerinally connect to two output layers - one 'softmax' for win\/loss prediction and one dense layer with no activation for point prediction.","a5cd2626":"**The 2020 field:** Just for fun here is a taste of what we missed in 2020!\n\n## Conclusions\nThe embeddings apppear to have learned which teams are better and which are worse. It seems that they are a better representation of true team skill than simple aggregating the statistics used in model training (wins and point differentials). When the time comes to build a model for the 2021 March Madness Kaggle competition, I will likely return to embeddings as an advanced input feature for my final model, which will be trained on real tournament games! Then will be the time to experiment with training the team embeddings on advanced statistics included in the detailed Kaggle data set in place of or in addition to the target features used here.\n\n","340e6c35":"**Colored by number of NCAA tournament games won that year:** The embeddings appear to be far less correlated to the number of games won by tournament. This is logical since, unlike the seeds, this statistic is not at all represented in the training set.","75775399":"Embeddings will be defined by the columns 'Season', 'WTeamID', and 'LTeamID'. 'WScore' and 'LScore' will be augmented slightly to be the predicted values and the game location will also be included as an embedding.","6725bf36":"**Tournament winners compared to the field (above):** One interesting insight below is how significantly different the 1985 Villanova team is from the other tournament winners. Multiple websites ([like this one](https:\/\/www.saturdayeveningpost.com\/2018\/05\/10-biggest-underdogs-win-championship\/)) list the 1985 Villanova team winning the championship as one of the greatest underdog stories ever. This is far more evident in the T-SNE representation of the embeddings than the plots of win percentage vs. points.","23d4ec29":"I want to be able to validate that the embedding training is going in the right direction. For the embedding training I will use the secondary tournament data. This allows us to avoid using the NCAA tournament data that we need for training\/testing later, but still get a sense that the embeddings are useful. Here is a preview of that data:","2b930280":"**Number of games won split out by season - yellow dot is tournament winner**: The spread of teams is quite variable year to year. Notably, the tournament that the 1985 Villanova team won as a heavy underdog has less spread in the competition than other years.","e3f1d869":"**Biggest upsets - underdogs in red:** Generally, the model agrees with the experts. These were upsets and wouldn't have been predicted by this method. If anything this method would likely have predicted *no upset* with even greater conviction tahn a model trained on just aggregated points and wins. The only exception to this is the 1986 \"upset\" of Cleveland State over the Indiana Hoosiers. Both the embeddings model and the aggregated statistics indicate that Cleveland State may have been the better team. Perhaps it was an issue of name recognition that lead this to be called an upset?","b1b54fa6":"## Exploratory Analysis\nLet's take a look at some comparisons between our embeddings (mapped non-linearly into 2D space by T-SNE) vs. the aggregated point differential and win percentage of each team. All of these plots (excluding the final plot for 2020) will only include teams that were included in the NCAA tournament that year.\n\nWe'll color the scatter plots by a few different factors:\n - Highlighting tournament winners\n - Tournament seed number\n - Number of tournament games won\n - Biggest opening weekend upsets according to [this article](https:\/\/www.cbssports.com\/college-basketball\/news\/march-madness-2019-the-10-biggest-upsets-ever-in-the-opening-weekend-of-the-tournament\/)\n\nHere is a preview of the data that will be fed into the visualizations:","a820cc54":"One notable aspect of the point prediction result is that the predictions are perfectly symmetrical. The network is able to give consistent predictions for \"Team A vs. Team B\" and \"Team B vs. Team A\" because the neural network is set up to treat the input features consistently for each team. Other ML models, such as XGBoost, treat the feature inputs of Team 1 and Team 2 differently, which can result in varying predictions when the teams are swapped. This can be an issue even when training sets contain matchups **and** swapped matchups [as documented in this discussion](https:\/\/www.kaggle.com\/c\/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament\/discussion\/130855).","c632cec8":"I will be training the embeddings using total point and point differential from each game. Because the training doesn't require the more detailed NCAA data set, we can train using NCAA data all the way back to 1985. Hopfully this will make the weights of the other layers more robust. Depending on the embedding results, the final tournament model could also be trained back to 1985. Let's preview the first few rows of that regular season data here:","2ec736fc":"# NCAA Embeddings\n> Learning complex features of NCAA teams with limited data.\n","feebfad5":"## Packages and Data\n\nI'll be implementing this in Keras. My previous attempt using [FastAI](https:\/\/docs.fast.ai\/) was quick and easy. Using embeddings for categorical data made the FastAI model a bit more elegant than XGBoost. However, we need two input variables (team 1 and team 2) to call the same embeddings matrix in this solution. FastAI can't do that out of the box and so I get to venture into the world of building my own in Keras. I plan on going one step deeper and building my final tournament model with TensorFlow.","99c8eb22":"## Intro\n\nOne of my big projects this past year was preparing to my first Kaggle competition - both the NCAA [Men's](https:\/\/www.kaggle.com\/c\/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament) and [Women's](https:\/\/www.kaggle.com\/c\/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament) basketball tournaments! Unfortunately, the NCAA tournament was cancelled due to the Covid-19 pandemic. Regardless, working through my different ideas was a great way to learn some of the basics and nuances of training an ML model effectively.\n\nI tried a few different types of models including a simple 2 layer neural net using [FastAI](https:\/\/docs.fast.ai\/) and an ensemble model (XGBoost). These were relatively comprable and both required a similar level of feature engineering to get a good result (I think due to the limited data set - ~64 games per year since 2003 with the detailed game data). The results were fine, but I was wondering if there was a solution that could allow for less feature engineering and still give a reasonable result.\n\nEnter embeddings... in this notebook I'm going to explore a simple Keras implementation of embeddings to represent each NCAA team (e.g. Duke 2019 $\\neq$ Duke 2020). For now, I will use those embeddings to perform some exploratory analysis to verify that they have learned useful features and in a following notebook I will use these embeddings as input features to an NCAA tournament model. This work was inspired by [this Kaggle notebook](https:\/\/www.kaggle.com\/abhijitbrahme\/embedding-ncaa-model). Though the predictions aren't spectacular, I think the exploratory analysis shows that embeddings could be a useful feature if paired with more detailed data and a model refined to predict NCAA tournament games as opposed to regular season games.\n\n**What you will see in this notebook at a high level:**\n - Brief data prep - we are only using wins\/losses, points, home\/away, and team IDs as inputs to the model. *This work will later be expanded to incorporate more advanced statistics.\n - Model build - This model is being built with the sole purpose of generating useful embeddings. To achieve that we are training the model to be predictive of features that we would ordinarily use as feature inputs to a real tournament model (in this case, regular season wins and losses).\n - Training and validation - the model is trained using only regular season data from all years and is validated on a secondary set of tournament data ([NIT](https:\/\/en.wikipedia.org\/wiki\/National_Invitation_Tournament)). This is difficult because we have a slight mismatch between our training and validation data. The validation data is generally similar and likely more representative of the real NCAA tournament.\n - Sense check and exploratory analysis - First thing is to check that predictions from the model are sensisble, but what we really care about is the embeddings. Do they carry more useful information than simple aggregations of the data they represent? In short, Yes!","984c0f98":"**Colored by seed:** We see a high correlation between the assigned seed and our embeddings. Our embeddings appear to be a better representation of the seeding than the aggregated statistics, which makes sense since our method uses pair-wise comparisons and effectively accounts for team strength while aggregated statistics do not.","047089b7":"## Results\nThe crossplots for point differential and win\/loss are generally well behaved. The loss and accuracy of the model are not great in comparison to results from the Kaggle competition. For reference, anything below a loss of 0.5 would be considered fantastic and flipping a coin would give you a loss of about 0.69. We see a similar effect in the point spread prediction with a rather loose correlation of 0.46.\n\nIf the goal of this project was to have the best model for predicting the winner of an NCAA tournament game we would be failing (especially considering only the best play in the tournament - making predictions even harder). However, the goal here was to train embeddings not to get accurate predictions. Instead, we are using regular season data to train an embedding set that is representative of each team. We have only trained on wins\/losses and points in this case, which might limit the utility of the features. Converserly, we will see in the next section that we have achieved a richer representation of the raw win\/loss data than simply aggregating by teams.","a6ada0b5":"## Training the model\nThe model is trained using regular season data and validated using secondary tournament data (not the 'Big Dance'). The weights of the two losses are adjusted so that they propogate a similar amount of error backward. Because the point differential data has been normalized, the losses are multiple orders of magnitude less than the log loss metric for wins\/losses."}}