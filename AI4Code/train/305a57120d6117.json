{"cell_type":{"1509d7f8":"code","b5484f4a":"code","89f27ee4":"code","fec4fdd4":"code","a9b63728":"code","57bf4211":"code","e5e73358":"code","eaee891c":"code","f45ae3bb":"code","28099486":"code","80cf49c5":"code","f29e4bb4":"code","1b19a69b":"code","42733159":"code","2bab1076":"code","de2549cd":"markdown","e5a3df50":"markdown"},"source":{"1509d7f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.metrics import confusion_matrix\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b5484f4a":"#2.1. Data Import\ndata = pd.read_csv('..\/input\/Iris.csv')","89f27ee4":"data.head()","fec4fdd4":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder() #Label Encoder s\u0131n\u0131f\u0131ndan bir nesne tan\u0131ml\u0131yoruz\n\n\n#Visualization\nX=data.iloc[:,1:3] #2d data\ny=data.iloc[:,-1:]\ny.iloc[:,0]=le.fit_transform(y.iloc[:,0]) # Now y is dataframe and we have to convert it into numpy array\ny=y.values.astype(int) #convert y into numpy array\ny_new = y.reshape((150,)) #reshape array from (150,1) to (150,) because we use y as color spectral\nprint(type(y_new))\nprint((y_new.shape))\n\nx_min, x_max= X.iloc[:,0].min() - .5, X.iloc[:,0].max() + .5\ny_min, y_max= X.iloc[:,1].min() - .5, X.iloc[:,1].max() + .5\n\nplt.figure(2, figsize=(8,6))\nplt.clf()\n\n\n#plot the training points\nplt.scatter(X.iloc[:,0], X.iloc[:,1], c =y_new, cmap=plt.cm.Set1, edgecolor=\"k\")\nplt.xlabel(\"Sepal length\")\nplt.ylabel(\"Sepal width\")\n\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\n\nfig= plt.figure(1, figsize=(8,6))\nax= Axes3D(fig, elev=-150, azim=110)\nax.scatter(data.iloc[:,1],data.iloc[:,2],data.iloc[:,3], c=y_new, cmap=plt.cm.Set1, edgecolor=\"k\", s=40)\nax.set_title(\"IRIS Verisi\")\nax.set_xlabel(\"birinci ozellik\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"\u0130kinci Ozellik\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"Ucuncu Ozellik\")\nax.w_zaxis.set_ticklabels([])\n\nplt.show()","a9b63728":"x = data.iloc[:,1:5].values #independent variables\ny = data.iloc[:,5:].values #dependent variable\nprint(y)","57bf4211":"#Test and Training Split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test,y_train,y_test = train_test_split(x,y,test_size=0.33, random_state=0)","e5e73358":"#Data scaling\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(x_train)\nX_test = sc.transform(x_test)","eaee891c":"# Classification algorithms start from here\n# 1. Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nlogr = LogisticRegression(random_state=0)\nlogr.fit(X_train,y_train) #train\n\ny_pred = logr.predict(X_test) #prediction\nprint(y_pred)\nprint(y_test)","f45ae3bb":"#Confusion matrix and Accuracy ratio\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)\nprint(\"Accuracy of Logistic Regression algo: \",logr.score(X_test,y_test))","28099486":"# 2. KNN\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5, metric='minkowski')\nknn.fit(X_train,y_train)\n\ny_pred = knn.predict(X_test)\n\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)\nprint(\"Accuracy of KNN algo: \",knn.score(X_test,y_test))","80cf49c5":"# 3. SVC (SVM classifier)\nfrom sklearn.svm import SVC\nsvc = SVC(kernel='rbf') #rfb of poly\nsvc.fit(X_train,y_train)\n\ny_pred = svc.predict(X_test)\n\ncm = confusion_matrix(y_test,y_pred)\nprint('SVC')\nprint(cm)\nprint(\"Accuracy of SVM algo: \",svc.score(X_test,y_test))","f29e4bb4":"# 4. NAive Bayes\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\ny_pred = gnb.predict(X_test)\n\ncm = confusion_matrix(y_test,y_pred)\nprint('GNB')\nprint(cm)\nprint(\"Accuracy of Naive Bayes algo: \",gnb.score(X_test,y_test))","1b19a69b":"\n# 5. Decision tree\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(criterion = 'entropy')\n\ndtc.fit(X_train,y_train)\ny_pred = dtc.predict(X_test)\n\ncm = confusion_matrix(y_test,y_pred)\nprint('DTC')\nprint(cm)\nprint(\"Accuracy of Decision Tree algo: \",dtc.score(X_test,y_test))","42733159":"# 6. Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=10, criterion = 'entropy')\nrfc.fit(X_train,y_train)\n\ny_pred = rfc.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint('RFC')\nprint(cm)\nprint(\"Accuracy of Random Forest algo: \",rfc.score(X_test,y_test))","2bab1076":"# 7. ROC , TPR, FPR de\u011ferleri \n\ny_proba = rfc.predict_proba(X_test)\nprint(y_test)\nprint(y_proba[:,0])\n\nfrom sklearn import metrics\nfpr , tpr , thold = metrics.roc_curve(y_test,y_proba[:,0],pos_label='Iris-virginica')\nprint(fpr)\nprint(tpr)","de2549cd":"# Data Preprocessing","e5a3df50":"# THAT'S ALL"}}