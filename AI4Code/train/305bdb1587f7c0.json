{"cell_type":{"a383d61a":"code","40837d61":"code","a96b6fed":"code","9c7f10cc":"code","2ade24a2":"code","ab66863f":"code","facfa23b":"code","984dd82b":"code","4ba8929a":"code","cd0cf386":"code","1ed41666":"code","7d16341c":"code","7eeac958":"code","d7875fd2":"code","a68e53d8":"code","3641dfeb":"code","3a163219":"code","f57d425e":"code","8762c47d":"code","e9ec3f9d":"code","2240601d":"code","27c3cce7":"code","541dbbd1":"code","3ef9ce30":"code","83f27efd":"code","55116606":"code","fb22fe66":"code","45994a6e":"code","ffbb025b":"code","c07615d2":"markdown","942243cd":"markdown","96da4131":"markdown","46453cbe":"markdown","ef985b72":"markdown","39511f50":"markdown","c70f14fa":"markdown","0a40733b":"markdown","508e4cca":"markdown","1c93e251":"markdown","e2474b9a":"markdown","3162ed95":"markdown","bcc0ba08":"markdown","50dd8756":"markdown","6b8c0e44":"markdown","94ab8023":"markdown","e7a178bd":"markdown","b81a904b":"markdown","da48765e":"markdown","a637a459":"markdown","1a579074":"markdown","6ae78866":"markdown","aef5f19d":"markdown","a1fa6311":"markdown","4fd36997":"markdown","a4f5cad0":"markdown","4f13578f":"markdown","f6eaef9c":"markdown","99b51867":"markdown","787da4a9":"markdown","1b93ce62":"markdown","aed6776d":"markdown","99c1f6ea":"markdown","a4030936":"markdown","1120c872":"markdown","6dcb2902":"markdown","beff5ca1":"markdown","78f673f6":"markdown","fa447b33":"markdown","f9f47c85":"markdown","e8daf8df":"markdown","9b09755e":"markdown","523dc40f":"markdown"},"source":{"a383d61a":"import pandas as pd\nimport numpy as np\nimport re\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.simplefilter('ignore')","40837d61":"path = '..\/input\/'\nhousing = pd.read_csv(path + 'train.csv')\nhousing.head()","a96b6fed":"housing.shape","9c7f10cc":"housing.isnull().sum()","2ade24a2":"housing = housing.loc[:, housing.isnull().sum() < 100]","ab66863f":"housing.columns","facfa23b":"correlations = housing.corr()['SalePrice']\ncorrelations","984dd82b":"correlations[(correlations > 0.5) | (correlations < -0.5)]","4ba8929a":"y = housing['SalePrice']\n\npredictor_cols = ['OverallQual', 'YearBuilt', \n                  'YearRemodAdd', 'TotalBsmtSF', \n                  '1stFlrSF', 'GrLivArea', \n                  'FullBath', 'TotRmsAbvGrd', \n                  'GarageCars', 'GarageArea',\n                 'Fireplaces', 'LotArea']\n\nX = housing[predictor_cols]\nX.head()","cd0cf386":"y.hist();","1ed41666":"y = np.log1p(y)\ny.hist();","7d16341c":"X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                        random_state = 0)","7eeac958":"linreg = LinearRegression().fit(X_train, y_train)\n\nprint('R-squared score (training): {:.3f}'\n     .format(linreg.score(X_train, y_train)))\nprint('R-squared score (validation): {:.3f}'\n     .format(linreg.score(X_valid, y_valid)))","d7875fd2":"from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\nX_train_poly, X_valid_poly, y_train_poly, y_valid_poly = train_test_split(X_poly, y,\n                                                   random_state = 0)","a68e53d8":"polyreg = LinearRegression().fit(X_train_poly, y_train_poly)\n\npolyreg_train_score = polyreg.score(X_train_poly, y_train_poly)\npolyreg_valid_score = polyreg.score(X_valid_poly, y_valid_poly)\n\nprint('R-squared score (training): {:.3f}'\n     .format(polyreg_train_score))\nprint('R-squared score (validation): {:.3f}'\n     .format(polyreg_valid_score))","3641dfeb":"polyreg_lasso = Lasso(alpha=100).fit(X_train_poly, y_train_poly)\n\nprint('R-squared score (training): {:.3f}'\n     .format(polyreg_lasso.score(X_train_poly, y_train_poly)))\nprint('R-squared score (validation): {:.3f}'\n     .format(polyreg_lasso.score(X_valid_poly, y_valid_poly)))","3a163219":"polyreg_ridge = Ridge(alpha=100).fit(X_train_poly, y_train_poly)\n\nprint('R-squared score (training): {:.3f}'\n     .format(polyreg_ridge.score(X_train_poly, y_train_poly)))\nprint('R-squared score (validation): {:.3f}'\n     .format(polyreg_ridge.score(X_valid_poly, y_valid_poly)))","f57d425e":"def get_scores(reg):\n    train_score = reg.score(X_train_poly, y_train_poly)\n    valid_score = reg.score(X_valid_poly, y_valid_poly)\n    return train_score, valid_score\n\ndef get_rmse(reg):\n    y_pred_train = reg.predict(X_train_poly)\n    train_rmse = np.sqrt(mean_squared_error(y_train_poly, y_pred_train))\n    y_pred_valid = reg.predict(X_valid_poly)\n    valid_rmse = np.sqrt(mean_squared_error(y_valid_poly, y_pred_valid))\n    return train_rmse, valid_rmse\n\ndef ridge_validation_curve(alpha):\n    reg = Ridge(alpha=alpha).fit(X_train_poly, y_train_poly)\n    train_score, valid_score = get_scores(reg)\n    train_rmse, valid_rmse = get_rmse(reg)  \n    return train_score, valid_score, train_rmse, valid_rmse\n\ndef lasso_validation_curve(alpha):\n    reg = Lasso(alpha=alpha).fit(X_train_poly, y_train_poly)\n    train_score, valid_score = get_scores(reg)\n    train_rmse, valid_rmse = get_rmse(reg)  \n    return train_score, valid_score, train_rmse, valid_rmse\n\nalphas = [0.1, 1, 5, 25, 50, 75, 100, 200, 300, 400, 500, 750, 1000, 2000]\n\nscores_lasso = [lasso_validation_curve(alpha) for alpha in alphas]\nscores_lasso_train = [s[0] for s in scores_lasso]\nscores_lasso_valid = [s[1] for s in scores_lasso]\nrmse_lasso_train = [s[2] for s in scores_lasso]\nrmse_lasso_valid = [s[3] for s in scores_lasso]\n\nscores_ridge = [ridge_validation_curve(alpha) for alpha in alphas]\nscores_ridge_train = [s[0] for s in scores_ridge]\nscores_ridge_valid = [s[1] for s in scores_ridge]\nrmse_ridge_train = [s[2] for s in scores_ridge]\nrmse_ridge_valid = [s[3] for s in scores_ridge]\n\nscores_poly_train = [polyreg_train_score]*len(alphas)\nscores_poly_valid = [polyreg_valid_score]*len(alphas)\ny_pred_train = polyreg.predict(X_train_poly)\nrmse_poly_train = [mean_squared_error(y_train_poly, y_pred_train)]*len(alphas)\ny_pred_valid = polyreg.predict(X_valid_poly)\nrmse_poly_valid = [mean_squared_error(y_valid_poly, y_pred_valid)]*len(alphas)","8762c47d":"plt.figure(figsize=(10, 6));\nplt.ylim([0.65, 0.9])\nplt.xlabel('Regularization parameter (alpha)')\nplt.ylabel('R-squared')\nplt.title('R-squared scores as function of regularization')\n\nplt.plot(alphas, scores_ridge_train, label='Poynomial with Ridge (training)')\nplt.plot(alphas, scores_poly_train, label='Polynomial (training)')\nplt.plot(alphas, scores_lasso_train, label='Poynomial with Lasso (training)')\n\nplt.plot(alphas, scores_lasso_valid, label='Poynomial with Lasso (validation)')\nplt.plot(alphas, scores_ridge_valid, label='Poynomial with Ridge (validation)')\nplt.plot(alphas, scores_poly_valid, label='Polynomial (validation)')\nplt.legend(loc=4);","e9ec3f9d":"plt.figure(figsize=(11, 6));\nplt.ylim([0.012, 0.3])\nplt.xlabel('Regularization parameter (alpha)')\nplt.ylabel('Root Mean-squared Error(RMSE)')\nplt.title('Root Mean-squared Error(RMSE) as a function of regularization')\n\nplt.plot(alphas, rmse_lasso_valid, label='Poynomial with Lasso (validation)')\nplt.plot(alphas, rmse_ridge_valid, label='Poynomial with Ridge (validation)')\nplt.plot(alphas, rmse_poly_valid, label='Polynomial (validation)')\n\nplt.plot(alphas, rmse_poly_train, label='Poynomial (training)')\nplt.plot(alphas, rmse_lasso_train, label='Poynomial with Lasso (training)')\nplt.plot(alphas, rmse_ridge_train, label='Poynomial with Ridge (training)')\n\nplt.legend(loc=1);","2240601d":"housing_test = pd.read_csv(path + 'test.csv')\nId = housing_test['Id']\nX_test = housing_test[predictor_cols]\nX_test.head()","27c3cce7":"X_test.isnull().sum()","541dbbd1":"X_test = X_test.fillna(method='ffill')","3ef9ce30":"reg = LinearRegression().fit(X_poly, y)","83f27efd":"X_test_poly = poly.transform(X_test)\npredictions = reg.predict(X_test_poly)\npredictions[:10]","55116606":"predictions = np.expm1(predictions) \npredictions[:10]","fb22fe66":"sample_submission = pd.read_csv(path + 'sample_submission.csv')\nsample_submission.head()","45994a6e":"submission = pd.DataFrame({'Id': Id,\n                          'SalePrice': predictions})\n\nsubmission.head()","ffbb025b":"submission.to_csv('my_submission.csv', index=False)","c07615d2":"To compare the effect of regularization parameter (alpha) as well as to compare different models, we define a few functions to plot the $R^2$ scores and another metric called root mean-squared error (RMSE), explained below. Please feel free to skip the code entirely.","942243cd":"On account of increased complexity, polynomial regression models can be prone to overfitting. In that case, they can be coupled with Ridge regression, Lasso regression or Elastic Net regression - extensions of linear regression algorithm that use regularization, a method to address overfitting.\n\n### Regularization:\nWhen we have weights that are higher in magnitude, the model is more likely to overfit, so we penalize the weights using a penalty term called regularization parameter (alpha) that keeps the weights small and thereby simplify the model.\n\nTo the linear regression formulation of the cost function, we add the model weights multiplied by the regularization parameter (alpha) so that when the learning process minimizes the cost function while updating the weights, it automatically keeps the weights in check. There are two common ways to add the weights term (using $L1$ and $L2$-norms) and hence the two different algorithms as below.\n\nCost function for Linear regression:\n$$ J = \\frac{1}{2 n} \\sum_{i=1}^n (y^{(i)} - y_{pred}^{(i)})^2 $$\n\nCost function for Ridge regression ($L2$-norm):\n$$ J = \\frac{1}{2 n} \\sum_{i=1}^n (y^{(i)} - y_{pred}^{(i)})^2 + \\alpha \\sum_{j=1}^m w_j^2$$\n\nCost function for Lasso regression ($L1$-norm):\n$$ J = \\frac{1}{2 n} \\sum_{i=1}^n (y^{(i)} - y_{pred}^{(i)})^2 + \\alpha \\sum_{j=1}^m |w_j|$$\n\n\nFor each algorithm, the learning process is the same as linear regression, that is we update the weights using gradient descent to minimize the respective cost function. Since weights are included in the cost function, the learning process takes a balanced approach between minimizing mean-square errors and the weights, thereby reducing overfitting. \n\nThis technique of regularization by adding the weight term to the cost function using either $L1$ or $L2$-norm is one of the two most commonly used and crucial techniques to address overfitting in deep neural networks as well. The other one being [Dropout regularization](https:\/\/medium.com\/@amarbudhiraja\/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5) - dropping out units in the network.\n\n\nWe use [`sklearn.linear_model.Lasso`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Lasso.html) to train a Lasso regression model on the polynomial features.","96da4131":"We will learn regression techniques using [House Prices: Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview) dataset. The aim is to predict house prices based on a set of features.","46453cbe":"These are the predictions for the sales prices for the houses. We have built a simple baseline model with a lot of scope for improvement, but it is ready for submission.","ef985b72":"It seems like the data is skewed to the right, that is there are a few houses with extra-ordinarily high prices. For linear regression techniques, symmetric data is more conducive to work with, so we take a log transform of the target variable and then plot its distribution.","39511f50":"The [metric used](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview\/evaluation) for evaluating the submissions for the competition is given below:\n\n> Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\n\nWe have already taken the log transformation for `y` to make sure \"that errors in predicting expensive houses and cheap houses will affect the result equally.\" We have encountered Root-Mean-Squared-Error (RMSE) in a disguised form in the cost function for linear regression and will revisit it later. ","c70f14fa":"This is one of the notebooks for the third session of the [Machine Learning workshop series at Harvey Mudd College](http:\/\/www.aashitak.com\/ML-Workshops\/). It involves a gentle introduction overviewing the various regression algorithms. Please follow your own pace. It would be wise to overlook the code at this point and pay attention to the explanation and review the results and charts and develop the conceptual understanding. In the exercise notebook, you will work with another dataset and apply the algorithms learned here. With that practise, the code will become clearer.\n\nYou are most welcome to ask questions and discuss the concepts with the instructor or the TAs.","0a40733b":"We discard all the columns that have more than 100 missing values.","508e4cca":"## Regression algorithms notebook (60 min)","1c93e251":"We import python modules:","e2474b9a":"Let us look at the remaining columns.","3162ed95":"There are ***81 columns in total***. Before looking deeply into the columns, we might want to first discard the columns that have a lot of missing values. We can revisit later to include and process all the columns, but at first we would want to work with fewer columns.","bcc0ba08":"Now we plot the $R^2$ scores for the four regression algorithms for different values of alpha for both training and validation set.","50dd8756":"Please try different alpha values for the above, such as alpha=5, 500, 5000, etc. and see how it affects the model performance.\n\nNow we use [`sklearn.linear_model.Ridge`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Ridge.html) to train a Ridge regression model. Please try to tune the alpha parameter to find an optimal value.","6b8c0e44":"Please proceed on to the [Exercise 3 notebook](https:\/\/www.kaggle.com\/aashita\/exercise-3).","94ab8023":"First we split the data into training and validation set.","e7a178bd":"[Features' description](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data):\n* SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n* OverallQual: Overall material and finish quality\n* YearBuilt: Original construction date\n* YearRemodAdd: Remodel date\n* TotalBsmtSF: Total square feet of basement area\n* 1stFlrSF: First Floor square feet\n* GrLivArea: Above grade (ground) living area square feet\n* FullBath: Full bathrooms above grade\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n* GarageCars: Size of garage in car capacity\n* GarageArea: Size of garage in square feet\n* Fireplaces: Number of fireplaces\n* LotArea: Lot size in square feet","b81a904b":"Polynomial regression model regularized using Lasso seems to be performing the best since it has the highest $R^2$ score on the validation set and almost no difference between the $R^2$ scores on the training and validation sets.\nConsidering the $R^2$ values on the validation set, the order of performance from the best to worse changes is:\n1. Polynomial regression coupled with Lasso\n2. Polynomial regression coupled with Ridge\n3. Polynomial regression","da48765e":"We fill in missing values for the columns.","a637a459":"Now we finally predict the examples in the `test.csv` file and [submit our predict to the competition](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview\/frequently-asked-questions). ","1a579074":"Let us also look at the root mean-squared error (RMSE). The mean-squared error (MSE) happens to be implicitly included in the cost function for all the regression methods seen so far.\n$$ \\frac{1}{n} \\sum_{i=1}^n (y^{(i)} - y_{pred}^{(i)})^2 $$\n\nIt is a measure of the error in predicting the target values. This also happens to be the metric used to evaluate the regression model for this dataset.\n\nWe will use [`sklearn.metrics.mean_squared_error`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_squared_error.html) function followed by `np.sqrt()` (square-root) to calculate the RMSE and plot a graph to compare the RMSE values for the above four models.","6ae78866":"We pick a few columns for now to train a model. We can anytime revisit and try a different set of columns for prediction.","aef5f19d":"### Acknowledgement:\n\nThe credits for the images used in the above are as follows.\n- Image 1: https:\/\/commons.wikimedia.org\/wiki\/File:Gaussian_kernel_regression.png","a1fa6311":"Next we transform the test data features to the quadratic polynomial features using the transformer `poly` that we fit earlier.","4fd36997":"Now we choose a model to build our final submission, you can pick any. I have simply used the polynomial regression trained on the ***entire data set*** transformed by the polynomial features, that is `X_poly, y`.\n\nNote that: We don't anymore need to keep the validation set out of training since we have chosen the regression algorithm and the alpha value already, so we are better off using the entire set for training. ","a4f5cad0":"It is great to see that the performance on the validation set has increased noticably by using the polynomial features.","4f13578f":"We save the dataframe as a csv file.","f6eaef9c":"Now we have a look at the sample submission. It is important that our submission file is in correct format to be graded without errors.","99b51867":"Let us see how does the columns correlate with the *SalePrice* using the Pearson correlation coefficients that we learned in the first session.","787da4a9":" We transform the original input features using [`sklearn.preprocessing.PolynomialFeatures`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.PolynomialFeatures.html) to add polynomial features up to degree 2 (quadratic).","1b93ce62":"Then we use the built-in [`LinearRegression()`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html) with the polynomial features to build a polynomial regression model.","aed6776d":"Let us select only those columns that have a correlation co-efficient of 0.5 or higher in magnitude with the *SalePrice* column.","99c1f6ea":"Then we train a linear regression model using the training set and calculate the $R^2$ score for both training and validation set. ","a4030936":"Go to the [competition page](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview\/evaluation) and make a submission by uploading the `my_submission.csv` file generated thru this notebook that is automatically stored in the same folder as this notebook in your laptop.\n\nNote: If you are using Kaggle kernels, then\n1. Commit the notebook\n2. Leave the edit mode\n3. Navigate to the output section\n4. Download the csv file\n5. Go to the [competition page](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview\/evaluation) \n6. Make a submission by uploading the `my_submission.csv` file","1120c872":"We revise that higher the $R^2$ score, the better fit is the regression curve to the data points. We also note that a big difference between the $R^2$ scores for the training and validation sets suggests overfitting.  ","6dcb2902":"As we learned earlier, the coefficient of determination, [R-squared](http:\/\/www.fairlynerdy.com\/what-is-r-squared\/) (denoted by $R^2$), is a statistical measure of how close the data are to the fitted regression line. We note there is quite some difference between the $R^2$ values for the training and the validation set.","beff5ca1":"Lower root mean-squared error (RMSE) is preferable. The root mean-squared error (RMSE) on the validation set follows the ascending order:\n1. Polynomial regression \n2. Polynomial regression coupled with Lasso\n3. Polynomial regression coupled with Ridge\n\nPolynomial regression coupled with Lasso has little to no difference between the RMSE for training and validation set, which suggests it is likely not overfitting to the training set. However, the simple polynomial regression model has a much lower RMSE value on both training and validation sets compared to all other algorithms. ","78f673f6":"### Optional:\n\nFeature engineering\n1. Try out different sets of columns, even including those that have missing values by filling them first. You can also explore using polynomial features of higher or lower degrees than 2 and choosing the regularization accordingly.\n\n","fa447b33":"Now, the target variable `y` is more symmetrically distributed (It is closer to [normal (or guassian) distribution also known as the bell curve](https:\/\/www.khanacademy.org\/math\/statistics-probability\/modeling-distributions-of-data\/more-on-normal-distributions\/v\/introduction-to-the-normal-distribution) if you are familiar with it).","f9f47c85":"Since we took the log transform for the target variable at the beginning, for the final answer, we need to use its inverse function, that is exponential to undo the transformation for the predictions.","e8daf8df":"We create a dataframe for submission.","9b09755e":"Let us have a look at how the sales prices are distributed by plotting the histogram.","523dc40f":"### Polynomial regression\nAn extension of linear regression is polynomial regression, which fits a polynomial curve instead of a line to the data points. \n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/8\/8a\/Gaussian_kernel_regression.png\" width=\"300\" height=\"350\" \/>\n<p style=\"text-align: center;\"> Polynomial regression curve of degree 3 (cubic) <\/p> \n\n \n\nAs a refresher, the equation for linear regression with features (variables) $x_1, x_2, \\dots, x_n$ is \n$$ y_{pred} = b + w_1 * x_1 + w_2 * x_2 + \\cdots + w_n * x_n$$\n\nTo model polynomial regression, we simply take higher degrees of the features (variables) and build a linear regression model on them.\nFor example, the polynomial regression equation for the cubic curve in the above figure would be would be \n$$ y_{pred} = b + w_1 * x + w_2 * x^2 + w_3 * x^3$$ \n\nIt can also be thought of as a linear regression model with three variables $x$, $x^2$ and $x^3$. So, we first transform the feature $x$ to create two additional features $x^2$ and $x^3$ by simply taking squares and cubes of the values of feature $x$ and then train a linear regression model on the three features $x$, $x^2$ and $x^3$."}}