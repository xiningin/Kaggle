{"cell_type":{"ab755dd9":"code","1e98e467":"code","1ef45bdc":"code","6910f550":"code","ffb3c33e":"code","1f1cf902":"code","47896b3c":"code","1753c513":"code","6cb81901":"code","70b1b3b9":"code","c413c375":"code","c8312c0b":"code","be9bd36b":"code","22b3a811":"code","c9266a0c":"code","93e624c0":"code","48c2282d":"code","9c311595":"markdown","73ad86be":"markdown","bf53eb71":"markdown"},"source":{"ab755dd9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1e98e467":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\ny_train = train['Survived']\ny_test = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","1ef45bdc":"train.Age.hist(bins=20, by=train['Sex'])","6910f550":"sns.barplot(x=train['Sex'], y=train['Age'])","ffb3c33e":"train.columns","1f1cf902":"train.info()","47896b3c":"import matplotlib.pyplot as plt\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","1753c513":"k = 10\ncols = corrmat.nlargest(k, 'Survived')['Survived'].index\ncm = np.corrcoef(train[cols].values.T)\nplt.figure(figsize=(10,5))\nsns.set(font_scale=1.5)\nhm = sns.heatmap(cm, cbar=True, annot=True, annot_kws={'size':15}, yticklabels=cols.values, xticklabels=cols.values, fmt=\".2f\")\nhm.set_title(\"Titanic\")","6cb81901":"sns.set()\nsns.pairplot(train[cols], height=2.5)","70b1b3b9":"total = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nsummary = pd.concat([total, percent], axis = 1, keys = ['Total', 'Percent'])\nsummary","c413c375":"#my_imputer = SimpleImputer(missing_value=np.nan, strategy='median', axis=0)\ntrain['Age'] = train['Age'].fillna(train['Age'].median())\ntest['Age'] = test['Age'].fillna(test['Age'].median())","c8312c0b":"total = train['Age'].isnull().sum()\ntotal","be9bd36b":"train.columns","22b3a811":"cols_new = ['Parch','SibSp', 'Age', 'Pclass']\nX = pd.get_dummies(train[cols_new])\nX_test = pd.get_dummies(test[cols_new])","c9266a0c":"from sklearn.neighbors import NearestCentroid\nmodel = NearestCentroid()\nmodel.fit(X, y_train)","93e624c0":"X_test","48c2282d":"predictions = model.predict(X_test)\n\nsubmissions = pd.DataFrame({'PassengerId':test.PassengerId, 'Survived':predictions})\nsubmissions.to_csv('submission.csv', index=False)\nprint('Finished!')","9c311595":"# Find Missing Data","73ad86be":"From the table above, we see that **Cabin** and **Age** are the two data that have > 15% missing values, meaning that we should probably dump them. However, because age provides so much value, we will still keep **Age**.","bf53eb71":"# Imputation"}}