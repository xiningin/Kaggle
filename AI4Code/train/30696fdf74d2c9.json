{"cell_type":{"9e6fae46":"code","44373450":"code","128eba62":"code","7b5101c2":"code","75ddc91b":"code","fd42d47b":"code","1015a875":"code","c4349ea3":"code","7762352d":"code","a45c679b":"code","0a76f129":"code","94e6fa54":"code","42e7e28d":"code","f0a8221d":"code","e45e728f":"code","444ccfe5":"code","fe0789e4":"code","5fb4c0e7":"code","af27bdcc":"code","1da9bed5":"code","3a5e4028":"code","9be67c82":"code","eaca3bc3":"code","e2bb9f50":"code","7ef9abcd":"code","be0d12ff":"code","5fa0c378":"code","c9e70c60":"code","c4a34f81":"code","ad25739e":"code","3a5da57a":"code","ba1bf2c3":"code","ba062cb9":"code","63437bc3":"code","dedd4b32":"code","54c2a543":"code","1545e6a7":"code","c0367da6":"code","88b75bdb":"code","d2808368":"code","c4a58f57":"code","c65c56b4":"code","3fcdb488":"code","412e7853":"code","dde48f36":"code","02eceaa3":"code","f35fffbd":"code","a8e0d5a6":"code","8167b5dd":"code","242b11f8":"code","7c1e6529":"code","4f3ccd5f":"code","8d111efe":"code","c9c7711b":"code","a485febe":"code","4a58c2f2":"code","8c3c2393":"code","ce9a9e4d":"code","2544ca6b":"code","236bb344":"code","c8824750":"code","bd794152":"code","81845c0e":"code","ec055bf9":"code","f591fa54":"code","2206e34c":"code","8d3ceef5":"code","251f38e3":"code","83aa32fe":"code","1fca82fe":"code","ac3df0dd":"code","4ceb3dd4":"code","6a72cc6c":"code","de83ad1c":"code","38344746":"markdown","c755c146":"markdown","2d03c4c9":"markdown","5073c643":"markdown","c62ce9b0":"markdown","0b549d73":"markdown","dd957108":"markdown","9eabb370":"markdown","0c480262":"markdown","dabaca59":"markdown","8d5739f1":"markdown","167ef77c":"markdown","52185ecf":"markdown","3de8bbce":"markdown","37990ba1":"markdown","13f0befb":"markdown","f5879414":"markdown","70673f0c":"markdown","fcb7c163":"markdown","dd931420":"markdown","f7bfd1a0":"markdown","dab1f082":"markdown","7685850c":"markdown","0fd5df09":"markdown","0051c3eb":"markdown","553ef694":"markdown","794da599":"markdown","213cf443":"markdown","fef4cc3c":"markdown","a7abff34":"markdown","f43d0601":"markdown","b911ee2c":"markdown","bb1992d3":"markdown","c04ce6c0":"markdown","75e0346b":"markdown","62f4e28b":"markdown","97d0c960":"markdown","b96984a7":"markdown","0ac97442":"markdown","9e3bac14":"markdown","bc303195":"markdown","5524468b":"markdown","ffc9af28":"markdown","01893102":"markdown","70e64972":"markdown","effe6ffe":"markdown","483674f3":"markdown","3c890db4":"markdown"},"source":{"9e6fae46":"import warnings\nwarnings.filterwarnings('ignore')","44373450":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #data visualization\nimport seaborn as sns\nfrom operator import add\n\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","128eba62":"#load the data\ndata = pd.read_csv('\/kaggle\/input\/framingham-heart-study-dataset\/framingham.csv')\ndata.drop(['education'],axis=1,inplace=True)\ndata.head()","7b5101c2":"#total percentage of missing data\nmissing_data = data.isnull().sum()\ntotal_percentage = (missing_data.sum()\/data.shape[0]) * 100\nprint(f'The total percentage of missing data is {round(total_percentage,2)}%')","75ddc91b":"# percentage of missing data per category\ntotal = data.isnull().sum().sort_values(ascending=False)\npercent_total = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending=False)*100\nmissing = pd.concat([total, percent_total], axis=1, keys=[\"Total\", \"Percentage\"])\nmissing_data = missing[missing['Total']>0]\nmissing_data","fd42d47b":"plt.figure(figsize=(9,6))\nsns.set(style=\"whitegrid\")\nsns.barplot(x=missing_data.index, y=missing_data['Percentage'], data = missing_data)\nplt.title('Percentage of missing data by feature')\nplt.xlabel('Features', fontsize=14)\nplt.ylabel('Percentage', fontsize=14)\nplt.show()","1015a875":"# drop missing entries\ndata.dropna(axis=0, inplace=True)","c4349ea3":"data.shape","7762352d":"# plot histogram to see the distribution of the data\nfig = plt.figure(figsize = (15,20))\nax = fig.gca()\ndata.hist(ax = ax)\nplt.show()","a45c679b":"sns.countplot(x='TenYearCHD',data=data)\nplt.show()\ncases = data.TenYearCHD.value_counts()\nprint(f\"There are {cases[0]} patients without heart disease and {cases[1]} patients with the disease\")","0a76f129":"def stacked_barchart(data, title = None, ylabel = None, xlabel = None):\n    default_colors = ['#008080', '#5f3c41', '#219AD8']\n    # From raw value to percentage\n    totals = data.sum(axis=1)\n    bars = ((data.T \/ totals) * 100).T\n    r = list(range(data.index.size))\n\n    # Plot\n    barWidth = 0.95\n    names = data.index.tolist()\n    bottom = [0] * bars.shape[0]\n\n    # Create bars\n    color_index = 0\n    plots = []\n    for bar in bars.columns:\n        plots.append(plt.bar(r, bars[bar], bottom=bottom, color=default_colors[color_index], edgecolor='white', width=barWidth))\n        bottom = list(map(add, bottom, bars[bar]))\n        color_index = 0 if color_index >= len(default_colors) else color_index + 1\n\n    # Custom x axis\n    plt.title(title)\n    plt.xticks(r, names)\n    plt.xlabel(data.index.name if xlabel is None else xlabel)\n    plt.ylabel(data.columns.name if ylabel is None else ylabel)\n    ax = plt.gca()\n        \n    y_labels = ax.get_yticks()\n    ax.set_yticklabels([str(y) + '%' for y in y_labels])\n\n    flat_list = [item for sublist in data.T.values for item in sublist]\n    for i, d in zip(ax.patches, flat_list):\n        data_label = str(d) + \" (\" + str(round(i.get_height(), 2)) + \"%)\"\n        ax.text(i.get_x() + 0.45, i.get_y() + 5, data_label, horizontalalignment='center', verticalalignment='center', fontdict = dict(color = 'white', size = 20))\n\n    for item in ([ax.title]):\n        item.set_fontsize(27)\n        \n    for item in ([ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n        item.set_fontsize(24)\n    \n    legend = ax.legend(plots, bars.columns.tolist(), fancybox=True)\n    plt.setp(legend.get_texts(), fontsize='20')","94e6fa54":"fig = plt.gcf()\nfig.set_size_inches(25, 35)\ngrid_rows = 3\ngrid_cols = 2\n\n#draw sex vs disease outcome\nplt.subplot(grid_rows, grid_cols, 1)\ntemp = data[['male','TenYearCHD']].groupby(['male','TenYearCHD']).size().unstack('TenYearCHD')\ntemp.rename(index={0:'Female', 1:'Male'}, columns={0:'No Disease', 1:'Has Disease'}, inplace = True)\nstacked_barchart(temp, title = 'CHD vs Sex', ylabel = 'Population')\n\n#draw smoking satus vs disease outcome\nplt.subplot(grid_rows, grid_cols, 2)\ntemp = data[['currentSmoker','TenYearCHD']].groupby(['currentSmoker','TenYearCHD']).size().unstack('TenYearCHD')\ntemp.rename(index={0:'Not a Smoker', 1:'Smoker'}, columns={0:'No Disease', 1:'Has Disease'}, inplace = True)\nstacked_barchart(temp, title = 'CHD vs Smoking', ylabel = 'Population')\n\n#draw diabetes vs disease outcome\nplt.subplot(grid_rows, grid_cols, 3)\ntemp = data[['diabetes','TenYearCHD']].groupby(['diabetes','TenYearCHD']).size().unstack('TenYearCHD')\ntemp.rename(index={0:'Not Diabetic', 1:'Diabetic'}, columns={0:'No Disease', 1:'Has Disease'}, inplace = True)\nstacked_barchart(temp, title = 'CHD vs Diabetes', ylabel = 'Population')\n\n#draw BP meds vs disease outcome\nplt.subplot(grid_rows, grid_cols, 4)\ntemp = data[['BPMeds','TenYearCHD']].groupby(['BPMeds','TenYearCHD']).size().unstack('TenYearCHD')\ntemp.rename(index={0:'Not on medication', 1:'On Medication'}, columns={0:'No Disease', 1:'Has Disease'}, inplace = True)\nstacked_barchart(temp, title = 'CHD vs BP meds', ylabel = 'Population')\n\n#draw Hypertension vs disease outcome\nplt.subplot(grid_rows, grid_cols, 5)\ntemp = data[['prevalentHyp','TenYearCHD']].groupby(['prevalentHyp','TenYearCHD']).size().unstack('TenYearCHD')\ntemp.rename(index={0:'Not Hypertensive', 1:'Hypertensive'}, columns={0:'No Disease', 1:'Has Disease'}, inplace = True)\nstacked_barchart(temp, title = 'CHD vs Hypertension', ylabel = 'Population')","42e7e28d":"positive_cases = data[data['TenYearCHD'] == 1]\nplt.figure(figsize=(15,6))\nsns.countplot(x='age',data = positive_cases, hue = 'TenYearCHD', palette='husl')\nplt.show()","f0a8221d":"plt.figure(figsize=(15,8))\nsns.heatmap(data.corr(), annot = True)\nplt.show()","e45e728f":"from sklearn.ensemble import RandomForestClassifier\nfrom boruta import BorutaPy","444ccfe5":"#define the features\nX = data.iloc[:,:-1].values\ny = data.iloc[:,-1].values\n\nforest = RandomForestClassifier(n_estimators=1000, n_jobs=-1, class_weight='balanced')\n\n# define Boruta feature selection method\nfeat_selector = BorutaPy(forest, n_estimators='auto', verbose=2)\n \n# find all relevant features\nfeat_selector.fit(X, y)","fe0789e4":"# show the most important features\nmost_important = data.columns[:-1][feat_selector.support_].tolist()\nmost_important","5fb4c0e7":"# select the top 6 features\ntop_features = data.columns[:-1][feat_selector.ranking_ <=6].tolist()\ntop_features","af27bdcc":"import statsmodels.api as sm","1da9bed5":"X_top = data[top_features]\ny = data['TenYearCHD']","3a5e4028":"res = sm.Logit(y,X_top).fit()\nres.summary()","9be67c82":"params = res.params\nconf = res.conf_int()\nconf['Odds Ratio'] = params\nconf.columns = ['5%', '95%', 'Odds Ratio']\nprint(np.exp(conf))","eaca3bc3":"sns.pairplot(data, hue = 'TenYearCHD', markers=[\"o\", \"s\"], vars = top_features, palette = sns.color_palette(\"bright\", 10))","e2bb9f50":"from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nfrom collections import Counter","7ef9abcd":"X = data[top_features]\ny = data.iloc[:,-1]","be0d12ff":"# the numbers before SMOTE\nnum_before = dict(Counter(y))\n\n#perform SMOTE\n\n# define pipeline\nover = SMOTE(sampling_strategy=0.8)\nunder = RandomUnderSampler(sampling_strategy=0.8)\nsteps = [('o', over), ('u', under)]\npipeline = Pipeline(steps=steps)\n\n# transform the dataset\nX_smote, y_smote = pipeline.fit_resample(X, y)\n\n\n#the numbers after SMOTE\nnum_after =dict(Counter(y_smote))","5fa0c378":"print(num_before, num_after)","c9e70c60":"labels = [\"Negative Cases\",\"Positive Cases\"]\nplt.figure(figsize=(15,6))\nplt.subplot(1,2,1)\nsns.barplot(labels, list(num_before.values()))\nplt.title(\"Numbers Before Balancing\")\nplt.subplot(1,2,2)\nsns.barplot(labels, list(num_after.values()))\nplt.title(\"Numbers After Balancing\")\nplt.show()","c4a34f81":"# new dataset\nnew_data = pd.concat([pd.DataFrame(X_smote), pd.DataFrame(y_smote)], axis=1)\nnew_data.columns = ['age', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose','TenYearCHD']\nnew_data.head()","ad25739e":"X_new = new_data[top_features]\ny_new= new_data.iloc[:,-1]\nX_new.head()","3a5da57a":"# split the dataset\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X_new,y_new,test_size=.2,random_state=42)","ba1bf2c3":"from sklearn.preprocessing import StandardScaler","ba062cb9":"scaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_train = pd.DataFrame(X_train_scaled)\n\nX_test_scaled = scaler.transform(X_test)\nX_test = pd.DataFrame(X_test_scaled)","63437bc3":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import recall_score,precision_score,classification_report,roc_auc_score,roc_curve","dedd4b32":"# search for optimun parameters using gridsearch\nparams = {'penalty':['l1','l2'],\n         'C':[0.01,0.1,1,10,100],\n         'class_weight':['balanced',None]}\nlogistic_clf = GridSearchCV(LogisticRegression(),param_grid=params,cv=10)","54c2a543":"#train the classifier\nlogistic_clf.fit(X_train,y_train)\n\nlogistic_clf.best_params_","1545e6a7":"#make predictions\nlogistic_predict = logistic_clf.predict(X_test)","c0367da6":"log_accuracy = accuracy_score(y_test,logistic_predict)\nprint(f\"Using logistic regression we get an accuracy of {round(log_accuracy*100,2)}%\")","88b75bdb":"cm=confusion_matrix(y_test,logistic_predict)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")","d2808368":"print(classification_report(y_test,logistic_predict))","c4a58f57":"logistic_f1 = f1_score(y_test, logistic_predict)\nprint(f'The f1 score for logistic regression is {round(logistic_f1*100,2)}%')","c65c56b4":"# ROC curve and AUC \nprobs = logistic_clf.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nlog_auc = roc_auc_score(y_test, probs)\n\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(y_test, probs)\n# plot curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(fpr, tpr, marker='.')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.title(f\"AUC = {round(log_auc,3)}\")\nplt.show()","3fcdb488":"from sklearn.neighbors import KNeighborsClassifier","412e7853":"# search for optimun parameters using gridsearch\nparams= {'n_neighbors': np.arange(1, 10)}\ngrid_search = GridSearchCV(estimator = KNeighborsClassifier(), param_grid = params, \n                           scoring = 'accuracy', cv = 10, n_jobs = -1)\nknn_clf = GridSearchCV(KNeighborsClassifier(),params,cv=3, n_jobs=-1)","dde48f36":"# train the model\nknn_clf.fit(X_train,y_train)\nknn_clf.best_params_ ","02eceaa3":"# predictions\nknn_predict = knn_clf.predict(X_test)","f35fffbd":"#accuracy\nknn_accuracy = accuracy_score(y_test,knn_predict)\nprint(f\"Using k-nearest neighbours we get an accuracy of {round(knn_accuracy*100,2)}%\")","a8e0d5a6":"cm=confusion_matrix(y_test,knn_predict)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")","8167b5dd":"print(classification_report(y_test,knn_predict))","242b11f8":"knn_f1 = f1_score(y_test, knn_predict)\nprint(f'The f1 score for K nearest neignbours is {round(knn_f1*100,2)}%')","7c1e6529":"# ROC curve and AUC \nprobs = knn_clf.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nknn_auc = roc_auc_score(y_test, probs)\n\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(y_test, probs)\n# plot curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(fpr, tpr, marker='.')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.title(f\"AUC = {round(knn_auc,3)}\")\nplt.show()","4f3ccd5f":"from sklearn.tree import DecisionTreeClassifier\ndtree= DecisionTreeClassifier(random_state=7)","8d111efe":"# grid search for optimum parameters\nparams = {'max_features': ['auto', 'sqrt', 'log2'],\n          'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15], \n          'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,11]}\ntree_clf = GridSearchCV(dtree, param_grid=params, n_jobs=-1)","c9c7711b":"# train the model\ntree_clf.fit(X_train,y_train)\ntree_clf.best_params_ ","a485febe":"# predictions\ntree_predict = tree_clf.predict(X_test)","4a58c2f2":"#accuracy\ntree_accuracy = accuracy_score(y_test,tree_predict)\nprint(f\"Using Decision Trees we get an accuracy of {round(tree_accuracy*100,2)}%\")","8c3c2393":"cm=confusion_matrix(y_test,tree_predict)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")","ce9a9e4d":"print(classification_report(y_test,tree_predict))","2544ca6b":"tree_f1 = f1_score(y_test, tree_predict)\nprint(f'The f1 score Descision trees is {round(tree_f1*100,2)}%')","236bb344":"# ROC curve and AUC \nprobs = tree_clf.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\ntree_auc = roc_auc_score(y_test, probs)\n\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(y_test, probs)\n# plot curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(fpr, tpr, marker='.')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.title(f\"AUC = {round(tree_auc,3)}\")\nplt.show()","c8824750":"from sklearn.svm import SVC","bd794152":"#grid search for optimum parameters\nCs = [0.001, 0.01, 0.1, 1, 10]\ngammas = [0.001, 0.01, 0.1, 1]\nparam_grid = {'C': Cs, 'gamma' : gammas}\nsvm_clf = GridSearchCV(SVC(kernel='rbf', probability=True), param_grid, cv=10)","81845c0e":"# train the model\nsvm_clf.fit(X_train,y_train)\nsvm_clf.best_params_ ","ec055bf9":"# predictions\nsvm_predict = svm_clf.predict(X_test)","f591fa54":"#accuracy\nsvm_accuracy = accuracy_score(y_test,svm_predict)\nprint(f\"Using SVM we get an accuracy of {round(svm_accuracy*100,2)}%\")","2206e34c":"cm=confusion_matrix(y_test,svm_predict)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")","8d3ceef5":"print(classification_report(y_test,svm_predict))","251f38e3":"svm_f1 = f1_score(y_test, svm_predict)\nprint(f'The f1 score for SVM is {round(svm_f1*100,2)}%')","83aa32fe":"# ROC curve and AUC \nprobs = svm_clf.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nsvm_auc = roc_auc_score(y_test, probs)\n\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(y_test, probs)\n# plot curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(fpr, tpr, marker='.')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.title(f\"AUC = {round(svm_auc,3)}\")\nplt.show()","1fca82fe":"comparison = pd.DataFrame({\n    \"Logistic regression\":{'Accuracy':log_accuracy, 'AUC':log_auc, 'F1 score':logistic_f1},\n    \"K-nearest neighbours\":{'Accuracy':knn_accuracy, 'AUC':knn_auc, 'F1 score':knn_f1},\n    \"Decision trees\":{'Accuracy':tree_accuracy, 'AUC':tree_auc, 'F1 score':tree_f1},\n    \"Support vector machine\":{'Accuracy':svm_accuracy, 'AUC':svm_auc, 'F1 score':svm_f1}\n}).T","ac3df0dd":"comparison","4ceb3dd4":"fig = plt.gcf()\nfig.set_size_inches(15, 15)\ntitles = ['AUC','Accuracy','F1 score']\nfor title,label in enumerate(comparison.columns):\n    plt.subplot(2,2,title+1)\n    sns.barplot(x=comparison.index, y = comparison[label], data=comparison)\n    plt.xticks(fontsize=9)\n    plt.title(titles[title])\nplt.show()","6a72cc6c":" from sklearn.model_selection import cross_val_score","de83ad1c":"cv_results = cross_val_score(svm_clf, X, y, cv=5) \n\nprint (f\"Cross-validated scores {cv_results}\")\nprint(f\"The Cross Validation accuracy is: {round(cv_results.mean() * 100,2)}%\")","38344746":"#### 5.2 Splitting data to Training and Testing set ","c755c146":"We see that age and systolic blood pressures are selected as the most important features for predicting the Ten year risk of developing CHD.\n\nHowever we will use the six most important features to build our models","2d03c4c9":"At 9.15%, the blood glucose entry has the highest percentage of missing data. The otherfeatures have very few missing entries.\n\nSince the missing entries account for only 12% of the total data we can drop these entries without losing alot of data.","5073c643":"#### 3.4 Number of people who have disease vs age","c62ce9b0":"After applying SMOTE, the new dataset is much more balanced: the new ratio between negative and positive cases is 1:1.2 up from 1:5.57","0b549d73":"### **3. Exploratory Data Analysis**","dd957108":"##### 5.4.2 k-Nearest Neighbours","9eabb370":"Since the dataset is imbalanced i.e for every positive case there are about 6 negative cases. We may end up with a classifier that is biased to the negative cases. The classifier may have a high accuracy but poor a precision and recall. To adress this we will balance the dataset using the Synthetic Minority Oversampling Technique (SMOTE).","0c480262":"We drop the education column because it has no correlation with heart disease","dabaca59":"#### 5.1 SMOTE","8d5739f1":"### 2. Dataset\nThe dataset is publically available on the [ Kaggle ](https:\/\/www.kaggle.com\/amanajmera1\/framingham-heart-study-dataset.) website, and it is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The classification goal is to predict whether the patient has 10-year risk of future coronary heart disease (CHD).The dataset provides the patients\u2019 information. It includes over 4,000 records and 15 attributes. Variables Each attribute is a potential risk factor. There are both demographic, behavioral and medical risk factors.\n\n#### **Attributes**:\n\n###### Demographic:\n*     Sex: male or female(Nominal)\n*     Age: Age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\n\n###### Education: no further information provided\n\n###### Behavioral:\n\n*     Current Smoker: whether or not the patient is a current smoker (Nominal)\n*     Cigs Per Day: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarettes, even half a cigarette.)\n    \n###### Information on medical history:\n\n*     BP Meds: whether or not the patient was on blood pressure medication (Nominal)\n*     Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)\n*     Prevalent Hyp: whether or not the patient was hypertensive (Nominal)\n*     Diabetes: whether or not the patient had diabetes (Nominal)\n    \n###### Information on current medical condition:\n\n*     Tot Chol: total cholesterol level (Continuous)\n*     Sys BP: systolic blood pressure (Continuous)\n*     Dia BP: diastolic blood pressure (Continuous)\n*     BMI: Body Mass Index (Continuous)\n*     Heart Rate: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.)\n*     Glucose: glucose level (Continuous)\n    \n#### **Target variable to predict:**\n*10 year risk of coronary heart disease (CHD) - (binary: \u201c1\u201d, means \u201cYes\u201d, \u201c0\u201d means \u201cNo\u201d)*","167ef77c":"There are no features with more than 0.5 correlation with the Ten year risk of developing CHD and this shows that the features a poor predictors. However the features with the highest correlations are age, prevalent hypertension and systolic blood pressure\n\nAlso there are a couple of features that are highly correlated with one another and it makes no sense to use both of them in building a machine learning model. These incluse: Blood glucose and diabetes (obviously); systolic and diastolic blood pressures; cigarette smoking and the number of cigarretes smoked per day. Therefore we need to carry out feature selection to pick the best features","52185ecf":"#### 4.1 Statistics on the top features","3de8bbce":"A decision tree is a tree-like graph with nodes representing the place where we pick an attribute and ask a question; edges represent the answers the to the question; and the leaves represent the actual output or class label. They are used in non-linear decision making with simple linear decision surface.\n\nDecision trees classify the examples by sorting them down the tree from the root to some leaf node, with the leaf node providing the classification to the example. Each node in the tree acts as a test case for some attribute, and each edge descending from that node corresponds to one of the possible answers to the test case. This process is recursive in nature and is repeated for every subtree rooted at the new nodes.","37990ba1":"# **PREDICTING THE TEN YEAR RISK OF DEVELOPING HEART DISEASE USING MACHINE LEARNING**","13f0befb":"#### 3.2.1 Case counts","f5879414":"> *SMOTE first selects a minority class instance a at random and finds its k nearest minority class neighbors. The synthetic instance is then created by choosing one of the k nearest neighbors b at random and connecting a and b to form a line segment in the feature space. The synthetic instances are generated as a convex combination of the two chosen instances a and b.\n*\n\u2014 Page 47, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013.\n\nThis procedure can be used to create as many synthetic examples for the minority class as are required. It suggests first using random undersampling to trim the number of examples in the majority class, then use SMOTE to oversample the minority class to balance the class distribution.","70673f0c":"Due to the imbalanced nature of the dataset it is difficult to make conclusions but based on what is observed but these are the conclusions that can be drawn:\n\n*      Slightly more males are suffering from CHD than females\n*      The percentage of people who have CHD is almost equal between smokers and non smokers\n*      The percentage of people who have CHD is higher among the diabetic, and those with prevalent hypertesion as compared to those who dont have similar morbidities\n*      A larger percentage of the people who have CHD are on blood pressure medication","fcb7c163":"#### 3.2 Data Distribution","dd931420":"**Closing statements**\n1. If you have found this notebook to be useful please upvote it\n2. If you have any queries or additions please leave them in the comments\n3. Feel free to use any part of this notebook in your personal work\n","f7bfd1a0":"The people with the highest risk of developing CHD are betwwen the ages of 51 and 63\n\nThe number of sick people generally increases with age","dab1f082":"#### 5.5 Model Comparison","7685850c":"### ***1. Introduction***\n\nHeart disease refers to various types of conditions that can affect heart function. These types include: coronary artery (atherosclerotic) disease that affects the blood supply to the heart, valvular heart disease that affects how the valves function to regulate blood flow, cardiomyopathies that affect heart muscles, heart rhythm disturbances (arrhythmias) that affect the electrical conduction and congenital heart diseases where the heart has structural defects that develop before birth.\n\nHeart disease is the major cause of morbidity and mortality globally: it accounts for more deaths annually than any other cause. For example an estimated 17.9 million people died from heart diseases in 2016, representing 31% of all global deaths. Over three quarters of these deaths took place in low- and middle-income countries.\n\nMost heart diseases are highly preventable and simple lifestyle modifications(such as reducing tobacco use, eating healthily, obesity and exercising) coupled with early treatment greately improve their prognoses. It is, however, difficult to identify high risk patients because of the mulfactorial nature of several contributory risk factors such as diabetes, high blood pressure, high cholesterol et cetera. Due to such constraints, scientists have turned towards modern approaches like Data Mining and Machine Learning for predicting the disease.\n\nMachine learning (ML), due to its superiority in pattern detection and classification, proves to be effective in assisting decision making and risk assesment from the large quantity of data produced by the healthcare industry on heart disease.\n\nIn this notebook, I will be exploring different Machine Learning approaches for predicting wheather a patient has 10-year risk of developing coronary heart disease (CHD) using the [Framingham dataset ](http:\/\/biolincc.nhlbi.nih.gov\/studies\/framcohort\/) that is publicly availabe on[ Kaggle ](https:\/\/www.kaggle.com\/amanajmera1\/framingham-heart-study-dataset.)","0fd5df09":"### 6. Conclusion","0051c3eb":"A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimentional space this hyperplane is a line dividing a plane in two parts where in each class lay in either side.","553ef694":"#### 3.5 Correlation Heat map","794da599":"#### 4.2 Pair plots","213cf443":"The k-nearest-neighbors is a data classification algorithm that attempts to determine what group a data point is in by looking at the data points around it.\n\nAn algorithm, looking at one point on a grid, trying to determine if a point is in group A or B, looks at the states of the points that are near it. The range is arbitrarily determined, but the point is to take a sample of the data. If the majority of the points are in group A, then it is likely that the data point in question will be A rather than B, and vice versa.\n\nThe k-nearest-neighbor is an example of a \"lazy learner\" algorithm because it does not generate a model of the data set beforehand. The only calculations it makes are when it is asked to poll the data point's neighbors. This makes k-nn very easy to implement for data mining.","fef4cc3c":"#### 5.4 Models","a7abff34":"#### 5.4.4 Support Vector Machine","f43d0601":"The four algorithms that will be used are:\n1. Logistic Regression\n2. k-Nearest Neighbours\n3. Decision Trees\n4. Support Vector Machine","b911ee2c":"###### 5.4.1 Logistic regression","bb1992d3":"Holding all other features constant, the odds of getting diagnosed with heart disease increases with about 2% for every increase in age an systolic blood pressure\n\nThe other factors show no significant positive odds","c04ce6c0":"#### 5.3 Feature Scaling\n\nFeature scaling is a method used to normalize the range of independent variables or features of data. It generally speeds up the running time of different algorithms","75e0346b":"#### 3.1 Missing variables\n\nHandling missing data is important as many machine learning algorithms do not support data with missing values.","62f4e28b":"Here we will use the Boruta algorithm  which is a wrapper built around the random forest classification algorithm. It tries to capture all the important, interesting features in a data set with respect to an outcome variable.\n\n##### Methodology:\n\n*     Firstly, it adds randomness to the given data set by creating shuffled copies of all features (which are called shadow features).\n    \n*     Then, it trains a random forest classifier on the extended data set and applies a feature importance measure (the default is Mean Decrease Accuracy) to evaluate the importance of each feature where higher means more important.\n    \n*     At every iteration, it checks whether a real feature has a higher importance than the best of its shadow features (i.e. whether the feature has a higher Z-score than the maximum Z-score of its shadow features) and constantly removes features which are deemed highly unimportant.\n    \n*     Finally, the algorithm stops either when all features get confirmed or rejected or it reaches a specified limit of random forest runs.\n    \nCheck the full reference [here]( http:\/\/danielhomola.com\/2015\/05\/08\/borutapy-an-all-relevant-feature-selection-method\/)","97d0c960":"The data is not properly balanced as the number of people without the disease greately exceeds the number of people with the disease. The ratio is about 1:5.57","b96984a7":"The top features are:\n    1. Age\n    2. Total cholesterol\n    3. Systolic blood pressure\n    4. Diastolic blood pressure\n    5. BMI\n    6. Heart rate\n    7. Blood glucose","0ac97442":"#### 5.4.3 Decision Trees","9e3bac14":"### **5. Models and predictions**","bc303195":"Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n\nIt is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train\/test split.\n\nThe general procedure is as follows:\n\n1. Shuffle the dataset randomly.\n2. Split the dataset into k groups\n3. For each unique group:\n    *         Take the group as a hold out or test data set\n    *         Take the remaining groups as a training data set\n    *         Fit a model on the training set and evaluate it on the test set\n    *         Retain the evaluation score and discard the model\n4. Summarize the skill of the model using the sample of model evaluation scores\n\nCheck the full reference [here](https:\/\/machinelearningmastery.com\/k-fold-cross-validation\/)","5524468b":"#### 5.5 Cross validation score of the best model","ffc9af28":"#### 3.3 Categorical variable comparisons","01893102":"### **4 Feature Selection**","70e64972":"There are no characteristics that can be used split the data well","effe6ffe":"The data on the prevalent stroke, diabetes, and blood pressure meds are poorly balanced","483674f3":"The goal of logistic regression is to find the best fitting (yet biologically reasonable) model to describe the relationship between the dichotomous characteristic of interest (dependent variable = response or outcome variable) and a set of independent (predictor or explanatory) variables. Logistic regression generates the coefficients (and its standard errors and significance levels) of a formula to predict a logit transformation of the probability of presence of the characteristic of interest:\n![logit function](http:\/\/www.statisticssolutions.com\/wp-content\/uploads\/2010\/01\/log23.jpg)\n\nRather than choosing parameters that minimize the sum of squared errors (like in ordinary regression), estimation in logistic regression chooses parameters that maximize the likelihood of observing the sample values.","3c890db4":"1. The most important features in predicting the ten year risk of developing CHD were age and systolic blood pressure\n2. The Support vector machine with the radial kernel was the best performing model in terms of accuracy and the F1 score. Its high AUC shows that it has a high true positive rate.\n3. Balancing the dataset by using the SMOTE technique helped in improving the models' sensitivity, this is when compared to the performance metrics of other models on different notebooks on the same dataset\n4. With more data(especially that of the minority class) better models can be built"}}