{"cell_type":{"80a42f92":"code","8d2df7fa":"code","b3237189":"code","c37a08e8":"code","f1bb835f":"code","7a640c73":"code","98e137d6":"code","711770e5":"code","03db13b6":"code","8d451df3":"code","7e92be3d":"code","dd6e7e9e":"code","ff1e18a7":"code","8f233460":"markdown","c96ea299":"markdown","bf742d93":"markdown","d60bbe6b":"markdown","1cddeb21":"markdown","7d836147":"markdown","579682c6":"markdown","0caefb6c":"markdown","e372e64e":"markdown","a5ba6f15":"markdown","d3623c62":"markdown","f4f5cbea":"markdown","db4850dd":"markdown","e0b6d2a3":"markdown","24144709":"markdown","42300af0":"markdown","06009be7":"markdown"},"source":{"80a42f92":"import pandas as pd # librairie de manipulation de donn\u00e9es\nimport numpy as np # librairie de calcul scientifique\nimport matplotlib.pyplot as plt # librairie de visualisation et graphiques\nimport matplotlib.image as mpimg\nimport gc # librairie de gestion de la RAM\nimport time # librairie de gestion du temps\n\n# Sklearn : librairie d'algorithmes de MachineLearning\nfrom sklearn import model_selection, ensemble, datasets\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, auc, accuracy_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, learning_curve\n\n# XGBoost : librairie d'algorithmes de MachineLearning\nfrom xgboost import XGBClassifier\n\n# Keras : librairie d'algorithmes de MachineLearning\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.utils import to_categorical","8d2df7fa":"def plot_learning_curve(est, X_set, y_set, split = 5, savefig = False, name = \"default\") :\n    begin_time = time.time()\n    train_sizes, train_scores, test_scores, fit_t, score_t = learning_curve(estimator = est, X = X_set, y = y_set,\n                                                                            train_sizes = np.linspace(0.1, 1.0, split), \n                                                                            cv = 5, n_jobs = -1, shuffle = True,\n                                                                            random_state = 1, return_times = True)\n    print(\"Training sizes: \" + str(train_sizes))\n    print(\"Total time spent: {:d}s\".format(int(time.time()-begin_time)))\n    \n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    \n    plt.figure(figsize=(8,5))\n    plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='training accuracy')\n    plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n    plt.plot(train_sizes, test_mean,color='green', linestyle='--',marker='s', markersize=5,label='validation accuracy')\n    plt.fill_between(train_sizes,test_mean + test_std,test_mean - test_std,alpha=0.15, color='green')\n    plt.grid(b='on')\n    plt.xlabel('Number of training samples')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylim([0.0, 1.0])\n    plt.title(\"Learning curve\")\n    if savefig : plt.savefig(name +\".jpg\", dpi=300)\n    plt.show()\n\n\ndef plot_tree_number_influence(X_train, y_train, X_test, y_test, est = \"RandomForest\", \n                               begin_step = 2.0, number_max = 100, savefig = False, name = \"default\") :\n    tree_number = np.ones((1,1), dtype=np.int8)\n    step = begin_step if begin_step > 1.0 else 2.0\n    while tree_number[-1] < number_max :\n        new_number = int(tree_number[-1]*step**1.005)\n        if new_number > tree_number[-1] :\n            tree_number = np.append(tree_number, new_number)\n        step **= 1.005\n    if (tree_number[-1]-number_max)\/number_max >= 0.1 : tree_number[-1] = number_max\n    elif (tree_number[-1]-number_max)\/number_max <= -0.1 : tree_number = np.append(tree_number, number_max)\n    print(\"Trees: \" + str(tree_number))\n    \n    accuracy = np.empty(tree_number.shape)\n    total_tree_number = np.sum(tree_number)\n    begin_time = time.time()\n    length_max = 0\n    clear = \"\"\n    for i in range(tree_number.shape[0]) :\n        current_tree_number = np.sum(tree_number[:i+1])\n        if est == \"XGBoost\" : \n            model = XGBClassifier(n_estimators=tree_number[i], random_state=1, use_label_encoder = False)\n            model.fit(X_train, y_train, eval_metric = \"mlogloss\")\n        else : \n            model = ensemble.RandomForestClassifier(n_estimators=tree_number[i], random_state=1)\n            model.fit(X_train, y_train)\n        y_model = model.predict(X_test)\n        accuracy[i] = accuracy_score(y_test, y_model)\n        progression = current_tree_number\/total_tree_number\n        to_print = \"\\rProgression: {:.2f} %    |    \".format(progression*100)\n        spent_time = time.time() - begin_time\n        if i == 0 : \n            length_max =  77 + len(str(int(spent_time*(1\/progression-1))))*3\n            for _ in range(length_max) : clear += \" \"\n        if i != tree_number.shape[0]-1 :\n            to_print += \"Time spent: {:d}s    |    Estimated remaining time: {:d}s\".format(int(spent_time), \n                                                                                           int(spent_time*(1\/progression-1)))\n        else :\n            to_print += \"Total time spent: {:d}s\".format(int(spent_time))\n        print(clear, end='\\r')\n        if i != tree_number.shape[0]-1 : print(to_print, end=\"\\r\")\n        else : print(to_print)\n            \n    plt.figure(figsize=(8,5))\n    plt.plot(tree_number, accuracy, color='blue', marker='o', markersize=5, label='accuracy')\n    best_accuracy = np.amax(accuracy)\n    plt.hlines(best_accuracy, 0, tree_number[-1], color='red', linestyles='dotted', label=str(round(best_accuracy, 3)))\n    plt.grid(b='on')\n    plt.xlabel('Number of trees')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylim([0.0, 1.0])\n    plt.title(\"Influence\")\n    if savefig : plt.savefig(name +\".jpg\", dpi=300)\n    plt.show()\n\n\ndef plot_scores(train, savefig = False, name = \"default\") :\n    accuracy = train.history['accuracy']\n    val_accuracy = train.history['val_accuracy']\n    epochs = range(len(accuracy))\n    plt.figure(figsize=(8,5))\n    plt.plot(epochs, accuracy, 'b', label='Score apprentissage')\n    plt.plot(epochs, val_accuracy, 'r', label='Score validation')\n    plt.grid(b='on')\n    plt.xlabel('Number of trees')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylim([0.0, 1.0])\n    plt.title(\"Scores\")\n    if savefig : plt.savefig(name +\".jpg\", dpi=300)\n    plt.show()\n\n\ndef show_image(path) :\n    plt.figure(figsize=(10,7))\n    plt.grid(b='off')\n    plt.axis('off')\n    plt.imshow(mpimg.imread(path))\n    plt.show()","b3237189":"df = pd.read_csv(\"..\/input\/skin-cancer-mnist-ham10000\/hmnist_28_28_RGB.csv\")\ndf.head(5) # on affiche les 5 premi\u00e8res lignes (0-4) pour voir l'agencement global","c37a08e8":"# On v\u00e9rifie s'il y a des NaN dans le DataFrame\nprint(\"Nombre de NaN dans le dataframe : \", df.isna().sum().sum())\n\n# On retient le nombre d'images (nombre de lignes)\nnb_images = df.index.shape[0]\nprint(\"Nombre d'images : \", nb_images)","f1bb835f":"# On v\u00e9rifie la proportion des classes\ndf.label.value_counts()","7a640c73":"# On s\u00e9pare le DataFrame en tableaux numpy X et y\ny = np.array(df.label)\nX = np.array(df.drop(['label'], axis=1))\nX_img = X.reshape(nb_images, 28, 28, 3)\n\n# On affiche 20 images au hasard pour avoir un aper\u00e7u\nplt.figure(figsize=(15,3))\nfor i in range(20) :\n    j = np.random.randint(0, nb_images)\n    plt.subplot(2,10,i+1)\n    plt.axis('off')\n    plt.imshow(X_img[j])\n    plt.title(y[j])\n\n# On d\u00e9salloue la RAM inutile\ngc.collect();","98e137d6":"# On s\u00e9pare X et y en train (80%) et en test (20%) \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n# On d\u00e9salloue la RAM inutile\ngc.collect();","711770e5":"# \/!\\ Temps estim\u00e9 avec les r\u00e9glages : ~ 30s\n# On initialise et entraine notre mod\u00e8le\nrf = ensemble.RandomForestClassifier(random_state=1)\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)\n\n# On affiche les conclusions\nprint(classification_report(y_test, y_rf, zero_division = 0))\nprint(confusion_matrix(y_test, y_rf))\n\n# On d\u00e9salloue la RAM inutile\ngc.collect();","03db13b6":"# \/!\\ Temps estim\u00e9 avec les r\u00e9glages : ~ 300s (= 5min)\n#plot_learning_curve(rf, X, y, split = 10, savefig = True, name = \"learningCurve_RF\")\nshow_image(\"..\/input\/outputplots-travail-ias-skincancer\/learningCurve_RF.jpg\")\n\n# On d\u00e9salloue la RAM inutile\ngc.collect();","8d451df3":"# \/!\\ Temps estim\u00e9 avec les r\u00e9glages : ~ 400s (= 6min 40s)\n#plot_tree_number_influence(X_train, y_train, X_test, y_test, est = \"RandomForest\", begin_step=2, number_max=500, savefig = True, name = \"treeNumberInfluence_RF\")\nshow_image(\"..\/input\/outputplots-travail-ias-skincancer\/treeNumberInfluence_RF.jpg\")\n\n# On d\u00e9salloue la RAM inutile\ngc.collect();","7e92be3d":"# \/!\\ Temps estim\u00e9 avec les r\u00e9glages : ~ 290s (= 4min 50s)\n# On initialise et entraine notre mod\u00e8le\nxgb = XGBClassifier(use_label_encoder = False)\nxgb.fit(X_train, y_train, eval_metric = \"mlogloss\")\ny_xgb = xgb.predict(X_test)\n\n# On affiche les conclusions\nprint(classification_report(y_test, y_xgb, zero_division = 0))\nprint(confusion_matrix(y_test, y_xgb))\n\n# On d\u00e9salloue la RAM inutile\ngc.collect();","dd6e7e9e":"# \/!\\ Temps estim\u00e9 avec les r\u00e9glages : ~ 14000s (= 3h 53min 20s)\n#plot_learning_curve(xgb, X, y, split = 10, savefig = True, name = \"learningCurve_XGB\")\nshow_image(\"..\/input\/outputplots-travail-ias-skincancer\/learningCurve_XGB.jpg\")\n\n# On d\u00e9salloue la RAM inutile\ngc.collect();","ff1e18a7":"# \/!\\ Temps estim\u00e9 avec les r\u00e9glages : ~ 2300s (= 33min 50s)\n#plot_tree_number_influence(X_train, y_train, X_test, y_test, est = \"XGBoost\", begin_step=2, number_max=500, savefig = True, name = \"treeNumberInfluence_XGB\")\nshow_image(\"..\/input\/outputplots-travail-ias-skincancer\/treeNumberInfluence_XGB.jpg\")\n\n# On d\u00e9salloue la RAM inutile\ngc.collect();","8f233460":"#### On peut constater plusieurs choses pour ce mod\u00e8le de For\u00eat Al\u00e9atoire :\n- Le f1-score du mod\u00e8le n'est vraiment pas incroyable **(71%)**. \n- D'ailleurs, comme on l'avait constat\u00e9, si on analyse par classe on voit que le mod\u00e8le n'a rien appris sur les classes 3 et 5 **(0%)** et on le remarque d'autant mieux gr\u00e2ce aux lignes 4 et 6 de la matrice de confusion.\n- Autre chose coh\u00e9rent avec notre constatation aussi, la classe 4 a la meilleure pr\u00e9cision et la matrice de confusion nous indique que le mod\u00e8le a \"trop appris\" avec cette classe, \u00e9tant donn\u00e9 qu'il pense que des images d'autres classes sont en fait des images de la classe 4 (colonne 5 de la matrice). \u00c7a explique pourquoi la pr\u00e9cision des autres classes est basse : c'est \u00e0 cause d'un ***overfitting*** !","c96ea299":"#### Conclusion\nLe mod\u00e8le de For\u00eats Al\u00e9atoires ne para\u00eet pas \u00eatre le plus adapt\u00e9 au vu de la r\u00e9partition des donn\u00e9es... Il ne fournit pas une bonne pr\u00e9cision, et les faux-positifs sont quand m\u00eame assez nombreux pour toutes les classes autres que la 4.","bf742d93":"#### Est-ce un probl\u00e8me de taille de la for\u00eat (nombre d'arbres) ?\nOn peut faire appel \u00e0 une fonction pour le savoir :","d60bbe6b":"#### Est-ce un probl\u00e8me de taille de la base (nombre d'\u00e9chantillons) ?\nOn peut faire appel \u00e0 une fonction pour le savoir :","1cddeb21":"On voit que globalement, **la taille des donn\u00e9es importe peu** : c'est encore une fois logique puisqu'on a identifi\u00e9 le principal probl\u00e8me qui est d\u00fb \u00e0 la r\u00e9partition des donn\u00e9es et \u00e0 un trop fort d\u00e9s\u00e9quilibre entre la classe 4 et les classes 3, 5 et 0 notamment... Et de par la m\u00e9thode employ\u00e9e, normalement toutes les tailles d'\u00e9chantillons ont plus ou moins la m\u00eame r\u00e9partition *(donc le m\u00eame probl\u00e8me !)*.","7d836147":"# Classification de photos - Cancer de la peau","579682c6":"#### On peut constater plusieurs choses pour ce mod\u00e8le de XGBoost :\n- Le f1-score du mod\u00e8le est tr\u00e8s l\u00e9g\u00e8rement meilleur \u00e0 celui d'une For\u00eat Al\u00e9atoire **(73% au lieu de 71%)**. \n- Pour autant encore, comme on l'avait constat\u00e9, si on analyse par classe on voit que le mod\u00e8le n'a que tr\u00e8s peu appris sur la classe 3 **(0.05%)**, et que les pr\u00e9cisions des classes autres que la 4 ont l\u00e9g\u00e8rement augment\u00e9 aussi.\n- Autre remarque coh\u00e9rente avec notre constatation, la classe 4 a la meilleure pr\u00e9cision et la matrice de confusion nous indique que le mod\u00e8le a \"trop appris\" avec cette classe, \u00e9tant donn\u00e9 qu'il pense que des images d'autres classes sont en fait des images de la classe 4 (colonne 5 de la matrice). \u00c7a explique pourquoi la pr\u00e9cision des autres classes est basse, puisqu'\u00e0 une exception pr\u00e8s (la classe 1, ligne 2 de la matrice), le classement d'une image en classe 4 est toujours majoritaire, quelle que soit l'image : on a bien affaire \u00e0 un ***overfitting***, pour lequel les arbres m\u00eame boost\u00e9s ne sont que peu appropri\u00e9s !","0caefb6c":"## Situation\n#### \u00c0 partir de la base ***MNIST Ham10000***, contenant une large collection d'images \u00e9pidermiques multi-sources de l\u00e9sions pigment\u00e9es, le but est de produire un algorithme pouvant classifier les images en **7 types** de l\u00e9sions :\n- k\u00e9ratoses actiniques, carcinomes intra\u00e9pith\u00e9liaux et maladies de Bowen *(\"akiec\", classe 0)* ;\n- carcinomes basocellulaires *(\"bcc\", classe 1)* ;\n- l\u00e9sions b\u00e9nignes de type k\u00e9ratose : lentigines solaires, k\u00e9ratoses s\u00e9borrh\u00e9iques et k\u00e9ratoses de type lichen-planus *(\"bkl\", classe 2)* ;\n- dermatofibromes *(\"df\", classe 3)* ; \n- m\u00e9lanomes *(\"mel\", classe 4)* ;\n- naevus m\u00e9lanocytaires *(\"nv\", classe 5)* ;\n- l\u00e9sions vasculaires : angiomes, angiok\u00e9ratomes, granulomes pyog\u00e8nes et h\u00e9morragies *(\"vasc\", classe 6)*.","e372e64e":"L\u00e0 encore, si le nombre d'arbres a effectivement une influence en-dessous de 100 dirons-nous, au-dessus visiblement **rajouter des arbres n'am\u00e9liore pas la pr\u00e9cision** du mod\u00e8le *(mais augmentera juste le temps !)*.","a5ba6f15":"## Pr\u00e9paration des donn\u00e9es - Ensembles de train et de test\n#### On va pr\u00e9-traiter les donn\u00e9es en les s\u00e9parer en 2 ensembles (train et validation) pour la suite. On en profite pour en afficher une vingtaine pour voir de quoi il s'agit.","d3623c62":"## Import du fichier CSV\n#### On importe le fichier CSV le plus d\u00e9fini en couleur (28x28, RGB) pour travailler dessus.","f4f5cbea":"On voit que les classes 3, 5 et 0 sont assez faiblement repr\u00e9sent\u00e9es alors que la classe 4 est v\u00e9ritablement sur-repr\u00e9sent\u00e9e. Il faudra en tenir compte dans les conclusions qu'on tirera des r\u00e9sultats *(et juger en cons\u00e9quence)* !","db4850dd":"## Import de biblioth\u00e8ques\n#### On commence par importer les biblioth\u00e8ques utiles pour le travail. Pour les librairies d'algorithmes de ***MachineLearning***, on n'importe que les fonctions n\u00e9cessaires.","e0b6d2a3":"## Fonctions utiles\n#### On d\u00e9finit quelques fonctions utiles tout au long du travail :\n- ***plot_learning_curve*** : utilis\u00e9e pour afficher la courbe d'apprentissage en fonction du nombre d'\u00e9chantillons pour un mod\u00e8le donn\u00e9 et des ensembles (X,y) donn\u00e9s ;\n- ***plot_tree_number_influence*** : utilis\u00e9e pour afficher l'influence du nombre d'arbres dans un mod\u00e8le *RandomForest* ou *XGBoost* ;\n- ***plot_scores*** : utilis\u00e9e pour afficher le score de train \/ de validation durant les \u00e9poques successives de l'entra\u00eenement d'un r\u00e9seau de neurones donn\u00e9 ;\n- ***show_image*** : utilis\u00e9e pour afficher une image pr\u00e9alablement sauvegard\u00e9e.","24144709":"## Traitement des donn\u00e9es - For\u00eats al\u00e9atoires\n#### On va utiliser un algorithme de **Random Forest** pour classifier les images et *(esp\u00e9rer)* avoir de bons r\u00e9sultats...","42300af0":"#### Est-ce un probl\u00e8me de taille de la base (nombre d'\u00e9chantillons) ?\nOn peut faire appel \u00e0 une fonction pour le savoir :","06009be7":"## Traitement des donn\u00e9es - XGBoost\n#### On va utiliser une variante de l'algorithme de Random Forest, en mettant les arbres en parall\u00e8le **(boosting)**. Le but \u00e9tant d'avoir de meilleurs r\u00e9sultats, notamment concernant la matrice de confusion..."}}