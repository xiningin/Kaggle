{"cell_type":{"fe2cd897":"code","63d263cb":"code","d641c743":"code","fa422d41":"code","afc0a725":"code","e53fc6dc":"code","9768021a":"code","c51da8f9":"code","06e9145d":"code","f77357aa":"code","380b18f6":"code","18c0b59d":"code","ac01cce9":"code","18f695fc":"code","31b7c553":"code","ea1795d1":"code","992eb393":"code","35fb9467":"code","53f7cb40":"code","8d6c7c82":"code","af129801":"code","b49f556f":"code","5b8201b2":"code","e149eb84":"code","a7f6edba":"code","1426e64d":"code","9e456172":"code","acafbae5":"code","1d4e2855":"code","7e188745":"code","d4eb0b4a":"code","186ffb9c":"code","e9240c3e":"code","548dddc6":"code","347d0a3f":"code","12c595bc":"code","de4dca86":"code","e5dee7da":"code","c194bafa":"code","f0c5ec36":"code","b0ab8899":"code","c274cd35":"code","7a173bc7":"code","dbf13000":"code","e99e8e99":"code","dd9e10b9":"code","fd76e343":"code","ae881684":"code","8c89c1c0":"code","779bbbcb":"code","937461a5":"code","30b9e0fa":"code","427b1fcd":"code","5aeab036":"code","c96063f9":"code","5fba6465":"code","30a83654":"code","8bbed33f":"code","33064991":"code","f7385943":"code","ef85f44b":"code","422006eb":"code","5baf6bc5":"code","115a24f2":"code","2095f3ff":"code","2a1de0bd":"code","52ca1abd":"code","fa4558e6":"code","f52a35c4":"code","bc98abb4":"code","1392c3c9":"code","89bd9ef9":"markdown","ff3c6528":"markdown","a2e85b04":"markdown","973a8a23":"markdown","3570f7e7":"markdown","304f7a62":"markdown","8b06670e":"markdown","95ba68d8":"markdown","79613e7a":"markdown","5cb1edeb":"markdown","d72fada7":"markdown","23d2378c":"markdown","1f4ec163":"markdown","e565df2a":"markdown","bf3d1bc2":"markdown","b7161677":"markdown","3c83b0f8":"markdown","bc577e39":"markdown","ae30c2e2":"markdown","4ad541ce":"markdown","aee099b9":"markdown","4b892041":"markdown","ecddd5d6":"markdown","610eefbd":"markdown","3e4c0cd0":"markdown","29fb3577":"markdown","6456f12b":"markdown","5f499f6a":"markdown","affa0b0f":"markdown","e8dbf992":"markdown","62bda821":"markdown","5461179e":"markdown","8a656510":"markdown","977195b1":"markdown","86572437":"markdown","a6888321":"markdown","e373bcad":"markdown","8547f2af":"markdown"},"source":{"fe2cd897":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","63d263cb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import scale, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import LabelEncoder","d641c743":"import warnings\n\nwarnings.filterwarnings('ignore', category = DeprecationWarning)\nwarnings.filterwarnings('ignore', category = FutureWarning)","fa422d41":"df = pd.read_csv('..\/input\/termdepositmarketing\/term-deposit-marketing-2020.csv')","afc0a725":"df.head()","e53fc6dc":"df.campaign.unique()","9768021a":"df.job = pd.Categorical(df.job)\ndf.marital = pd.Categorical(df.marital)\ndf.education = pd.Categorical(df.education)\ndf.housing = pd.Categorical(df.housing)\ndf.loan = pd.Categorical(df.loan)\ndf.contact = pd.Categorical(df.contact)\ndf.month = pd.Categorical(df.month)\ndf.default = pd.Categorical(df.default)","c51da8f9":"df.dtypes # I converted the object-data-types to category to define them to Python as category.","06e9145d":"df.shape","f77357aa":"df.columns  # Here I would like to see the all features names.","380b18f6":"f , ax = plt.subplots(1,figsize = (9,7))\ncolors = ['red', 'green']\nlabels = \"Did not Open\", \"Opened\"\nplt.suptitle('Information on Term Subscriptions', fontsize = 20)\ndf[\"y\"].value_counts().plot.pie(explode=[0,0.25], autopct='%1.2f%%', ax=ax, shadow=True, colors=colors, \n                                             labels=labels, fontsize=12, startangle=25);\n# ax[0].set_title('State of Loan', fontsize=16)\nax.set_xlabel('% of Condition of Loans', fontsize=14);\n\n","18c0b59d":"df1 = df.copy() # I did copy the 'df' to keep one original copy if it might be necessary.\ndf1.head()","ac01cce9":"df.describe().T # Results,before detecting missing value and other preprocessing process.","18f695fc":"df1.isnull().sum() # As you see here... There is nothing but unknown values. that's why we can not see the NaN values.","31b7c553":"df1.replace('unknown', np.NaN, inplace = True) ","ea1795d1":"df1.isnull().sum()","992eb393":"cont_nan_rate = ((df1['contact'].isnull().values.sum())\/((df1['contact'].isnull().values.sum()) \n                 + (df1['contact'].notnull().values.sum())))*100\nprint(\"'contact' -  missing value rate  :\",cont_nan_rate)\n\n\nedu_nan_rate = ((df1['education'].isnull().values.sum())\/((df1['education'].isnull().values.sum()) \n                 + (df1['education'].notnull().values.sum())))*100\nprint(\"'education' -  missing value rate  :\",edu_nan_rate)\n\n\njob_nan_rate = ((df1['job'].isnull().values.sum())\/((df1['job'].isnull().values.sum()) \n                 + (df1['job'].notnull().values.sum())))*100\nprint(\"'job' -  missing value rate  :\",job_nan_rate)","35fb9467":"#replaced the NaN values with the median of the variables.\ndf1.job.fillna(df1.job.mode()[0], inplace = True)\ndf1.education.fillna(df1.education.mode()[0], inplace = True)\ndf1.contact.fillna(df1.contact.mode()[0], inplace =True)","53f7cb40":"df1.isnull().sum()","8d6c7c82":"cat_class = {'job':df1['job'].value_counts().count(),\n             'marital':df1['marital'].value_counts().count(),\n             'education':df1['education'].value_counts().count(),\n             'defoult':df1['default'].value_counts().count(),\n             'housing':df1['housing'].value_counts().count(),\n             'loan':df1['loan'].value_counts().count(),\n             'contact':df1['contact'].value_counts().count(),\n             'month':df1['month'].value_counts().count()}# consists of 10 category\nprint('Names\/Classes : ', cat_class)","af129801":"df1['job'].value_counts().plot.barh().set_title('job types frequency');","b49f556f":"plt.subplots(figsize = (8,5))\npalette = ['red','green']\nsns.barplot(x = df1.job.index, y = df1.job, data = df1, palette = palette, hue = 'y')","5b8201b2":"plt.subplots(figsize = (8,5))\npalette = ['red','green']\nsns.barplot(x = df1.education, y = df1.education.index, data = df, palette = palette, hue = 'y');","e149eb84":"plt.subplots(figsize = (8,5))\npalette = ['red','green']\nsns.barplot(x = df.marital, y = df.marital.index, data = df, palette = palette, hue = 'y');","a7f6edba":"le = LabelEncoder()\ndf_target = pd.DataFrame(df1.loc[:,'y'].values)\ndf_target.columns = ['y']\ndf_target.y = le.fit_transform(df_target.y)\ndf_target.head()","1426e64d":"palette = ['red','green']\nplt.scatter(x = df1.duration, y = df_target);\nplt.xlabel('Duration')\nplt.ylabel('yes = 1 - no = 0')\nplt.show()\nprint('Max- duration for phone call: ', df1.duration.max())\nprint('Min- duration for phone call: ', df1.duration.min())\nprint('Mean- duration for phone call: ', df1.duration.mean())\nprint('Mode- duration for phone call: ', df1.duration.mode()[0])","9e456172":"plt.subplots(figsize = (8,5))\npalette = ['red','green']\nsns.barplot(x = np.sort(df1.month), y = df.month.index, data = df, palette = palette, hue = 'y');","acafbae5":"palette = ['red','green']\nsns.pairplot(df1, hue = 'y',palette = palette);","1d4e2855":"f, ax = plt.subplots(figsize = (10,8))\nsns.heatmap(df1.corr(), annot = True, linewidths = .5, fmt = '2g', ax = ax);","7e188745":"df1.head(1)","d4eb0b4a":"df_cat = pd.DataFrame(df1.loc[:,['job','marital','education','default', 'housing', 'loan', 'contact', 'month']].values)","186ffb9c":"df_cat.columns = ['job','marital','education','default', 'housing', 'loan', 'contact', 'month']","e9240c3e":"df_cat.head()","548dddc6":"df_cat.job = le.fit_transform(df_cat.job)\ndf_cat.marital = le.fit_transform(df_cat.marital)\ndf_cat.education = le.fit_transform(df_cat.education)\ndf_cat.default = le.fit_transform(df_cat.default)\ndf_cat.housing = le.fit_transform(df_cat.housing)\ndf_cat.loan = le.fit_transform(df_cat.loan)\ndf_cat.contact = le.fit_transform(df_cat.contact)\ndf_cat.month = le.fit_transform(df_cat.month)\ndf_cat.head()","347d0a3f":"df_num = pd.DataFrame(df1.loc[:,['age', 'balance','day', 'duration','campaign']].values)\ndf_num.columns = ['age', 'balance','day', 'duration','campaign']\ndf_num.head()","12c595bc":"df2 = pd.concat([df_cat,df_num], axis = 1)\ndf3 = pd.concat([df2,df_target], axis = 1)\ndf2.head()","de4dca86":"from sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import LinearSVC\n\n\nx_train, x_test, y_train, y_test = train_test_split(df2,df_target, stratify = df_target, random_state = 0 )\n\ndf2_indices = np.arange(df2.shape[-1])\n\nselector = SelectKBest(f_classif, k=5)\nselector.fit(x_train, y_train)\nscores = -np.log10(selector.pvalues_)\nscores \/= scores.max()\nplt.bar(df2_indices - .45, scores, width=.2,\n        label=r'Univariate score ($-Log(p_{value})$)')\n\n\nclf = make_pipeline(MinMaxScaler(), LinearSVC())\nclf.fit(x_train, y_train)\nprint('Classification accuracy without selecting features: {:.3f}'\n      .format(clf.score(x_test, y_test)))\n\nsvm_weights = np.abs(clf[-1].coef_).sum(axis=0)\nsvm_weights \/= svm_weights.sum()\n\nplt.bar(df2_indices - .25, svm_weights, width=.2, label='SVM weight')\n\nclf_selected = make_pipeline(\n        SelectKBest(f_classif, k=5), MinMaxScaler(), LinearSVC()\n)\nclf_selected.fit(x_train, y_train)\nprint('Classification accuracy after univariate feature selection: {:.3f}'\n      .format(clf_selected.score(x_test, y_test)))\n\nsvm_weights_selected = np.abs(clf_selected[-1].coef_).sum(axis=0)\nsvm_weights_selected \/= svm_weights_selected.sum()\n\nplt.bar(df2_indices[selector.get_support()] - .05, svm_weights_selected,\n        width=.2, label='SVM weights after selection')\n\n\nplt.title(\"Comparing feature selection\")\nplt.xlabel('Feature number')\nplt.yticks(())\nplt.axis('tight')\nplt.legend(loc='upper right')\nplt.show()","e5dee7da":"df1.head(1)","c194bafa":"df2_duration = df2.duration\nsns.boxplot(x = df2.balance);","f0c5ec36":"Q1 = df2_duration.quantile(0.25)\nQ3 = df2_duration.quantile(0.75)\nIQR = Q3 - Q1 #inter Quantile we calculate the treshold value by using this IQR result\n\nprint('Q1', Q1)\nprint('Q3', Q3)\nprint('IQR', IQR)","b0ab8899":"lower_limit = Q1 - 1.5*IQR\nupper_limit = Q3 + 1.5*IQR\ndf2_duration < lower_limit\n\nprint('lower limit:',lower_limit)\nprint('upper limit:',upper_limit)","c274cd35":"#threshold querying. Let us see if there is any values out of upper-lower limit values and how to reach them to see:\n\n(df2_duration < lower_limit) | (df2_duration> upper_limit) #this created us a vector called threshold value True-False vector.\n#That helps us to see the threshold values as True and the others will be False which are between the upper-lower bounds","7a173bc7":"t_TF = (df2_duration < lower_limit)\n\nt_TF.head()","dbf13000":"from sklearn.neighbors import LocalOutlierFactor\n\nclf = LocalOutlierFactor(n_neighbors = 20, contamination = 0.1)\nclf # this 'clf' contains figural features of LOF (Local Outlier Factor)\n\n","e99e8e99":"# now I will start the algorithm: Note: if there are any string or object type values, it will not work you have to convert the values by using label encoder and one hot encoder\nclf.fit_predict(df2)","dd9e10b9":"df_sc = clf.negative_outlier_factor_\n\n# we want to sort them to see the gap value to indicating the outlier limit. \n# Here, -6,17722 will be the best value to define it because of a sudden jump to lower values. its index is [2]\nnp.sort(df_sc)[0:20]","fd76e343":"# threshold value:\nt_value = np.sort(df_sc)[2] # we will accept that as a threshold value.","ae881684":"df2[df_sc == t_value]","8c89c1c0":"anomal_tf = df_sc > t_value\n\nprint_value = df2[df_sc == t_value] # We have defined the print values which will be written instead of the threshold values\nanomalous = df2[~anomal_tf] # anomalous defines the true-false format threshold values array.<\n\nanomal_tf","779bbbcb":"res = anomalous.to_records(index = False)  # here we erased their indexes to reunion them again. that's why we defined the printing values. \nres[:] = print_value.to_records(index = False)\n\ndf2[~anomal_tf] # we have checked if there is any difference we made or not","937461a5":"df2[~anomal_tf] = pd.DataFrame(res, index = df2[~anomal_tf].index) # we changed them all.\n\ndf2[~anomal_tf] # checked again it is ok or not. and it is ok.","30b9e0fa":"f, ax = plt.subplots(figsize = (15,12))\nsns.heatmap(df3.corr(), annot = True, linewidths = .8, fmt = '2f', ax = ax);","427b1fcd":"df2_new = pd.DataFrame(df2.loc[:,['job','education','housing','month','duration']].values)\ndf2_new.columns = ['job','education','housing','month','duration']\ndf2_new.head()","5aeab036":"df2_new.shape","c96063f9":"from sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import LinearSVC\n\n\nx_train, x_test, y_train, y_test = train_test_split(df2,df_target, stratify = df_target, random_state = 0 )\n\ndf2_indices = np.arange(df2.shape[-1])\n\nselector = SelectKBest(f_classif, k=5)\nselector.fit(x_train, y_train)\nscores = -np.log10(selector.pvalues_)\nscores \/= scores.max()\nplt.bar(df2_indices - .45, scores, width=.2,\n        label=r'Univariate score ($-Log(p_{value})$)')\n\n\nclf = make_pipeline(MinMaxScaler(), LinearSVC())\nclf.fit(x_train, y_train)\nprint('Classification accuracy without selecting features: {:.3f}'\n      .format(clf.score(x_test, y_test)))\n\nsvm_weights = np.abs(clf[-1].coef_).sum(axis=0)\nsvm_weights \/= svm_weights.sum()\n\nplt.bar(df2_indices - .25, svm_weights, width=.2, label='SVM weight')\n\nclf_selected = make_pipeline(\n        SelectKBest(f_classif, k=5), MinMaxScaler(), LinearSVC()\n)\nclf_selected.fit(x_train, y_train)\nprint('Classification accuracy after univariate feature selection: {:.3f}'\n      .format(clf_selected.score(x_test, y_test)))\n\nsvm_weights_selected = np.abs(clf_selected[-1].coef_).sum(axis=0)\nsvm_weights_selected \/= svm_weights_selected.sum()\n\nplt.bar(df2_indices[selector.get_support()] - .05, svm_weights_selected,\n        width=.2, label='SVM weights after selection')\n\n\n\nplt.title(\"Comparing feature selection\")\nplt.xlabel('Feature number')\nplt.yticks(())\nplt.axis('tight')\nplt.legend(loc='upper right')\nplt.show()","5fba6465":"sns.boxplot(x = df2_new.duration); #now it seems better","30a83654":"df2_new.info()","8bbed33f":"df2_new.shape,df_target.shape","33064991":"X = df2.loc[:,['job','education','housing','month','duration']].values\ndf1.y = le.fit_transform(df1.y)\nY = df1.loc[:,'y'].values\n\n\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix,classification_report","f7385943":"scaler = MinMaxScaler()\n\nxlr_train,xlr_test,ylr_train,ylr_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)\n\nxlr_train = scaler.fit_transform(xlr_train)\nxlr_test = scaler.fit_transform(xlr_test)\nylr_train = scaler.fit_transform(ylr_train.reshape(-1,1))\nylr_test = scaler.fit_transform(ylr_test.reshape(-1,1))\n\nlgr = LogisticRegression(random_state = 0)\nlgr.fit(xlr_train,ylr_train)\n\nylr_hat = lgr.predict(xlr_test)\n\n#confussion matrix & classifying report:\n\n\nc_m = confusion_matrix(ylr_test,ylr_hat)\nprint(c_m)\nprint(classification_report(ylr_test,ylr_hat))\n\n\nfpr, tpr, threshold = metrics.roc_curve(ylr_test, ylr_hat);\nroc_auc = metrics.auc(fpr, tpr);\n\nplt.title('Receiver Operating Characteristic');\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc);\nplt.legend(loc = 'lower right');\nplt.plot([0, 1], [0, 1],'r--');\nplt.xlim([0, 1]);\nplt.ylim([0, 1]);\nplt.ylabel('True Positive Rate');\nplt.xlabel('False Positive Rate');","ef85f44b":"cls_KN = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\ncls_KN.fit(xlr_train,ylr_train)\n\nykn_hat = cls_KN.predict(xlr_test)\n\ncon_mat = confusion_matrix(ylr_test,ykn_hat)\nprint(con_mat)\nprint(classification_report(ylr_test,ykn_hat))\n\n\nfpr, tpr, threshold = metrics.roc_curve(ylr_test, ykn_hat);\nroc_auc = metrics.auc(fpr, tpr);\n\nplt.title('Receiver Operating Characteristic');\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc);\nplt.legend(loc = 'lower right');\nplt.plot([0, 1], [0, 1],'r--');\nplt.xlim([0, 1]);\nplt.ylim([0, 1]);\nplt.ylabel('True Positive Rate');\nplt.xlabel('False Positive Rate');","422006eb":"from sklearn.naive_bayes import GaussianNB\n\nnb_G = GaussianNB()\nnb_G.fit(xlr_train,ylr_train)\n\nynb_hat = nb_G.predict(xlr_test)\n\nco_ma = confusion_matrix(ylr_test,ynb_hat)\nprint('C-M:', co_ma)\nprint(classification_report(ylr_test,ynb_hat))\n\n\nfpr, tpr, threshold = metrics.roc_curve(ylr_test, ynb_hat);\nroc_auc = metrics.auc(fpr, tpr);\n\nplt.title('Receiver Operating Characteristic');\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc);\nplt.legend(loc = 'lower right');\nplt.plot([0, 1], [0, 1],'r--');\nplt.xlim([0, 1]);\nplt.ylim([0, 1]);\nplt.ylabel('True Positive Rate');\nplt.xlabel('False Positive Rate');","5baf6bc5":"svc_c = SVC(kernel = 'rbf', random_state = 0, C = 1.0, degree = 3, gamma = 'auto')\n\nsvc_c.fit(xlr_train,ylr_train)\nyvm_hat = svc_c.predict(xlr_test)\n\nprint(confusion_matrix(ylr_test,yvm_hat))\nprint(classification_report(ylr_test,yvm_hat))\n\n\nfpr, tpr, threshold = metrics.roc_curve(ylr_test, yvm_hat);\nroc_auc = metrics.auc(fpr, tpr);\n\nplt.title('Receiver Operating Characteristic');\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc);\nplt.legend(loc = 'lower right');\nplt.plot([0, 1], [0, 1],'r--');\nplt.xlim([0, 1]);\nplt.ylim([0, 1]);\nplt.ylabel('True Positive Rate');\nplt.xlabel('False Positive Rate');","115a24f2":"rfc = RandomForestClassifier(n_estimators=10, criterion = 'entropy')\nrfc.fit(xlr_train,ylr_train)\n\nyrfc_hat= rfc.predict(xlr_test)\ncm = confusion_matrix(ylr_test,yrfc_hat)\nprint('RFC')\nprint('c-m',cm)\nprint(classification_report(ylr_test,yrfc_hat))\n\n\n\nfpr, tpr, threshold = metrics.roc_curve(ylr_test, yrfc_hat);\nroc_auc = metrics.auc(fpr, tpr);\n\nplt.title('Receiver Operating Characteristic');\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc);\nplt.legend(loc = 'lower right');\nplt.plot([0, 1], [0, 1],'r--');\nplt.xlim([0, 1]);\nplt.ylim([0, 1]);\nplt.ylabel('True Positive Rate');\nplt.xlabel('False Positive Rate');","2095f3ff":"dtc = DecisionTreeClassifier(criterion = 'entropy')\n\ndtc.fit(xlr_train,ylr_train)\nydtc_hat = dtc.predict(xlr_test)\n\nprint(confusion_matrix(ylr_test,ydtc_hat))\nprint(classification_report(ylr_test,ydtc_hat))\n\n\n\nfpr, tpr, threshold = metrics.roc_curve(ylr_test, ydtc_hat);\nroc_auc = metrics.auc(fpr, tpr);\n\nplt.title('Receiver Operating Characteristic');\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc);\nplt.legend(loc = 'lower right');\nplt.plot([0, 1], [0, 1],'r--');\nplt.xlim([0, 1]);\nplt.ylim([0, 1]);\nplt.ylabel('True Positive Rate');\nplt.xlabel('False Positive Rate');","2a1de0bd":"print('TPR:', tpr[1])\nprint('FPR: ', fpr[1])","52ca1abd":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport tensorflow as tf","fa4558e6":"classifier=Sequential()\nclassifier.add(Dense(6,kernel_initializer='glorot_uniform',activation='relu',input_dim=5))\nclassifier.add(Dense(6,kernel_initializer='glorot_uniform',activation='relu'))\nclassifier.add(Dense(1,kernel_initializer='glorot_uniform',activation='sigmoid'))\nclassifier.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\nclassifier.fit(xlr_train,ylr_train,epochs=50)\nyys_hat=classifier.predict(xlr_test)\nyys_hat=(yys_hat>0.5)\n\nprint(confusion_matrix(ylr_test,yys_hat))\nprint(classification_report(ylr_test,yys_hat))\n\nfpr, tpr, threshold = metrics.roc_curve(ylr_test, yys_hat);\nroc_auc = metrics.auc(fpr, tpr);\n\nplt.title('Receiver Operating Characteristic');\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc);\nplt.legend(loc = 'lower right');\nplt.plot([0, 1], [0, 1],'r--');\nplt.xlim([0, 1]);\nplt.ylim([0, 1]);\nplt.ylabel('True Positive Rate');\nplt.xlabel('False Positive Rate');\n\n","f52a35c4":"from sklearn.model_selection import cross_val_score","bc98abb4":"from sklearn.metrics import accuracy_score\n\nfor clf in (lgr, cls_KN, nb_G, svc_c, rfc, dtc):\n    clf.fit(xlr_train, ylr_train)\n    predict = clf.predict(xlr_test)\n    print(clf.__class__.__name__, accuracy_score(ylr_test, predict))","1392c3c9":"accrcy = cross_val_score(estimator = lgr, X = xlr_train, y = ylr_train, cv = 5);\nprint('LOG-REG - accuracy mean % :', accrcy.mean()*100);\nprint('LOG-REG - Std % : ', accrcy.std()*100);\n\naccrcy = cross_val_score(estimator = cls_KN, X = xlr_train, y = ylr_train, cv = 5);\nprint('KN - accuracy mean % :', accrcy.mean()*100);\nprint('KN - Std % : ', accrcy.std()*100);\n\naccrcy = cross_val_score(estimator = nb_G, X = xlr_train, y = ylr_train, cv = 5);\nprint('NB_G - accuracy mean % :', accrcy.mean()*100);\nprint('NB_G - Std % : ', accrcy.std()*100);\n\naccrcy = cross_val_score(estimator = svc_c, X = xlr_train, y = ylr_train, cv = 5);\nprint('SVM - accuracy mean % :', accrcy.mean()*100);\nprint('SVM - Std % : ', accrcy.std()*100);\n\naccrcy = cross_val_score(estimator = rfc, X = xlr_train, y = ylr_train, cv = 5);\nprint('R_F - accuracy mean % :', accrcy.mean()*100);\nprint('R_F - Std % : ', accrcy.std()*100);\n\naccrcy = cross_val_score(estimator = dtc, X = xlr_train, y = ylr_train, cv = 5);\nprint('D_T - accuracy mean % :', accrcy.mean()*100);\nprint('D_T - std % : ', accrcy.std()*100);\n","89bd9ef9":"## Artificaial Neural Networks :","ff3c6528":"## Random Forest : ","a2e85b04":"## Categoric Values Summary :\nchecking the categoric variables classes and number of classes","973a8a23":"## Descriptional Side:","3570f7e7":"## TPR, FPR Values :","304f7a62":"we can select the campaign instead of day","8b06670e":"Above, on the table I see great amount of 'unknown'. I would like to keep them as 'NaN' at first. Then <br> probably I will aggregate them around threshold value.","95ba68d8":"### What is a Term Deposit?\nA term deposit is a fixed-term investment that includes the deposit of money into an account at a financial institution.  Term deposit investments usually carry short-term maturities ranging from one month to a few years and will have varying levels of required minimum deposits. \nFrom investopedia:https:\/\/www.investopedia.com\/terms\/t\/termdeposit.asp","79613e7a":"## Attribute Descriptions:","5cb1edeb":"## Missing Values :","d72fada7":"## Naive Bayes : ","23d2378c":"### Visualization :\n##### JOB","1f4ec163":"As a conclusion we can choose: <br> \n**1.**Logistic Regression: <br>***Mean Accuracy : 93,20***<br> ***AUC           : 0,75***<br>\n**2.**NB_G : <br> ***Mean Accuracy : 93,10***<br> ***AUC           : 0,79***<br>\n\n\n#### RESULT :\n\n***Logistic Regression Model will be the best fit for the prediction!***<br>\n\n\n<br>**Age:** For the next campaign company should focus on the retired-student groups more than others and start a campaign think about calling  the retired people maybe more to explain better and campaign maker should find a way to explain elders better.<br>\n<br>**Job:** The potential customers are students and retired people. In addition on the phone if consultant explains that is a short term investment retired group will be interested in term deposit .<br>\n<br>**Education:** For the educational side of the campaign, company should focus on the low-educational-level customers and should explain it's short-term functionality.<br>\n<br>**Duration:** Duration is a very important key because of annpying people if it not suitable for its purpose. Especially, the call-center consultant must be very clear and understandable that's why the campaign should start from inside. That means better communication. If call-center can do that, duration will be shorter and it will effect the performans of the call-center and they can have chance to finding new customers.<br>\n<br>**Default & Housing:** Default and housing might be bad. But there is a possibility that they can save   small amounts of their income for their housing monthly payment. This idea might be possibility to reaching out the customer for selling.<br>\n<br>**month:** People, in general, save part of their income to buy new things(home,car,bike,... etc.) or be ready for the seasonal activities or any other reason. Here we can see the seasonal affect on the term deposit. That's why the company should prepare any other special offers or different reason to be a part of this campaign.<br>\n\n<br>In conclusion company should tend to more effective variables than the other to be successful. And Company should simplify the strategies depends on the situations. \n\n","e565df2a":"**1.balance**here we can say from the table that there are some minus balance owners in our bank<br>\n**2.age**mean age is 40 our mostl target age cluster probably will be that might be between 30 and 50 years old people<br>\n**3.day**our customers avarage day of the month is middle of the months to sell their term deposits<br>\n**4.duration**avarage duration of term deposit is 254  days and its standard daviation is 259 that means the duration might be between 250<br> and 500 days<br> \n**5.Campaign**campaing is the categoric and yes\/no answers that's why it will not be able to assess.","bf3d1bc2":"Duration(0.46) - month(0.04) - housing(0.05) - education(0.47) - job<br>\nThey will be our model features.","b7161677":"#### DURATION","3c83b0f8":"With this result, we can say that we do not need to erase whole column.<br>\nBecause their rates are not above the 50 % or more. Only the 'Contact' column has 31 %, others have very small rates to do something.<br>\nI may replace the NaN values with the median value of its own column.","bc577e39":"## Support Vector Machine :","ae30c2e2":"We observe from the table that there is a seasonal affect. In spring and autumn the customers tend to buy financial products.","4ad541ce":"Mostly our customers consist of blue-collars. And if we want to sort them as top 5 we can say:<br>\n**1.** Blue-Collar<br>\n**2.** Management<br>\n**3.** Technicians<br>\n**4.** Admins<br>\n**5.** Services<br>\n\nAlso we can understand from this graph that, great amount of our term deposite customers consist of middile income groups.\n\nHousemaids, students, entrepreneurs, self-employed people and retireds they are not exactly buying this product.<br>\nThat's why they will be our focus groups for term-deposit campaign.","aee099b9":"##### MARITAL\nAcording to plot below marital status doesn't show us clearly if there is a certain situation.","4b892041":"#####  Missing Value Rates :","ecddd5d6":"## Threshold Detection & Fullfilling:","610eefbd":"Duration is the important featue for us because it is the time duration when we make a phone call.<br> It is really important because we can see from the plot that the customers do not prefer the long call durations.<br> That can make people annoying and the company might loose potential customers because of the redial frequency and its duration.","3e4c0cd0":"## MODEL ESTIMATION\n\n## Logistic Regression : ","29fb3577":"## Data Transform and feature selection","6456f12b":" Replaced them with the NaN values. Now we can detect and understand the weight of the NaN values on their own columns.","5f499f6a":"#### df_sc : it is our score table which is created by using LocalOutlierFactor : df_sc = clf.negative_otlier_factor_\n#### t_value: it is our print value which will be used for filling instead of the threshold value","affa0b0f":"#### Feature Selection Test Before Threshold :","e8dbf992":"#### Feature Selection after Threshold :","62bda821":"That Pie chart shows us, \"Term-deposite opened & did not opened\"<br>\nOpened : 7.24 %<br>\nDid not opened : 92.76 %<br>","5461179e":"#### EDUCATION\nEducation plot shows us that, when the educational level is getting higher, also tendency of making saving is also getting higher.<br>\nAlso we can say from this conclusion that, students are also our potential customers. We can start a campaign also for them.<br> Maybe special offers for the university students, might be more effective.","8a656510":"## Decission Trees :","977195b1":"## KNeighbors Classification :","86572437":"Classification accuracy without selecting features: 0.932<br>\nClassification accuracy after univariate feature selection: 0.932<br>\n\nI choose the variables depend on the SVM weights<br>\nThat's why the variable will be the 12th,11th, 9th, 7th, 4th and 2nd variables","a6888321":"**1 - age:** (numeric)<br>\n**2 - job:** type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')<br>\n**3 - marital:** marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)<br>\n**4 - education:** (categorical: primary, secondary, tertiary and unknown)<br>\n**5 - default:** has credit in default? (categorical: 'no','yes','unknown')<br>\n**6 - housing:** has housing loan? (categorical: 'no','yes','unknown')<br>\n**7 - loan:** has personal loan? (categorical: 'no','yes','unknown')<br>\n**8 - balance:** Balance of the individual.<br>\n**9 - contact:** contact communication type (categorical: 'cellular','telephone') <br>\n**10 - month:** last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')<br>\n**11 - day:** last contact day of the week (categorical: 'mon','tue','wed','thu','fri')<br>\n**12 - duration:** last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.<br>\n**13 - campaign:** number of contacts performed during this campaign and for this client (numeric, includes last contact)<br>\n**14 - y** - has the client subscribed a term deposit? (binary: 'yes','no')","e373bcad":"###### here I will be changing the all missing values with their median values.","8547f2af":"As I wroted above, our focus group maight be Housemaids, students entrepreneurs, self-employed perople and retireds.<br>\nSeemingly, especially the student group has the higher tendency to buying term-deposit.<br>"}}