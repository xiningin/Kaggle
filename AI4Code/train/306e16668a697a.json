{"cell_type":{"f127caba":"code","91b32110":"code","18a3b0de":"code","ff86f997":"code","a1a2ff47":"code","64765673":"code","5727d715":"code","51dde851":"code","2a440d2b":"code","77fc7c4f":"code","ad6db368":"code","66b26540":"code","0ad659bb":"code","8430f399":"code","bb669176":"code","602f2e42":"markdown","13798014":"markdown"},"source":{"f127caba":"import pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport re\nfrom string import punctuation","91b32110":"train = pd.read_csv(\"..\/input\/train.csv\")[:100]\ntest = pd.read_csv(\"..\/input\/test.csv\")[:100]","18a3b0de":"train.head()","ff86f997":"test.head()","a1a2ff47":"# Check for any null values\nprint(train.isnull().sum())\nprint(test.isnull().sum())","64765673":"# Add the string 'empty' to empty strings\ntrain = train.fillna('empty')\ntest = test.fillna('empty')","5727d715":"# Preview some of the pairs of questions\na = 0 \nfor i in range(a,a+10):\n    print(train.question1[i])\n    print(train.question2[i])\n    print()","51dde851":"stop_words = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n              'Is','If','While','This']","2a440d2b":"def text_to_wordlist(text, remove_stop_words=True, stem_words=False):\n    # Clean the text, with the option to remove stop_words and to stem words.\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n    text = re.sub(r\"what's\", \"\", text)\n    text = re.sub(r\"What's\", \"\", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"I'm\", \"I am\", text)\n    text = re.sub(r\" m \", \" am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"60k\", \" 60000 \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e-mail\", \"email\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    text = re.sub(r\"quikly\", \"quickly\", text)\n    text = re.sub(r\" usa \", \" America \", text)\n    text = re.sub(r\" USA \", \" America \", text)\n    text = re.sub(r\" u s \", \" America \", text)\n    text = re.sub(r\" uk \", \" England \", text)\n    text = re.sub(r\" UK \", \" England \", text)\n    text = re.sub(r\"india\", \"India\", text)\n    text = re.sub(r\"switzerland\", \"Switzerland\", text)\n    text = re.sub(r\"china\", \"China\", text)\n    text = re.sub(r\"chinese\", \"Chinese\", text) \n    text = re.sub(r\"imrovement\", \"improvement\", text)\n    text = re.sub(r\"intially\", \"initially\", text)\n    text = re.sub(r\"quora\", \"Quora\", text)\n    text = re.sub(r\" dms \", \"direct messages \", text)  \n    text = re.sub(r\"demonitization\", \"demonetization\", text) \n    text = re.sub(r\"actived\", \"active\", text)\n    text = re.sub(r\"kms\", \" kilometers \", text)\n    text = re.sub(r\"KMs\", \" kilometers \", text)\n    text = re.sub(r\" cs \", \" computer science \", text) \n    text = re.sub(r\" upvotes \", \" up votes \", text)\n    text = re.sub(r\" iPhone \", \" phone \", text)\n    text = re.sub(r\"\\0rs \", \" rs \", text) \n    text = re.sub(r\"calender\", \"calendar\", text)\n    text = re.sub(r\"ios\", \"operating system\", text)\n    text = re.sub(r\"gps\", \"GPS\", text)\n    text = re.sub(r\"gst\", \"GST\", text)\n    text = re.sub(r\"programing\", \"programming\", text)\n    text = re.sub(r\"bestfriend\", \"best friend\", text)\n    text = re.sub(r\"dna\", \"DNA\", text)\n    text = re.sub(r\"III\", \"3\", text) \n    text = re.sub(r\"the US\", \"America\", text)\n    text = re.sub(r\"Astrology\", \"astrology\", text)\n    text = re.sub(r\"Method\", \"method\", text)\n    text = re.sub(r\"Find\", \"find\", text) \n    text = re.sub(r\"banglore\", \"Banglore\", text)\n    text = re.sub(r\" J K \", \" JK \", text)\n    \n    # Remove punctuation from text\n    text = ''.join([c for c in text if c not in punctuation])\n    \n    # Optionally, remove stop words\n    if remove_stop_words:\n        text = text.split()\n        text = [w for w in text if not w in stop_words]\n        text = \" \".join(text)\n    \n    # Optionally, shorten words to their stems\n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stemmed_words = [stemmer.stem(word) for word in text]\n        text = \" \".join(stemmed_words)\n    \n    # Return a list of words\n    return(text)","77fc7c4f":"def process_questions(question_list, questions, question_list_name, dataframe):\n    '''transform questions and display progress'''\n    for question in questions:\n        question_list.append(text_to_wordlist(question))\n        if len(question_list) % 100000 == 0:\n            progress = len(question_list)\/len(dataframe) * 100\n            print(\"{} is {}% complete.\".format(question_list_name, round(progress, 1)))","ad6db368":"train_question1 = []\nprocess_questions(train_question1, train.question1, 'train_question1', train)","66b26540":"train_question2 = []\nprocess_questions(train_question2, train.question2, 'train_question2', train)","0ad659bb":"test_question1 = []\nprocess_questions(test_question1, test.question1, 'test_question1', test)","8430f399":"test_question2 = []\nprocess_questions(test_question2, test.question2, 'test_question2', test)","bb669176":"# Preview some transformed pairs of questions\na = 0 \nfor i in range(a,a+10):\n    print(train_question1[i])\n    print(train_question2[i])\n    print()","602f2e42":"# The Importance of Cleaning the Text","13798014":"After a few different iterations, I think that I have found a pretty good way to clean the questions to improve the performance of a model. I was able to reduce my loss value by a few points because of this method.  Feel free to use this code and improve upon the method!"}}