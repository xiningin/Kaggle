{"cell_type":{"a1d200b5":"code","bdc2ff58":"code","6778849b":"code","50e864fa":"code","9a254fa0":"code","207f6717":"code","5201d871":"code","27cc1f99":"code","90f94f93":"code","cd1739f2":"code","fbf3b59c":"code","4bc8e690":"code","0ccf2d61":"code","322fdd94":"markdown","2171031e":"markdown","f014ac28":"markdown","0115d87e":"markdown"},"source":{"a1d200b5":"\"\"\"\nsource Github : https:\/\/github.com\/DTrimarchi10\/confusion_matrix.git\nThis file contains a function called make_confusion_matrix which can be used to create a useful visualzation of a Confusion Matrix passed in as a two dimensional numpy array.\n\"\"\"\nfrom shutil import copyfile\ncopyfile(src = \"\/kaggle\/input\/confusion-matrix\/cf_matrix.py\", dst = \"..\/working\/cf_matrix.py\")\nfrom cf_matrix import make_confusion_matrix","bdc2ff58":"# Imports\nimport os\nimport math\nimport numpy as np\nfrom keras.applications import DenseNet121\nimport matplotlib.pyplot as plt\nfrom keras import models\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler\nfrom sklearn.metrics import confusion_matrix","6778849b":"# Parameters\nbatch_size = 128\ndim = 224\nepochs_1 = 20\nepochs_2 = 20\ninitial_learning_rate = 1e-3\ndropEvery = 4\ndrop_rate = 0.5\nsave_model_name = 'garbage_classifier.h5'","50e864fa":"# Constants & functions\ndef count_img(dir):\n    nb = 0\n    for subdirectory in os.listdir(dir):\n        nb += len(os.listdir(os.path.join(dir, subdirectory)))\n    return nb\n\ndef plot_cm(labels, predictions, categories):\n    predictions = np.argmax(predictions, axis=1)\n    cm = confusion_matrix(labels, predictions)\n    make_confusion_matrix(cm, percent=False, categories=categories)\n    \ndef lr_step_decay(epoch, lr, d=drop_rate, e=dropEvery):\n    drop_rate = d\n    epochs_drop = e\n    return initial_learning_rate * math.pow(drop_rate, math.floor(epoch\/epochs_drop))\n\nmain_dir = '\/kaggle\/input\/augmenteddatasetv4\/'\nclass_mode = 'categorical'\nclasses = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\ntrain_dir = os.path.join(main_dir, 'train')\nvalidation_dir = os.path.join(main_dir, 'validation')\ntest_dir = os.path.join(main_dir, 'test')\nsteps_per_epoch = count_img(train_dir) \/\/ batch_size+1\nvalidation_steps = count_img(validation_dir) \/\/ batch_size+1\ntest_steps = count_img(test_dir) \/\/ batch_size+1","9a254fa0":"# Data preprocessing\ntrain_datagen = ImageDataGenerator(rescale=1.\/255)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(dim, dim),\n    batch_size=batch_size,\n    class_mode=class_mode,\n    classes=classes)\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(dim, dim),\n    batch_size=batch_size,\n    class_mode=class_mode,\n    classes=classes,\n    shuffle=False)\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_dir,\n    target_size=(dim, dim),\n    batch_size=batch_size,\n    class_mode=class_mode,\n    classes=classes)","207f6717":"# Setup the model\nconv_base = DenseNet121(weights='imagenet', include_top=False, input_shape=(dim, dim, 3))\nconv_base.trainable = False\nmodel = models.Sequential()\nmodel.add(conv_base)\nmodel.add(layers.GlobalAveragePooling2D(input_shape=(4, 4, 1024)))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(len(classes), activation='softmax'))\n\nmodel.compile(optimizer=optimizers.Adam(lr=initial_learning_rate), loss='categorical_crossentropy', metrics=['acc'])\nhistory = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=epochs_1, batch_size=batch_size,\n                    validation_data=validation_generator, validation_steps=validation_steps, callbacks=[LearningRateScheduler(lr_step_decay, verbose=1)])","5201d871":"# Plot training metrics before fine-tuning\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","27cc1f99":"# Test the model before fine-tuning\ntest_loss, test_acc = model.evaluate(test_generator, steps=test_steps)\nprint('test accuracy : ', test_acc)","90f94f93":"# Fine-tuning : unfreezing some of the top layers in the base of the pretrained convnet\nconv_base.trainable = True\nset_trainable = False\nfor layer in conv_base.layers:\n    if 'conv5_block16' in layer.name:\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False\n\nmodel.compile(optimizer=optimizers.Adam(lr=initial_learning_rate), loss='categorical_crossentropy', metrics=['acc'])\nhistory = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=epochs_2, batch_size=batch_size,\n                    validation_data=validation_generator, validation_steps=validation_steps, callbacks=[LearningRateScheduler(lr_step_decay, verbose=1)])\nmodel.save(save_model_name)","cd1739f2":"# Plot final training metrics\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","fbf3b59c":"# Test the final model\ntest_loss, test_acc = model.evaluate(test_generator, steps=test_steps)\nprint('test accuracy : ', test_acc)","4bc8e690":"# Plot the onfusion matrix\ntest_predictions = model.predict(test_generator, steps=test_steps)\ntest_labels = test_generator.classes\nplot_cm(test_labels, test_predictions, classes)\nplt.show()","0ccf2d61":"# By VocoJax : https:\/\/stackoverflow.com\/questions\/43017017\/keras-model-predict-for-a-single-image\n\nfrom PIL import Image\nfrom skimage import transform\n\n\ndef load(filename):\n    np_image = Image.open(filename)\n    np_image = np.array(np_image).astype('float32')\/255\n    np_image = transform.resize(np_image, (224, 224, 3))\n    np_image = np.expand_dims(np_image, axis=0)\n    return np_image\n\n\ndirectory = '..\/input\/internet-random-images\/random images intenet'\nenvelop = os.path.join(directory, 'paper.jpeg')\nbottle = os.path.join(directory, 'plastic.jpeg')\ncan = os.path.join(directory, 'metal.jpeg')\nbox = os.path.join(directory, 'cardboard.jpeg')\nbeer = os.path.join(directory, 'glass.jpeg')\nwrapping = os.path.join(directory, 'trash.jpg')\nimages = [envelop, bottle, can, box, beer, wrapping]\nmodel = models.load_model('..\/input\/garbage-classifier\/garbage_classifier.h5')\nfor image in images:\n    img = load(image)\n    result = model.predict(img)\n    result = np.argmax(result)\n    print(image + ' : ' + classes[result])","322fdd94":"# Testing the model on random images\n\nInspired by the notebook : https:\/\/www.kaggle.com\/aadhavvignesh\/pytorch-garbage-classification-95-accuracy\n\nI came across this notebook while working on the dataset, and I thought it was a great idea to test it on images taken randomly from the internet.\n\n![cardboard.jpeg](attachment:92a35e16-fe08-4edb-b9c3-209b85db2ec0.jpeg)![glass.jpeg](attachment:779932ad-63fe-4090-bce8-9b7ca444fbce.jpeg)![metal.jpeg](attachment:067a5508-f2e4-44b4-bea0-2b1e73446674.jpeg)![paper.jpeg](attachment:271a9032-98e1-4c6a-a787-253706e99b31.jpeg)![plastic.jpeg](attachment:052c6adc-a10c-4a7a-a8f4-ea8df79786a6.jpeg)![trash.jpg](attachment:47eca78c-ebea-4535-8868-860f20a1ca3f.jpg)","2171031e":"# Conclusion\n\nAfter a lot of testing, my personal best is 93% accuracy on this dataset. Which I am proud of even if it surely can be improved.\n\nThe final test with internet images is interesting because it shows the produced model in live action, and also points out a possible improvement in the initial dataset. The only class which was not recognized by the model is 'trash'. If you take a closer look at this class, you found out that some of it is plastic, or paper, etc... So this class is both undersized in the dataset and extremely vague in practice.\n\nThat's all for me. I hope you enjoyed this content, and please let me know if that is the case !","f014ac28":"# Code\n\nThis section contains the entire code for data preprocessing, training, evaluation and testing of the final network (which results are just above). The code to split the dataset has not been added to the notebook, because ... well it is already long enough. In case you want to check the augmented datasets, I have uploaded 3 versions as inputs for this notebook.","0115d87e":"Welcome !\n\nThis is my second public notebook, so please give me any feedback that would help me improve, I'm eager to learn ! \n\n# Quick dataset overview and observations\n\nOverall balance of each classes in the dataset\n\n    cardboard :  403\n    glass :  501\n    metal :  410\n    paper :  594\n    plastic :  482\n    trash :  137\n\nThe dataset is unbalanced, especially the 'trash' class. Three solutions come to my mind to adress this problem, first to undersample every class and use data augmentation for the 'trash' class. Second is to use data augmentation for every class but the most represented one (balance aligned with the 'paper' class). Last is to use data augmentation on all 6 classes of the dataset with no regards for the initial unbalance.\n\nThe notebook is divided in 4 sections. First a very simple baseline to get an idea of the problem complexity. Next I try to beat the baseline using pretrained convets, then adding new features to improve results and fine-tuning all parameters. Finally all the code to obtain the final network is included, with a bonus test on internet images. Various network architectures were tested, you can check the different versions of this notebook if you want the code of one in particular. Each time I show at least the training accuracy curve and the results of the model on the testing set.\n\n\n# Baseline\n\nFirst let's check what we can get out of this dataset with a basic plain CNN model and using the second idea presented earlier regarding the training dataset.\n\n![model_3_6_acc.png](attachment:f0db4e8b-7675-42a4-842c-386367fe6224.png)\n\n    test accuracy :  0.6299212574958801\n\n\n# Pretrained convnets\n\nThe objective was to beat the baseline. In this section of the notebook you will find the best results obtained while trying various parameters for each of the following pretrained models (importing imagenet settings).\n\n## VGG16 & VGG19\n\n**VGG16**\n    \n![model_5_5_1_acc.png](attachment:18bbff52-7496-48cc-86b8-bd87e72601bc.png)\n\n    test accuracy :  0.7559055089950562\n\n**VGG19**\n\n![model_8_1_1_acc.png](attachment:be37ef57-ccb6-4cce-9748-119245bf04b9.png)\n\n    test accuracy :  0.7338582873344421\n    \nNo concrete improvements between the two versions, although the VGG19 seems to converge faster.\n\n\n## ResNet\n\n**ResNet50v2**\n\n![model_9_3_1_acc.png](attachment:d61beae4-0f7c-4496-b824-3eec341ea430.png)\n\n    test accuracy :  0.7984251976013184\n\n**ResNet101v2**\n\n![model_10_4_1_acc.png](attachment:0b73ea4c-6c19-40cc-ad9e-22e142569337.png)\n\n    test accuracy :  0.8141732215881348\n\n## DenseNet\n\n**DenseNet121**\n\n![model_11_1_1_acc.png](attachment:6d4fa722-ff87-41dd-af65-a705c1499f0e.png)\n\n    test accuracy :  0.7921259999275208\n\n\n# Fine tuning\n\n## Useful ressources\n\nIn order to fine-tune a model I needed to research ideas to make that happen. Bellow are the ressources that helped me the most for this task.\n\n* Deep Learning with Python, Fran\u00e7ois Chollet\n* https:\/\/www.pyimagesearch.com\/2020\/04\/27\/fine-tuning-resnet-with-keras-tensorflow-and-deep-learning\/\n* https:\/\/towardsdatascience.com\/transfer-learning-from-pre-trained-models-f2393f124751\n* https:\/\/medium.com\/octavian-ai\/which-optimizer-and-learning-rate-should-i-use-for-deep-learning-5acb418f9b2\n* https:\/\/www.pyimagesearch.com\/2019\/07\/22\/keras-learning-rate-schedules-and-decay\/\n\n\n## Data augmentation & Learning rate\n\nAs I was saying in the introduction, one of the ideas to be tested is to augment the data of all classes regardless of the initial unbalance. Because I didn't know to which point this method will stop improving results, I created multiple version of an augmented dataset, gradually generating more images for the training set (and from, validation and test splits are not to be augmented). I started with 500 images for each class in the training set, and ended up with 1200 images for each class. I also changed from RMSprop to Adam optimizer. SGD also seemed interesting as it was used in multiple articles that I checked online, but so far it is the only one I have not tested. Here is a list of representative results obtained while trying to balance parameters with increasingly augmented data. For all versions I used 65% of the dataset for training, 10% for validation and 25% for testing. Images of each classes were shuffled before segmentation between these three sets.\n\n**500 training images**\n\n* type : ResNet50v2 -> Dense(256)\n* batch_size = 64\n* dimension of input images = (224, 224, 3)\n* epochs = 15\n* learning rate = 1e-4\n\n![image.png](attachment:767ccb7d-a438-405a-94db-aa44ba7c4b3e.png)\n\n![image.png](attachment:87fecf3d-01e0-4664-8831-d162059e3d51.png)\n\n    test accuracy :  0.8283464312553406\n\n**600 training images**\n\n* type : ResNet50v2 -> Dense(256) -> Dense(128) -> Dense(64)\n* batch_size = 64\n* dimension of input images = (224, 224, 3)\n* epochs = 50\n* learning rate = 1e-3\n\n![image.png](attachment:bd781da6-9b63-41f1-a69d-d811886de21f.png)\n\n![image.png](attachment:aecb114a-42a0-4e19-8b7b-3760a34bbe49.png)\n\n    test accuracy :  0.861417293548584\n\n**900 training images**\n\n* type : ResNet50v2 -> Dense(256) -> Dense(128)\n* batch_size = 64\n* dimension of input images = (224, 224, 3)\n* epochs to train the upper layer = 20\n* learning rate = 1e-4\n* epochs to train last pretrained block with upper layer = 20\n* learning rate = 1e-3\n\n![image.png](attachment:02932488-7270-4218-b471-0fcc2382c3da.png)\n\n![image.png](attachment:3c000bab-e5e5-4472-a2a1-6bef5c672937.png)\n\n    test accuracy :  0.8850393891334534\n    \n**1200 training images**\n\n* type : ResNet50v2 -> Dense(256)\n* batch_size = 64\n* dimension of input images = (224, 224, 3)\n* epochs to train the upper layer  = 20\n* epochs to train last pretrained block with upper layer = 20\n* initial_learning_rate = 1e-3\n* Step-based learning rate decay every 4 epochs (drop_rate = 0.5)\n\n![image.png](attachment:a1e9e853-7775-4cba-96db-f93d7126d32a.png)\n\n![image.png](attachment:2debb805-fc16-44de-98e1-1ff12d90ac5f.png)\n\n    test accuracy :  0.8929134011268616\n    \n![image.png](attachment:81d2a792-d09b-4d12-9883-34c6dbcd6e51.png)\n\n\n## Batch size\n\n* initial tests with the plain CNN : 32 \n* pretrained convnets : 64 \n* fine-tuned models :  128\n\nAdmittedly possible thanks to the GPU computation time available through Kaggle (not trying to bribe anyone, trully grateful I promiss ;) )\n\n\n## Pooling layers\n\n**Adding a Pooling(7, 7) layer to reduce data dimensionality before it is processed by the Dense layer**\n\n* type : ResNet50v2 -> AveragePooling2D(pool_size=(7, 7)) -> Dense(256)\n* batch_size = 128\n* dimension of input images = (224, 224, 3)\n* epochs to train the upper layer  = 20\n* epochs to train last pretrained block with upper layer = 20\n* initial_learning_rate = 1e-3\n* Step-based learning rate decay every 4 epochs (drop_rate = 0.5)\n\n![image.png](attachment:8b76f7ec-2060-42d3-9a77-dec2414cd852.png)\n\n![image.png](attachment:dbd36148-97b6-4909-93b1-28cb09c85bef.png)\n\n    test accuracy :  0.8976377844810486\n\n![image.png](attachment:ba2e621e-953f-4d8e-8f83-81ccc07352a7.png)\n\n\n**Switching from (Flatten + Dense) layers to GlobalAveragePooling2D layer**\n\n* type : DenseNet121 -> GlobalAveragePooling2D(input_shape=(4, 4, 1024))\n* batch_size = 128\n* dimension of input images = (224, 224, 3)\n* epochs to train the upper layer  = 20\n* epochs to train last pretrained block with upper layer = 20\n* initial_learning_rate = 1e-3\n* Step-based learning rate decay every 4 epochs (drop_rate = 0.5)\n\n![image.png](attachment:147fc053-6cd6-42e1-9727-7f99ae4395cc.png)\n\n![image.png](attachment:36c1da62-4255-46bb-97a5-e3031db77b5e.png)\n\n    test accuracy :  0.9322834610939026\n\n![image.png](attachment:e5565a53-3d39-4de8-94ec-7cb69928c91e.png)"}}