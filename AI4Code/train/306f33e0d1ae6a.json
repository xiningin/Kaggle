{"cell_type":{"fe5e6e1c":"code","43e7574d":"code","657bc6b9":"code","88afd801":"code","1dce9ef1":"code","c49a42be":"code","5970c476":"code","7bfc5c88":"code","fce54638":"code","8dcb0338":"code","397c9fd9":"code","e58438ee":"code","af9b14b0":"code","bafea1b2":"code","6e21a23c":"code","d7d36133":"code","f123f95e":"code","153121fd":"code","ed610d90":"code","20a8c4de":"code","35bfec8a":"code","33f1495e":"code","4f528d94":"code","876a0bf2":"code","56a78f57":"code","29d53cfd":"code","fa0ff126":"code","85896cb2":"code","15590751":"code","76176c29":"code","5a79a19b":"code","093e9c4d":"code","85223b38":"code","7183044d":"code","61c5a1f0":"code","a3328366":"code","b65c189d":"code","cc10af57":"code","d4e7e0f9":"code","fd5cea12":"code","bc4a950e":"code","56acfabc":"code","330304e6":"code","021d2b4c":"code","bc0e34ea":"code","6c516c3d":"code","428c54ca":"code","1078a4f2":"code","284e0035":"code","cf786712":"code","ac8c21b1":"code","3017f48e":"code","26b70778":"code","eded9401":"code","6038fad4":"code","7bd976eb":"code","5e834751":"code","e223f2d0":"code","e002ddd3":"code","9255566c":"code","3c7d169e":"code","6d8280ee":"code","78b43f50":"code","b55a6a64":"code","a82c9c11":"code","b18ed027":"code","347e90b6":"code","8c9a3b5c":"markdown","50852d01":"markdown","04740a64":"markdown","b2e0d48b":"markdown","4309b752":"markdown","3abf98ce":"markdown","ac3b66d4":"markdown","c6a5aa0f":"markdown","69c78fc7":"markdown","08683073":"markdown","54a2ac60":"markdown","1207ac10":"markdown","391b866e":"markdown","7a6940d2":"markdown","24dbc0ce":"markdown","d381bce1":"markdown","854bb2be":"markdown","8f2c175d":"markdown","fbdf5d59":"markdown","878e79b1":"markdown","6d51aa65":"markdown","13ea7420":"markdown","c80bc6a8":"markdown","bddd7d25":"markdown","392a33a2":"markdown","b779e186":"markdown","ff8425ad":"markdown","0d5f9ebf":"markdown","6d93ebfe":"markdown","9cd2794c":"markdown","3abd262a":"markdown","c93b80d4":"markdown","9a3bc8dd":"markdown","c2b1f0b1":"markdown","46834c8c":"markdown","2554a493":"markdown","59b69bad":"markdown","0dd15c15":"markdown","e35bb8b6":"markdown","bfda6bae":"markdown","2ca75f42":"markdown","8a27a8db":"markdown","8e48887a":"markdown","7357ec9a":"markdown","e9747ca6":"markdown","ca71f91d":"markdown"},"source":{"fe5e6e1c":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\nstop_words=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n\nimport warnings\nwarnings.simplefilter('ignore')","43e7574d":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","657bc6b9":"train.head()","88afd801":"test.head()","1dce9ef1":"sns.countplot(x = 'target', data=train)","c49a42be":"train['num_words'] = train['text'].apply(lambda x: len(str(x).split()))","5970c476":"train.head()","7bfc5c88":"sns.distplot(x=train[train['target'] == 0]['num_words'], label='Not Disaster')\nsns.distplot(x=train[train['target'] == 1]['num_words'], label='Disaster')\nplt.legend()","fce54638":"train['avg_len_words'] = train['text'].apply(lambda x: str(x).split())\ntrain['avg_len_words'] = train['avg_len_words'].apply(lambda x: [len(word) for word in x])","8dcb0338":"train['avg_len_words'] = train['avg_len_words'].apply(lambda x: np.mean(x))","397c9fd9":"sns.distplot(x=train[train['target'] == 0]['avg_len_words'], label='Not Disaster')\nsns.distplot(x=train[train['target'] == 1]['avg_len_words'], label='Disaster')\nplt.legend()","e58438ee":"train['length'] = train['text'].apply(lambda x: len(x))","af9b14b0":"plt.figure(figsize=(15, 6))\nsns.distplot(x=train[train['target'] == 1]['length'], label='Disaster')\nsns.distplot(x=train[train['target'] == 0]['length'], label='Not Disaster')\nplt.legend()","bafea1b2":"def create_corpus(target):\n    corpus = []\n    for words in train[train['target'] == target]['text'].str.split():\n        for word in words:\n            corpus.append(word)\n    return corpus","6e21a23c":"corpus = create_corpus(0)\n\ndic = defaultdict(int)\nfor word in corpus:\n    if word in stop_words:\n        dic[word] += 1\n\ntop = sorted(dic.items(), key = lambda x: x[1], reverse=True)[:10]","d7d36133":"np.array(stop_words)","f123f95e":"plt.figure(figsize=(10, 5))\nx, y = zip(*top)\nplt.bar(x, y, color='green')\nplt.title('Top 10 stop words in Non-Disaster Tweets', fontsize=20)","153121fd":"corpus1 = create_corpus(1)\n\ndic1 = defaultdict(int)\nfor word in corpus1:\n    if word in stop_words:\n        dic1[word] += 1\n\ntop1 = sorted(dic1.items(), key = lambda x: x[1], reverse=True)[:10]","ed610d90":"plt.figure(figsize=(10, 5))\nx, y = zip(*top1)\nplt.bar(x, y, color='red')\nplt.title('Top 10 stop words in Disaster Tweets', fontsize=20)","20a8c4de":"dic = defaultdict(int)\nfor word in corpus:\n    if word in string.punctuation:\n        dic[word] += 1\n        \ntop = sorted(dic.items(), key = lambda x: x[1], reverse=True)\n\nplt.figure(figsize=(10, 5))\nx, y = zip(*top)\nplt.bar(x, y, color='green')\nplt.title('Punctuation in Non Disaster Tweets', fontsize=20)","35bfec8a":"dic = defaultdict(int)\nfor word in corpus1:\n    if word in string.punctuation:\n        dic[word] += 1\n        \ntop = sorted(dic.items(), key = lambda x: x[1], reverse=True)\n\nplt.figure(figsize=(10, 5))\nx, y = zip(*top)\nplt.bar(x, y, color='red')\nplt.title('Punctuation in Disaster Tweets', fontsize=20)","33f1495e":"counter = Counter(corpus)\n\ncommon_words = counter.most_common()\nx = []\ny = []\nfor word, count in common_words[:40]:\n    if word not in stop_words:\n        if word not in string.punctuation:\n            x.append(word)\n            y.append(count)\n\nplt.figure(figsize=(8, 8))\nsns.barplot(x=y, y=x)\nplt.title('Common words in Non Disaster tweets', fontsize=20)","4f528d94":"df = pd.concat([train, test])","876a0bf2":"example=\"New competition launched :https:\/\/www.kaggle.com\/c\/nlp-getting-started\"","56a78f57":"def remove_urls(text):\n    '''Removes url from the tweets'''\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)\nremove_urls(example)","29d53cfd":"df['text'] = df['text'].apply(lambda x: remove_urls(x))","fa0ff126":"example = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"","85896cb2":"def remove_html(text):\n    '''Removes html from the tweets'''\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', text)\n\nprint(remove_html(example))","15590751":"df['text'] = df['text'].apply(lambda x: remove_html(x))","76176c29":"def remove_emojis(text):\n    '''Removes emojis from the tweets'''\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emojis('Omg another Earthquake \ud83d\ude14\ud83d\ude14')","5a79a19b":"df['text'] = df['text'].apply(lambda x: remove_emojis(x))","093e9c4d":"def remove_punct(text):\n    '''Removes punctuations from the tweets'''\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\nexample=\"I am ,.a #king\"\nprint(remove_punct(example))","85223b38":"df['text'] = df['text'].apply(lambda x: remove_punct(x))","7183044d":"def get_top_ngrams(corpus,n_grams, n=None):\n    vec = CountVectorizer(ngram_range=(n_grams,n_grams)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n    \n    return words_freq[:n]","61c5a1f0":"plt.figure(figsize=(10, 10))\n\ntop_tweet_ngrams = get_top_ngrams(df['text'], 2, 10)\nx, y = map(list, zip(*top_tweet_ngrams))\nsns.barplot(x=y, y=x)","a3328366":"plt.figure(figsize=(10, 10))\n\ntop_tweet_ngrams = get_top_ngrams(df[df['target'] == 0]['text'], 3, 10)\nx, y = map(list, zip(*top_tweet_ngrams))\nsns.barplot(x=y, y=x)","b65c189d":"plt.figure(figsize=(10, 10))\n\ntop_tweet_ngrams = get_top_ngrams(df[df['target'] == 1]['text'], 3, 10)\nx, y = map(list, zip(*top_tweet_ngrams))\nsns.barplot(x=y, y=x)","cc10af57":"from wordcloud import WordCloud","d4e7e0f9":"def create_corpus_df(df, target):\n    corpus = []\n    for words in df[df['target'] == target]['text'].str.split():\n        for word in words:\n            corpus.append(word)\n    return corpus","fd5cea12":"corpus_new0 = create_corpus_df(df, 0)\nlen(corpus_new0)","bc4a950e":"corpus_new0[:10]","56acfabc":"\n# Generating the wordcloud with the values under the category dataframe\nplt.figure(figsize=(12, 8))\nwordcloud = WordCloud(background_color='black', max_font_size=80).generate(\" \".join(corpus_new0[:50]))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","330304e6":"corpus_new1 = create_corpus_df(df, 1)","021d2b4c":"corpus_new1[:10]","bc0e34ea":"plt.figure(figsize=(12, 8))\nwordcloud = WordCloud(background_color='black', max_font_size=80).generate(\" \".join(corpus_new1[:50]))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","6c516c3d":"df.head(10)","428c54ca":"def cv(data):\n    count_vectorizer = CountVectorizer()\n    emb = count_vectorizer.fit_transform(data)\n    return emb, count_vectorizer\n\nlist_corpus = df['text'].tolist()\nlist_labels = df['target'].tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2,\n                                                    random_state=1)\n\nX_train_counts, count_vectorizer = cv(X_train)\nX_test_counts = count_vectorizer.transform(X_test)","1078a4f2":"def plot_LSA(test_data, test_labels, savepath='PCA_demp.csv', plot=True):\n    lsa = TruncatedSVD(n_components=2)\n    lsa.fit(test_data)\n    lsa_scores = lsa.transform(test_data)\n    color_mapper = {label:idx for idx, label in enumerate(set(test_labels))}\n    color_column = [color_mapper[label] for label in test_labels]\n    colors = ['orange', 'blue']\n    if plot:\n        plt.scatter(lsa_scores[:, 0], lsa_scores[:, 1], s=8, alpha=0.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n        orange_patch = mpatches.Patch(color='orange', label='Non Disaster')\n        blue_patch = mpatches.Patch(color='blue', label='Disaster')\n        plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})\n        \nfig = plt.figure(figsize=(16, 16))\nplot_LSA(X_train_counts, y_train)\nplt.show()","284e0035":"def tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer()\n    \n    train = tfidf_vectorizer.fit_transform(data)\n    \n    return train, tfidf_vectorizer\n\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)","cf786712":"fig = plt.figure(figsize=(16,16))\nplot_LSA(X_train_tfidf, y_train)\nplt.show()","ac8c21b1":"def create_corpus_new(df):\n    corpus = []\n    for text in tqdm(df['text']):\n        words = [word.lower() for word in word_tokenize(text)]\n        corpus.append(words)\n    return corpus","3017f48e":"corpus = create_corpus_new(df)","26b70778":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding_dict[word] = vectors\nf.close()","eded9401":"Max_len = 50\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences = tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad = pad_sequences(sequences, maxlen=Max_len, truncating='post', padding='post')","6038fad4":"word_index = tokenizer_obj.word_index\nprint('Number of unique words : ', len(word_index))","7bd976eb":"num_words = len(word_index) + 1\nembedding_matrix = np.zeros((num_words, 100))\n\nfor word, i in tqdm(word_index.items()):\n    if i < num_words:\n        emb_vec = embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i] = emb_vec","5e834751":"tweet_pad[0][0:]","e223f2d0":"model = Sequential()\n\nembedding = Embedding(num_words, 100, embeddings_initializer=Constant(embedding_matrix), input_length=Max_len, trainable=False)\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\noptimizer = Adam(learning_rate=3e-4)\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])","e002ddd3":"model.summary()","9255566c":"train1 = tweet_pad[:train.shape[0]]\ntest1 = tweet_pad[train.shape[0]:]","3c7d169e":"X_train, X_test, y_train, y_test = train_test_split(train1, train['target'].values, test_size=0.2)\nprint('Shape of train', X_train.shape)\nprint('Shape of validation', X_test.shape)","6d8280ee":"train.head()","78b43f50":"fig = plt.figure(figsize=(16, 16))\nplot_LSA(train1, train['target'])\nplt.show()","b55a6a64":"history = model.fit(X_train, y_train, batch_size=4, epochs=10,\n                    validation_data=(X_test, y_test), verbose=2)","a82c9c11":"train_pred_Glove = model.predict(train1)\ntrain_pred_Glove_int = train_pred_Glove.round().astype('int')","b18ed027":"def plot_cm(y_true, y_pred, title, figsize=(5, 5)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n                \n            elif c == 0:\n                annot[i, j] = ''\n                \n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n                \n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap='YlGnBu', annot=annot, fmt='', ax=ax)","347e90b6":"plot_cm(train_pred_Glove_int, train['target'].values, 'Confusion matrix for Glove model', figsize=(7, 7))","8c9a3b5c":"So the class is little biased towards the Disaster tweets","50852d01":"## NLP:\n* EDA (with WordCloud)\n* Bag of Words\n* TF IDF\n* GloVe\n* PCA visualization for the main models\n* Showing Confusion Matrices for GloVe","04740a64":"Reference - https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert","b2e0d48b":"## TF IDF vectorizer","4309b752":"## Calculate number of words in texts\n","3abf98ce":"so here we can see both categories are in the normalized form","ac3b66d4":"## Common words in tweets","c6a5aa0f":"### Disaster tweets","69c78fc7":"Now we can see the clear difference in words of both disaster and non diaster tweets ","08683073":"## Non-Disaster Tweets","54a2ac60":"## Analyzing Punctuations","1207ac10":"In both of them,\"the\" dominates which is followed by \"a\" in class 0 and \"in\" in class 1.","391b866e":"## Create function for Diaster and Non-Disaster tweet words","7a6940d2":"## Number of character's in the tweet","24dbc0ce":"## Plot Confusion matrix","d381bce1":"### for class 1","854bb2be":"### Let's see tri-gram analysis for diaster and non-disaster tweets","8f2c175d":"### Create embedding dictionary to store vecotors","fbdf5d59":"## N-grams analysis","878e79b1":"## WordCloud","6d51aa65":"# Disaster or Not Disaster Tweets","13ea7420":"## Now create corpus for class 1","c80bc6a8":"## Tokenize the text using Tokenizer()","bddd7d25":"## Fit the model","392a33a2":"## let's calculate average length of words","b779e186":"The distribution of both seems to be almost same.120 to 140 characters in a tweet are the most common among both.","ff8425ad":"## Import necessary Libraries","0d5f9ebf":"### Bigram analysis","6d93ebfe":"### for class 0","9cd2794c":"## Plot number of words to check","3abd262a":"## Load data","c93b80d4":" ### Let's First clean tweets","9a3bc8dd":"## About Data\nEach sample in the train and test set has the following information:\n\n* The text of a tweet\n* A keyword from that tweet\n* The location the tweet was sent from","c2b1f0b1":"Lot of cleaning needed !","46834c8c":"## Baseline Model with GloVe results","2554a493":"Here we will use GloVe pretrained corpus model to represent our words. It is available in 3 varieties : 50D, 100D and 200 Dimentional. We will try 100D here.","59b69bad":"## Class Distribution\nlet's check the class distribution","0dd15c15":"## Split the data into train and test","e35bb8b6":"## Create embedding matrix of words","bfda6bae":"### First we will analyze tweets with class 0","2ca75f42":"## GloVe Model\nGloVe method is built on an important idea,\n  \n  **\"You can derive semantic relationships between words   from the co-occurrence matrix.\"**\n  \nGiven a corpus having V words, the co-occurrence matrix X will be a V x V matrix, where the i th row and j th column of X, X_ij denotes how many times word i has co-occurred with word j. An example co-occurrence matrix might look as follow.\n\n![Glove-matrix](https:\/\/miro.medium.com\/max\/347\/1*QWcK8CIDs8kMkOwsOxvywA.png)","8a27a8db":"Read more about GloVe - https:\/\/towardsdatascience.com\/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010","8e48887a":"### Common stopwords","7357ec9a":"## Bag of Words counts","e9747ca6":"It's my first notebook on kaggle \nI hope you find this notebook useful and enjoyble\n\nYour comment and feedback are most welcome.","ca71f91d":"## Visualizing the Embeddings"}}