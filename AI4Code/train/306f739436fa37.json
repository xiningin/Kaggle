{"cell_type":{"b71b03bb":"code","5be531f8":"code","12253b5c":"code","47b8525d":"code","d2b210c0":"code","78538488":"code","c9d939f1":"code","f66122c5":"code","92ae0ea8":"code","96ecb7af":"code","f8560616":"code","5f0d205b":"code","4bb1bc9e":"code","4f29b5c3":"code","2881a6d4":"code","afc4f246":"code","f419c286":"code","81829945":"code","5dca71cc":"code","932e1989":"code","bd8a1cb4":"code","b0b525a9":"code","96f61db0":"markdown"},"source":{"b71b03bb":"# The requests library is the standard for making HTTP requests in Python.\n# BeautifulSoup is a Python package for parsing HTML and XML documents.\n# Pickle library saves data in another file, to use later.\nimport requests\nfrom bs4 import BeautifulSoup\nimport pickle\n\n","5be531f8":"# Scrapes transcript data from tbr.fun\/important-speeches-in-21st-century-america\/\ndef url_to_speeches(url):\n    '''Returns transcript data specifically from br.fun\/important-speeches-in-21st-century-america\/.'''\n    page = requests.get(url).text\n    soup = BeautifulSoup(page, \"lxml\")\n\n    all = iter(soup.find(class_=\"entry-content clearfix\").find_all('p')) # Get all paragraphs\n    next(all) # Skip the first one\n\n    text = [p.text for p in all]\n    print(url)\n    return text\n\n# URLs of transcripts in scope\nurls = ['https:\/\/www.tbr.fun\/michael-bloomberg-address-in-support-of-religious-tolerance-and-new-york-city-mosque\/',\n        'https:\/\/www.tbr.fun\/barack-obama-a-more-perfect-union\/',\n        'https:\/\/www.tbr.fun\/al-gore-2000-presidential-concession-speech\/',\n        'https:\/\/www.tbr.fun\/hillary-clinton-address-to-the-united-nations-commission-on-the-status-of-women\/',\n        'https:\/\/www.tbr.fun\/bill-clinton-farewell-address-to-the-nation\/',\n        'https:\/\/www.tbr.fun\/susan-collins-senate-floor-speech-in-support-of-brett-kavanaugh\/',\n        'https:\/\/www.tbr.fun\/rudy-giuliani-opening-remarks-to-the-united-nations-general-assembly\/',\n        'https:\/\/www.tbr.fun\/george-w-bush-first-official-presidential-state-of-the-union-address\/',\n        'https:\/\/www.tbr.fun\/condoleezza-rice-wriston-lecture-at-the-manhattan-institute\/',\n        'https:\/\/www.tbr.fun\/mitt-romney-faith-in-america\/' ]\n\n# Politician names\npoliticians = [ 'Bloomberg', 'Obama', 'Al Gore', 'H.Clinton', 'B.Clinton', 'Collins', 'Giuliani', 'Bush', 'Rice', 'Romney'] ","12253b5c":"speeches = [url_to_speeches(u) for u in urls]","47b8525d":"# # Make a new directory to hold the text files\n! mkdir speeches","d2b210c0":"# # Pickle files for later use\n\nfor i, p in enumerate(politicians):\n    with open(\"speeches\/\" + p + \".txt\", \"wb\") as file:\n        pickle.dump(speeches[i], file)","78538488":"# Load pickled files\ndata = {}\nfor i, p in enumerate(politicians):\n    with open(\"speeches\/\" + p + \".txt\", \"rb\") as file:\n        data[p] = pickle.load(file)","c9d939f1":"# Double check to make sure data has been loaded properly\ndata.keys()","f66122c5":"# More checks\ndata['Bush'][:7]","92ae0ea8":"# Let's take a look at our data again\n# data is our dictionary, keys are the politicians\nnext(iter(data.keys()))","96ecb7af":"# Notice that our dictionary is currently in key: politician, value: list of text format\nnext(iter(data.values()))\n","f8560616":"# We are going to change this to key: politician, value: string format\ndef combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combine_text = ' '.join(list_of_text)\n    return combine_text","5f0d205b":"# Combine it!\ndata_combined = {key: [combine_text(value)] for (key, value) in data.items()}","4bb1bc9e":"# We can either keep it in dictionary format or put it into a pandas dataframe\nimport pandas as pd\npd.set_option('max_colwidth',150)\n\ndata_df = pd.DataFrame.from_dict(data_combined).transpose()\ndata_df.columns = ['speeches']\ndata_df = data_df.sort_index()\ndata_df","4f29b5c3":"# Let's take a look at the speech of Obama\ndata_df.speeches.loc['Obama']","2881a6d4":"# Apply a first round of text cleaning techniques\n# re = regular expressions\n# string allows us to get the text punctuation\nimport re\nimport string\n\ndef clean_text_round1(text):\n    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('\\(.*?\\)', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('[\u2018\u2019\u201c\u201d\u2026]', '', text)\n    text = re.sub('\\n', '', text)\n    return text\n\nround1 = lambda x: clean_text_round1(x)","afc4f246":"# Let's take a look at the updated text\ndata_clean = pd.DataFrame(data_df.speeches.apply(round1))\ndata_clean","f419c286":"data_df\n\n","81829945":"# Let's add the politicians' full names as well\nfull_names= ['Al Gore Jr','Bill Clinton','Michael Bloomberg','George W Bush','Susan Collins', 'Rudy Giuliani','Hilary Clinton', 'Barack Obama','Condolezza Rice','Mitt Romney']\ndata_df['full_name'] = full_names\ndata_df","5dca71cc":"# Let's pickle it for later use\ndata_df.to_pickle(\"corpus.pkl\")","932e1989":"# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(stop_words='english')\ndata_cv = cv.fit_transform(data_clean.speeches)\ndata_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\ndata_dtm.index = data_clean.index\ndata_dtm\n","bd8a1cb4":"# Let's pickle it for later use\ndata_dtm.to_pickle(\"dtm.pkl\")","b0b525a9":"# Let's also pickle the cleaned data (before we put it in document-term matrix format) and the CountVectorizer object\ndata_clean.to_pickle('data_clean.pkl')\npickle.dump(cv, open(\"cv.pkl\", \"wb\"))","96f61db0":"# Organize the Data"}}