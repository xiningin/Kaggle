{"cell_type":{"85790c0a":"code","9fa4e190":"code","ab760488":"code","e013b6a4":"code","6fd5fe88":"code","f44fdbc8":"code","a8da05f0":"code","a8706cef":"code","9ef3fc28":"code","08bb72ef":"code","ed9f5664":"code","436c2605":"code","cabd3adf":"code","a7463e6c":"code","746296ea":"code","c154f11e":"code","16d5bb02":"code","0479ecf5":"markdown","dd2dc580":"markdown"},"source":{"85790c0a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn import model_selection, preprocessing, metrics\nimport warnings\nimport datetime\nwarnings.filterwarnings(\"ignore\")\nimport os\nprint(os.listdir(\"..\/input\"))","9fa4e190":"#Loading Train and Test Data\ndf_train = pd.read_csv(\"..\/input\/train.csv\", parse_dates=[\"first_active_month\"])\ndf_test = pd.read_csv(\"..\/input\/test.csv\", parse_dates=[\"first_active_month\"])\nprint(\"{} observations and {} features in train set.\".format(df_train.shape[0],df_train.shape[1]))\nprint(\"{} observations and {} features in test set.\".format(df_test.shape[0],df_test.shape[1]))","ab760488":"# Let's explore New and Old Merchants\ndf_new_trans = pd.read_csv(\"..\/input\/new_merchant_transactions.csv\")\ndf_hist_trans = pd.read_csv(\"..\/input\/historical_transactions.csv\")\n","e013b6a4":"df_h = df_hist_trans.groupby(\"card_id\").size().reset_index().rename({0:'transactions'},axis=1)\ndf_n = df_new_trans.groupby(\"card_id\").size().reset_index().rename({0:'transactions'},axis=1)","6fd5fe88":"print(\"Historic Transactions ---->   Average transactions per card : {:.0f}, Maximum transactions : {}.\\nNew Transactions ---->   Average transactions per card : {:.0f}, Maximum transactions : {}. \".format(df_h['transactions'].mean(),df_h['transactions'].max(),df_n['transactions'].mean(),df_n['transactions'].max()))","f44fdbc8":"#Comparting Historic Transactions vs New Transactions for Top 50 Merchants \nm_df_h = df_hist_trans.groupby(\"merchant_id\").size().reset_index().rename({0:'transactions'},axis=1)\nm_df_n = df_new_trans.groupby(\"merchant_id\").size().reset_index().rename({0:'transactions'},axis=1)\n\n","a8da05f0":"print(\"Historic Transactions ---->   Average transactions per merchant : {:.0f}, Maximum transactions : {}.\\nNew Transactions ---->   Average transactions per merchant : {:.0f}, Maximum transactions : {}. \".format(m_df_h['transactions'].mean(),m_df_h['transactions'].max(),m_df_n['transactions'].mean(),m_df_n['transactions'].max()))","a8706cef":"df_train[\"year\"] = df_train[\"first_active_month\"].dt.year\ndf_test[\"year\"] = df_test[\"first_active_month\"].dt.year\ndf_train[\"month\"] = df_train[\"first_active_month\"].dt.month\ndf_test[\"month\"] = df_test[\"first_active_month\"].dt.month\ndf_train['elapsed_time'] = (datetime.date(2018, 2, 1) - df_train['first_active_month'].dt.date).dt.days\ndf_test['elapsed_time'] = (datetime.date(2018, 2, 1) - df_test['first_active_month'].dt.date).dt.days\n\ndf_hist_trans['authorized_flag'] = df_hist_trans['authorized_flag'].map({'Y':1, 'N':0})\ndf_new_trans['authorized_flag'] = df_new_trans['authorized_flag'].map({'Y':1, 'N':0})\n#df_merch_trans = pd.concat([df_hist_trans,df_new_trans])","9ef3fc28":"def aggregate_hist_transactions(trans):\n    \n    trans.loc[:, 'purchase_date'] = pd.DatetimeIndex(trans['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max']\n        }\n    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n    agg_trans.columns = ['hist_' + '_'.join(col).strip() \n                           for col in agg_trans.columns.values]\n    agg_trans.reset_index(inplace=True)\n    \n    df = (trans.groupby('card_id')\n          .size()\n          .reset_index(name='transactions_count'))\n    \n    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n    \n    return agg_trans\n\ndef aggregate_new_transactions(trans):\n    \n    trans.loc[:, 'purchase_date'] = pd.DatetimeIndex(trans['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max']\n        }\n    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n    agg_trans.columns = ['new_' + '_'.join(col).strip() \n                           for col in agg_trans.columns.values]\n    agg_trans.reset_index(inplace=True)\n    \n    df = (trans.groupby('card_id')\n          .size()\n          .reset_index(name='transactions_count'))\n    \n    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n    \n    return agg_trans","08bb72ef":"merch_hist = aggregate_hist_transactions(df_hist_trans)\nmerch_new = aggregate_new_transactions(df_new_trans)","ed9f5664":"#Merging history with training and test data\ndf_train = pd.merge(df_train, merch_hist, on='card_id',how='left')\ndf_test = pd.merge(df_test, merch_hist, on='card_id',how='left')\n\ndf_train = pd.merge(df_train, merch_new, on='card_id',how='left')\ndf_test = pd.merge(df_test, merch_new, on='card_id',how='left')\n\ntarget = df_train['target']\n\nuse_cols  = [c for c in df_train.columns if c not in ['card_id', 'first_active_month', 'target']]\n\nfeatures = list(df_train[use_cols].columns)\ncategorical_features = [col for col in features if 'feature_' in col]","436c2605":"df_train = df_train[use_cols]\ndf_test = df_test[use_cols]","cabd3adf":"df_train.shape","a7463e6c":"df_test.shape","746296ea":"df_train['target'] = 0\ndf_test['target'] = 1","c154f11e":"train_test = pd.concat([df_train, df_test], axis =0)\n\ntarget = train_test['target'].values","16d5bb02":"param = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'binary',\n         'max_depth': 5,\n         'learning_rate': 0.001,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 17,\n         \"metric\": 'auc',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1}\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\noof = np.zeros(len(train_test))\n\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_test.values, target)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data = lgb.Dataset(train_test.iloc[trn_idx][features], label=target[trn_idx], categorical_feature=categorical_features)\n    val_data = lgb.Dataset(train_test.iloc[val_idx][features], label=target[val_idx], categorical_feature=categorical_features)\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 100)\n    oof[val_idx] = clf.predict(train_test.iloc[val_idx][features], num_iteration=clf.best_iteration)\n","0479ecf5":"We see that the oof AUC is **VERY** close to 0.5, so these two datasets seem very statistically similar. In other words, relying on your local validation should work very well for this competition. ","dd2dc580":"One of the most important things to establish for every Kaggle competition is whether there is a significatn differnece in distributions of the train and test sets. So far the CV validation scores for kernels and for public LB have been pretty close, but the local CV seems to be consistently about 0.01 better than the LB scores. It would be interesting, and potentially very valuable, to find out in a more quantitative and specific way how do these distributions compare. For that purpose we'll build an adverserial validation scheme - we'll run a CV classifier that tries to predict if any given question belongs to the train or the test set.\n\nFirts, we'll have to deal with data gregation adn building of the combined train and test datasets. This work has already been doen in many of the kernels, and in this kernel we'll realy on [Rahul Bamola](https:\/\/www.kaggle.com\/bamola)'s excelent [kernel](https:\/\/www.kaggle.com\/bamola\/elo-eda-and-modelling-lgbm)."}}