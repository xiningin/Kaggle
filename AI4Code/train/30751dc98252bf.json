{"cell_type":{"98bd97ed":"code","908b4a78":"code","0036ade8":"code","95ad868a":"code","fab8ad49":"code","4c40599e":"code","b56440f4":"code","d2f0441c":"code","37f77805":"code","6e7c6b8a":"code","61cde14c":"code","b15575d0":"code","434578cb":"code","2f614c5b":"code","27bcf0d4":"code","b133e293":"code","e3e29420":"code","a4ea1b55":"code","a9af6ea2":"markdown"},"source":{"98bd97ed":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom tqdm import tqdm\n\nimport pandas as pd\nimport numpy as np\nimport re\n\n\ntrain_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndel train_data['id']\n\ntrain_data['keyword'].fillna('none',inplace=True)\ntrain_data['location'].fillna('none',inplace=True)\n\ntest_data=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntest_data['keyword'].fillna('none',inplace=True)\ntest_data['location'].fillna('none',inplace=True)\n\ntrain_data=train_data.sample(frac=1)\ntrain_data.head()","908b4a78":"def refine_text(text):\n    text = re.sub(r'[^a-z\\s]',' ',re.sub(r'http\\S+', '',re.sub(r'#','',text)).lower())\n    return text\n\nfor col in ['keyword', 'location', 'text']:\n    train_data[col] = [refine_text(t) for t in train_data[col].values.tolist()]\n    test_data[col] = [refine_text(t) for t in test_data[col].values.tolist()]\n    \ntrain_data.head()","0036ade8":"from sklearn.model_selection import train_test_split\ntrain_data, val_data = train_test_split(train_data,\n                                        test_size=0.1,\n                                        shuffle=True)\nlen(train_data), len(val_data)","95ad868a":"import spacy\nspacy_nlp=spacy.load('en')\nfrom nltk.corpus import stopwords\nstopword_list=stopwords.words('english')\n\ndef tokenizer(text, MAX_LEN=20000):\n    text=re.sub(' +', ' ',\n                re.sub(r\"[\\*\\\"\u201c\u201d\\n\\\\\u2026\\+\\-\\\/\\=\\(\\)\u2018\u2022:\\[\\]\\|\u2019\\!;]\",\n                       \" \",text))\n    text= text if len(text)<=MAX_LEN else text[:MAX_LEN]\n    return [x.text for x in spacy_nlp.tokenizer(text) if (x.text!=' ') and (x.text not in stopword_list)]\n","fab8ad49":"from torchtext.vocab import Vectors, Vocab\nfrom collections import Counter\n\ngloveVectors=Vectors(name='..\/input\/glove6b\/glove.6B.100d.txt')\n\ncounter = Counter()\nfor i in train_data.index:\n    counter.update(tokenizer(train_data['text'][i]+' '+train_data['keyword'][i]+' '+train_data['location'][i]))\n#     counter.update(i.text+i.keyword+i.location)\n    \nvocabulary=Vocab(counter, max_size=20000, min_freq=2, vectors=gloveVectors, specials=['<pad>', '<unk>'])\n\nprint('Embedding vocab size: ', vocabulary.vectors.size(0))","4c40599e":"import torchtext\n\nclass ClassifyDataset(torchtext.data.Dataset):\n    def __init__(self, df, fields, train=True, **kwargs):\n        examples=[]\n        for i, row in df.iterrows():\n            examples.append(torchtext.data.Example.fromlist([row.text, row.target if train else None],\n                                                           fields))\n        super().__init__(examples, fields, **kwargs)\n        \n    @staticmethod\n    def sort_key(x):\n        return len(x.text)\n    \n    @classmethod\n    def splits(cls, fields, train_df=None, val_df=None, test_df=None, **kwargs):\n        train_data, val_data, test_data=(None, None, None)\n        \n        if train_df is not None:\n            train_data=cls(train_df.copy(), fields, **kwargs)\n            \n        if val_df is not None:\n            val_data=cls(val_df.copy(), fields, **kwargs)\n            \n        if test_df is not None:\n            test_data=cls(test_df.copy(), fields, train=False, **kwargs)\n            \n        return tuple(d for d in (train_data, val_data, test_data) if d is not None)","b56440f4":"import torch\n\nText=torchtext.data.Field(tokenizer, include_lengths=True)\nLabel=torchtext.data.LabelField(dtype=torch.float)\n\nfields= [('text', Text),('label', Label)]\n\ntrain_ds, val_ds, test_ds= ClassifyDataset.splits(fields, train_df=train_data, val_df=val_data, test_df=test_data)\n\n#sampling random example\nprint(vars(train_ds[61]), vars(val_ds[61]))","d2f0441c":"from torchtext.vocab import Vectors\n\ngloveVectors=Vectors(name='..\/input\/glove6b\/glove.6B.100d.txt')\n\nText.build_vocab(train_ds,\n                max_size=20000,\n                vectors=gloveVectors,\n                unk_init=torch.Tensor.zero_)\n\nLabel.build_vocab(train_ds)","37f77805":"batch_size=64\ndevice= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iterator, valid_iterator= torchtext.data.BucketIterator.splits(\n                                (train_ds, val_ds),\n                                batch_size=batch_size,\n                                sort_within_batch=True,\n                                device=device)","6e7c6b8a":"num_epochs=20\nlr=0.001\n\ninput_dims=len(Text.vocab)\nembedding_dims=100\nhidden_dims=256\noutput_dims=1\nn_layers=2\nbidirectional=True\ndrop=0.2\n\npad_idx=Text.vocab.stoi[Text.pad_token]","61cde14c":"def accuracy(preds, y):\n    rounded_preds=torch.round(torch.sigmoid(preds))\n    correct=(rounded_preds==y).float()\n    return correct.sum()\/len(correct)\n\nclass LSTMnn(torch.nn.Module):\n    def __init__(self, vocab_size, embedding_dims, hidden_dims, output_dims, n_layers, bidirectional,pad_idx, dropout):\n        super().__init__()\n        self.embeddings=torch.nn.Embedding(vocab_size, embedding_dims, padding_idx=pad_idx)\n        self.lstm=torch.nn.LSTM(embedding_dims, hidden_dims,\n                               num_layers=n_layers,\n                               bidirectional=bidirectional,\n                               dropout=dropout)\n        self.fc1=torch.nn.Linear(hidden_dims*2, hidden_dims)\n        self.fc2=torch.nn.Linear(hidden_dims, output_dims)\n        self.dropout=torch.nn.Dropout(dropout)\n        \n    def forward(self, text, text_lengths, train=True):\n        \n        #text and text_lengths : [seq_len, batch_size] and [batch_size]\n        embedding=self.embeddings(text) #[seq_len, batch_size, emb_size]\n        packed_embeddings=torch.nn.utils.rnn.pack_padded_sequence(embedding, text_lengths)\n        packed_out, (hidden, cell)=self.lstm(packed_embeddings)\n        #hidden:[num_layers*num_dir, batch size, hidden dims]\n        hidden=self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n        \n        output=self.fc1(hidden)\n        output=self.dropout(self.fc2(output))\n        \n        return output\n        \nmodel=LSTMnn(vocab_size=input_dims,embedding_dims=embedding_dims, \n             hidden_dims=hidden_dims,output_dims=output_dims,\n             n_layers=n_layers, bidirectional=bidirectional,\n             pad_idx=pad_idx, dropout=drop)\n\nmodel.embeddings.weight.data.copy_(Text.vocab.vectors)\n\nmodel.to(device)\n\ncriterion=torch.nn.BCEWithLogitsLoss()\noptimizer=torch.optim.Adam(model.parameters(), lr=lr)","b15575d0":"# for i in train_iterator:\n#     model(i.text[0], i.text[1], train=True)\n#     break\n# torch.Size([1, 4]) tensor([4]) torch.Size([1])\n# torch.Size([1, 4, 100]) torch.Size([1])","434578cb":"def train(model, iterator):\n    \n    epoch_loss=0\n    epoch_acc=0\n    \n    model.train()\n    for batch in iterator:\n        text, text_len=batch.text\n        \n        optimizer.zero_grad()\n        pred=model(text, text_len).squeeze(1)\n        \n        loss=criterion(pred, batch.label)\n        acc=accuracy(pred, batch.label)\n        \n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss+=loss.item()\n        epoch_acc+=acc.item()\n    \n    return (epoch_loss\/len(iterator), epoch_acc\/len(iterator))\n\ndef evaluate(model, iterator):\n\n    epoch_acc=0\n    \n    model.eval()\n    \n    with torch.no_grad():\n        for batch in iterator:\n            text, text_len=batch.text\n\n            pred=model(text, text_len).squeeze(1)\n\n            acc=accuracy(pred, batch.label)\n            \n            epoch_acc+=acc.item()\n    \n    return epoch_acc\/len(iterator)","2f614c5b":"import time\nt = time.time()\nloss=[]\nacc=[]\nval_acc=[]\n\nfor epoch in range(num_epochs):\n    \n    train_loss, train_acc = train(model, train_iterator)\n    valid_acc = evaluate(model, valid_iterator)\n    \n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Acc: {valid_acc*100:.2f}%')\n    \n    loss.append(train_loss)\n    acc.append(train_acc)\n    val_acc.append(valid_acc)\n    \nprint(f'time:{time.time()-t:.3f}')","27bcf0d4":"torch.save(model.state_dict(),'model.pt')","b133e293":"import matplotlib.pyplot as plt\n\nplt.plot(range(len(loss)), loss)\nplt.plot(range(len(acc)), acc)\nplt.plot(range(len(val_acc)), val_acc)","e3e29420":"def infer(text):\n    text_arr=[]\n    for i in tokenizer(text):\n        text_arr.append(Text.vocab.stoi[i])\n        \n    if len(text_arr)>0:\n        model.eval()\n        with torch.no_grad():\n            text=torch.LongTensor([text_arr]).view(-1,1)\n            text_len=torch.LongTensor([text.shape[1]])\n            return int(torch.round(torch.sigmoid(model(text, text_len).squeeze(1))).item())\n    else:\n        return 0\n\ntest_preds=[]\nfor i in test_data.iterrows():\n    test_preds.append(infer(i[1]['text']))","a4ea1b55":"my_submissions=pd.DataFrame({'id':test_data['id'].values,'target':test_preds})\nmy_submissions.to_csv('submission.csv', index=False)","a9af6ea2":"## Model - Simple LSTM"}}