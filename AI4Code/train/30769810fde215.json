{"cell_type":{"d5b4c97d":"code","dcd50750":"code","dbdf1263":"code","9705fb04":"code","9db61743":"code","2fd3293a":"code","31cb3c32":"code","ed725a0a":"code","7dc1b467":"code","62d5626c":"code","cae3d63c":"code","5ee56ff9":"code","cb7045c4":"code","09f33f8b":"code","62bbaad8":"code","ab819b9e":"code","c11eed18":"code","9a4ded55":"code","a731c68a":"code","35747c17":"code","d8f0b07a":"code","9e707026":"code","7e1332fc":"code","3c76251f":"code","c51e8d57":"code","9efcfe9b":"code","3857122e":"code","718a7e31":"code","0f32a421":"code","561e4ccc":"code","c2ac036b":"code","71d66dca":"code","7a35fb79":"code","ce6e0336":"code","224a4ebe":"code","20b41d7e":"code","8d757e42":"code","e0681991":"code","2bc7b718":"code","ab9b94fd":"markdown","3bb944ab":"markdown","6f72051d":"markdown","4cd936a3":"markdown","b56c09d8":"markdown","779ad9a4":"markdown","4b1ef3b1":"markdown","2b292630":"markdown","a7b59ae5":"markdown","d253b856":"markdown"},"source":{"d5b4c97d":"# Import the needed functions and tools\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\n\n\n# Input data\nX_data = np.array([[12, 5],\n                   [15, 4],\n                   [8, 5],\n                   [7, 7],\n                   [-1, 3],\n                   [-2, 8],\n                   [-8, 7],\n                   [-5, 10],\n                   [-2, 8],\n                   [-5, -4],\n                   [-4, -10],\n                   [-8, -6]])\n\ny_data = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2])","dcd50750":"# Data visualization - general\nplt.figure(figsize = (7, 5))\nplt.scatter(X_data[:, 0], X_data[:, 1], c = y_data, cmap = 'Accent')\n\nplt.title('Ex. 1. Input data')\nplt.xlabel('X_data - coordinate #1')\nplt.ylabel('X_data - coordinate #2')\n\nplt.show()","dbdf1263":"# Division of the data set into a test and training set\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.2)\n\n# Creating a model and setting up a training set\nknn = KNeighborsClassifier(n_neighbors = 1).fit(X_train, y_train)\n\n# Predicting the results for the test set\ny_pred = knn.predict(X_test)\n\n# Checking the accuracy of the model\nacc = metrics.accuracy_score(y_test, y_pred)","9705fb04":"# Data visualization - division into a test and training set\nfig = plt.figure(figsize = (15, 5))\n\nplt.subplot(121)\nplt.scatter(X_train[:, 0], X_train[:, 1], c = y_train, cmap = 'Accent')\nplt.title('Ex. 1. Training data set')\nplt.xlabel('X_train - coordinate #1')\nplt.ylabel('X_train - coordinate #2')\nplt.xlim(-9, 16)\nplt.ylim(-11, 11)\n\nplt.subplot(122)\nplt.scatter(X_test[:, 0], X_test[:, 1], c = 'red', s = 150, marker = '$?$')\nplt.title('Ex. 1. Test data set')\nplt.xlabel('X_test - coordinate #1')\nplt.ylabel('X_test - coordinate #2')\nplt.xlim(-9, 16)\nplt.ylim(-11, 11)\n\nplt.subplots_adjust(wspace = 0.3)\nplt.show()","9db61743":"# Obtained results and accuracy\npd.DataFrame({'X_test - coordinate #1': X_test[:, 0], 'X_test - coordinate #2': X_test[:, 1], 'Primary category (y_test)': y_test, 'Category predicted by the model (y_pred)': y_pred})","2fd3293a":"print(f'The accuracy of the classification of points from the test set is {acc * 100 :.1f}%.')","31cb3c32":"# Import the needed functions and tools\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport numpy as np\nimport pandas as pd\n\n# Input data\nX_data = np.array([[2, 10, 1],\n                   [4, 11, 2],\n                   [3, 12, 1],\n                   [4, 9, 3],\n                   [-3, -10, 10],\n                   [-3, -11, 9],\n                   [-4, -12, 8],\n                   [-5, -9, 11],\n                   [-3, 10, 1],\n                   [-3, 11, 2],\n                   [-4, 9, 3],\n                   [-5, 12, 1]])\n\ny_data = np.array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2])","ed725a0a":"# Data visualization - general\nfig = plt.figure(figsize = (9, 7), facecolor = 'white')\nax = fig.add_subplot(projection = '3d')\n\nax.scatter(X_data[:, 0], X_data[:, 1], X_data[:, 2], c = y_data, s = 40, cmap = 'Accent')\nax.set(xlabel = 'X_data - coordinate #1', ylabel = 'X_data - coordinate #2', zlabel = 'X_data - coordinate #3', title = 'Ex. 2. Input data', facecolor = 'white')\nplt.show()","7dc1b467":"# Division of the data set into a test and training set\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.2)\n\n# Creating a model and setting up a training set\nnb = BernoulliNB().fit(X_train, y_train)\n\n# Predicting the results for the test set\ny_pred = nb.predict(X_test)\n\n# Checking the accuracy of the model\nacc = metrics.accuracy_score(y_test, y_pred)","62d5626c":"# Data visualization - division into a test and training set\nfig = plt.figure(figsize = (20, 7), facecolor = 'white')\n\nax1 = fig.add_subplot(1, 2, 1, projection = '3d')\nax1.scatter(X_train[:, 0], X_train[:, 1], X_train[:, 2], c = y_train, s = 40, cmap = 'Accent')\nax1.set(xlabel = 'X_train - coordinate #1', ylabel = 'X_train - coordinate #2', zlabel = 'X_train - coordinate #3', title = 'Ex. 2. Training data set', facecolor = 'white')\nax1.set_xlim,(-6, 5)\nax1.set_ylim(-10, 13)\nax1.set_zlim(-0, 11)\n\nax2 = fig.add_subplot(1, 2, 2, projection = '3d')\nax2.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c = 'red', s = 150, marker = '$?$')\nax2.set(xlabel = 'X_test - coordinate #1', ylabel = 'X_test - coordinate #2', zlabel = 'X_test - coordinate #3', title = 'Ex. 2. Test data set', facecolor = 'white')\nax2.set_xlim,(-6, 5)\nax2.set_ylim(-10, 13)\nax2.set_zlim(-0, 11)\n\nplt.show()","cae3d63c":"# Obtained results and accuracy\npd.DataFrame({'X_test - coordinate #1': X_test[:, 0], 'X_test - coordinate #2': X_test[:, 1], 'X_test - coordinate #3': X_test[:, 2], 'Primary category (y_test)': y_test, 'Category predicted by the model (y_pred)': y_pred})","5ee56ff9":"print(f'The accuracy of the classification of points from the test set is {acc * 100 :.1f}%.')","cb7045c4":"# Import the needed functions and tools\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nimport numpy as np\nimport pandas as pd\n\n# Input data\n# The number 1 in the matrix \/ vector forms the shape of the corresponding number \nX_data = np.array([[1,1,1,1,1,\n                    1,0,0,0,1,\n                    1,0,0,0,1,\n                    1,0,0,0,1,\n                    1,1,1,1,1], # number 0, version I\n                   \n                   [0,1,1,1,0,\n                    1,0,0,0,1,\n                    1,0,0,0,1,\n                    1,0,0,0,1,\n                    0,1,1,1,0], # number 0, version II\n                   \n                   [0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0], # number 1, version I\n                   [0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0], # number 1, version II\n                   [0,1,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0], # number 1, version III\n                   \n                   [1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1], # number 2, version I\n                   [1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,1,0,0,0,0,1,1,1,1,1], # number 2, version II\n                   [1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,1,0,0,0,0,0,1,1,1,1], # number 2, version III\n                   \n                   [1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1], # number 3, version I\n                   [1,1,1,1,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,0], # number 3, version II\n                   [1,1,1,1,0,0,0,0,0,1,0,1,1,1,0,0,0,0,0,1,1,1,1,1,0], # number 3, version III\n                   \n                   [1,0,0,0,1,1,0,0,0,1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,1], # number 4, version I\n                   [1,0,0,0,1,1,0,0,0,1,0,1,1,1,1,0,0,0,0,1,0,0,0,0,1], # number 4, version II\n                   [0,0,1,0,1,0,1,0,0,1,1,1,1,1,1,0,0,0,0,1,0,0,0,0,1], # number 4, version III\n                   \n                   [1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1], # number 5, version I\n                   [1,1,1,1,1,1,0,0,0,0,1,1,1,1,0,0,0,0,0,1,1,1,1,1,0], # number 5, version II\n                   [0,1,1,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0,0,1,1,1,1,1,0], # number 5, version III\n                   \n                   [1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1], # number 6, version I\n                   [0,1,1,1,1,1,0,0,0,0,1,1,1,1,0,1,0,0,0,1,0,1,1,1,0], # number 6, version II\n                   [0,1,1,1,1,1,0,0,0,0,1,1,1,1,0,1,0,0,0,1,1,1,1,1,0], # number 6, version III\n                   \n                   [1,1,1,1,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1], # number 7, version I\n                   [0,1,1,1,1,0,0,0,0,1,0,0,0,1,1,0,0,0,0,1,0,0,0,0,1], # number 7, version II\n                   [1,1,1,1,1,0,0,0,0,1,0,0,1,1,1,0,0,0,0,1,0,0,0,0,1], # number 7, version III\n                   \n                   [1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1], # number 8, version I\n                   [0,1,1,1,0,1,0,0,0,1,0,1,1,1,0,1,0,0,0,1,0,1,1,1,0], # number 8, version II\n                   [1,1,1,1,1,1,0,0,0,1,0,1,1,1,0,1,0,0,0,1,1,1,1,1,1], # number 8, version III\n                   \n                   [1,1,1,1,1,\n                    1,0,0,0,1,\n                    1,1,1,1,1,\n                    0,0,0,0,1,\n                    1,1,1,1,1], # number 9, version I\n                   \n                   [0,1,1,1,0,\n                    1,0,0,0,1,\n                    0,1,1,1,1,\n                    0,0,0,0,1,\n                    1,1,1,1,0], # number 9, version II\n\n                   [0,1,1,1,1,\n                    1,0,0,0,1,\n                    0,1,1,1,1,\n                    0,0,0,0,1,\n                    1,1,1,1,0]]) # number 9, version III\n\n\ny_data = np.array([0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9])\n\ny_data_info = np.array(['number 0, version I', 'number 0, version II',\n                        'number 1, version I', 'number 1, version II', 'number 1, version II',\n                        'number 2, version I', 'number 2, version II', 'number 2, version II',\n                        'number 3, version I', 'number 3, version II', 'number 3, version II',\n                        'number 4, version I', 'number 4, version II', 'number 4, version II',\n                        'number 5, version I', 'number 5, version II', 'number 5, version II',\n                        'number 6, version I', 'number 6, version II', 'number 6, version II',\n                        'number 7, version I', 'number 7, version II', 'number 7, version II',\n                        'number 8, version I', 'number 8, version II', 'number 8, version II',\n                        'number 9, version I', 'number 9, version II', 'number 9, version II'])","09f33f8b":"# Division of the data set into a test and training set\nX_train, X_test, y_train, y_test, y_train_info, y_test_info = train_test_split(X_data, y_data, y_data_info, test_size = 0.1)\n\n# Creating a model and setting up a training set\ntree = DecisionTreeClassifier().fit(X_train, y_train)\n\n# Predicting the results for the test set\ny_pred = tree.predict(X_test)\n\n# Checking the accuracy of the model\nacc = metrics.accuracy_score(y_test, y_pred)","62bbaad8":"# Obtained results and accuracy\npd.DataFrame({'Primary category (y_test)': y_test, 'Detailed information (y_test_info)': y_test_info, 'Category predicted by the model (y_pred)': y_pred})","ab819b9e":"print(f'The accuracy of the classification of data from the test set is {acc * 100 :.1f}%.')","c11eed18":"# Import the needed functions and tools\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport numpy as np\nimport pandas as pd\n\n# Input data\nXy_data = np.array([[0, 2300],\n                    [1, 2500],\n                    [2, 3200],\n                    [3, 3800],\n                    [4, 4200],\n                    [5, 4800],\n                    [6, 5000],\n                    [7, 5200],\n                    [8, 6000],\n                    [9, 6900]])\n\nX_data = Xy_data[:, 0].reshape((-1, 1))\ny_data = Xy_data[:, 1]","9a4ded55":"# Data visualization - general\nplt.figure(figsize = (7, 5))\nplt.scatter(X_data, y_data)\n\nplt.title('Ex. 4. Input data')\nplt.xlabel('X_data')\nplt.ylabel('y_data')\n\nplt.show()","a731c68a":"# Division of the data set into a test and training set\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.3)\n\n# Creating a model and setting up a training se\nlinreg = LinearRegression().fit(X_train, y_train)\n\n# Predicting the results for the test set\ny_pred = linreg.predict(X_test)\n\n# Absolute error computation\nmae = metrics.mean_absolute_error(y_test, y_pred)","35747c17":"# Data visualization - division into a test and training set\nfig = plt.figure(figsize = (15, 5))\n\nplt.subplot(121)\nplt.scatter(X_train, y_train)\nplt.title('Ex. 4. Training data set')\nplt.xlabel('X_train')\nplt.ylabel('y_train')\nplt.xlim(-0.5, 9.5)\nplt.ylim(0, 7500)\n\nplt.subplot(122)\nplt.scatter(X_test, y_test, c = 'red', s = 1000, marker = '$|$')\nplt.title('Ex. 4. Test data set')\nplt.xlabel('X_test')\nplt.ylabel('y_test')\nplt.xlim(-0.5, 9.5)\nplt.ylim(0, 7500)\n\nplt.subplots_adjust(wspace = 0.3)\nplt.show()","d8f0b07a":"# Obtained results and absolute error\npd.DataFrame({'Designation (X_test)': X_test[:, 0], 'Actual value (y_test)': y_test, 'Value predicted by the model (y_pred)': y_pred, 'Difference': abs(y_pred - y_test)})","9e707026":"print(f'The absolute error is {mae:.1f}')","7e1332fc":"# Data visualization - summary\ndef y(x):\n    a = linreg.coef_\n    b = linreg.intercept_\n    \n    return a*x + b\n\nx = np.linspace(0, 9, 100)\n\nfig = plt.figure(figsize = (7, 5))\nplt.plot(x, y(x), 'g-', label = 'Linear Regression')\nplt.plot(X_train, y_train, 'bo', label = 'Training data')\nplt.plot(X_test, y_test, 'ro', label = 'Test data')\n\nplt.legend(bbox_to_anchor = (1.05, 1), loc = 'upper left')\nplt.title('Ex. 4. Linear Regression - summary')\nplt.xlabel('X')\nplt.ylabel('y')\n\nplt.show()","3c76251f":"# Import the needed functions and tools\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport numpy as np\nimport pandas as pd\n\n# Input data\nXy_data = np.array([[0, 1300],\n                    [1, 2500],\n                    [2, 3200],\n                    [3, 3800],\n                    [4, 3200],\n                    [5, 3800],\n                    [6, 5500],\n                    [7, 6200],\n                    [8, 6000],\n                    [9, 5200]])\n\nX_data = Xy_data[:, 0].reshape((-1, 1))\ny_data = Xy_data[:, 1]","c51e8d57":"# Data visualization - general\nplt.figure(figsize = (7, 5))\nplt.scatter(X_data, y_data)\n\nplt.title('Ex. 5. Input data')\nplt.xlabel('X_data')\nplt.ylabel('y_data')\n\nplt.show()","9efcfe9b":"# Division of the data set into a test and training set\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.2)\n\n# Creating a model and setting up a training set\npoly = make_pipeline(PolynomialFeatures(degree = 4), LinearRegression()).fit(X_train, y_train)\n\n# Predicting the results for the test set\ny_pred = poly.predict(X_test)\n\n# Absolute error computation\nmae = metrics.mean_absolute_error(y_test, y_pred)","3857122e":"# Data visualization - division into a test and training set\nfig = plt.figure(figsize = (15, 5))\n\nplt.subplot(121)\nplt.scatter(X_train, y_train)\nplt.title('Ex. 5. Training data set')\nplt.xlabel('X_train')\nplt.ylabel('y_train')\nplt.xlim(-0.5, 9.5)\nplt.ylim(0, 6500)\n\nplt.subplot(122)\nplt.scatter(X_test, y_test, c = 'red', s = 1000, marker = '$|$')\nplt.title('Ex. 5. Test data set')\nplt.xlabel('X_test')\nplt.ylabel('y_test')\nplt.xlim(-0.5, 9.5)\nplt.ylim(0, 6500)\n\nplt.subplots_adjust(wspace = 0.3)\nplt.show()","718a7e31":"# Obtained results and absolute error\npd.DataFrame({'Designation (X_test)': X_test[:, 0], 'Actual value (y_test)': y_test, 'Value predicted by the model (y_pred)': y_pred, 'Difference': abs(y_pred - y_test)})","0f32a421":"print(f'The absolute error is {mae:.1f}')","561e4ccc":"# Data visualization - summary\ndef y(x):\n    return poly.predict(x.reshape(-1, 1))\n\nx = np.linspace(0, 9, 100)\n\nfig = plt.figure(figsize = (7, 5))\nplt.plot(x, y(x), 'g-', label = 'Polynominal Regression')\nplt.plot(X_train, y_train, 'bo', label = 'Training data')\nplt.plot(X_test, y_test, 'ro', label = 'Test data')\n\nplt.legend(bbox_to_anchor = (1.05, 1), loc = 'upper left')\nplt.title('Ex. 5. Polynomial Regression - summary')\nplt.xlabel('X')\nplt.ylabel('y')\n\nplt.show()","c2ac036b":"# Import the needed functions and tools\nfrom sklearn.cluster import KMeans\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport numpy as np\nimport pandas as pd\n\n# Input data\nX_data = np.array([[12, 5],\n                   [15, 4],\n                   [12, 3],\n                   [0, 0],\n                   [1, 1],\n                   [0, 1],\n                   [-10, -14],\n                   [-15, -10],\n                   [-14, -12],\n                   [-14, -13]])","71d66dca":"# Data visualization - general\nplt.figure(figsize = (7, 5))\nplt.scatter(X_data[:, 0], X_data[:, 1])\n\nplt.title('Ex. 6. Input data')\nplt.xlabel('X_data - coordinate #1')\nplt.ylabel('X_data - coordinate #2')\n\nplt.show()","7a35fb79":"# Creating a model and setting up a training set\nkmeans = KMeans(n_clusters = 3).fit(X_data)\n\n# Predicting the results \ny_pred = kmeans.predict(X_data)","ce6e0336":"# Obtained results\npd.DataFrame({'X_data - coordinate #1': X_data[:, 0], 'X_data - coordinate #2': X_data[:, 1], 'Category determined by the model (y_pred)': y_pred})","224a4ebe":"# Data visualization - summary\nlabel = kmeans.labels_\ncenters = kmeans.cluster_centers_\n\nplt.figure(figsize = (7, 5))\nplt.scatter(X_data[:, 0], X_data[:, 1], c = label, s = 50, cmap = 'plasma')\nplt.scatter(centers[:, 0], centers[:, 1], c = 'blue', s = 10000, alpha = 0.1)\n\nplt.title('Ex. 6. K-Means - summary')\nplt.xlabel('X_data - coordinate #1')\nplt.ylabel('X_data - coordinate #2')\n\nplt.show()","20b41d7e":"# Import the needed functions and tools\nfrom sklearn.svm import OneClassSVM\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\n\n\n# Input data\nX_data_background = np.random.randint(0, 100, size = (20, 2))\nX_data_compaction_1 = np.random.randint(60, 70, size = (40, 2))\nX_data_compaction_2 = np.random.randint(40, 50, size = (40, 2))\nX_data = np.concatenate((X_data_background, X_data_compaction_1, X_data_compaction_2))","8d757e42":"# Data visualization - general\nplt.figure(figsize = (7, 5))\nplt.scatter(X_data[:, 0], X_data[:, 1])\n\nplt.title('Ex. 7. Input data')\nplt.xlabel('X_data - coordinate #1')\nplt.ylabel('X_data - coordinate #2')\n\nplt.show()","e0681991":"# Creating a model and setting up a training set\nsvm = OneClassSVM().fit(X_data)","2bc7b718":"# Data visualization - summary\nplt.figure(figsize = (7, 5))\ndx_mesh, dy_mesh = np.meshgrid(np.linspace(0, 100, 500), np.linspace(0, 100, 500))\nz = svm.decision_function(np.c_[dx_mesh.ravel(), dy_mesh.ravel()])\nz = z.reshape(dx_mesh.shape)\n\nplt.title(\"Ex. 7. OneClassSVM: summary\")\nplt.contourf(dx_mesh, dy_mesh, z, cmap = 'Greens')\nplt.scatter(X_data[:, 0], X_data[:, 1], c = '#d6d6d6', edgecolors = 'gray')\nplt.show()","ab9b94fd":"---\n### **Ex. 1: Nearest Neighbors Algorithm**\nMachine Learning > Supervised Learning > Classification > K-NN\n\n> Take a look: https:\/\/scikit-learn.org\/stable\/modules\/neighbors.html","3bb944ab":"---\n### **Ex. 2: Naive Bayes**\nMachine Learning > Supervised Learning > Classification > Naive Bayes\n\n> Take a look: https:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html","6f72051d":"---\n### **Ex. 6. K-Means**\nMachine Learning > Unsupervised Learning > Clustering > K-Means\n\n> Take a look: https:\/\/scikit-learn.org\/stable\/modules\/clustering.html","4cd936a3":"---\n### **Ex. 3: Decision Tree**\nMachine Learning > Supervised Learning > Classification > Decision Tree\n\n> Take a look: https:\/\/scikit-learn.org\/stable\/modules\/tree.html","b56c09d8":"## **Examples**\n\nA short message:\n\n*Below you will find some examples of the use of selected machine learning algorithms. These examples are based on a very simple data set - in order to present the effects of using these algorithms in the clearest possible way. Additionally, the examples use popular and useful modules such as \"pandas\", \"numpy\" or \"matplotlib\" to show their basic practical applicatio.*\n\n*I hope that the following examples will clear the doubts of people who have just started their adventure with machine learning. A few simple, practical examples can teach you more than read a dozen pages on machine learning theory.*","779ad9a4":"**Thanks!**\nIf you have any comments or tips - share them in the comments.","4b1ef3b1":"---\n### **Ex. 4. Linear Regression**\nMachine Learning > Supervised Learning > Regression > Linear Regression\n\n> Take a look: https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html","2b292630":"## **Machine learning - a brief introduction**\n\nMachine learning (ML) is the study of computer algorithms that can improve automatically through experience and by the use of data. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so.\n\n#### **Approaches**\n\nMachine learning approaches are traditionally divided into three broad categories, depending on the nature of the \"signal\" or \"feedback\" available to the learning system:\n\n* **Supervised learning:** \nThe computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs.\n* **Unsupervised learning:** \nNo labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).\n* **Reinforcement learning:**\nA computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximize.\n\n![https:\/\/www.boozallen.com\/content\/boozallen\/en\/author-groups\/sig\/si-group\/insight\/blog\/how-do-machines-learn\/_jcr_content\/blog_infographic_parsys\/image_1690394832.img.png\/1523652550770.png](https:\/\/www.boozallen.com\/content\/boozallen\/en\/author-groups\/sig\/si-group\/insight\/blog\/how-do-machines-learn\/_jcr_content\/blog_infographic_parsys\/image_1690394832.img.png\/1523652550770.png)\n> Source: https:\/\/www.boozallen.com\/assets\/boozallen_site\/sig\/pdf\/infographic\/how-do-machines-learn.pdf","a7b59ae5":"---\n### **Ex. 5. Polynomial Regression**\nMachine Learning > Supervised Learning > Regression > Polynomial Regression\n\n> Take a look: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.PolynomialFeatures.html","d253b856":"---\n### **Ex. 7. OneClassSVM**\n*Machine Learning > Unsupervised Learning > Anomaly Detection > OneClassSVM*\n\n> Take a look: https:\/\/scikit-learn.org\/stable\/modules\/outlier_detection.html#outlier-detection"}}