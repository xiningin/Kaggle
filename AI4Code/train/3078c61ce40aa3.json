{"cell_type":{"66887796":"code","14dbccd0":"code","739378d4":"code","62077d7d":"code","520eb476":"code","1b2191f5":"code","034079da":"code","739191bf":"code","a718fcba":"code","4d4c6b5a":"code","45c3b596":"code","06259ef2":"code","d2c2c148":"code","fa4fe1bb":"code","a3c96cff":"code","cf6ab500":"code","66ea6314":"code","18d75eb1":"code","62c47702":"code","cfaa6b87":"code","4c73296e":"code","34a425d5":"code","c9dd4ce9":"code","7894b646":"code","7f49e650":"code","c00607ee":"code","1717584c":"code","bf3fae17":"code","7165778b":"code","92f46dd3":"code","fd62a67e":"code","1b256db6":"code","483bc8db":"code","926de849":"code","f89644bd":"code","4a3427a7":"code","ca144cd8":"code","ba8a1305":"code","81656f7b":"code","913c70b5":"markdown","e8e8001f":"markdown","4a8da7e0":"markdown","93ae505f":"markdown","ba3c7ee5":"markdown","d2927b99":"markdown","fff4a88a":"markdown","05c09e13":"markdown","1bb5bf70":"markdown","8e72486a":"markdown","87a1004e":"markdown","5416e380":"markdown","4779aa99":"markdown","f9137741":"markdown","a2b4ca28":"markdown","8584ab48":"markdown","1ccc7f8a":"markdown","cdac8e39":"markdown","c915d1af":"markdown","1b5044c7":"markdown","5686c226":"markdown","1dc54eb6":"markdown","e11e3a38":"markdown","c617a868":"markdown","217b30bc":"markdown","f8e871dd":"markdown","12dc6726":"markdown","d5ab36ad":"markdown","79211481":"markdown","dade1204":"markdown"},"source":{"66887796":"import torch as th \nimport torch.nn as nn\nimport torchvision \nfrom torchvision.transforms import Normalize, Compose, ToTensor\nfrom torchvision.datasets import MNIST\nimport matplotlib.pyplot as plt\nimport os \nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import save_image \nfrom IPython.display import Image","14dbccd0":"train_mnist = MNIST(\n    root=\"\/data\",\n    train=True, \n    download=True,\n    transform= Compose([ToTensor(),Normalize(mean=(0.5,),std=(0.5,))]))","739378d4":"img , label = train_mnist[0]\n#creating a fonction that denormalize an image so that we can visualize it. \ndef denorm(img):\n    img = (img + 1) \/ 2\n    return img.clamp(0,1)\nimg_denorm = denorm(img)\nplt.imshow(img_denorm[0],cmap=\"gray\")\nplt.show()\n","62077d7d":"batch_size=  100\ntrain_dl = DataLoader(train_mnist,batch_size,shuffle=True,)","520eb476":"for images,lables in train_dl: \n    print(lables)\n    print(images.shape)\n    plt.imshow(images[3][0],cmap=\"gray\")\n    break\n    ","1b2191f5":"# configure our device \ndevice = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\ndevice","034079da":"# we have images of 28x28, we are going to flattened the data to a vector of 784\nimage_size =  784\nhidden_size = 256\n","739191bf":"Descriminator = nn.Sequential(\n    nn.Linear(image_size,hidden_size),\n    nn.LeakyReLU(0.2),\n    nn.Linear(hidden_size,hidden_size),\n    nn.LeakyReLU(0.2),\n    nn.Linear(hidden_size,1),\n    nn.Sigmoid()\n)\ncnn_Descriminator = nn.Sequential(\n    nn.Conv2d(1,32,kernel_size=3,stride=2), # 28 * 28 * 1 ==>   13 * 13 * 32\n    nn.LeakyReLU(0.2),\n    nn.Conv2d(32,64,kernel_size=3,stride=2), # 13 * 13 * 32 == > 6 * 6 * 64 \n    nn.LeakyReLU(0.2),\n    nn.Flatten(),\n#     nn.Dropout(p=0.2),\n    nn.Linear(6 * 6 * 64,1),\n    nn.Sigmoid()\n)\n# cnn_Descriminator.to(device)","a718fcba":"Descriminator.to(device)\ncnn_Descriminator.to(device)","4d4c6b5a":"input_size = 64\n# for the linear model we are using the ReLU activation fonction \nGen = nn.Sequential(\n    nn.Linear(input_size , hidden_size),\n    nn.ReLU(), \n    nn.Linear(hidden_size , hidden_size ), \n    nn.ReLU(),\n    nn.Linear(hidden_size , image_size),\n    nn.Tanh(),\n)\n# for the cnn model we are using the LReLU activation fonction \n\ncnn_Gen = nn.Sequential(\n    nn.Conv2d(input_size, 32, kernel_size = 1 ,stride=2),  # 1 * 1 * 64 ==> 1 * 1* 32 \n    nn.ReLU(), \n    nn.Conv2d(32 , 512 , kernel_size=1 ,stride=2), # 1 * 1 * 32 ==> 1 * 1* 512 \n    nn.ReLU(),\n    nn.Flatten(),\n    nn.Linear(512,image_size),\n    nn.Tanh(),\n)","45c3b596":"#Defining the input data for the cnn based generator and the linear based gen\n\ninput_data_cnn = th.randn(batch_size , input_size,1,1)\nprint(input_data_cnn.shape)\ncnn_output = cnn_Gen(input_data_cnn) \nprint(input_data_cnn.shape)\ncnn_output  = cnn_output.reshape(-1,28,28)\ncnn_gen_image = denorm(cnn_output.detach())\n\ninput_data = th.randn(2,input_size)\n\noutput = Gen(input_data)\noutput  = output.reshape(-1,28,28)\ngen_image = denorm(output.detach())\n\n# .detach() : tell pytorch to get just the value from a tensor, and detach the tracking of \n# the gradient and the computations associated with this tensor ans alsor with the diffrent operations \n# that are happening to this tensor ","06259ef2":"plt.imshow(gen_image[0],cmap=\"gray\")","d2c2c148":"print(\"plotting the image generated from the cnn based Generator\")\nplt.imshow(cnn_gen_image[1],cmap=\"gray\")","fa4fe1bb":"print(\"plotting the image generated from the linear based Generator\")\nplt.imshow(gen_image[1],cmap=\"gray\")","a3c96cff":"Gen.to(device)\ncnn_Gen.to(device)","cf6ab500":"criterion = nn .BCELoss()\n\nd_optim = th.optim.Adam(Descriminator.parameters(),lr=0.0002)\ncnn_d_optim = th.optim.SGD(cnn_Descriminator.parameters(),lr=0.0001)\ng_optim = th.optim.Adam(Gen.parameters(),lr=0.0002)\ncnn_g_optim = th.optim.SGD(cnn_Gen.parameters(),lr=0.0001)","66ea6314":" def reset_grad(): \n    d_optim.zero_grad()\n    g_optim.zero_grad()\n    cnn_d_optim.zero_grad()\n    cnn_g_optim.zero_grad()\n# noting that in this case we are not interessted in the labels of the data but if it's real of fake\n\ndef train_discriminator(images,Des,Gen): \n    # as we are interessted if the images are fake or real so we will define 2 tensors : \n        # one that contains just Ones (many 1) which represent the real images that will be the input for the BCE fonction with the output of the descriminator  \n        # the other contains just Zeros (many 0) which represent the fake images that we will give to the BCE when computing the loss of the discriminator after generating some images from the generators \n    real_labels = th.ones(batch_size , 1).to(device)\n    fake_labels = th.zeros(batch_size, 1).to(device)\n    \n    # computing the loss for the real images (when giving just the real images to the descriminator)\n    outputs = Des(images)\n    # here we expect form the descriminator to output 1 for all the predictions \n    des_loss_real = criterion(outputs, real_labels)\n    real_score = outputs\n    \n    #computing the loss for the fake images that are generated from the Generator \n    inputs = th.randn(batch_size, input_size).to(device)\n    fake_images = Gen(inputs)\n    outputs = Des(fake_images)\n    # here we expect from the descriminator to expect 0 for all the inputs\n    des_loss_fake = criterion(outputs, fake_labels)\n    fake_score = outputs \n    \n    # combining the losses \n    d_loss = des_loss_real + des_loss_fake\n    # reset the gradient \n    reset_grad()\n    # compute the gradient\n    d_loss.backward()\n    \n    # adjust the weights using the backpropagation\n    d_optim.step()\n\n    return d_loss , real_score , fake_score , Des\n\n\ndef train_cnn_des(images,Des,Gen):\n    fake_labels = th.zeros(batch_size , 1).to(device)\n    real_labels = th.ones(batch_size , 1).to(device)\n    \n    output = Des(images)\n    real_loss = criterion(output,real_labels)\n    real_score = output\n    \n    inputs = th.randn(batch_size , input_size,1,1).to(device)\n    fake_images = Gen(inputs)\n    fake_images= fake_images.reshape(100,1,28,28)\n    output = Des(fake_images)\n    fake_loss = criterion(output,fake_labels)\n    fake_score = output\n    \n    \n    des_loss = real_loss + fake_loss \n    reset_grad()\n    des_loss.backward()\n    \n    cnn_d_optim.step()\n    \n    \n    \n    return des_loss, real_score , fake_score, Des\n    ","18d75eb1":"def train_generator(Des,Gen): \n    #So first we will generate images based on random vectors\n    inputs = th.randn(batch_size,input_size).to(device) \n    # inputs for the cnn based generator \n#     cnn_inputs = th.randn(input_size, 2 ,3,3).permute(1,0,2,3)\n    fake_images = Gen(inputs)\n    # then we are going to create labels for the BCE inputs , the labels will be all ones because \n    # our goal is to pushing the descriminator to predict all 1 (real images)for the generated images\n    # so if the outputs of the des are all Ones so our generator work perfectly and the loss needs to be smaller\n    labels = th.ones(batch_size, 1).to(device)\n    g_loss = criterion(Des(fake_images),labels) #here the generator try to generate images that fools the descriminator\n    # so the loss will be smaller if the descriminator predict that all the most of the images are reals\n    \n    #Backpropagation and optimization\n    reset_grad()\n    g_loss.backward()\n    g_optim.step()\n    \n    # in the backpropagation we adjust the gen weights (params) in a way that in the next fake images generation\n    # the discriminator will be giving values\/predictions closer to 1, the fake images will aprear more and more real to the descriminator\n    \n    \n    return g_loss,fake_images,Gen\n    \n    \n\ndef train_cnn_gen(Des,Gen): \n    inputs = th.randn(batch_size , input_size,1,1).to(device)\n    fake_images = Gen(inputs)\n    labels = th.ones(batch_size , 1).to(device)\n    fake_images= fake_images.reshape(100,1,28,28)\n    gen_loss = criterion(Des(fake_images), labels)\n    reset_grad()\n    gen_loss.backward()\n    cnn_g_optim.step()\n    return gen_loss, fake_images, Gen\n    ","62c47702":"sample_dir = \".\/samples_\" \nif not os.path.exists(sample_dir): \n    os.makedirs(sample_dir)\n","cfaa6b87":"#saving a batch of images in the dir so that we can compare the generated images with the real ones \n\nfor images,_ in train_dl : \n    # we can specify to the .save_image() the number of the images that will save, and it create a grid of images ans save it for us \n    save_image(denorm(images),os.path.join(sample_dir,\"real_images.png\"),nrow=10)\n    break\n\n","4c73296e":"Image(os.path.join(sample_dir,\"real_images.png\"))","34a425d5":"#creating a sample vectors to give them to the generator each epoch, this for observing the evolution #of the fake images generated from the generator.\ncnn_sample_vectors = th.randn(batch_size , input_size,1,1).to(device)\nsample_vectors =th.randn(batch_size , input_size).to(device)\ndef save_fake_images(indx,Gen,cnn=False):\n    if cnn :         \n        generated_images= Gen(cnn_sample_vectors)\n        \n    else : \n        generated_images= Gen(sample_vectors)\n\n    generated_images = generated_images.reshape(generated_images.size(0),1,28,28)\n    fake_file_name = \"fake_images_{0:0=4d}.png\".format(indx)\n    print(f\"saving {fake_file_name}\")\n    save_image(denorm(generated_images),os.path.join(sample_dir,fake_file_name),nrow=10)\n\nindx = 0\nsave_fake_images(indx,Gen,False)\n\nImage(os.path.join(sample_dir,f\"fake_images_000{indx}.png\"))","c9dd4ce9":"def train(Des,Gen,epochs,train_dl,cnn=False): \n    des_losses, gen_losses,fake_scores,real_scores  = [], [], [], []\n    total_steps = len(train_dl)\n    for epoch in range(1,epochs+1):\n        # as sad before we are not interessted to use the labels \n        for i ,(images,_) in enumerate(train_dl): \n            # Loading a batch to GPU and transform to vectors \n            images = images.reshape(batch_size,-1).to(device)\n            # Train  the descriminator and the generator\n            # we must train the descriminator then the generator because we are using the des in the training of the gen \n            if not cnn: \n                \n                d_loss, real_score , fake_score, Des  = train_discriminator(images,Des,Gen)\n                g_loss , fake_images, Gen = train_generator(Des,Gen)\n            else : \n                images = images.reshape(100,1,28,28)\n                d_loss , real_score , fake_score , Des = train_cnn_des(images,Des,Gen)\n                g_loss , fake_images , Gen = train_cnn_gen(Des,Gen)\n            # as we are going to use the cnn based des and gen so we are going to add the logic to train the cnn_gen\/des\n            \n            \n        \n            \n            # inspect the losses at each 200 itteration , remember that there are 600 batch of 100 images each one \n            # so for every 200 batch we will save the losses \n            if (i+1 )%200 == 0:\n                # saving the gen and des losses for the 200 batches \n                des_losses.append(d_loss.item())\n                gen_losses.append(g_loss.item())\n                # noting the the fake\/real_scores are the output of the des on fake\/real images \n                # so we are saving the avrg propability of the des for the real and fake images\n                fake_scores.append(fake_score.mean().item())\n                real_scores.append(real_score.mean().item())\n                print(f\"Epoch : {epoch}\/{epochs} Step : {i+1} \/ {total_steps} d_loss : {round(d_loss.item(),3)} g_loss : {round(g_loss.item(),3)} D(x) : {round(real_score.mean().item(),3)} D(G(input)) : {round(fake_score.mean().item(),3)}\")\n                \n                \n                \n        # saving the fake images for visualisation\n        save_fake_images(epoch,Gen,cnn=cnn)\n    return des_losses, gen_losses,fake_scores,real_scores , Des,Gen\n            ","7894b646":"%%time\nEPOCHS = 100\ndes_losses, gen_losses,fake_scores,real_scores , Descriminator,Gen  = train(Descriminator,Gen,EPOCHS,train_dl)","7f49e650":"Image(os.path.join(sample_dir,f\"fake_images_0001.png\"))","c00607ee":"Image(os.path.join(sample_dir,f\"fake_images_0008.png\"))","1717584c":"Image(os.path.join(sample_dir,f\"fake_images_0010.png\"))","bf3fae17":"# Image(os.path.join(sample_dir,f\"fake_images_100.png\"))\n","7165778b":"th.save(Gen.state_dict(),\"Gen.ckpt\")\nth.save(Descriminator.state_dict(),\"Des.ckpt\")","92f46dd3":"import cv2\nfrom IPython.display import FileLink\n\nvideo_file_name = 'Gan_Train_evol.avi'\n# loading all the savec images \nfiles = [os.path.join(sample_dir,f) for f in os.listdir(sample_dir) if \"fake_images\" in f]\nfiles.sort()\n# now we have all the files are sorted and saved in the files var \n# next we are going to combine them into 1 video using the cv2.VideoWriter() fonction \nout = cv2.VideoWriter(video_file_name,cv2.VideoWriter_fourcc(*\"MP4V\"),8,(302,302))\n# 4 param : Frame per second\n[out.write(cv2.imread(file_name)) for file_name in files]\nout.release()\nFileLink(video_file_name)\n","fd62a67e":"plt.plot(des_losses)\nplt.plot(gen_losses)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Des loss\",\"Gen loss\"])","1b256db6":"plt.plot(fake_scores)\nplt.plot(real_scores)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Scores\")\nplt.legend([\"Real scores\",\"Fake scores\"])","483bc8db":"%%time\nEPOCHS = 50\ncnn_des_losses, cnn_gen_losses,cnn_fake_scores,cnn_real_scores , cnn_Descriminator,cnn_Gen  = train(cnn_Descriminator,cnn_Gen,EPOCHS,train_dl,cnn=True)","926de849":"%%time\nEPOCHS = 50\ncnn_des_losses, cnn_gen_losses,cnn_fake_scores,cnn_real_scores , cnn_Descriminator,cnn_Gen  = train(cnn_Descriminator,cnn_Gen,EPOCHS,train_dl,cnn=True)","f89644bd":"Image(os.path.join(sample_dir,f\"fake_images_0010.png\"))","4a3427a7":"Image(os.path.join(sample_dir,f\"fake_images_0020.png\"))","ca144cd8":"Image(os.path.join(sample_dir,f\"fake_images_0010.png\"))","ba8a1305":"plt.plot(cnn_des_losses)\nplt.plot(cnn_gen_losses)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Des loss\",\"Gen loss\"])\n","81656f7b":"plt.plot(cnn_fake_scores)\nplt.plot(cnn_real_scores)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Scores\")\nplt.legend([\"Fake socre\",\"Rezl Score\"])\n","913c70b5":"We are changing the range of the data from [0,1] to [-1,1]","e8e8001f":"We are ready now to train the model, at each epoch we are going to train the descriminator firs and then the generator \n","4a8da7e0":"## Training the Discriminator","93ae505f":"The leaky relu is an activation fonction based on ReLU, but it has a small slope for negative values instead of a flat slope, the LeakyReLU instead of setting the negative values to 0 it put a small value based on a slope which get in param\n    The leaky-ReLU is popular for tasks like Training GANs ","ba3c7ee5":"## Importing the packages needed","d2927b99":"## Training the Generator","fff4a88a":"We observes that scores are likely good, but the fake images generated by the cnn_Generator are not similar to the real ones,\nand that maybe due to the choosen CNN architecture on the Generator Network.\nSo we will try to use some better architectures next time. ","05c09e13":"Now that we define the Generator and the Descriminator, we are going to output an image using the Gen and ploting it ","1bb5bf70":"In this step , we are going first to create a directory that will helps us to save a version of the output of the generator vor visualize it and analyse the progression of the model ","8e72486a":"## Using the CNN based Descriminator and Generator ","87a1004e":"## Saving the Model","5416e380":"In this notebook we are using the MNIST dataset to generate a handwritten images using GANs","4779aa99":"## Visualizing the results\n\nFor visualizing the evolution of the model we are going to create a video based on the fake images that we saved earlier, and that using OpenCv\n\nWe are getting a video that contains the evolutions of the images generated by the generator while traing ","f9137741":" We are going now to create some helper fonctions to reset the gradient and train the descriminator ","a2b4ca28":"We'll also define a helper fonction for saving some fake images at the end of every epoch and that to see the evolution of the generator , we are going to give the generator a fixed set of random vectors and this to see how each image envolve in time.","8584ab48":"as the descriminator is a binary classifyer so we are going to use the **binary cross entropy loss fonction** to quantify how well is able to deffrencied between real and generated images","1ccc7f8a":"## Training the Model","cdac8e39":"we see that the output from the generator is random noise and that because it's not trained yet.\n\nwe are going to create a helper fonction that save a batch of output of the models into a file \n","c915d1af":"We are using the TanH activation fonction that transforms the values in the range of [-1 , 1], that's why we have transforms our real data into that range when loading it.\n\nThe reason for using the TanH it's because it gives better results when training it with Generator net","1b5044c7":"Like the Descriminator, we are going to use first a feedforward with 3 linear layers and another with 2 conv2d layer with leakyReLU fonction activation  \n\nThe generator will be a model that has a vector of 64 values as an input and it will output a vector of 784 values, which is the generated image \n\nAs we now, we will give a random vector to the generator, and based on this vector the generator will generate some image, so the dimentions of the vector are called latent_size, and when training the generator this dimentions starts to take some importance and define \/ identified some feature about the image to generate.\n\nso along the training, the dimentions starts to learn and affects some parts of the generator image","5686c226":"## Generator Network","1dc54eb6":"_______________________________________________________________________________________","e11e3a38":"##### The GANs are devided into 2 models : \n1. **Generator** : it's the model that generate new samples of the domain, it get vector as input and generate new sample. The vector is choosen from a latent space (that is a projection of data distribution usaly *Gaussian*) \n2. **Descriminator** : it take a sample from the domain (real or generated from the generator) and predict if it's real or fake.\n\nThe generator generates a batch of samples, and these, along with real examples from the domain, are provided to the discriminator and classified as real or fake. The discriminator is then updated to get better at discriminating real and fake samples in the next round, and importantly, the generator is updated based on how well, or not, the generated samples fooled the discriminator.\n\nSo when the descriminator has good results in the prediction of the new samples (if they real or fake) we don't need to change it's parameters and weights, whereas the parameters of the generator needs to get a large update  \n\nWhen the generator generate some samples that are perfet and similaire to the real samples, then the descriminator can predict \"unsure\", this means that it predict 50% for fake  and 50% for real \n\nGANs works with image data, so they used the CNN for generates new samples and also for the descrimination and the prediction if it's real or fake\n\nAn important extension to the GAN is in their use for conditionally generating an output.\nThe generator and the descriminator can have some additional input\n\nfor the training we train the descriminator for a few epochs the we train the generator and we repeat this process\n\nThe training of GANs is very difficult and sensitive to the hyperparameters and the activation fonctions, and that because when the Descriminator is good (well trained) so the generator can not generate good images and the inverse is true","c617a868":"## Discriminator Network \n\nthe discriminator will get an image as input and try to predict if it's real or generated image, for this task we will use 2 solutions: \n1. first we will create a simple feed forward neural network with 3 linear layers and relue activation \n2. second we will use a cnn with 2 conv2d layers and 1 linear layer with the leaky relu activation fonction","217b30bc":"After observing the results that gives us the Linear based Descriminator and Generator, we are going now to use the CNN based models and train them.","f8e871dd":"## Visualizing the losses and scores","12dc6726":"We are going to use the descriminator for evaluating the predictions of the generator and this by giving the generated images to the descriminator and try to push him to predict that they are real images not fake and use this for computing the loss of the generator.\n\nthe core idea is that when passing the generated images into the Descriminator, the outputs needs to be all 1 , so that the generator is perfectly generating the images because the descrimination thaught that they are real images","d5ab36ad":"We are now visualising the data before starting to train the GAN","79211481":"As all the binary classification, the output of the descriminator will be a proba of the image being fake or real.","dade1204":"### Loading the data"}}