{"cell_type":{"11180e22":"code","8beebfb1":"code","7c8c148d":"code","2aa22da7":"code","66c6781b":"code","7c0bfd9c":"code","b2e3253d":"code","8734b2a4":"code","cc7f1cb9":"code","5ea7b663":"markdown","508f3d19":"markdown"},"source":{"11180e22":"import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.metrics import f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\n\nimport os","8beebfb1":"# \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30\ntrain = pd.read_csv('\/kaggle\/input\/kakr-4th-competition\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/kakr-4th-competition\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/kakr-4th-competition\/sample_submission.csv')","7c8c148d":"# 1) column \uc81c\uac70\ndef col_reduction(df):\n    df.drop(['id','fnlwgt','education','relationship','native_country','workclass'], axis=1, inplace=True)\n    \n    return df\n\n# 2) marital_status \uc870\uc815\ndef mar_st(df):\n    df['marital_status'] = (df['marital_status'] == 'Married-civ-spouse').astype(int)\n    \n    return df\n\n# 3) capital_gain, loss \uc870\uc815\ndef capital(df):\n    df['cap_gain_high'] = (df['capital_gain'] != 0).astype(int)\n    df['cap_loss_high'] = (df['capital_loss'] >= 1700).astype(int)\n    df['capital_gain'] = df['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    \n    return df\n\n# 4) age \uc870\uc815 \ud568\uc218\ndef age(df):\n    df.loc[df['age'] < 20, 'age_range'] = '~20'\n    df.loc[df['age'] >= 65, 'age_range'] = '~65'\n\n    down = 20\n    for i in range(45\/\/5):\n        df.loc[(df['age'] >= down) & (df['age'] < down+5), 'age_range'] = str(down)+'~'+str(down+5)\n        down += 5\n\n    df['age'] = df['age_range']\n    df.drop(['age_range'], axis=1, inplace=True)\n    \n    return df\n    \n# 5) One-hot encoding\uc740 \ub9cc\ub4e4\uc9c0 \uc54a\uc558\ub2e4.\n\n# 6) edu_num \uc0c8 \ubcc0\uc218 \ub9cc\ub4e4\uae30\ndef edu(df):\n    df['edu_num_high'] = (df['education_num'] >= 13).astype(int)\n    \n    return df\n\n# 7) hpw \uc0c8 \ubcc0\uc218 \ub9cc\ub4e4\uae30\n    \ndef hpw(df):\n    df['hpw_high'] = (df['hours_per_week'] >= 50).astype(int)\n\n    return df\n\n# 8) MinMaxScaler\ndef mm_feature(df, feature):\n    mm_scaler = MinMaxScaler()\n    \n    df[feature] = mm_scaler.fit_transform(df[feature].values.reshape(-1,1))\n    \n    return df, mm_scaler\n\n# 9) target \ubd84\ub9ac: train\uc740 \ud558\uace0, test\ub294 \uc548\ud558\ubbc0\ub85c \ub530\ub85c \ub9cc\ub4e4\uaca0\ub2e4.\ndef target_handle(df):\n    df['income'] = df['income_>50K']\n    df.drop(['income_>50K','income_<=50K'], axis=1, inplace=True)\n    \n    y_df = df.income\n    X_df = df.drop(['income'], axis=1, inplace=False)\n    \n    return X_df, y_df\n\ndef main(df):\n    \n    df1 = col_reduction(df)\n    df2 = mar_st(df1)\n    df3 = capital(df2)\n    df4 = age(df3)\n    \n    df5 = pd.get_dummies(df4)\n    \n    df6 = edu(df5)\n    df_fin = hpw(df6)\n    \n    return df_fin","2aa22da7":"# \uc801\uc6a9\n## main: 1) ~ 7)\ntrain = main(train)\nX_test = main(test)\n\n## 8) minmax scaler\ntrain, mm_scaler1 = mm_feature(train,'education_num')\ntrain, mm_scaler2 = mm_feature(train,'hours_per_week')\n\nX_test['education_num'] = mm_scaler1.transform(X_test['education_num'].values.reshape(-1,1))\nX_test['hours_per_week'] = mm_scaler2.transform(X_test['hours_per_week'].values.reshape(-1,1))\n\n## 9) X, y split\nX_train, y_train = target_handle(train)","66c6781b":"svm_clf1 = LinearSVC()\n\n# k = 5\uc778 KFold\uc640 Fold\ubcc4 \uc815\ud655\ub3c4\ub97c \ub2f4\uc744 list \uc0dd\uc131\nkfold = KFold(n_splits=5) # default = 3\ncv_accuracy = [] # \uc608\uce21 \uc131\ub2a5\uc744 list\uc5d0 \ub2f4\uc744 \uac83\uc774\ub2e4.\nprint('\ub370\uc774\ud130 \uc138\ud2b8 \ud06c\uae30:',X_train.shape[0])\n\nn_iter = 0\n\n# KFold.split( ) \ud638\ucd9c: Fold \ubcc4 \ud559\uc2b5, \uac80\uc99d Data\uc758 row index\ub97c array\ub85c \ubc18\ud658  \nfor train_index, test_index in kfold.split(X_train):\n    \n    # kfold.split( )\uc73c\ub85c \ubc18\ud658\ub41c \uc778\ub371\uc2a4\ub97c \uc774\uc6a9\ud558\uc5ec \ud559\uc2b5\uc6a9, \uac80\uc99d\uc6a9 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \ucd94\ucd9c\n    X_train_, X_val = X_train.iloc[train_index], X_train.iloc[test_index]\n    y_train_, y_val = y_train[train_index], y_train[test_index]\n    \n    # \ud559\uc2b5 \ubc0f \uc608\uce21 \n    svm_clf1.fit(X_train_ , y_train_)    \n    pred = svm_clf1.predict(X_val)\n    n_iter += 1\n    \n    # \ubc18\ubcf5 \uc2dc \ub9c8\ub2e4 \uc815\ud655\ub3c4 \uce21\uc815 \n    accuracy = np.round(f1_score(y_val, pred, average='micro'), 4)\n    train_size = X_train_.shape[0]\n    test_size = X_val.shape[0]\n    print('\\n#{0} \uad50\ucc28 \uac80\uc99d \uc815\ud655\ub3c4 :{1}, \ud559\uc2b5 \ub370\uc774\ud130 \ud06c\uae30: {2}, \uac80\uc99d \ub370\uc774\ud130 \ud06c\uae30: {3}'\n          .format(n_iter, accuracy, train_size, test_size))\n    cv_accuracy.append(accuracy)\n    \n# \uac1c\ubcc4 iteration\ubcc4 \uc815\ud655\ub3c4\ub97c \ud569\ud558\uc5ec \ud3c9\uade0 \uc815\ud655\ub3c4 \uacc4\uc0b0 \nprint('\\n## \ud3c9\uade0 \uac80\uc99d \uc815\ud655\ub3c4:', np.mean(cv_accuracy))","7c0bfd9c":"svm_clf2 = SVC(gamma = 'auto')\n\n# k = 5\uc778 KFold\uc640 Fold\ubcc4 \uc815\ud655\ub3c4\ub97c \ub2f4\uc744 list \uc0dd\uc131\nkfold = KFold(n_splits=5) # default = 3\ncv_accuracy = [] # \uc608\uce21 \uc131\ub2a5\uc744 list\uc5d0 \ub2f4\uc744 \uac83\uc774\ub2e4.\nprint('\ub370\uc774\ud130 \uc138\ud2b8 \ud06c\uae30:',X_train.shape[0])\n\nn_iter = 0\n\n# KFold.split( ) \ud638\ucd9c: Fold \ubcc4 \ud559\uc2b5, \uac80\uc99d Data\uc758 row index\ub97c array\ub85c \ubc18\ud658  \nfor train_index, test_index in kfold.split(X_train):\n    \n    # kfold.split( )\uc73c\ub85c \ubc18\ud658\ub41c \uc778\ub371\uc2a4\ub97c \uc774\uc6a9\ud558\uc5ec \ud559\uc2b5\uc6a9, \uac80\uc99d\uc6a9 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \ucd94\ucd9c\n    X_train_, X_val = X_train.iloc[train_index], X_train.iloc[test_index]\n    y_train_, y_val = y_train[train_index], y_train[test_index]\n    \n    # \ud559\uc2b5 \ubc0f \uc608\uce21 \n    svm_clf2.fit(X_train_ , y_train_)    \n    pred = svm_clf2.predict(X_val)\n    n_iter += 1\n    \n    # \ubc18\ubcf5 \uc2dc \ub9c8\ub2e4 \uc815\ud655\ub3c4 \uce21\uc815 \n    accuracy = np.round(f1_score(y_val, pred, average='micro'), 4)\n    train_size = X_train_.shape[0]\n    test_size = X_val.shape[0]\n    print('\\n#{0} \uad50\ucc28 \uac80\uc99d \uc815\ud655\ub3c4 :{1}, \ud559\uc2b5 \ub370\uc774\ud130 \ud06c\uae30: {2}, \uac80\uc99d \ub370\uc774\ud130 \ud06c\uae30: {3}'\n          .format(n_iter, accuracy, train_size, test_size))\n    cv_accuracy.append(accuracy)\n    \n# \uac1c\ubcc4 iteration\ubcc4 \uc815\ud655\ub3c4\ub97c \ud569\ud558\uc5ec \ud3c9\uade0 \uc815\ud655\ub3c4 \uacc4\uc0b0 \nprint('\\n## \ud3c9\uade0 \uac80\uc99d \uc815\ud655\ub3c4:', np.mean(cv_accuracy))","b2e3253d":"# \uc2dc\uac04\uc774 \ub108\ubb34 \uc624\ub798 \uac78\ub824\uc11c \uc77c\ub2e8 skip\n'''\nsvm_clf3 = SVC(kernel = 'poly', gamma = 'auto')\n\n# k = 5\uc778 KFold\uc640 Fold\ubcc4 \uc815\ud655\ub3c4\ub97c \ub2f4\uc744 list \uc0dd\uc131\nkfold = KFold(n_splits=5) # default = 3\ncv_accuracy = [] # \uc608\uce21 \uc131\ub2a5\uc744 list\uc5d0 \ub2f4\uc744 \uac83\uc774\ub2e4.\nprint('\ub370\uc774\ud130 \uc138\ud2b8 \ud06c\uae30:',X_train.shape[0])\n\nn_iter = 0\n\n# KFold.split( ) \ud638\ucd9c: Fold \ubcc4 \ud559\uc2b5, \uac80\uc99d Data\uc758 row index\ub97c array\ub85c \ubc18\ud658  \nfor train_index, test_index in kfold.split(X_train):\n    \n    # kfold.split( )\uc73c\ub85c \ubc18\ud658\ub41c \uc778\ub371\uc2a4\ub97c \uc774\uc6a9\ud558\uc5ec \ud559\uc2b5\uc6a9, \uac80\uc99d\uc6a9 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \ucd94\ucd9c\n    X_train_, X_val = X_train.iloc[train_index], X_train.iloc[test_index]\n    y_train_, y_val = y_train[train_index], y_train[test_index]\n    \n    # \ud559\uc2b5 \ubc0f \uc608\uce21 \n    svm_clf3.fit(X_train_ , y_train_)    \n    pred = svm_clf3.predict(X_val)\n    n_iter += 1\n    \n    # \ubc18\ubcf5 \uc2dc \ub9c8\ub2e4 \uc815\ud655\ub3c4 \uce21\uc815 \n    accuracy = np.round(f1_score(y_val, pred, average='micro'), 4)\n    train_size = X_train_.shape[0]\n    test_size = X_val.shape[0]\n    print('\\n#{0} \uad50\ucc28 \uac80\uc99d \uc815\ud655\ub3c4 :{1}, \ud559\uc2b5 \ub370\uc774\ud130 \ud06c\uae30: {2}, \uac80\uc99d \ub370\uc774\ud130 \ud06c\uae30: {3}'\n          .format(n_iter, accuracy, train_size, test_size))\n    cv_accuracy.append(accuracy)\n    \n# \uac1c\ubcc4 iteration\ubcc4 \uc815\ud655\ub3c4\ub97c \ud569\ud558\uc5ec \ud3c9\uade0 \uc815\ud655\ub3c4 \uacc4\uc0b0 \nprint('\\n## \ud3c9\uade0 \uac80\uc99d \uc815\ud655\ub3c4:', np.mean(cv_accuracy))\n'''","8734b2a4":"svm_clf4 = SVC(kernel = 'sigmoid', gamma = 'auto')\n\n# k = 5\uc778 KFold\uc640 Fold\ubcc4 \uc815\ud655\ub3c4\ub97c \ub2f4\uc744 list \uc0dd\uc131\nkfold = KFold(n_splits=5) # default = 3\ncv_accuracy = [] # \uc608\uce21 \uc131\ub2a5\uc744 list\uc5d0 \ub2f4\uc744 \uac83\uc774\ub2e4.\nprint('\ub370\uc774\ud130 \uc138\ud2b8 \ud06c\uae30:',X_train.shape[0])\n\nn_iter = 0\n\n# KFold.split( ) \ud638\ucd9c: Fold \ubcc4 \ud559\uc2b5, \uac80\uc99d Data\uc758 row index\ub97c array\ub85c \ubc18\ud658  \nfor train_index, test_index in kfold.split(X_train):\n    \n    # kfold.split( )\uc73c\ub85c \ubc18\ud658\ub41c \uc778\ub371\uc2a4\ub97c \uc774\uc6a9\ud558\uc5ec \ud559\uc2b5\uc6a9, \uac80\uc99d\uc6a9 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130 \ucd94\ucd9c\n    X_train_, X_val = X_train.iloc[train_index], X_train.iloc[test_index]\n    y_train_, y_val = y_train[train_index], y_train[test_index]\n    \n    # \ud559\uc2b5 \ubc0f \uc608\uce21 \n    svm_clf4.fit(X_train_ , y_train_)    \n    pred = svm_clf4.predict(X_val)\n    n_iter += 1\n    \n    # \ubc18\ubcf5 \uc2dc \ub9c8\ub2e4 \uc815\ud655\ub3c4 \uce21\uc815 \n    accuracy = np.round(f1_score(y_val, pred, average='micro'), 4)\n    train_size = X_train_.shape[0]\n    test_size = X_val.shape[0]\n    print('\\n#{0} \uad50\ucc28 \uac80\uc99d \uc815\ud655\ub3c4 :{1}, \ud559\uc2b5 \ub370\uc774\ud130 \ud06c\uae30: {2}, \uac80\uc99d \ub370\uc774\ud130 \ud06c\uae30: {3}'\n          .format(n_iter, accuracy, train_size, test_size))\n    cv_accuracy.append(accuracy)\n    \n# \uac1c\ubcc4 iteration\ubcc4 \uc815\ud655\ub3c4\ub97c \ud569\ud558\uc5ec \ud3c9\uade0 \uc815\ud655\ub3c4 \uacc4\uc0b0 \nprint('\\n## \ud3c9\uade0 \uac80\uc99d \uc815\ud655\ub3c4:', np.mean(cv_accuracy))","cc7f1cb9":"# 'rbf' \ucc44\ud0dd\ny_test_predict = svm_clf2.predict(X_test).astype(int)\n\nsample_submission['prediction'] = y_test_predict\nsample_submission.to_csv('submission4.csv', index=False)\n\n# Test f1-score: 0.86314","5ea7b663":"# 1. \uc774\uc804\uae4c\uc9c0\uc758 \ubc29\ubc95\uc73c\ub85c \ub370\uc774\ud130 \uc804\ucc98\ub9ac","508f3d19":"# 2. \ubaa8\ub378\ub9c1 (+KFold (k = 5))\n1. Linear SVM\n    - penalty: 'l1', '12'\n    - loss: 'square hinge', 'hinge'\n    1. 'l2', 'square hinge': 0.85024\n    2. '12', 'hinge': 0.8436\n2. kernel = 'rbf'\n    - gamma: 'scale', 'auto', float\n    1. 'scale': 0.77286\n    2. 'auto': 0.8560800000000001\n3. kernel = 'poly'\n    - gamma: 'scale', 'auto'\n    - degree\n    - coefficient\n    1. 'scale': 0.77386\n    2. 'auto': \n4. kernel = 'sigmoid'\n    - gamma: 'scale', 'auto'\n    - coefficient\n    1. 'scale': 0.7591800000000001\n    2. 'auto': 0.77242"}}