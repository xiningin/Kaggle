{"cell_type":{"a2e59636":"code","e3eeea4d":"code","f2c6b977":"code","3f1cc446":"code","cad2052f":"code","026aaec2":"code","a800f904":"code","d6f60908":"code","49e378d3":"code","84cb902d":"code","7417e536":"code","34c990a1":"code","821ec7db":"code","76471008":"code","0e0bfbbb":"code","83084877":"code","8ce20ed0":"code","0aa13086":"code","bc007496":"code","0e425778":"code","387de2bb":"code","de6fbe3d":"code","36873eb0":"code","8100fb02":"code","537aa652":"code","06d345fd":"code","a21569de":"code","02ff1a3b":"code","643f90dd":"code","20b83804":"code","1bd6bf1a":"code","2ed45eb1":"code","6aa35bcb":"code","c81b8646":"code","252d8ecd":"code","f47fe370":"code","1834288c":"code","57333c49":"code","495126bf":"code","1ff4f6d3":"code","4072915c":"code","18884e7e":"markdown","890e9419":"markdown","2ef90457":"markdown","f8e04f14":"markdown","cdb95891":"markdown","5ac53906":"markdown","ee3c2a44":"markdown","dfa79a92":"markdown","ae8aaa7f":"markdown","6dc3b8f7":"markdown","9703e0fa":"markdown","14900830":"markdown","6b99f53f":"markdown","9df99827":"markdown","853b77cf":"markdown"},"source":{"a2e59636":"# Titanic dataset\n# Humberto Barrantes\n# 06-2021","e3eeea4d":"\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.model_selection import cross_val_score\n","f2c6b977":"# train dataset\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain.head()","3f1cc446":"# test dataset\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest.head()","cad2052f":"# submission example\ngender_data = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ngender_data.head()","026aaec2":"datasets = [train, test]","a800f904":"# train nulls\n\nprint(train.isnull().sum())","d6f60908":"# test nulls\n\nprint(test.isnull().sum())","49e378d3":"# Fill Age Nulls\n\nfor df in datasets:\n    \n    age_mean = df[\"Age\"].mean()\n    age_std = df[\"Age\"].std()\n    is_null = df[\"Age\"].isnull().sum()\n    \n    # random numbers between the mean +- std\n    random_ages = np.random.randint(age_mean - age_std, age_mean + age_std, size = is_null)\n    \n    # fill Age NaN values with the random values previously generated\n    age_slice = df[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = random_ages\n    df[\"Age\"] = age_slice\n    df[\"Age\"] = df[\"Age\"].astype(int)","84cb902d":"# Fill Embarked Nulls\n\nembarked_mode = train['Embarked'].mode()[0]\n\nfor df in datasets:\n    df['Embarked'].fillna(embarked_mode, inplace=True)\n","7417e536":"# Fill Embarked Nulls\n\nfor df in datasets:\n    df['Fare'].fillna(train['Fare'].mean(), inplace=True)\n","34c990a1":"# Survival Rate by class\n\nfor c in train['Pclass'].unique():\n    subset_c = train[train['Pclass'] == c][\"Survived\"]\n    survival_rate = int((sum(subset_c)\/len(subset_c))*100)\n    print(f\"{survival_rate}% of passengers from class {c} survived\")","821ec7db":"# Survival Rate by embarked\n\nfor e in train['Embarked'].unique():\n    subset_e = train[train['Embarked'] == e][\"Survived\"]\n    survival_rate = int((sum(subset_e)\/len(subset_e))*100)\n    print(f\"{survival_rate}% of passegenrs who embarked at {e} survived\")","76471008":"# Survival Rate by gender\n\nfor s in train['Sex'].unique():\n    subset_s = train[train['Sex'] == s][\"Survived\"]\n    survival_rate = int((sum(subset_s)\/len(subset_s))*100)\n    print(f\"{survival_rate}% of {s}s survived\")","0e0bfbbb":"fig = make_subplots(\n    rows=4, \n    cols=2,\n    column_widths = [10] * 2,\n    row_heights = [10] * 4,\n    subplot_titles=[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n)\n\nfig.append_trace(go.Histogram(x=train['Survived'], name=\"Survived\"), 1, 1)\nfig.append_trace(go.Histogram(x=train['Pclass'], name=\"Pclass\"), 1, 2)\nfig.append_trace(go.Histogram(x=train['Sex'], name=\"Sex\"), 2, 1)\nfig.append_trace(go.Histogram(x=train['Age'], name=\"Age\"), 2, 2)\nfig.append_trace(go.Histogram(x=train['SibSp'], name=\"SibSp\"), 3, 1)\nfig.append_trace(go.Histogram(x=train['Parch'], name=\"Parch\"), 3, 2)\nfig.append_trace(go.Histogram(x=train['Fare'], name=\"Fare\"), 4, 1)\nfig.append_trace(go.Histogram(x=train['Embarked'], name=\"Embarked\"), 4, 2)\n\nfig.update_layout(showlegend=False, height=1000)\n\nfig.show()","83084877":"columns = [\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n\nfig = make_subplots(\n    rows=len(columns)\/\/2, \n    cols=2,\n    column_widths = [10] * 2,\n    row_heights = [10] * 4,\n    subplot_titles=[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n)\n\nfor i, column in enumerate(columns):\n    r = (i)\/\/2\n    c = (i) - r*2\n    \n    survived = train[train.Survived == 1][column]\n    died = train[train.Survived == 0][column]\n    fig.append_trace(go.Histogram(x=survived, name='survived', marker_color='#EB89B5'), r+1, c+1)\n    fig.append_trace(go.Histogram(x=died, name='died', marker_color='#330C73'), r+1, c+1)\n\nfig.update_layout(showlegend=False, height=1000)\n\nfig.show()","8ce20ed0":"for df in datasets:\n    df['Relatives'] = df['SibSp'] + df['Parch']\n    df['Travelled_alone'] = df['Relatives'].apply(lambda x: 1 if x == 0 else 0 )","0aa13086":"fig = px.histogram(train, x=\"Relatives\", color=\"Survived\")\nfig.show()","bc007496":"train.head()","0e425778":"train_encoder = OrdinalEncoder()\n\ntrain[['Sex', 'Embarked']] = train_encoder.fit_transform(train[['Sex', 'Embarked']])","387de2bb":"test_encoder = OrdinalEncoder()\n\ntest[['Sex', 'Embarked']] = test_encoder.fit_transform(test[['Sex', 'Embarked']])","de6fbe3d":"corr = train.corr()","36873eb0":"fig = px.imshow(corr)\nfig.show()","8100fb02":"X_train = train[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Relatives', 'Travelled_alone']].copy()\n\ny_train = train['Survived'].copy()\n\nX_test = test[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Relatives', 'Travelled_alone']].copy()","537aa652":"X_train.head()","06d345fd":"X_test.head()","a21569de":"from sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression(max_iter=500, C=10)\n\n# check the accuracy with cross validation\nscores = cross_val_score(lr_model, X_train, y_train, cv=5)\n\n# now lets train our final model\nlr_model.fit(X_train, y_train)\n\nfinal_score = lr_model.score(X_train, y_train)\n\n\nprint(f\"Scores: {scores} \\nMean: {scores.mean()} \\nFinal Score: {final_score}\")","02ff1a3b":"y_pred = lr_model.predict(X_train)\n\ncm = confusion_matrix(y_train, y_pred)\n\nfig = px.imshow(cm, x=['Died', 'Survived'], y=['Died', 'Survived'], range_color=[0,500])\nfig.show()","643f90dd":"from sklearn.naive_bayes import GaussianNB\n\nnbmodel = GaussianNB()\n\n# check the accuracy with cross validation\nscores = cross_val_score(nbmodel, X_train, y_train, cv=5)\n\n# now lets train our final model\nnbmodel.fit(X_train, y_train)\n\nfinal_score = nbmodel.score(X_train, y_train)\n\nprint(f\"Scores: {scores} \\nMean: {scores.mean()} \\nFinal Score: {final_score}\")","20b83804":"y_pred = nbmodel.predict(X_train)\n\ncm = confusion_matrix(y_train, y_pred)\n\nfig = px.imshow(cm, x=['Died', 'Survived'], y=['Died', 'Survived'], range_color=[0,500])\nfig.show()","1bd6bf1a":"from sklearn.neighbors import KNeighborsClassifier\n\nk_values = [x for x in range(1, 50, 10)]\nscores = []\n\nfor k in k_values:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    scores.append(knn.score(X_train, y_train))\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=k_values, y=scores, mode='lines+markers'))\nfig.update_layout(xaxis_title=\"knn score\", yaxis_title=\"k\")\nfig.show()","2ed45eb1":"# define the model\nknn_model = KNeighborsClassifier(n_neighbors=3)\n\n# check the accuracy with cross validation\nscores = cross_val_score(knn_model, X_train, y_train, cv=5)\n\n# now lets train our final model\nknn_model.fit(X_train, y_train)\n\nfinal_score = knn_model.score(X_train, y_train)\n\nprint(f\"Scores: {scores} \\nMean: {scores.mean()} \\nFinal Score: {final_score}\")","6aa35bcb":"y_pred = knn_model.predict(X_train)\n\ncm = confusion_matrix(y_train, y_pred)\n\nfig = px.imshow(cm, x=['Died', 'Survived'], y=['Died', 'Survived'], range_color=[0,500])\nfig.show()","c81b8646":"from sklearn.ensemble import RandomForestClassifier","252d8ecd":"from sklearn.model_selection import GridSearchCV\n\n# A basic formula to calculate the weight of each class is:\n# total observations \/ (number of classes * observations in class)\n\nclasses_weights = {\n    0: train.shape[0] \/ (len(train.Survived.unique()) * train[train.Survived == 0].shape[0]),\n    1: train.shape[0] \/ (len(train.Survived.unique()) * train[train.Survived == 1].shape[0])\n}\n\n\nforest_params = dict( \n    n_estimators = [n for n in range(50, 100, 25)], # default = 100\n    criterion = [\"gini\", \"entropy\"], # default = \"gini\"\n    max_depth = [n for n in range(5, 15, 5)], # default = None\n    min_samples_split = [n for n in range(2, 10, 3)], # default = 2\n    min_samples_leaf = [n for n in range(1, 10, 3)], # default = 1\n    # min_weight_fraction_leaf = [n for n in np.arange(0.0, 0.5, 0.1)], # default = 0.0\n    #max_features = [\"auto\", \"sqrt\", \"log2\"], # default = \"auto\"\n    max_leaf_nodes = [n for n in range(10, 100, 40)], # default = None\n    # min_impurity_decrease = 0.0, # default = 0.0\n    # min_impurity_split = None, # default = None\n    # bootstrap = True, # default = True\n    # oob_score = False, # default = False\n    # n_jobs = -1, # default = None\n    # random_state = None, # default = None\n    # verbose = 0, # default = 0\n    # warm_start = False, # default = False\n    class_weight = [\"balanced\", \"balanced_subsample\", classes_weights],\n    # ccp_alpha = 0.0, # default = 0.0\n    # max_samples = None # default = None\n)\n\nforest = RandomForestClassifier(n_jobs = -1)\n\nforest_cv = GridSearchCV(estimator=forest, param_grid=forest_params, cv=5) \nforest_cv.fit(X_train, y_train)","f47fe370":"print(\"Best score: {}\".format(forest_cv.best_score_))\nprint(\"Optimal params: {}\".format(forest_cv.best_estimator_))","1834288c":"rfc_model = RandomForestClassifier(\n    n_estimators=50,\n    criterion='entropy',\n    max_depth=10,\n    max_leaf_nodes=25,\n    min_samples_split=5,\n    class_weight = classes_weights,\n    n_jobs=-1\n)\n\nrfc_model.fit(X_train, y_train)\n\nscores = cross_val_score(rfc_model, X_train, y_train, cv=5)\n\nfinal_score = rfc_model.score(X_train, y_train)\n\nprint(f\"Scores: {scores} \\nMean: {scores.mean()} \\nFinal Score: {final_score}\")","57333c49":"# print importances\n\nimportances = [round(i, 2) for i in rfc_model.feature_importances_]\nindices = np.argsort(importances)[::-1]\ncols = X_train.columns[indices]\n\nfor column, importance in zip(list(cols), importances):\n    print(f\"Column '{column}', Importance {importance}\")","495126bf":"y_pred = rfc_model.predict(X_train)\n\ncm = confusion_matrix(y_train, y_pred)\n\nfig = px.imshow(cm, x=['Died', 'Survived'], y=['Died', 'Survived'], range_color=[0,500])\nfig.show()","1ff4f6d3":"final_predictions = rfc_model.predict(X_test)\n\n#final_predictions = nbmodel.predict(X_test)","4072915c":"submission = pd.DataFrame({'PassengerId':test['PassengerId'], 'Survived':final_predictions})\n\nsubmission.to_csv('gender_submission.csv',index=False)","18884e7e":"# Imports","890e9419":"# Models","2ef90457":"## 3. Random Forest","f8e04f14":"# Get X and y","cdb95891":"# Handle nulls","5ac53906":"# Predict and Save","ee3c2a44":"## 1. Logistic Regression","dfa79a92":"## 2. Naive Bayes","ae8aaa7f":"## Correlation Matrix","6dc3b8f7":"## 3. KNN","9703e0fa":"# Histograms","14900830":"# Encoding Categorical Variables","6b99f53f":"# Data Exploration","9df99827":"# Data","853b77cf":"# Interactions"}}