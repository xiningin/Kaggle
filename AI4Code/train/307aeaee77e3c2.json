{"cell_type":{"b43af793":"code","5578200b":"code","1f77ed63":"code","707c4bbe":"code","fc9b28e6":"code","a4f0bd2a":"code","830d4c32":"code","4a013afc":"code","da436584":"code","e5bb9fc0":"code","5ef25025":"code","d17e1aad":"code","8471a3c4":"code","5a2198fe":"code","9d4e451f":"code","356399c1":"code","96df8588":"code","957ec917":"code","64acad8d":"code","92d8d14b":"code","5e5130ed":"code","cb3c2035":"code","97e76abe":"code","bb31ba5f":"code","0c3d4880":"code","f90554e7":"code","8dc3e152":"code","21add0ba":"code","8905eadb":"code","b20ac6ee":"code","7199af7f":"code","1ed66175":"code","c009a751":"code","cd4876b5":"code","aedfc42b":"markdown","ffbc719c":"markdown","11c8100e":"markdown","c69bed1c":"markdown","4a046c60":"markdown","61a79430":"markdown","21aef28d":"markdown","d1ba5938":"markdown","ca8fefbd":"markdown","aed9a0b7":"markdown","c8180b1a":"markdown","e01474be":"markdown","ca86e95c":"markdown","79264f9f":"markdown","2ac9c08f":"markdown","7ff58551":"markdown","ec44eb8c":"markdown","b1381006":"markdown","f19bd755":"markdown","2ba0cd5d":"markdown","f8bd51eb":"markdown","128694c8":"markdown","a5536ca4":"markdown","a8da346e":"markdown","08124d5f":"markdown","2508817e":"markdown","b20f3faa":"markdown","b1416c81":"markdown","ca8a3d6e":"markdown","f7ab4f57":"markdown","5c5b16c9":"markdown"},"source":{"b43af793":"# load the most important libraries for the data analysis and preprocessing\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport matplotlib.dates as mdates\nfrom sklearn import preprocessing\nimport pickle\nfrom dateutil import rrule\nsns.set()\nfrom sklearn.feature_selection import SelectFromModel\n\n# Turn off warnings\nimport warnings\nwarnings.filterwarnings('ignore')","5578200b":"weight_wrz = pd.read_csv('..\/input\/data-for-swarm-detector\/gewicht 12.csv', sep = ';');\nweight_wrz = weight_wrz.append(pd.read_csv('..\/input\/data-for-swarm-detector\/gewicht 13.csv', sep = ';'));\nweight_wrz = weight_wrz.append(pd.read_csv('..\/input\/data-for-swarm-detector\/gewicht 14.csv', sep = ';'));\nweight_wrz = weight_wrz.append(pd.read_csv('..\/input\/data-for-swarm-detector\/gewicht 15.csv', sep = ';'));\nweight_wrz = weight_wrz.append(pd.read_csv('..\/input\/data-for-swarm-detector\/gewicht 16.csv', sep = ';'));\nweight_wrz = weight_wrz.append(pd.read_csv('..\/input\/data-for-swarm-detector\/gewicht 17.csv', sep = ';'));\nweight_wrz = weight_wrz.append(pd.read_csv('..\/input\/data-for-swarm-detector\/gewicht 18.csv', sep = ';'));\nweight_wrz = weight_wrz.append(pd.read_csv('..\/input\/data-for-swarm-detector\/gewicht 19.csv', sep = ';'));","1f77ed63":"# create Pandas time series for the data\ntime_arr = pd.to_datetime(weight_wrz.Time.str[0:18])\nts_wrz = pd.Series(data=np.array(weight_wrz.Value), name=\"s_Gewicht\", index=pd.DatetimeIndex(time_arr), dtype=\"float\")\n\n# clean measurement errors (weight below 10kg or above 100kg)\nts_wrz[ts_wrz < 10] = np.NaN\nts_wrz[ts_wrz > 100] = np.NaN\n\n# manual removal of measurement errors\nts_wrz['2018-10-03':'2018-10-13 17:00:00'] = np.NaN\nts_wrz['2016-06-03':'2016-08-24 17:00:00'] = np.NaN\nts_wrz['2013-11-07 09:50:00':'2013-11-07 16:00:00'] = np.NaN\nts_wrz['2017-09-18 04:50:00':'2017-09-18 15:00:00'] = np.NaN\nts_wrz['2016-12-10 10:45:00':'2016-12-10 15:13:00'] = np.NaN\nts_wrz['2017-05-12 05:30:00':'2017-05-12 19:30:00'] = np.NaN\nts_wrz['2017-05-15 09:00:00':'2017-05-15 13:30:00'] = np.NaN\n\n# resampling time series to hours (biggest time period in the data)\nts_wrz_hour = ts_wrz.resample(\"H\").mean()\n#ts_wrz_day = ts_wrz.resample(\"D\").mean()\n\n# plot data over all years\nax = plt.figure(figsize=(6,3), dpi=200).add_subplot(111)\nts_wrz_hour.plot(ax=ax, title=\"Time course of the hive's weight in W\u00fcrzburg\")","707c4bbe":"weight_sch = pd.read_csv('..\/input\/data-for-swarm-detector\/gewicht 1519.csv', sep = ';');\ntime_arr_sch = pd.to_datetime(weight_sch.Time.str[0:18])\nts_sch = pd.Series(data=np.array(weight_sch.Value), name=\"s_Gewicht\", index=pd.DatetimeIndex(time_arr_sch), dtype=\"float\")\n\n# manual removal of measurement errors\nts_sch['2015-02-13 03:00:00':'2015-02-15 17:00:00'] = np.NaN\n\n# convert weight from grams into kilograms:\nts_sch = ts_sch\/1000\n\n# resampling time series to hours (biggest time period in the data)\nts_sch_hour = ts_sch.resample(\"H\").mean()\n#ts_sch_day = ts_sch.resample(\"D\").mean()\n\n# plot the time serie over all years\nax = plt.figure(figsize=(6,3), dpi=200).add_subplot(111)\nts_sch_hour.plot(ax=ax, title=\"Time course of the hive's weight in Schwartau\")","fc9b28e6":"# copy the original time series for this preprocessing experiment\nts_wrz_std = ts_wrz_hour.copy(deep=True)\n\n# get start values for the years 2012 to 2020\nstart_values = {}\nfor year in range(2012,2020):\n    start_values[year] = ts_wrz_std[pd.Timestamp(str(year)+'-03-01')]\n\n# subtract start values (check: values on Mar\/01 should be 0)\nfor item in ts_wrz_std.index:\n    ts_wrz_std[item] = ts_wrz_std[item] - start_values[item.year]\n\n# data in 2016 is not useful and, thus, removed\nts_wrz_tmp = ts_wrz_std['2012':'2015']\nts_wrz_tmp = ts_wrz_tmp.append(ts_wrz_std['2017':'2019'])\n\n# finally create a copy for data analysis\/preprocessing\nts_wrz_h_tmp = ts_wrz_hour.copy(deep=True)\nts_wrz_h_tmp = ts_wrz_hour['2012':'2015']\nts_wrz_h_tmp = ts_wrz_h_tmp.append(ts_wrz_hour['2017':'2019'])","a4f0bd2a":"# repeat the procedure for Schwartau\nts_sch_std = ts_sch_hour.copy(deep=True)\n\nstart_values = {}\nfor year in range(2015,2020):\n    start_values[year] = ts_sch_std[pd.Timestamp(str(year)+'-03-01')]\n          \nfor item in ts_sch_std['2015':'2020'].index:\n    ts_sch_std[item] = ts_sch_std[item] - start_values[item.year]\n\nts_sch_tmp = ts_sch_std['2015']\nts_sch_tmp = ts_sch_tmp.append(ts_sch_std['2019'])\n\nts_sch_h_tmp = ts_sch_hour.copy(deep=True)\nts_sch_h_tmp = ts_sch_hour['2015']\nts_sch_h_tmp = ts_sch_h_tmp.append(ts_sch_hour['2019'])","830d4c32":"# create a dataframe for the original weight in W\u00fcrzburg\ndf_gewicht = ts_wrz_tmp.to_frame()\ndf_gewicht['Ort'] = \"W\u00fcrzburg\"\n\n# adding data of Schwartau\ndf_sch_tmp = ts_sch_tmp.to_frame()\ndf_sch_tmp['Ort'] = 'Schwartau'\ndf_gewicht = df_gewicht.append(df_sch_tmp)\n\n# identifier is the city (Ort) combined with the year; also convert date to a proper datetime value; remove \"Ort\"\ndf_gewicht['id'] = df_gewicht['Ort']  + \", \" +  np.datetime_as_string(df_gewicht.index.values, unit='Y')\ndf_gewicht['Day'] = pd.to_datetime(df_gewicht.index.dayofyear-1, unit='D', origin=pd.Timestamp('2019-01-01'))\ndf_gewicht.drop(columns=[\"Ort\"], inplace=True)\n# df_gewicht contains the time series with the original weight data\n\n# same procedure to create the 'standardized' weight\ndf_gewicht_full = ts_wrz_h_tmp.to_frame()\ndf_gewicht_full['Ort'] = \"W\u00fcrzburg\"\n\n# adding data of Schwartau\ndf_sch_full_tmp = ts_sch_h_tmp.to_frame()\ndf_sch_full_tmp['Ort'] = 'Schwartau'\ndf_gewicht_full = df_gewicht_full.append(df_sch_full_tmp)\n\n# create identifer, correct datetime field and remove 'Ort'\ndf_gewicht_full['id'] = df_gewicht_full['Ort']  + \", \" +  np.datetime_as_string(df_gewicht_full.index.values, unit='Y')\ndf_gewicht_full['Day'] = pd.to_datetime(df_gewicht_full.index.dayofyear-1, unit='D', origin=pd.Timestamp('2019-01-01'))\ndf_gewicht_full.drop(columns=[\"Ort\"], inplace=True)\n# df_gewicht_full is the standardized weight (0kg on March 01)","4a013afc":"# original time series\nax = plt.figure(figsize=(6,3), dpi=200).add_subplot(111)\npd.Series(data=np.array(weight_wrz.Value), index=pd.DatetimeIndex(time_arr), dtype=\"float\")\npiv = pd.pivot_table(df_gewicht_full, index=['Day'], columns=['id'])\npiv['2019-03-01':'2019-07-01'].plot(ax=ax, title=\"Development of weight from March to July\")\nax.xaxis.set_major_locator(mdates.MonthLocator(bymonthday=1))\nax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\nax.xaxis.set_minor_formatter(mdates.DateFormatter('%d'))\nax.tick_params(which='minor',labelsize=5, pad=1)\nax.legend(loc='upper left', prop={'size': 6}, labels=piv.columns.levels[1])","da436584":"# standardized time series (shift to 0kg axis)\nax = plt.figure(figsize=(6,3), dpi=200).add_subplot(111)\npd.Series(data=np.array(weight_wrz.Value), index=pd.DatetimeIndex(time_arr), dtype=\"float\")\npiv = pd.pivot_table(df_gewicht, index=['Day'],columns=['id'])\npiv['2019-03-01':'2019-07-01'].plot(ax=ax, title=\"Development of (standardized) weight from March to July\")\nax.xaxis.set_major_locator(mdates.MonthLocator(bymonthday=1))\nax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\nax.xaxis.set_minor_formatter(mdates.DateFormatter('%d'))\nax.tick_params(which='minor',labelsize=5, pad=1)\nax.legend(loc='upper left', prop={'size': 6}, labels=piv.columns.levels[1])","e5bb9fc0":"# copy standardized weight from March 01 to July 01\ndf_ausschnitt = df_gewicht_full[(df_gewicht_full.Day >= '2019-03-01') & (df_gewicht_full.Day < '2019-07-01')].copy(deep=True)\ndf_ausschnitt[\"cycle\"] = (df_ausschnitt.index.dayofyear * 24) + df_ausschnitt.index.hour\ndf_ausschnitt.reset_index(inplace=True)\n\n# (manually) remove sections without swarm events\ndf_ausschnitt.drop(df_ausschnitt.query('id in [\"W\u00fcrzburg, 2018\", \"W\u00fcrzburg, 2017\", \"W\u00fcrzburg, 2013\", \"Schwartau, 2017\", \"Schwartau, 2016\"]').index, inplace=True)\n\n# (manually) cut time series after the swarm events - W\u00fcrzburg\ndf_ausschnitt.drop(df_ausschnitt[(df_ausschnitt.id == \"W\u00fcrzburg, 2019\") & (df_ausschnitt.Time >= '2019-05-01')].index, inplace=True)\ndf_ausschnitt.drop(df_ausschnitt[(df_ausschnitt.id == \"W\u00fcrzburg, 2015\") & (df_ausschnitt.Time >= '2015-05-15')].index, inplace=True)\ndf_ausschnitt.drop(df_ausschnitt[(df_ausschnitt.id == \"W\u00fcrzburg, 2014\") & (df_ausschnitt.Time >= '2014-05-09')].index, inplace=True)\ndf_ausschnitt.drop(df_ausschnitt[(df_ausschnitt.id == \"W\u00fcrzburg, 2012\") & (df_ausschnitt.Time >= '2012-05-17')].index, inplace=True)\n# Schwartau\ndf_ausschnitt.drop(df_ausschnitt[(df_ausschnitt.id == \"Schwartau, 2015\") & (df_ausschnitt.Time >= '2015-05-12')].index, inplace=True)\ndf_ausschnitt.drop(df_ausschnitt[(df_ausschnitt.id == \"Schwartau, 2019\") & (df_ausschnitt.Time >= '2019-05-13')].index, inplace=True)","5ef25025":"ax = plt.figure(figsize=(6,3), dpi=200).add_subplot(111)\npiv = pd.pivot_table(df_ausschnitt[['Day', 'id', 's_Gewicht']], index=['Day'],columns=['id'])\n\npiv['2019-03-01':'2019-07-01'].plot(ax=ax, title= \"Development of weight until the swarm events\")\nax.xaxis.set_major_locator(mdates.MonthLocator(bymonthday=1))\nax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\nax.tick_params(which='minor',labelsize=5, pad=1)\nax.legend(loc='upper left', prop={'size': 6}, labels=piv.columns.levels[1])","d17e1aad":"# create overall dataframe\ndf_gesamt = df_ausschnitt\n\n# read in and add number of outgoing bees for Schwartau\ndaten = pd.read_csv('..\/input\/data-for-swarm-detector\/ausflge 1519.csv', sep = ';');\ntime_arr = pd.to_datetime(daten.Time.str[0:18])\nts = pd.Series(data= np.array(daten.Value), name=\"s_Ausfl\u00fcge\", index=pd.DatetimeIndex(time_arr), dtype=\"float\")\nts = ts.resample(\"H\").mean()\ndf_temp = ts.to_frame()\ndf_temp['Ort'] = \"Schwartau\"\n\n# read in and add number of outgoing bees for W\u00fcrzburg\ndaten = pd.read_csv('..\/input\/data-for-swarm-detector\/Ausflge 12141519.csv', sep = ';');\ntime_arr = pd.to_datetime(daten.Time.str[0:18])\nts = pd.Series(data= np.array(daten.Value), name=\"s_Ausfl\u00fcge\", index=pd.DatetimeIndex(time_arr), dtype=\"float\")\nts = ts.resample(\"H\").mean()\ndf_wrz = ts.to_frame()\ndf_wrz['Ort'] = \"W\u00fcrzburg\"\ndf_temp = df_temp.append(df_wrz)\n\n# create identifier and drop 'Ort' attribute\ndf_temp['id'] = df_temp['Ort']  + \", \" +  np.datetime_as_string(df_temp.index.values, unit='Y')\ndf_temp.drop(columns=[\"Ort\"], inplace=True)\ndf_temp.reset_index(inplace=True)\ndf_gesamt = pd.merge(df_gesamt, df_temp, how = 'left', on = ['Time', 'id'])","8471a3c4":"# read in and add number of incoming bees for Schwartau\ndaten = pd.read_csv('..\/input\/data-for-swarm-detector\/einflge 1519.csv', sep = ';');\ntime_arr = pd.to_datetime(daten.Time.str[0:18])\nts = pd.Series(data= np.array(daten.Value), name=\"s_Einfl\u00fcge\", index=pd.DatetimeIndex(time_arr), dtype=\"float\")\nts = ts.resample(\"H\").mean()\ndf_temp = ts.to_frame()\ndf_temp['Ort'] = \"Schwartau\"\n\n# read in and add number of incoming bees for W\u00fcrzburg\ndaten = pd.read_csv('..\/input\/data-for-swarm-detector\/Einflge 12141519.csv', sep = ';');\ntime_arr = pd.to_datetime(daten.Time.str[0:18])\nts = pd.Series(data= np.array(daten.Value), name=\"s_Einfl\u00fcge\", index=pd.DatetimeIndex(time_arr), dtype=\"float\")\nts = ts.resample(\"H\").mean()\ndf_wrz = ts.to_frame()\ndf_wrz['Ort'] = \"W\u00fcrzburg\"\ndf_temp = df_temp.append(df_wrz)\n\n# create identifier and drop 'Ort' attribute\ndf_temp['id'] = df_temp['Ort']  + \", \" +  np.datetime_as_string(df_temp.index.values, unit='Y')\ndf_temp.drop(columns=[\"Ort\"], inplace=True)\ndf_temp.reset_index(inplace=True)\ndf_gesamt = pd.merge(df_gesamt, df_temp, how = 'left', on = ['Time', 'id'])","5a2198fe":"# read in and add humidity in the hives for Schwartau\ndaten = pd.read_csv('..\/input\/data-for-swarm-detector\/nestfeuchte 19.csv', sep = ';');\ntime_arr = pd.to_datetime(daten.Time.str[0:18])\nts = pd.Series(data= np.array(daten.Value), name=\"s_Nestfeuchte\", index=pd.DatetimeIndex(time_arr), dtype=\"float\")\n\n# manual data cleaning (values have to be between 0% and 100%)\nts[ts  >= 100] = np.NaN\nts[ts <= 0] = np.NaN\n# resampling to hourly values\nts = ts.resample(\"H\").mean()\ndf_temp = ts.to_frame()\ndf_temp['Ort'] = \"Schwartau\"\n\n# read in and add humidity in the hives for W\u00fcrzburg\ndaten = pd.read_csv('..\/input\/data-for-swarm-detector\/nestfeuchte 12141519.csv', sep = ';');\ntime_arr = pd.to_datetime(daten.Time.str[0:18])\nts = pd.Series(data= np.array(daten.Value), name=\"s_Nestfeuchte\", index=pd.DatetimeIndex(time_arr), dtype=\"float\")\n\n# manual data cleaning (values have to be between 0% and 100%)\nts[ts  >= 100] = np.NaN\nts[ts <= 0] = np.NaN\n# resampling to hourly values\nts = ts.resample(\"H\").mean()\ndf_wrz = ts.to_frame()\ndf_wrz['Ort'] = \"W\u00fcrzburg\"\ndf_temp = df_temp.append(df_wrz)\n\n# create identifier and drop 'Ort' attribute\ndf_temp['id'] = df_temp['Ort']  + \", \" +  np.datetime_as_string(df_temp.index.values, unit='Y')\ndf_temp.drop(columns=[\"Ort\"], inplace=True)\ndf_temp.reset_index(inplace=True)\ndf_gesamt = pd.merge(df_gesamt, df_temp, how = 'left', on = ['Time', 'id'])","9d4e451f":"# read in and add temparature for Schwartau\ndaten = pd.read_csv('..\/input\/data-for-swarm-detector\/temperatur 1519.csv', sep = ';');\ntime_arr = pd.to_datetime(daten.Time.str[0:18])\nts = pd.Series(data= np.array(daten.Value), name=\"s_Temperatur\", index=pd.DatetimeIndex(time_arr), dtype=\"float\")\nts = ts.resample(\"H\").mean()\ndf_temp = ts.to_frame()\ndf_temp['Ort'] = \"Schwartau\"\n\n# read in and add temparature for W\u00fcrzburg\ndaten = pd.read_csv('..\/input\/data-for-swarm-detector\/temperatur v3 12141519.csv', sep = ';');\ntime_arr = pd.to_datetime(daten.Time.str[0:18])\nts = pd.Series(data= np.array(daten.Value), name=\"s_Temperatur\", index=pd.DatetimeIndex(time_arr), dtype=\"float\")\nts = ts.resample(\"H\").mean()\ndf_wrz = ts.to_frame()\ndf_wrz['Ort'] = \"W\u00fcrzburg\"\ndf_temp = df_temp.append(df_wrz)\n\n# create identifier and drop 'Ort' attribute\ndf_temp['id'] = df_temp['Ort']  + \", \" +  np.datetime_as_string(df_temp.index.values, unit='Y')\ndf_temp.drop(columns=[\"Ort\"], inplace=True)\ndf_temp.reset_index(inplace=True)\ndf_gesamt = pd.merge(df_gesamt, df_temp, how = 'left', on = ['Time', 'id'])\n\nax = plt.figure(figsize=(6,3), dpi=200).add_subplot(111)\nts['2012-03-01':'2019-07-01'].plot(ax=ax, title=\"Temperature\")","356399c1":"# copy and describe resulting dataframe\ndf = df_gesamt.copy()\ndf.describe()","96df8588":"# manual correction of measurment errors (use last value)    \ndf.loc[(df.Time == \"2014-03-31 05:00:00\") & (df.id==\"W\u00fcrzburg, 2014\"), 's_Nestfeuchte'] = df.loc[(df.Time == pd.to_datetime(\"2014-03-31 05:00:00\") - timedelta(hours=1)) & (df.id==\"W\u00fcrzburg, 2014\"), 's_Nestfeuchte'].values[0]\ndf.loc[(df.Time == \"2014-04-01 02:00:00\") & (df.id==\"W\u00fcrzburg, 2014\"), 's_Nestfeuchte'] = df.loc[(df.Time == pd.to_datetime(\"2014-04-01 02:00:00\") - timedelta(hours=1)) & (df.id==\"W\u00fcrzburg, 2014\"), 's_Nestfeuchte'].values[0]\ndf.loc[(df.Time == \"2014-04-01 07:00:00\") & (df.id==\"W\u00fcrzburg, 2014\"), 's_Nestfeuchte'] = df.loc[(df.Time == pd.to_datetime(\"2014-04-01 07:00:00\") - timedelta(hours=1)) & (df.id==\"W\u00fcrzburg, 2014\"), 's_Nestfeuchte'].values[0]\ndf.loc[(df.Time == \"2014-04-01 08:00:00\") & (df.id==\"W\u00fcrzburg, 2014\"), 's_Nestfeuchte'] = df.loc[(df.Time == pd.to_datetime(\"2014-04-01 08:00:00\") - timedelta(hours=1)) & (df.id==\"W\u00fcrzburg, 2014\"), 's_Nestfeuchte'].values[0]\ndf.loc[(df.Time == \"2014-04-01 09:00:00\") & (df.id==\"W\u00fcrzburg, 2014\"), 's_Nestfeuchte'] = df.loc[(df.Time == pd.to_datetime(\"2014-04-01 09:00:00\") - timedelta(hours=1)) & (df.id==\"W\u00fcrzburg, 2014\"), 's_Nestfeuchte'].values[0]\ndf.loc[(df.Time == \"2014-04-01 10:00:00\") & (df.id==\"W\u00fcrzburg, 2014\"), 's_Nestfeuchte'] = df.loc[(df.Time == pd.to_datetime(\"2014-04-01 10:00:00\") - timedelta(hours=1)) & (df.id==\"W\u00fcrzburg, 2014\"), 's_Nestfeuchte'].values[0]\ndf.loc[(df.Time == \"2014-04-01 11:00:00\") & (df.id==\"W\u00fcrzburg, 2014\"), 's_Nestfeuchte'] = df.loc[(df.Time == pd.to_datetime(\"2014-04-01 11:00:00\") - timedelta(hours=1)) & (df.id==\"W\u00fcrzburg, 2014\"), 's_Nestfeuchte'].values[0]\ndf.loc[(df.Time == \"2014-04-01 12:00:00\") & (df.id==\"W\u00fcrzburg, 2014\"), 's_Nestfeuchte'] = df.loc[(df.Time == pd.to_datetime(\"2014-04-01 12:00:00\") - timedelta(hours=1)) & (df.id==\"W\u00fcrzburg, 2014\"), 's_Nestfeuchte'].values[0]\ndf.loc[(df.Time == \"2014-04-01 13:00:00\") & (df.id==\"W\u00fcrzburg, 2014\"), 's_Nestfeuchte'] = df.loc[(df.Time == pd.to_datetime(\"2014-04-01 13:00:00\") - timedelta(hours=1)) & (df.id==\"W\u00fcrzburg, 2014\"), 's_Nestfeuchte'].values[0]\ndf.loc[(df.Time == \"2019-04-09 11:00:00\") & (df.id==\"W\u00fcrzburg, 2019\"), 's_Nestfeuchte'] = df.loc[(df.Time == pd.to_datetime(\"2019-04-09 11:00:00\") - timedelta(hours=1)) & (df.id==\"W\u00fcrzburg, 2019\"), 's_Nestfeuchte'].values[0]\ndf.loc[(df.Time == \"2019-04-09 12:00:00\") & (df.id==\"W\u00fcrzburg, 2019\"), 's_Nestfeuchte'] = df.loc[(df.Time == pd.to_datetime(\"2019-04-09 12:00:00\") - timedelta(hours=1)) & (df.id==\"W\u00fcrzburg, 2019\"), 's_Nestfeuchte'].values[0]\ndf.loc[(df.Time == \"2019-04-09 13:00:00\") & (df.id==\"W\u00fcrzburg, 2019\"), 's_Nestfeuchte'] = df.loc[(df.Time == pd.to_datetime(\"2019-04-09 13:00:00\") - timedelta(hours=1)) & (df.id==\"W\u00fcrzburg, 2019\"), 's_Nestfeuchte'].values[0]\ndf.loc[(df.Time == \"2019-04-09 14:00:00\") & (df.id==\"W\u00fcrzburg, 2019\"), 's_Nestfeuchte'] = df.loc[(df.Time == pd.to_datetime(\"2019-04-09 14:00:00\") - timedelta(hours=1)) & (df.id==\"W\u00fcrzburg, 2019\"), 's_Nestfeuchte'].values[0]\n   \n# replace spaces durch to time change (summer time!)\nfor col in ['s_Gewicht', 's_Ausfl\u00fcge', 's_Einfl\u00fcge', 's_Nestfeuchte', 's_Temperatur']:\n    df.loc[(df.Time == \"2012-03-25 02:00:00\") & (df.id==\"W\u00fcrzburg, 2012\"), col] = df.loc[(df.Time == \"2012-03-25 01:00:00\") & (df.id==\"W\u00fcrzburg, 2012\"), col].values[0]\n    df.loc[(df.Time == \"2014-03-30 02:00:00\") & (df.id==\"W\u00fcrzburg, 2014\"), col] = df.loc[(df.Time == \"2014-03-30 01:00:00\") & (df.id==\"W\u00fcrzburg, 2014\"), col].values[0]\n    df.loc[(df.Time == \"2015-03-29 02:00:00\") & (df.id==\"W\u00fcrzburg, 2015\"), col] = df.loc[(df.Time == \"2015-03-29 01:00:00\") & (df.id==\"W\u00fcrzburg, 2015\"), col].values[0]\n    df.loc[(df.Time == \"2015-03-29 02:00:00\") & (df.id==\"Schwartau, 2015\"), col] = df.loc[(df.Time == \"2015-03-29 01:00:00\") & (df.id==\"Schwartau, 2015\"), col].values[0]\n    df.loc[(df.Time == \"2019-03-31 02:00:00\") & (df.id==\"W\u00fcrzburg, 2019\"), col] = df.loc[(df.Time == \"2019-03-31 01:00:00\") & (df.id==\"W\u00fcrzburg, 2019\"), col].values[0]\n    df.loc[(df.Time == \"2019-03-31 02:00:00\") & (df.id==\"Schwartau, 2019\"), col] = df.loc[(df.Time == \"2019-03-31 01:00:00\") & (df.id==\"Schwartau, 2019\"), col].values[0]\n\nfor i in range(0,25):\n    for col in ['s_Gewicht', 's_Ausfl\u00fcge', 's_Einfl\u00fcge', 's_Nestfeuchte', 's_Temperatur']:\n        df.loc[(df.Time == pd.to_datetime(\"2019-03-09 00:00:00\") + timedelta(hours=i)) & (df.id==\"W\u00fcrzburg, 2019\"), col] = df.loc[(df.Time == pd.to_datetime(\"2019-03-09 00:00:00\") + timedelta(hours=i) - timedelta(days=1)) & (df.id==\"W\u00fcrzburg, 2019\"), col].values[0]\n        df.loc[(df.Time == pd.to_datetime(\"2019-03-09 00:00:00\") + timedelta(hours=i)) & (df.id==\"Schwartau, 2019\"), col] = df.loc[(df.Time == pd.to_datetime(\"2019-03-09 00:00:00\") + timedelta(hours=i) - timedelta(days=1)) & (df.id==\"Schwartau, 2019\"), col].values[0]\n        \n# clear selected columns (Time, Day, encode identifiers)\ndf.drop(columns=[\"Time\", \"Day\"], inplace=True)\ndf.replace([\"W\u00fcrzburg, 2012\", \"W\u00fcrzburg, 2014\", \"W\u00fcrzburg, 2015\", \"W\u00fcrzburg, 2019\", \"Schwartau, 2015\", \"Schwartau, 2019\"], [1, 2, 3, 4, 5, 6], inplace=True)\nsnams = [\"s_Gewicht\", \"s_Ausfl\u00fcge\", \"s_Einfl\u00fcge\", \"s_Nestfeuchte\", \"s_Temperatur\"]","957ec917":"## window size for rolling means \/ standard deviation\nws =  7\n\nMAX = (df.groupby(df.id, as_index = False).cycle.max(skipna = True).rename(index = str, columns = {\"cycle\" : \"MAX\"}))\ndf = pd.merge(df, MAX, how = 'inner', on = 'id')\ndf['RUL'] = df.MAX - df.cycle\ndf.drop('MAX', 1, inplace = True)","64acad8d":"# Calculate rolling means and standard deviations, groubed by time series ID\nrmeans = df.groupby('id')[snams].rolling(window = ws, min_periods = 1).mean()\nrsds = df.groupby('id')[snams].rolling(window = ws, min_periods = 1).std().fillna(0)\n\nanams = [i.replace('s_','a_') for i in snams]\nrmeans.columns = anams\n\nsdnams = [i.replace('s_','sd_') for i in snams]\nrsds.columns = sdnams\n\n# add to dataframe\ndf = pd.concat([df, \n                   rmeans.reset_index(drop = True), \n                   rsds.reset_index(drop = True)], \n                  axis = 1)\ndf.head()","92d8d14b":"df = df.loc[:,df.nunique() != 1]","5e5130ed":"df_viz = df.copy()","cb3c2035":"dontscale = ['RUL', 'id']\nscale = df.columns.drop(dontscale)\nscaler = preprocessing.StandardScaler().fit(df[scale])\ndf = pd.concat([pd.DataFrame(scaler.transform(df[scale]), columns = scale),\n                   df.RUL,\n                   df.id], \n                  axis = 1)\ndf.head()","97e76abe":"#1: W\u00fcrzburg, 2012\n#2: W\u00fcrzburg, 2014\n#3: W\u00fcrzburg, 2015\n#4: W\u00fcrzburg, 2019\n#5: Schwartau, 2015\n#6: Schwartau, 2019\n\ntrainIDs = [1,3,5,6]\ntestIDs = [2, 4]\ntrain = df[df.id.isin(trainIDs)]\ntest = df[df.id.isin(testIDs)]","bb31ba5f":"cols = ['id', 'cycle','s_Ausfl\u00fcge', 's_Einfl\u00fcge', 's_Nestfeuchte', 's_Temperatur', 's_Gewicht']\ndata = (df[cols]\n        .melt(id_vars = ['id', 'cycle'], var_name = 'sensor')\n        .query('id in [1,3]')\n        )\ngrid = (sns.FacetGrid(data, \n                      col = 'sensor', col_wrap = 4, \n                      hue = 'id', \n                      sharex = True, sharey = False,\n                      legend_out = True,\n                      margin_titles = True\n                      )\n         .map(plt.plot, 'cycle', 'value')\n         .add_legend()\n        )\nplt.subplots_adjust(top=0.9)\ngrid.fig.suptitle('Sensor Data f\u00fcr zwei Bienenst\u00f6cke', fontsize = 24)","0c3d4880":"from sklearn.metrics import confusion_matrix, accuracy_score","f90554e7":"X_train = train.drop(['RUL', 'id'], axis = 1)","8dc3e152":"X_test = test.drop(['RUL', 'id'], axis = 1)\n\n# create copies with NaN values (humidity in Schwartau is null)\nX_train_complete = X_train.copy()\nX_test_complete = X_test.copy()\n\n# Important remark: at this point we do not use the humidity\nX_train = X_train.drop(['s_Nestfeuchte', 'a_Nestfeuchte', 'sd_Nestfeuchte'], axis = 1)\nX_test = X_test.drop(['s_Nestfeuchte', 'a_Nestfeuchte', 'sd_Nestfeuchte'], axis = 1)","21add0ba":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nY_train = train['RUL']\nY_test = test['RUL']","8905eadb":"from sklearn.linear_model import LinearRegression\nmodel_r_linearModel = LinearRegression(fit_intercept=True)\nmodel_r_linearModel.fit(X_train, Y_train)\npred = model_r_linearModel.predict(X_test)\nstats_r_linearModel = sqrt(mean_squared_error(pred, Y_test))\nstats_r_linearModel","b20ac6ee":"from scipy import stats\nimport statsmodels.api as sm\nmodel_r_poissonReg = sm.genmod.GLM(Y_train, X_train, family=sm.families.Poisson()).fit_regularized()\npred = model_r_poissonReg.predict(X_test)\nstats_r_poissonReg = sqrt(mean_squared_error(pred, Y_test))\nstats_r_poissonReg","7199af7f":"from sklearn.ensemble import RandomForestRegressor\nmodel_r_randomForest = RandomForestRegressor()\nmodel_r_randomForest.fit(X_train, Y_train)\npred = model_r_randomForest.predict(X_test)\nstats_r_randomForest = sqrt(mean_squared_error(pred, Y_test))\nstats_r_randomForest","1ed66175":"from sklearn.svm import SVR\nmodel_r_svm = SVR()\nmodel_r_svm.fit(X_train, Y_train)\npred = model_r_svm.predict(X_test)\nstats_r_svm = sqrt(mean_squared_error(pred, Y_test))\nstats_r_svm","c009a751":"from sklearn.neural_network import MLPRegressor\nmodel_r_nnet = MLPRegressor(hidden_layer_sizes=100, max_iter=300)\nmodel_r_nnet.fit(X_train, Y_train)\npred = model_r_nnet.predict(X_test)\nstats_r_nnet = sqrt(mean_squared_error(pred, Y_test))\nstats_r_nnet","cd4876b5":"res = pd.Series({'Linear Regression' : stats_r_linearModel,\n       'Random Forest' : stats_r_randomForest,\n       'Support Vector Machines' : stats_r_svm,\n       'Neural Network' : stats_r_nnet,\n       'Regularized Poisson Regression' : stats_r_poissonReg})\nres.sort_values(inplace=True)\nprint(res)\nprint(\"\\nBest performance by \" + res.keys()[0] + \".\")","aedfc42b":"Random Forest:","ffbc719c":"**Number of incoming bees:**","11c8100e":"### Add additional sensor data","c69bed1c":"# Split data into training and test data\n\nFor the training four time series (id 1, 3, 5 and 6) are used - time series with the ids 2 and 4 are used to validate the model.","4a046c60":"## Data cleaning\n\n* Missings for a single point in time: re-use last value\n* Missings for a whole day: re-use last day (March 9, 2019)","61a79430":"**Now we process the weight data of Schwartau:**","21aef28d":"Prepare training data:","d1ba5938":"## Visualization","ca8fefbd":"**Compile relevant time series (from March to the swarm event) and calculate the cycles**\n\nIn this case cycle refers to the number of hours until the 1st of March.","aed9a0b7":"## Result\n\nIn this example the Neural Network Regressor seems to work best (for this specific data-set). To evalute this prediction model in practice, it would be necessary to embed sensors in a real hive and record the time series. After standardization, these recorded time series can be used to predict the 'RUL' for a bee hive (*model_r_nnet.predict(X_test)*), i.e. the number of cycles (hourse) until a swarm event will happen.","c8180b1a":"Linear Regression:","e01474be":"Regularized Poisson Regression:","ca86e95c":"Prepare test data:","79264f9f":"Support Vector Machine:","2ac9c08f":"**Temperature:**","7ff58551":"**Number of outgoing bees of the relevant time periods:**","ec44eb8c":"Neural Network:\n","b1381006":"**Daigram for the development of the weight starting with March 01 (over all years except 2016):**","f19bd755":"**Combining data from W\u00fcrzburg und Schwartau:**","2ba0cd5d":"### Explorative data analysis and data engineering based on the weight\n\nAs HOBOS does not provide event data about their hives (e.g., interventions of beekeepers, diseases, death of a queen bee, honey harvesting etc) it was necessary to decide for an event type that can be identified in the data.\n\n**Swarm events** can easily be detected, as they lead to a clear pattern in the monitoring data. Precisely, bees consume a lot of honey before they leave, so the pattern comprises an increase of the temperature inside the hive followed by a sudden decrease of the weight of the hive (by 2-3 kilograms). Moreover, the count numbers of incoming and outgoing bees can be used to confirm the patterns that were identified.\n\nThe data-set and particularly the weight also allow to identify other events, such as beekeeping activities (sudden decrease and increase of weight within a timeframe of 20 to 30 minutes) or the harvesting of honey (decrease of 15kg and more). However, this approach mainly focuses on the prediction of swarm events.\n\n#### In a first step the weight data of one place (W\u00fcrzburg) is loaded:","f8bd51eb":"Keep copy for visualization:    ","128694c8":"Summary:","a5536ca4":"**Humidity in the hives:**","a8da346e":"**Standardizing the time series**\n\nThe starting weight of each hive varies resp. depends on the place and on the year. Therefore, the value of March 1 was used as starting points for the time series (includes the trend).","08124d5f":"### Regression\nPrepare data:","2508817e":"**Diagramm f\u00fcr die Gewichtsver\u00e4nderung bis zum Ausschwarm-Event zeichnen:**","b20f3faa":"Scale data:","b1416c81":"## Swarm Detection Model based on the HOBOS data-set\n\nIn the following the development of a prediction model for beekeeping-related events is described. Based on the data-set of the HOBOS project (https:\/\/www.hobos.de) it is shown how exploratory data analysis leads to a good idea for a prediction model. In further consequence the data engineering as well as the training and validation of a prediction model are highlighted.\n\n**Acknowledgements**\nMany thanks go to the Smart Beekeeping project team in my Machine Learning course. Without their hard work on data exploration, data engineering and model development, this Jupyter Notebook would not have been possible.","ca8a3d6e":"## Train and assess predictive models","f7ab4f57":"Remove constant columns:","5c5b16c9":"## Calclulate RUL (Remaining Useful Life)\n\nFor this task the timeseries are used to develop a model for predicting in how many cyles (hours) the swarm event will take place.\n\nFor the rolling mean\/standard deviation a window of 7 cycles is used."}}