{"cell_type":{"88cb9139":"code","c03e2c9e":"code","fac2353f":"code","d4303718":"code","599d9cd6":"code","d30a96e1":"code","56e839ae":"code","991f6e27":"code","d9c873f6":"code","818ee83f":"code","1eefe03b":"code","d753d402":"code","372d489e":"code","7d3651c1":"markdown","4603fbb7":"markdown","867cca12":"markdown","be6486ce":"markdown","7d329d87":"markdown","9c093d91":"markdown"},"source":{"88cb9139":"# Import the necessary packages\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport sklearn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\nfrom sklearn.metrics import r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","c03e2c9e":"# Import and read dataset\ninput_ = \"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\"\ndf = pd.read_csv(input_)\n\ndf.head(10)","fac2353f":"plt.figure(figsize=(15,15))\nsns.heatmap(df.corr(),annot=True)\nplt.show()","d4303718":"df.describe()","599d9cd6":"x = df.drop(columns='DEATH_EVENT')\ny = df['DEATH_EVENT']\n\nmodel = RandomForestClassifier()\nmodel.fit(x,y)\nprint(model.feature_importances_)\nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(12).plot(kind='barh')\nplt.show()","d30a96e1":"# Delete outlier\ndf=df[df['ejection_fraction']<70]","56e839ae":"#inp_data = df.drop(df[['DEATH_EVENT']], axis=1)\ninp_data = df.iloc[:,[0,4,7,11]]\nout_data = df[['DEATH_EVENT']]\n\nX_train, X_test, y_train, y_test = train_test_split(inp_data, out_data, test_size=0.2, random_state=0)\n\n## Applying Transformer\nsc=StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","991f6e27":"## X_train, X_test, y_train, y_test Shape\n\nprint(\"X_train Shape : \", X_train.shape)\nprint(\"X_test Shape  : \", X_test.shape)\nprint(\"y_train Shape : \", y_train.shape)\nprint(\"y_test Shape  : \", y_test.shape)","d9c873f6":"## I coded this method for convenience and to avoid writing the same code over and over again\n\ndef result(clf):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    \n    print('Accuracy Score: {:.4f}'.format(accuracy_score(y_test, y_pred)))\n    print('Random Forest Classifier f1-score      : {:.4f}'.format(f1_score( y_test , y_pred)))\n    print('Random Forest Classifier precision     : {:.4f}'.format(precision_score(y_test, y_pred)))\n    print('Random Forest Classifier recall        : {:.4f}'.format(recall_score(y_test, y_pred)))\n    print(\"Random Forest Classifier roc auc score : {:.4f}\".format(roc_auc_score(y_test,y_pred)))\n    print(\"\\n\",classification_report(y_pred, y_test))\n    \n    plt.figure(figsize=(6,6))\n    cf_matrix = confusion_matrix(y_test, y_pred)\n    sns.heatmap((cf_matrix \/ np.sum(cf_matrix)*100), annot = True, fmt=\".2f\", cmap=\"Blues\")\n    plt.title(\"RandomForestClassifier Confusion Matrix (Rate)\")\n    plt.show()\n    \n    cm = confusion_matrix(y_test,y_pred)\n    plt.figure(figsize=(6,6))\n    sns.heatmap(cm, annot=True, cmap=\"Blues\",\n                xticklabels=[\"FALSE\",\"TRUE\"],\n                yticklabels=[\"FALSE\",\"TRUE\"],\n                cbar=False)\n    plt.title(\"RandomForestClassifier Confusion Matrix (Number)\")\n    plt.show()\n    \ndef sample_result(\n    n_estimators=100,\n    max_features='auto',\n    max_depth=None,\n    min_samples_split=2):    \n    \n    scores = [] \n    for i in range(0,100): # 100 samples\n        n_estimators, max_features, max_depth, min_samples_split\n        X_train, X_test, y_train, y_test = train_test_split(inp_data, out_data, test_size=0.2)\n        clf = RandomForestClassifier(n_estimators= n_estimators,\n                                     max_features=max_features,\n                                     max_depth=max_depth,\n                                     min_samples_split=min_samples_split) \n        sc=StandardScaler()\n        X_train = sc.fit_transform(X_train)\n        X_test = sc.fit_transform(X_test)\n        clf.fit(X_train, y_train)\n        scores.append(accuracy_score(clf.predict(X_test), y_test)) \n    \n    plt.hist(scores)\n    plt.show()\n    print(\"Best Score: {}\\nMean Score: {}\".format(np.max(scores), np.mean(scores)))","818ee83f":"clf = RandomForestClassifier(random_state=0)\nresult(clf)\nsample_result()","1eefe03b":"param_grid = {\n    \"n_estimators\": [100, 500, 1000],\n    \"max_features\": [0.5,1,'auto'],\n    \"max_depth\": [1,2,3,4,None],\n    \"min_samples_split\": [2,5,8]\n}\n\nclf = RandomForestClassifier()\ngrid = GridSearchCV(clf, param_grid, n_jobs=-1, verbose=2, cv=10)\ngrid.fit(X_train, y_train)\ngrid.best_params_","d753d402":"clf = RandomForestClassifier(\n    n_estimators=1000,\n    max_features=0.5,\n    max_depth=3,\n    min_samples_split=5,\n    random_state=0\n)\n\nresult(clf)\nsample_result(1000,0.5,3,5)","372d489e":"Importance = pd.DataFrame({'Importance':clf.feature_importances_*100},index=df.iloc[:,[0,4,7,11]].columns)\nImportance.sort_values(by='Importance',axis=0,ascending=True).plot(kind='barh',color='lightblue')\nplt.xlabel('Importance for variable');","7d3651c1":"Random Forest is a community model where multiple decision trees are combined to achieve a stronger model. The derived model will be more robust, accurate, and will handle overfitting better than constitutive models.\n\n## Basic Theory\nRandom Forest has a series of decision trees combined with the \"bagging method\" to obtain classification and regression outputs. In classification, the output is calculated using majority voting, while in regression the average is calculated.\n\nRandom Forest creates a robust and accurate model that can process a wide variety of input data with binary, categorical, continuous features.\n\n\n![](https:\/\/miro.medium.com\/max\/592\/1*i0o8mjFfCn-uD79-F1Cqkw.png)\n\n\n## Lost Function\nWe use the entropy \/ Gini score to calculate the missing value of data sets.\n\n\n## Advantages\n- The correct and powerful model.\n- It efficiently handles overfitting.\n- Supports implicit feature selection and derives feature significance.\n\n\n## Disadvantages\n- When the forest grows, it is computationally complex and slower.\n- It's not a well descriptive model on prediction.\n\n\n## Hyperparameters\n- **n_estimators:**\n    - Default: 100\n    - It is the number of trees in the forest. With a large number of trees comes high accuracy, but high computational complexity.\n- **max_features:**\n    - Default: 'auto'\n    - the maximum number of features allowed in a single tree.\n- **min_samples_split:**\n    - Default: 2\n    - The minimum number of samples required to split an internal node.\n- **min_samples_leaf:**\n    - Default: 1\n    - The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n- **criterion:**\n    - Default: 'gini'\n    - The function to measure the quality of a split. Supported criteria are \u201cgini\u201d for the Gini impurity and \u201centropy\u201d for the information gain. Note: this parameter is tree-specific.\n- **max_depth:**\n    - Default: None\n    - The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n    \n    \n## Comparison with Other Models\nThe Random Forest comparison is quite similar to Decision tree comparisons.\n\n### Random Forest vs Naive Bayes\n- Random Forest is a complex and large model, while Naive Bayes is a relatively small model.\n- While Naive Bayes perform better with small training data, RF needs a larger set of training data.\n\n### Random Forest vs Artificial Neural Networks (NN)\n- Both are very powerful and highly accurate algorithms.\n- Both have property interactions internally and are less explainable.\n- While Random Forest feature does not need scaling, NN does need to scale features.\n- The batch version of both models will be strong.","4603fbb7":"## Reporting\nI evaluated the results I found with Confusion Matrix, the results are as follows: \n\n**Correctly predicted -> %95.00 (282 of 297 predict are correct)**\n- True Negative -> %68.33 (41 people) -> Those who were predicted not to die and who did not die\n- True Positive -> %26.67 (16 people) -> Those who were predicted to die and who did die\n\n**Wrong predicted-> %10.98 (15 of 297 predict are wrong)**\n- False Positive -> %3.33 (2 people) -> Those who were predicted to die but who did not die\n- False Negative -> %01.67 (1 people) -> Those who were predicted to not die but who did die","867cca12":"### Simple Metod\nI applied Random Forests directly without changing anything and the result is as follows:","be6486ce":"### Advanced Method","7d329d87":"![](https:\/\/i.ibb.co\/yW9HZS8\/random-forest.png)\n\n- **ML Part 1** - Logistic Regression\n- **ML Part 2** - K-Nearest Neighbors (KNN)\n- **ML Part 3** - Support Vector Machine (SVM)\n- **ML Part 4** - Artificial Neural Network (NN)\n- **ML Part 5** - Classification and Regression Tree (CART)\n- **ML Part 6 - Random Forests**\n- **ML Part 7** - Gradient Boosting Machines (GBM)\n- **ML Part 8** - XGBoost\n- **ML Part 9** - LightGBM\n- **ML Part 10** - CatBoost","9c093d91":"When we examine the graph above, we can predict that time, serum_creatinine, ejection_fraction, age values will increase accuracy in education."}}