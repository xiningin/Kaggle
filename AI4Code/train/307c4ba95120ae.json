{"cell_type":{"4dc85bea":"code","48e27854":"code","f1e6f0a5":"code","4e1888da":"code","0835ecdb":"code","7a578b8a":"code","7290e78b":"code","267b4f02":"code","cb55495a":"code","7ee0ef32":"code","8438a5d6":"code","4ff2f4fb":"code","adb23b84":"code","7cd3b527":"code","fc66d92d":"code","95c00b5f":"code","95005068":"code","18f93a8e":"code","4b5888dc":"code","7a1cd889":"code","206bf674":"markdown","56c94263":"markdown","dd7f59e6":"markdown","84d7baf0":"markdown","1a1d85a9":"markdown","1d6ef032":"markdown","89efa4f6":"markdown","4697781b":"markdown","6b5ae812":"markdown","50d5b1e2":"markdown","bef298a3":"markdown","e593161e":"markdown","9322dc95":"markdown","def438c9":"markdown","20637a22":"markdown","a0eb8f34":"markdown","6f76ccbb":"markdown","0a72d6bd":"markdown","0eb5a2d2":"markdown","5f2ea430":"markdown","16cccae9":"markdown","3a691733":"markdown","7613e9d3":"markdown","1d810372":"markdown","e2a4a396":"markdown","df651be6":"markdown","373ed400":"markdown","b7d2e151":"markdown","22cfec7e":"markdown","b3ff4593":"markdown"},"source":{"4dc85bea":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n\n# Load dataset.\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\nPassengerId = test['PassengerId']\n\n#fill NaN values in the age column with the median of that column\ntrain['Age'].fillna(train['Age'].mean(), inplace = True)\n#fill test with the train mean to test\ntest['Age'].fillna(train['Age'].mean(), inplace = True)\n\n#fill NaN values in the embarked column with the mode of that column\ntrain['Embarked'].fillna(train['Embarked'].mode()[0], inplace = True)\n#fill test NaN values in the embarked column with the mode from the train set\ntest['Embarked'].fillna(train['Embarked'].mode()[0], inplace = True)\n\n#fill NaN values in the fare column with the median of that column\ntrain['Fare'].fillna(train['Fare'].median(), inplace = True)\ntest['Fare'].fillna(train['Fare'].median(), inplace = True)\n\n#delete the cabin feature\/column and others \ndrop_column = ['PassengerId','Cabin', 'Ticket']\ntrain.drop(drop_column, axis=1, inplace = True)\ntest.drop(drop_column, axis=1, inplace = True)\n\n#create a new column which is the combination of the sibsp and parch column\ntrain['FamilySize'] = train ['SibSp'] + train['Parch'] + 1\ntest['FamilySize'] = test ['SibSp'] + test['Parch'] + 1\n\n#create a new column and initialize it with 1\ntrain['IsAlone'] = 1 #initialize to yes\/1 is alone\ntrain['IsAlone'].loc[train['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1\ntest['IsAlone'] = 1 #initialize to yes\/1 is alone\ntest['IsAlone'].loc[test['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1\n\n#quick and dirty code split title from the name column\ntrain['Title'] = train['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\ntest['Title'] = test['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\n#Continuous variable bins; qcut vs cut: https:\/\/stackoverflow.com\/questions\/30211923\/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n#Fare Bins\/Buckets using qcut or frequency bins: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.qcut.html\ntrain['FareBin'] = pd.qcut(train['Fare'], 4)\ntest['FareBin'] = pd.qcut(train['Fare'], 4)\n\n#alternatively, you can split them yourselves based on the bins you prefer, and you can do the same for the age too\n#     #Mapping Fare\n#     dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n#     dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n#     dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n#     dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n#     # Mapping Age\n#     dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n#     dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n#     dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n#     dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n#     dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;\n\n#Age Bins\/Buckets using cut or value bins: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.cut.html\ntrain['AgeBin'] = pd.cut(train['Age'].astype(int), 5)\ntest['AgeBin'] = pd.cut(train['Age'].astype(int), 5)\n\n#so create stat_min and any titles less than 10 will be put into Misc category\nstat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http:\/\/nicholasjjackson.com\/2012\/03\/08\/sample-size-is-10-a-magic-number\/\ntitle_names = (train['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\ntitle_names_test = (test['Title'].value_counts() < stat_min)\n\n#apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https:\/\/community.modeanalytics.com\/python\/tutorial\/pandas-groupby-and-python-lambda-functions\/\ntrain['Title'] = train['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\ntest['Title'] = test['Title'].apply(lambda x: 'Misc' if title_names_test.loc[x] == True else x)\n\n#convertion from categorical data to dummy variables\nlabel = LabelEncoder()  \ntrain['Sex_Code'] = label.fit_transform(train['Sex'])\ntrain['Embarked_Code'] = label.fit_transform(train['Embarked'])\ntrain['Title_Code'] = label.fit_transform(train['Title'])\ntrain['AgeBin_Code'] = label.fit_transform(train['AgeBin'])\ntrain['FareBin_Code'] = label.fit_transform(train['FareBin'])\n\ntest['Sex_Code'] = label.fit_transform(test['Sex'])\ntest['Embarked_Code'] = label.fit_transform(test['Embarked'])\ntest['Title_Code'] = label.fit_transform(test['Title'])\ntest['AgeBin_Code'] = label.fit_transform(test['AgeBin'])\ntest['FareBin_Code'] = label.fit_transform(test['FareBin'])\n\n#train columns to drop final\ndrop_column = ['Sex','Name', 'Embarked', 'Title','Age','Fare', 'FareBin','AgeBin']\ntrain.drop(drop_column, axis=1, inplace = True)\ntest.drop(drop_column, axis=1, inplace = True)\n\n#define y variable aka target\/outcome\nTarget = ['Survived']","48e27854":"# #you need to run one by one, but just for compilation sake I will put them together.\n# train.tail()\n# test.tail()\n# train.isnull().sum()\n# test.isnull().sum()","f1e6f0a5":"# # Some useful parameters which will come in handy later on\n# ntrain = train.shape[0]\n# ntest = test.shape[0]\n# SEED = 0 # for reproducibility\n# NFOLDS = 5 # set folds for out-of-fold prediction\n# kf = KFold(n_splits= NFOLDS, random_state=SEED)\n\n# # Class to extend the Sklearn classifier\n# class SklearnHelper(object):\n#     def __init__(self, clf, seed=0, params=None):\n#         params['random_state'] = seed\n#         self.clf = clf(**params)\n\n#     def train(self, x_train, y_train):\n#         self.clf.fit(x_train, y_train)\n\n#     def predict(self, x):\n#         return self.clf.predict(x)\n    \n#     def fit(self,x,y):\n#         return self.clf.fit(x,y)\n    \n#     def feature_importances(self,x,y):\n#        return self.clf.fit(x,y).feature_importances_","4e1888da":"# def get_oof(clf, x_train, y_train, x_test):\n#     oof_train = np.zeros((ntrain,))\n#     oof_test = np.zeros((ntest,))\n#     oof_test_skf = np.empty((NFOLDS, ntest))\n\n#     for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n#         x_tr = x_train[train_index]\n#         y_tr = y_train[train_index]\n#         x_te = x_train[test_index]\n\n#         clf.train(x_tr, y_tr)\n\n#         oof_train[test_index] = clf.predict(x_te)\n#         oof_test_skf[i, :] = clf.predict(x_test)\n\n#     oof_test[:] = oof_test_skf.mean(axis=0)\n#     return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","0835ecdb":"# # Put in our parameters for said classifiers\n# # Random Forest parameters\n# rf_params = {\n#     'n_jobs': -1,\n#     'n_estimators': 500,\n#      'warm_start': True, \n#      #'max_features': 0.2,\n#     'max_depth': 6,\n#     'min_samples_leaf': 2,\n#     'max_features' : 'sqrt',\n#     'verbose': 0\n# }\n\n# # Extra Trees Parameters\n# et_params = {\n#     'n_jobs': -1,\n#     'n_estimators':500,\n#     #'max_features': 0.5,\n#     'max_depth': 8,\n#     'min_samples_leaf': 2,\n#     'verbose': 0\n# }\n\n# # AdaBoost parameters\n# ada_params = {\n#     'n_estimators': 500,\n#     'learning_rate' : 0.75\n# }\n\n# # Gradient Boosting parameters\n# gb_params = {\n#     'n_estimators': 500,\n#      #'max_features': 0.2,\n#     'max_depth': 5,\n#     'min_samples_leaf': 2,\n#     'verbose': 0\n# }\n\n# # Support Vector Classifier parameters \n# svc_params = {\n#     'kernel' : 'linear',\n#     'C' : 0.025\n#     }","7a578b8a":"# # Create 5 objects that represent our 4 models\n# rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n# et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n# ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n# gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n# svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)","7290e78b":"# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data","267b4f02":"# # Create our OOF train and test predictions. These base results will be used as new features\n# et_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\n# rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\n# ada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \n# gb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\n# svc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier","cb55495a":"# rf_feature = rf.feature_importances(x_train,y_train)\n# et_feature = et.feature_importances(x_train, y_train)\n# ada_feature = ada.feature_importances(x_train, y_train)\n# gb_feature = gb.feature_importances(x_train,y_train)","7ee0ef32":"# cols = train.columns.values\n# # Create a dataframe with features\n# feature_dataframe = pd.DataFrame( {'features': cols,\n#      'Random Forest feature importances': rf_feature,\n#      'Extra Trees  feature importances': et_feature,\n#       'AdaBoost feature importances': ada_feature,\n#     'Gradient Boost feature importances': gb_feature\n#     })","8438a5d6":"# # Scatter plot \n# trace = go.Scatter(\n#     y = feature_dataframe['Random Forest feature importances'].values,\n#     x = feature_dataframe['features'].values,\n#     mode='markers',\n#     marker=dict(\n#         sizemode = 'diameter',\n#         sizeref = 1,\n#         size = 25,\n# #       size= feature_dataframe['AdaBoost feature importances'].values,\n#         #color = np.random.randn(500), #set color equal to a variable\n#         color = feature_dataframe['Random Forest feature importances'].values,\n#         colorscale='Portland',\n#         showscale=True\n#     ),\n#     text = feature_dataframe['features'].values\n# )\n# data = [trace]\n\n# layout= go.Layout(\n#     autosize= True,\n#     title= 'Random Forest Feature Importance',\n#     hovermode= 'closest',\n# #     xaxis= dict(\n# #         title= 'Pop',\n# #         ticklen= 5,\n# #         zeroline= False,\n# #         gridwidth= 2,\n# #     ),\n#     yaxis=dict(\n#         title= 'Feature Importance',\n#         ticklen= 5,\n#         gridwidth= 2\n#     ),\n#     showlegend= False\n# )\n# fig = go.Figure(data=data, layout=layout)\n# py.iplot(fig,filename='scatter2010')\n\n# # Scatter plot \n# trace = go.Scatter(\n#     y = feature_dataframe['Extra Trees  feature importances'].values,\n#     x = feature_dataframe['features'].values,\n#     mode='markers',\n#     marker=dict(\n#         sizemode = 'diameter',\n#         sizeref = 1,\n#         size = 25,\n# #       size= feature_dataframe['AdaBoost feature importances'].values,\n#         #color = np.random.randn(500), #set color equal to a variable\n#         color = feature_dataframe['Extra Trees  feature importances'].values,\n#         colorscale='Portland',\n#         showscale=True\n#     ),\n#     text = feature_dataframe['features'].values\n# )\n# data = [trace]\n\n# layout= go.Layout(\n#     autosize= True,\n#     title= 'Extra Trees Feature Importance',\n#     hovermode= 'closest',\n# #     xaxis= dict(\n# #         title= 'Pop',\n# #         ticklen= 5,\n# #         zeroline= False,\n# #         gridwidth= 2,\n# #     ),\n#     yaxis=dict(\n#         title= 'Feature Importance',\n#         ticklen= 5,\n#         gridwidth= 2\n#     ),\n#     showlegend= False\n# )\n# fig = go.Figure(data=data, layout=layout)\n# py.iplot(fig,filename='scatter2010')\n\n# # Scatter plot \n# trace = go.Scatter(\n#     y = feature_dataframe['AdaBoost feature importances'].values,\n#     x = feature_dataframe['features'].values,\n#     mode='markers',\n#     marker=dict(\n#         sizemode = 'diameter',\n#         sizeref = 1,\n#         size = 25,\n# #       size= feature_dataframe['AdaBoost feature importances'].values,\n#         #color = np.random.randn(500), #set color equal to a variable\n#         color = feature_dataframe['AdaBoost feature importances'].values,\n#         colorscale='Portland',\n#         showscale=True\n#     ),\n#     text = feature_dataframe['features'].values\n# )\n# data = [trace]\n\n# layout= go.Layout(\n#     autosize= True,\n#     title= 'AdaBoost Feature Importance',\n#     hovermode= 'closest',\n# #     xaxis= dict(\n# #         title= 'Pop',\n# #         ticklen= 5,\n# #         zeroline= False,\n# #         gridwidth= 2,\n# #     ),\n#     yaxis=dict(\n#         title= 'Feature Importance',\n#         ticklen= 5,\n#         gridwidth= 2\n#     ),\n#     showlegend= False\n# )\n# fig = go.Figure(data=data, layout=layout)\n# py.iplot(fig,filename='scatter2010')\n\n# # Scatter plot \n# trace = go.Scatter(\n#     y = feature_dataframe['Gradient Boost feature importances'].values,\n#     x = feature_dataframe['features'].values,\n#     mode='markers',\n#     marker=dict(\n#         sizemode = 'diameter',\n#         sizeref = 1,\n#         size = 25,\n# #       size= feature_dataframe['AdaBoost feature importances'].values,\n#         #color = np.random.randn(500), #set color equal to a variable\n#         color = feature_dataframe['Gradient Boost feature importances'].values,\n#         colorscale='Portland',\n#         showscale=True\n#     ),\n#     text = feature_dataframe['features'].values\n# )\n# data = [trace]\n\n# layout= go.Layout(\n#     autosize= True,\n#     title= 'Gradient Boosting Feature Importance',\n#     hovermode= 'closest',\n# #     xaxis= dict(\n# #         title= 'Pop',\n# #         ticklen= 5,\n# #         zeroline= False,\n# #         gridwidth= 2,\n# #     ),\n#     yaxis=dict(\n#         title= 'Feature Importance',\n#         ticklen= 5,\n#         gridwidth= 2\n#     ),\n#     showlegend= False\n# )\n# fig = go.Figure(data=data, layout=layout)\n# py.iplot(fig,filename='scatter2010')","4ff2f4fb":"# # Create the new column containing the average of values\n\n# feature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\n# feature_dataframe.head(3)\n\n# y = feature_dataframe['mean'].values\n# x = feature_dataframe['features'].values\n# data = [go.Bar(\n#             x= x,\n#              y= y,\n#             width = 0.5,\n#             marker=dict(\n#                color = feature_dataframe['mean'].values,\n#             colorscale='Portland',\n#             showscale=True,\n#             reversescale = False\n#             ),\n#             opacity=0.6\n#         )]\n\n# layout= go.Layout(\n#     autosize= True,\n#     title= 'Barplots of Mean Feature Importance',\n#     hovermode= 'closest',\n# #     xaxis= dict(\n# #         title= 'Pop',\n# #         ticklen= 5,\n# #         zeroline= False,\n# #         gridwidth= 2,\n# #     ),\n#     yaxis=dict(\n#         title= 'Feature Importance',\n#         ticklen= 5,\n#         gridwidth= 2\n#     ),\n#     showlegend= False\n# )\n# fig = go.Figure(data=data, layout=layout)\n# py.iplot(fig, filename='bar-direct-labels')","adb23b84":"# base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n#      'ExtraTrees': et_oof_train.ravel(),\n#      'AdaBoost': ada_oof_train.ravel(),\n#       'GradientBoost': gb_oof_train.ravel()\n#     })\n# base_predictions_train.head()","7cd3b527":"# data = [\n#     go.Heatmap(\n#         z= base_predictions_train.astype(float).corr().values ,\n#         x=base_predictions_train.columns.values,\n#         y= base_predictions_train.columns.values,\n#           colorscale='Viridis',\n#             showscale=True,\n#             reversescale = True\n#     )\n# ]\n# py.iplot(data, filename='labelled-heatmap')","fc66d92d":"# x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\n# x_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)","95c00b5f":"# gbm = xgb.XGBClassifier(\n#     #learning_rate = 0.02,\n#  n_estimators= 2000,\n#  max_depth= 4,\n#  min_child_weight= 2,\n#  #gamma=1,\n#  gamma=0.9,                        \n#  subsample=0.8,\n#  colsample_bytree=0.8,\n#  objective= 'binary:logistic',\n#  nthread= -1,\n#  scale_pos_weight=1).fit(x_train, y_train)\n# predictions = gbm.predict(x_test)","95005068":"# # Generate Submission File \n# StackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n#                             'Survived': predictions })\n# StackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)","18f93a8e":"from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nvote_est = [\n    #Ensemble Methods: http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassifier()),\n\n    #Gaussian Processes: http:\/\/scikit-learn.org\/stable\/modules\/gaussian_process.html#gaussian-process-classification-gpc\n    ('gpc', gaussian_process.GaussianProcessClassifier()),\n    \n    #GLM: http:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#logistic-regression\n    ('lr', linear_model.LogisticRegressionCV()),\n    \n    #Navies Bayes: http:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html\n    ('bnb', naive_bayes.BernoulliNB()),\n    ('gnb', naive_bayes.GaussianNB()),\n    \n    #Nearest Neighbor: http:\/\/scikit-learn.org\/stable\/modules\/neighbors.html\n    ('knn', neighbors.KNeighborsClassifier()),\n    \n    #SVM: http:\/\/scikit-learn.org\/stable\/modules\/svm.html\n    ('svc', svm.SVC(probability=True)),\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n   ('xgb', XGBClassifier())\n\n]","4b5888dc":"#Hard Vote or majority rules\nvote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\nvote_hard_cv = model_selection.cross_validate(vote_hard, x_train, y_train, cv  = 10, return_train_score = True)\nvote_hard.fit(x_train, y_train)\nhard_predict = vote_hard.predict(x_test)\n\nprint(\"Hard Voting Training w\/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting Test w\/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting Test w\/bin score 3*std: +\/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n\n#Soft Vote or weighted probabilities\nvote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\nvote_soft_cv = model_selection.cross_validate(vote_soft, x_train, y_train, cv  = 10 , return_train_score = True)\nvote_soft.fit(x_train, y_train)\nsoft_predict = vote_soft.predict(x_test)\n\n\nprint(\"Soft Voting Training w\/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting Test w\/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting Test w\/bin score 3*std: +\/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n","7a1cd889":"# Generate Submission File \nHardVoteSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': hard_predict })\nHardVoteSubmission.to_csv(\"HardVoteSubmission.csv\", index=False)\n\nSoftVoteSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': soft_predict })\nSoftVoteSubmission.to_csv(\"SoftVoteSubmission.csv\", index=False)","206bf674":"The next part is to visualize the heatmap of the relevance comparing the output of the first layer\/level for each model, the relevance here refers to how same the outputs of the first layer\/level are for each model, to be used as the second layer's input.\n\nThere have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores. ","56c94263":"For this demo of manual ensemble, we will use the xgboost classifier as the second level.\n\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:","dd7f59e6":"Then, we train the dataset on each of the models respectively, and the function returns a list of output, to check for accuracy, for the validation set and also the test set","84d7baf0":"The code below creates a dataframe for the features' importances, for easy plotting via the Plotly package.","1a1d85a9":"There you go, another way to actually use ensemble, it can be seen as sort of a random forest, amongst models, and since it is inside the emsemble library, so I do put it here to be a part of the template options. We get the output based on majority voting, or by weighted calculation. ","1d6ef032":"Okay since this is meant to focus on creating a template for ensemble\/stacking, therefore we will just focus on that and I will take codes from some other reference to quickly preprocess our data, or data cleaning.\n\nData preprocessing consists of multiple steps such as: (a) Correcting (b) Completing (c) Creating (d) Converting (e) Correlating (f) Classifying\n\nYou can read more about them at these notebooks. Feel free to go through them. :) https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions","89efa4f6":"There you go. this is the first part of manually stacking. You can add more stacks to test whether it increases your acuracy, however, for those that have little to no experience in terms of trying and understand models, it would be better if we let the computer to do the computations for us to make the right decisions in making the ensemble, which is the next part.","4697781b":"![image.png](attachment:image.png)","6b5ae812":"There are a number of ways for you to do ensemble, you can build your stack manually, or you can use the built in libraries, depending on how much you want control over your model stack. Either way, we will go through them individually so that it is ready and each of these methods can be applied whenever you want.NOTE: I do not own most of the codes, who owns them anyway, but again, this is meant to be for my personal use, but I am still going to share it, as who knows it might be useful to some people.\n\n","50d5b1e2":"After the params are set, we setup the models that will be used to train the dataset and predict the test set.","bef298a3":"Now we have done filling the missing values\/removing columns etc that are necessary, (again for more infor please refer to the links given above), the next is to create the template.","e593161e":"> I'm not stealing any contents, it's just for my personal use. :) And you might need to change your mindset to progress further in life if you were to think I am.","9322dc95":"![image.png](attachment:image.png)","def438c9":"2. The code at the bottom produces a voting classifier where, from the documentation:\n> If \u2018hard\u2019, uses predicted class labels for majority rule voting. Else if \u2018soft\u2019, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers.\n\nThus, depending on the mode, hard or soft, then it will return the predicted labels based on whichever methodology to obtain the label.\nTo run the codes below, just execute the preprocessing\/data cleaning part, and the training and test data splitting above.","20637a22":"As a precaution, we will check all the variables whether they are ready or not, and the categorical columns have been converted nicely.","a0eb8f34":"Hi guys, this is meant to be for my own reference, but who knows, it might come in handy to other newcomers in the future who feel overwhelmed by the available tools but don't let that scare you from having the most exciting journey in your life. the journey is just starting!","6f76ccbb":"![image.png](attachment:image.png)","0a72d6bd":"Okay. back to data cleaning. I'm just going to copy the code below for you all to catch up.","0eb5a2d2":"Now then, we prepare the training data, the training output and also the test data to fit into the models.","5f2ea430":"1. The first section will describe how we build stacks\/ensemble manually. That is why, a class is preferred so that things are tidier and it can be reused anytime. You can think of classes as the basic blueprint of your model, so e.g., like the basic blueprint to build a car, you might have another class that extends the general Car class, which might consist of tyres, seats, engine etc.You would a Toyota() class which extends the Car() class, and the Toyota() class would have extra features specific to their brand. I hope I was able to explain it, if not then I am sorry, but there are a lot of resources online that would help build your intuition and have a better understanding, never stop learning!\n\nAlso, if you are still unclear, this is where I got the code from, so they might have better explanation.\nhttps:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python","16cccae9":"Once the class is defined, the next thing is to create the function to train and predict, and returns the output of training and test results. (this is similar to using the cross_val_score function from sklearn.model_selection) Again, a function is created here as it be used repeatedly, so it might be more efficient do so.","3a691733":"What we do next is to combine the training output (focus, NOT the test output here) of the different models into a dataframe, for visualization purposes for the next part.","7613e9d3":"As per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in .featureimportances. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such.\n\nTherefore, what we can do is store the list of feature importances into a variable. The values can then be used to visualize the relevance of each features to the output variable.","1d810372":"Producing the Submission file\n\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:","e2a4a396":"Next, we create and prepare the params for each of the models, We will talk about grid search in another kernel not this one, where basically you can give the function a range of values for it to try and return the best params for your model.Here we just set the params as below.","df651be6":"The code below is additional if you would like to compute the mean and then plot the features based on the mean with different models, but I will still run for you to see it, and then comment the codes.","373ed400":"As a side note, the algorithms can be further improved by using Grid Search to find the optimal hyperparams, and then using KFolds to validate them. Besides that, better feature engineering, data cleaning will also contribute to an increase in the accuracy. As you can see from this kernel here https:\/\/www.kaggle.com\/cdeotte\/titantic-mega-model-0-84210, you can see how important feature engineering is, difference is almost 5%. That's it for now guys. ","b7d2e151":"We then plot the features' importances below. Since we are focusing on the template, I will not explain much regarding the plotting. If you want to have more info, https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python is where I got the code from.","22cfec7e":"So that is it for the first layer\/level. So to start stacking\/ensembling, all we need to do is connect the output of the first layer as the input to the second layer\/level.\n\nBefore that, we have to concatenate and join both the first-level train and test predictions as x_train and x_test, and then we can fit it into the second-level learning model.","b3ff4593":"Okay. that's it for now. basically what I have seen so far, most of them would use the voting classifier. If there are more varying methods to do so, I will update this content, though it might be very fundamental for fellow data scientists, but for me, a software engineer aspiring to be a data scientist, it is useful for me, and might be for some people too."}}