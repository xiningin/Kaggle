{"cell_type":{"a36d4338":"code","de7ef102":"code","ba44d940":"code","11ede3ba":"code","ebb0a7bb":"code","a65d646e":"code","b7352332":"code","da5a298d":"code","db11fdef":"code","0f40cb10":"code","cea2dc32":"code","ee24d5d5":"code","894606a0":"code","592acf4c":"code","018c198e":"code","e459e8c4":"code","b5160aa4":"code","c5f618d8":"code","ca3f853e":"code","2941e1c9":"code","46361e59":"code","745f9309":"code","931b056c":"code","dc5ec516":"code","79acd6ea":"code","22c29598":"code","d2f70a6d":"code","1bb3450a":"code","cb0414fd":"code","77c2b4b9":"code","547b4d6e":"code","7557126b":"code","b61e0c68":"code","847199c9":"code","e8df5359":"code","ba2e4388":"code","12b41c60":"code","73a7083d":"code","caaec703":"code","2d31e81a":"code","7aaebdff":"code","d2568bcc":"code","c91461d4":"code","db57bd6c":"code","268a5969":"code","0e919a3b":"code","1b637b40":"code","f46c6821":"code","49889364":"code","04e78f12":"code","ef81cd8a":"code","2b201e5f":"code","bb13c02b":"code","2d39bb6c":"code","9245ffb1":"code","b93cd2f9":"code","227ed344":"code","fe9a74d0":"code","78b7a842":"code","e73faad5":"code","25f4c1df":"markdown","c4cef54a":"markdown","305881fc":"markdown","d02c4dd6":"markdown","604dbf1c":"markdown","5579cbff":"markdown","4eae31e4":"markdown","b3d7cc1d":"markdown","a5212ff3":"markdown","b852db3c":"markdown","b77c9d93":"markdown","b42792ce":"markdown","0b0f61fe":"markdown","1ae7c3df":"markdown","43a0dc0e":"markdown","ee2c7e8a":"markdown","8b36d29b":"markdown","43f11d43":"markdown","85d61195":"markdown","73c0dc18":"markdown","79426fe8":"markdown","39af3299":"markdown","2df1b457":"markdown","f634f53f":"markdown","ea349525":"markdown","a1f20225":"markdown","079927ae":"markdown","729006c5":"markdown","90460866":"markdown","11b6778b":"markdown","754fdb07":"markdown","9fbd6508":"markdown","1d0f6475":"markdown","dcd0b094":"markdown","a0dc85c2":"markdown","b67bf525":"markdown","47ae2a94":"markdown","2a46f4f0":"markdown","a648f5c0":"markdown","097521df":"markdown","8ae61f61":"markdown","fcd57095":"markdown","9112e270":"markdown","b5da01a4":"markdown","03399bbd":"markdown","fe286c32":"markdown","ae138e46":"markdown","e1540473":"markdown","77342634":"markdown","a00c95a6":"markdown","aba0e640":"markdown","8eef1337":"markdown","466dcb5c":"markdown","644a28c6":"markdown","2046f7a0":"markdown","170e3f4a":"markdown","d3e79ebd":"markdown","f289275a":"markdown"},"source":{"a36d4338":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix,f1_score, make_scorer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, f1_score, classification_report\nfrom sklearn.model_selection import GridSearchCV, GroupShuffleSplit\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis","de7ef102":"df = pd.read_csv('..\/input\/noshowappointments\/KaggleV2-May-2016.csv')\ndf.head(3)","ba44d940":"df.shape","11ede3ba":"df.info()","ebb0a7bb":"# Check duplicates\n\ndf.duplicated().sum()","a65d646e":"# Dataset has no null values.\n# \"PatientId\" column's datatype is float and not approvable.\n# Convert \"PatientId\" column into integer\n\ndf.PatientId = df.PatientId.astype('int64')\ndf.PatientId.dtypes","b7352332":"# \"No-show\" column seems in string format.\n# For converting them to boolean, show the unique values.\n\ndf[\"No-show\"].unique()","da5a298d":"# Rename 'No-show' column as 'Show' and values to 1 or 0\ndf = df.rename(columns={\"No-show\": \"Show\"})\ndf['Show'] = df['Show'].replace({'No': 1, 'Yes': 0})","db11fdef":"# Rename 'Gender' column as 'IsMale' and values to 1 or 0\ndf = df.rename(columns={\"Gender\": \"IsMale\"})\ndf['IsMale'] = df['IsMale'].replace({'F': 0, 'M': 1})","0f40cb10":"df.describe()","cea2dc32":"# Convert \"ScheduledDay\" and \"AppointmentDay\" columns into date-time\n\ndf.ScheduledDay = pd.to_datetime(df.ScheduledDay)\ndf.AppointmentDay = pd.to_datetime(df.AppointmentDay)\ndf[['ScheduledDay','AppointmentDay']].dtypes","ee24d5d5":"# Get time columns from date-time columns\ndf['ScheduledHour'] = pd.to_datetime(df.ScheduledDay).dt.hour\ndf['AppointmentTime'] = pd.to_datetime(df.AppointmentDay).dt.time\n\n# Convert time included date-time columns to only date columns\ndf['ScheduledDay'] = df['ScheduledDay'].dt.date\ndf['AppointmentDay'] = df['AppointmentDay'].dt.date\n\n# Get month and week of day information from dates\ndf['AppointmentWeekDay'] = pd.to_datetime(df.AppointmentDay).dt.day_name()\ndf['AppointmentMonth'] = pd.to_datetime(df.AppointmentDay).dt.month_name()","894606a0":"# Calculate waiting days before appointment\ndf['WaitingDays'] = df.AppointmentDay - df.ScheduledDay\n\n# Convert datatype to int\ndf.WaitingDays = df.WaitingDays.astype('str')\ndf.WaitingDays = df.WaitingDays.apply(lambda x: x.split()[0])\ndf.WaitingDays = df.WaitingDays.astype('int64')","592acf4c":"# I will scale 'Age', 'WaitingDays', 'ScheduledHour' column with standard scaler\n\nstd=StandardScaler()\n\ncolumns = ['Age', 'WaitingDays', 'ScheduledHour']\nnew_columns = ['AgeScaled', 'WaitingDaysScaled', 'ScheduledHourScaled']\nscaled = std.fit_transform(df[columns])\nscaled = pd.DataFrame(scaled, columns=new_columns, index= df.index.values)\n\ndf = df.merge(scaled, left_index=True, right_index=True, how = \"left\")\nprint(df.shape)","018c198e":"df.head(3)","e459e8c4":"# \"AppointmentTime\" seems same in each rows.\n# Is it really same in all dataset?\n\ndf.AppointmentTime.nunique()","b5160aa4":"# \"AppointmentTime\" has only one unique value.\n# Drop the \"AppointmentTime\" column\n\ndf.drop(columns='AppointmentTime',inplace=True)","c5f618d8":"# Negative values in \"age\" column is not possible.\n# 115 for max age is not common but possible.\n# Remove rows which have age less than 0\n\ndf = df[df[\"Age\"] >= 0]","ca3f853e":"data = df.Age\nplt.figure(figsize=(5,5),dpi=70)\nbp = plt.boxplot(data,patch_artist = True,notch ='True');\n\nfor whisker in bp['whiskers']:\n    whisker.set(linewidth = 2,\n                linestyle =\"-\")\n    \nfor cap in bp['caps']:\n    cap.set(linewidth = 2)\n    \nfor median in bp['medians']:\n    median.set(linewidth = 4)\n    \nplt.ylabel(\"Age\",size=12)\nplt.title(\"\\nBox plot of 'Age' column\\n\",size=15);","2941e1c9":"# Control for \"Is there any scheduled day after appointment day?\" question.\n# As you know, this is impossible.\n\ndf_error = df[df.ScheduledDay > df.AppointmentDay]\ndf_error","46361e59":"# Remove this rows from dataset\n\ndf = df[~(df.ScheduledDay > df.AppointmentDay)]","745f9309":"# Rename some columns for easier coding\n\nlabels = {'PatientId':'PatientID', 'Hipertension':'Hypertension',\n          'Handcap':'Handicap', 'SMS_received':'SMSreceived'}\ndf.rename(columns=labels,inplace=True)","931b056c":"df.sample(3)","dc5ec516":"# Correlation of values with each other\ncorrelation = df.corr()\nfig, axes = plt.subplots(figsize=(7,7))\nsns.heatmap(correlation, vmax=.8, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10});","79acd6ea":"# Visualize age and missing appointment correlation.\nplt.figure(figsize=(10, 5))\n\nsns.distplot(df[df['Show'] == 1][\"Age\"], bins=max(df['Age']), kde=False)\nsns.distplot(df[df['Show'] == 0][\"Age\"], bins=max(df['Age']), color='red', kde=False)\n\nplt.title('Age Distribution and Missing Appointment Status by Age', fontsize=15)\nplt.xlim([min(df['Age']),max(df['Age'])])\nplt.show()","22c29598":"# Visualize age and stroke correlation\nplt.figure(figsize=(10, 5))\n\nsns.distplot(df[df['Show'] == 0][\"Age\"], bins=max(df['Age']), kde=True)\nsns.distplot(df[df['Show'] == 1][\"Age\"], bins=max(df['Age']), color='red', kde=True)\n\nplt.title('Age Distributions in Stroke Negative vs Stroke Positinve Patient Groups', fontsize=15)\nplt.xlim([min(df['Age']),max(df['Age'])])\nplt.show()","d2f70a6d":"fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, sharey=True)\nsns.countplot(x='IsMale', data=df, hue='Show', ax=ax1, palette='magma');\nsns.countplot(x='SMSreceived', data=df, hue='Show', ax=ax2, palette='magma');\nsns.countplot(x='Scholarship', data=df, hue='Show', ax=ax3, palette='magma');\nfig.set_figwidth(12)\nfig.set_figheight(4)","1bb3450a":"fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, sharey=True)\nsns.countplot(x='Hypertension', data=df, hue='Show', ax=ax1, palette='magma');\nsns.countplot(x='Diabetes', data=df, hue='Show', ax=ax2, palette='magma');\nsns.countplot(x='Alcoholism', data=df, hue='Show', ax=ax3, palette='magma');\nfig.set_figwidth(12)\nfig.set_figheight(4)","cb0414fd":"# Handicap and Show relationship\nfig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False)\nsns.countplot(x='Handicap', data=df, hue='Show', ax=ax1, palette='magma')\nsns.countplot(x='Handicap', data=df[-(df.Handicap == 0)], hue='Show', ax=ax2, palette='magma')\nfig.set_figwidth(15)\nfig.set_figheight(5)","77c2b4b9":"# Analyze the handicap\n\n#Create empty dataframe\nhandicap = pd.DataFrame()\n\nhandicap['total_dow'] = df.groupby('Handicap').Show.count()\nhandicap['show_dow'] = df.groupby('Handicap').Show.sum()\nhandicap['show_prop'] = df.groupby('Handicap').Show.mean()\nhandicap.reset_index(inplace=True)\n\nhandicap","547b4d6e":"# AppointmentWeekDay and Show relationship\nsns.countplot(x='AppointmentWeekDay', data=df, hue='Show', palette='magma')\nfig.set_figwidth(15)\nfig.set_figheight(5)","7557126b":"# ScheduledHour and Show relationship\nsns.countplot(x='ScheduledHour', data=df, hue='Show', palette='magma')\nfig.set_figwidth(15)\nfig.set_figheight(5)","b61e0c68":"# Visualize ScheduledHour and Show correlation\nplt.figure(figsize=(10, 5))\n\nsns.distplot(df[df['Show'] == 0][\"ScheduledHour\"], bins=max(df['ScheduledHour']), kde=True)\nsns.distplot(df[df['Show'] == 1][\"ScheduledHour\"], bins=max(df['ScheduledHour']), color='red', kde=True)\n\nplt.title('ScheduledHour Distributions in Show Status', fontsize=15)\nplt.xlim([min(df['ScheduledHour']),max(df['ScheduledHour'])])\nplt.show()","847199c9":"# Analyze the week of days\n\n#Create empty dataframe\nappointment_day_of_week = pd.DataFrame()\n\nappointment_day_of_week['total_dow'] = df.groupby('AppointmentWeekDay').Show.count()\nappointment_day_of_week['show_dow'] = df.groupby('AppointmentWeekDay').Show.sum()\nappointment_day_of_week['show_prop'] = df.groupby('AppointmentWeekDay').Show.mean()\nappointment_day_of_week.reset_index(inplace=True)\n\nappointment_day_of_week","e8df5359":"# AppointmentMonth and Show relationship\nsns.countplot(x='AppointmentMonth', data=df, hue='Show', palette='magma')\nfig.set_figwidth(15)\nfig.set_figheight(5)","ba2e4388":"# Visualize WaitingDays and Show ratio correlation\nplt.figure(figsize=(10, 5))\n\nsns.distplot(df[df['Show'] == 0][\"WaitingDays\"], bins=max(df['WaitingDays']), kde=True)\nsns.distplot(df[df['Show'] == 1][\"WaitingDays\"], bins=max(df['WaitingDays']), color='red', kde=True)\n\nplt.title('WaitingDays Distributions in Missing and Non-Missing Patient Groups', fontsize=15)\nplt.xlim([min(df['WaitingDays']),max(df['WaitingDays'])])\nplt.show()","12b41c60":"# Analyze the neighbourhoods\n\n#Create empty dataframe\nneighbour_df = pd.DataFrame()\n\nneighbour_df['total_appointments'] = df.groupby(by='Neighbourhood').Show.count()\nneighbour_df['show_appointments'] = df.groupby(by='Neighbourhood').Show.sum()\nneighbour_df['show_prop'] = df.groupby(by='Neighbourhood').Show.mean()\nneighbour_df.reset_index(inplace=True)\n\n# Sort by descending and show top 10\nhigh_appointment_neigh = neighbour_df.sort_values('total_appointments',ascending=False).head(10)\nhigh_appointment_neigh","73a7083d":"df.columns","caaec703":"# Drop 'AppointmentID' column, it is unnecessary\n# (We will use 'PatientID' column for preventing data leakage when \n#  splitting the data for train and test)\ndf = df.drop(columns='AppointmentID', axis=1)\n\n# Drop 'AppointmentDay', 'ScheduledDay' groups as we created features from them\ndf = df.drop(columns=['AppointmentDay', 'ScheduledDay'], axis=1)","2d31e81a":"# All unique values and value counts of every column.\nfor column in df.columns:\n    print(\"\\n\" + column)\n    print(df[column].value_counts())","7aaebdff":"# One-hot encode 'Neighbourhood','Handicap', 'AgeGroup', \n# 'AppointmentWeekDay', 'AppointmentMonth', 'WaitingDaysGroup' columns\ndf1 = pd.get_dummies(df, columns=['Neighbourhood','Handicap', 'AppointmentWeekDay', 'AppointmentMonth'], prefix=['Neighbourhood','Handicap', 'AppointmentWeekDay', 'AppointmentMonth'])","d2568bcc":"# Drop 'Show' from predictors dataset\nX = df1.drop('Show', axis=1)\n# Drop 'Age', 'WaitingDays', 'ScheduledHour' features as we scaled them and we will use scaled features.\nX = X.drop(columns = ['Age', 'WaitingDays', 'ScheduledHour'], axis=1)\n\ny = df1.Show","c91461d4":"gs = GroupShuffleSplit(n_splits=2, test_size=.25, random_state=0)\ntrain_ix, test_ix = next(gs.split(X, y, groups=X.PatientID))","db57bd6c":"len(train_ix)","268a5969":"X_train = X.iloc[train_ix]\nX_train = X_train.drop('PatientID',1)\ny_train = y.iloc[train_ix]\n\nX_test = X.iloc[test_ix]\nX_test = X_test.drop('PatientID',1)\ny_test = y.iloc[test_ix]","0e919a3b":"print(\"Train set data shape: \" + str(X_train.shape))\nprint(\"Test set data shape: \" + str(X_test.shape))","1b637b40":"# Modeling step: Test differents algorithms \nrandom_state = 42\n\nclassifiers = []\n#classifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LinearDiscriminantAnalysis())\nclassifiers.append(CatBoostClassifier(random_state=random_state))\nclassifiers.append(lgb.LGBMClassifier(random_state=random_state))\n\nresults = []\nfor classifier in classifiers :\n    print(classifier)\n\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    \n    f1 = f1_score(y_test, y_pred)\n    acc = accuracy_score(y_test, y_pred)\n    #precision = precision_score(y_test, y_pred)\n    #recall = recall_score(y_test, y_pred)\n    roc_auc = roc_auc_score(y_test, y_pred)\n    \n    results.append({\"classifier\":classifier, \"f1\":f1, \"acc\":acc, \"roc_auc\":roc_auc})\n\nf1_list = []\nacc_list = []\nauc_list = []\nfor result in results:\n    f1_list.append(result.get('f1'))\n    acc_list.append(result.get('acc'))\n    auc_list.append(result.get('roc_auc'))\n\nres_df = pd.DataFrame({\"f1\":f1_list,\"acc\": acc_list,\"auc\":auc_list,\"Algorithm\":[\n    #\"SVC\",\n    \"DecisionTree\",\n    \"AdaBoost\",\n    \"RandomForest\",\n    \"ExtraTrees\",\n    \"GradientBoosting\",\n    \"MultipleLayerPerceptron\",\n    \"KNeighboors\",\n    \"LinearDiscriminantAnalysis\",\n    \"CatBoostClassifier\",\n    \"LGBMClassifier\"\n]})","f46c6821":"# Results of all models:\nres_df","49889364":"g = sns.barplot(\"f1\",\"Algorithm\", data = res_df, palette=\"Set3\",orient = \"h\")\ng.set_xlabel(\"f1-Score\")\ng = g.set_title(\"f1-scores\")","04e78f12":"g = sns.barplot(\"acc\",\"Algorithm\", data = res_df, palette=\"Set3\",orient = \"h\")\ng.set_xlabel(\"Accuracy Score\")\ng = g.set_title(\"Accuracy Scores\")","ef81cd8a":"# sort model predictions by accuracy values\nres_df.sort_values(\"acc\", ascending=False).head(3)","2b201e5f":"# sort model predictions by f1-score values\nres_df.sort_values(\"f1\", ascending=False).head(3)","bb13c02b":"gb_clf = GradientBoostingClassifier(random_state=random_state)\ngb_clf.fit(X_train, y_train)\n\ncb_clf = CatBoostClassifier(random_state=random_state)\ncb_clf.fit(X_train, y_train)\n\nlgbm_clf = lgb.LGBMClassifier(random_state=random_state)\nlgbm_clf.fit(X_train, y_train)\n\ny_perd_gb = gb_clf.predict(X_test)\ny_perd_cb = cb_clf.predict(X_test)\ny_perd_lgbm = lgbm_clf.predict(X_test)   ","2d39bb6c":"cm_gb = confusion_matrix(y_test, y_perd_gb)\ncm_cb = confusion_matrix(y_test, y_perd_cb)\ncm_lgbm = confusion_matrix(y_test, y_perd_lgbm)\n\nf, axes = plt.subplots(1, 3, figsize=(15, 4))\nf.suptitle('Confusion Matrixes of Models')\n\nsns.heatmap(cm_gb, annot=True,fmt='d',cmap=\"flare\", ax=axes[0]);\naxes[0].set_title(\"GradientBoostingClassifier\")\nsns.heatmap(cm_cb, annot=True,fmt='d',cmap=\"flare\", ax=axes[1]);\naxes[1].set_title(\"CatBoostClassifier\")\nsns.heatmap(cm_lgbm, annot=True,fmt='d',cmap=\"flare\",  ax=axes[2]);\naxes[2].set_title(\"LGBMClassifier\")","9245ffb1":"# Create ensemble classifier object with VotingClassifier()\n\nvotingC = VotingClassifier(estimators=[('gb_clf', gb_clf),\n                                       ('cb_clf', cb_clf),\n                                       ('lgbm_clf',lgbm_clf)], voting='soft', n_jobs=8)\n\nvotingC = votingC.fit(X_train, y_train)\ny_pred_ensemble = votingC.predict(X_test)","b93cd2f9":"def report(y_test, y_pred):\n    \"\"\"\n    Creates confusion matrix and classification report for model evaluation\n    \n    Args:\n    y_test: splitted test dataframe labels\n    y_pred: predictions for test dataframe\n    \n    Returns:\n    None\n    \"\"\"\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True,fmt='d',cmap=\"flare\");\n\n    print(classification_report(y_test,y_pred))","227ed344":"report(y_test, y_pred_ensemble)","fe9a74d0":"for name, importance in zip(X.columns, np.sort(gb_clf.feature_importances_)[::-1][:8]):\n    print(\"{} -- {:.2f}\".format(name, importance))","78b7a842":"for name, importance in zip(X.columns, np.sort(cb_clf.feature_importances_)[::-1][:8]):\n    print(\"{} -- {:.2f}\".format(name, importance))","e73faad5":"for name, importance in zip(X.columns, np.sort(lgbm_clf.feature_importances_)[::-1][:8]):\n    print(\"{} -- {:.2f}\".format(name, importance))","25f4c1df":"All of the models could be trained with more detailed parameters.\nGridSearch parameters I chosen were improper, I could not get result. These parameters can be improved.\nSVM, deep learning methods can also be tried when there is a large time.\nAlso, there were some data limitations. If data providers publish the new dataset model training can be done again.","c4cef54a":"# My Medium blog post about data analysis of this project","305881fc":"According to f1-score and accuracy, our best model solution is ensembling of \n\n**Best model:**  Ensembling model ,  **f1-score:**  0.89, **Accuracy:** 0.80\n\nBest model feature importances are:  (Print only top 8 for each-one)","d02c4dd6":"#### Converting binary value columns datatype from string to integer 1-0","604dbf1c":"# Limitations","5579cbff":"* Most of the appointments were taken by individuals without disabilities. \n* Most common handicap is 1 and, 3-4 are very rare.\n* Handicap 4 misses appointments the most.","4eae31e4":"### Model Selection - Implementation","b3d7cc1d":"#### Scaling Continious Values","a5212ff3":"Accuracy value can be misleading for unbalanced datasets, as our dataset. f1-score gives better insight such this situations.\n\nLet's rank models by f1-score, then ensemble best three model.","b852db3c":"# Results","b77c9d93":"If different appointments of the same patient take place in both train, test groups at the same time, data leakage may occur. \n\nSo we have to take that into consideration when splitting data. ","b42792ce":"**Which Patient will not Come to the Appointment? | End-to-End Machine Learning Code Example**\n\nSolution to a daily life problem guided by Medical Appointment No Shows data from 2016\n\nhttps:\/\/medium.com\/@sahika.betul\/which-patient-will-not-come-to-the-appointment-end-to-end-machine-learning-code-example-e952f65888ac","0b0f61fe":"The ratio not much differs between two groups","1ae7c3df":"Best **f1-score improved** from 0.88 to 0.89\n\nBest **accuracy improved** from 0.79 to 0.80\n\nOur ensemble model is better than all of the models by two metrics.","43a0dc0e":"A person makes a doctor appointment, receives all the instructions and no-show. Could we predict this patients before the appointment?\n\n\"The Medical-Appointment-No-Shows\" dataset includes details of appointments of Brazilian people with the doctor. Data of appointments was combined from more than 45 clinics and hospitals at municipal level in Brazil betweeen 29\/4\/2016 and 6\/6\/2016.\n\n**Problem:** Many patients book the appointment with doctor then didn't show up on scheduled day. After so many studies happening on this data the average No-Show is 20%. The city has to lose 20 million every year due to patients not showing at their scheduled appointment.\n\n**Objective of the analysis:** Investigate the reason why some patients do not show up to their scheduled appointments and find the probability of missing the appointment.","ee2c7e8a":"* Data collected between 29\/4\/2016 and 6\/6\/2016. If data had been collected over a period of more than one year, it would better reflect time-dependent changes. \n* The appointment time was on the ScheduledDay, but it was not in the AppointmentDay. During busy times of the day, the probability of missing an appointment may be higher. \n* Doctor's specialty may also be correlated with missed appointment. \n* Distance between the hospital and destination, insurance situation, education level would be helpful for better analysis.","8b36d29b":"Retrain best three models","43f11d43":"### Data Preprocessing","85d61195":"* Until 4 days, not missing ratio is higher.\n* After 4 days, missing ratio is higher.","73c0dc18":"# Data Understanding","79426fe8":"### Reflection","39af3299":"Heatmap shows three correlations:\n\n1. Hypertension-Age\n2. Hypertension-Diabetes\n3. Diabetes-Age\n\nThese are medically expected results.\n\nThere is no strong correlation between any feature with no-show.","2df1b457":"#### Time transformations","f634f53f":"* Females make appointments more than males.\n* Suprisingly, no-show ratio is high in SMS received group than un-received group.","ea349525":"* Show proportion between the neighbourhood is not differs very much.\n* Most of the patients from JARDIM CAMBURI","a1f20225":"#### Detecting Unnecessary Columns","079927ae":"* Controlling null values\n* Converting binary value columns datatype from string to integer 1-0\n* Detecting unrealistic values\n","729006c5":"### Justification","90460866":"We made the dataset clean by doing the preprocessing step meticulously. Afterwards, we tried the 10 most preferred machine learning models and combined the forces of the three that gave the best results. With this approach, we created a supermodel that produces better results than all models.","11b6778b":"# Exploratory Data Analysis","754fdb07":"#### Detecting unrealistic values","9fbd6508":"### Model Evaluation and Validation","1d0f6475":"### Improvement","dcd0b094":"# Business Understanding","a0dc85c2":"## Data Splitting","b67bf525":"# Methodology","47ae2a94":"# Prepare Data","2a46f4f0":"We have 110.527 medical appointments and its 13 associated variables (characteristics) for predicting show-up status.","a648f5c0":"* Data collected between end of the April and beginning of the May. this chart does not give information in terms of the difference between the months.","097521df":"110.527 medical appointments and its 14 associated variables (characteristics). The most important one if the patient show-up or no-show to the appointment.\n\n**Attribute Information**\n\n1. **PatientId:** Identification of a patient\n2. **AppointmentID:** Identification of each appointment\n3. **Gender:** Male or Female.\n4. **AppointmentDay:** The day of the actual appointment, when they have to visit the doctor.\n5. **ScheduledDay:** The day someone called or registered the appointment, this is before appointment of course.\n6. **Age:** How old is the patient.\n7. **Neighbourhood:** Where the appointment takes place.\n8. **Scholarship:** True of False . Observation, this is a broad topic, consider reading this article https:\/\/en.wikipedia.org\/wiki\/Bolsa_Fam%C3%ADlia\n9. **Hipertension:** True or False\n10. **Diabetes:** True or False\n11. **Alcoholism:** True or False\n12. **Handcap:** True or False\n13. **SMS_received:** 1 or more messages sent to the patient.\n14. **No-show:** True or False.\n\n**Dataset URL**\nhttps:\/\/www.kaggle.com\/joniarroba\/noshowappointments","8ae61f61":"\"ScheduledDay\" and \"AppointmentDay\" columns' datatypes are string. I will convert them into date-time format for using to get better analysis. We can extract the date, day of the week, month and time from the date-time format.","fcd57095":"**EDA**\n\n* Between the ages of 10 and 30, missing an appointment is more common than not.\n* Showing rate for men and women are similar.\n* Hypertension is an important factor for higher attendance frequency.\n* Suprisingly, no-show ratio is high in SMS received group than un-received group.\n* There is no significant difference between the days of the week in terms of not missing an appointment.\n\n**Model Predictions**\n\nTo bring the model to the prediction stage, we first did preprocessing: \n* Controlling null values\n* Converting binary value columns datatype from string to integer 1-0,\n* Time transformations\n* Scaling Continious Values\n* Detecting Unnecessary Columns\n* Detecting unrealistic values\n\nIn the modeling phase, we first trained **10 different classification models**. We chose the three best models, considering the f1-score metrics since our dataset is unbalanced:\n* GradientBoosting\n* LGBMClassifier\n* CatBoostClassifier\n\nBy ensembling, we built the super-model, which is better than what we have in terms of accuracy and f1-score.\n\nWe also determined the features that the model gives the most weight. **Gender, Age, Neighbourhood, Scholarship and Hypertension** are some of the top features that would help us determine if the patient who has taken an appointment will Show\/NoShow.","9112e270":"* Missing ratio increases between 10-30 age group.","b5da01a4":"115 is an outlier 'Age' value.\nI will scale age column with standard scaler later, with waiting day feature.\n","03399bbd":"I will add new columns from ScheduledDay and AppointmentDay:\n\n* **ScheduledHour:** hour of Scheduled Day\n* **AppointmentTime:** time of Appointment Day\n* **ScheduleDate:** date of Scheduled Day\n* **AppointmentDate:** date of Appointment Day\n* **AppointmentWeekDay:** the day of the week of Appointment Day\n* **AppointmentMonth:** the month of Appointment Day\n* **AppointmentMonth:** the month of Appointment Day\n* **WaitingDays:** waiting days before Appointment Day","fe286c32":"* It is rare for patients with hypertension to not go to an appointment.\n* The graphs above show that hypertension is more common than diabetes in the patients participating in the study. ","ae138e46":"# Conclusion","e1540473":"Complication that occurred during the training process was training of SVC model. It took too long. So, I comment that line. If you have much time, uncomment it.\n\nLet's see the results and visualize the model metrics.","77342634":"Firstly, I compared 11 popular classifiers. In the \"improvements\" section, I will ensemble best 3 models by f1-score and best 3 models by accuracy. (I will explain why I use this metrics in the same section.) \n\n* Decision Tree                              (default parameter settings)\n* AdaBoost                                   (default parameter settings)\n* Random Forest                              (default parameter settings)\n* Extra Trees                                (default parameter settings)\n* Gradient Boosting                          (default parameter settings)\n* Multiple layer perceprton (neural network) (default parameter settings)\n* KNN                                        (default parameter settings)\n* Logistic regression                        (default parameter settings)\n* Linear Discriminant Analysis               (default parameter settings)","a00c95a6":"# Load Libraries","aba0e640":"### Refinement","8eef1337":"#### Controlling null values","466dcb5c":"The success of our models is not very good considering the accuracy and especially f1-score values.\n\n**Ensemble methods** usually produces more accurate solutions than a single model would. I will use our best three models for model ensembling:\n\n* GradientBoosting\n* LGBMClassifier\n* CatBoostClassifier\n\nBefore the ensembling step, I performed a **grid search** optimization for these models. Grid search refers to a technique used to identify the optimal hyperparameters for a model. Unlike parameters, finding hyperparameters in training data is unattainable. As such, to find the right hyperparameters, we create a model for each combination of hyperparameters.","644a28c6":"* Appointments are less on Friday.\n* No significant difference on weekdays for missing appointment.","2046f7a0":"# Load Data","170e3f4a":"#### Ensembling","d3e79ebd":"#### Medical Appointment No-Shows Dataset","f289275a":"All three models of ensembling have same feature importance order. So, ensembling model has same order too.\n\n**The most important factor is gender.** Scholarship, chronic diseases, alcoholism and SMS receiving status are affects also."}}