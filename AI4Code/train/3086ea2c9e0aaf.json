{"cell_type":{"5e348f47":"code","ae720800":"code","552bf8cb":"code","813cd6ec":"code","d1e7e12c":"code","6fed79c2":"code","8c2881b4":"code","003b2000":"code","e33b6132":"code","7b3ddbc1":"code","985c3e94":"code","0140003b":"code","91d9f1bf":"code","1f6ab113":"code","7fd5c4fb":"code","b91a4de7":"code","29130a7c":"code","00899c73":"code","18bc4310":"code","c0aefd96":"code","54e1748d":"code","c3f75d9e":"code","0cba5bb8":"code","4fa073ec":"code","ea999213":"code","bc7196ce":"code","66920c9c":"code","342b711a":"code","298c3697":"code","9090434e":"code","c5ee76a4":"code","30cc3e08":"code","c712a9cb":"code","62511d20":"code","c6704320":"code","6b764958":"code","0abf8304":"code","47d96be9":"code","35b2acba":"code","b967058b":"code","df1e124c":"markdown"},"source":{"5e348f47":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ae720800":"train=pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')","552bf8cb":"print('Train Size:',train.shape)\nprint('Test Size:',test.shape)","813cd6ec":"train.head()","d1e7e12c":"test.head()","6fed79c2":"numeric_data=train.select_dtypes(exclude='object').drop(['SalePrice','Id'],axis=1).copy()\nnumeric_data.head()\n\ncategorical_data=train.select_dtypes(include='object')\ncategorical_data.head()","8c2881b4":"#Distribution plot\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig=plt.figure(figsize=(20,30))\nfor i in range(len(numeric_data.columns)):\n    fig.add_subplot(9,4,i+1)\n    sns.distplot(numeric_data.iloc[:,i].dropna(), rug=True, hist=True, kde_kws={'bw':0.1})\n    plt.xlabel(numeric_data.columns[i])\nplt.tight_layout()\nplt.show()","003b2000":"#Boxplot(Univariate Analysis)\nfig=plt.figure(figsize=(10,15))\nfor i in range(len(numeric_data.columns)):\n    fig.add_subplot(9,4,i+1)\n    sns.boxplot(y=numeric_data.iloc[:,i])\nplt.tight_layout()\nplt.show()","e33b6132":"#Count plot (categorical, univariate analysis)\nfig=plt.figure(figsize=(18,20))\nfor i,col in enumerate(categorical_data):\n    fig.add_subplot(11,4,i+1)\n    sns.countplot(train[col])\n    plt.xlabel(col)\n    plt.xticks(rotation=90)\nplt.tight_layout(pad=1)\nplt.show()","7b3ddbc1":"#Scatterplot(bivariate)\nfig=plt.figure(figsize=(20,30))\nfor i in range(len(numeric_data.columns)):\n    fig.add_subplot(9,4,i+1)\n    sns.scatterplot(numeric_data.iloc[:,i],train['SalePrice'])\nplt.tight_layout()\nplt.show()","985c3e94":"#Correlation\nnum=train.select_dtypes(exclude='object').drop('Id',axis=1)\nnumeric_correlation=num.corr()\nplt.figure(figsize=(10,10))\nplt.title('Correlation')\nsns.heatmap(numeric_correlation>0.8, annot=True, square=True)","0140003b":"print(numeric_correlation['SalePrice'].sort_values(ascending=False))","91d9f1bf":"#dropping features due to high correlation\ntrain.drop(['GarageYrBlt','TotRmsAbvGrd','GarageCars','1stFlrSF','YearBuilt'],axis=1,inplace=True)\ntest.drop(['GarageYrBlt','TotRmsAbvGrd','GarageCars','1stFlrSF','YearBuilt'],axis=1,inplace=True)","1f6ab113":"#no linear relationship with SalePrice\ntrain.drop(['MSSubClass','OverallCond','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','Fireplaces','MoSold','YrSold'],axis=1,inplace=True)\ntest.drop(['MSSubClass','OverallCond','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','Fireplaces','MoSold','YrSold'],axis=1,inplace=True)","7fd5c4fb":"#how many missing values\ntrain.isnull().mean().sort_values(ascending=False)","b91a4de7":"#train=train.dropna(thresh=train.shape[0]*0.50, axis=1)\n'''for col in train.columns: \n    if sum(train[col].isnull())\/float(len(train.index))>0.9:\n        del train[col]'''\n'''del_col=[col for col in train.columns if sum(train[col].isnull())\/float(len(train.index))>0.9]\ntrain.drop(del_col,axis=1,inplace=True)'''","29130a7c":"#drop columns with high no. of null values\ntrain.drop(['PoolQC','MiscFeature','Alley','Fence'],axis=1,inplace=True)\ntest.drop(['PoolQC','MiscFeature','Alley','Fence'],axis=1,inplace=True)","00899c73":"#drop categorical columns with mostly a single value in it, in this case more than 95%\nremove_col=[]\ncategorical_data=train.select_dtypes(include='object').columns\nfor i in categorical_data:\n    total_count=train[i].value_counts()\n    zero_count=total_count.iloc[0]\n    if zero_count\/len(train)*100 > 95:\n        remove_col.append(i)\n\nprint(remove_col)\ntrain.drop(remove_col,axis=1,inplace=True)\ntest.drop(remove_col,axis=1,inplace=True)","18bc4310":"#drop numeric columns\nremove_col=[]\nnumeric_data=train.select_dtypes(exclude='object').columns\nfor i in numeric_data:\n    total_count=train[i].value_counts()\n    zero_count=total_count.iloc[0]\n    if zero_count\/len(train)*100 > 95:\n        remove_col.append(i)\n\nprint(remove_col)\ntrain.drop(remove_col,axis=1,inplace=True)\ntest.drop(remove_col,axis=1,inplace=True)","c0aefd96":"#Identifying outliers\nnumeric_data=train.select_dtypes(exclude='object').drop(['Id','SalePrice'],axis=1).copy()\nfig=plt.figure(figsize=(10,15))\nfor i in range(len(numeric_data.columns)):\n    fig.add_subplot(9,4,i+1)\n    sns.boxplot(y=numeric_data.iloc[:,i])\nplt.tight_layout()\nplt.show()","54e1748d":"#remove outliers\ntrain=train.drop(train[train['LotFrontage']>200].index)\ntrain=train.drop(train[train['LotArea']>100000].index)\ntrain=train.drop(train[train['BsmtFinSF1']>4000].index)\ntrain=train.drop(train[train['TotalBsmtSF']>4000].index)\ntrain=train.drop(train[train['GrLivArea']>4000].index)\ntrain=train.drop(train[train['EnclosedPorch']>400].index)","c3f75d9e":"#Missing Values\npd.DataFrame(test.isnull().sum(), columns=['sum']).sort_values(by=['sum'],ascending=False).head(51)","0cba5bb8":"test.head()","4fa073ec":"num=train.select_dtypes(exclude='object').drop(['SalePrice'],axis=1).columns\ncat=train.select_dtypes(include='object').columns","ea999213":"#changing NA in numerical features to their mean\nfor i in train,test:\n    for j in num:\n        i[j]=i[j].fillna(i[j].mean())\n        \ntrain['MasVnrArea']=train['MasVnrArea'].fillna(0)\ntest['MasVnrArea']=test['MasVnrArea'].fillna(0)","bc7196ce":"#changing NA in categorical features to 'None'\nordinal_features=['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1','BsmtFinType2', 'Neighborhood', 'BldgType', 'HouseStyle', 'MasVnrType', 'FireplaceQu']\ncategorical_features=['MSZoning','LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'Condition1', 'RoofStyle','Electrical', 'Functional', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'ExterQual', 'ExterCond','Foundation', 'HeatingQC', 'CentralAir', 'PavedDrive', 'SaleCondition']\n\nfor i in train,test:\n    for j in ordinal_features:\n        i[j]=i[j].fillna('None')\n\nfor i in train,test:\n    for j in categorical_features:\n        i[j]=i[j].fillna(i[j].mode()[0])","66920c9c":"pd.DataFrame(test.isnull().sum(),columns=['Sum']).sort_values(by='Sum',ascending=False).head(10)","342b711a":"train.columns","298c3697":"#Feature Engineering\n\ntrain['TotalLot'] = train['LotFrontage'] + train['LotArea']\ntrain['TotalBsmtFin'] = train['BsmtFinSF1'] + train['BsmtFinSF2']\ntrain['TotalSF'] = train['TotalBsmtSF'] + train['2ndFlrSF']\ntrain['TotalPorch'] = train['OpenPorchSF'] + train['EnclosedPorch'] + train['ScreenPorch']\n\ntest['TotalLot'] = test['LotFrontage'] + test['LotArea']\ntest['TotalBsmtFin'] = test['BsmtFinSF1'] + test['BsmtFinSF2']\ntest['TotalSF'] = test['TotalBsmtSF'] + test['2ndFlrSF']\ntest['TotalPorch'] = test['OpenPorchSF'] + test['EnclosedPorch'] + test['ScreenPorch']","9090434e":"#changing columns to binary to check if that feature is there in the house, 1(yes), 0(no)\nbin_columns = ['MasVnrArea','TotalBsmtFin','TotalBsmtSF','2ndFlrSF','WoodDeckSF','TotalPorch']\nfor i in bin_columns:\n    col=i+'_bin'\n    train[col] = train[i].apply(lambda x: 1 if x>0 else 0)\n    test[col] = test[i].apply(lambda x: 1 if x>0 else 0)","c5ee76a4":"train.head()","30cc3e08":"plt.figure(figsize=(10,6))\nplt.title(\"Distrubution of SalePrice\")\ndist = sns.distplot(train['SalePrice'],norm_hist=False)","c712a9cb":"plt.figure(figsize=(10,6))\nplt.title(\"Distrubution of SalePrice\")\ndist = sns.distplot(np.log(train['SalePrice']),norm_hist=False)","62511d20":"from sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom catboost import CatBoostRegressor","c6704320":"from sklearn.model_selection import train_test_split\n\nx = train.drop(['SalePrice'], axis=1) \ny = np.log1p(train['SalePrice'])\nX_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=1)\n\ncategorical_cols = [cname for cname in x.columns if\n                    x[cname].nunique() <= 30 and\n                    x[cname].dtype == \"object\"] \n                \n\n\nnumerical_cols = [cname for cname in x.columns if\n                 x[cname].dtype in ['int64','float64']]\n\n\nmy_cols = numerical_cols + categorical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test = test[my_cols].copy()","6b764958":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nnum_transformer = Pipeline(steps=[\n    ('num_imputer', SimpleImputer(strategy='constant'))\n    ])\n\ncat_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, numerical_cols),       \n        ('cat',cat_transformer,categorical_cols),\n        ])","0abf8304":"# Reversing log-transform on y\ndef inv_y(transformed_y):\n    return np.exp(transformed_y)\n\nn_folds = 10","47d96be9":"# XGBoost\nmodel = XGBRegressor(learning_rate=0.01, n_estimators=3460, max_depth=3, min_child_weight=0,gamma=0, subsample=0.7,colsample_bytree=0.7,objective='reg:squarederror', nthread=-1,scale_pos_weight=1, seed=27, reg_alpha=0.00006)\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)])\nclf.fit(X_train, y_train)\npredict = clf.predict(X_val)\nprint('XGBoost: ' + str(mean_absolute_error(inv_y(predict), inv_y(y_val))))\n\n\n# Lasso  \nfrom sklearn.linear_model import LassoCV\n\nmodel = LassoCV(max_iter=1e7,  random_state=14, cv=n_folds)\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('model', model)])\nclf.fit(X_train, y_train)\npredict = clf.predict(X_val)\nprint('Lasso: ' + str(mean_absolute_error(inv_y(predict), inv_y(y_val))))\n  \n      \n      \n# GradientBoosting   \nmodel = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, random_state=5)\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('model', model)])\nclf.fit(X_train, y_train)\npredict = clf.predict(X_val)\nprint('Gradient: ' + str(mean_absolute_error(inv_y(predict), inv_y(y_val))))","35b2acba":"model = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n                     max_depth=3, min_child_weight=0,\n                     gamma=0, subsample=0.7,\n                     colsample_bytree=0.7,\n                     objective='reg:squarederror', nthread=-1,\n                     scale_pos_weight=1, seed=27,\n                     reg_alpha=0.00006)\n\nfinal_model = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('model', model)])\n\nfinal_model.fit(X_train, y_train)\nfinal_predictions = final_model.predict(X_test)","b967058b":"output = pd.DataFrame({'Id': X_test.Id,\n                       'SalePrice': inv_y(final_predictions)})\n\noutput.to_csv('submission.csv', index=False)","df1e124c":"This gives a skewed result, so we apply log on that"}}