{"cell_type":{"c68bab55":"code","eb99d1d5":"code","dd13ba1b":"code","140bb5cf":"code","1f448ce1":"code","05ffdaee":"code","2c22985c":"code","c8a4791d":"code","45a7c4d4":"code","b5069abe":"code","3b67fb74":"code","9d914201":"code","903ee437":"code","d3500ebb":"code","ba751a6a":"code","b6410ee3":"code","afbc27dd":"code","f1b12742":"code","33402c9c":"code","df99e256":"code","e15fd8a7":"code","ec3faafc":"code","76c1f4c4":"code","ac3529fa":"code","04019f65":"code","10d92d8a":"code","17c032f7":"code","6fdecf5d":"code","2bb15dfa":"code","a102783d":"code","a9a962d5":"code","066d18f6":"code","2fdf2928":"code","d42f82d8":"code","48b89874":"code","26c6faf4":"code","7c10635d":"code","93fa25dc":"code","8c5ebd7c":"code","794996ac":"code","42ff0cb0":"code","04c76d70":"code","57820cae":"code","d8ac6af9":"code","7607840e":"code","29738de9":"code","e9d45df7":"code","ddfe4790":"code","dd7b75be":"code","ac18aa40":"code","e55d6606":"code","9fc54170":"code","41d3c845":"code","2957e5a2":"code","fa214501":"code","94ad9cfa":"code","35d4b830":"code","6e072445":"code","e683a088":"code","4174c4a8":"code","95287532":"code","ecbc8584":"code","b018706f":"code","17532548":"code","1febfb1a":"code","33d1ad37":"code","12ab6b7d":"code","a5b9e6f8":"code","8611a306":"code","78c570a6":"code","c4bf1c07":"code","a9855aa0":"code","b6cdee34":"code","9b46db42":"code","53a34eec":"code","a0020fe2":"code","d2738614":"code","297afbe4":"code","797bb67a":"code","a8cd667d":"code","71035458":"code","e5fb5988":"code","752bbf82":"code","d19ca76f":"code","b91bd977":"code","844202fd":"code","5e0a0cf1":"code","dcd6ea56":"code","c749e688":"code","add6fc36":"code","6d0f67c8":"code","70f1e5b0":"code","ecf89ddf":"code","2c64ef63":"code","6a0fc393":"code","b8f3c339":"code","a8011171":"code","83c7fece":"code","a2d27023":"code","cbe5d2b4":"code","40337576":"code","a4303972":"code","ba17d5e1":"code","afdfed95":"code","a0df9650":"code","570e9122":"code","33427e3b":"code","9d170fd5":"code","272926de":"code","5aee3e37":"code","e9757a6b":"code","b7c3d127":"code","90b3189f":"code","fa5d6d58":"code","36ae3c01":"code","437be33b":"code","7b4b9fa0":"code","a4942975":"code","afb7bfcc":"code","dd959848":"code","61f690c6":"code","a7b6ac7c":"code","dcbd5689":"code","3813d262":"markdown","8e259e4a":"markdown","e7d26e9e":"markdown","1ed3086a":"markdown","73055e29":"markdown","4754e9f2":"markdown","9b77c834":"markdown","966c767f":"markdown","fe5292af":"markdown","f7d2e63d":"markdown","8f8ca148":"markdown","99b0dd0c":"markdown","956823bb":"markdown","a662323a":"markdown","07739246":"markdown","03e956fa":"markdown","ea26eafa":"markdown","2638f2e9":"markdown","449e83cf":"markdown","a2f88ff2":"markdown","68dd7f80":"markdown","c1dd6ef4":"markdown","342f9185":"markdown","b66bd03f":"markdown","9e03e562":"markdown","ff66468c":"markdown","b52600db":"markdown","d3dee8d9":"markdown","2de828a0":"markdown","aa25bc48":"markdown","bd4beae4":"markdown","987cb3f8":"markdown","2c53e7ee":"markdown","33a98cdb":"markdown","e475fa06":"markdown","a304bebd":"markdown","a790013c":"markdown","09a11c57":"markdown","6bfc77f3":"markdown","ef213565":"markdown","c392f823":"markdown","34ea7a82":"markdown","307a3518":"markdown","fef898c8":"markdown","2513cbef":"markdown","e8f03ae5":"markdown","7f288015":"markdown","8f0a99bd":"markdown","f606d2a4":"markdown","d507684a":"markdown","2a038c29":"markdown","2e968416":"markdown","deda7e6c":"markdown","03117d6c":"markdown","ebb8fe36":"markdown","f8ec8862":"markdown","203bc146":"markdown","20a8720e":"markdown","aa58428e":"markdown","7e5d75e6":"markdown","cab8524c":"markdown","ed38eff6":"markdown","13940762":"markdown","76cf97ae":"markdown","cad526e9":"markdown","2e0dae98":"markdown","9782484c":"markdown","aad209e5":"markdown","7cc2b588":"markdown","bbe0d143":"markdown","e935fc0e":"markdown","885dafc5":"markdown","e1a0dae4":"markdown","721fc1cf":"markdown","4a64d7a0":"markdown","576182ac":"markdown","f82b3404":"markdown"},"source":{"c68bab55":"%matplotlib inline\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.formula.api as smf\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.model_selection  import train_test_split\nimport numpy as np\nfrom scipy.stats import norm # for scientific Computing\nfrom scipy import stats, integrate\nimport matplotlib.pyplot as plt","eb99d1d5":"melbourne_data  = pd.read_csv(\"..\/input\/melbourne-housing-market\/Melbourne_housing_FULL.csv\")","dd13ba1b":"melbourne_data.head(10)","140bb5cf":"melbourne_data.shape","1f448ce1":"melbourne_data.info()","05ffdaee":"## verifying columns with object data type\nprint(melbourne_data.select_dtypes([\"object\"]).columns)","2c22985c":"##changing all object data types to category - This step is necessary to be able to plot categorical data for our analysis\nobjdtype_cols = melbourne_data.select_dtypes([\"object\"]).columns\nmelbourne_data[objdtype_cols] = melbourne_data[objdtype_cols].astype('category')","c8a4791d":"melbourne_data.info()","45a7c4d4":"## looking at data information above, we can notice that \"Data\" is also converted to category. \n## In this step we will cast date to datetime\nmelbourne_data['Date'] =  pd.to_datetime( melbourne_data['Date'])","b5069abe":"## the following command suggests that all our data types are now in required format\nmelbourne_data.info()","3b67fb74":"## describe command will give us all statstical information about our numeric variables\nmelbourne_data.describe().T","9d914201":"melbourne_data[\"Postcode\"] = melbourne_data[\"Postcode\"].astype('category')","903ee437":"melbourne_data.describe().T","d3500ebb":"## in this step we will first confirm our above statemnt by obesrving \"Rooms\" and \"Bedroom2\"\n\nmelbourne_data['b 2 r'] = melbourne_data[\"Bedroom2\"] - melbourne_data[\"Rooms\"]\nmelbourne_data[['b 2 r', 'Bedroom2', 'Rooms']].head()","ba751a6a":"## We can see that the difference is very minimal here that is will be wise to remove one of the 2 columns\nmelbourne_data = melbourne_data.drop(['b 2 r', 'Bedroom2'], 1)","b6410ee3":"## visualizing missing values\nfig, ax = plt.subplots(figsize=(15,7))\nsns.heatmap(melbourne_data.isnull(), yticklabels=False,cmap='viridis')","afbc27dd":"# Percentage of missing values\nmelbourne_data.isnull().sum()\/len(melbourne_data)*100","f1b12742":"melbourne_data = melbourne_data.drop([\"Landsize\", \"BuildingArea\", \"YearBuilt\"], axis=1)","33402c9c":"## Also since our target variable is price, it makes sense to drop rows for Price columns wher price values are missing\nmelbourne_data.dropna(subset=[\"Price\"], inplace=True)","df99e256":"#from sklearn.preprocessing import Imputer\n#X=melbourne_data[['Bathroom','Car']]\n#imp=Imputer(missing_values='NaN',strategy='median',axis=0)\n#imp.fit(X)\n#X=pd.DataFrame(data=imp.transform(X),columns=X.columns)\n#melbourne_data[['Bathroom','Car']]=X","e15fd8a7":"melbourne_data['Car']=melbourne_data['Car'].fillna(melbourne_data['Car'].mode()[0])\nmelbourne_data['Bathroom']=melbourne_data['Bathroom'].fillna(melbourne_data['Bathroom'].mode()[0])","ec3faafc":"melbourne_data.shape","76c1f4c4":"# Percentage of missing values\nmelbourne_data.isnull().sum()\/len(melbourne_data)*100","ac3529fa":"melbourne_data.describe().T","04019f65":"## to findout outliers lets divide data into different price ranges to identify number of occurences of data in different price ranges\nmelbourne_data['PriceRange'] = np.where(melbourne_data['Price'] <= 100000, '0-100,000',  \n                                       np.where ((melbourne_data['Price'] > 100000) & (melbourne_data['Price'] <= 1000000), '100,001 - 1M',\n                                                np.where((melbourne_data['Price'] > 1000000) & (melbourne_data['Price'] <= 3000000), '1M - 3M',\n                                                        np.where((melbourne_data['Price']>3000000) & (melbourne_data['Price']<=5000000), '3M - 5M',\n                                                                np.where((melbourne_data['Price']>5000000) & (melbourne_data['Price']<=6000000), '5M - 6M',\n                                                                        np.where((melbourne_data['Price']>6000000) & (melbourne_data['Price']<=7000000), '6M - 7M',\n                                                                                np.where((melbourne_data['Price']>7000000) & (melbourne_data['Price']<=8000000), '7M-8M', \n                                                                                         np.where((melbourne_data['Price']>8000000) & (melbourne_data['Price']<=9000000), '8M-9M', \n                                                                                                 np.where((melbourne_data['Price']>9000000) & (melbourne_data['Price']<=10000000), '9M-10M', \n                                                                                                         np.where((melbourne_data['Price']>10000000) & (melbourne_data['Price']<=11000000), '10M-11M', \n                                                                                                                 np.where((melbourne_data['Price']>11000000) & (melbourne_data['Price']<=12000000), '11M-12M', '')\n                                                                                                                 ))))))))))","10d92d8a":"melbourne_data.groupby(['PriceRange']).agg({'PriceRange': ['count']})\n","17c032f7":"melbourne_data.info()","6fdecf5d":"melbourne_data.describe().T","2bb15dfa":"melbourne_data.drop(melbourne_data[(melbourne_data['PriceRange'] == '0-100,000') |\n                                   (melbourne_data['PriceRange'] == '7M-8M') |\n                                   (melbourne_data['PriceRange'] == '8M-9M') |\n                                   (melbourne_data['PriceRange'] == '11M-12M')].index, inplace=True)","a102783d":"melbourne_data.describe().T","a9a962d5":"melbourne_data.groupby(['Rooms'])['Rooms'].count()","066d18f6":"melbourne_data.drop(melbourne_data[(melbourne_data['Rooms'] == 12) | \n                                   (melbourne_data['Rooms'] == 16)].index, inplace=True)","2fdf2928":"melbourne_data.describe().T","d42f82d8":"##sns.distplot(melbourne_data, kde=False, bins=20).set(xlabel='Price');\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n##melbourne_data.select_dtypes(include = numerics)\nmelbourne_data.select_dtypes(include = numerics).hist(bins=15, figsize=(15, 6), layout=(2, 4))","48b89874":"melbourne_data['Distance'] = round(melbourne_data['Distance'])","26c6faf4":"melbourne_data.shape","7c10635d":"## extract year from date\nmelbourne_data['Year']=melbourne_data['Date'].apply(lambda x:x.year)\nmelbourne_data.head(5)","93fa25dc":"#data subset by type\n#house price\nmelbourne_data_h=melbourne_data[melbourne_data['Type']=='h']\n#condo price\nmelbourne_data_u=melbourne_data[melbourne_data['Type']=='u']\n#townhouse price\nmelbourne_data_t=melbourne_data[melbourne_data['Type']=='t']\n#house,condo and town house price groupby year and mean\nmelbourne_data_h_y=melbourne_data_h.groupby('Year').mean()\nmelbourne_data_u_y=melbourne_data_u.groupby('Year').mean()\nmelbourne_data_t_y=melbourne_data_t.groupby('Year').mean()\nmelbourne_data_h_y.head()\n","8c5ebd7c":"#sns.lmplot(x=\"Year\", y=\"Price\", hue=\"Type\", data=melbourne_data,  x_estimator=np.mean);\nmelbourne_data_h_y['Price'].plot(kind='line', color='r',label='House')\nmelbourne_data_u_y['Price'].plot(kind='line', color='g',label='Condo')\nmelbourne_data_t_y['Price'].plot(kind='line', color='b',label='Townhouse')\nyear_xticks=[2016,2017,2018]\nplt.ylabel('Price')\nplt.xticks( year_xticks)\nplt.title('Melboune price trend Vs Year per type')\nplt.legend()","794996ac":"melbourne_data.shape","42ff0cb0":"melbourne_data.columns","04c76d70":"\nmelbourne_data_South_M=melbourne_data[melbourne_data['Regionname']=='Southern Metropolitan']\nmelbourne_data_South_M_average=melbourne_data_South_M.groupby(['Year'])['Price'].mean()\n# Series.to_frame()\n","57820cae":"# create X and y\n\nX = melbourne_data_South_M[[ 'Year']]\ny = melbourne_data_South_M[['Price']]\n\n# instantiate and fit\nlm2 = LinearRegression()\nlm2.fit(X, y)\n\n# print the coefficients\nprint (lm2.intercept_)\nprint (lm2.coef_)","d8ac6af9":"\n### STATSMODELS ###\n\n# you have to create a DataFrame since the Statsmodels formula interface expects it\nX_new = pd.DataFrame({'Year': [2019,2020,2021]})\n\n# predict for a new observation\nlm2.predict(X_new)","7607840e":"melbourne_data_SM=melbourne_data[melbourne_data['Regionname']=='Southern Metropolitan']\nmelbourne_data_SM_u=melbourne_data_SM[melbourne_data_SM['Type']=='u']\nmelbourne_data_SM_u.shape","29738de9":"\n### STATSMODELS ###\n\n# create a fitted model\nlm1 = smf.ols(formula='Price ~ Year', data=melbourne_data_SM_u).fit()\n\n# print the coefficients\nlm1.params","e9d45df7":"\n# you have to create a DataFrame since the Statsmodels formula interface expects it\nX_new = pd.DataFrame({'Year': [2016,2017,2018,2019,2020,2021]})\n\n# predict for a new observation\nlm1.predict(X_new)","ddfe4790":"lm1.rsquared","dd7b75be":"\nmelbourne_data_E=melbourne_data[melbourne_data['Regionname']=='Eastern Metropolitan']\nmelbourne_data_E_u=melbourne_data_E[melbourne_data_E['Type']=='u']\n\nlme = smf.ols(formula='Price ~ Year', data=melbourne_data_E_u).fit()\n\n# print the coefficients\nlme.params","ac18aa40":"melbourne_data_E_u.shape","e55d6606":"X_new = pd.DataFrame({'Year': [2016,2017,2018,2019,2020,2021]})\n\n# predict for a new observation\nlme.predict(X_new)","9fc54170":"#get month information from date \n#df['year_month']=df.datetime_column.apply(lambda x: str(x)[:7])\n#per = df.Date.dt.to_period(\"M\")\n# How many calls, sms, and data entries are in each month?\n#data.groupby(['month', 'item'])\n#df['birthdate'].groupby([df.birthdate.dt.year, df.birthdate.dt.month]).agg('count')\nmelbourne_data['Month']=pd.DatetimeIndex(melbourne_data['Date']).month\n\n#lois[_y_m]=lois['Price'].groupby(['Month']).mean()\n#Prepare data for pie chart to check sales based on month in order to see which month sell most.\nmelbourne_data_2016=melbourne_data[melbourne_data['Year']==2016]\nmelbourne_data_2017=melbourne_data[melbourne_data['Year']==2017]\nmelbourne_data_2018=melbourne_data[melbourne_data['Year']==2018]\nmelbourne_data_2016_count=melbourne_data_2016.groupby(['Month']).count()\nmelbourne_data_2017_count=melbourne_data_2017.groupby(['Month']).count()\nmelbourne_data_2018_count=melbourne_data_2018.groupby(['Month']).count()\nComparison={2016:melbourne_data_2016.shape,2017:melbourne_data_2017.shape,2018:melbourne_data_2018.shape}\nComparison","41d3c845":"label_2016=['January','March','April','May','June','July','August','September','October','November','December']\nplt.pie(melbourne_data_2016_count['Price'],labels=label_2016,autopct='%.1f %%')\nplt.title('Year 2016')\nplt.show()","2957e5a2":"label_2017=['January','February','March','April','May','June','July','August','September','October','November','December']\nplt.pie(melbourne_data_2017_count['Price'],labels=label_2017,autopct='%.1f %%')\nplt.title('Year 2017')","fa214501":"label_2018=['January','February','March','June','October']\nplt.pie(melbourne_data_2018_count['Price'],labels=label_2018,autopct='%.1f %%')\nplt.title('Year 2018')","94ad9cfa":"\n# Abbreviate Regionname categories for presentation\nmelbourne_data['Regionabb'] = melbourne_data['Regionname'].map({'Northern Metropolitan':'N Metro',\n                                            'Western Metropolitan':'W Metro', \n                                            'Southern Metropolitan':'S Metro', \n                                            'Eastern Metropolitan':'E Metro', \n                                            'South-Eastern Metropolitan':'SE Metro', \n                                            'Northern Victoria':'N Vic',\n                                            'Eastern Victoria':'E Vic',\n                                            'Western Victoria':'W Vic'})","35d4b830":"\nsns.lmplot(x=\"Year\", y=\"Price\",hue=\"Type\", data=melbourne_data,col='Regionabb', x_estimator=np.mean,col_wrap=2)\nplt.ylim(200000, 2000000)\nplt.xlim(2015,2020)","6e072445":"#South region price change vs year per type\nsns.lmplot(x=\"Year\", y=\"Price\",hue=\"Type\", data=melbourne_data[melbourne_data['Regionabb']=='S Metro'], x_estimator=np.mean);","e683a088":"# East region price change vs year one type\nmelbourne_data_S=melbourne_data[melbourne_data['Regionabb']=='S Metro']\nsns.lmplot(x=\"Year\", y=\"Price\", data=melbourne_data_S[melbourne_data_S['Type']=='u'], x_estimator=np.mean);","4174c4a8":"Pct_change=melbourne_data.groupby(['Year','Regionabb','Type'],as_index=False)['Price'].mean()\nPct_change = Pct_change.sort_values(['Regionabb', 'Type','Year']).set_index(np.arange(len(Pct_change.index)))\n\nPct_change.info()","95287532":"melbourne_data_count_region_y=melbourne_data.groupby(['Year','Regionabb','Type'],as_index=False)['Price'].count()\nmelbourne_data_count_region_y = melbourne_data_count_region_y.sort_values(['Regionabb', 'Type','Year']).set_index(np.arange(len(melbourne_data_count_region_y.index)))\nmelbourne_data_count_region_y.rename(columns={'Price':'Count'}, inplace=True)\n","ecbc8584":"# define fucntion to get year growth rate again price per region and type\ndef PCTM(gg):\n    df=pd.DataFrame(gg['Price'].pct_change())\n    df['Year']=gg['Year']\n    df['region']=gg['Regionabb']\n    df['Type']=gg['Type']\n    df=df[df['Year']!=2016]\n    return df","b018706f":"#df2[df2['id'].isin(['SP.POP.TOTL','NY.GNP.PCAP.CD'])]\nmelboune_growthrate_y_t=PCTM(Pct_change)\nmelboune_growthrate_y_t1=melboune_growthrate_y_t[melboune_growthrate_y_t['region'].isin(['N Metro','S Metro','E Metro','SE Metro','W Metro','S Metro'])]\nmelboune_growthrate_y_t1.rename(columns={'Price':'Price Growth Rate'}, inplace=True)\nmelboune_growthrate_y_t1[melboune_growthrate_y_t1['Price Growth Rate']>0.05]","17532548":"Sales_count=melbourne_data.groupby(['Regionabb'])['Price'].count()\nSales_count.head(10)","1febfb1a":"Sales_count=melbourne_data.groupby(['Regionabb','Type'])['Price'].count()\nSales_count.nlargest(20)","33d1ad37":"\n            # define fucntion to get year growth rate again count per region and type\ndef PCTMC(gg):\n    df=pd.DataFrame(gg['Count'].pct_change())\n    df['Year']=gg['Year']\n    df['region']=gg['Regionabb']\n    df['Type']=gg['Type']\n    df=df[df['Year']!=2016]\n    return df","12ab6b7d":"#df2[df2['id'].isin(['SP.POP.TOTL','NY.GNP.PCAP.CD'])]\nmelboune_growthrate_y_c=PCTMC(melbourne_data_count_region_y)\nmelboune_growthrate_y_c1=melboune_growthrate_y_c[melboune_growthrate_y_c['region'].isin(['N Metro','S Metro','E Metro','SE Metro','W Metro','S Metro'])]\n\nmelboune_growthrate_y_c1.rename(columns={'Count':'Count Growth Rate'}, inplace=True)\nmelboune_growthrate_y_c1[melboune_growthrate_y_c1['Count Growth Rate']>0.2]","a5b9e6f8":"melboune_count1=melbourne_data_count_region_y[melbourne_data_count_region_y['Regionabb'].isin(['S Metro','E Metro','SE Metro','W Metro','S Metro','N Metro'])]\nmelboune_count1[melboune_count1['Count']>1000]","8611a306":"sns.boxplot(x = 'Method', y = 'Price', data = melbourne_data)\nplt.show()\n#Sold method did not affect price","78c570a6":"sns.lmplot(x=\"Year\", y=\"Price\", hue=\"Rooms\", data=melbourne_data,  x_estimator=np.mean);","c4bf1c07":"sns.lmplot(x=\"Distance\", y=\"Price\", data=melbourne_data, x_estimator=np.mean);","a9855aa0":"sns.lmplot(x=\"Car\", y=\"Price\", data=melbourne_data, x_estimator=np.mean);","b6cdee34":"\n\nIdeal_House=melbourne_data.groupby(['Regionabb','Type','Rooms','Bathroom'])['Price'].count()\n\n\nIdeal_House.loc[['S Metro'],'h'].nlargest(10)","9b46db42":"Ideal_House.nlargest(10)","53a34eec":"Ideal_House.loc[['E Metro'],'u'].nlargest(10)","a0020fe2":"corrmat=melbourne_data.corr()","d2738614":"fig,ax=plt.subplots(figsize=(12,10))\nsns.heatmap(corrmat,annot=True,annot_kws={'size': 12})","297afbe4":"#define function to refine those correlation more than 0.3 with abs value\ndef getCorrelatedFeature(corrdata,threshold):\n    feature=[]\n    value=[]\n    \n    for i, index in enumerate(corrdata.index):\n        if abs(corrdata[index])>threshold:\n            feature.append(index)\n            value.append(corrdata[index])\n    df=pd.DataFrame(data=value,index=feature,columns=['Corr Value'])\n    return df","797bb67a":"threshold=0.4\ncorr_value=getCorrelatedFeature(corrmat['Price'],threshold)\ncorr_value","a8cd667d":"melbourne_data.isnull().sum()","71035458":"melbourne_data['Type_Code'] = melbourne_data['Type'].map({'h':3,\n                                            't':2, \n                                            'u':1, \n                                            'dev site':0, \n                                            'o res':0, \n                                            'br':0})\n","e5fb5988":"# Group Regionname categories \nmelbourne_data1 = pd.get_dummies(melbourne_data['Regionabb'],drop_first=False)\nmelbourne_data=pd.concat([melbourne_data,melbourne_data1],axis=1)\nmelbourne_data.columns.values","752bbf82":"\n#fig,ax=plt.subplots(figsize=(12,10))\n#df=melbourne_data[['Price','Rooms','Distance', 'Bathroom',  'Year', 'Type_Code','RegionCode']]\n#sns.heatmap(df,annot=True)\n#dff=melbourne_data[['Price','Rooms','Distance', 'Bathroom', 'Car', 'Year', 'Propertycount','Type_Code',]].groupby('RegionCode')\n#dff.head()","d19ca76f":"melbourne_data_NN=melbourne_data[['Rooms','Distance', 'Bathroom', 'Car', 'Year', 'Propertycount','Type_Code','N Metro','W Metro','S Metro','E Metro','SE Metro','N Vic','E Vic','W Vic','Price']].dropna()\nmelbourne_data_NN[['Rooms','Distance', 'Bathroom', 'Car', 'Year', 'Propertycount','Type_Code','N Metro','W Metro','S Metro','E Metro','SE Metro','N Vic','E Vic','W Vic','Price']].isnull().sum()","b91bd977":"melbourne_data_NN.shape","844202fd":"#Finding coefficient\n\nX=melbourne_data_NN[['Rooms','Distance', 'Bathroom', 'Car', 'Year', 'Propertycount','Type_Code','N Metro','W Metro','S Metro','E Metro','SE Metro','N Vic','E Vic','W Vic']]\ny=melbourne_data_NN['Price']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .20, random_state=5)","5e0a0cf1":"# Fit\n# Import model\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear regression object\nregressor = LinearRegression()\n\n# Fit model to training data\nregressor.fit(X_train,y_train)","dcd6ea56":"\n# Predict\n# Predicting test set results\ny_pred = regressor.predict(X_test)","c749e688":"regressor.score(X_test,y_test)","add6fc36":"\nfrom sklearn import metrics\nprint('MAE:',metrics.mean_absolute_error(y_test,y_pred))\nprint('MSE:',metrics.mean_squared_error(y_test,y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","6d0f67c8":"print('R^2 =',metrics.explained_variance_score(y_test,y_pred))","70f1e5b0":"plt.scatter(y_test, y_pred)","ecf89ddf":"# Histogram of the distribution of residuals\nsns.distplot((y_test - y_pred))","2c64ef63":"cdf = pd.DataFrame(data = regressor.coef_, index = X.columns, columns = ['Coefficients'])\ncdf","6a0fc393":"X.head()","b8f3c339":"from sklearn.ensemble import RandomForestClassifier\n#model=RandomForestClassifier(n_estimators=20)\n#model.fit(X_train,y_train)","a8011171":"clf=RandomForestClassifier(n_jobs=2,random_state=0)\nclf.fit(X,y)","83c7fece":"clf.predict(X)","a2d27023":"clf.score(X_test,y_test)","cbe5d2b4":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfolds=StratifiedKFold(n_splits=3)","40337576":"\ndef get_score(model, X_train, X_test, y_train, y_test):\n    model.fit(X_train, y_train)\n    return model.score(X_test, y_test)","a4303972":"print(get_score(LogisticRegression(solver='liblinear',multi_class='ovr'), X_train, X_test, y_train, y_test))","ba17d5e1":"print(get_score(LinearRegression(), X_train, X_test, y_train, y_test))","afdfed95":"correlated_data=melbourne_data_NN[corr_value.index]\ncorrelated_data.head()","a0df9650":"corr_value.index","570e9122":"sns.pairplot(correlated_data)\nplt.tight_layout()","33427e3b":"sns.heatmap(correlated_data.corr(),annot=True,annot_kws={'size':12})","9d170fd5":"X1=correlated_data.drop(labels=['Price'],axis=1)\ny1=correlated_data['Price']\nX1.head()","272926de":"X1_train,X1_test,y1_train,y1_test=train_test_split(X1,y1,test_size=0.2,random_state=0)","5aee3e37":"X1_train.shape,X1_test.shape","e9757a6b":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error","b7c3d127":"model = LinearRegression()\nmodel.fit(X1_train,y1_train)","90b3189f":"y1_predict=model.predict(X1_test)","fa5d6d58":"y1_predict,y1_test","36ae3c01":"df=pd.DataFrame(data=[y1_predict,y1_test])\ndf.T.head(5)\n","437be33b":"from sklearn.metrics import r2_score","7b4b9fa0":"score=r2_score(y1_test,y1_predict)\nmae=mean_absolute_error(y1_test,y1_predict)\nmse=mean_squared_error(y1_test,y1_predict)\nprint(\"r2_score\", score)\nprint(\"mae\", mae)\nprint(\"mse\", mse)","a4942975":"#store feature performance\ntotal_features=[]\ntotal_features_name=[]\nselected_correlation_value=[]\nr3_score=[]\nmae_value=[]\nmse_value=[]\n","afb7bfcc":"def performance_metrics(features, th, y_true,y_pred):\n    score=r2_score(y_true,y_pred)\n    mae=mean_absolute_error(y_true,y_pred)\n    mse=mean_squared_error(y_true,y_pred)\n    \n    total_features.append(len(features)-1)\n    total_features_name.append(str(features))\n    selected_correlation_value.append(th)\n    r3_score.append(score)\n    mae_value.append(mae)\n    mse_value.append(mse)\n    \n    metrics_dataframe=pd.DataFrame(data=[total_features_name, total_features,selected_correlation_value,r3_score,mae_value,mse_value],index=['Features name','Total features','corre value','r2 score','mae','mse'])\n    return metrics_dataframe.T","dd959848":"def get_y_predict(corrdata):\n    X=corrdata.drop(labels=['Price'],axis=1)\n    y=corrdata['Price']\n    \n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n    model=LinearRegression()\n    model.fit(X_train,y_train)\n    y_predict=model.predict(X_test)\n    return y_predict\n    ","61f690c6":"th5=0.4\ncorr_value=getCorrelatedFeature(corrmat['Price'],th5)\ncorrelated_data=melbourne_data_NN[corr_value.index]\ny_predict=get_y_predict(correlated_data)\nperformance_metrics(correlated_data.columns,th5,y_test,y_predict)","a7b6ac7c":"#Ploting learning curves\nfrom sklearn.model_selection import learning_curve, ShuffleSplit","dcbd5689":"def plot_learning_curve(estimator,title,X,y,ylim=None,cv=None,n_jobs=None,train_sizes=np.linspace(0.1,1.0,10)):\n    plt.figure()\n    plt.title(title)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    \n    train_sizes,train_scores,test_scores=learning_curve(estimator,X,y,cv=cv,n_jobs=n_jobs,train_sizes=train_sizes)\n    \n    train_scores_mean=np.mean(train_scores,axis=1)\n    train_scores_std=np.std(train_scores,axis=1)\n    test_scores_mean=np.mean(test_scores,axis=1)\n    test_scores_std=np.std(test_scores,axis=1)\n    \n    plt.grid()\n    \n    plt.fill_between(train_sizes,train_scores_mean - train_scores_std,train_scores_mean+train_scores_std,alpha=0.1,color=\"r\")\n    plt.fill_between(train_sizes,test_scores_mean - test_scores_std,test_scores_mean+test_scores_std,alpha=0.1,color=\"g\")\n    plt.plot(train_sizes,train_scores_mean,'o-',color=\"r\",label=\"Training score\")\n    plt.plot(train_sizes,test_scores_mean,'o-',color=\"g\",label=\"Cross-validation score\")\n    \n    plt.legend(loc=\"best\")\n    return plt\n\nX=correlated_data.drop(labels=['Price'],axis=1)\ny=correlated_data['Price']\n\ntitle=\"learning curves (linear regression)\" + str(X.columns.values)\ncv=ShuffleSplit(n_splits=100,test_size=0.2,random_state=0)\n\nestimator=LinearRegression()\nplot_learning_curve(estimator,title,X1,y1,ylim=(0.7,1.01),cv=cv,n_jobs=-1)\n\nplt.show()","3813d262":"###### 2.5.1 Method vs Price","8e259e4a":"# 2.1.1 Price trend against year per house type","e7d26e9e":"From above information count growth percentage and acutal sales count by year, South metro and N Metro seems to be the area where people tend to pay more and buy more, but as price kept going up, those live in south try to move to E and SE Metro","1ed3086a":"The best Condo type in East region Metropolitan is 2 rooms with one bed room.","73055e29":"In general, it looks like that winters in 2016 an 2017 have the least sales count. that means house sales will favors more from May to November. Year 2018 seems a lot of missing data and date shape only one third compard to the others thus it is hard to make conclusion. ","4754e9f2":"# 3.3 Random tree forest model for coefficient to present impact for 'Rooms','Distance', 'Bathroom', 'Car', 'Year', 'Propertycount','Type_Code',region","9b77c834":"From the statstical summary above we can see that max price in our data is nearly $11.2 million. That looks like a clear outlier. But before removing it, lets first ensure that we have very few values in that range.","966c767f":"Only extract data with high relations to price","fe5292af":"# 2.6 Ideal house type","f7d2e63d":"House price was going down dramatically by 100,000 units\nCondo price climb up slowly while Townhouse price kept steady.\nFrom this graph, it is anticipated that house price will keptgoing down but less slope\nTownhouse price will kepte unchanged\nCondo price will increase.\nTo developer, it is time to built more condos in 2019.House budget need to be cut\nFor home buyers, it is time to buy house in 2019.","8f8ca148":"For E Metro, price grow 10% from 2016 to 2017 and 7.7% from 2018 to 2019 can be expected. though count is less compared to southern region\n","99b0dd0c":"It looks like training examples between 9 000 and 15 000 will give less change to the model.","956823bb":"By exploring above table, it can be concluded that:\n- 1 data item in the range 0-100,00 \n- 2 data item in range 7M - 8M\n- 1 data item in range 8M - 9M\n- 1 data item in range 11M - 12M\nFor the purpose of this study, let us drop rows that match above mentioned conditions","a662323a":"##### 2.4.2 Feature engineering to get count growth and price growth against year per region and type","07739246":"\nThere are multiple ways that we can use to explore missisng data. Here we will be using a visual way first to get some hint. In later step we will do some calculations to get exact number of missing data from each variables. Based on data, our experience and business need we will either fill in missing values or we will drop rows or columns having null values","03e956fa":"# 2.2.1 Predict prcie for S Metro unit","ea26eafa":"Before using fillna.mode\nEvery unit increase in those features will:\nRooms is linked to an increase in Price by 170624.6\nDistance is linked to an decrease in Price by 44797\nBathroom is linked to an increase in Price by 185170\nCar space is linked to an increase in Price by 38890\nYear is linked to an increase in Price by 44305\nType code is associated with increase in Price by 271838\n\nAfter using fillna\nEvery unit increase in those features will:\nRooms is linked to an increase in Price by 217253.4\nDistance is linked to an decrease in Price by 42651.8\nBathroom is linked to an increase in Price by 141730.5\nCar space is linked to an increase in Price by 37952\nYear is linked to an increase in Price by 29458.7\nType code is associated with increase in Price by 235465\n\nType code, Rooms and bathroom are very important in house price. With limited landsize, the more rooms and bathroom, the higher the price is.\n \nS Metro, SE Metro seems to be linked to an increase in price","2638f2e9":"\nAfter carefully evaluating data, it can be noticed that variables \"Rooms\" and \"Bedroom2\" are pretty much similar and one of the columns should be removed to avoid duplication of data","449e83cf":"Linear has better score","a2f88ff2":"Due to a lot of missing data in 2018, S Metro units has 8.7% in 2017 and 2.7% in 2018. If 2017 was chozen to look at growth in price, units and townhouse in E Metro,SE Metro and S Metro, townhouse in W Metro and house in S Metro present positive growth over 5%. looking at 2018, it seems people shift to buy more from SE or E Metro from S Metro. But this shift was not that significant yet in count as it showed in later table. Trend is there.","68dd7f80":"\n##### 4.2 Learning curve : Plot training scores against validation score","c1dd6ef4":"\nFrom the information above, we can notice that few feature varaibles still have large percentage of missing values. At this point we are ignoring it, but at later state if we will take those as our feature variables for oour model, we will explore ways to fill in those information or to remove those from our data.","342f9185":"### 1. Data structure. It inclues data cleaning against data type, outliers, null value.\n\n##### 1.1 Exploring data, such select_dtypes().columns, describe(), info() and shape()\n\n##### 1.2 Changing data type, date type and catogory type\n\n##### 1.3 Dealing with null value, mainly replacing null in price with mode using fillna()\n\n##### 1.4 Finding outliers\n\n\n### 2. Data presentation and prediction 7 section. (27239 records)\n\n##### 2.1 Year vs price per type\n\n##### 2.2 Price prediction for 2019\n\n###### 2.2.1 Predict price for unit in S Metro \n\n###### 2.2.1 Predict price for unit in E Metro \n\n##### 2.3 Season vs price \n\n##### 2.4 Region vs price change and growth rate vs year\n\n###### 2.4.1 Region price change vs year per type\n\n###### 2.4.2 Feature engineering to get count growth and price growth against year per region and type\n\n###### 2.4.2.1 Price growth rate over 0.05 table per region and type\n\n###### 2.4.2.2 Top 20 region per type with largest count\n\n###### 2.4.2.3 Count growth rate over 0.2 table per region and type\n\n###### 2.4.2.4 Actual count over 1000 for each year pet type and region\n\n##### 2.5 Other features method, distance, rooms, car vs price\n\n###### 2.5.1 Method vs Price\n\n###### 2.5.2 Rooms # impact on Price VS Year\n\n###### 2.5.3 Distance vs Price\n\n###### 2.5.4 Car spot vs Price\n\n##### 2.6 Ideal house type for top 10 region per type max count of sales\n\n##### 2.6.1 Top 10 house type in S Metro with different rooms and bathrooms by count\n\n##### 2.6.2 Top 10 house type in all regions per type with different rooms and bathrooms by count\n\n##### 2.6.3 Top 10 units in E Metro with different rooms and bathrooms by count\n\n##### 2.7 Heat map for relationships\n\n### 3. Data linear modeling to see which variable contribute most (20394 records)\n\n##### 3.1 Prepare the dataset and label for training models, include removing all null value, get_dummies of region, change type into numeric\n\n##### 3.2 Getting coefficient\n\n##### 3.3 Random forest model\n\n##### 3.4 Model comparison get score function\n\n\n### 4. Performance evaluation  sample size vs machine learning\n\n##### 4.1 Define fuction to get correlation, refine results and storage value\n\n##### 4.2 Learning curve : Plot training scores against validation score\n\n### 5. Conclusion\n","b66bd03f":"# 2.0 Data presentation and relationship","9e03e562":"Insights: Increase distance reduce price","ff66468c":"##### 2.4.2.1 Price growth rate over 0.05 table per region and type","b52600db":"# 1.1 Exploring data","d3dee8d9":"Define function to refine those correlation more than 0.3 with abs value","2de828a0":"# 1.2 Changing data type","aa25bc48":"To Conclude this section:\n\n1.Regarding house, S Metro has over 5 % in price growth in 2017 and 4718 sales count in three years, ranking second place among all.In terms of Units\/Condo, S Metro has 2782 in sales count ranking No.4 and 8.7 % in price growth in 2017.\n\n2.Units and townhouses in E Metro and SE MEtro has great potential tough now they dont have such attraction as S Metro. They have count growth over 100% and price growth rate over 8%.","bd4beae4":"In general, Townhouse in E Metro, Condo in East Metro, House in S Metro, Condo in S Metro and Condo in N Metro are growing each year.","987cb3f8":"In this section, Count will be used to find out leads about the best sales type in all region.","2c53e7ee":"\n\nBased on all 4 sentions, some finding and conclusion can be found as follow:\n\n1.From 2.1.1 section, house price in general will go down and Condo\/units price will go up. that means investing in Condo will be better.\n\n\n2.Regarding house, S Metro has over 5 % in price growth in 2017 and 4718 sales count in three years, ranking second place among all.In terms of Units\/Condo, S Metro has 2782 in sales count ranking No.4 and 8.7 % in price growth in 2017 from Section 2.4.\n\n3.Units and townhouses in E Metro and SE MEtro has great potential tough now they dont have such attraction as S Metro. They have count growth over 100% and price growth rate over 8% from Section 2.4.\n\n4.From 2.6 ideal house type, we can see uints\/condo in Southern Metro was ranked no.3. Though house in North and West Metro has the most count but their price is going down as showed in section 2.4 sns.lmplot. In terms of counts, Southern condo\/units has great market potential as its count rank No.3 and price kept increasing by years presented in section 2.4. Southen house also has great market and rank no.4 after its unit in same region. But the price kept dropping compared to units in same region.\n\n5.From section 3 and 4, though the score is low due to low coefficient values, Southern Metro show No.2 biggest coefficient compared to other regions. E vic coefficient is high but is low in count thus can be neglected. That means Southern Metro has the great market in price.\n\n\n6.Linear model has some limitations as if more rooms does equal to higher price and more sales count. But its coefficient can help to understand which factor has great impact. If price was only output dependent variables, the conclusion will be biased and not looking at the whole pictures. But if slaes count and price be presented as output variables, a clear picture will be clear. On the other hand, data is data, the perfect r2 sometimes doesn't mean the great insights of a business. The combination of business unstanding and data can present more real insights of business. The dataset miss great amount of value in 2018 and it results in that count cannot be used as model output. Only price can be output variable. So Cout in past three years and count in 2016 to 2017 will provide more leads for market.\n\n7.Continue improvement: \nDue to limited timeframe, some work need to done to improve linear relationship and reduce r2 value:\n\na.replace null value with median to see if r2 drops (Yes, it drops by 0.01 and reduce effect of increasing unit against price regarding bathroom, car spots; reduce training score and vailidation score by 0.02 and validation score is always under training score by 0.01 throughout 1400 samples). so the code is there in 1.3 but not in effect and more investigation using different strategy will be applied.\n\nb.different regressor or K-Fold model will be applied to reduced r2; \n\nc.more feature engineering against sales count will be explored, such as count growth in region against year.\n\n8.On the basis of conclusion 1-4, Units with 2 room and one bathroom in Southern Metro will be recommended to investor or home developer as it has 955 count in 3 years, ranking No.3 in counts and price kept increaing steadily 6 %. It is safe area for conservative investment agaist the unstable market. \n\n\n\nGreat Thanks to the team work!","33a98cdb":"Looking at the data information above, we can see that non-numerical data is being considered as object. The list included following columns: 'Suburb', 'Address', 'Type', 'Method', 'SellerG', 'Date', 'CouncilArea','Regionname'\nIn next few steps we will be changing object data types to categorical and date data types","e475fa06":"# 3.4 Model comparison","a304bebd":"# 1.4 Finding Outliers\n","a790013c":"27236 records will applied to the model","09a11c57":"# 2.4 Region versus Price","6bfc77f3":"# 3.1 Prepare the dataset and label for training models, include removing all null value, get_dummies of region, change type into numeric","ef213565":"# 4.0 Performance evaluation","c392f823":"##### 2.5 Suplots of other numeric features v price","34ea7a82":"Get mean value for price for each year, region per type","307a3518":"# 1.Exploring Data and data cleaning","fef898c8":"###### 2.2.2 Predicting prcie for E Metro unit","2513cbef":"###### 2.2 Predicting house price for all types in South Metro, units in South Metro and units in East Melbourne for 2019 and 2020","e8f03ae5":"Outliers can significantly impact data analysis and can also impact normalization of data. It is very important in during data prepration to indentify them remove them. In next few steps we will work in our data to get rid of outliers (if any)","7f288015":"From the visual above, it can be concluded that we have few missing data in Price, Bathroom, Car and Landsize, Longitude and Latitude columns. There are so many missing values in Building Area and Year Built features. In next step let's explore the count of missing values","8f0a99bd":"###### 2.6.2 Top 10 house type in all regions per type with different rooms and bathrooms by count","f606d2a4":"\nFor next few steps we will be doing data prepration for Numerical Feature Variables","d507684a":"###### 2.6.1 Top 10 house type in S Metro with different rooms and bathrooms by count","2a038c29":"Used get_dummies to change category data into numeric date. Region will be expressed in numerics","2e968416":"###### 2.5.2 Rooms # impact on Prcie VS Year","deda7e6c":"##### 2.3 Seasonal performance","03117d6c":"27239 record will be used for presenting","ebb8fe36":"# 2.4.1 Region price change vs year per type\n\nIn General, East, north, south and west Metro are popular area based on sales count.\n\nNext look at price against region per type\n","f8ec8862":"Insights: In S Metro, house with 3 rooms and 1 or 2 bathroom and house with 4 rooms and 2 bathrooms have the most sales among all.","203bc146":"###### 2.6.3 Top 10 units in E Metro with different rooms and bathrooms by count","20a8720e":"##### 2.4.2.2 Top 20 region per type with largest count","aa58428e":"##### 2.4.2.4 Actual count over 1000 for each year pet type and region ","7e5d75e6":"# Melboune house price prediction,growth area identification and modeling (linear, random tree forest and logistic) Layout","cab8524c":"##### 3.2 Getting coefficient","ed38eff6":"### Team members: Mehul Haria, Naureen Pethani and Raymond (shanhua) Huang","13940762":"For Condo in S Metro region, price will be aroud 795272 dollars with lower R2","76cf97ae":"###### 2.5.4 Car spot vs Price","cad526e9":"# 5.CONCLUSION","2e0dae98":"##### 4.1 Define fuction to get correlation, refine results and storage value\n","9782484c":"# 2.1 The first factor we look at is the price versus year and season. Then predict price using linear function for 2019 and 2020.","aad209e5":"From this rough approximation, 2019 average price will be 1557639, 1630019 for 2020 for all types in melboune","7cc2b588":"Rooms and Bathroom has highest correlation with house price compared to other factors.","bbe0d143":"Insights:North metro house was the most favorable types among those and W Metro follow behind. In general, house is more favorable than other types. South Metro condo with 2 bed rooms and 1 bathroom was listed as top 3 in sales count.","e935fc0e":"###### 2.7 Heat map for presenting relationship","885dafc5":"Using define function to evaluate different input threshold for correlation value\n\nExplore relationships between input sample size and machine learning scores\n\nOnly correlation more than 0.4 was selected below. so Rooms and Bathroom fall into this category","e1a0dae4":"##### 2.4.2.3 Count growth rate over 0.2 table per region and type","721fc1cf":"# 3.0 Linear model for coefficient to present impact for 'Rooms','Distance', 'Bathroom', 'Car', 'Year', 'Propertycount','Type_Code',region","4a64d7a0":"Observing above information abount numerical data, it can be noticed that Postcode is also being treated as numerical data. Since we know that Postcode is a catergorical data, we will be casting it to category","576182ac":"# 1.3 Working with missing data\n","f82b3404":"2.5.3 Distance vs Price"}}