{"cell_type":{"2ee46faa":"code","26a2bd89":"code","833be80b":"code","9fc8be86":"code","4fa9cbaf":"code","2297279c":"code","ff50f48e":"code","aca5470f":"code","cbfb72e3":"code","1b546d5d":"code","db74773e":"code","db804256":"code","c4f819f1":"code","327fb32c":"code","aca4914f":"code","db06628b":"code","39e04159":"code","c559e2ad":"code","c309bc06":"code","97bd36ab":"code","e5f9e105":"code","98fec8c9":"markdown","1895113b":"markdown","8b26a923":"markdown","6358a41e":"markdown","48e1c124":"markdown","fbac03c8":"markdown","c4bc4fed":"markdown","60a7efbb":"markdown","2cabb24c":"markdown","6759f3ec":"markdown","4f9d020f":"markdown","ed1ef1ea":"markdown","fe3c4c8e":"markdown","f3774442":"markdown","d1f87758":"markdown","68366f7d":"markdown"},"source":{"2ee46faa":"import numpy as np\nimport random\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nimport seaborn as sns\nimport pandas as pd\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nfrom utilities_x_ray import read_xray,showXray\nfrom tqdm import tqdm\nimport pydicom\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","26a2bd89":"def seedAll(seed=355):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    random.seed(seed)\nseedAll()","833be80b":"train = pd.read_csv('..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train.csv')\nss = pd.read_csv('..\/input\/vinbigdata-chest-xray-abnormalities-detection\/sample_submission.csv')","9fc8be86":"train.head()","4fa9cbaf":"ss.head()","2297279c":"plt.figure(figsize=(8,10))\nplt.imshow(read_xray('..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train\/0108949daa13dc94634a7d650a05c0bb.dicom'),cmap=plt.cm.bone)","ff50f48e":"showXray('..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train\/0108949daa13dc94634a7d650a05c0bb.dicom',train,with_boxes=True)","aca5470f":"print(\"Number of rows in train dataframe: {}\".format(train.shape[0]))\nprint(\"Number of Unique images in train set: {}\".format(train.image_id.nunique()))\nprint(\"Number of Classes: {}\\n\".format(train.class_name.nunique()))\nprint(\"Class Names: {}\".format(list(train.class_name.unique())))","cbfb72e3":"print(\"Null Values:\")\ntrain.isna().sum().to_frame().rename(columns={0:'Null Value count'}).style.background_gradient('viridis')","1b546d5d":"plt.figure(figsize=(9,6))\nsns.countplot(train[\"class_id\"]);\nplt.title(\"Class Distributions\");","db74773e":"plt.figure(figsize=(9,6))\nsns.countplot(train[\"rad_id\"]);\nplt.title(\"rad_id Distributions\");","db804256":"class_names = sorted(train.class_name.unique())\ndel class_names[class_names.index('No finding')]\nclass_names = class_names+['No finding']\nclasses = dict(zip(list(range(15)),class_names))","c4f819f1":"def prepareDataFrame(train_df= train):\n    train_df = train_df.fillna(0)\n    cols = ['image_id','label']+list(range(4*len(class_names[:-1])))\n    return_df = pd.DataFrame(columns=cols)\n    \n    for image in tqdm(train_df.image_id.unique()):\n        df = train_df.query(\"image_id==@image\")\n        label = np.zeros(15)\n        for cls in df.class_id.unique():\n            label[int(cls)]=1\n        bboxes_df = df.groupby('class_id')[['x_min','y_min','x_max','y_max']].mean().round()\n        \n        bboxes_list = [0 for i in range(60)]\n        for ind in list(bboxes_df.index):\n            bboxes_list[4*ind:4*ind+4] = list(bboxes_df.loc[ind,:].values)\n        return_df.loc[len(return_df),:] = [image]+[label]+bboxes_list[:-4]\n    return return_df\ntrain_df = prepareDataFrame()","327fb32c":"train_df.head(2)","aca4914f":"def generateFolds(n_splits = None):\n    kf = KFold(n_splits= n_splits)\n    for id,(tr_,val_) in enumerate(kf.split(train_df[\"image_id\"],train_df[\"label\"])):\n        train_df.loc[val_,'kfold'] = int(id)\n    train_df[\"kfold\"].astype(int)\n\ngenerateFolds(n_splits=5)","db06628b":"class DataLoader:\n    def __init__(self,path = None,train_df=train_df,val_df=None):\n        self.path = path\n        self.df = train_df\n        self.val_df = val_df\n        self.train_list = [f'{img}.npy' for img in train_df[\"image_id\"].unique()]\n        np.random.shuffle(self.train_list)\n        self.test_list = [f'{img}.npy' for img in val_df[\"image_id\"].unique()]\n        np.random.shuffle(self.test_list)\n    \n    def read_image(self):\n        for img in self.train_list:\n            im_name = img.split('.npy')[0]\n            image = np.load(self.path+img)\n            temp = self.df[self.df.image_id==im_name]\n            c_label,bb = temp.iloc[0,1],temp.iloc[0,2:].values.astype('float')\n            yield image,c_label,bb\n    \n    \n    def batch_generator(self,items,batch_size):\n        a=[]\n        i=0\n        for item in items:\n            a.append(item)\n            i+=1\n\n            if i%batch_size==0:\n                yield a\n                a=[]\n        if len(a) is not 0:\n            yield a\n            \n    def flow(self,batch_size):\n        \"\"\"\n        flow from given directory in batches\n        ==========================================\n        batch_size: size of the batch\n        \"\"\"\n        while True:\n            for bat in self.batch_generator(self.read_image(),batch_size):\n                batch_images = []\n                batch_c_labels = []\n                batch_bb = []\n                for im,im_c_label,im_bb in bat:\n                    batch_images.append(im)\n                    batch_c_labels.append(im_c_label)\n                    batch_bb.append(im_bb)\n                batch_images = np.stack(batch_images,axis=0)\n                batch_labels =  (np.stack(batch_c_labels,axis=0),np.stack(batch_bb,axis=0))\n                yield batch_images,batch_labels\n    \n    def getVal(self):\n        images = []\n        c_labels = []\n        bb_labels = []\n        for img in self.test_list:\n            im_name = img.split('.npy')[0]\n            image = np.load(self.path+img)\n            temp = self.val_df[self.val_df.image_id==im_name]\n            c_label,bb = temp.iloc[0,1],temp.iloc[0,2:].values.astype('float')\n            images.append(image)\n            c_labels.append(c_label)\n            bb_labels.append(bb)\n        return np.stack(images,axis=0),(np.stack(c_labels,axis=0),np.stack(bb_labels,axis=0))\n    ","39e04159":"def build():\n    in1 = L.Input(shape=(256,256,1))\n    \n    out1 = L.Conv2D(32,(3,3),activation=\"relu\")(in1)\n    out1 = L.Conv2D(32,(3,3),activation=\"relu\")(out1)\n    out1 = L.MaxPooling2D((2,2))(out1)\n    \n    out1 = L.Conv2D(64,(3,3),activation=\"relu\")(out1)\n    out1 = L.Conv2D(64,(3,3),activation=\"relu\")(out1)\n    out1 = L.MaxPooling2D((2,2))(out1)\n    \n    out1 = L.Conv2D(128,(3,3),activation=\"relu\")(out1)\n    out1 = L.Conv2D(128,(3,3),activation=\"relu\")(out1)\n    out1 = L.MaxPooling2D((2,2))(out1)\n    out1 = L.Flatten()(out1)\n    \n    out2 = L.Dense(50,activation=\"relu\",kernel_initializer=\"lecun_normal\")(out1)\n    out2 = L.Dense(30,activation=\"relu\",kernel_initializer=\"lecun_normal\")(out2)\n    out2 = L.Dense(15,activation=\"sigmoid\",kernel_initializer=\"lecun_normal\",name='class_out')(out2)\n    \n    out3 = L.Dense(50,activation=\"relu\",kernel_initializer=\"lecun_normal\")(out1)\n    out3 = L.Dense(30,activation=\"relu\",kernel_initializer=\"lecun_normal\")(out3)\n    out3 = L.Dense(56,activation=\"relu\",kernel_initializer=\"lecun_normal\",name=\"bb_out\")(out3)\n    \n    model = tf.keras.Model(inputs=in1,outputs=[out2,out3])\n    model.compile(loss={'class_out':'categorical_crossentropy','bb_out':'mse'},optimizer=\"adam\")\n    return model","c559e2ad":"model = build()","c309bc06":"tf.keras.utils.plot_model(model)","97bd36ab":"def getTest(path=None):\n    images = []\n    for img in tqdm(os.listdir(path)):\n        im_name = img.split('.npy')[0]\n        image = np.load(path+img)\n        images.append(image)\n    return np.stack(images,axis=0)\n\nX_test = getTest('..\/input\/xraynumpy\/images\/test\/')","e5f9e105":"class_label = np.zeros((len(X_test),15))\nbb_label = np.zeros((len(X_test),56))\n\nfor fold in range(5):\n    print(f'\\nFold: {fold}\\n')\n    \n    X_train = train_df[train_df.kfold!=fold].drop('kfold',axis=1)\n    X_val = train_df[train_df.kfold==fold].drop('kfold',axis=1)\n    \n    dl = DataLoader('..\/input\/xraynumpy\/images\/train\/',X_train,X_val)\n    train_set = dl.flow(batch_size=32)\n    X_eval,Y_eval = dl.getVal()\n    \n    chckpt = tf.keras.callbacks.ModelCheckpoint(f'.\/model_f{fold}.hdf5',monitor='val_loss',mode='min',save_best_only=True)\n    \n    K.clear_session()\n    model = build()\n    \n    model.fit(train_set,\n             epochs=10,\n              steps_per_epoch=int(15000\/32),\n              validation_data = (X_eval,Y_eval),\n              callbacks = [chckpt]\n             )\n    \n    c,b = model.predict(X_test)\n    class_label+=c\n    bb_label+=b\nclass_label = class_label\/5\nbb_label = bb_label\/5\nnp.save('.\/class_label.npy',class_label)\nnp.save('.\/bb_label.npy',bb_label)","98fec8c9":"The submission file must contain the image id and the prediction string in the format \"a b (c,d,e,f)\"<br>where\n<ul>\n    <li>a = predicted class ; 14 for no abnormality<\/li>\n    <li>b= confidence<\/li>\n    <li>(c,d,e,f) = (x_min,y_min,x_max,y_max)<\/li>\n<\/ul>","1895113b":"<ul>\n<li><code>image_id<\/code> - unique image identifier<\/li>\n<li><code>class_name<\/code>&nbsp;- the name of the class of detected object (or \"No finding\")<\/li>\n<li><code>class_id<\/code>&nbsp;- the ID of the class of detected object<\/li>\n<li><code>rad_id<\/code>&nbsp;- the ID of the radiologist that made the observation<\/li>\n<li><code>x_min<\/code>&nbsp;- minimum X coordinate of the object's bounding box<\/li>\n<li><code>y_min<\/code>&nbsp;- minimum Y coordinate of the object's bounding box<\/li>\n<li><code>x_max<\/code>&nbsp;- maximum X coordinate of the object's bounding box<\/li>\n<li><code>y_max<\/code>&nbsp;- maximum Y coordinate of the object's bounding box<\/li>\n<\/ul>","8b26a923":"<h1 style=\"display:inline\"><a id=\"second\">EDA<\/a><\/h1>&emsp;&emsp;&emsp;&emsp;&emsp;<a href=\"#home\" style=\"color:blue\"><img src=\"https:\/\/toppng.com\/uploads\/preview\/light-blue-up-arrow-11550117759k4je61afsa.png\" style=\"display:inline;width:2em;height:2em\"><\/a>","6358a41e":"<h2>Training Loop<\/h2>","48e1c124":"### Distribution of Radiologists","fbac03c8":"<h1 style=\"display:inline\"><a id=\"third\"> An Intuition of the Data<\/a><\/h1>&emsp;&emsp;&emsp;&emsp;&emsp;<a href=\"#home\" style=\"color:blue\"><img src=\"https:\/\/toppng.com\/uploads\/preview\/light-blue-up-arrow-11550117759k4je61afsa.png\" style=\"display:inline;width:2em;height:2em\"><\/a><br><br>\n<h5>Before proceeding further let us try and get an intuition of the data and what exactly we need to do.<\/h5>\n<h5> In this competition we have been given 15000 images for training. Parallelly we have a dataframe containing the ground truths for various abnormalities. Every sample in the datframe contains:<\/h5>\n  <ul>\n      <li>the image id<\/li><li>the id of the radiologist who annoted it<\/li><li>the name of the corresponding class<\/li><li>the class id<\/li><li>the bounding box coordinates<\/li>\n  <\/ul>\n<b style=\"font-weight:700\">Important points to be noted here are:<\/b>\n<ul>\n    <li>Each image may have multiple corresponding abnormalities. Therefore this is a multilabel prediction<\/li>\n    <li>Bounding boxes for each image have been annoted by multiple radiologists. Therefore for every sample we have multiple ground truths. A naive way to deal with this is to take mean of bounding box coordinates by every radiologists for a particular abnormality<\/li>\n    <li>There is a significant class imbalance which is likely to affect the performance of models a lot.<\/li>\n<\/ul>\n<h4 style=\"font-weight:700\">Information about dicom can be found: <a href=\"https:\/\/en.wikipedia.org\/wiki\/DICOM\" style=\"font-size:1em\">Here<\/a><\/h4>\n<h4 style=\"font-weight:700\">Procedure to extract DICOM metadata can be found in: <a href=\"https:\/\/www.kaggle.com\/mrutyunjaybiswal\/vbd-chest-x-ray-abnormalities-detection-eda\" style=\"font-size:1em\">this notebook<\/a><\/h4>","c4bc4fed":"The number of null values are same as the number of samples that do not have any abnormality","60a7efbb":"# Work in Progress....\n<h2 style=\"color:blue\">To Do:<\/h2>\n<ul>\n    <li><h2 style=\"color:blue\">1.Implement submission pipeline<\/h2><\/li>\n    <li><h2 style=\"color:blue\">2.Implement an evaluation metric corresponding to competition evaluation criteria<\/h2><\/li>\n    <li><h2 style=\"color:blue\">3.Choose better loss functions<\/h2><\/li>\n<\/ul>","2cabb24c":"## 2. Images","6759f3ec":"## 1. DataFrames","4f9d020f":"<h2 style=\"color:blue\">Updates:<\/h2>\n<ul>\n    <li>Improved data loading speed by using numpy files dataset<\/li>\n    <li>Implemented Kfold cross validation<\/li>\n    <li>Wrote a training and prediction loop<\/li>\n<\/ul>","ed1ef1ea":"<h1 style=\"display:inline\"><a id=\"fifth\">Model Building and Training<\/a><\/h1>&emsp;&emsp;&emsp;&emsp;&emsp;<a href=\"#home\" style=\"color:blue\"><img src=\"https:\/\/toppng.com\/uploads\/preview\/light-blue-up-arrow-11550117759k4je61afsa.png\" style=\"display:inline;width:2em;height:2em\"><\/a>","fe3c4c8e":"### The Distribution of Classes\nWe can see there is a huge class imbalance. The number of negative examples are very high and a few abnormalities have very few examples ","f3774442":"<h1 style=\"display:inline\"><a id=\"fourth\">Data Preparation<\/a><\/h1>&emsp;&emsp;&emsp;&emsp;&emsp;<a href=\"#home\" style=\"color:blue\"><img src=\"https:\/\/toppng.com\/uploads\/preview\/light-blue-up-arrow-11550117759k4je61afsa.png\" style=\"display:inline;width:2em;height:2em\"><\/a>","d1f87758":"<img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/24800\/logos\/header.png?t=2020-12-17-19-26-15\">\n<center>\n    <h1 style=\"color:red;font-weight:900;font-size:2.5em\">VinBigData Chest X-ray Abnormalities Detection<\/h1>\n    <h3>Automatically localize and classify thoracic abnormalities from chest radiographs<\/h3>\n<\/center>\n<br>\n<br>\n<hr>\n<h2 style=\"color:blue;font-weight:600\"> About Competition <\/h2>\n<p>\n    Radiologists diagnose and treat medical conditions using imaging techniques like CT and PET scans, MRIs, and, of course, X-rays. Yet, as it happens when working with such a wide variety of medical tools, radiologists face many daily challenges, perhaps the most difficult being the chest radiograph. The interpretation of chest X-rays can lead to medical misdiagnosis, even for the best practicing doctor. Computer-aided detection and diagnosis systems (CADe\/CADx) would help reduce the pressure on doctors at metropolitan hospitals and improve diagnostic quality in rural areas.\n<\/p>\n<p>\n    In this competition we are to predict the thoracic abnormalities in given X-Ray images and also locate those abnormalities. The data provided include:\n    <ul>\n    <li>Train and Test X-Ray images in folders <b style=\"font-weight:700\">Train<\/b> and <b style=\"font-weight:700\">Test<\/b>\n    <li> sample submission file in sample_submission.csv\n    <li> train dataframe in train.csv\n    <\/ul>\n<\/p>\n<hr>\n<br>\n<a id=\"home\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"background: rgb(49,114,163);\nbackground: radial-gradient(circle, rgba(49,114,163,1) 0%, rgba(26,136,181,1) 15%, rgba(1,159,200,1) 52%, rgba(0,212,255,1) 60%, rgba(0,182,224,1) 64%, rgba(0,145,186,1) 69%, rgba(1,66,104,1) 82%, rgba(2,33,70,1) 95%, rgba(24,23,50,1) 100%);\">\n  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"style=\"background: rgb(49,114,163);\nbackground: radial-gradient(circle, rgba(49,114,163,1) 0%, rgba(26,136,181,1) 15%, rgba(1,159,200,1) 52%, rgba(0,212,255,1) 60%, rgba(0,182,224,1) 64%, rgba(0,145,186,1) 69%, rgba(1,66,104,1) 82%, rgba(2,33,70,1) 95%, rgba(24,23,50,1) 100%);\">Table of Contents<\/h3>\n    <center>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#first\" role=\"tab\" aria-controls=\"profile\">First Look at the Data<span class=\"badge badge-primary badge-pill\">1<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#second\" role=\"tab\" aria-controls=\"profile\">EDA<span class=\"badge badge-primary badge-pill\">2<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#third\" role=\"tab\" aria-controls=\"profile\">An insight of the Data<span class=\"badge badge-primary badge-pill\">2<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#fourth\" role=\"tab\" aria-controls=\"messages\">Data Preparation<span class=\"badge badge-primary badge-pill\">3<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#fifth\" role=\"tab\" aria-controls=\"messages\">Model Building and training<span class=\"badge badge-primary badge-pill\">4<\/span><\/a>\n    <\/center>\n<\/div>\n<hr>\n<h1 style=\"color:red\">Note:<\/h1>\n<h5 style=\"color:red\">The utilities_x_ray module used here is a script that I have written(can be found <a href=\"https:\/\/www.kaggle.com\/bibhash123\/utilities-x-ray\">here<\/a>). It contains some functions for visualization of the X-Ray images. The dicom image reading pipeline is taken from <a href=\"https:\/\/www.kaggle.com\/raddar\/popular-x-ray-image-normalization-techniques\"> this Notebook<\/a> by <a href=\"https:\/\/www.kaggle.com\/raddar\">@raddar<\/a><\/h5>\n<h5 style=\"color:red\">The numpy files dataset used in this notebook can be found <a href=\"https:\/\/www.kaggle.com\/bibhash123\/xraynumpy\">Here<\/a><\/h5>","68366f7d":"<h1 style=\"display:inline\"> <a id=\"first\"> First Look at the data<\/a><\/h1>&emsp;&emsp;&emsp;&emsp;&emsp;<a href=\"#home\" style=\"color:blue\"><img src=\"https:\/\/toppng.com\/uploads\/preview\/light-blue-up-arrow-11550117759k4je61afsa.png\" style=\"display:inline;width:2em;height:2em\"><\/a>"}}