{"cell_type":{"b527f142":"code","f808f97d":"code","19b8f1e6":"code","2965d509":"code","7cdebab1":"code","f371c9ee":"code","1058aa36":"code","bbc91564":"code","b83d810d":"code","5a1e0253":"code","2b0cf586":"code","f0d595f1":"code","ff0ce701":"code","679b2977":"code","0ec86033":"code","875bfee9":"code","8da96c1b":"code","b3cb6daa":"code","39aa8c07":"code","e52ae56a":"code","7c7a00ba":"code","09c1da46":"code","d082e337":"code","dd0a9fca":"code","c6fa8d99":"code","55d57254":"code","4ad2df4f":"code","ff8dd525":"code","f552c1ac":"code","3fe6c3b3":"code","7ac7d451":"code","fbb0d40d":"code","8f17a846":"code","c2fc66ac":"code","a90323fd":"code","9eeca427":"code","837ae626":"code","f59702ef":"code","cad0acb1":"code","48b1af09":"code","0f6c0097":"code","53428da8":"code","c149cd7c":"code","6e6fefb9":"code","9ff70070":"code","1fb24193":"markdown","8a3add3d":"markdown","ee071a0e":"markdown","fd093feb":"markdown","ae9e42cf":"markdown","2612d8a5":"markdown","f0b64d78":"markdown","4ffa2770":"markdown","26c75133":"markdown","4599fc4e":"markdown","3b593a06":"markdown","8f605a1c":"markdown","ef584256":"markdown","c94a3516":"markdown","34d0f4f3":"markdown","9ca7b5ff":"markdown","b17e270a":"markdown","1338e248":"markdown","1d3f7719":"markdown","c17fe696":"markdown","e551befd":"markdown","bb271bde":"markdown","97df6b0f":"markdown","27896731":"markdown","5addf5a9":"markdown","8d334c0a":"markdown","1f73c21c":"markdown"},"source":{"b527f142":"import pandas as pd\nimport sklearn\nimport matplotlib.pyplot as plt","f808f97d":"df_train = pd.read_csv('.\/data\/train.csv')\ndf_test = pd.read_csv('.\/data\/test.csv')","19b8f1e6":"df_train","2965d509":"df_train[\"Survived\"].value_counts()","7cdebab1":"df_train[ df_train[\"Sex\"] == \"female\" ][\"Survived\"].value_counts()","f371c9ee":"%matplotlib inline\n\n# Plots the \ndf_train[\"Survived\"].value_counts().plot( kind = 'bar' )","1058aa36":"%matplotlib inline\n\ndf_train[ df_train[\"Sex\"] == \"female\" ][\"Survived\"].value_counts().plot( kind = \"bar\" )","bbc91564":"df_train[\"Pclass\"].value_counts()","b83d810d":"df_train[ df_train[\"Pclass\"] == 3 ][\"Survived\"].value_counts()","5a1e0253":"FirstClass = df_train[ df_train[\"Pclass\"] == 1 ][\"Survived\"].value_counts()\nSecondClass = df_train[ df_train[\"Pclass\"] == 2 ][\"Survived\"].value_counts()\nThirdClass = df_train[ df_train[\"Pclass\"] == 3 ][\"Survived\"].value_counts()\n\nfig, axs = plt.subplots(1,3)\nfig.suptitle('The relationship between class and survival')\n\naxs[0].bar( [1, 0], FirstClass.to_numpy()\/FirstClass.to_numpy().sum() )\naxs[0].set_ylim([0, 1])\n\naxs[1].bar( [0, 1], SecondClass.to_numpy()\/SecondClass.to_numpy().sum() )\naxs[1].set_ylim([0, 1])\n\naxs[2].bar( [0, 1], ThirdClass.to_numpy()\/ThirdClass.to_numpy().sum() )\naxs[2].set_ylim([0, 1])\n","2b0cf586":"df_train[ (df_train[\"Pclass\"] == 1) & (df_train[\"Sex\"] == \"female\") ][\"Survived\"].value_counts().plot( kind = \"bar\" )","f0d595f1":"# Gives the number of unique values of a column\n\nlen( df_train[\"Name\"].unique() )","ff0ce701":"df_train.drop( columns = [ \"Name\", \"Ticket\", \"Cabin\"], inplace = True)\ndf_test.drop( columns = [ \"Name\", \"Ticket\", \"Cabin\"], inplace = True)","679b2977":"# Finds all NaN values in a column\n\ndf_train[ df_train[\"Age\"].isna() ]","0ec86033":"df_train.loc[ (df_train[\"Age\"].isna()) & (df_train[\"Sex\"] == \"male\"), \"Age\" ] = df_train[ (~df_train[\"Age\"].isna()) & (df_train[\"Sex\"] == \"male\") ][\"Age\"].mean()\ndf_train.loc[ (df_train[\"Age\"].isna()) & (df_train[\"Sex\"] == \"female\"), \"Age\" ] = df_train[ (~df_train[\"Age\"].isna()) & (df_train[\"Sex\"] == \"female\") ][\"Age\"].mean()","875bfee9":"df_test.loc[ (df_test[\"Age\"].isna()) & (df_test[\"Sex\"] == \"male\"), \"Age\" ] = df_test[ (~df_test[\"Age\"].isna()) & (df_test[\"Sex\"] == \"male\") ][\"Age\"].mean()\ndf_test.loc[ (df_test[\"Age\"].isna()) & (df_test[\"Sex\"] == \"female\"), \"Age\" ] = df_test[ (~df_test[\"Age\"].isna()) & (df_test[\"Sex\"] == \"female\") ][\"Age\"].mean()","8da96c1b":"df_train","b3cb6daa":"len( df_train[ df_train[\"Embarked\"].isna()] )","39aa8c07":"len( df_train[\"Embarked\"].unique() )","e52ae56a":"len( df_train[ df_train[\"Pclass\"].isna()] )","7c7a00ba":"df_train.loc[ df_train[\"Sex\"] == \"male\", \"Sex\"] = 1\ndf_train.loc[ df_train[\"Sex\"] == \"female\", \"Sex\"] = 0\n\ndf_test.loc[ df_test[\"Sex\"] == \"male\", \"Sex\"] = 1\ndf_test.loc[ df_test[\"Sex\"] == \"female\", \"Sex\"] = 0","09c1da46":"df_train","d082e337":"df_train = pd.get_dummies( df_train )\ndf_train.drop( columns = [ \"Sex_0\" ], inplace = True)\ndf_train = df_train.rename(columns={ \"Sex_1\": \"Sex\"})","dd0a9fca":"df_test = pd.get_dummies( df_test )\ndf_test.drop( columns = [ \"Sex_0\" ], inplace = True)\ndf_test = df_test.rename(columns={ \"Sex_1\": \"Sex\"})","c6fa8d99":"df_train","55d57254":"df_test","4ad2df4f":"from sklearn.ensemble import ExtraTreesClassifier","ff8dd525":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split( df_train.drop(columns = [\"PassengerId\", \"Survived\"]),\n                                                                  df_train[\"Survived\"],\n                                                   test_size = 0.01, random_state = 50 )","f552c1ac":"X_train","3fe6c3b3":"clf = ExtraTreesClassifier(n_estimators=100, max_depth = 20, random_state=0)","7ac7d451":"clf.fit(X_train, y_train)","fbb0d40d":"from sklearn.metrics import accuracy_score, confusion_matrix\n\naccuracy_score(  list(y_test), clf.predict(X_test) ) ","8f17a846":"confusion_matrix(  list(y_test), clf.predict(X_test) ) ","c2fc66ac":"df_test","a90323fd":"df_test.columns[ df_test.isna().any()]","9eeca427":"df_test[ df_test[\"Fare\"].isna()]","837ae626":"df_test.loc[ df_test[\"Fare\"].isna(), \"Fare\"] = df_test[ df_test[\"Pclass\"] == 3][\"Fare\"].mean()\ndf_test.columns[ df_test.isna().any()]","f59702ef":"df_test","cad0acb1":"clf.predict( df_test.drop(columns = [\"PassengerId\"]))","48b1af09":"df_test[\"Survived\"] = clf.predict( df_test.drop(columns = [\"PassengerId\"]))\nresults = df_test[[\"PassengerId\", \"Survived\"]]","0f6c0097":"results","53428da8":"results.to_csv(\"titanic_submission.csv\", index = False)","c149cd7c":"pd.read_csv(\"titanic_submission.csv\")","6e6fefb9":"import numpy as np\n\nscore = []\n\nfor dep in np.arange(1, 110, 5):\n    for ran in np.arange(0, 1001, 50):\n\n        res = []\n\n        X_train, X_test, y_train, y_test = train_test_split( df_train.drop(columns = [\"PassengerId\", \"Survived\"]),\n                                                                      df_train[\"Survived\"],\n                                                       test_size = 0.2, random_state = ran )\n\n        clf = ExtraTreesClassifier(n_estimators=100, max_depth = dep, random_state=0)\n        clf.fit(X_train, y_train)\n        res.append( accuracy_score(list(y_test), clf.predict(X_test)) )\n\n    score.append( np.array(res).mean() )","9ff70070":"fig = plt.plot(np.arange(1, 110, 5), score )","1fb24193":"# Titanic survivers","8a3add3d":"Now we need to now prepare the feature so that they can be recieved by the model. Lets first look at what our feature vectors look like and see how they need to be adjusted so that they can be inputed into the model.","ee071a0e":"## The Model ","fd093feb":"# Improving scores and Hyperparameter tuning ","ae9e42cf":"Here we are going to change one of the paramenters of the model and see which value produces the best accuracy.","2612d8a5":"We have processed the test data in the same way as for the training data. Now we need to feed it to the model and save the results to a .csv file.","f0b64d78":"We now import the model and split our training set into a training set and a test set so that we can see the accuracy of the model.","4ffa2770":"Now we need to see which features we want to include in the model. Features like **cabin number**, **ticket number** and **name** will not be the best indicators of survivability. So lets remove them. Also as mentioned earlier, one needs to remove or fill in missing data, we do not want any NaN values in our training set.\n\nWe remove **Name**, **Ticket number**, and **Cabin number** because they do not serve as good indicators of survivability and they contain many NaN values.\n\n**Name** is called a *high cardinality* feature since there are several unique values.","26c75133":"Now we are plotting some interesting aspects of the data and seeing if we can get a \"feel\" for the data. Without conducting a extensive analysis of the data we are able to identify general trends of the data, for example, females have a much greater probability of serviving. All this information will be useful when we construct our model, this might inform us on which models to use and where we should include biases, etc.","4599fc4e":"Now we want to remove the NaN values from the rest of the features and we will just insert a \"reasonable\" value, i.e. we can use the average age of a passenger to fill in any missing age values in the data frame.\n\nThere are 177 passegers that we do not know the age of, we must iether remove this feature or fill in in with a *reasonable value* like the average, hopefully this won't mess with the statistics to badly. This i were we need to worry about baises, maybe most of the $3^{\\text{rd}}$ class passegers didn't record their ages, etc.","3b593a06":"And if you were a female first class passenger then you were **very** likely to survive.","8f605a1c":"Lets now change the features of the data frame to binary or numerical features. The **Sex** clumn is done by replaceing every occurance of \"male\" with a $1$ and \"female\" will be replaced by a $0$","ef584256":"Now we have done data exploratortion, cleened the data and prepared our *feature vector* for the input into our model. We will be using SciKit-learn to create a model for this problem.\n\nWe will use the *Extra tree classifier* from the SicKit-learn library. One can read the documentation [here.](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html)\n\nThe Extra tree classifier is a modification of the randm forest model that introduces randomness in the classifier.","c94a3516":"All the features are ready to be used (are numerical or binary values) besides **Sex** and **Embarked**. *Sex* can easily be converted into a binary value, $1$ for males and $0$ for females. Lets see how we can deal with the *Embarked* feature.\n\nLets see how many values this feature can take as well as how many missing values there are. There are only $2$ missing entries and the feature takes 4 values 1 of which is NaN.","34d0f4f3":"### Preparing the data for the model","9ca7b5ff":"The training data set consists of a binary label (1 if the passenger survived and 0 if the opposite is true) and a vector $X$ often called the *feacture vector*. This vector $X$ consists of $n$ features which can be used to describe a passenger such as, Class, Sex, Age, Fare, etc. \n\nThe main idea is to use this training set to predicted if a passenger from an unlabelled data set survived or not depending on their *feature vector*. The feature vector $X$ is a mapping from the real world to an object (vectors and\/or matrices) that a computer can understand and describes the situation who's outcome one wants to predict which we can denote as $y$.\n\nSo we want to see how $X$ is correlater\/related to $y$.","b17e270a":"# Calculating the predictions and saving them","1338e248":"Now we can see how well the model has made predictions by using some more useful functions from SciKit-learn.\n\nWe can measure the accuracy and compute the confusion matrix using built-in functions.","1d3f7719":"As you can see, there are elements of this *feature vector* that need to be edited (removed or simplified) so that the computer can deal with it more easily. One also needs to make sure that all the the vectors have values in all locations, we do not want any gaps in the data otherwise we will get errors.\n\n### Initial data exploration\n\nWe will also introduce some useful functions from the **Pandas** library so use in later projects. This will all go towards a \"standard approach\" to a data science\/machine learning problem that can be applied to most problems in the future.","c17fe696":"Importing the appropriate libraries. Pandas for manipulating the data and SciKit-Learn for the machine learning models. Matplotlib for graphics and plots.","e551befd":"Reading in the traning and test data sets. The data sets arw provided by *Kaggle* and are part of the *Titanic Dataset* which is an entry level machine learning project that is traditionally one of the classic machine learning problems for beginners.","bb271bde":"Insering **missing** values in the test data set.","97df6b0f":"Now for the **Embarked** feature. Here we use a nice Pandas function, namely *get_dummies*","27896731":"#### References:\n\n1. P. Geurts, D. Ernst., and L. Wehenkel, \u201cExtremely randomized trees\u201d, Machine Learning, 63(1), 3-42, 2006.","5addf5a9":"What values can *Pclass* take? It can take in a numerical value from 1 to 3 for First, Second and Third class. Note that there are also some obvious trends that can be seen just at this level of data exploration. Looking at the data we see that the class of a passenger is important when determining if a passenger survived or not. There are some plots to illusrtate this.","8d334c0a":"Now we replace the missing ages with the **averages** of each sex in both the training and test data set.","1f73c21c":"So we see that for a depth of $10$ or more we have the best predictions."}}