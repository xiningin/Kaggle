{"cell_type":{"cca2429b":"code","5c2094a1":"code","8f55c799":"code","1df587a8":"code","67447a28":"code","0879f171":"code","e4730d73":"code","fa53520b":"code","bc10b3be":"code","c8e6dd2c":"code","10203041":"code","798c0b86":"code","dc9bd875":"code","68909158":"code","99293453":"code","45a03d4b":"code","71e3b5e6":"code","1192887c":"code","6c83ed08":"code","419f3e3e":"code","09dfcf24":"code","e94c2060":"code","106f6a46":"code","29caac0d":"code","2a84405a":"markdown","8ebaaa31":"markdown","9b8912f6":"markdown","30df99e8":"markdown","2f9a03d4":"markdown","6ae95c71":"markdown","f3e28586":"markdown","61ae2c05":"markdown"},"source":{"cca2429b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5c2094a1":"#Imports\nimport numpy as np\nimport pandas as pd\nimport os\nimport sklearn\nfrom  matplotlib import pyplot as plt\nfrom matplotlib.pyplot import subplots\nimport seaborn as sns","8f55c799":"#Reading the train meta data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/petfinder-pawpularity-score\/train.csv\")\ntrain_data","1df587a8":"# Reading the test metadata\ntest_data = pd.read_csv(\"\/kaggle\/input\/petfinder-pawpularity-score\/test.csv\")\ntest_data","67447a28":"#Assigning the X and y columns and performing the train test split\nX = train_data.iloc[:,1:13]\nX","0879f171":"y = train_data[\"Pawpularity\"]\ny","e4730d73":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y)","fa53520b":"#Checking for correlation between the input and target variable to spot the obvious trends if any\nfig, ax = plt.subplots(4, 3, figsize=(15, 15))\nfor var, subplot in zip(X, ax.flatten()):\n    sns.boxplot(x=var, y='Pawpularity', data=train_data, ax=subplot)","bc10b3be":"fig, ax = plt.subplots(4, 3, figsize=(15, 15))\nfor var, subplot in zip(X, ax.flatten()):\n    sns.countplot(x=var, data=train_data, ax=subplot);","c8e6dd2c":"from sklearn.model_selection import StratifiedShuffleSplit\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=77)\nsss.get_n_splits(X, y)\nprint(sss)\nfor train_index, test_index in sss.split(X, y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X.iloc[list(train_index)], X.iloc[list(test_index)]\n    y_train, y_test = y.iloc[list(train_index)], y.iloc[list(test_index)]","10203041":"from sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn import metrics\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=77)\nsss.get_n_splits(X, y)\n# X, y = make_regression(n_samples=9912, n_features=12, random_state=1)\nfor train_index, test_index in sss.split(X, y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X.iloc[list(train_index)], X.iloc[list(test_index)]\n    y_train, y_test = y.iloc[list(train_index)], y.iloc[list(test_index)]\n    model = LinearRegression()\n\n    model.fit(X_train, y_train)\n\n    importance = model.coef_\n\n    print(importance)\n\n    importance_linear_reg = pd.DataFrame(importance, columns=[\"Importance\"])\n    X_columns = train_data.iloc[:,1:13]\n    importance_linear_reg[\"Importance\"] = round(importance_linear_reg[\"Importance\"],2)\n    importance_linear_reg[\"Features\"] = X_columns.columns\n    importance_linear_reg = importance_linear_reg.sort_index(ascending = True, axis = 1)\n    importance_linear_reg\n\n    # plot feature importance\n    pyplot.bar(importance_linear_reg[\"Features\"], importance_linear_reg[\"Importance\"])\n    pyplot.xticks(rotation=90)\n    pyplot.show()\n    \n    y_prediction =  model.predict(X_test)\n    y_prediction\n    \n    print(\"Mean Absolute Error:\", metrics.mean_absolute_error(y_test, y_prediction))\n    print(\"Mean Squared Error:\", metrics.mean_squared_error(y_test, y_prediction))\n    print(\"Root Mean Squared Error:\", np.sqrt(metrics.mean_absolute_error(y_test, y_prediction)))\n    print(\"Mean Absolute Percentage Error:\", np.mean(np.abs((y_test - y_prediction)\/y_test))*100)\n    \n    \n    test_pred = model.predict(test_data.iloc[:,1:13])\n    print(test_pred)\n    \n    Solution_Linear_Reg = pd.DataFrame(test_data[\"Id\"], columns=[\"Id\"])\n    Solution_Linear_Reg['Pawpularity']=test_pred\n    print(Solution_Linear_Reg)","798c0b86":"# Preparing for loading the images\nfrom pathlib import Path\nimport os.path\nimport pandas as pd\n\ntrainpaths = {}\ntestpaths ={}\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if 'train\/' in (os.path.join(dirname, filename)):\n            filenames = filename.rstrip(\".jpg\")\n            trainpaths[filenames] = os.path.join(dirname, filename)\n        if 'test\/' in (os.path.join(dirname, filename)):\n            filenames = filename.rstrip(\".jpg\")\n            testpaths[filenames] = os.path.join(dirname, filename)\n\ntrain_path = pd.DataFrame(list(trainpaths.items()),columns=['Id', 'filepath'])\ntrain_path = train_path.set_index('Id')\ntrain_path\ntest_path = pd.DataFrame(list(testpaths.items()),columns=['Id', 'filepath'])\ntest_path= test_path.set_index('Id')\ntest_path\n","dc9bd875":"#Join the image file paths with the meta data files containing pawpularity \ntrain_merged = pd.merge(train_data, train_path, on=[\"Id\"], how='left')\ntrain_merged\ntest_merged = pd.merge(test_data, test_path, on=[\"Id\"], how='left')\ntest_merged","68909158":"#train test split\nX = train_merged['filepath']\ny = train_merged['Pawpularity']\ns = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=77)\ns.get_n_splits(X, y)\nfor train_index, test_index in s.split(X, y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X.iloc[list(train_index)], X.iloc[list(test_index)]\n    y_train, y_test = y.iloc[list(train_index)], y.iloc[list(test_index)]\ntrain_final = pd.merge(X_train, y_train, left_index=True, right_index=True)\nprint(train_final)\ntest_final = pd.merge(X_test, y_test, left_index=True, right_index=True)\nprint(test_final)","99293453":"#Loading the images, rescaling the image pixels from range 0-255 to range 0-1\nimport tensorflow as tf\n\ntrain_gen = tf.keras.preprocessing.image.ImageDataGenerator(\nrescale =1.\/255,\nvalidation_split = 0.2\n)\n\ntest_gen = tf.keras.preprocessing.image.ImageDataGenerator(\nrescale = 1.\/255\n)","45a03d4b":"#flow from Df function keras: image augmentation\n\ntrain_images = train_gen.flow_from_dataframe(\n    dataframe = train_final,\n    x_col = 'filepath',\n    y_col = 'Pawpularity',\n    target_size = (32,32),\n    color_mode = 'rgb',\n    shuffle = False,\n    class_mode = 'raw',\n    subset = 'training',\n    batch_size = 32,\n    seed = 77    \n)\n\nvalidation_images = train_gen.flow_from_dataframe(\n    dataframe = train_final,\n    x_col = 'filepath',\n    y_col = 'Pawpularity',\n    target_size = (32,32),\n    color_mode = 'rgb',\n    shuffle = False,\n    class_mode = 'raw',\n    subset = 'validation',\n    batch_size = 32,\n    seed = 77    \n)\n\ntest_images = test_gen.flow_from_dataframe(\n    dataframe = test_merged,\n    x_col = 'filepath',\n    y_col = None,\n    target_size = (32,32),\n    color_mode = 'rgb',\n    shuffle = False,\n    class_mode = None,\n    batch_size = 32,\n    seed = 77    \n)","71e3b5e6":"#Training the NN\nlayer1 = tf.keras.Input(shape=(32,32,3))\nlayer2 = tf.keras.layers.Conv2D(filters=32, kernel_size = (3,3), padding = 'same')(layer1)\nlayer3 = tf.keras.activations.relu(layer2, alpha=0.0, max_value=None, threshold=0.0)\nlayer4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(1, 1))(layer3)\nlayer5 = tf.keras.layers.Conv2D(filters=32, kernel_size = (3,3))(layer4)\nlayer6 = tf.keras.activations.relu(layer5, alpha=0.0, max_value=None, threshold=0.0)\nlayer7 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(1, 1))(layer6)\n\nlayer8 = tf.keras.layers.Conv2D(filters=64, kernel_size = (3,3), padding = 'same')(layer7)\nlayer9 = tf.keras.activations.relu(layer8, alpha=0.0, max_value=None, threshold=0.0)\nlayer10 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(1, 1))(layer9)\nlayer11 = tf.keras.layers.Conv2D(filters=64, kernel_size = (3,3))(layer10)\nlayer12 = tf.keras.activations.relu(layer11, alpha=0.0, max_value=None, threshold=0.0)\nlayer13 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(1, 1))(layer12)\n\nlayer14 = tf.keras.layers.GlobalAveragePooling2D()(layer13)\n\nlayer15 = tf.keras.layers.Dense(64, activation='relu')(layer14)\nlayer16 = tf.keras.layers.Dense(64, activation='relu')(layer15)\nlayer17 = tf.keras.layers.Dense(1, activation='linear')(layer16)\n\nmodel = tf.keras.Model(inputs = layer1, outputs = layer17)\n\nmodel.compile(optimizer='adam',loss=\"mse\", metrics=['accuracy'])\n\nmodelfit = model.fit(train_images, epochs = 5, validation_data = validation_images,\n                    callbacks = [\n                        tf.keras.callbacks.EarlyStopping(\n                        monitor='val_loss',\n                        patience=4,\n                        restore_best_weights = True)\n                    ])","1192887c":"#Printing the output of all the intermediate layes to understand the network better\nprint(layer1)\nprint(layer2)\nprint(layer3)\nprint(layer4)\nprint(layer5)\nprint(layer6)\nprint(layer7)\nprint(layer8)\nprint(layer9)\nprint(layer10)\nprint(layer11)\nprint(layer12)\nprint(layer13)\nprint(layer14)\nprint(layer15)\nprint(layer16)\nprint(layer17)","6c83ed08":"#Train set prediction calculations\nimport numpy as np\npred = np.squeeze(model.predict(train_images))\nprint(pred)\nreal_pawpularity = train_images.labels\nprint(real_pawpularity)\nscore = model.evaluate(train_images, verbose=1)\nprint(score)\nnp.sqrt(score)","419f3e3e":"#Validation set prediction calculations\nimport numpy as np\npred = np.squeeze(model.predict(validation_images))\nprint(pred)\nreal_pawpularity = validation_images.labels\nprint(real_pawpularity)\nscore = model.evaluate(validation_images, verbose=1)\nprint(score)\nnp.sqrt(score)","09dfcf24":"#Test set  prediction calculations\nimport numpy as np\nfinal_pred = model.predict(test_images)\nprint(final_pred)","e94c2060":"# Adding the final prediction for the test images into a CSV file\noutput = pd.DataFrame(test_merged['Id'], columns=[\"Id\"])\noutput['Pawpularity']=final_pred\noutput['Pawpularity']= output['Pawpularity'].apply(lambda x: round(x, 2))\noutput = output.set_index('Id')\nprint(output)","106f6a46":"output","29caac0d":"# Submit the CSV\noutput.to_csv('.\/submission.csv', index = 'Id')","2a84405a":"# **Visuals**","8ebaaa31":"# **Stratified ShuffleSplit cross validator example**","9b8912f6":"Since the boxes of each feature against the target variable does not show a striking difference & the boxes of 0's and 1's are almost at the same level. Hence, there's no direct relationship of each variable with the target.","30df99e8":"From the above histograms, we can deduce that the feature values are highly unbalanced in the given datatset. Hence, before applying the ML models, we need to perform stratified sampling on the train set.","2f9a03d4":"The linear regression gives us a MAE of approximately 15. This means that the prediction can deviate by +-15 pawpularity score. Also, the MAPE is about 78%, hence we need to explore image regression techniques.","6ae95c71":"# Linear Regression on the metadata (Baseline)","f3e28586":"# CNN for regression prediction with Keras","61ae2c05":"# **EDA**"}}