{"cell_type":{"ee625d66":"code","063b8a1f":"code","c7ed87bb":"code","12ab8e47":"code","aad63fb0":"code","7ae8067b":"code","d6d22ba3":"code","cbb8632c":"code","b88559c8":"code","e9296485":"code","f91ea687":"code","d5eb2768":"code","02908fa6":"code","db115d7a":"code","e0ed0644":"code","e6c4f925":"code","63142b9d":"code","991e76d0":"code","95b6dfcc":"code","bde691e2":"code","35d96a1d":"code","1f61c0e6":"code","dea6a856":"code","45cece26":"code","b9e8dcd0":"code","b1e75849":"code","598db40a":"code","86a0a3e7":"code","6658dcff":"code","2628c3f0":"code","69a8c597":"code","471c44a3":"code","27b2303d":"code","dcf58d50":"code","06124c50":"code","df49507a":"code","739d5bdb":"code","46f3ca02":"code","d27ffaa5":"code","48b08fcb":"code","3dd2443a":"code","b06c1b45":"code","0c0ee66a":"code","c3df991f":"code","60cc6d49":"code","de69fb44":"code","d0eee7eb":"code","bf966785":"code","8b15493a":"code","891b28c6":"code","c83fa517":"code","12f80370":"code","bfede6c2":"code","8efbb4d8":"code","dc6a416e":"code","6698539b":"code","65aaa06d":"markdown","938ef608":"markdown","edf07ca7":"markdown","9a7e5a4c":"markdown","e162072f":"markdown","96eeb696":"markdown","f51c4a51":"markdown","0e7db7e8":"markdown","f48fe37f":"markdown","bbc51b1c":"markdown","05b95679":"markdown","673e2b1d":"markdown","4809bac5":"markdown","7ce71fd3":"markdown","f5cb3bfb":"markdown","5a839454":"markdown","1f23ed01":"markdown","8d47754b":"markdown","5755b39b":"markdown","2de34df8":"markdown","3c74e3c7":"markdown","99176dd7":"markdown","f809c15a":"markdown","27d7b74f":"markdown","7951dead":"markdown","25defdea":"markdown","c4092714":"markdown","8856c2a0":"markdown","77c8737a":"markdown","99d36a8f":"markdown","022dbb52":"markdown","60359218":"markdown","2119fd76":"markdown","1d245c05":"markdown","0cf6ce7e":"markdown","a9ec4b6e":"markdown","1467b014":"markdown","c35b5f57":"markdown","fd11cafe":"markdown","250bdd0f":"markdown","687753cf":"markdown","8b674b15":"markdown","6c59c506":"markdown","0ab2c195":"markdown","e35a0924":"markdown","c178525d":"markdown","c0a00904":"markdown","6e2a274e":"markdown","2ac9c1da":"markdown","453746a0":"markdown","1a1a5bbd":"markdown","40d80b87":"markdown","52cc590c":"markdown","6f9711c8":"markdown","738dbc93":"markdown","c12a9291":"markdown","628fde53":"markdown","c2356e3d":"markdown","c78df57e":"markdown","614a7b3c":"markdown","7a11fbc6":"markdown","2d053928":"markdown","f8641696":"markdown","01bfcae8":"markdown","ea9e1547":"markdown","c8167d60":"markdown","8deb26f8":"markdown","405be432":"markdown","e6575645":"markdown","c03022b3":"markdown","b086eb4f":"markdown","1cffac49":"markdown","daffd2f1":"markdown","d8dfb57d":"markdown","856e6565":"markdown","411f474b":"markdown","839e183b":"markdown","e602bbdf":"markdown","bc65be6e":"markdown","01a99b96":"markdown","aa611de0":"markdown","99b721ee":"markdown","af1dedb5":"markdown","f8a5c541":"markdown","e07d856b":"markdown","918d34fe":"markdown","b8182c3a":"markdown","3655c318":"markdown","02e7b500":"markdown","5d5b53a9":"markdown","4712f5c7":"markdown","90223362":"markdown","53e43076":"markdown","53f74a9e":"markdown","a3001ab9":"markdown","4c1bdaaf":"markdown"},"source":{"ee625d66":"import numpy as np\nimport pandas as pd  # To read data\nimport matplotlib.pyplot as plt  # To visualize\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.stats import uniform as sp_rand\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import Ridge","063b8a1f":"data = pd.read_csv('..\/input\/insurance.csv')","c7ed87bb":"# show the first 10 lines\ndata.head(10)","12ab8e47":"data.describe()","aad63fb0":"categorical_summaries = [data[c].value_counts() for c in data.columns if data[c].dtype == 'object']\n\nfor i in categorical_summaries:\n    display(pd.DataFrame(i))","7ae8067b":"def plot_numeric_distribution(colnumber, plot_type='histogram', data=data):\n    \"\"\"\n    Function for plotting histogram of the column number\n    corresponding to the numerical variable selected, \n    or scatter plot of final target var against numerical variable\n    \"\"\"\n    \n    # if scatter, plot scatter plot with final variable as y axis\n    if plot_type=='scatter':\n        # x label for the plot\n        plt.xlabel(data.columns[colnumber])\n        plt.ylabel(data.columns[-1])\n        plt.title(data.columns[-1] + ' distribution depending on ' + data.columns[colnumber])\n        plt.scatter(data.iloc[:,colnumber], data.iloc[:,-1],marker='x', alpha=0.5)\n       \n    # else if boxplot, plot boxplots with final variable as y axis\n    elif plot_type=='boxplot':\n        data2=data\n        data2[data2.columns[colnumber]]=data2[data2.columns[colnumber]].astype('category',copy=False)\n        sns.set(style=\"ticks\", color_codes=True)\n        sns.catplot(x=data2.columns[colnumber], y=data2.columns[-1], kind='box',data=data2);\n        plt.title(data2.columns[-1] + ' distribution depending on ' + data2.columns[colnumber])        \n        \n    # otherwise plot histogram of the column \n    elif plot_type=='histogram':\n        # x label for the plot\n        plt.xlabel(data.columns[colnumber])\n        plt.ylabel('Frequency')\n        plt.title(data.columns[colnumber] + ' distribution')\n        data.iloc[:,colnumber].plot.hist()\n","d6d22ba3":"plot_numeric_distribution(-1)","cbb8632c":"plot_numeric_distribution(0,'boxplot')","b88559c8":"plot_numeric_distribution(0, 'scatter')","e9296485":"plot_numeric_distribution(2)","f91ea687":"plot_numeric_distribution(2, 'scatter')","d5eb2768":"plot_numeric_distribution(3)","02908fa6":"plot_numeric_distribution(3, 'scatter')","db115d7a":"plot_numeric_distribution(3, 'boxplot')","e0ed0644":"plot_numeric_distribution(-1)","e6c4f925":"# specify which features you want to transform to be more Gaussian\nnon_normal_features = [0,3,6]\n\n# load the powertransformer tool\npower_transformer = preprocessing.PowerTransformer(standardize=False)\n\n# fit the model and immediately use it to transform your data\ndata.iloc[:,non_normal_features] = power_transformer.fit_transform(data.iloc[:,non_normal_features])","63142b9d":"plot_numeric_distribution(6)","991e76d0":"data.describe()","95b6dfcc":"scaler = preprocessing.StandardScaler()\ndata[data.columns[data.dtypes!='object']] = scaler.fit_transform(data[data.columns[data.dtypes!='object']])","bde691e2":"data.head()","35d96a1d":"def categorical_distribution (colnumber,plot_type):\n    \"\"\"\n    Function for plotting histogram of the column number\n    corresponding to the categorical variable selected against the final target, \n    or boxplot of final target var against categorical variable.\n    \"\"\"    \n    \n    if plot_type=='histogram':\n        # x, y and title labels\n        plt.xlabel(data.columns[colnumber])\n        plt.ylabel('Frequency')\n        plt.title(data.columns[colnumber] + ' distribution')\n        data.iloc[:,colnumber].value_counts().plot(kind='bar')\n        \n    elif plot_type=='boxplot':\n        # setting type of plot\n        sns.set(style=\"ticks\", color_codes=True)\n        # setting what values we plot\n        sns.catplot(x=data.columns[colnumber], y=data.columns[-1], kind='box',data=data);\n        #title\n        plt.title(data.columns[-1] + ' distribution depending on ' + data.columns[colnumber])        ","1f61c0e6":"categorical_distribution(1,'histogram')","dea6a856":"categorical_distribution(1,'boxplot')","45cece26":"categorical_distribution(4,'histogram')","b9e8dcd0":"categorical_distribution(4,'boxplot')","b1e75849":"categorical_distribution(5,'histogram')","598db40a":"categorical_distribution(5,'boxplot')","86a0a3e7":"# 1,4 and 5 are the categorical variables index\ndummy= pd.get_dummies(data.iloc[:,[1,4,5]])\ndummy.head()","6658dcff":"# we select the numerical variables (0, 2 and 3 are the index)\nnum_data=data.iloc[:,[0,2,3]]\n\n# we select the output variable\ny=data.iloc[:,-1]\n\n# we concatenate the numerical variables, to the dummy variables and the ouput variable \ndata=pd.concat([num_data,dummy,y],axis=1)\n\n# first 10 lines of the final dataset\ndata.head(10)","2628c3f0":"# values visualization\ndata.corr()","69a8c597":"# graphical visualization\nf = plt.figure(figsize=(19, 15))\nplt.matshow(data.corr(), fignum=f.number)\nplt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=45)\nplt.yticks(range(data.shape[1]), data.columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\nplt.title('Correlation Matrix', fontsize=16, y=1.11); ","471c44a3":"data=data.drop(['sex_male','smoker_no'], axis=1)\ndata.head()","27b2303d":"# create training and testing variables. test_size describes what proportion \n# of the initial set we want for our test set\n# you can select the proportion you want\nX_train, X_test, y_train, y_test = train_test_split(data.iloc[:,:-1], data.iloc[:,-1], test_size=0.2)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","dcf58d50":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()  \nregressor.fit(X_train, y_train) #training the algorithm","06124c50":"print(\"R2_score=\"+str(round(regressor.score(X_train, y_train), 4)))\n# To retrieve the intercept:\nprint(\"intercept = \" + str(round(regressor.intercept_, 4)))\n# For retrieving the slope:\nfor i in range(0,len(regressor.coef_)):\n    print(data.columns[i]+ \" coefficient = \" + str(round(regressor.coef_[i], 4)))","df49507a":"# prediction\ny_pred = regressor.predict(X_test)\n\n# comparison of the predictions to the actual values\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf.head(10)","739d5bdb":"# 3 accuracy scores\nprint('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test, y_pred), 4))  \nprint('Mean Squared Error:', round(metrics.mean_squared_error(y_test, y_pred), 4))\nprint('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)), 4))","46f3ca02":"ridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)","d27ffaa5":"print(\"R2_score=\"+str(round(ridge.score(X_train, y_train), 4)))\nprint(\"intercept = \" + str(round(ridge.intercept_, 4)))\n\n# print the coefficients\nfor i in range(0,len(regressor.coef_)):\n    print(data.columns[i]+ \" coefficient = \" + str(round(ridge.coef_[i], 4)))","48b08fcb":"# predictions\ny_pred = ridge.predict(X_test)\n\n# comparison of the predictions to the actual values\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf.head(10)","3dd2443a":"# 3 accuracy scores\nprint('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test, y_pred), 4))  \nprint('Mean Squared Error:', round(metrics.mean_squared_error(y_test, y_pred), 4))\nprint('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)), 4))","b06c1b45":"from sklearn import linear_model\nlasso = linear_model.Lasso(alpha=0.1)\nlasso.fit(X_train, y_train)","0c0ee66a":"print(\"R2_score=\"+str(round(lasso.score(X_train, y_train), 4)))\n# plot the intercept\nprint(\"intercept = \"+ str(round(lasso.intercept_, 4)))\n# plot the slope:\nfor i in range(0,len(regressor.coef_)):\n    print(data.columns[i]+ \" coefficient = \" + str(round(lasso.coef_[i], 4)))","c3df991f":"#predictions\ny_pred = lasso.predict(X_test)\n\n# comparison of the predictions to the actual values\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf.head(10)","60cc6d49":"# 3 accuracy scores\nprint('Mean Absolute Error:', round(metrics.mean_absolute_error(y_test, y_pred), 4))\nprint('Mean Squared Error:', round(metrics.mean_squared_error(y_test, y_pred), 4))\nprint('Root Mean Squared Error:', round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)), 4))","de69fb44":"def GridSearch (input_train, output_train, input_test, output_test, function, alpha):\n    \"\"\"\n    Function taking: - training and test data\n                     - ML model (function)\n                     - alpha values to be tested\n    and plotting the grid search values, the best model and the accuracy of the model on the test  \n    \"\"\"   \n    # we set the alpha parameters we want to try\n    params = {'alpha':alpha, \n            'fit_intercept':[False,True]}\n    \n    # we do the gridseach, it will select the parameter giving the best result\n    GS_models = GridSearchCV(function,\n                            param_grid=params,\n                            scoring='explained_variance',cv=5).fit(input_train, output_train)\n    \n    # we stock the scores\n    scores = GS_models.cv_results_.get('mean_test_score')\n    \n    # we initialize the vectors stocking the values for the plot\n    scores_true=[]\n    scores_false=[]\n    alphas_true=[]\n    alphas_false=[]\n    \n    # we loop all the the hyperparameter combinations\n    for i in range(0,len(scores)) :\n        GS_models.cv_results_['params'][i].get('alpha')\n        \n        # if intercept true we stock the values in the true vectors\n        if GS_models.cv_results_['params'][i].get('fit_intercept'):\n            scores_true.append(scores[i])\n            alphas_true.append(GS_models.cv_results_['params'][i].get('alpha'))\n        else:\n            scores_false.append(scores[i])\n            alphas_false.append(GS_models.cv_results_['params'][i].get('alpha'))\n    # plot         \n    plt.plot(alphas_true, scores_true, 'bx',label='Intercept true')\n    plt.plot(alphas_false, scores_false, 'r*',label='Intercept false')\n    plt.legend()\n    plt.xlabel('alpha')\n    plt.ylabel('R2')\n    plt.show()\n    \n    print(\"The best parameters are the ones maximizing the R2_score\")\n    print(\"Best model description:\")\n    # best parameter\n    print(    GS_models.best_params_)\n    # best score\n    print(\"   R2_score= \" + str(GS_models.best_score_))\n    \n    # 3 accuracy scores\n    print('Accuracy results on the test set')\n    print('   Mean Absolute Error=', round(metrics.mean_absolute_error(y_test, y_pred), 4))\n    print('   Mean Squared Error=',  round(metrics.mean_squared_error(y_test, y_pred), 4))  \n    print('   Root Mean Squared Error=',  round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)), 4))\n    return GS_models","d0eee7eb":"alpha=[0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\nlasso_GS_models=GridSearch (X_train, y_train, X_test, y_test, linear_model.Lasso(), alpha)","bf966785":"# predictions\ny_pred = lasso_GS_models.predict(X_test)\n\n# comparison of the predictions to the actual values\nprint(\"Predictions comparison:\")\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf.head()","8b15493a":"alpha=[0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\nridge_GS_models=GridSearch (X_train, y_train, X_test, y_test, linear_model.Ridge(), alpha)","891b28c6":"# predictions\ny_pred = ridge_GS_models.predict(X_test)\n\n# comparison of the predictions to the actual values\nprint(\"Predictions comparison:\")\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf.head()","c83fa517":"def RandomSearch(input_train, output_train, input_test, output_test, function):\n    \"\"\"\n    Function taking: - training and test data\n                     - ML model (function)\n    and plotting the random search values, the best model and the accuracy of the model on the test  \n    \"\"\"   \n    params_grid= {'alpha': sp_rand() , 'fit_intercept':[False,True]}\n    \n    # create and fit a ridge regression model, testing random alpha values\n    rsearch = RandomizedSearchCV(estimator=function, \n                                   param_distributions=params_grid, \n                                   scoring='explained_variance',\n                                   n_iter=100, cv=5)\n    rsearch.fit(input_train, output_train)\n    \n    # get scores\n    scores=np.array(rsearch.cv_results_.get('mean_test_score'))\n\n    # get parameters\n    params=rsearch.cv_results_.get('params')\n\n    # alpha parameters tried\n    alpha_params=np.array([params[i].get('alpha') for i in range(0,len(params))])\n\n    # intercept parameters tried\n    intercept_params=np.array([params[i].get('fit_intercept') for i in range(0,len(params))])\n\n    # get scores where intercept is True\n    with_int=np.where(intercept_params == True)[0]\n    # get scores where intercept is False\n    no_int=np.where(intercept_params == False)[0]\n\n    # plotting scores depending on alpha parameter where intercept is True\n    plt.plot(alpha_params[with_int], scores[with_int],'x',label='fit intercept: ' + 'True')\n    # plotting scores depending on alpha parameter where intercept is False\n    plt.plot(alpha_params[no_int], scores[no_int],'x',label='fit intercept: ' + 'False')\n\n    # setting legends\n    plt.legend()\n    plt.xlabel('alpha')\n    plt.ylabel('R2')\n    plt.show()\n    \n    # summarize the results of the random parameter search\n    print(\"The best parameters are the ones maximizing the R2_score\")\n    print(\"Best model description\")\n    print(\"   R2_score= \" + str(rsearch.best_score_))\n    print(   rsearch.best_params_)\n    \n    # predictions\n    print(\"Predictions accuracy:\")\n    print('   Mean Absolute Error= ', metrics.mean_absolute_error(output_test, y_pred))  \n    print('   Mean Squared Error= ', metrics.mean_squared_error(output_test, y_pred))  \n    print('   Root Mean Squared Error= ', np.sqrt(metrics.mean_squared_error(output_test, y_pred)))\n    \n    return rsearch","12f80370":"lasso_rsearch=RandomSearch (X_train, y_train, X_test, y_test, linear_model.Lasso())","bfede6c2":"# predictions\ny_pred = lasso_rsearch.predict(X_test)\n\n# comparison of the predictions to the actual values\nprint(\"Predictions comparison:\")\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf.head()","8efbb4d8":"ridge_rsearch=RandomSearch (X_train, y_train, X_test, y_test, linear_model.Ridge())","dc6a416e":"# predictions\ny_pred = ridge_rsearch.predict(X_test)\n\n# comparison of the predictions to the actual values\nprint(\"Predictions comparison:\")\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf.head()","6698539b":"df_sum = pd.DataFrame(columns = ['Model','R2_score','Mean Absolute error','Mean Squared Error'])\ndf_sum.loc[0]=['Linear regression',regressor.score(X_train, y_train),metrics.mean_absolute_error(y_test, regressor.predict(X_test)),metrics.mean_squared_error(y_test, regressor.predict(X_test))]\ndf_sum.loc[1]=['Lasso regression',lasso.score(X_train, y_train),metrics.mean_absolute_error(y_test, lasso.predict(X_test)), metrics.mean_squared_error(y_test, lasso.predict(X_test))]\ndf_sum.loc[2]=['Ridge regression',ridge.score(X_train, y_train),metrics.mean_absolute_error(y_test, ridge.predict(X_test)),metrics.mean_squared_error(y_test, ridge.predict(X_test))]\ndf_sum.loc[3]=['Lasso regression grid search',lasso_GS_models.score(X_train, y_train),metrics.mean_absolute_error(y_test, lasso_GS_models.predict(X_test)),metrics.mean_squared_error(y_test, lasso_GS_models.predict(X_test))]\ndf_sum.loc[4]=['Ridge regression grid search',ridge_GS_models.score(X_train, y_train),metrics.mean_absolute_error(y_test, ridge_GS_models.predict(X_test)),metrics.mean_squared_error(y_test, ridge_GS_models.predict(X_test))]\ndf_sum.loc[5]=['Lasso regression random search',lasso_rsearch.score(X_train, y_train),metrics.mean_absolute_error(y_test, lasso_rsearch.predict(X_test)),metrics.mean_squared_error(y_test, lasso_rsearch.predict(X_test))]\ndf_sum.loc[6]=['Ridge regression random search',ridge_rsearch.score(X_train, y_train),metrics.mean_absolute_error(y_test, ridge_rsearch.predict(X_test)),metrics.mean_squared_error(y_test, ridge_rsearch.predict(X_test))]\ndf_sum.loc[7]=['Baseline model',0,metrics.mean_absolute_error(y_test,np.full(np.shape(y_test), np.mean(y_train))), metrics.mean_squared_error(y_test,np.full(np.shape(y_test), np.mean(y_train)))]\nround(df_sum, 4)","65aaa06d":"## a. Basic linear regression\n\nRegression coefficients are estimated by identifying coefficient values that minimise the mean squared error, which is a summary of the differences between the **predicted** and **actual** values of the output variable (`charges`):\n\n$$MSE=\\frac{1}{n}\\sum_{i=1}^{p}{\\left|y_{i}-\\sum_{j=1}^{p}a_{j}x_{i,j}\\right|^{2}}$$","938ef608":"##### Distribution of feature 5 (region)","edf07ca7":"### ii. Categorical variables to dummy variables \n\nMany machine learning models cannot deal with raw categorical variables directly \u2013 we first need to transform our categorical variables to numerical ones before they can be used as input for the regression model. A common method for doing this is creating binary **dummy variables**:","9a7e5a4c":"So to summarise, our data has 6 input variables and 1 output variable:\n- **3 numerical variables**: `age`, `bmi` and `children`\n- **3 categorical variables**: \n    - `sex` with 2 possible values: `male`, `female`\n    - `smoker` with 2 possible values: `yes`, `no`\n    - `region` with 4 possible values: `northwest`, `northeast`, `southeast`, `southwest`\n- **1 output variable** (last column): `charges`, which is a numerical variable","e162072f":"#### Distribution of feature 4 (smoker)","96eeb696":"### ii. Transforming non-normal variables","f51c4a51":"## b. Ridge linear regression\n\n**Ridge regression** is a modified form of linear regression which puts a restriction on the size of the coefficients. Specifically, we add a penalty term to the loss function, so that the model **favours smaller values for coefficients over larger ones** (all else being equal). Smaller weights tend to lead to \"simpler\" models that are less prone to overfitting. Regularisation techniques in general are techniques for reducing overfitting.\n\nWe modify the normal MSE loss function from above by additing a penalty term on the end of it:\n \n$$MSE + \\lambda\\sum_{j=1}^{p}\\left|a_{j}\\right|^2$$\n\nThis means that instead of the model trying to find the coefficient values that simply minimise MSE, we are now trying to find the coefficient values that minimise the sum of MSE and the sum of the weights squared (scaled by a factor $\\lambda$ that determines how much priority we should give to the weights over the MSE).","0e7db7e8":"## b. Random search\n**Random Search** does the optimization as gridsearch but it selects the hyperparameter values differently. Instead of manually setting the hyperparameters, these are **selected randomly** from a given range of values.\n\nWe build thins function to plot all the important information of the random search.","f48fe37f":"We can examine the distributions of our numerical variables using the following functions:","bbc51b1c":"#### Boxplot of response variable (charges) against feature 5 (region)","05b95679":"# Linear Regression Notebook","673e2b1d":"It can be helpful to transform variables to be \"more normal\/Gaussian\" for ML algorithms with certain assumptions. We can apply a power transformation to our features. \n\nIn code:","4809bac5":"# 3. Model selection and training","7ce71fd3":"### i. Training the model on the training data","f5cb3bfb":"The predictions on the test look as follows:","5a839454":"#### Boxplot of response variable (charges) against feature 4 (smoker)","1f23ed01":"#### Distribution of feature 2 (BMI)","8d47754b":"### iii. Scaling all other numerical features\nSometimes the features in our dataset have different **scales**, for example:\n- `age` values are between 8.4 and 19.89\n- Whereas `children` values are between 0 and 1.31 (much smaller)\n- And `charges` values are between 8.2 and 14.2 \n\nMany machine learning algorithms behave better if the features have similar scales. The most common approaches for tackling this are: \n- **Normalisation** \u2013  this scales a variable to have values between 0 and 1. Normalisation can also refers to making the distribution of a variable be \"more normal\" or \"more Gaussian\".\n- **Standardisation** \u2013 while this scaled a variable to have a mean of zero and a standard deviation of 1. This is done by calculating the mean and standard deviation for a variable. Then, for each observed value of the variable, you subtract the mean and divide by the standard deviation. .\n\nThis is not always necessary but we include it here as best practice. \n\n*In general \u2013 if some of your features have been rescaled\/standardised\/normalised, **all of the other features should be too**.*\n\nWe need to apply standardisation to the numerical variable(s):","5755b39b":"The histogram shows a balanced distribution of `sex` in dataset. ","2de34df8":"The histogram shows a balanced distribution of `region` in the dataset. ","3c74e3c7":"**This notebook runs through some of the steps in a linear regression model using the insurance dataset seen in the presentation as an example.**\n\nThe overall goal of linear regression is to fit a model to predict a **continuous output variable** by estimating the **coefficients** of each input feature. For example, here is a simple model predicting insurance cost (or `charges`) for people from their input features `age`, `bmi`, and `sex`: \n\n\\begin{align}\ncharges = 500 + 2.1*age + 3.5*bmi - 0.3*sex\n\\end{align}\n\nIn linear regression, our goal is to estimate the **coefficients** from the **data** (in this case, the estimated coefficients are 500, 2.1, 3.5 and -0.3). When starting out, the model has no knowledge of the correct coefficient values, but these are estimated from the data.\n\nThe entire process of training a linear regression model can be summarised with this diagram:\n\n![fig](fig.svg)\n\nThe rest of this notebook takes you through this process step-by-step. \n\n**Note**: this notebook can be used with other datasets if your output variable is in the last column.","99176dd7":"Now, the numerical values all have mean 0 and a variance of 1:","f809c15a":"### ii. Model description\n\nWe will again describe the fitted model by examining the **R2 score** and the **coefficient values**. As a reminder:\n\n- The **R2 score**  is the explained variance (the closer to 1 the better)\n- Positive coefficients mean that increasing the leads to higher charges. Negative coefficients mean that increasing the variable leads to lower charges. ","27d7b74f":"### i. Distributions of charges against categorical variable values","7951dead":"There may be complex and unknown relationships between the variables in the dataset. Variables within a dataset can be related for lots of reasons. \n- One variable could cause or depend on the values of another variable.\n- One variable could be lightly associated with another variable.\n- Two variables could depend on a third unknown variable.\n\nA **correlation** can have a:\n- **Positive value**, meaning both variables move in the **same direction**\n- **Negative value**, meaning that when one variable\u2019s value increases, the other variables\u2019 values decrease.\n- **Zero value** meaning that the variables are **unrelated**.\n\nIn order to describe the correlations between variables, we plot the correlation matrix with `data.corr()`","25defdea":"# 2. Data visualization and preparation\nIn this section, we visualize all the variables and do some transformations if necessary. \n\nVariables are sometimes also called **attributes** or **features**. \n\n## a. Numerical variables\nA **numerical** or **continuous** variable is one that may take on any value within a finite or infinite interval (e.g. height, weight, temperature, blood glucose).\n\nThere are two types of numerical variables, **interval** and **ratio**:\n- An **interval variable** has values whose differences are interpretable, but it does not have a true zero. For example, `credit_score`. \n- A **ratio variable** has values that are numerical and have a true zero, for example `age`.","c4092714":"A more graphical view can be displayed so we can more easily identify the high correlation values:","8856c2a0":"The histogram shows an unbalanced distribution of `smoker` in the dataset. ","77c8737a":"This is our final dataset. Now, let's observe the correlations between the variables.","99d36a8f":"The boxplot shows that `charges` is not very influenced by `sex`.","022dbb52":"# 5. Summary\n\n## a. Accuracy of all models\n\nTo compare the performance of all the models created in this notebook, we can gather all of their performance statistics into a single table, and include a baseline model for comparison (the baseline model always predicts the average charge of the training set):","60359218":"#### Lasso regression","2119fd76":"#### Lasso regression","1d245c05":"#### Distribution of the response variable (charges)","0cf6ce7e":"### ii. Model description","a9ec4b6e":"## b. Understanding the importance of features","1467b014":"- The training set is composed of 1070 rows with 9 input variables and 1 output variable\n- The test set is composed of 268 rows and with 9 input variables and 1 output variable","c35b5f57":"Now that we have optimized the final lasso model, we can do the predictions on the test set:","fd11cafe":"3 categorical variables are transformed into 8 numerical variables.\n- **sex**\u2192`sex_female`,`sex_male`\n- **smoker**\u2192`smoker_no`,`smoker_yes`\n- **region**\u2192`region_northeast`,`region_northwest`,`region_southeast`,`region_southwest`","250bdd0f":"We can also show summaries of the **categorical variables** (instead of just the numerical variables) using this command:","687753cf":"#### Ridge regression","8b674b15":"We can examine how the distribution of `charges` changes depending on the values of our categorical variables using this function:","6c59c506":"### iii. Model predictions\nNow that we have the trained ridge linear regression model, we can make predictions on the test set:","0ab2c195":"We see non-normal distributions for the variables `age`, `children` and `charges`.","e35a0924":"For example, here is the \"after transformation\" distribution of the `charges` variable, which has a more normal bell-shaped distribution:","c178525d":"The predictions on the test look as follows:","c0a00904":"We observe an **`R2_score` of 0.68**, which is similar to the R2 scores of the previous 2 models. \n\nYou can see that the coefficients of several features have been set to 0. The coefficients are 0 for features which provide little information in the predictive task, and therefore can be removed from our linear model. \n\nHere, the model removed `bmi`,`sex_female`,`region_southeast`, `region_southwest`,`region_northwest` and `region_northeast`. As a consequence, the model went from having 10 coefficients to only 4. \n\nLooking at the remaining non-zero coefficients, we can interpret their meaning:\n \n - The `smoker_yes` coefficient is the highest coefficient and is equal to **1.09**. From that observation, we can say that smoking leads to an increase of the output variable.\n - The `age` coefficient is the second highest coefficient and is equal to **0.43**. As a consequence, we can say that the older the individual is, the higher the charges will be.\n - The `children` coefficient (**0.02**) is very small and so has little impact on the charges response.","6e2a274e":"### iii. Model predictions\nNow that we have the model, we can make predictions on the test set:","2ac9c1da":"#### Distribution of feature 1 (sex)","453746a0":"The above visualisation is a bit confusing, since it looks like many points are on top of each other \u2013 it is difficult to tell the difference in the distribution of charges depending on the number of children. We can visualise this information more clearly using a boxplot:","1a1a5bbd":"# 0. Library imports","40d80b87":"We can print the **data summary** using the `describe()` command to observe the counts, ranges, means, and quartiles of the numerical values. Using this information, we can also quickly see if we have any defective cells (like missing values):","52cc590c":"**How should we interpret a correlation matrix heatmap? Values close to 1 and -1 mean a high correlation between variables**.\n\n- We observe a **significant correlation** between  `age` and `charges` (**0.52**) meaning that the age of an individual can have a significant effect on the charges value.\n- We observe a **high correlation** between `smoker_yes` and `charges` (**0.67**) meaning that smoking can have a significant impact on the output variable. \n- We observe a **total correlation** in the binary dummy variables (`smoker_yes`\/`smoker_no` (**1.00**) and `sex_female`\/`sex_male`(**1.00**)). We don't need both variables, the information is redundant. We will keep just `smoker_yes` and `sex_female`.\n\nOur final dataset will look as follows:","6f9711c8":"# 1. Data collection\nNext, we load the insurance dataset. Our goal is to predict the annual charges (in the final `charges` column) based on the other variables.\n","738dbc93":"# 4. Hyperparameter tuning","c12a9291":"### ii. Model description\nWe can describe the behaviour of a linear regression model using two sets of values: \n\n- The **R2 score** \u2013 this is a measure of how well the model explains the variance in the data (the closer to 1 the better). Put another way, R2 is a statistical measure of how close the data are to the fitted regression line.\n- The **regression coefficients** \u2013 the values of the coefficients can help us evaluate the influence of each variable on the output variable. \n    - **Negative coefficients** mean that increasing that variable leads to **a decrease in charges**.\n    - While **positive coefficients** mean that increasing that variable leads to **higher charges**.","628fde53":"The bmi distribution is gaussian. We will not transform it.","c2356e3d":"#### Scatter plot of response variable (charges) against feature 2 (BMI)","c78df57e":"## c. Correlation between variables","614a7b3c":"The boxplot shows that `charges` is very influenced by `smoker` status.","7a11fbc6":"#### Scatter plot of response variable (charges) against feature 0 (age)","2d053928":"We can calculate the error metrics on the test set:","f8641696":"### i. Training the model on the training data\n\nWe train our ridge linear regression model in a similar way to before:","01bfcae8":"**The penalization parameter `alpha` is set by default to 0.1**","ea9e1547":"##  c. Split into training and test\nWe split our final dataset into a **training** (80% of the data) and a **test** set (20% of the data):","c8167d60":"The boxplot shows that the `charges` is not very influenced by `region` values.","8deb26f8":"We observe an **R2 score of 0.77** which is relatively close to 1. So, our linear model fits the data fairly well. As a consequence, the coefficients can give us an interpretation of the impact of each feature on the output variable:\n \n - The `smoker_yes` coefficient is the highest coefficient with a value of **1.70**. From this observation, we can infer that smoking leads to an increase in the output variable. We already expected this influence from the correlation matrix.\n - The `age` coefficient is the second highest coefficient with a value of **0.52**. As a consequence, we can infer that the older the individual is, the higher the charges will be. We also expected this influence from the correlation matrix.\n - Other variables responsible for increases in `charges` are: `bmi` (`charges` increases as bmi increases), `children`(`charges` increases with as the number of children increases) and `region_northeast`\n - Other variables responsible for the `charges` decrease are: `region_southeast`\n - Variables with very little influence on `charges` are: `sex_female`,`region_northwest` and `region_southwest`","405be432":"### i. Distribution","e6575645":"Let's visualise all of our numerical variables one at a time:","c03022b3":"#### Ridge regression","b086eb4f":"We can load a linear regression model from the `sklearn` library, a popular machine learning library:","1cffac49":"We observe an **R2 score of 0.77** which is close to 1. So, our linear model fits the data fairly well. As a consequence, we can interpret the coefficients to give us insight into how the variable values impact the output variable. **The interpretations of the features is the same as the normal linear regression.**","daffd2f1":"### i. Learning","d8dfb57d":"We calculate the error metrics as before:","856e6565":"#### Scatter plot of response variable (charges) against feature 3 (children)","411f474b":"### iii. Model predictions\nNow that we have the trained model, we can make predictions on the test set and examine the first 10 predictions:","839e183b":"As we saw from our correlation analysis and later confirmed with the coefficient values, the most important features for predicting the `charges` output variable are: \n - `smoker_yes` is the feature with highest weight in the model with a positive correlation to `charges`. From this observation, we can say that smoking leads to an increase of the output variable.\n - `age` is the feature with the second higher weight in the model with a positive correlation to `charges`. As a consequence, we can say that the older the individual is, the higher the charges will be.\n - `children` has a small positive correlation to `charges`. As a consequence, the more children you have the higher the charges will be, but the effect is smaller compared to the impact of smoking and age. ","e602bbdf":"The best model corresponds to the one maximizing the `R2_score` and minimizing the `MSE` \u2013 here, the winners are the **linear regression** and **ridge regression with grid search** models. \n\nFrom this table, we can also observe that **all models do better than the baseline model** to a similar extent. \n\nThere is one exception for the **lasso regression** model without hyperparameter tuning, which leads to a very simplified model to the detriment of the accuracy.","bc65be6e":"## c. Lasso linear regression\n\n**Lasso regression** is quite similar conceptually to ridge regression. It also adds a penalty to the coefficient values, but unlike ridge regression which penalizes sum of squared coefficients (the so-called L2 penalty), lasso penalizes the sum of their absolute values (L1 penalty). As a result, for **high values of \u03bb**, **many coefficients are exactly zeroed** under lasso, which is usually not the case in ridge regression. The function to be minimized in lasso regression is:\n\n$$MSE + \\lambda\\sum_{j=1}^{p}\\left|a_{j}\\right|$$\n","01a99b96":"First, we run a cell to import all of the necessary libraries we use in the rest of the notebook:","aa611de0":"Now, let's do a first training attempt with 3 different linear regression models:\n\n1. A **basic** linear regression model\n2. A **ridge** linear regression model\n3. A **lasso** linear regression model","99b721ee":"BMI has a normal distribution","af1dedb5":"The predictions on the test look as follows:","f8a5c541":"## b. Categorical variables\n\nA categorical or discrete variable is one that has two or more categories (values).  \n\nThere are two types of categorical variable, nominal and ordinal:\n- A **nominal variable** has no intrinsic ordering to its categories. For example, gender is a categorical variable having two categories (male and female) with no intrinsic ordering to the categories. \n- An **ordinal variable** has a clear ordering. For example, a variable indicating someone's education level, where the more diplomas you have the higher the education level.\n\nIn our case, all of the categorical variables are nominal.","e07d856b":"**Here, we show how tuning is done for two different hyperparameters using both grid search and random search.** The two hyperparameters we will tune are:\n- The **penalization** parameter `alpha`\n- A parameter called `fit_intercept` which controls if there will be an **intercept** for the linear regression model\n\n\nHere we will present two common approaches for the problem:\n\n## a. Gridsearch","918d34fe":"**The penalization parameter `alpha` is set by default to 1.0.**","b8182c3a":"This is what the data looks like:","3655c318":"#### Distribution of feature 0 (age)","02e7b500":"#### Distribution of feature 3 (children)","5d5b53a9":"In machine learning, hyperparameter tuning is the problem of **choosing a set of optimal hyperparameters** for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (here the regression coefficients) are learned.","4712f5c7":"####  Boxplot of response variable (charges) against feature 1 (sex)","90223362":"Finally, we can prepare our final, formatted dataset:","53e43076":"The age distribution is not gaussian. We will transform it later.","53f74a9e":"This is an exhaustive searching technique which tries all combinations of hyperparameter values that are **manually specified**. A grid search algorithm must be guided by some performance metric. Here, we will use the `R2_score`. It launches model training for every combination of hyperparameters and will select the combination leading to the best score.\n\nWe build thins function to plot all the important information of the grid search.","a3001ab9":"We can describe this trained model using the R2 score and coefficients:","4c1bdaaf":"We calculate the error metrics as before:"}}