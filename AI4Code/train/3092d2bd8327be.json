{"cell_type":{"830da9b0":"code","ca6906e8":"code","82ec21cc":"code","41ce4bc6":"code","9aeada91":"code","72c616d7":"code","5b2d06b7":"code","936e3f54":"code","a22d554e":"code","28026813":"code","7830017f":"code","5a4665dc":"code","a973092b":"markdown","46f5a3f5":"markdown","3bb36366":"markdown","0af3fbc6":"markdown","d9e84438":"markdown","89174802":"markdown","1a129d3a":"markdown","64e62836":"markdown","02866578":"markdown"},"source":{"830da9b0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\n# Load data\ndf = pd.read_csv ('..\/input\/dblp-2020-05-01-pubs-per-year\/pubs.csv')\n\n# Draw plot\nplt.style.use('dark_background')\nplot=df.plot(kind = 'bar', x = 'Year', y = 'Pubs', color = 'orange', figsize = (16,9))\n\n# Format plot\nplt.title('Records in DBLP', fontweight ='bold', color = 'orange')\nplt.xlabel('Year', labelpad = 20, fontweight ='bold', color = 'orange')\nplt.ylabel('Publications', labelpad = 20, fontweight ='bold', color = 'orange',)\n\n# Draw horizontal axis lines\naxes = plt.gca()\naxes.get_legend().remove()\naxes.yaxis.grid(color = 'grey', linestyle = 'dashed')\n\n# Format x axis ticks\nl=np.array(df['Year'])\nplt.xticks(range(0, len(l), 5), l[::5], rotation = 0, fontweight ='bold')\n\n# Format y axis ticks\nplot.yaxis.set_major_formatter(ticker.EngFormatter())\nplt.yticks(fontweight ='bold')\n\nplt.show()\n\n","ca6906e8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Load data\ndata = pd.read_csv('..\/input\/dblp-2020-05-01-pubs-per-year\/pubs_cut.csv',parse_dates=[0])\ncon=data['Year']\ndata['Year']=pd.to_datetime(data['Year'])\ndata.set_index('Year', inplace=True)\n\n# cCheck datatype of index\ndata.index\n\n# Convert to time series:\nts = data['Pubs']\n\n# Log transform time series\nts_log=np.log(ts)\n\n# Create plots\ndecomposition=seasonal_decompose(ts_log)\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\nplt.figure(figsize = (16,9))\nplt.subplot(411)\nplt.plot(ts_log, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()\n\n# Perform Dickey-Fuller test:\nprint(\n    'Results of Dickey-Fuller Test:')\ndftest = adfuller(ts_log, autolag='AIC')\ndfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\nfor key, value in dftest[4].items():\n    dfoutput['Critical Value (%s)' % key] = value\nprint(dfoutput)","82ec21cc":"import pandas as pd\nfrom matplotlib import pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n\n# Load data\ndf = pd.read_csv ('..\/input\/dblp-2020-05-01-pubs-per-year\/pubs_cut.csv') \n\n# Set Year as index\ndf = df.set_index(\"Year\")\n\n# Create acf and pacf plots\nplt.style.use('fivethirtyeight')\nplt.figure(figsize = (16,9))\nax1=plt.subplot(211)\nplot_acf(df, ax=plt.gca(), lags=20)\nplt.title('Autocorrelation', color='black')\nax1.tick_params(axis='y', colors='black')\nax1.tick_params(axis='x', colors='black')\nax2=plt.subplot(212)\nplot_pacf(df, ax=plt.gca(), lags=20)\nax2.tick_params(axis='y', colors='black')\nax2.tick_params(axis='x', colors='black')\nplt.title('Partial Autocorrelation', color='black')\nplt.axis(color='black')\nplt.show()\n","41ce4bc6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nimport statistics\nfrom statsmodels.tools.eval_measures import rmse\nfrom statsmodels.tsa.arima_model import ARIMA\n\nwarnings.filterwarnings(\"ignore\")\n\n# Insert data without years 1918 & 2020\ndf = pd.read_csv ('..\/input\/dblp-2020-05-01-pubs-per-year\/pubs_cut.csv', parse_dates=[0])\n\n# Format 'Year' column as year type\ndf.Year = pd.to_datetime(df.Year).dt.year\n\n# Build train and test set\ntrain_data = df[:len(df)-12]\ntrain_data = train_data.astype('float64')\ntest_data = df[len(df)-12:]\ntest_data = test_data.astype('float64')\n\n# Build ARIMA model (2,1,0)\nmodel = ARIMA(train_data['Pubs'], order=(2,1,0))\nmodel_fit = model.fit(disp=0)\narima_pred = model_fit.predict(start = len(train_data), end = len(df)-1, typ=\"levels\").rename(\"ARIMA Forecast\")\nprint(model_fit.summary())\n","9aeada91":"# Build plot\nplt.style.use('fivethirtyeight')\ntrain_data['Pubs'].plot(figsize = (16,9), legend=True, marker='o')\ntest_data['Pubs'].plot(figsize = (16,9), legend=True, marker='o')\nplt.title(\"Time Series Forecasting with ARIMA\", fontweight ='bold', color='black')\nplt.xlabel('Year', fontweight ='bold', color='black')\nplt.ylabel('Publications', fontweight ='bold', color='black')\n\n# Format x axis ticks\nplt.xlabel('Year')\nl=np.array(df['Year'])\nplt.xticks(range(0, len(l), 5), l[::5], rotation = 0, color='black')\nplt.yticks(color='black')\n\n# Add markers to predictions line\narima_pred.plot(legend = True, marker='o');\n\n# Rename legend labels\nL=plt.legend()\nL.get_texts()[0].set_text('Actual publications')\nL.get_texts()[1].set_text('Train Data')\nL.get_texts()[2].set_text('Predicted Data')\nplt.setp(L.get_texts(), color = 'black')\n\n# Calculate test_data mean\nx= statistics.mean(test_data['Pubs'])\nprint(\"Mean is:\", x)\n\n# Calculate RMSE\narima_rmse_error = rmse(test_data['Pubs'], arima_pred)\nprint(f'RMSE = {arima_rmse_error}')\n\nplt.show()","72c616d7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statistics\nfrom statsmodels.tools.eval_measures import rmse\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# Insert data without years 1918 & 2020\ndf = pd.read_csv ('..\/input\/dblp-2020-05-01-pubs-per-year\/pubs_cut.csv', parse_dates=[0])\n\n# Format 'Year' column as year type\ndf.Year = pd.to_datetime(df.Year).dt.year\n\n# Build train and test set\ntrain_data = df[:len(df)-12]\ntrain_data = train_data.astype('float64')\ntest_data = df[len(df)-12:]\ntest_data = test_data.astype('float64')\n\ndef arima(p,d,q):\n    try:\n        try:\n            model = ARIMA(train_data['Pubs'], order=(p,d,q))\n            model_fit = model.fit(disp=0)\n            arima_pred = model_fit.predict(start=len(train_data), end=len(df) - 1, typ=\"levels\").rename(\"ARIMA Forecast\")\n            arima_rmse_error = sqrt(mean_squared_error(test_data['Pubs'], arima_pred))\n        except np.linalg.LinAlgError as e:\n            arima_rmse_error=100000\n    except ValueError as e:\n        arima_rmse_error=100000\n    return(arima_rmse_error)\n\nfor p in range (0,4):\n    for d in range (0,3):\n        for q in range  (0,7):\n            arima_rmse_error=arima(p,d,q)\n            if q == 0 and d == 0 and p == 0:\n                rmse = arima_rmse_error\n            if rmse > arima_rmse_error:\n                rmse = arima_rmse_error\n                P,D,Q=p,d,q\n\nmodel = ARIMA(train_data['Pubs'], order=(P, D, Q))\nmodel_fit = model.fit(disp=0)\narima_pred = model_fit.predict(start=len(train_data), end=len(df) - 1, typ=\"levels\").rename(\"ARIMA Forecast\")\narima_rmse_error = sqrt(mean_squared_error(test_data['Pubs'], arima_pred))\nprint(model_fit.summary())\n\n# Calculate test_data mean\nx= statistics.mean(test_data['Pubs'])\nprint(\"Mean is :\", x)\nplt.show()\n\n# Show RMSE\nprint(f'RMSE = {arima_rmse_error}')\n\ntest_data[\"ARIMA\"] = arima_pred ","5b2d06b7":"# Build plot\nplt.style.use('fivethirtyeight')\ntrain_data['Pubs'].plot(figsize = (16,9), legend=True, marker='o')\ntest_data['Pubs'].plot(figsize = (16,9), legend=True, marker='o')\nplt.title(\"Time Series Forecasting with ARIMA\", fontweight ='bold', color='black')\nplt.xlabel('Year', fontweight ='bold', color='black')\nplt.ylabel('Publications', fontweight ='bold', color='black')\n\n# Format x axis ticks\nplt.xlabel('Year')\nl=np.array(df['Year'])\nplt.xticks(range(0, len(l), 5), l[::5], rotation = 0, color='black')\nplt.yticks(color='black')\n\n# Add markers to predictions line\narima_pred.plot(legend = True, marker='o');\n\n# Rename legend labels\nL=plt.legend()\nL.get_texts()[0].set_text('Actual publications')\nL.get_texts()[1].set_text('Train Data')\nL.get_texts()[2].set_text('Predicted Data')\nplt.setp(L.get_texts(), color = 'black')\n\nplt.show()","936e3f54":"from pandas import DataFrame\nfrom pandas import Series\nfrom pandas import concat\nfrom pandas import read_csv\nfrom pandas import datetime\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom math import sqrt\nfrom matplotlib import pyplot\nimport numpy\nimport pandas as pd\n\n# datetime parsing function for loading the dataset\n\ndef parser(x):\n   return datetime.strptime(x, '%Y')\n\n# frame a sequence as a supervised learning problem\n\ndef timeseries_to_supervised(data, lag=1):\n   df = DataFrame(data)\n   columns = [df.shift(i) for i in range(1, lag+1)]\n   columns.append(df)\n   df = concat(columns, axis=1)\n   df.fillna(0, inplace=True)\n   return df\n\n# create a differenced series\n\ndef difference(dataset, interval=1):\n   diff = list()\n   for i in range(interval, len(dataset)):\n      value = dataset[i] - dataset[i - interval]\n      diff.append(value)\n   return Series(diff)\n\n# invert differenced value\n\ndef inverse_difference(history, yhat, interval=1):\n   return yhat + history[-interval]\n\n# scale train and test data to [-1, 1]\n\ndef scale(train, test):\n   # fit scaler\n   scaler = MinMaxScaler(feature_range=(-1, 1))\n   scaler = scaler.fit(train)\n   # transform train\n   train = train.reshape(train.shape[0], train.shape[1])\n   train_scaled = scaler.transform(train)\n   # transform test\n   test = test.reshape(test.shape[0], test.shape[1])\n   test_scaled = scaler.transform(test)\n   return scaler, train_scaled, test_scaled\n\n# inverse scaling for a forecasted value\n\ndef invert_scale(scaler, X, value):\n   new_row = [x for x in X] + [value]\n   array = numpy.array(new_row)\n   array = array.reshape(1, len(array))\n   inverted = scaler.inverse_transform(array)\n   return inverted[0, -1]\n\n# fit an LSTM network to training data\n\ndef fit_lstm(train, batch_size, nb_epoch, neurons):\n   X, y = train[:, 0:-1], train[:, -1]\n   X = X.reshape(X.shape[0], 1, X.shape[1])\n   model = Sequential()\n   model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n   model.add(Dense(1))\n   model.compile(loss='mean_squared_error', optimizer='adam')\n   for i in range(nb_epoch):\n      model.fit(X, y, epochs=10, batch_size=batch_size, verbose=1, shuffle=False)\n      model.reset_states()\n   return model\n\n# make a one-step forecast\n\ndef forecast_lstm(model, batch_size, X):\n   X = X.reshape(1, 1, len(X))\n   yhat = model.predict(X, batch_size=batch_size)\n   return yhat[0,0]\n\n# load dataset\n\nseries = read_csv('..\/input\/dblp-2020-05-01-pubs-per-year\/pubs_cut.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n\n# transform data to be stationary\n\nraw_values = series.values\ndiff_values = difference(raw_values, 1)\n\n# transform data to be supervised learning\n\nsupervised = timeseries_to_supervised(diff_values, 1)\nsupervised_values = supervised.values\n\n# split data into train and test-sets\n\ntrain, test = supervised_values[0:-12], supervised_values[-12:]\n\n# transform the scale of the data\n\nscaler, train_scaled, test_scaled = scale(train, test)\n\n# fit the model\n\nlstm_model = fit_lstm(train_scaled, 1, 3, 4)\n\n# forecast the entire training dataset to build up state for forecasting\n\ntrain_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\nlstm_model.predict(train_reshaped, batch_size=1)\n\n# walk-forward validation on the test data\n\npredictions = list()\nfor i in range(len(test_scaled)):\n   # make one-step forecast\n   X, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n   yhat = forecast_lstm(lstm_model, 1, X)\n   # invert scaling\n   yhat = invert_scale(scaler, X, yhat)\n   # invert differencing\n   yhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n   # store forecast\n   predictions.append(yhat)\n   expected = raw_values[len(train) + i + 1]\n   print('Year=%d, Predicted=%f, Expected=%f' % (i+2008, yhat, expected))\n\n# report performance\n\nrmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\nprint('Test RMSE: %.3f' % rmse)\n\n# line plot of observed vs predicted\n\nb = numpy.arange(1936, 2020, 1)\ndf1 = pd.DataFrame(b)\ndf2 = pd.DataFrame(raw_values)\ndf3 = pd.DataFrame(predictions)\nfor i in range (0,72):\n   df3.loc[len(df3)] = numpy.nan\n   df3 = df3.shift()\n   df3.loc[0] = numpy.nan\nprint(df3)\npdlist=[df1,df2,df3]\ndf1.reset_index(drop=True, inplace=True)\ndf2.reset_index(drop=True, inplace=True)\ndf3.reset_index(drop=True, inplace=True)\ndf = pd.concat(pdlist, axis=1)\ndf.columns=['Year','Actual Values','Predicted Values']\nprint(df)\n\ntest_data['LSTM'] = lstm_predictions","a22d554e":"# Draw plot\n\npyplot.style.use('fivethirtyeight')\nplt.figure(figsize = (16,9))\npyplot.plot(df['Actual Values'], marker='o', label=\"Actual\")\npyplot.plot(df['Predicted Values'], marker='o', label=\"LSTM Forecat\")\npyplot.title(\"Time Series Forecasting with LSTM\", fontweight ='bold', color = 'black')\npyplot.xlabel('Year', fontweight ='bold', color = 'black')\npyplot.ylabel('Publications', fontweight ='bold', color = 'black')\na=numpy.array(df['Year'])\npyplot.xticks(range(0, len(a), 5), a[::5], rotation = 0, color = 'black')\npyplot.yticks(color = 'black')\nL=plt.legend()\nL.get_texts()[0]\nL.get_texts()[1]\nplt.setp(L.get_texts(), color = 'black')\n\npyplot.show()","28026813":"# Draw comparison plot\n                  \npyplot.style.use('fivethirtyeight')\nplt.figure(figsize = (16,9))\nplt.plot(test_data['Year'], test_data[\"Pubs\"], label=\"Actual\",)\nplt.plot(test_data['Year'], test_data[\"ARIMA\"], linestyle=\"--\", label=\"ARIMA\")\nplt.plot(test_data['Year'], test_data[\"LSTM\"], linestyle=\"--\", label=\"LSTM\", color=\"purple\")\npyplot.title(\"ARIMA vs LSTM\", fontweight ='bold', color = 'black')\npyplot.xlabel('Year', fontweight ='bold', color = 'black')\npyplot.ylabel('Publications', fontweight ='bold', color = 'black')\npyplot.xticks(color = 'black')\npyplot.yticks(color = 'black')\nL=plt.legend()\nL.get_texts()[0]\nL.get_texts()[1]\nL.get_texts()[2]\nplt.setp(L.get_texts(), color = 'black')\n\nplt.show()","7830017f":"from pandas import DataFrame\nfrom pandas import Series\nfrom pandas import concat\nfrom pandas import read_csv\nfrom pandas import datetime\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom math import sqrt\nfrom matplotlib import pyplot\nimport numpy\nimport pandas as pd\n\n# date-time parsing function for loading the dataset\ndef parser(x):\n\treturn datetime.strptime(x, '%Y')\n\n# frame a sequence as a supervised learning problem\ndef timeseries_to_supervised(data, lag=1):\n\tdf = DataFrame(data)\n\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n\tcolumns.append(df)\n\tdf = concat(columns, axis=1)\n\tdf.fillna(0, inplace=True)\n\treturn df\n\n# create a differenced series\ndef difference(dataset, interval=1):\n\tdiff = list()\n\tfor i in range(interval, len(dataset)):\n\t\tvalue = dataset[i] - dataset[i - interval]\n\t\tdiff.append(value)\n\treturn Series(diff)\n\n# invert differenced value\ndef inverse_difference(history, yhat, interval=1):\n\treturn yhat + history[-interval]\n\n# scale train and test data to [-1, 1]\ndef scale(train, test):\n\t# fit scaler\n\tscaler = MinMaxScaler(feature_range=(-1, 1))\n\tscaler = scaler.fit(train)\n\t# transform train\n\ttrain = train.reshape(train.shape[0], train.shape[1])\n\ttrain_scaled = scaler.transform(train)\n\t# transform test\n\ttest = test.reshape(test.shape[0], test.shape[1])\n\ttest_scaled = scaler.transform(test)\n\treturn scaler, train_scaled, test_scaled\n\n# inverse scaling for a forecasted value\ndef invert_scale(scaler, X, value):\n\tnew_row = [x for x in X] + [value]\n\tarray = numpy.array(new_row)\n\tarray = array.reshape(1, len(array))\n\tinverted = scaler.inverse_transform(array)\n\treturn inverted[0, -1]\n\n# fit an LSTM network to training data\ndef fit_lstm(train, batch_size, nb_epoch, neurons):\n\tX, y = train[:, 0:-1], train[:, -1]\n\tX = X.reshape(X.shape[0], 1, X.shape[1])\n\tmodel = Sequential()\n\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n\tmodel.add(Dense(1))\n\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n\tfor i in range(nb_epoch):\n\t\tmodel.fit(X, y, epochs=10, batch_size=batch_size, verbose=1, shuffle=False)\n\t\tmodel.reset_states()\n\treturn model\n\n# make a one-step forecast\ndef forecast_lstm(model, batch_size, X):\n\tX = X.reshape(1, 1, len(X))\n\tyhat = model.predict(X, batch_size=batch_size)\n\treturn yhat[0,0]\n\n# load dataset\nseries = read_csv('..\/input\/dblp-2020-05-01-pubs-per-year\/pubs.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)[1:]\n\n# transform data to be stationary\nraw_values = series.values\ndiff_values = difference(raw_values, 1)\n\n# transform data to be supervised learning\nsupervised = timeseries_to_supervised(diff_values, 1)\nsupervised_values = supervised.values\n\n# split data into train and test-sets\ntrain, test = supervised_values[0:-13], supervised_values[-13:]\n\n# transform the scale of the data\nscaler, train_scaled, test_scaled = scale(train, test)\n\n# fit the model\nlstm_model = fit_lstm(train_scaled, 1, 3, 4)\n# forecast the entire training dataset to build up state for forecasting\ntrain_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\nlstm_model.predict(train_reshaped, batch_size=1)\n\n# walk-forward validation on the test data\npredictions = list()\nfor i in range(len(test_scaled)):\n\t# make one-step forecast\n\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n\tyhat = forecast_lstm(lstm_model, 1, X)\n\t# invert scaling\n\tyhat = invert_scale(scaler, X, yhat)\n\t# invert differencing\n\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n\t# store forecast\n\tpredictions.append(yhat)\n\texpected = raw_values[len(train) + i + 1]\n\tprint('Year=%d, Predicted=%f, Expected=%f' % (i+2008, yhat, expected))\n\n# report performance\nrmse = sqrt(mean_squared_error(raw_values[-13:-1], predictions[-13:-1]))\nprint('Test RMSE: %.3f' % rmse)\n# line plot of observed vs predicted\n\nb = numpy.arange(1936, 2021, 1)\ndf1 = pd.DataFrame(b)\ndf2 = pd.DataFrame(raw_values)\ndf3 = pd.DataFrame(predictions)\nfor i in range (0,72):\n\tdf3.loc[len(df3)] = numpy.nan\n\tdf3 = df3.shift()\n\tdf3.loc[0] = numpy.nan\npdlist=[df1,df2,df3]\ndf1.reset_index(drop=True, inplace=True)\ndf2.reset_index(drop=True, inplace=True)\ndf3.reset_index(drop=True, inplace=True)\ndf = pd.concat(pdlist, axis=1)\ndf.columns=['Year','Actual Values','Predicted Values']\nprint(df)","5a4665dc":"# Draw plot\n\npyplot.style.use('fivethirtyeight')\nax=df.plot.bar(x='Year', y='Actual Values', legend=True, figsize = (16,9))\ndf[:-12].plot(y='Actual Values', linestyle='-',ax=ax,color='red',  legend=True)\ndf.plot(y='Predicted Values', linestyle='-',ax=ax,color='green',  legend=True)\nplt.title('Prediction For 2020 Publications',fontweight ='bold', color = 'black')\nplt.xlabel('Year', fontweight ='bold', color='black')\nplt.ylabel('Publications', fontweight ='bold', color='black')\na = numpy.arange(1936, 2022, 1)\nplt.xticks(range(0, len(a), 5), a[::5], rotation = 30, color='black')\nplt.yticks(color='black')\nL=pyplot.legend()\nL.get_texts()[2].set_text('Actual publications')\nL.get_texts()[0].set_text('Train Data')\nL.get_texts()[1].set_text('Predicted Data')\nplt.setp(L.get_texts(), color = 'black')\n\npyplot.show()","a973092b":"## ARIMA (2,1,0)\n\nAfter interpreting the acf and pacf plots we draw to the conclusion that the best model is the Autoregressive model \u2013 AR(2). More information about this interpretation can be found [here](https:\/\/www.linkedin.com\/pulse\/reading-acf-pacf-plots-missing-manual-cheatsheet-saqib-ali\/). Additionally, since our time series is non-stationary we have to use at least one difference to make it stationary and that's why we set **d=1**. So for our first test we will use **ARIMA(2,1,0)**.","46f5a3f5":"## Comparison\n\nComparing the two forecasting models created with **ARIMA (RMSE = 9483.94)** and LSTM **(RMSE = 7667.14)** we draw to the conclusion that the best fit (the one with the smallest RMSE) for our time series is the LSTM, despite we have noticed it incorporates an element of randomness. Nonetheless, it is the most accurate of the two models. \n\n\n","3bb36366":"## ARIMA (1,1,5)","0af3fbc6":"For the purpose of forecasting we will remove the years 1918 and 2020 (for year 2020 there are available data only up until May). For both ARIMA and LSTM we will use as train data the information from years from 1936 until 2007 and test data the years from 2008 until 2019. Before moving on to build our model we have to check our time series for **stationarity**, **seasonality** and **trend**.","d9e84438":"From the above diagrams we see that there is no seasonality in our data and there is a strong uptrend. Moreover, by using the Dickey-Fuller test we get a p-value of 0,579 which  is not statistically significant (p-value>a=0,05) and indicates strong evidence for the null hypothesis. That means we can not reject the null hypothesis of the test (the data has a unit root and is non-stationary). [Here](https:\/\/pythondata.com\/stationary-data-tests-for-time-series-forecasting\/) is a helpful article.  \n\nSince our time series proved to be non-stationary we can use an ARIMA(p,d,q) model to make it stationary by differencing. ARIMA stands for Autoregressive(**AR**) Integrated(**I**) Moving \u0391verage(**MA**) model and it consists of three parameters p, d and q:\n\n* **p** - The number of Autoregressive terms\n* **d** - The number of differences needed for stationarity\n* **q** - The number of Moving \u0391verage terms\n\nAnother helpful article, written by Caner Dabakoglu, is this [one](https:\/\/medium.com\/@cdabakoglu\/time-series-forecasting-arima-lstm-prophet-with-python-e73a750a9887).\n\nIn order to select the values of p and q we will use the autocorrelation and partial autocorrelation graphs.","89174802":"## LSTM","1a129d3a":"## Forecast for 2020\n\nFinally, we decided to make a forecast for 2020. The model predicted that during 2020 there will be 397264 publications.","64e62836":"In order to measure the accuracy of our forecast we will use the Root Mean Square Error \u2013 RMSE. For the **ARIMA(2,1,0)** model we have an **RMSE of 10556.69**, meaning the model was wrong by about 10.5k publications per year for each prediction made, while the mean publications per year are 279182.75  \n\nAnother approach to find the best ARIMA model is to run multiple combinations of p,d and q values and find the best fit based on which model has the lowest RMSE. By doing that we find out the **ARIMA(1,1,5)** was the most accurate model with **RMSE = 9483.94** ","02866578":"# Time series forecasting with ARIMA and LSTM\n\nThe main purpose of this kernel, as being stated at the title, will be the analysis and forecasting of a time series using two different techniques: ARIMA and LSTM. The dataset we will use is the [DBLP - Publications per Year](https:\/\/www.kaggle.com\/stefanosnikolaou\/dblp-2020-05-01-pubs-per-year) dataset. So now, let's take a first look of our data: \n\n"}}