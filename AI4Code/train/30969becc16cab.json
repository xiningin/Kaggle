{"cell_type":{"7ab04fd1":"code","5fd00fe9":"code","669e1183":"code","ea799cd8":"code","d891a48a":"code","4a049556":"code","93e6e9c3":"code","131e8426":"code","f810c331":"code","7056c43c":"code","be81c0cd":"code","6f232798":"code","045afe2d":"code","482bf820":"code","ca4b58aa":"code","0b4af71b":"code","7601e868":"code","c79b5c79":"code","b160b8e1":"code","c145e774":"code","24bd6216":"code","a0f07fa4":"code","f0ce440f":"code","b9f9b7b1":"code","89745f7a":"code","2a3dea5c":"code","d88a4d94":"code","7309a40a":"code","51c6a45a":"code","a8cde761":"code","2177e5a0":"code","7e01256d":"code","599e2dc4":"code","f3f38772":"code","9c2f1c15":"code","5ee44e0a":"code","891fa2d4":"code","06d73f6d":"code","c6dccfdc":"code","25159b70":"code","9394f8df":"code","2370eced":"code","2b824186":"code","f974c1c3":"code","ba38403e":"code","3f31c88e":"code","34694464":"code","2bfa9bc7":"code","39f94099":"code","e686546f":"code","549bb757":"code","3d2a822b":"code","ca7dcef2":"code","6f539b73":"code","7c5380ac":"code","dbaaf5d9":"code","7282ad9d":"code","fe76efcd":"code","cc9788b2":"code","016cc581":"code","031ffcee":"code","a69689ae":"code","155d364e":"code","700670da":"code","f1646630":"code","282d5b0d":"code","f7e95b2c":"code","dfc39ea5":"code","a1bbf260":"code","e15f5f8b":"code","84367494":"code","6f3a34b2":"code","a13b47e7":"code","1987ad88":"code","66f05599":"markdown","0836b420":"markdown","cdd13d76":"markdown","e3bff2c3":"markdown","599c343b":"markdown","73023bdf":"markdown","a048177b":"markdown","9291b9e1":"markdown","ee5e5259":"markdown","85451430":"markdown","9bb3a971":"markdown","5da70685":"markdown","0a76f117":"markdown","8be87b8b":"markdown","dfa07e7f":"markdown","e0cf8d3b":"markdown","7688282c":"markdown","f3474795":"markdown","c0946a55":"markdown","5f1dd219":"markdown","d3a0da3c":"markdown","9e80a661":"markdown"},"source":{"7ab04fd1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder # just to raise an ImportError if Scikit-Learn < 0.20\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix , classification_report\nimport xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5fd00fe9":"train_data = pd.read_csv(\"..\/input\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/test.csv\")","669e1183":"train_data.head()","ea799cd8":"train_data.info()","d891a48a":"train_data.describe()","4a049556":"#Let's check that the target is indeed 0 or 1:\n\ntrain_data[\"Survived\"].value_counts()","93e6e9c3":"# Now let's take a quik look at all the catagorical values\n\ntrain_data[\"Pclass\"].value_counts()","131e8426":"train_data[\"Sex\"].value_counts()","f810c331":"train_data[\"Embarked\"].value_counts()","7056c43c":"# A class to select numerical or categorical columns \n# since Scikit-Learn doesn't handle DataFrames yet\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]\n    ","be81c0cd":"#removing mentioned columns from dataset\ntrain_data = train_data.drop(['Name','Ticket','Cabin','SibSp','Parch','PassengerId'],axis=1)\ntest_data = test_data.drop(['Name','Ticket','Cabin','SibSp','Parch'],axis=1)","6f232798":"# Let's build the pipeline for the numerical attributes:\n\n# num_pipeline = Pipeline([\n#     (\"select_numeric\", DataFrameSelector([\"Age\",\"Fare\"])),\n#     (\"imputer\", SimpleImputer(strategy=\"median\"))\n# ])","045afe2d":"# num_pipeline.fit_transform(train_data)","482bf820":"class MostFrequentImputer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],\n                                        index=X.columns)\n        return self\n    def transform(self, X, y=None):\n        return X.fillna(self.most_frequent_)","ca4b58aa":"# cat_pipeline = Pipeline([\n#     (\"select_cat\", DataFrameSelector([\"Pclass\", \"Sex\", \"Embarked\"])),\n#     (\"imputer\", MostFrequentImputer()),\n#     (\"cat_encoder\", OneHotEncoder(sparse=False))\n# ])","0b4af71b":"# cat_pipeline.fit_transform(train_data)","7601e868":"# Finally, let's join the numerical and categorical pipelines:\n\n# preprocess_pipeline = FeatureUnion(transformer_list=[\n#     (\"num_pipeline\", num_pipeline),\n#     (\"cat_pipeline\", cat_pipeline)\n# ])","c79b5c79":"#checking for any null values\ntrain_data.isnull().any() # True means null value present","b160b8e1":"test_data.isnull().any()","c145e774":"# age columns\nprint('mean age in train data :',train_data['Age'].mean())\nprint('mean age in test data :',test_data['Age'].mean())","24bd6216":"combined_data=[train_data,test_data]\n#replacing null values with 30 in age column\nfor df in combined_data:\n    df['Age'] = df['Age'].replace(np.nan,30).astype(int)","a0f07fa4":"train_data['Embarked'].value_counts()","f0ce440f":"#most people embarked from 'S'. So, we'll replace the missing missing Embarked value by 'S'.\ntrain_data['Embarked'] = train_data['Embarked'].replace(np.nan,'S')","b9f9b7b1":"#finding mean fare in test data\ntest_data['Fare'].mean()","89745f7a":"#replace missing fare values in test data by mean\ntest_data['Fare'] = test_data['Fare'].replace(np.nan,36).astype(int)","2a3dea5c":"combined_data=[train_data,test_data]\nfor df in combined_data:\n    print(df.isnull().any()) #bool value = False means that there are no nulls in the column.","d88a4d94":"train_data.head()","7309a40a":"train_onehot = train_data.copy()","51c6a45a":"train_onehot = pd.get_dummies(train_onehot, columns=['Embarked', 'Sex'], \n                              prefix=['Embarked', 'Sex'])","a8cde761":"train_onehot = train_onehot.astype(int)\ntrain_onehot.head()","2177e5a0":"test_data.head()","7e01256d":"test_onehot = test_data.copy()","599e2dc4":"test_onehot = pd.get_dummies(test_onehot, columns=['Embarked', 'Sex'], \n                              prefix=['Embarked', 'Sex'])","f3f38772":"test_onehot = test_onehot.astype(int)\ntrain_onehot.head()","9c2f1c15":"print(len(train_onehot.columns))\nprint(len(train_onehot))","5ee44e0a":"print(len(test_onehot.columns))\nprint(len(test_onehot))","891fa2d4":"X_train, X_test, y_train, y_test = train_test_split(train_onehot.drop(['Survived'],axis=1), train_onehot['Survived'], test_size=0.2, random_state=42)","06d73f6d":"# X_train = train_onehot.drop(['Survived'], axis = 1)\n# X_train.head()","c6dccfdc":"# Let's not forget to get the labels:\n\n# y_train = train_onehot[\"Survived\"]","25159b70":"# X_test = test_onehot.drop(['PassengerId'], axis=1).copy()","9394f8df":"xgb_model = xgb.XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.01,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      n_estimators=1000, \n                      reg_alpha = 0.3,\n                      max_depth=4, \n                      gamma=10,\n                      eval_metric = 'auc')","2370eced":"xgb_model.fit(X_train,y_train)","2b824186":"preds_xgb = xgb_model.predict(X_test)","f974c1c3":"print(confusion_matrix(y_test,preds_xgb))\nprint(classification_report(y_test,preds_xgb))","ba38403e":"params = {\n \"learning_rate\"    : list(np.arange(0.05,0.6,0.05)) ,\n \"max_depth\"        : list(np.arange(1,20,2)),\n \"min_child_weight\" : list(np.arange(1,9,1)),\n \"gamma\"            : list(np.arange(0.05,0.7,0.05)),\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]}","3f31c88e":"grid = RandomizedSearchCV(xgb.XGBClassifier(objective='binary:logistic'),\n                         param_distributions=params,\n                         scoring='accuracy',cv=5,verbose=5)","34694464":"grid.fit(X_train,y_train)","2bfa9bc7":"grid.best_params_, grid.best_score_","39f94099":"best_matrix = {'min_child_weight': 8,\n  'max_depth': 13,\n  'learning_rate': 0.5,\n  'gamma': 0.15000000000000002,\n  'colsample_bytree': 0.7}","e686546f":"pre_finalized_model = xgb.XGBClassifier(params=best_matrix, objective='binary:logistic')","549bb757":"pre_finalized_model.fit(X_train,y_train)","3d2a822b":"pre_final_preds = pre_finalized_model.predict(X_test)","ca7dcef2":"print(confusion_matrix(y_test,pre_final_preds))\nprint(classification_report(y_test,pre_final_preds))","6f539b73":"X = train_onehot.drop(['Survived'],axis=1)\ny = train_onehot['Survived']","7c5380ac":"model_to_submit_from = xgb.XGBClassifier(params=best_matrix, objective='binary:logistic')","dbaaf5d9":"model_to_submit_from.fit(X,y)","7282ad9d":"final_test = test_onehot.drop(['PassengerId'], axis=1).copy()","fe76efcd":"final_prediction = model_to_submit_from.predict(final_test)","cc9788b2":"accu_rafo = model_to_submit_from.score(X, y)\nround(accu_rafo*100,2)","016cc581":"svm_clf = SVC(gamma=\"auto\")\nsvm_clf.fit(X, y)","031ffcee":"svm_pred = svm_clf.predict(final_test)","a69689ae":"svm_scores = cross_val_score(svm_clf, X, y, cv=10)\nsvm_scores.mean()","155d364e":"forest_clf = RandomForestClassifier(n_estimators=200, random_state=42)\nforest_scores = cross_val_score(forest_clf, X, y, cv=10)\nforest_scores.mean()","700670da":"forest_clf.fit(X, y)","f1646630":"forest_pred = forest_clf.predict(final_test)\naccu_rafo = forest_clf.score(X, y)\nround(accu_rafo*100,2)","282d5b0d":"#first applying Logistic Regression\n\nlg = LogisticRegression()\nlg.fit(X, y)\nlg_pred = lg.predict(final_test)\naccu_lg = (lg.score(X, y))\nround(accu_lg*100,2)","f7e95b2c":"lg_scores = cross_val_score(lg, X, y, cv=10)\nlg_scores.mean()","dfc39ea5":"voting_clf = VotingClassifier(\nestimators=[('xgb', model_to_submit_from), ('lr', lg)],\nvoting='hard',\nn_jobs=-1)","a1bbf260":"voting_clf.fit(X, y)","e15f5f8b":"ensemble_predict = voting_clf.predict(final_test)","84367494":"plt.figure(figsize=(8, 4))\nplt.plot([1]*10, svm_scores, \".\")\nplt.plot([2]*10, forest_scores, \".\")\nplt.boxplot([final_prediction, svm_scores, forest_scores, lg_scores], labels=(\"xgBoost\", \"SVM\",\"Random Forest\", \"Logistic Reg\"))\nplt.ylabel(\"Accuracy\", fontsize=14)\nplt.show()","6f3a34b2":"# train_data[\"AgeBucket\"] = train_data[\"Age\"] \/\/ 15 * 15\n# train_data[[\"AgeBucket\", \"Survived\"]].groupby(['AgeBucket']).mean()","a13b47e7":"# train_data[\"RelativesOnboard\"] = train_data[\"SibSp\"] + train_data[\"Parch\"]\n# train_data[[\"RelativesOnboard\", \"Survived\"]].groupby(['RelativesOnboard']).mean()","1987ad88":"submission = pd.DataFrame(columns = ['PassengerId', 'Survived']) \nprint(submission.head())\nsubmission.PassengerId = test_data.PassengerId\nsubmission.Survived = ensemble_predict\nprint(len(submission))\nprint(submission.head())\nsubmission.to_csv('ensemble_titanic_pred.csv', index=False)","66f05599":"The Embarked attribute tells us where the passenger embarked: C=Cherbourg, Q=Queenstown, S=Southampton.","0836b420":"Okay, the **Age**, **Cabin** and **Embarked** attributes are sometimes null (less than 891 non-null), especially the **Cabin** (77% are null). We will ignore the **Cabin** for now and focus on the rest. The **Age** attribute has about 19% null values, so we will need to decide what to do with them. Replacing null values with the median age seems reasonable.","cdd13d76":"Great, our model is trained, let's use it to make predictions on the test set:","e3bff2c3":"## Logistic Regression","599c343b":"# Ensemble Voting","73023bdf":"The **Name** and **Ticket** attributes may have some value, but they will be a bit tricky to convert into useful numbers that a model can consume. So for now, we will ignore them.","a048177b":"To improve this result further, you could:\n* Compare many more models and tune hyperparameters using cross validation and grid search,\n* Do more feature engineering, for example:\n  * replace **SibSp** and **Parch** with their sum,\n  * try to identify parts of names that correlate well with the **Survived** attribute (e.g. if the name contains \"Countess\", then survival seems more likely),\n* try to convert numerical attributes to categorical attributes: for example, different age groups had very different survival rates (see below), so it may help to create an age bucket category and use it instead of the age. Similarly, it may be useful to have a special category for people traveling alone since only 30% of them survived (see below).","9291b9e1":"# Data cleaning, processing, analysing","ee5e5259":"Training on full data","85451430":"We can see that, 'Name' 'Cabin' and 'Ticket' columns are random, and have no impact on Survival of passenger as other features had. Seriously - \"Whats in a name?!\"\n* Hence, we shall remove these columns (done below), as they don't contribute to our analysis.\n* Also, we saw earlier, SibSp, Parch didn't have any effect on Survival of a passenger, so remove those too.\n* Note : We will remove Passenger Id from the training data set also.\n","9bb3a971":"Instead of just looking at the mean accuracy across the 10 cross-validation folds, let's plot all 10 scores for each model, along with a box plot highlighting the lower and upper quartiles, and \"whiskers\" showing the extent of the scores (thanks to Nevin Yilmaz for suggesting this visualization). Note that the `boxplot()` function detects outliers (called \"fliers\") and does not include them within the whiskers. Specifically, if the lower quartile is $Q_1$ and the upper quartile is $Q_3$, then the interquartile range $IQR = Q_3 - Q_1$ (this is the box's height), and any score lower than $Q_1 - 1.5 \\times IQR$ is a flier, and so is any score greater than $Q3 + 1.5 \\times IQR$.","5da70685":"Now let's build our preprocessing pipelines. We will reuse the `DataframeSelector` to select specific attributes from the `DataFrame`:","0a76f117":"Trying to improve xgbClassifier using random search","8be87b8b":"We will also need an imputer for the string categorical columns (the regular SimpleImputer does not work on those):","dfa07e7f":"# Load Data","e0cf8d3b":"* Yikes, only 38% **Survived**. :(  That's close enough to 40%, so accuracy will be a reasonable metric to evaluate our model.\n* The mean **Fare** was \u00a332.20, which does not seem so expensive (but it was probably a lot of money back then).\n* The mean **Age** was less than 30 years old.","7688282c":"## SVC","f3474795":"# Submission","c0946a55":"# Training","5f1dd219":"## Random Forest","d3a0da3c":"Cool! Now we have a nice preprocessing pipeline that takes the raw data and outputs numerical input features that we can feed to any Machine Learning model we want.","9e80a661":"## XGBoost"}}