{"cell_type":{"d5194e07":"code","a1542d07":"code","222ca648":"code","a3097eb3":"code","943b0bda":"code","21e9e6c9":"code","a202938b":"code","6b2fc574":"markdown","7683efa1":"markdown","f8004a5d":"markdown","7f4086c2":"markdown","123c5bf6":"markdown","b9002f63":"markdown","ce20d0a1":"markdown","eb7f154b":"markdown"},"source":{"d5194e07":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a1542d07":"from numpy import array\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\n","222ca648":"# define documents\ndocs = ['Well done!',\n\t\t'Good work',\n\t\t'Great effort',\n\t\t'nice work',\n\t\t'Excellent!',\n\t\t'Weak',\n\t\t'Poor effort!',\n\t\t'not good',\n\t\t'poor work',\n\t\t'Could have done better.']\n# define class labels\nlabels = array([1,1,1,1,1,0,0,0,0,0])\n","a3097eb3":"# integer encode the documents\nvocab_size = 50\nencoded_docs = [one_hot(d, vocab_size) for d in docs]\nprint(\"\\nEncoded Documents :- \\n\\n \",encoded_docs)\n","943b0bda":"# pad documents to a max length of 4 words\nmax_length = 4\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(\"\\nPadded Documents :- \\n\\n \",padded_docs)\n","21e9e6c9":"# define the model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 8, input_length=max_length))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n# summarize the model\nprint(model.summary())\n\n","a202938b":"# fit the model\nmodel.fit(padded_docs, labels, epochs=50, verbose=0)\n# evaluate the model\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\nprint('Accuracy: %f' % (accuracy*100))","6b2fc574":"Note : Don't forget to upvote the tutorial.","7683efa1":"# Integer Encoding Each Document\n\nEmbedding layer will require sequences of integers as input :-\n* We could experiment with other more sophisticated bag of word model encoding like counts or TF-IDF\n* Keras provides the one_hot() function that creates a hash of each word as an efficient integer encoding\n\nWe will estimate the vocabulary size of 50, which is much larger than needed to reduce the probability of collisions from the hash function\n","f8004a5d":"# Defining Embedding Layer\n\nWe are now ready to define our Embedding layer as part of our neural network model. The Embedding has a vocabulary of 50 and an input length of 4. We will choose a small embedding space of 8 dimensions.\n\nImportantly, the output from the Embedding layer will be 4 vectors of 8 dimensions each, one for each word\nWe flatten this to a one 32-element vector to pass on to the Dense output layer\n","7f4086c2":"# Normalizing Length of Input\n\nThe sequences have different lengths and Keras prefers inputs to be vectorized and all inputs to have the same length\nWe will pad all input sequences to have the length of 4\nAgain, we can do this with a built in Keras function, in this case the pad_sequences() function\n","123c5bf6":"# Define Documents and Their Class Labels","b9002f63":"# Text Classification Problem\n\nThis is a simple sentiment analysis problem. We will define a small problem where we have 10 text documents, each with a comment about a piece of work a student submitted. \n\nEach text document is classified as positive \u201c1\u201d or negative \u201c0\u201d. \n","ce20d0a1":"# Defining a Model with Embedding Layer\n\nThe model is a simple binary classification model.\n","eb7f154b":"# Training and Evaluation"}}