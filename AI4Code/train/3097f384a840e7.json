{"cell_type":{"aa91408c":"code","79ae96d2":"code","d4653763":"code","e82704e9":"code","037ec08f":"code","6c284c9f":"code","730ad95d":"code","92235622":"code","ec69779f":"code","0175d7d7":"code","f0cb616c":"code","c21b1ebd":"code","d805c8bd":"code","dcc13e52":"code","ee82ecb3":"code","fde0132a":"code","3ced2eaf":"code","fd344c2b":"code","24c6f914":"markdown","ac49408d":"markdown","8c15a02b":"markdown","821f7483":"markdown","417fbfa9":"markdown","bb7906ed":"markdown","b5a34766":"markdown","9e44872f":"markdown","1b673139":"markdown","1f72835f":"markdown","0b8aca97":"markdown","f088570c":"markdown","3544915e":"markdown","7a163fdd":"markdown","51dfd46a":"markdown","64623518":"markdown","24299271":"markdown","017b7af2":"markdown"},"source":{"aa91408c":"!pip install cloud-tpu-client==0.10 https:\/\/storage.googleapis.com\/tpu-pytorch\/wheels\/torch_xla-1.7-cp37-cp37m-linux_x86_64.whl","79ae96d2":"%matplotlib inline\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport seaborn as sns # visualizations\nfrom PIL import Image # image processing\nimport matplotlib.pyplot as plt # visualizations\n\n#PyTorch\nimport torch\nimport torch.nn as nn \nfrom torch.autograd import Variable\nimport torchvision\nfrom torchvision import datasets, models,transforms,datasets\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random, time\nfrom sklearn import metrics\nimport itertools\nimport torch.optim as optim\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\nfrom torch.optim import lr_scheduler\n\nprint(torch.__version__)  #version de Pytorch","d4653763":"# Get Path to Train and Valid folders\ntrain_path = '..\/input\/new-plant-diseases-dataset\/New Plant Diseases Dataset(Augmented)\/New Plant Diseases Dataset(Augmented)\/train'\nvalid_path = '..\/input\/new-plant-diseases-dataset\/New Plant Diseases Dataset(Augmented)\/New Plant Diseases Dataset(Augmented)\/valid'\n\n# Function to creta DataFrame with plantpath, name, etc...\ndef Create_DF_DataSet_Info(path):\n    List_Plant=[]\n    List_Category=[]\n    List_Disease=[]\n    List_Path=[]\n    List_Plant___Category=[]\n    List_Image_size=[]\n    List_Image_type=[]\n    List_size=[]\n\n    for path, dirs, files in os.walk(path,topdown=False):\n\n        for name in files:\n\n            plant___category=category=path.split('\/')[-1]\n            plant=plant___category.split('___')[0]\n            category=plant___category.split('___')[-1]\n            disease= 0 if category=='healthy' else 1\n            full_path=path+'\/'+name\n            Image_type=name.split('.')[-1]\n            size=os.path.getsize(full_path)\n\n            with Image.open(full_path) as img:\n                width, height = img.size\n            List_Plant.append(plant)\n            List_Category.append(category)\n            List_Disease.append(disease)\n            List_Path.append(full_path)\n            List_Plant___Category.append(plant___category)\n            List_Image_size.append(str(width)+'x'+str(height))\n            List_Image_type.append(Image_type)\n            List_size.append(size)\n\n    df_info=pd.DataFrame.from_dict({'Plant':List_Plant,'Category':List_Category,'Disease':List_Disease,'Path':List_Path,\n                                    'Plant___Category':List_Plant___Category,'Image_size':List_Image_size,\n                                    'Image_type':List_Image_type,\"Size\":List_size}, orient = 'columns')\n    \n    df_image_count=pd.DataFrame(df_info.groupby(['Plant','Category','Plant___Category']).size())\n    df_image_count.rename(columns={0: 'nb'}, inplace=True)\n    df_image_count=df_image_count.reset_index()\n\n    return df_info,df_image_count\n\ntrain_info,train_count=Create_DF_DataSet_Info(train_path)\nvalid_info,valid_count=Create_DF_DataSet_Info(valid_path)","e82704e9":"#Plot image for each plant categorie and for each plant diseases\n\nlist_plant=list(train_count.Plant.unique())\n\nfor plant in list_plant:  \n    list_disease=train_info.Category[train_info['Plant']==plant].unique().tolist()\n  \n    if len(list_disease)<5:\n        rows=1\n        cols = len(list_disease)\n    else:\n        cols = 4\n        rows = len(list_disease)\/\/4+1\n    fig, ax = plt.subplots(nrows=rows, ncols=cols,figsize=(cols*5,rows*5 ),squeeze=False)\n\n    fig.suptitle(plant, fontsize=20)\n    row=0\n    col=0\n\n    for diseases in list_disease:\n        if col <4:\n            col+=1\n            row=row\n        else:\n            col=1\n            row+=1 \n    \n        img = plt.imread(train_info[(train_info.Plant==plant) & (train_info.Category==diseases)].iloc[2,3])\n        ax[row, col-1].axis('off')\n        ax[row, col-1].set_title(diseases)\n        ax[row, col-1].imshow(img);","037ec08f":"# Plot Healthy VS Disease plant number\ndef plot_image_healthy_vs_disease(dataset_count):\n    plt.rcParams['font.size'] = 14\n    plt.pie([dataset_count.nb[dataset_count.Category=='healthy'].sum(), dataset_count.nb[dataset_count.Category!='healthy'].sum()],labels=[\"Healthy\", \"Disease\"],autopct = lambda x: str(round(x, 1)) + '%',colors = [\"mediumaquamarine\",\"lightcoral\"],shadow=False,explode=(0.05,0))\n\n\nplt.figure(figsize=(15,7))\nplt.subplot(1,3,1)\nplt.title('Train\/Valid dataset distribution', fontsize=15)\nplt.rcParams['font.size'] = 14\nplt.pie([valid_count.nb.sum(), train_count.nb.sum()],labels=[\"Valid set\", \"Train set\"],autopct = lambda x: str(round(x, 2)) + '%',colors = [\"wheat\",\"lightseagreen\"],shadow=False,explode=(0.05,0))\nplt.subplot(1,3,2)\nplt.title('Train Set - Healthy VS disease plant', fontsize=15)\nplot_image_healthy_vs_disease(train_count)\nplt.subplot(1,3,3)\nplt.title('Valid Set - Healthy VS disease plant', fontsize=15)\nplot_image_healthy_vs_disease(valid_count);","6c284c9f":"# Plot the classes repartition\ndef plot_classes_distribution(dataset_count,dataset_info):\n    liste_plant_modif = ['Cherry' if x == 'Cherry_(including_sour)' else x for x in list(dataset_count.Plant.unique())]\n\n    # Data to plot\n    colors1 = ['#99ffcc', '#99ffff', '#99ffcc', '#99ffff','#99ffcc', '#99ffff','#99ffcc', '#99ffff','#99ffcc', '#99ffff','#99ffcc', '#99ffff','#99ffcc', '#99ffff',]\n    colors2 = ['#ff999a','#ffcb99', '#fffe99','#99ffcc',\n             '#99ffff',\n             '#ffcb99', '#99ffcc',\n             '#ff999a','#ffcb99','#fffe99','#99ffff',\n             '#ff999a','#ffcb99','#fffe99','#99ffcc',\n             '#99ffff',\n             '#fffe99','#99ffcc',\n             '#fffe99','#99ffff',\n             '#ffcb99','#fffe99','#99ffcc',\n             '#99ffff',\n             '#99ffcc',\n             '#99ffff',\n             '#fffe99','#99ffcc',\n             '#ff999a','#ff9990','#ffb299','#ffc5b3','#ffece6','#ffedb3','#fff3cd','#d1ffcd','#cdffdf','#99ffff']\n \n  # Plot Pie chart\n    plt.pie(dataset_count.nb, labels=dataset_count.Category,  startangle=90,frame=True,radius=1,rotatelabels=True,colors=colors2,wedgeprops=dict(width=0.9, edgecolor='w'),autopct='%1.f%%',pctdistance=0.90, textprops={'fontsize': 12})\n    plt.pie(dataset_info.groupby(['Plant']).size(),labels=liste_plant_modif,radius=0.75,startangle=90,labeldistance=0.3,rotatelabels=True,colors=colors1,wedgeprops=dict(width=0.9, edgecolor='w'),autopct='%1.f%%',pctdistance=0.90, textprops={'fontsize': 12})\n    centre_circle = plt.Circle((0,0),0.5,color='black', fc='white',linewidth=0)\n    fig = plt.gcf()\n    fig.gca().add_artist(centre_circle)\n \n    plt.axis('equal')\n    plt.tight_layout()\n    plt.show()\n\nplt.figure(figsize=(12,12))\nplot_classes_distribution(train_count,train_info)","730ad95d":"# DataSet folder\n#------------------------------------------------------------------------------------------------------------\nData_path=r'..\/input\/new-plant-diseases-dataset\/New Plant Diseases Dataset(Augmented)\/New Plant Diseases Dataset(Augmented)'\n\n# Location of saved model\n#------------------------------------------------------------------------------------------------------------\npath_save='..\/output\/kaggle\/working\/model_CNN'\n\n\n# Parameters\n#------------------------------------------------------------------------------------------------------------\nbatch_size = 128 # Google recommends using 128 images per batch for the best performance\nTARGET_SIZE = 64, 64 # Used for resize images: from 256x256 to 64x64\nnum_workers = 4 # how many subprocesses to use for data loading\nlearning_rate = 0.001 # Default learning rate for Adam optimizer\nEpochs = 25 # number of epoch used to train the model\nDisplay_results_iter = 100 #used to display results every n iterations","92235622":"# Function fo creation of dataset\n#------------------------------------------------------------------------------------------------------------\ndef create_dataset():\n    composed_transforms = transforms.Compose([transforms.Resize(TARGET_SIZE),transforms.ToTensor()])\n    dataset = {x: datasets.ImageFolder(os.path.join(Data_path, x), composed_transforms) for x in ['train', 'valid']}\n    nb_category=len(dataset['train'].classes)\n    return dataset, nb_category\n\n# Create dataset and extract number of Classes\n#------------------------------------------------------------------------------------------------------------\nSERIAL_EXEC = xmp.MpSerialExecutor() #Utility to run a function in a serialized fashion among multi-core processes\ndataset , nb_category  = SERIAL_EXEC.run(create_dataset) # Avoid all cores downloading the same data with the serial executor\n\nprint('the number of images in the train set is:',len(dataset['train']))\nprint('the number of images in the valid set is:',len(dataset['valid']))","ec69779f":"  #Sampler that restricts data loading to a subset of the dataset.\n  #It is especially useful in conjunction with torch.nn.parallel.DistributedDataParallel.\n  #In such a case, each process can pass a DistributedSampler instance as a DataLoader sampler, and load a subset of the original dataset that is exclusive to it.\n  \ntrain_sampler = torch.utils.data.distributed.DistributedSampler(\n    dataset['train'],\n    num_replicas=xm.xrt_world_size(), #Number of processes participating in distributed training = number of available TPU cores\n    rank=xm.get_ordinal(), # Rank of the current process within num_replicas = replication ordinal of the current process\n    shuffle=True)\n\ntrain_loader = torch.utils.data.DataLoader(\n      dataset['train'],\n      batch_size=batch_size,\n      sampler=train_sampler,\n      num_workers=num_workers,\n      drop_last=True)#drops the last non-full batch of each worker\u2019s dataset replica.\n\nvalid_loader = torch.utils.data.DataLoader(\n      dataset['valid'],\n      batch_size=batch_size,\n      shuffle=False,\n      num_workers=num_workers,\n      drop_last=True)#drops the last non-full batch of each worker\u2019s dataset replica.\n\ndict_index_to_class= {v: k for k, v in dataset['train'].class_to_idx.items()}\nCLASSES=[v for k, v in dict_index_to_class.items()]","0175d7d7":"class Model(nn.Module):\n\n    def __init__(self):\n        super(Model, self).__init__()\n        \n        self.categories = 38\n        \n        self.block1 = nn.Sequential(  \n            nn.Conv2d(in_channels = 3, out_channels = 128, kernel_size = 3, padding = 1),\n            nn.LeakyReLU(inplace = True),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(kernel_size = 2)) \n        \n        self.block2 = nn.Sequential(          \n            nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1),\n            nn.LeakyReLU(inplace = True),\n            nn.BatchNorm2d(256),\n            nn.MaxPool2d(kernel_size = 2))\n            \n        self.block3 = nn.Sequential(              \n            nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, padding = 1),\n            nn.LeakyReLU(inplace = True),\n            nn.BatchNorm2d(512),\n            nn.MaxPool2d(kernel_size = 2))\n\n\n        \n        self.classify = nn.Sequential(\n\n            nn.Dropout(p=0.2),\n            nn.Linear(in_features = 512 * 8 * 8, out_features = 512),\n            nn.LeakyReLU(inplace = True),\n\n            nn.Linear(in_features = 512, out_features = self.categories),\n        )       \n\n    def forward(self, x):\n        x1 = self.block1(x)\n        x2 = self.block2(x1)\n        x3 = self.block3(x2)\n        x4 = x3.view(x3.size(0), -1)\n        x5 = self.classify(x4)\n        return x5\n\n# instantiate model\n#------------------------------------------------------------------------------- \n\"\"\" MpModelWrapper: \n  Instead of creating models on each multiprocessing process, hence replicating\n  the model's initial host memory, the model is created once at global scope,\n  and then moved into each device inside the `spawn()` target function.\"\"\"\n\nWRAPPED_MODEL = xmp.MpModelWrapper(Model())","f0cb616c":"# Learning rate:\n#-------------------------------------------------------------------------------\nlr = learning_rate * xm.xrt_world_size() # Scale learning rate to number of TPU\n\n# Get loss function, optimizer, and model\n#-------------------------------------------------------------------------------\ndevice = xm.xla_device()\nmodel = WRAPPED_MODEL.to(device)\noptimizer = optim.Adam(model.parameters(), lr=lr,weight_decay=1e-3)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.2)\nloss_fn = nn.CrossEntropyLoss()\n\n# Number of availblae TPU\n#-------------------------------------------------------------------------------\nprint('number of available TPU cores',xm.xrt_world_size())","c21b1ebd":"  # Function to train the model with train dataset\n  #----------------------------------------------------------------------------- \n  def train_fun(loader):\n    total_samples = 0\n    correct = 0\n    total_loss_per_epoch=0\n    tracker = xm.RateTracker()\n    model.train() #train mod, active layers like dropout, batchnorm...\n    \n    for x, (data, target) in enumerate(loader):\n        optimizer.zero_grad() #set the gradients to zero before starting to do backpropragation to not accumulate gradients\n        output = model(data.squeeze(1))\n        loss = loss_fn(output, target) #calculate loss\n        total_loss_per_epoch+=loss.item() #calculate the sum of loss per epoch\n        loss.backward() #back propagation\n        xm.optimizer_step(optimizer) #apply optimizer to calculate weights\n        tracker.add(batch_size)\n\n        pred = output.max(1, keepdim=True)[1] #determine the classe with highest value\n        correct += pred.eq(target.view_as(pred)).sum().item() #calculate the number of good pred\n        total_samples += data.size()[0]\n\n        if x % Display_results_iter == 0:\n            print('[xla:{}]({}) | Loss={:.5f} | Learning rate={}'.format(\n                xm.get_ordinal(), x, loss.item(),optimizer.param_groups[0]['lr'], time.asctime()), flush=True)\n    loss_per_epoch = total_loss_per_epoch\/x #calculate the mean loss per epoch\n    accuracy = 100.0 * correct \/ total_samples #calculate the accuracy \n    print('[xla:{}] | For TRAIN dataset: Accuracy ={:.2f}% | Loss ={:.5f} '.format(xm.get_ordinal(), accuracy,loss_per_epoch), flush=True)\n    return accuracy, loss_per_epoch\n\n  # Function to eval the model with validation dataset\n  #-----------------------------------------------------------------------------\n  def test_fun(loader):\n    total_samples = 0\n    correct = 0\n    total_loss_per_epoch=0\n    model.eval()\n    data, pred, target = None, None, None\n    list_pred=[]\n    list_target=[]\n    list_prob=[]\n\n    for x,(data, target) in enumerate(loader):\n        output = model(data.squeeze(1)) #Prediction for valid DataSet\n        loss = loss_fn(output, target) #calculate loss\n        total_loss_per_epoch+=loss.item() #calculate the sum of loss per epoch\n        output_prob=F.softmax(output, dim=1) #calculate the probability with softmax\n        prob=output_prob.max(1, keepdim=True)[0] #determine the max prob\n        pred = output.max(1, keepdim=True)[1] #determine the class with highest value\n        correct += pred.eq(target.view_as(pred)).sum().item() #calculate the number of good pred\n        total_samples += data.size()[0]\n\n        list_pred.extend( pred.squeeze().tolist()) #add pret to a list\n        list_target.extend( target.squeeze().tolist()) #add target to a list\n        list_prob.extend( prob.squeeze().tolist()) #add probs to a list\n\n    loss_per_epoch = total_loss_per_epoch\/x #calculate the mean loss per epoch\n    accuracy = 100.0 * correct \/ total_samples #calculate the accuracy\n    print('[xla:{}] | For VALID dataset: Accuracy ={:.2f}% | Loss ={:.5f}'.format(\n        xm.get_ordinal(), accuracy,loss_per_epoch), flush=True)\n    return accuracy,loss_per_epoch, data, list_pred, list_target, list_prob\n\n  # Train and eval loops\n  #-----------------------------------------------------------------------------\n  \nt0 = time.time()\naccuracy = 0.0\nList_acc_valid=[]\nList_acc_train=[]  \nList_loss_valid=[]\nList_loss_train=[]\n\ndata, pred, target = None, None, None\n\nfor epoch in range(1, Epochs + 1):\n    print('------------------- Epoch {} ---------------------'.format(epoch))\n    para_loader = pl.ParallelLoader(train_loader, [device]) #train the model parallelly, Wraps an existing PyTorch DataLoader with background data upload\n    accuracy_train, loss_train = train_fun(para_loader.per_device_loader(device)) #run train function\n    scheduler.step() #apply scheduler to reduce learning rate\n    xm.master_print(\"Finished training epoch {}\".format(epoch))\n    para_loader = pl.ParallelLoader(valid_loader, [device]) #eval the model parallelly, Wraps an existing PyTorch DataLoader with background data upload\n    accuracy_valid,loss_valid, data, list_pred, list_target,list_prob  = test_fun(para_loader.per_device_loader(device)) #run test function\n    List_acc_valid.append(accuracy_valid)\n    List_acc_train.append(accuracy_train)\n    List_loss_valid.append(loss_valid)\n    List_loss_train.append(loss_train) \n\nprint('time :',round((time.time()-t0)\/60,0),' min') #print the time that model tooks to run\n\n#Save data by taking care of previously moving it to CPU\n#xm.save(model.state_dict(), path_save) ","d805c8bd":"xm.save(model.state_dict(), 'Model_CNN')","dcc13e52":"#Train and Valid Accuracy & Loss curve\n#-------------------------------------------------------------------------------\nprint('\\033[1m Accuracy for train dataset:{:.2f}%\\033[0m'.format(List_acc_train[-1]))\nprint('\\033[1m Accuracy for valid dataset:{:.2f}%\\033[0m'.format(List_acc_valid[-1]))\nprint('\\n')\n\n#Accuracy curves\n#-------------------------------------------------------------------------------\nplt.figure(figsize=(15,7))\nplt.subplot(121)\nplt.plot(List_acc_train,\"royalblue\",linewidth=2)\nplt.plot(List_acc_valid,\"tomato\",linewidth=2)\nplt.title('Model accuracy per epoch',fontsize=16)\nplt.ylabel('accuracy',fontsize=14)\nplt.xlabel('epoch',fontsize=14)\nplt.legend(['train', 'test'], loc='right',fontsize=14)\n\n#Loss curves\n#-------------------------------------------------------------------------------\nplt.subplot(122)\nplt.plot(List_loss_train,\"royalblue\",linewidth=2)\nplt.plot(List_loss_valid,\"tomato\",linewidth=2)\nplt.title('Model loss per epoch',fontsize=16)\nplt.ylabel('loss',fontsize=14)\nplt.xlabel('epoch',fontsize=14)\nplt.legend(['train', 'test'], loc='right',fontsize=14)\nplt.show()","ee82ecb3":"#Confusion matrix\n#-------------------------------------------------------------------------------\ncnf_matrix = metrics.confusion_matrix(list_target, list_pred)\nclasses = range(0,10)\n\nplt.figure(figsize=(20,20))\nplt.imshow(cnf_matrix, interpolation='nearest',cmap='Blues')\nplt.title(\"Confusion Matrix\",fontsize=20)\nplt.colorbar()\ntick_marks = np.arange(len(CLASSES))\n\nplt.xticks(tick_marks, CLASSES, rotation='vertical')\nplt.yticks(tick_marks, CLASSES)\n\nfor i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n    plt.text(j, i, cnf_matrix[i, j],\n             horizontalalignment=\"center\",\n             color=\"white\" if cnf_matrix[i, j] > ( cnf_matrix.max() \/ 2) else \"black\")\n\nplt.ylabel('True labels')\nplt.xlabel('Pr\u00e9dict labels')\nplt.show()","fde0132a":"#Plot image with pred classes and true classes\nplt.figure(figsize=(20,10))\nj=1\nfor i in np.random.randint(0, len(list_target), size=[6]):\n    plt.subplot(2,3,j)\n\n    if dataset['valid'][i][1]==list_pred[i]:\n        color='green'\n    else:\n        color='red'\n\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(transforms.ToPILImage()(dataset['valid'][i][0].squeeze_(0)))\n    plt.title('Pred Classe :{} - {:2.0f}%'.format(CLASSES[list_pred[i]],round(100*list_prob[i],1)) , color=color,fontweight='bold',fontsize=10)\n    plt.xlabel('True Classe :'+str(CLASSES[dataset['valid'][i][1]]),fontweight='bold',fontsize=10)\n    j+=1","3ced2eaf":"class Model(nn.Module):\n\n    def __init__(self):\n        super(Model, self).__init__()\n        \n        self.categories = 38\n        \n        self.block1 = nn.Sequential(  \n            nn.Conv2d(in_channels = 3, out_channels = 128, kernel_size = 3, padding = 1),\n            nn.LeakyReLU(inplace = True),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(kernel_size = 2)) \n        \n        self.block2 = nn.Sequential(          \n            nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1),\n            nn.LeakyReLU(inplace = True),\n            nn.BatchNorm2d(256),\n            nn.MaxPool2d(kernel_size = 2))\n            \n        self.block3 = nn.Sequential(              \n            nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, padding = 1),\n            nn.LeakyReLU(inplace = True),\n            nn.BatchNorm2d(512),\n            nn.MaxPool2d(kernel_size = 2))\n\n\n        \n        self.classify = nn.Sequential(\n\n            nn.Dropout(p=0.2),\n            nn.Linear(in_features = 512 * 8 * 8, out_features = 512),\n            nn.LeakyReLU(inplace = True),\n\n            nn.Linear(in_features = 512, out_features = self.categories),\n        )       \n\n    def forward(self, x):\n        x1 = self.block1(x)\n        x2 = self.block2(x1)\n        x3 = self.block3(x2)\n        x4 = x3.view(x3.size(0), -1)\n        x5 = self.classify(x4)\n        return x5, x4, x3, x2 , x1, x\n\n# instantiate model\nModel = Model()\nModel.load_state_dict(torch.load('Model_CNN'))\nModel.eval()","fd344c2b":"img = Image.open('..\/input\/new-plant-diseases-dataset\/test\/test\/AppleCedarRust2.JPG')\ncomposed_transforms = transforms.Compose([transforms.Resize(TARGET_SIZE),transforms.ToTensor()])\nimg_tensor = composed_transforms(img)\nimg_tensor.unsqueeze_(0)\nx5, x4, x3, x2 , x1, x=Model(img_tensor)\nfiltre=0\n\nplt.figure(figsize=(20,10))\nplt.subplot(1,4,1)\nplt.title('Original')\nplt.imshow(transforms.ToPILImage()(x[0]))\nplt.subplot(1,4,2)\nplt.title('Block1')\nplt.imshow(transforms.ToPILImage()(x1[0][filtre]),'gray')\nplt.subplot(1,4,3)\nplt.title('Block2')\nplt.imshow(transforms.ToPILImage()(x2[0][filtre]),'gray')\nplt.subplot(1,4,4)\nplt.title('Block3')\nplt.imshow(transforms.ToPILImage()(x3[0][filtre]),'gray')","24c6f914":"## 3. Model evaluation\n### 1. Train and Valid Accuracy & Loss curves","ac49408d":"## DataSet Information\n**Train Set Summary :**\n* Images type : JPEG\n* Images dimensions : 256 x 256\n* Images color: RGB\n* Images number : 70295\n* Images size : min= 2.7 kb , max = 30.7 kb, mean = 16.2 kb, Total = 1.14 Gb\n\n**Valid Set Summary :**\n* Images type : JPEG\n* Images dimensions : 256 x 256\n* Images colro: RGB\n* Images number : 17 572\n* Images size : min= 3.9 kb , max = 28.2 kb, mean = 16.2 kb, Total = 285 Mb\n\n","8c15a02b":"## Images repartition","821f7483":"# Dataset Description\n\nThe purpose of this project is, from a photo of a plant leaf, to determine which plant it is, whether the plant is healthy or which disease the plant is affected.  \n\nThe names and categories of each plants are based on the directory structure.\n\nFolders structure is as below:\n- For diseased plants: `[Name of the plants]___[name of the disease]`\n- For healthy plants: `[Name of the plants]___healthy`","417fbfa9":"## Visualisation of sample leaves","bb7906ed":"![CNN_v2.jpg](attachment:CNN_v2.jpg)","b5a34766":"Parameters for model training are:","9e44872f":"# CNN with PyTorch and TPU","1b673139":"The code below create a dataset using \"datasets.ImageFolder\" with the images paths, the labels extracted from the folder names and define the applied images transformations will perform by the dataloader during the model trainning.\nAs in the dataset, images are already flipped and croped, transformations applied in the dataset are: resize the images and convert image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0], Classes are converted to intergers","1f72835f":"## Model trainning","0b8aca97":"### 2. Confusion Matrix for valid DataSet","f088570c":"### 4. For fun, let's plot the image after each block (filter = 0)","3544915e":"Visualisation of one leaf for each plant and for each plant category.\n\nWe can notice that:\n- There is one leaf per image.\n- Leaves seems to be centered in the image.\n- Leaves' photos are not taken on the trees.\n- Background is black.","7a163fdd":"## Model creation","51dfd46a":"## Optimizer & Loss function\n**Optimizer** : Adam with high weight decay (L2 penalty) in order to reduce overfitting.\n\n**Loss function** : CrossEntropyLoss(combines nn.LogSoftmax() and nn.NLLLoss() in one single class)","64623518":"![TPU%20-%20v2.bmp](attachment:TPU%20-%20v2.bmp)","24299271":"To sum up what we have seen above - for both Train and Validation set:\n* Images type : JPEG\n* Images dimensions : 256 x 256\n* Image color: RGB\n* DataSet has **14** types of plants, under **38** categories\n* Train set represents 80% and Validation set represents 20% of the whole Dataset.\n* Images are already fliped, croped, etc...\n* The datset is balanced\n* No Data Augmentation is needed\n","017b7af2":"### 3. Display 6 images randomly with prediction and true class"}}