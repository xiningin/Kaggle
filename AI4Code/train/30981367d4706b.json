{"cell_type":{"48d3ca59":"code","1bb053e6":"code","c68a838b":"code","7f935002":"code","5a9998a2":"code","924a1365":"code","9eaf260a":"code","162f7281":"markdown","e989b012":"markdown","6dfcdbac":"markdown","7a429c26":"markdown","a6ebd3dd":"markdown","21285778":"markdown","b908c178":"markdown","1e050a15":"markdown"},"source":{"48d3ca59":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport requests\nfrom bs4 import BeautifulSoup","1bb053e6":"link = \"https:\/\/www.holidify.com\/explore\/\"\nP_link = requests.get(link)\nprint(P_link)\n\nP_html = P_link.text\nP_soup = BeautifulSoup(P_html, \"html.parser\")","c68a838b":"containers = P_soup.findAll(\"div\", {\"class\" : \"col-12 col-md-6 pr-md-3 result\"})\nprint(len(containers))","7f935002":"container = containers[0]\ncolumn = ['Place','State', 'Ratings', 'About','Price', 'Attraction']\nPlaces = pd.DataFrame(columns = column)","5a9998a2":"for container in containers:\n    p_name = container.findAll(\"h2\", {\"class\":\"card-heading\"})\n    p_nameN = p_name[0].text[4:].strip().split()\n    if len(p_nameN) == 2:        \n          p_nameP = p_nameN[0]\n          p_nameP = p_nameP.replace(',','')\n          p_nameC = p_nameN[1]\n    elif len(p_nameN) == 3:\n          p_nameP = p_nameN[0]\n          p_nameP = p_nameP.replace(',','')\n          p_nameC = p_nameN[1] + \" \" + p_nameN[2]\n    elif len(p_nameN) == 4:\n          p_nameP = p_nameN[0]\n          p_nameP = p_nameP.replace(',','')\n          p_nameC = p_nameN[1] + \" \" + p_nameN[2] + \" \" + p_nameN[3]      \n    else:\n          p_nameP = p_nameN[0]\n          p_nameC = \"NaN\"\n    p_rating = container.findAll(\"span\", {\"class\" : \"rating-badge\"})\n    p_rating = p_rating[0].text[1:4]\n    p_about = container.findAll(\"p\",{\"class\": \"card-text\"})\n    p_about = p_about[0].text\n    p_price = container.findAll(\"p\",{\"class\": \"collection-cta\"})\n    if len(p_price) == 1:\n        p_num = p_price[0].text.replace(',','')\n        p_numb = re.findall(r'\\d+', p_num)\n        num = \"\"\n        for i in p_numb:\n            num += i\n    else:\n        num = \"NaN\"\n    p_attraction = container.findAll(\"div\", {\"class\":\"content-card-footer\"})\n    p_attraction = p_attraction[0].text[:-12].strip()\n    \n    Data = pd.DataFrame([[p_nameP ,p_nameC, p_rating , p_about, num , p_attraction]])\n    Data.columns = column\n    Places = Places.append(Data, ignore_index = True)","924a1365":"print(Places.head())","9eaf260a":"Places.to_csv(\"Places.csv\", index = None)","162f7281":"Here we first go to the First Table of the Website Holidify and open the HTML page by **Right Clicking**, then go to **Inspect Element**.\n\nWhere we find that the all the values of the first table is in the \"div\" \"Class\" of \"col-12 col-md-6 pr-md-3 result\". \n\nThus we type **findAll** value in our **P_soup**  inside the div class of \"col-12 col-md-6 pr-md-3 result\".\n\nAt last we will print the len of the total numbers of table on one webpage. Here in this case there are 60 values on first WebPage.","e989b012":"And here comes the link, first we copy and paste the url and assigned it to the variable link, then we send a request to the web page to reture the information.\n\nAfter requesting, we will print out the value like I did here\nprint(P_link) , which gives the output of \"<Response [200]>\" or \"200\" which means you are allowed to do web scraping on such websites.\n\nAfter Getting the Response we will convert the HTML page into readable form of text and assigned it to the variable P_html. \n\nThen we use Beatiful Soup to Convert the HTML page in to readable form by passing \"html.parser\".\n","6dfcdbac":"And at the end we will create a CSV file and export our newly created Data.","7a429c26":"Then here we assigned the first value in the table with containers[0] and assigned it a new variable container.\nWe will do all the scraping work on the first value of table first then we move on create a loop and do it for the rest of table.\n\nAfter that we create a new **DataFrame** which became our actual file, which contain all the data, which we want to create from the start.\n\nWe create a list of values and assigned a name **column**,this will become our columns name of our Data **Places**.","a6ebd3dd":"In this kaggle kernel, I'm going to do web scraping of Website \"https:\/\/www.holidify.com\/explore\/\" Page.\nThis Webpage Contain 60 Places to Visit in India and I'm going to scrap Those 60 Places information like Name of the Place, then the State it is in, then Average Rating given by people who went there, then short Paragraph describing the Place, then the Total Estimated Price, then finally there numbers of major attraction points.\n\n**Gangtok, Sikkim**\n![](http:\/\/www.holidify.com\/images\/bgImages\/GANGTOK.jpg)\n","21285778":"Then Finally after extracting all the Data and adding it to our DataFrame which we created earlier. We will print out the value by typing **print(Places.head())** to print the first five values.","b908c178":"First we Import all the Important Packages that are required to do Web Scraping.\n\n**Numpy** and **Pandas** are standard and I always import it, who knows when it come handy.\n\n**re** is the package required to do Regular Expression, with the help of this package, we can easily search for particular type of words or digits in a string, and re is one of the most important string manipulation package.\n\n**requests** is a Python module that you can use to send all kinds of HTTP requests. It is an easy-to-use library with a lot of features ranging from passing parameters in URLs to sending custom headers and SSL Verification.\n\n**BeautifulSoup** is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It saves hours or days of work.","1e050a15":"Here we create a loop and print all the Values which we want to extract from the Webpage, here I want to import values like **Places Name, City Name, Ratings, About, Prices, Attraction**. \n\nRemember it's not always necessery that each value you want to extract are always available separately or will be in the structured form, we have to **string manipulation** to clean the data and also **split()** the values to create two or more values."}}