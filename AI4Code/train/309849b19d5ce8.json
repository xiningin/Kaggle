{"cell_type":{"337c5cf9":"code","c674a895":"code","0902ca8b":"code","76d402a9":"code","46e94da4":"code","fd206ad1":"code","1ad57793":"code","6d76e163":"code","bfff5b74":"code","da16c482":"code","11c8e1c1":"code","306e1480":"code","4791ba8f":"markdown","f37e3091":"markdown","28dd8062":"markdown","5a88113e":"markdown","542a5145":"markdown","1207eda9":"markdown","b3e33aef":"markdown","6ee6d924":"markdown","99f2055e":"markdown","7a796f15":"markdown","551c752d":"markdown","515baac3":"markdown","5f686ecd":"markdown","830e00b6":"markdown","27b3a8c6":"markdown"},"source":{"337c5cf9":"import numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs, make_moons, make_circles\nfrom sklearn.model_selection import train_test_split \nimport random\nfrom scipy.spatial import distance\nfrom sklearn.metrics import accuracy_score","c674a895":"X1, y1 = make_blobs(n_samples=300, centers=4,random_state=0, cluster_std=0.60)\nplt.scatter(X1[:, 0], X1[:, 1], s=50, alpha=.8)\nplt.title(\"Dataset 1\")\nplt.show()","0902ca8b":"X2, y2 = noisy_circles = make_circles(n_samples=300, factor=.5, noise=.05)\nprint(\"First five rows and col values \\nX2 : \\n\",X2[:5], \" \\n y2 :\\n\",y2[:5])\nplt.scatter(X2[:, 0], X2[:, 1], s=50, cmap='winter', alpha=.5)\nplt.title(\"Dataset 2\")\nplt.show()","76d402a9":"class DBScan:\n    def __init__(self, eps = 1, minPts = 30):\n        self.eps = eps\n        self.minPts = minPts\n        \n    def train(self, X, y):\n        self.X = X\n        self.y = y\n\n        # list for storing visited points, noise and core points\n        visited = []\n        noise = []\n        all_core = []\n        \n        # all points in X\n        for p in X:\n            # storing points inside tuple like: (x,y) for easy understandability\n            if ((p[0],p[1])) not in visited: # if it is not visited, it will not have any label\n                visited.append((p[0],p[1]))\n                neighbours = self.getNeighbours(X,p,visited) # find neighbours\n\n                # if neighbour points count < minimum points, consider those points as noise\n                if len(neighbours) < self.minPts: \n                    noise = noise + neighbours \n                    continue\n                else:\n                    # add all theneighbouring points and their neighbours to a new core\n                    new_core = []\n                    new_core.append((p[0],p[1]))\n                    # since we already visted current point, p\n                    neighbours.remove((p[0],p[1]))\n                    \n                    # seedset containing neighbours and their neighbours\n                    seedset = neighbours                    \n                    while len(seedset) > 0:\n                        q = seedset.pop(0) # like a queue, pop each element from begining, let's call it q (p is already referencing X)\n\n                        # if q in noise, assign it to new core \n                        if (q[0],q[1]) in noise:\n                            noise.remove((q[0],q[1]))\n                            new_core.append((q[0],q[1])) # adding q as border point\n                        \n                        # if q is not noise and not visited\n                        if (q[0],q[1]) not in visited:\n                            visited.append((q[0],q[1])) # add it to visited list\n                            new_core.append((q[0],q[1])) # its label is new core\n                            neighbours = self.getNeighbours(X,q,visited)\n                            if len(neighbours) >= self.minPts: # find neighbours of q\n                                seedset = seedset + neighbours # add it to the seedset  \n\n                    # storing all cores\n                    all_core.append(new_core)\n        self.plot(all_core, noise)\n        return all_core, noise\n            \n    def getNeighbours(self, X, p, visited):\n        neighbours = []\n        neighbours.append((p[0],p[1]))\n        for q in X:\n            if distance.euclidean(p, q) < self.eps:\n                neighbours.append((q[0],q[1]))\n        return neighbours\n   \n    def plot(self, all_core, noise):\n        for i,core in enumerate(all_core):\n            X_p = []\n            Y_p = []\n            pt = []\n            for x_p,y_p in core:\n                X_p.append(x_p)\n                Y_p.append(y_p)\n                pt.append([x_p,y_p])\n            plt.scatter(X_p,Y_p, label=i)\n        plt.title(\"DBScan\")\n        plt.legend()\n        plt.show()            ","46e94da4":"dbscan = DBScan(eps = 0.2, minPts = 5)\nall_core, noises = dbscan.train(X2,y2)\nprint(len(all_core))","fd206ad1":"dbscan = DBScan(eps = 0.5, minPts = 10)\nall_core, noises = dbscan.train(X1,y1)\nprint(len(all_core))","1ad57793":"class KMeans:\n    def __init__(self, k=4, epoches=10):\n        self.k = k\n        self.epoches = epoches\n        \n    def train(self,X,y):\n            self.X = X\n            self.y = y\n            n = X.shape[0] # training data instances\n            m = X.shape[1] # features count\n            \n            # generating random centroids using mean and std deviation\n            mean = np.mean(X, axis = 0) \n            std = np.std(X, axis = 0)\n            new_centers = np.random.randn(self.k,m)*std + mean            \n            prev_centers = np.zeros((self.k,m)) # (0,0) k times\n            iter_count = 0\n            cluster_info = {}\n            \n            # if the previous center is too close to the new center\n            while(self.centers_stable(prev_centers,new_centers, 0.005) != True):\n                \n                prev_centers = new_centers\n                \n                # dictionary storing centers as key and their nearby points in list as value\n                cluster_info = self.closest_cluster(X, new_centers)\n               \n                # re-calculating centers \n                new_centers = self.find_new_center(cluster_info)\n\n                # either we find center ro finish epoches\n                iter_count += 1\n                if iter_count > self.epoches:\n                    break\n            \n            return new_centers,cluster_info\n\n    # if the previous center is too close to the new center which means, centroids are not changing\n    def centers_stable(self,prev_centers, new_centers, min_dist):\n        if prev_centers.shape == new_centers.shape:        \n            dist = 0\n            for i in range(new_centers.shape[0]):\n                dist += distance.euclidean(prev_centers[i], new_centers[i])\n            return True if dist < min_dist else False\n\n    # dictionary storing centers as key and their nearby points in list as value\n    def closest_cluster(self, X, new_centers):    \n        cluster_info = {}\n        # initizling the dictionary\n        for i in range(self.k):\n            cluster_info[i] = []\n        \n        for i in range(X.shape[0]):\n            short_dist = 999 # max distance\n            closest_cluster = 0\n            for j in range(len(new_centers)):\n                dist = distance.euclidean(X[i,:], new_centers[j])\n                if dist < short_dist: # if dist < shortest dist found\n                    short_dist = dist\n                    closest_cluster = j # new closest cluster index\n            ls = cluster_info.get(closest_cluster)\n            ls.append(list(X[i,:]))\n            cluster_info[closest_cluster] = ls\n        return cluster_info\n    \n    # re-calculating new centers\n    def find_new_center(self,cluster_info):\n        new_centers = []\n        keys = cluster_info.keys()\n        # key => cluster index \n        for key in keys:\n            ls = cluster_info.get(key)\n            if len(ls) == 0:\n                continue\n            # using mean to find new centers\n            mean = np.mean(ls, axis = 0)\n            new_centers.append(mean)\n        return np.array(new_centers)\n    \n    # plot is entirely based on the dictionary, and not X,y\n    def plot(self,X,y,new_centers,cluster_info):\n        for i,key in enumerate(cluster_info.keys()):\n            data = cluster_info.get(key)\n            if len(data)>0:\n                data = np.array(data)                \n                plt.scatter(data[:,0], data[:,1], alpha = 0.5, label = key)\n                plt.scatter(new_centers[i,0], new_centers[i,1], marker='*', s=150, c=\"black\")\n                plt.title(\"KMeans\")\n        plt.legend()\n        plt.show()","6d76e163":"kmeans = KMeans(k=4)\nnew_centers,cluster_info = kmeans.train(X1,y1)\nkmeans.plot(X1,y1,new_centers, cluster_info)","bfff5b74":"kmeans = KMeans(k=2)\nnew_centers,cluster_info = kmeans.train(X2,y2)\nkmeans.plot(X2,y2,new_centers, cluster_info)","da16c482":"# creating testing and training set to train model, will be used for k means\nX_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.3, random_state=1)\nplt.scatter(X_train[:, 0], X_train[:, 1], s=50, alpha=.8)\nplt.title(\"Train dataset\")\nplt.show()","11c8e1c1":"from statistics import mode \nclass KNN:\n    def __init__(self, n_neighbour = 10):\n        self.n_neighbour = n_neighbour\n        \n    def predict_class(self, X_train, y_train, X_test):\n        prediction = []\n        # for each instance in test set\n        for x_test_item in X_test:\n            pred = self.get_nieghbour(X_train, y_train, x_test_item)\n            prediction.append(pred)\n        return prediction\n\n    # finding (n_neighbour) nearest neighbours\n    def get_nieghbour(self, X_train, y_train, x_test_item):\n        dist_list = []\n        for i in range(len(X_train)):\n            X_train_item = X_train[i]\n            y_train_item = y_train[i]\n            dist = distance.euclidean(X_train_item, x_test_item)\n            dist_list.append((dist, y_train_item))\n\n        # sorting the dictionary based on distance\n        dist_list.sort(key=lambda dist_x: dist_x[0])\n        \n        # storing labels of n_neighbour after sorting\n        label_list = [ dist[1] for dist in dist_list[:self.n_neighbour]] \n        \n        # return most occured label\n        return max(set(label_list), key=label_list.count)    ","306e1480":"knn = KNN(n_neighbour = 10)\npred = knn.predict_class(X_train, y_train, X_test)\naccuracy_score(pred, y_test)","4791ba8f":"<center><h2> KMeans <\/h2><\/center>\n<br><br>","f37e3091":"## Thank you","28dd8062":"Also, according to the algorithm, the loss is:\n\n**Loss**: J(C<sub>1<\/sub>, C<sub>2<\/sub>,... C<sub>m<\/sub>, \u03bc<sub>1<\/sub>, \u03bc<sub>2<\/sub>,... \u03bc<sub>k<\/sub>) = 1\/m \u03a3( ||x<sup>i<\/sup> - \u03bc<sub>C<sup>i<\/sup><\/sub>|| )\n\n\nC<sub>1<\/sub>, C<sub>2<\/sub>, ..., C<sub>m<\/sub> is the cluster index of each data point x<sup>i<\/sup> <br>\n\u03bc<sub>1<\/sub>, \u03bc<sub>2<\/sub>, ..., \u03bc<sub>k<\/sub> is the clusters centers","5a88113e":"## Creating dataset","542a5145":"## References:\n\n1. [Develop k-Nearest Neighbors in Python From Scratch, Machine Learning Mastery](https:\/\/machinelearningmastery.com\/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch\/)\n\n2. [Find most frequent element in a list, Geeks for geeks](https:\/\/www.geeksforgeeks.org\/python-find-most-frequent-element-in-a-list\/)\n\n3. [DBSCAN, Wikipedia](https:\/\/en.wikipedia.org\/wiki\/DBSCAN)","1207eda9":"\n<center><h2> DBSCAN <\/h2><\/center>\n\n<br><br><br>\n\n`Denisty Based Spatial Clustering Applications with Noise` (DBSCAN) deals with forming clusters based on the points that closely packed together. It is based on the assumption that `clusters are dense regions seperated by regions of lower density`. \n\nIt is useful in finding `arbitary shaped clusters` and `sepearates outliers` too.\n\nParameters:\n\n1. **\u03b5** (Epsilon) - radius of neighbourhood regions\n2. **minPts** - minimum number of points required to form that dense region\n\nEach point is either a core point,a border point or an outlier.\n\n1. **Core point** : If you draw a circle of radius \u03b5, from a point, and the circle contains minPts number of points inside then the point is a core point.\n\n2. **Border point** : If you draw a circle of radius \u03b5, the circle contains less number of points then minPts but is reachable from core point.\n\n3. **Outlier** : If you draw a circle of radius \u03b5, and the circle and it does not contains minPts number of points, and is not within the reach of any core point, then it is an outlier. \n\nThe representation is shown in the following figure. `All the core points and border points will be merged to form a cluster.` \n\n*For more information, check this [youtube tutorial](https:\/\/www.youtube.com\/watch?v=6jl9KkmgDIw)* ","b3e33aef":"`\nRangeQuery(DB, distFunc, Q, eps) {\n    Neighbors N := empty list\n    for each point P in database DB {                      \/* Scan all points in the database *\/\n        if distFunc(Q, P) \u2264 eps then {                     \/* Compute distance and check epsilon *\/\n            N := N \u222a {P}                                   \/* Add to result *\/\n        }\n    }\n    return N\n}\n`","6ee6d924":"## K Nearest Neighbours\n","99f2055e":"![KMeans Algo](https:\/\/miro.medium.com\/max\/700\/1*7LuOOmBXcnbxm7AeD1xAQQ.png)\nSource: https:\/\/heartbeat.fritz.ai\/understanding-the-mathematics-behind-k-means-clustering-40e1d55e2f4c \n\n\nFrom the above algorithm, let's try to create our own K means algorithm.","7a796f15":"I am taking the algorithm mentioned in [Wikipedia DBScan](https:\/\/en.wikipedia.org\/wiki\/DBSCAN)\n\n`\nDBSCAN(DB, distFunc, eps, minPts) {\n    C := 0                                                  \/* Cluster counter *\/\n    for each point P in database DB {\n        if label(P) \u2260 undefined then continue               \/* Previously processed in inner loop *\/\n        Neighbors N := RangeQuery(DB, distFunc, P, eps)     \/* Find neighbors *\/\n        if |N| < minPts then {                              \/* Density check *\/\n            label(P) := Noise                               \/* Label as Noise *\/\n            continue\n        }\n        C := C + 1                                          \/* next cluster label *\/\n        label(P) := C                                       \/* Label initial point *\/\n        SeedSet S := N \\ {P}                                \/* Neighbors to expand *\/\n        for each point Q in S {                             \/* Process every seed point Q *\/\n            if label(Q) = Noise then label(Q) := C          \/* Change Noise to border point *\/\n            if label(Q) \u2260 undefined then continue           \/* Previously processed (e.g., border point) *\/\n            label(Q) := C                                   \/* Label neighbor *\/\n            Neighbors N := RangeQuery(DB, distFunc, Q, eps) \/* Find neighbors *\/\n            if |N| \u2265 minPts then {                          \/* Density check (if Q is a core point) *\/\n                S := S \u222a N                                  \/* Add new neighbors to seed set *\/\n            }\n        }\n    }\n}\n`","551c752d":"The algorithm is very simple and the general idea is:\n\n\n    For each point in training set (X_train):\n        1. If the point is not visited, find its neighbours\n        2. If neighbours < minPTS , label it noise and continue\n        3. Else , create a list and add all the neighbours to this list\n        4. For each point in the list (while list is not empty),\n            4.1. If the point is in noise, change its label as core \n            4.2. If it is not vistied, label it as core \n            4.2. Find its neighbours\n            4.2. If their neighbours count > minPTS \n            4.3. add them to the list to explore further \n\n","515baac3":"KNN is a simple algorithm and its based on: \n\n    For every instance in X_test \n        1. Find the n neighbour points from the training set, and store their label\n        2. The label which is repeated more number of times, return it","5f686ecd":"**Note**: \n\nThe plot is based on clsuter info, which stores cluster index and the points inside the cluster and not based on original dataset. We can do that too by changing\n\nplt.scatter(data[:,0], data[:,1], alpha = 0.5, label = key)\n\nto\n\nplt.scatter(X[:,0], X[:,1], alpha = 0.5, label = key)","830e00b6":"<center><h1> DBSCAN , K-Means Clustering and KNN Classifier <\/h1><\/center>\n\n<br><br>\nThis notebook implements KMeans, DBScan clustering and KNN classifier from scratch. Let's learn these algorithms and logic behind them.","27b3a8c6":"## Importing all the libraries"}}