{"cell_type":{"355d0e18":"code","2fc03303":"code","6f891e2c":"code","2d71d215":"code","aadd728b":"code","b74b1491":"code","e28f6258":"code","cf214a8d":"code","e8b3aed6":"code","00f5b5ea":"code","185b4d46":"code","b27555de":"code","42f43560":"code","ff93224b":"code","12f49e95":"markdown","b666b901":"markdown","d84af39c":"markdown","9cc5064d":"markdown","60607563":"markdown","931e8a91":"markdown","3eddc7a5":"markdown","09efc14d":"markdown","99245187":"markdown","1d0b6dd4":"markdown","25eaa24f":"markdown","ab9474b4":"markdown"},"source":{"355d0e18":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\n\n## Read Data\ndf=pd.read_csv('..\/input\/prescriber-info.csv')\ndf.head()","2fc03303":"print(df.shape)\n\n#here I will add my standard 3-5 things to get info","6f891e2c":"opioids=pd.read_csv('..\/input\/opioids.csv')\nname=opioids['Drug Name']\nimport re\nnew_name=name.apply(lambda x:re.sub(\"\\ |-\",\".\",str(x)))\ncolumns=df.columns\nAbandoned_variables = set(columns).intersection(set(new_name))\nKept_variable=[]\nfor each in columns:\n    if each in Abandoned_variables:\n        pass\n    else:\n        Kept_variable.append(each)","2d71d215":"df=df[Kept_variable]\nprint(df.shape)","aadd728b":"train,test = train_test_split(df,test_size=0.2,random_state=42)\nprint(train.shape)\nprint(test.shape)","b74b1491":"Categorical_columns=['Gender','State','Credentials','Specialty']\nfor col in Categorical_columns:\n    train[col]=pd.factorize(train[col], sort=True)[0]\n    test[col] =pd.factorize(test[col],sort=True)[0]","e28f6258":"features=train.iloc[:,1:245]\nfeatures.head()","cf214a8d":"import sklearn\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import VotingClassifier","e8b3aed6":"features=train.iloc[:,1:244]\ntarget = train['Opioid.Prescriber']\nName=[]\nAccuracy=[]\nmodel1=LogisticRegression(random_state=22,C=0.000000001,solver='liblinear',max_iter=200)\nmodel2=GaussianNB()\nmodel3=RandomForestClassifier(n_estimators=200,random_state=22)\nmodel4=GradientBoostingClassifier(n_estimators=200)\nmodel5=KNeighborsClassifier()\nmodel6=DecisionTreeClassifier()\nmodel7=LinearDiscriminantAnalysis()\nEnsembled_model=VotingClassifier(estimators=[('lr', model1), ('gn', model2), ('rf', model3),('gb',model4),('kn',model5),('dt',model6),('lda',model7)], voting='hard')\nfor model, label in zip([model1, model2, model3, model4,model5,model6,model7,Ensembled_model], ['Logistic Regression','Naive Bayes','Random Forest', 'Gradient Boosting','KNN','Decision Tree','LDA', 'Ensemble']):\n    scores = cross_val_score(model, features, target, cv=5, scoring='accuracy')\n    Accuracy.append(scores.mean())\n    Name.append(model.__class__.__name__)\n    print(\"Accuracy: %f of model %s\" % (scores.mean(),label))","00f5b5ea":"Name_2=[]\nAccuracy_2=[]\nEnsembled_model_2=VotingClassifier(estimators=[('rf', model3),('gb',model4)], voting='hard')\nfor model, label in zip([model3, model4,Ensembled_model_2], ['Random Forest', 'Gradient Boosting','Ensemble']):\n    scores = cross_val_score(model, features, target, cv=5, scoring='accuracy')\n    Accuracy_2.append(scores.mean())\n    Name_2.append(model.__class__.__name__)\n    print(\"Accuracy: %f of model %s\" % (scores.mean(),label))","185b4d46":"from sklearn.metrics import accuracy_score\nclassifers=[model3,model4,Ensembled_model_2]\nout_sample_accuracy=[]\nName_2=[]\nfor each in classifers:\n    fit=each.fit(features,target)\n    pred=fit.predict(test.iloc[:,1:244])\n    accuracy=accuracy_score(test['Opioid.Prescriber'],pred)\n    Name_2.append(each.__class__.__name__)\n    out_sample_accuracy.append(accuracy)","b27555de":"in_sample_accuracy=Accuracy_2","42f43560":"Index = [1,2,3]\nplt.bar(Index,in_sample_accuracy)\nplt.xticks(Index, Name_2,rotation=45)\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.title('In sample accuracy of models')\nplt.show()","ff93224b":"plt.bar(Index,out_sample_accuracy)\nplt.xticks(Index, Name_2,rotation=45)\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.title('Out sample accuracies of models')\nplt.show()","12f49e95":"## Fire up","b666b901":"**The hard-voting classifer does not provide a better result in terms of out sample error. Therefore, in this case, the voting classifer cannot reach the satisfied result. A soft voting method can be considered as a substitute.**","d84af39c":"## Try Different Classifiers","9cc5064d":"## Evaluating with the test set","60607563":"**We can see that the ensembled model putting two models together performed better than the other three models. I will further use the test set to compare the performances between models.**","931e8a91":"**Thanks for @ Alan (AJ) Pryor, Jr 's feedback! My method was so 'dirty' that I even included misleading variables in my model. Now I will modify my Kernel to a more completed, accurate one.**","3eddc7a5":"## Data Proprecessing","09efc14d":"**Even a quick and dirty classification model requires the transformation of non-numeric feature, in order to enable Python to identify the variable.**","99245187":"# Quickly trying hard voting classifier in this case ","1d0b6dd4":"**We have a high dimensional data set again. How can we create an effective predictive model quickily on such a complicated data-set?**","25eaa24f":"**For the model with the accuracy worse than 80%, we drop them and re_ensemble the model.**","ab9474b4":"## In-sample and out-sample evaluation"}}