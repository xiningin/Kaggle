{"cell_type":{"f6f6ef32":"code","b472bb0b":"code","10e4ce5f":"code","6f134870":"code","361ac4f8":"code","76bea636":"code","4396bc8d":"code","7acc8613":"code","9d0505bc":"code","cc10210e":"code","6ecd16c1":"code","6b9b993d":"code","e6fa2d7f":"code","a73cff62":"code","e57f4112":"code","6763fbb7":"code","37436837":"code","f5f7d926":"code","1a1e441a":"code","22bc3993":"code","6ed4cb99":"code","a4f7288b":"code","67a2b645":"code","b8067fe6":"code","4f3bcc83":"code","6a943996":"code","9cc40290":"code","3c76b87b":"code","77fadfd7":"code","34df9c06":"code","0154707f":"code","41cfbc3d":"code","9e10cf5d":"code","fd2a8960":"code","18065027":"code","5e749eed":"code","086abcdf":"code","1b29fba7":"code","6513b590":"code","ba7ea052":"code","a04c97ed":"code","03e25f75":"code","e1de42ab":"code","25fb3a51":"code","3c8772e0":"code","8794a0a0":"code","f80ba8a4":"code","aa34d1f2":"code","517cb3f4":"code","1fc035b6":"code","c35a903c":"code","4d719e3d":"code","020cd54a":"code","91497410":"code","36af5a0c":"code","4684f6cc":"code","953959e1":"code","d4abc2a9":"code","77a5e5d1":"markdown","d2dbaa13":"markdown","f74d1af8":"markdown","21ecf5b5":"markdown","31eadc7a":"markdown","dff78e38":"markdown","415271fe":"markdown","82e0178b":"markdown","e93a5036":"markdown","5df6035b":"markdown","6104733c":"markdown","a7d5867a":"markdown","fe03a624":"markdown","9aa3d84e":"markdown","3979599c":"markdown","b65db33b":"markdown","04819937":"markdown","7caa71bd":"markdown","9d92136c":"markdown","c26ffc9b":"markdown","f8f50eda":"markdown","a5b01fed":"markdown","7a2492ed":"markdown","a0e7d87f":"markdown","050b4a78":"markdown","11f9e07e":"markdown","292f6b53":"markdown","39d1a515":"markdown","dc6cba1c":"markdown","edfc3e1d":"markdown","b96e060d":"markdown","7851aebe":"markdown","c1f55621":"markdown","22f1ecbf":"markdown","f6ffd304":"markdown","e0ba1f3e":"markdown","bce01c72":"markdown","5d881876":"markdown","f1f16aca":"markdown","6f7fb19d":"markdown","f3c0ac43":"markdown","9871d5e3":"markdown","ee4fc395":"markdown","61710199":"markdown","d3bce2bb":"markdown","943c051b":"markdown","a7385dca":"markdown","3c9dc516":"markdown","10564a75":"markdown","78271fed":"markdown","457b2067":"markdown","c248355b":"markdown","4fc91839":"markdown"},"source":{"f6f6ef32":"!pip install chart-studio","b472bb0b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy import stats\n\nimport os\n\nfrom IPython.display import HTML\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Now it comes the part where we plot out plotly United States map\nfrom plotly import tools\nimport chart_studio.plotly as py\nimport plotly.figure_factory as ff\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.express as px\nimport warnings\ninit_notebook_mode(connected=True)\nwarnings.filterwarnings(\"ignore\")\nimport regex \n\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression, mutual_info_regression\nfrom sklearn.metrics import mutual_info_score, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom scipy.stats import reciprocal\nfrom sklearn.preprocessing import LabelEncoder\nsns.set()\nfrom sklearn.linear_model import Lasso, LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\n#-------------------------------------------------------------------------------\n\n\n# importa objetos de keras\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Input, Dense, Dropout\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom tensorflow.keras.metrics import RootMeanSquaredError\n\n# optimizador\nfrom tensorflow.keras.optimizers import Adam\n\n# Graphics in retina format are more sharp and legible\n%config InlineBackend.figure_format = 'retina'\n\n# Increase the default plot size and set the color scheme\nplt.rcParams[\"figure.figsize\"] = 8, 5\nplt.rcParams[\"image.cmap\"] = \"viridis\"\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","10e4ce5f":"calendar_boston = pd.read_csv(\"..\/input\/boston\/calendar.csv\")\nlistings_boston = pd.read_csv(\"..\/input\/boston\/listings.csv\")\nreviews_boston = pd.read_csv(\"..\/input\/boston\/reviews.csv\")\n\n#---------------------Data from Airbn Seatle\n\ncalendar_seatle = pd.read_csv(\"..\/input\/seattle\/calendar.csv\")\nlistings_seatle = pd.read_csv(\"..\/input\/seattle\/listings.csv\")\nreviews_seatle = pd.read_csv(\"..\/input\/seattle\/reviews.csv\")","6f134870":"###Dimension of datasets\n#------------------------Calendar\n\nprint(\"Dimension df for Calendar for Boston:\", calendar_boston.shape)\nprint(\"Dimension df for Calendar for Seatle:\", calendar_seatle.shape)\n#-------------------------Listings\n\nprint(\"Dimension df for Listings for Boston:\", listings_boston.shape)\nprint(\"Dimension df for Listings for Seatle:\", listings_seatle.shape)\n\n\n#---------------------------reviews\n\nprint(\"Dimension df for Reviews for Boston:\", calendar_boston.shape)\nprint(\"Dimension df for Reviews for Seatle:\", calendar_seatle.shape)","361ac4f8":"columns_lb= listings_boston.columns\ncolumns_ls = listings_seatle.columns\n\naux = []\n\nfor column in columns_lb:\n    if column not in columns_ls:\n        aux.append(column)\n\nprint(\"We don't have this information in Seatle data:\",aux)\n\n#We drop this variables.\n\nlistings_boston.drop(aux, axis=1,inplace=True)","76bea636":"def plot_null_values(df, ss):\n    '''\n    Input: Dataframe\n    Output: Plot for the percentage of null values in dataset\n    '''\n    df_aux = df.isnull().sum()\/df.shape[0]\n    df_aux.plot(kind='barh')\n    plt.title(\"%Null values in: \"+ ss)\n    plt.ylabel(\"Variable\")\n    plt.xlabel(\"% of null values\")\n    #plt.savefig(\"\/images\/\"+ss+'.png')\n    plt.show()\n\n    \nboston_dfs = { \"Calendar Boston\": calendar_boston, \n              \"Reviews Boston\": reviews_boston}\n\nseatle_dfs = { \"Calendar Seatle\": calendar_seatle,\n              \"Reviews Seatle\": reviews_seatle}\n\nfor name, df in boston_dfs.items():\n     plot_null_values(df, name)\n        \nfor name, df in seatle_dfs.items():\n     plot_null_values(df, name)","4396bc8d":"### Join the datasets\n\ncalendar_boston['city'] = \"Boston\"\ncalendar_seatle[\"city\"] = \"Seatle\"\ncalendar = pd.concat([calendar_boston, calendar_seatle], axis=0)\nprint(\"Calendar listings unique values.\", calendar.drop_duplicates(\"listing_id\").shape[0]\/calendar.shape[0])\n\n\nlistings_boston[\"city\"] = \"Boston\"\nlistings_seatle[\"city\"] = \"Seatle\"\nlistings = pd.concat([listings_boston, listings_seatle], axis=0)\nprint(\"Listings unique values.\", listings.drop_duplicates(\"id\").shape[0]\/listings.shape[0])\n\nreviews_boston[\"city\"] = \"Boston\"\nreviews_seatle[\"city\"] = \"Seatle\"\n\nreviews = pd.concat([reviews_boston, reviews_seatle],axis=0)\nprint(\"Reviews unique values:\", reviews.drop_duplicates(\"listing_id\").shape[0]\/ reviews.shape[0])","7acc8613":"#cleaning price variable.\n\nlistings.price = listings.price.apply(lambda x: x.split('.')[0]).replace('[^0-9]', '', regex=True).apply(lambda x: int(x))\n\nlistings.extra_people = listings.extra_people.apply(lambda x: x.split('.')[0]).replace('[^0-9]', '', regex=True).apply(lambda x: int(x))\n\nlistings.cleaning_fee = listings.cleaning_fee.fillna(\"$0\")\n\nlistings.cleaning_fee = listings.cleaning_fee.apply(lambda x: x.split('.')[0]).replace('[^0-9]', '', regex=True).apply(lambda x: int(x))\n\nlistings.weekly_price   = listings.weekly_price.fillna(\"$0\")\n\nlistings.weekly_price   = listings.weekly_price.apply(lambda x: x.split('.')[0]).replace('[^0-9]', '', regex=True).apply(lambda x: int(x))\n\nlistings.monthly_price    = listings.monthly_price.fillna(\"$0\")\n\nlistings.monthly_price  = listings.monthly_price.apply(lambda x: x.split('.')[0]).replace('[^0-9]', '', regex=True).apply(lambda x: int(x))\n\nlistings.security_deposit    = listings.security_deposit.fillna(\"$0\")\n\nlistings.security_deposit   = listings.security_deposit.apply(lambda x: x.split('.')[0]).replace('[^0-9]', '', regex=True).apply(lambda x: int(x))\n\n\ndates = [\"calendar_last_scraped\", \"last_scraped\", \"host_since\"]\nfor col_date in dates:\n    \n    listings[col_date] = pd.to_datetime(listings[col_date]) #good format for date variable\n\nlistings.reset_index(drop=True, inplace=True)","9d0505bc":"listings.describe().iloc[:,20:30]","cc10210e":"sns.heatmap(listings.iloc[:, 0:25].isnull(), cbar=False )\nsns.heatmap(listings.iloc[:, 25:50].isnull(), cbar=False)\nsns.heatmap(listings.iloc[:, 50:75].isnull(), cbar=False )\nsns.heatmap(listings.iloc[:, 75:95].isnull(), cbar=False)","6ecd16c1":"dic_null_values = {}\n\nfor col in listings.columns:\n    pcte = listings[col].isnull().sum()\/listings.shape[0]\n    dic_null_values[col] = pcte\n\ncols_drop= []\n\nfor col, pcte in dic_null_values.items():\n    if pcte > 0.7:\n        cols_drop.append(col)\n        \n\nlistings.drop(cols_drop, axis=1, inplace=True)\n\n\n# Detecting cardinality and features with a lot of same values\n\n#We have that the following features have a big variance: id, scrape_id, hot_id. So we will drop it.\n\nlistings.drop([\"id\", \"scrape_id\", \"host_id\"],axis=1, inplace=True)\n\n\nprint(\"New dimension of our dataset:\", listings.shape)","6b9b993d":"###Using the methods of sklearn for preprocessing data\n\nsel = VarianceThreshold(threshold=0.01)\nsel.fit(listings.select_dtypes(exclude=[\"object\", \"datetime64\"])) \n\n# alternate way of finding non-constant features\nprint(\"number of non-constant features:\",len(listings.select_dtypes(exclude=[\"object\", \"datetime\"]).columns[sel.get_support()]))","e6fa2d7f":"price = listings['price'].values.tolist()\nboston_price = listings['price'].loc[listings['city'] == 'Boston'].values.tolist()\nseatle_price = listings['price'].loc[listings['city'] == 'Seatle'].values.tolist()\n\ntrace0 = go.Histogram(\n    x=boston_price,\n    histnorm='probability',\n    name=\"Boston Prices\",\n    marker = dict(\n        color = 'rgba(100, 149, 237, 0.6)',\n    )\n)\ntrace1 = go.Histogram(\n    x= seatle_price,\n    histnorm='probability',\n    name=\"Seatle prices\",\n    marker = dict(\n        color = 'rgba(255, 182, 193, 0.6)',\n    )\n)\ntrace2 = go.Histogram(\n    x=price,\n    histnorm='probability',\n    name=\"Prices distribution\",\n     marker = dict(\n        color = 'rgba(169, 169, 169, 0.6)',\n    )\n)\nfig = tools.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n                          subplot_titles=('Boston','Seatle', 'Two cities'))\n\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\n\nfig['layout'].update(showlegend=True, title='Distribution of Prices of Airbnbs', bargap=0.05)\niplot(fig, filename='custom-sized-subplot-with-subplot-titles')\n\nprint(\"Null values:\", listings.price.isnull().sum())\n","a73cff62":"fig = px.box(listings, x=\"city\", y=\"price\", color= 'accommodates')\nfig.update_traces(quartilemethod=\"exclusive\") # or \"inclusive\", or \"linear\" by default\nfig.show()","e57f4112":"sns.displot(listings, x=\"number_of_reviews\", hue=\"city\", kind=\"kde\", height=5, aspect=3,)\nplt.title(\"Distribution of Number reviews \", fontsize=16);\nprint(\"null  of  numbers scores:\", listings.number_of_reviews.isnull().sum())\n\nsns.displot(listings, x=\"review_scores_accuracy\", hue=\"city\", kind=\"kde\", height=5, aspect=3, )\nplt.title(\"Distribution of  score Accuracy review \", fontsize=16);\nprint(\"null  of  score reviews:\", listings.review_scores_accuracy.isnull().sum())\n\nsns.displot(listings, x=\"review_scores_rating\", hue=\"city\", kind=\"kde\", height=5, aspect=3, )\nplt.title(\"Distribution of  score ratting \", fontsize=16);\nprint(\"null  of  score rating reviews:\", listings.review_scores_rating.isnull().sum())\n\nsns.displot(listings, x=\"review_scores_cleanliness\", hue=\"city\", kind=\"kde\", height=5, aspect=3, )\nplt.title(\"Distribution of  score Cleanliness review \", fontsize=16);\nprint(\"null  of  score cleanlinnes reviews:\", listings.review_scores_cleanliness.isnull().sum())\n\nsns.displot(listings, x=\"review_scores_checkin\", hue=\"city\", kind=\"kde\", height=5, aspect=3,)\nplt.title(\"Distribution of  score Checking review \", fontsize=16);\nprint(\"null  of  score checkin reviews:\", listings.review_scores_checkin.isnull().sum())\n\nsns.displot(listings, x=\"review_scores_communication\", hue=\"city\", kind=\"kde\", height=5, aspect=3,)\nplt.title(\"Distribution of  score Communication review \", fontsize=16);\nprint(\"null  of  score communication reviews:\", listings.review_scores_communication.isnull().sum())\n\n\nsns.displot(listings, x=\"review_scores_location\", hue=\"city\", kind=\"kde\", height=5, aspect=3, )\nplt.title(\"Distribution of  score Location review \", fontsize=16);\nprint(\"null  of  score location reviews:\", listings.review_scores_location.isnull().sum())\n\n\nsns.displot(listings, x=\"review_scores_value\", hue=\"city\", kind=\"kde\", height=5, aspect=3, )\nplt.title(\"Distribution of  Value score review \", fontsize=16);\nprint(\"null  of  score value reviews:\", listings.review_scores_value.isnull().sum())","6763fbb7":"f = plt.figure(figsize= (15,15))\nsns.set_context(\"paper\", font_scale=1.)\nsns.heatmap(listings.corr(),  annot=True,  linewidths=.3, fmt= '.1f')\nplt.title(\"Corration values for feaures\"); ","37436837":"corr_matrix = listings.corr().abs()\nhigh_corr_var=np.where(corr_matrix>0.7)\nhigh_corr_var=[(corr_matrix.columns[x],corr_matrix.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y]\n\nprint(high_corr_var)\n\ndrop_corr_var = []\nfor num in enumerate(high_corr_var):\n   drop_corr_var.append(num[1][1])\n\ndrop_corr_var.remove(\"longitude\")\ndrop_corr_var.append(\"neighbourhood\")\n\nlistings.drop(drop_corr_var, axis=1, inplace=True)\nprint(\"        \")\nprint(\"The new shape of listings dataset is:\", listings.shape)","f5f7d926":" px.violin(listings, y=\"price\", x=\"property_type\", color=\"city\", box=True, title=\"Price and type of Airbnb\")","1a1e441a":"neighbourhood_cities= listings.groupby([\"city\", \"neighbourhood_cleansed\"]).agg({\"listing_url\":\"count\"}).rename(columns={\"listing_url\":\"count\"}).reset_index()\nfig = px.bar(neighbourhood_cities, x=\"neighbourhood_cleansed\", y=\"count\", color=\"city\", title=\"Number of Airbnb per neighbourhood in each city\")\nfig.show()","22bc3993":"fig = px.strip(listings, y=\"price\", x=\"neighbourhood_cleansed\", color=\"city\", \n           title=\"Price distribution per neighbourhood\")\n\nfig.show()","6ed4cb99":"fig = px.scatter_mapbox(listings, lat=\"latitude\", lon=\"longitude\", color='review_scores_rating', zoom=3, mapbox_style='open-street-map')\nfig.show()","a4f7288b":"sns.set_context(\"paper\", font_scale=3)\nsns.displot(data=listings, x=\"price\", hue=\"room_type\", linewidth = 4.5,row=\"city\", kind=\"kde\", height=10, aspect=3.5 )\nplt.title(\"Price distribution for room type city: Seatle\", fontsize=30);","67a2b645":"sns.set_context(\"paper\", font_scale=3)\nsns.displot(data=listings, x=\"price\", hue=\"bed_type\", row=\"city\", linewidth = 5.5, kind=\"kde\", height=10, aspect=3.5, )\nplt.title(\"Price distribution for bed type: Seatle\", fontsize=30);\n\nsns.set_context(\"paper\", font_scale=3)\nsns.displot(data=listings[listings.bed_type!=\"Real Bed\"], x=\"price\", linewidth = 5.5,hue=\"bed_type\", row=\"city\", kind=\"kde\", height=10, aspect=3.5, )\nplt.title(\"Price distribution for bed type: Seatle\", fontsize=30);","b8067fe6":"f = plt.figure(figsize= (15,5))\nsns.set_context(\"paper\", font_scale=1.5)\nsns.heatmap(listings.query('price <= 4000')\\\n                .groupby(['bathrooms', 'accommodates'])\\\n                .mean()['price']\\\n                .reset_index()\\\n                .pivot('bathrooms', 'accommodates', 'price')\\\n                .sort_index(ascending=False),\n            cmap=\"Greens\", fmt='.0f', annot=True, linewidths=0.5,)","4f3bcc83":"def z_score(df,var, threshold =3):\n    \n    '''Function for remove outliers with the definition of z score value\n    Input: dataframe, variable and you can modify the threshold\n    Out: Dataframe without outliers'''\n    \n    z = np.abs(stats.zscore(df[var]))\n    df[\"zscore\"] = z\n    df = df[(df.zscore>-  threshold) & (df.zscore< threshold)]\n    df.drop(\"zscore\", axis=1, inplace=True)\n    \n    return df\n\nlistings_copy = listings.copy()\nlistings = z_score(listings, \"price\")\n\nprint(\"Number of outliers:\", listings_copy.shape[0]-listings.shape[0])","6a943996":"#We are going to create variables from dates variables:\n#this columns seems very unusefull:\nprint(listings.last_scraped.value_counts())\nlistings[\"age_host\"] = (listings['calendar_last_scraped'] -listings['host_since']).dt.days\nlistings.drop(dates, axis=1,inplace=True)\nlistings = listings[(listings.property_type.notna()) & (listings.age_host.notna())]","9cc40290":"# Prepare the Imputation Objects\n\n\nlistings[\"host_response_rate\"]  = listings[\"host_response_rate\"].replace('[%]', '', regex=True).apply(lambda x: float(x))\nlistings[\"host_acceptance_rate\"]  = listings[\"host_acceptance_rate\"].replace('[%]', '', regex=True).apply(lambda x: float(x))\n\n\nmedian_impute = SimpleImputer(strategy = 'median')\nmode_impute = SimpleImputer(strategy = 'most_frequent')\nmean_impute = SimpleImputer(strategy = 'mean')\n\ndef apply_imputation(impute_object, column):\n    \n    '''Function that applies the imputation to the desired column.\n    Returns the values for train and test.'''\n\n    imputed = impute_object.fit_transform(X = listings[[column]])\n    \n    return imputed\n\n\nlistings[\"bathrooms\"]  = apply_imputation(mean_impute,\"bathrooms\")  \n#listings[\"beds\"]  = apply_imputation(mean_impute,\"beds\")  \n#listings[\"bedrooms\"]  = apply_imputation(mean_impute,\"bedrooms\")  \nlistings[\"reviews_per_month\"]   = apply_imputation(mean_impute,\"reviews_per_month\")\n\nvar_reviews = [\"review_scores_rating\",\"review_scores_accuracy\",\"review_scores_checkin\",\n              \"review_scores_location\", \"review_scores_communication\",\n              \"host_response_time\",\"host_response_rate\",\"host_acceptance_rate\" ]\n\nfor rev in var_reviews:\n    \n        listings[rev]   = listings[rev].fillna(0)\n        \n        \ncategorical_aux =  [ \"has_availability\", \"host_has_profile_pic\", \"host_identity_verified\",\n                    \"is_location_exact\", \"host_is_superhost\", \"require_guest_profile_picture\",\n                   \"require_guest_phone_verification\", \"instant_bookable\", ]\n\nfor cat in categorical_aux:\n    \n    listings[cat]= listings[cat].apply(lambda x: 0 if x==\"f\" else 1)\n\n\n","3c76b87b":"X =  listings.drop(\"price\", axis=1)\ny = listings[[\"price\"]]\n\nX = X.select_dtypes(exclude= [\"object\", \"datetime\"])\n#We eliminate variables that are not very useful for prediction \n#or those that have information about our target variable such as: \"security_deposit\", \"monthly_price\", \"weekly_price\", \nX = X.drop([\"security_deposit\", \"monthly_price\", \"weekly_price\", \"latitude\", \"longitude\", \"availability_30\", \"availability_365\"],axis=1)","77fadfd7":" #-----------------Now we explore feature selecction with mutual information score.\n\ndef calc_MI(x, y):\n    \n    scores = []\n    for col in x.columns:\n        r = np.corrcoef(X[\"maximum_nights\"].values, y.values.flatten())[0,1]\n        bins = int(np.round((1\/np.sqrt(2))*pow(1+np.sqrt( 1+ (24*X.shape[0])\/(1-r*r)), 0.5)))\n        c_xy = np.histogram2d(x[col].values, y.values.flatten(), bins=bins)[0]\n        mi = mutual_info_score(None, None, contingency=c_xy)\n        scores.append(mi)\n        \n    mi_scores = pd.Series(scores, name=\"MI Scores\", index=x.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return  mi_scores\n\n####How select the number of bins?\n\nmi_scores = calc_MI(X, y)\nprint(\"-----------------------------------------\")\nprint(\"Mutual information scores:\")\nprint( mi_scores[::3])  # show a few features with their MI scores\n\n#Now we are going to estimate the best number of features \n\n","34df9c06":"#Recursive feature elimination\nn_splits = 10\nto_try = X \n\nmean_drop_scores = []\nstd_drop_scores = []\n\nfor feat in  mi_scores.sort_values().index[0:-1]: # we strat to drop the less importarnt features\n    \n    to_try = to_try.drop(str(feat), axis=1)\n    X_ = to_try.values\n    y_ = y.values\n    \n    rf = RandomForestRegressor() #baseline model\n    stk = StratifiedKFold(n_splits=n_splits, random_state=0)\n    scores = []\n    for train_idx, test_idx in stk.split(X_, y_):\n        x_train, x_test = X_[train_idx], X_[test_idx]\n        y_train, y_test = y_[train_idx], y_[test_idx]\n        \n        rf.fit(x_train, y_train)\n        proba = rf.predict(x_test)\n        scores.append(mean_squared_error(y_test.flatten(), proba))\n    \n        \n    mean_score = np.mean(scores)\n    std_score = np.std(scores)\n    mean_drop_scores.append(mean_score)\n    std_drop_scores.append(std_score)","0154707f":"plt.figure(figsize=(20,5))\nplt.plot(np.arange(0, mi_scores.shape[0]-1), mean_drop_scores, 'o-', color=\"red\")\nplt.fill_between(np.arange(0, mi_scores.shape[0]-1),\n                 mean_drop_scores-np.array(std_drop_scores),\n                 mean_drop_scores+np.array(std_drop_scores), color=\"tomato\", alpha=0.2)\n\nplt.title(\"Scores per number of features used in the baseline model\")\nplt.ylabel(\"MSE\")","41cfbc3d":"var_noimportant = list(mi_scores.index[14:])\nprint(var_noimportant)\nvar_noimportant = var_noimportant+[\"security_deposit\", \"monthly_price\", \"weekly_price\", \"latitude\", \"longitude\", \"availability_30\", \"availability_365\"]","9e10cf5d":"\nlistings['amenities'] = listings['amenities'].map( lambda vars: \",\".join([var.replace(\"}\", \"\").replace(\"{\", \"\").replace('\"', \"\")\\\n                           for var in vars.split(\",\")]))\n\nlistings['host_verifications'] = listings['host_verifications'].map( lambda vars: \",\".join([var.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\\\n \n                                                                                            for var in vars.split(\",\")]))\n#------------------------------------------------------------------------------------------------------------------------------------------------------\ncols_amenities = np.concatenate(listings['amenities'].map(lambda amns: amns.split(\",\")).values)\n\ncols_host_verifications = np.concatenate(listings['host_verifications'].map(lambda amns: amns.split(\",\")).values)\n\n\nfor col in cols_amenities:\n    listings[col] = 0\n    \nfor col in cols_host_verifications:\n    listings[col] = 0\n    \n    \nfor col in cols_amenities:\n    \n    listings[col] = listings[\"amenities\"].apply(lambda x: 1 if col in x else 0) \n    \nfor col in cols_host_verifications:\n    \n    listings[col] = listings[\"host_verifications\"].apply(lambda x: 1 if col in x else 0) ","fd2a8960":"drop_var = [\"neighbourhood_cleansed\",\"requires_license\",\"name\", \"summary\", \"space\",\"description\", \"neighborhood_overview\",\n               \"notes\", \"transit\", \"host_about\",\"calendar_updated\", \"neighbourhood_group_cleansed\",\"listing_url\", \"experiences_offered\", \"thumbnail_url\", \"medium_url\", \"picture_url\", \n            \"xl_picture_url\", \"host_url\", \"host_name\", \"host_location\", \n            \"host_thumbnail_url\", \"host_picture_url\", \"host_neighbourhood\", \"host_verifications\", \"street\",\n           \"state\", \"zipcode\", \"market\", \"smart_location\", \"country_code\", \"country\", \"latitude\", \"longitude\",\n           \"amenities\", \"first_review\", \"last_review\",  \"availability_30\", \"availability_365\"]\n\n\ndrop_var = drop_var + var_noimportant\nlistings.drop(drop_var, axis=1, inplace=True)\n\nprint(\"New dimension of Dataframe:\", listings.shape)","18065027":"listings_  = listings.copy()","5e749eed":"listings = pd.get_dummies(listings,drop_first=True)","086abcdf":"\nvar_cat_1 = list(listings.iloc[:,15:].columns)\n\nfor var in var_cat_1:\n    \n  listings[var] = listings[var].astype(int) \n\nprint(\"We have {} categorical features\".format(listings.select_dtypes(include=\"int\").shape[1]))\n\ncat_df = listings.select_dtypes(include=\"int\")\n\n#label_encoder = LabelEncoder()\n\n#for var in cat_df.columns:\n#    cat_df[var]= label_encoder.fit_transform(cat_df[var])\n\ncat_df[\"price\"] = listings[\"price\"]\n\ndef train_test_split_scaled(df, target):\n\n    scaler = StandardScaler()\n\n    X_train, X_test, y_train, y_test = train_test_split(\n                                                    df.drop(labels=[target], axis=1),\n                                                    df[target],\n                                                    test_size=0.3,\n                                                    random_state=0)\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    return  X_train, X_test, y_train, y_test\n\n\nX_train, X_test, y_train, y_test = train_test_split_scaled(cat_df, \"price\")\n\nsel_ = SelectFromModel(Lasso(alpha=1))\nsel_.fit(X_train, y_train)\n\nlasso_model = Lasso(alpha=1)\nlasso_model.fit(X_train, y_train)\n\ny_pred = lasso_model.predict(X_test)\n\nprint(\"The rmse score for this simple model and all categorical features is:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n\nselected_feat = cat_df.drop(\"price\", axis=1).columns[(sel_.get_support())]\n\nprint(\"------------------------------------------------------\")\n\n\nprint(\"------------------------------------------------------\")\n\n\nprint('total features: {}'.format((X_train.shape[1])))\nprint('selected features: {}'.format(len(selected_feat)))\nprint('features with coefficients shrank to zero: {}'.format(\n      np.sum(sel_.estimator_.coef_ == 0)))\n\nremoved_feats = cat_df.drop(\"price\", axis=1).columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\n\n\nlistings = listings.drop(removed_feats, axis=1)\nprint(\"------------------------------------------------------\")\n\n\n\nprint(\"------------------------------------------------------\")\n\n\n","1b29fba7":"listings.to_csv(\".\/pre_procesing_data.csv\", index=False)","6513b590":"listings.head(1)","ba7ea052":"listings[\"weighted_score\"] =  (listings['reviews_per_month'] * listings['review_scores_rating']) \/ 10\n\nlistings[\"weighted_score\"].fillna(0, inplace=True)\n\n#How to choose the threshoold for low Listings?\nfig = px.box(listings, y=\"weighted_score\")\nfig.show()","a04c97ed":"top90flag = listings[\"weighted_score\"].quantile(0.9)\nupto25flag = listings[\"weighted_score\"].quantile(0.25)\n\nlistings['top90'] = listings[\"weighted_score\"] >= top90flag\nlistings['upto25'] = listings[\"weighted_score\"]<= upto25flag\n\nprint('The boundaries of top performer listings:',top90flag)\nprint('The boundaries of low performer listings:',upto25flag)\n\ndef rangeScore(x):\n    '''\n    Set the bins for the score-range.\n    '''\n    value = ''\n    if (x>= 0 and x < 10):\n        value = '0-10'\n    elif (x>= 10 and x < 20):\n        value = '10-20'\n    elif (x>= 20 and x < 30):\n        value = '20-30'\n    elif (x>= 30.0 and x < 40.0):\n        value = '30-40'\n    elif (x>= 40 and x < 50):\n        value = '40-50'\n    elif (x>= 50 and x < 60):\n        value = '50-60'\n    elif (x>= 60 and x < 70):\n        value = '60-70'        \n    elif (x>= 70 and x < 80):\n        value = '70-80'\n    elif (x>= 80 and x < 90):\n        value = '80-90'\n    elif (x>= 90 and x < 100):\n        value = '90-100'\n    elif x>= 100:\n        value = '100+'\n        \n    return value\n\n\ndef gen_xaxis(title):\n    \"\"\"\n    Creates the X Axis layout and title\n    \"\"\"\n    xaxis = dict(\n            title=title,\n            titlefont=dict(\n                color='#AAAAAA'\n            ),\n            showgrid=False,\n            color='#AAAAAA',\n            )\n    return xaxis\n\n\ndef gen_yaxis(title):\n    \"\"\"\n    Creates the Y Axis layout and title\n    \"\"\"\n    yaxis=dict(\n            title=title,\n            titlefont=dict(\n                color='#AAAAAA'\n            ),\n            showgrid=False,\n            color='#AAAAAA',\n            )\n    return yaxis\n\n\ndef gen_layout(charttitle, xtitle, ytitle, lmarg, h, annotations=None):  \n    \"\"\"\n    Creates whole layout, with both axis, annotations, size and margin\n    \"\"\"\n    return go.Layout(title=charttitle, \n                     height=h, \n                     width=800,\n                     showlegend=False,\n                     xaxis=gen_xaxis(xtitle), \n                     yaxis=gen_yaxis(ytitle),\n                     annotations = annotations,\n                     margin=dict(l=lmarg),\n                    )\n\n\n\ndef gen_bars(data, color, orient):\n    \"\"\"\n    Generates the bars for plotting, with their color and orient\n    \"\"\"\n    bars = []\n    for label, label_df in data.groupby(color):\n        if orient == 'h':\n            label_df = label_df.sort_values(by='x', ascending=True)\n        if label == 'a':\n            label = 'lightgray'\n        bars.append(go.Bar(x=label_df.x,\n                           y=label_df.y,\n                           name=label,\n                           marker={'color': label},\n                           orientation = orient\n                          )\n                   )\n    return bars\n\n\ndef gen_annotations(annot):\n    \"\"\"\n    Generates annotations to insert in the chart\n    \"\"\"\n    if annot is None:\n        return []\n    \n    annotations = []\n    # Adding labels\n    for d in annot:\n        annotations.append(dict(xref='paper', x=d['x'], y=d['y'],\n                           xanchor='left', yanchor='bottom',\n                           text= d['text'],\n                           font=dict(size=13,\n                           color=d['color']),\n                           showarrow=False))\n    return annotations\n\n\ndef generate_barplot(text, annot_dict, orient='v', lmarg=120, h=400):\n    \"\"\"\n    Generate the barplot with all data, using previous helper functions\n    \"\"\"\n    layout = gen_layout(text[0], text[1], text[2], lmarg, h, gen_annotations(annot_dict))\n    fig = go.Figure(data=gen_bars(barplot, 'color', orient=orient), layout=layout)\n    return iplot(fig)","03e25f75":"\nlistings['score_ranges'] = listings[\"weighted_score\"].apply(rangeScore)\n\n# table coloring purpose.\ntop90 = listings.groupby('score_ranges', as_index = False)['top90'].max().rename(columns={'score_ranges':'Score'})\nupto25 = listings.groupby('score_ranges', as_index = False)['upto25'].max( ).rename(columns={'score_ranges':'Score'})\n\n# count distributions of score bins.\nbarplot = listings.reset_index()[['index',\"weighted_score\"]]\nbarplot['Qty'] = barplot[\"weighted_score\"].apply(rangeScore)\nbarplot = barplot.Qty.value_counts(sort=True).to_frame().reset_index()\nbarplot = barplot.rename(columns={'index': 'Score'})\n\n# merging color flag.\nbarplot = barplot.merge(top90, on = 'Score')\nbarplot = barplot.merge(upto25)\n# creating color for the vis.\nbarplot['color'] = barplot.top90.apply(lambda x: 'mediumaquamarine' if x else 'lightgray')\n# manually change the color of the first index become crimson, to indicate the class of low performer listings.\nbarplot.iloc[0,4] = 'crimson'\n\n# change Score column and Qty column into x and y for the vis purpose.\nbarplot = barplot.rename(columns={'Score':'x','Qty':'y'})\n\n# Some of the annotations for the vis.\ntitle_text = ['<b>Comparison Listings Performance between Top and Low<\/b>', 'Reviews per Month x Review Score Ratings \/ 10', 'Quantity of Listings']\nannotations = [{'x': 0.03, 'y': 4000, 'text': 'Low Performer Had Score Up to 25 Percentile','color': 'gray'},\n              {'x': 0.39, 'y': 300, 'text': 'Top Performer Had Score above 90 Percentile','color': 'mediumaquamarine'}]\n\ngenerate_barplot(title_text, annotations)","e1de42ab":"var = [\"number_of_reviews\",     \n        \"review_scores_accuracy\",   \n        \"review_scores_rating\",\"review_scores_location\"]\n\nlistings_aux = listings.drop(var, axis=1)","25fb3a51":"corr = listings_aux.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(21, 19))\n\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","3c8772e0":"aux= listings_aux.corr().unstack().sort_values(ascending=False)\nprint('The correlation of the new_score_reviews against all:', aux['weighted_score'][1:10])","8794a0a0":"def groups(x,y):\n    if x==True:\n        return \"Top Scores\"\n    if y==True:\n        return \"Worst Scores\"\n    \nlistings_aux[\"group\"] = listings_aux.apply(lambda x: groups(x[\"top90\"], x[\"upto25\"]) ,axis=1)\n\nsns.set_context(\"paper\", font_scale=3.4)\nsns.displot(data=listings_aux, x=\"host_acceptance_rate\", hue=\"group\", linewidth = 5.5, kind=\"kde\", height=10, aspect=3.5 )\nplt.title(\"Host acceptance rate distribution\", fontsize=30)\n\nsns.displot(data=listings_aux, x=\"host_response_rate\", hue=\"group\", linewidth = 5.5, kind=\"kde\", height=10, aspect=3.5)\nplt.title(\"Host response rate distribution\", fontsize=30);","f80ba8a4":"table_insights = listings_aux.drop([\"weighted_score\", \"top90\", \"reviews_per_month\",\"host_response_rate\",\"host_acceptance_rate\"  ],axis=1).groupby(\"group\").mean()\n\ntable_insights","aa34d1f2":"for var in table_insights.columns:\n    \n    if table_insights[var][0] > table_insights[var][1]:\n    \n        table_insights[var] = np.abs((table_insights[var][0]-table_insights[var][1])\/table_insights[var].max())\n    else:\n        \n        table_insights.drop(var, axis=1, inplace=True)\n    \n\nfor var in table_insights.columns:\n     \n     if table_insights[var][0]  >= 0.2:\n            table_insights[var]= table_insights[var]\n     else:\n            table_insights.drop(var, axis=1, inplace=True)","517cb3f4":"table_insights = table_insights.reset_index().drop(\"group\",axis=1).iloc[:1,:]\ntable_insights.stack()","1fc035b6":"listings = pd.get_dummies(listings, drop_first=True)","c35a903c":"print(\"Dimesion of dataset:\",listings.shape)","4d719e3d":"#scaling \nX = listings.drop([\"price\"], axis=1)\ny = listings[[\"price\"]]\n\nsc = StandardScaler()\n\n\nX_train, X_test_, y_train, y_test_ = train_test_split(X, y, test_size=0.35, random_state=42)\n\n#We also need a validation set for our future ANN\n\nX_test, X_valid, y_test, y_valid = train_test_split(X_test_, y_test_, test_size=0.7, random_state=42)\n\n\n#Scaler in action!\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nX_valid = sc.transform(X_valid)","020cd54a":"def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, dropout=0.15,input_shape=[X_train.shape[1]]):\n    \n \"\"\"\" \n      Simple Neural Net for regression task (output is one dimensional)  \n      Inputs: n_hidden = Amount of Layers of ANN\n              n_neurons = Amount of neurons of each layer\n              learning_rate = step for gradient descent algorithm\n              dropout = regularization term for avoid overfitting\n              input_shape = Number of features \n \"\"\"\n\n model = keras.models.Sequential()\n model.add(keras.layers.InputLayer(input_shape=input_shape))\n    \n for layer in range(n_hidden):\n    \n     print(\"Layer number:\", layer)\n     print(n_neurons)\n    \n     model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n    \n    \n model.add(Dropout(dropout))\n model.add(keras.layers.Dense(1))\n optimizer = keras.optimizers.Adam(learning_rate= learning_rate)\n model.compile(loss=\"mse\", optimizer=optimizer,  metrics=[RootMeanSquaredError(name='rmse')])\n\n return model","91497410":"keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n\n#Set the parameters to build our ANN \n\nparam_distribs = {\n \"input_shape\": [X_train.shape[1]],\n \"n_hidden\": [2,3,5,15,25,35,45 ],\n \"n_neurons\": [3,5, 15,30,45],\n}\n\nrnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n\n\nmodel_rnd_cv = rnd_search_cv.fit(X_train, y_train, epochs=200, validation_data=(X_valid, y_valid),\n                  callbacks=[keras.callbacks.EarlyStopping(patience=20)], verbose=0)","36af5a0c":"print(\"Best parameters =\", rnd_search_cv.best_params_)\n\nmodel = rnd_search_cv.best_estimator_.model\n","4684f6cc":"history = model.fit(X_train, y_train, epochs=200, validation_data=(X_valid, y_valid),\n                  callbacks=[keras.callbacks.EarlyStopping(patience=15)], verbose=1)\n\n\n\nf = plt.figure(figsize= (10,5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.title(\"Loss function for train and validation set\")\nplt.show()","953959e1":"y_pred = model.predict(X_test)\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"RMSE in test:\", rmse_test)\n\n#Visualize model prediction\ndef model_prediction_plot(y_test, pred, name_test, name_pred, Title):\n    \n    plt.figure(figsize=(10, 8))\n    ax1 = sns.distplot(y_test, hist=False, color=\"r\", label=name_test)\n    ax2 = sns.distplot(pred, hist=False, color=\"b\", label=name_pred, ax=ax1)\n    plt.title(Title)\n    plt.xlabel('Price (dollars)')\n    plt.legend()\n    plt.show()\n    plt.close()\nmodel_prediction_plot(y_test, y_pred, 'Actual Values', 'Predicted Values', \n                 'Distribution  Plot of  Predicted Value vs Test Data Distribution')","d4abc2a9":"listings.to_csv(\".\/pre_procesing_data.csv\", index=False)","77a5e5d1":"# **\ud83d\udcca Results: Loss for ANN and Predicctions**","d2dbaa13":"* The most expensives Airbnbs in Boston are in Downtwon and South Boston Waterfront.","f74d1af8":"## **Final Predicction in test set**","21ecf5b5":"**Load the Datasets**\n\nWe are going to use the Airbnb data from to locations: Seatle and Boston","31eadc7a":"In this Notebook we are going to use only Listings Datasets, but in future works like a recommendation machine I will also use Reviews dataset. So, this dataset has 30 numerical variables:","dff78e38":"We are going to delete unuseful features, with a high percentage of null values, with a lot of same values and features with high cardinality.","415271fe":"# References\n\n[1]. Pankaj Mehta, Ching-Hao Wang, Alexandre G. R. Day, and Clint Richardson, A high-bias, low-variance introduction to Machine Learning for physicists. 2019\n\n[2]. F. Marquardt, Machine Learning and Quantum Devices. 2021.","82e0178b":"From the figure above we can see that the error increases as the number of variables to train the model increases. The inflection point can be seen at point **14**. Meaning that the first **14** features in the mi_scores list will be used to do the final model. We are going to create new variables from the following two:  host_verification, ameneties. To do this we must extract the information and vectorize it.","e93a5036":"**Here we are going to predict the prices of different option of Airbnbs, fot this task we are going to create a Neural Net!!**, so we will drop some features unuseful for the prediction.","5df6035b":"**Imputation**","6104733c":"Ok we can choose the thresholds for each group with the quantile values, the good listings group has high values.","a7d5867a":"## **The Map and Review Score for each Airbnb**","fe03a624":"**Prepare de data for Neural Net**","9aa3d84e":"We have 3 datasets and we can infer that the calendar dataset contains information about the reservation of each available Airbnbn (7403). Listings dataset has a specific description of each Airbnb, so we have information about the neighborhood, scores, price, number of bedrooms, and others. **I believe the Calendar dataset is not very useful**.","3979599c":"* We have found that some of the variables related to the score have practically equal distributions, we should study this further and eliminate some of these variables. ","b65db33b":"**It is clear that in order to improve, the response rate must be improved for hosts**","04819937":"Airbnb is a somewhat recent company that offers vacation rental services. Additionally, it offers coverage of 193 countries with more than 2 million properties and is perhaps the most popular and used. It is a safe application, at least I speak from my experience, yes, I have used it too. Of all the times I have used it I have only had one bad experience and that was in New York (the room I rented was not at all the one in the pictures on the app). But well, this motivated me to dig a little deeper with this data and find out what are those things that we should all look at before renting an Airbnb and guarantee a good experience.\n\n\nOn the other hand, we **will also study the relationship that the price has with some of the variables we have available**, such as the neighborhood, the number of rooms, people's reviews, among others. We will try to **predict the price** of each Airbnb according to its characteristics. In this way we will try to avoid the risk of the host setting prices too high or too low and thus increase the probability of acquiring more customers.\n\n\n## **Goals**\n\n\n* Predict the price of each Airbnb offered by hosts and in this way try to help them to have more listings by decreasing the risk of charging the wrong price.\n\n* Delivering inputs for hosts with low earnings and making their listings more profitable and earning more revenue.\n\n* Investigate the influence of some variables on the price of each Airbnb.\n","7caa71bd":"**What is Neural Networt?**","9d92136c":"From the results above we can noticed that we have more data in Boston datasets. For the other hand, we have more variables in the Boston Listings Dataset than Seatle, we will check this result deeper and get insights. Now let's join the datasets.\n\nIn calendar's daframes we have information about dates and price of specific airbnb (listing_id). For both datasets we have a lot of null values in price variable!","c26ffc9b":"**\u00bfHow about null values?**","f8f50eda":"# Apply the concepts of ANN!","a5b01fed":"**Removing Outliers**","7a2492ed":"**We have a lot of featurues with high correlation value, so I will drop some features with correlation value > 0.7**","a0e7d87f":"## **Feature Selection**","050b4a78":"## What a great job this is a great result considering that the RSME obtained in our first simple model (LASSO) was 62.5 and here it is **51.4**, AMAZING!","11f9e07e":"**Lasso Regression for a simple variable selection**\n\nFor define Lasso Regression, first we should do a brief introducction to OLS (Ordinary least squares linear regression (OLS)). This is defined as the minimization of the L2 norm of the difference between the response $y_i$ and the predictor $f(x^{(i)}; w) =w^Tx^{(i)}$:\n\n\n$$\\hat{w}_{LS} = \\arg\\min||\\vec{X}\\vec{w}- y||^2$$\n\nOne of the most interesting results is te following:\n\n\n\n\n$$\\bar{E}_{in} = \\sigma^2(1-\\frac{p}{n})$$\n$$\\bar{E}_{out} = \\sigma^2(1-\\frac{p}{n})$$ \n$$ \\bar{E}_{in} - \\bar{E}_{out} = 2 * \\sigma^2\\frac{p}{n} $$\n\nWhere $p$ is the number of predictors and $n$ is the number of points in sample data.\nThis imparts an important message: if we have $p\\gg n$ (high-dimensional data), the generalization error is extremely large, meaning the model is not learning. Even\nwhen we have $p$ \u2248 $n$, we might still not learn well due\nto the intrinsic noise $\\sigma^2$. One way to ameliorate this is, as we shall see in the following  to use regularization, in this case Lasso Regularization  or L1.\n\nIn general, LASSO tends to give sparse solutions, meaning many components of $\\hat{w}$ \nare zero. An intuitive justification for this result is provided in the next figure:\n\n\n![lasso.PNG](attachment:f1bcbf1c-bd0f-4b70-bd66-327d20a92027.PNG)\n\nTo solve a constrained optimization problem with a fixed regularization strength\nt \u2265 0,  one first carves out the \u201cfeasible region\" specified by the regularizer in the\n$\\{w_1, \u00b7 \u00b7 \u00b7 , w_d\\}$ space. This means that a solution $\\hat{w}$ is legitimate only if it falls in this region. So in the Figure, the blue concentric ovals are the contours of the regression function while the red shaded regions represent the constraint function: |w1| + |w2| \u2264 t and. Intuitively, since the constraint function of LASSO has more\nprotrusions (vertices), the ovals tend to intersect the constraint at the\nvertex. Since the vertices correspond to\nparameter vectors $w$ with only one non-vanishing component,\nLASSO tends to give sparse solution.","292f6b53":"# \ud83d\udd0d Data Exploration (EDA)","39d1a515":"**Exploring correlation values**","dc6cba1c":"**Categorical data Selection**","edfc3e1d":"The Id type variables we will not employ in our Airbnb price prediction model as they have high cardinality. On the other hand, it seems that the host_listings_counts and host_total_listings_count variables are highly correlated.\n\n* On average each Airbnb has availability of 3 persons, 1 bedroom, and 1 bathroom.\n\n* The first glance at the price is that on average per night you have $\\$$150, $ \\$$325 per week and $\\$$978 per month. Additionally, we have properties up to\n $\\$$4000  per night.\n\n* Some extra expenses are: insurance deposit $\\$$131, cleaning fee $\\$$ 46.12\n\n* Other features: average guests 1, minimum number of nights: 2.75, number of reviews 29.7 and number of reviews per month 2.02.","b96e060d":"**From the loss functions we can observe that both the loss in training and in validation are decreasing as the epochs increase. No overfitting is observed (the validation curve starts to increase dramatically almost like a parabola).**","7851aebe":"* The neighbourhood with most and lest Airbnb in Boston is: Jamaica Plain and Leather district\n\n* The neighbourhood with most and lest Airbnb in Seatle is: Broadway and Roxhill\n","c1f55621":"#  Suggestions for Hosts\n","22f1ecbf":"**The model**","f6ffd304":"# \ud83d\udd04 Feature Engineering and Selection","e0ba1f3e":"**Libraries**","bce01c72":"# \ud83d\udcd1 Business Context","5d881876":"**Target: Price**","f1f16aca":"<center><img src=\"https:\/\/media.itsnicethat.com\/original_images\/563b992b7fa44cff9d001763.gif\"><\/center>\n<center><h1>The price of a good Airbnb experience \ud83d\udcb0<\/h1><\/center>","6f7fb19d":"In this section, we will discuss two methods for variable selection. On the other hand, we will select variables according to their type, i.e. the first method will be applied to continuous variables and the second will be applied to categorical variables. The methods are:\n\n* Mutual information score\n* Lasso Regression","f3c0ac43":"Since we are going to employ a regression model to predict the price of Airbnb places, using a neural network, we need to scale the data to get the right results. Next we will perform all the data processing to make the model.","9871d5e3":"**Create Amenities Columns**","ee4fc395":"**Training and tunning ANN for regression task!**","61710199":"* In the first view of the insights table we see that the worst scores are for hosts that have a very high deposit and cleaning fee. It is therefore recommended to adjust these prices.\n\n* On the other hand, we can also see that the minimum number of nights is approximately double for those hosts with high scores. It is recommended to review this restriction.\n\n* The list above shows those variables that the host should check. For example, they could offer shampoo, be pet friendly, offer breakfast, have checking in for 24 hrs.\n\n*  The last variables are related to host verification so it is recommended that you please certify your account through the Airbnb app.","d3bce2bb":"The Neural Nets are inspired on biological neural connections. As we will see neural nets can be viewed as an extension of supervised  learning models like Logistic regression. Essentially, neural nets are very powerful general-purpose function approximators that can be trained using many examples. For example:\n\n$$y = F_\\theta(x)$$\n\nFor a neural net the above equation becomes:\n\n$$y = F(x)$$ \n\nWhere F(x) is a smooth function. So the goal here will be to approximate $F(x)$ as well as possible  by choosing the right parameter in $F_\\theta$.\n\nFor a Neural Network problem we need to construct the representation of $F_\\theta$ with two conditions:\n\n* Scalable\n* Efficient\n\n**The layer of NN**\n\nNeuron is the basic unit of ANN (artificial neural network), wich holds a scalar value (real number). The operation of this neuron is quite simply:  \n\n* As in the Figure (1) we can see that each neuron takes an input (a linear combinarion of $n$ vectors)\n\n$$z = \\sum_i w_i*y_i +b_i$$\n\n\n* Aftherwards a non-linear function $f$ is applied to get the output value of each neuron **(b)** in figure(1).\n$$y = f(z)$$\n\n* To obtain a non-linear function $F_\\theta$ that can truly represent arbitrary functions $F(x)$, multiple layers of neurons are needed. Each neuron receives the values of all the neurons of the precending layers. For the multi-layer case we have:\n\n$$z_j^{(n+1)}= \\sum_k w_{k,j}^{(n+1)}* y_k^n + b_j^{(n+1)}$$\n$$ y_j^{(n+1)}= f(z_j^{(n+1)})$$\n\n\nThe computational effort and memory consumption scales quadratically of the typical number of neurons ( $N^{(n+1)}* N^n$).\n\n![nn.PNG](attachment:ac29ef82-acef-4362-ad9e-926d4f05605e.PNG)\n\nFigure (1). General description of Neural Nets.\n\n\n**How to train a Neural Net?**\n\nOne of the most important questions for train a ANN is: how can we measure the deviation between the real and the estimate function? to do this we will introduce the cost function $C$:\n\n$$ C_x(\\theta) = |F_\\theta(x)-F(x)|^2$$\n$$ C_\\theta  =  <C_x(\\theta)>_x$$\n\n\nOnce the cost function has been defined, the principal idea is to try finding its minimum by gradient descent in high dimensional space of $\\theta$ parameters.\n\n*The problem of gradient descent*\n\nWhenever we use a library, like Keras, to design a neural network we are always unaware of the optimization algorithm underneath. But it is important, so here is my interpretation of this algorithm.\n\nIn principle gradient descent is simple. We just move along the negative gradient of the cost function.\n\n$$\\delta\\theta_k = -\\eta\\frac{\\partial C(\\theta)}{\\partial \\theta_k}$$\n\nThe parameter $\\eta$ is called the learning rate. If it is too small, learning will proceed slowly, but if it is too large, one may overshoot the optimum. Now we will show you a interest result for $\\eta$ very small:\n\n$$ \\delta C = -\\eta (\\frac{\\delta C}{\\delta \\theta})^2 + O(\\eta^2)$$ \n\n\nBut we have two principal problems with the calculation of minimun of cost function:\n\n* The cost function is defined as an average over all possible inputs, wich is much too expensive to calculate at each step.\n\n* The cost function depends on many parameters, and we have to find a way to calculate the gradient efficiently.\n\n\nThe first problem is solved by averaging only over a small number of randomly selected\ntraining samples (called a \u201cbatch\u201d, or sometimes more precisely a \u201cmini batch\u201d):\n\n$$ C(\\theta) \\approx \\frac{1}{N}\\sum_{j=1}^N C_{x,j}(\\theta) \\equiv  <C_x(\\theta)>_{batch}$$\n\n$$ \\delta\\theta_k = - \\eta <\\frac{\\partial C(\\theta)}{\\partial \\theta_k }>_{batch} = \\frac{\\partial C(\\theta)}{\\partial \\theta_k } + noise $$\n\n\n**Back propagation Algorithm**\n\nWe did'nt solve the problem to calculate the gradient in high dimensional space of parameters. The chain rule will be the savior, for example for the quadratic cost function [1]:\n\n$$ \\frac{\\partial C_x(\\theta)}{\\partial \\theta_k} = 2 \\sum_l([F_\\theta(x)]_l - [F(x)]_l) \\frac{\\partial [F_\\theta(x)]_l}{\\partial \\theta_k}$$\n\nWith $F_\\theta(x) = y$ with this we should to calculate:\n\n$$ \\frac{\\partial y_l^{(n)}}{\\partial \\theta_k } = f^*(z_l)\\frac{\\partial z^{(n)}}{\\partial \\theta_k}$$\n\n$$ \\frac{\\partial z^{(n)}}{\\partial \\theta_k} = \\sum_m w_{l,m}^{n,n-1} \\frac{\\partial y_m^{(n-1)}}{\\partial \\theta_k}$$\n\n$$ M_{l,m}^{(n, n-1)} = w_{l,m}^{(n, n-1)} f^{*} (z_m^{(n-1)}) $$\n\n**The above equations are telling us: we need to calculate the gradient of each neuron value regarding to all paramters!!**\n\n**Did you see the recursive structure? if the answer is no, please write to me**\n","943c051b":"Below we can see the exact location of each of the properties, examine the neighborhood, what amenities the neighborhood has, transportation facilities, and most importantly the review score. This visualization will be of great help to make decisions to rent or not to rent an Airbnb according to the needs of each person.","a7385dca":"For this variable we have no missing values. On the other hand, it can be observed that we have quite skewed distributions, where the values of the prices vary up to $\\$$ 4000 in de Boston and $\\$$ 1000 in Seatle.","3c9dc516":"## **We start the explorarion of the Listings dataset:**","10564a75":"**We are going to examinate the most importans features**","78271fed":"**What about price in each neighbourhood?**","457b2067":"It seems that we have outliers in Boston. Now check features related with reviews scores.","c248355b":"# The Model: Predicting  price of Airbnb with Artificial Neural Network\n","4fc91839":"**Mutual Information**\n\nTo understand this concept, first, we should understand what is entropy and why is so useful for feature selection. The entropy of a random variable is a function which attempts to characterize the \u201cunpredictability\u201d of a random variable, in other words, is the amount of information that we need to predict the state of the system of, in this case, random variable.\n\n$$H(X) = -\\sum_{x\\in X} P(x)\\log(P(x)) $$\n\n\nWith this concept now we can define the mutual information.\n\nMutual information is a quantity that measures a relationship between two\nrandom variables that are sampled simultaneously. In particular, **it measures\nhow much information is communicated, on average, in one random variable\nabout another**. Intuitively, one might ask, how much does one random variable\ntell me about another? So the mutual information between two variables is 0 if and only if the two variables **are statistically independent**. The formal definition is:\n\n$$I(X;Y) = \\sum_{x\\in X}\\sum_{y\\in Y} P(x,y) \\log(\\frac{P(x,y)}{P(x)P(y)})$$\n\nIn this definition, P(X) and P(Y ) are the marginal distributions of X and\nY. For the other hand we can rewrite the above equation in terms of entropy and joint entropy:\n\n$$I(X;Y) = H(X) + H(Y)- H(X|Y)$$\n\nNote: If X and Y are independent we have:  $H(X|Y)=0$"}}