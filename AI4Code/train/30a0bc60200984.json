{"cell_type":{"4334da18":"code","1f1c3809":"code","15d7a0e3":"code","5aea2b1b":"code","672223c9":"code","fe6518d2":"code","f9763220":"code","98cfec36":"code","d4c850a0":"code","42a7bb7c":"code","7ee30541":"code","a7d32924":"code","629aa1bf":"code","59f902a4":"code","5a90ebf1":"code","1e44ec63":"code","9b00f38a":"code","d3bcfa1d":"code","44f689b3":"code","99313f28":"code","ece76e72":"code","799015ad":"code","60466433":"code","6ffea29b":"code","0c5925c2":"code","ad5e12a3":"code","6f152e4d":"markdown","9c34a72c":"markdown","1762db91":"markdown","c1a5558d":"markdown","eec9b2ef":"markdown","0687d978":"markdown","8a21920f":"markdown","dc89251c":"markdown","ce6cce5a":"markdown","429a0fef":"markdown"},"source":{"4334da18":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n\nimport torch\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torchvision import models\nfrom sklearn import preprocessing\n\nimport os\nimport time\nimport copy\nfrom tqdm.notebook import trange, tqdm","1f1c3809":"path = '..\/input\/animal-image-dataset-90-different-animals'\n\nwith open(path + \"\/name of the animals.txt\") as f:\n    classes = f.read().split('\\n')\n\n# View classes\nnum_classes = len(classes)\nprint(f'Number of classes: {num_classes}')\nprint(f'Classes of animals: {\", \".join(classes)}')","15d7a0e3":"animal_path = path + \"\/animals\/animals\"\n\n# See the number of images for each class\nfor animal in os.listdir(animal_path):\n    print(f'Number of {animal} images: {len(os.listdir(animal_path + \"\/\" + animal))}')","5aea2b1b":"def shuffle_data(X, seed):\n    if seed:\n        np.random.seed(seed)\n\n    idx = np.arange(len(X))\n    np.random.shuffle(idx)\n    \n    return np.array(X)[idx]\n    \n    \ndef data_split(animal_path, split_size=0.1):\n    \"\"\"\n        Based on split size, \n        Each folder is split into training and test sets.\n\n        Returns:\n            Shuffled training, validation and test dataset\n            Each set is in the form of: ['id.jpg', 'string label of id']\n                Example: [['1234.jpg', 'antelope'], ['12312.jpg', 'iguana'] ...]\n    \"\"\"\n    \n    X_train, X_val, X_test = [], [], []\n    y_train, y_val, y_test = [], [], []\n    \n    # For each label\n    for animal in os.listdir(animal_path):\n        \n        # Extract same label for training, validation, test data\n        label = animal\n        \n        # Image path for each animal\n        image_list = os.listdir(animal_path + \"\/\" + animal)\n        \n        # Shuffle images (path) \n        shuffled_data = shuffle_data(image_list, seed=None)\n        \n        # Get the train, val, test split\n        split_i = len(shuffled_data) - int(len(shuffled_data) \/\/ (1 \/ (split_size * 2)))\n        split_j = len(shuffled_data) - int(len(shuffled_data) \/\/ (1 \/ split_size)) \n        \n        # Get training data from train_val_data split[:split_j]\n        train_data = shuffled_data[:split_i]\n        \n        # Add training data, label to final training data\n        X_train.extend(train_data) \n        y_train.extend([label] * len(train_data)) \n        \n        # Get val data and label from train_val_data split[split_j:]\n        val_data = shuffled_data[split_i:split_j]\n        \n        # Add val data, label to final val data\n        X_val.extend(val_data) \n        y_val.extend([label] * len(val_data)) \n        \n        # Get test data from shuffled_data[split_i:]\n        test_data = shuffled_data[split_j:]\n       \n        # Add test data, label to final val data\n        X_test.extend(test_data) \n        y_test.extend([label] * len(test_data)) \n    \n    \n    # Each image is presented as: ['image id.jpg', 'string label']\n    training_data = list(zip(X_train, y_train))\n    val_data = list(zip(X_val, y_val))\n    test_data = list(zip(X_test, y_test))\n    \n    #Shuffle all data in respective sets\n    shuffled_training_data = shuffle_data(training_data, seed=None)\n    shuffled_val_data = shuffle_data(val_data, seed=None)\n    shuffled_test_data = shuffle_data(test_data, seed=None)\n    \n    return shuffled_training_data, shuffled_val_data, shuffled_test_data","672223c9":"# Split size for training, validation, test\nsplit_size = 0.08\n\ntrain_data, val_data, test_data = data_split(animal_path, split_size=split_size)\nprint('Number of training images: {}'.format(len(train_data)))\nprint('Number of validation images: {}'.format(len(val_data))) \nprint('Number of testing images: {}'.format(len(test_data)))","fe6518d2":"def image_plot(data):\n    fig = plt.figure(figsize=(20, 6))\n    rows = 3\n    columns = 10\n\n    ax = []\n\n    # See the first 30 images\n    for i in range(30):\n        image, label = data[i]\n        \n        ax.append(fig.add_subplot(rows, columns, i+1))\n        \n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0)\n            ax[-1].set_title(le.inverse_transform([label])[0], fontsize=12, fontweight='bold')\n        else:\n            image_path =  os.path.join(animal_path + \"\/\" + label + \"\/\" + image)\n            image = mpimg.imread(image_path)\n            ax[-1].set_title(label, fontsize=12, fontweight='bold')\n        \n        plt.imshow(image)\n        plt.axis('off')","f9763220":"# See some of the training images before pre-processing\nimage_plot(train_data)","98cfec36":"# To convert string labels into integers\nle = preprocessing.LabelEncoder()\nle.fit(classes)","d4c850a0":"class _Dataset(Dataset):\n    \n    def __init__(self, data, transform=None):\n        data, label = zip(*data)\n        \n        # Transform string labels into integers\n        label = le.transform(label)\n        \n        self.x = np.array(data)\n        self.y = np.array(label)\n        self.num_samples = len(data)\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        sample = self.x[index], self.y[index]\n        img_path = os.path.join(animal_path, le.inverse_transform([sample[1]])[0], sample[0])\n        img = mpimg.imread(img_path)\n        \n        if self.transform:\n            \n            features = self.transform(img)\n        \n        return features, sample[1]\n    \n    def __len__(self):\n        return self.num_samples\n    \ndata_transforms = {\n    'train': transforms.Compose(\n        [transforms.ToPILImage(),\n         transforms.Resize(256),\n         transforms.RandomCrop(224),\n         transforms.RandomHorizontalFlip(p=0.5),\n         transforms.GaussianBlur(3),\n         transforms.ColorJitter(0.5),\n         transforms.RandomRotation(15),\n         transforms.ToTensor(),\n         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]),\n    'val': transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n}\n","42a7bb7c":"training_dataset = _Dataset(train_data, transform=data_transforms['train'])\nvalidation_dataset = _Dataset(val_data, transform=data_transforms['val'])\ntest_dataset = _Dataset(test_data, transform=data_transforms['val'])","7ee30541":"# Visualise augmented ugly images\nimage_plot(training_dataset)","a7d32924":"BATCH_SIZE=32\n\ntrain_loader = torch.utils.data.DataLoader(dataset=training_dataset, shuffle=True, batch_size=BATCH_SIZE)\nval_loader = torch.utils.data.DataLoader(dataset=validation_dataset, shuffle=False, batch_size=BATCH_SIZE)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, shuffle=False, batch_size=BATCH_SIZE)","629aa1bf":"# Early stopping adapted from: https:\/\/debuggercafe.com\/using-learning-rate-scheduler-and-early-stopping-with-pytorch\/\n# Slightly modified to reset counter if increasing loss is not across consecutive epochs\nclass EarlyStopping():\n    \"\"\"\n    Early stopping to stop the training when the loss does not improve after\n    certain epochs.\n    \"\"\"\n    def __init__(self, patience=5, min_delta=0):\n        \"\"\"\n        :param patience: how many epochs to wait before stopping when loss is\n               not improving\n        :param min_delta: minimum difference between new loss and old loss for\n               new loss to be considered as an improvement\n        \"\"\"\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        \n        \n    def __call__(self, val_loss):\n        if self.best_loss == None:\n            self.best_loss = val_loss\n        elif self.best_loss - val_loss > self.min_delta:\n            self.best_loss = val_loss\n            if self.counter > 0:\n                print(f\"Early stopping counter resetting from {self.counter} to 0.\")\n                self.counter = 0\n        elif self.best_loss - val_loss < self.min_delta:\n            self.counter += 1\n            print(f\"Early stopping counter {self.counter} of {self.patience}\")\n            if self.counter >= self.patience:\n                self.early_stop = True","59f902a4":"class NeuralNet(nn.Module):\n    def __init__(self):\n        super(NeuralNet, self).__init__()\n        \n        # Use pre-trained model\n        self.model = models.densenet161(pretrained=True)\n        \n        # Freeze all layers (No training)\n        for param in self.parameters():\n            param.requires_grad = False\n            \n        # Change final FC layer to num_classes output. This is trainable by default\n        self.model.classifier = nn.Linear(2208, num_classes)\n    \n    def forward(self, x):\n        x = self.model(x)\n        return x\n          \n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = NeuralNet().to(device)\n\n# Model Summary\nprint(model)","5a90ebf1":"# Boilerplate code adapted: https:\/\/pytorch.org\/tutorials\/beginner\/transfer_learning_tutorial.html\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, scheduler=None, early_stopping=None):\n    since = time.time()\n    \n    training_accuracies, training_losses, val_accuracies, val_losses = [], [], [], []\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        \n        print('Epoch {}\/{}'.format(epoch+1, num_epochs))\n        print('-' * 60)\n        \n        # Each epoch has a training and validation phase\n        for phase in ['Training', 'Validation']:\n            if phase == 'Training':\n                model.train()  # Set model to training mode\n                dataloaders = train_loader\n                dataset_sizes = len(training_dataset)\n            else:\n                model.eval()   # Set model to evaluate mode\n                dataloaders = val_loader\n                dataset_sizes = len(validation_dataset)\n\n            running_loss = 0.0\n            running_corrects = 0\n     \n            # Iterate over data.\n            for inputs, labels in tqdm(dataloaders):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'Training'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'Training':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            if phase == 'Training':\n                scheduler.step()\n\n            epoch_loss = running_loss \/ dataset_sizes\n            epoch_acc = running_corrects.double() \/ dataset_sizes\n\n            if phase == 'Training':\n                training_accuracies.append(epoch_acc)\n                training_losses.append(epoch_loss)\n            else:\n                val_accuracies.append(epoch_acc)\n                val_losses.append(epoch_loss)\n\n            # deep copy the model\n            if phase == 'Validation' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n            \n            print('{} Loss after epoch: {:.4f}, Acc after epoch: {:.4f}\\n'.format(\n            phase, epoch_loss, epoch_acc))\n            \n        \n        # Early stopping check with last average validation loss after end of epoch\n        if early_stopping is not None:\n            early_stopping(val_losses[-1])\n\n            if early_stopping.early_stop:\n                print('Early Stopping Initiated')\n                break\n\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n    \n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, training_accuracies, training_losses, val_accuracies, val_losses   \n\nlearning_rate=0.01\n\nnum_epochs=100\n\ncriterion = nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer_ft = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Decay LR by a factor of 0.1 every 10 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n\n# Early stopping if val_loss does not improve from the best - 1e-5 over next 5 epochs\nes = EarlyStopping(patience=5, min_delta=1e-5)","1e44ec63":"model_ft, training_accuracies, training_losses, val_accuracies, val_losses = train_model(model=model,\n                                                                                         train_loader= train_loader,\n                                                                                         val_loader=val_loader, \n                                                                                         criterion=criterion, \n                                                                                         optimizer=optimizer_ft, \n                                                                                         num_epochs=num_epochs, \n                                                                                         scheduler=exp_lr_scheduler, \n                                                                                         early_stopping=es)","9b00f38a":"# Plots for accuracy and loss\ndef plot_history():\n    with plt.style.context('seaborn-darkgrid'):\n        fig = plt.figure(figsize=(25,10))\n\n        # Summarize history for accuracy\n        plt.subplot(1, 2, 1)\n        plt.plot([*range(len(training_accuracies))], training_accuracies)\n        plt.plot([*range(len(val_accuracies))], val_accuracies)\n        plt.title('Accuracy ', fontsize=15)\n        plt.ylabel('Accuracy', fontsize=15)\n        plt.xlabel('Epoch', fontsize=15)\n    #     plt.ylim([0.4, 1.0])\n        plt.legend(['Training Accuracy', 'Validation Accuracy'], loc='upper right', fontsize=15)\n\n        # Summarize history for loss\n        plt.subplot(1, 2, 2)\n        plt.plot([*range(len(training_losses))], training_losses, label='training loss')\n        plt.plot([*range(len(val_losses))], val_losses, label='validation loss')\n        plt.title('Losses', fontsize=15)\n        plt.ylabel('Loss', fontsize=15)\n        plt.xlabel('Epoch', fontsize=15)\n    #     plt.ylim([0.0, 1.5])\n        plt.legend(['Training Loss', 'Validation Loss'], loc='upper right', fontsize=15)\n\n        plt.show()\n","d3bcfa1d":"plot_history()","44f689b3":"def visualize_model(model, data_loader, num_images=25):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure(figsize=(25, 30))\n    \n    fig.canvas.draw()\n    \n    rows = 5\n    columns = 5\n    \n    MEAN = torch.tensor([0.485, 0.456, 0.406])\n    STD = torch.tensor([0.229, 0.224, 0.225])\n    \n    with torch.no_grad():\n            \n        for i, (inputs, labels) in enumerate(data_loader):\n            inputs = inputs.to(device)\n\n            outputs = model(inputs)\n            val, preds = torch.max(outputs, 1)\n\n            preds_invtrans = le.inverse_transform(preds.cpu())\n            \n            labels_invtrans = le.inverse_transform(labels.cpu())\n            \n            ax = []\n            for j in range(inputs.size()[0]):\n                images_so_far +=1\n                ax.append(plt.subplot(rows, columns, images_so_far))\n\n                # Reverse normalize \n                image = inputs.cpu().data[j] * STD[:, None, None] + MEAN[:, None, None]\n\n                image = image.permute(1, 2, 0)\n\n                ax[-1].set_title(f'Ground Truth: {labels_invtrans[j]}\\nPredicted: {preds_invtrans[j]}', fontsize=15, fontweight='bold')\n                \n                # Draw boxes around subplots\n                bbox = ax[-1].get_tightbbox(fig.canvas.renderer)\n                x0, y0, width, height = bbox.transformed(fig.transFigure.inverted()).bounds\n                \n                # slightly increase the very tight bounds:\n                xpad = 0.01 * width\n                ypad = 0.01 * height\n                \n                if preds[j] == labels[j]:\n                     fig.add_artist(plt.Rectangle((x0-xpad, y0-ypad), width+2*xpad, height+2*ypad, edgecolor='green', linewidth=5, fill=False))\n                else:\n                     fig.add_artist(plt.Rectangle((x0-xpad, y0-ypad), width+2*xpad, height+2*ypad, edgecolor='red', linewidth=5, fill=False))\n\n                plt.imshow(image)\n                plt.axis('off')\n\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n\n        model.train(mode=was_training)\n        ","99313f28":"# Visualise on validation dataset\nvisualize_model(model_ft, val_loader)","ece76e72":"labels_list = []\npreds_list = []\nwith torch.no_grad():\n    for inputs, labels in tqdm(test_loader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        labels_list.extend(label.item() for label in labels)\n        \n        outputs = model_ft(inputs)\n        \n        predictions = torch.argmax(outputs, dim=1)\n    \n        preds_list.extend(prediction.item() for prediction in predictions)\n\n        \ncount = 0\nfor i in range(len(preds_list)):\n    if preds_list[i] == labels_list[i]:\n        count += 1\n\naccuracy = count \/ len(preds_list)\nprint(f'Test accuracy = {accuracy:.4f}')","799015ad":"# Visualise predictions on test dataset\nvisualize_model(model_ft, test_loader)","60466433":"from sklearn.metrics import classification_report\n\nreport = classification_report(labels_list, preds_list)\nprint(report)","6ffea29b":"from PIL import Image\nimport requests\n\ndef predict(model, url):\n    \n    tfms = transforms.Compose([\n        transforms.Resize(256),\n        transforms.RandomCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n    \n    plt.figure(figsize=(5,5))\n    \n    image = Image.open(requests.get(url, stream=True).raw)\n    img_tensor = tfms(image).to('cuda').unsqueeze(0)\n    output = model(img_tensor)\n    prediction = torch.argmax(output, dim=1)\n    prediction = le.inverse_transform(prediction.cpu())\n\n    \n    plt.imshow(image)\n    plt.title(f'Prediction: {prediction[0]}', fontsize=\"15\")\n        \n    plt.axis('off')","0c5925c2":"# Insert image address for single image prediction\nurl_list = [\n    \"https:\/\/static.scientificamerican.com\/sciam\/cache\/file\/ACF0A7DC-14E3-4263-93F438F6DA8CE98A_source.jpg?w=590&h=800&896FA922-DF63-4289-86E2E0A5A8D76BE1\",\n    \"https:\/\/static.scientificamerican.com\/sciam\/cache\/file\/5C51E427-1715-44E6-9B14D9487D7B7F2D_source.jpg\",\n    \"https:\/\/thumbor.forbes.com\/thumbor\/fit-in\/1200x0\/filters%3Aformat%28jpg%29\/https%3A%2F%2Fspecials-images.forbesimg.com%2Fimageserve%2F521433364%2F0x0.jpg\",\n    \"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQRUu8hCQUvHGXVZ79cQW_WFc9Adzsp-X0jqA&usqp=CAU\",\n    \"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRrtoRmgkcDwR7QUu0Nvpqi7h7geW6pMD5zqw&usqp=CAU\",\n    \"https:\/\/static.wikia.nocookie.net\/characters\/images\/a\/ac\/DSNY-SM-13.jpg\/revision\/latest?cb=20141231230015\"\n]\n\nfor url in url_list:\n    predict(model_ft, url)","ad5e12a3":"untrained_url_list = [\n    \"https:\/\/images.ctfassets.net\/81iqaqpfd8fy\/3r4flvP8Z26WmkMwAEWEco\/870554ed7577541c5f3bc04942a47b95\/78745131.jpg?w=1200&h=1200&fm=jpg&fit=fill\",\n    \"https:\/\/media.tacdn.com\/media\/attractions-splice-spp-674x446\/06\/6c\/2e\/d4.jpg\",\n    \"https:\/\/i.guim.co.uk\/img\/media\/5dab212de07d8009dd06560efeb1ac87b114e23d\/0_640_4183_2510\/master\/4183.jpg?width=1200&height=1200&quality=85&auto=format&fit=crop&s=7c9461c31fb8530f24343e669d6730ba\",\n    \"https:\/\/images.wsj.net\/im-307680?width=1280&size=1\"\n]\n\nfor url in untrained_url_list:\n    predict(model_ft, url)","6f152e4d":"### Model","9c34a72c":"### Split data ","1762db91":"Overall quite surprising that the model can perform well given:\n1. Lack of data (60 images for each of the 90 classes) \n2. Similarity of different labels (whale vs shark, coyote vs wolf, rat vs mouse, antelope vs goat vs reindeer)","c1a5558d":"#### Predict animals that model have not been trained on","eec9b2ef":"## Predict single images\n\nLets play around abit","0687d978":"### Dataset preparation and Label Encoding","8a21920f":"### Visualise model predictions","dc89251c":"### Test Set","ce6cce5a":"### Training and Validation Plots","429a0fef":"From the first 30 images, it is pretty understandable where the model gets wrong.\n\nMouse and rat are literally the same animal, Darwin made a mistake there.\n\nReindeer's antlers may have been mistaken for goat's horns."}}