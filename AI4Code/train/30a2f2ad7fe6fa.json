{"cell_type":{"9bc9a96e":"code","e131acd8":"code","5f87db8d":"code","a4f702f8":"code","48f9b57d":"code","3889daa1":"code","4989a6e2":"code","c0cc7741":"code","aae2ad5d":"code","8f0c85b6":"code","498059e9":"code","42d55d63":"code","190eedc4":"code","1b60bbd0":"code","6d6cb6ef":"markdown","24085ed7":"markdown","1d65fc62":"markdown","1fdb1d56":"markdown","9283461a":"markdown","8cb37a6d":"markdown","a5f78bb5":"markdown","ccc49f7b":"markdown","8a632c61":"markdown","98b8c275":"markdown","516cbabb":"markdown","36cd4fbd":"markdown","312d4951":"markdown","46ea90a3":"markdown","eb78aca6":"markdown","62deb024":"markdown","7feac174":"markdown","ca7f2860":"markdown","11e129ed":"markdown"},"source":{"9bc9a96e":"import numpy as np\n\nnp.random.seed(800)","e131acd8":"from sklearn.datasets import make_moons\n\nx,y = make_moons(n_samples=100,noise=0.2,random_state=1)","5f87db8d":"#plot the graph\nimport matplotlib.pyplot as plt\nplt.scatter(x[:,0],x[:,1],c=y,s=100)\nplt.show()","a4f702f8":"#importing libraries\nimport tensorflow as tf\nimport warnings\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.model_selection import train_test_split","48f9b57d":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.33,random_state=42)","3889daa1":"model = Sequential()\nmodel.add(Dense(500, input_dim=2, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', \n              optimizer='adam', \n              metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train, \n                    validation_data=(x_test, y_test), \n                    epochs=4000, verbose=0)","4989a6e2":"plt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","c0cc7741":"\nmodel = Sequential()\nmodel.add(Dense(500, input_dim=2, activation='relu',kernel_regularizer='l2'))\nmodel.add(Dense(1, activation='sigmoid',kernel_regularizer='l2'))\nmodel.compile(loss='binary_crossentropy', \n              optimizer='adam', \n              metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train, \n                    validation_data=(x_test, y_test), \n                    epochs=4000, verbose=0)","aae2ad5d":"plt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","8f0c85b6":"from tensorflow.keras.layers import Dropout\nmodel = Sequential()\nmodel.add(Dense(500, input_dim=2, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', \n              optimizer='adam', \n              metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train, \n                    validation_data=(x_test, y_test), \n                    epochs=500, verbose=0)","498059e9":"plt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","42d55d63":"from keras.preprocessing.image import ImageDataGenerator\n\naug = ImageDataGenerator(\n    rotation_range=20,\n    zoom_range=0.15,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.15,\n    horizontal_flip=True,\n    fill_mode=\"nearest\")","190eedc4":"from tensorflow.keras.callbacks import EarlyStopping\n\nmodel = Sequential()\nmodel.add(Dense(128, input_dim=2, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', \n              optimizer='adam', \n              metrics=['accuracy'])\n\ncallback= EarlyStopping(monitor='val_loss')\nhistory = model.fit(x_train, y_train, \n                    validation_data=(x_test, y_test), \n                    epochs=2000,callbacks=[callback])","1b60bbd0":"plt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()\n","6d6cb6ef":"## Adding early Callbacks","24085ed7":"## Results ","1d65fc62":"<img src=\"https:\/\/i2.wp.com\/dataaspirant.com\/wp-content\/uploads\/2020\/08\/1-handle-overfitting-in-deep-learning-models.png?resize=768%2C452&ssl=1\" \/>","1fdb1d56":"We can prevent the model from being overfitted by training the model on more numbers of examples.  We can increase the size of the data by applying some minor changes in the data. \n\nExamples: \n\n* Translations, \n* Rotations, \n* Changes in scale, \n* Shearing, \n* Horizontal (and in some cases, vertical) flips.\n\nThis technique mostly used for only CNN\u2019s","9283461a":"<img src=\"https:\/\/i2.wp.com\/dataaspirant.com\/wp-content\/uploads\/2020\/08\/8-Deep-learning-dropout.png?resize=768%2C394&ssl=1\" \/>\n<hr>\n\nDropout is simply dropping the neurons in neural networks. During training a deep learning model, it drops some of its neurons and trains on rest. It updates the weights of only selected or activated neurons and others remain constant. \n\nFor every next\/new epoch again it selects some nodes randomly based on the dropout ratio and keeps the rest of the neurons deactivated. It helps to create a more robust model that is able to perform well on unseen data.\n\n You can see the example below","8cb37a6d":"Deep learning is one of the most revolutionary technologies at present. It gives machines the ability to think and learn on their own. The key motivation for deep learning is to build algorithms that mimic the human brain. \n\nTo achieve this we need to feed as much as relevant data for the models to learn. Unlike machine learning algorithms the deep learning algorithms learning won\u2019t be saturated with feeding more data. But feeding more data to deep learning models will lead to overfitting issue.\n\nThat\u2019s why developing a more generalized deep learning model is always a challenging problem to solve. Usually, we need more data to train the deep learning model. In order to get an efficient score we have to feed more data to the model. But unfortunately, in some cases, we face issues with a lack of data. \n\nOne of the most common problems with building neural networks is overfitting. The key reason is, the build model is not generalized well and it\u2019s well-optimized only for the training dataset. In layman terms, the model memorized how to predict the target class only for the training dataset. \n\n\nThe other cases overfitting usually happens when we don\u2019t have enough data, or because of complex architectures without regularizations.\n\nIf we don't have the sufficient data to feed, the model will fail to capture the trend in data. It tries to understand each and every data point in training data and performs poorly on test\/unseen data.","a5f78bb5":"### You can see the demo of Data Augmentation below","ccc49f7b":"## Adding Dropout","8a632c61":"# Data Augementation","98b8c275":"<h1> Techniques to Handle Overfitting In Deep Learning<\/h1>\n\n* Regularization\n* Dropout\n* Data Augmentation\n* Early stopping","516cbabb":"# Conclusion\n\nEach technique approaches the problem differently and tries to create a model more generalized and robust to perform well on new data. We have different types of techniques to avoid overfitting, you can also use all of these techniques in one model.\n\nDon't limit youself to consider only these techniques for handle overfitting, you can try other new and advanced techniques to handle overfitting while building deep learning models.\n\nWe can't say which technique is better, try to use all of the techniques and select the best according to your data.\n\n<h3>Suggestions<\/h3>\n    <b>Classical approach:<\/b> use early stopping and L2 regularization\n    \n <b>The modern approach:<\/b> use early stopping and dropout, in addition to regularization.","36cd4fbd":"## Adding Regularization\n\n<h4><b>Regularization<\/b> is one of the best techniques to avoid overfitting. It can be done by simply adding a penalty to the loss function with respect to the size of the weights in the model. By adding regularization to neural networks it may not be the best model on training but it is able to outperform well on unseen data. \n    <\/h4>\n\nYou can see the example below:","312d4951":"## Data preparation\n\nThe make_moons() function is for binary classification and will generate a swirl pattern, or two moons\n\nparameters:\n\nn_samples - int: the total number of points generated optional (default=100)\n\nshuffle- bool: whether to shuffle the samples.optional (default=True)\n\nnoise- double or None: the standard deviation of Gaussian noise added to the data (default=None)\n\nrandom_state- int: RandomState instance, default=None\n\nReturns:\n\nXarray of shape [n_samples, 2]\n\nY array of shape [n_samples], the integer labels (0 or 1) for class membership of each sample","46ea90a3":"### code snippet for augmentation in Keras","eb78aca6":"## Base Model Creation","62deb024":"# Early Stopping\n\n<img src=\"https:\/\/i0.wp.com\/dataaspirant.com\/wp-content\/uploads\/2020\/08\/early-stopping-graph.png?resize=768%2C409&ssl=1\" \/>\n\n\n<hr>\nIt is one of the most universally used techniques in which we can smartly overcome the overfitting in deep learning. Too many epochs can lead to overfitting of the training dataset. In a way this a smar way to handle overfitting.\n\nEarly stopping is a technique that monitors the model performance on validation or test set based on a given metric and stops training when performance decreases.\n","7feac174":"## Model with overfitting issue\n\nNow we are going to build a deep learning model which suffers from overfitting issue. Later we will apply different techniques to handle the overfitting issue. \n\nWe are going to learn how to apply these techniques, then we will build the same model to show how we improve the deep learning model performance.\n\nBefore that let\u2019s quickly see the synopsis of the model flow.\n\n<h3>Synopsis of the model we are going to build<\/h3>\nBefore we are going to handle overfitting, we need to create a Base model .\nFirst, we are going to create a base model in order to showcase the overfitting\n\nIn order to create a model and showcase the example, \n\nfirst, we need to create data. we are going to create data by using make_moons() function.\n\nThen we fit a very basic model (without applying any techniques) on newly created data points\n\nThen we will walk you through the different techniques to handle overfitting issues with example codes and graphs.","ca7f2860":"<h1 style=\"color:green\";> Thank you for reading, please do upvote if it helps you \ud83d\ude4f\ud83d\ude4f<\/h1>","11e129ed":"<img src=\"https:\/\/i0.wp.com\/dataaspirant.com\/wp-content\/uploads\/2020\/08\/data-augmentation-example.png?resize=768%2C339&ssl=1\" \/>"}}