{"cell_type":{"9a039621":"code","cdae9fef":"code","2a9cda36":"code","eb46614d":"code","b40940b5":"code","52395d89":"code","32e1d662":"code","59c4fb93":"code","2ce55556":"code","46de7360":"code","83f9e5aa":"code","a2455f7b":"code","95516cda":"code","379a06e5":"code","9cd5846d":"code","2fcab3de":"code","03522eba":"code","0fb1c799":"code","ad7addb4":"code","1dceaa3a":"code","ddc4c01d":"code","77a4edbb":"code","aa8a948b":"code","ab16dd49":"code","38f126f9":"code","aa63e302":"code","f7d55ed5":"code","b4da3b7a":"code","0d169288":"code","3b88a7dc":"code","8ba10680":"code","946694ea":"code","bf01911b":"code","911b5e03":"code","5b5d60f7":"code","66c2c6b8":"code","4dee9c21":"code","3d0a470c":"code","59ffe807":"code","4ed449c1":"code","004fc3b1":"code","f165789c":"code","aa148185":"code","c9ec43c5":"code","cd40c360":"code","5202ed7e":"markdown","234df658":"markdown","c9c873c3":"markdown","db45deb9":"markdown","ce95bf52":"markdown","3c9abcb9":"markdown","b169f675":"markdown","89dbd315":"markdown","c7184219":"markdown","8b7a4fc2":"markdown","a91d336b":"markdown","9d629f48":"markdown","4665e969":"markdown","db110535":"markdown"},"source":{"9a039621":"import pandas as pd\nimport numpy as np \nimport tensorflow as tf\n#from sklearn.cross_validation import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\n# from show_confusion_matrix import show_confusion_matrix \n# the above is from http:\/\/notmatthancock.github.io\/2015\/10\/28\/confusion-matrix.html","cdae9fef":"df = pd.read_csv(\"..\/input\/creditcard.csv\")","2a9cda36":"df.head()","eb46614d":"df.describe()","b40940b5":"df.isnull().sum()","52395d89":"print (\"Fraud\")\nprint (df.Time[df.Class == 1].describe())\nprint ()\nprint (\"Normal\")\nprint (df.Time[df.Class == 0].describe())","32e1d662":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n\nbins = 50\n\nax1.hist(df.Time[df.Class == 1], bins = bins)\nax1.set_title('Fraud')\n\nax2.hist(df.Time[df.Class == 0], bins = bins)\nax2.set_title('Normal')\n\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Number of Transactions')\nplt.show()","59c4fb93":"print (\"Fraud\")\nprint (df.Amount[df.Class == 1].describe())\nprint ()\nprint (\"Normal\")\nprint (df.Amount[df.Class == 0].describe())","2ce55556":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n\nbins = 30\n\nax1.hist(df.Amount[df.Class == 1], bins = bins)\nax1.set_title('Fraud')\n\nax2.hist(df.Amount[df.Class == 0], bins = bins)\nax2.set_title('Normal')\n\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.yscale('log')\nplt.show()","46de7360":"df['Amount_max_fraud'] = 1\ndf.loc[df.Amount <= 2125.87, 'Amount_max_fraud'] = 0","83f9e5aa":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,6))\n\nax1.scatter(df.Time[df.Class == 1], df.Amount[df.Class == 1])\nax1.set_title('Fraud')\n\nax2.scatter(df.Time[df.Class == 0], df.Amount[df.Class == 0])\nax2.set_title('Normal')\n\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()","a2455f7b":"#Select only the anonymized features.\nv_features = df.ix[:,1:29].columns","95516cda":"plt.figure(figsize=(12,28*4))\ngs = gridspec.GridSpec(28, 1)\nfor i, cn in enumerate(df[v_features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(df[cn][df.Class == 1], bins=50)\n    sns.distplot(df[cn][df.Class == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('histogram of feature: ' + str(cn))\nplt.show()","379a06e5":"#Drop all of the features that have very similar distributions between the two types of transactions.\ndf = df.drop(['V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8'], axis =1)","9cd5846d":"#Based on the plots above, these features are created to identify values where fraudulent transaction are more common.\ndf['V1_'] = df.V1.map(lambda x: 1 if x < -3 else 0)\ndf['V2_'] = df.V2.map(lambda x: 1 if x > 2.5 else 0)\ndf['V3_'] = df.V3.map(lambda x: 1 if x < -4 else 0)\ndf['V4_'] = df.V4.map(lambda x: 1 if x > 2.5 else 0)\ndf['V5_'] = df.V5.map(lambda x: 1 if x < -4.5 else 0)\ndf['V6_'] = df.V6.map(lambda x: 1 if x < -2.5 else 0)\ndf['V7_'] = df.V7.map(lambda x: 1 if x < -3 else 0)\ndf['V9_'] = df.V9.map(lambda x: 1 if x < -2 else 0)\ndf['V10_'] = df.V10.map(lambda x: 1 if x < -2.5 else 0)\ndf['V11_'] = df.V11.map(lambda x: 1 if x > 2 else 0)\ndf['V12_'] = df.V12.map(lambda x: 1 if x < -2 else 0)\ndf['V14_'] = df.V14.map(lambda x: 1 if x < -2.5 else 0)\ndf['V16_'] = df.V16.map(lambda x: 1 if x < -2 else 0)\ndf['V17_'] = df.V17.map(lambda x: 1 if x < -2 else 0)\ndf['V18_'] = df.V18.map(lambda x: 1 if x < -2 else 0)\ndf['V19_'] = df.V19.map(lambda x: 1 if x > 1.5 else 0)\ndf['V21_'] = df.V21.map(lambda x: 1 if x > 0.6 else 0)","2fcab3de":"#Create a new feature for normal (non-fraudulent) transactions.\ndf.loc[df.Class == 0, 'Normal'] = 1\ndf.loc[df.Class == 1, 'Normal'] = 0","03522eba":"#Rename 'Class' to 'Fraud'.\ndf = df.rename(columns={'Class': 'Fraud'})","0fb1c799":"#492 fraudulent transactions, 284,315 normal transactions.\n#0.172% of transactions were fraud. \nprint(df.Normal.value_counts())\nprint()\nprint(df.Fraud.value_counts())","ad7addb4":"pd.set_option(\"display.max_columns\",101)\ndf.head()","1dceaa3a":"#Create dataframes of only Fraud and Normal transactions.\nFraud = df[df.Fraud == 1]\nNormal = df[df.Normal == 1]","ddc4c01d":"# Set X_train equal to 80% of the fraudulent transactions.\nX_train = Fraud.sample(frac=0.8)\ncount_Frauds = len(X_train)\n\n# Add 80% of the normal transactions to X_train.\nX_train = pd.concat([X_train, Normal.sample(frac = 0.8)], axis = 0)\n\n# X_test contains all the transaction not in X_train.\nX_test = df.loc[~df.index.isin(X_train.index)]","77a4edbb":"#Shuffle the dataframes so that the training is done in a random order.\nX_train = shuffle(X_train)\nX_test = shuffle(X_test)","aa8a948b":"#Add our target features to y_train and y_test.\ny_train = X_train.Fraud\ny_train = pd.concat([y_train, X_train.Normal], axis=1)\n\ny_test = X_test.Fraud\ny_test = pd.concat([y_test, X_test.Normal], axis=1)","ab16dd49":"#Drop target features from X_train and X_test.\nX_train = X_train.drop(['Fraud','Normal'], axis = 1)\nX_test = X_test.drop(['Fraud','Normal'], axis = 1)","38f126f9":"#Check to ensure all of the training\/testing dataframes are of the correct length\nprint(len(X_train))\nprint(len(y_train))\nprint(len(X_test))\nprint(len(y_test))","aa63e302":"'''\nDue to the imbalance in the data, ratio will act as an equal weighting system for our model. \nBy dividing the number of transactions by those that are fraudulent, ratio will equal the value that when multiplied\nby the number of fraudulent transactions will equal the number of normal transaction. \nSimply put: # of fraud * ratio = # of normal\n'''\nratio = len(X_train)\/count_Frauds \n\ny_train.Fraud *= ratio\ny_test.Fraud *= ratio","f7d55ed5":"#Names of all of the features in X_train.\nfeatures = X_train.columns.values\n\n#Transform each feature in features so that it has a mean of 0 and standard deviation of 1; \n#this helps with training the neural network.\nfor feature in features:\n    mean, std = df[feature].mean(), df[feature].std()\n    X_train.loc[:, feature] = (X_train[feature] - mean) \/ std\n    X_test.loc[:, feature] = (X_test[feature] - mean) \/ std","b4da3b7a":"# Split the testing data into validation and testing sets\nsplit = int(len(y_test)\/2)\n\ninputX = X_train.as_matrix()\ninputY = y_train.as_matrix()\ninputX_valid = X_test.as_matrix()[:split]\ninputY_valid = y_test.as_matrix()[:split]\ninputX_test = X_test.as_matrix()[split:]\ninputY_test = y_test.as_matrix()[split:]","0d169288":"# Number of input nodes.\ninput_nodes = 37\n\n# Multiplier maintains a fixed ratio of nodes between each layer.\nmulitplier = 1.5 \n\n# Number of nodes in each hidden layer\nhidden_nodes1 = 18\nhidden_nodes2 = round(hidden_nodes1 * mulitplier)\nhidden_nodes3 = round(hidden_nodes2 * mulitplier)\n\n# Percent of nodes to keep during dropout.\npkeep = tf.placeholder(tf.float32)","3b88a7dc":"# input\nx = tf.placeholder(tf.float32, [None, input_nodes])\n\n# layer 1\nW1 = tf.Variable(tf.truncated_normal([input_nodes, hidden_nodes1], stddev = 0.15))\nb1 = tf.Variable(tf.zeros([hidden_nodes1]))\ny1 = tf.nn.sigmoid(tf.matmul(x, W1) + b1)\n\n# layer 2\nW2 = tf.Variable(tf.truncated_normal([hidden_nodes1, hidden_nodes2], stddev = 0.15))\nb2 = tf.Variable(tf.zeros([hidden_nodes2]))\ny2 = tf.nn.sigmoid(tf.matmul(y1, W2) + b2)\n\n# layer 3\nW3 = tf.Variable(tf.truncated_normal([hidden_nodes2, hidden_nodes3], stddev = 0.15)) \nb3 = tf.Variable(tf.zeros([hidden_nodes3]))\ny3 = tf.nn.sigmoid(tf.matmul(y2, W3) + b3)\ny3 = tf.nn.dropout(y3, pkeep)\n\n# layer 4\nW4 = tf.Variable(tf.truncated_normal([hidden_nodes3, 2], stddev = 0.15)) \nb4 = tf.Variable(tf.zeros([2]))\ny4 = tf.nn.softmax(tf.matmul(y3, W4) + b4)\n\n# output\ny = y4\ny_ = tf.placeholder(tf.float32, [None, 2])","8ba10680":"# Parameters\ntraining_epochs = 5 # should be 2000, it will timeout when uploading\ntraining_dropout = 0.9\ndisplay_step = 1 # 10 \nn_samples = y_train.shape[0]\nbatch_size = 2048\nlearning_rate = 0.005","946694ea":"# Cost function: Cross Entropy\ncost = -tf.reduce_sum(y_ * tf.log(y))\n\n# We will optimize our model via AdamOptimizer\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n\n# Correct prediction if the most likely value (Fraud or Normal) from softmax equals the target value.\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))","bf01911b":"# Note: some code will be commented out below that relate to saving\/checkpointing your model.","911b5e03":"accuracy_summary = [] # Record accuracy values for plot\ncost_summary = [] # Record cost values for plot\nvalid_accuracy_summary = [] \nvalid_cost_summary = [] \nstop_early = 0 # To keep track of the number of epochs before early stopping\n\n# Save the best weights so that they can be used to make the final predictions\n#checkpoint = \"location_on_your_computer\/best_model.ckpt\"\nsaver = tf.train.Saver(max_to_keep=1)\n\n# Initialize variables and tensorflow session\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    \n    for epoch in range(training_epochs): \n        for batch in range(int(n_samples\/batch_size)):\n            batch_x = inputX[batch*batch_size : (1+batch)*batch_size]\n            batch_y = inputY[batch*batch_size : (1+batch)*batch_size]\n\n            sess.run([optimizer], feed_dict={x: batch_x, \n                                             y_: batch_y,\n                                             pkeep: training_dropout})\n\n        # Display logs after every 10 epochs\n        if (epoch) % display_step == 0:\n            train_accuracy, newCost = sess.run([accuracy, cost], feed_dict={x: inputX, \n                                                                            y_: inputY,\n                                                                            pkeep: training_dropout})\n\n            valid_accuracy, valid_newCost = sess.run([accuracy, cost], feed_dict={x: inputX_valid, \n                                                                                  y_: inputY_valid,\n                                                                                  pkeep: 1})\n\n            print (\"Epoch:\", epoch,\n                   \"Acc =\", \"{:.5f}\".format(train_accuracy), \n                   \"Cost =\", \"{:.5f}\".format(newCost),\n                   \"Valid_Acc =\", \"{:.5f}\".format(valid_accuracy), \n                   \"Valid_Cost = \", \"{:.5f}\".format(valid_newCost))\n            \n            # Save the weights if these conditions are met.\n            #if epoch > 0 and valid_accuracy > max(valid_accuracy_summary) and valid_accuracy > 0.999:\n            #    saver.save(sess, checkpoint)\n            \n            # Record the results of the model\n            accuracy_summary.append(train_accuracy)\n            cost_summary.append(newCost)\n            valid_accuracy_summary.append(valid_accuracy)\n            valid_cost_summary.append(valid_newCost)\n            \n            # If the model does not improve after 15 logs, stop the training.\n            if valid_accuracy < max(valid_accuracy_summary) and epoch > 100:\n                stop_early += 1\n                if stop_early == 15:\n                    break\n            else:\n                stop_early = 0\n            \n    print()\n    print(\"Optimization Finished!\")\n    print()   \n    \n#with tf.Session() as sess:\n    # Load the best weights and show its results\n    #saver.restore(sess, checkpoint)\n    #training_accuracy = sess.run(accuracy, feed_dict={x: inputX, y_: inputY, pkeep: training_dropout})\n    #validation_accuracy = sess.run(accuracy, feed_dict={x: inputX_valid, y_: inputY_valid, pkeep: 1})\n    \n    #print(\"Results using the best Valid_Acc:\")\n    #print()\n    #print(\"Training Accuracy =\", training_accuracy)\n    #print(\"Validation Accuracy =\", validation_accuracy)","5b5d60f7":"# Plot the accuracy and cost summaries \nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10,4))\n\nax1.plot(accuracy_summary) # blue\nax1.plot(valid_accuracy_summary) # green\nax1.set_title('Accuracy')\n\nax2.plot(cost_summary)\nax2.plot(valid_cost_summary)\nax2.set_title('Cost')\n\nplt.xlabel('Epochs (x10)')\nplt.show()","66c2c6b8":"# Find the predicted values, then use them to build a confusion matrix\n#predicted = tf.argmax(y, 1)\n#with tf.Session() as sess:  \n#    # Load the best weights\n#    saver.restore(sess, checkpoint)\n#    testing_predictions, testing_accuracy = sess.run([predicted, accuracy], \n#                                                     feed_dict={x: inputX_test, y_:inputY_test, pkeep: 1})\n#    \n#    print(\"F1-Score =\", f1_score(inputY_test[:,1], testing_predictions))\n#    print(\"Testing Accuracy =\", testing_accuracy)\n#    print()\n#    c = confusion_matrix(inputY_test[:,1], testing_predictions)\n#    show_confusion_matrix(c, ['Fraud', 'Normal'])","4dee9c21":"#reload the original dataset\ntsne_data = pd.read_csv(\"..\/input\/creditcard.csv\")","3d0a470c":"#Set df2 equal to all of the fraulent and 10,000 normal transactions.\ndf2 = tsne_data[tsne_data.Class == 1]\ndf2 = pd.concat([df2, tsne_data[tsne_data.Class == 0].sample(n = 10000)], axis = 0)","59ffe807":"#Scale features to improve the training ability of TSNE.\nstandard_scaler = StandardScaler()\ndf2_std = standard_scaler.fit_transform(df2)\n\n#Set y equal to the target values.\ny = df2.ix[:,-1].values","4ed449c1":"tsne = TSNE(n_components=2, random_state=0)\nx_test_2d = tsne.fit_transform(df2_std)","004fc3b1":"#Build the scatter plot with the two types of transactions.\ncolor_map = {0:'red', 1:'blue'}\nplt.figure()\nfor idx, cl in enumerate(np.unique(y)):\n    plt.scatter(x = x_test_2d[y==cl,0], \n                y = x_test_2d[y==cl,1], \n                c = color_map[idx], \n                label = cl)\nplt.xlabel('X in t-SNE')\nplt.ylabel('Y in t-SNE')\nplt.legend(loc='upper left')\nplt.title('t-SNE visualization of test data')\nplt.show()","f165789c":"#Set df_used to the fraudulent transactions' dataset.\ndf_used = Fraud\n\n#Add 10,000 normal transactions to df_used.\ndf_used = pd.concat([df_used, Normal.sample(n = 10000)], axis = 0)","aa148185":"#Scale features to improve the training ability of TSNE.\ndf_used_std = standard_scaler.fit_transform(df_used)\n\n#Set y_used equal to the target values.\ny_used = df_used.ix[:,-1].values","c9ec43c5":"x_test_2d_used = tsne.fit_transform(df_used_std)","cd40c360":"color_map = {1:'red', 0:'blue'}\nplt.figure()\nfor idx, cl in enumerate(np.unique(y_used)):\n    plt.scatter(x=x_test_2d_used[y_used==cl,0], \n                y=x_test_2d_used[y_used==cl,1], \n                c=color_map[idx], \n                label=cl)\nplt.xlabel('X in t-SNE')\nplt.ylabel('Y in t-SNE')\nplt.legend(loc='upper left')\nplt.title('t-SNE visualization of test data')\nplt.show()","5202ed7e":"## Exploring the Data","234df658":"First we are going to use t-SNE with the original data, then with the data we used for training our neural network. I expect\/hope that the second scatter plot will show a clearer contrast between the normal and the fraudulent transactions. If this is the case, its signals that the work done during the feature engineering stage of the analysis was beneficial to helping the neural network understand the data.","c9c873c3":"The goal for this analysis is to predict credit card fraud in the transactional data. I will be using tensorflow to build the predictive model, and t-SNE to visualize the dataset in two dimensions at the end of this analysis. If you would like to learn more about the data, visit: https:\/\/www.kaggle.com\/dalpozz\/creditcardfraud.\n\nThe sections of this analysis include: \n\n - Exploring the Data\n - Building the Neural Network \n - Visualizing the Data with t-SNE.","db45deb9":"## Visualizing the Data with t-SNE","ce95bf52":"The 'Time' feature looks pretty similar across both types of transactions. You could argue that fraudulent transactions are more uniformly distributed, while normal transactions have a cyclical distribution. This could make it easier to detect a fraudulent transaction during at an 'off-peak' time.\n\nNow let's see if the transaction amount differs between the two types.","3c9abcb9":"Although the neural network can detect most of the fraudulent transactions (82.93%), there are still some that got away. About 0.10% of normal transactions were classified as fraudulent, which can unfortunately add up very quickly given the large number of credit card transactions that occur each minute\/hour\/day. Nonetheless, this models performs reasonably well and I expect that if we had more data, and if the features were not pre-transformed, we could have created new features, and built a more useful neural network.","b169f675":"The are two main groupings of fraudulent transactions, while the remaineder are mixed within the rest of the data.\n\nNote: I have only used 10,000 of the 284,315 normal transactions for this visualization. I would have liked to of used more, but my laptop crashes if many more than 10,000 transactions are included. With only 3.15% of the data being used, there should be some accuracy to this plot, but I am confident that the layout would look different if all of the transactions were included.","89dbd315":"No missing values, that makes things a little easier.\n\nLet's see how time compares across fraudulent and normal transactions.","c7184219":"The data is mostly transformed from its original form, for confidentiality reasons.","8b7a4fc2":"## Train the Neural Net","a91d336b":"Most transactions are small amounts, less than $100. Fraudulent transactions have a maximum value far less than normal transactions, $2,125.87 vs $25,691.16.\n\nLet's compare Time with Amount and see if we can learn anything new.","9d629f48":"# Predicting Credit Card Fraud","4665e969":"Nothing too useful here.\n\nNext, let's take a look at the anonymized features.","db110535":"It appears that the work we did in the feature engineering stage of this analysis has been for the best. We can see that the fraudulent transactions are all part of a group of points. This suggests that it is easier for a model to identify the fraudulent transactions in the testing data, and to learn about the traits of the fraudulent transactions in the training data."}}