{"cell_type":{"f7d5f1b9":"code","497ef5a0":"code","e5b6638c":"code","8f3f9f7d":"code","0e84e8d9":"code","5883ebe3":"code","fa3c99fc":"code","79569c2e":"code","6375a9c9":"code","0daafc83":"code","ec250a60":"code","8a2cfa66":"code","bf113d7b":"code","20980578":"code","9b1d454e":"code","4c6ab4b8":"code","652b51a2":"code","49ced6d8":"code","5d0f31ea":"code","f9343b7a":"code","7c4eab9e":"code","6db2ce35":"code","3c0fe905":"code","ab81131d":"code","2d1f0eee":"code","81e68a2a":"code","9e074f1a":"code","d2b56a8c":"code","ff4e81d2":"code","84b7dc82":"code","e217eb4a":"code","7811d3ce":"code","c91d25a8":"code","5a30c720":"code","2c6b5a9d":"code","bf036b8c":"code","ef7cc5f7":"code","27c9e1f8":"code","4106cbb0":"code","510e4856":"code","dbea5dae":"code","9841d57d":"code","2122c450":"code","e493a69b":"code","a2066f95":"code","2dda727a":"code","122fd042":"code","1699cae0":"code","333d4b20":"code","a7e3ff72":"code","3da819b7":"code","d15c3b3b":"code","e819925c":"markdown","efbc3ffd":"markdown","bbee2e9f":"markdown","f1b2362f":"markdown","a7ccfa82":"markdown","09dd00d0":"markdown","ab50faba":"markdown","12e4473f":"markdown","ab5cddfe":"markdown","cf3ccf58":"markdown","63a5e6cd":"markdown","64122b90":"markdown","5d75efb4":"markdown","f81e202a":"markdown","ce013283":"markdown","93a2351e":"markdown","fe327216":"markdown","5a03bca1":"markdown","e6ba1548":"markdown"},"source":{"f7d5f1b9":"# Import The Necessary Packages\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn import metrics\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.model_selection import GridSearchCV\n\nfrom plotly.offline import init_notebook_mode,iplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","497ef5a0":"data = \"..\/input\/housesalesprediction\/kc_house_data.csv\"\ndf = pd.read_csv(data)\ndf.head(10)","e5b6638c":"print(\"Data Shape: \", df.shape)","8f3f9f7d":"df.info()","0e84e8d9":"df.describe().T","5883ebe3":"plt.subplots(figsize=(17,14))\nsns.heatmap(df.corr(), annot=True, linewidth=0.5, linecolor=\"Black\", fmt=\"1.1f\")\nplt.title(\"Attributes Correlation\", fontsize=28)\nplt.show()","fa3c99fc":"hist1 = [go.Histogram(x=df.grade,marker=dict(color='rgb(102, 0, 102)'))]\nhistlayout1 = go.Layout(title=\"Grade Counts of Houses\",xaxis=dict(title=\"Grades\"),yaxis=dict(title=\"Counts\"))\nhistfig1 = go.Figure(data=hist1,layout=histlayout1)\niplot(histfig1)","79569c2e":"hist2 = [go.Histogram(x=df.yr_built,xbins=dict(start=np.min(df.yr_built),size=1,end=np.max(df.yr_built)),marker=dict(color='rgb(0,102,0)'))]\n\nhistlayout2 = go.Layout(title=\"Built Year Counts of Houses\",xaxis=dict(title=\"Years\"),yaxis=dict(title=\"Built Counts\"))\n\nhistfig2 = go.Figure(data=hist2,layout=histlayout2)\n\niplot(histfig2)","6375a9c9":"v21 = [go.Box(y=df.bedrooms,name=\"Bedrooms\",marker=dict(color=\"rgba(51,0,0,0.9)\"),hoverinfo=\"name+y\")]\nv22 = [go.Box(y=df.bathrooms,name=\"Bathrooms\",marker=dict(color=\"rgba(0,102,102,0.9)\"),hoverinfo=\"name+y\")]\nv23 = [go.Box(y=df.floors,name=\"Floors\",marker=dict(color=\"rgba(204,0,102,0.9)\"),hoverinfo=\"name+y\")]\n\nlayout2 = go.Layout(title=\"Bedrooms,Bathrooms and Floors\",yaxis=dict(range=[0,13])) #I hate 33 bedroom\n\nfig2 = go.Figure(data=v21+v22+v23,layout=layout2)\niplot(fig2)","0daafc83":"import plotly.express as px\n\ndataplus = df[np.logical_and(df.grade >= 7,df.yr_built >= 2000)] \n#list lat and long\nlats = list(dataplus.lat.values)\nlongs = list(dataplus.long.values)\n\nfig = px.scatter_mapbox(lat=lats, lon=longs, zoom=10, height=500)\nfig.update_layout(mapbox_style=\"open-street-map\")\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()","ec250a60":"models_and_scores = []","8a2cfa66":"X = df[['sqft_living']].values\ny = df.price.values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2,random_state=42)","bf113d7b":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\nmodel_score = lr.score(X_test,y_test)\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\n\nprint(\"Model Score            : \", model_score*100)\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","20980578":"new_df = df[['sqft_living15','lat','sqft_basement', 'sqft_above', 'grade','view','sqft_basement', 'sqft_living', 'bathrooms', 'floors', 'waterfront','yr_built']]\nX = new_df.values\ny = df.price.values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2, random_state=42)","9b1d454e":"multi_lr_model = LinearRegression() # model\n\nmulti_lr_model.fit(X_train, y_train) # fit\n\ny_pred = multi_lr_model.predict(X_test) # prediction\n\nmodel_score = multi_lr_model.score(X_test,y_test)\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\nmodels_and_scores.append([\"Multiple Linear\",r_square])\n\nprint(\"Model Score            : \", model_score*100)\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","4c6ab4b8":"X1 = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]\ny1 = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]\n\nmymodel = np.poly1d(np.polyfit(X1, y1, 3))\nmyline = np.linspace(1, 22, 100)\n\nplt.scatter(X1, y1)\nplt.plot(myline, mymodel(myline))\nplt.show()","652b51a2":"speed = mymodel(17)\nprint(speed)","49ced6d8":"new_df = df[['sqft_living15','lat','sqft_basement', 'sqft_above', 'grade','view','sqft_basement', 'sqft_living', 'bathrooms', 'floors', 'waterfront','yr_built']]\nX = new_df.values\ny = df.price.values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2, random_state=42)","5d0f31ea":"from sklearn.tree import DecisionTreeRegressor\n\ndtr = DecisionTreeRegressor(random_state=42)\ndtr.fit(X_train, y_train)\ny_pred = dtr.predict(X_test)\n\nmodel_score = dtr.score(X_test,y_test)\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\n\nprint(\"Model Score            : \", model_score*100)\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","f9343b7a":"# Model Tuning\nparam_grid = {\n    'max_depth': list(np.arange(1,30)),\n    'min_samples_split': list(np.arange(1,10))\n}\n\ndtr_model = DecisionTreeRegressor(random_state=42)\ndtr_cv_model = GridSearchCV(dtr_model, param_grid, cv=10, n_jobs=-1, verbose=2)\ndtr_cv_model.fit(X_train, y_train)\nprint(\"Best Params: \", dtr_cv_model.best_params_)\nprint(\"Best Score : \", dtr_cv_model.best_score_)\n\ny_pred = dtr_cv_model.predict(X_test)\n\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\nmodels_and_scores.append(['Decision Tree', dtr_cv_model.best_score_])\n\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","7c4eab9e":"columns = ['sqft_living15','lat','sqft_basement', 'sqft_above', 'grade','view','sqft_basement', 'sqft_living', 'bathrooms', 'floors', 'waterfront','yr_built']\n\nImportance = pd.DataFrame({\n    'Importance': dtr_cv_model.best_estimator_.feature_importances_*100}, index=columns)\nImportance.sort_values(by=\"Importance\", axis=0, ascending=True).plot(kind=\"barh\", color=\"b\")\nplt.xlabel(\"Variable Importance\")\nplt.gca().legend_ = None","6db2ce35":"new_df = df[['sqft_living15','lat','sqft_basement', 'sqft_above', 'grade','view','sqft_basement', 'sqft_living', 'bathrooms', 'floors', 'waterfront','yr_built']]\nX = new_df.values\ny = df.price.values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2, random_state=42)","3c0fe905":"from sklearn.ensemble import RandomForestRegressor\n\n\nrfr = RandomForestRegressor(random_state=42)\nrfr.fit(X_train, y_train)\ny_pred = rfr.predict(X_test)\n\nmodel_score = rfr.score(X_test,y_test)\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\n\nprint(\"Model Score            : \", model_score*100)\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","ab81131d":"# Model Tuning\nparam_grid = {\n    'max_depth': [1,5,10,30,50,100]\n}\n\nrfr_model = RandomForestRegressor(random_state=42)\nrfr_cv_model = GridSearchCV(rfr_model, param_grid, cv=10, n_jobs=-1, verbose=2)\nrfr_cv_model.fit(X_train, y_train)\nprint(\"Best Params: \", rfr_cv_model.best_params_)\nprint(\"Best Score : \", rfr_cv_model.best_score_)\n\ny_pred = rfr_cv_model.predict(X_test)\n\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\nmodels_and_scores.append(['Random Forest', rfr_cv_model.best_score_])\n\n\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","2d1f0eee":"columns = ['sqft_living15','lat','sqft_basement', 'sqft_above', 'grade','view','sqft_basement', 'sqft_living', 'bathrooms', 'floors', 'waterfront','yr_built']\n\nImportance = pd.DataFrame({\n    'Importance': rfr_cv_model.best_estimator_.feature_importances_*100}, index=columns)\nImportance.sort_values(by=\"Importance\", axis=0, ascending=True).plot(kind=\"barh\", color=\"b\")\nplt.xlabel(\"Variable Importance\")\nplt.gca().legend_ = None","81e68a2a":"new_df = df[['sqft_living15','lat','sqft_basement', 'sqft_above', 'grade','view','sqft_basement', 'sqft_living', 'bathrooms', 'floors', 'waterfront','yr_built']]\nX = new_df.values\ny = df.price.values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2, random_state=42)","9e074f1a":"from sklearn.svm import SVR\n\nsvr = SVR(kernel='linear')\nsvr.fit(X_train, y_train)\ny_pred = svr.predict(X_test)\n\nmodel_score = svr.score(X_test,y_test)\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\n\nprint(\"Model Score            : \", model_score*100)\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","d2b56a8c":"# Model Tuning\nparam_grid = {\n    'C': [0.1, 0.5, 1, 5]\n}\n\nsvr = SVR(kernel='linear')\nsvr_cv_model = GridSearchCV(svr, param_grid, cv=5, n_jobs=-1, verbose=2)\nsvr_cv_model.fit(X_train, y_train)\nprint(\"Best Params: \", svr_cv_model.best_params_)\nprint(\"Best Score : \", svr_cv_model.best_score_)\n\ny_pred = svr_cv_model.predict(X_test)\n\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\nmodels_and_scores.append(['SVM', svr_cv_model.best_score_])\n\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","ff4e81d2":"new_df = df[['sqft_living15','lat','sqft_basement', 'sqft_above', 'grade','view','sqft_basement', 'sqft_living', 'bathrooms', 'floors', 'waterfront','yr_built']]\nX = new_df.values\ny = df.price.values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2, random_state=42)","84b7dc82":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n                           colsample_bytree=1, max_depth=7)\nxgb.fit(X_train,y_train)\ny_pred = xgb.predict(X_test)\n\nmodel_score = xgb.score(X_test,y_test)\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\n\nprint(\"Model Score            : \", model_score*100)\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","e217eb4a":"# Model Tuning\nparam_grid = {\n    'learning_rate': [0.1, 0.01, 0.5],\n    'max_depth': [2,3,5],\n    'n_estimators': [100, 200, 300],\n    'colsample_bytree': [0.4, 0.7, 1]\n}\n\nxgb_model = XGBRegressor()\nxgb_cv_model = GridSearchCV(xgb_model, param_grid, cv=5, n_jobs=-1, verbose=2)\nxgb_cv_model.fit(X_train, y_train)\nprint(\"Best Params: \", xgb_cv_model.best_params_)\nprint(\"Best Score : \", xgb_cv_model.best_score_)\n\ny_pred = xgb_cv_model.predict(X_test)\n\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\nmodels_and_scores.append(['XGB', xgb_cv_model.best_score_])\n\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","7811d3ce":"columns = ['sqft_living15','lat','sqft_basement', 'sqft_above', 'grade','view','sqft_basement', 'sqft_living', 'bathrooms', 'floors', 'waterfront','yr_built']\n\nImportance = pd.DataFrame({\n    'Importance': xgb_cv_model.best_estimator_.feature_importances_*100}, index=columns)\nImportance.sort_values(by=\"Importance\", axis=0, ascending=True).plot(kind=\"barh\", color=\"b\")\nplt.xlabel(\"Variable Importance\")\nplt.gca().legend_ = None","c91d25a8":"new_df = df[['sqft_living15','lat','sqft_basement', 'sqft_above', 'grade','view','sqft_basement', 'sqft_living', 'bathrooms', 'floors', 'waterfront','yr_built']]\nX = new_df.values\ny = df.price.values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2, random_state=42)","5a30c720":"from sklearn.linear_model import Lasso\n\nlass = Lasso()\nlass.fit(X_train, y_train)\ny_pred = lass.predict(X_test)\n\nmodel_score = lass.score(X_test,y_test)\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\n\nprint(\"Model Score            : \", model_score*100)\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","2c6b5a9d":"# Model Tuning\nfrom sklearn.linear_model import LassoCV\n\nalphas = 10**np.linspace(10,-1,100)*0.5\n\nlass_cv_model = LassoCV(alphas=alphas, cv=10, n_jobs=-1, verbose=2, max_iter=100000)\nlass_cv_model.fit(X_train, y_train)\n\ny_pred = lass_cv_model.predict(X_test)\n\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\nmodels_and_scores.append(['Lasso', r_square])\n\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","bf036b8c":"new_df = df[['sqft_living15','lat','sqft_basement', 'sqft_above', 'grade','view','sqft_basement', 'sqft_living', 'bathrooms', 'floors', 'waterfront','yr_built']]\nX = new_df.values\ny = df.price.values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2, random_state=42)","ef7cc5f7":"from sklearn.linear_model import Ridge\n\nridge = Ridge()\nridge.fit(X_train, y_train)\ny_pred = ridge.predict(X_test)\n\nmodel_score = ridge.score(X_test,y_test)\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\nmodels_and_scores.append(['Ridge', r_square])\n\nprint(\"Model Score            : \", model_score*100)\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","27c9e1f8":"new_df = df[['sqft_living15','lat','sqft_basement', 'sqft_above', 'grade','view','sqft_basement', 'sqft_living', 'bathrooms', 'floors', 'waterfront','yr_built']]\nX = new_df.values\ny = df.price.values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2, random_state=42)","4106cbb0":"from sklearn.linear_model import ElasticNet\n\nelasticN = ElasticNet()\nelasticN.fit(X_train, y_train)\ny_pred = elasticN.predict(X_test)\n\nmodel_score = elasticN.score(X_test,y_test)\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\nmodels_and_scores.append(['ElasticNet', r_square])\n\nprint(\"Model Score            : \", model_score*100)\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","510e4856":"new_df = df[['sqft_living15','lat','sqft_basement', 'sqft_above', 'grade','view','sqft_basement', 'sqft_living', 'bathrooms', 'floors', 'waterfront','yr_built']]\nX = new_df.values\ny = df.price.values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2, random_state=42)","dbea5dae":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nknn = KNeighborsClassifier(n_neighbors=11)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nmodel_score = knn.score(X_test,y_test)\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\nmodels_and_scores.append(['KNN', r_square])\n\nprint(\"Model Score            : \", model_score*100)\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","9841d57d":"new_df = df[['sqft_living15','lat','sqft_basement', 'sqft_above', 'grade','view','sqft_basement', 'sqft_living', 'bathrooms', 'floors', 'waterfront','yr_built']]\nX = new_df.values\ny = df.price.values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2, random_state=42)","2122c450":"from sklearn.ensemble import GradientBoostingRegressor\n\ngbm = GradientBoostingRegressor(random_state=42)\ngbm.fit(X_train, y_train)\ny_pred = gbm.predict(X_test)\n\nmodel_score = gbm.score(X_test,y_test)\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\n\nprint(\"Model Score            : \", model_score*100)\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","e493a69b":"# Model Tuning\n\"\"\"\nparam_grid = {\n    'learning_rate': [0.001, 0.01, 0.1],\n    'max_depth': [3,5,8],\n    'n_estimators': [100,200,500],\n    'subsample': [1, 0.5, 0.8 ],\n    'loss': ['ls', 'lad', 'quantile']\n}\n\"\"\"\nparam_grid = {\n    'learning_rate': [0.001, 0.01],\n    'max_depth': [3,5],\n    'n_estimators': [150,200,250],\n    'subsample': [1, 0.5]\n}\n\ngbm_model = GradientBoostingRegressor(random_state=42)\ngbm_cv_model = GridSearchCV(gbm_model, param_grid, cv=10, n_jobs=-1, verbose=2)\ngbm_cv_model.fit(X_train, y_train)\nprint(\"Best Params: \", gbm_cv_model.best_params_)\nprint(\"Best Score : \", gbm_cv_model.best_score_)\n\ny_pred = gbm_cv_model.predict(X_test)\n\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\nmodels_and_scores.append(['GBM',  gbm_cv_model.best_score_])\n\nprint(\"Model Score            : \", model_score*100)\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","a2066f95":"columns = ['sqft_living15','lat','sqft_basement', 'sqft_above', 'grade','view','sqft_basement', 'sqft_living', 'bathrooms', 'floors', 'waterfront','yr_built']\n\nImportance = pd.DataFrame({\n    'Importance': gbm_cv_model.best_estimator_.feature_importances_*100}, index=columns)\nImportance.sort_values(by=\"Importance\", axis=0, ascending=True).plot(kind=\"barh\", color=\"b\")\nplt.xlabel(\"Variable Importance\")\nplt.gca().legend_ = None","2dda727a":"new_df = df[['sqft_living15','lat','sqft_basement', 'sqft_above', 'grade','view','sqft_basement', 'sqft_living', 'bathrooms', 'floors', 'waterfront','yr_built']]\nX = new_df.values\ny = df.price.values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2, random_state=42)","122fd042":"from lightgbm import LGBMRegressor\n\nlgbm = LGBMRegressor(random_state=42)\nlgbm.fit(X_train, y_train)\ny_pred = lgbm.predict(X_test)\n\nmodel_score = lgbm.score(X_test,y_test)\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\n\nprint(\"Model Score            : \", model_score*100)\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","1699cae0":"# Model Tuning\nparam_grid = {\n    \"learning_rate\": [0.01, 0.1, 0.5, 1],\n    \"n_estimators\": [20, 40, 100, 500, 1000],\n    \"max_depth\": [-1,1,3,5]\n}\n\nlgbm_model = LGBMRegressor(random_state=42)\nlgbm_cv_model = GridSearchCV(lgbm_model, param_grid, cv=10, n_jobs=-1, verbose=2)\nlgbm_cv_model.fit(X_train, y_train)\nprint(\"Best Params: \", lgbm_cv_model.best_params_)\nprint(\"Best Score : \", lgbm_cv_model.best_score_)\n\ny_pred = lgbm_cv_model.predict(X_test)\n\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\nmodels_and_scores.append(['LightGBM',  lgbm_cv_model.best_score_])\n\nprint(\"Model Score            : \", model_score*100)\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","333d4b20":"columns = ['sqft_living15','lat','sqft_basement', 'sqft_above', 'grade','view','sqft_basement', 'sqft_living', 'bathrooms', 'floors', 'waterfront','yr_built']\n\nImportance = pd.DataFrame({\n    'Importance': lgbm_cv_model.best_estimator_.feature_importances_*100}, index=columns)\nImportance.sort_values(by=\"Importance\", axis=0, ascending=True).plot(kind=\"barh\", color=\"b\")\nplt.xlabel(\"Variable Importance\")\nplt.gca().legend_ = None","a7e3ff72":"from catboost import CatBoostRegressor\n\ncatb = CatBoostRegressor(random_state=42)\ncatb.fit(X_train, y_train)\ny_pred = catb.predict(X_test)\n\nmodel_score = catb.score(X_test,y_test)\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\n\nprint(\"Model Score            : \", model_score*100)\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","3da819b7":"# Model Tuning\nparam_grid = {\n    \"iterations\": [200, 500, 1000],\n    'learning_rate': [0.01, 0.1],\n    \"depth\": [3,6,8]\n}\n\ncatb_model = LGBMRegressor(random_state=42)\ncatb_cv_model = GridSearchCV(catb_model, param_grid, cv=10, n_jobs=-1, verbose=2)\ncatb_cv_model.fit(X_train, y_train)\nprint(\"Best Params: \", catb_cv_model.best_params_)\nprint(\"Best Score : \", catb_cv_model.best_score_)\n\ny_pred = catb_cv_model.predict(X_test)\n\nr_square = metrics.r2_score(y_test, y_pred)\nmse = metrics.mean_squared_error(y_test, y_pred)\nmae = metrics.median_absolute_error(y_test, y_pred)\nev = metrics.explained_variance_score(y_test, y_pred)\nmodels_and_scores.append(['CatBoost',  catb_cv_model.best_score_])\n\nprint(\"Model Score            : \", model_score*100)\nprint(\"R Square               : \", r_square*100)\nprint(\"Mean Squared Error     : \", mse)\nprint(\"Root Mean Squared Error: \", mse**(1\/2))\nprint(\"Median Absolute Error  : \", mae)\nprint(\"Explained Variance     : \", ev)","d15c3b3b":"models, scores = [], []\n\nfor x in models_and_scores:\n    models.append(x[0])\n    scores.append(x[1])\n\nplt.figure(figsize=(15,10))\nax = sns.barplot(x=scores, y=models, palette=\"Blues_d\")\nax.set_title(\"Models And Scores - Comparison\")\nax.set_ylabel(\"Models\")\nax.set_ylabel(\"Scores\")\nplt.show()","e819925c":"## K-Nearest Neighbors Regression (KNN)","efbc3ffd":"# Preapare Data","bbee2e9f":"Machine learning briefly; It is a scientific technique where computers learn how to solve a problem without programming.\n- Simple Lineer Regression\n- Multiple Lineer Regression\n- Polynomial  Lineer Regression\n- Decision Tree Regression\n- Random Forest Regression\n- Support Vector Regression (SVR)\n- XGBoost Regression\n- Lasso Regression\n- Ridge Regression\n- ElasticNet Regression\n- K-Nearest Neighbors Regression (KNN)\n- Gradient Boosting Machines (GBM)\n- LightGBM Regression\n- CatBoost","f1b2362f":"## And Finally - Comparison","a7ccfa82":"## Gradient Boosting Machines (GBM)","09dd00d0":"R Square               :  68.4670734001445 <br>\nMean Squared Error     :  47670441712.02933 <br>\nRoot Mean Squared Error:  218335.6171402855 <br>\nMedian Absolute Error  :  89123.50729138404 <br>\nExplained Variance     :  0.6847409925653468 ","ab50faba":"## Lasso Regression","12e4473f":"# XGBoost","ab5cddfe":"## Polynomial Linear Regression","cf3ccf58":"## Random Forest Regression","63a5e6cd":"## Multiple Linear Regression","64122b90":"## CatBoost","5d75efb4":"## ElasticNet Regression","f81e202a":"## Simple Linear Regression","ce013283":"## Decision Tree Regression","93a2351e":"## Support Vector Regression","fe327216":"Model Score            :  81.43669400244507 <br>\nR Square               :  81.43669400244507 <br>\nMean Squared Error     :  28063395693.283432 <br>\nRoot Mean Squared Error:  167521.3290697141 <br>\nMedian Absolute Error  :  48228.138245502836 <br>\nExplained Variance     :  0.8144066661447161","5a03bca1":"## LightGBM","e6ba1548":"## Ridge Regression"}}