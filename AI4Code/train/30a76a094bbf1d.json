{"cell_type":{"845d03ab":"code","d2d273f1":"markdown"},"source":{"845d03ab":"\n\n# Import the required libraries\n\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nnp.set_printoptions(threshold=sys.maxsize,precision=3)\nfrom IPython.display import display\npd.options.display.max_columns = None\nfrom sklearn.metrics import confusion_matrix as cm\nfrom sklearn.metrics import accuracy_score as accuracy\nimport warnings\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n# Defining a function which will be used later to analyze the accuracy of the model\ndef PrintAccuracy(classifier_name, y_test, y_pred):\n    print(f'Accuracy using {classifier_name} is :- ', (accuracy(y_test,y_pred)*100))\n\n# Ignore python warnings\nwarnings.filterwarnings('ignore')\n\n# Import the data set\ndataset = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')  # This is Dataset Breast Cancer Wisconsin dataset\n\n# Check if there are any missing values\ndataset.isnull().sum()  # Here there are no missing values observed.\n\n# Get the dependent and independent variables\nx = dataset.iloc[:,2:32].values\ny = dataset.iloc[:,1].values.reshape(-1,1)\n\n# Encode diagnosis column\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\ndiagnosis_encoder = LabelEncoder()\ny = diagnosis_encoder.fit_transform(y)\n# diagnosis_encoder = OneHotEncoder()\n# y = diagnosis_encoder.fit_transform(y.reshape(-1,1)).toarray() # TODO :- Check if there is any other better way\n\n# Split the data into training and test set\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x, y, test_size = 0.25)\n\n# Fit the decision tree classifier in the data set\nfrom sklearn.tree import DecisionTreeClassifier\ntree_classifier = DecisionTreeClassifier()\ntree_classifier.fit(x_train,y_train)\n\n# Make predictions\ntree_pred = tree_classifier.predict(x_test)\nPrintAccuracy(classifier_name= 'Decision Tree', y_pred= tree_pred, y_test= y_test)  # 92.3\n\n# Try fitting Logistic Regression to the data\nfrom sklearn.linear_model import LogisticRegression\nlogistic_classifier = LogisticRegression()\nlogistic_classifier.fit(x_train,y_train)\n\n# Make predictions\nlogistic_pred = logistic_classifier.predict(x_test)\nPrintAccuracy(classifier_name= 'Logistic Regression', y_pred= logistic_pred, y_test= y_test) # 95.8\n\n# Fit SVM into the dataset\nfrom sklearn.svm import SVC\nsvc_classifier = SVC(kernel= 'rbf')\nsvc_classifier.fit(x_train, y_train)\n\n# Make predictions\nsvc_pred = svc_classifier.predict(x_test)\nPrintAccuracy(classifier_name= 'Support Vector Machine', y_pred= svc_pred, y_test= y_test) # 62.23\n\n# Fit Naive Bayes to the dataset\nfrom sklearn.naive_bayes import GaussianNB\nnb_classifier = GaussianNB()\nnb_classifier.fit(x_train, y_train)\n\n# Make predictions\nnb_pred = nb_classifier.predict(x_test)\nPrintAccuracy(classifier_name= 'Naive Bayes', y_pred= nb_pred, y_test= y_test) # 94.405\n\n# Fit KNN to the dataset\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_classifier = KNeighborsClassifier(n_neighbors= 5 , metric = 'minkowski', p = 2)\nknn_classifier.fit(x_train, y_train)\n\n# Make predictions\nknn_pred = knn_classifier.predict(x_test)\nPrintAccuracy(classifier_name= 'KNN', y_pred= knn_pred, y_test= y_test) # 92.307\n\n# N- Fold Cross Validations for the different model used\n\n# Utility function\nfrom sklearn.model_selection import cross_val_score\ndef ApplyCrossValidation(regressor, x_train, y_train):\n    accuracies = cross_val_score(estimator = regressor, X = x_train, y = y_train, cv = 10)\n    return accuracies.mean()*100\n\n# 1. Decision Tree Classifier\nprint(f'Average accuracy of Decision Tree Classifier after applying 10- Fold CV is {ApplyCrossValidation(tree_classifier, x_train, y_train)}')\n# 91.07203630175837\n\n# 2. Logistic Regression\nprint(f'Average accuracy of Logistic Regression after applying 10- Fold CV is {ApplyCrossValidation(logistic_classifier, x_train, y_train)}')\n# 95.25808281338628\n\n# 3. SVM\nprint(f'Average accuracy of SVM after applying 10- Fold CV is {ApplyCrossValidation(svc_classifier, x_train, y_train)}')\n# 62.91548496880317\n\n# 4. Naive Bayes\nprint(f'Average accuracy of Naive Bayes after applying 10- Fold CV is {ApplyCrossValidation(nb_classifier, x_train, y_train)}')\n# 93.85138967668748\n\n# 5. KNN\nprint(f'Average accuracy of KNN after applying 10- Fold CV is {ApplyCrossValidation(knn_classifier, x_train, y_train)}')\n# 93.85138967668748","d2d273f1":"This note book is created to give a basic idea on how to apply different classification algorithm to a dataset\nand evaluate their respective performance.\n\nData Set used :- Breast Cancer Wisconsin (Diagnostic) Data Set\n\n"}}