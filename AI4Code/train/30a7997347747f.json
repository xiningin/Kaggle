{"cell_type":{"61ef0043":"code","a22e2217":"code","e7c1bb1d":"code","56c4d7d6":"code","60a6715c":"code","2b2519f1":"code","a65fb093":"code","ba8b3d32":"code","f6eddcce":"code","5e2c9d59":"code","95dd50f9":"code","7b0bd05d":"code","6a4814de":"code","63dcc28c":"code","5d7dc2bb":"code","d88d3533":"code","d927c07d":"code","f8f33ca2":"code","e21c9121":"code","8251026d":"code","b3cef23f":"code","64b92f82":"code","622494ce":"code","dc08a2eb":"code","40eeac06":"code","794ff45a":"code","434f8afe":"code","ea34ceee":"code","89c6a0c6":"code","deb28479":"code","81eae778":"code","65a971f8":"code","837235bc":"code","b079b4b1":"code","2b955066":"code","edf09161":"code","6fd48bda":"code","9d70d6d1":"code","e8df1e71":"code","332441c8":"code","a3a091f9":"code","7f7ff4bb":"code","e860ed14":"code","2c4b23e9":"code","6193acd5":"code","57088ae0":"code","88c01cfc":"code","5f4cab5b":"code","2d7939d6":"code","74ab644f":"code","4097e3e1":"code","9e9352bf":"code","d8b90291":"code","371e342a":"code","df7ab695":"code","04cfe6f2":"code","c51c39d3":"code","1052ac36":"code","9e653947":"code","97b34b07":"code","61f7e084":"code","233f4b6c":"code","49c1aa60":"code","401db5ea":"code","e3ffd237":"code","e86a27e5":"code","8525b06d":"code","d5fe07b0":"code","3c600aea":"code","a64bba79":"code","a71d6d82":"code","ec67e2af":"code","7ea9b1d2":"code","dc6124b4":"code","9e04a4c0":"code","e0ec5138":"markdown","ea76bfcd":"markdown","0db2f839":"markdown","a56a4b8d":"markdown","c5a4989b":"markdown","77bf4dee":"markdown","400a49dd":"markdown","1b2d8da8":"markdown","edda7f89":"markdown","eb3fe2d4":"markdown","be579b40":"markdown","d87f5974":"markdown","ba9183e8":"markdown","6f22aff9":"markdown","e06fdc37":"markdown","6d1d47c1":"markdown","4ed8b85d":"markdown","e2e3a55b":"markdown"},"source":{"61ef0043":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set()","a22e2217":"pip install xlrd","e7c1bb1d":"pip install openpyxl","56c4d7d6":"train_data =  pd.read_excel('..\/input\/flight-fare-prediction-mh\/Data_Train.xlsx', engine='openpyxl')","60a6715c":"train_data.head()","2b2519f1":"train_data.info()","a65fb093":"train_data[\"Duration\"].value_counts()","ba8b3d32":"train_data.dropna(inplace = True)","f6eddcce":"train_data.isnull().sum()","5e2c9d59":"train_data[\"Journey_day\"] = pd.to_datetime(train_data.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day","95dd50f9":"train_data[\"Journey_month\"] = pd.to_datetime(train_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month","7b0bd05d":"train_data.head()","6a4814de":"# Since we have converted Date_of_Journey column into integers, Now we can drop the date of journey column\n\ntrain_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)","63dcc28c":"\n# Similar to Date_of_Journey we can extract values from Dep_Time\n\n# Extracting Hours\ntrain_data[\"Dep_hour\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.hour\n\n# Extracting Minutes\ntrain_data[\"Dep_min\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.minute\n\n# Now we can drop Dep_Time \ntrain_data.drop([\"Dep_Time\"], axis = 1, inplace = True)","5d7dc2bb":"train_data.head()","d88d3533":"# Arrival time is when the plane pulls up to the gate.\n# Similar to Date_of_Journey we can extract values from Arrival_Time\n\n# Extracting Hours\ntrain_data[\"Arrival_hour\"] = pd.to_datetime(train_data.Arrival_Time).dt.hour\n\n# Extracting Minutes\ntrain_data[\"Arrival_min\"] = pd.to_datetime(train_data.Arrival_Time).dt.minute\n\n# Now we can drop Arrival_Time \ntrain_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)","d927c07d":"train_data.head()","f8f33ca2":"# Time taken by plane to reach destination is stored in column called Duration\n# It is the differnce betwwen Departure Time and Arrival time\n\n\n# Assigning and converting Duration column into list\nduration = list(train_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration","e21c9121":"# Adding duration_hours and duration_mins list to train_data dataframe\n\ntrain_data[\"Duration_hours\"] = duration_hours\ntrain_data[\"Duration_mins\"] = duration_mins","8251026d":"train_data.drop([\"Duration\"], axis = 1, inplace = True)","b3cef23f":"train_data.head()","64b92f82":"train_data[\"Airline\"].value_counts()","622494ce":"# From graph we can see that Jet Airways Business have the highest Price.\n# Apart from the first Airline almost all are having similar median\n\n# Airline vs Price\nsns.catplot(y = \"Price\", x = \"Airline\", data = train_data.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 6, aspect = 3)\nplt.show()","dc08a2eb":"# As Airline is Nominal Categorical data we can perform OneHotEncoding\n\nAirline = train_data[[\"Airline\"]]\n\nAirline = pd.get_dummies(Airline, drop_first= True)\n\nAirline.head()","40eeac06":"train_data[\"Source\"].value_counts()","794ff45a":"# Source vs Price\n\nsns.catplot(y = \"Price\", x = \"Source\", data = train_data.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 4, aspect = 3)\nplt.show()","434f8afe":"# As Source is Nominal Categorical data we will perform OneHotEncoding\n\nSource = train_data[[\"Source\"]]\n\nSource = pd.get_dummies(Source, drop_first= True)\n\nSource.head()","ea34ceee":"train_data[\"Destination\"].value_counts()","89c6a0c6":"# As Destination is Nominal Categorical data we can perform OneHotEncoding\n\nDestination = train_data[[\"Destination\"]]\n\nDestination = pd.get_dummies(Destination, drop_first = True)\n\nDestination.head()","deb28479":"train_data[\"Route\"]","81eae778":"# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\n\ntrain_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)","65a971f8":"train_data[\"Total_Stops\"].value_counts()","837235bc":"# As this is case of Ordinal Categorical type we perform LabelEncoder\n# Here Values are assigned with corresponding keys\n\ntrain_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)","b079b4b1":"train_data.head()","2b955066":"# Concatenate dataframe --> train_data + Airline + Source + Destination\n\ndata_train = pd.concat([train_data, Airline, Source, Destination], axis = 1)","edf09161":"data_train.head()","6fd48bda":"data_train.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)","9d70d6d1":"data_train.head()","e8df1e71":"data_train.shape","332441c8":"test_data = pd.read_excel(r\"..\/input\/flight-fare-prediction-mh\/Test_set.xlsx\", engine='openpyxl')","a3a091f9":"test_data.head()","7f7ff4bb":"# Preprocessing\n\nprint(\"Test data Info\")\nprint(\"-\"*75)\nprint(test_data.info())\n\nprint()\nprint()\n\nprint(\"Null values :\")\nprint(\"-\"*75)\ntest_data.dropna(inplace = True)\nprint(test_data.isnull().sum())\n\n# EDA\n\n# Date_of_Journey\ntest_data[\"Journey_day\"] = pd.to_datetime(test_data[\"Date_of_Journey\"], format=\"%d\/%m\/%Y\").dt.day\ntest_data[\"Journey_month\"] = pd.to_datetime(test_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month\ntest_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n\n# Dep_Time\ntest_data[\"Dep_hour\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.hour\ntest_data[\"Dep_min\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.minute\ntest_data.drop([\"Dep_Time\"], axis = 1, inplace = True)\n\n# Arrival_Time\ntest_data[\"Arrival_hour\"] = pd.to_datetime(test_data.Arrival_Time).dt.hour\ntest_data[\"Arrival_min\"] = pd.to_datetime(test_data.Arrival_Time).dt.minute\ntest_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)\n\n# Duration\nduration = list(test_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n\n# Adding Duration column to test set\ntest_data[\"Duration_hours\"] = duration_hours\ntest_data[\"Duration_mins\"] = duration_mins\ntest_data.drop([\"Duration\"], axis = 1, inplace = True)\n\n\n# Categorical data\n\nprint(\"Airline\")\nprint(\"-\"*75)\nprint(test_data[\"Airline\"].value_counts())\nAirline = pd.get_dummies(test_data[\"Airline\"], drop_first= True)\n\nprint()\n\nprint(\"Source\")\nprint(\"-\"*75)\nprint(test_data[\"Source\"].value_counts())\nSource = pd.get_dummies(test_data[\"Source\"], drop_first= True)\n\nprint()\n\nprint(\"Destination\")\nprint(\"-\"*75)\nprint(test_data[\"Destination\"].value_counts())\nDestination = pd.get_dummies(test_data[\"Destination\"], drop_first = True)\n\n# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\ntest_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n\n# Replacing Total_Stops\ntest_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n\n# Concatenate dataframe --> test_data + Airline + Source + Destination\ndata_test = pd.concat([test_data, Airline, Source, Destination], axis = 1)\n\ndata_test.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n\nprint()\nprint()\n\nprint(\"Shape of test data : \", data_test.shape)\n\n","e860ed14":"data_test.head()","2c4b23e9":"data_train.shape","6193acd5":"data_train.columns","57088ae0":"X = data_train.loc[:, ['Total_Stops', 'Journey_day', 'Journey_month', 'Dep_hour',\n       'Dep_min', 'Arrival_hour', 'Arrival_min', 'Duration_hours',\n       'Duration_mins', 'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',\n       'Airline_Jet Airways', 'Airline_Jet Airways Business',\n       'Airline_Multiple carriers',\n       'Airline_Multiple carriers Premium economy', 'Airline_SpiceJet',\n       'Airline_Trujet', 'Airline_Vistara', 'Airline_Vistara Premium economy',\n       'Source_Chennai', 'Source_Delhi', 'Source_Kolkata', 'Source_Mumbai',\n       'Destination_Cochin', 'Destination_Delhi', 'Destination_Hyderabad',\n       'Destination_Kolkata', 'Destination_New Delhi']]\nX.head()","88c01cfc":"y = data_train.iloc[:, 1]\ny.head()","5f4cab5b":"# Finds correlation between Independent and dependent attributes\n\nplt.figure(figsize = (18,18))\nsns.heatmap(train_data.corr(), annot = True, cmap = \"RdYlGn\")\n\nplt.show()","2d7939d6":"# Important feature using ExtraTreesRegressor\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nselection = ExtraTreesRegressor()\nselection.fit(X, y)","74ab644f":"print(selection.feature_importances_)","4097e3e1":"#plot graph of feature importances for better visualization\n\nplt.figure(figsize = (12,8))\nfeat_importances = pd.Series(selection.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()\n","9e9352bf":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","d8b90291":"from sklearn.ensemble import RandomForestRegressor\nreg_rf = RandomForestRegressor()\nreg_rf.fit(X_train, y_train)","371e342a":"y_pred = reg_rf.predict(X_test)","df7ab695":"reg_rf.score(X_train, y_train)","04cfe6f2":"reg_rf.score(X_test, y_test)","c51c39d3":"sns.distplot(y_test-y_pred)\nplt.show()","1052ac36":"\nplt.scatter(y_test, y_pred, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","9e653947":"from sklearn import metrics","97b34b07":"print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","61f7e084":"metrics.r2_score(y_test, y_pred)","233f4b6c":"from sklearn.model_selection import RandomizedSearchCV","49c1aa60":"#Randomized Search CV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","401db5ea":"# Create the random grid\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","e3ffd237":"# Random search of parameters, using 5 fold cross validation, \n# search across 100 different combinations\nrf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","e86a27e5":"rf_random.fit(X_train,y_train)","8525b06d":"rf_random.best_params_","d5fe07b0":"prediction = rf_random.predict(X_test)","3c600aea":"plt.figure(figsize = (8,8))\nsns.distplot(y_test-prediction)\nplt.show()","a64bba79":"plt.figure(figsize = (8,8))\nplt.scatter(y_test, prediction, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","a71d6d82":"print('MAE:', metrics.mean_absolute_error(y_test, prediction))\nprint('MSE:', metrics.mean_squared_error(y_test, prediction))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))","ec67e2af":"best  = rf_random.best_estimator_\nimport pickle\n# open a file, where you ant to store the data\nfile = open('flight_model.pkl', 'wb')\n\n# dump information to that file\npickle.dump(best, file)","7ea9b1d2":"model = open('flight_model.pkl','rb')\nforest = pickle.load(model)","dc6124b4":"y_prediction = forest.predict(X_test)","9e04a4c0":"metrics.r2_score(y_test, y_prediction)","e0ec5138":"---","ea76bfcd":"<h2 style = \"font-size:35px; font-family:times ; font-weight : bold; color : #AA3073; text-align: center; border-radius: 10px 100px;\">EDA<\/h2>\n","0db2f839":"---","a56a4b8d":"<h2 style = \"font-size:35px; font-family:times ; font-weight : bold; color : #AA3073; text-align: center; border-radius: 10px 100px;\">Test Set <\/h2>\n To ensure no 'peeking ahead'. Train data was preprocessed separately and we can apply the same preprocessing parameters used for the train set, onto the test set as though the test set didn't exist before.","c5a4989b":"---","77bf4dee":"---","400a49dd":"<h2 style = \"font-size:35px; font-family:times ; font-weight : bold; color : #AA3073; text-align: center; border-radius: 10px 100px;\">Fitting model using Random Forest <\/h2>\n\n\n1. Split dataset into train and test set in order to prediction w.r.t X_test\n2. If needed do scaling of data\n    * Scaling is not done in Random forest\n3. Import model\n4. Fit the data\n5. Predict w.r.t X_test\n6. In regression check **RSME** Score\n7. Hyper Parameter Tuning for better score","1b2d8da8":"## Hyperparameter Tuning\n\n\n* Choose following method for hyperparameter tuning\n    1. **RandomizedSearchCV** --> Fast but might skip some important parameters, computationally Less than grid searchcv\n    2. **GridSearchCV**\n* Assign hyperparameters in form of dictionery\n* Fit the model\n* Check best paramters and best score","edda7f89":"<br>\n<h1 style = \"font-size:35px; font-family:times ; font-weight : bold; color : #AA3073; text-align: center; border-radius: 10px 100px;\">Flight Fare Prediction\u2708\ufe0f<\/h1>\n<br>\n\n---","eb3fe2d4":"---","be579b40":"---","d87f5974":"## Save the model to reuse it again","ba9183e8":"From description we can see that Date_of_Journey is a object data type,\\\nTherefore, we have to convert this datatype into timestamp so as to use this column properly for prediction\n\nFor this we require pandas **to_datetime** to convert object data type to datetime dtype.\n\n<span style=\"color: red;\">**.dt.day method will extract only day of that date**<\/span>\\\n<span style=\"color: red;\">**.dt.month method will extract only month of that date**<\/span>","6f22aff9":"<h2 style = \"font-size:35px; font-family:times ; font-weight : bold; color : #AA3073; text-align: center; border-radius: 10px 100px;\">Categorical Data <\/h2>\n\n\nOne can find many ways to handle categorical data. Some of them categorical data are,\n1. <span style=\"color: blue;\">**Nominal data**<\/span> --> data are not in any order --> <span style=\"color: green;\">**OneHotEncoder**<\/span> is used in this case\n2. <span style=\"color: blue;\">**Ordinal data**<\/span> --> data are in order --> <span style=\"color: green;\">**LabelEncoder**<\/span> is used in this case","e06fdc37":"---","6d1d47c1":"## Feature Selection\n\nFinding out the best feature which will contribute and have good relation with target variable.\nFollowing are some of the feature selection methods,\n\n\n1. <span style=\"color: purple;\">**heatmap**<\/span>\n2. <span style=\"color: purple;\">**feature_importance_**<\/span>\n3. <span style=\"color: purple;\">**SelectKBest**<\/span>","4ed8b85d":"---","e2e3a55b":"<h2 style = \"font-size:35px; font-family:times ; font-weight : bold; color : #AA3073; text-align: center; border-radius: 10px 100px;\">Importing Data <\/h2>\n\n1. Since data is in form of excel file can use pandas read_excel to load the data\n2. After loading I checked complete information of data as it can give indication many of the hidden infomation such as null values in a column or a row\n3. Then I  Checked whether any null values are there or not. if it is present then following can be done,\n    1. Imputing data using Imputation method in sklearn\n    2. Filling NaN values with mean, median and mode using fillna() method or KNN Imputer\n4. Then I tried to Describe data which can give statistical analysis"}}