{"cell_type":{"cb0b22f0":"code","6ae84c08":"code","e6fc8256":"code","daf85915":"code","083ed4d6":"code","781e3d01":"code","b1c9d873":"code","779c9896":"code","fdf9e8ad":"code","71f829b5":"code","1f76bc82":"code","4a228af1":"code","04bdbf74":"code","58a1a62e":"markdown","0f461cd8":"markdown"},"source":{"cb0b22f0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nimport re","6ae84c08":"#import data\ntrain_data = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv')\ntest_data = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv')","e6fc8256":"#Taining data\ntrain_data.head(10)","daf85915":"test_data.head(5)","083ed4d6":"target_labels = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']","781e3d01":"#check for any null comments\nnull_comments = train_data[train_data['comment_text'].isnull()]\nprint(\"Number of null comments: \" + str(len(null_comments)))","b1c9d873":"#Counting total comments with above labels\nprint(train_data[target_labels].sum())","779c9896":"#Plot for text length in each comments\ntrain_data['char_length'] = train_data['comment_text'].apply(lambda x:len(str(x)))\nsns.set()\ntrain_data['char_length'].hist()\nplt.show()","fdf9e8ad":"#cleaning up comment text for every comment in train and test data\n#function for cleaning up comments\ndef clean_comment(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub('\\W', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip(' ')\n    return text\n    ","71f829b5":"train_data['comment_text'] = train_data['comment_text'].apply(lambda x:clean_comment(x))\ntest_data['comment_text'] = test_data['comment_text'].apply(lambda x:clean_comment(x))","1f76bc82":"#vectorizing the text data\nvec = TfidfVectorizer(max_features = 5000,stop_words = 'english')\nx_train = vec.fit_transform(train_data['comment_text'])\nx_test = vec.transform(test_data['comment_text'])","4a228af1":"submission = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv')\nclassifier = LogisticRegression(C=12.0)\nfor label in target_labels:\n    print('Fitting the model for label: ' + label)\n    y_labels = train_data[label]\n    classifier.fit(x_train,y_labels.values)\n    y_pred = classifier.predict(x_train)\n    print('Training accuracy: ' + str(accuracy_score(y_labels,y_pred)))\n    y_test_prob = classifier.predict(x_test)\n    submission[label] = y_test_prob","04bdbf74":"#create submission file\nsubmission.to_csv('test_submissions.csv',index = False)","58a1a62e":"## EDA","0f461cd8":"### Using logistic regression to create as many number of binary classifiers as number of labels"}}