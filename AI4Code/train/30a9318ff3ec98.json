{"cell_type":{"9c8e6de2":"code","4a06dda3":"code","60358106":"code","ae264fcc":"code","d3a80a82":"code","2072147c":"code","c7696a22":"code","8d600e95":"code","452ed62d":"code","ce135390":"code","4983daec":"code","81a95ade":"code","ba11c00e":"code","984f8416":"code","cf9d3582":"code","6fe2bbb8":"code","9688ddb2":"code","18962643":"code","ef235ab6":"code","11aeddd3":"code","c3bbed57":"code","afd254a1":"code","85f1d145":"code","9fc21f3a":"code","003a1f59":"code","e4d7b082":"code","2896dc47":"code","44e9db07":"code","9a4df277":"markdown","4f7ab580":"markdown","5d0eb082":"markdown","2292005c":"markdown","1b41a33f":"markdown","16ac3d67":"markdown","3d5d0c80":"markdown","bdc8de85":"markdown","bfea869f":"markdown","d806de71":"markdown","f1795f22":"markdown","2ada057b":"markdown","fe2271c0":"markdown"},"source":{"9c8e6de2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport keras\nfrom scipy.io import arff\nfrom io import StringIO\nimport tensorflow as tf","4a06dda3":"# Importing data\nf = open('..\/input\/bankurptcy-data\/4year.arff','r') \ndata, details = arff.loadarff(f)\nf.close()","60358106":"d=[]\nfor i in data:\n    t=list(i)\n    t=list(map(float,t))\n    d.append(t)","ae264fcc":"df=pd.DataFrame(d)","d3a80a82":"df.fillna(method='ffill',inplace=True)","2072147c":"df.shape","c7696a22":"df[64].value_counts()","8d600e95":"# ratio of majority to minority:\n9277\/515","452ed62d":"# defining weights based on imbalancing\nweights={0:1., 1:18.}","ce135390":"y=df[64].values\ndel df[64]\nx=df.values","4983daec":"x.shape,y.shape","81a95ade":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(x,y,test_size=0.3,stratify=y)","ba11c00e":"Y_test.shape[0]-Y_test[Y_test>0].sum()","984f8416":"Y_test.shape","cf9d3582":"def create_model(hidden_units,learning_rate):\n    model=keras.models.Sequential()\n    model.add(keras.layers.Dense(hidden_units,activation='relu'))\n    model.add(keras.layers.Dense(1,activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","6fe2bbb8":"# definig call back\nes = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min')","9688ddb2":"new_dic=[(0.002, 24), (0.04, 8), (0.06, 64), (0.08, 80), (0.005, 48), (0.07, 40), (0.001, 20), (0.1, 52), (0.009, 36), (0.01, 76)]","18962643":"accuracies={}\nfor i,params in enumerate(new_dic):\n    accuracies[i]=[]\n    for j in range(20):\n        model=create_model(params[1],params[0])\n        print('Fitting model for {}th iteration for {}th parameters'.format(j,i))\n        X_train1,X_test1,Y_train1,Y_test1=train_test_split(X_train,Y_train,test_size=0.3,stratify=Y_train)\n        model.fit(X_train1,Y_train1,validation_data=(X_test1,Y_test1),class_weight=weights,epochs=100,batch_size=32,verbose=0,callbacks=[es])\n        accuracies[i].append(model.evaluate(X_train,Y_train)[1]*100)\n        \n        ","ef235ab6":"import matplotlib.pyplot as plt\nfor i in range(10):\n    plt.boxplot(accuracies[i])\n    plt.title('Learning rate = {}, Hidden units = {}'.format(round(new_dic[i][0],2),new_dic[i][1]))\n    plt.show()\n","11aeddd3":"# Applying algorithm\nv_max=-1\ni_opt=-1\nfor i in range(len(accuracies)):\n    q1=np.percentile(accuracies[i],25,interpolation='midpoint')*100\n    q2=np.percentile(accuracies[i],50,interpolation='midpoint')*100\n    q3=np.percentile(accuracies[i],75,interpolation='midpoint')*100\n    #print(q1,q2,q3)\n    M=q2\n    IQR=q3-q1\n    v=(M*M)\/IQR\n    if(v>v_max):\n        v_max=v\n        i_opt=i\n        #print('new v: {} and new i_opt: {}'.format(v_max,i_opt))\n    print('For Lr: {}, Hu: {}, M: {} & IQR: {} => V: {}'.format(new_dic[i][0],new_dic[i][1],round(M,2),round(IQR,2),round(v,2)))\nprint('Best possible combiation: Learning rate: {}, Hidden units: {}'.format(new_dic[i_opt][0],new_dic[i_opt][1]))","c3bbed57":"# Checking test accuracy on optimal hyper parameters:\ni_opt=0\nmodel = create_model(new_dic[i_opt][1],new_dic[i_opt][0])\nprint('Fitting model')\nX_train1,X_test1,Y_train1,Y_test1=train_test_split(X_train,Y_train,test_size=0.3,stratify=Y_train)\nmodel.fit(X_train1,Y_train1,validation_data=(X_test1,Y_test1),class_weight=weights,epochs=100,batch_size=32,verbose=0,callbacks=[es])\nprint(\"Testing accuracy: {} and loss: {}\".format(model.evaluate(X_test,Y_test)[1],model.evaluate(X_test,Y_test)[0]))","afd254a1":"prediction=model.predict(X_test)","85f1d145":"predictions=[\n]\nfor i in prediction:\n    predictions.append(round(i[0]))","9fc21f3a":"predictions1=[]\nfor label in predictions:\n    if(label==0):\n        predictions1.append([0,1])\n    else:\n        predictions1.append([1,0])\npredictions=np.array(predictions1)","003a1f59":"labels=list(Y_test)","e4d7b082":"ranks = np.zeros(len(labels))\n\nfor i in range(len(labels)) :\n    if labels[i] in predictions[i] :\n        firstOccurance = np.argmax(predictions[i]== labels[i])        \n        for j in range(firstOccurance, len(labels)) :            \n            ranks[j] +=1","2896dc47":"ranks","44e9db07":"plt.plot(ranks)\nplt.title('CMC curve')\nplt.show()","9a4df277":"# Choosing bext possible hyperparameter based on value of M^2\/IQR where M is the median accuracy","4f7ab580":"# Hyper parameter values","5d0eb082":"# Filling missing data with next value.","2292005c":"# Plotting box plot for each hyperparameter configuration","1b41a33f":"# Extracting training data from raw_data","16ac3d67":"# Training the model","3d5d0c80":"# Splitting data into training  and test data","bdc8de85":"Conclusion: Since fraudlent cases are very less, this data is an imbalanced data.","bfea869f":"# Importing data","d806de71":"# Plotting CMC curve","f1795f22":"# Importing modules","2ada057b":"learning rate, no. of hidden neurons","fe2271c0":"# Defining model architecture"}}