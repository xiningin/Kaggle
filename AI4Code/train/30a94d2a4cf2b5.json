{"cell_type":{"6db808c1":"code","27ab7db7":"code","57070f44":"code","bceec706":"code","d27f1dc0":"code","456c09f2":"code","324765f3":"code","56bb27fc":"code","c639c452":"code","dcaab7c9":"code","07f42a18":"code","9522e0dc":"code","a7539e64":"code","05202b67":"code","f3c2d0d0":"code","21bf824b":"code","9270482e":"code","17ecad7b":"code","a59e7653":"code","81356f53":"code","b96206c5":"code","ccf9e977":"code","b7fc2424":"code","604650c9":"code","958ec888":"code","fbe771a5":"code","90b9b9ae":"code","fc44a8c6":"code","b616557e":"code","d2b8bf99":"code","575e4c18":"code","0aa27fd0":"code","cfb82d1b":"code","51c393d9":"code","6f2f1a79":"code","116dcc39":"code","4e4a4628":"markdown","62a5ab73":"markdown","9a8c3f49":"markdown","2d77fd54":"markdown","36d50fd3":"markdown","d0928a46":"markdown","718dbeb3":"markdown","591bb415":"markdown","518dc2ab":"markdown","ad8a6c9c":"markdown","36877889":"markdown","ba718b5c":"markdown","460ce4a4":"markdown","bb876f95":"markdown","13cfe509":"markdown","6e722254":"markdown","86760328":"markdown","35b415e4":"markdown","eeec6ef5":"markdown","a3448def":"markdown","a2cad7db":"markdown","80847130":"markdown","d9afb99a":"markdown","cee3680e":"markdown","150e7965":"markdown","e1761c5f":"markdown","b3d62896":"markdown"},"source":{"6db808c1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","27ab7db7":"rawdata = pd.read_csv(\"..\/input\/churn_data.csv\")","57070f44":"rawdata.dtypes","bceec706":"rawdata.pop('customerID');","d27f1dc0":"rawdata = rawdata.replace(r'^\\s+$', np.nan, regex=True)","456c09f2":"rawdata.isnull().sum()","324765f3":"rawdata[rawdata.isnull().any(axis=1)]","56bb27fc":"rawdata.TotalCharges = rawdata.TotalCharges.replace(np.nan, 0.0)","c639c452":"rawdata.TotalCharges = pd.to_numeric(rawdata.TotalCharges)","dcaab7c9":"rawdata[rawdata.select_dtypes('object').columns] = rawdata.select_dtypes('object').apply(lambda x: x.astype('category'))","07f42a18":"rawdata.dtypes","9522e0dc":"rawdata.SeniorCitizen.unique()","a7539e64":"rawdata.SeniorCitizen = rawdata.SeniorCitizen.astype('category')\nrawdata.SeniorCitizen = rawdata.SeniorCitizen.replace(0, 'No')\nrawdata.SeniorCitizen = rawdata.SeniorCitizen.replace(1, 'Yes')","05202b67":"count_churned = pd.value_counts(rawdata['Churn'])\ncount_churned.plot(kind='bar', rot=0)\nplt.title('Churn class distribution')\nplt.xticks(range(2), ['Active', 'Churned'])\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\");","f3c2d0d0":"churned = rawdata[rawdata.Churn == 'Yes']\nactive = rawdata[rawdata.Churn == 'No']","21bf824b":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Tenure per Account by Class')\n\nbins = 24\n\nax1.hist(churned.tenure, bins = bins)\nax1.set_title('Churned')\n\nax2.hist(active.tenure, bins = bins)\nax2.set_title('Active')\n\nplt.xlabel('Tenure (months)')\nplt.ylabel('Number of Accounts')\nplt.xlim((0, rawdata.tenure.max()+10))\nplt.show();","9270482e":"f, (ax1, ax2) = plt.subplots(2, 1, sharex = True)\nf.suptitle('Monthly Charges per Account by Class')\n\nbins = 50\n\nax1.hist(churned.MonthlyCharges, bins = bins)\nax1.set_title('Churned')\n\nax2.hist(active.MonthlyCharges, bins = bins)\nax2.set_title('Active')\n\nplt.xlabel('Monthly Charge ($)')\nplt.ylabel('Number of Accounts')\nplt.xlim((rawdata.MonthlyCharges.min()-10, rawdata.MonthlyCharges.max()+10))\nplt.yscale('log')\nplt.show();","17ecad7b":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Total Charged per Account by Class')\n\nbins = 80\n\nax1.hist(churned.TotalCharges, bins = bins)\nax1.set_title('Churned')\n\nax2.hist(active.TotalCharges, bins = bins)\nax2.set_title('Active')\n\nplt.xlabel('Total Charged ($)')\nplt.ylabel('Number of Accounts')\nplt.xlim((0, rawdata.TotalCharges.max()+100))\nplt.yscale('log')\nplt.show();","a59e7653":"from sklearn.preprocessing import StandardScaler\ndata = rawdata.copy(deep = True)","81356f53":"data['tenure'] = StandardScaler().fit_transform(data['tenure'].values.reshape(-1, 1))","b96206c5":"data['MonthlyCharges'] = StandardScaler().fit_transform(data['MonthlyCharges'].values.reshape(-1, 1))","ccf9e977":"data['TotalCharges'] = StandardScaler().fit_transform(data['TotalCharges'].values.reshape(-1, 1))","b7fc2424":"from sklearn.model_selection import train_test_split\nRANDOM_SEED = 47\nX_train, X_test = train_test_split(data, test_size=0.2, random_state=RANDOM_SEED)\nX_train = X_train[X_train.Churn == 'No']\nX_train = X_train.drop(['Churn'], axis=1)\n\nX_train_noisy = X_train.copy(deep = True)\ncolumns = X_train_noisy.select_dtypes(['object', 'category']).columns\nfor col in columns:\n    X_train_noisy[col] = X_train_noisy[[col]].apply(lambda x: np.random.choice(X_train[col].unique()), axis = 1)\n    \ncolumns = X_train_noisy.select_dtypes(['int64', 'float64']).columns\nfor col in columns:\n    X_train_noisy[col] = X_train_noisy[[col]].apply(lambda x: np.random.normal(X_train[col].mean(), X_train[col].std()), axis = 1)\n    \nX_train = pd.get_dummies(X_train, columns=X_train.select_dtypes(['object', 'category']).columns)\nX_train_noisy = pd.get_dummies(X_train_noisy, columns=X_train_noisy.select_dtypes(['object', 'category']).columns)\n\ny_test = X_test['Churn']\nX_test = X_test.drop(['Churn'], axis=1)\nX_test= pd.get_dummies(X_test, columns=X_test.select_dtypes(['object', 'category']).columns)\nX_train = X_train.values\nX_test = X_test.values\nX_train.shape","604650c9":"from keras.models import Model, load_model\nfrom keras.layers import Input, Dense\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras import regularizers","958ec888":"nb_epoch = 200\nbatch_size = 32\nlearning_rate = 1e-3","fbe771a5":"input_dim = X_train.shape[1]\nencoding_dim = 10\n\ninput_layer = Input(shape=(input_dim, ))\nencoder = Dense(encoding_dim, activation=\"tanh\", \n                activity_regularizer=regularizers.l1(10e-5))(input_layer)\nencoder = Dense(int(encoding_dim \/ 2), activation=\"relu\")(encoder)\ndecoder = Dense(int(encoding_dim \/ 2), activation='tanh')(encoder)\ndecoder = Dense(encoding_dim, activation='relu')(decoder)\noutput_layer = Dense(input_dim, activation='tanh')(decoder)\nautoencoder = Model(inputs=input_layer, outputs=output_layer)","90b9b9ae":"autoencoder.compile(optimizer='adam', \n                    loss='mean_squared_error', \n                    metrics=['accuracy'])\ncheckpointer = ModelCheckpoint(filepath=\"model.z1\",\n                               verbose=0,\n                               save_best_only=True)\ntensorboard = TensorBoard(log_dir='.\/logs',\n                          histogram_freq=0,\n                          write_graph=True,\n                          write_images=True)\nhistory = autoencoder.fit(X_train, X_train,\n                    epochs=nb_epoch,\n                    batch_size=batch_size,\n                    shuffle=True,\n                    validation_data=(X_test, X_test),\n                    verbose=1,\n                    callbacks=[checkpointer, tensorboard]).history","fc44a8c6":"plt.plot(history['loss'])\nplt.plot(history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right');","b616557e":"predictions = autoencoder.predict(X_test)\nmse = np.mean(np.power(X_test - predictions, 2), axis=1)\nerror_df = pd.DataFrame({'reconstruction_error': mse,\n                        'true_class': y_test})\nerror_df.describe()","d2b8bf99":"fig = plt.figure()\nax = fig.add_subplot(111)\nnormal_error_df = error_df[(error_df['true_class']== 'No') & (error_df['reconstruction_error'] < 10)]\n_ = ax.hist(normal_error_df.reconstruction_error.values, bins=10)","575e4c18":"fig = plt.figure()\nax = fig.add_subplot(111)\nnormal_error_df = error_df[(error_df['true_class']== 'Yes')]\n_ = ax.hist(normal_error_df.reconstruction_error.values, bins=10)","0aa27fd0":"autoencoder.compile(optimizer='adam', \n                    loss='mean_squared_error', \n                    metrics=['accuracy'])\ncheckpointer = ModelCheckpoint(filepath=\"model.z1\",\n                               verbose=0,\n                               save_best_only=True)\ntensorboard = TensorBoard(log_dir='.\/logs',\n                          histogram_freq=0,\n                          write_graph=True,\n                          write_images=True)\nhistory_noisy = autoencoder.fit(X_train_noisy, X_train,\n                    epochs=nb_epoch,\n                    batch_size=batch_size,\n                    shuffle=True,\n                    validation_data=(X_test, X_test),\n                    verbose=1,\n                    callbacks=[checkpointer, tensorboard]).history","cfb82d1b":"plt.plot(history_noisy['loss'])\nplt.plot(history_noisy['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right');","51c393d9":"predictions = autoencoder.predict(X_test)\nmse = np.mean(np.power(X_test - predictions, 2), axis=1)\nerror_df = pd.DataFrame({'reconstruction_error': mse,\n                        'true_class': y_test})\nerror_df.describe()","6f2f1a79":"fig = plt.figure()\nax = fig.add_subplot(111)\nnormal_error_df = error_df[(error_df['true_class']== 'No') & (error_df['reconstruction_error'] < 10)]\n_ = ax.hist(normal_error_df.reconstruction_error.values, bins=10)","116dcc39":"fig = plt.figure()\nax = fig.add_subplot(111)\nnormal_error_df = error_df[(error_df['true_class']== 'Yes')]\n_ = ax.hist(normal_error_df.reconstruction_error.values, bins=10)","4e4a4628":"### Reconstruction Errors of Churned Accounts","62a5ab73":"### Reconstruction Errors of Active Accounts","9a8c3f49":"Most of the features in the dataset are of data type object but are essentially categorical data. Let\u2019s change their data type -","2d77fd54":"Seems we have 11 null values in **_TotalCharges_**. Lets take a look at those rows -","36d50fd3":"Now that the features are set, we will look for **empty\/missing** values. For that lets replace all the empty values with NaN -","d0928a46":"Load the _Churn_ dataset.","718dbeb3":"Import required libraries.","591bb415":"### Reconstruction Errors of Active Accounts","518dc2ab":"### Preparing Training and Test Set","ad8a6c9c":"### Conclusion\nFor the Churn dataset SDA analysis doesn\u2019t help us to find any insight. It is expected as autoencoders are not meant for this kind of tabular data analysis and is meant more as an enhancer of other learning approaches where loss of interpretability is acceptable.","36877889":"### Transforming the Data\nNow we will normalize the numeric values and then perform one-hot encoding on the categorical values, as the number of categories are really small this is an acceptable number of extra dimension. From the rawdata we will create one original traning dataset and one corrupted training dataset.","ba718b5c":"### Reconstruction Errors of Churned Accounts","460ce4a4":"### Train the Model with Uncorrupted Data","bb876f95":"#### Cleaning the Data\nRemove redundant features from the dataset. Following is the list of features - ","13cfe509":"#### Goal in Training\nTraining an autoencoder with this data is going to be different as an autoencoder is not used for this purpose in general. We are going to take the Anomaly Detection approach in training the model. In this case the profile of those who are leaving will be detected as an anomaly. For that our training will run only on the active users class. Our target here is to train the model to reduce the reconstruction error on the active users. If a churned user profile is provided as the input the reconstruction error will be higher and will be detected as an anomaly.","6e722254":"Now to introduce noise we will perform random sampling on the categorical values and will generate random numbers from a normal distribution of specific mean and standard deviation for the numerical values.","86760328":"The model has following layers - \n**Input Layer -> Encoder 1 -> Encoder 2 -> Decoder 1 -> Decoder 2 -> Output Layer**","35b415e4":"Seems we have a very imbalanced dataset. Number of active users are much higher than those who left. Let's look at the two classes through the users **tenure** period, their **monthly charges** and the **total amount** they were charged -  ","eeec6ef5":"Before introducing noise in the data let's take a look at the class distribution - ","a3448def":"### Train the Model with Corrupted\/Noisy Data","a2cad7db":"Analysing these rows we see that all of their **_tenure_** is 0, which might mean they are just new clients as none of them are churned, but we cant say for sure as there is no start date in the dataset. We can either remove all these rows or keep these by making **_TotalCharges_** equal to 0. Lets keep these - ","80847130":"## Analysis of the _Churn_ dataset with SDA\nBy _Abdullah Ahmad Zarir_","d9afb99a":"**_customerID_** should be removed. **_TotalCharges_** can also be removed as it is a composite feature of **_tenure_** mutiplied with **_MonthlyCharges_**, but there are some inconsistencies in the given values which is why keeping it as it is at this point.","cee3680e":"Previously we saw that **_SeniorCitizen_** contains only 0 or 1, this can be made categorical.","150e7965":"### Building the Model","e1761c5f":"#### Noisy Data Generation ","b3d62896":"### Data Preprocessing"}}