{"cell_type":{"c4cb99cd":"code","7633c5d5":"code","a568cb17":"code","1f79f961":"code","709fd8c7":"code","be7409e5":"code","94990ff9":"code","db3b5d7b":"code","4217349b":"code","d7b359b9":"code","fb11a6b1":"code","e83d879c":"code","29cac59d":"code","d1478ebc":"code","e4b06b94":"code","b691a766":"code","6f7b4e97":"code","f2100128":"code","00bb6eaf":"code","6dd49a48":"code","526a681c":"code","81d58a93":"code","358e59a7":"code","78249612":"code","6ed66660":"code","e23ce78a":"code","58ebc916":"code","7967cf56":"code","50786037":"code","85f581e8":"code","34c6b413":"code","f6a1e6d7":"code","ec74fd4e":"code","11b79c36":"markdown","933a0faf":"markdown","bf9471e8":"markdown","b9ef2444":"markdown","24d89e0d":"markdown"},"source":{"c4cb99cd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7633c5d5":"#Importa as bibliotecas\nimport pandas as pd\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import plot_confusion_matrix, classification_report\nfrom imblearn.over_sampling import RandomOverSampler\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier","a568cb17":"#L\u00ea arquivos csv\ndf = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/test.csv')","1f79f961":"#Verifica nulos\ndf_nan = df.isna().sum()[df.isna().sum() >0]\ndf_nan","709fd8c7":"#Fun\u00e7\u00e3o para imputar valores NA \ndef imput_na(df):\n    df['v2a1'] = df['v2a1'].fillna(df['v2a1'].median())\n    df['v18q1'] = df['v18q1'].fillna(-1)\n    df['rez_esc'] = df['rez_esc'].fillna(-1)\n    df['meaneduc'] = df['meaneduc'].fillna(df['meaneduc'].median())\n    df['SQBmeaned'] = df['SQBmeaned'].fillna(df['SQBmeaned'].median())\n    return df","be7409e5":"#Substitui yes e no por 1 e 0\n# yes = 1 \n# no  = 0\ndef yes_no(df, column, tipo): \n    map_yes_no = {'yes': 1, 'no': 0}\n    df[column] = df[column].replace(map_yes_no).astype(tipo)\n    return(df)","94990ff9":"#Imputa\ndf = imput_na(df)\n##\n#Substitui yes e no\ndf = yes_no(df, 'edjefa', int)\ndf = yes_no(df, 'edjefe', int)\ndf = yes_no(df, 'dependency', float)","db3b5d7b":"#Visualiza features do tipo object\ndf.select_dtypes('object').head()","4217349b":"#Verifica dados em dependency\ndf['dependency'].unique()","d7b359b9":"#Verifica dados em edjefe\ndf['edjefe'].unique()","fb11a6b1":"#Verifica dados em edjefa\ndf['edjefe'].unique()","e83d879c":"df.info()","29cac59d":"feats = list(df.select_dtypes(['float','int']).columns)\n## Identificando vari\u00e1veis\ninv = []\nbinary = []\ncat = []\nquant = []\nfor feat in feats:\n    if df[feat].nunique() == 1:    #Se tiver somente um valor se encaixa em inv\u00e1lida pois n\u00e3o agrega nada\n        inv.append(feat)\n    elif df[feat].nunique() == 2:  #Se tiver somente dois valores vai pra lista de bin\u00e1rios\n        binary.append(feat)\n    elif df[feat].nunique() < 16:  #Se tiver at\u00e9 15 valores distintos ser\u00e1 interpretada como categ\u00f3rica\n        cat.append(feat)\n    else:\n        quant.append(feat)         #Se n\u00e3o for nenhuma das anteriores ser\u00e1 interpretada como quantitativa","d1478ebc":"#Printa \/ plota resultado da avalia\u00e7\u00e3o das features (informativo)\n##-----------------------------------------------------\n##- Features com um \u00fanico valor\nif len(inv) > 0:\n    for feat in inv:\n        print('Feature inv\u00e1lida:', feat)\n##-----------------------------------------------------\n##- Bin\u00e1rias\n#Verifica quantidade para adequar linhas e colunas:\nprint('***********************************')\nprint('Quantidade de bin\u00e1rias     :', len(binary))\nprint('Quantidade de categ\u00f3ricas  :', len(cat))\nprint('Quantidade de quantitativas:', len(quant))\nprint('***********************************')","e4b06b94":"## Histograma de vari\u00e1veis discretas\nax = df[quant].hist(bins=10, grid=True, figsize=(20,20), rwidth=0.8)\nplt.suptitle(\"Distribui\u00e7\u00e3o de features Quantitativas\", fontsize = 12)\nplt.show();","b691a766":"# Histograma da Vari\u00e1vel Target\n#Agrupa por valor\ntar = (\n    pd.DataFrame(df.groupby('Target').size())\n    .reset_index()\n    .rename(columns={0:'quantidade'})\n)\ntar['Target'] = tar.Target.astype(str)\n#Plota\nplt.figure(figsize=(8,5))\nplt.grid(linestyle=':', linewidth=0.7)\nplt.bar(tar.Target,tar.quantidade)\nplt.show();","6f7b4e97":"#Verifica os valores\ntar['percent'] = round((tar.quantidade\/tar.quantidade.sum())*100,2)\ntar","f2100128":"#Separa features em target e explicativas\nX = df.drop(['Id','Target', 'idhogar','elimbasu5'], axis=1) #elimbasu5 retirada pois tem o mesmo valor para todos os registros\ny = df['Target']\n#Separa em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","00bb6eaf":"# Fazendo o over-sampling\nros = RandomOverSampler(random_state=42)\nX_ros,y_ros= ros.fit_resample(X_train,y_train)\n# Verificando o resultado\ny_ros.value_counts()","6dd49a48":"#Instancia xgb\nxgbc = XGBClassifier(n_estimators=100, learning_rate=1, random_state=42, max_depth=1)\n# Treinando o modelo xgbc\nxgbc.fit(X_ros, y_ros);","526a681c":"# Instanciando o random forest classifier\nrfc = RandomForestClassifier(n_jobs=-1, n_estimators=200, random_state=42)\n# Treinando rfc\nrfc.fit(X_ros, y_ros);","81d58a93":"# Instanciando GBM\ngbm = GradientBoostingClassifier(n_estimators=200, learning_rate=0.9, random_state=42, max_depth=1)\ngbm.fit(X_ros, y_ros)","358e59a7":"#Visualizar a import\u00e2ncia das features\n#---------------------------------------------------------------\n#Prepara vizualiza\u00e7\u00e3o coeficientes xgbc\nxgbc_importance = (\n    pd.DataFrame(xgbc.feature_importances_, X_ros.columns)\n    .rename(columns={0:'valor'})\n    .sort_values('valor')\n    .reset_index()  \n)\n#\n#---------------------------------------------------------------\n#Prepara vizualiza\u00e7\u00e3o coeficientes rfc\nrfc_importance = (\n    pd.DataFrame(rfc.feature_importances_, X_ros.columns)\n    .rename(columns={0:'valor'})\n    .sort_values('valor')\n    .reset_index()  \n)\n#\n#---------------------------------------------------------------\n#Prepara vizualiza\u00e7\u00e3o coeficientes gbm\ngbm_importance = (\n    pd.DataFrame(gbm.feature_importances_, X_ros.columns)\n    .rename(columns={0:'valor'})\n    .sort_values('valor')\n    .reset_index()  \n)\n#---------------------------------------------------------------\n#plota\nfig, ax =  plt.subplots(nrows=1,ncols=3, figsize=(24,26))\ny_pos_xgbc = np.arange(len(xgbc_importance['index']))\ny_pos_rfc = np.arange(len(rfc_importance['index']))\ny_pos_gbm = np.arange(len(gbm_importance['index']))\n#\nax[0].barh(xgbc_importance['index'], xgbc_importance['valor'])\nax[0].set_ylim(min(y_pos_xgbc)-1, max(y_pos_xgbc)+1)\nax[0].title.set_text('XGBClassifier')\n#\nax[1].barh(rfc_importance['index'], rfc_importance['valor'])\nax[1].set_ylim(min(y_pos_rfc)-1, max(y_pos_rfc)+1)\nax[1].title.set_text('RFClassifier')\n#\nax[2].barh(gbm_importance['index'], gbm_importance['valor'])\nax[2].set_ylim(min(y_pos_gbm)-1, max(y_pos_gbm)+1)\nax[2].title.set_text('GBM')\n#---------------------------------------------------------------\nplt.show();","78249612":"#Visualiza acur\u00e1cia\npredict_xgbc = xgbc.predict(X_test)\naccuracy_xgbc = accuracy_score(y_test, predict_xgbc) * 100\n#\npredict_rfc = rfc.predict(X_test)\naccuracy_rfc = accuracy_score(y_test, predict_rfc) * 100\n#\npredict_gbm = gbm.predict(X_test)\naccuracy_gbm = accuracy_score(y_test, predict_gbm) * 100\n#\nprint('Acur\u00e1cia XGBClassifier:', accuracy_xgbc)\nprint('Acur\u00e1cia RFClassifier :', accuracy_rfc)\nprint('Acur\u00e1cia GBM          :', accuracy_gbm)","6ed66660":"#Matriz de confus\u00e3o xgbc\nplot_confusion_matrix(xgbc,X_test,y_test);","e23ce78a":"#Matriz de confus\u00e3o random forest\nplot_confusion_matrix(rfc,X_test,y_test);","58ebc916":"#Matriz de confus\u00e3o gbm\nplot_confusion_matrix(gbm,X_test,y_test);","7967cf56":"#Imputa\ntest = imput_na(test)\n##\n#Substitui yes e no\ntest = yes_no(test, 'edjefa', int)\ntest = yes_no(test, 'edjefe', int)\ntest = yes_no(test, 'dependency', float)","50786037":"#Separa as features explicativas\nfeats_exp = test.drop(['Id','idhogar','elimbasu5'], axis=1) #elimbasu5 retirada pois tem o mesmo valor para todos os registros e Target n\u00e3o existe neste dataset","85f581e8":"# Prever o Target de teste usando o modelo xgbc\n#test['Target'] = xgbc.predict(feats_exp).astype(int)","34c6b413":"# Prever o Target de teste usando o modelo rfc\n#test['Target'] = rfc.predict(feats_exp).astype(int)","f6a1e6d7":"# Prever o Target de teste usando o modelo gbm\ntest['Target'] = gbm.predict(feats_exp).astype(int)","ec74fd4e":"# Criando o arquivo para submiss\u00e3o\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","11b79c36":"Como pode ser observado existe um problema de desbalanceamento de classes, a categoria 4 ocupa quase 63% do dataset - Over-sampling","933a0faf":"Todo tratamento feito nas features explicativas de treino deve ser feito nas features de teste. \n\nRelat\u00f3rios e verifica\u00e7\u00f5es n\u00e3o ser\u00e3o feitas","bf9471e8":"Random Forest","b9ef2444":"XGBoost","24d89e0d":"GBM"}}