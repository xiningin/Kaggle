{"cell_type":{"b07ad8af":"code","bfd20e18":"code","4bd0fd2d":"code","d95ed52b":"code","e01e409e":"code","1683f2b6":"code","1038dae6":"code","4f01c014":"code","47035b6b":"code","bc569b3c":"code","af881be3":"code","7b6eb771":"code","9d73ef84":"code","f52d0cc6":"code","b6fe3156":"code","83956749":"code","1e2bd58c":"code","b45da9eb":"code","f3c04633":"code","9af16da8":"code","2cf90d1c":"code","62be929b":"code","85829293":"code","b824897c":"code","dfc9b3ff":"code","8fa0bf18":"code","481b566e":"code","874592d6":"code","00119f99":"code","4327cbde":"code","d163c293":"code","26a5a803":"code","dee89179":"code","8454a5f9":"code","a879e6c2":"code","b2568be0":"code","dfefa623":"code","b82cafe1":"code","37ff4f33":"code","1479b485":"code","80ecaf56":"code","30f467bc":"code","a4b87eca":"code","146ae578":"code","3b0274bc":"code","78ea1982":"code","6362aec2":"code","85a03274":"code","09109ec7":"code","013a4f0f":"code","7cca6d1b":"code","23de8d4e":"code","a3b86050":"code","b3063c2f":"code","019fbd75":"code","d4b4669b":"code","a86bcfa6":"code","48f1c503":"code","d57d1ef8":"code","4d712b2f":"code","690b3686":"code","f03bddf8":"code","41423151":"code","012ad30a":"code","3703d74b":"code","8f931a57":"markdown","01d2eb1b":"markdown","0b3a9bab":"markdown","90e4874f":"markdown","dde6a9b8":"markdown","7850c300":"markdown","d3ee9a35":"markdown","de828358":"markdown","d71bd59c":"markdown","86168416":"markdown","c48df9d3":"markdown","63051f3e":"markdown","b8dcf985":"markdown","eddfc8a1":"markdown","121a50f4":"markdown","5c45fe02":"markdown","38e58bbe":"markdown","e73c72cc":"markdown","1b39317a":"markdown","286befc6":"markdown","8466df79":"markdown","6d52150b":"markdown","2f48f951":"markdown","02218822":"markdown","2892a3d0":"markdown","9de3cd84":"markdown","07e7487a":"markdown","0d5bba23":"markdown","11fa74ca":"markdown","be9f83b7":"markdown","1fd3a776":"markdown","d1747d78":"markdown","a9375cff":"markdown","d9035c47":"markdown","6c9357a6":"markdown","9cd41982":"markdown","062ee6a9":"markdown","7e2d278b":"markdown","000bfcfa":"markdown","3450e368":"markdown","dc98c1f0":"markdown","09b2c0c4":"markdown","7788fd79":"markdown","a807f06c":"markdown","f786a6c3":"markdown","dff25c1e":"markdown","7ad43303":"markdown","e9a970c4":"markdown","ce3ac7e7":"markdown","3658987e":"markdown","76b01895":"markdown","66717c51":"markdown"},"source":{"b07ad8af":"# Load in our libraries\nimport random\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nfrom xgboost import XGBClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, cross_val_score\n","bfd20e18":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\nprint(\"Train set size:\", train.shape)\nprint(\"Test set size:\", test.shape)","4bd0fd2d":"# Store our passenger ID for easy access \nPassengerId = test['PassengerId'] \ntrain.head(5)","d95ed52b":"train.info()\nprint('_'*40)\ntest.info()","e01e409e":"train.describe()\n\n# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.\n# Review Parch distribution using `percentiles=[.75, .8]`\n# SibSp distribution `[.68, .69]`\n# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`","1683f2b6":"train.describe(include=['O'])","1038dae6":"corr_matrix = train.corr()\nplt.figure(figsize=(8, 7))\nsns.heatmap(data = corr_matrix,cmap='BrBG', annot=True, linewidths=0.2)","4f01c014":"train.isnull().sum()","47035b6b":"plt = train[['Pclass', 'Survived']].groupby('Pclass').mean().Survived.plot('bar')\nplt.set_xlabel('Pclass')\nplt.set_ylabel('Survival Probability')","bc569b3c":"plt = train[['Sex', 'Survived']].groupby('Sex').mean().Survived.plot('bar')\nplt.set_xlabel('Sex')\nplt.set_ylabel('Survival Probability')","af881be3":"plt = train[['Embarked', 'Survived']].groupby('Embarked').mean().Survived.plot('bar')\nplt.set_xlabel('Embarked')\nplt.set_ylabel('Survival Probability')","7b6eb771":"plt = train[['SibSp', 'Survived']].groupby('SibSp').mean().Survived.plot('bar')\nplt.set_xlabel('SibSp')\nplt.set_ylabel('Survival Probability')","9d73ef84":"plt = train[['Parch', 'Survived']].groupby('Parch').mean().Survived.plot('bar')\nplt.set_xlabel('Parch')\nplt.set_ylabel('Survival Probability')","f52d0cc6":"from matplotlib import pyplot as plt\nfig = plt.figure(figsize=(20, 7))\nsns.violinplot(x='Sex', y='Age', \n               hue='Survived', data=train, \n               split=True,\n               palette={0: \"blue\", 1: \"yellow\"}\n              );","b6fe3156":"y = train.shape[0]\nprint(y)\ny2 = test.shape[0]","83956749":"features = pd.concat([train, test]).reset_index(drop=True)\nprint(features.shape)","1e2bd58c":"features['Name_length'] = features['Name'].str.len()","b45da9eb":"features['Has_Cabin'] = features[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)","f3c04633":"features['FamilySize'] = features['SibSp'] + features['Parch'] + 1","9af16da8":"features['IsAlone'] = 0\nfeatures.loc[features['FamilySize'] == 1, 'IsAlone'] = 1","2cf90d1c":"features['Embarked'] = features['Embarked'].fillna('S')","62be929b":"    features['Fare'] = features['Fare'].fillna(train['Fare'].median())\n# Mapping Fare\n    features.loc[ features['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    features.loc[(features['Fare'] > 7.91) & (features['Fare'] <= 14.454), 'Fare'] = 1\n    features.loc[(features['Fare'] > 14.454) & (features['Fare'] <= 31), 'Fare']   = 2\n    features.loc[ features['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    features['Fare'] = features['Fare'].astype(int)","85829293":"features.head(3)","b824897c":"features['Age'] = features['Age'].apply(lambda x: np.random.choice(features['Age'].dropna().values) if np.isnan(x) else x)\nprint(features['Age'].head(20))","dfc9b3ff":"features['Age'] = features['Age'].astype(int)\n# Mapping Age\nfeatures.loc[ features['Age'] <= 16, 'Age'] \t\t\t\t\t= 0\nfeatures.loc[(features['Age'] > 16) & (features['Age'] <= 32), 'Age'] = 1\nfeatures.loc[(features['Age'] > 32) & (features['Age'] <= 48), 'Age'] = 2\nfeatures.loc[(features['Age'] > 48) & (features['Age'] <= 64), 'Age'] = 3\nfeatures.loc[ features['Age'] > 64, 'Age'] = 4 ;\n","8fa0bf18":"features['Title'] = features['Name'].str.extract(pat = ' ([A-Za-z]+)\\.' )\nfeatures['Title'] = features['Title'].replace('Mlle', 'Miss')\nfeatures['Title'] = features['Title'].replace('Ms', 'Miss')\nfeatures['Title'] = features['Title'].replace('Mme', 'Mrs')\n\nfeatures['Title'] = features['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')","481b566e":"# Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    features['Title'] = features['Title'].map(title_mapping)\n    features['Title'] = features['Title'].fillna(0)","874592d6":"features.tail(3)","00119f99":"# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp', 'Name_length']\nfeatures = features.drop(drop_elements, axis = 1)","4327cbde":" # Mapping Categorical data\nfeatures['Embarked'] = features['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\nfeatures['Sex'] = features['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n#features['Age*Class'] = features['Age']*features['Pclass']\n\n#features = pd.get_dummies(features, prefix=['Embarked', 'Sex'], columns=['Embarked', 'Sex'])\n# One hot encoding is not helping in this case","d163c293":"train = features.iloc[:y]\ntest = features.iloc[-y2:]","26a5a803":"train.shape","dee89179":"test.shape","8454a5f9":"test  = test.drop('Survived', axis = 1)\ntest.head(3)","a879e6c2":"plt = train[['Title', 'Survived']].groupby('Title').mean().Survived.plot('bar')\nplt.set_xlabel('Title')\nplt.set_ylabel('Survival Probability')","b2568be0":"import matplotlib.pyplot as plt\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","dfefa623":"X_train = train.drop(\"Survived\", axis=1)\nY_train = train[\"Survived\"]\nX_test  = test.copy()\nX_train.shape, Y_train.shape, X_test.shape","b82cafe1":"# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","37ff4f33":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test).astype(int)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","1479b485":"coeff_df = pd.DataFrame(train.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","80ecaf56":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test).astype(int)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","30f467bc":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nknn.predict(X_test).astype(int)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","a4b87eca":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\ndecision_tree.predict(X_test).astype(int)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","146ae578":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nrandom_forest.predict(X_test).astype(int)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","3b0274bc":"xgboost = XGBClassifier(\n learning_rate = 0.95,\n n_estimators= 5000,\n max_depth= 4,\n min_child_weight= 2,\n gamma=1,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1)\n\nxgboost.fit(X_train, Y_train)\nxgboost.predict(X_test).astype(int)\nxgboost.score(X_train, Y_train)\nacc_xgboost = round(xgboost.score(X_train, Y_train) * 100, 2)\nacc_xgboost","78ea1982":"from xgboost import plot_tree\nfrom matplotlib.pylab import rcParams\n\n##set up the parameters\nrcParams['figure.figsize'] = 80,50\n# plot single tree\nplot_tree(xgboost, rankdir='LR')\n","6362aec2":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Decision Tree', 'Xgboost'],\n    'Confidence Score': [acc_svc, acc_knn, acc_log, acc_random_forest, acc_decision_tree,acc_xgboost],\n'Real Score': [0.78947,0.74641,0.77511,0.76076,0.76076,0.74162]})\nmodels.sort_values(by='Real Score', ascending=False)","85a03274":"submission = pd.DataFrame({\n        \"PassengerId\": PassengerId,\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv(\"Submission2new.csv\", index=False)","09109ec7":"# Helpers via Python Classes\n\nntrain = train.shape[0]\nntest = test.shape[0]\n# Choose a random seed\nSEED = 0\nNFOLDS = 5 # set folds for out-of-fold prediction\nn_splits = 5 #needed later\nkf = KFold(n_splits = NFOLDS, random_state = SEED)\n\n# Class to extend the Sklearn classifier\n# Standard code taken from http:\/\/blog.keyrus.co.uk\/ensembling_ml_models.html.\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        \"\"\"Get the name of the model for labelling purposes and set a random seed.\"\"\"\n        self.name = re.search(\"('.+')>$\", str(clf)).group(1)\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        \"\"\"Fit with training data.\"\"\"\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        \"\"\"Make a prediction.\"\"\"\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        \"\"\"Refit and get the feature importances.\"\"\"\n        print(self.clf.fit(x,y).feature_importances_)\n        return(self.clf.fit(x,y).feature_importances_)\n    \n# Class to extend XGboost classifer","013a4f0f":"def get_oof(clf, x_train, y_train, x_test):\n    \"\"\"Get out-of-fold predictions for a given classifier.\"\"\"\n    # Initialise the correct sized dfs that we will need to store our results.\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((n_splits, ntest))\n    \n    # Loop through our kfold object\n    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n        \n        # Use kfold object indexes to select the fold for train\/test split\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        # Train the model\n        clf.train(x_tr, y_tr)\n        \n        # Predict on the in-fold training set\n        oof_train[test_index] = clf.predict(x_te)\n        \n        # Predict on the out-of-fold testing set\n        oof_test_skf[i, :] = clf.predict(x_test)\n    \n    # Take the mean of the 5 folds predictions\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    \n    # Returns both the training and testing predictions\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","7cca6d1b":"# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 575,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 3\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':575,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 3,\n    'verbose': 3\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.95\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 575,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 3,\n    'verbose': 3\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025\n    }","23de8d4e":"# Create 5 objects that represent our 5 models\n# # Initialize classifier objects\nrf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)","a3b86050":"# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data","b3063c2f":"# Create our out-of-fold train and test predictions. These base results will be used as new features.\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")","019fbd75":"rf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)","d4b4669b":"rf_features = list(rf_feature) \net_features = list(et_feature) \nada_features = list(ada_feature) \ngb_features = list(gb_feature) \n","a86bcfa6":"cols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n    'Random Forest feature importances': rf_features,\n    'Extra Trees  feature importances': et_features,\n    'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })","48f1c503":"# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Random Forest Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Extra Trees  feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Extra Trees  feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Extra Trees Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['AdaBoost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['AdaBoost feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'AdaBoost Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Gradient Boost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Gradient Boost feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Gradient Boosting Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","d57d1ef8":"# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(10)","4d712b2f":"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()","690b3686":"data = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')","f03bddf8":"# Concatenate the training and test sets\nx_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)","41423151":"xgboost = XGBClassifier(\n learning_rate = 0.95,\n n_estimators= 5000,\n max_depth= 4,\n min_child_weight= 2,\n gamma=1,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1)\n\nxgb_model_full_data = xgboost.fit(x_train, y_train)\npredictions = xgb_model_full_data.predict(x_test).astype(int)","012ad30a":"from xgboost import plot_tree\nfrom matplotlib.pylab import rcParams\n\n##set up the parameters\nrcParams['figure.figsize'] = 80,50\n# plot single tree\nplot_tree(xgb_model_full_data, rankdir='LR')\n","3703d74b":"# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,'Survived': predictions })\nStackingSubmission.to_csv(\"Submission.csv\", index=False)","8f931a57":"We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n\n* Logistic Regression\n* KNN or k-Nearest Neighbors\n* Support Vector Machines\n* Naive Bayes classifier\n* Decision Tree\n* Random Forrest\n* Perceptron\n* Artificial neural network\n* RVM or Relevance Vector Machine","01d2eb1b":"#### Correlation Heatmap of the Second Level Training set","0b3a9bab":"### 3. Pclass wise - Survival probability","90e4874f":"### Trying Different Model, predict and solve without Any Cross Validation","dde6a9b8":"### 1. Pearson Correlation Heatmap","7850c300":"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe.","d3ee9a35":"### 9. Correlation between columns","de828358":"### 8. Age wise - Survival probability","d71bd59c":"#### Plotting above XGBoost Decision Tree ( Without Cross Validation )","86168416":"The passengers having three children\/parents has more survival probability.\n\n'3' > '1' > '2' > '0' > '5'","c48df9d3":"### 5. Embarked wise - Survival probability","63051f3e":"The passengers having one sibling\/spouse has more survival probability.\n\n'1' > '2' > '0' > '3' > '4'","b8dcf985":"# Feature Engineering","eddfc8a1":"#### Feature importances generated from the different classifiers\nWe can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.","121a50f4":"### Setup cross validation method","5c45fe02":"Younger male tend to survive.\n\nInfants (Age <=4) had high survival rate.\n\nOldest passengers (Age = 80) survived.\n\nA large number of passengers between 20 and 40 die.\n\nThe age doesn't seem to have a direct impact on the female survival","38e58bbe":"#### What is the distribution of categorical features?\n\n* Names are unique across the dataset (count=unique=891).\n* Sex variable as two possible values with 65% male (top=male, freq=577\/count=891).\n* Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\n* Embarked takes three possible values. S port used by most passengers (top=S).\n* Ticket feature has high ratio (22%) of duplicate values (unique=681).","e73c72cc":"# Titanic : Visualization & Prediction\n\nPredict survival on the Titanic and get familiar with ML basics. (\u2b50\ufe0f Upvote my notebook \u2014 it helps! )\n\n<img src=\"https:\/\/miro.medium.com\/max\/1680\/1*vLzwEHLZH0vt3t3PzZnTAg.jpeg\" height=\"70%\" width=\"80%\" >\n\nGetting started with competitive data science can be quite intimidating. So I build this notebook for quick overview on `Titanic: Machine Learning from Disaster` competition. If there is interest, I\u2019m happy to do deep dives into the intuition behind the feature engineering and models used in this kernel.\n\nI encourage you to fork this kernel, play with the code and enter the competition. Good luck!\n\n#### Competition Description\n\nIn this challenge, it ask you to build a predictive model that predicts which passengers survived the Titanic shipwreck and answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\n#### Executive Summary\n\nI started this competition by just focusing on getting a good understanding of the dataset. The EDA & Visualizations are included to allow developers to dive into analysis of dataset.\n\n### Key features of the model training process in this kernel\n\nK Fold Cross Validation: Using 5-fold cross-validation.\n\nFirst Level Learning Model: On each run of cross-validation tried fitting following models :-\n1. Random Forest classifier\n2. Extra Trees classifier\n3. AdaBoost classifer\n4. Gradient Boosting classifer\n5. Support Vector Machine\n\nSecond Level Learning Model : Trained a XGBClassifier using xgboost","1b39317a":"### Interactive feature importances via Plotly scatterplots","286befc6":"## Content in Notebook\n\n1. Data Preprocessing\n2. Exploratory Visualization \n3. Feature Engineering  \n\n   31. Value Mapping \n   32. Simplification  \n   33. Feature Selection  \n   34. Handling Categorical Data\n4. Modeling & Evaluation \n\n    41. Trying Different Model without Validation\n    42. Cross-validation method\n    43. Model scoring function\n    44. Setting Up Models\n5. Train & Fit Model\n6. Our Base First-Level Models\n7. Second-Level Predictions From The First-Level Output\n8. Output as Prediction file ( .csv)\n9. Acknowledgments","8466df79":"There are no very highly correlated columns","6d52150b":"# Visualisations","2f48f951":"#### Explaination for above & below code :-\n\nIn each split of K-Folds, the original train data will be splited to new train data and new test data, which will be uesd to fit a model later. kf generates an index array based on current split, so the location selected as train data is marked as \"trainindex\", and the location selected as test data is marked as \"testindex\".\n\n\"ooftrain[testindex] = clf.predict(xte)\" can only provide prediction values on the \u201ctestindex\u201d location, so other values on \"trainindex\" location of \"ooftrain\" will hold zero.\n\nIn the next split, the values of \"ooftrain\" will hold the values which are provided in the last split, and only the values on the new \"testindex\" location in current split will be provided by current prediction.\n\nThe location of \"testindex\" in each split is different. So \"the last iteration will overwrite the previous oftrain[test_index] result\" will not happen.\n\nAfter n splits, all of the values of \"oof_train\" will be given by n predictions.","02218822":"Survival probability: C > Q > S","2892a3d0":"# Second-Level Predictions from the First-level Output","9de3cd84":"#### Plot final XGBoost Decision Tree","07e7487a":"### Second level learning model via XGBoost\n\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms.\n\nWe call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data.\n\nJust a quick run down of the XGBoost parameters used in the model:\n\nmax_depth : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n\ngamma : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.","0d5bba23":"All right so now having cleaned the features and extracted relevant information and dropped the redundant columns, converted categorical columns to numerical ones, a format suitable to feed into our Machine Learning models.","11fa74ca":"Now, we can use the results from our level 1 classifier as inputs for our level 2 classifier. For our level 2 learner, we are going to use an XGBoost model.","be9f83b7":"#### Producing the Submission file\n\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:","1fd3a776":"### 4. Sex wise - Survival probability","d1747d78":"1st class has high chance of surviving than the other two classes.","a9375cff":"# Model ","d9035c47":"### 7. Parch - Children\/Parents wise - Survival probability","6c9357a6":"Now, we can use our helper functions to initialise our level 1 classifiers and then return our results as a dataframe. We have already successfully imported all of your classifiers (from sklearn.ensemble import RandomForestClassifier) and created dictionaries of their respective hyper-parameters (rf_params).","9cd41982":"If you really enjoyed this kernel, then you might find following kernels interesting :-\n\nHouse Prices : Visualization & Prediction - https:\/\/www.kaggle.com\/iamrohitsingh\/house-prices-visualization-prediction\n\nFlight Crash Investigation - https:\/\/www.kaggle.com\/iamrohitsingh\/flight-crash-investigation\/ \n\nYouTube India Data Exploration - https:\/\/www.kaggle.com\/iamrohitsingh\/youtube-india-trending-data-exploration\n","062ee6a9":"### Output of the First level Predictions\n\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions.","7e2d278b":"So now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\n\n* Random Forest classifier\n* Extra Trees classifier\n* AdaBoost classifer\n* Gradient Boosting classifer\n* Support Vector Machine\n\n#### Parameters\n\nJust a quick summary of the parameters that we will be listing here for completeness,\n\n**n_jobs :** Number of cores used for the training process. If set to -1, all cores are used.\n\n**n_estimators :** Number of classification trees in your learning model ( set to 10 per default)\n\n**max_depth :** Maximum depth of tree, or how much a node should be expanded. Beware if set to too high a number would run the risk of overfitting as one would be growing the tree too deep\n\n**verbose :** Controls whether you want to output any text during the learning process. A value of 0 suppresses all text while a value of 3 outputs the tree learning process at every iteration.","000bfcfa":"#### Model Comparison","3450e368":"The survival probability for 'Mrs' and 'Miss' is high comapred to other classes.","dc98c1f0":"#### What is the distribution of numerical feature values across the samples?\n\nThis helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.\n\n* Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n* Survived is a categorical feature with 0 or 1 values.\n* Around 38% samples survived representative of the actual survival rate at 32%.\n* Most passengers (> 75%) did not travel with parents or children.\n* Nearly 30% of the passengers had siblings and\/or spouse aboard.\n* Fares varied significantly with few passengers (<1%) paying as high as $512.\n* Few elderly passengers (< 1%) within age range 65-80.","09b2c0c4":"We are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.","7788fd79":"# Our Base First-Level Models","a807f06c":"### 2. Number of missing values","f786a6c3":"### Out-of-Fold Predictions","dff25c1e":"# Acknowledgments\n\nInspirations are drawn from various Kaggle notebooks but majorly motivation is from the following :\n\n1. https:\/\/www.kaggle.com\/arthurtok\/0-808-with-simple-stacking\n2. https:\/\/www.kaggle.com\/usharengaraju\/data-visualization-titanic-survival\n3. https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python\n4. https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n\nTill next time, Peace Out\n\nMy GitHub Project Link - https:\/\/github.com\/RohitLearner\/Titanic","7ad43303":"### 6. SibSp - Siblings\/Spouse wise - Survival probability","e9a970c4":"The columns 'Age' and 'Cabin' contains more null values.","ce3ac7e7":"# Data Pre-processing","3658987e":"### 8. Title wise - Survival probability","76b01895":"The survival probaility for Female is more. They might have given more priority to female than male.","66717c51":"\"kf\u201d means K-Folds cross-validator, so \"kf = KFold(n_splits = NFOLDS, random_state = SEED)\" will generates a K-Folds cross-validator which provides train\/test indices to split data in train\/test sets. "}}