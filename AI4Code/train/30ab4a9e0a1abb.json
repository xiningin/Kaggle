{"cell_type":{"d9b4df39":"code","a43afda7":"code","5308b74d":"code","9053125e":"code","c2a58cfd":"code","d8d2629c":"code","944e5f96":"code","cc3789b4":"code","33d0afbe":"code","6e00ea50":"code","e2a895b9":"code","94cb1d01":"code","0cf38d89":"code","eae0bbfa":"code","0946a55b":"code","e8820d0c":"code","85503529":"code","7d46667a":"code","b1327a03":"code","f403be51":"code","11f304e0":"code","a2b4df36":"code","b7ef80ad":"code","3b18ce21":"code","a642458f":"code","46b0f6a4":"code","f1f4798b":"code","6b04613e":"code","7d656d4b":"code","640d7586":"code","6ad7adb9":"code","9cf8fbc0":"code","8b792906":"code","c5e8df59":"code","46e1d77c":"code","b44a609d":"code","cf0d912e":"code","61f2647c":"code","4ae8faf9":"markdown","fc372fab":"markdown","256e8112":"markdown","d3b57f1e":"markdown","6e1446a1":"markdown","4288f211":"markdown","cc050d76":"markdown","4a326a2c":"markdown","7da0f944":"markdown","b2d62a4e":"markdown","4088c95b":"markdown","f79ba839":"markdown"},"source":{"d9b4df39":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#Visualize\nimport matplotlib.pyplot as plt #For visualization\nfrom matplotlib import rcParams #add styling to the plots\nfrom matplotlib.cm import rainbow #for colors\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Process the data \nfrom sklearn.model_selection import train_test_split #split the available dataset for testing and training\nfrom sklearn.preprocessing import StandardScaler #To scale the features\n#Machine Learning algorithms I will be using.\n\nfrom sklearn.neighbors import KNeighborsClassifier #K Neighbors Classifier\nfrom sklearn.svm import SVC #Support Vector Classifier\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree Classifier\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest Classifier\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n\n# Any results you write to the current directory are saved as output.","a43afda7":"# read the data file\ndataset = pd.read_csv('..\/input\/heart.csv')","5308b74d":"dataset.shape # number of rows and columns","9053125e":"dataset.info()","c2a58cfd":"dataset.describe()","d8d2629c":"dataset.sample(5)","944e5f96":"# dataset.isnull().sum()\n# dataset.isnull().values.any()","cc3789b4":"dataset.columns","33d0afbe":"#Rename the columns\ndataset.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n       'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target']","6e00ea50":"dataset.columns","e2a895b9":"dataset.dtypes","94cb1d01":"# Understanding the data\n# Visualizations to better understand data and do any processing if needed.\nrcParams['figure.figsize'] = 20, 14\nplt.matshow(dataset.corr())\nplt.yticks(np.arange(dataset.shape[1]), dataset.columns)\nplt.xticks(np.arange(dataset.shape[1]), dataset.columns)\nplt.colorbar()","0cf38d89":"dataset.hist()","eae0bbfa":"# Check for the target classes to evaluate if they are of approximately same size\n\nrcParams['figure.figsize'] = 8,6\nplt.bar(dataset['target'].unique(), dataset['target'].value_counts(), color = ['red', 'green'])\nplt.xticks([0, 1])\nplt.xlabel('Target Classes')\nplt.ylabel('Count')\nplt.title('Count of each Target Class')","0946a55b":"# Male vs Female data\nmale = len(dataset[dataset.sex == 1])\nfemale = len(dataset[dataset.sex == 0])\nplt.pie(x=[male, female], explode=(0, 0), labels=['Male', 'Female'], autopct='%1.2f%%', shadow=True, startangle=90)\nplt.show()","e8820d0c":"# Chest Pain Type\nx = [len(dataset[dataset['chest_pain_type'] == 0]),len(dataset[dataset['chest_pain_type'] == 1]), len(dataset[dataset['chest_pain_type'] == 2]), len(dataset[dataset['chest_pain_type'] == 3])]\nplt.pie(x, data=dataset, labels=['chest_pain_type(1) typical angina', 'chest_pain_type(2) atypical angina', 'chest_pain_type(3) non-anginal pain', 'chest_pain_type(4) asymptomatic'], autopct='%1.2f%%', shadow=True,startangle=90)\nplt.show()","85503529":"# Fasting Blood Sugar\n\nx = [len(dataset[dataset['fasting_blood_sugar'] == 0]),len(dataset[dataset['fasting_blood_sugar'] == 1])]\nplt.pie(x, data=dataset, labels=['fasting_blood_sugar(1) >120mg', 'fasting_blood_sugar(2) <120mg',], autopct='%1.2f%%', shadow=True,startangle=90)\nplt.show()\n\n\n","7d46667a":"#\nplt.figure(figsize=(10,6))\ncount= dataset.sex.value_counts()\nsns.barplot(x=count.index, y=count.values)\nplt.ylabel(\"Number of ca\")\nplt.xlabel(\"Ca values\")\nplt.title(\"Ca values in data\", color=\"black\", fontsize=\"12\")\n\n","b1327a03":"# Heart disease frequency by age\nplt.figure(figsize=(15, 15))\nsns.countplot(x='age', hue='target', data=dataset, palette=['blue', 'red'])\nplt.legend([\"No Disease\", \"Have Disease\"])\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()","f403be51":"fig, axes = plt.subplots(3, 2, figsize=(12,12))\nfs = ['chest_pain_type', 'fasting_blood_sugar', 'rest_ecg','exercise_induced_angina', 'st_slope', 'num_major_vessels']\nfor i, axi in enumerate(axes.flat):\n    sns.countplot(x=fs[i], hue='target', data=dataset, palette='bwr', ax=axi) \n    axi.set(ylabel='Frequency')","11f304e0":"# Data Processing\n# Convert some categorical variables into dummy variables and scale all the values before training the Machine Learning models. \n# First,use the get_dummies method to create dummy columns for categorical variables.\ndataset = pd.get_dummies(dataset, columns = ['sex', 'chest_pain_type', 'fasting_blood_sugar', 'rest_ecg', 'exercise_induced_angina', 'st_slope', 'num_major_vessels', 'thalassemia'])","a2b4df36":"# Use the StandardScaler from sklearn to scale my dataset.\n#dataset.columns\nstandardScaler = StandardScaler()\ncolumns_to_scale = ['age', 'resting_blood_pressure', 'cholesterol', 'max_heart_rate_achieved', 'st_depression']\ndataset[columns_to_scale] = standardScaler.fit_transform(dataset[columns_to_scale])\n#dataset.columns","b7ef80ad":"y = dataset['target']\nX = dataset.drop(['target'], axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n","3b18ce21":"knn_scores = []\nfor k in range(1,10):\n    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n    knn_classifier.fit(X_train, y_train)\n    knn_scores.append(knn_classifier.score(X_test, y_test))","a642458f":"# Scores for different neighbor values are in the array knn_scores. Plot and see for which value of K we get the best scores.\n\nplt.plot([k for k in range(1, 10)], knn_scores, color = 'red')\nfor i in range(1,10):\n    plt.text(i, knn_scores[i-1], (i, knn_scores[i-1]))\nplt.xticks([i for i in range(1, 10)])\nplt.xlabel('Number of Neighbors (K)')\nplt.ylabel('Scores')\nplt.title('K Neighbors Classifier scores for different K values')","46b0f6a4":"print(\"The score for K Neighbors Classifier is {:0.2f}% with {} neighbors.\".format(knn_scores[7]*100, 8))","f1f4798b":"# Support Vector Classifier\n# There are several kernels for Support Vector Classifier. We will test some of them and check which has the best score.\n\n\nsvc_scores = []\nkernels = ['linear', 'poly', 'rbf', 'sigmoid']\nfor i in range(len(kernels)):\n    svc_classifier = SVC(kernel = kernels[i])\n    svc_classifier.fit(X_train, y_train)\n    svc_scores.append(svc_classifier.score(X_test, y_test))","6b04613e":"# Plot a bar plot of scores for each kernel and see which performed the best.\n\ncolors = rainbow(np.linspace(0, 1, len(kernels)))\nplt.bar(kernels, svc_scores, color = colors)\nfor i in range(len(kernels)):\n    plt.text(i, svc_scores[i], svc_scores[i])\nplt.xlabel('Kernels')\nplt.ylabel('Scores')\nplt.title('Support Vector Classifier scores for different kernels')","7d656d4b":"print(\"The score for Support Vector Classifier is {:0.2f}% with {} kernel.\".format(svc_scores[3]*100, 'sigmoid'))","640d7586":"# Decision Tree Classifier\n# Use the Decision Tree Classifier. Vary between a set of max_features and see which returns the best accuracy.\n\ndt_scores = []\nfor i in range(1, len(X.columns) + 1):\n    dt_classifier = DecisionTreeClassifier(max_features = i, random_state = 0)\n    dt_classifier.fit(X_train, y_train)\n    dt_scores.append(dt_classifier.score(X_test, y_test))","6ad7adb9":"# Select the maximum number of features from 1 to 30 for split and see the scores for each of those cases.\n\nplt.plot([i for i in range(1, len(X.columns) + 1)], dt_scores, color = 'green')\nfor i in range(1, len(X.columns) + 1):\n    plt.text(i, dt_scores[i-1], (i, dt_scores[i-1]))\nplt.xticks([i for i in range(1, len(X.columns) + 1)])\nplt.xlabel('Max features')\nplt.ylabel('Scores')\nplt.title('Decision Tree Classifier scores for different number of maximum features')","9cf8fbc0":"print(\"The score for Decision Tree Classifier is {:0.2f}% with {} maximum features.\".format(dt_scores[9]*100, [10]))","8b792906":"# Random Forest Classifier\n# Use the ensemble method, Random Forest Classifier, to create the model and vary the number of estimators to see their effect.\nrf_scores = []\nestimators = [10, 100, 200, 500,1000]\nfor i in estimators:\n    rf_classifier = RandomForestClassifier(n_estimators = i, random_state = 0)\n    rf_classifier.fit(X_train, y_train)\n    rf_scores.append(rf_classifier.score(X_test, y_test))","c5e8df59":"rf_scores[0]","46e1d77c":"colors = rainbow(np.linspace(0, 1, len(estimators)))\nplt.bar([i for i in range(len(estimators))], rf_scores, color = colors, width = 0.8)\nfor i in range(len(estimators)):\n    plt.text(i, rf_scores[i], rf_scores[i])\nplt.xticks(ticks = [i for i in range(len(estimators))], labels = [str(estimator) for estimator in estimators])\nplt.xlabel('Number of estimators')\nplt.ylabel('Scores')\nplt.title('Random Forest Classifier scores for different number of estimators')","b44a609d":"print(\"The score for Random Forest Classifier is {:0.2f}% with {} estimators.\".format(rf_scores[2]*100, [200]))","cf0d912e":"import xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nrf_scores = []\nxgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\nxgb_model.fit(X_train, y_train)\nY_pred_xgb = xgb_model.predict(X_test)\nrf_scores.append(xgb_model.score(X_test, y_test))","61f2647c":"score_xgb = round(accuracy_score(Y_pred_xgb,y_test)*100,2)\n\nprint(\"The accuracy score achieved using XGBoost is: \"+str(score_xgb)+\" %\")\nprint(\"The score for Random Forest Classifier is {:0.2f}% with {} estimators.\".format(rf_scores[0]*100, 2))\n","4ae8faf9":"**Machine Learning**\n\nimport train_test_split to split our dataset into training and testing datasets. ","fc372fab":"**Decision Tree Classifier**","256e8112":"**K Neighbors Classifier**\n\nThe classification score varies based on different values of neighbors that we choose. Plot a score graph for different values of K (neighbors) and check when we achieve the best score.","d3b57f1e":"dataset has a total of 303 rows and there are no missing values. There are a total of 13 features along with one target value which we will train the models to find.","6e1446a1":"**Data Processing**","4288f211":"**Descriptions of the columns in the dataset**\n\nage: The person's age in years\n\nsex: The person's sex (1 = male, 0 = female)\n\ncp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n\ntrestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n\nchol: The person's cholesterol measurement in mg\/dl\n\nfbs: The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)\n\nrestecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n\nthalach: The person's maximum heart rate achieved\n\nexang: Exercise induced angina (1 = yes; 0 = no)\n\noldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n\nslope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n\nca: The number of major vessels (0-3)\n\nthal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n\ntarget: Heart disease (0 = no, 1 = yes)\n","cc050d76":"**Visualize the data**","4a326a2c":"**Random Forest Classifier with a score of  88.52%  was the best model.**","7da0f944":"Each feature has a different range of distribution.  Scaling before our predictions should be helpful. Also, the categorical features do stand out.\n","b2d62a4e":"**Support Vector Classifier**","4088c95b":"The scale of each feature column is different and quite varied as well. While the maximum for age reaches 77, the maximum of chol (serum cholestoral) is 564.","f79ba839":"**Random Forest Classifier**"}}