{"cell_type":{"bd24a6ee":"code","1203a9cd":"code","b481b932":"code","6ed93807":"code","4dc5b739":"code","82873282":"code","e9ae609a":"code","4a7a7f41":"code","51f9e143":"code","fe406d83":"code","65be9056":"code","99d9d091":"code","e2dcd619":"code","2b0c1e2e":"code","e26579ca":"code","9c40828f":"code","2ab2a7f1":"code","c9580c05":"code","d4b530f2":"code","54910ab5":"code","99ca688b":"code","62e9a117":"code","47bdeda9":"code","3061f248":"code","fe39e3d5":"code","4f13e896":"code","27fd64f0":"code","da9e7901":"code","21d560f9":"code","222c7d3d":"code","2272acd9":"code","2b76176e":"code","4baa2c85":"code","75040973":"code","2293c7cd":"code","334c7691":"code","11e5dd20":"code","bfa62ec2":"code","20228ac6":"markdown","9d2f165c":"markdown","2a445c95":"markdown","0e1c3e66":"markdown","601ac410":"markdown","54448bf4":"markdown","60106827":"markdown","459af7b7":"markdown","72cddce5":"markdown","57e509b5":"markdown","f7d51071":"markdown","889f4373":"markdown","6bbbc726":"markdown","b3043a35":"markdown","1e66d816":"markdown","dec940af":"markdown","56193bc9":"markdown","c4257cf1":"markdown"},"source":{"bd24a6ee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']","1203a9cd":"import pandas as pd\ndf = pd.read_csv(\"\/kaggle\/input\/company-bankruptcy-prediction\/data.csv\")\ndf.head()","b481b932":"num_samples, num_features = df.shape\nprint(f\"Number of samples: {num_samples}\")\nprint(f\"Number of features: {num_features}\")","6ed93807":"# check statistics of dataframe\ndf.describe()","4dc5b739":"# check balance on target class -> required to ensure that target class is balanced, if not: need to apply some counter meastures\ncount_target_1 = df.loc[df[\"Bankrupt?\"] == 1].shape[0]\ncount_target_0 = df.loc[df[\"Bankrupt?\"] == 0].shape[0]\nprint(f\"Number of positive samples: {count_target_1} ({100 * count_target_1 \/ num_samples}% of total data).\")\nprint(f\"Number of negative samples: {count_target_0} ({100 * count_target_0 \/ num_samples}% of total data).\")","82873282":"# create X and Y data for further analysis\nX = df.drop(\"Bankrupt?\", axis=1).to_numpy()\nY = df[\"Bankrupt?\"].to_numpy()","e9ae609a":"# use a decision tree to find most important features to analyze further \nfrom sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier()\nmodel.fit(X, Y)","4a7a7f41":"#plot graph of feature importances for better visualization\nplt.figure(figsize=(12,14))\nfeat_importances = pd.Series(model.feature_importances_, index=df.columns[1:])\nfeat_importances.nlargest(50).plot(kind='barh')\nplt.show()","51f9e143":"feature_importances_50_largest = list(feat_importances.nlargest(50).index)\nfeature_importances_sorted = list(feat_importances.nlargest(num_features).index)","fe406d83":"import seaborn as sns","65be9056":"sns.pairplot(data=df[list([*feature_importances_50_largest[0:3], \"Bankrupt?\"])], hue=\"Bankrupt?\")","99d9d091":"sns.pairplot(data=df[list([*feature_importances_50_largest[3:6], \"Bankrupt?\"])], hue=\"Bankrupt?\")","e2dcd619":"corr_matrix = df.corr().abs().round(2)\n\ndisplay(corr_matrix)","2b0c1e2e":"indices_upper_tri = np.triu_indices(n=corr_matrix.shape[0])\ncorr_vals = corr_matrix.to_numpy()[indices_upper_tri]\nplt.hist(x=corr_vals, bins=20, range=[0.0, 0.99])\nplt.xlabel(\"Correlation value\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram of correlation value distribution\")\nplt.show()","e26579ca":"def get_list_of_important_features(df, sorted_feature_importance, corr_threshold):\n    # compute correlation matrix\n    corr_matrix = df.drop(\"Bankrupt?\", axis=1).corr().abs().round(2)\n    \n    # loop over sorted_feature_importance list and store only features which are not correlated more than corr_threshold\n    final_feature_list = []\n    blacklist_features = []\n    for feature in sorted_feature_importance:\n        if feature in blacklist_features: # skip if feature is on blacklist\n            continue\n        final_feature_list.append(feature)\n        list_features_above_th = list(corr_matrix.loc[feature, corr_matrix[feature] > corr_threshold].index)\n        \n        for feature_above_th in list_features_above_th:\n            # add to blacklist except feature_above_th is current feature (then has a correlation of 1)\n            if feature_above_th == feature:\n                continue\n            else:\n                blacklist_features.append(feature_above_th)\n    return final_feature_list","9c40828f":"final_feature_list = get_list_of_important_features(df, feature_importances_sorted, corr_threshold=0.6)\nprint(final_feature_list)","2ab2a7f1":"number_features_final = len(final_feature_list)\nprint(f\"New number of features: {number_features_final}\")","c9580c05":"X_new = df[final_feature_list]\nY_new = df[\"Bankrupt?\"]","d4b530f2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_new.to_numpy(), Y_new.to_numpy(), test_size=0.3)\n\nnum_samples_train = X_train.shape[0]\nnum_samples_test = X_test.shape[0]\nprint(f\"Number of samples of training data: {num_samples_train}\")\nprint(f\"Number of samples of test data: {num_samples_test}\")","54910ab5":"# check balance on target class -> required to ensure that target class is balanced, if not: need to apply some counter meastures\ncount_target_1 = np.sum(y_train)\ncount_target_0 = np.sum(y_train==0)\nprint(f\"Number of positive samples: {count_target_1} ({100 * count_target_1 \/ num_samples_train}% of total data).\")\nprint(f\"Number of negative samples: {count_target_0} ({100 * count_target_0 \/ num_samples_train}% of total data).\")","99ca688b":"# check the same for the final test set\n# check balance on target class -> required to ensure that target class is balanced, if not: need to apply some counter meastures\ncount_target_1 = np.sum(y_test)\ncount_target_0 = np.sum(y_test==0)\nprint(f\"Number of positive samples: {count_target_1} ({100 * count_target_1 \/ num_samples_test}% of total data).\")\nprint(f\"Number of negative samples: {count_target_0} ({100 * count_target_0 \/ num_samples_test}% of total data).\")","62e9a117":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","47bdeda9":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(max_iter=1000).fit(X_train, y_train)","3061f248":"print(f\"Accuracy on test set: {model.score(X_test, y_test)}\")\nprint(f\"Recall: {recall_score(y_test, model.predict(X_test))}\")\nprint(f\"Precision: {precision_score(y_test, model.predict(X_test))}\")\nprint(f\"F1-Score: {f1_score(y_test, model.predict(X_test))}\")","fe39e3d5":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(model, X_test, y_test)","4f13e896":"model_balanced = LogisticRegression(class_weight=\"balanced\", max_iter=1000).fit(X_train, y_train)\nprint(f\"Accuracy: {model_balanced.score(X_test, y_test)}\")\nprint(f\"Recall: {recall_score(y_test, model_balanced.predict(X_test))}\")\nprint(f\"Precision: {precision_score(y_test, model_balanced.predict(X_test))}\")\nprint(f\"F1-Score: {f1_score(y_test, model_balanced.predict(X_test))}\")","27fd64f0":"plot_confusion_matrix(model_balanced, X_test, y_test)","da9e7901":"# first: balance data set using random over-sampling to set focus on recall\nfrom imblearn.over_sampling import RandomOverSampler\n\nover_sample = RandomOverSampler()\nX_train, y_train = over_sample.fit_resample(X_train, y_train)\n\n# check balance on target class -> required to ensure that target class is balanced, if not: need to apply some counter meastures\nnum_samples_train = X_train.shape[0]\ncount_target_1 = np.sum(y_train)\ncount_target_0 = np.sum(y_train==0)\nprint(f\"Number of positive samples: {count_target_1} ({100 * count_target_1 \/ num_samples_train}% of total data).\")\nprint(f\"Number of negative samples: {count_target_0} ({100 * count_target_0 \/ num_samples_train}% of total data).\")","21d560f9":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport xgboost as xgb\n\nclassifiers_dict = {\n    \"decision tree\": DecisionTreeClassifier(),\n    \"random forest\": RandomForestClassifier(n_estimators=100),\n    \"gradient boosting\": GradientBoostingClassifier(n_estimators=100),\n    \"xgboost\": xgb.XGBClassifier(eval_metric=\"logloss\", max_depth=5, learning_rate=0.01, n_estimators=100)\n}","222c7d3d":"for clf_name, clf in classifiers_dict.items():\n    print(f\"Start evaluating {clf_name}...\")\n    clf.fit(X_train, y_train)\n    \n    print(f\"Accuracy: {clf.score(X_test, y_test)}\")\n    \n    f1_val = f1_score(y_test, clf.predict(X_test))\n    precision_val = precision_score(y_test, clf.predict(X_test))\n    recall_val = recall_score(y_test, clf.predict(X_test))\n    print(f\"Precision: {precision_val}\")\n    print(f\"Recall: {recall_val}\")\n    print(f\"F1-Score: {f1_val}\")\n    print(\"\\n\\n\")","2272acd9":"plot_confusion_matrix(classifiers_dict[\"decision tree\"], X_test, y_test)\n    ","2b76176e":"plot_confusion_matrix(classifiers_dict[\"random forest\"], X_test, y_test)\n    ","4baa2c85":"plot_confusion_matrix(classifiers_dict[\"gradient boosting\"], X_test, y_test)\n    ","75040973":"plot_confusion_matrix(classifiers_dict[\"xgboost\"], X_test, y_test)\n    ","2293c7cd":"from skopt import BayesSearchCV\n\nopt = BayesSearchCV(\n    GradientBoostingClassifier(n_iter_no_change=10),\n    {\n        \"loss\": [\"deviance\", \"exponential\"],\n        \"learning_rate\": (1e-4, 1e-1, \"log-uniform\"),\n        \"n_estimators\": (10, 100),\n        \"subsample\": (0.5, 1.0),\n        \"criterion\": [\"friedman_mse\", \"mse\", \"mae\"],\n        \"max_depth\": (2, 5),\n        \"max_features\": [\"auto\", \"sqrt\", \"log2\", None],\n    },\n    n_iter=32,\n    cv=3,\n    verbose=1,\n    scoring=\"recall\"\n)","334c7691":"opt.fit(X_train, y_train)","11e5dd20":"print(\"val. score: %s\" % opt.best_score_)\nprint(\"test score: %s\" % opt.score(X_test, y_test))\nprint(\"best params: %s\" % str(opt.best_params_))","bfa62ec2":"print(f\"Recall: {recall_score(y_test, opt.predict(X_test))}\")\nprint(f\"Precision: {precision_score(y_test, opt.predict(X_test))}\")\nprint(f\"F1-Score: {f1_score(y_test, opt.predict(X_test))}\")","20228ac6":"# Hyperparameter Optimization Gradient Boosting Classifier\n\nLets start with bayesian hyperparameter optimization of the gradient boosting classifier, because he had the best recall while still having feasible precision measured on the test set.","9d2f165c":"# Create Dataset by removing too strong correlated features\n\nThe goal is to create a dataset only containing the most important features while removing too strong correlated features, because they don't include additional information and can lead to a decreasing prediction performance of the final model. ","2a445c95":"As one can see, the accuracy is now lower than before, but the model has more positive predicted labels which are indeed true. In reality, this model would be preferred over the first one, because it's way more important to correctly predict whether a company is banrupt than to predict that the company is not bankrupt.","0e1c3e66":"Now only 58 features are left, but they should be enough because all other features had a too large correlation to the current feature set and therefore are not boosting the performance further.","601ac410":"# Visualizations\n\nLet's now start with visualizing some of the relationships of the features to the target data in order to get a better understanding of the data.","54448bf4":"# Feature Correlation\n\nCheck the correlation between the features. This is to ensure, that the model doesn't get too highly correlated features.","60106827":"# Create Train and Test sets","459af7b7":"The accuracy is quite larger, but be carefull: The accuracy can be misleading, because the dataset is very imbalanced. The model could only predict False and would have an accuracy of 0.9687!\nLet's take a closer look into the Confusion Matrix.","72cddce5":"# Train Logistic Regression Baseline Model\n\nIt's always good to start with an easy baseline model. In general, the easier a model, the better it can be understood. So it's preferable to have an as easy as possible model. The baseline can then be used to benchmark other approaches in order to find the best performing one.","57e509b5":"# Analyze Dataframe","f7d51071":"# Train different Classifiers to Improve Performance","889f4373":"As one can see, the model has a lot of false negatives which is very bad! <br>\n<br>\nLet's include sample weights in order to boost the learning towards the minority class. ","6bbbc726":"Lets now train more classifiers to further improve the performance. <br>\nThe following classifiers shall be evaluated: \n<ol>\n    <li> Decision Tree <\/li>\n    <li> Random Forest <\/li>\n    <li> Gradient Boosting Classifier <\/li>\n<\/ol>","b3043a35":"Most of the features are only correlated very small to each other and only a small portion of the features is correlated larger. Therefore, a correlation threshold can be choosen to be something like 0.6 or 0.7. ","1e66d816":"Get upper triangle of correlation matrix in order to plot the distribution of correlation values. The upper triangle is only required, because the correlation matrix is symmetric.","dec940af":"Check imbalance on training data.","56193bc9":"As one can see, the dataset is very imbalanced with only a small ratio of positive samples and the majority of samples being negative. Therefore, the accuracy is not a quite good indicator here. It's better to use the F1-Score.","c4257cf1":"# Scale Data \n<br>\nScale the data using the StandardScaler of scikit-learn. Important: Only train standard scaler on training data to not introduce a data leakage from training data to test data."}}