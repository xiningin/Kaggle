{"cell_type":{"cfae1ec8":"code","f082b36e":"code","9ed224a6":"code","94fb3361":"code","7b877937":"code","f112e7e8":"code","64eec54b":"code","94b2e50f":"code","27d46a56":"code","2149a783":"code","c7b78c8e":"code","a57a7517":"code","a3410c2e":"code","2badc3d8":"code","a0904f3b":"code","14e0c9ba":"code","b0865148":"code","fd566c1e":"code","986fd0c7":"code","5af02612":"code","ee80a972":"code","6eff90dd":"code","67ceb6cb":"code","9fd99c87":"code","ac7f74be":"code","0e0a814e":"code","83666bc4":"code","69349781":"code","c205e934":"code","89289a22":"code","f9c2beac":"markdown","5eaa641b":"markdown","457aa3ef":"markdown","76c8bda7":"markdown","c56c473e":"markdown","a795cb2d":"markdown","4775e4d6":"markdown","bee2c037":"markdown","a53369e0":"markdown","fd25ddd7":"markdown","6bc273db":"markdown","cb9fe68c":"markdown","6f59d9ec":"markdown","387318a1":"markdown","8495bf21":"markdown"},"source":{"cfae1ec8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f082b36e":"training = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n%matplotlib inline","9ed224a6":"num_train = training[['SibSp', 'Age', 'Parch', 'Fare']]\nnonnum_train = training[['Survived', 'Pclass', 'Sex', 'Ticket', 'Cabin', 'Embarked']]","94fb3361":"# distribution for all numberical features\nfor i in num_train.columns:\n    plt.hist(num_train[i])\n    plt.title(i)\n    plt.show()","7b877937":"pd.pivot_table(training, index = 'Survived', values = num_train)","f112e7e8":"for i in nonnum_train.columns:\n    freq_series = pd.Series(nonnum_train[i].value_counts())\n\n    plt.figure()\n    fig = freq_series.plot(kind='bar')\n    fig.set_title(i)\n    fig.set_ylabel('Deaths')","64eec54b":"print(pd.pivot_table(nonnum_train, index = 'Survived', columns = 'Pclass', values = 'Ticket' ,aggfunc ='count'))\nprint(pd.pivot_table(nonnum_train, index = 'Survived', columns = 'Embarked', values = 'Ticket' ,aggfunc ='count'))\nprint(pd.pivot_table(nonnum_train, index = 'Survived', columns = 'Sex', values = 'Ticket' ,aggfunc ='count'))","94b2e50f":"training['Cabin'].value_counts().head(10)","27d46a56":"lam = (lambda x: 0 if pd.isna(x) else len(x.split(\" \")))\n# function returns 0 if value is not available in df, return length of list if value(s) found\ntraining['Cabin_mul'] = training.Cabin.apply(lam)\nprint(training['Cabin_mul'].value_counts())\n\npd.pivot_table(training, index = 'Survived', columns = 'Cabin_mul', values = 'Ticket' ,aggfunc ='count')","2149a783":"lam2 = (lambda x: str(x)[0])\n# function returns first item inside string list, returning the letter of the cabin, or lack of cabin object\ntraining['Cabin_let'] = training['Cabin'].apply(lam2)\nprint(training['Cabin_let'].value_counts())\npd.pivot_table(training, index = 'Survived', columns = 'Cabin_let', values = 'Ticket' ,aggfunc ='count')","c7b78c8e":"def ticket_str(x):\n    p1 = ''.join(x.split(\" \")[:-1])\n    p1 = p1.replace('\/', '').replace('.', '').lower()\n    return p1\n    \ndef ticket_num(x):\n    if x.isnumeric():\n        return 1\n    return 0\n\ntraining['Ticket_str'] = training['Ticket'].apply(ticket_str)\ntraining['Ticket_num'] = training['Ticket'].apply(ticket_num)","a57a7517":"pd.pivot_table(training, index = 'Survived', columns = 'Ticket_num', values = 'Ticket' ,aggfunc ='count')","a3410c2e":"pd.pivot_table(training, index = 'Survived', columns = 'Ticket_str', values = 'Ticket' ,aggfunc ='count')","2badc3d8":"def get_title(x):\n    title = x.split(\",\")[1].split('.')[0].strip()\n    return title","a0904f3b":"training['Title'] = training.Name.apply(get_title)\npd.pivot_table(training, index = 'Survived', columns = 'Title', values = 'Ticket' ,aggfunc ='count')","14e0c9ba":"training['train_test'] = 1\ntest['train_test'] = 0\ntest['Survived'] = np.NaN\n\nall_data = pd.concat([training, test])\n\nall_data.dropna(subset=['Embarked'],inplace = True)\n\nall_data.Age = all_data.Age.fillna(training.Age.median())\nall_data.Fare = all_data.Fare.fillna(training.Fare.median())\n\nall_data['norm_fare'] = np.log(all_data.Fare+1)\n\nall_data['Cabin_mul'] = all_data.Cabin.apply(lam)\nall_data['Cabin_let'] = all_data['Cabin'].apply(lam2)\nall_data['Ticket_str'] = all_data['Ticket'].apply(ticket_str)\nall_data['Ticket_num'] = all_data['Ticket'].apply(ticket_num)\nall_data['Title'] = all_data.Name.apply(get_title)\n\n\nall_dummies = pd.get_dummies(all_data[['Pclass','Sex','Age','SibSp','Parch','norm_fare',\n                                       'Embarked','Cabin_let','Cabin_mul','Ticket_num','Title','train_test']])\n\nx_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis =1)\nx_test = all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = all_data[all_data.train_test==1].Survived","b0865148":"from sklearn.preprocessing import StandardScaler\n\nscale = StandardScaler()\nall_dummies_scaled = all_dummies.copy()\nall_dummies_scaled[['Age', 'SibSp', 'Parch', 'norm_fare']]= scale.fit_transform(all_dummies_scaled[['Age', 'SibSp', 'Parch', 'norm_fare']])\n\nx_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis =1)\nx_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = all_data[all_data.train_test==1].Survived","fd566c1e":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier","986fd0c7":"lr = LogisticRegression(max_iter = 1500)\ncv = cross_val_score(lr, x_train_scaled, y_train, cv=5)\nprint(cv)\nprint(cv.mean()*100)","5af02612":"svc = SVC(probability = True)\ncv = cross_val_score(svc, x_train_scaled, y_train, cv=5)\nprint(cv)\nprint(cv.mean()*100)","ee80a972":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn,x_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean()*100)","6eff90dd":"from sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators = [('lr',lr), ('knn',knn), ('svc',svc)], voting = 'soft') \n\ncv = cross_val_score(voting_clf, x_train_scaled, y_train, cv=5)\nprint(cv)\nprint(cv.mean()*100)","67ceb6cb":"from sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV ","9fd99c87":"def clf_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))","ac7f74be":"lr = LogisticRegression()\nparam_grid = {'max_iter' : [2000],\n              'penalty' : ['l1', 'l2'],\n              'C' : np.logspace(-4, 4, 20),\n              'solver' : ['liblinear']}\n\nclf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_lr = clf_lr.fit(x_train_scaled,y_train)\nclf_performance(best_clf_lr,'Logistic Regression')","0e0a814e":"knn = KNeighborsClassifier()\nparam_grid = {'n_neighbors' : [3,5,7,9],\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'p' : [1,2]}\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(x_train_scaled,y_train)\nclf_performance(best_clf_knn,'KNN')","83666bc4":"svc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n                                  'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(x_train_scaled,y_train)\nclf_performance(best_clf_svc,'SVC')","69349781":"best_lr = best_clf_lr.best_estimator_\nbest_knn = best_clf_knn.best_estimator_\nbest_svc = best_clf_svc.best_estimator_\n\nvoting_clf_hard = VotingClassifier(estimators = [('knn',best_knn),('svc',best_svc)], voting = 'hard') \nvoting_clf_soft = VotingClassifier(estimators = [('knn',best_knn),('svc',best_svc)], voting = 'soft') \nvoting_clf_all = VotingClassifier(estimators = [('knn',best_knn),('svc',best_svc), ('lr', best_lr)], voting = 'soft') \n\nprint('voting_clf_hard mean :',cross_val_score(voting_clf_hard,x_train,y_train,cv=5).mean())\n\nprint('voting_clf_soft mean :',cross_val_score(voting_clf_soft,x_train,y_train,cv=5).mean())\n\nprint('voting_clf_all mean :',cross_val_score(voting_clf_all,x_train,y_train,cv=5).mean())","c205e934":"voting_clf_hard.fit(x_train_scaled, y_train)\nvoting_clf_soft.fit(x_train_scaled, y_train)\nvoting_clf_all.fit(x_train_scaled, y_train)\n\ny_hat_vc_hard = voting_clf_hard.predict(x_test_scaled).astype(int)\ny_hat_vc_soft = voting_clf_soft.predict(x_test_scaled).astype(int)\ny_hat_vc_all = voting_clf_all.predict(x_test_scaled).astype(int)","89289a22":"final_data = {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_hard}\nsubmission = pd.DataFrame(data=final_data)\n\nfinal_data_2= {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_soft}\nsubmission_2= pd.DataFrame(data=final_data_2)\n\nfinal_data_3= {'PassengerId': test.PassengerId, 'Survived': y_hat_vc_all}\nsubmission_3= pd.DataFrame(data=final_data_3)\n\nsubmission.to_csv('submission_vc_hard.csv',index=False)\nsubmission_2.to_csv('submission_vc_soft.csv', index=False)\nsubmission_3.to_csv('submission_vc_all.csv', index=False)","f9c2beac":"**The table comparing the letters on the ticket and survival shows certain tickets which increase or decrease rate of survival within realistic bounds**\n\n**the result of this feature engineering proves to be a useful feature of the dataset as the data shows correlation to the column survival**","5eaa641b":"# Titanic Exploratory Data Analysis \n\n### 1. Data exploration\n### 2. Feature engineering\n### 3. Data preprocessing\n### 4. Model building\n### 5. Model tuning\n### 6. Model comparison\n### 7. Results\n\nKen Jee's project example was used to learn about the process of dealing with this problem","457aa3ef":"**This table compares the numerical features with survived.**\n\n**On average, younger passengers survived, and those who spent more on their fare survived.**\n\n**Being apart of a family looks to improve your chances of survival, but having a spouse or more siblings decreases your chance of survival.\nThis is difficult to draw any conclusions from at this stage.**\n\n**From this table the largest influencing feature on survivability is Fare.**","76c8bda7":"after tuning, the support vector classifier shows the greatest cross validation performance, analysis of these models is further require to understand bias and variance","c56c473e":"## 4) Model building","a795cb2d":"new feature Title shows strong correlation with survived, making this a valuable addition\n\nhaving explored the data, only certain variables are now useful for predicting survived including:\n\n* title, ticket_num, cabin_let, cabin_mul, pclass, embarked, sex, fare, age, parch, sibsp\n\n","4775e4d6":"**creating a new feature via a function to return the number of cabins booked under a single ticket.\nthe data is incomplete, showing the majority of tickets with no cabin, however it shouldnt be a problem**\n\n**this column is useful for determining if booking multiple cabins influences survival rates.**\n\n**inside the table, comparison between survival and cabins_mul shows a increase in survival rates with multiple cabins (2+) in a range of 50% to 100% survival, greater than the 38% chance of survival with fewer cabins**\n\n**this proves to be a useful feature of the dataset as the data shows correlation to the target feature**","bee2c037":"## 3) Data preprocessing","a53369e0":"**new features breaking down ticket into numeric boolean and the letters on the ticket provide more value than ticket as a whole**\n\n**the string portion of the ticket still doesnt show any particular value or relevant correlation to survived**","fd25ddd7":"## 1) Data exploration\nGet an idea of the shape of the data, split numerical and non numerical data into dataframes","6bc273db":"**Age is the only numerical feature that shows a loose normal distribution and is reasonably scaled.**\n\n**All other features dont show any normal distribution and could be normalised.**","cb9fe68c":"## 5) Model tuning","6f59d9ec":"## 2) Feature engineering","387318a1":"in a soft voting classifier, each model return their respective confidence on survival of given data","8495bf21":"**From these bar charts, certain features show distinct correlation to the number of deaths, including: pclass, sex and embarked.\nFurther exploration of these features might show meaning behind the data.**\n\n**Other features, cabin and ticket are messy, making it hard to derive meaning from.**"}}