{"cell_type":{"d52d7e13":"code","2c8b9959":"code","e5cfb9ec":"code","58ae86ff":"code","76b574b2":"code","dad0c3e8":"code","ed245d96":"code","d88b381d":"code","99f81238":"code","e65b32c7":"code","f70871d1":"code","f07cc854":"code","13ca87f6":"code","f7f81483":"code","b2d03dce":"code","5bbe6597":"code","07f42784":"code","dca7865b":"code","be32393d":"code","1b3c7a2f":"code","1c7d3550":"code","af4b27cb":"code","672922a7":"code","9b37e86b":"code","bda65c3e":"code","6dd0937a":"code","b29e81e3":"code","3660b160":"code","0a2a0331":"code","b57d713f":"code","5dca81f0":"code","3e7da824":"code","a804f1c7":"code","1115813b":"code","27892bb6":"code","993e2d2f":"code","acf8b650":"markdown","81124ba2":"markdown","032f5981":"markdown","4a87de41":"markdown","21ccc6b5":"markdown","a8470983":"markdown","f49e7f68":"markdown","b90ca824":"markdown","3d307918":"markdown","6f42acea":"markdown","6c12787b":"markdown","1a2cb45a":"markdown","d2e1d87d":"markdown","bb99aa84":"markdown","5e8b89a3":"markdown","62a0d887":"markdown","67b7503b":"markdown","acb87787":"markdown","70b033ea":"markdown"},"source":{"d52d7e13":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2c8b9959":"cancer_dataset = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")","e5cfb9ec":"cancer_dataset","58ae86ff":"print(cancer_dataset.info())","76b574b2":"cancer_dataset.drop([\"id\", \"Unnamed: 32\"], axis = 1, inplace=True)","dad0c3e8":"cancer_dataset","ed245d96":"cancer_dataset.diagnosis = [1 if i == \"M\" else 0 for i in cancer_dataset.diagnosis]","d88b381d":"print(cancer_dataset.info())","99f81238":"cancer_dataset","e65b32c7":"x = cancer_dataset.drop([\"diagnosis\"], axis = 1)","f70871d1":"type(x)","f07cc854":"x","13ca87f6":"y = cancer_dataset[\"diagnosis\"].values","f7f81483":"type(y)","b2d03dce":"y","5bbe6597":"features_mean=list(x)\ndfM = cancer_dataset[cancer_dataset[\"diagnosis\"] == 1]\ndfB = cancer_dataset[cancer_dataset[\"diagnosis\"] == 0]\nplt.rcParams.update({\"font.size\": 10})\nfig, axes = plt.subplots(nrows = 5, ncols = 2, figsize=(15,20))\naxes = axes.ravel()\n\nfor idx,ax in enumerate(axes):\n    ax.figure\n    binwidth= (max(cancer_dataset[features_mean[idx]]) - min(cancer_dataset[features_mean[idx]]))\/50\n    ax.hist([dfM[features_mean[idx]],dfB[features_mean[idx]]], bins=np.arange(min(cancer_dataset[features_mean[idx]]),\n            max(cancer_dataset[features_mean[idx]]) + binwidth, binwidth) , alpha=0.5, stacked=True, density = True,\n            label=[\"M\", \"B\"], color = [\"red\",\"green\"])\n    ax.legend(loc = \"upper right\")\n    ax.set_title(features_mean[idx] + \"(mm)\")\nplt.tight_layout()\nplt.show()","07f42784":"x = (x - np.min(x)) \/ (np.max(x) - np.min(x)).values    # (x-min(x))\/(max(x)-minx)) -> Normalization formula","dca7865b":"x","be32393d":"from sklearn.model_selection import train_test_split","1b3c7a2f":"xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.2, random_state = 42)","1c7d3550":"print(\"xtrain shape:\", xtrain.shape)\nprint(\"xtest shape:\", xtest.shape)\nprint(\"ytrain shape:\", ytrain.shape)\nprint(\"ytest shape:\", ytest.shape)","af4b27cb":"xtrain_transpose = xtrain.T","672922a7":"xtrain_transpose.head(10)","9b37e86b":"xtest_transpose = xtest.T\nytrain_transpose = ytrain.T\nytest_transpose = ytest.T","bda65c3e":"print(\"Shape of the xtrain_transpose:\", xtrain_transpose.shape)\nprint(\"Shape of the xtest_transpose:\", xtest_transpose.shape)\nprint(\"Shape of the ytrain_transpose:\", ytrain_transpose.shape)\nprint(\"Shape of the ytest_transpose:\", ytest_transpose.shape)","6dd0937a":"def inialize_weights_and_bias(dimension): # dimension = 30\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b","b29e81e3":"w,b = inialize_weights_and_bias(30)\nprint(\"w:\", w)\nprint(\"\")\nprint(\"b:\",b)","3660b160":"def sigmoid_function(z):\n    result = 1\/(1+np.exp(-z))\n    return result","0a2a0331":"sigmoid_function(0) # The result should be 0.5.","b57d713f":"w.shape","5dca81f0":"xtrain_transpose.shape","3e7da824":"def forward_and_backward_propagation(w, b, xtrain_transpose, ytrain_transpose):\n    \n    # ---------- Forward propagation -----------\n    # z = wT . xtrain + b\n    z = np.dot(w.T,xtrain_transpose) + b\n    y_head = sigmoid_function(z)\n    # loss = -(ylog(y^) + (1-y) * log(1-y^))\n    loss_function = -(ytrain_transpose * np.log(y_head) + (1-ytrain_transpose) * np.log(1-y_head))\n    cost_function = (np.sum(loss_function) \/ xtrain_transpose.shape[1])\n    # xtrain_transpose.shape[1] -> 455 : This part is done for scaling.\n    \n    # --------- Backward Propagation ------------\n    \n    derivative_weight = (np.dot(xtrain_transpose, ((y_head-ytrain_transpose).T))) \/ xtrain_transpose.shape[1]\n    derivative_bias = np.sum(y_head-ytrain_transpose) \/ xtrain_transpose.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost_function, gradients  ","a804f1c7":"def update_parameters(w, b, x_train_transpose, y_train_transpose, learning_rate, iteration_number):\n    all_costs = []\n    each_10_costs = []\n    index = []\n    \n    for i in range(iteration_number):\n        \n        # --------- finding cost and gradient values -----------\n        \n        cost, gradients = forward_and_backward_propagation(w, b, x_train_transpose, y_train_transpose)\n        all_costs.append(cost)\n        \n        # --------- Updating weight and bias ----------\n        \n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        \n        if i % 10 == 0:\n            each_10_costs.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # Updating(learning) weights and bias parameters\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.figure(figsize=(10,6))\n    plt.plot(index, each_10_costs, color = \"orange\")\n    plt.xticks(index, rotation='vertical')\n    plt.xlabel(\"Iteration number\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, all_costs","1115813b":"def predict(w, b, xtest_transpose):\n    \n    z = sigmoid_function(np.dot(w.T, xtest_transpose) + b)\n    prediction = np.zeros((1,xtest_transpose.shape[1]))\n    \n    # z > 0.5 -> y_head=1\n    # z < 0.5 -> y_head=0\n    \n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            prediction[0,i] = 0\n        else:\n            prediction[0,i] = 1\n\n    return prediction","27892bb6":"def logistic_regression(xtrain_transpose, ytrain_transpose, xtest_transpose, ytest_transpose, learning_rate ,  iteration_number):\n\n    dimension =  xtrain_transpose.shape[0] \n    w, b = inialize_weights_and_bias(dimension)\n    parameters, gradients, all_costs = update_parameters(w, b, xtrain_transpose, ytrain_transpose, learning_rate, iteration_number)\n    \n    prediction = predict(parameters[\"weight\"],parameters[\"bias\"],xtest_transpose)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(prediction - ytest_transpose)) * 100))","993e2d2f":"logistic_regression(xtrain_transpose, ytrain_transpose, xtest_transpose, ytest_transpose, learning_rate = 1, iteration_number = 300)    ","acf8b650":"## Parameter Initialization and Sigmoid Function","81124ba2":"## Forward and Backward Propagation","032f5981":"![](https:\/\/machinethink.net\/images\/tensorflow-on-ios\/LogisticRegression@2x.png)","4a87de41":"Since we do not need \"id\" and \"Unnamed: 32\" columns, I am going to drop them.","21ccc6b5":"![](https:\/\/miro.medium.com\/max\/4000\/0*0XRrnsr7h5hebu8r.png)","a8470983":"## Creating the Model","f49e7f68":"## Dataset Information\n\n* Dataset Characteristics: Multivariate\n* Attribute Characteristics: Real\n* Attribute Characteristics: Classification\n* Number of Instances: 569\n* Number of Attributes: 32\n* Missing Values: No","b90ca824":"## Logistic Regression","3d307918":"## Updating Parameters","6f42acea":"![](https:\/\/www.foxchase.org\/sites\/fccc\/files\/breast-cancer-awareness.jpg)","6c12787b":"As you see, we converted each values between 0 and 1.","1a2cb45a":"In \"diagnosis\" column, instead of using M or B, I will convert them 0's and 1's.","d2e1d87d":"![](https:\/\/miro.medium.com\/max\/2908\/1*Hs7RCpyvj4NrjANdwiFHaQ@2x.jpeg)","bb99aa84":"## Normalization","5e8b89a3":"In the following picture, you can see the overall idea. The only difference is we want to predict if the tumor is malignant or benign.","62a0d887":"## Prediction Part","67b7503b":"* w.shape = (30,1)\n* xtrain_transpose.shape = (30,455)\n\nIn order to do a matrix multiplication, we need to take the transpose of the weight variables.","acb87787":"### Sigmoid function:\n\n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcT_G9kn6bEs0wdfwt9lH4I5R7ZFHb7RHrUxqQ&usqp=CAU)","70b033ea":"## Column Names and Meanings\n* id: ID number\n* diagnosis: The diagnosis of breast tissues (M = malignant, B = benign)\n* radius_mean: mean of distances from center to points on the perimeter\n* texture_mean: standard deviation of gray-scale values\n* perimeter_mean: mean size of the core tumor\n* area_mean: area of the tumor\n* smoothness_mean: mean of local variation in radius lengths\n* compactness_mean: mean of perimeter^2 \/ area - 1.0\n* concavity_mean: mean of severity of concave portions of the contour\n* concave_points_mean: mean for number of concave portions of the contour\n* symmetry_mean\n* fractal_dimension_mean: mean for \"coastline approximation\" - 1\n* radius_se: standard error for the mean of distances from center to points on the perimeter\n* texture_se: standard error for standard deviation of gray-scale values\n* perimeter_se\n* area_se\n* smoothness_se: standard error for local variation in radius lengths\n* compactness_se: standard error for perimeter^2 \/ area - 1.0\n* concavity_se: standard error for severity of concave portions of the contour\n* concave_points_se: standard error for number of concave portions of the contour\n* symmetry_se\n* fractal_dimension_se: standard error for \"coastline approximation\" - 1\n* radius_worst: \"worst\" or largest mean value for mean of distances from center to points on the perimeter\n* texture_worst: \"worst\" or largest mean value for standard deviation of gray-scale values\n* perimeter_worst\n* area_worst\n* smoothness_worst: \"worst\" or largest mean value for local variation in radius lengths\n* compactness_worst: \"worst\" or largest mean value for perimeter^2 \/ area - 1.0\n* concavity_worst: \"worst\" or largest mean value for severity of concave portions of the contour\n* concave_points_worst: \"worst\" or largest mean value for number of concave portions of the contour\n* symmetry_worst\n* fractal_dimension_worst: \"worst\" or largest mean value for \"coastline approximation\" - 1"}}