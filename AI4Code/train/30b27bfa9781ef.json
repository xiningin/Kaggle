{"cell_type":{"820f286d":"code","3005022a":"code","b9621dad":"code","e5c57d94":"code","5ddf65ff":"code","6317d606":"code","f21ddae5":"code","970fca5e":"code","bc54a133":"code","0acbf8ff":"code","7c6b1978":"code","fd059041":"code","3596c14e":"code","4e7b1c27":"markdown","e95c68fc":"markdown","11f68879":"markdown","6cb4fd7e":"markdown","37604497":"markdown","5dad7de3":"markdown","b329f6a2":"markdown","af03eb15":"markdown","52226975":"markdown","af274c87":"markdown","9c01aa7d":"markdown","cd5d8351":"markdown"},"source":{"820f286d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot  as plt # data visualization","3005022a":"!pip install --upgrade pip\n!pip install pymap3d==2.1.0\n!pip install -U l5kit","b9621dad":"import os\nfrom l5kit.configs import load_config_data\n\n# set env variable for data\nos.environ[\"L5KIT_DATA_FOLDER\"] = \"..\/input\/lyft-motion-prediction-autonomous-vehicles\"\n# get config\ncfg = load_config_data(\"..\/input\/lyft-config-files\/visualisation_config.yaml\")","e5c57d94":"print(f'current raster_param:\\n')\nfor k,v in cfg[\"raster_params\"].items():\n    print(f\"{k}:{v}\")","5ddf65ff":"from l5kit.data import ChunkedDataset, LocalDataManager\n\ndm = LocalDataManager()\ndataset_path = dm.require(cfg[\"val_data_loader\"][\"key\"])\nzarr_dataset = ChunkedDataset(dataset_path)\nzarr_dataset.open()\nprint(zarr_dataset)","6317d606":"from tqdm import tqdm\n\nframes = zarr_dataset.frames\ncoords = np.zeros((len(frames), 2))\nfor idx_coord, idx_data in enumerate(tqdm(range(len(frames)), desc=\"getting centroid to plot trajectory\")):\n    frame = zarr_dataset.frames[idx_data]\n    coords[idx_coord] = frame[\"ego_translation\"][:2]","f21ddae5":"plt.scatter(coords[:, 0], coords[:, 1], marker='.')\naxes = plt.gca()\naxes.set_xlim([-1500, 1600])\naxes.set_ylim([-2500, 1600])","970fca5e":"from l5kit.dataset import (EgoDataset,      # iterates over the AV annotations\n                           AgentDataset)    #iterates over other agents annotations\n\nfrom l5kit.rasterization import build_rasterizer\n\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)","bc54a133":"from l5kit.geometry import transform_points\n\nfrom l5kit.visualization import (draw_trajectory,       # draws 2D trajectories from coordinates and yaws offset on an image\n                                 TARGET_POINTS_COLOR)\n\ndata = dataset[50]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n\nplt.imshow(im[::-1])\nplt.show()","0acbf8ff":"dataset = AgentDataset(cfg, zarr_dataset, rast)\ndata = dataset[50]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n\nplt.imshow(im[::-1])\nplt.show()","7c6b1978":"from IPython.display import display, clear_output\nimport PIL\n \ncfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"   # This can be changed to \"py_semantic\" to have a view similar to the ones before\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)\nscene_idx = 50\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","fd059041":"from matplotlib import animation, rc\n\ndef animate_solution(images):\n\n    def animate(i):\n        im.set_data(images[i])\n \n    fig, ax = plt.subplots()\n    im = ax.imshow(images[0])\n    \n    return animation.FuncAnimation(fig, animate, frames=len(images), interval=60)","3596c14e":"from IPython.display import HTML\n\nscene_idx = 50\nindexes = dataset.get_scene_indices(scene_idx)\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    images.append(PIL.Image.fromarray(im[::-1]))\n    \nanim = animate_solution(images)\nHTML(anim.to_jshtml())","4e7b1c27":"# Lyft basics: fetch the data, visualize it\n\nSince I need to get acquainted with the data myself, I thought sharing this notebook for other people who are starting now the competition would be good.\n\nAs I always do for this kind of notebooks, I am handling the imports cell by cell, so if you need just a part of what I am describing here it will be easier for you to figure out which imports you need (at least, this is my hope). I make an exception for the most common imports:","e95c68fc":"# Visualising the Autonomous Vehicle (AV)","11f68879":"This example shows how `.zarr` files support most of the traditional numpy array operations. \n\nAs an example, here we iterate over the frames to get a scatter plot of the AV locations:","6cb4fd7e":"# Seeing the scene in movement\n\nFor completion sake, I will now take some inspiration\/code from this other [notebook](https:\/\/www.kaggle.com\/nxrprime\/lyft-understanding-the-data-and-eda) to animate what we just saw","37604497":"First of all, let's pip install l5kit (Level 5 Kit). For this notebook I am relying heavily on their [documentation](https:\/\/github.com\/lyft\/l5kit).","5dad7de3":"# Work in progress - Will continue","b329f6a2":"Let's now move to ","af03eb15":"# Visualizing a whole scene","52226975":"# Visualising an agent","af274c87":"At this point we need a yaml file to configure the visualization. The one from [l5kit github](https:\/\/github.com\/lyft\/l5kit) will do just fine. If you aren't just copy this notebook, there is already a [dataset](https:\/\/www.kaggle.com\/jpbremer\/lyft-config-files) you can add to your notebook to have the files (or you can download your own copy and add it, but kaggle will suggest you use the existing dataset to avoid having too many copies of the same thing).","9c01aa7d":"\\- when loaded in python, the `yaml` file is converted into a python `dict`. \n\n`raster_params` contains all the information related to the transformation of the 3D world onto an image plane. Let's look into it:\n","cd5d8351":"  - `raster_size`: the image plane size\n  - `pixel_size`: how many meters correspond to a pixel\n  - `ego_center`: the raster is centered around an agent, this parameter moves the agent in the image\n  - `map_type`: the rasterizer to be employed. The ones currently supported are a satellite-based and a semantic-based one\n\nTo read the data, we simply do:"}}