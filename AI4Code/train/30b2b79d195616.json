{"cell_type":{"162bb449":"code","b2f9e7b5":"code","13ba2581":"code","5d87a2a1":"code","b09f1fa9":"code","f7575160":"code","ca57e0e7":"code","29bbc9da":"code","f8ea5513":"code","3bd311fd":"code","583d21c3":"code","4b4bf531":"code","cb4ad463":"code","6c4a3d64":"code","ffbf0a79":"code","31b791f5":"code","316b4288":"code","e80e9805":"code","928e945c":"code","af144e8b":"code","141d02ab":"code","2ff61eb6":"code","5125b7ad":"code","24ae574e":"code","f5c05f46":"code","90118d4b":"code","15b611b2":"code","ae6b2921":"code","3369f384":"code","3cc77527":"code","1a3147b6":"markdown","1f192da2":"markdown","966683f3":"markdown","323b6a11":"markdown","1c2e6ecd":"markdown","4ff48239":"markdown","71d1aff2":"markdown","a4fbffd0":"markdown","60872779":"markdown","069f281c":"markdown","b92601ba":"markdown","066c11ac":"markdown","e6907566":"markdown","bcecc4d6":"markdown","c87edbfa":"markdown","34689407":"markdown","192a4703":"markdown","2e4fc3cc":"markdown","da9b4c5f":"markdown","32fc5ac1":"markdown","5792d88b":"markdown"},"source":{"162bb449":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n\n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b2f9e7b5":"# Using pandas function read_csv to read the csv file\niris = pd.read_csv(\"..\/input\/iris\/Iris.csv\")","13ba2581":"# Below command display the first 5 rows. Since the number of rows is not defined, the default value is 5 and hence this command displays the first 5 records\niris.head()","5d87a2a1":"# To drop the \"Id\" column we use the drop function. We also specify the axis. Meaning, if we are dropping a column then axis = 1 and if we are dropping a specific row, then axis=0\n# Also, inplace implies that we want the changes to be reflected in the variable \"iris\" dataframe\niris.drop(\"Id\", axis=1, inplace=True)","b09f1fa9":"# again viewing the data to confirm that we dropped the \"Id\" column from our dataframe\niris.head()","f7575160":"# Lets take a look at the shape of the data set. This gives us an idea about the number of rows and columns that are present\niris.shape","ca57e0e7":"iris.describe()","29bbc9da":"iris.info()","f8ea5513":"sns.FacetGrid(iris, hue=\"Species\", height=5).map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\").add_legend()","3bd311fd":"sns.FacetGrid(iris, hue=\"Species\", height=6).map(sns.kdeplot, \"SepalLengthCm\").add_legend()","583d21c3":"sns.FacetGrid(iris, hue=\"Species\", height=6).map(sns.kdeplot, \"SepalWidthCm\").add_legend()","4b4bf531":"sns.FacetGrid(iris, hue=\"Species\", height=6).map(sns.kdeplot, \"PetalLengthCm\").add_legend()","cb4ad463":"sns.FacetGrid(iris, hue=\"Species\", height=6).map(sns.kdeplot, \"PetalWidthCm\").add_legend()","6c4a3d64":"sns.pairplot(iris, hue=\"Species\", height=3)","ffbf0a79":"#importing the required libraries\nfrom sklearn.linear_model import LogisticRegression     #importing Logistic Regression\nfrom sklearn.model_selection import train_test_split   #for splitting the data into train and test sets\nfrom sklearn.neighbors import KNeighborsClassifier    #importing K nearest neighbors algo\nfrom sklearn import svm                                 #importing Support vector machines\nfrom sklearn import metrics                             #for checking the algo performance\nfrom sklearn.tree import DecisionTreeClassifier         #importing decision tree classifier","31b791f5":"sns.heatmap(iris.corr(), annot=True)","316b4288":"#Splitting the data into training and test set\ntrain, test = train_test_split(iris, test_size=0.3) #the test_size=0.3 splits the train and test data in the ratio of 70:30\n\nprint(train.shape)\nprint(test.shape)","e80e9805":"train_X = train[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\ntrain_y = train[\"Species\"]\n\ntest_X = test[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\ntest_y = test[\"Species\"]\n","928e945c":"train_X.head()","af144e8b":"model = svm.SVC()                     # selecting the algo\nmodel.fit(train_X, train_y)           # we train the algo with the training data and training output\nprediction = model.predict(test_X)    # now we pass the test data to the trained algo\nacc_score = metrics.accuracy_score(prediction, test_y) # here we check the accuracy of our model. In order to do that,\n                                                        # we pass the predicted value by the model and actual value\nprint(\"The accuracy of SVM algo is: \", acc_score)","141d02ab":"model = LogisticRegression()                     # selecting the algo\nmodel.fit(train_X, train_y)           # we train the algo with the training data and training output\nprediction = model.predict(test_X)    # now we pass the test data to the trained algo\nacc_score = metrics.accuracy_score(prediction, test_y) # here we check the accuracy of our model. In order to do that,\n                                                        # we pass the predicted value by the model and actual value\nprint(\"The accuracy of Logistic Regression algo is: \", acc_score)","2ff61eb6":"model = DecisionTreeClassifier()                     # selecting the algo\nmodel.fit(train_X, train_y)           # we train the algo with the training data and training output\nprediction = model.predict(test_X)    # now we pass the test data to the trained algo\nacc_score = metrics.accuracy_score(prediction, test_y) # here we check the accuracy of our model. In order to do that,\n                                                        # we pass the predicted value by the model and actual value\nprint(\"The accuracy of Decision Tree classifier algo is: \", acc_score)","5125b7ad":"model=KNeighborsClassifier(n_neighbors=3)\nmodel.fit(train_X, train_y)           # we train the algo with the training data and training output\nprediction = model.predict(test_X)    # now we pass the test data to the trained algo\nacc_score = metrics.accuracy_score(prediction, test_y) # here we check the accuracy of our model. In order to do that,\n                                                        # we pass the predicted value by the model and actual value\nprint(\"The accuracy of KNN algo is: \", acc_score)","24ae574e":"petal = iris[[\"PetalLengthCm\", \"PetalWidthCm\", \"Species\"]]\nsepal = iris[[\"SepalLengthCm\", \"SepalWidthCm\", \"Species\"]]","f5c05f46":"sepal.head()","90118d4b":"train_p, test_p = train_test_split(petal, test_size=0.3, random_state=0)\ntrain_x_p = train_p[[\"PetalLengthCm\", \"PetalWidthCm\"]]\ntrain_y_p = train_p[\"Species\"]\ntest_x_p = test_p[[\"PetalLengthCm\", \"PetalWidthCm\"]]\ntest_y_p = test_p[\"Species\"]\n\ntrain_s, test_s = train_test_split(sepal, test_size=0.3, random_state=0)\ntrain_x_s = train_s[[\"SepalLengthCm\", \"SepalWidthCm\"]]\ntrain_y_s = train_s[\"Species\"]\ntest_x_s = test_s[[\"SepalLengthCm\", \"SepalWidthCm\"]]\ntest_y_s = test_s[\"Species\"]\n","15b611b2":"model = svm.SVC()\nmodel.fit(train_x_p, train_y_p)\nprediction = model.predict(test_x_p)\naccu_score = metrics.accuracy_score(prediction, test_y_p)\nprint(\"The accuracy of petals using SVM is: \", accu_score)\n\nmodel = svm.SVC()\nmodel.fit(train_x_s, train_y_s)\nprediction = model.predict(test_x_s)\naccu_score = metrics.accuracy_score(prediction, test_y_s)\nprint(\"The accuracy of sepals using SVM is: \", accu_score)","ae6b2921":"model = LogisticRegression()\nmodel.fit(train_x_p, train_y_p)\nprediction = model.predict(test_x_p)\naccu_score = metrics.accuracy_score(prediction, test_y_p)\nprint(\"The accuracy of petals using Logistic Regression is: \", accu_score)\n\nmodel = LogisticRegression()\nmodel.fit(train_x_s, train_y_s)\nprediction = model.predict(test_x_s)\naccu_score = metrics.accuracy_score(prediction, test_y_s)\nprint(\"The accuracy of sepals using Logistic Regression is: \", accu_score)","3369f384":"model = DecisionTreeClassifier()\nmodel.fit(train_x_p, train_y_p)\nprediction = model.predict(test_x_p)\naccu_score = metrics.accuracy_score(prediction, test_y_p)\nprint(\"The accuracy of petals using Decision Tree is: \", accu_score)\n\nmodel =  DecisionTreeClassifier()\nmodel.fit(train_x_s, train_y_s)\nprediction = model.predict(test_x_s)\naccu_score = metrics.accuracy_score(prediction, test_y_s)\nprint(\"The accuracy of sepals using Decision Tree is: \", accu_score)","3cc77527":"model = KNeighborsClassifier(n_neighbors=3)\nmodel.fit(train_x_p, train_y_p)\nprediction = model.predict(test_x_p)\naccu_score = metrics.accuracy_score(prediction, test_y_p)\nprint(\"The accuracy of petals using KNN is: \", accu_score)\n\nmodel = KNeighborsClassifier(n_neighbors=3)\nmodel.fit(train_x_s, train_y_s)\nprediction = model.predict(test_x_s)\naccu_score = metrics.accuracy_score(prediction, test_y_s)\nprint(\"The accuracy of sepals using KNN is: \", accu_score)","1a3147b6":"In all we have 6 columns. Out of these we try to get rid of columns which are not necessary like \"Id\".","1f192da2":"Now, lets re-visit the objective of this competition. Given a set of features (sepal length, sepal width, petal length and petal width), we need to classify if the given flower is setosa, versicolor or virginica. Basically, we need to classify them into one of the 3 types. Hence, we call this problem as a classification problem.\n\nNow, to tackle a classification problem, we need to use classification algorithm","966683f3":"# **SVM (Support Vector Machine)**","323b6a11":"# Observations:\n- Using petals over sepal for training, gives us better accuracy\n- This was expected, as we saw in the correlation map, that the correlation between Petal length and Petal width is high\n\nSo, overall we have implemented a few common machine learning algorithms\n","1c2e6ecd":"It is said that humans are visual creatures. They are more susceptible to images and visuals than just plain numbers.\nSo, the next few steps that we will be performing will be visualizing the data using various graphs and charts.\nWe'll be plotting different graphs and charts and making observations based on the visuals as to what the data is trying to tell us.","4ff48239":"SVM is giving us good accuracy. Next we will try with different algos. We will execute the same steps as above just the selection of algo will change","71d1aff2":"None of the attributes have missing values. So we need not worry on that front.","a4fbffd0":"### Hello Everyone!!!\n\nThis is a very basic tutorial on Machine learning. This notebook will be helpful to people who are complete beginner.\nI have tried my best to explain this notebook in the simplest way possible. Still if you have any queries, you can reach out to me in the comments section. I will try to answer the questions to the best of my knowledge.\n\nHope you find this notebook useful.","60872779":"# SVM","069f281c":"# K-NN (K-Nearest Neighbors)","b92601ba":"So, we have 150 rows or data points and 5 columns. Out of these 5 columns, 4 columns are the features\/attributes and 1 column is the class-label.\n\nFeatures:\n- SepalLengthCm\n- SepalWidthCm\n- PetalLengthCm\n- PetalWidthCm\n\nClass-label:\n- Species\n\nNow lets have a look at data. Basically, the range of values each column has, check if there are any missing values, check if we have any outliers etc.","066c11ac":"So, uptil now, we used all the features of the data set but now we will use Sepals and Petals separately","e6907566":"**Observations**\n- If we observe Petal length and Petal width, its clear that these two attributes are highly correlated.\n\nWhat we will do?\n- Instead of directly dropping one attribute we will first train our model with all the 4 attributes and check the accuracy\n- Next, we will use 1 sepal feature and 1 petal feature to check the accuracy of the algo. ","bcecc4d6":"# Logistic Regression","c87edbfa":"There are few important steps that we need to execute before we start training our model. They are as follows:\n1. Finding the correlation between features: The correlation between features plays a very important role in the overall performance of the model. If we have many such correlated features in our training stage, then this will have a direct impact on the accuracy of the model later. Thus, we will carefully select the features which are less correlated. This process of selecting features based on certain criteria is also known as Feature selection.\n2. Spliting the data into train and test: If we have only one data file (.csv file) - which is the case in this competition, we need to split the data into training and test sets. The purpose of doing so is we will train our model on the training set and then check the performance of our model using the test set. When we use the training data set, we expose the data to the model and now the model knows the kind of data present. The idea is to test our model on unseen data. A data which the model has not been exposed to before and check the performance on that.","34689407":"# Decision Tree","192a4703":"Once done with reading of the csv file, we try to have a look at how the data looks. Basically, the type of columns and their respective values that they hold. This gives us an idea about the columns and their data type and the values that they hold. ","2e4fc3cc":"The above table gives us a very clear idea about various parameters of all the 4 features. We get the mean value, standard deviation, minimum value, maximum value, the 25th percentile, 50th percentile and 75th percentile value.\n\nAlso, since all the values are of the same scale (in cms), there is no need to perform any kind of column normalization or standardization.","da9b4c5f":"# Logistic Regression","32fc5ac1":"From the above graph it is clear that if we plot SepalLengthCm and SepalWidthCm, the iris-setosa flowers are completely separable from others. Meaning, we can draw a simple line which would easily separate almost all the iris-setosa flowers from others","5792d88b":"# Decision Tree classifier"}}