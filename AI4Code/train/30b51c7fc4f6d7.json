{"cell_type":{"33a62c52":"code","77d36afd":"code","0519c5c4":"code","ed60b076":"code","3d7446d5":"code","75fd66d4":"code","b852bac2":"code","d4e2bd58":"code","032bb135":"code","911804a7":"code","4f292ca2":"code","06468baa":"code","7d4eb07f":"code","6accaa11":"code","8fd4de09":"code","b19224d1":"code","197f0ee6":"code","ab47eda8":"code","2d20bde5":"code","e6b8dc70":"code","f2b0ffd7":"code","9138b5aa":"code","3a48b7b5":"code","d5bccba7":"code","4a957c9e":"code","a1aa1d36":"code","618fc841":"code","19c27059":"code","9500ffd7":"code","bad942cb":"code","54b8d525":"code","3ea39e64":"code","64faee37":"code","3efdadfb":"code","f6d74592":"code","b9038a99":"code","5310d7ea":"code","dbf7893c":"code","ab68349c":"code","5ba0bbb8":"code","4ea1343c":"code","8236d153":"code","dc78d07d":"code","18fc15a8":"code","591aa33d":"code","2c47245e":"code","5eb79ef3":"code","8021c607":"code","a7b725cb":"code","d852a758":"code","47109e6c":"code","365ad61a":"code","9dfcbafc":"code","02c4f002":"code","e4e3a699":"code","c2900c7d":"code","2fadbdc9":"code","9d91ebe4":"code","0a989a70":"code","a43460ca":"code","a873347a":"code","92271fea":"code","991f65b3":"code","bd8dbf44":"code","8bf08818":"code","1e4eed25":"code","8585e065":"code","48325fbc":"markdown","817629ff":"markdown","b8d6effa":"markdown","56fcb1bb":"markdown","786a5eab":"markdown","0f0fdcdc":"markdown","e3b702a7":"markdown","1a5e8c7d":"markdown","a04bb2da":"markdown","04472994":"markdown","a726bfb8":"markdown","638a3561":"markdown","c9e43c8f":"markdown","dc910b74":"markdown","5e2f0d3c":"markdown","d909e9fc":"markdown","59eea3ea":"markdown","48ee695e":"markdown","942838d1":"markdown","47534429":"markdown","33bb91ff":"markdown","3400280b":"markdown","944ce56c":"markdown","8c38ec67":"markdown","5fc229ca":"markdown","144871de":"markdown","86990b2e":"markdown","aaff8636":"markdown","7c327360":"markdown","624621fb":"markdown","9fbed294":"markdown","6da9a3be":"markdown","1ca065bc":"markdown","0567555c":"markdown","920f5c81":"markdown","8fd68565":"markdown","9c148324":"markdown","da5a5da0":"markdown","e42d19e1":"markdown","44b5f75d":"markdown","71c2bdc9":"markdown","61157ef2":"markdown","3e0ee2c3":"markdown"},"source":{"33a62c52":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","77d36afd":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\n\n\nfrom catboost import CatBoostRegressor, Pool, cv\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, RepeatedKFold\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.feature_selection import RFECV\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nimport hyperopt\n\nfrom numpy.random import RandomState\nfrom os import listdir\n\n\nimport shap\n# load JS visualization code to notebook\nshap.initjs()","0519c5c4":"listdir(\"..\/input\")","ed60b076":"def run_catboost(traindf, testdf, holddf, params, n_splits=10, n_repeats=1,\n                 plot=False, use_features=None, plot_importance=True):\n    \n    \n    folds = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=1)\n    p_hold = np.zeros(holddf.shape[0])\n    y_hold = holddf.target\n    p_test = np.zeros(testdf.shape[0])\n    \n    if use_features is None:\n        use_features = testdf.columns.values\n    \n    cat_features = np.where(testdf.loc[:, use_features].dtypes==\"object\")[0]\n    x_hold = holddf.loc[:, use_features]\n    x_test = testdf.loc[:, use_features]\n    \n    feature_importance_df = pd.DataFrame(index=use_features)\n    \n    m = 0\n    cv_scores = []\n    for train_idx, dev_idx in folds.split(traindf):\n        x_train, x_dev = traindf.iloc[train_idx][use_features], traindf.iloc[dev_idx][use_features]\n        y_train, y_dev = traindf.target.iloc[train_idx], traindf.target.iloc[dev_idx]\n\n        train_pool = Pool(x_train, y_train, cat_features=cat_features)\n        dev_pool = Pool(x_dev, y_dev, cat_features=cat_features)\n        model = CatBoostRegressor(**params)\n        model.fit(train_pool, eval_set=dev_pool, plot=plot)\n\n        # bagging predictions for test and hold out data:\n        p_hold += model.predict(x_hold)\/(n_splits*n_repeats)\n        log_p_test = model.predict(x_test)\n        p_test += (np.exp(log_p_test) - 1)\/(n_splits*n_repeats)\n\n        # predict for dev fold:\n        y_pred = model.predict(x_dev)\n        feature_importance_df.loc[:, \"fold_\" + str(m)] = model.get_feature_importance(train_pool)\n        cv_scores.append(np.sqrt(mse(y_dev, y_pred)))\n        m+=1\n\n    print(\"hold out rmse: \" + str(np.sqrt(mse(y_hold, p_hold))))\n    print(\"cv mean rmse: \" + str(np.mean(cv_scores)))\n    print(\"cv std rmse: \" + str(np.std(cv_scores)))\n    \n    feature_importance_df[\"mean\"] = feature_importance_df.mean(axis=1)\n    feature_importance_df[\"std\"] = feature_importance_df.std(axis=1)\n    feature_importance_df = feature_importance_df.sort_values(by=\"mean\", ascending=False)\n    \n    if plot_importance:\n        plt.figure(figsize=(15,20))\n        sns.barplot(x=feature_importance_df[\"mean\"].values, y=feature_importance_df.index.values);\n        plt.title(\"Feature importances\");\n        plt.show()\n    \n    results = {\"last_model\": model,\n               \"last_train_pool\": train_pool,\n               \"feature_importance\": feature_importance_df, \n               \"p_hold\": p_hold,\n               \"p_test\": p_test,\n               \"cv_scores\": cv_scores}\n    return results","3d7446d5":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\", index_col=0)\ntrain.head()","75fd66d4":"train.shape","b852bac2":"test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\", index_col=0)\ntest.head()","d4e2bd58":"test.shape","032bb135":"train.shape[0] \/ test.shape[0]","911804a7":"submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\", index_col=0)\nsubmission.head()","4f292ca2":"plt.figure(figsize=(20,5))\nsns.distplot(train.SalePrice, color=\"tomato\")\nplt.title(\"Target distribution in train\")\nplt.ylabel(\"Density\");","06468baa":"train[train.SalePrice <=0 ]","7d4eb07f":"plt.figure(figsize=(20,5))\nsns.distplot(np.log(train.SalePrice), color=\"tomato\")\nplt.title(\"Log Target distribution in train\")\nplt.ylabel(\"Density\");","6accaa11":"train[\"LogSalePrice\"] = train.SalePrice.apply(np.log)","8fd4de09":"combined = train.drop([\"SalePrice\", \"LogSalePrice\"], axis=1).append(test)\nnan_percentage = combined.isnull().sum().sort_values(ascending=False) \/ combined.shape[0]\nmissing_val = nan_percentage[nan_percentage > 0]","b19224d1":"plt.figure(figsize=(20,5))\nsns.barplot(x=missing_val.index.values, y=missing_val.values * 100, palette=\"Reds_r\");\nplt.title(\"Percentage of missing values in train & test\");\nplt.ylabel(\"%\");\nplt.xticks(rotation=90);","197f0ee6":"num_candidates = list(combined.dtypes[combined.dtypes!=\"object\"].index.values)\nnum_candidates","ab47eda8":"unique_counts = combined.loc[:, num_candidates].nunique().sort_values()\n\nplt.figure(figsize=(20,5))\nsns.barplot(unique_counts.index, unique_counts.values, palette=\"Oranges_r\")\nplt.xticks(rotation=90);\nplt.yscale(\"log\")","2d20bde5":"cat_candidates = list(combined.dtypes[combined.dtypes==\"object\"].index.values)","e6b8dc70":"num_to_cats = [\"BsmtHalfBath\", \"HalfBath\", \"KitchenAbvGr\", \"BsmtFullBath\", \"Fireplaces\", \"FullBath\", \"GarageCars\",\n               \"BedroomAbvGr\", \"OverallCond\", \"OverallQual\", \"TotRmsAbvGrd\", \"MSSubClass\", \"YrSold\", \"MoSold\", \n               \"GarageYrBlt\", \"YearRemodAdd\"]\n\nfor feat in num_to_cats:\n    num_candidates.remove(feat)\n    cat_candidates.append(feat)\n    combined[feat] = combined[feat].astype(\"object\")\n    train[feat] = train[feat].astype(\"object\")\n    test[feat] = test[feat].astype(\"object\")","f2b0ffd7":"num_candidates","9138b5aa":"len(num_candidates)","3a48b7b5":"cat_candidates = combined.dtypes[combined.dtypes==\"object\"].index.values\ncat_candidates","d5bccba7":"frequencies = []\nfor col in cat_candidates:\n    overall_freq = combined.loc[:, col].value_counts().max() \/ combined.shape[0]\n    frequencies.append([col, overall_freq])\n\nfrequencies = np.array(frequencies)\nfreq_df = pd.DataFrame(index=frequencies[:,0], data=frequencies[:,1], columns=[\"frequency\"])\nsorted_freq = freq_df.frequency.sort_values(ascending=False)\n\nplt.figure(figsize=(20,5))\nsns.barplot(x=sorted_freq.index[0:30], y=sorted_freq[0:30].astype(np.float), palette=\"Blues_r\")\nplt.xticks(rotation=90);","4a957c9e":"example = \"Utilities\"\ncombined.loc[:,example].value_counts()","a1aa1d36":"example = \"Street\"\ncombined.loc[:,example].value_counts()","618fc841":"example = \"Condition2\"\ncombined.loc[:,example].value_counts()","19c27059":"cats_to_drop = [\"Utilities\"]","9500ffd7":"combined = combined.drop(cats_to_drop, axis=1)\ntrain = train.drop(cats_to_drop, axis=1)\ntest = test.drop(cats_to_drop, axis=1)","bad942cb":"cat_candidates = combined.dtypes[combined.dtypes==\"object\"].index.values\ncat_candidates","54b8d525":"def build_map(useless_levels, plugin_level, train_levels):\n    plugin_map = {}\n    for level in useless_levels:\n        plugin_map[level] = plugin_level\n    for level in train_levels:\n        plugin_map[level] = level\n    return plugin_map","3ea39e64":"def clean_test_levels(train, test):\n    for col in test.columns:\n        train_levels = set(train[col].unique())\n        test_levels = set(test[col].unique())\n        in_test_not_in_train = test_levels.difference(train_levels)\n        if len(in_test_not_in_train)>0:\n            close_to_mean_level = train.groupby(col).LogSalePrice.mean() - train.SalePrice.apply(np.log).mean()\n            close_to_mean_level = close_to_mean_level.apply(np.abs)\n            plugin_level = close_to_mean_level.sort_values().index.values[0]\n            in_test_not_in_train = list(in_test_not_in_train)\n            plugin_map = build_map(in_test_not_in_train, plugin_level, train_levels)\n            test[col] = test[col].map(plugin_map)\n    return train, test","64faee37":"#train, test = clean_test_levels(train, test)","3efdadfb":"test[\"MSSubClass\"].value_counts()","f6d74592":"fig, ax = plt.subplots(len(num_candidates),3,figsize=(20,len(num_candidates)*6))\n\nfor n in range(len(num_candidates)):\n    feat = num_candidates[n]\n    ax[n,0].scatter(train[feat].values, np.log(train.SalePrice.values), s=4)\n    ax[n,0].set_ylabel(\"Log SalePrice\")\n    ax[n,0].set_xlabel(feat);\n    ax[n,1].scatter(np.log(train[feat].values+1), np.log(train.SalePrice.values), s=4)\n    ax[n,1].set_ylabel(\"Log SalePrice\")\n    ax[n,1].set_xlabel(\"Log\" + feat);\n    sns.distplot(test[feat].dropna(), kde=True, ax=ax[n,2], color=\"limegreen\")\n    ax[n,2].set_title(\"Distribution in test\")","b9038a99":"outlier_ids = set()\noutlier_ids = outlier_ids.union(set(train[train.LotArea > 60000].index.values))\noutlier_ids = outlier_ids.union(set(train[train.LotFrontage > 200].index.values))\noutlier_ids = outlier_ids.union(set(train[(train.LotFrontage > 150) & (train.SalePrice.apply(np.log) < 11)].index.values))\noutlier_ids = outlier_ids.union(set(train[train.GrLivArea > 4500].index.values))\noutlier_ids = outlier_ids.union(set(train[train[\"1stFlrSF\"] > 4000].index.values))\noutlier_ids = outlier_ids.union(set(train[train.MasVnrArea > 1400].index.values))\noutlier_ids = outlier_ids.union(set(train[train[\"BsmtFinSF1\"] > 5000].index.values))\noutlier_ids = outlier_ids.union(set(train[train.TotalBsmtSF > 6000].index.values))\noutlier_ids = outlier_ids.union(set(train[(train.OpenPorchSF > 500) & (np.log(train.SalePrice) < 11)].index.values))","5310d7ea":"outlier_ids","dbf7893c":"train.shape","ab68349c":"train = train.drop(list(outlier_ids))\ncombined = combined.drop(list(outlier_ids))","5ba0bbb8":"train.shape","4ea1343c":"def impute_na_trees(df, col):\n    if df[col].dtype == \"object\":\n        df[col] = df[col].fillna(\"None\")\n        df[col] = df[col].astype(\"object\")\n    else:\n        df[col] = df[col].fillna(0)\n    return df\n","8236d153":"for col in combined.columns:\n    combined = impute_na_trees(combined, col)","dc78d07d":"num_candidates = combined.dtypes[combined.dtypes!=\"object\"].index.values\nlen(num_candidates)","18fc15a8":"cat_candidates = combined.dtypes[combined.dtypes==\"object\"].index.values\nlen(cat_candidates)","591aa33d":"combined.isnull().sum().sum()","2c47245e":"combined[\"TotalSF\"] = combined[\"1stFlrSF\"] + combined[\"2ndFlrSF\"] + combined[\"TotalBsmtSF\"] \ncombined[\"GreenArea\"] = combined[\"LotArea\"] - combined[\"GrLivArea\"] - combined[\"GarageArea\"]","5eb79ef3":"traindf = combined.iloc[0:train.shape[0]].copy()\ntraindf.loc[:, \"target\"] = train.LogSalePrice\ntestdf = combined.iloc[train.shape[0]::].copy()","8021c607":"traindf, holddf = train_test_split(traindf, test_size=0.25, random_state=0)\nprint((traindf.shape, holddf.shape, testdf.shape))","a7b725cb":"org_params = {\n    'iterations': 10000,\n    'learning_rate': 0.08,\n    'eval_metric': 'RMSE',\n    'random_seed': 42,\n    'logging_level': 'Silent',\n    'use_best_model': True,\n    'loss_function': 'RMSE',\n    'od_type': 'Iter',\n    'od_wait': 1000,\n    'one_hot_max_size': 20,\n    'l2_leaf_reg': 100,\n    'depth': 3,\n    'rsm': 0.6,\n    'random_strength': 2,\n    'bagging_temperature': 10\n}","d852a758":"results = run_catboost(traindf, \n                       testdf,\n                       holddf,\n                       org_params,\n                       plot=True,\n                       n_splits=5,\n                       n_repeats=3)\np_hold = results[\"p_hold\"]\np_test = results[\"p_test\"]\nfeature_importance_df = results[\"feature_importance\"]","47109e6c":"feature_importance_df.head()","365ad61a":"feature_importance_df.tail()","9dfcbafc":"submission.loc[testdf.index.values, \"SalePrice\"] = p_test\nsubmission = submission.reset_index()\nsubmission.head()","02c4f002":"submission.to_csv(\"submission_before_feature_selection.csv\", index=False)\nsubmission = submission.set_index(\"Id\")\nsubmission.head()","e4e3a699":"plt.figure(figsize=(5,5))\nplt.scatter(holddf.target, p_hold, s=10, color=\"deepskyblue\")\nplt.xlabel(\"Target value\")\nplt.ylabel(\"Predicted value\");","c2900c7d":"hold_predictions = pd.DataFrame(holddf.target.values, index=holddf.index, columns=[\"target\"])\nhold_predictions[\"catboost_org_features\"] = p_hold\nhold_predictions.head()","2fadbdc9":"plt.figure(figsize=(20,5))\nsns.barplot(feature_importance_df[\"mean\"].index, feature_importance_df[\"mean\"].values, palette=\"Reds_r\")\nplt.xticks(rotation=90);","9d91ebe4":"to_drop = [\"Condition2\", \"RoofMatl\", \"PoolArea\", \"3SsnPorch\", \"LandSlope\", \"LowQualFinSF\",\n           \"Electrical\", \"MiscFeature\"]","0a989a70":"combined = combined.drop(to_drop, axis=1)\ntraindf = traindf.drop(to_drop, axis=1)\ntestdf = testdf.drop(to_drop, axis=1)\nholddf = holddf.drop(to_drop, axis=1)","a43460ca":"cat_features = np.where(testdf.dtypes == \"object\")[0]","a873347a":"x_train, x_dev = traindf.drop(\"target\", axis=1), holddf.drop(\"target\", axis=1)\ny_train, y_dev = traindf.target, holddf.target\n\ntrain_pool = Pool(x_train, y_train, cat_features=cat_features)\ndev_pool = Pool(x_dev, y_dev, cat_features=cat_features)\nmodel = CatBoostRegressor(**org_params)\nmodel.fit(train_pool, eval_set=dev_pool, plot=True)","92271fea":"interaction = model.get_feature_importance(train_pool, type=\"Interaction\")\ncolumn_names = testdf.columns.values \ninteraction = pd.DataFrame(interaction, columns=[\"feature1\", \"feature2\", \"importance\"])\ninteraction.feature1 = interaction.feature1.apply(lambda l: column_names[int(l)])\ninteraction.feature2 = interaction.feature2.apply(lambda l: column_names[int(l)])\ninteraction.head(20)","991f65b3":"interaction[\"feature1_type\"] = interaction.feature1.apply(\n    lambda l: np.where(testdf[l].dtype==\"object\", 0, 1)\n)\ninteraction[\"feature2_type\"] = interaction.feature2.apply(\n    lambda l: np.where(testdf[l].dtype==\"object\", 0, 1)\n)\ninteraction.head()","bd8dbf44":"interaction[\"combination\"] = interaction.feature1_type + interaction.feature2_type\ninteraction.combination.value_counts()","8bf08818":"numerical_combi = interaction[interaction.combination==2].copy()\nfor n in numerical_combi.index.values:\n    feat1 = numerical_combi.loc[n].feature1\n    feat2 = numerical_combi.loc[n].feature2\n    if traindf[feat1].max() > traindf[feat2].max():\n        traindf.loc[:, feat2 + \"_\" + feat1 + \"_frac\"] = traindf[feat2] \/ traindf[feat1]\n    else:\n        traindf.loc[:, feat1 + \"_\" + feat2 + \"_frac\"] = traindf[feat1] \/ traindf[feat2]\n    traindf.loc[:, feat2 + \"_\" + feat1 + \"_mult\"] = traindf[feat2] * traindf[feat1]\n    traindf.loc[:, feat2 + \"_\" + feat1 + \"_add\"] = traindf[feat2] + traindf[feat1]\n    traindf.loc[:, feat2 + \"_\" + feat1 + \"_sub\"] = traindf[feat2] - traindf[feat1]","1e4eed25":"mixed_combi = interaction[interaction.combination==1].copy()\nfor n in mixed_combi.index.values:\n    feat1 = mixed_combi.loc[n].feature1\n    feat2 = mixed_combi.loc[n].feature2\n    if traindf[feat1].dtype==\"object\":\n        traindf.loc[:, \"grouped_\" + feat1 + \"_mean_\" + feat2] = traindf[feat1].map(\n            traindf.groupby(feat1)[feat2].mean())\n        traindf.loc[:, \"grouped_\" + feat1 + \"_std_\" + feat2] = traindf[feat1].map(\n            traindf.groupby(feat1)[feat2].std())","8585e065":"traindf.head()","48325fbc":"With this rule in our mind, we find:\n\n* Features that count the number of rooms. This seems to be more categorical as you can't have 3.245 bedrooms for example.\n* The same holds for the number of fireplaces and garage cars.\n* Features like OverallCond and OverallQual can be considered as groups and are categorical variables with a natural kind of order (ordinal features). \n* Every feature with SF or area can be considered as numerical. \n* I think that we should treat temporal features like YrSold as categorical.","817629ff":"## Generating hold-out-data <a class=\"anchor\" id=\"holdout\"><\/a>\n\nI like to use the hold out data later for ensembling. ","b8d6effa":"Let's first look for numerical features in our categorical candidates:","56fcb1bb":"## Submission <a class=\"anchor\" id=\"submission\"><\/a>","786a5eab":"## Imputing missing values <a class=\"anchor\" id=\"impute\"><\/a>","0f0fdcdc":"But what about MisVal and 3SsnPorch? Reading in the description, we can see that the latter is related to area. The further stands for the value in $ of a miscellaneous feature like a tennis court. Consequently both are numerical.","e3b702a7":"I really love the next feature: ;-)","1a5e8c7d":"## Training data <a class=\"anchor\" id=\"training\"><\/a>","a04bb2da":"# Feature engineering <a class=\"anchor\" id=\"engineering\"><\/a>","04472994":"# House Prices Tutorial - With Catboost\n\nThis kernel was born during one of our KaggleDays meetup events in Cologne. The workshop was related to the task feature engineering even though many of us just used a brute force approach, this kernel has become much more detailed. During the last weeks (or months :-O) I tried out several ideas and consequently the content changed over time. ;-) Finally I found a stable structure to play with and the kernel has become my personal catboost tutorial that I like to share with the Kaggle community. \n\n\n<img src=\"https:\/\/images.unsplash.com\/photo-1551969014-7d2c4cddf0b6?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=2778&q=80\" width=\"900px\">\n\n\n\nIf you like to work with it, just fork and build uppon this sceleton. Of course you can change the model, build a model zoo etc.. Feel free to choose your personal learning path. ;-)\n\nAnd please... **don't forget to UPVOTE! ;-p**","a726bfb8":"The sale price distribution is right-skewed and shows extreme outliers. We should log-transform the target values, as we will use some loss built on mean-squared-error which assumes that our target distribution is normal.","638a3561":"# Baseline predictions with catboost <a class=\"anchor\" id=\"baseline\"><\/a>\n\nThis data has many categorical features and one model that can deal nicely with these kind of features is [catboost](https:\/\/catboost.ai\/). If you like to read more about it, you can find a [paper here](https:\/\/arxiv.org\/abs\/1706.09516).\n\n## Trying to demystify the learning process <a class=\"anchor\" id=\"demytify\"><\/a>\n\n**Under construction**\n\n* binary decision trees as base predictors","c9e43c8f":"### Insights\n\n* There are features that were never used by catboost.\n* There is a lot of variation of the feature importance between folds! What does this mean?\n    * How useful a feature is to fit the training data depends on the data itself and how big the dataset is.\n    * If you increase the number of splits the most important features will change! Try it out!\n    * Remember that our test dataset is the same size as the training dataset. It's very likely that we will overfit to our training data and that we will not be able to generalize well on the test data. \n* What can we do to reduce this problem?\n    * We should drop features that almost have no importance!\n    * We should include more randomess to our KFold as well (run it several times, make averaged predictions with each KFold)\n    * We should use different models that contribute to our prediction.\n\nLet's figure it out by playing with these concepts. ;-)\n    ","dc910b74":"The training data has almost the same size as the test data!","5e2f0d3c":"## A colorful bouquet of hyperparameters <a class=\"anchor\" id=\"hyparams\"><\/a>","d909e9fc":"# Data preparation <a class=\"anchor\" id=\"dataprep\"><\/a>","59eea3ea":"## Dealing with outliers <a class=\"anchor\" id=\"outliers\"><\/a>","48ee695e":"Can we be sure that each numerical candidate is indeed a numerical feature? A feature that measures the area is a numerical feature but what about MSSubClass? It holds different groups as is not numerical but rather categorical. A first attempt could be to assume that all numerical features should have various different values and not only a few.","942838d1":"## Running catboost and feature importances <a class=\"anchor\" id=\"run_catboost\"><\/a>","47534429":"## Generating obvious new features <a class=\"anchor\" id=\"obvious\"><\/a>","33bb91ff":"Using this submission we get a score around 0.141.","3400280b":"## Generating new features based on interactions","944ce56c":"We don't have to worry about negative or zero house prices. :-)","8c38ec67":"# Prepare to start <a class=\"anchor\" id=\"prepare\"><\/a>\n\n## Packages <a class=\"anchor\" id=\"packages\"><\/a>","5fc229ca":"We can see that PoolQC, MiscFeature, Alley etc. have more than 80 % missing values. In the description, we can see that this often tells us \"no pool\", \"no miscfeature\" etc.. In my opinion it's difficult to say if such a feature is important. For this reason, let's not drop them and plugin \"None\" later in the analysis. For numerical features we need to find an approriate strategy.","144871de":"## Finding categorical features in numerical candidates <a class=\"anchor\" id=\"num_to_cat_candidates\"><\/a>","86990b2e":"## Fusing seldom categorical levels <a class=\"anchor\" id=\"fusion\"><\/a>\n\n### Levels present in test but not in train","aaff8636":"## Feature Interaction <a class=\"anchor\" id=\"interaction\"><\/a>\n\nWith catboost we can also discover feature interactions with a depth > 1. This way we can see which features are often found as a combination in single trees:","7c327360":"## Helper methods <a class=\"anchor\" id=\"helpers\"><\/a>","624621fb":"# Feature selection <a class=\"anchor\" id=\"feature_selection\"><\/a>\n\n## Dropping features with almost no importance <a class=\"anchor\" id=\"fs_no_imp\"><\/a>","9fbed294":"### Insights\n\n* Many categorical candidates have one major level that occupies > 80 % of the data. That's bad. What should we learn from such a feature? Let's pick some examples:","6da9a3be":"# Peek at the data <a class=\"anchor\" id=\"peek\"><\/a>","1ca065bc":"## Log-transforming the target distribution <a class=\"anchor\" id=\"targets\"><\/a>","0567555c":"Comparing with the description this looks fine! It seems that there are no numerical features in categorical candidates. Some categorical features might still be completely useless as only some levels occur most of the time (no diversity). To get more insights, we can compute the frequency of the most common level in the train & test data:","920f5c81":"### Table of contents\n\n1. [Prepare to start](#prepare) (complete)\n    * [Packages](#packages) (complete)\n    * [Helper methods](#helpers) (complete)\n2. [Peek at the data](#peek) (complete)\n    * [Training data](#training) (complete)\n    * [Test data](#test) (complete)\n    * [Submission](#submission) (complete)\n3. [Data exploration & cleaning](#eda) \n    * [Log-transforming the target distribution](#targets) (complete)\n    * [Dropping nan-features](#nanfeatures) (complete)\n    * [Finding categorical features in numerical candidates](#num_to_cat_candidates) (complete)\n    * [Dropping useless categorical candidates](#useless_cat) (complete)\n    * [Fusing seldom categorical levels](#fusion)\n4. [Data preparation](#dataprep)\n    * [Dealing with outliers](#outliers) (complete)\n    * [Imputing missing values](#impute) (somehow complete)\n    * [Generating obvious new features](#obvious)\n5. [Baseline predictions with catboost](#baseline)\n    * [Trying to demystify the learning process](#demytify)\n    * [Generating hold-out-data](#holdout) (complete)\n    * [A colorful bouquet of hyperparameters](#hyparams)\n    * [Running catboost & feature importances](#run_catboost) (somehow complete)\n6. [Feature selection](#feature_selection)\n    * [Dropping features with almost no importance](#fs_no_imp)\n7. [Feature engineering](#engineering)\n    * [Feature interaction](#interaction)\n8. [Some further insights](#furtherinsights)\n9. [Outlook](#outlook)","8fd68565":"This is a completely useless feature!  ","9c148324":"## Useless categorical candidates <a class=\"anchor\" id=\"useless_cat\"><\/a>","da5a5da0":"### Insights\n\n* Even with log transformed features there are some really strange outliers that are often related to area features. \n\nLet's try to clean up a bit! :-)","e42d19e1":"# Data exploration <a class=\"anchor\" id=\"eda\"><\/a>","44b5f75d":"## Happy kaggling! ;-)","71c2bdc9":"## Dropping nan-features <a class=\"anchor\" id=\"nanfeatures\"><\/a>","61157ef2":"Ahh! A second problem arises! ;-) Do you see the seldom levels of the Condition2 feature?! They only have one sample. We will find a lot of levels with very low frequencies or some that are only present in train or test. We have to deal with this problem later. For now, let's drop only \"Utilities\".","3e0ee2c3":"## Test data <a class=\"anchor\" id=\"test\"><\/a>"}}