{"cell_type":{"354d5def":"code","b6849106":"code","5620e112":"code","bd4572a0":"code","3f1c235a":"code","9884527d":"code","af93f2e3":"code","55a7265b":"code","e716352f":"code","ecf69551":"code","1e618372":"code","0eace84c":"code","8dfea607":"code","9fd07ca3":"code","56e3104b":"code","9a6d9884":"markdown","4bab9487":"markdown","0aad7515":"markdown","c370f868":"markdown","46c90935":"markdown","7525c042":"markdown","24613899":"markdown","5585ab89":"markdown","f61b0dd5":"markdown"},"source":{"354d5def":"import numpy as np\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torch import optim\nimport torch.nn.functional as F","b6849106":"%%time \nfilepath = Path(\"..\/input\/kuzushiji\")\n\ntrain_dev_images = np.load(os.path.join(filepath, 'kmnist-train-imgs.npz'))['arr_0']\ntrain_dev_labels = np.load(os.path.join(filepath, 'kmnist-train-labels.npz'))['arr_0']\ntest_images = np.load(os.path.join(filepath, 'kmnist-test-imgs.npz'))['arr_0']\ntest_labels = np.load(os.path.join(filepath, 'kmnist-test-labels.npz'))['arr_0']\n\nprint('train images shape: {}'.format(train_dev_images.shape))\nprint('test images shape: {}'.format(test_images.shape))\nprint('train labels shape: {}'.format(train_dev_labels.shape))\nprint('test labels shape: {}'.format(test_labels.shape))","5620e112":"%%time \n# Visualization of dataset\nlabels, counts = np.unique(train_dev_labels, return_counts=True)\nplt.bar(labels, counts, 2\/3, color=\"grey\");\nplt.xticks(labels);\nplt.xlabel('Index (Characters)');\nplt.ylabel('Number of characters');\nplt.title('Distribution of dataset');","bd4572a0":"%%time \ncharacters = pd.read_csv(os.path.join(filepath, 'kmnist_classmap.csv'))","3f1c235a":"%%time \ncharacters","9884527d":"%%time \nrandom_sample = np.empty([1, 28, 28])\n\nfor i in range(len(labels)):\n    sample = train_dev_images[np.where(train_dev_labels == i)][:1]\n    sample = np.asarray(sample)\n    random_sample = np.concatenate([random_sample, sample], axis=0)\n    \nrandom_sample = np.delete(random_sample, (0), axis=0)\nprint('random sample shape: {}'.format(random_sample.shape))","af93f2e3":"%%time \nfigure = plt.figure(figsize=(40, 40))\n\nfor i in range(len(random_sample)):\n    sub = figure.add_subplot(10, 1, i+1)\n    sub.imshow(random_sample[i, :, :], interpolation='nearest')\n    sub.set_title('index = ' + str(i))","55a7265b":"%%time \ndef process_images(dataset, labels):\n    new_dataset = dataset.astype('float32')\n    new_dataset \/= 255\n    new_dataset = np.reshape(new_dataset, (len(new_dataset), 1, 28, 28))\n    new_labels = labels.astype('int64')\n    return new_dataset, new_labels","e716352f":"%%time \nnew_train_images, new_train_labels = process_images(train_dev_images, train_dev_labels)\n\n# Separacion del train data set en train y dev\nX_train, X_dev, y_train, y_dev = train_test_split(new_train_images, new_train_labels, train_size=.9, test_size=.1)\n\n# Transforming dataset into tensors\nX_train = torch.Tensor(X_train)\ny_train = torch.Tensor(y_train).type(torch.LongTensor)\nX_dev = torch.Tensor(X_dev)\ny_dev = torch.Tensor(y_dev).type(torch.LongTensor)\n\nprint('training dataset shape: {}'.format(X_train.shape))\nprint('training labels shape: {}'.format(y_train.shape))\nprint('validation dataset shape: {}'.format(X_dev.shape))\nprint('validation labels shape: {}'.format(y_dev.shape))\n\ntrain_dataset = torch.utils.data.TensorDataset(X_train, y_train)\ntrainloader = torch.utils.data.DataLoader(train_dataset, batch_size = 400, shuffle = False)\n\ndev_dataset = torch.utils.data.TensorDataset(X_dev, y_dev)\ndevloader = torch.utils.data.DataLoader(dev_dataset, batch_size = 400, shuffle = False)","ecf69551":"class ConvNet(nn.Module):\n    def __init__(self, modelo):\n        super(ConvNet, self).__init__()\n        self.modelo = modelo\n        if self.modelo == 1:\n            self.layer1 = nn.Sequential( # input = (100*1*28*28)\n            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0), # output = (100*6*24*24)\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)) # output = (100*6*12*12)\n            \n            self.layer2 = nn.Sequential(\n            nn.Conv2d(6, 12, kernel_size=4, stride=1, padding=0), # output = (100*12*9*9)\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3)) # output = (100*12*3*3)\n            \n            self.fc1 = nn.Linear(108, 10)\n        if self.modelo == 2:\n            self.layer1 = nn.Sequential( # input = (100*1*28*28)\n            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=0), # output = (100*16*24*24)\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)) # output = (100*16*12*12)\n            \n            self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=0), # output = (100*32*8*8)\n            nn.BatchNorm2d(32), \n            nn.ReLU(), \n            nn.MaxPool2d(kernel_size=2)) # output = (100*32*4*4)\n            \n            self.layer3 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=4, stride=1, padding=0), # output = (100*64*1*1)\n            nn.ReLU())\n            \n            self.fc1 = nn.Linear(64, 10)\n        if self.modelo == 3:\n            self.layer1 = nn.Sequential( # input = (100*1*28*28)\n            nn.Conv2d(1, 12, kernel_size=5, stride=1, padding=0), # output = (100*12*24*24)\n            nn.BatchNorm2d(12),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2)) # output = (100*12*11*11)\n            \n            self.layer2 = nn.Sequential(\n            nn.Conv2d(12, 24, kernel_size=3, stride=1, padding=0), # output = (100*24*9*9) \n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=1), # output = (100*24*7*7)\n            nn.Dropout(p=0.2)) \n            \n            self.layer3 = nn.Sequential(\n            nn.Conv2d(24, 36, kernel_size=3, stride=1, padding=0), # output = (100*36*5*5)\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=1)) # output = (100*36*4*4)\n            \n            self.layer4 = nn.Sequential(\n            nn.Conv2d(36, 48, kernel_size=3, stride=1, padding=0), # output = (100*48*2*2)\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=1), # output = (100*48*1*1)\n            nn.Dropout(p=0.2)) \n            \n            self.fc1 = nn.Linear(48, 10)\n\n    def forward(self, x):\n        if self.modelo == 1:\n            l1 = self.layer1(x)\n            l2 = self.layer2(l1)\n            l2 = l2.view(l2.size(0), -1)\n            l3 = self.fc1(l2)\n            output = F.softmax(l3, dim=1)\n            return output\n        if self.modelo == 2:\n            l1 = self.layer1(x)\n            l2 = self.layer2(l1)\n            l3 = self.layer3(l2)\n            l3 = l3.view(l3.size(0), -1)\n            l4 = self.fc1(l3)\n            output = F.softmax(l4, dim=1)\n            return output\n        if self.modelo == 3:\n            l1 = self.layer1(x)\n            l2 = self.layer2(l1)\n            l3 = self.layer3(l2)\n            l4 = self.layer4(l3)\n            l4 = l4.view(l4.size(0), -1)\n            l5 = self.fc1(l4)\n            output = F.softmax(l5, dim=1)\n            return output","1e618372":"%%time \nnum_epochs = 18\nlearning_rate = 0.001 \n# se puede mandar por par\u00e1metro 1, 2 o 3 para probar diferentes modelos (en este caso el 2 tuvo el mayor accuracy)\nmodel = ConvNet(2)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","0eace84c":"%%time \nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0\n    accuracy = 0\n    steps = 0\n    for i, (images, labels) in enumerate(trainloader):\n        #forward\n        output = model(images)\n        loss = criterion(output, labels)\n\n        #backwards\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = torch.max(output, 1)\n        accuracy += (predicted == labels).sum()\n        steps += 1\n        \n    print(f'epoch {epoch+1} \/ {num_epochs}, loss = {running_loss\/steps:.4f}, accuracy = {100 * accuracy\/\/len(train_dataset):.2f}%')\nprint('Finished Training')","8dfea607":"%%time\nfor epoch in range(num_epochs):\n    loss = 0\n    accuracy = 0\n    running_loss = 0\n    steps = 0\n    model.eval()\n    for i, (images, labels) in enumerate(devloader):\n        output = model(images)\n        loss = criterion(output, labels)\n        optimizer.zero_grad()\n        running_loss += loss.item()\n        _, predicted = torch.max(output, 1)\n        accuracy += (predicted == labels).sum()\n        steps += 1\n    print(f'epoch {epoch+1} \/ {num_epochs}, loss = {running_loss\/steps:.4f}, accuracy = {100 * accuracy \/\/ len(dev_dataset):.2f}%')","9fd07ca3":"%%time\nnew_test_images, new_test_labels = process_images(test_images, test_labels)\n\n# Transforming dataset into tensors\nX_test = torch.Tensor(new_test_images)\ny_test = torch.Tensor(new_test_labels).type(torch.LongTensor)\n\nprint('test dataset shape: {}'.format(X_test.shape))\nprint('test labels shape: {}'.format(y_test.shape))\n\ntest_dataset = torch.utils.data.TensorDataset(X_test, y_test)\ntestloader = torch.utils.data.DataLoader(test_dataset, batch_size = 400, shuffle = False)","56e3104b":"%%time\nmodel.eval()\nwith torch.no_grad():\n    accuracy = 0\n    total = 0\n    test_loss = 0\n    for images, labels in testloader:\n        output = model(images)\n        loss = criterion(output, labels)\n        test_loss += loss.item()\n        test_loss = test_loss\/len(testloader)\n        _, predicted = torch.max(output.data, 1)\n        total += labels.size(0)\n        accuracy += (predicted == labels).sum().item()\n        test_accuracy = (100*accuracy)\/total\n    print(f'test loss: {test_loss:.3f}, test accuracy: {(100*accuracy)\/total:.2f}%')","9a6d9884":"## Visualizar algunas im\u00e1genes","4bab9487":"# Entrenamiento del modelo (train set)","0aad7515":"# Accuracy del modelo (dev set)","c370f868":"# Accuracy del test set","46c90935":"# Preprocesamiento de la data (test set)","7525c042":"# Creaci\u00f3n del modelo ","24613899":"## Cargar el dataset","5585ab89":"# Clase CNN","f61b0dd5":"# Preprocesamiento de la data (train and dev set)"}}