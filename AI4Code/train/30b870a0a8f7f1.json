{"cell_type":{"f22a7b4b":"code","631ab354":"code","96186fb5":"code","33275e56":"code","09eee5dd":"code","b8d3df9f":"code","24bc555c":"code","5d4dd50d":"code","8b029989":"code","3df6de6f":"code","06f393e0":"code","33de90ae":"code","27908008":"code","b1a0caab":"code","d30100f1":"code","2ccbaf1a":"code","f76386c9":"code","8d78f2fe":"code","879b74e9":"code","2ef34a7f":"code","1782aa16":"code","579c63bf":"code","7f6d23d0":"code","cd4f95f7":"code","232aae2a":"code","bc87401b":"code","7678476b":"code","f52bf555":"code","42ba9aea":"code","8a138dce":"code","6df97f02":"code","22a53de4":"code","95530fda":"code","0336ef85":"code","f1190ead":"code","d0a3442a":"code","34c7b367":"code","00c87fa1":"code","d845aa15":"code","9a0d8f8d":"code","5315889a":"code","d46b7e71":"code","afa33518":"code","d7dafeaa":"code","f1b3affc":"code","7e11c3ef":"code","9b33b613":"code","5e7ef0ba":"code","bb964605":"code","675c9ba8":"code","64864b6d":"code","e1662313":"code","631c8769":"code","a49774fc":"code","02798e08":"code","2b87397a":"code","1b881d1c":"code","00dfb11d":"code","9e0cfa71":"code","3fb47b71":"code","bd54efdc":"code","df739ab0":"markdown","a61f9a84":"markdown","90732974":"markdown","75324e19":"markdown","8e7c8e54":"markdown","37d21572":"markdown","91f824ea":"markdown","ed521523":"markdown","fc1ca985":"markdown","b0f52168":"markdown","b50e74c6":"markdown","796cc4f4":"markdown","504a0d2b":"markdown","8598da41":"markdown","d2271c6d":"markdown","50005f78":"markdown","27f90c9e":"markdown","bc4076bd":"markdown","35fa9c39":"markdown","592efb55":"markdown","ce20d350":"markdown","0c6540b7":"markdown","c6e68ba1":"markdown","674dcb9f":"markdown"},"source":{"f22a7b4b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","631ab354":"!pip install forgebox","96186fb5":"from pathlib import Path","33275e56":"DATA = Path(\"\/kaggle\/input\/netflix-shows\/netflix_titles.csv\")\n\ndf = pd.read_csv(DATA)\n\ndf.sample(10)","09eee5dd":"df.listed_in.value_counts()","b8d3df9f":"df.rating.value_counts()","24bc555c":"df[\"listed_in\"] = df.listed_in.str\\\n.replace(\"&\",\",\")\\\n.replace(\" , \",\",\")\\\n.replace(\" ,\",\",\")\\\n.replace(\", \",\",\")\\\n.replace(\" , \",\",\")","5d4dd50d":"genre = list(set(i.strip() for i in (\",\".join(list(df.listed_in))).split(\",\")))","8b029989":"print(f\"Total genre: {len(genre)}\\n\")\nfor g in genre:\n    print(g,end=\"\\t\")","3df6de6f":"eye = np.eye(len(genre))\ngenre_dict = dict((v,eye[k]) for k,v in enumerate(genre))\n\ndef to_nhot(text):\n    return np.sum(list(genre_dict[g.strip()] for g in text.split(\",\")),axis=0).astype(np.int)\n\ndf[\"genre\"] = df.listed_in.apply(to_nhot)","06f393e0":"PROCESSED = \"processed.csv\"","33de90ae":"df.to_csv(PROCESSED,index = False)","27908008":"from forgebox.ftorch.prepro import split_df","b1a0caab":"train_df,val_df = split_df(df,valid=0.1)\nprint(f\"train:{len(train_df)}\\tvalid:{len(val_df)}\")","d30100f1":"from nltk.tokenize import TweetTokenizer\ntkz = TweetTokenizer()\ndef tokenize(txt):\n    return tkz.tokenize(txt)","2ccbaf1a":"tokenize(\"A man returns home after being released from \")","f76386c9":"from itertools import chain\nfrom multiprocessing import Pool\nfrom collections import Counter\nfrom torch.utils.data.dataset import Dataset\n\nclass Vocab(object):\n    def __init__(self, iterative, max_vocab = 12000, tokenize = tkz.tokenize):\n        \"\"\"\n        Count the most frequent words\n        Make the word<=>index mapping\n        \"\"\"\n        self.l = list(iterative)\n        self.max_vocab = max_vocab\n        self.tokenize = tokenize\n        self.word_beads = self.word_beads_()\n        self.counter()\n        \n    def __len__(self):\n        return len(self.words)\n        \n    def __repr__(self):\n        return f\"vocab {self.max_vocab}\"\n        \n    def word_beads_(self, nproc=10):\n        self.p = Pool(nproc)\n        return list(chain(*list(self.p.map(self.tokenize,self.l))))\n    \n    def counter(self):\n        vals = np.array(list((k,v) for k,v in dict(Counter(self.word_beads)).items()))\n        self.words = pd.DataFrame({\"tok\":vals[:,0],\"ct\":vals[:,1]}).sort_values(by= \"ct\",ascending=False)\\\n        .reset_index().drop(\"index\",axis=1).head(self.max_vocab-2)\n        self.words[\"idx\"] = (np.arange(len(self.words))+2)\n        self.words=pd.concat([self.words,pd.DataFrame({\"tok\":[\"<eos>\",\"<mtk>\"],\"ct\":[-1,-1],\"idx\":[0,1]})])\n        return self.words\n    \n    def to_i(self):\n        self.t2i = dict(zip(self.words[\"tok\"],self.words[\"idx\"]))\n        def to_index(t):\n            i = self.t2i.get(t)\n            if i==None:\n                return 1\n            else:\n                return i\n        return to_index\n    \n    def to_t(self):\n        return np.roll(self.words[\"tok\"],2)\n        \n        ","8d78f2fe":"vocab = Vocab(df.description)","879b74e9":"vocab.words","2ef34a7f":"class seqData(Dataset):\n    def __init__(self,lines,vocab):\n        self.lines = list(lines)\n        self.vocab = vocab\n        self.to_i = np.vectorize(vocab.to_i())\n        self.to_t = vocab.to_t()\n        self.bs=1\n        \n    def __len__(self):\n        return len(self.lines)\n    \n    def __getitem__(self,idx):\n        \"\"\"\n        Translate words to indices\n        \"\"\"\n        line = self.lines[idx]\n        words = self.vocab.tokenize(line)\n        words = [\"<eos>\",]+words+[\"<eos>\"]\n        return self.to_i(np.array(words))\n    \n    def backward(self,seq):\n        \"\"\"\n        This backward has nothing to do with gradrient\n        Just to error proof the tokenized line\n        \"\"\"\n        return \" \".join(self.to_t[seq])\n    \nclass arrData(Dataset):\n    def __init__(self, *arrs):\n        self.arr = np.concatenate(arrs,axis=1)\n    \n    def __len__(self):\n        return self.arr.shape[0]\n    \n    def __getitem__(self,idx):\n        return self.arr[idx]","1782aa16":"vocab = Vocab(df.description)\n\ntrain_seq = seqData(train_df.description,vocab)\ntrain_y = arrData(np.stack(train_df.genre.values))\n\nval_seq = seqData(val_df.description,vocab)\nval_y = arrData(np.stack(val_df.genre.values))","579c63bf":"len(train_seq),len(train_y)","7f6d23d0":"tokenized_line = train_seq[10]\ntokenized_line","cd4f95f7":"train_seq.backward(tokenized_line)","232aae2a":"import torch\nfrom torch.utils.data.dataloader import DataLoader","bc87401b":"def pad_collate(rows):\n    \"\"\"\n    this collate will pad any sentence that is less then the max length\n    \"\"\"\n    line_len = torch.LongTensor(list(len(row) for row in rows));\n    max_len = line_len.max()\n    ones = torch.ones(max_len.item()).long()\n    line_pad = max_len-line_len\n    return torch.stack(list(torch.cat([torch.LongTensor(row),ones[:pad.item()]]) for row,pad in zip(rows,line_pad[:,None])))","7678476b":"gen = iter(DataLoader(train_seq,batch_size=16, collate_fn=pad_collate))\nnext(gen).size()","f52bf555":"class fuse(Dataset):\n    def __init__(self, *datasets):\n        \"\"\"\n        A pytorch dataset combining the dataset\n        :param datasets:\n        \"\"\"\n        self.datasets = datasets\n        length_s = set(list(len(d) for d in self.datasets))\n        assert len(length_s) == 1, \"dataset lenth not matched\"\n        self.length = list(length_s)[0]\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        return tuple(d.__getitem__(idx) for d in self.datasets)","42ba9aea":"def combine_collate(*funcs):\n    def combined(rows):\n        xs = list(zip(*rows))\n        return tuple(func(x) for func, x in zip(funcs,xs))\n    return combined\n\nfrom torch.utils.data._utils.collate import default_collate","8a138dce":"DataLoader(train_y).collate_fn","6df97f02":"train_ds = fuse(train_seq,train_y)\nval_ds = fuse(val_seq,val_y)","22a53de4":"gen = iter(DataLoader(train_ds,batch_size=16, collate_fn=combine_collate(pad_collate,default_collate)))\nx,y = next(gen)\nprint(x.shape,y.shape)","95530fda":"from torch import nn\nimport torch","0336ef85":"class basicNLP(nn.Module):\n    def __init__(self, hs):\n        super().__init__()\n        self.hs = hs\n        self.emb = nn.Embedding(len(vocab),hs)\n        self.rnn = nn.LSTM(input_size = hs,hidden_size = hs,batch_first = True)\n        self.fc = nn.Sequential(*[\n            nn.BatchNorm1d(hs*2),\n            nn.ReLU(),\n            nn.Linear(hs*2,hs*2),\n            nn.BatchNorm1d(hs*2),\n            nn.ReLU(),\n            nn.Linear(hs*2,49),\n        ])\n        \n    def encoder(self,x):\n        x = self.emb(x)\n        o1,(h1,c1) = self.rnn(x)\n        # run sentence backward\n        o2,(h2,c2) = self.rnn(x.flip(dims=[1]))\n        return torch.cat([h1[0],h2[0]],dim=1)\n        \n    def forward(self,x):\n        vec = self.encoder(x)\n        return self.fc(vec)","f1190ead":"model = basicNLP(100)","d0a3442a":"x[:2,:]","34c7b367":"x.shape,model.emb(x).shape","00c87fa1":"%time \noutput,(hidden_state, cell_state) = model.rnn(model.emb(x))\nfor t in (output,hidden_state, cell_state):\n    print(t.shape)","d845aa15":"%time\ninit_hidden = torch.zeros((1,16,100))\ninit_cell = torch.zeros((1,16,100))\nlast_h,last_c = init_hidden,init_cell\noutputs = []\nx_vec = model.emb(x)\nfor row in range(x.shape[1]):\n    last_o, (last_h,last_c) = model.rnn(x_vec[:,row:row+1,:],(last_h,last_c))\n    outputs.append(last_o)","9a0d8f8d":"manual_iteration_result = torch.cat(outputs,dim=1)","5315889a":"manual_iteration_result.shape","d46b7e71":"(manual_iteration_result==output).float().mean()","afa33518":"lossf = nn.BCEWithLogitsLoss()","d7dafeaa":"from forgebox.ftorch.train import Trainer\nfrom forgebox.ftorch.callbacks import stat\nfrom forgebox.ftorch.metrics import metric4_bi","f1b3affc":"model = model.cuda()","7e11c3ef":"t = Trainer(train_ds, val_dataset=val_ds,batch_size=16,callbacks=[stat], val_callbacks=[stat] ,shuffle=True,)","9b33b613":"t.opt[\"adm1\"] = torch.optim.Adam(model.parameters())","5e7ef0ba":"t.train_data.collate_fn = combine_collate(pad_collate,default_collate)\nt.val_data.collate_fn = combine_collate(pad_collate,default_collate)","bb964605":"@t.step_train\ndef train_step(self):\n    self.opt.zero_all()\n    x,y = self.data\n    y_= model(x)\n    loss = lossf(y_,y.float())\n    loss.backward()\n    self.opt.step_all()\n    acc,rec,prec,f1 = metric4_bi(torch.sigmoid(y_),y)\n    return dict((k,v.item()) for k,v in zip([\"loss\",\"acc\",\"rec\",\"prec\",\"f1\"],(loss,acc,rec,prec,f1)))\n                \n@t.step_val\ndef val_step(self):\n    x,y = self.data\n    y_= model(x)\n    loss = lossf(y_,y.float())\n    acc,rec,prec,f1 = metric4_bi(torch.sigmoid(y_),y)\n    return dict((k,v.item()) for k,v in zip([\"loss\",\"acc\",\"rec\",\"prec\",\"f1\"],(loss,acc,rec,prec,f1)))","675c9ba8":"t.train(10)","64864b6d":"model = model.eval()\ndl = DataLoader(train_seq, batch_size=32, collate_fn=pad_collate)","e1662313":"text_gen = iter(dl)\nresult = []\nfor i in range(len(dl)):\n    x=next(text_gen)\n    x = x.cuda()\n    x_vec = model.encoder(x)\n    result.append(x_vec.cpu())","631c8769":"result_vec = torch.cat(result,dim=0).detach().numpy()\nresult_vec.shape","a49774fc":"def to_idx(line):\n    words = train_seq.vocab.tokenize(line)\n    words = [\"<eos>\",]+words+[\"<eos>\"]\n    return train_seq.to_i(np.array(words))[None,:]","02798e08":"to_idx(\"this\"), to_idx(\"to be or not to be\")","2b87397a":"def to_vec(line):\n    vec = torch.LongTensor(to_idx(line)).cuda()\n    return model.encoder(vec).cpu().detach().numpy()","1b881d1c":"to_vec(\"this\"), to_vec(\"to be or not to be\")","00dfb11d":"def l2norm(x):\n    \"\"\"\n    L2 Norm\n    \"\"\"\n    return np.linalg.norm(x,2,1).reshape(-1,1)","9e0cfa71":"pd.set_option(\"max_colwidth\",150)\n\ndef search(line):\n    vec = to_vec(line)\n    sim = ((vec* result_vec)\/l2norm(result_vec)).sum(-1)\n    return pd.DataFrame({\"text\":train_seq.lines,\"sim\":sim})\\\n        .sort_values(by=\"sim\",ascending=False)","3fb47b71":"search(\"Experience our planet's natural beauty\").head(10)","bd54efdc":"search(\"love story,marriage, girl\").head(10)","df739ab0":"### In terms of code","a61f9a84":"### Process the text","90732974":"### What does LSTM return?","75324e19":"### Search similar","8e7c8e54":"### What does embedding do?","37d21572":"For what is LSTM, read this [awesome blog](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/), from which I stole the following visualization from\n\n#### In short version\nRNN, it's about sharing model weights throughout temporal sequence, as convolusion share weights in spatial point of view\n![image.png](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/RNN-unrolled.png)\n\n* The above green \"A\" area re shared linear layer\n* GRU & LSTM are advanced version of RNN, with gate control\n* The black arrows above in GRU & LSTM are controlled by gates\n* Gates, are just linear layer with sigmoid activation $\\sigma(x)$, its outputs are between (0,1), hence the name gate, the following illustration is one of the gates in a lstm cell, called input gate\n![gate controls](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-focus-f.png)\n* Other gates control other things like should we forget the early part of then sentence, should we output this .etc\n\n","91f824ea":"Size of train dataset","ed521523":"Combined collate function","fc1ca985":"Build vocabulary and train dataset","b0f52168":"The 2 results are the same, of course, I thought manual python iteration is slower,but they are really close by the above test","b50e74c6":"Well, usually it should be more accurate if we have more data","796cc4f4":"### A custom made collate function\n\n* Collate function will do the following:\n>Make rows of dataset output into a batch of tensor","504a0d2b":"### Fusing data set","8598da41":"### Generate vocabulary map from material","d2271c6d":"### Training","50005f78":"### Vocabulary build from training","27f90c9e":"* Create a pytorch dataset for text\n* A BiLSTM model to predict multiple genre\n* Encode the text to vectors using the model we trained\n* Search the closest description","bc4076bd":"### Model","35fa9c39":"A vector representing each of the sentence","592efb55":"### Testing Generator","ce20d350":"### So... what Y?","0c6540b7":"# Netflix Dataset\n>and all those embedding funs","c6e68ba1":"Disect the iteration through the sentence","674dcb9f":"Reconstruct the sentence from indices\n\n>**<mtk\\>** means the missing tokens, for they are less frequent than we should hav cared"}}