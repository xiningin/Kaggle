{"cell_type":{"e76f7f85":"code","5838d1f9":"code","1ab903a4":"code","a86f242f":"code","18706050":"code","c8b0d9cb":"code","35518567":"code","60bfd5e0":"code","9952cc10":"code","73dedf88":"code","9ae8c437":"code","c4e1cc28":"code","ee113a6e":"code","75c7637f":"code","005065ef":"code","845984b3":"code","9fcdf2e8":"code","e6908083":"code","b5065c10":"code","e7174a47":"code","bd071c2b":"code","e58e6669":"code","a4ed4204":"code","3035cea3":"code","ad684879":"markdown","74b10896":"markdown","1b3d8671":"markdown","729ade6c":"markdown"},"source":{"e76f7f85":"import keras\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nkeras.__version__","5838d1f9":"#from keras.datasets import boston_housing\n#(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","1ab903a4":"train.shape","a86f242f":"test.shape","18706050":"train = train._get_numeric_data()\ntest = test._get_numeric_data()","c8b0d9cb":"train.info()\ntrain['LotFrontage'].hist()\ntrain['GarageYrBlt'].hist()\ntrain['MasVnrArea'].hist()","35518567":"train['SalePrice'].hist()","60bfd5e0":"train = train.fillna(train.median())","9952cc10":"train.info()","73dedf88":"test._get_numeric_data().info()","9ae8c437":"test = test.fillna(train.median())","c4e1cc28":"train_ID = train['Id']\ntest_ID = test['Id']","ee113a6e":"train.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","75c7637f":"train_targets = train['SalePrice']","005065ef":"y = train.SalePrice\nX = train.drop(['SalePrice'], axis=1)","845984b3":"# X.shape\n# y.shape","9fcdf2e8":"# mean = train.mean(axis=0)\n# train -= mean\n# std = train.std(axis=0)\n# train \/= std\n\n# test -= mean\n# test \/= std","e6908083":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","b5065c10":"from keras import models\nfrom keras import layers\nfrom keras import metrics\n\ndef build_model():\n    # Because we will need to instantiate\n    # the same model multiple times,\n    # we use a function to construct it.\n    model = models.Sequential()\n    model.add(layers.Dense(128, activation='relu'))\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dense(1))\n    model.compile(optimizer='adam', loss='mse', metrics=[keras.metrics.RootMeanSquaredError()])\n    return model","e7174a47":"# X_train.shape\n# y_train.shape\n# X_test.shape\n# y_test.shape","bd071c2b":"# Get a fresh, compiled model.\nmodel = build_model()\n# Train it on the entirety of the data.\nmodel.fit(X_train, y_train,\n          epochs=200, batch_size=16, verbose=0)\ntest_mse_score, test_mae_score = model.evaluate(X_test, y_test)","e58e6669":"test_mae_score","a4ed4204":"submission = pd.DataFrame()","3035cea3":"submission['Id'] = test_ID\n\nprediction = model.predict(test)\nsubmission['SalePrice'] = prediction\nsubmission.to_csv('submission1.csv', index=False)","ad684879":"### Check for null values ","74b10896":"## Preparing the data\n\n\nIt would be problematic to feed into a neural network values that all take wildly different ranges. The network might be able to \nautomatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice to deal \nwith such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix), we \nwill subtract the mean of the feature and divide by the standard deviation, so that the feature will be centered around 0 and will have a \nunit standard deviation. This is easily done in Numpy:","1b3d8671":"Drop id Column","729ade6c":"## Building our network\n\n\nBecause so few samples are available, we will be using a very small network with two \nhidden layers, each with 64 units. In general, the less training data you have, the worse overfitting will be, and using \na small network is one way to mitigate overfitting."}}