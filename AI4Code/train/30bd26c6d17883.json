{"cell_type":{"826936b3":"code","66df210a":"code","456f86c9":"code","119dc3f8":"code","f6b52d5f":"code","0aa80bcd":"code","13cbab9b":"code","c58c6c87":"code","7583a554":"code","fd086a46":"code","7859c852":"code","2a5ca687":"code","4feafb8d":"code","6e5c7121":"code","081088b3":"code","8b64e1e2":"code","ca445245":"code","a7cc6e09":"code","8520102b":"code","96a655f3":"code","5fcb8476":"code","ea365bed":"code","901eaa03":"code","0a63d324":"code","0d48db5f":"code","2f1ef654":"code","721c2c45":"code","cdbf1eba":"code","c9ca5417":"markdown","c054f6f3":"markdown","f4d9a37e":"markdown","fba1383a":"markdown","c7891d48":"markdown","b24ee87c":"markdown","36b79f04":"markdown","5e9797a7":"markdown"},"source":{"826936b3":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)","66df210a":"import torch\ntorch.__version__","456f86c9":"#!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n#!python pytorch-xla-env-setup.py --version nightly  --apt-packages libomp5 libopenblas-dev\n\n#!pip install -q cloud-tpu-client==0.10 https:\/\/storage.googleapis.com\/tpu-pytorch\/wheels\/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n!pip install cloud-tpu-client==0.10 https:\/\/storage.googleapis.com\/tpu-pytorch\/wheels\/torch_xla-1.7-cp37-cp37m-linux_x86_64.whl\n!pip install -q albumentations==0.4.6","119dc3f8":"# import os\n\n# os.environ['XLA_USE_BF16'] = \"1\"\n# os.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'","f6b52d5f":"import torch\nprint(torch.__name__, torch.__version__)\n\nimport torch_xla\nprint(torch_xla.__name__, torch_xla.__version__)\n\nimport numpy as np\nprint(np.__name__, np.__version__)\n\n#device = xm.xla_device()\n#print(device)","0aa80bcd":"torch.tensor([1.0]).numpy()","13cbab9b":"# import random\nimport os\nimport torch\n\nVERSION = ''\n\ndef get_config():\n    config = {\n        'VERSION':VERSION,\n        'OUTPUT_PATH':'.\/',\n        'INPUT_PATH':'..\/input\/gan-getting-started\/',\n\n        'pretrain_path':None, \n        \n        'resolution':(256,256),\n        'input_resolution':(256,256),\n        \n        'lambda_cyc':10,\n        'lambda_idt':5,\n        'lr_G':8*2e-4,\n        'lr_D':8*2e-4,\n        'beta1':0.5,\n        'beta2':0.999,\n        'n_ite_D':1,\n        'num_workers':0, #8,\n        'fixed_noise_size':32,\n        'seed':42,\n        'epochs':1000, #30,\n        'show_epoch_list':[1]+np.arange(0,1000+10,10).tolist(),\n        'output_freq':100, #10,\n        'h_out':30,\n        'w_out':30,\n        'nprocs':8, #1,\n        'label_smooth':True,\n        \n        'tta':1,\n        'batch_size':32, #1, #8,\n        \n        'FP16':False,\n        #'device':xm.xla_device()\n    }\n    return config\n\nconfig = get_config()\n#device = config['device']\n#print(device)","c58c6c87":"import numpy as np\nimport pandas as pd\npd.get_option(\"display.max_columns\")\npd.set_option('display.max_columns', 300)\npd.get_option(\"display.max_rows\")\npd.set_option('display.max_rows', 300)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport sys\nimport os\nfrom os.path import join as opj\nimport gc\nimport cv2\n\nos.makedirs(config['OUTPUT_PATH'], exist_ok=True)","7583a554":"import glob\n\nmonet_jpg_list = sorted(glob.glob(opj(config['INPUT_PATH'],'monet_jpg\/*')))\nphoto_jpg_list = sorted(glob.glob(opj(config['INPUT_PATH'],'photo_jpg\/*')))\n\nprint('len(monet_jpg_list) = ', len(monet_jpg_list))\nprint('len(photo_jpg_list) = ', len(photo_jpg_list))","fd086a46":"import random\nimport torch\nimport numpy as np\nimport os\nimport time\n\ndef fix_seed(seed):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \ndef elapsed_time(start_time):\n    return time.time() - start_time\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nfix_seed(2021)","7859c852":"import torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\n\ndef conv3x3(in_channel, out_channel): #not change resolusion\n    return nn.Conv2d(in_channel,out_channel,\n                      kernel_size=3,stride=1,padding=1,dilation=1,bias=False)\n\ndef conv1x1(in_channel, out_channel): #not change resolution\n    return nn.Conv2d(in_channel,out_channel,\n                      kernel_size=1,stride=1,padding=0,dilation=1,bias=False)\n\ndef init_weight(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        #nn.init.orthogonal_(m.weight, gain=1)\n        nn.init.normal_(m.weight, 0, 0.02)\n        if m.bias is not None:\n            m.bias.data.zero_()\n            \n    elif classname.find('Batch') != -1:\n        m.weight.data.normal_(1,0.02)\n        m.bias.data.zero_()\n    \n    elif classname.find('Linear') != -1:\n        #nn.init.orthogonal_(m.weight, gain=1)\n        nn.init.normal_(m.weight, 0, 0.02)\n        if m.bias is not None:\n            m.bias.data.zero_()\n    \n    elif classname.find('Embedding') != -1:\n        #nn.init.orthogonal_(m.weight, gain=1)\n        nn.init.normal_(m.weight, 0, 0.02)","2a5ca687":"class DownBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, use_norm=True):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, \n                              kernel_size=4, stride=2, padding=1, bias=False).apply(init_weight)\n        if use_norm:\n            self.norm = nn.InstanceNorm2d(out_channels, affine=True).apply(init_weight)\n        else:\n            self.norm = nn.Identity()\n        self.relu = nn.LeakyReLU(0.2, True)\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.norm(x)\n        x = self.relu(x)\n        return x\n\n\nclass UpBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, dropout=False):\n        super().__init__()\n        self.conv = nn.ConvTranspose2d(in_channels, out_channels, \n                                       kernel_size=4, stride=2, padding=1, bias=False).apply(init_weight)\n        self.norm = nn.InstanceNorm2d(out_channels, affine=True).apply(init_weight)\n        if dropout:\n            self.dropout = nn.Dropout2d(0.5)\n        else:\n            self.dropout = nn.Identity()\n        self.relu = nn.ReLU(True)\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.norm(x)\n        x = self.dropout(x)\n        x = self.relu(x)\n        return x\n\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.down_stack = nn.ModuleList([\n            DownBlock(  3, 64, use_norm=False), # (bs,64,128,128)\n            DownBlock( 64,128), # (bs,128,64,64)\n            DownBlock(128,256), # (bs,256,32,32)\n            DownBlock(256,512), # (bs,512,16,16)\n            DownBlock(512,512), # (bs,512,8,8)\n            DownBlock(512,512), # (bs,512,4,4)\n            DownBlock(512,512), # (bs,512,2,2)\n            #DownBlock(512,512), # (bs,512,1,1)\n           ])\n        self.up_stack = nn.ModuleList([\n            #UpBlock( 512,512, dropout=True), # (bs,512,2,2)\n            UpBlock( 512,512, dropout=True), # (bs,512,4,4)\n            UpBlock(1024,512, dropout=True), # (bs,512,8,8)\n            UpBlock(1024,512), # (bs,512,16,16)\n            UpBlock(1024,256), # (bs,256,32,32)\n            UpBlock( 512,128), # (bs,128,64,64)\n            UpBlock( 256, 64), # (bs,64,128,128)\n        ])\n        self.last_layer = nn.Sequential(\n            nn.ConvTranspose2d(128,3, kernel_size=4, stride=2, padding=1, bias=False).apply(init_weight),\n            nn.Tanh()\n        )\n\n    def forward(self, x): # (bs,3,256,256)\n        skips = []\n        for down in self.down_stack:\n            x = down(x)\n            skips.append(x)\n        skips = reversed(skips[:-1])\n        for up,skip in zip(self.up_stack, skips):\n            x = torch.cat([up(x), skip], dim=1)\n        x = self.last_layer(x)\n        return x \n\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.down_blocks = nn.Sequential(\n            DownBlock(  3, 64, use_norm=False),\n            DownBlock( 64,128),\n            DownBlock(128,256),\n        )\n        #self.pad = nn.ZeroPad2d(1)\n        self.pad = nn.ReflectionPad2d(1)\n        self.conv = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=0, bias=False).apply(init_weight),\n            nn.InstanceNorm2d(512, affine=True).apply(init_weight),\n            nn.LeakyReLU(0.2, True)\n        )\n        self.last_conv = nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False).apply(init_weight)\n\n    def forward(self, x): # (bs,3,256,256)\n        x = self.down_blocks(x) # (bs,256,32,32)\n        x = self.pad(x) # (bs,256,34,34)\n        x = self.conv(x) # (bs,256,31,31)\n        x = self.pad(x) # (bs,256,33,33)\n        x = self.last_conv(x) # (bs,256,30,30)\n        return x","4feafb8d":"print('count_paramters(Generator()) = {:.2f} M'.format(count_parameters(Generator()) \/ 1e+6))\nprint('count_paramters(Discriminator()) = {:.2f} M'.format(count_parameters(Discriminator()) \/ 1e+6))\n\ngc.collect()","6e5c7121":"# net = Generator()\n# a = torch.randn(2,3,256,256)\n# net(a).shape","081088b3":"import numpy as np\nfrom albumentations import (Compose, HorizontalFlip, VerticalFlip, Rotate, RandomRotate90,\n                            ShiftScaleRotate, ElasticTransform, GridDistortion,\n                            Resize, RandomResizedCrop, RandomSizedCrop, RandomCrop, CenterCrop,\n                            RandomBrightnessContrast, HueSaturationValue, IAASharpen,\n                            RandomGamma, RandomBrightness, RandomBrightnessContrast,\n                            GaussianBlur,CLAHE,\n                            Cutout, CoarseDropout, GaussNoise, ChannelShuffle, ToGray, OpticalDistortion,\n                            Normalize, OneOf, NoOp)\nfrom albumentations.pytorch import ToTensor, ToTensorV2\n#from get_config import *\n#config = get_config()\n\n#MEAN = np.array([0.485, 0.456, 0.406])\n#STD  = np.array([0.229, 0.224, 0.225])\n\nMEAN = np.array([0.5, 0.5, 0.5])\nSTD = np.array([0.5, 0.5, 0.5])\n\n\ndef get_transforms_train():\n    transforms = Compose([\n        Resize(config['input_resolution'][0], config['input_resolution'][1]),\n        RandomResizedCrop(config['input_resolution'][0], config['input_resolution'][1],\n                          scale=(0.75,1.0), ratio=(1,1), interpolation=1, p=1.0),\n        HorizontalFlip(p=0.5),\n        Normalize(mean=MEAN, std=STD),\n        ToTensorV2(),\n    ] )\n    return transforms\n\n\ndef get_transforms_test():\n    transforms = Compose([\n        Resize(config['input_resolution'][0], config['input_resolution'][1]),\n        Normalize(mean=MEAN, std=STD),\n        ToTensorV2(),\n    ] )\n    return transforms\n\ndef denormalize(z, mean=MEAN.reshape(-1,1,1), std=STD.reshape(-1,1,1)):\n    return std*z + mean","8b64e1e2":"from torch.utils.data import Dataset\n\nclass MonetPhotoDatasetTrain(Dataset):\n    def __init__(self, monet_jpg_list, photo_jpg_list, mode='train'):\n        super().__init__()\n        if mode=='train':\n            self.transforms = get_transforms_train()\n        elif mode=='valid':\n            self.transforms = get_transforms_test()\n        self.h, self.w = config['resolution']\n        self.monet_jpg_list = monet_jpg_list\n        self.photo_jpg_list = photo_jpg_list\n        self.rand = np.random.permutation(np.arange(len(self.photo_jpg_list)))[:len(self.monet_jpg_list)]\n\n    def __len__(self):\n        return len(self.monet_jpg_list)\n\n    def __getitem__(self, idx):\n        img_monet = cv2.imread(self.monet_jpg_list[idx])\n        img_monet = cv2.cvtColor(img_monet, cv2.COLOR_BGR2RGB)\n        img_photo = cv2.imread(self.photo_jpg_list[self.rand[idx]])\n        img_photo = cv2.cvtColor(img_photo, cv2.COLOR_BGR2RGB)\n        if self.transforms:\n            img_monet = self.transforms(image=img_monet.astype(np.uint8))['image']\n            img_photo = self.transforms(image=img_photo.astype(np.uint8))['image']\n        return {'img_monet':img_monet, 'img_photo':img_photo}\n\n\n\nclass PhotoDatasetTest(Dataset):\n    def __init__(self, photo_jpg_list):\n        super().__init__()\n        self.transforms = get_transforms_test()\n        self.h, self.w = config['resolution']\n        self.photo_jpg_list = photo_jpg_list\n\n    def __len__(self):\n        return len(self.photo_jpg_list)\n\n    def __getitem__(self, idx):\n        img_photo = cv2.imread(self.photo_jpg_list[idx])\n        img_photo = cv2.cvtColor(img_photo, cv2.COLOR_BGR2RGB)\n        if self.transforms:\n            img_photo = self.transforms(image=img_photo.astype(np.uint8))['image']\n        return {'img_photo':img_photo}","ca445245":"idx = 0\ndummy = MonetPhotoDatasetTrain(monet_jpg_list, photo_jpg_list, mode='train')[idx]\n\nimg_monet = dummy['img_monet'].numpy()\nimg_monet = denormalize(img_monet).transpose(1,2,0)\nimg_photo = dummy['img_photo'].numpy()\nimg_photo = denormalize(img_photo).transpose(1,2,0)\n\n\nplt.figure(figsize=(12,6))\nplt.subplot(1,2,1)\nplt.imshow(img_monet)\nplt.subplot(1,2,2)\nplt.imshow(img_photo)\nplt.show()","a7cc6e09":"def generate_img(epoch, imgs):\n    for i in range(len(imgs)):\n        # denormalize\n        img = denormalize(imgs[i].numpy())\n        img = (255*img).astype(np.uint8)\n        # save\n        save_path = opj(config['OUTPUT_PATH'], 'img_{:02d}_epoch{}.jpg'.format(i, epoch))\n        img = img.transpose(1,2,0)\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # rgb -> bgr\n        cv2.imwrite(save_path, img) # bgr -> rgb","8520102b":"import time\nimport pandas as pd\nimport numpy as np\nimport gc\nfrom os.path import join as opj\nimport pickle\nfrom tqdm import tqdm_notebook as tqdm\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\n\nimport torch\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.utils.serialization as xser\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data import DataLoader\n\n\ndef map_fn(index, netG_m2p, netG_p2m, netD_m, netD_p, config, monet_jpg_list, photo_jpg_list, fixed_img_photo):\n    # setup\n    start_time = time.time()\n    torch.manual_seed(config['seed'])\n    device = xm.xla_device()\n\n    # model\n    xm.master_print(\"model setup...\")\n\n    netG_m2p = netG_m2p.to(device)\n    netG_p2m = netG_p2m.to(device)\n    netD_m = netD_m.to(device)\n    netD_p = netD_p.to(device)\n\n    if config['label_smooth']:\n        real_label = 0.9\n    else:\n        real_label = 1.0\n    fake_label = 0.0\n\n    h_out = config['h_out']\n    w_out = config['w_out']\n    \n    G_m2p_loss_list = []\n    G_p2m_loss_list = []\n    D_m_loss_list = []\n    D_p_loss_list = []\n    consistency_loss_list = []\n    identity_loss_list = []\n    \n    xm.master_print('loss setup...')\n    dis_criterion = nn.BCEWithLogitsLoss().to(device)\n    #dis_criterion = nn.MSELoss().to(device)\n    cycle_criterion = nn.L1Loss().to(device)\n    identity_criterion = nn.L1Loss().to(device)\n\n    xm.master_print('optimizer setup...')\n    optimizerG_m2p = optim.Adam(netG_m2p.parameters(), lr=config['lr_G'], betas=(config['beta1'], config['beta2']))\n    optimizerG_p2m = optim.Adam(netG_p2m.parameters(), lr=config['lr_G'], betas=(config['beta1'], config['beta2']))\n    optimizerD_m = optim.Adam(netD_m.parameters(), lr=config['lr_D'], betas=(config['beta1'], config['beta2']))\n    optimizerD_p = optim.Adam(netD_p.parameters(), lr=config['lr_D'], betas=(config['beta1'], config['beta2']))\n\n    netG_m2p.train()\n    netG_p2m.train()\n    netD_m.train()\n    netD_p.train()\n\n    # Barrier to prevent master from exiting before workers connect.\n    xm.rendezvous('init')\n\n    #training\n    print(\"Process {}, training start.\".format(index)) \n    for epoch in range(1,config['epochs']+1):\n\n        # dataset\n        train_dataset = MonetPhotoDatasetTrain(monet_jpg_list, photo_jpg_list, mode='train')\n        \n        # sampler\n        train_sampler = DistributedSampler(\n            train_dataset,\n            num_replicas=config['nprocs'], #xm.xrt_world_size(),\n            rank=xm.get_ordinal(),\n            shuffle=True\n        )\n        \n        train_sampler.set_epoch(epoch)\n\n        # dataloader\n        train_loader = DataLoader(\n            train_dataset, \n            batch_size=config['batch_size'],\n            sampler=train_sampler, \n            num_workers=config['num_workers'], \n            drop_last=True,\n            )     \n\n        tracker = xm.RateTracker()\n        para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n\n        count = 0\n        D_m_running_loss = 0\n        D_p_running_loss = 0\n        G_m2p_running_loss = 0\n        G_p2m_running_loss = 0\n        consistency_loss = 0\n        identity_loss = 0\n        \n        for ii, data in enumerate(para_train_loader):\n            batch_size = len(data)\n            \n            # label\n            pos_label = torch.full((config['batch_size'], 1, h_out, w_out), real_label, device=device)\n            neg_label = torch.full((config['batch_size'], 1, h_out, w_out), fake_label, device=device)\n            \n            # real images\n            img_monet_real = data['img_monet'].to(device, non_blocking=True) \n            img_photo_real = data['img_photo'].to(device, non_blocking=True) \n            \n            ############################\n            # Update G network\n            ###########################\n            netG_p2m.zero_grad()\n            netG_m2p.zero_grad()\n            \n            # monet to photo back to monet\n            img_photo_fake  = netG_m2p(img_monet_real)\n            img_monet_cycle = netG_p2m(netG_m2p(img_monet_real))\n            \n            # photo to monet back to photo\n            img_monet_fake  = netG_p2m(img_photo_real)\n            img_photo_cycle = netG_m2p(netG_p2m(img_photo_real))\n            \n            # generating itself\n            img_monet_same = netG_p2m(img_monet_real)\n            img_photo_same = netG_m2p(img_photo_real)\n            \n            # loss for generator\n            loss_gen_monet = dis_criterion(netD_m(img_monet_fake), pos_label)\n            loss_gen_photo = dis_criterion(netD_p(img_photo_fake), pos_label)\n            \n            loss_gen_cycle  = config['lambda_cyc'] * cycle_criterion(img_monet_cycle, img_monet_real)\n            loss_gen_cycle += config['lambda_cyc'] * cycle_criterion(img_photo_cycle, img_photo_real)\n            \n            loss_gen_same   = config['lambda_idt'] * identity_criterion(img_monet_same, img_monet_real)\n            loss_gen_same  += config['lambda_idt'] * identity_criterion(img_photo_same, img_photo_real)\n            \n            # backward\n            loss_gen_monet.backward(retain_graph=True)\n            loss_gen_photo.backward(retain_graph=True)\n            loss_gen_cycle.backward(retain_graph=False)\n            loss_gen_same.backward(retain_graph=False)\n            \n            # update\n            xm.optimizer_step(optimizerG_m2p)  # Note: barrier=True not needed when using ParallelLoader \n            xm.optimizer_step(optimizerG_p2m)  # Note: barrier=True not needed when using ParallelLoader \n            \n            # logging\n            count += 1.0\n            G_p2m_running_loss += loss_gen_monet.item()\n            G_m2p_running_loss += loss_gen_photo.item()\n            consistency_loss   += loss_gen_cycle.item()\n            identity_loss      += loss_gen_same.item()\n            \n            ############################\n            # Update D network\n            ###########################\n            netD_m.zero_grad()\n            netD_p.zero_grad()\n            \n            # monet discriminator\n            dis_monet_real = netD_m(img_monet_real)\n            dis_monet_fake = netD_m(netG_p2m(img_photo_real).detach())\n            \n            # photo discriminator\n            dis_photo_real = netD_p(img_photo_real)\n            dis_photo_fake = netD_p(netG_m2p(img_monet_real).detach())\n            \n            # loss for discriminator\n            loss_dis_monet  = dis_criterion(dis_monet_real, pos_label)\n            loss_dis_monet += dis_criterion(dis_monet_fake, neg_label)\n            loss_dis_monet *= 0.5\n            loss_dis_photo  = dis_criterion(dis_photo_real, pos_label)\n            loss_dis_photo += dis_criterion(dis_photo_fake, neg_label)\n            loss_dis_photo *= 0.5\n            \n            # backward\n            loss_dis_monet.backward(retain_graph=False)\n            loss_dis_photo.backward(retain_graph=False)\n            \n            # update\n            xm.optimizer_step(optimizerD_m)  # Note: barrier=True not needed when using ParallelLoader \n            xm.optimizer_step(optimizerD_p)  # Note: barrier=True not needed when using ParallelLoader \n            \n            # logging\n            D_m_running_loss += loss_dis_monet.item()\n            D_p_running_loss += loss_dis_photo.item()\n            \n        \n        del para_train_loader\n        gc.collect()\n        \n        # normalize\n        D_m_running_loss \/= count\n        D_p_running_loss \/= count\n        G_m2p_running_loss \/= count\n        G_p2m_running_loss \/= count\n        consistency_loss \/= count\n        identity_loss \/= count\n        \n        # output\n        if (epoch==1) or (epoch % config['output_freq'] == 0):\n            #xm.save(netG_m2p.state_dict(), opj(config['OUTPUT_PATH'], f\"generator_m2p_epoch{epoch}.bin\"))\n            #xm.save(netG_p2m.state_dict(), opj(config['OUTPUT_PATH'], f\"generator_p2m_epoch{epoch}.bin\"))\n            #xm.save(netD_m.state_dict(), opj(config['OUTPUT_PATH'], f\"discriminator_m_epoch{epoch}.bin\"))\n            #xm.save(netD_p.state_dict(), opj(config['OUTPUT_PATH'], f\"discriminator_p_epoch{epoch}.bin\"))\n            #xm.do_on_ordinals(generate_img, (epoch, netG_p2m(fixed_img_photo.to(device)).detach()), (0,))\n            xm.master_print('[Process {}, {:d}\/{:d}] D_m_loss = {:.3f}, D_p_loss = {:.3f}, elapsed_time = {:.1f} min'.format(index, epoch, config['epochs'], \n                                                                                                      D_m_running_loss, D_p_running_loss,\n                                                                                                      elapsed_time(start_time)\/60))\n            xm.master_print('  G_m2p_loss = {:.3f}, G_p2m_loss = {:.3f}, consistency loss = {:.3f}, identity_loss = {:.3f}'.format(G_m2p_running_loss, G_p2m_running_loss,\n                                                                                                      consistency_loss, identity_loss))\n            \n        gc.collect()\n        # log\n        D_m_loss_list.append(D_m_running_loss)\n        D_p_loss_list.append(D_p_running_loss)\n        G_m2p_loss_list.append(G_m2p_running_loss)\n        G_p2m_loss_list.append(G_p2m_running_loss)\n        consistency_loss_list.append(consistency_loss)\n        identity_loss_list.append(identity_loss)\n            \n    gc.collect()\n    xm.master_print('Saving Model...')\n    xm.save(netG_m2p.state_dict(), opj(config['OUTPUT_PATH'], \"generator_m2p.bin\"))\n    xm.save(netG_p2m.state_dict(), opj(config['OUTPUT_PATH'], \"generator_p2m.bin\"))\n    xm.save(netD_m.state_dict(), opj(config['OUTPUT_PATH'], \"discriminator_m.bin\"))\n    xm.save(netD_p.state_dict(), opj(config['OUTPUT_PATH'], \"discriminator_p.bin\"))\n    xm.master_print('Model Saved.')\n\n    if xm.is_master_ordinal():  # Divergent CPU-only computation (no XLA tensors beyond this point!)\n        with open(opj(config['OUTPUT_PATH'], 'D_m_loss_list'), 'wb') as f:\n            pickle.dump(D_m_loss_list, f)\n        with open(opj(config['OUTPUT_PATH'], 'D_p_loss_list'), 'wb') as f:\n            pickle.dump(D_p_loss_list, f)\n        with open(opj(config['OUTPUT_PATH'], 'G_m2p_loss_list'), 'wb') as f:\n            pickle.dump(G_m2p_loss_list, f)\n        with open(opj(config['OUTPUT_PATH'], 'G_p2m_loss_list'), 'wb') as f:\n            pickle.dump(G_p2m_loss_list, f)\n        with open(opj(config['OUTPUT_PATH'], 'consistency_loss_list'), 'wb') as f:\n            pickle.dump(consistency_loss_list, f)\n        with open(opj(config['OUTPUT_PATH'], 'identity_loss_list'), 'wb') as f:\n            pickle.dump(identity_loss_list, f)\n\n\ndef run_on_TPU(config, monet_jpg_list, photo_jpg_list):\n#     netG_m2p = Generator()\n#     netG_p2m = Generator()\n#     netD_m = Discriminator()\n#     netD_p = Discriminator()\n#     print('count_paramters(netG_m2p) = {:.2f} M'.format(count_parameters(netG_m2p) \/ 1e+6))\n#     print('count_paramters(netG_p2m) = {:.2f} M'.format(count_parameters(netG_p2m) \/ 1e+6))\n#     print('count_paramters(netD_m) = {:.2f} M'.format(count_parameters(netD_m) \/ 1e+6))\n#     print('count_paramters(netD_p) = {:.2f} M'.format(count_parameters(netD_p) \/ 1e+6))\n    netG_m2p = xmp.MpModelWrapper(Generator())\n    netG_p2m = xmp.MpModelWrapper(Generator())\n    netD_m = xmp.MpModelWrapper(Discriminator())\n    netD_p = xmp.MpModelWrapper(Discriminator())\n\n    # dataset\n    train_dataset = MonetPhotoDatasetTrain(monet_jpg_list, photo_jpg_list, mode='train')\n    fixed_img_photo = torch.stack([train_dataset[i]['img_photo'] for i in range(config['fixed_noise_size']) ])\n    del train_dataset\n    gc.collect()\n    \n    xmp.spawn(map_fn, args=(netG_m2p, netG_p2m, netD_m, netD_p, \n                            config, monet_jpg_list, photo_jpg_list, fixed_img_photo, ), \n              nprocs=config['nprocs'], start_method='fork')","96a655f3":"gc.collect()\n!free -h","5fcb8476":"#%%time\nimport pickle\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nrun_on_TPU(config, monet_jpg_list, photo_jpg_list)","ea365bed":"with open(opj(config['OUTPUT_PATH'], 'D_m_loss_list'), 'rb') as f:\n    D_m_loss_list = pickle.load(f)\nwith open(opj(config['OUTPUT_PATH'], 'D_p_loss_list'), 'rb') as f:\n    D_p_loss_list = pickle.load(f)\nwith open(opj(config['OUTPUT_PATH'], 'G_m2p_loss_list'), 'rb') as f:\n    G_m2p_loss_list = pickle.load(f)\nwith open(opj(config['OUTPUT_PATH'], 'G_p2m_loss_list'), 'rb') as f:\n    G_p2m_loss_list = pickle.load(f)\nwith open(opj(config['OUTPUT_PATH'], 'consistency_loss_list'), 'rb') as f:\n    consistency_loss_list = pickle.load(f)\nwith open(opj(config['OUTPUT_PATH'], 'identity_loss_list'), 'rb') as f:\n    identity_loss_list = pickle.load(f)\n\nplt.figure(figsize=(12,6))\nplt.plot(D_m_loss_list, label='D_m_loss')\nplt.plot(D_p_loss_list, label='D_p_loss')\nplt.plot(G_m2p_loss_list, label='G_m2p_loss')\nplt.plot(G_p2m_loss_list, label='G_p2m_loss')\nplt.plot(consistency_loss_list, label='consistency_loss')\nplt.plot(identity_loss_list, label='identity_loss')\nplt.grid()\nplt.legend()\nplt.title('loss history');","901eaa03":"# import glob\n\n# def show_generate_imgs(img_path_list):\n#     fig = plt.figure(figsize=(25, 16))\n#     for i, path in enumerate(img_path_list):\n#         img = cv2.imread(path)\n#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n#         ax = fig.add_subplot(4, 8, i + 1, xticks=[], yticks=[])\n#         plt.imshow(img)\n#     plt.show()\n#     plt.close()\n\n# for epoch in config['show_epoch_list']:\n#     if epoch==0:\n#         continue\n#     print('epoch = ', epoch)\n#     img_path_list = sorted(glob.glob(opj(config['OUTPUT_PATH'], '*epoch{}.jpg'.format(epoch))))\n#     show_generate_imgs(img_path_list)","0a63d324":"# model\nnetG_p2m = Generator()\nnetG_p2m.load_state_dict(torch.load(opj(config['OUTPUT_PATH'],'generator_p2m.bin')))\nnetG_p2m = netG_p2m.to(xm.xla_device()).eval()\n\n# photo data\nphoto_ds = PhotoDatasetTest(photo_jpg_list)\nprint('len(photo_ds) = ', len(photo_ds))","0d48db5f":"for i in range(len(photo_ds)):\n    if i==4:\n        break\n    img_photo = photo_ds[i]['img_photo']\n    img_pred  = netG_p2m(img_photo[None].to(xm.xla_device())).cpu().detach().numpy()[0]\n    img_pred  = denormalize(img_pred).transpose(1,2,0)\n    img_pred  = (255*img_pred).astype(np.uint8)\n    img_photo = denormalize(img_photo.numpy()).transpose(1,2,0)\n    img_photo = (255*img_photo).astype(np.uint8)\n\n    plt.figure(figsize=(12,6))\n    plt.subplot(1,2,1)\n    plt.imshow(img_photo)\n    plt.title('photo')\n    plt.subplot(1,2,2)\n    plt.imshow(img_pred)\n    plt.title('monet-esque')\n    plt.show()","2f1ef654":"%%time\n\nimport PIL\nfrom tqdm.notebook import tqdm\n\nos.makedirs('..\/images', exist_ok=True)\n\nfor i in tqdm(range(len(photo_ds))):\n    img_photo = photo_ds[i]['img_photo']\n    img_pred  = netG_p2m(img_photo[None].to(xm.xla_device())).cpu().detach().numpy()[0]\n    img_pred  = denormalize(img_pred).transpose(1,2,0)\n    img_pred  = (255 * img_pred).astype(np.uint8)\n    #img_pred = cv2.cvtColor(img_pred, cv2.COLOR_RGB2BGR) # rgb -> bgr\n    save_path = '..\/images\/{:04d}.jpg'.format(i)\n    #cv2.imwrite(save_path, img_pred) # bgr -> rgb\n    im = PIL.Image.fromarray(img_pred)\n    im.save(save_path)","721c2c45":"import shutil\n\nshutil.make_archive('\/kaggle\/working\/images', 'zip', root_dir='..\/images')","cdbf1eba":"import os\nimport glob\n\ndef remove_glob(pathname, recursive=True):\n    for p in glob.glob(pathname, recursive=recursive):\n        if os.path.isfile(p):\n            os.remove(p)\n\nremove_glob('..\/images\/*.jpg')","c9ca5417":"# Config","c054f6f3":"# Model","f4d9a37e":"# Dataset","fba1383a":"# Import Libraries and Data","c7891d48":"  --- baseline ---  \n* v02 : batch_size=1, label smooth, ReflectionPad2d, LeakyReLU(0.2), BCELoss for adv_loss, 30epochs, aug(h-flip), init_normal, 0.5 x dis_loss, lambda_cyc=10, lambda_idt=5, CycleGAN baseline, lr_G=2e-4, lr_D=2e-4, beta=(0.5,0.999), LB=62.62883  \n* v03 : set_epoch, affine=True for InstanceNorm2d, batch_size=1, label smooth, ReflectionPad2d, LeakyReLU(0.2), BCELoss for adv_loss, 30epochs, aug(h-flip), init_normal, 0.5 x dis_loss, lambda_cyc=10, lambda_idt=5, CycleGAN baseline, lr_G=2e-4, lr_D=2e-4, beta=(0.5,0.999), LB=61.30498  \n* v07 : batch_size=32, n_procs=8, 1000epochs, aug(random resized crop, h-flip), set_epoch, affine=True for InstanceNorm2d, label smooth, ReflectionPad2d, LeakyReLU(0.2), BCELoss for adv_loss, init_normal, 0.5 x dis_loss, lambda_cyc=10, lambda_idt=5, CycleGAN baseline, lr_G=2e-4, lr_D=2e-4, beta=(0.5,0.999), LB=  ","b24ee87c":"# Inference","36b79f04":"# Utils","5e9797a7":"# Train (on Multicore TPU)"}}