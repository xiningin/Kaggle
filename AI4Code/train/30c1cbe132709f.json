{"cell_type":{"22b337fe":"code","425b2537":"code","d39c1731":"code","3fd32fd6":"code","fc9a51c4":"code","ab144a38":"code","f6af55e4":"code","89ee9033":"code","62ed8589":"code","f7d66497":"code","d033dcb7":"code","29ac06c4":"code","0c2981a2":"code","2a3e203b":"code","942b835e":"code","62d2f6e2":"markdown","98a60379":"markdown","8bd41339":"markdown","707ce6f0":"markdown","7e27f4f4":"markdown","ca0282df":"markdown"},"source":{"22b337fe":"# install pytorh efficientnet\n!pip install efficientnet_pytorch","425b2537":"# set the path for iertstrat for creating cross-validation CSV file\nimport sys\niterstrat = '..\/input\/iterative-stratification\/iterative-stratification-master\/'\nsys.path.insert(0, iterstrat)","d39c1731":"import os\nimport pandas as pd\nimport joblib\nimport albumentations\nimport numpy as np\nimport torch\nimport torchvision.transforms as transforms\nimport cv2\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\n\nfrom efficientnet_pytorch import EfficientNet\nfrom tqdm import tqdm\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom PIL import Image\n\nstyle.use('ggplot')","3fd32fd6":"df = pd.read_csv('..\/input\/plant-pathology-2020-fgvc7\/train.csv')\nprint(df.head)\n# create a kfold col\ndf.loc[:, 'kfold'] = -1\n\n# shuffle the dataframe (reset index keeps the index same as before)\ndf = df.sample(frac=1).reset_index(drop=True)\nprint(df.head)\n\nX = df.image_id.values\n# we have multiple label columns\ny = df[['healthy', 'multiple_diseases', 'rust', 'scab']].values\n\nmskf = MultilabelStratifiedKFold(n_splits=5)\n\nfor fold, (trn_, val_) in enumerate(mskf.split(X, y)):\n    # trn_ and val_ are index values in the df\n    print('TRAIN: ', trn_, 'VAL: ', val_)\n    df.loc[val_, 'kfold'] = fold\n\nprint(df.kfold.value_counts())\ndf.to_csv('train_folds.csv', index=False)","fc9a51c4":"class PlantDatasetTrain:\n    def __init__(self, folds, img_height, img_width, mean, std):\n        df = pd.read_csv('train_folds.csv')\n        # only grab the cols that we need\n        df = df[['image_id', 'healthy', 'multiple_diseases', 'rust', 'scab', 'kfold']]\n\n        df = df[df.kfold.isin(folds)].reset_index(drop=True)\n        self.image_ids = df.image_id.values\n        self.healthy = df.healthy.values\n        self.multiple_diseases = df.multiple_diseases.values\n        self.rust = df.rust.values\n        self.scab = df.scab.values\n\n        # apply augmentations\n        if len(folds) == 1: # if validating\n            self.aug = albumentations.Compose([\n                albumentations.Resize(img_height, img_width, always_apply=True),\n                albumentations.Normalize(mean, std, always_apply=True)\n            ])\n        else: # if training\n            self.aug = albumentations.Compose([\n                albumentations.Resize(img_height, img_width, always_apply=True),\n                albumentations.ShiftScaleRotate(\n                    shift_limit=0.0625,\n                    scale_limit=0.1,\n                    rotate_limit=5,\n                    p=0.9\n                ),\n                albumentations.Normalize(mean, std, always_apply=True)\n            ])\n        \n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, item):\n        image = cv2.imread(f\"..\/input\/resized\/train_224\/{self.image_ids[item]}.jpg\")\n        image = cv2.resize(image, (224, 224))\n        image = self.aug(image=np.array(image))['image']\n        # from (h, w, c) to (c, h, w)\n        image = np.transpose(image, (2, 0, 1)).astype(np.float)\n        return {\n            'image': torch.tensor(image, dtype=torch.float),\n            'healthy': torch.tensor(self.healthy[item], dtype=torch.long),\n            'multiple_diseases': torch.tensor(self.multiple_diseases[item], dtype=torch.long),\n            'rust': torch.tensor(self.rust[item], dtype=torch.long),\n            'scab': torch.tensor(self.scab[item], dtype=torch.long)\n        }","ab144a38":"class EfficientNetB3(nn.Module):\n    def __init__(self, pretrained):\n        super(EfficientNetB3, self).__init__()\n        if pretrained == True:\n            self.model = EfficientNet.from_pretrained('efficientnet-b3')\n        else:\n            self.model = EfficientNet.from_name('efficientnet-b3')\n        \n        self.l0 = nn.Linear(1536, 2)\n        self.l1 = nn.Linear(1536, 2)\n        self.l2 = nn.Linear(1536, 2)\n        self.l3 = nn.Linear(1536, 2)\n\n    def forward(self, x):\n        # get the batch size only, ignore (c, h, w)\n        bs, _, _, _ = x.shape\n        x = self.model.extract_features(x)\n        # reshape makes the number of rows equal to batch_size\n        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n        l0 = self.l0(x)\n        l1 = self.l1(x)\n        l2 = self.l2(x)\n        l3 = self.l3(x)\n        return l0, l1, l2, l3","f6af55e4":"MODELS = {\n    'efficientnet-b3': EfficientNetB3,\n}","89ee9033":"def loss_fn(outputs, targets):\n    o1, o2, o3, o4 = outputs\n    t1, t2, t3, t4 = targets\n    l1 = nn.CrossEntropyLoss()(o1, t1)\n    l2 = nn.CrossEntropyLoss()(o2, t2)\n    l3 = nn.CrossEntropyLoss()(o3, t3)\n    l4 = nn.CrossEntropyLoss()(o4, t4)\n    return (l1 + l2 + l3 + l4) \/ 4\n\ndef train(dataset, data_loader, model, optimizer):\n    model.train()\n    final_loss = 0\n    counter = 0\n    for bi, d in tqdm(enumerate(data_loader), total=int(len(dataset)\/data_loader.batch_size)):\n        counter = counter + 1\n        image = d['image']\n        healthy = d['healthy']\n        multiple_diseases = d['multiple_diseases']\n        rust = d['rust']\n        scab = d['scab']\n\n        image = image.to(DEVICE, dtype=torch.float)\n        healthy = healthy.to(DEVICE, dtype=torch.long)\n        multiple_diseases = multiple_diseases.to(DEVICE, dtype=torch.long)\n        rust = rust.to(DEVICE, dtype=torch.long)\n        scab = scab.to(DEVICE, dtype=torch.long)\n\n        optimizer.zero_grad()\n        outputs = model(image)\n        targets = (healthy, multiple_diseases, rust, scab)\n        loss = loss_fn(outputs, targets)\n        final_loss += loss\n\n        loss.backward()\n        optimizer.step()\n    print(f\"Train loss: {(final_loss\/counter):.3f}\")\n    return final_loss\/counter\n\ndef evaluate(dataset, data_loader, model):\n    model.eval()\n    final_loss = 0\n    counter = 0\n    with torch.no_grad():\n        for bi, d in tqdm(enumerate(data_loader), total=int(len(dataset)\/data_loader.batch_size)):\n            counter = counter + 1\n            image = d['image']\n            healthy = d['healthy']\n            multiple_diseases = d['multiple_diseases']\n            rust = d['rust']\n            scab = d['scab']\n\n            image = image.to(DEVICE, dtype=torch.float)\n            healthy = healthy.to(DEVICE, dtype=torch.long)\n            multiple_diseases = multiple_diseases.to(DEVICE, dtype=torch.long)\n            rust = rust.to(DEVICE, dtype=torch.long)\n            scab = scab.to(DEVICE, dtype=torch.long)\n\n            outputs = model(image)\n            targets = (healthy, multiple_diseases, rust, scab)\n            loss = loss_fn(outputs, targets)\n            final_loss += loss\n    print(f\"Val loss: {(final_loss\/counter):.3f}\")\n    return final_loss \/ counter\n\ndef main():\n    model = MODELS[BASE_MODEL](pretrained=True)\n    model.to(DEVICE)\n    \n    # Find total parameters and trainable parameters\n#     total_params = sum(p.numel() for p in model.parameters())\n#     print(f'{total_params:,} total parameters.')\n#     total_trainable_params = sum(\n#         p.numel() for p in model.parameters() if p.requires_grad)\n#     print(f'{total_trainable_params:,} training parameters.')\n\n    train_dataset = PlantDatasetTrain(\n        folds=TRAINING_FOLDS,\n        img_height=IMG_HEIGHT,\n        img_width=IMG_WIDTH,\n        mean=MODEL_MEAN,\n        std=MODEL_STD\n    )\n\n    train_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        shuffle=True,\n        num_workers=4,\n    )\n\n    valid_dataset = PlantDatasetTrain(\n        folds=VALIDATION_FOLDS,\n        img_height=IMG_HEIGHT,\n        img_width=IMG_WIDTH,\n        mean=MODEL_MEAN,\n        std=MODEL_STD,\n    )\n\n    valid_loader = torch.utils.data.DataLoader(\n        dataset=valid_dataset,\n        batch_size=TEST_BATCH_SIZE,\n        shuffle=False,\n        num_workers=4,\n    )\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( \n        optimizer,\n        mode='min',\n        patience=5,\n        factor=0.5,\n        verbose=True\n    )\n\n    # train\n    train_loss, val_loss = [], []\n    for epoch in range(EPOCHS):\n        print(f\"Epoch {epoch+1} of {EPOCHS}\")\n        train_score = train(train_dataset, train_loader, model, optimizer)\n        val_score = evaluate(valid_dataset, valid_loader, model)\n        train_loss.append(train_score)\n        val_loss.append(val_score)\n        # run scheduler based on val_score\n        scheduler.step(val_score)\n        torch.save(model.state_dict(), f\"{BASE_MODEL}_fold{VALIDATION_FOLDS[0]}.bin\")\n\n    # loss plots\n    plt.figure(figsize=(10, 7))\n    plt.plot(train_loss, color='orange', label='train loss')\n    plt.plot(val_loss, color='red', label='validataion loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.savefig(f\"loss_{BASE_MODEL}_{VALIDATION_FOLDS}.png\")\n    plt.show()","62ed8589":"if torch.cuda.is_available():\n    DEVICE = 'cuda'\nelse:\n    DEVICE = 'cpu'\n    \n# parameters \/ options\nIMG_HEIGHT=224\nIMG_WIDTH=224\nEPOCHS=30\nTRAIN_BATCH_SIZE=32\nTEST_BATCH_SIZE=16\nMODEL_MEAN=[0.485, 0.456, 0.406]\nMODEL_STD=[0.229, 0.224, 0.225]\nBASE_MODEL='efficientnet-b3'\nTRAINING_FOLDS_CSV='..\/input\/train_folds.csv'","f7d66497":"TRAINING_FOLDS=(0, 1, 2, 3)\nVALIDATION_FOLDS=(4,)\nmain()","d033dcb7":"TRAINING_FOLDS=(0, 1, 2, 4)\nVALIDATION_FOLDS=(3,)\nmain()","29ac06c4":"TRAINING_FOLDS=(0, 1, 4, 3)\nVALIDATION_FOLDS=(2,)\nmain()","0c2981a2":"TRAINING_FOLDS=(0, 4, 2, 3)\nVALIDATION_FOLDS=(1,)\nmain()","2a3e203b":"TRAINING_FOLDS=(4, 1, 2, 3)\nVALIDATION_FOLDS=(0,)\nmain()","942b835e":"import glob \nimport albumentations\nimport torch\nimport pandas as pd\nimport joblib\nimport torch.nn as nn\nimport numpy as np\nimport cv2 as cv\n\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom torch.nn import functional as F\nfrom efficientnet_pytorch import EfficientNet\n\nTEST_BATCH_SIZE = 96\nMODEL_MEAN = (0.485, 0.456, 0.406)\nMODEL_STD = (0.229, 0.224, 0.225)\nIMG_HEIGHT = 224\nIMG_WIDTH = 224\n\nif torch.cuda.is_available():\n    DEVICE = 'cuda'\nelse:\n    DEVICE = 'cpu'\n\n###################################################################################################\nclass EfficientNetB3(nn.Module):\n    def __init__(self, pretrained):\n        super(EfficientNetB3, self).__init__()\n        if pretrained == True:\n            self.model = EfficientNet.from_pretrained('efficientnet-b3')\n        else:\n            self.model = EfficientNet.from_name('efficientnet-b3')\n        \n        # number of classes per label (3 different linear layers)\n        self.l0 = nn.Linear(1536, 2)\n        self.l1 = nn.Linear(1536, 2)\n        self.l2 = nn.Linear(1536, 2)\n        self.l3 = nn.Linear(1536, 2)\n\n    def forward(self, x):\n        # get the batch size only, ignore (c, h, w)\n        bs, _, _, _ = x.shape\n        x = self.model.extract_features(x)\n        # reshape makes the number of rows equal to batch_size\n        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n        l0 = self.l0(x)\n        l1 = self.l1(x)\n        l2 = self.l2(x)\n        l3 = self.l3(x)\n        return l0, l1, l2, l3\n##########################################################################\n\nclass PlantDatasetTest:\n    def __init__(self, df, img_height, img_width, mean, std):\n        self.image_ids = df.image_id.values\n        self.aug = albumentations.Compose([\n            albumentations.Resize(img_height, img_width, always_apply=True),\n            albumentations.Normalize(mean, std, always_apply=True)\n        ])\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, item):\n        image = cv.imread(f\"..\/input\/plant-pathology-2020-fgvc7\/images\/{self.image_ids[item]}.jpg\")\n        img_id = self.image_ids[item]\n        image = cv.resize(image, (224, 224))\n        image = self.aug(image=np.array(image))['image']\n        # from (h, w, c) to (c, h, w)\n        image = np.transpose(image, (2, 0, 1)).astype(np.float)\n        return {\n            'image': torch.tensor(image, dtype=torch.float),\n            'image_id': img_id\n        }\n\n''' Models '''\nmodels = []\nmodel4 = EfficientNetB3(pretrained=False).to(DEVICE)\nmodel4.load_state_dict(torch.load(f\"efficientnet-b3_fold4.bin\"))\nmodels.append(model4)\n\nmodel3 = EfficientNetB3(pretrained=False).to(DEVICE)\nmodel3.load_state_dict(torch.load(f\"efficientnet-b3_fold3.bin\"))\nmodels.append(model3)\n\nmodel2 = EfficientNetB3(pretrained=False).to(DEVICE)\nmodel2.load_state_dict(torch.load(f\"efficientnet-b3_fold2.bin\"))\nmodels.append(model2)\n\nmodel1 = EfficientNetB3(pretrained=False).to(DEVICE)\nmodel1.load_state_dict(torch.load(f\"efficientnet-b3_fold1.bin\"))\nmodels.append(model1)\n\nmodel0 = EfficientNetB3(pretrained=False).to(DEVICE)\nmodel0.load_state_dict(torch.load(f\"efficientnet-b3_fold0.bin\"))\nmodels.append(model0)\n''' Models '''\n\ndef model_predict(i):\n    with torch.no_grad():\n        h_pred, m_pred, r_pred, s_pred = [], [], [], []\n        img_ids_list = [] \n        \n        df = pd.read_csv(f\"..\/input\/plant-pathology-2020-fgvc7\/test.csv\")\n\n        dataset = PlantDatasetTest(df=df,\n                                    img_height=IMG_HEIGHT,\n                                    img_width=IMG_WIDTH,\n                                    mean=MODEL_MEAN,\n                                    std=MODEL_STD)\n\n        data_loader = torch.utils.data.DataLoader(\n            dataset=dataset,\n            batch_size=TEST_BATCH_SIZE,\n            shuffle=False,\n            # num_workers=4\n        )\n\n        for bi, d in enumerate(data_loader):\n            image = d[\"image\"]\n            img_id = d[\"image_id\"]\n            image = image.to(DEVICE, dtype=torch.float)\n\n            h, m, r, s = models[i](image)\n\n            for ii, imid in enumerate(img_id):\n                h_pred.append(h[ii].cpu().detach().numpy())\n                m_pred.append(m[ii].cpu().detach().numpy())\n                r_pred.append(r[ii].cpu().detach().numpy())\n                s_pred.append(s[ii].cpu().detach().numpy())\n                img_ids_list.append(imid)\n        \n    return h_pred, m_pred, r_pred, s_pred, img_ids_list\n\nfinal_h_pred = []\nfinal_m_pred = []\nfinal_r_pred = []\nfinal_s_pred = []\nfinal_img_ids = []\n\nfor i in range(len(models)):\n    # model.eval()\n    h_pred, m_pred, r_pred, s_pred, img_ids_list = model_predict(i)\n    \n    final_h_pred.append(h_pred)\n    final_m_pred.append(m_pred)\n    final_r_pred.append(r_pred)\n    final_s_pred.append(s_pred)\n    if i == 0:\n        final_img_ids.extend(img_ids_list)\n\nfinal_h = (np.mean(np.array(final_h_pred), axis=0))\nfinal_m = (np.mean(np.array(final_m_pred), axis=0))\nfinal_r = (np.mean(np.array(final_r_pred), axis=0))\nfinal_s = (np.mean(np.array(final_s_pred), axis=0))\n\npredictions = []\nfor ii, imid in enumerate(final_img_ids):\n    predictions.append((f\"{imid}\", final_h[ii][1], final_m[ii][1], final_r[ii][1], final_s[ii][1]))\n\nsub = pd.DataFrame(predictions, columns=['image_id', 'healthy', 'multiple_diseases', 'rust', 'scab'])\n\nsub.to_csv('submission.csv', index=False)\n\nprint('Successfully created submission file')","62d2f6e2":"## 1) Introduction\n* In this notebook, we will train the an EfficientNet-B3 pre-trained model on the plant pathology image data.\n* We will use on 224x224 images for faster training. But perhaps, larger images will give more accuracy. You can find the data [HERE](https:\/\/www.kaggle.com\/sovitrath\/resized). \n* Also, we will employ five fold cross-validation technique.\n* I hope that this works as a baseline approach and others can improve this notebook further on.","98a60379":"## 3) Create the Custom Dataset Module","8bd41339":"## 4) Create the EfficientNet-B3 Model Class","707ce6f0":"## Predictions","7e27f4f4":"## 2) Create the Cross-Validation CSV","ca0282df":"## 5) Train the Model"}}