{"cell_type":{"12cb7e0f":"code","9be79aad":"code","6b8886c3":"code","bf71a62f":"code","da96c592":"code","7c1426e5":"code","2947c097":"code","90fa12ac":"markdown"},"source":{"12cb7e0f":"import numpy as np\nimport pandas as pd\nimport tensorflow\nimport keras\nimport sklearn.model_selection\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","9be79aad":"# ############ Loading and splitting data set ################# #\n\ntrain_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\n\ntweet = np.array(train_data['text'] , dtype = 'str')\ntarget = np.array(train_data['target'])\n\nx_train = tweet[0:6850]\ny_train  = target[0:6850]\n\nx_test = tweet[6851:]\ny_test  = target[6851:]\n\n\nprint(x_train.dtype)\nprint(y_train.dtype)\n\nprint(x_train.shape , y_train.shape)\nprint(x_test.shape , y_test.shape)","6b8886c3":"# ############ Tokenizing \/ Sequencing \/ Padding ############### #\n\ntokenizer = Tokenizer(num_words = 10000 , oov_token= \"<OOV>\")\ntokenizer.fit_on_texts(x_train) ######### This only generates words data base of train #########\nword_index = tokenizer.word_index\n\n# ######### We dont tokenize test so that we dont have dat base of test words ############# #\n\nsequence_train = tokenizer.texts_to_sequences(x_train)\nsequence_test = tokenizer.texts_to_sequences(x_test)\n\npad_train = pad_sequences(sequence_train , padding = 'pre')\npad_test = pad_sequences(sequence_test , padding = 'pre')\n\n# print(pad_train[0].size)\n# print(pad_test[0].size)\n# print(pad_test[0])\n\n# print(x_train[0])\n# print(sequence_train[0])\n# print()\n# print(x_test[0])\n# print(sequence_test[0])","bf71a62f":"# ############# Model Creation ############### #\n\nmodel = keras.Sequential([\nkeras.layers.Embedding( 10000 , 16 ),  # ######### Creates vectors in diff dimensions ############ #\nkeras.layers.GlobalAveragePooling1D(), ###### Sum up vectors to understand context ########\n\n# #### Output and Dense layers #### #\n\nkeras.layers.Dense(24, activation='relu'),\nkeras.layers.Dense(1, activation='sigmoid'),\n\n])\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","da96c592":"model.summary()\n\nmodel.fit(pad_train , y_train , epochs=50 , validation_data=(pad_test , y_test))\n\nloss,  acc = model.evaluate(pad_test , y_test)\n\nprint(loss , acc)","7c1426e5":"# ######### Checking on validation data set ############ #\n\npredictions = model.predict(pad_test)\n\nfor i in range(len(predictions)):\n  print(  np.round(predictions[i] , 0)   , '\\t' , y_test[i])","2947c097":"# ########### Making sample submission file ############### #\n\ndata = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\nsen = np.array(data['text'])\nid_num = np.array(data['id'])\n\n\nsequence_sen = tokenizer.texts_to_sequences(sen)\npad_sen = pad_sequences(sequence_sen , padding='pre')\n\n\n# print(sen[0])\n# print(sequence_sen[0])\n# print(pad_sen[0])\n\n# ############## Predictions ############## #\n\nprediction = model.predict(pad_sen)\nprediction = np.round(prediction , 0)\nfinal_add = [int(j) for i in prediction for j in i]\n\nprint(final_add[0])\nprint(type(final_add[0]))\nz  = { \n'id' :  id_num ,    \n'target' : final_add , \n}\n\n\n\nsample = pd.DataFrame(z)\nsample.to_csv('Sample.csv')","90fa12ac":"# ***NLP for Text Classifaction***"}}