{"cell_type":{"4775687e":"code","ff32daae":"code","d54d21b1":"code","0a13509a":"code","644c07ab":"code","5970cdda":"code","b6c8df3b":"code","e1620275":"code","cf3f47bf":"code","d190098b":"code","a39e452f":"code","3eb44e41":"code","e1bf55aa":"code","5600521a":"code","a92c137d":"code","deec4bc4":"code","8c00ca57":"code","bf13cc82":"markdown","cb1addd7":"markdown","96ec4ece":"markdown","2a7862fc":"markdown","172337c9":"markdown","74305d71":"markdown","53839b95":"markdown","85b356f2":"markdown","67f16976":"markdown","0f22e83c":"markdown","ffdd7d9b":"markdown","85ea36df":"markdown","c00d3292":"markdown","52a0cc3a":"markdown"},"source":{"4775687e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ff32daae":"# Open the train dataset\ntrain_df = pd.read_csv('..\/input\/actuarial-loss-estimation\/train.csv')","d54d21b1":"# Selecting some columns to work with\nClaim_description = train_df[['ClaimNumber', 'DateTimeOfAccident', 'ClaimDescription', 'InitialIncurredCalimsCost', 'UltimateIncurredClaimCost']].copy()","0a13509a":"# Converting the date data to year only\nClaim_description['DateTimeOfAccident'] = pd.to_datetime(Claim_description['DateTimeOfAccident'])\nClaim_description['DateTimeOfAccident'] = Claim_description['DateTimeOfAccident'].dt.year","644c07ab":"Claim_description","5970cdda":"# Import the wordcloud library\nfrom wordcloud import WordCloud\n# Join the different descriptions together\nlong_string = ','.join(list(Claim_description['ClaimDescription'].values))\n# Create a WordCloud object\nwordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n# Generate a word cloud\nwordcloud.generate(long_string)\n# Visualize the word cloud\nwordcloud.to_image()\n","b6c8df3b":"# Importing libraries\nimport gensim # open-source library for unsupervised topic modeling and NLP\nfrom gensim.utils import simple_preprocess\nimport nltk # Natural Language Toolkit is a library to work with human language data\n\n# Storing a list of stopwords\nnltk.download('stopwords') # start the NLTK Downloader and download the stopwords\nfrom nltk.corpus import stopwords \nstop_words = stopwords.words('english') # Selecting english stopwords\nstop_words.extend(['from', 'subject', 're', 'edu', 'use']) # adding new stopwords\n\n# Defining a function to convert our descriptions to word tokens and remove ponctuation\ndef sent_to_words(descriptions):\n    i = 0\n    for description in descriptions:\n        # deacc=True removes punctuations\n        yield(gensim.utils.simple_preprocess(str(description), deacc=True)) \n        \n# Defining a function to remove stopwords    \ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) \n             if word not in stop_words] for doc in texts]\n        \ndata = Claim_description.ClaimDescription.values.tolist()\ndata_words = list(sent_to_words(data))\ndata_words = remove_stopwords(data_words)\n\nprint(data_words[0]) #Showing our first description tokens","e1620275":"import gensim.corpora as corpora\n# Create Dictionary\nid2word = corpora.Dictionary(data_words)\n# Create Corpus\ntexts = data_words\n\n# Term Document Frequency\n# Converts a collection of words to  a list of (word_id, word_frequency) 2-tuples.\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View first description\nprint(corpus[0])","cf3f47bf":"from pprint import pprint\n# number of topics\nnum_topics = 10\n# Build LDA model\nlda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                       id2word=id2word,\n                                       num_topics=num_topics,\n                                       random_state=0)\n# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","d190098b":"import pyLDAvis.gensim\nimport pickle \nimport pyLDAvis","a39e452f":"pyLDAvis.enable_notebook()\nLDAvis_data_filepath = os.path.join('.\/ldavis_prepared_'+str(num_topics))","3eb44e41":"LDAvis_data_filepath","e1bf55aa":"# # this is a bit time consuming - make the if statement True\n# # if you want to execute visualization prep yourself\nif 1 == 1:\n    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n    with open(LDAvis_data_filepath, 'wb') as f:\n        pickle.dump(LDAvis_prepared, f)","5600521a":"# load the pre-prepared pyLDAvis data from disk\nwith open(LDAvis_data_filepath, 'rb') as f:\n    LDAvis_prepared = pickle.load(f)\npyLDAvis.save_html(LDAvis_prepared, '.\/ldavis_prepared_'+ str(num_topics) +'.html')\nLDAvis_prepared","a92c137d":"stop_words.extend(['left', 'right']) # adding new stopwords\n\n     \ndata_wrl = Claim_description.ClaimDescription.values.tolist()\ndata_words_wrl = list(sent_to_words(data_wrl))\ndata_words_wrl = remove_stopwords(data_words_wrl)\n\n# Create Dictionary\nid2word = corpora.Dictionary(data_words_wrl)\n# Create Corpus\ntexts = data_words_wrl\n\n# Term Document Frequency\n# Converts a collection of words to  a list of (word_id, word_frequency) 2-tuples.\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View first description\nprint(data_words_wrl[0])\nprint(corpus[0])","deec4bc4":"# number of topics\nnum_topics = 15\n# Build LDA model\nlda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                       id2word=id2word,\n                                       num_topics=num_topics,\n                                           random_state=0)\n# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]\n\npyLDAvis.enable_notebook()\nLDAvis_data_filepath = os.path.join('.\/ldavis_prepared_'+str(num_topics))\n\n# # this is a bit time consuming - make the if statement True\n# # if you want to execute visualization prep yourself\nif 1 == 1:\n    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n    with open(LDAvis_data_filepath, 'wb') as f:\n        pickle.dump(LDAvis_prepared, f)\n\n# load the pre-prepared pyLDAvis data from disk\nwith open(LDAvis_data_filepath, 'rb') as f:\n    LDAvis_prepared = pickle.load(f)\npyLDAvis.save_html(LDAvis_prepared, '.\/ldavis_prepared_'+ str(num_topics) +'.html')\nLDAvis_prepared      \n","8c00ca57":"# Code from https:\/\/www.machinelearningplus.com\/nlp\/topic-modeling-gensim-python\/#18dominanttopicineachsentence\n# This may take several minutes to run, don't panick\n\ndef format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n    \n    # Get main topic in each document\n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data)\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n\n# Show\ndf_dominant_topic.head(20)","bf13cc82":"Most of this work (code and explanations) are from Shashank Kapadia, you can find his work [here](https:\/\/towardsdatascience.com\/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0)!\n\nI just want to show it applied to our subject!","cb1addd7":"## New try without left and right keywords\n\nIn the future, we can add them back in a new column of our dataset with a simple if statement.","96ec4ece":"## LDA model training with default parameters\nWe will start to build a model with 10 topics where each topic is a combination of keywords, and each keyword contributes a certain weight to the topic.","2a7862fc":"Next, we convert the tokenized object into a corpus and dictionary.\n> A Corpus in genism is a collection of documents (descriptions in our case).\n\n> A Dictionary encapsulates the mapping between normalized words and their integer ids.","172337c9":"## Analyzing LDA model results\nNow that we have a trained model let\u2019s visualize the topics for interpretability. To do so, we\u2019ll use a popular visualization package, pyLDAvis which is designed to help interactively with:\n\n1. Better understanding and interpreting individual topics, and\n2. Better understanding the relationships between the topics.\n\nFor (1), you can manually select each topic to view its top most frequent and\/or \u201crelevant\u201d terms, using different values of the \u03bb parameter. This can help when you\u2019re trying to assign a human interpretable name or \u201cmeaning\u201d to each topic.\n\nFor (2), exploring the Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics.","74305d71":"I would like to remind that most of this work (code and explanations) are from Shashank Kapadia go check his work [here](https:\/\/towardsdatascience.com\/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0)!\n\nI just wanted to show it applied to our subject!","53839b95":"## Outputting the dominant topic for each text","85b356f2":"## Prepare data for LDA Analysis\nWe are going to transform the textual data in a format that will serve as an input for training LDA model. LDA means Latent Dirichlet Allocation, it's a \"topic modeling technique that assumes each topic is a mixture over an underlying set of words and each document is a mixture of a set of topic probabilities\".\n\nWe start by tokenizing the text : \n> Splitting each description into smaller units (words and ponctuation) that are called tokens.\n\nThen, we remove the stopwords : \n> Stopwords are words that are so common that it would be useless to use it in our model. EX : Can, I, ponctuation ....\n","67f16976":"## Exploratory Analysis\nWe are going to make a word cloud to get a visual representation of most common words. It is an important step to verify if any preprocessing is necessary before training the model.","0f22e83c":"This work has been done following [Shashank Kapadia's tutorial](https:\/\/towardsdatascience.com\/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0) on towards data science.\n\nThe code is based on his work, I paramaetrized the code, modified comments so that it sticks to our dataset and added new comments to better understand the process from my data science level.","ffdd7d9b":"The information we are getting are already meaningfull. \nWe can make the hypothesis that two really different pain kind, like 'left hand' and 'back strain' will have a different impact on our target (ultimate claim cost).","85ea36df":"We already have quite good results without tweaking the parameters of the model!\n* Topic 1: Hand Injuries\n* Topic 2: Back Pains\n* Topic 3: Mixed topics\n* Topic 4: Mixed topics but looks like Topic 3 with right keyword\n* Topic 5: Back Pains\n* Topic 6: Mixed pain kind\n* Topic 7: Really mixed topic\n* Topic 8: Hand Injuries\n* Topic 9: Little category\n* Topic 10: Little category\n\nThose topics start to make sense, but we will have to tweak our parameters: \n* Lots of topics mainly contain right or left as first words, while it should be a topic on its own or be considered as stop words not to pollute our data. Since about 90% of the population is right-handed, it would be interesting to keep this in the data.\n* Lots of topics are made of several topics and should be divided.\n* In our dataset it would be interesting to separate the body member from the kind of pain, thus we will need more topics \n","c00d3292":"```\nNow right and left disappeared:\n['lifting', 'tyre', 'injury', 'right', 'arm', 'wrist', 'injury']\nis now:\n['lifting', 'tyre', 'injury', 'arm', 'wrist', 'injury']\n```\n","52a0cc3a":"This is already way better!"}}