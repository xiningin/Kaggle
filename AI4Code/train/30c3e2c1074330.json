{"cell_type":{"a029c46f":"code","c36a961e":"code","1ae253bb":"code","b6cdefda":"code","1f9542d4":"code","7b084dd4":"code","2ebb2785":"code","6a50e9c2":"code","bb0ebe59":"code","9593e117":"code","bd7010e0":"code","ec15c4db":"code","c3288474":"code","98805518":"code","4211426b":"code","263cbb76":"code","0468e1f4":"code","45ce5270":"code","85861a84":"code","0421a732":"code","34f9ee0e":"code","738056e0":"code","894e04ab":"code","a633f4c7":"code","97f59c60":"code","8f811009":"code","31d03dd0":"code","cc5c066a":"code","464bb1a5":"code","730d4c1f":"code","9faa4d10":"code","2c94b8e6":"code","f6bc344c":"code","0e2ac76c":"code","3e47d247":"code","481268c0":"code","2939f238":"code","668d071c":"code","49db33cd":"code","10beb133":"code","f833123c":"code","90f28b92":"code","91acf92c":"code","d6f699a4":"code","17868976":"code","624ac946":"code","6d166701":"code","57ce8bf5":"code","bc6bff95":"code","f62e9764":"code","51fd5d7d":"code","c634ff29":"code","9486bf53":"code","ab37cb8f":"code","89362346":"code","ca898915":"code","5cfb7476":"code","489270f7":"code","f58a3c9e":"code","01443b71":"code","73f83860":"code","7effcdcb":"code","2f802d08":"code","b554a5eb":"code","0e013f2e":"code","fdd561be":"code","3c003861":"code","f1a2c5ea":"code","5ed5eaf5":"code","f4ad0a71":"code","e208bd3b":"code","99c37c5d":"code","2a110049":"code","07c26c82":"code","f8ef49fa":"code","902c4e70":"code","22a06768":"code","3825faf1":"code","a065b6f5":"code","74197780":"code","646f954a":"code","c004f9f7":"code","daca0ae6":"code","35498f9b":"code","a165a1ea":"code","7b25e863":"code","75835d69":"code","e86b065b":"code","6346bd1c":"code","6a451e3d":"code","f351239b":"code","60e15e29":"code","8e3db84c":"code","d8f8d3d9":"code","b1f623ca":"code","c32857f4":"code","67783180":"code","a061fd81":"code","c0ae65d9":"code","59c12eae":"code","03623b87":"code","e8ae7432":"code","9bb158cd":"code","633f9bc8":"code","d914ac6d":"code","04a559d8":"code","563f3b22":"code","0e8f9730":"code","a899a76a":"code","c7240db9":"code","fc364aa9":"code","3c907dca":"code","00ca8073":"code","93c658da":"code","63df8077":"code","b623ed2e":"code","3395edc4":"code","08e85459":"code","8f651f1e":"code","00592b7e":"code","171931ca":"code","5dcece6b":"code","8d460229":"code","03e96ca7":"code","21cae1cd":"code","1afa6bcb":"code","6c823757":"code","d2916dac":"code","644a7af5":"code","6b726d9b":"code","a01657e2":"code","40b74da2":"code","069cecae":"code","7700d299":"code","f00db4b4":"code","5e3f3b13":"code","6bf0041c":"code","6bb43737":"code","b1b33fc2":"code","0eb83252":"code","5f96039c":"code","2bb89925":"code","f61b6847":"code","ed78f4d9":"code","1f2b3489":"code","6a95ee59":"code","26192428":"code","32f39f10":"code","53d8fd05":"code","36a45a6d":"code","17ef7257":"code","44a58f66":"code","4d13cfa7":"code","1f06b27f":"code","a69422af":"code","c568f28c":"code","fea96321":"code","288ddec4":"code","e4a786e3":"code","127d688c":"code","cd510fa9":"code","15a40852":"code","0140f5eb":"code","ecf6d98e":"code","236b8645":"code","81142485":"code","b6c22605":"code","688e24ab":"code","21ae241f":"code","ca197db9":"code","54d75034":"code","42cb3332":"code","07aec50a":"code","9b68ae57":"code","4f4fcf81":"code","04cd978a":"code","5421c6ef":"code","0764f6e2":"code","c75bbc3b":"code","4c28fd3c":"code","45c065c4":"markdown","bb4701fe":"markdown","6a9d6505":"markdown","fbfcd8f2":"markdown","800deb50":"markdown","644c512e":"markdown","0f47b6f9":"markdown","8e50c81c":"markdown","5842fcf3":"markdown","cd27817e":"markdown","fd3dff4c":"markdown","8c6f72bb":"markdown","1563f18c":"markdown","a40bbfb4":"markdown","1bd41e05":"markdown","c05ac817":"markdown","dfee8dfd":"markdown","14476823":"markdown","7f2acff6":"markdown","582ab533":"markdown","4c0feb89":"markdown","dd248088":"markdown","d19aa7bc":"markdown","ccc1ff30":"markdown","eb71de02":"markdown","4b805f1c":"markdown","e49e2494":"markdown","9b5b2efe":"markdown","bdbfdb3c":"markdown","878e797c":"markdown","f91d11e4":"markdown","bc413c11":"markdown","88f70987":"markdown","7cb18a0e":"markdown","a709090a":"markdown","492af65b":"markdown","900c3d90":"markdown","f5b00a51":"markdown","6f6093f4":"markdown","8d301ed0":"markdown","b0f71d47":"markdown"},"source":{"a029c46f":"# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))\n\n","c36a961e":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\n\n","1ae253bb":"\n# using the SQLite Table to read data.\nimport sqlite3\nshow_tables = \"select tbl_name from sqlite_master where type = 'table'\" \nconn = sqlite3.connect('..\/input\/database.sqlite') \npd.read_sql(show_tables,conn)","b6cdefda":"\n#filtering only positive and negative reviews i.e. \n# not taking into consideration those reviews with Score=3\nfiltered_data = pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score != 3\"\"\", conn) ","1f9542d4":"# Give reviews with Score>3 a positive rating, and reviews with a score<3 a negative rating.\ndef partition(x):\n    if x < 3:\n        return '0'\n    return '1'\n\n#changing reviews with score less than 3 to be positive and vice-versa\nactualScore = filtered_data['Score']\npositiveNegative = actualScore.map(partition) \nfiltered_data['Score'] = positiveNegative","7b084dd4":"#print(filtered_data.shape) #looking at the number of attributes and size of the data\nfiltered_data.head()","2ebb2785":"display= pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\nWHERE Score != 3 AND UserId=\"AR5J8UI46CURR\"\nORDER BY ProductID\n\"\"\", conn)\ndisplay.head()","6a50e9c2":"#Sorting data according to ProductId in ascending order\nsorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')","bb0ebe59":"#Deduplication of entries\nfinal=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\nfinal.shape","9593e117":"#Checking to see how much % of data still remains\n(final['Id'].size*1.0)\/(filtered_data['Id'].size*1.0)*100","bd7010e0":"display= pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\nWHERE Score != 3 AND Id=44737 OR Id=64422\nORDER BY ProductID\n\"\"\", conn)","ec15c4db":"final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]","c3288474":"#Before starting the next phase of preprocessing lets see the number of entries left\nprint(final.shape)\n\n#How many positive and negative reviews are present in our dataset?\nfinal['Score'].value_counts()","98805518":"# get 40k random reviews from the overall dataset for Brute and 20K for KD-tree\n\nfrom random import sample\nfinal_dataset_Bruteforce = final.ix[np.random.choice(final.index, 40000)]\nfinal_dataset_KDTREE = final.ix[np.random.choice(final.index, 20000)]","4211426b":"#Sorting data according to Time in ascending order\nKNN_DATASET_BRUTEFORCE=final_dataset_Bruteforce.sort_values('Time', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')","263cbb76":"#Sorting data according to Time in ascending order\nKNN_DATASET_KDTREE=final_dataset_KDTREE.sort_values('Time', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')","0468e1f4":"# find sentences containing HTML tags (DATASET FOR BRUTEFORCE)\nimport re\ni=0;\nfor sent in KNN_DATASET_BRUTEFORCE['Text'].values:\n    if (len(re.findall('<.*?>', sent))):\n        print(i)\n        print(sent)\n        break;\n    i += 1;","45ce5270":"# find sentences containing HTML tags (DATASET FOR KD_TREE)\nimport re\ni=0;\nfor sent in KNN_DATASET_KDTREE['Text'].values:\n    if (len(re.findall('<.*?>', sent))):\n        print(i)\n        print(sent)\n        break;\n    i += 1;","85861a84":"import nltk.corpus\nfrom nltk.corpus import stopwords","0421a732":"\nstop = set(stopwords.words('english')) #set of stopwords\nsno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer\n\ndef cleanhtml(sentence): #function to clean the word of any html-tags\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', sentence)\n    return cleantext\ndef cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]',r' ',cleaned)\n    return  cleaned\nprint(stop)\nprint('************************************')\nprint(sno.stem('tasty'))","34f9ee0e":"#Code for implementing step-by-step the checks mentioned in the pre-processing phase (BRUTE-FORCE_DATASET)\ni=0\nstr1=' '\nfinal_string=[]\nall_positive_words=[] # store words from +ve reviews here\nall_negative_words=[] # store words from -ve reviews here.\ns=''\nfor sent in KNN_DATASET_BRUTEFORCE['Text'].values:\n    filtered_sentence=[]\n    #print(sent);\n    sent=cleanhtml(sent) # remove HTMl tags\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n                if(cleaned_words.lower() not in stop):\n                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n                    filtered_sentence.append(s)\n                    if (KNN_DATASET_BRUTEFORCE['Score'].values)[i] == '1': \n                        all_positive_words.append(s) #list of all words used to describe positive reviews\n                    if(KNN_DATASET_BRUTEFORCE['Score'].values)[i] == '0':\n                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n                else:\n                    continue\n            else:\n                continue \n    #print(filtered_sentence)\n    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n    #print(\"***********************************************************************\")\n    \n    final_string.append(str1)\n    i+=1","738056e0":"#Code for implementing step-by-step the checks mentioned in the pre-processing phase (KD-TREE_DATASET)\ni=0\nstr1=' '\nfinal_string_KDTREE=[]\nall_positive_words=[] # store words from +ve reviews here\nall_negative_words=[] # store words from -ve reviews here.\ns=''\nfor sent in KNN_DATASET_KDTREE['Text'].values:\n    filtered_sentence=[]\n    #print(sent);\n    sent=cleanhtml(sent) # remove HTMl tags\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n                if(cleaned_words.lower() not in stop):\n                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n                    filtered_sentence.append(s)\n                    if (KNN_DATASET_KDTREE['Score'].values)[i] == '1': \n                        all_positive_words.append(s) #list of all words used to describe positive reviews\n                    if(KNN_DATASET_KDTREE['Score'].values)[i] == '0':\n                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n                else:\n                    continue\n            else:\n                continue \n    #print(filtered_sentence)\n    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n    #print(\"***********************************************************************\")\n    \n    final_string_KDTREE.append(str1)\n    i+=1","894e04ab":"KNN_DATASET_BRUTEFORCE['CleanedText']=final_string #adding a column of CleanedText which displays the data after pre-processing of the review \nKNN_DATASET_BRUTEFORCE['CleanedText']=KNN_DATASET_BRUTEFORCE['CleanedText'].str.decode(\"utf-8\")","a633f4c7":"KNN_DATASET_KDTREE['CleanedText']=final_string_KDTREE #adding a column of CleanedText which displays the data after pre-processing of the review \nKNN_DATASET_KDTREE['CleanedText']=KNN_DATASET_KDTREE['CleanedText'].str.decode(\"utf-8\")","97f59c60":"# ============================== loading libraries ===========================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import model_selection","8f811009":"#  data preprocessing\n\n# define column names\nnames = ['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n       'HelpfulnessDenominator', 'Time', 'Summary', 'Text','CleanedText']\n\n\n# create design matrix X and target vector y\nX_bf =  KNN_DATASET_BRUTEFORCE[names]\ny_bf = KNN_DATASET_BRUTEFORCE['Score']\n\nX_train_bf, X_test_bf, y_train_bf, y_test_bf = model_selection.train_test_split(X_bf, y_bf, test_size=0.3, random_state=0)\n","31d03dd0":"# Get the BoW vector for Train and Test data\n\ncount_vect = CountVectorizer() \n\nbow_bf = count_vect.fit(X_train_bf['CleanedText'].values)\n\nbow_train_bf = bow_bf.transform(X_train_bf['CleanedText'].values)\n\nbow_test_bf  = bow_bf.transform(X_test_bf['CleanedText'].values)","cc5c066a":"# ============================== loading libraries ===========================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import model_selection\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n# =============================================================================================","464bb1a5":"### KNN using Cross Validation (K=1) and computing acuracy and CM\nknn = KNeighborsClassifier(1)\nknn.fit(bow_train_bf, y_train_bf)\npred = knn.predict(bow_test_bf)\n","730d4c1f":"# Print Accuracy and Confusion Matrix\nacc = accuracy_score(y_test_bf, pred, normalize=True) * float(100)\nprint('\\n****Test accuracy for k = 1 is %d%%' % (acc))\n\nlabels = ['0','1']\ncm = confusion_matrix(y_test_bf, pred)\n\nprint(\"---------------------Plot of Confusion Matrix.....................\")\n\ndf_cm = pd.DataFrame(cm)\n\nimport seaborn as sn\nsn.set(font_scale=2) #for label size\n\nsn.heatmap(df_cm, annot=True, cmap='Oranges', annot_kws={\"size\": 20})# font size","9faa4d10":"# Print Training and Test Error rate (BF AND SIMPLE CV)\n\n\n    train_error_knn_BF_simple_cv = np.float128(1)- np.float128 (knn.score(bow_train_bf, y_train_bf))\n    \n    #Compute accuracy on the test set\n    test_error_knn_BF_simple_cv = np.float128(1)- np.float128(knn.score(bow_test_bf, y_test_bf))\n    \n    print('\\n****Training Error for k = 1 (simple CV) is %.2f%%' % (train_error_knn_BF_simple_cv))\n    \n    print('\\n****Test Error for k = 1 (10-FOLD CV) is %.2f%%' % (test_error_knn_BF_simple_cv))\n    \n    ","2c94b8e6":"#  Finding K in KNN using 10- Fold  Cross-Validation\n\nknn_BF_10FOLD_CV = KNeighborsClassifier(algorithm = 'brute')\nparameters = {\"n_neighbors\": np.arange(1, 5, 2),\n\t\"metric\": [\"euclidean\"]}\nclf = GridSearchCV(knn_BF_10FOLD_CV, parameters, cv=10)\nclf.fit(bow_train_bf, y_train_bf)\n","f6bc344c":"clf.best_params_\nclf.score(bow_test_bf,y_test_bf)\ny_pred = clf.best_estimator_.predict(bow_test_bf)","0e2ac76c":"#  plotting the  CONFUSION MATRIX\nlabels = ['0','1']\ncm = confusion_matrix(y_test_bf, y_pred, labels)\nprint(\"Computation over... Confusion Matrix is as follows .....................\")\ndf_cm = pd.DataFrame(cm)\n\nimport seaborn as sn\nsn.set(font_scale=2) #for label size\n\nsn.heatmap(df_cm, annot=True, cmap='Oranges', annot_kws={\"size\": 20})# font size\n","3e47d247":"# Getting the Training and Test Error rate (BF AND 10-fold CV)\n\n    train_error_knn_BF_10FOLD_cv = np.float(1) - np.float(clf.score(bow_train_bf, y_train_bf))\n    \n    #Compute accuracy on the test set\n    test_error_knn_BF_10FOLD_cv = 1- np.float(clf.score(bow_test_bf, y_test_bf))\n    \n  ","481268c0":"# Printing  the Training and Test Error rate (BF AND 10-FOLD CV) \nprint('\\n****Training Error for {0}'.format(clf.best_params_),'with 10-FOLD CV is %.3f%%'%(train_error_knn_BF_10FOLD_cv))\nprint('\\n****Test Error for {0}'.format(clf.best_params_), 'with 10-FOLD CV is %.3f%%'%(test_error_knn_BF_10FOLD_cv))","2939f238":"from prettytable import PrettyTable\ntablenew = PrettyTable()","668d071c":"# Display the Training and Test Errors \n\ntablenew.field_names = ([\"Model\", \"hyper parameter (k)\", \"train error\", \"test error\"])\n\ntablenew.add_row([\"BOW-KNN with BF and simple CV\", 1, train_error_knn_BF_simple_cv, test_error_knn_BF_simple_cv])\ntablenew.add_row([\"BOW -KNN_BF and 10 fold CV\", clf.best_params_, train_error_knn_BF_10FOLD_cv, test_error_knn_BF_10FOLD_cv])","49db33cd":"print (tablenew)","10beb133":"# ============================== data preprocessing ===========================================\n\n# define column names\nnames = ['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n       'HelpfulnessDenominator', 'Time', 'Summary', 'Text','CleanedText']\n\n\n# create design matrix X and target vector y\nX_kd =  KNN_DATASET_KDTREE[names]\ny_kd = KNN_DATASET_KDTREE['Score']\nX_train_kd, X_test_kd, y_train_kd, y_test_kd = model_selection.train_test_split(X_kd, y_kd, test_size=0.3, random_state=0)","f833123c":"# Get the BoW vector for Train and Test data (kd tree)\n\ncount_vect = CountVectorizer() \n\nbow_kd = count_vect.fit(X_train_kd['CleanedText'].values)\n\n","90f28b92":"bow_train_kd = bow_kd.transform(X_train_kd['CleanedText'].values)\n\nbow_test_kd  = bow_kd.transform(X_test_kd['CleanedText'].values)","91acf92c":"# Standardizing the data for KD-TREE\nfrom sklearn.preprocessing import StandardScaler\n\nstandardized_data_kd_train =StandardScaler(with_mean=False).fit_transform(bow_train_kd) \n\nstandardized_data_kd_test =StandardScaler(with_mean=False).fit(bow_train_kd).transform(bow_test_kd) ","d6f699a4":"# Reducing Dimensions using Truncated SVD method\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom scipy.sparse import csr_matrix\n\n\nstandardized_data_sparse_train = csr_matrix(standardized_data_kd_train)\n\nstandardized_data_sparse_test = csr_matrix(standardized_data_kd_test)\n\ntsvd = TruncatedSVD(n_components=10)\n\nstandardized_data_sparse_tsvd_train_kd = tsvd.fit(standardized_data_sparse_train).transform(standardized_data_sparse_train)\n\nstandardized_data_sparse_tsvd_test_kd = tsvd.fit(standardized_data_sparse_train).transform(standardized_data_sparse_test)\n\n","17868976":"#tsvd = TruncatedSVD(n_components=2, random_state=42)\nprint(tsvd.explained_variance_ratio_)","624ac946":"print('Original number of features:', standardized_data_sparse_train.shape[1])\nprint('Reduced number of features:', standardized_data_sparse_tsvd_train_kd.shape[1])","6d166701":"### KNN using Cross Validation (K=1) and computing acuracy and CM (using KDTREE Algo)\nknn_KDTREE = KNeighborsClassifier(1, algorithm ='kd_tree')\nknn_KDTREE.fit(standardized_data_sparse_tsvd_train_kd, y_train_kd)\npred_KDTREE = knn_KDTREE.predict(standardized_data_sparse_tsvd_test_kd)","57ce8bf5":"# Print Accuracy and Confusion Matrix\n\n\nacc = accuracy_score(y_test_kd, pred_KDTREE, normalize=True) * float(100)\nprint('\\n****Test accuracy for k = 1 is %d%%' % (acc))\n\nlabels = ['0','1']\ncm = confusion_matrix(y_test_kd, pred_KDTREE)\n\nprint(\"---------------------Plot of Confusion Matrix.....................\")\n\ndf_cm = pd.DataFrame(cm)\n\nimport seaborn as sn\nsn.set(font_scale=2) #for label size\n\nsn.heatmap(df_cm, annot=True, cmap='Oranges', annot_kws={\"size\": 20})# font size","bc6bff95":"# Getting the Training and Test Error rate (BF AND 10-fold CV)\n\ntrain_error_knn_KD_simple_cv = 1 - knn_KDTREE.score(standardized_data_sparse_tsvd_train_kd, y_train_kd)\n    \n#Compute accuracy on the test set\n    \ntest_error_knn_KD_simple_cv = 1- knn_KDTREE.score(standardized_data_sparse_tsvd_test_kd, y_test_kd)\n    \n  ","f62e9764":"# Printing  the Training and Test Error rate (KD AND simple CV) \n\nprint('\\n****Training Error for  KNN with KD and simple CV is %.3f%%'%(train_error_knn_KD_simple_cv))\n\nprint('\\n****Test Error for  for  KNN with KD and simple CV is %.3f%%'%(test_error_knn_KD_simple_cv))","51fd5d7d":"tablenew.add_row([\"BOW -KNN_KD and simple CV\", 1,train_error_knn_KD_simple_cv,  test_error_knn_KD_simple_cv])\n\n\nprint (tablenew)","c634ff29":"#  Finding K in KNN using 10- Fold  Cross-Validation\n\nknn_KDTREE_10FOLD = KNeighborsClassifier(algorithm = 'kd_tree')\nparameters = {\"n_neighbors\": np.arange(1, 21, 2),\n\t\"metric\": [\"euclidean\"]}\nclf_BOW_KDTREE_10FOLD = GridSearchCV(knn_KDTREE_10FOLD, parameters, cv=10)\nclf_BOW_KDTREE_10FOLD.fit(standardized_data_sparse_tsvd_train_kd, y_train_kd)","9486bf53":"clf_BOW_KDTREE_10FOLD.best_params_\nclf_BOW_KDTREE_10FOLD.score(standardized_data_sparse_tsvd_test_kd,y_test_kd)\ny_pred = clf_BOW_KDTREE_10FOLD.best_estimator_.predict(standardized_data_sparse_tsvd_test_kd)","ab37cb8f":"# plotting the  CONFUSION MATRIX\nlabels = ['0','1']\ncm = confusion_matrix(y_test_kd, y_pred, labels)\nprint(\"Plot of the Confusion Matrix.....................\")\ndf_cm = pd.DataFrame(cm)\n\nsn.set(font_scale=2) #for label size\nsn.heatmap(df_cm, annot=True, cmap='Oranges', annot_kws={\"size\": 20})# font size","89362346":"# Getting the Training and Test Error rate (BF AND 10-fold CV)\n\ntrain_error_knn_BOW_KD_10FOLD_cv = 1 - clf_BOW_KDTREE_10FOLD.score(standardized_data_sparse_tsvd_train_kd, y_train_kd)\n    \n#Compute accuracy on the test set\n\ntest_error_knn_BOW_KD_10FOLD_cv = 1- clf_BOW_KDTREE_10FOLD.score(standardized_data_sparse_tsvd_test_kd, y_test_kd)\n    \n# Printing  the Training and Test Error rate (BF AND 10-FOLD CV) \n\nprint('\\n****Training Error for {0}'.format(clf_BOW_KDTREE_10FOLD.best_params_),'with 10-FOLD CV is %.3f%%'%(train_error_knn_BOW_KD_10FOLD_cv))\n\nprint('\\n****Test Error for {0}'.format(clf_BOW_KDTREE_10FOLD.best_params_), 'with 10-FOLD CV is %.3f%%'%(test_error_knn_BOW_KD_10FOLD_cv))","ca898915":"tablenew.add_row([\"BOW -KNN_KD and 10-fold CV\", clf_BOW_KDTREE_10FOLD.best_params_,round(train_error_knn_BOW_KD_10FOLD_cv,3),  round(test_error_knn_BOW_KD_10FOLD_cv,3)])\n\n\nprint (tablenew)","5cfb7476":"tf_idf_vect_bf = TfidfVectorizer(ngram_range=(1,2))\n\ntfidf_bf = tf_idf_vect_bf.fit(X_train_bf['CleanedText'].values)\n\ntfidf_train_bf = tfidf_bf.transform(X_train_bf['CleanedText'].values)\n\ntfidf_test_bf  = tfidf_bf.transform(X_test_bf['CleanedText'].values)\n\n#final_tf_idf = tf_idf_vect.fit_transform(final['CleanedText'].values)\n\n\n#print(\"the type of count vectorizer (TRAIN DATA) \",type(tfidf_train_bf))\n#print(\"the shape of out text TFIDF vectorizer (TRAIN DATA) \",tfidf_train_bf.get_shape())\n#print(\"the number of unique words including both unigrams and bigrams(TRAIN DATA) \", tfidf_train_bf.get_shape()[1])","489270f7":"features_bf = tf_idf_vect_bf.get_feature_names()\nprint(\"some sample features(unique words in the corpus)\",features_bf[10000:10010])","f58a3c9e":"# source: https:\/\/buhrmann.github.io\/tfidf-analysis.html\ndef top_tfidf_feats(row, features, top_n=25):\n    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n    topn_ids = np.argsort(row)[::-1][:top_n]\n    top_feats = [(features[i], row[i]) for i in topn_ids]\n    df = pd.DataFrame(top_feats)\n    df.columns = ['feature', 'tfidf']\n    return df\n\ntop_tfidf_bf = top_tfidf_feats(tfidf_train_bf[1,:].toarray()[0],features_bf,25)","01443b71":"top_tfidf_bf","73f83860":"### KNN using Cross Validation (K=1) and computing acuracy and CM\nknn_TFIDF_BF_SIMPLE_CV = KNeighborsClassifier(1, algorithm = 'brute')\nknn_TFIDF_BF_SIMPLE_CV.fit(tfidf_train_bf, y_train_bf)\npred_TFIDF_BF_SIMPLE_CV = knn_TFIDF_BF_SIMPLE_CV.predict(tfidf_test_bf)","7effcdcb":"# Print Accuracy and Confusion Matrix\n\nAacc = accuracy_score(y_test_bf, pred_TFIDF_BF_SIMPLE_CV, normalize=True) * float(100)\nprint('\\n****Test accuracy for k = 1 is %d%%' % (acc))\n\nlabels = ['0','1']\ncm = confusion_matrix(y_test_bf, pred_TFIDF_BF_SIMPLE_CV)\n\nprint(\"---------------------Plot of Confusion Matrix.....................\")\n\ndf_cm = pd.DataFrame(cm)\n\n#sn.set(font_scale=2) #for label size\nsn.heatmap(df_cm, annot=True, cmap='Oranges', annot_kws={\"size\": 20})# font size","2f802d08":"# Print Training and Test Error rate FOR TFIDF(BF AND SIMPLE CV)\n\ntrain_error_knn_TFIDF_BF_simple_cv = 1- knn_TFIDF_BF_SIMPLE_CV.score(tfidf_train_bf, y_train_bf)\n    \n#Compute accuracy on the test set\n\ntest_error_knn_TFIDF_BF_simple_cv = 1- knn_TFIDF_BF_SIMPLE_CV.score(tfidf_test_bf, y_test_bf)\n\nprint('\\n****Training Error for k = 1 (simple CV) is %.2f%%' % (train_error_knn_TFIDF_BF_simple_cv))\n    \nprint('\\n****Test Error for k = 1 (10-FOLD CV) is %.2f%%' % (test_error_knn_TFIDF_BF_simple_cv))","b554a5eb":"tablenew.add_row([\"TFIDF-KNN with BF and simple CV\", 1, round(train_error_knn_TFIDF_BF_simple_cv,3), round(test_error_knn_TFIDF_BF_simple_cv,3)])\n\nprint (tablenew)","0e013f2e":"#  Finding K in KNN using 10- Fold  Cross-Validation\n\nknn_TFIDF_BF_10FOLD_CV = KNeighborsClassifier(algorithm = 'brute')\nparameters = {\"n_neighbors\": np.arange(1, 5, 2),\n\t\"metric\": [\"euclidean\"]}\nclf_TFIDF_BF_10FOLD_CV = GridSearchCV(knn_TFIDF_BF_10FOLD_CV, parameters, cv=10)\nclf_TFIDF_BF_10FOLD_CV.fit(tfidf_train_bf, y_train_bf)\n","fdd561be":"clf_TFIDF_BF_10FOLD_CV.best_params_\nclf_TFIDF_BF_10FOLD_CV.score(tfidf_test_bf,y_test_bf)\ny_pred_TFIDF_BF_10FOLD_CV = clf_TFIDF_BF_10FOLD_CV.best_estimator_.predict(tfidf_test_bf)","3c003861":" #Arriving at CONFUSION MATRIX\nlabels = ['0','1']\ncm = confusion_matrix(y_test_bf, y_pred_TFIDF_BF_10FOLD_CV, labels)\nprint(\"Computation over... time to plot Confusion Matrix.....................\")\ndf_cm = pd.DataFrame(cm)\nsn.heatmap(df_cm, annot=True, cmap='Oranges', annot_kws={\"size\": 20})# font size","f1a2c5ea":"# Getting the Training and Test Error rate (BF AND 10-fold CV)\n\ntrain_error_knn_TFIDF_BF_10FOLD_CV = 1 - clf_TFIDF_BF_10FOLD_CV.score(tfidf_train_bf, y_train_bf)\n    \n#Compute accuracy on the test set\n\ntest_error_knn_TFIDF_BF_10FOLD_CV = 1- clf_TFIDF_BF_10FOLD_CV.score(tfidf_test_bf, y_test_bf)\n    \n# Printing  the Training and Test Error rate (BF AND 10-FOLD CV) \n\nprint('\\n****Training Error for {0}'.format(clf_TFIDF_BF_10FOLD_CV.best_params_),'with 10-FOLD CV is %.3f%%'%(train_error_knn_TFIDF_BF_10FOLD_CV))\n\nprint('\\n****Test Error for {0}'.format(clf_TFIDF_BF_10FOLD_CV.best_params_), 'with 10-FOLD CV is %.3f%%'%(test_error_knn_TFIDF_BF_10FOLD_CV))\n\n","5ed5eaf5":"# Adding the entries to Pretty Table and diplaying the Cumulative list of entries appended so far...\n\ntablenew.add_row([\"TFIDF-KNN with BF and 10-fold CV\", clf_TFIDF_BF_10FOLD_CV.best_params_, train_error_knn_TFIDF_BF_10FOLD_CV, test_error_knn_TFIDF_BF_10FOLD_CV])\nprint (tablenew)","f4ad0a71":"# Get the TFIDF  vector for Train and Test data (kd tree)\n\ntf_idf_vect_kd = TfidfVectorizer(ngram_range=(1,2))\n\ntfidf_kd = tf_idf_vect_kd.fit(X_train_kd['CleanedText'].values)\n\ntfidf_train_kd = tfidf_kd.transform(X_train_kd['CleanedText'].values)\n\ntfidf_test_kd  = tfidf_kd.transform(X_test_kd['CleanedText'].values)\n\n#print(\"the type of count vectorizer (TRAIN DATA) \",type(tfidf_train_kd))\n#print(\"the shape of out text TFIDF vectorizer (TRAIN DATA) \",tfidf_train_kd.get_shape())\n#print(\"the number of unique words including both unigrams and bigrams(TRAIN DATA) \", tfidf_train_kd.get_shape()[1])","e208bd3b":"features_kd = tf_idf_vect_kd.get_feature_names()\n#print(\"some sample features(unique words in the corpus)\",features_kd[10000:10010])","99c37c5d":"top_tfidf_kd = top_tfidf_feats(tfidf_train_kd[1,:].toarray()[0],features_kd,25)","2a110049":"# Standardizing the data for KD-TREE\n\nstandardized_data_tfidf_kd_train =StandardScaler(with_mean=False).fit_transform(tfidf_train_kd) \n\nstandardized_data_tfidf_kd_test =StandardScaler(with_mean=False).fit(tfidf_train_kd).transform(tfidf_test_kd) ","07c26c82":"# TFIDF- Reducing Dimensions using Truncated SVD method\n\nstandardized_data_sparse_train_TFIDF = csr_matrix(standardized_data_tfidf_kd_train)\n\nstandardized_data_sparse_test_TFIDF = csr_matrix(standardized_data_tfidf_kd_test)\n\ntsvd = TruncatedSVD(n_components=4)\n\nstandardized_data_sparse_tsvd_TFIDF_train_kd = tsvd.fit(standardized_data_sparse_train_TFIDF).transform(standardized_data_sparse_train_TFIDF)\n\nstandardized_data_sparse_tsvd_TFIDF_test_kd = tsvd.fit(standardized_data_sparse_train_TFIDF).transform(standardized_data_sparse_test_TFIDF)","f8ef49fa":"#tsvd = TruncatedSVD(n_components=2, random_state=42)\nprint(tsvd.explained_variance_ratio_)","902c4e70":"### KNN using Cross Validation (K=1) and computing acuracy and CM (using KDTREE Algo)\nknn_KDTREE_TFIDF_Simple_CV = KNeighborsClassifier(1, algorithm ='kd_tree')\nknn_KDTREE_TFIDF_Simple_CV.fit(standardized_data_sparse_tsvd_TFIDF_train_kd, y_train_kd)\npred_KDTREE_TFIDF_Simple_CV = knn_KDTREE_TFIDF_Simple_CV.predict(standardized_data_sparse_tsvd_TFIDF_test_kd)","22a06768":"# Print Accuracy and Confusion Matrix\nacc = accuracy_score(y_test_kd, pred_KDTREE_TFIDF_Simple_CV, normalize=True) * float(100)\nprint('\\n****Test accuracy for k = 1 is %d%%' % (acc))\n\nlabels = ['0','1']\ncm = confusion_matrix(y_test_kd, pred_KDTREE_TFIDF_Simple_CV)\n\n","3825faf1":"# plot of CM\nprint(\"---------------------Plot of Confusion Matrix.....................\")\ndf_cm = pd.DataFrame(cm)\nsn.heatmap(df_cm, annot=True, cmap='Oranges', annot_kws={\"size\": 20})# font size","a065b6f5":"# Print Training and Test Error rate (BF AND SIMPLE CV)\n\ntrain_error_knn_TFIDF_KD_simple_cv = 1-knn_KDTREE_TFIDF_Simple_CV.score(standardized_data_sparse_tsvd_TFIDF_train_kd, y_train_kd)\n    \n#Compute accuracy on the test set\ntest_error_knn_TFIDF_KD_simple_cv =1-knn_KDTREE_TFIDF_Simple_CV.score(standardized_data_sparse_tsvd_TFIDF_test_kd, y_test_kd)\n    \nprint('\\n****Training Error for k = 1 (simple CV) is %.2f%%' % (train_error_knn_TFIDF_KD_simple_cv))\n    \nprint('\\n****Test Error for k = 1 (10-FOLD CV) is %.2f%%' % (test_error_knn_TFIDF_KD_simple_cv))\n","74197780":"\ntablenew.add_row([\"TFIDF-KNN with KD and simple CV\", 1, train_error_knn_TFIDF_KD_simple_cv, test_error_knn_TFIDF_KD_simple_cv])\n\n\nprint (tablenew)","646f954a":"#  Finding K in KNN using 10- Fold  Cross-Validation\n\nknn_KDTREE_10FOLD_TFIDF = KNeighborsClassifier(algorithm = 'kd_tree')\nparameters = {\"n_neighbors\": np.arange(1, 21, 2),\n\t\"metric\": [\"euclidean\"]}\nclf_KDTREE_10FOLD_TFIDF = GridSearchCV(knn_KDTREE_10FOLD_TFIDF, parameters, cv=10)\nclf_KDTREE_10FOLD_TFIDF.fit(standardized_data_sparse_tsvd_TFIDF_train_kd, y_train_kd)","c004f9f7":"clf_KDTREE_10FOLD_TFIDF.best_params_\nclf_KDTREE_10FOLD_TFIDF.score(standardized_data_sparse_tsvd_TFIDF_test_kd,y_test_kd)\ny_pred_KDTREE_10FOLD_TFIDF = clf_KDTREE_10FOLD_TFIDF.best_estimator_.predict(standardized_data_sparse_tsvd_TFIDF_test_kd)","daca0ae6":"# PLOT OF CONFUSION MATRIX\nlabels = ['0','1']\ncm = confusion_matrix(y_test_kd, y_pred_KDTREE_10FOLD_TFIDF, labels)\nprint(\"Computation over... time to plot Confusion Matrix.....................\")\ndf_cm = pd.DataFrame(cm)\nsn.heatmap(df_cm, annot=True, cmap='Oranges', annot_kws={\"size\": 20})# font size","35498f9b":"# Getting the Training and Test Error rate (BF AND 10-fold CV)\ntrain_error_TFIDF_KD_10FOLD_cv = 1- clf_KDTREE_10FOLD_TFIDF.score(standardized_data_sparse_tsvd_TFIDF_train_kd,y_train_kd)\n    \n#Compute accuracy on the test set\ntest_error_TFIDF_KD_10FOLD_cv = 1- clf_KDTREE_10FOLD_TFIDF.score(standardized_data_sparse_tsvd_TFIDF_test_kd,y_test_kd)","a165a1ea":"\ntablenew.add_row([\"TFIDF-KNN with KD and 10-fold CV\", clf_KDTREE_10FOLD_TFIDF.best_params_, train_error_TFIDF_KD_10FOLD_cv, test_error_TFIDF_KD_10FOLD_cv])\n\n\nprint (tablenew)","7b25e863":"i=0\nlist_of_sent_bf=[]\nfor sent in KNN_DATASET_BRUTEFORCE['CleanedText'].values:\n    list_of_sent_bf.append(sent.split())","75835d69":"print(KNN_DATASET_BRUTEFORCE['CleanedText'].values[0])\nprint(\"*****************************************************************\")\nprint(list_of_sent_bf[0])","e86b065b":"# min_count = 1 considers only words that occured atleast 1 times\nw2v_model_bf =Word2Vec(list_of_sent_bf,min_count=1,size=50, workers=4)","6346bd1c":"w2v_words_bf = list(w2v_model_bf.wv.vocab)\nprint(\"number of words that occured minimum 1 times \",len(w2v_words_bf))\nprint(\"sample words \", w2v_words_bf[0:50])","6a451e3d":"w2v_model_bf.wv.most_similar('tasti')","f351239b":"w2v_model_bf.wv.most_similar('like')","60e15e29":"sentence_vectors_bf = []; # the avg-w2v for each sentence\/review is stored in this list\n\nfor sent in list_of_sent_bf: # for each review\/sentence\n    sent_vec_bf = np.zeros(50) # initialize numpy vector of size 50  \n    cnt_words_bf =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        if word in w2v_words_bf:\n            vec = w2v_model_bf.wv[word]\n            sent_vec_bf += vec\n            cnt_words_bf += 1\n    if cnt_words_bf != 0:\n        sent_vec_bf \/= cnt_words_bf\n    sentence_vectors_bf.append(sent_vec_bf)\nprint(len(sentence_vectors_bf))\nprint(len(sentence_vectors_bf[0]))","8e3db84c":"sentence_vectors_bf[1]","d8f8d3d9":"w2v_model_bf.wv['tasti']","b1f623ca":"w2Vdf_bf= pd.DataFrame(sentence_vectors_bf)","c32857f4":"w2Vdf_bf.shape","67783180":"sentence_vectors_bf_vec= np.array(sentence_vectors_bf)","a061fd81":"avgword2vec_train_BF= sentence_vectors_bf_vec[0:28000,]","c0ae65d9":"avgword2vec_test_BF= sentence_vectors_bf_vec[28000:40001,]","59c12eae":"avgword2vec_train_BF.shape","03623b87":"avgword2vec_test_BF.shape","e8ae7432":"### KNN using Cross Validation (K=1) and computing acuracy and CM\nknn_avgw2v_bf_simple_cv = KNeighborsClassifier(1, algorithm = 'brute')\nknn_avgw2v_bf_simple_cv.fit(avgword2vec_train_BF, y_train_bf)\npred_avgw2v_bf_simple_cv = knn_avgw2v_bf_simple_cv.predict(avgword2vec_test_BF)","9bb158cd":"# Print Accuracy and Confusion Matrix\n\nacc = accuracy_score(y_test_bf, pred_avgw2v_bf_simple_cv, normalize=True) * float(100)\nprint('\\n****Test accuracy for k = 1 is %d%%' % (acc))\n\nlabels = ['0','1']\ncm = confusion_matrix(y_test_bf, pred_avgw2v_bf_simple_cv)\n\nprint(\"---------------------Plot of Confusion Matrix.....................\")\n\ndf_cm = pd.DataFrame(cm)\n\nsn.heatmap(df_cm, annot=True, cmap='Oranges', annot_kws={\"size\": 20})# font size","633f9bc8":"# Getting the Training and Test Error rate (BF AND simple CV)\n\ntrain_error_knn_avgword2vec_BF_10FOLD_cv = 1- knn_avgw2v_bf_simple_cv.score(avgword2vec_train_BF, y_train_bf)\n    \n#Compute accuracy on the test set\ntest_error_knn_avgword2vec_BF_10FOLD_cv = 1- knn_avgw2v_bf_simple_cv.score(avgword2vec_test_BF, y_test_bf)\n    \n  \n# Printing  the Training and Test Error rate (BF AND 10-FOLD CV) \n\nprint('\\n****Training Error for AVGW2VEC with simple CV is %.3f%%'%(train_error_knn_avgword2vec_BF_10FOLD_cv))\n\nprint('\\n****Test Error for AVGW2VEC with simple CV is %.3f%%'%(test_error_knn_avgword2vec_BF_10FOLD_cv))","d914ac6d":"\ntablenew.add_row([\"AVGWORD2VEC-KNN with BF and simple CV\",1, round(train_error_knn_avgword2vec_BF_10FOLD_cv,4), round(test_error_knn_avgword2vec_BF_10FOLD_cv,4)])\n\n\nprint (tablenew)","04a559d8":"tablenew.del_row(8)","563f3b22":"# ============================== loading libraries ===========================================\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import model_selection\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n# =============================================================================================","0e8f9730":"#  Finding K in KNN using 10- Fold  Cross-Validation\n\nknn_avgw2v_bf_10foldcv = KNeighborsClassifier(algorithm = 'brute')\nparameters = {\"n_neighbors\": np.arange(1, 5, 2),\n\t\"metric\": [\"euclidean\"]}\nclf_avgw2v_bf_10foldcv = GridSearchCV(knn_avgw2v_bf_10foldcv, parameters, cv=10)\nclf_avgw2v_bf_10foldcv.fit(avgword2vec_train_BF, y_train_bf)","a899a76a":"clf_avgw2v_bf_10foldcv.best_params_\nclf_avgw2v_bf_10foldcv.score(avgword2vec_test_BF,y_test_bf)\ny_pred_avgw2v_bf_10foldcv = clf_avgw2v_bf_10foldcv.best_estimator_.predict(avgword2vec_test_BF)","c7240db9":"# Arriving at CONFUSION MATRIX and plotting CM\nlabels = ['0','1']\ncm = confusion_matrix(y_test_bf, y_pred_avgw2v_bf_10foldcv, labels)\nprint(\"Computation over... time to plot Confusion Matrix.....................\")\n# Plot of CM\ndf_cm = pd.DataFrame(cm)\n\nsn.heatmap(df_cm, annot=True, cmap='Oranges', annot_kws={\"size\": 20})# font size\n","fc364aa9":"# Getting the Training and Test Error rate (BF AND 10-fold CV)\n\ntrain_error_knn_AVGWORD2VEC_BF_10FOLD_cv = 1- clf_avgw2v_bf_10foldcv.score(avgword2vec_train_BF,y_train_bf)\n    \n#Compute accuracy on the test set\ntest_error_knn_AVGWORD2VEC_BF_10FOLD_cv = 1-  clf_avgw2v_bf_10foldcv.score(avgword2vec_test_BF,y_test_bf)\n    \n# Printing  the Training and Test Error rate (BF AND 10-FOLD CV) \n\nprint('\\n****Training Error for {0}'.format(clf_avgw2v_bf_10foldcv.best_params_),'with 10-FOLD CV is %.3f%%'%(train_error_knn_AVGWORD2VEC_BF_10FOLD_cv))\n\nprint('\\n****Test Error for {0}'.format(clf_avgw2v_bf_10foldcv.best_params_), 'with 10-FOLD CV is %.3f%%'%(test_error_knn_AVGWORD2VEC_BF_10FOLD_cv))","3c907dca":"\ntablenew.add_row([\"AVGWORD2VEC-KNN with BF and 10-fold CV\", clf_avgw2v_bf_10foldcv.best_params_, train_error_knn_AVGWORD2VEC_BF_10FOLD_cv, test_error_knn_AVGWORD2VEC_BF_10FOLD_cv])\n\n\nprint (tablenew)","00ca8073":"i=0\nlist_of_sent_kd=[]\nfor sent in KNN_DATASET_KDTREE['CleanedText'].values:\n    list_of_sent_kd.append(sent.split())","93c658da":"#print(KNN_DATASET_KDTREE['CleanedText'].values[0])\n#print(\"*****************************************************************\")\n#print(list_of_sent_kd[0])","63df8077":"# min_count = 1 considers only words that occured atleast 1 times\nw2v_model_kd =Word2Vec(list_of_sent_kd,min_count=1,size=50, workers=4)","b623ed2e":"w2v_words_kd = list(w2v_model_kd.wv.vocab)\nprint(\"number of words that occured minimum 1 times \",len(w2v_words_kd))\nprint(\"sample words \", w2v_words_kd[0:50])","3395edc4":"#w2v_model_kd.wv.most_similar('tasti')","08e85459":"#w2v_model_bf.wv.most_similar('like')","8f651f1e":"sentence_vectors_kd = []; # the avg-w2v for each sentence\/review is stored in this list\n\nfor sent in list_of_sent_kd: # for each review\/sentence\n    sent_vec_kd = np.zeros(50) # initialize numpy vector of size 50  \n    cnt_words_kd =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        if word in w2v_words_kd:\n            vec = w2v_model_kd.wv[word]\n            sent_vec_kd += vec\n            cnt_words_kd += 1\n    if cnt_words_kd != 0:\n        sent_vec_kd \/= cnt_words_kd\n    sentence_vectors_kd.append(sent_vec_kd)\nprint(len(sentence_vectors_kd))\nprint(len(sentence_vectors_kd[0]))","00592b7e":"#sentence_vectors_kd[1]","171931ca":"#w2v_model_kd.wv['tasti']","5dcece6b":"\nw2Vdf_kd= pd.DataFrame(sentence_vectors_kd)","8d460229":"w2Vdf_kd.shape","03e96ca7":"sentence_vectors_kd_vec= np.array(sentence_vectors_kd)","21cae1cd":"avgword2vec_train_KD= sentence_vectors_kd_vec[0:14000,]","1afa6bcb":"avgword2vec_test_KD= sentence_vectors_kd_vec[14000:20001,]","6c823757":"avgword2vec_train_KD.shape\n","d2916dac":"avgword2vec_test_KD.shape","644a7af5":"# Standardizing the data for KD-TREE\n#from sklearn.preprocessing import StandardScaler\n\nstandardized_data_avgword2vec_kd_train =StandardScaler(with_mean=False).fit_transform(avgword2vec_train_KD) \n\nstandardized_data_avgword2vec_kd_test =StandardScaler(with_mean=False).fit(avgword2vec_train_KD).transform(avgword2vec_test_KD) ","6b726d9b":"standardized_data_avgword2vec_kd_train.shape","a01657e2":"##  ABGWORD2VEC- Reducing Dimensions using Truncated SVD method\n\n#from sklearn.decomposition import TruncatedSVD\n#from scipy.sparse import csr_matrix\n\n\nstandardized_data_sparse_train_WORD2VEC = csr_matrix(standardized_data_avgword2vec_kd_train)\n\nstandardized_data_sparse_test_WORD2VEC = csr_matrix(standardized_data_avgword2vec_kd_test)\n\ntsvd_WORD2VEC = TruncatedSVD(n_components=12)\n\n\nstandardized_data_sparse_tsvd_WORD2VEC_train_kd = tsvd_WORD2VEC.fit(standardized_data_sparse_train_WORD2VEC).transform(standardized_data_sparse_train_WORD2VEC)\n\nstandardized_data_sparse_tsvd_WORD2VEC_test_kd = tsvd_WORD2VEC.fit(standardized_data_sparse_train_WORD2VEC).transform(standardized_data_sparse_test_WORD2VEC)","40b74da2":"print(tsvd_WORD2VEC.explained_variance_ratio_)","069cecae":"\n### KNN using Cross Validation (K=1) and computing acuracy and CM\nknn_avgw2v_kd_simple_cv = KNeighborsClassifier(1, algorithm = 'kd_tree')\nknn_avgw2v_kd_simple_cv.fit(standardized_data_sparse_tsvd_WORD2VEC_train_kd, y_train_kd)\npred_avgw2v_kd_simple_cv = knn_avgw2v_kd_simple_cv.predict(standardized_data_sparse_tsvd_WORD2VEC_test_kd)\n","7700d299":"# Print Confusion Matrix\n\n#acc = accuracy_score(y_test_kd, pred_avgw2v_kd_simple_cv, normalize=True) * float(100)\n#print('\\n****Test accuracy for k = 1 is %d%%' % (acc))\n\nlabels = ['0','1']\ncm = confusion_matrix(y_test_kd, pred_avgw2v_kd_simple_cv)\n\nprint(\"---------------------Plot of Confusion Matrix.....................\")\n\ndf_cm = pd.DataFrame(cm)\n\nsn.heatmap(df_cm, annot=True, cmap='Oranges', annot_kws={\"size\": 20})# font size","f00db4b4":"# Getting the Training and Test Error rate (KD AND simple CV)\n\ntrain_error_knn_AVGWORD2VEC_KD_simple_cv = 1- knn_avgw2v_kd_simple_cv.score(standardized_data_sparse_tsvd_WORD2VEC_train_kd, y_train_kd)\n    \n#Compute accuracy on the test set\ntest_error_knn_AVGWORD2VEC_KD_simple_cv = 1- knn_avgw2v_kd_simple_cv.score(standardized_data_sparse_tsvd_WORD2VEC_test_kd, y_test_kd)\n    \n  \n# Printing  the Training and Test Error rate (BF AND 10-FOLD CV) \n\nprint('\\n****Training Error for AVGWORD2VEC with simple CV is %.3f%%'%(train_error_knn_AVGWORD2VEC_KD_simple_cv))\n\nprint('\\n****Test Error for AVGWORD2VEC with simple CV is %.3f%%'%(test_error_knn_AVGWORD2VEC_KD_simple_cv))","5e3f3b13":"\ntablenew.add_row([\"AVGWORD2VEC-KNN with KD and simple CV\", 1, train_error_knn_AVGWORD2VEC_KD_simple_cv, test_error_knn_AVGWORD2VEC_KD_simple_cv])\n\n\nprint (tablenew)","6bf0041c":"#  Finding K in KNN using 10- Fold  Cross-Validation\n\nknn_avgw2v_kd_10foldcv = KNeighborsClassifier(algorithm = 'brute')\nparameters = {\"n_neighbors\": np.arange(1, 5, 2),\n\t\"metric\": [\"euclidean\"]}\nclf_avgw2v_kd_10foldcv = GridSearchCV(knn_avgw2v_kd_10foldcv, parameters, cv=10)\nclf_avgw2v_kd_10foldcv.fit(standardized_data_sparse_tsvd_WORD2VEC_train_kd, y_train_kd)","6bb43737":"clf_avgw2v_kd_10foldcv.best_params_\nclf_avgw2v_kd_10foldcv.score(standardized_data_sparse_tsvd_WORD2VEC_test_kd,y_test_kd)\ny_pred_avgw2v_kd_10foldcv = clf_avgw2v_kd_10foldcv.best_estimator_.predict(standardized_data_sparse_tsvd_WORD2VEC_test_kd)\n","b1b33fc2":"# Arriving at CONFUSION MATRIX and plotting CM\n\nlabels = ['0','1']\ncm = confusion_matrix(y_test_kd, y_pred_avgw2v_kd_10foldcv, labels)\nprint(\"Computation over... time to plot Confusion Matrix.....................\")\n# Plot of CM\ndf_cm = pd.DataFrame(cm)\nsn.heatmap(df_cm, annot=True, cmap='Oranges', annot_kws={\"size\": 20})# font size\n","0eb83252":"# Getting the Training and Test Error rate (KD AND 10-fold CV)\n\ntrain_error_knn_WORD2VEC_KD_10FOLD_cv = 1- clf_avgw2v_kd_10foldcv.score(standardized_data_sparse_tsvd_WORD2VEC_train_kd, y_train_kd)\n    \n    #Compute accuracy on the test set\ntest_error_knn_WORD2VEC_KD_10FOLD_cv = 1-clf_avgw2v_kd_10foldcv.score(standardized_data_sparse_tsvd_WORD2VEC_test_kd, y_test_kd)\n    \n  \n# Printing  the Training and Test Error rate (BF AND 10-FOLD CV) \n\nprint('\\n****Training Error for {0}'.format(clf_avgw2v_kd_10foldcv.best_params_),'with 10-FOLD CV is %.3f%%'%(train_error_knn_WORD2VEC_KD_10FOLD_cv))\n\nprint('\\n****Test Error for {0}'.format(clf_avgw2v_kd_10foldcv.best_params_), 'with 10-FOLD CV is %.3f%%'%(test_error_knn_WORD2VEC_KD_10FOLD_cv))","5f96039c":"\ntablenew.add_row([\"AVGWORD2VEC-KNN with KD and 10-fold CV\", clf_avgw2v_kd_10foldcv.best_params_, train_error_knn_WORD2VEC_KD_10FOLD_cv, test_error_knn_WORD2VEC_KD_10FOLD_cv])\nprint (tablenew)","2bb89925":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os","f61b6847":"tf_idf_vect_bf = TfidfVectorizer()\n\ntfidf_bf = tf_idf_vect_bf.fit(X_bf['CleanedText'].values)\n\ntfidf_train_bf = tfidf_bf.transform(X_train_bf['CleanedText'].values)\n\ntfidf_test_bf  = tfidf_bf.transform(X_test_bf['CleanedText'].values)\n\n#tf_idf_matrix_bf = tf_idf_vect_bf.fit_transform(X_train_bf['CleanedText'].values)\n\n#words_array_bf= tf_idf_matrix_bf.toarray()\n\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(tf_idf_vect_bf.get_feature_names(), list(tf_idf_vect_bf.idf_)))\n\n\n","ed78f4d9":"from tqdm import tqdm","1f2b3489":"# Compute the TFIDF-Word2Vec vector\n\ntfidf_sent_vectors_WEIGHTED_BF = []; # the tfidf-w2v for each sentence\/review is stored in this list\n\nrow=0;\n\nfor sent in tqdm(list_of_sent_bf): # for each review\/sentence \n    sent_vec_bf = np.zeros(50) # as word vectors are of zero length\n    weight_sum_bf =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        if word in w2v_words_bf:\n            vec = w2v_model_bf.wv[word]\n            # obtain the tf_idfidf of a word in a sentence\/review\n            \n            # --------------->   tf_idf = tfidf_all[row, tfidf_feat.index(word)]\n            tf_idf = dictionary[word]*sent.count(word)\n            sent_vec_bf += (vec * tf_idf)\n            weight_sum_bf += tf_idf\n    if weight_sum_bf != 0:\n        sent_vec_bf \/= weight_sum_bf\n    tfidf_sent_vectors_WEIGHTED_BF.append(sent_vec_bf)\n    row += 1","6a95ee59":"tfidf_w2vec_BF= pd.DataFrame(tfidf_sent_vectors_WEIGHTED_BF)","26192428":"tfidf_w2vec_BF.shape","32f39f10":"tfidf_w2vec_BF_vec= np.array(tfidf_w2vec_BF)","53d8fd05":"tfidf_w2vec_train_BF= tfidf_w2vec_BF_vec[0:28000,]","36a45a6d":"tfidf_w2vec_test_BF= tfidf_w2vec_BF_vec[28000:40001,]","17ef7257":"tfidf_w2vec_train_BF.shape","44a58f66":"tfidf_w2vec_test_BF.shape","4d13cfa7":"### KNN using Cross Validation (K=1) and computing acuracy and CM\nknn_tfidfw2v_bf_simple_cv = KNeighborsClassifier(1, algorithm = 'brute')\nknn_tfidfw2v_bf_simple_cv.fit(tfidf_w2vec_train_BF, y_train_bf)\npred_tfidfw2v_bf_simple_cv = knn_tfidfw2v_bf_simple_cv.predict(tfidf_w2vec_test_BF)","1f06b27f":"# Print Accuracy and Confusion Matrix\n\nacc = accuracy_score(y_test_bf, pred_tfidfw2v_bf_simple_cv, normalize=True) * float(100)\nprint('\\n****Test accuracy for k = 1 is %d%%' % (acc))\n\nlabels = ['0','1']\ncm = confusion_matrix(y_test_bf, pred_tfidfw2v_bf_simple_cv)\n\nprint(\"---------------------Plot of Confusion Matrix.....................\")\n\ndf_cm = pd.DataFrame(cm)\n\nsn.heatmap(df_cm, annot=True, cmap='Oranges', annot_kws={\"size\": 20})# font size","a69422af":"# Print Training and Test Error rate (BF AND SIMPLE CV)\n\ntrain_error_knn_tfidf_w2vec_BF_simple_cv = 1- knn_tfidfw2v_bf_simple_cv.score(tfidf_w2vec_train_BF, y_train_bf)\n    \n    #Compute accuracy on the test set\ntest_error_knn_tfidf_w2vec_BF_simple_cv = 1 - knn_tfidfw2v_bf_simple_cv.score(tfidf_w2vec_test_BF, y_test_bf)\n    \nprint('\\n****Training Error for k = 1 (simple CV) is %.2f%%' % (train_error_knn_tfidf_w2vec_BF_simple_cv))\n    \nprint('\\n****Test Error for k = 1 (simple CV) is %.2f%%' % (test_error_knn_tfidf_w2vec_BF_simple_cv))\n","c568f28c":"\ntablenew.add_row([\"TFIDF-W2V-KNN with BF and simple CV\", 1, train_error_knn_tfidf_w2vec_BF_simple_cv, test_error_knn_tfidf_w2vec_BF_simple_cv])\n\nprint (tablenew)","fea96321":"#  Finding K in KNN using 10- Fold  Cross-Validation\nknn_tfidfw2v_bf_10foldcv = KNeighborsClassifier(algorithm = 'brute')\nparameters = {\"n_neighbors\": np.arange(1,5, 2),\n\t\"metric\": [\"euclidean\"]}\nclf_tfidfw2v_bf_10foldcv = GridSearchCV(knn_tfidfw2v_bf_10foldcv, parameters, cv=10)\nclf_tfidfw2v_bf_10foldcv.fit(tfidf_w2vec_train_BF, y_train_bf)","288ddec4":"clf_tfidfw2v_bf_10foldcv.best_params_\nclf_tfidfw2v_bf_10foldcv.score(tfidf_w2vec_test_BF,y_test_bf)\ny_pred_tfidfw2v_bf_10foldcv = clf_tfidfw2v_bf_10foldcv.best_estimator_.predict(tfidf_w2vec_test_BF)","e4a786e3":"# Arriving at CONFUSION MATRIX and plotting CM\nlabels = ['0','1']\ncm = confusion_matrix(y_test_bf, y_pred_tfidfw2v_bf_10foldcv, labels)\nprint(\"Computation over... time to plot Confusion Matrix.....................\")\n# Plot of CM\ndf_cm = pd.DataFrame(cm)\n\nsn.heatmap(df_cm, annot=True, cmap='Oranges', annot_kws={\"size\": 20})# font size","127d688c":"# Getting the Training and Test Error rate (BF AND 10-fold CV)\n\ntrain_error_knn_tfidf_w2vec_BF_10FOLD_cv = 1 -clf_tfidfw2v_bf_10foldcv.score(tfidf_w2vec_train_BF, y_train_bf)\n    \n    #Compute accuracy on the test set\ntest_error_knn_tfidf_w2vec_BF_10FOLD_cv = 1- clf_tfidfw2v_bf_10foldcv.score(tfidf_w2vec_test_BF, y_test_bf)\n    \n  \n# Printing  the Training and Test Error rate (BF AND 10-FOLD CV) \n\nprint('\\n****Training Error for {0}'.format(clf_tfidfw2v_bf_10foldcv.best_params_),'with 10-FOLD CV is %.3f%%'%(train_error_knn_tfidf_w2vec_BF_10FOLD_cv))\n\nprint('\\n****Test Error for {0}'.format(clf_tfidfw2v_bf_10foldcv.best_params_), 'with 10-FOLD CV is %.3f%%'%(test_error_knn_tfidf_w2vec_BF_10FOLD_cv))\n","cd510fa9":"tablenew.add_row([\"TFIDF-W2V-KNN with BF and 10-fold CV\", clf_tfidfw2v_bf_10foldcv.best_params_, train_error_knn_tfidf_w2vec_BF_10FOLD_cv, test_error_knn_tfidf_w2vec_BF_10FOLD_cv])\nprint (tablenew)","15a40852":"tf_idf_w2v_vect_kd = TfidfVectorizer()\n\ntfidf_w2v_kd = tf_idf_w2v_vect_kd.fit(X_kd['CleanedText'].values)\n\ntfidf_w2v_train_kd = tfidf_w2v_kd.transform(X_train_kd['CleanedText'].values)\n\ntfidf_w2v_test_kd  = tfidf_w2v_kd.transform(X_test_kd['CleanedText'].values)\n\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary_kd = dict(zip(tf_idf_w2v_vect_kd.get_feature_names(), list(tf_idf_w2v_vect_kd.idf_)))\n\n\n","0140f5eb":"# Compute the TFIDF-Word2Vec vector  (KD-TREE)\n\ntfidf_sent_vectors_WEIGHTED_kd = []; # the tfidf-w2v for each sentence\/review is stored in this list\n\nrow=0;\n\nfor sent in tqdm(list_of_sent_kd): # for each review\/sentence \n    sent_vec_kd = np.zeros(50) # as word vectors are of zero length\n    weight_sum_kd =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        if word in w2v_words_kd:\n            vec = w2v_model_kd.wv[word]\n            # obtain the tf_idfidf of a word in a sentence\/review\n            \n            # --------------->   tf_idf = tfidf_all[row, tfidf_feat.index(word)]\n            tf_idf = dictionary_kd[word]*sent.count(word)\n            sent_vec_kd += (vec * tf_idf)\n            weight_sum_kd += tf_idf\n    if weight_sum_kd != 0:\n        sent_vec_kd \/= weight_sum_kd\n    tfidf_sent_vectors_WEIGHTED_kd.append(sent_vec_kd)\n    row += 1","ecf6d98e":"tfidf_w2vec_kd= pd.DataFrame(tfidf_sent_vectors_WEIGHTED_kd)","236b8645":"tfidf_w2vec_vectors_kd = np.array(tfidf_w2vec_kd)","81142485":"tfidf_w2vec_train_KD= tfidf_w2vec_vectors_kd[0:14000,]","b6c22605":"tfidf_w2vec_test_KD= tfidf_w2vec_vectors_kd[14000:20001,]","688e24ab":"# Standardizing the data for KD-TREE\n#from sklearn.preprocessing import StandardScaler\n\nstandardized_data_tfidfw2v_kd_train =StandardScaler(with_mean=False).fit_transform(tfidf_w2vec_train_KD) \n\nstandardized_data_tfidfw2v_kd_test =StandardScaler(with_mean=False).fit(tfidf_w2vec_train_KD).transform(tfidf_w2vec_test_KD) ","21ae241f":"##  TFIDF-W2V - Reducing Dimensions using Truncated SVD method\n\n#from sklearn.decomposition import TruncatedSVD\n#from scipy.sparse import csr_matrix\n\nstandardized_data_sparse_train_TFIDFW2V = csr_matrix(standardized_data_tfidfw2v_kd_train)\n\nstandardized_data_sparse_test_TFIDFW2V = csr_matrix(standardized_data_tfidfw2v_kd_test)\n\ntsvd_TFIDFW2V = TruncatedSVD(n_components=12)\n\n\nstandardized_data_sparse_tsvd_TFIDFW2V_train_kd = tsvd_TFIDFW2V.fit(standardized_data_sparse_train_TFIDFW2V).transform(standardized_data_sparse_train_TFIDFW2V)\n\nstandardized_data_sparse_tsvd_TFIDFW2V_test_kd = tsvd_TFIDFW2V.fit(standardized_data_sparse_train_TFIDFW2V).transform(standardized_data_sparse_test_TFIDFW2V)","ca197db9":"print(tsvd_TFIDFW2V.explained_variance_ratio_)","54d75034":"### KNN using Cross Validation (K=1) and computing acuracy and CM\n\nknn_tfidfw2v_kd_simple_cv = KNeighborsClassifier(1, algorithm = 'kd_tree')\nknn_tfidfw2v_kd_simple_cv.fit(standardized_data_sparse_tsvd_TFIDFW2V_train_kd, y_train_kd)\npred_tfidfw2v_kd_simple_cv = knn_tfidfw2v_kd_simple_cv.predict(standardized_data_sparse_tsvd_TFIDFW2V_test_kd)","42cb3332":"# Print Accuracy and Confusion Matrix\n\nacc = accuracy_score(y_test_kd, pred_tfidfw2v_kd_simple_cv, normalize=True) * float(100)\nprint('\\n****Test accuracy for k = 1 is %d%%' % (acc))\n\nlabels = ['0','1']\ncm = confusion_matrix(y_test_kd, pred_tfidfw2v_kd_simple_cv)\n\nprint(\"---------------------Plot of Confusion Matrix.....................\")\n\ndf_cm = pd.DataFrame(cm)\n\nsn.heatmap(df_cm, annot=True, cmap='Oranges', annot_kws={\"size\": 20})# font size","07aec50a":"# Getting the Training and Test Error rate (KD AND 10-fold CV)\n\ntrain_error_knn_TFIDF_W2V_KD_simple_cv = 1 - knn_tfidfw2v_kd_simple_cv.score(standardized_data_sparse_tsvd_TFIDFW2V_train_kd, y_train_kd)\n    \n#Compute accuracy on the test set\ntest_error_knn_TFIDF_W2V_KD_simple_cv = 1- knn_tfidfw2v_kd_simple_cv.score(standardized_data_sparse_tsvd_TFIDFW2V_test_kd, y_test_kd)\n    \n# Printing  the Training and Test Error rate (BF AND 10-FOLD CV) \n\nprint('\\n****Training Error for TFDIF-W2V with simple CV is %.3f%%'%(train_error_knn_TFIDF_W2V_KD_simple_cv))\n\nprint('\\n****Test Error for  for TFDIF-W2V with simple CV is is %.3f%%'%(test_error_knn_TFIDF_W2V_KD_simple_cv))","9b68ae57":"tablenew.add_row([\"TFIDF-W2V-KNN with KD and simple CV\", 1, train_error_knn_TFIDF_W2V_KD_simple_cv, test_error_knn_TFIDF_W2V_KD_simple_cv])\n\nprint (tablenew)","4f4fcf81":"#  Finding K in KNN using 10- Fold  Cross-Validation\n\nknn_TFIDFW2V_kd_10foldcv = KNeighborsClassifier(algorithm = 'kd_tree')\nparameters = {\"n_neighbors\": np.arange(1, 5, 2),\n\t\"metric\": [\"euclidean\"]}\nclf_TFIDFW2V_kd_10foldcv = GridSearchCV(knn_TFIDFW2V_kd_10foldcv, parameters, cv=10)\n","04cd978a":"clf_TFIDFW2V_kd_10foldcv.fit(standardized_data_sparse_tsvd_TFIDFW2V_train_kd, y_train_kd)","5421c6ef":"clf_TFIDFW2V_kd_10foldcv.best_params_\nclf_TFIDFW2V_kd_10foldcv.score(standardized_data_sparse_tsvd_TFIDFW2V_test_kd,y_test_kd)\ny_pred_TFIDFW2V_kd_10foldcv = clf_TFIDFW2V_kd_10foldcv.best_estimator_.predict(standardized_data_sparse_tsvd_TFIDFW2V_test_kd)\n","0764f6e2":"# Arriving at CONFUSION MATRIX and plotting CM\n\nlabels = ['0','1']\ncm = confusion_matrix(y_test_kd, y_pred_TFIDFW2V_kd_10foldcv, labels)\nprint(\"Computation over... time to plot Confusion Matrix.....................\")\n# Plot of CM\ndf_cm = pd.DataFrame(cm)\n\nsn.heatmap(df_cm, annot=True, cmap='Oranges', annot_kws={\"size\": 20})# font size","c75bbc3b":"# Getting the Training and Test Error rate (KD AND 10-fold CV)\n\ntrain_error_knn_TFIDF_W2V_KD_10fold_cv = 1 - clf_TFIDFW2V_kd_10foldcv.score(standardized_data_sparse_tsvd_TFIDFW2V_train_kd, y_train_kd)\n    \n#Compute accuracy on the test set\ntest_error_knn_TFIDF_W2V_KD_10fold_cv = 1 - clf_TFIDFW2V_kd_10foldcv.score(standardized_data_sparse_tsvd_TFIDFW2V_test_kd, y_test_kd)\n    \n# Printing  the Training and Test Error rate (BF AND 10-FOLD CV) \n\nprint('\\n****Training Error for {0}'.format(clf_TFIDFW2V_kd_10foldcv.best_params_),'with 10-FOLD CV is %.3f%%'%(train_error_knn_TFIDF_W2V_KD_10fold_cv))\n\nprint('\\n****Test Error for {0}'.format(clf_TFIDFW2V_kd_10foldcv.best_params_), 'with 10-FOLD CV is %.3f%%'%(test_error_knn_TFIDF_W2V_KD_10fold_cv))","4c28fd3c":"tablenew.add_row([\"TFIDF-W2V-KNN with KD and 10-fold CV\", clf_TFIDFW2V_kd_10foldcv.best_params_, train_error_knn_TFIDF_W2V_KD_10fold_cv, test_error_knn_TFIDF_W2V_KD_10fold_cv])\n\nprint (tablenew)","45c065c4":"* # ASSIGNMENT- PART 1: KNN  CLASSIFIER  ON BOW  VECTOR","bb4701fe":"# Step 2)** KNN Classifier for TFIDF (using BRUTE force algo and Simple CV)**\n","6a9d6505":"## ** Here we will take N_COMPONENTS = 12, **since beyond 12, variance explained is very minimal%","fbfcd8f2":"Here we will take N_COMPONENTS = 10, **since beyond 10, variance ratio drops by 50%**","800deb50":"# Step 1) computing the TF-IDF  vector (for BF algo)","644c512e":"# Assignment - part 4: KNN on TFIDF-W2V  (BRUTE-FORCE)","0f47b6f9":"# Step 3) KNN Classifier for TFIDF-W2V (using Brute-force and 10-FOLD CV)","8e50c81c":"# Step 2) KNN Classifier for TFIDF-Word2Vec ( BRUTE force and Simple CV)","5842fcf3":"* # Step1 )  Arriving at TFIDF-W2V  vector  (BRUTEFORE)","cd27817e":"## Text Preprocessing: Stemming, stop-word removal and Lemmatization.\n\nNow that we have fi****nished deduplication our data requires some preprocessing before we go on further with analysis and making the prediction model.\n\nHence in the Preprocessing phase we do the following in the order below:-\n\n1. Begin by removing the html tags\n2. Remove any punctuations or limited set of special characters like , or . or # etc.\n3. Check if the word is made up of english letters and is not alpha-numeric\n4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n5. Convert the word to lowercase\n6. Remove Stopwords\n7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)<br>\n\nAfter which we collect the words used to describe positive and negative reviews","fd3dff4c":"# Step 1) **computing** the Word2vec  model  (BF algo)","8c6f72bb":"> # Step 3)** KNN Classifier for TFIDF (using BRUTE force algo and 10-FOLD CV)**\n","1563f18c":"# Step 9) KNN Classifier for WORD2VEC  (using KD-TREE algo and Simple CV)","a40bbfb4":"# Step 6) **computing** the Word2vec  model  (KD-TREE)","1bd41e05":"* 1. # Part 2: KNN classifier on TF-IDF vector  for AMZN reviews","c05ac817":"# Step 2) compute Avg W2V (dataset is 40k reviews since I am running in to memory error for data > 40k reviews)  (BRUTEFORCE )****","dfee8dfd":"# Step 7) compute Avg W2V    (KD-TREE)     \n","14476823":"> > # Step 6) KNN Classifier for BOW (using KD-TREE algo and 10-FOLD CV)**\n","7f2acff6":"# Step 5) KNN Classifier for BOW (using KD-TREE algo and Simple CV)**\n","582ab533":"# Step 5) KNN Classifier for TFIDF-W2V  (using KD-TREE algo and Simple CV)","4c0feb89":"# Step8)  Arrive at the  Avg W2V dataframe (Test and Train) (KD-TREE)","dd248088":"# Step 6) KNN Classifier for TFIDF-W2V  (using KD-TREE algo and 10-fold  CV)","d19aa7bc":"# Data Cleaning: Deduplication\n#It is observed (as shown in the table below) that the reviews data had many duplicate entries. Hence it was necessary to remove duplicates in order to get unbiased results for the analysis of the data. Following is an example:","ccc1ff30":"> > # Step 5) KNN Classifier for WORD2VEC  (using BRUTE force algo and 10-FOLD CV) ","eb71de02":"# Step3)  Arrive at the  Avg W2V dataframe (Test and Train) (BRUTEFORCE)*","4b805f1c":"# Part 3: KNN classifier on ON AVG-WORD2VEC for AMZN reviews","e49e2494":"# # KNN CLASSFIER on Amazon Reviews by  Sundar Viswanathan\n\nData Source: https:\/\/www.kaggle.com\/snap\/amazon-fine-food-reviews\n\n#### Objective:\n\nPerform KNN- Classification (both Brute Force and KD-Tree)  for the following vectors\n\na) BOW\nb) TF-IDF\nc) Avg Word2Vec\nd) TF-IDF Word2Vec\n\n","9b5b2efe":"# Step 4)  Arriving at TFIDF-W2V  vector  (KD-TREE)","bdbfdb3c":"*## Observation:-   It was also seen that in two rows given below the value of HelpfulnessNumerator is greater than HelpfulnessDenominator which is not practically possible hence these two rows too are removed from calcualtions","878e797c":"# Step 6) KNN Classifier for tfidf (using KDTREE force algo and 10-fold CV)**\n","f91d11e4":"   # STEP 1) Computing the Bag of Words (BoW) ( for BRUTEFORCE)","bc413c11":"# Step 4) KNN Classifier for WORD2VEC  (using BRUTE force algo and Simple CV)","88f70987":"# Step 4) Getting TF-IDF Vector  for KD-Tree Algo\n","7cb18a0e":"# Step 5) KNN Classifier for tfidf (using KDTREE force algo and Simple CV)**\n","a709090a":"> # Step 2)** KNN Classifier for BOW (using BRUTE force algo and Simple CV)**\n","492af65b":"# STEP 3)  KNN Classifier for BOW (using BRUTE force algo and 10-FOLD CV)","900c3d90":"# Step 4) Computing the Bag of Words (BoW) ( for KD-TREE)","f5b00a51":"![](http:\/\/)# #  splitting the 40 k reviews in to training and test data(for BRUTEFORCE) (limiting to 40K reviews** since we run in to Memory Error for reviews more than 40 K)**","6f6093f4":"##  Here we will take N_COMPONENTS = 12, **since beyond 12, variance explained is very minimal%","8d301ed0":"Here we will take N_COMPONENTS = 4, **since beyond 4, variance ratio drops by 50%**","b0f71d47":"\n# Step 10) KNN Classifier for WORD2VEC  (using KD_TREE and 10-FOLD CV)\n"}}