{"cell_type":{"33e702bf":"code","4a2766bc":"code","00a55cc5":"code","bc1ba070":"code","1153b09d":"code","e2a6d8aa":"code","68565eae":"code","7b7bbcfa":"code","1b9ab47d":"code","0f153a6b":"code","c14e193f":"code","6c3d5a29":"code","293135c6":"code","5e320468":"code","882f6f05":"code","4793e828":"code","0bd9ddcc":"code","d4842021":"code","70e38441":"code","dd61e131":"code","e515aa0b":"code","7b3887b5":"code","7a0d6cc6":"markdown","f03b98e3":"markdown","dbe58728":"markdown","7bae4321":"markdown","acfce6e5":"markdown"},"source":{"33e702bf":"import pandas as pd, numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom scipy.sparse import hstack\nimport gc\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))","4a2766bc":"# Loading data\n\ntrain1 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\ntrain1['lang'] = 'en'\n\ntrain_es = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-es-cleaned.csv')\ntrain_es['lang'] = 'es'\n\ntrain_fr = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-fr-cleaned.csv')\ntrain_fr['lang'] = 'fr'\n\ntrain_pt = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-pt-cleaned.csv')\ntrain_pt['lang'] = 'pt'\n\ntrain_ru = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-ru-cleaned.csv')\ntrain_ru['lang'] = 'ru'\n\ntrain_it = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-it-cleaned.csv')\ntrain_it['lang'] = 'it'\n\ntrain_tr = pd.read_csv('\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-tr-cleaned.csv')\ntrain_tr['lang'] = 'tr'\n\n#train2 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\n#train2.toxic = train2.toxic.round().astype(int)\n#train2['lang'] = 'en'\n\ntrain = pd.concat([\n    \n    train1[['comment_text', 'lang', 'toxic']],\n    train_es[['comment_text', 'lang', 'toxic']],\n    train_tr[['comment_text', 'lang', 'toxic']],\n    train_fr[['comment_text', 'lang', 'toxic']],\n    train_pt[['comment_text', 'lang', 'toxic']],\n    train_ru[['comment_text', 'lang', 'toxic']],\n    train_it[['comment_text', 'lang', 'toxic']]\n    \n]).sample(n=300000).reset_index(drop=True)\n\ndel train1, train_es, train_fr, train_pt, train_ru, train_it, train_tr\ngc.collect()","00a55cc5":"#train = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv')\n#train1 = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv')\n\n#valid = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-test-translated\/jigsaw_miltilingual_valid_translated.csv')\n#valid1 = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\n\n#test = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-test-translated\/jigsaw_miltilingual_test_translated.csv')\ntest = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')\n\nsubm = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')","bc1ba070":"train.head()","1153b09d":"#train = pd.concat([train,train1])\n#train = pd.concat([train,valid])\n#valid['comment_text'] = valid['translated'] #+' '+valid1['comment_text']\n#test['content'] = test['translated'] #+' '+test1['content']\n#train = pd.concat([train,valid])\n#train = valid.copy()","e2a6d8aa":"train.head()","68565eae":"train['comment_text'][0]","7b7bbcfa":"train['comment_text'][2]","1b9ab47d":"lens = train.comment_text.str.len()\nlens.mean(), lens.std(), lens.max()","0f153a6b":"lens.hist();","c14e193f":"label_cols = ['toxic']\ntrain['none'] = 1-train[label_cols].max(axis=1)\ntrain.describe()","6c3d5a29":"len(train),len(test)","293135c6":"train['comment_text'].fillna(\"unknown\", inplace=True)\ntest['content'].fillna(\"unknown\", inplace=True)","5e320468":"import re, string\nre_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","882f6f05":"n = train.shape[0]\n\nvec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\n\n\ntrn_term_doc = vec.fit_transform(train['comment_text'])\ntest_term_doc = vec.transform(test['content'])\n","4793e828":"trn_term_doc, test_term_doc","0bd9ddcc":"def pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) \/ ((y==y_i).sum()+1)","d4842021":"x = trn_term_doc\ntest_x = test_term_doc","70e38441":"def get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) \/ pr(0,y))\n    m = LogisticRegression(C=4, dual=False)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r","dd61e131":"preds = np.zeros((len(test), len(label_cols)))\n\nfor i, j in enumerate(label_cols):\n    print('fit', j)\n    m,r = get_mdl(train[j])\n    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]","e515aa0b":"submid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\n\nsubmission.to_csv('submission.csv', index=False)","7b3887b5":"submission.head(n=20)","7a0d6cc6":"#### Please refer to the original kernel: https:\/\/www.kaggle.com\/jhoward\/nb-svm-strong-linear-baseline","f03b98e3":"# Looking at the data\n## The training data contains a row per comment, with an id, the text of the comment, and 6 different labels that we'll try to predict.","dbe58728":"train1 = pd.read_csv(\"\/kaggle\/input\/jigsaw-train-translated\/train_mic.csv\")\n#train2 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\n#train2.toxic = train2.toxic.round().astype(int)\n\nvalid = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-test-translated\/jigsaw_miltilingual_valid_translated.csv')\n#valid1 = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')","7bae4321":"train = pd.concat([\n    train1[['comment_text', 'toxic']],\n    train1[['tr', 'toxic']].rename(columns={'tr': 'comment_text'}).dropna(),\n    train1[['ru', 'toxic']].rename(columns={'ru': 'comment_text'}).dropna(),\n    train1[['it', 'toxic']].rename(columns={'it': 'comment_text'}).dropna(),\n    train1[['fr', 'toxic']].rename(columns={'fr': 'comment_text'}).dropna(),\n    train1[['pt', 'toxic']].rename(columns={'pt': 'comment_text'}).dropna(),\n    train1[['es', 'toxic']].rename(columns={'es': 'comment_text'}).dropna(),\n    #train2[['comment_text', 'toxic']].query('toxic==1'),\n    #train2[['comment_text', 'toxic']].query('toxic==0'),\n    valid[['comment_text', 'toxic']]\n    #valid1[['comment_text', 'toxic']]\n]).reset_index(drop=True)","acfce6e5":"valid = pd.concat([\n    valid[['comment_text', 'toxic']],\n    valid[['translated', 'toxic']].rename(columns={'translated': 'comment_text'})])\n\n#valid['comment_text'] = valid['translated'] #+' '+valid1['comment_text']"}}