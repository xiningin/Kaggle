{"cell_type":{"bd232c99":"code","1b625ba8":"code","d93a864d":"code","6d991f94":"code","0cdd15c3":"code","1a828e54":"code","21d6b65c":"code","35713e3f":"code","ef6b3402":"code","3fabc305":"code","c716d2a7":"code","98736eb6":"code","85d26206":"code","c1a0d446":"code","ab6b9597":"code","0eecc333":"code","a30dfe50":"code","f1545eb4":"code","801db890":"code","effcc437":"code","b467cb6f":"code","2fed9cda":"code","2659f878":"code","e6c54215":"code","d5db7d33":"code","ca496a49":"code","2d0ee7f7":"code","b2b35df7":"code","662a037c":"code","038dd8ef":"code","b8347245":"code","41e3be76":"markdown","9a3ba882":"markdown","84a8a057":"markdown","d701f5c1":"markdown","cbe7f2ed":"markdown","7801f550":"markdown","dde4de88":"markdown","cc2bd36e":"markdown","9880a0e5":"markdown","86700948":"markdown","9b46caef":"markdown","ddd53643":"markdown","f727764d":"markdown","a13ee840":"markdown"},"source":{"bd232c99":"import numpy as np \nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","1b625ba8":"train_data=pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/train.csv\")\ntest_data=pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/test.csv\")\nsubmition=pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")","d93a864d":"print(train_data.shape)\nprint(test_data.shape)","6d991f94":"train_data.head(10)","0cdd15c3":"# Here I get rid of one of the columns\ntrain_data=train_data.drop([\"id\"],axis=1)","1a828e54":"test_data.head(5)","21d6b65c":"# Here I get rid of one of the columns\ntest_data=test_data.drop([\"id\"],axis=1)","35713e3f":"#Let's look at the different values for each column.\ntrain_data.columns","ef6b3402":"for i in train_data.columns:\n  print(i, train_data[i].unique())","3fabc305":"# Let's be more specific in choosing the column.\ntrain_data[\"loss\"].unique()","c716d2a7":"#Let's see the most frequent values.\ntrain_data[\"loss\"].value_counts()","98736eb6":"# Let's see the most frequent values in the graph\nplt.figure(figsize=(12,6))\nsns.countplot(train_data[\"loss\"])\nplt.title(\"Countplot for diagnosis\")\nplt.show();","85d26206":"train_data.info()","c1a0d446":"train_data.corr()['loss'].sort_values(ascending=False).head(20)","ab6b9597":"data_dtype = train_data.dtypes\ndata_dtype.value_counts()","0eecc333":"train_data.isnull().sum().any()","a30dfe50":"train_data.duplicated().sum()","f1545eb4":"train_data.describe()","801db890":"train_data.hist(figsize=(100,100))\nplt.show()","effcc437":"corr_1=train_data.corr()\nmost=corr_1.nlargest(25,\"loss\")\nmost","b467cb6f":"from sklearn.preprocessing import MinMaxScaler","2fed9cda":"test_data.isnull().sum().any()","2659f878":"#MinMaxScaler for test data\nscaler = MinMaxScaler(copy=True, feature_range=(0, 1))\ntest= scaler.fit_transform(test_data)\n","e6c54215":"from sklearn.preprocessing import MinMaxScaler","d5db7d33":"target=train_data[\"loss\"]\nfeatures=train_data.drop([\"loss\"],axis=1)","ca496a49":"#MinMaxScaler for Data\nscaler = MinMaxScaler(copy=True, feature_range=(0, 1))\nfeatur_train= scaler.fit_transform(features)\n","2d0ee7f7":"featur_train","b2b35df7":"from sklearn.linear_model import LinearRegression","662a037c":"model = LinearRegression(fit_intercept=True, normalize=True,copy_X=True,n_jobs=-1)\nmodel.fit(featur_train, target)","038dd8ef":"pred=model.predict(test)\n","b8347245":"result = pd.DataFrame({'id': submition.id,\n                       'loss': pred, \n                       })\n\nresult.to_csv('submission.csv', index = False)\nresult","41e3be76":"# reading dataset","9a3ba882":"# take a look at it","84a8a057":"> **Wow, it seems that the data does not contain duplicate rows.**","d701f5c1":"# missing values","cbe7f2ed":"# prepare test file","7801f550":"# Initial exploration of data","dde4de88":"> **The values seem to be clean in terms of missing values.**","cc2bd36e":"> **From the first look at the data, we notice that the data have many directions, and we will notice that some of them have positive and negative skewness and symmetry as well.**","9880a0e5":"# Adjusting data in preparation for building models:","86700948":"# Tabular Playground Series\n**In the following code, we will do some work, such as:**\n\n* Read the data.\n* take a look at it.\n* data report.\n* Analyze and explore data.\n* Building a machine learning algorithm for prediction.\n* Building a deep learning algorithm for prediction.\n* The end .\n\n**let's enjoy...**","9b46caef":"> **Now let's pay more attention to training data.**","ddd53643":"# The End\n\n> **I'm done, that code page is just a little warm-up for what's to come.**\n\n# Thank you for following up.","f727764d":"# Linear Regression algorithm","a13ee840":"# Building models with machine learning:"}}