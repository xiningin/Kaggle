{"cell_type":{"aa0b1bfd":"code","fc26c9c6":"code","f7ae1334":"code","7ffc42b5":"code","45a03f2d":"code","d33c730a":"code","7323322c":"code","42cbd532":"code","d2c85b66":"code","fe28cb79":"code","f5fffa58":"code","d890ff5b":"code","818e704f":"code","2cc9ff76":"code","a3ac52df":"code","5795eae1":"code","73c84381":"code","f24442ad":"code","5181591b":"code","fcab069d":"code","995d63f3":"code","acb36ed5":"code","cbd4501a":"code","aba0b8b9":"code","3320269b":"code","6c9e6f7c":"code","4fd7f980":"code","1937e547":"code","6e9f3fd7":"code","de5130bf":"code","ad6de56a":"code","ea2862e2":"code","6ab3f470":"code","cc7670cd":"code","fa397488":"code","a6d3fbaf":"code","ea05c390":"code","8e1ef78f":"code","b652956c":"code","1409e9b5":"code","34de53cc":"code","0657d949":"code","0c9ece47":"code","c9eaf9d2":"code","eb3df8b5":"code","6ae59d28":"code","e333f303":"code","21fab2a5":"code","fa77e5e2":"code","8576a523":"code","9305dfc3":"code","52113596":"code","432656d6":"code","da43006c":"code","155d826d":"code","22a02012":"code","844fab74":"code","bbafdcd6":"markdown","607202f5":"markdown","543a470c":"markdown","fc322e21":"markdown","2f14934e":"markdown","f5d51557":"markdown","16df67a8":"markdown","76f00420":"markdown","87859026":"markdown","6081f4ae":"markdown","f694a261":"markdown","a1b8b0f1":"markdown","4ab7d642":"markdown","4f13d808":"markdown","71342686":"markdown","eb4ebb08":"markdown","5d49a93a":"markdown","a7d8f268":"markdown","c5d696b4":"markdown","c204869c":"markdown","961649d9":"markdown","427bc799":"markdown","f29756ce":"markdown","85e873b3":"markdown","1eb53338":"markdown","c97452a1":"markdown","692756f9":"markdown","c6675cb8":"markdown","95bf9024":"markdown","1a1965f0":"markdown","4bf9008c":"markdown","68df0e6d":"markdown","11e49627":"markdown","a025d366":"markdown","a117e5db":"markdown","54b0a974":"markdown","c55bd86a":"markdown","eb0299c6":"markdown","ca0d1d9e":"markdown","afe2ab49":"markdown","c592a10e":"markdown","2798a2f8":"markdown","33bd5550":"markdown","876ee1fd":"markdown","ff45964f":"markdown","e2348a48":"markdown","342d1edd":"markdown","26a057ab":"markdown","37a51538":"markdown","e4e4fff4":"markdown","6e89a956":"markdown","6e0c89a0":"markdown","91743d8e":"markdown","703b69f0":"markdown","695a2b5d":"markdown","ac6f685b":"markdown","8b589df8":"markdown","8dd58e87":"markdown","174000ba":"markdown","5b382cd0":"markdown","495a8e1e":"markdown","0a41a07a":"markdown","dd956d37":"markdown","083f5379":"markdown","cb723e99":"markdown","1fae4b5a":"markdown","319c08b6":"markdown","07c3797a":"markdown","77833fdc":"markdown","09ea7091":"markdown","c9c41710":"markdown","47b6275b":"markdown","d55d94cd":"markdown","ea13a087":"markdown"},"source":{"aa0b1bfd":"#Imports \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport warnings\nimport gc\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)\n%matplotlib inline","fc26c9c6":"start_df = pd.read_csv('..\/input\/loan.csv', low_memory=False)","f7ae1334":"df = start_df.copy(deep=True)\ndf.head()","7ffc42b5":"df.shape","45a03f2d":"df.columns","d33c730a":"df_description = pd.read_excel('..\/input\/LCDataDictionary.xlsx').dropna()\ndf_description.style.set_properties(subset=['Description'], **{'width': '1000px'})","7323322c":"def null_values(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        print (\"Dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        return mis_val_table_ren_columns","42cbd532":"# Missing values statistics\nmiss_values = null_values(df)\nmiss_values.head(20)","d2c85b66":"target_list = [1 if i=='Default' else 0 for i in df['loan_status']]\n\ndf['TARGET'] = target_list\ndf['TARGET'].value_counts()","fe28cb79":"df.drop('loan_status',axis=1,inplace=True)","f5fffa58":"# Number of each type of column\ndf.dtypes.value_counts().sort_values().plot(kind='barh')\nplt.title('Number of columns distributed by Data Types',fontsize=20)\nplt.xlabel('Number of columns',fontsize=15)\nplt.ylabel('Data type',fontsize=15)","d890ff5b":"df.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","818e704f":"df['emp_length'].head(3)","2cc9ff76":"df['emp_length'].fillna(value=0,inplace=True)\n\ndf['emp_length'].replace(to_replace='[^0-9]+', value='', inplace=True, regex=True)\n\ndf['emp_length'].value_counts().sort_values().plot(kind='barh',figsize=(18,8))\nplt.title('Number of loans distributed by Employment Years',fontsize=20)\nplt.xlabel('Number of loans',fontsize=15)\nplt.ylabel('Years worked',fontsize=15);","a3ac52df":"fig = plt.figure(figsize=(12,6))\nsns.violinplot(x=\"TARGET\",y=\"loan_amnt\",data=df, hue=\"pymnt_plan\", split=True)\nplt.title(\"Payment plan - Loan Amount\", fontsize=20)\nplt.xlabel(\"TARGET\", fontsize=15)\nplt.ylabel(\"Loan Amount\", fontsize=15);","5795eae1":"temp = [i for i in df.count()<887379 *0.30]\ndf.drop(df.columns[temp],axis=1,inplace=True)","73c84381":"corr = df.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', corr.tail(10))\nprint('\\nMost Negative Correlations:\\n', corr.head(10))","f24442ad":"df.corr()['dti'].sort_values().tail(6)","5181591b":"fig = plt.figure(figsize=(22,6))\nsns.kdeplot(df.loc[df['TARGET'] == 1, 'int_rate'], label = 'target = 1')\nsns.kdeplot(df.loc[df['TARGET'] == 0, 'int_rate'], label = 'target = 0');\nplt.xlabel('Interest Rate (%)',fontsize=15)\nplt.ylabel('Density',fontsize=15)\nplt.title('Distribution of Interest Rate',fontsize=20);","fcab069d":"fig = plt.figure(figsize=(12,6))\nsns.violinplot(x=\"TARGET\",y=\"loan_amnt\",data=df, hue=\"term\", split=True,color='pink')\nplt.title(\"Term - Loan Amount\", fontsize=20)\nplt.xlabel(\"TARGET\", fontsize=15)\nplt.ylabel(\"Loan Amount\", fontsize=15);","995d63f3":"fig = plt.figure(figsize=(12,6))\nsns.violinplot(x=\"TARGET\",y=\"loan_amnt\",data=df, hue=\"application_type\", split=True,color='green')\nplt.title(\"Application Type - Loan Amount\", fontsize=20)\nplt.xlabel(\"TARGET\", fontsize=15)\nplt.ylabel(\"Loan Amount\", fontsize=15);","acb36ed5":"df['application_type'].value_counts()","cbd4501a":"fig = plt.figure(figsize=(18,8))\nsns.violinplot(x=\"TARGET\",y=\"int_rate\",data=df, hue=\"grade\")\nplt.title(\"Grade - Interest Rate\", fontsize=20)\nplt.xlabel(\"TARGET\", fontsize=15)\nplt.ylabel(\"Interest Rate\", fontsize=15);","aba0b8b9":"df.corr()['annual_inc'].sort_values().tail(10)","3320269b":"fig = plt.figure(figsize=(18,10))\ndf[df['TARGET']==1].groupby('addr_state')['TARGET'].count().sort_values().plot(kind='barh')\nplt.ylabel('State',fontsize=15)\nplt.xlabel('Number of loans',fontsize=15)\nplt.title('Number of defaulted loans per state',fontsize=20);","6c9e6f7c":"fig = plt.figure(figsize=(18,10))\ndf[df['TARGET']==0].groupby('addr_state')['TARGET'].count().sort_values().plot(kind='barh')\nplt.ylabel('State')\nplt.xlabel('Number of loans')\nplt.title('Number of not-defaulted loans per state');","4fd7f980":"df['member_id'].value_counts().head(2)","1937e547":"df['emp_title'].value_counts().head()","6e9f3fd7":"df.drop(['id','member_id','emp_title','title','zip_code','url'],axis=1,inplace=True)","de5130bf":"df.shape","ad6de56a":"df.info()","ea2862e2":"df['issue_d']= pd.to_datetime(df['issue_d']).apply(lambda x: int(x.strftime('%Y')))\ndf['last_pymnt_d']= pd.to_datetime(df['last_pymnt_d'].fillna('2016-01-01')).apply(lambda x: int(x.strftime('%m')))\ndf['last_credit_pull_d']= pd.to_datetime(df['last_credit_pull_d'].fillna(\"2016-01-01\")).apply(lambda x: int(x.strftime('%m')))\ndf['earliest_cr_line']= pd.to_datetime(df['earliest_cr_line'].fillna('2001-08-01')).apply(lambda x: int(x.strftime('%m')))\ndf['next_pymnt_d'] = pd.to_datetime(df['next_pymnt_d'].fillna(value = '2016-02-01')).apply(lambda x:int(x.strftime(\"%Y\")))","6ab3f470":"from sklearn import preprocessing","cc7670cd":"count = 0\n\nfor col in df:\n    if df[col].dtype == 'object':\n        if len(list(df[col].unique())) <= 2:     \n            le = preprocessing.LabelEncoder()\n            df[col] = le.fit_transform(df[col])\n            count += 1\n            print (col)\n            \nprint('%d columns were label encoded.' % count)","fa397488":"df = pd.get_dummies(df)\nprint(df.shape)","a6d3fbaf":"df['mths_since_last_delinq'] = df['mths_since_last_delinq'].fillna(df['mths_since_last_delinq'].median())","ea05c390":"df.dropna(inplace=True)","8e1ef78f":"df.count().sort_values().head(3)","b652956c":"df['TARGET'].value_counts()","1409e9b5":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix","34de53cc":"def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        print(\"Train Result:\\n\")\n        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_train, clf.predict(X_train))))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_train, clf.predict(X_train))))\n        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_train, clf.predict(X_train))))\n\n        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n        print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n        print(\"Accuracy SD: \\t\\t {0:.4f}\".format(np.std(res)))\n        \n    elif train==False:\n        print(\"Test Result:\\n\")        \n        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, clf.predict(X_test))))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, clf.predict(X_test))))\n        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, clf.predict(X_test))))    \n        ","0657d949":"from sklearn.model_selection import train_test_split","0c9ece47":"X_train, X_test, y_train, y_test = train_test_split(df.drop('TARGET',axis=1),df['TARGET'],test_size=0.15,random_state=101)","c9eaf9d2":"del start_df\ngc.collect()","eb3df8b5":"from sklearn.preprocessing import StandardScaler","6ae59d28":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test=sc.transform(X_test)","e333f303":"from imblearn.over_sampling import SMOTE","21fab2a5":"sm = SMOTE(random_state=12, ratio = 1.0)\nx_train_r, y_train_r = sm.fit_sample(X_train, y_train)","fa77e5e2":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(C = 0.0001,random_state=21)\n\nlog_reg.fit(x_train_r, y_train_r)","8576a523":"print_score(log_reg, x_train_r, y_train_r, X_test, y_test, train=False)","9305dfc3":"from sklearn.ensemble import RandomForestClassifier","52113596":"clf_rf = RandomForestClassifier(n_estimators=40, random_state=21)\nclf_rf.fit(x_train_r, y_train_r)","432656d6":"print_score(clf_rf, x_train_r, y_train_r, X_test, y_test, train=False)","da43006c":"from sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom lightgbm import LGBMClassifier","155d826d":"def kfold_lightgbm(train_df, num_folds, stratified = False):\n    print(\"Starting LightGBM. Train shape: {}\".format(train_df.shape))\n    \n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=47)\n    else:\n        folds = KFold(n_splits= num_folds, shuffle=True, random_state=47)\n\n    oof_preds = np.zeros(train_df.shape[0])\n\n    feature_importance_df = pd.DataFrame()\n    feats = [f for f in train_df.columns if f not in ['TARGET']]\n    \n    # Splitting the training set into folds for Cross Validation\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n\n        # LightGBM parameters found by Bayesian optimization\n        clf = LGBMClassifier(\n            nthread=4,\n            n_estimators=10000,\n            learning_rate=0.02,\n            num_leaves=32,\n            colsample_bytree=0.9497036,\n            subsample=0.8715623,\n            max_depth=8,\n            reg_alpha=0.04,\n            reg_lambda=0.073,\n            min_split_gain=0.0222415,\n            min_child_weight=40,\n            silent=-1,\n            verbose=-1,\n            )\n\n        # Fitting the model and evaluating by AUC\n        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n            eval_metric= 'auc', verbose= 1000, early_stopping_rounds= 200)\n        print_score(clf, train_x, train_y, valid_x, valid_y, train=False)\n        # Dataframe holding the different features and their importance\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        \n        # Freeing up memory\n        del clf, train_x, train_y, valid_x, valid_y\n        gc.collect()\n\n    display_importances(feature_importance_df)\n    return feature_importance_df","22a02012":"def display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(15, 12))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances.png')","844fab74":"feat_importance = kfold_lightgbm(df, num_folds= 3, stratified= False)","bbafdcd6":"# Modeling","607202f5":"Printing out the column names,","543a470c":"## Exploratory Data Analysis","fc322e21":"Let us also check the correlation of annual income with loan amount taken. ","2f14934e":"Then, seeing the distribution of data types we are working with,","f5d51557":"We have high precision but a low recall for our validation set. Using this model is not a good idea as most of our default loans will be falsely classified.","16df67a8":"> The annual income of the applicant has high positive correlation with the amount of loan they have taken.","76f00420":"**Creating a baseline for accuracy and recall using Logistic regression, **","87859026":">  We would want to label encode the columns having only 2 categorical data and one-hot encode columns with more than 2 categorical data. Also, columns like emp_title, url, desc, etc. should be dropped because there aren't any large number of unique data for any of the categories they contain. Also, Principal Component Analysis can be carried out for the one-hot encoded columns to bring the feature dimensions down.","6081f4ae":"Freeing up the memory.","f694a261":"# Understanding the data ","a1b8b0f1":"And one-hot encoding the rest categorical columns,","4ab7d642":"In this kernel I will be going over the Lending Club Loan Data where the data is imbalanced, big and has multiple features with different data types. For the purpose of modelling, I will be taking all default loans as the target variable and will be trying to predict if a loan will default or not.\n\n---","4f13d808":"For our case, overfitting will be a huge concern. So, I'm using Random Forest as it is known to decrease overfitting by selecting features at random. ","71342686":"First, let's check the description of the various column fields in the dataset.","eb4ebb08":"Let us do make some Kernel Density Estimation Plots to see how the interest rate and debt to income ratio are distributed for the two classes in the TARGET column.","5d49a93a":"> Looking at the columns description, a good thing we could do is find columns that carry importance and at the same time find columns that are redundant for their lack of information.\n\n\nLet us also see the number and percentage of missing values,","a7d8f268":"And checking the count,","c5d696b4":"We are now left with a reasonable amount of data for modelling.\n\n---","c204869c":"However for columns like 'total_rev_hi_lim','tot_col_ammnt',etc.\u00a0, I won't be filling in the missing data because they will certainly be of high feature importance due to their description. If they do not seem to be of high importance we can always re-iterate and fill the missing values later.\u00a0\n\nSo, dropping all remaining null values,","961649d9":"\nAs we had observe, some columns like annual_inc, int_rate, etc. may be much useful for building our model but on the other hand, some columns like id, member_id, etc. will not be helping. \n\nAlso, columns like 'title' and 'emp_title' are text which cannot be one-hot encoded \/ label encoded as they have arbitrary categorical text and very less unique data for each of their categories.","427bc799":"** Violin-plot of TARGET classes with distribution of loan amount differentiated by the terms. **","f29756ce":"The accuracy came out to be satisfactory for the baseline along with the recall score. However, precision seems to be very off.","85e873b3":"The memory usage is 325+ MB. Some of these columns still look like they could need some work i.e. more cleaning!\u00a0\n\nI will be fixing the data types and then handling the missing data.","1eb53338":"> The density of interest rates follow kind of a Gaussian distribution with more density on interest rates between 12%-18%.","c97452a1":"> Seeing the number of joint applicants in comparison to the the total applicants, it **isn't** significant enough to conclude that the loan taken by all Joint applicants are paid back. ","692756f9":"> So we have quite a number of columns having objects data type which are going to pose a problem while modelling. ","c6675cb8":"> Besides from the perfect correlation of TARGET column with itself, columns like int_rate which is interest rate, out_prncp_inv which is remaining outstanding principal, etc. have high positive correlation with the TARGET column and these are quite true as higher the interest rate, higher it is harder for a borrower to pay back a loan. However, columns like out_prncp_inv, out_prncp, total_rec_int, total_rec_late_fee, inq_last_6mths and revol_util are bound to be higher when a borrower doesn't pay back a loan and thus doesn't carry much significance. So, the column of interest after int_rate could be the dti which is the Debt to Income ratio which understandably will affect if a borrower can pay back a loan or not.\n\n> Also, columns like recoveries, total_rev_hi_lim, etc. have negative correlation with the TARGET column as a borrower who has paid back money is more likely to repay the loan.","95bf9024":"As we can see, LightGBM did a great job for getting high precision as well as a high recall. Hence, this model is the best in terms of the 3 models that we evaluated.","1a1965f0":"For the 'mths_since_last_delinq' column, I'll be filling in the missing value with the median of the columns as the data in the column is continuous.","4bf9008c":"> It can be seen that the interest rate is also highly positively correlated with the debt to income ratio.","68df0e6d":"\nLet's see how we can handle our categorical data. Two methods we can use are Label Encoding and One Hot Encoding.\n\nThe problem with label encoding is that it gives the categories an arbitrary ordering. The value assigned to each of the categories is random and does not reflect any inherent aspect of the category. So, If we only have two unique values for a categorical variable (such as Yes\/No), then label encoding is fine, but for more than 2 unique categories, one-hot encoding is the better option.\n\nHowever, due to the large number of columns originated after One-Hot Encoding, we may have to conduct Principle Component Analysis (PCA) for dimensionality reduction.","11e49627":"Let's see if we have any members taking multiple loans.","a025d366":"> The percentage of missing data in many columns are far more than we can work with. So, we'll have to remove columns having a certain percentage of data less than the total data later on.","a117e5db":"# Cleaning the data","54b0a974":"Working on a copy of the dataframe so that I do not have to re-read the entire dataset again in order to save memory.","c55bd86a":"\n# Importing the data","eb0299c6":"Let us check for any anomalies on the data we might have. Possible data anamolies are often found in columns dealing with time like years of employment. Let's quickly go through them.","ca0d1d9e":"# 1) Bagging - Random Forest\n\n* Ensemble of Decision Trees\n\n* Training via the bagging method (Repeated sampling with replacement)\n  * Bagging: Sample from samples\n  * RF: Sample from predictors. $m=sqrt(p)$ for classification and $m=p\/3$ for regression problems.\n\n* Utilise uncorrelated trees","afe2ab49":"For further enhancements to the model, feature engineering could be done. Also a broader term like 'good loan' and 'bad loan' could have been used by encompassing different loan statuses together to get a more balanced counts of classes rather than default\/non-default.","c592a10e":"First, I'll be converting the date object columns into integer number of years or months just because I do not want to blow up the number of feature columns by performing one-hot encoding on them. For filling the null values I have taken the dates with the highest number of counts.","2798a2f8":"I'll be filling the null values with 0 assuming that the borrower hasn't worked many years for his data to be recorded. Also, I'll be using regex to extract the number of years from all of the data.","33bd5550":"Function to use LightGBM with Kfold cross validation,","876ee1fd":"> Suprisingly there is not a single member taking loan more than once. So, member id column can also be dropped along with the id column.","ff45964f":"While we are looking at distributions, some other distributions that would be interesting to examine are,","e2348a48":"Oversampling only the training set using Synthetic Minority Oversampling Technique ([SMOTE](https:\/\/jair.org\/index.php\/jair\/article\/view\/10302))","342d1edd":"** Violin-plot of TARGET classes with distribution of interest rate differentiated by the loan grades. **","26a057ab":"Now, I'll be trying out different models to get the best prediction score.","37a51538":"> Most of the Loans of higher terms have high amount and vice versa for the TARGET classes.","e4e4fff4":"So, we've got a fair amount of columns that we need to understand. Knowing what the columns mean can help us a lot for feature engineering later on.\n\n---","6e89a956":"** From where do most of the loans tend to be defaulted? **","6e0c89a0":"> This clearly is a case of an imbalanced class problem where the value of class is far less than the other. There are cost function based approaches and sampling based approaches for handling this kind of problem which we will use later so that our model doesn't exhibit high bias while trying to predict if a loan will default or not.","91743d8e":"** Violin-plot of TARGET classes with distribution of loan amount differentiated by the application type. **","703b69f0":"## Anomaly Detection","695a2b5d":"> It can be seen that there are more number of loans taken amount from the same states where there are more number of defaulted risk. This is why the state cannot be taken as a major feature for knowing if a loan will be defaulted or not.\n","ac6f685b":"For boosting I will be using the [LightGBM](https:\/\/www.youtube.com\/watch?v=5CWwwtEM2TA) classifier (evalulation metric as AUC) along with [Kfold cross validation](https:\/\/www.youtube.com\/watch?v=TIgfjmp-4BA).","8b589df8":"> Both target classes have similar kind of interest rates by grades.","8dd58e87":"First, importing necessary libraries,","174000ba":"Standardizing features by removing the mean and scaling to unit variance","5b382cd0":"Conducting train test split.","495a8e1e":"Function for displaying the importance of the features,","0a41a07a":"Examining further on debt to income ratio and interest rate,","dd956d37":"> The column looks fine. Also, it can be seen that people who have worked for 10 or more years are more likely to take a loan.","083f5379":"So all the loans that have been defaulted are from individuals rather than from two or more people. ","cb723e99":"Checking the dimensions,","1fae4b5a":"## 2) Boosting:\n\n* Train weak classifiers \n* Add them to a final strong classifier by weighting. Weighting by accuracy (typically)\n* Once added, the data are reweighted\n  * Misclassified samples gain weight \n  * Algo is forced to learn more from misclassified samples    ","319c08b6":"Let us see how many categorical data do the columns having 'object' data types contain:","07c3797a":"So, now we have 48 columns remaining. Let's print them out to get a quick look of what we are dealing with,","77833fdc":"Let me remove all the columns with more than 70% missing data as they won't be helping for modelling and exploration.","09ea7091":"Creating a classification report function,","c9c41710":"Now, for modeling I will be using two ensemble methods and comparing them.\n\ni) Bootstrap Aggregrating or Bagging\n\nii) Boosting","47b6275b":"> Naturally, the defaulted loans had no payment plan","d55d94cd":"Another thing we would want to examine is that how many loans have a default loan status in comparison to other loans. A common thing to predict in datasets like these are if a new loan will get default or not. I'll be keeping loans with default status as my target variable.","ea13a087":"# A Hitchhiker's Guide to Lending Club Loan Data"}}