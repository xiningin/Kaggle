{"cell_type":{"c2923047":"code","1de5afda":"code","087efd9b":"code","95d67b60":"code","f8800b36":"code","a7c36bd2":"code","ca4c216e":"code","c8aea431":"code","683332eb":"code","6922d691":"code","407f5319":"code","be2acd5e":"code","bef970fe":"code","a29421e1":"code","b36ba754":"code","ce46da81":"code","131b8b9e":"code","c4302525":"code","96811035":"code","f34619f2":"code","fb63bb60":"code","3951956f":"code","ab7ca767":"code","3744cd78":"code","94053df1":"code","5b9162c8":"code","bd6018e0":"code","a51c547b":"code","863cbeaa":"code","ab32d5eb":"code","aae1710d":"code","59a27c3e":"code","71d11f21":"code","03d6e901":"code","20680e7c":"code","dd5a411a":"code","b90e5551":"code","2ebe6e1a":"code","74a55d3c":"code","2f3e4b0b":"code","c24db517":"code","008f9ba5":"code","df6a8296":"code","e42c9cdd":"code","0490d68c":"code","944c75f6":"code","6e2890ea":"code","ca88622f":"code","cc7dbd02":"code","1ce33700":"code","b977d3c8":"code","89527c7d":"code","880d15f8":"code","be791b41":"markdown","d059ab32":"markdown","e6c3eb96":"markdown","8d6be6e1":"markdown","ac4f7bc4":"markdown","1165bd05":"markdown","eeb6b9dc":"markdown","19657e25":"markdown","1ba9869d":"markdown","a47985d8":"markdown","b5c524c2":"markdown","88759323":"markdown","14405ebb":"markdown","78272843":"markdown","c333416f":"markdown","334ee298":"markdown","865f6dc8":"markdown","9b7eaef8":"markdown","108be846":"markdown","066dbb85":"markdown","14fad611":"markdown","37dd61a6":"markdown","684156d1":"markdown","83688bf7":"markdown","9552985d":"markdown","f55091c7":"markdown","40cde053":"markdown","12ff5341":"markdown","4382dd3b":"markdown","adfd67c4":"markdown","bd998088":"markdown","3eb9283c":"markdown","86377507":"markdown","c3cfd644":"markdown","d91dd36c":"markdown","bd0d245b":"markdown","35825242":"markdown","7904be4b":"markdown","78d16d2c":"markdown","94f74ce8":"markdown","f2befeaf":"markdown","e79a787c":"markdown","352d7964":"markdown","6d4a0eb2":"markdown","e6b7a66f":"markdown","dd8d9bd3":"markdown"},"source":{"c2923047":"#Regular EDA\nimport pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n#Evaluation\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import precision_score,recall_score,f1_score\nfrom sklearn.metrics import plot_roc_curve\n\n\n#Models\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom joblib import dump, load\n  \n\n\n","1de5afda":"data = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/sunnymoon-sultan\/Heart_failure_classifier\/main\/heart.csv\",error_bad_lines = False)","087efd9b":"data.head()","95d67b60":"data.describe()","f8800b36":"data.info()","a7c36bd2":"print(f\"the size of the data is: {data.size}. and the shape of the dataset is: {data.shape}\" )","ca4c216e":"data.columns","c8aea431":"data.drop(\"time\",axis=1,inplace=True)","683332eb":"data.nunique()","6922d691":"data[\"DEATH_EVENT\"].value_counts().plot(kind='bar',color=[\"salmon\",\"lightblue\"]);\nprint(data.DEATH_EVENT.value_counts())\n","407f5319":"! pip install imblearn","be2acd5e":"from imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(random_state=42)\n##we have to define the x(feature variable) and the y(target variable)\nx = data.iloc[:,:-1]\ny = data.iloc[:,-1]\n# fitting the predictor and the target variable\nx_ros,y_ros = ros.fit_resample(x,y)\ny_ros.value_counts().plot(kind=\"bar\",color=[\"salmon\",\"blue\"],figsize=(10,6));","bef970fe":"pd.crosstab(data.anaemia,data.DEATH_EVENT).plot(kind=\"bar\",color=[\"black\",\"red\"],figsize=(10,6));\nplt.ylabel(\"Amount\")\nplt.title(\"Aneamia[0]=Don't have Aneamia,Aneamia[1]=Have Aneamia\")\n\nplt.xticks(rotation= 0);","a29421e1":"pd.crosstab(data.diabetes,data.DEATH_EVENT).plot(kind=\"bar\",color=[\"green\",\"red\"],figsize=(10,6));\nplt.ylabel(\"Amount\")\nplt.xticks(rotation= 0);","b36ba754":"corr_matrix= data.corr()\nfig,ax = plt.subplots(figsize=(15,10))\nax = sns.heatmap(corr_matrix,\nannot=True,\nlinewidths=0.5,\nfmt = \".2f\",\ncmap=\"YlGnBu\");","ce46da81":"sns.pairplot(data)","131b8b9e":"sns.distplot(data.serum_sodium);\n","c4302525":"sns.distplot(data.age);","96811035":"sns.distplot(data.serum_creatinine);\n","f34619f2":"models = {\"Logistic Regression\":LogisticRegression(),\n\"KNN\":KNeighborsClassifier(),\n\"Random Forest\": RandomForestClassifier(),\n         \"SVC\":SVC(),\n          \"Gradient_boosting\":GradientBoostingClassifier(),\n          \"DecissionTree\":DecisionTreeClassifier(),\n         \"lightgbm\":lgb.LGBMClassifier(),\n         \"Xg boost\":xgb.XGBClassifier()}\n#the function!\ndef fit_and_score(models,x_train,x_test,y_train,y_test):\n    np.random.seed(42)\n    model_scores={}\n    for name,model in models.items():\n        model.fit(x_train,y_train)\n        model_scores[name] = model.score(x_test,y_test)\n    return model_scores\n#splitting the data into test and tarin sets!\nnp.random.seed(42)\nx_train, x_test, y_train, y_test = train_test_split(x_ros, y_ros, test_size=0.2)\nx_t,x_te,y_t,y_te = train_test_split(x,y,test_size = 0.2)\n\n","fb63bb60":"model_scores = fit_and_score(models = models,\nx_train=x_train,\nx_test=x_test,\ny_train = y_train,\ny_test = y_test)\nprint(model_scores)","3951956f":"model_compare = pd.DataFrame(model_scores,index=[\"accuracy\"])\nmodel_compare.T.plot.bar(color=[\"salmon\"],figsize=(15,6))\nplt.xticks(rotation=0);\nprint(f\"{max(model_scores.values())*100}\")","ab7ca767":"## randomized search cv has been used to tune hyper parameter tuning\n\nxg_grid = {\"n_estimators\":np.arange(200,800),\n\"learning_rate\":[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],\n\"max_depth\":[3,4,5,9,6],\n\"maximize\" : [True],\n'min_child_weight': [1, 5, 10,6],\n\"subsample\":[0.8,0.884,0.9,1,0.7,0.5,2],\n\"gamma\":[0.5,0.6,0.9,3,5,1,2,4],\n\"alpha\":np.arange(0,5),\n\"objective\":[\"reg:logistic\",\"binary:logistic\"],\n\"colsample_bytree\":[0.1,1,0.5,0.3,2,5],\n\"booster\":[\"gbtree\",\"gblinear\",\"dart\"]}","3744cd78":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nlgbm_grid={'objective': ['binary'],\n             'metric': ['auc'],\n             'is_unbalance':[True],\n             'bagging_freq':[5],\n             'boosting':['dart'],\n             'num_boost_round':[300],\n             'early_stopping_rounds':[30]}\n\nestimator = lgb.LGBMClassifier()\nrs_lgbm = RandomizedSearchCV(\n    estimator, param_distributions=lgbm_grid, \n    n_iter=100,\n    cv=5,\n    scoring='roc_auc',\n    random_state=314,\n    verbose=True)","94053df1":"rs_lgbm.fit(x_train,y_train)\n","5b9162c8":"rs_lgbm.best_params_","bd6018e0":"print(f\"Before tuning:{model_scores['lightgbm']*100} after tuning:{rs_lgbm.score(x_test,y_test)*100}\")","a51c547b":"rf_grid = {\"n_estimators\":np.arange(100,900),\n\"max_depth\": [None,3,5,10,6,7],\n\"min_samples_split\":np.arange(2,20),\n\"min_samples_leaf\":np.arange(1,20),\n\"max_features\":[\"auto\",\"sqrt\"]}\nestimator = RandomForestClassifier()\nrs_rf = RandomizedSearchCV(\n    estimator, param_distributions=rf_grid, \n    n_iter=90,\n    cv=5,\n    verbose=True)\n","863cbeaa":"rs_rf.fit(x_train,y_train)","ab32d5eb":"rs_rf.best_params_","aae1710d":"print(f\"Before tuning:{model_scores['Random Forest']*100} after tuning:{rs_rf.score(x_test,y_test)*100}\")","59a27c3e":"gb_grid = {\"n_estimators\":np.arange(200,800),\n\"learning_rate\":[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],\n\"max_depth\":[3,4,5,9,6],\n\"subsample\":[0.8,0.884,0.9,1,0.7,0.5,2],\n\"min_samples_leaf\":[20,10,50,30],\n\"max_features\":[\"sqrt\"],\n\"max_depth\":[5,6,7,8]}\n\nestimator = GradientBoostingClassifier()\nrs_gb = RandomizedSearchCV(\n    estimator, param_distributions=gb_grid, \n    n_iter=90,\n    cv=5,\n    n_jobs = -1,\n    verbose=True)\nrs_gb.fit(x_train,y_train)\n","71d11f21":"rs_gb.best_params_","03d6e901":"print(f\"Before tuning:{model_scores['Gradient_boosting']*100} after tuning:{rs_gb.score(x_test,y_test)*100}\")","20680e7c":"def cross_val_plot(model,x,y,CV):\n    cv_acc = np.mean(cross_val_score(model,x,y,cv=CV,scoring=\"accuracy\")*100)\n    cv_acc_con = float(format(cv_acc,\".2f\"))\n    \n    cv_precision =np.mean(cross_val_score(model,x,y,cv=CV,scoring=\"precision\")*100)\n    cv_precision_con = float(format(cv_precision,\".2f\"))\n    \n    cv_f1 = np.mean(cross_val_score(model,x,y,cv=CV,scoring=\"f1\")*100)\n    cv_f1_con = float(format(cv_f1,\".2f\"))\n    \n    cv_recall = np.mean(cross_val_score(model,x,y,cv=CV,scoring=\"recall\")*100)\n    cv_recall_con = float(format(cv_recall,\".2f\"))\n    \n    empty_dict = {\"accuracy\":cv_acc_con,\n                 \"precision\":cv_precision_con,\n                 \"f1\":cv_f1_con,\n                 \"recall\":cv_recall_con}\n    score_dict = pd.DataFrame(empty_dict,index=[\"score\"])\n    ploting = score_dict.T.plot.bar()\n    plt.title(f\"Accuracy:{cv_acc_con},Precision:{cv_precision_con},f1:{cv_f1_con},recall:{cv_recall_con}\")\n    return ploting","dd5a411a":"model_xgb = xgb.XGBClassifier()\ncross_val_plot(model_xgb,x_ros,y_ros,5)","b90e5551":"model_xgb.fit(x_train,y_train)\ny_preds_xgb = model_xgb.predict(x_test)\nsns.set(font_scale=1.5)\ndef plot_conf_mat(y_test,y_preds):\n    fig,ax = plt.subplots(figsize=(3,3))\n    ax = sns.heatmap(confusion_matrix(y_test,y_preds),\n    annot=True,\n    cbar=True)\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predicted label\")\n\nplot_conf_mat(y_test,y_preds_xgb)","2ebe6e1a":"plot_roc_curve(model_xgb,x_test,y_test);","74a55d3c":"print(classification_report(y_test,y_preds_xgb))","2f3e4b0b":"model_rf = RandomForestClassifier(n_estimators= 531,\n min_samples_split= 2,\n min_samples_leaf= 1,\n max_features= \"sqrt\",\n max_depth= 10)\nmodel_rf.fit(x_train,y_train)\ny_preds_rf = model_rf.predict(x_test)","c24db517":"cross_val_plot(model_rf,x_ros,y_ros,5);","008f9ba5":"plot_conf_mat(y_test,y_preds_rf)","df6a8296":"plot_roc_curve(model_rf,x_test,y_test);","e42c9cdd":"print(classification_report(y_test,y_preds_rf))","0490d68c":"clf_lgbm = lgb.LGBMClassifier(bagging_freq=5, boosting='dart', early_stopping_rounds=30,\n               is_unbalance=True, metric='auc', num_boost_round=300,\n               objective='binary')\nclf_lgbm.fit(x_train,y_train)\nclf_lgbm.score(x_test,y_test)*100\n","944c75f6":"cross_val_plot(clf_lgbm,x_ros,y_ros,5);","6e2890ea":"y_preds_lgbm = clf_lgbm.predict(x_test)","ca88622f":"plot_conf_mat(y_test,y_preds_lgbm);plot_roc_curve(clf_lgbm,x_test,y_test);\n","cc7dbd02":"print(f\"LGBM:{classification_report(y_test,y_preds_lgbm)}\"),print(f\"RANDOM FOREST:{classification_report(y_test,y_preds_rf)}\")","1ce33700":"print(f\"Random forest:{plot_conf_mat(y_test,y_preds_rf)}\"),print(f\"LGBM:{plot_conf_mat(y_test,y_preds_lgbm)}\")","b977d3c8":"final_model = RandomForestClassifier(n_estimators= 531,\n min_samples_split= 2,\n min_samples_leaf= 1,\n max_features= \"sqrt\",\n max_depth= 10)","89527c7d":"final_model.fit(x_train,y_train)\ntrain_score=final_model.score(x_train,y_train)*100\ntest_score = final_model.score(x_test,y_test)*100\nprint(f\"The test score is:{test_score}\")\nprint(f\"The train score is:{train_score}\")","880d15f8":"dump(final_model, 'Heart_attack_algorithm.joblib')","be791b41":"## Importing the data ","d059ab32":"### The corr() matrix","e6c3eb96":"## We have choosen RANDOM FOREST CLASSIFIER as our best estimator!","8d6be6e1":"Let's see the AGE columns distribution","ac4f7bc4":"### Let's see how well our lightgbm is doing!","1165bd05":"Now, we are going to fit our data into a algoritjm\/model .\nFor that we are gonna create a functio that will give us the accuracy of all models tha twe have chosen in one line!","eeb6b9dc":"Information about the data:\n\n\n    1.Sex - Gender of patient Male = 1, Female =0\n    2.Age - Age of patient\n    3.Diabetes - 0 = No, 1 = Yes\n    4.Anaemia - 0 = No, 1 = Yes\n    5.High_blood_pressure - 0 = No, 1 = Yes\n    6.Smoking - 0 = No, 1 = Yes\n    7.DEATH_EVENT - 0 = No, 1 = Yes\n \n\n","19657e25":"NICE! Random forest is giving us much better level of accuracy! ","1ba9869d":"## EDA(explorotary data analysis)","a47985d8":"Which one should we use?\nWell as we can see \nour best model  so far is LIGHTGBM,Xgboost,Random_forest!\n","b5c524c2":"Let's find out the unique values in our dataset!","88759323":"Cross validation !","14405ebb":"### Tuning Light gbm","78272843":"## Let's save our model!","c333416f":"In this dataset we don't need the time feature because it's not necessary and irrelevant!\nSo we are just gonna drop it","334ee298":"As we can see lightgbm is giving us a accuracy of 95% EXCEELENT!","865f6dc8":"## confusion matrix(XGB BOOST)","9b7eaef8":"Cross validation of random forest","108be846":"Let's visualize it!","066dbb85":"## Classification  report","14fad611":"## Random forest Evaluation!","37dd61a6":"ROC\/AUC curve of random forest ","684156d1":"Confusion matrix of random forest","83688bf7":"## MODELING","9552985d":"## Tuning Gradient boosting","f55091c7":"Let's see if we can improve our model's score.","40cde053":"# ROC AUC","12ff5341":"### XG BOOST tuning","4382dd3b":"As we can see there is a data imbalance in the dataset!","adfd67c4":"# Heart failure(death) model","bd998088":"### Let's evaluate our model ","3eb9283c":"For this we will use oversampling technique to balance our data!","86377507":"## Tuning Ramdom forest","c3cfd644":"## Hyperperameter tuning!","d91dd36c":"#### Requirements","bd0d245b":"We are gonna define a function which gonna plot the scores of cv!","35825242":"Let'S see which one's performence is better! XGBOOST vs RANDOMFOREST!","7904be4b":"WOAH! our Random forest classifier is doing best!","78d16d2c":"### XG boost evaluation","94f74ce8":"Classification report of Random forest classifier","f2befeaf":"Let's focus on our EDA now!","e79a787c":"## The models we will work with:\n### 1.XG boost,\n### 2.lightgbm,\n### 3.Random forest,\n### 4.Gradient boosting","352d7964":"# Thank you!","6d4a0eb2":"Releaition between the the age and DEATH_EVENT","e6b7a66f":"### As we can see our Random forest classifier is doing great ! So we can say that we will choose RFclassifier over xgboost!","dd8d9bd3":"### There are some factors that affects Death Event. This dataset contains person's information like age ,sex , blood pressure, smoke, diabetes,ejection fraction, creatinine phosphokinase, serum_creatinine, serum_sodium, time and we have to predict their DEATH EVENT."}}