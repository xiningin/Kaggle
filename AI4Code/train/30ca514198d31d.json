{"cell_type":{"90ed5305":"code","b7aea3fc":"code","c303fe17":"code","657db614":"code","2b6c0906":"code","0d6b7fb0":"code","ac3f2a86":"code","3834d5a5":"code","2c0f024a":"code","af7d2d4e":"code","75b87857":"code","3028702f":"code","fd2689fa":"code","986cb1f7":"code","851e887a":"code","4d30acd3":"code","6a5a8578":"code","1567e609":"code","1721c2e0":"code","9d6cb21c":"code","1093f79b":"markdown","97f5e36a":"markdown","8dca71e7":"markdown","7a422073":"markdown"},"source":{"90ed5305":"import os, imp\ntry:\n    imp.find_module('kerastuner')\n    print('Kerastuner module is already installed')\nexcept:\n    os.system('pip install keras-tuner')\n    print('Keras tuner was not available but now it is installed')","b7aea3fc":"import numpy as np\nimport pandas as pd\nimport datetime\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.applications import InceptionV3  # as the input size is 224 x 224\nfrom tensorflow.keras.layers import Convolution2D, Dense, Dropout, Flatten, MaxPooling2D, Input, Lambda\nfrom tensorflow.keras.losses import categorical_crossentropy  # multiple classification\nfrom tensorflow.keras.activations import softmax, relu, sigmoid\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\nfrom kerastuner.tuners import RandomSearch\nimport tensorflow as tf","c303fe17":"rand_seed = 4\nmodel_path = r'\/kaggle\/working\/best_model.hdf5'","657db614":"def inp_path(rand_seed):\n    train_path = r'\/kaggle\/input\/100-bird-species\/train'\n    test_path = r'\/kaggle\/input\/100-bird-species\/test'\n    val_path = r'\/kaggle\/input\/100-bird-species\/valid'\n    cons_path = r'\/kaggle\/input\/100-bird-species\/consolidated'\n    birds_cat = os.listdir(cons_path)\n    all_path = [train_path, test_path, val_path]\n    return [train_path, test_path, val_path, birds_cat, all_path]","2b6c0906":"def create_label(rand_seed):\n    bird = []\n    count = []\n    folder = []\n    for fold in range(len(inp_path(rand_seed)[4])):\n        for cat in range(len(inp_path(rand_seed)[3])):\n            folder.append(inp_path(rand_seed)[4][fold])\n            bird.append(inp_path(rand_seed)[3][cat])\n            count.append(len([name for name in os.listdir(inp_path(rand_seed)[4][fold] + '\/' + inp_path(rand_seed)[3][cat])]))\n    return [folder, bird, count]","0d6b7fb0":"def inp_chk(rand_seed):\n    folder, bird, count = create_label(rand_seed)\n    df = pd.DataFrame({'Folder': create_label(rand_seed)[0], 'Bird': create_label(rand_seed)[1], 'Count': create_label(rand_seed)[2]})\n    print('Number of different Bird species in each folder\\n')\n    print(df.groupby(['Folder']).Bird.count())\n    print('\\nNumber of images in each folder\\n')\n    print(df.groupby(['Folder']).Count.sum())\n    print('\\nCount of each Bird species images in Training folder\\n')\n    print(df[df.Folder.str.contains('train')])\n    print('\\n\\n')","ac3f2a86":"def sample_disp(rand_seed):\n    gen = image.ImageDataGenerator().flow_from_directory(inp_path(rand_seed)[0], class_mode='binary', target_size=(224, 224))\n    x, y = gen.next()\n    idx = gen.class_indices.items()\n    img = []\n    label = []\n    for i in range(x.shape[0]):\n        img.append(image.array_to_img(x[i]))\n        for item in idx:\n            if item[1] == y[i]:\n                label.append(item[0])\n    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15, 12), dpi=100)\n    for pic, axes in enumerate(ax.flat):\n        axes.imshow(img[pic])\n        axes.set_title(label[pic])","3834d5a5":"def data_gen(rand_seed):\n    train_gen = image.ImageDataGenerator(rescale=1. \/ 255,\n                                         preprocessing_function=tf.keras.applications.inception_v3.preprocess_input).flow_from_directory(\n        inp_path(rand_seed)[0], target_size=(224, 224),\n        class_mode='categorical', batch_size=32)\n    test_gen = image.ImageDataGenerator(rescale=1. \/ 255,\n                                        preprocessing_function=tf.keras.applications.inception_v3.preprocess_input).flow_from_directory(\n        inp_path(rand_seed)[1], target_size=(224, 224),\n        class_mode='categorical', batch_size=32)\n    val_gen = image.ImageDataGenerator(rescale=1. \/ 255,\n                                       preprocessing_function=tf.keras.applications.inception_v3.preprocess_input).flow_from_directory(\n        inp_path(rand_seed)[2], target_size=(224, 224),\n        class_mode='categorical', batch_size=32)\n    return [train_gen, val_gen, test_gen]","2c0f024a":"def plot(trained):\n\n    epoch = []\n    tacc = []\n    vacc = []\n    tloss = []\n    vloss = []\n    train = True\n    tf.keras.backend.clear_session()\n    while train:        \n        history = trained\n        for t_acc  in history.history['acc']:\n            tacc.append(t_acc)\n        for v_acc in history.history['val_acc']:\n            vacc.append(v_acc)\n        for t_loss in history.history['loss']:\n            tloss.append(t_loss)\n        for v_loss in history.history['val_loss']:\n            vloss.append(v_loss)\n        train=False\n    epoch_count = len(tacc)\n    for i in range(epoch_count):\n        epoch.append(i+1)\n    plt.style.use('fivethirtyeight')\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n    axes[0].plot(np.array(epoch), np.array(tloss), 'r', label='Training Loss')\n    axes[0].plot(np.array(epoch), np.array(vloss), 'g', label='Validation Loss')\n    axes[0].set_title('Training Vs Validation Loss')\n    axes[0].set_xlabel('Epochs')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[1].plot(np.array(epoch), np.array(tacc), 'r', label='Training Accuracy')\n    axes[1].plot(np.array(epoch), np.array(vacc), 'g', label='Validation Accuracy')\n    axes[1].set_title('Training Vs Validation Accuracy')\n    axes[1].set_xlabel('Epochs')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].legend()\n    plt.tight_layout()\n    plt.show()","af7d2d4e":"def prediction(best_model):\n    pred_model = load_model(best_model)\n    print('Evaluating Model now\\n')\n    evaluation = pred_model.evaluate(data_gen(rand_seed)[2], verbose=0)\n    print('\\nThe Model accuracy is: {}%'.format(round(evaluation[1]*100, 2)))\n    #pred_gen = pred_model.predict(data_gen(rand_seed)[2])\n    #print(pred_gen)\n","75b87857":"def tuner_model(hp):\n    init_model = InceptionV3(include_top=False, weights='imagenet', input_tensor=Input(shape=(224, 224, 3)))\n    flat_1 = Flatten()(init_model.output)\n    for i in range(hp.Int('num_layers', 1, 6)):\n        model_in = Dense(units=hp.Int('units', min_value=128, max_value=1024, step=32, default=512), activation=relu,\n                         name='First_FC_Layer')(flat_1)   \n\n    output_layer = Dense(units=200, activation=softmax, name='Output_Layer')(model_in)\n    tune_model = Model(inputs=init_model.input, outputs=output_layer)\n    print(\"Number of Layers Built:\", len(tune_model.layers))\n    tune_model.compile(\n        optimizer=SGD(hp.Choice('lr', values=[18e-4, 185e-4, 19e-4, 195e-4, 20e-4, 15e-4], default=15e-4),\n                      momentum=hp.Choice('momentum', [0.4, 0.6, 0.7, 0.8, 0.9],\n                                         default=0.8)),\n        loss=categorical_crossentropy, metrics=['acc'])\n    return tune_model","3028702f":"def best_mod(rand_seed):\n    tf.keras.backend.clear_session()\n    start_time = datetime.datetime.now()\n    print('Tuning start time: ', start_time.strftime('%d-%b-%Y %I:%M:%S%p'))\n    tuner = RandomSearch(tuner_model, objective='val_acc', max_trials=2, executions_per_trial=2, project_name='ML_git', seed=rand_seed)\n    tuner.search(data_gen(rand_seed)[0], epochs=5, validation_data=data_gen(rand_seed)[1], steps_per_epoch=len(data_gen(rand_seed)[0]), verbose=0)\n    params = tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n    model = tuner.get_best_models()[0]\n    stop_time = datetime.datetime.now()\n    print('Time taken to complete the tuning is:', str((stop_time - start_time).seconds\/\/3600) + ' Hours ' + str((stop_time - start_time).seconds\/\/60) +' Mins '\n    + str((stop_time - start_time).seconds - ((stop_time - start_time).seconds\/\/60) * 60) + ' Sec' )\n    return model","fd2689fa":"model = best_mod(rand_seed)","986cb1f7":"tf.keras.callbacks.CSVLogger(r'\/kaggle\/working\/training.log', append=False, separator=',')\n# This is optional whether you would like to save the verbose\/ non verbose log details of the model training.","851e887a":"class epoch_average_print(tf.keras.callbacks.Callback):\n    \n    def on_epoch_begin(self, epoch, logs=None):\n        print('Epoch {:d}, learning rate: {:.7f}'.format(epoch + 1, tf.keras.backend.get_value(self.model.optimizer.lr)),'\\n')\n    \n    \n        \n    def on_epoch_end(self, epoch, logs=None): # function keywords  are predefined as per keras. visit 'ref' link below to know more\n        print('Params value at end of epoch, training loss: {:.7f}, val loss: {:.7f}, train acc: {:.7f}, val acc: {:.7f}.'\n              .format(logs['loss'], logs['val_loss'], logs['acc'], logs['val_acc']), '\\n')\n        \n        # Taking the average values to display as it would help to understand whether there's big increase or decrease in the values.\n        # Note - Don't need to use average function as the inbuilt function stores the average value in the parameters \n        # ref - https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/BaseLogger","4d30acd3":"class stopearly(tf.keras.callbacks.Callback):\n    \n    def __init__(self, patience = 2):\n        # patience defines the number of epoch with no improvement seen, after which the training will be stopped. \n        # I would like to give the training one buffer epoch to improve its accuracy and losses ;) \n        self.patience = patience\n\n        \n    def on_train_begin(self, logs=None):\n        self.wait = 0 # The number of epochs passed when the loss is not declining\n        self.stopped_epoch = 0 # The epoch at which the training is stopped\n        self.best_loss = np.inf # setting the loss to infinity as it would help us to compare with training loss\n    \n    \n    def on_epoch_end(self, epoch, logs=None):\n        if logs.get('loss') < self.best_loss:\n            self.best_loss = logs.get('loss')\n            self.wait = 0\n        else:\n            self.wait = self.wait + 1\n            if self.wait > self.patience:\n                self.stopped_epoch = epoch\n                self.model.stop_training = True # stopping the training \n                print('Model Training stopped as the training loss is plateaued. The model is saved with best weights before the plateauing of loss')\n    \n    \n    def on_train_end(self, logs=None):\n        if self.stopped_epoch > 0:\n            print('The model is stopped at epoch: {}'.format(self.stopped_epoch + 1))","6a5a8578":"def modelcheck():\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_acc', save_best_only=True, mode='max')\n    return checkpoint","1567e609":"class adjustlr(tf.keras.callbacks.Callback):\n    tr_high_acc = 0 # setting 0 to highest training accuracy\n    tr_low_loss = np.inf # setting lowest training loss to infinity\n    val_high_acc = 0 # setting 0 to highest validation accuracy\n    low_v_loss = np.inf # setting lowest validation loss to infinity \n    epochs = 0\n    model = model\n    best_weights = None\n    best_model = None\n    int_model = None\n    lr = float(tf.keras.backend.get_value(model.optimizer.lr))\n    \n    \n    def __init__(self):\n        super(adjustlr, self).__init__()\n        self.tr_high_acc = 0\n        self.low_v_loss = np.inf\n        self.model = model\n        try:\n            self.best_weights = self.model.get_weights()\n        except:\n            self.best_weights = None\n        self.int_model = None\n        self.epochs = 0\n        self.tr_low_loss = np.inf\n        self.lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n\n        \n    def on_epoch_end(self, epoch, logs=None): # Function naming format is available in the tensorflow site. If you use anyother format, it won't recognise the function.\n        # as per tf.keras documentation, epochs return as dict of parameters' average  value per particular epoch. \n        val_loss = logs.get('val_loss') # getting the average validation loss for this epoch \n        tr_loss = logs.get('loss') # getting the average validation loss for this epoch\n        val_acc = logs.get('val_acc') # getting the average validation accuracy for this epoch\n        tr_acc = logs.get('acc') # getting the average training accuracy for this epoch\n        adjustlr.lr = float(tf.keras.backend.get_value(self.model.optimizer.lr)) # fetching the lr value used in this epoch\n        adjustlr.epochs = adjustlr.epochs + 1\n        # checking whether the current epoch's training accuracy is better than previous training accuracy\n        if adjustlr.tr_high_acc < tr_acc: \n            adjustlr.tr_high_acc = tr_acc \n        # checking whether the current epoch's validation accuracy is better than previous validation accuracy\n        if adjustlr.val_high_acc < val_acc:\n            adjustlr.val_high_acc = val_acc\n        # checking whether the current epoch's validation loss is better than previous validation loss\n        if adjustlr.low_v_loss > val_loss:\n            adjustlr.low_v_loss = val_loss\n        # checking whether the current epoch's training loss is better than previous validation loss\n        if adjustlr.tr_low_loss > tr_loss:\n            adjustlr.tr_low_loss = tr_loss\n        # LR Adjustment -1: checks if the training accuracy is less than 0.95 then adjust the LR to improve it  \n        if tr_acc <= 0.95 and tr_acc < adjustlr.tr_high_acc:\n            lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n            adjusted_lr = lr * 0.8 * (tr_acc \/ adjustlr.tr_high_acc)\n            tf.keras.backend.set_value(self.model.optimizer.lr, adjusted_lr) \n            print(\"The current Training accuracy {:.7f} is lower than previous Training accuracy {:.7f}, hence reducing the LR to {:.7f}\\n\".format(tr_acc, adjustlr.tr_high_acc, adjusted_lr))\n            self.model.set_weights(adjustlr.best_weights)\n            print('Loading back the previous best model\\n')\n        # LR Adjustment -2 : if the Validation loss increases, adjust LR to improve the validation loss - Avoid Overfitting\n        elif tr_loss > adjustlr.tr_low_loss and tr_acc <= 0.95:\n            lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n            adjusted_lr = lr * 0.8 * (adjustlr.tr_low_loss \/ tr_loss)\n            tf.keras.backend.set_value(self.model.optimizer.lr, adjusted_lr)\n            print(\"The current Training loss {:.7f} is higher than previous Training Loss {:.7f}, hence reducing the LR to {:.7f}\\n\".format(tr_loss, adjustlr.tr_low_loss, adjusted_lr))\n            self.model.set_weights(adjustlr.best_weights)\n            print('Loading back the previous best model\\n')\n        # LR Adjustment -3 : if the Training loss increases, adjust LR to improve the loss - Avoid Underfitting\n        elif val_loss > adjustlr.low_v_loss and tr_acc <= 0.95:\n            lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n            adjusted_lr = lr * 0.8 * (adjustlr.low_v_loss \/ val_loss)\n            tf.keras.backend.set_value(self.model.optimizer.lr, adjusted_lr)\n            print(\"The current Validation loss {:.7f} is higher than previous validation loss {:.7f}, hence reducing the LR to {:.7f}\\n\".format(val_loss, adjustlr.low_v_loss, adjusted_lr))\n            self.model.set_weights(adjustlr.best_weights)             \n            print('Loading back the previous best model\\n')\n            \n        else:\n            adjustlr.best_weights = self.model.get_weights()","1721c2e0":"def train_model(rand_seed):\n    tf.keras.backend.clear_session()\n    start_time = datetime.datetime.now()\n    print('Training start time: ', start_time.strftime('%d-%b-%Y %I:%M:%S%p'))\n    train = model.fit(x = data_gen(rand_seed)[0], validation_data = data_gen(rand_seed)[1], steps_per_epoch = len(data_gen(rand_seed)[0]), epochs=20, \n                               validation_steps = len(data_gen(rand_seed)[1]), verbose=0, callbacks = [epoch_average_print(), adjustlr(), stopearly(), modelcheck()])\n    stop_time = datetime.datetime.now()\n    print('Time taken to complete the training is: ', str((stop_time - start_time).seconds\/\/3600) + ' Hours ' + str((stop_time - start_time).seconds\/\/60) +' Mins '\n    + str((stop_time - start_time).seconds - ((stop_time - start_time).seconds\/\/60) * 60) + ' Sec')\n    plot(train)\n    prediction(model_path)\n    ","9d6cb21c":"train_model(rand_seed)","1093f79b":"* # Summary\n* I have tried to implement pretrained InceptionV3 model to classify the birds. It is logical to implement the pretrained model which has been trained with more than 23Mil parameters rather than creating a model from scratch. \n* I have followed below steps to create the model apart from basic file\/image checking and all. \n* \n* 1. Create a Model using InceptionV3\n* 2. Use Kerastuner to find the optimal config\n* 3. Train the model using the model resulted from tuning \n    *     i) Use callbacks to have control over the training process and improve the model if required. **This step is absolutely necessary** \n* 4. plot accuracy and loss graph. \n5. Evaluate the model with testset and predict ","97f5e36a":"The training loss is plateaued around 0.000 and validation loss is around 0.14. The training accuracy is 100% whereas the validation accuracy is 98%","8dca71e7":"# KerasTuner\nKeras-tuner module is hyperparameter tuning module, that helps you to find the best number of layers, optimizer, learning rate, number of FC layers, number of neurons. Two below urls may help if you would like to know better about it \n1. https:\/\/www.sicara.ai\/blog\/hyperparameter-tuning-keras-tuner\n2. https:\/\/www.curiousily.com\/posts\/hackers-guide-to-hyperparameter-tuning\/","7a422073":"Initilizing the random seed to have consistent results and model path to store the best model from the training below "}}