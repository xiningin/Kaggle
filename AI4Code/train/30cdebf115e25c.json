{"cell_type":{"1c237e4f":"code","bffb78f9":"code","110987aa":"code","c01b059a":"code","032c2a9a":"code","f42b175b":"code","95355079":"code","69dfce7b":"code","34335b30":"code","7d3a7b1c":"code","bd392f01":"code","9f77a34e":"code","6b105ec0":"code","16c50963":"code","c45793fa":"code","2ad58b88":"code","6d670eb5":"markdown","3055c3d0":"markdown","6c9a9ad8":"markdown","60ef21a8":"markdown","f563fbb3":"markdown","3dba187c":"markdown","a6862fbf":"markdown","18af0aef":"markdown","468dbb9e":"markdown","d18b4259":"markdown","f2ca5593":"markdown","0082b38f":"markdown","affc4df8":"markdown","78e3627b":"markdown","e16573ef":"markdown","577a83aa":"markdown"},"source":{"1c237e4f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bffb78f9":"# from transformers import *\n# import tokenizers\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub","110987aa":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\nimport tokenization","c01b059a":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for i, text in enumerate(texts):\n        \n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n        if i%100 == 0:\n            print(i,\"\/\", len(texts))\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","032c2a9a":"def build_model(bert_layer, max_len=512):\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n\n#     # instantiate a distribution strategy\n#     tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n#     # instantiating the model in the strategy scope creates the model on the TPU\n#     with tpu_strategy.scope():\n\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n\n    return model","f42b175b":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/2\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","95355079":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","69dfce7b":"train =pd.read_csv('..\/input\/fake-news\/train.csv')\ntrain.info()\ntrain.fillna('null', inplace = True)","34335b30":"test = pd.read_csv('..\/input\/fake-news\/test.csv')\ntest.info()\ntest.fillna('null', inplace=True)","7d3a7b1c":"train_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.label.values","bd392f01":"np.save('train_input.npy', train_input)\nnp.save('test_input.npy', test_input)","9f77a34e":"# a = bert_encode(train.text.values[:1], tokenizer, max_len=160)\n# a = (tf.convert_to_tensor(a[0], tf.int32), tf.convert_to_tensor(a[1], tf.int32), tf.convert_to_tensor(a[2], tf.int32))","6b105ec0":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","16c50963":"train_history = model.fit(\n    [train_input[0], train_input[1], train_input[2]], train_labels,\n    validation_split=0.2,\n    epochs=2,\n    batch_size=8,\n    verbose = 2\n)\n\nmodel.save('model.h5')","c45793fa":"a = test\na['label'] = model.predict([test_input[0], test_input[1], test_input[2]])\na['label'] = a['label'].apply(round)\na.head()\na[['id', 'label']].to_csv('submission1.csv',index=False)","2ad58b88":"# # a = bert_encode(train.text.values[:2], tokenizer, max_len=160)\n# max_len = 10\n# input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n# input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n# segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n# _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n# clf_output = sequence_output[:, 0, :]\n# out = Dense(1, activation='sigmoid')(clf_output)\n# model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n\n# model(a)","6d670eb5":"# config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n# bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n# bert_model(x_train, attention_mask=att_tok, token_type_ids=tok)","3055c3d0":"y[:10]","6c9a9ad8":"for i, label in enumerate(y):\n    print(i, label)\n","60ef21a8":"PATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt',\n    add_prefix_space=True,\n    lowercase=True\n)","f563fbb3":"x_test = np.zeros((m_test, MAX_LEN), dtype='int32')\natt_tok_t = np.ones((m_test, MAX_LEN), dtype='int32')\ntok_t = np.zeros((m_test, MAX_LEN), dtype='int32')\nfor k in range(m_test):\n    text = \" \"+\" \".join(test.loc[k,'text'].split())\n    author = \" \"+\" \".join(test.loc[k, 'author'].split())\n    title = \" \"+\" \".join(test.loc[k,'title'].split())\n#     embeds = [0] + tokenizer.encode(title).ids + [2, 2] + tokenizer.encode(author).ids + [2, 2] + tokenizer.encode(text).ids + [2]\n    embeds = [0] + tokenizer.encode(text).ids + [2]\n    if len(embeds)<MAX_LEN:\n        att_tok_t[k, len(embeds):] = 0\n        embeds += (MAX_LEN-len(embeds)) * [0]\n    x_test[k] = embeds[:MAX_LEN]\n    ","3dba187c":"y = df['label']","a6862fbf":"y_train = np.ones((m, 2), dtype='int32')\n\nfor i, label in enumerate(y):\n    if label==0:\n        y_train[i, 1] = 0\n    else:\n        y_train[i, 0] = 0","18af0aef":"x_train\n","468dbb9e":"MAX_LEN = 512\nx_train = np.zeros((m, MAX_LEN), dtype='int32')\natt_tok = np.ones((m, MAX_LEN), dtype='int32')\ntok = np.zeros((m, MAX_LEN), dtype='int32')\nfor k in range(m):\n    text = \" \"+\" \".join(df.loc[k,'text'].split())\n    author = \" \".join(df.loc[k,'author'].split())\n    title = \" \"+\" \".join(df.loc[k, 'title'].split()) \n#     embeds = [0] + tokenizer.encode(title).ids + [2, 2] + tokenizer.encode(author).ids + [2, 2] + tokenizer.encode(text).ids + [2]\n    embeds = [0] + tokenizer.encode(text).ids + [2]\n    if len(embeds)<MAX_LEN:\n        temp_len = (MAX_LEN-len(embeds))\n        att_tok[k, len(embeds):] = 0 \n        embeds += temp_len * [0]\n        \n    x_train[k] = embeds[:MAX_LEN]\n    \ny_train = df['label']","d18b4259":"y_train[0:10]","f2ca5593":"m = df.shape[0]\nm","0082b38f":"y_train[0]","affc4df8":"def build_model():\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n\n    # instantiate a distribution strategy\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    with tpu_strategy.scope():\n        ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n        att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n        tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n        config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n        bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n        x = bert_model(ids, attention_mask=att, token_type_ids=tok)\n\n        x1 = tf.keras.layers.Dropout(0.2)(x[0]) \n        x1 = tf.keras.layers.Conv1D(1,1)(x1)\n        x1 = tf.keras.layers.Flatten()(x1)\n        x1 = tf.keras.layers.Dense(2, activation = 'softmax')(x1)\n\n        model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1])\n        optimizer = tf.keras.optimizers.Adam()\n        model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n    return model","78e3627b":"model.predict([x_test[0:10], att_tok_t[0:10], tok_t[0:10]])","e16573ef":"model = build_model()\nmodel.fit([x_train[:16000], att_tok[:16000], tok[:16000]], [y_train[:16000]], epochs = 5, batch_size = 100, verbose=1, validation_data=[[x_train[16000:], att_tok[16000:], tok[16000:]], [y_train[16000:]]])","577a83aa":"m_test = test.shape[0]"}}