{"cell_type":{"b054faa4":"code","3f7fde61":"code","f5efa107":"code","e353abab":"code","488ee76e":"code","09844b3a":"code","f2d0c9b4":"code","dd1aae7c":"code","54d42d2b":"code","1239b7a6":"code","d67f7094":"code","3b3c7788":"code","f340cb70":"code","4c827ca1":"code","ae35d155":"code","89d243b5":"code","6a8fcf61":"code","ba7e378f":"code","1441ca88":"code","cf7ce2ad":"code","190eceff":"code","9e89c6e7":"code","8da58b22":"code","b84c4c65":"code","335cd492":"code","656861b8":"code","3c297b70":"code","19a2be1b":"code","8a7c7ced":"code","ad5a629b":"code","9bb3328c":"code","bc2ece3e":"code","2f5cdbd2":"code","d29996a6":"markdown","55fe81ec":"markdown","0d0add5f":"markdown","49476a30":"markdown","9a626cc0":"markdown","f57703b8":"markdown","8e2d88f1":"markdown","66821575":"markdown","cbe5df72":"markdown","a5f9a7ec":"markdown","54cd32fb":"markdown","672568d1":"markdown","7a09ddc8":"markdown","105e54ae":"markdown","24fb1f77":"markdown","b4602041":"markdown","e7e41415":"markdown"},"source":{"b054faa4":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_predict, cross_validate, cross_val_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom xgboost import XGBClassifier\nfrom tqdm import tqdm","3f7fde61":"input_dir = '\/kaggle\/input\/lish-moa'\ntrain_features = pd.read_csv(os.path.join(input_dir, 'train_features.csv'))\ntrain_targets_scored = pd.read_csv(os.path.join(input_dir, 'train_targets_scored.csv'))\ntrain_targets_nonscored = pd.read_csv(os.path.join(input_dir, 'train_targets_nonscored.csv'))\ntest_features = pd.read_csv(os.path.join(input_dir, 'test_features.csv'))","f5efa107":"train_features.shape, train_targets_scored.shape, train_targets_nonscored.shape, test_features.shape","e353abab":"train_features.head(3)","488ee76e":"cat_cols = ['cp_type', 'cp_time', 'cp_dose']\n\nplt.figure(figsize=(16,4))\n\nfor idx, col in enumerate(cat_cols):\n    plt.subplot(int(f'13{idx + 1}'))\n    labels = train_features[col].value_counts().index.values\n    vals = train_features[col].value_counts().values\n    sns.barplot(x=labels, y=vals)\n    plt.xlabel(f'{col}')\n    plt.ylabel('Count')\nplt.tight_layout()\nplt.show()","09844b3a":"# select all indices when 'cp_type' is 'ctl_vehicle'\nctl_vehicle_idx = (train_features['cp_type'] == 'ctl_vehicle')\n\n# evaluate number of 1s we have in the total train scores when cp_type = ctl_vehicle\ntrain_targets_scored.loc[ctl_vehicle_idx].iloc[:, 1:].sum().sum()","f2d0c9b4":"# take a copy of all our training sig_ids for reference\ntrain_sig_ids = train_features['sig_id'].copy()","dd1aae7c":"# drop cp_type column since we no longer need it\nX = train_features.drop(['sig_id', 'cp_type'], axis=1).copy()\nX = X.loc[~ctl_vehicle_idx].copy()\n\ny = train_targets_scored.drop('sig_id', axis=1).copy()\ny = y.loc[~ctl_vehicle_idx].copy()","54d42d2b":"X.shape, y.shape","1239b7a6":"X.head(3)","d67f7094":"plt.figure(figsize=(8,5))\nsns.distplot(X.iloc[:, 2:].mean())\nplt.show()","3b3c7788":"plt.figure(figsize=(8,5))\nsns.distplot(y.mean())\nplt.show()","f340cb70":"plt.figure(figsize=(8,5))\nsns.distplot(train_targets_nonscored.mean())\nplt.show()","4c827ca1":"train_targets_scored.head(3)","ae35d155":"train_targets_nonscored.head(3)","89d243b5":"y.sum().sort_values()[:30].plot.bar(figsize=(18,6))\nplt.show()","6a8fcf61":"class DataProcessor(BaseEstimator, TransformerMixin):\n    \"\"\" Data preprocessor and loader \"\"\"\n    \n    def __init__(self, rm_ctl_vehicle=True, std_features=True, encode_cat=True,\n                 cat_cols=['cp_time', 'cp_dose']):\n        self.rm_ctl_vehicle = rm_ctl_vehicle\n        self.std_features = std_features\n        self.cat_cols = cat_cols\n        self.encode_cat = encode_cat\n        \n        \n    def fit(self, X, y=None):\n\n        return self\n                \n    \n    def transform(self, X):\n        \"\"\" Process features from our given data \"\"\"\n        \n        new_df = self._remove_features(X).copy()\n        \n        if self.std_features:\n            # standardise numerical cols\n            pass\n        \n        if self.encode_cat:\n            \n            # one-hot encode cat vars\n            new_df = pd.concat([pd.get_dummies(new_df.cp_dose, prefix='cp_dose'), \n                                new_df.drop('cp_dose', axis=1)], axis=1)\n            \n            new_df = pd.concat([pd.get_dummies(new_df.cp_time, prefix='cp_time'),\n                                new_df.drop('cp_time', axis=1)], axis=1)\n            \n        return new_df\n    \n    \n    def _remove_features(self, dataframe):\n        \"\"\" Remove unwanted features from our dataframes \"\"\"\n        \n        if self.rm_ctl_vehicle:\n            new_df = dataframe.drop(['sig_id', 'cp_type'], axis=1)\n        \n        else:\n            new_df = dataframe.drop('sig_id', axis=1)\n        \n        return new_df\n    \n    \n    def _standardise_features(self, dataframe):\n        pass","ba7e378f":"# take a copy of all our training sig_ids for reference\ntrain_sig_ids = train_features['sig_id'].copy()\n\n# select all indices when 'cp_type' is 'ctl_vehicle'\ntrain_ctl_vehicle_idx = (train_features['cp_type'] == 'ctl_vehicle')\n\n# initialise class and use to transform our dataset\ndata_processor = DataProcessor()\n\n# remove unwanted feats and encode categorical\nX = data_processor.fit_transform(train_features)\ny = train_targets_scored.drop('sig_id', axis=1).copy()\n\n# remove cases where cp_type is ctl_vehicle from train features and targets\nX = X.loc[~train_ctl_vehicle_idx].copy()\ny = train_targets_scored.drop('sig_id', axis=1).copy()\ny = y.loc[~train_ctl_vehicle_idx].copy()\n\n# standardise our numerical columns only\nstd_scaler = StandardScaler()\nnum_cols = [x for x in X.columns.values if not x.startswith(('cp_time', 'cp_dose'))]\nX_std = X.copy()\nX_std[num_cols] = std_scaler.fit_transform(X.loc[:, num_cols])","1441ca88":"X_std.head(3)","cf7ce2ad":"lin_reg = LinearRegression()","190eceff":"# evaluate using cross-validation\nlin_reg = LinearRegression()\nlr_val_preds = cross_val_predict(lin_reg, X_std, y, cv=5)\n\n# in order to effective work out log loss, we need to flatten both arrays before computing log loss\nlr_log_loss = log_loss(np.ravel(y), np.ravel(lr_val_preds))\nprint(f\"Log loss for our Linear Regression Model: {lr_log_loss:.5f}\\n\")","9e89c6e7":"n_range = [1, 2, 5, 10, 25, 50, 100, 150, 200, 250]\nlog_losses = []\n\nlin_reg = LinearRegression()\n\nfor n in n_range:\n    pca = PCA(n_components=n)\n    lr_model = Pipeline(steps=[('pca', pca), ('linear regression', lin_reg)])\n    \n    # evaluate using cross-validation\n    lr_val_preds = cross_val_predict(lr_model, X_std, y, cv=5)\n\n    # in order to effective work out log loss, we need to flatten both arrays before computing log loss\n    lr_log_loss = log_loss(np.ravel(y), np.ravel(lr_val_preds))\n    print(f\"Log loss for Linear Regression with PCA (n={n}): {lr_log_loss:.5f}\\n\")\n    \n    log_losses.append(lr_log_loss)","8da58b22":"plt.figure(figsize=(14,5))\nsns.lineplot(x=n_range, y=log_losses)\nplt.ylabel(\"Average Log Loss\", weight='bold')\nplt.xlabel(\"PCA n components\", weight='bold')\nplt.grid()\nplt.show()","b84c4c65":"# take a copy of all our training sig_ids for reference\ntest_sig_ids = test_features['sig_id'].copy()\n\n# select all indices when 'cp_type' is 'ctl_vehicle'\ntest_ctl_vehicle_idx = (test_features['cp_type'] == 'ctl_vehicle')\n\nX_test = data_processor.transform(test_features)","335cd492":"# standardise our numerical columns only using the training standard scaler\ntest_num_cols = [x for x in X_test.columns.values if not x.startswith(('cp_time', 'cp_dose'))]\nX_test_std = X_test.copy()\nX_test_std[test_num_cols] = std_scaler.transform(X_test.loc[:, test_num_cols])","656861b8":"lin_reg = LinearRegression()\npca = PCA(n_components=5)\nlr_model = Pipeline(steps=[('pca', pca), ('linear regression', lin_reg)])\n%time lr_model.fit(X_std, y)","3c297b70":"%time test_preds = lr_model.predict(X_test_std)","19a2be1b":"test_preds[test_sig_ids[test_ctl_vehicle_idx].index.values].sum()","8a7c7ced":"# change all cp_type == ctl_vehicle predictions to zero\ntest_preds[test_sig_ids[test_ctl_vehicle_idx].index.values] = 0\n\n# confirm all values now sum to zero for these instances\ntest_preds[test_sig_ids[test_ctl_vehicle_idx].index.values].sum()","ad5a629b":"# we have some values above 1 and below 0 - this needs amending since probs should only be 0-1\ntest_preds.max(), test_preds.min()","9bb3328c":"# in addition, let's set all negative values to 0.0 and all values above 1 to 1.0\ntest_preds[test_preds > 1.0] = 1.0\ntest_preds[test_preds < 0.0] = 0.0\n\n# confirm these values are all corrected\ntest_preds.max(), test_preds.min()","bc2ece3e":"lr_test_submission = pd.DataFrame({'sig_id' : test_sig_ids})\nlr_test_submission[train_targets_scored.columns[1:]] = test_preds\nlr_test_submission.head(3)","2f5cdbd2":"# save our submission as csv\nlr_test_submission.to_csv('submission.csv', index=False)","d29996a6":"Not too bad for such a simple method of model production. Let's explore the impact \/ benefits of applying dimensionality reduction to our model. Usually, this results in a performance decrease, however for datasets with a large number of redundant features (which could be the case here), we may actuall get a performance increase. Let's see whether this is the case or not: ","55fe81ec":"## 1. Import dependencies and data","0d0add5f":"## 5. Baseline model predictions on the test set\n\nWe first need to preprocess our test set inputs so that it is consistent with regards to our training data.","49476a30":"## 3. Preprocessing","9a626cc0":"With this in the correct format, we can now save it and make a basic submission for the competition:","f57703b8":"## 4. Basic Model Exploration","8e2d88f1":"The total sum is zero, which confirms the statement above on all targets being zero for cases where cp_type is ctl_vehicle. The best thing to do with this is simply fill our targets for zero when this is the case.\n\nWe shall also remove all of these from the training set, since there is no need to unnecessarily complicate our model.","66821575":"# Basic exploration and submission using a classical multi-output linear model\n\nWithin this notebook the basics features of the data are explored, and subsequently a baseline model is produced using linear regression.\n\nWhy linear regression? In short, because it is extremely simple and quick to train for this multi-output task and gets a quick solution in place to understand the dataset. Although this is a classification task, we actually want output probabilities for the final submissions, and therefore we can actually make use of regression models rather than hard classification models (that produce integer labels).  \n\nThis is likely a weak model to choose for obtaining a good score, and we could undoubtedly get much better results using more suitable deep neural network variants and\/or ensemble methods.","cbe5df72":"Good, our data is now ready for producing a basic set of predictions on the test set using a baseline model.","a5f9a7ec":"## 2. Basic Exploratory Data Analysis","54cd32fb":"We now need to update all of the predictions for cp_type == ctl_vehicle so that they are zero.","672568d1":"Lets create our preprocessing functions, which include some of the exploration we conducted above and will start again from the raw dataset(s) as they are imported in.\n\nThis will include the following steps:\n\n- Take reference of the sig_id's and cp_types for our data\n- Remove the above two features \n- Remove all instances where cp_type == ctl_vehicle, since these will result in all zero targets\n- Encode our two categorical features cp_time and cp_dose\n- Standardise our numerical features prior to training","7a09ddc8":"It's interesting in this case that our dimensionality reduction actually appears to improve the generalisation performance of our model. Perhaps we have many redundant features within our high dimensional dataset. Let's now train this on the entire training set and make a prediction on the test set for the competition.","105e54ae":"We have only three categorical columns at the beginning, with the remainder of the columns being numerical.","24fb1f77":"For 'cp_type', the 'ctl_vehicle' refers to samples treated with a control perturbation. For control perturbations, our targets are all zero, since they have no Mechanism of Action (MoA).\n\nTo deal with this, a good strategy could be to identify samples that are ctl_vehicle (through training a classification model or simply using the feature as its in the test data!), and set all of these to zero. We can then process the test set accordingly, by first setting all test instance targets to zero if its a ctl_vehicle, followed by processing all of the others normally using our trained model.","b4602041":"The problem with using basic machine learning models for this multi-output classification task is that generally we need to train 200+ individual models, which can take an extremely long time if we can't parallelise these training operations.\n\nConversely, a neural network can handle this type of complexity with just one unified model, which makes it much more preferable for this competition.\n\nFor this simple example, we'll produce a simple linear regression model to provide output probabilities. This could be much improved upon using more advanced models, but this simple approach will suffice for this notebook.","e7e41415":"Some output classes only have 1 instance in the entire training set. This is problematic and is no where near enough data if we expect our models to effectively make predictions across the whole range of targets. Imbalanced dataset techniques such as minority class over-sampling may have to be introduced, which may help our models generalise better to new data."}}