{"cell_type":{"a05a08d6":"code","7eed6bc2":"code","e31fb0c7":"code","a62eae88":"code","9402ed47":"code","58d8ea71":"code","76e51849":"code","73fdce7e":"code","68ad8bde":"code","e2dc0557":"code","efa7c0dc":"code","09059176":"code","d5ce6227":"code","b9db04d4":"code","cb2ce190":"code","6a645861":"code","1828d362":"code","aa609158":"code","87ca8a89":"code","dc5d5dd3":"code","f590be2a":"code","16213cd5":"code","69a1ae94":"code","1c502d22":"code","45875a6f":"code","57ccdc29":"code","c5f54eae":"code","77f3a638":"code","0a37c112":"code","f15b3bf9":"code","c0cfe6ff":"code","bd73bd27":"code","4bf63dd5":"code","5c75dd4e":"code","902e0b01":"code","1b669ad8":"code","a2175667":"code","4f4c1bb5":"code","ff9d95e2":"code","8dfaf321":"code","2f59e165":"code","a97c2306":"code","a54cbc07":"markdown","22aeeb64":"markdown","6643b550":"markdown","47d78b45":"markdown","10c0ab3e":"markdown","cac3731d":"markdown","2b244730":"markdown","c0bf3380":"markdown","0683ffbe":"markdown","521043ce":"markdown","7bb4971a":"markdown","54dd1808":"markdown","12f62aba":"markdown","18afa592":"markdown","13573737":"markdown","0731d087":"markdown","6b8e9310":"markdown","468a7ffb":"markdown","d0c75696":"markdown","1b238e9e":"markdown","f32b9dd9":"markdown"},"source":{"a05a08d6":"#importing the required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","7eed6bc2":"#Get the dataset\ndataset = pd.read_csv(r'..\/input\/50-startups-by-superdatascience-team\/50_Startups.csv')","e31fb0c7":"#Get a glimpse of the Dataset\ndataset.head()","a62eae88":"#Separating the independent and dependent features\n#Dependent features\ny = np.asarray(dataset['Profit'].values.tolist()) \n\n# Independent Features\n# Now, our dataset has only independent features\ndataset.drop([\"Profit\"], axis = 1, inplace = True)","9402ed47":"# Handling the Categorical Variable \"State\" with the One Hot Encoding Technique\n# There are many ways of handling categorical variables, One Hot Encosing is one of them\n# First, we get the counts of each value that the feature \"State\" can take\ndataset.iloc[:,3].value_counts()","58d8ea71":"# Performing Label Encoding\n# Replacing California by 1, New York by 2, Florida by 3\ndataset.replace(to_replace=[\"California\",\"New York\", \"Florida\"], value=[1,2,3])","76e51849":"# There is no hierarchy relation between the labels\n# i.e label 3 does not have more importance than label 1\n# To handle this, we create 3 columns with the respective US State names\ndataset[\"California\"] = dataset.iloc[:, 3]\ndataset[\"New York\"] = dataset.iloc[:,3]\ndataset[\"Florida\"] = dataset.iloc[:,3]","73fdce7e":"#Let's have a look at the dataset now \ndataset.head()","68ad8bde":"# Performing One Hot Encoding for the column \"California\"\ndataset.loc[dataset[\"California\"]!=\"California\", \"California\"] = 0\ndataset.loc[dataset[\"California\"]==\"California\", \"California\"] = 1","e2dc0557":"# Performing One Hot Encoding for the column \"New York\"\ndataset.loc[dataset[\"New York\"]!=\"New York\", \"New York\"] = 0\ndataset.loc[dataset[\"New York\"]==\"New York\", \"New York\"] = 1","efa7c0dc":"# Performing One Hot Encoding for the column \"Florida\"\ndataset.loc[dataset.Florida!=\"Florida\", \"Florida\"] = 0\ndataset.loc[dataset.Florida==\"Florida\", \"Florida\"] = 1","09059176":"#Let's have a look at the Data\ndataset.head()","d5ce6227":"# Dropping the columns\ndataset.drop([\"State\",\"Florida\"], axis = 1, inplace = True)","b9db04d4":"dataset.head()","cb2ce190":"# Get the processed Independent features \nX = np.asarray(dataset.values.tolist())","6a645861":"#Get the shapes of X and y\nprint(\"The shape of the independent fatures are \",X.shape)\nprint(\"The shape of the dependent fatures are \",y.shape)","1828d362":"#Reshaping the Dependent features\ny = y.reshape(len(y),1) # Changing the shape from (50,) to (50,1)","aa609158":"#Feature Scaling for Independent Variables\nfor i in range(X.shape[1]-2):\n  X[:,i] = (X[:,i] - int(np.mean(X[:,i])))\/np.std(X[:,i])\n","87ca8a89":"#Feature Scaling for Dependent Variables\ny = (y - int(np.mean(y)))\/np.std(y)","dc5d5dd3":"#Adding the feature X0 = 1, so we have the equation: y =  (W1 * X1) + (W0 * X0) \nX = np.concatenate((X,np.ones((50,1))), axis = 1)","f590be2a":"X","16213cd5":"y","69a1ae94":"#Let's create a DataFrame \"Independent_Variables\" to visualize our final independent features\nIndpendent_Variables = pd.DataFrame(X)","1c502d22":"Indpendent_Variables","45875a6f":"# The method \"split_data\" splits the given dataset into trainset and testset\n# This is similar to the method \"train_test_split\" from \"sklearn.model_selection\"\ndef split_data(X,y,test_size=0.2,random_state=0):\n    np.random.seed(random_state)                  #set the seed for reproducible results\n    indices = np.random.permutation(len(X))       #shuffling the indices\n    data_test_size = int(X.shape[0] * test_size)  #Get the test size\n\n    #Separating the Independent and Dependent features into the Train and Test Set\n    train_indices = indices[data_test_size:]\n    test_indices = indices[:data_test_size]\n    X_train = X[train_indices]\n    y_train = y[train_indices]\n    X_test = X[test_indices]\n    y_test = y[test_indices]\n    return X_train, y_train, X_test, y_test","57ccdc29":"class multipleLinearRegression():\n\n  def __init__(self):\n    #No instance Variables required\n    pass\n\n  def forward(self,X,y,W):\n    \"\"\"\n    Parameters:\n    X (array) : Independent Features\n    y (array) : Dependent Features\/ Target Variable\n    W (array) : Weights \n\n    Returns:\n    loss (float) : Calculated Sqaured Error Loss for y and y_pred\n    y_pred (array) : Predicted Target Variable\n    \"\"\"\n    y_pred = sum(W * X)\n    loss = ((y_pred-y)**2)\/2    #Loss = Squared Error, we introduce 1\/2 for ease in the calculation\n    return loss, y_pred\n\n  def updateWeights(self,X,y_pred,y_true,W,alpha,index):\n    \"\"\"\n    Parameters:\n    X (array) : Independent Features\n    y_pred (array) : Predicted Target Variable\n    y_true (array) : Dependent Features\/ Target Variable\n    W (array) : Weights\n    alpha (float) : learning rate\n    index (int) : Index to fetch the corresponding values of W, X and y \n\n    Returns:\n    W (array) : Update Values of Weight\n    \"\"\"\n    for i in range(X.shape[1]):\n      #alpha = learning rate, rest of the RHS is derivative of loss function\n      W[i] -= (alpha * (y_pred-y_true[index])*X[index][i]) \n    return W\n\n  def train(self, X, y, epochs=10, alpha=0.001, random_state=0):\n    \"\"\"\n    Parameters:\n    X (array) : Independent Feature\n    y (array) : Dependent Features\/ Target Variable\n    epochs (int) : Number of epochs for training, default value is 10\n    alpha (float) : learning rate, default value is 0.001\n\n    Returns:\n    y_pred (array) : Predicted Target Variable\n    loss (float) : Calculated Sqaured Error Loss for y and y_pred\n    \"\"\"\n\n    num_rows = X.shape[0] #Number of Rows\n    num_cols = X.shape[1] #Number of Columns \n    W = np.random.randn(1,num_cols) \/ np.sqrt(num_rows) #Weight Initialization\n\n    #Calculating Loss and Updating Weights\n    train_loss = []\n    num_epochs = []\n    train_indices = [i for i in range(X.shape[0])]\n    for j in range(epochs):\n      cost=0\n      np.random.seed(random_state)\n      np.random.shuffle(train_indices)\n      for i in train_indices:\n        loss, y_pred = self.forward(X[i],y[i],W[0])\n        cost+=loss\n        W[0] = self.updateWeights(X,y_pred,y,W[0],alpha,i)\n      train_loss.append(cost)\n      num_epochs.append(j)\n    return W[0], train_loss, num_epochs\n\n  def test(self, X_test, y_test, W_trained):\n    \"\"\"\n    Parameters:\n    X_test (array) : Independent Features from the Test Set\n    y_test (array) : Dependent Features\/ Target Variable from the Test Set\n    W_trained (array) : Trained Weights\n    test_indices (list) : Index to fetch the corresponding values of W_trained,\n                          X_test and y_test \n\n    Returns:\n    test_pred (list) : Predicted Target Variable\n    test_loss (list) : Calculated Sqaured Error Loss for y and y_pred\n    \"\"\"\n    test_pred = []\n    test_loss = []\n    test_indices = [i for i in range(X_test.shape[0])]\n    for i in test_indices:\n        loss, y_test_pred = self.forward(X_test[i], W_trained, y_test[i])\n        test_pred.append(y_test_pred)\n        test_loss.append(loss)\n    return test_pred, test_loss\n    \n\n  def predict(self, W_trained, X_sample):\n    prediction = sum(W_trained * X_sample)\n    return prediction\n\n  def plotLoss(self, loss, epochs):\n    \"\"\"\n    Parameters:\n    loss (list) : Calculated Sqaured Error Loss for y and y_pred\n    epochs (list): Number of Epochs\n\n    Returns: None\n    Plots a graph of Loss vs Epochs\n    \"\"\"\n    plt.plot(epochs, loss)\n    plt.xlabel('Number of Epochs')\n    plt.ylabel('Loss')\n    plt.title('Plot Loss')\n    plt.show()\n  \n","c5f54eae":"#Splitting the dataset\nX_train, y_train, X_test, y_test = split_data(X,y)","77f3a638":"#declaring the \"regressor\" as an object of the class LinearRegression\nregressor = multipleLinearRegression()","0a37c112":"#Training \nW_trained, train_loss, num_epochs = regressor.train(X_train, y_train, epochs=200, alpha=0.0001)","f15b3bf9":"#Testing on the Test Dataset\ntest_pred, test_loss = regressor.test(X_test, y_test, W_trained)","c0cfe6ff":"#Plot the Train Loss\nregressor.plotLoss(train_loss, num_epochs)","bd73bd27":"import pandas\nimport matplotlib.pyplot as plt \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder","4bf63dd5":"#Get the Dataset and separating the independent and dependent features\ndataset_sk = pd.read_csv(r'..\/input\/50-startups-by-superdatascience-team\/50_Startups.csv')\nX_sk = dataset_sk.iloc[:, :-1].values\ny_sk = dataset_sk.iloc[:, 4].values","5c75dd4e":"# Performing One hot encoding using sklearn\nlabelencoder_X_sk = LabelEncoder()\nX_sk[:,3] = labelencoder_X_sk.fit_transform(X_sk[:,3])\n\nonehotencoder = OneHotEncoder(handle_unknown='ignore')\nX_sk_categorical = onehotencoder.fit_transform(X_sk[:,3].reshape(-1,1)).toarray()\nX_sk = np.concatenate((X_sk,X_sk_categorical),axis=1)","902e0b01":"# Dropping the \"State\" and \"California\" columns\nX_sk = X_sk[:, [0,1,2,5,6]]","1b669ad8":"X_sk.shape","a2175667":"# Splitting the dataset into the Training set and Test set\nX_train_sk, X_test_sk, y_train_sk, y_test_sk = train_test_split(X_sk, y_sk, test_size = 0.2, random_state = 0)","4f4c1bb5":"# Fitting Simple Linear Regression to the Training set\nregressor_sk = LinearRegression()\nregressor_sk.fit(X_train_sk, y_train_sk)","ff9d95e2":"# Predicting the Test set results\ny_pred = regressor_sk.predict(X_test_sk)","8dfaf321":"X_train_sk.shape","2f59e165":"#Making the Prediction using Sklearn Regression\nprint(regressor_sk.predict([[160000,140000,5000000,1,0]]))","a97c2306":"#Making a Prediction\npred = regressor.predict(W_trained,[160000,140000,5000000,1,0,1])\nprint(pred)","a54cbc07":"### Importing dependencies","22aeeb64":"## Multiple Linear Regression from Scratch without SKlearn","6643b550":"### Get the Data and Data Preprocessing","47d78b45":"### Utility Methods","10c0ab3e":"## Introduction","cac3731d":"## Comparing the Performance of the Two Regressors","2b244730":"In this notebook, first, we implement Multiple Linear Regression from Scratch using Numpy without Sklearn. After the scratch implementation, we also implement the Multiple Linear Regression using Sklearn and compare the two models. The complete code is written and executed in Google Colab. No need of installing any additional packages is required. Download this notebook and upload to Google Colab to run it by yourself. Don't forget to grab your Datasets! \n\nThe Dataset used here is the **50_Startups** dataset provided by the Super Data Science Team under their programme **Machine Learning A-Z: Hands on Python & R in Data Science**. Find the various datasets provided by them [here](https:\/\/www.superdatascience.com\/pages\/machine-learning).\n\n**Response Variable** : R&D Spend, Administration, Marketing Spend, State\n\n**Target Variable** : Profit\n\n**Equation Used** : y = W0 * X0 + W1 * X1 + W2 * X2 + .... + WN * XN\n\n**For Scratch Implementation:**\n    \n    Loss Function : Mean Squared Error\n\n    Optimization Algorithm : SGD\n\n    Weight Initialization : Xavier Initialization\n\n","c0bf3380":"## Multople Linear Regression using Sklearn","0683ffbe":"### Data Preprocessing","521043ce":"### Data Preprocessing Continued","7bb4971a":"#### Handling the Categorical Variable \"**State**\"","54dd1808":"### Visualizing Results","12f62aba":"Now, we replace the values of the particular US State of the particular US State Column by one and others by zero. This means, in the column California, we will replace all the California Values by 1 and the values New York, Florida by zero. This is called **One Hot Encoding**. We do this for each of the three columns. After this, we can drop our original column \"State\".\n\nAlso, note that to avoid \"**Dummy Variable Trap**\", we need to drop one of the three US State columns. Basically, Linear Regression has an assumption that all the Independent Variables(X), in the equation **y = W0 * X0 + W1 * X1 + W2 * X2 + ... WN * XN**, are independent of each other. If we don't drop one of the three columns, we violate this assumption.","18afa592":"The values used for Independent features resemble closely to the first set of features of our Dataset. Therefore, the prediction should be somewhere around 2,00,000. Therefore, SKlearn Regressor predicts better than our Naive Regressor.","13573737":"By looking at the US State columns and the \"State\" column, we can clearly see that we have successfully performed one hot encoding. Now, it's time to drop the \"State\" column and one of the three State columns. We will drop the column \"Florida\".","0731d087":"There can be various reasons behind these results like choice of loss function, Optimization Algorithm used, etc. However, the prime goal of this notebook was to demonstrate the implementation of Multiple Linear Regression and not to perform better than Sklearn. \n\nDefinitely, you can download this notebook and change hyperparameters, Optimization Algorithm, etc. and start your ML Journey!\n\n**Best of Luck!**\n\n","6b8e9310":"### Performing Linear Regression","468a7ffb":"### Importing the Dependencies","d0c75696":"### Performing the Linear Regression","1b238e9e":"### Coding the LinearRegression Class","f32b9dd9":"# ML Regression Algorithms from Scratch - Multiple Linear Regression "}}