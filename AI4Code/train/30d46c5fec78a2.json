{"cell_type":{"3f0532cd":"code","4a67e739":"code","616d2116":"code","9c518c36":"code","76cbd1b3":"code","721fbe12":"code","7d1495f3":"code","3bc5bec6":"code","7e9e68b3":"code","16ed1da5":"code","bdacb4a7":"code","779f0dcb":"code","79e36e0c":"code","4a5cf621":"code","a3db1bd0":"code","cce35c9a":"code","92eb177b":"code","6e9070a2":"code","4e753f44":"code","8e9682f0":"code","2f82fb62":"code","02c72fae":"code","eabf8cf1":"code","8d2f5302":"code","d454ed78":"code","c6c0030b":"code","548a2b86":"code","b34e7d40":"code","94604bf3":"code","582ffbf8":"code","6349854a":"code","f61d8abf":"code","8b290a0d":"code","cde05287":"code","d93ccd2f":"code","56eb85aa":"code","415c1175":"code","dfb7e380":"code","d48ab9c4":"code","fda37974":"code","4de0756f":"code","f013b2f4":"code","bacfcf23":"code","e67f97bc":"code","163be0c2":"code","e733b15c":"code","b5d4d747":"code","6c98a4e4":"code","db6145b9":"code","61ffc976":"code","a39baf18":"code","76901736":"code","a5ffce51":"code","83c8036b":"code","6652aec8":"code","7bd350ce":"code","a2534066":"code","82654b76":"code","2a576e2d":"code","e53de39a":"code","738c9b80":"code","4bb71f2f":"code","694a65d5":"code","af89a11b":"code","3c20ad62":"code","3dffc200":"code","491daa8a":"code","df301337":"code","cc813abd":"code","6d08fb87":"code","d90f220e":"code","185bc25c":"code","7c2171da":"code","5545a4df":"code","191e9f5e":"code","a6085ef5":"code","f4e8a917":"code","3f5c7ce3":"code","0cc2675d":"code","9ea4cb44":"code","9aaf5d6c":"code","be50ed30":"code","532f23f9":"code","e391fb99":"code","3e3bf418":"code","b7959ed6":"code","35d84085":"code","8df10bf7":"code","95fecf5d":"code","9d8415af":"code","b3bd1836":"code","53b1bf4a":"code","48c68056":"code","8d270e14":"code","57df14cc":"code","4bf5e576":"code","af592688":"code","61bf58d1":"code","32f423e5":"code","f07be28e":"code","143c2aae":"code","23d37ace":"code","264fe97c":"code","30cc1953":"code","052f69e5":"code","71d7da5e":"markdown","50a77e0d":"markdown","215be126":"markdown","c0e801c5":"markdown","10c45857":"markdown","c5a9a5ad":"markdown","d36dea65":"markdown","9fa30bf2":"markdown","bd5c5086":"markdown","5e420ec3":"markdown","d608aa49":"markdown","8efa7a59":"markdown","1a7a0125":"markdown","f4656510":"markdown","43dea2a9":"markdown","de277520":"markdown","b716792e":"markdown","dd637780":"markdown","51d2aa67":"markdown","aa9aae99":"markdown","160f9427":"markdown","c74de046":"markdown","f77eec9f":"markdown","6ff7a91f":"markdown","bd86e843":"markdown","03cb1bb9":"markdown"},"source":{"3f0532cd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","4a67e739":"import os\nimport seaborn as sns\nos.environ['KMP_DUPLICATE_LIB_OK']='True'","616d2116":"sns.set() ##set defaults","9c518c36":"%matplotlib inline","76cbd1b3":"data = pd.read_csv(\"..\/input\/adult.csv\")","721fbe12":"data.shape","7d1495f3":"data.head()","3bc5bec6":"data.info()","7e9e68b3":"data['workclass'].value_counts()","16ed1da5":"data['income'].value_counts()    ","bdacb4a7":"data['income'] = data['income'].apply(lambda inc: 0 if inc == \"<=50K\" else 1) # Binary encoding of the target variable","779f0dcb":"plt.figure(figsize=(10,5))\nsns.countplot(data['income'])","79e36e0c":"plt.figure(figsize=(14,6))\nsns.countplot(data['marital.status'])","4a5cf621":"plt.figure(figsize=(15,6))\nax=sns.barplot(x='marital.status',y='income',data=data,hue='sex')\nax.set(ylabel='Fraction of people with income > $50k')\ndata['marital.status'].value_counts()","a3db1bd0":"plt.figure(figsize=(12,6))\nsns.countplot(data['workclass'])","cce35c9a":"plt.figure(figsize=(12,6))\nax=sns.barplot('workclass', y='income', data=data, hue='sex')\nax.set(ylabel='Fraction of people with income > $50k')","92eb177b":"plt.figure(figsize=(10,8))  \nsns.heatmap(data.corr(),cmap='Accent',annot=True)\n#data.corr()\nplt.title('Heatmap showing correlations between numerical data')","6e9070a2":"plt.figure(figsize=(12,6))\nsns.boxplot(x=\"income\", y=\"age\", data=data, hue='sex')\n#data[data['income']==0]['age'].mean()","4e753f44":"norm_fnl = (data[\"fnlwgt\"] - data['fnlwgt'].mean())\/data['fnlwgt'].std()\nplt.figure(figsize=(8,6))\nsns.boxplot(x=\"income\", y=norm_fnl, data=data)","8e9682f0":"data[norm_fnl>2].shape","2f82fb62":"plt.figure(figsize=(10,5))\nax = sns.barplot(x='sex',y='income',data=data)\nax.set(ylabel='Fraction of people with income > $50k')","02c72fae":"plt.figure(figsize=(12,6))\nsns.boxplot(x='income',y ='hours.per.week', hue='sex',data=data)","eabf8cf1":"plt.figure(figsize=(15,6))\nax = sns.barplot(x='marital.status',y='hours.per.week',data=data,hue='sex')\nax.set(ylabel='mean hours per week')","8d2f5302":"plt.figure(figsize=(10,6))\nax = sns.barplot(x='income', y='education.num',hue='sex', data=data)\nax.set(ylabel='Mean education')","d454ed78":"print(data['race'].value_counts())\nplt.figure(figsize=(12,6))\nax=sns.barplot(x='race',y='income',data=data)\nax.set(ylabel='Fraction of people with income > $50k')","c6c0030b":"plt.figure(figsize=(12,6))\nsns.jointplot(x=data['capital.gain'], y=data['capital.loss'])\n#print(data[((data['capital.gain']!=0) & (data['capital.loss']!=0))].shape)","548a2b86":"plt.figure(figsize=(12,8))\nsns.distplot(data[(data['capital.gain']!=0)]['capital.gain'],kde=False, rug=True)","b34e7d40":"plt.figure(figsize=(12,8))\nsns.distplot(data[(data['capital.loss']!=0)]['capital.loss'], kde=False,rug=True)","94604bf3":"plt.figure(figsize=(20,6))\nax=sns.barplot(x='occupation', y='income', data=data)\nax.set(ylabel='Fraction of people with income > $50k')","582ffbf8":"print(data['native.country'].value_counts())\nnot_from_US = np.sum(data['native.country']!='United-States')\nprint(not_from_US, 'people are not from the United States')","6349854a":"data['native.country'] = (data['native.country']=='United-States')*1\n#data['US_or_not']=np.where(data['native.country']=='United-States',1,0)","f61d8abf":"data.select_dtypes(exclude=[np.number]).head()","8b290a0d":"#Replace all '?'s with NaNs.\ndata = data.applymap(lambda x: np.nan if x=='?' else x)","cde05287":"data.isnull().sum(axis=0) # How many issing values are there in the dataset?","d93ccd2f":"data.shape[0] - data.dropna(axis=0).shape[0]   # how many rows will be removed if I remove all the NaN's?","56eb85aa":"data = data.dropna(axis=0) ## Drop all the NaNs","415c1175":"#data.education.value_counts()  # I will label-encode the education column since it is an ordinal categorical variable","dfb7e380":"## This computes the fraction of people by country who earn >50k per annum\n#mean_income_bycountry_df = data[['native.country','income']].groupby(['native.country']).mean().reset_index()","d48ab9c4":"#edu_encode_dict = {'Preschool':0,'1st-4th':1, '5th-6th':2, '7th-8th':3, '9th':4, '10th':5,\n#                  '11th':6, '12th':7, 'HS-grad':8, 'Some-college':9, 'Bachelors':10, 'Masters':11, 'Assoc-voc':12, \n#                   'Assoc-acdm':13, 'Doctorate':14, 'Prof-school':15}\n\n#data['education'] = data['education'].apply(lambda ed_level: edu_encode_dict[ed_level])","fda37974":"data = pd.get_dummies(data,columns=['workclass','sex', 'marital.status',\n                                    'race','relationship','occupation'],\n               prefix=['workclass', 'is', 'is', 'race_is', 'relation', 'is'], drop_first=True)\n### native country is ignored because that feature will be dropped later","4de0756f":"plt.figure(figsize=(20,12))\nsns.heatmap(data.corr())","f013b2f4":"data.select_dtypes(exclude=[np.number]).shape","bacfcf23":"data.groupby('income').mean()","e67f97bc":"data.shape","163be0c2":"y = data.income\nX = data.drop(['income', 'education', 'native.country', 'fnlwgt'],axis=1)","e733b15c":"from sklearn.model_selection import train_test_split","b5d4d747":"# Split the dataset into training and testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","6c98a4e4":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom xgboost import XGBClassifier as xgb\nfrom sklearn import metrics","db6145b9":"baseline_train = np.zeros(y_train.shape[0])\nbaseline_test = np.zeros(y_test.shape[0])\nprint('Accuracy on train data: %f%%' % (metrics.accuracy_score(y_train, baseline_train)))\nprint('Accuracy on test data: %f%%' %  (metrics.accuracy_score(y_test, baseline_test)))","61ffc976":"rfmodel = RandomForestClassifier(n_estimators=300,oob_score=True,min_samples_split=5,max_depth=10,random_state=10)\nrfmodel.fit(X_train,y_train)\nprint(rfmodel)","a39baf18":"def show_classifier_metrics(clf, y_train=y_train,y_test=y_test, print_classification_report=True, print_confusion_matrix=True):\n    print(clf)\n    if print_confusion_matrix:\n        print('confusion matrix of training data')\n        print(metrics.confusion_matrix(y_train, clf.predict(X_train)))\n        print('confusion matrix of test data')\n        print(metrics.confusion_matrix(y_test, clf.predict(X_test)))\n    if print_classification_report:\n        print('classification report of test data')\n        print(metrics.classification_report(y_test, clf.predict(X_test)))\n    print('Accuracy on test data: %f%%' % (metrics.accuracy_score(y_test, clf.predict(X_test))*100))\n    print('Accuracy on training data: %f%%' % (metrics.accuracy_score(y_train, clf.predict(X_train))*100))\n    print('Area under the ROC curve : %f' % (metrics.roc_auc_score(y_test, clf.predict(X_test))))","76901736":"show_classifier_metrics(rfmodel,y_train)\nprint('oob score = %f'% rfmodel.oob_score_)","a5ffce51":"importance_list = rfmodel.feature_importances_\nname_list = X_train.columns\nimportance_list, name_list = zip(*sorted(zip(importance_list, name_list)))\nplt.figure(figsize=(20,10))\nplt.barh(range(len(name_list)),importance_list,align='center')\nplt.yticks(range(len(name_list)),name_list)\nplt.xlabel('Relative Importance in the Random Forest')\nplt.ylabel('Features')\nplt.title('Relative importance of Each Feature')\nplt.show()","83c8036b":"from sklearn.model_selection import cross_val_score, GridSearchCV","6652aec8":"def grid_search(clf, parameters, X, y, n_jobs= -1, n_folds=4, score_func=None):\n    if score_func:\n        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, n_jobs=n_jobs, scoring=score_func,verbose =2)\n    else:\n        print('Doing grid search')\n        gs = GridSearchCV(clf, param_grid=parameters, n_jobs=n_jobs, cv=n_folds, verbose =1)\n    gs.fit(X, y)\n    print(\"mean test score (weighted by split size) of CV rounds: \",gs.cv_results_['mean_test_score'] )\n    print (\"Best parameter set\", gs.best_params_, \"Corresponding mean CV score\",gs.best_score_)\n    best = gs.best_estimator_\n    return best","7bd350ce":"rfmodel2 = RandomForestClassifier(min_samples_split=5,oob_score=True, n_jobs=-1,random_state=10)\nparameters = {'n_estimators': [100,200,300], 'max_depth': [10,13,15,20]}\nrfmodelCV = grid_search(rfmodel2, parameters,X_train,y_train)","a2534066":"rfmodelCV.fit(X_train,y_train)\nshow_classifier_metrics(rfmodelCV,y_train)\nprint('oob score = %f'% rfmodelCV.oob_score_)","82654b76":"from xgboost.sklearn import XGBClassifier","2a576e2d":"param = {}\nparam['learning_rate'] = 0.1\nparam['verbosity'] = 1\nparam['colsample_bylevel'] = 0.9\nparam['colsample_bytree'] = 0.9\nparam['subsample'] = 0.9\nparam['reg_lambda']= 1.5\nparam['max_depth'] = 5\nparam['n_estimators'] = 400\nparam['seed']=10\nxgb= XGBClassifier(**param)\nxgb.fit(X_train, y_train, eval_metric=['error'], eval_set=[(X_train, y_train),(X_test, y_test)],early_stopping_rounds=40)","e53de39a":"show_classifier_metrics(xgb,y_train)","738c9b80":"importance_list = xgb.feature_importances_\nname_list = X_train.columns\nimportance_list, name_list = zip(*sorted(zip(importance_list, name_list)))\nplt.figure(figsize=(20,10))\nplt.barh(range(len(name_list)),importance_list,align='center')\nplt.yticks(range(len(name_list)),name_list)\nplt.xlabel('Relative Importance in XGBoost')\nplt.ylabel('Features')\nplt.title('Relative importance of Each Feature')\nplt.show()","4bb71f2f":"xgbmodel2 = XGBClassifier(seed=42)\nparam = {\n'learning_rate': [0.1],#[0.1,0.2],\n#'verbosity': [1],\n'colsample_bylevel': [0.9],\n'colsample_bytree': [0.9],\n'subsample' : [0.9],\n'n_estimators': [300],\n'reg_lambda': [1.5,2,2.5],\n'max_depth': [3,5,7],\n 'seed': [10]   \n}\nxgbCV = grid_search(xgbmodel2, param,X_train,y_train)","694a65d5":"xgbCV.fit(X_train, y_train, eval_metric=['error'], eval_set=[(X_train, y_train),(X_test, y_test)],early_stopping_rounds=40)","af89a11b":"show_classifier_metrics(xgbCV,y_train)","3c20ad62":"#X_test.iloc[np.where(y_test != xgbCV.predict(X_test))]","3dffc200":"from sklearn.linear_model import LogisticRegression","491daa8a":"param = {\n'C': [3,5,10], \n'verbose': [1],\n    'max_iter': [100,200,500,700]\n}   \nlogreg = LogisticRegression(random_state=10)\nlogreg_grid = grid_search(logreg, param, X_train,y_train, n_folds=3)","df301337":"logreg_grid.fit(X_train, y_train)","cc813abd":"show_classifier_metrics(logreg_grid)","6d08fb87":"from sklearn.naive_bayes import GaussianNB","d90f220e":"NBmodel = GaussianNB()","185bc25c":"NBmodel.fit(X_train, y_train)","7c2171da":"NBmodel.predict(X_test)","5545a4df":"show_classifier_metrics(NBmodel,y_train)","191e9f5e":"def create_stacked_dataset(clfs,modelnames, X_train=X_train,X_test=X_test):\n    X_train_stack, X_test_stack = X_train, X_test\n    for clf,modelname in zip(clfs,modelnames):\n        temptrain = pd.DataFrame(clf.predict(X_train),index = X_train.index,columns=[modelname+'_prediction'])\n        temptest  = pd.DataFrame(clf.predict(X_test),index = X_test.index,columns=[modelname+'_prediction'])\n        X_train_stack = pd.concat([X_train_stack, temptrain], axis=1)\n        X_test_stack = pd.concat([X_test_stack, temptest], axis=1)\n    return (X_train_stack,X_test_stack)","a6085ef5":"X_train_stack,X_test_stack = create_stacked_dataset([rfmodelCV,logreg_grid,xgbCV],modelnames=['rfmodel','logreg', 'xgb'])","f4e8a917":"X_train_stack.head(2)","3f5c7ce3":"param = {}\nparam['learning_rate'] = 0.1\nparam['verbosity'] = 1\nparam['colsample_bylevel'] = 0.9\nparam['colsample_bytree'] = 0.9\nparam['subsample'] = 0.9\nparam['reg_lambda']= 1.5\nparam['max_depth'] = 5#10\nparam['n_estimators'] = 400\nparam['seed']=10\nxgbstack= XGBClassifier(**param)\nxgbstack.fit(X_train_stack, y_train, eval_metric=['error'], eval_set=[(X_train_stack, y_train),(X_test_stack, y_test)],early_stopping_rounds=30)\n","0cc2675d":"print(metrics.classification_report(y_test, xgbstack.predict(X_test_stack)))\nprint('Accuracy on test data: %f%%' % (metrics.accuracy_score(y_test, xgbstack.predict(X_test_stack))*100))\nprint('Accuracy on training data: %f%%' % (metrics.accuracy_score(y_train, xgbstack.predict(X_train_stack))*100))","9ea4cb44":"xgbstackCV = XGBClassifier(seed=10)\nparam_grid = {}\nparam_grid['learning_rate'] = [0.1]\nparam_grid['colsample_bylevel'] = [0.9]\nparam_grid['colsample_bytree'] = [0.9]\nparam_grid['subsample'] = [0.9]\nparam_grid['n_estimators'] = [300]\nparam_grid['reg_lambda']= [1.5]\nparam_grid['seed'] =[10]\nparam_grid['max_depth'] = [3,5,8,10]\nxgbstackCV_grid = grid_search(xgbstackCV, param_grid,X_train_stack,y_train)","9aaf5d6c":"xgbstackCV_grid.fit(X_train_stack, y_train, eval_metric=['error'], eval_set=[(X_train_stack, y_train),(X_test_stack, y_test)],early_stopping_rounds=30)","be50ed30":"print(metrics.classification_report(y_test, xgbstack.predict(X_test_stack)))\nprint('Accuracy on test data: %f%%' % (metrics.accuracy_score(y_test, xgbstack.predict(X_test_stack))*100))\nprint('Accuracy on training data: %f%%' % (metrics.accuracy_score(y_train, xgbstack.predict(X_train_stack))*100))","532f23f9":"from catboost import CatBoostClassifier","e391fb99":"catb = CatBoostClassifier(learning_rate=0.3,iterations=400,verbose=0,random_seed=10,eval_metric='Accuracy',rsm=0.9)","3e3bf418":"catb.fit(X_train,y_train,eval_set=[(X_train,y_train), (X_test,y_test)],early_stopping_rounds=40)","b7959ed6":"show_classifier_metrics(catb)","35d84085":"### Catboost grid search\n\ncatbCV = CatBoostClassifier(verbose=0,random_seed=10,eval_metric='Accuracy')\nparam_grid = {}\nparam_grid['learning_rate'] = [0.1]#, 0.3]\nparam_grid['rsm'] = [0.9]\n#param_grid['subsample'] = [0.9]\nparam_grid['iterations'] = [200,300]\nparam_grid['reg_lambda']= [3] #2\nparam_grid['depth'] = [8,10]#5\ncatbCV_grid = grid_search(catbCV, param_grid,X_train,y_train)","8df10bf7":"catbCV_grid.fit(X_train,y_train,eval_set=[(X_train,y_train), (X_test,y_test)],early_stopping_rounds=30)","95fecf5d":"show_classifier_metrics(catbCV_grid)","9d8415af":"from imblearn.over_sampling import RandomOverSampler","b3bd1836":"np.sum(y_train)\/y_train.shape[0]","53b1bf4a":"ros = RandomOverSampler(random_state=1,sampling_strategy=0.8)","48c68056":"X_resampled, y_resampled = ros.fit_resample(X_train, y_train)","8d270e14":"catb_ros = CatBoostClassifier(learning_rate=0.1,iterations=400,reg_lambda=2,verbose=0,random_seed=10,eval_metric='Accuracy')","57df14cc":"catb_ros.fit(X_resampled,y_resampled,eval_set=[(X_resampled,y_resampled), (X_test,y_test)],early_stopping_rounds=40)","4bf5e576":"print('Accuracy on test data: %f%%' % (metrics.accuracy_score(y_test, catb_ros.predict(X_test))*100))\nprint('Accuracy on training data: %f%%' % (metrics.accuracy_score(y_resampled, catb_ros.predict(X_resampled))*100))\nprint('Area under the ROC curve : %f' % (metrics.roc_auc_score(y_test, catb_ros.predict(X_test))))","af592688":"from imblearn.over_sampling import SMOTE","61bf58d1":"smt = SMOTE(random_state=10,sampling_strategy=0.7)\nX_train_smt, y_train_smt = smt.fit_sample(X_train, y_train)","32f423e5":"y_train.value_counts()","f07be28e":"np.bincount(y_train_smt)","143c2aae":"catb_smote = CatBoostClassifier(learning_rate=0.1,iterations=400,reg_lambda=2,verbose=0,random_seed=10,eval_metric='Accuracy')","23d37ace":"catb_smote.fit(X_train_smt,y_train_smt,eval_set=[(X_train_smt,y_train_smt), (X_test,y_test)],early_stopping_rounds=40)","264fe97c":"print(metrics.classification_report(y_test, catb_smote.predict(X_test)))\nprint('Accuracy on test data: %f%%' % (metrics.accuracy_score(y_test, catb_smote.predict(X_test))*100))\nprint('Accuracy on training data: %f%%' % (metrics.accuracy_score(y_train_smt, catb_smote.predict(X_train_smt))*100))\nprint('Area under the ROC curve : %f' % (metrics.roc_auc_score(y_test, catb_ros.predict(X_test))))","30cc1953":"param = {}\nparam['learning_rate'] = 0.1\nparam['verbosity'] = 1\nparam['colsample_bylevel'] = 0.9\nparam['colsample_bytree'] = 0.9\nparam['subsample'] = 0.9\nparam['reg_lambda']= 1.5\nparam['max_depth'] = 5\nparam['n_estimators'] = 400\nparam['seed']=10\nxgb_smote= XGBClassifier(**param)\nxgb_smote.fit(X_train_smt, y_train_smt, eval_metric=['error'], eval_set=[(X_train_smt, y_train_smt),(X_test.values, y_test.values)],early_stopping_rounds=30)","052f69e5":"print(metrics.classification_report(y_test, xgb_smote.predict(X_test.values)))\nprint('Accuracy on test data: %f%%' % (metrics.accuracy_score(y_test, xgb_smote.predict(X_test.values))*100))\nprint('Accuracy on training data: %f%%' % (metrics.accuracy_score(y_train_smt, xgb_smote.predict(X_train_smt))*100))\nprint('Area under the ROC curve : %f' % (metrics.roc_auc_score(y_test, xgb_smote.predict(X_test.values))))","71d7da5e":"The fraction of rich among men is significantly higher than that among women.","50a77e0d":"# Adult Income Prediction notebook\nThis data was extracted from the 1994 Census bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). *The prediction task is to determine whether a person makes over  USD 50K a year.*  \nThis is a binary classification problem where we have to predict whether a person earns over $50k per year or not. The scoring function to optimize is accuracy. The notebook follows the following steps to analyse the data and build predictive models.\n- Data cleaning and preprocessing\n- Exploratory data analysis\n- Modelling: I have tried out different classification algorithms.\n  - Random forest\n  - XGBoost\n  - Naive Bayes\n  - Logistic regression\n  - CatBoost  \nThese models were then optimized by tuning the hyper-parameters through Grid Search, keeping a close check on the cross-validation scores to prevent overfitting. Thereafter I also tried out stacking different models together to improve the accuracy but it didn't improve the accuracy sigificantly.\n- Finally I explored oversampling techniques including SMOTE (in progress).\n\nPlease feel free to suggest\/comment.\n\n","215be126":"On the basis of the above plot, we can only conclude that males put in slightly more hours per week than women on an average.","c0e801c5":"## Naive Bayes","10c45857":"### Stacked model","c5a9a5ad":"Those with `Never-married` and `Married-civ-spouse` labels dominate the dataset.","d36dea65":"#### SMOTE with XGBoost","9fa30bf2":"As one can see, there is considerable class imbalance in the target variable, i.e. income. This is also intuitively obvious as one expects fewer 'rich' people (earning>50k\/annum) than 'not-so-rich' people (earning <50k\/annum). Therefore we might need to consider over-sampling techniques in our ML model to improve our accuracy.","bd5c5086":"### Random Forest classifier","5e420ec3":"The above plot shows the the fraction of people earning more than $50k per annum, grouped by their marital status and gender. The data shows that married people have a higher %age of high-earners, compared to those who either never married or are widowed\/divorced\/separated. The black lines indicate 2 standard deviations (or 95\\% confidence interval) in the data set. The married spouses of armed forces personnel have a much higher variation in their income compared to civil spouses because of low-number statistics.","d608aa49":"The `education.num` is label encoded such that a higher number corresponds to a higher level of education. As on would na\u00efvely expect, people who earn more (>50k per annum) are also highly educated. The mean education level for `income=1` class is between 11 (Assoc-voc) and 12 (Assoc-acdm) whereas that for the `income=0` class is between 9 (HS-grad) and 10 (Some-college).","8efa7a59":"#### Convert the `native.country` feature to binary since there is a huge imbalance in this feature","1a7a0125":"## Logistic regression","f4656510":"The mean age of people earning more than 50k per annum is around 44 whereas the mean age of of those earning less than 50k per annum is 36.","43dea2a9":"### Random forest: Grid Search and cross-validation","de277520":"### Stacked model Grid Search","b716792e":"#### Encode the target variable to binary","dd637780":"### SMOTE","51d2aa67":"## Modelling\nThis section explores different classification algorithms to maximise the accuracy for predicting income of a person (> 50k\/yr or < 50k\/yr).","aa9aae99":"- `income` is dropped from X because it is the target variable.\n- `Education` is dropped because it is already label-encoded in `education.num`. One can notice the high correlation between `education` and `education.num` in the heatmap.\n- `native country` is dropped because it showed very little feature importance in random forest classifer.\n- `fnlwgt` is dropped because it has no correlation with `income`.","160f9427":"As evident from the plot above, there are many outliers in the `fnlwgt` column and this feature is uncorrelated with `income`, our target variable. The correlation coefficient (which one can read from the heatmap) is -0.0095. The number of outliers, i.e. the number of records which are more than 2 s.d's away from the mean, is 1249.","c74de046":"## XGBoost ","f77eec9f":"### One-hot encoding of the categorical columns","6ff7a91f":"### Grid search with cross validation: XGBoost model","bd86e843":"### Baseline model\nIn the baseline model, we predict the minority class for all our train and test (or validation) examples. The resulting accuracy will serve as a benchmark for the ML models. In other words, the sophisticated ML models should have an accuracy which should at least better the baseline one.","03cb1bb9":"## Exploratory analysis"}}