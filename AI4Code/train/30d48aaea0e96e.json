{"cell_type":{"314ccd66":"code","3646dcae":"code","86857a66":"code","0c37c859":"code","4e298789":"code","24bcf707":"code","7cfc3893":"code","a4c35548":"code","d63edf42":"code","b504e2a2":"code","a291fcda":"code","00ff5669":"code","76a4cdff":"code","971bac5f":"code","b846213e":"code","be434c73":"code","0a886655":"code","cf7b24f5":"code","6a79faff":"code","8004cebe":"code","9b301418":"code","720e76f4":"code","0497f72f":"code","28d7b70e":"code","9eda94bd":"code","036daea9":"code","ab04afca":"code","b1d08e39":"markdown","4bc12e6c":"markdown","9c707907":"markdown","b1df3d22":"markdown"},"source":{"314ccd66":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.metrics import confusion_matrix,classification_report, roc_auc_score,matthews_corrcoef,precision_score, recall_score, f1_score, accuracy_score\n\n\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings('ignore')","3646dcae":"df = pd.read_csv('..\/input\/drug-classification\/drug200.csv')","86857a66":"df.head()","0c37c859":"df.isna().sum()","4e298789":"df['BP'].value_counts()","24bcf707":"df['Cholesterol'].value_counts()","7cfc3893":"labelencoder = LabelEncoder()","a4c35548":"def label_encode(cat_feature):\n    return labelencoder.fit_transform(df[cat_feature])","d63edf42":"cat_features = ['Sex','BP','Cholesterol','Drug']","b504e2a2":"for feature in cat_features:\n    df[feature+'_cat'] = label_encode(feature)","a291fcda":"df.head()","00ff5669":"X,y = df[['Age','Sex_cat','BP_cat','Cholesterol_cat','Na_to_K']],df['Drug_cat']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,stratify=y,random_state=42)","76a4cdff":"xgb_classifier = xgb.XGBClassifier()","971bac5f":"xgb_classifier.fit(X_train,y_train)","b846213e":" prediction = xgb_classifier.predict(X_test)","be434c73":"print(classification_report(prediction,y_test))","0a886655":"print(\"Precision:{}\".format(precision_score(prediction,y_test,average='weighted')))\nprint(\"Recall:{}\".format(recall_score(prediction,y_test,average='weighted')))\nprint(\"F1 Score:{}\".format((f1_score(prediction,y_test,average='weighted'))))","cf7b24f5":"from bayes_opt import BayesianOptimization","6a79faff":"\ndef bo_params_xgb(max_depth, gamma,learning_rate,n_estimators,subsample):\n    \n    params = {\n        'max_depth': int(max_depth),\n        'gamma': gamma,\n        'learning_rate':learning_rate,\n        'subsample': subsample,\n        'eta': 0.1,\n        'eval_metric': 'auc',\n        'n_estimators':int(n_estimators)\n    }\n    \n    scores = cross_val_score(xgb.XGBClassifier(random_state=123, **params,use_label_encoder=False),\n                             X_train, y_train,cv=5,scoring=\"f1_weighted\").mean()\n    \n    #cv_result = xgb.cv(params, train_dmatrix, nfold=5)\n    return scores.mean()","8004cebe":"xgb_bo = BayesianOptimization(bo_params_xgb, {'max_depth': (3, 10),\n                                             'gamma': (0, 1),\n                                             'learning_rate':(0,1),\n                                              'subsample':(0.5,1),\n                                              'n_estimators':(100,200)\n                                             })","9b301418":"results = xgb_bo.maximize(n_iter=200, init_points=2)","720e76f4":"params = xgb_bo.max['params']\nprint(params)","0497f72f":"params['max_depth']= int(params['max_depth'])\nparams['n_estimators']= int(params['n_estimators'])","28d7b70e":"xgb_classifier2 = xgb.XGBClassifier(**params,use_label_encoder=False)\nxgb_classifier2.fit(X_train,y_train)","9eda94bd":" prediction = xgb_classifier2.predict(X_test)","036daea9":"print(classification_report(prediction,y_test))","ab04afca":"print(\"Precision:{}\".format(precision_score(prediction,y_test,average='weighted')))\nprint(\"Recall:{}\".format(recall_score(prediction,y_test,average='weighted')))\nprint(\"F1 Score:{}\".format((f1_score(prediction,y_test,average='weighted'))))","b1d08e39":"For understanding how bayesian optimization works,we will finetune for 'max_depth','gamma','n_estimators','n_estimators' and 'subsample' hyperparameters.","4bc12e6c":"# Problem Statement\n\nTo develop multiclass classification algorithm to predict the drug type based on the given features","9c707907":"# WIP\n\n1) Detailed mathematical formulation of each acquistion functions <br>\n2) K-fold cross validation for model validation","b1df3d22":"# Hyperparamete tuning\n\n\nReference: https:\/\/proceedings.neurips.cc\/paper\/2012\/file\/05311655a15b75fab86956663e1819cd-Paper.pdf\n\nMachine learning models have parameters and hyperparameters. Parameters are learnt from the data and hyperparameters are pre-defined for a model. For example, in linear\/logistic regression, the coefficients and biases are the parameters that is being optimised while training, whereas learning rate is one hyperparameter that is set to a constant before training the model. The process of finding the best hyperparameters from a range of values is called hyperparameter tuning.\n\n## Bayesian optimization for hyperparameter tuning\n\n\nBayesian optimization (BO) is an automated procedure for hyperparameter tuning. BO uses gaussian process to model mean and variance of the target function.\n\n### Gaussian process\n\nGuassian process is a prior distribution on functions, and in the case of hyperparameter tuning it will be of the form:\n\n$\n\\begin{align}\nf:\\chi \\rightarrow {\\mathbb{R}} \n\\end{align}\n$ <br>\n<br>\nWhere $\\chi$ is the space of hyperparameter set we are tuning, and $f$ is the target function decided based on the criteria on which the hyperparameters needs to be tuned. For a gaussian process, $p(f|\\chi)$ follows a normal distribution. In our problem, we define the mean of weighted F1 score from five fold cross validation as the target function. \n\n### Acquisition function\n\nIt is assumed that the function $f(x)$ is drawn from a gaussain process prior and our observation is of the form ${x_i,y_i}$, where <br>\n\n$ y_i \\thicksim N(f(x_i), \u03bd) $ , $\u03bd$ is the variance of noise introduced into the function observations.\n\nThis prior and the data induce a posterior over functions,the acquisition function, $a:\\chi \\rightarrow {\\mathbb{R^{+}}} \n$, determines what points in $\\chi$ to be evaluated next.\n\nThere are different choices for acquisition functions, mainly <br>\n\n1) Upper Confidence Bounds method <br>\n2) Expected improvement method <br>\n3) Probability Of Improvement method <br>\n\n\nAcquisiton function follows a greedy approach to find the optimum hyperparameters in each step, and with a fixed step of iteration, we are  expected to get the suitable hyperparameters for our model.\n\n\n\n\n"}}