{"cell_type":{"71412fd5":"code","75a449d1":"code","183f4ca2":"code","7636e18c":"code","38ba57d7":"code","189782f8":"code","97d7fc85":"code","4562a6b5":"code","8c2c049d":"code","0f290e9d":"code","490d8613":"code","43081300":"code","7bee526f":"code","c6897c06":"code","6b1cf669":"code","68360334":"code","30a229f4":"code","353b775a":"code","23a54f23":"code","f2498730":"code","9941f3e0":"code","b284c387":"code","b3dbc0ad":"code","197f6754":"code","11ea8c4e":"code","65626c4f":"code","3c8d45d6":"code","e3922ccf":"code","acea4d35":"code","e2126e7b":"code","105ee787":"code","afda68e4":"code","6269eab4":"code","6a744ee2":"code","6268acab":"code","40c5158f":"code","cc6219da":"code","79a6d3ae":"code","d3d482c2":"code","028136c0":"code","84a37887":"code","28a67e65":"code","e1b7ea79":"code","6e560514":"code","d747b343":"code","1fbb4f79":"code","863fb155":"code","a52b56ef":"code","9aaaf07c":"code","ae986b7f":"code","f38348a1":"code","ff361bbf":"code","6cb7bdfc":"code","d3b37e46":"code","cbee2696":"code","5d33d1e5":"code","6c78144d":"code","c6e93dc0":"code","723869f4":"code","2088e9f2":"code","117c28ef":"code","b33af5df":"code","91a2ec77":"code","118e3891":"code","fb70ee91":"code","535ad6e2":"code","b90e2825":"code","2f2496df":"code","b06cae74":"code","aba15fdf":"code","9bb9e3b3":"code","f1c83e63":"code","c3e8c26c":"code","19753fa5":"code","ee25d048":"code","cab8597a":"code","f4f74613":"code","c397c830":"code","8755b641":"code","9972b977":"code","61ff1987":"code","d85a52bb":"code","e6fd4d0a":"code","4d030e10":"code","21b35782":"code","b7375296":"code","ceef0a5d":"code","5970d791":"code","2b385730":"code","fd50026b":"code","bd93a026":"code","2bd9ed67":"code","6720bd14":"code","51d0654b":"code","6fe83be5":"code","02bf5805":"markdown","df50f039":"markdown","94a8cc2c":"markdown","356e9415":"markdown","3b2e95c6":"markdown","0e8e0e80":"markdown","a7a4928a":"markdown","3398413b":"markdown","90bf5a6a":"markdown","bca58324":"markdown","de500879":"markdown","a36dd8eb":"markdown","3bdd0bae":"markdown","ca90cbad":"markdown","352ead17":"markdown","52b09cfc":"markdown","ee05dd26":"markdown","bfb29a67":"markdown","a2dfd6f2":"markdown","a3c4dd10":"markdown","60361e23":"markdown","d3eef847":"markdown","f1792938":"markdown","aa8f790f":"markdown","7e40b13c":"markdown","6332f770":"markdown","75d06892":"markdown","ce9fd78d":"markdown","a0e05404":"markdown","ae80b226":"markdown","f7f31445":"markdown","acd6a433":"markdown","34298637":"markdown","d4579e79":"markdown","df2c7c82":"markdown","a18c4fb1":"markdown","1029009a":"markdown","e5d23833":"markdown","0383955d":"markdown","36e664f4":"markdown","8b83ed02":"markdown","1fb8ebb3":"markdown","7bcf7460":"markdown","cce32e71":"markdown","731e1c03":"markdown","b728781d":"markdown","b061d4a6":"markdown","a463c9e7":"markdown","4b36b180":"markdown","6a9c4d3e":"markdown","0fb5f412":"markdown","be786dd8":"markdown","2dda9052":"markdown"},"source":{"71412fd5":"!pip3 install seaborn","75a449d1":"# EDA and plotiing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns # seaborn gets shortened to sns\n\n%matplotlib inline\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model evaluators\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","183f4ca2":"# Load the data\ndf = pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")","7636e18c":"df.info()","38ba57d7":"df[\"smoking_status\"].unique()","189782f8":"df.head()","97d7fc85":"df.shape","4562a6b5":"# Let's see how many positives(1) and negatives(0) in our target\ndf[\"stroke\"].value_counts()","8c2c049d":"# Normalized value counts\ndf[\"stroke\"].value_counts(normalize=True)*100","0f290e9d":"# Visulazing the value counts\ndf[\"stroke\"].value_counts().plot(kind=\"bar\", color=[\"skyblue\", \"lightgreen\"]);","490d8613":"# Getting metrics on the columns\ndf.describe()","43081300":"pd.crosstab(df.gender, df.stroke)","7bee526f":"df = df[df[\"gender\"] != \"Other\"]","c6897c06":"pd.crosstab(df.gender, df.stroke).plot(kind=\"bar\", color=[\"skyblue\", \"lightgreen\"]);","6b1cf669":"pd.crosstab(df[\"ever_married\"], df[\"stroke\"])","68360334":"# Visualizing the crosstab\npd.crosstab(df[\"ever_married\"], df[\"stroke\"]).plot(kind=\"bar\", color=[\"skyblue\", \"lightgreen\"]);","30a229f4":"pd.crosstab(df[\"Residence_type\"], df[\"stroke\"])","353b775a":"# Visualizing residence_type with target\npd.crosstab(df[\"Residence_type\"], df[\"stroke\"]).plot(kind=\"bar\", color=[\"skyblue\", \"lightgreen\"]);","23a54f23":"pd.crosstab(df[\"smoking_status\"], df[\"stroke\"])","f2498730":"pd.crosstab(df[\"smoking_status\"], df[\"stroke\"]).plot(kind=\"bar\");","9941f3e0":"df[\"smoking_status\"].value_counts(normalize=True)","b284c387":"pd.crosstab(df[\"work_type\"], df[\"stroke\"])","b3dbc0ad":"df[\"work_type\"].value_counts(normalize=True)","197f6754":"pd.crosstab(df[\"work_type\"], df[\"stroke\"]).plot(kind=\"bar\");","11ea8c4e":"pd.crosstab(df[\"hypertension\"], df[\"stroke\"])","65626c4f":"pd.crosstab(df[\"hypertension\"], df[\"stroke\"]).plot(kind=\"bar\", color=[\"skyblue\", \"lightgreen\"]);","3c8d45d6":"pd.crosstab(df[\"age\"], df[\"stroke\"]).plot(kind=\"line\");","e3922ccf":"pd.crosstab(df[\"bmi\"], df[\"stroke\"]).plot(kind=\"line\");","acea4d35":"# Create another figure\nplt.figure(figsize=(10,6))\n\n# Start with positve examples\nplt.scatter(df.age[df.stroke==0], \n            df.bmi[df.stroke==0], \n            c=\"lightblue\") # define it as a scatter figure\n\n# Now for negative examples, we want them on the same plot, so we call plt again\nplt.scatter(df.age[df.stroke==1], \n            df.bmi[df.stroke==1], \n            c=\"salmon\") # axis always come as (x, y)\n\n# Add some helpful info\nplt.title(\"Stroke in function of Age and Hyper tension\")\nplt.xlabel(\"Age\")\nplt.legend([\"No Disease\", \"Disease\"])\nplt.ylabel(\"BMI\");","e2126e7b":"# Create another figure\nplt.figure(figsize=(10,6))\n\n# Start with positve examples\nplt.scatter(df.age[df.stroke==0], \n            df.avg_glucose_level[df.stroke==0], \n            c=\"lightblue\") # define it as a scatter figure\n\n# Now for negative examples, we want them on the same plot, so we call plt again\nplt.scatter(df.age[df.stroke==1], \n            df.avg_glucose_level[df.stroke==1], \n            c=\"salmon\") # axis always come as (x, y)\n\n# Add some helpful info\nplt.title(\"Stroke in function of Age and Hyper tension\")\nplt.xlabel(\"Age\")\nplt.legend([\"No Disease\", \"Disease\"])\nplt.ylabel(\"Average Glucose level\");","105ee787":"df.isna().sum()","afda68e4":"df_with_nan_bmi = df[df[\"bmi\"].isnull()]\ndf_with_nan_bmi.to_csv(\"nan-bmi-samples.csv\")","6269eab4":"# Now we've saved the nan bmi samples to a scv let's drop them from the dataframe\ndf = df.dropna()","6a744ee2":"df.head()","6268acab":"df[\"gender\"].unique()","40c5158f":"df['gender'] = np.where((df.gender == 'Male'),'0',df.gender)\ndf['gender'] = np.where((df.gender == 'Female'),'1',df.gender)","cc6219da":"df[\"gender\"] = df[\"gender\"].astype('int64')","79a6d3ae":"df.info()","d3d482c2":"df[\"ever_married\"].unique()","028136c0":"df['ever_married'] = np.where((df.ever_married == 'Yes'),'0',df.ever_married)\ndf['ever_married'] = np.where((df.ever_married == 'No'),'1',df.ever_married)\ndf[\"ever_married\"] = df[\"ever_married\"].astype('int64')\ndf.head()","84a37887":"df[\"work_type\"].unique()","28a67e65":"df['work_type'] = np.where((df.work_type == 'Private'),'0',df.work_type)\ndf['work_type'] = np.where((df.work_type == 'Self-employed'),'1',df.work_type)\ndf['work_type'] = np.where((df.work_type == 'Govt_job'),'2',df.work_type)\ndf['work_type'] = np.where((df.work_type == 'children'),'3',df.work_type)\ndf['work_type'] = np.where((df.work_type == 'Never_worked'),'4',df.work_type)\ndf[\"work_type\"] = df[\"work_type\"].astype('int64')\ndf.head()","e1b7ea79":"df[\"Residence_type\"].unique()","6e560514":"df['Residence_type'] = np.where((df.Residence_type == 'Urban'),'0',df.Residence_type)\ndf['Residence_type'] = np.where((df.Residence_type == 'Rural'),'1',df.Residence_type)\ndf[\"Residence_type\"] = df[\"Residence_type\"].astype('int64')\ndf.head()","d747b343":"df[\"smoking_status\"].unique()","1fbb4f79":"df['smoking_status'] = np.where((df.smoking_status == 'formerly smoked'),'0',df.smoking_status)\ndf['smoking_status'] = np.where((df.smoking_status == 'never smoked'),'1',df.smoking_status)\ndf['smoking_status'] = np.where((df.smoking_status == 'smokes'),'2',df.smoking_status)\ndf['smoking_status'] = np.where((df.smoking_status == 'Unknown'),'3',df.smoking_status)\ndf[\"smoking_status\"] = df[\"smoking_status\"].astype('int64')","863fb155":"df.head()","a52b56ef":"df.dtypes","9aaaf07c":"corr_matrix = df.corr()\ncorr_matrix","ae986b7f":"# Let's plot the correlation matrix\nplt.figure(figsize=(30,20))\nsns.heatmap(corr_matrix,\n           annot=True,\n           linewidths=0.5,\n           fmt=\".2f\",\n           cmap=\"YlGnBu\");","f38348a1":"X = df.drop(\"stroke\", axis=1)\nY = df.stroke.values","ff361bbf":"# Random seed for probabality\nnp.random.seed(42)\n\n# Splitting the data into train and test sets\nX_train, X_test, Y_train, Y_test = train_test_split(X,\n                                                    Y,\n                                                    test_size=0.2)","6cb7bdfc":"X_train.head()","d3b37e46":"len(X_train), len(Y_train), len(X_test), len(Y_test)","cbee2696":"# Put models in a dictionary\nmodels = {\"KNN\": KNeighborsClassifier(),\n          \"Logistic Regression\": LogisticRegression(), \n          \"Random Forest\": RandomForestClassifier()}\n\ndef fit_and_score(models, x_train, x_test, y_train, y_test):\n    \"\"\"\n    Model to fit the data to a model and score the model with test data\n    models - dictionary of models\n    x_train - training features\n    x_test - test features\n    y_train - training features\n    y_test - test_features\n    \"\"\"\n    np. random.seed(42)\n    model_scores = {}\n    for name, model in models.items():\n        model.fit(x_train, y_train)\n        model_scores[name] = model.score(x_test, y_test)\n\n    return model_scores","5d33d1e5":"model_scores = fit_and_score(models=models,\n                             x_train=X_train,\n                             x_test=X_test,\n                             y_train=Y_train,\n                             y_test=Y_test)\nmodel_scores","6c78144d":"model_compare = pd.DataFrame(model_scores, index=['accuracy'])\nmodel_compare.T.plot.bar();","c6e93dc0":"# Creating a list for train scores\ntrain_scores = []\n\n# Creating a list for test scores\ntest_scores = []\n\n# Creating a list for n neighbours\nneighbours = range(1,21)\n\nknn= KNeighborsClassifier()\n\n# Loop through different neighbours values\nfor i in neighbours:\n    # Set neighbours\n    knn.set_params(n_neighbors = i)\n    \n    # Fitting the model\n    knn.fit(X_train, Y_train)\n    \n    # Scoring the model\n    train_scores.append(knn.score(X_train, Y_train))\n    \n    # Scoring the model on test data set\n    test_scores.append(knn.score(X_test, Y_test))","723869f4":"train_scores, test_scores","2088e9f2":"plt.plot(neighbours, train_scores, label=\"Train score\")\nplt.plot(neighbours, test_scores, label=\"Test score\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")","117c28ef":"# Different LogisticRegression hyperparameters\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Different RandomForestClassifier hyperparameters\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}","b33af5df":"# Setup random seed\nnp.random.seed(42)\n\n# Setup Random hyper parameter tuning for Logistic Regression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(), param_distributions=log_reg_grid,\n                               cv=5,\n                               n_iter=20,\n                               verbose=True)\n\nrs_log_reg.fit(X_train, Y_train)","91a2ec77":"# Finding the best hyper parameters\nrs_log_reg.best_params_","118e3891":"rs_log_score = rs_log_reg.score(X_test, Y_test)","fb70ee91":"model_scores[\"Logistic Regression\"] - rs_log_score","535ad6e2":"# Tuning random forest classifier using randomsearchcv parameters\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                          param_distributions=rf_grid,\n                          cv=5,\n                          n_iter=20,\n                          verbose=True)\n\nrs_rf.fit(X_train, Y_train)\n\nrs_rf.score(X_test, Y_test)","b90e2825":"rs_rf.best_params_","2f2496df":"rs_rf_score = rs_rf.score(X_test, Y_test)","b06cae74":"model_scores[\"Random Forest\"] - rs_rf_score","aba15fdf":"Y_preds = rs_rf.predict(X_test)","9bb9e3b3":"# Import roc curve from metrics module\nfrom sklearn.metrics import plot_roc_curve\n\nplot_roc_curve(rs_rf, X_test, Y_test)","f1c83e63":"print(confusion_matrix(Y_test, Y_preds))","c3e8c26c":"# Import seaborn\nimport seaborn as sns\n\ndef plot_conf_matrix(y_test, y_preds):\n    \"\"\"\n    Funtion to plot confusion matrix\n    \"\"\"\n\n    fig, ax = plt.subplots(figsize=(3,3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                    annot=True,\n                    cbar=False)\n\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predicted label\")\n\n#plot_conf_matrix(Y_test, Y_preds)","19753fa5":"print(classification_report(Y_test, Y_preds))","ee25d048":"from sklearn.model_selection import cross_val_score","cab8597a":"rs_rf.best_params_","f4f74613":"clf = RandomForestClassifier(n_estimators=260,\n                            min_samples_split=8,\n                            min_samples_leaf=13,\n                            max_depth=10)","c397c830":"# Cross-validated F1 score\ncv_f1 = np.mean(cross_val_score(clf,\n                                X,\n                                Y,\n                                cv=5, # 5-fold cross-validation\n                                scoring=\"f1\")) # f1 as scoring\ncv_f1","8755b641":"!pip install -U imbalanced-learn","9972b977":"from imblearn.under_sampling import TomekLinks, RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN","61ff1987":"# Put models in a dictionary\nmodels = {\"KNN\": KNeighborsClassifier(),\n          \"LogisticRegression\": LogisticRegression(), \n          \"RandomForest\": RandomForestClassifier()}\n\nresamplers = {\n    \"ros\": RandomOverSampler(sampling_strategy='minority'),\n    \"smote\": SMOTE(sampling_strategy='minority'),\n    \"adasyn\": ADASYN(sampling_strategy='minority'),\n    \"rus\": RandomUnderSampler(sampling_strategy=\"majority\"),\n    \"tomek\": TomekLinks(sampling_strategy=\"majority\")\n}\n\ndef fit_resample_and_score(models, samplers, x, y):\n    \"\"\"\n    Model to resample data to a model and score the model with test data\n    models - dictionary of models\n    samplers - samplers to resample the data\n    x - features\n    y - labels\n    \"\"\"\n    np. random.seed(42)\n    model_scores = {}\n    for sname, sampler in samplers.items():\n        \n        # resampling the data\n        X_resampled, Y_resampled = sampler.fit_resample(x, y)\n        \n        # Splitting the data\n        X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2)\n        \n        for mname, model in models.items():\n            #print(sname + mname)\n            model.fit(X_train, Y_train)\n            model_scores[sname+mname] = model.score(X_test, Y_test)\n\n    return model_scores","d85a52bb":"model_scores = fit_resample_and_score(models=models,\n                      samplers=resamplers,\n                      x=X,\n                      y=Y)\n\nmodel_scores","e6fd4d0a":"model_compare = pd.DataFrame(model_scores, index=['accuracy'])\nmodel_compare.T.plot(kind=\"bar\", figsize=(20,10));","4d030e10":"print(f\"The model with highest accuracy is rosKNN with {max(model_scores.values())}\")","21b35782":"X_resampled, Y_resampled = RandomOverSampler(sampling_strategy='minority').fit_resample(X,Y)","b7375296":"X_train, X_test, Y_train, Y_test = train_test_split(X_resampled, Y_resampled)","ceef0a5d":"train_scores = []\ntest_scores = []\nneigbors = range(1,21)\nknn = KNeighborsClassifier()\n\nfor i in neigbors:\n    \n    knn.set_params(n_neighbors = i)\n    \n    knn.fit(X_train, Y_train)\n    \n    train_scores.append(knn.score(X_train, Y_train))\n    \n    test_scores.append(knn.score(X_test, Y_test))","5970d791":"plt.plot(neighbours, train_scores, label=\"Train score\")\nplt.plot(neighbours, test_scores, label=\"Test score\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")","2b385730":"# Using the best hyperparameters\nclf_knn = KNeighborsClassifier(n_neighbors=1)\n\nclf_knn.fit(X_train, Y_train)\n\nclf_knn.score(X_test, Y_test)","fd50026b":"plot_roc_curve(clf_knn, X_test, Y_test);","bd93a026":"Y_preds = clf_knn.predict(X_test)\nprint(classification_report(Y_test, Y_preds))","2bd9ed67":"plot_conf_matrix(Y_test, Y_preds)","6720bd14":"id_final = pd.Series(X_test[\"id\"])\nstroke_final = pd.Series(Y_test)","51d0654b":"df_final = pd.DataFrame({\"id\": id_final, \"stroke\": stroke_final})","6fe83be5":"df_final.dropna(inplace=True)\ndf_final.isna().sum()\ndf_final.to_csv(\"submission.csv\")","02bf5805":"4. Let's convert `residence_type feature`, \n* `Urban`: `0`\n* `Rural`: `1`","df50f039":"One Final EDA to check the relationship between independent variables using correlation matrix","94a8cc2c":"There's 0.0010183299389002753 decrease in logistic regression score after hyper tuning","356e9415":"### `work_type` with respect to `stroke`","3b2e95c6":"The hyper tuned random classifier performs better than the model\n\nLet's find out more metrics regading the RandomSearchCV randomforestclassifier model\n\n## Model evaluation beyond accuracy\n\nWe'll use the below metrics,\n\n1. ROC and AUC curve\n2. Confusion matrix\n3. Classification report\n4. Recall score\n5. F1 score\n\nTo make comparisons and evalutions we'll need predictions, let's get em.","0e8e0e80":"Looks like our model seems to be overfitting, since it's an unbalanced dataset. It's choosing the true negative for most cases to minimize error. Let's evaluate the model using F1 score","a7a4928a":"This is the final model KNN with RandomOverSampling","3398413b":"3. Let's convert `work_type` feature using below mapping\n* `Private`: `0`\n* `Self-employed`: `1`\n* `Govt_job`: `2`\n* `children`: `3`\n* `Never_worked`: `4`","90bf5a6a":"### `age` and `average_glucose_level` impact on `stroke`","bca58324":"We've written a function to resample and model the data \ud83d\ude0e.","de500879":"### `residence_type` with respect to `stroke`","a36dd8eb":"**hypertension** people are more susceptible to stroke","3bdd0bae":"There seems to be **no improvement** after hypertuning the knneighbours\n\nModel seems to be working very well on train data like mentioned above, seems like overfitting. Let's perform hypertuning for other models. Let's check some more metrics like classification_report, ROC_curve","ca90cbad":"RandomizedSearchCV random forest model does good with an auc of 0.84\n\nLet's proceed with confusion matrix","352ead17":"**urban** people have a little edge than **rural** on stroke possiblity","52b09cfc":"## Dataset based imbalance techniques - oversampling","ee05dd26":"Let's give a try if the model improves with best hyper parameters and calculate the `cross_val_score` for all the metrics and visualize them","bfb29a67":"In cross validation F1 become zero meaning our model performance is \ud83d\ude15\ud83d\ude11\n\nCross validation is not working.","a2dfd6f2":"Higer age and less bmi are contributing towards a possiblity in stroke","a3c4dd10":"### `age` with respect to `stroke`","60361e23":"### `bmi` with respect to `stroke`","d3eef847":"**never smoked** are affected by stroke higher than other categories, but the sample size for never_smoked is 37%.","f1792938":"### Getting data ready\n\n1. Let's convert `Male` to `0` and `Female` to `1` in `gender` feature","aa8f790f":"Let's convert the categorical features to numbers using pandas","7e40b13c":"### `Stroke` with respect to `gender`","6332f770":"Let's visualize the confusion matrix using sns heatmap\n\n#### Confusion matrix","75d06892":"# Hypertuning once again","ce9fd78d":"**ever_married** people have stroke more than **never_married**. (Not sure what to say about this one \ud83d\ude1c)","a0e05404":"Let's see if we can improve the performace of this model","ae80b226":"### `hyper_tension` with respect to `stroke`","f7f31445":"## Modelling","acd6a433":"## Exploratory Data Analysis","34298637":"## Hypertuning the three models","d4579e79":"### `smoking_type` with respect to `stroke`","df2c7c82":"**Now all the categorical features are converted to numerical** Let's model yeah\ud83d\ude0e","a18c4fb1":"**Female** with stroke are more than **Male**, There's an outlier with one sample in **other** gender we'll drop the sample.","1029009a":"## 1. Problem Definition\n\nCan we predict or classify whether the patient is susceptible for stroke or not?","e5d23833":"1. Let's split our features and target","0383955d":"### `Stroke` with respect to `ever_married`","36e664f4":"#### Classification report","8b83ed02":"Much better. A higher positive value means a potential positive correlation (increase) and a higher negative value means a potential negative correlation (decrease).","1fb8ebb3":"**Aged** people are more susceptible to stroke than young ones","7bcf7460":"4. Let's convert `smoking_status feature`, \n* `formerly smoked`: `0`\n* `never smoked`: `1`\n* `smokes`: `2`\n* `Unknown`: `3`","cce32e71":"2. Let's convert `Yes` to `0` and `No` to `1` in `ever_married` feature","731e1c03":"Let's find below metrics as a final destination\n1. ROC curve\n2. classificationreport\n3. confusionmatrix","b728781d":"### Inferences\n\nLooking at the classification report and confusion matrix, The model is unable to predict true positives at all due to the class imbalance in dataset.\n\nWe can see this more clearly in classificatio report's f1-score, 0.97 for class 0 and 0 for class 1.\n\nmacro average which will be bad when there are class imbalances and it's 0.49 which is pretty poor\n","b061d4a6":"### Tuning model with RandomizedSearchCV","a463c9e7":"We have an **unbalanced** target column, we have more samples for false and very less samples for negative","4b36b180":"### `age` and `bmi` impact on `stroke`","6a9c4d3e":"People in **private** are more susceptibe to stroke compared to other categories, but *private* category sample is 57% - conveying they have the option to visit the hospital or have a health check more compared to other categories","0fb5f412":"### Let's train the data with few models and see how it goes","be786dd8":"The model had improved a bit after hypertuning with n_neighbors=1","2dda9052":"Older people with low glucose level are more susceptible to stroke"}}