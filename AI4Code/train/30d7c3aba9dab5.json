{"cell_type":{"dff6ca6f":"code","3fe8e7af":"code","6e32d24f":"code","b013be74":"code","ce20afa6":"code","4efdc453":"code","aba0249c":"code","793b23ef":"code","902051a8":"code","fda48239":"code","fb65cd55":"code","e8d9dbe2":"code","0947824d":"markdown","d8c1868b":"markdown","8364f29a":"markdown","7db4944b":"markdown","1b01e864":"markdown","56095a71":"markdown"},"source":{"dff6ca6f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3fe8e7af":"from pynvml import *\nimport torch\nfrom torch import tensor, from_numpy\nfrom torch import nn, stack\nimport numpy as np\nimport pickle\nfrom random import randint\nimport time\nfrom tqdm.notebook import tqdm\n#!mkdir -p khazar","6e32d24f":"def mem_gpu(message = False): #\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u0432\u043e\u0431\u043e\u0434\u043d\u043e\u0433\u043e \u043c\u0435\u0441\u0442\u0430 \u0432 GPU\n    if message:\n        nvmlInit()\n        h = nvmlDeviceGetHandleByIndex(0)\n        info = nvmlDeviceGetMemoryInfo(h)\n        return info.free\n    else:\n        nvmlInit()\n        h = nvmlDeviceGetHandleByIndex(0)\n        info = nvmlDeviceGetMemoryInfo(h)\n        print(f'total    : {info.total}')\n        print(f'free     : {info.free}')\n        print(f'used     : {info.used}')\nmem_gpu()","b013be74":"def save_dict(obj, name): #\u0421\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0441\u043b\u043e\u0432\u0430\u0440\u044f \u0441 \u0440\u0435\u0431\u0440\u0430\u043c\u0438\n    with open(name + '.pkl', 'wb') as f:\n        #pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n        pickle.dump(obj, f, 4)\ndef load_dict(name): #\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0441\u043b\u043e\u0432\u0430\u0440\u044f \u0441 \u0440\u0451\u0431\u0440\u0430\u043c\u0438\n    with open(name + '.pkl', 'rb') as f:\n        return pickle.load(f)\n        #<span style=\"color:blue\">some *blue* text<\/span>.","ce20afa6":"ohe = np.load('..\/input\/genome-data-of-a-khazar-origin\/khazar_ohe.npz')['arr_0']\nprint('\u0420\u0430\u0437\u043c\u0435\u0440 \u0442\u0435\u043d\u0437\u043e\u0440\u0430:', ohe.shape)","4efdc453":"with open('..\/input\/genome-data-of-a-khazar-origin\/snip_dict.pkl', 'rb') as f:\n    snip_dict = pickle.load(f)\nprint('\u0414\u043b\u0438\u043d\u0430 \u0441\u043b\u043e\u0432\u0430\u0440\u044f: ', len(snip_dict))","aba0249c":"cnt_ohe = ohe.sum(axis=0, dtype = np.int16) #\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043b\u043e\u043a\u0443\u0441 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432 \u043f\u043e \u0432\u0441\u0435\u043c \u043e\u0431\u0440\u0430\u0437\u0446\u0430\u043c (1774, 270898, 4) -> (270898, 4)\nsample_size = ohe.shape[0] #\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0431\u0440\u0430\u0437\u0446\u043e\u0432 \u0414\u041d\u041a\np = (cnt_ohe \/ ohe.shape[0]).astype(np.float32) #\u0447\u0430\u0441\u0442\u043e\u0442\u0430 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043b\u043e\u043a\u0443\u0441 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430 \u0432 \u043d\u0430\u0431\u0440\u0435 \u0438\u0437 1774 \u0441\u044d\u043c\u043f\u043b\u043e\u0432\nidx_ohe_flattened_zeros = (cnt_ohe.flatten() == 0).astype(np.bool) #\u0432\u0435\u043a\u0442\u043e\u0440\u043d\u0430\u044f \u043c\u0430\u0441\u043a\u0430 \u043d\u0443\u043b\u0435\u0439\ncnt_ohe_flattened_variating = cnt_ohe.flatten()[~idx_ohe_flattened_zeros] #\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043b\u043e\u043a\u0443\u0441 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432 \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c \u0438\u0445 \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u0432 \u0432\u0435\u043a\u0442\u043e\u0440 \u0438 \u0443\u0434\u0430\u043b\u044f\u0435\u043c \u043d\u0443\u043b\u0435\u0432\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\np_flattened_variating = (cnt_ohe_flattened_variating \/ sample_size).astype(np.float32) #\u0447\u0430\u0441\u0442\u043e\u0442\u044b \u0432 \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u0435","793b23ef":"if torch.cuda.is_available(): \n    print('CUDA available')\n    device = torch.device(\"cuda:0\")\n    torch.cuda.empty_cache()\n    #mem_gpu()","902051a8":"%%time\ncuda_ohe = from_numpy(ohe).to(device) #\u043e\u0441\u043d\u043e\u0432\u043d\u043e\u0439 \u0442\u0435\u043d\u0437\u043e\u0440\ncuda_p_flattened_variating = from_numpy(p_flattened_variating).to(device)\ncuda_cnt_ohe_flattened_variating = from_numpy(cnt_ohe_flattened_variating).to(device)\ncuda_idx_ohe_flattened_zeros = from_numpy(idx_ohe_flattened_zeros).to(device)\nprint('cuda_ohe:', cuda_ohe.dtype,  'cuda_p_flattened_variating:', cuda_p_flattened_variating.dtype)\nprint('cuda_idx_ohe_flattened_zeros:', cuda_idx_ohe_flattened_zeros.dtype, \n        'cuda_cnt_ohe_flattened_variating:', cuda_cnt_ohe_flattened_variating.dtype)","fda48239":"%%time\nbase_locus_variant_sorting = from_numpy(np.abs(cnt_ohe_flattened_variating - int(ohe.shape[0]\/2)).argsort()).to(device)\ncorrection_k = from_numpy(np.arange(cnt_ohe.flatten().shape[0])[~idx_ohe_flattened_zeros] -\\\n        np.arange(cnt_ohe.flatten().shape[0])[:cnt_ohe_flattened_variating.shape[0]]).to(device) #\u043a\u043e\u0440\u0440\u0435\u043a\u0442\u0438\u0440\u0443\u044e\u0449\u0438\u0435 \u043a\u043e\u044d\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u044b \u0434\u043b\u044f \u0432\u043e\u0441\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u044f \n                                                                                                #\u0438\u0437\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u043e\u0439 \u0430\u0431\u0441\u043e\u043b\u044e\u0442\u043d\u043e\u0439 \u043f\u043e\u0437\u0438\u0446\u0438\u0438 \u043b\u043e\u043a\u0443\u0441 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430\n                                                                                                #\u0432 \u0442\u0435\u043d\u0437\u043e\u0440\u0435 \u043f\u043e\u0441\u043b\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u043c\u0430\u0441\u043a\u0438 \u0443\u0434\u0430\u043b\u044f\u044e\u0449\u0435\u0439 \u043d\u0443\u043b\u0438\nmem_gpu()\n\nprint('base_locus_variant_sorting:', base_locus_variant_sorting.dtype, 'correction_k:', correction_k.dtype)","fb65cd55":"def checkpoint(phase, base_loc_n, final_graph_dict, group_array, mask_array,\n               path = '.\/'):\n    save_dict(final_graph_dict, f'{path}phase{phase}_graph_dict_{base_loc_n}')\n    np.save(f'{path}phase{phase}_group_array{base_loc_n}.npy', \n            group_array)\n    np.save(f'{path}phase{phase}_mask_array{base_loc_n}.npy', \n            mask_array.cpu().numpy())\n    np.save(f'{path}phase{phase}_base_loc_n{base_loc_n}.npy', \n            np.array(base_loc_n))","e8d9dbe2":"restore = False #\u041d\u0430\u0447\u0438\u043d\u0430\u0435\u043c \u0441\u043d\u0430\u0447\u0430\u043b\u0430\njoin_possibilities = 2 #\u0424\u043b\u0430\u0433 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0433\u0440\u0443\u043f\u043f, \u043a\u043e\u0433\u0434\u0430 \u0432\u0441\u0435 \u043b\u043e\u043a\u0443\u0441 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u0441\u043b\u0438\u0432\u0430\u044e\u0442\u0441\u044f \u0432 \u043e\u0434\u0438\u043d \u0433\u0440\u0430\u0444, \u0441\u0431\u043e\u0440\u043a\u0430 \u043e\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442\u0441\u044f, \u043d\u0430\u0447\u0430\u043b\u044c\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 >1\nphase = 1 #\u0424\u0430\u0437\u0430 \u0441\u0431\u043e\u0440\u043a\u0438 2^pahse = \u0440\u0430\u0437\u043c\u0435\u0440 \u0433\u0440\u0443\u043f\u043f\u044b \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u043e\u0431\u0438\u0440\u0430\u044e\u0442 \u043b\u043e\u043a\u0443\u0441-\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u0432 \u0442\u0435\u043a\u0443\u0449\u0435\u0439 \u0444\u0430\u0437\u0435\nwhile join_possibilities > 1:    \n    base_loc_n = 0 #\u041d\u043e\u043c\u0435\u0440 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438         \n    mask_array = from_numpy(np.ones(cnt_ohe_flattened_variating.shape[0], dtype = bool)).to(device) #\u043c\u0430\u0441\u043a\u0430 \u043e\u0442\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0445 (\u0432\u044b\u0431\u0440\u0430\u043d\u043d\u044b\u0445) \u043b\u043e\u043a\u0443\u0441-\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432\n    group_array = np.zeros(mask_array.shape[0]) #\u0432\u0435\u043a\u0442\u043e\u0440 \u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0438 \u043b\u043e\u043a\u0443\u0441-\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432   \n    final_graph_dict = {} #\u0421\u043b\u043e\u0432\u0430\u0440\u044c \u0440\u0435\u0431\u0435\u0440\n    if restore:\n        base_loc_n = ... #\u041d\u043e\u043c\u0435\u0440 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438 \u0432 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0438 \u0444\u0430\u0439\u043b\u0430\n        mask_array = from_numpy(np.load(f'phase{phase}_mask_array{base_loc_n}.npy')).to(device) #\u043c\u0430\u0441\u043a\u0430 \u043e\u0442\u043e\u0431\u0440\u0430\u043d\u043d\u044b\u0445 (\u0432\u044b\u0431\u0440\u0430\u043d\u043d\u044b\u0445) \u043b\u043e\u043a\u0443\u0441-\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432\n        group_array = from_numpy(np.load(f'phase{phase}_group_array{base_loc_n}.npy')).to(device) #\u0432\u0435\u043a\u0442\u043e\u0440 \u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0438 \u043b\u043e\u043a\u0443\u0441-\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432   \n        final_graph_dict = load_dict(f'phase{phase}_graph_dict_{base_loc_n}') #\u0421\u043b\u043e\u0432\u0430\u0440\u044c \u0440\u0435\u0431\u0435\u0440\n        restore = False\n    pbar = tqdm()\n    pbar.reset(total=int(mask_array.sum().cpu().numpy().flatten()[0])) #\u0441\u043b\u0430\u0439\u0434\u0435\u0440 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f\n    start = time.time()\n    #-------------------------------------------------------------------------------\n    while (int(mask_array.sum()) > 1):\n        base_loc_n +=1\n        pos2dict = max2dict = max_edge2dict = min2dict = min_edge2dict = 'Nan'\n        #-----------------------------------\u0432\u044b\u0431\u043e\u0440 \u043e\u043f\u043e\u0440\u043d\u043e\u0433\u043e(\u0431\u0430\u0437\u043e\u0432\u043e\u0433\u043e) \u043b\u043e\u043a\u0443\u0441-\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430------------------------\n        if int(mask_array.sum()) > 12:\n            f10 = randint(0, 11)  #\u043e\u0434\u0438\u043d \u0438\u0437 \u043f\u0435\u0440\u0432\u044b\u0445 10 \u043e\u043f\u043e\u0440\u043d\u044b\u0445 \u043b\u043e\u043a\u0443\u0441 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432\n        else:\n            f10 = 0\n        position = base_locus_variant_sorting[mask_array[base_locus_variant_sorting]][f10] #\u0432\u044b\u0431\u043e\u0440 \u0431\u0430\u0437\u043e\u0432\u043e\u0433\u043e \u043b\u043e\u043a\u0443\u0441 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430 (\u043f\u043e\u0437\u0438\u0446\u0438\u044f \u0432 \u0432\u0435\u043a\u0442\u043e\u0440\u0435)\n        if phase == 1:\n            mask_array[position] = False   #\u041e\u0431\u043d\u0443\u043b\u044f\u0435\u043c \u043c\u0430\u0441\u043a\u0443\n            group_array[position] = base_loc_n #\u041f\u0440\u0438\u0441\u0432\u0430\u0435\u0432\u0430\u0435\u043c \u043b\u043e\u043a\u0443\u0441 \u0432\u0430\u0440\u0438\u0430\u043d\u0442 \u0433\u0440\u0443\u043f\u043f\u0435\n        else:\n            group_idx = group_array[position] #\u041d\u0430\u0445\u043e\u0434\u0438\u043c \u0433\u0440\u0443\u043f\u043f\u0443 \u043f\u043e \u0434\u0435\u043b\u0435\u0433\u0430\u0442\u0443\n            mask_array[(group_array == group_idx).nonzero()] = False #\u041e\u0431\u043d\u0443\u043b\u044f\u0435\u043c \u043c\u0430\u0441\u043a\u0438 \u0432 \u0433\u0440\u0443\u043f\u043f\u0435\n\n        cuda_p_A = cuda_p_flattened_variating[position]\n        position += correction_k[position] #\u0410\u0431\u0441\u043e\u043b\u044e\u0442\u043d\u044b\u0439 \u0438\u043d\u0434\u0435\u043a\u0441 \u043b\u043e\u043a\u0443\u0441 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430\n        pos2dict = position.cpu().numpy().flatten()[0]\n        if mask_array.any(): #\u0435\u0441\u043b\u0438 \u043e\u0441\u0442\u0430\u043b\u0438\u0441\u044c \u043b\u043e\u043a\u0443\u0441-\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b\n            ind1 = position \/\/ 4 #\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u043d\u043e\u043c\u0435\u0440 \u043e\u043f\u043e\u0440\u043d\u043e\u0433\u043e \u043b\u043e\u043a\u0443\u0441\u0430\n            state1 = position % 4 #\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u043d\u043e\u043c\u0435\u0440 \u043e\u043f\u043e\u0440\u043d\u043e\u0433\u043e \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430 \u0434\u043b\u044f \u043b\u043e\u043a\u0443\u0441\u0430\n            if ~(ohe[:, ind1,state1]==True).any(): #\u043d\u0435\u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430, \u0447\u0442\u043e \u0447\u0430\u0441\u0442\u043e\u0442\u0430 \u043d\u0435 \u043d\u0443\u043b\u0435\u0432\u0430\u044f\n                print('cuda_ohe = 0  ', ind1, state1)\n                break\n            #-----------------------------------\u0432\u044b\u0431\u043e\u0440 \u043e\u043f\u043e\u0440\u043d\u043e\u0433\u043e(\u0431\u0430\u0437\u043e\u0432\u043e\u0433\u043e) \u043b\u043e\u043a\u0443\u0441-\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430 end------------------------\n            #-----------------------------------\u0432\u044b\u0431\u043e\u0440 \u0434\u0435\u043b\u0435\u0433\u0430\u0442\u043e\u0432 \u043b\u043e\u043a\u0443\u0441-\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432 \u0438 \u0440\u0430\u0441\u0447\u0435\u0442 \u0440\u0451\u0431\u0435\u0440---------------------\n            cuda_ohe_l_A = cuda_ohe[cuda_ohe[:, ind1,state1]==True,:,:]\n            sample_size_l_A = cuda_ohe_l_A.shape[0]\n            cuda_cnt_ohe_l_A = cuda_ohe_l_A.sum(axis=0, dtype=torch.int16)\n            cuda_cnt_ohe_l_A_flattened_variating = cuda_cnt_ohe_l_A.flatten()[~(idx_ohe_flattened_zeros)]\n            cuda_p_l_A_flattened_variating = cuda_cnt_ohe_l_A_flattened_variating \/ sample_size_l_A\n            tmp_p = (cuda_cnt_ohe_flattened_variating + cuda_cnt_ohe_l_A_flattened_variating) \/ (sample_size + sample_size_l_A)\n            z =  (cuda_cnt_ohe_flattened_variating\/sample_size -\\\n            cuda_cnt_ohe_l_A_flattened_variating\/sample_size_l_A) \/ torch.sqrt(tmp_p*(1-tmp_p)*(1.0\/sample_size + 1.0\/sample_size_l_A)) #z\n            minimax = torch.max(z) - torch.min(z) + 1.0\n\n            outlier_max = torch.argmax(z - (minimax)*(~mask_array))\n            edge_weight_max = cuda_p_l_A_flattened_variating[outlier_max] * cuda_p_A #cuda_p_flattened_variating[outlier_max]\n            if phase == 1:\n                mask_array[outlier_max] = False ###\n                group_array[outlier_max] = base_loc_n ###\n            else:\n                group_idx_opp = group_array[outlier_max]\n                mask_array[(group_array == group_idx_opp).nonzero()] = False\n                group_array[(group_array == group_idx_opp).nonzero()] = group_idx\n            outlier_max += correction_k[outlier_max]\n            max2dict = outlier_max.cpu().numpy().flatten()[0]\n            max_edge2dict = edge_weight_max.cpu().numpy().flatten()[0]\n            if mask_array.any():\n                outlier_min = torch.argmin(z + (minimax)*(~mask_array))\n                edge_weight_min = cuda_p_l_A_flattened_variating[outlier_min] * cuda_p_A #cuda_p_flattened_variating[outlier_min] \n                if phase == 1:\n                    mask_array[outlier_min] = False ###\n                    group_array[outlier_min] = base_loc_n ###\n                else:\n                    group_idx_opp = group_array[outlier_min]\n                    mask_array[(group_array == group_idx_opp).nonzero()] = False\n                    group_array[(group_array == group_idx_opp).nonzero()] = group_idx          \n                outlier_min += correction_k[outlier_min] \n                min2dict = outlier_min.cpu().numpy().flatten()[0]\n                min_edge2dict = edge_weight_min.cpu().numpy().flatten()[0]           \n            #-----------------------------------\u0432\u044b\u0431\u043e\u0440 \u0434\u0435\u043b\u0435\u0433\u0430\u0442\u043e\u0432 \u043b\u043e\u043a\u0443\u0441-\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432 \u0438 \u0440\u0430\u0441\u0447\u0435\u0442 \u0440\u0451\u0431\u0435\u0440 end-----------------\n        final_graph_dict.update({\n                        pos2dict:\n                                (\n                                  max2dict, \n                                  max_edge2dict,\n                                  min2dict, \n                                  min_edge2dict \n                                )\n                                })\n\n\n        if base_loc_n \/ 100 == base_loc_n \/\/ 100:       \n            #pbar.write(f' \u0424\u0430\u0437\u0430-{phase}|\u0421\u043a\u043e\u0440\u043e\u0441\u0442\u044c:{round((time.time() - start)\/base_loc_n, 4)}\\\n#-\u0441\u0435\u043a\/\u0440\u0435\u0431\u0440\u043e|GPU:{round(mem_gpu(message=True) \/ 1000000000, 1)}GB free')\n            #\u041e\u0447\u0438\u0441\u0442\u043a\u0430 GPU \u043e\u0442 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445\n            del cuda_ohe_l_A, cuda_cnt_ohe_l_A, cuda_cnt_ohe_l_A_flattened_variating\n            del cuda_p_l_A_flattened_variating, z\n            torch.cuda.empty_cache()\n        \n        if base_loc_n \/ 50000 == base_loc_n \/\/ 50000:\n            #print(f'SAVING......phase {phase}, \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u044f {base_loc_n}')\n            checkpoint(phase, base_loc_n, final_graph_dict, group_array, mask_array)\n        pbar.update(pow(2, phase))\n    #---------------------------------------------------------------------------------\n    #tqdm.write(f'SAVING......phase {phase}, \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u044f {base_loc_n}')\n    pbar.refresh()\n    checkpoint(phase, base_loc_n, final_graph_dict, group_array, mask_array)\n    join_possibilities = len(np.unique(group_array))\n    phase +=1","0947824d":"### <font color=\"chocolate\"><ins>\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u044b<\/ins><\/font>\n\n<img src=\"https:\/\/www.mdpi.com\/remotesensing\/remotesensing-08-00636\/article_deploy\/html\/images\/remotesensing-08-00636-g003.png\" width=\"560\" height=\"280\">\n\n### <font color=\"chocolate\">\u041b\u043e\u043a\u0443\u0441-\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u0432 \u0442\u0435\u043d\u0437\u043e\u0440\u0435<\/font>","d8c1868b":"### <font color=\"chocolate\"><ins>\u0417\u0430\u0440\u0430\u043d\u0435\u0435 \u0441\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0432\u0441\u0435 \u043b\u043e\u043a\u0443\u0441 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b \u043d\u0430 \u043f\u0440\u0435\u0434\u043c\u0435\u0442 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438<\/ins><\/font>\n\n\n<img src=\"https:\/\/www.hershey.k12.pa.us\/cms\/lib\/PA09000080\/Centricity\/Domain\/513\/Normal.gif\" width=\"420\" height=\"280\">\n\n<font color=\"chocolate\">\u0441\u0430\u043c\u044b\u0435 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u0432\u043d\u044b\u0435 (p = 0.5) \u0434\u043e\u043b\u0436\u043d\u044b \u0441\u0442\u0430\u0442\u044c \u043f\u0435\u0440\u0432\u044b\u043c\u0438 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u0430\u043c\u0438 \u043d\u0430 \u0431\u0430\u0437\u043e\u0432\u044b\u0435 \u043b\u043e\u043a\u0443\u0441 - \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u044b<\/font>","8364f29a":"### <font color=\"chocolate\"><ins>\u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u043d\u0430\u043b\u0438\u0447\u0438\u0435 GPU \u0438 \u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u044b\u0439 \u043e\u0431\u044a\u0451\u043c<\/ins><\/font>\n\n<img src=\"https:\/\/images-na.ssl-images-amazon.com\/images\/I\/61S5ttfWftL._AC_SL1250_.jpg\" width=\"420\" height=\"280\">","7db4944b":"### <font color=\"chocolate\"><ins>\u041e\u0441\u043d\u043e\u0432\u043d\u043e\u0439 \u0446\u0438\u043a\u043b<\/ins><\/font>\n\n<img src=\"https:\/\/www.wilsoninfo.com\/bicycle\/2018-racing-bike.gif\" width=\"280\" height=\"160\">","1b01e864":"### <font color=\"chocolate\"><ins>\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0442\u0435\u043d\u0437\u043e\u0440\u0430<\/ins><\/font>\n\n<img src=\"https:\/\/image.made-in-china.com\/44f3j00MlptHwdRAZrv\/Sidelifter-Hydraulic-Lift-a-Load-40FT-Container-Crane-Trailer-Truck.jpg\" width=\"280\" height=\"280\">","56095a71":"### <font color=\"chocolate\"><ins>\u041f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 \u0442\u0435\u043d\u0437\u043e\u0440\u044b numpy -> torch (\u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u044b)<\/ins><\/font>\n\n<img src=\"https:\/\/torchio.readthedocs.io\/_images\/fpg_progressive.gif\" width=\"420\" height=\"280\">"}}