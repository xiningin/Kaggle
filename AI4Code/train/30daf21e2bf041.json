{"cell_type":{"d1960f99":"code","f8eed461":"code","205da291":"code","a9dec62f":"code","492018a3":"code","bb6ee250":"code","ace9bf74":"code","82cebdda":"code","84eca893":"code","d0c8b3c9":"code","efef9831":"code","54767bc6":"code","4cf510e6":"code","fed2aed3":"code","98cd66f9":"code","d717c0e1":"code","cedf0519":"code","84f13603":"code","d8131219":"code","2db825a6":"code","600f3a9c":"code","ff91236f":"code","e6c75bb2":"code","39825e8c":"code","e354ecb5":"code","32b65e65":"code","972dd13f":"code","06b0000e":"code","c152c661":"code","46ec06fd":"code","e55a9524":"code","98248ff7":"code","7d09f704":"code","343925af":"code","2526a0f7":"code","e4067749":"code","c6b7a19d":"code","caef7e58":"code","750847e2":"code","c5488ea7":"code","2d4cffa6":"code","4a8c07a2":"code","dd89f33f":"code","8c45379e":"code","e8723de7":"code","56f1a270":"code","825953be":"markdown","5a16451c":"markdown","de16863f":"markdown","433255ab":"markdown","e485df75":"markdown","19ac0594":"markdown","88cdeb75":"markdown","e5497b7c":"markdown","4f319d55":"markdown","c474d0df":"markdown","0b428850":"markdown","205a4de4":"markdown","d1f1d5b3":"markdown","975da723":"markdown","8b627ee1":"markdown"},"source":{"d1960f99":"# Importing needed libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# NLTK tools for text processing\nimport re, nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS\n\n\n# Modeling packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'","f8eed461":"## Run the codes below to install language translator and have a view at the supported languages.\n!pip install google_trans_new\nimport google_trans_new\nprint(google_trans_new.LANGUAGES)\n\n# Special Portuguese stemmer package but this was not helpful when I used it.\n#nltk.download('rslp')\n#stemmer = nltk.stem.RSLPStemmer()","205da291":"# Running an example of google translate(from portuguese to english), Data Connnection is required.\nfrom google_trans_new import google_translator\ntranslator = google_translator()\ntranslate_text = translator.translate('Excelente mochila, entrega super r\u00e1pida.',lang_tgt='en',lang_src='pt' )\nprint(translate_text)","a9dec62f":"# Reading in the reviews dataset\nreview_df = pd.read_csv('..\/input\/brazilian-ecommerce\/olist_order_reviews_dataset.csv')\nreview_df.head()\nreview_df.shape","492018a3":"# We have missing values in the reviews and their titles\nreview_df.isnull().sum()","bb6ee250":"review_data_title = review_df['review_comment_title']\nreview_data = review_df.drop(['review_comment_title'],axis=1)\n","ace9bf74":"# Dropping NaN values\nreview_data  = review_data.dropna()\nreview_data_title = review_data_title.dropna()\n","82cebdda":"# Resetting the reviews index and visualizing the data\nreview_data = review_data.reset_index(drop=True)\nreview_data.head(3)\nreview_data.shape","84eca893":"# Resetting the reviews titles index and visualizing the data\nreview_data_title = review_data_title.reset_index(drop=True)\nreview_data_title.head(3)\nreview_data_title.shape","d0c8b3c9":"# List of Portuguese stopwords to be used( Uncomment to see)\n#stopwords.words('portuguese')","efef9831":" comments = []\nstop_words = set(stopwords.words('portuguese'))\n\n\nfor words in review_data['review_comment_message']:\n    only_letters = re.sub(\"[^a-zA-Z]\", \" \",words)\n    tokens = nltk.word_tokenize(only_letters) #tokenize the sentences\n    lower_case = [l.lower() for l in tokens] #convert all letters to lower case\n    filtered_result = list(filter(lambda l: l not in stop_words, lower_case)) #Remove stopwords from the comments\n    comments.append(' '.join(filtered_result))","54767bc6":"# Visualizing the cleaned reviews data(uncomment to see)\n# comments","4cf510e6":"#Using wordcloud to visualize the comments\nunique_string=(\" \").join(comments)\nwordcloud = WordCloud(width = 2000, height = 1000,background_color='white').generate(unique_string)\nplt.figure(figsize=(20,12))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","fed2aed3":"# Using CountVectorizer to get the most important unigrams\nco = CountVectorizer(ngram_range=(1,1))\ncounts = co.fit_transform(comments)\nimportant_unigrams = pd.DataFrame(counts.sum(axis=0),columns=co.get_feature_names()).T.sort_values(0,ascending=False).head(50)","98cd66f9":"# Next, we reset the index, rename the columns and apply the translate module to get the english translations \nimportant_unigrams = important_unigrams.reset_index()\nimportant_unigrams.rename(columns={'index':'unigrams',0:'frequency'},inplace=True)\n\nimportant_unigrams['english_translation'] = important_unigrams['unigrams'].apply(translator.translate)\nimportant_unigrams","d717c0e1":"# Using CountVectorizer to get the most important bigrams\nco = CountVectorizer(ngram_range=(2,2))\ncounts = co.fit_transform(comments)\nimportant_bigrams = pd.DataFrame(counts.sum(axis=0),columns=co.get_feature_names()).T.sort_values(0,ascending=False).head(50)","cedf0519":"# Next, we reset the index, rename the columns and apply the translate module to get the english translations \nimportant_bigrams=important_bigrams.reset_index()\nimportant_bigrams.rename(columns={'index':'bigrams',0:'frequency'},inplace=True)\n\nimportant_bigrams['english_translation'] = important_bigrams['bigrams'].apply(translator.translate)\nimportant_bigrams","84f13603":"# Using CountVectorizer to get the most important trigrams\nco = CountVectorizer(ngram_range=(3,3))\ncounts = co.fit_transform(comments)\nimportant_trigrams = pd.DataFrame(counts.sum(axis=0),columns=co.get_feature_names()).T.sort_values(0,ascending=False).head(50)","d8131219":"# Next, we reset the index, rename the columns and apply the translate module to get the english translations \nimportant_trigrams=important_trigrams.reset_index()\nimportant_trigrams.rename(columns={'index':'trigrams',0:'frequency'},inplace=True)\n\nimportant_trigrams['english_translation'] = important_trigrams['trigrams'].apply(translator.translate)\nimportant_trigrams","2db825a6":"# Before removing Nan values\nplt.figure(figsize = (14,6))\nsns.countplot(review_df['review_score'], color= 'red')","600f3a9c":"# After removing NaN values\nplt.figure(figsize = (14,6))\nsns.countplot(review_data['review_score'], color= 'red')","ff91236f":"# Processing the reviews titles data\ncomments_titles = []\nstop_words = set(stopwords.words('portuguese'))\n\n\nfor words in review_data_title:\n    only_letters = re.sub(\"[^a-zA-Z]\", \" \",words)\n    tokens = nltk.word_tokenize(only_letters) #tokenize the sentences\n    lower_case = [l.lower() for l in tokens] #convert all letters to lower case\n    filtered_result = list(filter(lambda l: l not in stop_words, lower_case)) #Remove stopwords from the comments\n    \n    comments_titles.append(' '.join(filtered_result))","e6c75bb2":"#Using wordcloud to visualize the comments titles\nunique_string=(\" \").join(comments_titles)\nwordcloud = WordCloud(width = 2000, height = 1000,background_color='white').generate(unique_string)\nplt.figure(figsize=(20,12))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","39825e8c":"# Using CountVectorizer to get the most important unigrams\nco = CountVectorizer(ngram_range=(1,1))\ncounts = co.fit_transform(comments_titles)\nimportant_unigrams_title = pd.DataFrame(counts.sum(axis=0),columns=co.get_feature_names()).T.sort_values(0,ascending=False).head(50)","e354ecb5":"important_unigrams_title=important_unigrams_title.reset_index()\nimportant_unigrams_title.rename(columns={'index':'unigrams_title',0:'frequency'},inplace=True)\n\nimportant_unigrams_title['english_translation'] = important_unigrams_title['unigrams_title'].apply(translator.translate)\nimportant_unigrams_title","32b65e65":"co = CountVectorizer(ngram_range=(2,2))\ncounts = co.fit_transform(comments_titles)\nimportant_bigrams_title = pd.DataFrame(counts.sum(axis=0),columns=co.get_feature_names()).T.sort_values(0,ascending=False).head(50)","972dd13f":"important_bigrams_title=important_bigrams_title.reset_index()\nimportant_bigrams_title.rename(columns={'index':'bigrams_title',0:'frequency'},inplace=True)\n\nimportant_bigrams_title['english_translation'] = important_bigrams_title['bigrams_title'].apply(translator.translate)\nimportant_bigrams_title","06b0000e":"co = CountVectorizer(ngram_range=(3,3))\ncounts = co.fit_transform(comments_titles)\nimportant_trigrams_title = pd.DataFrame(counts.sum(axis=0),columns=co.get_feature_names()).T.sort_values(0,ascending=False).head(50)","c152c661":"important_trigrams_title=important_trigrams_title.reset_index()\nimportant_trigrams_title.rename(columns={'index':'trigrams_title',0:'frequency'},inplace=True)\n\nimportant_trigrams_title['english_translation'] = important_trigrams_title['trigrams_title'].apply(translator.translate)\nimportant_trigrams_title","46ec06fd":"# Getting the number of words by splitting them by a space\nwords_per_review = review_data.review_comment_message.apply(lambda x: len(x.split(\" \")))\nwords_per_review.hist(bins = 100)\nplt.xlabel('Review Length (words)')\nplt.ylabel('Frequency')\nplt.show()","e55a9524":"print('Average words:', words_per_review.mean())\nprint('Skewness:', words_per_review.skew())","98248ff7":"# Checking the percentage of the review scores. 3 and 4 are the smallest scores here.\npercent_scores = 100 * review_data['review_score'].value_counts()\/len(review_data)\npercent_scores","7d09f704":"# Mapping the ratings\nreview_data['Sentiment_rating'] = np.where(review_data.review_score > 3,1,0)\n\n# Removing neutral reviews \nreview_data = review_data[review_data.review_score != 3]\n\n# Printing the counts of each class\nreview_data['Sentiment_rating'].value_counts()","343925af":"# Having a look, Rows having 3 as review scores have been removed.\nreview_data.head()\nreview_data['Sentiment_rating'].shape","2526a0f7":"comments = []\nstop_words = set(stopwords.words('portuguese'))\n\n\nfor words in review_data['review_comment_message']:\n    only_letters = re.sub(\"[^a-zA-Z]\", \" \",words)\n    tokens = nltk.word_tokenize(only_letters) #tokenize the sentences\n    lower_case = [l.lower() for l in tokens] #convert all letters to lower case\n    filtered_result = list(filter(lambda l: l not in stop_words, lower_case)) #Remove stopwords from the comments\n    \n    comments.append(' '.join(filtered_result))","e4067749":"co_counts = CountVectorizer(stop_words =set(stopwords.words('portuguese')),\n                             ngram_range=(1,4)) # unigrams to trigrams\n\nco_data = co_counts.fit_transform(comments)","c6b7a19d":"co_data","caef7e58":"# Splitting the sentiment scores column into train and test sets\nX_train_co, X_test_co, y_train_co, y_test_co= train_test_split(co_data,\n                                                                    review_data['Sentiment_rating'],\n                                                                    test_size = 0.2,\n                                                                    random_state = 0)","750847e2":"y_test_co.value_counts()\/y_test_co.shape[0]\n# 70% of sentiments are classified as positive\n# 30% of sentiments are classified as negative","c5488ea7":"# Defining and training the model\nlr_model = LogisticRegression(max_iter = 200)\nlr_model.fit(X_train_co, y_train_co)\n\n# Predicting the results\ntest_pred = lr_model.predict(X_test_co)\n\nprint(\"F1 score: \", f1_score(y_test_co,test_pred))","2d4cffa6":"lr_weights = pd.DataFrame(list(zip(co_counts.get_feature_names(), # get all the n-gram feature names\n                                   lr_model.coef_[0])), # get the logistic regression coefficients\n                          columns= ['words','weights']) # defining the colunm names\n\nPositive_sentiments = pd.DataFrame(lr_weights.sort_values(['weights'], ascending = False)[:15]) # top 15 more important features for positive reviews","4a8c07a2":"Positive_sentiments['english_translation'] = Positive_sentiments['words'].apply(translator.translate)\nPositive_sentiments","dd89f33f":"Negative_Sentiments = pd.DataFrame(lr_weights.sort_values(['weights'], ascending = False)[-15:]) # top-15 more important features for negative reviews\nNegative_Sentiments['english_translation'] = Negative_Sentiments['words'].apply(translator.translate)\nNegative_Sentiments","8c45379e":"from sklearn.decomposition import LatentDirichletAllocation, NMF\nvectorizer = CountVectorizer()\nmodel = vectorizer.fit(review_data.review_comment_message)\ndocs = vectorizer.transform(review_data.review_comment_message)\nlda = LatentDirichletAllocation(20)\nlda.fit(docs)\ndef print_top_words(model, feature_names, n_top_words):\n  for topic_idx, topic in enumerate(model.components_):\n    message = \"Topic #%d: \" % topic_idx\n    message += \" \".join([(feature_names[i])\n    for i in topic.argsort()[:-n_top_words - 1:-1]])\n    print(message)\n  print()\nprint_top_words(lda,vectorizer.get_feature_names(),10)","e8723de7":"pd.options.display.max_colwidth = 100\n\nLda_model = pd.DataFrame({'Model_Topics':['que comprei para vez um demorou n\u00e3o primeira foi mais',' da conforme produto entrega data antes como na prevista tudo','bom muito produto ok tudo recomendo entrega obrigado mto atendimento','que correios n\u00e3o de produto por dos foi correio no','n\u00e3o com que produto um veio do tem defeito muito','recomendo super sempre r\u00e1pido loja otimo produtos todos perfeito eu','do que melhor mas um de achei pouco produto eu','de com sem produto problemas que nunca lannister pois tive','de site no que as comprar lannister para qualidade n\u00e3o','prazo do antes produto chegou bem entrega no entregue dentro','n\u00e3o produto recebi ainda meu de at\u00e9 que do estou','muito gostei bem com satisfeita estou produto satisfeito bonito fiquei','na n\u00e3o da uma do cor de s\u00f3 que demora','n\u00e3o com que estou veio produto para em me comprei','produto veio as expectativas minhas diferente da n\u00e3o que um','produto que n\u00e3o do com veio no de na embalagem','produto qualidade entrega de excelente \u00f3timo r\u00e1pida boa \u00f3tima super','de um comprei dois produtos veio recebi s\u00f3 uma apenas','dia dias de at\u00e9 entrega produto em para compra chegou','foi entregue no produto prazo que eu em condi\u00e7\u00f5es esperava']})\nLda_model['Translation'] = Lda_model['Model_Topics'].apply(translator.translate) \nLda_model\n# ML generated topics","56f1a270":"review_data['topic']=lda.transform(docs).argmax(axis=1)\nreview_data.topic.value_counts(normalize=True).plot.bar()","825953be":"# IMPORTING PACKAGES AND LOADING THE DATA","5a16451c":"### Before building our sentiment analysis model, let us perform some extra EDA that would be beneficial for insights.","de16863f":"#### From the plot above, close to 60,000 people gave 5 star ratings while a little above 10,000 people gave 1 star ratings","433255ab":"#### From the code cells above, we can deduce that 58% of customers did not leave any reviews and only 11.7% of customers cared to give titles to their reviews. \n\n#### Next, we would preprocess the reviews and the titles in preparation for visualization and modelling","e485df75":"# SENTIMENT ANALYSIS","19ac0594":"#### To deal with these missing values, we would seperate the reviews and the titles and drop the missing rows seperately so that we don't have unequal shapes of rows.","88cdeb75":"#### From the output above, we can notice that customers say a lot of reviews containing words such as: product, delivery, deadline, arrived, received, recommend, good, excellent etc. However, we do not want to base our conclusion on these words,  we need to carry out further analysis to get more insights about these words. The bigrams and trigrams of the reviews would also be looked at.","e5497b7c":"# DATA PREPROCESSING AND EXPLORATORY DATA ANALYSIS","4f319d55":"### Observations\n#### The unigrams, bigrams and trigrams of the reviews titles data have further revealed the unhappy comments of displeased customers.These comments include: Getting incomplete delivery, Not receiving ordered goods, Delay in delivery, Low quality of delivered goods, Receiving wrong\/defect products. So far, these have been the major complains of unhappy customers and we have also seen a high degree of satisfaction among other customers.","c474d0df":"### Observations\n#### 1. From the unigrams,bigrams and trigrams above, we can safely say that most of the customers were satisfied with the delivery service, some others were highly satisfied with the products quality.\n\n#### 2. However, there are others who were not satisfied with the services rendered and we would like to investigate this anomaly further.","0b428850":"### From the plot above, After removing NaN values, close to 10,000 people gave 1 star ratings while a little above 20,000 people gave 5 star ratings which means that:\n\n#### 1. About 36% of 5 star reviewers gave reviews while 79% of 1 star reviewers gave reviews so a customer was more likely to give reviews when he\/she is displeased.\n\n#### 2. To properly understand the displeased customers, we would build a sentiment analysis model that would classify these sentiments. But before we need to do that, we need to also have a look at the reviews titles.","205a4de4":"# TOPIC MODELLING","d1f1d5b3":"#### In the next cell, we would transform the reviews data by removing stopwords, using regular expressions module to accept only letters, tokenize those words and then make all the words lower case for consistency.","975da723":"#### This would be a supervised learning case so we would have to create a new column representing sentiment score (1 or 0). 1 is for positive words and 0, for negative words. We would exclude the 3-point review score as that represents neutral and include 1 and 2 review scores as negative words and 4 and 5 review scores for positive words.","8b627ee1":"### Plotting the review scores before and after removing NaN values "}}