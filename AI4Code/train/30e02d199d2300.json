{"cell_type":{"5e4701b0":"code","f32501a0":"code","f31063a7":"code","6f96cb30":"code","0a212058":"code","19714bda":"code","bfbfdf1f":"code","300597ca":"code","61e72cca":"code","bcf6b77d":"code","73bd0bbb":"code","a4617f58":"code","1e0545d3":"code","765a78a6":"code","03ad77d0":"code","da92abe5":"code","ccd6bd81":"code","e377e4b8":"code","7dd5e979":"code","b7ec1010":"code","7797beaa":"code","3984c710":"code","04491cb7":"code","db13d4fd":"code","8a525f01":"code","698b0fd7":"code","ae28f88b":"code","1655819b":"code","b6cf8d6f":"code","bddbaa89":"code","09e090e9":"code","24b281d6":"code","eec3d247":"code","7cfa25b3":"code","5286bae5":"code","1f0aa73a":"code","d28b9416":"code","5a856cb4":"code","a11bf7a5":"code","d257799a":"code","e367ef90":"code","bc00173a":"code","4d68f577":"code","ff19d7d4":"code","04ba4053":"code","d15740f4":"code","eac28254":"code","5957510f":"code","63d50ae9":"code","55caea42":"code","3978ece5":"code","4ce9fafc":"code","933d4217":"code","1541bd84":"code","67b56f24":"code","042d1a71":"code","55026863":"code","cb42fbf7":"code","d63284ef":"code","27068855":"code","f8f23da2":"code","7bc82dfc":"code","760cf089":"code","f0c63363":"code","05abf128":"code","771f856c":"code","376381b6":"code","6ae65153":"code","3fad42b7":"code","a6c43bc3":"code","4604f4d5":"code","41731019":"code","90be6b9a":"code","67b5df66":"code","409c8145":"code","08c25aa3":"code","61fc3257":"code","41c43283":"code","d04d12fb":"code","ce1d3f2d":"code","e9b8c140":"code","f88f9eb7":"code","0e9cd019":"code","30f5539f":"code","122a5567":"code","2b5fcf34":"code","9469d1a2":"code","abcf55d6":"code","2c56a81d":"code","b424d4aa":"code","5a9eadf2":"code","00a536f6":"code","b8d0474b":"code","a2be706b":"code","58b5b2c1":"code","a2ff30ed":"code","df35d101":"code","a7c33409":"code","4a71947c":"code","b88d56b6":"code","8eb6fbc2":"code","41be1fda":"code","14d335ec":"code","676da248":"markdown","82cb9a85":"markdown","1332d7e1":"markdown","b71261e0":"markdown","917dfbe7":"markdown","99056e1e":"markdown","fc4874b8":"markdown","5536a961":"markdown","0bc05f16":"markdown","247e2eaf":"markdown","73997df0":"markdown","068eb07e":"markdown","ee0f2939":"markdown","98b23b09":"markdown","737793e3":"markdown"},"source":{"5e4701b0":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'\n\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","f32501a0":"pip install tweepy","f31063a7":"import tweepy,codecs\n\nconsumer_key = 'kUc1RbgjP2EJMo6wHhG7uNM0N'\nconsumer_secret = 'V1qxzAqiyAQCz7DHMj4Zn6ff4DWqFxSxV31DDwC43ZaHlqDQ3Z'\naccess_token = '802519976909357056-Kjr2DP0vDIzncl4dYZfAZOn3QBW3vRy'\naccess_token_secret = '8ghm5GSwZM1gVDSCSfydOr2dtg4jofGxt20ghzNsUbzX3'\n\n\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\napi = tweepy.API(auth)","6f96cb30":"user = api.get_user(\"DataScienceCtrl\")","0a212058":"dir(user) #accessible features","19714bda":"user.name","bfbfdf1f":"user.profile_image_url\n","300597ca":"user.screen_name\n","61e72cca":"user.verified","bcf6b77d":"user.location","73bd0bbb":"user.statuses_count","a4617f58":"user.friends_count","1e0545d3":"user.followers_count","765a78a6":"user.favourites_count","03ad77d0":"# Scraping tweets from the timeline\ntweets = api.user_timeline(id = 'DataScienceCtrl') \nfor i in tweets:\n    print(i.text)","da92abe5":"# Creating a DataFrame.\n\ndef  timeline_df(tweets):\n    \n    import pandas as pd\n    df = pd.DataFrame()\n    df['id'] = list(map(lambda tweet: tweet.id, tweets))\n    df['created_at'] = list(map(lambda tweet: tweet.created_at, tweets))\n    df['text'] = list(map(lambda tweet: tweet.text, tweets))\n    df['favorite_count'] = list(map(lambda tweet: tweet.favorite_count, tweets))\n    df['retweet_count'] = list(map(lambda tweet: tweet.retweet_count, tweets))\n    df['source'] = list(map(lambda tweet: tweet.source, tweets))\n    \n    return df\n","ccd6bd81":"df= timeline_df(tweets)\ndf.head()","e377e4b8":"# top 5 favorite tweets\ndf.sort_values(\"favorite_count\", ascending = False)[[\"text\",\"favorite_count\"]].iloc[0:5] ","7dd5e979":"# top 5 retweet\ndf.sort_values(\"retweet_count\", ascending = False)[[\"text\",\"retweet_count\"]].iloc[0:5] ","b7ec1010":"sns.distplot(df.favorite_count, color = \"blue\"); ","7797beaa":"sns.distplot(df.retweet_count, color = \"red\"); ","3984c710":"df[\"tweet_time\"] = df[\"created_at\"].apply(lambda x: x.strftime(\"%H\")) \ndf.head()","04491cb7":"df[\"tweet_time\"] = pd.to_numeric(df[\"tweet_time\"])\ndf.info()","db13d4fd":"sns.distplot(df.tweet_time, kde=False, color=\"navy\") ","8a525f01":"df[\"days\"] = df[\"created_at\"].dt.day_name()\ndf.head()","698b0fd7":"gun_freq = df.groupby(\"days\").count()[\"id\"]\ngun_freq","ae28f88b":"new_data= df.days\ng = sns.countplot(new_data, palette=\"icefire\")\nplt.title(\"Days Vs. Tweets\")\nplt.show()","1655819b":"labels = df.days.value_counts().index\ncolors = ['darkviolet','violet'] \nexplode = [0,0]\nsizes =df.days.value_counts().values\nplt.figure(figsize = (5,5))\nplt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%') \nplt.title('Days Vs. Tweets',color = 'indigo',fontsize = 15) \nplt.show()","b6cf8d6f":"source_freq = df.groupby(\"source\").count()[\"id\"] ","bddbaa89":"source_freq = df.groupby(\"source\").count()[\"id\"] ","09e090e9":"# Source & Time & Day\ndf.groupby([\"source\",\"tweet_time\",\"days\"])[[\"tweet_time\"]].count()","24b281d6":"user = api.get_user(id = \"DataScienceCtrl\")","eec3d247":"for friend in user.friends():\n    print(friend.screen_name)","7cfa25b3":"friends= user.friends()\nfollowers= user.followers()","5286bae5":"def followers_df(follower):\n    import pandas as pd\n    idler = [user.id for user  in follower]\n    df = pd.DataFrame(idler, columns = [\"id\"])\n    \n    df[\"created_at\"] = [user.created_at for user in follower]\n    df[\"screen_name\"] = [user.screen_name for user in follower]\n    df[\"location\"] = [user.location for user in follower]\n    df[\"followers_count\"] = [user.followers_count for user in follower]\n    df[\"statuses_count\"] = [user.statuses_count for user in follower]\n    df[\"friends_count\"] = [user.friends_count for user in follower]\n    df[\"favourites_count\"] = [user.favourites_count for user in follower]\n    \n    return df","1f0aa73a":"df = followers_df(followers)\ndf.head()","d28b9416":"df.index= df[\"screen_name\"]\ndf.head()","5a856cb4":"s_data = df[[\"followers_count\", \"statuses_count\"]]\ns_data.head()","a11bf7a5":"s_data.info()","d257799a":"s_data[\"followers_count\"] = s_data[\"followers_count\"] + 0.01\ns_data[\"statuses_count\"] = s_data[\"statuses_count\"] + 0.01\ns_data = s_data.apply(lambda x: (x-min(x))\/(max(x)-min(x))) #normalization\ns_data.head()","e367ef90":"score = s_data[\"followers_count\"] * s_data[\"statuses_count\"]  \nscore\n","bc00173a":"s_data[\"score\"] = score\ns_data[\"segment\"] = np.where(s_data[\"score\"] >= score.median() + score.std()\/len(score), \"A\",\"B\")\ns_data.head(10)","4d68f577":"from plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\ndf5 = s_data.followers_count[s_data['segment']=='A']\ndf6= s_data.followers_count[s_data['segment']=='B']\n\ntrace1 = go.Histogram(\n    x=df5,\n    opacity=0.75,\n    name = \"A\",\n    marker=dict(color='rgba(171, 50, 96, 0.6)'))\n\ntrace2 = go.Histogram(\n    x=df6,\n    opacity=0.75,\n    name = \"B\",\n    marker=dict(color='rgba(12, 50, 196, 0.6)'))\n\ndata = [trace1, trace2]\nlayout = go.Layout(barmode='overlay',\n                   title='',\n                   xaxis=dict(title='followers_count'),\n                   yaxis=dict( title=''),\n)\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","ff19d7d4":" api.trends_available(); ","04ba4053":"def country_codes():\n    places = api.trends_available()\n    all_woeids = {place['name'].lower(): place['woeid'] for place in places}\n    return all_woeids","d15740f4":"country_codes()","eac28254":"def country_woeid(country_name):\n    country_name = country_name.lower()\n    trends = api.trends_available() \n    all_woeids =country_codes()\n    return all_woeids[country_name]","5957510f":"country_woeid(\"turkey\")\n","63d50ae9":"trends = api.trends_place(id = 23424969)","55caea42":"import json\nprint(json.dumps(trends, indent = 3))","3978ece5":"turkey = api.trends_place(id = 23424969)\ntrends= turkey[0][\"trends\"]\n\nfor i in trends:\n    print(i[\"name\"])","4ce9fafc":"tweets = api.search(q = \"#foxhaber\", \n                      lang = \"tr\", \n                      result_type = \"recent\", \n                     count = 1000)","933d4217":"def hashtag_df(tweetler):\n  \n    id_list = [tweet.id for tweet  in tweetler]\n    df = pd.DataFrame(id_list, columns = [\"id\"])\n    \n    df[\"text\"] = [tweet.text for tweet in tweetler]\n    df[\"created_at\"] = [tweet.created_at for tweet in tweetler]\n    df[\"retweeted\"] = [tweet.retweeted for tweet in tweetler]\n    df[\"retweet_count\"] = [tweet.retweet_count for tweet in tweetler]\n    df[\"user_screen_name\"] = [tweet.author.screen_name for tweet in tweetler]\n    df[\"user_followers_count\"] = [tweet.author.followers_count for tweet in tweetler]\n    df[\"user_location\"] = [tweet.author.location for tweet in tweetler]\n    df[\"Hashtags\"] = [tweet.entities.get('hashtags') for tweet in tweetler]\n    \n    return df","1541bd84":"df = hashtag_df(tweets)\ndf.head()","67b56f24":"df.user_screen_name.unique().size","042d1a71":"df.groupby(\"user_screen_name\")[\"id\"].count().sum()","55026863":"df.groupby(\"user_screen_name\").count()[\"id\"].sort_values(ascending = False)[0:5]","cb42fbf7":"# We can remove retweets for the original contributors.\ndf[~df[\"text\"].str.startswith(\"RT\")].count()[\"id\"]","d63284ef":"df=df.head(3) \nuser_list= list(df['user_screen_name'].unique())\n\nuser_followers_count= []\nretweet_count= []\n\nfor i in user_list:    \n    x = df[df['user_screen_name']==i]\n    user_followers_count.append(sum(x.user_followers_count)\/len(x))\n    retweet_count.append(sum(x.retweet_count)\/len(x))\n    \nf,ax = plt.subplots(figsize = (5,8))\nsns.barplot(x=user_followers_count,y=user_list,color='blue',alpha = 0.5,label='user_followers_count' )\nsns.barplot(x=retweet_count,y=user_list,color='red',alpha = 0.5,label='retweet_count' )\nax.legend(loc='lower right',frameon = True)   \nax.set(xlabel=' Follower and Retweet Rate', ylabel='Users')\nplt.show()","27068855":"df[\"text\"].str.startswith(\"RT\")","f8f23da2":"# The original contribution rate can be found by dividing the number of original tweets by the number of tweets.\n\ndf[~df[\"text\"].str.startswith(\"RT\")].count()[\"id\"] \/ len(df) ","7bc82dfc":"#ratio of non-original contribution to original contribution\n\ndf[~df[\"text\"].str.startswith(\"RT\")].count()[\"id\"] \/ df[df[\"text\"].str.startswith(\"RT\")].count()[\"id\"]  ","760cf089":"# sorting data by the retweet_count\ndf.sort_values(\"retweet_count\", ascending = False).head(3) ","f0c63363":"?api.search","05abf128":"tweets = api.search(q = \"#datascience\", lang = \"en\", count = 5000) # result_type=\"popular\"","771f856c":"def hashtag_df(tweets):\n    import pandas as pd\n    id_list = [tweet.id for tweet  in tweets]\n    df = pd.DataFrame(id_list, columns = [\"id\"])\n    \n    df[\"text\"] = [tweet.text for tweet in tweets]\n    df[\"created_at\"] = [tweet.created_at for tweet in tweets]\n    df[\"retweeted\"] = [tweet.retweeted for tweet in tweets]\n    df[\"retweet_count\"] = [tweet.retweet_count for tweet in tweets]\n    df[\"source\"] = [tweet.source for tweet in tweets]\n    df[\"user_screen_name\"] = [tweet.author.screen_name for tweet in tweets]\n    df[\"user_followers_count\"] = [tweet.author.followers_count for tweet in tweets]\n    df[\"user_location\"] = [tweet.author.location for tweet in tweets]\n    df[\"Hashtags\"] = [tweet.entities.get('hashtags') for tweet in tweets]\n    \n    return df","376381b6":"df = hashtag_df(tweets)\ndf.shape","6ae65153":"df[\"tweet_time\"] = df[\"created_at\"].apply(lambda x: x.strftime(\"%H\"))\ndf.head()","3fad42b7":"df[\"tweet_time\"] = pd.to_numeric(df[\"tweet_time\"])\ndf.info()","a6c43bc3":"sns.distplot(df[\"tweet_time\"], kde = False, color =\"blue\");","4604f4d5":"df[\"days\"] = df[\"created_at\"].dt.day_name()\ndays_freq = df.groupby(\"days\").count()[\"id\"]\ndays_freq.plot.bar(x = \"days\", y = \"id\", color=\"purple\")","41731019":"## Tweet sources\nplt.figure(figsize=(12,5))\nplt.title(\"Tweet Sources\")\nkaynak_freq = df.groupby(\"source\").count()[\"id\"]\nkaynak_freq.plot.bar(x = \"source\", y = \"id\")","90be6b9a":"df.groupby(\"source\").count()[\"id\"].sort_values(ascending=False)","67b5df66":"df = hashtag_df(tweets)\n#upper-lower case\ndf['text'] = df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ndf['text'] = df['text'].str.replace('[^\\w\\s]','')\n#numbers\ndf['text'] = df['text'].str.replace('\\d','')\n#stopwords\nimport nltk\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nsw = stopwords.words('english')\ndf['text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n#lemmatization\nfrom textblob import Word\n#nltk.download('wordnet')\ndf['text'] = df['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])) \ndf['text'] = df['text'].str.replace('rt','')","409c8145":"df[\"text\"]","08c25aa3":"freq_df = df[\"text\"].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis =0).reset_index()\nfreq_df.head()","61fc3257":"freq_df.columns=[\"word\",\"frequency\"]\nfreq_df.head()","41c43283":"freq_df.shape","d04d12fb":"#datasicence word frequencies","ce1d3f2d":"a= freq_df[freq_df.frequency>freq_df.frequency.mean()+freq_df.frequency.std()]","e9b8c140":"a.plot.bar(x=\"word\",y=\"frequency\", color=\"red\")","f88f9eb7":"text = \" \".join(i for i in df.text)","0e9cd019":"wordcloud = WordCloud(background_color = \"white\").generate(text)\nplt.figure(figsize=(12,5))\nplt.imshow(wordcloud, interpolation = \"bilinear\")\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\nplt.show()","30f5539f":"from textblob import TextBlob","122a5567":"def sentiment_score(df):\n\n    text = df[\"text\"]\n\n    for i in range(0,len(text)):\n        textB = TextBlob(text[i])\n        sentiment_score = textB.sentiment.polarity\n        df.loc[i, 'sentiment_score'] = sentiment_score \n        \n        if sentiment_score <0.00:\n            polarity = 'Negative'\n            df.loc[i, 'polarity'] = polarity\n\n        elif sentiment_score >0.00:\n            polarity = 'Pozitive'\n            df.loc[i, 'polarity'] = polarity\n\n        else:\n            polarity = 'Notr'\n            df.loc[i, 'polarity'] = polarity\n            \n    return df ","2b5fcf34":"sentiment_score(df)\ndf.head(3)","9469d1a2":"df.groupby(\"polarity\").count()[\"id\"]","abcf55d6":"sentiment_freq = df.groupby(\"polarity\").count()[\"id\"]\nsentiment_freq.plot.bar(x = \"polarity\",y = \"id\", color=\"purple\");","2c56a81d":"tweets = api.search(q = \"#samsung\", lang = \"en\")","b424d4aa":"df = hashtag_df(tweets)","5a9eadf2":"import re\nimport nltk \nimport nltk as nlp\n\nnltk.download(\"stopwords\") \nfrom nltk.corpus import stopwords","00a536f6":"tweets  = [ word for word in df.text if not word in set(stopwords.words(\"english\"))]\nlemma = nlp.WordNetLemmatizer()\ntweets = [ lemma.lemmatize(word) for word in tweets ] ","b8d0474b":"tweet_list = []\nfor tweets in df.text:\n    tweets  = re.sub(\"[^a-zA-Z]\",\" \",tweets )\n    tweets  = tweets .lower() \n    tweets  = nltk.word_tokenize(tweets )\n    lemma = nlp.WordNetLemmatizer()\n    tweets  = [ lemma.lemmatize(word) for word in tweets ]\n    tweets  = \" \".join(tweets )\n    tweet_list.append(tweets )","a2be706b":"# %% bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer\nmax_features = 10\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words = \"english\")\nsparce_matrix = count_vectorizer.fit_transform(tweet_list).toarray()  \nprint(\"frequently used {} words: {}\".format(max_features,count_vectorizer.get_feature_names()))","58b5b2c1":"df=pd.DataFrame(tweet_list,columns=['tweets'])","a2ff30ed":"df['sentiment'] = df['tweets'].map(lambda text: TextBlob(text).sentiment.polarity)\ndf.head()","df35d101":"import numpy as np\ncut = pd.cut(\n    df['sentiment'],\n    [-np.inf, -.01, .01, np.inf],\n    labels=['negative', 'neutral', 'positive']\n)\ndf['polarity'] = cut.values\ndf[['polarity','sentiment']].head()","a7c33409":"plt.figure(figsize=(5,5))\ndata= df.polarity\ng = sns.countplot(data, palette=\"Set3\")\nplt.title(\" #Samsung Polarity\")\nplt.show()","4a71947c":"tweets = api.search(q = \"#apple\", lang = \"en\")\ndf = hashtag_df(tweets)","b88d56b6":"tweets = [ word for word in df.text if not word in set(stopwords.words(\"english\"))]\nlemma = nlp.WordNetLemmatizer()\ntweets = [ lemma.lemmatize(word) for word in tweets] \n\ntweet_list = []\nfor tweets in df.text:\n    tweets = re.sub(\"[^a-zA-Z]\",\" \",tweets)\n    tweets = tweets.lower() \n    tweets = nltk.word_tokenize(tweets)\n    lemma = nlp.WordNetLemmatizer()\n    tweets = [ lemma.lemmatize(word) for word in tweets]\n    tweets = \" \".join(tweets)\n    tweet_list.append(tweets)","8eb6fbc2":"# %% bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer\nmax_features = 10\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words = \"english\")\nsparce_matrix = count_vectorizer.fit_transform(tweet_list).toarray()  \nprint(\"frequently used {} words: {}\".format(max_features,count_vectorizer.get_feature_names()))","41be1fda":"df=pd.DataFrame(tweet_list,columns=['tweets'])\ndf['sentiment'] = df['tweets'].map(lambda text: TextBlob(text).sentiment.polarity)\n\ncut = pd.cut(\n    df['sentiment'],\n    [-np.inf, -.01, .01, np.inf],\n    labels=['negative', 'neutral', 'positive']\n)\ndf['polarity'] = cut.values\ndf[['polarity','sentiment']].head()","14d335ec":"plt.figure(figsize=(5,5))\ndata= df.polarity\ng = sns.countplot(data, palette=\"Set3\")\nplt.title(\"#Apple Polarity\")\nplt.show()","676da248":"<a id=\"7\"><\/a> <br>\n\n### Follower Segmentation","82cb9a85":"<a id=\"5\"><\/a> <br>\n### Tweet Sources","1332d7e1":"<a id=\"13\"><\/a> <br>\n### Apple Vs. Samsung","b71261e0":"<a id=\"11\"><\/a> <br>\n\n### Twitter Text Mining","917dfbe7":"<a id=\"9\"><\/a> <br>\n### Analyzing Hashtags","99056e1e":"## Twitter Analytics\n\n![1.png](attachment:1.png)\n- [Import Libraries](#0)\n- [API Connection](#1)\n- [User Analysis](#2)\n- [Distribution of Retweets & Favorites](#3)\n- [Day & Time Distribution](#4)\n- [Tweet Sources](#5)\n- [Followers and Friends Analysis](#6)\n- [Follower Segmentation](#7)\n- [Scraping Data From Hashtags](#8)\n- [Top contributors to the hashtags](#9)\n- [Analyzing Hashtags](#10)\n- [Twitter Text Mining](#11)\n- [Twitter Sentimental Analysis](#12)\n","fc4874b8":"<a id=\"3\"><\/a> <br>\n### Distribution of Retweets & Favorites\n","5536a961":"<a id=\"10\"><\/a> <br>\n### Top contributors to the hashtags","0bc05f16":"<a id=\"1\"><\/a> <br>\n### API Connection","247e2eaf":"<a id=\"8\"><\/a> <br>\n### Scraping Data From Hashtags","73997df0":"<a id=\"6\"><\/a> <br>\n###  Followers and Friends Analysis","068eb07e":"<a id=\"12\"><\/a> <br>\n## Twitter Sentimental Analysis","ee0f2939":"<a id=\"0\"><\/a> <br>\n### Import Libraries","98b23b09":"<a id=\"4\"><\/a> <br>\n### Day & Time Distribution","737793e3":"<a id=\"2\"><\/a> <br>\n### User Analysis"}}