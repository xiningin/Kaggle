{"cell_type":{"8671cc7b":"code","f790eb40":"code","e5edd8ac":"code","ad4cd99c":"code","672da3d1":"code","24000aa1":"code","30294719":"code","7625e996":"code","a9da6b28":"code","63bb272d":"code","c7f1c4f8":"code","5a73083d":"code","0e37b233":"code","e7a59e3f":"code","aa51021b":"code","a1597ad0":"code","f01926c7":"code","182f6079":"code","3cb9d450":"code","45275b64":"code","66704f60":"code","5532301d":"code","d22d9f6a":"code","7581518b":"code","98e91053":"code","5e333b2a":"code","80ac0035":"code","9783ad76":"code","72c6cbe3":"code","ee026a25":"code","eb4872d3":"code","0bbdc0fb":"code","070ef327":"code","bb51d3c9":"code","738a599e":"code","0aff085c":"markdown","9a693ec5":"markdown","5729d2ab":"markdown","50f92125":"markdown","bd50e6c0":"markdown","4425ff00":"markdown","c5763c95":"markdown","87a8fae2":"markdown","187c7034":"markdown","595e3400":"markdown","2c790b86":"markdown","7cd8b264":"markdown","3995bcf8":"markdown","1f697871":"markdown","bc37409c":"markdown","2076bed0":"markdown","936fc3ce":"markdown","34590865":"markdown","40ebe05a":"markdown","573c626e":"markdown","906d5a83":"markdown","9eaf7140":"markdown","ea4c4d2e":"markdown","5d5d0180":"markdown","6698636b":"markdown","b555dbb4":"markdown","307ae508":"markdown","631953c0":"markdown"},"source":{"8671cc7b":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import RandomizedSearchCV\nimport xgboost\n\npd.set_option('display.max_column', 120)\npd.set_option('display.max_row', 30)","f790eb40":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","e5edd8ac":"train.head()","ad4cd99c":"train.shape","672da3d1":"train.info()","24000aa1":"train.dtypes.value_counts()","30294719":"train.describe()","7625e996":"train.isna().sum() \/ train.shape[0] *100","a9da6b28":"test.isna().sum() \/ test.shape[0] *100","63bb272d":"train.nunique()[train.nunique()<1000] ","c7f1c4f8":"train.f97.value_counts() ","5a73083d":"print('minimal value in f97 column is : ', train.f97.min())\nprint('maximal value in f97 column is : ', train.f97.max())","0e37b233":"dup_rows = train[train.duplicated()]\nprint('Number of duplicated rows is : ', dup_rows.shape)","e7a59e3f":"train.claim.value_counts() \/ train.shape[0] *100 ","aa51021b":"corr = train.corr()\nhighest_corr = corr.index[abs(corr[\"claim\"])>0.01]","a1597ad0":"highest_corr = train[highest_corr].corr()","f01926c7":"fig, ax = plt.subplots(1, 1, figsize=(14 , 14))\n\n#mask = np.zeros_like(highest_corr, dtype=np.bool)\n#mask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(highest_corr, ax=ax,\n            \n        square=True, center=0, linewidth=1,\n        cmap=sns.diverging_palette(240, 10, as_cmap=True),\n        #cmap = 'Greens',\n        annot = True,\n        fmt = '.3f',\n        cbar_kws={\"shrink\": .6},    \n        mask=np.triu(highest_corr)\n       ) \n\nax.set_title(f'Correlation', loc='left', fontweight='bold')     \n\nplt.show()","182f6079":"highest_corr.claim.to_frame().T.sort_values(by = 'claim', ascending = True)","3cb9d450":"def check_outliers(col) :\n    outliers = []\n    Q1 = col.quantile(.25)\n    Q3 = col.quantile(.75)\n    IQR = Q3 - Q1\n    lowerLimit = Q1 - 1.5*IQR\n    higherLimit = Q3 + 1.5*IQR\n    \n    for elt in col :\n        if elt < lowerLimit or elt > higherLimit :\n            outliers.append(elt)\n            \n    return np.array(outliers), lowerLimit, higherLimit\n\n    ","45275b64":"def change_outliers(data) :\n    for col in data.columns :\n        arr,lowerLimit,higherLimit = check_outliers(data[col])\n        #print(col, len(arr))\n\n        data[col] = np.where(data[col]>higherLimit,higherLimit,data[col])\n        data[col] = np.where(data[col] <lowerLimit,lowerLimit,data[col])\n        \nchange_outliers(train)\nchange_outliers(test)","66704f60":"train.describe().T[['min', 'max']].sort_values(by='max')","5532301d":"# let's first set the target \ny  = train.claim","d22d9f6a":"for col in train.columns :\n    train[col] = train[col].fillna(train[col].median())\n    if col != 'claim' :\n        test[col] = test[col].fillna(train[col].median())","7581518b":"scaler = StandardScaler()\ntrain = scaler.fit_transform(train.drop(['id', 'claim'], axis = 1))\n\ntest = scaler.transform(test.drop('id', axis = 1))","98e91053":"# let's try first to determine the appropriate number of components\nplt.figure(figsize=(14,7))\npca = PCA().fit(train)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')","5e333b2a":"pca = PCA(n_components=110)\npca.fit(train)\nX_train_pca = pca.transform(train)\nX_test_pca = pca.transform(test)\nprincipalDf = pd.DataFrame(data = X_train_pca)\nprincipalDf.head(10)","80ac0035":"model=xgboost.XGBClassifier( tree_method=\"gpu_hist\",\n        gpu_id=1,\n        predictor=\"gpu_predictor\")","9783ad76":"## Hyper Parameter Optimization\n\nparams={\n \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n    \n}","72c6cbe3":"random_search=RandomizedSearchCV(model,param_distributions=params,n_iter=5,scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)","ee026a25":"random_search.fit(X_train_pca,y)","eb4872d3":"random_search.best_params_","0bbdc0fb":"random_search.best_estimator_","070ef327":"model=xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.7, gamma=0.1, gpu_id=1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.25, max_delta_step=0, max_depth=4,\n              min_child_weight=5, missing=np.nan, monotone_constraints='()',\n              n_estimators=100, n_jobs=2, num_parallel_tree=1,\n              predictor='gpu_predictor', random_state=0, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='gpu_hist', validate_parameters=1, verbosity=None)","bb51d3c9":"model.fit(X_train_pca,y)\ntest_preds = model.predict(X_test_pca)\nsub.claim = test_preds\nsub","738a599e":"sub.to_csv('submission.csv', index=False)","0aff085c":"  The plot explained variance ratio and choose a number of components that \"capture\" at least 95% of the variance, thus 110 is a good choice in our case.","9a693ec5":"<p style=\"background-color:pink; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><b>Importing Libraries & Data<\/b><\/p> ","5729d2ab":"Here, we notice that features have different ranges, so later we should do some feature scaling wich refers to the methods used to normalize the range of values of independent variables.","50f92125":"    The importance of dimensionality reduction is that we can compress the dataset by removing redundancy and retaining only useful information. Too many input variables can lead to the curse of dimensionality and then the model will not be able to perform well because the model will also learn from noise in the training dataset and be overfitted.\n\n    Principal Component Analysis is a powerful technique used for dimensionality reduction, increasing interpretability but at the same time minimizing information loss.","bd50e6c0":"<p style=\"background-color:pink; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><b>EDA<\/b><\/p> ","4425ff00":"    Those values may be the result of some human errors or system failures. So we cannot simply accept them and we cannot drop them as well since then we will miss other features data. So we can use IQR.\n     IQR or interquartile range is a measurement of variability based on dividing the dataset into different quantiles.\n\n    We can calculate the lower limit and upper limit using quantiles. Then we replace the values that are less than the lower limit with the lower limit and the values that are greater than the upper limit with the upper limit. This will work with left-skewed or right-skewed data as well.","c5763c95":"<p style=\"background-color:pink; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><b>Modeling and Submission<\/b><\/p> ","87a8fae2":"### Read The Data :","187c7034":"**EDA Conclusions :**\n- Number of rows and columns : (957919, 120)\n- Types of Features : all features are numerical\n- No categorical variables\n- Almost features have 1% to 2% Nan values in both train and test set\n- Features are in different ranges --> Need some Feature Scaling\n- No duplicated rows\n- Target variable is balanced\n- Week correlations between features\n- There are many outliers in our data, so we change their values using the IQR","595e3400":"### Modeling :","2c790b86":"We can thought that f97 is a categorical column, but let's check first ;)","7cd8b264":"### PCA :","3995bcf8":"## **Objective :** predict whether a customer made a claim upon an insurance policy.\n\n\n<img src=\"https:\/\/www.claimsmadesimple.org\/wp-content\/uploads\/2020\/01\/claims.jpg\" width=\"500\"><\/img>","1f697871":"Even with changing outliers, we always have different ranges of values, so we always need to do some feature scaling.","bc37409c":"### Variables Correlations :","2076bed0":"<p style=\"background-color:pink; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><b>Pre-Processing<\/b><\/p> ","936fc3ce":"## Please If you find this notebook usefull don't forget to Upvote it!","34590865":"### Import Libraries :","40ebe05a":"Almost features have 1% to 2% Nan values","573c626e":"## **Content :**\n\n- Import Libraries\n- Load Data\n- EDA :\n     - Dealing With Missing Values\n     - Outliers\n- Pre-Processing\n     - Feature Scaling\n     - PCA\n- Modeling\n- Submission","906d5a83":"### Submission :","9eaf7140":"The two values in target variable are balanced ","ea4c4d2e":"Let's check for outliers :\nlet 's train model without changing outliers then with changing it","5d5d0180":"### Feature Scaling :","6698636b":"### Outliers :","b555dbb4":"### Data Imputation :","307ae508":"It appears that it is not a categorical columnl; it's just that all values varies in a small range.\n\nSo yeah, it is not a categorical column","631953c0":"There are week correlations between independent and dependent variable."}}