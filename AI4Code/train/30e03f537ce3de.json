{"cell_type":{"39154ae8":"code","c3a681ca":"code","d3395de2":"code","4748280f":"code","d4ce09cb":"code","ebc99719":"code","672b7e36":"code","43fcc697":"code","331773c7":"code","95a06fbd":"code","74bbd336":"code","91e53258":"code","4348358a":"code","07b214a8":"code","37234afa":"code","bef674b1":"code","0317d4e3":"code","2df0dfc6":"code","2970b496":"code","c16d5621":"code","ac321af1":"code","ea643dfd":"markdown","6fcbd8a2":"markdown","5f15c1df":"markdown","43b3313e":"markdown","9ce2d6cd":"markdown","f6d38fab":"markdown","d75ad963":"markdown","40bd10db":"markdown","f0d24242":"markdown","deaee099":"markdown"},"source":{"39154ae8":"import os, sys, time\nimport cv2\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","c3a681ca":"frames_per_vid = [17, 25, 30, 32, 35, 36, 40]\npublic_LB = [0.46788, 0.46776, 0.46611, 0.46542, 0.46643, 0.46484, 0.46635]\ndf_viz = pd.DataFrame({'frames_per_vid': frames_per_vid, 'public_LB':public_LB})","d3395de2":"df_plot = df_viz.plot(x='frames_per_vid', y='public_LB')","4748280f":"test_dir = \"\/kaggle\/input\/deepfake-detection-challenge\/test_videos\/\"\n\ntest_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])\nframe_h = 5\nframe_l = 5\nlen(test_videos)","d4ce09cb":"print(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA version:\", torch.version.cuda)\nprint(\"cuDNN version:\", torch.backends.cudnn.version())","ebc99719":"gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ngpu","672b7e36":"import sys\nsys.path.insert(0, \"\/kaggle\/input\/blazeface-pytorch\")\nsys.path.insert(0, \"\/kaggle\/input\/deepfakes-inference-demo\")","43fcc697":"from blazeface import BlazeFace\nfacedet = BlazeFace().to(gpu)\nfacedet.load_weights(\"\/kaggle\/input\/blazeface-pytorch\/blazeface.pth\")\nfacedet.load_anchors(\"\/kaggle\/input\/blazeface-pytorch\/anchors.npy\")\n_ = facedet.train(False)","331773c7":"from helpers.read_video_1 import VideoReader\nfrom helpers.face_extract_1 import FaceExtractor\n\nframes_per_video = 64\nvideo_reader = VideoReader()\nvideo_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\nface_extractor = FaceExtractor(video_read_fn, facedet)","95a06fbd":"input_size = 224","74bbd336":"from torchvision.transforms import Normalize\n\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\nnormalize_transform = Normalize(mean, std)","91e53258":"def isotropically_resize_image(img, size, resample=cv2.INTER_AREA):\n    h, w = img.shape[:2]\n    if w > h:\n        h = h * size \/\/ w\n        w = size\n    else:\n        w = w * size \/\/ h\n        h = size\n\n    resized = cv2.resize(img, (w, h), interpolation=resample)\n    return resized\n\n\ndef make_square_image(img):\n    h, w = img.shape[:2]\n    size = max(h, w)\n    t = 0\n    b = size - h\n    l = 0\n    r = size - w\n    return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)","4348358a":"import torch.nn as nn\nimport torchvision.models as models\n\nclass MyResNeXt(models.resnet.ResNet):\n    def __init__(self, training=True):\n        super(MyResNeXt, self).__init__(block=models.resnet.Bottleneck,\n                                        layers=[3, 4, 6, 3], \n                                        groups=32, \n                                        width_per_group=4)\n        self.fc = nn.Linear(2048, 1)","07b214a8":"checkpoint = torch.load(\"\/kaggle\/input\/deepfakes-inference-demo\/resnext.pth\", map_location=gpu)\n\nmodel = MyResNeXt().to(gpu)\nmodel.load_state_dict(checkpoint)\n_ = model.eval()\n\ndel checkpoint","37234afa":"def predict_on_video(video_path, batch_size):\n    try:\n        # Find the faces for N frames in the video.\n        faces = face_extractor.process_video(video_path)\n\n        # Only look at one face per frame.\n        face_extractor.keep_only_best_face(faces)\n        \n        if len(faces) > 0:\n            # NOTE: When running on the CPU, the batch size must be fixed\n            # or else memory usage will blow up. (Bug in PyTorch?)\n            x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8)\n\n            # If we found any faces, prepare them for the model.\n            n = 0\n            for frame_data in faces:\n                for face in frame_data[\"faces\"]:\n                    # Resize to the model's required input size.\n                    # We keep the aspect ratio intact and add zero\n                    # padding if necessary.                    \n                    resized_face = isotropically_resize_image(face, input_size)\n                    resized_face = make_square_image(resized_face)\n\n                    if n < batch_size:\n                        x[n] = resized_face\n                        n += 1\n                    else:\n                        print(\"WARNING: have %d faces but batch size is %d\" % (n, batch_size))\n                    \n                    # Test time augmentation: horizontal flips.\n                    # TODO: not sure yet if this helps or not\n                    #x[n] = cv2.flip(resized_face, 1)\n                    #n += 1\n\n            if n > 0:\n                x = torch.tensor(x, device=gpu).float()\n\n                # Preprocess the images.\n                x = x.permute((0, 3, 1, 2))\n\n                for i in range(len(x)):\n                    x[i] = normalize_transform(x[i] \/ 255.)\n\n                # Make a prediction, then take the average.\n                with torch.no_grad():\n                    y_pred = model(x)\n                    y_pred = torch.sigmoid(y_pred.squeeze())\n                    return y_pred[:n].mean().item()\n\n    except Exception as e:\n        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n\n    return 0.5","bef674b1":"from concurrent.futures import ThreadPoolExecutor\n\ndef predict_on_video_set(videos, num_workers):\n    def process_file(i):\n        filename = videos[i]\n        y_pred = predict_on_video(os.path.join(test_dir, filename), batch_size=frames_per_video)\n        return y_pred\n\n    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n        predictions = ex.map(process_file, range(len(videos)))\n\n    return list(predictions)","0317d4e3":"speed_test = False  # you have to enable this manually","2df0dfc6":"if speed_test:\n    start_time = time.time()\n    speedtest_videos = test_videos[:5]\n    predictions = predict_on_video_set(speedtest_videos, num_workers=4)\n    elapsed = time.time() - start_time\n    print(\"Elapsed %f sec. Average per video: %f sec.\" % (elapsed, elapsed \/ len(speedtest_videos)))","2970b496":"predictions = predict_on_video_set(test_videos, num_workers=4)","c16d5621":"submission_df = pd.DataFrame({\"filename\": test_videos, \"label\": predictions})\nsubmission_df.to_csv(\"submission.csv\", index=False)","ac321af1":"#submission_df.head()","ea643dfd":"## Prediction loop","6fcbd8a2":"## Import Required Libraries","5f15c1df":"## Now, Inference Demo","43b3313e":"## The Viz","9ce2d6cd":"## Make the submission","f6d38fab":"# Inference Kernel Demo - Analyzing Frames Per Video\n\nCredit to Wei Hao Khoong (Please view his kernel).\n\nI've noticed some people pointing out that varying `frames_per_video` does help in improving the public LB score. Indeed it does, so I'll be continuously updating this kernel's viz as I make submissions with different `frames_per_video` each day, while working on my main kernel in the cloud.","d75ad963":"As I experiment with different frames_per_video, I'll continue to update this kernel with the results.","40bd10db":"\n\n## The rest of this notebook is an update of these work: [work_1](https:\/\/www.kaggle.com\/humananalog\/inference-demo) and [work_2](https:\/\/www.kaggle.com\/mmmarchetti\/inference-demo-ii). \n## If it is useful for you, please consider your upvote.\n\nThis is the kernel I\u2019ve used for my recent submissions. It takes about 5-6 hours on the test set, using only CPU. \n\nI\u2019ve provided this kernel because a lot of people have problems making submissions. This method works and has never errored out for me. (Although I haven't tried making a submission using the GPU yet -- so no guarantees there.)\n\nIt uses BlazeFace for face extraction (see also [my BlazeFace kernel](https:\/\/www.kaggle.com\/humananalog\/starter-blazeface-pytorch)) and ResNeXt50 as the classifier model.\n\nWe take the average prediction over 17 frames from each video. (Why 17? Using more frames makes the kernel slower, but doesn't appear to improve the score much. I used an odd number so we don't always land on even frames.)\n\n**Please use this kernel only to learn from...** Included is the checkpoint for a ResNeXt50 model that hasn't really been trained very well yet. I'm sure you can improve on it by training your own model!\n\nYou could use the included trained weights to get yourself an easy top-50 score on the leaderboard (as of 24 Jan 2020) but it\u2019s nicer to use it as a starting point for your own work. :-)","f0d24242":"## Get the test videos","deaee099":"## Create helpers"}}