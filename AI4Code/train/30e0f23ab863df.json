{"cell_type":{"8d728011":"code","01059a84":"code","142ec9e7":"code","64aea615":"code","1f5755dc":"code","7745b19a":"code","9f3e5b7a":"code","48f18681":"code","ad0fb073":"code","8210567a":"code","8a11fd6a":"markdown","65a8cde4":"markdown","52407a7c":"markdown","4d79f2bd":"markdown"},"source":{"8d728011":"! pip install git+https:\/\/github.com\/openai\/CLIP.git","01059a84":"import torch\nimport torchvision\nimport clip\nfrom PIL import Image\nimport os\nimport skimage\nimport IPython.display\nimport matplotlib.pyplot as plt\nimport numpy as np","142ec9e7":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B\/32\", device=device)","64aea615":"tokens = clip.tokenize('Photo of many people near the beach. They are playing with beach ball')","1f5755dc":"food_images = []\ntravel_images = []\nfood_img_root = '..\/input\/image-classification\/images\/images\/food and d rinks'\ntravel_img_root = '..\/input\/image-classification\/images\/images\/travel and  adventure'\n\nfor filename in os.listdir(food_img_root):\n    food_images.append(os.path.join(food_img_root, filename))\nfor filename in os.listdir(travel_img_root):\n    travel_images.append(os.path.join(travel_img_root, filename))","7745b19a":"original_images = []\nimages = []\nfor i, filename in enumerate(food_images[:1000]+travel_images[:1000]):\n    image = Image.open(filename).convert(\"RGB\")\n    original_images.append(image)\n    images.append(preprocess(image))\n    if i % 200 == 0:\n        print('{}th images complete'.format(i))","9f3e5b7a":"fig = plt.figure(figsize = (20,20))\nfor i in range(25):\n    idx = np.random.randint(0,1999)\n    plt.subplot(5,5,i+1)\n    plt.imshow(original_images[idx])","48f18681":"class ImgRetrieval:\n    def __init__(self, image_data, original_images, model):\n        self.original_images = original_images\n        self.model = model\n        self.img_vecs = []\n        for i in range(0,2000):\n            with torch.no_grad():\n                temp = image_data[i].reshape(1,3,224,224).cuda()\n                img_vecs = model.encode_image(temp).float()\n                temp = temp.to('cpu')\n                self.img_vecs.append(img_vecs)\n                torch.cuda.empty_cache()\n            if i % 200 == 0:\n                print('{}th encoding complete'.format(i))\n        self.img_vecs = torch.vstack(self.img_vecs)\n        self.img_vecs = self.img_vecs \/ self.img_vecs.norm(dim = -1, keepdim = True)\n    def retrieve(self, text):\n        text_vec = self.model.encode_text(clip.tokenize(text).cuda()).float()\n        text_vec = text_vec \/ text_vec.norm(dim = -1, keepdim = True)\n        pick = torch.argmax(text_vec @ self.img_vecs.T)\n        return self.original_images[pick]","ad0fb073":"# initiate text to image retrieval engine\nret_machine = ImgRetrieval(images, original_images, model)","8210567a":"testing_texts = ['blue cocktail with lemon',\n                 'a photo of great mountain',\n                 'beautiful ocean with emerald color',\n                 'photo of fresh food']\n\nfig = plt.figure(figsize = (10, 20))\nfor i, text in enumerate(testing_texts):\n    p = ret_machine.retrieve(text)\n    plt.subplot(5,1,i+1)\n    plt.imshow(p)\n    plt.title(text)","8a11fd6a":"# 2. Loading Image Data","65a8cde4":"# 3. Building Text to Image Retrieval Engine based on Pre-trained CLIP","52407a7c":"## Let's Check out the result!","4d79f2bd":"# 1. Install Necessary Libraries"}}