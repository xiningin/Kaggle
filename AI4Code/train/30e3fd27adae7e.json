{"cell_type":{"c9e19c2d":"code","8b460790":"code","e34bd7dd":"code","b727b081":"code","ef00f00a":"code","77b26bcd":"code","1b114ae5":"code","1b49949e":"code","d0b93cfe":"code","652a4277":"code","572ecdba":"code","40f0ca2e":"code","48f9e57f":"code","edcde6b8":"code","605f424c":"code","718863b0":"code","88a8825e":"code","86ac5076":"code","02be38dd":"code","de28df10":"code","31f00393":"code","21a73853":"code","801d86d9":"code","66dc1a84":"code","1a762fe7":"code","4d5ddf83":"code","eac33210":"code","3e541ff0":"markdown","fe33e0b3":"markdown","7443632f":"markdown","e0ce0855":"markdown","5d6394eb":"markdown","cb5045aa":"markdown","4755d51d":"markdown","266d7125":"markdown","8f9f9405":"markdown","d799d428":"markdown","edb44715":"markdown","bc568e55":"markdown","bf6e7f60":"markdown","c6f05e25":"markdown","207ff562":"markdown","b9de20b8":"markdown","39f231a8":"markdown","0c917ffe":"markdown","1a343ec7":"markdown","35d1dcf8":"markdown","2e1e0220":"markdown","be9f9576":"markdown","df2bfb5c":"markdown"},"source":{"c9e19c2d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt","8b460790":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e34bd7dd":"data = pd.read_csv('..\/input\/gooogle-stock-price\/Google_Stock_Price_Train.csv',sep=\",\")\ndata.head()","b727b081":"# We assign column \"Open\" to variable \"Data\"\ndata.loc[:,[\"Open\"]].values","ef00f00a":"data = data.loc[:,[\"Open\"]].values\n\ntrain = data[:len(data)-50] \ntest = data[len(train):] # last 50 data will be our test data\n\n# reshape\ntrain=train.reshape(train.shape[0],1)","77b26bcd":"train.shape","1b114ae5":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range= (0,1)) # defining of Scaler\ntrain_scaled = scaler.fit_transform(train) # applying to Scaler to train\n\nplt.plot(train_scaled)\nplt.show()","1b49949e":"# We add first 50 location to \"X_train\" and we 51. location to \"y_train\" .\nX_train = []\ny_train = []\ntimesteps = 50\n\nfor i in range(timesteps, train_scaled.shape[0]):\n    X_train.append(train_scaled[i-timesteps:i,0])\n    y_train.append(train_scaled[i,0])\n\nX_train, y_train = np.array(X_train), np.array(y_train)\n\n\n# Reshaping\nX_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)  # Dimension of array is 3.","d0b93cfe":"# --- RNN ---\n\n# Importing the Keras libraries and packages\n\nfrom keras.models import Sequential  \nfrom keras.layers import Dense \nfrom keras.layers import SimpleRNN\nfrom keras.layers import Dropout # it block to overfitting \n\n# Initialising the RNN\nregressor = Sequential()\n\n# Adding the first RNN layer and some Dropout regularisation\nregressor.add(SimpleRNN(units = 50,activation='tanh', return_sequences = True, input_shape = (X_train.shape[1], 1)))\nregressor.add(Dropout(0.2)) \n\n# Adding a second RNN layer and some Dropout regularisation.\nregressor.add(SimpleRNN(units = 50,activation='tanh', return_sequences = True))\nregressor.add(Dropout(0.2))\n\n# Adding a third RNN layer and some Dropout regularisation. \nregressor.add(SimpleRNN(units = 50,activation='tanh', return_sequences = True))\nregressor.add(Dropout(0.2))\n\n# Adding a fourth RNN layer and some Dropout regularisation.\nregressor.add(SimpleRNN(units = 50))\nregressor.add(Dropout(0.2))\n\n\n# Adding the output layer\nregressor.add(Dense(units = 1))\n\n# Compiling the RNN\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# Fitting the RNN to the Training set\nregressor.fit(X_train, y_train, epochs = 100, batch_size = 32)","652a4277":"inputs = data[len(data) - len(test) - timesteps:]\ninputs = scaler.transform(inputs)  # min max scaler","572ecdba":"X_test = []\nfor i in range(timesteps, inputs.shape[0]):\n    X_test.append(inputs[i-timesteps:i, 0]) # 0 dan 50 ye, 1 den 51 e gibi kaydirarark 50 eleman aliyoruz \nX_test = np.array(X_test)\nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)","40f0ca2e":"predicted_data = regressor.predict(X_test)\npredicted_data = scaler.inverse_transform(predicted_data)","48f9e57f":"plt.figure(figsize=(8,4), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(test,color=\"orange\",label=\"Real value\")\nplt.plot(predicted_data,color=\"c\",label=\"RNN predicted result\")\nplt.legend()\nplt.xlabel(\"Days\")\nplt.ylabel(\"Values\")\nplt.grid(True)\nplt.show()","edcde6b8":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error","605f424c":"model = Sequential()\nmodel.add(LSTM(10, input_shape=(None,1))) # We want to add 10 LSTM block. One layer has 10 LSTM unit (node).\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(X_train, y_train, epochs=50, batch_size=1)","718863b0":"predicted_data2=model.predict(X_test)\npredicted_data2=scaler.inverse_transform(predicted_data2)","88a8825e":"plt.figure(figsize=(8,4), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(test,color=\"LimeGreen\",label=\"Real values\")\nplt.plot(predicted_data2,color=\"Gold\",label=\"Predicted LSTM result\")\nplt.legend()\nplt.xlabel(\"Days\")\nplt.ylabel(\"Values\")\nplt.grid(True)\nplt.show()","86ac5076":"plt.figure(figsize=(8,4), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(test,color=\"green\", linestyle='dashed',label=\"Real values\")\nplt.plot(predicted_data2,color=\"blue\", label=\"LSTM predicted result\")\nplt.plot(predicted_data,color=\"red\",label=\"RNN predicted result\") # ben ekledim\nplt.legend()\nplt.xlabel(\"Days)\")\nplt.ylabel(\"Real values\")\nplt.grid(True)\nplt.show()","02be38dd":"# RNN Modified\n\nfrom keras.models import Sequential  \nfrom keras.layers import Dense \nfrom keras.layers import SimpleRNN\nfrom keras.layers import Dropout ","de28df10":"# Initialising the RNN\nregressor = Sequential()\n\n\nregressor.add(SimpleRNN(units = 100,activation='relu', return_sequences = True, input_shape = (X_train.shape[1], 1)))\nregressor.add(Dropout(0.2))\n\n\nregressor.add(SimpleRNN(units = 50))\nregressor.add(Dropout(0.2))\n\n\n# Adding the output layer\nregressor.add(Dense(units = 1)) \n\n# Compiling the RNN\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# Fitting the RNN to the Training set\nregressor.fit(X_train, y_train, epochs = 500, batch_size = 16)","31f00393":"predicted_data_modified = regressor.predict(X_test)\npredicted_data_modified = scaler.inverse_transform(predicted_data_modified)","21a73853":"plt.figure(figsize=(8,4), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(test,color=\"gray\",label=\"Real values\")\nplt.plot(predicted_data,color=\"cyan\",label=\"RNN result\")\nplt.plot(predicted_data_modified,color=\"blue\",label=\"RNN Modified Result\")\n\nplt.legend()\nplt.xlabel(\"Days\")\nplt.ylabel(\"Values\")\nplt.grid(True)\nplt.show()","801d86d9":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler \n\n\nmodel = Sequential()\nmodel.add(LSTM(10, input_shape=(None,1)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(X_train, y_train, epochs=200, batch_size=4) #degistirdim train leri RNN kilerle","66dc1a84":"predicted_data2_modified=model.predict(X_test)\npredicted_data2_modified=scaler.inverse_transform(predicted_data2_modified)","1a762fe7":"plt.figure(figsize=(8,4), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(test,color=\"DimGray\",label=\"Real values\", linestyle=\"dashed\")\nplt.plot(predicted_data2,color=\"Magenta\",label=\"LSTM predicted\")\nplt.plot(predicted_data2_modified,color=\"c\", label=\"Modified LSTM predicted\")\nplt.legend()\nplt.xlabel(\"Days\")\nplt.ylabel(\"Values\")\nplt.grid(True)\nplt.show()","4d5ddf83":"\nplt.figure(figsize=(16,8), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(test,color=\"DimGray\",label=\"Real value\", linestyle=\"dashed\")\nplt.plot(predicted_data2,color=\"blue\",label=\"LSTM predicted\")\nplt.plot(predicted_data2_modified,color=\"red\", linestyle=\"dashed\", label=\"LSTM Modified predicted\")\nplt.plot(predicted_data,color=\"c\",label=\"RNN predicted\")\nplt.plot(predicted_data_modified,color=\"green\", linestyle=\"dashed\", label=\"RNN modified predicted\")\nplt.legend()\nplt.xlabel(\"Days\")\nplt.ylabel(\"Values\")\nplt.grid(True)\nplt.show()","eac33210":"# Visualization Modified RNN vs Modified LSTM\n\nplt.figure(figsize=(16,8), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(test,color=\"DimGray\", linestyle= \"dashed\", label=\"true result\")\nplt.plot(predicted_data2_modified,color=\"Magenta\",  label=\"LSTM Modified predicted\")\nplt.plot(predicted_data_modified,color=\"c\",  label=\"RNN modified predicted\")\nplt.legend()\nplt.xlabel(\"Days\")\nplt.ylabel(\"Values\")\nplt.grid(True)\nplt.show()","3e541ff0":"# Predicting Values","fe33e0b3":"# Visualisation of LSTM","7443632f":"# Creating Data Frame","e0ce0855":"* It looks Modified RNN more successful than RNN.","5d6394eb":"# Visualisation of RNN vs Modified RNN","cb5045aa":"# Feature Scaling","4755d51d":"# Visualisation of LSTM vs Modified LSTM","266d7125":"# Modified LSTM","8f9f9405":"# Visualization of LSTM vs Modified LSTM vs RNN vs Modified RNN","d799d428":"# Visualisation of RNN","edb44715":"# RNN-Model","bc568e55":"# Visualisation of LSTM & RNN","bf6e7f60":"# Importing LSTM Modules","c6f05e25":"# Importing RNN Libraries","207ff562":"# Importing Data","b9de20b8":"# Importing Modules","39f231a8":"## Conclusion\n* Changing hyperparameter can increase to accuracy.\n* LSTM looks more successful than RNN in predict to values about time.","0c917ffe":"# Content\n* Import Data\n* RNN\n* Visualizing of RNN\n* LSTM\n* Visualizing of LSTM\n* Visualizing of RNN vs LSTM\n* Modified RNN\n* Visualizing of RNN vs Modified RNN\n* Modified LSTM\n* Visualizing of LSTM vs Modified LSTM\n* Visualizing of RNN vs Modified RNN vs LSTM vs Modified LSTM\n* Visualizing of Modified RNN vs Modified LSTM\n\n### Conclusion","1a343ec7":"# Min-Max-Sclar","35d1dcf8":"* Modified LSTM looks more successful than LSTM.","2e1e0220":"# Introduction\n\n* The aim of this study is to compare to accuracies of RNN and LSTM. I compare results of RNN and LSTM to each others and I changed hyperparameters in these methods and I compared them again with their modified versions.\n\n* The main differece of LSTM from RNN is that LSTM can store data longer than RNN.","be9f9576":"# Splitting Data","df2bfb5c":"* LSTM looks greatly successful from the RNN.\n* Now we change hyperparemeter like \"units, number of layers, epochs, batch_size, activation\" in RNN"}}