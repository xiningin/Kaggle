{"cell_type":{"2a12dda4":"code","b4c96ca4":"code","f1af78d8":"code","1599d86c":"code","9193aabd":"code","8758d826":"code","e44e397c":"code","145c163d":"code","a2378cc3":"code","8f09b20a":"code","192bc543":"code","ce7b8b47":"code","643ee249":"code","8813ef03":"code","52830179":"code","44a50c06":"code","c615bd2b":"code","4fbb9a2b":"code","2fe25c41":"code","f0e5ff64":"code","b42a5fbc":"code","44e6927e":"code","f4b420c5":"code","002abbbf":"code","75f9e492":"code","77d207b3":"code","838a6d30":"code","51ebdf1c":"code","3c23a48a":"code","71eb04a7":"code","4fd764f3":"code","bea9e146":"code","864e3285":"code","4b45b253":"code","12e5d2c5":"code","31176b29":"code","08ec35ed":"code","fab4999a":"code","69feaa02":"code","ea4ea818":"code","e3bb7b8e":"code","93fb3591":"code","dc78defa":"code","78300dc6":"code","f7eb9e6d":"code","8e0f76dc":"code","a755dc63":"code","6af6ed3d":"code","a7a13596":"code","b91d11dc":"code","436cf6de":"code","85d2e76e":"code","8ca98b2b":"code","6b72881d":"code","c93bd080":"code","68633949":"code","ee2b4c7f":"code","4dfaa0b1":"code","99371174":"code","0a9b464c":"markdown","88fa63f2":"markdown","15da1924":"markdown","df89f530":"markdown","8276f0e5":"markdown","771dea2e":"markdown","70c99e7b":"markdown","c8a0cc27":"markdown","f46c409c":"markdown","bfd77c35":"markdown","52dffc1e":"markdown","43376fbc":"markdown","b4d08d91":"markdown","270fb9bd":"markdown","0f8b52db":"markdown","bb161a04":"markdown","12b4d891":"markdown","79db1950":"markdown","a5ecc0ea":"markdown","2cd57c67":"markdown","5a3c885e":"markdown","0fdbef73":"markdown","d2499355":"markdown","c9c18ac9":"markdown","eb11a715":"markdown","fc6fb0f9":"markdown","89a43800":"markdown","45a8511a":"markdown","4705f33f":"markdown","bd89299b":"markdown","4abf3399":"markdown","1285198a":"markdown"},"source":{"2a12dda4":"! pip install gradio","b4c96ca4":"# Import libraries\nimport os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nimport timeit\nimport cv2 as cv\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix \n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils.vis_utils import plot_model\nfrom keras import backend as K\nimport kerastuner as kt\nfrom keras.models import load_model\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dense\n\nimport gradio as gr","f1af78d8":"warnings.filterwarnings('ignore')","1599d86c":"device_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","9193aabd":"# Define path to data\nannotations_dir = '..\/input\/stanford-dogs-dataset\/annotations\/Annotation' \nimages_dir = '..\/input\/stanford-dogs-dataset\/images\/Images'","8758d826":"# Count the number of classes (dogs breeds)\nbreed_list = os.listdir(images_dir)\nprint(\"Number of breeds in dataset:\", (len(breed_list)))","e44e397c":"# Count number of pictures for each breed\ndf_breeds = pd.DataFrame(\n    index=[breed.split('-',1)[1]\n           for breed in breed_list],\n    data=[len(os.listdir(images_dir + \"\/\" + name))\n          for name in breed_list],\n    columns=[\"num_pictures\"])\n\n# Plot results\nfig, ax = plt.subplots(1, 1, figsize=(25,12))\ndf_breeds.plot(kind=\"bar\",\n               legend=False,\n               ax=ax)\nax.axhline(df_breeds[\"num_pictures\"].mean(),\n           color='r', alpha=.7,\n           linestyle='--',\n           label=\"Mean of pictures\")\nplt.title(\"Number of pictures for each \"\\\n          \"dogs breeds of Dataset\",\n          color=\"#343434\", fontsize=22)\nplt.legend()\nplt.show()","145c163d":"def show_images_classes(path, classes, num_sample):\n    \"\"\"This function is used to display the first \n    n images of a directory passed as an argument. \n    It is adapted to subdirectories. \n    \n    The matplotlib.image library must be loaded \n    with the alias mpimg. \n\n    Parameters\n    ----------------------------------------\n    path : string\n        Link of root directory\n    classes : string \n        Name of the subdirectory\n    num_smaple : integer\n        Number of picture to show\n    ----------------------------------------\n    \"\"\"\n    fig = plt.figure(figsize=(20,20))\n    fig.patch.set_facecolor('#343434')\n    plt.suptitle(\"{}\".format(classes.split(\"-\")[1]), y=.83,\n                 color=\"white\", fontsize=22)\n    images = os.listdir(path + \"\/\" + classes)[:num_sample]\n    for i in range(num_sample):\n        img = mpimg.imread(path+\"\/\"+classes+\"\/\"+images[i])\n        plt.subplot(num_sample\/5+1, 5, i+1)\n        plt.imshow(img)\n        plt.axis('off')\n    plt.show()","a2378cc3":"for i in np.random.randint(0, len(breed_list), size=3):\n    show_images_classes(images_dir, breed_list[i], 5)","8f09b20a":"# Define test image\nimg_test = (images_dir \n            + \"\/\" \n            + \"n02085782-Japanese_spaniel\/n02085782_1626.jpg\")\nimg_test = cv.imread(img_test)","192bc543":"# setting dim of the resize\nheight = 299\nwidth = 299\ndim = (width, height)\n# resize image with OpenCV\nres_img = cv.resize(img_test, dim, interpolation=cv.INTER_LINEAR)\n\n# Show both img\nfig = plt.figure(figsize=(16,6))\nplt.subplot(1, 2, 1)\nplt.imshow(img_test)\nplt.title(\"Original shape : {}\".format(img_test.shape))\nplt.subplot(1, 2, 2)\nplt.imshow(res_img)\nplt.title(\"Resized shape : {}\".format(res_img.shape))\nplt.suptitle(\"Resizing image\",\n             color=\"black\", \n             fontsize=22, y=.98)\nplt.show()","ce7b8b47":"# Transform image with differents color sets\nimg_RGB = cv.cvtColor(img_test, cv.COLOR_BGR2RGB)\nimg_grayscale = cv.cvtColor(img_test, cv.COLOR_RGB2GRAY)\nimg_YUV = cv.cvtColor(img_test,cv.COLOR_BGR2YUV)","643ee249":"# Create histogram\ndef plot_histogram(init_img, convert_img):\n    \"\"\"Function allowing to display the initial\n    and converted images according to a certain\n    colorimetric format as well as the histogram\n    of the latter. \n\n    Parameters\n    -------------------------------------------\n    init_img : list\n        init_img[0] = Title of the init image\n        init_img[1] = Init openCV image\n    convert_img : list\n        convert_img[0] = Title of the converted\n        convert_img[1] = converted openCV image\n    -------------------------------------------\n    \"\"\"\n    hist, bins = np.histogram(\n                    convert_img[1].flatten(),\n                    256, [0,256])\n    # Cumulative Distribution Function\n    cdf = hist.cumsum()\n    cdf_normalized = cdf * float(hist.max()) \/ cdf.max()\n\n    # Plot histogram\n    fig = plt.figure(figsize=(25,6))\n    plt.subplot(1, 3, 1)\n    plt.imshow(init_img[1])\n    plt.title(\"{} Image\".format(init_img[0]), \n              color=\"#343434\")\n    plt.subplot(1, 3, 2)\n    plt.imshow(convert_img[1])\n    plt.title(\"{} Image\".format(convert_img[0]), \n              color=\"#343434\")\n    plt.subplot(1, 3, 3)\n    plt.plot(cdf_normalized, \n             color='r', alpha=.7,\n             linestyle='--')\n    plt.hist(convert_img[1].flatten(),256,[0,256])\n    plt.xlim([0,256])\n    plt.legend(('cdf','histogram'), loc = 'upper left')\n    plt.title(\"Histogram of convert image\", color=\"#343434\")\n    plt.suptitle(\"Histogram and cumulative \"\\\n                 \"distribution for test image\",\n              color=\"black\", fontsize=22, y=.98)\n    plt.show()","8813ef03":"plot_histogram([\"RGB\", img_RGB], [\"YUV\", img_YUV])","52830179":"# Equalization\nimg_YUV[:,:,0] = cv.equalizeHist(img_YUV[:,:,0])\nimg_equ = cv.cvtColor(img_YUV, cv.COLOR_YUV2RGB)\nplot_histogram([\"RGB\", img_RGB], [\"Equalized\", img_equ])","44a50c06":"# Apply non-local means filter on test img\ndst_img = cv.fastNlMeansDenoisingColored(\n    src=img_equ,\n    dst=None,\n    h=10,\n    hColor=10,\n    templateWindowSize=7,\n    searchWindowSize=21)\n\n# Show both img\nfig = plt.figure(figsize=(16,6))\nplt.subplot(1, 2, 1)\nplt.imshow(img_equ)\nplt.title(\"Original Image\")\nplt.subplot(1, 2, 2)\nplt.imshow(dst_img)\nplt.title(\"Filtered Image\")\nplt.suptitle(\"Non-local Means Filter\",\n             color=\"black\", \n             fontsize=22, y=.98)\nplt.show()","c615bd2b":"#Initilize Data Generator Keras\naugmented_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest')\n\n# Convert test img to array\nx = image.img_to_array(img_test)\nx = x.reshape((1,) + x.shape)\n\ni=0\nfig = plt.figure(figsize=(16,12))\nfor batch in augmented_datagen.flow(x, batch_size=1):\n    ax = fig.add_subplot(3,4,i+1)\n    ax.imshow(image.array_to_img(batch[0]))\n    i += 1\n    if i % 12 == 0:\n        break\n\nplt.suptitle(\"Data Augmentation with Keras\",\n             color=\"black\", \n             fontsize=22, y=.90)\nplt.show()\n","4fbb9a2b":"def preprocessing_cnn(directories, img_width, img_height):\n    \"\"\"Preprocessing of images in order to integrate them \n    into a convolutional neural network. Equalization, \n    Denoising and transformation of the image into Array. \n    Simultaneous creation of labels (y). \n\n    Parameters\n    ---------------------------------------------------\n    directoriesList : list\n        List of files to be processed.\n    img_width : integer\n        width of the image to be reached for resizing\n    img_height : integer\n        height of the image to be reached for resizing\n    ---------------------------------------------------\n    \"\"\"\n    img_list=[]\n    labels=[]\n    for index, breed in enumerate(directories):\n        for image_name in os.listdir(images_dir+\"\/\"+breed):\n            # Read image\n            img = cv.imread(images_dir+\"\/\"+breed+\"\/\"+image_name)\n            img = cv.cvtColor(img,cv.COLOR_BGR2RGB)\n            # Resize image\n            dim = (img_width, img_height)\n            img = cv.resize(img, dim, interpolation=cv.INTER_LINEAR)\n            # Equalization\n            img_yuv = cv.cvtColor(img,cv.COLOR_BGR2YUV)\n            img_yuv[:,:,0] = cv.equalizeHist(img_yuv[:,:,0])\n            img_equ = cv.cvtColor(img_yuv, cv.COLOR_YUV2RGB)\n            # Apply non-local means filter on test img\n            dst_img = cv.fastNlMeansDenoisingColored(\n                src=img_equ,\n                dst=None,\n                h=10,\n                hColor=10,\n                templateWindowSize=7,\n                searchWindowSize=21)\n            \n            # Convert modified img to array\n            img_array = image.img_to_array(dst_img)\n            \n            # Append lists of labels and images\n            img_list.append(np.array(img_array))\n            labels.append(breed.split(\"-\")[1])\n    \n    return img_list, labels","2fe25c41":"fr_breed_list = [\n    'n02096294-Australian_terrier',\n    'n02093256-Staffordshire_bullterrier',\n    'n02099601-golden_retriever',\n    'n02106662-German_shepherd',\n    'n02086240-Shih-Tzu',\n    'n02099712-Labrador_retriever',\n    'n02088364-beagle',\n    'n02100735-English_setter',\n    'n02102318-cocker_spaniel',\n    'n02108915-French_bulldog',\n    'n02094433-Yorkshire_terrier',\n    'n02085620-Chihuahua',\n    'n02110185-Siberian_husky',\n    'n02106166-Border_collie',\n    'n02106550-Rottweiler']","f0e5ff64":"# Define numbers of breeds to preprocess\nnum_breeds = len(fr_breed_list) \n\n# Execute preprocessor on selection\nstart_time = timeit.default_timer()\n# X = images\n# y = labels\nX, y = preprocessing_cnn(fr_breed_list, 299, 299)\n# Convert in numpy array\nX = np.array(X)\ny = np.array(y)\npreprocess_time = timeit.default_timer() - start_time\nprint(\"-\" * 50)\nprint(\"Execution time for preprocessing :\")\nprint(\"-\" * 50)\nprint(\"Number of images preprocessed : {}\"\\\n     .format(len(y)))\nprint(\"Shape of images np.array : {}\"\\\n     .format(X.shape))\nprint(\"Total time : {:.2f}s\".format(preprocess_time))","b42a5fbc":"# Show exemple preprocessed image\nplt.imshow(image.array_to_img(X[1234]))","44e6927e":"# Using np.shuffle\nimg_space = np.arange(X.shape[0])\nnp.random.seed(8)\n# Shuffle the space\nnp.random.shuffle(img_space)\n# Apply to X and y in same order\nX = X[img_space]\ny = y[img_space]","f4b420c5":"# Change X type \nX = X.astype(np.float32)\n# Encode y text data in numeric\nencoder = LabelEncoder()\nencoder.fit(y)\ny = encoder.transform(y)","002abbbf":"# Verifie encoder created classes\nprint(encoder.classes_)","75f9e492":"### Create train and test set\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nprint(\"-\" * 50)\nprint(\"Size of created sets :\")\nprint(\"-\" * 50)\nprint(\"Train set size = \",x_train.shape[0])\nprint(\"Test set size = \",x_test.shape[0])","77d207b3":"# Data generator on train set with Data Augmentation\n# Validation set is define here\ntrain_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    validation_split=0.2)\n\n#For validation and test, just rescale\ntest_datagen = ImageDataGenerator(rescale=1.\/255)","838a6d30":"def recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","51ebdf1c":"K.clear_session()\nmodel = Sequential()\n# Convolution layer\nmodel.add(Conv2D(filters=16,\n                 kernel_size=(3,3), \n                 padding='same',\n                 use_bias=False,\n                 input_shape=(299,299,3)))\nmodel.add(BatchNormalization(axis=3, scale=False))\nmodel.add(Activation('relu'))\n# Pooling layer\nmodel.add(MaxPooling2D(pool_size=(4, 4),\n                       strides=(4, 4),\n                       padding='same'))\n# Second convolution layer\nmodel.add(Conv2D(filters=32,\n                 kernel_size=(3,3), \n                 padding='same',\n                 use_bias=False))\nmodel.add(BatchNormalization(axis=3, scale=False))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding='same'))\nmodel.add(Dropout(0.2))\n# Third convolution layer\nmodel.add(Conv2D(filters=64,\n                 kernel_size=(3,3), \n                 padding='same',\n                 use_bias=False))\nmodel.add(BatchNormalization(axis=3, scale=False))\nmodel.add(Activation('relu'))\nmodel.add(GlobalAveragePooling2D())\n# Fully connected layers\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(num_breeds, activation='softmax'))\nplot_model(model, to_file='CNN_model_plot.png', \n           show_shapes=True, show_layer_names=True)","3c23a48a":"# Compile the CNN Model\nmodel.compile(optimizer=\"adam\",\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\", f1_m])","71eb04a7":"history = model.fit(\n    train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='training'),\n    validation_data=train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='validation'),\n    steps_per_epoch=len(x_train) \/ 32,\n    epochs=20,\n    verbose=2)","4fd764f3":"def plot_history_scores(dict_history, first_score, second_score):\n    with plt.style.context('seaborn-whitegrid'):\n        fig = plt.figure(figsize=(25,10))\n        # summarize history for accuracy\n        plt.subplot(1, 2, 1)\n        plt.plot(dict_history.history[first_score], color=\"g\")\n        plt.plot(dict_history.history['val_' + first_score],\n                 linestyle='--', color=\"orange\")\n        plt.title('CNN model ' + first_score, fontsize=18)\n        plt.ylabel(first_score)\n        plt.xlabel('epoch')\n        plt.legend(['train', 'validation'], loc='upper left')\n        # summarize history for loss\n        plt.subplot(1, 2, 2)\n        plt.plot(dict_history.history[second_score], color=\"g\")\n        plt.plot(dict_history.history['val_' + second_score],\n                 linestyle='--', color=\"orange\")\n        plt.title('CNN model ' + second_score, fontsize=18)\n        plt.ylabel(second_score)\n        plt.xlabel('epoch')\n        plt.legend(['train', 'validation'], loc='upper left')\n        plt.show()","bea9e146":"plot_history_scores(\n    dict_history = history, \n    first_score = \"accuracy\", \n    second_score = \"f1_m\")","864e3285":"K.clear_session()\n# Import Xception trained model\nxception_model = tf.keras.applications.xception.Xception(\n    weights='imagenet',\n    include_top=False, \n    pooling='avg',\n    input_shape=(299,299,3))\n\n# look at the differents layers\nprint(\"-\" * 50)\nprint(\"Xception base model layers :\")\nprint(\"-\" * 50)\nfor layer in xception_model.layers:\n    print(layer)","4b45b253":"for layer in xception_model.layers:\n    layer.trainable = False","12e5d2c5":"# Add new fully-connected layers\nbase_output = xception_model.output\nbase_output = Dense(128, activation='relu')(base_output)\nbase_output = Dropout(0.2)(base_output)\n# Output : new classifier\npredictions = Dense(num_breeds, activation='softmax')(base_output)\n\n# Define new model\nmy_xcept_model = Model(inputs=xception_model.input,\n                       outputs=predictions)\nmy_xcept_model.compile(optimizer=\"adam\",\n                       loss=\"sparse_categorical_crossentropy\",\n                       metrics=[\"accuracy\", f1_m])","31176b29":"Xception_plot = plot_model(my_xcept_model,\n                           to_file='xcept_model_plot.png',\n                           show_shapes=True,\n                           show_layer_names=False)","08ec35ed":"# Data generator on train set with Data Augmentation\n# and preprocess_input Xception\n# Validation set is define here\ntrain_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    validation_split=0.2,\n    preprocessing_function=tf.keras.applications.xception.preprocess_input)\n\n#For validation and test, just rescale\ntest_datagen = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.xception.preprocess_input)","fab4999a":"history_xcept = my_xcept_model.fit(\n    train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='training'),\n    validation_data=train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='validation'),\n    steps_per_epoch=len(x_train) \/ 32,\n    epochs=20,\n    verbose=2)","69feaa02":"plot_history_scores(\n    dict_history = history_xcept, \n    first_score = \"accuracy\", \n    second_score = \"f1_m\")","ea4ea818":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(13,10))\n    plt.plot(history.history['accuracy'],\n             label='CNN')\n    plt.plot(history_xcept.history['accuracy'],\n             label='Xception')\n    plt.title('Accuracy of differents ConvNet tested over epochs',\n              fontsize=18)\n    plt.ylabel('Accuracy')\n    plt.xlabel('epoch')\n    plt.legend(loc='upper left')\n    plt.show()","e3bb7b8e":"xcept_mean_accuracy = np.mean(history_xcept.history['val_accuracy'])\nxcept_mean_f1 = np.mean(history_xcept.history['val_f1_m'])\nprint(\"-\" * 50)\nprint(\"Xception base model validation Scores :\")\nprint(\"-\" * 50)\nprint(\"Mean validation accuracy: {:.2f}\"\\\n      .format(xcept_mean_accuracy))\nprint(\"Mean validation F1 score: {:.2f}\"\\\n      .format(xcept_mean_f1))","93fb3591":" K.clear_session()\n# Import ResNet50 trained model\nresnet_model = tf.keras.applications.ResNet50(\n    weights='imagenet',\n    include_top=False, \n    pooling='avg',\n    input_shape=(299,299,3))\n\n# Dont retrain layers\nfor rn_layer in resnet_model.layers:\n    rn_layer.trainable = False\n    \n# Add new fully-connected layers\nrn_base_output = resnet_model.output\nrn_base_output = Dense(128, activation='relu')(rn_base_output)\nrn_base_output = Dropout(0.2)(rn_base_output)\n# Output : new classifier\nrn_predictions = Dense(num_breeds, activation='softmax')(rn_base_output)\n\n# Define new model\nmy_resnet_model = Model(inputs=resnet_model.input,\n                        outputs=rn_predictions)\nmy_resnet_model.compile(optimizer=\"adam\",\n                       loss=\"sparse_categorical_crossentropy\",\n                       metrics=[\"accuracy\", f1_m])\n\n# Data generator on train set with Data Augmentation\n# and preprocess_input Resnet\n# Validation set is define here\nrn_train_datagen = ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    validation_split=0.2,\n    preprocessing_function=tf.keras.applications.resnet.preprocess_input)\n\n#For validation and test, just rescale\nrn_test_datagen = ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.resnet.preprocess_input)","dc78defa":"history_resnet = my_resnet_model.fit(\n    rn_train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='training'),\n    validation_data=rn_train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='validation'),\n    steps_per_epoch=len(x_train) \/ 32,\n    epochs=20,\n    verbose=2)","78300dc6":"plot_history_scores(\n    dict_history = history_resnet, \n    first_score = \"accuracy\", \n    second_score = \"f1_m\")","f7eb9e6d":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(13,10))\n    plt.plot(history.history['accuracy'],\n             label='CNN - Mean accuracy: {:.2f}'.format(\n                 np.mean(history.history['accuracy'])))\n    plt.plot(history_xcept.history['accuracy'],\n             label='Xception - Mean accuracy: {:.2f}'.format(\n                 np.mean(history_xcept.history['accuracy'])))\n    plt.plot(history_resnet.history['accuracy'],\n             label='Resnet50 - Mean accuracy: {:.2f}'.format(\n                 np.mean(history_resnet.history['accuracy'])))\n    plt.title('Accuracy of differents ConvNet tested over epochs',\n              fontsize=18)\n    plt.ylabel('Accuracy')\n    plt.xlabel('epoch')\n    plt.legend(loc='upper left')\n    plt.show()","8e0f76dc":"def model_builder(hp):\n    # Load base model\n    xception_model = tf.keras.applications.xception.Xception(\n        weights='imagenet',\n        include_top=False,\n        pooling='avg',\n        input_shape=(299,299,3))\n    \n    for layer in xception_model.layers:\n        layer.trainable = False\n    \n    base_output = xception_model.output\n    \n    # Tune dense units\n    hp_units = hp.Int('dense_units',\n                      min_value=32,\n                      max_value=300,\n                      step=32,\n                      default=128)\n\n    base_output = Dense(units=hp_units, \n                        activation='relu')(base_output)\n    \n    base_output = Dropout(0.2)(base_output)\n    \n    # Output : new classifier\n    predictions = Dense(num_breeds, activation='softmax')(base_output)\n\n    # Define new model\n    my_xcept_model = Model(inputs=xception_model.input,\n                       outputs=predictions)\n    \n    # Tune learning rate\n    hp_learning_rate = hp.Choice(\n        name='learning_rate',\n        values=[1e-2, 1e-3, 1e-4])\n\n    my_xcept_model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\", f1_m])\n    \n    return my_xcept_model","a755dc63":"# Tune the learning rate for the optimizer\n# Constuct the tuner of kerastuner\ntuner = kt.RandomSearch(\n    model_builder, \n    objective='val_accuracy',\n    max_trials=5)\n\n# Define a early stopping\nstop_early = tf.keras.callbacks.EarlyStopping(\n    monitor='val_accuracy',\n    patience=5)\n\n# Search best params\ntuner.search(\n    train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='training'),\n    validation_data=train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='validation'),\n    epochs=10,\n    callbacks=[stop_early])\n\n# Get the optimal hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(\"-\" * 50)\nprint(\"Xception Hyperparameters optimization :\")\nprint(\"-\" * 50)\nprint(f\"\"\"\nBest learning rate : {best_hps.get('learning_rate')}.\\n\nBest Dense units : {best_hps.get('dense_units')}.\"\"\")","6af6ed3d":"hypermodel = tuner.hypermodel.build(best_hps)\nhypermodel.fit(\n    train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='training'), \n    epochs=30,\n    validation_data=train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        subset='validation'),\n    verbose=2)\nhypermodel.save('xception_hypermodel.h5')\nprint(\"Model saved\")","a7a13596":"def xception_fine_tune(nb_layers):\n    # Load the pre trained model\n    hypermodel_t = load_model('.\/xception_hypermodel.h5', custom_objects={\"f1_m\": f1_m})\n    \n    # re train the last layers\n    for i, layer in enumerate(hypermodel_t.layers):\n        if i < nb_layers:\n            layer.trainable = False\n        else:\n            layer.trainable = True\n            \n    # Compile model\n    hypermodel_t.compile(\n        optimizer='adam',\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\", f1_m])\n    \n    return hypermodel_t","b91d11dc":"# Define a early stopping\nstop_early = tf.keras.callbacks.EarlyStopping(\n    monitor='val_accuracy',\n    patience=5)\n\n# Dont train the 115 first layers\nmy_tuned_xcept_model = xception_fine_tune(115)\nfine_tuned_history = my_tuned_xcept_model.fit(\n    train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        shuffle=False,\n        subset='training'), \n    epochs=20,\n    validation_data=train_datagen.flow(\n        x_train, y_train,\n        batch_size=16,\n        shuffle=False,\n        subset='validation'),\n    callbacks=[stop_early],\n    verbose=2)","436cf6de":"plot_history_scores(\n    dict_history = fine_tuned_history, \n    first_score = \"accuracy\", \n    second_score = \"f1_m\")","85d2e76e":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(13,10))\n    plt.plot(history.history['accuracy'],\n             label='CNN - Mean accuracy: {:.2f}'.format(\n                 np.mean(history.history['accuracy'])))\n    plt.plot(history_xcept.history['accuracy'],\n             label='Xception - Mean accuracy: {:.2f}'.format(\n                 np.mean(history_xcept.history['accuracy'])))\n    plt.plot(history_resnet.history['accuracy'],\n             label='Resnet50 - Mean accuracy: {:.2f}'.format(\n                 np.mean(history_resnet.history['accuracy'])))\n    plt.plot(fine_tuned_history.history['accuracy'],\n             label='Fine-tuned Xception - Mean accuracy: {:.2f}'.format(\n                 np.mean(fine_tuned_history.history['accuracy'])))\n    plt.title('Accuracy of differents ConvNet tested over epochs',\n              fontsize=18)\n    plt.ylabel('Accuracy')\n    plt.xlabel('epoch')\n    plt.legend(loc='upper left')\n    plt.show()","8ca98b2b":"# Model evaluation on test set\nxception_eval = fine_tuned_history.model.evaluate(\n    test_datagen.flow(\n        x_test, y_test,\n        batch_size=16,\n        shuffle=False),\n    verbose=1)\nprint(\"-\" * 50)\nprint(\"Xception model evaluation :\")\nprint(\"-\" * 50)\nprint('Test Loss: {:.3f}'.format(xception_eval[0]))\nprint('Test Accuracy: {:.3f}'.format(xception_eval[1]))\nprint('Test F1 score: {:.3f}'.format(xception_eval[2]))","6b72881d":"# Make predictions\nY_pred = fine_tuned_history.model.predict(\n    test_datagen.flow(\n        x_test, y_test,\n        batch_size=16,\n        shuffle=False))\ny_pred = np.argmax(Y_pred, axis=1)\n\n# Inverse transform of encoding\ny_pred_s = encoder.inverse_transform(y_pred)\ny_test_s = encoder.inverse_transform(y_test)\n\n# Confusion Matrix\ncf_matrix = confusion_matrix(y_test, y_pred)\n\nfig = plt.figure(figsize=(12,10))\nax = sns.heatmap(cf_matrix, annot=True)\nax.set_xlabel(\"Predicted labels\", color=\"g\")\nax.set_ylabel(\"True labels\", color=\"orange\")\nax.xaxis.set_ticklabels(encoder.classes_, \n                        rotation='vertical')\nax.yaxis.set_ticklabels(encoder.classes_,\n                        rotation='horizontal')\nplt.title(\"Confusion Matrix on Xception predicted results\\n\",\n          fontsize=18)\nplt.show()","c93bd080":"# Classification report\nprint(classification_report(\n    y_test, y_pred, \n    target_names=sorted(set(y_test_s))))","68633949":"fig = plt.figure(1, figsize=(20,20))\nfig.patch.set_facecolor('#343434')\nplt.suptitle(\"Predicted VS actual for Xception model fine-tuned\",\n             y=.92, fontsize=22,\n             color=\"white\")\n\nn = 0\n\nfor i in range(12):\n    n+=1\n    r = int(np.random.randint(0, x_test.shape[0], 1))\n    plt.subplot(3,4,n)\n    plt.subplots_adjust(hspace = 0.1, wspace = 0.1)\n    plt.imshow(image.array_to_img(x_test[r]))\n    plt.title('Actual = {}\\nPredicted = {}'.format(y_test_s[r] , y_pred_s[r]),\n              color=\"white\")\n    plt.xticks([]) , plt.yticks([])\n    \nplt.show()","ee2b4c7f":"# Save the last model\nfine_tuned_history.model.save('xception_trained_model.h5')\nprint(\"Last model saved\")","4dfaa0b1":"# Load model\nmodel = load_model('.\/xception_hypermodel.h5', custom_objects={\"f1_m\": f1_m})\n\n# Define the full prediction function\ndef breed_prediction(inp):\n    # Convert to RGB\n    img = cv.cvtColor(inp,cv.COLOR_BGR2RGB)\n    # Resize image\n    dim = (299, 299)\n    img = cv.resize(img, dim, interpolation=cv.INTER_LINEAR)\n    # Equalization\n    img_yuv = cv.cvtColor(img,cv.COLOR_BGR2YUV)\n    img_yuv[:,:,0] = cv.equalizeHist(img_yuv[:,:,0])\n    img_equ = cv.cvtColor(img_yuv, cv.COLOR_YUV2RGB)\n    # Apply non-local means filter on test img\n    dst_img = cv.fastNlMeansDenoisingColored(\n        src=img_equ,\n        dst=None,\n        h=10,\n        hColor=10,\n        templateWindowSize=7,\n        searchWindowSize=21)\n\n    # Convert modified img to array\n    img_array = keras.preprocessing.image.img_to_array(dst_img)\n    \n    # Apply preprocess Xception\n    img_array = img_array.reshape((-1, 299, 299, 3))\n    img_array = tf.keras.applications.xception.preprocess_input(img_array)\n    \n    # Predictions\n    prediction = model.predict(img_array).flatten()\n    \n    #return prediction\n    return {encoder.classes_[i]: float(prediction[i]) for i in range(num_breeds)}\n\n# Construct the interface\nimage = gr.inputs.Image(shape=(299,299))\nlabel = gr.outputs.Label(num_top_classes=3)\n\ngr.Interface(\n    fn=breed_prediction,\n    inputs=image,\n    outputs=label,\n    capture_session=True\n).launch(share=True)","99371174":"from IPython.display import IFrame\nIFrame('https:\/\/dogs-breeds-detection-cnn.herokuapp.com\/', width=1000, height=900)","0a9b464c":"Nous allons devoir **modifier la taille des images** pour qu'elles s'adaptent aux contraintes du mod\u00e8le CNN de transfert learning. Cela aura \u00e9galement pour effet de diminuer les temps de calculs de notre mod\u00e8le \"from scratch\".\n\n## <span style=\"color:#343434\" id=\"section_1_2\">1.2. Modification de la taille des images<\/span>\n\nOn peut remarquer dans les images en exemple ci-dessus que les chiens pr\u00e9sents sur les photos ne sont pas toujours au centre de la photo, que les zooms sont diff\u00e9rents. Redimensionner les images ne va pas changer ces attributs, **l'image va m\u00eame \u00eatre d\u00e9form\u00e9e** pour coller au nouvelles dimensions.\n\nNous allons r\u00e9aliser la transformation sur une image test pour commencer.","88fa63f2":"On remarque que les races de chien sont toutes bien aliment\u00e9es en images. La moyenne se situe \u00e0 171 photos par classe. Aucune race n'est sous repr\u00e9sent\u00e9e nous pouvons donc toutes les conserver pour le moment.\n\n**Regardons quelques exemples des photos par races** disponibles dans la base d'\u00e9tude :","15da1924":"On voit parfaitement ainsi que les pr\u00e9dictions finales sont bonnes dans la plupart des cas. \n\n# <span style=\"color:#343434\" id=\"section_6\">6. Interface de pr\u00e9diction sur de nouvelles donn\u00e9es<\/span>\nA pr\u00e9sent, nous allons d\u00e9velopper un script pour permettre la pr\u00e9diction de la race du chien \u00e0 partir d'une photo. Notre mod\u00e8le pr\u00e9-entrain\u00e9 servira de base. Pour cela, nous allons utiliser `Gradio` pour importer les images *(inputs)* et appliquer notre classifier afin de retourner la pr\u00e9diction","df89f530":"On voit tr\u00e8s bien ici que les m\u00e9triques du mod\u00e8le Xception sur nos donn\u00e9es sont bien meilleures que le mod\u00e8le \"from scratch\" quand les couches profondes ne sont pas entrain\u00e9es. D'autre part, le mod\u00e8le apprend tr\u00e8s vite et l'accuracy augmente rapidement *(tout comme la perte diminue)*. Comparons les 2 mod\u00e8les :","8276f0e5":"Nous allons cr\u00e9er et entrainer les mod\u00e8les sur **15 des 20 races de chiens les plus populaires en France en 2020** selon l'\u00e9tude de [centrale-canine.fr](https:\/\/www.centrale-canine.fr\/actualites\/lof-2020-les-races-de-chiens-preferees-des-francais). Cet algorithme pourra ensuite \u00eatre \u00e9tendu \u00e0 l'ensemble des races s'il est satisfaisant.","771dea2e":"On remarque dans les layers import\u00e9s du mod\u00e8le que la derni\u00e8re couche import\u00e9e est une couche `GlobalAveragePooling2D`. Nous allons donc **ajouter une couche compl\u00e9tement connect\u00e9e, un DropOut et enfin le classifier** dans un nouveau mod\u00e8le :# <span style=\"color:#343434\" id=\"section_1\">1. Preprocessing des images<\/span>","70c99e7b":"A pr\u00e9sent, nous pouvons **cr\u00e9er les sets d'entrainement et de test qui serviront \u00e0 l'entrainement de nos mod\u00e8les**. Le set de **validation** quant \u00e0 lui sera cr\u00e9\u00e9 directement dans le g\u00e9n\u00e9rateur Keras.","c8a0cc27":"On voit que sur ce mod\u00e8le CNN from scratch les m\u00e9triques ne sont pas bonnes. L'Accuracy ne d\u00e9passe pas 13% et le score F1 est \u00e9lev\u00e9 et stable sur les 50 \u00e9poques.\n\nNous allons maintenant tester des mod\u00e8les pr\u00e9-entrain\u00e9s et v\u00e9rifier si les performances sont meilleures compar\u00e9es \u00e0 notre baseline.\n\n# <span style=\"color:#343434\" id=\"section_3\">3. Transfert Learning : Mod\u00e8le CNN pr\u00e9-entrain\u00e9 Xception<\/span>\nLe mod\u00e8le Xception est d\u00e9riv\u00e9 de l'architecture Inception. Inception a pour but de r\u00e9duire la consommation de ressources des CNN profonds. Il repose sur l'utilisation de blocs de traitement suivants :\n\n![inception_block](http:\/\/www.mf-data-science.fr\/images\/projects\/inception_block.png)\n\nLa dimension de l\u2019image analys\u00e9e est r\u00e9duite par les filtres 3x3 et 5x5, en ajoutant une \u00e9tape de filtrage 1x1 en amont. De cette fa\u00e7on, la convolution de taille 1x1 effectue une op\u00e9ration de pooling sur les valeurs d\u2019un pixel dans l\u2019espace des dimensions de l\u2019image.      \nXception remplace les modules Inception par des modules de convolutions s\u00e9parables en profondeur *(en anglais depthwise separable convolution)* et ajoute des liaisons r\u00e9siduelles. Ce type d\u2019approche permet de consid\u00e9rablement r\u00e9duire l\u2019utilisation de ressources lors du calcul matriciel, sans modifier le nombre de param\u00e8tres.      \n*Pour en apprendre plus :* https:\/\/arxiv.org\/abs\/1610.02357\n\nL'architecture Xception est la suivante :\n\n![inception_block](http:\/\/www.mf-data-science.fr\/images\/projects\/imagenet_xception_flow.png)\n\n## <span style=\"color:#343434\" id=\"section_3_1\">3.1. Importation du mod\u00e8le Xception pr\u00e9-entrain\u00e9<\/span>\n\nChargeons le mod\u00e8le de base pr\u00e9-entrain\u00e9 de Keras mais **sans les couches fully-connected**. **Nous allons ajouter notre propre classifier final**. ","f46c409c":"### Egalisation\n\nOn constate ici des pics importants au centre de l'histogram. Dans le cadre d'une bonne \u00e9galisation (am\u00e9lioration du contraste), il est n\u00e9cessaire de r\u00e9partir la lumi\u00e8re dans tout le spectre de l'image. \n\n**Testons l'\u00e9galisation avec OpenCV :**     \nL'int\u00e9r\u00eat de convertir l'image dans l'espace colorim\u00e9trique YUV est de pouvoir agir sur le canal \"luminance\" (Y) ind\u00e9pendamment des autres canaux de chrominance. Nous allons donc r\u00e9aliser l'\u00e9galisation sur ce seul canal Y :","bfd77c35":"La matrice de confusion et le rapport de classification nous indiquent que les r\u00e9sultats sont satisfaisants.ccuracy globale sur le jeu de test est de 0.72 et la matrice pr\u00e9sente bien les couples predict \/ true majoritaires en diagonale.Nous allons visualiser quelques-unes de ces pr\u00e9dictions avec leurs labels :","52dffc1e":"# <span style=\"color:#343434\" id=\"section_2\">2. Mod\u00e8le CNN from scratch<\/span>\n\nA pr\u00e9sents, nos images test ont \u00e9t\u00e9 pr\u00e9-trait\u00e9es gr\u00e2ce notamment \u00e0 l'\u00e9galisation, le d\u00e9bruitage et le redimensionnement. Nous avons cr\u00e9\u00e9 des listes de tableaux Numpy regroupant les images sous format num\u00e9riques.      \nPour entrainer notre premier mod\u00e8le, nous devons dans un premier temps **m\u00e9langer les images** car actuellement, toutes les photos d'une m\u00eame race se suivent.\n\n## <span style=\"color:#343434\" id=\"section_2_1\">2.1. Pr\u00e9paration des donn\u00e9es pour le mod\u00e8le CNN<\/span>\nNous allons donc m\u00e9langer les datas dans X et y pour le premier passage dans le r\u00e9seau. Ce brassage a pour objectif de r\u00e9duire la variance et de s'assurer que les mod\u00e8les ne soient pas sur-entra\u00een\u00e9s.","43376fbc":"<h1><span style=\"color:#343434\" id=\"sommaire\">Sommaire<\/span><\/h1>\n\n1. [Preprocessing des images](#section_1)     \n    1.1. [Visualisation de la liste des races (classes) et un exemple de donn\u00e9es](#section_1_1)     \n    1.2. [Modification de la taille des images](#section_1_2)     \n    1.3. [Modification des histogrammes des images](#section_1_3)      \n    1.4. [Application de filtres](#section_1_4)      \n    1.5. [Augmentation de donn\u00e9es](#section_1_5)      \n    1.6. [Fonction de traitement par lot pour le preprocessing](#section_1_6)      \n    \n2. [Mod\u00e8le CNN from scratch](#section_2)      \n    2.1. [Pr\u00e9paration des donn\u00e9es pour le mod\u00e8le CNN](#section_2_1)      \n    2.2. [Construction du mod\u00e8le CNN](#section_2_2)      \n    2.3. [Entrainement et \u00e9valuation du mod\u00e8le CNN](#section_2_3)      \n\n3. [Transfert Learning : Mod\u00e8le CNN pr\u00e9-entrain\u00e9 Xception](#section_3)      \n    3.1. [Importation du mod\u00e8le Xception pr\u00e9-entrain\u00e9](#section_3_1)      \n    3.2. [Entrainement du nouveau classifier sur Xception](#section_3_2)      \n    3.3. [Comparaison avec l'architecture du mod\u00e8le ResNet50](#section_3_3)      \n    3.4. [Optimization des param\u00e8tres sur la couche classifier du mod\u00e8le Xception](#section_3_4)      \n    \n4. [Xception fine tuning](#section_4)    \n5. [Evaluation Xception fine tuned sur donn\u00e9es test](#section_5)      \n6. [Interface de pr\u00e9diction sur de nouvelles donn\u00e9es](#section_6)","b4d08d91":"## <span style=\"color:#343434\" id=\"section_3_3\">3.3. Comparaison avec l'architecture du mod\u00e8le ResNet50<\/span>\n\nDans un r\u00e9seau de neurones \u00e0 convolution profonde, plusieurs couches sont empil\u00e9es et entra\u00een\u00e9es \u00e0 la t\u00e2che \u00e0 accomplir. Le r\u00e9seau apprend plusieurs caract\u00e9ristiques de niveau bas\/moyen\/haut \u00e0 la fin de ses couches. Dans l'**apprentissage r\u00e9siduel**, au lieu d'essayer d'apprendre certaines caract\u00e9ristiques, le r\u00e9seau apprend certaines caract\u00e9ristiques r\u00e9siduelles. Le r\u00e9sidu peut \u00eatre simplement compris comme une soustraction d'une caract\u00e9ristique apprise \u00e0 partir de l'entr\u00e9e de cette couche. **ResNet** le fait en utilisant des connexions de raccourci *(connectant directement l'entr\u00e9e de la ni\u00e8me couche \u00e0 une (n+x)i\u00e8me couche)*. Son architecture est la suivante :\n\n![ResNet50](http:\/\/www.mf-data-science.fr\/images\/projects\/RenNet50.jpg)","270fb9bd":"Une fois les meilleurs param\u00e8tres trouv\u00e9s, **on peut r\u00e9-entrainer le mod\u00e8le gr\u00e2ce au tuner**, sur 30 \u00e9poques avant de le sauvegarder pour les prochaines am\u00e9liorations *(fine-tuning)* :","0f8b52db":"Les images sont \u00e0 pr\u00e9sent m\u00e9lang\u00e9es. Pour les mod\u00e9lisations Keras, il faut \u00e9galement modifier les types de nos donn\u00e9es X et convertir nos donn\u00e9es \u00e0 pr\u00e9dire (y) en variable num\u00e9rique. En effet, pour le moment, ce sont des donn\u00e9es textuelles qui sont stock\u00e9es dans y. Nous allons donc utiliser un simple ***LabelEncoder*** pour les convertir.","bb161a04":"On voit bien ici que le redimensionnement en 299 x 299 d\u00e9forme la photo initiale. **Une fois ce filtre appliqu\u00e9 \u00e0 toutes les images, elles seront probablement toutes d\u00e9form\u00e9es**.\n\nAutre fait, avec un set d'image relativement important, les expositions, contraste, ... sont relativement diff\u00e9rents pour chaque photo. Nous allons \u00e0 pr\u00e9sent utiliser des m\u00e9thodes bas\u00e9es sur les histogrammes de ces images pour pre-processer au mieux ces donn\u00e9es.\n\n## <span style=\"color:#343434\" id=\"section_1_3\">1.3. Modification de l'histogramme des images<\/span>\n\nL'histogramme d'une image num\u00e9rique est une courbe statistique repr\u00e9sentant la **r\u00e9partition de ses pixels selon leur intensit\u00e9**. Commen\u00e7ons par regarder une image en particulier.\n\nNous allons transformer l'image dans diff\u00e9rents codages couleurs. Le syst\u00e8me de codage **YUV** est cr\u00e9\u00e9 depuis une source RVB. Il est cod\u00e9 en trois composantes : **Y** repr\u00e9sente la luminance *(informations de luminosit\u00e9)* tandis que les deux autres (**U** et **V**) sont des donn\u00e9es de chrominance *(informations de couleur)*. Ce format nous permet de visualiser au mieux l'histogramme pour les 3 dimensions :","12b4d891":"A cette \u00e9tape, sans am\u00e9lioration des param\u00e8tres et sans fine-tuning, **le mod\u00e8le Xception offre les meilleurs r\u00e9sultats**. Nous conserverons donc ce mod\u00e8le pour la suite.\n\n## <span style=\"color:#343434\" id=\"section_3_4\">3.4. Optimization des param\u00e8tres sur la couche classifier du mod\u00e8le Xception<\/span>\n\nIci nous allons utiliser `kerastuner` pour am\u00e9liorer les param\u00e8tres de la couche que nous avons ajout\u00e9 au mo\u00e8le Xception de Keras et s\u00e9l\u00e9ctionner les meilleurs.","79db1950":"Derni\u00e8re \u00e9tape de la pr\u00e9paration, nous allons **cr\u00e9er les g\u00e9n\u00e9rateurs Keras** pour les sets de donn\u00e9es en incluant la **Data Augmentation pour le jeu d'entrainement** :","a5ecc0ea":"Notre mod\u00e8le est cr\u00e9\u00e9, nous allons pouvoir l'**entrainer sur 50 \u00e9poques gr\u00e2ce aux g\u00e9n\u00e9rateur pr\u00e9c\u00e9dement cr\u00e9\u00e9s et stocker les r\u00e9sultats** pour ensuite les analyser :\n\n## <span style=\"color:#343434\" id=\"section_2_3\">2.3. Entrainement et \u00e9valuation du mod\u00e8le CNN<\/span>","2cd57c67":"Ces g\u00e9n\u00e9rateurs seront ainsi utilis\u00e9s dans les pipeline de mod\u00e9lisation Keras.\n\n## <span style=\"color:#343434\" id=\"section_2_2\">2.2. Construction du mod\u00e8le CNN<\/span>\nNous allons ici initialiser un premier mod\u00e8le de r\u00e9seau de neurones \u00e0 convolution en imbriquant plusieurs couches :\n- **Couches de convolution** : Son but est de rep\u00e9rer la pr\u00e9sence d'un ensemble de features dans les images re\u00e7ues en entr\u00e9e. Pour cela, on r\u00e9alise un filtrage par convolution.\n- **Couches de Pooling** : L'op\u00e9ration de pooling consiste \u00e0 r\u00e9duire la taille des images, tout en pr\u00e9servant leurs caract\u00e9ristiques importantes.\n- **Couches de correction ReLU** : La couche de correction ReLU remplace toutes les valeurs n\u00e9gatives re\u00e7ues en entr\u00e9es par des z\u00e9ros. Elle joue le r\u00f4le de fonction d'activation.\n- **Couches Fully connected** : Ce type de couche re\u00e7oit un vecteur en entr\u00e9e et produit un nouveau vecteur en sortie. Pour cela, elle applique une combinaison lin\u00e9aire puis \u00e9ventuellement une fonction d'activation aux valeurs re\u00e7ues en entr\u00e9e.\n- **DropOut** : La m\u00e9thode du dropout consiste \u00e0 \u00ab d\u00e9sactiver \u00bb des sorties de neurones al\u00e9atoirement pour \u00e9viter le sur-entra\u00eenement.\n\nPour notre mod\u00e8le, nous allons tester une architecture simple, pas trop profonde qui nous servira de baseline pour les procheins mod\u00e8les. Nous allons donc impl\u00e9menter uniquement 3 couches de convolution.\n\nNous allons \u00e9galement d\u00e9finir des **m\u00e9triques plus pr\u00e9cises que Accuracy** pour l'\u00e9valuation de notre mod\u00e8le, comme par exemple le ***Score F1*** qui combine *precision* et *recall* :","5a3c885e":"Apr\u00e8s plusieurs test sur notre programme, on s'apper\u00e7oit que les r\u00e9sultats sont bons et que les pr\u00e9dictions sur des images inconnues sont exactes.\n\nCi-dessous, l'Iframe de l'**application en ligne d\u00e9velopp\u00e9e \u00e0 partir du meilleur mod\u00e8le Xception et d\u00e9ploy\u00e9 sur Heroku** (https:\/\/dogs-breeds-detection-cnn.herokuapp.com\/)","0fdbef73":"Puis nous allons \u00e9diter la **matrice de confusion** et le **rapport de classification** gr\u00e2ce aux donn\u00e9es pr\u00e9dites :","d2499355":"# <span style=\"color:#343434\" id=\"section_4\">4. Xception fine tuning<\/span>\nComme nous l'avons vu dans le sch\u00e9ma de l'architecture Xception, ce mod\u00e8le dispose de 3 blocs : le flux, d'entr\u00e9e, le flux moyen et le flux de sortie. **Nous allons r\u00e9-entrainer le dernier bloc sur le mod\u00e8le sauvegard\u00e9** dont les param\u00e8tres ont \u00e9t\u00e9 am\u00e9lior\u00e9s :","c9c18ac9":"L'image r\u00e9sultante est donc bien \u00e9gualis\u00e9e, l'histogramme couvre l'ensemble du spectre et la CDF est constante et lin\u00e9aire. **Ce pr\u00e9-traitement sera donc appliqu\u00e9 \u00e0 l'ensemble des images**.\n\n## <span style=\"color:#343434\" id=\"section_1_4\">1.4. Application de filtres<\/span>\nLes images peuvent pour de nombreuses raisons \u00eatre bruit\u00e9es, c'est \u00e0 dire comporter des pixels atypiques qui peuvent fausser la d\u00e9tection de features. Par exemple de causes de bruits :\n* Caract\u00e9ristiques de l'appareil photo,\n* Compression des images JPEG\n* Mauvaise r\u00e9solution ...\n\nPour pallier au bruit pr\u00e9sent dans les images, il est possible d'appliquer un filtre. Il en existe de plusieurs types : les filtres lin\u00e9aires (comme le filtre Gaussien), non lin\u00e9aires comme le filtre m\u00e9dian par exemple.      \nun des meilleurs filtre pour d\u00e9bruiter l'image est le **filtre non-local means**.\n\n\n### Filtre non-local means\nContrairement aux filtres \u00ab moyenne locale \u00bb, qui prennent la valeur moyenne d'un groupe de pixels entourant un pixel cible pour lisser l'image, le filtrage des moyennes non locales prend une moyenne de **tous les pixels de l'image**, pond\u00e9r\u00e9e par la similarit\u00e9 de ces pixels avec le pixel cible. Cela se traduit par une clart\u00e9 de post-filtrage beaucoup plus grande et moins de perte de d\u00e9tails dans l'image par rapport aux algorithmes de moyenne locale.\n\nTestons ce fitre sur notre image test :","eb11a715":"120 races de chien sont donc pr\u00e9sentes dans notre jeu de donn\u00e9es, ce qui repr\u00e9sente **120 classes pour notre classifier**.\nNous allons \u00e0 pr\u00e9sent compter le nombre d'images de chaque race afin de v\u00e9rifier si la distribution est \u00e9quitable entre les classes :","fc6fb0f9":"# <span style=\"color:#343434\" id=\"section_1\">1. Preprocessing des images<\/span>\n\nUn des modules impos\u00e9 dans ce Notebook est d'utiliser le **transfert learning**. Une liste des applications et mod\u00e8les pr\u00e9-entrain\u00e9s de Keras est disponible \u00e0 l'adresse : https:\/\/keras.io\/api\/applications\/.\n\nNous allons dans un premier temps s\u00e9lectionner le mod\u00e8le que nous allons utiliser car en d\u00e9pendra notamment la **taille des images \u00e0 utiliser** *(et donc le redimensionnement \u00e0 effectuer)*. Nous allons s\u00e9lectionner un mod\u00e8le :\n* Performant *(avec une bonne Accuracy)*,\n* De taille contenue pour des raisons mat\u00e9rielles et logistiques,\n* Avec une profondeur moyenne pour ne pas surcharger les temps de calculs.\n\nSur ces crit\u00e8res, le mod\u00e8le **Xception** semble adapt\u00e9 pour notre transfert learning : Taille de 88 MB, Top-5 Accuracy \u00e0 0.945 et Depth \u00e0 126. **Les images en entr\u00e9e de ce mod\u00e8le seront de 299px x 299px**. Nous testerons \u00e9galement une seconde architecture pour comparer les r\u00e9sultats obtenus. Nous testerons donc l'architecture **ResNet50**.\n\nPour commencer, nous allons rapidement analyser les donn\u00e9es en regardant notamment l'\u00e9tat de r\u00e9partition des races de chiens dans le r\u00e9pertoire images :\n\n## <span style=\"color:#343434\" id=\"section_1_1\">1.1. Visualisation de la liste des races *(classes)* et un exemple de donn\u00e9es.<\/span>","89a43800":"## <span style=\"color:#343434\" id=\"section_3_2\">3.2. Entrainement du nouveau classifier sur Xception<\/span>\n\nNotre jeu de donn\u00e9es est petit et relativement similaire au dataset original. Si nous entrainons le r\u00e9seau complet, nous risquons de rencontrer des probl\u00e8mes d'over-fitting. Nous allons donc **\"geler\" tous les layers de Xception et entrainer uniquement le classifier**.","45a8511a":"On voit ici que le meilleur mod\u00e8le test\u00e9 est le mod\u00e8le **Xception pr\u00e9-entrain\u00e9 avec fine-tuning**. Nous allons donc r\u00e9aliser les **\u00e9valuations sur nos donn\u00e9es test**.\n\n# <span style=\"color:#343434\" id=\"section_5\">5. Evaluation Xception fine tuned sur donn\u00e9es test<\/span>\nNous allons \u00e9valuer ce dernier mo\u00e8le sur les donn\u00e9es test g\u00e9n\u00e9r\u00e9es gr\u00e2ce au `ImageDataGenerator` de Keras d\u00e9finit pr\u00e9alablement :","4705f33f":"## <span style=\"color:#343434\" id=\"section_1_5\">1.5. Augmentation de donn\u00e9es<\/span>\n\nLe risque sur ce type de dataset comportant peu de donn\u00e9es (20 000 images) est de sur-entrainer notre mod\u00e8le, il ne pourra pas d\u00e9velopper des r\u00e8gles de d\u00e9cisions pouvant \u00eatre g\u00e9n\u00e9ralis\u00e9 \u00e0 de nouvelles donn\u00e9es. Il faut donc augmenter le nombre de data et pour cela, nous allons utiliser la **Data Augmentation**.\n\nL'objectif de la data augmentation est de reproduire les images pr\u00e9existantes en leur appliquant une **transformation al\u00e9atoire**. Pour cela, Keras mets \u00e0 disposition la m\u00e9thode `ImageDataGenerator` qui permet de faire \u00e0 la fois du mirroring, de la rotation, des zoom ... et ce de mani\u00e8re al\u00e9atoire !","bd89299b":"Il est \u00e9galement n\u00e9cessaire de modifier les g\u00e9n\u00e9rateurs pour y **int\u00e9grer le `preprocess_input` Xception** recommand\u00e9 par Keras :","4abf3399":"La m\u00e9thode de Data Augmentation nous permet d'obtenir des images avec diff\u00e9rents zoom, des effets mirroir et des rotations qui vont donc augmenter notre base d'images significativement et efficacement.\n\n## <span style=\"color:#343434\" id=\"section_1_6\">1.6. Fonction de traitement par lot pour le preprocessing<\/span>\n\nNous allons d\u00e9j\u00e0 tester les fonctions et la classification sur un nombre restreint de race de chien afin de limiter les temps de calcul. Constuisons la fonction de preprocessing qui retournera les labels et images :","1285198a":"![baner_dogs](http:\/\/www.mf-data-science.fr\/images\/projects\/dogs.jpg)\n<h1><div style=\"padding:5px; margin-top:-40px; background: #343434; border-bottom: 2px solid #e4bf13; color:white; text-align:center !important;\" id=\"intro\">D\u00e9tection de race de chien<br\/>\u00e0 partir d'images gr\u00e2ce aux <br\/>r\u00e9seaux de neurones \u00e0 convolution<\/div><\/h1>\n\nL'objectif de ce Notebook est de d\u00e9tailler la mise en place d'un **algorithme de d\u00e9tection de la race du chien sur une photo**, afin d'acc\u00e9l\u00e9rer le travail d\u2019indexation dans une base de donn\u00e9es.\n\n### Les contraintes impos\u00e9es :\n- **Pr\u00e9-processer les images** avec des techniques sp\u00e9cifiques *(e.g. whitening, equalization, \u00e9ventuellement modification de la taille des images)*.\n- R\u00e9aliser de la **data augmentation** *(mirroring, cropping...)*.\n- Mise en oeuvre de 2 approches de l'utilisation des CNN :\n    - R\u00e9aliser un r\u00e9seau de neurones CNN from scratch en optimisant les param\u00e8tres.     \n    - Utiliser le transfert learning et ainsi utiliser un r\u00e9seau d\u00e9j\u00e0 entrain\u00e9.\n    \n### Repo et interface de test en ligne :\nUn repo Git est disponible pour ce projet \u00e0 l'adresse : https:\/\/github.com\/MikaData57\/ingenieur-ml-computer-vision-cnn.\nCe repo reprend ce Notebook versionn\u00e9 ainsi que l'architecture de d\u00e9ploiement sur Heroku *(sp\u00e9cifique pour Tensorflow, OpenCV et Gradio)*.\n\nPour tester le mod\u00e8le online sur Heroku : https:\/\/dogs-breeds-detection-cnn.herokuapp.com\/"}}