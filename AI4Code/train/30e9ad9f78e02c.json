{"cell_type":{"d9b5f193":"code","deabe042":"code","4296eac3":"code","a4b99806":"code","1d9f3296":"code","1dccf0ee":"code","38f49092":"code","30b2d3cc":"code","0c259fed":"code","f2fee7d6":"code","fcce3cb1":"code","c9423a63":"code","c817382d":"code","3acdffc9":"code","b9e23d33":"code","a0a33f58":"code","858a925e":"code","903c0fd3":"code","20e3dc44":"code","ec8fb587":"markdown","963393b8":"markdown","cc9740d6":"markdown","5b331542":"markdown"},"source":{"d9b5f193":"import pandas as pd\nimport numpy as np \n","deabe042":"\"\"\"\nI got  starter  code that filters the articles from another kernel\nhttps:\/\/www.kaggle.com\/mlconsult\/summary-page-covid-19-risk-factors \n\n\"\"\"\n\n\n\nimport re\nimport os\nimport json\n# keep only documents with covid -cov-2 and cov2\ndef search_focus(df):\n    dfa = df[df['abstract'].str.contains('covid')]\n    dfb = df[df['abstract'].str.contains('-cov-2')]\n    dfc = df[df['abstract'].str.contains('cov2')]\n    dfd = df[df['abstract'].str.contains('ncov')]\n    frames=[dfa,dfb,dfc,dfd]\n    df = pd.concat(frames)\n    df=df.drop_duplicates(subset='title', keep=\"first\")\n    return df\n\n\ndf=pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv', usecols=['title','journal','abstract','authors','doi','publish_time','sha','pdf_json_files'])\nprint ('All CORD19 documents ',df.shape)\n#fill na fields\ndf=df.fillna('no data provided')\n#drop duplicate titles\ndf = df.drop_duplicates(subset='title', keep=\"first\")\n#keep only 2020 dated papers\ndf=df[df['publish_time'].str.contains('2020')]\n# convert abstracts to lowercase\ndf[\"abstract\"] = df[\"abstract\"].str.lower()+df[\"title\"].str.lower()\n#show 5 lines of the new dataframe\ndf=search_focus(df)\nprint (\"COVID-19 focused docuemnts \",df.shape)\n\n\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\nfor index, row in df.iterrows():\n    if ';' not in row['sha'] and os.path.exists('\/kaggle\/input\/CORD-19-research-challenge\/'+row['pdf_json_files']+'\/'+row['pdf_json_files']+'\/pdf_json\/'+row['sha']+'.json')==True:\n        with open('\/kaggle\/input\/CORD-19-research-challenge\/'+row['pdf_json_files']+'\/'+row['pdf_json_files']+'\/pdf_json\/'+row['sha']+'.json') as json_file:\n            data = json.load(json_file)\n            body=format_body(data['body_text'])\n            keyword_list=['TB','incidence','age']\n            #print (body)\n            body=body.replace(\"\\n\", \" \")\n\n            df.loc[index, 'abstract'] = body.lower()\n\ndf=df.drop(['pdf_json_files'], axis=1)\ndf=df.drop(['sha'], axis=1)\n# df.head()","4296eac3":"df.reset_index(inplace=True)\ndf.drop(\"index\",axis=1,inplace=True)","a4b99806":"import nltk\nnltk.download(\"punkt\")\n\nfrom nltk import word_tokenize,sent_tokenize\nfrom nltk.stem  import PorterStemmer\n\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\nnltk.download('stopwords')\nstops = stopwords.words(\"english\")\n\n\ndef removepunc(my_str):\n    punctuations = '''!()-[]{};:'\"\\,<>.\/?@#$%^&*_~'''\n    no_punct = \"\"\n    for char in my_str:\n        if char not in punctuations:\n            no_punct = no_punct + char\n    return no_punct\n\ndef hasNumbers(inputString):\n    return (bool(re.search(r'\\d', inputString)))\nsnowstem = SnowballStemmer(\"english\")\nportstem = PorterStemmer()\n\n\n\n\"\"\"\nThese are the queries related to risk factors\n\n\"\"\"\n\nusequeries = sent_tokenize(\"\"\"Smoking, pre-existing pulmonary disease\nCo-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities.\ncardiovascular disease , chronic obstructive pulmonary disease and diabetes.\nNeonates and pregnant women.\nSocio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\nTransmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\nSeverity of disease, including risk of fatality among symptomatic hospitalized patients, and high risk patient groups\nSusceptibility of populations.\nPublic health mitigation measures that could be effective for control.\nimmune system disorders.\nheart failure.\ndrinking.\ndiabetes.\n\n\"\"\")\nqueryarticle = [\" \".join([snowstem.stem(removepunc(i.lower())) for i in word_tokenize(x) if i not in stops ]) for x in usequeries]\n","1d9f3296":"\"\"\"\n\nhad to reduce the vocabulary of the data\n\n\n\"\"\"\ndf[\"usetext\"] = df.abstract.apply(lambda x: \" \".join([snowstem.stem(i) for i in word_tokenize(removepunc(x.lower())) if not hasNumbers(i) if i not in stops]))","1dccf0ee":"\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nencArticles = vectorizer.fit_transform(df.usetext)\nencQueries = vectorizer.transform(queryarticle)","38f49092":"from sklearn.metrics.pairwise import cosine_similarity\n\nsimilarity_matrix  = cosine_similarity(encQueries,encArticles)\n","30b2d3cc":"print(np.sort(similarity_matrix[1])[-5:][::-1]) #sorting to get the most similar articles for a given query\nnp.argsort(similarity_matrix[1])[-5:][::-1]","0c259fed":"import torch\nfrom transformers import  AutoTokenizer,AutoModelForQuestionAnswering","f2fee7d6":"tokenizer = AutoTokenizer.from_pretrained(\"ktrapeznikov\/biobert_v1.1_pubmed_squad_v2\")\n\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"ktrapeznikov\/biobert_v1.1_pubmed_squad_v2\")","fcce3cb1":"\ndef ask(question,context):\n  input_ids = tokenizer.encode(question, context)\n  sep_index = input_ids.index(tokenizer.sep_token_id)\n\n  num_seg_a = sep_index + 1\n\n  num_seg_b = len(input_ids) - num_seg_a\n  segment_ids = [0]*num_seg_a + [1]*num_seg_b\n  assert len(segment_ids) == len(input_ids)\n  tokens = tokenizer.convert_ids_to_tokens(input_ids)\n\n\n  start_scores, end_scores = model(torch.tensor([input_ids]),\n                                 token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n  answer_end = 0\n  answer_start = torch.argmax(start_scores)\n  answer_ends = torch.argsort(end_scores).numpy()[::-1]\n  for i in answer_ends[0]:\n    if answer_start<= i:\n      answer_end= i\n\n  answer = ' '.join(tokens[answer_start:answer_end+1])\n  answer = answer.replace(\" ##\",\"\").replace(\"[CLS] \",\"\")\n\n  pack = [answer,answer_start,answer_end,torch.max(start_scores),end_scores[0][answer_end],(torch.max(start_scores)+end_scores[0][answer_end]),context]\n  return pack","c9423a63":"from IPython.display import display, HTML","c817382d":"\"\"\"\nthis function is used to visualize the answers with their contexts \n\"\"\"\n\ndef highlightTextInContext(answer, context):\n    if \"?\"  in answer:\n        answer =\" \".join(answer[answer.index(\"?\")+1:].split(\" \"))\n    \n    antokens = word_tokenize(answer)\n    cotokens = word_tokenize(context)\n    startword= \"\"\n    startindex= \"\"\n    for i,w in enumerate(antokens):\n        for c in cotokens:\n            if c==w:\n                startword = c \n                selectedText = context[context.index(w):context.index(antokens[-1])+len(antokens[-1])]\n                highlighted = f'<span style=\"color: green; font-weight: bold\">{selectedText}<\/span>'\n                return context.replace(selectedText,highlighted)\n                # return is an easy way to break two nested loops\ndef showTopAnswers(answers):\n        for i in np.argsort(answers[:,5])[-8:][::-1]:\n            display(HTML(\"<p>\"+highlightTextInContext(answers[i,0],answers[i,6])+\"<\/p>\"))","3acdffc9":"def getanswers(question):\n  recommendations = []\n  for i in range(len(usequeries)):\n    indecies = np.argsort(similarity_matrix[i])[-7:][::-1] ## I choose to show N recommended queries from every query\n    for t in indecies:\n        recommendations.append(word_tokenize(df.abstract[t]))\n  \n  processedQuestion =   \" \".join([snowstem.stem(i) for i in word_tokenize(removepunc(question)) if i not in stops])\n  vector = vectorizer.transform([processedQuestion])\n  questionSimilarityMatrix = cosine_similarity(vector,encArticles)\n  indecies = np.argsort(questionSimilarityMatrix[0])[-7:][::-1] \n  for t in indecies:\n    recommendations.append(word_tokenize(df.abstract[t]))\n          \n  questions= []\n  contexts= []\n  for bigcontext in recommendations:\n    for i in range(int(len(bigcontext)\/60)):\n      contexts.append(\" \".join(bigcontext[i*60:60*(i+1)]))\n      questions.append(question)\n\n  answers = []\n  for  question, context in zip(questions,contexts):\n    result = ask(question,context)\n    if len(result[0]) < 7 and \"[CLS]\" in result[0] :\n      continue\n    answers.append(result)\n  answers = np.array(answers)\n\n  return answers","b9e23d33":"answers = getanswers(\"are pregnant women at risk ?\")\nshowTopAnswers(answers)","a0a33f58":"answers = getanswers(\"what are the risk factors ?\")\nshowTopAnswers(answers)","858a925e":"answers = getanswers(\"how will the virus affect neonates ?\")\nshowTopAnswers(answers)","903c0fd3":"answers = getanswers(\"are infected diabetic patients at risk?\")\nshowTopAnswers(answers)","20e3dc44":"answers = getanswers(\"how will hypertension affect patients?\")\nshowTopAnswers(answers)","ec8fb587":"## The method\n* define some queries related to the risk factors\n* stemming and removing stop words for reducing vocab size\n* use TFIDF to vectorize the documents and cosine simialrity as a measure to get the most relevant articles (fast information retrieval )\n* use BioBERT to get answers to questions related to risk factors","963393b8":"Keeping only the articles with \"covid\" in them\n ","cc9740d6":"Biobert is bert pretrained on  more medical text","5b331542":"BERT provides a score with every answer so answers can be sorted and filtered throw their scores"}}