{"cell_type":{"03fa756f":"code","d0f3218a":"code","b7bd0f52":"code","e2a46ff2":"code","32e0a810":"code","9ed62d9b":"code","591ffca6":"code","0e433a88":"code","cee6bd72":"code","2b005b79":"code","575aa6c2":"code","7a85e707":"code","a6938b8a":"code","d3ca3b91":"code","6839c059":"code","1cd1c56b":"code","6c128242":"code","5fd71c44":"code","309df664":"code","899887f2":"code","ec83ee22":"code","51847da2":"code","2ac20d5c":"markdown","bd4f7b78":"markdown","4afe35a2":"markdown","1f625f4d":"markdown","5c257883":"markdown","40b0857c":"markdown","0763c0e1":"markdown","3ffe01a7":"markdown","743a1d06":"markdown","df33b676":"markdown","b97c6e8a":"markdown","44ce6a5c":"markdown"},"source":{"03fa756f":"import os\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score\n\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model, Sequential\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\nprint(tf.__version__)","d0f3218a":"# # Detect TPU, return appropriate distribution strategy\n# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n#     print('Running on TPU ', tpu.master())\n# except ValueError:\n#     tpu = None\n\n# if tpu:\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# else:\n#     strategy = tf.distribute.get_strategy() \n\n# print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\n# BATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync","b7bd0f52":"device_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","e2a46ff2":"BATCH_SIZE = 8\nEPOCHS = 20\nIMAGE_SIZE = (512, 512)\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntf.random.set_seed(10)","32e0a810":"for dirname, _, filenames in os.walk('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/'):\n    print(dirname)","9ed62d9b":"filenames = tf.io.gfile.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train\/*\/*')\n# filenames.extend(tf.io.gfile.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/val\/*\/*'))\nfilenames.extend(tf.io.gfile.glob('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/test\/*\/*'))\n\nfilenames[:3]","591ffca6":"data = pd.DataFrame()\nfor el in range(0, len(filenames)):\n    target = filenames[el].split('\/')[-2]\n    path = filenames[el]\n    data.loc[el, 'filename'] = path\n    data.loc[el, 'class'] = target\n\nprint(data['class'].value_counts(dropna=False))\ndata[:10]","0e433a88":"data = shuffle(data, random_state=42)\ndata.reset_index(drop=True, inplace=True)\ndata[:10]","cee6bd72":"indexes=[]\n\ndef func(x):\n    if x[-5:] != '.jpeg':\n        idx = data[data['filename'] == x].index\n        indexes.append(idx[0])\n        print(idx[0], x)\n    return x\n\ndata['filename'].map(func)\n\nprint(data.shape)\ndata.drop(index=indexes, axis=0, inplace=True)\ndata.reset_index(drop=True, inplace=True)\nprint(data.shape)","2b005b79":"train_data, test_data = train_test_split(data, test_size=0.1, random_state=42, stratify=data['class'])\nprint(test_data.shape)\ntest_data = test_data[ : test_data.shape[0] \/\/ BATCH_SIZE * BATCH_SIZE]\nprint(test_data.shape)\nprint(train_data['class'].value_counts(dropna=False))\nprint(test_data['class'].value_counts(dropna=False))","575aa6c2":"train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42, stratify=train_data['class'])\nprint(train_data['class'].value_counts(dropna=False))\nprint(val_data['class'].value_counts(dropna=False))","7a85e707":"# Declare an image generator for image augmentation\ndatagen = ImageDataGenerator(rescale = 1.\/255,\n                             zoom_range=0.1, # 0.05\n                             brightness_range=[0.9, 1.0],\n                             height_shift_range=0.05, \n                             width_shift_range=0.05,\n                             rotation_range=10,  # 5\n                            )\n\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntrain_gen = datagen.flow_from_dataframe(train_data,\n                                        target_size=(IMAGE_SIZE),\n                                        color_mode='grayscale',\n                                        batch_size=BATCH_SIZE,\n                                        class_mode='binary',\n                                        shuffle=True,\n                                        num_parallel_calls=AUTOTUNE)\n\nval_gen = test_datagen.flow_from_dataframe(val_data,\n                                        target_size=(IMAGE_SIZE),\n                                        color_mode='grayscale',\n                                        batch_size=BATCH_SIZE,\n                                        class_mode='binary',\n                                        shuffle=False,\n                                        num_parallel_calls=AUTOTUNE)\n\ntest_gen = test_datagen.flow_from_dataframe(test_data,\n                                        target_size=(IMAGE_SIZE),\n                                        color_mode='grayscale',\n                                        batch_size=BATCH_SIZE,\n                                        class_mode='binary',\n                                        shuffle=False,\n                                        num_parallel_calls=AUTOTUNE)","a6938b8a":"train_ds = tf.data.Dataset.from_generator(lambda: train_gen,\n                                            output_types=(tf.float32, tf.int32),\n                                            output_shapes=([None, *IMAGE_SIZE, 1], [None, ]))\n\nval_ds = tf.data.Dataset.from_generator(lambda: val_gen,\n                                          output_types=(tf.float32, tf.int32),\n                                          output_shapes=([None, *IMAGE_SIZE, 1], [None, ]))\n\ntest_ds = tf.data.Dataset.from_generator(lambda: test_gen,\n                                           output_types=(tf.float32, tf.int32),\n                                           output_shapes=([None, *IMAGE_SIZE, 1], [None, ]))","d3ca3b91":"plt.figure(figsize=(10,10))\nfor i in range(BATCH_SIZE):\n    plt.subplot(BATCH_SIZE\/\/4 ,4,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_gen[0][0][i], cmap=plt.cm.binary)\nplt.show()","6839c059":"# Define the CNN Keras model\ndef create_model():\n    \n    with tf.device('\/gpu:0'):\n#     with strategy.scope():\n    \n        # Model input\n        input_layer = layers.Input(shape=(*IMAGE_SIZE, 1), name='input')  \n        \n        # First block\n        x = layers.Conv2D(filters=128, kernel_size=3, \n                          activation='relu', padding='same', \n                          name='conv2d_1')(input_layer)\n        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_1')(x)\n\n        # Second block\n        x = layers.Conv2D(filters=128, kernel_size=3, \n                          activation='relu', padding='same', \n                          name='conv2d_2')(x)\n        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_2')(x)\n\n        # Third block\n        x = layers.Conv2D(filters=128, kernel_size=3, \n                          activation='relu', padding='same', \n                          name='conv2d_3')(x)\n        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_3')(x)\n\n        # Fourth block\n        x = layers.Conv2D(filters=256, kernel_size=3, \n                          activation='relu', padding='same', \n                          name='conv2d_4')(x)\n        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_4')(x)\n\n        # Fifth block\n        x = layers.Conv2D(filters=256, kernel_size=3, \n                          activation='relu', padding='same', \n                          name='conv2d_5')(x)\n        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_5')(x)\n\n        # Sixth block\n        x = layers.Conv2D(filters=512, kernel_size=3, \n                          activation='relu', padding='same', \n                          name='conv2d_6')(x)\n        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_6')(x)\n\n        # Seventh block\n        x = layers.Conv2D(filters=512, kernel_size=3, \n                          activation='relu', padding='same', \n                          name='conv2d_7')(x)\n        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_7')(x)\n        \n        # Eighth block\n        x = layers.Conv2D(filters=512, kernel_size=3, \n                          activation='relu', padding='same', \n                          name='conv2d_8')(x)\n        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_8')(x)\n        \n#         # Ninth block\n#         x = layers.Conv2D(filters=512, kernel_size=3, \n#                           activation='relu', padding='same', \n#                           name='conv2d_9')(x)\n#         x = layers.MaxPool2D(pool_size=2, name='maxpool2d_9')(x)\n\n        # GlobalAveragePooling\n        x = layers.GlobalAveragePooling2D(name='global_average_pooling2d')(x)   \n        x = layers.Flatten()(x)\n        \n        # Head\n#         x = layers.BatchNormalization()(x)\n#         x = layers.Dropout(0.1, name='dropout_h1')(x)\n        \n#         x = layers.Dense(1024,activation='relu')(x)\n#         x = layers.BatchNormalization()(x)\n#         x = layers.Dropout(0.3, name='dropout_head_2')(x)\n        \n#         x = layers.Dense(128,activation='relu')(x)\n\n        \n        # Output\n        output = layers.Dense(units=1, \n                              activation='sigmoid', \n                              name='output')(x)\n\n\n        model = Model (input_layer, output)    \n        model.compile(optimizer='adam', \n                      loss='binary_crossentropy', \n                      metrics=['accuracy'])\n\n    return model\n\nmodel = create_model()\nmodel.summary()","1cd1c56b":"def feed_data(dataset):\n    return dataset.prefetch(buffer_size=AUTOTUNE)  ","6c128242":"init_time = datetime.datetime.now()\n\n\nSTEPS_PER_EPOCH = train_gen.samples \/\/ BATCH_SIZE\nVALIDATION_STEPS = val_gen.samples \/\/ BATCH_SIZE\n\nearly_stopping = EarlyStopping(monitor=\"val_accuracy\", patience=5, mode=\"max\")\ncheckpoint = ModelCheckpoint(\"acc-{val_accuracy:.2f}.h5\", monitor=\"val_accuracy\", verbose=1, \n                             save_best_only=True, save_weights_only=True, mode=\"max\")\nlearning_rate_reduction = ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.3, patience=1, \n                                            min_lr=1e-6, verbose=1, mode=\"max\")\n\n\nhistory = model.fit(\n    feed_data(train_ds),\n    validation_data=feed_data(val_ds),\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_steps = VALIDATION_STEPS,\n    callbacks=[checkpoint, early_stopping, learning_rate_reduction],\n    verbose=1,\n    )\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[0:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));\n\n\n\nrequared_time = datetime.datetime.now() - init_time\nprint(f'\\nRequired time:  {str(requared_time)}\\n')","5fd71c44":"test_steps = test_gen.samples \/\/ BATCH_SIZE\n\ntest_loss, test_acc = model.evaluate(test_ds, steps=test_steps)\nprint('\\naccuracy:', test_acc, 'loss: ',test_loss)","309df664":"predict = model.predict(test_ds, steps=test_steps)\ny_hat = np.where(predict > 0.5, 1, 0).ravel()\ny_hat[:20]","899887f2":"test_labels_df = pd.DataFrame()\ntest_labels_df[['class']] = test_data[['class']]\ntest_labels_df['class'] = test_labels_df['class'].map({'NORMAL':0, 'PNEUMONIA':1})\n\ny_test = np.array(test_labels_df['class'])\ny_test[:20]","ec83ee22":"print(classification_report(y_test, y_hat), '\\n')\ncm = confusion_matrix(y_test, y_hat)\nsns.heatmap(cm, annot=True, cmap=\"Blues\", fmt='.0f', cbar=False)","51847da2":"model.save('\/kaggle\/working\/model\/')","2ac20d5c":"# Validation Data","bd4f7b78":"# All filenames","4afe35a2":"# Define ImageDataGenerator and Augmentation(for train only!)","1f625f4d":"# Augmentated data","5c257883":"# Pneumonia detection with CNN","40b0857c":"# Classification report","0763c0e1":"# Drop out trash","3ffe01a7":"# Thank you for your time and curiosity ;)","743a1d06":"# To DataFrame","df33b676":"# SPLIT train_data, val_data","b97c6e8a":"# Shuffle Data","44ce6a5c":"# SPLIT train_data, test_data"}}