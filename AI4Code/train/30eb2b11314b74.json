{"cell_type":{"7e3b0599":"code","dcff929c":"code","9fa4b4b5":"code","d96c3fb4":"code","954e7040":"code","375d2033":"code","62d4535c":"code","824f83e9":"code","fda3e4ed":"code","87887af1":"code","45e071ef":"code","bf4e8cf8":"code","a4a18b1a":"code","f404cde3":"code","bee093d7":"code","4f2623f3":"code","f99b4e06":"code","494ea583":"code","73cd2d30":"code","93fda7d4":"code","fcf18da9":"code","9122a9af":"code","1c58026b":"code","62e698e0":"code","a502c43a":"code","d80b6da5":"code","2d42ff42":"code","80c06c9e":"code","7adbb5be":"code","6795f920":"code","183da5aa":"code","5a24fdfb":"code","155fe971":"code","ee91712b":"markdown","62b3f856":"markdown","c4747740":"markdown"},"source":{"7e3b0599":"import tensorflow as tf\nimport numpy as np\nimport os\nimport random\nimport pandas as pd\nimport seaborn as sns\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\nplt.rc('font', size=16)\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings\nwarnings.filterwarnings('ignore')\ntf.get_logger().setLevel('ERROR')\n\ntfk = tf.keras\ntfkl = tf.keras.layers\nprint(tf.__version__)\n","dcff929c":"# Random seed for reproducibility\nseed = 42\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)","9fa4b4b5":"dataset = pd.read_csv('..\/input\/secondchallenge\/Training.csv')\nprint(dataset.shape)\ndataset.head()","d96c3fb4":"dataset.columns = [c.replace(' ', '_') for c in dataset.columns]\ndataset.info()","954e7040":"def inspect_dataframe(df, columns):\n    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(30,30))\n    for i, col in enumerate(columns):\n        axs[i].plot(df[col])\n        axs[i].set_title(col)\n    plt.show()\n    \ninspect_dataframe(dataset, dataset.columns)","375d2033":"test_size = 2528\nX_train_raw = dataset.iloc[:-test_size]\n# y_train_raw = y.iloc[:-test_size]\nX_test_raw = dataset.iloc[-test_size:]\n# y_test_raw = y.iloc[-test_size:]\nprint(X_train_raw.shape, X_test_raw.shape)\n\n# Normalize both features and labels\nX_min = X_train_raw.min()\nX_max = X_train_raw.max()\n\nX_train_raw = (X_train_raw-X_min)\/(X_max-X_min)\nX_test_raw = (X_test_raw-X_min)\/(X_max-X_min)\n\nplt.figure(figsize=(17,5))\nplt.plot(X_train_raw.Sponginess, label='Train (Sponginess)')\nplt.plot(X_test_raw.Sponginess, label='Test (Sponginess)')\nplt.title('Train-Test Split')\nplt.legend()\nplt.show()","62d4535c":"plt.figure(figsize=(17,5))\nplt.plot(X_train_raw.Hype_root, label='Train (Sponginess)')\nplt.plot(X_test_raw.Hype_root, label='Test (Sponginess)')\nplt.title('Train-Test Split')\nplt.legend()\nplt.show()","824f83e9":"window = 108\nstride = 9","fda3e4ed":"future = dataset[-window:]\nfuture = (future-X_min)\/(X_max-X_min)\nfuture = np.expand_dims(future, axis=0)\nfuture.shape","87887af1":"def build_sequences(df, target_labels=['Sponginess'], window=window, stride=stride, telescope=108):\n    # Sanity check to avoid runtime errors\n    assert window % stride == 0\n    dataset = []\n    labels = []\n    temp_df = df.copy().values\n    temp_label = df[target_labels].copy().values\n    padding_len = len(df)%window\n\n    if(padding_len != 0):\n        # Compute padding length\n        padding_len = window - len(df)%window\n        padding = np.zeros((padding_len,temp_df.shape[1]), dtype='float64')\n        temp_df = np.concatenate((padding,df))\n        padding = np.zeros((padding_len,temp_label.shape[1]), dtype='float64')\n        temp_label = np.concatenate((padding,temp_label))\n        assert len(temp_df) % window == 0\n\n    for idx in np.arange(0,len(temp_df)-window-telescope,stride):\n        dataset.append(temp_df[idx:idx+window])\n        labels.append(temp_label[idx+window:idx+window+telescope])\n\n    dataset = np.array(dataset)\n    labels = np.array(labels)\n    return dataset, labels","45e071ef":"def inspect_multivariate(X, y, columns, telescope, idx=None):\n    if(idx==None):\n        idx=np.random.randint(0,len(X))\n\n    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17,17))\n    for i, col in enumerate(columns):\n        axs[i].plot(np.arange(len(X[0,:,i])), X[idx,:,i])\n        axs[i].scatter(np.arange(len(X[0,:,i]), len(X_train[0,:,i])+telescope), y[idx,:,i], color='orange')\n        axs[i].set_title(col)\n        axs[i].set_ylim(0,1)\n    plt.show()","bf4e8cf8":"target_labels = dataset.columns\ntelescope = 108","a4a18b1a":"X_train, y_train = build_sequences(X_train_raw, target_labels, window, stride, telescope)\nX_test, y_test = build_sequences(X_test_raw, target_labels, window, stride, telescope)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","f404cde3":"inspect_multivariate(X_train, y_train, target_labels, telescope)","bee093d7":"input_shape = X_train.shape[1:]\noutput_shape = y_train.shape[1:]\nbatch_size = 64\nepochs = 300","4f2623f3":"def build_CONV_LSTM_model(input_shape,output_shape):\n\n    # Build the neural network layer by laver\n    input_layer = tfkl.Input (shape=input_shape, name='Input')\n    \n    lstm=tfkl.Bidirectional(tfkl.LSTM(128,return_sequences=True))(input_layer)\n    lstm=tfkl.Bidirectional(tfkl.LSTM(128,return_sequences=True))(lstm)\n    lstm = tfkl.Conv1D (512, 5, padding='same', activation='relu') (lstm)\n    lstm = tfkl.MaxPool1D()(lstm)\n    \n    lstm = tfkl.Bidirectional(tfkl.LSTM(128,return_sequences=True))(lstm)\n    lstm = tfkl.Bidirectional(tfkl.LSTM(128,return_sequences=True))(lstm)\n    lstm = tfkl.Conv1D (512, 5, padding='same', activation='relu') (lstm)\n    lstm = tfkl.MaxPool1D()(lstm)\n    \n    lstm = tfkl.GlobalAveragePooling1D() (lstm)\n    lstm = tfkl.Dropout(.5) (lstm)\n    \n    # In order to predict the next values for more than one channel,\n    # we can use a Dense layer with a number given by telescope*num_channels,\n    # followed by a Reshape layer to obtain a tensor of dimension \n    # [None, telescope, num_channels]\n    dense = tfkl.Dense(output_shape[-1]*output_shape[-2], activation='relu')(lstm)\n    output_layer = tfkl.Reshape((output_shape[-2],output_shape[-1]))(dense)\n    output_layer = tfkl.Conv1D(output_shape[-1], 1, padding='same')(output_layer)\n    \n    # Connect input and output through the Model class\n    model = tfk.Model (inputs=input_layer, outputs=output_layer, name='model')\n    \n    # Compile the model\n    model.compile(loss=tfk.losses.MeanSquaredError(),optimizer=tfk.optimizers.Adam(),metrics=['mae'])\n    \n    # Return the model\n    return model","f99b4e06":"model = build_CONV_LSTM_model(input_shape, output_shape)\nmodel.summary()\ntfk.utils.plot_model(model, expand_nested=True)","494ea583":"# Train the model\nhistory = model.fit(\n    x = X_train,\n    y = y_train,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_split=.1,\n    callbacks = [\n        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True),\n        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-5)\n    ]\n).history","73cd2d30":"best_epoch = np.argmin(history['val_loss'])\nplt.figure(figsize=(17,4))\nplt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Squared Error (Loss)')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(17,4))\nplt.plot(history['mae'], label='Training accuracy', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_mae'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Absolute Error')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(18,3))\nplt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()","93fda7d4":"model.save('telescope_108')\n#model = tfk.models.load_model('AuroregressiveForecasting')","fcf18da9":"import shutil\nsaved_model = \".\/telescope_108\"\nshutil.make_archive(\"telescope_108\", 'zip', saved_model)","9122a9af":"# Predict the test set \npredictions = model.predict(X_test)\nprint(predictions.shape)\n\nmean_squared_error = tfk.metrics.mse(y_test.flatten(),predictions.flatten())\nmean_absolute_error = tfk.metrics.mae(y_test.flatten(),predictions.flatten())\nmean_squared_error, mean_absolute_error","1c58026b":"def inspect_multivariate_prediction(X, y, pred, columns, telescope, idx=None):\n    if(idx==None):\n        idx=np.random.randint(0,len(X))\n\n    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17,17))\n    for i, col in enumerate(columns):\n        axs[i].plot(np.arange(len(X[0,:,i])), X[idx,:,i])\n        axs[i].plot(np.arange(len(X[0,:,i]), len(X_train[0,:,i])+telescope), y[idx,:,i], color='orange')\n        axs[i].plot(np.arange(len(X[0,:,i]), len(X_train[0,:,i])+telescope), pred[idx,:,i], color='green')\n        axs[i].set_title(col)\n        axs[i].set_ylim(0,1)\n    plt.show()","62e698e0":"inspect_multivariate_prediction(X_test, y_test, predictions, target_labels, telescope)","a502c43a":"reg_telescope = 864\nX_test_reg, y_test_reg = build_sequences(X_test_raw, target_labels, window, stride, reg_telescope)\nX_test_reg.shape, y_test_reg.shape","d80b6da5":"# Autoregressive Forecasting\nreg_predictions = np.array([])\nX_temp = X_test_reg\nfor reg in range(0,reg_telescope,telescope):\n    pred_temp = model.predict(X_temp)\n    if(len(reg_predictions)==0):\n        reg_predictions = pred_temp\n    else:\n        reg_predictions = np.concatenate((reg_predictions,pred_temp),axis=1)\n        print(reg_predictions.shape)\n    X_temp = np.concatenate((X_temp[:,telescope:,:],pred_temp), axis=1)","2d42ff42":"reg_predictions.shape","80c06c9e":"mean_squared_error = tfk.metrics.mse(y_test_reg.flatten(),reg_predictions.flatten())\nmean_absolute_error = tfk.metrics.mae(y_test_reg.flatten(),reg_predictions.flatten())\nmean_squared_error, mean_absolute_error","7adbb5be":"inspect_multivariate_prediction(X_test_reg, y_test_reg, reg_predictions, target_labels, reg_telescope)","6795f920":"maes = []\nfor i in range(reg_predictions.shape[1]):\n    ft_maes = []\n    for j in range(reg_predictions.shape[2]):\n        ft_maes.append(np.mean(np.abs(y_test_reg[:,i,j]-reg_predictions[:,i,j]), axis=0))\n    ft_maes = np.array(ft_maes)\n    maes.append(ft_maes)\nmaes = np.array(maes)","183da5aa":"reg_future = np.array([])\nX_temp = future\nfor reg in range(int(reg_telescope\/telescope)):\n    pred_temp = model.predict(X_temp)\n    if(len(reg_future)==0):\n        reg_future = pred_temp\n    else:\n        reg_future = np.concatenate((reg_future,pred_temp),axis=1)\n    X_temp = np.concatenate((X_temp[:,108:,:],pred_temp), axis=1)","5a24fdfb":"reg_future.shape","155fe971":"figs, axs = plt.subplots(len(target_labels), 1, sharex=True, figsize=(17,17))\nfor i, col in enumerate(target_labels):\n    axs[i].plot(np.arange(len(future[0,:,i])), future[0,:,i])\n    axs[i].plot(np.arange(len(future[0,:,i]), len(future[0,:,i])+reg_telescope), reg_future[0,:,i], color='orange')\n    axs[i].fill_between(\n        np.arange(len(future[0,:,i]), len(future[0,:,i])+reg_telescope), \n        reg_future[0,:,i]+maes[:,i], \n        reg_future[0,:,i]-maes[:,i], \n        color='orange', alpha=.3)\n    axs[i].set_title(col)\n    axs[i].set_ylim(0,1)\nplt.show()","ee91712b":"Multivariate Forecating (Autoregression)","62b3f856":"Predict the future:","c4747740":"Sequential Train-Test split and normalization"}}