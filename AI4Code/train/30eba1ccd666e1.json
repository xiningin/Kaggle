{"cell_type":{"3def2d79":"code","a74c5397":"code","3c2001c6":"code","74eff251":"code","dbced26c":"code","8a27472d":"code","e4b3a42a":"code","93db8f94":"code","4ab5ba6f":"code","be863417":"code","461710b3":"code","5c3c6f0a":"code","ff877102":"markdown"},"source":{"3def2d79":"import pandas as pd\nimport numpy as np\n\nfrom catboost import CatBoostClassifier\nimport category_encoders as ce\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score","a74c5397":"path = \"..\/input\/tabular-playground-series-apr-2021\/\"\ntrain = pd.read_csv(path+'train.csv', index_col=0)\ntest = pd.read_csv(path+'test.csv', index_col=0)\npseudo_label = pd.read_csv('..\/input\/tps-apr-2021-pseudo-labeling-voting-ensemble\/voting_submission.csv', index_col=0)\nsubmission = pd.read_csv(path+'sample_submission.csv')","3c2001c6":"# pseudo-label from https:\/\/www.kaggle.com\/hiro5299834\/tps-apr-2021-voting-pseudo-labeling\ntest['Survived'] = [x for x in pseudo_label.Survived]","74eff251":"# Calcule SameFirstName\n\ntrain['FirstName'] = train['Name'].apply(lambda x:x.split(', ')[0])\ntrain['n'] = 1\ngb = train.groupby('FirstName')\ndf_names = gb['n'].sum()\ntrain['SameFirstName'] = train['FirstName'].apply(lambda x:df_names[x])\n\ntest['FirstName'] = test['Name'].apply(lambda x:x.split(', ')[0])\ntest['n'] = 1\ngb = test.groupby('FirstName')\ndf_names = gb['n'].sum()\ntest['SameFirstName'] = test['FirstName'].apply(lambda x:df_names[x])\n\n# To preprocess\n\ndata = pd.concat([train, test], axis=0)\n\n# Before fill missing\ndata['AnyMissing'] = np.where(data.isnull().any(axis=1) == True, 1, 0)\n\n# Family\ndata['FamilySize'] = data['SibSp'] + data['Parch'] + 1\ndata['IsAlone'] = np.where(data['FamilySize'] <= 1, 1, 0)\n\n# Cabin\ndata['Has_Cabin'] = data[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ndata['Cabin'] = data['Cabin'].fillna('X').map(lambda x: x[0].strip())\ncabin_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5,\n             'F': 6, 'G': 7, 'T': 1, 'X': 8}\ndata['Cabin'] = data['Cabin'].str[0].fillna('X').replace(cabin_map)\n\n# Embarked\n#map_Embarked = train.Embarked.mode().item()\ndata['Embarked'] = data['Embarked'].fillna(\"No\")\nconditions = [\n    (data['Embarked']==\"S\"),\n    (data['Embarked']==\"Q\"),\n    (data['Embarked']==\"C\"),\n    (data['Embarked']==\"No\")\n]\nchoices = [0, 1, 2, -1]\ndata[\"Embarked\"] = np.select(conditions, choices)\ndata['Embarked'] = data['Embarked'].astype(int)\n\n# Name\ndata['SecondName'] = data.Name.str.split(', ', 1, expand=True)[1] # to try\ndata['IsFirstNameDublicated'] = np.where(data.FirstName.duplicated(), 1, 0)\n\n# Fare\ndata['Fare'] = data['Fare'].fillna(train['Fare'].median())\n# train['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n# [(0.679, 10.04] < (10.04, 24.46] < (24.46, 33.5] < (33.5, 744.66]]\n# From original Titanic:\nconditions = [\n    (data['Fare'] <= 7.91),\n    ((data['Fare'] > 7.91) & (data['Fare'] <= 14.454)),\n    ((data['Fare'] > 14.454) & (data['Fare'] <= 31)),\n    (data['Fare'] > 31)\n]\n\nchoices = [0, 1, 2, 3]\ndata[\"Fare\"] = np.select(conditions, choices)\ndata['Fare'] = data['Fare'].astype(int)\n\n# Fix Ticket\n# data['TicketNum'] = data.Ticket.str.extract(r'(\\d+)').\\\n#                     astype('float64', copy=False) # to_try\ndata['Ticket'] = data.Ticket.str.replace('\\.','', regex=True).\\\n                    str.replace('(\\d+)', '', regex=True).\\\n                    str.replace(' ', '', regex=True).\\\n                    replace(r'^\\s*$', 'X', regex=True).\\\n                    fillna('X')\n\n#data['Ticket'] = data['Ticket'].astype('category').cat.codes # to_try\n\n# Age \nconditions = [\n    ((data.Sex==\"female\")&(data.Pclass==1)&(data.Age.isnull())),\n    ((data.Sex==\"male\")&(data.Pclass==1)&(data.Age.isnull())),\n    ((data.Sex==\"female\")&(data.Pclass==2)&(data.Age.isnull())),\n    ((data.Sex==\"male\")&(data.Pclass==2)&(data.Age.isnull())),\n    ((data.Sex==\"female\")&(data.Pclass==3)&(data.Age.isnull())),\n    ((data.Sex==\"male\")&(data.Pclass==3)&(data.Age.isnull()))\n]\nchoices = data[['Age', 'Pclass', 'Sex']].\\\n            dropna().\\\n            groupby(['Pclass', 'Sex']).\\\n            mean()['Age']\n\ndata[\"Age\"] = np.select(conditions, choices)\n\nconditions = [\n    (data['Age'].le(16)),\n    (data['Age'].gt(16) & data['Age'].le(32)),\n    (data['Age'].gt(32) & data['Age'].le(48)),\n    (data['Age'].gt(48) & data['Age'].le(64)),\n    (data['Age'].gt(64))\n]\nchoices = [0, 1, 2, 3, 4]\n\ndata[\"Age\"] = np.select(conditions, choices)\n\n# Sex\ndata['Sex'] = np.where(data['Sex']=='male', 1, 0)\n\n# Drop columns\ndata = data.drop(['Name', 'n'], axis = 1)\n\n# Transform object to category\n#for col in data.columns[data.dtypes=='object'].tolist():\n#    data.loc[:,col] = data.loc[:,col].astype('category')","dbced26c":"# Splitting into train and test\ntrain = data.iloc[:train.shape[0]]\ntest = data.iloc[train.shape[0]:].drop(columns=['Survived'])","8a27472d":"train.head(3)","e4b3a42a":"lab_cols = ['Pclass','Age', 'Ticket', 'Fare', 'Cabin', 'Embarked']\ntarget = 'Survived'\n\nfeatures_selected = ['Pclass', 'Sex', 'Age','Embarked','Parch','SibSp','Fare','Cabin','Ticket','SameFirstName']\n\nX = data.drop(target, axis=1)\nX = X[features_selected]\ny = data[target]\n\ntest = test[features_selected]","93db8f94":"def kfold_prediction(X, y, X_test, K, od_wait = 500):\n\n    yp = pd.DataFrame()\n    trs = []\n    acc_trs = []\n    \n    kf = StratifiedKFold(n_splits=K, shuffle=True, random_state=314)\n    \n    for i, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n        print(f\"\\n FOLD {i} ...\")\n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[test_idx]\n        y_val = y.iloc[test_idx]\n        \n        # https:\/\/catboost.ai\/docs\/concepts\/parameter-tuning.html\n        params = {'iterations': 10000,\n                  'use_best_model':True ,\n                  'eval_metric': 'AUC', # 'Accuracy'\n                  'loss_function':'Logloss',\n                  'od_type':'Iter',\n                  'od_wait':od_wait,\n                  'depth': 6, # [4, 10]\n                  'l2_leaf_reg': 3,\n                  # random_strength ??\n                  'bootstrap_type': 'Bayesian',\n                  'bagging_temperature': 2,\n                  'max_bin': 254,\n                  'grow_policy': 'SymmetricTree',\n                  'cat_features': lab_cols,\n                  'verbose': od_wait,\n                  'random_seed': 314\n         }\n        \n        #params = {'loss_function':'Logloss',\n        #          'eval_metric': 'AUC', # 'Accuracy'\n        #          'od_wait':od_wait,\n        #          'od_type':'Iter', \n        #          'n_estimators': 10000,\n        #          'cat_features': lab_cols,\n        #          'verbose': od_wait,\n        #          'random_seed': 314\n        # }\n        \n        clf = CatBoostClassifier(**params)\n        \n        model_fit = clf.fit(X_train,y_train,\n                            eval_set=[(X_train, y_train), (X_val, y_val)],\n                            use_best_model=True,\n                            plot=False)\n        \n        yp_val = model_fit.predict_proba(X_val)[:, 1]\n        acc = accuracy_score(y_val, np.where(yp_val>0.5, 1, 0))\n        print(f\"- Accuracy before : {acc} ...\")\n        \n        # Moving threshold\n        thresholds = np.arange(0.0, 1.0, 0.01)\n        accuracy_scores = []\n        for thresh in thresholds:\n            accuracy_scores.append(\n                accuracy_score(y_val, [1 if m>thresh else 0 for m in yp_val]))\n\n        accuracies = np.array(accuracy_scores)\n        max_accuracy = accuracies.max() \n        max_accuracy_threshold =  thresholds[accuracies.argmax()] \n        trs = trs + [max_accuracy_threshold]\n        \n        print(\"- Max accuracy threshold: \"+str(max_accuracy_threshold))\n        \n        acc = accuracy_score(y_val, \n                             np.where(yp_val>max_accuracy_threshold, 1, 0)) \n        acc_trs = acc_trs + [acc]\n        print(f\"- Accuracy after: {acc} !\")\n        \n        yp_test = model_fit.predict_proba(X_test)[:, 1]\n        yp_fold = pd.DataFrame({\n            'fold'+str(i): np.where(yp_test>max_accuracy_threshold, 1, 0)})\n        \n        yp = pd.concat([yp, yp_fold], axis=1)\n    \n    return yp, trs, acc_trs","4ab5ba6f":"yp, trs, acc = kfold_prediction(X, y, test, 5)","be863417":"print('Model with train + pseudo train')\nprint(\"Final mean and std accuracy: \", np.mean(acc), round(np.std(acc), 5))\nprint(\"Final mean and std accuracy with Threshold: \", np.mean(trs), round(np.std(trs), 5))","461710b3":"def vote(r, columns):\n    \"\"\"https:\/\/www.kaggle.com\/belov38\/catboost-lb\/\"\"\"\n    ones = 0\n    zeros = 0\n    for i in columns:\n        if r[i]==0:\n            zeros+=1\n        else:\n            ones+=1\n    if ones>zeros:\n        return 1\n    else:\n        return 0","5c3c6f0a":"submission_pseudo = pd.DataFrame({\n    'PassengerId': test.index,\n    'Survived':yp.apply(lambda x:vote(x, yp.columns.tolist()),axis=1)\n})\n\nsubmission_pseudo.to_csv('submission_pseudo_test.csv', index = False) # best 0.80398 LB","ff877102":"# Problem definition\n\nThe dataset is used for this competition is synthetic but based on a real dataset (in this case, the actual Titanic data!) and generated using a CTGAN.\n\nData description: \n\n| Variable        | Definition           | Key  |\n|---------------|:-------------|------:|\n|survival |\tSurvival | 0 = No, 1 = Yes |\n|pclass |\tTicket class | 1 = 1st, 2 = 2nd, 3 = 3rd |\n|sex |\tSex\t ||\n|Age |\tAge in years\t ||\n|sibsp |\t# of siblings \/ spouses aboard the Titanic\t ||\n|parch |\t# of parents \/ children aboard the Titanic\t ||\n|ticket |\tTicket number\t ||\n|fare |\tPassenger fare\t ||\n|cabin |\tCabin number\t| |\n|embarked |\tPort of Embarkation\t| C = Cherbourg, Q = Queenstown, S = Southampton |\n\n<br>\n\nWhere `survival` will be our target variable! \ud83c\udfaf\n\n<br>\n\nCheck out: \n\n  \u279c [TPS-Apr2021 EDA Profiling + RF Pipeline Baseline](https:\/\/www.kaggle.com\/gomes555\/tps-apr2021-eda-profiling-rf-pipeline-baseline)\n\n  \u279c [Tuning of a Lightgbm with Bayesian Optimization using the `tidymodels` framework in R](https:\/\/www.kaggle.com\/gomes555\/tps-apr2021-r-eda-lightgbm-bayesopt)\n\n  \u279c [AutoML (lgbm + catboost) with mljar](https:\/\/www.kaggle.com\/gomes555\/tps-apr2021-autoboost-mljar)\n  \n  \u279c [Feature Selection with RFE + Boruta](https:\/\/www.kaggle.com\/gomes555\/tps-apr2021-feature-selection-rfe-boruta)\n  \n  \u279c [Simple CatBoost + Preprocess](https:\/\/www.kaggle.com\/gomes555\/tps-apr2021-simple-catboost)\n  \n  \u279c [CatBoost + Pseudo + MovingThreshold](https:\/\/www.kaggle.com\/gomes555\/tps-apr2021-catboost-pseudo-movingthreshold)\n  \n  \u279c [Catboost + combination of techniques + Optuna](https:\/\/www.kaggle.com\/gomes555\/tps-apr2021-catboost-optuna)\n  \nStrongly inspired by:\n\n  \u279c [https:\/\/www.kaggle.com\/hiro5299834\/tps-apr-2021-voting-pseudo-labeling](https:\/\/www.kaggle.com\/hiro5299834\/tps-apr-2021-voting-pseudo-labeling)\n  \n  \u279c [https:\/\/www.kaggle.com\/belov38\/catboost-lb](https:\/\/www.kaggle.com\/belov38\/catboost-lb)\n  \n  \u279c [https:\/\/www.kaggle.com\/remekkinas\/ensemble-learning-meta-classifier-for-stacking\/output\n](https:\/\/www.kaggle.com\/remekkinas\/ensemble-learning-meta-classifier-for-stacking\/output)\n  \n<br>\n\n<p align=\"right\"><span style=\"color:firebrick\">Dont forget the upvote if you liked the notebook! \u270c\ufe0f <\/p>"}}