{"cell_type":{"97bdf3b5":"code","b9d70f57":"code","6a4a73fa":"code","9ba3d95d":"code","29238914":"code","ad24e1b4":"code","bd22b985":"code","5deb4622":"code","4697d940":"code","10c7521e":"code","499f5efe":"code","8c977b01":"code","bb735ffa":"code","c0c0ae40":"code","b89b4d34":"code","0b6795cb":"code","d57f65bf":"code","ad772544":"code","fb5a115b":"code","b9be39a7":"code","1eb8e8a3":"code","26496ea3":"code","ab502d87":"code","b7b9219c":"code","f6a6e556":"code","49fa9669":"code","af729562":"code","4be15541":"code","5ffd3330":"code","163ca2ea":"code","aaf7dbd1":"code","64c50cc8":"code","be9e9b57":"code","2e65ed87":"code","a05c6c30":"code","75d3ea09":"code","e0a4d5d4":"code","3e9f3010":"code","b082e532":"code","d657203f":"code","e55d2d9b":"code","80e9bb37":"code","11a9fd45":"code","18ab498b":"code","f6511699":"code","809e2870":"code","62a1a1ac":"code","86374327":"markdown","f0b35d59":"markdown","2660b8e1":"markdown","8bc1f5c0":"markdown","a633b9e6":"markdown","edcdf16a":"markdown","7625ad5d":"markdown","315ce535":"markdown","c90dd479":"markdown","272b32a9":"markdown","3bee8b56":"markdown","6736af83":"markdown","22bc63b9":"markdown","a3b2bac4":"markdown","b515a22a":"markdown","fab6e7cc":"markdown","e449ebab":"markdown","7078a223":"markdown","1786459b":"markdown","107486e8":"markdown","8cccb3e6":"markdown","84865a01":"markdown","a93c8077":"markdown","3cf252a9":"markdown","f34c091b":"markdown","d52e5449":"markdown","5b8bf876":"markdown","733dec30":"markdown","64f99255":"markdown","0f6d6acf":"markdown","ed68646d":"markdown","b0677437":"markdown","77f99522":"markdown","6762ec25":"markdown","b8e467d6":"markdown","716750ec":"markdown","8529ced0":"markdown","0be437da":"markdown","2450f68d":"markdown","6df6b62a":"markdown"},"source":{"97bdf3b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b9d70f57":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nprint(\"Data imported.\")","6a4a73fa":"train.head(3)","9ba3d95d":"train.shape","29238914":"print(train.isnull().sum())\nprint(test.isnull().sum())","ad24e1b4":"def data_preprocessing(train, test):\n    # divide in train and test\n    X_train = train.iloc[:,1:]\n    y_train = train['label']\n    X_test = test\n    # normalize the data\n    X_train = (X_train.astype('float').values)\/255\n    X_test = (X_test.astype('float').values)\/255\n    # convert in numpy array and reshape\n    X_train = X_train.reshape(-1,28,28,1)\n    X_test = X_test.reshape(-1,28,28,1)\n    # encode target variable \n    y_train = pd.get_dummies(y_train).values\n    return X_train, y_train, X_test\n  ","bd22b985":"X_train, y_train, X_test = data_preprocessing(train, test)","5deb4622":"X_train.shape","4697d940":"y_train.shape","10c7521e":"print(\"X_train shape:\", X_train.shape)\nprint(\"y_train shape :\", y_train.shape)\nprint(\"X_test shape:\", X_test.shape)","499f5efe":"import matplotlib.pyplot as plt","8c977b01":"def plot_images(X_train,y_train, n_images):\n    n_rows = int(np.ceil(n_images \/ 5))\n    fig , ax = plt.subplots(nrows = n_rows, ncols= 5)\n    r = -1\n    c = 0\n    for i in range(n_images):\n        if (i%5)==0:\n            r = r +1\n            c = 0\n        ax[r,c].imshow(X_train[i] ,cmap = plt.get_cmap('gray'))\n        ax[r,c].set_title(f'{list(y_train[i]).index(max(list(y_train[i])))}')\n        c = c +1\n    plt.tight_layout()\n    plt.show()\n","bb735ffa":"plot_images(X_train,y_train, 10)","c0c0ae40":"from sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, AveragePooling2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n","b89b4d34":"model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(AveragePooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(AveragePooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))","0b6795cb":"optimizer = RMSprop(learning_rate=0.001,rho=0.9,momentum=0.0,epsilon=1e-07,)\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","d57f65bf":"from keras.preprocessing.image import ImageDataGenerator","ad772544":"datagen = ImageDataGenerator(rotation_range = 4,\n                             width_shift_range=0.2,\n                             height_shift_range=0.2                             \n                         )","fb5a115b":"datagen.fit(X_train)","b9be39a7":"history  = model.fit(datagen.flow(X_train, y_train, batch_size=64),\n          steps_per_epoch=len(X_train) \/ 64, epochs = 4)","1eb8e8a3":"from sklearn.metrics import accuracy_score","26496ea3":"print(f\"Accuract score for CNN in keras is: {accuracy_score(np.argmax(model.predict(X_train),axis =1), np.argmax(y_train,axis =1))}\")","ab502d87":"history.history","b7b9219c":"fig , ax = plt.subplots(nrows = 2 , ncols = 1, figsize = (18,10))\n\nax[0].plot(range(1, len(history.history['loss'])+1) , history.history['loss'], label = 'Loss', color = 'red', marker = 's')\nax[0].legend(loc = 'best')\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel('Loss')\nax[0].set_title(\"Loss afor epochs\")\nax[1].plot(range(1, len(history.history['loss'])+1) , history.history['accuracy'], label = 'Accuracy', color = 'blue', marker = '*')\nax[1].legend(loc = 'best')\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel('Accuracy')\nax[1].set_title(\"Accuracy for epochs\")\nplt.show()","f6a6e556":"model.summary()","49fa9669":"plt.imshow(X_test[32])","af729562":"results = model.predict(X_test[32:33])\n","4be15541":"results","5ffd3330":"np.argmax(results)","163ca2ea":"layers = [layer.output for layer in  model.layers]","aaf7dbd1":"layers","64c50cc8":"from keras import models\nactivation_model = models.Model(inputs=model.input, outputs=layers)","be9e9b57":"activations = activation_model.predict(X_train[32:33])","2e65ed87":"len(activations)","a05c6c30":"layer_names = []\nfor layer in model.layers[:8]:\n    layer_names.append(layer.name) # Names of the layers, so you can have them as part of your plot\n    \nimages_per_row = 16\n\nfor layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n    n_features = layer_activation.shape[-1] # Number of features in the feature map\n    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n    n_cols = n_features \/\/ images_per_row # Tiles the activation channels in this matrix\n    display_grid = np.zeros((size * n_cols, images_per_row * size))\n    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n        for row in range(images_per_row):\n            channel_image = layer_activation[0,:, :,col * images_per_row + row]\n            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n            channel_image \/= channel_image.std()\n            channel_image *= 64\n            channel_image += 128\n            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n            display_grid[col * size : (col + 1) * size, # Displays the grid\n                         row * size : (row + 1) * size] = channel_image\n    scale = 1. \/ size\n    plt.figure(figsize=(scale * display_grid.shape[1],\n                        scale * display_grid.shape[0]))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n\n","75d3ea09":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nprint(\"Data imported\")","e0a4d5d4":"X_train = train.iloc[:,1:]\ny_train = train.iloc[:, 0]","3e9f3010":"from sklearn.preprocessing import OneHotEncoder","b082e532":"y_train_enc = OneHotEncoder().fit_transform(y_train.values.reshape(-1,1))","d657203f":"from sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder","e55d2d9b":"pipe_classificator = Pipeline([('sc', StandardScaler()),('pca', PCA(n_components = 3)), ('rfr', RandomForestClassifier())]) ","80e9bb37":"pipe_classificator.fit(X_train, y_train)","11a9fd45":"y_pred = pipe_classificator.predict(X_train)","18ab498b":"print(f\"Accurcy score for supervised model is {accuracy_score(y_train, y_pred)}\")","f6511699":"results = model.predict_classes(X_test,verbose = 0)","809e2870":"results = pd.Series(results, name ='Label')","62a1a1ac":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","86374327":"Digit recognition can also be treated as a supervised learning problem. We will use a very simple model made by a RandomForestClassifier.","f0b35d59":"Prediction is 4 and is correct.","2660b8e1":"A common thing to do especially is to augment the numbers of the images with ImageDataGenerator: that is by rotating , flipping, stretching the original images we can icrease the numbers of images of the dataset. However this has a downside: that for example the numbers 6 and 9: in this case we can not rotate the data because in this case we can generata confusion in the model between the two digits.","8bc1f5c0":"We can plot the first 10 images of out train dataset.","a633b9e6":"## 2 - understand how a CNN works by visually representing the layers of the model for a single digit prediction","edcdf16a":"We can define a function to take a look at the first n images","7625ad5d":"The goal of this notebook is threefold:\n\n1.  build a CNN model with keras\n1. understand how a CNN works by visually representing the layers of the model for a single digit prediction\n1.  develop a supervised model (a simple RandomForestRegressor) with PCA to predict digits\n","315ce535":"## Submitting predictions to competition\n","c90dd479":"We can evaluate the model with the data in history.hystory object which is a dictionary containing information about loss anche accuarcy for each epochs. ","272b32a9":"history.history dictionary has two keys, one for loss and the other one for accuaracy. We can plot loss and accuracy for each epochs.","3bee8b56":"After preprocessing we can see that if we take the shape of X_train we get (42000,28,28,1), that is 42000 rows representing 42000 digits with dimesion (28,28,1). The third value (1) means that digit has no color.","6736af83":"Importing data","22bc63b9":"A common thing to do is to increment the numbers of the filters in the model between the first and last layers. As we will see the first layers capture the whole digit whereas the last layers focus only on specific details of the digits.","a3b2bac4":"### Visualizing digits","b515a22a":"There are 42000 rows each one representing a digit. One column is the target variable, the others columns represent a pixel in the digit so every digit is represented by 784 pixel which means that every digit is  28 x 28 pixel image.Target variable is in the label field","fab6e7cc":"As we can see only in the first of the 32 layers is it possible to distinguish the number 4. As you progress through the layers it becomes more difficult to to distinguish the number 4 given that each kernel focus only on a detail of the digit. \n\nWhen reaching layers with 64 filters it becomes almost impossible to distinguish the digit ad data in the layers become more abstract.\n\n","e449ebab":"### Evaluate the model","7078a223":"We can extract activation layers for the prediction of digit 4 of the model.","1786459b":"###  Simple CCN model with keras","107486e8":"Digit at indice 32 is a 4:","8cccb3e6":"The AveragePooling2D is applied after the convolution and has a dimension (pool_size) of 2x2. This means that a matrix of 2x2 is applied to the output of the Conv2D filters and takes the average of the pixels covered by the matrix. This results in the reduction in size of the Conv2D layers. ","84865a01":"The layers of the model are the following:","a93c8077":"The shape of the data is:","3cf252a9":"# Digit recognizer - RandomForestRegressor vs CNN Keras","f34c091b":"Lets' take a look at training data:","d52e5449":"## 3 - develop a supervised model (a simple RandomForestRegressor) with PCA to predict digits\n","5b8bf876":"This is the summary of the model.","733dec30":"Another interesting think to notice it's that no all layers are activated. For example in the dropout_1 layer kernel number 63 and 62 are not active.","64f99255":"This part of the code is take from the book \"Deep learning with Python\" by Francois Chollet.","0f6d6acf":"len of activation is 12 equal to the number of layers of the model","ed68646d":"We define a sequential model in Keras. First two layers are Conv2D layers with 32 filters and a kernel size of (5,5). The first layer has another variable which is input shape that defines the shape of the inputs of the model. Kernel_size represent the dimension of the filters applied to the images. When appling filters to the image you multiply each value of the filter with the corresponding value of the image covered by the filter. The resulting values are then passed to the activation function ('relu' in this case) that generate a non linear output. At the beginning the values of the filters are set randomly and then are modified by the optimizer in order to reduce the loss.","b0677437":"We can define a function to preprocess the data:\n\n*  divide the data in train and test\n*  normalize the data \n* convert train and test in numpy array and reshape them to be dimension (28,28,1). When using a CNN model we need to reshape our data in (n_samples, image_width, image_heigh, channels). Channels need to be set to 1 for grey images like these ones and to 3 for coloured images. When defining a Conv2D layer we can define if channel is 'channel_first' or 'channel_last'. From the documentation in keras \" channels_last corresponds to inputs with shape (batch_size, height, width, channels) while channels_first corresponds to inputs with shape (batch_size, channels, height, width)\"\n* encode target variable with get_dummies: target variable is ranging from 0 to 9; we need to encode it (we can use pd.get_dummies like in this notebook or to_categorical function of keras","77f99522":"As we can see for every epochs accuracy improve while loss decrease. ","6762ec25":"As result we have an array of probabilities, we need np.argmax to take the prediction of the model:","b8e467d6":"Here the prediction of the model for this digit:","716750ec":"Dropout layers are applied to prevent overfitting.","8529ced0":"There are no null values.","0be437da":"## 1 - Building a CNN with Keras","2450f68d":"Checking for null values:","6df6b62a":"Now target variable is encoded, which means that y_train is an array with 10 columns each one representing a digit to predict."}}