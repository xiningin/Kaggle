{"cell_type":{"6734f380":"code","cef11883":"code","527f0558":"code","7268dfbe":"code","fa9435a8":"code","81d6c4ad":"code","7e682767":"code","7b09cdae":"code","52bd8d15":"code","0d4ad92c":"code","58d4a8b0":"code","02f270f2":"code","5f012a9a":"code","f5c4cc8d":"code","2806733e":"code","a41c521d":"code","76161495":"code","9e037e37":"code","d95c135e":"code","788d175e":"code","ce0bb03d":"code","cf5950a5":"code","49b212ee":"code","0c995ea2":"code","85b1c087":"code","256e832f":"code","0640d6a6":"code","c657c76b":"code","7b109431":"code","ec07a5a2":"code","a2effcd4":"code","197f1f1c":"code","56971c4d":"code","dac0ada2":"code","25aeab73":"code","82e08dde":"code","5655148e":"code","93cd5148":"code","21eb2c0c":"code","7daf9517":"code","5bc5fdb3":"code","e97d00aa":"code","906dc291":"code","dc867100":"code","5219fa33":"code","857105ba":"code","4c425367":"code","dccbf8d7":"markdown","70fc3d6d":"markdown","e9d988e5":"markdown","6f588d67":"markdown","e5f45a68":"markdown","ce4b4e1f":"markdown","247b5581":"markdown","c09faa07":"markdown","32eb1961":"markdown","4f59bedd":"markdown","2582d5c0":"markdown","9b06d75f":"markdown","65a74154":"markdown","a333b0ec":"markdown","46a8c3a8":"markdown","830ceafd":"markdown","d657f83f":"markdown","254e97c3":"markdown","8b7a9bbf":"markdown","dc064322":"markdown","1e522827":"markdown","5e99970a":"markdown","199180aa":"markdown","154612e9":"markdown","277a382a":"markdown","168b51b4":"markdown","b3accefb":"markdown","fbe8a706":"markdown","8a26644c":"markdown","b481b25b":"markdown","69513250":"markdown","398fc2da":"markdown","53d8fdf0":"markdown","72481a9a":"markdown","fbee1b9e":"markdown","df7718df":"markdown","7b2e0b59":"markdown","8620ffe2":"markdown","a1e1cfc7":"markdown","9c491632":"markdown","8a589563":"markdown","57aa44e2":"markdown","11fcd0d2":"markdown","83fc9c03":"markdown","1c5355f4":"markdown","c5b92770":"markdown","81c6a504":"markdown","fe876f22":"markdown","d9feb42f":"markdown","15e76116":"markdown","e8fd2d9e":"markdown","3b8ec079":"markdown","cf704558":"markdown","86b66344":"markdown","8238a562":"markdown","fb4904e9":"markdown","c3364b28":"markdown","ba3fb8c5":"markdown","87f8b38f":"markdown","cf343477":"markdown","b60c9292":"markdown","4296c821":"markdown","7d2277dc":"markdown","546a1465":"markdown","1fd76c46":"markdown","0e1732c1":"markdown","c5195fce":"markdown","50bad33e":"markdown","bd01e958":"markdown","1187441f":"markdown","31273064":"markdown","f2c046af":"markdown","780fc33a":"markdown","9b319591":"markdown","6c3a5a99":"markdown","f3fbae70":"markdown","b54b1ff6":"markdown","3fe78415":"markdown"},"source":{"6734f380":"!pip install openpyxl","cef11883":"# Import libraries\nfrom scipy.interpolate import make_interp_spline\nimport pandas as pd\nfrom pandas.api.types import CategoricalDtype\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib.patheffects as path_effects\nfrom matplotlib.colors import LinearSegmentedColormap\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport ipywidgets as widgets\nimport numpy as np\nimport operator\nfrom IPython.display import display, HTML\nimport openpyxl\nimport warnings\n%matplotlib inline\n\n# Set pandas options\npd.set_option(\"display.max_colwidth\", None)\npd.set_option(\"display.max_columns\", None)\npd.options.display.float_format = '{:,.1f}'.format\nwarnings.filterwarnings(\"ignore\")","527f0558":"html_contents =\"\"\"\n<!DOCTYPE html>\n<html lang=\"en\">\n    <head>\n    <style>\n    .toc h2{\n        color: white;\n        background: #00005A;\n        font-weight: 600;\n        font-family: Helvetica;\n        font-size: 23px;\n        padding: 6px 12px;\n        margin-bottom: 2px;\n    }\n    \n    .toc ol li{\n        list-style:none;\n        line-height:normal;\n        }\n     \n    .toc li{\n        background: #626EFA;\n        color: white;\n        font-weight: 600;\n        font-family: Helvetica;\n        font-size: 18px;\n        margin-bottom: 2px;\n        padding: 6px 12px;\n    }\n\n    .toc ol ol li{\n        background: white;\n        color: #626EFA;\n        font-weight: 400;\n        font-size: 15px;\n        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n        margin-top: 0px;\n        margin-bottom: 0px;\n        padding: 3px 12px;\n    }\n    .section_title{\n        background-color: #00005A;\n        color: white;\n        font-family: Helvetica;\n        font-size: 25px;\n        padding: 6px 12px;\n        margin-bottom: 5px;\n    }\n    .subsection_title{\n        background: white;\n        color: #B00068;\n        font-family: Helvetica;\n        font-size: 21px;\n        padding: 0px 0px;\n        margin-bottom: -30px;\n    }\n    .heading{\n        background: white;\n        color: #626EFA;\n        font-family: Helvetica;\n        font-size: 16px;\n        padding: 0px 42px;\n        margin-bottom: 12px;\n    }\n    <\/style>\n    <\/head>\n    <body>\n        <div class=\"toc\">\n        \n        <ol> \n        <h2> Table of Contents <\/h2>\n        <li>1. Subject of the Study <\/li> \n        <li>2. Structure of the Study <\/li>\n        <li>3. Findings <\/li>\n        <ol> \n            <li>3.1. The distribution of programming experience is positively skewed <\/li>\n            <li>3.2. Africa and Asia lead in launching novices <\/li> \n            <li>3.3. The older the data scientist is, the more programming experience he has <\/li>\n            <li>3.4. Kagglers usually learn coding to do machine learning <\/li>            \n            <li>3.5. Women take more part in programming than in the past <\/li> \n            <li>3.6. The highest level of formal education for the majority of novices is a bachelor's degree <\/li> \n            <li>3.7. Veterans are generally on the production front <\/li> \n            <li>3.8. Veterans are versatile programmers <\/li>\n            <li>3.9. Selected groups agree on Python as the recommended language to learn first for data sciences <\/li>\n            <li>3.10. Veterans use both older and new editors <\/li> \n            <li>3.11. Veterans win in visualization libraries too <\/li>\n            <li>3.12. Veterans' expertise level is even higher for machine learning frameworks than the novices <\/li>            \n            <li>3.13. Traditional methods are popular for both groups <\/li> \n            <li>3.14. It seems too early for novices to use computer vision methods <\/li> \n            <li>3.15. Nearly 90% of the respondents don't use NLP regularly <\/li> \n            <li>3.16. Academics, computer\/technology, and small companies are top places of employment <\/li>            \n            <li>3.17. Veterans have more activities to perform than novices <\/li>\n            <li>3.18. Veterans spend more than novices <\/li> \n            <li>3.19. Veterans share their work more than the novices <\/li>\n            <li>3.20. Veterans are more eager to learn <\/li>            \n            <li>3.21. Veterans favor media sources more than the novices <\/li> \n            <li>3.22.The top three choices of the veterans and novices are Amazon Web Services, Google Cloud Platform, and Microsoft Azure <\/li> \n            <li>3.23. %90 of the respondents in the selected groups don't use managed machine learning products <\/li> \n            <li>3.24. MySQL is and remains to be the top choice of the selected groups <\/li>   \n            <li>3.25. Tableau is and remains to be the top choice of the selected groups <\/li> \n            <li>3.26. Both veterans and novices want to learn more about automated model selection and automation of full ML pipelines <\/li> \n            <li>3.27. TensorBoard and MLflow are and remain to be the top choices of the selected groups <\/li> \n            <li>3.28. Veterans tend to be less ambitious and more realistic, and the novices tend to be more ambitious and less realistic <\/li>                \n        <\/ol>\n        <li>4. Summary and Conclusion <\/li>\n        <ol> \n            <li>4.1. Veterans are proved to be more skilled than novices in several defined departments <\/li> \n            <li>4.2. Proven skills pay off <\/li>\n        <\/ol>\n        <li>5. Notes <\/li>\n        <li>6. Sources <\/li>\n        <\/ol>\n        <\/div>\n    <\/body>\n<\/html>\n\"\"\"\n\nHTML(html_contents)","7268dfbe":"# Read the data\ndata = pd.read_csv(\"..\/input\/kaggle-survey-2021\/kaggle_survey_2021_responses.csv\")\ndata = data.iloc[1: , :]\n\n# Rename some country names\nold_country_names = [\"United States of America\", \"Viet Nam\", \"United Kingdom of Great Britain and Northern Ireland\", \n                     \"Czech Republic\", \"Iran, Islamic Republic of...\", \"Hong Kong (S.A.R.)\"]\nnew_country_names = [\"United States\", \"Vietnam\", \"United Kingdom\", \"Czechia\", \"Iran\", \"Hong Kong\"]\ndata['Q3'] = data['Q3'].replace(old_country_names, new_country_names)\n\n# Replace the long responses\ndata[\"Q6\"].replace({\"I have never written code\": \"0 years\"}, inplace=True)\nold_mlex_names = [\"I do not use machine learning methods\", \"20 or more years\", \"Under 1 year\"]\nnew_mlex_names = [\"0 years\", \"20+ years\", \"< 1 years\"]\ndata['Q15'] = data['Q15'].replace(old_mlex_names, new_mlex_names)\nold_titles = [\"DBA\/Database Engineer\" , \"Machine Learning Engineer\", \"Program\/Project Manager\", \"Developer Relations\/Advocacy\", \"Currently not employed\"]\nnew_titles = [\"DBA\/DB Engineer\", \"ML Engineer\", \"Prog\/Project Man.\", \"Dev.Rels\/Advocacy\", \"Unemployed\"]\ndata['Q5'] = data['Q5'].replace(old_titles, new_titles)\n\n# Change dtype\ndata = data.astype(\"category\")\n\n# Check for all nan value rows\nidx = data.index[data.isnull().all(1)]\nnans = data.loc[idx]\n\n# Isolate the programming experience column\nprex = data.loc[1:,\"Q6\"]\n\n# Reorder categories\nprex_categories = [\"0 years\", \"< 1 years\", \"1-3 years\", \"3-5 years\", \"5-10 years\", \"10-20 years\", \"20+ years\"]\ncat_prex_order = CategoricalDtype(prex_categories, ordered=True)\n\nage_categories = ['18-21', '22-24', '25-29', '30-34', '35-39', '40-44', '45-49', '50-54', '55-59', '60-69', '70+']\ncat_age_order = CategoricalDtype(age_categories, ordered=True)\n\nprex_rev_categories = [\"20+ years\", \"10-20 years\", \"5-10 years\", \"3-5 years\", \"1-3 years\", \"< 1 years\", \"0 years\"]\ncat_prex_rev_order = CategoricalDtype(prex_rev_categories, ordered=True)\n\nmlex_categories = [\"0 years\", \"< 1 years\", \"1-2 years\", \"2-3 years\", \"3-4 years\", \"4-5 years\", \"5-10 years\", \"10-20 years\", \"20+ years\"]\ncat_mlex_order = CategoricalDtype(mlex_categories, ordered=True)\n\ncom_size_categories = [\"0-49 employees\", \"50-249 employees\", \"250-999 employees\", \"1000-9,999 employees\", \"10,000 or more employees\"]\ncat_com_size_order = CategoricalDtype(com_size_categories, ordered=True)\n\n# Create dataframes for Novices and Veterans\ndf_novices = data[data[\"Q6\"]==\"< 1 years\"]\ndf_veterans = data[data[\"Q6\"]==\"20+ years\"]\n\n# Color codes\nclass color:\n   PURPLE = '\\033[95m'\n   CYAN = '\\033[96m'\n   DARKCYAN = '\\033[36m'\n   BLUE = '\\033[94m'\n   GREEN = '\\033[92m'\n   YELLOW = '\\033[93m'\n   RED = '\\033[91m'\n   BOLD = '\\033[1m'\n   UNDERLINE = '\\033[4m'\n   END = '\\033[0m'\n\n# Plot parameters\nplot_height=500\nplot_width=800\nfont_small = 10\nfont_medium = 11\nfont_large = 14\nfont_margin = 2\nfigure_title_color = \"#B00068\"\nsubplot_title_color = \"#626EFA\"\nln_color = \"#778899\"\nbox_color = \"#314253\"\n\n# Create custom color map\ncolors1 = [\"#EEFAFF\", \"#DAE6FF\", \"#C6D2FF\", \"#B2BEFF\", \"#9EAAFF\", \"#8A96FF\", \"#7682FF\", \"#626EFA\",\n            \"#4E5AE6\", \"#3A46D2\", \"#2632BE\", \"#121EAA\", \"#000A96\", \"#000082\", \"#00006E\", \"#00005A\"]\ncmap1 = LinearSegmentedColormap.from_list(\"\", colors1)\n\ncolors2 = [\"#FFF0FF\", \"#FFDCFF\", \"#FFC8FF\", \"#FFB4FF\", \"#FFA0FF\", \"#FF8CF4\", \"#FF78E0\", \"#FF64CC\", \n           \"#FF50B8\", \"#EC3CA4\", \"#D82890\", \"#C4147C\", \"#B00068\", \"#9C0054\", \"#880040\", \"#74002C\"]\ncmap2 = LinearSegmentedColormap.from_list(\"\", colors2)","fa9435a8":"# Fill nan values with 0, else with 1\ndef fill_nan(x):\n    if x != \"\":\n        return 1\n    else:\n        return 0\n\n    \n# Hide axes\ndef hide_axes(this_ax):\n    this_ax.set_frame_on(False)\n    this_ax.set_xticks([])\n    this_ax.set_yticks([])\n    return this_ax\n\n\ndef extract_columns(questions):\n\n    # Find all the columns of the interested questions and none columns in them\n    all_columns = []\n    none_columns = []\n    blocked_choices = [\"None\", \"No \/ None\", \"None of these activities are an important part of my role at work\", \"I do not share my work publicly\"]\n    for question in questions:\n        all_cols = data.columns[data.columns.str.startswith(question)].to_list()\n        for blocked_choice in blocked_choices:\n            non_cols = data[all_cols].columns[data[all_cols].isin([blocked_choice]).any()].to_list()\n            if non_cols:\n                none_columns.extend(non_cols)\n       \n        all_columns.extend(all_cols)        \n\n    # Remove none columns\n    all_columns = [ele for ele in all_columns if ele not in none_columns]\n    \n    return all_columns\n\n\ndef highlight_cols(x):\n\n    # Copy df to new - original data are not changed\n    df = x.copy()\n    \n    # Select all values to default value - light blue color\n    df.loc[:,:] = 'background-color: #e5ecf6'\n    \n    # Overwrite values different colors\n    df[[\"Veterans True #\", \"Veterans False #\", \"Veterans All #\", \"Veterans True %\"]] = 'background-color: #B00068'\n    df[[\"Novices True #\", \"Novices False #\", \"Novices All #\", \"Novices True %\"]] = 'background-color: #626EFA'\n\n    return df\n\n\n# Create score dataframe\ndef create_score_df(data, title):\n    df = pd.DataFrame.from_dict(data)\n    print(title)  \n    df = df.style.apply(highlight_cols, axis=None)\n    display(df)\n    \n# Score containers\nnov_scores = {}\nvet_scores = {}\n        \n        \ndef get_stats(novices, veterans, new_col_name):\n    \n    # Find the response statistics\n    novices_all_counts = len(novices)\n    novices_true_counts = novices[novices[new_col_name] != 0].count(axis=0)[new_col_name]\n    novices_false_counts = novices_all_counts - novices_true_counts\n    novices_true_rate = round(novices_true_counts \/ novices_all_counts, 2) * 100\n\n    veterans_all_counts = len(veterans)\n    veterans_true_counts = veterans[veterans[new_col_name] != 0].count(axis=0)[new_col_name]\n    veterans_false_counts = veterans_all_counts - veterans_true_counts\n    veterans_true_rate = round(veterans_true_counts \/ veterans_all_counts, 2) * 100\n\n    total_true_counts = novices_true_counts + veterans_true_counts\n    total_false_counts = novices_false_counts + veterans_false_counts\n    total_counts = total_true_counts + total_false_counts\n    \n    stats = {\"Novices True #\": [int(novices_true_counts)], \n          \"Novices False #\": [int(novices_false_counts)],\n          \"Novices All #\": [int(novices_all_counts)], \n          \"Novices True %\": [novices_true_rate],\n          \"Veterans True #\": [int(veterans_true_counts)], \n          \"Veterans False #\": [int(veterans_false_counts)], \n          \"Veterans All #\": [int(veterans_all_counts)], \n          \"Veterans True %\": [veterans_true_rate],\n          \"Total True #\": [int(total_true_counts)],\n          \"Total False #\": [int(total_false_counts)] \n        }\n    \n    return stats\n\n\n# Prepare dataframes for response statistics\ndef prep_for_stats(target_column):\n    \n    # Deep copy dataframes\n    data_novices = df_novices.copy()\n    data_veterans = df_veterans.copy()\n    \n    # Replace NaN values\n    data_novices[target_column].replace(np.nan, 0, inplace=True)\n    data_veterans[target_column].replace(np.nan, 0, inplace=True)\n\n    return data_novices, data_veterans\n\n\n# Success scores for bar plot data\ndef calculate_score(y1, y20, value_type, success_factor):\n    \n    if value_type == \"percentage\":\n        if success_factor != None:\n            novices_score = y1[success_factor]\n            veterans_score = y20[success_factor]\n        else:\n            novices_score = \"\"\n            veterans_score = \"\"\n    else:\n        novices_score = \"\"\n        veterans_score = \"\"\n    \n    return novices_score, veterans_score\n        \n\ndef get_plot_data(target_column, orientation, value_type, x_order, success_factor=None):\n  \n    # Get data for < 1 years\n    y1_data = df_novices.groupby([\"Q6\", target_column]).size()[\"< 1 years\"]\n    y1_data = y1_data.to_dict()\n\n    # Get data for 20+ years\n    y20_data = df_veterans.groupby([\"Q6\", target_column]).size()[\"20+ years\"]\n    y20_data = y20_data.to_dict()\n\n    # x axis values (checks if there is a specific order)\n    if x_order == None:\n        x = sorted(data[target_column].dropna().unique().tolist())\n    else:\n        x = x_order\n\n    # Calculate percentage y axis values\n    y1 = {}\n    y20 = {}\n    if value_type == \"percentage\":\n        for item in x:\n            y1[item] = round(y1_data[item] \/ sum(y1_data.values()) * 100, 1)\n            y20[item] = round(y20_data[item] \/ sum(y20_data.values()) * 100, 1)\n    else:\n        for item in x:\n            y1[item] = y1_data[item]\n            y20[item] = y20_data[item]\n                    \n    novices_score, veterans_score = calculate_score(y1, y20, value_type, success_factor)\n            \n    # Sort dictionaries in ascending order\n    if x_order == None: \n        y1 = dict(sorted(y1.items(), key=operator.itemgetter(1),reverse=True))\n        y20 = dict(sorted(y20.items(), key=operator.itemgetter(1),reverse=True))\n       \n    return y1, y20, novices_score, veterans_score\n\n\ndef box_with_annot(left, bottom, width, height, txt, color, ax, fill_value=False, ec=\"None\", fc=\"None\", alp=0.7):\n    \n    rect = patches.Rectangle((left,bottom), width, height, fill=fill_value, linewidth=4, edgecolor=ec, facecolor=fc, alpha=alp)\n    ax.add_patch(rect)\n    \n    an1 = ax.annotate(txt, xy=(left+width+0.2, bottom+0.1), fontsize=20, color=color, weight='bold', verticalalignment='center', horizontalalignment='center')","81d6c4ad":"# Prepare plot data for box and bar subplots\ndef prepare_data(question, new_columns, options, new_col_name, other=\"yes\"):\n    \n    # Total number of resources used\n    resources_used = len(new_columns) - 1 # Ignore the 'None' column\n        \n    # Establish data columns\n    old_columns = []\n    for option in range(1, options+1):\n        column_name = \"Q\" + question + \"_Part_\" + str(option)\n        old_columns.append(column_name)\n\n    if other == \"yes\":\n        column_name = \"Q\" + question + \"_OTHER\"\n        old_columns.append(column_name)\n        \n    # Generate new column names dictionary\n    zip_iterator = zip(old_columns, new_columns)\n    column_dict = dict(zip_iterator)\n\n    # Create the dataframe to prepare the charts\n    old_columns.insert(0, \"Q6\")\n    data_plot_novices = df_novices[old_columns]\n    data_plot_veterans = df_veterans[old_columns]\n\n    # Rename columns of interest\n    data_plot_novices = data_plot_novices.rename(columns=column_dict)\n    data_plot_veterans = data_plot_veterans.rename(columns=column_dict)\n    \n    # Add a summation column\n    data_plot_novices[new_col_name] = data_plot_novices[data_plot_novices.columns.difference(['Q6', 'None'])].count(axis=1)\n    data_plot_veterans[new_col_name] = data_plot_veterans[data_plot_veterans.columns.difference(['Q6', 'None'])].count(axis=1)\n        \n    # Create series by programming experience\n    y1_data = data_plot_novices.count(axis=0)\n    y20_data = data_plot_veterans.count(axis=0)\n    \n    # Find the percentage of each column\n    y_per_1_data = pd.Series()\n    y_per_20_data = pd.Series()\n    for key, value in column_dict.items():\n        y_per_1_data[value] = round(y1_data[value] \/ y1_data[\"Q6\"] * 100,1)\n        y_per_20_data[value] = round(y20_data[value] \/ y20_data[\"Q6\"] * 100, 1)\n    \n    # Axes ticks    \n    x = [\"< 1 years\", \"20+ years\"]\n    y1 = data_plot_novices[new_col_name]\n    y20 = data_plot_veterans[new_col_name]  \n    \n    # Calculate the median for the summation column\n    y1_median = y1.median()\n    y20_median = y20.median()\n    \n    # Calculate success scores\n    y1_score = round(y1_median \/ resources_used * 100, 0)\n    y20_score = round(y20_median \/ resources_used * 100, 0)\n    \n    stats = get_stats(data_plot_novices, data_plot_veterans, new_col_name)\n        \n    scores = {\"Novices Score %\": [y1_score], \"Veterans Score %\": [y20_score]}\n    \n    del data_plot_novices\n    del data_plot_veterans\n\n    return y1, y20, y_per_1_data, y_per_20_data, stats, scores\n\n\n# Create traces for subplots\ndef create_traces(y, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#626EFA\", line_color='#303CC8', plot_type=\"Box\", name=\"\", showlegend=True):\n    \n    # Box plot traces\n    if plot_type == \"Box\":\n        trace = go.Box(y=y,fillcolor=fillcolor, \n                        marker_color=fillcolor, \n                        marker_size=2,\n                        line_color=line_color, \n                        boxmean=True,\n                        boxpoints='all', \n                        jitter=0.3, \n                        pointpos=-1.8, \n                        showlegend=False,\n                        name=name\n                       )       \n        \n    # Vertical bar plot traces\n    elif plot_type == \"Vertical Bar\":\n        trace = go.Bar(x=y.index.tolist(),\n                y=y.values.tolist(),\n                name=name,\n                marker_color=fillcolor,\n                showlegend=showlegend,\n                xaxis=x_ax,\n                yaxis=y_ax\n                )\n        \n    return trace\n\n\n# Box and bar subplots\ndef box_bar_plot(fig, xaxis_titles, yaxis_titles, fig_title, cols=1, orientation=\"v\", legendy=1):\n\n    for col in cols:\n        # Update xaxis properties\n        fig.update_xaxes(title_text=xaxis_titles[col-1], titlefont_size=font_medium, tickfont_size=font_small, row=1, col=col)\n        \n        # Update yaxis properties\n        fig.update_yaxes(title_text=yaxis_titles[col-1], titlefont_size=font_medium, tickfont_size=font_small, row=1, col=col)\n        \n    # Update subplot title font sizes\n    fig.update_annotations(font=dict(size=font_medium, color=subplot_title_color), y=1.02) ##1d728b\n\n    # Update title and height\n    fig.update_layout(\n        title={\n            'text': fig_title,\n            'y':1,\n            'x':0.5,\n            'xanchor': 'center',\n            'yanchor': 'top'\n        },\n        title_font_color=figure_title_color,\n        title_font_size=font_large,        \n        showlegend=True,\n        legend=dict(\n            orientation=orientation,\n            yanchor=\"bottom\",\n            y=legendy,\n            xanchor=\"right\",\n            x=1,\n            font=dict(\n                size=font_small\n            )\n        ),\n        height=plot_height, \n        width=plot_width\n    )   \n    \n    return fig","7e682767":"# Group bar plot\ndef group_bar_plot(target_column, orientation, fig_title, xaxis_title, yaxis_title, categoryorder, height, width, value_type, x_order=None, axis_ticks=None, success_factor=None):\n    \n    # Get plot data\n    y1, y20, novices_score, veterans_score = get_plot_data(target_column, orientation, value_type, x_order, success_factor)\n    \n    # Horizontal or vertical bar plot\n    if axis_ticks == None:\n        if orientation == \"h\":\n            x1 = list(y1.values())  \n            x2 = list(y20.values())       \n            y1 = list(y1.keys())  \n            y2 = list(y20.keys())\n        else:\n            x1 = list(y1.keys())  \n            x2 = list(y20.keys()) \n            y1 = list(y1.values())  \n            y2 = list(y20.values())\n    else:\n        if orientation == \"h\":\n            x1 = list(y1.values())  \n            x2 = list(y20.values())     \n            y1 = axis_ticks  \n            y2 = axis_ticks\n        else:\n            x1 = axis_ticks\n            x2 = axis_ticks \n            y1 = list(y1.values())  \n            y2 = list(y20.values())       \n\n    fig = go.Figure()\n\n    # < 1 years\n    fig.add_trace(go.Bar(x=x1,\n                    y=y1,\n                    name=\"Novices\", # < 1 years\n                    orientation=orientation,\n                    marker_color='#626EFA',\n                    ))\n\n    # 20+ years\n    fig.add_trace(go.Bar(x=x2,\n                    y=y2,\n                    name='Veterans',\n                    orientation=orientation,\n                    marker_color='#B00068',\n                    ))\n\n    # Title and axis labels layout\n    fig.update_layout(\n        title={\n            'text': fig_title,\n            'y':1.0,\n            'x':0.5,\n            'xanchor': 'center',\n            'yanchor': 'top'\n        },\n        title_font_color=figure_title_color,\n        title_font_size=font_large,\n        xaxis=dict(\n            title=xaxis_title,\n            titlefont_size=font_medium,\n            tickfont_size=font_small,\n        ),    \n        yaxis=dict(\n            title=yaxis_title,\n            titlefont_size=font_medium,\n            tickfont_size=font_small,\n            categoryorder=categoryorder\n        ),\n        barmode='group',\n        bargap=0.15,\n        bargroupgap=0.1,\n        height=height, \n        width=width,\n        showlegend=True,\n        legend=dict(\n            orientation=\"h\",\n            yanchor=\"bottom\",\n            y=1,\n            xanchor=\"right\",\n            x=1,\n            font=dict(\n                size=font_small\n            )\n        )\n    )\n    \n    return fig, novices_score, veterans_score","7b09cdae":"# Products correlations\ndef create_sub_dfs(df, new_cls_1, new_cls_2):\n    \n    # Create two separate dataframes first\n    df_1 = df[df[\"Q6\"] == \"< 1 years\"]\n    df_20 = df[df[\"Q6\"] == \"20+ years\"]\n    \n    # Create correlation matrix\n    df_1 = df_1.corr()\n    df_20 = df_20.corr()\n    \n    # Drop unnecessary columns\n    df_1.drop(new_cls_1, inplace=True, axis=1)\n    df_20.drop(new_cls_1, inplace=True, axis=1)\n    \n    # Drop unnecessary rows\n    df_1.drop(new_cls_2, inplace=True, axis=0)\n    df_20.drop(new_cls_2, inplace=True, axis=0)\n    \n    # Change index column name\n    df_1.index.name = 'Product_1'\n    df_20.index.name = 'Product_1'\n\n    # Change columns name\n    df_1.columns.name = 'Product_2'\n    df_20.columns.name = 'Product_2'\n    \n    return df_1, df_20\n    \n    \n# Fill the dataframe cells, 0 for NaN, 1 for non-NaN values\ndef fill_nans(df, new_cls):\n    \n#     df[new_cls] = df[new_cls].notnull().astype('int')\n    df[new_cls] = np.where(df[new_cls].isnull(), 0, 1)\n\n    return df\n\n\ndef create_df(questions, new_cls_1, new_cls_2):\n    \n    # Unite the columns\n    new_cls = new_cls_1 + new_cls_2\n    \n    # Create empty data containers\n    cls_dict = {}\n    old_cls = []\n    \n    # Find the related columns for the given question\n    for question in questions:\n        cols = data.columns[data.columns.str.startswith(question)].to_list()\n        old_cls.extend(cols)\n    \n    # Delete unnecessary list\n    del cols\n    \n    # Create a new df with the columns of given questions\n    df = data[[\"Q6\", *old_cls]]\n    \n    # Create a dictionary with old and new column names\n    len_cls = len(old_cls)\n    for len_cl in range(len_cls):\n        cls_dict[old_cls[len_cl]] = new_cls[len_cl]\n        \n    # Rename columns\n    df = df.rename(columns=cls_dict)  \n    \n    # Fill the dataframe cells, 0 for NaN, 1 for non-NaN values\n    df = fill_nans(df, new_cls)\n    \n    # Create two dataframes for novices and veterans\n    df_1, df_20 = create_sub_dfs(df, new_cls_1, new_cls_2)\n    \n    return df_1, df_20\n\n\n# Heatmap\ndef plot_heatmap(df, heatmap_title, cmap, title_color, x_label, y_label, y_loc):\n    f, ax = plt.subplots(figsize=(12,10)) \n    plt.suptitle(heatmap_title, fontsize=font_large+font_margin, color=title_color, ha=\"center\", y=y_loc)\n    heatplot_1 = sns.heatmap(df, cmap=cmap, linewidths=5, fmt=\"0.01g\", annot=True, square=False, xticklabels=True, yticklabels=True, cbar=True)\n    heatplot_1.set_xlabel(x_label,fontsize=font_medium+font_margin, labelpad=10)\n    heatplot_1.set_ylabel(y_label,fontsize=font_medium+font_margin, labelpad=10)\n    heatplot_1.yaxis.set_label_position(\"left\")\n    heatplot_1.xaxis.set_label_position(\"top\")\n    heatplot_1.tick_params(axis='x', labelbottom = False, bottom=False, top = False, labeltop=True, labelrotation=90, pad=10)\n    heatplot_1.tick_params(axis='y', labelleft=True, labelright=False, left=False, labelrotation=0, pad=10)\n    heatplot_1.set_xticklabels(labels=heatplot_1.get_xticklabels(), ha='left')\n    plt.show()","52bd8d15":"# Q6: Percentage distribution of data scientists by programming experience\n\nfig = go.Figure(data=[go.Histogram(x=prex, histnorm='percent', marker_color=\"#778899\")])\n\n# x axis categories in order\ncategories = [\"0 years\", \"< 1 years\", \"1-3 years\", \"3-5 years\", \"5-10 years\", \"10-20 years\", \"20+ years\"]\nfig.update_xaxes(categoryorder=\"array\", categoryarray=categories)\n\n# Title and axis labels layout\ntitle = \"Fig.3.1.1 - Percentage Distribution of Data Scientists by Programming Experience\"\nfig.update_layout(\n    title={\n        'text': title,\n        'y':0.90,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'\n    },\n    title_font_color=figure_title_color,\n    title_font_size=font_large,\n    xaxis=dict(\n        title=\"Programming Experience\",\n        titlefont_size=font_medium,\n        tickfont_size=font_small,\n    ),    \n    yaxis=dict(\n        title=\"Percent\",\n        titlefont_size=font_medium,\n        tickfont_size=font_small,\n    ),\n    height=plot_height, \n    width=plot_width,\n)\n    \n# Annotation for \"Novices\"\nfig.add_annotation(\n        x=\"< 1 years\",\n        y=23,\n        xref=\"x\",\n        yref=\"y\",\n        text=\"Novices\",\n        showarrow=True,\n        font=dict(\n            family=\"Arial\",\n            size=font_large,\n            color=\"#ffffff\"\n            ),\n        align=\"center\",\n        arrowhead=2,\n        arrowsize=1,\n        arrowwidth=2,\n        arrowcolor=\"#626EFA\",\n        ax=-20,\n        ay=-40,\n        bordercolor=\"#626EFA\",\n        borderwidth=0,\n        borderpad=4,\n        bgcolor=\"#626EFA\",\n        opacity=0.8\n        )\n\n# Annotation for \"Veterans\"\nfig.add_annotation(\n        x=\"20+ years\",\n        y=7.5,\n        xref=\"x\",\n        yref=\"y\",\n        text=\"Veterans\",\n        showarrow=True,\n        font=dict(\n            family=\"Arial\",\n            size=font_large,\n            color=\"#ffffff\"\n            ),\n        align=\"center\",\n        arrowhead=2,\n        arrowsize=1,\n        arrowwidth=2,\n        arrowcolor=\"#B00068\",\n        ax=5,\n        ay=-40,\n        bordercolor=\"#B00068\",\n        borderwidth=0,\n        borderpad=4,\n        bgcolor=\"#B00068\",\n        opacity=0.8\n        )\n\nfig.show()\n\n# Get response statistics\ndata_novices, data_veterans = prep_for_stats(\"Q6\")\nstats = get_stats(data_novices, data_veterans, \"Q6\")\ntitle_1 = \"Response Statistics for Coding Experience (Q6):\"\ncreate_score_df(stats, title_1)\ndel data_novices\ndel data_veterans","0d4ad92c":"# The Number of Data Scientists by the Selected Groups\n\nx = [\"Novices\", \"Veterans\"]\ny1 = len(df_novices)\ny2 = len(df_veterans)\ny = [y1, y2]\n\n# Bar colors\ncolors = ['#626EFA',] * 2\ncolors[1] = '#B00068'\n\n# Use textposition='auto' for direct text\nfig = go.Figure(data=[go.Bar(\n            x=x, y=y,\n            text=y,\n            textposition='auto',\n            textfont_size=font_small,\n            marker_color=colors,\n            width=0.3\n        )])\n\n# Title and axis labels layout\nfig.update_layout(\n    title={\n        'text': \"Fig.3.1.2 - The Number of Data Scientists by the Selected Groups\",\n        'y':0.90,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'\n    },\n    title_font_color=figure_title_color,\n    title_font_size=font_large,\n    xaxis=dict(\n        title=\"Programming Experience\",\n        titlefont_size=font_medium,\n        tickfont_size=font_small,\n    ),    \n    yaxis=dict(\n        title=\"# of Responses\",\n        titlefont_size=font_medium,\n        tickfont_size=font_small,\n    ),\n    height=plot_height, \n    width=plot_width\n)\n\nfig.show()\n\n# Get response statistics\ndata_novices, data_veterans = prep_for_stats(\"Q6\")\nstats = get_stats(data_novices, data_veterans, \"Q6\")\ntitle_1 = \"Response Statistics for Coding Experience (Q6):\"\ncreate_score_df(stats, title_1)\ndel data_novices\ndel data_veterans\n\nnov_scores[\"Programming Experience\"] = 5.0 # PRX\nvet_scores[\"Programming Experience\"] = 100.0","58d4a8b0":"# Create a new dataframe for countries\ndf_countries = data[[\"Q6\", \"Q3\"]]\n\n# Calculate the number of novices and veterans in each country\ndf_countries[\"Size\"] = 1\ndf_countries = df_countries.groupby([\"Q6\", \"Q3\"]).sum().reset_index()\ndf_countries = df_countries[(df_countries['Q6'] == \"< 1 years\") | (df_countries['Q6'] == \"20+ years\")]\ndf_countries = df_countries.rename(columns={\"Q6\": \"Programming Experience\"})\n\n# Replace values\ndf_countries[\"Programming Experience\"].replace({\"< 1 years\": \"Novices\"}, inplace=True)\ndf_countries[\"Programming Experience\"].replace({\"20+ years\": \"Veterans\"}, inplace=True)\n\n# Plot the graph\nfig = px.scatter_geo(\n    df_countries, locations=\"Q3\", locationmode='country names', \n    color=\"Programming Experience\", \n    size='Size', hover_name=\"Programming Experience\", \n    projection=\"natural earth\",\n    title=\"Geographical Distribution of the Data Scientists with Different Years of Programming Experience\", \n    color_discrete_map = {\"Novices\": \"#626EFA\", \"Veterans\": \"#B00068\"}\n    )\n\n# Title and axis labels layout\nfig.update_layout(\n    title={\n        'text': \"Fig.3.2.1 - Geographical Distribution of the Novices and Veterans\",\n        'y':0.90,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'\n    },\n    title_font_color=figure_title_color,\n    title_font_size=font_large,\n    height=plot_height,\n    width=plot_width,\n    showlegend=True,\n    legend=dict(\n        orientation=\"v\",\n        yanchor=\"bottom\",\n        y=0.35,\n        xanchor=\"right\",\n        x=0.28,\n        font=dict(\n            size=font_small\n        )\n    )\n)\n\ndel df_countries\n\nfig.show()","02f270f2":"# Import data\ncountries_by_continent = pd.read_csv(\"..\/input\/countries-by-continent\/Countries by continents.csv\")\nworld_population = pd.read_excel(\"..\/input\/world-population-by-country-2020\/World population by country 2020.xlsx\", sheet_name='Sheet1')\ndf_geog = data[[\"Q6\", \"Q3\"]]\n                \n# Rename column\ndf_geog = df_geog.rename(columns={\"Q3\": \"Country\", \"Q6\": \"Programming Experience\"})\n\n# Remove rows that are not specified\ndf_geog = df_geog.apply(lambda x: x.str.strip())\nto_drop = [\"I do not wish to disclose my location\", \"Other\"]\ndf_geog = df_geog[~df_geog['Country'].isin(to_drop)]\n\n# Get the continent with merge\ndf_geog = df_geog.merge(countries_by_continent[[\"Country\", \"Continent\"]], on='Country', how='left')\n# df_geog[\"Continent\"] = df_geog[\"Q3\"].apply(find_continent)\n\n# Rename column values\nold_prex_names = [\"< 1 years\", \"20+ years\"]\nnew_prex_names = [\"Novices\", \"Veterans\"]\ndf_geog['Programming Experience'] = df_geog['Programming Experience'].replace(old_prex_names, new_prex_names)\n\n# Filter, group by and sum data scientists\ndf_geog[\"# of Data Scientists\"] = 1\ndf_geog = df_geog[(df_geog[\"Programming Experience\"]==\"Novices\") | (df_geog[\"Programming Experience\"]==\"Veterans\")]\ndf_geog = df_geog.groupby([\"Programming Experience\", \"Country\", \"Continent\"]).sum().reset_index()\n\n# Get the population with merge\ndf_geog = df_geog.merge(world_population[[\"Country\", \"Population\"]], on='Country', how='left')\n                \n# Find the # of data scientists per 10_000,000 persons of the population\ndf_geog[\"# per 10M\"] =  round((df_geog[\"# of Data Scientists\"] * 10_000_000) \/ df_geog[\"Population\"], 1)\n\n# Plot treemap\nfig = px.treemap(df_geog, path=[px.Constant(\"world\"), 'Continent', 'Country', \"Programming Experience\"], \n                 values='# per 10M', maxdepth=-1, names=\"Programming Experience\", \n                 color='# per 10M', hover_data=['Continent'], \n                 color_continuous_scale='RdBu_r')\n\n# Title and axis labels layout\nfig.update_layout(\n    title={\n        'text': \"Fig.3.2.2 - Geographical Distribution of the Novices and Veterans by <br> Their Sizes per 10M People of the Country Population\",\n        'y':0.97,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'\n    },\n    title_font_color=figure_title_color,\n    title_font_size=font_large,\n    height=plot_height, \n    width=plot_width,\n    margin = dict(t=50, l=25, r=25, b=25),\n    showlegend=True,\n    legend=dict(\n        orientation=\"h\",\n        yanchor=\"bottom\",\n        y=1,\n        xanchor=\"right\",\n        x=1,\n        font=dict(\n            size=font_small\n        )\n    )\n)\n\ndel df_geog\n\nfig.show()","5f012a9a":"# Distribution of Each Selected Group by Country of Residence\n# Data scientists of < 1 years and 20+ years by country\n\ntarget_column = \"Q3\"\nvalue_type=\"absolute\"\norientation = \"h\"\nfig_title = \"Fig.3.2.3 - Distribution of the Selected Groups by Country of Residence\"\nxaxis_title = \"# of Responses\"\nyaxis_title = \"Country of Residence\"\ncategoryorder='total ascending'\nvalue_type=\"absolute\"\nheight=plot_height*4\nwidth=plot_width\nfig, _, _ = group_bar_plot(target_column, orientation, fig_title, xaxis_title, yaxis_title, categoryorder, height, width, value_type)\n\nfig.show()\n\n# Get response statistics\ndata_novices, data_veterans = prep_for_stats(target_column)\nstats = get_stats(data_novices, data_veterans, target_column)\ntitle_1 = \"Response Statistics for Country of Residence (Q3):\"\ncreate_score_df(stats, title_1)\ndel data_novices\ndel data_veterans","f5c4cc8d":"# Correlation between Age and Programming Experience\n\n# Create a new df\ndata_2 = data[[\"Q6\", \"Q1\"]]\n\n# Create a new column\ndata_2['cnt'] = 1\n\n# Change the programming experience column type\ndata_2['Q6'] = data_2['Q6'].astype(cat_prex_rev_order)\n\n# Histogram data\nhist_data_q1 = data_2[\"Q1\"].sort_values()\nhist_data_q6 = data_2[\"Q6\"].astype(cat_prex_order).sort_values()\n\n# Group by programming experience and age\ndata_2 = data_2.groupby([\"Q6\", \"Q1\"]).agg({\"cnt\": sum})\n\n# Turn into correlation matrix\ndata_2 = data_2.unstack()\ndata_2 = data_2[\"cnt\"]\n\n# Heatmap and marginal histograms\nplt.style.use('seaborn-dark')\nf, ax = plt.subplots(nrows=2, ncols=2, figsize=(17.5,13.5), gridspec_kw={'height_ratios':[1.4,5], 'width_ratios':[1,5], 'wspace':0.1, 'hspace':0.1})\nplt.suptitle(\"Fig.3.3.1 - Correlation between Age and Programming Experience\", fontsize=font_large+font_margin, color=figure_title_color, ha=\"center\", y=0.98)\nthis_ax = ax[0,0]\nhide_axes(this_ax)\n\n# Age histogram\nthis_ax = ax[0,1]\nbins = np.arange(12) - 0.5\nthis_ax.hist(hist_data_q1, bins=bins, facecolor=figure_title_color, edgecolor = \"white\", linewidth=1.5, rwidth=0.4)\nthis_ax.xaxis.set_label_position(\"top\")\nthis_ax.xaxis.tick_top()\nthis_ax.set_ylim([0, 5500])\nthis_ax.set_xlabel('Age', fontsize=font_medium+font_margin, labelpad=10)\nthis_ax.set_ylabel('# of Responses', fontsize=font_medium+font_margin, labelpad=10)\n\n# Programming experience histogram\nthis_ax = ax[1,0]\nbins = np.arange(8) - 0.5\nthis_ax.hist(hist_data_q6, bins=bins, facecolor=subplot_title_color, edgecolor = \"white\", linewidth=1.5, orientation=u'horizontal', rwidth=0.4)\nthis_ax.set_yticklabels([\"0\", \"0-1\", \"1-3\", \"3-5\", \"5-10\", \"10-20\", \"20+\"])\nthis_ax.yaxis.set_label_position(\"left\")\nthis_ax.xaxis.tick_top()\nthis_ax.xaxis.set_label_position(\"top\")\nthis_ax.set_xlim([0, 8500])\nthis_ax.set_xlim(this_ax.get_xlim()[::-1])\nthis_ax.set_xlabel('# of Responses', fontsize=font_medium+font_margin, labelpad=10)\nthis_ax.set_ylabel('Programming Experience (years)', fontsize=font_medium+font_margin, labelpad=10)\n\n# Heatmap\nheatplot_1 = sns.heatmap(data_2, cmap=\"Blues\", linewidths=5, annot=True, fmt=\"d\", square=False, xticklabels=True, yticklabels=True, cbar=False)\nheatplot_1.set_xlabel(\"Age\",fontsize=font_medium+font_margin, labelpad=10)\nheatplot_1.set_ylabel(\"Programming Experience (years)\",fontsize=font_medium+font_margin, labelpad=10)\nheatplot_1.yaxis.set_label_position(\"right\")\nheatplot_1.set_yticklabels([\"20+\", \"10-20\", \"5-10\", \"3-5\", \"1-3\", \"0-1\", \"0\"])\nheatplot_1.tick_params(axis='x', which='major', labelbottom = True, bottom=True, top = True, labeltop=False, pad=10)\nheatplot_1.tick_params(axis='y', labelleft=False, labelright=True, labelrotation=0, pad=10)\n \n# Regression line\nthis_ax = ax[1,1]\n\n# Dataset\nx = np.array([0.31, 0.57, 1.2, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5])\ny = np.array([5.40, 5, 4, 3.60, 2.7, 2.0, 1.5, 1.2, 1.0, 0.9 ] )\nX_Y_Spline = make_interp_spline(x, y)\n \n# Returns evenly spaced numbers over a specified interval.\nX_ = np.linspace(x.min(), x.max(), 1000)\nY_ = X_Y_Spline(X_) \nthis_ax.plot(X_, Y_, color=ln_color, linewidth=5, linestyle =\"--\", alpha=0.8) # #F57504\n\n# Rectangles\nthis_ax = ax[1,1]\nbox_with_annot(0.1, 2.1, 3.8, 3.8, \"A\", box_color, this_ax, ec=box_color)\nbox_with_annot(0.2, 3.1, 2.7, 2.7, \"B\", box_color, this_ax, ec=box_color)\nbox_with_annot(0.1, 0.1, 2.8, 0.8, \"C\", box_color, this_ax, ec=box_color)\nbox_with_annot(4.1, 3.1, 6.7, 2.8, \"D\", box_color, this_ax, ec=box_color)\n\nplt.show()\n\ndel data_2","2806733e":"# Correlation between Machine Learning Experience and Programming Experience\n\n# Create a new df\ndata_3 = data[[\"Q6\", \"Q15\"]]\n\n# Create and populate a new column\ndata_3['cnt'] = data_3['Q15'].apply(fill_nan)\n\n# Fill nan values with 0\ndata_3['cnt'] = data_3['cnt'].fillna(0)\n\n# Change dtype to int8\ndata_3['cnt'] = data_3['cnt'].astype(\"int\")\n\n# Change the programming and machine learning experience columns types\ndata_3['Q15'] = data_3['Q15'].astype(cat_mlex_order)\ndata_3['Q6'] = data_3['Q6'].astype(cat_prex_rev_order)\n\n# Histogram data\nhist_data_q15 = data_3[\"Q15\"].sort_values()\nhist_data_q15.dropna(inplace=True)\nhist_data_q6 = data_3[\"Q6\"].astype(cat_prex_order).sort_values()\n\n# Group by programming experience and age\ndata_3 = data_3.groupby([\"Q6\", \"Q15\"]).agg({\"cnt\": sum})\n\n# Turn into correlation matrix\ndata_3 = data_3.unstack()\ndata_3 = data_3[\"cnt\"]\n\n# Heatmap and marginal histograms\nplt.style.use('seaborn-dark')\nf, ax = plt.subplots(nrows=2, ncols=2, figsize=(13.5,10.5), gridspec_kw={'height_ratios':[2,5], 'width_ratios':[1.2,5],'wspace':0.1, 'hspace':0.1}) # 10.8,8.4\nplt.suptitle(\"Fig.3.4.1 - Correlation between Machine Learning Experience and Programming Experience\", fontsize=font_large+font_margin, color=figure_title_color, ha=\"center\", y=1.01)\nthis_ax = ax[0,0]\nhide_axes(this_ax)\n\n# Machine learning histogram\nthis_ax = ax[0,1]\nbins = np.arange(10) - 0.5\nthis_ax.hist(hist_data_q15, bins=bins, facecolor=figure_title_color, edgecolor = \"white\", linewidth=1.5, rwidth=0.4)\nthis_ax.xaxis.tick_top()\nthis_ax.set_xticklabels([\"0\", \"0-1\", \"1-2\", \"2-3\", \"3-4\", \"4-5\", \"5-10\", \"10-20\", \"20+\"])\nthis_ax.tick_params(axis='x', labelrotation=0)\nthis_ax.xaxis.set_label_position(\"top\")\nthis_ax.set_ylim([0, 10000])\nthis_ax.set_xlabel('Machine Learning Experience (years)', fontsize=font_medium+font_margin, labelpad=10)\nthis_ax.set_ylabel('# of Responses', fontsize=font_medium+font_margin, labelpad=10)\n\n# Programming experience histogram\nthis_ax = ax[1,0]\nbins = np.arange(8) - 0.5\nthis_ax.hist(hist_data_q6, bins=bins, facecolor=subplot_title_color, edgecolor = \"white\", linewidth=1.5, orientation=u'horizontal', rwidth=0.5)\nthis_ax.set_yticklabels([\"0\", \"0-1\", \"1-3\", \"3-5\", \"5-10\", \"10-20\", \"20+\"])\nthis_ax.yaxis.set_label_position(\"left\")\nthis_ax.xaxis.tick_top()\nthis_ax.xaxis.set_label_position(\"top\")\nthis_ax.set_xlim([0, 8500])\nthis_ax.set_xlim(this_ax.get_xlim()[::-1])\nthis_ax.set_xlabel('# of Responses', fontsize=font_medium+font_margin, labelpad=10)\nthis_ax.set_ylabel('Programming Experience (years)', fontsize=font_medium+font_margin, labelpad=10)\n\n# Heatmap\nheatplot_1 = sns.heatmap(data_3, cmap=\"Blues\", linewidths=5, annot=True, fmt=\"d\", square=False, xticklabels=True, yticklabels=True, cbar=False)\nheatplot_1.set_xlabel(\"Machine Learning Experience (years)\",fontsize=font_medium+font_margin, labelpad=10)\nheatplot_1.set_ylabel(\"Programming Experience (years)\",fontsize=font_medium+font_margin, labelpad=10)\nheatplot_1.yaxis.set_label_position(\"right\")\nheatplot_1.set_yticklabels([\"20+\", \"10-20\", \"5-10\", \"3-5\", \"1-3\", \"0-1\", \"0\"])\nheatplot_1.set_xticklabels([\"0\", \"0-1\", \"1-2\", \"2-3\", \"3-4\", \"4-5\", \"5-10\", \"10-20\", \"20+\"])\nheatplot_1.tick_params(axis='x', which='major', labelrotation=0, labelbottom = True, bottom=True, top = True, labeltop=False, pad=10)\nheatplot_1.tick_params(axis='y', labelleft=False, labelright=True, labelrotation=0, pad=10)\n\nthis_ax = ax[1,1]\n\n# Line\nx = list(range(3, 9))\ny = list(range(6, 0, -1))\nthis_ax.plot(x, y, color=ln_color, linewidth=5, linestyle =\"--\", alpha=0.4)\n\n# Rectangles\nbox_with_annot(1.1, 4.1, 2.8, 1.8, \"A\", box_color, this_ax, ec=box_color)\nbox_with_annot(0.1, 0.1, 2.8, 0.8, \"B\", box_color, this_ax, ec=box_color)\nbox_with_annot(0.2, 0.2, 1.6, 6.6, \"C\", figure_title_color, this_ax, fill_value=True, fc=figure_title_color, alp=0.2)\nbox_with_annot(0.3, 5.1, 8.5, 1.7, \"D\", subplot_title_color, this_ax, fill_value=True, fc=subplot_title_color, alp=0.2)\n\nplt.show()\n\ndel data_3","a41c521d":"# Distribution of Each Selected Group by Gender\n\n# Filter the df by programming experience and create new dataframes\ny1_data = df_novices[[\"Q6\", \"Q2\"]]\ny20_data = df_veterans[[\"Q6\", \"Q2\"]]\n\n# Extablish x and y axes values\ny = ['Novices', 'Veterans']\nx_a_1 = y1_data[y1_data[\"Q2\"]==\"Man\"][\"Q2\"].count()\nx_a_20 = y20_data[y20_data[\"Q2\"]==\"Man\"][\"Q2\"].count()\n\nx_b_1 = y1_data[y1_data[\"Q2\"]==\"Woman\"][\"Q2\"].count()\nx_b_20 = y20_data[y20_data[\"Q2\"]==\"Woman\"][\"Q2\"].count()\n\nx_c_1 = y1_data[y1_data[\"Q2\"]==\"Prefer not to say\"][\"Q2\"].count()\nx_c_20 = y20_data[y20_data[\"Q2\"]==\"Prefer not to say\"][\"Q2\"].count()\n\nx_d_1 = y1_data[y1_data[\"Q2\"]==\"Nonbinary\"][\"Q2\"].count()\nx_d_20 = y20_data[y20_data[\"Q2\"]==\"Nonbinary\"][\"Q2\"].count()\n\nx_e_1 = y1_data[y1_data[\"Q2\"]==\"Prefer to self-describe\"][\"Q2\"].count()\nx_e_20 = y20_data[y20_data[\"Q2\"]==\"Prefer to self-describe\"][\"Q2\"].count()\n\nfig = go.Figure()\nfig.add_trace(go.Bar(y=y, x=[x_a_1, x_a_20], name='Man', orientation='h', marker=dict(color=ln_color), width=0.5)) # \"#A8B4FF\"\nfig.add_trace(go.Bar(y=y, x=[x_b_1, x_b_20], name='Woman', orientation='h', marker=dict(color=box_color), width=0.5)) # \"#F9463C\"\nfig.add_trace(go.Bar(y=y, x=[x_c_1, x_c_20], name='Prefer not to say', orientation='h', marker=dict(color=subplot_title_color), width=0.5)) # \"#0989D3\"\nfig.add_trace(go.Bar(y=y, x=[x_d_1, x_d_20], name='Nonbinary', orientation='h', marker=dict(color=figure_title_color), width=0.5)) # \"#FFFF14\"\nfig.add_trace(go.Bar(y=y, x=[x_e_1, x_e_20], name='Prefer to self-describe', orientation='h', marker=dict(color=\"#00005A\"), width=0.5)) # \"#00AF13\"\n\n# Update layout\nfig.update_layout(\n    barmode='stack',\n    title={\n        'text': \"Fig.3.5.1 - Distribution of the Selected Groups by Gender\",\n        'y':1.0,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'\n    },\n    title_font_color=figure_title_color,\n    title_font_size=font_large,\n    xaxis=dict(\n        title='# of Responses',\n        titlefont_size=font_medium,\n        tickfont_size=font_small,\n        categoryorder='total descending'\n    ),    \n    yaxis=dict(\n        title='Programming Experience',\n        titlefont_size=font_medium,\n        tickfont_size=font_small,\n    ),\n    height=plot_height,\n    width=plot_width,\n    showlegend=True,\n    legend=dict(\n        orientation=\"h\",\n        yanchor=\"bottom\",\n        y=1,\n        xanchor=\"right\",\n        x=1,\n        font=dict(\n            size=font_small\n        )\n    )\n)\n\nfig.show()\n\n# Get response statistics\ndata_novices, data_veterans = prep_for_stats(\"Q2\")\nstats = get_stats(data_novices, data_veterans, \"Q2\")\ntitle_1 = \"Response Statistics for Gender (Q2):\"\ncreate_score_df(stats, title_1)\ndel data_novices\ndel data_veterans","76161495":"# QUESTION-4: Highest Level of Formal Education\n\ntarget_column = \"Q4\"\norientation = \"v\"\nfig_title = \"Fig.3.6.1 - Highest Formal Education Attained or to be Attained in the Next Two Years\"\nxaxis_title = 'Formal Education Levels'\nyaxis_title = \"% of the Selected Groups' Responses\"\ncategoryorder='total descending'\nvalue_type=\"percentage\"\nx_order = [\"No formal education past high school\",\n            \"Some college\/university study without earning a bachelor\u2019s degree\", \n            \"Bachelor\u2019s degree\", \n            \"Master\u2019s degree\",\n            \"Doctoral degree\",\n            \"Professional doctorate\",\n            \"I prefer not to answer\"\n          ]\n\naxis_ticks = [\"No formal education <br> past high school\",\n            \"Some college\/university <br> study without earning <br> a bachelor\u2019s degree\", \n            \"Bachelor\u2019s degree\", \n            \"Master\u2019s degree\",\n            \"Doctoral degree\",\n            \"Professional <br> doctorate\",\n            \"I prefer <br> not to answer\"\n             ]\n\nsuccess_factor = \"Doctoral degree\"\nfig, novices_score, veterans_score = group_bar_plot(target_column, orientation, fig_title, xaxis_title, yaxis_title, categoryorder, plot_height, plot_width, value_type, x_order, axis_ticks, success_factor)\n \nfig.update_xaxes(tickangle=30)\n    \nfig.show()\n\n# Get response statistics\ndata_novices, data_veterans = prep_for_stats(target_column)\nstats = get_stats(data_novices, data_veterans, target_column)\ntitle_1 = \"Response Statistics for Formal Education (Q4):\"\ncreate_score_df(stats, title_1)\ndel data_novices\ndel data_veterans\n\nnov_scores[\"Formal Education\"] = novices_score # FEL\nvet_scores[\"Formal Education\"] = veterans_score","9e037e37":"# QUESTION-5: The Title Most Similar to the Current Role of the Selected Groups' Members\n\ntarget_column = \"Q5\"\norientation = \"v\"\nfig_title = \"Fig.3.7.1 - The Title Most Similar to the Current Role\"\nxaxis_title = 'Titles'\nyaxis_title = \"# of the Selected Groups' Responses\"\ncategoryorder='total descending'\nvalue_type=\"absolute\"\n\nfig, novices_score, veterans_score = group_bar_plot(target_column, orientation, fig_title, xaxis_title, yaxis_title, categoryorder, plot_height, plot_width, value_type)\n\nfig.update_xaxes(tickangle=30)\n\nfig.show()\n\n# Get response statistics\ndata_novices, data_veterans = prep_for_stats(target_column)\nstats = get_stats(data_novices, data_veterans, target_column)\ntitle_1 = \"Response Statistics for the Current Role Title (Q5):\"\ncreate_score_df(stats, title_1)\ndel data_novices\ndel data_veterans","d95c135e":"# QUESTION-7: Programming Languages Used by the Selected Groups\n\n# Establish the question number and the number of options\nquestion_1 = \"7\"\noptions = 12\nother = \"yes\"\n\n# List the new names of columns\nnew_columns = [\"Python\", \"R\", \"SQL\", \"C\", \"C++\", \"Java\", \"Javascript\", \"Julia\", \"Swift\", \"Bash\", \"MATLAB\", \"None\", \"Other\"]\n\n# Add a new column and sum the number of new column\nnew_col_name = \"No_of_prog_lang\"\n\n# Get plot data\nq1_y1, q1_y20, q1_y_per_1_data, q1_y_per_20_data, stats, scores = prepare_data(question_1, new_columns, options, new_col_name, other=\"yes\")\n\nq1_y_per_1_data.sort_values(ascending=False, inplace=True)\nq1_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# Define plot features and build traces\ntrace1 = create_traces(q1_y1, fillcolor=\"#626EFA\", line_color='#303CC8', plot_type=\"Box\", name=\"Novices\")\ntrace2 = create_traces(q1_y20, fillcolor=\"#B00068\", line_color='#FF50B8', plot_type=\"Box\", name=\"Veterans\")\ntrace3 = create_traces(q1_y_per_1_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=True)\ntrace4 = create_traces(q1_y_per_20_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=True)\ntraces = [trace1, trace2, trace3, trace4]\n\n# Plot graph\nsubplot_titles = (\"Distribution of the <br> Programming <br> Languages Used\", \"Current Usage of the Programming Languages\")\nfig = make_subplots(rows=1, cols=2, column_widths=[0.2, 1], subplot_titles=subplot_titles)\ncols = [1, 1, 2, 2]\n\nfor index, trace in zip(cols, traces):\n    fig.add_trace(trace, row=1, col=index)\n\nxaxis_titles = [\"Programming <br> Experience\", \"Programming Languages\"]\nyaxis_titles = [\"# of the Programming Languages\", \"% of the Selected Groups' Responses\"]\nfig_title = \"Fig.3.8.1 - Programming Languages Used\"\nfig = box_bar_plot(fig, xaxis_titles, yaxis_titles, fig_title, cols=list(set(cols)))\n\nfig.update_xaxes(tickangle=30)\n\nfig.show()\n\n# Create statistics dataframe\ntitle_1 = \"Response Statistics for the Programming Languages Used (Q7):\"\ncreate_score_df(stats, title_1)\n\n# Get scores\nnov_scores[\"Programming Languages\"] = stats[\"Novices True %\"][0] # PRL\nvet_scores[\"Programming Languages\"] = stats[\"Veterans True %\"][0]","788d175e":"# QUESTION-8: Programming Languages Recommended by the Selected Groups to Learn First for Data Science\n\ntarget_column = \"Q8\"\norientation = \"v\"\nfig_title = \"Fig.3.9.1 - Programming Languages Recommended to Learn First for Data Science\"\nxaxis_title = 'Programming Languages'\nyaxis_title = \"% of the Selected Groups' Responses\"\ncategoryorder='total descending'\nvalue_type=\"percentage\"\n\nfig, _, _ = group_bar_plot(target_column, orientation, fig_title, xaxis_title, yaxis_title, categoryorder, plot_height, plot_width, value_type)\n        \nfig.show()\n\n# Get response statistics\ndata_novices, data_veterans = prep_for_stats(target_column)\nstats = get_stats(data_novices, data_veterans, target_column)\ntitle_1 = \"Response Statistics for the Recommended Programming Languages (Q8):\"\ncreate_score_df(stats, title_1)\ndel data_novices\ndel data_veterans","ce0bb03d":"# QUESTION-9: Integrated Development Environments (IDEs) Used by the Selected Groups\n\n# Establish the question number and the number of options\nquestion_1 = \"9\"\noptions = 12\nother = \"yes\"\n\n# List the new names of columns\nnew_columns = [\"JupyterLab\", \"RStudio\", \"Visual Studio\", \"VSCode\", \"PyCharm\", \"Spyder\", \n               \"Notepad++\", \"Sublime Text\", \"Vim, Emacs, <br> or similar\", \"MATLAB\", \"Jupyter Notebook\", \"None\", \"Other\"]\n              \n\n# Add a new column and sum the number of new column\nnew_col_name = \"No_of_ide\"\n\n# Get plot data\nq1_y1, q1_y20, q1_y_per_1_data, q1_y_per_20_data, stats, scores = prepare_data(question_1, new_columns, options, new_col_name, other=\"yes\")\nq1_y_per_1_data.sort_values(ascending=False, inplace=True)\nq1_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# Define plot features and build traces\ntrace1 = create_traces(q1_y1, fillcolor=\"#626EFA\", line_color='#303CC8', plot_type=\"Box\", name=\"Novices\")\ntrace2 = create_traces(q1_y20, fillcolor=\"#B00068\", line_color='#FF50B8', plot_type=\"Box\", name=\"Veterans\")\ntrace3 = create_traces(q1_y_per_1_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=True)\ntrace4 = create_traces(q1_y_per_20_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=True)\ntraces = [trace1, trace2, trace3, trace4]\n\n# Plot graph\nsubplot_titles = (\"Distribution of the <br> IDEs Used\", \"Current Usage of the Integrated <br> Development Environments\")\nfig = make_subplots(rows=1, cols=2, column_widths=[0.2, 1], subplot_titles=subplot_titles)\ncols = [1, 1, 2, 2]\n\nfor index, trace in zip(cols, traces):\n    fig.add_trace(trace, row=1, col=index)\n\nxaxis_titles = [\"Programming <br> Experience\", \"Integrated Development Environments\"]\nyaxis_titles = [\"# of the Integrated Development Environments Used\", \"% of the Selected Groups' Responses\"]\nfig_title = \"Fig.3.10.1 - Integrated Development Environments Used\"\nfig = box_bar_plot(fig, xaxis_titles, yaxis_titles, fig_title, cols=list(set(cols)))\n\nfig.update_xaxes(tickangle=30)\n\nfig.show()\n\n# Create statistics dataframe\ntitle_1 = \"Response Statistics for the IDEs Used (Q9):\"\ncreate_score_df(stats, title_1)\n\n# Get scores\nnov_scores[\"IDEs\"] = stats[\"Novices True %\"][0] # IDE\nvet_scores[\"IDEs\"] = stats[\"Veterans True %\"][0]","cf5950a5":"# Correlation between\n# QUESTION-7: Programming Languages Used by the Selected Groups\n# QUESTION-9: Integrated Development Environments (IDEs) Used by the Selected Groups\n\n# New column names\nnew_cls_a_1 = [\"Python\", \"R\", \"SQL\", \"C\", \"C++\", \"Java\", \"Javascript\", \"Julia\", \"Swift\", \"Bash\", \"MATLAB\", \"PRL_None\", \"PRL_Other\"]\n\nnew_cls_a_2 = [\"JupyterLab\", \"RStudio\", \"Visual Studio\", \"VSCode\", \"PyCharm\", \"Spyder\", \n               \"Notepad++\", \"Sublime Text\", \"Vim, Emacs,\\nor similar\", \"MATLAB\", \"Jupyter\\nNotebook\", \"IDE_None\", \"IDE_Other\"]\nx_label = \"Integrated Development Environments\"\ny_label = \"Programming Languages\"\n\n# Create two dataframes for novices and veterans\ndf_a_1, df_a_20 = create_df([\"Q7\", \"Q9\"], new_cls_a_1, new_cls_a_2)\n\ny_loc = 1.10\nheatmap_title_a = \"Fig.3.10.2.a - Programming Languages vs. Integrated Development Environments for Novices\"\nplot_heatmap(df_a_1, heatmap_title_a, cmap1, subplot_title_color, x_label, y_label, y_loc)\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(1,1))\nhide_axes(ax)\n\nheatmap_title_a = \"Fig.3.10.2.b - Programming Languages vs. Integrated Development Environments for Veterans\"\nplot_heatmap(df_a_20, heatmap_title_a, cmap2, figure_title_color, x_label, y_label, y_loc)","49b212ee":"# QUESTION-14: Visualization Libraries Used by the Selected Groups\n\n# Establish the question number and the number of options\nquestion_1 = \"14\"\noptions = 11\nother = \"yes\"\n\n# List the new names of columns\nnew_columns = [\"Matplotlib\", \"Seaborn\", \"Plotly \/ <br> Plotly Express\", \"Ggplot \/ ggplot2\", \"Shiny\",\n               \"D3js\", \"Altair\", \"Bokeh\", \"Geoplotlib\", \"Leaflet \/ Folium\", \"None\", \"Other\"]\n\n# Add a new column and sum the number of new column\nnew_col_name = \"No_of_vis_libs\"\n\n# Get plot data\nq1_y1, q1_y20, q1_y_per_1_data, q1_y_per_20_data, stats, scores = prepare_data(question_1, new_columns, options, new_col_name, other=\"yes\")\nq1_y_per_1_data.sort_values(ascending=False, inplace=True)\nq1_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# Define plot features and build traces\ntrace1 = create_traces(q1_y1, fillcolor=\"#626EFA\", line_color='#303CC8', plot_type=\"Box\", name=\"Novices\")\ntrace2 = create_traces(q1_y20, fillcolor=\"#B00068\", line_color='#FF50B8', plot_type=\"Box\", name=\"Veterans\")\ntrace3 = create_traces(q1_y_per_1_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=True)\ntrace4 = create_traces(q1_y_per_20_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=True)\ntraces = [trace1, trace2, trace3, trace4]\n\n# Plot graph\nsubplot_titles = (\"Distribution of the <br> Visualization <br> Libraries Used\", \"Current Usage of the Visualization Libraries\")\nfig = make_subplots(rows=1, cols=2, column_widths=[0.2, 1], subplot_titles=subplot_titles)\ncols = [1, 1, 2, 2]\n\nfor index, trace in zip(cols, traces):\n    fig.add_trace(trace, row=1, col=index)\n\nxaxis_titles = [\"Programming <br> Experience\", \"Visualization Libraries\"]\nyaxis_titles = [\"# of the Visualization Libraries Used\", \"% of the Selected Groups' Responses\"]\nfig_title = \"Fig.3.11.1 - Visualization Libraries Used\"\nfig = box_bar_plot(fig, xaxis_titles, yaxis_titles, fig_title, cols=list(set(cols)))\n\nfig.update_xaxes(tickangle=30)\n\nfig.show()\n\n# Create statistics dataframe\ntitle_1 = \"Response Statistics for the Visualization Libraries Used (Q14):\"\ncreate_score_df(stats, title_1)\n\n# Get scores\nnov_scores[\"Visualization Libraries\"] = stats[\"Novices True %\"][0] # VSL\nvet_scores[\"Visualization Libraries\"] = stats[\"Veterans True %\"][0]","0c995ea2":"# QUESTION-16: Machine Learning Frameworks Used by the Selected Groups\n\n# Establish the question number and the number of options\nquestion_1 = \"16\"\noptions = 17\nother = \"yes\"\n\n# List the new names of columns\nnew_columns = [\"Scikit-learn\", \"TensorFlow\", \"Keras\", \"PyTorch\", \"Fast.ai\", \"MXNet\", \"Xgboost\", \"LightGBM\", \"CatBoost\", \n               \"Prophet\", \"H2O3\", \"Caret\", \"Tidymodels\", \"JAX\", \"PyTorch Lightning\", \"Huggingface\", \"None\", \"Other\"\n              ]\n\n# Add a new column and sum the number of new column\nnew_col_name = \"No_ML_frameworks\"\n\n# Get plot data\nq1_y1, q1_y20, q1_y_per_1_data, q1_y_per_20_data, stats, scores = prepare_data(question_1, new_columns, options, new_col_name, other=\"yes\")\nq1_y_per_1_data.sort_values(ascending=False, inplace=True)\nq1_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# Define plot features and build traces\ntrace1 = create_traces(q1_y1, fillcolor=\"#626EFA\", line_color='#303CC8', plot_type=\"Box\", name=\"Novices\")\ntrace2 = create_traces(q1_y20, fillcolor=\"#B00068\", line_color='#FF50B8', plot_type=\"Box\", name=\"Veterans\")\ntrace3 = create_traces(q1_y_per_1_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=True)\ntrace4 = create_traces(q1_y_per_20_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=True)\ntraces = [trace1, trace2, trace3, trace4]\n\n# Plot graph\nsubplot_titles = (\"Distribution of the <br> ML Frameworks <br> Used\", \"Current Usage of the Machine Learning Frameworks\")\nfig = make_subplots(rows=1, cols=2, column_widths=[0.2, 1], subplot_titles=subplot_titles)\ncols = [1, 1, 2, 2]\n\nfor index, trace in zip(cols, traces):\n    fig.add_trace(trace, row=1, col=index)\n\nxaxis_titles = [\"Programming <br> Experience\", \"Machine Learning Frameworks\"]\nyaxis_titles = [\"# of the Machine Learning Frameworks Used\", \"% of the Selected Groups' Responses\"]\nfig_title = \"Fig.3.12.1 - Machine Learning Frameworks Used\"\nfig = box_bar_plot(fig, xaxis_titles, yaxis_titles, fig_title, cols=list(set(cols)))\n\nfig.update_xaxes(tickangle=30)\n\nfig.show()\n\n# Create statistics dataframe\ntitle_1 = \"Response Statistics for the Machine Learning Frameworks Used (Q16):\"\ncreate_score_df(stats, title_1)\n\n# Get scores\nnov_scores[\"ML Frameworks\"] = stats[\"Novices True %\"][0] # MLF\nvet_scores[\"ML Frameworks\"] = stats[\"Veterans True %\"][0]","85b1c087":"# QUESTION-17: Machine Learning Algorithms Used by the Selected Groups\n\n# Establish the question number and the number of options\nquestion_1 = \"17\"\noptions = 11\nother = \"yes\"\n\n# List the new names of columns\nnew_columns = [\"Linear or Logistic <br> Regression\", \"Decision Trees or <br> Random Forests\", \"Gradient Boosting <br> Machines\",\n                \"Bayesian <br> Approaches\", \"Evolutionary <br> Approaches\", \"Dense Neural <br> Networks\", \"Convolutional <br> Neural Networks\",\n                \"Generative Adversarial <br> Networks\", \"Recurrent Neural <br> Networks\", \"Transformer <br> Networks\", \"None\", \"Other\"]\n\n# Add a new column and sum the number of new column\nnew_col_name = \"No_ML_algorithms\"\n\n# Get plot data\nq1_y1, q1_y20, q1_y_per_1_data, q1_y_per_20_data, stats, scores = prepare_data(question_1, new_columns, options, new_col_name, other=\"yes\")\nq1_y_per_1_data.sort_values(ascending=False, inplace=True)\nq1_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# Define plot features and build traces\ntrace1 = create_traces(q1_y1, fillcolor=\"#626EFA\", line_color='#303CC8', plot_type=\"Box\", name=\"Novices\")\ntrace2 = create_traces(q1_y20, fillcolor=\"#B00068\", line_color='#FF50B8', plot_type=\"Box\", name=\"Veterans\")\ntrace3 = create_traces(q1_y_per_1_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=True)\ntrace4 = create_traces(q1_y_per_20_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=True)\ntraces = [trace1, trace2, trace3, trace4]\n\n# Plot graph\nsubplot_titles = (\"Distribution of the <br> ML Algorithms <br> Used\", \"Current Usage of the Machine Learning Algorithms\")\nfig = make_subplots(rows=1, cols=2, column_widths=[0.2, 1], subplot_titles=subplot_titles)\ncols = [1, 1, 2, 2]\n\nfor index, trace in zip(cols, traces):\n    fig.add_trace(trace, row=1, col=index)\n\nxaxis_titles = [\"Programming <br> Experience\", \"Machine Learning Algorithms\"]\nyaxis_titles = [\"# of the Machine Learning Algorithms Used\", \"% of the Selected Groups' Responses\"]\nfig_title = \"Fig.3.13.1 - Machine Learning Algorithms Used\"\nfig = box_bar_plot(fig, xaxis_titles, yaxis_titles, fig_title, cols=list(set(cols)))\n\nfig.update_xaxes(tickangle=90)\n\nfig.show()\n\n# Create statistics dataframe\ntitle_1 = \"Response Statistics for the Machine Learning Algorithms Used (Q17):\"\ncreate_score_df(stats, title_1)\n\n# Get scores\nnov_scores[\"ML Algorithms\"] = stats[\"Novices True %\"][0] # MLA\nvet_scores[\"ML Algorithms\"] = stats[\"Veterans True %\"][0]","256e832f":"# Correlation between\n# QUESTION-16: Machine Learning Frameworks Used by the Selected Groups\n# QUESTION-17: Machine Learning Algorithms Used by the Selected Groups\n\n# New column names\nnew_cls_b_1 = [\"Scikit-learn\", \"TensorFlow\", \"Keras\", \"PyTorch\", \"Fast.ai\", \"MXNet\", \"Xgboost\", \"LightGBM\", \"CatBoost\", \n               \"Prophet\", \"H2O3\", \"Caret\", \"Tidymodels\", \"JAX\", \"PyTorch Lightning\", \"Huggingface\", \"MLF_None\", \"MLF_Other\"]\n\nnew_cls_b_2 = [\"Linear or Logistic\\nRegression\", \"Decision Trees or\\nRandom Forests\", \"Gradient Boosting\\nMachines\",\n                \"Bayesian\\nApproaches\", \"Evolutionary\\nApproaches\", \"Dense Neural\\nNetworks\", \"Convolutional\\nNeural Networks\",\n                \"Generative\\nAdversarial Networks\", \"Recurrent\\nNeural Networks\", \"Transformer\\nNetworks\", \"MLA_None\", \"MLA_Other\"]\n\nx_label = \"Machine Learning Algorithms\"\ny_label = \"Machine Learning Frameworks\"\n\n# Create two dataframes for novices and veterans\ndf_b_1, df_b_20 = create_df([\"Q16\", \"Q17\"], new_cls_b_1, new_cls_b_2)\n\ny_loc = 1.16\nheatmap_title_b = \"Fig.3.13.2.a - Machine Learning Frameworks vs. Machine Learning Algorithms for Novices\"\nplot_heatmap(df_b_1, heatmap_title_b, cmap1, subplot_title_color, x_label, y_label, y_loc)\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(1,1))\nhide_axes(ax)\n\nheatmap_title_b = \"Fig.3.13.2.b - Machine Learning Frameworks vs. Machine Learning Algorithms for Veterans\"\nplot_heatmap(df_b_20, heatmap_title_b, cmap2, figure_title_color, x_label, y_label, y_loc)","0640d6a6":"# QUESTION-18: Computer Vision Methods Used by the Selected Groups\n\n# Establish the question number and the number of options\nquestion_1 = \"18\"\noptions = 6\nother = \"yes\"\n\n# List the new names of columns\nnew_columns = [\"General purpose image \/ <br> video tools\", \"Image segmentation <br> methods\", \"Object detection <br> methods\",\n                \"Image classification\", \"Generative Networks\", \"None\", \"Other\"]\n\n# Add a new column and sum the number of new column\nnew_col_name = \"No_comp_vis_meth\"\n\n# Get plot data\nq1_y1, q1_y20, q1_y_per_1_data, q1_y_per_20_data, stats, scores = prepare_data(question_1, new_columns, options, new_col_name, other=\"yes\")\nq1_y_per_1_data.sort_values(ascending=False, inplace=True)\nq1_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# Define plot features and build traces\ntrace1 = create_traces(q1_y1, fillcolor=\"#626EFA\", line_color='#303CC8', plot_type=\"Box\", name=\"Novices\")\ntrace2 = create_traces(q1_y20, fillcolor=\"#B00068\", line_color='#FF50B8', plot_type=\"Box\", name=\"Veterans\")\ntrace3 = create_traces(q1_y_per_1_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=True)\ntrace4 = create_traces(q1_y_per_20_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=True)\ntraces = [trace1, trace2, trace3, trace4]\n\n# Plot graph\nsubplot_titles = (\"Distribution of the <br> Computer Vision <br> Methods Used\", \"Current Usage of the Computer Vision Methods\")\nfig = make_subplots(rows=1, cols=2, column_widths=[0.2, 1], subplot_titles=subplot_titles)\ncols = [1, 1, 2, 2]\n\nfor index, trace in zip(cols, traces):\n    fig.add_trace(trace, row=1, col=index)\n\nxaxis_titles = [\"Programming <br> Experience\", \"Computer Vision Methods\"]\nyaxis_titles = [\"# of the Computer Vision Methods Used\", \"% of the Selected Groups' Responses\"]\nfig_title = \"Fig.3.14.1 - Computer Vision Methods Used\"\nfig = box_bar_plot(fig, xaxis_titles, yaxis_titles, fig_title, cols=list(set(cols)))\n\nfig.update_xaxes(tickangle=30)\n\nfig.show()\n\n# Create statistics dataframe\ntitle_1 = \"Response Statistics for the Computer Vision Methods Used (Q18):\"\ncreate_score_df(stats, title_1)\n\n# Get scores\nnov_scores[\"Computer Vision\"] = stats[\"Novices True %\"][0] # CVM\nvet_scores[\"Computer Vision\"] = stats[\"Veterans True %\"][0]","c657c76b":"# Correlation between\n# QUESTION-17: Machine Learning Algorithms Used by the Selected Groups\n# QUESTION-18: Computer Vision Methods Used by the Selected Groups\n\n# New column names\nnew_cls_c_1 = [\"Linear or Logistic\\nRegression\", \"Decision Trees or\\nRandom Forests\", \"Gradient Boosting\\nMachines\",\n                \"Bayesian\\nApproaches\", \"Evolutionary\\nApproaches\", \"Dense Neural\\nNetworks\", \"Convolutional\\nNeural Networks\",\n                \"Generative\\nAdversarial Networks\", \"Recurrent\\nNeural Networks\", \"Transformer\\nNetworks\", \"MLA_None\", \"MLA_Other\"]\n\nnew_cls_c_2 = [\"General purpose \\nimage\/video tools\", \"Image segmentation \\nmethods\", \"Object detection \\nmethods\",\n                \"Image \\nclassification\", \"Generative \\nNetworks\", \"CVM_None\", \"CVM_Other\"]\n\nx_label = \"Computer Vision Methods\"\ny_label = \"Machine Learning Algorithms\"\n\n# Create two dataframes for novices and veterans\ndf_c_1, df_c_20 = create_df([\"Q17\", \"Q18\"], new_cls_c_1, new_cls_c_2)\n\ny_loc = 1.16\nheatmap_title_c = \"Fig.3.14.2.a - Machine Learning Algorithms vs. Computer Vision Methods for Novices\"\nplot_heatmap(df_c_1, heatmap_title_c, cmap1, subplot_title_color, x_label, y_label, y_loc)\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(1,1))\nhide_axes(ax)\n\nheatmap_title_c = \"Fig.3.14.2.b - Machine Learning Algorithms vs. Computer Vision Methods for Veterans\"\nplot_heatmap(df_c_20, heatmap_title_c, cmap2, figure_title_color, x_label, y_label, y_loc)","7b109431":"# QUESTION-19: Natural Language Processing (NLP) Methods Used by the Selected Groups\n\n# Establish the question number and the number of options\nquestion_1 = \"19\"\noptions = 5\nother = \"yes\"\n\n# List the new names of columns\nnew_columns = [\"Word embeddings\/<br>vectors\", \"Encoder-decoder<br>models\", \"Contextualized<br>embeddings\", \n               \"Transformer<br>language models\", \"None\", \"Other\"]\n\n# Add a new column and sum the number of new column\nnew_col_name = \"No_nlp_meth\"\n\n# Get plot data\nq1_y1, q1_y20, q1_y_per_1_data, q1_y_per_20_data, stats, scores = prepare_data(question_1, new_columns, options, new_col_name, other=\"yes\")\nq1_y_per_1_data.sort_values(ascending=False, inplace=True)\nq1_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# Define plot features and build traces\ntrace1 = create_traces(q1_y1, fillcolor=\"#626EFA\", line_color='#303CC8', plot_type=\"Box\", name=\"Novices\")\ntrace2 = create_traces(q1_y20, fillcolor=\"#B00068\", line_color='#FF50B8', plot_type=\"Box\", name=\"Veterans\")\ntrace3 = create_traces(q1_y_per_1_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=True)\ntrace4 = create_traces(q1_y_per_20_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=True)\ntraces = [trace1, trace2, trace3, trace4]\n\n# Plot graph\nsubplot_titles = (\"Distribution of the <br> NLP Methods <br> Used\", \"Current Usage of the <br> Natural Language Processing Methods\")\nfig = make_subplots(rows=1, cols=2, column_widths=[0.2, 1], subplot_titles=subplot_titles)\ncols = [1, 1, 2, 2]\n\nfor index, trace in zip(cols, traces):\n    fig.add_trace(trace, row=1, col=index)\n\nxaxis_titles = [\"Programming <br> Experience\", \"Natural Language Processing Methods\"]\nyaxis_titles = [\"# of the NLP Methods Used\", \"% of the Selected Groups' Responses\"]\nfig_title = \"Fig.3.15.1 - Natural Language Processing Methods Used\"\nfig = box_bar_plot(fig, xaxis_titles, yaxis_titles, fig_title, cols=list(set(cols)))\n\nfig.update_xaxes(tickangle=30)\n\nfig.show()\n\n# Create statistics dataframe\ntitle_1 = \"Response Statistics for the Natural Language Processing Methods Used (Q19):\"\ncreate_score_df(stats, title_1)\n\n# Get scores\nnov_scores[\"NLP Methods\"] = stats[\"Novices True %\"][0] # NLP\nvet_scores[\"NLP Methods\"] = stats[\"Veterans True %\"][0]","ec07a5a2":"# Correlation between\n# QUESTION-17: Machine Learning Algorithms Used by the Selected Groups\n# QUESTION-19: Natural Language Processing (NLP) Methods Used by the Selected Groups\n\n# New column names\nnew_cls_d_1 = [\"Linear or Logistic\\nRegression\", \"Decision Trees or\\nRandom Forests\", \"Gradient Boosting\\nMachines\",\n                \"Bayesian\\nApproaches\", \"Evolutionary\\nApproaches\", \"Dense Neural\\nNetworks\", \"Convolutional Neural\\nNetworks\",\n                \"Generative\\nAdversarial Networks\", \"Recurrent\\nNeural Networks\", \"Transformer\\nNetworks\", \"MLA_None\", \"MLA_Other\"]\n\nnew_cls_d_2 = [\"Word embeddings\/\\nvectors\", \"Encoder-decoder\\nmodels\", \"Contextualized\\nembeddings\", \n               \"Transformer\\nlanguage models\", \"NLP_None\", \"NLP_Other\"]\n\nx_label = \"Natural Language Processing Methods\"\ny_label = \"Machine Learning Algorithms\"\n\n# Create two dataframes for novices and veterans\ndf_d_1, df_d_20 = create_df([\"Q17\", \"Q19\"], new_cls_d_1, new_cls_d_2)\n\ny_loc = 1.15\nheatmap_title_d = \"Fig.3.15.2.a - Machine Learning Algorithms vs. Natural Language Processing Methods for Novices\"\nplot_heatmap(df_d_1, heatmap_title_d, cmap1, subplot_title_color, x_label, y_label, y_loc)\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(1,1))\nhide_axes(ax)\n\nheatmap_title_d = \"Fig.3.15.2.b - Machine Learning Algorithms vs. Natural Language Processing Methods for Veterans\"\nplot_heatmap(df_d_20, heatmap_title_d, cmap2, figure_title_color, x_label, y_label, y_loc)","a2effcd4":"# Industry of Employment of the Selected Groups bar plot\ntarget_column = \"Q20\"    \norientation = \"v\"\nfig_title = \"Fig.3.16.1 - Industries of Employment\"\nxaxis_title = 'Industries'\nyaxis_title = \"% of the Selected Groups' Responses\"\ncategoryorder='total descending'\nvalue_type=\"percentage\"\n\nfig_1, _, _ = group_bar_plot(target_column, orientation, fig_title, xaxis_title, yaxis_title, categoryorder, plot_height*1.3, plot_width, value_type)\n\nfig_1.update_xaxes(tickangle=90)\n\nfig_1.show()\n\n# Get response statistics\ndata_novices, data_veterans = prep_for_stats(target_column)\nstats = get_stats(data_novices, data_veterans, target_column)\ntitle_1 = \"Response Statistics for the Industries of Employment (Q20):\"\ncreate_score_df(stats, title_1)\ndel data_novices\ndel data_veterans","197f1f1c":"# Employment of the Selected Groups by Industry and Company Size\n\n# Two subplots (one row, two columns)\nfig = make_subplots(rows=1, cols=2, column_widths=[0.9, 0.3], subplot_titles=(\"By Industry\", \"By Company Size\"))\n\n# Left side bar plot\ntarget_column_1 = \"Q20\"\norientation = \"v\"\nvalue_type=\"percentage\"\nx_order = None\n\n# Get plot data\nay1, ay20, _, _ = get_plot_data(target_column_1, orientation, value_type, x_order, success_factor=None)\nx = list(ay1.keys())  \nxb = ['Academics\/Education', 'Computers\/Technology', 'Other', 'Accounting\/Finance', 'Manufacturing\/Fabrication', \"Government\/Public Service\", \n 'Medical\/Pharmaceutical', 'Energy\/Mining', 'Retail\/Sales', 'Non-profit\/Service', 'Online Service\/<br>Internet-based Services', 'Marketing\/CRM', \n 'Shipping\/Transportation', 'Online Business\/<br>Internet-based Sales', 'Broadcasting\/Communications', 'Insurance\/Risk Assessment', \n 'Hospitality\/Entertainment\/Sports', 'Military\/Security\/Defense']\n\ny1 = list(ay1.values())\ny2 = []\nfor item in x:\n    y2.append(ay20[item])\n\n# Plot\nfig.add_trace(\n    go.Bar(x=xb,\n        y=y1,\n        name='Novices',\n        marker_color='#626EFA',\n        ),\n        row=1, col=1\n    )\n\nfig.add_trace(\n    go.Bar(x=xb,\n        y=y2,\n        name='Veterans',\n        marker_color='#B00068',\n        ),\n        row=1, col=1\n    )\n\n# Right side bar plot\ntarget_column_2 = \"Q21\"\n\n# Get plot data\nby1, by20, _, _ = get_plot_data(target_column_2, orientation, value_type, x_order, success_factor=None)\nbx = [\"0-49 employees\", \"50-249 employees\", \"250-999 employees\", \"1000-9,999 employees\", \"10,000 or more employees\"]\nx = \"0-49\", \"50-249\", \"250-999\", \"1000-9,999\", \"10,000+\"\ny1 = []  \ny2 = []\nfor item in bx:\n    y1.append(by1[item])\n    y2.append(by20[item])  \n    \n# Plot\nfig.add_trace(\n    go.Bar(x=x,\n        y=y1,\n        name='Novices',\n        marker_color='#626EFA',\n        showlegend=False,\n        ),\n    row=1, col=2\n    )\n\nfig.add_trace(\n    go.Bar(x=x,\n        y=y2,\n        name='Veterans',\n        marker_color='#B00068',\n        showlegend=False,\n        ),\n    row=1, col=2\n    )\n\n# Update subplot title font sizes\nfig.update_annotations(font=dict(size=font_medium, color=subplot_title_color), y=1.02)\n    \nfig.update_layout(\n        title={\n            'text': \"Fig.3.16.2 - Employment by Industry and Company Size\",\n            'y':1.0,\n            'x':0.5,\n            'xanchor': 'center',\n            'yanchor': 'top'\n        },\n        title_font_color=figure_title_color,\n        title_font_size=font_large,\n        xaxis=dict(\n            title=\"Industries\",\n            titlefont_size=font_medium,\n            tickfont_size=font_small,\n            tickangle=-60,\n        ),    \n        yaxis=dict(\n            title=\"% of the Selected Groups' Responses\",\n            titlefont_size=font_medium,\n            tickfont_size=font_small,\n        ),\n        xaxis2=dict(\n            title=\"Company Size by the <br> # of Employees\",\n            titlefont_size=font_medium,\n            tickfont_size=font_small,\n        ),\n        yaxis2=dict(\n            title=\"% of the Selected Groups' Responses\",\n            titlefont_size=font_medium,\n            tickfont_size=font_small,  \n        ),\n        showlegend=True,\n        legend=dict(\n            orientation=\"v\",\n            yanchor=\"bottom\",\n            y=1,\n            xanchor=\"left\",\n            x=-0.02,\n            font=dict(\n                size=font_small\n            )\n        ),\n        height=plot_height*1.30,\n        width=plot_width,\n    )\n\nfig.update_xaxes(tickangle=90)\n\nfig.show()\n\n# Get response statistics\ndata_novices, data_veterans = prep_for_stats(target_column_1)\nstats = get_stats(data_novices, data_veterans, target_column_1)\ntitle_1 = \"Response Statistics for the Industries of Employment (Q20):\"\ncreate_score_df(stats, title_1)\n\ndata_novices, data_veterans = prep_for_stats(target_column_2)\nstats = get_stats(data_novices, data_veterans, target_column_2)\ntitle_2 = \"Response Statistics for the Company Size (Q21):\"\ncreate_score_df(stats, title_2)\n\ndel data_novices\ndel data_veterans","56971c4d":"# Correlation between Industry and Company Size by Programmming Experience\n\n# Create a new df\ndata_novices = df_novices[[\"Q6\", \"Q20\", \"Q21\"]]\ndata_veterans = df_veterans[[\"Q6\", \"Q20\", \"Q21\"]]\n\n# Create a new column\ndata_novices['cnt'] = 1\ndata_veterans['cnt'] = 1\n\n# Change the company size column type\ndata_novices['Q21'] = data_novices['Q21'].astype(cat_com_size_order)\ndata_veterans['Q21'] = data_veterans['Q21'].astype(cat_com_size_order)   \n\n# Histogram data\nhist_data_q20_y1 = data_novices[\"Q20\"].dropna().sort_values()\nhist_data_q21_y1 = data_novices[\"Q21\"].dropna().sort_values()\nhist_data_q20_y20 = data_veterans[\"Q20\"].dropna().sort_values()\nhist_data_q21_y20 = data_veterans[\"Q21\"].dropna().sort_values()\n\n# Aggregate sum  \ndata_novices = data_novices.groupby([\"Q20\", \"Q21\"]).agg({\"cnt\": sum})\ndata_veterans = data_veterans.groupby([\"Q20\", \"Q21\"]).agg({\"cnt\": sum})\n\n# Turn into correlation matrix\ndata_novices = data_novices.unstack()\ndata_novices = data_novices[\"cnt\"]\ndata_veterans = data_veterans.unstack()\ndata_veterans = data_veterans[\"cnt\"]\n\n# Heatmap and marginal histograms\nplt.style.use('seaborn-dark')\nf, ax = plt.subplots(nrows=5, ncols=2, figsize=(12,36), gridspec_kw={'height_ratios':[1.1,8,0.7,1.1,8], 'width_ratios':[2,5],'wspace':0.1, 'hspace':0.1})\nplt.suptitle(\"Fig.3.16.3 - Correlation between Industry and Company Size by Programmming Experience\", fontsize=font_large+font_margin, color=figure_title_color, ha=\"center\", y=0.925)\nhide_axes(ax[0,0])\nhide_axes(ax[2,0])\nhide_axes(ax[2,0])\nhide_axes(ax[2,1])\nhide_axes(ax[3,0])\n\n# < 1 year programming experience\n# Company size histogram\nthis_ax = ax[0,1]\nbins = np.arange(6) - 0.5\nthis_ax.hist(hist_data_q21_y1, bins=bins, facecolor=figure_title_color, edgecolor = \"white\", linewidth=1.5, rwidth=0.4)\nthis_ax.set_title(label='Novices',fontsize=font_medium+font_margin, color='#626EFA', pad=20, verticalalignment='top')\nthis_ax.set_xticklabels([\"0-49\", \"50-249\", \"250-999\", \"1000-9,999\", \"10,000+\"])\nthis_ax.xaxis.set_label_position(\"top\")\nthis_ax.xaxis.tick_top()\nthis_ax.tick_params(which='major', labelrotation=0)\nthis_ax.set_ylim([0, 1200])\nthis_ax.set_xlabel('Company Size (# of Employees)', fontsize=font_medium+font_margin, labelpad=10)\nthis_ax.set_ylabel('# of Responses', fontsize=font_medium+font_margin, labelpad=10)\n\n# Industry histogram\nthis_ax = ax[1,0]\nbins = np.arange(19) - 0.5\nthis_ax.hist(hist_data_q20_y1, bins=bins, facecolor=subplot_title_color, edgecolor = \"white\", linewidth=1.5, orientation=u'horizontal', rwidth=0.7)\nthis_ax.yaxis.set_label_position(\"left\")\nthis_ax.xaxis.tick_top()\nthis_ax.xaxis.set_label_position(\"top\")\nthis_ax.set_xlim([0, 600])\nthis_ax.set_xlim(this_ax.get_xlim()[::-1])\nthis_ax.set_xlabel('# of Responses', fontsize=font_medium+font_margin, labelpad=10)\nthis_ax.set_ylabel('Industries', fontsize=font_medium+font_margin, labelpad=10)\n\n# Heatmap-1\nthis_ax = ax[1,1]\ndata_novices.sort_values(by=['Q20'], ascending=False, inplace=True)\nheatmap_1 = sns.heatmap(data_novices, cmap=cmap1, linewidths=5, annot=True, fmt=\"d\", square=False, xticklabels=True, yticklabels=True, cbar=False, ax=this_ax)\nheatmap_1.set_xticklabels([\"0-49\", \"50-249\", \"250-999\", \"1000-9,999\", \"10,000+\"])\nheatmap_1.set_xlabel(\"Company Size (# of Employees)\",fontsize=font_medium+font_margin, labelpad=10)\nheatmap_1.set_ylabel(\"Industries \",fontsize=font_medium+font_margin, labelpad=10)\nheatmap_1.yaxis.set_label_position(\"right\")\nheatmap_1.tick_params(axis='x', which='major', labelrotation=0, labelbottom = True, bottom=True, top = True, labeltop=False, pad=10)\nheatmap_1.tick_params(axis='y', labelleft=False, labelright=True, labelrotation=0, pad=10)\n\n# 20+ year programming experience\n# Company size histogram\nthis_ax = ax[3,1]\nbins = np.arange(6) - 0.5\nthis_ax.hist(hist_data_q21_y20, bins=bins, facecolor=figure_title_color, edgecolor = \"white\", linewidth=1.5, rwidth=0.4) # #8157A2\nthis_ax.set_title(label='Veterans',fontsize=font_medium+font_margin, color='#B00068', pad=20, verticalalignment='top')\nthis_ax.set_xticklabels([\"0-49\", \"50-249\", \"250-999\", \"1000-9,999\", \"10,000+\"])\nthis_ax.xaxis.set_label_position(\"top\")\nthis_ax.xaxis.tick_top()\n# this_ax.axes.xaxis.set_ticks([])\nthis_ax.tick_params(which='major', labelrotation=0)\nthis_ax.set_ylim([0, 1200])\nthis_ax.set_xlabel('Company Size (# of Employees)', fontsize=font_medium+font_margin, labelpad=10)\nthis_ax.set_ylabel('# of Responses', fontsize=font_medium+font_margin, labelpad=10)\n\n# Industry histogram\nthis_ax = ax[4,0]\nbins = np.arange(19) - 0.5\nthis_ax.hist(hist_data_q20_y20, bins=bins, facecolor=subplot_title_color, edgecolor = \"white\", linewidth=1.5, orientation=u'horizontal', rwidth=0.7)\nthis_ax.xaxis.tick_top()\n# this_ax.axes.yaxis.set_ticks([])\nthis_ax.xaxis.set_label_position(\"top\")\nthis_ax.yaxis.set_label_position(\"left\")\nthis_ax.set_xlim([0, 600])\nthis_ax.set_xlim(this_ax.get_xlim()[::-1])\nthis_ax.set_xlabel('# of Responses', fontsize=font_medium+font_margin, labelpad=10)\nthis_ax.set_ylabel('Industries', fontsize=font_medium+font_margin, labelpad=10)\n\n# Heatmap-2\nthis_ax = ax[4,1]\ndata_veterans.sort_values(by=['Q20'], ascending=False, inplace=True)\nheatmap_2 = sns.heatmap(data_veterans, cmap=cmap2, linewidths=5, annot=True, fmt=\"d\", square=False, xticklabels=True, yticklabels=True, cbar=False, ax=this_ax)\nheatmap_2.set_xticklabels([\"0-49\", \"50-249\", \"250-999\", \"1000-9,999\", \"10,000+\"])\nheatmap_2.set_xlabel(\"Company Size (# of Employees)\",fontsize=font_medium+font_margin, labelpad=10)\nheatmap_2.set_ylabel(\"Industries\",fontsize=font_medium+font_margin, labelpad=10)\nheatmap_2.yaxis.set_label_position(\"right\")\nheatmap_2.tick_params(axis='x', which='major', labelrotation=0, labelbottom = True, bottom=True, top = True, labeltop=False, pad=10)\nheatmap_2.tick_params(axis='y', labelleft=False, labelright=True, labelrotation=0, pad=10)\n\nf.tight_layout(pad=1.0)\n\n# Rectangles\nthis_ax = ax[1,1]\nbox_with_annot(0.1, 17.2, 0.8, 0.6, \"A\", box_color, this_ax, ec=box_color)\nbox_with_annot(0.1, 14.2, 4.7, 0.6, \"B\", box_color, this_ax, ec=box_color)\nbox_with_annot(0.1, 16.2, 4.7, 0.6, \"C\", box_color, this_ax, ec=box_color)\nbox_with_annot(0.1, 9.2, 4.7, 0.6, \"D\", box_color, this_ax, ec=box_color)\nbox_with_annot(0.1, 1.2, 0.8, 0.6, \"E\", box_color, this_ax, ec=box_color)\nbox_with_annot(0.1, 3.2, 0.8, 2.6, \"E\", box_color, this_ax, ec=box_color)\nbox_with_annot(0.1, 7.2, 0.8, 1.6, \"E\", box_color, this_ax, ec=box_color)\nbox_with_annot(4.1, 10.2, 0.7, 0.6, \"F\", box_color, this_ax, ec=box_color)\n\nthis_ax = ax[4,1]\nbox_with_annot(3.1, 17.2, 0.8, 0.6, \"A\", box_color, this_ax, ec=box_color)\nbox_with_annot(0.1, 14.2, 4.7, 0.6, \"B\", box_color, this_ax, ec=box_color)\nbox_with_annot(0.1, 16.2, 4.7, 0.6, \"C\", box_color, this_ax, ec=box_color)\nbox_with_annot(0.1, 9.2, 4.7, 0.6, \"D\", box_color, this_ax, ec=box_color)\nbox_with_annot(0.1, 3.2, 0.8, 2.6, \"E\", box_color, this_ax, ec=box_color)\nbox_with_annot(0.1, 8.2, 0.8, 0.6, \"E\", box_color, this_ax, ec=box_color)\nbox_with_annot(4.1, 6.2, 0.7, 0.6, \"F\", box_color, this_ax, ec=box_color)\nbox_with_annot(4.1, 10.2, 0.7, 0.6, \"F\", box_color, this_ax, ec=box_color)\nbox_with_annot(4.1, 13.2, 0.7, 0.6, \"F\", box_color, this_ax, ec=box_color)\nbox_with_annot(4.1, 15.2, 0.7, 0.6, \"F\", box_color, this_ax, ec=box_color)\n\nplt.show()\n\ndel data_novices\ndel data_veterans","dac0ada2":"# QUESTION-24: Important Work Activities Performed by the Selected Groups\n\n# Establish the question number and the number of options\nquestion_1 = \"24\"\noptions = 7\nother = \"yes\"\n\n# List the new names of columns\nnew_column_names = [\"Analyze and understand data to influence product or business decisions\",\n                \"Build and\/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data\",\n                \"Build prototypes to explore applying machine learning to new areas\",\n                \"Build and\/or run a machine learning service that operationally improves my product or workflows\",\n                \"Experimentation and iteration to improve existing ML models\",\n                \"Do research that advances the state of the art of machine learning\",\n                \"None\",\n                \"Other\"\n              ]\n\nnew_columns = [\"Analyze data (1)\", \"Build infrastructure (2)\", \"Build prototype (3)\", \n               \"Build ML service (4)\", \"Experimentation (5)\", \"Research (6)\", \"None (7)\", \"Other (8)\"]\n\n# Add a new column and sum the number of new column\nnew_col_name = \"imp_work_acts\"\n\n# Get plot data\nq1_y1, q1_y20, q1_y_per_1_data, q1_y_per_20_data, stats, scores = prepare_data(question_1, new_columns, options, new_col_name, other=\"yes\")\nq1_y_per_1_data.sort_values(ascending=False, inplace=True)\nq1_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# Define plot features and build traces\ntrace1 = create_traces(q1_y1, fillcolor=\"#626EFA\", line_color='#303CC8', plot_type=\"Box\", name=\"Novices\")\ntrace2 = create_traces(q1_y20, fillcolor=\"#B00068\", line_color='#FF50B8', plot_type=\"Box\", name=\"Veterans\")\ntrace3 = create_traces(q1_y_per_1_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=True)\ntrace4 = create_traces(q1_y_per_20_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=True)\ntraces = [trace1, trace2, trace3, trace4]\n\n# Plot graph\nsubplot_titles = (\"Distribution of the <br> Important Work <br> Activities\", \"Performance of the Important Activities at Work\")\nfig = make_subplots(rows=1, cols=2, column_widths=[0.2, 1], subplot_titles=subplot_titles)\ncols = [1, 1, 2, 2]\n\nfor index, trace in zip(cols, traces):\n    fig.add_trace(trace, row=1, col=index)\n\nxaxis_titles = [\"Programming <br> Experience\", \"Important Activities at Work\"]\nyaxis_titles = [\"# of the Important Activities Performed at Work\", \"% of the Selected Groups' Responses\"]\nfig_title = \"Fig.3.17.1 - Important Activities Performed at Work\"\nfig = box_bar_plot(fig, xaxis_titles, yaxis_titles, fig_title, cols=list(set(cols)))\n\nfig.update_xaxes(tickangle=30)\n\nfig.show()\n\nnew_columns = list(map(lambda x: x[:-3], new_columns))\ndata_dict = {'Tick Label': new_columns, 'Activities': new_column_names}\ndf_tick_label = pd.DataFrame.from_dict(data_dict)\ndf_tick_label.index += 1 \ndf_tick_label = df_tick_label.style.set_properties(**{'text-align': 'left', 'background-color': '#E0F3FF', 'color': 'black'}) # #D6E9FF\ndf_tick_label = df_tick_label.set_table_styles([dict(selector = 'th', props=[('text-align', 'left')])])\ndisplay(df_tick_label)\n\n# Create statistics dataframe\ntitle_1 = \"Response Statistics for the Important Activities Performed at Work (Q24):\"\ncreate_score_df(stats, title_1)\n\n# Get scores\nnov_scores[\"Important Work Acts\"] = stats[\"Novices True %\"][0] # IWA\nvet_scores[\"Important Work Acts\"] = stats[\"Veterans True %\"][0]","25aeab73":"# Industry of Employment of the Selected Groups bar plot\ntarget_column = \"Q26\"    \norientation = \"v\"\nfig_title = \"Fig.3.18.1 - USD Amount Spent on Machine Learning and\/or Cloud Computing Services\"\nxaxis_title = 'US$ Amount'\nyaxis_title = \"% of the Selected Groups' Responses\"\ncategoryorder='total descending'\nvalue_type=\"percentage\"\nx_order = [\"$0 ($USD)\", \"$1-$99\", \"$100-$999\", \"$1000-$9,999\", \"$10,000-$99,999\", \"$100,000 or more ($USD)\"]\n# x_axis_ticks = [\"USD 0\", \"USD 1 - USD 99\", \"USD 100 - USD 999\", \"USD 1000 - USD 9,999\", \"USD 10,000 - USD 99,999\", \"USD 100,000 or more USD\"]\nx_axis_ticks = [\"0\", \"1 - 99\", \"100 - 999\", \"1000 - 9,999\", \"10,000 - 99,999\", \"100,000+\"]\n\nfig_1, _, _ = group_bar_plot(target_column, orientation, fig_title, xaxis_title, yaxis_title, categoryorder, plot_height, plot_width, value_type, x_order, x_axis_ticks)\n\nfig_1.show()\n\n# Get response statistics\ndata_novices, data_veterans = prep_for_stats(target_column)\nstats = get_stats(data_novices, data_veterans, target_column)\ntitle_1 = \"Response Statistics for the USD Amount Spent on ML and\/or CCS (Q26):\"\ncreate_score_df(stats, title_1)\ndel data_novices\ndel data_veterans","82e08dde":"# QUESTION-39: Public Share Platforms Used by the Selected Groups for the Deployment of Data Analyses & Machine Learning Applications\n\n# Establish the question number and the number of options\nquestion_1 = \"39\"\noptions = 9\nother = \"yes\"\n\n# List the new names of columns\nnew_columns = [\"Plotly Dash\", \"Streamlit\", \"NBViewer\", \"GitHub\", \"Personal blog\", \"Kaggle\", \"Colab\", \"Shiny\", \"None\", \"Other\"]\n\n# Add a new column and sum the number of new column\nnew_col_name = \"No_pub_share_plats\"\n\n# Get plot data\nq1_y1, q1_y20, q1_y_per_1_data, q1_y_per_20_data, stats, _ = prepare_data(question_1, new_columns, options, new_col_name, other=\"yes\")\nq1_y_per_1_data.sort_values(ascending=False, inplace=True)\nq1_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# Define plot features and build traces\ntrace1 = create_traces(q1_y1, fillcolor=\"#626EFA\", line_color='#303CC8', plot_type=\"Box\", name=\"Novices\")\ntrace2 = create_traces(q1_y20, fillcolor=\"#B00068\", line_color='#FF50B8', plot_type=\"Box\", name=\"Veterans\")\ntrace3 = create_traces(q1_y_per_1_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=True)\ntrace4 = create_traces(q1_y_per_20_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=True)\ntraces = [trace1, trace2, trace3, trace4]\n\n# Plot graph\nsubplot_titles = (\"Distribution of the <br> Public Sharing <br> Platforms Used\", \"Current Usage of the Public Sharing Platforms\")\nfig = make_subplots(rows=1, cols=2, column_widths=[0.2, 1], subplot_titles=subplot_titles)\ncols = [1, 1, 2, 2]\n\nfor index, trace in zip(cols, traces):\n    fig.add_trace(trace, row=1, col=index)\n\nxaxis_titles = [\"Programming <br > Experience\", \"Public Sharing Platforms\"]\nyaxis_titles = [\"# of the Public Sharing Platforms Used\", \"% of the Selected Groups' Responses\"]\nfig_title = \"Fig.3.19.1 - Public Sharing Platforms Used\"\nfig = box_bar_plot(fig, xaxis_titles, yaxis_titles, fig_title, cols=list(set(cols)))\n\nfig.update_xaxes(tickangle=30)\n\nfig.show()\n\n# Create statistics dataframe\ntitle_1 = \"Response Statistics for the Public Sharing Platforms Used (Q39):\"\ncreate_score_df(stats, title_1)\n\n# Get scores\nnov_scores[\"Public Sharing\"] = stats[\"Novices True %\"][0] # PSP\nvet_scores[\"Public Sharing\"] = stats[\"Veterans True %\"][0]","5655148e":"# QUESTION-40: Data Science Education Platforms\n\n# Establish the question number and the number of options\nquestion_1 = \"40\"\noptions = 11\nother = \"yes\"\n\n# List the new names of columns\nnew_columns = [\"Coursera\", \"edX\", \"Kaggle Learn Courses\", \"DataCamp\", \"Fast.ai\", \"Udacity\", \"Udemy\", \"LinkedIn Learning\", \n               \"Cloud-certification <br> Programs\", \"University Courses <br> with a Degree\", \"None\", \"Other\"]\n\n# Add a new column and sum the number of new column\nnew_col_name = \"No_ds_edu_plats\"\n\n# Get plot data\nq1_y1, q1_y20, q1_y_per_1_data, q1_y_per_20_data, stats, scores = prepare_data(question_1, new_columns, options, new_col_name, other=\"yes\")\nq1_y_per_1_data.sort_values(ascending=False, inplace=True)\nq1_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# Define plot features and build traces\ntrace1 = create_traces(q1_y1, fillcolor=\"#626EFA\", line_color='#303CC8', plot_type=\"Box\", name=\"Novices\")\ntrace2 = create_traces(q1_y20, fillcolor=\"#B00068\", line_color='#FF50B8', plot_type=\"Box\", name=\"Veterans\")\ntrace3 = create_traces(q1_y_per_1_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=True)\ntrace4 = create_traces(q1_y_per_20_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=True)\ntraces = [trace1, trace2, trace3, trace4]\n\n# Plot graph\nsubplot_titles = (\"Distribution of the <br> Education <br> Platforms Used\", \"Current Usage of the Data <br> Science Education Platforms\")\nfig = make_subplots(rows=1, cols=2, column_widths=[0.2, 1], subplot_titles=subplot_titles)\ncols = [1, 1, 2, 2]\n\nfor index, trace in zip(cols, traces):\n    fig.add_trace(trace, row=1, col=index)\n\nxaxis_titles = [\"Programming <br> Experience\", \"Data Science Education Platforms\"]\nyaxis_titles = [\"# of the Data Science Education Platforms Used\", \"% of the Selected Groups' Responses\"]\nfig_title = \"Fig.3.20.1 - Data Science Education Platforms Used\"\nfig = box_bar_plot(fig, xaxis_titles, yaxis_titles, fig_title, cols=list(set(cols)))\n\nfig.update_xaxes(tickangle=45)\n\nfig.show()\n\n# Create statistics dataframe\ntitle_1 = \"Response Statistics for the Data Science Education Platforms Used (Q40):\"\ncreate_score_df(stats, title_1)\n\n# Get scores\nnov_scores[\"DS Education\"] = stats[\"Novices True %\"][0] # DSE\nvet_scores[\"DS Education\"] = stats[\"Veterans True %\"][0]","93cd5148":"# QUESTION-42: Favorite Media Sources on Data Science\n\n# Establish the question number and the number of options\nquestion_1 = \"42\"\noptions = 11\nother = \"yes\"\n\n# List the new names of columns\nnew_columns = [\"Twitter\", \"Email newsletters\", \"Reddit\", \"Kaggle\", \"Course Forums\", \"YouTube\", \n               \"Podcasts\", \"Blogs\", \"Journal Publications\", \"Slack Communities\", \"None\", \"Other\"]\n\n# Add a new column and sum the number of new column\nnew_col_name = \"No_fav_media_sources\"\n\n# Get plot data\nq1_y1, q1_y20, q1_y_per_1_data, q1_y_per_20_data, stats, scores = prepare_data(question_1, new_columns, options, new_col_name, other=\"yes\")\nq1_y_per_1_data.sort_values(ascending=False, inplace=True)\nq1_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# Define plot features and build traces\ntrace1 = create_traces(q1_y1, fillcolor=\"#626EFA\", line_color='#303CC8', plot_type=\"Box\", name=\"Novices\")\ntrace2 = create_traces(q1_y20, fillcolor=\"#B00068\", line_color='#FF50B8', plot_type=\"Box\", name=\"Veterans\")\ntrace3 = create_traces(q1_y_per_1_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=True)\ntrace4 = create_traces(q1_y_per_20_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=True)\ntraces = [trace1, trace2, trace3, trace4]\n\n# Plot graph\nsubplot_titles = (\"Distribution of the <br> Favorite Media <br> Sources Used\", \"Current Usage of the Favorite <br> Media Sources on Data Science\")\nfig = make_subplots(rows=1, cols=2, column_widths=[0.2, 1], subplot_titles=subplot_titles)\ncols = [1, 1, 2, 2]\n\nfor index, trace in zip(cols, traces):\n    fig.add_trace(trace, row=1, col=index)\n\nxaxis_titles = [\"Programming <br> Experience\", \"Favorite Media Sources on Data Science\"]\nyaxis_titles = [\"# of the Favorite Media Sources <br> on Data Science Used\", \"% of the Selected Groups' Responses\"]\nfig_title = \"Fig.3.21.1 - Favorite Media Sources on Data Science Used\"\nfig = box_bar_plot(fig, xaxis_titles, yaxis_titles, fig_title, cols=list(set(cols)))\n\nfig.update_xaxes(tickangle=30)\n\nfig.show()\n\n# Create statistics dataframe\ntitle_1 = \"Response Statistics for the Favorite Media Sources on Data Science Used (Q42):\"\ncreate_score_df(stats, title_1)\n\n# Get scores\nnov_scores[\"Favorite Media\"] = stats[\"Novices True %\"][0] # FMS\nvet_scores[\"Favorite Media\"] = stats[\"Veterans True %\"][0]","21eb2c0c":"# QUESTIONS 27-A and 27-B: Cloud Computing Platforms\n\n# 1st question\n# Establish the question number and the number of options\nquestion_1 = \"27_A\"\noptions = 11\nother = \"yes\"\n\n# List the new names of columns\nnew_columns = [\"Amazon Web Services\", \"Microsoft Azure\", \"Google Cloud Platform\", \"IBM Cloud \/ Red Hat\", \"Oracle Cloud\", \n                \"SAP Cloud\", \"Salesforce Cloud\", \"VMware Cloud\", \"Alibaba Cloud\", \"Tencent Cloud\", \"None\", \"Other\"]\n\n\n# Add a new column and sum the number of new column\nnew_col_name = \"No_cloud_comp_plat\"\n\n# Get plot data\nq1_y1, q1_y20, q1_y_per_1_data, q1_y_per_20_data, stats_1, scores = prepare_data(question_1, new_columns, options, new_col_name, other=\"yes\")\nq1_y_per_1_data.sort_values(ascending=False, inplace=True)\nq1_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# 2nd question\n# Establish the question number and the number of options\nquestion_2 = \"27_B\"\n\n# List the new names of columns\nnew_columns = [\"Amazon Web Services\", \"Microsoft Azure\", \"Google Cloud Platform\", \"IBM Cloud \/ Red Hat\", \"Oracle Cloud\", \n                \"SAP Cloud\", \"VMware Cloud\", \"Salesforce Cloud\", \"Alibaba Cloud\", \"Tencent Cloud\", \"None\", \"Other\"]\n\n# Get plot data\nq2_y1, q2_y20, q2_y_per_1_data, q2_y_per_20_data, stats_2, _ = prepare_data(question_2, new_columns, options, new_col_name, other=\"yes\")\nq2_y_per_1_data.sort_values(ascending=False, inplace=True)\nq2_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# Define plot features and build traces\ntrace1 = create_traces(q1_y1, fillcolor=\"#626EFA\", line_color='#303CC8', plot_type=\"Box\", name=\"Novices\")\ntrace2 = create_traces(q1_y20, fillcolor=\"#B00068\", line_color='#FF50B8', plot_type=\"Box\", name=\"Veterans\")\ntrace3 = create_traces(q1_y_per_1_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=False)\ntrace4 = create_traces(q1_y_per_20_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=False)\ntrace5 = create_traces(q2_y_per_1_data, x_ax=\"x3\", y_ax=\"y3\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=True)\ntrace6 = create_traces(q2_y_per_20_data, x_ax=\"x3\", y_ax=\"y3\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=True)\ntraces = [trace1, trace2, trace3, trace4, trace5, trace6]\n\n# Plot graph\nsubplot_titles = (\"Distribution of the <br> Cloud Computing <br> Platforms Used\", \n                  \"Current Usage of the <br> Cloud Computing Platforms\", \n                  \"Future Familiarity with the <br> Cloud Computing Platforms\")\nfig = make_subplots(rows=1, cols=3, column_widths=[0.2, 0.5, 0.5], subplot_titles=subplot_titles)\ncols = [1, 1, 2, 2, 3, 3]\n\nfor index, trace in zip(cols, traces):\n    fig.add_trace(trace, row=1, col=index)\n\nxaxis_titles = [\"Programming <br> Experience\", \"Cloud Computing Platforms\", \"Cloud Computing Platforms\"]\nyaxis_titles = [\"# of the Cloud Computing Platforms Used\", \"% of the Selected Groups' Responses\", \"% of the Selected Groups' Responses\"]\nfig_title = \"Fig.3.22.1 - Cloud Computing Platforms Currently Used and to be Known More in the Future\"\nfig = box_bar_plot(fig, xaxis_titles, yaxis_titles, fig_title, cols=list(set(cols)), orientation=\"h\", legendy=1.15)\nfig.update_yaxes(title_standoff = 3)\nfig.update_xaxes(tickangle=90)\n\nfig.show()\n\n# Get response statistics\ntitle_1 = \"Response Statistics for the Cloud Computing Platforms Used (Q27_A):\"\ncreate_score_df(stats_1, title_1)\n\ntitle_2 = \"Response Statistics for the Future Familiarity with the Cloud Computing Platforms (Q27_B):\"\ncreate_score_df(stats_2, title_2)\n\n# Get scores\nnov_scores[\"Cloud Computing\"] = stats_1[\"Novices True %\"][0] # CCP\nvet_scores[\"Cloud Computing\"] = stats_1[\"Veterans True %\"][0]","7daf9517":"# QUESTIONS 31-A and 31-B: Managed Machine Learning Products\n\n# 1st question\n# Establish the question number and the number of options\nquestion_1 = \"31_A\"\noptions = 9\nother = \"yes\"\n\n# List the new names of columns\nnew_columns = [\"Amazon SageMaker\", \"Azure ML Studio\", \"Google Cloud Vertex AI\", \n                \"DataRobot\", \"Databricks\", \"Dataiku\", \"Alteryx\", \"Rapidminer\", \"None\", \"Other\"]\n\n# Add a new column and sum the number of new column\nnew_col_name = \"No_ML_products\"\n\n# Get plot data\nq1_y1, q1_y20, q1_y_per_1_data, q1_y_per_20_data, stats_1, scores = prepare_data(question_1, new_columns, options, new_col_name, other=\"yes\")\nq1_y_per_1_data.sort_values(ascending=False, inplace=True)\nq1_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# 2nd question\n# Establish the question number and the number of options\nquestion_2 = \"31_B\"\n\n# Get plot data\nq2_y1, q2_y20, q2_y_per_1_data, q2_y_per_20_data, stats_2, _ = prepare_data(question_2, new_columns, options, new_col_name, other=\"yes\")\nq2_y_per_1_data.sort_values(ascending=False, inplace=True)\nq2_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# Define plot features and build traces\ntrace1 = create_traces(q1_y1, fillcolor=\"#626EFA\", line_color='#303CC8', plot_type=\"Box\", name=\"Novices\")\ntrace2 = create_traces(q1_y20, fillcolor=\"#B00068\", line_color='#FF50B8', plot_type=\"Box\", name=\"Veterans\")\ntrace3 = create_traces(q1_y_per_1_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=False)\ntrace4 = create_traces(q1_y_per_20_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=False)\ntrace5 = create_traces(q2_y_per_1_data, x_ax=\"x3\", y_ax=\"y3\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=True)\ntrace6 = create_traces(q2_y_per_20_data, x_ax=\"x3\", y_ax=\"y3\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=True)\ntraces = [trace1, trace2, trace3, trace4, trace5, trace6]\n\n# Plot graph\nsubplot_titles = (\"Distribution of the <br> Managed ML <br> Products Used\", \n                  \"Current Usage of the <br> Managed Machine Learning Products\", \n                  \"Future Familiarity with the <br> Managed Machine Learning Products\")\nfig = make_subplots(rows=1, cols=3, column_widths=[0.2, 0.5, 0.5], subplot_titles=subplot_titles)\ncols = [1, 1, 2, 2, 3, 3]\n\nfor index, trace in zip(cols, traces):\n    fig.add_trace(trace, row=1, col=index)\n\nxaxis_titles = [\"Programming <br> Experience\", \"Managed Machine Learning Products\", \"Managed Machine Learning Products\"]\nyaxis_titles = [\"# of the Managed ML Products Used\", \"% of the Selected Groups' Responses\", \"% of the Selected Groups' Responses\"]\nfig_title = \"Fig.3.23.1 - Managed ML Products Currently Used and to be Known More in the Future\"\nfig = box_bar_plot(fig, xaxis_titles, yaxis_titles, fig_title, cols=list(set(cols)), orientation=\"h\", legendy=1.15)\nfig.update_yaxes(title_standoff = 3)\nfig.update_xaxes(tickangle=90)\n\nfig.show()\n\n# Get response statistics\ntitle_1 = \"Response Statistics for the Managed ML Products Used (Q31_A):\"\ncreate_score_df(stats_1, title_1)\n\ntitle_2 = \"Response Statistics for the Future Familiarity with the Managed ML Products (Q31_B):\"\ncreate_score_df(stats_2, title_2)\n\n# Get scores\nnov_scores[\"Managed ML Products\"] = stats_1[\"Novices True %\"][0] # MML\nvet_scores[\"Managed ML Products\"] = stats_1[\"Veterans True %\"][0]","5bc5fdb3":"# QUESTIONS 32-A and 32-B: Big Data Products\n\n# 1st question\n# Establish the question number and the number of options\nquestion_1 = \"32_A\"\noptions = 20\nother = \"yes\"\n\n# List the new names of columns\nnew_columns = [\"MySQL\", \"PostgreSQL\", \"SQLite\", \"Oracle Database\", \"MongoDB\", \"Snowflake\", \"IBM Db2\", \"Microsoft SQL Server\", \"Microsoft Azure SQL Database\", \n               \"Microsoft Azure Cosmos DB\", \"Amazon Redshift\", \"Amazon Aurora\", \"Amazon RDS\", \"Amazon DynamoDB\", \"Google Cloud BigQuery\", \"Google Cloud SQL\", \n               \"Google Cloud Firestore\", \"Google Cloud BigTable\", \"Google Cloud Spanner\", \"None\", \"Other\"]\n\n# Add a new column and sum the number of new column\nnew_col_name = \"No_big_data_prods\"\n\n# Get plot data\nq1_y1, q1_y20, q1_y_per_1_data, q1_y_per_20_data, stats_1, scores = prepare_data(question_1, new_columns, options, new_col_name, other=\"yes\")\nq1_y_per_1_data.sort_values(ascending=False, inplace=True)\nq1_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# 2nd question\n# Establish the question number and the number of options\nquestion_2 = \"32_B\"\n\n# List the new names of columns\nnew_columns = [\"MySQL\", \"PostgreSQL\", \"SQLite\", \"Oracle Database\", \"MongoDB\", \"Snowflake\", \"IBM Db2\", \"Microsoft SQL Server\", \"Microsoft Azure SQL Database\", \n               \"Microsoft Azure Cosmos DB\", \"Amazon Redshift\", \"Amazon Aurora\", \"Amazon DynamoDB\", \"Amazon RDS\", \"Google Cloud BigQuery\", \"Google Cloud SQL\", \n               \"Google Cloud Firestore\", \"Google Cloud BigTable\", \"Google Cloud Spanner\", \"None\", \"Other\"]\n\n# Get plot data\nq2_y1, q2_y20, q2_y_per_1_data, q2_y_per_20_data, stats_2, _ = prepare_data(question_2, new_columns, options, new_col_name, other=\"yes\")\nq2_y_per_1_data.sort_values(ascending=False, inplace=True)\nq2_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# Define plot features and build traces\ntrace1 = create_traces(q1_y1, fillcolor=\"#626EFA\", line_color='#303CC8', plot_type=\"Box\", name=\"Novices\")\ntrace2 = create_traces(q1_y20, fillcolor=\"#B00068\", line_color='#FF50B8', plot_type=\"Box\", name=\"Veterans\")\ntrace3 = create_traces(q1_y_per_1_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=False)\ntrace4 = create_traces(q1_y_per_20_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=False)\ntrace5 = create_traces(q2_y_per_1_data, x_ax=\"x3\", y_ax=\"y3\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=True)\ntrace6 = create_traces(q2_y_per_20_data, x_ax=\"x3\", y_ax=\"y3\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=True)\ntraces = [trace1, trace2, trace3, trace4, trace5, trace6]\n\n# Plot graph\nsubplot_titles = (\"Distribution of the <br> Big Data <br> Products Used\", \"Current Usage of the <br> Big Data Products\", \"Future Familiarity with the <br> Big Data Products\")\nfig = make_subplots(rows=1, cols=3, column_widths=[0.2, 0.5, 0.5], subplot_titles=subplot_titles)\ncols = [1, 1, 2, 2, 3, 3]\n\nfor index, trace in zip(cols, traces):\n    fig.add_trace(trace, row=1, col=index)\n\nxaxis_titles = [\"Programming <br> Experience\", \"Big Data Products\", \"Big Data Products\"]\nyaxis_titles = [\"# of the Big Data Products Used\", \"% of the Selected Groups' Responses\", \"% of the Selected Groups' Responses\"]\nfig_title = \"Fig.3.24.1 - Big Data Products Currently Used and to be Known More in the Future\"\nfig = box_bar_plot(fig, xaxis_titles, yaxis_titles, fig_title, cols=list(set(cols)), orientation=\"h\", legendy=1.15)\nfig.update_yaxes(title_standoff = 3)\nfig.update_xaxes(tickangle=90)\n\nfig.update_layout(\n    height=plot_height,\n    width=plot_width*1.1\n)\n\nfig.show()\n\n# Get response statistics\ntitle_1 = \"Response Statistics for the Big Data Products Used (Q32_A):\"\ncreate_score_df(stats_1, title_1)\n\ntitle_2 = \"Response Statistics for the Future Familiarity with the Big Data Products Used (Q32_B):\"\ncreate_score_df(stats_2, title_2)\n\n# Get scores\nnov_scores[\"Big Data Products\"] = stats_1[\"Novices True %\"][0] # BDP\nvet_scores[\"Big Data Products\"] = stats_1[\"Veterans True %\"][0]","e97d00aa":"# QUESTIONS 34-A and 34-B: Business Intelligence Tools\n\n# 1st question\n# Establish the question number and the number of options\nquestion_1 = \"34_A\"\noptions = 16\nother = \"yes\"\n\n# List the new names of columns\nnew_columns = [\"Amazon QuickSight\", \"Microsoft Power BI\", \"Google Data Studio\", \"Looker\", \"Tableau\", \"Salesforce\", \"Tableau CRM\", \"Qlik\",\n                \"Domo\", \"TIBCO Spotfire\", \"Alteryx\", \"Sisense\", \"SAP Analytics Cloud\", \"Microsoft Azure Synapse\", \"Thoughtspot\", \"None\", \"Other\"]\n\n# Add a new column and sum the number of new column\nnew_col_name = \"No_bus_int_tools\"\n\n# Get plot data\nq1_y1, q1_y20, q1_y_per_1_data, q1_y_per_20_data, stats_1, scores = prepare_data(question_1, new_columns, options, new_col_name, other=\"yes\")\nq1_y_per_1_data.sort_values(ascending=False, inplace=True)\nq1_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# 2nd question\n# Establish the question number and the number of options\nquestion_2 = \"34_B\"\n\n# List the new names of columns\nnew_columns = [\"Microsoft Power BI\", \"Amazon QuickSight\", \"Google Data Studio\", \"Looker\", \"Tableau\", \"Salesforce\", \"Tableau CRM\", \"Qlik\",\n                \"Domo\", \"TIBCO Spotfire\", \"Alteryx\", \"Sisense\", \"SAP Analytics Cloud\", \"Microsoft Azure Synapse\", \"Thoughtspot\", \"None\", \"Other\"]\n\n# Get plot data\nq2_y1, q2_y20, q2_y_per_1_data, q2_y_per_20_data, stats_2, _ = prepare_data(question_2, new_columns, options, new_col_name, other=\"yes\")\nq2_y_per_1_data.sort_values(ascending=False, inplace=True)\nq2_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# Define plot features and build traces\ntrace1 = create_traces(q1_y1, fillcolor=\"#626EFA\", line_color='#303CC8', plot_type=\"Box\", name=\"Novices\")\ntrace2 = create_traces(q1_y20, fillcolor=\"#B00068\", line_color='#FF50B8', plot_type=\"Box\", name=\"Veterans\")\ntrace3 = create_traces(q1_y_per_1_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=False)\ntrace4 = create_traces(q1_y_per_20_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=False)\ntrace5 = create_traces(q2_y_per_1_data, x_ax=\"x3\", y_ax=\"y3\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=True)\ntrace6 = create_traces(q2_y_per_20_data, x_ax=\"x3\", y_ax=\"y3\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=True)\ntraces = [trace1, trace2, trace3, trace4, trace5, trace6]\n\n# Plot graph\nsubplot_titles = (\"Distribution of the <br> BI Tools Used\", \n                  \"Current Usage of the <br> Business Intelligence Tools\", \n                  \"Future Familiarity with the <br> Business Intelligence Tools\")\nfig = make_subplots(rows=1, cols=3, column_widths=[0.2, 0.5, 0.5], subplot_titles=subplot_titles)\ncols = [1, 1, 2, 2, 3, 3]\n\nfor index, trace in zip(cols, traces):\n    fig.add_trace(trace, row=1, col=index)\n\nxaxis_titles = [\"Programming <br> Experience\", \"Business Intelligence Tools\", \"Business Intelligence Tools\"]\nyaxis_titles = [\"# of the Business Intelligence Tools Used\", \"% of the Selected Groups' Responses\", \"% of the Selected Groups' Responses\"]\nfig_title = \"Fig.3.25.1 - Business Intelligence Tools Currently Used and to be Known More in the Future\"\nfig = box_bar_plot(fig, xaxis_titles, yaxis_titles, fig_title, cols=list(set(cols)), orientation=\"h\", legendy=1.15)\nfig.update_yaxes(title_standoff = 3)\nfig.update_xaxes(tickangle=90)\n\nfig.show()\n\n# Get response statistics\ntitle_1 = \"Response Statistics for the Business Intelligence Tools Used (Q34_A):\"\ncreate_score_df(stats_1, title_1)\n\ntitle_2 = \"Response Statistics for the Future Familiarity with the Business Intelligence Tools (Q34_B):\"\ncreate_score_df(stats_2, title_2)\n\n# Get scores\nnov_scores[\"Business Intelligence\"] = stats_1[\"Novices True %\"][0] # BIT\nvet_scores[\"Business Intelligence\"] = stats_1[\"Veterans True %\"][0]","906dc291":"# QUESTIONS 36-A and 36-B: Automated Machine Learning Tools\n\n# 1st question\n# Establish the question number and the number of options\nquestion_1 = \"36_A\"\noptions = 7\nother = \"yes\"\n\n# List the new names of columns\nnew_columns = [\"Automated data <br> augmentation\", \"Automated feature <br> engineering\/selection\", \"Automated model <br> selection\", \n               \"Automated model <br> architecture searches\", \"Automated hyperparameter <br> tuning\", \"Automation of full <br> ML pipelines\",\n               \"None\", \"Other\"]\n\n# Add a new column and sum the number of new column\nnew_col_name = \"No_aut_ml_tools\"\n\n# Get plot data\nq1_y1, q1_y20, q1_y_per_1_data, q1_y_per_20_data, stats_1, scores = prepare_data(question_1, new_columns, options, new_col_name, other=\"yes\")\nq1_y_per_1_data.sort_values(ascending=False, inplace=True)\nq1_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# 2nd question\n# Establish the question number and the number of options\nquestion_2 = \"36_B\"\n\n# Get plot data\nq2_y1, q2_y20, q2_y_per_1_data, q2_y_per_20_data, stats_2, _ = prepare_data(question_2, new_columns, options, new_col_name, other=\"yes\")\nq2_y_per_1_data.sort_values(ascending=False, inplace=True)\nq2_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# Define plot features and build traces\ntrace1 = create_traces(q1_y1, fillcolor=\"#626EFA\", line_color='#303CC8', plot_type=\"Box\", name=\"Novices\")\ntrace2 = create_traces(q1_y20, fillcolor=\"#B00068\", line_color='#FF50B8', plot_type=\"Box\", name=\"Veterans\")\ntrace3 = create_traces(q1_y_per_1_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=False)\ntrace4 = create_traces(q1_y_per_20_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=False)\ntrace5 = create_traces(q2_y_per_1_data, x_ax=\"x3\", y_ax=\"y3\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=True)\ntrace6 = create_traces(q2_y_per_20_data, x_ax=\"x3\", y_ax=\"y3\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=True)\ntraces = [trace1, trace2, trace3, trace4, trace5, trace6]\n\n# Plot graph\nsubplot_titles = (\"Distribution of the <br> Automated ML <br> Tools Used\", \n                  \"Current Usage of the Automated <br> Machine Learning Tools\", \n                  \"Future Familiarity with the <br> Automated Machine Learning Tools\")\nfig = make_subplots(rows=1, cols=3, column_widths=[0.2, 0.5, 0.5], subplot_titles=subplot_titles)\ncols = [1, 1, 2, 2, 3, 3]\n\nfor index, trace in zip(cols, traces):\n    fig.add_trace(trace, row=1, col=index)\n\nxaxis_titles = [\"Programming <br> Experience\", \"Automated Machine Learning Tools\", \"Automated Machine Learning Tools\"]\nyaxis_titles = [\"# of the Automated ML Tools Used\", \"% of the Selected Groups' Responses\", \"% of the Selected Groups' Responses\"]\nfig_title = \"Fig.3.26.1 - Automated ML Tools Currently Used and to be Known More in the Future\"\nfig = box_bar_plot(fig, xaxis_titles, yaxis_titles, fig_title, cols=list(set(cols)), orientation=\"h\", legendy=1.15)\nfig.update_yaxes(title_standoff = 3)\nfig.update_xaxes(tickangle=90)\n\nfig.show()\n\n# Get response statistics\ntitle_1 = \"Response Statistics for the Automated ML Tools Used (Q36_A):\"\ncreate_score_df(stats_1, title_1)\n\ntitle_2 = \"Response Statistics for the Future Familiarity with the Automated ML Tools (Q36_B):\"\ncreate_score_df(stats_2, title_2)\n\n# Get scores\nnov_scores[\"Automated ML\"] = stats_1[\"Novices True %\"][0] # AML\nvet_scores[\"Automated ML\"] = stats_1[\"Veterans True %\"][0]","dc867100":"# QUESTIONS 38-A and 38-B: Machine Learning Experiments\n\n# 1st question\n# Establish the question number and the number of options\nquestion_1 = \"38_A\"\noptions = 11\nother = \"yes\"\n\n# List the new names of columns\nnew_columns = [\"Neptune.ai\", \"Weights & Biases\", \"Comet.ml\", \"Sacred + Omniboard\", \"TensorBoard\",\n               \"Guild.ai\", \"Polyaxon\", \"ClearML\", \"Domino Model Monitor\", \"MLflow\", \"None\", \"Other\"]\n\n# Add a new column and sum the number of new column\nnew_col_name = \"No_ml_exp_tools\"\n\n# Get plot data\nq1_y1, q1_y20, q1_y_per_1_data, q1_y_per_20_data, stats_1, scores = prepare_data(question_1, new_columns, options, new_col_name, other=\"yes\")\nq1_y_per_1_data.sort_values(ascending=False, inplace=True)\nq1_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# 2nd question\n# Establish the question number and the number of options\nquestion_2 = \"38_B\"\n\n# Get plot data\nq2_y1, q2_y20, q2_y_per_1_data, q2_y_per_20_data, stats_2, _ = prepare_data(question_2, new_columns, options, new_col_name, other=\"yes\")\nq2_y_per_1_data.sort_values(ascending=False, inplace=True)\nq2_y_per_20_data.sort_values(ascending=False, inplace=True)\n\n# Define plot features and build traces\ntrace1 = create_traces(q1_y1, fillcolor=\"#626EFA\", line_color='#303CC8', plot_type=\"Box\", name=\"Novices\")\ntrace2 = create_traces(q1_y20, fillcolor=\"#B00068\", line_color='#FF50B8', plot_type=\"Box\", name=\"Veterans\")\ntrace3 = create_traces(q1_y_per_1_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=False)\ntrace4 = create_traces(q1_y_per_20_data, x_ax=\"x2\", y_ax=\"y2\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=False)\ntrace5 = create_traces(q2_y_per_1_data, x_ax=\"x3\", y_ax=\"y3\", fillcolor=\"#626EFA\", line_color='', plot_type=\"Vertical Bar\", name=\"Novices\", showlegend=True)\ntrace6 = create_traces(q2_y_per_20_data, x_ax=\"x3\", y_ax=\"y3\", fillcolor=\"#B00068\", line_color='', plot_type=\"Vertical Bar\", name=\"Veterans\", showlegend=True)\ntraces = [trace1, trace2, trace3, trace4, trace5, trace6]\n\n# Plot graph\nsubplot_titles = (\"Distribution of the <br> ML Experiment <br> Tools Used\", \n                  \"Current Usage of the Machine <br> Learning Experiment Tools\", \n                  \"Future Familiarity with the <br> Machine Learning Experiment Tools\")\nfig = make_subplots(rows=1, cols=3, column_widths=[0.2, 0.5, 0.5], subplot_titles=subplot_titles)\ncols = [1, 1, 2, 2, 3, 3]\n\nfor index, trace in zip(cols, traces):\n    fig.add_trace(trace, row=1, col=index)\n\nxaxis_titles = [\"Programming <br> Experience\", \"Machine Learning Experiment Tools\", \"Machine Learning Experiment Tools\"]\nyaxis_titles = [\"# of the ML Experiment Tools Used\", \"% of the Selected Groups' Responses\", \"% of the Selected Groups' Responses\"]\nfig_title = \"Fig.3.27.1 - ML Experiment Tools Currently Used and to be Known More in the Future\"\nfig = box_bar_plot(fig, xaxis_titles, yaxis_titles, fig_title, cols=list(set(cols)), orientation=\"h\", legendy=1.15)\nfig.update_yaxes(title_standoff = 3)\nfig.update_xaxes(tickangle=90)\n\nfig.show()\n\n# Get response statistics\ntitle_1 = \"Response Statistics for the ML Experiment Tools Used (Q38_A):\"\ncreate_score_df(stats_1, title_1)\n\ntitle_2 = \"Response Statistics for the Future Familiarity with the ML Experiment Tools (Q38_B):\"\ncreate_score_df(stats_2, title_2)\n\n# Get scores\nnov_scores[\"ML Experiment\"] = stats_1[\"Novices True %\"][0] # MLE\nvet_scores[\"ML Experiment\"] = stats_1[\"Veterans True %\"][0]","5219fa33":"# Supplementary questions\nsup_questions = [\"Q27_B\", \"Q29_B\", \"Q30_B\", \"Q31_B\", \"Q32_B\", \"Q34_B\", \"Q36_B\", \"Q37_B\", \"Q38_B\"]\n\nall_columns = extract_columns(sup_questions)\n\n# Add programming experience column\nall_columns.insert(0, \"Q6\")\n\n# Create a new dataframe with the filtered columns\ndata_plot = data[all_columns]\ndata_plot = data_plot[(data_plot[\"Q6\"] == \"< 1 years\") | (data_plot[\"Q6\"] == \"20+ years\")]\n\n# Replace values\ndata_plot[\"Q6\"].replace({\"< 1 years\": \"Novices\"}, inplace=True)\ndata_plot[\"Q6\"].replace({\"20+ years\": \"Veterans\"}, inplace=True)\n\n# Count column-wise all tools, products or platforms\ndata_plot[\"cnt\"] = data_plot.iloc[:,1:].count(axis=1)\n\n# Plot graph\nsubplot_titles = (\"Histogram of Novices' Future <br> Familiarity with the Products & Tools\", \n                  \"Histogram of Veterans' Future <br> Familiarity with the Products & Tools\")\n\nfig = make_subplots(rows=1, cols=2, column_widths=[0.6, 0.6], subplot_titles=subplot_titles)\n\nfig.add_trace(go.Histogram(\n    x=data_plot[data_plot[\"Q6\"]==\"Novices\"][\"cnt\"],\n    histnorm='percent',\n    name='Novices', \n    xbins=dict( \n        start=0.5,\n        end=80,\n        size=0.5\n    ),\n    marker_color='#626EFA',\n    showlegend=True),\n    row=1, col=1)\n\nfig.add_trace(go.Histogram(\n    x=data_plot[data_plot[\"Q6\"]==\"Veterans\"][\"cnt\"],\n    histnorm='percent',\n    name='Veterans',\n    xbins=dict(\n        start=0.5,\n        end=80,\n        size=0.5\n    ),\n    marker_color='#B00068',\n    showlegend=True), \n    row=1, col=2)\n\n# Update xaxis properties\nfig.update_xaxes(title_text=\"The Number of Tools, Platforms or Products\", titlefont_size=font_medium, tickfont_size=font_small, row=1, col=1)\nfig.update_xaxes(title_text=\"The Number of Tools, Platforms or Products\", titlefont_size=font_medium, tickfont_size=font_small, row=1, col=2)\n\n# Update yaxis properties\nfig.update_yaxes(title_text=\"Percentage of the Selected Groups' Members\", titlefont_size=font_medium, tickfont_size=font_small, row=1, col=1)\nfig.update_yaxes(title_text=\"Percentage of the Selected Groups' Members\", titlefont_size=font_medium, tickfont_size=font_small, row=1, col=2)\n\n# Update subplot title font sizes\nfig.update_annotations(font=dict(size=font_medium, color=subplot_title_color), y=1.02)\n\nfig.add_vrect(x0=0, x1=13, line_width=0, fillcolor=\"yellow\", opacity=0.4)\nfig.add_hrect(y0=0, y1=0.2, line_width=0, fillcolor=\"green\", opacity=0.2)\n\n\nfig.update_layout(\n    title={\n        'text': \"Fig.3.28.1 - Future Familiarity with the Tools, Platforms or Products\",\n        'y':1.0,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'\n    },\n    title_font_color=figure_title_color,\n    title_font_size=font_large,        \n    showlegend=False,\n    height=plot_height,\n    width=plot_width\n)\n\nfig.show()","857105ba":"def create_polar_data(plot_dict):\n\n    # Find polar directions\n    directions = list(plot_dict.keys())\n\n    # Find polar strength\n    scores = list(plot_dict.values())\n    scores = [round(score, 0) for score in scores]\n    \n    return directions, scores\n\n# Get data\nnov_directions, nov_scors = create_polar_data(nov_scores)\nvet_directions, vet_scors = create_polar_data(vet_scores)\n\nfig = make_subplots(rows=2, cols=1, specs=[[{'type': 'polar'}]*1]*2)\n\n# Polar plot for Novices\nfig.add_trace(go.Barpolar( \n        r=nov_scors, \n        theta=nov_directions, \n        marker_color=[\"#626EFA\", \"#121EAA\", \"#1C28B4\", \"#2632BE\", \"#303CC8\", \"#3A46D2\", \"#4450DC\", \"#4E5AE6\", \"#5864F0\", \n                    \"#6C78FF\", \"#7682FF\", \"#808CFF\", \"#8A96FF\", \"#94A0FF\", \"#9EAAFF\", \"#A8B4FF\", \"#B2BEFF\", \"#BCC8FF\", \"#C6D2FF\"], #\"#0814A0\"\n        name=\"Novices\",\n        showlegend=True\n        ),\n    row=1, col=1)\n\n# Polar plot for Veterans\nfig.add_trace(go.Barpolar( \n        r=vet_scors, \n        theta=vet_directions, \n        marker_color=[\"#B00068\", \"#600018\", \"#6A0022\", \"#74002C\", \"#7E0036\", \"#880040\", \"#92004A\", \"#9C0054\", \"#A6005E\", \n                     \"#BA0A72\",\"#C4147C\", \"#CE1E86\", \"#D82890\", \"#E2329A\", \"#EC3CA4\", \"#F646AE\", \"#FF50B8\", \"#FF5AC2\", \"#FF64CC\"], # \"#56000E\"\n        name=\"Veterans\",\n        showlegend=True\n        ),\n    row=2, col=1)\n\nfig.update_layout(\n    title={\n        'text': \"Fig.4.1.1 - Data Science Skills Overview\",\n        'y':1.0,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'\n    },\n    title_font_color=figure_title_color,\n    title_font_size=font_large,        \n    showlegend=True,\n    legend=dict(\n        orientation=\"v\",\n        yanchor=\"bottom\",\n        y=1,\n        xanchor=\"right\",\n        x=1,\n        font=dict(\n            size=font_small\n        )\n    ),\n    height=plot_height*2.2,\n    width=plot_width\n)\n\nfig.show()","4c425367":"# Find the median salary for each data scientist\ndef find_median_salary(x):\n    \n    x = x.replace(\"$\", \"\").replace(\",\", \"\").replace(\">\", \"\").split(\"-\")\n    \n    if len(x) == 2:\n        salaries = [int(x[0]), int(x[1]) + 1]\n        median_salary = sum(salaries) \/ 2\n    else:\n        median_salary = int(x[0])\n    \n    return median_salary\n\n\n# Import purchasing power index data\ndf_ppi = pd.read_excel(\"..\/input\/comparison-of-worldwide-cost-of-living-2020\/Comparison of worldwide cost of living.xlsx\", sheet_name='Sheet1')\n\n# Get necessary columns\nquestions = [\"Q7\", \"Q9\", \"Q14\", \"Q16\", \"Q17\", \"Q18\", \"Q19\", \"Q24\", \"Q25\", \"Q27_A\", \n             \"Q31_A\", \"Q32_A\", \"Q34_A\", \"Q36_A\", \"Q38_A\", \"Q39\", \"Q40\", \"Q42\"]\nnec_columns = extract_columns(questions)\n\n# Insert extra columns\nextra_cols = [\"Q6\", \"Q3\", \"Q4\", \"Q5\", \"Q15\", \"Q20\", \"Q21\"]\nfor extra_col in range(len(extra_cols)):\n    nec_columns.insert(extra_col, extra_cols[extra_col])\n\n# Creata a new dataframe with necessary columns\ndf_salary = data[nec_columns]\n\n# Remove \"Currently not employed\" rows\ndf_salary = df_salary[df_salary[\"Q5\"] != \"Currently not employed\"]\n\n# Find the median salary\ndf_salary[\"Median_Salary\"] = df_salary[\"Q25\"].apply(find_median_salary)\ndf_salary[\"Median_Salary\"] = df_salary[\"Median_Salary\"].astype(\"float32\")\n\n# Drop NaN salary rows\ndf_salary.dropna(subset=['Q25'], inplace=True)\n\n# Remove countries that have no purchasing price index\nto_drop = ['Peru', 'Argentina', 'Taiwan', 'Belarus', 'Uganda', 'Ethiopia', 'I do not wish to disclose my location', 'Other']\ndf_salary[\"Q3\"] = df_salary[\"Q3\"].apply(lambda x: x.strip())\ndf_salary = df_salary[~df_salary['Q3'].isin(to_drop)]\n\n# Rename the column as \"country\" before the merge\ndf_salary = df_salary.rename(columns={\"Q3\": \"country\"})\n\n# Merge df_salary with df_ppi\ndf_salary = df_salary.merge(df_ppi[[\"country\", \"purchasing_power_index\"]], on='country', how='left')\n\n# Calculate PPP adjusted salary\ndf_salary[\"PPI_Adj_Salary\"] = (df_salary[\"Median_Salary\"] \/ 100) * df_salary[\"purchasing_power_index\"]\n\n# Remove salary column\ndf_salary.drop(\"Q25\", inplace=True, axis=1)\n\n# Find relevant columns for further analysis\ndata_columns = df_salary.columns.to_list()\ndata_columns = data_columns[7:-3]\n\n# Fill NaN values with 0, and non NaN values with 1\ndf_salary[data_columns] = df_salary[data_columns].notnull().astype('int')\n\n# Calculate the sum of group columns\ntarget_cols = [\"Q7\", \"Q9\", \"Q14\", \"Q16\", \"Q17\", \"Q18\", \"Q19\", \"Q24\", \"Q27_A\", \"Q31_A\", \"Q32_A\", \"Q34_A\", \"Q36_A\", \"Q38_A\", \"Q39\", \"Q40\", \"Q42\"]\nskills = [\"Programming Languages\", \"IDEs\", \"Visualization Libraries\", \"ML Frameworks\", \"ML Algorithms\", \"Computer Vision Methods\", \"NLP Methods\",\n         \"Important Work Activities\", \"Cloud Computing Platforms\", \"Managed ML Products\", \"Big Data Products\", \"Business Intelligence Tools\",\n         \"Automated ML Tools\", \"ML Experiments Tools\", \"Public Sharing Platforms\", \"Data Science Courses\", \"Favorite Media Sources\"]\nfor i, target_col in enumerate(target_cols):\n    group_cols = df_salary.columns[df_salary.columns.str.startswith(target_col)].to_list()\n    df_salary[skills[i]] = df_salary[group_cols].sum(axis=1)\n    \n# Create the dataframes for graphs    \n# 1st df\ndf_sal_analysis_1 = df_salary[[\"Q6\", \"Q4\", \"Q5\", \"Q15\", \"Q20\", \"Q21\", \"PPI_Adj_Salary\"]]\n\n# Rename some cell values\n#1\nold_degree_names = [\"No formal education past high school\", \"Some college\/university study without earning a bachelor\u2019s degree\"] \nnew_degree_names = [\"High school\", \"Incomplete college\"]\ndf_sal_analysis_1['Q4'] = df_sal_analysis_1['Q4'].replace(old_degree_names, new_degree_names)\n#2\nnew_com_size = [\"0-49\", \"50-249\", \"250-999\", \"1000-9,999\", \"10,000+\"]\ndf_sal_analysis_1['Q21'] = df_sal_analysis_1['Q21'].replace(com_size_categories, new_com_size)\n\n# Sort\nfedu_categories = [\"High school\", \"Incomplete college\", \"Bachelor\u2019s degree\", \"Master\u2019s degree\", \n                   \"Doctoral degree\", \"Professional doctorate\", \"I prefer not to answer\"]\ndf_sal_analysis_1['Q6'] = pd.Categorical(df_sal_analysis_1['Q6'], prex_categories)\ndf_sal_analysis_1['Q4'] = pd.Categorical(df_sal_analysis_1['Q4'], fedu_categories)\ndf_sal_analysis_1['Q15'] = pd.Categorical(df_sal_analysis_1['Q15'], mlex_categories)\ndf_sal_analysis_1['Q21'] = pd.Categorical(df_sal_analysis_1['Q21'], new_com_size)\ndf_sal_analysis_1.sort_values([\"Q6\", \"Q4\", \"Q15\", \"Q21\"], inplace=True)\n\n# Rename columns\nnew_col_names = {\"Q4\": \"Formal Education Degree\", \"Q5\": \"Current Role Title\", \"Q15\": \"ML Experience\", \"Q20\": \"Industry\", \"Q21\": \"Company Size\"}\ndf_sal_analysis_1 = df_sal_analysis_1.rename(columns=new_col_names)\n\n# 2nd df\nanalyzed_cols = df_salary.columns.to_list()[-18:]\nanalyzed_cols.insert(0, \"Q6\")\ndf_sal_analysis_2 = df_salary[analyzed_cols]\n\n# Group by programming experience\ndf_sal_analysis_2 = df_sal_analysis_2.groupby(\"Q6\").mean().reset_index().round(2)\n\n# Sort\ndf_sal_analysis_2['Q6'] = pd.Categorical(df_sal_analysis_2['Q6'], prex_categories)\ndf_sal_analysis_2.sort_values(\"Q6\", inplace=True)\n\n# y axis values\ny_feats = df_sal_analysis_2.columns.to_list()[2:]\ny_feats_2 = df_sal_analysis_1.columns.to_list()[1:-1]\nfor i, item in enumerate(y_feats_2):\n    y_feats.insert(i, item)\n    \n# Subplot title numeration by letters\nletters = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\"]\n\n# Plot\nplt.style.use('seaborn-darkgrid')\nf, ax = plt.subplots(nrows=11, ncols=2, figsize=(18,64))\ntitle = \"Fig.4.2.1 - Purchasing Power Index-Adjusted Monthly Salaries Depending on the \\nProgramming Experience, Education, Role, Industry, Company Size and Skill Level\"\nplt.suptitle(title, fontsize=font_large+2, color=figure_title_color, ha=\"center\", y=1)\n\n# Axis parameters\ndef plot_data(this_ax, subplot_title, y_label, x, y, color, size):\n    this_ax.scatter(x, y, s=size, c=color, cmap=\"cool\", alpha=0.8, edgecolors=\"white\", linewidth=2)\n    this_ax.set_title(subplot_title, fontsize=font_medium, color=subplot_title_color, y=1.05)\n    this_ax.tick_params(axis='x', which='major', labelsize=font_small, labelrotation=45)\n    this_ax.set_xlabel('Programming Experience', fontsize=font_medium, labelpad=10)\n    this_ax.set_ylabel(y_label, fontsize=font_medium, labelpad=10)\n    return this_ax\n\n# Plot data population\nfor i, y_feat in enumerate(y_feats):\n    p_row = (i \/\/ 2)\n    p_col = (i % 2)\n    this_ax = ax[p_row, p_col]\n    \n    # First 5 plots\n    if p_row * p_col < 2 and p_row + p_col < 3:\n        df_for_bubble = df_sal_analysis_1[[\"Q6\", y_feat, \"PPI_Adj_Salary\"]].groupby([\"Q6\", y_feat]).mean().reset_index().round(2)\n        x = df_for_bubble[\"Q6\"]\n        y = df_for_bubble[y_feat]\n        subplot_title = \"(\" + letters[i] + \")\" + \" Programming Experience and the \\n \" + y_feat\n        color1 = df_for_bubble[\"PPI_Adj_Salary\"]\n        size1 = df_for_bubble[\"PPI_Adj_Salary\"] \/ 100\n        this_ax = plot_data(this_ax, subplot_title, y_feat, x, y, color1, size1)\n        \n    # Rest of the plots\n    else:\n        x = df_sal_analysis_2[\"Q6\"]\n        y = df_sal_analysis_2[y_feat]\n        subplot_title = \"(\" + letters[i] + \")\" + \" Programming Experience and the \\nUse of \" + y_feat\n        color2 = df_sal_analysis_2[\"PPI_Adj_Salary\"]\n        size2 = df_sal_analysis_2[\"PPI_Adj_Salary\"] \/ 100\n        y_label = \"# of \" + y_feat\n        this_ax = plot_data(this_ax, subplot_title, y_label, x, y, color2, size2)\n    \nf.tight_layout(pad=2)\n\nplt.show()","dccbf8d7":"We find different implications when we look at the industry and company size together. Those are:<br>\n**3.16.9.** <span style=\"font-weight:bold; color:#626EFA\">Novices<\/span> work for <i>small companies<\/i> in <i>Academics\/Education<\/i>, whereas the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> work for <i>larger companies<\/i> in the same sphere (<i>Rectangle A<\/i>).<br>\n**3.16.10.** Both <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> and <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> work for either <i>small<\/i> or <i>large companies<\/i> in <i>Computers\/Technology<\/i>. The finding suggested by <b>3.16.8<\/b> for the general case is more prominent in <i>Computers\/Technology<\/i> (<i>Rectangle B<\/i>).<br>\n**3.16.11.** Both <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> and <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> work for either <i>small<\/i> or <i>larger companies<\/i> in <i>Accounting\/Finance<\/i> (<i>Rectangle C<\/i>).<br>\n**3.16.12.** In <i>Manufacturing\/Fabrication<\/i>, employment is more balanced across company sizes for <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>, while <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> fill such positions at <i>larger companies<\/i> (<i>Rectangle D<\/i>).<br>\n**3.16.13.** <i>Small companies<\/i> in <i>Online Service\/Internet-based Services<\/i>, <i>Online Business\/Internet-based Sales<\/i>, <i>Non-profit\/Service<\/i>, <i>Retail\/Sales<\/i>, <i>Medical\/Pharmaceutical<\/i>, and <i>Marketing\/CRM<\/i> businesses hire <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> more than other size companies (<i>Rectangle E<\/i>).<br>\n**3.16.14.** <i>Small companies<\/i> in <i>Online Service\/Internet-based Services<\/i>, <i>Online Business\/Internet-based Sales<\/i>, <i>Non-profit\/Service<\/i>, and <i>Marketing\/CRM<\/i> businesses hire <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> more than other size companies (<i>Rectangle E<\/i>).<br>\n**3.16.15.** <i>Large companies<\/i> in <i>Insurance\/Risk Assessment<\/i> hire both <span style=\"font-weight:bold; color:#778899\">selected groups<\/span> while they hire <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> in <i>Military\/Securit\/Defense<\/i>, <i>Enery\/Mining<\/i>, and <i>Broadcasting\/Communications<\/i> more than companies of other sizes (<i>Rectangle F<\/i>).<br>","70fc3d6d":"**3.8.1.** When we look at the percentage distribution(3) of the <span style=\"font-weight:bold; color:#778899\">selected groups<\/span> among the programming languages, we see that <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> are way ahead of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> in all languages except <i>Python<\/i>. Many years of experience must have allowed them to learn several programming languages. On average (median), the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> know three, the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> know two programming languages.<br>\n**3.8.2.** Only in <i>Python<\/i>, the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> surpass the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> by a small margin. Easiness and the growing popularity of <i>Python<\/i>[10] should be an ideal starting point for those who are new to programming and data science.<br>\n**3.8.3.** <span style=\"font-weight:bold; color:#B00068\">Veterans<\/span> by far excel in languages such as <i>SQL<\/i>, <i>Javascript<\/i>, and <i>Bash<\/i> compared to the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>.<br>\n**3.8.4.** In knowing <i>Other<\/i> languages, the comparative state of 28.8% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> versus 4% of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> <u>further confirms<\/u> the programming skill versatility of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>.<br>\n**3.8.5.** 97% of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> and 99% of <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> stated that they use at least one of the languages asked. Yet, the difference between the median numbers of the languages each group knows is only one. Because <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> have 20 more years of programming experience than <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>, their proficiency levels across the same languages should not be close.<br>","e9d988e5":"**3.16.1.** Above 20% of each group work in academics, yet probably they have different positions. Due to their younger age and beginner status in programming, the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> should be students mostly (refer to <b>3.7.1<\/b>).<br>\n**3.16.2.** From <b>3.6.2<\/b>, we know that 27.6% of <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> have doctoral degrees, and according to <b>3.7.3<\/b>, many have research scientist positions. These together may help <u>partially<\/u> explain the job role of <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> in <i>Academics\/Education<\/i>.<br>\n**3.16.3.** 26% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> (largest subgroup) work in <i>Computers\/Technology<\/i> firms.<br>\n**3.16.4.** <i>Academics\/Education<\/i> and <i>Computers\/Technology<\/i> are two top industries of employment for the <span style=\"font-weight:bold; color:#778899\">selected groups<\/span> by a large margin.<br>\n**3.16.5.** Interestingly, less than 5% of each group work in <i>Online Service\/Internet-based Services<\/i> and <i>Online Business\/Internet-based Sales<\/i>.<br>","6f588d67":"<a id=\"3.22.\"><\/a> <br>\n## <div class=\"subsection_title\">3.22. The top three choices of the veterans and novices are Amazon Web Services, Google Cloud Platform, and Microsoft Azure.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q27-A: Which of the following cloud computing platforms do you use on a regular basis?<\/div><div class=\"heading\">Q27-B: Which of the following cloud computing platforms do you hope to become more familiar with in the next 2 years?<\/div>","e5f45a68":"<a id=\"b\"><\/a> <br>\n# <div class=\"section_title\">5. Notes<\/div>\n\n(1) Survey response statistics are classified as follows: <b>True #<\/b> represents the total number of answers that don't involve \"<i>NaN<\/i>\" or \"<i>None<\/i>.\" <b>False #<\/b> represents the total number of replies that <i>only<\/i> include \"<i>NaN<\/i>\" or \"<i>None<\/i>\" responses.  <b>All #<\/b> gives the total number of <b>True<\/b> and <b>False<\/b> answers for each <span style=\"font-weight:bold; color:#778899\">selected group<\/span>. <b>True %<\/b> is the true response rate, which is found by dividing <b>True #<\/b> by <b>All #<\/b>. <b>Total True #<\/b> is the sum of <b>True #<\/b> counts of <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>. <b>Total False #<\/b> is the sum of <b>False #<\/b> counts of both groups.<br> \n(2) For obvious reasons, in this calculation, I had to remove the answers designated as <i>Other<\/i> or <i>I do not wish to disclose my location<\/i>.<br> \n(3) Percentages do not add up to one as the respondents can select more than one choice. This way, we can also compare the skill versatility levels of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>.<br>\n(4) To avoid wordiness and confusion, I had to develop little terminology here. The term <i>large company<\/i> denotes companies with 10,000 or more employees. <i>Larger companies<\/i> indicate companies that have more than 999 employees. Companies with 0-49 employees are <i>small companies<\/i>.<br>\n(5) Polar graphs used the <b>Novice True %<\/b> and <b>Veterans True %<\/b> as data. The calculation method of these statistics is described in (1). For example, if 52% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> selected <u>at least one<\/u> tool, product, or platform on any question included in the polar graph data, then the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>' skill grade on that topic would be plotted on the graph as 52.<br>\n(6) Purchasing power in each country is different, so I had to adjust the salaries. But there can be various indices out there to accomplish this task. The index[22] I have found doesn't involve some countries included in the survey. These are Peru, Argentina, Taiwan, Belarus, Uganda, and Ethiopia. I had to remove these countries in calculating the purchasing power index-adjusted salaries and plotting the related graphs, otherwise, the calculation wouldn't have been standard across all countries as I would use different indices together. Still, this shouldn't affect the general results. Alternatively, a safer approach for the reader could be to interpret the results for the included countries only.<br>\nA second issue, I also eliminated the responses given as <i>I do not wish to disclose my location<\/i> or <i>Other<\/i> as I cannot identify the particular country.<br>\n(7) I used the programming experience as the x-axis variable. To see the salary trend better, I took all levels of coding experience. y-axis variables are the questions and topics we have seen in the <b>Findings<\/b> section. The size of the bubbles gives the relative magnitudes of the adjusted salaries.<br> \n(8) I calculated the related y-axis parameter as the mean number of products, tools, or platforms known in each skill department by the respondents in each category of programming experience. For example, the mean number of IDEs known and used by the respondents of 3-5 years programming experience.<br>  ","ce4b4e1f":"<a id=\"3.17.\"><\/a> <br> \n## <div class=\"subsection_title\">3.17. Veterans have more activities to perform than novices.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q24: Select any activities that make up an important part of your role at work<\/div>","247b5581":"**3.10.6.** In the above graph, we need to focus on the darkest cells, which happen to be above 0.3. <span style=\"font-weight:bold; color:#778899\">Selected groups<\/span> generally use <i>R<\/i> with <i>R Studio<\/i> and <i>Python<\/i> with <i>Jupyter Notebook<\/i>.<br>\n**3.10.7.** <span style=\"font-weight:bold; color:#B00068\">Veterans<\/span> also use <i>Bash<\/i> in <i>Vim<\/i> or <i>Emacs<\/i>.<br>","c09faa07":"**3.1.4.** More precisely, the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> are 5881 in number while the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> are only 1860. In total, they are 7741 people.","32eb1961":"<a id=\"3.4.\"><\/a> <br>\n## <div class=\"subsection_title\">3.4. Kagglers usually learn coding to do machine learning.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q15: For how many years have you used machine learning methods?<\/div>","4f59bedd":"**3.18.1.** More than half of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> and over 30% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> spent zero USD on machine learning or cloud computing services in the past years.<br>\n**3.18.2.** <i>Up to USD 1000<\/i>, a higher percentage of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> spend on machine learning or cloud computing services than the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>. At the <i>USD 1000 and higher<\/i> budget levels, a higher percentage of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> spend on the said resources than the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>. This contrast should stem from the findings suggested by <b>3.7.1<\/b>, <b>3.17.1<\/b>, and <b>3.16.6<\/b>.<br>\n**3.18.3.** Half of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> and 6% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> are students or unemployed. Therefore, the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> overall have fewer means to spend than the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>.<br>\n**3.18.4.** Since over 40% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> work for larger companies and over 40% of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> work for small companies, the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> may access more financial resources to utilize for machine learning or cloud computing services.<br>","2582d5c0":"<a id=\"3.18.\"><\/a> <br>\n## <div class=\"subsection_title\">3.18. Veterans spend more than novices.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q26: Approximately how much money have you (or your team) spent on machine learning and\/or cloud computing services at home (or at work) in the past 5 years (approximate $USD)?<\/div>","9b06d75f":"**3.24.1.** We see the same trends suggested by <b>3.22.1<\/b> and <b>3.22.2<\/b>. A higher percentage of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> currently use big data products than the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>, but <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> hope to become familiar with them more than <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> percentage-wise.<br>\n**3.24.2.** 50% of <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and 15% of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> currently use at least one big data product. In the future, these ratios are expected to be %22 and %48, respectively.<br>\n**3.24.3.** <i>MySQL<\/i>, <i>Microsoft SQL Server<\/i>, and <i>PostgreSQL<\/i> are the top three used products of the <span style=\"font-weight:bold; color:#778899\">selected groups<\/span>.<br>\n**3.24.4.** <i>MySQL<\/i>, <i>MongoDB<\/i>, and <i>Google Cloud SQL<\/i> are the first three choices of the <span style=\"font-weight:bold; color:#778899\">selected groups<\/span> in the next two years.<br>\n**3.24.5.** Overall, <i>Amazon<\/i> products are less preferred than <i>Google<\/i> and <i>Microsoft<\/i> products both currently and soon. <i>AWS<\/i> may want to give some consideration to this.<br>","65a74154":"As we see in the six figures from <b>3.22<\/b> to <b>3.27<\/b>:<br>\n**3.28.1.** Current top choices do not change much for the future.<br>\n**3.28.2.** No more than 50% of each <span style=\"font-weight:bold; color:#778899\">selected group<\/span> currently uses or plans to learn more about the tools and products soon.<br>\n**3.28.3.** Generally speaking, no matter how many <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> currently use a product or tool, a lesser number of them hope to become familiar with such products or services in the next two years.<br>\n**3.28.4.** The number of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> who hope to get familiar with the tools and products is from 3 to 6 times as many as the number of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> who use them today.<br>\n**3.28.5.** <span style=\"font-weight:bold; color:#626EFA\">Novices<\/span> seem to have a lot of potential learning energy, of which activation reality needs further consideration.<br>\n**3.28.6.** Considering <b>3.28.3<\/b> and <b>3.28.4<\/b>, one may feel that the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> tend to be less ambitious and more realistic, and the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> tend to be more ambitious and less realistic. I will attempt to clarify this point more the next.<br>","a333b0ec":"<a id=\"3.20.\"><\/a> <br> \n## <div class=\"subsection_title\">3.20. Veterans are more eager to learn.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q40: On which platforms have you begun or completed data science courses?<\/div>","46a8c3a8":"**3.15.4.** In the above graph, we need to focus on the darkest cells, which happen to be above 0.3. <span style=\"font-weight:bold; color:#B00068\">Veterans<\/span> use <i>Recurrent Neural Networks<\/i> and <i>Transformer Networks<\/i> in almost all openly specified <i>NLP methods<\/i>.<br>\n**3.15.5.** <span style=\"font-weight:bold; color:#626EFA\">Novices<\/span> use the same algorithms in the same methods, except <i>Contextualized Embeddings<\/i>. Again, these findings are not clear-cut defined, but rather stated according to the threshold mentioned in <b>3.15.4<\/b>. It was an assumption I have made to make the most meaningful and safest interpretation possible.<br>","830ceafd":"**3.17.1.** On average, 89% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> perform important activities at work, whereas 43% of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> do the same. As <b>3.7.1<\/b> suggests, half of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> are either students or unemployed, while only 6% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> belong to those categories. Therefore, the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> have more chances to practice such activities at work than the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>.<br>\n**3.17.2.** <i>Analyzing and understanding the data<\/i> is the most frequent activity done by both groups.<br>\n**3.17.3.** <i>Building prototypes to explore applying machine learning to new areas<\/i> is the second most important activity of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>. <b>3.12.1<\/b>, <b>3.12.2<\/b> and <b>3.13.1<\/b> should have to do with this finding.<br>\n**3.17.4.** Experience also enables the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> to perform several activities together as suggested by the distribution graph on the left. 25% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> perform more than nearly half of the activities asked in the survey while 25% of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> fulfill up to 2 activities.<br>","d657f83f":"**3.20.1.** The top three platforms are <i>Coursera<\/i>, <i>Kaggle Learn Courses<\/i>, and <i>Udemy<\/i>.<br>\n**3.20.2.** Except for <i>Datacamp<\/i> and courses designated as <i>Other<\/i> where <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> have a little edge, the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> surpass the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> in all other platforms. This finding is an interesting fact as one may expect the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> to be more aggressive learners as they are in the early stage of learning.<br>\n**3.20.3.** 75% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> have participated in one or more courses, while 58% of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> have begun or completed at least one data science course.<br>\n**3.20.4.** <span style=\"font-weight:bold; color:#B00068\">Veterans<\/span> prefer <i>Coursera<\/i>, <i>University Degree Courses<\/i>, <i>edX<\/i>, and <i>Udacity<\/i> in particular compared to the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>.<br>\n**3.20.5.** 19.6% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and 11.7% of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> received university courses with a degree. If these courses are related to the formal education asked in <b>Question 4<\/b> of the survey and described in <b>3.6<\/b>, then most of the formal education received by the <span style=\"font-weight:bold; color:#778899\">selected groups<\/span> is in fields other than data science.","254e97c3":"<a id=\"3.24.\"><\/a> <br> \n## <div class=\"subsection_title\">3.24. MySQL is and remains to be the top choice of the selected groups.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q32-A: Which of the following big data products (relational databases, data warehouses, data lakes, or similar) do you use on a regular basis?<\/div><div class=\"heading\">Q32-B: Which of the following big data products (relational databases, data warehouses, data lakes, or similar) do you hope to become more familiar with in the next 2 years?<\/div>","8b7a9bbf":"**3.10.1.** When compared to the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>,  the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>, by a fair margin, use <i>Visual Studio<\/i>, <i>Notepad++<\/i>, <i>Vim<\/i>, and <i>Emacs<\/i> because these are older editors, which can even date back to 1976. It seems that people do not give up their old habits and continue to use these editors.<br>\n**3.10.2.** However, the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> also keep up with the new IDEs and use them almost as much as or sometimes more than the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> use, as we see in <i>JupyterLab<\/i>, <i>Visual Studio Code<\/i>, <i>Jupyter Notebook<\/i>, and <i>PyCharm<\/i>. This conclusion proves the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>' IDE versatility, which is further confirmed by the following finding.<br>\n**3.10.3.** The median number of IDEs used by the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> is three, while it is two for the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>. That is not surprising of programmers who have 20 years or more experience.<br>\n**3.10.4.** The <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> generally turn to relatively newly released IDEs such as <i>Jupyter Notebook<\/i>, <i>Visual Studio Code<\/i>, <i>PyCharm<\/i>, and <i>JupyterLab<\/i>.<br>\n**3.10.5.** Both <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> use <i>R Studio<\/i> to write in R. But, almost 2-3% of each group prefer another editor to use R programming language as <b>Figure 3.8.1<\/b> and <b>Figure 3.10.1<\/b> together suggest. In <b>Figure 3.10.1<\/b>, the percentage usage of R Studio by the <span style=\"font-weight:bold; color:#778899\">selected groups<\/span> is nearly 2-3% less for each than the percentage usage of R programming language by both <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> as depicted in <b>Figure 3.8.1<\/b>.<br>","dc064322":"**3.5.1.** Out of <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>, 1665 are <i>men<\/i>, 143 are <i>women<\/i>. Out of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>, 4366 are <i>men<\/i>, 1418 are <i>women<\/i>. Therefore, 24.5% of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> are <i>women<\/i>, while only 7.9% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> are <i>women<\/i>. The Kaggle survey suggests, now, more <i>women<\/i> go into programming than 20 years ago.<br>","1e522827":"**4.1.1.** The polar plots present the total skills and capability of each <span style=\"font-weight:bold; color:#778899\">selected group<\/span> and a summary of the previous findings(5). As the figure suggests, <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> have more advanced and versatile skills than the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>.<br>\n**4.1.2.** <span style=\"font-weight:bold; color:#626EFA\">Novices<\/span> can equal the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> in <i>IDEs<\/i> and <i>programming languages<\/i>, and come close in <i>visualization libraries<\/i>. However, <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> are well ahead of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> in other areas such as <i>machine learning frameworks<\/i>, <i>machine learning algorithms<\/i>, <i>important activities at work<\/i>, <i>data science education<\/i>, <i>favorite media source following<\/i>, <i>cloud computing<\/i>, <i>big data products<\/i>, <i>business intelligence<\/i>, <i>computer vision methods<\/i>, <i>NLP methods<\/i>, and <i>managed machine learning products<\/i>. <span style=\"font-weight:bold; color:#B00068\">Veterans<\/span> also have higher formal education than <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>.<br>\n**4.1.3.** This study takes each <span style=\"font-weight:bold; color:#778899\">selected group<\/span> as a <u>single body<\/u> and measures their collective capability by the percentage of the group population, which engages with at least one product in a skill category. This approach naturally leaves out the proficiency level in a particular skill of each member of that group because such a proficiency statistic is unavailable, and that works to the disadvantage of <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>. For example, if two <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> out of 10 know a programming language, and one <span style=\"font-weight:bold; color:#626EFA\">novice<\/span> out of 10 knows the same language, we measure the collective capability for each group as 20% and 10%, respectively. Yet, we don't know how well those two <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and one <span style=\"font-weight:bold; color:#626EFA\">novice<\/span> know and how long they use that programming language. We have a reason to claim that the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> must have excelled at that skill better than the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> as they spent 20 or more years in programming. Many <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> have longer experience time in several skills than <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>. This point is a critical element that we shouldn't overlook.<br>\nIn conclusion, the verdict is...<br> ","5e99970a":"**3.7.1.** As <b>3.3.5<\/b> and <b>3.6<\/b> suggest, the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>, as young programmers, are still <i>studying<\/i> (37.5%) or <i>searching for a job<\/i> (12.7%).<br>\n**3.7.2.** As expected from a <span style=\"font-weight:bold; color:#B00068\">veteran<\/span> coder, <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> are mostly <i>Software Engineers<\/i> (368 respondents).<br> \n**3.7.3.** Thanks to their experience, <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> also have heavy-weight roles such as <i>Research Scientist<\/i> or <i>Program\/Project Manager<\/i>.<br>\n**3.7.4.** Only 23 <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> are <i>Students<\/i>, and 87 <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> are <i>Unemployed<\/i>. Thus, safe to say, the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> are generally on the production front.<br>","199180aa":"<a id=\"c\"><\/a> <br>  \n# <div class=\"section_title\">6. Sources<\/div>\n\n[1] 2020. Enthusiast to Data Professional - What changes? Retrieved from Kaggle:https:\/\/www.kaggle.com\/spitfire2nd\/enthusiast-to-data-professional-what-changes<br>\n[2] 2021, October 14. State of Machine Learning and Data Science 2021, Kaggle.<br>\n[3] 2021, November 9. Population, total. Retrieved from The World Bank: https:\/\/data.worldbank.org\/indicator\/SP.POP.TOTL<br>\n[4] 2021, November 9. IS_608\/NanosatDB_munging\/Countries-Continents.csv. Retrieved from The GitHub: https:\/\/github.com\/dbouquin\/IS_608\/blob\/master\/NanosatDB_munging\/Countries-Continents.csv<br>\n[5] 2021, June 16. Coming soon: AWS launching new Region in Spain by mid-2022. Retrieved from AWS Public Sector Blog: https:\/\/aws.amazon.com\/blogs\/publicsector\/coming-soon-aws-launching-new-region-spain-2022\/<br>\n[6] 2019, October 28. Germany: New Immigration Acts to Attract and Retain Skilled Workers Published. Retrieved from Library of Congress: https:\/\/www.loc.gov\/item\/global-legal-monitor\/2019-10-28\/germany-new-immigration-acts-to-attract-and-retain-skilled-workers-published\/<br>\n[7] 2019, May 09. German government defends planned immigration laws. Retrieved from DW: https:\/\/www.dw.com\/en\/german-government-defends-planned-immigration-laws\/a-48676952<br>\n[8] 2020, June 25. Foreign Worker Visas Are the Tech Industry\u2019s Dirty Secret. Retrieved from Foreign Policy: https:\/\/foreignpolicy.com\/2020\/06\/25\/foreign-workers-visas-suspended-trump-tech-industry\/<br>\n[9] 2019. A story told through a heatmap. Retrieved from Kaggle: https:\/\/www.kaggle.com\/tkubacka\/a-story-told-through-a-heatmap<br>\n[10] 2020, November 4. Programming language Python's popularity: Ahead of Java for first time but still trailing C. Retrieved from ZDNet: https:\/\/www.zdnet.com\/article\/programming-language-pythons-popularity-ahead-of-java-for-first-time-but-still-trailing-c\/<br>\n[11] 2021, November 11. Project Jupyter. Retrieved from The Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Project_Jupyter<br>\n[12] 2021, November 11. Visual Studio Code. Retrieved from The Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Visual_Studio_Code<br>\n[13] 2021, November 11. Project Jupyter. Retrieved from The Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Project_Jupyter<br>\n[14] 2021, October 24. PyCharm. Retrieved from The Wikipedia: https:\/\/en.wikipedia.org\/wiki\/PyCharm<br>\n[15] 2021, October 27. Notepad++. Retrieved from The Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Notepad%2B%2B<br>\n[16] 2021, November 10. Microsoft Visual Studio. Retrieved from The Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Microsoft_Visual_Studio<br>\n[17] 2021, October 20. Vim (text editor). Retrieved from The Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Vim_(text_editor)<br> \n[18] 2021, October 22. Emacs. Retrieved from The Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Emacs<br>\n[19] 2021, October 25. Matplotlib. Retrieved from The Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Matplotlib<br>\n[20] 2021, November 12. D3.js. Retrieved from The Wikipedia: https:\/\/en.wikipedia.org\/wiki\/D3.js<br>\n[21] 2021, November 19. Cloud infrastructure services vendor market share worldwide from 4th quarter 2017 to 3rd quarter 2021. Retrieved from Statista: https:\/\/www.statista.com\/statistics\/967365\/worldwide-cloud-infrastructure-services-market-share-vendor\/<br>\n[22] 2021, November 16. Comparison of worldwide cost of living. Retrieved from WorldData.info: https:\/\/www.worlddata.info\/cost-of-living.php<br>","154612e9":"<a id=\"3.21.\"><\/a> <br>\n## <div class=\"subsection_title\">3.21. Veterans favor media sources more than the novices.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q42: Who\/what are your favorite media sources that report on data science topics?<\/div>","277a382a":"<a id=\"3.11.\"><\/a> <br>\n## <div class=\"subsection_title\">3.11. Veterans win in visualization libraries too.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q14: What data visualization libraries or tools do you use on a regular basis?<\/div>","168b51b4":"<a id=\"3.6.\"><\/a> <br>\n## <div class=\"subsection_title\">3.6. The highest level of formal education for the majority of novices is a bachelor's degree.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q4: What is the highest level of formal education that you have attained or plan to attain within the next 2 years?<\/div>","b3accefb":"<a id=\"3.28.\"><\/a> <br>\n## <div class=\"subsection_title\">3.28. Veterans tend to be less ambitious and more realistic, and the novices tend to be more ambitious and less realistic.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q27-A, Q27-B, Q29-B, Q30-B, Q31-A, Q31-B, Q32-A, Q32-B, Q34-A, Q34-B, Q36-A, Q36-B, Q37-B, Q38-A, Q38-B<\/div>","fbe8a706":"From this point on, we will analyze the A and B versions of some questions in the next six graphs. In all these graphs, we will encounter an asymmetric situation like black and white. That state will repeat itself in all these six cases. So, each time I will mention them briefly only. Now, let's start with the first one:<br>\n**3.22.1.** In all cloud computing platforms, <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> outperform novices. On average, 47% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> use at least one platform while only 15% of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> use them regularly.<br>\n**3.22.2.** However, when it comes to getting familiar with one of these cloud services in the next two years, we see a reverse situation. 51% of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> say they hope to become familiar with at least one cloud computing platform. On the other hand, this ratio is 24% for the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>. <span style=\"font-weight:bold; color:#626EFA\">Novices<\/span> are ahead of <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> in getting to know better all platforms in the near future.<br>\n**3.22.3.** It seems that <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> who have long experience in other skills and familiarity with cloud services are less motivated to get more involved in them in the coming years. Conversely, the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> want to close the knowledge gap.<br>\n**3.22.4.** Currently, <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> prefer <i>Amazon Web Services<\/i> the most at 28.5%, and <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> use <i>Google Cloud Platform<\/i> the most at 6.8%.<br>\n**3.22.5.** In the coming years, the top three preferred platforms don't change. But, <i>Google Cloud Platform<\/i> is the number one future choice of both <span style=\"font-weight:bold; color:#778899\">selected groups<\/span>. <i>Amazon Web Services<\/i> comes in second place by a low margin.<br>\n**3.22.6.** Unlike the kagglers' preferences, the current market shares of <i>Amazon<\/i>, <i>Microsoft<\/i>, and <i>Google<\/i> are 38%, 21%, and 8%, respectively. So, <i>Google<\/i> comes well behind the other two. <i>Oracle<\/i> and <i>IBM<\/i> have even lower market shares than <i>Google<\/i>[21].<br>\n**3.22.7.** Another point to note for <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> is their expected future percentage usage per current percentage usage is higher for <i>IBM<\/i> and <i>Oracle<\/i> than the other providers. That means that they hope to focus relatively more than today on these cloud computing services. Though this orientation, if realized, should not change the rankings, it could make <i>IBM<\/i> and <i>Oracle<\/i> more prominent among the kagglers.","8a26644c":"<a id=\"3.27.\"><\/a> <br> \n## <div class=\"subsection_title\">3.27. TensorBoard and MLflow are and remain to be the top choices of the selected groups.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q38-A: Do you use any tools to help manage machine learning experiments?<\/div><div class=\"heading\">Q38-B: In the next 2 years, do you hope to become more familiar with any of these tools for managing ML experiments?<\/div>","b481b25b":"<a id=\"3.10.\"><\/a> <br>\n## <div class=\"subsection_title\">3.10. Veterans use both older and new editors.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q9: Which of the following integrated development environments (IDE's) do you use on a regular basis?<\/div>","69513250":"![veteran_trophy_2.png](attachment:806464c6-fb07-4f02-9f6d-9c968e442acb.png)<br>\n<div align=\"center\"><font size=\"2\">Trophy icon from: https:\/\/icons8.com <\/font><\/div>","398fc2da":"**3.27.1.** We see the same trends suggested by <b>3.22.1<\/b> and <b>3.22.2<\/b>. A higher percentage of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> currently use machine learning experiment tools than the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>, but <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> hope to become familiar with them more than <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> percentage-wise.<br>\n**3.27.2.** 20% of <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and 5% of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> currently use at least one machine learning experiment tool. In the future, these ratios are expected to be %13 and %28, respectively.<br>\n**3.27.3.** For both groups, interest in these tools seems to be limited. On average, only 8.4% of the respondents in the <span style=\"font-weight:bold; color:#778899\">selected groups<\/span> use machine learning experiment tools, and 24% of them hope to know more about them in the next two years.<br>\n**3.27.4.** <i>TensorBoard<\/i> and <i>MLflow<\/i> are and remain to be the top choices of the <span style=\"font-weight:bold; color:#778899\">selected groups<\/span>.<br>\n**3.27.5.** <span style=\"font-weight:bold; color:#626EFA\">Novices<\/span> and <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> have an almost perfect match of the preference order for the ML experiment tools both currently and soon.<br>","53d8fdf0":"<a id=\"3.1.\"><\/a> <br>\n## <div class=\"subsection_title\">3.1. The distribution of programming experience is positively skewed.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div>","72481a9a":"<a id=\"3.16.\"><\/a> <br>\n## <div class=\"subsection_title\">3.16. Academics, computer\/technology, and small companies are top places of employment.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q20: In what industry is your current employer\/contract (or your most recent employer if retired)?<\/div>","fbee1b9e":"**3.6.1.** According to the Kaggle survey[2], 47.7% of the respondents have a <i>master's degree<\/i> as the highest educational level, the largest group among the respondents. 42.3% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> have <i>master's degrees<\/i>, and 47.4% of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> have <i>bachelor's degrees<\/i> as the highest level of formal education. As <b>3.3.5<\/b> suggests, the novices are young programmers, so they may not have come to a master study stage yet.<br>\n**3.6.2.** The age difference between the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> seems to show its effect on the highest level of formal education. <span style=\"font-weight:bold; color:#B00068\">Veterans<\/span> had more time to pursue further studies. 27.6% of them have <i>doctoral degrees<\/i> as the highest level of formal education, whereas the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> who completed their <i>Ph.D.<\/i> hold only 4.1%.<br>\n**3.6.3.** Kaggle survey results[2] reveal that 15% of the respondents have <i>doctoral degrees<\/i>, and 1.4% have <i>professional doctorates<\/i>. 27.6% and 3.3% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> have these degrees, respectively. Therefore, <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> are way above the average same education level.<br>\n**3.6.4.** On the other hand, 4.1% of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> have <i>doctoral degrees<\/i>, 1% have <i>professional doctorates<\/i>. Thus, <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> are well below the average of the same educational level. This situation may stem from age.<br>\n**3.6.5.** However, it would be premature to judge that age is a strong indicator of formal education. Other non-veteran and non-novice kagglers still had time to pursue a doctoral study as <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> but didn't do so, otherwise, general averages would not have stayed well below the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>' highest educational norm (See <b> 3.6.3<\/b>).<br>\n**3.6.6.** According to the Kaggle survey[2], <i>doctoral degrees<\/i> attained are decreasing straight from 2017 on. The same pattern occurs with the <i>master's degrees<\/i> from 2018. It is still too early to declare that the role of formal education in data science is falling among kagglers. Though, as we will see later, non-formal education platforms are popular to learn data science. In total, we could feel that as data science grows more popular and more people join the Kaggle community, the highest formal education can be shown percentage-wise to regress to lower academic levels. In addition, the proliferation and practicality of online courses facilitate filling the education gap.<br>","df7718df":"To interpret this graph in a more meaningful way, we need a piece of background information like the dates of release of the included IDEs. The major ones that are necessary for our analysis have the release dates as specified below:<br>\n\n| IDE Name                | Date of Release       |\n| :---------------------- |:--------------------  |\n| JupyterLab[11]          | February 20, 2018     |\n| Visual Studio Code[12]  | April 29, 2015        |\n| Jupyter Notebook[13]    | February 2015         |\n| PyCharm: [14]           | February 3, 2010      |\n| Notepad++[15]           | November 24, 2003     |\n| Visual Studio[16]       | 1997                  |\n| Vim[17]                 | November 2, 1991      |\n| Emacs[18]               | 1976                  |","7b2e0b59":"<a id=\"3.14.\"><\/a> <br>  \n## <div class=\"subsection_title\">3.14. It seems too early for novices to use computer vision methods.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q18: Which categories of computer vision methods do you use on a regular basis?<\/div>","8620ffe2":"**3.11.1.** <i>Matplotlib<\/i> and <i>Seaborn<\/i> are the most preferred visualization libraries by both groups.<br>\n**3.11.2.** A higher percentage of <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> use each visualization library than <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> though these libraries are not very old, that is, created after 2003[19].<br>\n**3.11.3.** <span style=\"font-weight:bold; color:#626EFA\">Novices<\/span> use <i>Matplotlib<\/i> and <i>Seaborn<\/i> slightly less than the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>, as the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> usually learn these packages first when they start the machine learning journey.<br>\n**3.11.4.** 9.2% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> use <i>D3.js<\/i>, while only 1.3% of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> use it. This difference can be explained by the fact that the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>, as <b>3.8.3<\/b> suggests, use <i>JavaScript<\/i> much more than the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> (29% versus 8%), and <i>D3.js<\/i> is a JavaScript library[20].<br>\n**3.11.5.** 24.6 % of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> use <i>Ggplot<\/i>, and 9% of them use <i>Shiny<\/i>. These percentages are 15% and 2.4% for the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>, respectively. As seen in <b>Figure 3.8<\/b>, 27.3% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> use R, while 18.9% of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> use this programming language. This finding might explain the preference of the <span style=\"font-weight:bold; color:#778899\">selected groups<\/span> for <i>Ggplot<\/i> and <i>Shiny<\/i>.<br>\n**3.11.6.** According to the Response Statistics, on average, 84% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> (<i>Veterans True %<\/i>) use visualization libraries, whereas 73% of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> (<i>Novices True %<\/i>) use them on a regular basis.","a1e1cfc7":"<a id=\"4\"><\/a> <br>  \n# <div class=\"section_title\">4. Summary and Conclusion<\/div>\n\n<p>Now, we can sum up and conclude the findings.<\/p>","9c491632":"<a id=\"4.1.\"><\/a> <br>\n## <div class=\"subsection_title\">4.1. Veterans are proved to be more skilled than novices in several defined departments.<\/div>","8a589563":"**3.26.1.** We see the same trends suggested by <b>3.22.1<\/b> and <b>3.22.2<\/b>. A higher percentage of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> currently use automated machine learning tools than the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>, but <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> hope to become familiar with them more than <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> percentage-wise.<br>\n**3.26.2.** 18% of <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and 6% of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> currently use at least one automated machine learning tool. In the future, these ratios are expected to be %18 and %36, respectively.<br>\n**3.26.3.** For both groups, interest in these tools seems to be limited. On average, only 8.8% of the respondents in the <span style=\"font-weight:bold; color:#778899\">selected groups<\/span> use automated machine learning tools, and 31.4% of them hope to know more about them in the next two years.<br>\n**3.26.4.** Both <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> want to learn more about <i>Automated model selection<\/i> and <i>Automation of full ML pipelines<\/i>.<br>","57aa44e2":"**3.2.17.** <span style=\"font-weight:bold; color:#B00068\">Veterans<\/span> outnumber the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> in countries such as the <i>United States<\/i>, <i>Australia<\/i>, <i>Norway<\/i>, <i>Spain<\/i>, <i>Switzerland<\/i>, <i>Netherlands<\/i>, and <i>Germany<\/i>.<br>\n**3.2.18.** Out of these countries, the <i>United States<\/i> is worth mentioning as the one that experiences a dramatic difference between the <span style=\"font-weight:bold; color:#778899\">selected groups<\/span>. In the <i>United States<\/i>, there are 457 <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>, while the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> are 341. According to the Kaggle Survey 2021, the percentage of <i>United States<\/i> kaggler data scientists has steadily been decreasing since 2017[2]. These findings may indicate comparatively less interest in coding and data science among American residents considering people in Asian countries.<br>\n**3.2.19.** Since the <i>United States<\/i> is the leader and high-performing in information technology with corporations like Amazon, Google, Microsoft, and others, they should meet the talent gap by hiring foreign specialists[8].","11fcd0d2":"# <a><span style=\"font-weight:bold; color:#00005A\">2021 Kaggle Machine Learning & Data Science Survey<\/span><a\/>","83fc9c03":"**3.25.1.** We see the same trends suggested by <b>3.22.1<\/b> and <b>3.22.2<\/b>. A higher percentage of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> currently use business intelligence tools than the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>, but <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> hope to become familiar with them more than <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> percentage-wise.<br>\n**3.25.2.** 30% of <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and 14% of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> currently use at least one business intelligence tool. In the future, these ratios are expected to be %18 and %43, respectively.<br>\n**3.25.3.** <i>Tableau<\/i>, <i>Microsoft Power BI<\/i>, and <i>Google Data Studio<\/i> are and remain to be the top choices of the <span style=\"font-weight:bold; color:#778899\">selected groups<\/span>. Again, <i>Amazon<\/i> falls behind its main competitors.<br>","1c5355f4":"**3.9.1.** There is almost unanimity over <i>Python<\/i> as the recommended language. No matter how many different programming languages <span style=\"font-weight:bold; color:#778899\">selected groups<\/span> use and how well they master them, they agree on <i>Python<\/i> as the recommended language to learn first for data science. This question also leaves out a <u>proven answer<\/u> for the second most important language to learn given that <i>SQL<\/i> and <i>R<\/i> are head-to-head as in the above graph.<br>\n**3.9.2.** In general, we can say <i>Python<\/i>, <i>SQL<\/i>, and <i>R<\/i> are good to learn for data science.<br>","c5b92770":"<a id=\"3.19.\"><\/a> <br>  \n## <div class=\"subsection_title\">3.19. Veterans share their work more than the novices.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q39: Where do you publicly share or deploy your data analysis or machine learning applications?<\/div>","81c6a504":"**3.16.6.** While over 40% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> work for <i>larger companies<\/i>(4), over 40% of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> work for <i>small companies<\/i>.<br>\n**3.16.7.** The largest single source of employment for the <span style=\"font-weight:bold; color:#778899\">selected groups<\/span> is <i>companies with 0-49 employees<\/i>.<br>\n**3.16.8.** On average, the lowest employment for both groups occurs at <i>companies with employees between 50-999<\/i>.<br>","fe876f22":"**3.2.1.** <span style=\"font-weight:bold; color:#626EFA\">Novices<\/span> mostly appear in less developed regions in the world.<br>\n**3.2.2.** The reddish spot in the <i>United States<\/i> denotes the concentration of <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> in this country.<br>\n**3.2.3.** Reddish spots where the <span style=\"font-weight:bold; color:#B00068\">veteran<\/span> population is relatively more prominent emerge in developed regions.<br>\n**3.2.4.** Where the IT revolution has begun and spread first, there exist proportionally more <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>.<br>\n**3.2.5.** <i>India<\/i> seems to be the largest producer of coders.<br>","d9feb42f":"**3.14.1.** Computer vision methods are a more technical topic, and it is too early for <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> to let themselves in thoroughly yet. That's why here we see a more widening gap between user percentages of the <span style=\"font-weight:bold; color:#778899\">selected groups<\/span>.<br>\n**3.14.2.** The percentage of <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> who use computer vision methods is three times as much as the percentage of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> who use these methods. Only 11% of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> answer this question positively, while 33% percent of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> respond they use the computer vision methods.<br> \n**3.14.3.** On average, only 15.9% of the respondents from both groups regularly use computer vision methods. So, these methods are not hot among the members of the <span style=\"font-weight:bold; color:#778899\">selected groups<\/span>.<br>\n**3.14.4.** Image classification is the most used method by both <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>. <i>Image segmentation<\/i>, <i>object detection<\/i>, and <i>general-purpose image\/video tools<\/i> are close in usage frequency and follow the <i>image classification<\/i> in order.<br>","15e76116":"<p>When we look at the <span style=\"font-weight:bold; color:#626EFA\">novice<\/span> and <span style=\"font-weight:bold; color:#B00068\">veteran<\/span> kaggler populations relative to the overall population in a particular country [3][4], we see a different picture. I have found the total number of <span style=\"font-weight:bold; color:#626EFA\">novice<\/span> and <span style=\"font-weight:bold; color:#B00068\">veteran<\/span> kagglers per 10 million people in each related country(2). The total programmers in Figures <b>3.2.1<\/b> and <b>3.2.2<\/b> represent only <span style=\"font-weight:bold; color:#626EFA\">novice<\/span> and <span style=\"font-weight:bold; color:#B00068\">veteran<\/span> totals.<\/p>\n\n**3.2.6.** As far as <span style=\"font-weight:bold; color:#626EFA\">novice<\/span> and <span style=\"font-weight:bold; color:#B00068\">veteran<\/span> population concentration is concerned, looking at the geographical distribution above, it is possible to divide the kagglers world into six segments: <i>advanced laggards<\/i>, <i>balanced majors<\/i>, <i>lagging majors<\/i>, <i>leading majors<\/i>, <i>emergents<\/i>, and <i>developing laggards<\/i>.<br>\n<i>Advanced laggards<\/i> are advanced countries that have the lowest <span style=\"font-weight:bold; color:#626EFA\">novice<\/span> and <span style=\"font-weight:bold; color:#B00068\">veteran<\/span> population ratios (i.e., the number of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> and <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> per 10 million of country population).<br>\n<i>Balanced majors<\/i> are the countries that have relatively large and highly balanced <span style=\"font-weight:bold; color:#626EFA\">novice<\/span> and <span style=\"font-weight:bold; color:#B00068\">veteran<\/span> population ratios.<br>\n<i>Lagging majors<\/i> are countries with a relatively high ratio of programmers, most of which are <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>.<br>\n<i>Leading majors<\/i> are those countries with a relatively high ratio of programmers, most of which are <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>.<br>\n<i>Emergents<\/i> are developing countries that have a low programmer population ratio but an increasing portion of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>.<br> \n<i>Developing laggards<\/i> are developing countries that have the lowest <span style=\"font-weight:bold; color:#626EFA\">novice<\/span> and <span style=\"font-weight:bold; color:#B00068\">veteran<\/span> population ratios.<br> \n**3.2.7.** In Europe, highly advanced countries such as <i>Germany<\/i>, <i>Italy<\/i>, <i>France<\/i>, <i>Austria<\/i> and a less advanced <i>Poland<\/i> do not stand out in the <span style=\"font-weight:bold; color:#778899\">selected groups'<\/span> population. They lack both the experience of <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and the potential of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>. These countries are <i>advanced laggards<\/i>.<br>\n**3.2.8.** According to my taxonomy, <i>advanced laggards<\/i> are currently located in Europe only.<br>\n**3.2.9.** <i>Israel<\/i> has many programmers and a highly balanced distribution of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> and <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> with 22.8 and 21.7, respectively. <i>Israel<\/i> is a typical <i>balanced major<\/i>. <i>Portugal<\/i>, <i>United Kingdom<\/i>, <i>Denmark<\/i>, and <i>Canada<\/i> are the other <i>balanced major countries<\/i>. The last four countries have almost equally sized groups and total programmers between 24-30 per 10 million.<br> \n**3.2.10.** Lagging majors are <i>Norway<\/i>, <i>Spain<\/i>, <i>Switzerland<\/i>, <i>Netherlands<\/i>, <i>Sweden<\/i>, <i>Belgium<\/i>, <i>Australia<\/i>, and the <i>United States<\/i>. These are the major players, thanks to their veterans. <i>Australia<\/i> has a score of 39.3, yet most of which comes from <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>. Later in the future, they might become <i>advanced laggards<\/i> except <i>Spain<\/i>.<br>\n**3.2.11.** Among the European lagging majors, <i>Spain<\/i> is the most populated country with around 47 million[3]. Populations of the other European lagging majors stay below 20 million. At the same time, <i>Spain<\/i> is the second largest after <i>Norway<\/i> in terms of programmer population ratio in its league. That leaves <i>Spain<\/i> with the highest number of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> and <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>  among the lagging majors in Europe. In June 2021, Amazon Web Services declared they would build data centers in <i>Spain<\/i>, investing 2.5 billion euros. The investment will benefit from the existing programmers and contribute to their growth in the future as well[5].<br>\n**3.2.12.** <i>Singapore<\/i>, as a great <i>leading major<\/i>, stands out by 58 <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> per 10 million. <i>Singapore<\/i> also has the largest <span style=\"font-weight:bold; color:#626EFA\">novice<\/span> and <span style=\"font-weight:bold; color:#B00068\">veteran<\/span> population ratio.<br>\n**3.2.13.** Other countries that contribute to Asia's performance in <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> launching as <i>leading majors<\/i> are <i>Taiwan<\/i>, <i>Hong Kong<\/i>, <i>United Arab Emirates<\/i>, <i>Japan<\/i>, <i>South Korea<\/i>, <i>India<\/i>, and <i>Russia<\/i> at descending order.<br>\n**3.2.14.** <i>India<\/i>, which has the highest number of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>, falls back in relative size calculation due to its large citizen population. <i>India<\/i> has 16.2 <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> per 10 million people.<br>\n**3.2.15.** The only EU countries that made a jump in <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> production are <i>Ireland<\/i> and <i>Greece<\/i> with 26 and 16.8, respectively. They are, together with <i>Belarus<\/i>, the only <i>leading majors<\/i> in Europe.<br>\n**3.2.16.** African <i>emergents<\/i>, <i>Kenya<\/i>, <i>Egypt<\/i>, and <i>Nigeria<\/i> receive their strength from their exploding <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> growth with 16, 13.7, and 11.7, respectively. Other <i>emergents<\/i> are <i>Argentina<\/i> and <i>Chile<\/i> in South America, <i>Malaysia<\/i> and <i>Sri Lanka<\/i> in Asia. <i>Emergents<\/i> might become <i>balanced majors<\/i> years later.<br>\n**3.2.17.** <i>Developing laggards<\/i> are not advanced countries and have low population ratios of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> and <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>. Countries such as <i>Tunisia<\/i>, <i>South Africa<\/i>, <i>Uganda<\/i>, <i>Colombia<\/i>, <i>Ecuador<\/i>, <i>Peru<\/i>, <i>Mexico<\/i>, <i>Iran<\/i>, <i>Kazakhstan<\/i>, <i>Pakistan<\/i>, <i>Nepal<\/i>, <i>Saudi Arabia<\/i>, <i>Romania<\/i>, <i>Turkey<\/i>, <i>Ukraine<\/i> are <i>developing laggards<\/i>. These countries seem to lack a sufficient number of programmers relative to their overall populations. <i>Developing laggards<\/i> has a drawback that differentiates them from <i>advanced laggards<\/i>. The former has less capacity than the latter in attracting programmers from abroad. For example, <i>Germany<\/i> passed a law in August 2019 to attract and retain skilled workers[6], in particular, qualified IT specialists and engineers[7].","e8fd2d9e":"<a id=\"1\"><\/a> <br>\n# <div class=\"section_title\">1. Subject of the Study<\/div>\n    \n<p>When entering an online store, many of us want to read reviews before making a buying decision. I prefer to read one-star and five-star reviews only, believing that extremes expose the subject better. Medium ratings could leave the reader in confusion and dim light.<\/p> \n    \n<p>For a similar reason, in this study, I decided to analyze the kagglers who have a coding experience of 1 year or less and the kagglers who have a coding experience of 20 or more years. Differences between them should provide us with meaningful insights. I call the former group <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> and the latter <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and refer to both of them together as <span style=\"font-weight:bold; color:#778899\">selected groups<\/span>. I belong to neither of these groups, so I have no conflict of interest!<\/p>\n\n<p>One might think it would be more precise to consider <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>, those data scientists who have 20 or more years of experience in both coding and machine learning rather than in coding only. Such a thought makes sense, yet these double <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> make up only 190 people. So, we won't get highly significant results.<\/p>\n\n<p>Another question that may come into mind is why I took the data scientists who have one year or less programming experience as the other extreme, but not those who have no experience at all. No-experience group constitutes only 4% of the respondents, being the smallest segment in the survey. In addition, they should have very little or no experience in some other skill categories also. Without any programming experience, one would find it very hard to get deeper and even use various data science skills, such as NLP, Computer Vision Methods, IDEs, Databases, etc. To draw some reliable and guiding conclusions from comparing the two opposite extremes, we need significant data. And that only happens if both groups have a certain degree of experience, which would mean to be and result in such significant data.<\/p>\n\n<p>I tried to focus on the skill set of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> and <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>. Intuitively, one can rightly say the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> should have more skills than <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>. And this is true. However, we need to prove it. Second, we need to know in what and how much the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> are skilled. Hence, we can visualize the path taken to be a <span style=\"font-weight:bold; color:#B00068\">veteran<\/span>.<\/p>\n\n<a id=\"2\"><\/a> <br>\n# <div class=\"section_title\">2. Structure of the Study<\/div>\n\n<p>This section describes the structure of the study, and the next one discusses the topics. Each topic is related to one or more questions. Then follow findings, each of which is enumerated, so it's possible to refer to a previous or later finding to discover connections. Not to disturb the flow of logic, remarks that explain the methodology are in the <b>Notes<\/b> section. A number in parentheses within the text points to the <b>Notes<\/b> section for further information. Some findings naturally lead to some implications which beg proof. I tried to add those proofs from external data wherever possible. Numbers in square brackets throughout the text indicate the source of information which one can find in the <b>Sources<\/b> section.<\/p>\n\n<a id=\"3\"><\/a> <br> \n# <div class=\"section_title\">3. Findings<\/div>\n\n<p>Now, we can start presenting the findings. <b>Let the Game Begin!<\/b><\/p>\n\n![match.png](attachment:a862964a-1a83-4096-9691-5a3926a94b52.png)","3b8ec079":"<a id=\"4.2.\"><\/a> <br>\n## <div class=\"subsection_title\">4.2. Proven skills pay off.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q25: What is your current yearly compensation (approximate $USD)?<\/div>","cf704558":"**3.15.1.** Out of the total of 7741 respondents, only 791 people use NLP methods. These are the least used methods and tools among those asked in the survey.<br>\n**3.15.2.** Percentage-wise, <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> use NLP almost four times as intense as the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> with 23% and 6%, respectively.<br>\n**3.15.3.** <i>Word embeddings\/vectors<\/i> and <i>Transformer language models<\/i> are the two most used techniques.<br>","86b66344":"**3.12.1.** As for machine learning frameworks, the difference between the user percentages of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> is higher than for any other skill mentioned before. The median number of frameworks used by the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> are 3 and 1, respectively. Only after programming languages and EDA visualization libraries, one learn machine learning frameworks. Therefore, we might think that <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>' penetration rate for these frameworks will be lower than for the programming languages and visualization libraries.<br>\n**3.12.2.** Python as a programming language, Jupyter Notebook as an IDE,  Matplotlib as a visualization library, and <i>Scikit-learn<\/i> as a machine learning framework are the most used tools by <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> and <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>. Usage percentages for the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> are 86%, 62.1%, 61.7%, and 39.6, respectively. The same rates for <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> are 81.7%, 59.6%, 66.1%, and 59.4%, respectively. These are the statistics we have learned from the previous figures. One can see that in the first three tools, <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> and <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> are close percentage-wise, but in the machine learning framework, <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>' percentage shrinks much more considerably than the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>. As suggested in <b>3.12.1<\/b>, <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>, as the least experienced programmers, have not come to the phase of fully utilizing the existing machine learning frameworks.<br>\n**3.12.3.** On average, 79% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> use machine learning frameworks, whereas 52% of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> use them.<br>\n**3.12.4.** <i>Scikit-learn<\/i>, <i>Tensorflow<\/i>, and <i>Keras<\/i> are the most used libraries in descending order by both <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>.<br>\n**3.12.5.** 25% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> use four or more frameworks regularly as the distribution graph exhibits. On the other hand, 25% of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> use two or more frameworks. The maximum number of frameworks used by a <span style=\"font-weight:bold; color:#626EFA\">novice<\/span> is five, while it is eight for a <span style=\"font-weight:bold; color:#B00068\">veteran<\/span>.<br>","8238a562":"<a id=\"3.5.\"><\/a> <br> \n## <div class=\"subsection_title\">3.5. Women take more part in programming than in the past.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q2: What is your gender?<\/div>","fb4904e9":"**3.1.1.** <span style=\"font-weight:bold; color:#B00068\">Veterans<\/span> make up a little more than %7 of the respondents, and the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> are above 22%. That leaves us with the fact that <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> are about three times as many as the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>(1).<br>\n**3.1.2.** The distribution of programming experience is positively skewed. We have more of less coding-experienced kagglers and fewer more coding-experienced kagglers.<br>\n**3.1.3.** More than half of the respondents have a programming experience of fewer than three years. As we will see later, this finding has to do with young age as more than half of all data scientists are between the ages of 22 and 34[2]. ","c3364b28":"**3.19.1.** Not many members of the <span style=\"font-weight:bold; color:#778899\">selected groups<\/span> are interested in showcasing their work. Their median usage is zero. On average, 19.5 % of the respondents in the <span style=\"font-weight:bold; color:#778899\">selected groups<\/span> share their work on some platform.<br>\n**3.19.2.** The top three platforms which one publicly shares or deploy data analysis or machine learning applications are <i>GitHub<\/i>, <i>Kaggle<\/i>, and <i>Colab<\/i> in descending order.<br>\n**3.19.3.** On average, 14% of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> publish their work somewhere, while 36% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> share their work on similar platforms. Such difference should stem from the difference in work, activities, and skills of the <span style=\"font-weight:bold; color:#778899\">selected groups<\/span> as seen in the previous findings. When someone does more, he also shows more.<br>","ba3fb8c5":"<a id=\"3.25.\"><\/a> <br>\n## <div class=\"subsection_title\">3.25. Tableau is and remains to be the top choice of the selected groups.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q34-A: Which of the following business intelligence tools do you use on a regular basis?<\/div><div class=\"heading\">Q34-B: Which of the following business intelligence tools do you hope to become more familiar with in the next 2 years?<\/div>","87f8b38f":"**3.3.1.** A positive correlation[9] between <i>age<\/i> and <i>programming experience<\/i> is not a surprising fact. The relationship between them follows a parabolic trend with a decreasing slope (dashed line). The respondents' coding time grows less than their age growth on average. In other words, increasing age doesn't add the same years to the <i>programming experience<\/i>. That means when people age, some of them just start programming, pause or stop programming, or intermittently code.<br>\n**3.3.2.** Up to 25-29 years of age, we exactly see the same trend as depicted in the <i>programming experience<\/i> histogram on the left. The number of programmers, after peaking in 1-3 years of <i>coding experience<\/i>, starts gradually declining. However, at ages after 25-29, we see a slightly different trend. Their number first increases, then declines around 3-5 years of <i>coding experience<\/i>, and then starts climbing after that.<br>\n**3.3.3.** Kagglers usually start participating in the programmer community until 35 years of age (<i>Rectangle A<\/i>).<br>\n**3.3.4.** Kagglers who are younger than 30 and have up to 5 years of <i>coding experience<\/i> hold the majority of respondents (<i>Rectangle B<\/i>).<br>\n**3.3.5.** The 2935 <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> (half of the body of the whole <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>) are younger than 24.<br>\n**3.3.6.** Some unrealistic outliers exist unless we have child prodigies (<i>Rectangle C<\/i>).<br>\n**3.3.7.** A decent number of kagglers either changed their careers to a coding-related work or have found a new area of interest for themselves (<i>Rectangle D<\/i>). As we see in the heatmap, we may have all reason to think that more kagglers than the <i>Rectangle D<\/i> represents must have chosen the coding later in their lives. However, how many they are is uncertain, so <i>Rectangle D<\/i> is a safe finding.<br>","cf343477":"The great question is this: if the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> are so skilled, do those skills pay off(6)? The short answer is Yes. Now, we will look into it, how(7).<br>\n**4.2.1.** Throughout all the subplots, we will see that <u>coding experience is positively correlated with the salary level independent of any y-axis variable<\/u>. Especially, it becomes more prominent with three years or more of <i>programming experience<\/i>.<br>\n**4.2.2.** It seems that the level of <i>formal education<\/i> has no direct impact on the salary (<i>graph-a<\/i>).<br>\n**4.2.3.** For all levels of <i>programming experience<\/i>, <i>Developer Relations\/Advocacy<\/i> and <i>Product Manager<\/i> roles generally pay more. For programmers experienced for more than five years, additional roles such as <i>Machine Learning Engineer<\/i>, <i>Data Scientist<\/i>, and <i>Data Engineer<\/i> can give more pay. (<i>graph-b<\/i>).<br>\n**4.2.4.** Another high-paying experience type is <i>machine learning<\/i>. Even if a data scientist is not a <span style=\"font-weight:bold; color:#B00068\">veteran<\/span> coder, he can earn a higher salary, becoming a <i>machine learning veteran<\/i>. Interestingly, the effect on salaries of the more <i>machine learning experience<\/i> for <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> is not so strong as the other <i>programming experience<\/i> levels. This issue may stem from that the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>, as the subplots reveal, earn more due to <i>coding experience<\/i>, so the <i>machine learning experience<\/i> as extra might be doing relatively less contribution to their salaries. An alternative explanation may be like this: As we will see in the next finding, one can reach financial nirvana faster in <i>machine learning<\/i> than in <i>programming<\/i>. Therefore, if a <span style=\"font-weight:bold; color:#B00068\">veteran<\/span> who does some <i>machine learning<\/i> performs a job role defined as a <i>programmer<\/i> rather than a <i>machine learning engineer<\/i>, his compensation may match the dominant role. Of course, we need further proofs of these explanations (<i>graph-c<\/i>).<br>\n**4.2.5.** The <i>machine learning<\/i> experience has a higher salary growth potential than the <i>programming experience<\/i> (<i>graph-c<\/i>).<br>\n**4.2.6.** Industry factor presents some implications too. <i>Non-profit\/Service<\/i> pays lower, an intuitively expected result (<i>graph-d<\/i>).<br>\n**4.2.7.** <i>Military\/Security\/Defense<\/i> and <i>Insurance\/Risk Assessment<\/i> are high payers across most <i>coding experience<\/i> levels (<i>graph-d<\/i>).<br>\n**4.2.8.** <i>Medical\/Pharmaceutical<\/i>, <i>Hospitality\/Entertainment\/Sports<\/i>, and <i>Accounting\/Finance<\/i> are among the highest payers for five years or more of <i>programming experience<\/i> (<i>graph-d<\/i>).<br>\n**4.2.9.** <i>Broadcasting\/Communications<\/i> pursues a very consistent compensation policy. The more experienced a kaggler is, the more he earns in this industry. Salary size is proportional to the <i>programming experience<\/i> years (<i>graph-d<\/i>).<br>\n**4.2.10.** The general tendency with the <i>Company Size<\/i> is that the larger the company is, the more it pays (<i>graph-e<\/i>).<br><br>\nThe following subplot y-axis parameters are about the number of applications for each skill(8).<br>\n**4.2.11.** We witness two main trends in all subplots from f through v. The first one is, as the level of experience increases, so <u>generally<\/u> does the number of products, tools, or platforms known and used, which is a result that we have already discovered in the <b>Findings<\/b> section. Especially there is a direct positive linear relationship between <i>programming experience<\/i> and the use of any of <i>programming languages<\/i>, <i>IDEs<\/i>, <i>machine learning algorithms<\/i>, <i>cloud computing platforms<\/i>, and <i>big data products<\/i> (<i>graph-f through v<\/i>).<br>\n**4.2.12.** The second trend is, up to the 3-5 years threshold, the number of products, tools, or platforms known and used doesn't impact the salary level. Only when the <i>programming<\/i> or <i>machine learning experience<\/i> exceeds three years, salaries steadily get larger. In other words, one should have more than three years of <i>programming<\/i> or <i>machine learning experience<\/i> to expect remarkable increases in his compensation (<i>graph-f through v<\/i>).<br>\n**4.2.13.** In several subplots, we see that more experienced data scientists in <i>coding<\/i> (especially <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>) may use fewer products than the less experienced data scientists in <i>programming<\/i>.  However, still, the former group earns more than the latter one. Seniority certainly pays more, but seniority is also proof of established skills (<i>graphs h, i, k, l, m, o, q, r, s, t, u, v<\/i>).<br>\n**4.2.14.** Based on the findings of <b>4.2<\/b>, we may draw a <i>honey and milk combination<\/i> like this:  a double <span style=\"font-weight:bold; color:#B00068\">veteran<\/span> in <i>programming<\/i> and <i>machine learning<\/i>, working for a <i>big company<\/i> in one of <i>Military\/Security\/Defense<\/i>, <i>Insurance\/Risk Assessment<\/i>, <i>Medical\/Pharmaceutical<\/i>, <i>Hospitality\/Entertainment\/Sports<\/i>, or <i>Accounting\/Finance<\/i> businesses, and having one of the <i>Product Manager<\/i>, <i>Machine Learning Engineer<\/i>, <i>Developer Relations\/Advocacy<\/i>, <i>Data Scientist<\/i>, or <i>Data Engineer<\/i> roles.<br><br><br>","b60c9292":"**3.4.1.** Kagglers usually learn <i>coding<\/i> to do <i>machine learning<\/i> (<i>Rectangle A<\/i>).<br>\n**3.4.2.** 38.5% of veterans can be considered as <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> in <i>machine learning<\/i> (<i>Rectangle B<\/i>).<br>\n**3.4.3.** Kagglers have gained experience in both <i>machine learning<\/i> and <i>programming<\/i> in parallel (<i>Above the line<\/i>).<br>\n**3.4.4.** Similarly, not many people spend years in <i>machine learning<\/i> without spending time <i>coding<\/i> at the same time. (<i>Below the line<\/i>).<br> \n**3.4.5.** Kagglers who have one year or less of <i>machine learning<\/i> experience (13,052 respondents, <i>Rectangle C<\/i>) outnumber the people who have the same amount of experience in <i>programming<\/i> (5,498, <i>Rectangle D<\/i>). This picture is reversed for all other corresponding levels of experience. In other words, the number of kagglers who have certain years of experience in <i>programming<\/i> exceeds the number of kagglers who have the same amount of experience in <i>machine learning<\/i>. That means kagglers who already had <i>programming experience<\/i> later started <i>machine learning<\/i>.<br>\n**3.4.6.** 3889 respondents who have some <i>programming experience<\/i> have no prior <i>machine learning experience<\/i>. Probably, these programmers joined the Kaggle community to give a start in <i>machine learning<\/i>.<br>\n**3.4.7.** Unlike <b>3.4.6<\/b>, kagglers who have no <i>programming experience<\/i> also don't have any <i>machine learning experience<\/i>.<br>\n**3.4.8.** Based on <b>3.4.6<\/b> and <b>3.4.7<\/b>, we can reach this conclusion: kagglers don't want and attempt to do <i>machine learning<\/i> without <i>coding experience<\/i>.<br>","4296c821":"<a id=\"3.13.\"><\/a> <br>  \n## <div class=\"subsection_title\">3.13. Traditional methods are popular for both groups.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q17: Which of the following ML algorithms do you use on a regular basis?<\/div>","7d2277dc":"<a id=\"3.7.\"><\/a> <br>\n## <div class=\"subsection_title\">3.7. Veterans are generally on the production front.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q5: Select the title most similar to your current role.<\/div>","546a1465":"**3.14.5.** In the above graph, we need to focus on the darkest cells, which happen to be above 0.3. <span style=\"font-weight:bold; color:#778899\">Selected groups<\/span> use <i>Generative Adversarial Networks<\/i> in <i>Generative Networks<\/i> and <i>Convolutional Neural Networks<\/i> in other <i>Computer Vision Methods<\/i>. It is an expected finding.<br>","1fd76c46":"### **<span style=\"font-weight:bold; color:#B00068\">\u201cAnyone who stops learning is old, whether at twenty or eighty. Anyone who keeps learning stays young.\u201d<\/span>**\n\n#### Henry Ford\n<br>\n<br>","0e1732c1":"<a id=\"3.12.\"><\/a> <br> \n## <div class=\"subsection_title\">3.12. Veterans' expertise level is even higher for machine learning frameworks than the novices.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q16: Which of the following machine learning frameworks do you use on a regular basis?<\/div>","c5195fce":"**3.13.1.** <b>Figure 3.13.1<\/b> conveys the same messages as <b>Figure 3.12.1<\/b> since they relate to the same subject. Their response statistics are very close, with 82% for the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and 53% for the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>. Therefore, there is again a wide gap between the user percentage of <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and the user percentage of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>.<br>\n**3.13.2.**  Median numbers of algorithms used are three for <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> and one for <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>. The maximum number of algorithms used by a <span style=\"font-weight:bold; color:#626EFA\">novice<\/span> is five, while it is 11 for a <span style=\"font-weight:bold; color:#B00068\">veteran<\/span>.<br>\n**3.13.3.** The distribution of machine learning algorithms used by the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> is considerably wide. 25% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> use five or more algorithms regularly.<br>\n**3.13.4.** Traditional methods such as <i>Linear or Logistic Regression<\/i>, <i>Decision Trees or Random Forests<\/i> come at the top for both groups. This finding also coincides with the popularity of Scikit-learn suggested by <b>3.12.4<\/b>.<br>\n**3.13.5.** The percentage of <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> who use neural networks is almost three times more than the percentage of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> who use these algorithms. Traditional methods don't have such a large spread.<br>","50bad33e":"<a id=\"3.23.\"><\/a> <br>\n## <div class=\"subsection_title\">3.23. On average, %90 of the respondents in the selected groups don't use managed machine learning products.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q31-A: Do you use any of the following managed machine learning products on a regular basis?<\/div><div class=\"heading\">Q31-B: In the next 2 years, do you hope to become more familiar with any of these managed machine learning products?<\/div>","bd01e958":"**3.28.7.** Respondents may select more than one choice in questions <b>Q27-B<\/b>, <b>Q29-B<\/b>, <b>Q30-B<\/b>, <b>Q31-B<\/b>, <b>Q32-B<\/b>, <b>Q34-B<\/b>, <b>Q36-B<\/b>, <b>Q37-B<\/b>, <b>Q38-B<\/b>. These questions investigate the products and tools that the kagglers hope to become more familiar with in the next two years. The questions present 83 not <i>None<\/i> choices in total, which means respondents are free to select any number of products and tools up to 83 if we don't count <i>None<\/i> answers.<br>\n**3.28.8.** <span style=\"font-weight:bold; color:#626EFA\">Novices<\/span> have selected 77 while the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> have specified 69 products at maximum. <span style=\"font-weight:bold; color:#626EFA\">Novices<\/span> all stretch over to the right (<i>green shaded area<\/i>).<br>\n**3.28.9.** The <i>yellow shaded area<\/i> represents 12 or fewer products. I have determined this number as a threshold arbitrarily, considering one respondent should learn one tool every two months on average to accomplish all 12 in two years. That is not a very easy task. 52.2% of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> say they will become more familiar with 12 or fewer products in the coming two years while this ratio is 60.4% for the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>. In other words, 47.8% of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> say they will learn more about 13 or more products soon while 39.6% of <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> say so. Since <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> are far more experienced and skilled, as we have seen in earlier findings, they should also be more capable of learning new tools easily and quickly.  So, how can we interpret these numbers? Out of every 100 <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>, 48 are planning to learn up to 77 tools in two years, having only one year or less of coding experience. On the other hand, out of every 100 <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> who have 20 or more years of programming experience, 40 want to become more familiar with up to 66 products in the same period.<br>\n**3.28.10.** Well, as Adidas put it, <i>\"Impossible is Nothing.\"<\/i> Anyone can see his own possibilities...<br>","1187441f":"<a id=\"3.9.\"><\/a> <br>\n## <div class=\"subsection_title\">3.9. Selected groups agree on Python as the recommended language to learn first for data science.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q8: What programming language would you recommend an aspiring data scientist to learn first?<\/div>","31273064":"<a id=\"3.26.\"><\/a> <br>\n## <div class=\"subsection_title\">3.26. Both veterans and novices want to learn more about automated model selection and automation of full ML pipelines.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q36-A: Do you use any automated machine learning tools (or partial AutoML tools) on a regular basis?<\/div><div class=\"heading\">Q36-B: Which categories of automated machine learning tools (or partial AutoML tools) do you hope to become more familiar with in the next 2 years?<\/div>","f2c046af":"<a id=\"3.15.\"><\/a> <br> \n## <div class=\"subsection_title\">3.15. Nearly 90% of the respondents don't use NLP regularly.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q19: Which of the following natural language processing (NLP) methods do you use on a regular basis?<\/div>","780fc33a":"<a id=\"3.3.\"><\/a> <br> \n## <div class=\"subsection_title\">3.3. The older the data scientist is, the more programming experience he has.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q1: What is your age (# years)?<\/div>","9b319591":"**3.13.6.** In the above graph, we need to focus on the darkest cells, which happen to be above 0.3. <span style=\"font-weight:bold; color:#778899\">Selected groups<\/span> use <i>Scikit-learn<\/i> for <i>linear regression<\/i>, <i>logistic regression<\/i>, <i>decision trees<\/i>, <i>random forests<\/i>, and <i>gradient boosting machines<\/i>.<br>\n**3.13.7.** They use <i>XGBoost<\/i> for classification problems.<br>\n**3.13.8.** They also use <i>XGBoost<\/i> and <i>LightGBM<\/i> for <i>gradient boosting machine algorithms<\/i> naturally.<br>\n**3.13.9.** <span style=\"font-weight:bold; color:#778899\">Selected groups<\/span> run <i>Convolutional Neural Networks<\/i> and <i>Recurrent Neural Networks<\/i> on <i>Tensorflow<\/i> and <i>Keras<\/i>.<br>\n**3.13.10.** The above points were the common ones, and we see differences between both groups as well. <span style=\"font-weight:bold; color:#B00068\">Veterans<\/span> use <i>Hugginface<\/i> for <i>Transformer Networks<\/i>, and <i>Pytorch<\/i> for <i>Convolutional Neural Networks<\/i>. On the other hand, <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> apply <i>Dense Neural Networks<\/i> on <i>Tensorflow<\/i> and <i>Keras<\/i>. All these points have been found according to the threshold (0.3) declared in <b>3.13.6<\/b>.<br>","6c3a5a99":"**3.21.1.** The top three favorite media sources are <i>Kaggle<\/i>, <i>YouTube<\/i>, and <i>Blogs<\/i>.<br>\n**3.21.2.** <span style=\"font-weight:bold; color:#B00068\">Veterans<\/span> widely prefer <i>Journal Publications<\/i> also. This finding can be the result of the implications of <b>Figure 3.6<\/b> and <b>Figure 3.16<\/b>. 30.9% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> have a doctoral degree or professional doctorate. Also, 20.4% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> work in academics.<br>\n**3.21.3.** <span style=\"font-weight:bold; color:#B00068\">Veterans<\/span> outperform the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> in following all media sources that report on data science topics. <span style=\"font-weight:bold; color:#B00068\">Veterans<\/span> do not only participate in data science courses more than the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> as <b>3.20<\/b> shows, but also the former uses various media sources more to learn data science or be familiar with the latest news in this field.<br>\n**3.21.4.** 79% of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> follow one or more media sources. 57% of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> use at least one of those asked in the survey.<br>\n**3.21.5.** One lesson we can take as taught up to this point is, <i>\"one who learns knows more, and one who knows more keeps learning more.\"<\/i><br>","f3fbae70":"<a id=\"3.2.\"><\/a> <br> \n## <div class=\"subsection_title\">3.2. Africa and Asia lead in launching novices.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q3: In which country do you currently reside?<\/div>","b54b1ff6":"<a id=\"3.8.\"><\/a> <br> \n## <div class=\"subsection_title\">3.8. Veterans are versatile programmers.<\/div>\n## <div class=\"heading\">Q6: For how many years have you been writing code and\/or programming?<\/div><div class=\"heading\">Q7: What programming languages do you use on a regular basis?<\/div>","3fe78415":"**3.23.1.** We see the same trends suggested by <b>3.22.1<\/b> and <b>3.22.2<\/b>. A higher percentage of the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> currently use managed machine learning products than the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span>, but <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> hope to become familiar with them more than <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> percentage-wise.<br>\n**3.23.2.** The vast majority of <span style=\"font-weight:bold; color:#B00068\">veterans<\/span> neither use the products currently (80%) nor plan to know more in the future (83%).<br>\n**3.23.3.** Only 7% of <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> currently use managed machine learning products, but 42% say they will become more familiar with them in the next two years. So, we see a lot of potential learning energy here.<br>\n**3.23.4.** <i>Google Cloud Vertex AI<\/i> is the <span style=\"font-weight:bold; color:#B00068\">veterans<\/span>' first choice in the future, while <i>Amazon Sagemaker<\/i> is their current first choice.<br>\n**3.23.5.** The top two choices of the <span style=\"font-weight:bold; color:#626EFA\">novices<\/span> are <i>Google Cloud Vertex AI<\/i> and <i>Azure Machine Learning Studio<\/i>, and they will remain so, soon.<br>\n**3.23.6.** <span style=\"font-weight:bold; color:#626EFA\">Novices<\/span> also have a considerable interest in learning and using <i>DataRobot<\/i> (10.2%) and <i>Databricks<\/i> (9.8%) in the coming two years.<br>"}}