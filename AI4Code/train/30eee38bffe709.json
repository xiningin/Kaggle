{"cell_type":{"9b246d08":"code","d5c81a8b":"code","ff717860":"code","07f545ef":"code","831a89de":"code","955f212e":"code","d99f0753":"code","59a05e78":"code","fb2ea506":"code","0d26698e":"code","8490468e":"code","70cd26e1":"code","12ad1783":"code","5320cce5":"code","3100b170":"code","04b7851d":"code","1c12f5c4":"code","a0db4f92":"code","90578559":"code","41437d1a":"code","e540ab7d":"code","6c3b4249":"code","b3ed4bb3":"code","35f5db8f":"code","807f70c0":"code","411921c5":"code","10da4756":"code","6f30b10c":"code","7eab2aee":"code","81872469":"code","87d3a8d8":"code","acd067db":"code","eeedf340":"code","a8f68a17":"code","48214c17":"markdown","cffa1650":"markdown","d2c07baf":"markdown","3088b364":"markdown","45e847a8":"markdown","8f2bbf4e":"markdown","da6c60be":"markdown","becb3804":"markdown","3b694cea":"markdown","19026167":"markdown","35f109eb":"markdown","00b24cfc":"markdown","97c5bd45":"markdown","0d486b77":"markdown","eba76d58":"markdown","646931f7":"markdown","4643cd2f":"markdown","45a2e004":"markdown","bd0b69f3":"markdown","0883a8a8":"markdown","5f944819":"markdown","69d1dba8":"markdown","077c248e":"markdown","9e334473":"markdown","50083aab":"markdown","8712a3d6":"markdown","78647c80":"markdown","e9d6225e":"markdown","e39a43d2":"markdown"},"source":{"9b246d08":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/digit-recognizer\"))\n\n# Any results you write to the current directory are saved as output.","d5c81a8b":"'''In pytorch, matrix(array) is called tensors. 3*3 matrix koy. This is 3x3 tensor.'''\n# numpy array\narray = [[1,2,3],[4,5,6]]\nfirst_array = np.array(array) # We create numpy array with np.numpy() method -> 2x3 array\nprint(\"Array Type: {}\".format(type(first_array))) # Type(): type of the array. In this example it is numpy\nprint(\"Array Shape: {}\".format(np.shape(first_array))) # np.shape(): shape of the array. Row x Column\nprint(first_array)","ff717860":"import torch  # import pytorch library\n\ntensor = torch.Tensor(2, 3)  # pytorch array\n\n'''This code creates a tensor of size (2,3) filled with zeros. In this example, \nthe first number is the number of rows, the second is the number of columns.'''\nprint(\"Array Type: {}\".format(tensor.type)) # type\nprint(\"Array Shape: {}\".format(tensor.shape)) # shape\nprint(tensor)","07f545ef":"\n'''We can also create a tensor filled with random floats:'''\nx = torch.rand(2, 3)\nprint(x)","831a89de":"'''Multiplying tensors, adding to each other, and other algebraic operations are simple:'''\nx = torch.ones(2,3)  # tensor filled 1\nprint(x, '\\n')\ny = torch.ones(2,3) * 2  # tensor filled 2\nprint(y)\nprint('\\n\\n Result of adding tensors: \\n', x + y)  # result of adding tensors","955f212e":"# numpy ones\nprint(\"Numpy {}\\n\".format(np.ones((2,3))))\n\n# pytorch ones\nprint(torch.ones((2,3)))","d99f0753":"# numpy random\nprint(\"Numpy {}\\n\".format(np.random.rand(2,3)))\n\n# pytorch random\nprint(torch.rand(2,3))","59a05e78":"# random numpy array\narray = np.random.rand(2,2)\nprint(\"{} {}\\n\".format(type(array),array))\n\n# from numpy to tensor\nfrom_numpy_to_tensor = torch.from_numpy(array)\nprint(\"{}\\n\".format(from_numpy_to_tensor))\n\n# from tensor to numpy\ntensor = from_numpy_to_tensor\nfrom_tensor_to_numpy = tensor.numpy()\nprint(\"{} {}\\n\".format(type(from_tensor_to_numpy),from_tensor_to_numpy))","fb2ea506":"print('before: \\n', y)\n'''It is also available to work with the slice function in numpy. For example y[:,1]:'''\ny[:,1] = y[:,1] + 1\nprint('\\n after: \\n', y)","0d26698e":"# create tensor \ntensor = torch.ones(3,3)\nprint(\"\\n\",tensor)\n\n# Resize\nprint(\"{}{}\\n\".format(tensor.view(9).shape,tensor.view(9)))\n\n# Addition\nprint(\"Addition: {}\\n\".format(torch.add(tensor,tensor)))\n\n# Subtraction\nprint(\"Subtraction: {}\\n\".format(tensor.sub(tensor)))\n\n# Element wise multiplication\nprint(\"Element wise multiplication: {}\\n\".format(torch.mul(tensor,tensor)))\n\n# Element wise division\nprint(\"Element wise division: {}\\n\".format(torch.div(tensor,tensor)))\n\n# Mean\ntensor = torch.Tensor([1,2,3,4,5])\nprint(\"Mean: {}\".format(tensor.mean()))\n\n# Standart deviation (std)\nprint(\"std: {}\".format(tensor.std()))","8490468e":"from torch.autograd import Variable  # import variable from pytorch library\n\n'''Let's create a variable from a simple tensor:'''\nx = Variable(torch.ones(2, 2) * 2, requires_grad=True)\nprint(x)","70cd26e1":"'''Next, let's create a new variable based on x.'''\nz = 2 * (x * x) + 5 * x\nprint(z)","12ad1783":"'''\nTo calculate the gradient of this operation in x, dz\/dx, we can analytically obtain 4x + 5. \nIf all elements of x are twos, then the gradient of dz\/dx is a tensor of dimension (2,2) \nfilled with numbers 13. However, first you need to run the inverse operation spreads .backwards() \nto calculate the gradient relative to something. In our case, the unit tensor (2,2) is initialized, \nrelative to which we calculate the gradient. \nIn this case, the calculation is just a d\/dx operation:\n'''\n\nz.backward(torch.ones(2, 2))\nprint('The result is the following: \\n', x.grad)","5320cce5":"# lets make basic backward propagation\n# we have an equation that is y = x^2\narray = [2,4]\ntensor = torch.Tensor(array)\nx = Variable(tensor, requires_grad = True)\ny = x**2\nprint(\" y =  \",y)\n\n# recap o equation o = 1\/2*sum(y)\no = (1\/2)*sum(y)\nprint(\" o =  \",o)\n\n# backward\no.backward() # calculates gradients\n\n# As I defined, variables accumulates gradients. In this part there is only one variable x.\n# Therefore variable x should be have gradients\n# Lets look at gradients with x.grad\nprint(\"gradients: \",x.grad)","3100b170":"'''\nThe main data structure torch.nn is a module, which is an abstract concept that can represent a specific layer in a neural network, \nor a neural network containing many layers. In practice the most common way is to inherit nn.Module and write your own network\/layer. \nLet's first see how to use nn.Module to implement your own fully connected layer. A fully connected layer, also known as an affine layer\n'''\n# libraries\nimport torch.nn as nn  # base class\nimport torch.nn.functional as F\n\nclass Net(nn.Module):  # In such a definition, you can see the inheritance of the base class nn.Module\n    def __init__(self):  # In the first line of class initialization, \n        super(Net, self).__init__()  # Python super() function that creates an object of the base class\n        \n        # in the next three lines, we create fully connected layers \n        '''\n        A fully connected neural network layer is represented by an nn.Linear object, \n        in which the first argument is the number of nodes in the i-th layer, and the second \n        is the number of nodes in the i+1 layer. As you can see from the code, the first layer \n        takes 28x28 pixels as input and connects to the first hidden layer with 200 nodes.\n        '''\n        self.fc1 = nn.Linear(28 * 28, 200)\n        \n        # Next comes the connection to another hidden layer with 200 nodes\n        self.fc2 = nn.Linear(200, 200)\n        \n        \n        # And finally, connecting the last hidden layer to the output layer with 10 nodes\n        self.fc3 = nn.Linear(200, 10)\n        \n        '''\n        After defining the skeleton of the network architecture, it is necessary to set \n        the principles by which data will move through it. This is done with the forward() method \n        being defined, which overrides the dummy method in the base class and requires a per-network definition\n        '''\n\n    def forward(self, x):  # For the forward() method, we take the input data x as the main argument\n        # Next, load everything in the first fully connected layer self.fc1(x) and apply the ReLU activation\n        # function to the nodes in that layer using F.relu()\n        x = F.relu(self.fc1(x))\n\n        # Due to the hierarchical nature of this neural network, we replace x at each stage and send \n        # it to the next layer\n        x = F.relu(self.fc2(x))\n\n        # We do this procedure on three connected layers, except for the last one.       \n        x = self.fc3(x)\n\n        # On the last layer, we return not ReLU, but the logarithmic softmax activation function. \n        # This, combined with the negative log-likelihood loss function, yields a multi-class \n        # cross-entropy-based loss function that we will use to train the network.  \n        return F.log_softmax(x)   \n        \n    ","04b7851d":"'''We have defined a neural network. The next step is to create an instance of this architecture:'''\n\nmodel = Net()\nprint('When outputting an instance of the Net class, we get the following: \\n', model)\nprint('Which is very convenient, as it confirms the structure of our neural network.')","1c12f5c4":"import torch.optim as optim  # is a package implementing various optimization algorithms. \n# Most commonly used methods are already supported, and the interface is general enough, \n# so that more sophisticated ones can be also easily integrated in the future\n\nlearning_rate = 0.01\n# In the first line, we create an optimizer based on stochastic gradient descent, \n# setting the learning rate (in our case, we will define this indicator at 0.01)\n\n# Perform optimization by stochastic gradient descent\n# Even in the optimizer, you need to define all the other network parameters, \n# but this is done easily in PyTorch thanks to the .parameters() method \n# in the nn.Module base class, which is inherited from it into the new Net class\noptimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n\n'''\nNext, a quality control metric is set, the negative log-likelihood loss function. \nThis type of function, combined with the logarithmic softmax function at the output \nof the neural network, gives the equivalent cross-entropy loss for 10 classes of the \nclassification problem.\n'''\n# Create a loss function\nerror = nn.NLLLoss()","a0db4f92":"# Import Libraries\nfrom torch.utils.data import DataLoader\n\nfrom sklearn.model_selection import train_test_split","90578559":"# Prepare Dataset\n# load data\ntrain = pd.read_csv(r\"..\/input\/digit-recognizer\/train.csv\",dtype = np.float32)\n\n# split data into features(pixels) and labels(numbers from 0 to 9)\ntargets_numpy = train.label.values\nfeatures_numpy = train.loc[:,train.columns != \"label\"].values\/255 # normalization\n\n# train test split. Size of train data is 80% and size of test data is 20%. \nfeatures_train, features_test, targets_train, targets_test = train_test_split(features_numpy,\n                                                                             targets_numpy,\n                                                                             test_size = 0.2,\n                                                                             random_state = 42) \n\n# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\nfeaturesTrain = torch.from_numpy(features_train)\ntargetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n\n# create feature and targets tensor for test set.\nfeaturesTest = torch.from_numpy(features_test)\ntargetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n\n# batch_size, epoch and iteration\nbatch_size = 100\nn_iters = 10000\nnum_epochs = n_iters \/ (len(features_train) \/ batch_size)\nnum_epochs = int(num_epochs)\n\n# Pytorch train and test sets\ntrain = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\ntest = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n\n# data loader\ntrain_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n","41437d1a":"# visualize one of the images in data set\nplt.imshow(features_numpy[1].reshape(28,28))\nplt.axis(\"off\")\nplt.title(str(targets_numpy[10]))\nplt.savefig('graph.png')\nplt.show()","e540ab7d":"#  Traning the Model\n\ncount = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\n\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        '''\n        The .view() function works with PyTorch variables and transforms their shape. If we don't know exactly the dimension of a given dimension, \n        we can use the '-1' notation in the dimension definition. So when using data.view(-1.28*28) we can say that the second dimension \n        should be 28 x 28 and the first dimension should be calculated from the size of the original data variable. In practice, \n        this means that the data will now be of size (batch_size, 784). We can pass this batch of input data into our neural network, \n        and the magical PyTorch will do the hard work for us, effectively performing the necessary calculations with tensors.\n        '''\n        # Define variables\n        train = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        \n        '''On the next line, we run optimizer.zero_grad() which zeros out or restarts the gradients in the model so that they are ready \n        for further backpropagation. Other libraries implement this implicitly, but keep in mind that PyTorch does this explicitly.'''\n        # Clear gradients\n        optimizer.zero_grad()\n        \n        \n        # Forward propagation\n        '''In the next line, we submit a portion of data to the input of our model, calls the forward() method in the Net class.'''\n        outputs = model(train)\n        \n        # Calculate softmax and cross entropy loss\n        # This line of code initializes the negative log-likelihood loss between the output of our neural network and the true labels of the given batch of data.\n        '''After running the string, the outputs variable will have the logarithmic softmax output from our neural network for \n        the given batch of data. This is one of the great things about PyTorch, as you can activate any standard Python debugger \n        you normally use and instantly see what's going on in the neural network. This is in contrast to other deep learning libraries, \n        TensorFlow and Keras, which require complex debugging to find out what your neural network is actually creating. \n        I hope you play around with the code for this tutorial and see how handy PyTorch's debugger'''        \n        loss = error(outputs, labels)  \n        \n        # Calculate gradients\n        '''The next line runs the error backpropagation operation from the loss variable back through the neural network. \n        If you compare this with the .backward() operation mentioned above, which we looked at in the tutorial, you can see \n        that no argument is used in the .backward() operation. Scalar variables require no argument when .backward() is used on them; \n        only tensors need a matched argument to pass to the .backward() operation.'''\n        loss.backward()\n        \n        # Update parameters\n        '''In the next line, we ask PyTorch to perform stepwise gradient descent based on the gradients computed during the .backward() operation.'''\n        optimizer.step()\n        \n        count += 1\n        \n        # Prediction\n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Predict test dataset\n            for images, labels in test_loader:\n                '''\n                The .view() function works with PyTorch variables and transforms their shape. If we don't know exactly the dimension of a given dimension, \n                we can use the '-1' notation in the dimension definition. So when using data.view(-1.28*28) we can say that the second dimension \n                should be 28 x 28 and the first dimension should be calculated from the size of the original data variable. In practice, \n                this means that the data will now be of size (batch_size, 784). We can pass this batch of input data into our neural network, \n                and the magical PyTorch will do the hard work for us, effectively performing the necessary calculations with tensors.\n                '''\n                test = Variable(images.view(-1, 28*28))\n                \n                # Forward propagation\n                '''In the next line, we submit a portion of data to the input of our model, calls the forward() method in the Net class.'''\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                '''the data.max(1) method, which returns the index of the largest value in a particular tensor dimension. \n                Now the output of our neural network will have a size of (batch_size, 10), where each value from the second dimension \n                of length 10 is the log probability that the neural network assigns to each output class (that is, it is the log \n                probability of the picture belonging to the symbol from 0 to 9). The value with the highest logarithmic probability \n                is the number from 0 to 9 that the neural network recognizes in the input image. \n                In other words, it is the best prediction for the given input feature. The .max(1) function determines this maximum \n                value in the second space (if we want to find the maximum in the first space, we must change the function \n                argument from 1 to 0) and immediately returns both the maximum found value and the corresponding index. \n                Therefore, this construct has a size of (batch_size, 2). \n                In this case, we are interested in the index of the maximum found value, which we access by calling .max(1)[1].'''\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n                \n                # Total correct predictions\n                '''We now have a neural network prediction for each example in a particular batch of inputs, and we can compare \n                it to the actual class label from the training dataset. This is used to count the number of correct answers.'''\n                correct += (predicted == labels).sum()\n            \n            '''We get a counter of the number of times the neural network gives the correct answer. Based on the accumulated sum \n            of correct predictions, one can determine the overall accuracy of the network on the training dataset. \n            Finally, iterating over each batch of input data, we derive the average value of the loss function and the accuracy of the model:'''\n            accuracy = 100 * correct \/ float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            accuracy_list.append(accuracy)\n            iteration_list.append(count)\n        \n        '''Finally, we will print the results every time the model reaches a certain number of iterations:\n        This function prints out our progress over the epochs of training and shows the error of the neural network at that moment.'''\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {}%  Epoch:{}'.format(count, loss.data, accuracy, epoch))","6c3b4249":"# visualization\nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Base class: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list, accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Base class: Accuracy vs Number of iteration\")\nplt.show()","b3ed4bb3":"# Create Logistic Regression Model\nclass LogisticRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LogisticRegressionModel, self).__init__()\n        # Linear part\n        self.linear = nn.Linear(input_dim, output_dim)\n        # There should be logistic function right?\n        # However logistic function in pytorch is in loss function\n        # So actually we do not forget to put it, it is only at next parts\n    \n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\n# Instantiate Model Class\ninput_dim = 28*28 # size of image px*px\noutput_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n\n# create logistic regression model\nmodel = LogisticRegressionModel(input_dim, output_dim)\n\n# Cross Entropy Loss  \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer \nlearning_rate = 0.001\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","35f5db8f":"# Traning the Model\ncount = 0\nloss_list = []\niteration_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        \n        # Define variables\n        train = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and cross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculate gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        # Prediction\n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Predict test dataset\n            for images, labels in test_loader: \n                test = Variable(images.view(-1, 28*28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n                \n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {}%'.format(count, loss.data, accuracy))","807f70c0":"# visualization\nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Logistic Regression Model: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list, accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Logistic Regression Model: Accuracy vs Number of iteration\")\nplt.show()","411921c5":"# Create ANN Model\nclass ANNModel(nn.Module):\n    \n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ANNModel, self).__init__()\n        \n        # Linear function 1: 784 --> 150\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity 1\n        self.relu1 = nn.ReLU()\n        \n        # Linear function 2: 150 --> 150\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 2\n        self.tanh2 = nn.Tanh()\n        \n        # Linear function 3: 150 --> 150\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 3\n        self.elu3 = nn.ELU()\n        \n        # Linear function 4 (readout): 150 --> 10\n        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n    \n    def forward(self, x):\n        # Linear function 1\n        out = self.fc1(x)\n        # Non-linearity 1\n        out = self.relu1(out)\n        \n        # Linear function 2\n        out = self.fc2(out)\n        # Non-linearity 2\n        out = self.tanh2(out)\n        \n        # Linear function 2\n        out = self.fc3(out)\n        # Non-linearity 2\n        out = self.elu3(out)\n        \n        # Linear function 4 (readout)\n        out = self.fc4(out)\n        return out\n\n# instantiate ANN\ninput_dim = 28*28\nhidden_dim = 150 #hidden layer dim is one of the hyper parameter and it should be chosen and tuned. For now I only say 150 there is no reason.\noutput_dim = 10\n\n# Create ANN\nmodel = ANNModel(input_dim, hidden_dim, output_dim)\n\n# Cross Entropy Loss \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.02\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","10da4756":"# ANN model training\ncount = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n\n        train = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and ross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Predict test dataset\n            for images, labels in test_loader:\n\n                test = Variable(images.view(-1, 28*28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))","6f30b10c":"# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"ANN: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"ANN: Accuracy vs Number of iteration\")\nplt.show()","7eab2aee":"# Create CNN Model\nclass CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n        \n        # Convolution 1\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n        self.relu1 = nn.ReLU()\n        \n        # Max pool 1\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n     \n        # Convolution 2\n        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n        self.relu2 = nn.ReLU()\n        \n        # Max pool 2\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n        \n        # Fully connected 1\n        self.fc1 = nn.Linear(32 * 4 * 4, 10) \n    \n    def forward(self, x):\n        # Convolution 1\n        out = self.cnn1(x)\n        out = self.relu1(out)\n        \n        # Max pool 1\n        out = self.maxpool1(out)\n        \n        # Convolution 2 \n        out = self.cnn2(out)\n        out = self.relu2(out)\n        \n        # Max pool 2 \n        out = self.maxpool2(out)\n        \n        # flatten\n        out = out.view(out.size(0), -1)\n\n        # Linear function (readout)\n        out = self.fc1(out)\n        \n        return out\n\n# batch_size, epoch and iteration\nbatch_size = 100\nn_iters = 2500\nnum_epochs = n_iters \/ (len(features_train) \/ batch_size)\nnum_epochs = int(num_epochs)\n\n# Pytorch train and test sets\ntrain = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\ntest = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n\n# data loader\ntrain_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False)\n    \n# Create CNN\nmodel = CNNModel()\n\n# Cross Entropy Loss \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.1\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","81872469":"# CNN model training\ncount = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        \n        train = Variable(images.view(100,1,28,28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and ross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                \n                test = Variable(images.view(100,1,28,28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n                \n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))","87d3a8d8":"# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"CNN: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"CNN: Accuracy vs Number of iteration\")\nplt.show()","acd067db":"from torch.utils.data import TensorDataset\n\n# Create RNN Model\nclass RNNModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super(RNNModel, self).__init__()\n        \n        # Number of hidden dimensions\n        self.hidden_dim = hidden_dim\n        \n        # Number of hidden layers\n        self.layer_dim = layer_dim\n        \n        # RNN\n        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n        \n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n    \n    def forward(self, x):\n        \n        # Initialize hidden state with zeros\n        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n            \n        # One time step\n        out, hn = self.rnn(x, h0)\n        out = self.fc(out[:, -1, :]) \n        return out\n\n# batch_size, epoch and iteration\nbatch_size = 100\nn_iters = 8000\nnum_epochs = n_iters \/ (len(features_train) \/ batch_size)\nnum_epochs = int(num_epochs)\n\n# Pytorch train and test sets\ntrain = TensorDataset(featuresTrain,targetsTrain)\ntest = TensorDataset(featuresTest,targetsTest)\n\n# data loader\ntrain_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n    \n# Create RNN\ninput_dim = 28    # input dimension\nhidden_dim = 100  # hidden layer dimension\nlayer_dim = 1     # number of hidden layers\noutput_dim = 10   # output dimension\n\nmodel = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n\n# Cross Entropy Loss \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.05\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","eeedf340":"seq_dim = 28  \nloss_list = []\niteration_list = []\naccuracy_list = []\ncount = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n\n        train  = Variable(images.view(-1, seq_dim, input_dim))\n        labels = Variable(labels )\n            \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and ross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        if count % 250 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                images = Variable(images.view(-1, seq_dim, input_dim))\n                \n                # Forward propagation\n                outputs = model(images)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += labels.size(0)\n                \n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))","a8f68a17":"# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"RNN: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"RNN: Accuracy vs Number of iteration\")\nplt.savefig('graph.png')\nplt.show()","48214c17":"The variable declaration uses a 2x2 double tensor and additionally states that the variable needs a gradient. When using this variable in neural networks, it becomes capable of learning. If the last parameter is False, then the variable cannot be used for learning. In this simple example, we won't be training anything, but we want to query the gradient for this variable.\n","cffa1650":"It's time to train the neural network. During training, the data will be fetched from the data load object. From the loader, input and target data will come in batches, which will be fed into our neural network and loss function, respectively. Below is the complete code for training:","d2c07baf":"### Specifications\nThe main features of PyTorch are mentioned below \u2212\n\n1. **Simple Interface** - PyTorch offers an easy to use API hence it is considered very easy to work with Python. Running code in this environment is quite easy.\n\n2. **Using Python** - This library seamlessly integrates with the Python data stack. Thus, it can use all the services and functionality offered by the Python environment.\n\n3. **Computational Graphs** - PyTorch provides an excellent platform that offers dynamic computational graphs. So the user can change them at run time. This is very useful when the developer does not know how much memory is required to create a neural network model.\n\nPyTorch is known for having three levels of abstraction as mentioned below:\n\n* Tensor is an imperative n-dimensional array running on the GPU.\n\n* Variable is a node in the computational graph. This stores the data and the gradient.\n\n* Module - The level of the neural network in which data about the state or the weights being studied will be stored.","3088b364":"Note that this is exactly what we predicted at the beginning. Note that the gradient is stored in the x variable in the .grad property.\n\n- Variables  accumulates gradients.\n- We will use pytorch in neural network. And as you know, in neural network we have backpropagation where gradients are calculated. Therefore we need to handle gradients.\n- Difference between variables and tensor is variable accumulates gradients.\n- We can make math operations with variables, too.\n- In order to make backward propagation we need variables","45e847a8":"We have learned the simplest operations with tensors, variables and the autogradient function in PyTorch. Now let's start writing a simple neural network in PyTorch, which will be a showcase for these functions in the future.","8f2bbf4e":"### Basic Math with Pytorch\n- Resize: view()\n- a and b are tensor.\n- Addition: torch.add(a,b) = a + b\n- Subtraction: a.sub(b) = a - b\n- Element wise multiplication: torch.mul(a,b) = a * b\n- Element wise division: torch.div(a,b) = a \/ b\n- Mean: a.mean()\n- Standart Deviation (std): a.std()","da6c60be":"Even if when I use pytorch for neural networks, I feel better if I use numpy. Therefore, usually convert result of neural network that is tensor to numpy array to visualize or examine.\nLets look at conversion between tensor and numpy arrays.\n- torch.from_numpy(): from numpy to tensor\n- numpy(): from tensor to numpy","becb3804":"### Class for building a neural network\nTo create a neural network in PyTorch, the nn.Module class is used. To use it, you need inheritance, which will allow you to use all the functionality of the nn.Module base class, but it is still possible to rewrite the base class to construct a model or directly pass through the network. The code below will help explain this:","3b694cea":"Allocation is one of the most used technique in coding. Therefore lets learn how to make it with pytorch.\nIn order to learn, compare numpy and tensor\n- np.ones() = torch.ones()\n- np.random.rand() = torch.rand()","19026167":"### Automatic Differentiation in PyTorch\nDeep learning libraries have mechanisms for calculating the error gradient and backpropagating the error through the computational graph. This mechanism, called autogradient in PyTorch, is easily accessible and intuitive. The variable class is the main component of the autogradient system in PyTorch. The variable class wraps the tensor and allows you to automatically calculate the gradient on the tensor when you call the ***.backward()*** function. The object contains the data from the tensor, the gradient of the tensor (computed once with respect to some other value, loss) and also contains a reference to any function created by the variable (if it is a user-created function, the reference will be empty).\n\n","35f109eb":"Now you know how to create and work with tensors in PyTorch. The next step of the tutorial will be an overview of the more complex constructs in the library.","00b24cfc":"- As a result, as you can see from plot, while loss decreasing, accuracy is increasing and our model is learning(training). \n- Thanks to convolutional layer, model learnt better and accuracy(almost 98%) is better than accuracy of ANN. Actually while tuning hyperparameters, increase in iteration and expanding convolutional neural network can increase accuracy but it takes too much running time that we do not want at kaggle.  ","97c5bd45":"<a id=\"3\"><\/a> <br>\n### Prepare Dataset\n\n\n        - Dataset - there are 28*28 images and 10 labels from 0 to 9\n        - Data is not normalized so we divide each image to 255 that is basic normalization for images.\n        - In order to split data, we use train_test_split method from sklearn library\n        - Size of train data is 80% and size of test data is 20%.\n        - Create feature and target tensors. At the next parts we create variable from these tensors. As you remember we need to define variable for accumulation of gradients.\n        - batch_size = batch size means is that for example we have data and it includes 1000 sample. We can train 1000 sample in a same time or we can divide it 10 groups which include 100 sample and train 10 groups in order. Batch size is the group size. For example, I choose batch_size = 100, that means in order to train all data only once we have 336 groups. We train each groups(336) that have batch_size(quota) 100. Finally we train 33600 sample one time.\n        - epoch: 1 epoch means training all samples one time.\n        - In our example: we have 33600 sample to train and we decide our batch_size is 100. Also we decide epoch is 29(accuracy achieves almost highest value when epoch is 29). Data is trained 29 times. Question is that how many iteration do I need? Lets calculate: \n            - training data 1 times = training 33600 sample (because data includes 33600 sample) \n            - But we split our data 336 groups(group_size = batch_size = 100) our data \n            - Therefore, 1 epoch(training data only once) takes 336 iteration\n            - We have 29 epoch, so total iterarion is 9744(that is almost 10000 which I used)\n        - TensorDataset(): Data set wrapping tensors. Each sample is retrieved by indexing tensors along the first dimension.\n        - DataLoader(): It combines dataset and sample. It also provides multi process iterators over the dataset.\n        - Visualize one of the images in dataset","0d486b77":"### PyTorch - Introduction\n\nPyTorch is defined as an open source machine learning library for Python. It is used for applications such as natural language processing. It was originally developed by Facebook's artificial intelligence research group and the probabilistic programming software Uber Pyro, which is based on it.\n\nPyTorch was originally developed by Hugh Perkins as a Python wrapper for LusJIT based on the Torch framework.\n\nPyTorch reverse-engineers and embeds Torch in Python, sharing the same core C libraries for internal code. The PyTorch developers have tuned this internal code to work effectively with Python. They also retained GPU-based hardware acceleration as well as expandability features.","eba76d58":"<a id=\"5\"><\/a> <br>\n### Convolutional Neural Network (CNN)\n- CNN is well adapted to classify images.\n\n- **Steps of CNN:**\n\n    1. Convolutional layer: \n        - Create feature maps with filters(kernels).\n        - Padding: After applying filter, dimensions of original image decreases. However, we want to preserve as much as information about the original image. We can apply padding to increase dimension of feature map after convolutional layer.\n        - We use 2 convolutional layer.\n        - Number of feature map is out_channels = 16\n        - Filter(kernel) size is 5*5\n    1. Pooling layer: \n        - Prepares a condensed feature map from output of convolutional layer(feature map) \n        - 2 pooling layer that we will use max pooling.\n        - Pooling size is 2*2\n    1. Flattening: Flats the features map\n    1. Fully Connected Layer: \n        - Artificial Neural Network that we learnt at previous part.\n        - Or it can be only linear like logistic regression but at the end there is always softmax function.\n        - We will not use activation function in fully connected layer.\n        - You can think that our fully connected layer is logistic regression.\n        - We combine convolutional part and logistic regression to create our CNN model.\n    1. Instantiate Model Class\n        - create model\n    1. Instantiate Loss\n        - Cross entropy loss\n        - It also has softmax(logistic function) in it.\n    1. Instantiate Optimizer\n        - SGD Optimizer\n    1. Traning the Model\n    1. Prediction        ","646931f7":"### Great, we have learned how to create and train our basic model!\nHowever, we went to this slowly and measuredly, understanding each step. That's how it should be done. No need to mindlessly copy the code, you need to understand what kind of code it is and what it does. \n\n[Here](https:\/\/www.kaggle.com\/andrej0marinchenko\/pytorch-base-class-for-beginners) I prepared the same code in a compressed form, but with a test data set, and also received the result of the model prediction for evaluation on the leaderboard.\n\n**Public Score - 0.97125**","4643cd2f":"<a id=\"3\"><\/a> <br>\n### Logistic Regression\n\n\n- We use logistic regression for classification.\n- linear regression + logistic function(softmax) = logistic regression\n\n- **Steps of Logistic Regression**\n    \n    1. Create Logistic Regression Model\n        - Same with linear regression.\n        - However as you expect, there should be logistic function in model right?\n        - In pytorch, logistic function is in the loss function where we will use at next parts.\n    2. Instantiate Model\n        - input_dim = 28x28 # size of image px*px\n        - output_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n        - create model\n    3. Instantiate Loss \n        - Cross entropy loss\n        - It calculates loss that is not surprise :)\n        - It also has softmax(logistic function) in it.\n    4. Instantiate Optimizer \n        - SGD Optimizer\n    5. Traning the Model\n    6. Prediction\n- As a result, as you can see from plot, while loss decreasing, accuracy(almost 85%) is increasing and our model is learning(training).  ","45a2e004":"The outer training loop goes through the number of epochs, and the inner training loop goes through all the training data in batches, the size of which is set in the code as batch_size. On the next line, we convert the data and the target variable into PyTorch variables. The input dataset has a size of (batch_size, 1, 28, 28) when retrieved from the data loader. Such a 4D tensor is more suitable for a convolutional neural network architecture than for our fully connected network. However, it is necessary to reduce the data dimension from (1,28,28) to the one-dimensional case for 28 x 28 = 784 input nodes.","bd0b69f3":"### Benefits of PyTorch\nFollowing are the benefits of PyTorch:\n\n1. The code is easy to debug and understand.\n\n2. It includes many layers, just like a torch.\n\n3. Includes many loss functions.\n\n4. It can be thought of as a NumPy extension for GPUs.\n\n5. This allows you to build networks, the structure of which depends on the calculations themselves.","0883a8a8":"- As a result, as you can see from plot, while loss decreasing, accuracy is increasing and our model is learning(training). \n- Thanks to hidden layers model learnt better and accuracy(almost 95%) is better than accuracy of logistic regression model.","5f944819":"<a id=\"4\"><\/a> <br>\n### Artificial Neural Network (ANN)\n- Logistic regression is good at classification but when complexity(non linearity) increases, the accuracy of model decreases.\n- Therefore, we need to increase complexity of model.\n- In order to increase complexity of model, we need to add more non linear functions as hidden layer. \n- What we expect from artificial neural network is that when complexity increases, we use more hidden layers and our model can adapt better. As a result accuracy increase.\n- **Steps of ANN:**\n   \n    1. Create ANN Model\n        - We add 3 hidden layers.\n        - We use ReLU, Tanh and ELU activation functions for diversity.\n    2. Instantiate Model Class\n        - input_dim = 28x28 # size of image px*px\n        - output_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n        - Hidden layer dimension is 150. I only choose it as 150 there is no reason. Actually hidden layer dimension is hyperparameter and it should be chosen and tuned. You can try different values for hidden layer dimension and observe the results.\n        - create model\n    3. Instantiate Loss\n        - Cross entropy loss\n        - It also has softmax(logistic function) in it.\n    4. Instantiate Optimizer\n        - SGD Optimizer\n    5. Traning the Model\n    6. Prediction\n","69d1dba8":"### Network training\nNext, you need to specify the optimization method and quality criterion:","077c248e":"### Computational graphs\nThe first thing to understand about any deep learning library is the idea of computational graphs. A computational graph is a set of computations, called nodes, that are connected in direct computational order. In other words, the selected node depends on the nodes in the input, which in turn performs calculations for other nodes. Below is a simple example of a computational graph for evaluating the expression a = (b + c) * (c + 2). You can break the calculation into the following steps:\n\n\n![Simple-graph-example-260x300.png](attachment:2611ca42-e3a4-40c1-9b29-8074c0fe2ee7.png)\n\nThe advantage of using a computational graph is that each node is an independent functioning piece of code if it receives all the necessary input data. This allows you to optimize performance when performing calculations using multi-channel processing, parallel computing. All major deep learning frameworks (TensorFlow, Theano, PyTorch, and so on) include computational graph constructs that perform operations inside neural networks and backpropagate the error gradient.","9e334473":"### Tensors\nTensors are matrix-like data structures that are integral components in deep learning libraries and are used for efficient computation. Graphics processing units (GPUs) are efficient at computing operations between tensors, which has spurred a wave of opportunity in deep learning. In PyTorch, tensors can be defined in several ways:","50083aab":"### Pytorch Tutorial for beginners\n\n\nIf you've tried building your own deep neural networks with TensorFlow and Keras, you're probably familiar with the frustration of debugging these libraries. Although they have a Python API, it's still hard to figure out exactly what went wrong with an error. They also don't work well with numpy, scipy, scikit-learn, Cython, and others. The PyTorch deep learning library has the claimed advantage of working well with Python and built for Python apologists. In addition, a nice property of PyTorch is the construction of a computational dynamic graph, the opposite of the static computational graphs presented in TensorFlow and Keras. PyTorch is now on the rise and is being used in development by Facebook, Twitter, NVIDIA and other companies.\n\nThe first question to consider is is PyTorch really better than TensorFlow? This is subjective as there are no big differences in terms of performance. In any case, PyTorch has become a serious contender in the competition between deep learning libraries. Let's start exploring the library, leaving the question of which is better to think about.","8712a3d6":"One more example:\n- Assume we have equation y = x^2\n- Define x = [2,4] variable\n- After calculation we find that y = [4,16] (y = x^2)\n- Recap o equation is that o = (1\/2)sum(y) = (1\/2)sum(x^2)\n- deriavative of o = x\n- Result is equal to x so gradients are [2,4]\n\n\nLets implement:","78647c80":"### Conclusion\nIn this tutorial, we learn: \n1. Basics of pytorch\n2. Logistic regression with pytorch\n3. Artificial neural network with with pytorch\n4. Convolutional neural network with pytorch\n5. Recurrent neural network with pytorch","e9d6225e":"<a id=\"1\"><\/a> <br>\n### Recurrent Neural Network (RNN)\n- RNN is essentially repeating ANN but information get pass through from previous non-linear activation function output.\n- **Steps of RNN:**\n\n    1. Create RNN Model\n        - hidden layer dimension is 100\n        - number of hidden layer is 1 \n    2. Instantiate Model\n    3. Instantiate Loss\n        - Cross entropy loss\n        - It also has softmax(logistic function) in it.\n    4. Instantiate Optimizer\n        - SGD Optimizer\n    5. Traning the Model\n    6. Prediction","e39a43d2":"### Creating a Neural Network in PyTorch\nHere we will create a simple neural network with 4 layers, including an input layer and two hidden layers, to classify handwritten characters in the MNIST dataset. The architecture that we will use is shown in the picture:\n\n![CNTK-Dense-example-architecture-1.jpg](attachment:cc02e7c8-9c9e-4821-86a9-7c9ecc97d994.jpg)\n\n\nThe input layer consists of 28 x 28 = 784 grayscale pixels that make up the input data in the MNIST dataset. The input data is then passed through two hidden layers, each containing 200 nodes using a linear rectifier activation function (ReLU). Finally, we have an output layer with ten nodes corresponding to ten handwritten digits from 0 to 9. For such a classification problem, we will use a softmax output layer."}}