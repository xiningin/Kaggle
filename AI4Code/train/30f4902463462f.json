{"cell_type":{"e254a50c":"code","c5045317":"code","5fca426f":"code","5a94b1d1":"code","cd8c8d86":"code","8585270c":"code","1f5c532e":"code","93b12b41":"code","1002a35d":"code","69f549bd":"code","1d6dd6b8":"code","38219454":"code","41080e05":"code","4e3db392":"code","03784e45":"code","61555dd9":"code","0fba8f76":"code","9afe3a0b":"code","8b3b11d3":"code","0c93d905":"code","1d3e2b53":"markdown","66ae4afe":"markdown","47fad7aa":"markdown","3333c796":"markdown","bc372b5f":"markdown","38faa973":"markdown","9bf8d1bd":"markdown","19710069":"markdown","3b91aa07":"markdown","1e831669":"markdown","f15c9fbb":"markdown","ebe66770":"markdown"},"source":{"e254a50c":"# Data Handling\nimport pandas as pd\nimport numpy as np\n\n# Visualizations \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Machine Learning Models \/ Algorithms \nimport xgboost as xgb\nimport sklearn\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","c5045317":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\nPassengerId = test_df['PassengerId']\n\nprint(\"Heads and Tail:\")\ndisplay(train_df.head(5))\ndisplay(train_df.tail(5))\n\nprint(\"Columns Overview:\")\nprint(train_df.columns)\n\nprint(\"General Description:\")\ndisplay(train_df.describe())\n\nprint(\"Interesting Cross Tables\")\ndisplay(pd.crosstab(train_df.Survived, train_df.Sex))\ndisplay(pd.crosstab(train_df.Survived, train_df.Age))\ndisplay(pd.crosstab(train_df.Survived, train_df.Pclass))\ndisplay(pd.crosstab(train_df.Survived, train_df.SibSp))\ndisplay(pd.crosstab(train_df.Survived, train_df.Parch))\ndisplay(pd.crosstab(train_df.Survived, train_df.Fare))\ndisplay(pd.crosstab(train_df.Survived, train_df.Embarked))\n\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):\n    print(\"Covariance Matrix:\")\n    display(train_df.cov())\n    print(\"Correlation Matrix:\")\n    display(train_df.corr())","5fca426f":"datasets = [train_df, test_df]\n\nfor dataset in datasets:\n    dataset['HasCabin'] = dataset['Cabin'].apply(lambda x: 0 if type(x) == float else 1)\n    \n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    \n    dataset['Fare'].fillna(dataset['Fare'].mean(), inplace=True)\n    dataset['CategoricalFare'] = pd.qcut(dataset['Fare'], 5, labels=False)\n    \n    dataset['Age'].fillna(dataset['Age'].mean(), inplace=True)\n    dataset['CategoricalAge'] = pd.qcut(dataset['Age'], 5, labels=False)\n    \ndisplay(datasets[0])\n","5a94b1d1":"sex_mapping = {'male': 1, 'female': 2,}\nembark_mapping = {'C': 1, 'S': 2, 'Q': 3}\n\nfor dataset in datasets:\n    dataset = dataset.replace({'Sex': sex_mapping, 'Embarked': embark_mapping}, inplace=True)\n","cd8c8d86":"# Replace last NaN values in Embarked Feautre\nfor dataset in datasets:\n    dataset['Embarked'].fillna(dataset['Embarked'].mean(), inplace=True)","8585270c":"# train_df = train_df.filter(items=['Survived', 'Sex', 'Age', 'Pclass', 'SibSp', 'Parch', 'Fare', 'Embarked', 'HasCabin', 'FamilySize', 'IsAlone', 'CategoricalFare', 'CategoricalAge'])\n# test_df = test_df.filter(items=['PassengerId', 'Sex', 'Age', 'Pclass', 'SibSp', 'Parch', 'Fare', 'Embarked', 'HasCabin', 'FamilySize', 'IsAlone', 'CategoricalFare', 'CategoricalAge'])\ntrain_df = train_df.filter(items=['Survived', 'Sex', 'Age', 'Pclass', 'Fare', 'HasCabin', 'FamilySize'])\ntest_df = test_df.filter(items=['PassengerId', 'Sex', 'Age', 'Pclass', 'Fare', 'HasCabin', 'FamilySize'])","1f5c532e":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train_df.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","93b12b41":"class SklearnHelper(object):\n    def __init__(self, classifier, seed=0, params=None):\n        params['random_state'] = seed\n        self.classifier = classifier(**params)\n        \n    def train(self, x_train, y_train):\n        self.classifier.fit(x_train, y_train)\n    \n    def predict(self, x):\n        return self.classifier.predict(x)\n    \n    def fit(self, x, y):\n        return self.classifier.fit(x, y)\n    \n    def feature_importances(self, x, y):\n        return self.classifier.fit(x, y).feature_importances_","1002a35d":"TRAINSIZE = train_df.shape[0]\nTESTSIZE = test_df.shape[0]\nSEED = 0\nNFOLDS = 5\nkf = KFold(n_splits=NFOLDS, random_state=SEED)","69f549bd":"def get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((TRAINSIZE, ))\n    oof_test = np.zeros((TESTSIZE, ))\n    oof_test_skf = np.empty((NFOLDS, TESTSIZE))\n    \n    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n        \n        clf.train(x_tr, y_tr)\n        \n        oof_train[test_index] = clf.predict(x_te)\n        \n        oof_test_skf[i, :] = clf.predict(x_test)\n        \n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","1d6dd6b8":"# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025\n}","38219454":"# Create 5 objects that represent our 4 models\nrf = SklearnHelper(classifier=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(classifier=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(classifier=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(classifier=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnHelper(classifier=SVC, seed=SEED, params=svc_params)","41080e05":"y_train = train_df['Survived'].ravel()\ntrain_df = train_df.drop(['Survived'], axis=1)\ntest_df = test_df.drop(['PassengerId'], axis=1)\nx_train = train_df.values\nx_test = test_df.values","4e3db392":"# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf, x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")","03784e45":"cols = train_df.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n    'Random Forest feature importances': rf.feature_importances(x_train,y_train),\n    'Extra Trees  feature importances': et.feature_importances(x_train, y_train),\n    'AdaBoost feature importances': ada.feature_importances(x_train, y_train),\n    'Gradient Boost feature importances': gb.feature_importances(x_train,y_train)\n})\n\n# Create the new column containing the average of values\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)","61555dd9":"y = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='bar-direct-labels')","0fba8f76":"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()","9afe3a0b":"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)","8b3b11d3":"gbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)","0c93d905":"# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId, 'Survived': predictions})\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)","1d3e2b53":"# Generate First-Level Model (Base)\n\n## Parameter definitions for various models","66ae4afe":"## Create Submission file","47fad7aa":"# Second-Level Predictions from First-Level Output","3333c796":"## Out Of Fold Predictions\n\nReduces the risk of overfitting.","bc372b5f":"# Data Pre-processing\n\n## Feature Engineering\nTo get more features that can help during the training of our model we can create new features based on the already existing ones. I choose a subset of new features from [Anisotropic's Introduction](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python). \n\nAt the same time I am handling the 'not a number' (NaN) entries or Null-entries in some columns. To keep proper dimensions for the submission file I decided to replace them by the mean value instead of removing the data rows entirely. At the same time I replace not a number (NaN) values with the mean of the column.","38faa973":"## Nominal to Ordinal Data Mapping\nMap nomial data to ordinal data:","9bf8d1bd":"# Visualisations","19710069":"## Feature Selection\n\nSelect features \/ columns that might be promissing for classification.","3b91aa07":"# Ensambling and Stacking models\n\n## Helper Class to reduce duplicate code","1e831669":"## Hyperparameters","f15c9fbb":"# Data Exploration\n\nBefore extracting or engineering features I would like to get to know the data and explore it. For that I just display the first and last few rows, get a general description and look into some of the more promising features and cross tables. ","ebe66770":"# Titanic Survival Prediction\n\nThis is a short and simple introduction into machine learning ensambles. \nThe first three commits were based on my own exeperiments. After that I reworked it after the tutorial from [Anisotropic's Introduction](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python).\n\nMy primary purpose here, is to practice this craft myself and learn new things. Maybe some others will find it interesting and helpful also. For suggestions and remarks on how I can improve, please feel free to give any advice. "}}