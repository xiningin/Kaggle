{"cell_type":{"d03317ef":"code","c975dfae":"code","31a86b72":"code","812e0d6f":"code","b1f220f0":"code","5064d937":"code","af1ed4d4":"code","3da6e6d4":"code","fc65a48c":"code","375203aa":"code","068804f7":"code","380cb10e":"code","58344f56":"code","b7edbd20":"code","63a8296c":"code","3dae0f01":"code","53a585bf":"code","9aa29c8b":"code","6d881ede":"code","c910c9ab":"code","a138e83d":"code","114784ba":"code","8c646402":"code","b8d33468":"code","1375c166":"code","f35cb497":"code","fc2b5f24":"code","20d4ea13":"code","f1154a40":"code","61986dc1":"code","ff990293":"code","badde774":"code","e0486e29":"code","b1b957d8":"code","67f61e0d":"code","3360cb20":"markdown","8e7952b5":"markdown","8abe6573":"markdown","7f906336":"markdown","5a2e0ec6":"markdown","9c5d7a34":"markdown","6ce16593":"markdown","64ab281d":"markdown","5c3f7a82":"markdown","c04fdc4e":"markdown","4bf2fbf0":"markdown","d1005f7a":"markdown","9c023cf3":"markdown","2e8635ed":"markdown","eed31be7":"markdown","024437b9":"markdown","b98b56b1":"markdown","2ace009b":"markdown","b1dcaea0":"markdown","93f09552":"markdown","fa17c78d":"markdown","429359c5":"markdown","7bb67803":"markdown","21dfc058":"markdown","1586e332":"markdown"},"source":{"d03317ef":"import pandas as pd\nimport numpy as np","c975dfae":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","31a86b72":"train = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\n\ntest = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')","812e0d6f":"train = train.dropna()\ntest = test.dropna()\n\n# This is the simplest approach, however you can replace N\/A values with mean\/median, Nth percentile, to avoid data distortion.","b1f220f0":"# Here I will use a very powerful library which provides almost all the necessary EDA features out-of-the-box.\n\nimport pandas_profiling as pp\npp.ProfileReport(train)","5064d937":"train['target'].value_counts()","af1ed4d4":"print(pd.pivot_table(train, values='target',\n                    columns=['relevent_experience'], aggfunc=np.sum).T.sort_values('target', ascending=False))\n\nprint(pd.pivot_table(train, values='target',\n                    columns=['education_level'], aggfunc=np.sum).T.sort_values('target', ascending=False))\n\nprint(pd.pivot_table(train, values='target',\n                    columns=['enrolled_university'], aggfunc=np.sum).T.sort_values('target', ascending=False))\n\nprint(pd.pivot_table(train, values='target',\n                    columns=['gender'], aggfunc=np.sum).T.sort_values('target', ascending=False))\n\nprint(pd.pivot_table(train, values='target',\n                    columns=['major_discipline'], aggfunc=np.sum).T.sort_values('target', ascending=False))\n\nprint(pd.pivot_table(train, values='target',\n                    columns=['company_type'], aggfunc=np.sum).T.sort_values('target', ascending=False))","3da6e6d4":"from sklearn.preprocessing import LabelEncoder\n\n# I do this manually to explicitly tell the model that a better education & experience serves well as a trustworthy input.\n\n# However, later we wil see the feature importanes report in SHAP and notice interesting results.\nexperience_dict = {'Has relevent experience' : 1,\n             'No relevent experience': 0}\n\neducation_dict = {'Graduate' : 2,\n             'Masters' : 1,\n             'Phd' : 0}\n\nenrollment_dict = {'no_enrollment' : 2,\n             'Full time course' : 1,\n             'Part time course' : 0}\n\ngender_dict = {'Male' : 2,\n             'Female' : 1,\n             'Other' : 0}\n\ndiscipline_dict = {'STEM' : 5,\n             'Humanities' : 4,\n             'Business Degree' : 3,\n             'Other' : 2,\n             'No Major' : 1,\n             'Arts' : 0 }\n\ncompany_dict = {'Pvt Ltd' : 5,\n             'Funded Startup' : 4,\n             'Public Sector' : 3,\n             'Early Stage Startup' : 2,\n             'NGO' : 1,\n             'Other' : 0 }\n\n\n# Train encoding\nle = LabelEncoder()\ntrain['gender'] = train['gender'].map(gender_dict)\ntrain['relevent_experience'] = train['relevent_experience'].map(experience_dict)\ntrain['education_level'] = train['education_level'].map(education_dict)\ntrain['enrolled_university'] = train['enrolled_university'].map(enrollment_dict)\ntrain['major_discipline'] = train['major_discipline'].map(discipline_dict)\ntrain['experience'] = le.fit_transform(train['experience'].astype(str))\ntrain['company_size'] = le.fit_transform(train['company_size'].astype(str))\ntrain['company_type'] = train['company_type'].map(company_dict)\ntrain['last_new_job'] = le.fit_transform(train['last_new_job'].astype(str))\n#train['city'] = le.fit_transform(train['city'].astype(str))\n\ntrain = pd.get_dummies(train, columns=['city']) # I do one-hot encoding here, since a higher value of the encoded feature is not related to the 'importance' of a feature.\n\n# Test encoding\ntest['gender'] = le.fit_transform(test['gender'].astype(str))\ntest['relevent_experience'] = test['relevent_experience'].map(experience_dict)\ntest['education_level'] = test['education_level'].map(education_dict)\ntest['enrolled_university'] = test['enrolled_university'].map(enrollment_dict)\ntest['major_discipline'] = test['major_discipline'].map(discipline_dict)\ntest['experience'] = le.fit_transform(test['experience'].astype(str))\ntest['company_size'] = le.fit_transform(test['company_size'].astype(str))\ntest['company_type'] = test['company_type'].map(company_dict)\ntest['last_new_job'] = le.fit_transform(test['last_new_job'].astype(str))\n#test['city'] = le.fit_transform(test['city'].astype(str))\n\n\ntest = pd.get_dummies(test, columns=['city'])","fc65a48c":"#train = train.drop('enrollee_id', axis=1)\n#test = test.drop('enrollee_id', axis=1)","375203aa":"train['city_development_index'].value_counts()","068804f7":"X = train.drop('target', axis=1)\ny = train['target']","380cb10e":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)\n\n# Further in this notebook we will use 'val' for validation dataset, since we have all the corresponding data and columns unlike in the 'test' dataset.\n# Test dataset does not contain the target and thus we will not be able to measure the performance of the model.","58344f56":"#X_test = test\n#y_test = test['target']","b7edbd20":"train['city_development_index'].value_counts()","63a8296c":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, ThresholdedReLU","3dae0f01":"norm = tf.keras.layers.LayerNormalization(\n    epsilon=0.001,\n    center=True,\n    scale=True\n)","53a585bf":"metrics = [\n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.AUC(name='auc'),\n]","9aa29c8b":"model = Sequential()\n\nmodel.add(norm)\nmodel.add(ThresholdedReLU(theta=10)) # Theta is a threshold which determines the output result of a particular neuron.\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(100, activation='softmax'))\nmodel.add(Dense(1, activation='sigmoid'))","6d881ede":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n#model.summary()","c910c9ab":"model.fit(X_train, y_train,\n          epochs=10) # A higher N of epochs doesn't improve the performance since the dataset is small.","a138e83d":"model.evaluate(X_val, y_val)","114784ba":"#conda install -c conda-forge lightgbm \nfrom lightgbm import LGBMClassifier","8c646402":"lgbm = LGBMClassifier(objective='binary', num_leaves=10, learning_rate=0.05, \n                      max_depth=1, n_estimators=50, boosting_type='goss') # You can play with hyperparameters, pay special attention to num_leaves, max_depth and n_estimators.\nlgbm.fit(X_train, y_train)\ny_pred = lgbm.predict(X_val)\n\n#cross_val_score(lgbm, X_test, y_test, cv=3)","b8d33468":"pd.DataFrame(lgbm.feature_importances_, X_train.columns)","1375c166":"from sklearn.metrics import f1_score, confusion_matrix, recall_score, precision_score, accuracy_score\n\n#confusion_matrix(y_test, y_pred)\nprint('Accuracy: %f, \\nRecall: %f \\nPrecision: %f'\n      % (accuracy_score(y_val, y_pred), recall_score(y_val, y_pred), precision_score(y_val, y_pred)))","f35cb497":"import shap\n\nX_importance = X_train\n\nexplainer = shap.TreeExplainer(lgbm)\nshap_values = explainer.shap_values(X_importance)\nshap.summary_plot(shap_values, X_importance)","fc2b5f24":"train[train['target'] == 0]['city_development_index'].describe()","20d4ea13":"train[train['target'] == 1]['city_development_index'].describe()","f1154a40":"cdi = pd.DataFrame(train['city_development_index'].value_counts())\ncdi.head(10)","61986dc1":"def change(x):\n    x = np.random.randint(400, 800)\/1000\n    return x\n\ndef change2(x):\n    x = np.random.randint(0, 21)\n    return x","ff990293":"# Slice 1\n\ninsert1 = train[train['city_development_index'] == 0.897].sample(frac=1)\ninsert1['experience'] = insert1['experience'].apply(lambda x: change2(x))\n#insert1['city_development_index'] = insert1['city_development_index'].apply(lambda x: change(x))","badde774":"# Slice 2\n\ninsert2 = train[train['city_development_index'] == 0.926].sample(frac=1)\ninsert2['experience'] = insert2['experience'].apply(lambda x: change2(x))\n#insert2['city_development_index'] = insert2['city_development_index'].apply(lambda x: change(x))","e0486e29":"dfs = [train, insert1, insert2]\ntrain_new = pd.concat(dfs)","b1b957d8":"y_train_new = train_new['target']\nX_train_new = train_new.drop(['target'], axis=1)","67f61e0d":"lgbm2 = LGBMClassifier()\nlgbm2.fit(X_train_new, y_train_new)\ny_pred2 = lgbm2.predict(X_val)\n\nprint('Accuracy: %f, \\nRecall: %f \\nPrecision: %f'\n      % (accuracy_score(y_val, y_pred2), recall_score(y_val, y_pred2), precision_score(y_val, y_pred2)))","3360cb20":"## 6.1 Feature importances","8e7952b5":"**Now we observe a spike in both accuracy and precision. Thus, bootstrapping has proven its efficiencty in this particular dataset. Even though we have used only 2 samples, we can take it further and improve the model performance.**","8abe6573":"**We need to get samples of the DataFrame with target=1 and pick several slices of the dataset with underrepresented 'city_development_index' features (with indices between 0.4 and 0.7).**\n\n**Why? Because cities with lower indies have more Data Scientists who change the job. And they are poorly represented in the original dataset.**","7f906336":"**As the performance of both DNN and LGBM model is not perfect, the further steps to complete might be as follows:**\n\n1. Bootstrapping the dataset to make it more balanced. (see **Step 7**)\n\n2. Feature insertion & feature engineering based on the most important features.\n\n3. Play with neural networks & try to use recurrent networks. Or add different layers.\n\n4. Use other conventional ML models and\/or Boosting (e.g. CAT boost).\n\n","5a2e0ec6":"**To interpret the results of the Neural network, you can use:**\n\n1) feature permutation;\n\n2) SHAP library (see further);\n\n3) LIME library.","9c5d7a34":"# 4. Model building\n## 4.1. Deep Neural Network","6ce16593":"**A lot of background-related differences in these features. We will need to encode them manually to improve the model.**\n\n**The number of people who change the job vary significantly and inconsistenly!**","64ab281d":"# 6. Conclusion","5c3f7a82":"**Apparently, the most important feature in the task is city development index. Which is not quite good, because it predominates over other features.**","c04fdc4e":"# 7. Bootstrapping","4bf2fbf0":"# HR Analytics notebook","d1005f7a":"**So, the dataset appears to be quite small, which is quite good for the purpose of exercise, though lack of data can result in ending up with a poor performing model.**","9c023cf3":"**Another thing, the target has a long-tail distribution which means that the dataset is quite imbalanced.\n80% of target is '0', while 20% is '1'. Therefore, we need to evaluate a model based not only on accuracy score, but also precision & recall (confusion matrix).**","2e8635ed":"# 1. Import data","eed31be7":"**Let's look at how the number of Data Scientists who change the job varies across features.**","024437b9":"**First, let's create a normalization layer for input data.**","b98b56b1":"## 6.2 Further steps","2ace009b":"## 4.2. Gradient Boosting","b1dcaea0":"**Here you can add whatever metrics you are interested in.**","93f09552":"# 2. EDA","fa17c78d":"<img src=\"https:\/\/www.digitalvidya.com\/wp-content\/uploads\/2019\/05\/HR-Analytics.jpg\" width=500 height=200>","429359c5":"**We will use a very basic neural network here with 5 layers.**","7bb67803":"# 3. Feature preparation","21dfc058":"**Not the stellar performance, but anyway. Let's try Gradient Boosting.**","1586e332":"* **Task type:** classification\n* **Models used:** DNN, LGBM\n* **Other methods used:** shap"}}