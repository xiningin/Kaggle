{"cell_type":{"3990b182":"code","ccdf8e0b":"code","417088d5":"code","3e641787":"code","776d4f4a":"code","8e1009d3":"code","eff53b8a":"code","b5120397":"code","34da80b8":"code","770407c7":"code","16314b5d":"code","a7f5fc85":"code","04fca7b2":"code","f043ca7f":"code","bf548392":"code","1e2e2333":"code","1c1979ee":"code","1c2f041c":"code","911f3502":"code","ae66ba3f":"code","d70d7f75":"markdown","aff614d4":"markdown","cadf2b28":"markdown","4be73791":"markdown","ed3c5cbc":"markdown","3c9aff39":"markdown","4e73cd58":"markdown","7393d19a":"markdown","6bfb4002":"markdown","dfd444d4":"markdown","8cd4d057":"markdown","06a58e5a":"markdown","e3ac78e8":"markdown","92d5f257":"markdown","36d4f90c":"markdown"},"source":{"3990b182":"import os\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\nfrom torch.utils.tensorboard import SummaryWriter\nimport torchvision.models as models\nfrom torchvision import datasets, transforms, utils\n\nimport cv2\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom skimage import io, transform\nimport tensorboard as tb\nfrom dataclasses import dataclass\nfrom typing import Iterable\nfrom sklearn.model_selection import KFold\n\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","ccdf8e0b":"@dataclass\nclass SystemConfig:\n    '''Describes the common system setting needed for reproducible training'''\n    seed: int = 11 \n    cudnn_benchmark_enabled: bool = True\n    cudnn_deterministic: bool = True \n   \n\n@dataclass\nclass TrainingConfig:\n    '''Describes configuration of the training process'''\n    device: str = 'cuda'\n    model_save_best: bool = True\n    batch_size: int = 32\n    epochs_count: int = 12\n    log_interval: int = 5  \n    test_interval: int = 1  \n    model_name: str = 'resnet50'\n    num_workers: int = 4\n    num_classes: int = 43\n    data_augmentation: bool = False\n    mean = [0.3447, 0.3131, 0.3243]\n    std = [0.1565, 0.1575, 0.1670]\n\n@dataclass\nclass DataConfig:\n    root_dir: str = '..\/input\/gtsrb-german-traffic-sign'\n    train_dir: str = 'train\/'\n    test_dir: str = 'test\/'\n    train_csv: str = 'Train.csv'\n    test_csv: str = 'Test.csv'\n    model_dir: str = os.path.join('\/kaggle\/working\/', 'models\/')\n    log_dir: str = os.path.join('\/kaggle\/working\/', 'logs\/')\n\n@dataclass\nclass OptimizerConfig:\n    init_learning_rate: float = 0.0001 \n    weight_decay: float = 0.0001\n    scheduler_step_size: int = 6\n    scheduler_gamma: float = 0.1\n\n    \ndef setup_system(system_config: SystemConfig) -> None:\n    torch.manual_seed(system_config.seed)\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda:0\")\n        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic\n        \nsc = SystemConfig()\ntc = TrainingConfig()\ndc = DataConfig()\noc = OptimizerConfig()\n\nsetup_system(sc)\n\n\nfor path in [dc.log_dir, dc.model_dir]:\n    if not os.path.exists(path):\n        os.makedirs(path)\n        \ntb_writer = SummaryWriter(dc.log_dir)","417088d5":"train_file = os.path.join(dc.root_dir, dc.train_csv)\ntest_file = os.path.join(dc.root_dir, dc.test_csv)","3e641787":"df_train = pd.read_csv(train_file)\ndf_train.head()","776d4f4a":"df_test = pd.read_csv(test_file)\ndf_test.head()","8e1009d3":"print(f'Train csv shape: {df_train.shape}, \\nTest csv shape: {df_test.shape}')","eff53b8a":"class_list = []\nimgs = []\nfor index, data in df_train.iterrows():\n    file_name = data['Path']\n    class_id = data['ClassId']\n    if class_id not in class_list:\n      class_list.append(class_id)\n      imgs.append(mpimg.imread(os.path.join(dc.root_dir,file_name)))\n    if index % 10000 == 0:\n      print(f'Currently on row {index} of 39209')","b5120397":"plt.figure(figsize=(20,15))\nplt.suptitle('Sample images from each class')\ncolumns = 10\nfor i, image in enumerate(imgs):\n    plt.subplot(len(imgs) \/ columns + 1, columns, i + 1)\n    plt.title(f'Class: {i+1}', color='black')\n    plt.imshow(image)","34da80b8":"c = df_train['ClassId'].nunique()\nx = df_train['ClassId'].value_counts()\n\nplt.bar(x=x.index.sort_values(), height=x, color='#0066ff')\nplt.title('Distibution of occurences in each class', color='black')\nplt.xlabel(\"Classes\", color='black')\nplt.ylabel(\"Occurences\", color='black')\nplt.tick_params(colors='black')","770407c7":"class GTSR43Dataset(Dataset):\n    \"\"\"German Traffic Sign Recognition dataset.\"\"\"\n    def __init__(self, root_dir, train_file, transform=None):\n        self.root_dir = root_dir\n        self.train_file_path = train_file\n        self.label_df = pd.read_csv(os.path.join(self.root_dir, self.train_file_path))\n        self.transform = transform\n        self.classes = list(self.label_df['ClassId'].unique())\n\n    def __getitem__(self, idx):\n        \"\"\"Return (image, target) after resize and preprocessing.\"\"\"\n        img = os.path.join(self.root_dir, self.label_df.iloc[idx, 7])\n        \n        X = Image.open(img)\n        y = self.class_to_index(self.label_df.iloc[idx, 6])\n\n        if self.transform:\n            X = self.transform(X)\n\n        return X, y\n    \n    def class_to_index(self, class_name):\n        \"\"\"Returns the index of a given class.\"\"\"\n        return self.classes.index(class_name)\n    \n    def index_to_class(self, class_index):\n        \"\"\"Returns the class of a given index.\"\"\"\n        return self.classes[class_index] \n    \n    def get_class_count(self):\n        \"\"\"Return a list of label occurences\"\"\"\n        cls_count = dict(self.label_df.ClassId.value_counts())\n#         cls_percent = list(map(lambda x: (1 - x \/ sum(cls_count)), cls_count))\n        return cls_count\n    \n    def __len__(self):\n        \"\"\"Returns the length of the dataset.\"\"\"\n        return len(self.label_df)\n\nclass GTSR43Subset(GTSR43Dataset):\n    \"\"\"A subset helper class for splitting the main dataset\"\"\"\n    def __init__(self, subset, transform=None):\n        self.subset = subset\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        \"\"\"Retrieves one item from the dataset.\"\"\"\n        X, y = self.subset[idx]\n        \n        if self.transform:\n            X = self.transform(X)\n\n        return X, y\n\n    def __len__(self):\n        return len(self.subset)\n    \nclass GTSR43Testset(Dataset):\n    \"\"\"German Traffic Sign Recognition dataset\"\"\"\n    def __init__(self, root_dir, test_file, transform=None):\n        self.root = root_dir\n        self.test_file_path = test_file\n        self.label_df = pd.read_csv(os.path.join(self.root_dir, self.test_file_path))\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        \"\"\"Retrieves one item from the dataset.\"\"\"\n        img = os.path.join(self.root_dir, self.label_df.iloc[idx, 7])\n        \n        image = Image.open(img)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image\n    \n    def __len__(self):\n        return len(self.label_df)","16314b5d":"ds = GTSR43Dataset(dc.root_dir, dc.train_csv)\nX, y = ds.__getitem__(5)\nprint(f'Train image: {X}\\nTarget class: {y}\\nClass count: {ds.get_class_count()}')","a7f5fc85":"def image_resize():\n    \"\"\"Transforms for resizing, cropping.\"\"\"\n    resize_transforms = transforms.Compose([transforms.Resize(32),\n                                            transforms.CenterCrop(30),\n                                           ])\n    return resize_transforms\n\ndef image_preprocess():\n    \"\"\"Transforms for resizing, cropping, then converting to Tensor.\"\"\"\n    preprocess_transforms = transforms.Compose([transforms.Resize(32),\n                                                transforms.CenterCrop(30),\n                                                transforms.ToTensor()\n                                               ])\n    return preprocess_transforms\n\ndef common_transforms(mean, std):\n    \"\"\"Transforms which are common to both the train and test set.\"\"\" \n    common_transforms = transforms.Compose([image_preprocess(),\n                                            transforms.Normalize(mean, std)\n                                           ])\n    return common_transforms\n\ndef data_aug(mean, std):\n    \"\"\"Data augmentation transforms.\"\"\"\n    data_aug_transforms = transforms.Compose([transforms.Resize(32),\n                                              transforms.CenterCrop(30),\n                                              transforms.RandomVerticalFlip(),\n                                              transforms.RandomHorizontalFlip(),\n                                              transforms.ColorJitter(),\n                                              transforms.ToTensor(),\n                                              transforms.Normalize(mean, std),\n                                              transforms.RandomErasing(),\n                                             ])        \n    return data_aug_transforms","04fca7b2":"# def get_mean_std():\n#     \"\"\"Gets mean and standard deviation.\"\"\"  \n#     ds = GTSR43Dataset(dc.root_dir, \n#                        dc.train_csv, \n#                        transform=image_preprocess())       \n#     loader = DataLoader(ds, \n#                         batch_size = 10, \n#                         shuffle = False, \n#                         num_workers = 4)\n    \n#     mean = 0.0\n#     std = 0.0\n    \n#     for images, _ in loader:\n#         batch_samples = images.size(0) # the last batch can have smaller size\n#         images = images.view(batch_samples, images.size(1), -1)\n#         mean += images.mean(2).sum(0)\n#         std += images.std(2).sum(0)\n\n#     mean \/= len(loader.dataset)\n#     std \/= len(loader.dataset)\n    \n#     return mean, std\n\n# mean, std = get_mean_std()\n# print(mean, std)","f043ca7f":"def get_data(root_dir, batch_size, num_workers=4, data_augmentation=False):\n    \"\"\"Loads and splits data into train and test subsets. \"\"\"  \n    dataset = GTSR43Dataset(root_dir, dc.train_csv)\n    \n    if data_augmentation:\n        X_transforms = data_aug(tc.mean, tc.std)\n    else:\n        X_transforms = common_transforms(tc.mean, tc.std)\n    \n    y_transforms = common_transforms(tc.mean, tc.std)\n    \n    X_size = int(0.8 * len(dataset))\n    y_size = len(dataset) - X_size\n    \n    X_dataset, y_dataset = torch.utils.data.random_split(dataset, [X_size, y_size])\n    \n    X_subset = GTSR43Subset(X_dataset, X_transforms)\n    y_subset = GTSR43Subset(y_dataset, y_transforms)\n\n    X_loader = DataLoader(X_subset, \n                          batch_size=batch_size, \n                          shuffle=False,\n                          num_workers=num_workers)  \n    \n    y_loader = DataLoader(y_subset,\n                          batch_size=batch_size, \n                          shuffle=False, \n                          num_workers=num_workers)\n    \n    return X_loader, y_loader","bf548392":"def train(tc: TrainingConfig, \n          model: nn.Module, \n          optimizer: torch.optim.Optimizer,\n          X_loader: torch.utils.data.DataLoader, \n          epoch_idx: int) -> None:\n    \n    model.train()\n    \n    batch_loss = np.array([])\n    batch_acc = np.array([])\n    \n    for batch, (X, y) in enumerate(X_loader):\n        \n        if batch == 0:\n            print(f'Device: {torch.cuda.get_device_name(0)}, Data: {X.shape}, Target: {y.shape}')       \n\n        index_y = y.clone()\n        X = X.to(tc.device)\n        y = y.to(tc.device)\n        \n        optimizer.zero_grad()\n        output = model(X)\n        loss = F.cross_entropy(output, y)\n        loss.backward()\n        optimizer.step()\n        \n        batch_loss = np.append(batch_loss, [loss.item()])\n        prob = F.softmax(output, dim=1)\n        pred = prob.data.max(dim=1)[1]  \n        correct = pred.cpu().eq(index_y).sum()\n        acc = float(correct) \/ float(len(X))\n        batch_acc = np.append(batch_acc, [acc])\n            \n    epoch_loss = batch_loss.mean()\n    epoch_acc = 100. * batch_acc.mean()\n    \n    print(f'Training   - loss: {epoch_loss:.4f}, accuracy: {epoch_acc:.2f}%')\n    \n    return epoch_loss, epoch_acc","1e2e2333":"def validate(tc: TrainingConfig,\n             model: nn.Module,\n             y_loader: torch.utils.data.DataLoader) -> float:\n\n    model.eval()\n    \n    loss = 0.0\n    correct = 0.0\n    for X, y in y_loader:\n        index_y = y.clone()\n        \n        X = X.to(tc.device)\n        y = y.to(tc.device)\n\n        output = model(X)\n        loss += F.cross_entropy(output, y).item()\n        prob = F.softmax(output, dim=1)\n        pred = prob.data.max(dim=1)[1] \n        correct += pred.cpu().eq(index_y).sum()\n\n    loss = loss \/ len(y_loader)  \n    accuracy = 100. * correct \/ len(y_loader.dataset)\n    \n    print(f'Validation - loss: {loss:.4f}, accuracy: {accuracy:.2f}%, {correct}\/{len(y_loader.dataset)}')\n    \n    return loss, accuracy\/100.0","1c1979ee":"def trainable_parameters(model):\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            print(name)\n            \n\ndef pretrained_net():        \n    model_path = os.path.join(dc.model_dir, tc.model_name)\n    try:\n        model = torch.load('\/kaggle\/working\/models\/resnet50_3_pretrained.pt')\n    except FileNotFoundError:\n        os.environ['TORCH_HOME'] = '\/kaggle\/working\/german_traffic_sign_recognition'\n        model = models.resnet50(pretrained=True, progress=True)\n        torch.save(model, '\/kaggle\/working\/models\/resnet50_pretrained.pt')\n        model = torch.load('\/kaggle\/working\/models\/resnet50_pretrained.pt')\n        \n    for param in model.parameters():\n        param.requires_grad = False\n        \n    model.fc = nn.Sequential(nn.Linear(2048, 512),\n                             nn.ReLU(inplace=True),\n                             nn.Dropout(0.5),\n                             \n                             nn.Linear(512, 512), \n                             nn.ReLU(inplace=True),\n                             nn.Dropout(0.2),\n                             \n                             nn.Linear(512, tc.num_classes))\n        \n    layers = [\n        model.layer2,\n        model.layer3,\n        model.layer4,\n        model.avgpool,\n      ]\n    \n    for layer in layers:\n        for param in layer.parameters():\n            param.requires_grad = True\n    \n    return model\n\npt_model = pretrained_net()\n\nprint(\"Layers: \\n\")\nfor name, child in pt_model.named_children():\n    print(name)\n\nprint(\"\\nCurrent Trainable Parameters: \\n\")\nprint(trainable_parameters(pt_model))","1c2f041c":"def save_model(model, device, accuracy):\n    \n    if not os.path.exists(dc.model_dir):\n        os.makedirs(dc.model_dir)\n\n    model_path = os.path.join(dc.model_dir, tc.model_name)\n\n    if device == 'cuda':\n        model.to('cpu')\n\n    torch.save(model.state_dict(), model_path + '_retrained.pt' )\n\n    if device == 'cuda':\n        model.to('cuda')\n    return\n\ndef load_model(model):\n    \n    model_path = os.path.join(dc.model_dir, tc.model_name)\n    model.load_state_dict(torch.load(model_path + '_retrained.pt'))\n    \n    return model","911f3502":"def main(model, optimizer, tb_writer, scheduler = None, data_augmentation = True):\n\n    if torch.cuda.is_available():\n        tc.device = \"cuda\"\n    else:\n        tc.device = \"cpu\"\n        batch_size_to_set = 10\n        num_workers_to_set = 2\n    \n    model.to(tc.device)\n    \n    X_loader, y_loader = get_data(root_dir=dc.root_dir, \n                                  batch_size=tc.batch_size, \n                                  num_workers=tc.num_workers, \n                                  data_augmentation=tc.data_augmentation)\n    \n    best_loss = torch.tensor(np.inf)\n    \n    epoch_X_loss = np.array([])\n    epoch_y_loss = np.array([])\n    epoch_X_acc = np.array([])\n    epoch_y_acc = np.array([])\n    \n    t_begin = time.time()      \n                            \n    for epoch in range(tc.epochs_count):\n        print(f'\\nEpoch: {epoch + 1}\/{tc.epochs_count}')    \n        \n        X_loss, X_acc = train(tc=tc, \n                              model=model, \n                              optimizer=optimizer, \n                              X_loader=X_loader, \n                              epoch_idx=epoch)\n\n        epoch_X_loss = np.append(epoch_X_loss, [X_loss])\n        epoch_X_acc = np.append(epoch_X_acc, [X_acc])\n                                \n        elapsed_time = time.time() - t_begin\n        speed_epoch = elapsed_time \/ (epoch + 1)\n        speed_batch = speed_epoch \/ len(X_loader)\n        eta = speed_epoch * tc.epochs_count - elapsed_time\n\n        tb_writer.add_scalar('Loss\/Train', X_loss, epoch)\n        tb_writer.add_scalar('Accuracy\/Train', X_acc, epoch)\n        tb_writer.add_scalar('Time\/elapsed_time', elapsed_time, epoch)\n        tb_writer.add_scalar('Time\/speed_epoch', speed_epoch, epoch)\n        tb_writer.add_scalar('Time\/speed_batch', speed_batch, epoch)\n        tb_writer.add_scalar('Time\/eta', eta, epoch)\n                             \n        if epoch % tc.test_interval == 0:\n\n            current_loss, current_acc = validate(tc, model, y_loader)\n                                                 \n            epoch_y_loss = np.append(epoch_y_loss, [current_loss])\n            epoch_y_acc = np.append(epoch_y_acc, [current_acc])\n                                    \n            if current_loss < best_loss:\n                best_loss = current_loss\n                save_model(model, device = tc.device, accuracy = current_acc)\n                print('Model Improved! Saved!')\n\n            tb_writer.add_scalar('Loss\/Validation', current_loss, epoch)\n            tb_writer.add_scalar('Accuracy\/Validation', current_acc, epoch)\n            tb_writer.add_scalars('Loss\/train-val', {'train': X_loss, 'validation': current_loss}, epoch)\n            tb_writer.add_scalars('Accuracy\/train-val', {'train': X_acc,'validation': current_acc}, epoch)\n                                  \n        if scheduler is not None:\n            scheduler.step()  \n\n        print(f'Time: {elapsed_time:.2f}s, {speed_epoch:.2f} s\/epoch, {speed_batch:.2f} s\/batch, Learning rate: {scheduler.get_last_lr()[0]}') \n        \n    print(f'Total time: {time.time() - t_begin:.2f}, Best loss: {best_loss:.3f}')    \n    \n    return model, epoch_X_loss, epoch_X_acc, epoch_y_loss, epoch_y_acc","ae66ba3f":"model = pretrained_net()\n\noptimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n                             lr=oc.init_learning_rate,\n                             weight_decay=oc.weight_decay)\n\nscheduler = optim.lr_scheduler.StepLR(optimizer, \n                                      step_size=oc.scheduler_step_size, \n                                      gamma=oc.scheduler_gamma)\n\nprint(f'Device: {tc.device}\\n\\\nEpochs: {tc.epochs_count}\\n\\\nBatch size: {tc.batch_size}\\n\\\nData Augmentation: {tc.data_augmentation}\\n\\\nScheduler step size: {oc.scheduler_step_size}\\n\\\nScheduler gamma: {oc.scheduler_gamma}\\n\\\nLearning rate: {oc.init_learning_rate}\\n\\\nL2 weight decay: {oc.weight_decay}')\n\nmodel, train_loss, train_acc, val_loss, val_acc = main(model, \n                                                       optimizer, \n                                                       tb_writer, \n                                                       scheduler=scheduler, \n                                                       data_augmentation=tc.data_augmentation)","d70d7f75":"# Mean and Std Dev\nHelper function for extracting the mean and std dev of the images in the dataset","aff614d4":"# Configs and hyper-params","cadf2b28":"# Custom Dataset\nCreate a custom dataset class with the proper methods for importing the data","4be73791":"Putting it all together","ed3c5cbc":"# Data augmentation\nHelper functions for handling data augmenation.","3c9aff39":"Test Function","4e73cd58":"Verify that the custom dataset and methods work","7393d19a":"Helper function for loading the data into Train\/Test subsets","6bfb4002":"Load and Save functions","dfd444d4":"Train function","8cd4d057":"# Data\n\nLoad the csv files and view some of the data","06a58e5a":"# Package imports","e3ac78e8":"Main function","92d5f257":"# Data distribution\n\nDisplaying a bar chart which shows the distribution of class occurences within the dataset.","36d4f90c":"Load pretrained model"}}