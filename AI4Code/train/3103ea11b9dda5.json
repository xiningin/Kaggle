{"cell_type":{"2cd08fae":"code","46e51330":"code","d493fc34":"code","11f1afb1":"code","eeb6fc8f":"code","5169009c":"code","f5a86144":"code","861b7924":"code","73f3ec9d":"code","66bc492c":"code","bd7b36f1":"code","f33057f5":"code","11bce499":"code","bfcba605":"code","d9b07b47":"markdown","f07d6dae":"markdown","0926ae6f":"markdown","f0f2ae3b":"markdown","81911cbd":"markdown","9eaf2565":"markdown","c7435610":"markdown","d18caae8":"markdown"},"source":{"2cd08fae":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport re\nimport warnings\n\nimport lightgbm as lgb\nfrom unidecode import unidecode\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom itertools import combinations\nfrom datetime import datetime\n\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = 50\nplt.style.use('ggplot')\n\n%matplotlib inline","46e51330":"df_train = pd.read_csv('\/kaggle\/input\/kalapas\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/kalapas\/test.csv')\ndf_all = df_train.drop(['label'], 1).append(df_test)\ndf_all.info()","d493fc34":"# Process date\/datetime fields\nDATE = [\"Field_{}\".format(i) for i in [5, 6, 7, 8, 9, 11, 15, 25, 32, 33, 34, 35, 40]]\nDATETIME = [\"Field_{}\".format(i) for i in [1, 2, 43, 44]]\n\ndf_all[DATE + DATETIME + [\"Field_34\", \"ngaySinh\"]].sample(10)","11f1afb1":"def correct_34_ngaysinh(s):\n    if s != s:\n        return np.nan\n    try:\n        s = int(s)\n    except ValueError:\n        s = s.split(\" \")[0]\n        \n    return datetime.strptime(str(s)[:6], \"%Y%m\")\n\ndef datetime_normalize(s):\n    if s != s:\n        return np.nan\n    \n    s = s.split(\".\")[0]\n    if s[-1] == \"Z\":\n        s = s[:-1]\n        \n    date, time = s.split(\"T\")\n    datetime_obj = datetime.strptime(s, \"%Y-%m-%dT%H:%M:%S\")\n    return datetime_obj\n\ndef date_normalize(s):\n    if s != s:\n        return np.nan\n    \n    try:\n        datetime_obj = datetime.strptime(s, \"%m\/%d\/%Y\")\n    except:\n        datetime_obj = datetime.strptime(s, \"%Y-%m-%d\")\n        \n    return datetime_obj\n\ndef process_datetime_cols(df):\n    cat_cols = []\n    for col in DATETIME:\n        df[col] = df[col].apply(datetime_normalize)\n        \n    for col in DATE:\n        if col == \"Field_34\":\n            continue\n        df[col] = df[col].apply(date_normalize)\n\n    df[\"Field_34\"] = df[\"Field_34\"].apply(correct_34_ngaysinh)\n    df[\"ngaySinh\"] = df[\"ngaySinh\"].apply(correct_34_ngaysinh)\n\n    cat_cols += DATE + DATETIME\n    for col in DATE + DATETIME:\n        df[col] = df[col].dt.strftime('%m-%Y')\n    \n    for cat in ['F', 'E', 'C', 'G', 'A']:\n        df[f'{cat}_startDate'] = pd.to_datetime(df[f\"{cat}_startDate\"], infer_datetime_format=True)\n        df[f'{cat}_endDate'] = pd.to_datetime(df[f\"{cat}_endDate\"], infer_datetime_format=True)\n        \n        df[f'{cat}_startDate'] = df[f'{cat}_startDate'].dt.strftime('%m-%Y')\n        df[f'{cat}_endDate'] = df[f'{cat}_endDate'].dt.strftime('%m-%Y')\n        \n        cat_cols.append(f'{cat}_startDate')\n        cat_cols.append(f'{cat}_endDate')\n    \n    for col in cat_cols:\n        df[col] = df[col].astype(\"category\")\n        \n    return df","eeb6fc8f":"def str_normalize(s):\n    s = str(s).strip().lower()\n    s = re.sub(' +', \" \", s)\n    return s\n\ndef process_location(df):\n    for col in [\"currentLocationLocationId\", \"homeTownLocationId\", \"currentLocationLatitude\", \"currentLocationLongitude\", \n                   \"homeTownLatitude\", \"homeTownLongitude\"]:\n        df[col].replace(0, np.nan, inplace=True)\n\n    df[\"currentLocationLocationId\"] = df[\"currentLocationLocationId\"].apply(str_normalize).astype(\"category\")\n    df[\"homeTownLocationId\"] = df[\"homeTownLocationId\"].apply(str_normalize).astype(\"category\")\n\n    return df","5169009c":"def job_category(x):\n    if type(x) == str:\n        if \"c\u00f4ng nh\u00e2n\" in x or \"cnv\" in x or \"cn\" in x or \"may c\u00f4ng nghi\u1ec7p\" in x or \"lao \u0111\u1ed9ng\" in x\\\n        or \"th\u1ee3\" in x or \"co\u00f5ng nha\u00f5n tr\u1eed\u00f9c tie\u1ecfp ma\u1ef1y may co\u00f5ng nghie\u1ecdp\" in x or \"c.n\" in x or \"l\u0111\" in x:\n            return \"CN\"\n        elif \"gi\u00e1o vi\u00ean\" in x or \"gv\" in x or \"g\u00edao vi\u00ean\" in x:\n            return \"GV\"\n        elif \"nh\u00e2n vi\u00ean\" in x or \"k\u1ebf to\u00e1n\" in x or \"c\u00e1n b\u1ed9\" in x or \"nv\" in x or \"cb\" in x or \"nh\u00f5n vi\u1eddn\" in x:\n            return \"NV\"\n        elif \"t\u00e0i x\u1ebf\" in x or \"l\u00e1i\" in x or \"t\u00e0i x\u00ea\" in x:\n            return \"TX\"\n        elif \"qu\u1ea3n l\u00fd\" in x or \"ph\u00f3 ph\u00f2ng\" in x or \"hi\u1ec7u ph\u00f3\" in x:\n            return \"QL\"\n        elif \"undefined\" in x:\n            return \"missing\"\n        elif \"gi\u00e1m \u0111\u1ed1c\" in x or \"hi\u1ec7u tr\u01b0\u1edfng\" in x:\n            return \"G\u0110\"\n        elif \"ph\u1ee5c v\u1ee5\" in x:\n            return \"PV\"\n        elif \"chuy\u00ean vi\u00ean\" in x:\n            return  \"CV\"\n        elif \"b\u00e1c s\u0129\" in x or \"d\u01b0\u1ee3c s\u0129\" in x or \"y s\u0129\" in x or \"y s\u1ef9\" in x:\n            return \"BS\"\n        elif \"y t\u00e1\" in x:\n            return \"YT\"\n        elif \"h\u1ed9 sinh\" in x:\n            return \"HS\"\n        elif \"ch\u1ee7 t\u1ecbch\" in x:\n            return \"CT\"\n        elif \"b\u1ebfp\" in x:\n            return \"\u0110B\"\n        elif \"s\u01b0\" in x:\n            return \"KS\"\n        elif \"d\u01b0\u1ee1ng\" in x:\n            return \"\u0110D\"\n        elif \"k\u1ef9 thu\u1eadt\" in x or \"k\u0129 thu\u1eadt\" in x:\n            return \"KTV\"\n        elif \"di\u1ec5n vi\u00ean\" in x:\n            return \"DV\"\n        else:\n            return \"missing\"\n    else:\n        return x    \n    \ndef process_diaChi_maCv(df):\n    df[\"maCv\"] = df[\"maCv\"].apply(str_normalize).apply(job_category).astype(\"category\")\n    return df","f5a86144":"def combine_gender(s):\n    x, y = s\n    \n    if x != x and y != y:\n        return \"nan\"\n    \n    if x != x:\n        return y.lower()\n    \n    return x.lower()\n\ndef process_gender(df):\n    df[\"gender\"] = df[[\"gioiTinh\", \"info_social_sex\"]].apply(combine_gender, axis=1).astype(\"category\")\n    return df","861b7924":"def process_misc(df):        \n    df[\"subscriberCount\"].replace(0, np.nan, inplace=True)\n    df[\"friendCount\"].replace(0, np.nan, inplace=True)\n    \n    df[\"Field_13\"] = df[\"Field_13\"].apply(lambda x: 1 if x == x else 0)\n    df[\"Field_38\"] = df[\"Field_38\"].map({0: 0.0, 1: 1.0, \"DN\": np.nan, \"TN\": np.nan, \"GD\": np.nan})\n    df[\"Field_62\"] = df[\"Field_62\"].map({\"I\": 1, \"II\": 2, \"III\": 3, \"IV\": 4, \"V\": 5, \"Ngo\u00e0i qu\u1ed1c doanh Qu\u1eadn 7\": np.nan})\n    df[\"Field_47\"] = df[\"Field_47\"].map({\"Zezo\": 0, \"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4})\n    \n    df[\"Field_27\"] = df[\"Field_27\"].replace({0.0: np.nan})\n    df[\"Field_28\"] = df[\"Field_28\"].replace({0.0: np.nan})\n        \n    for col in df.columns:\n        if df[col].dtype.name == \"object\":\n            df[col] = df[col].apply(str_normalize).astype(\"category\")\n            \n    return df\n        ","73f3ec9d":"# drop some fields we do not need (homeTown is optionally)\nDROP = [\"gioiTinh\", \"info_social_sex\", \"ngaySinh\", \"namSinh\"] + \\\n        [f\"Field_{c}\" for c in [14, 16, 17, 24, 26, 30, 31, 37, 52, 57]]\n\ndef transform(df):\n    df = process_datetime_cols(df)\n    df = process_gender(df)\n    df = process_location(df)\n    df = process_diaChi_maCv(df)\n    df = process_misc(df)\n    return df.drop(DROP, 1)","66bc492c":"df_all_fe = transform(df_all.copy())\ndf_all_fe.info()","bd7b36f1":"df_fe = df_all_fe.copy()\ndf_fe.replace([np.inf, -np.inf], 999, inplace=True)\n\nfor col in df_fe.columns:\n    if df_fe[col].dtype.name == \"category\":\n        if df_fe[col].isnull().sum() > 0:\n            df_fe[col] = df_fe[col].cat.add_categories(f'missing_{col}')\n            df_fe[col].fillna(f'missing_{col}', inplace=True)\n    else:\n        df_fe[col].fillna(-1, inplace=True)\n\ny_label = df_train[\"label\"]\ntrain_fe = df_fe[df_fe[\"id\"] < df_train.shape[0]]\ntest_fe = df_fe[df_fe[\"id\"] >= df_train.shape[0]]\n\nprint(train_fe.shape)\nprint(test_fe.shape)","f33057f5":"def gini(y_true, y_score):\n    return roc_auc_score(y_true, y_score)*2 - 1\n\ndef lgb_gini(y_pred, dataset_true):\n    y_true = dataset_true.get_label()\n    return 'gini', gini(y_true, y_pred), True\n\nNUM_BOOST_ROUND = 1000\n\nlgbm_param = {'objective':'binary',\n              'boosting_type': 'gbdt',\n              'metric' : 'auc',\n              'learning_rate': 0.015,\n              \"bagging_freq\": 1,\n              \"bagging_fraction\" : 0.25,\n              'tree_learner': 'serial',\n              'reg_lambda': 2,\n              'reg_alpha': 1,              \n              \"feature_fraction\": 0.15,\n              'num_leaves': 16,\n              'max_depth': 8,\n              'random_state': 16111997,\n            }\n\nseeds = np.random.randint(0, 10000, 3)\npreds = 0    \nfeature_important = None\navg_train_gini = 0\navg_val_gini = 0\n\nfor s in seeds:\n    skf = StratifiedKFold(n_splits=5, random_state=s, shuffle=True)        \n    lgbm_param['random_state'] = s    \n    seed_train_gini = 0\n    seed_val_gini = 0\n    for i, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(y_label)), y_label)):                \n        X_train, X_val = train_fe.iloc[train_idx].drop([\"id\"], 1), train_fe.iloc[val_idx].drop([\"id\"], 1)                \n        y_train, y_val = y_label[train_idx], y_label[val_idx]\n\n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_eval  = lgb.Dataset(X_val, y_val)\n\n        evals_result = {} \n        model = lgb.train(lgbm_param,\n                    lgb_train,\n                    num_boost_round=NUM_BOOST_ROUND,  \n                    early_stopping_rounds=50,\n                    feval=lgb_gini,\n                    verbose_eval=False,\n                    evals_result=evals_result,\n                    valid_sets=[lgb_train, lgb_eval])\n\n        seed_train_gini += model.best_score[\"training\"][\"gini\"] \/ skf.n_splits\n        seed_val_gini += model.best_score[\"valid_1\"][\"gini\"] \/ skf.n_splits\n\n        avg_train_gini += model.best_score[\"training\"][\"gini\"] \/ (len(seeds) * skf.n_splits)\n        avg_val_gini += model.best_score[\"valid_1\"][\"gini\"] \/ (len(seeds) * skf.n_splits)\n\n        if feature_important is None:\n            feature_important = model.feature_importance() \/ (len(seeds) * skf.n_splits)\n        else:\n            feature_important += model.feature_importance() \/ (len(seeds) * skf.n_splits)        \n\n        pred = model.predict(test_fe.drop([\"id\"], 1))\n        preds += pred \/ (skf.n_splits * len(seeds))\n        \n        print(\"Fold {}: {}\/{}\".format(i, model.best_score[\"training\"][\"gini\"], model.best_score[\"valid_1\"][\"gini\"]))\n    print(\"Seed {}: {}\/{}\".format(s, seed_train_gini, seed_val_gini))\n\nprint(\"-\" * 30)\nprint(\"Avg train gini: {}\".format(avg_train_gini))\nprint(\"Avg valid gini: {}\".format(avg_val_gini))\nprint(\"=\" * 30)","11bce499":"# Let's display importances of features\nfeatures = df_fe.columns.tolist()\nfeatures.remove(\"id\")\ndf_imp = pd.DataFrame(data = {'col' : features , 'imp' : feature_important})\ndf_imp = df_imp.sort_values(by='imp', ascending=False).reset_index(drop=True)\ndf_imp.head(50)","bfcba605":"# make submission file\ndf_test[\"label\"] = preds\ndf_test[['id', 'label']].to_csv('submission.csv', index=False)","d9b07b47":"### diaChi, maCv\nTo make the best use of these 2 fields, we should fit them into a much smaller range of categories. Temporarily, we just handle `maCv` and leave `diaChi` as a ordinary category type","f07d6dae":"### other fields\n\nJust clean, map some noisy data (`Field_38`, `Field_13`, `Field_27`, `Field_28`), normalize ordinal data (`Field_62`, `Field_47`)","0926ae6f":"## Modelling\nLet's fillout missing values and prepair training files","f0f2ae3b":"### Date \/ Datetime, xxx_startDate, xxx_endDate\n\nWe can see `Field_34` and `ngaySinh` need to be corrected. With date & datetime kind of data, we can extract some new features like whether it is on weekends, which period of time in a day, how far from the present. From information extracted from `ngaySinh`, we can infer the age of the record owner and then bin them.\n\nFor the sake of simplicity, in this version we just do the normalization \/ cleaning process and not make any new features. ","81911cbd":"### gioiTinh, info_social_sex\nCombine two columns to reduce missing cases","9eaf2565":"LightGBM is chosen because it can handle small\/imbalanced\/ dataset and categorical features very well.\nTraining different models with different seeds and average the results sometimes helps improve the performance.","c7435610":"### xxxLocationId, xxxCountry, xxxState, xxxLongtitute, xxxLatitude\n\nThere are 0 values in these fields which seem to be irrelevant and kind of noisy, we should eliminate them.","d18caae8":"## Mannualy inspect each field and group them for further processing\nWe divide features into different groups:\n1. Date \/ Datetime, xxx_startDate, xxx_endDate\n2. xxxLocationId, xxxCountry, xxxState, xxxLongtitute, xxxLatitude\n3. diaChi, maCv\n4. gioiTinh, info_social_sex\n5. to be dropped (not used in training): fields that have too little meaning\/contribution (e.g. too many missing; just 1 value across all cells; duplicates)\n6. other"}}