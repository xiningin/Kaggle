{"cell_type":{"b11026be":"code","b3ac5198":"code","32124446":"code","7b2aebf3":"code","0c60852b":"code","7d5b511b":"code","3fda902c":"code","43a958c3":"code","2b370f05":"code","ee1fd3a1":"code","90350336":"code","b94bf503":"code","163c6149":"code","9fdc52dc":"code","9ad1fed9":"code","6d9315d9":"code","20f17e41":"code","29cf37bf":"code","8b551c77":"code","17c7f0fa":"code","6e7754d9":"code","d57a7c78":"code","49c73b92":"code","3722ffb6":"code","f46b5c38":"code","c2327b46":"code","663c4d3d":"code","2a926b75":"markdown","346c2d7a":"markdown","06bc2cba":"markdown"},"source":{"b11026be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b3ac5198":"data = pd.read_csv('\/kaggle\/input\/credit-card-customers\/BankChurners.csv')","32124446":"# Lets delete those strange variables\ndata = data.drop([\"CLIENTNUM\", \n              \"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1\", \n              \"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2\"\n             ], axis=1)\n\ndata.head()","7b2aebf3":"# Encode binary variables\ndata['Gender'] = data['Gender'].map({'M':1, 'F':0})","0c60852b":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndata['class'] = le.fit_transform(data['Attrition_Flag'])\ndata = data.drop('Attrition_Flag', axis=1)","7d5b511b":"categorical_columns = data.select_dtypes(exclude=['int64','float64']).columns\nnumerical_columns = data.drop('class', axis=1).select_dtypes(include=['int64','float64']).columns\ncategorical_columns","3fda902c":"# One hot encoding independent variable x\ndef encode_and_bind(original_dataframe, feature_to_encode):\n    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n    res = pd.concat([original_dataframe, dummies], axis=1)\n    res = res.drop([feature_to_encode], axis=1)\n    return(res) ","43a958c3":"for feature in categorical_columns:\n    data = encode_and_bind(data, feature)\n\ndata.head()","2b370f05":"# Generate x and y sets\nx = data.drop('class', axis=1).values\ny = data['class']","ee1fd3a1":"# Splitting the dataset into training set and test set\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, data['class'], test_size = 0.2, random_state=1234)","90350336":"!pip install h2o\n\nimport h2o\nfrom h2o.automl import H2OAutoML\n\nh2o.init()","b94bf503":"from imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\nover = BorderlineSMOTE(sampling_strategy=0.3)\nunder = RandomUnderSampler(sampling_strategy=0.6)\n\nsteps = [('o', over), ('u', under)]","163c6149":"from imblearn.pipeline import Pipeline\n\npipeline = Pipeline(steps=steps)\n\n# transform the dataset\nx_sm_us, y_sm_us = pipeline.fit_resample(x_train, y_train)","9fdc52dc":"from sklearn.tree import DecisionTreeClassifier\n\ntree_clf = DecisionTreeClassifier(random_state=1234)\ntree_clf.fit(x_sm_us, y_sm_us)","9ad1fed9":"# Names of the independent variables\nfeature_names = list(data.drop('class', axis=1).columns)","6d9315d9":"features_to_plot = 25\n\nimportances = tree_clf.feature_importances_\nindices = np.argsort(importances)\n\nbest_vars = np.array(feature_names)[indices][-features_to_plot:]\nvalues = importances[indices][-features_to_plot:]\nbest_vars","20f17e41":"sm_us_x = np.concatenate((x_sm_us, x_test))\nsm_us_y = np.concatenate((y_sm_us, y_test))","29cf37bf":"sm_us_df = pd.DataFrame(np.column_stack([sm_us_y, sm_us_x]), columns=['class'] + feature_names)\nsm_us_df.head()","8b551c77":"hf = h2o.H2OFrame(sm_us_df[['class'] + list(best_vars)])\nhf.head()","17c7f0fa":"hf['class'] = hf['class'].asfactor()\npredictors = hf.drop('class').columns\nresponse = 'class'","6e7754d9":"# Split into train and test\ntrain, valid = hf.split_frame(ratios=[.8], seed=1234)","d57a7c78":"# Add a Stopping Creterias: max number of models and max time\n# We are going to exclude DeepLearning algorithms because they are too slow\naml = H2OAutoML(\n    max_models=20,\n    max_runtime_secs=300,\n    seed=1234,\n    exclude_algos = [\"DeepLearning\"]\n)","49c73b92":"# Train the model\naml.train(x=predictors,\n        y=response,\n        training_frame=train,\n        validation_frame=valid\n)","3722ffb6":"# View the AutoML Leaderboard\nlb = aml.leaderboard\nlb.head(rows=5)  # Print the first 5 rows","f46b5c38":"print('The model performance in Accuracy: {}'.format(aml.leader.accuracy(valid=True)))\nprint('The model performance in AUC: {}'.format(aml.leader.auc(valid=True)))","c2327b46":"# Get third model\nm = h2o.get_model(lb[2,\"model_id\"])","663c4d3d":"m.varimp(use_pandas=True)","2a926b75":"Feature Engineering","346c2d7a":"Modelling","06bc2cba":"Import Data"}}