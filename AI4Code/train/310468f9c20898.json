{"cell_type":{"0d9febad":"code","ce5cb389":"code","42f4b920":"code","31d5a034":"code","8123b98b":"code","993483aa":"code","56756424":"code","8381c2fd":"code","74a4218a":"code","04bdd449":"code","caad9499":"code","fb49e794":"code","f011a000":"code","efb8a7ef":"code","587d6660":"code","61081691":"code","a3570aaf":"code","fa629804":"code","1a64ab69":"code","7591cf5c":"code","d37fac13":"code","7cec6859":"code","63d68045":"code","9583a5c1":"code","2d22d56a":"code","417d1a30":"code","c480bb2c":"code","541616ce":"code","5d9d36bc":"code","1372c3dd":"code","38e3d785":"code","37a1492e":"code","a8721523":"code","9dcf2a9c":"code","4b4167b1":"code","e5175bc5":"code","9e052a4e":"markdown","96ebf711":"markdown","e0639b4d":"markdown","e62017b2":"markdown","a27a71e1":"markdown","120050f3":"markdown","9fd9944f":"markdown","28db5b3f":"markdown","f31082eb":"markdown","61430d6c":"markdown"},"source":{"0d9febad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ce5cb389":"import pandas as pd\nimport numpy as  np\nimport matplotlib.pyplot as  plt\nimport seaborn as sns\npd.set_option('display.max_columns',None)\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n","42f4b920":"df= pd.read_csv(r'\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv',header=0)\ndf.head()","31d5a034":"# understanding the data \nprint(df.shape)\nprint(df.dtypes)\n\nprint(df.info())\n","8123b98b":"df.columns","993483aa":"df.describe()","56756424":"#Understanding the categorical variables\na=['sex','cp','fbs','restecg','exng','slp','caa','thall']\nfor i in a:\n    print(df[i].value_counts())\n    sns.countplot(df[i])\n    plt.title([i])\n    plt.show()\n","8381c2fd":"# as per the data set  description the variable 'Thal  has to have  only  3  values 1,2 & 3 .  the  value  0  is  given  are  null- values.\ndf.thall=df.thall.replace(0,np.nan)\n\nprint(df.thall.mode())","74a4218a":"df.thall.fillna(df.thall.mode()[0],inplace=True)\nprint(df.thall.value_counts())\n","04bdd449":"# checking for  other  mising values \ndf.isnull().sum().sort_values(ascending=False)","caad9499":"# Checking for correlation between variables \nX=df[[\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]] \ncorr_df=X.corr(method=\"pearson\")\nprint(corr_df)\n\nsns.heatmap(corr_df,vmax=1.0,vmin=-1.0,annot=True)\nplt.savefig(\"heatmap.jpg\")","fb49e794":"# Relationship between Sex, age and  output \nsns.barplot(x='sex',y='age',data=df,hue='output',palette='rocket')\nplt.show()","f011a000":"# Different chestpains  and coletrol and  output\nsns.barplot(x='cp',y='chol',data=df,hue='output')\nplt.show()","efb8a7ef":"# check biasness in data \nsns.countplot(df.output)\nplt.show()","587d6660":"\nsns.pairplot(x_vars=[\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"],y_vars='age',data=df,hue='output')\nplt.show()\n","61081691":"X=df.values[:,0:-1]\nY=df.values[:,-1]\nY=Y.astype(int)","a3570aaf":"print(X.shape)\nprint(Y.shape)","fa629804":"# Scaling the data\nfrom sklearn.preprocessing  import StandardScaler\nscaler=StandardScaler()\nscaler.fit(X)\nX=scaler.transform(X)\nprint(X)","1a64ab69":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=10)","7591cf5c":"from sklearn.linear_model import LogisticRegression\nlm=LogisticRegression()\nlm.fit(X_train,Y_train)\nY_predict=lm.predict(X_test)","d37fac13":"print(list(zip(Y_test,Y_predict)))","7cec6859":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\ncfm=confusion_matrix(Y_test,Y_predict) \nprint(cfm)\n\nprint(\"Classification report: \")\n\nprint(classification_report(Y_test,Y_predict))\n\nacc=accuracy_score(Y_test, Y_predict)\nprint(\"Accuracy of the model: \",acc)","63d68045":"y_pred_prob = lm.predict_proba(X_test)\nprint(y_pred_prob)","9583a5c1":"for i in np.arange(0.4,0.61,0.01):\n    predict_new=np.where(y_pred_prob[:,1]>i,1,0)\n    cfm=confusion_matrix(Y_test,predict_new)\n    total_err=cfm[0,1]+cfm[1,0]\n    print('errors at threshold:',i,':',total_err,'Type_1:',cfm[0,1],'type2_error:',cfm[1,0])","2d22d56a":"y_pred_class=[]\nfor i in y_pred_prob[:,1]:\n    if i>0.40:\n        y_pred_class.append(1)\n    else:\n        y_pred_class.append(0)\nprint(y_pred_class)","417d1a30":"from sklearn.metrics  import confusion_matrix, accuracy_score,classification_report\naccuracy_score= accuracy_score(Y_test,y_pred_class)\nprint('accuracy:',accuracy_score)\ncfm=confusion_matrix(Y_test,y_pred_class)\nprint('confusion_matrix',cfm)\nprint(classification_report(Y_test,y_pred_class))","c480bb2c":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier","541616ce":"estimators=[]\nsvc_model=SVC(kernel='rbf',C=30.0,gamma=0.01)\nestimators.append(('SVM',svc_model))\nmodel_RF=RandomForestClassifier(n_estimators=35,random_state=10,max_depth=10,min_samples_leaf=5,min_samples_split=6)\nestimators.append(('RF',model_RF))\nmodel_LR=LogisticRegression()\nestimators.append(('LR',model_LR))\nmodel_knn=model_knn=KNeighborsClassifier(n_neighbors=int(np.sqrt(len(X_train))),metric='manhattan')\nestimators.append(('KNN',model_knn))","5d9d36bc":"ensemble=VotingClassifier(estimators)\nensemble.fit(X_train,Y_train)\nY_pred=ensemble.predict(X_test)","1372c3dd":"print(list(zip(Y_test,Y_pred)))","38e3d785":"from sklearn.metrics  import confusion_matrix, accuracy_score,classification_report\naccuracy_score= accuracy_score(Y_test,Y_pred)\nprint('accuracy:',accuracy_score)\ncfm=confusion_matrix(Y_test,Y_pred)\nprint('confusion_matrix',cfm)\nprint(classification_report(Y_test,Y_pred))","37a1492e":"estimators=[]\nsvc_model=SVC(kernel='rbf',C=30.0,gamma=0.01)\nestimators.append(('SVM',svc_model))\nmodel_RF=RandomForestClassifier(n_estimators=35,random_state=10,max_depth=10,min_samples_leaf=5,min_samples_split=6)\nestimators.append(('RF',model_RF))\n","a8721523":"ensemble=VotingClassifier(estimators)\nensemble.fit(X_train,Y_train)\nY_pred=ensemble.predict(X_test)","9dcf2a9c":"print(list(zip(Y_test,Y_pred)))","4b4167b1":"from sklearn.metrics  import confusion_matrix, accuracy_score,classification_report\naccuracy_score= accuracy_score(Y_test,Y_pred)\nprint('accuracy:',accuracy_score)\ncfm=confusion_matrix(Y_test,Y_pred)\nprint('confusion_matrix',cfm)\nprint(classification_report(Y_test,Y_pred))","e5175bc5":"# Using Ensemble  model  with SVM  and  Random forest Classifiers\nsns.heatmap(cfm,annot=True)\nplt.show()","9e052a4e":"# Preprocessing steps","96ebf711":"# Splitting the data","e0639b4d":"# Handling Missing  Values","e62017b2":"#the accuracy has  slightly increased  to  ","a27a71e1":"# Logistic Regression","120050f3":"# Splitting the Variables","9fd9944f":"#As its a sensitive data set and we want  to keep the type 2 error  minimal, we choose  0.4  ","28db5b3f":"#Insight- Data is  not biased ","f31082eb":"# Tuning Logistic Regression ","61430d6c":"# USING THE  MODELS  GIVING  BETTER RECALL VALUE  FOR  CLASS1 "}}