{"cell_type":{"f9c401e8":"code","f3f7bded":"code","b801281b":"code","8e6bd703":"code","ab9d2c05":"code","5298a29a":"code","77a92c2e":"code","f34af377":"code","d535d02d":"code","556e8304":"code","ab0f9373":"code","61101f59":"code","73514f56":"code","bb07b91e":"code","4055a727":"code","e110fe5a":"code","feee4ba8":"code","bf26c59d":"code","3de698d7":"code","c21bbeaf":"code","5a8513e3":"code","3fc457cd":"code","06f7af66":"code","1067a89c":"code","7dc6ef39":"code","96b296f9":"code","5f6b7cc8":"code","39b46582":"code","d9e1fcab":"code","5bf7383a":"code","203de7c7":"code","1cffed5a":"code","607bae6b":"code","73f413ae":"code","f6f2fbed":"code","310e8851":"code","eb60969f":"code","aba2fd2b":"code","228562a7":"code","83703178":"code","d7a30743":"code","9e5fa748":"code","58da7dec":"code","c0e5221b":"code","9804edd6":"code","05905c6c":"code","4257a113":"code","36454af3":"code","f9a048fc":"code","3789cc09":"code","945c9611":"code","65e7f6a5":"markdown","55ca6689":"markdown","15923e96":"markdown","c6a53a42":"markdown","34e7af34":"markdown","66355dcb":"markdown","4f6651c3":"markdown","c32a7f61":"markdown","324d5a1c":"markdown","2dd3b0d7":"markdown","db4dd469":"markdown","fd4d0f65":"markdown","b08fcbd7":"markdown","4539ee0e":"markdown","dc159957":"markdown","c30e3cd3":"markdown","3383fc52":"markdown","3dd5c468":"markdown","ee0cc5fe":"markdown","79b91b19":"markdown","c5641cc0":"markdown","89f5a95d":"markdown","f382eb0f":"markdown","0bea1a43":"markdown","6aa1ab1d":"markdown","bc999089":"markdown","ef0f3d53":"markdown","62edee56":"markdown","75afc99e":"markdown","2152762a":"markdown","9bb93d71":"markdown","3de11151":"markdown","c03f998a":"markdown","c75105c3":"markdown","802be53d":"markdown","0a90dffe":"markdown","e90b86f9":"markdown","03718414":"markdown","2b82a6fc":"markdown","2c0d62c8":"markdown","5d3013bb":"markdown","39ac8ea3":"markdown","e8ca42a0":"markdown","e2c594b7":"markdown","f0fd5dd6":"markdown","acbb6fa5":"markdown","60d0eaf1":"markdown","16936f07":"markdown","83978f6c":"markdown","41eba046":"markdown","adb61a6c":"markdown","9a54528d":"markdown"},"source":{"f9c401e8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here are several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f3f7bded":"train=pd.read_csv(\"..\/input\/hackerearth-ml-challenge-pet-adoption\/train.csv\")\ntest=pd.read_csv(\"..\/input\/hackerearth-ml-challenge-pet-adoption\/test.csv\")\ntrain.head()","b801281b":"test.head()","8e6bd703":"train.shape,test.shape","ab9d2c05":"train.info()","5298a29a":"test.info()","77a92c2e":"train['breed_category'].value_counts()","f34af377":"#cheak\na=train['breed_category'][(np.isnan(train['condition']))]\na.value_counts()","d535d02d":"#copy all test id to create submission file\ntest_id=test['pet_id']\n#save the train left...it will use when the combine data will split into the previous train and test data after doing feature engineering\nntrain=train.shape[0]","556e8304":"#save target variable i.e label\ny1=train['breed_category']\ny2=train['pet_category']","ab0f9373":"#combine test and train data\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['breed_category','pet_category'], axis=1, inplace=True)","61101f59":"all_data['condition'].value_counts()","73514f56":"all_data['condition'].fillna(-1,inplace=True)","bb07b91e":"all_data['condition'].value_counts()","4055a727":"all_data.info()","e110fe5a":"all_data['issue_date']=pd.to_datetime(all_data['issue_date'])\nall_data['listing_date']=pd.to_datetime(all_data['listing_date'])\n","feee4ba8":"x=[]\nfor d in all_data['issue_date']:\n    y=d.month\n    x.append(y)\nall_data['issue_month']=x","bf26c59d":"x=[]\nfor d in all_data['listing_date']:\n    y=d.month\n    x.append(y)\nall_data['listing_month']=x","3de698d7":"x=[]\nfor d in all_data['listing_date']:\n    y=d.year+(d.month\/12.0)+(d.day\/365.0)\n    x.append(y)\nall_data['modified_listing_date']=x","c21bbeaf":"x=[]\nfor d in all_data['issue_date']:\n    y=d.year+(d.month\/12.0)+(d.day\/365.0)\n    x.append(y)\nall_data['modified_issue_date']=x","5a8513e3":"all_data['took_time']=abs(all_data['modified_listing_date']-all_data['modified_issue_date'])","3fc457cd":"\nall_data['1stnum'] = all_data['pet_id'].str[:6]\nall_data['1st2num'] = all_data['pet_id'].str[:7]","06f7af66":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","1067a89c":"#drop some unnecessary features\nx=train.drop(['pet_id','issue_date','listing_date','modified_issue_date'],axis=1)\ntest=test.drop(['pet_id','issue_date','listing_date','modified_issue_date'],axis=1)\n","7dc6ef39":"x.select_dtypes(exclude='number').columns.to_list()","96b296f9":"x.shape","5f6b7cc8":"x=pd.get_dummies(x)\ntest=pd.get_dummies(test)\n","39b46582":"x.shape,test.shape","d9e1fcab":"a=set(x.columns)-set(test.columns)","5bf7383a":"a=list(a)\na","203de7c7":"x=x.drop(a,axis=1)","1cffed5a":"x.shape,test.shape","607bae6b":"#again combining\nall_data = pd.concat((x, test)).reset_index(drop=True)","73f413ae":"from sklearn import preprocessing\n# Get column names first\nnames = all_data.columns\n# Create the Scaler object\nscaler = preprocessing.StandardScaler()\n# Fit your data on the scaler object\nscaled_df = scaler.fit_transform(all_data)\nall_data = pd.DataFrame(scaled_df, columns=names)","f6f2fbed":"x = all_data[:ntrain]\ntest = all_data[ntrain:]","310e8851":"from sklearn.model_selection import train_test_split\nx1_train,x1_test,y1_train,y1_test=train_test_split(x,y2,test_size=0.2,random_state=44,shuffle=True)","eb60969f":"from xgboost import XGBClassifier","aba2fd2b":"model1 = XGBClassifier()\nmodel1.fit(x1_train, y1_train)","228562a7":"#new_feat is new feature i.e the predicted pet_category of model 1 for train data\nnew_feat=model1.predict(x)\n#output1 is new first output i.e the predicted pet_category of model 1 for test data\noutput1=model1.predict(test)\n#vld1 is validation 1 i.e we'll check score with the predicted result of validation data of model 1\nvld1=model1.predict(x1_test)","83703178":"x2 = pd.DataFrame(x, columns=names)\ntest2 = pd.DataFrame(test, columns=names)","d7a30743":"#the predicted pet_category of model 1 for train data is used as a input variable or feature of the train data of model 2\nx2['output1']=new_feat\n#the predicted pet_category of model 1 for test data is used as a input variable or feature of the test data of model 2\ntest2['output1']=output1","9e5fa748":"x2_train,x2_test,y2_train,y2_test=train_test_split(x2,y1,test_size=0.2,random_state=44)","58da7dec":"model2 = XGBClassifier()\nmodel2.fit(x2_train, y2_train)","c0e5221b":"#output 2 is the predicted breed_category of model 2 for test data\noutput2=model2.predict(test)\n#vld2 is validation 2 i.e we'll check score with the predicted result of validation data of model 2\nvld2=model2.predict(x2_test)","9804edd6":"from sklearn.metrics  import f1_score","05905c6c":"s1=f1_score(y1_test,vld1,average='weighted')\ns2=f1_score(y2_test,vld2,average='weighted')\naccuracy=100*((s1+s2)\/2)\naccuracy","4257a113":"sub_new=pd.DataFrame({\n    \"pet_id\":test_id,\n    \"breed_category\":output2,\n    \"pet_category\":output1\n})\nsub_new.to_csv(\"sub_new.csv\",index=False)","36454af3":"y1.value_counts()","f9a048fc":"y2.value_counts()","3789cc09":"from scipy.stats import skew","945c9611":"y1.skew(axis = 0, skipna = True),y2.skew(axis = 0, skipna = True)","65e7f6a5":"So, there is no null values in any column of all_data.","55ca6689":"* I will not provide the hyperparameter tuning of model. After all, it is a small task for you. I will update it with that.\n* i think you should try yourself to obtain a good score.\n* I will just give you an example, but I can't promise that it will give you my final score of 91.06. To achive that, you must do hyperparameter tuning.","15923e96":"It will be a great feature as all the animals which are available in shelter must be matured. Maybe, the issue date can be considered as birth time and the listing date as the time of the animals' being matured for staying in shelter. So, the difference between two of them indicates the time for getting mature of an animal which is not same for all the animals. Therefore, it is also an important feature.  ","c6a53a42":"**Build model 2 and train the model with new data**","34e7af34":"# This is my first notebook of my life. I am just a beginner.So,if I made any mistake,please inform me. I will be glad. Please Upvote and Share this with others.","66355dcb":"Let's check skewness","4f6651c3":"ok.wait,i am going to submit it on hackerearth","c32a7f61":"#  check  missing values","324d5a1c":"* when the dataset was creating, it might be happened that it was listing with special id for a particular animal.\n* just like ANSL_69903.so,all ANSL_6**** i.e., the id starts with 6 maybe a particular animal because we can see that all the ids are not coming serially.\n* so extracting the 1st and both 1st and 2nd numbers maybe a good feature.","2dd3b0d7":"**Scaling using StandardScaler()**","db4dd469":"We will use the output of **MODEL 1** as an input feature of **MODEL 2** .Trust me, it will increase your score.","fd4d0f65":"Since all the pets are not found in the same season,  it is an important feature. For example,\n*  Best Birth Months for the Dog: January, February, March, August, December\n*  Best Birth Months for the Dog: March, April, July, August.\n* so both are not same.","b08fcbd7":"* To handle missing values, I used KnnImputer. But it doesn't results good score.\n* First, I filled the missing values with 3 as a unique variable. But I achived a good score by filling it with -1.","4539ee0e":"# model 1 build i.e pet_category prediction","dc159957":"# Feature Engineering","c30e3cd3":"# 91.06581 scored and 25 ranked","3383fc52":"**split data for model 2**","3dd5c468":"**time difference between issue and listing date(new feature)**","ee0cc5fe":"# Oh WOW! without any hyperparameter tuning,Hackerearth shows 90.67787","79b91b19":"* At the beginning of the competition, I ran my model with Logistic Regression, KNN, Support Vector Machine. It resulted a poor score and couldn't cross 80 score.\n* After using randomforestclassifier,it is around 88-89 score.\n* But after checking with XgboostClassifier, Adaboost, Lightgbm, CatboostClassifier, etc. tree based models, I have achieved a better score above 90. So,you should try tree-based models I think.","c5641cc0":"* It is a multi-label or multi output classification problem. You have to predict two labels: 'breed_catagory' and 'pet_catagory'.\n* Formally, multi-label classification is the approach of finding a model that maps inputs, x to binary vectors, y (assigning a value of 0 or 1 for each element (label) in y).\n* How to solve it??\n* The best solution here is to train two models.\n1. Build one classification model and predict the output.\n2. Use the predicted output of 1st model as input feature to 2nd model.\n* you can also try multi-class classification i.e., predicting both labels simultaneously, but it will give you poor score.","89f5a95d":"without any hyperparameter tuning,it shows a good score.let's create a submission file","f382eb0f":"* MD AKIL RAIHAN IFTEE.\n* COMPUTER SCIENCE AND ENGINEERING.\n* KHULNA UNIVERSITY OF ENGINEERING AND TECHENOLOGY, BANGLADESH\n* YEAR: 2ND (UNDERGRADUATE B.Sc.)","0bea1a43":"It shows, there is skewness of labels. To handle it, I used LOG Transformation(log1p of numpy). But it didn't give me better score than before","6aa1ab1d":"now it's ready to do feature engineering","bc999089":"There are a lot of ways to handle categorical variables:\n* Replacing values\n* Encoding labels\n* One-Hot encoding\n* Binary encoding\n* Backward difference encoding\n* Miscellaneous features\n* I used one-hot encoding and achived a great score","ef0f3d53":"* You can clearly see it is an imbalanced data.\n* To handle this imbalanced data, I have used SMOTE, OVERSAMPLING, UNDERSAMPLING, but they don't give me better score also.\n* So, you must select a good cross-validation where all the labels come as a sample of same amount.\n","62edee56":"**missing value fillup with -1**","75afc99e":"**Modified pet id and extract important feature**","2152762a":"You can see that both shapes are not same. Train has 97 and test has 95 columns. It means the train and test data contain 2 extra columns after one-hot endcoding. We have to remove these 2 columns from the train data.","9bb93d71":"# some tips","3de11151":"* **handle categorical variable**","c03f998a":"split back to the train and test data","c75105c3":"we found missing values only in the column of 'Condition'.","802be53d":"**seasonal feature(new feature)**","0a90dffe":"* Use the predicted output of 'pet_catagory' as a feature of model 2 i.e., predict 'breed_catagory'\n* BUT if you use 'breed_catagory' as a feature of model 2 i.e., predict 'pet_catagory' it will not give you a good score.\n* I checked both and found it","e90b86f9":"let's see,the outputs","03718414":"# A baseline model for the begineers ","2b82a6fc":"It means all the missing values belong to a single particular label. So, we can fill them with a unique value like -1.","2c0d62c8":"# Check Accuracy","5d3013bb":"* I also used PCA, TruncatedSVD for dimension reduction and feature sclection.\n* It gave me poor score.\n* you must do feature selection considering the correlation of features.","39ac8ea3":"One Hot endcode","e8ca42a0":"![image.png](attachment:image.png)","e2c594b7":"![image.png](attachment:image.png)","f0fd5dd6":"**combine train and test data** ","acbb6fa5":"# Create Submission file","60d0eaf1":"# split data for 1st model i.e., pet_category prediction","16936f07":"Finally, a free advise for the beginners(I am also a beginner after all),keep learning new things what found in this notebook and keep practicing. Try to achive better score than me and share your ideas with me...","83978f6c":"Last of all,you must do hyperparameter tuning to earned an excellent score..","41eba046":"# build 2nd model i.e predict breed_category","adb61a6c":"build new dataset for model 2","9a54528d":"* I am new to data science. This is my 2nd competetion of machine learing challenge. If I score better than this, I will keep updating it.\n* It is also my first notebook in my life. So, if I made any mistake, please feel free to correct me."}}