{"cell_type":{"15ce827a":"code","dc6486bc":"code","df2b6bfc":"code","8ef246fd":"code","8a5b582d":"code","c06b3095":"code","d018fbf1":"code","262bc7a6":"code","44491181":"code","d2d4cd22":"code","4f2e903e":"code","c205878e":"markdown","9380a08c":"markdown","ec94dce3":"markdown","d5645366":"markdown","ebc7dadd":"markdown"},"source":{"15ce827a":"import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\nfrom torchvision import datasets, transforms\nfrom torch.utils import data\nimport torch.optim as optim\nfrom torch import nn\nimport torch.nn.functional as F","dc6486bc":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","df2b6bfc":"# just downloading the data\ntrain = datasets.MNIST('mnist',transform=transforms.ToTensor(), train=True, download = True)\ntest = datasets.MNIST('mnist',transform=transforms.ToTensor(), train=False, download = True)","8ef246fd":"# and puting them into DataLoaders to iterate them by batches\ntrainloader = data.DataLoader(train, batch_size = 128, shuffle=True)\ntestloader = data.DataLoader(test, batch_size = 128, shuffle=False)","8a5b582d":"class Autoencoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.encode = nn.Sequential(\n            # \u043c\u044b \u0445\u043e\u0442\u0438\u043c \u043f\u0435\u0440\u0435\u0432\u0435\u0441\u0442\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \u0432 \u043a\u0430\u043a\u043e\u0435-\u043d\u0438\u0431\u0443\u0434\u044c X-\u043c\u0435\u0440\u043d\u043e\u0435 \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u043e\n            nn.Linear(784, 300),\n            nn.ReLU(),\n            nn.Linear(300, 50),\n            nn.ReLU(),\n            nn.Linear(50,20),\n            nn.ReLU()\n            \n        )\n        \n        self.decode = nn.Sequential(\n            # \u0430 \u0442\u0435\u043f\u0435\u0440\u044c \u043d\u0430\u043e\u0431\u043e\u0440\u043e\u0442 - \u0438\u0437 \u0425-\u043c\u0435\u0440\u043d\u043e\u0433\u043e \u0432 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443\n            nn.Linear(20, 50),\n            nn.ReLU(),\n            nn.Linear(50, 300),\n            nn.ReLU(),\n            nn.Linear(300,784),\n            \n            nn.Sigmoid()\n            # \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 -- \u044d\u0442\u043e \u0442\u0435\u043d\u0437\u043e\u0440\u044b \u0441\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u043e\u0442 0 \u0434\u043e 1\n            # \u043d\u0435\u0442 \u043e\u0441\u043e\u0431\u043e\u0433\u043e \u0441\u043c\u044b\u0441\u043b\u0430 \u0432\u044b\u0432\u043e\u0434\u0438\u0442\u044c \u0447\u0442\u043e-\u0442\u043e \u043d\u0435 \u0438\u0437 \u044d\u0442\u043e\u0433\u043e \u043f\u0440\u043e\u043c\u0435\u0436\u0443\u0442\u043a\u0430\n        )\n    \n    def forward(self, x):\n        return self.decode(self.encode(x))\n\nmodel = Autoencoder().cuda()\ncriterion = torch.nn.MSELoss()\n#                    ^ \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0439\u0442\u0435 \u0442\u0430\u043a\u0436\u0435 \u0434\u0440\u0443\u0433\u0438\u0435 \u043c\u0435\u0440\u044b \u0440\u0430\u0437\u043d\u043e\u0441\u0442\u0438 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0430\u0431\u0441\u043e\u043b\u044e\u0442\u043d\u0443\u044e \u043e\u0448\u0438\u0431\u043a\u0443)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)","c06b3095":"# the process of learning\nfor epoch in range(10):\n    train_loss = 0\n    print('epoch:',epoch)\n#   V V V  we don't use targets V V V\n    for X, _ in trainloader:\n        #     ^ \u043b\u044d\u0439\u0431\u043b\u044b \u043d\u0430\u043c \u043d\u0435 \u043d\u0443\u0436\u043d\u044b\n        X = X.cuda().view(-1, 784)\n        \n        optimizer.zero_grad()\n        \n        reconstructed = model(X)\n        loss = criterion(X, reconstructed)\n        \n        loss.backward()\n\n        train_loss += loss.item()\n        optimizer.step()\n        \n\n    print('epoch %d, loss %.4f' % (epoch, train_loss \/ len(trainloader)))","d018fbf1":"from matplotlib import animation\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML, display","262bc7a6":"def get(x):\n    return trainloader.dataset[x][0].view(1, 784)\n\ndef imshow(img):\n    pic = img.numpy().astype('float')\n    plt.axis('off')\n    return plt.imshow(pic, cmap='Greys', animated=True)\n\ndef morph(inputs, steps, delay):\n    # \u043f\u0435\u0440\u0435\u0433\u043e\u043d\u044f\u0435\u043c \u0432 \u043b\u0430\u0442\u0435\u043d\u0442\u043d\u043e\u0435 \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u043e \u0432\u0441\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u043d\u0430 \u0432\u0445\u043e\u0434\u0435\n    latent = [model.encode(get(k)).data for k in inputs]\n    fig = plt.figure()\n    images = []\n    for a, b in zip(latent, latent[1:] + [latent[0]]):\n        for t in np.linspace(0, 1, steps):\n            # \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043f\u0440\u043e\u0438\u043d\u0442\u0435\u0440\u043f\u043e\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u0443\u044e \u0442\u043e\u0447\u043a\u0443\n            c = a*(1-t)+b*t\n            # ...\u0438 \u0434\u0435\u043a\u043e\u0434\u0438\u0440\u0443\u0435\u043c \u0435\u0451 \u0432 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435\n            morphed = model.decode(c).data\n            morphed = morphed.view(28, 28)\n            images.append([imshow(morphed)])\n    \n    ani = animation.ArtistAnimation(fig, images, interval=delay)\n\n    display(HTML(ani.to_html5_video()))","44491181":"model = model.cpu()","d2d4cd22":"# I don't really know why the second image shows below\n\n# we just put the list of indexes of desired elements to show in gif\nmorph([1,47], 20, 30)","4f2e903e":"# here I choose 4 random elements from train dataset to visualize them\nmorph(np.random.randint(0, len(train.data), 4), 20, 30)","c205878e":"Some functions to make animations","9380a08c":"Let's move back our model to cpu","ec94dce3":"I set the device to be 'cuda' in order to do calculations on GPU. It's way more faster","d5645366":"That's so-called autoencoder. We don't need target vector while learning it","ebc7dadd":"**Thanks for your attention. Please leave comments!**"}}