{"cell_type":{"f48d043b":"code","e3415942":"code","a1df138d":"code","bdd92c1b":"code","55d94060":"code","84e79b3b":"code","cf78cebe":"code","c0e295b2":"code","3defab6d":"code","f357b118":"code","50ea13b5":"code","e2b51aa6":"code","055c1a53":"code","beefe682":"code","87e37ba0":"code","0d025dbf":"code","e98de5a2":"code","1d092c7f":"code","d35c20c0":"code","87579ff4":"code","d0f2969a":"code","3ebe1df5":"code","72a879a8":"code","ec81e7f2":"code","6cada647":"code","3f92d166":"code","3a5c4ac3":"code","9bb23be6":"code","843fc0b8":"markdown","8e3ecbce":"markdown","c85d620a":"markdown","d44de56b":"markdown","cbd2091d":"markdown","e5e61825":"markdown","e0bf4fc1":"markdown","1db0637c":"markdown"},"source":{"f48d043b":"import numpy as np \nimport pandas as pd\nimport os\nimport seaborn as sns\nprint(os.listdir(\"..\/input\"))\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nfrom tqdm import tqdm\nimport re\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom nltk import word_tokenize\nimport gensim\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport matplotlib.colors as mcolors\nfrom sklearn.manifold import TSNE\nfrom gensim.models import word2vec\nimport nltk\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\nfrom keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.preprocessing import text, sequence\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\nimport json","e3415942":"df = pd.read_csv('..\/input\/amazon-fine-food-reviews\/Reviews.csv')\n\nprint(df.shape)\ndf.head()","a1df138d":"df.shape","bdd92c1b":"df.dtypes","55d94060":"df.info()","84e79b3b":"df.describe()","cf78cebe":"##Removing the Duplicates if any\ndf.duplicated().sum()\ndf.drop_duplicates(inplace=True)","c0e295b2":"#Check for the null values in each column\ndf.isnull().sum()","3defab6d":"##Remove the NaN values from the dataset\ndf.isnull().sum()\ndf.dropna(how='any',inplace=True)","f357b118":"df.head()","50ea13b5":"import seaborn as sns\nsns.countplot(df['Score'], palette=\"plasma\")\nfig = plt.gcf()\nfig.set_size_inches(10,10)\nplt.title('Score')","e2b51aa6":"rating_df = pd.DataFrame(df, columns=['Score', 'Text'])\n\nprint(rating_df.shape)\nrating_df.head()","055c1a53":"rating_df['Score'].astype('category').value_counts()","beefe682":"dummies = pd.get_dummies(rating_df['Score'])\ndummies.head()","87e37ba0":"## Lower Casing\nrating_df['Text'] = rating_df['Text'].str.lower()\nrating_df.head()","0d025dbf":"import string\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\nrating_df['Text'] = rating_df['Text'].apply(lambda text: remove_punctuation(text))\nrating_df.head()","e98de5a2":"## Removal of Stopwords\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\nrating_df['Text'] = rating_df['Text'].apply(lambda text: remove_stopwords(text))\nrating_df.head()","1d092c7f":"## Removal of urls\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\nrating_df['Text'] = rating_df['Text'].apply(lambda text: remove_urls(text))\nrating_df.head()","d35c20c0":"x_train, x_test, y_train, y_test = train_test_split(\n    rating_df['Text'], \n    dummies, \n    test_size=0.1, random_state=19\n)","87579ff4":"def build_matrix(word_index, path):\n    def get_coefs(word, *arr):\n        return word, np.asarray(arr, dtype='float32')\n\n    def load_embeddings(path):\n        with open(path) as f:\n            embedding_index = {}\n            \n            for line in tqdm(f):\n                word, arr = get_coefs(*line.strip().split(' '))    \n                if word in word_index:\n                    embedding_index[word] = arr\n            \n        return embedding_index\n\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    \n    for word, i in tqdm(word_index.items()):\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix","d0f2969a":"def build_model(embedding_matrix):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.3)(x)\n    x = Bidirectional(LSTM(256, return_sequences=True))(x)\n \n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    hidden = Dense(512, activation='relu')(hidden)\n    \n    result = Dense(5, activation='softmax')(hidden)\n    \n    model = Model(inputs=words, outputs=result)\n    model.compile(\n        loss='categorical_crossentropy', \n        optimizer='adam',\n        metrics=['accuracy']\n    )\n\n    return model","3ebe1df5":"%%time\nCHARS_TO_REMOVE = '!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n\u201c\u201d\u2019\\'\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014'\ntokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\ntokenizer.fit_on_texts(list(x_train) + list(x_test))","72a879a8":"embedding_matrix = build_matrix(tokenizer.word_index, '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec')","ec81e7f2":"maxlen=512\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\nx_train = sequence.pad_sequences(x_train, maxlen=512)\nx_test = sequence.pad_sequences(x_test, maxlen=512)","6cada647":"model = build_model(embedding_matrix)\nmodel.summary()\n\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n    monitor='val_acc', \n    verbose=1, \n    save_best_only=True, \n    save_weights_only=False,\n    mode='auto'\n)\n\nhistory = model.fit(\n    x_train,\n    y_train,\n    batch_size=512,\n    callbacks=[checkpoint],\n    epochs=10,\n    validation_split=0.1\n)","3f92d166":"print(history)","3a5c4ac3":"# Plot training & validation accuracy values\nplt.plot(history.history['val_accuracy'])\nplt.plot(history.history['accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.show()","9bb23be6":"# Plot training & validation loss values\nplt.plot(history.history['val_loss'])\nplt.plot(history.history['loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.show()","843fc0b8":"### Importing Libraries","8e3ecbce":"The Amazon Fine Food Reviews dataset consists of reviews of fine foods from Amazon.\n\nNumber of reviews: 568,454\n\nNumber of users: 256,059\n\nNumber of products: 74,258\n\nTimespan: Oct 1999 \u2014 Oct 2012\n\nNumber of Attributes\/Columns in data: 10\n\n#### The column or features in the dataset:\n - Id\n - ProductId \u2014 unique identifier for the product\n - UserId \u2014 unqiue identifier for the user\n - ProfileName\n - HelpfulnessNumerator \u2014 number of users who found the review helpful\n - HelpfulnessDenominator \u2014 number of users who indicated whether they found the review helpful or not\n - Score \u2014 rating between 1 and 5\n - Time \u2014 timestamp for the review\n - Summary \u2014 brief summary of the review\n - Text \u2014 text of the review","c85d620a":"## End of the Notebook","d44de56b":"### Text Preprocessing","cbd2091d":"I have used 300-dimensional pretrained FastText English word vectors released by Facebook.","e5e61825":"### References\n - [Getting started with Text Preprocessing](https:\/\/www.kaggle.com\/sudalairajkumar\/getting-started-with-text-preprocessing)\n - [Tutorial: Sentiment Analysis](https:\/\/www.kaggle.com\/rtatman\/tutorial-sentiment-analysis-in-r)\n - [A Detailed Explanation of Keras Embedding Layer](https:\/\/www.kaggle.com\/rajmehra03\/a-detailed-explanation-of-keras-embedding-layer)\n - [Amazon Fine Food Reviews Dataset](https:\/\/medium.com\/@urytrayudu1\/amazon-fine-food-reviews-dataset-a48f311a8d61)\n \n \n <h2>Please UPVOTE if you found these helpful :) <\/h2>","e0bf4fc1":"## Introduction\n\n### What is sentiment analysis?\nSentiment analysis is the computational task of automatically determining what feelings a writer is expressing in text. Sentiment is often framed as a binary distinction (positive vs. negative), but it can also be a more fine-grained, like identifying the specific emotion an author is expressing (like fear, joy or anger).\n\nSentiment analysis is used for many applications, especially in business intelligence. Some examples of applications for sentiment analysis include:\n\n - Analyzing the social media discussion around a certain topic\n - Evaluating survey responses\n - Determining whether product reviews are positive or negative\n\nSentiment analysis is not perfect, and as with any automatic analysis of language, you will have errors in your results. It also cannot tell you why a writer is feeling a certain way. However, it can be useful to quickly summarize some qualities of text, especially if you have so much text that a human reader cannot analyze all of it.\n\nFor this project,the goal is to to classify Food reviews based on customers' text.","1db0637c":"## Loading the dataset"}}