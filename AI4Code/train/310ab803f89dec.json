{"cell_type":{"eb2f2445":"code","cfcf098f":"code","c1cb0e84":"code","43e19a8b":"code","a1ff869e":"code","22146a0d":"code","dd3846c3":"code","fcdbdd5f":"code","f92a6ef8":"code","89d944c3":"code","90298421":"code","debba3da":"code","9f545ec9":"code","b4e2a205":"code","78fc4517":"code","fe071390":"code","2fb1437e":"code","2485b435":"code","a3a99717":"code","f3f92322":"code","0efaec15":"code","3e93ce4d":"code","55d3e817":"code","b06a662c":"code","d2bdc2e2":"code","96bb73a4":"code","334b2c2d":"code","a9799f9b":"code","a8a9b798":"code","f7b79566":"code","efe3df24":"code","9447b453":"code","c0df61ea":"code","c01b39eb":"code","cf2db826":"code","132070d6":"code","3befecc7":"code","9df2d487":"code","cd7d5685":"code","fd5e6d67":"code","b6a39202":"code","0b4498be":"code","bde6dbbc":"code","282d6424":"code","34ce598e":"code","5081da09":"code","eca55fb6":"code","28e6c504":"code","a773edbf":"markdown","15124eb3":"markdown","eeefbfd8":"markdown","8bd70a51":"markdown","0755fddd":"markdown","15267403":"markdown","f839934e":"markdown","7905e9ca":"markdown","c7886117":"markdown","32e3635b":"markdown","50181cfd":"markdown","783b3ebd":"markdown","83a1ca62":"markdown","63e3b9dd":"markdown","27d4eb43":"markdown","06c2d9df":"markdown","c2916942":"markdown","a99cbe6e":"markdown","667a0163":"markdown","f34267a7":"markdown","15baa6b2":"markdown","c74e2460":"markdown","568376dc":"markdown","cd3d37f4":"markdown","6d52c045":"markdown","df63be5b":"markdown","77f21c29":"markdown","f65e69da":"markdown","2ab8228e":"markdown","e6bb167f":"markdown","4ec70df9":"markdown","ad2b67a2":"markdown","2787f25e":"markdown","45abcbfb":"markdown","0f6b8546":"markdown","119239bd":"markdown","dcd48e5e":"markdown"},"source":{"eb2f2445":"import pandas as pd \nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")","cfcf098f":"df=pd.read_csv(\"https:\/\/raw.githubusercontent.com\/dphi-official\/Datasets\/master\/Heart_Disease\/Training_set_heart.csv\" )\ndf.head(5)","c1cb0e84":"df.shape","43e19a8b":"df.info()","a1ff869e":"df.describe().T","22146a0d":"df.corr()","dd3846c3":"df.corr()[\"target\"].sort_values(ascending=False)","fcdbdd5f":"df.drop('target', axis=1).corrwith(df.target).plot(kind='bar', grid=True, figsize=(12, 8), title=\"Correlation with target\")","f92a6ef8":"# Import libraries for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns","89d944c3":"fig, axes = plt.subplots(4, 3, figsize=(16,16))\nfs = ['sex','age','cp', 'trestbps','fbs','chol','restecg','exang', 'slope', 'ca','thal','thalach']\nfor i, axi in enumerate(axes.flat):\n  sns.countplot(x=fs[i], hue='target', data=df, ax=axi)\n  axi.set(ylabel='Frequency')\n  axi.legend([\"Healthy\", \"Sick\"])","90298421":"import matplotlib.pyplot as plt\nimport seaborn as sns\nax=sns.countplot(x=df[\"target\"])","debba3da":"df[\"target\"].value_counts()","9f545ec9":"y=df[\"target\"] \ny","b4e2a205":"X = df.drop(\"target\",axis=1)\nX.head(3)","78fc4517":"from sklearn.model_selection import train_test_split","fe071390":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)","2fb1437e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score","2485b435":"model = LogisticRegression()\nmodel.fit(X_train,y_train)\npreds=model.predict(X_test)\nprint(f1_score(y_test, preds))\nprint(accuracy_score(y_test,preds))","a3a99717":"preds=model.predict(X_test)\npreds","f3f92322":"# Evaluation\nprint(f1_score(y_test, preds))\nprint(accuracy_score(y_test,preds))\nprint(\"=================================================\")\n# Classification report\nprint(classification_report(y_test, preds, digits=4))","0efaec15":"from sklearn.ensemble import RandomForestClassifier","3e93ce4d":"model_rf= RandomForestClassifier()\nmodel_rf.fit(X_train,y_train)\npred_rf=model_rf.predict(X_test)\n\nprint(f1_score(y_test,pred_rf))\nprint(accuracy_score(y_test,pred_rf))","55d3e817":"from sklearn.svm import SVC\nmodel_svm=SVC()\nmodel_svm.fit(X_train,y_train)\n\npred_svm=model_svm.predict(X_test)\n\nprint(f1_score(y_test,pred_svm))\nprint(accuracy_score(y_test,pred_svm))","b06a662c":"from sklearn.model_selection import GridSearchCV","d2bdc2e2":"\n# For Random Forest\nparam_grid = { \n    'n_estimators': [200, 300,400],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'class_weight': ['balanced'],\n    'max_depth' : [8,9,10,11],\n    'criterion' :['gini', 'entropy']\n}\n\ngrid_search = GridSearchCV(model_rf, param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\npred_grid=grid_search.predict(X_test)\n\nprint(\"==========================================\")\nprint(\"Best parameters for Grid search is:\")\nprint(grid_search.best_params_)\nprint(\"==========================================\")\nprint(f1_score(y_test,pred_grid))\nprint(accuracy_score(y_test,pred_grid))\n","96bb73a4":"# For Logistic Regression\nimport numpy as np\n\nlog_grid_params={\n    \"C\":np.logspace(-3,3,7),\n    \"penalty\":[\"l1\",\"l2\"]\n} # l1 lasso l2 ridge\n\ngrid_log = GridSearchCV(model, log_grid_params, cv=10)\ngrid_log.fit(X_train, y_train)\npred_grid=grid_log.predict(X_test)\n\nprint(\"==========================================\")\nprint(\"Best parameters for Grid search is:\")\nprint(grid_log.best_params_)\nprint(\"==========================================\")\nprint(f1_score(y_test,pred_grid))\nprint(accuracy_score(y_test,pred_grid))","334b2c2d":"!pip install lightgbm\n!pip install xgboost","a9799f9b":"from lightgbm import LGBMClassifier\n\nmodel_LGBM = LGBMClassifier().fit(X_train, y_train)\npred_LGBM = model.predict(X_test)\n\nprint(f1_score(y_test,pred_LGBM))\nprint(accuracy_score(y_test,pred_LGBM))","a8a9b798":"from xgboost import XGBClassifier\n\nmodel_XGB = XGBClassifier().fit(X_train, y_train)\npred_XGB = model_XGB.predict(X_test)\nprint(f1_score(y_test,pred_XGB))\nprint(accuracy_score(y_test,pred_XGB))","f7b79566":"final_model = LogisticRegression(penalty=\"l2\",C=1.0)\nfinal_model.fit(X_train,y_train)\nfinal_preds=final_model.predict(X_test)\nprint(f1_score(y_test, final_preds))\nprint(accuracy_score(y_test,final_preds))","efe3df24":"test_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/dphi-official\/Datasets\/master\/Heart_Disease\/Testing_set_heart.csv')\ntest_data.head(10)","9447b453":"target = final_model.predict(test_data) # you must do a pre-processing on your evaluation data","c0df61ea":"target = final_model.predict(test_data)","c01b39eb":"res = pd.DataFrame(target) #preditcions are nothing but the final predictions of your model on input features of your new unseen test data\nres.index = test_data.index # its important for comparison. Here \"test_new\" is your new test dataset\nres.columns = [\"prediction\"]\nres.to_csv(\"prediction_results.csv\", index = False)      # the csv file will be saved locally on the same location where this notebook is located.","cf2db826":"!pip install SHAP","132070d6":"# Install the library if you don't have it\n# !pip install SHAP \nimport shap","3befecc7":"explainer = shap.LinearExplainer(final_model, X_train)\nshap_values = explainer.shap_values(X_test)\npd.DataFrame(shap_values, columns=X_test.columns).head(5)","9df2d487":"print('Expected Value:', explainer.expected_value)","cd7d5685":"shap.summary_plot(shap_values, X_test, plot_type='bar')","fd5e6d67":"print(\"Test data (actual observation): {}\".format(y_test.iloc[1]))\nprint(\"Model's prediction: {}\".format(preds[1]))\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[1, :], X_test.iloc[1, :])","b6a39202":"X_test.iloc[6,:]","0b4498be":"print(\"Test data (actual observation): {}\".format(y_test.iloc[5]))\nprint(\"Model's prediction: {}\".format(preds[5]))\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[5, :], X_test.iloc[5, :])","bde6dbbc":"print(\"Test data (actual observation): {}\".format(y_test.iloc[6]))\nprint(\"Model's prediction: {}\".format(preds[6]))\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[6, :], X_test.iloc[6, :])","282d6424":"shap.decision_plot(explainer.expected_value, shap_values[6,:], feature_order=\"importance\", feature_names=list(X_test.columns),\n                   link='logit', highlight=0)","34ce598e":"shap.initjs()\nshap.force_plot(explainer.expected_value, \n                shap_values[:,:], X_test.iloc[:,:])","5081da09":"shap.initjs()\nshap.summary_plot(shap_values, X_test)","eca55fb6":"shap.initjs()\nshap.dependence_plot(ind='chol', interaction_index='chol',\n                     shap_values=shap_values, \n                     features=X_test)","28e6c504":"shap.initjs()\nshap.dependence_plot(ind='chol', interaction_index='age',\nshap_values=shap_values, features=X_test)","a773edbf":"This plot seems like summary plat we presented earlier. This one is also listed from the most important to least important feature for our model. \n\n- For thalach values, when they pass a SHAP value of 4, it drives model to predict a heart disease existance and this is the most important feature for our model.\n\n- For trestbps values, when they go below 3,5 it contributes to predicting a heart disease but when it is bigger than -3,5 (0-3,5 range) it reduces the probability od heart disease existance.","15124eb3":"### Gridsearch","eeefbfd8":"## Split the data into Train and Test Sets\n\n","8bd70a51":"Best performing model is obtained after implementing gridsearch on logistic regression model.","0755fddd":"# Evaluting test data","15267403":"This is a very comprehensive graph that will help us to see which rows of our data, which patients are more likely to have a heart disease or not.\n\nAs we can see, 22 and after there, the patients are more likely to have heart disease because they exceed the base value and red region is tilted towards to the other side. \n\nWe can say that thalach, trestbps and thal are the most important features for our model as they appear in larger bands in the graph. We already confirmed this in our summary plot as well.\n\nIndividual effects of top 5 important features on the model:\n- thalach: It drives our model towards the label 1. In other words, as thalach increases, the patient is more likely to have a heart disease.\n- trestbps: It drives our model towards label 0. As it increases, patient gets less likely to have a heart disease.\n- thal: It drives our model towards label 0. As it increases, patient gets less likely to have a heart disease. Also we can see that it's range is larger when compared to trestbps.\n- cp: It drives our model towards the label 1. In other words, as it increases, the patient is more likely to have a heart disease. We see the same range issue as thal in cp as well. \n","f839934e":"The above matrix includes our \"SHAP\" values. They are basically coefficients representing the **importance of those features** in determining whether the patient has heart disease or not. Positive values direct that prediction to 1 when negatives do the opposite. If the sum of all individual coefficients exceeds **base value** which is approximately **0.33445** here, prediction appears as 1. If it cannot exceed the base value, prediction is 0. \n\n*Local explanation*: Each row has its coefficients and they show us how are the related features contribute to prediction of that specific row. \n\nRather than examining things locally, if we want to see *global* (overall) feature importances,  we could sum up and average all the columns individually. ","7905e9ca":"## Model Building","c7886117":"The dataset consists of 212 rows and 14 columns.","32e3635b":"The red features drives our prediction to be 1: patient doesn't have a heart disease. 4 most contributing features are as follows:\n- cp : 3\n- exang : 0\n- ca : 0\n\nThe blue features indicate features reducing the probability of the patient's heart disease existence. The only factor contributing here is:\n- oldpeak : 4.2\n- tretbps : 178\n- thal : 3\n- slope : 0\n- restecg : 0\n- sex : 1 (male)","50181cfd":"### Random Forest","783b3ebd":"### XGBOOST","83a1ca62":"The red features drives our prediction to be 1: patient doesn't have a heart disease. 4 most contributing features are as follows:\n- sex : 0 (female)\n- exang : 0\n- thal : 2\n- restecg : 1\n- age : 62\n\nThe blue features indicate features reducing the probability of the patient's heart disease existence. The only factor contributing here is:\n\n\n- thalach : 106\n- ca : 3\n- cp : 0\n- oldpeak : 1.9 \n- slope : 1\n- tretbps : 138","63e3b9dd":"## Load the data\nLet's load our data and take a look at the first 5 rows","27d4eb43":"Let's see if any features are correlated with our target variable.","06c2d9df":"## Final Model","c2916942":"Information related to columns can be examined with the following code. Looking at the shape of data and the number of non-null count in here, we can say that we don't have any missing values for our columns. All columns constitute of numeric values, mostly integers. ","a99cbe6e":"## Perform Basic Exploratory Data Analysis","667a0163":"## Two way PDP","f34267a7":"##  Use a SHAP Explainer to derive SHAP Values for the logistic regression model.","15baa6b2":"Now that we trained our model, we can go ahead and predict values for X_test. We will store it in preds variable to measure how well our model performs later.","c74e2460":"## Plot a SHAP summary plot using all the features in the data","568376dc":"## Plot a SHAP force plot for all the rows of the data","cd3d37f4":"### LightGBM","6d52c045":"Correlation measures the linear depencence of features. It varies from -1 to 1. The closer to 1 (or -1) the strength of our relationship increases. Negative values refer to negative correlation, that means, if we increase one of the values, the other one decreases.","df63be5b":"### Logistic Regression","77f21c29":"We are seeing a linear dependency in our plot because logistic regression is categorized as a linear model. For chol values untill 250, model is pushed towards predicting no heart disease. After this limit, model is pushed towards predicting a heart disease existance.","f65e69da":"### Support Vector Machines","2ab8228e":"The classes are not equal, we can check the value counts with the following code.","e6bb167f":"## Plot a SHAP force plot for the first row of test data.","4ec70df9":"Let's take a look at the summary plot. It shows which features affect the prediction on a global level.","ad2b67a2":"The red features drives our prediction to be 1: patient has a heart disease. 4 most contributing features are as follows:\n- thal :0\n- cp : 2\n- sex : 0 (female)\n- exang : 0\n- oldpeak : 0\n- ca : 0\n- slope : 2\n\nThe blue features indicate features reducing the probability of the patient's heart disease existence. The only factor contributing here is:\n- thalach : 115\n- restecg : 0\n","2787f25e":"## Separate the Input and Target Features of the data","45abcbfb":"Let's take a closer look at the target column.","0f6b8546":"To dig deeper and explore the columns, namely our features more, we can use describe() function. It helps us to see some descriptive statistics about our features and helps us to get an idea of their distribution.","119239bd":"# Explainable AI ","dcd48e5e":"## Plot a dependence plot to show the effect of \u2018chol\u2019 across the whole dataset."}}