{"cell_type":{"1d4ba423":"code","560b6b73":"code","b49a4490":"code","5ebd1154":"code","ced98b13":"code","abdacd36":"code","96c3aa16":"code","ee858f22":"code","8b2125d4":"code","0551b745":"code","ee5a67cb":"code","b54d0782":"code","0b068d6c":"code","46356f34":"code","55f49d0d":"code","b353b45c":"code","b60eba60":"code","17bdd4a1":"code","13e009ed":"markdown","c21e77dc":"markdown","15eddf50":"markdown","856daf3c":"markdown","37800a58":"markdown","a1a822f7":"markdown","e934e97e":"markdown","f4ea2236":"markdown","3aeef50a":"markdown","9cd77ee1":"markdown","363c65cf":"markdown","34c2f215":"markdown","e1592a03":"markdown","fb810363":"markdown","3c534645":"markdown","f053554f":"markdown","a766c5bd":"markdown","93ed8ee3":"markdown","d5ffd283":"markdown","d352deef":"markdown","7dac8eca":"markdown","41044303":"markdown","40ca4626":"markdown"},"source":{"1d4ba423":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport umap as umap\n\n# hide setting with copy warning to keep things clean\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option(\"max_columns\", 102)\n\n\nsns.set_style('whitegrid')","560b6b73":"UK_COLOUR = '#C8102E'\nEU_COLOUR = '#1E448A'\ncolor_dict = {'EU': EU_COLOUR, 'UK':UK_COLOUR}\neu_countries = ['Austria','Belgium','Bulgaria','Croatia','Cyprus','Czech Republic','Denmark','Estonia','Finland','France',\n               'Germany','Greece','Hungary','Ireland','Italy','Latvia','Lithuania','Luxembourg','Malta','Netherlands',\n               'Poland','Portugal','Romania','Slovakia','Slovenia','Spain','Sweden']\n\nsurvey_years = np.r_[2019, 2018, 2017]","b49a4490":"# some helper functions used throughout the notebook\ndef map_ages_to_intervals(x):\n    \"\"\"\n    Bins a continuous range of ages into discrete intervals.\n\n    Parameters:\n    -----------\n    x : pd.Series,\n        Series of float values of ages\n        \n    Returns:\n    --------\n    x_binned, pd.Series,\n        The binned ages.\n    \"\"\"\n\n    age_bins = pd.IntervalIndex.from_tuples([(18, 21), (22, 24), (25, 29),\n                                             (30, 34), (35, 39), (40, 44),\n                                             (45, 49), (50, 54), (55, 59),\n                                             (60,64), (65,69), (70, 100)])\n    \n    x = pd.cut(x, age_bins).astype(str)\n    x = (x.str.replace(', ', '-')\n          .str.strip('(]')\n          .str.replace('.0-','-')\n          .apply(lambda x: x[:-2]))\n    return x\n\ndef turn_off_lines(ax):\n    \"\"\"\n    Turns off lines on a matplotlib axes object.\n    \"\"\"\n    ax.spines['right'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    ax.yaxis.set_ticks_position('left')\n    ax.xaxis.set_ticks_position('bottom')\n    ax.grid(False)\n\ndef load_and_process(eu_countries, drop_non_eu=True):\n    \"\"\"\n    Loads the kaggle ML & DS survey results from 2017, 2018 and 2019.\n    It casts to the results to a DataFrame with a structure consistent \n    across the years. It only extracts a subset of the total questions\n    and only takes countries in the EU (including the UK).\n    \n    Currently, in the interest of time, it is not robust to the addition \n    of future years (i.e., 2020 onwards).\n    \n    Parameters:\n    -----------\n    eu_countries: iterable, \n        List-like object containing the names of each country (excluding the UK) in the EU\n    \n    Yields:\n    --------\n    df: \n        Returns a list of the survey results where each entry s a pandas DataFrame with the\n        results from a given year.\n    \"\"\"\n    \n    # load the data\n    prefix = '\/kaggle\/input\/kaggle-survey'\n    df_2019 = pd.read_csv(prefix + '-2019\/multiple_choice_responses.csv')\n    df_2018 = pd.read_csv(prefix + '-2018\/multipleChoiceResponses.csv')\n    df_2017 = pd.read_csv(prefix + '-2017\/multipleChoiceResponses.csv', encoding=\"ISO-8859-1\")\n    \n    # align the columns we want to create in out dataframes\n    cols_to_generate = ['country','gender','age','education',\n                        'employment_status','salary']\n    col_names_2019 = ['Q3','Q2','Q1','Q4','Q5','Q10']\n    col_names_2018 = ['Q3','Q1','Q2','Q4','Q6','Q9']\n    col_names_2017 = ['Country','GenderSelect','Age','FormalEducation',\n                      'EmploymentStatus','CompensationAmount']\n\n    for df, cols in zip([df_2019, df_2018, df_2017], \n                        [col_names_2019, col_names_2018, col_names_2017]):\n        # instantiate our new df\n        processed_df = pd.DataFrame({'body': df[cols[0]].values})\n        \n        # classify countries as EU member state or UK\n        EU_mask = processed_df['body'].isin(eu_countries)\n        UK_mask = processed_df['body'].str.contains('United Kingdom', case=False)==True\n        processed_df.loc[UK_mask,'body'] = 'UK'\n        processed_df.loc[EU_mask,'body'] = 'EU'\n\n        # create the other columns\n        for new_col, old_col in zip(cols_to_generate, cols):\n            processed_df[new_col] = df[old_col].values\n            \n        # perform some manual corrections\n        \n        # map the age column for 2017 to intervals\n        if 'Age' in cols:\n            processed_df.loc[processed_df['age']<17, 'age'] = np.nan\n            processed_df['age'] = map_ages_to_intervals(processed_df['age'])\n        \n        # group 'out' of bound ages and fill NaNs\n        high_ages = ['80+','70-100','70-79']\n        processed_df.loc[processed_df['age'].isin(high_ages), 'age'] = '70+'\n        sixties = ['60-64','65-69']\n        processed_df.loc[processed_df['age'].isin(sixties), 'age'] = '60-69'\n        processed_df.loc[processed_df['age']=='n','age'] = np.nan\n        \n        # process the salary column for the 2019 survey\n        if cols[5]=='Q10':\n\n            for i, col in enumerate(['salary_lower', 'salary_higher']):\n                # remove the row featuring the questions\n                processed_df = processed_df[~(processed_df.salary.str.contains('What is your current')==True)]\n                # format the salary columns (remove $,commas and > signs)\n                processed_df[col] = (processed_df['salary']\n                                    .str.split('-', expand=True)[i]\n                                    .str.replace('$','')\n                                    .str.replace(',','')\n                                    .str.replace('>','')\n                                    .astype(float)\n                                    )\n            \n            # add media question responses\n            media_questions = df.columns[df.columns.str.contains('Q12')]\n            processed_df[media_questions] = df[media_questions]\n            # add IDE questions\n            IDE_questions = df.columns[df.columns.str.contains('Q16')]\n            processed_df[IDE_questions] = df[IDE_questions]\n            # add programming questions\n            prog_questions = df.columns[df.columns.str.contains('Q18')]\n            processed_df[prog_questions] = df[prog_questions]\n            processed_df['job'] = df['Q5']\n        \n        # corrections for 2018 and 2019 survies\n        if 'Q1' in cols:\n             # add the survery response time\n            processed_df['completion_time'] = df['Time from Start to Finish (seconds)']\n        \n            # Add coding experience\n            experience_mapping = {'0-1':'0-1','< 1':'0-1','1-2':'1-2','2-3':'3-5',\n                                  '3-4':'3-5','4-5':'3-5','3-5':'3-5','5-10':'5-10',\n                                  '10-15':'10-20','10-20':'10-20','15-20':'10-20',\n                                  '20+':'20-+','20-25':'20-+','25-30':'20-+','30 +':'20-+',\n                                  'I have never written code': '0-1'}\n            try:\n                # 2019 survey, map it to single categories\n                processed_df['coding_experience'] = df['Q15']\n                processed_df['coding_experience'] = (processed_df['coding_experience']\n                                                     .str.replace('years','')\n                                                     .str.strip()\n                                                     .map(experience_mapping))\n            except:\n                # 2018 survey, map it to single categories\n                processed_df['coding_experience'] = df['Q8']\n                processed_df['coding_experience'] = (processed_df['coding_experience']\n                                                     .str.strip()\n                                                     .map(experience_mapping))\n        \n        if drop_non_eu:\n            # drop countries which are not EU or UK\n            processed_df = processed_df[processed_df['body'].isin(['UK','EU'])]  \n            \n        yield processed_df\n        \ndef autolabel(rects):\n    \"\"\"\n    Attach a text label above a bar in a bar chart displaying its height.\n    \n    This is taken from the matplotlib example gallery:\n    https:\/\/matplotlib.org\/examples\/api\/barchart_demo.html\n    \"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width()\/2., 1.01*height,\n                '%d' % int(height),\n                ha='center', va='bottom')\n     ","5ebd1154":"# a list of dfs for each survey [2019, 2018, 2017]\nyearly_dfs = list(load_and_process(eu_countries))","ced98b13":"# prepare data for this analysis\neu_respondents = []\nuk_respondents = []\nfor df in yearly_dfs:\n    counts = df.groupby('body')['country'].count()\n    eu_respondents.append(counts['EU'])\n    uk_respondents.append(counts['UK'])\n    \neu_respondents = np.array(eu_respondents)\nuk_respondents = np.array(uk_respondents)","abdacd36":"fig, (ax, ax2) = plt.subplots(1, 2)\n# plot bar chart and offset them along x axis to make them side by side\neu_rects = ax.bar(survey_years-0.2, eu_respondents, 0.4, color=EU_COLOUR, label='EU', alpha=0.8)\nuk_rects = ax.bar(survey_years+0.2, uk_respondents, 0.4, color=UK_COLOUR, label='UK', alpha=0.8)\n\n\n# add labels and sort out tick labels\nax.legend()\nax.set_ylabel('Number of responses')\nax.set_xlabel('Year')\nax.set_xticklabels(['','','2017','','2018','','2019'])\nax.set_ylim(0,4500)\nax.set_title('Net Participation')\n\n# add counts to the top of the bars\nautolabel(eu_rects)\nautolabel(uk_rects)\n\n\n# plot the normalised data on the second axis\n\n# population figures from wikipedia\neu_pop = np.r_[513481691,(513481691+511643456)\/2,511643456]\nuk_pop = np.r_[66647112,(66647112+65808573)\/2,65808573]\neu_pop = eu_pop - uk_pop\n\neu_rects = ax2.bar(survey_years-0.2, eu_respondents\/eu_pop * 100, 0.4, color=EU_COLOUR, label='EU', alpha=0.8)\nuk_rects = ax2.bar(survey_years+0.2, uk_respondents\/uk_pop * 100, 0.4, color=UK_COLOUR, label='UK', alpha=0.8)\n\n\n# add labels and sort out tick labels\nax2.legend()\nax2.set_ylabel('Fraction of population responding (%)')\nax2.set_xlabel('Year')\n_ = ax2.set_xticklabels(['','','2017','','2018','','2019'])\nax2.set_title('Participation normalised by population')\n\nfor ax in [ax,ax2]:\n    turn_off_lines(ax)\n\nfig.set_size_inches(13,5)","96c3aa16":"fig, axes = plt.subplots(ncols=2, sharey=True)\nmax_comp_time = 7500 # cut-off for max response time to plot\nbins = np.linspace(0,max_comp_time,50)\nfor i, ax in enumerate(axes):\n    for body in ['EU','UK']:\n        body_mask = yearly_dfs[i]['body']==body\n        time_2019 = yearly_dfs[i].loc[body_mask,'completion_time'].astype(float, errors='ignore')\n        sns.kdeplot(time_2019[time_2019<max_comp_time], color=color_dict[body],alpha=0.3, label=body, ax=ax, shade=True)\n        #ax.hist(time_2019[time_2019<max_comp_time], bins=bins, color=color_dict[body],\n                #density=True, alpha=0.4, label=body)\n        ax.set_title(survey_years[i])\n        ax.set_xlabel('Survey completion time (seconds)')\n        turn_off_lines(ax)\n# finishing touches\naxes[0].set_ylabel('Probabilty density')\naxes[1].legend()\nfig.set_size_inches(10,5)","ee858f22":"fig, axes = plt.subplots(3,3, gridspec_kw= {'wspace':0.5, 'hspace':0,'height_ratios': [4, 4,2]})\n\nresiduals = []\nfor axs, body in zip(axes, ('EU','UK')):\n    for i, ax in enumerate(axs):\n        # make the plot\n        if body=='UK':\n            ax.invert_yaxis()\n            #ax.axhline(0.12,color='grey', alpha=0.5)\n        grouped_by_age = yearly_dfs[i][yearly_dfs[i]['body']==body].groupby('age')['body'].count() \n        grouped_by_age = grouped_by_age \/ grouped_by_age.sum() # normalise\n        ax.bar(grouped_by_age.index, grouped_by_age.values, color=color_dict[body], label=body, alpha=0.5,lw=2)\n        turn_off_lines(ax)\n        ax.grid(axis='y')\n        if i==2:\n            ax.legend()\n        # store values for residual plot \n        residuals.append(grouped_by_age.values)\n        if body=='EU':\n            ax.set_title(survey_years[i])\n\n# plot the difference graph\nresiduals = np.array(residuals).reshape(33,2)      \ndifferences = (residuals[:,0]-residuals[:,1])#\/(residuals[:,0]+residuals[:,1])\ndifferences = differences.reshape(3,11)\n\nfor ax, resid in zip(axes[2], differences):\n    # make the colours for the differance graph\n    colors = pd.Series(resid.copy()) # be flamboyent with types\n    mask = colors>0\n    colors[mask] = color_dict['EU']\n    colors[~mask] = color_dict['UK']\n    ax.bar(grouped_by_age.index, resid, color=colors, alpha=0.3)\n    turn_off_lines(ax)\n    plt.setp(ax.get_xticklabels(), rotation=90)\n    ax.set_ylim(-0.2,0.2)\n    ax.set_xlabel('Age group (years)')\n    ax.spines['left'].set_visible(False)\n    ax.yaxis.set_visible(False)\n    ax.axhline(0, color='grey', alpha=0.3)\naxes[0][0].set_ylabel('Fraction of respondents')\naxes[0][0].yaxis.set_label_coords(-0.175, 0)\n\nfig.set_size_inches(15,4)","8b2125d4":"UK_MULTI_COLOURS = ['#810a1e','#ef3654','#b00e29','#e01233']\nEU_MULTI_COLOURS = ['#10254b','#2c63c9','#193a75','#234e9f']\npi_labels = ['UK: Female','UK: Male','UK: Other','EU: Female','EU: Male','EU: Other']\nsize = 0.3 # used to determine size of the doughnuts\n# kwargs shared across the charts\nshared_pi_formats = {'wedgeprops':dict(width=size, edgecolor='w'),\n                     'textprops':dict(color='white', fontsize=8),\n                     'autopct':'%.1f %%'}\n\nfig, axes = plt.subplots(ncols=3)\nfor i, ax in enumerate(axes):\n    # group all the other genders to 'Other' for simplicity see disclaimer above\n    yearly_dfs[i].loc[~yearly_dfs[i]['gender'].isin(['Male','Female']), 'gender'] = 'Other'\n    UK_mask = yearly_dfs[i]['body']=='UK'\n    EU_mask = yearly_dfs[i]['body']=='EU'\n    ax.pie(yearly_dfs[i][UK_mask].groupby('gender')['body'].count(), radius=1,\n           colors=UK_MULTI_COLOURS, pctdistance=0.85, **shared_pi_formats)\n    ax.pie(yearly_dfs[i][EU_mask].groupby('gender')['body'].count(), radius=1-size,\n           colors=EU_MULTI_COLOURS, pctdistance=0.78, **shared_pi_formats)\n    ax.set(aspect=\"equal\", title=survey_years[i])\n\n    \naxes[2].legend(loc=(0.8,0.8), labels=pi_labels)\nfig.set_size_inches(16,6)","0551b745":"# create a dictionary with eductation data in\nEU_dict = {'Bachelor':[],'Master':[],'Doctoral':[]}\nUK_dict = {'Bachelor':[],'Master':[],'Doctoral':[]}\nfor i in range(0,3):\n    uk_mask = yearly_dfs[i]['body']=='UK'\n    eu_mask = yearly_dfs[i]['body']=='EU'\n    eu_degree_fractions = (yearly_dfs[i][eu_mask].groupby('education')['body'].count() \/ \n                           yearly_dfs[i][eu_mask].groupby('education')['body'].count().sum())\n    uk_degree_fractions = (yearly_dfs[i][uk_mask].groupby('education')['body'].count() \/ \n                           yearly_dfs[i][uk_mask].groupby('education')['body'].count().sum())\n    # extract values for each degree and append to global dicts\n    for degree in ['Bachelor','Master','Doctoral']:\n        eu_degree_mask = eu_degree_fractions.index.str.contains(degree)\n        uk_degree_mask = uk_degree_fractions.index.str.contains(degree)\n        EU_dict[degree].append(eu_degree_fractions[eu_degree_mask].values)\n        UK_dict[degree].append(uk_degree_fractions[uk_degree_mask].values)\n        \n# plot the data we created\n\nfig, ax = plt.subplots(1, 1)\nax.bar(survey_years-0.2,  np.ravel(EU_dict['Doctoral']), 0.4, color=EU_MULTI_COLOURS[0],\n       alpha=0.8)\nax.bar(survey_years+0.2, np.ravel(UK_dict['Doctoral']), 0.4, color=UK_MULTI_COLOURS[0],\n       alpha=0.8)\nax.bar(survey_years-0.2, np.ravel(EU_dict['Master']), 0.4, color=EU_MULTI_COLOURS[1],\n       label='EU', alpha=0.8, bottom = np.ravel(EU_dict['Doctoral']))\nax.bar(survey_years+0.2, np.ravel(UK_dict['Master']), 0.4, color=UK_MULTI_COLOURS[1],\n       label='UK', alpha=0.8, bottom = np.ravel(UK_dict['Doctoral']))\nax.bar(survey_years-0.2, np.ravel(EU_dict['Bachelor']), 0.4, color=EU_MULTI_COLOURS[3],\n       alpha=0.5,  bottom = np.ravel(EU_dict['Doctoral']) +  np.ravel(EU_dict['Master']))\nax.bar(survey_years+0.2, np.ravel(UK_dict['Bachelor']), 0.4, color=UK_MULTI_COLOURS[3], \n       alpha=0.5,  bottom = np.ravel(UK_dict['Doctoral']) + np.ravel(UK_dict['Master']))\n\n\n# add labels and sort out tick labels\nax.legend(loc=(1.01,0.878))\nax.set_ylabel('Fraction of responses')\nax.set_xlabel('Year')\nax.text(0.09,0.1,'PhD', transform=ax.transAxes, color='white', rotation=0, fontsize=10)\nax.text(0.07,0.47,'Master', transform=ax.transAxes, color='white', rotation=0, fontsize=10)\nax.text(0.057,0.82,'Bachelor', transform=ax.transAxes, color='white', rotation=0, fontsize=10)\n_ = ax.set_xticklabels(['','','2017','','2018','','2019'])\nax.set_ylim(0,1)\n# turn off all the lines to remove clutter\nturn_off_lines(ax)\nfig.set_size_inches(7,5)","ee5a67cb":"# only looking at students for now.\njobs_to_use = ['Student','Other']\n\n# map all the other categories to 'other'\nyearly_dfs[0].loc[~yearly_dfs[0]['employment_status'].isin(jobs_to_use), 'employment_status'] = 'Other'\nyearly_dfs[1].loc[~yearly_dfs[1]['employment_status'].isin(jobs_to_use), 'employment_status'] = 'Other'\n\nEU_students = []\nUK_students = []\nstudents = {'UK':UK_students, 'EU':EU_students}\nfor body in ['UK','EU']:\n    student_2019  = yearly_dfs[0][yearly_dfs[0]['body']==body].groupby('employment_status')['body'].count()\n    student_2019 = student_2019 \/ student_2019.sum()\n    student_2018 = yearly_dfs[1][yearly_dfs[1]['body']==body].groupby('employment_status')['body'].count()\n    student_2018 = student_2018 \/ student_2018.sum()\n    students[body].append([student_2019.values[1], student_2018.values[1]])\n    \nfig, ax = plt.subplots(1, 1)\nax.bar(survey_years[:-1]-0.2,  np.ravel(students['EU'])*100, 0.4, color=EU_COLOUR, alpha=0.8, label='EU')\nax.bar(survey_years[:-1]+0.2, np.ravel(students['UK'])*100, 0.4, color=UK_COLOUR, alpha=0.8, label='UK')\n\nax.legend()\nax.set_ylabel('Fraction of responses from students (%)')\nax.set_xlabel('Year')\n_ = ax.set_xticklabels(['','','2018','','','','2019'])\nax.set_ylim(0,25)\nturn_off_lines(ax)\nfig.set_size_inches(7,5)","b54d0782":"fig, axes = plt.subplots(1,2, sharey=True)\noffset = {'UK':0.2, 'EU':-0.2}\nfor i, ax in enumerate(axes):\n    yearly_dfs[i]['coding_lower'] =  yearly_dfs[i]['coding_experience'].str.split('-', expand=True)[0].astype(float)\n    yearly_dfs[i]['coding_higher'] =  yearly_dfs[i]['coding_experience'].str.split('-', expand=True)[1]\n    for body in ['EU','UK']:\n        coding_grouping = yearly_dfs[i][yearly_dfs[i]['body']==body].groupby(['coding_lower','coding_higher'])['body'].count()\n        coding_grouping = coding_grouping \/ coding_grouping.sum()\n        coding_grouping = coding_grouping.reset_index()\n        coding_grouping['bins'] = coding_grouping['coding_lower'].astype(int).astype(str) + '-' + coding_grouping['coding_higher'].astype(str)\n        labels = coding_grouping['bins'].values\n        ax.bar(np.arange(0, len(labels))-offset[body], coding_grouping['body'], alpha=0.6, color=color_dict[body], label=body, width=0.4)\n        ax.set_title(survey_years[i])\n        ax.set_xlabel('Coding experience (years)')\n        # turn off all the lines to remove clutter\n        ax.set_xticklabels(['']+list(labels[:-1])+['20+'])\n        turn_off_lines(ax)\n        ax.grid(axis='y', alpha=0.4)\n    \n# tidy up the axes\naxes[0].set_ylabel('Fraction of respondents')\naxes[1].legend()\n\nfig.set_size_inches(14,5.5)","0b068d6c":"fig, axes = plt.subplots(1,3, sharey=True, gridspec_kw = {'wspace':0, 'hspace':0,\n                                                         'width_ratios': [3, 3,1]})\ngroupings = []\nfor ax, body in zip(axes,['UK','EU']):\n    salary_grouping = yearly_dfs[0][yearly_dfs[0]['body']==body].groupby(['salary_lower','salary_higher'])['body'].count()\n    salary_grouping = salary_grouping \/ salary_grouping.sum()\n    salary_grouping = salary_grouping.reset_index()\n    salary_grouping['salary_lower'] = salary_grouping['salary_lower'] \n    salary_grouping['bins'] = salary_grouping['salary_lower'].astype(str) + '-' + salary_grouping['salary_higher'].astype(str)\n    groupings.append((salary_grouping['bins'], salary_grouping['body']))\n    ax.barh(salary_grouping['bins'], salary_grouping['body'], alpha=0.5, color=color_dict[body], label=body)\n    ax.set_title(body, fontsize=14)\n    plt.setp(ax.spines.values(), visible=False)\n    ax.patch.set_visible(False)\n    ax.grid(False)\n    ax.set_xlabel('Fraction of respondents')\n\naxes[0].invert_xaxis()\naxes[0].set_ylabel('Compensation amount ($)')\n\n#ax.set_title('Compensation amount in 2019')\n\n# calulate and plot the residuals on the third axis\n# calculate normalised difference\nasymmetry = (groupings[1][1][:-1]-groupings[0][1])\/(groupings[1][1][:-1]+groupings[0][1])\n#\u00a0make the colours of the bars\ncolors = asymmetry.copy()\nmask = asymmetry>0\ncolors[mask] = color_dict['EU']\ncolors[~mask] = color_dict['UK']\n#\u00a0plot\naxes[2].barh(groupings[0][0], asymmetry, alpha=0.3,color=colors)\naxes[2].axvline(0, alpha=0.2, color='gray')\naxes[2].grid(False)\nplt.setp(axes[2].spines.values(), visible=False)\nplt.setp(axes[2].get_xticklabels(), visible=False)\naxes[2].patch.set_visible(False)\naxes[2].set_title('Normalized differences', fontsize=14)\nplt.suptitle('Reported income by body', fontsize=16)\nfig.set_size_inches(15,12)","46356f34":"\n#\u00a0group the compensation level in low, medium or high income.\nsalary_mapping = {'$0-999':'low', '1,000-1,999':'low', \n                  '10,000-14,999':'low', '100,000-124,999':'high',\n                  '125,000-149,999':'high', '15,000-19,999':'low', \n                  '150,000-199,999':'high', '2,000-2,999':'low',\n                  '20,000-24,999':'low', '200,000-249,999':'high', \n                  '25,000-29,999':'low', '250,000-299,999':'high',\n                  '3,000-3,999':'low','30,000-39,999':'medium',\n                  '300,000-500,000':'high', '4,000-4,999':'low',\n                  '40,000-49,999':'medium', '5,000-7,499':'low', \n                  '50,000-59,999':'medium', '60,000-69,999':'medium',\n                  '7,500-9,999':'low', '70,000-79,999':'medium', \n                  '80,000-89,999':'medium', '90,000-99,999':'medium',\n                  '> $500,000':'high'}\n# make the salary grouping\nyearly_dfs[0]['income_group'] = yearly_dfs[0]['salary'].map(salary_mapping)\n    \nfig, axes = plt.subplots(ncols=3)\nfor i, (ax, income) in enumerate(zip(axes, ['low', 'medium','high'])):\n    # group all the other genders to 'Other' for simplicity see disclaimer above\n    yearly_dfs[0].loc[~yearly_dfs[0]['gender'].isin(['Male','Female']), 'gender'] = 'Other'\n    \n    # respondent masks\n    UK_mask = yearly_dfs[0]['body']=='UK'\n    EU_mask = yearly_dfs[0]['body']=='EU'\n    income_mask = yearly_dfs[0]['income_group']==income\n    \n    # Count respondents of each gender in subgroup\n    EU_gender_count = yearly_dfs[0][EU_mask & income_mask].groupby('gender')['body'].count()\n    UK_gender_count = yearly_dfs[0][UK_mask & income_mask].groupby('gender')['body'].count()\n\n    # plot\n    ax.pie(UK_gender_count, radius=1, colors=UK_MULTI_COLOURS, pctdistance=0.85, **shared_pi_formats)\n    ax.pie(EU_gender_count, radius=1-size, colors=EU_MULTI_COLOURS, pctdistance=0.78, **shared_pi_formats)\n    ax.set(aspect=\"equal\", title=f'{income.capitalize()} income')\n\n\naxes[1].legend(loc=(2.1,0.8), labels=pi_labels)\nfig.set_size_inches(16,6)","55f49d0d":"# extract the question on engagement with media sources\ndf_media = yearly_dfs[0].loc[:,(yearly_dfs[0].columns.str.contains('Q12')==True) | yearly_dfs[0].columns.str.contains('body')]\n\n# extract name of websites for each response\ncol_names = df_media.mode().values[0]\nengagement_types = [str(name).split('(')[0].strip() for name in col_names]\n\n# make the column names the name of the media\ndf_media.columns = ['body'] + engagement_types[1:]\n\n# drop the free field text column\ndf_media.drop('-1', axis=1, inplace=True)\n\n# count the number of each media type\nmedia_grouped = df_media.groupby('body').count()-1 # -1 for the question row\n\n# make the plot\nfig, ax = plt.subplots()\nfor body in ['EU','UK']:\n    data = media_grouped.loc[body,:] \/ media_grouped.loc[body,:].sum()\n    data.sort_values(inplace=True)\n    ax.bar(data.index, data.values, alpha=0.35, color=color_dict[body], label=body)\n    turn_off_lines(ax)\nax.legend()\nplt.setp(ax.get_xticklabels(), rotation=90)    \nax.set_ylabel('Fraction of total engagements')\nax.set_title('2019 Media engagements')\nfig.set_size_inches(8,5)","b353b45c":"body_series = df_media['body'].copy()\ndf_media[~df_media.isna()] = 1\ndf_media[df_media.isna()] = 0\n\nfig, axes = plt.subplots(1,2)\n\nfor ax, body, cmap in zip(axes, ['EU','UK'], ['Blues','Reds']):\n    mask = body_series==body\n    # create the cross occurances df\n    cross_occ_df = pd.DataFrame(index=df_media.columns[1:], columns=df_media.columns[1:])\n    cols = df_media.columns[1:]\n    for col_1 in cols:\n        for col_2 in cols:\n            # calculate the co-occurances (i.e, when respondent said they used them both)\n            cross_occ = pd.crosstab(df_media.loc[mask,col_1], df_media.loc[mask,col_2])\n            try:\n                cross_occ_df.loc[col_1,col_2] = cross_occ.loc[1,1]\n            except:\n                cross_occ_df.loc[col_1, col_2] = 0\n    # plot and format the graph\n    ax.imshow(cross_occ_df, cmap=cmap)\n    ax.set_title(body)\n    ax.set_xticks(np.arange(0,12))\n    ax.set_yticks(np.arange(0,12))\n    ax.set_xticklabels(cols, rotation=90)\n    ax.set_yticklabels(cols)\n    fig.set_size_inches(10,10)\n    turn_off_lines(ax)\n    if body=='UK':\n        ax.set_yticklabels([])\n        ax.spines['left'].set_visible(False)\n        ax.set_yticks([])\n        \nplt.suptitle('Co-occurances of media use', fontsize=15)\nfig.set_size_inches(10,5)","b60eba60":"cols_for_analysis = ['job'] + list(yearly_dfs[0].columns[yearly_dfs[0].columns.str.contains('Q18')])\njob_titles = ['Data Scientist', 'Software Engineer', 'Student', 'Data Analyst', 'Research Scientist']\n\nfig, axes = plt.subplots(1,2, facecolor=(0.99, 0.99, 0.99), sharey=True)\n\nfor ax, body, cmap in zip(axes,['EU','UK'], ['Blues','Reds']):\n    # extract the data\n    df_IDE = yearly_dfs[0][cols_for_analysis]\n    # make the column names those of the tick box\n    ide_names = yearly_dfs[0][cols_for_analysis].mode().values[0]\n    df_IDE.columns = ['job'] + list(ide_names[1:])\n    # make a mask for the body\n    body_mask = yearly_dfs[0].body==body\n    \n    # count number of ide used by each job type\n    grouped_df = df_IDE.loc[body_mask,:].groupby('job').count().loc[job_titles, :'None']\n    values = grouped_df.values \/ grouped_df.values.sum(axis=1)[:,np.newaxis]\n    # make a grid to plot on\n    X, Y = np.meshgrid(np.arange(1,12), np.arange(1,len(job_titles)+1))\n    \n    # get and clean up the tick labels\n    x_tick_labels = [x.split('(')[0].split('\/')[0].strip() for x in grouped_df.columns]\n    y_tick_labels = list(grouped_df.index)\n\n    ax.scatter(x=np.ravel(X), y=np.ravel(Y), s=np.ravel(values)*2500, c=np.ravel(values), cmap=cmap)\n\n    # tidy up axis, make ticks and lines invsible.\n    plt.setp(ax.spines.values(), visible=False)\n    ax.patch.set_visible(False)\n    ax.grid(False)\n    ax.set_yticks(np.arange(1,len(job_titles)+1))\n    ax.set_yticklabels(y_tick_labels)\n    ax.set_xticks(np.arange(1,12))\n    ax.set_xticklabels(x_tick_labels, rotation=90)\n    ax.set_title(f'{body} respondents')\nplt.suptitle('Programming by Profession', fontsize=14)\n    \nfig.set_size_inches(15,5)","17bdd4a1":"cols_for_analysis = ['job'] + list(yearly_dfs[0].columns[yearly_dfs[0].columns.str.contains('Q16')])\njob_titles = ['Data Scientist', 'Software Engineer', 'Student', 'Data Analyst', 'Research Scientist']\n\nfig, axes = plt.subplots(1,2, facecolor=(0.99, 0.99, 0.99), sharey=True)\n\nfor ax, body, cmap in zip(axes,['EU','UK'], ['Blues','Reds']):\n    # extract the data\n    df_IDE = yearly_dfs[0][cols_for_analysis]\n    # make the column names those of the tick box\n    ide_names = yearly_dfs[0][cols_for_analysis].mode().values[0]\n    df_IDE.columns = ['job'] + list(ide_names[1:])\n    # make a mask for the body\n    body_mask = yearly_dfs[0].body==body\n    \n    # count number of ide used by each job type\n    grouped_df = df_IDE.loc[body_mask,:].groupby('job').count().loc[job_titles, :'None']\n    values = grouped_df.values \/ grouped_df.values.sum(axis=1)[:,np.newaxis]\n    # make a grid to plot on\n    X, Y = np.meshgrid(np.arange(1,12), np.arange(1,len(job_titles)+1))\n    \n    # get and clean up the tick labels\n    x_tick_labels = [x.split('(')[0].split('\/')[0].strip() for x in grouped_df.columns]\n    y_tick_labels = list(grouped_df.index)\n\n    ax.scatter(x=np.ravel(X), y=np.ravel(Y), s=np.ravel(values)*2500, c=np.ravel(values), cmap=cmap)\n\n    # tidy up axis, make ticks and lines invsible.\n    plt.setp(ax.spines.values(), visible=False)\n    ax.patch.set_visible(False)\n    ax.grid(False)\n    ax.set_yticks(np.arange(1,len(job_titles)+1))\n    ax.set_yticklabels(y_tick_labels)\n    ax.set_xticks(np.arange(1,12))\n    ax.set_xticklabels(x_tick_labels, rotation=90)\n    ax.set_title(f'{body} respondents')\n    \nplt.suptitle('IDE use by Profession', fontsize=14)    \nfig.set_size_inches(15,5)","13e009ed":"### Figure 5.1 Reported compensation in 2019\nThis figure displays the fraction of respondents reporting a salary in a given compensation window (in dollars) for the both UK (red bars) and EU (blue bars). The figure in the far right panel (titled 'Normalized differences') displays the normalized difference between the number of respondents per income category for respondents in the  UK and EU. When the fractional respondents is higher for the EU the bars are coloured blue, when the UK respondents report more often in a given income category the bars are coloured red.","c21e77dc":"### Figure 4.2 Student engagment\n\nThis figure displays the fraction of respondents who are a Student in 2018 (left two bars) and 2019 (right two bars) for respondents located in the EU (solid blue bars) and UK (solid red bars).","15eddf50":"### Figure 1.1 Survey participation\n\nThe below graphic displays the number of responses to the 2017, 2018 and 2019 Kaggle data science and machine learning surveys. The left panel displays the net number of responses, with responses from persons located in the UK and EU represented by solid red bars and solid blue bars respectively. The right hand panel displays the number of respondents normalised by the population of the two bodies (as reported by [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Demographics_of_the_European_Union#Population_shifts)). We have excluded the UK population from the population of the EU.","856daf3c":"# 5. Compensation\n\nIn this section we look at the the reported compensation of respondents in the 2019 Kaggle survey. Firstly, we display the reported compensation values for respondents located in the UK and EU in Figure 5.1. The biggest  difference in the distribution is the UK respondents, on average, report higher salaries than EU respondents (compare the normalized difference graphic in the right panel. The higher values are mainly shaded red and the lower value incomes are mainly shaded blue, which indicate that the UK respondents generally reported higher income). The earning disparity could be accounted for by some of the insights we previously extracted: UK respondents are more likely to have a PhD (Figure 4.1) and are slightly older (Figure 2.1), which both should account for a higher levels of compensation.  Finally, [compared to about half of the EU member states the UK has a higher cost of living](https:\/\/www.numbeo.com\/cost-of-living\/rankings_by_country.jsp?title=2018&region=150) and this could result in a higher realtive compensation amount of UK respondents. Its unlikely the number of student respondents affect the results (see Figure 4.2) as the EU saw fractionally less student responses in 2019 compared to the UK.\n\nFollow this, we also grouped respondents into three income categories (low, medium and high income) and then looked at the gender distribution across these income bands (Figure 5.2). Our graphic reveals a striking gender imbalance for both UK and EU respondents: higher earners are nearly exclusively Male and the Female to Male gender ratio drops as the income category reduces (i.e., lower earners are more likely to be Female than higher earner). This is the stark reminder to addresses the gender biases and barriers in the data science and machine learning community. \n\nThere are concerns that a No deal Brexit will cause a dramatic [hit to the UK economy](https:\/\/news.sky.com\/story\/live-chancellor-philip-hammond-to-reveal-brexit-economic-impact-analysis-11565724), and so a particularily intresting comparison could be made after Brexit event itself (i.e., after the 2010 Kaggle survey). \n\n**Key points:**\n>- A higher fraction of UK respondents fell into higher wage brackets than EU respondents.\n- A significantly higher fraction of EU respondents earned less then \\$15,000 a year compared to UK respondents.\n- There is a large gender imbalance for recievers of high income: they are nearly exclusively male for both the EU and UK.","37800a58":"# 0. Robustness of analysis\nWith any analysis, it is important to outline the limitations of the work and any assumptions we use.\n\n**Limitations:**\n\n> - This survey was not commissioned to look at EU\/UK relationship. It was not even commissioned with a UK\/EU focus. We must question at each step whether the secondary data usage is relevant.\n- We are dealing with a relatively small number of yearly responses (~500 from UK, and ~2500 from EU member states). We must therefore be careful to interrogate whether any findings are statistically significant for this number of responses.\n- The survey targets a primarily a specific demographic (primarily, users heavily engaged with the Kaggle platform). Therefore, we must be careful when extrapolating our results beyond this demographic.\n- There is an inherent flaw of comparing a number of countries (the EU excluding the UK) with a large economic and social diversity to a single country (the UK) with a more restricted social and economic landscape. In reality it would be more meaningful to compare the UK only to an EU member state reflective of its politics and economy (e.g., France with a similar GDP per capita). However, the UK vs EU is a bit more theatrical  so I have chosen to do this. This is likely the largest flaw in this, so please hold it in your minds when reading through this notebook. See the book 'Factfulness' by H. Rosling for more insights about such questionable comparisons.\n\n**Assumptions:**\n\n>- It is likely some EU countries (only 17 of the 28 member states are represented in the 2019 results) are listed in the 'Other' category and these are excluded from my analysis.","a1a822f7":"# 8. Conclusions\n\nWe performed a high level analysis of the previous three Kaggle ML\/DS surveys, focussing on comparing the responses of persons located in the UK to persons located in the other EU member states. We found that the UK and EU largely share similar problems: primarily, a large gender imbalance (Figure 3.1) in the data science and machine learning communities. We found noticeable differences between the reported compensation amounts of EU and UK respondents (Figure 5.1), and found a large gender imbalance between low and high earners both in the UK and EU (Figure 5.2). We discovered subtle differences in the degree attainment levels of respondents (Figure 4.1), with EU respondents more likely to have a higher degree but UK respondents more likely to have a PhD. We investigated the media UK and EU respondents used (Figure 6.1 and 6.2), media platform engagement was similar for both communities but the UK respondents were more likely to use Twitter to engage with data science media.\n\nOverall we found that the UK data science community is very similar to the EU community and there are no obvious signs in the survey of negative effects arising as a result of the referendum on the UK data science community. We found that, due to the different format of the three surveys (2017, 2018, 2019), it was difficult to draw out reliable temporal trends. Going forward, it will be important to keep a similar format to the 2019 survey such that in future years (i.e., the 2020 survey and beyond) robust temporal analysis can be performed which, for example, could look at the changes in the UK data science community post-Brexit (particularily in the event of a No deal brexit, which would result in the largest changes to the UK and EU economies and culture).\n\n**Five key findings:**\n\n>1. The EU and UK data science communities have a large gender imbalance.\n2. UK respondents report a higher average salary than their EU counterparts; there is a significant gender imbalance between low and high earners.\n3. A larger fraction of EU respondents obtained higher degrees (Masters or PhD) but more UK respondents have PhDs.\n4. Python and Jupyter are the most used programming language and IDE respectively, for both UK and EU respondents.\n5. Most survery respondents are under 30: a young demographic responded to the survey.\n\n## 8.1 How could this analysis be improved?\n>- Additional years of questioning and increase sample size\n- Direct comparison of countries (e.g., UK vs France)\n- Comparison of clusters in the data (e.g., cluster on IDE and investigate education levels)\n\n**Please feel free to provide constructive feedback and let me know the figures you find easiest to interpret!**","e934e97e":"### Figure 3.1 Gender distribution\n\nThis figure displays the gender distribution for respondents to the Kaggle DS\/ML survey undertaken in 2019, 2018 and 2017 (left to right panel). We have grouped several responses (Did not respond, self-identify and non-binary) under one heading 'Other': this is to keep the graphic simple and to minimize the number of labels. UK responses are represented by the outer circle (shades of red) and EU responses are represented by the inner circle (shades of blue). ","f4ea2236":"### Figure 7.2: IDE use by profession in 2019\n\nIn this figure we look at the IDEs used by respondents in five of the top professions (rows of figure). The size (bigger is more) and colour (darker is more) indicate the number of repsondents of a given job type who indicated they use this IDE. \n\nThe left panel (blue circles) corresponds to EU respondents and the right panel (red circles) corresponds to UK respondents.","3aeef50a":"### Figure 5.2 Compensation by gender in 2019\n\nThe below graphic outlines the gender distribution for persons with a low income (\\$0-\\$29999, approximately less than the [UK average income](https:\/\/www.ons.gov.uk\/peoplepopulationandcommunity\/personalandhouseholdfinances\/incomeandwealth\/bulletins\/householddisposableincomeandinequality\/yearending2018)), a medium income (\\$30,000-\\$99999) and a high income (\\$100000+) in both the EU (blue segments) and UK (red segments). This is adapted from [my original post](https:\/\/www.kaggle.com\/fchmiel\/who-will-your-analysis-story-be-about#Method-3:-Using-UMAP) outlining how to create this graphic.\n\nOnly 26 respondents reported a low income in the UK.","9cd77ee1":"# <img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/b7\/Flag_of_Europe.svg\" width=\"200\" height=\"200\"> <img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/en\/a\/ae\/Flag_of_the_United_Kingdom.svg\" width=\"200\" height=\"200\">\n\n# EU vs UK \n\nAs a British Data Scientist working in academia I am acutely aware of the potential impact Brexit is having (and expected to have) on the data science and academic landscape. For example, there are already indications of [reduced grant money for UK scientists](https:\/\/royalsociety.org\/-\/media\/news\/2019\/brexit-uk-science-impact.pdf) and signs that Brexit will cause long term damage to the [wider British economy](https:\/\/www.piie.com\/publications\/working-papers\/brexit-everyone-loses-britain-loses-most). From a non-economic perspective, UK universities [face the threat](https:\/\/academic.oup.com\/oxrep\/article\/33\/suppl_1\/S155\/3066079) of a restriction in hiring academic staff members from EU states and the UK as a whole is expected to see [reduced migration from EU member states](https:\/\/journals.sagepub.com\/doi\/abs\/10.1177\/002795011623800111?journalCode=nera).\n\nMotivated by the imminent British exit from the European Union, I was interested to see  how members of the UK data science and machine learning community compares to those in the wider European community. The most recent Kaggle data science and machine learning survey (now in it's third iteration) gives us an opportunity to compare these communites; investigating their differing demographics, compensation, and  education levels.\n\nIn this notebook I will use the previous three surveys to compare the responses of persons located in the UK to those located in other EU member states. While I focus on the results of the 2019 survey, I do use results from the previous two years where possible. This temporal dependance gives us a chance to investigate the changing nature of post-referendum UK data science and machine learning community. The results should be taken with a large pinch of salt (please read the limitations below). Because this survey was not designed to investigate  EU\/UK differences it is unlikely to hold much value, however, I hope it motivates some readers to think about the impact of Brexit (and other global) events on the wider data science community. Within Kaggle, (writing this before I have done any significant analysis!) I hope this work supports the inclusive and cross-border it culture maintains.\n\n\n**Note when reading this notebook**\n\n>- All figures use a consistent colour scheme: <span style=\"color:red\">UK results are represented by red<\/span> and <span style=\"color:blue\">EU results are represented by blue<\/span>.\n- When I refer to the 'EU' I actually mean the European Union excluding the UK (that is 27 member states). \n- Each section (index on left pane of notebook viewer) starts with a textual summary of the findings. There is then a 'Key points' section with bullet points summarizing the key findings of the analysis presented in the given section. Following this, there are the individual figures associated with each section, each with their own caption. Captions are above the figure and you won't need to read them unless you can't understand the figure.\n\nEnjoy!\n\n\n","363c65cf":"# 3. Gender\n\nThis section investigates the self-reported gender of EU\/UK respondents to the survey, the results are summarised in Figure 3.1. Overall, we can see there is a large gender imbalance in respondents (approximately 80% of respondents identify as Male for all years); the computer science and technology sectors are known to have [one of the largest gender imbalances](https:\/\/www.stemwomen.co.uk\/blog\/2019\/09\/women-in-stem-percentages-of-women-in-stem-statistics) and the results of this survey reflect this. However, in 2018 and 2019 there was a slight increase in Female participants compared to 2017, this may be a result of advertising differences between the years attracting more female participants in the survey as opposed to an underlying change in the gender distribution of the sector however.\n\nNotably, the UK has a higher fraction of Female participation when compared to the EU as a whole (18% to 14%), while small, this is likely significant (the UK attracted over sFemale respondents in 2017 and 2018). I'm aware the UK, for a long time,  has been trying to increase Female participation in STEM subjects at school and [success is being seen already](https:\/\/www.thetimes.co.uk\/article\/a-level-results-2019-more-girls-than-boys-study-science-for-the-first-time-f3bgjgrsd). The lower gender imbalance reported here compared to the EU as a whole may reflect the impact of such policies within the UK.\n\n**Key points:**\n>- There is a large gender imbalance (~80% identifying as Male) reported in the Kaggle survey.\n- The UK has a slightly smaller gender imbalance than the EU as a whole.\n- The gender imbalance decreases from 2017 to 2018, but there has been no sigificant change from 2018 to 2019.","34c2f215":"### Figure 6.1 Favourite data science media sources\n\nThe below figures displays how many respondents said a given media type  was one of their favouriet sources of data science media (normalized by the total number favourites selected). Red bars represent UK responses and blue bars the EU ones. The bars are translucent and overlaid to allow a direct comparison of their heights.","e1592a03":"### Figure 1.2 : Survey completion time\n\nThis figure displays a kernel density estimate of the probability distribution of the survey completion time  for the 2019 (left figure) and the 2018 (right figure) Kaggle surveys  for respondents located in the UK (red shaded region) and EU (blue shaded region). Persons with completion times exceeding 7500 seconds were considered as outliers and ignored in this analysis.","fb810363":"### Figure 2.1 Age distribution\nThe panels (left to right) display the age distribution for respondents located in UK (red bars) and EU (blue bars) in 2019, 2018 and 2017.The bottom panel of each years graphic displays the difference between the fraction of EU and UK respondents reporting in a given age category, the bars in the difference plots are coloured corresponding to which body (UK or EU) have the higher fraction of respondents in the given age bracket. The 2017 survey had a significantly different question set and age grouping, which may have effected the observed results.","3c534645":"### Figure 4.1 Reported highest education levels\n\nThis figure displays the reported highest education levels of the respondents. For each year, the highest degree attainment is stacked such that the bar is cumulative (e.g, for the 2017 bar you can read off that ~90% of respondents had a Bachelors Degree). Responses from persons located in the EU are shaded in blue and those from the UK in red. All bars are stacked in the same order (i.e, from bottom to top Doctoral degrees, Master's degrees and Bachelor's degrees). ","f053554f":"# 1. Kaggle survey participation\n\nIn this section we look at how the participation in the Kaggle survey has changed over the last three years. In the left panel of Figure of 1.1 we display the net number of responses to the survey for persons located either in the UK or other EU member states. In the right panel of Figure 1.1 we display the reponse rate normalised by the respective population (as reported by [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Demographics_of_the_European_Union#Population_shifts)) of the two bodies. Between 2017 and 2018 a large (~50%) increase in the survey participation for both the EU and UK was observed. In 2019 participation levels return to the levels observed in 2017; potentially showing either a reduced engagement with the Kaggle platform in 2019 compared to 2018 for persons in the EU and UK, reduced advertisement of the survey and\/or greater entry restrictions. There were more responses per captia for the UK compared to the EU as a whole (Figure 1.1 right), but the difference is reducing each year with more EU participation relative to UK participation. This may be the result of reduced UK participation as a result of  ['survey\/referendum\/polling' fatigue](https:\/\/www.theguardian.com\/uk-news\/2019\/oct\/26\/voters-disillusionment-renders-expected-election-tough-to-call) - perhaps the UK public are getting tired of being asked things?\n\nAs well as looking at the participation rates, we also looked at the time taken by respondents to complete the survey (recorded in 2018 and 2019 only). Histograms of the response times is displayed in Figure 1.2. On average, persons spent longer completing the 2018 survey than 2019 survey (with an average difference of ~400 seconds) and the 2018 survey completion times had a greater variation (i.e., a broader distribution). In 2019 there was no significance difference in completion time between EU and UK respondents. However, UK residents did appear to take slightly longer on average in 2018 (compare the two maxima in Figure 1.2). This is likely a result of UK respondents answering more questions in total (which also explains the two disinct maxima in survey completion time).\n\n**Key points:**\n>- UK survey participation is less in 2019 compared to 2017, where as EU participation is slightly higher. (See left panel of Figure 1.1).\n- Participation per capita is higher for the UK compared to the (mean) participation per capita for EU member states, but the EU is catching up.\n- People spent less time completing the survey in 2019  (approximately 400 seconds less on average) compared to the 2018 survey.\n- There where no significant differences in completion time for UK \/ EU respondents in 2019.","a766c5bd":"# 4. Education\n\nIn this section we look at the education levels of respondents, the number of students participating and the coding experience of respondents. We begin with Figure 4.1 which displays the fraction of survey respondents with either a Bachelor's, Master's or Doctoral degree. We see that a large fraction of all respondents have obtained a minimum of a Bachelor's degree (~90%) and, unsurprisingly, both the EU and UK have a high fraction of respondents who have obtained a higher degree (Master's or PhD). The EU, however, does have a noticeably higher fraction of respondents with a higher degree (~75%) compared to the UK (~70%). Despite the greater attainment levels of higher degrees for the EU, UK respondents were more likely to have obtained a Doctoral degree (compare size darkest red bars to darkest blue bars in Figure 4.1). We cannot be sure why more EU respondents have a higher degree but it maybe a result of cheaper fees for a Master's programme in the EU compared to the UK, a different cultural attitude of the need to attain these degrees and\/or a greater number of univerities. \n\nThere are no temporal dependances of note in the data, although, there is a slight drop in degree attainment levels for UK respondents. This could indicate that the Kaggle survey is reaching a wider audience in the UK since 2017 (i.e., less specialized people who have not obtained a university degree).\n\nLooking at the engagement of current students (Figure 4.2), we see that Kaggle surveys have a modest response (~15%) from students in the UK and EU. Between 2018 and 2019 there were fractionally less student responses from persons located in the EU, but UK student engagement remained (proportionally) the same.\n\nFinally, we discuss the coding experience of respondents (Figure 4.3). Firstly, we remark that the 2018 respondents had less coding experience than the 2019 respondents. The 2018 survey saw higher participation levels (Figure 1.1) and likely reached a broader audience with lower net experience in the data science \/ machine learning fields, resulting in lower reported coding experiences. Secondly, comparing the UK respondents to the EU respondents, the UK respondents had more coding experience on average that the EU respondents. This may reflect the slightly older demographic of UK respondents (Figure 2.1) and the higher fraction with Doctoral degrees (likely resulting in more opportunity to code). \n\n**Key points:**\n>- Both the UK and EU respondents have similar overall degree attainment levels.\n- Overall,  EU respondents are more likely to have obtained either a Master's or Doctoral degree compared to the UK.\n- However, UK respondents are more likely to hold a PhD than EU respondents.\n- Total UK degree attainment of respondents has dropped slightly over three years.\n- Approximately 15% of all responses from the EU\/UK were from students.\n- On average, the UK respondents had slightly more coding experience than there EU counterparts.\n- Overall, the 2018 survey (UK\/EU) respondents had less coding exprience than the 2019 respondents. Likely reflecting a broader audience scope in the 2018 survey.","93ed8ee3":"# 6. Media Engagement\n\nIn the following, we look at the data science media sources UK and EU respondents said were there favourite for accessing data science media in 2019. Figure 6.1 displays the number of times respondent a indicated a given media type was one of their favourite media sources (normalized by the total number of indications). Overall, Blogs, Kaggle and Youtube were used most by respondents to engage with data science media. The media engangement of UK \/ EU respondents were similar, but UK respondents were more likely to be engaged with Twitter compared to EU respondents.\n\nWe also investigated what media sources repondents selected *together* as their favourite media sources (Figure 6.2), which could indicate complementary sources. We found that users rarely use a single source of media but tend to use a number of media sources, for example Journal Publications hold a media source complementary to Kaggle (e.g., novel methods for use in competitions).\n\n**Key points:**\n>- Types of platforms used to engage with data science media is similar in the UK and EU.\n- UK respondents are more likely to have used Twitter to engage with data science media.\n- Respondents tended to indicate several sources in the favourite media sources.","d5ffd283":"### Figure 6.2 Co-occurances of media use\n\nThis figure highlights the types of media sources respondents indicated were their favourites. The diagonal elements show the number of times respondents said the given media type was one of their favourites (darker colour is more respondents). The off-diagonal elements show media sources which users indicated *were both* in their favoure media sources for accessing data science media. For example, read the Kaggle row and the colour denotes the frequency that the respondent selected Kaggle and the other media (denoted by the column) as one of their favourites. This graph not only tells you that Kaggle, Blogs and Youtube are the favourite media sources (look at the diagonal elements) but the off-diagonal elements also tell you that respondents who selected Kaggle as one of their favourite media sources also liked Blogs and Youtube.","d352deef":"# 7. Software use\n\nIn this section we dicuss respondents use of different software and programming languages. In our first figure (Figure 7.1) we look at the programming language used by five of the top professions responding to the 2019 Kaggle survey. The first thing to note is that Python is clearly the language used most regularily by many of the respondents. Secondly, we can look as specific professions. For example, Data Analysts (2nd from top) responded they use Python, R and SQL regularily but few of them used any other languages. Comparitively, Software Engineers (2nd from bottom) used a more diverse range of languages. Data Scientists (bottom) used similar languages to Data Analysts but more Analysts used SQL. Matlab was not used extensively, with the exception of (UK) Students and Research Scientists. Personally, the author knows from experience that Matlab is a favourite of UK universities for undergraduate teaching and this may explain this finding. Overall, there were only small differences between programming language use between EU and UK respondents.\n\nIn our next figure (Figure 7.2) we investigate respondents use of different IDE. Again we see only small differences between UK and EU respondents but we observe quite large differences between respondents with different Jobs. For example, Software Engineers (2nd from bottom) particuarily favour Visual Studio more than other respondents and Research Scientists (top row) tend to use the most diverse range of IDEs. Overall, Jupyter, PyCharm and RStudio are the IDEs used by the most respondents. \n\n**Key points:**\n>- Python is the language most used by respondents. R and SQL are the 2nd most.\n- Data Analysts have the most restricted language use.\n- Jupyter, Rstudio and Pycharm and the most used IDES.\n- Software Engineers use Visual Studio more than other professions do.","7dac8eca":"### Figure: 7.1 Programming language by profession in 2019\n\nIn this figure we look at the programming languages respondents use on a regular basis. The size (bigger is more) and colour (darker is more) indicate the number of repsondents of a given job (row) type who indicated they use the given programming language (column) on a regular basis. EU respondents (left panel) are represented by blue circles and UK respondents (right panel) are represented by red circles.","41044303":"### Figure 4.3 Coding experience\n\nIn the below figure we display the reported coding experience for respondents in 2019 (left panel) and 2018 (right panel). Colour denotes the location of the respondents, either in the UK or in a EU member state (red and blue respectively). The bars are translucent and overlaid on top of each other to allow for easy comparison of bar heights.","40ca4626":"# 2. Age\n\nIn the following section we investigate the age of the respondents to the survey. We display histograms (Figure 2.1) of the respondents reported ages for each year of the Kaggle survey. The survey clearly attacts a relatively young UK (and EU) audience compared to the [nominal distribution](https:\/\/www.statista.com\/statistics\/281174\/uk-population-by-age\/) of the UK. We also found that a slightly higher proportion of EU respondents report a younger age bracket compared to the UK (note the differences in the bottom panels for persons between 22-39, EU citizens are more likely to respond in all of these categories compared to an UK respondent). UK respondents are more likely to be between the ages of 40 and 49 compared to EU respondents.\n\nThere is a distinct worry that, as well as reducing migration from the UK, Brexit could also cause a large number of young UK professionals to migrate from the UK. There is already evidence of this from a a surge in [citizenship applications](https:\/\/www.theguardian.com\/politics\/2017\/oct\/13\/brexit-vote-creates-surge-in-eu-citizenship-applications) to other EU countries from British persons following the Brexit referendum in 2016. While in the Kaggle survey we observed a drop in the number of UK respondents reporting in the 25-29 age bracket from 2017 to 2019 (compare right and left panels of Figure 2.1) and this could reflect post-brexit referendum migration to the EU of early career professionals, we cannot robustly conclude this because of the different methods of collecting age between 2017 and 2018 onwards.\n\n**Key points:**\n>- The survey attracts a relatively young audience.\n- EU respondents have a slightly higher fraction of reported ages under 30 compared to the UK.\n- UK respondents have a higher fraction of respondents aged 40-49.\n"}}