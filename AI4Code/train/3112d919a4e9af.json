{"cell_type":{"b5e9661e":"code","8304f4d6":"code","c1a389f4":"code","779feb3b":"code","d5909803":"code","01d78f56":"code","c1fc2017":"code","ee538247":"markdown","02ef6ce0":"markdown","ea5fb5fb":"markdown","80a37f75":"markdown","3f12e46e":"markdown"},"source":{"b5e9661e":"#Import Statements\nimport pandas as pd\nimport numpy as np\nimport gc\nimport os\nfrom math import floor, ceil\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.model_selection import GroupKFold\nfrom scipy.stats import spearmanr\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow.keras.backend as K\n\nfrom transformers import BertTokenizer\nfrom transformers import BertConfig\nfrom transformers import TFBertModel","8304f4d6":"#*===================================================Read data and tokenizer==============================================*\n\n#Read tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)\n\nPATH = '..\/input\/google-quest-challenge\/'\n\nBERT_PATH = '..\/input\/bert-base-uncased-huggingface-transformer\/'\ntokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')\n\nMAX_SEQUENCE_LENGTH = 384\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\noutput_categories = list(df_train.columns[11:])\ninput_categories = list(df_train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","c1a389f4":"#*======================================================Preprocessing Functions========================================================*\n\n# These are some functions that will be used to preprocess the raw text data into useable Bert inputs.\n\ndef _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n    \n    def return_id(str1, str2, truncation_strategy, length):\n\n        inputs = tokenizer.encode_plus(str1, str2,\n            add_special_tokens=True,\n            max_length=length,\n            truncation_strategy=truncation_strategy)\n        \n        input_ids =  inputs[\"input_ids\"]\n        input_masks = [1] * len(input_ids)\n        input_segments = inputs[\"token_type_ids\"]\n        padding_length = length - len(input_ids)\n        padding_id = tokenizer.pad_token_id\n        input_ids = input_ids + ([padding_id] * padding_length)\n        input_masks = input_masks + ([0] * padding_length)\n        input_segments = input_segments + ([0] * padding_length)\n        \n        return [input_ids, input_masks, input_segments]\n    \n    input_ids_q, input_masks_q, input_segments_q = return_id(\n        title + ' ' + question, None, 'longest_first', max_sequence_length)\n    \n    input_ids_a, input_masks_a, input_segments_a = return_id(\n        answer, None, 'longest_first', max_sequence_length)\n    \n    return [input_ids_q, input_masks_q, input_segments_q,\n            input_ids_a, input_masks_a, input_segments_a]\n\ndef compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n    input_ids_q, input_masks_q, input_segments_q = [], [], []\n    input_ids_a, input_masks_a, input_segments_a = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n        \n        input_ids_q.append(ids_q)\n        input_masks_q.append(masks_q)\n        input_segments_q.append(segments_q)\n\n        input_ids_a.append(ids_a)\n        input_masks_a.append(masks_a)\n        input_segments_a.append(segments_a)\n        \n    return [np.asarray(input_ids_q, dtype=np.int32), \n            np.asarray(input_masks_q, dtype=np.int32), \n            np.asarray(input_segments_q, dtype=np.int32),\n            np.asarray(input_ids_a, dtype=np.int32), \n            np.asarray(input_masks_a, dtype=np.int32), \n            np.asarray(input_segments_a, dtype=np.int32)]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","779feb3b":"#*============================================Model Creation and Metric Calculation=======================================*\n\ndef compute_spearmanr_ignore_nan(trues, preds):\n    rhos = []\n    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n        rhos.append(spearmanr(tcol, pcol).correlation)\n    return np.nanmean(rhos)\n\n\nq_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\na_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n\nq_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\na_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n\nq_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\na_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n\n\ndef create_model():\n    \n    config = BertConfig() # print(config) to see settings\n    config.output_hidden_states = False # Set to True to obtain hidden states\n    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n    \n    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n    # pretrained model has been downloaded manually and uploaded to kaggle. \n    bert_model = TFBertModel.from_pretrained(\n        BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n    \n    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    \n    x = tf.keras.layers.Concatenate()([q, a])\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn,], outputs=x)\n    \n    return model","d5909803":"#*=====================================Generating Input and Output for Traina & Test=======================================*\n\noutputs = compute_output_arrays(df_train, output_categories)\ninputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n","01d78f56":"#*===============================================Training, Validation and Testing==========================================*\n\ngkf = GroupKFold(n_splits=5).split(X=df_train.question_body, groups=df_train.question_body)\n\nvalid_preds = []\ntest_preds = []\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n    \n    # will actually only do 2 folds (out of 5) to manage < 2h\n    if fold in [0, 2]:\n\n        train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n        train_outputs = outputs[train_idx]\n\n        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n        valid_outputs = outputs[valid_idx]\n        \n        K.clear_session()\n        model = create_model()\n        optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n        model.compile(loss='binary_crossentropy', optimizer=optimizer)\n        model.fit(train_inputs, train_outputs, epochs=3, batch_size=6)\n        # model.save_weights(f'bert-{fold}.h5')\n        valid_preds.append(model.predict(valid_inputs))\n        test_preds.append(model.predict(test_inputs))\n        \n        rho_val = compute_spearmanr_ignore_nan(valid_outputs, valid_preds[-1])\n        print('validation score = ', rho_val)","c1fc2017":"#*===============================================Process and Submit Test Predictions==========================================*\n\ndf_sub.iloc[:, 1:] = np.average(test_preds, axis=0)\n\ndf_sub.to_csv('submission.csv', index=False)","ee538247":"**Observations:**\n\n1) Loops over the folds in gkf and trains each fold for 3 epochs with a learning rate of 3e-5 and batch_size of 6.\n\n2) A simple binary crossentropy is used as the objective loss-function.","02ef6ce0":"**Observations:**\n\nAverage fold predictions, then save as submission.csv","ea5fb5fb":"**Observations**\n\n1) Our train dataset consists of 6079 rows and test dataset consists of 476 rows.\n\n2) We have total 30 output labels and 3 input columns i.e. Question Title, Question Body and Answer.","80a37f75":"**Observations:**\n\n1) _convert_to_transformer_inputs is a generic method to convert our dataset into equivalent input that any model of transformer library is expecting.\n\n2) Question body and Question Title are merged into one and Answer is taken as a separate segment.\n","3f12e46e":"**Observations:**\n\n1) compute_spearmanr_ignore_nan is used to compute the competition metric for the validation set\n\n2) create_model contains the actual architecture that will be used to finetune BERT to our dataset."}}