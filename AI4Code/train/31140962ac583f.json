{"cell_type":{"173797ea":"code","d0f6ef6c":"code","67c3ad67":"code","08ff2a70":"code","4903dbcb":"code","6262e041":"code","e857b1be":"code","d53b7279":"code","60c6af06":"code","4a4f552e":"code","64da1310":"code","bdba7a51":"code","a157acb2":"code","d7144f62":"markdown","d6b048f3":"markdown","9ebc731a":"markdown","7163b3c2":"markdown","f5b1c7d5":"markdown","145bc777":"markdown","7e8c9b53":"markdown","758446d5":"markdown","007b8631":"markdown","a3753eb1":"markdown","6d8a35d1":"markdown","d03e833d":"markdown","18c7a812":"markdown"},"source":{"173797ea":"# Import the required modules for you analysis\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nfrom sklearn import preprocessing # To get MinMax Scaler function\nfrom sklearn.model_selection import train_test_split # required to split the data\nfrom sklearn.feature_selection import SelectKBest # to select best feature\nfrom sklearn.feature_selection import chi2 # for feature Selection\nfrom sklearn.metrics import classification_report # to extract classsification report\nfrom sklearn.metrics import confusion_matrix # to extract confusion matrix\nfrom sklearn.metrics import accuracy_score # to get the scores\nfrom sklearn.linear_model import LogisticRegression # for logistic regression\nfrom sklearn.ensemble import RandomForestClassifier # for random forrest \nfrom sklearn.naive_bayes import GaussianNB # for Naive Bayes\nfrom sklearn.feature_selection import RFE # Recursive Feature Selection\nfrom sklearn.feature_selection import RFECV # Recursive Feature Selection with Cross Validation\nfrom sklearn.decomposition import PCA # To apply PCA\n\n# To plot inline\n%matplotlib inline\n\n# Load the csv to a dataframe\ndf = pd.read_csv('..\/input\/weatherAUS.csv')\n\n# Let us see whats inside..\ndf.head()","d0f6ef6c":"# Drop RISK_MM\ndf = df.drop(columns=['RISK_MM'],axis=1)\ndf.head()","67c3ad67":"# Droping columns\ndf = df.drop(columns=['Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm'],axis=1)\ndf.head()","08ff2a70":"# Check for any null\ndf.isnull().sum()","4903dbcb":"df = df.drop(columns=['Sunshine', 'Evaporation', 'Cloud3pm', 'Cloud9am'],axis=1)\ndf.head()","6262e041":"# substituting 1 and 0 inplace of yes and no\ndf['RainToday'].replace({'No': 0, 'Yes': 1},inplace = True)\ndf['RainTomorrow'].replace({'No': 0, 'Yes': 1},inplace = True)\ndf.head()","e857b1be":"# lets take a look at the values from each column\ndf.describe()","d53b7279":"# Since only less than 1% of following columns are missing values, lets just replace these with the mean.\n# (You can also use the imput pre-processing class to do this)\ndf.MinTemp.fillna(df.MinTemp.mean(),inplace=True)\ndf.MaxTemp.fillna(df.MaxTemp.mean(),inplace=True)\ndf.Rainfall.fillna(df.Rainfall.mean(),inplace=True)\ndf.WindSpeed9am.fillna(df.WindSpeed9am.mean(),inplace=True)\ndf.WindSpeed3pm.fillna(df.WindSpeed3pm.mean(),inplace=True)\ndf.Humidity9am.fillna(df.Humidity9am.mean(),inplace=True)\ndf.Temp9am.fillna(df.Temp9am.mean(),inplace=True)\ndf.Temp3pm.fillna(df.Temp3pm.mean(),inplace=True)\ndf.RainToday.fillna(df.RainToday.mean(),inplace=True)\ndf.Humidity3pm.fillna(df.Humidity3pm.mean(),inplace=True)\n\n# And for columns with close to 5% missing values lets randomly fill them with values close to the mean value but \n# within one standard deviation.\nWindGustSpeed_avg = df['WindGustSpeed'].mean()\nWindGustSpeed_std = df['WindGustSpeed'].std()\nWindGustSpeed_null_count = df['WindGustSpeed'].isnull().sum()\nWindGustSpeed_null_random_list = np.random.randint(WindGustSpeed_avg - WindGustSpeed_std, WindGustSpeed_avg + WindGustSpeed_std, size=WindGustSpeed_null_count)\ndf['WindGustSpeed'][np.isnan(df['WindGustSpeed'])] = WindGustSpeed_null_random_list\n\nPressure9am_avg = df['Pressure9am'].mean()\nPressure9am_std = df['Pressure9am'].std()\nPressure9am_null_count = df['Pressure9am'].isnull().sum()\nPressure9am_null_random_list = np.random.randint(Pressure9am_avg - Pressure9am_std, Pressure9am_avg + Pressure9am_std, size=Pressure9am_null_count)\ndf['Pressure9am'][np.isnan(df['Pressure9am'])] = Pressure9am_null_random_list\n\nPressure3pm_avg = df['Pressure3pm'].mean()\nPressure3pm_std = df['Pressure3pm'].std()\nPressure3pm_null_count = df['Pressure3pm'].isnull().sum()\nPressure3pm_null_random_list = np.random.randint(Pressure3pm_avg - Pressure3pm_std, Pressure3pm_avg + Pressure3pm_std, size=Pressure3pm_null_count)\ndf['Pressure3pm'][np.isnan(df['Pressure3pm'])] = Pressure3pm_null_random_list\n\n\n#replace negative values with 0\ndf.clip(lower=0)\ndf[df < 0] = 0\n","60c6af06":"# From selectkbest find the best score and plot it\nfrom sklearn.feature_selection import SelectKBest, chi2\nX = df.loc[:,df.columns!='RainTomorrow']\ny = df[['RainTomorrow']]\nselector = SelectKBest(chi2, k=3)\nselector.fit(X, y)\nX_new = selector.transform(X)\nscores = selector.scores_\nprint(X.columns[selector.get_support(indices=True)]) #top 3 columns\n\n# Plot the scores. Lets see if we can visualise which are the best?\nplt.bar(range(len(X.columns)), scores)\nplt.xticks(range(len(X.columns)), X.columns, rotation='vertical')\nplt.show()","4a4f552e":"# Plotting the corealation heat map \nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(df.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.show()","64da1310":"# Select the required columns for testing and training\nX = df.loc[:, ['Rainfall', 'Humidity3pm', 'Humidity9am', 'WindGustSpeed', 'Temp3pm', 'MaxTemp']].shift(-1).iloc[:-1].values\ny = df.iloc[:-1, -1:].values.astype('int')\n\n# Logistic Regression \n# Split the data to appropriate testing sample size\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\nmodel_lr = LogisticRegression(random_state=0)\nmodel_lr.fit(X_train,y_train)\nprediction_lr = model_lr.predict(X_test)\nscore = accuracy_score(y_test,prediction_lr)\nprint('Accuracy - Logistic Regression:',score)\n\n# Making the Confusion Matrix\ncm = confusion_matrix(y_test, prediction_lr)\n\n# Ploting the Confusion Matrix\nf, ax = plt.subplots(figsize = (3,3))\nsns.heatmap(cm,annot=True,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predictions\")\nplt.ylabel(\"Test Values\")\nplt.show()","bdba7a51":"# Splitting the data\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n\n# Random Forrest\nmodel_rf = RandomForestClassifier(n_estimators= 25, max_depth= None,max_features = 0.4,random_state= 11 )\n# Fitting the data\nmodel_rf.fit(X_train, y_train)\n# Making a prediction\nprediction_rf =model_rf.predict(X_test)\nscore = accuracy_score(y_test,prediction_rf)\nprint('Accuracy - Random Forrest:',score)\nprint(classification_report(y_test, prediction_rf))\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, prediction_rf)\n\n# Plotting the data\nf, ax = plt.subplots(figsize = (3,3))\nsns.heatmap(cm,annot=True,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predictions\")\nplt.ylabel(\"Test Values\")\nplt.show()","a157acb2":"# Splitting the data    \nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n\n# Gaussian Navie Bayes\nmodel_gNB = GaussianNB()\nmodel_gNB.fit(X_train,y_train)\nprediction_gNB = model_gNB.predict(X_test)\nscore = accuracy_score(y_test,prediction_gNB)\nprint('Accuracy - GaussianNB:',score)\n\n# Making the Confusion Matrix\ncm = confusion_matrix(y_test, prediction_gNB)\n\n# Plotting the matrix\nf, ax = plt.subplots(figsize = (3,3))\nsns.heatmap(cm,annot=True,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predictions\")\nplt.ylabel(\"Test Values\")\nplt.show()\n","d7144f62":"Now, that we have a glimspe of the dataset lets drop the **RISK_MM** column as recommended by the author of the dataset. Using **RISK_MM** in our analysis might leak unwanted noise to our data..\n","d6b048f3":"# For Beginners - Lets take a look at that rain dataset","9ebc731a":"We have a few catagoerical columns, lets convert and assign integer values --> Yes will be 1 and No will be 0","7163b3c2":"#### Random Forrest","f5b1c7d5":"### Finding the best coulmns that conrtibute to rain tomorrow : 'Feature Selection'\n\nFrom sklearn we can use the SelectKBest to find the best features available to use","145bc777":"Any exploratory analysis should start with a clear understanding about the goals that you wish to achieve. So lets define our key objectives..\n\n-  Clean up the data set\n-  Feature selection\n-  Establish various models to predict rain tomorrow\n-  Visualize the best models and results\n\n### Lets check what we have and clean the data set:  \" Data Pre-processing\" ","7e8c9b53":"The most important piece of any analysis is handling missing data. When done correctly it will contribute in training a much more accurate model.","758446d5":"Also drop columns with location infromation and non-decisive data which contributes very little to the analysis.","007b8631":"#### Naive Bayes","a3753eb1":"### Training through varrious models : \"Model Selection\"\n\nAt this point since we have an idea of the features that contribute to our target value lets train and fit our data through various models.\n\n#### Logistic Regression","6d8a35d1":"Lets take a look at the columns with missing info","d03e833d":"Its quite clear that columns like Sunshine, Evaporation, Cloud3pm and Cloud9am has considerable amount missing information. It wont be a good idea to use these columns in our analysis. Lets drop them ","18c7a812":"We can also visualize the correlation between the columns. Based on the resulting heat map, we can also try to find the varriables which might have a direct or partial relation with our target varriable."}}