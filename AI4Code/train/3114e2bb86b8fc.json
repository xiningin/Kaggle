{"cell_type":{"c50915b4":"code","363199f7":"code","571426b0":"code","ea80baf3":"code","5ecdc3c5":"code","cc301e75":"code","2786d859":"code","a05d6319":"code","2a2404f9":"code","57fee0a9":"code","775ed598":"code","ada49f2f":"code","e05e2def":"code","80959c74":"code","1f6b36f4":"code","6b9711f9":"code","cf38238a":"code","e99037f6":"code","621bb4a9":"code","74d194d1":"code","47656b25":"code","edd10cd5":"code","72ec111c":"code","f3563a93":"code","77f8ab53":"code","26f321d0":"code","4d73cfec":"code","8f4bab2f":"code","69f3bf14":"code","20833c8b":"code","87f0a0c6":"code","6aae1428":"markdown","83037db7":"markdown","cb076767":"markdown","f2f0d7a3":"markdown","f567b6f6":"markdown","5f317368":"markdown","eb9db3e4":"markdown","2eb98213":"markdown","5b4f42a1":"markdown","70f0382e":"markdown","b620754e":"markdown","54961873":"markdown","9aa21957":"markdown","2bfe348e":"markdown","c60b9ec9":"markdown","60ee7e8f":"markdown","360c77d2":"markdown","d73e85db":"markdown"},"source":{"c50915b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import nn, optim\nfrom torch.autograd import Variable\nfrom sklearn.decomposition import PCA\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\n\n# Any results you write to the current directory are saved as output.","363199f7":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","571426b0":"DATA_PATH = '\/kaggle\/input\/wineuci\/Wine.csv'\npd.read_csv(DATA_PATH, sep=',', header=None, names=['Wine', 'Alcohol','Malic.acid','Ash','Acl',\n                                                    'Mg', 'Phenols', 'Flavanoids','Nonflavanoid.phenols',\n                                                    'Proanth','Color.int','Hue', 'OD','Proline']).head()","ea80baf3":"def load_data(path):\n    # read in from csv\n    df = pd.read_csv(DATA_PATH, sep=',', header=None, names=['Wine', 'Alcohol','Malic.acid','Ash','Acl',\n                                                    'Mg', 'Phenols', 'Flavanoids','Nonflavanoid.phenols',\n                                                    'Proanth','Color.int','Hue', 'OD','Proline'])\n    # replace nan with -99\n    df = df.fillna(-99)\n    df_base = df.iloc[:, 1:]\n    # get wine Label\n    df_wine = df.iloc[:,0].values\n    x = df_base.values.reshape(-1, df_base.shape[1]).astype('float32')\n    # stadardize values\n    standardizer = preprocessing.StandardScaler()\n    x = standardizer.fit_transform(x)    \n    return x, standardizer, df_wine","5ecdc3c5":"def numpyToTensor(x):\n    x_train = torch.from_numpy(x).to(device)\n    return x_train","cc301e75":"x_pca, standardizer, df_wine = load_data(DATA_PATH)","2786d859":"x_pca.shape","a05d6319":"pca = PCA(n_components=3)\nprincipalComponents = pca.fit_transform(x_pca)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2', 'principal component 3'])","2a2404f9":"finalDf = pd.concat([principalDf, pd.DataFrame(df_wine, columns = ['wine'])], axis = 1)\nfinalDf.head()","57fee0a9":"from torch.utils.data import Dataset, DataLoader\nclass DataBuilder(Dataset):\n    def __init__(self, path):\n        self.x, self.standardizer, self.wine = load_data(DATA_PATH)\n        self.x = numpyToTensor(self.x)\n        self.len=self.x.shape[0]\n    def __getitem__(self,index):      \n        return self.x[index]\n    def __len__(self):\n        return self.len","775ed598":"data_set=DataBuilder(DATA_PATH)\ntrainloader=DataLoader(dataset=data_set,batch_size=1024)","ada49f2f":"type(trainloader.dataset.x)","e05e2def":"data_set.x","80959c74":"class Autoencoder(nn.Module):\n    def __init__(self,D_in,H=50,H2=12,latent_dim=3):\n        \n        #Encoder\n        super(Autoencoder,self).__init__()\n        self.linear1=nn.Linear(D_in,H)\n        self.lin_bn1 = nn.BatchNorm1d(num_features=H)\n        self.linear2=nn.Linear(H,H2)\n        self.lin_bn2 = nn.BatchNorm1d(num_features=H2)\n        self.linear3=nn.Linear(H2,H2)\n        self.lin_bn3 = nn.BatchNorm1d(num_features=H2)\n        \n#         # Latent vectors mu and sigma\n        self.fc1 = nn.Linear(H2, latent_dim)\n        self.bn1 = nn.BatchNorm1d(num_features=latent_dim)\n        self.fc21 = nn.Linear(latent_dim, latent_dim)\n        self.fc22 = nn.Linear(latent_dim, latent_dim)\n\n#         # Sampling vector\n        self.fc3 = nn.Linear(latent_dim, latent_dim)\n        self.fc_bn3 = nn.BatchNorm1d(latent_dim)\n        self.fc4 = nn.Linear(latent_dim, H2)\n        self.fc_bn4 = nn.BatchNorm1d(H2)\n        \n#         # Decoder\n        self.linear4=nn.Linear(H2,H2)\n        self.lin_bn4 = nn.BatchNorm1d(num_features=H2)\n        self.linear5=nn.Linear(H2,H)\n        self.lin_bn5 = nn.BatchNorm1d(num_features=H)\n        self.linear6=nn.Linear(H,D_in)\n        self.lin_bn6 = nn.BatchNorm1d(num_features=D_in)\n        \n        self.relu = nn.ReLU()\n        \n    def encode(self, x):\n        lin1 = self.relu(self.lin_bn1(self.linear1(x)))\n        lin2 = self.relu(self.lin_bn2(self.linear2(lin1)))\n        lin3 = self.relu(self.lin_bn3(self.linear3(lin2)))\n\n        fc1 = F.relu(self.bn1(self.fc1(lin3)))\n\n        r1 = self.fc21(fc1)\n        r2 = self.fc22(fc1)\n        \n        return r1, r2\n    \n    def reparameterize(self, mu, logvar):\n        if self.training:\n            std = logvar.mul(0.5).exp_()\n            eps = Variable(std.data.new(std.size()).normal_())\n            return eps.mul(std).add_(mu)\n        else:\n            return mu\n        \n    def decode(self, z):\n        fc3 = self.relu(self.fc_bn3(self.fc3(z)))\n        fc4 = self.relu(self.fc_bn4(self.fc4(fc3)))\n\n        lin4 = self.relu(self.lin_bn4(self.linear4(fc4)))\n        lin5 = self.relu(self.lin_bn5(self.linear5(lin4)))\n        return self.lin_bn6(self.linear6(lin5))\n\n\n        \n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        # self.decode(z) ist sp\u00e4ter recon_batch, mu ist mu und logvar ist logvar\n        return self.decode(z), mu, logvar","1f6b36f4":"class customLoss(nn.Module):\n    def __init__(self):\n        super(customLoss, self).__init__()\n        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n    \n    # x_recon ist der im forward im Model erstellte recon_batch, x ist der originale x Batch, mu ist mu und logvar ist logvar \n    def forward(self, x_recon, x, mu, logvar):\n        loss_MSE = self.mse_loss(x_recon, x)\n        loss_KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n        return loss_MSE + loss_KLD","6b9711f9":"# takes in a module and applies the specified weight initialization\ndef weights_init_uniform_rule(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # get the number of the inputs\n        n = m.in_features\n        y = 1.0\/np.sqrt(n)\n        m.weight.data.uniform_(-y, y)\n        m.bias.data.fill_(0)","cf38238a":"D_in = data_set.x.shape[1]\nH = 50\nH2 = 12\nmodel = Autoencoder(D_in, H, H2).to(device)\nmodel.apply(weights_init_uniform_rule)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)","e99037f6":"loss_mse = customLoss()","621bb4a9":"epochs = 1500\nlog_interval = 50\nval_losses = []\ntrain_losses = []","74d194d1":"def train(epoch):\n    model.train()\n    train_loss = 0\n    for batch_idx, data in enumerate(trainloader):\n        data = data.to(device)\n        optimizer.zero_grad()\n        recon_batch, mu, logvar = model(data)\n        loss = loss_mse(recon_batch, data, mu, logvar)\n        loss.backward()\n        train_loss += loss.item()\n        optimizer.step()\n#        if batch_idx % log_interval == 0:\n#            print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n#                epoch, batch_idx * len(data), len(trainloader.dataset),\n#                       100. * batch_idx \/ len(trainloader),\n#                       loss.item() \/ len(data)))\n    if epoch % 200 == 0:        \n        print('====> Epoch: {} Average loss: {:.4f}'.format(\n            epoch, train_loss \/ len(trainloader.dataset)))\n        train_losses.append(train_loss \/ len(trainloader.dataset))","47656b25":"for epoch in range(1, epochs + 1):\n    train(epoch)","edd10cd5":"standardizer = trainloader.dataset.standardizer","72ec111c":"model.eval()\ntest_loss = 0\n# no_grad() bedeutet wir nehmen die vorher berechneten Gewichte und erneuern sie nicht\nwith torch.no_grad():\n    for i, data in enumerate(trainloader):\n        data = data.to(device)\n        recon_batch, mu, logvar = model(data)","f3563a93":"standardizer.inverse_transform(recon_batch[65].cpu().numpy())","77f8ab53":"standardizer.inverse_transform(data[65].cpu().numpy())","26f321d0":"mu_output = []\nlogvar_output = []\n\nwith torch.no_grad():\n    for i, (data) in enumerate(trainloader):\n            data = data.to(device)\n            optimizer.zero_grad()\n            recon_batch, mu, logvar = model(data)\n\n            \n            mu_tensor = mu   \n            mu_output.append(mu_tensor)\n            mu_result = torch.cat(mu_output, dim=0)\n\n            logvar_tensor = logvar   \n            logvar_output.append(logvar_tensor)\n            logvar_result = torch.cat(logvar_output, dim=0)","4d73cfec":"mu_result.shape","8f4bab2f":"mu_result[1:5,:]","69f3bf14":"from mpl_toolkits import mplot3d\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt","20833c8b":"\n\nax = plt.axes(projection='3d')\n\n\n# Data for three-dimensional scattered points\nwinetype = finalDf.iloc[:,3].values\nzdata = finalDf.iloc[:,0].values\nxdata = finalDf.iloc[:,1].values\nydata = finalDf.iloc[:,2].values\nax.scatter3D(xdata, ydata, zdata, c=winetype);\n\n","87f0a0c6":"ax = plt.axes(projection='3d')\n\n\n# Data for three-dimensional scattered points\nwinetype = data_set.wine\nzdata = mu_result[:,0].cpu().numpy()\nxdata = mu_result[:,1].cpu().numpy()\nydata = mu_result[:,2].cpu().numpy()\nax.scatter3D(xdata, ydata, zdata, c=winetype);","6aae1428":"# PyTorch Autoencoder ","83037db7":"Usually VAE are used for Image processing and creating (like GANS Models). This is explained here: https:\/\/towardsdatascience.com\/understanding-variational-autoencoders-vaes-f70510919f73\nHowever, we can adapt this approach to use it on tabular data. This way the Autoencoder helps us reducing dimensionality of data and also reduce noise.","cb076767":"## Train Model ","f2f0d7a3":"## Get a quick view of the data ","f567b6f6":"These are the embeddings calculated by our VAE.","5f317368":"# Plot Embeddings of PCA ","eb9db3e4":"## Imports ","2eb98213":"## Evaluate ","5b4f42a1":"# Variational Autoencoder with PyTorch v PCA ","70f0382e":"## Get Embeddings ","b620754e":"# Plot Embeddings from VAE ","54961873":"## Build Data Loader ","9aa21957":"The recon_batch is the reconstructed data. So after putting the data through the encoder we compress the dimensions to 3 (embeddings). With these 3 representaions\/embeddings of each row the decoder tries to convert it back to the original data. So basically, this recontructed row feeds itself from only three numbers.","2bfe348e":"## Define Functions ","c60b9ec9":"If there are any questions, feel free to ask.","60ee7e8f":"In this notebook I want to show two types of dimensionality reduction for tabular data: PCA and Autoencoders.\n\nI use the wine dataset to show how Variational Autoencoder (VAE) with PyTorch on tabular data works and compare it to the classic PCA approach. I use the PCA\/VAE to reduce the dimensionality of dataset, in this case don to 3 Variables (embeddings). I then plot the embeddings in a 3D graph to show how VAE is similar to a PCA but works in a non-linear way.\n","360c77d2":"## Build Model and train it","d73e85db":"# Create PCA with 3 dimensions"}}