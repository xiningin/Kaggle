{"cell_type":{"6d6dd055":"code","46d2e99e":"code","b04706cb":"code","158f2a1e":"code","cf3049cd":"code","d726b0ae":"code","34a71ee3":"code","b6926f9e":"code","c73a9980":"code","b0603bf5":"code","cc837514":"code","210bf79c":"code","a66ed783":"code","1d760d61":"code","feeeb723":"code","850900fb":"code","8d2ecd71":"code","c2d5ef47":"code","2ca25bff":"code","1e7e3190":"code","9503ab1f":"code","311e6e91":"code","c9a680f6":"code","89dbfd74":"code","b9742223":"code","39c6fb1d":"code","cbf0f148":"code","9055eef7":"code","64cdae54":"code","44dfd990":"code","3e867742":"code","a797687c":"code","e3987df4":"code","867b1c82":"code","979fc480":"code","1c765d74":"code","f6b05954":"code","cbf81208":"code","243c09d7":"code","c289c227":"code","ad506167":"code","48dab859":"code","984a9af2":"code","a7fd6fad":"code","eea4d842":"code","24a8598c":"code","89528ea8":"code","49d733c4":"code","b13aeb54":"code","e4342877":"code","fea7eea3":"code","0b23d56d":"code","46e03786":"code","617415a8":"code","096b475e":"code","2f187d5d":"code","a095c58b":"code","d1190b75":"code","51b63f63":"code","dae87e71":"code","5820e919":"code","eede7d0f":"code","3720061f":"code","56141e0b":"code","959a6236":"code","013f5130":"code","64da7ef7":"code","6c0d0919":"code","b1a76cf4":"code","b51a16f3":"code","4afb93a7":"code","1e535235":"code","16874333":"code","d3ea350f":"code","0b6941bd":"code","a1d8943a":"code","bb6f5442":"code","90ee47ce":"code","cf3d3840":"code","8a2efe8c":"code","00ebc774":"code","48a7c487":"code","feaf899c":"code","28c33534":"code","075afa8a":"code","babdd24e":"code","1599c8c6":"code","e0bd3ffb":"code","72f8f623":"code","3210b574":"code","7c30e7d9":"code","6fdf7bc3":"code","251bce7f":"code","ebbf9ce0":"code","54177d96":"code","5b042164":"code","d7e959c6":"code","c7009b72":"code","21a91a4f":"code","385c23fc":"code","e7c6eaa0":"code","eaa8c13e":"code","7f306b61":"code","4ad57154":"code","6099cb03":"markdown","3426d72f":"markdown","a9c4e9c8":"markdown","75c86541":"markdown","79bb41be":"markdown","a7d9cbc1":"markdown","45096013":"markdown","63de660e":"markdown","f61dbbb4":"markdown","b1648a33":"markdown","a408cb59":"markdown","1d252f41":"markdown","3051b290":"markdown","95768989":"markdown","1751fda7":"markdown","c5703f20":"markdown","92bfedc9":"markdown","6cb1b036":"markdown","fb6d34cf":"markdown","0f8a8eda":"markdown","0d20c693":"markdown","e6953272":"markdown","1c734603":"markdown","dc9972b9":"markdown","b68df12b":"markdown","6f0a5313":"markdown","d425db75":"markdown","d754524b":"markdown","3165fe73":"markdown","285bb435":"markdown","38026087":"markdown","ba728063":"markdown","830f8797":"markdown"},"source":{"6d6dd055":"import numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns","46d2e99e":"df = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","b04706cb":"df.head()","158f2a1e":"df.info()","cf3049cd":"df.describe()","d726b0ae":"df.isnull().sum().sum()","34a71ee3":"# There are some missing values that we need to take care for\n# Let's start by dropping the id column because it doesn't give any info related to strokes\ndf.drop('id', axis=1, inplace=True)","b6926f9e":"df.head()","c73a9980":"df['stroke'].sum()","b0603bf5":"# So we mostly have cases that didn't result in a stroke","cc837514":"df['bmi'].hist(bins=40)","210bf79c":"# Most people have bmi ranging from 15 to 40\n# I think that the best way to fill bmi is by using the average value of bmi for every age and gender\nmean_bmi = df.groupby(['age', 'gender']).mean()['bmi']\nmean_bmi.head()","a66ed783":"import math\ndef fill_bmi(df, mean_bmi):\n    if math.isnan(df['bmi']): \n        return mean_bmi[df['age']][df['gender']]\n    else:\n        return df['bmi']\n    \ndf['bmi'] = df.apply(fill_bmi, axis=1, args=(mean_bmi, ))\ndf.isnull().sum().sum()","1d760d61":"# There is one missing value. We need to check why we still have one missing value\ndf.info()","feeeb723":"df[df['bmi'].isnull()]","850900fb":"print(str(mean_bmi[0.48]['Male']) + '\\n')\nprint(mean_bmi[0.48])","8d2ecd71":"# We can see that this male is the only male at a given age so the mean in his age group is equal to Nan\n# We will manually assign the value for this male by using female bmi mean for this age group\ndf.loc[2030, 'bmi'] = mean_bmi[0.48]['Female']","c2d5ef47":"df.isnull().sum().sum()","2ca25bff":"df.info()","1e7e3190":"df['bmi'].hist(bins=40)","9503ab1f":"df.head()","311e6e91":"sns.pairplot(df)","c9a680f6":"sns.heatmap(df.corr())","89dbfd74":"df['gender'].value_counts()","b9742223":"# Time to verify what this other gender means\ndf[df['gender'] == 'Other']","39c6fb1d":"# The best thing we can do here is to get drop this row\ndf.drop([3116], inplace=True)\ndf['gender'].value_counts()","cbf0f148":"sns.set(font_scale=1)\nplt.figure(figsize=(12, 6))\nax = sns.countplot(data=df, x='stroke', hue='gender')\nplt.title('Distribution of strokes based on gender')\nplt.xlabel('Stroke')\nplt.ylabel('How many people had a stroke')\nfor column in ax.patches:\n    ax.annotate(column.get_height(), (column.get_x() + 0.15, column.get_height() + 50))\nplt.ylim(0, 3100)\nplt.show()","9055eef7":"data = df.groupby(['gender', 'stroke']).count()['age']\nprint('Percentage of female that had a stroke: ', (data['Female'][1]\/(data['Female'][0] + data['Female'][1])) * 100)\nprint('Percentage of male that had a stroke: ', (data['Male'][1]\/(data['Male'][0] + data['Male'][1])) * 100)","64cdae54":"df['age'].hist(bins=40)","44dfd990":"number_of_strokes = df.groupby('age').sum()\nwhole_population = df.groupby('age').count()\npercentage_had_a_stroke = (number_of_strokes['stroke']\/whole_population['stroke']) * 100\n\nplt.figure(figsize=(12, 8))\nplt.plot(percentage_had_a_stroke.index, percentage_had_a_stroke.values)\nplt.title('Distribution of strokes based on age', fontsize=25)\nplt.xlabel('Age', fontsize=15)\nplt.ylabel('Percentage of people that had a stroke', fontsize=15)\n\nplt.show()","3e867742":"def get_ilnesses_of_given_patient(df):\n    if df['heart_disease'] == 1 and df['hypertension'] == 1:\n        return 'Heart disease and hypertension'\n    elif df['heart_disease'] == 1:\n        return 'Heart disease'\n    elif df['hypertension'] == 1:\n        return 'Hypertension'\n    else:\n        return 'No ilness'\n\nsns.set(font_scale=2)\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(18, 24))\nax1.set_ylim(0, 5000)\nax2.set_ylim(0, 5000)\nax3.set_ylim(0, 5000)\nfig.suptitle('Distribution of strokes based on other ilnesses', fontsize=36)\n\nsns.countplot(data=df, x='hypertension', hue='stroke', ax=ax1)\nsns.countplot(data=df, x='heart_disease', hue='stroke', ax=ax2)\n\ndata = df[['heart_disease', 'hypertension', 'stroke']] \ndata = data.assign(ilness = data.apply(get_ilnesses_of_given_patient, axis=1))\nsns.countplot(data=data, x='ilness', hue='stroke', ax=ax3)\n\nfor column in ax1.patches:\n    ax1.annotate(column.get_height(), (column.get_x() + 0.15, column.get_height() + 50))\n    \nfor column in ax2.patches:\n    ax2.annotate(column.get_height(), (column.get_x() + 0.15, column.get_height() + 50))\n    \nfor column in ax3.patches:\n    ax3.annotate(column.get_height(), (column.get_x() + 0.15, column.get_height() + 50))\n\nfig.tight_layout()\nfig.subplots_adjust(top=0.94)\nplt.show()","a797687c":"# Reseting the font scale for feature\nsns.set(font_scale=1)","e3987df4":"df['ever_married'].value_counts()","867b1c82":"df.groupby('ever_married').describe()['age']","979fc480":"# We can see that this data also include kids that cannot get married\n# To see whether being married is correlated to strokes we will analyse this data but with relation to age\nplt.figure(figsize=(12, 8))\nsns.lineplot(data=df, x='age', y='stroke', hue='ever_married')\nplt.ylabel('Chance of having a stroke')","1c765d74":"df['work_type'].value_counts()","f6b05954":"plt.figure(figsize=(12, 6))\nsns.countplot(data=df, x='work_type', hue='stroke')","cbf81208":"df.groupby('work_type').sum()['stroke']","243c09d7":"df[df['stroke'] == 0].groupby('work_type').count()['gender']","c289c227":"(df.groupby('work_type').sum()['stroke'] \/ df[df['stroke'] == 0].groupby('work_type').count()['gender']) * 100","ad506167":"df['Residence_type'].value_counts()","48dab859":"# It's good that the amount of people in residence_type is well balanced.\nsns.countplot(data=df, x='Residence_type', hue='stroke')","984a9af2":"df['avg_glucose_level'].hist(bins=40)","a7fd6fad":"df['bmi'].hist(bins=40)","eea4d842":"df.groupby('stroke').mean()","24a8598c":"plt.figure(figsize=(12, 6))\nax = sns.scatterplot(data=df[df['stroke'] == 0], x='avg_glucose_level', y='bmi', alpha=0.3, label='No Stroke')\nsns.scatterplot(data=df[df['stroke'] == 1], x='avg_glucose_level', y='bmi', alpha=1, ax=ax, label='Stroke')\nplt.show()","89528ea8":"df['smoking_status'].value_counts()","49d733c4":"# Quick check why there are many Unknown features in smoking_status\ndf[df['smoking_status'] == 'Unknown'].describe()","b13aeb54":"# So Unknown is just a mix of people that for which we don't have any knowledge about their smoking status\nsns.countplot(data=df, x='smoking_status', hue='stroke')","e4342877":"(df.groupby('smoking_status').sum()['stroke'] \/ df[df['stroke'] == 0].groupby('smoking_status').count()['gender']) * 100","fea7eea3":"df.head()","0b23d56d":"# Time to create some dummy variables\ndata = pd.get_dummies(df['gender'], drop_first=True)\ndf = pd.concat([df, data], axis=1)\n\ndata = pd.get_dummies(df['ever_married'], drop_first=True)\ndf = pd.concat([df, data], axis=1)\n\ndata = pd.get_dummies(df['work_type'], drop_first=True)\ndf = pd.concat([df, data], axis=1)\n\ndata = pd.get_dummies(df['Residence_type'], drop_first=True)\ndf = pd.concat([df, data], axis=1)\n\ndata = pd.get_dummies(df['smoking_status'], drop_first=True)\ndf = pd.concat([df, data], axis=1)\n\ndf.drop(['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'], axis=1, inplace=True)","46e03786":"df.head()","617415a8":"# Time to do some feature scaling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nX = df.drop('stroke', axis=1)\ny = df['stroke']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nnorm = MinMaxScaler()\nnorm.fit(X_train)\nX_train = norm.transform(X_train)\nX_test = norm.transform(X_test)","096b475e":"X_train","2f187d5d":"from sklearn.metrics import confusion_matrix, classification_report, f1_score","a095c58b":"from sklearn.linear_model import LogisticRegression","d1190b75":"lg = LogisticRegression()\nlg.fit(X_train, y_train)\npredictions = lg.predict(X_test)","51b63f63":"predictions","dae87e71":"np.unique(predictions)","5820e919":"print(confusion_matrix(y_test, predictions))\nprint()\nprint(classification_report(y_test, predictions))\nprint()\nprint('F1 score: ', f1_score(y_test, predictions))","eede7d0f":"from sklearn.tree import DecisionTreeClassifier","3720061f":"dt = DecisionTreeClassifier(random_state=101)\ndt.fit(X_train, y_train)\npredictions = dt.predict(X_test)","56141e0b":"predictions","959a6236":"np.unique(predictions)","013f5130":"print(confusion_matrix(y_test, predictions))\nprint()\nprint(classification_report(y_test, predictions))\nprint()\nprint('F1 score: ', f1_score(y_test, predictions))","64da7ef7":"from sklearn.ensemble import RandomForestClassifier","6c0d0919":"rt = RandomForestClassifier(random_state=101)\nrt.fit(X_train, y_train)\npredictions = rt.predict(X_test)","b1a76cf4":"predictions","b51a16f3":"np.unique(predictions)","4afb93a7":"print(confusion_matrix(y_test, predictions))\nprint()\nprint(classification_report(y_test, predictions))\nprint()\nprint('F1 score: ', f1_score(y_test, predictions))","1e535235":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","16874333":"model = Sequential()\nmodel.add(Dense(12, input_dim=len(df.columns)-1, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","d3ea350f":"model.fit(X_train, y_train, epochs=150, batch_size=10)","0b6941bd":"predictions = model.predict(X_test).round()","a1d8943a":"predictions","bb6f5442":"np.unique(predictions)","90ee47ce":"print(confusion_matrix(y_test, predictions))\nprint()\nprint(classification_report(y_test, predictions))\nprint()\nprint('F1 score: ', f1_score(y_test, predictions))","cf3d3840":"predictions = dt.predict(X_train)\npredictions","8a2efe8c":"np.unique(predictions)","00ebc774":"print(confusion_matrix(y_train, predictions))","48a7c487":"print(confusion_matrix(y_train, predictions))\nprint()\nprint(classification_report(y_train, predictions))\nprint()\nprint('F1 score: ', f1_score(y_train, predictions))","feaf899c":"dt = DecisionTreeClassifier(splitter='random', random_state=101)\ndt.fit(X_train, y_train)\npredictions = dt.predict(X_test)\nprint(confusion_matrix(y_test, predictions))\nprint()\nprint(classification_report(y_test, predictions))\nprint('F1 score: ', f1_score(y_test, predictions))","28c33534":"predictions = dt.predict(X_train)\nprint(confusion_matrix(y_train, predictions))\nprint()\nprint(classification_report(y_train, predictions))\nprint()\nprint('F1 score: ', f1_score(y_train, predictions))","075afa8a":"# Time to set max_depth to the optimal value\ndef max_depths_accuracy():\n    max_depth_train = {}\n    max_depth_test = {}\n    for i in range(1, 100):\n        dt = DecisionTreeClassifier(splitter='random', max_depth=i, random_state=101)\n        dt.fit(X_train, y_train)\n        \n        predictions = dt.predict(X_train)\n        f1 = f1_score(y_train, predictions)\n        max_depth_train[i] = f1\n        \n        predictions = dt.predict(X_test)\n        f1 = f1_score(y_test, predictions)\n        max_depth_test[i] = f1\n    \n    return max_depth_train, max_depth_test\n    \nmax_depth_train, max_depth_test = max_depths_accuracy()","babdd24e":"plt.figure(figsize=(12, 6))\nsns.lineplot(x=max_depth_train.keys(), y=max_depth_train.values())","1599c8c6":"plt.figure(figsize=(12, 6))\nsns.lineplot(x=max_depth_test.keys(), y=max_depth_test.values())","e0bd3ffb":"max_depth_train","72f8f623":"max_depth_test","3210b574":"dt = DecisionTreeClassifier(splitter='random', max_depth=22, random_state=101)\ndt.fit(X_train, y_train)\npredictions = dt.predict(X_test)\nprint(confusion_matrix(y_test, predictions))\nprint()\nprint(classification_report(y_test, predictions))\nprint('F1 score: ', f1_score(y_test, predictions))","7c30e7d9":"# Time to set min_samples_split to the optimal value\ndef min_samples_split_accuracy():\n    min_samples_split_train = {}\n    min_samples_split_test = {}\n    for i in range(2, 50):\n        dt = DecisionTreeClassifier(splitter='random', max_depth=22, min_samples_split=i, random_state=101)\n        dt.fit(X_train, y_train)\n        \n        predictions = dt.predict(X_train)\n        f1 = f1_score(y_train, predictions)\n        min_samples_split_train[i] = f1\n        \n        predictions = dt.predict(X_test)\n        f1 = f1_score(y_test, predictions)\n        min_samples_split_test[i] = f1\n    \n    return min_samples_split_train, min_samples_split_test\n\nmin_samples_split_train, min_samples_split_test = min_samples_split_accuracy()","6fdf7bc3":"plt.figure(figsize=(12, 6))\nsns.lineplot(x=min_samples_split_train.keys(), y=min_samples_split_train.values())","251bce7f":"plt.figure(figsize=(12, 6))\nsns.lineplot(x=min_samples_split_test.keys(), y=min_samples_split_test.values())","ebbf9ce0":"min_samples_split_train","54177d96":"min_samples_split_test","5b042164":"# Maybe min_samples_leaf will improve the algorithm\ndef min_samples_leaf_accuracy():\n    min_samples_leaf_train = {}\n    min_samples_leaf_test = {}\n    for i in range(1, 30):\n        dt = DecisionTreeClassifier(splitter='random', max_depth=22, min_samples_split=2, min_samples_leaf=i, random_state=101)\n        dt.fit(X_train, y_train)\n        \n        predictions = dt.predict(X_train)\n        f1 = f1_score(y_train, predictions)\n        min_samples_leaf_train[i] = f1\n        \n        predictions = dt.predict(X_test)\n        f1 = f1_score(y_test, predictions)\n        min_samples_leaf_test[i] = f1\n    \n    return min_samples_leaf_train, min_samples_leaf_test\n\nmin_samples_leaf_train, min_samples_leaf_test = min_samples_leaf_accuracy()","d7e959c6":"plt.figure(figsize=(12, 6))\nsns.lineplot(x=min_samples_leaf_train.keys(), y=min_samples_leaf_train.values())","c7009b72":"plt.figure(figsize=(12, 6))\nsns.lineplot(x=min_samples_leaf_test.keys(), y=min_samples_leaf_test.values())","21a91a4f":"min_samples_leaf_train","385c23fc":"min_samples_leaf_test","e7c6eaa0":"# Maybe criterion will change something\ndt = DecisionTreeClassifier(criterion='entropy', splitter='random',  max_depth=22, min_samples_split=2, \n                            min_samples_leaf=1, random_state=101)\ndt.fit(X_train, y_train)\npredictions = dt.predict(X_test)\nprint(confusion_matrix(y_test, predictions))\nprint()\nprint(classification_report(y_test, predictions))\nprint('F1 score: ', f1_score(y_test, predictions))","eaa8c13e":"# We stay at gini criterion\ndt = DecisionTreeClassifier(criterion='gini', splitter='random',  max_depth=22, min_samples_split=2, \n                            min_samples_leaf=1, random_state=101)\ndt.fit(X_train, y_train)\npredictions = dt.predict(X_test)\nprint(confusion_matrix(y_test, predictions))\nprint()\nprint(classification_report(y_test, predictions))\nprint('F1 score: ', f1_score(y_test, predictions))","7f306b61":"dt = DecisionTreeClassifier(criterion='gini', splitter='random',  max_depth=22, min_samples_split=2, \n                                min_samples_leaf=1, random_state=101)\ndt.fit(X_train, y_train)\npredictions = dt.predict(X_test)","4ad57154":"print(confusion_matrix(y_test, predictions))\nprint()\nprint(classification_report(y_test, predictions))\nprint('F1 Score: ', f1_score(y_test, predictions))","6099cb03":"# Improving machine learning algorithm","3426d72f":"# Preparing data for training","a9c4e9c8":"Neural networks did a little bit better job than random forest but still not good enough ","75c86541":"We can see that logistic regression always predicted that a given person didn't have a stroke.\nSo it's a terrible machine learning algorithm for this data","79bb41be":"Again no improvement","a7d9cbc1":"We can see that children almost never have a stroke (the same goes for never worked but this may be because we have a very low amount of people in this data). We see that self-employed people are most likely to have a stroke compared to other work_types","45096013":"We can see that our data got scaled properly","63de660e":"# Our  goal is to predict whether a given person had a stroke or not\n## We can see from the description of data that we have a lot of cases when a given person didn't have a stroke \n### So our main goal will be to build machine learning algorithm that will in some cases predict that a given person had a stroke even if we get a little bit lower prediction success rate (we can get a very high prediction rate if we predict that everyone didn't have a stroke)","f61dbbb4":"Better fit on the test data but we can still improve it","b1648a33":"# Training our improved machine learning algorithm","a408cb59":"## Reading data","1d252f41":"# Fill the missing bmi values","3051b290":"We can see that men have higher probability to get a stroke than women but not by much","95768989":"We can clearly see that residence type itself doesn't tell correlate to the strokes","1751fda7":"# Decision Tree","c5703f20":"We can see that the older we get the more likely we are to have a stroke","92bfedc9":"# Logistic regression","6cb1b036":"We can see that people that had ilnesses are much more likely to have a stroke. 1\/4 of people that have both hypertension and heart disease had a stroke, 1\/6 of people that had heart disease had a stroke and 1\/8 of people that had hypertension had a stroke. This is a lot compared to around 3,5% of people that had a stroke but were not ill beforehand","fb6d34cf":"Decision tree didn't do a very good job here but at least it predicted that some people had a strok","0f8a8eda":"We see that if we have a high glucose level than we are more likely to have a stroke.","0d20c693":"# Exploratory data analysis","e6953272":"# Random Forest","1c734603":"Random Forest did very similaras the logistic regression","dc9972b9":"Decision tree worked the best so we will try to improve it","b68df12b":"This won't really solve our problem but we will keep max_depth at 22 for future improvements","6f0a5313":"Decision tree perfectly fitted our training data but did not do such a good job at the test data","d425db75":"We see that the people that used to smoke have the highest chance of having a stroke.\nSuprisingly people that don't have smoking status have the lowest chance of having a stroke","d754524b":"# Neural networks","3165fe73":"We see that people that got married are less likely to have a stroke than people than people that didn't get married","285bb435":"We see that people with higher average glucose level or bmi are more likely to have a stroke","38026087":"## Importing needed libraries for all machine learning algorithms","ba728063":"# Creating machine learning algorithm","830f8797":"We've got around 0.22 F1 score. That's a pretty good score for the dataset where we mostly had people that didn't have a stroke"}}