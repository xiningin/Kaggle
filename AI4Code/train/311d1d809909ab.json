{"cell_type":{"7413d188":"code","3c36e4c1":"code","6c1be5ad":"code","76d98ef6":"code","b4c4fb24":"code","adc4cbb9":"code","cbebd1a9":"code","a9343948":"code","dc04e8b0":"code","fe08c6f0":"code","429be8d2":"code","5faeeb29":"code","6907f824":"code","39d49b83":"code","7f068ae6":"code","620b4ed4":"code","7688b107":"code","e917ee36":"code","b03415b8":"code","6198d1d1":"code","57eb42c6":"code","c6614c9a":"code","485140aa":"markdown","db11c0b4":"markdown","598c5ea1":"markdown","54c4a017":"markdown","2ecf67a2":"markdown","a03c23a3":"markdown","964155fe":"markdown","0ecf6c6b":"markdown","dbf80ae9":"markdown","7a5f713d":"markdown","f9450165":"markdown","65bc3acc":"markdown","fafc4b55":"markdown"},"source":{"7413d188":"!git clone https:\/\/github.com\/tensorflow\/models\n    \n# Check out a certain commit to ensure that future changes in the TF ODT API codebase won't affect this notebook.\n!cd models && git checkout ac8d06519","3c36e4c1":"%%bash\ncd models\/research\n\n# Compile protos.\nprotoc object_detection\/protos\/*.proto --python_out=.\n\n# Install TensorFlow Object Detection API.\n# Note: I fixed the version of some dependencies to make it work on Kaggle notebook. In particular:\n# * scipy==1.6.3 to avoid the missing GLIBCXX_3.4.26 error\n# * tensorflow to 2.6.0 to make it compatible with the CUDA version preinstalled on Kaggle.\n# When Kaggle notebook upgrade to TF 2.7, you can use the default setup.py script:\n# cp object_detection\/packages\/tf2\/setup.py .\nwget https:\/\/storage.googleapis.com\/odml-dataset\/others\/setup.py\npip install -q --user .\n\n# Test if the Object Dectection API is working correctly\npython object_detection\/builders\/model_builder_tf2_test.py","6c1be5ad":"import contextlib2\nimport io\nimport IPython\nimport json\nimport numpy as np\nimport os\nimport pathlib\nimport pandas as pd\nimport sys\nimport tensorflow as tf\nimport time\n\nfrom PIL import Image, ImageDraw\n\n# Import the library that is used to submit the prediction result.\nINPUT_DIR = '\/kaggle\/input\/tensorflow-great-barrier-reef\/'\nsys.path.insert(0, INPUT_DIR)\nimport greatbarrierreef","76d98ef6":"# The notebook is supposed to run with TF 2.6.0\nprint(tf.__version__)\nprint(tf.test.is_gpu_available())\nprint(tf.config.list_physical_devices('GPU'))","b4c4fb24":"# TRAINING_RATIO = 0.8\n\n# data_df = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\n\n# # Split the dataset so that no sequence is leaked from the training dataset into the validation dataset.\n# split_index = int(TRAINING_RATIO * len(data_df))\n# while data_df.iloc[split_index - 1].sequence == data_df.iloc[split_index].sequence:\n#     split_index += 1\n\n# # Shuffle both the training and validation datasets.\n# train_data_df = data_df.iloc[:split_index].sample(frac=1).reset_index(drop=True)\n# val_data_df = data_df.iloc[split_index:].sample(frac=1).reset_index(drop=True)\n\n# train_positive_count = len(train_data_df[train_data_df.annotations != '[]'])\n# val_positive_count = len(val_data_df[val_data_df.annotations != '[]'])\n\n# print('Training ratio (all samples):', \n#       float(len(train_data_df)) \/ (len(train_data_df) + len(val_data_df)))\n# print('Training ratio (positive samples):', \n#       float(train_positive_count) \/ (train_positive_count + val_positive_count))","adc4cbb9":"# # Take only the positive images for training and validation\n# train_data_df = train_data_df[train_data_df.annotations != '[]'].reset_index()\n# print('Number of positive images used for training:', len(train_data_df))\n# val_data_df = val_data_df[val_data_df.annotations != '[]'].reset_index()\n# print('Number of positive images used for validation:', len(val_data_df))","cbebd1a9":"# Let's take all available data for training and see if it improves score.\ndata_df = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\ntrain_data_df = data_df[data_df.annotations != '[]'].sample(frac=1).reset_index(drop=True)\nval_data_df = train_data_df","a9343948":"def image_with_annotation(video_id, video_frame, data_df, image_path):\n    \"\"\"Visualize annotations of a given image.\"\"\"\n    full_path = os.path.join(image_path, os.path.join(f'video_{video_id}', f'{video_frame}.jpg'))\n    image = Image.open(full_path)\n    draw = ImageDraw.Draw(image)\n\n    rows = data_df[(data_df.video_id == video_id) & (data_df.video_frame == video_frame)]\n    for _, row in rows.iterrows():\n        annotations = json.loads(row.annotations.replace(\"'\", '\"'))\n        for annotation in annotations:\n            draw.rectangle((\n                annotation['x'], \n                annotation['y'],\n                (annotation['x'] + annotation['width']), \n                (annotation['y'] + annotation['height']),\n                ), outline=(255, 255, 0))\n        \n    buf = io.BytesIO()\n    image.save(buf, 'PNG')\n    data = buf.getvalue()\n\n    return data\n\n# Test visualization of a randomly selected image\nimage_path = os.path.join(INPUT_DIR, 'train_images')\ntest_index = 38\nvideo_id = train_data_df.iloc[test_index].video_id\nvideo_frame = train_data_df.iloc[test_index].video_frame\nIPython.display.Image(image_with_annotation(video_id, video_frame, data_df, image_path))","dc04e8b0":"from object_detection.utils import dataset_util\nfrom object_detection.dataset_tools import tf_record_creation_util\n\n\ndef create_tf_example(video_id, video_frame, data_df, image_path):\n    \"\"\"Create a tf.Example entry for a given training image.\"\"\"\n    full_path = os.path.join(image_path, os.path.join(f'video_{video_id}', f'{video_frame}.jpg'))\n    with tf.io.gfile.GFile(full_path, 'rb') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = Image.open(encoded_jpg_io)\n    if image.format != 'JPEG':\n        raise ValueError('Image format not JPEG')\n\n    height = image.size[1] # Image height\n    width = image.size[0] # Image width\n    filename = f'{video_id}:{video_frame}'.encode('utf8') # Unique id of the image.\n    encoded_image_data = None # Encoded image bytes\n    image_format = 'jpeg'.encode('utf8') # b'jpeg' or b'png'\n\n    xmins = [] # List of normalized left x coordinates in bounding box (1 per box)\n    xmaxs = [] # List of normalized right x coordinates in bounding box\n             # (1 per box)\n    ymins = [] # List of normalized top y coordinates in bounding box (1 per box)\n    ymaxs = [] # List of normalized bottom y coordinates in bounding box\n             # (1 per box)\n    classes_text = [] # List of string class name of bounding box (1 per box)\n    classes = [] # List of integer class id of bounding box (1 per box)\n\n    rows = data_df[(data_df.video_id == video_id) & (data_df.video_frame == video_frame)]\n    for _, row in rows.iterrows():\n        annotations = json.loads(row.annotations.replace(\"'\", '\"'))\n        for annotation in annotations:\n            xmins.append(annotation['x'] \/ width) \n            xmaxs.append((annotation['x'] + annotation['width']) \/ width) \n            ymins.append(annotation['y'] \/ height) \n            ymaxs.append((annotation['y'] + annotation['height']) \/ height) \n\n            classes_text.append('COTS'.encode('utf8'))\n            classes.append(1)\n\n    tf_example = tf.train.Example(features=tf.train.Features(feature={\n      'image\/height': dataset_util.int64_feature(height),\n      'image\/width': dataset_util.int64_feature(width),\n      'image\/filename': dataset_util.bytes_feature(filename),\n      'image\/source_id': dataset_util.bytes_feature(filename),\n      'image\/encoded': dataset_util.bytes_feature(encoded_jpg),\n      'image\/format': dataset_util.bytes_feature(image_format),\n      'image\/object\/bbox\/xmin': dataset_util.float_list_feature(xmins),\n      'image\/object\/bbox\/xmax': dataset_util.float_list_feature(xmaxs),\n      'image\/object\/bbox\/ymin': dataset_util.float_list_feature(ymins),\n      'image\/object\/bbox\/ymax': dataset_util.float_list_feature(ymaxs),\n      'image\/object\/class\/text': dataset_util.bytes_list_feature(classes_text),\n      'image\/object\/class\/label': dataset_util.int64_list_feature(classes),\n    }))\n    \n    return tf_example\n\ndef convert_to_tfrecord(data_df, tfrecord_filebase, image_path, num_shards = 10):\n    \"\"\"Convert the object detection dataset to TFRecord as required by the TF ODT API.\"\"\"\n    with contextlib2.ExitStack() as tf_record_close_stack:\n        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(\n            tf_record_close_stack, tfrecord_filebase, num_shards)\n        \n        for index, row in data_df.iterrows():\n            if index % 500 == 0:\n                print('Processed {0} images.'.format(index))\n            tf_example = create_tf_example(row.video_id, row.video_frame, data_df, image_path)\n            output_shard_index = index % num_shards\n            output_tfrecords[output_shard_index].write(tf_example.SerializeToString())\n        \n        print('Completed processing {0} images.'.format(len(data_df)))\n\n!mkdir dataset\nimage_path = os.path.join(INPUT_DIR, 'train_images')\n\n# Convert train images to TFRecord\nprint('Converting TRAIN images...')\nconvert_to_tfrecord(\n  train_data_df,\n  'dataset\/cots_train',\n  image_path,\n  num_shards = 4\n)\n\n# Convert validation images to TFRecord\nprint('Converting VALIDATION images...')\nconvert_to_tfrecord(\n  val_data_df,\n  'dataset\/cots_val',\n  image_path,\n  num_shards = 4\n)","fe08c6f0":"# Create a label map to map between label index and human-readable label name.\n\nlabel_map_str = \"\"\"item {\n  id: 1\n  name: 'COTS'\n}\"\"\"\n\nwith open('dataset\/label_map.pbtxt', 'w') as f:\n  f.write(label_map_str)\n\n!more dataset\/label_map.pbtxt","429be8d2":"# Download the pretrained EfficientDet-D0 checkpoint\n!wget http:\/\/download.tensorflow.org\/models\/object_detection\/tf2\/20200711\/efficientdet_d0_coco17_tpu-32.tar.gz\n!tar -xvzf efficientdet_d0_coco17_tpu-32.tar.gz","5faeeb29":"from string import Template\n\nconfig_file_template = \"\"\"\n# SSD with EfficientNet-b0 + BiFPN feature extractor,\n# shared box predictor and focal loss (a.k.a EfficientDet-d0).\n# See EfficientDet, Tan et al, https:\/\/arxiv.org\/abs\/1911.09070\n# See Lin et al, https:\/\/arxiv.org\/abs\/1708.02002\n# Initialized from an EfficientDet-D0 checkpoint.\n#\n# Train on GPU\n\nmodel {\n  ssd {\n    inplace_batchnorm_update: true\n    freeze_batchnorm: false\n    num_classes: 1\n    add_background_class: false\n    box_coder {\n      faster_rcnn_box_coder {\n        y_scale: 10.0\n        x_scale: 10.0\n        height_scale: 5.0\n        width_scale: 5.0\n      }\n    }\n    matcher {\n      argmax_matcher {\n        matched_threshold: 0.5\n        unmatched_threshold: 0.5\n        ignore_thresholds: false\n        negatives_lower_than_unmatched: true\n        force_match_for_each_row: true\n        use_matmul_gather: true\n      }\n    }\n    similarity_calculator {\n      iou_similarity {\n      }\n    }\n    encode_background_as_zeros: true\n    anchor_generator {\n      multiscale_anchor_generator {\n        min_level: 3\n        max_level: 7\n        anchor_scale: 4.0\n        aspect_ratios: [1.0, 2.0, 0.5]\n        scales_per_octave: 3\n      }\n    }\n    image_resizer {\n      keep_aspect_ratio_resizer {\n        min_dimension: 1280\n        max_dimension: 1280\n        pad_to_max_dimension: true\n        }\n    }\n    box_predictor {\n      weight_shared_convolutional_box_predictor {\n        depth: 64\n        class_prediction_bias_init: -4.6\n        conv_hyperparams {\n          force_use_bias: true\n          activation: SWISH\n          regularizer {\n            l2_regularizer {\n              weight: 0.00004\n            }\n          }\n          initializer {\n            random_normal_initializer {\n              stddev: 0.01\n              mean: 0.0\n            }\n          }\n          batch_norm {\n            scale: true\n            decay: 0.99\n            epsilon: 0.001\n          }\n        }\n        num_layers_before_predictor: 3\n        kernel_size: 3\n        use_depthwise: true\n      }\n    }\n    feature_extractor {\n      type: 'ssd_efficientnet-b0_bifpn_keras'\n      bifpn {\n        min_level: 3\n        max_level: 7\n        num_iterations: 3\n        num_filters: 64\n      }\n      conv_hyperparams {\n        force_use_bias: true\n        activation: SWISH\n        regularizer {\n          l2_regularizer {\n            weight: 0.00004\n          }\n        }\n        initializer {\n          truncated_normal_initializer {\n            stddev: 0.03\n            mean: 0.0\n          }\n        }\n        batch_norm {\n          scale: true,\n          decay: 0.99,\n          epsilon: 0.001,\n        }\n      }\n    }\n    loss {\n      classification_loss {\n        weighted_sigmoid_focal {\n          alpha: 0.25\n          gamma: 1.5\n        }\n      }\n      localization_loss {\n        weighted_smooth_l1 {\n        }\n      }\n      classification_weight: 1.0\n      localization_weight: 1.0\n    }\n    normalize_loss_by_num_matches: true\n    normalize_loc_loss_by_codesize: true\n    post_processing {\n      batch_non_max_suppression {\n        score_threshold: 1e-8\n        iou_threshold: 0.5\n        max_detections_per_class: 100\n        max_total_detections: 100\n      }\n      score_converter: SIGMOID\n    }\n  }\n}\n\ntrain_config: {\n  fine_tune_checkpoint: \"efficientdet_d0_coco17_tpu-32\/checkpoint\/ckpt-0\"\n  fine_tune_checkpoint_version: V2\n  fine_tune_checkpoint_type: \"detection\"\n  batch_size: 2\n  sync_replicas: false\n  startup_delay_steps: 0\n  replicas_to_aggregate: 1\n  use_bfloat16: false\n  num_steps: $training_steps\n  data_augmentation_options {\n    random_horizontal_flip {\n    }\n  }\n  data_augmentation_options {\n    random_scale_crop_and_pad_to_square {\n      output_size: 1280\n      scale_min: 0.5\n      scale_max: 2.0\n    }\n  }\n  optimizer {\n    momentum_optimizer: {\n      learning_rate: {\n        cosine_decay_learning_rate {\n          learning_rate_base: 5e-3\n          total_steps: $training_steps\n          warmup_learning_rate: 5e-4\n          warmup_steps: $warmup_steps\n        }\n      }\n      momentum_optimizer_value: 0.9\n    }\n    use_moving_average: false\n  }\n  max_number_of_boxes: 100\n  unpad_groundtruth_tensors: false\n}\n\ntrain_input_reader: {\n  label_map_path: \"dataset\/label_map.pbtxt\"\n  tf_record_input_reader {\n    input_path: \"dataset\/cots_train-?????-of-00004\"\n  }\n}\n\neval_config: {\n  metrics_set: \"coco_detection_metrics\"\n  use_moving_averages: false\n  batch_size: 2;\n}\n\neval_input_reader: {\n  label_map_path: \"dataset\/label_map.pbtxt\"\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: \"dataset\/cots_val-?????-of-00004\"\n  }\n}\n\"\"\"","6907f824":"# Define the training pipeline\n\nTRAINING_STEPS = 20000\nWARMUP_STEPS = 2000\nPIPELINE_CONFIG_PATH='dataset\/pipeline.config'\n\npipeline = Template(config_file_template).substitute(\n    training_steps=TRAINING_STEPS, warmup_steps=WARMUP_STEPS)\n\nwith open(PIPELINE_CONFIG_PATH, 'w') as f:\n    f.write(pipeline)","39d49b83":"MODEL_DIR='cots_efficientdet_d0'\n!mkdir {MODEL_DIR}\n!python models\/research\/object_detection\/model_main_tf2.py \\\n    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n    --model_dir={MODEL_DIR} \\\n    --alsologtostderr","7f068ae6":"!python models\/research\/object_detection\/model_main_tf2.py \\\n    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n    --model_dir={MODEL_DIR} \\\n    --checkpoint_dir={MODEL_DIR} \\\n    --eval_timeout=0 \\\n    --alsologtostderr","620b4ed4":"!python models\/research\/object_detection\/exporter_main_v2.py \\\n    --input_type image_tensor \\\n    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n    --trained_checkpoint_dir={MODEL_DIR} \\\n    --output_directory={MODEL_DIR}\/output","7688b107":"!ls {MODEL_DIR}\/output","e917ee36":"# Load the TensorFlow COTS detection model into memory.\nstart_time = time.time()\ntf.keras.backend.clear_session()\ndetect_fn_tf_odt = tf.saved_model.load(os.path.join(os.path.join(MODEL_DIR, 'output'), 'saved_model'))\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint('Elapsed time: ' + str(elapsed_time) + 's')","b03415b8":"# Define some utils method for prediction.\n\ndef load_image_into_numpy_array(path):\n    \"\"\"Load an image from file into a numpy array.\n\n    Puts image into numpy array to feed into tensorflow graph.\n    Note that by convention we put it into a numpy array with shape\n    (height, width, channels), where channels=3 for RGB.\n\n    Args:\n    path: a file path (this can be local or on colossus)\n\n    Returns:\n    uint8 numpy array with shape (img_height, img_width, 3)\n    \"\"\"\n    img_data = tf.io.gfile.GFile(path, 'rb').read()\n    image = Image.open(io.BytesIO(img_data))\n    (im_width, im_height) = image.size\n    \n    return np.array(image.getdata()).reshape(\n      (im_height, im_width, 3)).astype(np.uint8)\n\ndef detect(image_np):\n    \"\"\"Detect COTS from a given numpy image.\"\"\"\n\n    input_tensor = np.expand_dims(image_np, 0)\n    start_time = time.time()\n    detections = detect_fn_tf_odt(input_tensor)\n    return detections","6198d1d1":"import greatbarrierreef\nenv = greatbarrierreef.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission","57eb42c6":"DETECTION_THRESHOLD = 0.3\n\nsubmission_dict = {\n    'id': [],\n    'prediction_string': [],\n}\n\nfor (image_np, sample_prediction_df) in iter_test:\n    height, width, _ = image_np.shape\n    \n    # Run object detection using the TensorFlow model.\n    detections = detect(image_np)\n    \n    # Parse the detection result and generate a prediction string.\n    num_detections = detections['num_detections'][0].numpy().astype(np.int32)\n    predictions = []\n    for index in range(num_detections):\n        score = detections['detection_scores'][0][index].numpy()\n        if score < DETECTION_THRESHOLD:\n            continue\n\n        bbox = detections['detection_boxes'][0][index].numpy()\n        y_min = int(bbox[0] * height)\n        x_min = int(bbox[1] * width)\n        y_max = int(bbox[2] * height)\n        x_max = int(bbox[3] * width)\n        \n        bbox_width = x_max - x_min\n        bbox_height = y_max - y_min\n        \n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n    \n    # Generate the submission data.\n    prediction_str = ' '.join(predictions)\n    sample_prediction_df['annotations'] = prediction_str\n    env.predict(sample_prediction_df)\n\n    print('Prediction:', prediction_str)","c6614c9a":"# Remove the dataset files to save space.\n!rm -rf dataset\n!rm -rf train_images\n!rm tensorflow-great-barrier-reef.zip\n\n# Remove other data downloaded during training.\n!rm -rf models\n!rm efficientdet_d0_coco17_tpu-32.tar.gz","485140aa":"# Clean up","db11c0b4":"# Import dependencies","598c5ea1":"# Install TensorFlow Object Detection API\n\nPip may report some dependency errors. You can safely ignore these errors and proceed if all tests in `model_builder_tf2_test.py` passed. ","54c4a017":"Split the `train` folder into training dataset and validation dataset. ","2ecf67a2":"# Evaluate the object detection model","a03c23a3":"# Train an object detection model\n\nWe'll use [TensorFlow Object Detection API](https:\/\/github.com\/tensorflow\/models\/tree\/master\/research\/object_detection) and an EfficientDet-D0 base model and apply transfer learning to train a COTS detection model. EfficientDet-D0 is the smallest model in the EfficientDet model family and we pick it to reduce training time for demonstration purpose. You can probably increase accuracy by switch to using a larger EfficientDet model.","964155fe":"Visualize one randomly selected image from the training data to see if the annotation looks correct.","0ecf6c6b":"Convert the training and validation dataset into TFRecord format as required by the TensorFlow Object Detection API.","dbf80ae9":"To save time for demonstration purpose, here we'll only take the positive images (images that contain at least 1 starfish) for training. TensorFlow Object Detection API will take the areas in the images that aren't annotated as containing a starfish to use as negative samples.","7a5f713d":"# Run inference on test images and create the submission file","f9450165":"# Prepare the training dataset","65bc3acc":"# Export as SavedModel for inference","fafc4b55":"This notebook contains code to train a crown-of-thorns starfish (COTS) detection model to serve as a baseline model for [this competition](https:\/\/www.kaggle.com\/c\/tensorflow-great-barrier-reef\/overview). We use [TensorFlow Object Detection API](https:\/\/github.com\/tensorflow\/models\/tree\/master\/research\/object_detection) to apply transfer learning on an [EfficientDet-D0](https:\/\/arxiv.org\/abs\/1911.09070) pretrained model. "}}