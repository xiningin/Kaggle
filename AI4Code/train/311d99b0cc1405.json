{"cell_type":{"55a99f39":"code","ffc94493":"code","afd00097":"code","6346429b":"code","83d814ef":"code","5bdd7a51":"code","fb2119ae":"code","5e332495":"code","e31bcac2":"code","514f5c1a":"code","6618ce0e":"code","456b06f9":"code","40ef549c":"code","08998c81":"code","06472960":"code","468de18d":"code","70c413a9":"code","e0a1910a":"code","32d67092":"code","a5073cea":"code","c9b38b8c":"code","0b4b1a34":"code","df217929":"code","f2ab9fa2":"code","978f25db":"markdown","42e58dc3":"markdown","71c986c2":"markdown","4788dcf1":"markdown","87a66ae3":"markdown","88a9d619":"markdown","60f418c0":"markdown","898d7e92":"markdown","ca0a08c4":"markdown","68692716":"markdown","baf29cd0":"markdown","e1f216c1":"markdown","ae42ddfd":"markdown","56a7e855":"markdown","f11f7969":"markdown"},"source":{"55a99f39":"!pip install -q efficientnet\nimport numpy as np \nimport pandas as pd \nimport re\nimport cv2\nimport math\nimport time\nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Input, Concatenate, Flatten, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import optimizers\nimport efficientnet.tfkeras as efn\nimport os","ffc94493":"print(\"Tensorflow version \" + tf.__version__)","afd00097":"AUTO = tf.data.experimental.AUTOTUNE\n\ntry: ## Trying to check if a TPU cluster exists\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('TPU Master ', tpu.master)\nexcept ValueError:\n    tpu = None\n    \nif tpu: #In the case the cluster exists, we initialize it and connect to it and create a strategy for parallel processing\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n    \nprint('Replicas', strategy.num_replicas_in_sync)    \n\nDATASET1 = 'melanoma-768x768'\nGCS_PATH1 = KaggleDatasets().get_gcs_path(DATASET1) #Getting the Google Cloud Storage path for the publically available dataset","6346429b":"train_details = pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/train.csv')\ntrain_details.isna().sum()","83d814ef":"train_details['sex'] = train_details['sex'].fillna('male')\ntrain_details['age_approx'] = train_details['age_approx'].fillna(train_details['age_approx'].mean())\ntrain_details['anatom_site_general_challenge'] = train_details['anatom_site_general_challenge'].fillna('head\/neck')","5bdd7a51":"from sklearn.preprocessing import LabelEncoder\nenc1 = LabelEncoder()\nenc2 = LabelEncoder()\n\ntrain_details['sex'] = enc1.fit_transform(train_details['sex'])\ntrain_details['anatom_site_general_challenge'] = enc2.fit_transform(train_details['anatom_site_general_challenge'])\n\nx_vec = train_details[['sex','age_approx','anatom_site_general_challenge']]","fb2119ae":"SEED = 42\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nSIZE1 = [768,768]\nLR = 0.00004\nEPOCHS = 12\nWARMUP = 5\nWEIGHT_DECAY = 0\nLABEL_SMOOTHING = 0.05\nTTA = 4","5e332495":"np.random.seed(SEED)\ntf.random.set_seed(SEED)","e31bcac2":"train_filenames1 = tf.io.gfile.glob(GCS_PATH1 + '\/train*.tfrec')\ntest_filenames1 = tf.io.gfile.glob(GCS_PATH1 + '\/test*.tfrec')","514f5c1a":"from sklearn.model_selection import train_test_split\ntrain_filenames1, valid_filenames1 = train_test_split(train_filenames1, test_size = 0.15, random_state = SEED)","6618ce0e":"def decode_image(image):\n    image = tf.image.decode_jpeg(image, channels = 3)\n    image = tf.cast(image, tf.float32)\/255.0\n    image = tf.reshape(image, [*SIZE1, 3])\n    return image\n\ndef data_augment(image, label = None, seed = SEED):\n    image = tf.image.rot90(image, k = np.random.randint(4))\n    image = tf.image.random_flip_left_right(image, seed = SEED)\n    image = tf.image.random_flip_up_down(image, seed = SEED)\n    if label is None:\n        return image\n    else:\n        return image, label\n\ndef read_labeled_tfrecord(example):\n    LFormat = {'image': tf.io.FixedLenFeature([], tf.string),\n             'target': tf.io.FixedLenFeature([], tf.int64)}\n    example = tf.io.parse_single_example(example, LFormat)\n    image = decode_image(example['image'])\n    label = tf.cast(example['target'], tf.int32)\n    \n    return image, label\n\ndef read_unlabeled_tfrecord(example):\n    UFormat = {'image': tf.io.FixedLenFeature([], tf.string),\n             'image_name': tf.io.FixedLenFeature([], tf.string)}\n    example = tf.io.parse_single_example(example, UFormat)\n    image = decode_image(example['image'])\n    image_name = example['image_name']\n    \n    return image, image_name\n\ndef load_dataset(filenames, labeled = True, ordered = False):\n    ignore_order = tf.data.Options()\n    \n    if not ordered:\n        ignore_order.experimental_deterministic = False\n    \n    dataset = (tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO).with_options(ignore_order).map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO))\n    \n    return dataset\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","456b06f9":"train1 = (load_dataset(train_filenames1).map(data_augment, num_parallel_calls = AUTO).shuffle(SEED).batch(BATCH_SIZE, drop_remainder = True).repeat().prefetch(AUTO))\nvalid_dataset1 = (load_dataset(valid_filenames1, labeled=True).batch(BATCH_SIZE).repeat().prefetch(AUTO))","40ef549c":"with strategy.scope():\n    model = tf.keras.Sequential([efn.EfficientNetB7(input_shape=(*SIZE1,3), weights='imagenet', include_top=False, pooling = 'avg'),\n            Dense(1, activation = 'sigmoid')])\n    \n    model.compile(optimizer='adam',loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = LABEL_SMOOTHING),\n        metrics=[tf.keras.metrics.AUC(name='auc')])","08998c81":"def get_cosine_schedule_with_warmup(lr,num_warmup_steps, num_training_steps, num_cycles=0.5):\n    def lrfn(epoch):\n        if epoch < num_warmup_steps:\n            return (float(epoch) \/ float(max(1, num_warmup_steps))) * lr\n        progress = float(epoch - num_warmup_steps) \/ float(max(1, num_training_steps - num_warmup_steps))\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr\n\n    return tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nlr_schedule= get_cosine_schedule_with_warmup(lr=LR,num_warmup_steps=WARMUP,num_training_steps=EPOCHS)","06472960":"validationsize1 = count_data_items(valid_filenames1)\nprint(validationsize1)","468de18d":"STEPS_PER_EPOCH = (count_data_items(train_filenames1)) \/\/ BATCH_SIZE\n#class_weights = {0:0.5089, 1:28.3613}\nmodel.fit(train1, epochs=EPOCHS, callbacks=[lr_schedule],steps_per_epoch=STEPS_PER_EPOCH,validation_data=valid_dataset1,validation_steps=validationsize1\/\/BATCH_SIZE)","70c413a9":"num_test_images = count_data_items(test_filenames1)\nsubmission_df1 = pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/sample_submission.csv')\nfor i in range(TTA):\n    test_dataset = (load_dataset(test_filenames1, labeled=False,ordered=True)\n    .map(data_augment, num_parallel_calls=AUTO)  \n    .batch(BATCH_SIZE))\n    test_dataset_images = test_dataset.map(lambda image, image_name: image)\n    test_dataset_image_name = test_dataset.map(lambda image, image_name: image_name).unbatch()\n    test_ids = next(iter(test_dataset_image_name.batch(num_test_images))).numpy().astype('U')\n    test_pred = model.predict(test_dataset_images, verbose=1) \n    pred_df = pd.DataFrame({'image_name': test_ids, 'target': np.concatenate(test_pred)})\n    temp = submission_df1.copy()   \n    del temp['target']  \n    submission_df1['target'] += temp.merge(pred_df,on=\"image_name\")['target']\/TTA","e0a1910a":"submission_df1.to_csv('efficientnetb7_784.csv', index = False)","32d67092":"DATASET2 = 'melanoma-512x512'\nGCS_PATH2 = KaggleDatasets().get_gcs_path(DATASET2) #Getting the Google Cloud Storage path for the publically available dataset\n\nSIZE2 = [512,512]\n\ntrain_filenames2 = tf.io.gfile.glob(GCS_PATH2 + '\/train*.tfrec')\ntest_filenames2 = tf.io.gfile.glob(GCS_PATH2 + '\/test*.tfrec')\n\ntrain_filenames2, valid_filenames2 = train_test_split(train_filenames2, test_size = 0.15, random_state = SEED)\n\ndef decode_image(image):\n    image = tf.image.decode_jpeg(image, channels = 3)\n    image = tf.cast(image, tf.float32)\/255.0\n    image = tf.reshape(image, [*SIZE2, 3])\n    return image\n\ndef data_augment(image, label = None, seed = SEED):\n    image = tf.image.rot90(image, k = np.random.randint(4))\n    image = tf.image.random_flip_left_right(image, seed = SEED)\n    image = tf.image.random_flip_up_down(image, seed = SEED)\n    if label is None:\n        return image\n    else:\n        return image, label\n\ndef read_labeled_tfrecord(example):\n    LFormat = {'image': tf.io.FixedLenFeature([], tf.string),\n             'target': tf.io.FixedLenFeature([], tf.int64)}\n    example = tf.io.parse_single_example(example, LFormat)\n    image = decode_image(example['image'])\n    label = tf.cast(example['target'], tf.int32)\n    \n    return image, label\n\ndef read_unlabeled_tfrecord(example):\n    UFormat = {'image': tf.io.FixedLenFeature([], tf.string),\n             'image_name': tf.io.FixedLenFeature([], tf.string)}\n    example = tf.io.parse_single_example(example, UFormat)\n    image = decode_image(example['image'])\n    image_name = example['image_name']\n    \n    return image, image_name\n\ndef load_dataset(filenames, labeled = True, ordered = False):\n    ignore_order = tf.data.Options()\n    \n    if not ordered:\n        ignore_order.experimental_deterministic = False\n    \n    dataset = (tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO).with_options(ignore_order).map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO))\n    \n    return dataset\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\ntrain2 = (load_dataset(train_filenames2).map(data_augment, num_parallel_calls = AUTO).shuffle(SEED).batch(BATCH_SIZE, drop_remainder = True).repeat().prefetch(AUTO))\nvalid_dataset2 = (load_dataset(valid_filenames2, labeled=True).batch(BATCH_SIZE).repeat().prefetch(AUTO))","a5073cea":"with strategy.scope():\n\n    model2 = tf.keras.Sequential([\n        efn.EfficientNetB7(input_shape=(*SIZE2, 3),weights='imagenet',pooling='avg',include_top=False),\n        Dense(1, activation='sigmoid')\n    ])\n    \n    model2.compile(optimizer='adam',loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = LABEL_SMOOTHING),\n        metrics=[tf.keras.metrics.AUC(name='auc')])","c9b38b8c":"validationsize2 = count_data_items(valid_filenames2)\nprint(validationsize2)","0b4b1a34":"STEPS_PER_EPOCH = (count_data_items(train_filenames2)) \/\/ BATCH_SIZE\n#class_weights = {0:0.5089, 1:28.3613}\nmodel2.fit(train2, epochs=EPOCHS, callbacks=[lr_schedule],steps_per_epoch=STEPS_PER_EPOCH,validation_data=valid_dataset2,validation_steps=validationsize2\/\/BATCH_SIZE)","df217929":"num_test_images = count_data_items(test_filenames2)\nsubmission_df2 = pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/sample_submission.csv')\nfor i in range(TTA):\n    test_dataset = (load_dataset(test_filenames2, labeled=False,ordered=True)\n    .map(data_augment, num_parallel_calls=AUTO)  \n    .batch(BATCH_SIZE))\n    test_dataset_images = test_dataset.map(lambda image, image_name: image)\n    test_dataset_image_name = test_dataset.map(lambda image, image_name: image_name).unbatch()\n    test_ids = next(iter(test_dataset_image_name.batch(num_test_images))).numpy().astype('U')\n    test_pred = model2.predict(test_dataset_images, verbose=1) \n    pred_df = pd.DataFrame({'image_name': test_ids, 'target': np.concatenate(test_pred)})\n    temp = submission_df2.copy()   \n    del temp['target']  \n    submission_df2['target'] += temp.merge(pred_df,on=\"image_name\")['target']\/TTA","f2ab9fa2":"submission_df2.to_csv('efficientnetb7_512.csv', index=False)","978f25db":"## Second Model for Ensemble","42e58dc3":"## Creating some Helper Functions","71c986c2":"### Setting a Seed for Everything","4788dcf1":"Please view Version 5 for the best results from this notebook","87a66ae3":"## Importing Libraries","88a9d619":"## Implementing the Efficientnet","60f418c0":"## Loading the train and validation datasets","898d7e92":"### Getting the train and test file paths","ca0a08c4":"## Setting the Hyperparameters for the Model","68692716":"## Setting up the TPU","baf29cd0":"Reference Code for this notebook\nhttps:\/\/www.kaggle.com\/jagadish13\/melanoma-detection-efficientnetb7-tpu-eda","e1f216c1":"## Data Cleaning","ae42ddfd":"## Training","56a7e855":"## Testing with Test Time Augmentation","f11f7969":"## Learning Rate Scheduling"}}