{"cell_type":{"62fbcafa":"code","51f40230":"code","2651eb5d":"code","fa4504fe":"code","d9451370":"code","eb94d274":"code","6970a823":"code","ef9a87b1":"code","f9f3b184":"code","32dedf1a":"code","53ec4b12":"code","78b8bf96":"code","039dfded":"code","efd41451":"code","18863d8b":"code","a3ba32cb":"code","b232a772":"code","125565ce":"code","7125b31f":"markdown","a3bb47d7":"markdown","d3547717":"markdown","2d1110d3":"markdown","cbb440c8":"markdown","5e936316":"markdown","1f654471":"markdown","ea8e88d6":"markdown","1e998d7f":"markdown","cd8beb33":"markdown","7efbdf6b":"markdown","6d7d0b6c":"markdown","1d5a7d45":"markdown","9c1bdd31":"markdown"},"source":{"62fbcafa":"import re\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import hstack\nfrom sklearn.metrics import f1_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer","51f40230":"df = pd.read_csv('\/kaggle\/input\/analyticsvidhya-hacklive3\/Train\/Train.csv')\ndf.drop('id', axis=1, inplace=True)\ndf.head(2)","2651eb5d":"print(df.shape)","fa4504fe":"df.loc[0, 'ABSTRACT']","d9451370":"test = pd.read_csv('\/kaggle\/input\/analyticsvidhya-hacklive3\/Test\/Test.csv')\ntest.head(2)","eb94d274":"TARGET_COLS = ['Analysis of PDEs', 'Applications',\n               'Artificial Intelligence', 'Astrophysics of Galaxies',\n               'Computation and Language', 'Computer Vision and Pattern Recognition',\n               'Cosmology and Nongalactic Astrophysics',\n               'Data Structures and Algorithms', 'Differential Geometry',\n               'Earth and Planetary Astrophysics', 'Fluid Dynamics',\n               'Information Theory', 'Instrumentation and Methods for Astrophysics',\n               'Machine Learning', 'Materials Science', 'Methodology', 'Number Theory',\n               'Optimization and Control', 'Representation Theory', 'Robotics',\n               'Social and Information Networks', 'Statistics Theory',\n               'Strongly Correlated Electrons', 'Superconductivity',\n               'Systems and Control']","6970a823":"def remove_punctuations(x):\n    x = str(x)\n    for punct in \"\/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~' + '\u201c\u201d\u2019' + '\u2026':\n        x = x.replace(punct, '')\n    return x","ef9a87b1":"def clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x","f9f3b184":"# This is a generalized replacement of misspelled words which i use for all projects so some words here may not be actually in abstract\ndef misspelled_words(x):\n    x = x.replace('colour', 'color').replace('centre', 'center').replace('didnt', 'did not').replace('doesnt', 'does not') \\\n        .replace('isnt', 'is not').replace('shouldnt', 'should not').replace('favourite', 'favorite').replace('travelling', 'traveling') \\\n        .replace('counselling', 'counseling').replace('theatre', 'theater').replace('cancelled', 'canceled').replace('labour', 'labor') \\\n        .replace('organisation', 'organization').replace('wwii', 'world war 2').replace('citicise', 'criticize') \\\n        .replace('instagram', 'social medium').replace('whatsapp', 'social medium').replace('WeChat', 'social medium') \\\n        .replace('snapchat', 'social medium').replace('Snapchat', 'social medium').replace('btech', 'B.Tech').replace('Quorans', 'Quora') \\\n        .replace('cryptocurrency', 'crypto currency').replace('cryptocurrencies', 'crypto currency').replace('behaviour', 'behavior') \\\n        .replace('analyse', 'analyze').replace('licence', 'license').replace('programme', 'program').replace('grey', 'gray') \\\n        .replace('realise', 'realize').replace('bcom', 'B.Com').replace('defence', 'defense').replace('mtech', 'M.Tech') \\\n        .replace('Btech', 'B.Tech').replace('honours', 'honors').replace('recognise', 'recognize').replace('programr', 'programmer') \\\n        .replace('programrs', 'programmer').replace('hasnt', 'has not').replace('litre', 'liter').replace('Isnt', 'is not') \\\n        .replace('learnt', 'learn').replace('favour', 'favor').replace('neighbour', 'neighbor').replace('demonetisation', 'demonetization') \\\n        .replace('\u20b9', '').replace('&', 'and')\n    return x","32dedf1a":"df[\"ABSTRACT\"] = df[\"ABSTRACT\"].apply(lambda x: remove_punctuations(x))\ndf[\"ABSTRACT\"] = df[\"ABSTRACT\"].apply(lambda x: clean_numbers(x))\ndf[\"ABSTRACT\"] = df[\"ABSTRACT\"].apply(lambda x: misspelled_words(x))\ntest[\"ABSTRACT\"] = test[\"ABSTRACT\"].apply(lambda x: remove_punctuations(x))\ntest[\"ABSTRACT\"] = test[\"ABSTRACT\"].apply(lambda x: clean_numbers(x))\ntest[\"ABSTRACT\"] = test[\"ABSTRACT\"].apply(lambda x: misspelled_words(x))","53ec4b12":"train, val = train_test_split(df, test_size=0.2, random_state=0)\ntrain.shape, val.shape","78b8bf96":"tfidfvec = TfidfVectorizer(min_df=3, max_features=None, ngram_range=(1, 2), strip_accents='unicode', stop_words='english')\ntfidfvec.fit(df['ABSTRACT'])\ntrain_vec = tfidfvec.transform(train['ABSTRACT'])\nval_vec = tfidfvec.transform(val['ABSTRACT'])\ntest_vec = tfidfvec.transform(test['ABSTRACT'])\ntrain_vec.shape, val_vec.shape, test_vec.shape","039dfded":"train_data = hstack((train_vec, train[['Computer Science', 'Mathematics', 'Physics', 'Statistics']]), format=\"csr\", dtype='float64')\nval_data = hstack((val_vec, val[['Computer Science', 'Mathematics', 'Physics', 'Statistics']]), format=\"csr\", dtype='float64')\ntest_data = hstack((test_vec, test[['Computer Science', 'Mathematics', 'Physics', 'Statistics']]), format=\"csr\", dtype='float64')\ntrain_data.shape, val_data.shape, test_data.shape","efd41451":"parameters = {\n    'estimator__C': [10 ** x for x in range(-2, 3)]\n}\n\nestimator = OneVsRestClassifier(LogisticRegression(max_iter=500, n_jobs=-1))\nmodel = GridSearchCV(estimator, parameters, scoring='f1_micro', cv=5, n_jobs=-1, refit=False)\nmodel.fit(train_data, train[TARGET_COLS])\nbest_C = model.best_params_['estimator__C']\nprint('The best value of C is', best_C)","18863d8b":"clf = OneVsRestClassifier(LogisticRegression(C = best_C, max_iter=500, n_jobs=-1))\nclf.fit(train_data, train[TARGET_COLS])\npred = clf.predict(val_data)\nf1_score(val[TARGET_COLS], pred, average='micro')","a3ba32cb":"#This is a simple hack which is used to find the optimal treshold to calculate the best F1 score\ndef get_best_thresholds(true, preds):\n    thresholds = [i\/100 for i in range(100)]\n    best_thresholds = []\n    for idx in range(25):\n        f1_scores = [f1_score(true[:, idx], (preds[:, idx] > thresh) * 1) for thresh in thresholds]\n        best_thresh = thresholds[np.argmax(f1_scores)]\n        best_thresholds.append(best_thresh)\n    return best_thresholds","b232a772":"val_preds = clf.predict_proba(val_data)\nbest_thresholds = get_best_thresholds(val[TARGET_COLS].values, val_preds)\nfor i, thresh in enumerate(best_thresholds):\n    val_preds[:, i] = (val_preds[:, i] > thresh) * 1\nf1_score(val[TARGET_COLS], val_preds, average='micro')","125565ce":"ss = pd.read_csv('..\/input\/analyticsvidhya-hacklive3\/SampleSubmission.csv')\npreds_test = clf.predict_proba(test_data)\n\nfor i, thresh in enumerate(best_thresholds):\n    preds_test[:, i] = (preds_test[:, i] > thresh) * 1\n\nss[TARGET_COLS] = preds_test\nss.to_csv('hacklive_submission', index = False)","7125b31f":"# HackLive 3: Guided Hackathon - NLP (Analytics vidhya)\n\n##### Click <a href='https:\/\/datahack.analyticsvidhya.com\/contest\/hacklive-3-guided-hackathon-text-classification\/'>here<\/a> to go to competition page\n\n#### Author: Palakodeti Nagendra Deepak\n* Performance metric used : Micro F1 score\n* Public leaderboard score: 0.7745629898 \n* Private leaderboard ranking: 7\n* Private leaderboard score: 0.7775161860\n* Private leaderboard ranking: 6","a3bb47d7":"<h2> Problem statement <\/h2>\n\n<h4> In real world scenario many research institutes go through huge archives of research papers, in such scenario tagging of research papers manually becomes a tedious task. The objective of this ML problem is to automatically tag the research paper in any of the 25 possible tags.  <\/h4>\n    \n**List of possible tags are as follows:**\n\n[Tags, Analysis of PDEs, Applications, Artificial Intelligence,Astrophysics of Galaxies, Computation and Language, Computer Vision and Pattern Recognition, Cosmology and Nongalactic Astrophysics, Data Structures and Algorithms, Differential Geometry, Earth and Planetary Astrophysics, Fluid Dynamics,Information Theory, Instrumentation and Methods for Astrophysics, Machine Learning, Materials Science, Methodology, Number Theory, Optimization and Control, Representation Theory, Robotics, Social and Information Networks, Statistics Theory, Strongly Correlated Electrons, Superconductivity, Systems and Control]\n\n<h2> Type of Machine Learning Problem <\/h2>\n\n<h4> Since there are 25 different labels and each research paper may belong to one or more category this is a multilabel classification problem <\/h4>\n\n<h2> Performance metric <\/h2>\n\n<h4> Micro F1 score <\/h4>","d3547717":"<h3> Splitiing the data into train and validation (80:20) <\/h3>","2d1110d3":"<h4> As you can see above the F1 score after finding optimal tresholds has drastically improved from 0.73 to 0.78.\nSuch improvements can lead to gaining more rankings in competitions and hackathons<\/h4>","cbb440c8":"<h3> Submitting the predictions <\/h3>","5e936316":"<h3> Vectorizing train, validation and test dataset using Tfidf vectorizer<\/h3>","1f654471":"<h3> Using Grid search to find best hyperparameters <\/h3>\n<h5> Note: Since there was only single hyperparameter to tune hence i used GridSearchCV. If there are more hyperparameters it is wise to choose RandomizedSearchCV <\/h5>","ea8e88d6":"<h5> Here after vectorizing we are stacking the remaining 4 features into csr format. Here if we use numpy array format instead of csr format then our RAM won't be able to suffice hence it is important to pass data to our model in csr format <\/h5>","1e998d7f":"<h4> PS: This is my first ever hackathon participation and kaggle notebook. Please provide feedbacks and upvote the notebook. <\/h4>","cd8beb33":"<h4> Above is the sample of a Abstract of a research paper <\/h4>","7efbdf6b":"<h3> Text preprocessing <\/h3>","6d7d0b6c":"## Importing the data and necessary libraries","1d5a7d45":"<h3> Applying ML model using best hyperparameter and predicting on validation data","9c1bdd31":"<h3> There are total 14004 research papers(rows) in which Abstract gives us the gist of the research paper, rows such as Computer Science, Mathematics, Physics, Statistics gives us the primary domain of the research paper and the remaining 25 columns are the target columns(labels) <\/h3>"}}