{"cell_type":{"27fd33fc":"code","c23d191d":"code","df404739":"code","c47bf486":"code","49f01478":"code","8bc450c0":"code","ccbe6b26":"code","e0ae3b55":"code","54312e2c":"code","6ca1be23":"code","d71077bc":"code","28792644":"code","04fd9c0c":"code","83d60e7a":"code","0a285221":"code","fef693df":"code","84cdcc0e":"code","643a8105":"code","352559a4":"code","ae12f1e3":"code","6f4df514":"code","ee1027f2":"code","d72556cf":"code","bcaa859e":"code","b5d341af":"markdown","6da35e06":"markdown","7eb3a456":"markdown","53c6b1a7":"markdown","414d37ef":"markdown","58868514":"markdown","1c56822a":"markdown","44c6c51e":"markdown","4f91959d":"markdown","daf78398":"markdown","62e9dcf1":"markdown","d0cced85":"markdown"},"source":{"27fd33fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport plotly.express as px # plotting\nimport plotly.graph_objs as go\nimport random\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.manifold import TSNE\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c23d191d":"# Get the directories\nTRAIN_DIR = '\/kaggle\/input\/digit-recognizer\/train.csv'\nTEST_DIR = '\/kaggle\/input\/digit-recognizer\/test.csv'\nSUBMISSION_DIR = '\/kaggle\/input\/digit-recognizer\/sample_submission.csv'","df404739":"# Load the dataframes\ntrain_df = pd.read_csv(TRAIN_DIR)\ntest_df = pd.read_csv(TEST_DIR)","c47bf486":"# View the top 10 records\ntrain_df.head(10)","49f01478":"# Examine the shape of data\ntrain_df.shape","8bc450c0":"# Value counts of the training labels\ntrain_df[\"label\"].value_counts()","ccbe6b26":"# Plot of the values\nfig = px.histogram(train_df, x = \"label\", nbins = 10, marginal = \"violin\", \n                   opacity = 0.8, color_discrete_sequence=['indianred'])\nfig.update_layout(bargap = 0.2)\nfig.show()","e0ae3b55":"# Check for null and missing values\ntrain_df.isnull().any().describe()","54312e2c":"# Shape of the testing dataframe\ntest_df.shape","6ca1be23":"# Check for null and missing values\ntest_df.isnull().any().describe()","d71077bc":"# Get the X train, Y train and X test variables\ny_train = train_df[\"label\"].values\nX_train = train_df.drop(\"label\", axis = 1)\nX_predict = test_df\n\n# Normalize the data\nX_train = X_train \/ 255.0\nX_predict = X_predict \/ 255.0","28792644":"# Reshape and convert to a matrix of images\nX_train = X_train.values.reshape(-1, 28, 28)\nX_predict = X_predict.values.reshape(-1, 28, 28)","04fd9c0c":"# Function to randomly view some images\ndef random_view(X, title, y = None):\n    fig, axs = plt.subplots(3, 3, sharex= True, sharey = True, figsize = (10, 10))\n    fig.suptitle(title)\n    \n    for i in range(3):\n        for j in range(3):\n            n = random.randint(0, len(X))\n            axs[i][j].imshow(X[n], cmap = 'Greys')\n            if y is not None:\n                axs[i][j].set_title(y[n])\n                \n    fig.show()","83d60e7a":"# Training Images\nrandom_view(X_train, \"TRAINING IMAGES\", y_train)","0a285221":"# Testing Images\nrandom_view(X_predict, \"TESTING IMAGES\")","fef693df":"# Making it work with the SVM classifier and Dimensionality Reduction\n# Get the X train, Y train and X test variables\ny_train = train_df[\"label\"]\nX_train = train_df.drop(\"label\", axis = 1)\nX_predict = test_df\n\n# Normalize the data\nX_train = X_train \/ 255.0\nX_predict = X_predict \/ 255.0","84cdcc0e":"# Combine the data and later separate for t-SNE to work on\ntrain_index = len(X_train)\nX_train = X_train.append(X_predict)","643a8105":"# t-Distributed Stochastic Neighbor Embedding\ntsne = TSNE(n_components = 3, perplexity = 50)\nX_train = tsne.fit_transform(X_train)","352559a4":"# Visualization of t-SNE\nprojection_df = pd.DataFrame(X_train[:train_index], columns = [\"X\", \"Y\", \"Z\"])\nprojection_df[\"Label\"] = y_train\n\nfig = px.scatter_3d(projection_df, x='X', y='Y', z='Z',\n                    color='Label')\nfig.show()","ae12f1e3":"# Seperate training and prediction data\ntransformed_data_train = X_train[:train_index]\ntransformed_data_predict = X_train[train_index:]","6f4df514":"# KNN with tSNE\n# 5 seems to be giving good results\nclf = KNeighborsClassifier(n_neighbors=5)","ee1027f2":"# K Fold Cross Validation\nN = 5; score = 0\nprobs = pd.DataFrame(np.zeros((len(X_predict), N*10)),\n                     columns=['Fold_{}_Prob_{}'.format(i, j) for i in range(1, N+1) for j in range(10)])\nfprs, tprs, scores = [], [], []\n\nskf = StratifiedKFold(n_splits = N, shuffle = True)\n\nfor fold, (train_index, validation_index) in enumerate(skf.split(transformed_data_train, y_train), 1):\n    print('Fold {}\\n'.format(fold))\n    \n    # Fitting the model\n    clf.fit(transformed_data_train[train_index], y_train.iloc[train_index])\n    \n    # X_test probabilities\n    probabilities = clf.predict_proba(transformed_data_predict)\n    for i in range(10):\n        probs.loc[:, 'Fold_{}_Prob_{}'.format(fold, i)] = probabilities[:, i]\n        \n    single_score = clf.score(transformed_data_train[validation_index], y_train.iloc[validation_index])\n    score += single_score \/ N\n    \n    print(\"Fold {} Score: {}\\n\".format(fold, single_score))\n\nprint(\"Average Score: {}\".format(score))","d72556cf":"# Make predictions for the model\nprediction = pd.DataFrame()\n\nfor i in range(10):\n    prob_column = [col for col in probs.columns if col.endswith('Prob_{}'.format(i))]\n    prediction[str(i)] = probs[prob_column].sum(axis = 1) \/ N\n    \npredictions = prediction.idxmax(axis = 1)","bcaa859e":"# Make the submission\nsubmissions = pd.read_csv(SUBMISSION_DIR)\nsubmissions[\"Label\"] = predictions.astype(int)\nsubmissions.to_csv('submissions.csv', index = False)","b5d341af":"# Dimensionality Reduction\n\nFeatures provided by Dimensionality Reduction\n\n- Faster training of ML model\n- Avoiding \"Curse of Dimensionality\" problem (if it arises)","6da35e06":"## Observations\n\n- There are 42000 training examples and 28000 labels to predict\n- There are 784 pixels in an image => (28 x 28 pixel matrix)\n- 1 has the highest number of instances and 5 has the smallest number of instances","7eb3a456":"# Basic Data Exploration","53c6b1a7":"# Predictions","414d37ef":"# References\n\n1. [Intro to Dimensionality Reduction](https:\/\/www.kaggle.com\/arthurtok\/interactive-intro-to-dimensionality-reduction)\n2. [PCA using Python](https:\/\/towardsdatascience.com\/pca-using-python-scikit-learn-e653f8989e60)\n3. [Manifold Learning](https:\/\/www.youtube.com\/watch?v=j8080l9Pvic)\n4. [How to use t-SNE effectively](https:\/\/distill.pub\/2016\/misread-tsne\/)","58868514":"## Normalization","1c56822a":"## Observations\n\n- Number of components cannot be greater than 4 due to constraints of the algorithm\n- There is no transform method for TSNE, so we compute the transformations for test and train simultaneously\n- Extremely slow","44c6c51e":"# Preprocessing the Data","4f91959d":"## View some images","daf78398":"# Machine Learn the Dataset","62e9dcf1":"# Load the dataset","d0cced85":"## Reshaping"}}