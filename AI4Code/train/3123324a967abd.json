{"cell_type":{"55f6083e":"code","8dfc68cf":"code","07467af1":"code","28d98bd2":"code","01e20e80":"code","6ba3a80d":"code","f51e1bfd":"code","38d4d5f7":"code","cfa56034":"code","9b07db46":"code","cbf07c25":"code","f54b9015":"code","a0361856":"code","2c608955":"code","ca30eb60":"code","02fab227":"code","519e261d":"code","0a636c95":"code","1d2b824e":"code","55ff72cc":"code","46e9fa95":"code","52737821":"code","b75ab593":"code","a8026bde":"code","f1a32268":"code","123570a0":"code","0d187007":"code","1f887eff":"code","d501ca9e":"code","c04920bc":"markdown","9055276e":"markdown","7a550764":"markdown","46028c04":"markdown","5710d196":"markdown","bf6437ea":"markdown","f18a8910":"markdown","ca7ce743":"markdown","832cd763":"markdown"},"source":{"55f6083e":"# Import all the libraries\nimport pandas as pd\nimport numpy as np\nfrom numpy import set_printoptions","8dfc68cf":"# import dataset\ntimes = pd.read_csv(\"..\/input\/world-university-rankings\/timesData.csv\")\ntimes.head()","07467af1":"print('dtypes of times dataset:')\ntimes.dtypes\n\n# mix between float, string and integer.","28d98bd2":"print('number of NaNs per column:')\ntimes.isna().sum()\n\n# 4 columns have missing values. Female male ratio has the most with 233. ","01e20e80":"# drop university name and country because they're strings\n# drop female male ratio because 233 rows are missing\n# drop total score because it's too similar to world rank\ntimes.drop(columns=['university_name', 'country', 'female_male_ratio', 'total_score'], inplace=True)","6ba3a80d":"times.head()","f51e1bfd":"# converting string values to numeric\n# world rank values are numeric from 1 to 100 afterwards they're string like 100-150\n# convert world rank to numeric and rest is converted to NaN\n# we're only interested in top 100, top 50, top 10 so we only care about the first 100 for the binarizer\n\ntimes['world_rank'] = pd.to_numeric(times['world_rank'], errors='coerce')\n\n# fill with 101 so it's below the binarize threshold of 100\ntimes['world_rank'].fillna(101, inplace=True)\n\n# binarizer converts value to 1 if it's above the threshold\n# so we need to invert world rank i.e. make negative\ntimes['world_rank'] = (times['world_rank'] * -1)\n\n# prepare object or string columns for numeric conversion\n# Few columns had \"-\" for missing value, replace with 0\n# num students has \",\", replace with nothing \"\"\n# international students has \"%\", replace with nothing \"\"\nstr_cols = times.select_dtypes(['object']).columns\ntimes[str_cols] = times[str_cols].replace('-', 0)\ntimes['num_students'] = times['num_students'].str.replace(',', '')\ntimes['international_students'] = times['international_students'].str.replace('%', '')\n\n# convert object or string columns to numeric\ntimes[str_cols] = times[str_cols].apply(pd.to_numeric, errors='coerce', axis=1)\n\n# convert international students percentage to decimal\ntimes['international_students'] = times['international_students'] \/ 100","38d4d5f7":"# determine number of NaNs\ntimes.isna().sum()","cfa56034":"# drop remaining NaNs \ntimes.dropna(inplace=True)","9b07db46":"# check dataframe, dtypes and NaNs\nprint(times.dtypes)\nprint(times.isna().sum())\ntimes.head()","cbf07c25":"# convert times dataframe to array\ntimes_array = times.values\nX = times_array[:,1:]\ny_ = times_array[:,[0]]","f54b9015":"set_printoptions(precision=3, suppress=True)\nX[:5]","a0361856":"y_[:5]","2c608955":"# drop world_rank, not needed\ntimes.drop(columns='world_rank', inplace=True)","ca30eb60":"# create binary variable\nfrom sklearn.preprocessing import Binarizer\n\ntop_n = -50 + (-1)\n\nbinarizer=Binarizer(threshold=top_n).fit(y_)\ny_binary=binarizer.transform(y_)\n\ny_binary[:5]\n\ny_reshaped = np.ravel(y_binary)\ny_reshaped","02fab227":"# reshape using ravel() so that it works with LogisticRegression\ny_reshaped = np.ravel(y_binary)\ny_reshaped","519e261d":"times.head()","0a636c95":"# Univariate selection using Chi-squared \nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2 \n\n# feature selection (we select the 3 best)\ntest = SelectKBest(score_func=chi2, k=3)\nfit = test.fit(X,y_reshaped)\nprint(\"Scores\")\n\nprint(fit.scores_)\n\nprint(\"The 3 attributes with the highest scores are: teaching, research and num_students \")\nprint()\nprint('teaching: university score for teaching')\nprint('reserach: university score for research (volume, income and reputation)')\nprint('num_students: number of students at the university')\n\nfeatures=fit.transform(X)\nfeatures[0:5,:]","1d2b824e":"# Recursive Feature Elimiantion\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n#Logistic regression\nmodel = LogisticRegression(solver='liblinear')\n\nrfe = RFE(model, 3) #  we want to find the 3 top features\nfit = rfe.fit(X, y_reshaped)\n\nprint(f'Number of features {fit.n_features_:d}')\nprint(f'Selected features {fit.support_}')\nprint(f'Ranking of features {fit.ranking_}')\nprint()\nprint(\"Top features seem to be teaching, research and citations\")","55ff72cc":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier(n_estimators=100)\nmodel.fit(X,y_reshaped)\n\nprint(model.feature_importances_)\nprint()\nprint(\"Top features seem to be citations, research and teaching\")","46e9fa95":"top_unis = [10, 50, 100]\nunivariate = []\nrfe_ranking = []\netc_features = []\n\nfor n in top_unis:\n\n    top_n = (n + 1) * (-1)\n\n    binarizer=Binarizer(threshold=top_n).fit(y_)\n    y_binary=binarizer.transform(y_)\n\n    y_reshaped = np.ravel(y_binary)\n\n    print('*************************************************************')\n    print('Univariate Selection using Chi-Squared: top', n)\n\n    #set_printoptions(precision=3, suppress)\n\n    # feature selection (we select the 3 best)\n    test = SelectKBest(score_func=chi2, k=3)\n    fit = test.fit(X,y_reshaped)\n    print(\"Scores\")\n\n    univariate.append(fit.scores_)\n\n    features=fit.transform(X)\n    print(features[0:5,:])\n\n    print('*************************************************************')\n    print('Recursive Feature Elimination: top', n)\n    print()\n\n    model = LogisticRegression(solver='liblinear')\n\n    rfe = RFE(model, 3) #  we want to find the 3 top features\n    fit = rfe.fit(X, y_reshaped)\n\n    print(f'Number of features {fit.n_features_:d}')\n    print(f'Selected features {fit.support_}')\n    print(f'Ranking of features {fit.ranking_}')\n\n    rfe_ranking.append(fit.ranking_)\n    print()\n\n    print('*************************************************************')\n    print('ExtraTreeClassifier: top', n)\n\n    model = ExtraTreesClassifier(n_estimators=100, random_state=7)\n    model.fit(X,y_reshaped)\n\n    print(model.feature_importances_)\n    etc_features.append(model.feature_importances_)\n\nprint(times.head())\n    \nprint('top unis:', top_unis)\nprint(univariate)\nprint(rfe_ranking)\nprint(etc_features)","52737821":"times.head()","b75ab593":"# Answer for Univariate Selection. \n# First row is top 10, then top 50, then top 100\nunivariate","a8026bde":"# Answer for Recursive Feature Selection.\n# First row is top 10, then top 50, then top 100\nrfe_ranking","f1a32268":"# Answer for ExtraTreeClassifier\n# First row is top 10, then top 50, then top 100\netc_features","123570a0":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score","0d187007":"top_unis = [10, 50, 100]\ntrain_test_split_accuracy = []\nk_fold_accuracy = []\n\nfor n in top_unis:\n\n    top_n = (n + 1) * (-1)\n\n    binarizer=Binarizer(threshold=top_n).fit(y_)\n    y_binary=binarizer.transform(y_)\n\n    y_reshaped = np.ravel(y_binary)\n    \n    print('*************************************************************')\n    print('train-test-split: top', n)\n    \n    # we need to make it reproducible, so we use a seed for the pseudo-random\n    test_size = 0.3\n    seed = 7\n\n    # the actual split\n    X_train, X_test, y_train, y_test = train_test_split(X, y_reshaped, test_size=test_size, random_state=seed)\n\n    # Let's do the log regresssion\n    model = LogisticRegression(solver='liblinear')\n    model.fit(X_train,y_train)\n\n    # Now let's find the accurary with the test split\n    result = model.score(X_test, y_test)\n    train_test_split_accuracy.append(result)\n\n    print(f'Accuracy {result*100:5.3f}')\n    print()\n    \n    print('*************************************************************')\n    print('k-fold-10 validation: top', n)\n    print()\n    \n    # KFold\n    splits = 10\n    kfold = KFold(n_splits=splits, random_state=seed)\n\n    #Logistic regression\n    model = LogisticRegression(solver='liblinear')\n\n    # Obtain the performance measure - accuracy\n    results = cross_val_score(model, X, y_reshaped, cv=kfold)\n    k_fold_accuracy.append(results.mean())\n    \n    print(f'Logistic regression, k-fold {splits:d} - Accuracy {results.mean()*100:5.3f}% ({results.std()*100:5.3f}%)')\n    print()\n    \n    \ntrain_test_accuracy = [ '%.3f' % elem for elem in train_test_split_accuracy]\nkfold_accuracy = [ '%.3f' % elem for elem in k_fold_accuracy]\n\nprint('Top unis: ', top_unis)\nprint(train_test_accuracy)\nprint(kfold_accuracy)\nprint('Accuracy decreases as the number of universities to be classified increases')","1f887eff":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","d501ca9e":"top_unis = [10, 50, 100]\nscoring = ['accuracy', 'neg_log_loss', 'roc_auc']\nk_fold_accuracy = []\n\nfor n in top_unis:\n\n    top_n = (n + 1) * (-1)\n\n    binarizer=Binarizer(threshold=top_n).fit(y_)\n    y_binary=binarizer.transform(y_)\n\n    y_reshaped = np.ravel(y_binary)\n\n    print('*************************************************************')\n    \n    for score in scoring:\n        \n        print('*************************************************************')\n        print(score, ', top', n)\n\n        # StratifiedKFold because top10 with kfold causes an error (bug)\n        splits = 10\n        skfold = StratifiedKFold(n_splits=splits, random_state=7)\n\n\n        #Logistic regression\n        model = LogisticRegression(solver='liblinear')\n\n        # Obtain the performance measure - accuracy\n        results = cross_val_score(model, X, y_reshaped, scoring=score, cv=skfold)\n\n        print(score, f': {results.mean():.3f}')\n        print()\n\n    print('*************************************************************')\n    print('Confusion Matrix, top', n)\n    \n    test_size=0.3\n    seed=7\n\n    X_train, X_test, Y_train, Y_test = train_test_split(X, y_reshaped, test_size=test_size, random_state=seed)\n\n    model = LogisticRegression(solver='liblinear')\n    log_reg = model.fit(X_train, Y_train)\n\n    Y_predicted = log_reg.predict(X_test)\n\n    c_matrix=confusion_matrix(Y_test, Y_predicted)\n\n    print(c_matrix)\n\n    print()\n    print(f'Accuracy {model.score(X_test, Y_test)*100:.3f}')\n    print(f'Accuracy check with conf. matrix {(c_matrix[0,0]+c_matrix[1,1])\/c_matrix.sum()*100:.3f}')\n    print()\n    \n    print('*************************************************************')\n    print('Classification Report, top', n)    \n    \n    report = classification_report(Y_test, Y_predicted, digits=3)\n    \n    print(f'Accuracy {model.score(X_test, Y_test)*100:.3f}')\n    print()\n    print(report)\n\nprint('All the scores decrease as the number of universities in the group to predict increases')","c04920bc":"# Recursive Feature Elimination using LogisticRegression","9055276e":"# Univariate selection using Chi-squared","7a550764":"# Model evaluation ","46028c04":"# Pre-processing","5710d196":"# Ranking feature importance using ExtraTreeClassifier","bf6437ea":"# Exploratory Analysis","f18a8910":"# Metrics evaluation using StratifiedKFold","ca7ce743":"##\u00a0train-test-split and k-fold-10 validation for top 10, top 50 and top 100","832cd763":"## Univariate, Recursive Feature Elimination and ExtraTreeClassifier for Top 10, Top 50 and Top 100"}}