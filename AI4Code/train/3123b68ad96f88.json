{"cell_type":{"3987a6e8":"code","7d8b42a7":"code","88d891ec":"code","8f00ad09":"code","556118c0":"code","95a35990":"code","67d6b95d":"code","c89d0b39":"code","f8de425a":"code","11af0af3":"code","eed667d0":"code","c6a42240":"code","ce41226c":"code","368374a9":"code","79a6da73":"code","7cf4c6ff":"code","27e6d58e":"code","113b8e71":"code","0c82f4e7":"code","81e4b8a2":"code","db62ee9c":"code","752968fe":"code","9cae09d9":"code","16a875f3":"code","5d0d9c4e":"code","d1d85d7f":"code","7a192f49":"code","9e5149fd":"code","0968a00c":"code","27e45112":"code","218bfd45":"code","8ea21e81":"code","7d408eb3":"markdown","a24ed759":"markdown","b41789cd":"markdown","e7f61ead":"markdown","4e5926a3":"markdown","8941a03a":"markdown","87d22d2d":"markdown","fca55965":"markdown","34de9073":"markdown","d8f478d9":"markdown","3d710dc7":"markdown","4c05ba9a":"markdown","9047bdf9":"markdown","8fd8deb3":"markdown","6579db60":"markdown","76d25f4d":"markdown","1ac2b6ca":"markdown","77eca30f":"markdown","586dd6f1":"markdown","78841250":"markdown","c9390c80":"markdown","ad2977f4":"markdown","f6fa0e8b":"markdown"},"source":{"3987a6e8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# for splitting data to train and test partition\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\n\n# Classifier for modelling\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Evaluates model\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n\n# Sample data provided by sklearn\nfrom sklearn.datasets import load_iris","7d8b42a7":"# load data from scikit learn datasets\niris = load_iris()\nprint(iris.DESCR)\ndf = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n                     columns= iris['feature_names'] + ['target'])\ndf.head()","88d891ec":"# observe data shape and target unique values\nprint(\"Data frame shape :\", df.shape)\nprint(\"target unique values :\", df['target'].unique())","8f00ad09":"# checking null value\nfor attribute in df.columns:\n    print(\"column\", attribute, \"null \\t:\", df[attribute].isnull().sum())","556118c0":"# load data from external file\ntennis = pd.read_csv('..\/input\/weather.nominal.csv')\ntennis.head()","95a35990":"# Gaussian\nclf_gnb = GaussianNB()\n\nclf_gnb.fit(iris.data, iris.target)","67d6b95d":"# fitting\nclf_tree = tree.DecisionTreeClassifier(criterion='entropy', max_leaf_nodes=6)\n\nclf_tree.fit(iris.data, iris.target)","c89d0b39":"# visualize model\nimport graphviz\ndot_data = tree.export_graphviz(clf_tree, out_file=None, \n                         feature_names=iris.feature_names,  \n                         class_names=iris.target_names,  \n                         filled=True, rounded=True,  \n                         special_characters=True)  \ngraph = graphviz.Source(dot_data) \ngraph\n# graph.render(\"iris\") ","f8de425a":"clf_neigh = KNeighborsClassifier(n_neighbors=3)\n\nclf_neigh.fit(iris.data, iris.target)","11af0af3":"clf_neuron = MLPClassifier(hidden_layer_sizes=(5, 2), random_state=42, max_iter=1000, solver='lbfgs')\n\nclf_neuron.fit(iris.data, iris.target)","eed667d0":"# Spltting\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1, random_state=42)\nclass_names = iris.target_names","c6a42240":"df.head(5)","ce41226c":"#standard way\nfeats = [f for f in df.columns if f not in ['target']]\nprint(df[feats].head(3))\nprint(df['target'].head(3))\n\n# Spltting\nX_train, X_test, y_train, y_test = train_test_split(df[feats], iris.target, test_size=0.1, random_state=42)","368374a9":"# plotting confusion matrix\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","79a6da73":"# Gaussian\nclf_gnb = GaussianNB()\n\nclf_gnb.fit(X_train, y_train)\n\npred_gnb = clf_gnb.predict(X_test)\nprint(\"accuracy score \\t=\", accuracy_score(y_test, pred_gnb, normalize=True))\nprint(\"recall score \\t=\", recall_score(y_test, pred_gnb, average='micro'))\nprint(\"f1 score \\t=\", f1_score(y_test, pred_gnb, average='micro'))","7cf4c6ff":"cnf_matrix_gnb = confusion_matrix(y_test, pred_gnb)\nnp.set_printoptions(precision=2)\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix_gnb, classes=class_names,normalize=True,\n                      title='GaussianNB Confusion Matrix')\n\nplt.show()","27e6d58e":"# fitting\nclf_tree = tree.DecisionTreeClassifier(criterion='entropy', max_leaf_nodes=3)\n\nclf_tree.fit(X_train, y_train)\n\npred_tree = clf_tree.predict(X_test)\nprint(\"accuracy score =\", accuracy_score(y_test, pred_tree, normalize=True))\nprint(\"recall score \\t=\", recall_score(y_test, pred_tree, average='micro'))\nprint(\"f1 score \\t=\", f1_score(y_test, pred_tree, average='micro'))","113b8e71":"cnf_matrix_tree = confusion_matrix(y_test, pred_tree)\nnp.set_printoptions(precision=2)\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix_tree, classes=class_names,normalize=True,\n                      title='DecisionTree Confusion Matrix')\n\nplt.show()","0c82f4e7":"clf_neigh = KNeighborsClassifier(n_neighbors=3)\n\nclf_neigh.fit(X_train, y_train)\n\npred_neigh = clf_neigh.predict(X_test)\nprint(\"accuracy score \\t=\", accuracy_score(y_test, pred_neigh, normalize=True))\nprint(\"recall score \\t=\", recall_score(y_test, pred_neigh, average='micro'))\nprint(\"f1 score \\t=\", f1_score(y_test, pred_neigh, average='micro'))","81e4b8a2":"cnf_matrix_neigh = confusion_matrix(y_test, pred_neigh)\nnp.set_printoptions(precision=2)\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix_neigh, classes=class_names,normalize=True,\n                      title='kNN Confusion Matrix')\n\nplt.show()","db62ee9c":"clf_neuron = MLPClassifier(hidden_layer_sizes=(5, 2), random_state=42, max_iter=1000, solver='lbfgs')\n\nclf_neuron.fit(X_train, y_train)\n\npred_neuron = clf_neuron.predict(X_test)\nprint(\"accuracy score \\t=\", accuracy_score(y_test, pred_neuron, normalize=True))\nprint(\"recall score \\t=\", recall_score(y_test, pred_gnb, average='micro'))\nprint(\"f1 score \\t=\", f1_score(y_test, pred_gnb, average='micro'))","752968fe":"cnf_matrix_neuron = confusion_matrix(y_test, pred_neuron)\nnp.set_printoptions(precision=2)\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix_neuron, classes=class_names,normalize=True,\n                      title='NeuralNetwork Confusion Matrix')\n\nplt.show()","9cae09d9":"## splitting and cross validate\nfrom sklearn.model_selection import KFold, cross_val_score, cross_val_predict, cross_validate\n\nX = iris.data\ny = iris.target\nkf = KFold(n_splits=10, random_state=False)\nprint(kf.get_n_splits())\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]","16a875f3":"# Gaussian\nclf_gnb = GaussianNB()\n\nclf_gnb.fit(X_train, y_train)\n\ngnb_cv_score = cross_val_score(clf_gnb, X_train, y_train, cv = 10)\nprint(\"cross validate score =\", gnb_cv_score)\npred_gnb = cross_val_predict(clf_gnb, X_test, y_test, cv=10)\nprint(\"accuracy score =\", accuracy_score(y_test, pred_gnb, normalize=True))","5d0d9c4e":"# fitting\nclf_tree = tree.DecisionTreeClassifier(criterion='entropy', max_leaf_nodes=6)\n\nclf_tree.fit(X_train, y_train)\n\ntree_cv_score = cross_val_score(clf_tree, X_train, y_train, cv = 10)\nprint(\"cross validate score =\", tree_cv_score)\npred_tree = cross_val_predict(clf_tree, X_test, y_test, cv=10)\nprint(\"accuracy score =\", accuracy_score(y_test, pred_tree, normalize=True))","d1d85d7f":"clf_neigh = KNeighborsClassifier(n_neighbors=3)\n\nclf_neigh.fit(X_train, y_train)\n\nneigh_cv_score = cross_val_score(clf_neigh, X_train, y_train, cv = 10)\nprint(\"cross validate score =\", neigh_cv_score)\npred_neigh = cross_val_predict(clf_neigh, X_test, y_test, cv=10)\nprint(\"accuracy score =\", accuracy_score(y_test, pred_neigh, normalize=True))","7a192f49":"clf_neuron = MLPClassifier(hidden_layer_sizes=(5, 2), random_state=42, max_iter=1000, solver='lbfgs')\n\nclf_neuron.fit(X_train, y_train)\n\nneuron_cv_score = cross_val_score(clf_neuron, X_train, y_train, cv = 10)\nprint(\"cross validate score =\", neuron_cv_score)\npred_neuron = cross_val_predict(clf_neuron, X_test, y_test, cv=10)\nprint(\"accuracy score =\", accuracy_score(y_test, pred_neuron, normalize=True))","9e5149fd":"# Save the model using Pickle Library\nimport pickle\n\nmodels = []\nmodels.append(clf_gnb)\nmodels.append(clf_tree)\nmodels.append(clf_neigh)\nmodels.append(clf_neuron)\n\npkl_filename = 'model-iris.pkl'\nwith open(pkl_filename, 'wb') as file:  \n    for model in models:\n        pickle.dump(model, file)","0968a00c":"models = []\npkl_filename = 'model-iris.pkl'\nwith open(pkl_filename, 'rb') as file:\n    while True:\n        try:\n            models.append(pickle.load(file))\n        except EOFError:\n            break","27e45112":"loaded_gnb = models[0]\nloaded_tree = models[1]\nloaded_neigh = models[2]\nloaded_neuron = models[3]","218bfd45":"import random as rd\n\n# get extreme value from each attribute, and make a random instance\nmax_v = np.amax(X, axis = 0)\nmin_v = np.amin(X, axis = 0)\nnew_instance = [round(rd.uniform(min_v[0],max_v[0]),1),\n               round(rd.uniform(min_v[1],max_v[1]),1),\n               round(rd.uniform(min_v[2],max_v[2]),1),\n               round(rd.uniform(min_v[3],max_v[3]),1)]\nprint(new_instance)","8ea21e81":"# predicting the new instance \nnew_i_pred1 = loaded_gnb.predict([new_instance])\nprint('new instance prediction using Naive bayes:', iris.target_names[new_i_pred1])\n\nnew_i_pred2 = loaded_tree.predict([new_instance])\nprint('new instance prediction using DecisionTree:', iris.target_names[new_i_pred2])\n\nnew_i_pred3 = loaded_neigh.predict([new_instance])\nprint('new instance prediction using kNN:', iris.target_names[new_i_pred3])\n\nnew_i_pred4 = loaded_neuron.predict([new_instance])\nprint('new instance prediction using NeuralNetwork:', iris.target_names[new_i_pred4])\n","7d408eb3":"### Naive Bayes","a24ed759":"# Import Modules","b41789cd":"### Decision Tree","e7f61ead":"### Decision Tree","4e5926a3":"### Neural Network (MLP)","8941a03a":"### Naive Bayes","87d22d2d":"<h1><center> Introduction to Scikit-Learn and Pandas <\/center><\/h1>\n<br><br>\n+ Scikit-learn <br>\n[Scikit-learn](http:\/\/scikit-learn.org\/stable\/) (formerly scikits.learn) is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\n<br>\n+ Matplotlib <br>\n[Matplotlib](https:\/\/matplotlib.org\/) is a plotting library for the Python programming language and its numerical mathematics extension NumPy. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK+. There is also a procedural \"pylab\" interface based on a state machine (like OpenGL), designed to closely resemble that of MATLAB, though its use is discouraged.\n<br>\n+ Pandas <br>\n[Pandas](https:\/\/pandas.pydata.org\/) (Python Data Analysis Library) is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. pandas is a NumFOCUS sponsored project. This will help ensure the success of development of pandas as a world-class open-source project, and makes it possible to donate to the project.","fca55965":"### k-Nearest Neighbors (kNN)","34de9073":"## Save the Model","d8f478d9":"### Naive Bayes","3d710dc7":"## Load Model","4c05ba9a":"In this notebook, we will use 3 modelling method: \n* Full Training : all of the sample data will be used as training data. Prediction is not a part of this method\n* Hold-out : split the data sample to two parts (training data and test data)\n* Cross-validation : the data will be split randomly into *k* group, one group will be used as train data, and the rest are used as test data","9047bdf9":"## Adding an Instance","8fd8deb3":"### Neural Network (MLP)","6579db60":"### Decision Tree","76d25f4d":"## 1. Full training","1ac2b6ca":"# Load, Preprocessing and Preview Data","77eca30f":"## 2. Hold-Out","586dd6f1":"### Neural Network (MLP)","78841250":"### kNN","c9390c80":"### kNN","ad2977f4":"## 3. Cross-Validation","f6fa0e8b":"# Modelling"}}