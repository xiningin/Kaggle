{"cell_type":{"e3a9a85b":"code","65f9e7ba":"code","2b2e7fe2":"code","6bcdf35a":"code","1c5ab2fd":"code","a53afd12":"code","6d630d3d":"code","6202e116":"code","617e5e04":"code","b40bd6c3":"code","2aa05f66":"code","ad1fe844":"code","1eedbc46":"code","077d8779":"code","665618ea":"code","dce0f14f":"code","9c24e023":"code","4c72ea5a":"code","211983b0":"code","09fa7b3d":"code","ae6a0d91":"code","9e1833c1":"code","dc3c7993":"code","1a44bff9":"code","71e0cea9":"code","9a98d970":"code","c785ec09":"code","62d89d24":"code","63dd6977":"code","615874b8":"code","c3138870":"code","2f954bfe":"code","6d9d8fa5":"code","6705d84d":"markdown"},"source":{"e3a9a85b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        \n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","65f9e7ba":"import torch\nfrom transformers import T5ForConditionalGeneration,T5Tokenizer\nimport ast\n\ndef set_seed(seed):\n  torch.manual_seed(seed)\n  if torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\nmodel = T5ForConditionalGeneration.from_pretrained('\/kaggle\/input\/generatewitht5\/result\/')\ntokenizer = T5Tokenizer.from_pretrained('t5-base')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print (\"device \",device)\nmodel = model.to(device)","2b2e7fe2":"def beam_search_decoding (inp_ids,attn_mask):\n        beam_output = model.generate(input_ids=inp_ids,\n                                         attention_mask=attn_mask,\n                                         max_length=512,\n                                       num_beams=20,\n                                       num_return_sequences=1,\n                                       no_repeat_ngram_size=2,\n                                       early_stopping=True\n                                       )\n        Questions = [tokenizer.decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True) for out in beam_output]\n        return [Question.strip().capitalize() for Question in Questions]","6bcdf35a":"def t5_answer(input_text):\n    con = \"Input: %s <\/s>\" %(input_text)\n    encoding = tokenizer.encode_plus(con, return_tensors=\"pt\")\n    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n    output = beam_search_decoding (input_ids, attention_masks)\n    return output[0]","1c5ab2fd":"text = \"Sachin is a great batsman and a\"","a53afd12":"t5_answer(text)","6d630d3d":"question = \"Coronavirus is a deadly disease\"","6202e116":"t5_answer(question)","617e5e04":"def greedy_decoding (inp_ids,attn_mask):\n    greedy_output = model.generate(input_ids=inp_ids, attention_mask=attn_mask, max_length=256)\n    Question =  tokenizer.decode(greedy_output[0], skip_special_tokens=True,clean_up_tokenization_spaces=True)\n    return Question.strip().capitalize()","b40bd6c3":"def t5_greedy(input_text):\n    con = \"Input: %s <\/s>\" %(input_text)\n    encoding = tokenizer.encode_plus(con, return_tensors=\"pt\")\n    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n    output = greedy_decoding (input_ids, attention_masks)\n    return output","2aa05f66":"t5_answer(question)","ad1fe844":"def top_kp (inp_ids,attn_mask):\n        beam_output = model.generate(input_ids=inp_ids,\n                                         attention_mask=attn_mask,\n                                         max_length=200,\n                                      do_sample=True, \n                                     top_k=0\n                                       )\n        Questions = [tokenizer.decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True) for out in beam_output]\n        return [Question.strip().capitalize() for Question in Questions]","1eedbc46":"def t5_topkp(input_text):\n    con = \"Input: %s <\/s>\" %(input_text)\n    encoding = tokenizer.encode_plus(con, return_tensors=\"pt\")\n    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n    output = top_kp (input_ids, attention_masks)\n    return output[0]","077d8779":"question","665618ea":"t5_topkp(question)","dce0f14f":"def top_kp2 (inp_ids,attn_mask):\n        beam_output = model.generate(input_ids=inp_ids,\n                                         attention_mask=attn_mask,\n                                        do_sample=True, \n    max_length=220, \n    top_k=0, \n    temperature=0.7\n                                       )\n        Questions = [tokenizer.decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True) for out in beam_output]\n        return [Question.strip().capitalize() for Question in Questions]","9c24e023":"def t5_topkp2(input_text):\n    con = \"Input: %s <\/s>\" %(input_text)\n    encoding = tokenizer.encode_plus(con, return_tensors=\"pt\")\n    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n    output = top_kp2 (input_ids, attention_masks)\n    return output[0]","4c72ea5a":"t5_topkp2(question)","211983b0":"def top_kp3 (inp_ids,attn_mask):\n        beam_output = model.generate(input_ids=inp_ids,\n                                         attention_mask=attn_mask,\n                                        do_sample=True, \n    max_length=220, \n    top_k=50 \n                                       )\n        Questions = [tokenizer.decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True) for out in beam_output]\n        return [Question.strip().capitalize() for Question in Questions]","09fa7b3d":"def t5_topkp3(input_text):\n    con = \"Input: %s <\/s>\" %(input_text)\n    encoding = tokenizer.encode_plus(con, return_tensors=\"pt\")\n    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n    output = top_kp3 (input_ids, attention_masks)\n    return output[0]","ae6a0d91":"t5_topkp3(question)","9e1833c1":"def top_kp4 (inp_ids,attn_mask):\n        beam_output = model.generate(input_ids=inp_ids,\n                                         attention_mask=attn_mask,\n                                        do_sample=True, \n    max_length=220, \n     top_p=0.92, \n    top_k=0\n                                       )\n        Questions = [tokenizer.decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True) for out in beam_output]\n        return [Question.strip().capitalize() for Question in Questions]","dc3c7993":"def t5_topkp4(input_text):\n    con = \"Input: %s <\/s>\" %(input_text)\n    encoding = tokenizer.encode_plus(con, return_tensors=\"pt\")\n    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n    output = top_kp4 (input_ids, attention_masks)\n    return output[0]","1a44bff9":"t5_topkp4(question)","71e0cea9":"def top_kp5 (inp_ids,attn_mask):\n        beam_output = model.generate(input_ids=inp_ids,\n                                         attention_mask=attn_mask,\n                                        do_sample=True, \n    max_length=300, \n     top_p=0.92, \n    top_k=50,\n                                     num_return_sequences=3\n                                     \n                                       )\n        Questions = [tokenizer.decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True) for out in beam_output]\n        return [Question.strip().capitalize() for Question in Questions]","9a98d970":"def t5_topkp5(input_text):\n    con = \"Input: %s <\/s>\" %(input_text)\n    encoding = tokenizer.encode_plus(con, return_tensors=\"pt\")\n    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n    output = top_kp5 (input_ids, attention_masks)\n    return output","c785ec09":"t5_topkp5(question)","62d89d24":"question = \"Links In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"","63dd6977":"t5_topkp5(question)","615874b8":"text = \"Apple is one of the largest\"","c3138870":"t5_topkp5(text)","2f954bfe":"# import tensorflow as tf\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n# add the EOS token as PAD token to avoid warnings\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)","6d9d8fa5":"# encode context the generation is conditioned on\ninput_ids = tokenizer.encode('Coronavirus is a deadly disease', return_tensors='pt')\n\n# generate text until the output length (which includes the context length) reaches 50\nsample_outputs = model.generate(\n    input_ids,\n    do_sample=True, \n    max_length=220, \n    top_k=50, \n    top_p=0.95, \n    num_return_sequences=3\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"Output:\\n\" + 100 * '-')\n    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","6705d84d":"This notebook uses T5 pretrained model on** COSMOS+SQUAD+QUAC dataset**. Answers are to the point but are accurate and address the interdependancy of the sentences."}}