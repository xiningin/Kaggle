{"cell_type":{"2d31ae2e":"code","77ffd5a5":"code","3a916925":"code","2b633834":"code","6b0d627a":"code","5e1336f5":"code","2208366b":"code","66c0cef9":"code","f1e1bdd2":"code","042d7a76":"code","c1899828":"code","c5e0a6ce":"code","f3cab8bf":"code","d92caa58":"code","2b41b9dd":"code","b7887b1a":"code","35d13f89":"code","a3976c94":"code","0356aeec":"code","b3de5796":"code","11e981f8":"code","f0166cef":"code","2db6ffab":"code","27e61c10":"code","0802c617":"code","0f9572c5":"code","c39175ae":"code","46c84bc2":"code","4b7b8a6a":"code","72482741":"code","70e0845f":"code","05eb1b7a":"code","eb390d0e":"code","5ff5e5fa":"code","4ee2cb8f":"code","7535b0ee":"code","f3297667":"code","1f33b229":"code","ced215b5":"code","b5eca69d":"code","fd132885":"code","5102b116":"code","80992120":"code","93b23c05":"code","f5a4dff1":"code","b979b397":"code","1610b8e0":"code","3ed4ba31":"code","fe12096f":"code","9c10dfb5":"code","0204db42":"code","374f8566":"code","589b11f3":"code","71b2fa58":"code","a10755c3":"markdown","58397110":"markdown","3749e3d9":"markdown","0ad91dfd":"markdown","7928cbc4":"markdown","d6f694ba":"markdown","66d56c21":"markdown","e3670817":"markdown","1ab1600c":"markdown","c5247006":"markdown","85f56f3d":"markdown","dc7f9621":"markdown","a3af6b2e":"markdown","daadbf45":"markdown","fafd4375":"markdown","c03d14be":"markdown","85b52183":"markdown","cc569109":"markdown","545d36d4":"markdown","9f0c76e5":"markdown","fbbd3ff8":"markdown","44ed6ca4":"markdown","2d7d0383":"markdown","0a691ce4":"markdown","1e3b1f10":"markdown","391fda93":"markdown","9562b413":"markdown"},"source":{"2d31ae2e":"inputs = [1,2,3]\nweights = [0.2, 0.8, -0.5]\nbias = 2\n\noutput = ( inputs[0]*weights[0]+\n         inputs[1]*weights[1]+\n         inputs[2]*weights[2] + bias )\nprint(output)","77ffd5a5":"inputs = [1,2,3,2.5]\nweights = [0.2, 0.8, -0.5,1.0]\nbias = 2\n\noutput = ( inputs[0]*weights[0]+\n         inputs[1]*weights[1]+\n         inputs[2]*weights[2]+\n         inputs[3]*weights[3]+ bias )\nprint(output)","3a916925":"inputs = [1,2,3,2.5]\nweights0 = [0.2, 0.8, -0.5,1.0]\nweights1 = [0.3, 0.4, -0.6,1.0]\nweights2 = [0.4, 0.1, -0.5,1.0]\n\nbias0 = 2\nbias1 = -1.0\nbias2 = 3\n\noutput = (inputs[0]*weights0[0]+inputs[1]*weights0[1]+inputs[2]*weights0[2]+inputs[3]*weights0[3]+ bias0,\n          inputs[0]*weights1[0]+inputs[1]*weights1[1]+inputs[2]*weights1[2]+inputs[3]*weights1[3]+ bias1,\n          inputs[0]*weights2[0]+inputs[1]*weights2[1]+inputs[2]*weights2[2]+inputs[3]*weights2[3]+ bias2\n         )\nprint(output)","2b633834":"inputs = [1,2,3,2.5]\nweights = [[0.2, 0.8, -0.5,1.0],\n           [0.3, 0.4, -0.6,1.0],\n           [0.4, 0.1, -0.5,1.0]]\nbiases = [2,-1,3]\n\nlayer_outputs = []\n\nfor neuron_weights,bias in zip(weights,biases):\n    output = 0\n    \n    for input,weight in zip(inputs,neuron_weights):\n        output+= input*weight\n    \n    #add bias\n    output+= bias\n    \n    #add to final\n    layer_outputs.append(output)\n\n\nprint(layer_outputs)","6b0d627a":"import numpy as np\n\ninputs = [1,2,3]\nweights = [0.2, 0.8, -0.5]\nbias = 2\n\noutput = np.dot(inputs,weights) + bias\nprint(output)","5e1336f5":"inputs = [1,2,3,2.5]\nweights = [[0.2, 0.8, -0.5,1.0],\n           [0.3, 0.4, -0.6,1.0],\n           [0.4, 0.1, -0.5,1.0]]\nbiases = [2,-1,3]\n\nlayer_outputs = np.dot(weights,inputs) + bias\nprint(layer_outputs)","2208366b":"inputs = [1,2,3,2.5]\nweights = [[0.2, 0.8, -0.5,1.0],\n           [0.3, 0.4, -0.6,1.0],\n           [0.4, 0.1, -0.5,1.0]]\nbiases = [2,-1,3]\n\nlayer_outputs = np.dot(inputs,weights) + bias\nprint(layer_outputs)\n\n","66c0cef9":"inputs = [1,2,3]\nweights = [[0.2, 0.8, -0.5],\n           [0.3, 0.4, -0.6],\n           [0.4, 0.1, -0.5]]\nbiases = [2,-1,3]\n\nlayer_outputs = np.dot(inputs,weights) + bias\nprint(layer_outputs)","f1e1bdd2":"a = np.array([1,2,3])\nb = np.array([10,11,12])\n\noutput = np.dot(a,b)\nprint(output)","042d7a76":"import numpy as np\n\ninputs = [[1.0, 2.0, 3.0, 2.5],\n          [2.0,5.0,-1.0,2.0],\n          [-1\/5, 2.7, 3.3, -0.8]]\n\nweights = [[0.2, 0.8, -0.5, 1.0],\n           [0.5, -0.91, 0.26, -0.5],\n           [-0.26, -0.27, 0.17, 0.87]]\n\nbiases  = [2.0, 3.0, 0.5]\n\noutput = np.dot(inputs, np.array(weights).T) + biases\n\nprint(output)","c1899828":"import numpy as np\n\ninputs = [1, 2, 3]\n\nweights = [[0.2, 0.8, -0.5 ],\n           [0.4,0.8,-0.6],\n           [0.9, 0.4, 0.8]]\n\nbiases = [2.0, -1.0, 0]\n\nweights2 = [[ 0.1, -0.14, 0.5],\n            [-0.5, 0.12, -0.33],\n            [-0.44, 0.73, -0.13]]\n\nbiases2 = [-1,2, -0.5]\n\nlayer_output = np.dot(inputs,weights) + biases\n\nprint(layer_output)","c5e0a6ce":"import numpy as np\n\ninputs = [[1.0, 2.0, 3.0, 2.5],\n          [2.0,5.0,-1.0,2.0],\n          [-1\/5, 2.7, 3.3, -0.8]]\n\nweights = [[0.2, 0.8, -0.5, 1.0],\n           [0.5, -0.91, 0.26, -0.5],\n           [-0.26, -0.27, 0.17, 0.87]]\n\nbiases = [2.0, -1.0, 0]\n\nweights2 = [[ 0.1, -0.14, 0.5],\n            [-0.5, 0.12, -0.33],\n            [-0.44, 0.73, -0.13]]\n\nbiases2 = [-1,2, -0.5]\n\nlayer1_output = np.dot(inputs, np.array(weights).T) + biases\n\nlayer2_output = np.dot(layer1_output, np.array(weights2).T) + biases2\n\nprint(layer2_output)","f3cab8bf":"!pip install nnfs","d92caa58":"import nnfs\nnnfs.init()","2b41b9dd":"import matplotlib.pyplot as plt\nfrom nnfs.datasets import spiral_data\nX,y = spiral_data(samples=100, classes=3)\nplt.scatter(X[:,0], X[:,1])\nplt.show()","b7887b1a":"import matplotlib.pyplot as plt\nfrom nnfs.datasets import spiral_data\nX,y = spiral_data(samples=100, classes=3)\nplt.scatter(X[:,0], X[:,1], c = y, cmap='brg')\nplt.show()","35d13f89":"import numpy as np \nimport nnfs\nfrom nnfs.datasets import spiral_data\n\nnnfs.init()\n#Dense Layer\nclass Layer_Dense:\n\n  def __init__(self, n_inputs, n_neurons):\n    #initialize weight and biases\n    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n    self.biases = np.zeros((1,n_neurons))\n\n  # forwards pass\n  def forward(self, inputs):\n    self.output = np.dot(inputs, self.weights) + self.biases\n\n# test class\n\nX,y = spiral_data(samples = 100, classes=3)\n\n#create the layer\ndense1 = Layer_Dense(2, 3)\n\n#perform a forward pass\ndense1.forward(X)\n\n# see output\nprint(dense1.output[:5])","a3976c94":"import numpy as np\n\ninputs = [0,2,-1,3.3,-2.7,1.1,2.2,-100]\noutput = np.maximum(0, inputs)\nprint(output)","0356aeec":"#Relu Activation Class\nclass Activation_Relu:\n    \n    #forward pass\n    def forward(self, inputs):\n        #calculate max of 0, input values\n        self.output = np.maximum(0, inputs)","b3de5796":"#example \nimport numpy as np \nimport nnfs\nfrom nnfs.datasets import spiral_data\n\nnnfs.init()\n#Dense Layer\nclass Layer_Dense:\n\n  def __init__(self, n_inputs, n_neurons):\n    #initialize weight and biases\n    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n    self.biases = np.zeros((1,n_neurons))\n\n  # forwards pass\n  def forward(self, inputs):\n    self.output = np.dot(inputs, self.weights) + self.biases\n\n# test class\n\nX,y = spiral_data(samples = 100, classes=3)\n\n#create the layer\ndense1 = Layer_Dense(2, 3)\nactivation1 = Activation_Relu()\n\n#perform a forward pass\ndense1.forward(X)\n\n#perform activation of Relu\nactivation1.forward(dense1.output)\n\n\n# see output\nprint(activation1.output[:5])","11e981f8":"#Using e^x because this function converts negative values to  positive values\n#Min value = 0\/ Max value = + infinity as e^x is monotonic funtion","f0166cef":"layer_outputs = [4.8, 1.21, 2.385]\n\n# need to do e^x\n# e = 2.71828182846\n\nexp_values = np.exp(layer_outputs)\nprint(exp_values)","2db6ffab":"layer_outputs = [4.8, 1.21, 2.385]\n\n# need to do e^x\n# e = 2.71828182846\n\nexp_values = np.exp(layer_outputs)\nprint(exp_values)\n\n# now normalize them \nnorm_values = exp_values \/ np.sum(exp_values)\nprint(norm_values)\n\nprint(\"sum of normalized values: \", np.sum(norm_values))","27e61c10":"layer_outputs = np.array([[4.8, 1.21, 2.385],\n                          [8.9, -1.81, 0.2],\n                          [1.41, 1.051, 0.026]])\n\nprint(\"sum without axis\", np.sum(layer_outputs))","0802c617":"print(\"sum with axis = 0 \", np.sum(layer_outputs, axis = 0))","0f9572c5":"print(\"sum with axis = 1 \", np.sum(layer_outputs, axis = 1))","c39175ae":"# to simplify to single value per sample, use keep_dims\nprint(\"sum with axis = 1 \", np.sum(layer_outputs, axis = 1, keepdims=True))","46c84bc2":"class Activation_Softmax:\n    \n    def forward(self, inputs):\n        \n        e = np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n        \n        #normalize\n        probs = e\/np.sum(e,axis=1,keepdims=True)\n        \n        self.output = probs","4b7b8a6a":"# To prevend exploding values let's substract the max value from each array to retain the values between (-infinity and 1)","72482741":"#Softmax Activation\nclass Activation_Softmax:\n\n  # forward pass \n  def forward(self, inputs):\n\n    exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims=True))\n\n    #normalize\n    probs = exp_values \/ np.sum(exp_values, axis = 1, keepdims=True)\n\n    self.output = probs\n  ","70e0845f":"np.exp(-100)","05eb1b7a":"np.exp(-10)","eb390d0e":"np.exp(0)","5ff5e5fa":"np.exp(10)","4ee2cb8f":"np.exp(100)","7535b0ee":"np.exp(1000)","f3297667":"softmax = Activation_Softmax()\n\ninput = [[1,2,3]]\n\nsoftmax.forward(input)\nprint(softmax.output)","1f33b229":"input = [[-2,-1,0]]\n\nsoftmax.forward(input)\nprint(softmax.output)","ced215b5":"input = [[-2,-1,0]]\n\nsoftmax.forward(input)\nprint(softmax.output)","b5eca69d":"#example \nimport numpy as np \nimport nnfs\nfrom nnfs.datasets import spiral_data\n\nnnfs.init()\n#Dense Layer\nclass Layer_Dense:\n\n  def __init__(self, n_inputs, n_neurons):\n    #initialize weight and biases\n    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n    self.biases = np.zeros((1,n_neurons))\n\n  # forwards pass\n  def forward(self, inputs):\n    self.output = np.dot(inputs, self.weights) + self.biases\n\n# test class\n\nX,y = spiral_data(samples = 100, classes=3)\n\n#create the layer\ndense1 = Layer_Dense(2, 3)\nactivation1 = Activation_Relu()\nactivation2 = Activation_Softmax()\n\n#perform a forward pass\ndense1.forward(X)\n\n#perform activation of Relu\nactivation1.forward(dense1.output)\n\n#perform softmax\nactivation2.forward(activation1.output)\n\n# see output\nprint(activation2.output[:5])","fd132885":"\n\nimport math\noutput = [0.7, 0.1 , 0.2]\n\ntarget = [1, 0, 0 ]\n\n\nloss = -(math.log(output[0]) * target[0] + \n                 math.log(output[1]) * target[1]+\n                 math.log(output[2]) * target[2])\n\nprint(loss)","5102b116":"# our numpy solution \nimport numpy as np \nnp.sum(-1*np.log(np.asarray(output)) * np.asarray(target))","80992120":"print(math.log(1))\nprint(math.log(0.95))\nprint(math.log(0.9))\nprint(math.log(0.8))\nprint(math.log(0.2))\nprint(math.log(0.1))\nprint(math.log(0.05))\nprint(math.log(0.01))","93b23c05":"import numpy as np\nsoftmax_outputs = np.array([[0.7, 0.1, 0.2],\n                   [0.1, 0.5, 0.4],\n                   [0.02, 0.9, 0.08]])\n\nclass_targets = [0,1,1]\n\nprint(softmax_outputs[range(len(softmax_outputs)),class_targets])","f5a4dff1":"negLog = -np.log(softmax_outputs[range(len(softmax_outputs)),class_targets])\nprint(negLog)\n\navg_loss = np.mean(negLog)\n\nprint(avg_loss)","b979b397":"softmax_outputs = np.array([[0.7, 0.1, 0.2],\n                   [0.1, 0.5, 0.4],\n                   [0.02, 0.9, 0.08]])\n\nclass_targets = np.array([[1, 0, 0],\n                          [0,1,0],\n                         [0,1,0]])\n\nprint(np.sum(softmax_outputs * class_targets, axis = 1))\nnegLog = -1 * np.log(np.sum(softmax_outputs * class_targets, axis = 1))\navg_loss = np.mean(negLog)\nprint(avg_loss)","1610b8e0":"np.log(0)","3ed4ba31":"-np.log(1.0000001)","fe12096f":"softmax_outputs = np.array([[0.7, 0.1, 0.2],\n                   [0.1, 0.5, 0.4],\n                   [0.02, 0.9, 0.08]])\n\nclass_targets = np.array([[1, 0, 0],\n                          [1,0,0],\n                         [0,1,0]])\n\nactual_scores = np.sum(softmax_outputs * class_targets, axis = 1)\nprint(actual_scores)\nactual_scores += 1e-7","9c10dfb5":"print(actual_scores)","0204db42":"preds = np.array([0, 1, 0.5])\npreds += 1e-7\n\npreds = np.clip(preds, 1e-7, 1-1e-7)\n\nprint(preds)\n\n-np.log(preds)","374f8566":"# Common loss class \n\nclass Loss : \n\n  #calculates data and reg loss given model output and truth values \n  def calculate(self, output, y):\n\n    #calculate sample losses \n\n    sample_losses = self.forward(output, y )\n\n    #mean loss \n    data_loss = np.mean(sample_losses)\n\n    #return \n    return data_loss \n","589b11f3":"# Categorical Cross Entropy Loss \n\nclass Loss_CategoricalCrossEntropy(Loss):\n\n  #forward pass \n  def forward(self, y_pred, y_true):\n\n    #number of samples in a batch \n    samples = len(y_pred)\n\n    #clip data to remove log 0 and negative loss \n    y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n\n    #probs for target values \n\n    #if categorical labels \n    if len(y_true.shape) == 1:\n      confidences = y_pred_clipped[range(samples), y_true]\n    \n    elif len(y_true.shape) == 2:\n      confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n\n    #Losses\n    negLog = -np.log(confidences)\n\n    return negLog\n    ","71b2fa58":"loss_fucntion = Loss_CategoricalCrossEntropy()\nloss = loss_fucntion.calculate(softmax_outputs, class_targets)\nprint(loss)","a10755c3":"### Example of what an array of a batch of samples looks like, compared to a single sample.","58397110":"### Using the dot product with a layer of neurons","3749e3d9":"Final Code","0ad91dfd":"Exponent","7928cbc4":"### This error is just to show why shapes are important.You can execute the below cells now","d6f694ba":"### Activation Functions\n\nActivation function is applied to the output of a neuron, which modifies outputs.\n\nWe use activation function because is activation function is non-linear, it allows for neural networks with 2 or more layers to map non-linear functions.\n\nThere are generally 2 types of activation functions used in NN. \nOne in the hidden layers and 1 in the final output layer. \n\nHow non-linearity comes will see later. Generally there are following types \n\n1. Step activation function \n2. Linear activation function (Last Layer for regression)\n3. Sigmoid activation function\n4. Rectified Linear Units \n\n\n","66d56c21":"Problems with log ","e3670817":"add a very small value to the actual values like 1e-7\n","1ab1600c":"### Why & how two or more hidden layers w\/ nonlinear activation functions works with neural networks\/deep learning\n\n#### Dense layer","c5247006":"# Let's understand Neural Network more Mathematically and Programmatically\n\n### Creation of Feed Forward Neural Networks using Python only !\nNo any other libaries are used here.You gonna see here is -:\n\n1.Creation of a basic neuron with 3 and different inputs \n\n2.Creation of a simple layer of neurons, with 4 inputs\n\n3.Doing dot product with a layer of neurons and multiple input\n\n4.How to use Activation Function\n\n5.Use Softmax Activation function with deep learning.*\n\n6.To have some measure of how wrong the model is we will use\na loss function. In our case, with a softmax classifier, so I used  categorical cross-entropy.\n","85f56f3d":"### Data  often comes in input batch. \n\nWhat is a batch? a group of input data at a time. \n\nNow we need to multiply this group of input data with group of weights both are matrix","dc7f9621":"### Softmax Activation Function\n\n","a3af6b2e":"## More hidden layers and non-linear data\n\nA deep neural network is a neural network with 2 or more hidden layers\n\nSo for each layer we will have a different set of weights\n","daadbf45":"### Extracting the data","fafd4375":"Calculating Loss\n\nSoftmax output array  is giving out the values which which are representing the corresponding to it's class","c03d14be":"Softmax Activation Code","85b52183":"### Matrix product with row and column vectors with a batch of inputs to the neural network\n\n#### Q-1 How many neurons we have in the second layer?","cc569109":"Final Code ","545d36d4":"What if we have 4 inputs?","9f0c76e5":"Categorical Cross Entropy\n\n1.Log(1) = 0 and to get +ve value of Log we added minus(-) sign because we want log function as positive\n\n2.And we are using Log because Log 1 is 0 and that means 100% where our Loss will be 0 and we have to approach to this value only and here we have achieve the concept of Categorical Cross Entropy\n","fbbd3ff8":"### 3 neuron layer with 4 inputs","44ed6ca4":"### Using the dot product for a neuron's calculation","2d7d0383":"### Let's design a code of single neuron with 3 inputs example","0a691ce4":"For batches","1e3b1f10":"We can see the values have been clipped to 0. ","391fda93":"### One hot encoded values. \n","9562b413":"ReLU Activation : simple code \n"}}