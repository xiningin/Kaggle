{"cell_type":{"8429e914":"code","06280c74":"code","d55cbef7":"code","985531ca":"code","6435c569":"code","96850a84":"code","26085ea4":"code","8f0b1e05":"code","df547579":"code","a0f1630b":"code","a6e0f597":"code","f77c3101":"code","ec08d658":"code","e985f4de":"code","dcf6154f":"code","a665eec4":"code","cec2a74c":"code","285252e5":"code","d20c43f3":"code","7a47b66e":"code","2324652f":"code","abb51295":"code","b695cfea":"code","6dfe011c":"code","fc115913":"code","41169893":"code","f6fb674e":"code","69febf90":"code","63a1c827":"markdown","2d06711b":"markdown","e37739da":"markdown","60397498":"markdown","818c082e":"markdown","d80e4fe5":"markdown","2a8b0ca7":"markdown","f89dc1be":"markdown","1489768c":"markdown","a6a86ccb":"markdown","a2835b08":"markdown","5cad7d2e":"markdown","07db0161":"markdown","0d731057":"markdown","37bb39cb":"markdown","cb14546e":"markdown","211bfff2":"markdown","64d4da71":"markdown","df328601":"markdown","9eb7b9b6":"markdown","a3ba0b64":"markdown","c35b5152":"markdown","c6fa28e6":"markdown","029547e5":"markdown","229a4739":"markdown","244ff716":"markdown","5f28a180":"markdown"},"source":{"8429e914":"%%HTML\n<style type=\"text\/css\">\n                                       \ndiv.h2 {\n    background-color: #3E5AE6;\n    background-image: linear-gradient(120deg, #3E5AE6, #A37CE6);\n    text-align: left;\n    color: white;              \n    padding:9px;\n    padding-right: 100px; \n    font-size: 20px; \n    max-width: 1500px; \n    margin: auto; \n    margin-top: 40px; \n}                                  \n                                      \nbody {\n  font-size: 12px;\n}    \n                                                                               \ndiv.h3 {\n    color: #3E5AE6; \n    font-size: 18px; \n    margin-top: 20px; \n    margin-bottom:4px;\n}\n                                     \ndiv.h4 {\n    color: #159957;\n    font-size: 15px; \n    margin-top: 20px; \n    margin-bottom: 8px;\n}\n                                         \nspan.note {\n    font-size: 5; \n    color: gray; \n    font-style: italic;\n}\n                                        \nhr {\n    display: block; \n    color: gray\n    height: 1px; \n    border: 0; \n    border-top: 1px solid;\n}\n  \n                                      \nhr.light {\n    display: block; \n    color: lightgray\n    height: 1px; \n    border: 0; \n    border-top: 1px solid;\n}   \n                                         \ntable.dataframe th \n{\n    border: 1px darkgray solid;\n    color: black;\n    background-color: white;\n}\n                                       \ntable.dataframe td \n{\n    border: 1px darkgray solid;\n    color: black;\n    background-color: white;\n    font-size: 14px;\n    text-align: center;\n} \n                                         \ntable.rules th \n{\n    border: 1px darkgray solid;\n    color: black;\n    background-color: white;\n    font-size: 14px;\n}\n                                          \ntable.rules td \n{\n    border: 1px darkgray solid;\n    color: black;\n    background-color: white;\n    font-size: 13px;\n    text-align: center;\n} \n                                      \n                                      \ntable.rules tr.best\n{\n    color: green;\n}    \n                                       \n.output { \n    align-items: center; \n}\n                                      \n.output_png {\n    display: table-cell;\n    text-align: center;\n    margin:auto;\n}                                          \n                                                                                                                                                                                                                                      \n<\/style>  ","06280c74":"!pip install -U vega_datasets notebook vega","d55cbef7":"!pip install ujson","985531ca":"%env JOBLIB_TEMP_FOLDER=\/tmp\n!pip install pyspark","6435c569":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 8)\nimport os\nimport gc\nimport ujson as json\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.patches as patches\nimport seaborn as sns\n\nimport plotly as py\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import download_plotlyjs\nfrom plotly.offline import init_notebook_mode\nfrom plotly.offline import plot,iplot\n\ninit_notebook_mode(connected=True)\n\nimport altair as alt\nfrom altair.vega import v5\nfrom IPython.display import HTML\nalt.renderers.enable('notebook')\n\nfrom IPython.display import HTML\nfrom IPython.display import Image\nfrom IPython.display import display\nfrom IPython.core.display import display\nfrom IPython.core.display import HTML\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn') \ncolor_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]\n%config InlineBackend.figure_format = 'svg'  \nth_props = [('font-size', '13px'), ('background-color', 'white'), ('color', '#666666')]\ntd_props = [('font-size', '15px'), ('background-color', 'white')]\nstyles = [dict(selector=\"td\", props=td_props), dict(selector=\"th\", props=th_props)]\n\nSMALL_SIZE = 8\nMEDIUM_SIZE = 10\nBIGGER_SIZE = 12\nplt.rc('font', size=SMALL_SIZE)          # controls default text sizes\nplt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\nplt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\nplt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\nplt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\nplt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\nplt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n\nDATA_PATH = '..\/input\/the-movies-dataset\/'","96850a84":"# using ideas from this kernel: https:\/\/www.kaggle.com\/notslush\/altair-visualization-2018-stackoverflow-survey\ndef prepare_altair():\n    \"\"\"\n    Helper function to prepare altair for working.\n    \"\"\"\n\n    vega_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega@' + v5.SCHEMA_VERSION\n    vega_lib_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lib'\n    vega_lite_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lite@' + alt.SCHEMA_VERSION\n    vega_embed_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-embed@3'\n    noext = \"?noext\"\n    \n    paths = {\n        'vega': vega_url + noext,\n        'vega-lib': vega_lib_url + noext,\n        'vega-lite': vega_lite_url + noext,\n        'vega-embed': vega_embed_url + noext\n    }\n    \n    workaround = f\"\"\"    requirejs.config({{\n        baseUrl: 'https:\/\/cdn.jsdelivr.net\/npm\/',\n        paths: {paths}\n    }});\n    \"\"\"\n    \n    return workaround\n    \ndef add_autoincrement(render_func):\n    # Keep track of unique <div\/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n           \n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    \"\"\"\n    Helper function to plot altair visualizations.\n    \"\"\"\n    chart_str = \"\"\"\n    <div id=\"{id}\"><\/div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    <\/script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n\n# setting up altair\nworkaround = prepare_altair()\nHTML(\"\".join((\n    \"<script>\",\n    workaround,\n    \"<\/script>\",\n)))","26085ea4":"print('Data Files in Directory')\nprint(os.listdir(DATA_PATH))","8f0b1e05":"ratings = pd.read_csv(DATA_PATH+'ratings.csv')\nlinks = pd.read_csv(DATA_PATH+'links.csv')\nmetadata = pd.read_csv(DATA_PATH+'movies_metadata.csv')","df547579":"# Function that I wrote to print all relevant infos in dataset\nimport io\n\ndef get_df_info(df):\n    display(df.head(3))\n    buf = io.StringIO()\n    df.info(buf=buf)\n    info = buf.getvalue().split('\\n')[-2]\n    display(f'Number of Rows: {df.shape[0]}, Number of Columns: {df.shape[1]}')\n    display('Data Types')\n    df_types = df.dtypes\n    df_types = pd.DataFrame({'Column':df_types.index, 'Type':df_types.values})\n    display(df_types) \n    display(info)\n    missing = df.isnull().sum().sort_values(ascending=False)\n    display('Missing Values')\n    if missing.values.sum() == 0:\n        display('No Missing Values')\n    else:\n        missing = missing[missing > 0]\n        missing = pd.DataFrame({'Column' : missing.index, 'Missing Values' : missing.values})\n        display(missing)","a0f1630b":"get_df_info(ratings)","a6e0f597":"get_df_info(links)","f77c3101":"get_df_info(metadata)","ec08d658":"plt.rcParams['figure.figsize'] = (7, 6)\nplt.hist(ratings['rating'], bins=10);\nplt.title('Ratings Count', size=10)\nplt.xlabel('Rating')\nplt.ylabel('Frequency')\nplt.show();","e985f4de":"values = ratings.rating.value_counts()\nlabels = values.index\ncolors = ['red', 'blue', 'green', 'yellow', 'black']\ntrace = go.Pie(labels=labels, \n               values=values,\n                marker=dict(colors=colors) \n              )\nlayout = go.Layout(title='Ratings by Total Percent')\nfig = go.Figure(data=trace, layout=layout)\niplot(fig)","dcf6154f":"df_aux = ratings['movieId'].value_counts().reset_index().head(10).rename(columns={'index': 'movieId', 'movieId': 'count'})\ndf_aux['movieId'] = df_aux['movieId'].astype(str)","a665eec4":"render(alt.Chart(df_aux).mark_bar().encode(\n    x=alt.X('movieId:N', axis=alt.Axis(title='Movie ID'), sort=list(df_aux['movieId'].values)),\n    y=alt.Y('count:Q', axis=alt.Axis(title='Total Count')),\n    tooltip=['movieId', 'count']\n).properties(title='Movie Count', height=300, width=800).interactive())","cec2a74c":"# Get the Movie on metadata\ndef get_movie_metadata(movieId):\n    metadata['imdb_id'] = metadata['imdb_id'].astype('category')\n    imdb_id = links[links['movieId'] == movieId]\n    imdb_id = imdb_id.imdbId.values[0]\n    if len(str(imdb_id)) == 7:\n        movie_rated = metadata[metadata['imdb_id'] == 'tt'+imdb_id.astype(str)]\n        df = movie_rated.loc[:,['title', 'overview', 'vote_average', 'release_date']]\n        return df.reset_index(drop=True)\n    elif len(str(imdb_id)) == 6:\n        movie_rated = metadata[metadata['imdb_id'] == 'tt0'+imdb_id.astype(str)]\n        df = movie_rated.loc[:,['title', 'overview', 'vote_average', 'release_date']]\n        return df.reset_index(drop=True)\n    elif len(str(imdb_id)) == 5:\n        movie_rated = metadata[metadata['imdb_id'] == 'tt00'+imdb_id.astype(str)]\n        df = movie_rated.loc[:,['title', 'overview', 'vote_average', 'release_date']]\n        return df.reset_index(drop=True)\n    elif len(str(imdb_id)) == 4:\n        movie_rated = metadata[metadata['imdb_id'] == 'tt000'+imdb_id.astype(str)]\n        df = movie_rated.loc[:,['title', 'overview', 'vote_average', 'release_date']]\n        return df.reset_index(drop=True)\n    elif len(str(imdb_id)) == 3:\n        movie_rated = metadata[metadata['imdb_id'] == 'tt0000'+imdb_id.astype(str)]\n        df = movie_rated.loc[:,['title', 'overview', 'vote_average', 'release_date']]\n        return df.reset_index(drop=True)\n    elif len(str(imdb_id)) == 2:\n        movie_rated = metadata[metadata['imdb_id'] == 'tt00000'+imdb_id.astype(str)]\n        df = movie_rated.loc[:,['title', 'overview', 'vote_average', 'release_date']]\n        return df.reset_index(drop=True)\n    elif len(str(imdb_id)) == 1:\n        movie_rated = metadata[metadata['imdb_id'] == 'tt000000'+imdb_id.astype(str)]\n        df = movie_rated.loc[:,['title', 'overview', 'vote_average', 'release_date']]\n        return df.reset_index(drop=True)\n    else:\n        pass\n# Get Movie List\ndef get_movie(df):\n    movieIdIdx = df['movieId'].values.astype(int)\n    df_aux_b = pd.DataFrame({'title': ['aaa'], \n                           'overview': ['bbb'], \n                           'vote_average': [1.7], \n                           'release_date': ['1999-01-01']\n        })\n    for i in movieIdIdx:\n        df_aux_b = df_aux_b.append(get_movie_metadata(i), ignore_index=True)\n\n    df_aux_b.drop(0, inplace=True)\n    df_aux_b['release_date'] = df_aux_b['release_date'].apply(lambda x : x.split('-')[0])\n    df_aux_b['release_date'] = df_aux_b['release_date'].astype(int)\n    df_aux_b.rename(columns={'release_date' : 'release_year'}, inplace=True)\n    return df_aux_b.reset_index(drop=True)","285252e5":"df_movies = get_movie(df_aux)\ndf_movies","d20c43f3":"df_aux = ratings['movieId'].value_counts().reset_index().head(1001).rename(columns={'index': 'movieId', 'movieId': 'count'})\ndf_aux['movieId'] = df_aux['movieId'].astype(str)\ndf_aux = get_movie(df_aux)\nget_df_info(df_aux)","7a47b66e":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom wordcloud import WordCloud\n\nstop_words = set(stopwords.words('english'))\ntokenizer = RegexpTokenizer(r'\\w+')\n\ndf_aux['overview'] = df_aux.overview.apply(lambda x : x.lower())\ndf_aux['overview'] = df_aux.overview.apply(lambda x : tokenizer.tokenize(x))\ndf_aux['overview'] = df_aux.overview.apply(lambda x : [w for w in x if w not in stop_words])\ndf_aux['overview'] = df_aux.overview.apply(lambda x : ' '.join(x))\n\nword_count = df_aux.overview.apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).sort_values(ascending=False)\nword_count = pd.DataFrame({'word' : word_count.index, 'count': word_count.values})\n# Plot the WordCloud\nd = {}\nfor a, x in word_count.values:\n    d[a] = x\n\nwordcloud = WordCloud(background_color = 'white',\n                      max_words = 50,\n                      width = 2000,\n                      height = 2000)\nwordcloud.generate_from_frequencies(frequencies=d)\nplt.rcParams['figure.figsize'] = (10, 10)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title('Most Frequent words in TOP 1000 rated movies', fontsize = 25)\nplt.show();","2324652f":"del ratings, df_aux, df_movies\ngc.collect()","abb51295":"import pyspark.sql.functions as sql_func\nfrom pyspark.sql.types import *\nfrom pyspark.ml.recommendation import ALS, ALSModel\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nsc = SparkContext('local')\nspark = SparkSession(sc)","b695cfea":"data_schema = StructType([\n    StructField('userId', IntegerType(), False),\n    StructField('movieId', IntegerType(), False),\n    StructField('rating', FloatType(), False),\n    StructField('timestamp',IntegerType(), False)\n])\nfinal_stat = spark.read.csv(\n    '\/kaggle\/input\/the-movies-dataset\/ratings.csv', header=True, schema=data_schema\n).cache()\n\nratings = (final_stat.select(\n    'userId',\n    'movieId',\n    'rating'\n)).cache()","6dfe011c":"(training, test) = ratings.randomSplit([0.7, 0.3], seed=42)","fc115913":"als = ALS(\n          rank=30,\n          maxIter=4, \n          regParam=0.1,\n          userCol='userId', \n          itemCol='movieId', \n          ratingCol='rating',\n          coldStartStrategy='drop',\n          implicitPrefs=False\n         )\nmodel = als.fit(training)\n\npredictions = model.transform(test)\nevaluator = RegressionEvaluator(metricName='mae', labelCol='rating',\n                                predictionCol='prediction')\n\nmae = evaluator.evaluate(predictions)\nprint(f'MAE (Test) = {mae}')","41169893":"model.recommendForAllUsers(1).show(5)","f6fb674e":"get_movie_metadata(156589)","69febf90":"model.recommendForAllItems(1).show(5)","63a1c827":"The words \"Life\" and \"World\" are commons and connecting with other words like \"Love\", \"Family\", \"Father\" and \"Wife\", we caan conclude most of the movies are Family Friendly. <br>\nAs we can see, the word \"Man\" have a big presence on the Word Cloud, and looking at the Top 10 Movies Rated most movies have a male protagonist. <br>\nThis concludes the EDA and it's time to start to build the model.","2d06711b":"Nice, we have good movies in the list, I'm a great fan of Forrest Gump and The Silence of the Lambs!<br>\nMost of the movies listed were released in 90's, the only exception is Star Wars (1977).<br>\nMaybe most people are always viewing 90's movies? It's a interesting information to take note.<br>\n<br>\nAnother curios information is that in the most rated movies, none have a rate above 9.0.<br>\nLet's expand it, let's get all the 1000 most rated movies and examine what words are frequent in they overviews.","e37739da":"As the Kernel have memory and storage limitations, I couldn't get better results, but what I got was enough to demonstrate how to work with Recommendation Systems in PySpark.\n\nThanks for your reading!","60397498":"<br>\n<a id='bkground'><\/a>\n<div class=\"h2\"><center>Model using PySpark<\/center><\/div>\n<br>","818c082e":"Let's start importing all needed models and setting Spark.","d80e4fe5":"<br>\n<a id='bkground'><\/a>\n<div class=\"h2\"><center>End Notes<\/center><\/div>\n<br>","2a8b0ca7":"<br>\n<a id='bkground'><\/a>\n<div class=\"h2\"><center>Exploratory Data Analysis<\/center><\/div>\n<br>","f89dc1be":"We can see most movies were rated with 4, on a scale of 1 to 5. A fewer movies (compared to the total dataset) were rated with low grades. <br>\nLet's see which movies were rated most times, taking the 10 most rated.","1489768c":"Create the Schema.","a6a86ccb":"<a id='bkground'><\/a>\n<div class=\"h3\"><center>Links Content<\/center><\/div>","a2835b08":"<center><iframe src='https:\/\/gfycat.com\/ifr\/AngelicAgonizingEidolonhelvum' frameborder='0' scrolling='no' allowfullscreen width='640' height='412'><\/iframe><\/center>","5cad7d2e":"In all the data info displayed, we can see that only ratings have a large memory usage, and I will use this dataset to make the recommendation system based on user ratings, the dataset have the relevant data like userId, movieId and ratings.<br>\nIt's important to say that the ratings dataset doesn't have any missing value, therefore, will not needed any treatment like data imputation or drop NA rows.<br>\nThe other datasets will be used for Exploratory Data Analysis.\n","07db0161":"Time to use Natural Language Processing (NLP) with NLTK module and transform everything in overview for lower case, word tokens and remove stopwords and make the Word Cloud.","0d731057":"<br>\n<a id='bkground'><\/a>\n<div class=\"h2\"><center>Introduction<\/center><\/div>\n<br>\n\nRecommendation systems having been much present right now, there are so many approaches to make and improve they.<br>\nWe can find they in online shops, music streaming and video streaming services.<br>\nIn this Kernel, I make an EDA on [The Movies Dataset](https:\/\/www.kaggle.com\/rounakbanik\/the-movies-dataset) and got the data used for movie ratings to build a Recommendation System with Alternating Least Square (ALS), with it, it will be possible make a user based system or a item based system.<br>\n<br>\nFor this job, I will work with Pandas Dataframes, visualizations with Altair, printable text with BeautifulText and PySpark for ALS.<br>\nThis Kernel is inspired by the great work made by these guys:\n* @tombresee for made this awesome [kernel](https:\/\/www.kaggle.com\/tombresee\/next-gen-eda) with great use of HTML.\n* @artgor with his great kernels using Altair for Data Visualization, [here's an example](https:\/\/www.kaggle.com\/artgor\/dota-eda-fe-and-models)\n* @vchulski for made this [kernel](https:\/\/www.kaggle.com\/vchulski\/tutorial-collaborative-filtering-with-pyspark) that explain so good how to use ALS with PySpark.\n\nAlright, let's start installing all nedeed modules and imports, you can in the buttons to see the code or output.","37bb39cb":"<a id='bkground'><\/a>\n<div class=\"h3\"><center>Ratings Content<\/center><\/div>","cb14546e":"And this finish the work!","211bfff2":"For now, I will ignore all small dataset versions.<br>\nTime to import relvant (Ratings, Links and Metadata) files and check the data.","64d4da71":"And for a better visualization, let's represent by a pie chart with the percent representation.","df328601":"<div class=\"h3\">Let's start with the following approaches<\/div><br>\n* Rating Frequency.\n* Analysis of most rated movies.\n* World cloud with most common words.\n\nLet's start plotting an Histogram to see the rating distribution.","9eb7b9b6":"It's time to take a look in all files provided by the dataset.","a3ba0b64":"Time to discover which movies have this IDs.<br> \nLet's check the IDs on IMDB and get some info.","c35b5152":"Split in Train (70%) and Test (30%).","c6fa28e6":"Show the most recommended user for each movie (Item Based Recommendation System).\nAgain, the movieId is the same of ratings dataframe.","029547e5":"<a id='bkground'><\/a>\n<div class=\"h3\"><center>Metadata Content<\/center><\/div>","229a4739":"And finally, generate the Best recommendation for each user (User Based Recommendation System).\nThe movieId, the first element in recommendations vector, is the same of ratings dataframe.","244ff716":"And train the model, the evaluation will be made on test set using Mean Absolute Error (MAE).","5f28a180":"Let's see which movie was recommended for a particular userId."}}