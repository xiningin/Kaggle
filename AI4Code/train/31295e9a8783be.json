{"cell_type":{"facb8653":"code","d80926fb":"code","a0c42bde":"code","32ef6bdc":"code","c77c5b7d":"code","a87a5ea7":"code","246f1431":"code","da176006":"code","4e995d0e":"code","f33dec55":"code","669de31d":"code","7f287fe5":"code","a090c925":"code","32afbdd8":"code","9283e57c":"code","7319bc25":"code","6430a98a":"code","22a673e6":"code","386bcaee":"markdown"},"source":{"facb8653":"import sys\npackage_path = '..\/input\/pytorch-image-models\/pytorch-image-models-master'\nsys.path.append(package_path)","d80926fb":"import os\nimport gc\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2, ToTensor\nimport albumentations as alb\nimport torchvision.models as tv_models\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport cv2\nimport timm\n\nimport warnings\nwarnings.simplefilter('ignore')","a0c42bde":"device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using %s\" % (str(device).upper()))","32ef6bdc":"def seed_everything(seed):\n    print(\"SEED SET TO : %d\" % seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","c77c5b7d":"BATCH_SIZE = 32\nNUM_SPLITS = 5\nIMG_SIZE = (512, 512)\nEPOCHS_USED = [1, 2, 3, 4, 5]\nTARGET_SIZE = 5\nTTA = 1\nSEED = 420\nTEST_PATH = \"..\/input\/cassava-leaf-disease-classification\/test_images\/\"\nseed_everything(SEED)","a87a5ea7":"train_df = pd.read_csv(\"..\/input\/cassava-leaf-disease-classification\/train.csv\")\nsample_sub = pd.read_csv(\"..\/input\/cassava-leaf-disease-classification\/sample_submission.csv\")","246f1431":"class Cassava_dataset:\n    def __init__(self, df, transforms, img_size, img_path, is_train=True):\n        self.transforms = transforms\n        self.df = df\n        self.img_size = img_size\n        self.is_train = is_train\n        self.img_path = img_path\n        \n    def __getitem__(self, index):\n        img_path = self.img_path + str(self.df[\"image_id\"].loc[index])\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.transforms:\n            image = self.transforms(image=image)['image']\n            \n        if self.is_train:\n            label = self.df[\"label\"].loc[index]\n            return {\n                \"image\": image,\n                \"label\": torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                \"image\": image\n            }\n        \n    def __len__(self):\n        return len(self.df)","da176006":"class EffNetb1(torch.nn.Module):\n    def __init__(self,):\n        super(EffNetb1, self).__init__()\n        self.model = timm.create_model(\"tf_efficientnet_b1_ns\", pretrained=False)\n        n_features = self.model.classifier.in_features\n        self.fc = nn.Linear(n_features, TARGET_SIZE)\n        self.model.classifier = nn.Identity()\n        \n    def forward(self, x):\n        x = self.model(x)\n        x = self.fc(x)\n        return x\n      \ndef res_spoon():\n    model = timm.create_model(\"resnext50_32x4d\", pretrained=False)\n    in_features = model.fc.in_features\n    model.fc = nn.Linear(in_features=in_features, out_features=TARGET_SIZE)\n    return model\n\ndef eff_spoon():\n    model = timm.create_model(\"tf_efficientnet_b3_ns\", pretrained=False)\n    in_features = model.classifier.in_features\n    model.classifier = nn.Linear(in_features=in_features, out_features=TARGET_SIZE)\n    return model\n    \nclass Resnext50(nn.Module):\n    def __init__(self, model_name='resnext50_32x4d', pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        n_features = self.model.fc.in_features\n        self.model.fc = nn.Linear(n_features, TARGET_SIZE)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\nclass EffNetb3(torch.nn.Module):\n    def __init__(self):\n        super(EffNetb3, self).__init__()\n        self.model = timm.create_model(\"tf_efficientnet_b3_ns\", pretrained=False)\n        n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Sequential(\n            nn.Linear(n_features, 1000),\n            nn.Dropout(0.2),\n            nn.Linear(1000, TARGET_SIZE)\n        )\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x","4e995d0e":"def get_transforms_vanilla():\n    return alb.Compose([\n        alb.Resize(height=IMG_SIZE[0], width=IMG_SIZE[0]),\n        alb.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n        ToTensorV2(p=1.0),\n    ], p=1.)\n\ndef get_transforms_effnet():\n    return alb.Compose([\n        alb.CenterCrop(IMG_SIZE[0], IMG_SIZE[0], p=1.),\n        alb.Resize(height=IMG_SIZE[0], width=IMG_SIZE[0]),\n        alb.Transpose(p=0.5),\n        alb.HorizontalFlip(p=0.5),\n        alb.VerticalFlip(p=0.5),\n        alb.ShiftScaleRotate(p=0.5),\n        alb.HueSaturationValue(hue_shift_limit=0.3, sat_shift_limit=0.3, val_shift_limit=0.3, p=0.6),\n        alb.RandomBrightnessContrast(brightness_limit=(-0.1,0.2), contrast_limit=(-0.1, 0.2), p=0.5),\n        alb.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n        ToTensorV2(p=1.0),\n    ], p=1.)\n\ndef get_transforms_resnet50():\n    return alb.Compose([\n        alb.CenterCrop(IMG_SIZE[0], IMG_SIZE[0], p=1.),\n        alb.Resize(height=IMG_SIZE[0], width=IMG_SIZE[0]),\n        alb.Transpose(p=0.5),\n        alb.HorizontalFlip(p=0.5),\n        alb.VerticalFlip(p=0.5),\n        alb.ShiftScaleRotate(p=0.6),\n        alb.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.3, val_shift_limit=0.3, p=0.4),\n        alb.RandomBrightnessContrast(brightness_limit=(-0.1,0.2), contrast_limit=(-0.1, 0.2), p=0.5),\n        alb.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n        ToTensorV2(p=1.0),\n    ], p=1.)\n\n\ndef get_transforms_spoon_b3():\n    return alb.Compose([\n            alb.Resize(height=IMG_SIZE[0], width=IMG_SIZE[0]),\n            alb.Cutout(num_holes=1, max_h_size=50, max_w_size=50),\n            alb.Transpose(p=0.5),\n            alb.HorizontalFlip(p=0.5),\n            alb.VerticalFlip(p=0.5),\n            alb.ShiftScaleRotate(p=0.5),\n            alb.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.3, val_shift_limit=0.3, p=0.5),\n            alb.RandomBrightnessContrast(brightness_limit=(-0.1,0.2), contrast_limit=(-0.1, 0.2), p=0.5),\n            alb.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n            ToTensor(),\n        ], p=1.)\n\ndef get_transforms_spoon_r50():\n    return alb.Compose([\n            alb.Resize(height=IMG_SIZE[0], width=IMG_SIZE[0]),\n            alb.Cutout(num_holes=1, max_h_size=50, max_w_size=50),\n            alb.Transpose(p=0.5),\n            alb.HorizontalFlip(p=0.5),\n            alb.VerticalFlip(p=0.5),\n            alb.ShiftScaleRotate(p=0.5),\n            alb.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.3, val_shift_limit=0.3, p=0.5),\n            alb.RandomBrightnessContrast(brightness_limit=(-0.1,0.2), contrast_limit=(-0.1, 0.2), p=0.5),\n            alb.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n            ToTensor(),\n        ], p=1.)","f33dec55":"### NO TTA ####\ndef inference_func(model, data_loader, device):\n    \"\"\"\n    Make predictions for different models\n    \"\"\"\n    \n    model.eval()\n    image_preds_all = []\n    \n    for step, data in enumerate(data_loader):\n        x = data[\"image\"].to(device)\n        image_preds = model(x)\n        image_preds_all += [image_preds.detach().cpu().numpy()]\n        \n    image_preds_all = np.concatenate(image_preds_all, axis=0)\n    return image_preds_all","669de31d":"def load_model_params(model, model_path, model_list, fold, contrib):\n    if contrib == \"kjs\":\n        model.load_state_dict(torch.load(model_path + model_list[fold]))\n    if contrib == \"spoon\":\n        model_back = model_list[0].split(\"-\")[0]\n        if model_back == \"tf_efficientnet_b3_ns\":\n            model.load_state_dict(torch.load(model_path + model_list[fold]))\n        else:\n            state_dict = torch.load(model_path + model_list[fold])['model']\n            state_dict = {k[13:] if k.startswith('module.model.') else k: state_dict[k] for k in state_dict.keys()}  \n            model.load_state_dict(state_dict)\n            \n    return model","7f287fe5":"def MMI(model_p, device, model_path, transforms, model_name, fold_used, model_list, contrib):\n    \"\"\"\n    Input : *args \\n\n    Ouput : N-fold predictions\n    \"\"\"\n    \n    print(\"Making prediction for %s\" % (model_name))\n    \n    model_p.to(device)\n    test_preds = []\n    \n    for fold in range(len(fold_used)):\n        test_dataset = Cassava_dataset(sample_sub,\n                                       transforms=transforms,\n                                       img_size=IMG_SIZE,\n                                       img_path=TEST_PATH,\n                                       is_train=False)\n\n        test_DataLoader = torch.utils.data.DataLoader(test_dataset,\n                                                      BATCH_SIZE,\n                                                      drop_last=False,\n                                                      num_workers=8,\n                                                      pin_memory=True,\n                                                      shuffle=False)\n        \n        model_p = load_model_params(model=model_p,\n                                    model_path=model_path,\n                                    model_list=model_list,\n                                    fold=fold,\n                                    contrib=contrib)\n        \n            \n        model_p.eval()\n        print(\"Making Inference %s\" % (model_list[fold]))\n        with torch.no_grad():\n            for i in range(TTA):\n                print(\"TTA step : %d\" % i)\n                test_preds += [inference_func(model_p, test_DataLoader, device)]\n     \n    # Garbage Collection Function\n    del model_p, test_DataLoader\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return np.mean(test_preds, axis=0)","a090c925":"# Effnetb1\nmodel_list = [\"effnet_b1_512_seed_5_fold_%d.pth\" % (i + 1) for i in range(len(EPOCHS_USED))]\nmodel = EffNetb1()\nb1_k = MMI(model_p=model,\n          device=device,\n          model_path=\"..\/input\/cassava-effnet-5fold-full\/\",\n          transforms=get_transforms_effnet(),\n          model_name=\"Effnet-b1\",\n          fold_used=EPOCHS_USED,\n          model_list=model_list,\n          contrib=\"kjs\")\n\nprint(\"\\n\")\nprint(\"PREDS : \", b1_k[0])","32afbdd8":"# Effnetb1\nmodel_list = [\"resnext50_32x4d-image512-fold%d_5.pth\" % (i) for i in range(len(EPOCHS_USED))]\nmodel = Resnext50()\nr50_k = MMI(model_p=model,\n          device=device,\n          model_path=\"..\/input\/spoon-kjs-cassava-model-weights\/best_kjs_cv\/\",\n          transforms=get_transforms_resnet50(),\n          model_name=\"resnext50_32x4d\",\n          fold_used=EPOCHS_USED,\n          model_list=model_list,\n          contrib=\"kjs\") \n\nprint(\"\\n\")\nprint(\"PREDS : \", r50_k[0])","9283e57c":"model_list = [\"resnext50_32x4d-image512-fold%d_5.pth\" % (i) for i in range(len(EPOCHS_USED))]\nmodel = res_spoon()\nr50_p = MMI(model_p=model,\n          device=device,\n          model_path=\"..\/input\/spoon-kjs-cassava-model-weights\/best_spoon_cv\/\",\n          transforms=get_transforms_vanilla(),\n          model_name=\"resnext50_32x4d\",\n          fold_used=EPOCHS_USED,\n          model_list=model_list ,\n          contrib=\"spoon\")\n\nprint(\"\\n\")\nprint(\"PREDS : \", r50_p[0])","7319bc25":"model_list = [\"tf_efficientnet_b3_ns-image512-fold%d_5.pth\" % (i) for i in range(len(EPOCHS_USED))]\nmodel = eff_spoon()\nb3_p = MMI(model_p=model,\n          device=device,\n          model_path=\"..\/input\/spoon-kjs-cassava-model-weights\/best_spoon_cv\/\",\n          transforms=get_transforms_spoon_b3(),\n          model_name=\"tf_efficientnet_b3_ns\",\n          fold_used=EPOCHS_USED,\n          model_list=model_list ,\n          contrib=\"spoon\")\n\nprint(\"\\n\")\nprint(\"PREDS : \", b3_p[0])","6430a98a":"test_sub = sample_sub.copy()\nensemble = (\n            (b1_k  * 0.25) +\n            (r50_k * 0.15) + \n            (b3_p  * 0.25) +\n            (r50_p * 0.35)\n           )\n\nprint(ensemble[0])\ntest_sub[\"label\"] = np.argmax(ensemble, axis=1)","22a673e6":"test_sub.to_csv(\"submission.csv\", index=False)\nprint(\"KERNEL RUN COMPLETED\")\ntest_sub.head()","386bcaee":"# Version Control\n"}}