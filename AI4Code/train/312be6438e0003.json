{"cell_type":{"722ae34a":"code","9bc9561a":"code","570439ab":"code","b9cd897b":"code","1e83379d":"code","44da4279":"code","1d073774":"code","a9b12885":"code","16df6f0f":"code","422bdb3c":"code","64db8706":"code","b3090706":"code","215a4c26":"code","3f40121b":"code","558edea6":"code","1e924392":"code","29f236a3":"code","ac9249b2":"code","f19094e1":"code","bb193b0f":"code","6fe7d9f9":"code","953063db":"code","e84ae4a5":"code","ef937851":"code","5db04184":"code","877047b0":"code","3952138a":"code","9e1d5ce6":"code","2f8e3c94":"code","90d0575c":"code","68ed600f":"code","a1c5e8c0":"code","ed2b790c":"markdown","f1cd4789":"markdown","2f219b8b":"markdown","7767915e":"markdown","6d07181a":"markdown","01d1b8b9":"markdown","856b8a03":"markdown","bc78581f":"markdown","16ff38d6":"markdown","2012ab7d":"markdown","a0c2fe9c":"markdown","65cc6e45":"markdown","2753bf9b":"markdown","085be947":"markdown"},"source":{"722ae34a":"train_url = '..\/input\/titanic\/train.csv'\ntest_url  = '..\/input\/titanic\/test.csv'","9bc9561a":"import pandas as pd\n\n# Si il ya un split train \/ test, on le fait.\n\n# Train Set\ndf = pd.read_csv(train_url, index_col=\"PassengerId\")\n# Test Set\ndf_test = pd.read_csv(test_url, index_col=\"PassengerId\")\n\ndf.head()","570439ab":"df_test.head()","b9cd897b":"# Pour le train test\ncolonne_cible = \"Survived\"\n\nX = df.drop(colonne_cible, axis='columns')\nY = df[colonne_cible]","1e83379d":"# Pour le test\nX_test = df_test","44da4279":"# Check valeur nulle sur le train\nX.isna().sum()","1d073774":"# Check valeur nulle sur le test\nX_test.isna().sum()","a9b12885":"# Les colonnes sur lesquelles on va faire un traitement\ncolonnes_cat\u00e9goriques = ['Sex']\ncolonnes_num\u00e9riques = []\ncolonnes_cat\u00e9goriques_avec_valeurs_manquantes = ['Embarked']\ncolonnes_num\u00e9riques_avec_valeurs_manquantes = ['Age', 'Fare']\ncolonnes_feature_engineering = ['Name' , 'Cabin']  # CountVectorizer \/ Cabin etc...\n\n# Le reste\ndrop_colonnes = ['Ticket']  # Colonnes \u00e0 lacher\npassthrough_colonnes = ['Pclass', 'SibSp', 'Parch'] # Colonnes \u00e0 laisser-passer telles quelles","16df6f0f":"# V\u00e9rifier qu'on a tout\n\nall_columns = (colonnes_cat\u00e9goriques \n               + colonnes_num\u00e9riques \n               + colonnes_cat\u00e9goriques_avec_valeurs_manquantes\n               + colonnes_num\u00e9riques_avec_valeurs_manquantes\n               + colonnes_feature_engineering\n               + drop_colonnes\n               + passthrough_colonnes)\n\nif set(all_columns) == set(X.columns):\n  print('Ok')\nelse:\n  # Alors je veux voir les diff\u00e9rences\n  print('dans all_columns mais pas dans X.columns   :', set(all_columns) - set(X.columns))\n  print('dans X.columns   mais pas dans all_columns :', set(X.columns) - set(all_columns))","422bdb3c":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\n\n","64db8706":"fill_missing_then_one_hot_encoder = make_pipeline(\n    SimpleImputer(strategy='constant', fill_value='manquante'),\n    OneHotEncoder(handle_unknown='ignore')\n)","b3090706":"def extraire_la_premi\u00e8re_lettre(serie):\n    return pd.DataFrame(serie.str[0])\n\nextraire_lettre_cabine = make_pipeline(\n    FunctionTransformer(extraire_la_premi\u00e8re_lettre),\n    fill_missing_then_one_hot_encoder,\n)","215a4c26":"data_cleaning = make_column_transformer(\n    ( OneHotEncoder(), colonnes_cat\u00e9goriques ),\n    ( fill_missing_then_one_hot_encoder , colonnes_cat\u00e9goriques_avec_valeurs_manquantes),\n    ( SimpleImputer(strategy='mean'), colonnes_num\u00e9riques_avec_valeurs_manquantes),\n    ( CountVectorizer(), 'Name'),\n    ( extraire_lettre_cabine, 'Cabin'),\n    ( 'drop' , drop_colonnes),\n    ( 'passthrough' , passthrough_colonnes)\n)\n","3f40121b":"# Il est conseill\u00e9 de v\u00e9rfiier que \u00e7a marche\n# pour \u00e7a, on va voir si on peut fit transform sans erreur\n\n# data_cleaning.fit(X)\n\n# data_cleaning.transform(X)\n\n# data_cleaning.transform(X_test)\n\nprint('C\\'est bon')","558edea6":"from sklearn.model_selection import GridSearchCV","1e924392":"from sklearn.model_selection import KFold\n\ncross_validation_design = KFold(n_splits=5,\n                                shuffle=True,\n                                random_state=77)\n\ncross_validation_design","29f236a3":"data_cleaning = make_column_transformer(\n    ( OneHotEncoder(), colonnes_cat\u00e9goriques ),\n    ( fill_missing_then_one_hot_encoder , colonnes_cat\u00e9goriques_avec_valeurs_manquantes),\n    ( SimpleImputer(strategy='mean'), colonnes_num\u00e9riques_avec_valeurs_manquantes),\n    ( CountVectorizer(), 'Name'),\n    ( extraire_lettre_cabine, 'Cabin'),\n    ( 'drop' , drop_colonnes),\n    ( 'passthrough' , passthrough_colonnes)\n)","ac9249b2":"from sklearn.neighbors import KNeighborsClassifier as KNN\n\nKNN_MODEL = {}\n\n# D\u00e9finir la pipeline\nKNN_MODEL['pipeline'] = Pipeline([\n                                  ('data_cleaning', data_cleaning),\n                                  ('knn', KNN())\n                                  ])\n\n# D\u00e9finir la grille\nKNN_MODEL['hyperparams'] = {}\nKNN_MODEL['hyperparams']['knn__n_neighbors'] = [1, 3, 9, 21, 51]\nKNN_MODEL['hyperparams']['knn__weights'] = ['uniform', 'distance']\n\n# Effectuer la GridSearch\nKNN_MODEL['gridsearch'] = GridSearchCV(\n    estimator=KNN_MODEL['pipeline'],\n    param_grid=KNN_MODEL['hyperparams'],\n    cv=cross_validation_design,\n    scoring='accuracy'\n    )\n\nKNN_MODEL['gridsearch'].fit(X, Y)","f19094e1":"KNN_MODEL['gridsearch'].best_params_","bb193b0f":"KNN_MODEL['gridsearch'].best_score_","6fe7d9f9":"data_cleaning = make_column_transformer(\n    ( OneHotEncoder(), colonnes_cat\u00e9goriques ),\n    ( fill_missing_then_one_hot_encoder , colonnes_cat\u00e9goriques_avec_valeurs_manquantes),\n    ( SimpleImputer(strategy='mean'), colonnes_num\u00e9riques_avec_valeurs_manquantes),\n    ( CountVectorizer(), 'Name'),\n    ( extraire_lettre_cabine, 'Cabin'),\n    ( 'drop' , drop_colonnes),\n    ( 'passthrough' , passthrough_colonnes)\n)","953063db":"from sklearn.linear_model import RidgeClassifier\nimport  numpy as np \n# D\u00e9finir la pipeline\nREGRESSION_MODEL = {}\nREGRESSION_MODEL['pipeline'] = Pipeline([\n                                        ('data_cleaning', data_cleaning ),\n                                        ('reg', RidgeClassifier())\n])\n\n# D\u00e9finir la grille\nREGRESSION_MODEL['hyperparams'] = {}\nREGRESSION_MODEL['hyperparams']['reg__alpha'] = np.arange(.1, 10., .1)\n\n# Faire la recherche\nREGRESSION_MODEL['gridsearch'] = GridSearchCV(\n                                      estimator=REGRESSION_MODEL['pipeline'],\n                                      param_grid=REGRESSION_MODEL['hyperparams'],\n                                      cv=cross_validation_design,\n                                      scoring='accuracy'\n                                      )\n\nREGRESSION_MODEL['gridsearch'].fit(X, Y)","e84ae4a5":"REGRESSION_MODEL['gridsearch'].best_params_","ef937851":"REGRESSION_MODEL['gridsearch'].best_score_","5db04184":"data_cleaning = make_column_transformer(\n    ( OneHotEncoder(), colonnes_cat\u00e9goriques ),\n    ( fill_missing_then_one_hot_encoder , colonnes_cat\u00e9goriques_avec_valeurs_manquantes),\n    ( SimpleImputer(strategy='mean'), colonnes_num\u00e9riques_avec_valeurs_manquantes),\n    ( CountVectorizer(), 'Name'),\n    ( extraire_lettre_cabine, 'Cabin'),\n    ( 'drop' , drop_colonnes),\n    ( 'passthrough' , passthrough_colonnes)\n)","877047b0":"from sklearn.ensemble import RandomForestClassifier as RandomForest\n\nRF_MODEL = {}\n\n# D\u00e9finir la pipe\nRF_MODEL['pipeline']=Pipeline([\n                               ('data_cleaning', data_cleaning),\n                               ('rf', RandomForest(random_state=7))])\n\n# D\u00e9finir la grille d'hyperparams\nRF_MODEL['hyperparams'] = {}\nRF_MODEL['hyperparams']['rf__n_estimators'] = [10, 30, 50, 100, 150]\nRF_MODEL['hyperparams']['rf__max_features'] = [None, 'sqrt', 'log2', .1, .25, .50, .75, .85]\nRF_MODEL['hyperparams']['rf__max_depth'] = [None, 4, 7, 10, 20]\n\n# Recherche d'HP\nRF_MODEL['gridsearch'] = GridSearchCV(\n    estimator=RF_MODEL['pipeline'],\n    param_grid=RF_MODEL['hyperparams'],\n    scoring='accuracy',\n    cv=cross_validation_design\n)\n\nRF_MODEL['gridsearch'].fit(X, Y)","3952138a":"RF_MODEL['gridsearch'].best_params_","9e1d5ce6":"RF_MODEL['gridsearch'].best_score_","2f8e3c94":"data_cleaning = make_column_transformer(\n    ( OneHotEncoder(), colonnes_cat\u00e9goriques ),\n    ( fill_missing_then_one_hot_encoder , colonnes_cat\u00e9goriques_avec_valeurs_manquantes),\n    ( SimpleImputer(strategy='mean'), colonnes_num\u00e9riques_avec_valeurs_manquantes),\n    ( CountVectorizer(), 'Name'),\n    ( extraire_lettre_cabine, 'Cabin'),\n    ( 'drop' , drop_colonnes),\n    ( 'passthrough' , passthrough_colonnes)\n)\n\nmodel_final = Pipeline([('data_cleaning', data_cleaning),\n                        ('rf', RandomForest(random_state=7,\n                                            max_depth=10,\n                                            max_features=0.25,\n                                            n_estimators=150))\n                        ])\n\n\n# on fit la meilleur pipe sur toute nos donn\u00e9es de train\nmodel_final.fit(X, Y)","90d0575c":"df_test.head()","68ed600f":"predictions = model_final.predict(df_test)\noutput = pd.DataFrame({'PassengerId': df_test.index, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","a1c5e8c0":"output","ed2b790c":"### Pipeline 1 : KNN (pas de PCA)","f1cd4789":"# Phase 3 - EDA\n\nOn va juste regarder les valeurs nulles.\n\n(en th\u00e9orie il faudrait aussi recharcher la pr\u00e9sence de valeur aberrante, tracer des graphiques etc...)","2f219b8b":"### 3. Les combiner dans un ColumnTransformer","7767915e":"# Fini","6d07181a":"# Phase 6 - Trouver la Meilleure Pipeline\n\n","01d1b8b9":"### Pipeline 2 : RidgeRegression Classifier (un type de r\u00e9gression lin\u00e9aire avec Feature Selection int\u00e9gr\u00e9e)","856b8a03":"# Phase 2 - Isoler les features `X` des cibles `Y`","bc78581f":"### \u00e9tape 1 : D\u00e9finir Proc\u00e9dure de Cross-Validation","16ff38d6":"# On selectionne le meilleur mod\u00e8le : on va r\u00e9entrainer sur tout (sans cross val) avec les meilleurs params","2012ab7d":"# Phase 1 - R\u00e9cuperer les donn\u00e9es","a0c2fe9c":"# Phase 4 - D\u00e9terminer quoi faire avec chaque colonne","65cc6e45":"### 2. Cr\u00e9er les transformers complexes (ceux qui sont pas ok, out-of-box)\n\nDans notre cas:\n\n\n*   `'Embarked'` : Valeur Manquante -> Categorique [One Hot] \n*   `'Cabin'` : Feature Engineering -> Valeur Manquante -> Categorique [One Hot] \n\nhttps:\/\/towardsdatascience.com\/pipeline-columntransformer-and-featureunion-explained-f5491f815f","2753bf9b":"### Pipeline 3 - RandomForest (le meilleur mod\u00e8le quand on y connait rien)","085be947":"# Phase 5 - Data Cleaning\n\n### 1. Importer les outils "}}