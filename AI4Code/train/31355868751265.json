{"cell_type":{"41a33b6a":"code","2616a26c":"code","1bfd6fd9":"code","8ef4d10b":"code","d7d6ec53":"code","19768b8b":"code","e38c787c":"code","4466ab01":"code","ebf8183e":"code","77e21e5e":"code","23a0921c":"code","e7162b82":"code","9923db89":"code","45735b0d":"code","1579ab3c":"code","8e7e3263":"code","3f74198e":"code","138c1b22":"code","fef28d54":"code","ad422604":"code","539bfce1":"code","49c28866":"code","aebcc64d":"code","652a8b8b":"code","2210f8cf":"code","07473ddb":"code","d5cd7dee":"code","ec542d3a":"code","489504ee":"code","3c6fec14":"code","611db788":"code","d4c691bc":"code","cf64d049":"code","bc0d9506":"code","4e4b1584":"code","0444ef6c":"code","69d78b02":"code","48c75b74":"markdown","3b038bfd":"markdown","55706749":"markdown","862bf4d0":"markdown","2317db82":"markdown","8cbb5da6":"markdown","82e3732c":"markdown","19462857":"markdown","8f231cc3":"markdown","8c7a6f15":"markdown","b07b1bac":"markdown","68a07896":"markdown","9c4d6235":"markdown","2c4b09be":"markdown","69771fbd":"markdown","750a86d8":"markdown","df059326":"markdown","daaef6f9":"markdown","ebb34713":"markdown","a2d46110":"markdown","5b0fb9c0":"markdown","f61347b2":"markdown","abd3a5b1":"markdown","947021a6":"markdown","08fa402d":"markdown","5a77af7b":"markdown","b6ab74d6":"markdown","0d3be3b6":"markdown"},"source":{"41a33b6a":"!pip install opendatasets --upgrade --quiet","2616a26c":"import opendatasets as od\n\ndataset_url = 'https:\/\/www.kaggle.com\/alxmamaev\/flowers-recognition\/flowers'\nod.download(dataset_url)","1bfd6fd9":"import os\n\n# Look into the data directory\nroot_dir = '.\/flowers-recognition'\ndata_dir = '.\/flowers-recognition\/flowers'\nprint(os.listdir(data_dir))","8ef4d10b":"# Importing shutil library to delete the folder with it's contents\nimport shutil\n\n# Delete the duplicate folder\ndup_dir = data_dir + '\/flowers'\nif os.path.exists(dup_dir) and os.path.isdir(dup_dir):\n    shutil.rmtree(dup_dir)\n\n# Look into the new data directory \nprint(os.listdir(data_dir))","d7d6ec53":"def rename_files(root_dir):\n    classes = os.listdir(root_dir)\n    for classes in classes:\n        for file in os.listdir(root_dir + '\/' + classes): \n            if file.endswith('jpg'):\n                os.rename((root_dir + '\/' + classes + '\/' + file),(root_dir + '\/' + classes + '\/' + classes + \"_\" + file))\n\nrename_files(data_dir)","19768b8b":"#import os\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\ndef parse_species(fname):\n    parts = fname.split('_')\n    return parts[0]\n\ndef open_image(path):\n    with open(path, 'rb') as f:\n        img = Image.open(f)\n        return img.convert('RGB')\n\nclass FlowersDataset(Dataset):\n    def __init__(self, root_dir, transform):\n        super().__init__()\n        self.root_dir = root_dir\n        self.files = []\n        self.classes = [fname for fname in os.listdir(root_dir) if fname != 'flowers']\n        for classes in self.classes:                         \n            for file in os.listdir(root_dir + '\/' + classes): \n                if file.endswith('jpg'):\n                    self.files.append(file)\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, i):\n        fname = self.files[i]\n        species = parse_species(fname)\n        fpath = os.path.join(self.root_dir, species, fname)\n        img = self.transform(open_image(fpath))\n        class_idx = self.classes.index(species)\n        return img, class_idx","e38c787c":"import torchvision.transforms as T\n\nimg_size = 64\nstats = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\ntransform = T.Compose([T.Resize((img_size, img_size)),\n                       T.RandomCrop(64, padding=4, padding_mode='reflect'),\n                       T.RandomHorizontalFlip(),\n                       T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n                       T.ToTensor(),\n                       T.Normalize(*stats,inplace=True)])\ndataset = FlowersDataset(data_dir, transform=transform)","4466ab01":"len(dataset)","ebf8183e":"import torch\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef denormalize(images, means, stds):\n    if len(images.shape) == 3:\n        images = images.unsqueeze(0)\n    means = torch.tensor(means).reshape(1, 3, 1, 1)\n    stds = torch.tensor(stds).reshape(1, 3, 1, 1)\n    return images * stds + means\n\ndef show_image(img_tensor, label):\n    print('Label:', dataset.classes[label], '(' + str(label) + ')')\n    img_tensor = denormalize(img_tensor, *stats)[0].permute((1, 2, 0))\n    plt.imshow(img_tensor)","77e21e5e":"show_image(*dataset[1746]);","23a0921c":"show_image(*dataset[3745]);","e7162b82":"from torch.utils.data import random_split\n\nrandom_seed = 43\ntorch.manual_seed(random_seed)\n\nval_pct = 0.1\nval_size = int(val_pct * len(dataset))\ntrain_size = len(dataset) - val_size","9923db89":"train_ds, valid_ds= random_split(dataset, [train_size, val_size])\nlen(train_ds), len(valid_ds)","45735b0d":"from torch.utils.data import DataLoader\n\nbatch_size = 64\n\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\nvalid_dl = DataLoader(valid_ds, batch_size*2, num_workers=3, pin_memory=True)","1579ab3c":"from torchvision.utils import make_grid\n\ndef show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(12, 12))\n        ax.set_xticks([]); ax.set_yticks([])\n        images = denormalize(images, *stats)\n        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))\n        break","8e7e3263":"show_batch(train_dl)","3f74198e":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","138c1b22":"device = get_default_device()\ndevice","fef28d54":"train_dl = DeviceDataLoader(train_dl, device)\nvalid_dl = DeviceDataLoader(valid_dl, device)","ad422604":"import torch.nn as nn\nimport torch.nn.functional as F\n\ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch\n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels)  # Calculate loss\n        return loss\n\n    def validation_step(self, batch):\n        images, labels = batch\n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n\n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n\n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}],{} train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, \"last_lr: {:.5f},\".format(result['lrs'][-1]) if 'lrs' in result else '', \n            result['train_loss'], result['val_loss'], result['val_acc']))","539bfce1":"def conv_block(in_channels, out_channels, pool=False):\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n              nn.BatchNorm2d(out_channels), \n              nn.ReLU(inplace=True)]\n    if pool: layers.append(nn.MaxPool2d(2))\n    return nn.Sequential(*layers)\n\nclass ResNet9(ImageClassificationBase):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        \n        self.conv1 = conv_block(in_channels, 64)\n        self.conv2 = conv_block(64, 128, pool=True)   \n        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n        \n        self.conv3 = conv_block(128, 256, pool=True)\n        self.conv4 = conv_block(256, 512, pool=True)    \n        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))   \n        \n        self.classifier = nn.Sequential(nn.AdaptiveMaxPool2d(1),\n                                        nn.Flatten(),     \n                                        nn.Dropout(0.2),\n                                        nn.Linear(512, num_classes))    \n        \n    def forward(self, xb):\n        out = self.conv1(xb)\n        out = self.conv2(out)\n        out = self.res1(out) + out\n        out = self.conv3(out)\n        out = self.conv4(out)\n        out = self.res2(out) + out\n        out = self.classifier(out)\n        return out","49c28866":"model = to_device(ResNet9(3, 5), device)\nmodel","aebcc64d":"import torch\nfrom tqdm.notebook import tqdm\n\n@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase\n        model.train()\n        train_losses = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,\n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n\n    # Set up custom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\n                                                steps_per_epoch=len(train_loader))\n\n    for epoch in range(epochs):\n        # Training Phase\n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n\n            # Gradient clipping\n            if grad_clip:\n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","652a8b8b":"history = [evaluate(model, valid_dl)]\nhistory","2210f8cf":"epochs = 10\nmax_lr = 0.01\ngrad_clip = 0.1\nweight_decay = 1e-4\nopt_func = torch.optim.Adam","07473ddb":"%%time\nhistory += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)","d5cd7dee":"train_time='2:42'","ec542d3a":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');","489504ee":"plot_accuracies(history)","3c6fec14":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","611db788":"plot_losses(history)","d4c691bc":"import numpy as np\n\ndef plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');","cf64d049":"plot_lrs(history)","bc0d9506":"def predict_image(img, model):\n    # Convert to a batch of 1\n    xb = to_device(img.unsqueeze(0), device)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    _, preds  = torch.max(yb, dim=1)\n    # Retrieve the class label\n    return dataset.classes[preds[0].item()]","4e4b1584":"img, label = valid_ds[6]\nplt.imshow(img.permute(1, 2, 0).clamp(0, 1))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","0444ef6c":"img, label = valid_ds[156]\nplt.imshow(img.permute(1, 2, 0).clamp(0, 1))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","69d78b02":"img, label = valid_ds[388]\nplt.imshow(img.permute(1, 2, 0).clamp(0, 1))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","48c75b74":"**Using a GPU:** To seamlessly use a GPU, if one is available, we define a couple of helper functions (`get_default_device` & `to_device`) and a helper class `DeviceDataLoader` to move our model & data to the GPU as required.","3b038bfd":"I'm using a validation percentage of 10%, but you can use a smaller or larger percentage. One good strategy is to determine a good set of hyperparameters, and then retrain on a smaller validation set for your final submission.  ","55706749":"We're now ready to train our model. We'll use the Adam optimizer which uses techniques like momentum and adaptive learning rates for faster training.","862bf4d0":"Identifying where our model performs poorly can help us improve the model, by collecting more training data, increasing\/decreasing the complexity of the model, and changing the hypeparameters.","2317db82":"Transforms can be chained using `transforms.Compose`.","8cbb5da6":"We can also plot the training and validation losses to study the trend.","82e3732c":"## **Model Architecture**\nWe will use the **ResNet9** architecture.\n\nLet's begin with defining the model by extending an ImageClassificationBase class which contains helper methods for training & validation.","19462857":"As expected, the learning rate starts at a low value, and gradually increases for 30% of the iterations to a maximum value of 0.01, and then gradually decreases to a very small value.","8f231cc3":"## **Summary**  \nHere's a summary of the different techniques used in this tutorial to improve our model performance and reduce the training time:\n\n- **Data normalization:** We normalized the image tensors by subtracting the mean and dividing by the standard deviation of pixels across each channel. Normalizing the data prevents the pixel values from any one channel from disproportionately affecting the losses and gradients. [Learn more](https:\/\/medium.com\/@ml_kid\/what-is-transform-and-transform-normalize-lesson-4-neural-networks-in-pytorch-ca97842336bd)\n\n- **Data augmentation:** We applied random transformations while loading images from the training dataset. Specifically, we will resize the images to 64 x 64 pixels, then we will randomly crop the image to 64 x 64 pixels with padding by 4 pixels, and then flip the image horizontally with a 50% probability. [Learn more](https:\/\/www.analyticsvidhya.com\/blog\/2019\/12\/image-augmentation-deep-learning-pytorch\/)\n\n- **Residual connections:** One of the key changes to our CNN model was the addition of the resudial block, which adds the original input back to the output feature map obtained by passing the input through one or more convolutional layers. We used the ResNet9 architecture. [Learn more](https:\/\/towardsdatascience.com\/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec?gi=68b790ca041)\n\n- **Batch normalization:** After each convolutional layer, we added a batch normalization layer, which normalizes the outputs of the previous layer. This is somewhat similar to data normalization, except it's applied to the outputs of a layer, and the mean and standard deviation are learned parameters. [Learn more](https:\/\/towardsdatascience.com\/batch-normalization-and-dropout-in-neural-networks-explained-with-pytorch-47d7a8459bcd)\n\n- **Learning rate scheduling:** Instead of using a fixed learning rate, we will use a learning rate scheduler, which will change the learning rate after every batch of training. There are many strategies for varying the learning rate during training, and we used the \"One Cycle Learning Rate Policy\". [Learn more](https:\/\/sgugger.github.io\/the-1cycle-policy.html)\n\n- **Weight Decay:** We added weight decay to the optimizer, yet another regularization technique which prevents the weights from becoming too large by adding an additional term to the loss function. [Learn more](https:\/\/towardsdatascience.com\/this-thing-called-weight-decay-a7cd4bcfccab)\n\n- **Gradient clipping:** We also added gradient clippint, which helps limit the values of gradients to a small range to prevent undesirable changes in model parameters due to large gradient values during training. [Learn more](https:\/\/towardsdatascience.com\/what-is-gradient-clipping-b8e815cdfb48#63e0)\n\n- **Adam optimizer:** Instead of SGD (stochastic gradient descent), we used the Adam optimizer which uses techniques like momentum and adaptive learning rates for faster training. There are many other optimizers to choose froma and experiment with. [Learn more](https:\/\/ruder.io\/optimizing-gradient-descent\/index.html)","8c7a6f15":"**Training & Validation sets:** As a good practice, we should split the data into training and validation datasets. Let's fix a seed for PyTorch (to ensure we always get the same validation set), and create the datasets using `random_split`.","b07b1bac":"Let's check how many samples the dataset contains\n","68a07896":"- There is a duplicate folder as _flowers_. Let's delete it.","9c4d6235":"# **Deep Learning with PyTorch for Flowers Classification**\n\n<p align=\"center\">\n  <img src=\"https:\/\/images.unsplash.com\/photo-1601743875461-a4db41e22da5?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=1050&q=80\" width=\"60%\" \/>\n<\/p>\n\n**Objective:** In this project, we will build a deep learning classification model using CNNs, ResNet and regularization techniques to identify the flower species.\n\n**Dataset Source:** https:\/\/www.kaggle.com\/alxmamaev\/flowers-recognition\/ - Version 2\n","2c4b09be":"Based on where you're running this notebook, your default device could be a CPU (`torch.device('cpu')`) or a GPU (`torch.device('cuda')`)\n\n","69771fbd":"Let's take a look at a sample image from the dataset. We'll define a function `show_image` to help us.\n\n","750a86d8":"## **Training the Model**\n\nBefore we train the model, we're going to make a bunch of small but important improvements to our `fit` function:\n\nLet's define a `fit_one_cycle` function to incorporate these changes. We'll also record the learning rate used for each batch.","df059326":"## **Preparing the Dataset**","daaef6f9":"**Data Loaders:** Next, we can create data loaders for retrieving images in batches.","ebb34713":"Finally, let's visualize how the learning rate changed over time, batch-by-batch over all the epochs.","a2d46110":"## **Testing the Model**\n\nWhile we have been tracking the overall accuracy of a model so far, it's also a good idea to look at model's results on some sample images. Let's test out our model with some images from the predefined validation dataset of 432 images.","5b0fb9c0":"We can now wrap our training and validation data loaders using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available).\n\n","f61347b2":"## **Downloading and Exploring the Data**\n\n**Downloading:** We can use the `opendatasets` library to download the [dataset](https:\/\/www.kaggle.com\/alxmamaev\/flowers-recognition) from Kaggle. `opendatasets` uses the [Kaggle Official API](https:\/\/github.com\/Kaggle\/kaggle-api) for downloading datasets from Kaggle. Follow these steps to find your API credentials:\n\n1. Sign in to [https:\/\/kaggle.com\/](https:\/\/kaggle.com\/), then click on your profile picture on the top right and select \"My Account\" from the menu.\n\n2. Scroll down to the \"API\" section and click \"Create New API Token\". This will download a file `kaggle.json` with the following contents: {\"username\":\"YOUR_KAGGLE_USERNAME\",\"key\":\"YOUR_KAGGLE_KEY\"}\n3. When you run `opendatsets.download`, you will be asked to enter your username & Kaggle API, which you can get from the file downloaded in step 2.\nNote that you need to download the `kaggle.json` file only once. On Google Colab, you can also upload the `kaggle.json` file using the files tab, and the credentials will be read automatically.","abd3a5b1":"Our model trained to over 76% accuracy in under 3 minutes! \n\nLet's plot the valdation set accuracies to study how the model improves over time.","947021a6":"Let's rename the images' file to include their class name.\n\n","08fa402d":"**Custom dataset:** We can now create a custom dataset by extending the `Dataset` class from PyTorch. We need to define the `__len__` and `__getitem__` methods to create a dataset. We'll also provide the option of adding transforms into the constructor.\n\n","5a77af7b":"**Exploring:** Here, we will get to know the dataset and clean it if necessary.","b6ab74d6":"Let's define the ResNet9 model.","0d3be3b6":"Let's have a look at some images."}}