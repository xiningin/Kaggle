{"cell_type":{"b5888abd":"code","1ebab4cc":"code","7f3dcbf7":"code","a70db9b4":"code","c4953a46":"code","1d2327bc":"code","d8cf9125":"code","7945448f":"code","b099c01b":"code","c7a16c3d":"code","50a60ae9":"code","65e2e36c":"code","042856f8":"code","49cb60fc":"code","e7f00f70":"code","9259a1cb":"code","620d4e73":"code","c92aae52":"code","30c248d6":"code","ab141db4":"code","454c8608":"code","8013fe5f":"code","1385a136":"code","ccfdc6a1":"code","ddb79f81":"code","011d72b9":"code","dd447fd7":"code","20b3777f":"code","05b7533d":"code","20fc1499":"code","8abbe39b":"code","f3a9a866":"code","cc018c7e":"code","056cf7f6":"code","fd8d8990":"code","69e77c15":"code","b15f02e0":"code","4cd2b4cd":"code","3c703790":"code","6e72558f":"code","13610710":"code","eeb21f6f":"markdown","2e0c94bd":"markdown","25099e6f":"markdown","ee16d240":"markdown","1c62b9d5":"markdown","2863d2db":"markdown","e3f0d227":"markdown","2516e79b":"markdown","fd87abca":"markdown","bd42fc6e":"markdown","d51491f1":"markdown","9b8b34c7":"markdown"},"source":{"b5888abd":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1ebab4cc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline","7f3dcbf7":"#Importing Dataset\n\ncolumn_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndf = pd.read_csv('\/kaggle\/input\/boston-house-prices\/housing.csv', header=None, delimiter=r\"\\s+\", names= column_names)\ndf.head()","a70db9b4":"df.info()    # Checking Data types and missing values","c4953a46":"# Another way (Most used) of checking missing data in dataset\ndf.isnull().sum()","1d2327bc":"df.drop(\"CHAS\", axis = 1).describe().transpose()","d8cf9125":"# Correlation Matrix\nplt.figure(figsize= (12, 8))\nsns.heatmap(df.drop(\"CHAS\", axis = 1).corr(), annot = True)","7945448f":"# Correlation with predictors \nplt.figure(figsize= (10, 6))\ncorrelation = df.drop(\"CHAS\", axis = 1).corr().iloc[0:12,-1]\ncorrelation.plot(kind = \"bar\")","b099c01b":"#Univariate Analysis of MEDV\nplt.figure(figsize= (8, 6))\nsns.distplot(df[\"MEDV\"])","c7a16c3d":"plt.figure(figsize= (8, 6))\nsns.scatterplot(x = df[\"LSTAT\"], y= df[\"MEDV\"])","50a60ae9":"plt.figure(figsize= (8, 6))\nsns.scatterplot(x = df[\"RM\"], y= df[\"MEDV\"])","65e2e36c":"plt.figure(figsize= (8, 6))\nsns.regplot(x = df[\"TAX\"], y= df[\"RAD\"])","042856f8":"plt.figure(figsize= (8, 6))\nsns.regplot(x = df[\"INDUS\"], y= df[\"NOX\"])","49cb60fc":"df[\"CHAS\"].value_counts().plot(kind = \"bar\")","e7f00f70":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score","9259a1cb":"X = df.drop(\"MEDV\", axis = 1)\ny = df[\"MEDV\"]","620d4e73":"X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.3, random_state = 42)","c92aae52":"y_train.hist()","30c248d6":"y_test.hist()","ab141db4":"from sklearn.preprocessing import StandardScaler","454c8608":"scaler = StandardScaler()","8013fe5f":"scaler.fit(X_train, y_train)","1385a136":"X_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","ccfdc6a1":"lr = LinearRegression()","ddb79f81":"lr.fit(X_train_scaled, y_train)","011d72b9":"y_pred_lr = lr.predict(X_test_scaled)","dd447fd7":"rmse = mean_squared_error(y_test, y_pred_lr)**(1\/2)\nrmse","20b3777f":"lr.score(X_test_scaled, y_test)","05b7533d":"plt.figure(figsize= (10, 6))\nsns.regplot(y_test, y_pred_lr)\nplt.xlim([0, 60])","20fc1499":"from sklearn.ensemble import RandomForestRegressor","8abbe39b":"rf = RandomForestRegressor(random_state = 42)","f3a9a866":"rf.fit(X_train_scaled, y_train)","cc018c7e":"y_pred_rf = rf.predict(X_test_scaled)","056cf7f6":"rmse = mean_squared_error(y_test, y_pred_rf)**0.5\nrmse","fd8d8990":"r2_score(y_test, y_pred_rf)","69e77c15":"plt.figure(figsize= (10, 6))\nsns.regplot(y_test, y_pred_rf)\nplt.xlim([0, 60])","b15f02e0":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nprint(random_grid)","4cd2b4cd":"# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\n# Fit the random search model\nrf_random.fit(X_train_scaled, y_train)","3c703790":"# Extract best hyperparameters from 'rf_random'\n\nbest_hyperparams = rf_random.best_params_\nprint('Best hyerparameters:\\n', best_hyperparams)","6e72558f":"# Extract best model from 'rf_random'\nbest_model = rf_random.best_estimator_\n\n# Predict the test set labels\ny_pred_rf = best_model.predict(X_test_scaled)\n\n# Evaluate the test set RMSE\nrmse_test = mean_squared_error(y_test, y_pred_rf)**(1\/2)\n\n# Print the test set RMSE\nprint('Test set RMSE of gb: {:.2f}'.format(rmse_test))","13610710":"r2_score(y_test, y_pred_rf)","eeb21f6f":"So, The score has improved a lot. Though we have not done hyperparameter tuning. Lets see the accuracy of the model with hyperparameter tuning using Randomized Search","2e0c94bd":"This indicates that the feature is imbalanced","25099e6f":"Each record in the database describes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970. The attributes are de\ufb01ned as follows (taken from the UCI Machine Learning Repository1): \n\n* CRIM: per capita crime rate by town\n* ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n* INDUS: proportion of non-retail business acres per town\n* CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n* NOX: nitric oxides concentration (parts per 10 million)\n* RM: average number of rooms per dwelling\n* AGE: proportion of owner-occupied units built prior to 1940\n* DIS: weighted distances to \ufb01ve Boston employment centers\n* RAD: index of accessibility to radial highways\n* TAX: full-value property-tax rate per 10k dollar\n* PTRATIO: pupil-teacher ratio by town \n* B: 1000(Bk\u22120.63)^2 where Bk is the proportion of blacks by town  \n* LSTAT: % lower status of the population\n* MEDV: Median value of owner-occupied homes in thousands dollars\n\n\n**Here \"MEDV\" is the predictor** \n\nCHAS is a categorical variable and rest are numerical variable","ee16d240":"1. \"DIS\" feature is highly correlated with \"INDUS\", \"NOX\" and \"AGE\"\n\n2. \"TAX\" feature is highly correlated with \"RAD\" ,\"INDUS\" and \"NOX\"\n\n3. \"MEDV\" has high positive correlation with \"RM\" which is the no. of rooms \n\n4. \"MEDV\" has high negative correlation with \"LSTATE\" \n\n5. We must take steps to high correlated features when using Linear Regression not to account multicolinearity \n","1c62b9d5":"It seems that y_train and y_test has similar distributions","2863d2db":"# Scaling The dataset","e3f0d227":"Lets see the statistical description of the whole dataset","2516e79b":"We can see from the histogram that the predictor is rightly skewed ","fd87abca":"So there is no object and no missing data","bd42fc6e":"# Multiple Linear Regression","d51491f1":"Lets try Random Forest Regressor","9b8b34c7":"#  Imporing Libraries"}}