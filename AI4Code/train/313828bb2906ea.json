{"cell_type":{"4fbea351":"code","60ce6b46":"code","3013d410":"code","5a1b9d8b":"code","ab6349de":"code","4d85399d":"code","5012a478":"code","8dcd7c7c":"code","7c354b37":"code","4118f4cf":"code","65f30611":"code","c3755c98":"code","0cf218b4":"code","510e9bcb":"code","a5283eae":"code","cc4fbbf7":"code","9026d630":"code","2c49bad9":"code","e8da0af7":"code","b4543f14":"code","6c0d7a19":"code","0ec8b277":"code","eeb25cc7":"code","ad6cecaf":"markdown","202f3f81":"markdown","ab967bb7":"markdown","5a3b9917":"markdown","ef27fa44":"markdown","0227b58f":"markdown","0fbeb9dc":"markdown","d6e43764":"markdown","4036a1db":"markdown","e86cf986":"markdown","a41b2efc":"markdown","d591e02b":"markdown","3b64db23":"markdown","af7bdee5":"markdown","1e86d323":"markdown","6a7ee131":"markdown","3dd7deca":"markdown","cd887ac1":"markdown","a507e37a":"markdown"},"source":{"4fbea351":"import bz2\nfrom nltk.corpus import stopwords,words\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import SnowballStemmer\nfrom string import punctuation\nstemmer = SnowballStemmer('english')\n\ndef process_sentence(sentence, tokens_to_remove, English_words, max_tokens=None):\n    words = word_tokenize(sentence) # Tokenize\n    if max_tokens is not None and len(words) < max_tokens:\n        return None\n    else:\n        words = [w.lower() for w in words if not w.isdigit()] # Convert to lowercase and also remove digits\n        filter_words = [stemmer.stem(w) for w in words if w not in tokens_to_remove and w in English_words] # remove tokens + check english words + stem\n        return filter_words\n","60ce6b46":"%%time\n\nmax_docs = None # test with this number of docs first. If would like to do for all docs, set this value to None\nfilename = bz2.open('..\/input\/amazonreviews\/test.ft.txt.bz2','rt',encoding='utf-8')\ncorpusfile = 'corpus_text.txt'\n\nstop_words = set(stopwords.words('english'))\nlabels = ['__label__1','__label__2']\ntokens_to_remove = stop_words.union(set(punctuation)).union(set(labels))\nEnglish_words = set(words.words())\n\n\ndoc_count = 0\nwith bz2.open('..\/input\/amazonreviews\/test.ft.txt.bz2','rt',encoding='utf-8') as inputfile:\n    with open(corpusfile, 'w') as outputfile:\n        for line in inputfile:\n            filter_words = process_sentence(line, tokens_to_remove, English_words, 100)\n            if filter_words is not None:\n                outputfile.write(f'{\" \".join(filter_words)}\\n') # write the results\n                doc_count += 1\n                if  max_docs and doc_count >= max_docs: # if we do define the max_docs\n                    break","3013d410":"# View the file if needed\nfrom IPython.display import FileLink\nFileLink('corpus_text.txt')","5a1b9d8b":"%%time\n# Give some summaries of the text.\nvocab = set()\nwith open('corpus_text.txt') as corpus_text:\n    documents = corpus_text.readlines()\n    doc_count = len(documents)\n    for doc in documents:\n        words = word_tokenize(doc)\n        vocab = vocab.union(words)\nprint(f'Number of documents {doc_count}, vocabulary size {len(vocab)}')","ab6349de":"import wordcloud\nimport matplotlib.pyplot as plt\ndef show_wordcloud(text, title=None):\n    # Create and generate a word cloud image:\n    wc = wordcloud.WordCloud(background_color='white').generate(text)\n    # Display the generated image:\n    plt.figure(figsize=(10, 10))\n    plt.imshow(wc, interpolation='bilinear')\n    plt.axis(\"off\")\n    if title is not None:\n        plt.title(title)\n    plt.show()\n    \ndef show_wordcloud_for_doc(docIdx, title=None):\n    show_wordcloud(documents[docIdx], title)","4d85399d":"# Create and generate a word cloud image:\nshow_wordcloud(\" \".join(documents), 'WordCloud for the whole corpus')","5012a478":"import numpy as np\n# Generate word cloud for a random document\ndocIdx = np.random.randint(doc_count)\nshow_wordcloud_for_doc(docIdx, f'Word Cloud for document index {docIdx}')","8dcd7c7c":"from nltk.tokenize import word_tokenize\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords, words\nstemmer = SnowballStemmer('english')\n\nstop_words = set(stopwords.words('english'))\nlabels = ['__label__1','__label__2']\ntokens_to_remove = stop_words.union(set(punctuation)).union(set(labels))\nEnglish_words = set(words.words())\n\n\nquery = 'game love music book fun good bad product money waste'\n\ntf = {} # a dictionary of terms in the query (key is the term in the query and value is an array of term frequencies in each of the document)\ndf = {} # a dictionary of terms in the query (key is the term in the query and value is the document appearance)\nwtf = {} # weighted term frequency using log\nidf = {} # inverse document frequency\ntfidf = {} # the tf-idf score for each term for each document\n\nquery_terms = process_sentence(query, tokens_to_remove, English_words)\n# Initializing\nfor term in query_terms:\n    tf[term] = [0 for _ in range(doc_count)]\n    df[term] = 0\n    wtf[term] = [0 for _ in range(doc_count)]\n    idf[term] = 0\n    tfidf[term] = [0 for _ in range(doc_count)]","7c354b37":"%%time\nfor docIdx, doc in enumerate(documents):\n    words = word_tokenize(doc) # they were cleaned so we don't have to clean any more\n    # update term frequency counts\n    for word in words:\n        if word in query_terms:\n            tf[word][docIdx] += 1 # increase term frequency count for the document\n    # update doc frequency counts\n    for term in query_terms:\n        if term in words: # if the term is inside the doc\n            df[term] += 1                ","4118f4cf":"# Now calculate the wtf and the idf\nimport math\nfor term in query_terms:\n    wtf[term] = [1+math.log10(tf_val) if tf_val != 0 else 0 for tf_val in tf[term]] # weighted term frequency as 1 + log10 of tf\n    idf[term] = math.log10(doc_count\/df[term]) # invert document frequency as N\/df\n    tfidf[term] = [wtf_val * idf[term] for wtf_val in wtf[term]] # tf idf is the weighted term frequency * inverse document frequency","65f30611":"%%time\ntfidfdoc = []\nfor docIdx, doc in enumerate(documents):\n    words = word_tokenize(doc) # they were cleaned so we don't have to clean any more\n    # initialize as zero\n    tfidfdoc.append(0)\n    for term in query_terms:\n        if term in words:\n            tfidfdoc[docIdx] += tfidf[term][docIdx]","c3755c98":"import numpy as np\nimport matplotlib.pyplot as plt","0cf218b4":"for term in query_terms:\n    max_count = np.max(tf[term])\n    bins =range(0, max_count+1)\n    plt.figure()\n    plt.hist(tf[term], label=term, bins = bins)\n    plt.title(f'Term frequency for {term}')","510e9bcb":"for term in query_terms:\n    plt.figure()\n    plt.hist(wtf[term], label=term)\n    plt.title(f'Weighted term frequency for {term}')","a5283eae":"plt.figure()\n_ = plt.bar(query_terms, [df[term] for term in query_terms])\n_ = plt.title(\"Document frequencies for the terms\")","cc4fbbf7":"plt.figure()\n_ = plt.bar(query_terms, [idf[term] for term in query_terms])\n_ = plt.title(\"Inverse document frequencies for the terms\")","9026d630":"_ = plt.hist(tfidfdoc)\n_ = plt.title(\"TF-IDF for the query\")","2c49bad9":"for docIdx in np.argsort(-np.array(tfidfdoc))[0:10]:\n    show_wordcloud_for_doc(docIdx, f'WordCloud for document index {docIdx}')","e8da0af7":"# Initialization (docs, query_terms)\ntfidfvecspace = np.zeros((doc_count, len(query_terms)))\n\n# Assign the tfidf for each term for each document\nfor docIdx in range(doc_count): # for each document\n    for termIdx, term in enumerate(query_terms): # for each term\n        tfidfvecspace[docIdx][termIdx] = tfidf[term][docIdx]","b4543f14":"%%time\n# normalize the vectors\ntfidfvecspace = np.array([vec\/np.linalg.norm(vec) if np.linalg.norm(vec) != 0 else vec for vec in tfidfvecspace])\n\n# due to very large number of documents in our corpus, we will do pairwise comparison of the first few number documents \ndoc_limit = 10000\n# for every pair\npairwise_comparison = {}\nfor docIdx1 in range(doc_limit-1):\n    vec1 = tfidfvecspace[docIdx1]\n    for docIdx2 in range(docIdx1 + 1, doc_limit):\n        vec2 = tfidfvecspace[docIdx2]\n        key = f'{docIdx1}-{docIdx2}'\n        pairwise_comparison[key] = vec1.dot(vec2)","6c0d7a19":"plt.figure()\nplt.hist(list(pairwise_comparison.values()))\n_ = plt.plot()","0ec8b277":"# sort the comparison by their similarities\npairwise_comparison = {k: v for k, v in sorted(pairwise_comparison.items(), key=lambda item: item[1], reverse=True)}","eeb25cc7":"iterator = iter(pairwise_comparison.items())\nfor i in range(10):\n    nextItem = next(iterator)\n    docIdxs = [int(docIdx) for docIdx in nextItem[0].split('-')]\n    doc0 = docIdxs[0]\n    doc1 = docIdxs[1]\n    print(f'Two similar documents {doc0}, {doc1} with similarity score of {nextItem[1]}, with two vectors {tfidfvecspace[doc0]} vs. {tfidfvecspace[doc1]}')\n    show_wordcloud_for_doc(docIdxs[0], f'WordCloud for document index {docIdxs[0]}')\n    show_wordcloud_for_doc(docIdxs[1], f'WordCloud for document index {docIdxs[1]}')","ad6cecaf":"### Visualize TF-IDF for the queries","202f3f81":"**Visualize the whole corpus to have a sense about the data**","ab967bb7":"# Task 5: Find a way to visualize your tf-idf results, e.g. Pie chart, bar chart","5a3b9917":"**See some quick statistics about the data**","ef27fa44":"# Task 7.2 Visualize document similarity results","0227b58f":"# Task 4: Calculate tf-idf score for each document based on your query\nTF-IDF score for a query $q$ for a document $d$ is calculated as:\n\\begin{equation}\nScore(q, d) = \\sum_{t \\in q \\cap d} tf.idf_{t, d}\n\\end{equation}","0fbeb9dc":"It is observable that we can retrieve documents relevant to the the query here due to larger word size for the words in the query.","d6e43764":"Visualize top 10 pairs","4036a1db":"# Task 1\nOur dataset was provided by Adam Bittlingmayer from Kaggle (https:\/\/www.kaggle.com\/bittlingmayer\/amazonreviews) covering appoximately 3,600,000 customer reviews from Amazon. \n\n\nThere are 2 categories of labels to classify the review as either positive or negative based on the number of stars given by the writer. To initialize the processes required in task 1, each review with more than 100 words was imported and tokenized. Afterward, all of the tokens that were punctuations, label, stopword, or not an English word (emoji, special character, foreign languages) were removed. \n\n\nThe remaining tokens would undergo the process of lemmatizing to reduce word to its root form, and stemming using Snowball algorithm to accquire primal terms before appending into a clean list.\n\n**Note:** Lemmatization won't really work on single words alone without context or knowledge of its POS tag (i.e. we need to know whether the word is a noun, verb, adjective, adverb) (https:\/\/www.kaggle.com\/alvations\/basic-nlp-with-nltk). By default, if you don't specify the part of speech, then all words are considered as nouns. With this in mind, we will use stemming only.\n\nFinally, the newly clean list was exported into a text file.\n","e86cf986":"# Task 6: Using your tf-idf scores, to represent each document on vector space\nWe will represent each document by the as the vector space  ","a41b2efc":"# Task 3. Term frequency, weighted term frequency, document frequency and inverse document frequency for each term in the query\nTerm frequency $tf_{t, d}$ is the count of occurences of a term $t$ in a document $d$.<br\/>\nDocument frequency $df_{t}$ is the total number of documents in which term $t$ appears.\n\nWe normally use the weighted term frequency as the following equation:\n\\begin{equation}\n    w_{t, d} = \\begin{cases}\n    1 + log_{10}(tf_{t, d}) & \\text{if tf_{t, d} > 0} \\\\\n    0  & \\text{if tf_{t, d} = 0}\n    \\end{cases}\n\\end{equation}\nwhere $tf_{t, d}$ means the count of occurrence of term $t$ in document $d$. <br\/>\n\nSince rare terms are more informative, we often consider inverse document frequency with the follwing equations:\n\\begin{equation}\n    idf_{t} = log_{10}(\\frac{N}{df_{t}})\n\\end{equation}\nwhere $N$ is the number of documents and $df_{t}$ is the number of documents in which the term $t$ appears.\n\nThe TF-IDF for each term for each document is calculated as:\n\\begin{equation}\n    tf.idf_{w, d} = w_{t, d} \\times idf_t\n\\end{equation}\n","d591e02b":"See the overall of the similarity distribution","3b64db23":"### Visualize top 10 documents which are similar most similar to the query","af7bdee5":"### Visualize document frequencies for the query terms","1e86d323":"# Task 2\nAfter cleaning process, we selected our query containing 10 terms. Our query is: \"game love music book fun good bad product money waste\". <font color=red>@Thanh: can you explain why do you choose this query?<\/font>","6a7ee131":"### Visualize term frequencies","3dd7deca":"# Task 7.1 Compare every two documents\u2019 similarity based on cosine similarity","cd887ac1":"### Visualize weighted term frequencies","a507e37a":"### Visualize inverse document frequencies for the query terms"}}