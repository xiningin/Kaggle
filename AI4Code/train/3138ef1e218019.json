{"cell_type":{"3231f6cb":"code","c9e3321a":"code","85132b45":"code","77faa427":"code","b252e193":"code","19cc9539":"code","91eb084e":"code","3bf105ef":"code","416fd9f8":"code","e0ba6084":"code","ae9374bc":"code","279e5b7d":"code","c0bd9c35":"code","5af436e8":"code","63f0ae2d":"code","c748acf5":"code","cd3d9e7b":"code","c7e55791":"code","6639d23c":"code","05ecf415":"code","7bf44fb9":"code","49e4f251":"code","faefcc0c":"code","8a7d114f":"code","5c393968":"markdown","ee2a4d47":"markdown","5be18027":"markdown","11de7217":"markdown","ed7bbce5":"markdown","95e0b675":"markdown","ca162420":"markdown"},"source":{"3231f6cb":"# [up to date code](https:\/\/github.com\/alik604\/cyber-security\/blob\/master\/Intrusion-Detection\/UNSW_NB15.ipynb)\n\n%config IPCompleter.greedy=True\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport re\nimport sklearn\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport matplotlib as matplot\n%matplotlib inline\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","c9e3321a":"import zipfile\n\nzip_ref = zipfile.ZipFile('intrusion-detection.zip', 'r')\nzip_ref.extractall()\nzip_ref.close()\n!ls","85132b45":"train = pd.read_csv('.\/Train_data.csv')\ntest = pd.read_csv('.\/test_data.csv')\ntest = test.drop('Unnamed: 0', axis=1)","77faa427":"# train=train.sample(frac =.50,random_state=1) # TODO ","b252e193":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\n\nset(list(train['xAttack']))\ntrain['xAttack'] = le.fit_transform(train['xAttack'])\ntest['xAttack'] = le.fit_transform(test['xAttack'])\n\ntrain['xAttack'].describe()\n\ntrain['protocol_type'] = le.fit_transform(train['protocol_type'])\ntest['protocol_type'] = le.fit_transform(test['protocol_type'])\n\nprint('\\n')\ntmp = train.corr().abs().sort_values('xAttack')\ngarbage = tmp['xAttack'].nsmallest(5)\n\ngarbage\ngarbage = list(garbage.index) + ['num_outbound_cmds','is_host_login']\n\n","19cc9539":"X_train = train.drop('xAttack', axis=1)\nY_train = train.loc[:,['xAttack']]\nX_test = test.drop('xAttack', axis=1)\nY_test = test.loc[:,['xAttack']]","91eb084e":"# df.ix[2,:]==0 # see where STD== 0, which means all values are the same... which is useless \n# train = train.drop('num_outbound_cmds', axis=1)\n## bad sample code I fixed \n\ncon_list = [\n    'protocol_type', 'service', 'flag', 'land', 'logged_in', 'su_attempted',\n    'is_host_login', 'is_guest_login'\n]\n\ndf = X_train.drop(con_list, axis=1)\n\n#drop n smallest std features\ndf = df.std(axis=0).to_frame()\ntmp = df.nsmallest(5, columns=0)\ntmp = list(\n    tmp.transpose().columns)  #fuckin pandas.core.indexes.base.Index   -_-\n#tmp = tmp.append('num_outbound_cmds')  # might not work...\ntmp = set(tmp + garbage)\nlen(tmp)\ntmp","3bf105ef":"from sklearn import linear_model\n\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import IsolationForest\n\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier","416fd9f8":"LR = linear_model.LinearRegression()\nLR.fit(X_train, Y_train)\nlr_score = LR.score(X_test, Y_test)\nprint('Linear regression processing ,,,')\nprint('Linear regression Score: %.2f %%' % lr_score)","e0ba6084":"try:  #TODO\n    X_train = X_train.drop(tmp,axis=1)\n    X_test = X_test.drop(tmp,axis=1)\nexcept:\n    None\n    \nX_train.shape\nX_test.shape","ae9374bc":"LR = linear_model.LinearRegression()\nLR.fit(X_train, Y_train)\nlr_score = LR.score(X_test, Y_test)\nprint('Linear regression processing ,,,')\nprint('Linear regression Score: %.2f %%' % lr_score)","279e5b7d":"AB = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, learning_rate=1.0)\nRF = RandomForestClassifier(n_estimators=10, criterion='entropy', max_features='auto', bootstrap=True)\nET = ExtraTreesClassifier(n_estimators=10, criterion='gini', max_features='auto', bootstrap=False)\nGB = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=200, max_features='auto')","c0bd9c35":"# y_train = Y_train['xAttack'].ravel()\n# x_train = X_train.values\n# x_test = X_test.values","5af436e8":"AB.fit(X_train, Y_train)\nAB_feature = AB.feature_importances_\n#AB_feature\nab_score = AB.score(X_test, Y_test)\n\nprint('AdaBoostClassifier processing ,,,')\nprint('AdaBoostClassifier Score: %.3f %%' % ab_score)","63f0ae2d":"RF.fit(X_train, Y_train)\nRF_feature = RF.feature_importances_\n#RF_feature\n\nrf_score = RF.score(X_test, Y_test)\n\nprint('RandomForestClassifier processing ,,,')\nprint('RandomForestClassifier Score: %.3f %%' % rf_score)","c748acf5":"ET.fit(X_train, Y_train)\nET_feature = ET.feature_importances_\n#ET_feature\n\net_score = ET.score(X_test, Y_test)\n\nprint('ExtraTreesClassifier processing ,,,')\nprint('ExtraTreeClassifier: %.3f %%' % et_score)","cd3d9e7b":"GB.fit(X_train, Y_train)\n\nGB_feature = GB.feature_importances_\n#GB_feature\n\ngb_score = GB.score(X_test, Y_test)\n\nprint('GradientBoostingClassifier processing ,,,')\nprint('GradientBoostingClassifier Score: %.3f %%' % gb_score)","c7e55791":"feature_df = pd.DataFrame({#'features': X_train.columns.values, # names\n                           'AdaBoost' : AB_feature,\n                           'RandomForest' : RF_feature,\n                           'ExtraTree' : ET_feature,\n                           'GradientBoost' : GB_feature\n                          })\n#feature_df.features\nfeature_df.head(2)","6639d23c":"n = 7\na_f = feature_df.nlargest(n, 'AdaBoost')\ne_f = feature_df.nlargest(n, 'ExtraTree')\ng_f = feature_df.nlargest(n, 'GradientBoost')\nr_f = feature_df.nlargest(n, 'RandomForest')\n\nresult = pd.concat([a_f, e_f, g_f, r_f])\nresult = result.drop_duplicates() \nresult.shape\n\nprint('\\n')\n\ngarbage = np.argsort(result.transpose().mean())\ngarbage = garbage.sort_index()[-5:] # FML... :'(\n    \ngarbage\n\nresult = result.drop(garbage.index)\nresult.shape\n\n\n\narr = X_train.columns.to_numpy()\nresult = result.set_index(np.take(arr,result.index))\nresult","05ecf415":"X_train_SF = X_train[result.index]\nX_test_SF = X_test[result.index]\n\nx = X_train_SF#.reshape(-1, 26)  # 31\ny = Y_train['xAttack'].ravel()\nx.shape\ny.size","7bf44fb9":"clf1 = ExtraTreesClassifier(n_estimators=10, criterion='gini', max_features='auto', bootstrap=False) # .76 \nclf2 = RandomForestClassifier(n_estimators=25, random_state=1)# .77\nclf3 = GradientBoostingClassifier() # .76\nET = ExtraTreesClassifier(n_estimators=10, criterion='gini', max_features='auto', bootstrap=False) # .76 # without this lil fucker, Acc: 0.75 [Ensemble], 0.78 with \n\neclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3),('et',ET)], voting='hard') \n# n =7 with better selection; .79\n# n =7 ; .77\n# n =10 ; .78\n# n =14 ; .77\n\n\n# eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3),('et',ET)], voting='soft') #.76\n\nfor clf, label in zip([clf1, clf2, clf3,ET, eclf], ['Logistic Regression', 'Random Forest', 'GradientBoostingClassifier','ExtraTreesClassifier', 'Ensemble']): \n    # scores = cross_val_score(clf, x, y, cv=2, scoring='accuracy') # cv= 5 \n    # print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n\n    tmp = clf.fit(x,y)\n    pred = clf.score(X_test_SF,Y_test)\n    print(\"Acc: %0.2f [%s]\" % (pred,label))","49e4f251":"# import multiprocessing\n\n# ET = ExtraTreesClassifier(n_estimators=10, criterion='gini', max_features='auto', bootstrap=False) # .76 \n# RF = RandomForestClassifier(n_estimators=25, random_state=1)# .75\n# GB = GradientBoostingClassifier() # .74\n# ET = ExtraTreesClassifier(n_estimators=10, criterion='gini', max_features='auto', bootstrap=False) # .77 # without this lil fucker, Acc: 0.75 [Ensemble]\n# clfList = [ET,RF,GB,ET]\n\n# def spawn(clf):\n#   clf.fit(x,y)\n#   print(\"Done another one!\")\n\n# import time\n# start = time.time()\n\n# if __name__ == '__main__':\n  \n#   for i in clfList:\n#     print(i)\n#     #spawn(i) # 16 secounds \n\n#     # # 16 secounds\n#     # p = threading.Thread(target=spawn, args=(i,))\n#     # p.start()\n#     # p.join()\n    \n#   p=multiprocessing.Pool(6) # 15.65\n#   results = p.map(spawn,clfList) # clfList has 4 models, first 2 are fast, last 2 are slow\n#   results\n\n\n# end = time.time()\n# print(end - start)\n","faefcc0c":"# ET.score(X_test_SF,Y_test)\n","8a7d114f":"import pandas as pd\ntest_data = pd.read_csv(\"..\/input\/intrusion-detection\/test_data.csv\")\nTrain_data = pd.read_csv(\"..\/input\/intrusion-detection\/Train_data.csv\")","5c393968":"### what the fuck am i doing with my life.... 45mins wasted on numpy ","ee2a4d47":"* https:\/\/stackoverflow.com\/questions\/42920148\/using-sklearn-voting-ensemble-with-partial-fit\n\n* https:\/\/gist.github.com\/tomquisel\/a421235422fdf6b51ec2ccc5e3dee1b4\n\n","5be18027":"## lets multithread this fucker","11de7217":"## Drop features with lowest STD","ed7bbce5":"# Intrusion Detection \n## [up to date code](https:\/\/github.com\/alik604\/cyber-security\/blob\/master\/Intrusion-Detection\/UNSW_NB15.ipynb)\n\n### Dataset from https:\/\/www.kaggle.com\/what0919\/intrusion-detection\n\n### Sample code used: https:\/\/www.kaggle.com\/nidhirastogi\/intrusion-detection\/data\n\n\n","95e0b675":"### Reduce train size for faster trainin, remove when in production","ca162420":"### The following few cells are taken from the 'sample code'"}}