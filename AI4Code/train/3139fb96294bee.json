{"cell_type":{"9a8b6cf9":"code","8e09d7b9":"code","11ccbce9":"code","4c380d6a":"code","1ac166e6":"code","20308359":"code","a636f456":"code","201504eb":"code","5b931873":"code","a914722f":"code","fbe7b687":"code","c93b99bd":"code","66738283":"code","4acd9211":"code","e8cd21e2":"code","d625b099":"code","c9f46126":"code","2c9a3931":"code","02cae3a5":"code","428dd246":"code","27df356e":"code","cb4923e4":"code","5289e591":"code","0310dd46":"code","6efff745":"code","34d4946c":"code","111e0159":"code","a058a0a6":"code","2d83e58f":"code","0b1c093f":"code","48f48f62":"code","83ef7223":"code","b265136f":"code","770a5331":"code","bb524757":"code","0d4562d8":"markdown","cb3e48d3":"markdown","147d1f89":"markdown","2cb99ba3":"markdown","bd253a90":"markdown","a38e3407":"markdown","a54b2735":"markdown","ec25bdf4":"markdown","8a6d842f":"markdown","f3a5ad33":"markdown","3cc8c39c":"markdown","d99c721b":"markdown","ad0c7548":"markdown","08af96ec":"markdown"},"source":{"9a8b6cf9":"import time\nstart_time = time.time()\n\n# Data analysis and wrangling\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport IPython\nimport IPython.display\nimport librosa\nimport librosa.display\nimport random\nfrom tqdm import tqdm_notebook\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.vision.data import *\nfrom fastai.imports import *\nfrom fastai.callback import *\nfrom fastai.callbacks import *\n\n# Machine learning\nfrom sklearn import preprocessing\nimport sklearn.metrics\nfrom sklearn.metrics import label_ranking_average_precision_score\n\n# File handling\nfrom pathlib import Path\nimport gc\nimport os\nprint(os.listdir(\"..\/input\"))","8e09d7b9":"# from official code https:\/\/colab.research.google.com\/drive\/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8#scrollTo=cRCaCIb9oguU\ndef _one_sample_positive_class_precisions(scores, truth):\n    \"\"\"Calculate precisions for each true class for a single sample.\n\n    Args:\n      scores: np.array of (num_classes,) giving the individual classifier scores.\n      truth: np.array of (num_classes,) bools indicating which classes are true.\n\n    Returns:\n      pos_class_indices: np.array of indices of the true classes for this sample.\n      pos_class_precisions: np.array of precisions corresponding to each of those\n        classes.\n    \"\"\"\n    num_classes = scores.shape[0]\n    pos_class_indices = np.flatnonzero(truth > 0)\n    # Only calculate precisions if there are some true classes.\n    if not len(pos_class_indices):\n        return pos_class_indices, np.zeros(0)\n    # Retrieval list of classes for this sample.\n    retrieved_classes = np.argsort(scores)[::-1]\n    # class_rankings[top_scoring_class_index] == 0 etc.\n    class_rankings = np.zeros(num_classes, dtype=np.int)\n    class_rankings[retrieved_classes] = range(num_classes)\n    # Which of these is a true label?\n    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n    retrieved_class_true[class_rankings[pos_class_indices]] = True\n    # Num hits for every truncated retrieval list.\n    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n    # Precision of retrieval list truncated at each hit, in order of pos_labels.\n    precision_at_hits = (\n            retrieved_cumulative_hits[class_rankings[pos_class_indices]] \/\n            (1 + class_rankings[pos_class_indices].astype(np.float)))\n    return pos_class_indices, precision_at_hits\n\n\ndef calculate_per_class_lwlrap(truth, scores):\n    \"\"\"Calculate label-weighted label-ranking average precision.\n\n    Arguments:\n      truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n        of presence of that class in that sample.\n      scores: np.array of (num_samples, num_classes) giving the classifier-under-\n        test's real-valued score for each class for each sample.\n\n    Returns:\n      per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each\n        class.\n      weight_per_class: np.array of (num_classes,) giving the prior of each\n        class within the truth labels.  Then the overall unbalanced lwlrap is\n        simply np.sum(per_class_lwlrap * weight_per_class)\n    \"\"\"\n    assert truth.shape == scores.shape\n    num_samples, num_classes = scores.shape\n    # Space to store a distinct precision value for each class on each sample.\n    # Only the classes that are true for each sample will be filled in.\n    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n    for sample_num in range(num_samples):\n        pos_class_indices, precision_at_hits = (\n            _one_sample_positive_class_precisions(scores[sample_num, :],\n                                                  truth[sample_num, :]))\n        precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n            precision_at_hits)\n    labels_per_class = np.sum(truth > 0, axis=0)\n    weight_per_class = labels_per_class \/ float(np.sum(labels_per_class))\n    # Form average of each column, i.e. all the precisions assigned to labels in\n    # a particular class.\n    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) \/\n                        np.maximum(1, labels_per_class))\n    # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n    #                = np.sum(precisions_for_samples_by_classes) \/ np.sum(precisions_for_samples_by_classes > 0)\n    #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n    #                = np.sum(per_class_lwlrap * weight_per_class)\n    return per_class_lwlrap, weight_per_class\n\n\n# Wrapper for fast.ai library\ndef lwlrap(scores, truth, **kwargs):\n    score, weight = calculate_per_class_lwlrap(to_np(truth), to_np(scores))\n    return torch.Tensor([(score * weight).sum()])","11ccbce9":"training_curated_df = pd.read_csv(\"..\/input\/train_curated.csv\")\ntraining_noisy_df = pd.read_csv(\"..\/input\/train_noisy.csv\")\ntraining_df = [training_curated_df, training_noisy_df]\ntesting_df = pd.read_csv('..\/input\/sample_submission.csv')","4c380d6a":"Path('trn_curated').mkdir(exist_ok=True, parents=True)\nPath('trn_noisy').mkdir(exist_ok=True, parents=True)\nPath('test').mkdir(exist_ok=True, parents=True)","1ac166e6":"# preview the data\ntraining_curated_df.head()","20308359":"# preview the data\ntraining_noisy_df.head()","a636f456":"training_curated_df.info()\nprint('_'*40)\ntraining_noisy_df.info()","201504eb":"labels_curated = training_curated_df['labels'].unique()\nprint(labels_curated.shape)\nprint('_'*40)\nprint(labels_curated)","5b931873":"labels_noisy = training_noisy_df['labels'].unique()\nprint(labels_noisy.shape)\nprint('_'*40)\nprint(labels_noisy)","a914722f":"training_curated_df.describe()","fbe7b687":"#EasyDict allows to access dict values as attributes (works recursively). A Javascript-like properties dot notation for python dicts.\n#It is mandatory in order to use the library below\n# Special thanks to https:\/\/github.com\/makinacorpus\/easydict\/blob\/master\/easydict\/__init__.py\nclass EasyDict(dict):\n\n    def __init__(self, d=None, **kwargs):\n        if d is None:\n            d = {}\n        if kwargs:\n            d.update(**kwargs)\n        for k, v in d.items():\n            setattr(self, k, v)\n        # Class attributes\n        for k in self.__class__.__dict__.keys():\n            if not (k.startswith('__') and k.endswith('__')) and not k in ('update', 'pop'):\n                setattr(self, k, getattr(self, k))\n\n    def __setattr__(self, name, value):\n        if isinstance(value, (list, tuple)):\n            value = [self.__class__(x)\n                     if isinstance(x, dict) else x for x in value]\n        elif isinstance(value, dict) and not isinstance(value, self.__class__):\n            value = self.__class__(value)\n        super(EasyDict, self).__setattr__(name, value)\n        super(EasyDict, self).__setitem__(name, value)\n\n    __setitem__ = __setattr__\n\n    def update(self, e=None, **f):\n        d = e or dict()\n        d.update(f)\n        for k in d:\n            setattr(self, k, d[k])\n\n    def pop(self, k, d=None):\n        delattr(self, k)\n        return super(EasyDict, self).pop(k, d)","c93b99bd":"#Thanks to https:\/\/github.com\/daisukelab\/ml-sound-classifier\ndef read_audio(conf, pathname, trim_long_data):\n    y, sr = librosa.load(pathname, sr=conf.sampling_rate) #Loads an audio file as a floating point time series. This functions samples the sound\n    # trim silence\n    if 0 < len(y): # workaround: 0 length causes error\n        y, _ = librosa.effects.trim(y) # trim, top_db=default(60)\n    # make it unified length to conf.samples\n    if len(y) > conf.samples: # long enough\n        if trim_long_data:\n            y = y[0:0+conf.samples]\n    else: # pad blank\n        padding = conf.samples - len(y)    # add padding at both ends\n        offset = padding \/\/ 2\n        y = np.pad(y, (offset, conf.samples - len(y) - offset), 'constant')\n    return y\n\ndef audio_to_melspectrogram(conf, audio):\n    spectrogram = librosa.feature.melspectrogram(audio, \n                                                 sr=conf.sampling_rate,\n                                                 n_mels=conf.n_mels,\n                                                 hop_length=conf.hop_length,\n                                                 n_fft=conf.n_fft,\n                                                 fmin=conf.fmin,\n                                                 fmax=conf.fmax)\n    spectrogram = librosa.power_to_db(spectrogram)\n    spectrogram = spectrogram.astype(np.float32) #Returns an 128 x L array corresponding to the spectrogram of the sound (L = 128*n\u00b0 of s)\n    return spectrogram\n\ndef melspectrogram_to_delta(mels):\n    return librosa.feature.delta(mels)\n\ndef show_melspectrogram(conf, mels, title='Log-frequency power spectrogram'):\n    librosa.display.specshow(mels, x_axis='time', y_axis='mel', \n                             sr=conf.sampling_rate, hop_length=conf.hop_length,\n                            fmin=conf.fmin, fmax=conf.fmax)\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(title)\n    plt.show()\n\ndef read_as_melspectrogram(conf, pathname, trim_long_data, debug_display=False):\n    x = read_audio(conf, pathname, trim_long_data)\n    mels = audio_to_melspectrogram(conf, x)\n    if debug_display:\n        delta = melspectrogram_to_delta(mels)\n        delta_squared = melspectrogram_to_delta(delta)\n        IPython.display.display(IPython.display.Audio(x, rate=conf.sampling_rate))\n        show_melspectrogram(conf, mels)\n        show_melspectrogram(conf, delta)\n        show_melspectrogram(conf, delta_squared)\n    return mels\n\nconf = EasyDict()\nconf.sampling_rate = 44100\nconf.duration = 2\nconf.hop_length = 347 * conf.duration # to make time steps 128\nconf.fmin = 20\nconf.fmax = conf.sampling_rate \/\/ 2\nconf.n_mels = 128\nconf.n_fft = conf.n_mels * 20\nconf.samples = conf.sampling_rate * conf.duration","66738283":"# example\npath = '..\/input\/train_curated\/0006ae4e.wav'\nx = read_audio(conf, path, trim_long_data=False)\nprint(x)\nprint('_'*40)\nprint(audio_to_melspectrogram(conf, x))\nprint(audio_to_melspectrogram(conf, x).shape)\nx1 = read_as_melspectrogram(conf, path, trim_long_data=False, debug_display=True)","4acd9211":"\"\"\"\nThe mono_to_color function takes as an input the spectrogram of our sound (list of array, see above). \nIt stacks it three times, so that it has the same shape as a classic RGB image.\nThen it standardize the array (take a matrix and change it so that its mean is equal to 0 and variance is 1). This improves performance.\nThen it normalizes each value between 0 and 255 (gray scale). \n\"\"\"\n\ndef mels_preprocessing(X1, X2, X3, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n    # Stack X as [X,X,X]\n    X = np.stack([X1, X2, X3], axis=-1)\n\n    # Standardize\n    mean = mean or X.mean()\n    std = std or X.std()\n    #Standardization. Xstd has 0 mean and 1 variance\n    Xstd = (X - mean) \/ (std + eps)\n    _min, _max = Xstd.min(), Xstd.max()\n    norm_max = norm_max or _max\n    norm_min = norm_min or _min\n    if (_max - _min) > eps:\n        # Scale to [0, 255]\n        V = Xstd\n        V[V < norm_min] = norm_min\n        V[V > norm_max] = norm_max\n        V = 255 * (V - norm_min) \/ (norm_max - norm_min)\n        V = V.astype(np.uint8)\n    else:\n        # Just zero\n        V = np.zeros_like(Xstd, dtype=np.uint8)\n    return V\n\ndef convert_wav_to_image(df, source, img_dest):\n    X = []\n    for i, row in tqdm_notebook(df.iterrows()):\n        x1 = read_as_melspectrogram(conf, source\/str(row.fname), trim_long_data=False)\n        x2 = melspectrogram_to_delta(x1)\n        x3 = melspectrogram_to_delta(x2)\n        x_preprocessed = mels_preprocessing(x1, x2, x3)\n        X.append(x_preprocessed)\n    return df, X","e8cd21e2":"training_curated_df, X_train_curated = convert_wav_to_image(training_curated_df, source=Path('..\/input\/train_curated'), img_dest=Path('trn_curated'))\ntesting_df, X_test = convert_wav_to_image(testing_df, source=Path('..\/input\/test'), img_dest=Path('test'))\n\nprint(f\"Finished data conversion at {(time.time()-start_time)\/3600} hours\")","d625b099":"for i in range(0,6):\n    a = np.asarray(X_train_curated[i:i+1])\n    a = np.squeeze(a)\n    print(a.shape)\n    plt.imshow(a)\n    plt.show()","c9f46126":"CUR_X_FILES, CUR_X = list(training_curated_df.fname.values), X_train_curated\n\ndef open_fat2019_image(fn, convert_mode, after_open)->Image:\n    # open\n    idx = CUR_X_FILES.index(fn.split('\/')[-1])\n    x = PIL.Image.fromarray(CUR_X[idx])\n    # crop\n    time_dim, base_dim = x.size\n    crop_x = 0\n    #crop_x = random.randint(0, time_dim - base_dim)\n    x = x.crop([crop_x, 0, crop_x+base_dim, base_dim])    \n    # standardize\n    return Image(pil2tensor(x, np.float32).div_(255))\n\nvision.data.open_image = open_fat2019_image","2c9a3931":"#Batch size --> How many images are trained at one time. Lower it if you run out of memory\nbs = 64\n#Image size. Square images makes the learning process faster. We can increase the size of the images once our model is stable, in order to improve accuracy.\nsize = 128\n\n#Performing data augmentation\ntfms = get_transforms(do_flip=False, max_rotate=0, max_lighting=0.1, max_zoom=0, max_warp=0.)\n\n#We put the transformed image data into \/kaggle\/working because ..\/input is a read-only directory.\nsrc = (ImageList.from_csv('\/kaggle\/working', '..\/input\/train_curated.csv', folder='..\/input\/train_curated')\n       .split_by_rand_pct(0.2).label_from_df(label_delim=','))\n\n#Creates a databunch, because our cnn_learner below needs a databunch.\ndata = src.transform(tfms, size=size).databunch(bs=bs).normalize()","02cae3a5":"data.show_batch(4)","428dd246":"arch = models.resnet18\n\nlearn = cnn_learner(data, arch, pretrained=False, metrics=[lwlrap], wd = 0.1, ps = 0.5)\n\nlearn.lr_find()\nlearn.recorder.plot()","27df356e":"learn.fit_one_cycle(5, 1e-2)\nlearn.save('first-attempt-128')","cb4923e4":"learn.unfreeze()\nlearn.fit_one_cycle(1)","5289e591":"learn.lr_find()\nlearn.recorder.plot()","0310dd46":"learn.fit(20, slice(2e-3, 2e-4))\nlearn.save('second-attempt-128')","6efff745":"learn.lr_find()\nlearn.recorder.plot()","34d4946c":"learn.fit(20, slice(1e-3, 1e-4))\nlearn.save('third-attempt-128')","111e0159":"size = 256\n\n#Creates a databunch, because our cnn_learner below needs a databunch.\ndata = src.transform(tfms, size=size).databunch(bs=bs).normalize(imagenet_stats)","a058a0a6":"#Replace with 256x256 databunch\nlearn.data = data\n#Freeze the model\nlearn.freeze()\n#Plot lr_find()\nlearn.lr_find()\nlearn.recorder.plot()","2d83e58f":"learn.fit(5, 3e-3)\nlearn.save('first-attempt-256')","0b1c093f":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot()","48f48f62":"learn.fit(7, slice(1e-3, 1e-4))\nlearn.save('second-attempt-256')","83ef7223":"learn.recorder.plot_losses()","b265136f":"learn.fit_one_cycle(20, slice(5e-4, 5e-5), callbacks=[SaveModelCallback(learn, monitor='lwlrap', mode='max')])","770a5331":"learn.export()","bb524757":"CUR_X_FILES, CUR_X = list(testing_df.fname.values), X_test\n\ntest = ImageList.from_csv(Path('\/kaggle\/working'), Path('..\/input\/sample_submission.csv'), folder=Path('..\/input\/test'))\nlearn = load_learner(Path('\/kaggle\/working'), test=test)\npreds, _ = learn.get_preds(ds_type=DatasetType.Test)\n\ntesting_df[learn.data.classes] = preds\ntesting_df.to_csv('submission.csv', index=False)\ntesting_df.head()","0d4562d8":"### **<div id=\"I2\">2. Tools importing<\/div>**\n\nHere we are importing every useful tool needed during our research process.","cb3e48d3":"### **<div id=\"IV4\">4. Normalizing images and performing data augmentation<\/div>**\n\nNow that we have transformed our sound into images, we want them to have the same scale (for example, 128x128), for training purposes.\nWe will also perform **data augmentation**.\n\n![test](https:\/\/cdn-images-1.medium.com\/max\/800\/1*C8hNiOqur4OJyEZmC7OnzQ.png)\n\nData augmentation consists in making minor alterations to our existing dataset. Minor changes such as flips or translations or rotations. Our neural network would think these are distinct images.\nA convolutional neural network that can robustly classify objects even if its placed in different orientations is said to have the property called invariance. More specifically, a CNN can be invariant to translation, viewpoint, size or illumination (Or a combination of the above).\n\nThis essentially is the premise of data augmentation. And augmentation can also help even with a large dataset; it can help to increase the amount of relevant data in your dataset. This is related to the way with which neural networks learn.\n\nOf course, each change are not good for each type of data. For example, in our problem, we may not want to flip or rotate our image, since it would alter our sound in a bad way. But we can for exmple increase the brightness of the image (resulting in a louder sound I guess), or translate it.  \n","147d1f89":"## **<div id=\"III\">III. Wrangle, cleanse and Prepare Data for Consumption<\/div>**","2cb99ba3":"### **<div id=\"I3\">3. Label ranking average precision<\/div>**\n\nThe task consists of predicting the audio labels (tags) for every test clip. Some test clips bear one label while others bear several labels. The predictions are to be done at the clip level, i.e., no start\/end timestamps for the sound events are required.\n\nThe primary competition metric will be label-weighted label-ranking average precision (lwlrap, pronounced \"Lol wrap\"). This measures the average precision of retrieving a ranked list of relevant labels for each test clip (i.e., the system ranks all the available labels, then the precisions of the ranked lists down to each true label are averaged). This is a generalization of the mean reciprocal rank measure (used in last year\u2019s edition of the competition) for the case where there can be multiple true labels per test item. The novel \"label-weighted\" part means that the overall score is the average over all the labels in the test set, where each label receives equal weight (by contrast, plain lrap gives each test item equal weight, thereby discounting the contribution of individual labels when they appear on the same item as multiple other labels).\n\nWe use label weighting because it allows per-class values to be calculated, and still have the overall metric be expressed as simple average of the per-class metrics (weighted by each label's prior in the test set). For participant\u2019s convenience, a Python implementation of lwlrap is provided in this public Google Colab.","bd253a90":"# **Audio Tagging**","a38e3407":"## **<div id=\"IV\">IV. Turning sound into bits<\/div>**\n\n### **<div id=\"IV1\">1. Sampling sound data<\/div>**\n\nThe first step in speech recognition is obvious\u200a\u2014\u200awe need to feed sound waves into a computer. But sound is transmitted as waves. How do we turn sound waves into numbers?\n\n![](https:\/\/cdn-images-1.medium.com\/max\/1200\/1*6_q1VIVJuavYa-9Uby_L-A.png)\n\nSound waves are one-dimensional. At every moment in time, they have a single value based on the height of the wave. To turn this sound wave into numbers, we just record of the height of the wave at equally-spaced points.\n\nThis is called **sampling**. We are taking a reading thousands of times a second and recording a number representing the height of the sound wave at that point in time. That\u2019s basically all an uncompressed .wav audio file is.\n\n\u201cCD Quality\u201d audio is sampled at 44.1khz (44,100 readings per second). For our problem, here is how we will proceed:\n* Handle sampling rate 44.1kHz as is, no information loss.\n* Size of each file will be 128 x L, L is audio seconds x 128; [128, 256] if sound is 2s long.\n\n### **<div id=\"IV2\">2. Pre-processing our sampled Sound Data<\/div>**\n\nWe now have an array of numbers with each number representing the sound wave\u2019s amplitude at 1\/44100th of a second intervals.\n\nWe could feed these numbers right into a neural network. But trying to recognize speech patterns by processing these samples directly is difficult. Instead, we can make the problem easier by doing some pre-processing on the audio data.\n\nTo make this data easier for a neural network to process, we are going to break apart this complex sound wave into it\u2019s component parts. We\u2019ll break out the low-pitched parts, the next-lowest-pitched-parts, and so on. Then by adding up how much energy is in each of those frequency bands (from low to high), we create a fingerprint of sorts for this audio snippet.\n\nWe do this using a mathematic operation called a Fourier transform. It breaks apart the complex sound wave into the simple sound waves that make it up. Once we have those individual sound waves, we add up how much energy is contained in each one.\n\nThe end result is a score of how important each frequency range is, from low pitch (i.e. bass notes) to high pitch.\n\n![](https:\/\/cdn-images-1.medium.com\/max\/1200\/1*A4CxgdyqYd_nrF3e-7ETWA.png)\n\nIf we repeat this process on every 20 millisecond chunk of audio, we end up with a spectrogram. This is what we have done below on one of our audio sound.","a54b2735":"The array above represents our spectrogram as an RGB image.\n* Each 1x3 list represents **one pixel of our image**. Each value in this list represents respectively the Red, Green and Blue value (between 0 and 255) of the pixel (see image below). In our case, as our \"color\" is \"one dimensional\" (one pixel in our spectrogram is juste a real number representing the db value for a particular frequency at a particular time), our pixel color will be a shade of grey. Thus, the three RGB values will always be the same.\n* Each list of 1x3 list represents **one horizontal line of our image**.\n* **The whole array is our sound**, represented as a **gray-scaled image**. \n\n![image.png](attachment:image.png)","ec25bdf4":"# Table of content\n\n## **<div id=\"I\">I. Define the problem<\/div>**\n\n### **<div id=\"I0\">0. Sources<\/div>**\n\nhttps:\/\/medium.com\/@ageitgey\/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a\nhttps:\/\/medium.com\/nanonets\/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced\nhttps:\/\/www.kaggle.com\/daisukelab\/cnn-2d-basic-solution-powered-by-fast-ai\n\n### **<div id=\"I1\">1. Problem description<\/div>**\n\nOne year ago, Freesound and Google\u2019s Machine Perception hosted an audio tagging competition challenging Kagglers to build a general-purpose auto tagging system. This year they\u2019re back and taking the challenge to the next level with multi-label audio tagging, doubled number of audio categories, and a noisier than ever training set.\n\n![](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/freesound\/task2_freesound_audio_tagging.png)\n\nHere's the background: Some sounds are distinct and instantly recognizable, like a baby\u2019s laugh or the strum of a guitar. Other sounds are difficult to pinpoint. If you close your eyes, could you tell the difference between the sound of a chainsaw and the sound of a blender?\n\nBecause of the vastness of sounds we experience, no reliable automatic general-purpose audio tagging systems exist. A significant amount of manual effort goes into tasks like annotating sound collections and providing captions for non-speech events in audiovisual content.\n\nTo tackle this problem, Freesound (an initiative by MTG-UPF that maintains a collaborative database with over 400,000 Creative Commons Licensed sounds) and Google Research\u2019s Machine Perception Team (creators of AudioSet, a large-scale dataset of manually annotated audio events with over 500 classes) have teamed up to develop the dataset for this new competition.\n\nTo win this competition, Kagglers will develop an algorithm to tag audio data automatically using a diverse vocabulary of 80 categories.\n\nIf successful, your systems could be used for several applications, ranging from automatic labelling of sound collections to the development of systems that automatically tag video content or recognize sound events happening in real time.","8a6d842f":"### **<div id=\"I4\">4. Data<\/div>**\n\n#### **Train set**\n\nThe train set is meant to be for system development. The idea is to limit the supervision provided (i.e., the manually-labeled data), thus promoting approaches to deal with label noise. The train set is composed of two subsets as follows:\n\n**Curated subset**\n\nThe curated subset is a small set of manually-labeled data from FSD.\n* Number of clips\/class: 75 except in a few cases (where there are less)\n* Total number of clips: 4970\n* Avge number of labels\/clip: 1.2\n* Total duration: 10.5 hours\n\nThe duration of the audio clips ranges from 0.3 to 30s due to the diversity of the sound categories and the preferences of Freesound users when recording\/uploading sounds. It can happen that a few of these audio clips present additional acoustic material beyond the provided ground truth label(s).\n\n**Noisy subset**\n\nThe noisy subset is a larger set of noisy web audio data from Flickr videos taken from the YFCC dataset.\n* Number of clips\/class: 300\n* Total number of clips: 19815\n* Avge number of labels\/clip: 1.2\n* Total duration: ~80 hours\n\nThe duration of the audio clips ranges from 1s to 15s, with the vast majority lasting 15s.\n\nConsidering the numbers above, per-class data distribution available for training is, for most of the classes, 300 clips from the noisy subset and 75 clips from the curated subset, which means 80% noisy - 20% curated at the clip level (not at the audio duration level, considering the variable-length clips).\n\n#### **Test set**\n\nThe test set is used for system evaluation and consists of manually-labeled data from FSD. Since most of the train data come from YFCC, some acoustic domain mismatch between the train and test set can be expected. All the acoustic material present in the test set is labeled, except human error, considering the vocabulary of 80 classes used in the competition.\n\nThe test set is split into two subsets, for the public and private leaderboards. In this competition, the submission is to be made through Kaggle Kernels. Only the test subset corresponding to the public leaderboard is provided (without ground truth).\n\nSubmissions must be made with inference models running in Kaggle Kernels. However, participants can decide to train also in the Kaggle Kernels or offline (see Kernels Requirements for details).\n\nThis is a kernels-only competition with two stages. The first stage comprehends the submission period until the deadline on June 10th. After the deadline, in the second stage, Kaggle will rerun your selected kernels on an unseen test set. The second-stage test set is approximately three times the size of the first. You should plan your kernel's memory, disk, and runtime footprint accordingly.\n\n#### **Files**\n\n* train_curated.csv - ground truth labels for the curated subset of the training audio files (see Data Fields below)\n* train_noisy.csv - ground truth labels for the noisy subset of the training audio files (see Data Fields below)\n* sample_submission.csv - a sample submission file in the correct format, including the correct sorting of the sound categories; it contains the list of audio files found in the test.zip folder (corresponding to the public leaderboard)\n* train_curated.zip - a folder containing the audio (.wav) training files of the curated subset\n* train_noisy.zip - a folder containing the audio (.wav) training files of the noisy subset\n* test.zip - a folder containing the audio (.wav) test files for the public leaderboard\n\n#### **Columns**\n\nEach row of the train_curated.csv and train_noisy.csv files contains the following information:\n\n* fname: the audio file name, eg, 0006ae4e.wav\n* labels: the audio classification label(s) (ground truth). Note that the number of labels per clip can be one, eg, Bark or more, eg, \"Walk_and_footsteps,Slam\".\n","f3a5ad33":"### **<div id=\"VI\">VI. New learner with increased size of images<\/div>**\n\nNow that we have a pretty good learner fed with 128x128 images, let's: \n* Create a new databunch full of 256x256 images, \n* Keep the same learner as before ('third-attempt-128'),\n* Replace the data inside the learner with my new 256x256 data,\n* Freeze the model again (in order to train only the last few layers).","3cc8c39c":"## **<div id=\"V\">V. Model data<\/div>**\n\nNow we will start training our model. We will use a **convolutional neural network** backbone and a fully connected head with a single hidden layer as a classifier. Our model takes images as input and will output the predicted probability for each of the categories.\n\nWe need to feed our learner with a databunch, and an architecture model. **resnet34** is a very good architecture to get started with. We can use **resnet50** to try getting better results once we are happy with our model. If we run out of memory while using resnet50, we can try to lower *bs* (batch size, how many images are trained at one time). Computing time will get a little bit longer, but we won't run out of memory anymore.\n\nWe have to set *pretrained* to *False*, as pretrained models are forbidden in this competition. For our metrics, we use *lwlrap* as it is the metric used in the competition.\n\nWe use **lr_find** to find the best learning rate for our model. It seems to start diverging when lr > 0,1, so we choose a value ten times lower, i.e. **lr = 0,01**.\n\nWe will train for 5 epochs (5 cycles through all our data).","d99c721b":"### **<div id=\"IV3\">3. Transforming sound into images<\/div>**\n\n\nNow that we can convert our sounds into spectrograms, we want to be able to utilize them. The next step is to convert our spectrograms into images. There is a very powerful model for image recognition which is called CNN (for Convolutional Neural Network), and this model gives also ver good results for audio recognition; but before using it, we need to actually transform our sounds into images, using these spectrograms.","ad0c7548":"## **<div id=\"II\">II. Gather the data<\/div>**\n\nWe start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.","08af96ec":"* The first array corresponds to our **sampled audio file**. It is an array, where every value corresponds to the sound's wave amplitude (Hz) every 1\/44100th of a second.\n* The second array corresponds to the **spectrogram of our audio file**. It is an list of array.\n    * First, we cut our sampled audio file into 20ms pieces (each piece contains 44100\/50 = 882 values.\n    * Then, we perform a Fourier transform for each 20ms piece. It breaks apart the complex sound wave into the simple sound waves that make it up.\n    * Once we have those individual sound waves, we add up how much energy is contained in each one. This creates the first list of our array\n    * Last, we do the same thing for each 20ms piece, in order to create our spectrogram.\n* The last item is a **representation of our spectrogram**. Each list in our previous array is displayed as a colored vertical bar. The more bright the color is, the more the frequency is represented for this audio snippet.\n"}}