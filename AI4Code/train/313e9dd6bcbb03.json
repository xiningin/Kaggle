{"cell_type":{"77ecae9d":"code","aa114fee":"code","415be706":"code","681d4ab8":"code","99f5dffc":"code","60105b14":"code","135d774c":"code","fce4d6be":"code","f341f630":"code","5624c9b0":"code","f649fb67":"code","fec59804":"code","db559838":"code","31b597c6":"code","fd60e6fc":"code","421d4f89":"code","6fba07c2":"code","6a13c8cc":"code","de390d69":"code","4dfab8c9":"code","b16ed753":"code","62241e9e":"markdown","a5856b4e":"markdown","b036567d":"markdown","7fbe360a":"markdown","c6b53976":"markdown","b2c98259":"markdown","4f8e8923":"markdown","126a51e4":"markdown","d552e4f7":"markdown"},"source":{"77ecae9d":"# import relevant libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport keras","aa114fee":"# MNIST data can be loaded from the keras library. \nfrom keras.datasets import mnist\n\ndef load_data():\n  (train_samples,train_labels), (test_samples,test_labels) = mnist.load_data()\n  return train_samples, train_labels, test_samples, test_labels\n\ntrain_samples, train_labels, test_samples, test_labels = load_data()","415be706":"# check the shape of the data\nprint(train_samples.shape)\nprint(train_labels.shape)\nprint(test_samples.shape)\nprint(test_labels.shape)\n\nprint(train_labels[0:8])","681d4ab8":"print(np.amax(train_samples))\nprint(np.amin(train_samples))","99f5dffc":"for i in range(0,3):\n  pixels=train_samples[i]\n  plt.imshow(pixels, cmap = plt.cm.binary)\n  plt.show()\n  print(\"Label of image is\", train_labels[i])","60105b14":"def convert_dtype(x):\n   \n    \n    x_float=x.astype('float32')\n    return x_float\n\ntrain_samples = convert_dtype(train_samples)\ntest_samples = convert_dtype(test_samples)","135d774c":"def normalize(x):\n  y = (x - np.min(x))\/np.ptp(x)   #ptp function is used to find the range\n  return y\n\ntrain_samples = normalize(train_samples)\ntest_samples = normalize(test_samples)","fce4d6be":"# to check if train_samples is normalized or not\nnp.isclose(np.amax(train_samples), 1)","f341f630":"# We need to reshape our train_data to be of shape (samples, height, width, channels) pass to Conv2D layer of keras\n\ndef reshape(x):\n    \n    \n    x_r=x.reshape(x.shape[0],x.shape[1],x.shape[2],1)\n    return x_r\n\ntrain_samples = reshape(train_samples)\ntest_samples = reshape(test_samples)\n\n","5624c9b0":"def oneHot(y, Ny):\n    \n    import tensorflow \n    from keras.utils import to_categorical\n    Ny=len(np.unique(y))\n    y_oh=to_categorical(y,num_classes=Ny)\n    return y_oh\n\n# example\ntrain_labels = oneHot(train_labels, 10)\ntest_labels = oneHot(test_labels, 10)","f649fb67":"from keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten\n\nmodel = Sequential()\nmodel.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\nmodel.add(Conv2D(32, kernel_size=3, activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation='softmax'))","fec59804":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","db559838":"results = model.fit(train_samples, train_labels, validation_split = 0.1, epochs=4, batch_size=250)","31b597c6":"results.history.keys()\nimport matplotlib.pyplot as plt\nplt.plot(range(len(results.history['val_loss'])), results.history['val_loss'])\nplt.show()","fd60e6fc":"plot = pd.DataFrame()\nplot['Validation Accuracy'] = model.history.history['val_accuracy']\nplot['Training Accuracy'] = model.history.history['accuracy']\nplot['Validation Loss'] = model.history.history['val_loss']\nplot['Training Loss'] = model.history.history['loss']\nplot['Epoch'] = plot.reset_index()['index']+1\nplot","421d4f89":"def predict(x):\n    y = model.predict(x)\n    return y\n\npredicted_labels_train = predict(train_samples)","6fba07c2":"def oneHot_tolabel(y):\n    \n    y_b=[]\n    from sklearn.preprocessing import LabelEncoder\n    labelencoder = LabelEncoder()\n    y_b[:, 0] = labelencoder.fit_transform(y_b[:, 0])\n    return y_b\n    ","6a13c8cc":"def accuracy(x_train, y_train, model):\n    \n    loss,acc = model.evaluate(train_samples, train_labels,verbose=0) \n    return acc\n\nacc = accuracy(train_samples, train_labels, model)\nprint('Train accuracy is, ', acc*100, '%')","de390d69":"def create_confusion_matrix(true_labels, predicted_labels):\n    \n    from sklearn.metrics import confusion_matrix\n    cm = confusion_matrix(true_labels.argmax(axis=1), predicted_labels.argmax(axis=1))\n    return cm\n\ncm = create_confusion_matrix((train_labels), (predict(train_samples)))\nprint(cm)","4dfab8c9":"def accuracy(x_test, y_test, model):\n    \n    loss,acc = model.evaluate(test_samples, test_labels,verbose=0) \n    return acc\n\nacc = accuracy(test_samples, test_labels, model)\nprint('Test accuracy is, ', acc*100, '%')","b16ed753":"# Final evaluation of the model\nscores = model.evaluate(test_samples, test_labels, verbose=0)\nprint(\"CNN Error: %.2f%%\" % (100-scores[1]*100))","62241e9e":"# Loading the dataset","a5856b4e":"# Summary\nIn this article I showed you how we can use Keras to develop a CNN that achieves 98.77% accuracy on the MNIST digits dataset. Since the images are rather simple (small + black&white), we expect deep learning models to have simple architectures and to perform reasonably well.\n\nKeras sits on top of Tensorflow and takes care of the low level details, leaving us to concentrate on data pre-processing and development of model architecture for our problem. \n\nHope you enjoyed this notebook. I welcome your comments & feedback.","b036567d":"The confusion matrix gives the performance of our model on a set of test data.","7fbe360a":"# Testing the data","c6b53976":"Normalize the inputs","b2c98259":"Keras is a very versatile, deep learning library that can run on-top-of several other deep learning frameworks \u2014 it supports Tensorflow, Theano and Microsoft CNTK, with Tensorflow being the default. \nKeras can be used both with a CPU as well as a GPU.\n\nFor this example, I am using Keras configured with Tensorflow on a CPU machine \u2014 for a simple model like MNIST, a CPU configuration suffices. For any serious deep learning projects, a GPU is highly recommended otherwise complex ML models will take excruciatingly long to train.","4f8e8923":"# Convolutional Neural network model","126a51e4":"In this notebook, we will develop a simple deep learning classifier using Keras library to achieve 98.77% accuracy on the MNIST digits database. Also, we will build a Convolutional Neural Network (CNN) for the classification. \n\nThe MNIST dataset is an image dataset of handwritten digits. It has has 60,000 training images and 10,000 test images, each of which are grayscale 28 x 28 sized images.\n\n","d552e4f7":"# Data Preparation"}}