{"cell_type":{"cb87cf8a":"code","b7599f98":"code","06e23182":"code","088e8767":"code","86696852":"code","830697d7":"code","a19a2b1f":"code","93f27079":"code","8074bd37":"code","c84f0353":"code","d6f91b82":"code","11bee0b8":"code","cc706859":"code","a5eb6d16":"code","e8211c71":"code","6aaf11c1":"code","f52d03d1":"code","0e5289ef":"code","51913266":"code","c34007d5":"code","13bc50da":"code","0ac86436":"code","b6a1713e":"code","7457829d":"code","82806d2b":"code","c8a246b3":"code","5728c8a8":"code","807c598e":"code","76a54edf":"code","a08c6d56":"code","1a9152b8":"code","e4451fbb":"code","b682fff4":"code","c0864b33":"code","9abb5abb":"code","f27eff47":"code","4a54481d":"code","9567832f":"code","f5d76428":"code","6d934dde":"code","148106bc":"code","5ebce32f":"code","5da4ad83":"code","a5d626e1":"code","58c2b259":"code","9099dba4":"code","4898d998":"code","02ae9484":"code","937443fa":"code","40fb0be9":"code","aa94b2a8":"code","c1c55409":"code","0c8b04d2":"code","651d5404":"code","d06a86ce":"code","675683ec":"code","3f6ade17":"code","74af76d9":"code","0ca9369f":"code","cedf8376":"code","1e14a1f3":"code","854f53af":"code","9c07f2d4":"code","92863faa":"code","e531d98b":"code","8ccc763f":"code","4bba8cf8":"code","caae405c":"code","c7a68e59":"code","25e63bb1":"code","1d37811a":"code","d4f14f5f":"code","c07284a2":"code","c5eb3e16":"code","e28094b2":"code","d51ec82d":"code","c4bbe05b":"code","60094de6":"code","4a27973b":"code","c1a93e6d":"code","051964ab":"code","d60fa1a7":"code","291167d8":"code","3dd0e974":"code","7fab4eae":"code","4faf37d9":"code","cfd09347":"code","dfd8cefe":"code","03eaef93":"code","dccc9a92":"code","6228cc15":"code","764c022f":"code","45115e00":"code","2a37eab2":"code","1b0fee16":"code","0b0fd439":"code","e48aa23f":"code","4aaaa509":"code","b44247df":"code","ed278466":"code","90af5e71":"code","8ed9e707":"code","336c484d":"code","c5db448e":"code","056a68fe":"code","f4691216":"code","409a2878":"code","35ee7edf":"code","9b90ed6e":"code","6aa386bd":"code","8956bcd9":"code","2940bd57":"code","d67ff85f":"code","57d70b28":"code","ad503f1b":"code","0b238a11":"code","2edb6da2":"code","0f0f3576":"code","37600b39":"code","5d207f5c":"code","927e867b":"code","1d402370":"code","87bf77e7":"code","db7ebc41":"code","44ddd5c4":"code","e3489b8a":"code","5a5f4e01":"code","af3ddb1d":"code","75d82876":"code","a2699e9f":"markdown","6f19b1ca":"markdown","f0bfea94":"markdown","08f7f74c":"markdown","2bd08e9b":"markdown","480da50e":"markdown","082adc78":"markdown","cb901871":"markdown","7de9e07a":"markdown","35f8151f":"markdown","19a92a93":"markdown","50ba4ec8":"markdown","08de6f12":"markdown","770426b0":"markdown","92d659ce":"markdown","485e80eb":"markdown","a82d9104":"markdown","73579940":"markdown","d050f2af":"markdown","2df97acf":"markdown","7024c259":"markdown","979ba5da":"markdown","a9bdc7b1":"markdown","ee2b725a":"markdown","bf80f393":"markdown","164f743f":"markdown","999c4593":"markdown","277f85b3":"markdown","050d13df":"markdown","586261a9":"markdown","d78a8ce3":"markdown","51633b4a":"markdown","0d0ad372":"markdown","333ee726":"markdown","e48eec73":"markdown","f97a56a7":"markdown","608f7792":"markdown","4fefa774":"markdown","10e13bdb":"markdown","75ac6bfd":"markdown","c7332adb":"markdown","1dc1a572":"markdown","008e3bfa":"markdown","692ea23b":"markdown","6ac5de9b":"markdown","f6ea5a6d":"markdown","27a3e10c":"markdown","398fd9cf":"markdown","e76dfe85":"markdown","b5744965":"markdown","94b57405":"markdown","f7e7298d":"markdown","f2451f59":"markdown","ea94759e":"markdown","2d793584":"markdown","e2c56d27":"markdown","fa657c72":"markdown","02845ef8":"markdown","bc46d7b3":"markdown","4a157549":"markdown","6315c8c3":"markdown","afe3b679":"markdown","5c6c0c6e":"markdown","e084a74c":"markdown","1bc0dd67":"markdown","82aa0994":"markdown","9484fc79":"markdown","f3f46f2c":"markdown","f8e91466":"markdown","736be815":"markdown","12a3552f":"markdown","ed0dea4f":"markdown","e9650d35":"markdown","510a17d2":"markdown","e91a298c":"markdown","9e1dca2e":"markdown","4a16a406":"markdown","b5c16c12":"markdown","663487d6":"markdown","ce8c6fd2":"markdown","909ec1b1":"markdown","1236d598":"markdown","a2a44628":"markdown","a2acf00a":"markdown","f496d4a6":"markdown","31180a5c":"markdown","a9afd7b6":"markdown","2c142e63":"markdown","0ce53f77":"markdown","91610765":"markdown","14abf748":"markdown","ac75835e":"markdown","a6ebc9cc":"markdown","deca5c00":"markdown","af101a8c":"markdown","8b739044":"markdown","fcf66a86":"markdown","9cd7b3a1":"markdown","c8e2fbd1":"markdown","16348b6e":"markdown","447de71c":"markdown","fe1bf8bd":"markdown","0bc9b04c":"markdown","cab8584e":"markdown","b5eb5280":"markdown","478ebb0e":"markdown","2f6dae2c":"markdown","470612bd":"markdown","90a3f0f6":"markdown","7251e737":"markdown","78ff10d4":"markdown","a26d4a59":"markdown","b705e717":"markdown","105f426a":"markdown","6f50d356":"markdown","5be1afda":"markdown","1d28ac66":"markdown","57dfc627":"markdown"},"source":{"cb87cf8a":"#Import libraries\n\n# data anlysis\nimport numpy as np\nimport pandas as pd\n\n# data visualization\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# data preparation for modelling\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\n# model optimization\nfrom sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, learning_curve\nfrom sklearn.feature_selection import SelectFromModel\nfrom scipy.stats import randint\nimport itertools\nfrom sklearn.metrics import confusion_matrix, roc_curve\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# Artificial Neural Network\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers import Input, Dense, Dropout, AlphaDropout\nfrom tensorflow.keras.optimizers import SGD, RMSprop, Adamax, Adagrad, Adam, Nadam, SGD\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n# Random Forest and Gradient Boosting (Appendix)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# ensure comparability of different runs\nnp.random.seed(42)","b7599f98":"#Import data\n\n# load train and test data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# merge train and test data for common data processingg\ndata = pd.concat([train, test], axis=0, ignore_index=True)\n","06e23182":"# show first 20 entries of dataset\ndata[0:20]","088e8767":"# get info about quality of the data\ndata.info()","86696852":"# check missing values\ndata.isnull().sum()","830697d7":"# statistics of nummeric features\ndata.describe()","a19a2b1f":"# Let's have a look at the correlation between Survived and all relevant numeric features in a correlation table ...\ndata.drop(columns=['PassengerId']).corr()","93f27079":"# ... or a color matrix\nmatrix = sns.heatmap(train[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(), cmap = \"Blues\", annot=True)","8074bd37":"# Number of passengers per passenger class\nplot = sns.catplot(x=\"Pclass\", kind=\"count\", data=train)","c84f0353":"# plot Pclass vs survival probability\nplot = sns.catplot(x=\"Pclass\", y=\"Survived\", kind=\"bar\", data=train).set_ylabels(\"survival probability\")","d6f91b82":"# Number of female and male passengers per passenger class\nplot = sns.catplot(x=\"Sex\", kind=\"count\", data=train, height=3.5);","11bee0b8":"# plot Sex vs survival probability\nplot = sns.catplot(x=\"Sex\", y=\"Survived\", kind=\"bar\", data=train, size=3.5).set_ylabels(\"survival probability\")","cc706859":"# Number of female and male passengers per passenger class\nplot = sns.catplot(x=\"Pclass\", hue=\"Sex\", kind=\"count\", data=train)","a5eb6d16":"plot = sns.catplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", kind=\"bar\", data=train).set_ylabels(\"survival probability\")","e8211c71":"# Explore Age histogram for NotSurvived\/Survived (0\/1)\nplot = sns.FacetGrid(train, col='Survived', height=4).map(sns.distplot, \"Age\", kde=False)\n","6aaf11c1":"# Explore the respective Age distibutions \nPlot = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 0) & (train[\"Age\"].notnull())], color=\"Red\", shade = True)\nplot = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 1) & (train[\"Age\"].notnull())], color=\"Green\", shade= True)\nplot.set_xlabel(\"Age\")\nplot = plot.legend([\"Not Survived\",\"Survived\"])","f52d03d1":"# We split the feature Age in two categories, child and adult\ntrain[\"AgeCat\"]= pd.cut(train[\"Age\"], bins=[0,15,max(train[\"Age\"]+1)], labels=['child','adult'])","0e5289ef":"# plot survival probability of cildren and adults\nplot = sns.catplot(x=\"AgeCat\", y=\"Survived\", kind=\"bar\", data=train, size=3.5).set_ylabels(\"survival probability\")","51913266":"# plot survival probability of cildren and adults regarding the different classes\nplot = sns.catplot(x=\"AgeCat\", y=\"Survived\", hue=\"Pclass\", kind=\"bar\", data=train).set_ylabels(\"survival probability\")","c34007d5":"# Number of passengers with no or several Siblings\/Spouses\nplot = sns.catplot(x=\"SibSp\",kind=\"count\", data=train, height=4.7, aspect=2.45)","13bc50da":"# Survival probability of passengers with no or several Siblings\/Spouses\nplot = sns.catplot(x=\"SibSp\", y=\"Survived\", kind=\"bar\", data=train, height=4, aspect=3).set_ylabels(\"survival probability\")","0ac86436":"# Number of passengers with no or several Siblings\/Spouses\nplot = sns.catplot(x=\"SibSp\", hue='AgeCat',kind=\"count\", data=train, height=4, aspect=3)","b6a1713e":"# Survival probability of passengers with no or several Siblings\/Spouses\nplot = sns.catplot(x=\"SibSp\", y=\"Survived\", hue=\"AgeCat\", kind=\"bar\", data=train, height=4, aspect=3).set_ylabels(\"survival probability\")","7457829d":"# Number of children with no or several siblings per class (assuming children younger than 15 are not married)\nplot = sns.catplot(x=\"SibSp\", hue='Pclass', kind=\"count\", data=train[train['AgeCat']=='child'], height=4, aspect=2.15)","82806d2b":"# Number of passengers with no or several parents\/children\nplot = sns.catplot(x=\"Parch\",kind=\"count\", data=train, height=4.7, aspect=2.55)","c8a246b3":"# Survival chance of passengers with no or several parents\/children\nplot = sns.catplot(x=\"Parch\", y='Survived',kind=\"bar\", data=train, height=4.7, aspect=2.55)","5728c8a8":"# Number of children\/adults with no or several parents\/children\nplot = sns.catplot(x=\"Parch\", hue='AgeCat',kind=\"count\", data=train, height=4, aspect=3)","807c598e":"# Survival chance of children\/adults with no or several parents\/children\nplot = sns.catplot(x=\"Parch\", y='Survived', hue='AgeCat',kind=\"bar\", data=train, height=4.1, aspect=3)","76a54edf":"# Explore Fare distibution (you can change the different parameters below \n#to better understand the Fare distribution)\nkde=False\nupper=100  # upper limit for data in histogram\nlower=0    # lower imit for data in histogram\nPclass=3   # type 0 for all classes\n\n# perserve bin size for different parameters\nbins=int((upper-lower)\/5) \nif Pclass == 1 or Pclass == 2 or Pclass == 3:\n    upPclass = Pclass+1\n    lowPclass = Pclass\nelse: \n    upPclass = 4\n    lowPclass = 1\n    \nplot = sns.distplot(train[\"Fare\"][(train[\"Survived\"] == 0) & (train[\"Fare\"]< upper) & (train[\"Fare\"]> lower) & (train[\"Pclass\"]<upPclass) & (train[\"Pclass\"]>=lowPclass)], bins=bins, color=\"Red\", kde=kde)\nplot = sns.distplot(train[\"Fare\"][(train[\"Survived\"] == 1) & (train[\"Fare\"]< upper) & (train[\"Fare\"]> lower) & (train[\"Pclass\"]<upPclass) & (train[\"Pclass\"]>=lowPclass)], bins=bins, color=\"Green\", kde=kde)\nplot.set_xlabel(\"Fare\")\nplot = plot.legend([\"Not Survived\",\"Survived\"])","a08c6d56":"train[['Fare','Name','Ticket','SibSp','Parch']].iloc[train.index[(train['Pclass']==3) & (train['Fare']>60)]]","1a9152b8":"# the code below finds person groups which share the same tickets and \n# devides the ticket fare by the number of people in the group\ntrain['FareCorr'] = train['Fare'].copy()\n\nm=0 # m=0 -> print first iteration as example how the code works\nfor grp, grp_df in train[['Ticket', 'Name', 'Pclass', 'Fare', 'PassengerId']].groupby(['Ticket']):\n\n    if (len(grp_df) != 1):\n        if m==0:\n            print(grp_df)\n            m=1\n        for ind, row in grp_df.iterrows():\n            passID = row['PassengerId']\n            train.loc[train['PassengerId'] == passID, 'FareCorr'] = train['Fare'][train['PassengerId'] == passID]\/len(grp_df)\n","e4451fbb":"# Explore Fare distibution (you can change the different parameters below \n#to better understand the Fare distribution)\nkde=False\nupper=100  # upper limit for data in histogram\nlower=0    # lower imit for data in histogram\nPclass=0   # type 0 for all classes\n\n# perserve bin size for different parameters\nbins=int((upper-lower)\/5) \n\nif Pclass == 1 or Pclass == 2 or Pclass == 3:\n    upPclass = Pclass+1\n    lowPclass = Pclass\nelse: \n    upPclass = 4\n    lowPclass = 1\n    \nplot = sns.distplot(train[\"FareCorr\"][(train[\"Survived\"] == 0) & (train[\"FareCorr\"]< upper) & (train[\"FareCorr\"]> lower) & (train[\"Pclass\"]<upPclass) & (train[\"Pclass\"]>=lowPclass)], bins=bins, color=\"Red\", kde=kde)\nplot = sns.distplot(train[\"FareCorr\"][(train[\"Survived\"] == 1) & (train[\"FareCorr\"]< upper) & (train[\"FareCorr\"]> lower) & (train[\"Pclass\"]<upPclass) & (train[\"Pclass\"]>=lowPclass)], bins=bins, color=\"Green\", kde=kde)\nplot.set_xlabel(\"Fare\")\nplot = plot.legend([\"Not Survived\",\"Survived\"])","b682fff4":"train['FareCat']=pd.cut(train['FareCorr'], bins=[0,15,65,max(train[\"FareCorr\"]+1)], labels=['low','mid','high'])\ntrain['FareCat'].value_counts()","c0864b33":"plot = sns.catplot(x=\"FareCat\", y='Survived',kind=\"bar\", data=train, height=5, aspect=1)\n","9abb5abb":"# Number of passengers that embarked a the three different habours\nplot = sns.catplot(x=\"Embarked\",kind=\"count\", data=train, height=4, aspect=1)","f27eff47":"# Survival Probability of passengers emarked a the three different habours\nplot = sns.catplot(x=\"Embarked\", y='Survived', kind=\"bar\", data=train, height=5, aspect=1)","4a54481d":"# Number of passengers emarked a the three different habours per class\nplot = sns.catplot(x=\"Embarked\", hue='Pclass',kind=\"count\", data=train, height=5, aspect=1)","9567832f":"# Survival Probability of passengers emarked a the three different habours per class\nplot = sns.catplot(x=\"Embarked\", y='Survived', hue='Pclass', kind=\"bar\", data=train, height=5, aspect=1)","f5d76428":"data['Name'][240:280]","6d934dde":"# extracting the Title (which always ends with a \".\")\nfor name_string in data['Name']:\n    data['Title'] = data['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n\n#transform titles which are synonyms\ndata['Title']=data['Title'].replace({'Ms':'Miss','Mlle':'Miss','Mme':'Mrs'})\n\ndata['Title'].value_counts()","148106bc":"# Noble passengers\ndata['Title']=data['Title'].replace(['Sir','Don','Dona','Jonkheer','Lady','Countess'], 'Noble')\n\n# passengers with a higher social standing\ndata['Title']=data['Title'].replace(['Dr', 'Rev','Col','Major','Capt'], 'Others')\n\ndata['Title'].value_counts()","5ebce32f":"data.groupby('Title')['Age'].median()","5da4ad83":"data['Parch'][(data['Age']<15) & (data['Title']=='Miss')].value_counts()","a5d626e1":"#Misses older than 15\ndata['Parch'][(data['Age']>=15) & (data['Age']<25) & (data['Title']=='Miss')].value_counts()","58c2b259":"print('Average age of young Miss(age<15): '+ str(data['Age'][(data['Title']== 'Miss') & (data['Age']<15)].median()) + ' years')\nprint('Average age of Miss (age>=15): '+ str(data['Age'][(data['Title']== 'Miss') & (data['Age']>=15)].median()) + ' years')","9099dba4":"title_list=data.groupby('Title')['Age'].median().index.to_list()\n# Filling missing age values \nfor title in title_list:\n    if title=='Miss':\n        # Miss with Parch = 0 and missing age value: set median age >15 to missing values\n        data.loc[(data['Age'].isnull()) & (data['Title'] == title) & (data['Parch'] == 0), 'Age'] \\\n        = data['Age'][(data['Title']== title) & (data['Age']>=15)].median()\n        \n        # Miss with Parch > 0 and missing age value: set median age <15 to missing values\n        data.loc[(data['Age'].isnull()) & (data['Title'] == title) & (data['Parch'] > 0), 'Age'] \\\n        = data['Age'][(data['Title']== title) & (data['Age']<15)].median()\n        \n        # all Misses with Age < 15: set Title to youngMiss \n        data.loc[(data['Age']<15) & (data['Title'] == title), 'Title'] \\\n        = 'youngMiss'    \n                                                 \n    else: \n        data.loc[(data['Age'].isnull()) & (data['Title'] == title), 'Age'] \\\n        = data['Age'][(data['Title']== title)].median()","4898d998":"#check the average age for different titles\ndata.groupby('Title')['Age'].median()","02ae9484":"#check number of titles\ndata['Title'].value_counts()","937443fa":"# We split the feature Age in five categories, where category 1 refers to children (age<15)\ndata[\"AgeCat\"]= pd.cut(data[\"Age\"], bins=[0,14.9,30,45,60,max(data[\"Age\"]+1)], labels=['1','2','3','4','5'])\n\n# size of each category\ndata['AgeCat'].value_counts()","40fb0be9":"plot = sns.catplot(x='AgeCat', y='Survived', kind='bar', data=data[:len(train)], height=4)","aa94b2a8":"# create feature FamilySize\ndata.loc[data['SibSp'] + data['Parch'] + 1 == 1, 'FamilySize'] = 'Single'\ndata.loc[data['SibSp'] + data['Parch'] + 1 > 1 , 'FamilySize'] = 'Small'\ndata.loc[data['SibSp'] + data['Parch'] + 1 > 4 , 'FamilySize'] = 'Big'","c1c55409":"# size of each category\ndata.FamilySize.value_counts()","0c8b04d2":"plot = sns.catplot(x='FamilySize', y='Survived', kind='bar', data=data[:len(train)])","651d5404":"# extract last names\n#data['Last_Name'] = data['Name'].apply(lambda x: str.split(x, \",\")[0])\ndata['Last_Name'] =  data['Name'].str.extract('([A-Za-z]+),', expand=True)\ndata['Last_Name'].value_counts()\n#data.groupby('Last_Name')","d06a86ce":"data['Last_Name'] = data['Name'].apply(lambda x: str.split(x, \",\")[0])\ndefault_value = 0.5\n\ndata['FamilySurvival'] = default_value\nfor grp, grp_df in data[['Survived', 'Last_Name', 'Ticket', 'PassengerId']].groupby(['Last_Name']):\n    \n    if (len(grp_df) != 1):\n        # A Family group is found.\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                data.loc[data['PassengerId'] == passID, 'FamilySurvival'] = 1\n            elif (smin == 0.0):\n                data.loc[data['PassengerId'] == passID, 'FamilySurvival'] = 0\n                \nfor _, grp_df in data.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        #print(grp_df)\n        for ind, row in grp_df.iterrows():\n            if (row['FamilySurvival'] == 0) | (row['FamilySurvival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    data.loc[data['PassengerId'] == passID, 'FamilySurvival'] = 1\n                elif (smin == 0.0):\n                    data.loc[data['PassengerId'] == passID, 'FamilySurvival'] = 0\n\n# check number of passengers in the different categories\ndata['FamilySurvival'].value_counts()    ","675683ec":"# Corret Fare per person\ndata['FareCorr'] = data['Fare'].copy()\n\nfor grp, grp_df in data[['Ticket','Name', 'Pclass', 'Fare', 'PassengerId']].groupby(['Ticket']):\n\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            passID = row['PassengerId']\n            data.loc[data['PassengerId'] == passID, 'FareCorr'] = data['Fare'][data['PassengerId'] == passID]\/len(grp_df)","3f6ade17":"# Replacing one missing Fare value\nfa = data[data[\"Pclass\"] == 3]\ndata['FareCorr'].fillna(fa['FareCorr'].median(), inplace = True)\ndata.info()","74af76d9":"data['FareCat']=pd.qcut(data['FareCorr'], 7, labels=['1','2','3','4','5','6','7'])\ndata['FareCat'].value_counts()","0ca9369f":"#Survival probability for each fare category\nplot = sns.catplot(x=\"FareCat\", y='Survived',kind=\"bar\", data=data, height=5, aspect=1)\n","cedf8376":"data[data['Embarked'].isnull()]","1e14a1f3":"data['Embarked'].fillna('C', inplace = True)","854f53af":"data.info()","9c07f2d4":"data_pre = data.drop(['Embarked','Survived','PassengerId','Name','Age','Parch','SibSp','Ticket','Fare','Cabin','Last_Name','FareCorr'],axis=1)","92863faa":"#check data\ndata_pre","e531d98b":"# check data\ndata_pre.info()","8ccc763f":"# create name lists for numerical an categorical features: we do not use any numerical features\nnum_attribs = []\ncat_attribs = list( data_pre.drop(labels=num_attribs, axis=1).columns)","4bba8cf8":"#Pipeline for numerical values\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('std_scaler', StandardScaler()),\n    ])","caae405c":"#Pipeline for categorical values\ncat_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n        ('cat', OneHotEncoder()),\n    ])","c7a68e59":"#Full pipeline\n\nfull_pipeline = ColumnTransformer([\n        #(\"num\", num_pipeline, num_attribs), # since we do not use any numerical features\n        ('cat', cat_pipeline, cat_attribs),\n    ])\n\ndata_post = full_pipeline.fit_transform(data_pre)","25e63bb1":"oneHot=OneHotEncoder()\ndata_post_alt=oneHot.fit_transform(data_pre[cat_attribs])","1d37811a":"# extract feature names after encoding\nfeature_names = oneHot.get_feature_names(data_pre.columns)\nfeature_names","d4f14f5f":"# training data\nX_train = data_post[:len(train['Survived']),:].toarray()\ny_train = train['Survived'].copy().to_numpy()   # traget\n\n# test data\nX_test = data_post[len(train['Survived']):,:].toarray()","c07284a2":"# check shape of X_train - number of instances and features\nprint('# instances: '+ str(X_train.shape[0]))\nprint('# features: '+ str(X_train.shape[1]))","c5eb3e16":"# define functions\ndef hyperparameter_analysis(searcher, top_values=5):\n    tested_hyperparameters=pd.DataFrame()\n    for i in range(len(searcher.cv_results_['params'])):\n        tested_hyperparameters = tested_hyperparameters.append(searcher.cv_results_['params'][i], ignore_index=True)\n    tested_hyperparameters['train score in %']=(searcher.cv_results_['mean_train_score']*100).round(3)\n    tested_hyperparameters['test score in %']=(searcher.cv_results_['mean_test_score']*100).round(3)\n    tested_hyperparameters['test std in %']=(searcher.cv_results_['std_test_score']*100).round(3)\n    tested_hyperparameters.index=searcher.cv_results_['rank_test_score']\n    return(tested_hyperparameters.sort_index().head(top_values))\n\ndef feature_importance(searcher_best_estimator, list_feature_names, top_values=5):\n    # calculate normed feature_importances\n    fi_norm = 1\/abs(searcher_best_estimator.feature_importances_).sum()*abs(searcher_best_estimator.feature_importances_)\n    # create DataFrame \n    fi_df= pd.DataFrame(data=(fi_norm*100).round(2), index=list_feature_names, columns=['feature importance in %'])\n    return(fi_df.sort_values(['feature importance in %'], ascending=[False]).head(top_values))\n\ndef feature_importance_ann(perm, list_feature_names, top_values=5, neg_values=False):    \n    if neg_values==True:\n        fi_df= pd.DataFrame(data=(perm.feature_importances_*100).round(2), index=list_feature_names, columns=['score reduction in % points'])\n        return(fi_df[fi_df['score reduction in % points']<0]).sort_values(['score reduction in % points'], ascending=[True]).head(top_values)\n    else:    \n        fi_df= pd.DataFrame(data=(perm.feature_importances_*100).round(2), index=list_feature_names, columns=['score reduction in % points'])\n        return(fi_df.sort_values(['score reduction in % points'], ascending=[False]).head(top_values))\n\ndef metric_scores(y_reference, y_prediction):\n    precision = precision_score(y_reference, y_prediction)\n    recall = recall_score(y_reference, y_prediction)\n    F1_score = f1_score(y_reference, y_prediction)\n    print('precision: '+ str(precision.round(3)))\n    print('recall: '+ str(recall.round(3)))\n    print('F1_score: '+ str(F1_score.round(3)))\n    return(precision, recall, F1_score)\n\ndef plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n\ndef pred_to_df(y_predicted):\n    survived = pd.DataFrame(y_predicted,index=test.index, columns=['Survived'])\n    survived['PassengerId'] = test.PassengerId \n    columns_titles = [\"PassengerId\",\"Survived\"]\n    survived=survived.reindex(columns=columns_titles)\n    return(survived)\n\ndef save_to_csv(survived, filename,index=False):\n    survived.to_csv(filename + '.csv',index=False, header=survived.columns)\n\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    ","e28094b2":"def create_model(input_shape=X_train.shape[1:],\n                number_hidden=2, \n                neurons_per_hidden=10,\n                hidden_drop_rate= 0.2,\n                hidden_activation = 'selu',\n                hidden_initializer=\"lecun_normal\",\n                output_activation ='sigmoid',\n                loss='binary_crossentropy',\n                optimizer = Nadam(lr=0.0005),\n                #lr=0.0005,\n                ):\n    \n    #create model\n    model = Sequential()\n    model.add(Input(shape=input_shape)),\n    for layer in range(number_hidden):\n        model.add(Dense(neurons_per_hidden, activation = hidden_activation ,kernel_initializer=hidden_initializer))\n        #model.add(Dropout(hidden_drop_rate))\n    model.add(Dense(1, activation = output_activation))\n\n    # Compile model\n    model.compile(loss=loss, \n                  #optimizer = Nadam(lr=lr), \n                  optimizer = Nadam(lr=0.0005),\n                  metrics = ['accuracy'])\n    return model","d51ec82d":"keras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# build and compile ANN classifier by calling the model function\ndnn_clf=create_model()\n\n# train the ANN classifier\nhistory = dnn_clf.fit(X_train, y_train, epochs=30, batch_size=30, verbose=0)","c4bbe05b":"# show the specifications of the model\ndnn_clf.summary()","60094de6":"# show a plot of the model architecture\nplot_model(dnn_clf, 'optimized_model.png', show_shapes=True)","4a27973b":"# score of Ann classifier on training data\nprint('Training score: ' + str((pd.DataFrame(history.history)['accuracy'].max()*100)) + '%')","c1a93e6d":"# calculate predictions for training dataset X_train\ny_train_pred_dnn = dnn_clf.predict_classes(X_train)","051964ab":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_train, y_train_pred_dnn)\n\n#plot confusion matrix\ncm_plot=plot_confusion_matrix(cnf_matrix, ['died','survived'])","d60fa1a7":"# show metrics\nmetric_scores(y_train, y_train_pred_dnn)","291167d8":"# calculate probabilities for training dataset X_train\ny_train_proba_dnn = dnn_clf.predict(X_train)","3dd0e974":"proba_df=pd.DataFrame(y_train_proba_dnn, columns=['probabilities'])*100\nproba_df['false_predictions']=(pd.DataFrame(y_train_pred_dnn)-pd.DataFrame(y_train))[0]\nproba_df['probabilities'][proba_df['false_predictions']==0].hist()","7fab4eae":"proba_df['probabilities'][proba_df['false_predictions']!=0].hist()","4faf37d9":"check_df=data[:891].copy()\ncheck_df['false_predictions']=proba_df['false_predictions']\ncheck_df['probabilities']=proba_df['probabilities']\n#check_df[check_df['false_predictions']!=0].head()\ncheck_df[check_df['PassengerId']==18]","cfd09347":"np.random.seed(42)\ntf.random.set_seed(42)\n\n# build and train KerasClassifier\ndnn_clf_fi=KerasClassifier(build_fn = create_model)\ndnn_clf_fi.fit(X_train, y_train, epochs=30, batch_size=30)\n\n# calculate feature importance\nperm = PermutationImportance(dnn_clf_fi, random_state=42).fit(X_train,y_train)\n","dfd8cefe":"feature_importance_ann(perm, feature_names, top_values=10, neg_values=False)","03eaef93":"feature_importance_ann(perm, feature_names, top_values=20, neg_values=True)","dccc9a92":"# select k best features\nk = 17\nsel = SelectFromModel(perm, threshold=-np.inf, max_features=k, prefit=True)\n#get index of best features\nbest_festures_index=np.where(sel.get_support()==True)\n\n# create training data with k selected features\nX_train_bf = data_post[:len(train['Survived']),best_festures_index[0]].copy()\n# create test data with k selected features\nX_test_bf = data_post[len(train['Survived']):,best_festures_index[0]].copy()","6228cc15":"# make predictions for X_test\ny_pred_dnn = dnn_clf.predict_classes(X_test)","764c022f":"#convert to DataFrame and check\nmy_submission=pred_to_df(y_pred_dnn)\nmy_submission.head()","45115e00":"# save to csv file\nsave_to_csv(my_submission,'submission')","2a37eab2":"#If you want to optimize with the \"best features\" dataset X_train_bf, uncomment below.\n\n# training data: default is X_train as used before \nX_train_gs=X_train.copy()\n#X_train_gs=X_train_bf.copy() #best feature dataset\n\n# test data: default is X_train as used before\nX_test_gs=X_test.copy()\n#X_test_gs=X_test_bf.copy()  #best feature dataset","1b0fee16":"# check shape of X_train - number of instances and features\nprint('# instances: '+ str(X_train_gs.shape[0]))\nprint('# features: '+ str(X_train_gs.shape[1]))","0b0fd439":"keras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# build classifier\ndnn_clf_gs = KerasClassifier(build_fn = create_model, verbose = 0)\n\n# define parameter grid: \n# uncomment the parameters you want to optimize\nparam_grid = {\n    #\"optimizer\": ['Adam', 'Nadam', 'Adagrad', 'Adamax'],\n    #\"lr\":[0.005,0.001,0.0005]\n    #\"epochs\": [15, 30, 45, 60],\n    #\"batch_size\": [20,30,40],\n    #\"number_hidden\": [1, 2, 3],\n    #\"neurons_per_hidden\": [5, 10, 15],\n    'input_shape': X_train_gs.shape[1:]  # keep this line\n}\n\n# build GridSearchCV model with ANN classifier\ngrid_search_dnn = GridSearchCV(dnn_clf_gs, param_grid, cv=5, n_jobs=-1, verbose=0, return_train_score=True)\n\n# fit GridSearchCV model\ntraining_gs = grid_search_dnn.fit(X_train_gs, y_train,\n                                 epochs = 30, \n                                 batch_size = 30,\n                                 validation_split=0.2, shuffle=True,\n                                 #callbacks=[keras.callbacks.EarlyStopping(patience=8)]\n                                )\n                                    ","e48aa23f":"# mean scores for training and validation model after cross-validation\nprint('Mean training score:   ' + str((grid_search_dnn.cv_results_['mean_train_score'][grid_search_dnn.best_index_]*100).round(2)) +'% (' + str((grid_search_dnn.cv_results_['std_train_score'][grid_search_dnn.best_index_]*100).round(2)) + '%)')\nprint('Mean validation score: ' + str((grid_search_dnn.cv_results_['mean_test_score'][grid_search_dnn.best_index_]*100).round(2)) +'% (' + str((grid_search_dnn.cv_results_['std_test_score'][grid_search_dnn.best_index_]*100).round(2)) + '%)')","4aaaa509":"# summarize history for accuracy\nplt.plot(grid_search_dnn.best_estimator_.model.history.history['accuracy'], color='red')\nplt.plot(grid_search_dnn.best_estimator_.model.history.history['val_accuracy'], color='green')\nplt.plot(grid_search_dnn.best_estimator_.model.history.history['loss'], color='red')\nplt.plot(grid_search_dnn.best_estimator_.model.history.history['val_loss'], color='green')\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epochs')\nplt.legend(['train accuracy\/loss', 'validation accuracy\/loss',], loc='best')\nplt.show()","b44247df":"# set of optimized hyperparameters -> best model\npd.DataFrame(grid_search_dnn.best_params_.items(), columns=['hyperparameter','value'])","ed278466":"# top 10 hyperparameter sets\nhyperparameter_analysis(grid_search_dnn, 10)","90af5e71":"# make predictions for X_test\ny_pred_dnn_opti = grid_search_dnn.best_estimator_.predict(X_test_gs)","8ed9e707":"# convert to DataFrame and check\nsurvived_dnn_opti=pred_to_df(y_pred_dnn_opti)\nsurvived_dnn_opti.head()","336c484d":"# save prediction obtained by GridSearch\nsave_to_csv(survived_dnn_opti,'survived_dnn_opti')","c5db448e":"# Find optimum hyperparameters with RandomSearch\n\nparam_distribs = {\n        'n_estimators': randint(low=50, high=150),\n        'max_features': randint(low=5, high=15),\n        'min_samples_split': randint(low=10, high=30),\n    }\n\nforest_clf = RandomForestClassifier(random_state=42)\nrnd_search_rf = RandomizedSearchCV(forest_clf, param_distributions=param_distribs, n_jobs=-1,\n                                n_iter=50, cv=5, scoring=\"accuracy\", random_state=42, return_train_score=True)\nrnd_search_rf.fit(X_train, y_train)","056a68fe":"# score of best model after cross-validation\nprint('Best score: ' + str((rnd_search_rf.best_score_*100).round(2)) + '%')","f4691216":"# mean scores for training and validation model after cross-validation\nprint('Mean training score:   ' + str((rnd_search_rf.cv_results_['mean_train_score'][rnd_search_rf.best_index_]*100).round(2)) +'% (' + str((rnd_search_rf.cv_results_['std_train_score'][0]*100).round(2)) + '%)')\nprint('Mean validation score: ' + str((rnd_search_rf.cv_results_['mean_test_score'][rnd_search_rf.best_index_]*100).round(2)) +'% (' + str((rnd_search_rf.cv_results_['std_test_score'][0]*100).round(2)) + '%)')","409a2878":"# set of optimized hyperparameters -> best model\npd.DataFrame(rnd_search_rf.best_params_.items(), columns=['hyperparameter','value'])","35ee7edf":"# top 5 hyperparameter sets\nhyperparameter_analysis(rnd_search_rf)","9b90ed6e":"feature_importance(rnd_search_rf.best_estimator_, feature_names, 10)","6aa386bd":"# set best estimator as final model\nfinal_rf_clf = rnd_search_rf.best_estimator_\nfinal_rf_clf","8956bcd9":"y_train_pred_rf = final_rf_clf.predict(X_train)","2940bd57":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_train, y_train_pred_rf)\n\n#plot confusion matrix\ncf=plot_confusion_matrix(cnf_matrix, ['died','survived'])","d67ff85f":"# show metrics\nmetric_scores(y_train, y_train_pred_rf)","57d70b28":"y_pred_rf = final_rf_clf.predict(X_test)","ad503f1b":"survived_rf=pred_to_df(y_pred_rf)\nsurvived_rf.head()","0b238a11":"save_to_csv(survived_rf,'survived_rf')","2edb6da2":"param_distribs = {\n        #'learning_rate': randint(low=0.1, high=0.1),\n        'max_depth': randint(low=1, high=5),\n        'n_estimators': randint(low=5, high=120),\n        'max_features': randint(low=5, high=15),\n    }\n\ngb_clf = GradientBoostingClassifier()\nrnd_search_gb = RandomizedSearchCV(gb_clf, param_distributions=param_distribs,\n                                n_iter=40, cv=5, scoring=\"accuracy\", random_state=42, return_train_score=True)\nrnd_search_gb.fit(X_train, y_train)","0f0f3576":"# mean scores for training and validation model after cross-validation\nprint('Mean training score:   ' + str((rnd_search_gb.cv_results_['mean_train_score'][rnd_search_gb.best_index_]*100).round(2)) +'% (' + str((rnd_search_gb.cv_results_['std_train_score'][0]*100).round(2)) + '%)')\nprint('Mean validation score: ' + str((rnd_search_gb.cv_results_['mean_test_score'][rnd_search_gb.best_index_]*100).round(2)) +'% (' + str((rnd_search_gb.cv_results_['std_test_score'][0]*100).round(2)) + '%)')","37600b39":"rnd_search_gb.cv_results_['mean_train_score']","5d207f5c":"# set of optimized hyperparameters -> best model\npd.DataFrame(rnd_search_gb.best_params_.items(), columns=['hyperparameter','value'])","927e867b":"# top 5 hyperparameter sets\nhyperparameter_analysis(rnd_search_gb, 5)","1d402370":"# top 10 important features\nfeature_importance(rnd_search_gb.best_estimator_, feature_names, 10)","87bf77e7":"# set best estimator as final model\nfinal_gb_clf = rnd_search_gb.best_estimator_\nfinal_gb_clf","db7ebc41":"y_train_pred_gb = final_gb_clf.predict(X_train)","44ddd5c4":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_train, y_train_pred_gb)\n\n#plot confusion matrix\nplot_confusion_matrix(cnf_matrix, ['died','survived'])","e3489b8a":"# show metrics\nmetric_scores(y_train, y_train_pred_gb)","5a5f4e01":"y_pred_gb = final_gb_clf.predict(X_test)","af3ddb1d":"survived_gb=pred_to_df(y_pred_gb)\nsurvived_gb.head()","75d82876":"save_to_csv(survived_gb,'survived_gb')","a2699e9f":"Now we can evaluate our trained classifier on the training dataset. ","6f19b1ca":"### 7.1 The model <a id=\"Model1\"><\/a>","f0bfea94":"The validation score evulated by cross-validation is quite close to the 83.3% score obtained for the test dataset. \nAlso the training and validation score are rather similar which suggests that there is no\/little overfitting. Hence, the model seems to be optimzed quite well.\n\nTo see when overfitting sets in one can plot the training versus validation score\/loss for several epochs (aka learning curve) as shown below. Note that in order to plot this curve you need to have validation data (stratified at best). However, since the Titanic data set is rather small you should train your final model using all the data (no validation split). So only use this during the optimization phase.  ","08f7f74c":"Only bout 12% of the Misses younger than 15 years have no parents\/children on board. whereas between 15 and 20 ...","2bd08e9b":"Note: We have set the age limit to still be a child to 15. So it might be that there are young adults (older than 15) which have parents on bord and children (15 or younger) that are already a parent (not as unusual as today at the beginning of the 20th century).","480da50e":"### GB: Predict training data for further evaluation","082adc78":"So let's build a ANN classifier and train it:","cb901871":"In this notebook I will not further analyze these two features. As mentioned above discussing the feature *Fare*, the feature *Ticket* is very valuble to find groups that share the same ticket to correct the fare prices and implement the new *FareCorr*. Apart from that some tikets habe a letter which refers to the deck\/level. Analyzing this together with the feature *Cabin*, where most of the entries are missing (or where the passengers had no cabin?!?), one could construct a new feature containing information about the survival probability regarding the deck\/level. For more info have a look at the [wiki entry](https:\/\/en.wikipedia.org\/wiki\/RMS_Titanic).","7de9e07a":"In general, it should be noted that the survival probability for women in general is rather high, so we have to see later on whether introducing the title \"young Miss\" improves our prediction.","35f8151f":"The titanic had three different stops before it went on to cross the Atlantic - Southhampton (England), Cherbourg (France), and Queenstown (Ireland).","19a92a93":"### 4.7 Embarked <a id=\"FAna7\"><\/a>","50ba4ec8":"Below you can see the top 10 features:","08de6f12":"Most passengers embarked in Southampton - one half 3rd class, the other half 1st and 2nd class passengers. The majority of the passengers which embarked in Cherbourg traveled in the 1st class, which explains the higher survival probability of this passenger group. It is remarkable that in Queenstwon mainly 3rd class passengers embarked, however, their survival probability is higher than for the passengers which embarked in Southamton. This might be relevant information.","770426b0":"Now let's go back to the histograms and check how the distributions look like for *FareCorr*:","92d659ce":"### RF: Modelling","485e80eb":"With this ANN classifier you should obtain 83.3% on the test dataset (kaggle). With the current set of features, the ANN classifier performs much better than strong ensemble methods like *Random Forests* and *Gradient Boosting* which only achieved about 80.8% on the test data. An analysis of both classifiers can be found in appedix A and B of this notebook.\n\nI experimented with lot of different approaches for feature analysis, feature engineering, data preparation and for the final model (hence, the large number of submissions) and tried to summarized the most important aspects in this notebook.\n\nWe learned that feature engineering and the implementation of new features based on a carefull analysis of the existing features is indispensable for building a powerfull classifier. Especially since ANNs seem to be quite sensitive for variations in the feature set.\n\nMoreover, we saw that GridSearchCV can be very usefull tool to optimize the hyperparameters. It should be noted that for a rather small dataset, as it is the case for the *Titanic Desaster*, it is not advisable to create a separate validation data set from the training data (tried it, model does not generalize well anymore), so using cross-validation is essential.\n\nIn general, with a good optimization, ANNs seem to perform very well on the titanic datasets. \n\nI hope you enjoyed this notebook and, of course, comments and feedback are highly appreciated. <p style=\"color:red\"><b>If you liked this notebook, please leave an upvote!!\n\n<br>\n<br>\n<br>\n\nHere are a few other good reads on basics of ML and ANNs\n\n* [Machine Learning Tutorial for Beginners](https:\/\/www.kaggle.com\/kanncaa1\/machine-learning-tutorial-for-beginners) by [kanncaa1](https:\/\/www.kaggle.com\/kanncaa1)\n* [Deep Learning Tutorial for Beginners](https:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners) by [kanncaa1](https:\/\/www.kaggle.com\/kanncaa1)\n\nANNs applied to the Titanic data set\n* [Titanic EDA + Model Pipeline + Keras NN](https:\/\/www.kaggle.com\/kabure\/titanic-eda-model-pipeline-keras-nn#6.-Modelling-:-) by [kabure](https:\/\/www.kaggle.com\/kabure) \n* [Complete Titanic tutorial with ML, NN & Ensembling](https:\/\/www.kaggle.com\/nhlr21\/complete-titanic-tutorial-with-ml-nn-ensembling) by [nhlr21](https:\/\/www.kaggle.com\/nhlr21) \n* [Titanic - Neural Networks (KERAS) - 81.8%](https:\/\/www.kaggle.com\/vincentlugat\/titanic-neural-networks-keras-81-8) by [vincentlugat](https:\/\/www.kaggle.com\/vincentlugat)\n    \nand ANNs applied to an actual real world problem ([M5 Competition](https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy))\n* [Neural networks for the prediction of product sales](https:\/\/www.kaggle.com\/dantefilu\/nn-on-store-level-with-3-fold-cv-store-wrmsse)\n\n<br>\n<br>\n<br>","a82d9104":"The survival probability for a man from the first class is twice as high than for a man from the second or third class. Regarding women, about 90 % of the women from the first and the second class survive, in contrast to only about 50% of the women in the third class.","73579940":"The graph shows that the model converges well (might be still some room for improvement) and that there is no overfitting (would e.g. be indicated by divergence of training and validation loss after validation loss had a minimum). An explanation why the validation score\/loss is higher\/lower than the on of the training is explained [here](https:\/\/keras.io\/getting-started\/faq\/#why-is-the-training-loss-much-higher-than-the-testing-loss).\n\nNow let's check which were the top parameter sets:","d050f2af":"### 4.6 Fare <a id=\"FAna6\"><\/a>","2df97acf":"The best strategy to fill the missing values might be to use the titles to guess the age, since we are goint to divide *Age* in two categories, namely <15 and >15 years.   ","7024c259":"Note: in the end I decided to not use this feature.","979ba5da":"### 7.2.1 Evaluation of trained model","a9bdc7b1":"[back to Table of Contents](#TOC)\n## 2. Import libraries and data <a id=\"Import\"><\/a> <br>","ee2b725a":"## A: Random Forests <a id=\"RF\"><\/a>","bf80f393":"The correlation matrix we analyzed above showed that *Fare* has a strong correlation with the chance to survive. Let's explore the distribution of fares and how it correlates with survival.","164f743f":"Regarding the **false** predictions, there is a majority around 0.5. Hence, cases where the information on the passenger was not conclusive enough to make a desicive prediction. By further feature engeneering or by improving our model, it might be possible to predict some of this cases around 0.5 right.\n\nHowever, there is also a high number of instances with probabilities close to 0, where the classifier obviously was convinced that the passenger died. \nA good example for such a case is the passenger with the ID 18 (see below) that was predicted to not survive. A young male passenger traveling in the second class whose other family mambers died - a clear case for our classifier - however, this man survived. Well, even the best classifier can not cover such cases if there is no additional information that would have indicated a survival of this passenger.","999c4593":"We have 11 features (not counting *Survived*, which is our target) of different types (int64, float64, object), where for some features (*Age, Fare, Cabin*) data points are missing.","277f85b3":"A negative value means that our model might work better if we do not consider the respective feature.\n\nUsing SelectFromModel one can create a new training dataset containing only the most important features. The code below shows how to select the top 17 features. Optimizing the model on the best feature dataset might improve the general prediction score. I tested the top 17 features (hence, all features without negative values) and found that the validation score the score on the test data (kaggle) for incomparison to using the original 30 features.\n\nI do not show this analysis here in detail but feel free to try and to further optimize the feature selection. To use the best feature datasets X_train_bf\/X_test_bf as definded below, uncomment the respective line for the definition of the GridSearchCV datasets X_train_gs\/X_test_gs as indicated below in section 7.3.","050d13df":"**Summary (also considering *SibSp*):**\nThe probability for a passenger to survive is the higher if he\/she belongs to a smaller family instead of traveling alone (adult) or belonging to big family (more than 3 *SibSp* or *Parch*). We should consider this for feature engeneering. ","586261a9":"# Artificial Neural Networks: A story about feature engineering, quality analysis and hyperparameter optimization","d78a8ce3":"### 4.5 Parch (Parents\/children) <a id=\"FAna5\"><\/a>","51633b4a":"### 4.8 Ticket and Cabin <a id=\"FAna8\"><\/a>","0d0ad372":"###  6.1 Drop useless features<a id=\"Prep1\"><\/a>","333ee726":"# Appendix \n\nIn order to see how other methods perform on the perpared data set, we introduce two strong ensemble methods, Random Forests and Gradient Boosting. Below you will see that despite a optimization with GridSearchCV, none of the the two methodes achieved higher sores than 80.8% on the test data set. Hence, with 83.3% test score, the rather simple ANN we discussed above performes much better.\n\nA carefull analysis would be needed to find out why the ANN scores better on the test data. One reason might be that for the ANN the feature \"Title_Master\" seems to be very important - a feature which is not relevant for both, Random Forests and Gradient Boosting.","e48eec73":"Most passengers travel as singles.","f97a56a7":"In general, the height of the score might not be the best measure to evaluate the performance of a classifier, especially if the dataset is skewed, i.e. one class is much smaller than the other (here about 38% survived, 62% died). It is more instructive to look at the confusion matrix and the metrics one can derive from it:","608f7792":"In general, it should be mentioned that the feature importance strongly depends on the architecture of the model and the hyperparameters, i.e. a different number of layers or neurons per layer, for instance, may change the absolute and relative importance of single features. \nFor instance, I testes the final model with and without implementing the title youngMiss. Although Title_youngMiss shows a feature importance of only 0.02 % points, including youngMiss improved the validation score and prediction on the test dataset by 0.5 % points. Also the absolut importances of the other features changed quite significnatly. \n\nSince some features are quite dominat and clear, one could try to build a wide & deep model where some features have more\/less hidden layers to improve te quality of the model. Nervertheless, one should keep in mind that most of the times, the simple models are the best and more general ones.","4fefa774":"This way we can easily extract the feature names after the encoding. This will help later on if we want to analyse the importance of the different features.","10e13bdb":"[back to Table of Contents](#TOC)\n## 8. Conclusions <a id=\"Conclusions\"><\/a>","75ac6bfd":"### 6.2 Prepare pipeline for numerical and categorical attributes<a id=\"Prep2\"><\/a>","c7332adb":"### 7.3 Find optimum hyperparameters using GridSearch and cross-validation <a id=\"Model3\"><\/a>","1dc1a572":"Indeed, children have a much higher probability to survive. Let's illustrate the different survival probabilities by spliting *Age* in two categories, child (younger than 15) and adult.","008e3bfa":"The only numerical feature that strongly correlates with *Survived* is *Fare*. This might be explaind by the fact that the cabins of the richer passengers were mainly located in the upper parts of the boat, which was not directly fluted after the Titanic hit the iceberg. Moreover, the richer passengers were first to go on the life boats. \n\nThis does not nessecarily mean that the other features are not relevant. \nFor instance, although *Age* in general only shows a rather weak correlation with *Survived*, we should have a closer look at the children, which were allowed to go on the life boats first. Same for women, so the feature *Sex* might be important as well.\n\nSo let's have a closer look a the individual features in order to learn more about their importance.","692ea23b":"### GB: Prediction for test dataset\n","6ac5de9b":"The *recall* value tells us that our classifier detects 74.8% of all survivors (256\/(256+86)).  Moreover, the *precision* value says that if the calssifier predicts a survivor (260), it is right in 82.8% (256\/(256+53)) of all cases.\n\nThe the F1_score is the harmonic mean of precicion and recall and is a good measure for the overall quality of our classifier. A perfect calassifier would have a F1_score equal to one.\n\nTo further evaluate our model, we calculate the probabilities for all instance. If the probability is <0.5, the prediction for the instance\/passenger is 0 (died). For probabilities <=0.5 the survival prediction is 1 (survived).","f6ea5a6d":"### 4.1 *Pclass* (Passenger Class)  <a id=\"FAna1\"><\/a>","27a3e10c":"The following histogram shows the distribution of the probabilities of all instances that were predicted **correctly** by the classifier.","398fd9cf":"### 5.6 Add missing values for *Emarked* <a id=\"FEng6\"><\/a>","e76dfe85":"The training score of about 85% already looks very promissing for the titanic dataset. But what about the 15% false predictions? The question is whether we can still improve this or whether the training data (or the feature preparation) does not offer enough information to get better results.\n\nSo let's run the model on the training data and analyze the prediction in more detail.","b5744965":"We implement it here only for the training dataset to use it for further data analysis below. Later on in section 5 we will implement this feature for the whole dataset.","94b57405":"In general, the pobability of a woman to survive is more than three time higher than for a man. \n\nLet's have a look at this trend regarding the different passenger classes.","f7e7298d":"## B: Gradient Boosting Classifier <a id=\"GB\"><\/a>","f2451f59":"## End of Appendix","ea94759e":"[back to Table of Contents](#TOC)\n## 3. A first glance at the data <a id=\"FirstGlance\"><\/a>","2d793584":"Now we are going to explore how an artificial neural network (ANN) performs on the titanic dataset. \n\nFor a first run and to get a feeling how the ANN works, in section 7.2 we will train a model with a given set of hyperparameters.  We are going to analyze the performance of the ANN and have a look at the importance of the different features.\n\nIn section 7.3, I show in detail how I ended up with the net architecture and the set of hyperparameters introduced in section 7.2. We will go through a step by step instruction of how to optimize the different hyperparameters of an ANN using GridSearch and cross-validation.\n\n","e2c56d27":"From this diagram we learn that the chance to survive for a child from the third class is only about half the value compared to a child from the first or second class. Moreover, a child from the third class has a lower or rather similar probability to survive than an adult of the first or second class. ","fa657c72":"Here, obviously the feature *Title_Mr* which was created during data preparation by the oneHotEncoder (chapter 6.2\/6.3) is the most important feature. Moreover, it seems that all new features we created are somehow relevant for a good classification, which points out the important role of a solid feature engeneering.\n\nIf we turn the above table upside down, we se that there are some features which show a negative score:","02845ef8":"Obviously, the price belongs to a ticket which was purchased for a whole family and not for a single passenger. Since the family relation is already coverd by other features licke *Parch*, *SibSp*, and *Ticket* it might be more informative to have ticket prices per passenger. So let's correct the ticket *Fare* and introduce a new feature *FareCorr* with the fare per passenger:","bc46d7b3":"Here, I use seven fare categories.","4a157549":"\n\n\n### 5.2 Add new feature *AgeCat* (age categories) <a id=\"FEng2\"><\/a>","6315c8c3":"[back to Table of Contents](#TOC)\n## 5. Feature engineering and missing values <a id=\"FEng\"><\/a>","afe3b679":"Looking for a way to improve the number of false predictions rises the question of how the model actually comes up with its decisions. \n\nFor some classifiers (e.g. desicion trees) it is quite easy to trace how the decisions of the model. A neural network, however, is bit like a black box and it is usually not straight forward to understand or visualize the decision making. A nice visualization for a few simple problems can be found [here](https:\/\/playground.tensorflow.org\/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.37306&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false).\n\nSo let's bring a bit of light into our model and have a look a the importance of the different features. To this end, we use [PermutationImportance](https:\/\/eli5.readthedocs.io\/en\/latest\/blackbox\/permutation_importance.html) which is capable of computing feature importances for a number of black-box estimators by measuring how the score decreases when a feature is not available. in concrete, the feature importances is computed as mean decrease of the score when a feature is permuted, i.e. substitued by noise. \n\nTo make use of PermutationImportance, we make use of the scikit-learn classifier API for Keras. To analyze the feature importances we have to create and train a KerasClassifier which is identical to our model. Here we go:","5c6c0c6e":"Most passengers have neither children nor parents on board.","e084a74c":"[back to Table of Contents](#TOC)\n## 4. Feature analysis \n","1bc0dd67":"In the previous section, we have already seen how to build a pretty good ANN classifer. However, the hard part of getting a top classifier is to find the optimum set of hyperparameters. And this is what we are going to do now. \n\nFor the optimization process I went through the following steps using GridSearchCV:\n\n1. Setting the activation function and the respective kernel initializer:\nAs the activation function for the hidden layers I chose the scaled exponential linear unit function *SELU* with the respective LeCun initialization. Using the [*SELU*](https:\/\/arxiv.org\/abs\/1706.02515) the network will self-normalize, where the output of each layer will preserve a mean of 0 and std of 1 during the training. This is in particular benefitial for deep networks which may face problems with vanishing\/exploding gradients. For our rather shallow model this should not be a problem, nevertheless, the *SELU* performed at least 0.5% better than *ElU* or *ReLU* for different hyperparameters. \n\n2. Finding the best Optimizer:\nI tried a number of different optimizers (see parameter grid below) and adtaptive moment estimation with the method [*Nadam*](http:\/\/cs229.stanford.edu\/proj2015\/054_report.pdf) turned out to be the best one.\n\n2. Evaluationg the optimum learning:\nThe learing rate is the most likely the most important parameter. There are several techniqus to optimize the learing rate like ploting the learning curve for various learning rates or using elaborate methods like [*1cycle sheduling*](https:\/\/arxiv.org\/abs\/1803.09820). I tried several techniques, but for our rather simple model it was most practical to try different learning rates using GridSearchCV. I found a learning rate of 0.0005 (half the default value for Nadam) to deliver the best results.\n\n3. Optimizing the number of epochs:\nHaving found a good learning rate, one has to optimize the number of epochs per training run. I found 30 epochs to be a good number not to overfit the model. In general, to prevent overfitting one can use techniques like EarlyStopping or plot\/compare the training vs the validation loss\/score for a series of epochs (discussed below).\nI implemented EarlyStopping with a callback for GridSearchCV (see below), however, it was more practical to simply optimze the epochs and check that the training and the validation score do not diverge with increasing epochs. Especially, since with *patience* one introduces another hyperparameter which has to be optimized. Moreover, it should be noted that before using EarlyStopping whith cross-validation, [a few things](https:\/\/stackoverflow.com\/questions\/48127550\/early-stopping-with-keras-and-sklearn-gridsearchcv-cross-validation) should to be considered.\n\n4. Finding the optimum batch size: \nAgain I used GridSearchCV and found 20 instances to be the best batch size. In general, what is the maximum batch size one should use is still under discussion. There are several controverse discussions in the community.\n\n5. Find the best model architecture:\nGridSearchCV found three combinations to be very similar in performance: #layers\/#neurons: 3\/10, 3\/15, 2\/10. \nThe score of the three models lay within 0.3%. In the end I chose 2 hidden layers with 10 neurons each, since it performed best on the test data.\n\nOne can optimze the hyperparameters by running GridSearch for all different parameters at once, which might take quite a while or find the best parameters by optimizing only one or a few parameters at a time and see how the model performs. Using the latter step-by-step method, it should be noted that one might have to go several times through the different steps in order to find the optimum parameters. \n\nSo here is the GridSearchCV approach using a KerasClassifier:","82aa0994":"Obviously, the children in the third class have more siblings and hence the families are bigger (also see analysis of *Parch*)","9484fc79":"Indeed, the pobability to survive is higher the higher the class. This is an important feature. ","f3f46f2c":"The survival chance of a passenger with 1 or 2 siblings\/spouses is significantly higher than than for a single passenger or a passenger with 3 or more siblings\/spouses. \n\nSince mainly adults (here older than 15 years) are married, let's analyse the data by splitting it between children and adults: ","f8e91466":"Here I used 5 different Age categories.","736be815":"### 6.3 Alternative data preparation<a id=\"Prep3\"><\/a>","12a3552f":"#### Competition Description\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n","ed0dea4f":"To get an overview of the model structure, we can plot the specifications and the model architecture.","e9650d35":"The survival chance of a child with 2 or less siblings is much higher than than for a child with 3 or more siblings. On the contrary, for an adult with 2 or less (or more than 4) siblings the survival chance is lower than for an adult with 3 or 4 siblings. Hence, the family size seems to play an important role, too. To confirm this, we first have to analize *Parch*, aswell.","510a17d2":"In the previous analysis we found that that family mebers of small families have a higher survival chance than singles or passengers with a big family. So let's implement this feature on the whole dataset:","e91a298c":"It is remarkable that the percentage of men in the third class is much bigger.","9e1dca2e":"We should check whether the high value of the maximum Fare is an outlier or just a very expensive Ticket.","4a16a406":"The inspiration for this feature came from [Konstantin](https:\/\/www.kaggle.com\/konstantinmasich\/titanic-0-82-0-83). The idea is to find groups of poeple, e.g. families (share the same LastName\/Ticket) or people that just travel together (share same Ticket) and use the survival status of the group members to derive the survival status *FamilySurvival*. ","b5c16c12":"[back to Table of Contents](#TOC)\n## 6. Data preparation for modelling <a id=\"Prep\"><\/a>","663487d6":"With the title \"Master\" we are able to identify young male passengers, which have a significatly higher survival probability than adult men. For female passengers there is no dedicated title for young girls.\n\nHowever, we can make use of the fact that young Misses (age<15) are most likely traveling at least with one parent, i.e. Parch >= 1. Let's check:","ce8c6fd2":"### 5.5 Add new features *FareCorr* and *FareCat* <a id=\"FEng5\"><\/a>","909ec1b1":"<center>![](https:\/\/media.giphy.com\/media\/uoAn5ik8zAuqI\/giphy.gif)\n","1236d598":"### 7.2.2 Make predictions for the test dataset","a2a44628":"## 1. Introduction <a id=\"Introduction\"><\/a>\n\nDon't panic!! This notebook will give an introduction for machine learning (ML) beginners on how to implement and optimize a simple artificial neural network (ANN) for data classification tasks.\n\nTo this end, we will apply a state-of-the-art ML framework for building and training deep learning models - the Keras API for TensorFlow 2 - to one of the most famous data sets for ML beginners: the Kaggle competition \"Titanic: Machine Learning from Disaster\". The goal of this challenge is to create a ML model that predicts which passengers survived the Titanic tragedy.\n\nIn the notebook, we will put a focus on how to systematically optimize and evaluate an ANN model - two very important aspects if you want to obtain a well performing artificial neural network. We will learn, step by step, how to optimize the hyperparameters of an ANN using GridSearch and cross-validation. Moreover, we are going to discuss in detail how to evaluate the quality of an ANN, analyzing e.g. the confusion matrix and the feature importance.\n\nIn general, we will see that a simple ANN performs very well as a classifier on the titanic dataset (top 2% on the competition leaderboard and with 83.3% test score one of the best ANNs published for this competition) and outperforms strong ensemble methods like Random Forests and Gradient Boosting (80.8% test score, see Appendix).\n\nBesides the detailed introduction to modeling with ANNs, the notebook includes standard techniques for feature analysis and engineering and shows how to prepare the data for the modeling process using pipelines.\n\nI hope you enjoy the notebook and, of course, I am always happy to get some comments and feedback.\n\nSo let's get started...","a2acf00a":"[back to Table of Contents](#TOC)\n## 7.  Modelling: Artificial Neural Network <a id=\"Model\"><\/a>","f496d4a6":"If there are only cotegorical features with no missing values as it is the case for the current feature selection, the features can be transformed by simply using the OneHotEncoder:","31180a5c":"In the third class there are twice as much passengers than in the first and second class, respectively.","a9afd7b6":"Now, knowing that our model is already quite good, we are ready to make the final predictions.","2c142e63":"The high number to the left (around 0) and to the right (around 1) suggests that the classifier was quite certain in most of the predictions that turned out to be **right**.","0ce53f77":"Indeed, \"youngMiss\" shows about the same average age as \"Master\", which supports the assumption we made above.\n\nNow we can fill the missing ages with the average values of the different titles and introduce \"youngMiss\" as a new title:","91610765":"#### Fill missing values of *Age*","14abf748":"### 4.3 *Age* <a id=\"FAna3\"><\/a>","ac75835e":"We already introduced these feature in section 4.6. Now let's implement them for the whole dataset:","a6ebc9cc":"The analysis reveals some more titles, some of which are noble titles or indicate a higher social standing. We now can summarize the different titles in the following categories: ","deca5c00":"There are twice as much men on board than women.","af101a8c":"###  6.4 Split prepared data into training and test datasets <a id=\"Prep4\"><\/a>","8b739044":"In the next section (5.2) we will introduce a further title called youngMiss, which is the female equivalent to master.","fcf66a86":"<a id=\"TOC\"><\/a>\n## Table of contents\n\n* **1. [Introduction](#Introduction)** <br>\n* **2. [Import libraries and data](#Import)** <br>\n* **3. [A first glance at the data](#FirstGlance)** <br>\n* **4. [Feature analysis](#FAna)** <br>\n    * 4.1 [Pclass (passenger class)](#FAna1) <br>\n    * 4.2 [Sex](#FAna2) <br>\n    * 4.3 [Age](#FAna3) <br>\n    * 4.4 [SibSp (siblings\/pouse)](#FAna4) <br>\n    * 4.5 [Parch (parents\/children)](#FAna5) <br>\n    * 4.6 [Fare](#FAna6) <br>\n    * 4.7 [Embarked](#FAna7) <br>\n    * 4.8 [Ticket and Cabin](#FAna8) <br>\n* **5. [Feature engineering](#FEng)** <br>\n    * 5.1 [Add new feature Title](#FEng1) <br>\n    * 5.2 [Add new feature AgeCat (age categories) ](#FEng2) <br>\n    * 5.3 [Add new feature FamilySize](#FEng3) <br>\n    * 5.4 [Add new feature FamilySurvival](#FEng4) <br>\n    * 5.5 [Add new features FareCorr and FareCat](#FEng5) <br>\n    * 5.6 [Add missing values for Emarked](#FEng6) <br>\n* **6. [Data preparation for modelling](#Prep)** <br>\n    * 6.1 [Drop useless features](#Prep1) <br>\n    * 6.2 [Prepare pipeline for numerical and categorical attributes](#Prep2) <br>\n    * 6.3 [Alternative data preparation](#Prep3) <br>\n    * 6.4 [Split prepared data into training and test datasets ](#Prep4) <br>\n* **7. [Modelling: Artificial Neural Network](#Model)** <br>\n    * 7.1 [The Model](#Model1) <br>\n    * 7.2 [A first look at neural networks on the titanic ](#Model2) <br>\n    * 7.3 [Find optimum hyperparameters using GridSearch and cross-validation ](#Model3) <br>\n* **8. [Conclusions](#Conclusions)** <br>\n\n**Appendix A: [Random Forest](#RF)** <br>\n**Appendix B: [Gradient Boosting](#GB)**","9cd7b3a1":"#### New feature *AgeCat* (age categories)","c8e2fbd1":"Analyzing the ditributions for different *Pclass* reveals that, for instance, some 3rd class tickets are much more expensive than the average 1st class ticket.\n\nSo let's have a look at these expensive 3rd class tickets with a price around 70:","16348b6e":"### 5.1 Add new feature *Title*  <a id=\"FEng1\"><\/a>","447de71c":"### 4.4 *SibSp* (Siblings\/Spouse) <a id=\"FAna4\"><\/a>","fe1bf8bd":"### 5.4 Add new feature *FamilySurvival* <a id=\"FEng4\"><\/a>","0bc9b04c":"### GB: Modelling","cab8584e":"It is important to find out how the different features correlate with *Survived*. To this end, we use the training data for which the information about the passenger's survival is given (column *Survived*).","b5eb5280":"### 4.2 *Sex* <a id=\"FAna2\"><\/a>","478ebb0e":"These distributions for the real fare per passenger now strongly correlate with *Pclass* and look more \"natural\". Hence, passengers ahich purchased a more expensive ticket have a higher probability to survive. This becomes obvious if we introduce fare categories *FareCat* based on *FareCorr* and analyze the survival rate:  ","2f6dae2c":"First of all, we define the function create_model() which gives us the freedom to try different model architectures by setting the respective hyperparameters. How I came up with default set of hyperparameters will be discussed in section 7.3.\n","470612bd":"### RF: Prediction for test dataset\n","90a3f0f6":"### 5.3 Add new feature *FamilySize* <a id=\"FEng3\"><\/a>","7251e737":"#### Implement new feature *AgeCat*","78ff10d4":"We already mentioned that most likley certain age groups might have a higher probability to survive. Let's have a look at the statistics:","a26d4a59":"The passengers have different titels which give information about their sex (Mr., Mrs., Miss., Master, Rev.), about their social status (Dr., Rev.) or about their age (Mrs., Miss., Master, Dr., Rev.). Characteristics which affect the survival probability. Hence, it makes sense to create a new feature *Title*.","b705e717":"The code above first sets the values for all passegers to 0.5. This basically means the families survival status is unknown. Then all passengers are grouped regarding their *Last_Name* to find families (at least two passengers with the same name). It selects each member of the family and checks if one of the other family members (from which the survival satus is known, train set) has survived (feature *Survived* is 1). If this is the case, *FamilySurvival* is set to 1. Else the feature *FamilySurvival* is set to 0. Ecept if the target *Survived* is unknown for all family members (all from test set). In this case the value of *FamilySurvival* remains 0.5. \nIn the last step, the code checks the feature *Ticket* of all passengers with *FamilySurvival*  0 and 0.5, if there is another passenger with the same ticket number and does the same thing. Passengers with the same ticket number are likely to travel togehther and hence, may have a correlation in their survival status.","105f426a":"### 7.2 A first look at neural networks on the titanic <a id=\"Model2\"><\/a>","6f50d356":"... already 63% of the Misses have no parents\/children on board (). So a passenger with the title 'Miss' and a non-zero Parch value is most likly a young female.\n\nSo let's consider two categories, \"Miss\" and \"young Miss\": ","5be1afda":"We will take care of the missing values later on in this notebook.","1d28ac66":"\n<center><font size=4 color=\"red\"> ... and please upvote this kernel if you like it. It motivates me to produce more quality content. Thanks!<\/font><\/center>\n","57dfc627":"### RF: Predict training data for further evaluation"}}