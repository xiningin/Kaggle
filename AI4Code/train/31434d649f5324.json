{"cell_type":{"3cc18f73":"code","a7090926":"code","7cb86013":"code","bac71b66":"code","7d2364fe":"code","035038c8":"code","30488c05":"code","c5810ab0":"code","3efab5e8":"code","71da4636":"code","d5b0f425":"code","c57e9303":"code","b62133ee":"code","868a5c46":"code","31691913":"code","a0a25190":"code","826abea8":"code","99a96973":"code","07f10b93":"code","495e3e58":"code","ffc506df":"code","a95a702c":"code","c8f6b724":"code","a77d7b3d":"code","5ec22e42":"code","bee77aeb":"code","d788daf1":"code","7e90cbcb":"code","869f5046":"code","a28ea1aa":"code","8f02fa76":"code","c1d59154":"code","c4390644":"code","1727829f":"code","e5648f8b":"code","529e029d":"code","778ccc21":"code","bbfea654":"code","04a19615":"code","5a02f9b7":"code","e7c57cf2":"code","90028b90":"code","73b22420":"code","8c86b6a3":"code","d8cb7808":"code","f959f1a9":"code","b010f13e":"markdown","f7bdcea0":"markdown","e381b7e6":"markdown","35b8fcbe":"markdown","cc5aea7d":"markdown","aeda30ee":"markdown","1ec4fa7e":"markdown","8c80cbff":"markdown","10d9a424":"markdown","b4b89f4b":"markdown","17970f77":"markdown","a90d20ae":"markdown","2b7f5b1f":"markdown","75ef7665":"markdown","95b84acb":"markdown"},"source":{"3cc18f73":"#Load packages\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom collections import defaultdict\nfrom collections import  Counter\nfrom matplotlib import pyplot as plt\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","a7090926":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', sep=',')\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv',sep=',')\ndf_train.shape\n","7cb86013":"ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n\ntrain_orig = df_train \ntrain_orig[train_orig['id'].isin(ids_with_target_error)]\n\ntrain_orig.at[train_orig['id'].isin(ids_with_target_error),'target'] = 0\n\ntrain_orig[train_orig['id'].isin(ids_with_target_error)]\n\ndf_train = train_orig","bac71b66":"#### Check missing values\ndf_train.isnull().sum()","7d2364fe":"### Check distribution of class labels. \nx = df_train.target.value_counts()\ncountplt = sns.barplot(x.index,x)\ncountplt.set_xticklabels(['0: Not Disaster (4342)', '1: Disaster (3271)'])\n#plt.gca().set_ylabel('samples')\n#Ref: https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove","035038c8":"### Plot tweets distribution over locations, that are aggregated in to a country's level\ntrain = df_train\ntrain['location'].replace({'United States':'USA',\n                           'New York':'USA',\n                            \"London\":'UK',\n                            \"Los Angeles, CA\":'USA',\n                            \"Washington, D.C.\":'USA',\n                            \"California\":'USA',\n                             \"Chicago, IL\":'USA',\n                             \"Chicago\":'USA',\n                            \"New York, NY\":'USA',\n                            \"California, USA\":'USA',\n                            \"FLorida\":'USA',\n                            \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                            \"United Kingdom\":'UK',\n                            \"Los Angeles\":'USA',\n                            \"Toronto\":'Canada',\n                            \"San Francisco, CA\":'USA',\n                            \"NYC\":'USA',\n                            \"Seattle\":'USA',\n                            \"Earth\":'Worldwide',\n                            \"Ireland\":'UK',\n                            \"London, England\":'UK',\n                            \"New York City\":'USA',\n                            \"Texas\":'USA',\n                            \"London, UK\":'UK',\n                            \"Atlanta, GA\":'USA',\n                            \"Mumbai\":\"India\"},inplace=True)\n\nsns.barplot(y=train['location'].value_counts()[:5].index,x=train['location'].value_counts()[:5],\n            orient='h')\n\n# Ref: https:\/\/www.kaggle.com\/alex094495\/getting-started-with-nlp-a-general-intro\/edit","30488c05":"## examine the \"keyword\" distribution\ndf_train['keyword'].value_counts()","c5810ab0":"print('keywords for disaster tweets:','\\n', df_train[df_train.target==1].keyword.value_counts().head(10), '\\n')\nprint('keywords for non-disaster tweets:','\\n',df_train[df_train.target==0].keyword.value_counts().head(10))","3efab5e8":"tweet = df_train\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n\nword=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\n#ax1.set_title('disaster')\nword=tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\n#ax2.set_title('Not disaster')\n#fig.suptitle('Average word length in each tweet')\n\n### As we can see, tweet with disasters have greater average lenth of text. \n### Our assumptions is that description of real disasters are more formal.","71da4636":"### Tweet lenghths distributions comparison between real disaster tweet and non-disaster ones\ndef length(text):    \n    '''a function which returns the length of text'''\n    return len(text)\ntweet = df_train\ntweet['length'] = tweet['text'].apply(length)\n\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nbins = 150\nplt.hist(tweet[tweet['target'] == 0]['length'], alpha = 0.6, bins=bins, label='Not')\nplt.hist(tweet[tweet['target'] == 1]['length'], alpha = 0.8, bins=bins, label='Real')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,150)\nplt.grid()\nplt.show()\n\n##Ref: https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert","d5b0f425":"df=pd.concat([df_train,df_test])","c57e9303":"df","b62133ee":"!pip install pyspellchecker","868a5c46":"\n\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\ndf['text']=df['text'].apply(lambda x : remove_URL(x))\n\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\ndf['text']=df['text'].apply(lambda x : remove_html(x))\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndf['text']=df['text'].apply(lambda x: remove_emoji(x))\n\ndef clean_text(text):\n    text = re.sub(r'https?:\/\/\\S+', '', text) # Remove link\n    text = re.sub(r'\\n',' ', text) # Remove line breaks\n    text = re.sub('\\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces\n    return text\ndf['text']=df['text'].apply(lambda x: clean_text(x))\n\n# Ref: https:\/\/www.kaggle.com\/aaroha33\/disaster-tweets-evaluation-with-nlp \n","31691913":"### Turn cleaned tweets into corpus (lower case, non stop, alphabetical words)\nfrom tqdm import tqdm ### This is for a progress bar\ndef create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus\ncorpus=create_corpus(df)\n","a0a25190":"### Generate embedding dict from the GloVe txt file. So in the dictionary, every word is \n### associated with the GloVe representation of them\nimport numpy as np\nembedding_dict={}\n\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.50d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","826abea8":"MAX_LEN=50\ntokenizer_obj=Tokenizer() ## Initialize tokenizer\ntokenizer_obj.fit_on_texts(corpus) \nsequences=tokenizer_obj.texts_to_sequences(corpus)### Convert each tweet in the corpus into sequence of numbers\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')### Pad the sequences so that they have the same length\n\nword_index=tokenizer_obj.word_index\nword_index\n#Now each word in the corpus is associated with a number representing the location it is in the sequence","99a96973":"### Generate embedding matrix using the dictionary \"embedding_dict={}\" from GloVe\nnum_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,50))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec","07f10b93":"### Train test split\ntrain=tweet_pad[:tweet.shape[0]]\ntest=tweet_pad[tweet.shape[0]:]\nX_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)","495e3e58":"### Neural Network\nmodel=Sequential() ### Initiate neural network\n\nembedding=Embedding(num_words,50,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False) ### First layer is the embedding layer\n\nmodel.add(embedding) ## Add the GloVe embedding layer\n\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2)) ### RNN\nmodel.add(Dense(1, activation='sigmoid')) ### Sigmoid for binary classification\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n","ffc506df":"\n\nmodel.summary() \n\n","a95a702c":"\n## Train the model\nhistory=model.fit(X_train,y_train,batch_size=4,epochs=10,validation_data=(X_test,y_test),verbose=True)\n\n","c8f6b724":"\n\npredictions=model.predict(test)\npredictions=np.round(predictions).astype(int).reshape(3263)\n","a77d7b3d":"sample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","5ec22e42":"sample_submission[\"target\"] = predictions","bee77aeb":"sample_submission.tail(50)","d788daf1":"sample_submission.to_csv(\"submission.csv\", index=False)","7e90cbcb":"from tensorflow.keras import metrics\nMETRICS = [\n      metrics.TruePositives(name='tp'),\n      metrics.FalsePositives(name='fp'),\n      metrics.TrueNegatives(name='tn'),\n      metrics.FalseNegatives(name='fn'), \n      metrics.BinaryAccuracy(name='accuracy'),\n      metrics.Precision(name='precision'),\n      metrics.Recall(name='recall'),\n      metrics.AUC(name='auc')]","869f5046":"import matplotlib.gridspec as gridspec\ndef plot_model_eval(history):\n\n    string = ['loss', 'accuracy']  \n    cnt = 0\n    ncols, nrows = 2, 1  \n    fig = plt.figure(constrained_layout=True, figsize = (10,10))\n    gs = gridspec.GridSpec(ncols = 3, nrows = 2, figure = fig)\n    for i in range(nrows):\n        for j in range(ncols):\n            ax = plt.subplot(gs[i,j]) \n            ax.plot(history.history[string[cnt]])\n            ax.plot(history.history['val_'+string[cnt]]) \n            ax.set_xlabel(\"Epochs\")\n            ax.set_ylabel(string[cnt])\n            ax.legend([string[cnt], 'val_'+string[cnt]])\n            cnt +=1\n        \nplot_model_eval(history)\n\n","a28ea1aa":"## Getting Started : https:\/\/www.kaggle.com\/alex094495\/getting-started-with-nlp-a-general-intro\/edit","8f02fa76":"!pip install tweepy","c1d59154":"import tweepy\nimport json","c4390644":"#API Keys\nCONSUMER_KEY = 'UtRoBSnR38yyn5wFkWsqRqFiU'\nCONSUMER_SECRET = '6XXbta8pgQR6lpiFuj1yzdZWGTgupiCNTNOlLNvV3Wq6w7LoOA'\n\n#Access Tokens\nOAUTH_TOKEN = '237972429-8LQBXOK1vdEJmi2ZAkxrZK9KdpLgcsmL5YkpOZDO'\nOAUTH_SECRET = 'a82bJ0KCKLbE36oBcD9P20HZjmS3rmUsVwM0mBBnOXGFa'\nauth = tweepy.OAuthHandler(CONSUMER_KEY,CONSUMER_SECRET)\nauth.set_access_token(OAUTH_TOKEN,OAUTH_SECRET)\napi = tweepy.API(auth)","1727829f":"query = 'covid'\nresult_tweets_covid = [status for status in tweepy.Cursor(api.search, q=query, lang='en', tweet_mode='extended').items(1000)]","e5648f8b":"count = 0\nfor tweet in result_tweets_covid:\n    if hasattr(tweet, 'retweeted_status'):\n        count +=1\nprint('RT count: ', count)\nprint('Number of tweets in total: ', len(result_tweets_covid))\nprint('Non-RT count: ', len(result_tweets_covid) - count)","529e029d":"tweet_covid_no_rt = [tweet for tweet in result_tweets_covid if not hasattr(tweet, 'retweeted_status')]\nprint(len(tweet_covid_no_rt))","778ccc21":"# Write result to a file to persist query result\n# Comment this cell out to just load the previous stored tweets without overwrting them\ntweet_covid_list = [tweet.full_text for tweet in tweet_covid_no_rt]\nwith open('covid.json', 'w') as f:\n    json.dump(tweet_covid_list, f)","bbfea654":"with open('covid.json') as json_file:\n    covid_tweet_list = json.load(json_file)","04a19615":"# Cleaning\ndf_extend = pd.DataFrame(covid_tweet_list,columns=['text'])\ndf_extend['text']=df_extend['text'].apply(lambda x : remove_URL(x))\ndf_extend['text']=df_extend['text'].apply(lambda x : remove_html(x))\ndf_extend['text']=df_extend['text'].apply(lambda x: remove_emoji(x))\ndf_extend['text']=df_extend['text'].apply(lambda x: clean_text(x))","5a02f9b7":"corpus_extend=create_corpus(df_extend)","e7c57cf2":"MAX_LEN=50\ntokenizer_obj=Tokenizer() ## Initialize tokenizer\ntokenizer_obj.fit_on_texts(corpus_extend) \nsequences_extend=tokenizer_obj.texts_to_sequences(corpus_extend)### Convert each tweet in the corpus into sequence of numbers\ntweet_pad_extend=pad_sequences(sequences_extend,maxlen=MAX_LEN,truncating='post',padding='post')### Pad the sequences so that they have the same length\n","90028b90":"predictions_extend=model.predict(tweet_pad_extend)\n\n# If you see an error when rerunning the code, use 249 instead of len(tweet_covid_no_rt)\n# Because query result is different every time the query runs\npredictions_label_extend=np.round(predictions_extend).astype(int).reshape(len(tweet_covid_no_rt))\n","73b22420":"table_extend = pd.DataFrame(predictions_label_extend, columns=['label_predict'])\nresult = pd.concat([df_extend, table_extend], axis=1, sort=False)","8c86b6a3":"result['label_predict'].value_counts()","d8cb7808":"# The recall of classifying COVID tweets as true disaster tweets\nresult.query('label_predict == 1').label_predict.count() \/ result.shape[0]","f959f1a9":"pd.set_option('display.max_colwidth', -1)\nresult.head(10)","b010f13e":"## Data cleaning\nremove urls, html, emoji, punctuation\n\nspell checker\n\nhttps:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove","f7bdcea0":"The following processing steps are basically the same as above.","e381b7e6":"### EDA","35b8fcbe":"Out of 211 non-RT COVID tweets, 95 are properly classified as disaster tweets.","cc5aea7d":"### Predicting real disaster tweet using GloVe and LSTM\nTo determine whether a person\u2019s words are actually announcing a disaster is important: \nDataset description:","aeda30ee":"### Data Prep Completed HERE","1ec4fa7e":"## Model Building","8c80cbff":"### Prediction using new tweets from Twitter\n\nNext, we query 1000 tweets that contains the word 'covid' (case insensitive) to see the recall of our model on new real new tweets from Twitter.\nWe assume that all tweets containing 'covid' are disaster tweets, so the true-labeled proportion is the recall of our model on the new tweets.","10d9a424":"### Model Evaluation\nhttps:\/\/www.kaggle.com\/mjvakili\/glove-bidirectional-lstm","b4b89f4b":"Some fetched tweets and their labels:","17970f77":"### Fixing data source error\nRef: https:\/\/www.kaggle.com\/wrrosa\/keras-bert-using-tfhub-modified-train-data\nThere are several tweets in the dataset are incorrectly labeled. In fact, they are not disaster related by they are labeled so. This will actually harm the model performance. ","a90d20ae":"The recall on this new dataset is not very high because the training dataset is very old and doesn't contain any information about this pandemic in 2020. But it's still better than expected.","2b7f5b1f":"We use only non-RT tweets because the training set does not contain RT comments.","75ef7665":"# NLP - EDA, Bag of Words, TF IDF, GloVe, BERT: https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert ","95b84acb":"### Make Prediction"}}