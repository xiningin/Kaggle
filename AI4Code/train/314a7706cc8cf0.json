{"cell_type":{"84336407":"code","a971f9c8":"code","8e5bfaf5":"code","644573a0":"code","6c702548":"code","65f92b2d":"code","3eab6091":"code","a535b9fd":"code","18927537":"code","7a25aba4":"code","8ea12127":"code","a439f6b0":"code","d6e1c02a":"code","57e1bf65":"code","026b80cb":"code","caf5d0c8":"code","d5f109dc":"code","d3a28b49":"code","b3a6b697":"code","d43545fa":"code","92670bcd":"code","3c9f410f":"code","1326135d":"code","d5099f83":"code","b2958746":"code","241d8814":"code","2c0d057f":"code","5e89ce79":"code","130bf3a8":"code","180f4bad":"code","67644e5b":"code","51ca9fa0":"code","6962171d":"code","c4bb9857":"code","f6e05dcb":"code","4522b782":"code","1410863d":"code","e41d0898":"code","a48cc9ab":"code","dfe3295e":"code","ff1b750d":"code","026d734e":"code","7963978f":"markdown","bc58855e":"markdown","d80112c9":"markdown","f83c7fae":"markdown","2892be2b":"markdown","92f83e96":"markdown","a51d2160":"markdown","dff9234e":"markdown","a27ca388":"markdown","cc4fbada":"markdown","d72911cb":"markdown","fe77481f":"markdown","fad66b24":"markdown","28d089e0":"markdown","547e0971":"markdown","d3e1d8c6":"markdown","c84b51a9":"markdown","2346b48a":"markdown","dd247a4a":"markdown","924e96d8":"markdown","e5fea92b":"markdown","9013653d":"markdown","395ac82d":"markdown"},"source":{"84336407":"import warnings\nimport random\nimport os\nimport gc\nimport shap\nimport torch","a971f9c8":"import pandas            as pd\nimport numpy             as np\nimport matplotlib.pyplot as plt \nimport seaborn           as sns\nimport joblib            as jb\nimport scikitplot        as skplt # conda install -c conda-forge\/label\/cf201901 scikit-plot","8e5bfaf5":"from sklearn.metrics           import silhouette_samples, silhouette_score\nfrom sklearn.model_selection   import train_test_split, KFold #, RepeatedStratifiedKFold, StratifiedKFold  \nfrom sklearn                   import metrics\nfrom sklearn.preprocessing     import QuantileTransformer, StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler\nfrom sklearn.cluster           import KMeans","644573a0":"from yellowbrick.cluster       import KElbowVisualizer, SilhouetteVisualizer\n#from prettytable               import PrettyTable\n#from sklearn.ensemble          import IsolationForest\nfrom datetime                  import datetime","6c702548":"import xgboost                 as xgb","65f92b2d":"def jupyter_setting():\n    \n    %matplotlib inline\n      \n    #os.environ[\"WANDB_SILENT\"] = \"true\" \n    #plt.style.use('bmh') \n    #plt.rcParams['figure.figsize'] = [20,15]\n    #plt.rcParams['font.size']      = 13\n     \n    pd.options.display.max_columns = None\n    #pd.set_option('display.expand_frame_repr', False)\n\n    warnings.filterwarnings(action='ignore')\n    warnings.simplefilter('ignore')\n    warnings.filterwarnings('ignore')\n    warnings.filterwarnings('ignore', category=DeprecationWarning)\n    warnings.filterwarnings('ignore', category=FutureWarning)\n    warnings.filterwarnings('ignore', category=RuntimeWarning)\n    warnings.filterwarnings('ignore', category=UserWarning)\n    \n    #pd.set_option('display.max_rows', 5)\n    #pd.set_option('display.max_columns', 500)\n    #pd.set_option('display.max_colwidth', None)\n\n    icecream = [\"#00008b\", \"#960018\",\"#008b00\", \"#00468b\", \"#8b4500\", \"#582c00\"]\n    #sns.palplot(sns.color_palette(icecream))\n    \n    return icecream\n\nicecream = jupyter_setting()","3eab6091":"def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    \n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.rcParams['font.size'] = 12\n    plt.title('Precision Recall vs threshold')\n    plt.xlabel('Threshold')\n    plt.legend(loc=\"lower left\")\n    \n    plt.grid(True)","a535b9fd":"def plot_precision_vs_recall(precisions, recalls):\n    plt.plot(recalls[:-1], precisions[:-1], \"b-\", label=\"Precision\")\n    \n    plt.rcParams['font.size'] = 12\n    plt.title('Precision vs recall')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    # plt.legend(loc=\"lower left\")\n    \n    plt.grid(True)","18927537":"def plot_roc_curve(fpr, tpr, label=None):\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, \"r-\", label=label)\n    ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.rcParams['font.size'] = 12\n    plt.title('XGBR ROC curve for TPS 09')\n    plt.xlabel('False Positive Rate (1 - Specificity)')\n    plt.ylabel('True Positive Rate (Sensitivity)')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True)","7a25aba4":"def confusion_plot(matrix, labels = None, title = None):\n    \n    \"\"\" Exibir matriz de confus\u00e3o bin\u00e1ria como um mapa de calor Seaborn \"\"\"\n    plt.figure(figsize=(7,5))\n    \n    labels = labels if labels else ['Negative (0)', 'Positive (1)']\n    \n    #labels      = ['No Churn', 'Churn']\n    \n    fig, ax = plt.subplots(nrows=1, ncols=1)\n    \n    sns.heatmap(data        = matrix, \n                cmap        = 'Blues', \n                annot       = True, \n                fmt         = 'd',\n                xticklabels = labels, \n                yticklabels = labels, \n                ax          = ax)\n    \n    ax.set_xlabel('\\n PREVISTO', fontsize=15)\n    ax.set_ylabel('REAL \\n', fontsize=15)\n    ax.set_title(title)\n    \n    plt.close()\n    \n    return fig;","8ea12127":"def reduce_memory_usage(df, verbose=True):\n    \n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    \n    for col in df.columns:\n        \n        col_type = df[col].dtypes\n        \n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n        \n    return df","a439f6b0":"def diff(t_a, t_b):\n    from dateutil.relativedelta import relativedelta\n    t_diff = relativedelta(t_b, t_a)  # later\/end time comes first!\n    return '{h}h {m}m {s}s'.format(h=t_diff.hours, m=t_diff.minutes, s=t_diff.seconds)","d6e1c02a":"!mkdir img\n!mkdir Data\n!mkdir Data\/pkl\n!mkdir Data\/submission\n\n!mkdir model\n!mkdir model\/preds\n!mkdir model\/optuna\n\n!mkdir model\/preds\/test\n!mkdir model\/preds\/test\/n1\n!mkdir model\/preds\/test\/n2\n!mkdir model\/preds\/test\/n3\n\n!mkdir model\/preds\/train\n!mkdir model\/preds\/train\/n1\n!mkdir model\/preds\/train\/n2\n!mkdir model\/preds\/train\/n3\n!mkdir model\/preds\/param","57e1bf65":"path   = '..\/input\/tps11001\/'\ntarget = 'target'","026b80cb":"%%time\ndf1_train     = jb.load(path + 'df1_nb_01_train.pkl.z')\ndf1_test      = jb.load(path + 'df1_nb_01_test.pkl.z')\ndf_submission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')\n\ndf1_train.drop('id', axis=1, inplace=True)\ndf1_test.drop('id', axis=1, inplace=True)\n\ngc.collect()\n\ndf1_train.shape, df1_test.shape, df_submission.shape","caf5d0c8":"df1_train.head()","d5f109dc":"feature_num = df1_train.columns.to_list() \nfeature_num.remove('target')","d3a28b49":"%%time\nscaler        = QuantileTransformer(output_distribution='normal', random_state=0)\ndf1_train_qt  = df1_train.copy().drop('target', axis=1) \ndf1_train_qt  = pd.DataFrame(scaler.fit_transform(df1_train_qt), columns=feature_num)\ndf1_test_qt   = pd.DataFrame(scaler.fit_transform(df1_test), columns=feature_num)","b3a6b697":"%%time \n\nplt.figure(figsize=(12, 7))\nvisualizer_1 = KElbowVisualizer(KMeans(random_state=12359), k=(2,12))\nvisualizer_1.fit(df1_train_qt.head(300000));\nvisualizer_1.poof();","d43545fa":"model_kmeans = KMeans(n_clusters=5, random_state=59)\nmodel_kmeans.fit(df1_train_qt);\n\nclusters_train = model_kmeans.predict(df1_train_qt)\nclusters_test  = model_kmeans.predict(df1_test_qt)\n\ndf1_train['fe_cluster'] = clusters_train\ndf1_test['fe_cluster']  = clusters_test\n\ndf1_train.shape, df1_test.shape","92670bcd":"df1_train.head()","3c9f410f":"df1_train = pd.get_dummies(df1_train, columns=['fe_cluster'])\ndf1_test = pd.get_dummies(df1_test, columns=['fe_cluster'])\n\ndf1_train.shape, df1_test.shape","1326135d":"df1_train.head()","d5099f83":"del df1_test_qt, df1_train_qt","b2958746":"df1_train['fe_mean']        = df1_train[feature_num].mean(axis=1)\ndf1_test['fe_mean']         = df1_test[feature_num].mean(axis=1)\ndf1_train['fe_median']      = df1_train[feature_num].median(axis=1)\ndf1_test['fe_median']       = df1_test[feature_num].median(axis=1)\ndf1_train['fe_min']         = df1_train[feature_num].min(axis=1)\ndf1_test['fe_min']          = df1_test[feature_num].min(axis=1)\ndf1_train['fe_max']         = df1_train[feature_num].max(axis=1)\ndf1_test['fe_max']          = df1_test[feature_num].max(axis=1)\ndf1_train['fe_skew']        = df1_train[feature_num].skew(axis=1)\ndf1_test['fe_skew']         = df1_test[feature_num].skew(axis=1)\n\ngc.collect()\n\ndf1_train.shape, df1_test.shape","241d8814":"df1_train.head()","2c0d057f":"jb.dump(df1_train,  \"Data\/pkl\/df2_nb_02_train.pkl.z\")\njb.dump(df1_test,  \"Data\/pkl\/df2_nb_02_test.pkl.z\")\n\ndel df1_train, df1_test\ngc.collect()","5e89ce79":"df2_train = jb.load('Data\/pkl\/df2_nb_02_train.pkl.z')\ndf2_test  = jb.load('Data\/pkl\/df2_nb_02_test.pkl.z')\n\ndf2_train.shape, df2_test.shape","130bf3a8":"print(feature_num)","180f4bad":"X      = df2_train.drop(['target'], axis=1)\ny      = df2_train['target']\n#cols   = X.columns\nX_test = df2_test\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                      test_size    = 0.2,\n                                                      shuffle      = True, \n                                                      stratify     = y,\n                                                      random_state = 0)\n\ndel df2_train, df2_test\ngc.collect()\n\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape ","67644e5b":"X_test.head()","51ca9fa0":"seed   = 12359\nparams = {'objective'     : 'binary:logistic',    \n          'eval_metric'   : 'auc',\n          'random_state'  : seed}\n\nif torch.cuda.is_available():           \n    params.update({'predictor'  : 'gpu_predictor', \n                   'tree_method': 'gpu_hist', \n                   'gpu_id'     :  0})\n\nparams","6962171d":"path=''\n\ndef save_data_model(model_, model_name_, path_, X_train_prob_, y_hat_test_, score_, seed_, level_='1'):\n\n    level_ = 'n'+ level_ + '\/'\n\n    if score_>.6:          \n\n        path_name_param = path_ + 'model\/preds\/param\/' + model_name_.format(score_, seed_)\n        path_name_train = path_ + 'model\/preds\/train\/' + level_ + model_name_.format(score_, seed_)\n        path_name_test  = path_ + 'model\/preds\/test\/'  + level_ + model_name_.format(score_, seed_)    \n        path_name_model = path_ + 'model\/mdl\/'         + model_name_.format(score_, seed_)    \n\n        jb.dump(X_train_prob_, path_name_train)\n        jb.dump(y_hat_test_, path_name_test)\n        jb.dump(model_, path_name_model)\n        jb.dump(pd.DataFrame([model_.get_params()]), path_name_param)   \n\n        if score_>.65:\n            # Gerar o arquivo de submiss\u00e3o \n            df_submission['target'] = y_hat_test_\n            name_file_sub =  path_ + 'Data\/submission\/tunning\/' + model_name_.format(score_, seed_) + '.csv'\n            df_submission.to_csv(name_file_sub, index = False)\n\ndef cross_val_model(model, X_train_, y_train_, X_test_,  scalers, name_model, \n                    FOLDS=5, verbose=False, seed=12359, use_ntree_limit=False): \n    \n    time_s = datetime.now()\n        \n    mdl_train   = []\n    feature_imp = 0 \n    auc_best    = 0\n    \n    for scaler in scalers: \n        \n        gc.collect()\n\n        df_submission.claim = 0           \n        feature_imp_best    = 0   \n        X_train_prob        = []\n        auc                 = []\n        lloss               = []\n        f1                  = []\n        ntree               = []\n        n_estimators        = model.get_params()['n_estimators'] \n        kfold               = KFold(n_splits=FOLDS, random_state=seed, shuffle=True)\n\n        if scaler!=None:\n            X_ts = scaler.fit_transform(X_test_.copy())\n        else:\n            X_ts = X_test_.copy()\n\n        print('='*80)\n        print('Scaler: {} - n_estimators: {}'.format(scaler,n_estimators))\n        print('='*80)\n\n        for i, (train_idx, test_idx) in enumerate(kfold.split(X_train_)):\n\n            time_start = datetime.now()\n            \n            i+=1\n\n            X_tr, y_tr = X_train_.iloc[train_idx], y_train_.iloc[train_idx]\n            X_vl, y_vl = X_train_.iloc[test_idx], y_train_.iloc[test_idx]\n\n            # Scaler\n            if scaler!=None:    \n                X_tr = scaler.fit_transform(X_tr)\n                X_vl = scaler.fit_transform(X_vl)                \n\n            model.fit(X_tr, y_tr, \n                      eval_set              = [(X_tr,y_tr), (X_vl,y_vl)],\n                      early_stopping_rounds = int(n_estimators*.1), \n                      verbose               = verbose\n                     )\n            \n            if use_ntree_limit:\n                y_hat_prob  = model.predict_proba(X_vl, ntree_limit=model.best_ntree_limit)[:, 1] # \n                best_ntree_ = model.best_ntree_limit\n            else: \n                y_hat_prob  = model.predict_proba(X_vl)[:, 1] # \n                best_ntree_ = n_estimators\n                            \n            y_hat         = (y_hat_prob >.5).astype(int) \n            log_loss_     = metrics.log_loss(y_vl, y_hat_prob)                \n            f1_score_     = metrics.f1_score(y_vl, y_hat)                    \n            auc_          = metrics.roc_auc_score(y_vl, y_hat_prob)\n\n            elapsed  = diff(time_start, datetime.now())\n            \n            stop = '*' if n_estimators > best_ntree_ else ' '\n            msg  = '[Fold {}] AUC: {:.5f} - F1: {:.5f} - L. LOSS: {:.5f} {} {} {}'\n            print(msg.format(i, auc_, f1_score_,log_loss_, stop, best_ntree_, elapsed))\n\n            # Getting mean feature importances (i.e. devided by number of splits)\n            feature_imp  += model.feature_importances_ \/ FOLDS\n            \n            df_submission['target'] += model.predict_proba(X_ts)[:, 1] \/ FOLDS\n            \n                  \n            f1.append(f1_score_)\n            lloss.append(log_loss_)\n            auc.append(auc_)\n            ntree.append(best_ntree_)\n            \n            X_train_prob.append(auc_)  \n            \n            gc.collect()\n                        \n        auc_mean   = np.mean(auc)\n        auc_std    = np.std(auc)\n        lloss_mean = np.mean(lloss)\n        f1_mean    = np.mean(f1)\n        ntree_mean = np.mean(ntree)\n        \n        if auc_mean > auc_best: \n            auc_best          = auc_mean\n            f1_best           = f1_mean\n            lloss_best        = lloss_mean\n            model_best        = model\n            feature_imp_best  = feature_imp\n            scaler_best       = scaler\n           \n        elapsed  = diff(time_s, datetime.now())\n            \n        print('-'*80)\n        msg = '[Mean Fold] AUC: {:.5f}(Std:{:.5f}) - F1: {:.5f} - L. LOSS: {:.5f} - {} {}'\n        print(msg.format(auc_mean,auc_std, f1_mean, lloss_mean, ntree_mean, elapsed))\n        print('='*80)\n        print('')\n\n        # Gerar o arquivo de submiss\u00e3o \n        name_file_sub = 'Data\/submission\/' + name_model + '_' + str(scaler).lower()[:4] + '.csv'\n        df_submission.to_csv(name_file_sub.format(auc_mean), index = False)\n\n        gc.collect()\n     \n    mdl_name_best = 'model\/' + name_model.format(auc_mean)\n    \n    jb.dump(model_best, mdl_name_best)\n    random       = str(np.random.rand(1)[0]).replace('.','')\n    #model_name_  = name_model + '_score_{:2.5f}_{}.pkl.z'#.format(auc_best, random)\n    \n    # save_data_model(model_        = model, \n    #                 model_name_   = model_name_, \n    #                 path_         = path, \n    #                 X_train_prob_ = X_train_prob, \n    #                 y_hat_test_   = df_submission['target'], \n    #                 score_        = auc_best, \n    #                 seed_         = seed, \n    #                 level_        = '1')\n    \n    print()\n    print('='*80)\n    print('Scaler Best: {}'.format(scaler_best))\n    print('AUC        : {:2.5f}'.format(auc_best))\n    print('F1-Score   : {:2.5f}'.format(f1_best))\n    print('L. Loss    : {:2.5f}'.format(lloss_best))\n    print('='*80)\n    print()\n            \n    gc.collect()  \n    \n    return model_best","c4bb9857":"%%time\n\nparams.update({'n_estimators': 1000})\n\nscalers = [None, \n           StandardScaler(), \n           RobustScaler(),           \n           QuantileTransformer(output_distribution='normal', random_state=0)]\n\nmodel_best = cross_val_model(model           = xgb.XGBClassifier(**params), \n                             X_train_        = X, \n                             y_train_        = y,\n                             X_test_         = X_test,                                            \n                             scalers         = scalers, \n                             name_model      = 'xgb_005_fe_{:2.5f}', \n                             FOLDS           = 5, \n                             seed            = seed, \n                             use_ntree_limit = True\n                             ) \n\ngc.collect()\nprint()","f6e05dcb":"feat_imp_best    = model_best.feature_importances_\nfeature_imp_     = feat_imp_best\ndf               = pd.DataFrame()\ndf[\"Feature\"]    = X.columns\ndf[\"Importance\"] = feature_imp_ \/ feature_imp_.sum()\n\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)","4522b782":"fig, ax = plt.subplots(figsize=(13, 70))\nbars    = ax.barh(df[\"Feature\"], \n                  df[\"Importance\"], \n                  height    = 0.4,\n                  color     = \"mediumorchid\", \n                  edgecolor = \"black\")\n\nax.set_title(\"Feature importances\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature name\", fontsize=20, labelpad=15)\n#ax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(df[\"Feature\"])\nax.set_yticklabels(df[\"Feature\"], fontsize=13)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\n\n# Adicionando r\u00f3tulos na parte superior\nax2 = ax.secondary_xaxis('top')\n#ax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=13)\nax2.tick_params(axis=\"x\", labelsize=15)\nax.margins(0.05, 0.01)\n\n# Inverter a dire\u00e7\u00e3o do eixo y \nplt.gca().invert_yaxis()","1410863d":"explainer = shap.Explainer(model_best)\nshap_values = explainer(X)","e41d0898":"shap.plots.waterfall(shap_values[0],max_display=110)","a48cc9ab":"shap.plots.force(shap_values[0])","dfe3295e":"shap.plots.beeswarm(shap_values)","ff1b750d":"shap.plots.beeswarm(shap_values, max_display=111)","026d734e":"shap.plots.bar(shap_values, max_display=111)","7963978f":"### 4.2.4.1. Feature Import Modelo","bc58855e":"<div class=\"alert alert-info\" role=\"alert\">    \n**`NOTA:`** <br>\nO gr\u00e1fico acima revela que as vari\u00e1veis f34, f43 e f8 tem alto impacto na previs\u00e3o do XGB.      \n<\/div>","d80112c9":"## 2.1. Step 01\nVamos fazer uma clusteriza\u00e7\u00e3o e gerar uma nova vari\u00e1vel, isso pode ajudar na identifica\u00e7\u00e3o de novos padr\u00f5es e melhor a predi\u00e7\u00e3o dos algoritmos, caso n\u00e3o surta efeito podemos remover a vari\u00e1vel.    ","f83c7fae":"<h1 div class='alert alert-success'><center> TPS-Set: Feature Engineering<\/center><\/h1>\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/26480\/logos\/header.png?t=2021-04-09-00-57-05)","2892be2b":"# <div class=\"alert alert-success\"> 5. Conclus\u00e3o <\/div>\n\n<div class=\"alert alert-info\" role=\"alert\">    \nNeste notebook criamos novas vari\u00e1veis utilizando a clusteriza\u00e7\u00e3o e vari\u00e1veis estat\u00edsticas, com a finalidade de ajudar os modelos a identificar padr\u00f5es no dados para melhor as previs\u00f5es, na valida\u00e7\u00e3o realizada com XGB obtivemos uma melhora, por\u00e9m nas submiss\u00f5es n\u00e3o tivemos melhora. <br>\n    \nAl\u00e9m da cria\u00e7\u00e3o de novas vari\u00e1vies, utilizamos o SHAP Values para avaliar o impacto das novas vari\u00e1veis e antigas nas previs\u00f5es do modelo XGB, podemos destacar que as vari\u00e1veis: **fe_mean, fe_max, fe_skew** que foram criadas tem impacto negativo nas predi\u00e7\u00f5es, assim como outras vari\u00e1veis puxam a predi\u00e7\u00e3o para baixo, esse talvez seja o favor de n\u00e3o temos melhoria nas nossas submiss\u00f5es, sendo assim, s\u00e3o vari\u00e1veis contidas a serem removidas do modelo. <br>\n    \nUm ponto importante que devemos destacar \u00e9 que ainda n\u00e3o fizemos os ajuste de parametros e estamos trabalhando apenas com um predito (XGB), sendo assim, essas vari\u00e1veis podem ter valor preditivos relevantes para outros classificadores, os quais vamos teste em outros notebooks.\n    \nNos pr\u00f3ximos notebooks vamos fazer o ajuste de parametros para os seguintes classificadores: \n- XGB\n- LGBM; \n- CatBoost; \n- Rede Neural; \n- KNN;\n- SVN; \n    \nA ideia \u00e9 criar uma stacking, sendo assim, vamos salvar todos os modelos.\n    \n<\/div>","92f83e96":"# <div class=\"alert alert-success\">  1. IMPORTA\u00c7\u00d5ES <\/div>","a51d2160":"## 1.1. Bibliotecas ","dff9234e":"## 1.2. Fun\u00e7\u00f5es","a27ca388":"<div class=\"alert alert-info\" role=\"alert\">\n    \n**`NOTA:`** <br>\n\nComo podemos obsevar acima, a m\u00e9dia de AUC na valida\u00e7\u00e3o cruzada foi de 0.73142 e nas submiss\u00f5es do kaggle foram os seguintes scores:\n\n- **`None`**: No treino obtever AUC  de 0.73179 e no kable AUC de 0.73818; <br>\n- **`RobustScaler`**: No treino obtever AUC  de 0.73139 e no kable AUC de 0.73534; <br>\n- **`QuatileTransfomer`**: No treino obtever AUC 0.73048 e no kable AUC de 0.73531 <br> \n\n    \n<\/div>","cc4fbada":"### 4.2.4.1. SHAP Values\nExplicar as previs\u00f5es do modelo usando SHAP.","d72911cb":"Vamos salvar os dataset, caso seja necess\u00e1rio refazer o processo podemos partir desse ponto. ","fe77481f":"# <div class=\"alert alert-success\"> 3. Split Train\/Test <\/div>","fad66b24":"<div class=\"alert alert-info\" role=\"alert\">\n    \n**`NOTA:`** <br>\n\nAcima podemos observar que algumas das vari\u00e1veis que criamos est\u00e3o entre as 25 vari\u00e1veis mais importantes para o modelo XGB, por\u00e9m n\u00e3o gosto muito desse jeito de analisar as vari\u00e1veis mais importantes, vamos utilizar o `Shap` que utilza v\u00e1rios m\u00e9todos diferentes para encontrar as vari\u00e1veis importantes. \n    \nSHAP (SHapley Additive exPlanations) \u00e9 uma abordagem te\u00f3rica de jogos para explicar a sa\u00edda de qualquer modelo de aprendizado de m\u00e1quina, ver o [artigos](https:\/\/github.com\/slundberg\/shap#citations) para detalhes e cita\u00e7\u00f5es.\n    \n<\/div>","28d089e0":"## 1.3. Carregar Dados","547e0971":"# <div class=\"alert alert-success\"> 4. Modelagem <\/div>","d3e1d8c6":"<div class=\"alert alert-info\" role=\"alert\">\n    \n**`NOTA:`** <br>\nVamos transforma a cluster em dammy.\n    \n    \n<\/div>\n","c84b51a9":"<div class=\"alert alert-info\" role=\"alert\">\n    \n**`NOTA:`** <br>\nO gr\u00e1fico acima mostra as vari\u00e1veis e suas contribui\u00e7\u00f5es na predi\u00e7\u00e3o do modelo, as vari\u00e1veis que empurram a previs\u00e3o para cima s\u00e3o mostradas em vermelho, e as que empurram o previs\u00e3o para baixo s\u00e3o mostradas em azul.<br>\nAs vari\u00e1veis que criamos que empurram a previs\u00e3o para baixo s\u00e3o: e_mean, fe_max, fe_skew, vamos visualzar a mesma explica\u00e7\u00e3o utilizando o gr\u00e1fico de for\u00e7a. \n<\/div>","2346b48a":"## 2.2. Step 02","dd247a4a":"<div class=\"alert alert-info\" role=\"alert\">\n    \n**`NOTA:`** <br>\n\nDe acordo com o gr\u00e1fico acima, podemos criar 5 clusteres nos datasets\n    \n    \n<\/div>\n","924e96d8":"<div class=\"alert alert-info\" role=\"alert\"> \n    \nVamos dar uma olhda no resumo das das vari\u00e1veis, o gr\u00e1fico abaixo classifica os recursos pela soma das magnitudes dos valores de SHAP em todas as amostras e usa os valores de SHAP para mostrar a distribui\u00e7\u00e3o dos impactos que cada vari\u00e1vel tem na predi\u00e7\u00e3o do modelo. A cor representa o valor da vari\u00e1vel (vermelho alto, azul baixo). \n    \n<\/div>","e5fea92b":"# <div class=\"alert alert-success\"> 2. Feature Engineering <\/div>","9013653d":"<div class=\"alert alert-info\" role=\"alert\">\n    \nTamb\u00e9m podemos apenas pegar o valor absoluto m\u00e9dio dos valores de SHAP para cada vari\u00e1vel para obter um gr\u00e1fico de barra padr\u00e3o:\n<\/div>","395ac82d":"## 4.1. Valida\u00e7\u00e3o Cruzada"}}