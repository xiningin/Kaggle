{"cell_type":{"1a114eec":"code","7cd5f327":"code","b5b8571c":"code","6e2761d2":"code","f1684ec2":"code","fa3b1e29":"code","34887774":"code","5dcf7a37":"code","ec0b9437":"code","aee118a7":"markdown","d8efca6a":"markdown","b4f38db9":"markdown"},"source":{"1a114eec":"# The imports...\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Data handling and analysis\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\n# Models\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","7cd5f327":"df=pd.read_csv('\/kaggle\/input\/performance-prediction\/summary.csv')\ndf.head()","b5b8571c":"df.describe()","6e2761d2":"df.isnull().sum()","f1684ec2":"# Code in this section from https:\/\/www.kaggle.com\/sachinsharma1123\/kernel439a1a3a5b\ndf['3PointPercent']=df['3PointPercent'].fillna(df['3PointPercent'].mean())","fa3b1e29":"# We do not need names, so we will drop the column.    \ndf=df.drop(['Name'],axis=1)","34887774":"# Split the data into targets and features \ny = df['Target']\nX = df.drop(['Target'],axis=1)","5dcf7a37":"# Use ANOVA to select best features\n# Since the data is small enough, we will check many models and features to be comprehensive.\n\nbestAcc = 0\nnumFeatures = 0\nKNN_size = 0\nlogistic = False\nSVM = False\ntree = False\nKNN = False\nnaive = False\n\n# Check different models and their accuracies \n# Loop 1 through 10, for 1-10 amount of ANOVA features\nfor i in range(1,13):\n        # Selection best i features        \n        fvalue_selector = SelectKBest(f_classif, k=i)\n        newX = fvalue_selector.fit_transform(X, y)\n        \n        X_train, X_test, y_train, y_test = train_test_split(newX, y, random_state=0, test_size=0.3)\n    \n        \n        # Check Logistic Regression model\n        logistic = LogisticRegression(max_iter = 10000)\n        logistic.fit(X_train, y_train)\n        prediction = logistic.predict(X_test)\n        score = accuracy_score(y_test, prediction)\n        if score > bestAcc:\n            bestAcc = score\n            logistic = True\n            SVM = False\n            tree = False\n            KNN = False\n            naive = False\n            numFeatures = i\n        \n        # Check KNN model\n        for j in range(1,10):\n            knn = KNeighborsClassifier(n_neighbors = j)\n            knn.fit(X_train, y_train)\n            prediction = knn.predict(X_test)\n            score = accuracy_score(y_test, prediction)\n            if score > bestAcc:\n                bestAcc = score\n                numFeatures = i\n                KNN_size = j\n                logistic = False\n                SVM = False\n                tree = False\n                KNN = True\n                naive = False\n                \n\n        # Check Naive Bayes\n        nb = GaussianNB()\n        prediction = nb.fit(X_train, y_train).predict(X_test)\n        score = accuracy_score(y_test, prediction)\n        if score > bestAcc:\n            bestAcc = score\n            logistic = False\n            SVM = False\n            tree = False\n            KNN = False\n            naive = True\n            numFeatures = i\n        \n        # Check SVM\n        sv = svm.SVC()\n        sv = sv.fit(X_train, y_train)\n        prediction = sv.predict(X_test)\n        score = accuracy_score(y_test, prediction)\n        if score > bestAcc:\n            bestAcc = score\n            logistic = False\n            SVM = True\n            tree = False\n            KNN = False\n            naive = False\n            numFeatures = i \n                \n                \nprint(\"The best accuracy was\", round(bestAcc, 5), \", using this many features:\", numFeatures)\n\n\nif logistic:\n    print(\"Logistic was the best model.\")\nelif SVM:\n    print(\"SVM was the best model.\")\nelif naive:\n    print(\"Naive Bayes was the best model.\")\nelse:\n    print(\"KNN was the best model\")","ec0b9437":"# See which features were important\n# Code modified from https:\/\/stackoverflow.com\/questions\/39839112\/the-easiest-way-for-getting-feature-names-after-running-selectkbest-in-scikit-le\n\nselector = SelectKBest(f_classif, k=numFeatures)\nselector.fit(X, y)\n# Get columns to keep and create new dataframe with those only\ncols = selector.get_support(indices=True)\ntopFeatures = X.iloc[:,cols]\n\nprint(\"The top 8 most important features were as follows\")\nlist(topFeatures.columns)","aee118a7":"#  Developing the Model and Finding the Best Solution","d8efca6a":"# Import and Look at Data","b4f38db9":"# Conclusion\n\nFirst off, a huge **THANK YOU** for taking the time to read my notebook.\n\nAfter checking various models with various features selected, I found that the ideal amount of features was 8 to predict if a player has been player for more than or less than 5 years. My model was able to predict with 0.7288 accuracy. \n\nThe most important features were\n* Games Played\n* Minutes Played\n* Field Goals Made\n* Free Throws Made\n* Free Throw Attemps\n* Offensive Rebounds\n* Rebounds\n\nThe first 2 make the most sense, as they would logically be the strongest correlation. I am not a basketball pro, so I do not know what field goals are, but free throws being correlated makes sense as many players would throw them, regardless of position, similar to the rebounds. I suspect if we had a feature of player positions, we could develop a model with higher accuracy as the stats likely vary heavily by position of the player. Ignoring positions groups the stats into one, making it more general."}}