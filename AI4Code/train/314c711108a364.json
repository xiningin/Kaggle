{"cell_type":{"cf8f647c":"code","0b5737e7":"code","0d5cf576":"code","1d2391f7":"code","d09fb4de":"code","95e7225d":"code","b5baf81f":"code","7b9be725":"code","2f940111":"code","b2e2ba55":"code","a191e808":"code","c4161ef1":"code","04b7f592":"code","9c0f07b5":"code","dffb4361":"code","5613feb1":"code","de9ac30e":"code","7d9f3f91":"code","fa2b6b2e":"code","7e5c6599":"code","30e259c6":"code","f37f1dba":"code","cd5c0890":"code","493e4491":"code","d0e60917":"code","934a76b2":"code","6bdc132d":"markdown","bfbcb625":"markdown","c9067339":"markdown","e47f580a":"markdown","1695683f":"markdown","1bada33e":"markdown","b05c1959":"markdown","c7ab9f21":"markdown","159f2910":"markdown","1cb75241":"markdown","b64e5ea3":"markdown","5fb89baf":"markdown","1114557c":"markdown","ac0e39a2":"markdown","95685a98":"markdown","cb85798a":"markdown","9f929f39":"markdown","968f7a36":"markdown","fe21bc61":"markdown","a937d729":"markdown","67ee079f":"markdown","e8e5942f":"markdown","c536fe71":"markdown"},"source":{"cf8f647c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0b5737e7":"# Data manipulation\nimport pandas as pd\nimport numpy as np\nimport re\n\n# Data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\n\n# preprocessing and resampling\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\n\n# metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\n\n# ML model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\nimport sklearn\n\nnp.warnings.filterwarnings('ignore')","0d5cf576":"df = pd.read_csv ('\/kaggle\/input\/company-bankruptcy-prediction\/data.csv')\ndf.rename(columns = {'Bankrupt?':'y'}, inplace = True)\n# rename columns\nlista_new_col = []\nfor col in df.columns:\n  col = re.sub(' ', '_', col)\n  col = re.sub('-', '_', col)\n  lista_new_col.append(col)\ndf.columns = lista_new_col\n\nprint ( 'Shape of dataset:', df.shape )\nprint ( '*' * 50 )\ndf.head()","1d2391f7":"df.isna().sum()","d09fb4de":"df.info()","95e7225d":"df.describe().T","b5baf81f":"print(df.y.value_counts())\nprint('-'* 30)\nprint('Financially stable: ', round(df.y.value_counts()[0]\/len(df) * 100, 2), '% of the dataset')\nprint('Financially unstable: ', round(df.y.value_counts()[1]\/len(df) * 100, 2), '% of the dataset')","7b9be725":"plt.figure(figsize = (7,4))\nsns.countplot(df.y)\nplt.title('Class Distributions Count \\n (0: Stable || 1: Unstable)', fontsize=12)\nplt.show()","2f940111":"cat_cols = df.select_dtypes(include=['object','category','int64']).columns\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7,4), constrained_layout=True)\nax=ax.flatten()\nfig.suptitle('\\nCount plot of categorical features\\n', size=12)\nfor x,i in enumerate(cat_cols[1:]):\n    sns.countplot(x=df[i], ax=ax[x])","b2e2ba55":"print ( 'The column \"Net income flag\" have just value:', df._Net_Income_Flag.unique() )\ndf.drop ('_Net_Income_Flag', axis=1, inplace=True )","a191e808":"# Looking at the histograms of numerical data\ndf.hist(figsize = (35,30), bins = 50 )\nplt.show()","c4161ef1":"# SMOTE\nX, y = df.iloc [ :, 1: ].values , df.iloc [:, 0].values\nX_train, X_test, y_train, y_test = train_test_split ( X, y,\n                                                     test_size = 0.3,\n                                                     random_state = 1,\n                                                     stratify = y)\nsmt = SMOTE ()\nX_train_sm, y_train_sm = smt.fit_resample (X_train, y_train)\n\nprint ('# class in y_train:', np.bincount (y_train_sm) )","04b7f592":"pipeline_lr = make_pipeline ( StandardScaler(),\n                          LogisticRegression ( penalty='none', C=1.0, solver='saga', random_state=24 )\n                          )\npipeline_lr.fit ( X_train_sm, y_train_sm )\nscores = cross_val_score ( estimator=pipeline_lr,\n                          X = X_train_sm,\n                          y = y_train_sm,\n                          cv=5,\n                          n_jobs = 2)\ny_pred_lr = pipeline_lr.predict (X_test)\nprint ( 'Accuracy train: %.3f' %pipeline_lr.score (X_train_sm, y_train_sm) )\nprint ( 'Accuracy cross-validation: %.3f' %scores.mean() )\nprint ( 'Accuracy test: %.3f' %pipeline_lr.score (X_test, y_test) )","9c0f07b5":"# learning curve\ntrain_sizes, train_scores, test_scores =\\\n                learning_curve(estimator=pipeline_lr,\n                               X=X_train_sm,\n                               y=y_train_sm,\n                               train_sizes=np.linspace(0.1, 1.0, 10),\n                               cv=10,\n                               n_jobs=2)\n\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)","dffb4361":"plt.plot(train_sizes, train_mean,\n         color='blue', marker='o',\n         markersize=5, label='Training accuracy')\n\nplt.fill_between(train_sizes,\n                 train_mean + train_std,\n                 train_mean - train_std,\n                 alpha=0.15, color='blue')\n\n\nplt.plot(train_sizes, test_mean,\n         color='green', linestyle='--',\n         marker='s', markersize=5,\n         label='Validation accuracy')\n\nplt.fill_between(train_sizes,\n                 test_mean + test_std,\n                 test_mean - test_std,\n                 alpha=0.15, color='green')\n\nplt.xlabel('Number of training examples', size=12)\nplt.ylabel('Accuracy', size=12)\nplt.legend(loc='upper left')\nplt.ylim([0.8, 1.03])\nplt.axvline(x=6670, color = 'red', linestyle = '--', alpha = 0.5)\nplt.axvline(x=7480, color = 'yellow', linestyle = '--', alpha = 0.7)\nplt.text(5750, 0.825, 'Underfitting', fontsize=12, color='white', bbox ={'facecolor':'grey', 'pad':2} )\nplt.text(7600, 0.96, 'Overfitting', fontsize=12, color='white',bbox ={'facecolor':'grey', 'pad':2} )\nplt.title ('Learning curve', size=14)\nplt.tight_layout()\nplt.grid(b=False)\nplt.show()","5613feb1":"pipeline_tree = make_pipeline ( StandardScaler(),\n                          DecisionTreeClassifier ( min_samples_split=4, random_state=42 )\n                          )\npipeline_tree.fit ( X_train_sm, y_train_sm )\nscores = cross_val_score ( estimator=pipeline_tree,\n                          X = X_train_sm,\n                          y = y_train_sm,\n                          cv=5,\n                          n_jobs = 2)\ny_pred_tree = pipeline_tree.predict (X_test)\nprint ( 'Accuracy train: %.3f' %pipeline_tree.score (X_train_sm, y_train_sm) )\nprint ( 'Accuracy cross-validation: %.3f' %scores.mean() )\nprint ( 'Accuracy test: %.3f' %pipeline_tree.score (X_test, y_test) )","de9ac30e":"# validation curve\nparam_range = [1,2,3,4,5,6,7,8,9,10]\n\ntrain_scores, test_scores = validation_curve(\n                estimator=pipeline_tree, \n                X=X_train, \n                y=y_train, \n                param_name='decisiontreeclassifier__min_samples_split', \n                param_range=param_range,\n                cv=5)\n\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)","7d9f3f91":"plt.plot(param_range, train_mean, \n         color='blue', marker='o', \n         markersize=5, label='Training accuracy')\n\nplt.fill_between(param_range, train_mean + train_std,\n                 train_mean - train_std, alpha=0.15,\n                 color='blue')\n\nplt.plot(param_range, test_mean, \n         color='green', linestyle='--', \n         marker='s', markersize=5, \n         label='Validation accuracy')\n\nplt.fill_between(param_range, \n                 test_mean + test_std,\n                 test_mean - test_std, \n                 alpha=0.15, color='green')\n\nplt.grid(b=False)\nplt.legend(loc='lower right')\nplt.xlabel('Parameter min_samples_split', size=12)\nplt.ylabel('Accuracy', size=12)\nplt.title ('Validation curve as a function of the regularization parameter\\n min_samples_split\\n', size=14)\nplt.ylim([0.8, 1.0])\nplt.axvline(x=4, color = 'red', linestyle = '--', alpha = 0.5)\nplt.text(4.5, 0.9, 'Best min_samples_split : 4', fontsize=12, color='white',bbox ={'facecolor':'grey', 'pad':2} )\nplt.tight_layout()\nplt.show()","fa2b6b2e":"pipeline_knn = make_pipeline ( StandardScaler(),\n                          KNeighborsClassifier (n_neighbors=5, weights='distance', p=2 )\n                          )\npipeline_knn.fit ( X_train_sm, y_train_sm )\nscores = cross_val_score ( estimator=pipeline_knn,\n                          X = X_train_sm,\n                          y = y_train_sm,\n                          cv=5,\n                          n_jobs = 2)\ny_pred_knn = pipeline_knn.predict (X_test)\nprint ( 'Accuracy train: %.3f' %pipeline_knn.score (X_train_sm, y_train_sm) )\nprint ( 'Accuracy cross-validation: %.3f' %scores.mean() )\nprint ( 'Accuracy test: %.3f' %pipeline_knn.score (X_test, y_test) )","7e5c6599":"pipeline_svm = make_pipeline ( StandardScaler(),\n                          SVC (random_state=1, C=1.0, kernel='rbf', gamma='scale')\n                          )\npipeline_svm.fit ( X_train_sm, y_train_sm )\nscores = cross_val_score ( estimator=pipeline_svm,\n                          X = X_train_sm,\n                          y = y_train_sm,\n                          cv=5,\n                          n_jobs = 2)\ny_pred_svm = pipeline_svm.predict (X_test)\nprint ( 'Accuracy train: %.3f' %pipeline_svm.score (X_train_sm, y_train_sm) )\nprint ( 'Accuracy cross-validation: %.3f' %scores.mean() )\nprint ( 'Accuracy test: %.3f' %pipeline_svm.score (X_test, y_test) )","30e259c6":"pipeline_rfc = make_pipeline ( StandardScaler(),\n                          RandomForestClassifier(criterion='entropy', random_state=1)\n                          )\npipeline_rfc.fit ( X_train_sm, y_train_sm )\nscores = cross_val_score ( estimator=pipeline_rfc,\n                          X = X_train_sm,\n                          y = y_train_sm,\n                          cv=5,\n                          n_jobs = 2)\ny_pred_rfc = pipeline_rfc.predict (X_test)\nprint ( 'Accuracy train: %.3f' %pipeline_rfc.score (X_train_sm, y_train_sm) )\nprint ( 'Accuracy cross-validation: %.3f' %scores.mean() )\nprint ( 'Accuracy test: %.3f' %pipeline_rfc.score (X_test, y_test) )","f37f1dba":"pred_list = [y_pred_lr, y_pred_tree, y_pred_knn, y_pred_rfc, y_pred_svm ]\nname_clf = [ 'Logistic Regression', 'Decision Tree', 'K-NN', 'Random Forest', 'Support vector machine' ]\nfor name, y_pred in zip(name_clf, pred_list ):\n  print (f'---> {name}')\n  print (f'Accuracy: %0.3f' %accuracy_score (y_test, y_pred))\n  print (f'Recall: %0.3f' %recall_score (y_test, y_pred))\n  print (f'F1 score: %0.3f' %f1_score (y_test, y_pred))\n  print ('')","cd5c0890":"# classification report before optimizing hyperparameters\nlabel = ['Stable', 'Unstable']\nprint(classification_report(y_test, y_pred_lr, target_names=label))","493e4491":"# pipeline\npipeline_lr = make_pipeline ( StandardScaler(),\n                          LogisticRegression ()\n                          )\n\n# range value C\nparam_range = [ 0.001, 0.01, 0.1, 1.0 ]\n# creo griglia parametri\ngrid_param = [ { 'logisticregression__C' : param_range,\n               'logisticregression__penalty' : ['l2', 'l1', 'none'],\n               'logisticregression__solver' : ['lbfgs', 'saga'] } ]\ngs = GridSearchCV ( estimator = pipeline_lr,\n                   param_grid = grid_param,\n                   scoring = 'recall',\n                   cv = 5,\n                   refit = True,\n                   n_jobs = 2\n                   )\n\ngs = gs.fit ( X_train_sm, y_train_sm )\n\nprint ( 'Best score: %.3f' %gs.best_score_ )\n\nprint ( 'Best hyperparameter:', gs.best_params_ )\n\ny_pred_gs = gs.predict (X_test)","d0e60917":"# classification report before optimizing hyperparameters\nlabel = ['Stable', 'Unstable']\nprint(classification_report(y_test, y_pred_gs, target_names=label))","934a76b2":"conf_matrix = confusion_matrix (  y_test, y_pred_gs )\n\n# plot\nfig, ax = plt.subplots(figsize=(5, 5))\nax.matshow(conf_matrix, cmap='CMRmap', alpha=0.7)\nfor i in range(conf_matrix.shape[0]):\n    for j in range(conf_matrix.shape[1]):\n        ax.text(x=j, y=i, s=conf_matrix[i, j], va='center', ha='center')\nplt.xlabel('Predicted label', size = 20)\nplt.ylabel('True label', size = 20)\nplt.tight_layout()\nplt.show()","6bdc132d":"We will see the performances on the SMOTE oversampled data (we call *X_train_sm* and y_train_sm). For this I decided to use a few different models classifier:\n\n---\n\n\n- Logistic Regression\n- Decision Tree Classifier\n- K-Nearest Neighbors\n- Support Vector Machine\n- Random Forest Classifier\n\n\n\n---\n\n","bfbcb625":"# Models","c9067339":"After the application of SMOTE the classes are rebalanced and both classes of target variable have the same number of 4619 observations.","e47f580a":"From the confusion matrix it is concluded that:\n\n- the optimized model commits 221 classification errors\n\n- deeming it more serious to make the mistake of classifying a company as stable in an unstable reality, our classifier does an excellent job because he makes only 15 mistakes (of 221\ntotals) of this type on the entire test dataset (consisting of 2046 examples).","1695683f":"## Decision Tree Classifier","1bada33e":"The classes are heavily skewed we need to solve this issue later, with algorithm SMOTE (**S**ynthetic **M**inority **O**versampling **TE**chnique).\n\nClass 1 represents 96.77% of the dataset, while class 2 only 3.23%.","b05c1959":"## Ensemble learning - Random Forest","c7ab9f21":"# Hyperparameter optimization","159f2910":"# Data analysis & visualization","1cb75241":"There is no missing data in the dataset.","b64e5ea3":"# <font color='Orange'> ML project <\/font>\n\n### Introduction:\n\nThe data were collected from the Taiwan Economic Journal for the years 1999 to 2009. Company bankruptcy was defined based on the business regulations of the Taiwan Stock Exchange.\n\n### Objective:\n\nThe goal of this notebook is various predictive algorms are available as far as we can predict which goals in order to see failure in the future.","5fb89baf":"Now, looking at the confusion matrix:","1114557c":"# Import and data cleansing","ac0e39a2":"Let's now apply the machine learning models with the training dataset resampled and balanced according to the target variable: therefore each class will have the same weight and will not be treated differently than the others.\n\nNB: I would like to clarify that the balancing of the classes is done in the training set, therefore in the training phase, and that a correct prediction is right to do it on unnoticed data, precisely the test set.","95685a98":"The feature 'Net income flag' always takes the value 1 for all observations, so it can be removed from the model.","cb85798a":"## Support vector machine","9f929f39":"As can be seen from the graph, the model performs well on both training and validation data if at least 6670 examples are submitted to it during training (as indicated by the dashed red vertical line).\n\nAlso note, as indicated by the yellow dashed vertical line, that the distance between accuracy in training and accuracy in validation widens with a dataset of more than 7480 examples: an indicator of an increasing level of overfitting.","968f7a36":"The results show that the model with the greatest accuracy is the Random Forest. However, in this case we are more concerned with minimizing the likelihood of not detecting companies that really are close to failure. This is why it is best to look at the results of *Recall*, for which it seems that the best decision is Logistic Regression.\n\nNow let's optimize its hyperparameters with a grid search.","fe21bc61":"## K-Nearest Neighbors","a937d729":"**RESUME:** Trained Classifiers Performance Report on Test Set:","67ee079f":"The dataset is composed of a combination of 6819 observations per each of our 96 features.","e8e5942f":"## Logistic Regression","c536fe71":"# Re-sampling train_set (SMOTE algorithm)"}}