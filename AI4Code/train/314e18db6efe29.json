{"cell_type":{"14af8fea":"code","56c26295":"code","98e83b74":"code","99ed7e39":"code","99affcdd":"code","7adf0db7":"code","2a645e86":"code","8cb8df93":"code","ef6a9e95":"code","a60309e9":"code","c39ff4f3":"code","1d10d8d4":"code","4948bcf5":"code","8badba18":"code","ef38efaf":"code","93d93458":"code","acad9d17":"code","edb93d9c":"code","3df138df":"markdown","d4533f2e":"markdown","b07af7ed":"markdown","640d9828":"markdown","65162764":"markdown","7ef4388b":"markdown","3e142bdf":"markdown"},"source":{"14af8fea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport warnings\nwarnings.filterwarnings('ignore')  # ignore all warnings\n\nimport sys, os, random\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nprint('Using Tensorflow version ', tf.__version__)\nimport cv2\nprint('Using OpenCV version ', cv2.__version__)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport glob\nfrom PIL import Image\n%matplotlib inline\n\nUSING_TF2 = (tf.__version__.startswith('2'))\n\nplt.style.use('seaborn')\nsns.set_style('darkgrid')\n\nfloat_formatter = lambda x: '%.4f' % x\nnp.set_printoptions(formatter={'float_kind':float_formatter})\nnp.set_printoptions(threshold=np.inf, suppress=True, precision=4, linewidth=110)\n\nseed = 123\nrandom.seed(seed)\nnp.random.seed(seed)\nif USING_TF2:\n    tf.random.set_seed(seed)\nelse:\n    tf.set_random_seed(seed)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        \n# based on output of 4 commented lines above\ntrain_folder_root = '\/kaggle\/input\/fruits\/fruits-360_dataset\/fruits-360\/Training'\ntest_folder_root = '\/kaggle\/input\/fruits\/fruits-360_dataset\/fruits-360\/Test'\n\n# Any results you write to the current directory are saved as output.","56c26295":"# a quick dict of category-id to category-name (category name is parent folder under which image is present)\nfruit_category_names = sorted(os.listdir(train_folder_root))\nfruit_categories = {}\nfor i, name in enumerate(fruit_category_names):\n    fruit_categories[i] = name\n\nNUM_CLASSES = len(fruit_categories.keys())\nprint('Detected %d classes' % NUM_CLASSES)","98e83b74":"def display_sample(sample_images, sample_labels, sample_predictions=None, num_rows=8, num_cols=8,\n                   plot_title=None, fig_size=None):\n    \"\"\" display a random selection of images & corresponding labels, optionally with predictions\n        The display is laid out in a grid of num_rows x num_col cells\n        If sample_predictions are provided, then each cell's title displays the prediction \n        (if it matches actual) or actual\/prediction if there is a mismatch\n    \"\"\"\n    from PIL import Image\n    import seaborn as sns\n    assert len(sample_images) == num_rows * num_cols\n\n    # a dict to help encode\/decode the labels\n    global fruit_categories\n    \n    with sns.axes_style(\"whitegrid\"):\n        sns.set_context(\"notebook\", font_scale=1.1)\n        sns.set_style({\"font.sans-serif\": [\"Verdana\", \"Arial\", \"Calibri\", \"DejaVu Sans\"]})\n\n        f, ax = plt.subplots(num_rows, num_cols, figsize=((14, 12) if fig_size is None else fig_size),\n            gridspec_kw={\"wspace\": 0.02, \"hspace\": 0.25}, squeeze=True)\n        #fig = ax[0].get_figure()\n        f.tight_layout()\n        f.subplots_adjust(top=0.93)\n\n        for r in range(num_rows):\n            for c in range(num_cols):\n                image_index = r * num_cols + c\n                ax[r, c].axis(\"off\")\n                # show selected image\n                pil_image = sample_images[image_index] #Image.fromarray(sample_images[image_index])\n                ax[r, c].imshow(pil_image)\n                \n                if sample_predictions is None:\n                    # show the actual labels in the cell title\n                    title = ax[r, c].set_title(\"%s\" % fruit_categories[sample_labels[image_index]])\n                else:\n                    # else check if prediction matches actual value\n                    true_label = sample_labels[image_index]\n                    pred_label = sample_predictions[image_index]\n                    prediction_matches_true = (sample_labels[image_index] == sample_predictions[image_index])\n                    if prediction_matches_true:\n                        # if actual == prediction, cell title is prediction shown in green font\n                        title = fruit_categories[true_label]\n                        title_color = 'g'\n                    else:\n                        # if actual != prediction, cell title is actua\/prediction in red font\n                        title = '%s\/%s' % (fruit_categories[true_label][:5], fruit_categories[pred_label][:5])\n                        title_color = 'r'\n                    # display cell title\n                    title = ax[r, c].set_title(title)\n                    plt.setp(title, color=title_color)\n        # set plot title, if one specified\n        if plot_title is not None:\n            f.suptitle(plot_title)\n\n        plt.show()\n        plt.close()","99ed7e39":"def show_plots(history, plot_title=None, fig_size=None):\n    \n    import seaborn as sns\n    \n    \"\"\" Useful function to view plot of loss values & accuracies across the various epochs\n        Works with the history object returned by the train_model(...) call \"\"\"\n    assert type(history) is dict\n\n    # NOTE: the history object should always have loss & acc (for training data), but MAY have\n    # val_loss & val_acc for validation data\n    loss_vals = history['loss']\n    val_loss_vals = history['val_loss'] if 'val_loss' in history.keys() else None\n    \n    # accuracy is an optional metric chosen by user\n    # NOTE: in Tensorflow 2.0, the keys are 'accuracy' and 'val_accuracy'!! Why Google?? Why!!??\n    acc_vals = history['acc'] if 'acc' in history.keys() else None\n    if acc_vals is None:\n        # try 'accuracy' key, as used by the Tensorflow 2.0 backend\n        acc_vals = history['accuracy'] if 'accuracy' in history.keys() else None\n        \n    assert acc_vals is not None, \"Something wrong! Cannot read 'acc' or 'accuracy' from history.keys()\"\n        \n    val_acc_vals = history['val_acc'] if 'val_acc' in history.keys() else None\n    if val_acc_vals is None:\n        # try 'val_accuracy' key, could be using Tensorflow 2.0 backend!\n        val_acc_vals = history['val_accuracy'] if 'val_accuracy' in history.keys() else None    \n        \n    assert val_acc_vals is not None, \"Something wrong! Cannot read 'val_acc' ot 'val_acuracy' from history.keys()\"\n        \n    epochs = range(1, len(history['loss']) + 1)\n    \n    col_count = 1 if ((acc_vals is None) and (val_acc_vals is None)) else 2\n    \n    with sns.axes_style(\"darkgrid\"):\n        sns.set_context(\"notebook\", font_scale=1.1)\n        sns.set_style({\"font.sans-serif\": [\"Verdana\", \"Arial\", \"Calibri\", \"DejaVu Sans\"]})\n\n        f, ax = plt.subplots(nrows=1, ncols=col_count, figsize=((16, 5) if fig_size is None else fig_size))\n    \n        # plot losses on ax[0]\n        #ax[0].plot(epochs, loss_vals, color='navy', marker='o', linestyle=' ', label='Training Loss')\n        ax[0].plot(epochs, loss_vals, label='Training Loss')\n        if val_loss_vals is not None:\n            #ax[0].plot(epochs, val_loss_vals, color='firebrick', marker='*', label='Validation Loss')\n            ax[0].plot(epochs, val_loss_vals, label='Validation Loss')\n            ax[0].set_title('Training & Validation Loss')\n            ax[0].legend(loc='best')\n        else:\n            ax[0].set_title('Training Loss')\n    \n        ax[0].set_xlabel('Epochs')\n        ax[0].set_ylabel('Loss')\n        ax[0].grid(True)\n    \n        # plot accuracies, if exist\n        if col_count == 2:\n            #acc_vals = history['acc']\n            #val_acc_vals = history['val_acc'] if 'val_acc' in history.keys() else None\n\n            #ax[1].plot(epochs, acc_vals, color='navy', marker='o', ls=' ', label='Training Accuracy')\n            ax[1].plot(epochs, acc_vals, label='Training Accuracy')\n            if val_acc_vals is not None:\n                #ax[1].plot(epochs, val_acc_vals, color='firebrick', marker='*', label='Validation Accuracy')\n                ax[1].plot(epochs, val_acc_vals, label='Validation Accuracy')\n                ax[1].set_title('Training & Validation Accuracy')\n                ax[1].legend(loc='best')\n            else:\n                ax[1].set_title('Training Accuracy')\n\n            ax[1].set_xlabel('Epochs')\n            ax[1].set_ylabel('Accuracy')\n            ax[1].grid(True)\n    \n        if plot_title is not None:\n            plt.suptitle(plot_title)\n    \n        plt.show()\n        plt.close()\n\n    # delete locals from heap before exiting (to save some memory!)\n    del loss_vals, epochs, acc_vals\n    if val_loss_vals is not None:\n        del val_loss_vals\n    if val_acc_vals is not None:\n        del val_acc_vals","99affcdd":"def save_keras_model(model, base_file_name, save_dir=os.path.join('.', 'model_states')):\n    \"\"\" save everything to one HDF5 file \"\"\"\n    \n    # save the model\n    if not base_file_name.lower().endswith('.h5'):\n        base_file_name = base_file_name + '.h5'\n\n    # base_file_name could be just a file name or complete path\n    if (len(os.path.dirname(base_file_name)) == 0):\n        # only file name specified e.g. kr_model.h5. We'll use save_dir to save\n        if not os.path.exists(save_dir):\n            # check if save_dir exists, else create it\n            try:\n                os.mkdir(save_dir)\n            except OSError as err:\n                print(\"Unable to create folder {} to save Keras model. Can't continue!\".format(save_dir))\n                raise err\n        model_save_path = os.path.join(save_dir, base_file_name)\n    else:\n        # user passed in complete path e.g. '.\/save_states\/kr_model.h5'\n        model_save_path = base_file_name\n        \n    #model_save_path = os.path.join(save_dir, base_file_name)\n    model.save(model_save_path)\n    print('Saved model to file %s' % model_save_path)\n    \ndef load_keras_model(base_file_name, save_dir=os.path.join('.', 'model_states'), \n                     custom_metrics_map=None, use_tf_keras_impl=True):\n                \n    \"\"\"load model from HDF5 file\"\"\"\n    if not base_file_name.lower().endswith('.h5'):\n        base_file_name = base_file_name + '.h5'\n        \n    # base_file_name could be just a file name or complete path\n    if (len(os.path.dirname(base_file_name)) == 0):\n        # only file name specified e.g. kr_model.h5\n        model_save_path = os.path.join(save_dir, base_file_name)\n    else:\n        # user passed in complete path e.g. '.\/save_states\/kr_model.h5'\n        model_save_path = base_file_name\n\n    if not os.path.exists(model_save_path):\n        raise IOError('Cannot find model state file at %s!' % model_save_path)\n        \n    # load the state\/weights etc.\n    if use_tf_keras_impl:\n        from tensorflow.keras.models import load_model \n    else:\n        from keras.models import load_model\n\n    # load the state\/weights etc. from .h5 file        \n    # @see: https:\/\/github.com\/keras-team\/keras\/issues\/3911\n    # useful when you have custom metrics\n    model = load_model(model_save_path, custom_objects=custom_metrics_map)\n    print('Loaded Keras model from %s' % model_save_path)\n    return model","7adf0db7":"# hyper-parameters for training\nIMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS = 100, 100, 3\nLR_START, LR_WEIGHT_DECAY = 0.001, 1e-4\nNUM_EPOCHS, BATCH_SIZE = 30, 128\nMODEL_SAVE_DIR = '.\/model_states'","2a645e86":"# we will use Keras' ImageDataGenerator\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# train_datagen = ImageDataGenerator(rescale=1.0\/255,\n#     rotation_range=20,\n# \tzoom_range=0.05,\n# \twidth_shift_range=0.05,\n# \theight_shift_range=0.05,\n# \tshear_range=0.05,\n# \thorizontal_flip=True,\n# \tfill_mode=\"nearest\")\n\ntrain_datagen = ImageDataGenerator(rescale=1.0\/255)\n# I am splitting test dataset into cross-val & test datasets in 70:30 ratio \n# (since both should ideally be drawn from same population)\ntest_datagen = ImageDataGenerator(rescale=1.0\/255, validation_split=0.3)\n\n# flow from train root folders\ntrain_generator = train_datagen.flow_from_directory(\n    train_folder_root,\n    target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),  # all images will be resizes to this size\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    class_mode='categorical')\n\neval_generator = test_datagen.flow_from_directory(\n    train_folder_root,\n    target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),  # all images will be resizes to this size\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    class_mode='categorical',\n    subset='training')\n\ntest_generator = test_datagen.flow_from_directory(\n    test_folder_root,\n    target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),  # all images will be resizes to this size\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    class_mode='categorical',\n    subset='validation')\n\n# calculate training\/cross-val and testing steps\ntrain_steps = train_generator.n \/\/ BATCH_SIZE\neval_steps = eval_generator.n \/\/ BATCH_SIZE\ntest_steps = test_generator.n \/\/ BATCH_SIZE\n\nprint('Training set size %d, step-size %d' % (train_generator.n, train_steps))\nprint('Cross-val set size %d, step-size %d' % (eval_generator.n, eval_steps))\nprint('Test set size %d, step-size %d' % (test_generator.n, test_steps))","8cb8df93":"# display a sample from the training dataset\nimages, labels = train_generator.next()\nlabels = np.argmax(labels, axis=1)\ndisplay_sample(images[:64], labels[:64], num_rows=8, num_cols=8, \n               plot_title='Sample Training Data', fig_size=(16,20))","ef6a9e95":"from tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras import backend as K\n\nK.clear_session()  # tf.reset_default_graph()\n\n# NOTE: will download the weights for imagenet\nvgg16_base = VGG16(\n    weights='imagenet',    # use weights for ImageNet\n    include_top=False,     # don't use upper Dense layers\n    input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS))\nprint(vgg16_base.summary())","a60309e9":"def build_model_vgg16(use_l2_loss=True):\n    from tensorflow.keras.optimizers import Adam, SGD\n    from tensorflow.keras.regularizers import l2\n    \n    l2_loss_lambda = 0.00002  # just a wee-bit :)\n    l2_loss = l2(l2_loss_lambda) if use_l2_loss else None\n    if l2_loss is not None: print('Using l2_loss_lambda = %f' % l2_loss_lambda)\n        \n    model = tf.keras.models.Sequential([\n        # our vgg16_base model added as a layer\n        vgg16_base,\n        # here is our custom prediction layer (same as before) \n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dropout(0.30),\n        tf.keras.layers.Dense(2048, activation='relu', kernel_regularizer=l2_loss),\n        tf.keras.layers.Dropout(0.10),      \n        tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=l2_loss),\n        tf.keras.layers.Dropout(0.10),        \n        tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=l2_loss),\n        tf.keras.layers.Dropout(0.10),         \n        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')    \n    ])\n    \n    # mark vgg16-base layer as non-trainable, so training updates\n    # weights and biases of just our newly added layers\n    vgg16_base.trainable = False\n    \n    adam = Adam(lr=LR_START)\n    sgd = SGD(lr=1e-1, momentum=0.9, decay=1e-1 \/ NUM_EPOCHS)\n    \n    model.compile(optimizer=sgd, \n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model","c39ff4f3":"try:\n    del model\nexcept NameError:\n    pass # model is not defined!\n\nmodel = build_model_vgg16()\nprint(model.summary())","1d10d8d4":"# train the model\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n\ndef lr_step(epoch, curr_lr):\n    power = (epoch % 5) # [0-19] = 0, [20-39] = 1, [40-59] = 2...\n    new_lr = LR_START * 1.0 \/ (1 + 0.01 * power)\n    return new_lr\n\nlr_callback = LearningRateScheduler(lr_step, verbose=1)\n\n# train model on generator\nhist = model.fit_generator(\n    train_generator,\n    steps_per_epoch=train_steps,\n    epochs=NUM_EPOCHS,\n    validation_data=eval_generator,\n    validation_steps=eval_steps,\n    workers=3,\n    #callbacks=[lr_callback]\n)","4948bcf5":"show_plots(hist.history, \"Keras VGG19 Model\")","8badba18":"# evaluate performance on train, cross-val & test datasets\nloss, acc = model.evaluate_generator(train_generator, steps=train_steps, verbose=1, workers=3)\nprint('Training data  -> loss: %.3f, acc: %.3f' % (loss, acc))\nloss, acc = model.evaluate_generator(eval_generator, steps=eval_steps, verbose=1, workers=3)\nprint('Cross-val data -> loss: %.3f, acc: %.3f' % (loss, acc))\nloss, acc = model.evaluate_generator(test_generator, steps=test_steps, verbose=1, workers=3)\nprint('Testing data   -> loss: %.3f, acc: %.3f' % (loss, acc))","ef38efaf":"save_keras_model(model, \"kr_fruits360_vgg16\", MODEL_SAVE_DIR)\ndel model","93d93458":"model = load_keras_model(\"kr_fruits360_vgg16\", MODEL_SAVE_DIR)\nprint(model.summary())","acad9d17":"print('Running predictions...')\n\ntest_generator = test_datagen.flow_from_directory(\n    test_folder_root,\n    target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),  # all images will be resizes to this size\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    class_mode='categorical',\n    subset='validation')\n\nall_predictions, all_labels = [], []\nfor i in range(test_steps):\n    images, labels = test_generator.next()\n    labels = np.argmax(labels, axis=1)\n    y_pred = np.argmax(model.predict(images), axis=1)\n    all_predictions.extend(y_pred.astype('int32'))\n    all_labels.extend(labels.astype('int32'))\n\nall_labels = np.array(all_labels)\nall_predictions = np.array(all_predictions)\n\nprint('First 25 results:')\nprint('  - Actuals    : ', all_labels[:25])\nprint('  - Predictions: ', all_predictions[:25])\ncorrect_pred_count = (all_labels == all_predictions).sum()\ntest_acc = correct_pred_count \/ len(all_labels)\nprint('We got %d of %d correct (or %.3f accuracy)' % (correct_pred_count, len(all_labels), test_acc))","edb93d9c":"# display sample predictions\nimages, labels = test_generator.next()\nlabels = np.argmax(labels, axis=1)\npredictions = np.argmax(model.predict(images), axis=1)\ndisplay_sample(images[:64], labels[:64], sample_predictions=predictions[:64], num_rows=8, num_cols=8, \n               plot_title='Sample Predictions on Test Dataset', fig_size=(18,20))","3df138df":"### Fruits360 - Multiclass classification of fruits &amp; vegitable images (using Keras)\n* This workbook illustrates my attempt to solve the [Kaggle Fruits360 challenge](https:\/\/www.kaggle.com\/moltean\/fruits).\n* I'll be using Keras on Tensorflow (actually Tensorflow's implementation of Keras, available in `tf.keras` package).\n* I will be using the VGG16 pre-trained model, which is a complex CNN trained on the ImageNet image set.\n\n<font color='yellow'>It is recommended that you run this notebook on a GPU only, like ones available on Google Colab or Kaggle<\/font>","d4533f2e":"**Observations:**\n\n>Configuration | Training Acc | Cross-Val Acc | Testing Acc \n>:---|:---:|:---:|:---:|\n>**VGG16 Base Model**|100%|100%|99.1%|\n\n* From the loss & accuracy plots, we see that the model trails the validation metrics for almost all epochs before 'catching up'.\n* We appear to have achieved the impossible :)! A near perfect model - with 100% training accuracy & almost 100% test accuracy.\n\n# <center> -- END -- <\/center>","b07af7ed":"### Utility Functions\nSome utility functions we will use in this workbook","640d9828":"Wow! Is that a perfect classifier ??!!\n* We got training & cross-validation accuracies = 100%\n* Training accuracy is 99% (almost 100%)\n\nPlease pinch me!!","65162764":"### Running Predictions from the trained model\nThe training epochs learnt weights of the Dense layers, which we inserted atop the base VGG16 convolutional ","7ef4388b":"### Training our Model\nWe will use ImageDataGenerator to read images off disk & a pre-trained VGG16 model for classification.","3e142bdf":"## Using the VGG16 pre-trained model\nIn this section we will use the VGG16 model's Convolutional part & discard the Dense part (predictor). We will add our custom 'predictor' part atop the convolutional layers"}}