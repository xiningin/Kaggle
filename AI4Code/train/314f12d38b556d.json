{"cell_type":{"f98b0f5e":"code","0733e855":"code","0246032f":"code","bad1fab4":"code","f9cbe76d":"code","69f7c230":"code","5a9f23f8":"code","b1dfeef8":"code","4280088a":"code","5761afc6":"code","3b5639b0":"code","9c11c39c":"code","2b2fbbd1":"code","59287ea6":"code","8a0e7be9":"code","0bbc7129":"code","8aeae6fc":"code","884dccc8":"code","5e0c2062":"code","974c0bdd":"code","c7cd51f1":"code","5e3556cb":"code","7e1d2857":"code","821def03":"code","d9554c00":"code","909f67a5":"code","0eb1a40b":"markdown","b3be05f2":"markdown","6024499a":"markdown","9b2fe767":"markdown","eb2d1afc":"markdown","262eb195":"markdown","82f39573":"markdown","955d2b09":"markdown","0683dfde":"markdown","54c762ef":"markdown","8827714a":"markdown","e29cb502":"markdown","37fe40eb":"markdown","febe55b6":"markdown","6199f9ce":"markdown","afdc2aa5":"markdown","50cc9bd3":"markdown","d524c185":"markdown","f5b8fe74":"markdown","8ba5ef64":"markdown","a25781b1":"markdown","7562809f":"markdown","244e424f":"markdown","9b13cab6":"markdown","02fc2fc8":"markdown","5be2e849":"markdown","c38f7b47":"markdown","50a74b98":"markdown","e9d30609":"markdown","21c7768d":"markdown","89703f6c":"markdown","fbcd5a64":"markdown","b3f2a5ac":"markdown","8ade3e1e":"markdown","d8080ebb":"markdown","09b82565":"markdown","3bd6be0e":"markdown","6f414bdb":"markdown","db6a09fd":"markdown","e2b4ca0a":"markdown","bcb8c4b2":"markdown","e927afbe":"markdown","bc036b58":"markdown","dd9bb8b5":"markdown","6e1aedb6":"markdown","233c908f":"markdown","0039c528":"markdown"},"source":{"f98b0f5e":"# data loading and processing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy.stats as sci #using the pearson\n\n# data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\n# model training and machine learning\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score # evaluate the model\n\n# file operation\nimport os\nprint(os.listdir(\"..\/input\"))\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0733e855":"# read the data and store data in DataFrame titled Mall_data\nMall_data = pd.read_csv(r\"..\/input\/Mall_Customers.csv\")\n# print a summary of the data in Melbourne data\nMall_data.info()","0246032f":"# Peak on the data\nMall_data.head()","bad1fab4":"#plotting with countplot\nplt.subplot(1,2,1)\nsns.countplot(x='Gender', data=Mall_data)\nplt.title('Customer Distribution Of Gender')\n\n#plotting with pie\nplt.subplot(1,2,2)\nexplode = [0, 0.1]\n\nplt.rcParams['figure.figsize'] = (9, 9)\nplt.pie(x=Mall_data['Gender'].value_counts(),\n        explode = explode, autopct = '%.2f%%')\nplt.title('Gender Ratio', fontsize = 20)\nplt.legend(labels = ['Female', 'Male'], loc='best')\nplt.show()","f9cbe76d":"Gender_Score = Mall_data.groupby('Gender')['Spending Score (1-100)'].agg([np.mean, max, min])\nGender_Score","69f7c230":"#Creating the figure\nfigure = plt.figure(figsize=(15,6), dpi=150)\n\n#Single Variable\nFeatures = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n\nn=0\nfor i in Features:\n    n += 1\n    plt.subplot(1, 3, n)\n    plt.subplots_adjust(wspace=1, right=1)\n    sns.distplot(Mall_data[i], rug=True) #rug setting the observation strip;find out data's concentration\n    plt.title(\"Distribution Of {}\".format(i))","5a9f23f8":"sns.swarmplot(data=Mall_data, x='Spending Score (1-100)', y='Gender')","b1dfeef8":"sns.boxenplot(data=Mall_data, x='Gender', y='Spending Score (1-100)')","4280088a":"sns.jointplot(x='Age', y='Annual Income (k$)', data=Mall_data, kind='hex', stat_func=sci.pearsonr, ratio=5)","5761afc6":"sns.jointplot(x='Annual Income (k$)', y='Spending Score (1-100)', \n              data=Mall_data, stat_func=sci.pearsonr, kind='scatter')","3b5639b0":"sns.pairplot(data=Mall_data, vars=['Age', 'Annual Income (k$)', 'Spending Score (1-100)'], \\\n             hue='Gender', kind='reg', diag_kind='kde', markers=['*','.'], size=5, palette='husl')","9c11c39c":"X1_Matrix = Mall_data.iloc[:, [2,4]].values # Age & Spending Score\nX2_Matrix = Mall_data.iloc[:, [3,4]].values # Annual Income & Spending Score","2b2fbbd1":"inertias_1 = []\nfor i in range(1,8):\n    kmeans = KMeans(n_clusters=i, init='k-means++',  max_iter=300, n_init=10,\n                   random_state=0)\n    kmeans.fit(X1_Matrix)\n    inertia = kmeans.inertia_\n    inertias_1.append(inertia)\n    print('For n_cluster =', i, 'The inertia is:', inertia)","59287ea6":"# Creating the figure\nfigure = plt.figure(1, figsize=(15,6), dpi=80)\n\nplt.plot(np.arange(1,8), inertias_1, alpha=0.8, marker='o')","8a0e7be9":"Kmeans = KMeans(n_clusters=5, init='k-means++',  max_iter=300, n_init=10,\n                   random_state=0)\nlabels = Kmeans.fit_predict(X1_Matrix)","0bbc7129":"centroids1 = Kmeans.cluster_centers_ # the centroid points in each cluster","8aeae6fc":"# Visualizing the 5 clusters\nplt.scatter(x=X1_Matrix[labels==0, 0], y=X1_Matrix[labels==0, 1], s=120, c='red', label='Potential and should be stumulated')\nplt.scatter(x=X1_Matrix[labels==1, 0], y=X1_Matrix[labels==1, 1], s=120, c='blue', label='Premium and should be retained')\nplt.scatter(x=X1_Matrix[labels==2, 0], y=X1_Matrix[labels==2, 1], s=120, c='grey', label='Potential and should be treated carefully')\nplt.scatter(x=X1_Matrix[labels==3, 0], y=X1_Matrix[labels==3, 1], s=120, c='orange', label='better than normal and should be paid more attention')\nplt.scatter(x=X1_Matrix[labels==4, 0], y=X1_Matrix[labels==4, 1], s=120, c='green', label='normal and should be ovserved for a while')\n\n#Visualizing every centroids in different cluster.\nplt.scatter(x=centroids1[:,0], y=centroids1[:,1], s=300, alpha=0.8, c='yellow', label='Centroids')\n\n#Style Setting\nplt.title(\"Cluster Of Customers\", fontsize=20)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Spending Score (1-100)\")\nplt.legend(loc=0)","884dccc8":"pd.Series(labels).value_counts()","5e0c2062":"inertias_2 = []\nfor i in range(1,8):\n    kmeans = KMeans(n_clusters=i, init='k-means++',  max_iter=300, n_init=10,\n                   random_state=1)\n    kmeans.fit(X2_Matrix)\n    inertia = kmeans.inertia_\n    inertias_2.append(inertia)\n    print('For n_cluster =', i, 'The inertia is:', inertia)","974c0bdd":"# Creating the figure\nfigure = plt.figure(1, figsize=(15,6), dpi=80)\n\nplt.plot(np.arange(1,8), inertias_2, alpha=0.8, marker='o')","c7cd51f1":"Kmeans = KMeans(n_clusters=5, init='k-means++',  max_iter=300, n_init=10,\n                   random_state=1)\nlabels = Kmeans.fit_predict(X2_Matrix)","5e3556cb":"centroids2 = Kmeans.cluster_centers_ # the centroid points in each cluster","7e1d2857":"# Visualizing the 5 clusters\nplt.scatter(x=X2_Matrix[labels==0, 0], y=X2_Matrix[labels==0, 1], s=120, c='red', label='normal income and hedonism')\nplt.scatter(x=X2_Matrix[labels==1, 0], y=X2_Matrix[labels==1, 1], s=120, c='blue', label='high income and frugalism')\nplt.scatter(x=X2_Matrix[labels==2, 0], y=X2_Matrix[labels==2, 1], s=120, c='grey', label='medium income and pragmatism')\nplt.scatter(x=X2_Matrix[labels==3, 0], y=X2_Matrix[labels==3, 1], s=120, c='orange', label='high income and hedonism')\nplt.scatter(x=X2_Matrix[labels==4, 0], y=X2_Matrix[labels==4, 1], s=120, c='green', label='normal income and frugalism')\n\n#Visualizing every centroids in different cluster.\nplt.scatter(x=centroids2[:,0], y=centroids2[:,1], s=300, alpha=0.8, c='yellow', label='Centroids')\n\n#Style Setting\nplt.title(\"Cluster Of Customers\", fontsize=20)\nplt.xlabel(\"Annual Income (k$)\")\nplt.ylabel(\"Spending Score (1-100)\")\nplt.legend(loc=7)","821def03":"for n_clusters in range(3,8):\n    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n    kmeans.fit(X2_Matrix)\n    clusters = kmeans.predict(X2_Matrix)\n    silhouette_avg = silhouette_score(X2_Matrix, clusters)\n    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)","d9554c00":"n_clusters = 5\nkmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\nkmeans.fit(X2_Matrix)\nclusters = kmeans.predict(X2_Matrix)\nsilhouette_avg = silhouette_score(X2_Matrix, clusters)\nprint(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)","909f67a5":"pd.Series(clusters).value_counts()","0eb1a40b":"#### It's so nice to get some opinions,please feel free to tell me.And if you like my work, please upvote it.~","b3be05f2":"Since there was no comparability while i merely found that the Customers came to the mall tended to be a lady rather than a gentleman.I decided to Add the variables `Spending Score (1-100)` to study on:\n1. Will the Male have more Consumption Power than the Female even if they have poorer numbers?","6024499a":"The goal of KMeans is to ensure that \u201cthe differences within the cluster are small as possible and the differences outside the cluster are large as possible\u201d, and we can measure the effect of clustering by measuring intra-cluster differences. As we said earlier, Inertia is a measure of the difference within a cluster using distance. So, can we use Inertia as a measure of clustering?\n\n1. Firstly, the smaller inertia is the better, but it is hard to recognize if a smaller inertia has reached **the limit** of the model and can continue to improve.\n2. Secondly, its calculation is **too easy to receive the influence of the number of features**. When the data dimension is large, the calculation amount of inertia will fall into the curse of the dimension, and **the calculation will be unimaginably huge**, which is not suitable for evaluating the model.","9b2fe767":"Finally, i check how many samples in each cluster:","eb2d1afc":"Since unsupervised algorithms only require the feature matrix X during training and do not require tags.","262eb195":"* **Age & Annual Income & Spending Score (1-100)**","82f39573":"* **Spending Score (1-100) & Gender**","955d2b09":"From the above visualization, it can be known that the relationship between different variables doesn't show particular strong hint.So, in the next step, i consider try using different features combination to do the clustering.","0683dfde":"From above comparison, we can know:\n* One of ladies get the highest score that is 99 and one of gentlemens get the highes score which is 97.\n* Even Male members accounts for 44% among the all but they still get an average score of around 48,which is only 3 scores lesser than the Female's.It shows a neck and neck Comsumption Power in men group.","54c762ef":"Next i will visualize the cluster and make recomendations on sales strategy by renaming the cluster name.","8827714a":"### Features Selecting","e29cb502":"### - Evaluating By Inertia","37fe40eb":"The inertia is getting lower with the increase of variable `n_clusters`.As we said, it's hard to select the optimal `k` for our model.Then, i choose to set the `n_clusters=5` and check the result.","febe55b6":"Before getting hands on the dataset, We should define the question on it so that we can know what should we deal with it and how to qualify or disqulify potential solutions to the specific problem. Obviously, this dataset is base on an actual Business Scenario of **HOW TO ORIENTATE THE CUSTOMER GROUP AND MAKE BETTER SEGMENTATION FOR THE SAKE OF ACCUATE SALES STRATEGY.**  \nThe questions we are going to find out are :\n1. What's the Gender\/age\/Annual Income\/Spending Score distribution of Customers in this mall\uff1f\n2. Is there any specific groups showing higher value to the mall?If it is, what do you suggest?\n3. Does the customer who gets high level of income means that he\/she spend more in the mall?\n4. How to make Customer Segmentation according to their performance on age & annual income & spending score?\n5. If using K-means, how to evaluate the accuracy of the final clustering result?\n    * Inertia\n    * Silhouette score\n","6199f9ce":"### Clusting and Evaluating","afdc2aa5":"## Libraries Importing","50cc9bd3":"**S = b-a\/max(a,b) >>> S = 1-a\/b,if a<b  |  S=0,if a=b   | S=b\/a-1 if a>b**","d524c185":"### Bivariate Plotting","f5b8fe74":"Obviously, it is a tiny size for a DataFrame with `200 entries and  5 columns`. Since there is no any `NaN` Values or abnormal objects that should be noticed, I can save a lot work on data cleaning or any preprocessing works. Let's just start with and data visualization and build the cluster models after having a qucik look on the dataset.","8ba5ef64":"It is easy to understand that the range of **Silhouette Score is (-1,1)**.If the value is closer to 1, It means that the sample is very similar to the sample within the same cluster, but is not similar to the sample points where it is outside the cluster.\n\nWhen the samples are similar to the samples which is not in the same cluster, the Silhouette Score wile be negative. \n\nWhen the Silhouette Score is about to 0, it means that the similarity of the samples in the two clusters is the same, and it could be said the two clusters should be the one cluster.","a25781b1":"* Annual Income (k$) & Spending Score (1-100)","7562809f":"* **Age & Annual Income (k$)**","244e424f":"### - Evaluating By Silhouette Score ","9b13cab6":"Both the Distribution seems not regular.I decided to observe under putting two variables together with pairplot","02fc2fc8":"## Data Overview","5be2e849":"* **Gender**","c38f7b47":"[About pearsonr & p](http:\/\/docs.scipy.org\/doc\/scipy-0.14.0\/reference\/generated\/scipy.stats.pearsonr.html)","50a74b98":"From the clusting, i find that the cunsumper expenditures vary by age.Nevertheless, Young guys have a great consumption power nowadays.The clusters of **blue** is the largest cluster,which should be treated closely and well retained.Besides, Both the clusters of **orange and red** have the same sample and different from the age group.The peple in **orange** group widely spread in age spend relatively lower in this segmentation.But the dataset didn't mention about the record time and period.So i suggest to oveserve for a while and poll out some survey to understand why the score are so low in this group.The group of **grey** are the lowest throng and the normal number group green have slightly higher than the grey group.","e9d30609":"In the forum, i noticed a kaggler said about the data should be standardlized before inputting into k-means.Actually there is no argument that we should do the data preprocessing before using the algorithms.But if we are take more look at the dataset itself, we can notice that Both the `Age`,`Spending Score (1-100)` and `Annual Income(k$)` are almost on a scale of 1 to 100.","21c7768d":"**Two situation about the Silhouette Score:**\n\nIf most of the samples in a cluster have relatively **high** Silhouette Score, then the clusters will have a high Silhouette Score,which results in higher mean Silhouette Score.And this the clustering is reasonable. \n\nIf many sample points have relatively **low** Silhouette Score or even show negative values.It means that the clustering is not appropriate, and the cluster's hyperparameter `K may be set too large or too small`.","89703f6c":"#### 1.Depending on Age & Spending Score","fbcd5a64":"# Defining the question","b3f2a5ac":"## Data Visualization","8ade3e1e":"### Mutilvariate Plotting","d8080ebb":"Since the pearsonr is closed to O, which means they have poor corelation.","09b82565":"## Clusting by K-means","3bd6be0e":"It's easy to find that `Annual Income(K$)` and `Spending Score(1-100)` are the key features to analyse and make clusting on Customer groups. But we also should not ignore the potential relevance that these two elements may depend on the features `Age`&`Gender`. Hence i attempt to observe the plotting based on Single features and then try to make comparison upon Bivarite plotting.","6f414bdb":"### About data preprocessing","db6a09fd":"The core task of KMeans is to find the `K` optimal centroids according to our K set, and assign the data closest to these centroids to the clusters represented by these centroids. The specific process can be summarized as follows:\n1. Extracting No. of k samples as the initial centroid randomly.\n2. Start to loop.\n3. Assign each sample point to the centroid closest to them, generating k clusters.\n4. For each cluster, calculating the average of all sample points assigned to the cluster as the new centroid.\n5. When the position of the centroid no longer changes, the iteration stops and the clustering is completed.","e2b4ca0a":"Obviously, the average Silhouette Score is 0.55 for n_cluster=5, which is more close to the 1.Compared with using `inertia` to find the best `K`, Is this not a better way to identify how many clusters should we make?","bcb8c4b2":"From the boxenplot, An extremely red dots is on the top of the Female box, which is pretty close to the 100 score line.And it also can be seen that both the group have the same median,which is 50.The majority of Female group get better score than the male's.It suggests us that female is still the main contributors for a shopping mall.","e927afbe":"According to the result shown above, all of the group have the apparent Segmentation.I classified into 5 clusters and define them based on the `Annual Income & Spending Score` of different level(**Based on income,i would describe like normal\/medium\/high income.Based on Spending Score, i would describe like the hedonism, frugalism and pragmatism)**.\n\nNo matter what I suggest that the customers should not be ignored simply because they spend less on the mall or they don't have a relatively high level of income.We should take different kinds of meseaure(like promotion on some specific commodity or discount) to stumalate the consumers and make them buy things at pleasure.And all the measure can be taken before taking consideration on recording whay types of commodity they bought in the past.","bc036b58":"# Data exploration","dd9bb8b5":"### Univariate Plotting","6e1aedb6":"KDE(Kernel density estimation) is used to estimate unknown density functions in probability theory.\nThrough plotting with the displot, the distribution characteristics of the data samples themselves can be seen relatively intuitive.","233c908f":"The last section i use the innertia to evaluate how many clusters should be divided for the best and dicuss about why it's not the best choice to use inertia.So, why we prefer to use Silhouette Score?\n\nIt can measure at the same time:\n1. The similarity a of the sample to other samples in the cluster in which it is located is equal to the average distance between the sample and all other points in the same cluster\n2. The similarity b between the sample and the samples in other clusters is equal to the average distance between the sample and all the points in the next nearest cluster. According to the requirements of the cluster, the difference within the cluster should be small, and the difference outside the cluster should be large. b is always greater than a, and the more you grow, the better.","0039c528":"#### 2.Depending on Annual Income & Spending Score"}}