{"cell_type":{"c8cdb2f4":"code","16cf08ce":"code","9d687a53":"code","72c20a4e":"code","dccf3558":"code","76cce78a":"code","73917da8":"code","d9c8a74f":"code","4cd1be9f":"code","10f330b3":"code","95a69450":"code","441eb05a":"code","5b0b8ba8":"code","eb2d95df":"code","51740765":"markdown","a4f90abc":"markdown","d0592af1":"markdown","70e0f7b8":"markdown","c8a81d4a":"markdown","908b94b8":"markdown","6698398f":"markdown","72699e94":"markdown","20519a24":"markdown","e4d4e8f0":"markdown","9e9136cb":"markdown","62a99126":"markdown","5d23e23a":"markdown","e6678114":"markdown","1b9ebb00":"markdown","ee4d3292":"markdown","c4118de8":"markdown","34632a4a":"markdown","427ec608":"markdown","3084b66d":"markdown"},"source":{"c8cdb2f4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # data visualization\nfrom matplotlib import cm\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","16cf08ce":"train.info() # basic information about the data set, such as how many values that are null and how much memory the data occupies","9d687a53":"train['Sex'] = (train['Sex'] == 'female').astype(int)","72c20a4e":"plt.figure(figsize=(18, 12)) # make the plot 18 by 12 inches\nsns.heatmap(train.corr(), cmap=cm.coolwarm) # plot it","dccf3558":"gender_by_survived = train.groupby(['Sex', 'Survived']) # group it\n# count the groups, could also use other aggregates\ngender_by_survived = gender_by_survived.size()\n# unstack it to make the survived or not counts into columns and the gender group into rows\ngender_by_survived = gender_by_survived.unstack()\ngender_by_survived.plot.bar(figsize=(18, 12))","76cce78a":"import sklearn.tree\nimport sklearn.linear_model","73917da8":"tree = sklearn.tree.DecisionTreeClassifier()\npredictors = ['Sex']","d9c8a74f":"tree.fit(train[predictors], train['Survived'])","4cd1be9f":"train['prediction'] = tree.predict(train[predictors])\ntrain[['Survived', 'prediction']].head()","10f330b3":"tree.score(train[predictors], train['Survived'])","95a69450":"train_on = train.iloc[:800] # train on the first 800 samples\nvalidate_on = train.iloc[800:] # save the rest for validation\ntree = sklearn.tree.DecisionTreeClassifier() # make a new model\ntree.fit(train_on[predictors], train_on['Survived']) # train it only on part of the data set\ntree.score(validate_on[predictors], validate_on['Survived']) # check using data the model hasn't seen","441eb05a":"import sklearn.model_selection\n\nsklearn.model_selection.cross_val_score(tree, train[predictors], train['Survived'], cv=5)","5b0b8ba8":"test['Sex'] = (test.Sex == 'female').astype(int)\n\n# Fit the model to the training data set\ntree.fit(train[predictors], train['Survived'])\ntest['Survived'] = tree.predict(test[predictors])\n\ntest[['PassengerId', 'Survived']].head()","eb2d95df":"test[['PassengerId', 'Survived']].to_csv('predictions.csv', header=True, index=False)","51740765":"# Posten hackathon starter kit\nThis kernel is meant as a starting off point for the Posten hackathon on `17.12.18`. ","a4f90abc":"Looks good for those few values, the prediction matched survived.  We can check the entire dataset in a single call like this:","d0592af1":"# Checking out correlations of the numeric variables","70e0f7b8":"# Submit to kaggle\n\nLet's submit our prediction to kaggle. We'll do that by predicting on the test data set, for which we don't know the answer, and write it to a CSV file.\n\nTo do the prediction, we need to make the exact same changes to the `test` data set as we've done to the `train` data set, or our model won't be able to make sense of it.","c8a81d4a":"Here we can see why the correlation is strong:  Women are much more likely to have survived than men.","908b94b8":"Still very good, and shows that we've picked out some pretty important attributes.\n\nBut this is a very tedious way to check how good the model is doing, so here's a better one: **cross validations**","6698398f":"Machine learning algorithms works best on numeric data, so let's try to convert some of the strings in the data set.  For instance, let's try to rewrite the data for Sex so that 0 means male and 1 means female:","72699e94":"What we just did, using the `cv=5` parameter was to ask for 5 cross validation. For each cross validation, scikit-learn splits the data set into 5 pieces, trains the model on 4 of them and validates on the last. That's why we got 5 scores out here. We can see that the model does better on some of the cross validations than others. But we have a pretty good indication that we can get about 75% of the data set correct.","20519a24":"# But don't do that\n\nDoes that mean we're done?\n\nShort answer: no\n\nThe reason why it's getting so many right is that the model is simply remembering every data point in the data set. To see how well it works, we actually need to show it some data it hasn't already used for learning. To do that, we need to split the data set before we train. Here's one way to do that:","e4d4e8f0":"Let's look at that discovery in another way:","9e9136cb":"# Where to next?\n\nI've already submitted this, so I know it'll give a decent score, but not a great one.  There's a lot of things we can do to improve on it, but that's what I'm hoping you will do today.  Here are some ideas:\n\n- See if another classifier will do better.  Decision trees are great because they're simple, not because they give the best results.\n- We've just looked at a single attribute, `Sex`, that's wasting a lot of data.  See if you can find other attributes which improve the model\n- There is a lot of data in the strings, see if you can extract something useful from those.  Perhaps the title in the `Name` column can be used for something?\n\nCan you get to 80%?  It's certainly possible, but it'll take a lot of work and some smart problem solving.\n\n**Good luck, and have fun**","62a99126":"We can see there are 891 rows (passengers) in the data set, and we have 12 attributes to work with.  Some are numeric and others are strings ( `object`).  Not all the attributes are complete, e.g. `Age` where we only have 714 non-null values.","5d23e23a":"# Investigating the data sets\n\nThe data has been loaded as a pandas dataframe, full [API documentation](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/)\n\nHere's a short demo of what pandas can do.","e6678114":"Now we can use the model to make predictions. Let's see if it gets everything right:","1b9ebb00":"The decision tree is the simplest possible model. It can work with data in almost all formats. But it's not necessarily the best model. We'll use it to demonstrate the scikit-learn API.\n\nTo train a scikit-learn model, call the `.fit()` method with the attributes you want the tree to do learning on, and the answers you want it to predict for that set of attributes:","ee4d3292":"# Making a simple model\n\nIt definitely looks like the gender is very predictive for whether someone survived. So we should use it in a model.\n\nWe will use scikit-learn to demonstrate, you can see the [supervised learning](https:\/\/scikit-learn.org\/stable\/supervised_learning.html#supervised-learning) section of the documentation for detailed information about how this works.","c4118de8":"# Titanic data set preparations\n\nStart by importing the libraries we will use and read the data sets from the disk.\n\n- The train data set is the part of the data set for which we know if a passenger survived\n- The task is to predict which passengers in the test data set who survived\n\nIt's easy to solve this using google, but a lot more fun to solve it using machine-learning.\n\nThis notebook contains a demo for a simple machine learning models and visualizations that you can use to predict a result. Feel free to fork the notebook and improve it by making the machine learning models consider more attributes of the passengers.","34632a4a":"Now we've written 'Survived' attribute for the test set, it's time to submit to kaggle. We can do that by writing a CSV file to the current working directory, containing _only_ the PassengerId and Survived column:","427ec608":"Studying the correlation matrix is a good way to find out which variables in the data that can help us predict whether a passenger survived.  Keep in mind that a strong negative correlation (aka the dark blue squares) are still strongly correlated.  Values close to 0 are the ones we don't care too much about.\n\nBy looking at the Survived column, we can see that it correlates well with Sex, meaning we can tell a lot about whether a person survived but looking at their gender.  We'll use this to our advantage later.","3084b66d":"You can't actually see the file yet, you'll have to commit the notebook and run it first.  When you've done that you can see the output and submit it to the competition.  However, there's a limit to how often you can do this, so please use the train dataset and cross validations to check if your model looks improved."}}