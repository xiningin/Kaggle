{"cell_type":{"e9d125ce":"code","1fc29fe5":"code","debd7585":"code","3d7c5362":"code","ff0ee9ce":"code","7f773ec2":"code","4a1cb3a8":"code","eaa7077f":"code","d605dc84":"code","9547823b":"code","b577ff78":"code","8d415e20":"code","8aaa6fa6":"code","01921619":"code","db02af52":"code","a6b02e97":"code","d03585af":"code","2f68bd19":"code","1e81900f":"code","8ac825e8":"code","4c7055a9":"code","06fee1ec":"code","4b8f0b42":"code","83ff7189":"code","682c6416":"code","f512b74c":"code","ad4d3582":"code","9d999f83":"code","3054952c":"code","97633596":"code","90d5ffab":"code","a6bc6e09":"code","c88ec86c":"code","065e872e":"code","cc8fddf3":"code","bfd42703":"code","7a312454":"code","afe64c7d":"code","335bc126":"code","49fa5372":"code","1f3211f7":"code","28005854":"code","807c01e2":"code","65c8a972":"code","ae742155":"code","068ead04":"code","798fbc47":"code","9eef8a63":"code","a630e6bd":"code","ee2dcce9":"markdown","c455f0e0":"markdown","324188d5":"markdown","3e24213f":"markdown","66f6dde4":"markdown","ee1cb74c":"markdown","5e79233b":"markdown","9b394913":"markdown","6047eac6":"markdown","ebf53c84":"markdown","bffca777":"markdown","dcd836f3":"markdown","f2072f6b":"markdown","42cb605e":"markdown","80a7d98d":"markdown","20a73f1d":"markdown","a4f76cdd":"markdown","9a1b6faf":"markdown","693d010d":"markdown","e1589195":"markdown","e5fc9b98":"markdown","7a968176":"markdown","2ae88fb4":"markdown","dc775998":"markdown","a60afe5e":"markdown","72761d52":"markdown","494de911":"markdown","d9818e9b":"markdown","a0c5c3b3":"markdown","58727536":"markdown","46733869":"markdown","f28ba9d3":"markdown"},"source":{"e9d125ce":"# Use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","1fc29fe5":"import pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn import cluster\nfrom sklearn import manifold\n\nfrom collections import defaultdict, Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Dropout\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow.keras as keras\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nimport transformers as ts\nfrom transformers import BertTokenizer, BertConfig, TFBertModel\nimport tokenization","debd7585":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsample_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\n\n# Print the shape of the training data\nprint('{} rows and {} cols in training dataset.'.format(train.shape[0], train.shape[1]))\nprint('{} rows and {} cols in training dataset.'.format(test.shape[0], test.shape[1]))\n\n# Inspecting the training data\ntrain.head(10)","3d7c5362":"# Frequency for taget variable\ncount_table = train.target.value_counts()\ndisplay(count_table)\n\n# Plot class distribution\nplt.figure(figsize=(6,5))\nplt.bar('False',count_table[0],label='False',width=0.6)\nplt.bar('True', count_table[1],label='True',width=0.6)\nplt.legend()\nplt.ylabel('Count of examples')\nplt.xlabel('Category')\nplt.title('Class Distribution')\nplt.ylim([0,4700])\nplt.show()","ff0ee9ce":"# Description of characters in 'text'\ntrain['length'] = train['text'].apply(len)\ntrain.length.describe()","7f773ec2":"# Plot the frequency of tweets length\nbins = 150\nplt.figure(figsize=(18,5))\nplt.hist(train[train['target']==0]['length'], label= 'False',bins=bins,alpha=0.8)\nplt.hist(train[train['target']==1]['length'], label= 'True', bins=bins,alpha=0.8) \nplt.xlabel('Length of text (characters)')\nplt.ylabel('Count')\nplt.title('Frequency of tweets length')\nplt.legend(loc='best')\nplt.show()","4a1cb3a8":"# Frequency of tweets length in 2 classes\nfg, (ax1, ax2)=plt.subplots(1,2,figsize=(14,5))\nax1.hist(train[train['target']==0]['length'],color='red')\nax1.set_title('Distribution of fake tweets')\nax1.set_xlabel('Tweets length (characters)')\nax1.set_ylabel('Count')\nax2.hist(train[train['target']==1]['length'],color='blue')\nax2.set_title('Distribution of true tweets')\nax2.set_xlabel('Tweets length (characters)')\nax2.set_ylabel('Count')\nfg.suptitle('Characater in classes')\nplt.show()","eaa7077f":"# Plot the distribution of count of words\nwords_true = train[train['target']==1]['text'].str.split().apply(len)\nwords_false = train[train['target']==0]['text'].str.split().apply(len)\nplt.figure(figsize=(10,5))\nplt.hist(words_false, label='False',alpha=0.8,bins=15)\nplt.hist(words_true, label='True',alpha=0.6,bins=15)\nplt.legend(loc='best')\nplt.title('Count of words in tweets')\nplt.xlabel('Count of words')\nplt.ylabel('Count')\nplt.show()","d605dc84":"# Define a function to create corpus.\ndef create_corpus(df, target):\n    '''This function is for creating corpus for a class in a df.'''\n    corpus = []\n    for word_list in df[df['target']==target]['text'].str.split():\n        corpus += word_list\n    return corpus","9547823b":"# Top 10 frequent stopwords in fake tweets\ncorpus0 = create_corpus(train,0)\n\ndic0 = defaultdict(int)\nfor word in corpus0:\n    if word in stop:\n        dic0[word] += 1\n\ntop_words0 = sorted(dic0.items(), key=lambda x: x[1], reverse=True)[:10]\n\n# Plot the top 10 frequent stopwords in fake tweets\nplt.figure(figsize=(12,5))\nwd, cnt = zip(*top_words0)\nplt.title('Top 10 frequent stopwords in fake tweets')\nplt.bar(wd, cnt);","b577ff78":"# Top 10 frequent stopwords in true tweets\ncorpus1 = create_corpus(train, 1)\n\ndic1 = defaultdict(int)\nfor word in corpus1:\n    if word in stop:\n        dic1[word] += 1\n\ntop_words1 = sorted(dic1.items(), key=lambda x:x[1], reverse=True)[:10]\n\nplt.figure(figsize=(12,5))\nwd, cnt = zip(*top_words1)\nplt.title('Top 10 frequent stopwords in true tweets')\nplt.bar(wd, cnt, color='blue');","8d415e20":"# Punctuation analysis for fake tweets\ndic0 = defaultdict(int)\npunc = string.punctuation\n\nfor item in corpus0:\n    if item in punc:\n        dic0[item] += 1\n\nplt.figure(figsize=(10,4))\nx, y = zip(*dic0.items())\nplt.title('Punctuations in fake tweets')\nplt.ylabel('Count')\nplt.bar(x, y);","8aaa6fa6":"# Punctuation analysis for class 1\ndic1 = defaultdict(int)\npunc = string.punctuation\n\nfor item in corpus1:\n    if item in punc:\n        dic1[item] += 1\n\nplt.figure(figsize=(10,4))\nx, y = zip(*dic1.items())\nplt.title('Punctuations in true tweets')\nplt.ylabel('Count')\nplt.bar(x, y, color='blue');","01921619":"counter = Counter(corpus1)\ncommon = counter.most_common()\n\nx = []\ny = []\nfor word, cnt in common[:40]:\n    if word not in stop:\n        x.append(word)\n        y.append(cnt)\n\nplt.figure(figsize=(12,5))\nplt.title('Frequent words in tweets')\nplt.ylabel('Count')\nplt.bar(x, y);","db02af52":"# Define a function to get frequent bi-grams\ndef get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","a6b02e97":"plt.figure(figsize=(16,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(train['text'])[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nplt.xlabel('Count')\nsns.barplot(x=y,y=x);","d03585af":"# Cleansing on the training data and testing data\ndf = pd.concat([train, test], sort=True)","2f68bd19":"# Define a function to remove URL\ndef remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n# Test function\ntest = 'Address of this kernel: https:\/\/www.kaggle.com\/lilstarboy\/kernel4d04fe5667\/edit'\nprint(remove_url(test))","1e81900f":"# Apply the function on 'text' column\ndf['text'] = df['text'].apply(remove_url)","8ac825e8":"# Define a function to remove html tag\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n# Test function\ntest = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"\nprint(remove_html(test))","4c7055a9":"# Apply the function on 'text' column\ndf['text'] = df['text'].apply(remove_html)","06fee1ec":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\n\n# Define a function to remove emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"To test \ud83d\ude80\")","4b8f0b42":"# Apply the function on 'text' column\ndf['text'] = df['text'].apply(remove_emoji)","83ff7189":"# Define a function to remove punctuations\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\n# Test function\ntest = 'This is very complex!!!!!??'\nprint(remove_punct(test))","682c6416":"# Apply the function on 'text' column\ndf['text'] = df['text'].apply(remove_punct)","f512b74c":"# Wordcloud for not disaster tweets\ncorpus_all_0 = create_corpus(df, 0)\n\n# Plot the wordcloud\nplt.figure(figsize=(15,8))\nword_cloud = WordCloud(\n                          background_color='white',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_all_0))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","ad4d3582":"# Wordcloud for disaster tweets\ncorpus_all_1 = create_corpus(df, 1)\n\n# Plot the wordcloud\nplt.figure(figsize=(15,8))\nword_cloud = WordCloud(\n                          background_color='white',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_all_1))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","9d999f83":"# Define a function to return both the embedding matrix and CountVectorizer.\ndef cv(train_data):\n    count_vectorizer = CountVectorizer(analyzer='word', stop_words = ['English']) # instantiate CountVectorizer object to convert text to matrix of token counts\n    count_vectorizer.fit(train_data) \n    train_emb = count_vectorizer.transform(train_data) \n    return train_emb, count_vectorizer\n\n# Test case\nlist_corpus = df[\"text\"].tolist() \nlist_labels = df[\"target\"].tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, \n                                                                                random_state=42)\n\nX_train_counts, count_vectorizer = cv(X_train) \nX_test_counts = count_vectorizer.transform(X_test)\nprint('Pass')","3054952c":"# Define plot_LSA function\ndef plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n        lsa = TruncatedSVD(n_components=2) # reduce dimensions\n        lsa.fit(test_data)\n        lsa_scores = lsa.transform(test_data)\n        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        colors = ['orange','blue']\n        if plot:\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            orange_patch = mpatches.Patch(color='orange', label='Not Disaster')\n            blue_patch = mpatches.Patch(color='blue', label='Disaster')\n            plt.legend(handles=[orange_patch, blue_patch], prop={'size': 15})\n\n# Plot the BOW embeddings\nfig = plt.figure(figsize=(12, 12))          \nplot_LSA(X_train_counts, y_train)\nplt.ylabel(\"Dimension 2\")\nplt.xlabel(\"Dimension 1\")\nplt.title('2 Dimensions Bag Of Words')\nplt.show()","97633596":"# Define a function to return both the TFIDF matrix and CountVectorizer.\ndef tfidf(train_data):\n    tfidf_vectorizer = TfidfVectorizer()\n    tfidf_vectorizer.fit(train_data)\n    train_tfidf = tfidf_vectorizer.transform(train_data)\n    return train_tfidf, tfidf_vectorizer\n\n#Test case\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\nprint('Pass')","90d5ffab":"fig = plt.figure(figsize=(12, 12))          \nplot_LSA(X_train_tfidf, y_train)\nplt.ylabel(\"Dimension 2\")\nplt.xlabel(\"Dimension 1\")\nplt.title('2 Dimensions TFIDF')\nplt.show()","a6bc6e09":"clusters = cluster.MiniBatchKMeans(n_clusters=14, init_size=1024, batch_size=2048, random_state=20).fit_predict(X_test_tfidf)\n\n# Define a function to get 10 frequent words for each cluster\ndef cluster_keys(x, clusters, labels, n_terms):\n    df = pd.DataFrame(x.todense()).groupby(clusters).mean()\n    for i,r in df.iterrows():\n        print('\\nCluster {}'.format(i))\n        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))\n            \ncluster_keys(X_test_tfidf, clusters, tfidf_vectorizer.get_feature_names(), 10)","c88ec86c":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\n\n# Define a function to remove links, non-ASCII characters and URLs\ndef clean_tweets(tweet):\n    tweet = ''.join([x for x in tweet if x in string.printable])\n    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n    return tweet","065e872e":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\n\n# Define a function to remove emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","cc8fddf3":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\n\n# Define a function to remove punctuations\ndef remove_punctuations(text):\n    punctuations = '@#!?+&*[]-%.:\/();$=><|{}^' + \"'`\"\n    \n    for p in punctuations:\n        text = text.replace(p, f' {p} ')\n\n    text = text.replace('...', ' ... ')\n    \n    if '...' not in text:\n        text = text.replace('..', ' ... ')\n    \n    return text","bfd42703":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\n\n# Define a function to convert abbreviations to text\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}\n\ndef convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n\ndef convert_abbrev_in_text(text):\n    tokens = word_tokenize(text)\n    tokens = [convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text","7a312454":"# Load data\ntrain = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","afe64c7d":"# Clean data using defined functions\ntrain[\"text\"] = train[\"text\"].apply(lambda x: clean_tweets(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: clean_tweets(x))\n    \ntrain[\"text\"] = train[\"text\"].apply(lambda x: remove_emoji(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: remove_emoji(x))\n    \ntrain[\"text\"] = train[\"text\"].apply(lambda x: remove_punctuations(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: remove_punctuations(x))\n    \ntrain[\"text\"] = train[\"text\"].apply(lambda x: convert_abbrev_in_text(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: convert_abbrev_in_text(x))","335bc126":"# Define hyperparameters\nMAXLEN = 128\nBATCH_SIZE = 32\nNUM_EPOCHS = 5\nLEARNING_RATE = 3e-6","49fa5372":"# Import bert tokenizer, config and model\ntokenizer = BertTokenizer.from_pretrained(\"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/bert-base-uncased-vocab.txt\")\nconfig = BertConfig.from_pretrained(\"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/bert-base-uncased-config.json\")\nbert_model = TFBertModel.from_pretrained(\"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/bert-base-uncased-tf_model.h5\",config=config)","1f3211f7":"# Convert the first sentence in 'text' column into word vector\ntext = train['text'][0]\nprint(text)\ninput_ids = tokenizer.encode(text,max_length=MAXLEN)\nprint(input_ids)\nprint(tokenizer.convert_ids_to_tokens(input_ids))","28005854":"# Build input values on the training data\ntrain_input_ids = []\ntrain_attension_mask = []\ntrain_token_type_ids = []\nfor text in train['text']:\n    input_ids = tokenizer.encode(text,max_length=MAXLEN)\n    padding_length = MAXLEN-len(input_ids)\n    train_input_ids.append(input_ids+[0]*padding_length)\n    train_attension_mask.append([1]*len(input_ids)+[0]*padding_length)\n    train_token_type_ids.append([0]*MAXLEN)\ntrain_input_ids = np.array(train_input_ids)\ntrain_attension_mask = np.array(train_attension_mask)\ntrain_token_type_ids = np.array(train_token_type_ids)","807c01e2":"# Build input values on the testing data\ntest_input_ids = []\ntest_attension_mask = []\ntest_token_type_ids = []\nfor text in test['text']:\n    input_ids = tokenizer.encode(text,max_length=MAXLEN)\n    padding_length = MAXLEN-len(input_ids)\n    test_input_ids.append(input_ids+[0]*padding_length)\n    test_attension_mask.append([1]*len(input_ids)+[0]*padding_length)\n    test_token_type_ids.append([0]*MAXLEN)\ntest_input_ids = np.array(test_input_ids)\ntest_attension_mask = np.array(test_attension_mask)\ntest_token_type_ids = np.array(test_token_type_ids)","65c8a972":"y_train = np.array(train['target'])","ae742155":"# Build the Bert-base-Uncased model\ninput_ids = keras.layers.Input(shape=(MAXLEN,),dtype='int32')\nattension_mask = keras.layers.Input(shape=(MAXLEN,),dtype='int32')\ntoken_type_ids = keras.layers.Input(shape=(MAXLEN,),dtype='int32')\n_, x = bert_model([input_ids,attension_mask,token_type_ids])\noutputs = keras.layers.Dense(1,activation='sigmoid')(x)\nmodel = keras.models.Model(inputs=[input_ids,attension_mask,token_type_ids],outputs=outputs)\nmodel.compile(loss='binary_crossentropy',optimizer=keras.optimizers.Adam(lr=LEARNING_RATE),metrics=['accuracy'])","068ead04":"# Fit the Bert-base-Uncased model\n(train_input_ids,valid_input_ids,\n train_attension_mask,valid_attension_mask,\n train_token_type_ids,valid_token_type_ids,y_train,y_valid) = train_test_split(train_input_ids,train_attension_mask,\n                                                               train_token_type_ids,y_train,test_size=0.1,\n                                                               stratify=y_train, random_state=0)\nearly_stopping = keras.callbacks.EarlyStopping(patience=3,restore_best_weights=True)\nmodel.fit([train_input_ids,train_attension_mask,train_token_type_ids],y_train,\n         validation_data=([valid_input_ids,valid_attension_mask,valid_token_type_ids],y_valid),\n         batch_size = BATCH_SIZE,epochs=NUM_EPOCHS,callbacks=[early_stopping])","798fbc47":"model.summary()","9eef8a63":"# Use the model to do prediction\ny_pred = model.predict([test_input_ids,test_attension_mask,test_token_type_ids],batch_size=BATCH_SIZE,verbose=1).ravel()\ny_pred = (y_pred>=0.5).astype(int)","a630e6bd":"# Export to submission\nsubmission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission['target'] = y_pred\nsubmission.to_csv('nlp_prediction.csv',index=False)","ee2dcce9":"### (1) Count of tweets in not disaster and real disaster tweets","c455f0e0":"## 8. Build Bert Model","324188d5":"We can see that tfidf seems so capture a bit more of the context of non-disaster and disaster tweets because we can now see some difference between the two groups on the vector space.","3e24213f":"### (6) Frequent words in tweets","66f6dde4":"#### Anaylsis of BOW embeddings:\nThe non-disaster is a little sparser, and distribute more to the upperright. However, overall, We see that the non-disaster tweets and real disaster tweets are very similar on this BOW vector space. ","ee1cb74c":"#### Analysis of frequent words in tweets:\nWe can see that frequent words are dominated by punctuations and stop words because of white space and letter case. Raw data are very messy and we need to do cleaning work.","5e79233b":"### (3) Romove emojis","9b394913":"### (1) Bag of words\nIn order to explore more visualizations, we convert the texts to vectors so as to capture the semantics in the texts.\n\nThe bag of words approach counts 1 if the word appears in the tweet, otherwise, counts 0.  We leverage the CountVectorizer to help lowercase, tokenize, omit words less than 2 characters, remove symbols, and then count the occurence of each token in each tweet.","6047eac6":"## 5. Word Cloud","ebf53c84":"#### Analysis of count of words in fake or true tweets:\nAs we can see from the graph, except for a valley in the middle of True tweets, two classes have similar distribution of count of words.","bffca777":"## 2. Importing Data","dcd836f3":"## 1. Importing Libraries","f2072f6b":"There is one thing we need to notice: when we create corpus, we split on ' '(white space) rather than '', thus the result count represents the counts of punctuations split by ' '.","42cb605e":"We would like to see whether the tokens are similar or different from each other, or are there any interesting clusters, so we reduce high dimensioned tfidf word embeddings into 2D, and then plot them. ","80a7d98d":"### (5) Punctuations in tweets","20a73f1d":"## 6. Word Embeddings","a4f76cdd":"## 3. Exploratory Data Analysis","9a1b6faf":"#### Analysis of stopwords in fake or true tweets:\n- 'The' is the most frequent stopword in both class. \n- 'a','to', 'and' are the following most frequent stopwords in class 0; \n- 'in', 'of', 'a' are the following most frequent stopwords in class 1.","693d010d":"### (7) Bi-gram Analysis","e1589195":"### (3) Count of words in not disaster and disaster tweets ","e5fc9b98":"### (4) Remove punctuations","7a968176":"# BUDT 758B Project Code\n# NLP with Disaster Tweets: Real or Not?\n## Group Pig: Li-Hsin Chang, Xinyu Guo, Jingyi Liu, Jiahui Zeng, Gan Zhao, Beilei Zhu\n\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster. \n\nhttps:\/\/www.kaggle.com\/c\/nlp-getting-started\/overview","2ae88fb4":"### (2) Count of characters in not disaster and disaster tweets ","dc775998":"### (1) Remove URL","a60afe5e":"### (4) Stopwords in tweets","72761d52":"#### Analysis of tweets length in fake or true tweets:\nWe can see that there is no significant difference of distribution between two classes. They both tend to have many characters and there is a peak in distribution around 140 in both class.","494de911":"## 7. Define Functions\nIn this part, we define functions to be applied on the final dataset.","d9818e9b":"### (2) Word cloud for disaster tweets","a0c5c3b3":"### (2) Remove html tag","58727536":"### (2) TFIDF\nWe use TF-IDF to take into account the importance(common in a tweets but rarer in the corpus) of the term in the corpus and weight the word frequency. ","46733869":"## 4. Data Cleaning","f28ba9d3":"### (1) Word cloud for not disaster tweets"}}