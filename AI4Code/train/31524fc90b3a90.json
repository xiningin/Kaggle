{"cell_type":{"ed9fd77e":"code","0b669798":"code","8e1e6a6c":"code","33358aa0":"code","80586eaf":"code","be825085":"code","2fdbf15d":"code","fd4c2fbf":"code","2e4a977a":"code","d54d7b11":"code","3c6c00ee":"code","b242aba0":"code","597cf98d":"code","bb9ca02d":"code","34c71504":"code","9f896afb":"code","c703047c":"code","107e85bc":"code","f4e48df2":"code","f6f4775d":"code","ed08eb5f":"code","2aa6d9fa":"code","0f6f8746":"code","11970cfd":"code","7396d106":"code","09c9b0f6":"code","e801eb35":"code","4e1cda4f":"code","183f4f79":"code","313f47cd":"code","266ccbc7":"markdown","93f7e422":"markdown","f2254200":"markdown","a8f9f4df":"markdown","9da26d5c":"markdown","8918c7bc":"markdown","6d81c713":"markdown","6ffc0566":"markdown","c71d2341":"markdown","55075b67":"markdown","dc804f68":"markdown","538fcb91":"markdown","6f5013ad":"markdown","18ee386a":"markdown","ca4a44e0":"markdown","b38dcffe":"markdown","652414a0":"markdown","febdaa0e":"markdown","6e287de0":"markdown","89469220":"markdown","deb28edd":"markdown","f16a4e84":"markdown","ccaf3ace":"markdown","29190e5d":"markdown","be3fdb68":"markdown","edd2360f":"markdown","a84dd548":"markdown","ddb52dfa":"markdown","4626c0e1":"markdown","d830d4d9":"markdown","855a6b9d":"markdown","6891de46":"markdown","745bb56c":"markdown","c9042b5a":"markdown","07268fef":"markdown","8de06633":"markdown","852d2844":"markdown"},"source":{"ed9fd77e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0b669798":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.cluster import KMeans","8e1e6a6c":"# Load the data\ndata = pd.read_csv('\/kaggle\/input\/iris-dataset\/iris-dataset.csv')\n# Check the data\ndata.head()","33358aa0":"# Create a scatter plot based on two corresponding features (sepal_length and sepal_width; OR petal_length and petal_width)\nplt.figure(figsize=(10, 10))\nplt.scatter(data['sepal_length'],data['sepal_width'])\n# Name your axes\nplt.xlabel('Lenght of sepal')\nplt.ylabel('Width of sepal')\nplt.show()","80586eaf":"# create a variable which will contain the data for the clustering\nx = data.copy()\n# create a k-means object with 2 clusters\nkmeans = KMeans(2)\n# fit the data\nkmeans.fit(x)","be825085":"# create a copy of data, so we can see the clusters next to the original data\nclusters = data.copy()\n# predict the cluster for each observation\nclusters['cluster_pred']=kmeans.fit_predict(x)","2fdbf15d":"# create a scatter plot based on two corresponding features (sepal_length and sepal_width; OR petal_length and petal_width)\nplt.figure(figsize=(10, 10))\nplt.scatter(clusters['sepal_length'], clusters['sepal_width'], c= clusters ['cluster_pred'], cmap = 'rainbow')\nplt.xlabel('sepal_length')\nplt.ylabel('sepal_width')\nplt.title(\"sepal_length v\/s sepal_width\")\nplt.show()","fd4c2fbf":"# import some preprocessing module\nfrom sklearn import preprocessing\n\n# scale the data for better results\nx_scaled = preprocessing.scale(data)\nx_scaled","2e4a977a":"# create a k-means object with 2 clusters\nkmeans_scaled = KMeans(2)\n# fit the data\nkmeans_scaled.fit(x_scaled)","d54d7b11":"# create a copy of data, so we can see the clusters next to the original data\nclusters_scaled = data.copy()\n# predict the cluster for each observation\nclusters_scaled['cluster_pred']=kmeans_scaled.fit_predict(x_scaled)","3c6c00ee":"# create a scatter plot based on two corresponding features (sepal_length and sepal_width; OR petal_length and petal_width)\nplt.figure(figsize=(10, 10))\nplt.scatter(clusters_scaled['sepal_length'], clusters_scaled['sepal_width'], c= clusters_scaled ['cluster_pred'], cmap = 'rainbow')\nplt.xlabel('sepal_length')\nplt.ylabel('sepal_width')\nplt.title(\"sepal_length v\/s sepal_width\")\nplt.show()","b242aba0":"wcss = []\n# 'cl_num' is a that keeps track the highest number of clusters we want to use the WCSS method for. We have it set at 10 right now, but it is completely arbitrary.\ncl_num = 10\nfor i in range (1,cl_num):\n    kmeans= KMeans(i)\n    kmeans.fit(x_scaled)\n    wcss_iter = kmeans.inertia_\n    wcss.append(wcss_iter)\nwcss","597cf98d":"number_clusters = range(1,cl_num)\nplt.plot(number_clusters, wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Within-cluster Sum of Squares')\nplt.show()","bb9ca02d":"kmeans_2 = KMeans(2)\nkmeans_2.fit(x_scaled)","34c71504":"# Remember that we are plotting the non-standardized values of the sepal length and width. \nclusters_2 = x.copy()\nclusters_2['cluster_pred']=kmeans_2.fit_predict(x_scaled)","9f896afb":"plt.figure(figsize=(10, 10))\nplt.scatter(clusters_2['sepal_length'], clusters_2['sepal_width'], c= clusters_2 ['cluster_pred'], cmap = 'rainbow')\nplt.xlabel('sepal_length')\nplt.ylabel('sepal_width')\nplt.title(\"sepal_length v\/s sepal_width\")\nplt.show()","c703047c":"kmeans_3 = KMeans(3)\nkmeans_3.fit(x_scaled)","107e85bc":"clusters_3 = x.copy()\nclusters_3['cluster_pred']=kmeans_3.fit_predict(x_scaled)","f4e48df2":"plt.figure(figsize=(10, 10))\nplt.scatter(clusters_3['sepal_length'], clusters_3['sepal_width'], c= clusters_3 ['cluster_pred'], cmap = 'rainbow')\nplt.xlabel('sepal_length')\nplt.ylabel('sepal_width')\nplt.title(\"sepal_length v\/s sepal_width\")\nplt.show()","f6f4775d":"kmeans_5 = KMeans(5)\nkmeans_5.fit(x_scaled)","ed08eb5f":"clusters_5 = x.copy()\nclusters_5['cluster_pred']=kmeans_5.fit_predict(x_scaled)","2aa6d9fa":"plt.figure(figsize=(10, 10))\nplt.scatter(clusters_5['sepal_length'], clusters_5['sepal_width'], c= clusters_5 ['cluster_pred'], cmap = 'rainbow')\nplt.xlabel('sepal_length')\nplt.ylabel('sepal_width')\nplt.title(\"sepal_length v\/s sepal_width\")\nplt.show()","0f6f8746":"real_data = pd.read_csv(r'\/kaggle\/input\/iris-dataset\/iris-with-answers.csv')","11970cfd":"real_data['species'].unique()","7396d106":"# We use the map function to change any 'yes' values to 1 and 'no'values to 0. \nreal_data['species'] = real_data['species'].map({'setosa':0, 'versicolor':1 , 'virginica':2})","09c9b0f6":"real_data.head()","e801eb35":"plt.figure(figsize=(10, 10))\nplt.scatter(real_data['sepal_length'], real_data['sepal_width'], c= real_data ['species'], cmap = 'rainbow')\nplt.xlabel('sepal_length')\nplt.ylabel('sepal_width')\nplt.title(\"sepal_length v\/s sepal_width\")\nplt.show()","4e1cda4f":"plt.figure(figsize=(10, 10))\nplt.scatter(real_data['petal_length'], real_data['petal_width'], c= real_data ['species'], cmap = 'rainbow')\nplt.xlabel('sepal_length')\nplt.ylabel('sepal_width')\nplt.title(\"sepal_length v\/s sepal_width\")\nplt.show()","183f4f79":"plt.figure(figsize=(10, 10))\nplt.scatter(clusters_3['sepal_length'], clusters_3['sepal_width'], c= clusters_3 ['cluster_pred'], cmap = 'rainbow')\nplt.xlabel('sepal_length')\nplt.ylabel('sepal_width')\nplt.title(\"sepal_length v\/s sepal_width\")\nplt.show()","313f47cd":"plt.figure(figsize=(10, 10))\nplt.scatter(clusters_3['petal_length'], clusters_3['petal_width'], c= clusters_3 ['cluster_pred'], cmap = 'rainbow')\nplt.xlabel('sepal_length')\nplt.ylabel('sepal_width')\nplt.title(\"sepal_length v\/s sepal_width\")\nplt.show()","266ccbc7":"It seems that our solution takes into account mainly the sepal features","93f7e422":"## Further clarifications","f2254200":"## Import the relevant libraries","a8f9f4df":"## Take Advantage of the Elbow Method","9da26d5c":"## Understanding the Elbow Curve","8918c7bc":"Start by seperating the standardized data into 2 clusters.","6d81c713":"## Scatter plots (which we will use for comparison)","6ffc0566":"Import and use the <i> scale <\/i> function from sklearn to standardize the data. ","c71d2341":"## 2 clusters","55075b67":"***","dc804f68":"#### Our clustering solution data","538fcb91":"## 3 Clusters","6f5013ad":"***","18ee386a":"The Iris flower dataset is one of the most popular ones for machine learning. You can read a lot about it online and have probably already heard of it: https:\/\/en.wikipedia.org\/wiki\/Iris_flower_data_set\n\nWe didn't want to use it in the lectures, but believe that it would be very interesting for you to try it out (and maybe read about it on your own).\n\nThere are 4 features: sepal length, sepal width, petal length, and petal width.\n\n***\n\nYou have already solved the first exercise, so you can start from there (you've done taken advantage of the Elbow Method).\n\nPlot the data with 2, 3 and 5 clusters. What do you think that means?\n\nFinally, import the CSV with the correct answers (iris_with_answers.csv) and check if the clustering worked as expected. Note that this is not how we usually go about clustering problems. If we have the answers prior to that, we would go for classification (e.g. a logistic regression).","ca4a44e0":"For this exercise, try to cluster the iris flowers by the shape of their sepal. \n\n<i> Hint: Use the 'sepal_length' and 'sepal_width' variables.<\/i> ","b38dcffe":"## Load the data","652414a0":"### The Elbow Method","febdaa0e":"# Species Segmentation with Cluster Analysis","6e287de0":"Examining the other scatter plot (petal length vs petal width), we see that in fact the features which actually make the species different are petals and NOT sepals!\n\nNote that 'real data' is the data observed in the real world (biological data)","89469220":"Redo the same for 3 and 5 clusters.","deb28edd":"## 5 Clusters","f16a4e84":"***","ccaf3ace":"Construct and compare the scatter plots to determine which number of clusters is appropriate for further use in our analysis. Based on the Elbow Curve, 2, 3 or 5 seem the most likely.","29190e5d":"### WCSS","be3fdb68":"Construct a scatter plot of the original data using the standartized clusters.","edd2360f":"## Clustering (scaled data)","a84dd548":"#### 'Real data'","ddb52dfa":"Load data from the csv file: <i> 'iris_dataset.csv'<\/i>.","4626c0e1":"## Compare your solutions to the original iris dataset\n\nThe original (full) iris data is located in <i>iris_with_answers.csv<\/i>. Load the csv, plot the data and compare it with your solution. \n\nObviously there are only 3 species of Iris, because that's the original (truthful) iris dataset.\n\nThe 2-cluster solution seemed good, but in real life the iris dataset has 3 SPECIES (a 3-cluster solution). Therefore, clustering cannot be trusted at all times. Sometimes it seems like x clusters are a good solution, but in real life, there are more (or less).","d830d4d9":"Instead of the petals...","855a6b9d":"## Standardize the variables","6891de46":"Separate the original data into 2 clusters.","745bb56c":"In fact, if you read about it, the original dataset has 3 sub-species of the Iris flower. Therefore, the number of clusters is 3.\n\nThis shows us that:\n<li> the Eblow method is imperfect (we might have opted for 2 or even 4) <\/li>\n<li> k-means is very useful in moments where we already know the number of clusters - in this case: 3 <\/li>\n<li> biology cannot be always quantified (or better).. quantified with k-means! Other methods are much better at that <\/li>\n\nFinally, you can try to classify them (instead of cluster them, now that you have all the data)! ","c9042b5a":"Looking at the first graph it seems like the clustering solution is much more intertwined than what we imagined (and what we found before)","07268fef":"## Plot the data","8de06633":"Based on the Elbow Curve, plot several graphs with the appropriate amounts of clusters you believe would best fit the data.","852d2844":"## Clustering (unscaled data)"}}