{"cell_type":{"efd3ca48":"code","6af7da1a":"code","fa93dfdd":"code","176a48cf":"code","20c07166":"code","b87c9589":"code","f4d60f9f":"code","cb9cdce0":"code","15556241":"code","a516dcc7":"code","3629f4d9":"code","58066b31":"code","7628a705":"code","f8ac13a6":"code","2d01a686":"code","c724b081":"code","9568ab3a":"code","d6c07f89":"code","b1d0d42c":"code","624c3bb8":"code","6e300e3b":"code","102d69f0":"code","1a6c0b01":"code","37487f90":"code","145ee4a0":"code","d582ed95":"code","9cc0e433":"code","f9706aca":"code","8858eccd":"code","137e37ba":"markdown","76342d90":"markdown","c2d02a76":"markdown","2a5d42b4":"markdown","7a96af9f":"markdown","f75e9f8d":"markdown","6231310d":"markdown","14029069":"markdown","6a0c0b63":"markdown","ce8de025":"markdown"},"source":{"efd3ca48":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n\n\n\n","6af7da1a":"# load dataset\ndf=pd.read_csv(\"\/kaggle\/input\/twitter-airline-sentiment\/Tweets.csv\")","fa93dfdd":"df.head()","176a48cf":"df.shape","20c07166":"df.text[0]","b87c9589":"df.columns","f4d60f9f":"tweets=df[[\"text\",\"airline_sentiment\"]]","cb9cdce0":"tweets.head()","15556241":"tweets.isnull().sum()","a516dcc7":"tweets.shape","3629f4d9":"# Take sentiments as only binary Classifier\ntweet_final=tweets[tweets[\"airline_sentiment\"]!=\"neutral\"]","58066b31":"tweet_final","7628a705":"# Total number of tweets accoeding to sentiments\ntweet_final[\"airline_sentiment\"].value_counts()","f8ac13a6":"# Convert labels to integer value by factorize method\nsentiment_label = tweet_final.airline_sentiment.factorize()\nsentiment_label","2d01a686":"tweet_data=tweet_final[\"text\"].values","c724b081":"tweet_data","9568ab3a":"#tokens are created from sklearn library keras\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(tweet_data)","d6c07f89":"# encoded data\nencoded_data = tokenizer.texts_to_sequences(tweet_data)\n","b1d0d42c":"# vocab size is length of vocabaloury\nvocab_size = len(tokenizer.word_index) + 1\n","624c3bb8":"tokenizer.word_index","6e300e3b":"# Creating equal dimension array of differnet words by padding\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\npadded_sequence = pad_sequences(encoded_data, maxlen=200)","102d69f0":"# Creating model\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D\nfrom tensorflow.keras.layers import Embedding\n\n\n\nembedding_vector_length = 32\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embedding_vector_length, input_length=200))\nmodel.add(SpatialDropout1D(0.25))\nmodel.add(LSTM(50, dropout=0.5, recurrent_dropout=0.5))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())","1a6c0b01":"#Train the  model for 5 epochs on the whole dataset with a batch size of 32 and a validation split of 20%.\n\nhistory = model.fit(padded_sequence,sentiment_label[0],validation_split=0.2, epochs=5, batch_size=32)\n","37487f90":"history","145ee4a0":"# Plot between losss ana Val_loss\n\nimport matplotlib.pyplot as plt\nplt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='val_loss')\n\nplt.legend()\nplt.show()\n\nplt.savefig(\"Loss plt.jpg\")","d582ed95":"# plot between acrruacy and val_accuracy\n\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'], label='acc')\nplt.plot(history.history['val_accuracy'], label='val_acc')\nplt.legend()\nplt.show()\n\nplt.savefig(\"Accuracy plot.jpg\")","9cc0e433":"# predicting Function\n\ndef predict_sentiment(text):\n    tw = tokenizer.texts_to_sequences([text])\n    tw = pad_sequences(tw,maxlen=200)\n    prediction = int(model.predict(tw).round().item())\n    print(\"Predicted label: \", sentiment_label[1][prediction])\n\n\n","f9706aca":"test_sentence1 = \"I enjoyed my journey on this flight.\"\npredict_sentiment(test_sentence1)\n\n","8858eccd":"test_sentence2 = \"This is the worst flight experience of my life!\"\npredict_sentiment(test_sentence2)","137e37ba":"# text into an array of vector embeddings.","76342d90":"# Summary","c2d02a76":"# Prediction","2a5d42b4":"Word embeddings are a beautiful way of representing the relationship between the words in the text.","7a96af9f":"# Sentiment Analysis On Tweets","f75e9f8d":"The obtained 95% accuracy on the training set and 93.33% accuracy on the test set.\n\n","6231310d":"# Build the Text Classifier","14029069":"# Training the model","6a0c0b63":"For sentiment analysis project, we use LSTM layers in the machine learning model.\nLSTM stands for Long Short Term Memory Networks\nOut(t)=Wh0h(t)+b0\nh(t)=tanh(WxhXt+Whhh(t-1)+bh)\n","ce8de025":"I have successfully developed python sentiment analysis model. \nIn this machine learning project, A binary text classifier  is built that classifies the sentiment of the tweets \ninto positive and negative.\nWe obtained more than 94% accuracy on validation"}}