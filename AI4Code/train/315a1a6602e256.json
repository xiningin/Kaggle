{"cell_type":{"a55b4fe4":"code","844f8422":"code","0a77d171":"code","c2bda538":"markdown","dd260712":"markdown","09d02e5a":"markdown","cfe538fb":"markdown","44b03cfc":"markdown","246a9fe3":"markdown","7b6b07cc":"markdown"},"source":{"a55b4fe4":"import gym\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nenv = gym.make('Acrobot-v1')\nstate = env.reset()\n\n# set parameters\niterations = 2\nsamples = 25\nelite = 7\niterations_cum_reward = []\n# initialise mean and standard deviation\nmu = np.random.uniform(size=state.shape)\nsigma = np.random.uniform(low=0.001, size=state.shape)\n\ndef sample_gaussian(mu, sigma, n, m):\n    weights = np.random.normal(loc=mu, scale=sigma, size=(n, m))\n    return weights\n\n\ndef evaluate_binary_policy(env, theta, render=False):\n    max_steps = 2000\n    cum_reward = 0\n    state = env.reset()\n    for _ in range(max_steps):\n        action = int(np.dot(theta, state) > 0)\n        state, reward, terminal, info = env.step(action)\n        cum_reward += reward\n        if render:\n            env.render()\n        if terminal:\n            break\n    return cum_reward","844f8422":"def learn_ce():\n    for _ in range(iterations):\n        # Obtain n samples from current sampling distribution\n            # Evaluate objective function at sampled points\n            # Sort X by objective function values and take n elite samples\n            # Update the parameters of sampling distribution\n        #iterations_cum_reward.append(cum_reward.mean())\n    env.close()","0a77d171":"env = gym.make('Acrobot-v1')\nstate = env.reset()\nlearn_ce()\nplt.scatter(range(iterations), iterations_cum_reward)\nplt.show()","c2bda538":"### Free Set-Up Code\n\nFeel free to change paremeters and or code, a lot more than 5 iterations will be needed to demonstrate learning","dd260712":"Algorithm above is adapted from [Abbeel, P]. The pseudo-code represents the Cross Entropy Method which can be defined as such:\n\n\\begin{equation}\n\\max_{\\theta}\\ U(\\theta) = \\max_{\\theta}\\ E[\\sum_{t=0}^{H} R(s_t \\mid \\pi_{\\theta})]\n\\end{equation}  \n\nHere, we maximise the expected sum of rewards $U$ under the policy $\\theta$, by sampling a distribution of policies $P\\mu^{i}(\\theta)$, parametrised by $\\mu$ at iteration $i$, where $\\mu^i$ is the mean of a Gaussian and $\\theta$ is sampled from the Guassian [Abbeel, P]. \n\nWhen using a Gaussian sampling distribution, the sample can be characterised by a vector $\\mu$ of means and a vector $\\sigma$ of standard deviations. As the algorithm is iterated, $\\mu$ and $\\sigma$ are generated such that $\\mu$ tends towards the optimal sample and  $\\sigma$ tends towards 0. This is known as *'normal updating'* [Benham et al.]. ","09d02e5a":"![Screenshot%20from%202019-10-12%2016-09-09.png](attachment:Screenshot%20from%202019-10-12%2016-09-09.png)","cfe538fb":"### In the below cell implement the Cross Entropy Method algorithm: basic steps outlined.","44b03cfc":"### The Fun Part\n\nBelow we have provided some set up code and a problem to apply the algorithm to (as how else will you know it works). OpenAI gym's Acrobot has been chosen due to its relative simplicity and applicability for CEM. Again, feel free to change the code including Acrobot. Cartpole or Mountain Car are good alternatives, whereas, Tetris from pixels is incredibly unlikely to achieve any results with this method.","246a9fe3":"### Run the code","7b6b07cc":"## Cross Entropy Method\n\nThe Cross Entropy Method (CEM) may be described as a Monte Carlo approach to the continuous optimisation of maximising a function. CEM is a sub-set of expectation-maximization algorithms, these are split into two main steps. The expectation step calculates the expected value which is consequently maximised by adjusting parameters.   \n\nFirstly, a distribution is randomly initialised, then parameter vectors are sampled from the distribution, the policy is then executed under these parameter vectors. The cumulative reward and parameters are stored over a number of iterations, so the best can be used to to fit the distribution parameters by maximising the log probability distribution that is determined by the average rewards experienced on the previous step.           \n\n"}}