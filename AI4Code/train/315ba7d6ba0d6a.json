{"cell_type":{"68302a6d":"code","de3825dd":"code","8b5d4bf1":"code","f13324e7":"code","522e5b17":"code","4092cf96":"code","db452ad3":"code","9591c6eb":"code","6838f9ac":"code","22eab7e8":"code","7b5c0f23":"code","bb07febb":"code","aa37ecd5":"code","7a5438b6":"code","023b42dd":"code","8a9832a6":"code","2e4a4168":"code","39141881":"code","8c55da5a":"code","ca3eaa23":"code","652df9d9":"code","731ae498":"code","a3c3e86e":"code","a8ce8732":"code","dff27481":"code","04a8c784":"code","323072ff":"code","703930ba":"code","9447f733":"code","15ab911f":"code","eb589124":"code","0c915d18":"code","13cd87d3":"code","cc8592b7":"code","e901d1e0":"code","e4da0734":"code","2b983544":"code","4e1e63ef":"code","273bea01":"markdown","81eac6c5":"markdown","1cc13a20":"markdown","e95981ee":"markdown","d4b97002":"markdown","ca572f5a":"markdown","67b90615":"markdown","42f9d9bf":"markdown","7babff0f":"markdown","0efa4204":"markdown","97e2d929":"markdown","a5961e27":"markdown","f0dd7f5f":"markdown","c8f7d6f8":"markdown","a53c94de":"markdown","7a0a93a5":"markdown","0ecea727":"markdown","e0c3651b":"markdown","abda38c3":"markdown","7f32ec0a":"markdown","d5160b18":"markdown","219829a4":"markdown","d4fce07c":"markdown","88774858":"markdown","8776ccf4":"markdown","ee1c9504":"markdown","8a08c41d":"markdown","38d091d0":"markdown","1872476f":"markdown","cc1ff99c":"markdown","2c2c9cce":"markdown","c4c719e6":"markdown"},"source":{"68302a6d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib as plt\nimport matplotlib.pyplot as plts\nfrom scipy.stats import norm \nfrom scipy import stats\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import preprocessing\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n# Input data files are available \n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","de3825dd":"test=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","8b5d4bf1":"test.head()","f13324e7":"train.head()","522e5b17":"print(\"the test shape: \",test.shape)\nprint(\"the trian shape :\",train.shape)","4092cf96":"test.columns","db452ad3":"data_tocor=train[['SalePrice','MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n       'SaleCondition']]\n\ncorr = data_tocor.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200\n                            ),\n    square=True\n)\n","9591c6eb":"train['SalePrice'].describe()","6838f9ac":"sns.distplot(train['SalePrice']);","22eab7e8":"print(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","7b5c0f23":"var='GrLivArea'\nplot_data=pd.concat([train['SalePrice'],train[var]],axis=1)\nplot_data.plot.scatter(x=var,y='SalePrice',ylim=(0.8)) ","bb07febb":"var = 'TotalBsmtSF'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","aa37ecd5":"#box plot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.pyplot.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","7a5438b6":"var = 'YearBuilt'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.pyplot.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.pyplot.xticks(rotation=90);","023b42dd":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = train.corr().nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.pyplot.show()","8a9832a6":"total = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","2e4a4168":"\ntrain_fixed = train.drop((missing_data[missing_data['Total'] > 1]).index,1)\ntrain_fixed = train_fixed.drop(train_fixed.loc[train['Electrical'].isnull()].index)\ntrain_fixed.isnull().sum().max()","39141881":"train.sort_values(by = 'GrLivArea', ascending = False)[:2]\ntrain = train.drop(train[train['Id'] == 1299].index)\ntrain = train.drop(train[train['Id'] == 524].index)","8c55da5a":"sns.distplot(train['SalePrice'], fit=norm);\nfig = plts.figure()\nres = stats.probplot(train['SalePrice'], plot=plts)","ca3eaa23":"c=train['SalePrice']\ntrain['SalePrice'] = np.log(train['SalePrice'])","652df9d9":"sns.distplot(train['SalePrice'], fit=norm);\nfig = plts.figure()\nres = stats.probplot(train['SalePrice'], plot=plts)","731ae498":"\nsns.distplot(train['GrLivArea'], fit=norm);\nfig = plts.figure()\nres = stats.probplot(train['GrLivArea'], plot=plts)","a3c3e86e":"#data transformation\ntrain['GrLivArea'] = np.log(train['GrLivArea'])\n#transformed histogram and normal probability plot\nsns.distplot(train['GrLivArea'], fit=norm);\nfig = plts.figure()\nres = stats.probplot(train['GrLivArea'], plot=plts  )                             ","a8ce8732":"#histogram and normal probability plot\nsns.distplot(train['TotalBsmtSF'], fit=norm);\nfig = plts.figure()\nres = stats.probplot(train['TotalBsmtSF'], plot=plts)","dff27481":"#if area>0 it gets 1, for area==0 it gets 0\ntrain['HasBsmt'] = pd.Series(len(train['TotalBsmtSF']), index=train.index)\ntrain['HasBsmt'] = 0 \ntrain.loc[train['TotalBsmtSF']>0,'HasBsmt'] = 1","04a8c784":"#transform data\ntrain.loc[train['HasBsmt']==1,'TotalBsmtSF'] = np.log(train['TotalBsmtSF'])","323072ff":"#histogram and normal probability plot\nsns.distplot(train[train['TotalBsmtSF']>0]['TotalBsmtSF'], fit=norm);\nfig = plts.figure()\nres = stats.probplot(train[train['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plts)","703930ba":"test_total = test.isnull().sum().sort_values(ascending=False)\ntest_percentage = (test.isnull().sum() \/ test.isnull().count()).sort_values(ascending=False)\ntest_missing_data = pd.concat([test_total, test_percentage], axis=1, keys=['Total', 'Percentage'])\ntest_missing_data.head(20)","9447f733":"imput=SimpleImputer(missing_values=np.nan,strategy='most_frequent')\ntest_final= pd.DataFrame(imput.fit_transform(test), columns=test.columns)\ntest_final\n","15ab911f":"test_total = test_final.isnull().sum().sort_values(ascending=False)\ntest_percentage = (test_final.isnull().sum() \/ test_final.isnull().count()).sort_values(ascending=False)\ntest_missing_data = pd.concat([test_total, test_percentage], axis=1, keys=['Total', 'Percentage'])\ntest_missing_data.head(20)","eb589124":"\ntrain_final=train\ntrain_final = train_final.drop('SalePrice', axis=1)\ny = c","0c915d18":"train_len = len(train_final)\n\ndf = pd.concat([train_final, test_final], ignore_index=True)\n\nEncoder = preprocessing.LabelEncoder()\ncategorical = df.select_dtypes(include=['object'])\n\nfor column in categorical:\n    df[column] = Encoder.fit_transform(df[column].astype('str'))\n\ntrain_final = df[:train_len]\ntest_final = df[train_len:]\n\nprint(\"DF Shape:\", df.shape)\nprint(\"Train Shape:\", train_final.shape)\nprint(\"Test Shape:\", test_final.shape)","13cd87d3":"train_final=train_final.drop(['HasBsmt'],axis=1)\ntest_final=test_final.drop(['HasBsmt'],axis=1)","cc8592b7":"predict_data=train_final[['TotalBsmtSF','OverallQual','YearBuilt','GrLivArea']]\ntest_predection=test_final[['TotalBsmtSF','OverallQual','YearBuilt','GrLivArea']]\n\nGBR = GradientBoostingRegressor(loss='huber', n_estimators=2000, learning_rate=.05)\n\ncv_score = cross_val_score(GBR, train_final, y, cv=10, n_jobs=-1)\n\nprint(f\"GBR Model Max Test Score: {cv_score.max().round(4)*100}%\")\n\nGBR.fit(train_final, y)\n\nprint(f\"GBR Model Train Score: {GBR.score(train_final, y).round(4)*100}%\")","e901d1e0":"\ny_pred = GBR.predict(test_final)\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nprint(\"y_pred shape:\", y_pred.shape)\n\nprint(\"submission shape:\", submission.shape)\ny_pred\n","e4da0734":"submission['SalePrice'] = y_pred","2b983544":"submission","4e1e63ef":"submission.to_csv('submission_final.csv', index=False)","273bea01":"# Missing Data in test data set ","81eac6c5":"Both GrLivArea & TotalBsmtSF have a linear-Relationship with the SalePrice, if one of the var increases the other increases. \nFor the Quatigroical varible we Have that the SalePrice increase when the   OverallQual gos up ....","1cc13a20":"let study our goal variable: ","e95981ee":"#### TotalBsmtSF","d4b97002":"This matrix shows that some variable are twins which provide us the ability to pick one of them for our ML","ca572f5a":"## Sale Price","67b90615":"### The columns's name","42f9d9bf":"Here, The TotalBsmtSF has an exponentioal relation with The SalePrice var","7babff0f":"As we see here , we have overallQual has a correlation with SalePrice of 0.75 , YearBuilt about 0.65 ,GrLivArea : 0.65 and TotalBsmSF 0.55.So we categorize the (OverallQual,YearBuilt) as Building Variable , (GrlivArea , TotalBsmSF) as space Variable.\n\n> Note:\n1. ***TotalBsmtSF***  :Total square feet of basement area  \n2. ****OverallQual**** :Overall materi al and finish quality \n3. ****YearBuilt****   :  Original construction date  \n4. ****GrlivArea****   : Above grade (ground) living area square feet","0efa4204":"As we see here the distribution is divatiting from the normal  to the left.It has positive skweeness ( for more information :****https:\/\/codeburst.io\/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa**** ).\n","97e2d929":"After some reading I conclude that we could delete the varaibles that has >15% of missing data .More of that , this data are not relevent to a client when he buys a house , also it contains outliers .","a5961e27":"To normalized the sale price we aplly a log transformation ,**WHY** ? We have a  **positive skewness** , so that give us the previlage to use the log tronsformation .","f0dd7f5f":"To apply a log transformation here, we'll create a variable that can get the effect of having or not having basement (binary variable). Then, we'll do a log transformation to all the non-zero observations, ignoring those with value zero. This way we can transform data, without losing the effect of having or not basement.","c8f7d6f8":"# Model ","a53c94de":"We see here that the SalePrice and GrLivArea have a linear RelationShip","7a0a93a5":"### The distribution of the Sale Price ","0ecea727":"### Numerical Varibles","e0c3651b":"### Colum & Rows Preview ","abda38c3":"## label Encoder","7f32ec0a":"## correaltion matrix","d5160b18":"****Our skewness & kurtosis****\n","219829a4":"## Normality","d4fce07c":"### Categorlical Variable","88774858":"## Note :","8776ccf4":"After some digging because it is my first time in data science projects ,I found some good resources to help me out chosing (****https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python**** ,****http:\/\/rstudio-pubs-static.s3.amazonaws.com\/411129_3dc9a8d86a6647f8824110047634eb9a.html****)  my featurs to use them in ML .I end up by taking OveralLQuaL ,YearBuilt,TotalBsmtSF,GrLivArea for those reasons : \n","ee1c9504":"# OutLiers","8a08c41d":"# Understanding The Data ","38d091d0":"*we have a kurtosis >3 which means that we have a Leptokurtic*","1872476f":"### The shape of the dataset ","cc1ff99c":"### Now let's check the GrLivArea","2c2c9cce":"# Missing Data","c4c719e6":"## Studying The Relationship Between the varibles :"}}