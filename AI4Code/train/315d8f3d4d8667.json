{"cell_type":{"05e15aab":"code","d21e467c":"code","fac688a0":"code","0d71bb7a":"code","5ec0d2e5":"code","dae8ed84":"code","d704f57e":"code","82030d22":"code","086bdfa8":"code","9acf9307":"code","73bef0ca":"code","1b0536f5":"code","062be803":"code","a2faced3":"code","c2d3fa10":"code","e33e5f89":"code","93e2689c":"code","4c4688a8":"code","58d52f39":"code","826b53ba":"code","fee05a31":"code","42a91024":"code","7762a666":"code","2143a304":"code","f78d4a33":"code","e4cc371e":"code","a0aa08bf":"code","aa5ff3bb":"code","7b13773f":"code","70c8989a":"code","845ea708":"code","d42e2872":"code","e2dd6413":"code","98e6d7fb":"code","d8642737":"code","27398426":"code","841f8780":"code","16fe5a8d":"code","125382ca":"code","51340de3":"code","a7d736eb":"code","f81673bd":"code","ae315800":"code","f15bbbcb":"code","3e3b07ee":"code","ca00c226":"code","b4dd4409":"code","c4fd21fe":"code","d3be8ac1":"code","b3c24de9":"code","bba76252":"code","b30fd7e1":"code","62166af6":"code","f68d1255":"code","898eeff2":"code","472da450":"code","38c51018":"code","fc3a3df0":"code","cffc9bae":"code","aa17e876":"code","a5d20110":"markdown","72f0b6ff":"markdown","c38bcb2f":"markdown","850025c2":"markdown","96e0e9c3":"markdown","cf18232a":"markdown","dbc5f0fe":"markdown","3a0429b9":"markdown","db471224":"markdown","03376590":"markdown","a290f314":"markdown","d494c6da":"markdown"},"source":{"05e15aab":"# Import the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport time\nimport warnings\nimport os\nfrom six.moves import urllib\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nplt.style.use('seaborn')\nfrom scipy.stats import norm, skew","d21e467c":"# Scalers\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.utils import shuffle\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\n\n# Models\n\nfrom sklearn.linear_model import Lasso, ElasticNet\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error\n\n#regression\nfrom sklearn.linear_model import LinearRegression,Ridge,LassoCV,RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom scipy.stats import reciprocal, uniform\n\n# Cross-validation\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.model_selection import cross_validate\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n# GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#Common data processors\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom scipy import sparse","fac688a0":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsample = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","0d71bb7a":"train.shape","5ec0d2e5":"test.shape","dae8ed84":"#Drop the id column\n\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","d704f57e":"# It seems that the price of recent-built houses are higher\n\nplt.figure(figsize=(15,8))\nsns.boxplot(train.YearBuilt, train.SalePrice)\n\n# From the graph we can surely see plenty of outliers.","82030d22":"plt.figure(figsize=(12,6))\nplt.scatter(x=train.GrLivArea, y=train.SalePrice)\nplt.xlabel(\"GrLivArea\", fontsize=13)\nplt.ylabel(\"SalePrice\", fontsize=13)\nplt.ylim(0,800000)","086bdfa8":"plt.figure(figsize=(12,6))\nplt.scatter(x=train.OverallQual, y=train.SalePrice)\nplt.xlabel(\"GrLivArea\", fontsize=13)\nplt.ylabel(\"SalePrice\", fontsize=13)\nplt.ylim(0,800000)","9acf9307":"train.drop(train[(train['GrLivArea']>=4500) & (train['SalePrice']<300000)].index, inplace=True)\ntrain.reset_index(drop=True, inplace=True)","73bef0ca":"# Graphs after removing outliers\nplt.figure(figsize=(12,6))\nplt.scatter(x=train.GrLivArea, y=train.SalePrice)\nplt.xlabel(\"GrLivArea\", fontsize=13)\nplt.ylabel(\"SalePrice\", fontsize=13)\nplt.ylim(0,800000)","1b0536f5":"train.shape","062be803":"train.info()","a2faced3":"train.describe()","c2d3fa10":"# Checking if the log is required for the housing sales price\nplt.subplot(1, 2, 1)\nsns.distplot(train.SalePrice, kde=True, fit = norm)","e33e5f89":"#Plot is right skewed, so we need to normalize this distribution\n\nplt.subplot(1, 2, 2)\nsns.distplot(np.log1p(train.SalePrice + 1), kde=True, fit = norm)\nplt.xlabel('Log SalePrice')","93e2689c":"#Applying log to house price\ntrain.SalePrice = np.log1p(train.SalePrice)","4c4688a8":"train_y = train.SalePrice.reset_index(drop=True)\ntrain_x = train.drop(['SalePrice'], axis=1)\ntest_x = test","58d52f39":"train_x.shape","826b53ba":"test_x.shape","fee05a31":"total_features = pd.concat([train_x, test_x]).reset_index(drop=True)\ntotal_features.shape","42a91024":"nulls = np.sum(total_features.isnull())\nnullcols = nulls.loc[(nulls != 0)]\ndtypes = total_features.dtypes\ndtypes2 = dtypes.loc[(nulls != 0)]\ninfo = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)\nprint(info)\nprint(\"There are\", len(nullcols), \"columns with missing values\")","7762a666":"total_features['Functional'] = total_features['Functional'].fillna('Typ')\ntotal_features['Electrical'] = total_features['Electrical'].fillna(\"SBrkr\")\ntotal_features['KitchenQual'] = total_features['KitchenQual'].fillna(\"TA\")\n\ntotal_features['Exterior1st'] = total_features['Exterior1st'].fillna(total_features['Exterior1st'].mode()[0])\ntotal_features['Exterior2nd'] = total_features['Exterior2nd'].fillna(total_features['Exterior2nd'].mode()[0])\n\ntotal_features['SaleType'] = total_features['SaleType'].fillna(total_features['SaleType'].mode()[0])","2143a304":"pd.set_option('max_columns', None)\ntotal_features[total_features['PoolArea'] > 0 & total_features['PoolQC'].isnull()]","f78d4a33":"total_features.loc[2418, 'PoolQC'] = 'Fa'\ntotal_features.loc[2501, 'PoolQC'] = 'Gd'\ntotal_features.loc[2597, 'PoolQC'] = 'Fa'","e4cc371e":"pd.set_option('max_columns', None)\ntotal_features[(total_features['GarageType'] == 'Detchd') & total_features['GarageYrBlt'].isnull()]","a0aa08bf":"total_features.loc[2124, 'GarageYrBlt'] = total_features['GarageYrBlt'].median()\ntotal_features.loc[2574, 'GarageYrBlt'] = total_features['GarageYrBlt'].median()\n\ntotal_features.loc[2124, 'GarageFinish'] = total_features['GarageFinish'].mode()[0]\ntotal_features.loc[2574, 'GarageFinish'] = total_features['GarageFinish'].mode()[0]\n\ntotal_features.loc[2574, 'GarageCars'] = total_features['GarageCars'].median()\n\ntotal_features.loc[2124, 'GarageArea'] = total_features['GarageArea'].median()\ntotal_features.loc[2574, 'GarageArea'] = total_features['GarageArea'].median()\n\ntotal_features.loc[2124, 'GarageQual'] = total_features['GarageQual'].mode()[0]\ntotal_features.loc[2574, 'GarageQual'] = total_features['GarageQual'].mode()[0]\n\ntotal_features.loc[2124, 'GarageCond'] = total_features['GarageCond'].mode()[0]\ntotal_features.loc[2574, 'GarageCond'] = total_features['GarageCond'].mode()[0]","aa5ff3bb":"# Basement Variables with NA, are now filled\n\ntotal_features.loc[332, 'BsmtFinType2'] = 'ALQ' #since smaller than SF1\ntotal_features.loc[947, 'BsmtExposure'] = 'No' \ntotal_features.loc[1485, 'BsmtExposure'] = 'No'\ntotal_features.loc[2038, 'BsmtCond'] = 'TA'\ntotal_features.loc[2183, 'BsmtCond'] = 'TA'\ntotal_features.loc[2215, 'BsmtQual'] = 'Po' #v small basement so let's do Poor.\ntotal_features.loc[2216, 'BsmtQual'] = 'Fa' #similar but a bit bigger.\ntotal_features.loc[2346, 'BsmtExposure'] = 'No' #unfinished bsmt so prob not.\ntotal_features.loc[2522, 'BsmtCond'] = 'Gd' #cause ALQ for bsmtfintype1","7b13773f":"subclass_group = total_features.groupby('MSSubClass')\nZoning_modes = subclass_group['MSZoning'].apply(lambda x : x.mode()[0])\ntotal_features['MSZoning'] = total_features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","70c8989a":"neighborhood_group = total_features.groupby('Neighborhood')\nlot_medians = neighborhood_group['LotFrontage'].median()\ntotal_features['LotFrontage'] = total_features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","845ea708":"#Filling in the rest of the NA's\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in total_features.columns:\n    if total_features[i].dtype in numeric_dtypes: \n        numerics.append(i)\n        \ntotal_features.update(total_features[numerics].fillna(0))\n\n# remaining columns \n\ncolumns = [\"PoolQC\" , \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\", \"GarageQual\", \n         \"GarageCond\", \"GarageFinish\", \"GarageYrBlt\", \"GarageType\", \"BsmtExposure\", \n         \"BsmtCond\", \"BsmtQual\", \"BsmtFinType2\", \"BsmtFinType1\", \"MasVnrType\"]\n\nfor col in columns:\n    total_features.update(total_features[col].fillna(\"None\", inplace=True))\n\n\nnulls = np.sum(total_features.isnull())\nnullcols = nulls.loc[(nulls != 0)]\ndtypes = total_features.dtypes\ndtypes2 = dtypes.loc[(nulls != 0)]\ninfo = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)\nprint(info)\nprint(\"There are\", len(nullcols), \"columns with missing values\")","d42e2872":"total_features = total_features.drop(['Utilities','Street'], axis=1)","e2dd6413":"#FEATURE ENGINEERING\n\ntotal_features['Total_sqr_footage'] = (total_features['BsmtFinSF1'] + total_features['BsmtFinSF2'] +\n                                 total_features['1stFlrSF'] + total_features['2ndFlrSF'])\n\ntotal_features['Total_Bathrooms'] = (total_features['FullBath'] + (0.5*total_features['HalfBath']) + \n                               total_features['BsmtFullBath'] + (0.5*total_features['BsmtHalfBath']))\n\ntotal_features['Total_porch_sf'] = (total_features['OpenPorchSF'] + total_features['3SsnPorch'] +\n                              total_features['EnclosedPorch'] + total_features['ScreenPorch'] +\n                             total_features['WoodDeckSF'])\n\n\n#simplified features\ntotal_features['haspool'] = total_features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntotal_features['has2ndfloor'] = total_features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ntotal_features['hasgarage'] = total_features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ntotal_features['hasbsmt'] = total_features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ntotal_features['hasfireplace'] = total_features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","98e6d7fb":"total_features.shape","d8642737":"final_features = pd.get_dummies(total_features).reset_index(drop=True)\nfinal_features.shape","27398426":"final_train_x = final_features.iloc[:len(train_y),:]\nfinal_test_x = final_features.iloc[len(final_train_x):,:] ","841f8780":"final_train_x.shape","16fe5a8d":"final_test_x.shape","125382ca":"#Now let's use t-SNE to reduce dimensionality down to 2D so we can plot the dataset:\n\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=42, verbose = 2)\nTSNE_X = tsne.fit_transform(final_train_x)\nTSNE_X_test = tsne.fit_transform(final_test_x)","51340de3":"plt.figure(figsize=(13,10))\nplt.scatter(TSNE_X[:, 0], TSNE_X[:, 1], c=train_y, cmap=\"jet\")\nplt.axis('off')\nplt.colorbar()\nplt.show()","a7d736eb":"from sklearn.decomposition import PCA\n\nPCA_train_x = PCA(n_components=300, random_state=42).fit_transform(final_train_x)\nplt.scatter(PCA_train_x[:, 0], PCA_train_x[:, 1], c=train_y, cmap=\"jet\")\nplt.axis('off')\nplt.colorbar()\nplt.show()","f81673bd":"from sklearn.decomposition import KernelPCA\n\nlin_pca = KernelPCA(n_components = 2, kernel=\"linear\", fit_inverse_transform=True)\nrbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\nsig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)\n\n\nplt.figure(figsize=(11, 4))\nfor subplot, pca, title in ((131, lin_pca, \"Linear kernel\"), (132, rbf_pca, \"RBF kernel, $\\gamma=0.04$\"), \n                            (133, sig_pca, \"Sigmoid kernel, $\\gamma=10^{-3}, r=1$\")):\n       \n    plt.subplot(subplot)\n    plt.title(title, fontsize=14)\n    plt.scatter(PCA_train_x[:, 0], PCA_train_x[:, 1], c=train_y, cmap=plt.cm.hot)\n    plt.xlabel(\"$z_1$\", fontsize=18)\n    if subplot == 131:\n        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n    plt.grid(True)\n\nplt.show()","ae315800":"from sklearn.manifold import LocallyLinearEmbedding\n\nlle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\nPCA_X = lle.fit_transform(final_train_x)","f15bbbcb":"plt.title(\"Unrolled swiss roll using LLE\", fontsize=14)\nplt.scatter(PCA_X [:, 0], PCA_X [:, 1], c= train_y, cmap=plt.cm.hot)\nplt.xlabel(\"$z_1$\", fontsize=18)\nplt.ylabel(\"$z_2$\", fontsize=18)\nplt.axis([-0.100, 0.215, -0.043, 0.14])\nplt.grid(True)\nplt.show()","3e3b07ee":"pca_tsne = Pipeline([\n    (\"pca\", PCA(n_components=0.95, random_state=42)),\n    (\"tsne\", TSNE(n_components=2, random_state=42))\n])\nX_pca_tsne = pca_tsne.fit_transform(final_train_x)\nplt.title(\"PCA and TSNE\", fontsize=14)\nplt.scatter(X_pca_tsne [:, 0], X_pca_tsne [:, 1], c= train_y, cmap=plt.cm.hot)\nplt.xlabel(\"$z_1$\", fontsize=18)\nplt.ylabel(\"$z_2$\", fontsize=18)\nplt.show()","ca00c226":"#Random Forest Regressor.\nforest_class = RandomForestRegressor(random_state = 42)\n\nn_estimators = [10,70,500,700]\nmax_features = [\"auto\",'sqrt','log2']\n\nparam_grid_forest = {'n_estimators' : n_estimators, 'max_features' : max_features}\n\nrand_search_forest = RandomizedSearchCV(forest_class, param_grid_forest, cv = 4, \n                                        scoring='neg_mean_squared_error', n_jobs = -1, verbose=2)\n\nrand_search_forest.fit(final_train_x, train_y)","b4dd4409":"random_estimator = rand_search_forest.best_estimator_ \ny_pred_rf= random_estimator.predict(final_train_x)\nrf_msle = mean_squared_error(train_y, y_pred_rf)\nrf_rmsle = np.sqrt(rf_msle)\nrf_rmsle","c4fd21fe":"GB_Regressor = GradientBoostingRegressor(random_state = 42)\n\nn_estimators = [50,500]\n\nparam_grid_grad_boost_class = {'n_estimators' : n_estimators}\n\nrand_search_grad_boost_class = GridSearchCV(GB_Regressor, param_grid_grad_boost_class, cv = 4, scoring='neg_mean_squared_error', \n                               refit = True, n_jobs = -1, verbose = 2)\n\nrand_search_grad_boost_class.fit(final_train_x, train_y)","d3be8ac1":"gb_estimator = rand_search_grad_boost_class.best_estimator_ \ny_pred_gb= gb_estimator.predict(final_train_x)\ngb_msle = mean_squared_error(train_y, y_pred_gb)\ngb_rmsle = np.sqrt(gb_msle)\ngb_rmsle","b3c24de9":"en = ElasticNet()\nen.fit(final_train_x, train_y)","bba76252":"#Implement an Elastic Net regressor\n\nElasticRegressor = ElasticNet()\n\nalpha = [.0001,.0005,.005,.05,1]\n\nparam_grid_elastic = {'alpha' : alpha}\n\nrand_search_elastic = GridSearchCV(ElasticRegressor, param_grid_elastic, cv = 4, scoring='neg_mean_squared_error', \n                               refit = True, n_jobs = -1, verbose = 2)\n\nrand_search_elastic.fit(final_train_x, train_y)","b30fd7e1":"elastic_estimator = rand_search_elastic.best_estimator_\ny_pred_elastic = elastic_estimator.predict(final_train_x)\nelastic_msle = mean_squared_error(train_y, y_pred_elastic)\nelastic_rmsle = np.sqrt(elastic_msle)\nelastic_rmsle","62166af6":"#Implement a lasso regresso\n\nLassoRegressor = LassoCV()\n\nmax_iter = [50,100,500,1000]\n\nparam_grid_lasso = {'max_iter' : max_iter}\n\nrand_search_lasso = GridSearchCV(LassoRegressor, param_grid_lasso, cv = 4, scoring='neg_mean_squared_error', \n                               refit = True, n_jobs = -1, verbose = 2)\n\nrand_search_lasso.fit(final_train_x, train_y)","f68d1255":"lasso_estimator = rand_search_lasso.best_estimator_\ny_pred_lasso= lasso_estimator.predict(final_train_x)\nlasso_msle = mean_squared_error(train_y, y_pred_lasso)\nlasso_rmsle = np.sqrt(lasso_msle)\nlasso_rmsle","898eeff2":"xgb = XGBRegressor(learning_rate =0.01, n_estimators=3460, max_depth=3,\n                     min_child_weight=0 ,gamma=0, subsample=0.7,\n                     colsample_bytree=0.7,objective= 'reg:linear',\n                     nthread=4,scale_pos_weight=1,seed=27, reg_alpha=0.00006)\n\nxgb_fit = xgb.fit(final_train_x, train_y)","472da450":"y_pred_xgb= xgb_fit.predict(final_train_x)\nxgb_msle = mean_squared_error(train_y, y_pred_xgb)\nxgb_rmsle = np.sqrt(xgb_msle)\nxgb_rmsle","38c51018":"lgbm_model = LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nlgbm_fit = xgb.fit(final_train_x, train_y)","fc3a3df0":"y_pred_lgbm= lgbm_fit.predict(final_train_x)\nlgbm_msle = mean_squared_error(train_y, y_pred_lgbm)\nlgbm_rmsle = np.sqrt(lgbm_msle)\nlgbm_rmsle","cffc9bae":"y_pred_gb_test= gb_estimator.predict(final_test_x)\ny_pred_rf_test= random_estimator.predict(final_test_x)\ny_pred_las_test = lasso_estimator.predict(final_test_x)\ny_pred_elas_test = elastic_estimator.predict(final_test_x)\ny_pred_xgb_test = xgb_fit.predict(final_test_x)\ny_pred_lgbm_test = lgbm_fit.predict(final_test_x)","aa17e876":"submission = sample\nsubmission.iloc[:,1] = (np.expm1(y_pred_gb_test) + np.expm1(y_pred_rf_test) + np.expm1(y_pred_xgb_test) + np.expm1(y_pred_lgbm_test))\/ 4\nsubmission.to_csv('submission.csv', index = False)","a5d20110":"Taking the averages of best 4 models - Gradient Boosting, Light GBM, XG Boost and Random Forest. This will be improved with Stacking Regressor in the next version.","72f0b6ff":"Impute the Missing Values","c38bcb2f":"Locally Linear Embedding","850025c2":"Check what value to impute, when the variable value is missing.","96e0e9c3":"Apply TSNE ","cf18232a":"PCA - Not better than TSNE","dbc5f0fe":"Submit the first submission - with Gradient Boosting as best model","3a0429b9":"As Suggested by many participants in Kaggle that the outliers shall be removed","db471224":"PCA + TSNE","03376590":"Determine the missing values","a290f314":"None of the above techniques could clearly identify the clusters. So I will drop the idea of dropping any dimensions\n\nNow We Build the Models","d494c6da":"Kernel PCA"}}