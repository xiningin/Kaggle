{"cell_type":{"03118f75":"code","629f58ba":"code","e8c5b19b":"code","b8f4943f":"code","9a57d3bc":"code","c103b228":"code","fc7af364":"code","9bd5ed4a":"code","6a0fb0b5":"code","4c7e74e9":"code","f9cc171e":"code","aae5154a":"code","f4af1221":"code","7c4f9c3a":"code","e88e9829":"code","4c79a9fa":"code","b1895140":"code","3615a023":"code","9e34129c":"code","c01e09e4":"code","b0235673":"code","5266f51f":"code","04593867":"code","bc847855":"code","7eb00e5e":"code","10a8548b":"code","afebd7dc":"code","69095aa6":"code","8306108a":"code","f6c10785":"code","31d70a44":"code","46390b04":"code","9f4c45ad":"code","266a2882":"code","75cfe5a8":"code","3264b19e":"code","8a5ae0b3":"code","a2a149df":"code","6a2f4ac1":"code","5946c8e2":"code","365c0d4d":"code","abc8c0f8":"code","af32c188":"code","bf79da00":"code","b9a0c2e6":"code","e428fb7c":"code","d479802f":"code","334196e3":"code","7b863372":"code","497716cc":"code","8dc7cbac":"code","b1ae5b01":"code","da1e9661":"code","6339e05b":"code","f6eba28a":"code","548b65cf":"code","4d0c7eb9":"code","817ff853":"code","260c6a5b":"code","9576cb25":"code","ae50faaf":"code","b05b73ee":"code","fd5c5897":"code","adabefcf":"code","655822a5":"code","573b6c07":"code","123c8c06":"code","6e0c3add":"code","64a325e7":"code","1af66046":"code","e481b1c4":"code","d60467c0":"code","36535fc4":"code","e8e738ee":"code","defc0b48":"code","b86939c6":"code","69e28712":"code","7ca9b85e":"code","230dc173":"code","854d7e38":"code","2b400870":"code","04019ae1":"code","ae5784b1":"code","ad9336c2":"code","ff97848f":"code","a46e5215":"code","a8d52b96":"code","602d3ae2":"code","b1653fc5":"code","15fbde41":"code","53f5a3d4":"code","7a3b12b0":"code","9734f5df":"code","b6f4ae6c":"code","d4c11b29":"code","9205fb0b":"code","59ec4423":"code","c86c9ed7":"code","6f89d730":"code","bcfc9550":"code","d3a1d7ae":"code","32a0a184":"code","cc5bfa0f":"code","cd4ad203":"code","413baf58":"code","192fcb1b":"code","0383902b":"code","5c0f51e5":"code","9a1374a8":"code","9b140864":"code","92018886":"code","90e087d8":"code","9f20ebb1":"code","db14b659":"code","44c20544":"code","27c5734c":"code","eec59369":"code","ce92e88b":"code","4e8affe8":"code","f952ec46":"code","7608c099":"code","f078e670":"code","31af8d60":"code","e54c69ee":"code","5b3c1277":"code","3ede7d10":"code","85402ccb":"code","c49b5acb":"code","c6c913f7":"code","ab8cd2a1":"code","62347ce9":"code","2aa9bce8":"code","9b96a297":"code","5eaf0196":"code","6fa50341":"code","be506a3c":"code","4a7ba3e1":"code","5ce50d48":"code","3a219524":"code","8bd0d5f2":"code","948a9e4e":"code","9d37564a":"code","05997e8d":"code","9759622a":"code","3d1ecccf":"code","128f551d":"code","a9ca7e34":"code","53fd0d6b":"code","9a102637":"code","085b19bb":"code","481ab23c":"code","edbdaab0":"code","7bc26098":"code","022f9a85":"code","593bcdf5":"code","1335da8f":"code","c4bbd49b":"code","09ccac05":"code","adaa943c":"code","7f3c9cb4":"code","ff609ce1":"code","26017ca7":"code","edf7fc23":"code","e67e3d2c":"code","3e315c64":"code","9a972979":"code","1023acb7":"code","ed5ec63e":"code","f659bebf":"code","710aa219":"code","39bf9b86":"code","9ed312ef":"code","9eac040e":"code","2327dab9":"code","41190d2f":"code","6cd7a196":"code","ce6509c1":"code","044e12a9":"code","25b9654d":"code","187192d7":"code","4f146ff1":"code","67bab596":"code","ff721b3a":"code","4e88a5a9":"code","daba0213":"code","0dd3e8dc":"code","e8724bbe":"code","55414369":"code","3938ed07":"code","4b34c017":"code","17cfe146":"code","95d1cd47":"code","2de4b5b1":"code","ba1a9464":"code","68735763":"code","ff610bb1":"code","9c5e40a7":"code","9b51d045":"code","4ccc2ddf":"code","b82e50a6":"code","b2edf939":"code","1ab35c2d":"code","0f588a90":"code","f2e42c1c":"code","5efd96a8":"code","cd691815":"code","a58add94":"code","87835c15":"code","0b5ebec6":"code","77796a0b":"code","71d76739":"code","03865393":"code","9b674c96":"code","ff56fd16":"code","71d137bc":"code","8132988d":"code","f1e83960":"code","82af6ab6":"code","dfd200ff":"code","d9e57226":"code","4121960e":"code","499e0be1":"code","32f5ce62":"code","30718426":"code","2601e290":"code","8b1fd549":"code","297a89f1":"code","ebbbc0e1":"code","23557e70":"code","56873793":"code","57406bd8":"code","bcaee011":"code","8014f729":"code","b2e023fd":"code","caac1eef":"code","a87d6cb5":"code","e0296cfe":"code","8bdd9965":"code","f213a78f":"code","14ef4665":"code","abfca0ee":"code","42a960ef":"code","8a223a81":"code","679c0d8f":"code","abaed2fe":"code","333bfdf0":"code","337fce22":"code","1470be53":"code","8189b73c":"code","5d6ee32a":"code","a9bacc32":"code","18a9c90c":"code","f84c1f65":"code","861421cb":"code","6e749022":"code","f8367d4d":"code","43b5acc4":"code","c3fff189":"code","929e5272":"code","19857338":"code","a9e5ca99":"code","40f72498":"code","300eb4c6":"code","42887f04":"code","8f170319":"code","f26f6600":"code","25d71e7d":"code","1e329520":"code","e649ef12":"code","df2d1fc6":"code","5726ad5d":"code","79feb854":"code","b15bf457":"code","244b862a":"code","8d6856bf":"code","6793feb7":"code","99a30ce2":"code","967cc490":"code","09415283":"code","78696349":"code","3a22722d":"code","54543ab4":"code","d5fe1f71":"code","0f574499":"code","b8dad243":"code","b1361261":"code","a33dcea2":"code","054c644e":"code","de21e49d":"code","c1584156":"code","d4e143cf":"code","fa547b9a":"code","60f20726":"code","894b5dc3":"code","aace064a":"code","32f7f409":"code","a6e4eb50":"code","9e7a50a6":"code","d5f969e8":"code","31d563cb":"code","46426a1e":"code","2356f8e3":"code","28419b79":"code","5e732187":"code","51dc42e6":"code","ca11de1e":"code","5b3b3cb5":"code","be1c3d71":"code","a4d703ca":"code","2df6181f":"code","23e97b03":"code","b7f911ee":"code","9dcab69c":"code","3a8ac6d3":"code","9ffece51":"code","d0770a02":"code","25bf4121":"code","bf5edd7b":"code","8c57ac57":"code","35cd5c51":"code","5d36c81d":"code","9a705db2":"code","59925d49":"code","f0295869":"code","e3b6c4cf":"code","668ce78f":"code","9e4dc65e":"code","cfdccba1":"code","936daaa8":"code","7f19cb7c":"code","d4bc480a":"code","eb3bebfc":"code","b1ae702b":"code","82e2faf9":"code","69c3b9d0":"code","2d87f3d5":"code","801f249e":"code","2a516e8d":"code","c110a767":"code","06e3131e":"code","577c1ffe":"code","0572c64b":"code","54d42058":"code","ddee71ab":"code","0db36ebd":"code","76168bb1":"code","1fd50029":"code","7f529353":"code","8fd00d72":"code","af921e3f":"code","ff9a161c":"code","273f5ea0":"code","35149c7d":"code","37b038fd":"code","d00a66ed":"code","829f24ff":"code","34d12b19":"code","dbf2143f":"code","410ad0c5":"code","31e268f4":"code","2df5230b":"code","35bd5b86":"code","7a8529ac":"code","51b16225":"code","576fafd1":"code","12dbf43a":"code","48db1542":"code","0006310e":"code","6cb6d0d4":"code","c3624505":"code","11f0cbcb":"code","e7ca18dc":"code","d1ce29e5":"code","70538091":"code","2608bbe3":"code","19aeb5ff":"code","eaa28d19":"code","1ce682ec":"code","c9546150":"code","dff14de2":"code","b943d29c":"code","e638c22f":"code","4adc0089":"code","6fb3f842":"code","36506ec6":"code","f1a92f8f":"code","475f82a7":"code","6d395ad2":"code","a81a2eba":"code","de3299ed":"code","31ecf18e":"code","5d44bed2":"code","317b00fe":"code","87aa90be":"code","81c47379":"code","037bad1d":"code","076a01b6":"code","8f307dec":"code","c62f4e7b":"code","dc1154e2":"code","10f195ab":"code","30cd36aa":"code","07501419":"code","2efb95c4":"code","020f9e6e":"code","890418b6":"code","66e7411a":"code","9da4f6f6":"code","bb41bebd":"code","6bd8b3dd":"code","10d0649b":"code","455756ec":"code","b3db5485":"code","865bf2d0":"code","189111eb":"code","3a102e83":"code","68ef2b14":"code","b9dcc5f1":"code","868db0bc":"code","f7b88f2b":"code","fc739a5b":"code","301d8d9c":"code","0d263934":"code","edd05084":"code","dd2f4153":"code","d9912df6":"code","d03999d2":"code","cb818cde":"code","3adf8668":"code","92045b2e":"code","13040eba":"code","931bcbe8":"code","a29bc68b":"code","4555e0fc":"code","c5d68df3":"code","d54bce88":"code","8d3e7bd4":"code","e304dc04":"code","e8ec94bd":"code","9872aca3":"code","ccf1e1f7":"code","586a9d2a":"code","46570a9d":"code","e36b4afa":"code","e1768269":"code","9d370c54":"code","0e03f980":"code","14b75c74":"code","7ec69468":"code","8a620368":"code","cf3cbea1":"code","cfcf9243":"code","5f8b2375":"code","6bf1d24b":"code","2bc60511":"code","59c2c1a6":"code","022867ca":"code","a149684e":"code","ec070b9a":"code","8dc7e463":"code","16a8e1bd":"code","3cc554e1":"code","d28d5ef2":"code","f755f419":"code","cabe9168":"code","91b33457":"code","c78a6eb1":"code","a4e94ecb":"code","783706dd":"code","b6b72cb2":"code","d0630f25":"code","028da516":"code","b1c43e9f":"code","139c5d90":"code","5cf9df55":"code","867c16dc":"code","34f7dc96":"code","9ddb32df":"code","80ca3504":"code","6dd837f3":"code","90c630c4":"code","c6330ed7":"code","6a49a22d":"code","455d4943":"code","d21c634b":"code","0414f5c0":"code","f76ded64":"code","3526c200":"code","aa1a2c98":"code","f5244519":"code","dbddffb2":"code","785ba5b6":"code","86c3f6c8":"code","b76f4844":"code","aaf2b9f3":"code","83964a28":"code","14083d34":"code","9791ce4f":"code","03eb70ea":"code","85e66e90":"code","f0cb7a6f":"code","2409b405":"code","5ab9a6cc":"code","6770b476":"code","450f08cd":"code","5cd2fdf2":"code","33ab3e3d":"code","8e495157":"code","68235a91":"code","fd7ea5bf":"code","de355ff5":"code","0f76099e":"code","f5cbfa9d":"code","7a744561":"code","b07bb310":"code","a1ee14fb":"code","da171baa":"code","b043aac7":"code","c612c450":"code","e007b514":"code","85fbecb1":"code","ae88568b":"code","b7bd1840":"code","1430091d":"code","8b3d1a2c":"code","94c49d7c":"code","1872a08c":"code","d69c4aab":"code","ae685d31":"code","bb9610db":"code","4810abc0":"code","d8ef09eb":"code","12d5bd70":"code","3e773460":"code","94f7ef64":"code","738972a8":"code","49f68cd5":"code","fb7453ad":"code","706a48b5":"code","5517889f":"code","b5aa4203":"code","145411da":"code","d39e9df8":"code","7c733ae3":"code","126034a5":"code","ecb0a529":"code","e4ebe715":"code","fba2fc32":"code","a42f6cd3":"code","f6319018":"code","51d2002b":"code","d773ca08":"code","a72eae5c":"code","0e9cf3b4":"code","25096cf3":"code","4feffa03":"code","0f0d1d4d":"code","6decd944":"code","05ffc91d":"code","f0b6cd31":"code","c0553995":"code","e04bd132":"code","7db7e4bf":"code","e4336463":"code","f5bdf2e0":"code","27ec155a":"markdown","24ea4569":"markdown","467f32b7":"markdown","bbeeb3fd":"markdown","bd5b8522":"markdown","e2d97526":"markdown","74089141":"markdown","fb947062":"markdown","ab1e56c8":"markdown","a4a2b3c5":"markdown","7183e01a":"markdown","376d079b":"markdown","ba7ed630":"markdown","cedd4d60":"markdown","da6f742e":"markdown","01bddeba":"markdown","ad735ab7":"markdown","4ee1a615":"markdown","532d053b":"markdown","9e50a5a6":"markdown","632fba7e":"markdown","f99491c6":"markdown","07729218":"markdown","324d3d51":"markdown","277fdd56":"markdown","7ddd1f2d":"markdown","21a9371d":"markdown","d470c4ce":"markdown","9cc78060":"markdown","b0884f05":"markdown","4e4dd1ec":"markdown","e16caa77":"markdown","3f15f27a":"markdown","7c51f321":"markdown","81d67e1f":"markdown","8ab4bba9":"markdown","1adac7f6":"markdown","eb95721f":"markdown","fac12791":"markdown","863e82f3":"markdown","94acda9b":"markdown","b442980f":"markdown","f6dc8d41":"markdown","9856f9b6":"markdown","3fcb829a":"markdown","dae3f28f":"markdown","8e85f729":"markdown","301fc901":"markdown","39e738dd":"markdown","072ae7ec":"markdown","aedc8542":"markdown","fef5b579":"markdown","f1d39adb":"markdown","e6b2c05e":"markdown","2fbce844":"markdown","b1d43a9a":"markdown","5ae5d20b":"markdown","83e6759b":"markdown","78595b82":"markdown","569a796b":"markdown","1f184c1f":"markdown","76274c4e":"markdown","a7e08306":"markdown","dbb3ca63":"markdown","3db36cd1":"markdown","ceb246db":"markdown","a1eea9af":"markdown","44e47190":"markdown","45860bbd":"markdown","31e0e1be":"markdown","09c0c72a":"markdown","b3446310":"markdown","3b9f9550":"markdown","7b89916b":"markdown","f9a6fa39":"markdown","c0f4b0bf":"markdown","acec9d43":"markdown","196b6f79":"markdown","6637caae":"markdown","8e67178e":"markdown","e583653c":"markdown","c83387d4":"markdown","b2ca92fe":"markdown","ac3ad139":"markdown","c1e7d2c9":"markdown","53378c1e":"markdown","6909f2ed":"markdown","c983f5a6":"markdown","8ce76607":"markdown","acd5a6f4":"markdown","f2551290":"markdown","a815d8e2":"markdown","30281e4a":"markdown","533cc988":"markdown","8d948ec8":"markdown","d589bb7a":"markdown","035833a9":"markdown","4ad284ef":"markdown","2457bf3b":"markdown","2884bef5":"markdown","c362259e":"markdown","c5556b2c":"markdown","3f6762b4":"markdown","6c9374ed":"markdown","e1ceaa55":"markdown","c1f813c7":"markdown","75ef34ea":"markdown","9b66ceab":"markdown","ced6ba66":"markdown","9deae734":"markdown","7f7c4634":"markdown","b13be584":"markdown","cc6f9226":"markdown","5fc9eacb":"markdown","a9155991":"markdown","1d390828":"markdown","6c6488d4":"markdown","04bb244c":"markdown","bb45b147":"markdown","dde7fcce":"markdown","440282b9":"markdown","d6a20870":"markdown","725becf3":"markdown","95333c5d":"markdown","4b98c5e0":"markdown","567aedbc":"markdown","14b8bbd2":"markdown","e81fee7d":"markdown","e168f6c7":"markdown","0410e71c":"markdown","2f72c2e5":"markdown","5357d9df":"markdown","ee4c23e0":"markdown","a01f6e1c":"markdown","222b25d2":"markdown","c5fdcf74":"markdown","26a28398":"markdown","02cff3ee":"markdown","f25827a6":"markdown","7faef508":"markdown","2b2c2a7b":"markdown","19da2393":"markdown","16af8971":"markdown","68ce3f15":"markdown","d6d6ac72":"markdown","d0d6b362":"markdown","4a77f60a":"markdown","a537b069":"markdown","07911d0f":"markdown","affbfca0":"markdown","7fe8fac6":"markdown","381980cc":"markdown","0eef9ecb":"markdown","3b925523":"markdown","560415e4":"markdown","e6e70871":"markdown","5b4eb42d":"markdown","8f701098":"markdown","2f92fa67":"markdown","06737b32":"markdown","60f55327":"markdown","75b5413d":"markdown","44c6a81b":"markdown","be3bac93":"markdown","248d7e12":"markdown","ee0eaaa4":"markdown","ccfaf06a":"markdown","4d5e1bcd":"markdown","ed405cf0":"markdown","87ef674f":"markdown","94f7653c":"markdown","b2b1e23e":"markdown","1ce8b95b":"markdown","12b54c9c":"markdown","643e69ea":"markdown","b3a66392":"markdown","d702b5d0":"markdown","52fcb044":"markdown","90799d62":"markdown","a9212910":"markdown","272a4b51":"markdown","6ffb6459":"markdown","72cede1d":"markdown","98c2902c":"markdown","3793e412":"markdown","2d6a5137":"markdown","778f34fe":"markdown","d189d480":"markdown","9e7ffb11":"markdown","a0835aa5":"markdown","30c10363":"markdown","dbbcec0c":"markdown","64a00749":"markdown","19294156":"markdown","728b5240":"markdown","6cdb16f3":"markdown","5042cb71":"markdown","d23cad78":"markdown","789d0b20":"markdown","b884aaca":"markdown","4ba47e3a":"markdown","5ce97079":"markdown","354a33f0":"markdown","2a5f3671":"markdown"},"source":{"03118f75":"# IMPORTING PACKAGES\nimport pandas as pd # data processing\nimport numpy as np # working with arrays\nimport matplotlib.pyplot as plt # visualization\n#from termcolor import colored as cl # text customization\nimport itertools # advanced tools\n\nfrom sklearn.preprocessing import StandardScaler # data normalization\nfrom sklearn.model_selection import train_test_split # data split\nfrom sklearn.tree import DecisionTreeClassifier # Decision tree algorithm\nfrom sklearn.neighbors import KNeighborsClassifier # KNN algorithm\nfrom sklearn.linear_model import LogisticRegression # Logistic regression algorithm\nfrom sklearn.svm import SVC # SVM algorithm\nfrom sklearn.ensemble import RandomForestClassifier # Random forest tree algorithm\nfrom xgboost import XGBClassifier # XGBoost algorithm\n\nfrom sklearn.metrics import confusion_matrix # evaluation metric\nfrom sklearn.metrics import accuracy_score # evaluation metric\nfrom sklearn.metrics import f1_score # evaluation metric","629f58ba":"from xgboost import XGBClassifier # XGBoost algorithm\n#You instal from anaconda power shell\n#conda install -c conda-forge xgboost","e8c5b19b":"#IMPORT SYSTEM","b8f4943f":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn import preprocessing","9a57d3bc":"import seaborn as sns;sns.set()\n\nfrom sklearn.linear_model    import LogisticRegression\nfrom sklearn.tree            import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing   import StandardScaler\nfrom sklearn.metrics         import confusion_matrix,classification_report,accuracy_score,roc_auc_score,roc_curve\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom IPython.display         import Image\nfrom sklearn.tree            import export_graphviz\n","c103b228":"try:\n    import datetime\n    import foo\n    import sys\nexcept ImportError:\n    pass","fc7af364":"#(sep = ',', error_bad_lines=False)","9bd5ed4a":"#just example\ndf=rawdata(without balanced data,i.e.Majority class(Genuine Transaction) and Minority clas(Fraud Transaction))\n\ndf1=refined data(with balanced data,i.e.Majority class and minority class are balanced using vaarious techniques\n\"Oversampling,SMOTE,ADASYN,SMOTE,Tomek link\" etc)","6a0fb0b5":"#Let's Load the dataset","4c7e74e9":"#!pwd\nimport pandas as pd\ndf=pd.read_csv(r'C:\\Users\\dell\\Downloads\\archive\\creditcard.csv')\n#open('\/users\/dell\/Desktop\/map\/data.csv')","f9cc171e":"#LABELS = [\"Normal\", \"Fraud\"]","aae5154a":"#fraud.Amount.describe()","f4af1221":"df.columns\n","7c4f9c3a":"\ndf.Amount.describe()","e88e9829":"#df = pd.read_csv('creditcard.csv')\n#df.drop('Time', axis = 1, inplace = True)\nprint(df.head())","4c79a9fa":"print(df.head())","b1895140":"print (df.columns)\n","3615a023":"#another way of getting columns printed","9e34129c":"genuine=df.loc[df['Class']==0]\nfraud=df.loc[df['Class']==1]\ngenuine\n","c01e09e4":"genuine","b0235673":"fraud","5266f51f":"genuine.count()","04593867":"fraud.count()","bc847855":"fraud.sum()","7eb00e5e":"genuine.sum()","10a8548b":"fraud.Time.describe()","afebd7dc":"fraud.Class.describe()","69095aa6":"genuine.Time.describe()","8306108a":"genuine.Amount.describe()","f6c10785":"genuine.Time.describe()","31d70a44":"#df.concat([fraud.Amount.describe(), normal.Amount.describe()], axis=1)","46390b04":"fraud.Amount.describe()","9f4c45ad":"len(fraud)","266a2882":"len(genuine)","75cfe5a8":"df.shape","3264b19e":"df.columns","8a5ae0b3":"#Lets understand the shape of the data","a2a149df":"df.shape","6a2f4ac1":"#here in the above,we have 284807 records i.e.rows and 31 columns i.e.variables","5946c8e2":"print(df.columns)","365c0d4d":"#here no of records getting printed as Fraud Transactions and genuine transactions","abc8c0f8":"df['Time'].value_counts()","af32c188":"df['Amount'].value_counts()","bf79da00":"df['Class'].value_counts()","b9a0c2e6":"# plot the time feature\nplt.figure(figsize=(10,8))\n\nplt.subplot(2, 2, 1)\nplt.title('Time Distribution (Seconds)')\n\nsns.distplot(df['Time'], color='blue');\n\n#plot the amount feature\nplt.subplot(2, 2, 2)\nplt.title('Amount Distribution')\nsns.distplot(df['Amount'],color='blue');","e428fb7c":"# data[data.Class == 0].Time.hist(bins=35, color='blue', alpha=0.6)\nplt.figure(figsize=(12, 10))\n\nplt.subplot(2, 2, 1)\ndf[df.Class == 1].Time.hist(bins=35, color='blue', alpha=0.6, label=\"Fraudulant Transaction\")\nplt.show()\n#plt.legend()\n\nplt.subplot(2, 2, 2)\ndf[df.Class == 0].Time.hist(bins=35, color='blue', alpha=0.6, label=\"Non Fraudulant Transaction\")\nplt.show()\n#plt.legend()","d479802f":"df['Amount'].value_counts()","334196e3":"df['Class'].value_counts()","7b863372":"#lets understand the sahpe of the dataset","497716cc":"print(df.shape)","8dc7cbac":"#lets understand the type of the data","b1ae5b01":"#lets understand the type of the dataset","da1e9661":"df.dtypes","6339e05b":"#here lets check if any missing values present in the dataset","f6eba28a":"df.isnull().sum()","548b65cf":"#here first five records are getting printed ","4d0c7eb9":"df","817ff853":"df.describe()","260c6a5b":"df.head()","9576cb25":"#here first 10 records atre getting printed","ae50faaf":"df.head(10)","b05b73ee":"#here first 15 records or rows are getting printed\ndf.head(15)","fd5c5897":"#here first 25 records\/rows are getting printed\ndf.head(25)","adabefcf":"#here last five records are getting printed\ndf.tail()","655822a5":"#here last 10 records are getting printed\ndf.tail(10)","573b6c07":"df.tail(15)","123c8c06":"df.tail(20)","6e0c3add":"#last 25 records will be displayed\ndf.tail(25)","64a325e7":"df.info","1af66046":"#### Here 284807 records\/rows and 31 columns has shown in the above","e481b1c4":"print(df)","d60467c0":"print(df.describe())","36535fc4":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set\nsns.set_style(\"whitegrid\")\n#sns.FaceGrid(df,hue=\"Class\",size=6).map(plt.scatter,\"Time\",\"Amount\").add_legend()\nplt.show()\n#sns.plt.show()","e8e738ee":"#sns.relplot(x='Amount',y='Time',hue=\"Class\",df=df)","defc0b48":"#sns.catplot(x='Amount',y='Time',hue=\"Class\",df=df)","b86939c6":"from sklearn import linear_model","69e28712":"from sklearn.model_selection import train_test_split","7ca9b85e":"#bifurcation of train and test dataset\n","230dc173":"x=df.iloc[:,:-1]\ny=df['Class']","854d7e38":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.35)","2b400870":"clf=linear_model.LogisticRegression()","04019ae1":"#sklearn.utils._pprint\n#clf.fit(x,y)\n#clf = clf.fit(x,y)","ae5784b1":"#!pip install joblib\n#import joblib","ad9336c2":"clf.fit(x_train,y_train)","ff97848f":"y_pred=clf.predict(x_test)","a46e5215":"y_pred=np.array(clf.predict(x_test))","a8d52b96":"y=np.array(y_test)","602d3ae2":"from sklearn.metrics import confusion_matrix,classification_report,accuracy_score","b1653fc5":"print(confusion_matrix(y_test,y_pred))","15fbde41":"print(confusion_matrix(y,y_pred))","53f5a3d4":"print(accuracy_score(y_test,y_pred))","7a3b12b0":"print(accuracy_score(y,y_pred))","9734f5df":"print(classification_report(y_test,y_pred))","b6f4ae6c":"print(classification_report(y,y_pred))","d4c11b29":"import findspark\nfindspark.init()\n\nimport pyspark # only run after findspark.init()","9205fb0b":"string = sys.argv[2].split(\",\")","59ec4423":"#string.split(sys.argv[2], ',')","c86c9ed7":"sys.argv[2].split(',')","6f89d730":"#string = split(\",\",sys.argv[2])","bcfc9550":"# XGBoost\nfrom pyspark.sql.functions import split\nfrom pyspark.sql.functions import *\nfrom xgboost import XGBClassifier\n\nnum_depth = [3, 4]\nlearning_rate = [0.1, 0.2, 0.3]\ninitial_trees = 1126\nnum_trees = 500\ntree_increment = 1\ntrees = range(initial_trees, num_trees+1, tree_increment)\nauc = np.zeros((len(trees)*len(num_depth)*len(learning_rate), num_cv_splits))\ncv_num = 0\nfor train_index, test_index in skf.split(X_train, y_train):\n    X_train_cv, X_test_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n    y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n    for lr_id, lr in enumerate(learning_rate):\n        for depth_id, depth in enumerate(num_depth):\n            clf = XGBClassifier(learning_rate=lr, max_depth=depth, min_child_weight=30, n_estimators=initial_trees,\n                                n_jobs=56)\n            clf.fit(X_train_cv, y_train_cv)\n            clf.save_model('lr_'+str(lr)+'_depth_'+str(depth)+'_tree_'+str(initial_trees)+'_cv_num_'+str(cv_num)+\\\n                           '.model')\n            auc_id = lr_id*len(num_depth)*len(trees)+depth_id*len(trees)\n            auc[auc_id, cv_num] = metrics.roc_auc_score(\n                y_true = y_test_cv, y_score = clf.predict_proba(X_test_cv)[:,1])\n            print('learning rate =', lr, '; depth =', depth, '; num_trees = ', initial_trees, '; auc =', auc[auc_id,\n                    cv_num], '; cv_num =', cv_num)\n            for tree_id,tree in enumerate(trees[1:]):\n                clf = XGBClassifier(learning_rate=lr, max_depth=depth, min_child_weight=30,\n                                    n_estimators=tree_increment, n_jobs=56)\n                clf.fit(X_train_cv, y_train_cv, xgb_model=(\n                    'lr_'+str(lr)+'_depth_'+str(depth)+'_tree_'+str(trees[tree_id])+'_cv_num_'+str(cv_num)+'.model'))\n                clf.save_model('lr_'+str(lr)+'_depth_'+str(depth)+'_tree_'+str(tree)+'_cv_num_'+str(cv_num)+'.model')\n                auc_id = lr_id*len(num_depth)*len(trees)+depth_id*len(trees)+tree_id+1\n                auc[auc_id, cv_num] = metrics.roc_auc_score(\n                    y_true = y_test_cv, y_score = clf.predict_proba(X_test_cv)[:,1])\n                print('learning rate =', lr, '; depth =', depth, '; num_trees =', tree, '; auc =', auc[auc_id,\n                        cv_num], '; cv_num =', cv_num)\n    cv_num += 1\nmean_auc = np.mean(auc, axis=1)\nlr_id = np.argmax(mean_auc)\/\/(len(num_depth)*len(trees))\ndepth_id = (np.argmax(mean_auc) - lr_id*len(num_depth)*len(trees))\/\/len(trees)\ntree_id = (np.argmax(mean_auc) - lr_id*len(num_depth)*len(trees))%len(trees)\nprint('Best Learning Rate:', learning_rate[lr_id])\nprint('Best Depth:', num_depth[depth_id])\nprint('Best Trees:', trees[tree_id])\nprint('Best auc corresponding to Best Learning Rate, Depth & Trees :', mean_auc[np.argmax(mean_auc)])\n# Best Learning Rate: 0.2\n# Best Depth: 3\n# Best Trees: 109\n# Best auc corresponding to Best Learning Rate, Depth & Trees : 0.9803831434994882","d3a1d7ae":"#feature density plot\nvar = df.columns.values\n\ny = 0\nn0 = df.loc[df['Class'] == 0]\nn1 = df.loc[df['Class'] == 1]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(8,4,figsize=(16,27))\n\nfor feature in var:\n    y += 1\n    plt.subplot(8,4,y)\n    sns.kdeplot(n0[feature], bw=0.5,label=\"Class = 0\")\n    sns.kdeplot(n1[feature], bw=0.5,label=\"Class = 1\")\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","32a0a184":"import seaborn as sns\nsns.set_style(\"whitegrid\")\n#sns.FaceGrid(df,hue=\"Class\",size=6).map(plt.scatter,\"Amount\",\"Time\",).add_legend()\nplt.show()\n","cc5bfa0f":"#tic=time.time()","cd4ad203":"df['Time'] = pd.to_datetime(df['Time'], format='%d\/%m\/%Y %H:%M', errors='coerce')","413baf58":"FilteredData=df[['Amount']]\nFilteredData","192fcb1b":"FilteredData=df[['Class']]\nFilteredData","0383902b":"FilteredData=df[['Amount','Class']]\nFilteredData","5c0f51e5":"df.columns","9a1374a8":"FilteredData=df[['Time']]\nFilteredData","9b140864":"\nFilteredData=df[['Amount','Class']]\nFilteredData","92018886":"FilteredData=df[['Amount','Class','Time']]\nFilteredData","90e087d8":"df['Amount'].value_counts()","9f20ebb1":"df['Class'].value_counts()","db14b659":"#df[['Date', 'Time']]\n#pd.to_datetime(df['Date', 'Time'])\n#pd.to_datetime(df.Date + ' ' + df.Time)","44c20544":"df['Time'].value_counts()","27c5734c":"#getting filtered data w.r.t.three columns Amount,Class and Time\nprint(FilteredData.shape)","eec59369":"plt.close;\nsns.set_style(\"whitegrid\");\nsns.pairplot(FilteredData,hue=\"Class\",size=5);\nplt.show","ce92e88b":"plt.close;\nsns.set_style(\"whitegrid\");\nsns.pairplot(FilteredData,hue=\"Amount\",size=5);\nplt.show","4e8affe8":"plt.close;\nsns.set_style(\"whitegrid\");\nsns.pairplot(FilteredData,hue=\"Time\",size=5);\nplt.show","f952ec46":"countLess=0\ncountMore=0\nfor i in range(284806):\n    if FilteredData.iloc[i][\"Amount\"]<(2500):\n        countLess=countLess+1\n    else:\n        countMore=countMore+1\nprint(countLess)\nprint(countMore)","7608c099":"#finding out the percentage count\npercentage=(countLess\/284807)*100\npercentage\n","f078e670":"#Another way to findout the Genuine and fraud transactions using for loop as class0 and class1","31af8d60":"class0=0\nclass1=0\nfor i in range(284806):\n     if FilteredData.iloc[i][\"Amount\"]<(2500):\n        class0=class0+1\n     else:\n        class1=class1+1\nprint(class0)\nprint(class1)","e54c69ee":"#finding out the percentage count\npercentage=(countLess\/284807)*100\npercentage\n","5b3c1277":"FilteredData['Class'].value_counts()","3ede7d10":"creditCard_genuine=FilteredData.loc[FilteredData[\"Class\"]==0]\ncreditCard_fraud=FilteredData.loc[FilteredData[\"Class\"]==1]","85402ccb":"creditCard_genuine","c49b5acb":"creditCard_fraud","c6c913f7":"#plt.plot(creditCard_genuine[\"Time\"],np.zeros_like(creditCard_genuine[\"Time\"]),\"o\")\n#plt.plot(creditCard_fraud[\"Time\"],np.zeros_like(creditCard_fraud[\"Time\"]),\"o\")\n#plt.show()","ab8cd2a1":"plt.plot(creditCard_genuine[\"Amount\"],np.zeros_like(creditCard_genuine[\"Amount\"]),\"o\")\nplt.plot(creditCard_fraud[\"Amount\"],np.zeros_like(creditCard_fraud[\"Amount\"]),\"o\")\nplt.show()","62347ce9":"plt.plot(creditCard_genuine[\"Class\"],np.zeros_like(creditCard_genuine[\"Class\"]),\"o\")\nplt.plot(creditCard_fraud[\"Class\"],np.zeros_like(creditCard_fraud[\"Class\"]),\"1\")\nplt.show()","2aa9bce8":"plt.plot(creditCard_genuine[\"Time\"],np.zeros_like(creditCard_genuine[\"Time\"]),\"o\")\nplt.plot(creditCard_fraud[\"Time\"],np.zeros_like(creditCard_fraud[\"Time\"]),\"1\")\nplt.show()","9b96a297":"#sns.FaceGrid(FilteredData,hue=\"class\",size=10).map(sns.distplot,\"Time\").add_legend()\n#plt.show()","5eaf0196":"counts,bin_edges=np.histogram(FilteredData[\"Amount\"],bins=10,density=True)\npdf=counts\/(sum(counts))\nprint(\"pdf=\",pdf)\nprint(\"\\n=\")\nprint(\"counts=\",counts)\nprint(\"\\n\")\nprint(\"Bin_edges=\",bin_edges)\n\ncdf=np.cumsum(pdf)\nplt.plot(bin_edges[1:],pdf)\nplt.plot(bin_edges[1:],cdf)\nplt.show()","6fa50341":"#Finding out Mean,Varience and Std Deviation:\nprint(\"---------------------------------------------------------------------------------------\")\nprint(\"Mean of Genuine and Fraud transactions:\")\nprint(\"----------------------------------------------------------------------------------------\")\nprint(\"mean of transaction amount of genuine transaction:\",np.mean(creditCard_genuine[\"Amount\"]))\nprint(\"mean of transaction amount of fraud transaction:\",np.mean(creditCard_fraud[\"Amount\"]))\n\nprint(\"---------------------------------------------------------------------------------------\")\nprint(\"Standard Deviation of Genuine and Fraud transactions:\")\nprint(\"---------------------------------------------------------------------------------------\")\nprint(\"standard deviation of transaction amount of genuine transaction:\",np.std(creditCard_genuine[\"Amount\"]))\nprint(\"standard deviation of transaction amount of fraud transaction:\",np.std(creditCard_fraud[\"Amount\"]))\n\n\n\n\n","be506a3c":"print(\"---------------------------------------------------------------------------------------\")\nprint(\"Median of Genuine and Fraud transactions:\")\nprint(\"----------------------------------------------------------------------------------------\")\nprint(\"median of transaction amount of genuine transaction:\",np.median(creditCard_genuine[\"Amount\"]))\nprint(\"mean of transaction amount of fraud transaction:\",np.median(creditCard_fraud[\"Amount\"]))","4a7ba3e1":"print(\"---------------------------------------------------------------------------------------\")\nprint(\"Quantiles of Genuine and Fraud transactions:\")\nprint(\"----------------------------------------------------------------------------------------\")\nprint(np.percentile(creditCard_genuine[\"Amount\"],np.arange(0,100,25)))\nprint(np.percentile(creditCard_fraud[\"Amount\"],np.arange(0,100,25)))\n","5ce50d48":"sns.boxplot(x=\"Class\",y=\"Amount\",data=df)\nplt.show()","3a219524":"sns.boxplot(x=\"Class\",y=\"Amount\",data=df)\nplt.ylim(0,5000)\nplt.show()","8bd0d5f2":"\nocc = df['Class'].value_counts()\nocc","948a9e4e":"plt.figure(figsize=(20,5))\nsns.heatmap(df.corr(),cmap='Reds')","9d37564a":"fig, (ax1,ax2) = plt.subplots(2,1,figsize=(20,6))\nsns.distplot(df.Amount,ax=ax1)\nsns.boxplot(df.Amount, ax=ax2)","05997e8d":"sns.boxplot(x='Class',y='Amount',data=df)","9759622a":"### Using time\nsns.distplot(df.Amount)","3d1ecccf":"### Using time \nsns.distplot(df.Class)","128f551d":"### Using time \n#sns.distplot(df.Time)","a9ca7e34":"import numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nimport time","53fd0d6b":"tic=time.time()\ndf=pd.read_csv(r'C:\\Users\\DELL\\Downloads\\archive\\creditcard.csv')","9a102637":"df=df.sample(frac=1)#Randomize the whole dataset HERE\nfull_features=df.drop([\"Time\",\"Class\"],axis=1)\nfull_labels=pd.DataFrame(df[[\"Class\"]])\nfull_features_array=full_features.values\nfull_labels_array=full_labels.values\ntrain_features,test_features,train_labels,test_labels=train_test_split(full_features_array,full_labels_array,train_size=0.90)\ntrain_features=normalize(train_features)\ntest_features=normalize(test_features)\n#k_means_classification..> \n#k_means_clustering, \n##confsion_matrix, \n#reassigning\nkmeans=KMeans(n_clusters=2,random_state=0,algorithm=\"elkan\",max_iter=10000,n_jobs=-1)\nkmeans.fit(train_features)\nkmeans_predicted_train_labels=kmeans.predict(train_features)","085b19bb":"# tn fp\n# fn tp\nprint(\"tn --> true negatives\")\nprint(\"fp --> false positives\")\nprint(\"fn --> false negatives\")\nprint(\"tp --> true positives\")\n\ntn,fp,fn,tp=confusion_matrix(train_labels,kmeans_predicted_train_labels).ravel()\nreassignflag=False\nif tn+tp<fn+fp:\n    \n\t# clustering is opposite of original classification\n\treassignflag=True\nkmeans_predicted_test_labels=kmeans.predict(test_features)\nif reassignflag:\n\tkmeans_predicted_test_labels=1-kmeans_predicted_test_labels","481ab23c":"#calculating confusion matrix for kmeans:\ntn,fp,fn,tp=confusion_matrix(test_labels,kmeans_predicted_test_labels).ravel()","edbdaab0":"#scoring kmeans::\nkmeans_accuracy_score=accuracy_score(test_labels,kmeans_predicted_test_labels)\nkmeans_precison_score=precision_score(test_labels,kmeans_predicted_test_labels)\nkmeans_recall_score=recall_score(test_labels,kmeans_predicted_test_labels)\nkmeans_f1_score=f1_score(test_labels,kmeans_predicted_test_labels)","7bc26098":"#PRINTING:\nprint(\"\")\nprint(\"K-MEANS\")\nprint(\"CONFUSION MATRIX\")\nprint(\"tn =\",tn,\"fp =\",fp)\nprint(\"fn =\",fn,\"tp =\",tp)\nprint(\"Scores\")\nprint(\"Accuracy -->\",kmeans_accuracy_score)\nprint(\"Precison -->\",kmeans_precison_score)\nprint(\"Recall -->\",kmeans_recall_score)\nprint(\"F1 -->\",kmeans_f1_score)","022f9a85":"#k_nearest_neighbours_classification\nfrom sklearn.metrics import accuracy_score\nknn=KNeighborsClassifier(n_neighbors=5,algorithm=\"kd_tree\",n_jobs=-1)\nknn.fit(train_features,train_labels.ravel())\nknn_predicted_test_labels=knn.predict(test_features)","593bcdf5":"#calculating confusion matrix for knn\ntn,fp,fn,tp=confusion_matrix(test_labels,knn_predicted_test_labels).ravel()","1335da8f":"#scoring knn\nknn_accuracy_score=accuracy_score(test_labels,knn_predicted_test_labels)\nknn_precison_score=precision_score(test_labels,knn_predicted_test_labels)\nknn_recall_score=recall_score(test_labels,knn_predicted_test_labels)\nknn_f1_score=f1_score(test_labels,knn_predicted_test_labels)","c4bbd49b":"from sklearn.metrics import accuracy_score\n#printing\nprint(\"\")\nprint(\"............................................\")\nprint(\"K-NEAREST NEIGHBOURS\")\nprint(\"CONFUSION MATRIX\")\nprint(\"............................................\")\nprint(\"tn =\",tn,\"fp =\",fp)\nprint(\"fn =\",fn,\"tp =\",tp)\nprint(\"Scores\")\nprint(\"Accuracy ----->\",knn_accuracy_score)\nprint(\"Precison ----->\",knn_precison_score)\nprint(\"Recall ------->\",knn_recall_score)\nprint(\"F1 ----------->\",knn_f1_score)","09ccac05":"#time elapsed\ntoc=time.time()\nelapsedtime=toc-tic\nprint(\"\")\nprint(\"TIME TAKEN FOR : \"+str(elapsedtime)+\"seconds\")\n","adaa943c":"#Separating the X and the Y values\n#Dividing the data into inputs parameters and outputs value format\n\n# dividing the X and the Y from the dataset \nX = df.drop(['Class'], axis = 1) \ny = df[\"Class\"] \nprint(X.shape) \nprint(y.shape) \n\n# getting just the values for the sake of processing  \n# (its a numpy array with no columns) \nxData = X.values \nyData = y.values ","7f3c9cb4":"#k_nearest_neighbours_classification\nknn=KNeighborsClassifier(n_neighbors=5,algorithm=\"kd_tree\",n_jobs=-1)\nknn.fit(train_features,train_labels.ravel())\nknn_predicted_test_labels=knn.predict(test_features)","ff609ce1":"#Using K Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.33, random_state = 42, stratify = y)\n#X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,stratify=y,random_state=42)\nknn = KNeighborsClassifier()\nxData = X.values \nyData = y.values \nknn.fit(X_train, y_train)\n#Predict output\nknn_pred = knn.predict(X_test)","26017ca7":"print(knn_pred)","edf7fc23":"print(knn)","e67e3d2c":"print(X_train.shape)","3e315c64":"print(y_train.shape)","9a972979":"#importing packages\n%matplotlib inline\nimport scipy.stats as stats\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#plt.style.use('ggplot')","1023acb7":"#loading data\ndf = pd.read_csv(r'C:\\Users\\DELL\\Downloads\\archive\\creditcard.csv')#'creditcard.csv')","ed5ec63e":"#shape of df\ndf.shape","f659bebf":"print('This data frame has {} rows and {} columns.'.format(df.shape[0], df.shape[1]))\n#This data frame has 284807 rows and 31 columns.","710aa219":"#peek at data\ndf.sample(5)","39bf9b86":"#visualizations of time and amount\n\nplt.figure(figsize=(10,8))\nplt.title('Distribution of Time Feature')\nsns.distplot(df.Time)","9ed312ef":"plt.figure(figsize=(10,8))\nplt.title('Distribution of Monetary Value Feature')\nsns.distplot(df.Amount)","9eac040e":"#count of fraud vs. normal transactions \n\ncounts = df.Class.value_counts()\nnormal = counts[0]\nfraudulent = counts[1]\nperc_normal = (normal\/(normal+fraudulent))*100\nperc_fraudulent = (fraudulent\/(normal+fraudulent))*100\nprint('There were {} non-fraudulent transactions ({:.3f}%) and {} fraudulent transactions ({:.3f}%).'.format(normal, perc_normal, fraudulent, perc_fraudulent))","2327dab9":"plt.figure(figsize=(10,8))\nsns.barplot(x=counts.index, y=counts)\nplt.title('Count of Fraudulent vs. Non-Fraudulent Transactions')\nplt.ylabel('Count')\nplt.xlabel('Class (0:Non-Fraudulent, 1:Fraudulent)')","41190d2f":"#finding correlation between columns and plotting heatmap\n\ncorr = df.corr()\nplt.figure(figsize=(12,10))\nheat = sns.heatmap(data=corr)\nplt.title('Heatmap of Correlation')","6cd7a196":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler2 = StandardScaler()","ce6509c1":"#scaling Time feature\n\nscaled_time = scaler.fit_transform(df[['Time']])\nflat_list1 = [item for sublist in scaled_time.tolist() for item in sublist]    # making a flat list out of a list of lists\nscaled_time = pd.Series(flat_list1)\n","044e12a9":"#scaling Amount feature\nscaled_amount = scaler2.fit_transform(df[['Amount']])\nflat_list2 = [item for sublist in scaled_amount.tolist() for item in sublist]\nscaled_amount = pd.Series(flat_list2)","25b9654d":"# adding the scaled features and dropping the unscaled features from the original dataframe\n\ndf = pd.concat([df, scaled_amount.rename('scaled_amount'), scaled_time.rename('scaled_time')], axis=1)\ndf.drop(['Amount', 'Time'], axis=1, inplace=True)\ndf.sample(5)","187192d7":"# undersampling with 1:1 ratio of legal and fraud transactions\n\nno_frauds = len(df[df['Class'] == 1])\nnon_fraud_indices = df[df.Class == 0].index\nrandom_indices = np.random.choice(non_fraud_indices,no_frauds, replace=False)\nfraud_indices = df[df.Class == 1].index\nunder_sample_indices = np.concatenate([fraud_indices,random_indices])\nunder_sample = df.loc[under_sample_indices]\n\n\n\nnew_counts = under_sample.Class.value_counts()\nplt.figure(figsize=(8,6))\nsns.barplot(x=new_counts.index, y=new_counts)\nplt.title('Count of Fraudulent vs. Non-Fraudulent Transactions In Subsample')\nplt.ylabel('Count')\nplt.xlabel('Class (0:Non-Fraudulent, 1:Fraudulent)')","4f146ff1":"# correlation between features in the subsample\ncorr = under_sample.corr()\ncorr = corr[['Class']]","67bab596":"corr[corr.Class < -0.5] # features with high negative correlation","ff721b3a":"#visualizing the features w high negative correlation\nf, axes = plt.subplots(nrows=2, ncols=4, figsize=(26,16))\n\nf.suptitle('Features With High Negative Correlation', size=35)\nsns.boxplot(x=\"Class\", y=\"V3\", data=under_sample, ax=axes[0,0])\nsns.boxplot(x=\"Class\", y=\"V9\", data=under_sample, ax=axes[0,1])\nsns.boxplot(x=\"Class\", y=\"V10\", data=under_sample, ax=axes[0,2])\nsns.boxplot(x=\"Class\", y=\"V12\", data=under_sample, ax=axes[0,3])\nsns.boxplot(x=\"Class\", y=\"V14\", data=under_sample, ax=axes[1,0])\nsns.boxplot(x=\"Class\", y=\"V16\", data=under_sample, ax=axes[1,1])\nsns.boxplot(x=\"Class\", y=\"V17\", data=under_sample, ax=axes[1,2])\nf.delaxes(axes[1,3])","4e88a5a9":"#visualizing the features w high positive correlation\nf, axes = plt.subplots(nrows=1, ncols=2, figsize=(18,9))\n\nf.suptitle('Features With High Positive Correlation', size=20)\nsns.boxplot(x=\"Class\", y=\"V4\", data=under_sample, ax=axes[0])\nsns.boxplot(x=\"Class\", y=\"V11\", data=under_sample, ax=axes[1])","daba0213":"# removing extreme outliers\nQ1 = under_sample.quantile(0.25)\nQ3 = under_sample.quantile(0.75)\nIQR = Q3 - Q1\n\ndf2 = under_sample[~((under_sample < (Q1 - 2.5 * IQR)) |(under_sample > (Q3 + 2.5 * IQR))).any(axis=1)]\n","0dd3e8dc":"len_after = len(df2)\nlen_before = len(under_sample)\nlen_difference = len(under_sample) - len(df2)\nprint('We reduced our data size from {} transactions by {} transactions to {} transactions.'\n      .format(len_before, len_difference, len_after))\n#We reduced our data size from 25092 transactions by 2848 transactions to 22244 transactions.","e8724bbe":"def warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn","55414369":"X = df2.drop('Class', axis=1)\ny = df2['Class']\n\nfrom sklearn.model_selection import train_test_split\n\nX_under_train, X_under_test, y_under_train, y_under_test = train_test_split(X,y,test_size = 0.2, random_state = 42)","3938ed07":"#importing relevant packages\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score","4b34c017":"# paramter tuning for Support Vector Machine\n\ndef svc_param_selection(X, y):\n    Cs = [0.001, 0.01, 0.1, 1, 10]\n    gammas = [0.001, 0.01, 0.1, 1]\n    param_grid = {'C': Cs, 'gamma' : gammas}\n    grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    return grid_search.best_params_\n\nbest_params_svm = svc_param_selection(X_under_train,y_under_train)\n\nprint(best_params_svm)","17cfe146":"# parameter tuning for Random Forest\n\ndef rf_param_selection(X, y):\n    param_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]}\n\n    grid_search = GridSearchCV(RandomForestClassifier(), param_grid)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    return grid_search.best_params_\n\nbest_params = rf_param_selection(X_under_train,y_under_train)\n\nprint(best_params)","95d1cd47":"def rf_param_selection(X, y):\n    param_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]}\n\n    grid_search = GridSearchCV(RandomForestClassifier(), param_grid)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    return grid_search.best_params_\n\nbest_params = rf_param_selection(X_under_train,y_under_train)\n\nprint(best_params)","2de4b5b1":"#Evaluating Algorithms The evaluation and comparison of algorithms is performed using K-fold cross validation. F1 score and ROC-AUC are used as performance metrics. The mean of each parameter across the k sets is displayed.","ba1a9464":"# comparison of the three algorithms\n\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nmodels = []\n\nmodels.append(('Logistic Regression', LogisticRegression()))\nmodels.append(('Support Vector Machine', SVC(kernel='rbf',C = best_params_svm['C'],gamma=best_params_svm['gamma'])))\nmodels.append(('Random Forest', RandomForestClassifier(n_estimators=best_params['n_estimators'],\n        min_samples_split=best_params['min_samples_split'],min_samples_leaf\n     =best_params['min_samples_leaf'],max_features=best_params['max_features'],max_depth = best_params['max_depth'],\n                                    criterion='gini')))\n\n#testing models\n\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=None)\n    cv_results_roc = cross_val_score(model, X_under_train, y_under_train, cv=kfold, scoring='roc_auc')\n    cv_results_f = cross_val_score(model, X_under_train, y_under_train, cv=kfold, scoring='f1_macro')\n    #cv_results_f = cross_val_score(model, X_under_train, y_under_train, cv=kfold, scoring='precision_macro')\n    #cv_results_f = cross_val_score(model, X_under_train, y_under_train, cv=kfold, scoring='recall_macro')\n    #cv_results_f = cross_val_score(model, X_under_train, y_under_train, cv=kfold, scoring='accuracy_macro')\n    results.append(cv_results_roc)\n    names.append(name)\n    msg = '%s:\\n' % (name)\n    print(msg)\n    print('F1 Score:',cv_results_f.mean())\n    print('ROC-AUC:',cv_results_roc.mean())\n    #print('Precision:',cv_results_roc.mean())\n    #print('Recall:',cv_results_roc.mean())\n    #print('Accuracy:',cv_results_roc.mean())\n    print('\\n')","68735763":"# Comparing the different algorithms\n\n\nfig = plt.figure(figsize=(12,10))\nplt.title('Comparison of Classification Algorithms')\nplt.xlabel('Algorithm')\nplt.ylabel('ROC-AUC Score')\nplt.boxplot(results)\nax = fig.add_subplot(111)\nax.set_xticklabels(names)\nplt.show()","ff610bb1":"### 15% Undersampling Ratio\n\n# 15% undersampling ratio\nno_frauds = len(df[df['Class'] == 1])\nnon_fraud_indices = df[df.Class == 0].index\nrandom_indices = np.random.choice(non_fraud_indices,(int)(no_frauds\/0.1), replace=False)\nfraud_indices = df[df.Class == 1].index\nunder_sample_indices = np.concatenate([fraud_indices,random_indices])\nunder_sample = df.loc[under_sample_indices]\n\nnew_counts = under_sample.Class.value_counts()\nplt.figure(figsize=(8,6))\nsns.barplot(x=new_counts.index, y=new_counts)\nplt.title('Count of Fraudulent vs. Non-Fraudulent Transactions In Subsample')\nplt.ylabel('Count')\nplt.xlabel('Class (0:Non-Fraudulent, 1:Fraudulent)')\n\n#Text(0.5, 0, 'Class (0:Non-Fraudulent, 1:Fraudulent","9c5e40a7":"#Comparison of the various algorithms\n\n# removing extreme outliers\n\ndf2 = under_sample[~((under_sample < (Q1 - 2.5 * IQR)) |(under_sample > (Q3 + 2.5 * IQR))).any(axis=1)]\n\nX = df2.drop('Class', axis=1)\ny = df2['Class']\n\n# splitting into train and test data\n\nX_under_train, X_under_test, y_under_train, y_under_test = train_test_split(X,y,test_size = 0.2, random_state = 42)","9b51d045":"models = []\n\nmodels.append(('Logistic Regression', LogisticRegression()))\nmodels.append(('Support Vector Machine', SVC(kernel='rbf',C = best_params_svm['C'],gamma=best_params_svm['gamma'])))\nmodels.append(('Random Forest', RandomForestClassifier(n_estimators=best_params['n_estimators'],\n        min_samples_split=best_params['min_samples_split'],min_samples_leaf\n     =best_params['min_samples_leaf'],max_features=best_params['max_features'],max_depth = best_params['max_depth'],\n                                    criterion='gini')))\n#testing models\n\nresults = []\nnames = []\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=None)\n    cv_results_roc = cross_val_score(model, X_under_train, y_under_train, cv=kfold, scoring='roc_auc')\n    cv_results_f = cross_val_score(model, X_under_train, y_under_train, cv=kfold, scoring='f1_macro')\n    results.append(cv_results_roc)\n    names.append(name)\n    msg = '%s:\\n' % (name)\n    print(msg)\n    print('F1 Score:',cv_results_f.mean())\n    print('ROC-AUC:',cv_results_roc.mean())\n    print('\\n')","4ccc2ddf":"# Comparing the different algorithms\n\n\nfig = plt.figure(figsize=(12,10))\nplt.title('Comparison of Classification Algorithms')\nplt.xlabel('Algorithm')\nplt.ylabel('ROC-AUC Score')\nplt.boxplot(results)\nax = fig.add_subplot(111)\nax.set_xticklabels(names)\nplt.show()","b82e50a6":"#10% Undersampling Ratio\n# 10% undersampling ratio\n\nno_frauds = len(df[df['Class'] == 1])\nnon_fraud_indices = df[df.Class == 0].index\nrandom_indices = np.random.choice(non_fraud_indices,(int)(no_frauds\/0.1), replace=False)\nfraud_indices = df[df.Class == 1].index\nunder_sample_indices = np.concatenate([fraud_indices,random_indices])\nunder_sample = df.loc[under_sample_indices]\n\n\nnew_counts = under_sample.Class.value_counts()\nplt.figure(figsize=(8,6))\nsns.barplot(x=new_counts.index, y=new_counts)\nplt.title('Count of Fraudulent vs. Non-Fraudulent Transactions In Subsample')\nplt.ylabel('Count')\nplt.xlabel('Class (0:Non-Fraudulent, 1:Fraudulent)')","b2edf939":"#Comparison of various algorithms\n\n# removing extreme outliers\n\ndf2 = under_sample[~((under_sample < (Q1 - 2.5 * IQR)) |(under_sample > (Q3 + 2.5 * IQR))).any(axis=1)]\n\nX = df2.drop('Class', axis=1)\ny = df2['Class']\n\n# splitting into train and test data\n\nX_under_train, X_under_test, y_under_train, y_under_test = train_test_split(X,y,test_size = 0.2, random_state = 42)","1ab35c2d":"models = []\n\nmodels.append(('Logistic Regression', LogisticRegression()))\nmodels.append(('Support Vector Machine', SVC(kernel='rbf',C = best_params_svm['C'],gamma=best_params_svm['gamma'])))\nmodels.append(('Random Forest', RandomForestClassifier(n_estimators=best_params['n_estimators'],\n        min_samples_split=best_params['min_samples_split'],min_samples_leaf\n     =best_params['min_samples_leaf'],max_features=best_params['max_features'],max_depth = best_params['max_depth'],\n                                    criterion='gini')))\n#testing models\n\nresults = []\nnames = []\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=None)\n    cv_results_roc = cross_val_score(model, X_under_train, y_under_train, cv=kfold, scoring='roc_auc')\n    cv_results_f = cross_val_score(model, X_under_train, y_under_train, cv=kfold, scoring='f1_macro')\n    results.append(cv_results_roc)\n    names.append(name)\n    msg = '%s:\\n' % (name)\n    print(msg)\n    print('F1 Score:',cv_results_f.mean())\n    print('ROC-AUC:',cv_results_roc.mean())\n    print('\\n')","0f588a90":"# Comparing the different algorithms\n\n\nfig = plt.figure(figsize=(12,10))\nplt.title('Comparison of Classification Algorithms')\nplt.xlabel('Algorithm')\nplt.ylabel('ROC-AUC Score')\nplt.boxplot(results)\nax = fig.add_subplot(111)\nax.set_xticklabels(names)\nplt.show()","f2e42c1c":"#5% Undersampling Ratio\n# 5% undersampling ratio\n\nno_frauds = len(df[df['Class'] == 1])\nnon_fraud_indices = df[df.Class == 0].index\nrandom_indices = np.random.choice(non_fraud_indices,(int)(no_frauds\/0.05), replace=False)\nfraud_indices = df[df.Class == 1].index\nunder_sample_indices = np.concatenate([fraud_indices,random_indices])\nunder_sample = df.loc[under_sample_indices]\n\n\nnew_counts = under_sample.Class.value_counts()\nplt.figure(figsize=(8,6))\nsns.barplot(x=new_counts.index, y=new_counts)\nplt.title('Count of Fraudulent vs. Non-Fraudulent Transactions In Subsample')\nplt.ylabel('Count')\nplt.xlabel('Class (0:Non-Fraudulent, 1:Fraudulent)')","5efd96a8":"#Comparison of various algorithms\n# removing extreme outliers\n\ndf2 = under_sample[~((under_sample < (Q1 - 2.5 * IQR)) |(under_sample > (Q3 + 2.5 * IQR))).any(axis=1)]\n\nX = df2.drop('Class', axis=1)\ny = df2['Class']\n\n# splitting into train and test data\n\nX_under_train, X_under_test, y_under_train, y_under_test = train_test_split(X,y,test_size = 0.2, random_state = 42)","cd691815":"models = []\n\nmodels.append(('Logistic Regression', LogisticRegression()))\nmodels.append(('Support Vector Machine', SVC(kernel='rbf',C = best_params_svm['C'],gamma=best_params_svm['gamma'])))\nmodels.append(('Random Forest', RandomForestClassifier(n_estimators=best_params['n_estimators'],\n        min_samples_split=best_params['min_samples_split'],min_samples_leaf\n     =best_params['min_samples_leaf'],max_features=best_params['max_features'],max_depth = best_params['max_depth'],\n                                    criterion='gini')))\n\n#testing models\n\nresults = []\nnames = []\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=None)\n    cv_results_roc = cross_val_score(model, X_under_train, y_under_train, cv=kfold, scoring='roc_auc')\n    cv_results_f = cross_val_score(model, X_under_train, y_under_train, cv=kfold, scoring='f1_macro')\n    results.append(cv_results_roc)\n    names.append(name)\n    msg = '%s:\\n' % (name)\n    print(msg)\n    print('F1 Score:',cv_results_f.mean())\n    print('ROC-AUC:',cv_results_roc.mean())\n    print('\\n')","a58add94":"#ROC AUC Score: 0.9783260964299432","87835c15":"# Comparing the different algorithms\n\nfig = plt.figure(figsize=(12,10))\nplt.title('Comparison of Classification Algorithms')\nplt.xlabel('Algorithm')\nplt.ylabel('ROC-AUC Score')\nplt.boxplot(results)\nax = fig.add_subplot(111)\nax.set_xticklabels(names)\nplt.show()","0b5ebec6":"#Import modules, methods and our dataset\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n#import os\n#print(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\n# Read csv\n#df = pd.read_csv(r'C:\\Users\\DELL\\Downloads\\archive(1)\\creditcard.csv')\ndf = pd.read_csv(r'C:\\Users\\DELL\\Downloads\\archive\\creditcard.csv')#'creditcard.csv')","77796a0b":"#Check and visulaize Fraud to Non-fraud Ratio\n# Explore the features avaliable in our dataframe\ndf.shape\ndf.info()\ndf.head()\ndf.describe()\nprint(df.Amount.describe())","71d76739":"# Count the occurrences of fraud and no fraud cases\nfnf = df[\"Class\"].value_counts()\n\n# Print the ratio of fraud cases \nprint(fnf\/len(df))\n\n# Plottingg your data\nplt.xlabel(\"Class\")\nplt.ylabel(\"Number of Observations\")\nfnf.plot(kind = 'bar',title = 'Frequency by observation number',rot=0)","03865393":"# Plot how fraud and non-fraud cases are scattered \nplt.scatter(df.loc[df['Class'] == 0]['V1'], df.loc[df['Class'] == 0]['V2'], label=\"Class #0\", alpha=0.5, linewidth=0.15)\nplt.scatter(df.loc[df['Class'] == 1]['V1'], df.loc[df['Class'] == 1]['V2'], label=\"Class #1\", alpha=0.5, linewidth=0.15,c='r')\nplt.show()","9b674c96":"#Distribution of 2 Features : Time and Amount\nimport seaborn as sns\n\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\n\n# Plot the distribution of 'Time' feature \nsns.distplot(df['Time'].values\/(60*60), ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Time', fontsize=14)\nax[0].set_xlim([min(df['Time'].values\/(60*60)), max(df['Time'].values\/(60*60))])\n\nsns.distplot(df['Amount'].values, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Amount', fontsize=14)\nax[1].set_xlim([min(df['Amount'].values), max(df['Amount'].values)])\nplt.show()\n                                ","ff56fd16":"#Cut Up the Dataset into Two Datasets and Summarize\n# Seperate total data into non-fraud and fraud cases\ndf_nonfraud = df[df.Class == 0] #save non-fraud df observations into a separate df\ndf_fraud = df[df.Class == 1] #do the same for frauds","71d137bc":"# Summarize statistics and see differences between fraud and normal transactions\nprint(df_nonfraud.Amount.describe())\nprint('_'*25)\nprint(df_fraud.Amount.describe())\n\n# Import the module\nfrom scipy import stats\nF, p = stats.f_oneway(df['Amount'][df['Class'] == 0], df['Amount'][df['Class'] == 1])\nprint(\"F:\", F)\nprint(\"p:\",p)","8132988d":"\n\nimport matplotlib.pyplot as plt\nk=(3,3,3,3)\nx, bins, p=plt.hist(k, density=True)  # used to be normed=True in older versions\nfrom numpy import *\nplt.xticks( arange(10) ) # 10 ticks on x axis\nplt.show()  \n\n\nprint (bins)\n[ 2.5  2.6  2.7  2.8  2.9  3.   3.1  3.2  3.3  3.4  3.5]","f1e83960":"# Plot of high value transactions($200-$2000)\nbins = np.linspace(200, 2000, 100)\nplt.hist(df_nonfraud.Amount, bins, alpha=1, density=True, label='Non-Fraud')\nplt.hist(df_fraud.Amount, bins, alpha=1, density=True, label='Fraud')\nplt.legend(loc='upper right')\nplt.title(\"Amount by percentage of transactions (transactions \\$200-$2000)\")\nplt.xlabel(\"Transaction amount (USD)\")\nplt.ylabel(\"Percentage of transactions (%)\")\nplt.show()","82af6ab6":"#Transaction period\/hr\n#Let's look at the transaction percentage from the day 0 to the next preceeding day\n# Plot of transactions in 48 hours\nbins = np.linspace(0, 48, 48) #48 hours\nplt.hist((df_nonfraud.Time\/(60*60)), bins, alpha=1, density=True, label='Non-Fraud')\nplt.hist((df_fraud.Time\/(60*60)), bins, alpha=0.6, density=True, label='Fraud')\nplt.legend(loc='upper right')\nplt.title(\"Percentage of transactions by hour\")\nplt.xlabel(\"Transaction time from first transaction in the dataset (hours)\")\nplt.ylabel(\"Percentage of transactions (%)\")\nplt.show()","dfd200ff":"#Transaction Amount versus\/Hour\n# Here the Plot of transactions in 48 hours\nplt.scatter((df_nonfraud.Time\/(60*60)), df_nonfraud.Amount, alpha=0.6, label='Non-Fraud')\nplt.scatter((df_fraud.Time\/(60*60)), df_fraud.Amount, alpha=0.9, label='Fraud')\nplt.title(\"Amount of transaction by hour\")\nplt.xlabel(\"Transaction time as measured from first transaction in the dataset (hours)\")\nplt.ylabel('Amount (INR)')\nplt.legend(loc='upper right')\nplt.show()","d9e57226":"# here we have Scaled  \"Time\" and \"Amount\" features.\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\ndf['scaled_amount'] = RobustScaler().fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = RobustScaler().fit_transform(df['Time'].values.reshape(-1,1))\n\n# Make a new dataset named \"df_scaled\" dropping out original \"Time\" and \"Amount\"\ndf_scaled = df.drop(['Time','Amount'],axis = 1,inplace=False)\ndf_scaled.head()","4121960e":"# Lets Calculate pearson correlation coefficience\ncorr = df_scaled.corr() ","499e0be1":"sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20})","32f5ce62":"# Plot heatmap of correlation\nf, ax = plt.subplots(3, 3, figsize=(24,20))\n\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20})\n#ax[1,1].plot(\"imbalanced\",fontsize=24)\n#ax = ax.flatten()\n#ax = ax.T.flatten()\nax.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=24)\n#Text(0.5,1,\"Imbalanced Corre","30718426":"#Extract the features from our scaled dataset that si,\"df_scaled\"\n# Define the prep_data function to extrac features \ndef prep_data(df):\n    X = df.drop(['Class'],axis=1, inplace=False) #  \n    X = np.array(X).astype(np.float)\n    y = df[['Class']]  \n    y = np.array(y).astype(np.float)\n    return X,y\n\n# Create X and y from the prep_data function \nX, y = prep_data(df_scaled)","2601e290":"def __init__(self, image, x, y):\n    super().__init__(image=image, x=x, y=y)\n    \"\"\"A test!\"\"\"\n    print(\"Test.\")","8b1fd549":"#Resample data with RUS, ROS and SMOTE\n#from imblearn.over_sampling import SMOTE\nfrom imblearn import under_sampling, over_sampling\nfrom imblearn.over_sampling import SMOTE\n#from imblearn.datasets import ...\n#(venv) jupyter notebook\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.pipeline import Pipeline # Inorder to avoid testing model on sampled data\n# Create the training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)\n\n# Define the resampling method\nundersam = RandomUnderSampler(random_state=0)\noversam = RandomOverSampler(random_state=0)#sampler = SMOTE(ratio={1: 1927, 0: 300},random_state=0).\nsmote = SMOTE(random_state=0)#oversampler=SMOTE(kind='regular',k_neighbors=2)\nborderlinesmote = BorderlineSMOTE(kind='borderline-2',random_state=0)\n\n# resample the training data\nX_undersam, y_undersam = undersam.fit_resample(X_train,y_train)\nX_oversam, y_oversam = oversam.fit_resample(X_train,y_train)\nX_smote, y_smote = smote.fit_resample(X_train,y_train)\nX_borderlinesmote, y_borderlinesmote = borderlinesmote.fit_resample(X_train,y_train)","297a89f1":"#xr, yr = SMOTE(k_neighbors=3).fit_resample(x, y)\n#oversampler=SMOTE(kind='regular',k_neighbors=2)","ebbbc0e1":"#oversampler=SMOTE(kind='regular',k_neighbors=2). This worked for me. ... \n#sampler = SMOTE(ratio={1: 1927, 0: 300},random_state=0).\n#(ratio='auto', random_state=None, k=None, k_neighbors=5, m=None, m_neighbors=10, out_step=0.5, kind='regular', ...","23557e70":"#Model Evaluation\nfrom sklearn.metrics import roc_curve,roc_auc_score, precision_recall_curve, average_precision_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","56873793":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\n\n#function\ndef train_test_rmse(x,y):\n    x = Iris_data[x]\n    y = Iris_data[y]\n    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,random_state=123)\n    linreg = LinearRegression()\n    linreg.fit(X_train, y_train)\n    y_pred = linreg.predict(X_test)\n    print(accuracy_score(y_test, y_pred))  # or you can save it in variable and return it \n    return np.sqrt(metrics.mean_squared_error(y_test, y_pred))","57406bd8":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\n\n","bcaee011":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","8014f729":"#import scikitplot as skplt\nimport matplotlib.pyplot as plt\n\n#y_true = # ground truth labels\n#y_probas = # predicted probabilities generated by sklearn classifier\n#skplt.metrics.plot_roc_curve(y_true, y_probas)\n#plt.show()","b2e023fd":"y_test.shape","caac1eef":"y_pred.shape","a87d6cb5":"import numpy as np\nfrom sklearn.linear_model import LinearRegression","e0296cfe":"#X = np.asarray([ 1994.,  1995.,  1996.,  1997.,  1998.,  1999.])\n#y = np.asarray([1.2, 2.3, 3.4, 4.5, 5.6, 6.7])\nX = np.asarray([df])\ny = np.asarray(['Class'])\nx\ny","8bdd9965":"x","f213a78f":"def perf_measure(y_actual, y_hat):\n    TP = 0\n    FP = 0\n    TN = 0\n    FN = 0\n\n    for i in range(len(y_hat)): \n        if y_actual[i]==y_hat[i]==1:\n           TP += 1\n        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n           FP += 1\n        if y_actual[i]==y_hat[i]==0:\n           TN += 1\n        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n           FN += 1\n\n    return(TP, FP, TN, FN)","14ef4665":"#from sklearn.model_selection import train_test_split(test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)","abfca0ee":"# Create true and false positive rates\n#from sklearn.linear_model\nimport sklearn.metrics as metrics\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=123,) \nfalse_positive_rate, true_positive_rate, threshold = roc_curve(y_test,y_pred)\n#skplt.metrics.plot_roc_curve(y_true, y_probas)\n#plt.show()","42a960ef":"x.shape\n","8a223a81":"y.shape","679c0d8f":"from sklearn import linear_model\n#from sklearn.cross_validation import train_test_split\n","abaed2fe":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split","333bfdf0":"\n#best_clf = grid_search\n#best_clf = grid_search.best_estimator_\nfrom sklearn.model_selection import learning_curve, GridSearchCV\n# lets Calculate AUC,ROC Curve \nprobs = model.predict_proba(X_test)\nroc_auc = roc_auc_score(y_test, probs[:, 1])\nprint('ROC AUC Score:',roc_auc)\nbest_clf = grid_search\nbest_clf = grid_search.best_estimator_","337fce22":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\n\n#function\ndef train_test_rmse(x,y):\n    x = Iris_data[x]\n    y = Iris_data[y]\n    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,random_state=123)\n    linreg = LinearRegression()\n    linreg.fit(X_train, y_train)\n    y_pred = linreg.predict(X_test)\n    print(accuracy_score(y_test, y_pred))  # or you can save it in variable and return it \n    return np.sqrt(metrics.mean_squared_error(y_test, y_pred))","1470be53":"# Obtain precision and recall \nprecision, recall \nthresholds = precision_recall_curve(y_test, y_pred)","8189b73c":"\n# Calculate average precision \naverage_precision = average_precision_score(y_test, y_predicted)\n","5d6ee32a":"\n# Define a roc_curve function\ndef plot_roc_curve(false_positive_rate,true_positive_rate,roc_auc):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=5, label='AUC = %0.3f'% roc_auc)\n    plt.plot([0,1],[0,1], linewidth=5)\n    plt.xlim([-0.01, 1])\n    plt.ylim([0, 1.01])\n    plt.legend(loc='upper right')\n    plt.title('Receiver operating characteristic curve (ROC)')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n\n# Define a precision_recall_curve function\ndef plot_pr_curve(recall, precision, average_precision):\n    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n    plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n#   plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n#   plt.show()\nlinreg = LinearRegression()\ny_pred = linreg.predict(X_test)\n# Print the classifcation report and confusion matrix\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred=linreg.predict(X_test)))\n\n# Plot the roc curve \nplot_roc_curve(false_positive_rate,true_positive_rate,roc_auc)\n\n# Plot recall precision curve\nplot_pr_curve(recall, precision, average_precision)\n","a9bacc32":"#spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n\n#pd_df = df_spark.toPandas()\n#pandas_df = spark_df.select(\"*\").toPandas()","18a9c90c":"#spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n\n#pd_df = df_spark.toPandas()\n#pandas_df = spark_df.select(\"*\").toPandas()\n#pandas_df=spark_df.toPandas()\n#features_to_use = ['Feature1', 'Feature2']\n#x5D = np.array(pandas_df[ features_to_use ])\n#y5D = np.array(pandas_df['TargetFeature'])\n#X_train, X_test, y_train, y_test = train_test_split(x5D, y5D, train_size=0.8)","f84c1f65":"np.random.seed(0)\ndf1 = pd.DataFrame(np.random.choice(10, (5, 4)), columns=list('ABCD'))\ny = df1.pop('C')\nz = df1.pop('D')\nX = df1\n\nsplits = train_test_split(X, y, z, test_size=0.2)\nlen(splits)\n# 6","861421cb":"#train_test_split(X, y, test_size=0.2)","6e749022":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","f8367d4d":"#y_val_true, val_pred = y_val_true.reshape((-1)), val_pred.reshape((-1))\n#val_ap = average_precision_score(y_val_true, val_pred)","43b5acc4":"np.isnan(y_pred).any() #to check the nan values in y_pred","c3fff189":"y_predicted","929e5272":"sklearn.metrics. average_precision_score (y_true, y_score, *, average='macro', pos_label=1, sample_weight=None)[source]\u00b6. Compute average precision ","19857338":"#example to understand the logic better\nimport numpy as np\nfrom sklearn.metrics import average_precision_score\ny_true = np.array([0, 0, 1, 1])\ny_score = np.array([0.1, 0.4, 0.35, 0.8])\naverage_precision_score(y_true, y_score)","a9e5ca99":"#from sklearn.metrics.average_precision_score(y_true, y_score, *, average='macro', pos_label=1, sample_weight=None)\n# Import the decision tree model from sklearn\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Create the training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# Fit a logistic regression model to our data\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\n# Obtain model predictions\ny_predicted = model.predict(X_test)\n\n# Calculate average precision \n#val_ap = average_precision_score(y_test.reshape((-1)), y_predicted.reshape((-1)))\n#y_test, y_predicted = y_test.reshape((-1)), y_predicted.reshape((-1))\n#val_ap = average_precision_score(y_test, y_predicted)\naverage_precision = average_precision_score(y_test, y_predicted)\n\n# Obtain precision and recall \nprecision, recall, _ = precision_recall_curve(y_test, y_predicted)\n\n# Plot the recall precision tradeoff\nplot_pr_curve(recall, precision, average_precision)\n\n# Print the classifcation report and confusion matrix\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))\n","40f72498":"#Decision Tree Classifier with SMOTE Data\n# Import the pipeline module we need for this from imblearn\nfrom imblearn.pipeline import Pipeline \nfrom imblearn.over_sampling import BorderlineSMOTE\n\n# Define which resampling method and which ML model to use in the pipeline\nresampling = BorderlineSMOTE(kind='borderline-2',random_state=0) # instead SMOTE(kind='borderline2') \nmodel = DecisionTreeClassifier() \n\n# Define the pipeline, tell it to combine SMOTE with the Logistic Regression model\npipeline = Pipeline([('SMOTE', resampling), ('Decision Tree Classifier', model)])\n\n# Fit your pipeline onto your training set and obtain predictions by fitting the model onto the test data \npipeline.fit(X_train, y_train) \ny_predicted = pipeline.predict(X_test)\n\n# Obtain the results from the classification report and confusion matrix \nprint('Classifcation report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',  confusion_matrix(y_true = y_test, y_pred= y_predicted))","300eb4c6":"#Precision = 0.56. The rate of true positive in all positive cases.\n#Recall = 0.73. The rate of true positive in all true cases.\n#F1-score = 0.63\n#False positives cases = 62.","42887f04":"#Model results using GridSearchCV\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Build a RandomForestClassifier using the GridSearchCV parameters\nmodel = RandomForestClassifier(bootstrap=True,\n                               class_weight = {0:1,1:12},\n                               criterion = 'entropy',\n                               n_estimators = 30,\n                               max_features = 'auto',\n                               min_samples_leaf = 10,\n                               max_depth = 8,\n                               n_jobs = -1,\n                               random_state = 5)\n\n# Fit the model to your training data and get the predicted results\nmodel.fit(X_train,y_train)\ny_predicted = model.predict(X_test)\n\n# Calculate average precision \naverage_precision = average_precision_score(y_test, y_predicted)\n\n# Obtain precision and recall \nprecision, recall, _ = precision_recall_curve(y_test, y_predicted)\n\n# Plot the recall precision tradeoff\nplot_pr_curve(recall, precision, average_precision)\n# Print the roc_auc_score,Classifcation report and Confusin matrix\nprobs = model.predict_proba(X_test)\nprint('roc_auc_score:', roc_auc_score(y_test,probs[:,1]))\nprint('Classification report:\\n',classification_report(y_test,y_predicted))\nprint('Confusion_matrix:\\n',confusion_matrix(y_test,y_predicted))\n","8f170319":"https:\/\/www.kaggle.com\/merryyundi\/credit-card-fraud-detection","f26f6600":"#Check for NULL\/MISSING values\n# percentage of missing values in each column\nround(100 * (df.isnull().sum()\/len(df)),2).sort_values(ascending=False)","25d71e7d":"df.shape","1e329520":"df.describe()","e649ef12":"#Duplicate Check\ndf_d=df.copy()\ndf_d.drop_duplicates(subset=None, inplace=True)\n","df2d1fc6":"df.shape","5726ad5d":"df_d.shape","79feb854":"## Assigning removed duplicate datase to original \ndf=df_d\ndf.shape","b15bf457":"#EXPLORATORY DATA ANALYSIS\u00b6\ndf.info()","244b862a":"def draw_histograms(dataframe, features, rows, cols):\n    fig=plt.figure(figsize=(16,20))\n    for i, feature in enumerate(features):\n        ax=fig.add_subplot(rows,cols,i+1)\n        dataframe[feature].hist(bins=25,ax=ax,facecolor='green')\n        ax.set_title(feature+\" Distribution\",color='DarkRed')\n        ax.set_yscale('log')\n    fig.tight_layout()  \n    plt.show()\ndraw_histograms(df,df.columns,10,7)","8d6856bf":"df.Class.value_counts()","6793feb7":"ax=sns.countplot(x='Class',data=df);\nax.set_yscale('log')","99a30ce2":"###Correlation Matrix\nplt.figure(figsize = (45,20))\nsns.heatmap(df.corr(), annot = True, cmap=\"tab20c\")\nplt.show()","967cc490":"estimators=['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n\nX1 = df[estimators]\ny = df['Class']","09415283":"col=X1.columns[:-1]\ncol","78696349":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3a22722d":"import statsmodels.api\nX = sm.add_constant(X1)\nreg_logit = sm.Logit(y,X)\nresults_logit = reg_logit.fit()","54543ab4":"#here we get complete summary","d5fe1f71":"results_logit.summary()","0f574499":"#Here we defining the function as BACK_FEATURE_ELIMINATION AS \"def back_feature_elimination\"","b8dad243":"def back_feature_elemination (data_frame,dep_var,col_list):\n    \"\"\" It takes in the dataframe, the dependent variable and a list of column names, runs the regression repeatedly eleminating feature with the highest\n    P-value above alpha one at a time and returns the regression summary with all p-values below alpha value\"\"\"\n\n    while len(col_list)>0 :\n        model=sm.Logit(dep_var,data_frame[col_list])\n        result=model.fit(disp=0)\n        largest_pvalue=round(result.pvalues,3).nlargest(1)\n        if largest_pvalue[0]<(0.0001):\n            return result\n            break\n        else:\n            col_list=col_list.drop(largest_pvalue.index)\n\nresult=back_feature_elemination(X,df.Class,col)","b1361261":"#Complete summary will be displayed here::","a33dcea2":"result.summary()","054c644e":"###Interpreting the results: \n#Odds Ratio, \n#Confidence Intervals \n#and Pvalues","de21e49d":"params = np.exp(result.params)\nconf = np.exp(result.conf_int())\nconf['OR'] = params\npvalue=round(result.pvalues,3)\nconf['pvalue']=pvalue\nconf.columns = ['CI 95%(2.5%)', 'CI 95%(97.5%)', 'Odds Ratio','pvalue']\nprint ((conf))","c1584156":"new_features=df[['Time','V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V20','V21', 'V22', 'V23', 'V25', 'V26', 'V27','Class']]\nx=new_features.iloc[:,:-1]\ny=new_features.iloc[:,-1]","d4e143cf":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.3,stratify=y)","fa547b9a":"from sklearn.linear_model import LogisticRegression\nlogreg=LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred=logreg.predict(x_test)","60f20726":"#Here we will hve a Model Evaluation\n#and Model accuracy\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","894b5dc3":"#Accuracy of the model is 0.9982","aace064a":"#Confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\");","32f7f409":"TN=cm[0,0]\nTP=cm[1,1]\nFN=cm[1,0]\nFP=cm[0,1]\nsensitivity=TP\/float(TP+FN)\nspecificity=TN\/float(TN+FP)\n#Model Evaluation - Statistics","a6e4eb50":"print('The acuuracy of the model is = TP+TN\/(TP+TN+FP+FN) =',(TP+TN)\/float(TP+TN+FP+FN),'\\n',\n\n'The Missclassification is = 1-Accuracy =                  ',1-((TP+TN)\/float(TP+TN+FP+FN)),'\\n',\n\n'Sensitivity or True Positive Rate is = TP\/(TP+FN) =       ',TP\/float(TP+FN),'\\n',\n\n'Specificity or True Negative Rate is  = TN\/(TN+FP) =       ',TN\/float(TN+FP),'\\n',\n\n'Positive Predictive value is = TP\/(TP+FP) =               ',TP\/float(TP+FP),'\\n',\n\n'Negative predictive Value is   = TN\/(TN+FN) =               ',TN\/float(TN+FN),'\\n',\n\n'Positive Likelihood Ratio is = Sensitivity\/(1-Specificity)=',sensitivity\/(1-specificity),'\\n',\n\n'Negative likelihood Ratio is = (1-Sensitivity)\/Specificity=',(1-sensitivity)\/specificity)                                                                 \n                                                                   \n                                                                \n                                                                   \n                                                                   \n                                                                   \n                                                                   \n                                                                   ","9e7a50a6":"#From the above statistics we can clearly estimate that,the model is highly specific than sensitive. \n#The negative values are predicted more accurately than the positives values here.\ny_pred_prob=logreg.predict_proba(x_test)[:,:]\ny_pred_prob_df=pd.DataFrame(data=y_pred_prob, columns=['PROBABILITY OF NOT FRAUD (0)','PROBABILITY OF FRAUD (1)'])\ny_pred_prob_df.head()","d5f969e8":"import numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\n","31d563cb":"mlb.fit_transform(labels.values.tolist())\nfrom sklearn.preprocessing import binarize\nfor i in range(0,11):\n    cm2=0\n    y_pred_prob_yes=logreg.predict_proba(x_test)\n    y_pred2=binarize(y_pred_prob_yes,i\/10)[:,1]\n    cm2=confusion_matrix(y_test,y_pred2)\n    print ('With',i\/10,'threshold the Confusion Matrix is ','\\n',cm2,'\\n',\n            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n          'Sensitivity: ',cm2[1,1]\/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]\/(float(cm2[0,0]+cm2[0,1])),'\\n\\n\\n')","46426a1e":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nsc = StandardScaler()    \nsc.fit(X_train)    \n\nX_train_std = sc.transform(X_train)    \nX_test_std = sc.transform(X_test)    \nX_combined_std = np.vstack((X_train_std, X_test_std))    \ny_combined = np.hstack((y_train, y_test))","2356f8e3":"from sklearn import svm, datasets\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","28419b79":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\n\n","5e732187":"#ROC Curve\n\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import train_test_split\n#X_test, X_train,y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_yes[:,1])\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for Fraud classifier')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.grid(True)","51dc42e6":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()    \nsc.fit(X_train)    \n\nX_train_std = sc.transform(X_train)    \nX_test_std = sc.transform(X_test)    \nX_combined_std = np.vstack((X_train_std, X_test_std))    \ny_combined = np.hstack((y_train, y_test))","ca11de1e":"sc = StandardScaler()\n\nsc.fit(X_train)\n\nX_train_std = sc.transform(X_train)\n\nX_test_std = sc.transform(X_test)","5b3b3cb5":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,y_pred_prob_yes[:,1])","be1c3d71":"# distribution of Amount\namount = [df['Amount'].values]\nsns.distplot(amount)","a4d703ca":"# distribution of Time\ntime = df['Time'].values\nsns.distplot(time)","2df6181f":"from matplotlib import gridspec\n# distribution of anomalous features\nfeatures = df.iloc[:,0:28].columns\n\nplt.figure(figsize=(12,28*4))\ngs = gridspec.GridSpec(28, 1)\nfor i, c in enumerate(df[features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(df[c][df.Class == 1], bins=50)\n    sns.distplot(df[c][df.Class == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('histogram of feature: ' + str(c))\nplt.show()","23e97b03":"# Plot histograms of each parameter \ndf.hist(figsize = (20, 20))\nplt.show()","b7f911ee":"# Determine number of fraud cases in dataset\n\nFraud = df[df['Class'] == 1]\nValid = df[df['Class'] == 0]\n\noutlier_fraction = len(Fraud)\/float(len(Valid))\nprint(outlier_fraction)\n\nprint('Fraud Cases: {}'.format(len(df[df['Class'] == 1])))\nprint('Valid Transactions: {}'.format(len(df[df['Class'] == 0])))","9dcab69c":"print(\"Amount details of fradulent transacation\")\nFraud.Amount.describe()","3a8ac6d3":"print(\"Amount details of valid transaction\")\nValid.Amount.describe()","9ffece51":"#seperating the X and the Y from the dataset\nX=df.drop(['Class'], axis=1)\nY=df[\"Class\"]\nprint(X.shape)\nprint(Y.shape)\n#getting just the values for the sake of processing (its a numpy array with no columns)\nX_df=X.values\nY_df=Y.values","d0770a02":"X_df","25bf4121":"#Using Scikit-learn to split data into training and testing sets\n# Using Skicit-learn to split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\n# Split the data into training and testing sets\nX_train, X_test, Y_train, Y_test = train_test_split(X_df, Y_df, test_size = 0.2, random_state = 42)","bf5edd7b":"from sklearn.metrics import classification_report, accuracy_score,precision_score,recall_score,f1_score,matthews_corrcoef\nfrom sklearn.metrics import confusion_matrix","8c57ac57":"#Isolation Forest Classifier\n#Building another model\/classifier ISOLATION FOREST\nfrom sklearn.ensemble import IsolationForest\nifc=IsolationForest(max_samples=len(X_train),\n                    contamination=outlier_fraction,random_state=1)\nifc.fit(X_train)\nscores_pred = ifc.decision_function(X_train)\ny_pred = ifc.predict(X_test)\n\n\n# Reshape the prediction values to 0 for valid, 1 for fraud. \ny_pred[y_pred == 1] = 0\ny_pred[y_pred == -1] = 1\n\nn_errors = (y_pred != Y_test).sum()\n","35cd5c51":"#evaluation of the model\n#printing every score of the classifier\n#scoring in any thing\n\nfrom sklearn.metrics import confusion_matrix\nn_outliers = len(Fraud)\n\nprint(\"the Model used is {}\".format(\"Isolation Forest\"))\nacc= accuracy_score(Y_test,y_pred)\n\nprint(\"The accuracy is  {}\".format(acc))\nprec= precision_score(Y_test,y_pred)\n\nprint(\"The precision is {}\".format(prec))\nrec= recall_score(Y_test,y_pred)\n\nprint(\"The recall is {}\".format(rec))\nf1= f1_score(Y_test,y_pred)\n\nprint(\"The F1-Score is {}\".format(f1))\nMCC=matthews_corrcoef(Y_test,y_pred)\n\nprint(\"The Matthews correlation coefficient is{}\".format(MCC))\n#printing the confusion matrix\nLABELS = ['Normal', 'Fraud']\nconf_matrix = confusion_matrix(Y_test, y_pred)\nplt.figure(figsize=(12, 12))\nsns.heatmap(conf_matrix, xticklabels=LABELS,\n            yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()\n\n# Run classification metrics\nplt.figure(figsize=(9, 7))\nprint('{}: {}'.format(\"Isolation Forest\", n_errors))\nprint(accuracy_score(Y_test, y_pred))\nprint(classification_report(Y_test, y_pred))\n\n","5d36c81d":"#Random Forest Classifier\n# Building the Random Forest Classifier (RANDOM FOREST)\nfrom sklearn.ensemble import RandomForestClassifier\n# random forest model creation\nrfc = RandomForestClassifier()\nrfc.fit(X_train,Y_train)\n# predictions\ny_pred = rfc.predict(X_test)","9a705db2":"#Evaluating the classifier\n#printing every score of the classifier\n#scoring in any thing\nfrom sklearn.metrics import classification_report, accuracy_score,precision_score,recall_score,f1_score,matthews_corrcoef\nfrom sklearn.metrics import confusion_matrix\nn_outliers = len(Fraud)\nn_errors = (y_pred != Y_test).sum()\n\nprint(\"The model used is Random Forest classifier\")\nacc= accuracy_score(Y_test,y_pred)\n\nprint(\"The accuracy is  {}\".format(acc))\nprec= precision_score(Y_test,y_pred)\n\nprint(\"The precision is {}\".format(prec))\nrec= recall_score(Y_test,y_pred)\n\nprint(\"The recall is {}\".format(rec))\nf1= f1_score(Y_test,y_pred)\n\nprint(\"The F1-Score is {}\".format(f1))\nMCC=matthews_corrcoef(Y_test,y_pred)\n\nprint(\"The Matthews correlation coefficient is {}\".format(MCC))\n\n\n#printing the confusion matrix\nLABELS = ['Normal', 'Fraud']\nconf_matrix = confusion_matrix(Y_test, y_pred)\n\nplt.figure(figsize=(12, 12))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()\n\n# Run classification metrics\nplt.figure(figsize=(9, 7))\n\nprint('{}: {}'.format(\"Random Forest\", n_errors))\nprint(accuracy_score(Y_test, y_pred))\nprint(classification_report(Y_test, y_pred))","59925d49":"#visualizing the random tree \nfeature_list = list(X.columns)\n# Import tools needed for visualization\nfrom IPython.display import Image\nfrom sklearn.tree import export_graphviz\nimport pydot","f0295869":"#pulling out one tree from the forest\ntree = rfc.estimators_[5]\nexport_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)","e3b6c4cf":"# Use dot file to create a graph\n(graph, ) = pydot.graph_from_dot_file('tree.dot')\n# Write graph to a png file\n#print(Image(graph.create_png()))\n\n#import os\n#os.environ[\"PATH\"] += os.pathsep + 'C:\/Program Files (x86)\/Graphviz2.38\/bin\/'\n\n\n","668ce78f":"X_train,y_train,X_test,y_test=train_test_split(X,y,train_size=0.8,stratify=y,random_state=100)","9e4dc65e":"X_train.shape","cfdccba1":"y_train.shape","936daaa8":"y_train.value_counts()\/len(y_train)","7f19cb7c":"y_test.value_counts()\/len(y_test)","d4bc480a":"from sklearn.preprocessing import StandardScaler\nScaler_X=StandardScaler()\nX_train=Scaler_X.fit_transform(X_train)\nX_test=Scaler_X.transform(X_test)\n#array.reshape(-1, 1)","eb3bebfc":"rf_model=RandomForestClassifier()\nrf_params={'n_estimators':estimators,\n           'max_depth':max_depth,\n           'min_samples_split':min_samples_split}\nclf_RF=RandomizedSearchCV(rf_model,rf_params,cv=cv,scoring='roc_auc',n_jobs=-1,n_iter=20,verbos=2)\nclf_RF.fit(X_train,y_train)\ntest_eval(clf_RF,X_test,y-test,'Random Forest','actual')","b1ae702b":"#SMOTE TECHNIQUE:\nfrom imblearn.over_sampling import SMOTE\nimport collections\nfrom collections import Counter\ncounter=Counter(y_train)\nprint('Before',counter)","82e2faf9":"\nfrom sklearn                        import metrics, svm\nfrom sklearn.linear_model           import LinearRegression\nfrom sklearn.linear_model           import LogisticRegression\nfrom sklearn.tree                   import DecisionTreeClassifier\nfrom sklearn.neighbors              import KNeighborsClassifier\nfrom sklearn.discriminant_analysis  import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes            import GaussianNB\nfrom sklearn.svm                    import SVC","69c3b9d0":"import numpy as np # linear algebra\nimport pandas as pd # data processing\n\n# for standardization\nfrom sklearn.preprocessing import StandardScaler as ss\n\n# for splitting into train and test datasets\nfrom sklearn.model_selection import train_test_split \n\n# for modelling\n#from xgboost.sklearn import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# for balancing dataset by oversampling\nfrom imblearn.over_sampling import SMOTE, ADASYN\n\n# for performance metrics\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_recall_curve\nfrom sklearn.metrics import auc, roc_curve, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix, classification_report, average_precision_score\n\n# for data visualization\nimport matplotlib.pyplot as plt\n\n#Miscellaneous\nimport time\nimport random\nimport os","2d87f3d5":"# set number of rows to be displayed\npd.options.display.max_columns = 300\n\n# reading the dataset\ndf=pd.read_csv(r'C:\\Users\\DELL\\Downloads\\archive (1)\\creditcard.csv')","801f249e":"#Data Exploration\nprint('Shape: ',df.shape)\n\nprint('\\nColumns: ',df.columns.values)\n\nprint('\\nData types:\\n',df.dtypes.value_counts())","2a516e8d":"# check if there are null values in the dataset\ndf.isnull().sum().sum()","c110a767":"# Time & Amount distributions\n\nfig = plt.figure(figsize=(14,5))\nax = fig.add_subplot(1,2,1)\ndf.Time.plot(kind = \"hist\", bins = 40)\nplt.xlabel('Time(in secs)', size='large')\nax = fig.add_subplot(1,2,2)\ndf.Amount.plot(kind = \"hist\", bins = 40)\nplt.xlabel('Amount', size='large')","06e3131e":"# my_file.py    \nfrom future.builtins import input\nstr_value = input('Type something in: ')","577c1ffe":"#Splitting data into predictors and target\ny = df.iloc[:,30]\nX = df.iloc[:,0:30]\n\nprint(X.columns)\n#print(y.head(3)","0572c64b":"df.shape","54d42058":"y.shape","ddee71ab":"y.head()","0db36ebd":"X_train, X_test, y_train, y_test =   train_test_split(X, y, test_size = 0.3, stratify = y)\n\nX_train.shape\ny_train.value_counts()","76168bb1":"# XGBoost - scale_pos_weight - Control the balance of positive and negative weights, useful for unbalanced classes.\n# A typical value to consider: sum(negative instances) \/ sum(positive instances)\n\nweight = 199020\/344\n\n# Using Random Forest and XGBoost\nrf = RandomForestClassifier(n_estimators=100, class_weight={0:1,1:7})\n#xg = XGBClassifier(scale_pos_weight = weight, learning_rate = 0.7,\n#                   reg_alpha= 0.8,\n#                   reg_lambda= 1)\n\nrf1 = rf.fit(X_train,y_train)\n#xg1 = xg.fit(X_train,y_train)","1fd50029":"y_pred_rf = rf1.predict(X_test)\n#y_pred_xg= xg1.predict(X_test)\n\ny_pred_rf_prob = rf1.predict_proba(X_test)\n#y_pred_xg_prob = xg1.predict_proba(X_test)\n\nprint(\"RF - Accuracy - \",accuracy_score(y_test,y_pred_rf))\n#print(\"XGBoost - Accuracy - \",accuracy_score(y_test,y_pred_xg))\n\nprint(\"RF:\\n\",confusion_matrix(y_test,y_pred_rf))\n#print(\"XGBoost:\\n\",confusion_matrix(y_test,y_pred_xg))\n\nfpr_rf1, tpr_rf1, thresholds = roc_curve(y_test,\n                                 y_pred_rf_prob[: , 1],\n                                         pos_label= 1\n                                 )\n\n#fpr_xg1, tpr_xg1, thresholds = roc_curve(y_test,\n#                                 y_pred_xg_prob[: , 1],\n#                                 pos_label= 1\n#                                 )\n\nprint(\"RF - AUC: \",auc(fpr_rf1,tpr_rf1))\n#print(\"XGBoost - AUC: \",auc(fpr_xg1,tpr_xg1))\n\np_rf,r_rf,f_rf,_ = precision_recall_fscore_support(y_test,y_pred_rf)\n#p_xg,r_xg,f_xg,_ = precision_recall_fscore_support(y_test,y_pred_xg)\n\nprint(\"Random Forest:\\n Precision: \",p_rf, \"Recall: \", r_rf)\n#print(\"XGBoost:\\n Precision: \",p_xg, \"Recall: \", r_xg)","7f529353":"# Oversampling and balancing using SMOTE\nsm = SMOTE(random_state=42)\nX_bal, y_bal = sm.fit_sample(X_train, y_train)\n\ncolumns = X_train.columns\nX_bal = pd.DataFrame(data = X_bal, columns = columns)\n\nprint(X_train.shape)\nprint(X_bal.shape)\nprint(np.unique(y_bal, return_counts=True))","8fd00d72":"# Initialising the models\nrf_sm = RandomForestClassifier(n_estimators=100)\n#xg_sm = XGBClassifier(learning_rate=0.7,\n#                   reg_alpha= 0.8,\n#                   reg_lambda= 1\n#                  )\n\n# training the models\nrf_sm1 = rf_sm.fit(X_bal,y_bal)\n#xg_sm1 = xg_sm.fit(X_bal,y_bal)\n","af921e3f":"print(\"BY USING SMOTE(SYNTHETIC MINORITY OVERSAMPLING TEchnique) Smote: \\n\")\n\n# Making predictions on the test data\ny_pred_rf1 = rf_sm1.predict(X_test)\n#y_pred_xg1= xg_sm1.predict(X_test)\n\ny_pred_rf_prob1 = rf_sm1.predict_proba(X_test)\n#y_pred_xg_prob1 = xg_sm1.predict_proba(X_test)\n\nprint(\"RF - Accuracy - \",accuracy_score(y_test,y_pred_rf1))\n#print(\"XGBoost - Accuracy - \",accuracy_score(y_test,y_pred_xg1))\n\nprint(\"RF:\\n\",confusion_matrix(y_test,y_pred_rf1))\n#print(\"XGBoost:\\n\",confusion_matrix(y_test,y_pred_xg1))\n\nfpr_rf2, tpr_rf2, thresholds = roc_curve(y_test,\n                                 y_pred_rf_prob1[: , 1],\n                                 pos_label= 1\n                                 )\n\n#fpr_xg2, tpr_xg2, thresholds = roc_curve(y_test,\n#                                y_pred_xg_prob1[: , 1],\n#                                pos_label= 1\n#                                )\n\nprint(\"RF - AUC: \",auc(fpr_rf2,tpr_rf2))\n#print(\"XGBoost - AUC: \",auc(fpr_xg2,tpr_xg2))\n\np_rf,r_rf,f_rf,_ = precision_recall_fscore_support(y_test,y_pred_rf1)\n#p_xg,r_xg,f_xg,_ = precision_recall_fscore_support(y_test,y_pred_xg1)\n\nprint(\"Random Forest:\\n Precision: \",p_rf, \"Recall: \", r_rf)\n#print(\"XGBoost:\\n Precision: \",p_xg, \"Recall: \", r_xg)\n","ff9a161c":"# oversampling and balancing dataset with ADASYN\nad = ADASYN()\nX_ad, y_ad = ad.fit_sample(X_train, y_train)\n \nX_ad = pd.DataFrame(data = X_ad, columns = X_train.columns)\n\nprint(X_train.shape)\nprint(X_ad.shape)\nprint(np.unique(y_bal, return_counts=True))","273f5ea0":"#Building model on dataset balanced using ADASYN.\n# oversampling and balancing dataset with ADASYN\nad = ADASYN()\nX_ad, y_ad = ad.fit_sample(X_train, y_train)\n \nX_ad = pd.DataFrame(data = X_ad, columns = X_train.columns)\n\nprint(X_train.shape)\nprint(X_ad.shape)\nprint(np.unique(y_bal, return_counts=True))","35149c7d":"# Initialising the models\nrf_ad = RandomForestClassifier(n_estimators=100)\n#xg_ad = XGBClassifier(learning_rate=0.8,\n#                  reg_alpha= 0.8,\n#                  reg_lambda= 0.8)\n\n# training the models\nrf_ad1 = rf_ad.fit(X_ad,y_ad)\n#xg_ad1 = xg_ad.fit(X_ad,y_ad)","37b038fd":"print(\"BY USING ADASYN(ADAPTIVE SYNTHETIC)TECHNIQUE:\\n\")\n# Making predictions on the test data\ny_pred_rf2 = rf_ad1.predict(X_test)\n#y_pred_xg2 = xg_ad1.predict(X_test)\n\ny_pred_rf_prob2 = rf_ad1.predict_proba(X_test)\n#y_pred_xg_prob2 = xg_ad1.predict_proba(X_test)\n\nprint(\"RF - Accuracy - \",accuracy_score(y_test,y_pred_rf2))\n#print(\"XGBoost - Accuracy - \",accuracy_score(y_test,y_pred_xg2))\n\nprint(\"RF:\\n\",confusion_matrix(y_test,y_pred_rf2))\n#print(\"XGBoost:\\n\",confusion_matrix(y_test,y_pred_xg2))\n\nfpr_rf3, tpr_rf3, thresholds = roc_curve(y_test,\n                                 y_pred_rf_prob2[: , 1],\n                                 pos_label= 1)\n\n#fpr_xg3, tpr_xg3, thresholds = roc_curve(y_test,\n#                                y_pred_xg_prob2[: , 1],\n#                                pos_label= 1\n#                                )\n\nprint(\"RF - AUC: \",auc(fpr_rf3,tpr_rf3))\n#print(\"XGBoost - AUC: \",auc(fpr_xg3,tpr_xg3))\n\np_rf,r_rf,f_rf,_ = precision_recall_fscore_support(y_test,y_pred_rf2)\n#p_xg,r_xg,f_xg,_ = precision_recall_fscore_support(y_test,y_pred_xg2)\n\nprint(\"Random Forest:\\n Precision: \",p_rf, \"Recall: \", r_rf)\n#print(\"XGBoost:\\n Precision: \",p_xg, \"Recall: \", r_xg)\n\n                                         ","d00a66ed":"#ROC curves\nfig = plt.figure(figsize=(14,16))   # Create window frame\n\n\nroc = [[\"Imbalanced\", fpr_rf1, tpr_rf1,'rf'],[\"SMOTE\", fpr_rf2, tpr_rf2,'rf'],\n[\"ADASYN\", fpr_rf3, tpr_rf3,'rf']]\n\nfor i in range(6):\n    #8.1 Connect diagonals\n    ax = fig.add_subplot(3,2,i+1) \n    ax.plot([0, 1], [0, 1], ls=\"--\")  # Dashed diagonal line\n","829f24ff":"#ROC curves\nfig = plt.figure(figsize=(14,16))   # Create window frame\n\n\nroc = [[\"Imbalanced\", fpr_rf1, tpr_rf1,'rf'],[\"SMOTE\", fpr_rf2, tpr_rf2,'rf'],\n[\"ADASYN\", fpr_rf3, tpr_rf3,'rf']]\n\nfor i in range(6):\n    #8.1 Connect diagonals\n    ax = fig.add_subplot(3,2,i+1) \n    ax.plot([0, 1], [0, 1], ls=\"--\")  # Dashed diagonal line\n\n    #8.2 Labels ","34d12b19":"prc = [[\"Imbalanced\", rf1,'rf'],[\"SMOTE\", rf_sm1,'rf'],\n[\"ADASYN\", rf_ad1,'rf']]\n\nfig = plt.figure(figsize=(14,16))\n\nfor i in range(6):\n    ax = fig.add_subplot(3,2,i+1)\n   \n    precision, recall, _ = precision_recall_curve(y_test,prc[i][1].predict_proba(X_test)[:,-1])\n\n    plt.step(recall, precision, color='b', alpha=0.2,\n         where='post')\n    plt.fill_between(recall, precision, step='post', alpha=0.2,\n                 color='b')\n\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n    ttl = prc[i][0]+' '+prc[i][2]\n    plt.title(ttl)","dbf2143f":"import numpy as np # linear algebra\nimport pandas as pd # data processing\n\n# for standardization\nfrom sklearn.preprocessing import StandardScaler as ss\n\n# for splitting into train and test datasets\nfrom sklearn.model_selection import train_test_split \n\n# for modelling\n#from xgboost.sklearn import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# for balancing dataset by oversampling\nfrom imblearn.over_sampling import SMOTE, ADASYN\n\n# for performance metrics\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_recall_curve\nfrom sklearn.metrics import auc, roc_curve, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix, classification_report, average_precision_score\n\n# for data visualization\nimport matplotlib.pyplot as plt\n\n#Miscellaneous\nimport time\nimport random\nimport os","410ad0c5":"import numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nimport time","31e268f4":"tic=time.time()\nfull_data=pd.read_csv(r'C:\\Users\\DELL\\Downloads\\archive (1)\\creditcard.csv')\nfull_data=full_data.sample(frac=1)#randomize the whole dataset\nfull_features=full_data.drop([\"Time\",\"Class\"],axis=1)\nfull_labels=pd.DataFrame(full_data[[\"Class\"]])\nfull_features_array=full_features.values\nfull_labels_array=full_labels.values\ntrain_features,test_features,train_labels,test_labels=train_test_split(full_features_array,full_labels_array,train_size=0.90)\ntrain_features=normalize(train_features)\ntest_features=normalize(test_features)\n#k_means_classification --> k_means_clustering, confsion_matrix, reassigning\nkmeans=KMeans(n_clusters=2,random_state=0,algorithm=\"elkan\",max_iter=10000,n_jobs=-1)\nkmeans.fit(train_features)\nkmeans_predicted_train_labels=kmeans.predict(train_features)","2df5230b":"#confusion matrix\n# tn fp\n# fn tp\nprint(\"tn --> true negatives\")\nprint(\"fp --> false positives\")\nprint(\"fn --> false negatives\")\nprint(\"tp --> true positives\")\ntn,fp,fn,tp=confusion_matrix(train_labels,kmeans_predicted_train_labels).ravel()\nreassignflag=False\nif tn+tp<fn+fp:\n\t# clustering is opposite of original classification\n\treassignflag=True\nkmeans_predicted_test_labels=kmeans.predict(test_features)\nif reassignflag:\n\tkmeans_predicted_test_labels=1-kmeans_predicted_test_labels","35bd5b86":"#calculating confusion matrix for kmeans\ntn,fp,fn,tp=confusion_matrix(test_labels,kmeans_predicted_test_labels).ravel()","7a8529ac":"#scoring kmeans\nkmeans_accuracy_score=accuracy_score(test_labels,kmeans_predicted_test_labels)\nkmeans_precison_score=precision_score(test_labels,kmeans_predicted_test_labels)\nkmeans_recall_score=recall_score(test_labels,kmeans_predicted_test_labels)\nkmeans_f1_score=f1_score(test_labels,kmeans_predicted_test_labels)","51b16225":"#printing\nprint(\"\")\nprint(\"K-Means\")\nprint(\"Confusion Matrix\")\nprint(\"tn =\",tn,\"fp =\",fp)\nprint(\"fn =\",fn,\"tp =\",tp)\nprint(\"Scores\")\nprint(\"Accuracy -->\",kmeans_accuracy_score)\nprint(\"Precison -->\",kmeans_precison_score)\nprint(\"Recall -->\",kmeans_recall_score)\nprint(\"F1 -->\",kmeans_f1_score)","576fafd1":"#k_nearest_neighbours_classification\nknn=KNeighborsClassifier(n_neighbors=5,algorithm=\"kd_tree\",n_jobs=-1)\nknn.fit(train_features,train_labels.ravel())\nknn_predicted_test_labels=knn.predict(test_features)","12dbf43a":"#calculating confusion matrix for knn\ntn,fp,fn,tp=confusion_matrix(test_labels,knn_predicted_test_labels).ravel()","48db1542":"#scoring knn\nknn_accuracy_score=accuracy_score(test_labels,knn_predicted_test_labels)\nknn_precison_score=precision_score(test_labels,knn_predicted_test_labels)\nknn_recall_score=recall_score(test_labels,knn_predicted_test_labels)\nknn_f1_score=f1_score(test_labels,knn_predicted_test_labels)","0006310e":"#printing\nprint(\"\")\nprint(\"K-Nearest Neighbours\")\nprint(\"Confusion Matrix\")\nprint(\"tn =\",tn,\"fp =\",fp)\nprint(\"fn =\",fn,\"tp =\",tp)\nprint(\"Scores\")\nprint(\"Accuracy -->\",knn_accuracy_score)\nprint(\"Precison -->\",knn_precison_score)\nprint(\"Recall -->\",knn_recall_score)\nprint(\"F1 -->\",knn_f1_score)\n","6cb6d0d4":"#time elapsed\ntoc=time.time()\nelapsedtime=toc-tic\nprint(\"\")\nprint(\"Time Taken : \"+str(elapsedtime)+\"seconds\")\n","c3624505":"### Printing the ratio of fraud cases and non-fraud transactions","11f0cbcb":"### Print the ratio of fraud cases\nratio_cases = occ\/len(df.index)\nprint(f'Ratio of fraudulent cases: {ratio_cases[1]}\\nRatio of non-fraudulent cases: {ratio_cases[0]}')\n","e7ca18dc":"df.groupby('Class').mean()","d1ce29e5":"df.groupby('Class').median()","70538091":"df['flag_as_fraud'] = np.where(np.logical_and(df.V1 < -3, df.V3 < -5), 1, 0)","2608bbe3":"pd.crosstab(df.Class, df.flag_as_fraud, rownames=['Actual Fraud'], colnames=['Flagged Fraud'])","19aeb5ff":"No_of_frauds= len(df[df[\"Class\"]==1])\nNo_of_normals = len(df[df[\"Class\"]==0])\ntotal= No_of_frauds + No_of_normals\nFraud_percent= (No_of_frauds \/ total)*100\nNormal_percent= (No_of_normals \/ total)*100\n\nprint(\"The number of normal transactions(Class 0) are: \", No_of_normals)\nprint(\"The number of fraudulent transactions(Class 1) are: \", No_of_frauds)\nprint(\"Class 0 percentage = \", Normal_percent)\nprint(\"Class 1 percentage = \", Fraud_percent)","eaa28d19":"fig,ax  = plt.subplots(figsize = (6,4))\nax = sns.countplot(x = 'Class',data= data)\nplt.tight_layout()","1ce682ec":"#unbalace of the data using bar graph\ncount = pd.value_counts(df['Class'], sort = True).sort_index()\ncount.plot(kind = 'bar')\nplt.title('Unbalance Data')\nplt.xlabel('Class')\nplt.ylabel('Frequency')\nplt.show()\n\n# unbalace of the data using pie chart\ncount = pd.value_counts(df['Class'], sort = True).sort_index()\ncount.plot(kind = 'pie')\nplt.title('Unbalance Data')\nplt.show()","c9546150":"#Obtain and print the accuracy score by comparing the actual labels y_test with our predicted labels predicted.","dff14de2":"#print(f'Accuracy Score:\\n{accuracy_score(y_test, predicted):0.3f}')\n","b943d29c":"#df   =pd.read_csv(r'C:\\Users\\DELL\\Downloads\\archive (1)\\creditcard.csv')\ndf2 = pd.read_csv(r'C:\\Users\\DELL\\Downloads\\archive\\creditcard.csv')\ndf2.head()","e638c22f":"#1.1.2.1 def prep_data\ndef prep_data(df: pd.DataFrame) -> (np.ndarray, np.ndarray):\n    \"\"\"\n    Convert the DataFrame into two variable\n    X: data columns (V1 - V28)\n    y: lable column\n    \"\"\"\n    X = df.iloc[:, 2:30].values\n    y = df.Class.values\n    return X, y","4adc0089":"X, y = prep_data(df2)\nprint(f'X shape: {X.shape}\\ny shape: {y.shape}')\n","6fb3f842":"#the above output sample look likethis X shape: #wrong figure here (7300, 28) y shape: (7300,)","36506ec6":"X[0, :]","f1a92f8f":"df2.Class.value_counts()\n#0    7000\n#1     300\n","475f82a7":"#Name: Class, dtype: int64\n# Count the total number of observations from the length of y\ntotal_obs = len(y)\ntotal_obs\n#7300\n","6d395ad2":"# Count the total number of non-fraudulent observations \nnon_fraud = [i for i in y if i == 0]\ncount_non_fraud = non_fraud.count(0)\ncount_non_fraud\n#7000\n","a81a2eba":"percentage = count_non_fraud\/total_obs * 100\nprint(f'{percentage:0.2f}%')","de3299ed":"# Run the prep_data function\nX, y = prep_data(df)\n","31ecf18e":"print(f'X shape: {X.shape}\\ny shape: {y.shape}')\n#X shape: (5050, 28)\n#y shape: (5050,)\n","5d44bed2":"# Count the occurrences of fraud and no fraud and print them\nocc = df['Class'].value_counts()\nocc","317b00fe":"# Print the ratio of fraud cases\nratio_cases = occ\/len(df.index)\nprint(f'Ratio of fraudulent cases: {ratio_cases[1]}\\nRatio of non-fraudulent cases: {ratio_cases[0]}')","87aa90be":"#df.drop('Time', axis = 1, inplace = True)\n#print(df.head())","81c47379":"cases = len(df)\nnonfraud_count = len(df[df.Class == 0])\nfraud_count = len(df[df.Class == 1])\nfraud_percentage = round(fraud_count\/nonfraud_count*100, 2)\n\nprint(cl('FRAUD AND NON-FRAUD CASE-WISE COUNT', attrs = ['bold']))\nprint(cl('-------------------------------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('Total number of cases are      :: {}'.format(cases), attrs = ['bold']))\nprint(cl('Number of Non-fraud cases are  :: {}'.format(nonfraud_count), attrs = ['bold']))\nprint(cl('Number of fraud cases are      :: {}'.format(fraud_count), attrs = ['bold']))\nprint(cl('Percentage of fraud cases is   :: {}'.format(fraud_percentage), attrs = ['bold']))\nprint(cl('-------------------------------------------------------------------------------------------------', attrs = ['bold']))","037bad1d":"nonfraud_cases = df[df.Class == 0]\nfraud_cases = df[df.Class == 1]\n\n#print(cl('--------------------------------------------', attrs = ['bold']))\nprint(cl('CASE-WISE AMOUNT OF STATISTICS', attrs = ['bold']))\nprint(cl('--------------------------------------------', attrs = ['bold']))\nprint(cl('NON-FRAUD CASES  AMOUNT OF STATS', attrs = ['bold']))\nprint(cl('--------------------------------------------', attrs = ['bold']))\nprint(nonfraud_cases.Amount.describe())\nprint(cl('--------------------------------------------', attrs = ['bold']))\nprint(cl('FRAUD CASES AMOUNT OF STATS', attrs = ['bold']))\nprint(cl('--------------------------------------------', attrs = ['bold']))\nprint(fraud_cases.Amount.describe())\nprint(cl('--------------------------------------------', attrs = ['bold']))","076a01b6":"#undersampling\nfrom imblearn.under_sampling import RandomUnderSampler\nrus = RandomUnderSampler()\nX_rus,y_rus = rus.fit_sample(X_train,y_train)\n\n#lets check sampled data\nprint(pd.Series(y_rus).value_counts())","8f307dec":"# IMPORTING PACKAGES\n\nimport pandas as pd # data processing\nimport numpy as np # working with arrays\nimport matplotlib.pyplot as plt # visualization\nfrom termcolor import colored as cl # text customization\nimport itertools # advanced tools\n\nfrom sklearn.preprocessing import StandardScaler # data normalization\nfrom sklearn.model_selection import train_test_split # data split\nfrom sklearn.tree import DecisionTreeClassifier # Decision tree algorithm\nfrom sklearn.neighbors import KNeighborsClassifier # KNN algorithm\nfrom sklearn.linear_model import LogisticRegression # Logistic regression algorithm\nfrom sklearn.svm import SVC # SVM algorithm\nfrom sklearn.ensemble import RandomForestClassifier # Random forest tree algorithm\n#from xgboost import XGBClassifier # XGBoost algorithm\n\nfrom sklearn.metrics import confusion_matrix # evaluation metric\nfrom sklearn.metrics import accuracy_score # evaluation metric\nfrom sklearn.metrics import f1_score # evaluation metric\n\n","c62f4e7b":"# IMPORTING DATA\n#df = pd.read_csv(r'Rc:\\Users\\DELL\\Downloads\\archive (1)\\creditcard.csv')\ndf=pd.read_csv(r'C:\\Users\\DELL\\Downloads\\archive (1)\\creditcard.csv')","dc1154e2":"df.drop('Time', axis = 1, inplace = True)\n\n","10f195ab":"print(df.head())","30cd36aa":"# EDA\n# 1. Count & percentage\ncases = len(df)\nnonfraud_count = len(df[df.Class == 0])\nfraud_count = len(df[df.Class == 1])\nfraud_percentage = round(fraud_count\/nonfraud_count*100, 2)\n","07501419":"print(cl('CASE COUNT', attrs = ['bold']))\nprint(cl('--------------------------------------------', attrs = ['bold']))\nprint(cl('Total number of cases are {}'.format(cases), attrs = ['bold']))\nprint(cl('Number of Non-fraud cases are {}'.format(nonfraud_count), attrs = ['bold']))\nprint(cl('Number of Non-fraud cases are {}'.format(fraud_count), attrs = ['bold']))\nprint(cl('Percentage of fraud cases is {}'.format(fraud_percentage), attrs = ['bold']))\nprint(cl('--------------------------------------------', attrs = ['bold']))","2efb95c4":"# 2. Description\nnonfraud_cases = df[df.Class == 0]\nfraud_cases = df[df.Class == 1]","020f9e6e":"print(cl('CASE AMOUNT STATISTICS', attrs = ['bold']))\nprint(cl('--------------------------------------------', attrs = ['bold']))\nprint(cl('NON-FRAUD CASE AMOUNT STATS', attrs = ['bold']))\nprint(nonfraud_cases.Amount.describe())\nprint(cl('--------------------------------------------', attrs = ['bold']))\nprint(cl('FRAUD CASE AMOUNT STATS', attrs = ['bold']))\nprint(fraud_cases.Amount.describe())\nprint(cl('--------------------------------------------', attrs = ['bold']))\n","890418b6":"# DATA SPLIT\n\nX = df.drop('Class', axis = 1).values\ny = df['Class'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n","66e7411a":"print(cl('X_train samples : ', attrs = ['bold']), X_train[:1])\nprint(cl('X_test samples : ', attrs = ['bold']), X_test[0:1])\nprint(cl('y_train samples : ', attrs = ['bold']), y_train[0:10])\nprint(cl('y_test samples : ', attrs = ['bold']), y_test[0:10])\n","9da4f6f6":"# MODELING\n\n# 1. Decision Tree\n\ntree_model = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')\ntree_model.fit(X_train, y_train)\ntree_yhat = tree_model.predict(X_test)","bb41bebd":"# 2. K-Nearest Neighbors\n\nn = 5\n\nknn = KNeighborsClassifier(n_neighbors = n)\nknn.fit(X_train, y_train)\nknn_yhat = knn.predict(X_test)\n\n","6bd8b3dd":"# 3. Logistic Regression\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nlr_yhat = lr.predict(X_test)\n","10d0649b":"# 4. SVM \nsvm = SVC()\nsvm.fit(X_train, y_train)\nsvm_yhat = svm.predict(X_test)\n","455756ec":"# 5. Random Forest Tree\n\nrf = RandomForestClassifier(max_depth = 4)\nrf.fit(X_train, y_train)\nrf_yhat = rf.predict(X_test)","b3db5485":"# 6. XGBoost\n\n#xgb = XGBClassifier(max_depth = 4)\n#xgb.fit(X_train, y_train)\n#xgb_yhat = xgb.predict(X_test)\n","865bf2d0":"# EVALUATION\n# 1. Accuracy score\nprint(cl('ACCURACY SCORE', attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('Accuracy score of the Decision Tree model is {}'.format(accuracy_score(y_test, tree_yhat)), attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('Accuracy score of the KNN model is {}'.format(accuracy_score(y_test, knn_yhat)), attrs = ['bold'], color = 'green'))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('Accuracy score of the Logistic Regression model is {}'.format(accuracy_score(y_test, lr_yhat)), attrs = ['bold'], color = 'red'))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('Accuracy score of the SVM model is {}'.format(accuracy_score(y_test, svm_yhat)), attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('Accuracy score of the Random Forest Tree model is {}'.format(accuracy_score(y_test, rf_yhat)), attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\n#print(cl('Accuracy score of the XGBoost model is {}'.format(accuracy_score(y_test, xgb_yhat)), attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))","189111eb":"# 2. F1 score\n\nprint(cl('F1 SCORE IS::', attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('F1 score of the Decision Tree model is      :: {}'.format(f1_score(y_test, tree_yhat)), attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('F1 score of the KNN model is                :: {}'.format(f1_score(y_test, knn_yhat)), attrs = ['bold'], color = 'green'))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('F1 score of the Logistic Regression model is:: {}'.format(f1_score(y_test, lr_yhat)), attrs = ['bold'], color = 'red'))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('F1 score of the SVM model is                :: {}'.format(f1_score(y_test, svm_yhat)), attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\nprint(cl('F1 score of the Random Forest Tree model is :: {}'.format(f1_score(y_test, rf_yhat)), attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\n#print(cl('F1 score of the XGBoost model is {}'.format(f1_score(y_test, xgb_yhat)), attrs = ['bold']))\nprint(cl('------------------------------------------------------------------------', attrs = ['bold']))\n","3a102e83":"# 3. Confusion Matrix\n\n# defining the plot function\n\ndef plot_confusion_matrix(cm, classes, title, normalize = False, cmap = plt.cm.Blues):\n    title = 'Confusion Matrix of {}'.format(title)\n    if normalize:\n        cm = cm.astype(float) \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","68ef2b14":"# Compute confusion matrix for the models\n\ntree_matrix = confusion_matrix(y_test, tree_yhat, labels = [0, 1]) # Decision Tree\nknn_matrix = confusion_matrix(y_test, knn_yhat, labels = [0, 1]) # K-Nearest Neighbors\nlr_matrix = confusion_matrix(y_test, lr_yhat, labels = [0, 1]) # Logistic Regression\nsvm_matrix = confusion_matrix(y_test, svm_yhat, labels = [0, 1]) # Support Vector Machine\nrf_matrix = confusion_matrix(y_test, rf_yhat, labels = [0, 1]) # Random Forest Tree\n#xgb_matrix = confusion_matrix(y_test, xgb_yhat, labels = [0, 1]) # XGBoost\n","b9dcc5f1":"# Plot the confusion matrix\n\nplt.rcParams['figure.figsize'] = (6, 6)","868db0bc":"# 1. Decision tree\ntree_cm_plot = plot_confusion_matrix(tree_matrix, \n                                classes = ['Non-Default(0)','Default(1)'], \n                                normalize = False, title = 'Decision Tree')\nplt.savefig('tree_cm_plot.png')\nplt.show()\n","f7b88f2b":"# 2. K-Nearest Neighbors\nknn_cm_plot = plot_confusion_matrix(knn_matrix, \n                                classes = ['Non-Default(0)','Default(1)'], \n                                normalize = False, title = 'KNN')\nplt.savefig('knn_cm_plot.png')\nplt.show()\n","fc739a5b":"# 3. Logistic regression\nlr_cm_plot = plot_confusion_matrix(lr_matrix, \n                                classes = ['Non-Default(0)','Default(1)'], \n                                normalize = False, title = 'Logistic Regression')\nplt.savefig('lr_cm_plot.png')\nplt.show()\n","301d8d9c":"# 4. Support Vector Machine\n\nsvm_cm_plot = plot_confusion_matrix(svm_matrix, \n                                classes = ['Non-Default(0)','Default(1)'], \n                                normalize = False, title = 'SVM')\nplt.savefig('svm_cm_plot.png')\nplt.show()","0d263934":"# 5. Random forest tree\nrf_cm_plot = plot_confusion_matrix(rf_matrix, \n                                classes = ['Non-Default(0)','Default(1)'], \n                                normalize = False, title = 'Random Forest Tree')\nplt.savefig('rf_cm_plot.png')\nplt.show()\n","edd05084":"# 6. XGBoost\n#xgb_cm_plot = plot_confusion_matrix(xgb_matrix, \n#                                classes = ['Non-Default(0)','Default(1)'], \n#                                normalize = False, title = 'XGBoost')\n#plt.savefig('xgb_cm_plot.png')\n#plt.show()","dd2f4153":"#1.1.2.2  def plot_data\n# Define a function to create a scatter plot of our data and labels\nimport numpy as np\ndef plot_data(X: np.ndarray, y: np.ndarray):\n    plt.scatter(X[y == 0, 0], X[y == 0, 1], label=\"Class #0\", alpha=0.5, linewidth=0.15)\n    plt.scatter(X[y == 1, 0], X[y == 1, 1], label=\"Class #1\", alpha=0.5, linewidth=0.15, c='r')\n    plt.legend()\n    return plt.show()","d9912df6":"# Create X and y from the prep_data function \nX, y = prep_data(df)\n","d03999d2":"# Plot our data by running our plot data function on X and y\nplot_data(X, y)","cb818cde":"#1.1.2.3  Reproduced using the DataFrame\nplt.scatter(df.V2[df.Class == 0], df.V3[df.Class == 0], label=\"Class #0\", alpha=0.5, linewidth=0.15)\nplt.scatter(df.V2[df.Class == 1], df.V3[df.Class == 1], label=\"Class #1\", alpha=0.5, linewidth=0.15, c='r')\nplt.legend()\nplt.show()","3adf8668":"#1.2.3.1  def compare_plot\ndef compare_plot(X: np.ndarray, y: np.ndarray, X_resampled: np.ndarray, y_resampled: np.ndarray, method: str):\n    plt.subplot(1, 2, 1)\n    plt.scatter(X[y == 0, 0], X[y == 0, 1], label=\"Class #0\", alpha=0.5, linewidth=0.15)\n    plt.scatter(X[y == 1, 0], X[y == 1, 1], label=\"Class #1\", alpha=0.5, linewidth=0.15, c='r')\n    plt.title('Original Set')\n    plt.subplot(1, 2, 2)\n    plt.scatter(X_resampled[y_resampled == 0, 0], X_resampled[y_resampled == 0, 1], label=\"Class #0\", alpha=0.5, linewidth=0.15)\n    plt.scatter(X_resampled[y_resampled == 1, 0], X_resampled[y_resampled == 1, 1], label=\"Class #1\", alpha=0.5, linewidth=0.15, c='r')\n    plt.title(method)\n    plt.legend()\n    plt.show()","92045b2e":"compare_plot(X, y, X_resampled, y_resampled, method='SMOTE')\n","13040eba":"import seaborn as sns\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=5000, n_features=2, n_informative=2,\n                           n_redundant=0, n_repeated=0, n_classes=3,\n                           n_clusters_per_class=1,\n                           weights=[0.01, 0.05, 0.94],\n                           class_sep=0.8, random_state=0)\n\nimport matplotlib.pyplot as plt\ncolors = ['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y]\nkwarg_params = {'linewidth': 1, 'edgecolor': 'black'}\nplt.scatter(X[:, 0], X[:, 1], c=colors, **kwarg_params)\nsns.despine()","931bcbe8":"from imblearn.under_sampling import ClusterCentroids\ntrans = ClusterCentroids(random_state=0)\nX_resampled, y_resampled = trans.fit_sample(X, y)\n\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled], **kwarg_params)\nsns.despine()","a29bc68b":"from imblearn.under_sampling import NearMiss\ntrans = NearMiss(version=1)\nX_resampled, y_resampled = trans.fit_sample(X, y)\n\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled], **kwarg_params)\nsns.despine()","4555e0fc":"from imblearn.under_sampling import NearMiss\ntrans = NearMiss(version=2)\nX_resampled, y_resampled = trans.fit_sample(X, y)\n\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled], **kwarg_params)\nsns.despine()","c5d68df3":"from imblearn.under_sampling import NearMiss\ntrans = NearMiss(version=3)\nX_resampled, y_resampled = trans.fit_sample(X, y)\n\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled], **kwarg_params)\nsns.despine()","d54bce88":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X_1 = LabelEncoder()\nX[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\nlabelencoder_X_2 = LabelEncoder()\n\nX[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\nonehotencoder = OneHotEncoder(categorical_features = [1])\nX = onehotencoder.fit_transform(X).toarray()\n#Traceback (most recent call last):\n\n#File \"<ipython-input-4-05deb1f02719>\", line 2, in <module>\n#onehotencoder = OneHotEncoder(categorical_features = [1])","8d3e7bd4":"import numpy as np\nsampler = TomekLinks(random_state=0)\n\n# minority class\nX_minority = np.transpose([[1.1, 1.3, 1.15, 0.8, 0.55, 2.1],\n                           [1., 1.5, 1.7, 2.5, 0.55, 1.9]])\n# majority class\nX_majority = np.transpose([[2.1, 2.12, 2.13, 2.14, 2.2, 2.3, 2.5, 2.45],\n                           [1.5, 2.1, 2.7, 0.9, 1.0, 1.4, 2.4, 2.9]])\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\nax_arr = (ax1, ax2)\ntitle_arr = ('Removing only majority samples',\n             'Removing all samples')\nfor ax, title, sampler in zip(ax_arr,\n                              title_arr,\n                              [TomekLinks(ratio='auto', random_state=0),\n                               TomekLinks(ratio='all', random_state=0)]):\n    X_res, y_res = sampler.fit_sample(np.vstack((X_minority, X_majority)),\n                                      np.array([0] * X_minority.shape[0] +\n                                               [1] * X_majority.shape[0]))\n    ax.scatter(X_res[y_res == 0][:, 0], X_res[y_res == 0][:, 1],\n               label='Minority class', s=200, marker='_')\n    ax.scatter(X_res[y_res == 1][:, 0], X_res[y_res == 1][:, 1],\n               label='Majority class', s=200, marker='+')\n\n# highlight the samples of interest\n    ax.scatter([X_minority[-1, 0], X_majority[1, 0]],\n               [X_minority[-1, 1], X_majority[1, 1]],\n               label='Tomek link', s=200, alpha=0.3)\n\n    ax.set_title(title)\n#     make_plot_despine(ax)\nfig.tight_layout()\n\nplt.show()","e304dc04":"import sklearn\nprint(sklearn.__version__)","e8ec94bd":"#from keras.models import load_model\n\n#classifier = load_model('path_to_your_model')","9872aca3":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)","ccf1e1f7":"#conda update conda \n#conda update scikit-learn\n\nfrom imblearn.under_sampling import TomekLinks\ntrans = TomekLinks(ratio='all')\nX_resampled, y_resampled = trans.fit_sample(X, y)\n\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled], **kwarg_params)\nsns.despine()","586a9d2a":"from imblearn.under_sampling import TomekLinks\ntrans = TomekLinks(ratio='all')\nX_resampled, y_resampled = trans.fit_sample(X, y)\n\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled], **kwarg_params)\nsns.despine()","46570a9d":"len(X), len(X_resampled)","e36b4afa":"_X_minority = X[np.where(y == 0)[0]]\nplt.scatter(_X_minority[:, 0], _X_minority[:, 1], c='white', linewidth=1, edgecolor='red')\n\ntrans = TomekLinks(ratio='all')\nX_resampled, y_resampled = trans.fit_sample(X, y)\n_X_minority = X_resampled[np.where(y_resampled == 0)[0]]\nplt.scatter(_X_minority[:, 0], _X_minority[:, 1], c='lightgreen', edgecolor='green', linewidth=1)\nplt.suptitle(\"Tomek Links Removed from Minority Class\")\npass","e1768269":"import seaborn as sns\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=5000, n_features=2, n_informative=2,\n                           n_redundant=0, n_repeated=0, n_classes=3,\n                           n_clusters_per_class=1,\n                           weights=[0.01, 0.05, 0.94],\n                           class_sep=0.8, random_state=0)\n\nimport matplotlib.pyplot as plt\ncolors = ['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y]\nkwarg_params = {'linewidth': 1, 'edgecolor': 'black'}\nplt.scatter(X[:, 0], X[:, 1], c=colors, **kwarg_params)\nsns.despine()\nplt.suptitle(\"Benchmark Data Resampled with SMOTE\")\npass","9d370c54":"from imblearn.over_sampling import SMOTE\n\nX_resampled, y_resampled = SMOTE().fit_sample(X, y)\nkwarg_params = {'linewidth': 1, 'edgecolor': 'black'}\ncolors = ['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled]\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=colors, **kwarg_params)\nsns.despine()\nplt.suptitle(\"Benchmark Data Resampled with SMOTE\")\npass","0e03f980":"colors = ['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled]\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=colors, **kwarg_params)\nax = plt.gca()\nax.set_xlim([-0.1, 1.6])\nax.set_ylim([0, 1.7])","14b75c74":"X, y = make_classification(n_samples=5000, n_features=2, n_informative=2,\n                           n_redundant=0, n_repeated=0, n_classes=3,\n                           n_clusters_per_class=1,\n                           weights=[0.10, 0.10, 0.80],\n                           class_sep=0.8, random_state=0)\nX_resampled, y_resampled = SMOTE().fit_sample(X, y)\nkwarg_params = {'linewidth': 1, 'edgecolor': 'black'}\ncolors = ['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled]\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=colors, **kwarg_params)\nsns.despine()\nplt.suptitle(\"Higher-Occurrence Benchmark Data Resampled with SMOTE\")\npass","7ec69468":"X, y = make_classification(n_samples=5000, n_features=2, n_informative=2,\n                           n_redundant=0, n_repeated=0, n_classes=3,\n                           n_clusters_per_class=1,\n                           weights=[0.01, 0.05, 0.94],\n                           class_sep=0.8, random_state=0)\n\n#X_resampled, y_resampled = SMOTE(kind='borderline1').fit_sample(X, y)\nkwarg_params = {'linewidth': 1, 'edgecolor': 'black'}\ncolors = ['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled]\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=colors, **kwarg_params)\nsns.despine()\nplt.suptitle(\"Benchmark Data Resampled with SMOTE-borderline1\")\npass","8a620368":"#X_resampled, y_resampled = SMOTE(kind='borderline2').fit_sample(X, y)\nkwarg_params = {'linewidth': 1, 'edgecolor': 'black'}\ncolors = ['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled]\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=colors, **kwarg_params)\nsns.despine()\nplt.suptitle(\"Benchmark Data Resampled with SMOTE-borderline2\")\npass","cf3cbea1":"#X_resampled, y_resampled = SMOTE(kind='svm').fit_sample(X, y)\nkwarg_params = {'linewidth': 1, 'edgecolor': 'black'}\ncolors = ['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled]\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=colors, **kwarg_params)\nsns.despine()\nplt.suptitle(\"Benchmark Data Resampled with SMOTE-borderline2\")\npass\n","cfcf9243":"from imblearn.over_sampling import ADASYN\n\nX_resampled, y_resampled = ADASYN().fit_sample(X, y)\nkwarg_params = {'linewidth': 1, 'edgecolor': 'black'}\ncolors = ['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled]\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=colors, **kwarg_params)\nsns.despine()\nplt.suptitle(\"Benchmark Data Resampled with SMOTE-borderline2\")\npass","5f8b2375":"#https:\/\/www.kaggle.com\/residentmario\/oversampling-with-smote-and-adasyn","6bf1d24b":"### SMOTE FOR BALANCING DATA\n# define dataset\n#refered link\n#https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/\nX, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n\tn_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)","2bc60511":"# summarize class distribution\nimport collections\n","59c2c1a6":">>> import collections\n#>>> Counter()\n#Traceback (most recent call last):\n#File \"<pyshell#5>\", line 1, in <module>\n#Counter()","022867ca":"#print collections.Counter(['a','b','c','a','b','b'])\ncounter = Counter(y)\nprint(counter)","a149684e":"class sampleclass: \n  count = 0  # class attribute \n\n  def increase(self):\n      sampleclass.count += 1\n\n# Calling increase() on an object \ns1 = sampleclass() \ns1.increase()        \nprint(s1.count)","ec070b9a":"from collections import Counter","8dc7e463":"# scatter plot of examples by class label\\\n\nfor label, _ in counter.items():\n\trow_ix = where(y == label)[0]\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\npyplot.legend()\npyplot.show()","16a8e1bd":"\nlen(X), len(X_resampled)","3cc554e1":"# Generate and plot a synthetic imbalanced classification dataset\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom matplotlib import pyplot\nfrom numpy import where\n# define dataset\nX, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n\tn_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n# summarize class distribution\ncounter = Counter(y)\nprint(counter)\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n\trow_ix = where(y == label)[0]\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\npyplot.legend()\npyplot.show()","d28d5ef2":"total=df.isnull().sum().sort_values(ascending=False)\nprint(total)\n","f755f419":"total","cabe9168":"percent=(df.isnull().sum()\/df.isnull().count()*100).sort_values(ascending=False)\npercent","91b33457":"s = pd.Series([1.1, 2.3])\na = np.array(s)\nprint(a)  # [1.1 2.3]","c78a6eb1":"s = pd.Series([1.1, np.nan])\na = np.ma.masked_invalid(s)\nprint(a)  # [1.1 --]","a4e94ecb":"#To find out pandas version;\nprint(pandas.__version__)","783706dd":"#pd.concat([total.percent],axis=1,keys=['Total','percent']).transpose()\npd.concat([df],axis=0, ignore_index=True)","b6b72cb2":"\npd.concat([df],axis=0).reset_index(drop=True)","d0630f25":"#df=pd.read_csv(r'C:\\Users\\DELL\\Downloads\\archive (1)\\creditcard.csv')\ndf.head()","028da516":"#Data Imbalance:\n    ","b1c43e9f":"temp=df['Class'].value_counts()\ndf=pd.DataFrame({'Class':temp.index,'values':temp.values})\nprint(df)","139c5d90":"len(df[df['Class']==1]),len(df[df['Class']==0])","5cf9df55":"percentage_of_class_0=((df[df['Class']==0].count())\/df['Class'].count())*100","867c16dc":"percentage_of_class_1=((df[df['Class']==1].count())\/df['Class'].count())*100","34f7dc96":"print(percentage_of_class_0['Class'],'%')\nprint(percentage_of_class_1['Class'],'%')\n","9ddb32df":"from matplotlib import *","80ca3504":"#%matplotlib inline\nimport matplotlib.pyplot as plt\n#from matplotlib import pyplot as plt\nax=sns.countplot('Class',data=df)\nannot_plot=annot_plot(ax,0.08,1)\n","6dd837f3":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom time import time\n","90c630c4":"#split data into training and test set\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=42)","c6330ed7":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(X_train,y_train)\ny_red=lr.predict(X_test)","6a49a22d":"from sklearn.metrics import accuracy_score\nlr.score(X_test,y_test)","455d4943":"from sklearn.metrics import confusion_matrix\n#from __future__ import print_function\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K\nfrom keras.callbacks import TensorBoard\nimport numpy as np\nbatch_size = 128\nnum_classes = 10\nepochs = 1","d21c634b":"from sklearn.metrics import accuracy_score #confusion_mattix\nprint(accuracy_score(y_pred,y_test))\nprint(confusion_matrix(y_pred,y_test))","0414f5c0":"from xgboost import XGBClassifire\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline\nimport matplotlib.pyplot as plt\nmodel=XGBClassifier()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\n\nprint(accuracy_score(y_pred,y_test))\nprint(confusion_matrix(y_pred,y_test))","f76ded64":"import matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sklearn.datasets import make_classification\nfrom imblearn.datasets import make_imbalance\n\n# for reproducibility purposes\nseed = 100\n\n# create balanced dataset\nX1, Y1 = make_classification(n_samples=700, n_features=2, n_redundant=0,\n                            n_informative=2, n_clusters_per_class=1,\n                            class_sep=1.0, flip_y=0.06, random_state=seed)\n\nplt.title('Balanced dataset')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n           s=25, edgecolor='k', cmap=plt.cm.coolwarm)\nplt.show()\n\n# concatenate the features and labels into one dataframe\ndf = pd.concat([pd.DataFrame(X1), pd.DataFrame(Y1)], axis=1)\ndf.columns = ['feature_1', 'feature_2', 'label']\n\n# save the dataset because we'll use it later\n#df.to_csv('df_base.csv', index=False, encoding='utf-8'","3526c200":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import SVC\n\n\ndef train_SVM(df):\n   # select the feature columns\n   X = df.loc[:, df.columns != 'label']\n   # select the label column\n   y = df.label\n\n   # train an SVM with linear kernel\n   clf = SVC(kernel='linear')\n   clf.fit(X, y)\n\n   return clf\n\n\ndef plot_svm_boundary(clf, df, title):\n   fig, ax = plt.subplots()\n   X0, X1 = df.iloc[:, 0], df.iloc[:, 1]\n\n   x_min, x_max = X0.min() - 1, X0.max() + 1\n   y_min, y_max = X1.min() - 1, X1.max() + 1\n   xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n\n   Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n   Z = Z.reshape(xx.shape)\n   out = ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n\n   ax.scatter(X0, X1, c=df.label, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n   ax.set_ylabel('y')\n   ax.set_xlabel('x')\n   ax.set_title(title)\n   plt.show()\n","aa1a2c98":">>> import numpy as np\n>>> a = np.zeros((156816, 36, 53806), dtype='uint8')\n>>> a.nbytes\n303755101056","f5244519":">>> 156816 * 36 * 53806 \/ 1024.0**3\n282.8939827680588","dbddffb2":"#mask = np.zeros(edges.shape)\n#mask = np.zeros(edges.shape,dtype='uint8')\n#df['label'] = df['label'].astype(np.uint8)\n#To fit and plot the model, do the following:\ndf=pd.read_csv(r'C:\\Users\\DELL\\Downloads\\archive (1)\\creditcard.csv')\n#df = pd.read_csv('creditcard.csv', encoding='utf-8', engine='python')\n#clf = train_SVM(df)\n#plot_svm_boundary(clf, df, 'Decision Boundary of SVM trained with a balanced dataset')","785ba5b6":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import SVC\n\n\ndef train_SVM(df):\n   # select the feature columns\n   X = df.loc[:, df.columns != 'label']\n   # select the label column\n   y = df.label\n\n   # train an SVM with linear kernel\n   clf = SVC(kernel='linear')\n   clf.fit(X, y)\n\n   return clf\n\n\ndef plot_svm_boundary(clf, df, title):\n   fig, ax = plt.subplots()\n   X0, X1 = df.iloc[:, 0], df.iloc[:, 1]\n\n   x_min, x_max = X0.min() - 1, X0.max() + 1\n   y_min, y_max = X1.min() - 1, X1.max() + 1\n   xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n\n   Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n   Z = Z.reshape(xx.shape)\n   out = ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n\n   ax.scatter(X0, X1, c=frame.label, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n   ax.set_ylabel('y')\n   ax.set_xlabel('x')\n   ax.set_title(title)\n   plt.show()","86c3f6c8":"df = pd.DataFrame({'Text': [u'Well I am', u\"Not my scene\", u\"Brutal\"], 'label': ['y', 'n', 'n']})","b76f4844":"import os \nos.system(\"pwd\") \nos.system(\"ls -a\") \nos.system(\"cd \/Desktop\") ","aaf2b9f3":"import sys\nimport pandas as pd\nimport scipy\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nimport sklearn\nimport matplotlib.pyplot as plt\npwd\n#df = pd.read_csv(r'C:\\Users\\DELL\\Downloads\\archive(1)\\creditcard.csv')","83964a28":"#df=pd.read_csv(r'C:\\Users\\DELL\\Downloads\\archive (1)\\creditcard.csv')\n#To fit and plot the model\nimport pandas as pd\ndf=pd.read_csv(r'C:\\Users\\DELL\\Downloads\\archive (1)\\creditcard.csv')#, encoding='utf-8', engine='python')\nclf = train_SVM(df)\nplot_svm_boundary(clf, df, 'Decision Boundary of SVM trained with a balanced dataset')","14083d34":"def makeOverSamplesSMOTE(X,y):\n #input DataFrame\n #X \u2192Independent Variable in DataFrame\\\n #y \u2192dependent Variable in Pandas DataFrame format\n from imblearn.over_sampling import SMOTE\n sm = SMOTE()\n X, y = sm.fit_sample(X, y)\n return X,y","9791ce4f":"def makeOverSamplesADASYN(X,y):\n #input DataFrame\n #X \u2192Independent Variable in DataFrame\\\n #y \u2192dependent Variable in Pandas DataFrame format\n from imblearn.over_sampling import ADASYN \n sm = ADASYN()\n X, y = sm.fit_sample(X, y)\n return(X,y)","03eb70ea":"trace=go.Bar(\n    x=df['Class'],y=df['values'],\n    name=\"(creditcard Fraud class_data unbalance(not fraud=0,Fraud=1)\",\n    marker=dict(color=\"Red\"),\n    text=df['values']\n)\ndata=[trace]","85e66e90":"#observe the different feature type present in the data\n\n","f0cb7a6f":"classes=df['Class'].value_counts()\nnormal_share=classes[0]\/df['Class'].count()*100\nfraud_share=classes[1]\/df['Class'].count()*100","2409b405":"fraud=df.loc[df['Class']==1]","5ab9a6cc":"normal=df.loc[df['Class']==0]","6770b476":"fraud","450f08cd":"fraud.count()","5cd2fdf2":"fraud.sum","33ab3e3d":"len(fraud)","8e495157":"len(normal)","68235a91":"df=pd.DataFrame({'test':range(9),'test2':range(9)})\nsns.lineplot(x=df.index, y='test', data=df)","fd7ea5bf":"sns.lineplot(x='index', y='test', data=pd.DataFrame({'test':range(9),'test2':range(9)}).reset_index())","de355ff5":"#df = df.reset_index()","0f76099e":"#sns.relplot(x='Amount',y='Time',hue=\"Class\",data=df)","f5cbfa9d":"#sns.catplot(x='Amount',y=\"Time\",hue=\"Class\",data=df)","7a744561":"from sklearn import linear_model\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split","b07bb310":"x=df.iloc[:,:-1]\nx=df.iloc[:,:-1]\ny=df[\"Class\"]","a1ee14fb":"# Create a bar plot for the number and percentage of fraudulent vs non-fraudulent transcations\n\n","da171baa":"# Create a scatter plot to observe the distribution of classes with time\n","b043aac7":"# Create a scatter plot to observe the distribution of classes with Amount","c612c450":"# Drop unnecessary columns","e007b514":"y= #class variable","85fbecb1":"from sklearn import model_selection\n\nX_train, X_test, y_train, y_test = ","ae88568b":"print(np.sum(y))\nprint(np.sum(y_train))\nprint(np.sum(y_test))","b7bd1840":"# plot the histogram of a variable from the dataset to see the skewness","1430091d":"# - Apply : preprocessing.PowerTransformer(copy=False) to fit & transform the train & test data","8b3d1a2c":"# plot the histogram of a variable from the dataset again to see the result ","94c49d7c":"# Logistic Regression\nfrom sklearn import linear_model #import the package\n\nnum_C = ______  #--> list of values\ncv_num =   #--> list of values","1872a08c":"clf = ___  #initialise the model with optimum hyperparameters\nclf.fit(X_train, y_train)\nprint --> #print the evaluation score on the X_test by choosing the best evaluation metric","d69c4aab":"var_imp = []\nfor i in clf.feature_importances_:\n    var_imp.append(i)\nprint('Top var =', var_imp.index(np.sort(clf.feature_importances_)[-1])+1)\nprint('2nd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-2])+1)\nprint('3rd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-3])+1)\n\n# Variable on Index-16 and Index-13 seems to be the top 2 variables\ntop_var_index = var_imp.index(np.sort(clf.feature_importances_)[-1])\nsecond_top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-2])\n\nX_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\nX_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n\nnp.random.shuffle(X_train_0)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [20, 20]\n\nplt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\nplt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n            label='Actual Class-0 Examples')\nplt.legend()","ae685d31":"# Logistic Regression\nfrom sklearn import linear_model #import the package\n\nnum_C = ______  #--> list of values\ncv_num =   #--> list of values","bb9610db":"from imblearn import over_sampling #- import the packages\n\n#perform cross validation & then balance classes on X_train_cv & y_train_cv using Random Oversampling\n\n#perform hyperparameter tuning\n\n#print the evaluation result by choosing a evaluation metric\n\n#print the optimum value of hyperparameters\n","4810abc0":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nsm = over_sampling.SMOTE(random_state=0)\nX_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\n# Artificial minority samples and corresponding minority labels from SMOTE are appended\n# below X_train and y_train respectively\n# So to exclusively get the artificial minority samples from SMOTE, we do\nX_train_smote_1 = X_train_smote[X_train.shape[0]:]\n\nX_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\nX_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n\n\nplt.rcParams['figure.figsize'] = [20, 20]\nfig = plt.figure()\n\nplt.subplot(3, 1, 1)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.legend()\n\nplt.subplot(3, 1, 2)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.scatter(X_train_smote_1[:X_train_1.shape[0], 0], X_train_smote_1[:X_train_1.shape[0], 1],\n            label='Artificial SMOTE Class-1 Examples')\nplt.legend()\n\nplt.subplot(3, 1, 3)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')\nplt.legend()","d8ef09eb":"#perform cross validation & then balance classes on X_train_cv & y_train_cv using SMOTE\n\n#perform hyperparameter tuning\n\n#print the evaluation result by choosing a evaluation metric\n\n#print the optimum value of hyperparameters\n","12d5bd70":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom imblearn import over_sampling\n\nada = over_sampling.ADASYN(random_state=0)\nX_train_adasyn, y_train_adasyn = ada.fit_resample(X_train, y_train)\n# Artificial minority samples and corresponding minority labels from ADASYN are appended\n# below X_train and y_train respectively\n# So to exclusively get the artificial minority samples from ADASYN, we do\nX_train_adasyn_1 = X_train_adasyn[X_train.shape[0]:]\n\nX_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\nX_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [20, 20]\nfig = plt.figure()\n\nplt.subplot(3, 1, 1)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.legend()\n\nplt.subplot(3, 1, 2)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.scatter(X_train_adasyn_1[:X_train_1.shape[0], 0], X_train_adasyn_1[:X_train_1.shape[0], 1],\n            label='Artificial ADASYN Class-1 Examples')\nplt.legend()\n\nplt.subplot(3, 1, 3)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')\nplt.legend()","3e773460":"#perform cross validation & then balance classes on X_train_cv & y_train_cv using ADASYN\n\n#perform hyperparameter tuning\n\n#print the evaluation result by choosing a evaluation metric\n\n#print the optimum value of hyperparameters\n","94f7ef64":"# perform the best oversampling method on X_train & y_train\n\nclf = ___  #initialise the model with optimum hyperparameters\nclf.fit( ) # fit on the balanced dataset\nprint() --> #print the evaluation score on the X_test by choosing the best evaluation metric","738972a8":"var_imp = []\nfor i in clf.feature_importances_:\n    var_imp.append(i)\nprint('Top var =', var_imp.index(np.sort(clf.feature_importances_)[-1])+1)\nprint('2nd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-2])+1)\nprint('3rd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-3])+1)\n\n# Variable on Index-13 and Index-9 seems to be the top 2 variables\ntop_var_index = var_imp.index(np.sort(clf.feature_importances_)[-1])\nsecond_top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-2])\n\nX_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\nX_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n\nnp.random.shuffle(X_train_0)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [20, 20]\n\nplt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\nplt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n            label='Actual Class-0 Examples')\nplt.legend()","49f68cd5":"#### Print the FPR,TPR & select the best threshold from the roc curve","fb7453ad":"#print('Train auc =', metrics.roc_auc_score(_________)\n#fpr, tpr, thresholds = metrics.roc_curve(_________)\n#threshold = thresholds[np.argmax(tpr-fpr)]\n#print(threshold)","706a48b5":"Overview\nGet familiar with class imbalance\nUnderstand various techniques to treat imbalanced classes such as-\nRandom under-sampling\nRandom over-sampling\n\nIntroduction\nWhen observation in one class is higher than the observation in other classes then there exists a class imbalance. Example: To detect fraudulent credit card transactions. As you can see in the below graph fraudulent transaction is around 400 when compared with non-fraudulent transaction around 90000.\n\nClass Imbalance is a common problem in machine learning, \nEspecially in classification problems. Imbalance data can hamper our model accuracy big time.\n\nClass Imbalance appear in many domains, including:\n\nFraud detection\nSpam filtering\nDisease screening\nSaaS subscription churn\nAdvertising click-throughs","5517889f":"Mean of Genuine and Fraud transactions:\n----------------------------------------------------------------------------------------\nmean of transaction amount of genuine transaction: 88.29102242225574\nmean of transaction amount of fraud transaction: 122.21132113821133\n---------------------------------------------------------------------------------------\nStandard Deviation of Genuine and Fraud transactions:\n---------------------------------------------------------------------------------------\nstandard deviation of transaction amount of genuine transaction: 250.1046523874637\nstandard deviation of transaction amount of fraud transaction: 256.42229861324483","b5aa4203":"pdf= [9.98553406e-01 1.26401388e-03 1.26401388e-04 3.51114966e-05\n 7.02229931e-06 3.51114966e-06 0.00000000e+00 7.02229931e-06\n 0.00000000e+00 3.51114966e-06]\n\n=\ncounts= [3.88675874e-04 4.92003427e-07 4.92003427e-08 1.36667619e-08\n 2.73335237e-09 1.36667619e-09 0.00000000e+00 2.73335237e-09\n 0.00000000e+00 1.36667619e-09]\n\n\nBin_edges= [    0.     2569.116  5138.232  7707.348 10276.464 12845.58  15414.696\n 17983.812 20552.928 23122.044 25691.16 ]","145411da":"K-Means\nConfusion Matrix\ntn = 28433 fp = 3\nfn = 14 tp = 31\nScores\nAccuracy --> 0.7973034654682069\nPrecison --> 0.004155124653739612\nRecall --> 0.5333333333333333\nF1 --> 0.008246005840920806","d39e9df8":"F1 SCORE IS::\n------------------------------------------------------------------------\nF1 score of the Decision Tree model is      :: 0.8105263157894738\n------------------------------------------------------------------------\nF1 score of the KNN model is                :: 0.7865168539325842\n------------------------------------------------------------------------\nF1 score of the Logistic Regression model is:: 0.7167630057803468\n------------------------------------------------------------------------\nF1 score of the SVM model is                :: 0.5\n------------------------------------------------------------------------\nF1 score of the Random Forest Tree model is :: 0.7657142857142858\n------------------------------------------------------------------------\n------------------------------------------------------------------------","7c733ae3":"ACCURACY SCORE\n------------------------------------------------------------------------\nAccuracy score of the Decision Tree model is 0.9993679997191109\n------------------------------------------------------------------------\nAccuracy score of the KNN model is 0.9993328885923949\n------------------------------------------------------------------------\nAccuracy score of the Logistic Regression model is 0.9991397773954567\n------------------------------------------------------------------------\nAccuracy score of the SVM model is 0.998735999438222\n------------------------------------------------------------------------\nAccuracy score of the Random Forest Tree model is 0.9992802219023208\n------------------------------------------------------------------------\n-----------------------------------------------------------","126034a5":"K-Means\nConfusion Matrix\ntn = 22684 fp = 5752\nfn = 21 tp = 24\nScores\nAccuracy --> 0.7973034654682069\nPrecison --> 0.004155124653739612\nRecall --> 0.5333333333333333\nF1 --> 0.008246005840920806","ecb0a529":"K-NEAREST NEIGHBOURS\nCONFUSION MATRIX\n............................................\ntn = 28431 fp = 4\nfn = 12 tp = 34\nScores\nAccuracy -----> 0.9994382219725431\nPrecison -----> 0.8947368421052632\nRecall -------> 0.7391304347826086\nF1 -----------> 0.8095238095238095\n","e4ebe715":"K-Nearest Neighbours\nConfusion Matrix\ntn = 28433 fp = 3\nfn = 14 tp = 31\nScores\nAccuracy --> 0.999403110845827\nPrecison --> 0.9117647058823529\nRecall --> 0.6888888888888889\nF1 --> 0.7848101265822784","fba2fc32":"Logistic Regression:\n\nF1 Score: 0.9364355202726674\nROC-AUC: 0.9708097762692439\n\nSupport Vector Machine:\n\nF1 Score: 0.9423767223844859\nROC-AUC: 0.9380699339730396\n\nRandom Forest:\n\nF1 Score: 0.9383577477629312\nROC-AUC : 0.9614369696489062   ","a42f6cd3":"Random Forest: 25\n0.9995594403129736\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     56656\n           1       0.99      0.73      0.84        90\n\n    accuracy                           1.00     56746\n   macro avg       0.99      0.87      0.92     56746\nweighted avg       1.00      1.00      1.00     56746","f6319018":"Logistic Regression:\n\nF1 Score: 0.9441241497856085\nROC-AUC: 0.9708256189013019\n\n\nSupport Vector Machine:\n\nF1 Score: 0.9522210641754583\nROC-AUC: 0.962450040836492\n\n\nRandom Forest:\n\nF1 Score: 0.9407970903671641\nROC-AUC: 0.9664211381559813\n","51d2002b":"Logistic Regression:\n\nF1 Score: 0.9380792537474003\nROC-AUC: 0.9791901372509362\n\n\nSupport Vector Machine:\n\nF1 Score: 0.9408339409992899\nROC-AUC: 0.9734510869702593\n\n\nRandom Forest:\n\nF1 Score: 0.9357422780139621\nROC-AUC: 0.9689659005682547\n\n\nSVM:ROC AUC Score: 0.9783260964299432","d773ca08":"roc_auc_score: 0.9783260964299432\nClassification report:\n               precision    recall  f1-score   support\n\n         0.0       1.00      1.00      1.00     85296\n         1.0       0.86      0.81      0.83       147\n\n    accuracy                           1.00     85443\n   macro avg       0.93      0.90      0.92     85443\nweighted avg       1.00      1.00      1.00     85443\n\nConfusion_matrix:\n [[85276    20]\n [   28   119]]","a72eae5c":"precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     84976\n           1       0.74      0.70      0.72       142\n\n    accuracy                           1.00     85118\n   macro avg       0.87      0.85      0.86     85118\nweighted avg       1.00      1.00      1.00     85118\n","0e9cf3b4":"The acuuracy of the model is = TP+TN\/(TP+TN+FP+FN) = 0.9990836250851759 \n The Missclassification is = 1-Accuracy =                   0.0009163749148241029 \n Sensitivity or True Positive Rate is = TP\/(TP+FN) =        0.704225352112676 \n Specificity or True Negative Rate is  = TN\/(TN+FP) =        0.9995763509696856 \n Positive Predictive value is = TP\/(TP+FP) =                0.7352941176470589 \n Negative predictive Value is   = TN\/(TN+FN) =                0.9995057776940999 \n Positive Likelihood Ratio is = Sensitivity\/(1-Specificity)= 1662.2848200312915 \n Negative likelihood Ratio is = (1-Sensitivity)\/Specificity= 0.29590000563778246","25096cf3":"The model used is Random Forest classifier\nThe accuracy is  0.9995594403129736\nThe precision is 0.9850746268656716\nThe recall is 0.7333333333333333\nThe F1-Score is 0.8407643312101911\nThe Matthews correlation coefficient is 0.8497412309528187","4feffa03":"the Model used is Isolation Forest\nThe accuracy is  0.9975504881401333\nThe precision is 0.24742268041237114\nThe recall is 0.26666666666666666\nThe F1-Score is 0.25668449197860965\nThe Matthews correlation coefficient is0.25563919667071605","0f0d1d4d":"BY USING SMOTE(SYNTHETIC MINORITY OVERSAMPLING TEchnique) Smote: \n\nRF - Accuracy -  0.9994967405170698\nRF:\n [[85281    14]\n [   29   119]]\nRF - AUC:  0.9720003944973168\nRandom Forest:\n Precision:  [0.99966006 0.89473684] Recall:  [0.99983586 0.80405405]","6decd944":"BY USING ADASYN(ADAPTIVE SYNTHETIC)TECHNIQUE:\n\nRF - Accuracy -  0.9994967405170698\nRF:\n [[85281    14]\n [   29   119]]\nRF - AUC:  0.9739444820281914\nRandom Forest:\n Precision:  [0.99966006 0.89473684] Recall:  [0.99983586 0.80405405]","05ffc91d":"THE BEST TECHNIQUE SPECIALLY FOR THE HIGHLY IMBALANCED DATASET IS \"ADASYN\" AND SMOTE TECHNIQUE\nWITH 0.999 PRECISION AND RECALL IS 0.999","f0b6cd31":"import numpy as np\n\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff","c0553995":"#fig = go.FigureWidget(ff.create_distplot(hist_data, group_labels, show_hist=True, colors=colors))\n#fig.layout.update(title='Density curve')\n#fig","e04bd132":"import numpy as np\nimport pandas as pd\n#import plotly.express as px\n#import plotly.figure_factory as ff\n#from plotly.subplots import make_subplots\n#import plotly.graph_objects as go\n","7db7e4bf":"#import plotly.figure_factory as ff","e4336463":"class_0 = df.loc[df['Class'] == 0][\"Time\"]\nclass_1 = df.loc[df['Class'] == 1][\"Time\"]\n\nhist_data = [class_0, class_1]\ngroup_labels = ['Not Fraud', 'Fraud']\n\n#fig = ff.create_distplot(hist_data, group_labels, show_hist=False, show_rug=False)\nfig = ff.create_distplot(hist_data, group_labels, show_hist=True, colors=colors)\nfig['layout'].update(title='Credit Card Transactions Time Density Plot', xaxis=dict(title='Time [s]'))\niplot(fig, filename='dist_only')","f5bdf2e0":"#feature density plot\nvar = df.columns.values\n\ni = 0\nt0 = df.loc[df['Class'] == 0]\nt1 = df.loc[df['Class'] == 1]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(8,4,figsize=(16,28))\n\nfor feature in var:\n    i += 1\n    plt.subplot(8,4,i)\n    sns.kdeplot(t0[feature], bw=0.5,label=\"Class = 0\")\n    sns.kdeplot(t1[feature], bw=0.5,label=\"Class = 1\")\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","27ec155a":"While seeing the statistics, it is seen that the values in the \u2018Amount\u2019 \nvariable are varying enormously when compared to the rest of the variables. \nTo reduce its wide range of values, we can normalize it using the \u2018StandardScaler\u2019 method in python.\n","24ea4569":"COMPLETED\n### Similarly explore other algorithms by building models like:\n- KNN\n- SVM\n- Decision Tree\n- Random Forest\n- XGBoost","467f32b7":"This version will pick balanced clusters of points that are closest to outlier points from other classes, instead of building giant blob(s) around one or a handful of such points.\n\nIt's mildly interesting that these algorithms exist; \nperhaps they could be adapted for some kind of data visualization exploring where the difficult points are?","bbeeb3fd":"We can see that out of 284,807 samples, there are only 492 fraud cases which is only 0.17 percent of the total samples. So, we can say that the data we are dealing with is highly imbalanced data and needs to be handled carefully when modeling and evaluating.\nNext, we are going to get a statistical view of both fraud and non-fraud transaction amount data using the \u2018describe\u2019 method in python.","bd5b8522":"STUDENT'S NAME                                :YADU SINGH\nAUTHORED AND PROJECT SUBMITTED BY             :YADU SINGH\nBATCH                                         :COHORT-8 MLAI\nPROGRAM ENROLLED                              :Masters in Machine Learning and AI jan 2021 Cohort\nUNIVERSITY ENROLLED FOR                       :Liverpool John Moore University,United Kingdom,Europe.\nRESEARCH TITLE                                :\"CREDIT CARD FRAUD DETECTION\"\nDATASET                                       :archive(1)\\creditcard.csv\nNO OF RECORDS\/ROWS\/TRANSACTIONS               :284807\nNO OF GENUINE TRANSACTIONS                    :284315\nNO OF FRAUD TRANSACTIONS                      :492\nNO OF COLUMNS\/VARIABLES\/FEATURES              :31\nNO OF TARGET VARIABLES                        :1\nACCURACY WITHOUT BALANCED DATA                :96%\nACCURACY WITH BALANCED DATA                   :99.66%\nOVERALL ACCURACY SCORE :\nOVERALL F1 SCORE:\nPRECISION:\nRECALL:\nMCC\nAUC-ROC\nQUANTUM OF FRAUD AMOUNT TRANSACTION TAKEN PLACE IN THE RANGE OF LESS THAN RS\/-<2500 \n    ","e2d97526":"### Hyperparameter Tuning:\n\nWhen the data is imbalanced or less, it is better to use K-Fold Cross Validation for evaluating the performance when the data set is randomly split into \u2018k\u2019 groups.\nStratified K-Fold Cross Validation is an extension of K-Fold cross-validation, in which we rearrange the data to ensure that each fold is a good representative of all the strata of the data.\nWhen you have a small data set, the computation time will be manageable to test out different hyperparameter combinations. In this scenario, it is advised to use a grid search.\nBut, with large data sets, it is advised to use a randomized search because the sampling will be random and not uniform. ","74089141":"### Plotting the distribution of a variable","fb947062":"## Exploratory data analytics (EDA): \nNormally, in this step, you need to perform univariate and bivariate analyses of the data, followed by feature transformations, if necessary. For the current data set, because Gaussian variables are used, you do not need to perform Z-scaling. However, you can check if there is any skewness in the data and try to mitigate it, as it might cause problems during the model-building phase.\n\nCan you think why skewness can be an issue while modelling? Well, some of the data points in a skewed distribution towards the tail may act as outliers for the machine learning models which are sensitive to outliers and hence that may cause a problem. Also, if the values of any independent feature are skewed, depending on the model, skewness may affect model assumptions or may impair the interpretation of feature importance. \n \n\nTrain\/Test Split: \nNow you are familiar with the train\/test split, which you can perform in order to check the performance of your models with unseen data. Here, for validation, you can use the k-fold cross-validation method. You need to choose an appropriate k value so that the minority class is correctly represented in the test folds.\n \nModel-Building\/Hyperparameter Tuning: \nThis is the final step at which you can try different models and fine-tune their hyperparameters until you get the desired level of performance.","ab1e56c8":"#### oversampling and balancing dataset with ADASYN","a4a2b3c5":"The money amount for fraud transaction are not high as normal transaction.","7183e01a":"Because borderline2 allows extending to any point class-wise, it will retain a bit of the stringiness of the regular SMOTE, which may make the result less separable. Otherwise the tradeoffs are the same.\n\nThe last option is kind='svm'. imlearn documentation is very vague as to how this works, simply stating that it \"uses an SVM classifier to find support vectors and generate samples considering them\":","376d079b":"can refer Good site kaggle https:\/\/www.kaggle.com\/faressayah\/credit-card-fraud-detection-anns-vs-xgboost","ba7ed630":"Run this command on the terminal\npip install --upgrade --no-deps statsmodels","cedd4d60":"ADASYN uses (1) the kind='normal' SMOTE algorithm (2) on point not in homogenous neighborhoods. The result is a kind of hybrid between regular SMOTE and borderline1 SMOTE. This technique inherits the primary weakness of SMOTE, e.g. its ability to create innerpoint-outerpoint bridges. Whether or not the heavy focus on the outlier points is a good thing or not is application dependent, but overall ADASYN feels like a very heavy transformation algorithm, and e.g. one requiring that the underlying point cluster be sufficiently large, as imblearn doesn't provide any modifications to this algorithm for modulating its tendency to create them (as it does for SMOTE).","da6f742e":"Highest correlations come from:\n#Time & V3 (-0.42)\n- Amount & V2 (-0.53)\n- Amount & V4 (0.4)\n\nWhile these correlations are high, I don't expect it to run the risk of multicollinearity.\n\nThe correlation matrix shows also that none of the V1 to V28 PCA components have any correlation to each other however if we observe Class has some form positive and negative correlations with the V components but has no correlation with Time and Amount.","01bddeba":"### Print the important features of the best model to understand the dataset\n- This will not give much explanation on the already transformed dataset\n- But it will help us in understanding if the dataset is not PCA transformed","ad735ab7":"### Various useful Methods\n\ndecision_function(X)         ::Apply transforms, and decision_function of the final estimator\nfit(X[, y])                  ::Fit the model\nfit_predict(X[, y])          ::Applies fit_predict of last step in pipeline after transforms.\nfit_transform(X[, y])        ::Fit the model and transform with the final estimator\nget_params([deep])           ::Get parameters for this estimator.\npredict(X, **predict_params) ::Apply transforms to the data, and predict with the final estimator\npredict_log_proba(X)         ::Apply transforms, and predict_log_proba of the final estimator\npredict_proba(X)             ::Apply transforms, and predict_proba of the final estimator\nscore(X[, y, sample_weight]) ::Apply transforms, and score with the final estimator\nscore_samples(X)             ::Apply transforms, and score_samples of the final estimator.\nset_params(**kwargs)         ::Set the parameters of this estimator.","4ee1a615":"Compare the Amount of transactions in two separate datasets\u00b6\nSee if we can flag fraud cases by transaction amount","532d053b":"Conclusion As we can observe above, as the ratio of fraud cases decreases the ROC-AUC and F1 score of Logistic Regression and Support Vector Machine show a downward trend whereas those of Random Forest show better performance. The F-score of Random Forest is better than the other two with decreasing ratio of fraud cases. This is an important performance indicator as it shows that the system is correctly classifying frauds as well as minimising errors in incorrect classification, both of which are extremely relevant to the real world scenario. The performance of Random Forest can be further explored if the features are not anonymised and with further tuning of the hyperparameters. The tuning here is done by Grid Search whereas in further study, tuning can be improved by using Random Search followed by Grid Search to obtain better hyperparameters.","9e50a5a6":"From the above boxplot image clearly shows that,\nThe Genuine and traud transactions have taken place throughout the time line,\nAnd hence there is no distinction between them.","632fba7e":"###### Featuring the density CURVE in the following code.in order to show featurure wise depiction of getting \ncomplete insights about all the variable from v1 to v28","f99491c6":"## Model Building\n- Build different models on the balanced dataset and see the result","07729218":"### RESEARCH PROJECT TITLE:CREDIT CARD FRAUD DECEPTION","324d3d51":"#Here the complete Summary of dataset:\nThe mean transaction amout among fraud cases is, and among non-fraud cases. \nAnd the difference is statistically significant.\n\nThe Transaction Amount Visualization in the above figure on statistical approach:\nExpect a lot of low-value transactions to be uninteresting (buying cups of tea etc)\nOnly visualizes the transactions below around Rs2500.00","277fdd56":"ClusterCentroids comes with the implicit expectation that the points being sampled belong to moderately well-defined clusters. In other words, this algorithm will not capture outlier points that are not sufficiently dense, as these points will be pulled back towards the inner points by the cluster mean operation. As a result the clusters generated by ClusterCentroids will thus be marginally tighter and better-defined than the ones generated by RandomUnderSampler. That is not to say that this algorithm will completely eliminate outliers; we can see in the plot above that there are outlier points from the white and red classes represented in the blue cluster.\n\nOverall I find the idea behind ClusterCentroids to be quite appealing.","7ddd1f2d":"#The ratio of fraudulent transactions is very low. This is a case of class imbalance problem, \nand you're going to learn how to deal with this in the next exercises.","21a9371d":"#### Building model on dataset balanced using ADASYN.","d470c4ce":"###### The following results are displayed as fraudulent transactions","9cc78060":"###### The completeGenuine Time of Transaction's descriptive statistics results are getting displayed","b0884f05":"###### These above features or variables from V! to V28 are transformed into PCA form,except the features \"Time\",'Amount\" and \"Class\"","4e4dd1ec":"With this rule, 22 out of 50 fraud cases are detected, 28 are not detected, and 16 false positives are identified.","e16caa77":"Random Undersampling The present dataset is highly imbalanced with the fraud cases forming an extremely small percentage of the whole. This poses a problem in training the algorithms. To deal with this problem, the technique of random undersampling - removal of samples from majority class, is used.","3f15f27a":"### Problem Statement\nThe problem statement chosen for this project is to predict fraudulent credit card transactions with the help of machine learning models.\n\nIn this project, you will analyse customer-level data which has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group. \n\nThe dataset is taken from the Kaggle website and it has a total of 2,84,807 transactions, out of which 492 are fraudulent. Since the dataset is highly imbalanced, so it needs to be handled before model building.\n\nBusiness Problem Overview\nFor many banks, retaining high profitable customers is the number one business goal. Banking fraud, however, poses a significant threat to this goal for different banks. In terms of substantial financial losses, trust and credibility, this is a concerning issue to both banks and customers alike.\n\nIt has been estimated by Nilson report that by 2020 the banking frauds would account to $30 billion worldwide. With the rise in digital payment channels, the number of fraudulent transactions is also increasing with new and different ways. \n\n In the banking industry, credit card fraud detection using machine learning is not just a trend but a necessity for them to put proactive monitoring and fraud prevention mechanisms in place. Machine learning is helping these institutions to reduce time-consuming manual reviews, costly chargebacks and fees, and denials of legitimate transactions.\n\nUnderstanding and Defining Fraud\nCredit card fraud is any dishonest act and behaviour to obtain information without the proper authorization from the account holder for financial gain. Among different ways of frauds, skimming is the most common one, which is the way of duplicating of information located on the magnetic stripe of the card.  Apart from this, following are other ways:\n\nManipulation\/alteration of genuine cards\nCreation of counterfeit cards\nStolen\/lost credit cards\nFraudulent telemarketing\n \nData Dictionary\nThe dataset can be download using this link.\n\nThe data set includes credit card transactions made by European cardholders over a period of two days in September 2013. Out of a total of 2,84,807 transactions, 492 were fraudulent. This data set is highly unbalanced, with the positive class (frauds) accounting for 0.172% of the total transactions. The data set has also been modified with Principal Component Analysis (PCA) to maintain confidentiality. Apart from \u2018time\u2019 and \u2018amount\u2019, all the other features (V1, V2, V3, up to V28) are the principal components obtained using PCA. The feature 'time' contains the seconds elapsed between the first transaction in the data set and the subsequent transactions. The feature 'amount' is the transaction amount. The feature 'class' represents class labelling, and it takes the value 1 in cases of fraud and 0 in others.\n\nProject Pipeline\nThe project pipeline can be briefly summarized in the following four steps:\n\nData Understanding: Here, you need to load the data and understand the features present in it. This would help you choose the features that you will need for your final model.\n\nExploratory data analytics (EDA): Normally, in this step, you need to perform univariate and bivariate analyses of the data, followed by feature transformations, if necessary. For the current data set, because Gaussian variables are used, you do not need to perform Z-scaling. However, you can check if there is any skewness in the data and try to mitigate it, as it might cause problems during the model-building phase.\n\nTrain\/Test Split: Now you are familiar with the train\/test split, which you can perform in order to check the performance of your models with unseen data. Here, for validation, you can use the k-fold cross-validation method. You need to choose an appropriate k value so that the minority class is correctly represented in the test folds.\n\nModel-Building\/Hyperparameter Tuning: This is the final step at which you can try different models and fine-tune their hyperparameters until you get the desired level of performance on the given dataset. You should try and see if you get a better model by the various sampling techniques.\n\nModel Evaluation: Evaluate the models using appropriate evaluation metrics. Note that since the data is imbalanced it is more important to identify which are fraudulent transactions accurately than the non-fraudulent. Choose an appropriate evaluation metric which reflects this business goal","7c51f321":"### Getting of printed Quantiles of Genuine and Fraud transactions:","81d67e1f":"### Splitting the data into train & test data","8ab4bba9":"#lets draw histogram graph","1adac7f6":"### Similarly explore other algorithms on balanced dataset by building models like:\n- KNN-COMPLETED\n- SVM-COMPLETED\n- Decision Tree-COMPLETED\n- Random Forest-COMPLETED\n- XGBoost-SYSTEM PROB BUT CODE SUBMITTED","eb95721f":"#### Building model on dataset balanced using SMOTE.","fac12791":"This is conceptually a very simple algorithm; to quote something a graph theory professor I once had liked to say, you could probably explain how it works to your grandmother.\n\nLet's see what this looks like in practice. Recall the following synthetic sample dataset from the previous notebook:","863e82f3":"Predicted probabilities of 0 is  for (No Fraud) \nAnd 1 is for ( Fraud) for the test data with a default classification threshold of 0.5\n\nLower the threshold\nSince the model is predicting Fraud too many type II errors is not advisable. \nFalse Negative ( ignoring the probability of Fraud when there actualy is one) is more dangerous \nThan a False Positive in this case. \nHence inorder to increase the sensitivity, threshold can be lowered here.","94acda9b":"\n#### Introduction\n\nJust to recap, sampling is a general-purpose preprocessing technique for turning imbalanced datasets into balanced ones, and thus, imbalanced machine learning models into balanced ones. \n\nThere are other approaches to achieving this end, like changing the algorithm cost function, but sampling is a general-purpose technique that works reasonably well everywhere. \n\nThe simplest, most explainable, and oftentime most effective techniques for performing sampling are \n1)Random under- sampling. \n2)Random over- sampling. \nThese two ssampling methodologies have broadly simpler characteristics. \n\nMore complex adaptations of over-sampling exist in the form of the SMOTE and ADASYN algorithms and variants. \nFor some datasets, these approaches may have better performance.\n\nThere are advanced under-sampling techniques as well. \nimblearn divides these into two broad classes. \n\nSampling the points randomly is just the most naive way of achieving this goal. \nA competing idea is prototype generation. \nSamplers which are prototype generation algorithms will not select exact points out of the dataset, \nbut will instead generate new points somehow representative of existing ones.","b442980f":"### Print the class distribution after applying SMOTE ","f6dc8d41":"### Select the oversampling method which shows the best result on a model\n- Apply the best hyperparameter on the model\n- Predict on the test dataset","9856f9b6":"## Model building with balancing Classes\n\n##### Perform class balancing with :\n- Random Oversampling\n- SMOTE\n- ADASYN","3fcb829a":"### STATISTICAL ANALYSIS","dae3f28f":"##### Build models on other algorithms to see the better performing on ADASYN","8e85f729":"This tells us that by doing nothing, we would be correct in 95.9% of the cases. So now you understand, that if we get an accuracy of less than this number, our model does not actually add any value in predicting how many cases are correct. Let's see how a random forest does in predicting fraud in our data.","301fc901":"### EXPLORATORY DATA ANALYSIS","39e738dd":"MY RESEARCH TOPIC::","072ae7ec":"#### Statistical descriptions of the dataset\nLets understand complete dataset,its behavior,insights etc.","aedc8542":"See the above image so it finds the 5 nearest neighbors to the sample points. then draws a line to each of them. Then create samples on the lines with class == minority class.","fef5b579":"### RESULTS ON VARIOUS ALGORITHMIC APPROACH WITH AND WITHOUT CLASS IMBALANCE::","f1d39adb":"###### Complete understanding of the given data:","e6b2c05e":"### LOGISTIC REGRESSION","2fbce844":"### Model Selection & understanding:\n\nLogistic regression works best when the data is linearly separable and needs to be interpretable. \nKNN is also highly interpretable, but not preferred when we have a huge amount of data as it will consume a lot of computation.\nThe decision tree model is the first choice when we want the output to be intuitive, but they tend to overfit if left unchecked.\nKNN is a simple, supervised machine learning algorithm used for both classification and regression tasks. The k value in KNN should be an odd number because you have to take the majority vote from the nearest neighbours by breaking the ties. \nIn Gradient Boosted machines\/trees newly added trees are trained to reduce the errors (loss function) of earlier models.\nXGBoost is an extended version of gradient boosting, with additional features like regularization and parallel tree learning algorithm for finding the best split. ","b1d43a9a":"ADASYN:\nIts a improved version of Smote. What it does is same as SMOTE just with a minor improvement. After creating those sample it adds a random small values to the points thus making it more realistic. In other words instead of all the sample being linearly correlated to the parent they have a little more variance in them i.e they are bit scattered.","5ae5d20b":"#From the above image,\nThe Hour \"zero\" corresponds to the hour the first transaction happened and not necessarily 12-1am.\nGiven the huge decrease in normal transactions from hours 1 to 8 and \nAgain roughly at hours 24 to 32, it seems to say that,the fraud tends to occur at higher rates during the night.\nStatistical tests could be used to give evidence for this fact.","83e6759b":"def plot_roc_curve(y_true, y_score, size=None):\n    \"\"\"plot_roc_curve.\"\"\"\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(\n        y_true, y_score)\n    if size is not None:\n        plt.figure(figsize=(size, size))\n        plt.axis('equal')\n    plt.plot(false_positive_rate, true_positive_rate, lw=2, color='navy')\n    plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.ylim([-0.05, 1.05])\n    plt.xlim([-0.05, 1.05])\n    plt.grid()\n    plt.title('Receiver operating characteristic AUC={0:0.2f}'.format(\n        roc_auc_score(y_true, y_score))) \n    return false_positive_rate, true_positive_rate,auc_score","78595b82":"###### In the following we are getting complete features\/variables getting displayed","569a796b":"##### ALGORITHMS,TOOLS,VARIOUS PLOTS FOR EDA AND OTHERS:","1f184c1f":"Introduction and preparing the given dataset\nlets Learn about the typical challenges associated with fraud deception. Learn how to resample the data in a smart way, and tackle problems with imbalanced data.","76274c4e":"Why does this happen? This surprising new structure shows up prominently because in the \"line constructor\" nature of SMOTE, a few of the outlier points inside of the blue point cloud will match up sometimes with points in the main body of the class cluster. These will then get interpolated into long spaghetti lines.\n\nThis is an artifact of the SMOTE algorithm, and is problematic because it introduces a feature into the dataset, this \"point bridge\", which doesn't actually exist in the underlying dataset. We can verify this if we increase the incidence rate for this rare class by 10x in the underlying dataset:","a7e08306":"$ python3 -m pip install --user ipykernel\n\n# add the virtual environment to Jupyter\n$ python3 -m ipykernel install --user --name=venv\n\n# create the virtual env in the working directory\n$ python3 -m venv .\/venv\n\n# activate the venv\n$ source venv\/bin\/activate\n\n# install the package\n(venv) pip install imbalanced-learn\n\n# fire up the notebook\n(venv) jupyter notebook\n","dbb3ca63":"sns.relplot(x='Amount',y='Time',hue=\"Class\",df=df)","3db36cd1":"By seeing the distributions we can have an idea how skewed are these features, we can also see further distributions of the other features. There are techniques that can help the distributions be less skewed which will be implemented in this notebook in the future.\n\nDoesn't seem like the time of transaction really matters here as per above observation. Now let us take a sample of the dataset for out modelling and prediction","ceb246db":"## Data Understanding\nNow that you have a brief overview of the project, let us go ahead and analyze the data which we will be working on in this capstone.\n\nAs you saw, the data set includes credit card transactions made by European cardholders over a period of two days in September 2013. Out of a total of 2,84,807 transactions, 492 were fraudulent. This data set is highly unbalanced, with the positive class (frauds) accounting for just 0.172% of the total transactions. The data set has also been modified with Principal Component Analysis (PCA) to maintain confidentiality. Apart from \u2018time\u2019 and \u2018amount\u2019, all the other features (V1, V2, V3, up to V28) are the principal components obtained using PCA. The feature 'time' contains the seconds elapsed between the first transaction in the data set and the subsequent transactions. The feature 'amount' is the transaction amount. The feature 'class' represents class labelling, and it takes the value 1 in cases of fraud and 0 in others.Data Understanding\nNow that you have a brief overview of the project, let us go ahead and analyze the data which we will be working on in this capstone.\n","a1eea9af":"COMPLETED::\n#perform cross validation\n\n#perform hyperparameter tuning\n\n#print the evaluation result by choosing a evaluation metric\n\n#print the optimum value of hyperparameters","44e47190":"###### Performance without Balancing the dataset\/with original dataset","45860bbd":"###### Here we are getting displayed genuine count ","31e0e1be":"## BY YADU SINGH","09c0c72a":"### RANDOM FOREST","b3446310":"### SUPPORT VECTOR MACHINE-SVM","3b9f9550":"#### Proceed with the model which shows the best result \n- Apply the best hyperparameter on the model\n- Predict on the test dataset","7b89916b":"### CHECKING THE DUPLICTE VALUES","f9a6fa39":"### Class Imbalances:\n\nIn Undersampling, you select fewer data points from the majority class for your model-building process in order to balance both the classes.\n\nIn Oversampling, you assign weights to randomly chosen data points from the minority class. It is done so that the algorithm can focus on this class while optimising the loss function.\n\nSMOTE is a process where you can generate new data points, which lie vectorially between two data points that belong to the minority class.\n\nADASYN is similar to SMOTE, with a minor change i.e. the number of synthetic samples that it will add will have a density distribution. The aim here is to create synthetic data for minority examples that are harder to learn, rather than the easier ones. ","c0f4b0bf":"###### By oversampling techniques","acec9d43":"### lets understand the statistics of the data","196b6f79":"#### in the above shows that,There is no missing data in the dataset","6637caae":"## Introduction\nWelcome to the Research Thesis on Credit Card Fraud Detection.\n\n\nAlthough digital transactions in India registered a 51% growth in 2018-19, their safety remains a concern. Fraudulent activities have increased severalfold, with around 52,304 cases of credit\/debit card fraud reported in FY'19 alone. Due to this steep increase in banking frauds, it is the need of the hour to detect these fraudulent transactions in time in order to help consumers as well as banks, who are losing their credit worth each day. Machine learning can play a vital role in detecting fraudulent transactions.\n\n\nUntil now, you have learnt about the different types of machine learning models, but now you will learn which model to choose for your purpose and why. Understanding the models based on different scenarios is an important skill that a data scientist\/machine learning engineer should possess. In addition, tuning your model is equally important to get the best fit for your given data.\n \nBy the end of this module, you will be able to understand how you can build a machine learning model capable of detecting fraudulent transactions. You will also learn how to handle class imbalances present in any data set, along with model selection and hyperparameter tuning.\n \n\nIn this session\nYou will learn the following:\n\nUnderstand the problem statement\nLearn how and why to handle class imbalances and normalise the data set\nLearn about model selection and model interpretability \nGet an understanding of hyperparameter tuning\nLearn about performance metrics\n \n\nNOTE: The questions in this module are not graded. It is advised that you go through them for a better understanding.\n\n ","8e67178e":"From the above figure,we can clearly concllude that 99.827% of transactions are genuine,\nwhere as 0.172% of transactions are Fraud","e583653c":"Here we can clearly shows that the transaction amount is less than 2500,there only the fraud transaction has taken place","c83387d4":"### When we apply SMOTE to this we get:","b2ca92fe":"###### The complete Genuine Time Transactions descriptive statistics results are getting displayed","ac3ad139":"### Print the class distribution after applying ADASYN","c1e7d2c9":"Instructions\n\nDefine the plot_data(X, y) function, that will nicely plot the given feature set X with labels y in a scatter plot. This has been done for you.\nUse the function prep_data() on your dataset df to create feature set X and labels y.\nRun the function plot_data() on your newly obtained X and y to visualize your results.","53378c1e":"###### Here we are getting displayed fraudulent count as shown below","6909f2ed":"Here we will observe the distribution of our classes","c983f5a6":"### To find out the fraudulent and non fraudulent transactions","8ce76607":"###### The complete fraud Timing transactions descriptive statistics results are getting displayed","acd5a6f4":"###### YADU'S FINAL RESEARCH THESIS PROJECT ON CCFD USING MLAI::","f2551290":"Credit Card Fraud Detection  using balancing technique,\nIn this project we will predict fraudulent credit card transactions with the help of \nMachine learning models. ","a815d8e2":"Splitting the data to evaluate performance After removing extreme outliers, the dataset is now split into training set and testing set for further processes.","30281e4a":"The Summary of the above graph:\nIn case of long tail, the fraud transaction happened more often infact.\nIt seems to say that,perhaps its hard to differentiate fraud from normal transactions by transaction amount alone.\n\n","533cc988":"Module: \nResampling for Imbalanced Data\nThere are two types of resampling methods to deal with imbalanced data, \none is under sampling \nand another one is over sampling.\n\nUnder sampling: you take ramdom draws from non-fraud observations to match the amount of fraud observations. But you're randomly throwing away a lot of data and infromation. aka: Random Under Sampling\n\nOver sampling: \nyou take ramdom draws from frad cases and copy these observations to increase to amount of fraud samples in your data. But you are traning your model many many duplicates. aka: Random Over Sampling & SMOTE\n\nSynthetic Minority Oversampling Technique(SMOTE): Adjust the data imbalance by oversampling the monority observations(fraud cases) using nearest neighbors of fraud cases to create new synthetic fraud cases instead of just coping the monority samples.","8d948ec8":"###### Here we are findingout the sum of fraud transactions as follows","d589bb7a":"### Getting of printed medians of Genuine and Fraud transactions:","035833a9":"This tendancy of SMOTE to connect inliers and outliers is the algorithm's primary weakness in practice. It limits the algorithm's applicability to datasets with sufficiently few samples and\/or sufficiently sparse point clouds. When applying SMOTE to your own data, make sure to take a good hard look at whether or not it's doing what you expect it to be doing.\n\nimlearn includes several adaptations of the naive SMOTE algorithm which attempt to address this weakness in various ways. There are in total four \"modes\" in imlearn. The one demonstrated thus far, the classic SMOTE algorithm, corresponds with kind='regular'. The remaining three are adaptations.\n\nkind='borderline1' and kind='borderline2' are one class of adaptations. These will classify points are being noise (all nearest neighbors are of a different class), in danger (half or more nearest neighbors are a different class), or safe (all nearest neighbors are of the same class). Only points in danger will be sampled in step one of the algorithm. Then, on step two of the algorithm, instead of selecting a point from n_neighbors belonging to the same class, borderline1 will select a point from the five nearest points not belonging to the given point's class, while borderline2 will select a point from the five nearest points of any class.","4ad284ef":"SVM will again focus sampling on points near the boundaries of the clusters.\n\nOversampling with ADASYN\nThe other oversampling technique implemented in imlearn is adaptive synthetic sampling, or ADASYN. ADASYN is similar to SMOTE, and derived from it, featuring just one important difference. it will bias the sample space (that is, the likelihood that any particular point will be chosen for duping) towards points which are located not in homogenous neighborhoods. Applied to our sample data, this results in the following:\n\n","2457bf3b":"### Tomek links::\nTomek links are points in the dataset whose nearest neighbor is a member of a different class. \nThis includes outlier points embedded in a point cloud from another class, and boundary points in regions where it is unclear which of two or more classes is dominant. The TomekLinks undersampler can be used to remove these points. \n\nHere by default it will remove only links from majority classes, \nbut I suggest passing ratio='all' \nSo that all of the classes will be considered and treated.","2884bef5":"COMPARATIVE ANALYSIS WITH EDA FOR AN HIGHLY IMBALANCED DATASET USING MACHINE LEARNING ALGORITHMS \nWITH SAMPLING TECHNIQUES FOR CREDIT CARD FRAUD DECEPTION","c362259e":"Here we do commit common mistakes,\nwhen doing resampling, that is testing your model on the oversampled or undersampled dataset. \nIf we want to implement cross validation, remember to split your data into training and testing before oversample or undersample and then just oversample or undersample the training part.\nAnother way to avoid this is to use \"Pipeline\" method.","c5556b2c":"Here in the above figure there is 0 represents as Non-Fraudulent of 284315 records transactions\nand 1 represents as Fraudulent transactions of 492 records","3f6762b4":"Classification Algorithms Parameter Tuning Three algorithms - Logistic Regression, Support Vector Machine and Random Forest are used to solve the problem. A comparative study is then followed by evaluating each algorithm with respect to selected performance metrics. Simple logistic regression is used. For the parameters of Support Vector Machine and Random Forest, Grid Search technique is employed to tune the parameters.","6c9374ed":"#The confusion matrix shows that,84941+94 = 85035 correct predictions and 48+35= 93 is incorrect one here.\n\nTrue Positives: 94\n\nTrue Negatives: 84941\n\nFalse Positives: 35 (Type I error)\n\nFalse Negatives: 48( Type II error)","e1ceaa55":"### Decision tree classifier","c1f813c7":"Dataset Summary:\nWe have 284807 entries within 30 features and 1 target (Class).\nThere are no \"Null\" values, so no need to work on ways to replace values.\nThe mean of all the mounts made is relatively small, approximately USD 88.\nMost of the transactions were Non-Fraud (99.83%) of the time, while Fraud transactions occurs (017%) of the time in the dataframe.","75ef34ea":"#### in the above just we have displayed 3 columns like,Amount,Time and Class ","9b66ceab":"##### FINAL RESEARCH PROJECT SUBMISSION DEADLINES:15TH OF SEPT-2021@11.59PM","ced6ba66":"### Random Oversampling-COMPLETED","9deae734":"###### Getting statistics of the given data","7f7c4634":"##### Build models on other algorithms to see the better performing on SMOTE","b13be584":"#ROC curves FOR XGBOOST::\nfig = plt.figure(figsize=(14,16))   # Create window frame\n\n\nroc = [[\"Imbalanced\", fpr_xg1, tpr_xg1,'xg'],[\"SMOTE\", fpr_xg2, tpr_xg2,'xg'],\n      [\"ADASYN\", fpr_xg3, tpr_xg3,'xg']]\n\nfor i in range(6):\n    #8.1 Connect diagonals\n    ax = fig.add_subplot(3,2,i+1) \n    ax.plot([0, 1], [0, 1], ls=\"--\")  # Dashed diagonal line\n\n    #8.2 Labels ","cc6f9226":"#### perfom cross validation on the X_train & y_train to create:\n- X_train_cv\n- X_test_cv \n- y_train_cv\n- y_test_cv ","5fc9eacb":"From the above statistics it is clear that the model is highly specific than sensitive. The negative values are predicted more accurately than the positives.","a9155991":"Oversampling with SMOTE\nThe SMOTE algorithm is one of the first and still the most popular algorithmic approach to generating new dataset samples. The algorithm, introduced and accessibly enough described in a 2002 paper, works by oversampling the underlying dataset with new synthetic points.\n\nThe SMOTE algorithm is parameterized with k_neighbors (the number of nearest neighbors it will consider) and the number of new points you wish to create. Each step of the algorithm will:\n\nRandomly select a minority point.\nRandomly select any of its k_neighbors nearest neighbors belonging to the same class.\nRandomly specify a lambda value in the range [0, 1].\nGenerate and place a new point on the vector between the two points, located lambda percent of the way from the original point.\nThe imbalanced-learn documentation includes the following illustration:","1d390828":"##### Through statistical approach:","6c6488d4":"Probability of point having transaction amount approximately 2500 is 1,\nit means to say that allmost all transaction have have transaction amount less than 2500\nand here cdf curve will finds out the real fact...","04bb244c":"DATASET SOURCE:KAGGLE SITE\n","bb45b147":"### SMOTE TECHNIQUE::","dde7fcce":"### IMPORTENT LIBRARIES AND PACKAGES TO BE INSTALLED DURING THE COURSE OF MY RESEARCH PROJECT JOURNEY:","440282b9":"Outlier Detection and Removal Here, features with high negative correlation and high positive correlation are identified. We then remove the outliers that lies outside 2.5 times the IQR in an attempt to eliminate the effect of outliers.","d6a20870":"1.2  Increase successful detections with data resampling\u00b6\nresampling can help model performance in cases of imbalanced data sets\n1.2.0.1  Undersampling","725becf3":"### Model Evaluation:\n\nAccuracy is not always the correct metric for solving classification problems of imbalanced data.\nBecause the ROC curve is measured at all thresholds, the best threshold would be one at which the TPR is high and FPR is low, i.e., misclassifications are low.\n\nDepending on the use case, you have to account for what you need: \n1)High precision\n2)or high recall.","95333c5d":"Undersampling the majority class (non-fraud cases)\nStraightforward method to adjust imbalanced data\nTake random draws from the non-fraud observations, to match the occurences of fraud observations (as shown in the picture)\n1.2.0.2  Oversampling","4b98c5e0":"###### here we are finding out the length of the total Fraud records","567aedbc":"Number of Algorithms applied for imbalanced dataset\/Original dataset:LR,KNN,KMeans,SVM,DT,RF,XGB\n\n\nNumber of Algorithms applied for Balanced dataset\/Origina dataset:LR,KNN,KMeans,SVM,DT,RF,XGB\nModel Evaluations Metrix used:Precision,Recall,F1-Score,Accuracy,AUC-ROC,MCC,etc\n    \nEDA tools used:\nPlots for EDA Graphs used:\nBar-plots,Scatter plots,Histograms,Pie-plots,Boxplots,pairplots,Dist-plots,Confusion matrix,etc\n    \nK-fold cross validation used:K=5,15,10,however the ideal value of K=5,\nA)Since Data is highly imbalanced,in-order to remove lots of zeros,\n1)We use under-sampling techniques,\n\nB)In-order to oversample,we use Randomly\/Uniformly oversample minority class\n2)Over-sampling Techniques\n\nC)Other imp techniques used:\n3)SMOTE-(SYNTHETIC MINORITY OVERSAMPLING TEchnique)\n4)ADASYN-(ADAptive SYNthetic Technique\n5)TomekLink","14b8bbd2":"#The above Histogrm image clearly dipicts the distribution of each features\/columns\nand class distribution shows data is very highly imbalanced w.r.t.Genuine and fradulent transactions","e81fee7d":"https:\/\/miro.medium.com\/max\/1050\/1*6UFpLFl59O9e3e38ffTXJQ.png","e168f6c7":"Here the heatmap clearly shows that all the variable are multicollinear in nature, \nAnd which variable have a high collinearity with the target variable.\ninfact,We will refer this map back-forth while we building the linear model\nso that,it validate different correlated values along with p-value, \nfor identifying the correct variable to select\/eliminate from the depicted model.","0410e71c":"### DECISION TREE","2f72c2e5":"## Here is what either algorithm looks like in practice:","5357d9df":"Using different Undersampling Ratios for evaluating performance The technique of undersampling is used for handling the problem of imbalanced dataset. We previously evaluated the algorithms on 1:1 undersampling ratio dataset. Now we model the dataset with the real-world scenario, where the percentage of fraud is usually small. We now observe the performance of each algorithm with different undersampling ratios - 15%, 10%, 5% and 2%. Each time we remove the extreme outliers as before and then evaluate the performance with K-fold cross validation.","ee4c23e0":"Credit Card Fraud Detection Case:\nAssume that you are employed to help a credit card company to detect potential fraud cases so that the customers are ensured that they won\u2019t be charged for the items they did not purchase. You are given a dataset containing the transactions between people, the information that they are fraud or not, and you are asked to differentiate between them. This is the case we are going to deal with. Our ultimate intent is to tackle this situation by building classification models to classify and distinguish fraud transactions.\n\nWhy Classification? \n\nClassification is the process of predicting discrete variables (binary, Yes\/no, etc.).\nGiven the case, it will be more optimistic to deploy a classification model rather than any others.","a01f6e1c":"## Advanced under-sampling and data cleaning","222b25d2":"### Print the important features of the best model to understand the dataset","c5fdcf74":"Accuracy score= 99.92% which is higher than the baseline 99.83%.\nPrecision = 91\/(12+91) = 0.88. The rate of true positive in all positive cases.\nRecall = 91\/ (56+91) = 0.62. The rate of true positive in all true cases.\nF1-score = 0.73\nFalse positives cases = 12.","26a28398":"###### BY BALANCING THE DATASET WITH SMOTE AND ADASYN TECHNIQUE","02cff3ee":"\n### Oversampling with SMOTE and ADASYN\n\nIntroduction In a previous notebook I discussed random oversampling and undersampling: what they are, how they work, and why they're useful. The literature on sampling imbalanced datasets extends past these \"naive\" approaches. The imbalanced-learn module in sklearn includes a number of more advanced sampling algorithms, and I'll discuss the oversampling-related ones here. Most of these work by generate or subsetting new synthetic points. \n\nOversampling with SMOTE and ADASYN Introduction In a previous notebook I discussed random oversampling and undersampling: what they are, how they work, and why they're useful. The literature on sampling imbalanced datasets extends past these \"naive\" approaches. The imbalanced-learn module in sklearn includes a number of more advanced sampling algorithms, and I'll discuss the oversampling-related ones here. Most of these work by generate or subsetting new synthetic points.\n\nOversampling with SMOTE The SMOTE algorithm is one of the first and still the most popular algorithmic approach to generating new dataset samples. The algorithm, introduced and accessibly enough described in a 2002 paper, works by oversampling the underlying dataset with new synthetic points.\n\nThe SMOTE algorithm is parameterized with k_neighbors (the number of nearest neighbors it will consider) and the number of new points you wish to create. Each step of the algorithm will:\n\nRandomly select a minority point. Randomly select any of its k_neighbors nearest neighbors belonging to the same class. Randomly specify a lambda value in the range [0, 1]. Generate and place a new point on the vector between the two points, located lambda percent of the way from the original point. The imbalanced-learn documentation includes the following illustration:\n\n","f25827a6":"### Finding out Total cases ,Non-Fraud cases,Fraud  cases and fraudulent percentile:","7faef508":"The borderline SMOTE algorithms are so named because they will only sample points on the border. Points that are located in a neighborhood of n_neighbors points all of their class are left untouched, so e.g. points inside of the class clusters are never sampled. Similarly, the noisy outlier points located inside of the blue cluster are ignored, solving the innerpoint-outerpoint join problem.\n\nHowever, as the red cluster demonstrates, as a result SMOTE-borderline will tend to focus extremely heavily on the same relatively small number of points. We have replaced extensions to outliers with extensions from borderline points into their neighborhood. New points created in dense, but not totally homogenous neighborhoods which are essentially jittered versions of existing ones: they are displaced just a small amount from their progenitors. But when boundary points are small in number and distant from the rest of their neighborhood, you once again get somewhat odd-looking results: look at those crazy clusters near the decision boundary in the red group!\n\nIn this example the clusters overall still look linearly separable, for example, so we would not expect this to significantly effect the performance of e.g. a Linear SVM. But you should always be careful about checking the effect of your transformations net-net.\n\nNow borderline2:","2b2c2a7b":"###### Getting complete details about all the columns\/variables\/attributes\/features","19da2393":"### lets find the value counts of and w.r.t. 'Amount'and 'Class' variables","16af8971":"###### The complete fraud Amount descriptive statistics results are getting displayed","68ce3f15":"### BUILDING MODELS WITH BALANCED DATASET USING SMOTE AND ADSYN TECHNIQUES::","d6d6ac72":"### MODEL BUILDING ON ORIGINAL DATASET  OR RAW DATASET\/WITHOUT BALANCED DATASET","d0d6b362":"###Over all Conclusion here is:\nAll attributes selected after the elimination process shows Pvalues lower than 5%\nAnd thereby suggesting significant role in the fraud Prediction.\n\nThe Area under the ROC curve is 90.66 which is good\n\nOverall model could be improved with more data.","4a77f60a":"###### The above results are displayed as genuine transactions","a537b069":"### MY RESEARCH PROJECT APPROACH","07911d0f":"ID:PN969207","affbfca0":"### Here the main undersaming techniques\nThe ClusterCentroids algorithm is the one example of a prototype generation algorithm provided in imblearn. \nThis algorithm performs K-means clustering on the majority classes, \nthen creates new points which are averages of the coordinates of the generated clusters.","7fe8fac6":"##### FRAME WORK USED:\nCRISP-DM:CROSS INDUSTRY STANDARD PROCESS FOR DATA-MINING AS UNDER\nComplete Business Understanding\nData Understanding\nDat-Preparation\nData-preprocessing\nModel Building\nEvaluation\nDeployment\nBlockdiagram link:\nhttps:\/\/lh6.googleusercontent.com\/yuopV2Qi7QJlHnTnoXeKBJY74nkn4Qx7sthqa8-MQg9klOXzFHHXD4_IQe1zvp79NU9lQODipJc3BUivUWFlCRo4l8iHuwwlY9OwYeZXS6WqkEKffv4oPs3xTfss01rASmodj9E\n\nhttps:\/\/lh4.googleusercontent.com\/nCGCaNfBCBJR7ZhVe-8FuBoDjoEkah-fEffkh-E8yVoLvS2Wc6-2Z3Pdv1BP0jh1oEzDCMTS8nMc2IuX3vh_bwRaBfc4VS_k4np1WQk6Edb1_7YXKDXzqR-Jk2xM7WYSW8o13LI","381980cc":"In the following plot I show which points from the minority class were removed, to better illustrate what kinds of points get cleaned from the dataset:","0eef9ecb":"Scaling Amount and Time Anonymised features appear to have been scaled and centred around zero but Amount and Time have not been scaled. For Logistic Regression to perform well, the features have to be scaled, so the code below scales Amount and Time features.","3b925523":"###### Lets understand the no of columns and getting printed using \"df.columns\"","560415e4":"### Exploring the traditional method of fraud detection\nIn this exercise you're going to try finding fraud cases in our credit card dataset the \"old way\". First you'll define threshold values using common statistics, to split fraud and non-fraud. Then, use those thresholds on your features to detect fraud. This is common practice within fraud analytics teams.\n\nStatistical thresholds are often determined by looking at the mean values of observations. Let's start this exercise by checking whether feature means differ between fraud and non-fraud cases. Then, you'll use that information to create common sense thresholds. Finally, you'll check how well this performs in fraud detection.\n\npandas has already been imported as pd.\n\n### Instructions\nUse groupby() to group df on Class and obtain the mean of the features.\nCreate the condition V1 smaller than -3, and V3 smaller than -5 as a condition to flag fraud cases.\nAs a measure of performance, use the crosstab function from pandas to compare our flagged fraud cases to actual fraud cases.","e6e70871":"Summary:\nTime: Most transactions happended in day time.\nMean of transaction amount is 88 USD and 75% quatile is 77 USD.\nWe should better scale these two skewed features also.","5b4eb42d":"1.1.2  Data visualization\nFrom the previous exercise we know that the ratio of fraud to non-fraud observations is very low. You can do something about that, for example by re-sampling our data, which is explained in the next video.\n\nIn this exercise, you'll look at the data and visualize the fraud to non-fraud ratio. It is always a good starting point in your fraud analysis, to look at your data first, before you make any changes to it.\n\nMoreover, when talking to your colleagues, a picture often makes it very clear that we're dealing with heavily imbalanced data. Let's create a plot to visualize the ratio fraud to non-fraud data points on the dataset df.\n\nThe function prep_data() is already loaded in your workspace, as well as matplotlib.pyplot as plt.","8f701098":"###There are 283253 records with no fraud status and 473 records with fraud status.","2f92fa67":"import sys\ntry:\n    import numpy as np\n    import pyfits as pf\n    import scipy.ndimage as nd\n    import pylab as pl\n    import os\n    import heapq\n    from scipy.optimize import leastsq\n\nexcept ImportError:\n    print (\"Error: missing one of the libraries (numpy, pyfits, scipy, matplotlib)\")\n    sys.exit()","06737b32":"###### Here we are getting displayed the shape of the dataset ","60f55327":"###### The complete fraud Class label's descriptive statistics results are getting displayed","75b5413d":"#### perfom cross validation on the X_train & y_train to create:\n- X_train_cv\n- X_test_cv \n- y_train_cv\n- y_test_cv ","44c6a81b":"## Model Building-COMPLETED\n- Build different models on the imbalanced dataset and see the result","be3bac93":"### NOTE::\nDEAR SIR\/EVALUATOR SIR,\nAT MY SYSTEM XGB IS NOT ALL WORKING,\nI TRIED MANY EFFORTS,UNABLE TO DO SO,\nTHE COD IS AVAILABLE...","248d7e12":"#### BUILDS VARIOUS MODELS IN COMPACT FORM:USING ORIGINAL DATA","ee0eaaa4":"### confusion matrix","ccfaf06a":"How do we Know if the model is imbalanced or not?\nWe check the count of the dependent categorical values the ratio should be 10:1 for the data set to be considered as a balanced Data set.\n\n2. Confusion matrix : After the prediction is done check the confusion matrix.\n[[TRUE POSITIVE] [ FALSE POSITIVE ]\n[FALSE NEGATIVE][TRUE NEGATIVE]]\n\nif any of the values become 0. Well your model is Biased and your data set is imbalanced\n\n3. Not a good opinion but I also checked the count of the predicted variable, got from testing the model on test Data. If the count is 0 or 1, the Your model is considered to be biased.\n\nWays to Handle Imbalanced data set:\nUnder sampling: In this method basically we downsize the actual data set in such a way that the ratio of the dependent categories become 10:1. Some of the ways to do under sampling is :\n\na.Condensed Nearest-Neighbor\nb. One-Sided Selection\nSee the list for more details and more algorithms:\nscikit-learn-contrib\/imbalanced-learn\nimbalanced-learn - Python module to perform under sampling and over sampling with various techniques.\ngithub.com\nMy personal opinion is not to use Under sampling.\nWhy?\nThe reason is that we are actually reducing the data set thus giving the model less data to feed on. I will give you a example we have a data set of 10000 data and there are only 100 data points for 1 while others are 0. Now after performing under sampling we are basically reducing the data set to 1100 samples where 1000 are 0 and 100 are 1 . So we are getting rid of almost 9000 samples and feeding it to the model. So the model is more prone to error. Though different models use different methods to do under sampling the end result is the same that is it has less number of rows in the data set.\n\nNOTE: I am discussing only the possibility not saying one is better than the other . This is my personal opinion. And we are not going to discuss it here.\n\n2. Over sampling: This method uses synthetic data generation to increase the number of samples in the data set.\nGoal: Increase the minority class so that the data set becomes balanced by creating synthetic observations based upon the existing minority observations.\n\nSMOTE:\nWhat smote does is simple. First it finds the n-nearest neighbors in the minority class for each of the samples in the class . Then it draws a line between the the neighbors an generates random points on the lines.\n","4d5e1bcd":"#Here the is a Feature Scaling\nAs we know that,before, features V1 to V28 have been transformed by PCA and scaled the given dataset already. \nWhereas feature \"Time\" and \"Amount\" have not been scaled. \nAnd considering that, we will analyze these two features with other V1-V28, \nThey should better be scaled before we could train the model using various algorithms approach. \n\n\n#here there is a process to which scaling mehtod should we use?\nThe Standard Scaler is not recommended as \"Time\" and \"Amount\" features are not normally distributed.\n\nThe Min-Max Scaler is also not recommende as there are noticeable outliers in feature \"Amount\".\n\nThe Robust Scaler are robust to outliers: (xi\u2013Q1(x))\/( Q3(x)\u2013Q1(x)) (Q1 and Q3 represent 25% and 75% quartiles).\nSo we choose Robust Scaler to scale these two features in the module.###CORRELATION MATRIX:::\nCorrelation matrices are the essence of understanding our dataset. \nHere,We want to know if there are features that influence heavily in whether a specific transaction is a fraudulent or not.","ed405cf0":"The common approach and way to visualize the trade-offs of different thresholds is y using an ROC curve, \nA plot of the true positive rate (# true positives\/ total # positives) versus the false positive rate (# false positives \/ \ntotal # negatives) \nFor all possible choices of threshold values. \nA model with good classification accuracy should have significantly more true positives than false positives at all thresholds.\n\nThe optimum position for roc curve is towards the top left corner where the specificity and sensitivity are at optimum levels\n\nArea Under The Curve (AUC) The area under the ROC curve quantifies model classification accuracy; the higher the area, \nThe greater the disparity between true and false positives, and the stronger the model in classifying members of the training \ndataset. An area of 0.5 corresponds to a model that performs no better than random classification \nAnd a good classifier stays as far away from that as possible. An area of 1 is ideal. The closer the AUC to 1 the better","87ef674f":"### K-NEAREST NEIGHBOURS K-NN AND KMeans ALGORITHMS\n","94f7653c":"The TomekLinks algorithm has the advantage that it is quite conservative; that is, it is good at finding and removing points we are quite certain are outliers (though it pays to inspect the result yourself afterwards, just to be sure the right things were removed).\n\nIt's not easily tunable however; we can't tell the algorithm to be more or less aggressive. Also Tomek's Links retains some outliers. In particular, points which are outliers which are closest to other points that are outliers will be retained. A handful of them show up in this example.","b2b1e23e":"Feature Selection: Backward elemination (P-value approach)\u00b6","1ce8b95b":"### In the above we have just filtered the data w.r.t.Time,Amount and Class and renamed as FilteredData","12b54c9c":"Advanced under-sampling and data cleaning refered site link as follows\nhttps:\/\/www.kaggle.com\/residentmario\/advanced-under-sampling-and-data-cleaning","643e69ea":"Note:\n#X \u2192Independent Variable in Pandas DataFrame\n#y \u2192dependent Variable in Pandas DataFrame format","b3a66392":"### #The following piece of code shows how we can create a fake dataset and plot it using python's matplotlib library","d702b5d0":"Here in the above  we got 284807 records abd 31 variables ","52fcb044":"The variance of amount is quite large, there are so many outliers in the data.","90799d62":"##### Preserve X_test & y_test to evaluate on the test data once you build the model","a9212910":">>> import numpy as np\n>>> from sklearn.metrics import precision_recall_curve\n>>> y_true = np.array([0, 0, 1, 1])\n>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n>>> precision, recall, thresholds = precision_recall_curve(\n...     y_true, y_scores)\n>>> precision\narray([0.66666667, 0.5       , 1.        , 1.        ])\n>>> recall\narray([1. , 0.5, 0.5, 0. ])\n>>> thresholds\n#array([0.35, 0.4 , 0.8 ])","272a4b51":"AUTHOR :YADU SINGH","6ffb6459":"### If there is skewness present in the distribution use:\n- <b>Power Transformer<\/b> package present in the <b>preprocessing library provided by sklearn<\/b> to make distribution more gaussian","72cede1d":"###### for imbleran install the following from anaconda prompt \n#pip install -U imbalanced-learn\n#conda install -c conda-forge imbalanced-learn","98c2902c":"###### Quantiles of Genuine and Fraud transactions:\n----------------------------------------------------------------------------------------\n[ 0.    5.65 22.   77.05]\n[  0.     1.     9.25 105.89]","3793e412":"###### The complete Genuine Amount's descriptive statistics results are getting displayed","2d6a5137":"Precision = 113\/(113+25) = 0.82. The rate of true positive in all positive cases.\nRecall = 113\/ (113+34) = 0.76. The rate of true positive in all true cases.\nF1-score = 0.79 False positives cases = 31.","778f34fe":"##Duplicates are found in the record as well","d189d480":"### CASE-WISE AMOUNT OF STATISTICS:","9e7ffb11":"Version 2 of NearMiss is similar.","a0835aa5":"Here's the result of an application to our sample dataset. It doesn't look much different overall.","30c10363":"## Credit Card Fraud Detection\n\nIn this project you will predict fraudulent credit card transactions with the help of Machine learning models. Please import the following libraries to get started.","dbbcec0c":"Note:\n#X \u2192Independent Variable in Pandas DataFrame\n#y \u2192dependent Variable in Pandas DataFrame format","64a00749":"#Here the is a Feature Scaling\nAs we know that,before, features V1 to V28 have been transformed by PCA and scaled the given dataset already. \nWhereas feature \"Time\" and \"Amount\" have not been scaled. \nAnd considering that, we will analyze these two features with other V1-V28, \nThey should better be scaled before we could train the model using various algorithms approach. \n\n\n#here there is a process to which scaling mehtod should we use?\nThe Standard Scaler is not recommended as \"Time\" and \"Amount\" features are not normally distributed.\n\nThe Min-Max Scaler is also not recommende as there are noticeable outliers in feature \"Amount\".\n\nThe Robust Scaler are robust to outliers: (xi\u2013Q1(x))\/( Q3(x)\u2013Q1(x)) (Q1 and Q3 represent 25% and 75% quartiles).\nSo we choose Robust Scaler to scale these two features in the module.","19294156":"### Here in the above,I have used for loop to get details about Fraud transaction as 449 and Genuine transactions as 284357\ncountless=Fraud\ncountmore=Genuine","728b5240":"Here in the above there are 492 fraudulent transactions are displayed with 31 features or variables","6cdb16f3":"There are no missing \/ Null values either in columns or rows","5042cb71":"fraud.Amount.describe()","d23cad78":"### VARIOUS USEFUL METHODS:","789d0b20":"This is version 1 of NearMiss. We can see that the points sampled out of the white and blue classes are those which are nearest to minority class (red) outlier points in their respective point clouds.","b884aaca":"The ratio of fraudulent transactions is very low. This is a case of class imbalance problem, and you're going to learn how to deal with this in the next exercises.","4ba47e3a":"The weakness of SMOTE is readily apparent from this quick test. Because the algorithm doesn't have any jitter, for minority class sample clouds with few enough points it tends to result in long \"data lines\". This results in some very funky looking resultant datasets. Somewhat funny-looking results are actually a trademark of these sampling techniques. For a broad class of data it's actually reasonably easy to tell when someone has used SMOTE on it. But remember, we're just building a new dataset sample. Funny looking results are fine, as long as they actually result in improved classifiers when used as input down the line!\n\nSMOTE has also done something here that I am less comfortable with: it's constructed a \"bridge\" between the main red point cloud and a handful of outlier points located in the blue cluster. We can see that these new points are quite dense if we zoom in on the structure:","5ce97079":"##Logistic regression equation\n\n##P=e^(\u03b20+\u03b21X1)\/1+e^(\u03b20+\u03b21X1)","354a33f0":"###### here we are finding out the length of the total Genuine records","2a5f3671":"##### Oservations:\nhere,now it has been calculated that there are 284357 transactions which has a transaction amount less than 2500.mean 99.84% of transaction have transactionleass than 2500."}}