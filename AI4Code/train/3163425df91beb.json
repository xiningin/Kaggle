{"cell_type":{"ff48a362":"code","2a72fca2":"code","72af7966":"code","31816fde":"code","8018de95":"code","0e9be5dc":"code","df64f6e2":"code","10ecd750":"markdown","9cbc5c2f":"markdown","c2e5cde9":"markdown","57cc130d":"markdown","11bf91b8":"markdown"},"source":{"ff48a362":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas_profiling","2a72fca2":"raw_data = pd.read_csv('\/kaggle\/input\/personal-loan-modeling\/Bank_Personal_Loan_Modelling.csv')","72af7966":"raw_data.profile_report()","31816fde":"from sklearn.model_selection import train_test_split\nX = raw_data.drop(['Personal Loan', 'ID'], axis=1)\nY = raw_data['Personal Loan']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2)","8018de95":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train, Y_train)","0e9be5dc":"predictions = model.predict(X_test)\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\nac = accuracy_score(Y_test, predictions)\nprecision = precision_score(Y_test, predictions)\nrecall = recall_score(Y_test, predictions)\nprint(f'Accuracy: {ac:.3}\\nPrecision:{precision:.3}\\nRecall: {recall:.3}')\ncm = confusion_matrix(Y_test, predictions)\nlabels = ['No', 'Yes']\nsns.heatmap(cm, xticklabels=labels, yticklabels=labels, square=True, annot=True, fmt=\"d\")\nplt.title(\"Confusion Matrix\", fontsize=24)\nplt.ylabel('Ground truth', fontsize=16)\nplt.xlabel('Prediction', fontsize=16)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)","df64f6e2":"feature_dict = {\n    'feature': X_train.columns,\n    'importance': model.feature_importances_\n}\nfeature_df = pd.DataFrame(feature_dict)\nfeature_df.sort_values(by='importance', ascending=False)","10ecd750":"We can see we have lots of correlations in our dataset, for example, income correlates with most other columns.\nWe can also see that Personal Loan correlates with other columns fairly strongly, which means we can expect decent accuracy from our predictive model.\n\nSpeaking of models, let's try to do a simple random forest. It's great because we don't need to scale data for it, unlike neural networks, so we can just straight up proceed to train our model.\n\nFirst, let's split data into train and test set, like this:","9cbc5c2f":"Let's see how well our model works on test data.","c2e5cde9":"It's always a good idea to get a dataset overview first. Kaggle built-in analysis is decent for this, but I like to get several overviews, and will start with pandas_profiling report:","57cc130d":"This result is incredibly good.\nIn fact, it's so good I suspect we have a leak in our dataset. Let's see what are the most important features.","11bf91b8":"Since I don't see how income can leak the target value, I am forced to conclude that maybe our dataset really does have such strong correlations."}}