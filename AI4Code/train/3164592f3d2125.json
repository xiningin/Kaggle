{"cell_type":{"4be00e67":"code","dcafca50":"code","a88cd771":"code","d74f2d22":"code","2e53c8cd":"code","495a6ff0":"code","e191a23c":"code","3590ac42":"code","d35dd7fb":"code","ae6e0952":"code","a528d694":"code","dc5a14da":"code","92bc9765":"code","255d5f08":"code","2c2334e6":"code","5b1d21ea":"code","d4a49f71":"code","1be6d36f":"code","0cd52ce5":"code","35bcc7aa":"code","04767e36":"code","78320403":"code","ece9b07c":"code","71bfdb97":"code","2259dcc4":"code","8a9beb1a":"code","8ea7df0e":"code","fcdcb0f0":"code","ca0fa33a":"code","4a61a83d":"code","5ec88ea2":"markdown"},"source":{"4be00e67":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n'''for dirname, _, filenames in os.walk('..\/input\/feedback-prize-2021'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))'''\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dcafca50":"#pip install pymorphy2","a88cd771":"import io\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_datasets as tfds","d74f2d22":"train_df = pd.read_csv('..\/input\/proc-data-2\/train_df.csv')\ntrain_df","2e53c8cd":"train_df.shape","495a6ff0":"drop_index_list = []\nfor i in range(len(train_df)):\n    if len(train_df.loc[i].discourse_text) < 50:\n        drop_index_list.append(i)\ntrain_df.loc[drop_index_list]","e191a23c":"train_df.drop(drop_index_list, inplace=True)\ntrain_df.shape","3590ac42":"train_df.reset_index(drop=True, inplace=True)\ntrain_df","d35dd7fb":"sent_num_list = []\nsent_num = 0\nfor i in range(0, len(train_df)):\n    sent_num_list.append(sent_num)\n    if i == len(train_df)-1:\n        break\n    if train_df.loc[i]['id'] == train_df.loc[i+1]['id']:\n        sent_num += 1\n    else:\n        sent_num = 0\ntrain_df['sent_num'] = sent_num_list\ntrain_df","ae6e0952":"import glob\n\nfile_name_list = []\nfor file in glob.glob(\"..\/input\/feedback-prize-2021\/test\/*\"):\n    file_name_list.append(file[-16:])\n    \nfile_name_list\n    \n","a528d694":"test_data = {}\nfor text in file_name_list:\n\n    with open('..\/input\/feedback-prize-2021\/test\/' + text) as f:\n        contents = f.read()\n        test_data[text[:-4]] = contents","dc5a14da":"test_data_df = pd.DataFrame.from_dict(test_data, orient='index', columns=['discourse_text'])\ntest_data_df.reset_index(inplace=True)\ntest_data_df","92bc9765":"test_data_df","255d5f08":"import nltk\n\nindex_list = []\nsentence_list = []\npredictionstring = []\nsent_num_list = []\nfor index in test_data_df['index'].values.tolist():\n    sent_num = 0\n    i = 0\n    text = test_data_df[test_data_df['index'] == index]['discourse_text'].values[0]\n    tokenized_sent = nltk.sent_tokenize(text)\n    for sent in tokenized_sent:\n        sent_num_list.append(sent_num)\n        sent_num += 1\n        string = []\n        if sent != '':\n            sentence_list.append(sent)\n            index_list.append(index)\n            for word in sent.split(' '):\n                string.append(str(i))\n                i += 1\n            predictionstring.append(' '.join(string))\n            \ntest_data = pd.DataFrame(index_list, columns=['id'])\ntest_data['discourse_text'] = sentence_list\ntest_data['sent_num']= sent_num_list\ntest_data['predictionstring'] = predictionstring\ntest_data['discourse_type'] = 'unknown'\ntest_data","2c2334e6":"from nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r\"\\w+\")\nproc_text = []\n\nfor sentence in test_data['discourse_text']:\n    proc_sentence = tokenizer.tokenize(''.join(sentence.lower()))\n    final_sentence = []\n    for word in proc_sentence:\n        new_word = ''.join(i for i in word if not i.isdigit())\n        if new_word not in ['', 'a', 'the', 's', 'm', 't', 've']:\n            final_sentence.append(new_word)\n    proc_text.append(' '.join(final_sentence))\n","5b1d21ea":"test_data['proc_discourse_text'] = proc_text\ntest_data","d4a49f71":"from gensim.models import doc2vec\n\ndef label_sentences(corpus, label_type):\n    \"\"\"\n    Gensim's Doc2Vec implementation requires each document\/paragraph to have a label associated with it.\n    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n    a dummy index of the complaint narrative.\n    \"\"\"\n    labeled = []\n    for i, v in enumerate(corpus):\n        label = label_type + '_' + str(i)\n        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n    return labeled","1be6d36f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_df.proc_discourse_text, train_df.discourse_type, random_state=0, test_size=0.3, stratify=train_df.discourse_type)\nX_train = label_sentences(X_train, 'Train')\nX_test = label_sentences(X_test, 'Test')\nX_sub = label_sentences(test_data.proc_discourse_text, 'Sub')\nall_data = X_train + X_test + X_sub\nall_data","0cd52ce5":"from tqdm import tqdm\nmodel_dbow = doc2vec.Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\nmodel_dbow.build_vocab([x for x in tqdm(all_data)])","35bcc7aa":"model_dbow = doc2vec.Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\nmodel_dbow.build_vocab([x for x in tqdm(all_data)])","04767e36":"from sklearn import utils","78320403":"%%time\nfor epoch in range(30):\n    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n    model_dbow.alpha -= 0.002\n    model_dbow.min_alpha = model_dbow.alpha","ece9b07c":"def get_vectors(model, corpus_size, vectors_size, vectors_type):\n    \"\"\"\n    Get vectors from trained doc2vec model\n    :param doc2vec_model: Trained Doc2Vec model\n    :param corpus_size: Size of the data\n    :param vectors_size: Size of the embedding vectors\n    :param vectors_type: Training or Testing vectors\n    :return: list of vectors\n    \"\"\"\n    vectors = np.zeros((corpus_size, vectors_size))\n    for i in range(0, corpus_size):\n        prefix = vectors_type + '_' + str(i)\n        vectors[i] = model.docvecs[prefix]\n    return vectors","71bfdb97":"train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\ntest_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')\nsub_vectors_dbow = get_vectors(model_dbow, len(X_sub), 300, 'Sub')","2259dcc4":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(multi_class='multinomial', solver = 'sag') # solver='lbfgs' raises AttributeError: 'str' object has no attribute 'decode'\nlogreg.fit(train_vectors_dbow, y_train)\nlogreg.score(test_vectors_dbow, y_test)","8a9beb1a":"logreg.predict(test_vectors_dbow)","8ea7df0e":"sub_predict = logreg.predict(sub_vectors_dbow)\nsub_predict","fcdcb0f0":"submission = pd.DataFrame(test_data['id'], columns=['id'])\nsubmission['class'] = sub_predict\nsubmission['predictionstring'] = predictionstring\nsubmission","ca0fa33a":"submission['class'].value_counts()","4a61a83d":"submission.to_csv('submission.csv', index=False)","5ec88ea2":"### Training the model"}}