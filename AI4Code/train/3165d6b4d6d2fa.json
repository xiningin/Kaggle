{"cell_type":{"d6318ed0":"code","f7a69f44":"code","e926725b":"code","727eca2c":"code","47d2fbe1":"code","cdb107c9":"code","fe843019":"code","195ca58c":"code","79645ad4":"code","bd81fe6d":"code","740c8498":"code","ce73d0a4":"code","66d093db":"code","790f9fd2":"code","707887fe":"code","b7bdd54e":"code","dd3e3291":"code","cbbc1276":"code","5c477564":"code","de6315ba":"code","0324e28f":"code","d4c61647":"code","f7f217fc":"code","e703b1d4":"code","033b12c0":"code","789ea5a3":"code","70b6d550":"code","2134243f":"code","17067e4f":"code","2c8df592":"code","bfc06506":"code","c9a861f4":"code","80f9a452":"code","75e4d9cd":"code","0da97303":"code","04e7ed4c":"code","feae44a8":"code","09570784":"code","984514b7":"code","faaadf75":"code","17e49e5c":"code","726d2bbb":"code","34191736":"code","b6cdd61a":"code","0ee38717":"code","6e2ba887":"code","a2cfd554":"code","70fd3c96":"code","35e6cc89":"code","77b38d97":"code","57e124e6":"code","800d3141":"code","4bd552ba":"code","85b12a7e":"code","0bb038cf":"code","64175262":"code","f300a255":"markdown","b998d575":"markdown","e1a95eab":"markdown","638570fe":"markdown","4fd0b530":"markdown","a4b4f127":"markdown","928deb92":"markdown","17f7792b":"markdown","abc5954d":"markdown","e564a9c0":"markdown","d473ae1b":"markdown","e09ae769":"markdown","2292c0de":"markdown","e523d9de":"markdown","6303109f":"markdown","2642bb4d":"markdown","93b34b2f":"markdown","69d7b7c7":"markdown","28c1fefd":"markdown"},"source":{"d6318ed0":"!pip install pyspark","f7a69f44":"import pyspark\nfrom pyspark.sql import SparkSession, Row, functions as f\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import StringType, TimestampType, DoubleType, FloatType, IntegerType, LongType, StructField, StructType\nfrom IPython.display import Image\n\nimport random\nrandom.seed(1990)","e926725b":"spark = SparkSession.builder.appName(\"spark_app\").getOrCreate()\nspark","727eca2c":"# Regions\ngeo_id = [random.choice([\"regA\",\"regB\",\"regC\",\"regD\",\"regE\"]) for x in range(500)]\n\n# Products\nprod_id = [random.choice([\"prodA\",\"prodB\",\"prodC\",\"prodD\",\"prodE\",\"prodF\",\n                          \"prodG\",\"prodH\",\"prodI\",\"prodJ\",\"prodK\",\"prodL\"]) for x in range(500)]\n\n# Values\nvalue = [random.uniform(1000,10000) for x in range(500)]\nvalue[5] = None\nvalue[15] = None\nvalue[245] = None","47d2fbe1":"df = spark.createDataFrame([Row(prod=p, geo=g, val=v) for p,g,v in zip(prod_id, geo_id, value)])\ndf.show(7)","cdb107c9":"df.createOrReplaceTempView(\"train_df\")","fe843019":"geo_df = spark.createDataFrame([Row(geo_id = \"regA\", geo_name = \"Europe\"),\n                                Row(geo_id = \"regB\", geo_name = \"Asia\"),\n                                Row(geo_id = \"regC\", geo_name = \"N_America\"),\n                                Row(geo_id = \"regD\", geo_name = \"S_America\"),\n                                Row(geo_id = \"regE\", geo_name = \"Africa\")])","195ca58c":"geo_df.createOrReplaceTempView(\"geo_df\")\ngeo_df.show()","79645ad4":"# if True instead of Spark syntax a SQL equivalent will be executed\nsql = False","bd81fe6d":"# SQL and Spark equivalent:\nspark.sql(\"SELECT prod FROM train_df\").show(7) if sql else \\\ndf.select(\"prod\").show(7)","740c8498":"prod_ids = [\"prodA\",\"prodB\",\"prodC\",\"prodD\",\"prodE\",\"prodF\", \"prodG\",\"prodH\",\"prodI\",\"prodJ\",\"prodK\",\"prodL\"]\nprod_names = [\"smarfone\", \"PC\", \"laptop\", \"headphones\", \"tv\", \"speaker\", \n              \"keyboard\", \"mouse\", \"charger\", \"powerbank\", \"microphone\", \"camera\"]\n\nprod_df = spark.createDataFrame([Row(prod_id = i, prod_name = n) for i,n in zip(prod_ids, prod_names)])\nprod_df.createOrReplaceTempView(\"prod_df\")\nprod_df.show()","ce73d0a4":"# SQL example of INNER JOIN\nspark.sql(\"SELECT geo_df.geo_name, prod_df.prod_name, train_df.val FROM train_df \\\n            INNER JOIN prod_df ON prod_df.prod_id = train_df.prod \\\n            INNER JOIN geo_df ON geo_df.geo_id = train_df.geo\").show(7)","66d093db":"# Spark join() is also 'inner' by default\ndf.groupBy(\"prod\").agg(f.round(f.sum(\"val\"), 2).alias(\"total value\"))\\\n                  .sort(\"total value\")\\\n                  .join(prod_df, df.prod == prod_df.prod_id)\\\n                  .select(\"prod_name\", \"total value\")\\\n                  .show()","790f9fd2":"# WHERE \/ where()\n# SQL and Spark equivalent:\nspark.sql(\"SELECT * FROM train_df WHERE prod != 'prodA' AND val > 9900\").show() if sql else \\\ndf.where(df[\"prod\"] != \"prodA\").where(f.col(\"val\") > 9900).show()","707887fe":"# LIKE \/ like()\n# SQL and Spark equivalent:\nspark.sql(\"select * from train_df where prod like '%A'\").show(7) if sql else \\\ndf.where(df.prod.like('%A')).show(7)","b7bdd54e":"# orderBy()\ndf.groupBy([\"prod\",\"geo\"]).sum().orderBy(\"geo\").show(7)","dd3e3291":"# SUM, AVG, COUNT \/ sum(), avg(), count()\n# SQL and Spark equivalent:\nq = (\"SELECT prod, geo, SUM(val) val_sum, AVG(val) val_avg, COUNT(*), COUNT(val) FROM train_df GROUP BY prod, geo\")\nspark.sql(q).show(7) if sql else \\\ndf.groupBy([\"prod\",\"geo\"]).agg(f.sum(\"val\").alias(\"val_sum\"), f.avg(\"val\").alias(\"val_avg\"), \n                               f.count(\"*\"), f.count(\"val\")).show(7)","cbbc1276":"# Select unique combinations\n# DISTINCT \/ distinct()\nspark.sql(\"SELECT DISTINCT prod, geo FROM train_df\").show(5) if sql else \\\ndf.select(\"prod\", \"geo\").distinct().show(5)\n\n# alternative\n# df.dropDuplicates([\"prod\", \"geo\"]).show(5)","5c477564":"# Drop rows with any null values\n# SQL and Spark equivalent:\nspark.sql(\"select * from train_df where val is not null\").count() if sql else \\\ndf.dropna(\"any\").count()","de6315ba":"# Show rows with null values in val column\ndf.where(df.val.isNull()).show()\n# df.where(f.isnull(\"val\")).show()","0324e28f":"# Replace null values with 1\n# SQL and Spark equivalent:\nq = \"SELECT prod, geo, IF(val is null, 1, val) AS val FROM train_df\"\nspark.sql(q).show() if sql else \\\ndf.fillna(1).show(7)\n# df.fillna({\"val\": 1}).show(7)","d4c61647":"# Replacing values with REGEXP_REPLACE \/ replace()\n# SQL and Spark equivalent:\nspark.sql(\"SELECT geo, REGEXP_REPLACE(prod, 'prodE', 'Product E') AS prod, val FROM train_df\").show(3) if sql else \\\ndf.replace(\"prodE\", \"Product E\").show(3)\n\n# or with dictionary\n# df.replace({\"prodA\": \"Product A\", \"prodB\": \"Product B\"}).show(3)","f7f217fc":"# Rename columns\n# SQL and Spark equivalent:\nspark.sql(\"SELECT prod, geo, val AS volume FROM train_df\").show(3) if sql else \\\ndf.withColumnRenamed(\"val\", \"volume\").show(3)\n\n# df.select(df.val.alias(\"volume\")).show(3)","e703b1d4":"# New column\n# SQL and Spark equivalent:\nspark.sql(\"SELECT *, val\/1000 AS minival FROM train_df\").show(3) if sql else \\\ndf.withColumn(\"minival\", df[\"val\"] \/ 1000).show(3)\n# df.select(\"*\", (df.val\/1000).alias(\"minival)).show(3)","033b12c0":"# CASE WHEN THEN\/ when() otherwise()\n# SQL and Spark equivalent:\nq = \"SELECT prod, CASE WHEN val > 7500 THEN 1 WHEN val < 2500 THEN 3 ELSE 2 END AS out FROM train_df\"\nspark.sql(q).show(5) if sql else \\\ndf.select(df.prod, f.when(df.val > 7500, 1).when(df.val < 2500, 3).otherwise(2).alias(\"out\")).show(5)","789ea5a3":"# SUBSTRING \/ substring() \n# SQL and Spark equivalent:\nspark.sql(\"SELECT SUBSTRING(prod, 4, 2) AS id FROM train_df\").show(5) if sql else \\\ndf.select(f.substring(\"prod\", 4, 2).alias(\"id\")).show(5)\n# or\n# df.select(df.prod.substr(4, 2).alias(\"id\")).show(5)","70b6d550":"windowSpec = Window.partitionBy('prod')\n\n# SQL and Spark equivalent:\nspark.sql(\"SELECT prod, val, SUM(val) OVER (PARTITION BY prod) AS prod_val FROM train_df\").show(3) if sql else \\\ndf.select(\"prod\", \"val\", f.sum(\"val\").over(windowSpec).alias(\"prod_val\")).show(3)","2134243f":"windowSpec = Window.partitionBy(\"prod\").orderBy(\"geo\")\n\n# rank() function returns rank value, if it's the same it returns the same score\n# then the number of row index\n# dense_rank() returns rank value in natural order\ndf.withColumn(\"ranked\", f.rank().over(windowSpec))\\\n  .withColumn(\"ranked_dense\", f.dense_rank().over(windowSpec))\\\n  .withColumn(\"row_number\", f.row_number().over(windowSpec)).show(10)","17067e4f":"windowSpec = Window.partitionBy(\"prod\").orderBy(\"val\")\n\n# Cumulative sum\ndf.withColumn(\"sum_from_start\", f.sum(df.val).over(windowSpec)).show(5)","2c8df592":"# Stop the app\nspark.stop()","bfc06506":"spark = SparkSession.builder.appName('bike_app').master(\"local[*]\").getOrCreate()","c9a861f4":"# Merge all 2018 csv files to one DataFrame\nfiles = \"..\/input\/ford-gobike-data\/2018*.csv\"\ngoBike = spark.read.csv(files, header=True, inferSchema=True)\nprint(f\"Total Records = {goBike.count()}\")","80f9a452":"# Add 2017 data\ngoBike = goBike.drop(\"bike_share_for_all_trip\")\ngoBike = goBike.unionAll(spark.read.csv(\"..\/input\/ford-gobike-data\/2017-fordgobike-tripdata.csv\", header=True, inferSchema=True))\nprint(f\"Total Records = {goBike.count()}\")","75e4d9cd":"goBike.printSchema()","0da97303":"# Exemplary record\ngoBike.show(1, vertical=True)","04e7ed4c":"# Some data is missing\ngoBike.where(goBike.member_birth_year.isNull()).count()","feae44a8":"# 1. Remove rows with null values\ngoBike = goBike.dropna(\"any\")","09570784":"# 2. Distribution of \"member_gender\" variable\ngoBike.groupBy(\"member_gender\").count().show()","984514b7":"# 3. Count min, max, avg age of customers\ngoBike.select((2020 - goBike[\"member_birth_year\"]).alias(\"age\")).describe()\\\n      .select(\"summary\", f.round(\"age\", 0).alias(\"age\")).show()","faaadf75":"# 4. Count total number of unique bikes\ngoBike.select(\"bike_id\").distinct().count()","17e49e5c":"# 5. Count total number of unique stations\ngoBike.select(\"start_station_id\").union(goBike.select(\"end_station_id\")).distinct().count()","726d2bbb":"# 6. Check bike with shortest and longest rental time\ngoBike.groupBy(\"bike_id\").agg(f.sum(\"duration_sec\").alias(\"total_time\")).orderBy(\"total_time\").show(1)\ngoBike.groupBy(\"bike_id\").agg(f.sum(\"duration_sec\").alias(\"total_time\")).orderBy(f.desc(\"total_time\")).show(1)","34191736":"# 7. Calculate average time of single rental\ngoBike.select(f.avg(\"duration_sec\").alias(\"average\")).show()","b6cdd61a":"# 8. Find stations with the most traffic between them\ngoBike.select(f.when(goBike[\"start_station_id\"] > goBike[\"end_station_id\"], \n                     f.array(goBike[\"start_station_id\"], goBike[\"end_station_id\"]))\\\n                     .otherwise(f.array(goBike[\"end_station_id\"], goBike[\"start_station_id\"]))\\\n                     .alias(\"route\"))\\\n      .groupBy(\"route\")\\\n      .count()\\\n      .orderBy(f.desc(\"count\"))\\\n      .show(1)","0ee38717":"# Show stations from route above\ngoBike.filter((goBike.start_station_id == 6) | (goBike.start_station_id == 15)) \\\n      .select(\"start_station_name\").distinct().show(truncate=False)\n# Source: google maps\nImage(\"..\/input\/images\/popular_route.JPG\", width=800)","6e2ba887":"# 9. Find rush hour\ngoBike.select(f.hour(\"start_time\").alias(\"hour\"))\\\n.groupBy(\"hour\").count().orderBy(f.desc(\"count\")).show(7)","a2cfd554":"# 10. Find average rentals grouped by weekday\ngoBike.select(f.date_format(\"start_time\", \"dd.MM.yyyy\").alias(\"date\"), \\\n              f.date_format(\"start_time\", \"E\").alias(\"weekday\")) \\\n      .groupBy(\"date\", \"weekday\").count() \\\n      .groupBy(\"weekday\").agg(f.avg(\"count\").alias(\"avg_use\")) \\\n      .orderBy(\"avg_use\").show()","70fd3c96":"# 11. Calculate average distance between stations for all trips\nfrom math import radians, cos, sin, asin, sqrt\nfrom pyspark.sql.types import FloatType\n\n# Credit: Michael Dunn https:\/\/stackoverflow.com\/a\/4913653\ndef haversine(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Calculate the great circle distance between two points \n    on the earth (specified in decimal degrees)\n    \"\"\"\n    # convert decimal degrees to radians \n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n\n    # haversine formula \n    dlon = lon2 - lon1 \n    dlat = lat2 - lat1 \n    a = sin(dlat\/2)**2 + cos(lat1) * cos(lat2) * sin(dlon\/2)**2\n    c = 2 * asin(sqrt(a)) \n    r = 6371 # Radius of earth in kilometers\n    return c * r\n\nhaversine_udf = f.udf(haversine, FloatType())","35e6cc89":"goBike.select(haversine_udf(\"start_station_longitude\", \"start_station_latitude\", \\\n                            \"end_station_longitude\", \"end_station_latitude\").alias(\"distance\")) \\\n.agg(f.avg(\"distance\").alias(\"avg distance [km]\")).show()","77b38d97":"spark.stop()","57e124e6":"spark = SparkSession.builder.appName('etl_app').master(\"local[*]\").getOrCreate()\n\ndef load_with_schema(spark):\n\n    schema = StructType([\n        StructField(\"duration_sec\", IntegerType(), True),\n        StructField(\"start_time\", TimestampType(), True),\n        StructField(\"end_time\", TimestampType(), True),\n        StructField(\"start_station_id\", StringType(), True),\n        StructField(\"start_station_name\", StringType(), True),\n        StructField(\"start_station_latitude\", DoubleType(), True),\n        StructField(\"start_station_longitude\", DoubleType(), True),\n        StructField(\"end_station_id\", StringType(), True),\n        StructField(\"end_station_name\", StringType(), True),\n        StructField(\"end_station_latitude\", DoubleType(), True),\n        StructField(\"end_station_longitude\", DoubleType(), True),\n        StructField(\"bike_id\", IntegerType(), True),\n        StructField(\"user_type\", StringType(), True),\n        StructField(\"member_birth_year\", IntegerType(), True),\n        StructField(\"member_gender\", StringType(), True)\n    ])\n\n    df = spark \\\n        .read \\\n        .format(\"csv\") \\\n        .schema(schema)         \\\n        .option(\"header\", \"true\") \\\n        .load(\"..\/input\/ford-gobike-data\/*.csv\")\n\n    return df\n\ngoBike = load_with_schema(spark)\nprint(f\"Total Records = {goBike.count()}\")","800d3141":"dailyData = goBike.withColumn(\"date\", f.date_format(\"start_time\", \"dd.MM.yyyy\")) \\\n                  .groupBy(\"date\") \\\n                  .agg(f.avg(\"duration_sec\").alias(\"avg_duration_sec\"), \n                       f.count(\"*\").alias(\"n_trips\"), \n                       f.countDistinct(\"bike_id\").alias(\"n_bikes\"), \n                       f.sum(f.when(goBike.user_type == \"Subscriber\", 1).otherwise(0))\n                       .alias(\"n_subscriber\"))","4bd552ba":"temp = goBike.select(f.date_format(\"start_time\", \"dd.MM.yyyy\").alias(\"date\"), \n                    f.when(goBike[\"start_station_id\"] > goBike[\"end_station_id\"], \n                           f.array(goBike[\"start_station_id\"], goBike[\"end_station_id\"]))\\\n                    .otherwise(f.array(goBike[\"end_station_id\"], goBike[\"start_station_id\"])).alias(\"route\"))\\\n            .groupBy(\"date\").agg(f.countDistinct(\"route\").alias(\"n_routes\"))","85b12a7e":"dailyData = dailyData.join(temp, \"date\")\ndailyData.show()","0bb038cf":"# Option 1: save data to .parquet file\n# goBike.write.parquet('path\/to\/location\/transformed.parquet')\n\n# Option 2: save data to .csv\n# goBike.write.csv('path\/to\/location\/transformed.csv')\n\n# Option 3: save data to pandas\n# goBike.toPandas()","64175262":"spark.stop()","f300a255":">Avarege distance of a single bike trip\n># [1.59 km]()\n","b998d575":"> The longest journey\n> # [11 days]()","e1a95eab":"># [\u2191 Almost 2 mln records]()","638570fe":"Let's generate first DataFrame with some random data","4fd0b530":"That's it. If you like it, please upvote. Thank you","a4b4f127":">Rush hours during day\n># [8am & 5pm]()","928deb92":"# 5. ETL pipeline example\n\nNow based on our data let's create exemplary ETL process:\n* Extract: Load the data from source to a DataFrame with defined structure.\n* Transform: Create new DataFrame with daily data\n* Load: Save transformed frame to file or pandas\n\n<br>\n<br>\n### EXTRACT\nThis time let's define a structure of loaded csv data:","17f7792b":"> Average rental\n> # [13 minutes]()","abc5954d":"`SparkSession` class is an entry point for any Spark application. It allows you to interact with Spark API. \n<br>\n`getOrCreate()` returns a new Spark app or points to already existing one.","e564a9c0":"<br>\n## 2. SQL \/ Spark functions comparison","d473ae1b":"<br>\n### TRANSFORM\nWe will create a new DataFrame `dailyData` with aggregated data by single day. <br>\nLet's transform our data into these new columns: \n\n- 'date' : date \n- 'avg_duration_sec' : average time of rental of that day\n- 'n_trips' : total number of trips of that day\n- 'n_bikes' : total number of unique bikes rented that day\n- 'n_routes' : total number of unique trips combinations (x -> y == y -> x) of that day\n- 'n_subscriber' : total number of subsribers' rentals of that day","e09ae769":">Most popular day for a ride\n># [Tuesday]()","2292c0de":"> **ZADANIE 2**: Utw\u00f3rz DataFrame `dataDaily` zawieraj\u0105cy dane zagregowane do poziomu dnia. Zbi\u00f3r ma zawiera\u0107 nast\u0119puj\u0105ce informacje (kolumny): \n- 'date' : data \n- 'avg_duration_sec' : \u015bredni czas wypo\u017cycze\u0144 danego dnia\n- 'n_trips' : liczba wypo\u017cycze\u0144 danego dnia\n- 'n_bikes' : liczba unikatowych rower\u00f3w u\u017cytych danego dnia\n- 'n_routes' : liczba unikatowych kombinacji stacji (x -> y == y -> x) danego dnia\n- 'n_subscriber' : liczba wypo\u017cycze\u0144 dokonanych przez subskrybent\u00f3w danego dnia","e523d9de":">About 75% of users are men\n># [Men: 3x more often]()","6303109f":"> Age of the oldest cyclist in the dataset. Probably a random number entered in registry. \n> # [139 years old (\u0ca0_\u0ca0)]()","2642bb4d":"<br>\n## 4. Bike trips data exploration\n\nBike rental systems is a growing part of mobility market. I will analyze Lyft bikeshare system data from 2017 and 2018. <br>\nThe data is available at https:\/\/www.lyft.com\/bikes\/bay-wheels\/system-data \n<br>\n<br>\nWe need to merge multiple csv files to one Spark DataFrame.\n<br>\n-------------------------------------------------------------------------------------\n","93b34b2f":"<br>\n## 3. Window functions\nWindow function calculates a return value for every input row of a table based on a group of rows. ","69d7b7c7":"<br>\n### LOAD\nBe aware of high computational cost of these operations depending on how big is your DataFrame\n","28c1fefd":"# Brief introduction to PySpark\n\n<br>\n> **Contents**\n1. Spark workflow and data structure\n2. Common SQL \/ PySpark functions and keywords comparison\n3. Window functions\n4. Data exploration of Ford GoBike dataset\n5. ETL example\n\n<br>\n---------------------------------------\n## 1. Spark workflow, RDD, DataFrame\n\nSpark is a unified analytics engine for large-scale data processing. It is built on a paradigm of functional programming - operations (transformations) in the pipeline are \"lazy\", they are not executed and applyied immediatelly, they are \"delayed\" until a result is needed. \n<br>\nFundamental data structure in Spark is RDD (Resilient Distributed Dataset). RDDs are spread across many machines in the cluster. <br>\nDataFrame is based on RDD, it is a distributed collection of data organized into named columns like a table in relational database. <br>If you want to learn more, everything is covered in detail at [databricks blog](https:\/\/databricks.com\/blog\/2016\/07\/14\/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html).\n<br>\n<br>\n**RDD characteristics:**\n* immutable\n* in-memory\n* lazy evaluated\n* parallel\n* structured and unstructured data\n* two types of operations: transformations and actions\n\n**DataFrame characteristics:** \n* immutable\n* in-memory\n* resilient\n* distributed\n* parallel\n* structured\n* allows SQL\/Hive queries\n\nFirst we need to install pyspark and import all dependencies. Remember that Spark works on Java 8, so you need to install this particular distribution on your machine as well."}}