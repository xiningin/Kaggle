{"cell_type":{"e3c46129":"code","b34bec0c":"code","63cb229a":"code","3f733932":"code","a7086aa0":"code","0439cb80":"code","ff09437f":"code","fbfb5349":"code","e46131e7":"code","8c18a837":"code","006f6a7d":"code","70266868":"code","063dff10":"code","39fce0ae":"code","e2395179":"code","3ec0256c":"code","c693918e":"code","a74fd3f2":"code","e005e1bb":"code","dc958dae":"code","0baf971d":"code","033f9fcc":"code","ddc479a3":"code","0e41de0d":"code","b2c37209":"code","40917c0f":"code","6b375c70":"code","ce6c961d":"code","fe00d9c1":"code","a70a4df7":"code","e2414996":"code","66c61a6c":"code","77bc351e":"code","66b587e6":"code","43a879ef":"code","33dd748a":"code","256fe471":"code","d1bf3b64":"code","55f68984":"code","81c52229":"code","ede34e3e":"code","f4f8d61d":"code","33c0ab3c":"code","367822bf":"code","fe32c047":"code","d8c8205c":"code","3e264dc0":"code","11ad2db4":"code","a7928986":"markdown","75505d84":"markdown","4a317eb5":"markdown","e21dad31":"markdown","a17008ca":"markdown","004e2670":"markdown","e6f66791":"markdown","75c04105":"markdown","d510c79c":"markdown","d2a947ba":"markdown","663f85e4":"markdown","8b1b7018":"markdown","bb92f847":"markdown","4d7a8873":"markdown","deb95621":"markdown","3e52fde4":"markdown","e44849a3":"markdown","55a73f2d":"markdown","658b5ec1":"markdown","2dfb6049":"markdown","e6aaf3b0":"markdown","b05a32ad":"markdown","7d8d27f6":"markdown","1c8af74c":"markdown","cbfadf1f":"markdown","e797d3b4":"markdown","a5b7e6ee":"markdown","6e92a314":"markdown","a01b2f2c":"markdown","f2c0287e":"markdown","c637f297":"markdown","df78ae1a":"markdown","f4cfa15d":"markdown","0089cf52":"markdown","67873fca":"markdown","13ad98c6":"markdown","7dc3b44b":"markdown","77999ccb":"markdown"},"source":{"e3c46129":"print('>> Installing Libraries')\n\n!pip3 install pandas matplotlib numpy scikit-surprise\n\nprint('>> Libraries Installed')","b34bec0c":"print('>> Importing Libraries')\n\nimport pandas as pd\n\nfrom surprise import Reader, Dataset, SVD\n\nfrom surprise.accuracy import rmse, mae\nfrom surprise.model_selection import cross_validate\n\nprint('>> Libraries imported.')","63cb229a":"df = pd.read_csv('\/kaggle\/input\/movielensratings\/ratings.csv')\ndf.head()","3f733932":"df.drop('timestamp', axis=1, inplace=True)\ndf.head()","a7086aa0":"df.isna().sum()","0439cb80":"n_movies = df['movieId'].nunique()\nn_users = df['userId'].nunique()\nprint(f'Number of unique movies: {n_movies}')\nprint(f'Number of unique users: {n_users}')","ff09437f":"available_ratings = df['rating'].count()\ntotal_ratings = n_movies*n_users\nmissing_ratings = total_ratings - available_ratings\nsparsity = (missing_ratings\/total_ratings) * 100\nprint(f'Sparsity: {sparsity}')","fbfb5349":"df['rating'].value_counts().plot(kind='bar')","e46131e7":"filter_movies = df['movieId'].value_counts() > 3\nfilter_movies = filter_movies[filter_movies].index.tolist()","8c18a837":"filter_users = df['userId'].value_counts() > 3\nfilter_users = filter_users[filter_users].index.tolist()","006f6a7d":"print(f'Original shape: {df.shape}')\ndf = df[(df['movieId'].isin(filter_movies)) & (df['userId'].isin(filter_users))]\nprint(f'New shape: {df.shape}')","70266868":"cols = ['userId', 'movieId', 'rating']","063dff10":"reader = Reader(rating_scale = (0.5, 5))\ndata = Dataset.load_from_df(df[cols], reader)","39fce0ae":"trainset = data.build_full_trainset()\nantiset = trainset.build_anti_testset()","e2395179":"algo = SVD(n_epochs =25, verbose = True)","3ec0256c":"cross_validate(algo, data, measures = ['RMSE', 'MAE'], cv=5, verbose= True)\nprint('>> Training Done')","c693918e":"predictions = algo.test(antiset)","a74fd3f2":"predictions[0]","e005e1bb":"from collections import defaultdict\ndef get_top_n(predictions, n=3):\n    top_n = defaultdict(list)\n    for uid, iid, _, est, _ in predictions:\n        top_n[uid].append((iid, est))\n        \n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key = lambda x: x[1], reverse = True)\n        top_n[uid] = user_ratings[:n]\n        \n    return top_n\n\ntop_n = get_top_n(predictions, n=3)\n\nfor uid, user_ratings in top_n.items():\n    print(uid, [iid for (iid, rating) in user_ratings])","dc958dae":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import WordPunctTokenizer","0baf971d":"import warnings\nwarnings.filterwarnings('ignore')","033f9fcc":"df = pd.read_csv('\/kaggle\/input\/yelpdata\/yelp_review_arizona.csv')\ndf_business = pd.read_csv('\/kaggle\/input\/yelpdata\/yelp_business.csv')","ddc479a3":"df.head()","0e41de0d":"df_business.head()","b2c37209":"#Select only stars and text\nyelp_data = df[['business_id', 'user_id', 'stars', 'text']]","40917c0f":"import string\nfrom nltk.corpus import stopwords\nstop = []\nfor word in stopwords.words('english'):\n    s = [char for char in word if char not in string.punctuation]\n    stop.append(''.join(s))","6b375c70":"def text_process(mess):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Returns a list of the cleaned text\n    \"\"\"\n    # Check characters to see if they are in punctuation\n    nopunc = [char for char in mess if char not in string.punctuation]\n\n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)\n    \n    # Now just remove any stopwords\n    return \" \".join([word for word in nopunc.split() if word.lower() not in stop])","ce6c961d":"yelp_data['text'] = yelp_data['text'].apply(text_process)","fe00d9c1":"#Split train test for testing the model later\nvld_size=0.15\nX_train, X_valid, y_train, y_valid = train_test_split(yelp_data['text'], df['business_id'], test_size = vld_size)","a70a4df7":"userid_df = yelp_data[['user_id','text']]\nbusiness_df = yelp_data[['business_id', 'text']]","e2414996":"userid_df[userid_df['user_id']=='ZwVz20be-hOZnyAbevyMyQ']['text']","66c61a6c":"userid_df = userid_df.groupby('user_id').agg({'text': ' '.join})\nbusiness_df = business_df.groupby('business_id').agg({'text': ' '.join})","77bc351e":"userid_df.head()","66b587e6":"userid_df.loc['ZwVz20be-hOZnyAbevyMyQ']['text']","43a879ef":"\nfrom sklearn.feature_extraction.text import TfidfVectorizer","33dd748a":"#userid vectorizer\nuserid_vectorizer = TfidfVectorizer(tokenizer = WordPunctTokenizer().tokenize, max_features=5000)\nuserid_vectors = userid_vectorizer.fit_transform(userid_df['text'])\n\n#Business id vectorizer\nbusinessid_vectorizer = TfidfVectorizer(tokenizer = WordPunctTokenizer().tokenize, max_features=5000)\nbusinessid_vectors = businessid_vectorizer.fit_transform(business_df['text'])","256fe471":"userid_rating_matrix = pd.pivot_table(yelp_data, values='stars', index=['user_id'], columns=['business_id'])\nuserid_rating_matrix.head()","d1bf3b64":"P = pd.DataFrame(userid_vectors.toarray(), index=userid_df.index, columns=userid_vectorizer.get_feature_names())\nQ = pd.DataFrame(businessid_vectors.toarray(), index=business_df.index, columns=businessid_vectorizer.get_feature_names())","55f68984":"P.head()","81c52229":"Q.head()","ede34e3e":"def matrix_factorization(R, P, Q, steps=25, gamma=0.001,lamda=0.02):\n    for step in range(steps):\n        for i in R.index:\n            for j in R.columns:\n                if R.loc[i,j]>0:\n                    eij=R.loc[i,j]-np.dot(P.loc[i],Q.loc[j])\n                    P.loc[i]=P.loc[i]+gamma*(eij*Q.loc[j]-lamda*P.loc[i])\n                    Q.loc[j]=Q.loc[j]+gamma*(eij*P.loc[i]-lamda*Q.loc[j])\n        e=0\n        for i in R.index:\n            for j in R.columns:\n                if R.loc[i,j]>0:\n                    e= e + pow(R.loc[i,j]-np.dot(P.loc[i],Q.loc[j]),2)+lamda*(pow(np.linalg.norm(P.loc[i]),2)+pow(np.linalg.norm(Q.loc[j]),2))\n        if e<0.001:\n            break\n        \n    return P,Q","f4f8d61d":"# %%time\n# P, Q = matrix_factorization(userid_rating_matrix, P, Q, steps=25, gamma=0.001,lamda=0.02) ## Only when you train on new examples either load from pickle","33c0ab3c":"# Store P, Q and vectorizer in pickle file\n# import pickle\n# output = open('yelp_recommendation_model_8.pkl', 'wb')\n# pickle.dump(P,output)\n# pickle.dump(Q,output)\n# pickle.dump(userid_vectorizer,output)\n# output.close()\n\n# Use when only you train on new data, then export","367822bf":"import pickle\ninput = open('..\/input\/yelpdata\/yelp_recommendation_model_5.pkl','rb')\nP = pickle.load(input)\nQ = pickle.load(input)\nuserid_vectorizer = pickle.load(input)\ninput.close()","fe32c047":"P.head()","d8c8205c":"Q.head()","3e264dc0":"Q.iloc[0].sort_values(ascending=False).head(10)","11ad2db4":"words = \"i want to have dinner with beautiful views\"\ntest_df= pd.DataFrame([words], columns=['text'])\ntest_df['text'] = test_df['text'].apply(text_process)\ntest_vectors = userid_vectorizer.transform(test_df['text'])\ntest_v_df = pd.DataFrame(test_vectors.toarray(), index=test_df.index, columns=userid_vectorizer.get_feature_names())\n\npredictItemRating=pd.DataFrame(np.dot(test_v_df.loc[0],Q.T),index=Q.index,columns=['Rating'])\ntopRecommendations=pd.DataFrame.sort_values(predictItemRating,['Rating'],ascending=[0])[:7]\n\nfor i in topRecommendations.index:\n    print(df_business[df_business['business_id']==i]['name'].iloc[0])\n    print(df_business[df_business['business_id']==i]['categories'].iloc[0])\n    print(str(df_business[df_business['business_id']==i]['stars'].iloc[0])+ ' '+str(df_business[df_business['business_id']==i]['review_count'].iloc[0]))\n    print('')","a7928986":"## 5.2 Create surprise dataset","75505d84":"## Select only stars and text","4a317eb5":"## 4.2 Filter users with less than 3 movies rated","e21dad31":"# Task 4: Dimensionality Reduction\n\nTo reduce the dimensionality of the dataset, we will filter out rarely rated movies and rarely rating users","a17008ca":"## 6.2 Training the model\n\n**Mean Absolute Error (MAE)**: MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. \n\n**Root mean squared error (RMSE)**:  RMSE is the square root of the average of squared differences between prediction and actual observation.","004e2670":"# Import Model\n\nImporting model is necessary when you are not running new training ","e6f66791":"## 1.2: Importing Libraries\n\nFirst of all, we will need to import some libraries. This includes surprise which we will use to create the recommendation system.","75c04105":"## 6.1 Creating the model\n\n**SVD (Singular Value Decomposition)**\n\nInteraction Matrix = A X B X C","d510c79c":"# Task 5: Create Training and Test Sets","d2a947ba":"## Clean Text","663f85e4":"## 7.2 Recommending top 3 movies movies based on predictions","8b1b7018":"## User Tfidf Vectorizer with 5000 Features (represent 88% of all words)\n","bb92f847":"# Sanity test","4d7a8873":"## 5.1 Columns used for training","deb95621":"# Task 1: Introduction\n\n![Movies Collage](https:\/\/i.imgur.com\/T7V0VZ6.png)\n\nFor this project we are going to create a recommendation engine for movies for users based on there past behaviour.\n\n---\n\nWe will focus on the **collaborative filtering** approach, that is: \n\nThe user is recommended items that people with similar tastes and preferences liked in the past. In another word, this method predicts unknown ratings by using the similarities between users.\n\n---\n\nNote: This notebook uses `python 3` and these packages: `pandas`, `numpy`, `matplotlib` and `scikit-surprise`\n\nWe can install them using:\n```Shell\npip3 install pandas matplotlib numpy scikit-surprise\n```","3e52fde4":"## Join the text with for each user_id and business_id","e44849a3":"## Matrix Factorization","55a73f2d":"## 1.1: Installing Libraries","658b5ec1":"## 2.2 Dropping timestamp\n\nWe won't be using the timestamp when user gave the particular rating. So we will drop that column.","2dfb6049":"## 4.1 Filter movies with less than 3 ratings","e6aaf3b0":"## 3.2 Sparsity of our data\n\nSparsity (%) = `(No of missing values\/ (Total Values))X100`","b05a32ad":"# Task 7: Predictions","7d8d27f6":"## 3.3 Ratings Distribution","1c8af74c":"## 5.3 Create Train-set and Prediction-set","cbfadf1f":"## 2.3 Check for Missing Data\n\nIt's a good practice to check if the data has any missing values. In real world data, this is quite common and must be taken care of before any data pre-processing or model training.","e797d3b4":"## 2.1: Importing the Data\n\nThe dataset is saved in a `ratings.csv` file. We will use pandas to take a look at some of the rows.","a5b7e6ee":"## Prediction for input text","6e92a314":"# Task 2: Importing Data\n\nWe will use open-source dataset from GroupLens Research ([movielens.org](http:\/\/movielens.org))","a01b2f2c":"## 7.1 Predict ratings for all pairs (user, items) that are NOT in the training set.","f2c0287e":"# Task 3: EDA (Exploratory data analysis)\n\nIn statistics, exploratory data analysis is an approach to analyzing data sets to summarize their main characteristics.","c637f297":"## Gradient Decent Optimization","df78ae1a":"\n## Export Model","f4cfa15d":"## 3.1 Number of movies\/users","0089cf52":"# Restaurant Recommendation System Using Latent Factor Collaborative Filtering\n\nLet\u2019s start building a Restaurant Recommendation System using the techniques discussed above which should be capable of recommending restaurants that best suits you","67873fca":"## Create two tables of user,text and business,text","13ad98c6":"## 4.3 Remove rarely rated movies and rarely rating users","7dc3b44b":"# Movie Recommendation System using Collaborative Filtering","77999ccb":"# Task 6: Creating and training the model"}}