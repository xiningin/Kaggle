{"cell_type":{"f1cffba5":"code","c8800f0b":"code","032de82f":"code","dd84a103":"code","589fdc43":"code","beda67b4":"code","9c533540":"code","f72afc3f":"code","32cd8c18":"code","2bed57b4":"code","def53f8e":"code","4daac525":"code","531764dd":"code","8b02c41c":"code","7bcea516":"code","d1ea8939":"code","808cc8ea":"code","29c96453":"code","a850ea57":"code","b9487ba3":"code","46f9d3cd":"code","8e838aa5":"code","33834652":"code","8d7401ce":"code","10b72d19":"code","fcbcdd89":"code","77042b61":"code","50883773":"code","3ea2b6cd":"code","2df65225":"code","f2645db6":"code","fcb87345":"code","a8301e2c":"markdown","42090df4":"markdown","e2ddd0fa":"markdown","81399128":"markdown","0314681c":"markdown"},"source":{"f1cffba5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c8800f0b":"import pandas as pd \nimport numpy as np \nimport seaborn as sns \nfrom matplotlib import pyplot as plt \nfrom transformers import RobertaModel,RobertaTokenizer,RobertaConfig,AdamW,get_linear_schedule_with_warmup\nfrom tokenizers import ByteLevelBPETokenizer\nimport torch \nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch import nn,optim \nimport os \nimport time \nimport random \nfrom collections import defaultdict\nfrom tqdm import tqdm\nimport re\nimport math\nimport pickle\nfrom sklearn.model_selection import StratifiedKFold\nimport gc \ngc.enable()","032de82f":"train=pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ntest=pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\nsubmission=pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')","dd84a103":"train['length']=train.text.astype(str).apply(lambda x:len(x))\ntrain['num_words']=train.text.astype(str).apply(lambda x:len(x.split()))","589fdc43":"train.describe()","beda67b4":"train","9c533540":"train.isnull().sum()","f72afc3f":"train=train.dropna()\ntrain=train.reset_index(drop=True)","32cd8c18":"train","2bed57b4":"class Config:\n    MAX_LENGTH=256\n    BATCH_SIZE=32\n    EPOCHS=5\n    TOKENIZER=ByteLevelBPETokenizer(\n      vocab='..\/input\/roberta-base\/vocab.json',\n      merges='..\/input\/roberta-base\/merges.txt',\n      add_prefix_space=True,\n      lowercase=True\n    )\n    TOKENIZER.enable_truncation(max_length=MAX_LENGTH)\n    VOCAB=TOKENIZER.get_vocab()\n    INT_TO_WORD={value:key for key,value in VOCAB.items()}\n    CLS_ID=VOCAB['<s>']\n    SEP_ID=VOCAB['<\/s>']\n    PAD_ID=VOCAB['<pad>']","def53f8e":"temp=Config.TOKENIZER.encode(' hello, my name is khanh')","4daac525":"temp.offsets","531764dd":"def get_data(tweet,sentiment,config):\n    tweet=\" \"+\" \".join(tweet.lower().split())\n    token=config.TOKENIZER.encode(tweet)\n    sentiment_values={\n      value:config.VOCAB[value] for value in ['positive','negative','neutral']\n    }\n    input_ids=[config.CLS_ID]+[sentiment_values[sentiment]]+[config.SEP_ID]+token.ids+[config.SEP_ID]\n    attention_mask=[1]*len(input_ids)\n    offsets=[(0,0)]*3+token.offsets+[(0,0)]\n    #padding_len=Config.MAX_LENGTH-len(input_ids)\n    # if padding_len>0:\n    #     input_ids=input_ids+[Config.PAD_ID]*padding_len\n    #     attention_mask=attention_mask+[0]*padding_len\n    #     offsets=offsets+[(0,0)]*padding_len\n    return tweet,input_ids,attention_mask,offsets\n","8b02c41c":"def find_index(tweet,selected_text,offsets,config):\n    selected_text=\" \"+\" \".join(selected_text.lower().split())\n    index1,index2=None,None\n    length=len(selected_text)-1\n    for value in [position for position,value in enumerate(tweet) if value==selected_text[1]]:\n        if \" \"+tweet[value:value+length]==selected_text:\n            index1=value\n            index2=value+length-1\n    temp=[0]*len(tweet)\n    start_index,end_index=None,None\n    #print(len(temp),'--->',index1,'---->',index2)\n    if index1!=None and index2!=None:\n        #print(index1,'-->',index2)\n        for i in range(index1,index2+1):\n            temp[i]=1\n        list_index=[]\n        for i,(offset1,offset2) in enumerate(offsets):\n            if sum(temp[offset1:offset2])>0:\n                list_index.append(i)\n        start_index=list_index[0]\n        end_index=list_index[-1]\n    return start_index,end_index\n","7bcea516":"start_indexs,end_indexs=[],[]\nfor i in range(len(train)):\n    tweet_temp=train['text'][i]\n    sentiment=train['sentiment'][i]\n    selected_temp=train['selected_text'][i]\n    tweet_temp,_,_,offsets=get_data(tweet_temp,sentiment,Config)\n    start_index,end_index=find_index(tweet_temp,selected_temp,offsets,Config)\n    if start_index is None:\n        print(tweet_temp,'------>',selected_temp)\n    start_indexs.append(start_index)\n    end_indexs.append(end_index)\n\ntrain['start_index']=start_indexs\ntrain['end_index']=end_indexs","d1ea8939":"train","808cc8ea":"class TweetSentimentExtraction(Dataset):\n    def __init__(self,data,config):\n        self.data=data\n        self.tokenizer=config.TOKENIZER\n        self.max_length=config.MAX_LENGTH\n        self.is_test=\"selected_text\" in self.data\n        self.config=config\n        self.cls_id=self.config.VOCAB['<s>']\n        self.sep_id=self.config.VOCAB['<\/s>']\n        self.pad_id=self.config.VOCAB['<pad>']\n\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self,idx):\n        row=self.data.iloc[idx]\n        tweet=row.text\n        sentiment=row.sentiment\n        tweet,input_ids,attention_mask,offsets=get_data(tweet,sentiment,self.config)\n        data={}\n        data['input_ids']=torch.tensor(input_ids,dtype=torch.long)\n        data['attention_mask']=torch.tensor(attention_mask,dtype=torch.long)\n        data['tweet']=tweet\n        data['offsets']=offsets\n        if self.is_test:\n            start_index=row.start_index\n            end_index=row.end_index\n            data['start_index']=start_index\n            data['end_index']=end_index\n        return data\n    \n\n\nclass MyCollate:\n    def __init__(self,pad_id,is_test=False):\n        self.pad_id=pad_id\n        self.is_test=is_test\n        \n    def __call__(self,batch):\n        input_ids=[item['input_ids'] for item in batch]\n        attention_mask=[item['attention_mask'] for item in batch]\n        tweet=[item['tweet'] for item in batch]\n        offsets=[item['offsets'] for item in batch]\n        input_ids=pad_sequence(input_ids,batch_first=True,padding_value=self.pad_id)\n        attention_mask=pad_sequence(attention_mask,batch_first=True,padding_value=0)\n        if len(offsets)<input_ids.size(1):\n            padding_len=input_ids.size(1)-len(offsets)\n            offsets=offsets+[(0,0)]*padding_len\n\n        if not self.is_test:\n            #print(start_index)\n            start_index=[item['start_index'] for item in batch]\n            end_index=[item['end_index'] for item in batch]\n            start_index=torch.tensor(start_index,dtype=torch.long)\n            end_index=torch.tensor(end_index,dtype=torch.long)\n            return {\n                \"input_ids\":input_ids,\n                \"attention_mask\":attention_mask,\n                \"offsets\":offsets,\n                'tweet':tweet,\n                \"start_index\":start_index,\n                \"end_index\":end_index\n            }\n        else:\n            return {\n                \"input_ids\":input_ids,\n                \"attention_mask\":attention_mask,\n                \"offsets\":offsets,\n                'tweet':tweet\n            }\n        ","29c96453":"def get_train_val_loader(df,train_index,val_index):\n    train=df.iloc[train_index]\n    val=df.iloc[val_index]\n    train_dataset=TweetSentimentExtraction(train,Config)\n    val_dataset=TweetSentimentExtraction(val,Config)\n    train_loader=DataLoader(train_dataset,batch_size=Config.BATCH_SIZE,shuffle=False,num_workers=2,collate_fn=MyCollate(Config.PAD_ID))\n    val_loader=DataLoader(val_dataset,batch_size=Config.BATCH_SIZE,shuffle=False,num_workers=2,collate_fn=MyCollate(Config.PAD_ID))\n    return train_loader,val_loader","a850ea57":"device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","b9487ba3":"class Model(nn.Module):\n    def __init__(self):\n        super(Model,self).__init__()\n        self.config=RobertaConfig.from_pretrained('..\/input\/roberta-base\/config.json',output_hidden_states=True)\n        self.bert=RobertaModel.from_pretrained('..\/input\/roberta-base',config=self.config)\n        self.hidden_size=self.bert.config.hidden_size\n        self.norm=nn.LayerNorm(self.hidden_size)\n        self.linear=nn.Sequential(\n            nn.Linear(self.hidden_size,self.hidden_size),\n            nn.LayerNorm(self.hidden_size),\n            nn.ReLU(),\n            nn.Linear(self.hidden_size,2)\n        )\n    \n    def __init__weight(self,module):\n        if isinstance(module,nn.Linear):\n            module.weight.data.normal_(mean=0,std=seld.bert.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module,nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n    \n    def forward(self,input_ids,attention_mask,token_type_ids=None):\n        outputs=self.bert(input_ids,attention_mask)\n        hidden_states=outputs.hidden_states#batch_size*seq_length*hidden_size\n        out=torch.stack([hidden_states[-1],hidden_states[-2],hidden_states[-3],hidden_states[-4]])\n        out=torch.mean(out,0)\n        out=self.linear(out)\n        start_logit,end_logit=torch.split(out,1,-1)\n        start_logit=start_logit.squeeze(dim=-1)\n        end_logit=end_logit.squeeze(dim=-1)\n        return start_logit,end_logit\n        ","46f9d3cd":"def get_selected_text(text,start_index,end_index,offsets):\n    selected_text=\"\"\n    for i in range(start_index,end_index+1):\n        selected_text+=text[offsets[i][0]:offsets[i][1]]\n#         if (i+1)<len(offsets) and offsets[i][0]<offsets[i+1][0]:\n#             selected_text+=\" \"\n    return selected_text","8e838aa5":"def get_loss(start_targets,end_targets,start_logit,end_logit):\n    loss_fn=nn.CrossEntropyLoss(reduction=\"mean\")\n    loss1=loss_fn(start_logit,start_targets)\n    loss2=loss_fn(end_logit,end_targets)\n    loss=loss1+loss2\n    return loss\n\ndef jaccard_score(str1,str2):\n    a=set(str1.lower().split())\n    b=set(str2.lower().split())\n    c=a.intersection(b)\n    return float(len(c))\/(len(a)+len(b)-len(c))\n\ndef computer_jaccard_score(text,start_index,end_index,start_logit,end_logit,offsets):\n    start_pred=np.argmax(start_logit)\n    end_pred=np.argmax(end_logit)\n    if start_pred>end_pred:\n        pred=text\n    else:\n        pred=get_selected_text(text,start_pred,end_pred,offsets)\n    true=get_selected_text(text,start_index,end_index,offsets)\n    return jaccard_score(true,pred)\n\ndef save_checkpoint(model_state_dict,fold):\n    #path=\"\/content\/drive\/MyDrive\/Model\/TweetSentimentExtraction\"\n#     if os.path.exists(path) is False:\n#         os.makedirs(path,exist_ok=True)\n        \n    # with open(path+f'\/history_{epoch}_fold{fold}.pickle','wb') as file:\n    #     pickle.dump(history,file,protocol=pickle.HIGHEST_PROTOCOL)\n    # print(\"Save history done\")\n    torch.save(model_state_dict,f\"\/model_fold{fold}.pth\")\n    print(\"Save model done\")","33834652":"def evaluate(model,loader,len_val):\n    print('---------------------------TIME FOR EVALUATE---------------------------')\n    model.eval()\n    val_loss=0\n    score=0\n    with torch.no_grad():\n        for idx,data in enumerate(loader):\n            input_ids=data['input_ids'].to(device)\n            attention_mask=data['attention_mask'].to(device)\n            offsets=data['offsets']\n            tweets=data['tweet']\n            start_logit,end_logit=model(input_ids,attention_mask)\n            loss=get_loss(\n                data['start_index'].to(device),\n                data['end_index'].to(device),\n                start_logit,\n                end_logit\n            )\n            val_loss+=loss.item()\n            start_indexs=data['start_index'].cpu().detach().numpy()\n            end_indexs=data['end_index'].cpu().detach().numpy()\n            start_logit=start_logit.cpu().detach().numpy()\n            end_logit=end_logit.cpu().detach().numpy()\n            for i in range(len(input_ids)):\n                score+=computer_jaccard_score(\n                    tweets[i],\n                    start_indexs[i],\n                    end_indexs[i],\n                    start_logit[i],\n                    end_logit[i],\n                    offsets[i]\n                )\n    return val_loss\/len(loader),score\/len_val","8d7401ce":"def train_model(model,train_loader,val_loader,optimizer,scheduler,fold,len_val):\n    model.train()\n    train_loss=0\n    history=defaultdict(list)\n    jaccard_score_final=None\n    print(f\"----------------------------------FOLD {fold}----------------------------------\\n\\n\")\n    for epoch in range(Config.EPOCHS+1):\n        train_loss=0\n        start_time=time.time()\n        print('---------------------------TIME FOR TRANING---------------------------')\n        for idx,data in enumerate(train_loader):\n            input_ids=data['input_ids'].to(device)\n            attention_mask=data['attention_mask'].to(device)\n            start_logit,end_logit=model(input_ids,attention_mask)\n            optimizer.zero_grad()\n            loss=get_loss(\n                data['start_index'].to(device),\n                data['end_index'].to(device),\n                start_logit,\n                end_logit\n            )\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            train_loss+=loss.item()\n            if idx%100==0:\n                print(idx,end=\" \")\n        print()\n        train_loss\/=len(train_loader)\n        val_loss,score=evaluate(model,val_loader,len_val)\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['jaccard_score'].append(score)\n        print(f\"Epochs:{epoch}---Train loss:{train_loss}---Val loss:{val_loss}---Jaccard score val:{score}---Time:{time.time()-start_time}\")\n        if jaccard_score_final is None or score>jaccard_score_final:\n            model_state=model.state_dict()\n            jaccard_score_final=score\n    \n    save_checkpoint(model_state,fold)             \n    print('\\n\\n')\n","10b72d19":"# kfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=43)\n\n# for fold,(train_indexs,val_indexs) in enumerate(kfold.split(train,train.sentiment),start=1):\n#     model=Model().to(device)\n#     optimizer=AdamW(model.parameters(),lr=1e-5)\n#     scheduler=get_linear_schedule_with_warmup(\n#         optimizer,\n#         num_warmup_steps=50,\n#         num_training_steps=len(train)\/\/Config.BATCH_SIZE*Config.EPOCHS    \n#     )\n\n#     train_loader,val_loader=get_train_val_loader(train,train_indexs,val_indexs)\n#     train_model(model,train_loader,val_loader,optimizer,scheduler,fold,len(val_indexs))\n#     del model,optimizer,scheduler\n#     torch.cuda.empty_cache()\n#     gc.collect()\n  ","fcbcdd89":"test_dataset=TweetSentimentExtraction(test,Config)\ntest_loader=DataLoader(\n                        test_dataset,batch_size=Config.BATCH_SIZE,\n                        shuffle=False,num_workers=2,\n                        collate_fn=MyCollate(Config.PAD_ID,is_test=True)\n                    )","77042b61":"models=[]\nfor file_model in os.listdir('..\/input\/tweetsentimentextractionroberta'):\n    model_temp=Model().to(device)\n    model_temp.load_state_dict(torch.load('..\/input\/tweetsentimentextractionroberta\/'+file_model))\n    model_temp.eval()\n    models.append(model_temp)","50883773":"def get_predict(input_ids,attention_mask,size_batch):\n    start_preds=torch.tensor([0]*size_batch,dtype=torch.long).to(device)\n    end_preds=torch.tensor([0]*size_batch,dtype=torch.long).to(device)\n    for i in range(len(models)):\n        model=models[i]\n        start_logit,end_logit=model(input_ids,attention_mask)\n        start_index=torch.argmax(start_logit,dim=1)\n        end_index=torch.argmax(end_logit,dim=1)\n        start_preds+=start_index\n        end_preds+=end_index\n    start_preds=start_preds.cpu().detach().numpy()\n    end_preds=end_preds.cpu().detach().numpy()\n    return (start_preds\/len(models)).astype(int),(end_preds\/len(models)).astype(int)\n        \n        ","3ea2b6cd":"preds=[]\nfor data in test_loader:\n    input_ids=data['input_ids'].to(device)\n    attention_mask=data['attention_mask'].to(device)\n    offsets=data['offsets']\n    tweets=data['tweet']\n    start_indexs,end_indexs=get_predict(input_ids,attention_mask,len(tweets))\n    for i in range(len(tweets)):\n        if start_indexs[i]>end_indexs[i]:\n            result=tweets[i]\n        else:\n            result=get_selected_text(tweets[i],start_indexs[i],end_indexs[i],offsets[i])\n        preds.append(result)","2df65225":"# preds=[]\n# for data in test_loader:\n#     input_ids=data['input_ids'].to(device)\n#     attention_mask=data['attention_mask'].to(device)\n#     tweet=data['tweet']\n#     offsets=data['offsets']\n#     start_logits=[]\n#     end_logits=[]\n#     for model in models:\n#         with torch.no_grad():\n#             outputs=model(input_ids,attention_mask)\n#             start_logits.append(torch.softmax(outputs[0],dim=1).cpu().detach().numpy())\n#             end_logits.append(torch.softmax(outputs[1],dim=1).cpu().detach().numpy())\n#     start_logits=np.mean(start_logits,axis=0)\n#     end_logits=np.mean(end_logits,axis=0)\n#     start_indexs=np.argmax(start_logits,axis=1)\n#     end_indexs=np.argmax(end_logits,axis=1)\n#     for i in range(len(tweet)):\n#         start_pred=start_indexs[i]\n#         end_pred=end_indexs[i]\n#         if start_pred>end_pred:\n#             result=tweet[i]\n#         else:\n#             result=get_selected_text(tweet[i],start_pred,end_pred,offsets[i])\n#         preds.append(result)","f2645db6":"def replace_string(x):\n    x=re.sub('!+','!',x)\n    x=re.sub('.+','.',x)\n    return x","fcb87345":"submission.selected_text=preds\n# submission['selected_text'] = submission['selected_text'].apply(lambda x: replace_string(x))\nsubmission.to_csv('submission.csv',index=False)\nsubmission.head(10)","a8301e2c":"# Model","42090df4":"# Preprocess Data","e2ddd0fa":"# Load data","81399128":"# Data Loader","0314681c":"We have 1 row which has nan value"}}