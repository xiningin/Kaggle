{"cell_type":{"5ac1dc19":"code","60dc77bd":"code","5e38a99b":"code","59d4fee5":"code","1606b6bd":"code","36fedc16":"code","2a478ded":"code","24fe9097":"code","b7792dec":"code","6a74f8b0":"code","f174ce3a":"code","533dba3a":"code","44137e74":"code","a4ba1f2f":"code","1d1e9246":"code","87d5690d":"code","11a81259":"code","aab9e7fd":"code","f690c2a3":"code","47ea20a8":"code","6d44604f":"code","093748d4":"code","4caa7819":"code","a7aa64ee":"code","b6e9431c":"code","2c6ec7fc":"code","03f8cbf6":"code","0b2e5287":"code","1da3d5be":"code","325982a9":"code","0b9f6c8c":"markdown","8af77d79":"markdown","c60bc320":"markdown","31654856":"markdown","064cfe8a":"markdown","b76dedf3":"markdown","b6597f38":"markdown","89a87dd8":"markdown","610b441b":"markdown","b4371450":"markdown","e4f80cb4":"markdown","7a640ed0":"markdown","f7189b39":"markdown","114d19c6":"markdown","046b73f5":"markdown","475a004b":"markdown","4eba0bd3":"markdown","23fc7bd5":"markdown","6b4e8e1e":"markdown","4da637d1":"markdown","41be5fc5":"markdown","70755c7d":"markdown","aa5f6b6b":"markdown","6204e591":"markdown","de321d03":"markdown","7ac45960":"markdown","45cf85a9":"markdown"},"source":{"5ac1dc19":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers\n\nprint(tf.__version__)","60dc77bd":"#### Downloading the Dataset\ndataset_path = keras.utils.get_file(\"auto-mpg.data\", \n                                    \"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/auto-mpg\/auto-mpg.data\")\n\n#### Loading dataset using Pandas\ncolumn_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n                'Acceleration', 'Model Year', 'Origin'] \ndataset = pd.read_csv(dataset_path, names=column_names,\n                      na_values = \"?\", comment='\\t',\n                      sep=\" \", skipinitialspace=True)\n\n#### Let's have a look at data\ndataset.head()","5e38a99b":"#### Checking for duplicated data\ndataset.duplicated().sum()","59d4fee5":"#### checking dimensionality of the dataset\nprint(f'The dataset contains {dataset.shape[0]} rows and {dataset.shape[1]} columns')","1606b6bd":"#### Checking for datatype of each columns\ndataset.dtypes","36fedc16":"#### Checking for null values\ndataset.isnull().sum().plot(kind='bar')\nplt.title('Columns with missing values',  fontweight=\"bold\")\nplt.ylabel('Count of Null Values')\nplt.show()","2a478ded":"#### Visualizing 'Horsepower' feature for outliers and skewnwss\nplt.figure(figsize=(15, 4))\n\nplt.subplot(1, 2, 1)\nsns.boxplot(x = dataset[\"Horsepower\"])\nplt.title(\"Boxplot for outliers detection\", fontweight=\"bold\")\n\nplt.subplot(1, 2, 2)\nsns.distplot(dataset[\"Horsepower\"])\nplt.title(\"Distribution plot for skewness\", fontweight=\"bold\")\n\nplt.show()","24fe9097":"#### Filling null values in 'Horsepower' column with median\ndataset[\"Horsepower\"].fillna(dataset[\"Horsepower\"].median(), inplace = True)","b7792dec":"#### visualizing our dependent variable for outliers and skewnwss\nplt.figure(figsize=(15, 4))\n\nplt.subplot(1, 2, 1)\nsns.boxplot(dataset[\"MPG\"])\nplt.title(\"Boxplot for outliers detection\", fontweight=\"bold\")\n\nplt.subplot(1, 2, 2)\nsns.distplot(dataset[\"MPG\"])\nplt.title(\"Distribution plot for skewness\", fontweight=\"bold\")\n\nplt.show()","6a74f8b0":"#### Count plot for all discrete features\ndiscrete_columns_list = ['Cylinders', 'Model Year', 'Origin']\nplt.figure(figsize = (15, 3))\nfor i in range(len(discrete_columns_list)):\n    plt.subplot(1, 3, i+1)\n    sns.countplot(dataset[discrete_columns_list[i]], \n                  order = dataset[discrete_columns_list[i]].value_counts().index, \n                  palette='Blues_r')\n    plt.title(discrete_columns_list[i], fontweight=\"bold\")\n    plt.xlabel('')","f174ce3a":"#### Converting categorical variable in origin to actual values\ndataset['Origin'] = dataset['Origin'].replace({1: 'USA', 2: 'Germany', 3: 'Japan'})\n\n#### Pie plot for origin\ndataset.groupby('Origin').size().plot(kind='pie', autopct='%.2f%%', ylabel = '')\nplt.show()","533dba3a":"#### Applying get_dummies to 'origin' column\ndataset = pd.get_dummies(dataset, columns=['Origin'])","44137e74":"#### Joint Distribution of a few pairs of columns\nsns.pairplot(dataset[['MPG', 'Displacement', 'Horsepower', 'Weight', 'Acceleration']],\n             diag_kind=\"kde\")\nplt.show()","a4ba1f2f":"#### heatmap to visualize the pearson's correlation matrix between the numeric variables\nplt.figure(figsize=(8, 5))\nsns.heatmap(dataset.corr(), annot=True, cmap=\"YlGnBu\", mask=np.triu(dataset.corr(), k=1))\nplt.show()","1d1e9246":"#### The describe() method shows a summary of the nummerical attributes.\ndataset.describe().T.round(decimals = 2)","87d5690d":"#### Plotting box plot to check the outliers\nplt.figure(figsize=(15, 7))\ncolumns = ['MPG', 'Displacement', 'Horsepower', 'Weight', 'Acceleration']\nfor i in range(5):\n    plt.subplot(2, 5, i+1)\n    sns.boxplot(dataset[columns[i]], orient=\"v\")\n    \nplt.tight_layout()","11a81259":"#### Converting data into dependent and independent variable \nX = dataset.drop('MPG',axis=1)\ny = dataset['MPG']","aab9e7fd":"#### Applying z-score normalization on dataset\ncnames=['Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Model Year']\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nfor col in cnames:\n    X[col] = sc.fit_transform(X[col].values.reshape(-1,1))\nX.head()","f690c2a3":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n\nprint(f'Shape of X_train = {X_train.shape}')\nprint(f'Shape of y_train = {y_train.shape}')\nprint(f'Shape of X_test = {X_test.shape}')\nprint(f'Shape of y_test = {y_test.shape}')","47ea20a8":"def build_model():\n    model = keras.Sequential([\n                            layers.Dense(64, activation = 'relu', input_shape = [len(X.keys())]),\n                            layers.Dense(64, activation = 'relu'),\n                            layers.Dense(1)\n  ])\n  \n    optimizer = tf.keras.optimizers.RMSprop(learning_rate = 0.001) # default learning rate is 0.001\n  \n    model.compile(loss = 'mse',\n                optimizer = optimizer,\n                metrics = ['mae', 'mse', 'mape'])\n    return model\n\nmodel = build_model()","6d44604f":"model.summary()","093748d4":"#Display the training progress by priniting a single dot for each complted epoch\nclass PrintDot(keras.callbacks.Callback):\n    def on_epoch_end(self,epoch,logs):\n        if epoch%500 == 0:print('')\n        if epoch%5 == 0:print('.',end='')\nEPOCHS = 1000\nhistory = model.fit(X_train, y_train,\n                  epochs = EPOCHS, validation_split = 0.2,\n                  verbose = 0, callbacks = [PrintDot()]) #calculate print dot","4caa7819":"hist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.tail()","a7aa64ee":"def plot_history(history):\n    hist = pd.DataFrame(history.history)\n    hist['epoch'] = history.epoch\n\n    plt.figure()\n    plt.xlabel('Epoch')\n    plt.ylabel('Mean Abs Error[MPG]') \n    plt.plot(hist['epoch'], hist['mae'], label='Train Error')\n    plt.plot(hist['epoch'], hist['val_mae'], label='Validation Error')\n    plt.ylim([0,5])\n    plt.legend()\n\n    plt.figure()\n    plt.xlabel('Epoch')\n    plt.ylabel('Mean Squared Error[$MPG^2$]')\n    plt.plot(hist['epoch'], hist['mse'], label='Train Error')\n    plt.plot(hist['epoch'], hist['val_mse'], label='Val Error')\n    plt.ylim([0,20])\n    plt.legend()\n    plt.show()  \nplot_history(history)","b6e9431c":"#### Applying early stopping method to avoid overfitting\nmodel = build_model()\n\n#### The patience is the amount of epochs to check for improvement\nearly_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10)\n\nhistory = model.fit(X_train, y_train,\n                    epochs = EPOCHS, validation_split = 0.2, \n                    verbose = 0, callbacks = [early_stop, PrintDot()])","2c6ec7fc":"plot_history(history)","03f8cbf6":"#### Evaluating the model\nmse=model.evaluate(X_test,y_test,verbose=0)\nprint('Testing set Mean Abs Error:',mse)","0b2e5287":"#### Predicting the model\ntest_predictions=model.predict(X_test).flatten()\ntrain_predictions=model.predict(X_train).flatten()","1da3d5be":"plt.scatter(y_test,test_predictions)\nplt.xlabel('True Values [MPG]')\nplt.ylabel('Predictions [MPG]')\nplt.axis('equal')\nplt.axis('square')\nplt.xlim([0,plt.xlim()[1]])\nplt.ylim([0,plt.ylim()[1]])\nplt.plot([-100,100],[-100,100],color='red')\nplt.show()","325982a9":"#### calculating the mean-squared error, mean absolute error, and the R-Squared error on traing and testing.\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\nmse=mean_squared_error(y_test,test_predictions)\nprint('MSE is',mse)\nmae=mean_absolute_error(y_test,test_predictions)\nprint('MAE is',mae)\nrsq=r2_score(y_train,train_predictions)\nprint('R-square,Training is',rsq)\nrsq=r2_score(y_test,test_predictions)\nprint('R-square,Testing is',rsq)","0b9f6c8c":"* **From the above it is observed that target variable is almost normally distributed.**","8af77d79":"* **From the above plot we can say that model is overfitting, so we need to perform early stopping which is used to avoid overfitting problem.**","c60bc320":"## Display the model summary ","31654856":"## Fit the model by taking validation data as 20% of training data.","064cfe8a":"## Plot the graph of Number of epochs vs\/ training and validation loss\n","b76dedf3":"**From the above heatmap**\n* **Cylinders, Displacement, HorsePower, weight are high negitive correlation with MPG.**\n* **Cylinders, Displacement, HorsePower, weight are having high positive correlation between them.**","b6597f38":"## Build the NN model with adequate number of hidden layers, appropriate optimizer and loss functions.\n* **Building a Sequential model with two densely connected hidden layers, and an output layer that return a single, continuous value.**\n","89a87dd8":"## Importing the Datset","610b441b":"**From the above visuals:**\n* **`Cylinders`: Most of the automobiles contains 4 cylinders.**\n* **`Model Year`: The no of cars manufactured in the year 1973.**\n* **`Origin`: Most of the automobiles are manufactured in USA.**  ","b4371450":"* **From the above it is clearly observed that there is no duplicate data present in dataset.**","e4f80cb4":"## Importing Neccesary Libraries","7a640ed0":"* **From the above it is observed that the column `Horsepower` contains `6 null values`**.","f7189b39":"## Perform appropriate normalization technique","114d19c6":"## Performing appropriate exploratory data analysis technique to treat missing values, to visualize the required plot etc.","046b73f5":"* **First Layer:** On input, Convolution is applied on 64 filters. We will get output shape as (None, 64)\n```python\n    Parameters = 9(Features)\n               = 9 + 1(Bias)\n               = 10 * 64(Number of Filters)\n               = 640\n```\n* **Second Layer:** Another layer with 64 filters. We will get output shape as (None, 64)\n```python\n    Parameters = 64(Filter size of previous layer)\n               = 64 + 1(Bias)\n               = 65 * 64(Number of Filters)\n               = 4160\n```\n* **Output Layer:** It return a single, continuous value. We will get output shape as (None, 1)\n```python\n    Parameters = 64(Filter size of previous layer)\n               = 64 + 1(Bias)\n               = 65 * 1(Number of Filters)\n               = 65\n```\n\nTotal **Trainable parameters** = 640 + 4160 + 65 = 4865","475a004b":"* **Since Origin column contains Nominal variables applying get_dummies.**","4eba0bd3":"* **From the above plot it is observed that model training stopped after 80 epochs with a less mean absolute error as around 2.**","23fc7bd5":"* **Let's re-plot the history to hopefully see the model training stopped before things get worse for the validation data.**","6b4e8e1e":"* **From the above plots it is observed the `Horsepower` column contains `few outliers` in upper boundary and it is `Right Skewed`**. \n* **So `replacing null values with Median` instead of Mean**.","4da637d1":"* **The dataset contains 5 columns as float datatype and 3 columns as integer datatpe.**","41be5fc5":"## Plot between actual values v\/s predicted values (regression line and scatter plot)","70755c7d":"<h1 align='center'style='font-size:40px'>Predicting Fuel Efficiency for AutoMobile<\/h1>\n\n# Problem statement:\n<br>&emsp;&emsp;In this notebook I am train an Artificial Neural Network(ANN) to give better estimates for MPG rates all over the automobile industry. The learning method involved is feedforward learning. Such calculation would help to reduce the efforts needed to design and analyze automobile fuel consumption with such limited factors \u201cattributes (cylinders, displacement factor, horsepower, weight, acceleration, model year and origins of manufacturing).\n\n# Attribute information:\n**MPG**: The fuel economy of an automobile is the relationship between the distance traveled and the amount of fuel consumed by the vehicle. MPG is continuous from 9 to 48 (*Continuous*).\n<br>**Cylinders**: A cylinder is the power unit of an engine; it's the chamber where the gasoline is burned and turned into power. Number of cylinder continuous from 3 to 8 (*Multi-Valued Discrete*)\n<br>**Displacement**:Engine displacement is the measure of the cylinder volume swept by all of the pistons of a piston engine, excluding the combustion chambers. Displacement is continuous from 68 to 455 (*Continuous*)\n<br>**Horsepower**: Horsepower is a unit of power used to measure the forcefulness of a vehicle's engine. Horse power continuous from 46 to 230 (*Continuous*).\n<br>**Weight**: The weight of an object is related to the amount of force acting on the object, either due to gravity or to a reaction force that holds it in place. The weight of car continuous from 1613 to 5140 (*Continuous*).\n<br>**Acceleration**: Is the rate of change of velocity of an object with respect to time. It continuous from 8 to 25 (*Continuous*).\n<br>**Model Year**: Is the year in which product is manufactured. The year ranges from 1970 to 1982 (*Multi-Valued Discrete*)\n<br>**Origin**: The Country which manufacturing the automobile. (*Multi-Valued Discrete*)\n<br>&emsp;&emsp; **1**: USA\n<br>&emsp;&emsp; **2**: GERMANY\n<br>&emsp;&emsp; **3**: JAPAN","aa5f6b6b":"* **There are very few ouliers present in the dataset which are very nearer to upper boundary. So, there is no need of outliers treatment.**","6204e591":"## Divide the data into train and test set with 80:20 ratio.","de321d03":"## Evaluate the model and predict it on testing data","7ac45960":"* **From the above plots it is observed that all the pairs of columns have high correlation between them and most of the columns are normally distributed.**","45cf85a9":"# Conclusion:\n\n&emsp;&emsp;By training Artificial Neural Network predicting Fuel Efficiency for AutoMobile that overcomes overfitting by using early stopping that provides high accuracy of 89% and less mean absolute loss of around 2.\n\n<h1 align='center'>Thank You\ud83d\ude42<\/h1>"}}