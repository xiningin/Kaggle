{"cell_type":{"22c19925":"code","a7ef3ab8":"code","748536aa":"code","d27c214f":"code","c22db9f2":"code","fac68072":"code","4143ae01":"code","460f8274":"code","975c8967":"code","e33ef8d5":"code","cadb7b0c":"code","4952ab4a":"code","eac9e6a0":"code","eecf64dd":"code","dc8e9e59":"code","392f4f4f":"code","f0899bc5":"code","392cd78a":"code","1ca22632":"code","ec64794a":"code","a9027afc":"code","92f80d2b":"code","6c13c443":"code","afd4eca1":"code","72b048d8":"code","d16c08e1":"code","e66fae25":"code","5192bccd":"code","83ba01d8":"code","333f895a":"code","dcc895af":"code","bfc7a02a":"code","71e4c0aa":"code","ad8f47a5":"code","7a501e92":"code","a44ad876":"code","9db85211":"markdown","3cb2f39f":"markdown","8d18d73c":"markdown","3f8e65d2":"markdown","eee592fc":"markdown","a7c1006c":"markdown","dcca12b2":"markdown","587d2bfd":"markdown","5b8de60b":"markdown","d34bb7aa":"markdown","6c70621c":"markdown","fb0a6913":"markdown","be174651":"markdown","9ee6e0f9":"markdown","898a6890":"markdown"},"source":{"22c19925":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Input\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout\nfrom keras.layers.convolutional import Conv1D, Conv2D\nfrom keras.layers.convolutional import MaxPooling1D, MaxPooling2D\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping\nimport scipy.signal as ss\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a7ef3ab8":"df = pd.read_csv('\/kaggle\/input\/eeg-leftright\/OpenBCI-RAW-2020-03-10_08-53-44.txt',skiprows=6,header=None)","748536aa":"df.head()","d27c214f":"df.columns=['index','channel1','channel2','channel3','channel4','acc1','acc2','acc3','time_std','timestamp']\ndf.drop(['index'],axis=1,inplace=True)","c22db9f2":"df.head()","fac68072":"plt.figure()\nplt.plot(df['channel2'])","4143ae01":"plt.plot(df['channel1'])\nplt.plot(ss.detrend(df['channel1']))","460f8274":"df['channel1'] = ss.detrend(df['channel1'])\ndf['channel2'] = ss.detrend(df['channel2'])\ndf['channel3'] = ss.detrend(df['channel3'])\ndf['channel4'] = ss.detrend(df['channel4'])","975c8967":"zchan1 = (df['channel1']-np.mean(df['channel1']))\/np.std(df['channel1'])\nzchan2 = (df['channel2']-np.mean(df['channel2']))\/np.std(df['channel2'])\nzchan3 = (df['channel3']-np.mean(df['channel3']))\/np.std(df['channel3'])\nzchan4 = (df['channel4']-np.mean(df['channel4']))\/np.std(df['channel4'])","e33ef8d5":"plt.figure(figsize=(18,8))\nplt.plot(zchan2)\nplt.plot(zchan2[np.abs(zchan2)>3],'r.')","cadb7b0c":"df['seconds_from_start']=(df['timestamp']-df['timestamp'].iloc[0])\/1000","4952ab4a":"y_labels = np.zeros((df['time_std'].shape[0],1))","eac9e6a0":"y_labels[(60<df.seconds_from_start)&(df.seconds_from_start<90)] = 1\ny_labels[(120<df.seconds_from_start)&(df.seconds_from_start<150)] = 1\ny_labels[(180<df.seconds_from_start)&(df.seconds_from_start<210)] = 1\ny_labels[(240<df.seconds_from_start)&(df.seconds_from_start<270)] = 1\ny_labels[(300<df.seconds_from_start)&(df.seconds_from_start<330)] = 1\ny_labels[(360<df.seconds_from_start)&(df.seconds_from_start<390)] = 1\ny_labels[(420<df.seconds_from_start)&(df.seconds_from_start<450)] = 1\ny_labels[(480<df.seconds_from_start)&(df.seconds_from_start<510)] = 1\ny_labels[(540<df.seconds_from_start)&(df.seconds_from_start<570)] = 1\ny_labels[(600<df.seconds_from_start)&(df.seconds_from_start<630)] = 1","eecf64dd":"plt.plot(zchan2)\nplt.plot(y_labels,'r')","dc8e9e59":"b, a = ss.iirnotch(60.0, 200.0, 200.)\n\nzchan1 = ss.filtfilt(b,a,zchan1)\nzchan2 = ss.filtfilt(b,a,zchan2)\nzchan3 = ss.filtfilt(b,a,zchan3)\nzchan4 = ss.filtfilt(b,a,zchan4)\n\nzchan1[np.abs(zchan1)>3] = np.nan\nzchan2[np.abs(zchan2)>3] = np.nan\nzchan3[np.abs(zchan3)>3] = np.nan\nzchan4[np.abs(zchan4)>3] = np.nan\ny_labels[(np.abs(zchan1)>3) | (np.abs(zchan2)>3) | (np.abs(zchan3)>3) | (np.abs(zchan4)>3)] = np.nan","392f4f4f":"df['channel1'] = zchan1\ndf['channel2'] = zchan2\ndf['channel3'] = zchan3\ndf['channel4'] = zchan4\ndf['y_labels'] = y_labels","f0899bc5":"X = df['channel2'].iloc[y_labels[:,0]==0]\nplt.psd(X[~np.isnan(X)],512,Fs=200,color='r')\nX = df['channel2'].iloc[y_labels[:,0]==1]\nplt.psd(X[~np.isnan(X)],512,Fs=200)\nplt.xlim(0,100)\n#plt.show()\nplt.show()","392cd78a":"from scipy.integrate import simps\nimport scipy.stats as sst\nfrom matplotlib.mlab import psd\n\ndef bandpower(trace,band):\n    [a1,f1]=psd(trace[~np.isnan(trace)],512,Fs=200)\n    total_power1 = simps(a1, dx=0.1)\n    ap1 = simps(a1[(f1>band[0]) & (f1<band[1])], dx=0.1)\n    return ap1\/total_power1\n    \ninput_mat = np.zeros((600,21))\nc=0\nfor i in np.arange(0,120000,200):\n    \n    X1=df['channel1'];X2=df['channel2'];X3=df['channel3'];X4 = df['channel4']\n    Y = df['y_labels']\n    \n    X1=X1[i:i+200];X2=X2[i:i+200];X3=X3[i:i+200];X4=X4[i:i+200]\n    Y=Y[i:i+200]\n    \n    input_mat[c,0] = bandpower(X1,[8,12]);input_mat[c,1] = bandpower(X2,[8,12]);\n    input_mat[c,2] = bandpower(X3,[8,12]);input_mat[c,3] = bandpower(X4,[8,12]);\n    \n    input_mat[c,4] = bandpower(X1,[3,8]);input_mat[c,5] = bandpower(X2,[3,8]);\n    input_mat[c,6] = bandpower(X3,[3,8]);input_mat[c,7] = bandpower(X4,[3,8]);\n    \n    input_mat[c,8] = bandpower(X1,[12,38]);input_mat[c,9] = bandpower(X2,[12,38]);\n    input_mat[c,10] = bandpower(X3,[12,38]);input_mat[c,11] = bandpower(X4,[12,38]);\n    \n    input_mat[c,12] = bandpower(X1,[38,100]);input_mat[c,13] = bandpower(X2,[38,100]);\n    input_mat[c,14] = bandpower(X3,[38,100]);input_mat[c,15] = bandpower(X4,[38,100]);\n    \n    input_mat[c,16] = bandpower(X1,[0.5,3]);input_mat[c,17] = bandpower(X2,[0.5,3]);\n    input_mat[c,18] = bandpower(X3,[0.5,3]);input_mat[c,19] = bandpower(X4,[0.5,3]);\n    \n    [u,n] = np.unique(Y,return_counts=True)\n    if n.shape[0]==1:\n        input_mat[c,-1] = u[0].astype(int)\n    else:\n        input_mat[c,-1] = np.nan\n        \n    c+=1\n\nfeat_eng_df = pd.DataFrame(input_mat, columns = ['alpha_power_1','alpha_power_2','alpha_power_3','alpha_power_4',\n                                                 'theta_power_1','theta_power_2','theta_power_3','theta_power_4',\n                                                 'beta_power_1','beta_power_2','beta_power_3','beta_power_4',\n                                                 'gamma_power_1','gamma_power_2','gamma_power_3','gamma_power_4',\n                                                 'delta_power_1','delta_power_2','delta_power_3','delta_power_4',\n                                                 'eyes_closed'])","1ca22632":"import seaborn as sns\nsns.boxplot(x='eyes_closed',y='alpha_power_3',data=feat_eng_df)","ec64794a":"from sklearn.preprocessing import scale\n\ntrain_df = feat_eng_df.iloc[0:420,:]\ntest_df = feat_eng_df.iloc[420:,:]\n\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\ntest_df = test_df.sample(frac=1).reset_index(drop=True)\n\nX_train = train_df.iloc[:,:-1]\ny_train = train_df.iloc[:,-1]\nX_test = test_df.iloc[:,:-1]\ny_test = test_df.iloc[:,-1]\n\nX_train = X_train[~np.isnan(y_train)]\ny_train = y_train[~np.isnan(y_train)]\nX_test = X_test[~np.isnan(y_test)]\ny_test = y_test[~np.isnan(y_test)]\n\n\nX_train = scale(X_train)\nX_test = scale(X_test)","a9027afc":"X_train.shape, X_test.shape","92f80d2b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix\n\nlr = LogisticRegression(random_state=231)\nrf = RandomForestClassifier(random_state=2124)\nmlp = MLPClassifier(hidden_layer_sizes=(100,),early_stopping=True,max_iter=1000,random_state=12)\n\nlr.fit(X_train,y_train.values)\nrf.fit(X_train,y_train.values)\nmlp.fit(X_train,y_train.values)\n\nyhatlr = lr.predict(X_test)\nyhatrf = rf.predict(X_test)\nyhatmlp = mlp.predict(X_test)\n\nprint(confusion_matrix(y_test,yhatlr))\nprint(confusion_matrix(y_test,yhatrf))\nprint(confusion_matrix(y_test,yhatmlp))","6c13c443":"df_train = df[df.seconds_from_start<=420]\ndf_test = df[df.seconds_from_start>420]","afd4eca1":"df_train.head()","72b048d8":"from tqdm import tqdm_notebook\n\nwindow_size = 50\nstride=20\n\ndf_class_train = np.zeros((int(df_train.shape[0]\/stride),window_size+1,4))\nc=0\nfor i in tqdm_notebook(np.arange(0,df_train.shape[0]-window_size,stride)):\n    df_class_train[c,0:window_size,0] = df_train['channel1'].iloc[i:i+window_size]\n    df_class_train[c,0:window_size,1] = df_train['channel2'].iloc[i:i+window_size]\n    df_class_train[c,0:window_size,2] = df_train['channel3'].iloc[i:i+window_size]\n    df_class_train[c,0:window_size,3] = df_train['channel4'].iloc[i:i+window_size]\n    \n    ylabs = df_train['y_labels'].iloc[i:i+window_size]\n    ones=ylabs[ylabs==1]\n    zeros=ylabs[ylabs==0]\n    \n    if ones.shape[0]>zeros.shape[0]:\n        df_class_train[c,-1,:] = 1\n    else:\n        df_class_train[c,-1,:] = 0\n    c+=1\n    \ndf_class_test = np.zeros((int(df_test.shape[0]\/stride),window_size+1,4))\nc=0\nfor i in tqdm_notebook(np.arange(0,df_test.shape[0]-window_size,stride)):\n    df_class_test[c,0:window_size,0] = df_test['channel1'].iloc[i:i+window_size]\n    df_class_test[c,0:window_size,1] = df_test['channel2'].iloc[i:i+window_size]\n    df_class_test[c,0:window_size,2] = df_test['channel3'].iloc[i:i+window_size]\n    df_class_test[c,0:window_size,3] = df_test['channel4'].iloc[i:i+window_size]\n    \n    ylabs = df_test['y_labels'].iloc[i:i+window_size]\n    ones=ylabs[ylabs==1]\n    zeros=ylabs[ylabs==0]\n    \n    if ones.shape[0]>zeros.shape[0]:\n        df_class_test[c,-1,:] = 1\n    else:\n        df_class_test[c,-1,:] = 0\n    c+=1\n","d16c08e1":"df_class_train = df_class_train[~np.isnan(df_class_train[:,:-1,0]).any(axis=1),:,:]\ndf_class_train = df_class_train[~np.isnan(df_class_train[:,:-1,1]).any(axis=1),:,:]\ndf_class_train = df_class_train[~np.isnan(df_class_train[:,:-1,2]).any(axis=1),:,:]\ndf_class_train = df_class_train[~np.isnan(df_class_train[:,:-1,3]).any(axis=1),:,:]\n\ndf_class_test = df_class_test[~np.isnan(df_class_test[:,:-1,0]).any(axis=1),:,:]\ndf_class_test = df_class_test[~np.isnan(df_class_test[:,:-1,1]).any(axis=1),:,:]\ndf_class_test = df_class_test[~np.isnan(df_class_test[:,:-1,2]).any(axis=1),:,:]\ndf_class_test = df_class_test[~np.isnan(df_class_test[:,:-1,3]).any(axis=1),:,:]","e66fae25":"df_class_train.shape,df_class_test.shape","5192bccd":"df_class_train.shape","83ba01d8":"import scipy\n################ TRAIN #######################\nX=np.mean(df_class_train[:,:-1,:],axis=1)\nY=np.var(df_class_train[:,:-1,:],axis=1)\nZ=scipy.stats.skew(df_class_train[:,:-1,:],axis=1)\nW=scipy.stats.kurtosis(df_class_train[:,:-1,:],axis=1)\n\nds = np.concatenate((X,Y,Z,W,df_class_train[:,-1,0:1]),axis=1)\nnp.random.shuffle(ds) \n\ndf_feats_train = pd.DataFrame(ds,columns=['meanch1','meanch2','meanch3','meanch4',\\\n                                    'varch1','varch2','varch3','varch4',\\\n                                    'skewch1','skewch2','skewch3','skewch4',\\\n                                    'kurtch1','kurtch2','kurtch3','kurtch4','output'])\n################ TEST #######################\nX=np.mean(df_class_test[:,:-1,:],axis=1)\nY=np.var(df_class_test[:,:-1,:],axis=1)\nZ=scipy.stats.skew(df_class_test[:,:-1,:],axis=1)\nW=scipy.stats.kurtosis(df_class_test[:,:-1,:],axis=1)\n\nds = np.concatenate((X,Y,Z,W,df_class_test[:,-1,0:1]),axis=1)\nnp.random.shuffle(ds) \n\ndf_feats_test = pd.DataFrame(ds,columns=['meanch1','meanch2','meanch3','meanch4',\\\n                                    'varch1','varch2','varch3','varch4',\\\n                                    'skewch1','skewch2','skewch3','skewch4',\\\n                                    'kurtch1','kurtch2','kurtch3','kurtch4','output'])","333f895a":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.decomposition import PCA\n\nrf = RandomForestClassifier()\nlr = LogisticRegression()\npca = PCA()\n\nX_train = df_feats_train.loc[:, df_feats_train.columns != 'output']\ny_train = df_feats_train.loc[:, df_feats_train.columns == 'output']\n\nX_test = df_feats_test.loc[:, df_feats_test.columns != 'output']\ny_test = df_feats_test.loc[:, df_feats_test.columns == 'output']\n\n\n#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=212)\n\nrf.fit(X_train,y_train)\nlr.fit(X_train,y_train)\npca.fit(X_train,y_train)\n\nyhat_rf = rf.predict(X_test)\nyhat_lr = lr.predict(X_test)\n\nprint(confusion_matrix(y_test,yhat_rf))\nprint(confusion_matrix(y_test,yhat_lr))","dcc895af":"plt.figure(figsize=(10,15))\ny_pos = np.arange(len(X_train.columns))\nplt.barh(y_pos, rf.feature_importances_)\n \n# Create names on the y-axis\nplt.yticks(y_pos, X_train.columns)\nplt.show()","bfc7a02a":"#import seaborn as sns\n#sns.pairplot(pd.concat((X_train[['meanch4']],y_train),axis=1),hue='output')","71e4c0aa":"#sns.scatterplot(x='meanch4',y='varch2',hue='output',data=pd.concat((X_train,y_train),axis=1))\n#plt.show()","ad8f47a5":"def focal_loss(gamma=4, alpha=0.15):\n\n    gamma = float(gamma)\n    alpha = float(alpha)\n\n    def focal_loss_fixed(y_true, y_pred):\n        \"\"\"Focal loss for multi-classification\n        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n        Notice: y_pred is probability after softmax\n        gradient is d(Fl)\/d(p_t) not d(Fl)\/d(x) as described in paper\n        d(Fl)\/d(p_t) * [p_t(1-p_t)] = d(Fl)\/d(x)\n        Focal Loss for Dense Object Detection\n        https:\/\/arxiv.org\/abs\/1708.02002\n\n        Arguments:\n            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n\n        Keyword Arguments:\n            gamma {float} -- (default: {2.0})\n            alpha {float} -- (default: {4.0})\n\n        Returns:\n            [tensor] -- loss.\n        \"\"\"\n        epsilon = 1.e-9\n        y_true = tf.convert_to_tensor(y_true, tf.float32)\n        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n\n        model_out = tf.add(y_pred, epsilon)\n        ce = tf.multiply(y_true, -tf.math.log(model_out))\n        weight = tf.multiply(y_true, tf.pow(tf.subtract(1., model_out), gamma))\n        fl = tf.multiply(alpha, tf.multiply(weight, ce))\n        reduced_fl = tf.reduce_max(fl, axis=1)\n        return tf.reduce_mean(reduced_fl)\n    return focal_loss_fixed\n\n# load the dataset, returns train and test X and y elements\ndef load_dataset(dftrain,dftest):\n    trainX = dftrain[:,:-1,:]\n    trainy = dftrain[:,-1,0:1]\n    \n    testX = dftest[:,:-1,:]\n    testy = dftest[:,-1,0:1]\n    \n    shuff_idx = np.random.choice(np.arange(0,trainX.shape[0]),trainX.shape[0],replace=False)\n    trainX = trainX[shuff_idx,:,:]\n    trainy = trainy[shuff_idx]\n    \n    shuff_idx = np.random.choice(np.arange(0,testX.shape[0]),testX.shape[0],replace=False)\n    testX = testX[shuff_idx,:,:]\n    testy = testy[shuff_idx]\n\n    trainy = to_categorical(trainy)\n    testy = to_categorical(testy)\n    print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n    return trainX, trainy, testX, testy\n\n# fit and evaluate a model\ndef evaluate_model(trainX, trainy, testX, testy):\n    verbose, epochs, batch_size = 1, 200, 32\n    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n    model = Sequential()\n    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Flatten())\n    model.add(Dense(100, activation='relu'))\n    model.add(Dense(n_outputs, activation='softmax'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    er = EarlyStopping(patience=10, min_delta=1e-4, monitor='val_accuracy')\n    # fit network\n    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose, \n              validation_split=0.15, callbacks=[er])\n    return model\n\n# fit and evaluate a model\ndef evaluate_model2d(trainX, trainy, testX, testy):\n    verbose, epochs, batch_size = 1, 20, 32\n    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n    input_layer = Input(shape=(trainX.shape[1], trainX.shape[2], trainX.shape[3]))\n\n    # handle image dimensions ordering\n    if tf.keras.backend.image_data_format() == 'channels_first':\n        latent = Permute((3, 1, 2))(input_layer)\n    else:\n        latent = input_layer\n\n    # define the network architecture\n    latent = Conv2D(filters=32, kernel_size=(3, 3),\n                        activation='relu')(latent)\n    latent = Conv2D(filters=64, kernel_size=(3, 3),\n                        activation='relu')(latent)\n    latent = Dropout(0.5)(latent)\n    latent = MaxPooling2D(pool_size=(2, 2))(latent)\n    latent = Flatten()(latent)\n    latent = Dense(units=100, activation='relu')(latent)\n    output_layer = Dense(units=trainy.shape[1], activation='softmax')(latent)\n\n    model = Model(inputs=input_layer, outputs=output_layer)\n\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    er = EarlyStopping(patience=10, min_delta=1e-4, monitor='val_accuracy')\n    # fit network\n    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose, \n              validation_split=0.15, callbacks=[er])\n    return model\n\n# summarize scores\ndef summarize_results(scores):\n    print(scores)\n    m, s = mean(scores), std(scores)\n    print('Accuracy: %.3f%% (+\/-%.3f)' % (m, s))\n\n# run an experiment\ndef run_experiment():\n    # load data\n    \n    trainX, trainy, testX, testy = load_dataset(df_class_train,df_class_test)\n    # repeat experiment\n    scores = list()\n    \n    model = evaluate_model(trainX, trainy, testX, testy)\n    \n    ypred = model.predict(testX)\n    class_preds = np.argmax(ypred, axis=-1)\n    testy = np.argmax(testy,axis=-1)\n\n    from sklearn.metrics import confusion_matrix\n    print(confusion_matrix(testy,class_preds))\n    return model","7a501e92":"model = run_experiment()","a44ad876":"%matplotlib inline\nplt.plot(df_class_test[0,:-1,0])\nplt.plot(df_class_test[0,:-1,1])\nplt.plot(df_class_test[0,:-1,2])\nplt.plot(df_class_test[0,:-1,3])","9db85211":"# Welcome\nThis notebook will walk you through how to analyse brain wave data. This data was collected in the Neural Engineering Laboratory at the University of Missouri - Columbia. ","3cb2f39f":"# Artifact removal\nWhenever the subject moved or blinked, there is a large deviation in the potential of all channels. We need to remove these.","8d18d73c":"# Explanation of the data\nAs you can see the dataframe consists of 8 columns. Each row is a sample. The data was sampled at 200 Hz. That means 200 samples per second. There are four channels on the EEG device, two of these were place on the frontal lobe (forehead) and two on the temporal lobe (sides of the head). The acc1-3 channels are the accelerometer. If the subject moves, these channels will detect that. The time is represented by the final two columns.","3f8e65d2":"Remove 60 Hz noise and artifacts. Any time a signal is recorded inside of a building, there will be a prominent 60 Hz noise from the power lines. We will remove this using a notch filter.","eee592fc":"In the plot above, the trace of a single channel is shown in blue and whether or not the eyes were closed is shown in red. y_labels == 1 means eyes closed and y_labels == 0 means eyes open.","a7c1006c":"# Feature engineering\n\nHere I manually engineer some features and try some basic models on that transformed data.","dcca12b2":"The below graph shows the difference between eyes closed and eyes open in the frequency domain. It is well known that the eyes closed condition generates alpha waves at 8-12 Hz, which we also see in this recording. There may still be some 60 Hz noise that our notch filter failed to remove.","587d2bfd":"Below, we will plot the entire time course of one channel.","5b8de60b":"##### STUDENTS, ASSIGNMENT STOPS HERE #####","d34bb7aa":"# Remove drift\nThe mean of the data changes over time. This is called drift. We can remove this trend in the data using scipy's detrend function.","6c70621c":"# Feature Engineering Model","fb0a6913":"# Make y labels\nThe subject was in two states during the experiment; their eyes were either open or closed. Below, we define the times at which the eyes were open or closed.","be174651":"# Model raw data\n\nBelow we transform the data into a frame suitable for supervised learning. A window size of 50 represents 0.25 seconds.","9ee6e0f9":"Most of these are artifacts.","898a6890":"Remove NaN rows"}}