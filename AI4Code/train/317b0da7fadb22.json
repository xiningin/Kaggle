{"cell_type":{"843f82ca":"code","d7830bd8":"code","08979313":"code","24b10cc7":"code","a83156ec":"code","ee44dda9":"code","ff3fc145":"code","d351dc9d":"code","ffc501ad":"code","dd723546":"code","bd356852":"code","659a2308":"code","aa278c6e":"code","96227011":"code","95cc401a":"code","b6b33376":"code","d78927d3":"code","e4813479":"code","6c680fa3":"markdown","1d1862b6":"markdown","8106feb0":"markdown","a0086d32":"markdown","48d6e81b":"markdown","a787661c":"markdown"},"source":{"843f82ca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d7830bd8":"# data frames from dictionary\ncountry = [\"Spain\",\"France\"]\npopulation = [\"11\",\"12\"]\nlist_label = [\"country\",\"population\"]\nlist_col = [country,population]\nzipped = list(zip(list_label,list_col))\ndata_dict = dict(zipped)\ndf = pd.DataFrame(data_dict)\ndf","08979313":"# Add new columns\ndf[\"capital\"] = [\"madrid\",\"paris\"]\ndf","24b10cc7":"# Broadcasting\ndf[\"income\"] = 0 #Broadcasting entire column\ndf","a83156ec":"# Plotting all data \ndata=pd.read_csv('..\/input\/pokemon.csv')\ndata1 = data.loc[:,[\"Attack\",\"Defense\",\"Speed\"]]\ndata1.plot()\n# it is confusing","ee44dda9":"# subplots\nimport matplotlib.pyplot as plt\ndata1.plot(subplots = True)\nplt.show()","ff3fc145":"# scatter plot  \ndata1.plot(kind = \"scatter\",x=\"Attack\",y = \"Defense\")\nplt.show()","d351dc9d":"# hist plot  \ndata1.plot(kind = \"hist\",y = \"Defense\",bins = 50,range= (0,250),normed = True)\nplt.show()","ffc501ad":"# histogram subplot with non cumulative and cumulative\nfig, axes = plt.subplots(nrows=2,ncols=1)\ndata1.plot(kind = \"hist\",y = \"Defense\",bins = 50,range= (0,250),normed = True,ax = axes[0])\ndata1.plot(kind = \"hist\",y = \"Defense\",bins = 50,range= (0,250),normed = True,ax = axes[1],cumulative = True)\nplt.savefig('graph.png')\nplt","dd723546":"data.describe()","bd356852":"time_list = [\"1992-03-08\",\"1992-04-12\"]\nprint(type(time_list[1])) # As you can see date is string\n# however we want it to be datetime object\ndatetime_object = pd.to_datetime(time_list)\nprint(type(datetime_object))","659a2308":"data.head()#indexes begins from zero ","aa278c6e":"# close warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# In order to practice lets take head of pokemon data and add it a time list\ndata2 = data.head()\ndate_list = [\"1992-01-10\",\"1992-02-10\",\"1992-03-10\",\"1993-03-15\",\"1993-03-16\"]\ndatetime_object = pd.to_datetime(date_list)\ndata2[\"date\"] = datetime_object\n# lets make date as index\ndata2= data2.set_index(\"date\")\ndata2 ","96227011":"# Now we can select according to our date index\nprint(data2.loc[\"1993-03-16\"])\n#print(data2.loc[\"1992-03-10\":\"1993-03-16\"])","95cc401a":"# We will use data2 that we create at previous part\ndata2.resample(\"A\").mean()","b6b33376":"# Lets resample with month\ndata2.resample(\"M\").mean()\n# As you can see there are a lot of nan because data2 does not include all months","d78927d3":"# In real life (data is real. Not created from us like data2) we can solve this problem with interpolate\n# We can interpolete from first value\ndata2.resample(\"M\").first().interpolate(\"linear\")","e4813479":"# Or we can interpolate with mean()\ndata2.resample(\"M\").mean().interpolate(\"linear\")","6c680fa3":"RESAMPLING PANDAS TIME SERIES\n* Resampling: statistical method over different time intervals\n        *     Needs string to specify frequency like \"M\" = month or \"A\" = year\n* Downsampling: reduce date time rows to slower frequency like from daily to weekly\n* Upsampling: increase date time rows to faster frequency like from daily to hourly\n* Interpolate: Interpolate values according to different methods like \u2018linear\u2019, \u2018time\u2019 or index\u2019\n*         https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.Series.interpolate.html","1d1862b6":"INDEXING PANDAS TIME SERIES\n* datetime = object\n* parse_dates(boolean): Transform date to ISO 8601 (yyyy-mm-dd hh:mm:ss ) format","8106feb0":"BUILDING DATA FRAMES FROM SCRATCH\u00b6\nWe can build data frames from csv as we did earlier.\nAlso we can build dataframe from dictionaries\nzip() method: This function returns a list of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables.\nAdding new column\nBroadcasting: Create new column and assign a value to entire column","a0086d32":"VISUAL EXPLORATORY DATA ANALYSIS\n* Plot\n* Subplot\n* Histogram:\n        *     bins: number of bins\n        *     range(tuble): min and max values of bins\n        *     normed(boolean): normalize or not\n        *     cumulative(boolean): compute cumulative distribution","48d6e81b":"PANDAS FOUNDATION\n\n\nREVIEW of PANDAS\n\n\nAs you notice, I do not give all idea in a same time. Although, we learn some basics of pandas, we will go deeper in pandas.\n\nsingle column = series\nNaN = not a number\ndataframe.values = numpy","a787661c":"STATISTICAL EXPLORATORY DATA ANALYSIS\u00b6\nI already explained it at previous parts. However lets look at one more time.\n\n* count: number of entries\n* mean: average of entries\n* std: standart deviation\n* min: minimum entry\n* 25%: first quantile\n* 50%: median or second quantile\n* 75%: third quantile\n* max: maximum entry"}}