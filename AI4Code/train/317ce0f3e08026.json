{"cell_type":{"ea94aaac":"code","6a8d17a2":"code","43c7c9b1":"code","2565700b":"code","826260af":"code","298625f5":"code","c58bf078":"code","eac0dcd7":"code","3f540a99":"code","f9677ef7":"code","33047a35":"code","12f0bf8d":"code","0a24d7fd":"code","fc97fab6":"code","56f618ea":"code","f0edc269":"code","550b771a":"code","98d261a2":"code","d06d67ac":"code","6471a8e2":"code","72c498dd":"code","14d5db85":"code","2d4dd9b1":"code","d3a67234":"markdown","3db79c0c":"markdown","541774ca":"markdown"},"source":{"ea94aaac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6a8d17a2":"import pandas as pd\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate, cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import Lasso,Ridge,BayesianRidge,ElasticNet,HuberRegressor,LinearRegression,LogisticRegression,SGDRegressor\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import r2_score\nwarnings.filterwarnings(\"ignore\")","43c7c9b1":"df = pd.read_csv('\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')\ndf.head()","2565700b":"del df['Serial No.']","826260af":"df.head()","298625f5":"%pylab inline","c58bf078":"sns.distplot(df['Chance of Admit '])","eac0dcd7":"pylab.hist(df['GRE Score'])\npylab.xlabel('GRE Score')","3f540a99":"pylab.hist(df['TOEFL Score'])\npylab.xlabel('TOEFL Score')","f9677ef7":"sns.relplot(x=\"GRE Score\", y=\"TOEFL Score\", kind=\"line\", ci=\"sd\", data=df)","33047a35":"sns.relplot(x=\"GRE Score\", y=\"CGPA\", kind=\"line\", ci=\"sd\",data=df)","12f0bf8d":"fig = sns.lmplot(x=\"CGPA\", y=\"LOR \", data=df, hue=\"Research\")\nplt.title(\"GRE Score vs CGPA\")\nplt.show()","0a24d7fd":"X = df.drop(['Chance of Admit '], axis=1)\ny = df['Chance of Admit ']","fc97fab6":"f,ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(X.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","56f618ea":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.20, shuffle=False)","f0edc269":"kfolds = 4 \nsplit = KFold(n_splits=kfolds, shuffle=True, random_state=42)\nbase_models = [(\"DT_model\",DecisionTreeRegressor(random_state=42)),\n               (\"RF_model\", RandomForestRegressor(random_state=42,n_jobs=-1)),\n               (\"LR_model\", LinearRegression(n_jobs=-1)),\n               (\"KN_model\", KNeighborsRegressor(n_jobs=-1)),\n              (\"SVR_model\",SVR()),\n              ('ABR_model',AdaBoostRegressor(random_state=42)),\n              ('GBR_model',GradientBoostingRegressor(random_state=42)),\n              ('XGB_model',XGBRegressor(random_state=42,n_jobs=-1))]\nfor name,model in base_models:\n    clf = model\n    cv_results = cross_val_score(clf, \n                                 X, y, \n                                 cv=split,\n                                 scoring=\"neg_mean_absolute_error\",\n                                 n_jobs=-1\n                                 )\n    min_score = round(min(cv_results), 4)\n    max_score = round(max(cv_results), 4)\n    mean_score = round(np.mean(cv_results), 4)\n    std_dev = round(np.std(cv_results), 4)\n    print(f\"{name} absolute error: {mean_score} +\/- {std_dev} (std) min: {min_score}, max: {max_score}\")","550b771a":"def GetScaledModel(nameOfScaler):\n    scaler = StandardScaler()\n    pipelines = []\n    \n    pipelines.append((nameOfScaler+'DT_model'  , Pipeline([('Scaler', scaler),(\"DT_model\",DecisionTreeRegressor())])))\n    pipelines.append((nameOfScaler+'RF_model' , Pipeline([('Scaler', scaler),(\"RF_model\", RandomForestRegressor())])))\n    pipelines.append((nameOfScaler+'LR_model' , Pipeline([('Scaler', scaler), (\"LR_model\", LinearRegression())])))\n    pipelines.append((nameOfScaler+'KN_model', Pipeline([('Scaler', scaler),(\"KN_model\", KNeighborsRegressor())])))\n    pipelines.append((nameOfScaler+'SVR_model'  , Pipeline([('Scaler', scaler),(\"SVR_model\",SVR())])))\n    pipelines.append((nameOfScaler+'ABR_model'  , Pipeline([('Scaler', scaler),('ABR_model',AdaBoostRegressor())])))\n    pipelines.append((nameOfScaler+'GBR_model' , Pipeline([('Scaler', scaler),('GBR_model',GradientBoostingRegressor())])  ))\n    pipelines.append((nameOfScaler+'XGB_model'  , Pipeline([('Scaler', scaler),('XGB_model',XGBRegressor(random_state=42,n_jobs=-1))]) ))\n    return pipelines","98d261a2":"def BasedLine2(X_train, y_train,models):\n    num_folds = 10\n    scoring=\"neg_mean_absolute_error\"\n\n    results = []\n    names = []\n    for name, model in models:\n        split = KFold(n_splits=kfolds, shuffle=True, random_state=42)\n        cv_results = cross_val_score(model, X_train, y_train, cv=split, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        \n    return names, results","d06d67ac":"def ScoreDataFrame(names,results):\n    def floatingDecimals(f_val, dec=3):\n        prc = \"{:.\"+str(dec)+\"f}\" \n    \n        return float(prc.format(f_val))\n\n    scores = []\n    for r in results:\n        scores.append(floatingDecimals(r.mean(),4))\n\n    scoreDataFrame = pd.DataFrame({'Model':names, 'Score': scores})\n    return scoreDataFrame","6471a8e2":"names,results = BasedLine2(X_train, y_train,base_models)\nbasedLineScore = ScoreDataFrame(names,results)\nbasedLineScore","72c498dd":"models = GetScaledModel('standard')\nnames,results = BasedLine2(X_train, y_train,models)\nscaledScoreStandard = ScoreDataFrame(names,results)\ncompareModels = pd.concat([basedLineScore,\n                           scaledScoreStandard], axis=1)\ncompareModels","14d5db85":"lr =  LinearRegression()\nlr.fit(X_train,y_train)\npredict = lr.predict(X_test)\nmean_absolute_error(y_test,predict)","2d4dd9b1":"r2_score(y_test,predict)","d3a67234":"LOOKING FOR THE BEST MODEL","3db79c0c":"LinearRegression shows the best result","541774ca":"DEPENDENCIES AND RELATIONS"}}