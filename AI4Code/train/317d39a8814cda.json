{"cell_type":{"9bc39a90":"code","883d0349":"code","8b90cced":"code","1b52dbb0":"code","09e60696":"code","c621db7e":"code","8898917d":"code","7d16f979":"code","824da7ab":"code","7eed09f2":"code","df60a0c6":"code","3b4380f2":"code","f9ff8a84":"code","ce66bae4":"code","b5a5d1ec":"code","457fe4e0":"code","11cfb1ee":"code","c4604bde":"code","aee428c1":"code","de40781f":"code","3e7b4a54":"code","0db0f8bd":"code","af5386ff":"code","30c5a6b3":"code","24ad2f8c":"code","e6b2a7d4":"code","006cdf86":"code","68cffa33":"code","09990ec2":"code","80243fc4":"code","5460be56":"code","fa2dd839":"code","53b0a0c1":"code","e142764c":"code","2418abbc":"code","c06cc4e9":"code","c9c576bc":"code","2aa9095e":"code","0a8bae8c":"code","b7223167":"code","f135fa04":"code","97eb751f":"code","78c97f4e":"code","5af6adcc":"code","23ac49ef":"code","bcd0c9f5":"code","e7cdafe4":"code","87539d9e":"markdown","e00ac57c":"markdown","a785683e":"markdown","910f7a58":"markdown","50602967":"markdown","3894055b":"markdown","3d8b095d":"markdown","5471b1bf":"markdown","83896082":"markdown","f1cd618d":"markdown","0fb5d78b":"markdown","8ae21f77":"markdown","e1e2f4b1":"markdown","e8d255cb":"markdown","be894bdb":"markdown","4738a9de":"markdown","e0374b31":"markdown","b1645365":"markdown","523d99ef":"markdown","4aa42ca9":"markdown","935cdbf0":"markdown","41c2174e":"markdown","2a6247af":"markdown","5f53aaf8":"markdown","0b95723d":"markdown"},"source":{"9bc39a90":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","883d0349":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport geoplot","8b90cced":"df = pd.read_csv(\"\/kaggle\/input\/india-air-quality-data\/data.csv\", encoding = \"ISO-8859-1\")\ndf.head()","1b52dbb0":"df['date'] = pd.to_datetime(df['date'],format='%Y-%m-%d') # date parse\ndf['year'] = df['date'].dt.year # year\ndf['year'] = df['year'].fillna(df[\"year\"].min())\ndf['year'] = df['year'].values.astype(int)","09e60696":"print (df.get_dtype_counts())","c621db7e":"def printNullValues(df):\n    total = df.isnull().sum().sort_values(ascending = False)\n    total = total[df.isnull().sum().sort_values(ascending = False) != 0]\n    percent = total \/ len(df) * 100\n    percent = percent[df.isnull().sum().sort_values(ascending = False) != 0]\n    concat = pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n    print (concat)\n    print ( \"-------------\")","8898917d":"printNullValues(df)","7d16f979":"df[\"type\"].value_counts()","824da7ab":"\nsns.catplot(x = \"type\", kind = \"count\",  data = df, height=5, aspect = 4)","7eed09f2":"grp = df.groupby([\"type\"]).mean()[\"so2\"].to_frame()\ngrp.plot.bar(figsize = (20,10))","df60a0c6":"grp = df.groupby([\"type\"]).mean()[\"no2\"].to_frame()\ngrp.plot.bar(figsize = (20,10))","3b4380f2":"\ndf[['so2', 'state']].groupby(['state']).median().sort_values(\"so2\", ascending = False).plot.bar(figsize=(20,10))\n","f9ff8a84":"df[['so2','year','state']].groupby([\"year\"]).median().sort_values(by='year',ascending=False).plot(figsize=(20,10))","ce66bae4":"\ndf[['no2', 'state']].groupby(['state']).median().sort_values(\"no2\", ascending = False).plot.bar(figsize=(20,10))\n","b5a5d1ec":"df[['no2','year','state']].groupby([\"year\"]).median().sort_values(by='year',ascending=False).plot(figsize=(20,10))","457fe4e0":"\ndf[['spm', 'state']].groupby(['state']).median().sort_values(\"spm\", ascending = False).plot.bar(figsize=(20,10))\n","11cfb1ee":"df[['spm','year','state']].groupby([\"year\"]).median().sort_values(by='year',ascending=False).plot(figsize=(20,10))","c4604bde":"fig, ax = plt.subplots(figsize=(20,10))      \nsns.heatmap(df.pivot_table('so2', index='state',columns=['year'],aggfunc='median',margins=True),ax = ax,annot=True, linewidths=.5)","aee428c1":"fig, ax = plt.subplots(figsize=(20,10))      \nsns.heatmap(df.pivot_table('no2', index='state',columns=['year'],aggfunc='median',margins=True),ax = ax,annot=True, linewidths=.5)","de40781f":"fig, ax = plt.subplots(figsize=(20,10))      \nsns.heatmap(df.pivot_table('spm', index='state',columns=['year'],aggfunc='median',margins=True),ax = ax,annot=False, linewidths=.5)","3e7b4a54":"temp = df.pivot_table('so2', index='year',columns=['state'],aggfunc='median',margins=True).reset_index()\ntemp = temp.drop(\"All\", axis = 1)\ntemp = temp.set_index(\"year\")\ntemp.plot(figsize=(20,10))","0db0f8bd":"temp = df.pivot_table('no2', index='year',columns=['state'],aggfunc='median',margins=True).reset_index()\ntemp = temp.drop(\"All\", axis = 1)\ntemp = temp.set_index(\"year\")\ntemp.plot(figsize=(20,10))","af5386ff":"temp = df.pivot_table('spm', index='year',columns=['state'],aggfunc='median',margins=True).reset_index()\ntemp = temp.drop(\"All\", axis = 1)\ntemp = temp.set_index(\"year\")\ntemp.plot(figsize=(20,10))","30c5a6b3":"india = gpd.read_file('\/kaggle\/input\/maps-of-india\/India_SHP\/INDIA.shp')\nindia.info()","24ad2f8c":"india.plot()","e6b2a7d4":"india[\"ST_NAME\"] = india[\"ST_NAME\"].apply(lambda x: x.lower())\n\nindia = india.set_index(\"ST_NAME\")\n\ndf[\"state\"] = df[\"state\"].apply(lambda x: x.lower())","006cdf86":"df_before_2000 = df[df[\"year\"] < 2000]\ndf_before_2000 = df_before_2000.groupby(\"state\").mean()","68cffa33":"df_after_2000 = df[df[\"year\"] > 2000]\ndf_after_2000 = df_after_2000.groupby(\"state\").mean()","09990ec2":"result = pd.concat([df_before_2000, india], axis=1, sort=False)\nresult = result [result[\"geometry\"] != None]\nresult = result [result[\"year\"] > 0]\nfrom geopandas import GeoDataFrame\ncrs = {'init': 'epsg:4326'}\ngdf = GeoDataFrame(result, crs=crs, geometry=result [\"geometry\"])\ngdf['centroid'] = gdf.geometry.centroid\nfig,ax = plt.subplots(figsize=(20,10))\ngdf.plot(column='so2',ax=ax,alpha=0.4,edgecolor='black',cmap='cool', legend=True)\nplt.title(\"Mean So2 before 2000\")\nplt.axis('off')\n\nfor x, y, label in zip(gdf.centroid.x, gdf.centroid.y, gdf.index):\n    ax.annotate(label, xy=(x, y), xytext=(3,3), textcoords=\"offset points\",color='gray')","80243fc4":"result = pd.concat([df_after_2000, india], axis=1, sort=False)\nresult = result [result[\"geometry\"] != None]\nresult = result [result[\"year\"] > 0]\nfrom geopandas import GeoDataFrame\ncrs = {'init': 'epsg:4326'}\ngdf = GeoDataFrame(result, crs=crs, geometry=result [\"geometry\"])\ngdf['centroid'] = gdf.geometry.centroid\nfig,ax = plt.subplots(figsize=(20,10))\ngdf.plot(column='so2',ax=ax,alpha=0.4,edgecolor='black',cmap='cool', legend=True)\nplt.title(\"Mean So2 after 2000\")\nplt.axis('off')\n\nfor x, y, label in zip(gdf.centroid.x, gdf.centroid.y, gdf.index):\n    ax.annotate(label, xy=(x, y), xytext=(3,3), textcoords=\"offset points\",color='gray')","5460be56":"df_so2 = df[[\"date\", \"so2\"]]\ndf_so2 = df_so2.set_index(\"date\")\ndf_so2 = df_so2.dropna()","fa2dd839":"df_so2_resample = df_so2.resample(rule = \"M\").mean().ffill()","53b0a0c1":"df_so2_resample.plot(figsize = (20,10))","e142764c":"df_so2_resample[\"so2\"].resample(\"A\").mean().plot.bar(figsize = (20,10))","2418abbc":"df_so2_resample.plot(figsize = (20,10))\ndf_so2_resample.rolling(window = 7).mean()[\"so2\"].plot(figsize = (20,10))","c06cc4e9":"df_so2_resample[\"EWMA-7\"] = df_so2_resample[\"so2\"].ewm(span=7).mean()","c9c576bc":"df_so2_resample.plot(figsize = (20,10))","2aa9095e":"from statsmodels.tsa.seasonal import seasonal_decompose\nresult = seasonal_decompose(df_so2_resample[\"so2\"], model = \"multiplicative\") ","0a8bae8c":"fig = result.plot()","b7223167":"from statsmodels.tsa.stattools import adfuller\nresult = adfuller(df_so2_resample[\"so2\"])\nprint('Augmented Dickey-Fuller Test:')\nlabels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n\nfor value,label in zip(result,labels):\n    print(label+' : '+str(value) )\n    \nif result[1] <= 0.05:\n    print(\"strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\")\nelse:\n    print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")","f135fa04":"df_so2_resample[\"so2_first_diff\"] = df_so2_resample[\"so2\"] - df_so2_resample[\"so2\"].shift(7)\n# CHECK\nresult = adfuller(df_so2_resample[\"so2_first_diff\"].dropna() )\nprint('Augmented Dickey-Fuller Test:')\nlabels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n\nfor value,label in zip(result,labels):\n    print(label+' : '+str(value) )\n    \nif result[1] <= 0.05:\n    print(\"strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\")\nelse:\n    print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")","97eb751f":"df_so2_resample[\"so2_first_diff\"].plot(figsize = (20,10))","78c97f4e":"df_so2_resample[\"so2_second_diff\"] = df_so2_resample[\"so2_first_diff\"] - df_so2_resample[\"so2_first_diff\"].shift(7)\ndf_so2_resample[\"so2_second_diff\"].plot(figsize = (20,10))","5af6adcc":"import statsmodels.api as sm\n\nmodel = sm.tsa.statespace.SARIMAX(df_so2_resample[\"so2\"],order=(0,1,0), seasonal_order=(1,1,1,48))\nresults = model.fit()\nprint(results.summary())\nresults.resid.plot()","23ac49ef":"results.resid.plot(kind='kde')","bcd0c9f5":"df_so2_resample['forecast'] = results.predict(start = 250, end= 400, dynamic= True)  \ndf_so2_resample[['so2','forecast']].plot(figsize=(20,10))","e7cdafe4":"from pandas.tseries.offsets import DateOffset\nfuture_dates = [df_so2_resample.index[-1] + DateOffset(months=x) for x in range(0,24) ]\nfuture_dates_df = pd.DataFrame(index=future_dates[1:],columns=df_so2_resample.columns)\nfuture_df = pd.concat([df_so2_resample,future_dates_df])\nfuture_df['forecast2'] = results.predict(start = 348, end = 540, dynamic= True)  \nfuture_df[['so2', 'forecast2']].plot(figsize=(20, 10)) ","87539d9e":"## Trends by regions","e00ac57c":"#### Check with known data ","a785683e":"# Forecast\n\n* so2","910f7a58":"* Match the names of states between the two datasets","50602967":"* Esponential weighted moving average EWMA\napply more weight to value more recent","3894055b":"* Simple Moving Average","3d8b095d":"* This data is combined(across the years and states) and largely clean version of the Historical Daily Ambient Air Quality Data released by the Ministry of Environment and Forests and Central Pollution Control Board of India under the National Data Sharing and Accessibility Policy (NDSAP).\n* Detect pollution trends","5471b1bf":"# Time Series Analysis\n\n* so2 Sulfur dioxide","83896082":"## ARIMA and Seasonal ARIMA\n\n#### Autoregressive Integrated Moving Averages\n\n* https:\/\/people.duke.edu\/~rnau\/411arim3.htm ARIMA explained\n\n* Make the time series data stationary\n* Plot the Correlation and AutoCorrelation Charts\n* Construct the ARIMA Model\n* Use the model to make predictions\n\n#### Testing the Stationarity\n\nBasically, we are trying to whether to accept the Null Hypothesis **H0** (that the time series has a unit root, indicating it is non-stationary) or reject **H0** and go with the Alternative Hypothesis (that the time series has no unit root and is stationary).\n\nWe end up deciding this based on the p-value return.\n\n* A small p-value (typically \u2264 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis.\n\n* A large p-value (> 0.05) indicates weak evidence against the null hypothesis, so you fail to reject the null hypothesis.\n\n","f1cd618d":"### ETS\n\n","0fb5d78b":"# EDA\n\n## Null Values","8ae21f77":"* the data is seasonal ---> use Seasonal ARIMA","e1e2f4b1":"## no2 \n\nNitrogen dioxide","e8d255cb":"## Type","be894bdb":"## Using the Seasonal ARIMA model","4738a9de":"### ETS Decomposition (Error Trend Seasonality)","e0374b31":"### Analysis by type and pollution means","b1645365":"* agency\u2019s name have nothing to do with how much polluted the state is. \n* stn_code is also unnecessary.\n* date and sampling_date are similar\n* location_monitoring_station ","523d99ef":"* it would make sense to fill the missing values with the mean between the last two available previous and next value?","4aa42ca9":"* **stn_code** : Station code. A code given to each station that recorded the data.\n* **sampling_date** : The date when the data was recorded.\n* **state** : It represents the states whose air quality data is measured.\n* **location** : It represents the city whose air quality data is measured.\n* **agency** : Name of the agency that measured the data.\n* **type** : The type of area where the measurement was made.\n* **so2** : The amount of Sulphur Dioxide measured.\n* **no2** : The amount of Nitrogen Dioxide measured\n* **rspm** : Respirable Suspended Particulate Matter measured.\n* **spm** : Suspended Particulate Matter measured.\n* **location_monitoring_station** : It indicates the location of the monitoring area.\n* **pm2_5** : It represents the value of particulate matter measured.\n* **date** : It represents the date of recording (It is cleaner version of \u2018sampling_date\u2019 feature)\n","935cdbf0":"## PIVOT tables","41c2174e":"* two maps show trends in So2 pollution mean","2a6247af":"# Geoplotting","5f53aaf8":"## SPM\n\n Suspended Particulate Matter","0b95723d":"## SO2\n\nSulfur dioxide"}}