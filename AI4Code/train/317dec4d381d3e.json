{"cell_type":{"1cf031e2":"code","7d6ec163":"code","8a478b1c":"code","4fbe6bb0":"code","7b861d0c":"code","f2577f35":"code","959457b6":"code","6b520487":"code","c46549eb":"code","5d2b51d1":"code","356b87f5":"markdown","21c5ab42":"markdown","6879703f":"markdown","02c2e68a":"markdown","1d89c061":"markdown","04bf3132":"markdown","5b002948":"markdown","31189f7f":"markdown"},"source":{"1cf031e2":"import numpy as np\nimport json\nimport os\nfrom tqdm.notebook import tqdm, trange\nimport ipywidgets as widgets\nimport matplotlib.pyplot as plt\nfrom collections import Counter, defaultdict\n\nfrom sklearn.feature_extraction import DictVectorizer\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport nltk\nfrom nltk.tag import pos_tag\n\n# Use GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'PyTorch device: {device}\\n')\n\n# List out data files\nprint('Data files:')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nmodel_files = \"\/kaggle\/input\/yelp-reviews-pos\/\"\nload_from_memory = True","7d6ec163":"dataset_filename = '\/kaggle\/input\/sp-21-cs-w182-nlp-project\/yelp_review_training_dataset.jsonl'\ndata_raw = []\nwith open(dataset_filename, 'r') as file:\n    for line in tqdm(file):\n        data_raw.append(json.loads(line))\ndata_raw = tuple(data_raw)\nprint(len(data_raw))","8a478b1c":"index_70 = int(0.7 * len(data_raw))\nindex_90 = int(0.9 * len(data_raw))\n\nraw_train = data_raw[:index_70]\nraw_valid = data_raw[index_70:index_90]\nraw_test  = data_raw[index_90:]","4fbe6bb0":"# Count words\nvocab_size = 200\nif load_from_memory:\n    word_to_index = torch.load(model_files + 'w2i.pt')\n    pos_to_index = torch.load(model_files + 'p2i.pt')\nif not load_from_memory:\n    word_counter = Counter()\n    pos_counter = Counter()\n    for review in tqdm(raw_train, desc='Counting words: '):\n        # TODO: Add more preprocessing steps here\n        words = review['text'].lower().split()\n        word_counter.update(words)\n        for _, pos in pos_tag(words):\n            pos_counter.update(pos)\n\n    # Convert counts to a dict mapping words to indices\n    vocab = tuple(map(lambda x: x[0], word_counter.most_common(vocab_size)))\n    word_to_index = {word: i for i, word in enumerate(vocab)}\n\n    # Print out some sanity checks\n    print(f'#{1:>6} most common word: {vocab[0]}')\n    print(f'#{vocab_size:>6} most common word: {vocab[-1]}')\n\n    # Convert counts to a dict mapping words to indices\n    vocab = tuple(map(lambda x: x[0], pos_counter.most_common(vocab_size)))\n    pos_to_index = {word: i for i, word in enumerate(vocab)}\n\n    # Print out some sanity checks\n    print(f'#{1:>6} most common word: {vocab[0]}')\n    print(f'#{10:>6} most common word: {vocab[-1]}')\n    \n    # Save the counters\n    torch.save(word_to_index, 'w2i.pt')\n    torch.save(pos_to_index, 'p2i.pt')","7b861d0c":"def convert_raw_to_tensor(raw_data, word_to_index, device='cpu'):\n    \"\"\"Converts raw data into PyTorch tensors.\n    \n    Args:\n        raw_data: A list of dicts with fields 'text' and 'rating'.\n        word_to_index: A dict mapping words to indices.\n        device: Which device to send the tensors to. Defaults to 'cpu'.\n    \n    Returns:\n        tuple: The first element is the design matrix (X), the second is\n            the label matrix (Y).\n    \"\"\"\n    vectorizer = DictVectorizer(sparse=False)\n    vectorizer.vocabulary_ = word_to_index\n    raw_counts = map(Counter, [review['text'].split() for review in raw_data])\n    X = torch.tensor(vectorizer.transform(raw_counts), dtype=torch.long).to(device)\n    y = torch.tensor([review['stars'] - 1 for review in raw_data], dtype=torch.long).to(device)\n    return X, y\n\ndef convert_pos_to_tensor(raw_data, pos_to_index, device='cpu'):\n    \"\"\"Converts raw data into PyTorch tensors representing parts of speech.\n    \n    Args:\n        raw_data: A list of dicts with fields 'text' and 'rating'.\n        word_to_index: A dict mapping words to indices.\n        device: Which device to send the tensors to. Defaults to 'cpu'.\n    \n    Returns:\n        X: a tensor of the pos vectors \n    \"\"\"\n    vectorizer = DictVectorizer(sparse=False)\n    vectorizer.vocabulary_ = pos_to_index\n    words_list = [review['text'].split() for review in raw_data]\n    pos_list = []\n    for words in tqdm(words_list):\n        new_pos = []\n        for _, pos in pos_tag(words):\n            new_pos.append(pos)\n        pos_list.append(new_pos)\n    raw_counts = map(Counter, pos_list)\n    X = torch.tensor(vectorizer.transform(raw_counts), dtype=torch.long).to(device)\n    return X","f2577f35":"if load_from_memory:\n    # Load the tensors\n    # NOTE: you have to do an annoying thing and press Add data, and then add the output\n    #       from the saved version of the notebook to retrieve those outputs\n    text_names = ['X_train','y_train', 'X_valid', 'y_valid', 'X_test','y_test']\n    pos_names = ['X_pos_train', 'X_pos_valid', 'X_pos_test']\n    text_tensors = {}\n    pos_tensors = {}\n    for name in text_names:\n        text_tensors[name] = torch.load(f'{model_files}{name}.pt')\n    for name in pos_names:\n        pos_tensors[name] = torch.load(f'{model_files}{name}.pt')\n\nif not load_from_memory:\n    # Convert to text\n    X_train, y_train = convert_raw_to_tensor(raw_train, word_to_index)\n    X_valid, y_valid = convert_raw_to_tensor(raw_valid, word_to_index)\n    X_test,  y_test  = convert_raw_to_tensor(raw_test, word_to_index)\n\n    # Convert to pos\n    X_pos_train = convert_pos_to_tensor(raw_train, pos_to_index)\n    X_pos_valid = convert_pos_to_tensor(raw_valid, pos_to_index)\n    X_pos_test  = convert_pos_to_tensor(raw_test, pos_to_index)\n    \n    # Save the tensors of the datasets\n    # NOTE: to ensure persistence, save this version of the notebook after running this cell.\n    text_tensors = [(X_train, 'X_train'), (y_train, 'y_train'), (X_valid, 'X_valid'), \n                    (y_valid, 'y_valid'), (X_test, 'X_test'), (y_test, 'y_test')]\n    pos_tensors = [(X_pos_train, 'X_pos_train'), (X_pos_valid, 'X_pos_valid'), (X_pos_test, 'X_pos_test')]\n    for tensor, name in text_tensors:\n        torch.save(tensor, f'{name}.pt')\n    for tensor, name in pos_tensors:\n        torch.save(tensor, f'{name}.pt')\n        \n    text_tensors_dict = {}\n    pos_tensors_dict = {}\n    for tensor, name in text_tensors:\n        text_tensors_dict[name] = tensor\n    for tensor, name in pos_tensors:\n        pos_tensors_dict[name] = tensor\n    text_tensors = text_tensors_dict\n    pos_tensors = pos_tensors_dict\n\nif device.type == 'cuda':\n    for k, v in text_tensors.items():\n        text_tensors[k] = v.cuda()\n    for k, v in pos_tensors.items():\n        pos_tensors[k] = v.cuda()","959457b6":"class TextClassificationModel(nn.Module):\n    \"\"\"\n    Parts of speech linear model.\n    \"\"\"\n\n    def __init__(self, vocab_size, embed_dim, num_class):\n        super(TextClassificationModel, self).__init__()\n        self.word_embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n        self.pos_embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n        self.fc = nn.Linear(embed_dim * 2, num_class)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.5\n        self.word_embedding.weight.data.uniform_(-initrange, initrange)\n        self.pos_embedding.weight.data.uniform_(-initrange, initrange)\n        self.fc.weight.data.uniform_(-initrange, initrange)\n        self.fc.bias.data.zero_()\n\n    def forward(self, text, pos):\n        word_embedded = self.word_embedding(text)\n        pos_embedded = self.pos_embedding(pos)\n        concat_embeds = torch.cat((word_embedded, pos_embedded), axis=1)\n        lin = self.fc(concat_embeds)\n        return lin\n\nclass TextClassificationModel2(nn.Module):\n    \"\"\"\n    Parts of speech parallel model.\n    \"\"\"\n\n    def __init__(self, vocab_size, embed_dim, num_class):\n        super(TextClassificationModel2, self).__init__()\n        self.word_embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n        self.pos_embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n        self.fc_word = nn.Linear(embed_dim, num_class)\n        self.fc_pos = nn.Linear(embed_dim, num_class)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.5\n        self.word_embedding.weight.data.uniform_(-initrange, initrange)\n        self.pos_embedding.weight.data.uniform_(-initrange, initrange)\n        for layer in [self.fc_word, self.fc_pos]:\n            layer.weight.data.uniform_(-initrange, initrange)\n            layer.bias.data.zero_()\n\n    def forward(self, text, pos):\n        word_embedded = self.word_embedding(text)\n        pos_embedded = self.pos_embedding(pos)\n        word_lin = self.fc_word(word_embedded)\n        pos_lin = self.fc_pos(pos_embedded)\n        avg = (word_lin + pos_lin) \/ 2\n        return avg","6b520487":"# Hyperparameters\nEMBED_DIM = 100\nLEARNING_RATE = 0.0025\nNUM_EPOCHS = 20\nBATCH_SIZE = 50\n\n# Initializing model, loss function, and optimizer\nmodel = TextClassificationModel(vocab_size, EMBED_DIM, 5).to(device)\nmodel2 = TextClassificationModel2(vocab_size, EMBED_DIM, 5).to(device)\nmodels = [model, model2]\nmodel_loss = []\nmodel_acc_valid = []\nmodel_acc_train = []\nfor model in models:\n    losses = []\n    accuracies_valid = []\n    accuracies_train = []\n    loss_function = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n\n    # Training\n    num_iters = text_tensors['X_train'].shape[0] \/\/ BATCH_SIZE\n    epoch_trange = trange(NUM_EPOCHS, desc='Epochs: ')\n    for epoch in epoch_trange:\n        epoch_losses = []\n        epoch_num_correct = 0\n        for iteration in trange(num_iters, desc='Iteration: ', leave=False):\n            # Get predicted labels\n            start_ind = iteration * BATCH_SIZE\n            end_ind = start_ind + BATCH_SIZE\n            y_pred = model(text_tensors['X_train'][start_ind:end_ind],\n                           pos_tensors['X_pos_train'][start_ind:end_ind])\n\n            # Compute training accuracy\n            with torch.no_grad():\n                epoch_num_correct += torch.sum(torch.argmax(y_pred, 1) == text_tensors['y_train'][start_ind:end_ind])\n\n            # Compute and print loss\n            loss = loss_function(y_pred, text_tensors['y_train'][start_ind:end_ind])\n            epoch_losses.append(loss.item())\n\n            # Zero gradients, perform a backward pass, and update the weights\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # Record the loss\n        epoch_mean_loss = np.mean(epoch_losses)\n        losses.append(epoch_mean_loss)\n        epoch_losses.clear()\n        epoch_trange.set_postfix({'loss': f'{epoch_mean_loss:<.7}'})\n\n        # Record training accuracy\n        accuracies_train.append(epoch_num_correct.item() \/ (num_iters * BATCH_SIZE))\n\n        # Compute and record validation accuracy\n        model.eval()\n        with torch.no_grad():\n            y_pred = model(text_tensors['X_valid'], pos_tensors['X_pos_valid'])\n            epoch_accuracy_valid = torch.sum(torch.argmax(y_pred, 1) == text_tensors['y_valid']) \/ text_tensors['y_valid'].size(0)\n            accuracies_valid.append(epoch_accuracy_valid.item())\n        model.train()\n\n    # Print final statistics\n    print(f\"\"\"Final results:\n        Loss: {losses[-1]}\n        Accuracy:\n            Training:   {accuracies_train[-1]:<1.6}\n            Validation: {accuracies_valid[-1]:<1.6}\n    \"\"\")\n    model_loss.append(losses)\n    model_acc_valid.append(accuracies_valid)\n    model_acc_train.append(accuracies_train)","c46549eb":"for i in range(len(models)):\n    plt.figure(figsize=(20, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(model_acc_train[i], label='Training Accuracy')\n    plt.plot(model_acc_valid[i], label='Validation Accuracy')\n    plt.title('Accuracy vs. Epoch')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(model_loss[i])\n    plt.title('Loss vs. Epoch')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.grid()\n\nplt.show()","5d2b51d1":"for j in range(2):\n    for i in range(5):\n        pred = models[j](text_tensors['X_train'][i:i+1], pos_tensors['X_pos_train'][i:i+1])\n        true = text_tensors['y_train'][i:i+1]\n        loss = loss_function(pred, true)\n        print(f\"Outputs: {pred.detach().to('cpu').numpy()[0]}\")\n        print(f\"True class (0-indexed): {true.detach().to('cpu').numpy()[0]}\")\n        print(f\"CE loss: {loss.detach().to('cpu').numpy()} \\n\")","356b87f5":"# Parts of Speech tagging\n\nThis variation uses the parts of speech features of the text. There are three ways we are proposing to use parts of speech:\n* (Suggested in SentiTurkNetget, implemented in this notebook) embedded vector for the sent, similar to bag of words, but instead bag of pos\n* identify different meanings of words\n* more adjectives means more polar review (guess)\n","21c5ab42":"# Creating a Model\n\nWe'll start by using a basic fully-connected neural network.\n\nNote that instead of performing a regression model, we will be using a categorical one. Among the reasons for this are:\n- Reviewers can only select discrete ratings.\n- Ratings are often not given on a linear scale, contrary to what the system may suggest.\n\nTODO:\n- Automatic hyperparameter tuning with RayTune ([link](https:\/\/pytorch.org\/tutorials\/beginner\/hyperparameter_tuning_tutorial.html#))\n- add pos vecs","6879703f":"# Data Preprocessing\n\nPartitioning the data into a training (70%), validation (20%), and testing (10%) set,","02c2e68a":"Both models are performing similarly, but the first one, with concatenated features into a shared linear layer rather than two parallel linear layers, seems to work slightly better. This makes sense, as the word\/pos features are more able to interact with each other in their losses.","1d89c061":"Then, the training data can be used to create the dictionary used for the bag-of-words model. First, we split the sentences into individual words and lower-case each word.\n\nAdditionally, we will track the parts of speech for each word.\nTODO:\n- [Spell-check](https:\/\/stackoverflow.com\/questions\/13928155\/spell-checker-for-python)\n- Remove very common words (e.g. \"the\", \"and\") [an example](https:\/\/www.kaggle.com\/c\/word2vec-nlp-tutorial\/overview\/part-1-for-beginners-bag-of-words)","04bf3132":"# Importing Libraries and Data\n## **NOTE: PoS tagging takes a while, so the relevant data has been preloaded in version 6 of the notebook. Click Add data at the top right to add the files from Yelp-Reviews-POS. variable load_from_memory should be True.**","5b002948":"We can now write a function that converts text and ratings into PyTorch tensors using the `word_to_index` dictionary obtained earlier.","31189f7f":"Assuming that the [sp-21-cs-w182-nlp-project](https:\/\/www.kaggle.com\/claytsay\/sp-21-cs-w182-nlp-project) dataset has already been loaded, the locations of the files would be\n\n- `\/kaggle\/input\/sp-21-cs-w182-nlp-project\/sample_eval.jsonl`\n- `\/kaggle\/input\/sp-21-cs-w182-nlp-project\/yelp_review_training_dataset.jsonl`\n- `\/kaggle\/input\/sp-21-cs-w182-nlp-project\/test_submission.py`\n- `\/kaggle\/input\/sp-21-cs-w182-nlp-project\/output.jsonl`\n\nLoading in the data into a tuple of Python dictionaries,"}}