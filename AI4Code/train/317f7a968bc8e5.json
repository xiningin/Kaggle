{"cell_type":{"5e347088":"code","e5f58525":"code","5a856133":"code","9c875ef1":"code","a317b8ec":"code","069153db":"code","bd65b4e6":"code","946a5053":"code","a557092d":"code","2da5918d":"code","4dd63405":"code","cc2cdf78":"code","247e658c":"code","82bcb15b":"markdown"},"source":{"5e347088":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt","e5f58525":"df = pd.read_csv('\/kaggle\/input\/heart-failure-prediction\/heart.csv')","5a856133":"df.head(3)","9c875ef1":"df['target'] = np.where(df['HeartDisease']==0, 0, 1)\n\n# Drop unused features.\ndf = df.drop(columns=['HeartDisease'])","a317b8ec":"train, val, test = np.split(df.sample(frac=1), [int(0.8*len(df)), int(0.9*len(df))])","069153db":"def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n    df = dataframe.copy()\n    labels = df.pop('target')\n    df = {key: value[:,tf.newaxis] for key, value in dataframe.items()}\n    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(batch_size)\n    return ds","bd65b4e6":"batch_size = 5\ntrain_ds = df_to_dataset(train, batch_size=batch_size)","946a5053":"[(train_features, label_batch)] = train_ds.take(1)\nprint('Every feature:', list(train_features.keys()))\nprint('A batch of ages:', train_features['Sex'])\nprint('A batch of targets:', label_batch )","a557092d":"def get_normalization_layer(name, dataset):\n  # Create a Normalization layer for the feature.\n    normalizer = layers.Normalization(axis=None)\n\n  # Prepare a Dataset that only yields the feature.\n    feature_ds = dataset.map(lambda x, y: x[name])\n\n  # Learn the statistics of the data.\n    normalizer.adapt(feature_ds)\n\n    return normalizer\ndef get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n  # Create a layer that turns strings into integer indices.\n    if dtype == 'string':\n        index = layers.StringLookup(max_tokens=max_tokens)\n  # Otherwise, create a layer that turns integer values into integer indices.\n    else:\n        index = layers.IntegerLookup(max_tokens=max_tokens)\n\n  # Prepare a `tf.data.Dataset` that only yields the feature.\n    feature_ds = dataset.map(lambda x, y: x[name])\n\n  # Learn the set of possible values and assign them a fixed integer index.\n    index.adapt(feature_ds)\n\n  # Encode the integer indices.\n    encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n\n  # Apply multi-hot encoding to the indices. The lambda function captures the\n  # layer, so you can use them, or include them in the Keras Functional model later.\n    return lambda feature: encoder(index(feature))\ndef plot_loss(history):\n    plt.plot(history.history['loss'], label='loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.ylim([0, 1])\n    plt.xlabel('Epoch')\n    plt.ylabel('Error [rating]')\n    plt.legend()\n    plt.grid(True)","2da5918d":"batch_size = 256\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)","4dd63405":"all_inputs = []\nencoded_features = []\n\n# Numerical features.\nfor header in ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak']:\n    numeric_col = tf.keras.Input(shape=(1,), name=header)\n    normalization_layer = get_normalization_layer(header, train_ds)\n    encoded_numeric_col = normalization_layer(numeric_col)\n    all_inputs.append(numeric_col)\n    encoded_features.append(encoded_numeric_col)","cc2cdf78":"categorical_cols = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n\nfor header in categorical_cols:\n    categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n    encoding_layer = get_category_encoding_layer(name=header,\n                                               dataset=train_ds,\n                                               dtype='string',\n                                               max_tokens=5)\n    encoded_categorical_col = encoding_layer(categorical_col)\n    all_inputs.append(categorical_col)\n    encoded_features.append(encoded_categorical_col)","247e658c":"all_features = tf.keras.layers.concatenate(encoded_features)\nx = tf.keras.layers.Dense(45, activation=\"relu\")(all_features)\nx = tf.keras.layers.Dropout(0.5)(x)\nx = tf.keras.layers.BatchNormalization()(x)\noutput = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n\nmodel = tf.keras.Model(all_inputs, output)\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\nmodel.compile(optimizer=tf.keras.optimizers.Adam(\n                learning_rate=0.005),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=[\"accuracy\"])\n\nhistory = model.fit(train_ds, epochs=500, verbose=2, validation_data=val_ds, callbacks=[callback])\nloss, accuracy = model.evaluate(test_ds)\nprint(\"Accuracy: \", accuracy,\" Loss:\", loss)\nplot_loss(history)","82bcb15b":"Read csv file with data"}}