{"cell_type":{"ca960193":"code","7c05d24c":"code","b960ae46":"code","2ee1a9b9":"code","03e39f2c":"code","81caf488":"code","1759ac53":"code","fb918377":"code","08da1a69":"code","dd87bd02":"code","2c51422f":"code","f15b6f5a":"code","1b3c3af2":"code","ceba1b4e":"code","f5d6d1b4":"code","90c64328":"code","38e623e5":"code","f6f61370":"code","7200ab9a":"code","e2f189fc":"code","3d959f25":"code","8e7961a9":"code","fb09c4f3":"code","90622bd5":"code","d1a75c77":"code","19f16288":"code","3c0665da":"code","39ce3679":"code","cc0ed630":"code","4420a0f6":"code","bacde9b8":"code","665d0e02":"code","d655de2e":"code","62883d64":"code","65b6fe70":"code","482ac664":"code","2ddf9f4b":"code","51c8437c":"code","d5863eb5":"code","9332749c":"code","6bc7be2e":"code","b0ec7aaa":"code","6f2f6758":"code","7c332257":"code","01b8732c":"markdown","83856142":"markdown","f468291c":"markdown","d6e713cf":"markdown","06e4cbbd":"markdown","42d9ceec":"markdown","5d70e10a":"markdown","ae56226b":"markdown","fbdb6ac1":"markdown","d56f52ca":"markdown","99267e98":"markdown","40a15810":"markdown","ffa7af4e":"markdown","d978162f":"markdown","e333e718":"markdown","18b81ebf":"markdown","a1babca5":"markdown","047771ac":"markdown","559605b1":"markdown","fe25f103":"markdown","907af067":"markdown","7cd81867":"markdown","6a78cf70":"markdown","ac1c5145":"markdown","eb6107fd":"markdown","84424874":"markdown","95a91fc3":"markdown","8a4f9359":"markdown","8d7ede8b":"markdown","c179d5bf":"markdown","8314f9aa":"markdown","9adc73c9":"markdown","0b39647c":"markdown","b08b0810":"markdown","98e9be65":"markdown","23e5b77a":"markdown"},"source":{"ca960193":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","7c05d24c":"train = pd.read_csv(\"..\/input\/machinehack-financial-risk-prediction\/Train.csv\")\ntrain.head()","b960ae46":"train.shape","2ee1a9b9":"train.columns","03e39f2c":"train.isnull().sum()","81caf488":"train.dtypes","1759ac53":"train.describe()","fb918377":"train.columns","08da1a69":"plt.figure(figsize=(16,6))\ntrain.boxplot(column=['Location_Score', 'Internal_Audit_Score',\n       'External_Audit_Score', 'Fin_Score', 'Loss_score'])","dd87bd02":"plt.figure(figsize=(14,8))\nclr=['red','blue','lime','orange','teal']\ncolumns = ['Location_Score', 'Internal_Audit_Score', 'External_Audit_Score', 'Fin_Score', 'Loss_score']\nfor i,j in zip(range(1,6),columns):\n    plt.subplot(2,3,i)\n    train[j].hist(color = clr[i-1], label=j)\n    plt.legend()\n    ","2c51422f":"plt.figure(figsize=(14,8))\ntrain[columns].plot(kind='density', subplots=True, \n                                                    layout=(2,3), sharex=False,\n                                                    sharey=False, figsize=(14,6))\nplt.show()","f15b6f5a":"plt.figure(figsize=(14,5))\n\nplt.subplot(1,2,1)\ntrain.City.value_counts().plot(kind='bar', label = 'City')\nplt.legend()\n\nplt.subplot(1,2,2)\ntrain.Past_Results.value_counts().plot(kind='bar', label = 'Past_Results')\nplt.legend()\n","1b3c3af2":"train.IsUnderRisk.value_counts().plot(kind='bar', color=['green', 'orange'])","ceba1b4e":"plt.figure(figsize=(20,8))\ncorr = train.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","f5d6d1b4":"import seaborn as sns\nsns.set(style=\"ticks\")\n\nsns.pairplot(train)","90c64328":"'''plt.figure(figsize=(14,10))\nclr=['red','blue','green','pink','lime','orange','indigo','teal',\n    'red','blue','green','pink','lime','orange','indigo','teal']\ncols = ['elevation_complaints_ratio', 'elevation_violation_ratio', 'avg_adv_weather_metric',\n        'adv_weather_metric_violation_ratio', 'avg_safety_score', 'safety_control_ratio', \n        'safety_turbulence_ratio', 'avg_complaints', 'avg_control_metric', 'avg_turbulence', \n        'avg_cabin_temp', 'avg_elevation', 'avg_violation', 'Total_Safety_Complaints_control_ratio',\n        'Turbulence_In_gforces_Total_Safety_Complaints_ratio', 'Violations_Total_Safety_Complaints_ratio']\n\nfor i,j in zip(range(1,17),cols):\n    plt.subplot(4,4,i)\n    train_deduplicated[j].hist(color = clr[i-1], label=j)\n    plt.legend()'''","38e623e5":"train.columns","f6f61370":"train.reset_index(drop=True, inplace=True)","7200ab9a":"train.head()","e2f189fc":"x = train.drop(['IsUnderRisk'], axis=1)\ny = train['IsUnderRisk']","3d959f25":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 100)","8e7961a9":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n# feature extraction\nmodel = LogisticRegression(solver='lbfgs')\nrfe = RFE(model, 3)\nfit = rfe.fit(x, y)\n\nprint(\"Num Features: %d\" % fit.n_features_)\nprint(\"Selected Features: %s\" % fit.support_)\nprint(\"Feature Ranking: %s\" % fit.ranking_)\n\n\ndf_feat = pd.DataFrame(fit.ranking_, x.columns)\ndf_feat.rename(columns = {0:\"Feature_Ranking\"}, inplace=True)\n","fb09c4f3":"df_feat.sort_values(by=\"Feature_Ranking\").plot(kind='bar', figsize=(18,7))","90622bd5":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, accuracy_score, roc_auc_score\nfrom sklearn.metrics import classification_report\n","d1a75c77":"from sklearn.tree import DecisionTreeClassifier\n\n#making the instance\nmodel= DecisionTreeClassifier(random_state=1234)\n\n#Hyper Parameters Set\nparam_grid = {'max_features': ['auto', 'sqrt', 'log2'],\n          'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15], \n          'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,11],\n          'random_state':[123]}\n\n# Create grid search object\nclf = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, cv=10)\n\n# Fit on data\nbest_clf_dt = clf.fit(X_train, y_train)\n\n#Predict\npredictions = best_clf_dt.predict(X_test)\n\nprint(\"*******************ACCURACY***************************************************************\")\n#Check Prediction Score\nprint(\"Accuracy of Decision Trees: \",accuracy_score(y_test, predictions))\n\nprint(\"*******************CLASSIFICATION - REPORT***************************************************************\")\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n\n\n","19f16288":"from sklearn.ensemble import RandomForestClassifier\n\n#making the instance\nmodel= RandomForestClassifier(random_state=1234)\n\n#Hyper Parameters Set\nparam_grid = {'criterion':['gini','entropy'],\n          'n_estimators':[10,15,20,25,30],\n          'min_samples_leaf':[1,2,3],\n          'min_samples_split':[3,4,5,6,7], \n          'random_state':[123],\n          'n_jobs':[-1]}\n\n# Create grid search object\nclf = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, cv=10)\n\n# Fit on data\nbest_clf_rf = clf.fit(X_train, y_train)\n\n#Predict\npredictions = best_clf_rf.predict(X_test)\n\n#Check Prediction Score\nprint(\"Accuracy of Random Forest: \",accuracy_score(y_test, predictions))\n\n#Print Classification Report\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n\n","3c0665da":"#RF On Full data\n\n#making the instance\nmodel= RandomForestClassifier(random_state=1234)\n\n#Hyper Parameters Set\nparam_grid = {'criterion':['gini','entropy'],\n          'n_estimators':[10,15,20,25,30],\n          'min_samples_leaf':[1,2,3],\n          'min_samples_split':[3,4,5,6,7], \n          'random_state':[123],\n          'n_jobs':[-1]}\n\n# Create grid search object\nclf = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, cv=10)\n\n# Fit on data\nbest_clf_rf1 = clf.fit(x, y)","39ce3679":"from sklearn import svm\n\n#making the instance\nmodel= svm.SVC()\n\n#Hyper Parameters Set\nparam_grid = {'C': [6,7,8,9,10,11,12], \n          'kernel': ['linear','rbf']}\n\n# Create grid search object\nclf = GridSearchCV(model, param_grid=param_grid, n_jobs=-1)\n\n# Fit on data\nbest_clf_svm = clf.fit(X_train, y_train)\n\n#Predict\npredictions = best_clf_svm.predict(X_test)\n\nprint(\"*******************ACCURACY***************************************************************\")\n#Check Prediction Score\nprint(\"Accuracy of SVM: \",accuracy_score(y_test, predictions))\n\n#Print Classification Report\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n","cc0ed630":"from sklearn.ensemble import AdaBoostClassifier\n\n#making the instance\nmodel= AdaBoostClassifier()\n\n#Hyper Parameters Set\nparam_grid = {'n_estimators':[500,1000,2000],'learning_rate':[.001,0.01,.1]}\n\n# Create grid search object\nclf = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, cv=10)\n\n# Fit on data\nbest_clf_adab = clf.fit(X_train, y_train)\n\n#Predict\npredictions = best_clf_adab.predict(X_test)\n\n#Check Prediction Score\nprint(\"Accuracy of Adaboost Classifier: \",accuracy_score(y_test, predictions))\n\n#Print Classification Report\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n","4420a0f6":"#making the instance\nmodel= AdaBoostClassifier()\n\n#Hyper Parameters Set\nparam_grid = {'n_estimators':[500,1000,2000],'learning_rate':[.001,0.01,.1]}\n\n# Create grid search object\nclf = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, cv=10)\n\n# Fit on data\nbest_clf_adab = clf.fit(x, y)\n","bacde9b8":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\n\nclf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=1)\nclf2 = RandomForestClassifier(n_estimators=50, random_state=1)\nclf3 = GaussianNB()\n\n\n#making the instance\nclf= VotingClassifier(estimators=[\n                                    ('lr', clf1), \n                                    ('rf', clf2), \n                                    ('gnb', clf3)], voting='hard')\n\n# Fit on data\nbest_clf = clf.fit(X_train, y_train)\n\n#Predict\npredictions = best_clf.predict(X_test)\n\n#Check Prediction Score\nprint(\"Accuracy of Voting Classifier: \",accuracy_score(y_test, predictions))\n\n#Print Classification Report\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n","665d0e02":"from sklearn.neural_network import MLPClassifier\nfrom scipy.stats import randint as sp_randint\nfrom random import uniform\n\n#making the instance\nmodel= MLPClassifier()\n\n#Hyper Parameters Set\nparam_grid = {'hidden_layer_sizes': [(sp_randint.rvs(100,600,1),sp_randint.rvs(100,600,1),), \n                                          (sp_randint.rvs(100,600,1),)],\n    'activation': ['tanh', 'relu', 'logistic'],\n    'solver': ['sgd', 'adam', 'lbfgs'],\n    'alpha': [uniform(0.0001, 0.9)],\n    'learning_rate': ['constant','adaptive']}\n\n# Create grid search object\nclf = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, cv=10)\n\n# Fit on data\nbest_clf_mlp = clf.fit(X_train, y_train)\n\n#Predict\npredictions = best_clf_mlp.predict(X_test)\n\n#Check Prediction Score\nprint(\"Accuracy of MLP Classifier: \",accuracy_score(y_test, predictions))\n\n#Print Classification Report\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n","d655de2e":"from xgboost import XGBClassifier\nfrom sklearn import metrics\nmodel = XGBClassifier()\n\n# Fit on data\nbest_clf_xgb = model.fit(X_train, y_train)\n\npredictions = best_clf_xgb.predict(X_test)\n\n#Check Prediction Score\nprint(\"Accuracy of MLP Classifier: \",accuracy_score(y_test, predictions))\n\n#Print Classification Report\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n","62883d64":"# Fit on data\nbest_clf_xgb1 = model.fit(x, y)","65b6fe70":"test = pd.read_csv('..\/input\/machinehack-financial-risk-prediction\/Test.csv')\n\ntest.shape","482ac664":"test.head(5)","2ddf9f4b":"test.columns","51c8437c":"test_for_prediction = test[['City', 'Location_Score', 'Internal_Audit_Score',\n       'External_Audit_Score', 'Fin_Score', 'Loss_score', 'Past_Results']]","d5863eb5":"'''#Predict\n\nprediction_from_dt  = best_clf_dt.predict_proba(test_for_prediction)\ndf_prediction_from_dt = pd.DataFrame(prediction_from_dt)\ndf_prediction_from_dt.to_excel(\"Final_output_prediction_from_dt.xlsx\")\n\nprediction_from_rf  = best_clf_rf.predict_proba(test_for_prediction)\ndf_prediction_from_rf = pd.DataFrame(prediction_from_rf)\ndf_prediction_from_rf.to_excel(\"Final_output_prediction_from_rf.xlsx\")\n'''","9332749c":"'''prediction_from_rf1  = best_clf_rf1.predict_proba(test_for_prediction)\ndf_prediction_from_rf1 = pd.DataFrame(prediction_from_rf1)\ndf_prediction_from_rf1.to_excel(\"Final_output_prediction_from_rf1.xlsx\")\n'''","6bc7be2e":"'''\nprediction_from_adab  = best_clf_adab.predict_proba(test_for_prediction)\ndf_prediction_from_adab = pd.DataFrame(prediction_from_adab)\ndf_prediction_from_adab.to_excel(\"Final_output_prediction_from_adab.xlsx\")\n'''","b0ec7aaa":"def predict_file(model, model_instance, test_data):\n    prediction_var = \"prediction_from\" + model\n    file_name = \"Final_output_prediction_from_\" + model + \".xlsx\"\n    prediction_var  = model_instance.predict_proba(test_data)\n    df_prediction_var = pd.DataFrame(prediction_var)\n    df_prediction_var.to_excel(file_name)\n    print(\"{} created.\".format(file_name))","6f2f6758":"predict_file(\"xgbclassifier\", best_clf_xgb1, test_for_prediction)","7c332257":"'''predict_file(\"mlpclassifier\", best_clf_mlp, test_for_prediction)'''","01b8732c":"## 3.4 New Variables Creation","83856142":"### 6.2.3 SVM","f468291c":"For any organization, even the slightest chance of financial risk can not be ignored. Organizations conduct regular inspections on their expenditures and revenue to make sure that they do not fall below the critical limit. In this hackathon, you as a data scientist must use the given data to predict whether an organization is under a possible financial risk or not.\n\nGiven are 7 distinguishing factors that can provide insight into whether an organization may face a financial risk or not. Your objective as a data scientist is to build a machine learning model that can predict if an organization will fall under the risk using the given features.","d6e713cf":"# 2.4 Bi-variate Analysis","06e4cbbd":"## 2.3 Univariate Analysis\n\nAt this stage, we explore variables one by one. Method to perform uni-variate analysis will depend on whether the variable type is categorical or continuous. Let\u2019s look at these methods and statistical measures for categorical and continuous variables individually:\n\n<b> Continuous Variables:- <\/b> In case of continuous variables, we need to understand the central tendency and spread of the variable. These are measured using various statistical metrics such as Histogram and Bar plots: ","42d9ceec":"Bi-variate Analysis finds out the relationship between two variables. Here, we look for association and disassociation between variables at a pre-defined significance level. We can perform bi-variate analysis for any combination of categorical and continuous variables. The combination can be: Categorical & Categorical, Categorical & Continuous and Continuous & Continuous. Different methods are used to tackle these combinations during analysis process.","5d70e10a":"### 2.3.5 Target Variable Plot","ae56226b":"# Step4: Separating X and Y","fbdb6ac1":"Below are the steps involved to understand, clean and prepare your data for building your predictive model:\n\n1. Variable Identification\n2. Univariate Analysis\n3. Bi-variate Analysis\n4. Missing values treatment\n5. Outlier treatment\n6. Variable transformation\n7. Variable creation","d56f52ca":"# Step3: Data Transformation","99267e98":"### 2.3.2 Plot for Continuous variables","40a15810":"### 6.2.5 Voting Classifier","ffa7af4e":"# Phase1: Model Building On Training Data","d978162f":"### 3.4.2 Plotting Newly Created Variables","e333e718":"# Phase2: Applying Model On Test Data","18b81ebf":"# Step6: Model Building","a1babca5":"## 4.2 Split Data","047771ac":"### 2.4.2 Scatterplot Matrix","559605b1":"## 6.1 Identification Of Best Features","fe25f103":"### 2.3.5 Discrete Variables Plot","907af067":"## 2.2 Data Type Analysis ","7cd81867":"## 6.2 Importing and Model Fitting","6a78cf70":"# Step1: Read Data","ac1c5145":"### 6.2.1 Decision Trees","eb6107fd":"### 2.3.1 Box Plot of CONTINUOUS variables ","84424874":"### 2.4.1 Correlation Matrix Plot","95a91fc3":"### 2.3.3 Histogram Plots Of Continuous Variables ","8a4f9359":"There are various methods used to transform variables. Some of them include square root, cube root, logarithmic, binning, reciprocal and many others. Let\u2019s look at these methods in detail by highlighting the pros and cons of these transformation methods. <br>\n\n<b> Logarithm: <\/b> Log of a variable is a common transformation method used to change the shape of distribution of the variable on a distribution plot. It is generally used for reducing right skewness of variables. Though, It can\u2019t be applied to zero or negative values as well. <br> <br>\n<b> Square \/ Cube root: <\/b> The square and cube root of a variable has a sound effect on variable distribution. However, it is not as significant as logarithmic transformation. Cube root has its own advantage. It can be applied to negative values including zero. Square root can be applied to positive values including zero. <br> <br>\n<b> Binning: <\/b> It is used to categorize variables. It is performed on original values, percentile or frequency. Decision of categorization technique is based on business understanding. For example, we can categorize income in three categories, namely: High, Average and Low. We can also perform co-variate binning which depends on the value of more than one variables.","8d7ede8b":"# Step2: Exploratory Data Analysis","c179d5bf":"# Step5: Creating Train and Test Set In Ratio 80:20","8314f9aa":"# Problem Statement","9adc73c9":"### 2.3.4 Density Plots Of Continuous Variables ","0b39647c":"## 4.1 Re-setting Index Before Splitting","b08b0810":"## 2.1 Missing Data Analysis ","98e9be65":"### 6.2.4 Adaboost","23e5b77a":"### 6.2.2 Random Forest"}}