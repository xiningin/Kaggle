{"cell_type":{"262bafd4":"code","a901f010":"code","030c2255":"code","1e5e4f13":"code","72bdeca3":"code","6ec6319b":"code","2ab23cb1":"code","a35dd36c":"code","fd785a3a":"code","2839a326":"code","4f5f724e":"markdown","e08c396e":"markdown","ec5381c6":"markdown","6d09d004":"markdown","ca939a21":"markdown","cd96e97b":"markdown"},"source":{"262bafd4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a901f010":"import pandas as pd\nimport numpy as np\nimport pickle\nimport nltk\n\nfrom torchtext.vocab import GloVe","030c2255":"train = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")","1e5e4f13":"EMBEDDING_DIM = 300\nEMBEDDING_NAME = \"6B\"","72bdeca3":"glove = GloVe(name=EMBEDDING_NAME, dim=EMBEDDING_DIM)","6ec6319b":"remove_words = nltk.corpus.stopwords.words('english') # Load standard english stopwords\nremove_words.extend([\".\", \",\", \"!\", \"?\", \"'\", \":\", \";\", '\"', \"-\", \"''\", '``']) # The nltk tokenizer will also use these as tokens, but I want to remove them\n\n\ndef get_vocab(text, stopword_removal=False):\n    words = nltk.tokenize.word_tokenize(text.lower())\n    words_clean = [word for word in words]\n    if stopword_removal:\n        words_clean = [word for word in words_clean if word not in remove_words]\n    return words_clean","2ab23cb1":"# The magic happens here, extract GloVe representations for each word\n\ndef get_word_vectors(text, stopword_removal=False, text_len=270):\n    words = get_vocab(text, stopword_removal=stopword_removal)\n    result = np.zeros((text_len, EMBEDDING_DIM))\n    for i, word in enumerate(words):\n        result[i,:] = glove[word]\n    return result","a35dd36c":"print(\"Stentence length\\n\\t* with stopword removal: {}\\n\\t* without stopword removal: {}\".format(train.excerpt.apply(lambda x: len(get_vocab(x, stopword_removal=True))).max(), train.excerpt.apply(lambda x: len(get_vocab(x, stopword_removal=False))).max()))","fd785a3a":"def extract_and_save_representation(data, stopword_removal, text_len, filename):\n    # extract\n    get_word_vectors_lambda = lambda x: get_word_vectors(x, stopword_removal=stopword_removal, text_len=text_len)\n    emb_text = data.excerpt.apply(get_word_vectors_lambda)\n    emb_text = np.stack(emb_text)\n    \n    # save\n    if \"target\" in data.columns: # Test data has no target column\n        pickle.dump({\"id\": data.id.values, \"target\": data.target.values, \"wordvectors\": emb_text}, open(filename, \"wb\"))\n    else:\n        pickle.dump({\"id\": data.id.values, \"wordvectors\": emb_text}, open(filename, \"wb\"))","2839a326":"# without stopword removal\nextract_and_save_representation(train, False, 270, \"train_with_stopwords_glove_{}_{}.pkl\".format(EMBEDDING_NAME, EMBEDDING_DIM))\nextract_and_save_representation(test, False, 270, \"test_with_stopwords_glove_{}_{}.pkl\".format(EMBEDDING_NAME, EMBEDDING_DIM))\n\n# with stopword removal\nextract_and_save_representation(train, True, 143, \"train_without_stopwords_glove_{}_{}.pkl\".format(EMBEDDING_NAME, EMBEDDING_DIM))\nextract_and_save_representation(test, True, 143, \"test_without_stopwords_glove_{}_{}.pkl\".format(EMBEDDING_NAME, EMBEDDING_DIM))\n\n# would be more clean as loop over configurations","4f5f724e":"# In this Notebook, I will preprocess the texts with GloVe and store the resulting vectors for usage in other Notebooks.\n(with and without stopword removal)","e08c396e":"# Look for the maximum sentence length","ec5381c6":"# Load the GloVe pretrained embedding\n\n## Options:\n\n* Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, ca. 822 MB download):\n  * EMBEDDING_NAME = \"6B\"\n  * EMBEDDING_DIM = [50, 100, 200, 300]\n* Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, ca. 1.75 GB download): \n  * EMBEDDING_NAME = \"42B\"\n  * EMBEDDING_DIM = 300\n* Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, ca. 2.03 GB download): \n  * EMBEDDING_NAME = \"840B\"\n  * EMBEDDING_DIM = 300\n* Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, ca. 1.42 GB download):\n  * EMBEDDING_NAME = \"twitter.27B\"\n  * EMBEDDING_DIM = [25, 50, 100, 200]","6d09d004":"## The output now contains 4 files: Train and test data with and without stopwords","ca939a21":"## Load train and test data","cd96e97b":"# First without stopword removal"}}