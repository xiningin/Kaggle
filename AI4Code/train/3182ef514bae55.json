{"cell_type":{"d26719e8":"code","7f24fd28":"code","a0c8a151":"code","24dad98a":"code","2994bb0c":"code","f8e5b001":"code","800951cf":"code","022b5f85":"code","7cb69eae":"code","c3a229dd":"code","0f360e85":"code","951238c4":"code","571c1704":"code","50313953":"code","3d2fb20c":"code","68f03b2b":"code","1ea06360":"code","0017ea34":"code","1ffe2b5c":"code","e9e7fade":"code","f179654f":"code","19a0ab61":"code","d11b9fd0":"code","da02b697":"code","28875d0a":"code","e3138537":"code","d200fc1a":"code","3541c2bc":"code","d6ad26a3":"code","e77327e7":"code","15a5c5ab":"code","7f2592b9":"code","85b13603":"code","1d26a402":"code","a2fd1909":"code","48cca297":"code","f4cdded2":"code","96490d9c":"code","69b893d0":"code","840a48fd":"code","5194b83b":"code","01f17867":"code","49a8d269":"code","ad584310":"code","4071f03d":"code","36680298":"code","b6d2340c":"code","f7f68e8a":"code","0df17c69":"code","1be82355":"code","09858ce2":"code","24e8ad48":"code","6f0d7d04":"code","9ea6f51a":"code","520ecd82":"code","6e0685b0":"code","e408d811":"code","c7027e10":"code","d98c6de7":"code","e9588545":"code","5e601a9c":"code","a707afdc":"code","ac32ecac":"code","829d0f17":"code","3befe936":"code","caa3d5a0":"markdown","2acc70e3":"markdown","660f7b23":"markdown","8a656ebd":"markdown","de050799":"markdown","77a4ef93":"markdown","f3a355c3":"markdown","41613eb9":"markdown","37d3e363":"markdown","6d65069c":"markdown","033a8b86":"markdown","e7f999de":"markdown","ff3c0a61":"markdown"},"source":{"d26719e8":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# from sklearn.cross_validation import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\n# from sklearn.cross_validation import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')","7f24fd28":"from sklearn.model_selection import cross_val_score","a0c8a151":"from sklearn.model_selection import train_test_split","24dad98a":"data =  pd.read_csv(\"..\/input\/adult.csv\")","2994bb0c":"data.head()","f8e5b001":"data.info()","800951cf":"data.isnull().sum()","022b5f85":"# select all categorical variables\ndf_categorical = data.select_dtypes(include=['object'])\n\n# checking whether any other columns contain a \"?\"\ndf_categorical.apply(lambda x: x==\"?\", axis=0).sum()","7cb69eae":"data[data['workclass'] == '?' ].count()","c3a229dd":"data[data['occupation'] == '?' ].count()","0f360e85":"data[data['native.country'] == '?' ].count()","951238c4":"(1836\/32561)\/100","571c1704":"data.count()","50313953":"data = data[data[\"workclass\"] != \"?\" ]","3d2fb20c":"data = data[data[\"occupation\"] != \"?\" ]","68f03b2b":"data = data[data[\"native.country\"] != \"?\" ]","1ea06360":"data.count()","0017ea34":"data.head()","1ffe2b5c":"data[\"income\"].unique()","e9e7fade":"data[\"income\"] = data[\"income\"].map({'<=50K' : 0, '>50K': 1})\ndata.head()","f179654f":"data[\"income\"].unique()","19a0ab61":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\ncatogorical_data = data.select_dtypes(include =['object'])","d11b9fd0":"catogorical_data.head()","da02b697":"catogorical_data = catogorical_data.apply(le.fit_transform)","28875d0a":"catogorical_data.head()","e3138537":"data = data.drop(catogorical_data.columns, axis=1)\ndata = pd.concat([data, catogorical_data], axis=1)\ndata.head()","d200fc1a":"data.info()","3541c2bc":"data['income'] = data['income'].astype('category')\n","d6ad26a3":"x=data.drop('income',axis=1)\ny=data['income']\n#Train & Test split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state= 476)","e77327e7":"tree = DecisionTreeClassifier()\nmodel_tree = tree.fit(x_train,y_train)\nmodel_tree","15a5c5ab":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score","7f2592b9":"model_tree = tree.fit(x_train,y_train)\npred_tree = tree.predict(x_test)\na1 = accuracy_score(y_test,pred_tree)\nprint(\"The Accuracy of Desicion Tree is \", a1)","85b13603":"confusion_matrix(y_test,pred_tree)","1d26a402":"print(classification_report(y_test, pred_tree))","a2fd1909":"rf = RandomForestClassifier()\nmodel_rf = rf.fit(x_train,y_train)\npred_rf = rf.predict(x_test)\na2 = accuracy_score(y_test, pred_rf)\nprint(\"The Accuracy of Random Forest is \", a2)","48cca297":"from sklearn.linear_model import LogisticRegression\nlg = LogisticRegression()\n\nmodel_lg = lg.fit(x_train,y_train)\npred_lg = lg.predict(x_test)\na3 = accuracy_score(y_test, pred_lg)\nprint(\"The Accuracy of logistic regression is \", a3)","f4cdded2":"from sklearn.neighbors import KNeighborsClassifier \nknn = KNeighborsClassifier()","96490d9c":"model_knn =knn.fit(x_train,y_train) \npred_knn = knn.predict(x_test)\na4 = accuracy_score(y_test, pred_knn)\nprint(\"The Accuracy of KNN is \", a4)","69b893d0":"rf_param = {\n    \"n_estimators\": [25,50,100],\n    \"criterion\" : [\"gini\"],\n    \"max_depth\" : [3,4,5,6],\n    \"max_features\" : [\"auto\",\"sqrt\",\"log2\"],\n    \"random_state\" : [123]\n}","840a48fd":"GridSearchCV(rf, rf_param, cv = 5)","5194b83b":"grid =GridSearchCV(rf, rf_param, cv = 5)","01f17867":"grid.fit(x_train,y_train).best_params_","49a8d269":"rf1 = RandomForestClassifier(criterion = 'gini',\n    max_depth = 6,\n    max_features = 'auto',\n    n_estimators = 100,\n    random_state = 123)\nmodel_rf1 = rf1.fit(x_train,y_train)\npred_rf1 = rf1.predict(x_test)\naccuracy_score(y_test, pred_rf1)","ad584310":"cross_val_score(tree,x_train,y_train,scoring= \"accuracy\", cv=10)","4071f03d":"cross_val_score(tree,x,y,scoring= \"accuracy\", cv=5).mean()","36680298":"cross_val_score(rf,x_train,y_train,scoring= \"accuracy\", cv=5).mean()","b6d2340c":"cross_val_score(lg,x_train,y_train,scoring= \"accuracy\", cv=5).mean()","f7f68e8a":"cross_val_score(knn,x_train,y_train,scoring= \"accuracy\", cv=5).mean()","0df17c69":"from sklearn.ensemble import VotingClassifier","1be82355":"model_vote = VotingClassifier(estimators=[('logistic Regression', lg), ('random forrest', rf), ('knn neighbors', knn),(' decision tree', tree)], voting='soft')\nmodel_vote = model_vote.fit(x_train, y_train)","09858ce2":"vote_pred = model_vote.predict(x_test)","24e8ad48":"a5 =  accuracy_score(y_test, vote_pred)\nprint(\"The Accuracy of voting classifier is \", a5)","6f0d7d04":"print(classification_report(y_test, vote_pred))","9ea6f51a":"from sklearn.ensemble import BaggingClassifier","520ecd82":"bagg = BaggingClassifier(base_estimator=rf1,n_estimators=15)","6e0685b0":"model_bagg =bagg.fit(x_train,y_train) \npred_bagg = bagg.predict(x_test)","e408d811":"a6 = accuracy_score(y_test, pred_bagg)\nprint(\"The Accuracy of BAAGING is \", a6)","c7027e10":"confusion_matrix(y_test,pred_bagg)","d98c6de7":"print(classification_report(y_test, pred_bagg))","e9588545":"from sklearn.ensemble import AdaBoostClassifier","5e601a9c":"Adaboost = AdaBoostClassifier(base_estimator=rf1, n_estimators=15)","a707afdc":"model_boost =Adaboost.fit(x_train,y_train) \npred_boost = Adaboost.predict(x_test)","ac32ecac":"a7 = accuracy_score(y_test, pred_boost)\nprint(\"The Accuracy of BOOSTING is \", a7)","829d0f17":"confusion_matrix(y_test,pred_boost)","3befe936":"print(classification_report(y_test, pred_boost))","caa3d5a0":"## Label Encoding","2acc70e3":"## Logistic Regression & KNN model","660f7b23":"## Random Forest Model with Default parameters","8a656ebd":"# Adult Census Income Analysis - Decision TREE, Random Forest, CV, Tuning the model with Ensemble Techniques(Baaging , ADAboost)\n","de050799":"## Decision Tree Model with Default parameters","77a4ef93":"# K FOLD Cross Validation","f3a355c3":"# Build optimized Random forest model with tuned hyperparameters from grid search model  ","41613eb9":"## Clean & Analyze Data,","37d3e363":" ### Missing Value % is very insignificant  so we will drop those values","6d65069c":"# Voting Classifier model","033a8b86":"#  Ensemble Technique  ADA Boost \n\n## Increase Accuracy by Applying Ensemble technique ADABOOST to our tuned random forest model","e7f999de":"# Ensemble Technique Bagging \n\n## Increase Accuracy by Applying Ensemble technique BAGGING to our tuned random forest model","ff3c0a61":"### A stable and optimized model to predict the income of a given population, which is labelled as <= 50K and >50K. The attributes (predictors) are age, working class type, marital status, gender, race etc.\n#### Following are the steps, \n#### 1.clean and prepare the data,\n#### 2.Analyze Data,\n#### 3.Label Encoding,\n#### 4.Build a decision tree and Random forest with default hyperparameters,\n#### 5.Build several classifier models to compare, cross validate and for voting classifier model\n#### 6.choose the optimal hyperparameters using grid search cross-validation.\n#### 7.Build optimized Random forest model with tuned hyperparameters from grid search model\n#### 8.Increase Accuracy by Applying Ensemble technique BAGGING to our tuned random forest model\n#### 9.Increase Accuracy by Applying Ensemble technique ADABOOST to our tuned random forest model\n####  I hope you enjoy this notebook and find it useful!"}}