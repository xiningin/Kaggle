{"cell_type":{"8e8348e1":"code","1bf401f1":"code","00a68a1a":"code","d2091ffb":"code","71e1f38a":"code","c141292d":"code","db38eb7f":"code","8bd2a7cf":"code","a6eb455a":"code","8ffa0c37":"code","306fc080":"code","283fb96a":"code","4ba31690":"code","278a7cda":"code","75707878":"code","cd6d1948":"code","852e0422":"code","1948d44c":"code","64d60375":"code","89eb5398":"code","55bfa041":"code","ba6405bb":"code","9a920aa0":"code","25325fd2":"code","17ffa592":"code","60d431de":"code","f7a8e888":"code","9b91668e":"code","bced786e":"code","ec207996":"code","3029e719":"code","35d1f674":"code","af8b6e53":"code","4bf4f4c0":"code","706ac081":"code","8470c096":"code","51210271":"code","2c481e92":"code","9fdce373":"code","dfaa3516":"code","286b41da":"code","46e48044":"code","4b2dc885":"code","36bd88e7":"code","e6f76aca":"code","02901fce":"code","59d9aa12":"code","10b33059":"code","bacbde21":"code","8b8b83ba":"code","d124f681":"code","d3d653f7":"code","14e64544":"code","a13807e5":"code","3a0e8e4e":"code","b6cfc456":"code","d56b2d43":"code","63accf92":"code","d6aee2cb":"code","792e6405":"code","bfae8309":"code","68eaf161":"code","23ad2dc5":"code","11a2339b":"code","25dcc6ea":"code","24731bdc":"code","27c57122":"code","be0a2101":"code","f0292cdb":"code","94ef425e":"code","00e36e98":"code","cb25ff21":"code","e034b50a":"code","92e5489b":"code","601901d6":"code","1dd80212":"code","cdfa7a09":"code","69a9f4d7":"code","c32939f3":"code","ed3241d9":"code","a653d8df":"code","45704861":"code","06451c12":"code","ebbbc3c9":"code","e807aa00":"code","f428aad2":"code","13563e2e":"code","e77f6bcc":"code","554e7f63":"markdown","92914339":"markdown","9ea920ea":"markdown","498ac5c0":"markdown","6225f808":"markdown","52bc369c":"markdown","ae81f7bf":"markdown","81e9ad06":"markdown","fd7e7fef":"markdown","01abdfe6":"markdown","276c6075":"markdown","a9eac97c":"markdown","cf4643a9":"markdown","0553934f":"markdown","b2090ba2":"markdown","8c551c23":"markdown","4227e258":"markdown","74b810ba":"markdown","dd24f232":"markdown","197af544":"markdown","104710d2":"markdown","600079a8":"markdown","e6242395":"markdown","b13e4988":"markdown","aaebdabb":"markdown","68b0130e":"markdown","61bc9fd2":"markdown","68d7f5b0":"markdown","fedef831":"markdown","8521c341":"markdown","c7bccaa7":"markdown","000aac60":"markdown","734490fe":"markdown","de0070f8":"markdown","f844e4c2":"markdown","1c688eec":"markdown","ae8543a2":"markdown","b9b269af":"markdown","4ae4ac16":"markdown","23e87203":"markdown","5330bddb":"markdown","837ef25d":"markdown","c9518098":"markdown","190cba86":"markdown","48d0b147":"markdown","88b072c7":"markdown","baaae996":"markdown","eb71ac37":"markdown","30e7b55d":"markdown","1db89924":"markdown","75652c72":"markdown"},"source":{"8e8348e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\") # --> helps to visulize tools with grids. You can use another ones with looking plt.style.available\nimport seaborn as sns\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings(\"ignore\") # dont show warnings based on python\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1bf401f1":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_PassengerId = test_df[\"PassengerId\"] # will be used next steps .","00a68a1a":"train_df.columns # columns of data","d2091ffb":"train_df.head() # first 5 information about data","71e1f38a":"train_df.describe()","c141292d":"train_df.info()","db38eb7f":"def bar_plot(variable):\n    \"\"\"\n        input: variable ex: \"Sex\"\n        output: bar plot & value count\n    \"\"\"\n    # get feature\n    var = train_df[variable]\n    # count number of categorical variable(value\/sample)\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{} : \\n{} : \".format(variable,varValue))\n    ","8bd2a7cf":"category1 = [\"Survived\",\"Sex\",\"Pclass\",\"Embarked\",\"SibSp\", \"Parch\"]\nfor c in category1:\n    bar_plot(c)","a6eb455a":"category2 = [\"Cabin\", \"Name\", \"Ticket\"]\nfor c in category2 : \n    print(\"{} \\n\".format(train_df[c].value_counts()))","8ffa0c37":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(train_df[variable], bins = 50) # frequency of bars, default = 10\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} disturbiton with hist\".format(variable))\n    plt.show()","306fc080":"numericVar = [\"Fare\", \"Age\", \"PassengerId\"]\nfor n in numericVar:\n    plot_hist(n)","283fb96a":"# Pclass vs Survived\ntrain_df[[\"Pclass\",\"Survived\"]].groupby([\"Pclass\"], as_index = False).mean().sort_values(by =\"Survived\", ascending = False)","4ba31690":"# Sex vs Survived\ntrain_df[[\"Sex\",\"Survived\"]].groupby([\"Sex\"], as_index = False).mean().sort_values(by =\"Survived\", ascending = False)","278a7cda":"# SibSp vs Survived\ntrain_df[[\"SibSp\",\"Survived\"]].groupby([\"SibSp\"], as_index = False).mean().sort_values(by =\"Survived\", ascending = False)","75707878":"# Parch vs Survived\ntrain_df[[\"Parch\",\"Survived\"]].groupby([\"Parch\"], as_index = False).mean().sort_values(by =\"Survived\", ascending = False)","cd6d1948":"# Fare vs Survived\ntrain_df[[\"Fare\",\"Survived\"]].groupby([\"Fare\"], as_index = False).mean().sort_values(by =\"Survived\", ascending = False)","852e0422":"# Pclass - Sex vs Survived\ntrain_df[[\"Pclass\",\"Sex\",\"Survived\"]].groupby([\"Pclass\",\"Sex\"], as_index = False).mean().sort_values(by =\"Survived\", ascending = False)","1948d44c":"def detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","64d60375":"train_df.loc[detect_outliers(train_df, [\"Age\",\"SibSp\",\"Parch\",\"Fare\"])]","89eb5398":"# drop outliers\ntrain_df = train_df.drop(detect_outliers(train_df,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"]),axis = 0).reset_index(drop = True)","55bfa041":"train_df_len = len(train_df)\ntrain_df = pd.concat([train_df,test_df], axis = 0).reset_index(drop = True)","ba6405bb":"train_df.head()","9a920aa0":"train_df.columns[train_df.isnull().any()] # in which columns there are missing values? ","25325fd2":"train_df.isnull().sum() # how many ? ","17ffa592":"train_df[train_df[\"Embarked\"].isnull()]","60d431de":"train_df.boxplot(column =\"Fare\", by =\"Embarked\")\nplt.show()","f7a8e888":"train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(\"C\") # filling missing values with C\ntrain_df[train_df[\"Embarked\"].isnull()] # checking\n","9b91668e":"train_df[train_df[\"Fare\"].isnull()]","bced786e":"train_df[\"Fare\"] = train_df[\"Fare\"].fillna(np.mean(train_df[train_df[\"Pclass\"] == 3][\"Fare\"])) # filling the missing value with mean who are 3. class passengers.","ec207996":"train_df[train_df[\"Fare\"].isnull()] #checking","3029e719":"num1 = [3,2,5,1]\n\nnum2 = [i**2 if i % 3 == 2 else i-1  for i in num1]\n\nprint(num2)","35d1f674":"list1 = [\"SibSp\", \"Parch\", \"Age\", \"Fare\", \"Survived\"]\nsns.heatmap(train_df[list1].corr(), annot=True, fmt=\".2f\")\nplt.show()","af8b6e53":"g = sns.factorplot(x=\"SibSp\", y=\"Survived\", data=train_df, kind=\"bar\", size=6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","4bf4f4c0":"g = sns.factorplot(x=\"Parch\", y=\"Survived\", data=train_df, kind=\"bar\", size=6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","706ac081":"g = sns.factorplot(x=\"Pclass\", y=\"Survived\", data=train_df, kind=\"bar\", size=6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","8470c096":"g = sns.FacetGrid(train_df, col = \"Survived\")\ng.map(sns.distplot, \"Age\", bins=25)\nplt.show()","51210271":"g = sns.FacetGrid(train_df, col=\"Survived\", row=\"Pclass\")\ng.map(plt.hist, \"Age\", bins=24)\ng.add_legend()\nplt.show()","2c481e92":"g = sns.FacetGrid(train_df, row = \"Embarked\", size = 2)\ng.map(sns.pointplot, \"Pclass\",\"Survived\",\"Sex\")\ng.add_legend()\nplt.show()","9fdce373":"g = sns.FacetGrid(train_df, row=\"Embarked\", col=\"Survived\")\ng.map(sns.barplot, \"Sex\", \"Fare\")\ng.add_legend()\nplt.show()","dfaa3516":"train_df[train_df[\"Age\"].isnull()]","286b41da":"sns.factorplot(x=\"Sex\", y=\"Age\", data=train_df, kind=\"box\")\nplt.show()","46e48044":"sns.factorplot(x=\"Sex\", y=\"Age\", hue=\"Pclass\", data=train_df, kind=\"box\")\nplt.show()","4b2dc885":"sns.factorplot(x = \"Parch\", y = \"Age\", data = train_df, kind = \"box\")\nsns.factorplot(x = \"SibSp\", y = \"Age\", data = train_df, kind = \"box\")\nplt.show()","36bd88e7":"#train_df[\"Sex\"] = [1 if i == \"male\" else 0  for i in train_df[\"Sex\"]]","e6f76aca":"sns.heatmap(train_df[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(), annot = True)\nplt.show()","02901fce":"index_nan_age = list(train_df[\"Age\"][train_df[\"Age\"].isnull()].index)\nfor i in index_nan_age:\n    age_pred = train_df[\"Age\"][((train_df[\"SibSp\"] == train_df.iloc[i][\"SibSp\"]) &(train_df[\"Parch\"] == train_df.iloc[i][\"Parch\"])& (train_df[\"Pclass\"] == train_df.iloc[i][\"Pclass\"]))].median()\n    age_med = train_df[\"Age\"].median()\n    if not np.isnan(age_pred):\n        train_df[\"Age\"].iloc[i] = age_pred\n    else:\n        train_df[\"Age\"].iloc[i] = age_med","59d9aa12":"train_df[train_df[\"Age\"].isnull()]","10b33059":"train_df[\"Name\"].head()","bacbde21":"train_df[\"Title\"] = [i.split(\".\")[0].split(\",\")[-1].strip() for i in train_df[\"Name\"]]","8b8b83ba":"train_df[\"Title\"].head(10)","d124f681":"sns.countplot(x=\"Title\", data=train_df)\nplt.xticks(rotation=60)\nplt.show()","d3d653f7":"# Convert them to categorical\ntrain_df[\"Title\"] = train_df[\"Title\"].replace([\"Lady\",\"the Countess\",\"Capt\",\"Col\",\"Don\",\"Dr\",\"Major\",\"Rev\",\"Sir\",\"Jonkheer\",\"Dona\"],\"other\")\ntrain_df[\"Title\"] = [0 if i == \"Master\" else 1 if i == \"Miss\" or i == \"Ms\" or i == \"Mlle\" or i == \"Mrs\" else 2 if i == \"Mr\" else 3 for i in train_df[\"Title\"]]\ntrain_df[\"Title\"].head(10)","14e64544":"sns.countplot(x=\"Title\", data = train_df)\nplt.xticks(rotation = 0)\nplt.show()","a13807e5":"g = sns.factorplot(x=\"Title\", y=\"Survived\", data=train_df, kind=\"bar\")\ng.set_xticklabels([\"Master\", \"Ms-Miss\", \"Mr\", \"Other\"])\ng.set_ylabels(\"Survival Probability\")\nplt.show()","3a0e8e4e":"train_df.drop(labels=[\"Name\"], axis=1, inplace=True)\ntrain_df.head()","b6cfc456":"train_df = pd.get_dummies(train_df, columns=[\"Title\"])\ntrain_df.head()","d56b2d43":"train_df[\"Fsize\"] = train_df[\"Parch\"] + train_df[\"SibSp\"] + 1 # each person is a 1 person family","63accf92":"train_df.head()","d6aee2cb":"g = sns.factorplot(x=\"Fsize\", y=\"Survived\", data=train_df, kind=\"bar\")\ng.set_ylabels(\"Survival Probability\")\nplt.show()","792e6405":"train_df[\"family_size\"] = [1 if i < 5 else 0 for i in train_df[\"Fsize\"]] #thresold","bfae8309":"train_df.head(8)","68eaf161":"sns.countplot(x = \"family_size\", data = train_df)\nplt.show()","23ad2dc5":"g = sns.factorplot(x=\"family_size\", y=\"Survived\", data=train_df, kind=\"bar\")\ng.set_ylabels(\"Survival Probability\")\nplt.show()","11a2339b":"train_df = pd.get_dummies(train_df, columns= [\"family_size\"])\ntrain_df.head()","25dcc6ea":"sns.countplot(x = \"Embarked\", data = train_df)\nplt.show()","24731bdc":"train_df = pd.get_dummies(train_df, columns=[\"Embarked\"])\ntrain_df.head()","27c57122":"train_df[\"Ticket\"].head(15)","be0a2101":"a = \"A\/5. 2151\"\na.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0]","f0292cdb":"tickets = []\nfor i in train_df[\"Ticket\"]:\n    if not i.isdigit():\n        tickets.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0])\n    else:\n        tickets.append(\"X\")\ntrain_df[\"Ticket\"] = tickets","94ef425e":"train_df[\"Ticket\"].head(10)","00e36e98":"train_df[\"Ticket\"].unique()","cb25ff21":"train_df = pd.get_dummies(train_df, columns= [\"Ticket\"], prefix = \"T\")\ntrain_df.head(10)","e034b50a":"sns.countplot(x = \"Pclass\", data = train_df)\nplt.show()","92e5489b":"train_df[\"Pclass\"] = train_df[\"Pclass\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df, columns=[\"Pclass\"])\ntrain_df.head()","601901d6":"train_df[\"Sex\"] = train_df[\"Sex\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df, columns=[\"Sex\"])\ntrain_df.head()","1dd80212":"train_df.drop(labels=[\"PassengerId\", \"Cabin\"], axis=1, inplace=True)","cdfa7a09":"train_df.columns","69a9f4d7":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score","c32939f3":"train_df_len","ed3241d9":"test = train_df[train_df_len:]\ntest.drop(labels = [\"Survived\"],axis = 1, inplace = True)","a653d8df":"test.head()","45704861":"train = train_df[:train_df_len]\nX_train = train.drop(labels=\"Survived\", axis=1)\ny_train = train[\"Survived\"]\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\nprint(\"X_train\",len(X_train))\nprint(\"X_test\",len(X_test))\nprint(\"y_train\",len(y_train))\nprint(\"y_test\",len(y_test))\nprint(\"test\",len(test))","06451c12":"lr = LogisticRegression()\nlr.fit(X_train, y_train)\nacc_log_train = lr.score(X_train, y_train)\nacc_log_test = lr.score(X_test, y_test)\nprint(\"Train accuracy : \", acc_log_train)\nprint(\"Test accuracy : \", acc_log_test)","ebbbc3c9":"random_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n             SVC(random_state = random_state),\n             RandomForestClassifier(random_state = random_state),\n             LogisticRegression(random_state = random_state),\n             KNeighborsClassifier(),\n             XGBClassifier(random_state=random_state),\n             AdaBoostClassifier(random_state=random_state)]\n\ndt_param_grid = {\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2)}\n\nsvc_param_grid = {\"kernel\" : [\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1],\n                 \"C\": [1,10,50,100,200,300,1000]}\n\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\",\"l2\"]}\n\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\n\nxgb_param_grid = {\"booster\" : [\"gbtree\", \"gblinear\", \"dart\" ],\n                  \"gamma\": [0.001, 0.01, 0.1, 1],\n                 \"max_depth\" : range(1,10)}\n\nada_param_grid = {\"n_estimators\" : range(1,15),\n                 \"learning_rate\" : range(1,4)}\n\nclassifier_param = [dt_param_grid,\n                   svc_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid,\n                   xgb_param_grid,\n                   ada_param_grid]","e807aa00":"cv_result = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n    clf.fit(X_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(\"Best result : \",cv_result[i])\n    print(\"Best estimators : \", best_estimators[i])","f428aad2":"cv_results = pd.DataFrame({\"Cross Validation Means\":cv_result, \"ML Models\":[\"DecisionTreeClassifier\", \"SVM\",\"RandomForestClassifier\",\n             \"LogisticRegression\",\n             \"KNeighborsClassifier\", \"XGBoostClassifier\", \"AdaBoostClassifier\"]})\n\ng = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")","13563e2e":"votingC = VotingClassifier(estimators = [(\"dt\",best_estimators[0]),\n                                        (\"rfc\",best_estimators[2]),\n                                        (\"xgbc\",best_estimators[5])],\n                                        voting = \"soft\", n_jobs = -1)\nvotingC = votingC.fit(X_train, y_train)\nprint(accuracy_score(votingC.predict(X_test),y_test))","e77f6bcc":"test_survived = pd.Series(votingC.predict(test), name = \"Survived\").astype(int)\nresults = pd.concat([test_PassengerId, test_survived],axis = 1)\nresults.to_csv(\"titanic.csv\", index = False)","554e7f63":"<a id = '3'><\/a><br>\n# Univariate Variable Analysis\n* Categorical Variable : Survived, Sex, Pclass, Embarked, Cabin, Name, Ticket, SibSp and Parch\n* Numerical Variable : Age, Fare, and PassengerId","92914339":"# Introduction\nIn history there are many interesting accidents. One of these notorious accident is Titanic shipwreck. Despite it was considered impossible to sink, however it sank colliding with an iceberg. Following this, 1502 people died out of 2224 passengers and crew on the terrifying cold night. \n\n<font color = 'purple'>\nContent : \n\n1. [Load and Check Data](#1)\n1. [Variable Description](#2)\n    * [Univariate Variable Analysis](#3)\n        - [Categorical Variable](#4)\n        - [Numerical Variable](#5)\n1. [Basic Data Analysis](#6)\n1. [Outlier Detection](#7)\n1. [Missing Value](#8)\n    * [Find Missing Value](#9)\n    * [Fill Missing Value](#10)\n1. [Visualization](#11)\n    * [Correlation Between Sibsp -- Parch -- Age -- Fare -- Survived](#12)\n    * [SibSp -- Survived](#13)\n    * [Parch -- Survived](#14)\n    * [Pclass -- Survived](#15)\n    * [Age -- Survived](#16)\n    * [Pclass -- Survived -- Age](#17)\n    * [Embarked -- Sex -- Pclass -- Survived](#18)\n    * [Embarked -- Sex -- Fare -- Survived](#19)\n    * [Fill Missing: Age Feature](#20)\n1. [Feature Engineering](#21)\n    * [Name -- Title](#22)\n    * [Family Size](#23)\n    * [Embarked](#24)\n    * [Ticket](#25)\n    * [Pclass](#26)\n    * [Sex](#27)\n    * [Drop Passenger ID and Cabin](#28)\n1. [Modeling](#29)\n    * [Train - Test Split](#30)\n    * [Simple Logistic Regression](#31)\n    * [Hyperparameter Tuning -- Grid Search -- Cross Validation](#32) \n    * [Ensemble Modeling](#33)\n    * [Prediction and Submission](#34)","9ea920ea":"<a id = \"13\"><\/a><br>\n## SibSp -- Survived","498ac5c0":"<a id = \"34\"><\/a><br>\n## Prediction and Submission","6225f808":"<a id = '2'><\/a><br>\n# Variable Description\n\n1. PassengerId : Uniqe Id number to each passenger.\n2. Survived : Passenger survived(1) or died(0). \n3. Pclass : Passenger class.\n4. Name : Name of passenger.\n5. Sex : Gender of passenger.\n6. Age : Age of passenger.\n7. SibSp : Number of siblings\/spouses.\n8. Parch : Number of parent\/children.\n9. Ticket : Ticket number.\n10. Fare : Cost of the ticket.\n11. Cabin : Cabin category.\n12. Embarked : Port where passengers embarked. (C = Cherbourg, Q = Queenstown, S = Southampthon)","52bc369c":"<a id = \"31\"><\/a><br>\n## Simple Logistic Regression","ae81f7bf":"<a id = \"25\"><\/a><br>\n## Ticket","81e9ad06":"<a id = \"28\"><\/a><br>\n## Drop Passenger ID and Cabin","fd7e7fef":"According to the result, small families have more chance than large families.","01abdfe6":"<a id = '6'><\/a><br>\n# Basic Data Analysis\n* Pclass - Survived\n* Sex - Survived\n* SibSp - Survived\n* Parch - Survived\n* Fare - Survived\n* Pclass - Sex - Survived","276c6075":"We will compare 7 Machine Learning classifier and evaluate mean accuracy of each of them by stratified cross validation.\n\n* Decision Tree\n* SVM\n* Random Forest\n* KNN\n* Logistic Regression\n* XGBoost\n* AdaBoost","a9eac97c":"<a id = '7'><\/a><br>\n# Outlier Detection","cf4643a9":"<a id = \"19\"><\/a><br>\n## Embarked -- Sex -- Fare -- Survived","0553934f":"* Passsengers who pay higher fare have better survival. Fare can be used as categorical for training.","b2090ba2":"<a id = '9'><\/a><br>\n## Find Missing Value","8c551c23":"<a id = '11'><\/a><br>\n\n# Visualization","4227e258":"<a id = \"21\"><\/a><br>\n# Feature Engineering","74b810ba":"<a id = '4'><\/a><br>\n## Categorical Variable","dd24f232":"<a id = \"24\"><\/a><br>\n## Embarked","197af544":"* Age is not correlated with sex but it is correlated with parch, sibsp and pclass.\n","104710d2":"* Female passengers have much better survival rate than males.\n* Males have better survival rate in pclass 3 in C.\n* Embarked and sex will be used in training.","600079a8":"<a id = \"33\"><\/a><br>\n## Ensemble Modeling","e6242395":"* Sibsp and parch can be used for new feature extraction with threshold = 3\n* Small familes have more chance to survive.\n* There is a std in survival of passenger with parch = 3","b13e4988":"<a id = '10'><\/a><br>\n## Fill Missing Value\n* Embarked has 2 missing values\n* Fare has only one.","aaebdabb":"<a id = \"17\"><\/a><br>\n## Pclass -- Survived -- Age","68b0130e":"<a id = \"26\"><\/a><br>\n## Pclass","61bc9fd2":"<a id = '1'><\/a><br>\n# Load and Check Data","68d7f5b0":"<a id = '8'><\/a><br>\n# Missing Value\n* Find Missing Value\n* Fill Missing Value","fedef831":"<a id = \"27\"><\/a><br>\n## Sex","8521c341":"Fare feature seems to have correlation with survived feature (0.26).","c7bccaa7":"<a id = '12'><\/a><br>\n\n## Correlation Between Sibsp -- Parch -- Age -- Fare -- Survived","000aac60":"* Age <= 10 has a high survival rate,\n* Oldest passengers (80) survived,\n* Large number of 20 years old did not survive,\n* Most passengers are in 15-35 age range,\n* Use age feature in training\n* Use age distribution for missing value of age","734490fe":"<a id = \"20\"><\/a><br>\n## Fill Missing: Age Feature","de0070f8":"<a id = \"14\"><\/a><br>\n## Parch -- Survived","f844e4c2":"* Sex is not informative for age prediction, age distribution seems to be same.","1c688eec":"* Having a lot of SibSp have less chance to survive.\n* if sibsp == 0 or 1 or 2, passenger has more chance to survive\n* We can consider a new feature describing these categories.","ae8543a2":"<a id = \"30\"><\/a><br>\n## Train - Test Split","b9b269af":"<a id = \"18\"><\/a><br>\n## Embarked -- Sex -- Pclass -- Survived","4ae4ac16":"* float64(2) : Age and Fare\n* int64(5) : PassengerId, Survived, Pclass, SibSp and Parch\n* object(5) : Name, Sex, Ticket, Cabin and Embarked","23e87203":"<a id = \"16\"><\/a><br>\n## Age -- Survived","5330bddb":"<a id = \"23\"><\/a><br>\n## Family Size","837ef25d":"This part is created for comparison of curious features.","c9518098":"<a id = \"22\"><\/a><br>\n## Name & Title","190cba86":"<a id = \"32\"><\/a><br>\n## Hyperparameter Tuning -- Grid Search -- Cross Validation","48d0b147":"* 1st class passengers are older than 2nd, and 2nd is older than 3rd class. ","88b072c7":"By analyzing graphic fare by embarked we can make this didaction : \nin C, We see that passengers boarding from c are closer to 80 and probably 1.class. ","baaae996":"* Pclass is important feature for model training.","eb71ac37":"<a id = \"29\"><\/a><br>\n# Modeling","30e7b55d":"<a id = '5'><\/a><br>\n## Numerical Variable","1db89924":"In categorical variable analysis, there are variables created from categories, options. As an illustration, Sex : Female or male there are two options.\nIn numerical variable analysis, there are variables created from numbers. No options. As an illustration, Age : 24","75652c72":"<a id = \"15\"><\/a><br>\n## Pclass -- Survived"}}