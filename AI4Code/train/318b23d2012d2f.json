{"cell_type":{"56828783":"code","e22b418f":"code","b2e1b019":"code","94c7f9a9":"code","d8b9e119":"code","aae3f4c8":"code","0d57002b":"code","5b9566b3":"code","96716160":"code","def7103a":"code","6734c2ed":"code","1c22cd70":"code","a4ead793":"code","773187fc":"code","61fb96f2":"code","faf48934":"code","a8e3da83":"code","11cefa1c":"code","12f2861a":"code","9a107674":"code","a966935a":"code","8acd0194":"code","1e74913c":"code","6bf00fba":"code","6b682fd4":"code","169ceadf":"code","a99d90f1":"code","28dc85e9":"code","6a841471":"code","b1ece40d":"code","a6e77c57":"code","f8178a22":"code","4857d6bd":"markdown","a6a80a8b":"markdown","03d11f5c":"markdown","a96c15e5":"markdown","a25aa705":"markdown","35235b85":"markdown","4cbd97a6":"markdown","e8496872":"markdown","b6b0f04b":"markdown","d14aa1a0":"markdown","c19a96b4":"markdown","aae77c6f":"markdown","adcb01d2":"markdown","47b24cbd":"markdown","60f78386":"markdown","898c5e4a":"markdown","0e3daa4c":"markdown","8a44c10d":"markdown"},"source":{"56828783":"import pandas as pd\nimport numpy as np\n\n# Libraries for plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Sklearn import\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Dataset import\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()","e22b418f":"# Load our dataset\n\ndf = pd.DataFrame(cancer.data, columns=cancer.feature_names)","b2e1b019":"df.head()","94c7f9a9":"# Vieweing the shape of our dataset\n\ndf.shape","d8b9e119":"# Load target into 'diagnoses' column\n\ndf['diagnoses'] = pd.Categorical.from_codes(cancer.target, cancer.target_names)","aae3f4c8":"# We can use Label Encoder to label our target variable\n\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ndf['diagnoses'] = labelencoder.fit_transform(df['diagnoses'])","0d57002b":"df.head()","5b9566b3":"# As we can see, all instances have non-null values and there are no categorical data\n\ndf.info()","96716160":"# Here we can se the percentage of malignant and benign tumors, checking how balanced is our dataset\n# This rate of malignant tumor is still ok for running a model\n\ndf['diagnoses'].value_counts(normalize=True)","def7103a":"# Correlation between target and other variables\n\ncorr_matrix = df.corr()\ncm = corr_matrix['diagnoses'].sort_values(ascending=False)\nprint(cm)","6734c2ed":"# We can plot a correlation map to see how variables correlate with each other.\n\nf,ax = plt.subplots(figsize=(14, 10))\nsns.heatmap(df[list(cm.index[:16])].corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax, cmap='Blues')\nplt.show()","1c22cd70":"# Dropping some variables\n\ndrop_list = ['worst perimeter', 'worst radius', 'worst area','mean area', 'mean radius','mean concave points', 'worst concavity', 'worst compactness', 'mean symmetry', 'mean smoothness','perimeter error','area error']\nX_drop = df.drop(drop_list, axis=1)","a4ead793":"# Checking again how correlations are\n\ncorr_matrix = X_drop.corr()\ncm2 = corr_matrix['diagnoses'].sort_values(ascending=False)\nprint(cm2)","773187fc":"# Now we can see there were left variables that do not correlate that much\n\nf,ax = plt.subplots(figsize=(14, 10))\nsns.heatmap(corr_matrix, annot=True, linewidths=.5, fmt= '.1f',ax=ax, cmap='Blues')\nplt.show()","61fb96f2":"# Let's store the index of all these remaining variables\n\nbest_corr = list(cm2.index[1:])","faf48934":"# Plotting the best 5 correlated variables\n# We can clearly see how malignant tumors tend to have higher values in these variables\n\nsns.pairplot(df[cm2.index[:5]],hue='diagnoses')\nplt.show()","a8e3da83":"# Here a stratified split is performed, keeping the proportion of the target variable\n\ntraindf, testdf = train_test_split(df, stratify=df['diagnoses'], test_size = 0.3, shuffle=True, random_state=42)","11cefa1c":"# Function to apply K-Fold and print score, taking as input the variables we want to use for training\n\ndef classification_model(model, df, predictors, outcome):\n    X_train = df[predictors]\n    y_train = df[outcome]\n    model.fit(X_train, y_train)\n    cvs = cross_val_score(model, X_train, y_train, cv=5, scoring=\"accuracy\")\n    for acc in cvs:\n        print(\"Cross validation score : %s\" % \"{0:.3%}\".format(acc))\n    print(\"Mean : %s\" % \"{0:.3%}\".format(np.mean(cvs)))","12f2861a":"# First, we shall apply the model to all variables\n\npredictors = list(df.columns[0:30])\noutcome = 'diagnoses'\nforest_clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\nclassification_model(forest_clf, traindf, predictors, outcome)","9a107674":"# For the fit given in classification_model, we have that the most important features are (the higher, the best):\n\nfeatimp = pd.Series(forest_clf.feature_importances_, index=predictors).sort_values(ascending=False)\nprint(featimp)","a966935a":"# Selecting the 15 most important features:\nbest_15 = list(featimp.index[:16])\n\n# Now we test our model with all these features\n\noutcome = 'diagnoses'\nforest_clf_15 = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\nclassification_model(forest_clf_15, traindf, best_15, outcome)","8acd0194":"# Evaluating with 6 features\n\nbest_5 = list(featimp.index[:5])\noutcome = 'diagnoses'\nforest_clf_5 = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\nclassification_model(forest_clf_5, traindf, best_5, outcome)","1e74913c":"# Now let's try with the best features obtained through Pearson's correlation\n\npredictors = best_corr\noutcome = 'diagnoses'\nforest_clf_p = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\nclassification_model(forest_clf_p, traindf, predictors, outcome)","6bf00fba":"# As done before, we can take the best 5 listed by feature_importance\n\nfeatimp_p = pd.Series(forest_clf_p.feature_importances_, index=predictors).sort_values(ascending=False)\nprint(featimp_p)","6b682fd4":"# Trying the best 6 obtained through Random Forest feature importance\n\npredictors = featimp_p.index[:6]\noutcome = 'diagnoses'\nforest_clf_pf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\nclassification_model(forest_clf_pf, traindf, predictors, outcome)","169ceadf":"# Function to print classification report and confusion matrix\n\ndef classification_metrics(trained_classifier, x_test, y_test, cmap='Purples'):\n    y_pred = trained_classifier.predict(x_test)\n    print(classification_report(y_test, y_pred))\n    plot_confusion_matrix(trained_classifier, x_test, y_test, cmap=cmap)","a99d90f1":"# Testing the best 15 features\n\nfrom sklearn.metrics import classification_report\n\nx_test = testdf[best_15]\ny_test = testdf['diagnoses']\nclassification_metrics(forest_clf_15, x_test, y_test, cmap='Purples')","28dc85e9":"# Testing the best 5 features\n\nx_test = testdf[best_5]\ny_test = testdf['diagnoses']\nclassification_metrics(forest_clf_5, x_test, y_test, cmap='Purples')","6a841471":"# Testing all features\n\npredictors = list(df.columns[0:30])\nx_test = testdf[predictors]\ny_test = testdf['diagnoses']\nclassification_metrics(forest_clf, x_test, y_test, cmap='Purples')","b1ece40d":"# Testing the best 6 features obtained through feature importance\n\npredictors = featimp_p.index[:6]\nx_test = testdf[predictors]\ny_test = testdf['diagnoses']\nclassification_metrics(forest_clf_pf, x_test, y_test, cmap='Purples')","a6e77c57":"from sklearn.metrics import confusion_matrix\nthresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in thresholds:\n    y_score = forest_clf_15.predict_proba(testdf[best_15])[:,1] > i\n    cm = confusion_matrix(testdf['diagnoses'], y_score)\n    tp = cm[1,1]\n    fn = cm[1,0]\n    print('Recall with threshold = %s :'%i, (tp\/(tp+fn)))","f8178a22":"y_score = forest_clf_15.predict_proba(testdf[best_15])[:,1] > 0.2\nprint(classification_report(testdf['diagnoses'], y_score))\nconf_m = confusion_matrix(testdf['diagnoses'], y_score)\nsns.heatmap(conf_m, annot=True, fmt='g', annot_kws={\"fontsize\":18}, cmap='Purples')\nplt.show()","4857d6bd":"Our goal here is to detect all malignant tumors. We can't tell a patient he has no malignant tumor and later on this tumor begins a metastasis process. So, we must make our model have a 100% Recall. For this, we must give up precision. While we are creating our models we should always think about the best relationship between Precision and Recall, and take what fits best for our application.","a6a80a8b":"## Selecting a Threshold for Recall = 100%","03d11f5c":"#### In this notebook, we will use the Random Forest classifier and see how it performs with your chosen variables.","a96c15e5":"#### Labels:\n* Benign = 0;\n* Malignant = 1;","a25aa705":"# Breast Cancer Prediction (Recall 100% and Accuracy 95%)","35235b85":"Plotting correlation of the 16 best features, we can see that some variables are highly correlated, so we can drop some. They won't help our model. Darker squares indicate high correlation.","4cbd97a6":"We were able to obtain a 100% Recall for a threshold of 0.2. Let's see it's accuracy and plot confusion matrix.","e8496872":"Here we split our model into 30% for testing and 70% for training. Our classifier will never see the test dataset during training. Only at the end, to evaluate our model, we will use and make predictions on the test set.","b6b0f04b":"According to Pearson's correlation, when this value is close to 1, it indicates a strong **positive correlation** between two variables. Meaning when a value of one of the variables **increases**, the value of the other variable will **increases** as well. When this value is close to -1, it indicates a strong **negative** correlation. Meaning that when a value of one of the variables **increases**, the value of the other variable will **deacrese**. While a value near 0 indicates the absence of any correlation between the two variables.","d14aa1a0":"#### Core Features:\n* radius (mean of distances from center to points on the perimeter)\n* texture (standard deviation of gray-scale values)\n* perimeter\n* area\n* smoothness (local variation in radius lengths)\n* compactness (perimeter^2 \/ area - 1.0)\n* concavity (severity of concave portions of the contour)\n* concave points (number of concave portions of the contour)\n* symmetry\n* fractal dimension","c19a96b4":"## Evaluating in Test Dataset","aae77c6f":"Random Forest classifier can tell us how important are the features for predicting the instances. This way we can choose the best ones to train again our model and reduce the amount of variables used in our model.","adcb01d2":"We will choose our model with 15 variables:","47b24cbd":"## Conclusion","60f78386":"## RandomForest","898c5e4a":"## Predicting with threshold = 0.2","0e3daa4c":"### Applying K-Fold cross validation","8a44c10d":"#### Note that we exchange Precision for Recall. From a Precision of 100% to 89% and Recall from 92% to 100%. This way, our model predicted 100% of the malignant tumors."}}