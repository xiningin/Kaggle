{"cell_type":{"dd70c5f0":"code","07263809":"code","4683a6fb":"code","8f3deb4a":"code","10f67098":"code","bf49307a":"code","2ee4110a":"code","e4d9a5c9":"code","a8ec9c98":"code","2474bd0e":"code","2c66dea2":"code","54043827":"code","7074bd83":"code","54f10163":"code","3d56fb22":"code","a79ca0ab":"code","53e8c14b":"code","a8dc5072":"code","782ebf42":"code","e513e893":"code","7780432c":"code","0ef022c3":"code","91f58e78":"code","d1dd7d09":"code","2ce3da6d":"code","ef2fcfaf":"code","def1dc9f":"code","8541133e":"code","96efd53e":"code","85021820":"code","c6b88d8f":"code","1c68b890":"code","d4377f09":"code","de5323c0":"markdown","3ca39c75":"markdown","26b033bb":"markdown","6535cb1d":"markdown","736398d0":"markdown","ae6c7423":"markdown","e29a7a64":"markdown","2fc5f578":"markdown","2dd0a97d":"markdown","0014e2b0":"markdown","e09ee5d0":"markdown","be3b6dfd":"markdown","cda88a80":"markdown","eb5c110c":"markdown","f1dd1c0f":"markdown","fce12448":"markdown","316f2076":"markdown","6beb34ad":"markdown","1afa5630":"markdown","4f663afb":"markdown","1c4f72f6":"markdown","e66a10d0":"markdown","8bd1857e":"markdown","08b64abc":"markdown","b849429e":"markdown","48b67a53":"markdown","d3a0fcb3":"markdown"},"source":{"dd70c5f0":"# !pip install Dataset\n!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ..\/input\/hf-datasets\/wheels datasets -qq\n# !pip install fsspec","07263809":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# from transformers import *\nfrom transformers import default_data_collator, Trainer\nfrom transformers import AutoTokenizer, TrainingArguments,AutoModelForQuestionAnswering\nimport tensorflow as tf\n# import collection\nfrom datasets import Dataset\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","4683a6fb":"train = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv')\ntrain.head()","8f3deb4a":"test = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv')\ntest.head()","10f67098":"tokenizer = AutoTokenizer.from_pretrained(\"..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-large-squad2\")","bf49307a":"batch_size = 4\nmax_length = 384 \ndoc_stride = 128\npad_on_right = tokenizer.padding_side == \"right\"","2ee4110a":"def prepare_train_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    # Let's label those examples!\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples[\"answers\"][sample_index]\n        # If no answers are given, set the cls_index as answer.\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            # Start\/end character index of the answer in the text.\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                # Note: we could go after the last offset if the answer is the last word (edge case).\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples","e4d9a5c9":"def convert_answers(r):\n    start = r[0]\n    text = r[1]\n    return {\n        'answer_start': [start],\n        'text': [text]\n    }\n\ntrain = train.sample(frac=1, random_state=42)\ntrain['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)\n\ndf_train = train[:-64].reset_index(drop=True)\ndf_valid = train[-64:].reset_index(drop=True)\n\ntrain_dataset = Dataset.from_pandas(df_train)\nvalid_dataset = Dataset.from_pandas(df_valid)","a8ec9c98":"train_dataset[0]","2474bd0e":"tokenized_train_ds = train_dataset.map(prepare_train_features, batched=True, remove_columns=train_dataset.column_names)\ntokenized_valid_ds = valid_dataset.map(prepare_train_features, batched=True, remove_columns=train_dataset.column_names)","2c66dea2":"%env WANDB_DISABLED=True\nargs = TrainingArguments(\n    f\"chaii-qa\",\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=3e-5,\n    warmup_ratio=0.1,\n    gradient_accumulation_steps=8,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=1,\n    weight_decay=0.01,\n)","54043827":"data_collator = default_data_collator\n\nmodel = AutoModelForQuestionAnswering.from_pretrained('..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-large-squad2')\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_train_ds,\n    eval_dataset=tokenized_valid_ds,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","7074bd83":"trainer.train()\ntrainer.save_model(\"chaii-bert-trained\")","54f10163":"def prepare_validation_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    # We keep the example_id that gave us this feature and we will store the offset mappings.\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n\n        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n        # position is part of the context or not.\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","3d56fb22":"validation_features = valid_dataset.map(\n    prepare_validation_features,\n    batched=True,\n    remove_columns=valid_dataset.column_names\n)","a79ca0ab":"len(validation_features)","53e8c14b":"valid_dataset","a8dc5072":"valid_feats_small = validation_features.map(lambda example: example, remove_columns=['example_id', 'offset_mapping'])\nvalid_feats_small","782ebf42":"raw_predictions = trainer.predict(valid_feats_small)","e513e893":"raw_predictions[0]","7780432c":"max_answer_length = 30","0ef022c3":"import collections\n\nexamples = valid_dataset\nfeatures = validation_features\n\nexample_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\nfeatures_per_example = collections.defaultdict(list)\nfor i, feature in enumerate(features):\n    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)","91f58e78":"from tqdm.auto import tqdm\n\ndef postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n    all_start_logits, all_end_logits = raw_predictions\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    # The dictionaries we have to fill.\n    predictions = collections.OrderedDict()\n\n    # Logging.\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    # Let's loop over all the examples!\n    for example_index, example in enumerate(tqdm(examples)):\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None # Only used if squad_v2 is True.\n        valid_answers = []\n        \n        context = example[\"context\"]\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            # This is what will allow us to map some the positions in our logits to span of texts in the original\n            # context.\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n\n            # Update minimum null prediction.\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            # Go through all possibilities for the `n_best_size` greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n            # failure.\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n        predictions[example[\"id\"]] = best_answer[\"text\"]\n\n    return predictions","d1dd7d09":"final_predictions = postprocess_qa_predictions(valid_dataset, validation_features, raw_predictions.predictions)","2ce3da6d":"prediction = pd.DataFrame([{\"questions\":x1['question'], \"pred_answer\":x2} for x1, x2 in zip(valid_dataset, [i for i in final_predictions.values()])])","ef2fcfaf":"prediction","def1dc9f":"test_dataset = Dataset.from_pandas(test)\n\ntest_features = test_dataset.map(\n    prepare_validation_features,\n    batched=True,\n    remove_columns=test_dataset.column_names\n)","8541133e":"test_feats_small = test_features.map(lambda example: example, remove_columns=['example_id', 'offset_mapping'])\ntest_feats_small","96efd53e":"test_predictions = trainer.predict(test_feats_small)","85021820":"test_features.set_format(type=test_features.format[\"type\"], columns=list(test_features.features.keys()))","c6b88d8f":"final_test_predictions = postprocess_qa_predictions(test_dataset, test_features, test_predictions.predictions)","1c68b890":"test['PredictionString'] = test['id'].apply(lambda r: final_test_predictions[r])\ntest.head()","d4377f09":"test = test.drop(columns=['context','question','language'], axis=1)\ntest.to_csv('submission.csv', index=False)","de5323c0":"Defining a function that will prepare training data for us.","3ca39c75":"Tokenizer","26b033bb":"## Introduction\nMay God bless you with all the data structure skills. Transformers have been revolutionary models that yield state-of-art variants like BERT, GPT, mt5, T5, tapas, Albert, Robert, and many more from their families. The Hugging face library has provided excellent documentation with the implementation of various real-world scenarios.\nHere, we\u2019ll try to implement the Roberta model for the question answering system. Rather than directly diving into our code, we\u2019ll revise some theoretical concepts that are mandatory to understand the code. Believe me, it takes a slightly longer span for a newcomer into Natural language processing to understand the logic behind the code. I tried my best in explaining it conceptually. It\u2019s also the duty of the reader to try understanding the code by going through it simultaneously until you don\u2019t get it, and browsing stuff that you aren\u2019t able to know within this article will definitely work for you.","6535cb1d":"# [amitnikhade.com](http:\/\/amitnikhade.com)","736398d0":"Loading the data","ae6c7423":"## Postprocessing\nPostprocessing converts the predictions of a question-answering model to answers that are substrings of the original contexts. The Postprocessing code consists of nested loops over the examples. It collects the indices of the features of ongoing examples in the loop and also the context. Furthermore, it loops over each of the features in the continuing example and collects predictions on the same features from the model consisting of two arrays containing the start logits and the end logits, respectively. The min_null_score is None which has to use during training on squad v2 data. The null answer is scored as the sum of the start_logit and end_logit associated with the [CLS] token that is our minimum null score. Any sensible combination of start and end logits, i.e. start_logit + end_logit can be considered a possible answer. Higher the combination score higher is the confidence of getting the best answer. If the End token falls before the start token, in this case, it should be excluded. Answers in which the start or end tokens are associated with question tokens are also excluded, as we know the answer to the question will not be obvious in the question. The number of best predictions for each example can be adjusted with the \u2014 n_best_size argument; the code goes through all possibilities to get the best answer. Answers with a length that is either less than 0 or greater than the max_answer_length are not included; neither answer out of scope is considered.","e29a7a64":"Set max_length, batch_size and doc_stride","2fc5f578":"## Model\nRoberta stands for Robustly Optimized BERT Pre-training Approach trained on 160 GB of data like the BookCorpus (Zhu et al., 2015), Wikipedia, and some additional data. Robert is just a BERT acquainted with dynamic masking, and without next sentence prediction, like how usually BERT is pretrained with masked language modelling and next sentence prediction, the NSP is eliminated in the case of Roberta model. I won\u2019t detail the internal model working, which I have already explained in my previous posts.","2dd0a97d":"## Terms you should be familiar of\n\n* Tokens: Tokens are the building blocks of natural language processing. The tokens are created after the tokenization of text. Tokenization can mainly be categorized into word, character, and subword, i.e. the n-gram characters tokenization.\n\n* input_ids: input_ids are token indices; they are the numerical representations of tokens; a list of these representations is called sequences used as input by the model.\n\n* sequence_ids: Sequence_ids tell us which part in the sequence is the question and which of these is the answer. The unique tokens are encoded as None, where 0\u2019s are questions, and 1\u2019s are context.\n\n* offset_mapping: offset_mapping parameter provides us with the token\u2019s position, which is a tuple of the start and end index of the tokens that makes it easy to find their original position.\n\n* overflow_sampling: Like how the examples, i.e. the context, are truncated due to the max_length limit, so the trimmed part is continued in the following sequence with a confident stride rate, the overflowing sample is the map of how many splits is the context fitted. Like in the below image, the 0\u2019s are the first context in the dataset, which was big enough and hence is held in lots of splits.\n\n* max_length: The constant length defined for the sequence to be processed and fed to the model.\n\n* Padding: padding satisfies the sequence with the given max_length like if the max_length is 20 and our text has only 15 words, so after tokenizing it, the text will get padded with 1\u2019s to get the sequence length of 20. The padding can be done from the front as well as the end of the sequence.\n\n* Stride: Stride is the rate with how much token length the truncated sequence should be patched with the next one. It is basically used to handle the overflow.\n\n* Truncation: cutting the sequence after the max_length is reached.\n\n* start_token\/end__token: The starting token of a specific observation needs to be fetched, for example, the start token of answers in the context, and the similar is the end token which is the ending token of that answer.\n\n* attention_mask: Attention masks tell the attention mechanism in the model to exclude the paddings that aren\u2019t the actual tokens. It makes us easy to distinguish between the actual tokens and the padding in the combined sequence.\n","0014e2b0":"## Implementation\nIn this competition, you will be predicting the answers to questions in Hindi and Tamil. The answers are drawn directly (see the Evaluation page for details) from a limited context. We have provided a small number of samples to check your code with. There is also a hidden test set.\nAll files should be encoded as UTF-8.","e09ee5d0":"Let us see how exactly this function works.\nWe\u2019ll be passing the training data to this function. Firstly, the whitespace will be removed, which is present on the left of some questions in train data. Next is one of the extensive steps which is used in every transformer model, the tokenizer. The tokenizer tokenizes sentences into chunks, as shown below.\n\n    <s> sequence_1 <\/s><\/s> sequence_2 <\/s> \n\nWhere <s> is the classifier token and <\/s> is the separator token. We pass the question and context to the tokenizer, checking whether the tokenizer padding position is on the right or left, but it is to the left by default. So the tokenizer will tokenize the sequences along with padding the sequences with max_length to satisfy the given sequence limit. We have added a truncation parameter to the tokenizer that truncates the sequence after the max_length limit. The stride specifies that by what length of tokens the sequence should overlap with the previous overflowed sequence. While truncating sequences, we may lose a lot of data which isn\u2019t a good practice in terms of data science. To overcome this problem, we need to return the overflowed sequences to forward them to further sequence to get overlapped with them. We also return the offset mappings, which gives us the map of the positions of the tokens.\nIn the next step, we extract the sample mapping that counts the split of a pair it has made. As we know the offset_mapping gives us the positional information of the sequences. Lastly, we start collecting the start and end tokens with some logic applied in the loop. We extract the sequence_ids, which helps us to distinguish between the question and context. The cls_index is the classifier token index. The function above processes a single sample at a time. In the case where the answer does not exist in the context, we place the start and end positions. The sample_index is the index of the example containing this span of text, i.e. the sample mapping index. Finally, we\u2019ll collect the start and end index of the specific instance in the loop; if the answer is not in the example, we\u2019ll label it with the CLS token; otherwise, we\u2019ll move the start and end index to the starting ending points of the answers. The function returns the list of preprocessed examples.","be3b6dfd":"Before passing the train data to the feature preparation function, we need to create a dimension named answers consisting of answer_start and answer_text; further, we generate a random sample of the data and represent the Dataset object from a pandas DataFrame.","cda88a80":"# Question-Answering in association with roBERTa\nTake a sip of chaii and refresh your mood with chaii \u2014 Hindi and Tamil Question Answering By google.\namitnikhade.com","eb5c110c":"The trainer.train() starts the training and thereafter it saves the model.","f1dd1c0f":"We will be passing the valid_dataset, validation_features, raw_predictions to the postprocess_qa_predictions function to get the final predictions.","fce12448":"Import libraries","316f2076":"Configure the training parameters. The data_collator automatically performs padding on the model inputs in a batch to the length of the most extended example in the dataset that eliminates the need to set a maximum sequence length that is usually fixed, resulting in an accelerated performance.","6beb34ad":"Prediction","1afa5630":"The above code will be used to preprocess the train and test data by applying the prepare_train_feature function to it.","4f663afb":"The data was taken from the Kaggle competition \u201cchaii \u2014 Hindi and Tamil Question Answering\u201d hosted by Google. Here\u2019s the link to the data. Our task is to Identify the answer to questions found in Indian language passages in the dataset. The columns in the data include id context question answer_text answer_start language.\nThe training set has 747 Hindi and 368 Tamil examples. In the test data, we have provided 5 examples; we don\u2019t have answer_text and answer_start. We have been provided with the context, question, answer_text, and answer_start. We just need to find the answer in the context using start and end span. We just need to predict the answer text.","1c4f72f6":"For validation, we don\u2019t need to compute start and end positions; instead, we\u2019ll collect the examples that combine to make a feature. The sample_mapping key gives us the map that provides information on the correspondence between the context and the split features from it due to the max_length limit. We\u2019ll take the sequence_ids to know where exactly the question and context lie in the sequences. The pad_to_right is True, so the context index will be 1. Lastly, we\u2019ll be setting the offset_mapping to None that aren\u2019t included in the context, making it simpler to detect the context.\nIn the same way as like train set we\u2019ll be applying the prepare_validation_features function to the data.","e66a10d0":"Dependencies","8bd1857e":"The below block of code informs us about the number of features an example is split in and gives us the list of examples and their features.\n","08b64abc":"## Steps included in the project:\nLoading the training and test data that was already given in my case.\nPreprocessing data seems one of the challenging parts of the overall problem. This includes preparing the training and validation features.\nTraining the model on our preprocessed Data\npostprocessing\nprediction","b849429e":"Submission","48b67a53":"Thanks | [Connect me on linkedIn](https:\/\/www.linkedin.com\/in\/theamitnikhade\/) | [amitnikhade.com](http:\/\/amitnikhade.com)","d3a0fcb3":"## ROBERTA Tokenization style\nRoberta uses the byte-level Byte-Pair-Encoding method derived from GPT-2. The vocabulary consists of 50000-word pieces. \\U0120 as the unique character is used in the byte pair encoding, which hasn\u2019t been made visible to us by hugging face. BPE is like a data compression algorithm in which the most common pair of consecutive bytes of data is replaced with a byte that does not occur in that Data.\nConsider data ggghgghggh during encoding this Data. The byte pair gg occurs most often, so we will replace it with K. The rare words are broken down into more subword tokens. So this is a basic idea behind BPE."}}