{"cell_type":{"4ca75409":"code","eeb862f4":"code","dc3b6932":"code","af906658":"code","8f4487b6":"code","2db79f6b":"code","50098ce1":"code","7e0af036":"code","86e13a60":"code","cde00943":"code","9723fd3e":"code","77739bd2":"code","c06b84f8":"code","a6baf5ac":"code","71942655":"code","7ed8ff29":"code","f6a4331e":"code","e1fc25bf":"code","904a3fc7":"code","9323342f":"code","cb4eb186":"code","6efdb733":"code","dcb279bf":"code","876a2c88":"code","97fef109":"code","d15775f8":"code","b5956710":"code","a4276c58":"code","da9a1d5d":"code","cacf27c4":"code","cf1b7a75":"code","e321110f":"code","ef4075b8":"code","9c605824":"code","ed6fac1d":"code","78ed6b30":"code","cc0bc9f0":"code","3791c83f":"code","783af9e9":"code","8a94c158":"code","84b53ed5":"code","e728f9e9":"code","6dab046d":"code","02e29a52":"code","1d524b3c":"code","0d3cead1":"code","b6901fdd":"code","585941a9":"code","14f4aef5":"code","8bf6417a":"markdown","42d014b6":"markdown","70a124da":"markdown","cbe36ed9":"markdown","b8523b2e":"markdown","592795a8":"markdown","c8978c88":"markdown","053fc7c9":"markdown","1a492575":"markdown","2a44ee0c":"markdown","67411044":"markdown","9b816c86":"markdown","8c9aeef3":"markdown","ccbe572d":"markdown","5b07e3c0":"markdown","c0d42919":"markdown","bb7f6b5d":"markdown","3f58b541":"markdown","73f40e00":"markdown","2db8bcdf":"markdown","89c7f7aa":"markdown","4f23cd08":"markdown","8ce6fa4c":"markdown","75a2a7e8":"markdown"},"source":{"4ca75409":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport os\nimport sys\nimport seaborn as sns\n%matplotlib inline\n\nnp.random.seed(42)\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","eeb862f4":"# Getting de training set\nTrain_set = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nTrain_set.head()","dc3b6932":"# Getting de testing set\nTest_set = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nTest_set.head()","af906658":"Train_set.isnull().sum()","8f4487b6":"Train_set.groupby(\"Embarked\").count()","2db79f6b":"mean_age = round(Train_set[\"Age\"].mean())\nTrain_set[\"Age\"].fillna(mean_age, inplace = True)\n\nTrain_set[\"Embarked\"].fillna(\"S\", inplace = True)\n\nTrain_set = Train_set.dropna(axis = 1)\n\nTrain_set.isnull().sum()","50098ce1":"len(Train_set.groupby(\"Ticket\").count())","7e0af036":"dummy_sex = pd.get_dummies(Train_set.Sex)\ndummy_embarked = pd.get_dummies(Train_set.Embarked)\nTrain_set_num = pd.concat([Train_set, dummy_sex[\"male\"], dummy_embarked[['C', 'Q']]], axis = 1)\n\npd.pivot_table(Train_set_num, index = \"male\", values = \"Survived\", aggfunc = np.mean)\n# Very high correlation between being woman and surviving or being man and dying","86e13a60":"pd.pivot_table(Train_set_num, index = ['C', 'Q'], values = \"Survived\", aggfunc = np.mean)\n# Exists correlation between the port and surviving","cde00943":"Train_set_num.drop([\"Ticket\", \"Name\", \"Sex\", \"Embarked\"], inplace = True, axis = 1)\nTrain_set_num.head()","9723fd3e":"plt.figure(figsize=(12,10))\ncor = Train_set_num.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","77739bd2":"sns.boxplot(data = Train_set)","c06b84f8":"sns.boxplot(data = Train_set.Fare)","a6baf5ac":"Train_set_Fare100 = Train_set[Train_set.Fare > 100]\nTrain_set_Fare100[[\"Fare\", \"Survived\"]].groupby(\"Fare\").mean()","71942655":"fare_classes = []\nfor fare in Train_set[\"Fare\"]:\n    if fare < 10:\n        fare_classes.append('0')\n    elif fare < 25:\n        fare_classes.append('1')\n    elif fare < 55:\n        fare_classes.append('2')\n    elif fare < 70:\n        fare_classes.append('3')\n    elif fare < 100:\n        fare_classes.append('4')\n    else:\n        fare_classes.append('5')\n        \nTrain_set[\"Fare_class\"] = fare_classes\nTrain_set.head()","7ed8ff29":"Train_set[[\"Fare_class\", \"Survived\"]].groupby(\"Fare_class\").mean()","f6a4331e":"sns.boxplot(data = Train_set.Age)","e1fc25bf":"age_classes = []\nfor age in Train_set[\"Age\"]:\n    if age < 16:\n        age_classes.append('0')\n    elif age < 32:\n        age_classes.append('1')\n    elif age < 48:\n        age_classes.append('2')\n    elif age < 70:\n        age_classes.append('3')\n    else:\n        age_classes.append('4')\n        \nTrain_set[\"Age_class\"] = age_classes\nTrain_set.head()","904a3fc7":"Train_set[[\"Age_class\", \"Survived\"]].groupby(\"Age_class\").mean()","9323342f":"Train_set[[\"SibSp\", \"Survived\"]].groupby(\"SibSp\").mean()","cb4eb186":"Train_set[[\"Parch\", \"Survived\"]].groupby(\"Parch\").mean()","6efdb733":"Train_set[\"Family_members\"] = Train_set[\"SibSp\"] + Train_set[\"Parch\"]","dcb279bf":"family_members = []\nfor number in Train_set[\"Family_members\"]:\n    if number < 3:\n        family_members.append('a')\n    elif number < 6:\n        family_members.append('b')\n    elif number < 9:\n        family_members.append('c')\n    elif number < 11:\n        family_members.append('d')\n    else:\n        family_members.append('e')\n        \nTrain_set[\"Family_class\"] = family_members\nTrain_set.head()","876a2c88":"Train_set[[\"Family_class\", \"Survived\"]].groupby(\"Family_class\").mean()","97fef109":"dummy_fare = pd.get_dummies(Train_set.Fare_class, prefix = \"fare\")\ndummy_age = pd.get_dummies(Train_set.Age_class, prefix = \"age\")\ndummy_family = pd.get_dummies(Train_set.Family_class, prefix = \"family\")\n\nTrain_set_num = pd.concat([Train_set_num, dummy_fare, dummy_age, dummy_family], axis = 1)\n\nTrain_set_num.columns","d15775f8":"Train_set_num.drop([\"Age\", \"Fare\", \"SibSp\", \"Parch\", \"PassengerId\"], inplace = True, axis = 1)\nTrain_set_num.head()","b5956710":"Cols = Train_set_num.columns\nfeature_cols = Cols.drop(\"Survived\")\n\nX_train = Train_set_num[feature_cols] \ny_train = Train_set_num.Survived","a4276c58":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nmax_score = -1\nbest_degree = -1\n    \nfor degree in range(1,6):\n\n    \n    poly_features = PolynomialFeatures(degree = degree)\n    X_poly = poly_features.fit_transform(X_train)\n    lrc = LogisticRegression(C = 5, solver = 'liblinear', max_iter = 1000)\n\n    lrc.fit(X_poly, y_train)\n    \n    train_score = lrc.score(X_poly, y_train)\n    \n    print(\"Polynomial of grade: {}\".format(degree))\n    print(\"Score on the test set: {}\".format(train_score))\n    \n    if train_score >= max_score:\n        max_score = train_score\n        best_degree = degree\n        \nprint()\nprint(\"The polynomial degree with which the score is the highest is: {}\".format(best_degree))","da9a1d5d":"from sklearn.svm import LinearSVC\n\nlinear_svc = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"svc\", LinearSVC(C = 1, loss = \"hinge\")),\n])\nlinear_svc.fit(X_train.astype(\"float64\"), y_train)\n\nprint(\"Score on the test set: {}\".format(linear_svc.score(X_train.astype(\"float64\"), y_train)))","cacf27c4":"scaler = StandardScaler()\n\nscaler.fit(X_train.astype(\"float64\"))\nX_train_scaled = X_train\nX_train_scaled = scaler.transform(X_train_scaled.astype(\"float64\"))","cf1b7a75":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\n# Polynomial kernel\nparams = {\"degree\": [1,2,3], \"coef0\": [0,0.25,0.5,0.75,1], 'C':[1, 10, 100, 1000]}\nsvc = SVC(kernel = \"poly\")\n\npoly_svc = GridSearchCV(svc, params, cv = 4, n_jobs = -1)\n\npoly_svc.fit(X_train_scaled.astype(\"float64\"), y_train)\n\nprint(\"Best parameters: {}\".format(poly_svc.best_params_))\nprint(\"Score on the test set: {}\".format(poly_svc.score(X_train_scaled.astype(\"float64\"), y_train)))","e321110f":"# Gaussian RBF Kernel\nparams = {\"gamma\": [0,0.12,0.24,0.48,0.60,0.72,0.84,1], 'C':[1, 10, 100, 1000]}\nsvc = SVC(kernel = \"rbf\")\n\nrbf_svc = GridSearchCV(svc, params, cv = 4, n_jobs = -1, iid = True)\n\nrbf_svc.fit(X_train_scaled.astype(\"float64\"), y_train)\n\nprint(\"Best parameters: {}\".format(rbf_svc.best_params_))\nprint(\"Score on the test set: {}\".format(rbf_svc.score(X_train_scaled.astype(\"float64\"), y_train)))","ef4075b8":"from sklearn.neighbors import KNeighborsClassifier\n\nparams = {\"n_neighbors\": [2,4,6,8,10,12,14,16,18,20,25,50]}\nknn = KNeighborsClassifier()\n\nknnc = GridSearchCV(knn, params, cv = 4, n_jobs = -1)\n\nknnc.fit(X_train_scaled.astype(\"float64\"), y_train)\n\nprint(\"Best parameters: {}\".format(knnc.best_params_))\nprint(\"Score on the test set: {}\".format(knnc.score(X_train_scaled.astype(\"float64\"), y_train)))","9c605824":"from sklearn.ensemble import RandomForestClassifier\n\nrnd = RandomForestClassifier()\n\nparams = {\"n_estimators\": [2,4,8,20,50,100,200], 'max_depth':[2,4,6,8,10], 'max_features': [2,4,6,8]}\n\n\nrndc = GridSearchCV(rnd, params, cv = 4, n_jobs = -1, iid = True)\n\nrndc.fit(X_train_scaled.astype(\"float64\"), y_train)\n\nprint(\"Best parameters: {}\".format(rndc.best_params_))\nprint(\"Score on the test set: {}\".format(rndc.score(X_train_scaled.astype(\"float64\"), y_train)))","ed6fac1d":"from sklearn.neural_network import MLPClassifier\n\nmlpc = MLPClassifier(random_state=42, max_iter = 5000)\nmlpc.fit(X_train_scaled, y_train)\n\nprint(\"Score on the test set: {}\".format(mlpc.score(X_train_scaled.astype(\"float64\"), y_train)))","78ed6b30":"from sklearn.ensemble import VotingClassifier\n\nlr_c = Pipeline([\n    (\"poly_features\", PolynomialFeatures(degree = 2)),\n    (\"lr\", LogisticRegression(C = 5, solver = 'liblinear'))\n])\n\nsvcp_c = SVC(kernel = \"poly\", degree = 2, C = 1, coef0 = 0.5, probability = True)\n\nsvc_c = SVC(kernel = \"rbf\", gamma = 0.12, C = 1, probability = True)\n\nknn_c = KNeighborsClassifier(weights = \"uniform\", n_neighbors = 6)\n\nrnd_c = RandomForestClassifier(max_depth = 8, n_estimators = 20, max_features = 2)\n\nmlp_c = MLPClassifier(random_state=42, max_iter = 5000)\n\nvoting_clf = VotingClassifier(\n    estimators = [(\"lr\", lr_c), (\"svcp\", svcp_c), (\"svc\", svc_c),(\"knn\", knn_c),(\"rnd\", rnd_c),(\"mlp\", mlp_c)], \n    voting = \"soft\")\n\nvoting_clf.fit(X_train_scaled, y_train)\n\ntrain_score = voting_clf.score(X_train_scaled, y_train)\n\nprint(\"Train score: {}\".format(train_score))","cc0bc9f0":"Test_set.isnull().sum()","3791c83f":"mean_age_test = round(Test_set[\"Age\"].mean())\nmean_fare_test = round(Test_set[\"Fare\"].mean())\n\nTest_set[\"Age\"].fillna(mean_age_test, inplace = True)\n\nTest_set[\"Fare\"].fillna(mean_fare_test, inplace = True)\n\nTest_set = Test_set.dropna(axis = 1)\n\nTest_set.isnull().sum()","783af9e9":"dummy_sex_test = pd.get_dummies(Test_set.Sex)\ndummy_embarked_test = pd.get_dummies(Test_set.Embarked)\nTest_set_num = pd.concat([Test_set, dummy_sex_test[\"male\"], dummy_embarked_test[['C', 'Q']]], axis = 1)\n\nTest_set_num.drop([\"Ticket\", \"Name\", \"Sex\", \"Embarked\"], inplace = True, axis = 1)\nTest_set_num.head()","8a94c158":"fare_classes = []\nfor fare in Test_set[\"Fare\"]:\n    if fare < 10:\n        fare_classes.append('0')\n    elif fare < 25:\n        fare_classes.append('1')\n    elif fare < 55:\n        fare_classes.append('2')\n    elif fare < 70:\n        fare_classes.append('3')\n    elif fare < 100:\n        fare_classes.append('4')\n    else:\n        fare_classes.append('5')\n        \nTest_set[\"Fare_class\"] = fare_classes\nTest_set.head()","84b53ed5":"age_classes = []\nfor age in Test_set[\"Age\"]:\n    if age < 16:\n        age_classes.append('0')\n    elif age < 32:\n        age_classes.append('1')\n    elif age < 48:\n        age_classes.append('2')\n    elif age < 70:\n        age_classes.append('3')\n    else:\n        age_classes.append('4')\n        \nTest_set[\"Age_class\"] = age_classes\nTest_set.head()","e728f9e9":"Test_set[\"Family_members\"] = Test_set[\"SibSp\"] + Test_set[\"Parch\"]\n\nfamily_members = []\nfor number in Test_set[\"Family_members\"]:\n    if number < 3:\n        family_members.append('a')\n    elif number < 6:\n        family_members.append('b')\n    elif number < 9:\n        family_members.append('c')\n    elif number < 11:\n        family_members.append('d')\n    else:\n        family_members.append('e')\n        \nTest_set[\"Family_class\"] = family_members\nTest_set.head()","6dab046d":"dummy_fare = pd.get_dummies(Test_set.Fare_class, prefix = \"fare\")\ndummy_age = pd.get_dummies(Test_set.Age_class, prefix = \"age\")\ndummy_family = pd.get_dummies(Test_set.Family_class, prefix = \"family\")\n\nTest_set_num = pd.concat([Test_set_num, dummy_fare, dummy_age, dummy_family], axis = 1)\n\nTest_set_num.drop([\"Age\", \"Fare\", \"SibSp\", \"Parch\", \"PassengerId\"], inplace = True, axis = 1)\nTest_set_num.head()","02e29a52":"Cols = Test_set_num.columns\nX_test = Test_set_num[Cols] \n\nscaler = StandardScaler()\n\nscaler.fit(X_test.astype(\"float64\"))\nX_test_scaled = X_test\nX_test_scaled = scaler.transform(X_test_scaled.astype(\"float64\"))","1d524b3c":"X_test_scaled.shape\nX_train.columns","0d3cead1":"predictions = voting_clf.predict(X_test_scaled)\npredictions","b6901fdd":"#Create a  DataFrame with the passengers ids and our prediction regarding whether they survived or not\nsubmission = pd.DataFrame({'PassengerId':Test_set['PassengerId'],'Survived':predictions})\n\nsubmission.head()","585941a9":"#Convert DataFrame to a csv file that can be uploaded\n#This is saved in the same directory as your notebook\nfile_name = 'Titanic Predictions2.csv'\n\ndf = submission\n# import the modules we'll need\nfrom IPython.display import HTML\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = file_name):  \n    csv = df.to_csv(index = False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe\ncreate_download_link(df)","14f4aef5":"submission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","8bf6417a":"#### MLP Classifier ","42d014b6":"###### It seems that the features thast are most correlated to Survived are Pclass, Fare, Sex (male) and Embarked (C and D)","70a124da":"#### Categorical features\n###### The data set has 4 categorical features (not including Cabin): Emberked, Ticket, Name and Sex\n* Ticket will be deleted because it has no correlation (almost each passenger has a different Ticket: 681\/891)\n* The same with Name\n* The sex will be strongly correlated, so I will create a dummy column which has a 1 if it's male or 0 if female\n* Embarked may be correlated too, so the same as with sex will be done","cbe36ed9":"#### Random Forest","b8523b2e":"I get the dummies of the new classes:","592795a8":"### Generating the submission","c8978c88":"#### Categorical features will be replaced with dummy variables again","053fc7c9":"And the same with the family members on board","1a492575":"#### Linear SVC","2a44ee0c":"#### K nearest neighbor","67411044":"#### I scale data again","9b816c86":"#### Lets pick up all the models that have get more than 80% of accuracy on the validation set and let's try to combine them into a better one:\n* Logistic Regression\n* SVC with polynomial Kernel\n* SVC with RBF Kernel\n* K nearest neighbor\n* Random Forest\n* MLP Classifier\n\n### Voting Classifier","8c9aeef3":"#### People who paid more than 100 must have survived, lets see","ccbe572d":"## Model selection\n\n#### Before starting training model, I will split the data into the training set and the validation set (200 instances) to make sure that I do not overfit the data","5b07e3c0":"#### I generate the predictions","c0d42919":"#### Kernelized SVC\n\n###### Polynomial Kernel","bb7f6b5d":"## Feature engineering\n\n#### NaN values\n* They are 177 Age missing values that will be filled by the mean age of the dataset\n* They are 2 Embarked missing values that will be filed with 'S' due to it is by far the most repeated ambarking port in the dataset (644\/891)\n* Most of Cabin values are missing (687\/891), so I will drop it from the dataset","3f58b541":"#### It has less accuracy than many of the predictor used, but many of them are overfitting the data, so the voting classifier is a way to regularize the whole model","73f40e00":"#### As did in the train set, Cabin feature will be eliminated and Age NaN values will be filled with the average age. One instance has also a NaN value on the Fare feature, that will be also filled with the average value","2db8bcdf":"It is a very high correlation betwenn having paid a big fare and surviving. Let's see it by spliting the Fare feature into discrete sections ","89c7f7aa":"#### Gaussian RBF Kernel","4f23cd08":"Let's try the same with the age. It should happen that the higher the age, the more probability of not surviving. Also child may have high probabilities to survive.","8ce6fa4c":"#### Logistic regression","75a2a7e8":"## Loading the data"}}