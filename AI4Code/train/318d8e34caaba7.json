{"cell_type":{"da32367a":"code","8eedf7db":"code","de33ec29":"code","fbcb88f7":"code","7b184929":"code","5c97e0df":"code","953c63b7":"code","f326f460":"code","b63d73c6":"code","73afbf36":"code","2135f946":"code","68b7142c":"code","ab5cf2fb":"code","8fd5ce3c":"code","fb2576f3":"code","23ac8d2b":"code","eba7d015":"code","e5bda59e":"code","31b83162":"code","f24408be":"code","4e31dc66":"code","9e890940":"code","eed5d4fc":"code","332cc6b7":"code","d515ba4e":"code","e1caf7df":"code","2a6874bb":"code","22c982c4":"code","b08cf82c":"code","cff37520":"code","69f5f61a":"code","2e5cd504":"code","439c25e8":"code","53017e19":"code","2f6b9f21":"code","2bd4995b":"code","626d22fc":"code","17bf8775":"code","c32b43c7":"code","74228e7f":"code","b42c8a2e":"code","bdd52d2d":"code","5039626d":"code","9b753703":"code","e33007ea":"code","6b15d9da":"code","e205508a":"code","fbe84222":"code","448f4425":"code","ee298d5c":"code","20397f39":"code","90740a0a":"code","e4a9ee36":"code","8b2546ed":"code","93ae0d1f":"code","4068880f":"code","a2345078":"code","c8f4395c":"code","2e69f0bf":"code","bc27511d":"code","f7d69dcb":"code","d79d5ca9":"code","f3a135fe":"code","eb18e6c4":"code","2650297e":"code","189c2ff0":"code","1e66badc":"code","aa8abbc5":"code","4a3a5f7e":"code","c37f3e3c":"code","413a57be":"code","2dd9dfe4":"code","80a62073":"code","b4ea3881":"code","07ba1367":"code","0c38e5b7":"markdown","08d282d6":"markdown","aa7af261":"markdown","106142ae":"markdown","64862086":"markdown","aed1a556":"markdown","3bf833c1":"markdown","9addd97a":"markdown","914a66cf":"markdown","2cc163c6":"markdown","8c5766cf":"markdown","bb0a5b73":"markdown","86536764":"markdown","6e7744de":"markdown","ad7db383":"markdown","5b93d734":"markdown","3706f22d":"markdown","f7720466":"markdown","dc5dddeb":"markdown","15fab447":"markdown","1c5c0666":"markdown","1143c523":"markdown","e66dd759":"markdown","a45cc5cf":"markdown","dcb98af0":"markdown","48b55cdc":"markdown","8dac6ba8":"markdown","3845e42b":"markdown","e5445a32":"markdown","525cc13d":"markdown","811e47da":"markdown","cc29f7ba":"markdown","cb679d3a":"markdown","297fc48e":"markdown","b1edb8f5":"markdown","aeebd3b9":"markdown","aeece1fe":"markdown","0c1b8ef9":"markdown","c27945f1":"markdown","d2250ad5":"markdown","7056a0cf":"markdown","f7ff907b":"markdown","22564d6d":"markdown","c671ff4b":"markdown","2ce782fe":"markdown","59132d91":"markdown","4ba3fa05":"markdown","057fd6a1":"markdown","ed37f5eb":"markdown","3f3f6688":"markdown","8648b4ca":"markdown","e3c9c16c":"markdown","9438fa79":"markdown","14637c22":"markdown","443c5c59":"markdown","4379f53a":"markdown","076a1569":"markdown","48885f43":"markdown","4f7b1563":"markdown","6c940144":"markdown","b61b2085":"markdown","76b94007":"markdown","592e4c1e":"markdown","c88fe03a":"markdown","faee5355":"markdown","f08c7286":"markdown","cf008b15":"markdown","7307f205":"markdown","2f4fc462":"markdown","cc589aa7":"markdown","45102637":"markdown","c5e30d77":"markdown","a5a99e48":"markdown","db8f5545":"markdown","fd3b6d8e":"markdown","4c1fd2cf":"markdown","1123f295":"markdown","aeaa8a5d":"markdown","14ad5c56":"markdown","25aebbf5":"markdown","a53b1f58":"markdown","bf61b9fe":"markdown","1817f3e0":"markdown","9341da31":"markdown","08f699ca":"markdown","8953c6b5":"markdown","8fab3c28":"markdown","a2c4029b":"markdown","c398ede1":"markdown","01235fd2":"markdown","5b6dcdb9":"markdown","46cc5b03":"markdown","2f27a72c":"markdown","a2697cf4":"markdown","7b46d227":"markdown","d5b5242d":"markdown","d41c5613":"markdown","1092b7a2":"markdown","cd424dec":"markdown","cd87c0ab":"markdown","a3a91189":"markdown","ae3fe38c":"markdown","d0bffaca":"markdown","98679a1e":"markdown","931e5706":"markdown","56eb9d2d":"markdown","3572ac13":"markdown","d48e86a7":"markdown","7d606706":"markdown"},"source":{"da32367a":"import pandas as pd\nimport numpy as np\nimport seaborn as sn\nfrom matplotlib import pyplot as plt\nimport re\nimport string\nimport warnings\nimport collections\nimport pickle\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('ggplot')","8eedf7db":"import spacy\nimport en_core_web_sm\nfrom spacy.attrs import *\n\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, hamming_loss, ConfusionMatrixDisplay, multilabel_confusion_matrix, confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\nfrom sklearn.multioutput import MultiOutputClassifier\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Embedding, GlobalMaxPool1D\nfrom keras.optimizers import Adam\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom gensim.models import word2vec\nfrom gensim.models import Doc2Vec\nfrom gensim.models.doc2vec import TaggedDocument\n\nimport multiprocessing\ncores = multiprocessing.cpu_count()\nfrom gensim.models.fasttext import FastText","de33ec29":"nlp = en_core_web_sm.load()","fbcb88f7":"df= pd.read_csv('..\/input\/arxiv-corpus-with-abstracts-and-paper-classes\/exportarxiv.csv')\ndel df['Title']\ndf['Abstract']=df['Abstract'].replace(to_replace='Abstract: ', value='', regex=True)\ndf.head()","7b184929":"df.info()","5c97e0df":"subjects=[]\nfor index, row in df.iterrows():\n    subjects=[x.strip() for x in row['Subjects'].split(';') if str(x.strip().split('(')[1]).startswith('cs')]\n    if len(subjects)>0:\n        df.loc[index,'MainSubject']=subjects[0]\n        for s in subjects:\n            df.loc[index,s]=1\n    else:\n        df.drop(index, inplace=True)\ndf.fillna(0, inplace=True)","953c63b7":"df.info()","f326f460":"subjects=list(df)[3:]\npickle.dump(subjects, open('subject_list.sav','wb'))","b63d73c6":"def preprocessTraining(text):\n    text=text.lower()\n    text=re.sub('\\r\\n', ' ', text)\n    text=re.sub('\\n', ' ', text)\n    text=str(\" \".join([t.text for t in nlp(text) if t.pos_ != 'PUNCT']))\n    text=str(\" \".join([t.lemma_ if t.lemma_ != \"-PRON-\" else t.text for t in nlp(text)]))\n    text=str(\" \".join([t.text for t in nlp(text) if not t.is_stop]))\n    text=re.sub('    ', ' ', text)\n    text=re.sub('   ', ' ', text)\n    text=re.sub('  ', ' ', text)\n    return text.strip()","73afbf36":"%%time\ndf['Abstract']=df['Abstract'].apply(lambda x: preprocessTraining(x))","2135f946":"dfg=df[['Abstract','MainSubject']].groupby('MainSubject').count().sort_values('Abstract', ascending=False)\ndfg.info()","68b7142c":"print('Baseline classifier accuracy: '+str(dfg['Abstract'].max()\/dfg['Abstract'].sum()))","ab5cf2fb":"plt.rcParams.update({'font.size': 12})\nplt.figure(1,figsize=(15,8))\nplt.bar(dfg.index, dfg.Abstract)\nplt.grid(color='gray', ls = '-.', lw = 0.2)\nplt.xticks(rotation=90)\nplt.ylabel('Number of articles')\nplt.show()","8fd5ce3c":"labelencoder = LabelEncoder()\ndf['MainSubject_code'] = labelencoder.fit_transform(df['MainSubject'])","fb2576f3":"pickle.dump(labelencoder, open('model_labelencoder.sav','wb'))","23ac8d2b":"X_train, X_test, y_train, y_test = train_test_split(df['Abstract'], \n                                                    df['MainSubject_code'], \n                                                    test_size=0.25,\n                                                    random_state=8, stratify=df['MainSubject_code'])\n\nX_trainM, X_testM, y_trainM, y_testM = train_test_split(df['Abstract'], \n                                                    df.iloc[:,3:41].values, \n                                                    test_size=0.25, \n                                                    random_state=8, stratify=df['MainSubject_code'])","eba7d015":"%%time\nngram_range = (1,3)\nmin_df = 10\nmax_df = 1.\nmax_features = 4000\n\ntfidf = TfidfVectorizer(encoding='utf-8',\n                        ngram_range=ngram_range,\n                        stop_words=None,\n                        lowercase=False,\n                        max_df=max_df,\n                        min_df=min_df,\n                        max_features=max_features,\n                        norm='l2',\n                        sublinear_tf=True)\ncv = CountVectorizer(ngram_range=ngram_range,\n                     stop_words=None,\n                     lowercase=False,\n                     max_features=max_features,\n                     max_df=max_df,\n                     min_df=min_df)\nhv = HashingVectorizer(n_features=2**10)\n                        \nfeatures_train = tfidf.fit_transform(X_train).toarray()\nfeatures_trainCV = cv.fit_transform(X_train).toarray()\nfeatures_trainHV = hv.fit_transform(X_train).toarray()\nlabels_train = y_train\n\nfeatures_test = tfidf.transform(X_test).toarray()\nfeatures_testCV = cv.transform(X_test).toarray()\nfeatures_testHV = hv.transform(X_test).toarray()\nlabels_test = y_test\n\nprint(features_train.shape)\nprint(features_test.shape)","e5bda59e":"pptext=[]\nfor x in X_train:\n    pptextrow=x.split(' ')\n    pptext.append(pptextrow)\npptexttest=[]\nfor x in X_test:\n    pptextrow=x.split(' ')\n    pptexttest.append(pptextrow)","31b83162":"%%time\n\ndef getW2VModel(text, feature_size=100, window_context=30, min_word_count=1, sample=1e-3, sg=0, workers=cores):\n    w2v_model = word2vec.Word2Vec(text, \n                              size=feature_size,\n                              seed=8,\n                              window=window_context, \n                              min_count=min_word_count,\n                              sg=sg,\n                              workers=workers,\n                              sample=sample, \n                              iter=50)\n    return w2v_model\n\nfeature_size = 100\nwindow_context = 30\nsg=0\nsample=1e-3\nworkers=cores\nwindow_context=30\n\nw2v_model_train = getW2VModel(pptext, window_context=window_context, feature_size=feature_size, sg=sg)","f24408be":"print(w2v_model_train['system'])","4e31dc66":"plt.rcParams[\"figure.figsize\"] = (13,10)\nplt.rcParams.update({'font.size': 10})\nlimit=60\n\nX = w2v_model_train[w2v_model_train.wv.vocab]\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\nplt.scatter(result[:limit, 0], result[:limit, 1], s=5)\nwords = list(w2v_model_train.wv.vocab)[:limit]\nfor i, word in enumerate(words):\n    plt.annotate(word, xy=(result[i, 0]+0.1, result[i, 1]+0.1))\nplt.show()","9e890940":"def averaged_word_vectorizer(corpus, model, num_features):\n    result=[]\n    for sent in corpus:\n        mean = []\n        for word in sent:\n            if word in model.wv.vocab:\n                mean.append(model.wv.get_vector(word))\n        mean = np.array(mean).mean(axis=0)\n        result.append(mean)\n    return np.array(result)","eed5d4fc":"%%time\nw2v_feature_array_train = averaged_word_vectorizer(corpus=pptext, \n                                                   model=w2v_model_train,\n                                                   num_features=feature_size)\nw2v_feature_array_test = averaged_word_vectorizer(corpus=pptexttest, \n                                                  model=w2v_model_train,\n                                                  num_features=feature_size)\ndfw2vtrain=pd.DataFrame(w2v_feature_array_train)\ndfw2vtest=pd.DataFrame(w2v_feature_array_test)","332cc6b7":"dfw2vtest.shape","d515ba4e":"class TfidfEmbeddingVectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n        self.word2weight = None\n        self.dim = len(word2vec.wv.vocab)\n\n    def fit(self, X):\n        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n        tfidf.fit(X)\n        max_idf = max(tfidf.idf_)\n        self.word2weight = collections.defaultdict(\n            lambda: max_idf,\n            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n        return self\n\n    def transform(self, X):\n        return np.array([\n                np.mean([self.word2vec[w] * self.word2weight[w]\n                         for w in words if w in self.word2vec] or\n                        [np.zeros(self.dim)], axis=0)\n                for words in X\n            ])","e1caf7df":"%%time\ntevtr=TfidfEmbeddingVectorizer(w2v_model_train)\ntevtr.fit(pptext)\nw2v_feature_array_train_tev = tevtr.transform(pptext)\nw2v_feature_array_test_tev = tevtr.transform(pptexttest)\ndftevtrtrain=pd.DataFrame(w2v_feature_array_train_tev)\ndftevtrtest=pd.DataFrame(w2v_feature_array_test_tev)","2a6874bb":"dftevtrtrain.shape","22c982c4":"def getD2VModel(text):\n    d2v_model = Doc2Vec(dm=0, \n                        vector_size=100,\n                        window=30,\n                        negative=5, \n                        hs=1, \n                        min_count=1, \n                        sample=1e-3, \n                        epochs=50, \n                        workers=cores)\n    d2v_model.build_vocab(text)\n    return d2v_model","b08cf82c":"%%time\ndocuments_train = [TaggedDocument(doc, [i]) for i, doc in enumerate(pptext)]\nd2v_model = getD2VModel(documents_train)\nd2v_model.train(documents_train, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)","cff37520":"testlist=[]\nfor x in pptexttest:\n    testlist.append(d2v_model.infer_vector(x))\n\ndfd2vtrain=pd.DataFrame([doc for doc in d2v_model.docvecs.vectors_docs])\ndfd2vtest=pd.DataFrame(testlist)","69f5f61a":"%%time\nmodel=LogisticRegression(random_state = 8)\ntest_accuracies= collections.defaultdict(dict)\nfor x in [1000,3000,5000,7000,9000,11660]:\n    for c in ['CV','TFIDF','HV','W2VMEV','W2VTV','D2V']:\n        if(c=='CV'): \n            ftr=features_trainCV\n            fts=features_testCV\n        if(c=='TFIDF'):\n            ftr=features_train\n            fts=features_test\n        if(c=='HV'):\n            ftr=features_trainHV\n            fts=features_testHV\n        if(c=='W2VMEV'):\n            ftr=dfw2vtrain.values\n            fts=dfw2vtest.values\n        if(c=='W2VTV'):\n            ftr=dftevtrtrain.values\n            fts=dftevtrtest.values\n        if(c=='D2V'):\n            ftr=dfd2vtrain.values\n            fts=dfd2vtest.values\n        model.fit(ftr[:x], labels_train[:x])\n        labels_pred = model.predict(fts)\n        accscoretest=accuracy_score(labels_test, labels_pred)\n        test_accuracies[c][x]=accscoretest\n        print('Accuracy ('+c+') for dataset size:'+str(x)+' is:'+str(accscoretest))","2e5cd504":"plt.rcParams[\"figure.figsize\"] = (8,6)\nplt.rcParams.update({'font.size': 14})\nplt.plot(list(test_accuracies['CV'].keys()), list(test_accuracies['CV'].values()), 'r', label='Count Vectorization')\nplt.plot(list(test_accuracies['TFIDF'].keys()), list(test_accuracies['TFIDF'].values()), 'b', label='TF-IDF Vectorization')\nplt.plot(list(test_accuracies['HV'].keys()), list(test_accuracies['HV'].values()), 'g', label='Hashing Vectorization')\nplt.plot(list(test_accuracies['W2VMEV'].keys()), list(test_accuracies['W2VMEV'].values()), 'c', label='Word2Vec (with averaging)')\nplt.plot(list(test_accuracies['W2VTV'].keys()), list(test_accuracies['W2VTV'].values()), 'c--', label='Word2Vec (with tf-idf)')\nplt.plot(list(test_accuracies['D2V'].keys()), list(test_accuracies['D2V'].values()), 'y', label='Doc2Vec')\nplt.xlabel('Training set size')\nplt.ylabel('Accuracy')\nplt.grid(color='gray', ls = '-.', lw = 0.3)\nplt.legend()\nplt.show()","439c25e8":"%%time\n\ndef getModels():\n    models = []\n    models.append(('LR', LogisticRegression(random_state = 8)))\n    models.append(('RF', RandomForestClassifier(random_state = 8)))\n    models.append(('SVM', svm.SVC(random_state=8)))\n    return models\n\ndef getAccuracies(features_train, labels_train, features_test, labels_test):\n    test_accuracies= collections.defaultdict(dict)\n    test_precisions= collections.defaultdict(dict)\n    test_recalls= collections.defaultdict(dict)\n    train_accuracies= collections.defaultdict(dict)\n    train_precisions= collections.defaultdict(dict)\n    train_recalls= collections.defaultdict(dict)\n    models= collections.defaultdict(dict)\n    for modelname, model in getModels():\n        for x in [1000,3000,5000,7000,9000,11660]:\n            model.fit(features_train[:x], labels_train[:x])\n            labels_pred = model.predict(features_test)\n            labels_train_pred=model.predict(features_train[:x])\n            accscoretest=accuracy_score(labels_test, labels_pred)\n            accscoretrain=accuracy_score(labels_train[:x], labels_train_pred)\n            test_accuracies[modelname][x]=accscoretest\n            train_accuracies[modelname][x]=accscoretrain\n            test_precisions[modelname][x]=precision_score(labels_test, labels_pred, average='macro')\n            train_precisions[modelname][x]=recall_score(labels_train[:x], labels_train_pred, average='macro')\n            test_recalls[modelname][x]=precision_score(labels_test, labels_pred, average='macro')\n            train_recalls[modelname][x]=recall_score(labels_train[:x], labels_train_pred, average='macro')\n            models[modelname][x]=model\n            print('Accuracy ('+modelname+') for '+str(x)+' training rows. Train set:'+str(accscoretrain)+' Test set:'+str(accscoretest))\n    return test_accuracies, train_accuracies, test_precisions, train_precisions, test_recalls, train_recalls, models\n\ntest_accuracies, train_accuracies, test_precisions, train_precisions, test_recalls, train_recalls, models=getAccuracies(features_train, \n                                                                                                                        labels_train, \n                                                                                                                        features_test, \n                                                                                                                        labels_test)","53017e19":"plt.rcParams.update({'font.size': 14})\nplt.figure(figsize=(18, 8))\nplt.subplot(1, 3, 1)\nplt.plot(list(test_accuracies['LR'].keys()), list(test_accuracies['LR'].values()), 'r', label='Logistic Regression')\nplt.plot(list(test_accuracies['RF'].keys()), list(test_accuracies['RF'].values()), 'b', label='Random Forest')\nplt.plot(list(test_accuracies['SVM'].keys()), list(test_accuracies['SVM'].values()), 'g', label='SVM')\nplt.plot(list(train_accuracies['LR'].keys()), list(train_accuracies['LR'].values()), 'r', linestyle='dashed')\nplt.plot(list(train_accuracies['RF'].keys()), list(train_accuracies['RF'].values()), 'b', linestyle='dashed')\nplt.plot(list(train_accuracies['SVM'].keys()), list(train_accuracies['SVM'].values()), 'g', linestyle='dashed')\nplt.xlabel('Training set size')\nplt.ylabel('Accuracy')\nplt.grid(color='gray', ls = '-.', lw = 0.3)\nplt.legend()\nplt.subplot(1, 3, 2)\nplt.plot(list(test_precisions['LR'].keys()), list(test_precisions['LR'].values()), 'r', label='Logistic Regression')\nplt.plot(list(test_precisions['RF'].keys()), list(test_precisions['RF'].values()), 'b', label='Random Forest')\nplt.plot(list(test_precisions['SVM'].keys()), list(test_precisions['SVM'].values()), 'g', label='SVM')\nplt.plot(list(train_precisions['LR'].keys()), list(train_precisions['LR'].values()), 'r', linestyle='dashed')\nplt.plot(list(train_precisions['RF'].keys()), list(train_precisions['RF'].values()), 'b', linestyle='dashed')\nplt.plot(list(train_precisions['SVM'].keys()), list(train_precisions['SVM'].values()), 'g', linestyle='dashed')\nplt.xlabel('Training set size')\nplt.ylabel('Precision')\nplt.grid(color='gray', ls = '-.', lw = 0.3)\nplt.legend()\nplt.subplot(1, 3, 3)\nplt.plot(list(test_recalls['LR'].keys()), list(test_recalls['LR'].values()), 'r', label='Logistic Regression')\nplt.plot(list(test_recalls['RF'].keys()), list(test_recalls['RF'].values()), 'b', label='Random Forest')\nplt.plot(list(test_recalls['SVM'].keys()), list(test_recalls['SVM'].values()), 'g', label='SVM')\nplt.plot(list(train_recalls['LR'].keys()), list(train_recalls['LR'].values()), 'r', linestyle='dashed')\nplt.plot(list(train_recalls['RF'].keys()), list(train_recalls['RF'].values()), 'b', linestyle='dashed')\nplt.plot(list(train_recalls['SVM'].keys()), list(train_recalls['SVM'].values()), 'g', linestyle='dashed')\nplt.xlabel('Training set size')\nplt.ylabel('Recall')\nplt.grid(color='gray', ls = '-.', lw = 0.3)\nplt.legend()\nplt.show()","2f6b9f21":"acc3000=[0.357, 0.48847457627118646, 0.536271186440678, 0.5511864406779661, 0.5671186440677967, 0.5827118644067797]\nacc4000=[0.341, 0.4847457627118644, 0.5338983050847458, 0.5552542372881356, 0.5728813559322034, 0.5938983050847457]\nacc5000=[0.338, 0.4810169491525424, 0.536271186440678, 0.559322033898305, 0.5738983050847457, 0.5935593220338983]\nacc6000=[0.326, 0.48033898305084743, 0.5352542372881356, 0.56, 0.5789830508474576, 0.5935593220338983]\nacc7000=[0.323, 0.4752542372881356, 0.5349152542372881, 0.5596610169491525, 0.5759322033898305, 0.5935593220338983]\nx=[1000,3000,5000,7000,9000,11700]","2bd4995b":"plt.rcParams[\"figure.figsize\"] = (6,10)\nplt.rcParams.update({'font.size': 14})\nplt.plot(x,acc3000, label='Model with 3000 features')\nplt.plot(x,acc4000, label='Model with 4000 features')\nplt.plot(x,acc5000, label='Model with 5000 features')\nplt.plot(x,acc6000, label='Model with 6000 features')\nplt.plot(x,acc7000, label='Model with 7000 features')\nplt.xlabel('Training set size')\nplt.ylabel('Accuracy')\nplt.grid(color='gray', ls = '-.', lw = 0.3)\nplt.legend()\nplt.show()","626d22fc":"mod=models['LR'][11660]\nlabels_pred = mod.predict(features_test)\naccscoretest=accuracy_score(labels_test, labels_pred)\n\nclasses=list(labelencoder.inverse_transform(range(0,38)))\n\nprint(\"Classification report\")\nprint(classification_report(labels_test,labels_pred, target_names=classes))","17bf8775":"plt.rcParams[\"figure.figsize\"] = (15,12)\nplt.rcParams.update({'font.size': 9})\ncf=confusion_matrix(labels_test, labels_pred)\nsn.heatmap(cf, annot=True, cmap='binary', fmt='g',\n           cbar=False,\n           cbar_kws= {'orientation': 'horizontal'},\n           xticklabels=classes, \n           yticklabels=classes)\nplt.show()","c32b43c7":"rfbestparams={'n_estimators': 600,\n 'min_samples_split': 2,\n 'min_samples_leaf': 2,\n 'max_features': 'auto',\n 'max_depth': 70,\n 'bootstrap': False}","74228e7f":"svmbestparams={'kernel': 'rbf', 'gamma': 0.1, 'class_weight': None, 'C': 10}","b42c8a2e":"%%time\n\ndef getOptimizedModels(rfparams, svmparams):\n    models = []\n    models.append(('RF', RandomForestClassifier()))\n    rf_opt=RandomForestClassifier()\n    rf_opt.set_params(**rfparams)\n    models.append(('RFOpt', rf_opt))\n    models.append(('SVM', svm.SVC()))\n    svm_opt=svm.SVC()\n    svm_opt.set_params(**svmparams)\n    models.append(('SVMOpt', svm_opt))\n    return models\n\ndef getOptAccuracies(features_train, labels_train, features_test, labels_test, rfparams, svmparams):\n    test_accuracies= collections.defaultdict(dict)\n    test_precisions= collections.defaultdict(dict)\n    test_recalls= collections.defaultdict(dict)\n    train_accuracies= collections.defaultdict(dict)\n    train_precisions= collections.defaultdict(dict)\n    train_recalls= collections.defaultdict(dict)\n    models= collections.defaultdict(dict)\n    for modelname, model in getOptimizedModels(rfparams, svmparams):\n        for x in [1000,3000,5000,7000,9000,11660]:\n            model.fit(features_train[:x], labels_train[:x])\n            labels_pred = model.predict(features_test)\n            labels_train_pred=model.predict(features_train[:x])\n            accscoretest=accuracy_score(labels_test, labels_pred)\n            accscoretrain=accuracy_score(labels_train[:x], labels_train_pred)\n            test_accuracies[modelname][x]=accscoretest\n            train_accuracies[modelname][x]=accscoretrain\n            test_precisions[modelname][x]=precision_score(labels_test, labels_pred, average='macro')\n            train_precisions[modelname][x]=recall_score(labels_train[:x], labels_train_pred, average='macro')\n            test_recalls[modelname][x]=precision_score(labels_test, labels_pred, average='macro')\n            train_recalls[modelname][x]=recall_score(labels_train[:x], labels_train_pred, average='macro')\n            models[modelname][x]=model\n            print('Accuracy ('+modelname+') for '+str(x)+' training rows. Train set:'+str(accscoretrain)+' Test set:'+str(accscoretest))\n    return test_accuracies, train_accuracies, test_precisions, train_precisions, test_recalls, train_recalls, models\n\ntest_accuracies, train_accuracies, test_precisions, train_precisions, test_recalls, train_recalls, models=getOptAccuracies(features_train,labels_train,features_test,labels_test,rfbestparams,svmbestparams)","bdd52d2d":"plt.rcParams.update({'font.size': 14})\nplt.figure(figsize=(18, 8))\nplt.subplot(1, 3, 1)\nplt.plot(list(test_accuracies['RF'].keys()), list(test_accuracies['RF'].values()), 'b', label='Random Forest')\nplt.plot(list(test_accuracies['SVM'].keys()), list(test_accuracies['SVM'].values()), 'g', label='SVM')\nplt.plot(list(test_accuracies['RFOpt'].keys()), list(test_accuracies['RFOpt'].values()), 'b--')\nplt.plot(list(test_accuracies['SVMOpt'].keys()), list(test_accuracies['SVMOpt'].values()), 'g--')\nplt.xlabel('Training set size')\nplt.ylabel('Accuracy')\nplt.grid(color='gray', ls = '-.', lw = 0.3)\nplt.legend()\nplt.subplot(1, 3, 2)\nplt.plot(list(test_precisions['RF'].keys()), list(test_precisions['RF'].values()), 'b', label='Random Forest')\nplt.plot(list(test_precisions['SVM'].keys()), list(test_precisions['SVM'].values()), 'g', label='SVM')\nplt.plot(list(test_precisions['RFOpt'].keys()), list(test_precisions['RFOpt'].values()), 'b--')\nplt.plot(list(test_precisions['SVMOpt'].keys()), list(test_precisions['SVMOpt'].values()), 'g--')\nplt.xlabel('Training set size')\nplt.ylabel('Precision')\nplt.grid(color='gray', ls = '-.', lw = 0.3)\nplt.legend()\nplt.subplot(1, 3, 3)\nplt.plot(list(test_recalls['RF'].keys()), list(test_recalls['RF'].values()), 'b', label='Random Forest')\nplt.plot(list(test_recalls['SVM'].keys()), list(test_recalls['SVM'].values()), 'g', label='SVM')\nplt.plot(list(test_recalls['RFOpt'].keys()), list(test_recalls['RFOpt'].values()), 'b--')\nplt.plot(list(test_recalls['SVMOpt'].keys()), list(test_recalls['SVMOpt'].values()), 'g--')\nplt.xlabel('Training set size')\nplt.ylabel('Recall')\nplt.grid(color='gray', ls = '-.', lw = 0.3)\nplt.legend()\nplt.show()","5039626d":"features_trainM = tfidf.fit_transform(X_trainM).toarray()\nlabels_trainM = y_trainM\nfeatures_testM = tfidf.transform(X_testM).toarray()\nlabels_testM = y_testM","9b753703":"\ndef hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n    '''\n    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n    https:\/\/stackoverflow.com\/q\/32239577\/395857\n    '''\n    acc_list = []\n    for i in range(y_true.shape[0]):\n        set_true = set( np.where(y_true[i])[0] )\n        set_pred = set( np.where(y_pred[i])[0] )\n        tmp_a = None\n        if len(set_true) == 0 and len(set_pred) == 0:\n            tmp_a = 1\n        else:\n            tmp_a = len(set_true.intersection(set_pred))\/\\\n                    float( len(set_true.union(set_pred)) )\n        acc_list.append(tmp_a)\n    return np.mean(acc_list)\n\n\n\ndef getMultiLabelAccuracies(model, features_trainM, labels_trainM, features_testM, labels_testM):\n    \n    multi_target_predictor = MultiOutputClassifier(model, n_jobs=-1)\n    multi_target_predictor.fit(features_trainM, labels_trainM)\n    labels_pred = multi_target_predictor.predict(features_testM)\n    accscoretest=accuracy_score(labels_testM, labels_pred)\n    hammscoretest=hamming_score(labels_testM, labels_pred)\n    precscoretest=precision_score(labels_testM, labels_pred, average='macro')\n    recscoretest=precision_score(labels_testM, labels_pred, average='macro')\n    return accscoretest, precscoretest, recscoretest, hammscoretest, multi_target_predictor","e33007ea":"%%time\nmodel=svm.SVC(kernel='rbf',gamma=0.1,class_weight=None,C=10)\n\naccscoretest, precscoretest, recscoretest, hammscoretest, model=getMultiLabelAccuracies(model,features_trainM,labels_trainM,features_testM,labels_testM)","6b15d9da":"print('Accuracy: '+str(accscoretest))\nprint('Precision: '+str(precscoretest))\nprint('Recall: '+str(recscoretest))\nprint('Hamming score: '+str(hammscoretest))","e205508a":"exp=collections.defaultdict(dict)\n\nexp['SVM default']['ham']=0.3678283607681756\nexp['SVM default']['acc']=0.29038065843621397\nexp['SVM default']['pre']=0.6994747813450405\nexp['SVM optimized']['ham']=0.47678326474622773\nexp['SVM optimized']['acc']=0.3626543209876543\nexp['SVM optimized']['pre']=0.7149146466625207\nexp['Logistic Regression']['ham']=0.3193287037037037\nexp['Logistic Regression']['acc']=0.24845679012345678\nexp['Logistic Regression']['pre']=0.661761098889519\nexp['RF default']['ham']=0.2580889917695473\nexp['RF default']['acc']=0.20550411522633744\nexp['RF default']['pre']=0.6463436918449496\nexp['RF optimized']['ham']=0.2532278806584362\nexp['RF optimized']['acc']=0.2052469135802469\nexp['RF optimized']['pre']=0.6122234415956405","fbe84222":"vacc=[]\nvpre=[]\nvham=[]\nlabels=list(exp.keys())\nx = np.arange(len(labels))\nwidth = 0.2\n\nfor key, value in exp.items():\n    vacc.append(value['acc'])\n    vpre.append(value['pre'])\n    vham.append(value['ham'])\n\nplt.rcParams.update({'font.size': 12})\nx = np.arange(len(exp.keys()))\n\n\nplt.figure(1,figsize=(12,6))\n\nplt.bar(x-width, vham, width=width, color='b', label='Hamming score')\nplt.bar(x, vacc, width=width, color='y', label='Accuracy')\nplt.bar(x+width, vpre, width=width, color='r', label='Precision')\nplt.xticks(np.arange(len(labels)), labels,rotation=90)\n\nplt.grid(color='gray', ls = '-.', lw = 0.2)\nplt.ylabel('Hamming score\/Accuracy\/Precision')\nplt.legend()\nplt.show()","448f4425":"plt.rcParams.update({'font.size': 14})\n\ndef plotLoss(history, acc):\n    history_dict = history.history\n    acc_values = history_dict['accuracy']\n    val_acc_values = history_dict['val_accuracy']\n    loss_values = history_dict['loss']\n    val_loss_values = history_dict['val_loss']\n    epochs = range(1, len(history_dict['accuracy']) + 1)\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, acc_values, 'b', label='Training accuracy')\n    plt.plot(epochs, val_acc_values, 'r', label='Validation accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.grid(color='gray', ls = '-.', lw = 0.3)\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, loss_values, 'b', label='Training loss')\n    plt.plot(epochs, val_loss_values, 'r', label='Validation loss')\n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.grid(color='gray', ls = '-.', lw = 0.3)\n    plt.legend()\n    plt.show()\n    \ndef getAcc(model, X_test, y_test):\n    predictions = model.predict(X_test)\n    predictions[predictions>=0.5] = 1\n    predictions[predictions<0.5] = 0\n    acc = accuracy_score(y_test, predictions)\n    prec = precision_score(y_test, predictions, average='macro')\n    rec = recall_score(y_test, predictions, average='macro')\n    f1 = f1_score(y_test, predictions, average='macro')\n    return predictions, acc, prec, rec, f1\n\ndef getFittedANNModel(annfttrain, labels_train_cat, scale, stratify):\n    scaler = StandardScaler()\n    if(scale==1):\n        scaler.fit(annfttrain)\n        annfttrain = scaler.transform(annfttrain)\n    \n    if(stratify):\n        X_train, X_test, y_train, y_test = train_test_split(annfttrain, \n                                                        labels_train_cat, \n                                                        test_size=0.25, \n                                                        random_state=42, \n                                                        stratify=labels_train_cat)\n    else:\n        X_train, X_test, y_train, y_test = train_test_split(annfttrain, \n                                                        labels_train_cat, \n                                                        test_size=0.25, \n                                                        random_state=42)\n    annmodel = Sequential()\n    annmodel.add(Dense(2048, input_dim=annfttrain.shape[1], activation='relu'))\n    annmodel.add(Dense(2048, activation='relu'))\n    annmodel.add(Dense(38, activation='softmax'))\n    annmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    history=annmodel.fit(X_train,\n                         y_train,\n                         validation_data=(X_test,y_test), \n                         epochs=20, batch_size=64, verbose=0)\n    return history, scaler,annmodel","ee298d5c":"%%time\nhistory, scaler, annmodel=getFittedANNModel(features_train, to_categorical(labels_train), 0, True)\npredictions, acc, prec, rec, f1=getAcc(annmodel, features_test, to_categorical(labels_test))\nplotLoss(history, acc)\nprint('Accuracy: ' + str(acc))\nprint('Precision: ' + str(prec))\nprint('Recall: ' + str(rec))\nprint('F1 score: ' + str(f1))","20397f39":"%%time\ntest_accuracies_ann= collections.defaultdict(dict)\nfor x in [1000,3000,5000,7000,9000,11660]:\n    historyX, scalerX, annmodelX=getFittedANNModel(features_train[:x], to_categorical(labels_train)[:x], 0, False)\n    predictions, acc, prec, rec, f1=getAcc(annmodelX, features_test, to_categorical(labels_test))\n    test_accuracies_ann['acc'][x]=acc\n    test_accuracies_ann['prec'][x]=prec\n    test_accuracies_ann['rec'][x]=rec\n    test_accuracies_ann['f1'][x]=f1\n    print('Run for '+str(x)+' completed with acc='+str(acc))","90740a0a":"plt.rcParams[\"figure.figsize\"] = (6,5)\nplt.rcParams.update({'font.size': 14})\n\nplt.plot(list(test_accuracies_ann['prec'].keys()), list(test_accuracies_ann['prec'].values()), 'g', label='Precision')\nplt.plot(list(test_accuracies_ann['rec'].keys()), list(test_accuracies_ann['rec'].values()), 'b', label='Recall')\nplt.plot(list(test_accuracies_ann['f1'].keys()), list(test_accuracies_ann['f1'].values()), 'c', label='f1')\nplt.plot(list(test_accuracies_ann['acc'].keys()), list(test_accuracies_ann['acc'].values()), 'r', label='Accuracy')\nplt.xlabel('Training set size')\nplt.ylabel('Accuracy')\nplt.grid(color='gray', ls = '-.', lw = 0.3)\nplt.legend()\nplt.show()","e4a9ee36":"exp=collections.defaultdict(dict)\n\nexp['1L 128 10 epochs']['acc']=0.5115755627009646\nexp['1L 128 10 epochs']['pre']=0.7182844243792325\nexp['1L 128 10 epochs']['rec']=0.5115755627009646\nexp['1L 128 10 epochs']['f1']=0.5975586854460094\n\nexp['1L 256 10 epochs']['acc']=0.5366559485530547\nexp['1L 256 10 epochs']['pre']=0.6842968429684296\nexp['1L 256 10 epochs']['rec']=0.5366559485530547\nexp['1L 256 10 epochs']['f1']=0.6015498287979816\n\nexp['1L 2048 10 epochs with scaling']['acc']=0.5466237942122186\nexp['1L 2048 10 epochs with scaling']['pre']=0.6086645184389545\nexp['1L 2048 10 epochs with scaling']['rec']=0.5466237942122186\nexp['1L 2048 10 epochs with scaling']['f1']=0.5759783161104524\n\nexp['1L 128 with scaling 10 epochs']['acc']=0.5482315112540193\nexp['1L 128 with scaling 10 epochs']['pre']=0.6326530612244898\nexp['1L 128 with scaling 10 epochs']['rec']=0.5482315112540193\nexp['1L 128 with scaling 10 epochs']['f1']=0.5874246339362619\n\nexp['3L 1024 + Dropout (0.3) 10 epochs']['acc']=0.5514469453376206\nexp['3L 1024 + Dropout (0.3) 10 epochs']['pre']=0.5833333333333334\nexp['3L 1024 + Dropout (0.3) 10 epochs']['rec']=0.5514469453376206\nexp['3L 1024 + Dropout (0.3) 10 epochs']['f1']=0.5669421487603307\n\nexp['3L 1024 10 epochs']['acc']=0.5520900321543408\nexp['3L 1024 10 epochs']['pre']=0.5828241683638832\nexp['3L 1024 10 epochs']['rec']=0.5520900321543408\nexp['3L 1024 10 epochs']['f1']=0.5670409511228534\n\nexp['2L 512 with scaling 10 epochs']['acc']=0.5581993569131832\nexp['2L 512 with scaling 10 epochs']['pre']=0.6317321688500728\nexp['2L 512 with scaling 10 epochs']['rec']=0.5581993569131832\nexp['2L 512 with scaling 10 epochs']['f1']=0.592693752133834\n\nexp['1L 512 10 epochs']['acc']=0.5601286173633441\nexp['1L 512 10 epochs']['pre']=0.672327286761868\nexp['1L 512 10 epochs']['rec']=0.5601286173633441\nexp['1L 512 10 epochs']['f1']=0.6111208559901772\n\nexp['2L 1024 with scaling 10 epochs']['acc']=0.5662379421221865\nexp['2L 1024 with scaling 10 epochs']['pre']=0.6282554405993578\nexp['2L 1024 with scaling 10 epochs']['rec']=0.5662379421221865\nexp['2L 1024 with scaling 10 epochs']['f1']=0.595636732623034\n\nexp['1L 1024 10 epochs']['acc']=0.5668810289389068\nexp['1L 1024 10 epochs']['pre']=0.6593118922961855\nexp['1L 1024 10 epochs']['rec']=0.5668810289389068\nexp['1L 1024 10 epochs']['f1']=0.609612724757953\n\nexp['1L 2048 10 epochs']['acc']=0.5678456591639871\nexp['1L 2048 10 epochs']['pre']=0.6487876561351947\nexp['1L 2048 10 epochs']['rec']=0.5678456591639871\nexp['1L 2048 10 epochs']['f1']=0.6056241426611798\n\nexp['2L 1024 10 epochs']['acc']=0.5717041800643087\nexp['2L 1024 10 epochs']['pre']=0.6214610276127228\nexp['2L 1024 10 epochs']['rec']=0.5717041800643087\nexp['2L 1024 10 epochs']['f1']=0.5955451348182883\n\nexp['2L 2048 10 epochs']['acc']=0.5771704180064309\nexp['2L 2048 10 epochs']['pre']=0.6241307371349096\nexp['2L 2048 10 epochs']['rec']=0.5771704180064309\nexp['2L 2048 10 epochs']['f1']=0.5997327096558637\n\nexp['2L 4096 10 epochs']['acc']=0.5787781350482315\nexp['2L 4096 10 epochs']['pre']=0.6221914967162115\nexp['2L 4096 10 epochs']['rec']=0.5787781350482315\nexp['2L 4096 10 epochs']['f1']=0.5997001499250374\n\nexp['2L 2048 20 epochs']['acc']=0.5807073954983922\nexp['2L 2048 20 epochs']['pre']=0.6142857142857143\nexp['2L 2048 20 epochs']['rec']=0.5807073954983922\nexp['2L 2048 20 epochs']['f1']=0.5970247933884297","8b2546ed":"vacc=[]\nvpre=[]\nvrec=[]\nvf1=[]\nlabels=list(exp.keys())\nx = np.arange(len(labels))\nwidth = 0.2\n\nfor key, value in exp.items():\n    vacc.append(value['acc'])\n    vpre.append(value['pre'])\n    vrec.append(value['rec'])\n    vf1.append(value['f1'])\n\nplt.rcParams.update({'font.size': 12})\nx = np.arange(len(exp.keys()))\n\n\nplt.figure(1,figsize=(15,6))\n\nplt.bar(x-3*width\/2, vacc, width=width, color='b', label='Accuracy')\nplt.bar(x-width\/2, vpre, width=width, color='y', label='Precision')\nplt.bar(x+width\/2, vrec, width=width, color='r', label='Recall')\nplt.bar(x+3*width\/2, vf1, width=width, color='c', label='f1')\nplt.xticks(np.arange(len(labels)), labels,rotation=90)\n\nplt.grid(color='gray', ls = '-.', lw = 0.2)\nplt.ylabel('Accuracy\/Precision\/Recall\/F1')\nplt.legend()\nplt.show()","93ae0d1f":"labels_pred = annmodel.predict(features_test)\nlabels_pred=np.argmax(labels_pred, axis=1)\n\naccscoretest=accuracy_score(labels_test, labels_pred)\nclasses=list(labelencoder.inverse_transform(range(0,38)))\n\nprint(\"Classification report\")\nprint(classification_report(labels_test,labels_pred, target_names=classes))","4068880f":"plt.rcParams[\"figure.figsize\"] = (15,12)\nplt.rcParams.update({'font.size': 9})\ncf=confusion_matrix(labels_test, labels_pred)\nsn.heatmap(cf, annot=True, cmap='binary', fmt='g',\n           cbar=False,\n           cbar_kws= {'orientation': 'horizontal'},\n           xticklabels=classes, \n           yticklabels=classes)\nplt.show()","a2345078":"def getFittedANNModelMulti(annfttrain, labels_train_cat):\n    X_train, X_test, y_train, y_test = train_test_split(annfttrain, \n                                                        labels_train_cat, \n                                                        test_size=0.25, \n                                                        random_state=42)\n    annmodel = Sequential()\n    annmodel.add(Dense(2048, input_dim=annfttrain.shape[1], activation='relu'))\n    annmodel.add(Dense(2048, activation='relu'))\n    annmodel.add(Dense(38, activation='sigmoid'))\n    annmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    history=annmodel.fit(X_train,\n                         y_train,\n                         validation_data=(X_test,y_test), \n                         epochs=20, batch_size=64, verbose=0)\n    return history, scaler,annmodel","c8f4395c":"%%time\nhistory, scaler, annmodel=getFittedANNModelMulti(features_train, y_trainM)\npredictions, acc, prec, rec, f1=getAcc(annmodel, features_test, y_testM)\nprint('Accuracy: ' + str(acc))\nprint('Precision: ' + str(prec))\nprint('Recall: ' + str(rec))\nprint('F1 score: ' + str(f1))\nprint('Hamming score: '+str(hamming_score(y_testM, predictions)))\nprint('Hamming loss: '+str(hamming_loss(y_testM, predictions)))","2e69f0bf":"%%time\nngram_range = (1,3)\nmin_df = 10\nmax_df = 1.\nmax_features = 4000\n\ntfidffinal = TfidfVectorizer(encoding='utf-8',\n                        ngram_range=ngram_range,\n                        stop_words=None,\n                        lowercase=False,\n                        max_df=max_df,\n                        min_df=min_df,\n                        max_features=max_features,\n                        norm='l2',\n                        sublinear_tf=True)\ntfidffinal.fit(df['Abstract'].values)","bc27511d":"pickle.dump(tfidffinal, open('tfidf_vectorizer.sav','wb'))","f7d69dcb":"features_final = tfidffinal.transform(df['Abstract'].values).toarray()\nlabels_final = df['MainSubject_code'].values\nlabels_final_multi = df.iloc[:,3:41].values","d79d5ca9":"%%time\nfinalmodel=svm.SVC(kernel='rbf',\n                  gamma=0.1,\n                  class_weight=None,\n                  C=10)\nfinalmodel.fit(features_final, labels_final)","f3a135fe":"pickle.dump(finalmodel, open('singlelabel_predictor.sav','wb'))","eb18e6c4":"%%time\nhistory, scaler, finalmodel_ann=getFittedANNModelMulti(features_final, labels_final_multi)","2650297e":"pickle.dump(finalmodel_ann, open('multilabel_predictor.sav','wb'))","189c2ff0":"dfi=pd.read_csv('..\/input\/icist-abstracts-corpus\/abstracts_icist.csv', names=['title','abstract','year'])\ndfi.head()","1e66badc":"%%time\ndfi['abstract']=dfi['abstract'].apply(lambda x: preprocessTraining(x))\ndfi.to_csv('abstracts_icist_processed.csv',index=False)","aa8abbc5":"tfidf_vectorizer=pickle.load(open('tfidf_vectorizer.sav','rb'))\npred_model=pickle.load(open('singlelabel_predictor.sav','rb'))\npred_model_multi=pickle.load(open('multilabel_predictor.sav','rb'))","4a3a5f7e":"labelencoder=pickle.load(open('model_labelencoder.sav','rb'))\nsubjectlist=pickle.load(open('subject_list.sav','rb'))","c37f3e3c":"features_icist = tfidf_vectorizer.transform(dfi['abstract'].values).toarray()\nfeatures_icist.shape","413a57be":"main_labels_icist=pred_model.predict(features_icist)\nmulti_labels_icist=pred_model_multi.predict(features_icist)","2dd9dfe4":"multi_labels_icist[multi_labels_icist>=0.3] = 1\nmulti_labels_icist[multi_labels_icist<0.3] = 0","80a62073":"dfi['mainsubject']=labelencoder.inverse_transform(main_labels_icist)","b4ea3881":"allsubjects=[]\nfor x in multi_labels_icist:\n    row=[]\n    add=''\n    i=0\n    for c in x:\n        if(c==1):\n            row.append(subjectlist[i])\n        i=i+1\n    if row:\n       add=';'.join(row)\n    allsubjects.append(add)\n\ndfi['othersubjects']=allsubjects\ndfi.head(10)","07ba1367":"dfi.to_csv('final_predictions.csv',index=False)","0c38e5b7":"## 2.3 Model training and evaluation","08d282d6":"In our case, MultiOutputClassifier method of sci-kit learn is used. This classifier implements binary relevance method for multi-label classification. It wraps a predictor and basically, implements the strategy of fitting one predictor per label.","aa7af261":"List of all subjects are serialized and stored on disk for later use in decoding the one-hot encoded results of classifiers.","106142ae":"While count vectorization feature engineering approach is considered better for smaller datasets, as expected, TF-IDF vectorization produced the best results in our case, with potential for improvement with additional data collection (at least in case of Logistic Regression method). Word2Vec with averaging vectors has shown the best performance (which is quite surprising when knowing how much information is lost when averaging vector representations of words in documents) for the smaller training sets (up to 4000 rows in this case). That can be explained by the fact that w2v are only models that to some extent capture semantics. However, they plateaued later and did not show the improved accuracies with adding more data. As expected, model with GloVe word embeddings did not produce good results because the model is trained with the general corpus.","64862086":"The performance of conventional ML algorithms is evaluated, by using tf-idf vectorized data.","aed1a556":"Text vectorization transforms the corpus to the dictionary of occuring lemmas or n-grams. Each lemma or n-gram is then considered as a feature where feature values for each of the abstracts correspond to some measure of frequency (count of tf-idf) of occurence of this feature in the specific abstract. Count and tf-idf vectorizer can be set with minimum and maximum document frequencies. Minimum document frequency (min_df) means that some terms will be ignored\/excluded if they have a document frequency (presence in % of documents) lower than a given threshold. For example, if min_df=0.66 is set, a term must appear in 66% of the documents in order to be considered as a part of vocabulary. Sometimes min_df is used to limit the vocabulary size, so it learns only those terms that appear in at least 10%, 20%, etc. of the documents. If maximum document frequency is set, that means that the terms that have a document frequency strictly higher than the given threshold will not be included in the dictionary. This could be used to exclude terms that are too frequent and are unlikely to help predict the label. Finally, vectorizer can reduce dimensionality of input vector by selecting only limited number of features (max_features parameter).","3bf833c1":"In this section, the results of the experiments to evaluate accuracy, precision and recall of models based on ANN are presented. Here, only so-called conventional ANN architectures are used, namely those made of densely-connected layers.","9addd97a":"The example of word embedding for a word 'system' is shown below.","914a66cf":"Pre-processing also includes label encoding of the predicted main categories and one-hot encoding for all assigned categories, text vectorization and separation of train and test data. Stratified train-test split was done, due to highly imbalanced distribution of output classes. In general, stratified split or stratified k-fold validation give more realistic results, but also better ones because of the equal distribution among output classes in two datasets. Two different splits were done, once for single-label (main subject code) and then for multi-label (all subject code associations) classification problems. 25% of all data will be used for test set.","2cc163c6":"So far, the work was related to predicting a main category of the article. Thus, the problem was single-label, multi-class classification. However, in reality, the scientific articles are most often assigned to more than one category, as it is the case with the dataset used. Now, RandomForest and SVM with optimal set of parameters are fitted with multi-label data. This kind of problems are specific because they cannot use accuracy score as a key metrics since it is considered as harsh - the predictions now cannot be only true or false. A prediction containing a subset of the actual classes should be considered better than a prediction that contains none of them, i.e., predicting two of the three labels correctly this is better than predicting no labels at all. In multi-label classification, accuracy score is so-called Exact Match Ratio (subset accuracy) metrics and it indicates the percentage of samples that have all their labels classified correctly. In other words, no partial predictions are taken into account.","8c5766cf":"Such an architecture provides better results than the best model based on shallow, conventional ML algorithms. Although accuracy of ANN model is similar, its hamming score is 3-4% better. This approach is adopted then as a final one, for use in production. ","bb0a5b73":"Can the performance of word embedding be improved? Grid-search optimization was carried-out on feature size, window context and min word count parameters, with dataset of reduced size. The results show that more expressive models, in terms of number of features and window size do not, in general produce better performance, for the fixed size dataset. For the given set of combinations of parameters, the accuracies varied in the range of 4%. Finally, the effect of improvement due to the parameters optimization was more visible for the training sets of the smaller sizes.","86536764":"Before vectorization, the corpus is pre-processed by using Spacy library, with the usual transformation steps, including removing line breaks, punctuations, stop words and by transforming the tokens to their lemmas.","6e7744de":"# <b>Review of supervised ML-based approaches for auto-tagging of computer science scientific literature<\/b>","ad7db383":"## 2.2 Feature engineering","5b93d734":"- Mikolov, T., Chen, K., Corrado, G., Dean, J. (2013) Efficient Estimation of Word Representations in Vector Space\n- Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R.S., Torralba, A., Urtasun, R., Fidler, S. (2015) Skip-Thought Vectors\n- Text classification in Python (with links to GitHub repo) https:\/\/towardsdatascience.com\/text-classification-in-python-dd95d264c802\n- Topic modeling with LSA, PLSA, LDA and lda2Vec https:\/\/medium.com\/nanonets\/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05\n- Pennington, J., Socher, R., Manning, C.D., 2014, GloVe: Global Vectors for Word Representation\n- Bojanowski, P., Grave, E., Joulin, A., Mikolov, T. 2017. Enriching Word Vectors with Subword Information\n- Multi-label classification problems https:\/\/www.analyticsvidhya.com\/blog\/2017\/08\/introduction-to-multi-label-classification\/\n- Harris, Z. (1954). Distributional structure. Word, 10(23): 146-162.","3706f22d":"For pre-processing, Spacy library will be used as it is proven as the fastest on the market. Scikit-learn library features will be used for bag-of-words vectorizations, model preparation, learning and assessment (metrics). Word and doc embedding approaches are implemented by the functions of Gensim library. Keras library will be used for implementing ANN and deep learning architectures. For serializing different objects (such as prediction models), pickle will be used.","f7720466":"Capability of the model to learn from additional data is evaluated. The model with the same architecture was used, with the exception that stratified split for training and validation of ANN was not used, due to small number of instances in the initial sizes of the dataset. The results are shown below. While the model plateaued at 7000 samples, the trend of increasing accuracy was clearly visible when the size od the training set was increased from 9000 to 11660.","dc5dddeb":"Review of the predictions based on the ICIST dataset has shown that SVM predictor was quite succesful in associating main subject code to the abstracts. It was particularly good in classifying articles related to artificial intelligence and machine learning topics. In contrast, the predictor overused information theory category. Multi-label classifier, based on ANN was not that succesful. However, it was very good in classifying the abstracts to Human-computer interaction category. \nIn final conclusion, fully automatic tagging by using supervised learning approaches is not feasible with the available number of samples. Still, it is considered very useful in assisted content management scenarios where the user who submits the abstracts is offered with primary and secondary recommendations from which he\/she can choose the most relevant categories.","15fab447":"Instead of computationally expensive grid search, in first step randomized search is used. Another compromise is using only 3000 of the training set samples. The goal is to identify the region of the best hyper-parameters, in which more specific - grid search can be used to find the best combination. 10 combinations (n_iter parameter of RandomizedSearchCV) will be tested with the 3-fold cross validation (cv=3). Best found configurations are shown below.","1c5c0666":"Corpus that will be used for training a model for topic classification is made by scrapping the arxiv.org website. It stores 16307 scientific abstracts, published in period 2000-2020, belonging to a Computer Science category. All articles in arxiv database are annotated with the original system, which is interoperable with ACM Computing Classification System. Each article is tagged with at least one subject class; one of the tags associated is considered as primary. The objective is to use arxiv database corpus to train a classifier for annotating the abstracts with one or more of the subjects. Obviously, this is a multi-class classification problem, with 38 classes - different subjects from the arxiv classification system.","1143c523":"For the visualization purposes, Principal Component Analysis (PCA) is implemented to reduce dimensionality of vectors to 2. Then, scatter plot was shown to illustrate the vectors for first 20 words in the dictionary.","e66dd759":"Different approaches to pre-processing will not be considered in evaluation. One unique pre-processing pipeline will be implemented, including removal of punctuation, removal of stop-words (including custom stop-words) and lemmatization.","a45cc5cf":"While ANN overfits even at 4th epoch, the experiments have shown that the increased number of training epochs anyway led to the improved accuracy.","dcb98af0":"In evaluating supervised approach, three key aspects will be considered: feature engineering, prediction models and prediction label dimensionality. \n- In feature engineering, the decisions on the choice of text vectorization approach (count vectorizer, has vectorizer, tf-idf vectorizer and word embeddings - word2vec and doc2vec) will be made, based on the model performance. \n- In prediction models, the performance of the different algorithms will be assessed, namely the group of conventional models (Logistic Regression, Support Vector Machine and Random Forest), densely connected Artificial Neural Networks (ANN) and deep learning architectures. For each approach, the experiments on finding the best configuration of hyper-parameters will be carried out.\n- Regarding label dimensionality, two different types of classification problems will be considered: single and multilabel. Single label classification problem is classification of main subject of the abstract. Multilabel classification occurs when the model is trained with all assigned subjects per abstract and consequently, it tries to predict all of them.","48b55cdc":"Then, the Random Forest and SVM based models with optimized hyper parameters are trained with data and compared with those with default ones.","8dac6ba8":"In the first step, output labels are structured and corpus is pre-processed. Basic exploratory data analysis is carried out.","3845e42b":"The attempt to optimize the configuration of two conventional methods, namely Random Forest and SVM will be made. Randomized and grid search methods are used.","e5445a32":"Label encoder object will be pickled, because it needs to be used for inverse operations in text classification step.","525cc13d":"Doc2vec model represents document as a paragraph vector. Below, the doc2vec model is created with the vocabulary from the training corpora and then trained with it. That model is then used to infer the text vectors. Doc2vec model works with one of two possible training algorithms: distributed memory (PV-DM, dm=1), analogous to Word2Vec CBOW or distributed bag of words (PV-DBOW, dm=0), analogous to Word2Vec SG.","811e47da":"The objective of the work was to implement feasible and the most effective, semi-automated approach to tagging the scientific articles, from the Computer Science domain with the arbitrary set of topics\/tags. The articles that need to be tagged are accepted manuscripts for the series of ICIST (International Conference on Information Society and Technologies) conferences. One article will be tagged with one or more topics\/tags, while there will be one primary topic suggesting the main categorization. The approach must take into account that tagging will be carried out based on the contents of the articles' abstracts (full texts not available). The research will take into account both unsupervised and supervised approaches. This report covers the part of the research related to using supervised approaches to text classification.","cc29f7ba":"Initially, SVM model with optimized configuration of hyper-parameters was evaluated. Still, other configurations including the models based on other conventional ML algorithms were tested. The results of those experiments are shown below. Obviously, same as it was the case for single-label classification, optimized SVM produces the best results.","cb679d3a":"Good results (precision and sensitivity) were visible for test samples labeled with Computation and Language (prec=0.71, rec=0.79), Computational Geometry (prec=0.73, rec=0.49), Computer Vision and Pattern Recognition (prec=0.74, rec=0.83), Cryptography and Security (prec=0.72, rec=0.75), Digital Libraries (prec=0.84, rec=0.62), and Information Theory (prec=0.72, rec=0.75).","297fc48e":"The vectorizer fitted with training data is then used to transform input values to term-frequency matrices.","b1edb8f5":"Classification report and confusion matrix for single-label classification problem with ANN and tf-idf vectorized input data features is shown below. In general, model performance to distinguish between classes does not always corresponds to the number of samples associated with particular classes\/labels. Good results (precision and sensitivity) were visible for test samples labeled with Computation and Language (prec=0.73, rec=0.68), Computer Vision and Pattern Recognition (prec=0.77, rec=0.73), Cryptography and Security (prec=0.77, rec=0.68) and Information Theory (prec=0.72, rec=0.75).","aeebd3b9":"## 2.1 Pre-processing","aeece1fe":"With regard to the aspect of dimensionality of predicted labels, it is obvious that two output tensors will be needed. First one is a vector containing all assigned main subjects. The another one is one-hot encoded representation of the output features, a sparse matrix with all subjects assigned to the abstracts. Some labels will be removed from the model: only Computer Science topics from Arxiv classification are taken into account; no tags from other areas are stored in the resulting dataset. Some rows (less than 800) will be deleted because they are not tagged with one of the Arxiv subject codes belonging to Computer Science domain.","0c1b8ef9":"First, tf-idf vectorized dataset was used. In fitting ANN model, in all experiments related to text classification with ANN, again, stratified split was used to define training and validation set.","c27945f1":"Two main architectures were considered. One, with keras embedding layer and the second one, with Convolutional Neural Networks (CNN). The results were not satisfactory; the deep learning architectures are expected to excel with much more data.","d2250ad5":"Embedding layer takes in vectors with indexes of tokenized lemmas. Indexes are assigned in tokenization process, for which keras preprocessing tokenizer class is used. Keras tokenizer class creates lists of indexes corresponding to the tokens in abstracts, with the given size of the vocabulary (in our case 1000) defining the number of the most common words to take into account for feature engineering. \nThis procedure is similar to the one which has been already implemented, where embeddings are trained outside of the ANN and then provided as an input to its densely-connected layers. In that case, word2vec and doc2vec approaches were used to create features. Since embedding layer is quite computationally expensive, modest ANN architecture was used in testing in order to validate the feasibility of approach.","7056a0cf":"The performance of the ANN depends to a great extent from some decisions made in setting up its architecture and behavior. Those decisions are typically related to choosing the values of the hyper-parameters of ANN. Most commonly considered hyper-parameters are: number of hidden layers (including the choice of activation function for each of the layers), number of units in each of the hidden layers, batch size, number of epochs, optimizer, loss function and metrics.","f7ff907b":"#### 2.3.1.2 Optimization of hyper-parameters","22564d6d":"#### 2.3.2.2 Multi-label classification problem","c671ff4b":"#### 2.3.1.3 Multi-label classification","2ce782fe":"In the previous sections, different approaches were evaluated to choose the best performing ones. All those approaches are implemented based on the training dataset, consisting of 75% of data. At this point, final models that take into account all data need to be compiled. Following will be carried out:\n\n- tf-idf vectorizer needs to be fitted with all data. Then, a feature set for training the final models will be generated. \n- For single-label classification or prediction of main category, SVM model with optimized configuration of hyper-parameters needs to be fitted with all data\n- For multi-label classification, conventional ANN needs to be fitted with all data","59132d91":"In a more exhaustive way, the confusion matrix illustrates the performance of the classification model with regard to individual categories. Predictions are represented in vertical columns. Only prediction in the cell that is the column and a row representing a specific category is correct prediction (or true positive in binary classification terms).","4ba3fa05":"#### 2.3.1.1 Single-label classification","057fd6a1":"Below, a dictionary with classification metrics results collected in different experiments with different ANN configurations is displayed. Those experiments were carried out with a train-test split without stratification. Their results are shown however to illustrate the effect of some decisions related to choosing the ANN architecture to the classification metrics.","ed37f5eb":"After the word vectors have been created, features\/vectors for the whole documents need to be built. Those features are then used as an input to a classification model. The most straightforward method to get a document vector from the vectors of its words is to average those.","3f3f6688":"### 2.3.3 Text classification with deep learning architectures","8648b4ca":"Fitted model is then serialized and stored on disk for the use in production.","e3c9c16c":"As mentioned before, three algorithms will be evaluated: Logistic Regression, Support Vector Machine and Random Forest. In the first run, Logistic Regression approach will be used to evaluate performance with different vectorization methods: count, hash, tf-idf and word2vec (with averaging and tf-idf). First, single-label classification of main category is carried out.","9438fa79":"## 3.1 Compilation of final production models","14637c22":"Bag of words cannot capture meaning of the text. Bag of words method assumes that each word (token) is represented by the single scalar, that highlights the frequency of its occurence in the single data item in absolute or relative way (while considering frequency of its occurences in the other data items). While the word is represented by a scalar value, in the dataset it is one hot-encoded in a sparse vector, relative to the whole dictionary. More complex representation of tokens, where individual words or n-grams are represented with vectors is proposed by Mikolov et al (2013). Representation of the word in this case is called word embedding. Those vectors are dense and thus, quite easier to process by the neural networks. The word embedding idea was implemented in the word2vec (and derived doc2vec) algorithm which is today one of the most popular methods for text vectorization. Both word2vec and doc2vec vectorization methods are implemented in gensim package.","443c5c59":"Another method is to use TF-IDF vectorization of word embeddings. TfIdf vectorizer class is adopted from this source http:\/\/nadbordrozd.github.io\/blog\/2016\/05\/20\/text-classification-with-word2vec\/ and revised.","4379f53a":"Three feature tensors for training and test sets will be created, by using three different approaches to vectorization: count, tf-idf and hashing. In case of hashing vectorizer, number of features equals 2**10. Method fit_transform is used to learn term frequencies and inverse document frequencies and generate term-frequency matrix. Former are learnt from the train set. Method transform is used only for generating term-frequency matrix by using previously learnt representation.","076a1569":"Analysis below shows the accuracies of Logistic Regression with models characterized by different number of tf-idf features. Obviously, the effect on the accuracy is not great and best results are achieved already with 4000 features, which is adopted as a reference model size.","48885f43":"Comparison of accuracies, precisions and recalls of the models fitted with default and optimized hyper parameters is then made.","4f7b1563":"For example, a prediction of Information Theory category was accurate in 373 cases. In other cells in one column, numbers of predictions of Information Theory category that were inaccurate (or false positives in binary classification terms) are represented. For example, predictor misplaced articles tagged with Network and Internet Architecture for Information Theory category 24 times. For Information Theory predictions: TP=373, FP=181 (sum of incorrect predictions of Information Theory category). Thus, Prec = TP\/(TP+FP) = 373\/(373+181)=0.67. Recall is percentage of correctly predicted samples. For example, for Information Theory, TP=373, total number of samples tagged with this category is 422 (this number is equal to a sum TP+FN). Hence, rec=373\/422=0.88.","6c940144":"Bag of words is the simplest method for text vectorization. It is made of fixed length vectors one per data item, where each entry (column in a bag of words dataset) stores a measure of word occurence in that item. The fixed length of the vector equals the size of pre-defined dictionary. The measure of the word occurence can be its count, term frequency, hash or tf-idf value. Vectorization is typically done on a pre-processed text (no punctuation, no stop-words, lemmatized) and it can consider words or n-grams. Bag of words vectorization's output is called Document-Term Matrix.","b61b2085":"Word embedding is a learnt (for predefined fixed size vocabulary, from corpus of text) representation from text where words that have same meaning have similar representation. The assumption on the sameness of meaning is derived from the similar use in the text, or according to the so-called distributional hypothesis from the linguistic theory of Harris (Harris, 1954): words that have similar context will have similar meanings. Thus, in word embedding approach, a token is represented with its context, namely a set of offen appearing surrounding tokens. Each word is represented by a real-valued vector, often with tens or hundreds of dimensions, corresponding to the indexes of the words in the neighbourhood of that word. This is much more efficient representation than one-hot encoded sparse ones which size correspond to the size of the dictionary, thus often with millions of dimensions.","76b94007":"## 3.2 Text classification ","592e4c1e":"Again, hamming score is used for assessing the performance of multi-label predictor. Stratified split of train and validation sense in case of multi-label classification problem does not make sense, due to exceptionally large number of possible values of the output features. ","c88fe03a":"Baseline classifier predicts the majority class.","faee5355":"# 5. References","f08c7286":"Artificial Neural Network (ANN) is a model consisting of computational units, organized in layers, capable to transform a set of inputs to a single or multiple outputs, by using parameters (weights) and so-called activation function, and learning a set of optimal parameters based on defined criteria - loss as a measure of accuracy of a produced output compared to desired one and optimizer function. As complexity of the network rises, namely number of sequential layers is being increased, the phenomenon of so-called vanishing gradient occurs - at some moment of increasing complexity, the network becomes untrainable.","cf008b15":"# 4. Conclusions","7307f205":"Serializations of classes required for prediction are then used to instantiate tf-idf vectorizer, single-label (SVM) and multi-label prediction (ANN) models. Also, serialized label encoder is open, as well as list of subjects, needed for decoding one-hot encoded representation of multi-label classifier predictions.","2f4fc462":"### 2.2.1 Text vectorization with bag of words approach","cc589aa7":"Two commonly used approaches to text vectorization are bag of words and word embedding.","45102637":"Tf-idf vectorizer is created with the parameters used during the evaluation procedure. The vectorizer is fitted with all data - all abstracts from the arxiv corpus. After fitting the vectorizer, it is serialized and stored on disk for later use in production. ","c5e30d77":"The loss is the quantity that is being minimized during the training, it is considered as the feedback signal to be used for learning. It represents a measure of success for the task ANN is trying to solve. Thus, it is tightly related to the problem. For multi-class classification, if outputs in multi-class classification are one-hot encoded, then categorical crossentropy loss function is used. If they are label encoded (integers), then sparse categorical crossentropy loss function should be used.","a5a99e48":"Vast majority of traditional ML algorithms are developed for single-label classification or regression problems (on contrary, ANN are by default working with multi-outputs, multi-label problems). For that reason, the approaches to implement those algorithms for that kind of problems are typically related to transforming multi-label to single-label problems, so that existing methods can be implemented.","db8f5545":"Finally, the predictions of multi label model classifier are decoded and stored as comma separated string in the othersubjects column of the final set.","fd3b6d8e":"### 2.3.1 Text classification with conventional ML methods ","4c1fd2cf":"Analysis of the performance of different methods show that both Logistic Regression and SVM achieve much better results than Random Forest with default set of hyper-parameters. This is due to large overfitting of RF which is notable when comparing the accuracy of models with training data (for RF, its 1.0 with all different sizes of training datasets). Another important finding is that the accuracy of both LR and SVM did not plateaued, meaning that the models can be improved with additional data collection.","1123f295":"Finally, the best performing approaches are used to compile the final prediction models. Then, those are used to auto-tag the ICIST dataset with categories from arxiv classification scheme.","aeaa8a5d":"The results of deep learning architectures with embeddings can be improved by downsampling the incoming feature vectors with a MaxPooling1D\/AveragePooling1D or a GlobalMaxPooling1D\/GlobalAveragePooling1D layer after the embedding. Global max\/average pooling takes the maximum\/average of all features whereas in case of MaxPooling\/AveragePooling the pool size need to be explicitly defined. Keras library has its own layer that can be added in the sequential model. Because of introducing the downsampling layer, ANN is trained with more epochs than in previous case. Still, results of the experiments were not satisfactory.","14ad5c56":"Word2vec model is then learnt from the pre-processed list of token lists. Each word is represented with the dense vector of size 100. The learning takes into account that the context window size is 30, meaning that each single word context is represented by previous and next 30 words in its surrounding. 50 training iterations are set. Every word in the corpus with at least a single occurence will be represented, as defined by min_count parameter. The used training algorithm is CBOW (Continuous Bag Of Words, sg parameter=0), not skip gram (sg=1). 5 CPU cores were used in training.","25aebbf5":"Vectors with input and output features of the term-frequency matrix are created and SVM model with optimized configuration of hyper-parameters is fitted with those.","a53b1f58":"Pre-processing of the abstracts is carried out, namely, removing of punctuation and stop words and lemmatization. Pre-processed dataset is serialized and stored on disk ","bf61b9fe":"Then, prediction models are used to predict main arxiv categories for each of the data instances and then, all relevant categories.","1817f3e0":"Exploratory Data Analysis includes the analysis of predicted classes distribution and highlighting baseline accuracy.","9341da31":"ICIST dataset is collection of titles and abstracts of scientific papers in the general domain of computer science, accepted for presentation on the International Conference for Information Society and Technologies (ICIST) in period of 2011-2019 (2012 missing). Dataset stores id of papers (in the eventiotic system for publishing online proceedings), titles, abstracts and years of publishing.","08f699ca":"# 1. Introduction","8953c6b5":"### 2.3.2 Text classification with artificial neural network","8fab3c28":"While accuracy is used for comparing models, full metrics (for Logistic Regression) is displayed below, including the confusion matrix. Beyond cumulative accuracy metrics, f1-score values and confusion matrix clearly highlight the capability of LR prediction model to distinguish between classes. Accuracy is a cumulative metrics which do not explicitly correspond to the capability of the prediction model. It is defined as acc=Total number of accurate predictions of all classes\/Total number of predictions of all classes. In other words, it does not accurately reflects the capability of the model to distinguish between classes. For that, precision and recall may be use, or f1-score as combined metrics.","a2c4029b":"Activation functions introduce non-linear correlation between inputs and outputs. If not, output signal would be a linear function (first-order polynomial) of the input signals - ANN without activation function would be a linear regression model. For multi-class classification problems, softmax activation function is used. It facilitates outputing probability distributions over n output classes.","c398ede1":"There are some other approaches that have not been tested in this paper. Instead of training own word embedding vectorizer, one option is to use pre-trained one: GloVe (Global Vectors for Word Representation) word vectors proposed by Pennington et al (2014). Another proposal by the authors of Word2Vec is FastText (Bojanowski et al, 2017). It captures the structure of the words themselves and thus semantics of the elements of their morphologies. Approach for sentence vectorization, namely skip-thought vectors encoder is proposed by Kiros et al (2015). The encoder reconstructs the surrounding sentences of an encoded passage. The method includes simple vocabulary expansion approach that facilitates efficient models development for new problems by using transfer learning paradigm.","01235fd2":"Now, ANN is fitted with multi-label data. ANN inherently support multi-label data. However, there is one important difference which requires different approach. In case of single-label classification problem, output features are label encoded, and thus, this is typical multi-class classification. In the case of multi-label classification, the output feature is one-hot encoded sparse matrix with 38 columns (corresponding to the number of classes). There, instead of categorical_crossentropy loss function which is used for multi-class classification, in this case, binary_crossentropy is used. Also, instead of softmax, sigmoid activation function is used in output layer. The reason is that in case of softmax, probability of one class is not independent from the other classes probabilities.","5b6dcdb9":"Then, a word embedding model is prepared by using word2vec method. First, data is pre-processed for learning word embeddings. The required format is a list of lists where latter are made of tokens of the individual abstracts.","46cc5b03":"Precision is percentage of good predictions. For individual class i, prec<sub>i<\/sub>=TP<sub>i<\/sub>\/TP<sub>i<\/sub>+FP<sub>i<\/sub>. Namely, it answers how many predictions of class i predictor made were accurate. Recall is sensitivity of the model and percentage of all instances that were correctly predicted. For individual class i, it is: rec<sub>i<\/sub>=TP<sub>i<\/sub>\/TP<sub>i<\/sub>+TN<sub>i<\/sub>.","2f27a72c":"For SVM, penalty parameter C, kernel coefficient (gamma) and class weight will be optimized.","a2697cf4":"Multiple ANN configurations have been tested and the best results were achieved with two hidden layers (each with 2048 neurons), 20 epochs and batch size of 64.","7b46d227":"### 2.2.2 Text vectorization with word embedding approach","d5b5242d":"# 3. Text classification","d41c5613":"Besides feature engineering and algorithm aspects, the performance of the approaches will be also compared by taking into account different sizes of the training set, in order to evaluate the opportunities for creating a more accurate model with additional data collection.","1092b7a2":"- The problem is very difficult to solve because of combined effects of very high dimensionality of the output vectors (38 classes), imbalanced distribution of output classes and multi-labeled output features.\n- The general problem with supervised ML-based text classification is that it relies on existing dataset. In other words, the approach is restricted to using the existing labeling, in this case arxiv classification scheme. Word2Vec capability to measure semantic similarity of the documents (abstracts vs. items in arbitrary classification scheme) could be applied as a solution for this scheme lock-in.\n- The trends of increasing accuracy with increasing number of samples show that both for conventional ML algorithms (especially the case of SVM), densely-connected ANN and deep learning architectures, collecting more data would improve the accuracy. However, in general, model performance to distinguish between classes (precision and recall) did not always correspond to the number of samples associated with particular classes\/labels.\n- Regarding text vectorization, tf-idf approach produces the best results. However, on the smaller data sets, despite the fact that the vectors are less expressive (measured in size) word embedding methods are superior. This is the expected behavior because of partially captured semantics of the documents. In the cases with the domain-specific corpuses such as arxiv repository, pre-trained word embeddings (GloVe) do not produce good results. Some improvements may be possible with re-training the GloVe word embedding. This experiment is not implemented.\n- In overall, conventional SVM algorithm in combination with tf-idf vectorization produces the best accuracy, even though the differences with the performance of Logistic Regression are minor. Densely connected ANN model and DNN were worse than conventional ML approaches. Some minor improvements with randomized search optimization of hyper-parameters were achieved (1-2% of accuracy), however at the cost of degradation in model's capability to distinguish between classes. Namely, precision and recall of the models with default parameters was better than the ones with optimized configurations. The scale of those improvements is insufficient to justify computationally expensive grid search fine tuning.\n- Precision of the ANN model was better than recall for as high as 10%. In other words, model is more confident in guessing the individual classes than it is sensitive to the differences in the input data corresponding to the same class.\n- In case of multi-label classification prediction, ANN performed significantly better than the models based on conventional ML algorithms, even with optimized configuration of hyper-parameters.\n- Hyper-parameter tuning for ANN model did not improve the classification performance significantly.","cd424dec":"Hamming-Loss metrics will be used for multilabel classification. It is the fraction of the wrong labels to the total number of labels. Hamming loss takes into account the prediction error (an incorrect label is predicted) and the missing error (relevant label not predicted), normalized over total number of classes and total number of examples.","cd87c0ab":"Distribution of the classification categories is highly imbalanced. 24 of 38 categories are represented by more than 200 data rows. The most represented categories are Information Theory, Machine Learning, Computer Vision and Pattern Recognition and Artificial Intelligence. There exist different approaches to address the problem of imbalanced output class representation, including but not restricted to additional data collection or undersampling - balancing the distribution by removing the data with the top represented categories. Given the size of the corpus, the latter was not acceptable. Additional data collection is feasible and it will be considered in the future. Another technique is up-sampling of minority class, the process of randomly duplicating observations from the minority classes in order to reinforce their signals. Similarly, down-sampling of majority classes refers to removing observations with the majority classes. Other approaches are: penalizing algorithms (cost-sensitive training) and use of tree-based algorithms, which are insensitive to imbalanced distribution of output features.\n\nFor now, initial approach to deal with these issues will be the use of stratified split of data to train and test set.  ","a3a91189":"# 2. Implementation","ae3fe38c":"Corpus is stored in a csv file. It is stored in a dataframe with two columns. First one stores the abstract and the second one - list of assigned subjects separated with semicolon. First assigned subject is considered as a main one.","d0bffaca":"The simplest way to vectorize text is to use count vectorizer. It transforms the dataset into a sparse matrix with number of columns equal to the size of used vocabulary. The value of each column for each row corresponds to a number of occurences of the referenced token in the text. The problem with count vectorization is that there are many common words that will often appear in all documents and thus, be marked as important. In order to address that problem count or Term Frequency (TF) in a document is combined with Inverse Document Frequency (IDF) measure that penalizes terms that appear a lot across documents. Both count and td-idf vectorization produces very large number of features (corresponding to the size of the dictionary) and this could impose large requirements on memory use during training and thus significantly slow down the process. Thus, for some basic classification problems and in cases where the technical performance may become an issue, documents are simply converted to arbitrary number of hashes (one-way process). For that, hashing vectorizer is used.","98679a1e":"New column with single-label text classification predictions is created. This column stores predicted main subjects of the ICIST papers.","931e5706":"For multi-label text classification, best results were achieved with ANN with two hidden layer with 2048 units in each of them. This configuration is now trained with complete dataset. Fitted model is then serialized and stored on disk for use in production.","56eb9d2d":"Model with the same architecture was trained with Word2Vec and Doc2Vec data. Similar to conventional ML methods, the results were not as good as with tf-idf approach. Use of Word2vec data resulted with accuracy around 0.55, while Doc2Vec underperformed, with accuracy of 0.48. The difference were on a similar scale also in comparing precision and recall metrics.","3572ac13":"#### 2.3.2.1 Single-label classification problem","d48e86a7":"The simplest approach is training an ensemble of classifiers where each of those predicts a single label (membership or non-membership of one class). This method is called binary relevance. Obviously, this method does not consider correlations\/dependencies between labels themselves. Classifier chains method, partially addresses that issue by chaining the n classifiers (where n equals number of classes) where classifier C<sub>i<\/sub> also considers predicted values of all classifiers 0,i-1 as input features. The address is partial because chains are linear structures in which given or chosen order already poses significant assumption on the dependency between labels. Label powerset method takes all possible correlations between output classes into account. Thus, the method requres 2<sup>n<\/sup> classifiers (n - number of classes) and hence, it is very computational complex. In our case, with 38 classes, this method is obviously not feasible.","7d606706":"For random forest algorithm, the hyper-parameters with the most expected impact to the performance are: number of trees in the forest (n_estimators) and maximum number of features considered for splitting a node (max_features). Others which tuning could produce some improvements are: maximum number of levels in each decision tree (max_depth), minimum number of data points placed in a node before the node is split (min_samples_split), minimum number of data points allowed in a leaf node (min_samples_leaf) and method for sampling data points (bootstrap)."}}