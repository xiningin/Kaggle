{"cell_type":{"7ef35e51":"code","7b8f8f69":"code","27843f74":"code","ca8795ba":"code","8417d482":"code","5da1609b":"code","bdce3944":"code","213f031f":"code","5ef2bf71":"code","10e68f5c":"code","e48a6933":"code","c87aeea4":"code","b8c4fc46":"code","056b5e58":"code","a46cc263":"code","a21a114b":"code","d1739700":"code","f31fab3d":"code","8a03753c":"code","eb089ce0":"code","a49dc08e":"code","e47d0e7f":"code","2e545fa1":"code","7f559177":"code","19904056":"code","a76053dd":"code","d9348e2f":"code","5ab0ffe8":"code","836828f4":"code","48bb640c":"code","af7fe02b":"code","88254b2d":"code","d2467e38":"code","33202d64":"code","f1a6b100":"code","ef1e8154":"code","041ac8a4":"code","f379632e":"code","8c81b70a":"code","f533562d":"code","40d457c6":"code","e717c922":"code","92824b64":"code","836da402":"code","e405bbcc":"code","404b7e57":"code","8eb081cf":"code","77959742":"code","c0dd7070":"code","4fc2eb40":"code","c398731e":"code","ff6a7852":"code","60845dc8":"code","796194ac":"code","8b40d576":"code","d97405f8":"code","f97bf0e3":"code","a331091c":"code","72d9dd0d":"code","4a1ffd5e":"code","151ee652":"code","fa7cc550":"code","5359614e":"code","7c9104ec":"code","d094a9f2":"code","20c01e50":"code","a6dcfad9":"code","f441414d":"code","8dddf0a6":"code","25ddda2f":"code","89196c14":"code","327b504d":"code","6029acde":"code","1a62356b":"code","1419929f":"code","786c1294":"code","c7e3166e":"code","5eeefe6b":"code","6ea6b61d":"code","725c726e":"code","3edef753":"code","fd5cbc42":"code","942b0b2a":"code","6f025238":"markdown","71f59356":"markdown","f8ef3fc6":"markdown","d06b397b":"markdown","fa516677":"markdown","22b03a8f":"markdown","9359fd63":"markdown","bc904544":"markdown","4cb4cef9":"markdown","74f2e2a8":"markdown","ed1a2c7b":"markdown","39e525ab":"markdown","3ee5e395":"markdown","a792c02f":"markdown","8e234d4b":"markdown","3e69a25b":"markdown","35bdf53a":"markdown","4a28d43d":"markdown","f10de4bb":"markdown","713d5539":"markdown","d51d6323":"markdown","d3928c19":"markdown","7b8af408":"markdown","7f20218c":"markdown","dbabe185":"markdown","e4488dd4":"markdown","516dd53d":"markdown","b979ff5b":"markdown","1f5acb13":"markdown","2d956d6a":"markdown","23ee9235":"markdown","b2332af3":"markdown","db87e701":"markdown","ab4edca6":"markdown","7e6f21fe":"markdown","3a4f0c8b":"markdown","5b80c156":"markdown","b7a1f3d5":"markdown","f69c7c1c":"markdown","72aeb2ad":"markdown","f1fbeff1":"markdown","9a4ff589":"markdown","a3210da8":"markdown","f9972a94":"markdown","a4263f42":"markdown","b625f210":"markdown","b5938a3e":"markdown","e0b4fb26":"markdown","c6792f6a":"markdown","95d0f277":"markdown","d37f1a98":"markdown","1617f9e0":"markdown","1b2f68a9":"markdown","c014c850":"markdown","5f4124aa":"markdown","a358ccb7":"markdown","65f09c51":"markdown","c7be5b9b":"markdown","bb31c9d6":"markdown","588ca43e":"markdown"},"source":{"7ef35e51":"# pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('fivethirtyeight')","7b8f8f69":"# Read in bureau\nbureau = pd.read_csv('..\/input\/bureau.csv')\nbureau.head()","27843f74":"# groupby the client id (SK_ID_CURR), count the # of previous loans, and rename the col.\n# every client has a # of prev. loans and we will count it and add them on main app DF\nprevious_loans_count = bureau.groupby('SK_ID_CURR', as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU' : 'previous_loan_counts'})\nprevious_loans_count.head()","ca8795ba":"# join to the main df \ntrain = pd.read_csv('..\/input\/application_train.csv')\ntrain = train.merge(previous_loans_count, on = 'SK_ID_CURR', how='left')\n\n# fill the missing values with 0\ntrain['previous_loan_counts'] = train['previous_loan_counts'].fillna(0)\ntrain.head()","8417d482":"#  Plots the disribution of a variable colored by value of the target\ndef kde_target(var_name, df):\n    \n    # calc the correlation coeff. between the new var and the target\n    corr = df['TARGET'].corr(df[var_name])\n    \n    # calc the median for repaid vs not repaid\n    avg_repaid = df.ix[df['TARGET'] == 0, var_name].median()\n    avg_not_repaid = df.ix[df['TARGET'] == 1, var_name].median()\n    \n    plt.figure(figsize = (12, 6))\n    \n    # plot the dist for target == 0 and 1\n    sns.kdeplot(df.ix[df['TARGET'] == 0 ,var_name], label = 'TARGET == 0')\n    sns.kdeplot(df.ix[df['TARGET'] == 1 ,var_name], label = 'TARGET == 1')\n    \n    # label the plot\n    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n    plt.legend();\n    \n    # print corr\n    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n    # Print out average values\n    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid)\n    print('Median value for loan that was repaid =     %0.4f' % avg_repaid)","5da1609b":"kde_target('EXT_SOURCE_3', train)","bdce3944":"kde_target('previous_loan_counts', train)","213f031f":"# Group by the client id, calculate aggregation statistics\nbureau_agg = bureau.drop(columns=['SK_ID_BUREAU']).groupby('SK_ID_CURR', as_index=False).agg(['count', 'mean', 'max','min', 'sum']).reset_index()\nbureau_agg.head()","5ef2bf71":"# list col names\ncolumns = ['SK_ID_CURR']\n\n# iterate through the variable names\nfor var in bureau_agg.columns.levels[0]:\n    # skip the id name\n    if var != 'SK_ID_CURR':\n        \n        # iter through the stat names\n        for stat in bureau_agg.columns.levels[1][:-1]:\n            columns.append('bureau_%s_%s' %(var, stat))","10e68f5c":"# Assign the list of columns names as the dataframe column names\nbureau_agg.columns = columns\nbureau_agg.head()","e48a6933":"# Merge with the training data\ntrain = train.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\ntrain.head()","c87aeea4":"# List of new correlations\nnew_corrs = []\n\n# Iterate through the columns \nfor col in columns:\n    # Calculate correlation with the target\n    corr = train['TARGET'].corr(train[col])\n    \n    # Append the list as a tuple\n\n    new_corrs.append((col, corr))","b8c4fc46":"#  Make sure to reverse to put the largest values at the front of list\nnew_corrs = sorted(new_corrs, key = lambda x:abs(x[1]), reverse = True)\nnew_corrs[:15]","056b5e58":"kde_target('bureau_DAYS_CREDIT_mean', train)","a46cc263":"def agg_numeric(df, group_var, df_name):\n    \"\"\" Aggregation the numeric values in a dataframe. thi can be\n    used to create features for each instance of the grouping variable.\n    \n    Parameters\n    ---------\n        df (dataframe):\n            the dataframe to calculate the statistics on\n        group_var (string):\n            the variable by which to griup df\n        df_name (string):\n            the variable used to rename the col\n            \n        return\n        ----------\n            agg (dataframe):\n                a dataframe with the statistics aggregated for \n                all numeric columns. Each instance of the grouping variable will have \n                the statistics (mean, min, max, sum; currently supported) calculated. \n                The columns are also renamed to keep track of features created.\n    \"\"\"\n    \n    # remove id variable other than grouping variable\n    for col in df:\n        if col != group_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    group_ids = df[group_var]\n    numeric_df = df.select_dtypes('number')\n    numeric_df[group_var] = group_ids\n    \n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n    \n    # Need to create new column names\n    columns = [group_var]\n    \n    # iter through var names\n    for var in agg.columns.levels[0]:\n        # Skip the grouping variable\n        if var != group_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1][:-1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n\n    agg.columns = columns\n    return agg","a21a114b":"bureau_agg_new = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_agg_new.head()","d1739700":"bureau_agg.head()","f31fab3d":"# Function to calculate correlations with the target for a dataframe\ndef target_corrs(df):\n    \n    # list of corr\n    corrs = []\n    \n    # Iterate through the columns \n    for col in df.columns:\n        print(col)\n        # skip the target col\n        if col != 'TARGET' :\n            corr = df['TARGET'].corr(df[col])\n            \n            # Append the list as a tuple\n            corrs.append((col, corr))\n            \n    # Sort by absolute magnitude of correlations\n    corrs = sorted(corrs, key = lambda x: abs(x[1]), reverse = True)\n    \n    return corrs","8a03753c":"categorical = pd.get_dummies(bureau.select_dtypes('object'))\ncategorical['SK_ID_CURR'] = bureau['SK_ID_CURR']\ncategorical.head()","eb089ce0":"categorical_grouped = categorical.groupby('SK_ID_CURR').agg(['sum', 'mean'])\ncategorical_grouped.head()","a49dc08e":"categorical_grouped.columns.levels[0][:10]","e47d0e7f":"categorical_grouped.columns.levels[1]","2e545fa1":"group_var = 'SK_ID_CURR'\n\n# Need to create new column names\ncolumns = []\n\n# Iterate through the variables names\nfor var in categorical_grouped.columns.levels[0]:\n    # Skip the grouping variable\n    if var != group_var:\n        # Iterate through the stat names\n        for stat in ['count', 'count_norm']:\n            # Make a new column name for the variable and stat\n            columns.append('%s_%s' % (var, stat))\n\n#  Rename the columns\ncategorical_grouped.columns = columns\n\ncategorical_grouped.head()","7f559177":"train = train.merge(categorical_grouped, left_on = 'SK_ID_CURR', right_index = True, how = 'left')\ntrain.head()","19904056":"train.shape","a76053dd":"train.iloc[:10, 123:]","d9348e2f":"def count_categorical(df, group_var, df_name):\n    \"\"\"Computes counts and normalized counts for each observation\n    of `group_var` of each unique category in every categorical variable\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    group_var : string\n        The variable by which to group the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with counts and normalized counts of each unique category in every categorical variable\n        with one row for every unique value of the `group_var`.\n        \n    \"\"\"\n    \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('object'))\n\n    # Make sure to put the identifying id on the column\n    categorical[group_var] = df[group_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['count', 'count_norm']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    return categorical","5ab0ffe8":"bureau_counts = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_counts.head()","836828f4":"bureau_counts.index","48bb640c":"# Read in bureau balance\nbureau_balance = pd.read_csv('..\/input\/bureau_balance.csv')\nbureau_balance.head()","af7fe02b":"# Counts of each type of status for each previous loan\nbureau_balance_counts = count_categorical(bureau_balance, group_var='SK_ID_BUREAU', df_name='bureau_balance')\nbureau_balance_counts.head()","88254b2d":"# Calculate value count statistics for each `SK_ID_CURR` \nbureau_balance_agg = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_agg.head()","d2467e38":"# Dataframe grouped by the loan\nbureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n\n# Merge to include the SK_ID_CURR\nbureau_by_loan = bureau_by_loan.merge(bureau[['SK_ID_BUREAU', 'SK_ID_CURR']], on = 'SK_ID_BUREAU', how = 'left')\n\nbureau_by_loan.head()","33202d64":"bureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')\nbureau_balance_by_client.head()","f1a6b100":"# Free up memory by deleting old objects\nimport gc\ngc.enable()\ndel  bureau_agg_new, bureau_balance_agg, bureau_balance_counts, bureau_by_loan, bureau_balance_by_client, bureau_counts\ngc.collect()","ef1e8154":"# Read in new copies of all the dataframes\ntrain = pd.read_csv('..\/input\/application_train.csv')\nbureau = pd.read_csv('..\/input\/bureau.csv')\nbureau_balance = pd.read_csv('..\/input\/bureau_balance.csv')","041ac8a4":"bureau_counts = count_categorical(bureau, group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_counts.head()","f379632e":"# bureau_count","8c81b70a":"bureau_agg = agg_numeric(bureau.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'bureau')\nbureau_agg.head()","f533562d":"bureau_balance_counts = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_counts.head()","40d457c6":"bureau_balance_agg = agg_numeric(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\nbureau_balance_agg.head()","e717c922":"# Dataframe grouped by the loan\nbureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\n\n# Merge to include the SK_ID_CURR\nbureau_by_loan = bureau[['SK_ID_BUREAU', 'SK_ID_CURR']].merge(bureau_by_loan, on = 'SK_ID_BUREAU', how = 'left')\n\n# Aggregate the stats for each client\nbureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns = ['SK_ID_BUREAU']), group_var = 'SK_ID_CURR', df_name = 'client')","92824b64":"original_features = list(train.columns)\nprint('original # of Features: ', len(original_features))","836da402":"# Merge with the value counts of bureau\ntrain = train.merge(bureau_counts, on = 'SK_ID_CURR' ,how = 'left')\n\n# Merge with the stats of bureau\ntrain = train.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the monthly information grouped by client\ntrain = train.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","e405bbcc":"new_features = list(train.columns)\nprint('Number of features using previous loans from other institutions data: ', len(new_features))","404b7e57":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","8eb081cf":"missing_train = missing_values_table(train)\nmissing_train.head(10)","77959742":"missing_train_vars = list(missing_train.index[missing_train['% of Total Values'] > 90])\nlen(missing_train_vars)","c0dd7070":"# Read in the test dataframe\ntest = pd.read_csv('..\/input\/application_test.csv')\n\n# Merge with the value counts of bureau\ntest = test.merge(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the stats of bureau\ntest = test.merge(bureau_agg, on = 'SK_ID_CURR', how = 'left')\n\n# Merge with the value counts of bureau balance\ntest = test.merge(bureau_balance_by_client, on = 'SK_ID_CURR', how = 'left')","4fc2eb40":"print('Shape of Testing Data: ', test.shape)","c398731e":"train_labels = train['TARGET']\n\n# Align the dataframes, this will remove the 'TARGET' column\ntrain, test = train.align(test, join = 'inner', axis = 1)\n\ntrain['TARGET'] = train_labels","ff6a7852":"print('Training Data Shape: ', train.shape)\nprint('Testing Data Shape: ', test.shape)","60845dc8":"missing_test = missing_values_table(test)\nmissing_test.head(10)","796194ac":"missing_test_vars = list(missing_test.index[missing_test['% of Total Values'] > 90])\nlen(missing_test_vars)","8b40d576":"missing_columns = list(set(missing_test_vars + missing_train_vars))\nprint('There are %d columns with more than 65%% missing in either the training or testing data.' % len(missing_columns))","d97405f8":"# Drop the missing columns\ntrain = train.drop(columns = missing_columns)\ntest = test.drop(columns = missing_columns)","f97bf0e3":"train.to_csv('train_bureau_raw.csv', index = False)\ntest.to_csv('test_bureau_raw.csv', index = False)","a331091c":"# Calculate all correlations in dataframe\ncorrs = train.corr()","72d9dd0d":"corrs = corrs.sort_values('TARGET', ascending = False)\n\n# Ten most positive correlations\npd.DataFrame(corrs['TARGET'].head(10))","4a1ffd5e":"# Ten most negative correlations\npd.DataFrame(corrs['TARGET'].dropna().tail(10))","151ee652":"kde_target(var_name='client_bureau_balance_counts_mean', df=train)","fa7cc550":"kde_target(var_name='bureau_CREDIT_ACTIVE_Active_count_norm', df=train)","5359614e":"# Set the threshold\nthreshold = 0.8\n\n# Empty dictionary to hold correlated variables\nabove_threshold_vars = {}\n\n# For each column, record the variables that are above the threshold\nfor col in corrs:\n    above_threshold_vars[col] = list(corrs.index[corrs[col] > threshold])","7c9104ec":"# Track columns to remove and columns already examined\ncols_to_remove = []\ncols_seen = []\ncols_to_remove_pair = []\n\n# Iterate through columns and correlated columns\nfor key, value in above_threshold_vars.items():\n    # Keep track of columns already examined\n    cols_seen.append(key)\n    for x in value:\n        if x == key:\n            next\n        else:\n            # Only want to remove one in a pair\n            if x not in cols_seen:\n                cols_to_remove.append(x)\n                cols_to_remove_pair.append(key)\n            \ncols_to_remove = list(set(cols_to_remove))\nprint('Number of columns to remove: ', len(cols_to_remove))","d094a9f2":"train_corrs_removed = train.drop(columns = cols_to_remove)\ntest_corrs_removed = test.drop(columns = cols_to_remove)\n\nprint('Training Corrs Removed Shape: ', train_corrs_removed.shape)\nprint('Testing Corrs Removed Shape: ', test_corrs_removed.shape)","20c01e50":"train_corrs_removed.to_csv('train_bureau_corrs_removed.csv', index = False)\ntest_corrs_removed.to_csv('test_bureau_corrs_removed.csv', index = False)","a6dcfad9":"import lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\nimport gc\n\nimport matplotlib.pyplot as plt","f441414d":"def model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ \/ k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] \/ k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","8dddf0a6":"def plot_feature_importance(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    features importance proi=vided that higher importance is better\n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n    \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalized the feeature importance to add up to one\n    df['importance_Normalizes'] = df['importance'] \/ df['importance'].sum()\n    \n    # Make a horizontal bar chart of feature importance\n    plt.figure(figsize=(10, 6))\n    ax = plt.subplot()\n    \n    # need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))),\n           df['importance_Normalizes'].head(15),\n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature importances')\n    plt.show()\n    \n    return df","25ddda2f":"train_control = pd.read_csv('..\/input\/application_train.csv')\ntest_control = pd.read_csv('..\/input\/application_test.csv')","89196c14":"submission, fi, metrics = model(train_control, test_control)","327b504d":"metrics","6029acde":"fi_sorted = plot_feature_importance(fi)","1a62356b":"submission.to_csv('control.csv', index = False)","1419929f":"submission_raw, fi_raw, metrics_raw = model(train, test)","786c1294":"metrics_raw","c7e3166e":"fi_raw_sorted = plot_feature_importances(fi_raw)","5eeefe6b":"top_100 = list(fi_raw_sorted['feature'])[:100]\nnew_features = [x for x in top_100 if x not in list(fi['feature'])]\n\nprint('%% of Top 100 Features created from the bureau data = %d.00' % len(new_features))","6ea6b61d":"submission_raw.to_csv('test_one.csv', index = False)","725c726e":"submission_corrs, fi_corrs, metrics_corr = model(train_corrs_removed, test_corrs_removed)","3edef753":"metrics_corr","fd5cbc42":"fi_corrs_sorted = plot_feature_importances(fi_corrs)","942b0b2a":"submission_corrs.to_csv('test_two.csv', index = False)","6f025238":"sort the correlations by the magnitude (absolute value)","71f59356":"Now for the new variable we just made, the number of previous loans at other institutions.","f8ef3fc6":"__Test one scores 0.759 ","d06b397b":"### Collinear Variables\n\nWe can calculate not only the correlations of the variables with the target, but also the correlation of each variable with every other variable. This will allow us to see if there are highly collinear variables that should perhaps be removed from the data. \n\nLet's look for any variables that have a greather than 0.8 correlation with other variables.","fa516677":"Examining the feature improtances, it looks as if a few of the feature we constructed are among the most important. Let's find the percentage of the top 100 most important features that we made in this notebook. However, rather than just compare to the original features, we need to compare to the _one-hot encoded_ original features. These are already recorded for us in `fi` (from the original data). ","22b03a8f":"#### we can test this function using the EXT_SOURCE_3","9359fd63":"#### Assessing Usefulness of New Variable with r value","bc904544":"Feature selection is the process of removing variables to help our model to learn and generalize better to the testing set. The objective is to remove useless\/redundant variables while preserving those that are useful. There are a number of tools we can use for this process, but in this notebook we will stick to removing columns with a high percentage of missing values and variables that have a high correlation with one another. Later we can look at using the feature importances returned from models such as the Gradient Boosting Machine or Random Forest to perform feature selection.\n","4cb4cef9":"#### Counts of a client's previous loans","74f2e2a8":"We can remove these columns from both the training and the testing datasets. We will have to compare performance after removing these variables with performance keeping these variables (the raw csv files we saved earlier).","ed1a2c7b":"### Test Two\n\nThat was easy, so let's do another run! Same as before but with the highly collinear variables removed. ","39e525ab":"### Test One\n\nLet's conduct the first test. We will just need to pass in the data to the function, which does most of the work for us.","3ee5e395":"### Feature Selection","a792c02f":"Now we simply merge with the training data","8e234d4b":"This variable represents the average number of monthly records per loan for each client. For example, if a client had three previous loans with 3, 4, and 5 records in the monthly data, the value of this variable for them would be 4. Based on the distribution, clients with a greater number of average monthly records per loan were more likely to repay their loans with Home Credit. Let's not read too much into this value, but it could indicate that clients who have had more previous credit history are generally more likely to repay a loan.","3e69a25b":"# Putting the Functions Together","35bdf53a":"The control slightly overfits because the training score is higher than the validation score. We can address this in later notebooks when we look at regularization (we already perform some regularization in this model by using `reg_lambda` and `reg_alpha` as well as early stopping). \n\nWe can visualize the feature importance with another function, `plot_feature_importances`. The feature importances may be useful when it's time for feature selection. ","4a28d43d":"##### Kernel Density Estimate Plots","f10de4bb":"### Counts of Bureau Dataframe","713d5539":"The definition of this column is: \"How many days before current application did client apply for Credit Bureau credit\". My interpretation is this is the number of days that the previous loan was applied for before the application for a loan at Home Credit. Therefore, a larger negative number indicates the loan was further before the current loan application. We see an extremely weak positive relationship between the average of this variable and the target meaning that clients who applied for loans further in the past potentially are more likely to repay loans at Home Credit. With a correlation this weak though, it is just as likely to be noise as a signal. \n\n#### The Multiple Comparisons Problem\n\nWhen we have lots of variables, we expect some of them to be correlated just by pure chance, a [problem known as multiple comparisons](https:\/\/towardsdatascience.com\/the-multiple-comparisons-problem-e5573e8b9578). We can make hundreds of features, and some will turn out to be corelated with the target simply because of random noise in the data. Then, when our model trains, it may overfit to these variables because it thinks they have a relationship with the target in the training set, but this does not necessarily generalize to the test set. There are many considerations that we have to take into account when making features! ","d51d6323":"* Well this distribution is all over the place. This variable represents the number of previous loans with a `CREDIT_ACTIVE` value of `Active` divided by the total number of previous loans for a client. The correlation here is so weak that I do not think we should draw any conclusions! ","d3928c19":"### Correlation Function\n\nBefore we move on, we can also make the code to calculate correlations with the target into a function.","7b8af408":"### Correlations of Aggregated Values with Target\n\nWe can calculate the correlation of all new values with the target. Again, we can use these as an approximation of the variables which may be important for modeling. ","7f20218c":"### Insert Computed Features into Training Data","dbabe185":"## Calculate Information for Testing Data","e4488dd4":"We need to create new names for each of these columns. The following code makes new names by appending the stat to the name. Here we have to deal with the fact that the dataframe has a multi-level index. I find these confusing and hard to work with, so I try to reduce to a single level index as quickly as possible.","516dd53d":"__The control scores 0.745","b979ff5b":"To make sure the function worked as intended, we should compare with the aggregated dataframe we constructed by hand. ","1f5acb13":"## Correlations\n\nFirst let's look at the correlations of the variables with the target. We can see in any of the variables we created have a greater correlation than those already present in the training data (from `application`). ","2d956d6a":"The function above returns a `submission` dataframe we can upload to the competition, a `fi` dataframe of feature importances, and a `metrics` dataframe with validation and test performance. ","23ee9235":"### Value counts of Bureau Balance dataframe by loan","b2332af3":"we will remove any columns in either the training or the testing data that have greater than 90% missing values.","db87e701":"### Aggregated Stats of Bureau Dataframe","ab4edca6":"### Aggregated stats of Bureau Balance dataframe by loan","7e6f21fe":"We can merge this dataframe into the training data.","3a4f0c8b":"Test Two scores 0.753","5b80c156":"The `sum` columns represent the count of that category for the associated client and the `mean` represents the normalized count. One-hot encoding makes the process of calculating these figures very easy!","b7a1f3d5":"The definition of this column is: \"How many days before current application did client apply for Credit Bureau credit\". My interpretation is this is the number of days that the previous loan was applied for before the application for a loan at Home Credit. Therefore, a larger negative number indicates the loan was further before the current loan application. We see an extremely weak positive relationship between the average of this variable and the target meaning that clients who applied for loans further in the past potentially are more likely to repay loans at Home Credit. With a correlation this weak though, it is just as likely to be noise as a signal. ","f69c7c1c":"Based on these numbers, the engineered features perform better than the control case. However, we will have to submit the predictions to the leaderboard before we can say if this better validation performance transfers to the testing data. ","72aeb2ad":"Based on these numbers, the engineered features perform better than the control case. However, we will have to submit the predictions to the leaderboard before we can say if this better validation performance transfers to the testing data. ","f1fbeff1":"### Control\n\nThe first step in any experiment is establishing a control. For this we will use the function defined above (that implements a Gradient Boosting Machine model) and the single main data source (`application`). ","9a4ff589":"Over half of the top 100 features were made by us! That should give us confidence that all the hard work we did was worthwhile. ","a3210da8":"### Function to Handle Categorical Variables\n","f9972a94":"# Results\n\nAfter all that work, we can say that including the extra information did improve performance! The model is definitely not optimized to our data, but we still had a noticeable improvement over the original dataset when using the calculated features. Let's officially summarize the performances:\n\n| __Experiment__ | __Train AUC__ | __Validation AUC__ | __Test AUC__  |\n|------------|-------|------------|-------|\n| __Control__    | 0.815 | 0.760      | 0.745 |\n| __Test One__   | 0.837 | 0.767      | 0.759 |\n| __Test Two__   | 0.826 | 0.765      | 0.753 |\n\n\n(Note that these scores may change from run to run of the notebook. I have not observed that the general ordering changes however.)\n\nAll of our hard work translates to a small improvement of 0.014 ROC AUC over the original testing data. Removing the highly collinear variables slightly decreases performance so we will want to consider a different method for feature selection. Moreover, we can say that some of the features we built are among the most important as judged by the model. \n\nIn a competition such as this, even an improvement of this size is enough to move us up 100s of spots on the leaderboard. By making numerous small improvements such as in this notebook, we can gradually achieve better and better performance. I encourage others to use the results here to make their own improvements, and I will continue to document the steps I take to help others. \n\n## Next Steps\n\nGoing forward, we can now use the functions we developed in this notebook on the other datasets. There are still 4 other data files to use in our model! In the next notebook, we will incorporate the information from these other data files (which contain information on previous loans at Home Credit) into our training data. Then we can build the same model and run more experiments to determine the effect of our feature engineering. There is plenty more work to be done in this competition, and plenty more gains in performance to be had! I'll see you in the next notebook.","a4263f42":"### Aggregated Stats of Bureau Balance by Client","b625f210":"First we one-hot encode a dataframe with only the categorical columns (`dtype == 'object'`).","b5938a3e":"### Applying Operations to another dataframe","e0b4fb26":"First, we can calculate the value counts of each status for each loan. Fortunately, we already have a function that does this for us! ","c6792f6a":"### Manual Feature Engineering","95d0f277":"From this it's difficult to tell if this variable will be important. The correlation coefficient is extremely weak and there is almost no noticeable difference in the distributions. \n\nLet's move on to make a few more variables from the bureau dataframe. We will take the mean, min, and max of every numeric column in the bureau dataframe.","d37f1a98":"## Missing Values\n\nAn important consideration is the missing values in the dataframe. Columns with too many missing values might have to be dropped. ","1617f9e0":"We need to align the testing and training dataframes, which means matching up the columns so they have the exact same columns. This shouldn't be an issue here, but when we one-hot encode variables, we need to align the dataframes to make sure they have the same columns.","1b2f68a9":"# Modeling \n\n__For all datasets, use the model shown below (with the exact hyperparameters).__\n\n* control: only the data in the `application` files. \n* test one: the data in the `application` files with all of the data recorded from the `bureau` and `bureau_balance` files\n* test two: the data in the `application` files with all of the data recorded from the `bureau` and `bureau_balance` files with highly correlated variables removed. ","c014c850":"## Function for Numeric Aggregations\nLet's encapsulate all of the previous work into a function","5f4124aa":"These results are better than the control, but slightly lower than the raw features. ","a358ccb7":"### Aggregating Numeric Columns","65f09c51":"We will now turn to the bureau balance dataframe. This dataframe has monthly information about each client's previous loan(s) with other financial institutions. Instead of grouping this dataframe by the `SK_ID_CURR` which is the client id, we will first group the dataframe by the `SK_ID_BUREAU` which is the id of the previous loan. This will give us one row of the dataframe for each loan. Then, we can group by the `SK_ID_CURR` and calculate the aggregations across the loans of each client. The final result will be a dataframe with one row for each client, with stats calculated for their loans.","c7be5b9b":"None of the new variables have a significant correlation with the TARGET. We can look at the KDE plot of the highest correlated variable, bureau_DAYS_CREDIT_mean, with the target in in terms of absolute magnitude correlation. ","bb31c9d6":"For each of these pairs of highly correlated variables, we only want to remove one of the variables. The following code creates a set of variables to remove by only adding one of each pair. ","588ca43e":"The dataframes now have the same columns (with the exception of the `TARGET` column in the training data). This means we can use them in a machine learning model which needs to see the same columns in both the training and testing dataframes.\n\nLet's now look at the percentage of missing values in the testing data so we can figure out the columns that should be dropped."}}