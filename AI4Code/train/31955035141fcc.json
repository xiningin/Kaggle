{"cell_type":{"4719db0b":"code","7c12844a":"code","e3299a7f":"code","8f5ccaae":"code","ffb935b2":"code","f70dd3e5":"code","c815fda4":"code","2631bcbc":"code","7633ac26":"code","e02b916f":"code","d9e989eb":"code","283e036f":"code","439ff505":"code","7ca8fb18":"code","7156264b":"code","ff972f00":"code","5737645a":"code","c3484fc4":"code","8a8992a7":"code","4aa4f548":"code","437dc8b3":"code","5e896bb3":"code","f7ba6ecb":"code","5ab3259c":"code","3ec4f0e3":"code","d4ea10ae":"code","2b9ec533":"code","dbe6e862":"code","5850b99a":"code","28a97c40":"markdown","6b20444e":"markdown","c21810bb":"markdown","ce8ac1dd":"markdown","1e480063":"markdown","881852b8":"markdown","fe4bea42":"markdown","760e414f":"markdown","01186d91":"markdown","c02cb14d":"markdown","dfa838b4":"markdown","b9586b78":"markdown","ad2bc1ec":"markdown","7766054d":"markdown","f9c04b15":"markdown","06420dde":"markdown"},"source":{"4719db0b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom keras.models import Model, load_model\nfrom keras.layers import Input ,BatchNormalization , Activation \nfrom keras.layers.convolutional import Conv2D, UpSampling2D\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import optimizers \nfrom sklearn.model_selection import train_test_split\nimport os\nimport nibabel as nib\nimport cv2 as cv\nimport matplotlib.pyplot as plt\nfrom keras import backend as K\nimport glob\nimport skimage.io as io\nimport skimage.color as color\nimport random as r\nimport math\nfrom nilearn import plotting\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7c12844a":"Flair= nib.load('..\/input\/brats20-dataset-training-validation\/BraTS2020_TrainingData\/MICCAI_BraTS2020_TrainingData\/BraTS20_Training_003\/BraTS20_Training_003_flair.nii')\nSeg= nib.load('..\/input\/brats20-dataset-training-validation\/BraTS2020_TrainingData\/MICCAI_BraTS2020_TrainingData\/BraTS20_Training_003\/BraTS20_Training_003_seg.nii')\nT1= nib.load('..\/input\/brats20-dataset-training-validation\/BraTS2020_TrainingData\/MICCAI_BraTS2020_TrainingData\/BraTS20_Training_003\/BraTS20_Training_003_t1.nii')\nT1ce= nib.load('..\/input\/brats20-dataset-training-validation\/BraTS2020_TrainingData\/MICCAI_BraTS2020_TrainingData\/BraTS20_Training_003\/BraTS20_Training_003_t1ce.nii')\nT2= nib.load('..\/input\/brats20-dataset-training-validation\/BraTS2020_TrainingData\/MICCAI_BraTS2020_TrainingData\/BraTS20_Training_003\/BraTS20_Training_003_t2.nii')","e3299a7f":"plotting.plot_anat(Flair,cut_coords=[-100, 100, 100],title=\"Flair\",draw_cross =False)","8f5ccaae":"plotting.plot_anat(Seg,cut_coords=[-100, 100, 100],title=\"Seg\",draw_cross =False)","ffb935b2":"plotting.plot_anat(T1,cut_coords=[-100, 100, 100],title=\"T1\",draw_cross =False)","f70dd3e5":"plotting.plot_anat(T1ce,cut_coords=[-100, 100, 100],title=\"T1ce\",draw_cross =False)","c815fda4":"plotting.plot_anat(T2,cut_coords=[-100, 100, 100],title=\"T2\",draw_cross =False)","2631bcbc":"Path= '..\/input\/brats20-dataset-training-validation\/BraTS2020_TrainingData\/MICCAI_BraTS2020_TrainingData'\np=os.listdir(Path)\nInput_Data= []\ndef Data_Preprocessing(modalities_dir):\n    all_modalities = []    \n    for modality in modalities_dir:      \n        nifti_file   = nib.load(modality)\n        brain_numpy  = np.asarray(nifti_file.dataobj)    \n        all_modalities.append(brain_numpy)\n    brain_affine   = nifti_file.affine\n    all_modalities = np.array(all_modalities)\n    all_modalities = np.rint(all_modalities).astype(np.int16)\n    all_modalities = all_modalities[:, :, :, :]\n    all_modalities = np.transpose(all_modalities)\n    return all_modalities\nfor i in p[:20]:\n    brain_dir = os.path.normpath(Path+'\/'+i)\n    flair     = glob.glob(os.path.join(brain_dir, '*_flair*.nii'))\n    t1        = glob.glob(os.path.join(brain_dir, '*_t1*.nii'))\n    t1ce      = glob.glob(os.path.join(brain_dir, '*_t1ce*.nii'))\n    t2        = glob.glob(os.path.join(brain_dir, '*_t2*.nii'))\n    gt        = glob.glob( os.path.join(brain_dir, '*_seg*.nii'))\n    modalities_dir = [flair[0], t1[0], t1ce[0], t2[0], gt[0]]\n    P_Data = Data_Preprocessing(modalities_dir)\n    Input_Data.append(P_Data)\n","7633ac26":"fig = plt.figure(figsize=(5,5))\nimmmg = Input_Data[1][100,:,:,3]\nimgplot = plt.imshow(immmg)\nplt.show()","e02b916f":"def Data_Concatenate(Input_Data):\n    counter=0\n    Output= []\n    for i in range(5):\n        print('$')\n        c=0\n        counter=0\n        for ii in range(len(Input_Data)):\n            if (counter != len(Input_Data)):\n                a= Input_Data[counter][:,:,:,i]\n                #print('a={}'.format(a.shape))\n                b= Input_Data[counter+1][:,:,:,i]\n                #print('b={}'.format(b.shape))\n                if(counter==0):\n                    c= np.concatenate((a, b), axis=0)\n                    print('c1={}'.format(c.shape))\n                    counter= counter+2\n                else:\n                    c1= np.concatenate((a, b), axis=0)\n                    c= np.concatenate((c, c1), axis=0)\n                    print('c2={}'.format(c.shape))\n                    counter= counter+2\n        c= c[:,:,:,np.newaxis]\n        Output.append(c)\n    return Output","d9e989eb":"InData= Data_Concatenate(Input_Data)","283e036f":"AIO= concatenate(InData, axis=3)\nAIO=np.array(AIO,dtype='float32')\nTR=np.array(AIO[:,:,:,1],dtype='float32')\nTRL=np.array(AIO[:,:,:,4],dtype='float32')","439ff505":"X_train , X_test, Y_train, Y_test = train_test_split(TR, TRL, test_size=0.15, random_state=32)\nAIO=TRL=0","7ca8fb18":"def Convolution(input_tensor,filters):\n    \n    x = Conv2D(filters=filters,kernel_size=(3, 3),padding = 'same',strides=(1, 1))(input_tensor)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x) \n    return x\n\ndef model(input_shape):\n    \n    inputs = Input((input_shape))\n    \n    conv_1 = Convolution(inputs,32)\n    maxp_1 = MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'same') (conv_1)\n    \n    conv_2 = Convolution(maxp_1,64)\n    maxp_2 = MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'same') (conv_2)\n    \n    conv_3 = Convolution(maxp_2,128)\n    maxp_3 = MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'same') (conv_3)\n    \n    conv_4 = Convolution(maxp_3,256)\n    maxp_4 = MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'same') (conv_4)\n    \n    conv_5 = Convolution(maxp_4,512)\n    upsample_6 = UpSampling2D((2, 2)) (conv_5)\n    \n    conv_6 = Convolution(upsample_6,256)\n    upsample_7 = UpSampling2D((2, 2)) (conv_6)\n    \n    upsample_7 = concatenate([upsample_7, conv_3])\n    \n    conv_7 = Convolution(upsample_7,128)\n    upsample_8 = UpSampling2D((2, 2)) (conv_7)\n    \n    conv_8 = Convolution(upsample_8,64)\n    upsample_9 = UpSampling2D((2, 2)) (conv_8)\n    \n    upsample_9 = concatenate([upsample_9, conv_1])\n    \n    conv_9 = Convolution(upsample_9,32)\n    outputs = Conv2D(1, (1, 1), activation='sigmoid') (conv_9)\n    \n    model = Model(inputs=[inputs], outputs=[outputs]) \n    \n    return model","7156264b":"# Loding the Light weighted CNN\nmodel = model(input_shape = (240,240,1))\nmodel.summary()","ff972f00":"# Computing Dice_Coefficient\ndef dice_coef(y_true, y_pred, smooth=1.0):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n# Computing Precision \ndef precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n\n# Computing Sensitivity      \ndef sensitivity(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    return true_positives \/ (possible_positives + K.epsilon())\n\n# Computing Specificity\ndef specificity(y_true, y_pred):\n    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n    return true_negatives \/ (possible_negatives + K.epsilon())","5737645a":"# Compiling the model \nAdam=optimizers.Adam(lr=0.001)\nmodel.compile(optimizer=Adam, loss='binary_crossentropy', metrics=['accuracy',dice_coef,precision,sensitivity,specificity])","c3484fc4":"# Fitting the model over the data\nhistory = model.fit(X_train,Y_train,batch_size=32,epochs=40,validation_split=0.20,verbose=1,initial_epoch=0)","8a8992a7":"# Evaluating the model on the training and testing data \nmodel.evaluate(x=X_train, y=Y_train, batch_size=32 , verbose=1, sample_weight=None, steps=None)\nmodel.evaluate(x=X_test, y=Y_test, batch_size=32, verbose=1, sample_weight=None, steps=None)","4aa4f548":"# Accuracy vs Epoch\ndef Accuracy_Graph(history):\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    #plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.subplots_adjust(top=1.00, bottom=0.0, left=0.0, right=0.95, hspace=0.25,\n                        wspace=0.35)\n    plt.show()\n    \n# Dice Similarity Coefficient vs Epoch\ndef Dice_coefficient_Graph(history):\n\n    plt.plot(history.history['dice_coef'])\n    plt.plot(history.history['val_dice_coef'])\n    #plt.title('Dice_Coefficient')\n    plt.ylabel('Dice_Coefficient')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.subplots_adjust(top=1.00, bottom=0.0, left=0.0, right=0.95, hspace=0.25,\n                        wspace=0.35)\n    plt.show()\n# Loss vs Epoch\ndef Loss_Graph(history):\n\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    #plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.subplots_adjust(top=1.00, bottom=0.0, left=0.0, right=0.95, hspace=0.25,\n                        wspace=0.35)\n    plt.show()","437dc8b3":"# Plotting the Graphs of Accuracy, Dice_coefficient, Loss at each epoch on Training and Testing data\nAccuracy_Graph(history)\nDice_coefficient_Graph(history)\nLoss_Graph(history)","5e896bb3":"from keras.utils import plot_model\nplot_model(model,to_file='model.png')","f7ba6ecb":"model.save('.\/BraTs2020.h5')","5ab3259c":"model.load_weights('.\/BraTs2020.h5')","3ec4f0e3":"X_train=X_test=Y_train=Y_test=0","d4ea10ae":"fig = plt.figure(figsize=(5,5))\nimmmg = TR[250,:,:]\nimgplot = plt.imshow(immmg)\nplt.show()","2b9ec533":"pref_Tumor = model.predict(TR)","dbe6e862":"fig = plt.figure(figsize=(5,5))\nimmmg = pref_Tumor[250,:,:,0]\nimgplot = plt.imshow(immmg)\nplt.show()","5850b99a":"plt.figure(figsize=(15,10))\n\n\nplt.subplot(341)\nplt.title('Sample 1')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[250,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[250,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(342)\nplt.title('Sample 2')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[550,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[550,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(343)\nplt.title('Sample 3')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[400,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[400,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(344)\nplt.title('Sample 4')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[690,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[690,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(345)\nplt.title('Sample 5')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[850,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[850,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(346)\nplt.title('Sample 6')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[1450,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[1450,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(347)\nplt.title('Sample 7')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[1800,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[1800,:,:]),alpha=0.3,cmap='Reds')\n\nplt.subplot(348)\nplt.title('Sample 8')\nplt.axis('off')\nplt.imshow(np.squeeze(TR[60,:,:]),cmap='gray')\nplt.imshow(np.squeeze(pref_Tumor[60,:,:]),alpha=0.3,cmap='Reds')","28a97c40":"![Unet](https:\/\/lmb.informatik.uni-freiburg.de\/people\/ronneber\/u-net\/u-net-architecture.png)","6b20444e":"# Brain Tumor Segmentation  Unet","c21810bb":"**The process of segmenting tumor from MRI image of a brain is one of the highly focused areas in the community of medical science as MRI is noninvasive imaging. Therefore, I decided to create Brain Tumor Segmentation Notebook  that you can easily use the capabilities of this powerful tool. I am currently in the early stages of designing this Notebook and will be constantly updating these codes to get to a usable version.**","ce8ac1dd":"**T1 weighted images :**\n\n**Magnetic resonance imaging uses the resonance of the protons to generate images. Protons are excited by a radio frequency pulse at an appropriate frequency (Larmor frequency) and then the excess energy is released in the form of a minuscule amount of heat to the surroundings as the spins return to their thermal equilibrium. The magnetization of the proton ensemble goes back to its equilibrium value with an exponential curve characterized by a time constant T1.**\n\n**T1 weighted images can be obtained by setting short repetition time (TR) such as < 750 ms and echo time (TE) such as < 40 ms in conventional spin echo sequences, while in Gradient Echo Sequences they can be obtained by using flip angles of larger than 50o while setting TE values to less than 15 ms.**\n\n**T1 is significantly different between grey matter and white matter and is used when undertaking brain scans. A strong T1 contrast is present between fluid and more solid anatomical structures, making T1 contrast suitable for morphological assessment of the normal or pathological anatomy, e.g., for musculoskeletal applications.**","1e480063":"# Data Preprocessing","881852b8":"**Fluid-attenuated inversion recovery (FLAIR) :**\n\n**FLAIR is an MRI sequence with an inversion recovery set to null fluids. For example, it can be used in brain imaging to suppress cerebrospinal fluid (CSF) effects on the image, so as to bring out the periventricular hyperintense lesions, such as multiple sclerosis (MS) plaques**","fe4bea42":"**Manual segmentation that annotated by an expert**","760e414f":"# **BraTs2020 Data Visualization**\n**In this section, I select a sample data from Brats2020  and visualize it to see the input data**","01186d91":"**If you are interested in working in this field or cooperating with me, you can contact me through the following links :**\n\n**Email :  mehrzadiarash@gmail.com**\n\n**GitHub : [Link](github.com\/arash-mehrzadi)**\n\n**Linkedin : [Link](https:\/\/ir.linkedin.com\/in\/arashmehrzadi)**\n\n**Website : [arashmehrzadi.com](https:\/\/www.arashmehrzadi.com\/)**\n","c02cb14d":"**The u-net is convolutional network architecture for fast and precise segmentation of images. Up to now it has outperformed the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. It has won the Grand Challenge for Computer-Automated Detection of Caries in Bitewing Radiography at ISBI 2015, and it has won the Cell Tracking Challenge at ISBI 2015 on the two most challenging transmitted light microscopy categories (Phase contrast and DIC microscopy) by a large margin. architecture (example for 32x32 pixels in the lowest resolution). Each blue box corresponds to a multi-channel feature map. The number of channels is denoted on top of the box. The x-y-size is provided at the lower left edge of the box. White boxes represent copied feature maps. The arrows denote the different operations.**","dfa838b4":"**Implement Unet**","b9586b78":"**Import Libraries** ","ad2bc1ec":"**T2 weighted image :**\n\n\n**T2 weighted image (T2WI) is one of the basic pulse sequences in MRI. The sequence weighting highlights differences in the T2 relaxation time of tissues.**\n\n**The amount of T2 decay a tissue experiences depends on multiple factors. Each tissue has an inherent T2 value, but external factors (such as magnetic field inhomogeneity) can decrease the T2 relaxation time. This additional effect is captured in T2*. The refocusing pulse in spin-echo sequences helps to mitigate these extraneous influences on the T2 relaxation time, trying to keep the image T2 weighted rather than T2* weighted.**","7766054d":"![](https:\/\/files.miamineurosciencecenter.com\/media\/filer_public_thumbnails\/filer_public\/76\/6b\/766b99f3-7395-4300-b031-1a04b4bc0e24\/brain_tumor_diagnosis.jpeg__1280x800_q85_crop_subsampling-2.jpg)","f9c04b15":"**Check the output**","06420dde":"**concatenate images**"}}