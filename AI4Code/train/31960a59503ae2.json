{"cell_type":{"1883c52a":"code","054684a7":"code","bcd74192":"code","345246ac":"code","40134edc":"code","a185d872":"code","de513b34":"code","a5fc2f09":"code","6124d2af":"code","0d33dd15":"code","8093d640":"code","f0058b98":"markdown","027442d8":"markdown","e8c41d4d":"markdown","43a4e98e":"markdown","f44f4485":"markdown","7b0c6512":"markdown","e469d0b3":"markdown","573b2f87":"markdown","da6d9f88":"markdown","0a3364fe":"markdown","d5dc41b1":"markdown","f140b246":"markdown","b198bcb3":"markdown","36d8ff37":"markdown"},"source":{"1883c52a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn; seaborn.set()","054684a7":"dataset = pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict.csv')","bcd74192":"\ncols =['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR ', 'CGPA', 'Chance of Admit ' ]\n\n\nfor x in cols:\n    plt.hist(dataset.loc[:,x])\n    plt.xlabel(x)\n    plt.ylabel('No Elem per Bin')\n    plt.title(x+'distr')\n    plt.show()","345246ac":"s= ' vs. Chance of Admit'\nfor x in cols[:-1]:\n    plt.scatter(dataset.loc[:,x], dataset.loc[:, 'Chance of Admit '], marker='x')\n    plt.xlabel(x)\n    plt.ylabel('Chance of Admit')\n    plt.title(x+s)\n    plt.show()","40134edc":"legend = ['COA < 0.5', '0.5 < COA < 0.6', '0.6 < COA < 0.7', '0.7 < COA < 0.8', '0.8 < COA < 0.9', 'COA > 0.9']\nfor x in cols[:-1]:\n    for y in cols[:-1]:\n        if(y>x):\n            plt.scatter(\n                dataset.loc[:,x][dataset.iloc[:,-1]<=0.5],\n                dataset.loc[:, y][dataset.iloc[:,-1]<=0.5]\n            )\n\n            plt.scatter(\n                dataset.loc[:,x][ (0.5 < dataset.iloc[:,-1]) & ( dataset.iloc[:,-1] <= 0.6)],\n                dataset.loc[:,y][  (0.5 < dataset.iloc[:,-1]) & ( dataset.iloc[:,-1] <= 0.6)]\n            )\n\n            plt.scatter(\n                dataset.loc[:,x][ (0.6 < dataset.iloc[:,-1]) & (dataset.iloc[:,-1] <=0.7 )],\n                dataset.loc[:,y][ (0.6 < dataset.iloc[:,-1]) & (dataset.iloc[:,-1]<=0.7 )]\n            )\n\n            plt.scatter(\n                dataset.loc[:,x][ (0.7< dataset.iloc[:,-1]) & (dataset.iloc[:,-1]<= 0.8) ],\n                dataset.loc[:,y][ (0.7< dataset.iloc[:,-1]) & (dataset.iloc[:,-1]<= 0.8) ]\n            )\n\n            plt.scatter(\n                dataset.loc[:,x][ (0.8 < dataset.iloc[:,-1]) & (dataset.iloc[:,-1] <= 0.9) ],\n                dataset.loc[:,y][ (0.8 < dataset.iloc[:,-1]) & (dataset.iloc[:,-1] <= 0.9) ]\n            )\n\n            plt.scatter(\n                dataset.loc[:,x][dataset.iloc[:,-1] > 0.9],\n                dataset.loc[:,y][dataset.iloc[:,-1] > 0.9]\n            )\n\n            plt.legend(legend)\n            plt.xlabel(x)\n            plt.ylabel(y)\n            plt.title(x +' vs '+y)\n            plt.show()\n\n\n","a185d872":"from sklearn.model_selection import train_test_split\n\nX= dataset.iloc[:,:-1].values\ny= dataset.iloc[:,-1].values\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\nX_train = X_train[:,1:]\nX_test = X_test[:,1:]","de513b34":"from sklearn.linear_model import LinearRegression\n\nMLR = LinearRegression().fit(X_train, y_train)\nMLR_results = MLR.predict(X_test)\n\nplt.scatter(np.arange(1,len(y_test)+1), y_test, c='r', marker='+')\nplt.scatter(np.arange(1,len(y_test)+1), MLR_results, c='black', marker='.')\nplt.title('Scatter data for MLR')\nplt.xlabel('Candidate Index')\nplt.ylabel('Chance of Admission')\nplt.show()","a5fc2f09":"def error(x1, x2):\n    return np.sum((x1-x2)**2)\n\nerr = np.abs(y_test-MLR_results)\n\nprint(\"Sum error square:\", error(MLR_results,y_test))\nprint(\"Error per prediction:\", np.sum(err)\/len(MLR_results))\nfrom sklearn.metrics import r2_score\nprint(\"r2 score:\",r2_score(MLR_results,y_test))\n\nplt.scatter(\n    y_test,\n    err,\n    marker='x',\n    color = 'black'\n)\nplt.ylabel('Absolute error')\nplt.xlabel('COA')\nplt.title ('Absolute Error distribution')\nplt.show()\n\nplt.scatter(\n    y_test,\n    err**2,\n    marker='x',\n    color = 'black'\n)\nplt.ylabel('Quadratic error')\nplt.xlabel('COA')\nplt.title ('Quadratic Error distribution')\nplt.show()\n\n","6124d2af":"MLR_big_err = y_test[np.abs(MLR_results - y_test) >= np.sum(err)\/len(MLR_results)]\nMLR_small_err = y_test[np.abs(MLR_results - y_test) < np.sum(err)\/len(MLR_results)]\n\nplt.hist(MLR_big_err,alpha=0.5)\nplt.hist(MLR_small_err, alpha=0.5)\nplt.legend(['Big err', 'Small err'])\nplt.title('Bin Population and Errors (MLR)')\nplt.xlabel('COA')\nplt.ylabel('Num of candidates')\nplt.show()","0d33dd15":"X_1= dataset.iloc[:,:-1][dataset.iloc[:,-1] >0.7].values\ny_1= dataset.iloc[:,-1][dataset.iloc[:,-1] >0.7].values\n\n\nX_train, X_test, y_train, y_test = train_test_split(X_1,y_1, test_size=0.2, random_state=42)\nX_train = X_train[:,1:]\nX_test = X_test[:,1:]\n\nMLR_1 = LinearRegression().fit(X_train, y_train)\nMLR_results = MLR_1.predict(X_test)\n\nplt.scatter(np.arange(1,len(y_test)+1), y_test, c='r', marker='+')\nplt.scatter(np.arange(1,len(y_test)+1), MLR_results, c='black', marker='.')\nplt.title('Scatter data for MLR (COA > 0.7)')\nplt.xlabel('Candidate Index')\nplt.ylabel('Chance of Admission')\nplt.show()\nprint(\"r2 score:\",r2_score(MLR_results,y_test))","8093d640":"X_1= dataset.iloc[:,:-1][dataset.iloc[:,-1] <=0.7].values\ny_1= dataset.iloc[:,-1][dataset.iloc[:,-1] <=0.7].values\n\n\nX_train, X_test, y_train, y_test = train_test_split(X_1,y_1, test_size=0.2, random_state=42)\nX_train = X_train[:,1:]\nX_test = X_test[:,1:]\n\nMLR_2 = LinearRegression().fit(X_train, y_train)\nMLR_results = MLR_2.predict(X_test)\n\nplt.scatter(np.arange(1,len(y_test)+1), y_test, c='r', marker='+')\nplt.scatter(np.arange(1,len(y_test)+1), MLR_results, c='black', marker='.')\nplt.title('Scatter data for MLR (COA < 0.7)')\nplt.xlabel('Candidate Index')\nplt.ylabel('Chance of Admission')\nplt.show()\nprint(\"r2 score:\",r2_score(MLR_results,y_test))","f0058b98":"We cannot reject the null hypothesis","027442d8":"Some visualisation of the data in the dataset. Preliminar view of how the data are distributed (Histograms), correlation between data and dipendent variable (Correlation) and individuation of potential clusters of two indipendent variables (Correlation)","e8c41d4d":"## 2- Multilinear Regression (MLR)","43a4e98e":"### 1-2 Correlations\n","f44f4485":"# Kaggle Acception - Rejection dataset","7b0c6512":"### 1-3 Eventual Clusters","e469d0b3":"### Biggest errors","573b2f87":"## 1- Data Visualisation","da6d9f88":"### Note\n\nThe black spots in the previous graph are the predicted COA for the test set of indipendent variables, whereas the red + are the actual results. The image shows that MLR is way more likely to overestimate the COA of a candidate if its actual COA $<0.6$. We can try to visualise that result better by introducing a couple of parameters.","0a3364fe":"Let's train the MLR only on the candidates with a high chance to be admitted (say $> 0.7$)","d5dc41b1":"### 1-1 Histograms","f140b246":"the r2 score is improved. If we do the same for the rest of the data (COA $< 0.7$)","b198bcb3":"This graphs shows exactly what we suggested, the chance of committing a big error and overestimate the COA is bigger for candidates with 'poor' results in the tests.","36d8ff37":"### 2-1 Score for MLR\nGiven the arrays of results ($x_i \\in$ results) and y_pred ($y_i \\in$ y_pred) determine $\\sum_i (x_i - y_i)^2$  "}}