{"cell_type":{"4a503d54":"code","14d637b6":"code","617afd45":"code","4e6c6661":"code","2ba31a4e":"code","fa1a561c":"code","c49f65ea":"code","ecf731a9":"code","5a81972a":"code","264ff6ca":"code","65fc0ca3":"code","4b0ffd2c":"code","bb0b36e3":"code","9ca3b1fc":"code","24594c7e":"code","6d8a13cc":"code","277d15f5":"code","62181506":"code","9cf6d90d":"code","10d820db":"code","deeb382f":"code","404e1325":"code","f8164ca8":"code","1c984b9a":"code","8ee311cf":"code","fe816d3f":"code","a09daed1":"code","cd55a940":"code","8ab801ab":"code","3ff7f456":"code","84867bb4":"code","85a1d77a":"code","fa9a9299":"code","b09cde90":"code","d561772e":"code","aab4d9c7":"code","56559a34":"code","daf0e5b8":"code","4d68b41c":"code","8d1503ce":"markdown","710c2427":"markdown"},"source":{"4a503d54":"#dir_path = os.path.dirname(os.path.realpath(\"cv000_29416.txt\"))#\n#dir_path","14d637b6":"# 1.0 Call libraries\n%reset -f\nimport os\nimport numpy as np\n\n# 1.1 Library to tokenize text (convert to integer-sequences)\nfrom keras.preprocessing.text import Tokenizer\n# 1.2 Make all sentence-tokens of equal length\nfrom keras.preprocessing.sequence import pad_sequences\n# 1.3 Modeling layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Embedding","617afd45":"# 2.0 Constants and files processing\n# This dataset contains just 1000 comments each per neg\/pos folder\n#imdb_dir = '\/home\/ashok\/Documents\/10.nlp_workshop\/imdb\/'\ntrain_dir =os.path.join(\"\/kaggle\/input\/glove-anagha\/\", 'train')\n#train_dir=os.chdir('\/kaggle\/input\/glove-anagha\/train')\n#train_dir","4e6c6661":"# 2.1 Look into folders\nos.listdir(train_dir)                # ['pos', 'neg']\nos.listdir(train_dir + '\/pos')[:5]   # List of files\nos.listdir(train_dir+'\/neg')[:5]     # List of files","2ba31a4e":"# 2.3\nmaxlen_comment = 100          # If comment exceeds this, it will be truncated\ntraining_samples = 400        # Use just 400 comments as training data\nvalidation_samples = 500      # Use 500 samples for validation\nmax_words = 10000             # Select top 10000 words","fa1a561c":"# List of sentiment labels and comments\n#     Start with none\nlabels = []\ntexts = []","c49f65ea":"# How many files are there?\nfname = train_dir+'\/neg'\nlen(os.listdir(fname))               # 1000\n\nfname = train_dir+'\/pos'\nlen(os.listdir(fname))  # 1000","ecf731a9":"# 2.4 Read files from each folder, one-by-one.\n#     As we do so, we  read complete\n#     file-text as one string element & append it\n#     in the list,'texts': \"this \\n is \\t black dog\"\n#     We also append its sentiment as indicated\n#     by its folder name (neg: 0, pos:1)\n#     in the list,'labels'.\n\nfor label_type in ['neg', 'pos']:\n    # 2.3.1 Which directory\n    dir_name = os.path.join(train_dir, label_type)\n    # 2.3.2 For every file in this folder\n    for fname in os.listdir(dir_name):\n        # 2.3.3 Open the file\n        f = open(os.path.join(dir_name, fname))\n        # 2.3.4 Append its text to texts[]\n        texts.append(f.read())\n        f.close()\n        # 2.3.5 And if the directory was 'neg'\n        if label_type == 'neg':\n            # 2.3.6 All comments are negative\n            labels.append(0)\n        else:\n            labels.append(1)\n","5a81972a":"type(texts)      # list\ntype(texts[5])   # 5th element is a str. It is a list of strings\ntexts[5]         # Read this review\nlabels[5]        # Its label is negative\nlen(texts)       # 2000 comments\nlabels[:5]       # look at labels\n","264ff6ca":"# 3. Start processing\n\n# 3.1 Create object to tokenize comments. Pick up\n#     top 'max_words' tokens (by frequency of occurrence)\n#     Its full syntax is:\n#   Tokenizer(num_words=None,     # How many top-freq words to take\n#             filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~#',  # Filter out these\n#             lower=True,         # Convert to lower\n#             split=' ',          # Split words here to create tokens\n#             char_level=False,   # Word-level and not characater level parsing\n#             oov_token=None,     # What token to assign when Out-of-vocab word is seen\n#             document_count=0    # Cannot be changed\n#             )\n\ntokenizer = Tokenizer(num_words = max_words)\n","65fc0ca3":"# 3.2 Train the object on full list\ntokenizer.fit_on_texts(texts)\n# 3.3 Tokenize texts. Actual transformation occurs here\nsequences = tokenizer.texts_to_sequences(texts)\n","4b0ffd2c":"# 3.2 Train the object on full list\ntokenizer.fit_on_texts(texts)\n# 3.3 Tokenize texts. Actual transformation occurs here\nsequences = tokenizer.texts_to_sequences(texts)","bb0b36e3":"len(sequences) \n#sequences[0] \n#texts[0] ","9ca3b1fc":"# 3.5 Which one of the comments have less than 100 words\nfor i in np.arange(len(sequences)):\n    l = len(sequences[i])\n    if (l < 100):\n        print(i)     # 45 (only 1 comment)\n\nlen(sequences[45])   # 16","24594c7e":"# Get word-to-integer dictionary?\nword_index = tokenizer.word_index\n#word_index","6d8a13cc":"# 3.7 Which are the top few most frequent words\nfor i in word_index:\n    if word_index[i] < 10:\n        print(i)\n\n# 3.8\nlen(word_index )","277d15f5":"# 3.9 Make all sequences of equal length\n\ndata = pad_sequences(sequences,               # A list of lists\n                    maxlen = maxlen_comment,  # MAx length of a sequence\n                    padding = \"pre\",          # pad before\n                    truncating = \"pre\"        # Remove values larger than sequence length\n                    )      # Returns numpy array with shape (len(sequences), maxlen)\n","62181506":"data[45]      \nlen(data[45])    # 100\ntype(data)       # numpy.ndarray\ndata.shape       # (2000 X 100)","9cf6d90d":"# 3.91 And what about labels?\ntype(labels)      # list\nlabels = np.asarray(labels)   # Transform list to array\nlabels","10d820db":"# 4.0 Shuffle comments randomly\n# 4.0.1 First generate a simple sequence\nindices = np.arange(data.shape[0])\n#4.0.2 Shuffle this sequence\nnp.random.shuffle(indices)\nindices","deeb382f":"# 4.1 Extract data and corresponding labels\ndata = data[indices, ]\nlabels = labels[indices]\n","404e1325":"# 4.2 Prepare train and validation data\nX_train = data[:training_samples, ]\ny_train = labels[:training_samples]\n\nx_val = data[training_samples:training_samples+validation_samples,]\ny_val = labels[training_samples:training_samples+validation_samples]","f8164ca8":"# 5.1 Put all glove vectors in a dictionary\nembeddings_index = {}","1c984b9a":"#dir_path = os.path.dirname(os.path.realpath(\"glove.6B.50D.txt\"))#\n#dir_path\n#os.listdir('\/kaggle\/input\/glove-6b50d-anagha')  ","8ee311cf":"\n# 5.2 Start reading the file line by line\n#f = open(os.path.join(glove_dir, 'glove.6B.50d.txt'), 'r')\nf=open('\/kaggle\/input\/glove-6b50d-anagha\/glove.6B.50d.txt','r')\nfor line in f:\n    # 5.2.1 Split each line on ' '\n    values = line.split()\n    # 5.2. The first token is the word\n    word = values[0]\n    # 5.2.3 Rest all numbers in the line are vectors for this word\n    vectors = np.asarray(values[1:], dtype = 'float32')\n    # 5.2.4 Update embeddings dictionary\n    embeddings_index[word] = vectors\n\nf.close()","fe816d3f":"# 5.3 Have a look at few vectors\nembeddings_index['at']\nlen(embeddings_index['at'])            # 50\nlen(embeddings_index)                   # 400000","a09daed1":"# 5.3 Have a look at few vectors\nembeddings_index['the']\nlen(embeddings_index['the'])            # 50\nlen(embeddings_index)   ","cd55a940":"# 6. We need to transform this dictionary into\n#    a matrix of shape max_words X vector-size\n#    OR, as: 10000 X 50 matrix. In this matrix\n#    1st row is a vector for 1st word, IInd row\n#    for IInd word and so on. The sequence of\n#    words is as in word_index:\n\nvector_size = 50\n# 6.1 Get an all zero matrix of equal dimension\nembedding_matrix = np.zeros(shape = (max_words, vector_size))\nembedding_matrix.shape","8ab801ab":"# 7. Now fill embedding matrix with vectors\n# word_index.items() is a tuple of form ('the', )\n\ntype(word_index)                # Dictionary\nword_index.items()\ntype(word_index.items())        # dict_items: One iterable collection object\nlist(word_index.items())[0:5]   # key-value tuple","3ff7f456":"for word, i in word_index.items():\n    # 8.1 If token value is less than 10000\n    #     Token coding is as per frequency. Higher\n    #     frequency means higher rank. Rank of\n    #     1 is highest. Word_index of ('the', 5)\n    #     means, 'the' is ranked 5th.\n    if i < max_words:\n        # 8.2 For the particluar key (ie 'word') get value-vector\n        embedding_vector = embeddings_index.get(word)\n        # 8.3 Store the vector in the matrix\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","84867bb4":"# 9. Have a look\nembedding_matrix\nembedding_matrix.shape    # 10000 X 50","85a1d77a":"# 10. Finally develop the model.\n#     Keep it simplest possible ie no lstm or RNN layer\n#     after Embedding layer as in file:\n\n\nmodel = Sequential()\n#  10.1  Adding EMbedding Layer                 10000        50                       100\nmodel.add(Embedding(max_words,vector_size, input_length = maxlen_comment))\nmodel.summary()\n","fa9a9299":"# 10.2 Seed embedding layers with glove weights (10000 X 50)\nmodel.layers[0].name\nmodel.layers[0].set_weights([embedding_matrix])\n# 10.3 And let weghts in this layer not change with back-propagation\nmodel.layers[0].trainable = False","b09cde90":"# 11 Adding some layers by self\n# 11.1 Adding LSTMlayer to model with number of cells equal to 'embedding_vector_length'\n#     And number of neurons in hidden layer equal to no_of_neurons_in_hidden_state\n\nno_of_neurons_in_hidden_state = 50\n# 11.1.1\nfrom keras.layers import LSTM\n# Adding LSTM Layer\nmodel.add(LSTM(\n               units = no_of_neurons_in_hidden_state,\n               return_sequences = False\n              )\n          )\nmodel.summary()","d561772e":"#11.2 Add Drop Out Layer\nfrom keras.layers import Dropout\n# Adding Dropout Layer to increase \nmodel.add(Dropout(0.2))\nmodel.summary()\n","aab4d9c7":"# 10.2\nmodel.add(Dense(32, activation = 'relu'))\nmodel.add(Dense(1,activation = 'sigmoid'))\nmodel.summary()","56559a34":"# 13. compile the model\nmodel.compile(optimizer = 'rmsprop',\n              loss = 'binary_crossentropy',\n              metrics = ['acc'])","daf0e5b8":"# 14. Train the model for 30 epochs\n#     With just 400 training samples we get 59% validation accuracy\nhistory = model.fit(X_train,y_train,\n                    epochs = 100,\n                    batch_size = 32,\n                    validation_data = (x_val,y_val)\n                    )","4d68b41c":"# 15 Draw the learning curve\nimport matplotlib.pyplot as plt\ndef plot_learning_curve():\n    val_acc = history.history['val_acc']\n    tr_acc=history.history['acc']\n    epochs = range(1, len(val_acc) +1)\n    plt.plot(epochs,val_acc, 'b', label = \"Validation accu\")\n    plt.plot(epochs, tr_acc, 'r', label = \"Training accu\")\n    plt.title(\"Training and validation accuracy\")\n    plt.legend()\n    plt.show()\n# 15.1\nplot_learning_curve()","8d1503ce":"**With Addition of 1 LSTM layer and 1 Dropout Layer Validation Accuracy Increased to 66% which is a significant improvement compare to only embedding, Flatten and 2 Dense Layer.**","710c2427":"**CC : Model Bulding:**\n\nBuild Sequential model\nEmbedding layer + Classification layer\nSet weights in embedding layers as per Glove matrix\nFreeze embedding layers from any futher trainning\nAdd classification layer\nCompile and build the model"}}