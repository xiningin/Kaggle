{"cell_type":{"1f47bc19":"code","0be30d2c":"code","54f9d2ce":"code","74f6cf28":"code","fec77171":"code","b45f77a1":"code","18815eaa":"markdown","636a08b8":"markdown","020b2f96":"markdown","db39a492":"markdown"},"source":{"1f47bc19":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0be30d2c":"def raw_data_filtering_and_factor_calculation(input_file,city_id_list,category_dict,p1_start_month,p1_end_month,p2_start_month,p2_end_month):#format of category [\"sscat\",\"KIDS FLAKES\"]\n    \n    all_months_in_total_period = list(range(p1_start_month,p1_end_month+1)) +  list(range(p2_start_month,p2_end_month+1))\n    city_filtered = input_file.loc[input_file.city_id.isin(city_id_list)] \n    city_time_filtered = city_filtered.loc[city_filtered.month_adj.isin(all_months_in_total_period)]\n\n\n    tghh = 0\n    for i in range(len(city_id_list)):\n        tghh += city_time_filtered.loc[city_time_filtered.city_id == city_id_list[i]].groupby(\"month_adj\").tghh.first().mean()\n    \n    sample_bintix_uid = set(city_time_filtered.loc[city_time_filtered.month_adj==all_months_in_total_period[0]].bintix_uid.unique())\n    for a in all_months_in_total_period[1:]:\n        sample_bintix_uid = sample_bintix_uid.intersection(set(city_time_filtered.loc[city_time_filtered.month_adj==a].bintix_uid.unique()))\n    factor = tghh\/len(sample_bintix_uid)\n\n    city_time_category_filtered = city_time_filtered\n    for a in category_dict:\n        city_time_category_filtered = city_time_category_filtered.loc[eval(\"city_time_category_filtered.\"+ a + category_dict[a])]\n    city_time_category_filtered_trimmed_sample = city_time_category_filtered.loc[city_time_category_filtered.bintix_uid.isin(list(sample_bintix_uid))]\n   #     return [city_time_category_filtered_trimmed_sample,factor]\n    return [city_time_category_filtered_trimmed_sample,factor]","54f9d2ce":"def GNL_analysis(p1_start_month,p1_end_month,p2_start_month,p2_end_month,ref_brand,city_id_list,category_dict,raw_data): \n    #ref_brand is a list which contains name of brand,sub_brand,variant but only upto the level which we want.\n    \n    input_file,factor_p1_p2 = raw_data_filtering_and_factor_calculation(raw_data,city_id_list,category_dict,p1_start_month,p1_end_month,p2_start_month,p2_end_month)\n    \n    #basic filtering,tagging,grouping\n    p1 = list(range(p1_start_month,p1_end_month+1))\n    p2 = list(range(p2_start_month,p2_end_month+1))\n    relevant_columns = [\"bintix_uid\",\"brand\",\"sub_brand\",\"variant\",\"std_unit_value\",\"volume\"]\n    p1_purchases = input_file.loc[input_file.month_adj.isin(p1),relevant_columns]\n    p2_purchases = input_file.loc[input_file.month_adj.isin(p2),relevant_columns]\n    \n    for a in [p1_purchases,p2_purchases] : \n        if len(ref_brand) == 1:\n            a.loc[:,[\"brand_other\"]] = np.where(a.brand==ref_brand[0],\"brand\",\"other\")\n        if len(ref_brand) == 2:\n            a.loc[:,[\"brand_other\"]] = np.where((a.brand==ref_brand[0])&(a.sub_brand==ref_brand[1]),\"brand\",\"other\")\n        if len(ref_brand) == 3:\n            a.loc[:,[\"brand_other\"]] = np.where((a.brand==ref_brand[0])&(a.sub_brand==ref_brand[1])&(a.variant==ref_brand[2]),\"brand\",\"other\")\n#         if len(ref_brand) == 4:\n#             a.loc[:,[\"brand_other\"]] = np.where((a.brand==ref_brand[0])&(a.sub_brand==ref_brand[1])&(a.variant==ref_brand[2])&(eval(\"a.std_unit_value\" +ref_brand[3])),\"brand\",\"other\")\n        \n    grouped_p1 = p1_purchases.groupby([\"bintix_uid\",\"brand_other\"]).volume.sum().reset_index()\n    grouped_p2 = p2_purchases.groupby([\"bintix_uid\",\"brand_other\"]).volume.sum().reset_index()\n    \n    \n    \n    #removing unwanted uids - nonbrand buyers for both period\n    other_uid_p1 = grouped_p1.loc[grouped_p1.brand_other==\"other\"].bintix_uid.unique()\n    brand_uid_p1 = grouped_p1.loc[grouped_p1.brand_other==\"brand\"].bintix_uid.unique()\n    other_uid_p2 = grouped_p2.loc[grouped_p2.brand_other==\"other\"].bintix_uid.unique()\n    brand_uid_p2 = grouped_p2.loc[grouped_p2.brand_other==\"brand\"].bintix_uid.unique()\n    p1_uid_only_others = grouped_p1.loc[grouped_p1.bintix_uid.isin(other_uid_p1)&(~grouped_p1.bintix_uid.isin(brand_uid_p1))].bintix_uid.unique()\n    p2_uid_only_others = grouped_p2.loc[grouped_p2.bintix_uid.isin(other_uid_p2)&(~grouped_p2.bintix_uid.isin(brand_uid_p2))].bintix_uid.unique()\n    p1_unwanted_uid = set(p1_uid_only_others)-set(brand_uid_p2)\n    p2_unwanted_uid = set(p2_uid_only_others)-set(brand_uid_p1)\n    grouped_p1_filtered = grouped_p1.loc[~grouped_p1.bintix_uid.isin(p1_unwanted_uid)]\n    grouped_p2_filtered = grouped_p2.loc[~grouped_p2.bintix_uid.isin(p2_unwanted_uid)]\n    merged = pd.merge(grouped_p1_filtered,grouped_p2_filtered,on=[\"bintix_uid\",\"brand_other\"],how=\"outer\").sort_values(by=\"bintix_uid\")\n    \n    #Segmenting\n    new_uid = set(brand_uid_p2) - set(brand_uid_p1) - set(other_uid_p1)\n    lapsed_uid = set(brand_uid_p1) - set(brand_uid_p2) - set(other_uid_p2)\n    volume_change_uid = set(set(brand_uid_p1)-set(other_uid_p1)).intersection(set(brand_uid_p2)-set(other_uid_p2))\n    switch_and_vol_change_uid = set(merged.bintix_uid) - new_uid - lapsed_uid - volume_change_uid\n    possible_dropped_repertoire_uid = set(brand_uid_p1) - set(brand_uid_p2)\n    possible_added_repertoire_uid = set(brand_uid_p2) - set(brand_uid_p1)\n    \n    \n    #tagging\n    merged.loc[merged.bintix_uid.isin(new_uid),[\"type_of_inc_dec\"]] = \"new\"\n    merged.loc[merged.bintix_uid.isin(lapsed_uid),[\"type_of_inc_dec\"]] = \"lapsed\"\n    merged.loc[(merged.bintix_uid.isin(volume_change_uid))&(merged.volume_y>merged.volume_x),[\"type_of_inc_dec\"]] = \"increase_buying\"\n    merged.loc[(merged.bintix_uid.isin(volume_change_uid))&(merged.volume_y<merged.volume_x),[\"type_of_inc_dec\"]] = \"decrease_buying\"\n    merged.loc[(merged.bintix_uid.isin(volume_change_uid))&(merged.volume_y==merged.volume_x),[\"type_of_inc_dec\"]] = \"same_buying\"\n    merged.loc[merged.bintix_uid.isin(switch_and_vol_change_uid),[\"type_of_inc_dec\"]]=\"switching and volume change\"\n    \n    \n    #calculating gain\/loss\n    gain_new = merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"new\")].volume_y.sum()\/1000\n    loss_lapsed = merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"lapsed\")].volume_x.sum()\/1000\n    gain_inc_buying_solus = (merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"increase_buying\")].volume_y.sum() - merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"increase_buying\")].volume_x.sum())\/1000\n    loss_dec_buying_solus = -(merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"decrease_buying\")].volume_y.sum() - merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"decrease_buying\")].volume_x.sum())\/1000\n    merged.volume_x.fillna(0,inplace=True)\n    merged.volume_y.fillna(0,inplace=True)\n    gain_switch_and_vol_change = (merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"switching and volume change\") & (merged.volume_y>merged.volume_x)].volume_y.sum() - merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"switching and volume change\") & (merged.volume_y>merged.volume_x)].volume_x.sum())\/1000\n    loss_switch_and_vol_change = -(merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"switching and volume change\") & (merged.volume_y<merged.volume_x)].volume_y.sum() - merged.loc[(merged.brand_other==\"brand\") & (merged.type_of_inc_dec==\"switching and volume change\") & (merged.volume_y<merged.volume_x)].volume_x.sum())\/1000\n    \n    \n    #switching_pattern\n    all_purchases = pd.concat([p1_purchases,p2_purchases])\n    all_wanted_uid_purchases = all_purchases.loc[~all_purchases.bintix_uid.isin(p1_unwanted_uid.union(p2_unwanted_uid))]\n    if len(ref_brand)==1:\n        all_wanted_uid_purchases.loc[:,[\"brand_sub_brand\"]] = all_wanted_uid_purchases.brand\n    if len(ref_brand)==2:\n        all_wanted_uid_purchases.loc[:,[\"brand_sub_brand\"]] = all_wanted_uid_purchases.brand + \"-\" + all_wanted_uid_purchases.sub_brand\n    if len(ref_brand)==3:\n        all_wanted_uid_purchases.loc[:,[\"brand_sub_brand\"]] = all_wanted_uid_purchases.brand + \"-\" + all_wanted_uid_purchases.sub_brand + \"-\" + all_wanted_uid_purchases.variant\n#     if len(ref_brand)==4\n    unique_brand_sub_brand = all_wanted_uid_purchases.brand_sub_brand.unique()\n    gain_due_to = dict()\n    loss_due_to = dict()\n    gain_due_to[\"increased_buying\"] = 0\n    loss_due_to[\"decreased_buying\"] = 0\n    added_to_repertoire = 0\n    dropped_from_repertoire = 0\n    \n    for a in unique_brand_sub_brand:\n        gain_due_to[a] = 0\n        loss_due_to[a] = 0\n    for uid in list(switch_and_vol_change_uid):\n        p1_uid_purchase = p1_purchases.loc[p1_purchases.bintix_uid == uid]\n        if len(ref_brand)==1:\n            p1_uid_purchase.loc[:,[\"brand_sub_brand\"]] = p1_uid_purchase.brand\n        if len(ref_brand)==2:\n            p1_uid_purchase.loc[:,[\"brand_sub_brand\"]] = p1_uid_purchase.brand + \"-\" + p1_uid_purchase.sub_brand\n        if len(ref_brand)==3:\n            p1_uid_purchase.loc[:,[\"brand_sub_brand\"]] = p1_uid_purchase.brand + \"-\" + p1_uid_purchase.sub_brand + \"-\" + p1_uid_purchase.variant\n#         if len(ref_brand)==4\n        p1_uid_grouped = p1_uid_purchase.groupby(\"brand_sub_brand\").volume.sum().reset_index()\n        \n        p2_uid_purchase = p2_purchases.loc[p2_purchases.bintix_uid == uid]\n        if len(ref_brand)==1:\n            p2_uid_purchase.loc[:,[\"brand_sub_brand\"]] = p2_uid_purchase.brand\n        if len(ref_brand)==2:\n            p2_uid_purchase.loc[:,[\"brand_sub_brand\"]] = p2_uid_purchase.brand + \"-\" + p2_uid_purchase.sub_brand\n        if len(ref_brand)==3:\n            p2_uid_purchase.loc[:,[\"brand_sub_brand\"]] = p2_uid_purchase.brand + \"-\" + p2_uid_purchase.sub_brand + \"-\" + p2_uid_purchase.variant\n#         if len(ref_brand)==4\n        p2_uid_grouped = p2_uid_purchase.groupby(\"brand_sub_brand\").volume.sum().reset_index()\n        \n        merged_switching = p1_uid_grouped.merge(p2_uid_grouped, on = \"brand_sub_brand\", how = \"outer\")\n        merged_switching.fillna(0,inplace=True)\n        merged_switching.loc[:,[\"volume_diff\"]] = merged_switching.volume_y - merged_switching.volume_x\n        vol_diff_uid = merged_switching.volume_diff.sum()\n        \n        if vol_diff_uid<0:\n            merged_switching = pd.concat([merged_switching,pd.DataFrame({\"brand_sub_brand\":[\"decreased_buying\"],\"volume_x\":[0],\"volume_y\":[0],\"volume_diff\":[-vol_diff_uid]})])\n        elif vol_diff_uid>0:\n            merged_switching = pd.concat([merged_switching,pd.DataFrame({\"brand_sub_brand\":[\"increased_buying\"],\"volume_x\":[0],\"volume_y\":[0],\"volume_diff\":[-vol_diff_uid]})])\n        merged_switching.loc[:,[\"sign\"]] = np.where(merged_switching.volume_diff<0,\"negative\",\"non_negative\") \n        sign_desired_brand = merged_switching.loc[merged_switching.brand_sub_brand == \"-\".join(ref_brand)].iloc[0][\"sign\"]\n        factor_for_assigning_gain_loss = merged_switching.loc[merged_switching.brand_sub_brand == \"-\".join(ref_brand)].iloc[0][\"volume_diff\"]\/merged_switching.loc[merged_switching.sign == sign_desired_brand].volume_diff.sum()\n        if sign_desired_brand == \"negative\":\n            for a in merged_switching.loc[merged_switching.sign != sign_desired_brand].brand_sub_brand:\n                if (a==\"decreased_buying\") and (uid in possible_dropped_repertoire_uid):\n                    dropped_from_repertoire += merged_switching.loc[merged_switching.brand_sub_brand == a].iloc[0][\"volume_diff\"]*factor_for_assigning_gain_loss\n                else:\n                    loss_due_to[a] -= merged_switching.loc[merged_switching.brand_sub_brand == a].iloc[0][\"volume_diff\"]*factor_for_assigning_gain_loss\n\n        else:\n            for a in merged_switching.loc[merged_switching.sign != sign_desired_brand].brand_sub_brand:\n                if (a==\"increased_buying\") and (uid in possible_added_repertoire_uid):\n                    added_to_repertoire +=  merged_switching.loc[merged_switching.brand_sub_brand == a].iloc[0][\"volume_diff\"]*factor_for_assigning_gain_loss\n                else:\n                    gain_due_to[a] -= merged_switching.loc[merged_switching.brand_sub_brand == a].iloc[0][\"volume_diff\"]*factor_for_assigning_gain_loss\n\n\n    for a in unique_brand_sub_brand:\n        if gain_due_to[a]==0 and loss_due_to[a]==0:\n            del gain_due_to[a]\n            del loss_due_to[a]\n    \n    #adjustment. gain\/loss due to volume change seperated from switching\n    dropped_from_repertoire = abs(dropped_from_repertoire)\/1000\n    added_to_repertoire = abs(added_to_repertoire)\/1000\n    gain_switch = gain_switch_and_vol_change - abs(gain_due_to[\"increased_buying\"])\/1000 - added_to_repertoire\n    loss_switch = loss_switch_and_vol_change - abs(loss_due_to[\"decreased_buying\"])\/1000 - dropped_from_repertoire\n    gain_inc_buying = gain_inc_buying_solus + abs(gain_due_to[\"increased_buying\"])\/1000\n    loss_dec_buying = loss_dec_buying_solus + abs(loss_due_to[\"decreased_buying\"])\/1000\n\n    \n    del gain_due_to[\"increased_buying\"]\n    del loss_due_to[\"decreased_buying\"]\n    \n    #results\n    gain_brand_switch_list = [round(x*factor_p1_p2\/1000) for x in gain_due_to.values()]\n    loss_brand_switch_list = [round(x*factor_p1_p2\/1000) for x in loss_due_to.values()]\n    gain = [round(x*factor_p1_p2) for x in [gain_new,gain_inc_buying,gain_switch,added_to_repertoire]]\n    loss= [round(x*factor_p1_p2) for x in [-loss_lapsed,-loss_dec_buying,-loss_switch,-dropped_from_repertoire]]\n    net_gain = factor_p1_p2*(p2_purchases.loc[p2_purchases.brand_other==\"brand\"].volume.sum() - p1_purchases.loc[p1_purchases.brand_other==\"brand\"].volume.sum())\n    \n    \n    #Intercation Index\n    interaction_df = pd.DataFrame({\"Brands\":gain_due_to.keys(),\"abs_gain\":gain_brand_switch_list,\"abs_loss\":[abs(x) for x in loss_brand_switch_list]})\n    interaction_df.loc[:,[\"total_gain_loss_brand\"]] = interaction_df.abs_gain + interaction_df.abs_loss\n    interaction_df.loc[:,[\"percent_gain_loss\"]] = (interaction_df.total_gain_loss_brand\/interaction_df.total_gain_loss_brand.sum())*100\n    volume_share_list=[]\n    if len(ref_brand)==1:\n        all_purchases_without_brand = all_purchases.loc[~(all_purchases.brand==ref_brand[0])]\n        all_purchases_without_brand.loc[:,[\"brand_sub_brand\"]] = all_purchases_without_brand.brand \n    if len(ref_brand)==2:\n        all_purchases_without_brand = all_purchases.loc[~((all_purchases.brand==ref_brand[0])&(all_purchases.sub_brand==ref_brand[1]))]\n        all_purchases_without_brand.loc[:,[\"brand_sub_brand\"]] = all_purchases_without_brand.brand + \"-\" + all_purchases_without_brand.sub_brand\n    if len(ref_brand)==3:\n        all_purchases_without_brand = all_purchases.loc[~((all_purchases.brand==ref_brand[0])&(all_purchases.sub_brand==ref_brand[1])&(all_purchases.variant==ref_brand[2]))]\n        all_purchases_without_brand.loc[:,[\"brand_sub_brand\"]] = all_purchases_without_brand.brand + \"-\" + all_purchases_without_brand.sub_brand + \"-\" + all_purchases_without_brand.variant\n#     if len(ref_brand)==4\n\n    for a in gain_due_to.keys():\n        volume_share_list+=[(all_purchases_without_brand.loc[all_purchases_without_brand.brand_sub_brand==a].volume.sum()\/all_purchases_without_brand.volume.sum())*100]\n    interaction_df.loc[:,[\"percent_volume_share_adjusted\"]] = volume_share_list\n    interaction_df.loc[:,[\"interaction_index\"]] = round((interaction_df.percent_gain_loss\/interaction_df.percent_volume_share_adjusted)*100)\n    \n    #presenting final numbers in the format used by Kelloggs's \n    gain_loss_segments = [\"Entry To\/Lapse From Category\",\"Incr\/Decr in Cons.of Ref.Brand\",\"Total Shift To\/From Ref. Brand\",\"Addn.\/Deletion from Repertoire\"]\n    gain_loss_segments_net = [(gain[i]+loss[i])\/1000 for i in range(4)]\n    gain_loss_segments_net_df = pd.DataFrame(data=[[gain_loss_segments[i],gain_loss_segments_net[i]] for i in range(4)],columns=[\"Type\/Brand\",\"net vol in 000Kgs\"])\n    \n    net_df = pd.DataFrame(data=[[\"Brand Net Shift\",round(net_gain\/1000)\/1000]],columns= [\"Type\/Brand\",\"net vol in 000Kgs\"])\n    \n    interaction_df_trimmed = interaction_df.loc[:,[\"Brands\",\"interaction_index\"]]\n    interaction_df_trimmed.rename(columns = {'Brands':'Type\/Brand'}, inplace = True)\n    interaction_df_trimmed.loc[:,[\"net vol in 000Kgs\"]] = [(gain_brand_switch_list[i] + loss_brand_switch_list[i])\/1000 for i in range(len(gain_brand_switch_list))]\n    interaction_df_trimmed = interaction_df_trimmed[[\"Type\/Brand\",\"net vol in 000Kgs\",\"interaction_index\"]]\n    \n\n    return pd.concat([net_df,gain_loss_segments_net_df,interaction_df_trimmed])\n    \n\n    \n    ","74f6cf28":"brand_for_GNL = [\"KELLOGG'S\",\"CHOCOS\"] #[brand] or [brand,sub_brand] or [brand,sub_brand,variant]\np1_start = 28 # month_adj\np1_end = 30 # month_adj\np2_start = 31 # month_adj\np2_end = 33 # month_adj\ncategory_info = {\"scat\":\".isin(['READY TO EAT CEREALS','OATS & PORRIDGE'])\"} # specify categories, scat, sscat to choose for which GNL to be analysed\ncomplete_data = pd.read_csv(\"..\/input\/allcities-nov14\/allcities_Nov14.csv\", encoding='latin-1') #complete data (all cities,cat\/scat\/sscat\/ time - p1 and p2 atleast) is required. Not filtered one. \ncity_name_list =[([1,2,3,4,5,6],\"6Metros\"),([1],\"Hyd\"),([2],\"Blr\"),([3],\"Del\"),([4],\"Mum\"),([5],\"Kol\"),([6],\"Che\")] #city\/city combination for sheet generation\n","fec77171":"pip install openpyxl","b45f77a1":"month_num_dict = {1:\"JAN\",2:\"FEB\",3:\"MAR\",4:\"APR\",5:\"MAY\",6:\"JUN\",7:\"JUL\",8:\"AUG\",9:\"SEP\",10:\"OCT\",11:\"NOV\",12:\"DEC\"}\np1_start_string = month_num_dict[p1_start%12]+str(19+p1_start\/\/12)\np1_end_string = month_num_dict[p1_end%12]+str(19+p1_end\/\/12)\np2_start_string = month_num_dict[p2_start%12]+str(19+p2_start\/\/12)\np2_end_string = month_num_dict[p2_end%12]+str(19+p2_end\/\/12)\ntime_period_string = p1_start_string+\"-\"+p1_end_string+\"_vs_\"+p2_start_string+\"-\"+p2_end_string\n\nwith pd.ExcelWriter(\"GNL_\"+ \"_\".join(brand_for_GNL)+\"_\"+time_period_string+\".xlsx\") as writer:  # doctest: +SKIP                                                                                                       \n    for c in city_name_list:\n        GNL_analysis(p1_start,p1_end,p2_start,p2_end,brand_for_GNL,c[0],category_info,complete_data).to_excel(writer, sheet_name=c[1])\n       ","18815eaa":"# Supporting function\n* For filtering data based on continous bintix uids\n* For calculating new factor","636a08b8":"# Enter the input","020b2f96":"# Output generation","db39a492":"# Main function"}}