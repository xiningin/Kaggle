{"cell_type":{"f8d79393":"code","7df77512":"code","1cfc651f":"code","db73bd49":"code","49846362":"code","7bb2fd47":"code","44121f05":"code","6fffd20b":"code","3ae913c5":"code","50b97bdd":"code","ce97c3ef":"code","3a9ccaab":"code","272628c9":"code","9dfbaedc":"code","37fe9eec":"code","a508909b":"code","85a444a3":"code","421c4653":"code","213fa1e8":"code","71b14af3":"code","3860d398":"code","268a4f96":"code","3ad4e497":"code","3629dbdd":"code","76767aca":"markdown","d11087b9":"markdown","d793a89e":"markdown","d2f00ae3":"markdown"},"source":{"f8d79393":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n\n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7df77512":"#Import library \nimport numpy as np\nimport pandas as pd\nimport itertools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix","1cfc651f":"#Read test data\ntest=pd.read_csv(\"\/kaggle\/input\/fake-news\/test.csv\")\n#Get shape and head\ntest.shape\ntest.head()","db73bd49":"#read train data\ntrain=pd.read_csv(\"\/kaggle\/input\/fake-news\/train.csv\")\n#Get shape and head\ntrain.shape\ntrain.head()","49846362":"##Now check how  many  missing values\nprint(test.isnull().sum())\nfor col in test.columns:\n    pct_missing = np.mean(test[col].isnull())\n    print('{} - {}%'.format(col, round(pct_missing*100)))\n","7bb2fd47":"#Filling  with dummy data As first approach\n\ntest.author = test.author.fillna(\"not provided\")\ntest.title = test.title.fillna(\"missing\")\ntest.text = test.text.fillna(\"missing\")\n\n","44121f05":"##Lets see if still there are null values \n\ntest.isnull().sum()","6fffd20b":"##Now check  many now missing values\nprint(train.isnull().sum())\nfor col in train.columns:\n    pct_missing = np.mean(train[col].isnull())\n    print('{} - {}%'.format(col, round(pct_missing*100)))\n","3ae913c5":"\n#Filling  with dummy data As first approach\n\ntrain.author = train.author.fillna(\"not provided\")\ntrain.title = train.title.fillna(\"missing\")\ntrain.text = train.text.fillna(\"missing\")\n","50b97bdd":"train.isnull().sum()","ce97c3ef":"#get the labels from the DataFrame\nlabels=train.label\nlabels.head()","3a9ccaab":"#checking and cross verifying shape of data's before and after concat .\nprint(test.shape)\n\nprint(train.shape)\n","272628c9":"# As we only have \"text\"  which have News information , So we can drop other columns as preprocessing only needed for this column only.\nnew_train= train['text'].copy()\nnew_test.head()\n","9dfbaedc":"#Do above same for test data as well .\nnew_test= test['text'].copy()\nnew_test.head()\n","37fe9eec":"x_train, x_val, y_train, y_val = train_test_split(new_train, labels, test_size=0.2, random_state=10)","a508909b":"#DataFlair - Initialize a TfidfVectorizer\ntfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)\n#DataFlair - Fit and transform train set, transform test set\ntfidf_train=tfidf_vectorizer.fit_transform(x_train) \ntfidf_test=tfidf_vectorizer.transform(new_test)\ntfidf_val=tfidf_vectorizer.transform(x_val) \n","85a444a3":"#cross verify final text data if its fine or not .\n#print(tfidf_vectorizer.get_feature_names())","421c4653":"#Initialize a PassiveAggressiveClassifier\npac=PassiveAggressiveClassifier(max_iter=80)\npac.fit(tfidf_train,y_train)\n#Predict on the test set and calculate accuracy\ny_pred=pac.predict(tfidf_val)\nscore=accuracy_score(y_val,y_pred)\nprint(f'Accuracy: {round(score*100,2)}%')","213fa1e8":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_val, y_pred)\nprint(cm)","71b14af3":"import seaborn as sns\nimport matplotlib.pyplot as plt\nax= plt.subplot()\nsns.heatmap(cm, annot=True, fmt='g', ax=ax);\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\nax.set_title('Confusion Matrix');\nax.xaxis.set_ticklabels(['TRUE', 'FAKE']); \nax.yaxis.set_ticklabels(['TRUE', 'FAKE']);","3860d398":"final_predictions = pac.predict(tfidf_test)","268a4f96":"output = pd.DataFrame({'id': test.id, 'label': final_predictions})\noutput.to_csv('submit.csv', index=False)\nprint(\"Your submission was successfully saved!\")","3ad4e497":"rows = len(test.axes[0])\n  ","3629dbdd":"df=pd.read_csv(\"\/kaggle\/working\/submit.csv\")\nlen(df.axes[0])\n","76767aca":"There is no null values for text field.","d11087b9":"Now data is ready as we don't have any null values , lets proceed for next steps","d793a89e":"#Let\u2019s initialize a TfidfVectorizer with stop words from the English language.Stop words are the most common words in a language that are to be filtered out before processing the natural language data. \n#And a TfidfVectorizer turns a collection of raw documents into a matrix of TF-IDF features","d2f00ae3":"Splitting train_data into train and validation set"}}