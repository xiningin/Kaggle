{"cell_type":{"22bf3b1a":"code","1dd495e3":"code","0e3f5bb3":"code","ea7eb8ca":"code","e77af928":"code","eeb03f0f":"code","09892b88":"code","48f283f6":"code","d9685164":"code","623768d2":"code","c5e58b29":"code","88273aa3":"code","86977a32":"code","4f0eb7fb":"code","0bd0b72f":"code","d79bc7a4":"code","f43dd8c6":"code","75c6c5e3":"code","95a6f16d":"code","9f1a9cf6":"code","8739620b":"code","3ba36dd2":"code","d3ddb32b":"code","2997d89a":"code","e088bbab":"code","1726ded1":"code","7c2680e8":"code","4496e959":"code","0f9707fc":"code","3a5cdd7c":"code","807772ee":"code","4f0f9f98":"code","9524c9a6":"code","8cd474f0":"code","951b556f":"code","fe8083a6":"code","122545f3":"code","028665f4":"code","77a0c120":"code","b4a7f56f":"code","5858dda9":"code","0a62bf27":"code","9c61381d":"code","d6c6946e":"code","094c7b7d":"code","518b5ea5":"code","c51070a9":"code","7bcbd959":"code","5e633bbb":"code","fd24e737":"code","ce24628b":"code","3c057c96":"code","286950b6":"code","5dfebc1a":"code","323e99b6":"code","792bd13e":"code","c6925057":"code","b3d802fc":"code","27ff9e3f":"code","56b9d3e3":"code","fa1d2a4a":"code","91988505":"code","306ac835":"code","fd676fca":"code","33649248":"code","92b45341":"code","20feaadd":"code","1c4a9f43":"code","7cf0d9d5":"code","6d707e8a":"code","f670a063":"code","d4ae634a":"code","24975ee0":"code","423b3064":"code","32cd90f0":"code","0d158eb7":"code","2cabe22d":"code","71edfc1e":"code","40850c5f":"code","8e4d4588":"code","d7ade0ff":"code","5977d13a":"code","ffa64ba5":"code","1b4ace89":"code","500215f1":"code","db7d580c":"code","daa67e67":"code","ef30643c":"code","f9e44f6e":"code","c59eb55e":"code","6e1d8306":"code","8d1bc586":"code","d9f3d38c":"code","36efc151":"code","bb30d008":"code","42360ba2":"code","702f2816":"code","04440ab3":"code","261a2592":"code","d6271a76":"code","1657c98f":"code","cef4f72c":"code","04bf5bcf":"code","7abe7815":"code","9da4cffb":"code","97aeec78":"code","899f0044":"code","2bf5a456":"markdown","1f7e6c0b":"markdown","7718e102":"markdown","bb1f1f9e":"markdown","39eeddf7":"markdown","da4b3a79":"markdown","2b05c17d":"markdown","92384891":"markdown","8151be81":"markdown","16002569":"markdown","d5313fd6":"markdown","9fbba3df":"markdown","40dce2bc":"markdown","f563327b":"markdown","e9112980":"markdown","fc145684":"markdown","eb25e72f":"markdown","150ea658":"markdown","bcbe0c86":"markdown","c7f758c5":"markdown","f9b3d70b":"markdown","f70dfc51":"markdown","e25c5b9f":"markdown","d6bc9d3f":"markdown","f9e0eb3b":"markdown","6053b1b7":"markdown","403d83f6":"markdown","1bdd4372":"markdown","312d60dc":"markdown","38b4ad21":"markdown","8bd6fce7":"markdown","f9c0ab89":"markdown","2e451d5d":"markdown","c96129b5":"markdown","6b7487a1":"markdown","c23a3f00":"markdown","41b39ae5":"markdown","f2f34338":"markdown","76abb5b7":"markdown","25201521":"markdown","c3945c77":"markdown","2e4f8abd":"markdown","851242c0":"markdown","8d9f6ec9":"markdown","b73f4437":"markdown","02190f9f":"markdown","e369d5ea":"markdown","ccc31f5b":"markdown","241e3092":"markdown","62b889b7":"markdown","51b250c4":"markdown","b351b875":"markdown","c3b53869":"markdown","051b85bd":"markdown","28223bc4":"markdown","7117e7c3":"markdown","18b4120a":"markdown","82cd70ec":"markdown","c983fe48":"markdown","a9a8d8a2":"markdown","dec9ca09":"markdown","ca6c440c":"markdown","e956807a":"markdown","e05ae880":"markdown","2958fb8e":"markdown","baea7106":"markdown","04709ee4":"markdown","4a0ed65c":"markdown","d92c3b0d":"markdown","0cf8aaa2":"markdown","7b9bd9e0":"markdown","b63ea10b":"markdown","8be250f5":"markdown","c97749f2":"markdown","81c8f7e9":"markdown","c71feda9":"markdown","dacac8b9":"markdown","3c9a9b4c":"markdown","01dcc665":"markdown","db6b640a":"markdown","2f1c27f4":"markdown","7df03c24":"markdown","82c4f59d":"markdown","83162be3":"markdown","c385486e":"markdown","6fdb2569":"markdown","4de8f147":"markdown","3b208658":"markdown","fd2894c4":"markdown","a41156d9":"markdown","c52cad48":"markdown","e9836474":"markdown","64f27317":"markdown","011500ba":"markdown","4f4f5dfc":"markdown","7bc21d93":"markdown","3f79a5d1":"markdown","513cc92d":"markdown","d42363c8":"markdown","4b0efeef":"markdown","72e17465":"markdown","251617f8":"markdown","2e53e431":"markdown","6c5ce85a":"markdown","086bfa8d":"markdown","985171ad":"markdown"},"source":{"22bf3b1a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'figure.max_open_warning': 0})\nimport seaborn as sns","1dd495e3":"data = pd.read_csv('..\/input\/house-sales-train\/house_sales_train_data.csv')","0e3f5bb3":"data.head()","ea7eb8ca":"data.tail()","e77af928":"print(\"Size of data: \", data.size)\nprint(\"Shape of data: \", data.shape)\nprint('There are {} data items with {} features \\n \\n '.format(data.shape[0], data.shape[1]))","eeb03f0f":"data.columns","09892b88":"data.dtypes\n# We see some int, float and some of type object!! These can be transformed if needed.","48f283f6":"data.info()","d9685164":"# find numerical variables\nnumerical = [var for var in data.columns if data[var].dtype!='O']\nprint('There are {} numerical variables'.format(len(numerical)))","623768d2":"# we have following numerical features with unique values\nfor var in numerical:\n    print(var, \" => unique count =\",len(data[var].unique()))","c5e58b29":"# Do we have discrte variables....... amoung numerical variables?\ndiscrete = []\nfor var in numerical:\n    if len(data[var].unique())<20:\n        discrete.append(var)\n        print(var , ' is discrete with unique count ' , len(data[var].unique()), ' => ', data[var].unique())\n        \nprint('There are {} discrete variables'.format(len(discrete)))\n\n# These discrete variables selectively should be handled differently than continous variables or numeric variable, \n# if by domain knowledge they are actually categorical example OverAllQual - is a rating 1 to 10","88273aa3":"# Identify continuous variables\ncontinuous = [var for var in numerical if var not in discrete and var not in ['Id', 'SalePrice']]\nprint('There are {} continuous variables'.format(len(continuous)))\ncontinuous","86977a32":"# find categorical variables\ncategorical = [var for var in data.columns if data[var].dtype=='O']\nprint('There are {} categorical variables'.format(len(categorical)))","4f0eb7fb":"for var in categorical:\n    print(var, \" => unique count =\",len(data[var].unique()),\" => unique set = \", data[var].unique(), \"\\n\")","0bd0b72f":"data.describe()","d79bc7a4":"data.describe(include=\"all\")","f43dd8c6":"for var in categorical:\n    print(var, \"=> unique count =\",len(data[var].unique()), \"\\n--------------------------\\n\", data[var].value_counts())","75c6c5e3":"corrMatrix = data.corr()\ncorrMatrix","95a6f16d":"# Correlation with output variable SalePrice\nfeatureCorrWithtarget = abs(corrMatrix[\"SalePrice\"])\nfeatureCorrWithtarget.sort_values(ascending = False)*100","9f1a9cf6":"#plot heat map\nplt.figure(figsize=(30,30))\nsns.heatmap(corrMatrix, annot=True, cmap=plt.cm.Reds)\nplt.show()","8739620b":"top_feature = corrMatrix.index[abs(corrMatrix['SalePrice']>0.5)]\nplt.subplots(figsize=(12, 8))\ntop_corr = data[top_feature].corr()\nsns.heatmap(top_corr, annot=True)\nplt.show()","3ba36dd2":"#plot of missing value attributes\nplt.figure(figsize=(12, 6))\nsns.heatmap(data.isnull())\nplt.show()","d3ddb32b":"# Any missing values\nprint('Any missing values - ', data.isnull().values.any())\n# Total number of missing values\nprint('Total number of missing values', data.isnull().sum().sum())","2997d89a":" # Columns having all nulls\nprint('Columns having all nulls - ',data.isnull().all(axis=0).sum())\n# Columns having at least one null\nprint('Columns having at least one null - ', data.isnull().any(axis=0).sum() )","e088bbab":"# Count of nulls in each row\n# Number of rows having at least one missing value\nprint('Number of rows having at least one missing value - ', data.isnull().any(axis=1).sum())\n# Number of rows with all column values null\nprint('Number of rows with all column values null - ', data.isnull().all(axis=1).sum())","1726ded1":"total = data.isnull().sum().sort_values(ascending = False)\npercent = (data.isnull().mean()*100).sort_values(ascending = False)\nmissingData = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissingData","7c2680e8":"missingDataFrame = (data.isnull().sum()\/len(data)*100).sort_values(ascending = False).to_frame()\nmissingDataFrame.columns = ['count']\nmissingDataFrame.index.names = ['Name']\nmissingDataFrame['Name'] = missingDataFrame.index\n\nplt.figure(figsize=(13, 5))\nsns.set(style='whitegrid')\nsns.barplot(x='Name', y='count', data=missingDataFrame)\nplt.xticks(rotation = 90)\nplt.show()","4496e959":"# PoolQC has missing % is 99 %. So, there is fill by Missing\ndata['PoolQC'] = data['PoolQC'].fillna('Missing')\n\n# Others where missing % is above and around 50 % replace with Missing \ndata['MiscFeature'] = data['MiscFeature'].fillna('Missing')\ndata['Alley'] = data['Alley'].fillna('Missing')\ndata['Fence'] = data['Fence'].fillna('Missing')\ndata['FireplaceQu'] = data['FireplaceQu'].fillna('Missing')","0f9707fc":"#GarageType, GarageFinish, GarageQual and GarageCond these are replacing with Missing\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    data[col] = data[col].fillna('Missing')","3a5cdd7c":"#GarageYrBlt, GarageArea and GarageCars these are replacing with zero\nfor col in ['GarageYrBlt', 'GarageArea', 'GarageCars']:\n    data[col] = data[col].fillna(int(0))","807772ee":"#BsmtFinType2, BsmtExposure, BsmtFinType1, BsmtCond, BsmtQual these are replacing with Missing\nfor col in ('BsmtFinType2', 'BsmtExposure', 'BsmtFinType1', 'BsmtCond', 'BsmtQual'):\n    data[col] = data[col].fillna('Missing')","4f0f9f98":"# Filling 'LotFrontage' according to Neighborhood. At times you fill data using alternative stategies like \n# -- imputing with median but with median of certain sub group\ndata['LotFrontage'] = data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","9524c9a6":"data['MasVnrArea'] = data['MasVnrArea'].fillna(int(0))\ndata['MasVnrType'] = data['MasVnrType'].fillna('Missing')","8cd474f0":"data['Electrical'] = data['Electrical'].fillna(data['Electrical']).mode()[0]","951b556f":"plt.figure(figsize=(10, 5))\nsns.heatmap(data.isnull(), cmap='coolwarm',)","fe8083a6":"# check absence of null values\nprint('This House Data contains empty\/NA values in', data.isnull().sum().sum(), 'cells')","122545f3":"# find numerical variables\nnumerical = [var for var in data.columns if data[var].dtype!='O']\nprint('There are {} numerical variables'.format(len(numerical)))\n# find categorical variables\ncategorical = [var for var in data.columns if data[var].dtype=='O']\nprint('There are {} categorical variables'.format(len(categorical)))","028665f4":"# MSSubClass is not actually number but is cat type \ndata['MSSubClass'] = data['MSSubClass'].astype('object')","77a0c120":"# find numerical variables\nnumerical = [var for var in data.columns if data[var].dtype!='O']\nprint('There are {} numerical variables'.format(len(numerical)))\n# find categorical variables\ncategorical = [var for var in data.columns if data[var].dtype=='O']\nprint('There are {} categorical variables'.format(len(categorical)))","b4a7f56f":"# We have YearBuilt, YearRemodAdd etc. as Time variable -- 2003 \n# if year subtracted from current year we will get better visibility of its impact on Sales Price\nimport datetime\nnow = datetime.datetime.now()\n# Of above YearBuilt - can be marked and transformed in categorical as this can be grouped in buckets\ndata['YearBuiltAgo'] = now.year - data['YearBuilt']\ndata['YearBuiltAgo'].unique()","5858dda9":"# Here we shall move the YearBuiltAgo new column into few buckets \n\nyearDiffBins= [0,10,25,50,80,100,200]\nyearLabels = [1,2,3,4,5,6]\ndata['year_build_ago_group'] = pd.cut(data['YearBuiltAgo'], bins=yearDiffBins, labels=yearLabels, right=False)\ndata['year_build_ago_group'] = data['year_build_ago_group'].astype('object')\ndata['year_build_ago_group'].unique()\n\n","0a62bf27":"# Getting Description\ndata['SalePrice'].describe()","9c61381d":"#histogram\nsns.distplot(data['SalePrice']);","d6c6946e":"plt.hist(data['SalePrice'], color='green')\nplt.show()","094c7b7d":"# Normalizing the target variable\ntarget = np.log(data['SalePrice'])\nplt.hist(target, color='blue')\nplt.show()","518b5ea5":"sns.distplot(target);","c51070a9":"# find numerical variables\nnumerical = [var for var in data.columns if data[var].dtype!='O']\nprint('There are {} numerical variables'.format(len(numerical)))\n# find categorical variables\ncategorical = [var for var in data.columns if data[var].dtype=='O']\nprint('There are {} categorical variables'.format(len(categorical)))","7bcbd959":"# transform some of the categorical values to numeric ones as they are oridinal\nprint(data['ExterQual'].unique())\n\ndata['ExterQual'].replace(\"Po\",1,inplace=True)\n\ndata['ExterQual'].replace(\"Fa\",2,inplace=True)\n\ndata['ExterQual'].replace(\"TA\",3,inplace=True)\n\ndata['ExterQual'].replace(\"Gd\",4,inplace=True)\n\ndata['ExterQual'].replace(\"Ex\",5,inplace=True)\n\nprint(data['ExterQual'].unique())","5e633bbb":"# find numerical variables\nnumerical = [var for var in data.columns if data[var].dtype!='O']\nprint('There are {} numerical variables'.format(len(numerical)))\n# find categorical variables\ncategorical = [var for var in data.columns if data[var].dtype=='O']\nprint('There are {} categorical variables'.format(len(categorical)))","fd24e737":"corrMatrix = data.corr()\ncorrMatrix","ce24628b":"# Correlation with output variable\nfeatureCorrWithtarget = abs(corrMatrix[\"SalePrice\"])\nfeatureCorrWithtarget.sort_values(ascending = False)*100","3c057c96":"plt.figure(figsize=(20,20))\nsns.heatmap(corrMatrix, annot=True, cmap=plt.cm.Reds)\nplt.show()","286950b6":"plt.figure(figsize=(30,30))\nsns.heatmap(corrMatrix[(corrMatrix >= 0.5) | (corrMatrix <= -0.5)], \n            annot=True, annot_kws={\"size\": 8}, square=True);\nplt.show()","5dfebc1a":"top_feature = corrMatrix.index[abs(corrMatrix['SalePrice']>0.5)]\nplt.subplots(figsize=(12, 8))\ntop_corr = data[top_feature].corr()\nsns.heatmap(top_corr, annot=True)\nplt.show()","323e99b6":"print('The Mean of SalePrice is ',data['SalePrice'].mean())","792bd13e":"print('The Median of SalePrice is ',data['SalePrice'].median())","c6925057":"print('The Mode of SalePrice is ', data['SalePrice'].mode())","b3d802fc":"print('Max-', data['SalePrice'].max())\nprint('Min-', data['SalePrice'].min())\nprint('Range-', data['SalePrice'].max()-data['SalePrice'].min())","27ff9e3f":"print('The Variance of SalePrice is ', data['SalePrice'].var())","56b9d3e3":"print('The standard deviation of SalePrice is ', data['SalePrice'].std())","fa1d2a4a":"# We saw this last time.\nprint('The Descriptive stats of SalePrice is ',data['SalePrice'].describe())","91988505":"# Computing IQR\nQ1 = data['SalePrice'].quantile(0.25)\nQ3 = data['SalePrice'].quantile(0.75)\nIQR = Q3 - Q1\nprint('Q1 - ', Q1)\nprint('Q3 - ', Q3)\nprint('IQR - ', IQR)","306ac835":"print('The skewness of SalePrice is ',data['SalePrice'].skew())","fd676fca":"print('The Kurtosis of SalePrice is- ',data['SalePrice'].kurt())","33649248":"potenQ1 = Q1 - (1.5 * IQR)\npotenQ3 = Q3 + (1.5 * IQR)\noutlierQ1 = Q1 - (3 * IQR)\noutlierQ3 = Q3 + (3 * IQR)\nprint('Q1 - ', Q1)\nprint('Q3 - ', Q3)\nprint('IQR - ', IQR)\nprint('The potential outliers lie outside the range of: [Q1 - (1.5 \u00d7 IQR), Q3 + (1.5 \u00d7 IQR)] are (', potenQ1, potenQ3,')');\nprint('The problematic outliers lie outside of: [Q1 - (3 \u00d7 IQR), Q3 + (3 \u00d7 IQR)] are (', outlierQ1, outlierQ3,')')","92b45341":"# Non Graphical Univariate Analysis\ndef non_graphical_numerical_analysis(varName):\n    print(\"\\n----------------------------------------------------------------------------------------------------------------\\n\")\n    print('non_graphical_numerical_analysis for ', varName)    \n    print(\"\\n----------------------------------------------------------------------------------------------------------------\\n\")\n    print('The Mean of', varName,' is ',data[varName].mean())\n    print('The Median of ', varName,' is ',data[varName].median())\n    print('The Mode of ', varName,' is ', data[varName].mode())\n    print('Max of ', varName,' is ', data[varName].max())\n    print('Min of ', varName,' is ', data[varName].min())\n    print('Range of ', varName,' is ', data[varName].max()-data[varName].min())\n    print('The Variance of ', varName,' is ', data[varName].var())\n    print('The Standard Deviation of ', varName,' is ', data[varName].std())\n    print('The Descriptive stats of ', varName,' is ',data[varName].describe())\n    print('The skewness of ', varName,' is ',data[varName].skew())\n    print('The Kurtosis of ', varName,' is ',data[varName].kurt())\n    print('Q1 - ', Q1)\n    print('Q3 - ', Q3)\n    print('IQR - ', IQR)\n    print('The potential outliers lie outside the range of: [Q1 - (1.5 \u00d7 IQR), Q3 + (1.5 \u00d7 IQR)] are (', potenQ1, potenQ3,')');\n    print('The problematic outliers lie outside of: [Q1 - (3 \u00d7 IQR), Q3 + (3 \u00d7 IQR)] are (', outlierQ1, outlierQ3,')')\n    print(\"\\n----------------------------------------------------------------------------------------------------------------\\n\")","20feaadd":"non_graphical_numerical_analysis('SalePrice')","1c4a9f43":"facet = None\nsns.boxplot(facet, 'SalePrice',data = data)","7cf0d9d5":"# SalePrice does have outliers","6d707e8a":"#histogram\nplt.hist(data['SalePrice'], color='blue')\nplt.show()","f670a063":"sns.distplot(data['SalePrice']);\nprint(\"Skewness: %f\" % data['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % data['SalePrice'].kurt())","d4ae634a":"# We make a log transformation, the resulting distribution looks much better \n# i.e. suits regression problems where normal distribution is needed\ndata['SalePrice'] = np.log(data['SalePrice'])","24975ee0":"#histogram\nplt.hist(data['SalePrice'], color='green')\nplt.show()","423b3064":"sns.distplot(data['SalePrice']);\n# skewness and kurtosis\nprint(\"Skewness: %f\" % data['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % data['SalePrice'].kurt())","32cd90f0":"print('There are {} numerical variables'.format(len(numerical)))","0d158eb7":"# Histograms - One good way of visualizing the distribution of a numerical variable.\nnum_cols = ['YearBuilt', 'TotalBsmtSF', 'GrLivArea', 'SalePrice','OverallQual','OverallCond']\nfor i in range(0,len(num_cols),2):\n    if len(num_cols) > i+1:\n        plt.figure(figsize=(10,4))\n        plt.subplot(121)\n        sns.distplot(data[num_cols[i]], kde=False)\n        plt.subplot(122)\n        sns.distplot(data[num_cols[i+1]], kde=False)\n        plt.tight_layout()\n        plt.show()\n    else:\n        sns.distplot(data[num_cols[i]], kde=False)","2cabe22d":"num_cols = ['YearBuilt', 'TotalBsmtSF', 'GrLivArea', 'SalePrice','OverallQual','OverallCond']\nfor i in range(0,len(num_cols),2):\n    if len(num_cols) > i+1:\n        plt.figure(figsize=(10,4))\n        plt.subplot(121)\n        sns.distplot(data[num_cols[i]], hist=True, kde=True)\n        plt.subplot(122)\n        sns.distplot(data[num_cols[i+1]], hist=True, kde=True)\n        plt.tight_layout()\n        plt.show()\n    else:\n        sns.distplot(data[num_cols[i]], hist=True, kde=True)","71edfc1e":"# observed that all of the histograms are left or right skewed, hence a transformation is required to make them linear.","40850c5f":"# BoxPlots\nnum_cols = ['YearBuilt', 'TotalBsmtSF', 'GrLivArea', 'SalePrice']\nfacet = None\nfor i in range(0,len(num_cols),2):\n    if len(num_cols) > i+1:\n        plt.figure(figsize=(10,4))\n        plt.subplot(121)\n        sns.boxplot(facet, num_cols[i],data = data)\n        plt.subplot(122)\n        sns.boxplot(facet, num_cols[i+1],data = data)\n        plt.tight_layout()\n        plt.show()\n    else:\n        sns.boxplot(facet, num_cols[i],data = data)","8e4d4588":"# The same can be done for all 40+ variables ....","d7ade0ff":"def graphical_numerical_analysis(data, featureName, title):\n    fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\n    \n    sns.boxplot(x = featureName, data = data, orient = 'v', ax = ax1)\n    ax1.set_xlabel(featureName, fontsize=15)\n    ax1.set_ylabel(featureName, fontsize=15)\n    ax1.set_title(title, fontsize=15)\n    ax1.tick_params(labelsize=15)\n\n    sns.distplot(data[featureName], ax = ax2)\n    sns.despine(ax = ax2)\n    ax2.set_xlabel(featureName, fontsize=15)\n    ax2.set_ylabel('Occurence', fontsize=15)\n    ax2.set_title(featureName+ ' x Ocucurence', fontsize=15)\n    ax2.tick_params(labelsize=15)\n\n    plt.subplots_adjust(wspace=0.5)\n    plt.tight_layout() ","5977d13a":"for i in range(0,len(numerical)):\n    non_graphical_numerical_analysis(numerical[i])","ffa64ba5":"dataNum = data.select_dtypes(include = ['float64', 'int64'])\ndataNum.head()\ndataNum.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8);","1b4ace89":"print('There are {} categorical variables'.format(len(categorical)))","500215f1":"for var in categorical:\n    print(var, \"=> unique count =\",len(data[var].unique()))\n    print(data[var].value_counts())\n    print(\"-----------------------------------------------\\n\")","db7d580c":"plt.figure(figsize=(12,6))\nsns.countplot(y='Neighborhood', data=data)","daa67e67":"sns.countplot('SaleCondition', data=data)","ef30643c":"data['SaleCondition'].value_counts().plot.bar()","f9e44f6e":"data['SaleCondition'].value_counts().plot.pie()","c59eb55e":"sns.countplot('SaleCondition', data=data)","6e1d8306":"f = pd.melt(data, value_vars=categorical)\ng = sns.FacetGrid(f, col=\"variable\", col_wrap=3, sharex=False, sharey=False, height = 5)\ng = g.map(sns.countplot, \"value\")\nplt.show()","8d1bc586":"corrMatrix = data.corr()","d9f3d38c":"plt.figure(figsize=(12,8))\nsns.heatmap(corrMatrix, cmap='viridis')","36efc151":"impFeatures = corrMatrix['SalePrice'][abs(corrMatrix['SalePrice']) > 0.5].sort_values(ascending=False)\nprint(\"There is {} strongly correlated values with SalePrice:\\n{}\".format(len(impFeatures), impFeatures))","bb30d008":"k = 10 #number of variables for heatmap\ncols = corrMatrix.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = data[cols].corr()\nplt.figure(figsize=(10,6))\nsns.heatmap(cm, annot=True, cmap = 'viridis')","42360ba2":"# lmplot is one of the plots to do a scatterplot in seaborn,\n# it'll by default fit a regression line on top which you can control using 'fit_reg' argument\nsns.lmplot('GrLivArea', 'SalePrice', data=data, fit_reg=True)","702f2816":"# Example of imputing outlier\ndata = data[data[\"GrLivArea\"] < 4000]","04440ab3":"sns.lmplot('GrLivArea', 'SalePrice', data=data, fit_reg=True)","261a2592":"# SalePrice vs all other Numerical variables...\n# scatter plot in pairs\nfor i in range(0, len(dataNum.columns), 5):\n    sns.pairplot(data=dataNum,\n                x_vars=dataNum.columns[i:i+5],\n                y_vars=['SalePrice'])","d6271a76":"# Trying to plot all the numerical features in a seaborn pairplot will take us too much time and will be hard to interpret\n# So we plot pair plot between 'SalePrice' and top correlated variables and heapmap also between top correlated variables","1657c98f":"# Scatter plots between 'SalePrice' and top correlated variables\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'ExterQual','GarageArea','TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(data[cols], height = 2.5)\nplt.show();","cef4f72c":"corr = dataNum.drop('SalePrice', axis=1).corr() \nplt.figure(figsize=(12, 10))\n\nsns.heatmap(corr[(corr >= 0.5) | (corr <= -0.5)], \n            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot=True, annot_kws={\"size\": 8}, square=True);","04bf5bcf":"categoricalList = data.select_dtypes(include = ['O'])\nprint('There are {} categorical variables'.format(len(categorical)))\nprint(categoricalList.columns.tolist())","7abe7815":"# To plot these relation between Categorical and Numerical we can use boxplots, example\nplt.figure(figsize=(15,8))\nplt.xticks(rotation = 45)\nsns.boxplot('Neighborhood', 'SalePrice', data=data)","9da4cffb":"fig, ax = plt.subplots()\nfig.set_size_inches(16, 5)\nsns.violinplot(x='Neighborhood', y='SalePrice', data=data, ax=ax)\nplt.xticks(rotation=45)\nplt.show()","97aeec78":"def boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\nf = pd.melt(data, id_vars=['SalePrice'], value_vars=categoricalList)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=3, sharex=False, sharey=False, height=5)\ng = g.map(boxplot, \"value\", \"SalePrice\")\nplt.show()","899f0044":"def violinplot(x, y, **kwargs):\n    sns.violinplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\nf = pd.melt(data, id_vars=['SalePrice'], value_vars=categoricalList)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=3, sharex=False, sharey=False, height=5)\ng = g.map(violinplot, \"value\", \"SalePrice\")\nplt.show()","2bf5a456":"### Data Glance\nThe head function will tell you the top records in the data set. By default, python shows you only the top 5 records.\n","1f7e6c0b":"# <a> Import libraries<\/a>\n\n* pandas: Used for storing and playing with data using DataFrame.\n* numpy: Used for working with arrays in python.\n* matplotlib, seaborn: Used for plotting graphs from data.","7718e102":"#### Box Plot\n\nBoxplot used for looking statistical data and outliers of data\nThey show mean, median, quartiles and Outliers on single plot.\nBox in graph represents the following statistics in visual:\n1. Minimum of data\n2. 25th Percentile\n3. Mean\n4. 75th Percentile\n5. Maximum of data\n6. Dots above that line are Outliers(Having values 3 times more than the interquartile range)","bb1f1f9e":"The histograms of the numeric data show that there are numeric values present in the dataset that consists of few discrete values, e.g. \"OverallQual\". These features will be transformed later in the data manipulation part from numerical to categorical.","39eeddf7":"In this case there are two houses which have an area above ~4500 and they don't follow the trend","da4b3a79":"* SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n\n* MSSubClass: The building class\n\n* MSZoning: The general zoning classification\n\n* LotFrontage: Linear feet of street connected to property\n\n* LotArea: Lot size in square feet\n\n* Street: Type of road access\n\n* Alley: Type of alley access\n\n* LotShape: General shape of property\n\n* LandContour: Flatness of the property\n\n* Utilities: Type of utilities available\n\n* LotConfig: Lot configuration\n\n* LandSlope: Slope of property\n\n* Neighborhood: Physical locations within Ames city limits\n\n* Condition1: Proximity to main road or railroad\n\n* Condition2: Proximity to main road or railroad (if a second is present)\n\n* BldgType: Type of dwelling\n\n* HouseStyle: Style of dwelling\n\n* OverallQual: Overall material and finish quality\n\n* OverallCond: Overall condition rating\n\n* YearBuilt: Original construction date\n\n* YearRemodAdd: Remodel date\n\n* RoofStyle: Type of roof\n\n* RoofMatl: Roof material\n\n* Exterior1st: Exterior covering on house\n\n* Exterior2nd: Exterior covering on house (if more than one material)\n\n* MasVnrType: Masonry veneer type\n\n* MasVnrArea: Masonry veneer area in square feet\n\n* ExterQual: Exterior material quality\n\n* ExterCond: Present condition of the material on the exterior\n\n* Foundation: Type of foundation\n\n* BsmtQual: Height of the basement\n\n* BsmtCond: General condition of the basement\n\n* BsmtExposure: Walkout or garden level basement walls\n\n* BsmtFinType1: Quality of basement finished area\n\n* BsmtFinSF1: Type 1 finished square feet\n\n* BsmtFinType2: Quality of second finished area (if present)\n\n* BsmtFinSF2: Type 2 finished square feet\n\n* BsmtUnfSF: Unfinished square feet of basement area\n\n* TotalBsmtSF: Total square feet of basement area\n\n* Heating: Type of heating\n\n* HeatingQC: Heating quality and condition\n\n* CentralAir: Central air conditioning\n\n* Electrical: Electrical system\n\n* 1stFlrSF: First Floor square feet\n\n* 2ndFlrSF: Second floor square feet\n\n* LowQualFinSF: Low quality finished square feet (all floors)\n\n* GrLivArea: Above grade (ground) living area square feet\n\n* BsmtFullBath: Basement full bathrooms\n\n* BsmtHalfBath: Basement half bathrooms\n\n* FullBath: Full bathrooms above grade\n\n* HalfBath: Half baths above grade\n\n* Bedroom: Number of bedrooms above basement level\n\n* Kitchen: Number of kitchens\n\n* KitchenQual: Kitchen quality\n\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n\n* Functional: Home functionality rating\n\n* Fireplaces: Number of fireplaces\n\n* FireplaceQu: Fireplace quality\n\n* GarageType: Garage location\n\n* GarageYrBlt: Year garage was built\n\n* GarageFinish: Interior finish of the garage\n\n* GarageCars: Size of garage in car capacity\n\n* GarageArea: Size of garage in square feet\n\n* GarageQual: Garage quality\n\n* GarageCond: Garage condition\n\n* PavedDrive: Paved driveway\n\n* WoodDeckSF: Wood deck area in square feet\n\n* OpenPorchSF: Open porch area in square feet\n\n* EnclosedPorch: Enclosed porch area in square feet\n\n* 3SsnPorch: Three season porch area in square feet\n\n* ScreenPorch: Screen porch area in square feet\n\n* PoolArea: Pool area in square feet\n\n* PoolQC: Pool quality\n\n* Fence: Fence quality\n\n* MiscFeature: Miscellaneous feature not covered in other categories\n\n* MiscVal: Value of miscellaneous feature\n\n* MoSold: Month Sold\n\n* YrSold: Year Sold\n\n* SaleType: Type of sale\n\n* SaleCondition: Condition of sale","2b05c17d":"#### Skewness\nSkewness is also another measure to check for normality \n\nNormal Distribution is symmetric, which means its tails on one side are the mirror image of the other side. But this is not the case with most datasets. We call these types of distributions Skewed Distributions. In the Normal Distribution, Mean, Median and Mode are equal\n\nLeft Skewed Distribution\nWhen data points cluster on the right side of the distribution, then the tail would be longer on the left side. This is the property of Left Skewed Distribution. The tail is longer in the negative direction so we also call it Negatively Skewed Distribution. The mean is generally smaller than the median.\n\nHere, Mode > Median > Mean.\n\n\nRight Skewed Distribution\nWhen data points cluster on the left side of the distribution, then the tail would be longer on the right side. This is the property of Right Skewed Distribution. Here, the tail is longer in the positive direction so we also call it Positively Skewed Distribution. The mean is generally higher than the median.\n\nMode < Median < Mean\n\nThe shorter the left or right tail, the closer the mean to the median.\n\nPandas has a function to compute skewness\n\n* If the value is less than -0.5, we consider the distribution to be negatively skewed or left-skewed\n* If the value is greater than 0.5, we consider the distribution to be positively skewed or right-skewed\n* If the value is between -0.5 and 0.5, we consider the distribution to be approximately symmetric\n","92384891":"* Deviate from the normal distribution.\n* Have appreciable positive skewness\/skewed to right\n* Show peakedness ","8151be81":"## Identification of variables and data types\n\nIn this step, we will check what the data set comprises of. We will execute the below commands on the data\n\n* head of the dataset\n* tail of the dataset\n* the shape of the dataset\n* columns in the dataset\n* Identify Predictor (Input) and Target (output) variables\n* datatype of columns in the dataset\n* info of the dataset","16002569":"### Temporal Variables\nTemporal Variables: ML Algorithms do not understand datetime vars,  extract info like no of years or age if dob given or doj given etc\u2026","d5313fd6":"## Basic Statistical Descriptions of Data.","9fbba3df":"### <a> Descriptive Statistics <\/a>\n\nIf we would like to get a statistical summary of each column, such as count, column mean value, column standard deviation, etc. We use the describe method:\n\n* Mean: The mean is the arithmetic average, for calculating the mean just add up all of the values and divide by the number of observations in your dataset.\n\n* Median: The median is the middle value. It is the value that splits the dataset in half. To find the median, order your data from smallest to largest, and then find the data point that has an equal amount of values above it and below it. The method for locating the median varies slightly depending on whether your dataset has an even or odd number of values.\n\n* Mode: The mode is the value that occurs the most frequently in your data set i.e. has the highest frequency. On a bar chart, the mode is the highest bar. If the data have multiple values that are tied for occurring the most frequently, you have a multimodal distribution. If no value repeats, the data do not have a mode.\n\n* Standard deviation: It measures how spread out the values in a data set are around the mean.\n* Range  - Max - Min\n* Quartiles - 25%, 50%, 75%\n\netc...\n\nTo do so use describe method on data\nNote - you would get stats on numerical data only.","40dce2bc":"#### Correlation","f563327b":" ## Clean Missing Data","e9112980":"### Measures of Spread\/Dispersion\nVariability or spread is the measure of how close or far the data lie from the center of the distribution. The less variability, the more data are closer to the center. The more variability, the more data are spread further away from it.","fc145684":"##### Normal Distribution\n* The curve is\u00a0symmetric\u00a0around the Mean\n* Mean, Median, and Mode are all the same\/same\n\n\n\nEmpirical Rule for Normal Distribution\n* 68.27% of data lies within 1 standard deviation of the mean\n* 95.45% of data lies within 2 standard deviations of the mean\n* 99.73% of data lies within 3 standard deviations of the mean\n\n\nThis rule enables us to check for Outliers\n","eb25e72f":"# Week 3 \n\nWe will look at the following steps of EDA today!!\n\n* Identification of variables and data types\n* Descriptive Statistics\n* Missing value treatment\n* Variable transformations ,Binning, Scaling\/Normalization\n* Correlation ","150ea658":"### Binning\n\nBinning is a process of transforming continuous numerical variables into discrete categorical 'bins', for grouped analysis. Decision of categorization technique is based on business understanding. For example, we can categorize income in three categories, namely: High, Average and Low.\n","bcbe0c86":"Another way is to create dummy variables to replace the categories in a categorical variable into features of each category and represent it using 1 or 0 based on the presence or absence of the categorical value in the record.\n\nThere are several different techniques which are used to encode categorical values which are\n1. pd.get_dummies - Converts categorical features into dummy variables Reference https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html \n2. LabelEncoder - A function present in the scikit- learn library of python which is used to convert categorical values in numerical values. Reference https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html?highlight=label%20encoder#sklearn.preprocessing.LabelEncoder\n3. OneHotEncoder - A function present in the scikit- learn library of python which is used to convert categorical values in numerical values as a one-hot numeric array.. Reference https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html ","c7f758c5":"<hr\/>","f9b3d70b":"#### columns data types\ndf.dtypes => Return the dtypes in the DataFrame.","f70dfc51":"Like the histogram, the KDE plots encodes the density of observations on one axis with height along the other\naxis:","e25c5b9f":"### Numerical vs. Numerical\n1. Scatterplot\n2. Heatmap for correlation","d6bc9d3f":"Data Exploration is about describing the data by means of statistical and visualization techniques.\n\nLets revist Basic Statistical Descriptions of Data.","f9e0eb3b":"box plots and violin plots can be plot against all categorical variable and Saleprice for analysis","6053b1b7":"### Categorical vs. Categorical\n1. Stacked Column Chart\n2. Grouped bar chart\n3. Point plot","403d83f6":"#### Modality\n\n * A distribution might be unimodal with one prominent peak, bimodal with two prominent peaks, or uniform with no prominent peaks.\n * With more than two prominent peaks a distribution is usually said to be multimodal.\n * A bimodal distribution might indicate that there are two distinct groups in your data.\n","1bdd4372":"##### How to identify Outliers\nMost commonly used method to detect outliers is visualization. We use various visualization methods, like Box-plot, Histogram, Scatter Plot . Some analysts also various thumb rules to detect outliers. Some of them are:\n\n* Any value, which is beyond the range of -1.5 x IQR to 1.5 x IQR\n* Use capping methods. Any value which out of range of 5th and 95th percentile can be considered as outlier\n* Data points, three or more standard deviation away from mean are considered outlier\n* Outlier detection is merely a special case of the examination of data for influential data points and it also depends on the business understanding","312d60dc":"### How to Treat Missing Value","38b4ad21":"#### Mean\nThe mean is the <b>arithmetic average<\/b> for calculating the mean just add up all of the values and divide by the number of observations in your dataset.","8bd6fce7":"#### Deviation\nThe (signed) deviation for an observation is the difference between that observation and the mean.\u00a0","f9c0ab89":"#### Variance\nThe mean of the squares of the deviations.","2e451d5d":"Connect with Data Source Collection \/ Extraction source to load it back","c96129b5":"### Scaling, Standardization, Normalizarion\nStandardization: Numerical value may be different units of measurement example kg vs lbs or currency of different countries like dollar vs Yen vs Euro\n\nScaling is process where you transforming your data so that it fits within a specific scale, like 0-100 or 0-1. By scaling your variables, you can help compare different variables on equal footing.\n\nNormalization is the process of transforming values of several variables into a similar range\/scale plus scaling the variable so the variable average is 0 and the variance is 1","6b7487a1":"df.describe() => Generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution, excluding NaN values.","c23a3f00":"### Correcting Data Type\nCorrecting Data Type: At times incorrect data type is assigned to column this should be fixed. Note we use df.dtype() to check data type and df.astype() to change data type","41b39ae5":"Numerical vs. Numerical\n1. Scatterplot\n2. Heatmap for correlation\n\n\nCategorical vs. Numerical\n1. Box plot\n2. Violin plot\n\n\nTwo Categorical Variables\n1. Stacked Column Chart\n2. Grouped bar chart\n3. Point plot","f2f34338":"### Distinct\/Unique Value Counts - numerical","76abb5b7":"#### Histogram\nA histogram is a display of statistical information that uses bars to show the frequency of data items in\nsuccessive numerical intervals of equal size. Histograms can be both vertical and horizontal.\n\nA histogram represents the distribution of data by forming bins along the range of the data and then drawing bars\nto show the number of observations that fall in each bin.\nThe bin can be of any size.\nLet's plot a histogram for our target variable:","25201521":"### Understand column's\/variable's data","c3945c77":"### Univariate Analysis for other numerical varaibles\nWe have many numerical variables, so to see example let pick up few features for univariate analysis\n'OverallQual', 'YearBuilt', 'TotalBsmtSF', 'GrLivArea', 'OverallQual','OverallCond' and the target variable 'SalePrice'.","2e4f8abd":"## <a> Correlation<\/a>","851242c0":"for i in range(0,len(numerical)):\n    graphical_numerical_analysis(data, numerical[i], numerical[i])","8d9f6ec9":"#### Fix by Dropping the column or row \n\nRows could be deleted if the number of missing values are insignificant in number, as that would not impact analysis.\n\ndf.dropna(axis=0, thresh=1, inplace=True)\n\nColumns could be removed if missing value are quite significant in number. \n\n#Dropping Columns with all Nulls\ndf.dropna(axis=1, thresh=1, inplace=True)\n\nThis is not suggested most times as though this is simple, as this method reduces the power of model because it reduces the sample size.\n\nIt is always better to keep data than to discard it. \n\nSometimes you can drop variables if the data is missing for more than 60% observations but only if that variable is insignificant.","b73f4437":"### Variable Types\nVariable are of different types \n1. Numerical\n2. Categorical\n\nLets know them","02190f9f":"### Measures of Central Tendency","e369d5ea":"#### Fix by Imputation\nImputation is a method to fill in the missing values with some value.\n\nThe value could be\n* Mean \/ Median imputation is one of the most frequently used methods for numerical columns. It consists of replacing the missing data for a given attribute by the mean or median of all known values of that variable.\n* Mode\/Static imputation is done for categorical columns. It is either word None\/Missing\/? or most frequent observation in the data set.\n\nSome ways are\n\n* Mark it as missing or with value you feel is apt wrt the column and business domain - For the categorical columns\n* Mark it as 0 or with value you feel is apt wrt the column and business domain - For the numerical columns\n* Impute with mode value - For the categorical column, you can replace the missing values with mode values i.e. the frequent ones.\n* Impute with mean value - For the numerical column,\u00a0But before replacing with mean value, it is advisable to check that the variable shouldn\u2019t have extreme values .i.e. outliers.\n* Impute with median value - For the numerical column,\u00a0In case you have extreme values such as outliers it is advisable to use the median approach.\n\nImputation can be Univariate or MultiVariate","ccc31f5b":"* 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'.\n* 'GarageCars' and 'GarageArea' are also correlated variables. However no of cars that can fit into a\ngarage is dependent on the garage area and one can remove one of these\n* TotalBsmtSF and 1stFlrSF are also highy correlated. We can drop one of these.","241e3092":"## Identify and handle Missing Value - ?, NaN, 0, \u201cN\/A\u201d, Blank Cell\nWhy missing values treatment is required? -  \nMissing data in the training data set can reduce the power \/ fit of a model or can lead to a biased model because we have not analysed the behavior and relationship with other variables correctly. It can lead to wrong prediction or classification.\n\n\n\nWhy my data has missing values? -  \n* MCAR (Missing completely at random) - if the probability of a missing data value is independent of any observation\/columns in the data set, then the data are said to be missing completely at random (MCAR). There\u2019s no relationship between whether a data point is missing and any values in the data set, missing or observed. For example MCAR is a weighing scale that ran out of batteries.\n* MAR (Missing at random) - If the probability of being missing is the same only within groups defined by the observed data, then the data are missing at random (MAR). For example, when a weighing scale placed on a soft surface may produce more missing values than when placed on a hard surface.\n* Missing not at random (MNAR). if the probability of a missing data value is unknown. For example, when a weighing scale goes bad and is giving missing data but is not noted.\n","62b889b7":"##### Kurtosis\n\nAnother numerical measure to check for Normality is Kurtosis.\nFor the symmetric type of distribution, the Kurtosis value will be close to Zero\n\nPandas has a function to compute Kurtosis\n* If the Kurtosis value is close to Zero, we call such types of distributions as Mesokurtic distribution. Normal Distribution have this.\n* If the Kurtosis value is less than zero, we call such types of distributions as Leptokurtic Distribution.  Here, Tail will be fatter and will have longer distribution\/Peak. This shows there are extreme values present in the data,\n* If the Kurtosis value is greater than zero, we call such types of distributions as Platykurtic Distribution.  It will have a thinner tail and a shorter distribution. \n","51b250c4":"Log of a variable is a\u00a0common transformation method used to change the shape of distribution of the variable on a distribution plot. It is generally\u00a0used for reducing right skewness of variables.","b351b875":"Now lets gets our hand dirty by applying exploratory data analysis and trying to get some insights about our dataset of house features and its sales prices ","c3b53869":"This number may sound big, but it could be houses dont have these features hence missing.... ","051b85bd":"The tail function will tell you the bottom records in the data set. By default, python shows you only the top 5 records.","28223bc4":"One points we can note here quickly is there is an exponential increase in SalePrice vs Yearbuilt in the recent past","7117e7c3":"#### countplot \n\nA count plot can be thought of as a histogram across a categorical\n\nThe countplots of the categorical data show the number of the different categories for each feature and their frequency distribution.","18b4120a":"## Graphical Univariate Analysis of Categorical  Variable\ncountplots, barplots and piecharts can be used for Graphical Univariate Analysis of Categorical  Variable\nFew examples","82cd70ec":"### Univariate Analysis for Categorical Variables","c983fe48":"## Univariate Analysis\n\n\u201cUni\u201d means \u201cone\u201d, so in other words your data has only one variable. Univariate analysis explores variables (attributes) one by one. \n\nVariables could be either categorical or numerical. \n\nThere are different statistical and visualization techniques of investigation for each type of variable.\n\nThe most commonly used visualization techniques for univariate analysis are\n* Histogram\n* Box Plot","a9a8d8a2":"##### Categorical Variables\nCategorical variables can also be nominal or ordinal. \n\n* Nominal data has no intrinsic ordering to the categories. For example gender (Male, Female, Other) has no specific ordering.\n\n* Ordinal data as clear ordering such as three settings on a toaster (high medium and low).\n","dec9ca09":"# Last Week\n\nWe looked at the following steps of EDA!!\n\n* Identification of variables and data types\n* Descriptive Statistics\n* Missing value treatment\n* Variable transformations ,Binning, Scaling\/Normalization\n* Correlation ","ca6c440c":"## <a id='2'> Loading the data set - Reading CSV files<\/a>\nWe use pandas.read_csv() function to read the csv file. \nIn the bracket, we put the train_data.csv, so that pandas will read the file into a data frame from that address. \nNote - The file path can be either an URL or your local file address.\n","e956807a":"Now no outliers for GrLivArea and SalePrice","e05ae880":"# Bivariate Analysis\nWhen we talk about bivariate analysis, it means analyzing 2 variables. Since we know there are numerical and categorical variables, there is a way of analyzing these variables as shown below:","2958fb8e":"What is the Impact of Outliers on a dataset?\n\n\nOutliers can drastically change the results of the data analysis and statistical modeling. There are numerous unfavourable impacts of outliers in the data set:\n\n* It increases the error variance and reduces the power of statistical tests\n* If the outliers are non-randomly distributed, they can decrease normality\n* They can introduce bias\n\nHence they should be identified and fixed.\n\nFormula to get outlier\n* The potential outliers lie outside the range of: [Q1 - (1.5 \u00d7 IQR), Q3 + (1.5 \u00d7 IQR)]\n* The problematic outliers lie outside of: [Q1 - (3 \u00d7 IQR), Q3 + (3 \u00d7 IQR)].","baea7106":"Now we have got basics of Basic Statistical Descriptions of Data lets start with Univariate and Bivariate analysis","04709ee4":"### SalePrice Our Target Variable Univariate Analysis","4a0ed65c":"#### Numerical\nNumerical variables can be \n1. binary\n2. continuous\n3. discrete","d92c3b0d":"We explore data in order to bring important aspects of that data into focus for further analysis.\n\nThe analysis is done in two ways\n* Univariate Analysis - Univariate analysis explores variables (attributes) one by one. Univariate analysis is done on both categorical\u00a0or\u00a0numerical variables. \n* Bivariate\/Multivariate Analyssis - Bivariate analysis is the simultaneous analysis of two or more variables (attributes). It explores the concept of relationship between two variables, whether there exists an association and the strength of this association, or whether there are differences between two variables and the significance of these differences.\n\nBoth these analysis can be done non-graphical or graphical way.\n\nNon graphical exploratory data analysis is the first step when beginning to analyze your data as part of the general data analysis approach.\n\nThis preliminary data analysis step focuses on four points\n* measures of central tendency, i.e. the mean, the median and mode,\n* measures of spread, i.e. variability, variants and standard deviation,\n* the shape of the distribution, and\n* the existence of\u00a0outliers.\n\nNon graphical exploratory data analysis may be followed by, or be engaged in concurrently with\u00a0graphical exploratory data analysis. Graphical EDA involves visual exploratory analysis of the data.\n\nThere are different statistical and visualization techniques of investigation for univariate and bivariate analysis","0cf8aaa2":"#### Correlation","7b9bd9e0":"### Categorical vs. Numerical\n1. Box Plots - A Box plot as describe above is used to visualize outliers, max, min, median, interquartile range, and also skewness\n2. Violin Plots - A Violin Plot is used to visualise the distribution of the data and its probability density. This chart is a combination of a Box Plot and a Density Plot that is rotated and placed on each side, to show the distribution shape of the data.\n\nBoth box plots and violin plots can be used to identify outliers","b63ea10b":"<hr\/>\n<hr\/>\n<hr\/>","8be250f5":"##  Variable Transformations, Binning, Normalization, Scaling,\n\nThis involves converting data from one format to another format that may be better for analysis or better suited for processing in Machine Learning algorithms\n\nLets look at each one by one ---","c97749f2":"##### Standard deviation\nThe standard deviation is the square root of the variance\u00a0","81c8f7e9":"# Next Week \/ Coming Weeks ---\n* Data Visualization in more Depth \n* Correlation Analysis\n* Dimension Detection\n* Imbalance Detection\n* More .......\n","c71feda9":"SalePrice is our target variable and also the dependent variable for prediction. According to the assumptions of Linear Regression, data should be normally distributed. By checking the distribution of SalePrice, we can decide if we need non-linear transformation, like log term, to make better prediction.","dacac8b9":"# Encode categorical variables using LabelEncoder\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ndata['MSZoning'] = le.fit_transform(data['MSZoning'])\nprint('Shape post encoding catgrorical columns', data.shape)\n\n# Same can be done for other also","3c9a9b4c":"#### Mode\n\nThe mode is the value that occurs the most frequently in your data set i.e. has the highest frequency. \n\nOn a bar chart, the mode is the highest bar. \n\nIf the data have multiple values that are tied for occurring the most frequently, you have a multimodal distribution. \n\nIf no value repeats, the data do not have a mode.","01dcc665":"# Week 4\n* Statistical Properties of Data, Distributions\n* Univariate Analysis and Bivariate\/Multivariate Analysis\n* Outlier Analysis and treatment","db6b640a":"Most of them seems to have a linear relationship with the SalePrice","2f1c27f4":"<a> What we conclude here <\/a>\n* There's strong relation between overall quality of the houses and their sale prices.\n\n* Again above grade living area seems strong indicator for sale price.\n* Garage features, number of baths and rooms, how old the building is etc. also having effect on the price on various levels too.\n","7df03c24":"### Distinct\/Unique Value Counts - categorical","82c4f59d":"#### Shape of Curve\n* Uniform \u2013 In reality NA!!!! Very Rare\n* bell shaped curve\/normal distribution\n","83162be3":"##### Outlier \nAn outlier is an observation that lies \"far away\" from other values. \n\nLet\u2019s take an example, we do customer profiling and find out that the average annual income of customers is  0.8 million dollar. But, there are two customers having annual income of 4 million dollar and 4.2 million dollar. These two customers annual income is much higher than rest of the population. These two observations will be seen as Outliers.\n\n\nThey may be an indicator of data errors or a rare events, and should be investigated carefully\n\n\nWhat causes Outliers?\n* Data Entry Errors - Human errors such as errors caused during data collection, recording, or entry can cause outliers in data\n* Measurement Error - It is the most common source of outliers. This is caused when the measurement instrument used turns out to be faulty\n* Experimental Error - Another cause of outliers is experimental error. Time to take recording was incorrect.\n* Intentional Outlier - This is commonly found in self-reported measures that involves sensitive data.\n* Data Processing Error - Whenever we perform data mining, we extract data from multiple sources. It is possible that some manipulation or extraction errors may lead to outliers in the dataset.\n* Natural Outlier - When an outlier is not artificial (due to error), it is a natural outlier\n","c385486e":"##### IQR: Inter-Quartile Range \n\nInter-quartile range is the distance between the 75th and 25th percentile.","6fdb2569":"<hr\/>","4de8f147":"#### Detetecting missing - \nStandard Missing Values - isNull detects \nNon-Standard Missing Values - isNull doesnt detects example XX, 9999 etc...\n\nAn easy way to detect these various formats is to put them in a list. Then when we import the data, Pandas will recognize them right away. Here\u2019s an example of how we would do that\n\nMake a list of missing value types\n\nmissing_values = [\"9999\", \"XX\", \"--\"]\n\ndf = pd.read_csv(\"property data.csv\", na_values = missing_values)\n\nYou might not be able to catch all of these right away. As you work through the data and see other types of missing values, you can add them to the list.\n\nIt\u2019s important to recognize these non-standard types of missing values for purposes of summarizing and transforming missing values. ","3b208658":"# Engineering Missing Data in numerical variables\nfor var in continuous:\n    data[var].fillna(data[var].median(), inplace=True)","fd2894c4":"#### Median\n\nThe median is the middle value. \n\nIt is the value that splits the dataset in half. \n\nTo find the median, order your data from smallest to largest, and then find the data point that has an equal amount of values above it and below it. \n\nThe method for locating the median varies slightly depending on whether your dataset has an even or odd number of values.","a41156d9":"### Categorical -> Numerical\n\nWe have 44 categorical these at times not understood by systems, These need to be converted to numeric fields.\n\nThere are various ways to do, We can use replace function or map function to replace with numeric\nExample below","c52cad48":"# <a> EDA <\/a>\n\nExploratory Data Analysis (EDA). It is a step in which we need to explore the data set, know the data set before you pass it to next steps like Feature Engineering, Model Building and Model Evaluation using multiple Machine Learning algorithm.\n\nBy defination - Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns, to spot anomalies, to test hypothesis and to check assumptions with the help of summary statistics and graphical representations. \n\nThe main aim of exploratory data analysis is to obtain confidence in your data to an extent where you\u2019re ready to engage a machine learning algorithm. By doing this you can get to know whether the selected features are good enough to model, are all the features required, are there any correlations based on which we can either go back to the Data Preprocessing step or move on to modeling.\n\nIt is very important for a data scientist to be able to understand the nature of the data without making assumptions.\nthe goal is to try and answer all questions on the data.\n","e9836474":"Above graph\/plots shows - the distribution of SalePrice changes with the individual neighborhoods","64f27317":"##### Outlier  Treatment\n* Drop the outlier value\n* Transforming and binning values\n* Imputing - Replace the outlier value using the IQR\n","011500ba":"<hr\/>","4f4f5dfc":"# Engineering Missing Data in categorical variables\n# add label indicating 'Missing' to categorical variables\nfor var in categorical:\n    data[var].fillna('Missing', inplace=True)","7bc21d93":"Range (max, min) \u2013 Max Value of Data - Min Value of data","3f79a5d1":"## <a> Now, there is no  missing values<\/a>","513cc92d":"# Lets take example to encode one or two the categorical variable. Let it be MSZoning, Street\n# one-hot encoding of categorical variables using pd.get_dummies\nprint('Shape before hot encoding column', data.shape)\nStreetEncoded = pd.get_dummies(data['Street'], prefix='Street', prefix_sep='_')\nMSZoningEncoded = pd.get_dummies(data['MSZoning'], prefix='MSZoning', prefix_sep='_')\n# merge with main\ndata = data.join(StreetEncoded)\ndata = data.join(MSZoningEncoded)\nprint('Shape before hot encoding column', data.shape)","d42363c8":"The shape tells us a number of data records\/rows and variables\/features we have in the data set.\n* Size -  df.size => Return an int representing the number of elements in this object.\n* Shape - df.shape => Return a tuple representing the dimensionality of the DataFrame.\n\nThis helps you to know the dimensions of the data!!!!","4b0efeef":"# Final Thoughts\nEDA is undoubtedly\/unarguably the most crucial step before we go Feature Selection \/ PCA \/ Encoding etc and Machine Learning","72e17465":"#### Summary\ndata.info provides a concise summary of your DataFrame.\n","251617f8":"#### columns \/ features\ndata.columns gives you its names","2e53e431":"### Graphical Univariate Analysis","6c5ce85a":"Features such as 1stFlrSF, TotalBsmtSF, LotFrontage, GrLiveArea... seems to share a similar distribution to the one we have with SalePrice","086bfa8d":"#### scatter plot\nA common plot for visualizing the relationship between two numerical variables.\nUsing a scatterplot we can also detect Multivariate outliers.","985171ad":"# <a> Study Missing Feature Data <\/a>"}}