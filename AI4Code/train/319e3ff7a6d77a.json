{"cell_type":{"692fc496":"code","53f0ddc8":"code","20ae15e1":"code","cf3fbf59":"code","630fcec0":"code","294095ae":"code","f13f9fa4":"code","260b4f87":"code","d90f2031":"code","62a3130d":"code","8a8cb37d":"code","e5ca0768":"code","299f8e0e":"code","f86dc809":"code","61355d7e":"code","4dbc9955":"code","3d830b22":"code","88189e30":"code","0a72c452":"code","e119ba95":"code","c653ec47":"markdown","4809e24f":"markdown","251b5b59":"markdown","d8af4ee8":"markdown","e690c449":"markdown","1aa00fe2":"markdown","71d0afd1":"markdown","41b80083":"markdown","62ca7b2c":"markdown","2e7c38bc":"markdown","659baa35":"markdown"},"source":{"692fc496":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","53f0ddc8":"import pandas as pd\n\n## Train set\ntr = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\ntr.head(5)","20ae15e1":"## Validation data set\nval = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-severity-rating\/validation_data.csv')\nval.head(5)","cf3fbf59":"## Sample submission data set\nsub = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv')\nsub.head(5)\n","630fcec0":"import re\nimport string\ndef clean(d):\n    ## lowercase the reviews\n    d = d.apply(lambda x:x.lower())\n    ## remove punctuation marks\n    d = d.apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n    # Removing extra spaces\n    d = d.apply(lambda x: re.sub(' +',' ',x))\n    ## Remove line breaks\n    d = d.replace('\\n',' ').replace('\\r',' ').replace('...',' ')\n    # Remove special characters\n    d = d.apply(lambda x:re.sub('[^a-zA-z0-9\\s]','',x))\n\n    ## Look at the text after cleaning \n    d.head(5)","294095ae":"clean(tr['text'])","f13f9fa4":"from textblob import TextBlob\ntr['polarity'] = tr['text'].apply(lambda x: round(TextBlob(x).sentiment.polarity),2)\nprint(\" 3 Comments which are positive (highest polarity)\")\nfor index, t in enumerate(tr.iloc[tr['polarity'].sort_values(ascending = True)[:3].index]['text']):\n    print(index+1,t,'\\n')\n","260b4f87":"tr.head(5)","d90f2031":"tr['polarity'].value_counts()","62a3130d":"x = tr.drop('polarity',axis = 1)\ny = tr['polarity'].values\n\ntexts = x.drop('comment_id',axis = 1).copy()\ntexts.reset_index(inplace = True, drop = True)\ntexts.head()","8a8cb37d":"## Create a clean text corpus \n##containing lower case words + no stopwords + stem words\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nimport tqdm\n\nps = PorterStemmer()\ncorpus = []\n\nfor i in range(0, len(texts)) :\n    cleaned = re.sub('[^a-zA-Z]', ' ', texts['text'][i])\n    cleaned = cleaned.lower().split()\n    \n    cleaned = [ps.stem(word) for word in cleaned if not word in stopwords.words('english')]\n    cleaned = ' '.join(cleaned)\n    corpus.append(cleaned)","e5ca0768":"import gensim\n\nDIM = 100\n\nX = [d.split() for d in corpus]\nw2v_model = gensim.models.Word2Vec(sentences = X, vector_size = DIM, window = 10, min_count = 1)","299f8e0e":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nt = Tokenizer()\nt.fit_on_texts(X)\nX = t.texts_to_sequences(X)\nX = pad_sequences(X, maxlen = 16)\nprint('Text tokens count ',len(t.word_index))\n\nvocab_size = len(t.word_index) + 1 \nvocab = t.word_index","f86dc809":"X[:3]","61355d7e":"def get_weights_matrix(model) :\n    weights_matrix = np.zeros((vocab_size, DIM))\n    \n    for word, i in vocab.items() :\n        weights_matrix[i] = model.wv[word]\n        \n    return weights_matrix\n\n\nembedding_vectors = get_weights_matrix(w2v_model) ","4dbc9955":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\nfrom tensorflow.keras.losses import BinaryCrossentropy\n\nmodel = Sequential([\n    Embedding(vocab_size, output_dim =DIM,weights = [embedding_vectors],input_length = 16,name='embedding'),\n    Dropout(0.2),\n    LSTM(64),\n    Dropout(0.2),\n    Dense(64,activation = 'relu'),\n    Dropout(0.2),\n    Dense(1,activation = 'sigmoid')\n])\nmodel.compile(optimizer='adam',loss= 'mean_squared_error',\n             metrics = ['accuracy'])\nmodel.summary()","3d830b22":"from tensorflow.keras.utils import plot_model\n\nplot_model(model)","88189e30":"from keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom sklearn.model_selection import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size =0.3,random_state = 0)\n\nes = EarlyStopping(patience=3, \n                   monitor='loss', \n                   #restore_best_weights=True, \n                   mode='min', \n                   verbose=1)\n# model train\nhistory = model.fit(xtrain,ytrain,\n                    validation_data = (xtest, ytest)\n                    ,batch_size= 100, \n                    epochs= 1,\n                    validation_split=0.1,\n                    callbacks=[es],\n                    shuffle=True,\n                    )\n","0a72c452":"import matplotlib.pyplot as plt\npreds = model.predict(X)*1000\nplt.hist(preds,label='Model Prediction')\nplt.legend()","e119ba95":"from sklearn.preprocessing import binarize\nu = pd.DataFrame()\nu['comment_id'] = tr['comment_id']\nu['score'] = binarize(preds)\nu.to_csv('submission.csv',index=False)","c653ec47":"# Submission","4809e24f":"# Train the model\n\n","251b5b59":"# Data cleaning","d8af4ee8":"# Cleaning Text\n\n* Stemming: \n\n        Extract the root element of a word.\n\n        Ex: raining, rained, had rained \n\n        Stem word: rain\n        \n\n* Stop words removal\n\n        Stopwords are the most commonly occuring  words in the text carrying no meaning. Ex: 'I', 'This','on','there','here','is','in'.\n\n        We will use nltk library to remove stopwords from the cleaned train set.","e690c449":"Introducing the feature Polarity to score toxicity of comments.\n\nSentiment analysis is the analysis of how much a piece of text is positive and opinionated.\n","1aa00fe2":"## Build the deep learning model\n\nAn lstm model can remember, learn and memorise sequences of padded vectors.","71d0afd1":"# Prediction","41b80083":"# Load dataset","62ca7b2c":"# Feature engineering","2e7c38bc":"# Text Tokenization\n\nTokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\n\nWe can pad the data to have same length","659baa35":"## Word2Vec\n\nConvert cleaned text into numbers using gensim"}}