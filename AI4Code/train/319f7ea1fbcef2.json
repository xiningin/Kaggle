{"cell_type":{"c62d74f6":"code","249ed044":"code","937b9eab":"code","3f4280bb":"code","8b97bcf1":"code","1b19174a":"code","db6726e7":"code","5c6cea15":"code","b9572540":"code","ce8ddd09":"code","0f395b1e":"code","d9814a31":"code","a3d27f8e":"code","97a0fb18":"code","7f3a18dd":"code","43baba59":"code","2000c175":"code","129b7140":"code","2a956a65":"code","80f899b0":"code","4837229f":"code","2850f953":"markdown","2d4c2177":"markdown","2da155b0":"markdown","70ed0504":"markdown","15bcb75f":"markdown","44e52a61":"markdown"},"source":{"c62d74f6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re\n\nfrom skmultilearn.problem_transform import ClassifierChain\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nfrom collections import defaultdict\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nimport gc\n\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","249ed044":"train_df = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv')\nval_severity_df = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-severity-rating\/validation_data.csv')\ntest_df = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")","937b9eab":"data = train_df.iloc[:,2:]\n\ncolormap = plt.cm.plasma\nplt.figure(figsize=(7,7))\nplt.title('Correlation of features & targets',y=1.05,size=14)\nsns.heatmap(data.astype(float).corr(),linewidths=0.1,vmax=1.0,square=True,cmap=colormap,\n           linecolor='white',annot=True)","3f4280bb":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub('\\W', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip(' ')\n    return text","8b97bcf1":"train_df[\"comment_text\"] = train_df[\"comment_text\"].apply(lambda x: clean_text(x))","1b19174a":"import re, string\n\n\n# import and instantiate TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features = 5000, ngram_range=(1,2), stop_words=\"english\")\nvect","db6726e7":"X_vec = vect.fit_transform(train_df[\"comment_text\"])","5c6cea15":"X_vec.shape","b9572540":"target_labels = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]","ce8ddd09":"def predict_val_set(model, vectorizer,df_val , process_text  = clean_text):\n    df_val[\"less_toxic\"] = df_val[\"less_toxic\"].apply(lambda x: process_text(x))\n    df_val[\"more_toxic\"] = df_val[\"more_toxic\"].apply(lambda x: process_text(x))\n    less_toxic_vec = vectorizer.transform(df_val[\"less_toxic\"])\n    more_toxic_vec = vectorizer.transform(df_val[\"more_toxic\"])\n    \n    less_toxic_scores = model.predict_proba(less_toxic_vec)\n    more_toxic_scores = model.predict_proba(more_toxic_vec)\n    \n    return (less_toxic_scores, more_toxic_scores)\n\n\ndef predict_test_set(model, vectorizer,df_test , process_text  = clean_text):\n    df_test[\"text\"] = df_test[\"text\"].apply(lambda x: process_text(x))\n    toxic_vec = vectorizer.transform(df_test[\"text\"])\n    \n    toxic_scores = model.predict_proba(toxic_vec)\n    \n    return (toxic_scores)","0f395b1e":"\n\ndef binary_relevance(model,model_name, X_vec, train_df, val_df, df_test, vectorizer):\n    target_labels = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n    less_toxic_scores = defaultdict(list)\n    more_toxic_scores = defaultdict(list)\n    test_toxic_scores = defaultdict(list)\n#     training_accuracies =[]\n    for label in target_labels:\n#         print('... Processing {}'.format(label))\n        y = train_df[label].values\n        # train the model using X_dtm & y\n        model.fit(X_vec, y)\n        # compute the training accuracy\n#         y_pred_X = model.predict(X_vec)\n#         print('Training accuracy is {}'.format(accuracy_score(y, y_pred_X)))\n        # predict on val_set\n        lt_s, mt_s ,  = predict_val_set(model = model, vectorizer = vectorizer ,df_val = val_df , process_text  = clean_text)\n        test_s = predict_test_set(model = model, vectorizer = vectorizer ,df_test = df_test , process_text  = clean_text)\n        less_toxic_scores[label] = lt_s[:,1]\n        more_toxic_scores[label] = mt_s[:,1]\n        test_toxic_scores[label] = test_s[:,1]\n    br_less_toxic_final_score = pd.DataFrame(less_toxic_scores).sum(axis=1)\n    br_more_toxic_final_score = pd.DataFrame(more_toxic_scores).sum(axis=1)\n    br_test_toxic_final_score = pd.DataFrame(test_toxic_scores).sum(axis=1)\n\n    val_accuracy = (br_less_toxic_final_score< br_more_toxic_final_score).mean()*100\n    print(\"BR validation accuracy for model {}  is {}\".format(model_name,val_accuracy))\n     \n    \n    del less_toxic_scores, more_toxic_scores, val_accuracy, lt_s, mt_s, model\n    _= gc.collect()\n    return br_less_toxic_final_score, br_more_toxic_final_score, br_test_toxic_final_score\n\n\n\n    ","d9814a31":"def classifier_chains(model,model_name, X_vec, train_df, val_df,df_test,  vectorizer):\n    print(\"running_classifier_chain\")\n\n    target_labels = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n    y = train_df[target_labels]\n    classifier = ClassifierChain(model)\n\n    classifier.fit(X_vec, y)\n    y_pred_X = classifier.predict(X_vec)\n    print('CC Training accuracy for model {} is {}'.format(model_name, accuracy_score(y, y_pred_X)))\n\n    less_toxic_scores, more_toxic_scores = predict_val_set(model = classifier, vectorizer = vectorizer ,df_val = val_df , process_text  = clean_text)\n    test_toxic_scores = predict_test_set(model = classifier, vectorizer = vectorizer,df_test= df_test , process_text  = clean_text)\n    \n    classifier_chain_less_toxic_final_score = pd.DataFrame(less_toxic_scores.toarray()).sum(axis=1)\n    classifier_chain_more_toxic_final_score = pd.DataFrame(more_toxic_scores.toarray()).sum(axis=1)\n    classifier_chain_test_toxic_final_score = pd.DataFrame(test_toxic_scores.toarray()).sum(axis=1)\n\n    val_accuracy = (classifier_chain_less_toxic_final_score< classifier_chain_more_toxic_final_score).mean()*100\n    print(\"CC validation accuracy for model {} is {}\".format(model_name, val_accuracy))\n    \n    del less_toxic_scores, more_toxic_scores, val_accuracy, classifier\n    gc.collect()\n    \n    return classifier_chain_less_toxic_final_score, classifier_chain_more_toxic_final_score, classifier_chain_test_toxic_final_score","a3d27f8e":"def combined_score(model,model_name, X_vec, train_df, val_df,df_test, vectorizer):\n \n    br_less_toxic_final_score, br_more_toxic_final_score,br_test_toxic_final_score  = binary_relevance(model,model_name, X_vec, train_df, val_df, df_test, vectorizer)\n    classifier_chain_less_toxic_final_score, classifier_chain_more_toxic_final_score, classifier_chain_test_toxic_final_score = classifier_chains(model,model_name, X_vec, train_df, val_df,df_test, vectorizer)\n    \n    combined_less_toxic_score = br_less_toxic_final_score + classifier_chain_less_toxic_final_score\n    combined_more_toxic_score = br_more_toxic_final_score + classifier_chain_more_toxic_final_score\n    combined_test_toxic_score = br_test_toxic_final_score + classifier_chain_test_toxic_final_score\n\n    val_accuracy = (combined_less_toxic_score< combined_more_toxic_score).mean()*100\n    print(\"Combined validation accuracy for model{} is {}\".format(model_name, val_accuracy))\n    \n    del br_less_toxic_final_score, br_more_toxic_final_score, classifier_chain_less_toxic_final_score, classifier_chain_more_toxic_final_score\n    gc.collect()\n    \n    return combined_less_toxic_score, combined_more_toxic_score, combined_test_toxic_score","97a0fb18":"results = pd.DataFrame()\nresults_test = pd.DataFrame()\nmodels_to_train ={}\n\n# models_to_train[\"SVM\"] = SVC(probability=True)\n# models_to_train[\"GaussianNB\"] = GaussianNB()\nmodels_to_train[\"LogisticRegression\"] = LogisticRegression(C= 0.1, solver = \"liblinear\")\n\nfor model_name, model in models_to_train.items():\n    print(\"---Training \" + model_name + \"-----\")\n    combined_less_toxic_score, combined_more_toxic_score, combined_test_toxic_score = combined_score(model,model_name, X_vec, train_df = train_df, val_df =val_severity_df , df_test = test_df, vectorizer = vect)\n    results[model_name + \"_less_toxic_score\"] = combined_less_toxic_score\n    results[model_name + \"_more_toxic_score\"]= combined_more_toxic_score\n    results_test[model_name + \"_test_toxic_score\"] = combined_test_toxic_score\n","7f3a18dd":"def nbsvm_model(train_df, val_df, df_test, process_text= clean_text):\n    \n    re_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\n    def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n    vectorizer = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1)\n    X_vec = vectorizer.fit_transform(train_df[\"comment_text\"])\n\n    def pr(y_i, y,x):\n        p = x[y==y_i].sum(0)\n        return (p+1) \/ ((y==y_i).sum()+1)\n\n\n    def get_mdl(y,x):\n        y = y.values\n        r = np.log(pr(1,y,x) \/ pr(0,y,x))\n        m = LogisticRegression(C=4, dual=True, solver=\"liblinear\")\n        x_nb = x.multiply(r)\n        return m.fit(x_nb, y), r\n    \n    target_labels = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n    \n    val_df[\"less_toxic\"] = val_df[\"less_toxic\"].apply(lambda x: process_text(x))\n    val_df[\"more_toxic\"] = val_df[\"more_toxic\"].apply(lambda x: process_text(x))\n    df_test[\"text\"] = df_test[\"text\"].apply(lambda x: process_text(x))\n    less_toxic_vec = vectorizer.transform(val_df[\"less_toxic\"])\n    more_toxic_vec = vectorizer.transform(val_df[\"more_toxic\"])\n    test_toxic_vec = vectorizer.transform(df_test[\"text\"])\n    \n    less_toxic_scores = defaultdict(list)\n    more_toxic_scores = defaultdict(list)\n    test_toxic_scores = defaultdict(list)\n    \n    for i, j in enumerate(target_labels):\n        print('fit', j)\n        m,r = get_mdl(train_df[j], X_vec)\n        \n        less_toxic_scores[j] = m.predict_proba(less_toxic_vec.multiply(r))[:,1]\n        more_toxic_scores[j]= m.predict_proba(more_toxic_vec.multiply(r))[:,1]\n        test_toxic_scores[j]= m.predict_proba(test_toxic_vec.multiply(r))[:,1]\n\n    nbsvm_less_toxic_final_score = pd.DataFrame(less_toxic_scores).sum(axis=1)\n    nbsvm_more_toxic_final_score = pd.DataFrame(more_toxic_scores).sum(axis=1)\n    nbsvm_test_toxic_final_score = pd.DataFrame(test_toxic_scores).sum(axis=1)\n\n    val_accuracy = (nbsvm_less_toxic_final_score< nbsvm_more_toxic_final_score).mean()*100\n    print(\"BR validation accuracy for NBSVM is {}\".format(val_accuracy))\n    \n    del less_toxic_scores, more_toxic_scores, val_accuracy\n    _= gc.collect()\n    return nbsvm_less_toxic_final_score, nbsvm_more_toxic_final_score,nbsvm_test_toxic_final_score\n    \n    \n    \n    ","43baba59":"results[\"NBsvm_less_toxic_score\"], results[\"NBsvm_more_toxic_score\"], results_test[\"NBsvm_test_toxic_score\"] =  nbsvm_model(train_df, val_severity_df, test_df)\n\n","2000c175":"results.head()","129b7140":"### Combined Validation Score\n(results.iloc[:,[0,2]].sum(axis=1) < results.iloc[:,[1,3]].sum(axis=1)).mean()","2a956a65":"# results_test[\"score\"] = results_test.sum(axis=1)\n# results_test","80f899b0":"# submission = pd.DataFrame(zip(test_df[\"comment_id\"], results_test[\"score\"]), columns=[\"comment_id\",\"score\"])\n# submission.to_csv(\"submission.csv\", index=False)","4837229f":"### only nb-svm\nscore = results_test[\"NBsvm_test_toxic_score\"]\nsubmission = pd.DataFrame(zip(test_df[\"comment_id\"], score), columns=[\"comment_id\",\"score\"])\nsubmission.to_csv(\"submission.csv\", index=False)","2850f953":"## Combined_score","2d4c2177":"### Submit Predictions","2da155b0":"# Using Multi-label Classification probabilities  (Logistic Regression + NB-SVM)\n\n This notebook uses the dataset from Jigsaw_toxic_comment_classification for training multi-label classifiers using techniques such as Binaey Relevance & Classifier Chains.\n For validation and test set predictions, the probability outputs of different classes are summed up to get the overall toxicity score which is used for submission.\n\nFollowing two awesome notebooks from the Jigsaw_toxic_comment_classification are referred for this work, with minor adjustments for this competition.\n-   _- Classifying multi-label comments_, https:\/\/www.kaggle.com\/rhodiumbeng\/classifying-multi-label-comments-0-9741-lb\n-   _- NB-SVM strong linear baseline_, https:\/\/www.kaggle.com\/jhoward\/nb-svm-strong-linear-baseline\/comments\n            ","70ed0504":"### Binary Relevance","15bcb75f":"### NaiveBayes- SVM Model","44e52a61":"### Classifier Chains"}}