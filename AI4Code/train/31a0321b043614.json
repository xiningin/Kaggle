{"cell_type":{"4ec5218b":"code","02ad15c6":"code","16270ed9":"code","ef5ea651":"code","cb3baf6e":"code","e950719f":"code","295ffa04":"code","b5d1a5f7":"code","defe86c2":"code","8ed6ce5e":"code","4ec83923":"code","988c0b81":"code","e0dda0d8":"code","f489c3ac":"code","4e97d932":"code","3c3ca2ce":"code","e51db033":"code","3692f2a0":"code","e8ffac86":"code","066ed651":"code","7fcd9725":"code","9d3ff401":"code","11a5dc12":"code","41620c30":"code","844cc818":"code","be7eec4f":"code","a587ad6c":"code","f5ab42d5":"code","fbcb583d":"code","bb3c09ae":"code","ce64f325":"code","86ac21b1":"code","30d16cea":"code","95b8df26":"code","33e24caa":"code","80d0d511":"code","b3d3a276":"code","5da31f4f":"code","5a8241de":"code","ed61aba1":"code","e0f0683c":"code","da7e4ce4":"code","09eac26a":"code","4c65988c":"code","66af0cc4":"code","3a899227":"code","e8dc889c":"code","1b0dc075":"code","c4935c8c":"code","d4ef7562":"markdown","d5d8e1dc":"markdown","5f6a90db":"markdown","cfaab2b4":"markdown","0055dabb":"markdown","7442be7a":"markdown","0b3d80bd":"markdown","47679a3e":"markdown","a90e5187":"markdown","71d91e5d":"markdown","1a6523db":"markdown","5d7dfa20":"markdown","1430f991":"markdown","efe4c6ff":"markdown","cb8cefab":"markdown","13582311":"markdown","60167be8":"markdown","33ca9372":"markdown","77e90b60":"markdown","06b8655f":"markdown","0a76e1aa":"markdown","9f80a95d":"markdown","4b2c5168":"markdown","5da3caae":"markdown","a1456c81":"markdown","5f2cac7f":"markdown","41c7e715":"markdown","4755da2b":"markdown","8fd42af2":"markdown","5e4b8134":"markdown","2fa2b2ea":"markdown","dfc37dc6":"markdown","b68c94f0":"markdown"},"source":{"4ec5218b":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split","02ad15c6":"import warnings\nwarnings.filterwarnings('ignore')","16270ed9":"# load datasets from csv files\nx = 2015\ndfs = []\nwhile True:\n    globals()[f'df{x}'] = pd.read_csv(f'\/kaggle\/input\/world-happiness\/{x}.csv')\n    dfs.append(globals()[f'df{x}'])\n    x += 1\n    if x == 2020:\n        break","ef5ea651":"# show first few records for each dataset\nfor i, df in enumerate(dfs):\n    print (f'201{i+5} dataset:')\n    display (dfs[i].head(3))","cb3baf6e":"# show the number of records and columns for each dataset\nfor i, df in enumerate(dfs):\n    print (f'Size of 201{i+5} Report:', dfs[i].shape)","e950719f":"# show column names for each dataset\nfor i, df in enumerate(dfs):\n    print (f'Column names for 201{i+5} dataset:', dfs[i].columns, '\\n')","295ffa04":"df2015.rename(columns = {'Economy (GDP per Capita)' : 'GDP',\n                        'Health (Life Expectancy)' : 'Life',\n                        'Trust (Government Corruption)' : 'Trust'}, inplace = True)","b5d1a5f7":"df2016.rename(columns = {'Economy (GDP per Capita)' : 'GDP',\n                        'Health (Life Expectancy)' : 'Life',\n                        'Trust (Government Corruption)' : 'Trust'}, inplace = True)","defe86c2":"df2017.rename(columns = {'Happiness.Rank' : 'Happiness Rank',\n                        'Happiness.Score' : 'Happiness Score',\n                        'Economy..GDP.per.Capita.' : 'GDP',\n                        'Health..Life.Expectancy.' : 'Life',\n                        'Dystopia.Residual' : 'Dystopia Residual',\n                        'Trust..Government.Corruption.' : 'Trust'}, inplace = True)","8ed6ce5e":"df2018.rename(columns = {'Overall rank' : 'Happiness Rank',\n                        'Score' : 'Happiness Score',\n                        'Country or region' : 'Country',\n                        'Social support' : 'Family',\n                        'Freedom to make life choices' : 'Freedom',\n                        'GDP per capita' : 'GDP',\n                        'Healthy life expectancy' : 'Life',\n                        'Perceptions of corruption' : 'Trust'}, inplace = True)","4ec83923":"df2019.rename(columns = {'Overall rank' : 'Happiness Rank',\n                        'Score' : 'Happiness Score',\n                        'Country or region' : 'Country',\n                        'Social support' : 'Family',\n                        'Freedom to make life choices' : 'Freedom',\n                        'GDP per capita' : 'GDP',\n                        'Healthy life expectancy' : 'Life',\n                        'Perceptions of corruption' : 'Trust'}, inplace = True)","988c0b81":"# add year column for each dataset\nfor i, df in enumerate(dfs, 2015):\n    df['Year'] = i","e0dda0d8":"# add \"Region\" column\nfor df in dfs:\n    if not ('Region') in df:\n        df['Region'] = None\n        temp = df.set_index('Country').Region.fillna(df2015.set_index('Country').Region).reset_index()\n        df.fillna(temp, inplace = True)","f489c3ac":"df2016['Standard Error'] = round((df2016['Upper Confidence Interval'] - df2016['Lower Confidence Interval']) \/ 2, 3)","4e97d932":"df2017['Standard Error'] = round((df2017['Whisker.high'] - df2017['Whisker.low']) \/ 2, 3)","3c3ca2ce":"temp = pd.merge(df2015[['Country', 'Standard Error']], df2016[['Country', 'Standard Error']], on = 'Country')\ntemp.rename(columns = {'Standard Error_x' : 'Standard Error 2015',\n                        'Standard Error_y' : 'Standard Error 2016'}, inplace = True)\nstandard_error_df = pd.merge(temp, df2017[['Country', 'Standard Error']], on = 'Country')\nstandard_error_df.rename(columns = {'Standard Error' : 'Standard Error 2017'}, inplace = True)\nstandard_error_df.head(3)","e51db033":"# claculate Standard Error values for df2018\nstandard_error_df['Standard Error 2018'] = round(standard_error_df.mean(axis = 1), 4)\nstandard_error_df.head(3)","3692f2a0":"# claculate Standard Error values for 2019 dataset \nstandard_error_df['Standard Error 2019'] = round(standard_error_df.mean(axis = 1), 3)\nstandard_error_df.head(3)","e8ffac86":"dfs[3] = pd.merge(dfs[3], standard_error_df[['Country','Standard Error 2018']], on = 'Country')\ndfs[3].rename(columns = {'Standard Error 2018' : 'Standard Error'}, inplace = True)\ndfs[4] = pd.merge(dfs[4], standard_error_df[['Country','Standard Error 2019']], on = 'Country')\ndfs[4].rename(columns = {'Standard Error 2019' : 'Standard Error'}, inplace = True)","066ed651":"for i, df in enumerate(dfs, 2015):\n    # drop \"Dystopia Residual\" columns\n    if 'Dystopia Residual' in df:\n        df.drop(['Dystopia Residual'], inplace = True, axis = 1)\n    # drop \"Confidence Interval\" columns\n    if ('Lower Confidence Interval' and 'Upper Confidence Interval') in df:\n        df.drop(['Lower Confidence Interval', 'Upper Confidence Interval'], inplace = True, axis = 1)\n    # drop \"Whisker\" columns\n    if ('Whisker.high' and 'Whisker.low') in df:\n        df.drop(['Whisker.high', 'Whisker.low'], inplace = True, axis = 1)","7fcd9725":"# check missing values\nfor i, df in enumerate(dfs, 2015):\n    print ('\\n' f'df{i} dataset:' '\\n', df.isnull().sum())","9d3ff401":"# show records with missing values in df2017 \ndfs[2][dfs[2].isnull().any(axis = 1)]","11a5dc12":"# fill missing values in df2017 manually\ndfs[2].loc[32, ['Region']] = 'Eastern Asia'\ndfs[2].loc[49, ['Region']] = 'Latin America and Caribbean'\ndfs[2].loc[70, ['Region']] = 'Eastern Asia'\ndfs[2].loc[92, ['Region']] = 'Sub-Saharan Africa'\ndfs[2].loc[110, ['Region']] = 'Sub-Saharan Africa'\ndfs[2].loc[146, ['Region']] = 'Sub-Saharan Africa'","41620c30":"# show records with missing values in df2018\ndfs[3][dfs[3].isnull().any(axis = 1)]","844cc818":"# fill Trust missing values in df2018 by the mean value of the previous years\nprevious_trust_uae = [dfs[0][dfs[0]['Country'] == 'United Arab Emirates']['Trust'].item(), dfs[1][dfs[1]['Country'] == 'United Arab Emirates']['Trust'].item(), dfs[2][dfs[2]['Country'] == 'United Arab Emirates']['Trust'].item()]\ndfs[3].loc[19, ['Trust']] = sum(previous_trust_uae) \/ len(previous_trust_uae)","be7eec4f":"# combine all datasets into a single dataframe\ngiant_df = pd.concat(dfs)","a587ad6c":"# create a new dataframe for the sake of visualisation\neda_df = giant_df","f5ab42d5":"import plotly.figure_factory as ff\nz = pd.DataFrame(eda_df.corr().values.tolist())\nz = z.round(2).values.tolist()\nfig = ff.create_annotated_heatmap(z, x = eda_df.corr().columns.tolist(), y = eda_df.corr().columns.tolist(), colorscale = 'Portland')\nfig.update_layout(title = {'text': 'Correlation Heatmap', 'y' : 0.93, 'x' : 0.5}, title_font_size = 25)\nfig.show()","fbcb583d":"import plotly.express as px\nhappiest_countries = eda_df.groupby(['Country'], sort = False)['Happiness Score', 'Year', 'GDP'].max()\ntop10 = happiest_countries.sort_values('Happiness Score', ascending = False)[:15]\nfig = px.scatter(top10,\n                x = top10.index,\n                y = 'Happiness Score',\n                size = 'GDP',\n                color = top10.index,\n                template = 'xgridoff',\n                animation_frame = 'Year',\n                title = 'The Top 10 Happiest Countries in The World <br> (Bubble Size Indicates GDP)')\nfig.show()","bb3c09ae":"eda_df['Continent'] = ['Asia' if (i == 'Eastern Asia' or i == 'Southern Asia' or i == 'Eastern Asia')\n                          else 'Europe' if (i == 'Western Europe' or i == 'Central and Eastern Europe')\n                          else 'Middle East' if (i == 'Middle East and Northern Africa')\n                          else 'Africa' if (i == 'Sub-Saharan Africa')\n                          else 'Australia' if (i == 'Australia and New Zealand')\n                          else 'North America' if (i == 'North America')\n                          else 'Latin America'\n                          for i in giant_df['Region']]\nfig = px.box(eda_df,\n             x = 'Year',\n             y = 'Happiness Score',\n             color = 'Continent',\n             template = 'xgridoff',\n             labels = {'Continent': 'Region'},\n             title = 'Happiness Score by Regions from 2015-2017')\nfig.show()","ce64f325":"eda_df['Happiness Change'] = (df2019['Happiness Score'] - df2015['Happiness Score']) \/ df2015['Happiness Score']\n# show countries with at least 1% change\ntemp = eda_df[np.abs(eda_df['Happiness Change']) > 0.01]\ntemp = eda_df.sort_values('Happiness Change')\ntemp['Year'] = temp['Year'].astype(str)\nfig = px.bar(temp,\n             x = 'Happiness Change',\n             y = 'Country',\n             color = 'Year',\n             orientation = 'h',\n             height = 900,\n#              width = 700,\n             template = 'gridon',\n             title = 'Change in Happiness Score from 2015-2017')\nfig.show()","86ac21b1":"fig = px.scatter(eda_df,\n                x = 'GDP',\n                y = 'Happiness Score',\n                size = 'Trust',\n                color = 'Country',\n                template = 'xgridoff',\n                animation_frame = 'Year',\n                title = 'GDP vs Happiness Score from 2015-2017 <br> (Bubble Size Indicates Trust)')\nfig.show()","30d16cea":"fig = px.scatter(eda_df,\n                x = 'Life',\n                y = 'Happiness Score',\n                size = 'GDP',\n                color = 'Country',\n                template = 'xgridoff',\n                animation_frame = 'Year',\n                labels = {'Life': 'Life Expectancy'},\n                title = 'Life Expectancy vs Happiness Score for Each Country from 2015-2017 <br> (Bubble Size Indicates GDP)')\nfig.show()","95b8df26":"fig = px.scatter(eda_df,\n                x = 'Family',\n                y = 'Happiness Score',\n                size = 'GDP',\n                color = 'Country',\n                template = 'xgridoff',\n                animation_frame = 'Year',\n                labels = {'Family': 'Family Support'},\n                title = 'Family Support vs Happiness Score from 2015-2017 <br> (Bubble Size Indicates GDP)')\nfig.show()","33e24caa":"fig = px.scatter(eda_df,\n                x = 'Freedom',\n                y = 'Happiness Score',\n                size = 'GDP',\n                color = 'Country',\n                template = 'xgridoff',\n                animation_frame = 'Year',\n                title = 'Freedom vs Happiness Score for Each Country from 2015-2017 <br> (Bubble Size Indicates GDP)')\nfig.show()","80d0d511":"# encode categorical variables in order to prepare them for modelling\nle = preprocessing.LabelEncoder()\ngiant_df['Region'] = le.fit_transform(giant_df['Region'])\ngiant_df['Country'] = le.fit_transform(giant_df['Country'])","b3d3a276":"# define the predictors\nfeatures = ['Country', 'Region', 'Happiness Rank', 'Standard Error', 'GDP', 'Family', 'Life', 'Freedom', 'Trust', 'Generosity', 'Year']\nX = giant_df[features]\n# define the target\ny = giant_df['Happiness Score']","5da31f4f":"# split into the two subsets using random selection (67-33 policy)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)","5a8241de":"# create linear regression object\nlr = LinearRegression()\n# train the model using the training set\nlr.fit(X_train, y_train)\nlr.score(X_train, y_train)","ed61aba1":"# how good is our model?\nprint('Coefficient:', lr.score(X_train, y_train))\nprint('Intercept:', lr.intercept_)\nprint('Slope:', lr.coef_)","e0f0683c":"coefficients = zip(X.columns, lr.coef_)\ncoefficients = pd.DataFrame(list(zip(X.columns, lr.coef_)), columns = ['Features', 'Coefficients'])\ncoefficients.sort_values('Coefficients', ascending = False)","da7e4ce4":"# make predictions using testset\ny_pred = lr.predict(X_test)","09eac26a":"pred = pd.DataFrame({'Actual': y_test.tolist(), 'Predicted': y_pred.tolist()}).head(25)\npred.head(10)","4c65988c":"plt.style.use(style = 'fivethirtyeight')\nplt.rcParams['figure.figsize'] = (10, 6)\nplt.scatter(y_test, y_pred, alpha = 0.7, color = 'r')\nm, b = np.polyfit(y_pred, y_test, 1)\nplt.plot(y_pred, (m * y_pred + b), color = 'g')\nplt.xlabel('Actual Score')\nplt.ylabel('Predicted Score')\nplt.title('Happiness Score')","66af0cc4":"import xgboost as xgb\nxgb = xgb.XGBRegressor(objective = 'reg:squarederror', n_estimators = 100, max_depth = 3, learning_rate = 0.1)","3a899227":"xgb.fit(X_train,y_train)","e8dc889c":"xgb.score(X_train, y_train)","1b0dc075":"y_preds = xgb.predict(X_test)","c4935c8c":"rmse = np.sqrt(mean_squared_error(y_test, y_preds))\nprint(xgb.score(X_test, y_test))","d4ef7562":"As we still have some missing values in \"Region\" columns (countries that have not represented in 2015 reports), we need to fill these records manually.","d5d8e1dc":"Since the datasets have a bit of a different naming convention we need to abstract them to a common name.","5f6a90db":"Since each row in these datasets represents a country, it's obvious that the number of countries covered in each report is different which means there no data available from some countries in some years!","cfaab2b4":"To o answer this question, we need to calculate the change in happiness score for each country in the period 2015 to 2019.","0055dabb":"# World Happiness Report 2015-2019 (EDA + Visualisation + Prediction)","7442be7a":"# How Countries' Happiness Has Changed from 2015-2017?","0b3d80bd":"Now we need to calculate \"Standard Error\" column for both 2016-2017 datasets based on Confidence Interval and Whisker values.","47679a3e":"Creating \"Year\" column for each dataset.","a90e5187":"# XGBoost","71d91e5d":"# Linear Regression","1a6523db":"There are many possibilities of regressors to use. A particularly simple one is \"LinearRegression\", it's basically a wrapper around an ordinary least squares calculation.","5d7dfa20":"So some datasets have different labels for the same columns (i.e. \"Social Support\" column is labeled as \"Family\" in latest reports), there are some changes in how data is represented between latest reports and their earlier counterparts (i.e. \"Standard Error\" column has defined in a different way for 2016-2017 reports than 2015 report (upper\/lower & high\/low values), some datasets have data that others do not have (i.e. \"Dystopia Residual\" column does not exist in both 2018-2019 reports). To make data tidy, here are several things need to be addressed in the data cleaning process:\n\n\n1. Unifying column names\n1. Creating \"Year\" column for each dataset\n1. Adding \"Region\" column for both 2017-2019 reports\n1. Adding \"Standard Error\" column for both 2016-2017 datasets based on Confidence Interval and Whisker values\n1. Adding \"Standard Error\" column for both 2017-2019 datasets based on the average value of the previous years for each country\n1. Handling missing values\n1. Merge all datasets into one giant dataset\n1. Dropping \"Dystopia Residual\" column as it's not represented in 2018\/2019 datasets","1430f991":"# Data Cleaning","efe4c6ff":"# Modelling","cb8cefab":"Adding Region column for both 2018-2019 datasets.","13582311":"This notebook shows some exploratory analysis and visualisation analysis for the World Happiness Reports from the years 2015 until 2019, then applies a Multiple Linear Regression model to predict country's Happiness Score and determine which factors are influence this score.","60167be8":"Taking a look at how the Happiness Score relates to each other variable in the dataset!","33ca9372":"Then adding \"Standard Error\" column for both 2017-2019 datasets by calculating the average value of the previous years for each country. To do so, we need to create a temporary dataframe that combine the \"Standard Error\" columns for previous years in order to calculate the \"Standard Error\" values for 2018-2019 datasets.","77e90b60":"# Family Support vs Happiness Score for Each Country from 2015-2017","06b8655f":"So the Happiness Score is highly correlated with GDP, Family Support and Life expectancy, and least correlated with Generosity.","0a76e1aa":"## Features Explanation\n\n**Happiness Rank:** Rank of any country in a particular year.<br>\n**Country:** Name of the country.<br>\n**Standard Error:** The standard error of the happiness score.<br>\n**Happiness Score:** Happiness score as the sum of all numerical columns in the datasets.<br>\n**Economy (GDP per Capita):** The extent to which GDP contributes to the calculation of the Happiness Score.<br>\n**Trust:** A quantification of the people\u2019s perceived trust in their governments.<br>\n**Health (Life Expectancy):** The extent to which Life expectancy contributed to the calculation of the Happiness Score.<br>\n**Generosity:** Numerical value estimated based on the perception of Generosity experienced by poll takers in their country.<br>\n**Family Support:** Metric estimating satisfaction of people with their friends and family.<br>\n**Freedom:** Perception of freedom quantified.<br>\n**Dystopia:** Hypothetically the saddest country in the world.<br>\n**Lower Confidence Interval:** Lower Confidence Interval of the Happiness Score.<br>\n**Upper Confidence Interval:** Upper Confidence Interval of the Happiness Score.<br>\n","9f80a95d":"# Comparing Happiness Scores Across Regions","4b2c5168":"# Data Exploration","5da3caae":"Next step is to split the dataset into train and test sets for unbiased evaluation of the final model where the dependent variable is \"Happiness Score\".","a1456c81":"Now it's time to use scikit-learn to perform a simple linear regression and XGBoost (Gradient booster) to predict Happiness Score.<br>\nFirst, categorical variables need to be encoded for the model, this can be done by using LabelEncoder class.","5f2cac7f":"# Relation of Freedom to Happiness Score?","41c7e715":"# How Is Life Expectancy Related to Happiness Score?","4755da2b":"Now that we have created our model and trained it, it's time we test the model with our testing set.","8fd42af2":"Droping columns that are not common to all five reports.","5e4b8134":"# What Are the 10 Happiest Countries in the World?","2fa2b2ea":"As we standardised the structure of the five datasets, last step is to combine them all into a single giant dataframe.","dfc37dc6":"# Data Visualisation","b68c94f0":"# Does Money Buy Happiness?"}}