{"cell_type":{"6b434a21":"code","abd3ff9a":"code","36018f20":"code","1e814f3d":"code","d6bf62ac":"code","bbbee8f7":"code","bd3875c8":"code","9ba9569a":"code","0c445904":"code","cfaec6fe":"code","75d3aef5":"code","d970c2eb":"code","08c0ace0":"code","ba722e9d":"code","7e987871":"code","d903ac60":"code","b4d9d624":"code","b68a607c":"code","84cb1b2f":"code","e1a62b1c":"code","c2e7c497":"code","ddfd9900":"code","9063ddda":"code","fc1db7f4":"code","f8df4b7e":"code","79213c03":"code","9679a6a0":"code","8f8bf726":"code","1448e60e":"code","e564a608":"code","7215e465":"code","20606452":"code","b4eafeee":"code","08c863a4":"code","0c1cd090":"code","2b580fbb":"code","036e0ac5":"code","f53c9036":"markdown","44acf3be":"markdown","1fcf2a07":"markdown","53cfce61":"markdown","7184a54f":"markdown","65d08d7d":"markdown","afef01c2":"markdown","0fb42b3f":"markdown","36c61be7":"markdown","ef23b028":"markdown","9f241ebd":"markdown","7da58906":"markdown","f78e63b2":"markdown","38ed4f4b":"markdown","33410ffe":"markdown","538a98e7":"markdown","cd9d926c":"markdown","0237930e":"markdown","fb8d353f":"markdown","7bd2562e":"markdown","4bf2ed4f":"markdown","70bb7707":"markdown","ed72be96":"markdown","2069f0ae":"markdown","4856e8a5":"markdown","d4c92b6f":"markdown","758afaf8":"markdown","3bce851b":"markdown","bfd86c3b":"markdown","5e4faf6b":"markdown"},"source":{"6b434a21":"import pandas as pd\nfull_data = pd.read_csv('\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\nfull_data.head()","abd3ff9a":"full_data.shape","36018f20":"full_data.info()","1e814f3d":"full_data['RainToday'].replace({'No': 0, 'Yes': 1},inplace = True)\nfull_data['RainTomorrow'].replace({'No': 0, 'Yes': 1},inplace = True)","d6bf62ac":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize = (8,5))\nfull_data.RainTomorrow.value_counts(normalize = True).plot(kind='bar', color= ['skyblue','navy'], alpha = 0.9, rot=0)\nplt.title('RainTomorrow Indicator No(0) and Yes(1) in the Imbalanced Dataset')\nplt.show()","bbbee8f7":"from sklearn.utils import resample\n\nno = full_data[full_data.RainTomorrow == 0]\nyes = full_data[full_data.RainTomorrow == 1]\nyes_oversampled = resample(yes, replace=True, n_samples=len(no), random_state=123)\noversampled = pd.concat([no, yes_oversampled])\n\nfig = plt.figure(figsize = (8,5))\noversampled.RainTomorrow.value_counts(normalize = True).plot(kind='bar', color= ['skyblue','navy'], alpha = 0.9, rot=0)\nplt.title('RainTomorrow Indicator No(0) and Yes(1) after Oversampling (Balanced Dataset)')\nplt.show()","bd3875c8":"# Missing Data Pattern in Training Data\nimport seaborn as sns\nsns.heatmap(oversampled.isnull(), cbar=False, cmap='PuBu')","9ba9569a":"total = oversampled.isnull().sum().sort_values(ascending=False)\npercent = (oversampled.isnull().sum()\/oversampled.isnull().count()).sort_values(ascending=False)\nmissing = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing.head(4)","0c445904":"oversampled.select_dtypes(include=['object']).columns","cfaec6fe":"# Impute categorical var with Mode\noversampled['Date'] = oversampled['Date'].fillna(oversampled['Date'].mode()[0])\noversampled['Location'] = oversampled['Location'].fillna(oversampled['Location'].mode()[0])\noversampled['WindGustDir'] = oversampled['WindGustDir'].fillna(oversampled['WindGustDir'].mode()[0])\noversampled['WindDir9am'] = oversampled['WindDir9am'].fillna(oversampled['WindDir9am'].mode()[0])\noversampled['WindDir3pm'] = oversampled['WindDir3pm'].fillna(oversampled['WindDir3pm'].mode()[0])","75d3aef5":"# Convert categorical features to continuous features with Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\nlencoders = {}\nfor col in oversampled.select_dtypes(include=['object']).columns:\n    lencoders[col] = LabelEncoder()\n    oversampled[col] = lencoders[col].fit_transform(oversampled[col])","d970c2eb":"import warnings\nwarnings.filterwarnings(\"ignore\")","08c0ace0":"# Multiple Imputation by Chained Equations\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nMiceImputed = oversampled.copy(deep=True) \nmice_imputer = IterativeImputer()\nMiceImputed.iloc[:, :] = mice_imputer.fit_transform(oversampled)","ba722e9d":"MiceImputed.head()","7e987871":"MiceImputed.isna()","d903ac60":"# Detecting outliers with IQR\nQ1 = MiceImputed.quantile(0.25)\nQ3 = MiceImputed.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","b4d9d624":"# Removing outliers from dataset\nMiceImputed = MiceImputed[~((MiceImputed < (Q1 - 1.5 * IQR)) |(MiceImputed > (Q3 + 1.5 * IQR))).any(axis=1)]\nMiceImputed.shape","b68a607c":"# Correlation Heatmap\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncorr = MiceImputed.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(20, 20))\ncmap = sns.diverging_palette(250, 25, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=None, center=0,square=True, annot=True, linewidths=.5, cbar_kws={\"shrink\": .9})","84cb1b2f":"sns.pairplot( data=MiceImputed, vars=('MaxTemp','MinTemp','Pressure9am','Pressure3pm', 'Temp9am', 'Temp3pm', 'Evaporation'), hue='RainTomorrow' )\n","e1a62b1c":"# Standardizing data\nfrom sklearn import preprocessing\nr_scaler = preprocessing.MinMaxScaler()\nr_scaler.fit(MiceImputed)\nmodified_data = pd.DataFrame(r_scaler.transform(MiceImputed), index=MiceImputed.index, columns=MiceImputed.columns)\nmodified_data.head()","c2e7c497":"# Feature Importance using Filter Method (Chi-Square)\nfrom sklearn.feature_selection import SelectKBest, chi2\nX = modified_data.loc[:,modified_data.columns!='RainTomorrow']\ny = modified_data[['RainTomorrow']]\nselector = SelectKBest(chi2, k=10)\nselector.fit(X, y)\nX_new = selector.transform(X)\nprint(X.columns[selector.get_support(indices=True)])","ddfd9900":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier as rf\n\nX = MiceImputed.drop('RainTomorrow', axis=1)\ny = MiceImputed['RainTomorrow']\nselector = SelectFromModel(rf(n_estimators=100, random_state=0))\nselector.fit(X, y)\nsupport = selector.get_support()\nfeatures = X.loc[:,support].columns.tolist()\nprint(features)\nprint(rf(n_estimators=100, random_state=0).fit(X,y).feature_importances_)","9063ddda":"import warnings\nwarnings.filterwarnings(\"ignore\")","fc1db7f4":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(rf(n_estimators=100, random_state=0).fit(X,y),random_state=1).fit(X,y)\neli5.show_weights(perm, feature_names = X.columns.tolist())","f8df4b7e":"features = MiceImputed[['Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustDir', \n                       'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', \n                       'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm', \n                       'RainToday']]\ntarget = MiceImputed['RainTomorrow']\n\n# Split into test and train\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=12345)\n\n# Normalize Features\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","79213c03":"def plot_roc_cur(fper, tper):  \n    plt.plot(fper, tper, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","9679a6a0":"import time\nfrom sklearn.metrics import accuracy_score, roc_auc_score, cohen_kappa_score, plot_confusion_matrix, roc_curve, classification_report\ndef run_model(model, X_train, y_train, X_test, y_test, verbose=True):\n    t0=time.time()\n    if verbose == False:\n        model.fit(X_train,y_train, verbose=0)\n    else:\n        model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    roc_auc = roc_auc_score(y_test, y_pred) \n    coh_kap = cohen_kappa_score(y_test, y_pred)\n    time_taken = time.time()-t0\n    print(\"Accuracy = {}\".format(accuracy))\n    print(\"ROC Area under Curve = {}\".format(roc_auc))\n    print(\"Cohen's Kappa = {}\".format(coh_kap))\n    print(\"Time taken = {}\".format(time_taken))\n    print(classification_report(y_test,y_pred,digits=5))\n    \n    probs = model.predict_proba(X_test)  \n    probs = probs[:, 1]  \n    fper, tper, thresholds = roc_curve(y_test, probs) \n    plot_roc_cur(fper, tper)\n    \n    plot_confusion_matrix(model, X_test, y_test,cmap=plt.cm.Blues, normalize = 'all')\n    \n    return model, accuracy, roc_auc, coh_kap, time_taken","8f8bf726":"from sklearn.linear_model import LogisticRegression\n\nparams_lr = {'penalty': 'l1', 'solver':'liblinear'}\n\nmodel_lr = LogisticRegression(**params_lr)\nmodel_lr, accuracy_lr, roc_auc_lr, coh_kap_lr, tt_lr = run_model(model_lr, X_train, y_train, X_test, y_test)","1448e60e":"from sklearn.tree import DecisionTreeClassifier\n\nparams_dt = {'max_depth': 16,\n             'max_features': \"sqrt\"}\n\nmodel_dt = DecisionTreeClassifier(**params_dt)\nmodel_dt, accuracy_dt, roc_auc_dt, coh_kap_dt, tt_dt = run_model(model_dt, X_train, y_train, X_test, y_test)","e564a608":"from sklearn.neural_network import MLPClassifier\n\nparams_nn = {'hidden_layer_sizes': (30,30,30),\n             'activation': 'logistic',\n             'solver': 'lbfgs',\n             'max_iter': 500}\n\nmodel_nn = MLPClassifier(**params_nn)\nmodel_nn, accuracy_nn, roc_auc_nn, coh_kap_nn, tt_nn = run_model(model_nn, X_train, y_train, X_test, y_test)","7215e465":"from sklearn.ensemble import RandomForestClassifier\n\nparams_rf = {'max_depth': 16,\n             'min_samples_leaf': 1,\n             'min_samples_split': 2,\n             'n_estimators': 100,\n             'random_state': 12345}\n\nmodel_rf = RandomForestClassifier(**params_rf)\nmodel_rf, accuracy_rf, roc_auc_rf, coh_kap_rf, tt_rf = run_model(model_rf, X_train, y_train, X_test, y_test)","20606452":"import lightgbm as lgb\nparams_lgb ={'colsample_bytree': 0.95, \n         'max_depth': 16, \n         'min_split_gain': 0.1, \n         'n_estimators': 200, \n         'num_leaves': 50, \n         'reg_alpha': 1.2, \n         'reg_lambda': 1.2, \n         'subsample': 0.95, \n         'subsample_freq': 20}\n\nmodel_lgb = lgb.LGBMClassifier(**params_lgb)\nmodel_lgb, accuracy_lgb, roc_auc_lgb, coh_kap_lgb, tt_lgb = run_model(model_lgb, X_train, y_train, X_test, y_test)","b4eafeee":"import catboost as cb\nparams_cb ={'iterations': 50,\n            'max_depth': 16}\n\nmodel_cb = cb.CatBoostClassifier(**params_cb)\nmodel_cb, accuracy_cb, roc_auc_cb, coh_kap_cb, tt_cb = run_model(model_cb, X_train, y_train, X_test, y_test, verbose=False)","08c863a4":"import xgboost as xgb\nparams_xgb ={'n_estimators': 500,\n            'max_depth': 16}\n\nmodel_xgb = xgb.XGBClassifier(**params_xgb)\nmodel_xgb, accuracy_xgb, roc_auc_xgb, coh_kap_xgb, tt_xgb = run_model(model_xgb, X_train, y_train, X_test, y_test)","0c1cd090":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nimport catboost as cb\nimport xgboost as xgb\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom mlxtend.plotting import plot_decision_regions\n\nvalue = 1.80\nwidth = 0.90\n\nclf1 = LogisticRegression(random_state=12345)\nclf2 = DecisionTreeClassifier(random_state=12345) \nclf3 = MLPClassifier(random_state=12345, verbose = 0)\nclf4 = RandomForestClassifier(random_state=12345)\nclf5 = lgb.LGBMClassifier(random_state=12345, verbose = 0)\nclf6 = cb.CatBoostClassifier(random_state=12345, verbose = 0)\nclf7 = xgb.XGBClassifier(random_state=12345)\neclf = EnsembleVoteClassifier(clfs=[clf4, clf5, clf6, clf7], weights=[1, 1, 1, 1], voting='soft')\n\nX_list = MiceImputed[[\"Sunshine\", \"Humidity9am\", \"Cloud3pm\"]] #took only really important features\nX = np.asarray(X_list, dtype=np.float32)\ny_list = MiceImputed[\"RainTomorrow\"]\ny = np.asarray(y_list, dtype=np.int32)\n\n# Plotting Decision Regions\ngs = gridspec.GridSpec(3,3)\nfig = plt.figure(figsize=(18, 14))\n\nlabels = ['Logistic Regression',\n          'Decision Tree',\n          'Neural Network',\n          'Random Forest',\n          'LightGBM',\n          'CatBoost',\n          'XGBoost',\n          'Ensemble']\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4, clf5, clf6, clf7, eclf],\n                         labels,\n                         itertools.product([0, 1, 2],\n                         repeat=2)):\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, \n                                filler_feature_values={2: value}, \n                                filler_feature_ranges={2: width}, \n                                legend=2)\n    plt.title(lab)\n\nplt.show()","2b580fbb":"accuracy_scores = [accuracy_lr, accuracy_dt, accuracy_nn, accuracy_rf, accuracy_lgb, accuracy_cb, accuracy_xgb]\nroc_auc_scores = [roc_auc_lr, roc_auc_dt, roc_auc_nn, roc_auc_rf, roc_auc_lgb, roc_auc_cb, roc_auc_xgb]\ncoh_kap_scores = [coh_kap_lr, coh_kap_dt, coh_kap_nn, coh_kap_rf, coh_kap_lgb, coh_kap_cb, coh_kap_xgb]\ntt = [tt_lr, tt_dt, tt_nn, tt_rf, tt_lgb, tt_cb, tt_xgb]\n\nmodel_data = {'Model': ['Logistic Regression','Decision Tree','Neural Network','Random Forest','LightGBM','Catboost','XGBoost'],\n              'Accuracy': accuracy_scores,\n              'ROC_AUC': roc_auc_scores,\n              'Cohen_Kappa': coh_kap_scores,\n              'Time taken': tt}\ndata = pd.DataFrame(model_data)\n\nfig, ax1 = plt.subplots(figsize=(12,10))\nax1.set_title('Model Comparison: Accuracy and Time taken for execution', fontsize=13)\ncolor = 'tab:green'\nax1.set_xlabel('Model', fontsize=13)\nax1.set_ylabel('Time taken', fontsize=13, color=color)\nax2 = sns.barplot(x='Model', y='Time taken', data = data, palette='summer')\nax1.tick_params(axis='y')\nax2 = ax1.twinx()\ncolor = 'tab:red'\nax2.set_ylabel('Accuracy', fontsize=13, color=color)\nax2 = sns.lineplot(x='Model', y='Accuracy', data = data, sort=False, color=color)\nax2.tick_params(axis='y', color=color)","036e0ac5":"fig, ax3 = plt.subplots(figsize=(12,10))\nax3.set_title('Model Comparison: Area under ROC and Cohens Kappa', fontsize=13)\ncolor = 'tab:blue'\nax3.set_xlabel('Model', fontsize=13)\nax3.set_ylabel('ROC_AUC', fontsize=13, color=color)\nax4 = sns.barplot(x='Model', y='ROC_AUC', data = data, palette='winter')\nax3.tick_params(axis='y')\nax4 = ax3.twinx()\ncolor = 'tab:red'\nax4.set_ylabel('Cohen_Kappa', fontsize=13, color=color)\nax4 = sns.lineplot(x='Model', y='Cohen_Kappa', data = data, sort=False, color=color)\nax4.tick_params(axis='y', color=color)\nplt.show()","f53c9036":"We observe that all 4 features have <50% missing data. So instead of completely discarding them, we will consider them in our model with proper imputation.","44acf3be":"Next, we will check whether the dataset is imbalanced or balanced. If the dataset is imbalanced, we need to undersample majority or oversample minority to balance it. ","1fcf2a07":"# Training with Different Models\nWe will split the entire data set into training (75%) and testing (25%) sets respectively. For getting better results, we will standardize our X_train and X_test data (i.e. features without target for training and testing data sets).","53cfce61":"We can observe that \"Sunshine\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\" are having higher importance compared to other features.\n\n**(2) Feature Selection by Wrapper Method (Random Forest):**","7184a54f":"We observe that the original dataset was having the shape (87927, 24). After running outlier-removal code snippet, the dataset is now having the shape (86065, 24). So, the dataset is now free of 1862 outliers. We will now check for multi-collinearity i.e. whether any feature is highly correlated with another.","65d08d7d":"Now, we will check whether all \"NaN\" values are imputed completely or not.","afef01c2":"# Imputation and Transformation\nWe will impute categorical columns with mode, and then we will use label encoder to convert them into numeric ones. Once the full dataframe's all columns get converted to numeric ones, we will impute missing values (NaN etc.) using **MICE package (Multiple Imputation by Chained Equations)**. Afterwards, we will detect outliers using **Inter-Quartile Range** and remove them to get the final working data set. Finally, we will check correlation among different variables, and if we find any pair of highly correlated variables, we will discard one keeping the other.","0fb42b3f":"So, after MICE imputation, the dataframe does not have any \"NaN\" value. We will now detect and discard the outliers from data set based on Inter-Quartile Range.","36c61be7":"The following pairs of features are having high correlation between them:\n\n* MaxTemp and MinTemp\n* Pressure9am and Pressure3pm\n* Temp9am and Temp3pm\n* Evaporation and MaxTemp\n* MaxTemp and Temp3pm\nBut in no case, the correlation value is equal to a perfect \"1\". So we are not discarding any feature. \n\nHowever, we can dig deeper into pairwise correlation among these highly correlated features by looking at the following pairplot. Each of the pairplots shows very clearly distinguishable clusters of RainTomorrow \"yes\" and \"no\". There is very minimal overlap between them.","ef23b028":"**Model-6: CatBoost**","9f241ebd":"**Model-7: XGBoost**","7da58906":"# Feature Selection\nWe will use both filter method and wrapper method for feature selection.\n\n**(1) Feature Selection by Filter Method (Chi-Square Value):** Before performing this, we need to standardize our data first. We are using MinMaxScaler instead of StandardScaler in order to avoid negative values.","f78e63b2":"# Importing Data","38ed4f4b":"We can observe that presence of \"0\" and \"1\" are almost in the ratio 78:22. So there is a class imbalance and we have to handle it. For tackling class imbalance, we will use **oversampling of minority class** here. Since the size of the data set is quite small, undersampling of majority class would not make much sense here. ","33410ffe":"**Model-3: Neural Network (Multilayer Perceptron)**","538a98e7":"**Model-4: Random Forest**","cd9d926c":"# Plotting Decision Region for all Models","0237930e":"We can observe the difference in class boundaries for different models including the *ensemble* one (plotting is done considering the training data only). CatBoost has the distinct regional boundary compared to all other models. However, XGBoost and Random Forest models are also having very lesser number of misclassified data points as compared to other models.\n\n# Model Comparison\n\nNow, we need to decide which model has performed the best based on accuracy score, ROC_AUC, Cohen's Kappa and total time taken for execution. A mentionable point here is: we could have considered F1-Score as a better metric to judge model performance instead of accuracy, but we already converted the imbalanced data set to a balanced one, so considering accuracy as a metric to decide the best model is justified in this case. For a better decision, we have chosen **\"Cohen's Kappa\"** which is actually an ideal choice as a metric to decide the best model in case of imbalanced datasets. Let's check which model has performed well in which front.","fb8d353f":"**Model-5: Light GBM**","7bd2562e":"Both \"RainToday\" and \"RainTomorrow\" are object (Yes\/No). We will convert them into binary (1\/0) for our convenience.","4bf2ed4f":"# Data Exploration\nWe will check the no. of rows and columns first. Then we will check the size of data set to decide whether it requires any compression of size.","70bb7707":"Visibly, 'Evaporation','Sunshine','Cloud9am','Cloud3pm' are the features having high missing percentage. So we will check the missing data details for these 4 features.","ed72be96":"I always liked to know about the parameters the meteorologists consider before forecasting weather, so I found the data set interesting. From an expert's point of view, this data set is pretty simple though. For the beginners, this notebook can be a good source of learning for:\n* How balancing is done for an imbalanced data set\n* How label encoding is done for categorical variables\n* How sophisticated imputation like MICE is used\n* How outliers can be detected and excluded from data\n* How filter method and wrapper methods are used for feature selection\n* How speed and performance trade-off can be compared for various popular models\n* Which metric can be the best one for judging the performance on imbalanced data set: Accuracy, F1-Score or Cohen's Kappa\n\nIf you find my notebook interesting, kindly upvote!","2069f0ae":"**Model-1: Logistic Regression penalized by Lasso**","4856e8a5":"# Conclusion\nWe can observe that **XGBoost, CatBoost and Random Forest** have performed better compared to other models. However, if speed is an important thing to consider, we can stick to Random Forest instead of XGBoost or CatBoost.","d4c92b6f":"Now, we will now check the missing data pattern in the dataset.","758afaf8":"*This is quite interesting to see all feature importances except for RISK_MM are nearing to zero*. This is possible in two scenarios. Either when all the features have high correlation among each other or when the features actually have very very low relative feature importances w.r.t. the target variable. Since we already plotted correlation, we know that the first possibility is not true. We will cross-check if the second possibility is true by using **Permutation Importance**.","3bce851b":"We can observe that all features except \"RISK_MM\" have very very low relative importance (all nearing to zero) with respect to target variable \"RainTomorrow\". As explained by the dataset creator [Joe Young](http:\/\/www.kaggle.com\/jsphyg), \"RISKMM is the amount of rainfall in millimeters for the next day. It includes all forms of precipitation that reach the ground, such as rain, drizzle, hail and snow. And it was the column that was used to actually determine whether or not it rained to create the binary target. **Since it contains information directly about the target variable, including it would leak the future information to our model**\" (Quoted from his comment). So \"RISK_MM\" is excluded from the model. \"Date\" is also excluded from the model for the obvious reason since it is not adding any relevance in the current context.","bfd86c3b":"# Handling Class Imbalance","5e4faf6b":"**Model-2: Decision Tree**"}}