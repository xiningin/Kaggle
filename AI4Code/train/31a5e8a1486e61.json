{"cell_type":{"c426c071":"code","d4c888ab":"code","47b2f3ed":"code","84265bdf":"code","3baaac20":"code","090dce2c":"code","416eb845":"code","9ab83bdd":"code","2853d3fa":"code","e3088b6b":"code","f2db4cfe":"code","0b7b3367":"code","42fa305f":"code","acc77570":"code","4e14261c":"code","0cbcdc6d":"code","5652eb35":"code","c9511843":"code","c1c9afe3":"code","42765892":"code","3ea8cccf":"code","9399578e":"code","61d5c8a5":"code","de43f7e0":"code","9fdcff87":"markdown","af885acb":"markdown","f0a7504c":"markdown","fe591e1a":"markdown","d16ce0c6":"markdown","c58246f6":"markdown","ea81813e":"markdown","881ce510":"markdown","84272469":"markdown","2eaf7e5e":"markdown"},"source":{"c426c071":"import gc\nimport pickle\nimport re\nimport string\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import Metric, CategoricalAccuracy\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom transformers import AutoTokenizer, TFAutoModel","d4c888ab":"tqdm.pandas()","47b2f3ed":"MAX_TOKEN_LEN = 48\nPT_MODEL = 'cahya\/xlm-roberta-large-indonesian-NER'\nPT_MODEL_FROM_TORCH = True","84265bdf":"test_df = pd.read_csv('\/kaggle\/input\/scl-2021-ds\/test.csv')\nprint(test_df.shape)\ntest_df.head()","3baaac20":"clean_instrs = pickle.load(open('\/kaggle\/input\/scl-21-da-preprocess-v1\/preprocess_replaces_v1.pkl', 'rb'))\nclean_instrs[:10]","090dce2c":"input_address = test_df['raw_address']\n\nfor from_text, to_text in tqdm(clean_instrs):\n    input_address = input_address.str.replace(from_text, to_text)\ntest_df['raw_address'] = input_address\n\nprint(test_df.shape)\ntest_df.head()","416eb845":"tokenizer = AutoTokenizer.from_pretrained(PT_MODEL)","9ab83bdd":"def tokenize_row(row):\n    # Unpack data from input row\n    data = row['raw_address']\n    \n    # Tokenize single row\n    tokens = tokenizer(\n        data,\n        padding='max_length',\n        max_length=MAX_TOKEN_LEN,\n        return_tensors='np',\n        return_offsets_mapping=True\n    )\n    \n    # Unpack data from tokenizer\n    input_ids = tokens['input_ids'][0]\n    attention_mask = tokens['attention_mask'][0]\n    offset_mapping =tokens['offset_mapping'][0]\n    \n    return pd.Series({\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'offset_mapping': offset_mapping,\n        'raw_address': data,\n    })","2853d3fa":"token_df = test_df.progress_apply(tokenize_row, axis=1)\n\nprint(token_df.shape)\ntoken_df.head()","e3088b6b":"# TPU? https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras#Load-text-data-into-memory\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n    \nAUTO = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 32 * strategy.num_replicas_in_sync\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","f2db4cfe":"def cast_col_to_ndarray(col):\n    return np.asarray(col.to_list()).astype(np.long)\n\ntest_ds = (\n    tf.data.Dataset\n        .from_tensor_slices((\n            {\n                'input_ids': cast_col_to_ndarray(token_df['input_ids']),\n                'attention_mask': cast_col_to_ndarray(token_df['attention_mask']),\n            },\n        ))\n        .batch(BATCH_SIZE)\n)","0b7b3367":"with strategy.scope():\n    input_word_ids = Input(shape=(MAX_TOKEN_LEN,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = Input(shape=(MAX_TOKEN_LEN,), dtype=tf.int32, name=\"attention_mask\")\n\n    transformer_layer = TFAutoModel.from_pretrained(PT_MODEL, from_pt=PT_MODEL_FROM_TORCH)\n    transformer_output = transformer_layer([input_word_ids, attention_mask])[0]    \n\n    classifier_layer = Dense(3, activation='softmax')\n    classifier_output=  classifier_layer(transformer_output)\n    \n    transformer_layer.trainable = False\n    classifier_layer.trainable = False\n\n    model = Model(inputs=[input_word_ids, attention_mask], outputs=classifier_output)\n    model.load_weights('\/kaggle\/input\/scl-21-da-model-v5\/best_weights.h5')\n    \n    model.compile(\n        Adam(lr=1e-5),\n        loss=None,\n        weighted_metrics=[],\n    )\n    \nmodel.summary()","42fa305f":"preds = model.predict(test_ds, verbose=1)\npreds","acc77570":"preds.argmax(axis=2)","4e14261c":"token_df['pred_ids'] = preds.argmax(axis=2).tolist()\n\nprint(token_df.shape)\ntoken_df.head()","0cbcdc6d":"def recover_label_full(row):\n    original_data = row['raw_address']\n    keep_character = [False for _ in original_data]\n    \n    for token_span, token_pred in zip(row['offset_mapping'], row['pred_ids']):\n        if token_pred != 0:\n            if token_span[0] > 0 and original_data[token_span[0]-1] == ' ':\n                keep_character[token_span[0]-1] = token_pred\n            for i in range(token_span[0], token_span[1]):\n                keep_character[i] = token_pred\n    \n    pred_a = ''.join([i for i, j in zip(original_data, keep_character) if j == 1]).strip()\n    pred_b = ''.join([i for i, j in zip(original_data, keep_character) if j == 2]).strip()\n    \n    return pd.Series({\n        'POI\/street': pred_a + '\/' + pred_b\n    })","5652eb35":"pred_labels = (\n    token_df\n        .progress_apply(recover_label_full, axis=1)\n        .reset_index()\n        .rename(columns={'index': 'id'})\n)\n\nprint(pred_labels.shape)\npred_labels.head()","c9511843":"pred_labels.to_csv('submission.csv', index=False)\n!md5sum submission.csv","c1c9afe3":"fix_charset = string.ascii_lowercase + '-'","42765892":"# Edge case for cutting off text after 'al-'\ndef recover_label_full_fix(row):\n    original_data = row['raw_address']\n    keep_character = [False for _ in original_data]\n    \n    for token_span, token_pred in zip(row['offset_mapping'], row['pred_ids']):\n        if token_pred != 0:\n            if token_span[0] > 0 and original_data[token_span[0]-1] == ' ':\n                keep_character[token_span[0]-1] = token_pred\n            for i in range(token_span[0], token_span[1]):\n                keep_character[i] = token_pred\n    \n    for i in range(len(original_data)):\n        if keep_character[i] != 0 and original_data[i:i+3] == 'al-':\n            for j in range(i+3, len(original_data)):\n                if original_data[j] in fix_charset:\n                    keep_character[j] = keep_character[i]\n                else:\n                    break\n    \n    pred_a = ''.join([i for i, j in zip(original_data, keep_character) if j == 1]).strip()\n    pred_b = ''.join([i for i, j in zip(original_data, keep_character) if j == 2]).strip()\n    \n    return pd.Series({\n        'POI\/street': pred_a + '\/' + pred_b\n    })","3ea8cccf":"pred_labels_fix = (\n    token_df\n        .progress_apply(recover_label_full_fix, axis=1)\n        .reset_index()\n        .rename(columns={'index': 'id'})\n)\n\nprint(pred_labels_fix.shape)\npred_labels_fix.head()","9399578e":"token_df.loc[pred_labels['POI\/street'].str.contains('(al- |al-\/)')].shape","61d5c8a5":"token_df.loc[pred_labels_fix['POI\/street'].str.contains('(al- |al-\/)')].shape","de43f7e0":"pred_labels_fix.to_csv('submission_fix.csv', index=False)\n!md5sum submission_fix.csv","9fdcff87":"## Data fixing","af885acb":"## Tokenize\n\nUse a pretrained tokenizer on bahasa indo","f0a7504c":"## Save","fe591e1a":"## Recover label","d16ce0c6":"# Model\n\nTransfer learn from pretrained xlm-roberta model on bahasa indo + token classification head\n\nOriginal model training was done on a TPU, the weights saved here is the best of around 20 training rounds (thanks kaggle TPU)","c58246f6":"# Predict","ea81813e":"## Clean data\n\nUnicode characters, some punctuation is replaced.\n\nWe have a list of short-form words that are expanded to long-form if the long-form was found more often in train labels.","881ce510":"## Run model","84272469":"# Prepare data","2eaf7e5e":"## Load test file"}}