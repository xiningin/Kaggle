{"cell_type":{"8e3929a8":"code","cdeffb8a":"code","aa41f986":"code","67ad7cfa":"code","ca27f5cf":"code","896d0fa3":"code","078cbbff":"code","80ab4081":"code","ec0f80ac":"code","1f9b9ea5":"code","575a2d0e":"code","9c3bf1f6":"code","613ef8d7":"code","1b597de3":"code","1d3353a1":"code","a39a440a":"code","a4e7f4e9":"code","ab1e8647":"code","e5a0a0eb":"code","d3235333":"code","2d8085bb":"code","1254a115":"code","b2d37560":"code","9fc0c0b8":"code","a136d739":"code","613ddbd0":"code","f0b17f65":"code","b34d6fcd":"code","6109d9e0":"code","764c8344":"code","3264e753":"markdown","1f68c12e":"markdown","9f6fd80a":"markdown","c7ada1c9":"markdown","568d6096":"markdown","8db6d021":"markdown","05a4f58b":"markdown","f0233b30":"markdown","4a274a20":"markdown","fbfb3c07":"markdown","0049fbb3":"markdown","1f1b4f70":"markdown","d4ec12cb":"markdown","92d64bcd":"markdown","d4ab1e39":"markdown","f6b244c9":"markdown","5f59fe3d":"markdown","9735d9c5":"markdown","3afddf6f":"markdown","35be948b":"markdown","ad599ec1":"markdown","fe665511":"markdown"},"source":{"8e3929a8":"# Load numpy, pandas, sklearn, torch, etc\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torch import *\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader, TensorDataset\n\nimport sklearn\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom collections import Counter\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier \nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.svm import SVC\nfrom sklearn.cross_validation import KFold\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import resample\nfrom sklearn.cross_validation import train_test_split\n\nfrom dateutil.parser import parse\n\n\n","cdeffb8a":"# Load already parsed data again\ntrain_df = pd.read_csv('..\/input\/train.csv',encoding ='latin1')\ntest_df = pd.read_csv('..\/input\/test.csv',encoding ='latin1')","aa41f986":"#make copy of original df\nX_train_1=train_df\nX_test_1=test_df\n\n# Drop useless variables \nX_train_1.drop(labels = ['Unnamed: 0','Unfalldatum'], axis = 1, inplace = True)\nX_test_1.drop(labels = ['Unnamed: 0','Unfalldatum'], axis = 1, inplace = True)\nX_train_1.head()","67ad7cfa":"#parse 24h and extract hour\nX_train_1['Zeit (24h)']=X_train_1['Zeit (24h)'].apply(lambda x: '{0:0>4}'.format(x))\nX_test_1['Zeit (24h)']=X_test_1['Zeit (24h)'].apply(lambda x: '{0:0>4}'.format(x))\nX_test_1['Zeit (24h)'] = pd.to_datetime(X_test_1['Zeit (24h)'], format = '%H%M')\nX_test_1['Zeit (24h)'] = X_test_1['Zeit (24h)'].dt.hour\nX_train_1['Zeit (24h)'] = pd.to_datetime(X_train_1['Zeit (24h)'], format = '%H%M')\nX_train_1['Zeit (24h)'] = X_train_1['Zeit (24h)'].dt.hour","ca27f5cf":"## month is a cyclic feature. hence some cyclic feature engineering, see https:\/\/ianlondon.github.io\/blog\/encoding-cyclical-features-24hour-time\/\nm_per_year = 12\n\nX_train_1['sin Monat'] = np.sin(2*np.pi*X_train_1['Monat']\/m_per_year)\nX_train_1['cos Monat'] = np.cos(2*np.pi*X_train_1['Monat']\/m_per_year)\nX_test_1['sin Monat'] = np.sin(2*np.pi*X_test_1['Monat']\/m_per_year)\nX_test_1['cos Monat'] = np.cos(2*np.pi*X_test_1['Monat']\/m_per_year)","896d0fa3":"le=LabelEncoder()\n\ncolumns = [\n 'Strassenklasse',\n 'Unfallklasse',\n 'Lichtverh\u00e4ltnisse',\n 'Bodenbeschaffenheit',\n 'Geschlecht',\n 'Fahrzeugtyp',\n 'Wetterlage']\n\nfor col in columns:\n\n       if train_df[col].dtypes=='object':\n        data=train_df[col].append(test_df[col])\n        le.fit(data.values)\n        train_df[col]=le.transform(train_df[col])\n        test_df[col]=le.transform(test_df[col])","078cbbff":"\ncolumns = ['Unfallschwere']\n\nenc=OneHotEncoder(sparse=False)\n\nfor col in columns:\n    data=X_train_1[[col]]\n    enc.fit(data)\n\n    temp = enc.transform(X_train_1[[col]])\n\n    temp=pd.DataFrame(temp,columns=[(col+\"_\"+str(i)) for i in data[col]\n            .value_counts().index])\n\n    temp=temp.set_index(X_train_1.index.values) \n    Y_train_1=pd.concat([temp],axis=1)\n    ","80ab4081":"\ncolumns = ['Alter', 'Verletzte Personen',\n 'Anzahl Fahrzeuge']\n\nfor col in columns:\n    data=X_train_1[[col]].append(X_test_1[[col]])\n    scaler = StandardScaler()\n\n    scaler.fit(data)\n    \n    temp = scaler.transform(X_train_1[[col]])\n\n    temp=pd.DataFrame(temp,columns=[(col+\"_\"+str('scaled'))])\n\n    temp=temp.set_index(X_train_1.index.values)\n       \n    X_train_1=pd.concat([X_train_1,temp],axis=1)\n\n    temp = scaler.transform(X_test_1[[col]])\n       \n    temp=pd.DataFrame(temp,columns=[(col+\"_\"+str('scaled'))])\n\n\n    temp=temp.set_index(X_test_1.index.values)\n\n    X_test_1=pd.concat([X_test_1,temp],axis=1)","ec0f80ac":"\nY_traintemp = Y_train_1\nX_traintemp = X_train_1.drop(labels = [\"Unfallschwere\"],axis = 1)","1f9b9ea5":"categorical_features = ['Strassenklasse',\n\n 'Unfallklasse',\n 'Lichtverh\u00e4ltnisse',\n 'Zeit (24h)',\n 'Geschlecht',\n\n 'Bodenbeschaffenheit',\n\n 'Fahrzeugtyp',\n 'Wetterlage']\n\ncont_features = [\n 'Verletzte Personen_scaled',\n 'Anzahl Fahrzeuge_scaled',\n\n 'Alter_scaled',\n 'sin Monat',\n 'cos Monat',\n\n                       ]\n","575a2d0e":"#how many unique values are in training and test dataset per categorial feature, construct embedding matrix\ntempc = pd.concat([X_traintemp,X_test_1])\n\ncat_dims = [int(tempc[col].nunique()) for col in categorical_features]\nemb_dims = [(x, min(50, (x + 1) \/\/ 2)) for x in cat_dims]\n\ncat_dims, emb_dims","9c3bf1f6":"X_train, x_test, Y_train, y_test = train_test_split(X_traintemp, Y_traintemp, test_size=0.2, random_state=107)","613ef8d7":"datatemp = pd.concat([pd.Series.astype(X_train,dtype = np.float64),pd.Series.astype(Y_train,dtype = np.float64)],axis=1)\n\n# Separate majority and minority classes\ndf_1 = datatemp[datatemp['Unfallschwere_1'].values==1]\ndf_2 = datatemp[datatemp['Unfallschwere_2'].values==1]\ndf_3 = datatemp[datatemp['Unfallschwere_3'].values==1]\n\n \n# Upsample minority class 2\ndf_2_upsampled = resample(df_2, \n                                 replace=True,     # sample with replacement\n                                 n_samples=df_1.shape[0],    # to match majority class\n                                 random_state=123) # reproducible results\n\n# Upsample minority class 3\ndf_3_upsampled = resample(df_3, \n                                 replace=True,     # sample with replacement\n                                 n_samples=df_1.shape[0],    # to match majority class\n                                 random_state=123) # reproducible results\n \n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_1, df_2_upsampled, df_3_upsampled])\n \n# Display new class counts\ndf_upsampled[df_upsampled['Unfallschwere_1'].values==1].shape, df_upsampled[df_upsampled['Unfallschwere_3'].values==1].shape\n\n## seperate features from labels again\nY_train = df_upsampled[[\"Unfallschwere_1\",\"Unfallschwere_2\",\"Unfallschwere_3\"]]\nX_train = df_upsampled.drop(labels = [\"Unfallschwere_1\",\"Unfallschwere_2\",\"Unfallschwere_3\"],axis = 1)\nY_train.shape,X_train.shape","1b597de3":"#seperate continous and categorial features for dataloader\ncont = X_train[cont_features]\ncat = X_train[categorical_features]\n\n#change datatype to torch tensor, create dataset and dataloader with batches\n\n#prepare train data for pytorch (categorial features are int, while continous are float)\nY_tensor_train  = torch.tensor(np.asarray(Y_train.values,dtype=np.float32))\nX_tensor_train  = torch.tensor(np.asarray(cat.values,dtype=np.int64))\ncont_data = torch.tensor(np.asarray(cont.values,dtype=np.float32))\n\ndataset = TensorDataset(X_tensor_train, Y_tensor_train, cont_data)\ndataloader = DataLoader(dataset, batch_size=100,\n                        shuffle=True, num_workers=4)\n\n#prepare validation data\ncont_val = x_test[cont_features]\ncat_val = x_test[categorical_features]\nX_tensor_val  = torch.tensor(np.asarray(cat_val.values,dtype=np.int64))\nY_tensor_val  = torch.tensor(np.asarray(y_test.values,dtype=np.float32))\ncont_data_val = torch.tensor(np.asarray(cont_val.values,dtype=np.float32))  \n\n#prepare test data\ncont_test = X_test_1[cont_features]\ncat_test = X_test_1[categorical_features]\nX_tensor_test  = torch.tensor(np.asarray(cat_test.values,dtype=np.int64))\ncont_data_test = torch.tensor(np.asarray(cont_test.values,dtype=np.float32))  \n","1d3353a1":"\nN_FEATURES =  X_train_1.shape[1]\nLR = 0.001\n#different dropout for different layers, more dropout for later layers\ndropout = torch.nn.Dropout(p=1 - (0.5))\ndropout1 = torch.nn.Dropout(p=1 - (0.9))\ndropout2 = torch.nn.Dropout(p=1 - (0.75))\nno_of_cont = cont_data.shape[1]\n\nN_LABELS = Y_train_1.shape[1]   #3 #n classes\n\n\nhiddenLayer1Size=512\nhiddenLayer2Size=int(hiddenLayer1Size\/2)\nhiddenLayer3Size=int(hiddenLayer1Size\/4)\nhiddenLayer4Size=int(hiddenLayer1Size\/8)\nhiddenLayer5Size=int(hiddenLayer1Size\/16)\n\nemb_layers = nn.ModuleList([nn.Embedding(x, y)\n                                     for x, y in emb_dims])\nno_of_embs = sum([y for x, y in emb_dims])\nbn1 = nn.BatchNorm1d(no_of_cont)\n\nlinear1=torch.nn.Linear(no_of_embs+no_of_cont, hiddenLayer1Size, bias=True) \ntorch.nn.init.xavier_uniform(linear1.weight)\n\nlinear2=torch.nn.Linear(hiddenLayer1Size, hiddenLayer2Size)\ntorch.nn.init.xavier_uniform(linear2.weight)\n\nlinear3=torch.nn.Linear(hiddenLayer2Size, hiddenLayer3Size)\ntorch.nn.init.xavier_uniform(linear3.weight)\n\nlinear4=torch.nn.Linear(hiddenLayer3Size, hiddenLayer4Size)\ntorch.nn.init.xavier_uniform(linear4.weight)\n\nlinear5=torch.nn.Linear(hiddenLayer4Size, N_LABELS)\ntorch.nn.init.xavier_uniform(linear5.weight)\n\n\n\nsigmoid = torch.nn.Sigmoid()\nsftmx = torch.nn.Softmax()\ntanh=torch.nn.Tanh()\nleakyrelu=torch.nn.LeakyReLU()\n\n\n","a39a440a":"\n#define classifier class, architecture of nn\nclass _classifier(nn.Module):\n    def __init__(self):\n        super(_classifier, self).__init__()\n        self.emb_layers = emb_layers\n        self.first_bn_layer = bn1\n        self.main = nn.Sequential(\n\n            linear1,leakyrelu,nn.BatchNorm1d(hiddenLayer1Size),dropout2,\n            linear2,leakyrelu,nn.BatchNorm1d(hiddenLayer2Size),dropout,          \n            linear3,leakyrelu,nn.BatchNorm1d(hiddenLayer3Size),dropout,\n            linear4,leakyrelu,nn.BatchNorm1d(hiddenLayer4Size),dropout,\n            linear5,sigmoid\n            \n        )\n#define pytorch forward pass        \n    def forward(self, cat_data, cont_data):\n        x = [emb_layer(cat_data[:, i]) for i,emb_layer in enumerate(self.emb_layers)]\n        x = torch.cat(x, 1)\n        x = dropout1(x)\n        normalized_cont_data = self.first_bn_layer(cont_data)\n        mainin = torch.cat([x, normalized_cont_data], 1) \n        \n        return self.main(mainin)\n\nclassifier = _classifier().cuda()\n#define optimizer and criterion, we dont use a LR sheduler for now\noptimizer = optim.Adam(classifier.parameters())\n#lr=LR,weight_decay=5e-3 learning rate and weight decay not implemented yet\ncriterion = nn.BCELoss()\n#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60,100,150,400], gamma = 0.1)\n\n#train network with n epochs and minibatches\nepochs = 500\nfor epoch in range(epochs):\n    losses = []\n    tu = [] \n    \n    for sample_batched, labels_batched, cont_data in dataloader:\n          \n        output = classifier(sample_batched.cuda(),cont_data.cuda()) # predict labels from input\n        loss = criterion(output.cuda(), labels_batched.cuda()) #compute loss\n\n        optimizer.zero_grad()  # clear gradients for next train\n        loss.backward() # backpropagation, compute gradients\n        optimizer.step() # apply gradients\n        losses.append(loss.data.mean())\n    #scheduler.step() #apply scheduler after each epoch\n        \n    print('[%d\/%d] Loss: %.3f' % (epoch+1, epochs, np.mean(losses)))\n\n     \n    if epoch % 10 == 0:\n  #check validation log loss every 10 epoch  \n        cl1 = classifier\n        prediction = (cl1(X_tensor_val.cuda(),cont_data_val.cuda()).data > 0.5).float() # zero or one\n        pred_y = prediction.cpu().numpy().squeeze()\n        target_y = Y_tensor_val.cpu().data.numpy()\n        tu.append(log_loss(target_y, pred_y))\n        print('[%d\/%d] Validation log loss: %.3f' % (epoch+1, epochs, np.mean(tu)))\n","a4e7f4e9":"cl1 = classifier\n\n#prediction = (cl1(X_tensor_val).data).float() # probabilities \nprediction = (cl1(X_tensor_val.cuda(),cont_data_val.cuda()).data > 0.5).float() # zero or one\n\npred_y = prediction.cpu().numpy().squeeze()\n\ntarget_y = Y_tensor_val.cpu().data.numpy()\n\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nlabels = ['0', '1','2']\ncm = confusion_matrix(\n    target_y.argmax(axis=1), pred_y.argmax(axis=1))\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix of the classifier')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\n#cplt.show()","ab1e8647":"prediction_test = (cl1(X_tensor_test.cuda(),cont_data_test.cuda()).data > 0.5).float() # zero or one\npred_y_test = prediction_test.cpu().numpy().squeeze()\n\nIDtest = pd.DataFrame(data=X_test_1.index.values,columns = ['Unfall_ID'])\npred = pd.Series(pred_y_test.argmax(axis=1), name=\"Unfallschwere\")\n\nresults = pd.concat([IDtest,pred],axis=1)\n\nresults.to_csv(\"MLP_pytorch_embed.csv\",index=False)","e5a0a0eb":"#labels instead of one-hot encoding\nY_train1 = Y_train.values\nY_train1  = np.argmax(Y_train1, axis=1)\nY_train1\n\n# Try different models, combine them later in ensemble learning, use 4 folds, as kaggle runtime is limited\nkfold = StratifiedKFold(n_splits=4)","d3235333":"# Gradient boosting\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] }\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsGBC.fit(X_train,Y_train1)\nGBC_best = gsGBC.best_estimator_\ngsGBC.best_score_","2d8085bb":"pred_y = pd.Series(GBC_best.predict(X_train), name=\"GBC\")\ntarget_y = Y_train1\n\nlabels = ['0', '1','2']\ncm = confusion_matrix(\n    target_y, pred_y)\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix of the classifier')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","1254a115":"pred_y = GBC_best.predict(x_test)\ntarget_y = np.argmax(y_test.values, axis=1)\ncm = confusion_matrix(\n    target_y, pred_y)\n\nprint(cm)\nlabels = ['0', '1','2']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix of the classifier')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","b2d37560":"ExtC = ExtraTreesClassifier()\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 7],\n              \"min_samples_split\": [2, 3, 7],\n              \"min_samples_leaf\": [1, 3, 7],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[300,600],\n              \"criterion\": [\"gini\"]}\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsExtC.fit(X_train,Y_train1)\nExtC_best = gsExtC.best_estimator_\ngsExtC.best_score_","9fc0c0b8":"random_forest = RandomForestClassifier(n_estimators=100)\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 7],\n              \"min_samples_split\": [2, 3, 7],\n              \"min_samples_leaf\": [1, 3, 7],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[300,600],\n              \"criterion\": [\"gini\"]}\ngsrandom_forest = GridSearchCV(random_forest,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsrandom_forest.fit(X_train,Y_train1)\n# Best score\nrandom_forest_best = gsrandom_forest.best_estimator_\ngsrandom_forest.best_score_","a136d739":"pred_y = pd.Series(random_forest_best.predict(X_train), name=\"random_forest\")\ntarget_y = Y_train1\nlabels = ['0', '1','2']\ncm = confusion_matrix(\n    target_y, pred_y)\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix of the classifier')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","613ddbd0":"pred_y = pd.Series(random_forest_best.predict(x_test), name=\"random_forest\")\ntarget_y = np.argmax(y_test.values, axis=1)\ncm = confusion_matrix(\n    target_y, pred_y)\n\nprint(cm)\nlabels = ['0', '1','2']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix of the classifier')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","f0b17f65":"DTC = DecisionTreeClassifier()\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsadaDTC.fit(X_train,Y_train1)\nadaDTC_best = gsadaDTC.best_estimator_\ngsadaDTC.best_score_","b34d6fcd":"SVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.1],\n                  'C': [10,200]}\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsSVMC.fit(X_train,Y_train1)\nSVMC_best = gsSVMC.best_estimator_\n# Best score\ngsSVMC.best_score_","6109d9e0":"\ntest_Unfallschwere_AdaDTC = pd.Series(adaDTC_best.predict(X_test_1), name=\"AdaDTC\")\ntest_Unfallschwere_ExtC = pd.Series(ExtC_best.predict(X_test_1), name=\"ExtC\")\ntest_Unfallschwere_GBC = pd.Series(GBC_best.predict(X_test_1), name=\"GBC\")\ntest_Unfallschwere_SVMC = pd.Series(SVMC_best.predict(X_test_1), name=\"SVMC\")\ntest_Unfallschwere_random_forest = pd.Series(random_forest_best.predict(X_test_1), name=\"random_forest\")\n\n\n# Concatenate all classifier results\nensemble_results = pd.concat([test_Unfallschwere_AdaDTC, test_Unfallschwere_ExtC, test_Unfallschwere_GBC,test_Unfallschwere_SVMC,test_Unfallschwere_random_forest],axis=1)\n\nVotingPredictor = VotingClassifier(estimators=[('ExtC', ExtC_best), ('GBC',GBC_best),\n('SVMC', SVMC_best), ('random_forest', random_forest_best)], voting='soft', n_jobs=4)\nVotingPredictor = VotingPredictor.fit(X_train, Y_train1)","764c8344":"#Save prediciton from ensemble voting\nIDtest = pd.DataFrame(data=X_test_1.index.values,columns = ['Unfall_ID'])\ntest = pd.Series(VotingPredictor.predict(X_test_1), name=\"Unfallschwere\")\n\nresults = pd.concat([IDtest,test],axis=1)\n\nresults.to_csv(\"ensemble_python_voting.csv\",index=False)","3264e753":"# Define categorial and continous features\nwe chose to embed daytime, while we use cyclic feature engineering for month - out of curiosity.\n\n","1f68c12e":"# Seperate continous and categorial features, construct dataloader\n","9f6fd80a":"# Upsample minority class\na more advance data augmentation technique would be useful here such as using GANs (https:\/\/stats.stackexchange.com\/questions\/358970\/can-a-gan-be-used-for-tabular-vector-data-augmentation) \nor Synthetic Minority Over-sampling Technique (SMOTE) (Chawla et al., 2012),","c7ada1c9":"# ExtraTrees ","568d6096":"# Save prediction for test set","8db6d021":"# Gradient boosting","05a4f58b":"# SVC classifier","f0233b30":"# One hot encoding for labels","4a274a20":"# Label encoding\nwe chose label encoding on the original data, we do not create additional features from e.g. 'Lichterv\u00e4ltnisse'\n","fbfb3c07":"# Split train data set into training and validation set\n","0049fbb3":"# Classification using different classical models","1f1b4f70":"# Random Forest","d4ec12cb":"# Construct embedding\nhow many unique values are in training and test dataset per feature, construct embedding dimensionality\n","92d64bcd":"# Load libraries and import data","d4ab1e39":"# Specify neural network parameters \n","f6b244c9":"# Predicitons for whole validation set","5f59fe3d":"# Save prediction","9735d9c5":"# Adaboost","3afddf6f":"# Construct classifier, forward pass, run training and validation","35be948b":"# Seperate features and labels","ad599ec1":"# Data normalization \nfor Age and other columns, we do not bin age groups, which might be useful: https:\/\/arxiv.org\/pdf\/1702.04415.pdf\ncould use embeddings instead","fe665511":"# Ensemble voting"}}