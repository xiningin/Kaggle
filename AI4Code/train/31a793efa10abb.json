{"cell_type":{"bb479e30":"code","5fb128cf":"code","d84865ef":"code","f0161381":"code","e030ab43":"code","1fb5fa00":"code","e973570a":"code","d5653093":"code","faccdc87":"code","08ee9c28":"code","457c4913":"code","60efea87":"code","db958cd5":"code","25e67547":"code","f637a070":"code","3ef6e14d":"markdown","b812144c":"markdown","2c9f8e20":"markdown","2c43f252":"markdown","ecda8e20":"markdown","68db4a04":"markdown","65d5b023":"markdown","b31c9324":"markdown"},"source":{"bb479e30":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5fb128cf":"df = pd.read_csv('..\/input\/iris\/Iris.csv')\ndf.head()","d84865ef":"df.shape","f0161381":"df.columns","e030ab43":"df.info()","1fb5fa00":"sns.countplot(x='Species', data=df)","e973570a":"df['Species'].value_counts()","d5653093":"sns.pairplot(data=df.drop('Id', axis=1), hue='Species')","faccdc87":"df_corr = df.corr()\nsns.heatmap(data=df_corr, cmap='coolwarm', annot=True)\nplt.title('Correlation between variables and species')","08ee9c28":"df = df.drop('Id', axis=1)\ndf.head()","457c4913":"sns.catplot(data=df, col='Species', kind='box')","60efea87":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclass model_evaluator:\n    def __init__(self):\n        #You can add more models here if you're curious\n        self.models = [\n            ('Logistic regression', LogisticRegression()),\n            ('Decision tree', DecisionTreeClassifier()),\n            ('SVC', SVC()),\n            ('Random forest', RandomForestClassifier()),\n            ('KNN', KNeighborsClassifier())\n        ]\n    \n        self.cv_models = []\n    \n        self.predictions = []\n\n        self.metrics = []\n    \n    def generate_metrics(self, name, model, X_test, y_test):\n        self.cv_models.append((name, model))\n        p = model.predict(X_test)\n        \n        #you can add more evaluation metrics in this dict here\n        #y test are the correct values and p the predicted values\n        d = {\n            'Name': name,\n            'accuracy': accuracy_score(y_test, p)\n        }\n        self.metrics.append(d)\n    \n    def evaluate_models(self, X_train, X_test, y_train, y_test):\n        for name, model in self.models:\n                \n            model.fit(X_train, y_train.values.ravel())\n            \n            self.generate_metrics(name, model, X_test, y_test)\n\n        self.models.extend(self.cv_models)    \n            \n        self.metrics = pd.DataFrame(self.metrics)\n        \n        self.metrics = self.metrics.set_index('Name').sort_values(by=['accuracy'], ascending=False)\n    \n    #This method returns the model that had the best R2 score in the evaluation\n    def select_best_accuracy(self):\n        best = self.metrics[self.metrics['r2 score'] == self.metrics['accuracy'].max()]\n        print(best)\n        for model in self.models:\n            if model[0] == best.index:\n                return model[1]\n","db958cd5":"X = df.drop('Species', axis=1)\ny = df['Species']","25e67547":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\ntester = model_evaluator()\ntester.evaluate_models(X_train, X_test, y_train, y_test)","f637a070":"tester.metrics","3ef6e14d":"Isn't really difficult to tell which of them are the same species, since every Specie has a distinct combination of each Feature it is possible to get a high score with almost any model without actually using any data engineering technics.","b812144c":"Fortunately our dataset is very balanced and every specie have 50 rows of data, also we don't have any null values.","2c9f8e20":"# Analysing correlation:","2c43f252":"The Id column must be eliminated, since it can lead the model to a bias that values between a certain range are always of an specie (which means the model won't learn, it'll actually only memorize and become useless to a real use), the Id is probably used by the scientist only to control the data.","ecda8e20":"# Conclusion:\nThe iris dataset seems like an \"ideal world data\", is very clean, already scaled and balanced, so this problem doesn't require data engineering skills, which makes it ideal for beginners.","68db4a04":"# Creating the models:\nIn order to test our hipothesis we're going to try these models to solve this problem without changing any data:\n1. Logistic regression\n2. Decision Tree\n3. SVC\n4. Random Forest\n5. KNN\n\nin order to do that i made this class, that will train classification and models.","65d5b023":"![Image of how you measure a flower](https:\/\/www.researchgate.net\/profile\/Zhi-Gang-Zhao-2\/publication\/272514310\/figure\/fig1\/AS:294815561469959@1447300915487\/Trollius-ranunculoide-flower-with-measured-traits-doi101371-journalpone0118299g001.png)\n\nThe competition to get polinators make flowers mutate a lot to have the most attractive characteristics, by this way the characteristics that attracts them are the ones that are the more selected, also this make some flowers species look like others of the same gender but have a few differences on measurements (like the iris gender)","b31c9324":"![picture explaining the parts of a iris flower](https:\/\/media.istockphoto.com\/vectors\/common-flower-parts-vector-id1020359338?k=20&m=1020359338&s=612x612&w=0&h=npJhyxEILDaXs812jaZMXc8Tua36sRO3sadHRYNlgQ8=)\n# Iris flower analysis\nThe purpose of this analysis is to understand which machine learning models perform well in the iris dataset and why it is so good for the beginners.\n## Exploring and understanding our data:"}}