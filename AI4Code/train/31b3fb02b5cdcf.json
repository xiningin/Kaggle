{"cell_type":{"b55486c6":"code","f669ad84":"code","06795813":"code","c5e9450d":"code","4ec5c4ee":"code","565e0a8e":"code","144a1b72":"code","57d4e580":"code","bbeffa52":"code","76b1f0d0":"code","283e75ce":"code","c3cb5693":"code","408aeba2":"code","9b54d646":"code","9c90e2d9":"code","a3c6627a":"code","0ff6c3b7":"code","95863615":"code","f2a57104":"code","05df7f22":"code","1ffa4514":"code","e1d1346a":"code","3176ee1c":"code","b736bc5d":"code","6ea6769f":"code","53f8f46a":"code","4db530c4":"code","9198fefc":"code","1bd082ba":"code","0909f976":"code","6ce8d131":"code","5e1c7a6e":"code","c8376160":"code","5659503a":"code","b120dfe5":"code","63257930":"code","527e72aa":"code","ea029e12":"code","25ab6a6a":"code","010343e6":"code","f149d536":"code","a0052477":"code","918b9142":"code","ff300460":"code","4c5a78d0":"code","142b08a5":"code","76faba4e":"code","2fef05a2":"code","6c25c6f3":"code","dbef6371":"code","33931583":"code","39f6bf26":"code","c734aad5":"code","4d26b105":"code","7db74523":"code","65608411":"code","e6447932":"code","7185a37e":"code","008be4cd":"code","e8f38d7f":"code","a594b8a7":"code","0fef19dc":"code","c425f40c":"code","7755acd2":"code","53a8d7fb":"code","8789aef1":"code","431eb94f":"code","60928359":"code","dd944a45":"code","748d8778":"code","a4bf793f":"code","8c7dedb7":"code","95a40cc7":"code","79570a35":"code","63ac52f2":"code","0e8b2acd":"code","eb71c641":"code","12fee390":"code","8ad4a33a":"code","9c886c04":"code","f5ad2aaa":"code","1786509c":"code","159391ba":"code","548124f9":"code","2f85a2e5":"code","e96d1022":"code","ca0a7b56":"code","82912007":"code","aac45db5":"code","c2143cf9":"code","7d4d30ab":"code","5ce70be5":"code","819e3946":"code","ddc72961":"code","4a33874a":"code","bb396f49":"code","0b5a7c2e":"code","8b5af330":"code","4e9fbfc9":"code","1f439dab":"code","1f3e881a":"code","3dfc77c7":"code","23da3203":"code","63d9161a":"code","93f49257":"code","dc7444d5":"code","8c71f9d5":"markdown","b4ecec9e":"markdown","e962f7a5":"markdown","afe2fc13":"markdown","26f9c6a0":"markdown","bee44444":"markdown","78bbf1ac":"markdown","e23363b2":"markdown","3317abea":"markdown","b0952196":"markdown","8fdc9b07":"markdown","0d857fc4":"markdown","c318e422":"markdown","cd8bf5ee":"markdown","18e5ba7a":"markdown","7a20b4a1":"markdown","47d5277b":"markdown","fe4124c5":"markdown","bb7515e1":"markdown","8838dbf1":"markdown","7e7b2ce2":"markdown","dc4f65fd":"markdown","6ad45dbf":"markdown","b91c6b58":"markdown","c44c89f9":"markdown","e9b1e9d9":"markdown","0af99d39":"markdown","e054feb4":"markdown","1dd2ef71":"markdown","b6b9cc3d":"markdown","0a707aa3":"markdown","b78ec655":"markdown","c5d320f2":"markdown","bd4cdfbc":"markdown","4b5ee07e":"markdown","18cc489a":"markdown","50f9a2ed":"markdown","81150cff":"markdown","d65eff02":"markdown","957b9d8c":"markdown","0f3bee9f":"markdown","b807c43c":"markdown","2ae39a76":"markdown","7cff0bae":"markdown","545c816b":"markdown","bb12b992":"markdown","94217f57":"markdown","a480a339":"markdown","7ec2f682":"markdown","48fb9b5c":"markdown","c5760f3f":"markdown","59e7e17c":"markdown","c6bf1723":"markdown","733b9509":"markdown","2aae50f0":"markdown","8c219bc2":"markdown","26eb7aba":"markdown","58dd7c46":"markdown","b4cc1ff4":"markdown","de2e79fe":"markdown","c1890abf":"markdown","68276556":"markdown","4d69743c":"markdown","d76de51d":"markdown","f25d42e4":"markdown","946d4a30":"markdown","94ca24c5":"markdown","0fb10131":"markdown","39bd6b01":"markdown","a2ce1391":"markdown","d455d99b":"markdown","92cf5960":"markdown","0cff2443":"markdown","c0bcf21e":"markdown","ac30f8c9":"markdown","4181cfa4":"markdown","63367430":"markdown","347ce8da":"markdown","e7cf418f":"markdown","3ce3492b":"markdown","3cd4bc49":"markdown","0c9c5fd9":"markdown","c9181ef3":"markdown","01b8389f":"markdown","60109374":"markdown","70505636":"markdown","73aaf955":"markdown","c1d7c2e1":"markdown","f7b56644":"markdown","658178af":"markdown","5b5e3440":"markdown","2d716db9":"markdown","1d1588ad":"markdown","16934742":"markdown","bba7abbf":"markdown","888a3053":"markdown","194d9ad2":"markdown","e7fae6b3":"markdown","7552ceea":"markdown","08eaccaf":"markdown","47b9e2df":"markdown","68f18f8d":"markdown","c4373907":"markdown","fa97aeaf":"markdown","17d6761f":"markdown"},"source":{"b55486c6":"!pip install numpy==1.19.3\n!pip install optbinning","f669ad84":"import numpy as np\nimport pandas as pd","06795813":"import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n%matplotlib inline\n\nimport seaborn as sns\nsns.set(rc={\"font.size\":18,\"axes.titlesize\":30,\"axes.labelsize\":18,\n            \"axes.titlepad\":22, \"axes.labelpad\":18, \"legend.fontsize\":15,\n            \"legend.title_fontsize\":15, \"figure.titlesize\":35})","c5e9450d":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nimport re","4ec5c4ee":"from sklearn import *\nfrom xgboost import XGBClassifier\n\nimport tensorflow as tf\nfrom tensorflow.keras import *","565e0a8e":"import warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_colwidth', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","144a1b72":"# Get missing totals and percentage for each column\ndef get_missing(df):    \n    missing = df.isnull().sum()\n    missing_percentage = df.isnull().sum() \/ df.isnull().count() * 100\n    missing_percentage = round(missing_percentage, 1)\n    missing_data = pd.concat([missing, missing_percentage], axis=1, keys=['Total', '%'])\n    missing_data = missing_data[missing_data['Total'] > 0].sort_values(by=['%'], ascending=False)\n    \n    return missing_data\n\n# Calculate the Variance Inflation Factor\ndef calc_vif(X):\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)","57d4e580":"df_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndf_train.head()","bbeffa52":"print(df_train.shape, df_test.shape)","76b1f0d0":"df_train.info()","283e75ce":"fig = plt.figure(figsize=(22,8))\nkde = sns.kdeplot(x=\"Age\", data=df_train, cut=0, hue=\"Survived\", fill=True, legend=True, palette=\"mako_r\")\n\nplt.xlim(0)\n\nkde.xaxis.set_major_locator(ticker.MultipleLocator(1))\nkde.xaxis.set_major_formatter(ticker.ScalarFormatter())\n\nfig.suptitle(\"AGE BY SURVIVED\", x=0.125, y=1.0, ha='left', fontweight=100, fontfamily='Segoe UI', size=39);","c3cb5693":"fig = plt.figure(figsize=(22, 8))\nhist = sns.histplot(df_train[\"Age\"], color=\"gold\", kde=True, bins=50, label=\"Train\")\nhist = sns.histplot(df_test[\"Age\"], color=\"crimson\", kde=True, bins=50, label=\"Test\")\n\nplt.xlim(0)\n\ntitle = fig.suptitle(\"DISTRIBUITION OF AGE IN TRAIN & TEST\", x=0.125, y=1.01, ha='left', \n                     fontweight=100, fontfamily='Segoe UI', size=39)\n\nhist.xaxis.set_major_locator(ticker.MultipleLocator(1))\nhist.xaxis.set_major_formatter(ticker.ScalarFormatter())\n\nplt.legend()\nplt.show()","408aeba2":"fig = plt.figure(figsize=(22,8))\nkde = sns.kdeplot(x=\"Fare\", data=df_train, cut=0, hue=\"Survived\", fill=True, legend=True, palette=\"mako_r\")\n\nkde.xaxis.set_major_locator(ticker.MultipleLocator(10))\nkde.xaxis.set_major_formatter(ticker.ScalarFormatter())\n\nplt.xlim(0)\n\nfig.suptitle(\"FARE BY SURVIVED\", x=0.125, y=1.01, ha='left',fontweight=100, fontfamily='Segoe UI', size=39);","9b54d646":"fig = plt.figure(figsize=(20,8))\nkde = sns.kdeplot(x=\"Fare\", data=df_train, cut=0, clip=[0,180], hue=\"Survived\", fill=True, legend=True, palette=\"mako_r\")\n\nplt.xlim(0)\n\nkde.xaxis.set_major_locator(ticker.MultipleLocator(4))\nkde.xaxis.set_major_formatter(ticker.ScalarFormatter())\n\nfig.suptitle(\"FARE BY SURVIVED - CLIPPED TO REMOVE OUTLIERS\", x=0.12, y=1.01, ha='left', \n             fontweight=100, fontfamily='Segoe UI', size=37);","9c90e2d9":"fig = plt.figure(figsize=(20,8))\n\ndist = sns.histplot(df_train[(df_train.Fare > 0) & (df_train.Fare <=180)]['Fare'],\n                    color=\"gold\", \n                    kde=True, \n                    bins=50, \n                    label='Train')\n\ndist = sns.histplot(df_test[(df_test.Fare > 0) & (df_test.Fare <=180)]['Fare'],\n                    color=\"crimson\", \n                    kde=True, \n                    bins=50, \n                    label='Test')\n\ntitle = fig.suptitle(\"DISTRIBUTION OF FARE IN TRAIN & TEST\", \n                     x=0.12, \n                     y=1.01, \n                     ha='left',\n                     fontweight=100, \n                     fontfamily='Segoe UI', \n                     size=37)\n\nplt.xlim(0)\n\ndist.xaxis.set_major_locator(ticker.MultipleLocator(4))\ndist.xaxis.set_major_formatter(ticker.ScalarFormatter())\n\nplt.legend()\nplt.show()","a3c6627a":"c1 = sns.catplot(x=\"Pclass\", \n                 hue=\"Survived\", \n                 kind=\"count\", \n                 data=df_train,\n                 aspect = 3.5, \n                 legend=True, \n                 palette=\"YlGnBu\")\n\ntitle = c1.fig.suptitle(\"COUNT BY PCLASS\", \n                        x=0.04, \n                        y=1.12, \n                        ha='left', \n                        fontweight=100, \n                        fontfamily='Segoe UI', \n                        size=42)","0ff6c3b7":"c1 = sns.catplot(x=\"Sex\", \n                 hue=\"Survived\", \n                 kind=\"count\", \n                 data=df_train,\n                 aspect = 3.5, \n                 legend=True, \n                 palette=\"YlGnBu\")\n\ntitle = c1.fig.suptitle(\"COUNT BY SEX\", \n                        x=0.04, \n                        y=1.12, \n                        ha='left', \n                        fontweight=100, \n                        fontfamily='Segoe UI', \n                        size=42)","95863615":"g = sns.catplot(x=\"Sex\", y=\"Survived\", data=df_train,kind=\"bar\", palette = \"YlGnBu\")\ng.set_ylabels(\"Survival probability\")","f2a57104":"c1 = sns.catplot(x=\"SibSp\", \n                 hue=\"Survived\", \n                 kind=\"count\", \n                 data=df_train,\n                 aspect = 3.5, \n                 legend=True, \n                 palette=\"YlGnBu\")\n\ntitle = c1.fig.suptitle(\"COUNT BY SibSp\", \n                        x=0.04, \n                        y=1.12, \n                        ha='left', \n                        fontweight=100, \n                        fontfamily='Segoe UI', \n                        size=42)","05df7f22":"c1 = sns.catplot(x=\"Parch\", \n                 hue=\"Survived\", \n                 kind=\"count\", \n                 data=df_train,\n                 aspect = 3.5, \n                 legend=True, \n                 palette=\"YlGnBu\")\n\ntitle = c1.fig.suptitle(\"COUNT BY Parch\", \n                        x=0.04, \n                        y=1.12, \n                        ha='left', \n                        fontweight=100, \n                        fontfamily='Segoe UI', \n                        size=42)","1ffa4514":"c1 = sns.catplot(x=\"Embarked\", \n                 hue=\"Survived\", \n                 kind=\"count\", \n                 data=df_train,\n                 aspect = 3.5, \n                 legend=True, \n                 palette=\"YlGnBu\")\n\ntitle = c1.fig.suptitle(\"COUNT BY Embarked\", \n                        x=0.04, \n                        y=1.12, \n                        ha='left', \n                        fontweight=100, \n                        fontfamily='Segoe UI', \n                        size=42)","e1d1346a":"# Let's merge train and test for future feature engineering\nfull_df = pd.concat([df_train, df_test]).reset_index(drop=True)\n\n# This is a validation sample, so we can avoid overfitting\ndf_train_test = df_train.sample(frac=0.2,random_state=123)\ny_train_test = df_train_test[[\"Survived\", \"PassengerId\"]]\ndf_train_test = df_train_test.drop([\"Survived\"], axis=1)\nlist_index = df_train_test.index.values.tolist()\ndf_train_train = df_train[~df_train.index.isin(list_index)]\nfull_df_model = pd.concat([df_train_test, df_train_train])\n\ntrain_shape = df_train.shape\ntest_shape = df_test.shape","3176ee1c":"print(full_df.shape, df_train.shape, df_train_train.shape, df_train_test.shape, full_df_model.shape)","b736bc5d":"full_df.loc[:, 'Sex'] = (full_df.loc[:, 'Sex'] == 'female').astype(int)","6ea6769f":"full_df.Name.head()","53f8f46a":"full_df[\"Title\"] = full_df[\"Name\"]\nfull_df[\"Title\"] = full_df[\"Name\"].str.extract(\"([A-Za-z]+)\\.\", expand=True)\n\nc1 = sns.catplot(x=\"Title\", hue=\"Survived\", kind=\"count\", data=full_df[:train_shape[0]],\n                 aspect = 3.5, legend=True, palette=\"YlGnBu\")\n\ntitle = c1.fig.suptitle(\"COUNT BY TITLE\", x=0.04, y=1.12, ha='left',\n             fontweight=100, fontfamily='Segoe UI', size=42)\n\n# Replacing rare titles \nmapping = {'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs', 'Major': 'Other', \n           'Col': 'Other', 'Dr' : 'Other', 'Rev' : 'Other', 'Capt': 'Other', \n           'Jonkheer': 'Royal', 'Sir': 'Royal', 'Lady': 'Royal', \n           'Don': 'Royal', 'Countess': 'Royal', 'Dona': 'Royal'}\n           \nfull_df.replace({'Title': mapping}, inplace=True)\n\nc2 = sns.catplot(x=\"Title\", hue=\"Survived\", kind=\"count\", data=full_df[:train_shape[0]],\n                 aspect = 3.5, legend=True, palette=\"YlGnBu\")\nc2.fig.suptitle(\"COUNT BY TITLE AGGREGATED\", x=0.04, y=1.12, ha='left',\n             fontweight=100, fontfamily='Segoe UI', size=42);","4db530c4":"full_df[\"Name_Length\"] = full_df.Name.str.replace(\"[^a-zA-Z]\", \"\").str.len()\n\nfig, ax = plt.subplots(ncols=1, figsize=(20,8))\nkde = sns.kdeplot(x=\"Name_Length\", data=full_df[:train_shape[0]], cut=True,\n                  hue=\"Survived\", fill=True, ax=ax, palette=\"mako_r\")\n\nkde.xaxis.set_major_locator(ticker.MultipleLocator(1))\nkde.xaxis.set_major_formatter(ticker.ScalarFormatter())\n\nfig.suptitle(\"NAME_LENGTH BY SURVIVED\", x=0.125, y=1.01, ha='left',\n             fontweight=100, fontfamily='Segoe UI', size=42);","9198fefc":"full_df['Title_C'] = full_df['Title']\n\nfull_df['Embarked'] = full_df['Embarked'].fillna('S')\nfull_df = pd.get_dummies(full_df, columns=[\"Embarked\",\"Title_C\"],prefix=[\"Emb\",\"Title\"], drop_first=False)\n\ntitle_dict = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Other': 4, 'Royal': 5, 'Master': 6}\nfull_df['Title'] = full_df['Title'].map(title_dict).astype('int')","1bd082ba":"full_df['Family_Size'] = full_df['Parch'] + full_df['SibSp'] + 1\nfull_df['Fsize_Cat'] = full_df['Family_Size'].map(lambda val: 'Alone' if val <= 1 else ('Small' if val < 5 else 'Big'))\nfull_df[\"isAlone\"] = full_df.Family_Size.apply(lambda x: 1 if x==1 else 0)","0909f976":"fig, ax = plt.subplots(ncols=1, figsize=(20,8))\nkde = sns.kdeplot(x=\"Family_Size\", data=full_df[:train_shape[0]], cut=True,\n                  hue=\"Survived\", fill=True, ax=ax, palette=\"mako_r\")\n\nkde.xaxis.set_major_locator(ticker.MultipleLocator(1))\nkde.xaxis.set_major_formatter(ticker.ScalarFormatter())\n\nplt.xlim(1)\n\nfig.suptitle(\"FAMILY SIZE BY SURVIVED\", x=0.125, y=1.01, ha='left',\n             fontweight=100, fontfamily='Segoe UI', size=42);","6ce8d131":"c1 = sns.catplot(x=\"Fsize_Cat\", \n                 hue=\"Survived\", \n                 kind=\"count\", \n                 data=full_df[:train_shape[0]],\n                 aspect = 3.5, \n                 legend=True, \n                 palette=\"YlGnBu\")\n\ntitle = c1.fig.suptitle(\"COUNT BY Fsize_Cat\", \n                        x=0.04, \n                        y=1.12, \n                        ha='left', \n                        fontweight=100, \n                        fontfamily='Segoe UI', \n                        size=42)","5e1c7a6e":"Fsize_dict = {'Alone':3, 'Small':2, 'Big':1}\nfull_df['Fsize_Cat'] = full_df['Fsize_Cat'].map(Fsize_dict).astype('int')","c8376160":"full_df['Family_Name'] = full_df['Name'].str.extract('([A-Za-z]+.[A-Za-z]+)\\,', expand=True)\nfull_df_model['Family_Name'] = full_df_model['Name'].str.extract('([A-Za-z]+.[A-Za-z]+)\\,', expand=True)","5659503a":"def create_family_rate(df, train):\n    MEAN_SURVIVAL_RATE = round(np.mean(train['Survived']), 4)\n\n    df['Family_Friends_Surv_Rate'] = MEAN_SURVIVAL_RATE\n    df['Surv_Rate_Invalid'] = 1\n    \n    for _, grp_df in df[['Survived', 'Family_Name', 'Fare', 'Ticket', 'PassengerId']].groupby(['Family_Name', 'Fare']):                       \n        if (len(grp_df) > 1):\n            if(grp_df['Survived'].isnull().sum() != len(grp_df)):\n                for ind, row in grp_df.iterrows():\n                    df.loc[df['PassengerId'] == row['PassengerId'],\n                                'Family_Friends_Surv_Rate'] = round(grp_df['Survived'].mean(), 4)\n                    df.loc[df['PassengerId'] == row['PassengerId'],\n                                'Surv_Rate_Invalid'] = 0\n\n    for _, grp_df in df[['Survived', 'Family_Name', 'Fare', 'Ticket', 'PassengerId', \n                              'Family_Friends_Surv_Rate']].groupby('Ticket'):\n        if (len(grp_df) > 1):\n            for ind, row in grp_df.iterrows():\n                if (row['Family_Friends_Surv_Rate'] == 0.) | (row['Family_Friends_Surv_Rate'] == MEAN_SURVIVAL_RATE):\n                    if(grp_df['Survived'].isnull().sum() != len(grp_df)):\n                        df.loc[full_df['PassengerId'] == row['PassengerId'],\n                                    'Family_Friends_Surv_Rate'] = round(grp_df['Survived'].mean(), 4)\n                        df.loc[full_df['PassengerId'] == row['PassengerId'],\n                                    'Surv_Rate_Invalid'] = 0\n                        \n    return df","b120dfe5":"full_df = create_family_rate(full_df, df_train)\nfull_df_model = create_family_rate(full_df_model, df_train_train)","63257930":"full_df = full_df.drop([\"Name\", \"Family_Name\"], axis=1)\nfull_df_model = full_df_model.drop([\"Name\", \"Family_Name\"], axis=1)","527e72aa":"# Replace missing values with 'U' for Cabin\nfull_df['Cabin_Clean'] = full_df['Cabin'].fillna('U')\nfull_df['Cabin_Clean'] = full_df['Cabin_Clean'].str.strip(' ').str[0]\n\n# Label Encoding\ncabin_dict = {'A':9, 'B':8, 'C':7, 'D':6, 'E':5, 'F':4, 'G':3, 'T':2, 'U':1}\nfull_df['Cabin_Clean'] = full_df['Cabin_Clean'].map(cabin_dict).astype('int')\nfull_df.drop([\"Cabin\"], axis=1, inplace=True)","ea029e12":"c1 = sns.catplot(x=\"Cabin_Clean\", \n                 hue=\"Survived\", \n                 kind=\"count\", \n                 data=full_df[:train_shape[0]],\n                 aspect = 3.5, \n                 legend=True, \n                 palette=\"YlGnBu\")\n\ntitle = c1.fig.suptitle(\"CABIN_CLEAN BY SURVIVED\", \n                        x=0.04, \n                        y=1.12, \n                        ha='left', \n                        fontweight=100, \n                        fontfamily='Segoe UI', \n                        size=42)","25ab6a6a":"def clean_ticket(each_ticket):\n    prefix = re.sub(r'[^a-zA-Z]', '', each_ticket)\n    if(prefix):\n        return prefix\n    else:\n        return \"NUM\"\n\nfull_df[\"Tkt_Clean\"] = full_df.Ticket.apply(clean_ticket)\n\nfig, ax = plt.subplots(ncols=1, figsize=(23,8))\nsns.countplot(x=\"Tkt_Clean\", data=full_df[:train_shape[0]], hue=\"Survived\", fill=True, ax=ax, palette=\"YlGnBu\")\nfig.suptitle(\"TKT_CLEAN BY SURVIVED\", x=0.125, y=1.01, ha='left',\n             fontweight=100, fontfamily='Segoe UI', size=42);","010343e6":"full_df['Ticket_Frequency'] = full_df.groupby('Ticket')['Ticket'].transform('count')\nfull_df.drop([\"Ticket\"], axis=1, inplace=True)\nfig, ax = plt.subplots(ncols=1, figsize=(23,8))\nsns.countplot(x=\"Ticket_Frequency\", data=full_df[:train_shape[0]], hue=\"Survived\", fill=True, ax=ax, palette=\"YlGnBu\")\n\nfig.suptitle(\"TICKET_FREQUENCY BY SURVIVED\", x=0.125, y=1.01, ha='left',\n             fontweight=100, fontfamily='Lato', size=42);","f149d536":"full_df = pd.get_dummies(full_df, columns=[\"Tkt_Clean\"], prefix=[\"Tkt\"], drop_first=True)","a0052477":"def fare_cat(fare):\n    if fare <= 7.0:\n        return 1\n    elif fare <= 39 and fare > 7.0:\n        return 2\n    else:\n        return 3\n\nfull_df.loc[:, 'Fare_Cat'] = full_df['Fare'].apply(fare_cat).astype('int')","918b9142":"full_df.loc[:, 'Fare_Family_Size'] = full_df['Fare'] \/ full_df['Family_Size']\n\nfull_df.loc[:, 'Fare_Cat_Pclass'] = full_df['Fare_Cat'] * full_df['Pclass']\nfull_df.loc[:, 'Fare_Cat_Title'] = full_df['Fare_Cat'] * full_df['Title']\n\nfull_df.loc[:, 'Fsize_Cat_Title'] = full_df['Fsize_Cat'] * full_df['Title']\nfull_df.loc[:, 'Fsize_Cat_Fare_Cat'] = full_df['Fare_Cat'] \/ full_df['Fsize_Cat'].astype('int')\n\nfull_df.loc[:, 'Pclass_Title'] = full_df['Pclass'] * full_df['Title']\nfull_df.loc[:, 'Fsize_Cat_Pclass'] = full_df['Fsize_Cat'] * full_df['Pclass']","ff300460":"colsToRemove = []\ncols = ['Tkt_AQ', 'Tkt_AS', 'Tkt_C', 'Tkt_CA',\n         'Tkt_CASOTON', 'Tkt_FC', 'Tkt_FCC', 'Tkt_Fa', 'Tkt_LINE', 'Tkt_LP',\n         'Tkt_NUM', 'Tkt_PC', 'Tkt_PP', 'Tkt_PPP', 'Tkt_SC', 'Tkt_SCA',\n         'Tkt_SCAH', 'Tkt_SCAHBasle', 'Tkt_SCOW', 'Tkt_SCPARIS', 'Tkt_SCParis',\n         'Tkt_SOC', 'Tkt_SOP', 'Tkt_SOPP', 'Tkt_SOTONO', 'Tkt_SOTONOQ',\n         'Tkt_SP', 'Tkt_STONO', 'Tkt_STONOQ', 'Tkt_SWPP', 'Tkt_WC', \n         'Tkt_WEP', 'Fare_Cat', 'Fare_Family_Size', 'Fare_Cat_Pclass',\n         'Fare_Cat_Title', 'Fsize_Cat_Title', 'Fsize_Cat_Fare_Cat', \n         'Pclass_Title', 'Fsize_Cat_Pclass']\n\nfor col in cols:\n    if full_df[col][:train_shape[0]].std() == 0: \n        colsToRemove.append(col)\n\nfull_df.drop(colsToRemove, axis=1, inplace=True)\nprint(\"Removed {} Constant Columns\\n\".format(len(colsToRemove)))\nprint(colsToRemove)","4c5a78d0":"features = [\"Survived\",'Family_Friends_Surv_Rate','Surv_Rate_Invalid']\ndf = full_df.copy()\ndf.loc[df.PassengerId.isin(full_df_model.PassengerId), features] = full_df_model[features]\npassenger_list = full_df_model[\"PassengerId\"].tolist()\nfull_df_model = df[df[\"PassengerId\"].isin(passenger_list)]","142b08a5":"imp_features = ['Pclass', \n                'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Title',\n                 'Name_Length',\n                'Emb_C', 'Emb_Q', 'Emb_S','Family_Size',\n                 'Fsize_Cat', 'Family_Friends_Surv_Rate', 'Surv_Rate_Invalid',\n                 'Cabin_Clean','Ticket_Frequency', 'Tkt_AS', 'Tkt_C', 'Tkt_CA',\n                 'Tkt_CASOTON', 'Tkt_FC', 'Tkt_FCC', 'Tkt_Fa', 'Tkt_LINE',\n                 'Tkt_NUM', 'Tkt_PC', 'Tkt_PP', 'Tkt_PPP', 'Tkt_SC', 'Tkt_SCA',\n                 'Tkt_SCAH', 'Tkt_SCAHBasle', 'Tkt_SCOW', 'Tkt_SCPARIS', 'Tkt_SCParis',\n                 'Tkt_SOC', 'Tkt_SOP', 'Tkt_SOPP', 'Tkt_SOTONO', 'Tkt_SOTONOQ',\n                 'Tkt_SP', 'Tkt_STONO', 'Tkt_SWPP', 'Tkt_WC', \n                 'Tkt_WEP', 'Fare_Cat', 'Fare_Family_Size', 'Fare_Cat_Pclass',\n                 'Fare_Cat_Title', 'Fsize_Cat_Title', 'Fsize_Cat_Fare_Cat', \n                 'Pclass_Title', 'Fsize_Cat_Pclass']\n\nimputer = impute.KNNImputer(n_neighbors=10, missing_values=np.nan)\nimputer.fit(full_df[imp_features])\nfull_df.loc[:, imp_features] = pd.DataFrame(imputer.transform(full_df[imp_features]), \n                                            index=full_df.index, columns = imp_features)","76faba4e":"imputer = impute.KNNImputer(n_neighbors=10, missing_values=np.nan)\nimputer.fit(full_df_model[imp_features])\nfull_df_model.loc[:, imp_features] = pd.DataFrame(imputer.transform(full_df_model[imp_features]), \n                                            index=full_df_model.index, columns = imp_features)","2fef05a2":"# def create_optmal_binning(df):\n#     optb = OptimalBinning(name=\"Age\", dtype=\"numerical\", solver=\"cp\")\n#     x = df[:train_shape[0]][\"Age\"].values\n#     y_train = df[:train_shape[0]][\"Survived\"]\n#     y = y_train[y_train.index.isin(df_train.index)]\n#     optb.fit(x, y)\n    \n#     return optb\n    \n# def transform_variable(df):\n#     list_index = df.index.values.tolist()\n#     col = df[\"Age\"].values\n#     x_transform = optb.transform(col, metric=\"event_rate\")\n#     x_transform = pd.Series(x_transform, index=list_index)\n#     x_transform.value_counts()\n#     x_transform = x_transform.rename(\"Age_Band\")\n#     df = pd.concat((df, x_transform), axis=1)\n    \n#     return df","6c25c6f3":"# optb = create_optmal_binning(full_df)","dbef6371":"# binning_table = optb.binning_table\n# binning_table.build()","33931583":"# full_df = transform_variable(full_df)","39f6bf26":"# optb = create_optmal_binning(full_df_model)\n# full_df_model = transform_variable(full_df_model)","c734aad5":"full_df['Child'] = full_df['Age'].map(lambda val:1 if val<18 else 0)\nfull_df['Senior'] = full_df['Age'].map(lambda val:1 if val>70 else 0)","4d26b105":"fig, ax = plt.subplots(ncols=2, figsize=(23,8))\n\nsns.countplot(x=\"Child\", data=full_df[:train_shape[0]], hue=\"Survived\", fill=True, ax=ax[0], palette=\"YlGnBu\")\nsns.countplot(x=\"Senior\", data=full_df[:train_shape[0]], hue=\"Survived\", fill=True, ax=ax[1], palette=\"YlGnBu\")","7db74523":"from sklearn.preprocessing import StandardScaler\n\nscaler_cols = ['Age', 'Fare', 'Name_Length', 'Family_Size',\n               'Ticket_Frequency', 'Fare_Family_Size', 'Fare_Cat_Pclass']\nstd = StandardScaler()\nstd.fit(full_df[scaler_cols])","65608411":"df_std = pd.DataFrame(std.transform(full_df[scaler_cols]), index=full_df.index, columns = scaler_cols)\nfull_df.drop(scaler_cols, axis=1, inplace=True)\nfull_df = pd.concat((full_df, df_std), axis=1)","e6447932":"features = [\"Survived\",'Family_Friends_Surv_Rate','Surv_Rate_Invalid']\ndf = full_df.copy()\ndf.loc[df.PassengerId.isin(full_df_model.PassengerId), features] = full_df_model[features]\npassenger_list = full_df_model[\"PassengerId\"].tolist()\nfull_df_model = df[df[\"PassengerId\"].isin(passenger_list)]","7185a37e":"df_train_final = full_df[:train_shape[0]]\ndf_test_final = full_df[train_shape[0]:]","008be4cd":"df_test_final.drop([\"Survived\"], axis=1, inplace=True)","e8f38d7f":"df_train_final.head()","a594b8a7":"corr_mat = df_train_final.astype(float).corr()\ncorr_mat_fil = corr_mat.loc[:, 'Survived'].sort_values(ascending=False)\ncorr_mat_fil = pd.DataFrame(data=corr_mat_fil[1:])","0fef19dc":"plt.figure(figsize=(15,14))\nbar = sns.barplot(x=corr_mat_fil.Survived, y=corr_mat_fil.index, data=corr_mat_fil, palette=\"Spectral\")\ntitle = bar.set_title(\"FEATURE CORRELATION\", x=0.0, y=1.01, ha='left',\n             fontweight=100, fontfamily='Segoe UI', size=30)","c425f40c":"df_corr = df_train_final.drop([\"PassengerId\"], axis=1)\ncorrmat = df_corr.corr()\nsorted_corrs = corrmat['Survived'].abs().sort_values(ascending=False)\nprint(sorted_corrs)","7755acd2":"corr = df_train_final.corr()\ntop_corr_cols = corr[abs((corr.Survived)>=.0)].Survived.sort_values(ascending=False).keys()\ntop_corr = corr.loc[top_corr_cols, top_corr_cols]\ndropSelf = np.zeros_like(top_corr)\ndropSelf[np.triu_indices_from(dropSelf)] = True\nplt.figure(figsize=(13, 13))\nsns.heatmap(top_corr, cmap=sns.diverging_palette(220, 10, as_cmap=True), annot=False, fmt=\".2f\", mask=dropSelf)\nplt.show()","53a8d7fb":"X = df_train_final.drop([\"Survived\", \"PassengerId\"], axis=1)\nX = X.assign(const=1)\ncalc_vif(X)","8789aef1":"passenger_train = df_train_train[\"PassengerId\"].tolist()\ndf_train = full_df_model[full_df_model[\"PassengerId\"].isin(passenger_train)]","431eb94f":"passenger_test = df_train_test[\"PassengerId\"].tolist()\ndf_test = full_df_model[full_df_model[\"PassengerId\"].isin(passenger_test)]\ndf_test.loc[df_test.PassengerId.isin(y_train_test.PassengerId), \"Survived\"] = y_train_test[\"Survived\"]","60928359":"X_train = df_train.drop([\"Survived\", \"PassengerId\"], axis=1)\ny_train = df_train[\"Survived\"]\nX_test = df_test.drop([\"Survived\", \"PassengerId\"], axis=1)\ny_test = df_test[\"Survived\"]","dd944a45":"print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","748d8778":"all_passenger = passenger_train + passenger_test\ndf_train_final = full_df[full_df[\"PassengerId\"].isin(all_passenger)]\ndf_test_final = full_df[~full_df[\"PassengerId\"].isin(all_passenger)]","a4bf793f":"k_fold = model_selection.KFold(n_splits=10, shuffle=True, random_state=0)","8c7dedb7":"from sklearn.metrics import accuracy_score, make_scorer\n\ndef get_kfold_accuracy(model):\n    score = model_selection.cross_val_score(model, X_train, y_train, cv=k_fold, scoring=\"accuracy\")\n    print(\"KFold Score:\", round(np.mean(score) * 100, 2))\n    \n    return score\n\ndef get_accuracy(prediction):\n    score = round(accuracy_score(prediction, y_test)*100,2)\n    print(\"Accuracy\", score)\n    \n    return score\n","95a40cc7":"ada_boost = ensemble.AdaBoostClassifier()\nada_boost.fit(X_train, y_train)\nprediction = ada_boost.predict(X_test)\nada_boost_score = get_accuracy(prediction)","79570a35":"bagging = ensemble.BaggingClassifier()\nbagging.fit(X_train, y_train)\nprediction = bagging.predict(X_test)\nbagging_score = get_accuracy(prediction)","63ac52f2":"gradient_boosting = ensemble.GradientBoostingClassifier()\ngradient_boosting.fit(X_train, y_train)\nprediction = gradient_boosting.predict(X_test)\ngradient_boosting_score = get_accuracy(prediction)","0e8b2acd":"extra_trees = ensemble.ExtraTreesClassifier()\nextra_trees.fit(X_train, y_train)\nprediction = extra_trees.predict(X_test)\nextra_trees_score = get_accuracy(prediction)","eb71c641":"random_forest = ensemble.RandomForestClassifier()\nrandom_forest.fit(X_train, y_train)\nprediction = random_forest.predict(X_test)\nrandom_forest_score = get_accuracy(prediction)","12fee390":"gaussian_pr = gaussian_process.GaussianProcessClassifier()\ngaussian_pr.fit(X_train, y_train)\nprediction = gaussian_pr.predict(X_test)\ngaussian_pr_score = get_accuracy(prediction)","8ad4a33a":"logistic_regression_cv = linear_model.LogisticRegressionCV(max_iter=100000)\nlogistic_regression_cv.fit(X_train, y_train)\nprediction = logistic_regression_cv.predict(X_test)\nlogistic_regression_cv_score = get_accuracy(prediction)","9c886c04":"logistic_regression = linear_model.LogisticRegression(random_state=1, max_iter=10000)\nlogistic_regression.fit(X_train, y_train)\nprediction = logistic_regression.predict(X_test)\nlogistic_regression_score = get_accuracy(prediction)","f5ad2aaa":"ridge = linear_model.RidgeClassifierCV()\nridge.fit(X_train, y_train)\nprediction = ridge.predict(X_test)\nridge_score = get_accuracy(prediction)","1786509c":"perceptron = linear_model.Perceptron()\nperceptron.fit(X_train, y_train)\nprediction = perceptron.predict(X_test)\nperceptron_score = get_accuracy(prediction)","159391ba":"passive_aggressive = linear_model.PassiveAggressiveClassifier()\npassive_aggressive.fit(X_train, y_train)\nprediction = passive_aggressive.predict(X_test)\npassive_aggressive_score = get_accuracy(prediction)","548124f9":"sdg = linear_model.SGDClassifier()\nsdg.fit(X_train, y_train)\nprediction = sdg.predict(X_test)\nsdg_score = get_accuracy(prediction)","2f85a2e5":"gaussian_nb = naive_bayes.GaussianNB()\ngaussian_nb.fit(X_train, y_train)\nprediction = gaussian_nb.predict(X_test)\ngaussian_nb_score = get_accuracy(prediction)","e96d1022":"bernoulli_nb = naive_bayes.BernoulliNB()\nbernoulli_nb.fit(X_train, y_train)\nprediction = bernoulli_nb.predict(X_test)\nbernoulli_nb_score = get_accuracy(prediction)","ca0a7b56":"knn = neighbors.KNeighborsClassifier(n_neighbors = 13)\nknn.fit(X_train, y_train)\nprediction = knn.predict(X_test)\nknn_score = get_accuracy(prediction)","82912007":"svc = svm.SVC(random_state=1, kernel='linear')\nsvc.fit(X_train, y_train)\nprediction = svc.predict(X_test)\nsvc_score = get_accuracy(prediction)","aac45db5":"svc_linear = svm.LinearSVC(random_state=1, max_iter=100000)\nsvc_linear.fit(X_train, y_train)\nprediction = svc_linear.predict(X_test)\nsvc_linear_score = get_accuracy(prediction)","c2143cf9":"svc_nu = svm.NuSVC(probability=True)\nsvc_nu.fit(X_train, y_train)\nprediction = svc_nu.predict(X_test)\nsvc_nu_score = get_accuracy(prediction)","7d4d30ab":"decision_tree = tree.DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\nprediction = decision_tree.predict(X_test)\ndecision_tree_score = get_accuracy(prediction)","5ce70be5":"linear_discriminant = discriminant_analysis.LinearDiscriminantAnalysis()\nlinear_discriminant.fit(X_train, y_train)\nprediction = linear_discriminant.predict(X_test)\nlinear_discriminant_score = get_accuracy(prediction)","819e3946":"xgboost = XGBClassifier(random_state=1, objective=\"binary:logistic\", \n                        n_estimators=10, eval_metric='mlogloss', use_label_encoder=False)\nxgboost.fit(X_train, y_train)\nprediction = xgboost.predict(X_test)\nxgboost_score = get_accuracy(prediction)","ddc72961":"from tensorflow.keras.metrics import *\n\ndef create_model():\n    metrics = ['accuracy', Precision(), Recall()]\n    model = Sequential()\n    model.add(Input(shape=X_train.shape[1], name='Input_'))\n    model.add(layers.Dense(8, activation='relu', kernel_initializer='glorot_normal', kernel_regularizer=regularizers.l2(0.001)))\n    model.add(layers.Dense(16, activation='relu', kernel_initializer='glorot_normal', kernel_regularizer=regularizers.l2(0.1)))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(16, activation='relu', kernel_initializer='glorot_normal', kernel_regularizer=regularizers.l2(0.1)))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(1, activation='sigmoid', kernel_initializer='glorot_normal'))\n\n    optimize = optimizers.Adam(lr = 0.0001)\n    model.compile(optimizer = optimize,loss = 'binary_crossentropy',metrics = metrics)\n    \n    return model","4a33874a":"keras = wrappers.scikit_learn.KerasClassifier(build_fn = create_model, epochs = 600, batch_size = 32, verbose = 0)\nkeras.fit(X_train, y_train)\nprediction = keras.predict(X_test)\nkeras_score = get_accuracy(prediction)","bb396f49":"model_performance = pd.DataFrame({\n    \"Model\": [\"Ada Boost\", \n              \"Bagging\", \n              \"Keras\", \n              \"XGBClassifier\", \n              \"Linear Discriminant Analysis\", \n              \"Extra Tree\",  \n              \"Decision Tree\", \n              \"SVM Nu\",\n             \"SVM Linear\",\n             \"SVM\",\n             \"kNN\",\n             \"Bernoulli Naive Bayes\",\n             \"Gaussian Naive Bayes\",\n             \"SDG\",\n             \"Passive Aggressive\",\n             \"Perceptron\",\n             \"Ridge\",\n             \"Logistic Regression\",\n             \"Logistic Regression CV\",\n             \"Gaussian Process\",\n             \"Random Forest\",\n             \"Gradient Boosting\"],\n    \n    \"Accuracy\": [ada_boost_score, \n                 bagging_score, \n                 keras_score,\n                xgboost_score,\n                linear_discriminant_score,\n                extra_trees_score,\n                decision_tree_score,\n                svc_nu_score,\n                svc_linear_score,\n                svc_score,\n                knn_score,\n                bernoulli_nb_score,\n                gaussian_nb_score,\n                sdg_score,\n                passive_aggressive_score,\n                perceptron_score,\n                ridge_score,\n                logistic_regression_score,\n                logistic_regression_cv_score,\n                gaussian_pr_score,\n                random_forest_score,\n                 gradient_boosting_score,\n                ]\n})\n\nmodel_performance.sort_values(by=\"Accuracy\", ascending=False)","0b5a7c2e":"estimators = [('Gaussian Process',gaussian_pr), \n              ('Linear Discriminant', linear_discriminant),\n              ('kNN', knn)]\n\nstack = ensemble.StackingClassifier(estimators=estimators)\nstack.fit(X_train, y_train)\nprediction = stack.predict(X_test)\nstack_score = get_accuracy(prediction)","8b5af330":"voting = ensemble.VotingClassifier(\n    estimators = [\n        ('Gaussian Process',gaussian_pr),\n        ('Linear Discriminant Analysis',linear_discriminant),\n        (\"SVM Nu\", svc_nu),\n        (\"Knn\", knn),\n],\n    voting = 'hard'\n)","4e9fbfc9":"voting.fit(X_train, y_train)\nprediction = voting.predict(X_test)\nvoting_score = get_accuracy(prediction)","1f439dab":"rf_clf = linear_model.LogisticRegression(random_state=1)\n\nparameters = {\n    'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n    'C' : np.logspace(1, -1),\n    'solver' : ['liblinear', \"newton-cg\", \"lbfgs\", \"sag\", \"saga\"]\n}\n\ngrid_cv = model_selection.GridSearchCV(rf_clf, parameters, scoring = make_scorer(accuracy_score))\ngrid_cv = grid_cv.fit(X_train, y_train)\n\nbest_estimator = grid_cv.best_estimator_\nbest_score = grid_cv.best_score_\nbest_params = grid_cv.best_params_\n\nbest_params","1f3e881a":"all_passenger = passenger_train + passenger_test\ndf_train_final = full_df[full_df[\"PassengerId\"].isin(all_passenger)]\nX_train = df_train_final.drop([\"PassengerId\", \"Survived\"], axis=1)\ny_train = df_train_final[\"Survived\"]\n\ndf_test_final = full_df[~full_df[\"PassengerId\"].isin(all_passenger)]\nX_test = df_test_final.drop([\"PassengerId\", \"Survived\"], axis=1)","3dfc77c7":"keras = wrappers.scikit_learn.KerasClassifier(build_fn = create_model, epochs = 600, batch_size = 32, verbose = 0)\nkeras.fit(X_train, y_train)\nprediction = keras.predict(X_test)\ny_pred = []\nfor y in prediction:\n    y_pred.append(y[0])","23da3203":"gaussian_pr = gaussian_process.GaussianProcessClassifier()\ngaussian_pr.fit(X_train, y_train)\n\nlinear_discriminant = discriminant_analysis.LinearDiscriminantAnalysis()\nlinear_discriminant.fit(X_train, y_train)\n\nknn = neighbors.KNeighborsClassifier(n_neighbors = 13)\nknn.fit(X_train, y_train)\n\nestimators = [('Gaussian Process',gaussian_pr), \n              ('Linear Discriminant', linear_discriminant),\n              ('kNN', knn)]\n\nstack = ensemble.StackingClassifier(estimators=estimators)\nstack.fit(X_train, y_train)\ny_pred = stack.predict(X_test)","63d9161a":"voting = ensemble.VotingClassifier(\n    estimators = [\n        ('Gaussian Process',gaussian_pr),\n                  ('Linear Discriminant Analysis',linear_discriminant),\n                  (\"SVM Nu\", svc_nu),\n        (\"Knn\", knn),\n                 ],\n    voting = 'hard'\n)\n\nvoting.fit(X_train, y_train)\nprediction = voting.predict(X_test)\ny_pred = stack.predict(X_test)","93f49257":"submission = pd.DataFrame({ \n    \"PassengerId\": df_test_final[\"PassengerId\"],\n    \"Survived\": y_pred\n})\nsubmission.Survived = submission.Survived.astype(int)\nsubmission.to_csv(\"submission.csv\")","dc7444d5":"submission.head(10)","8c71f9d5":"#### 9.2.12. Gaussian Naive Bayes","b4ecec9e":"### 6.4. Split Data back to Train and Test","e962f7a5":"#### 9.2.13. Bernoulli NB","afe2fc13":"#### 2.1.4. Modelling\n* Scikit-learn is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\n\n* TensorFlow is a free and open-source software library for machine learning. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks.","26f9c6a0":"#### 9.2.1. AdaBoost","bee44444":"The wealthier also seem to have a better chance of surviving. We can see that the fares of the people who survived are more distributed, while the distribution of the people who died is concentrated in low fares.","78bbf1ac":"### 3.5. SibSp","e23363b2":"#### 9.2.11. SGDClassifier","3317abea":"### 9.5. Voting","b0952196":"#### 9.2.8. Ridge Classifier","8fdc9b07":"### 8.2. Cross Validation (K-Fold)","0d857fc4":"### 9.1. Model Evaluation","c318e422":"### 3.2. Fare","cd8bf5ee":"Clearly, men have a low survival rate","18e5ba7a":"### 4.12. One-hot Encoding Ticket","7a20b4a1":"Machine learning models require all input and output variables to be numeric.\n\nThis means that if your data contains categorical data, you must encode it to numbers before you can fit and evaluate a model.","47d5277b":"#### Goal\n* The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\n#### Details & Description of Features:\n\n* PassengerID\n* Survived - (0 = No, 1 = Yes)\n* Pclass - Passenger Class (1 = 1st, 2 = 2nd, 3 = 3rd)\n* Name\n* Sex\n* Age\n* SibSp - Number of Siblings\/Spouses Aboard\n* Parch - Number of Parents\/Children Aboard\n* Ticket - Ticket Number\n* Fare - Passenger Fare in British pound\n* Cabin - Cabin Number\n* Embarked - Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)","fe4124c5":"### 6.3. Standard Scaling Data","bb7515e1":"### 10.2. Submitting Using Stack","8838dbf1":"If passengers with the same Last names are present, we group them and attach a calculated survival rate based on the train survival data. \n\nFor no matching last names, the Ticket feature is used to group and calculate the survival rate in the same way.\n\nFrom the data, we see Tickets are given to groups travelling together and they all have the same Ticket number.\n\nOur target is directly influencing this variable. Thus, it is important that we create the variable in our validation dataset to not cause overfitting.","7e7b2ce2":"Look at the output below how the library already grouped the variable for us according to the survival rate!","dc4f65fd":"Remembering: Number of Parents\/Children Aboard\n\nThe idea is the same as SibSp. People who don't have anybody seem to have a low survival rate.","6ad45dbf":"#### 9.2.9. Perceptron","b91c6b58":"### 4.7. Label Encoding Family Size","c44c89f9":"### 4.4. Name Length","e9b1e9d9":"We can observe that children and the elderly are more likely to survive","0af99d39":"#### 9.2.20. XGBoost","e054feb4":"<a id=\"datacleaning\"><\/a>\n#### 2.1.1 Data Cleaning\n* Pandas is a software library for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series.\n* NumPy is a library for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.","1dd2ef71":"### 9.6. Tunning Parameters","b6b9cc3d":"# 6. Final Adjustments","0a707aa3":"### 4.13. Fare into Categorical Bins\n\nKernal density estimation plot of Fare gave us some insights on its distribution and impact on survival. We will use those to add a derived categorical feature from Fare.","b78ec655":"In many machine learning algorithms, to bring all features in the same standing, we need to do scaling so that one significant number doesn\u2019t impact the model just because of their large magnitude.\n\nFeature scaling in machine learning is one of the most critical steps during the pre-processing of data before creating a machine learning model. Scaling can make a difference between a weak machine learning model and a better one.\n\nThe most common techniques of feature scaling are Normalization and Standardization.\n\nNormalization is used when we want to bound our values between two numbers, typically, between [0,1] or [-1,1]. While Standardization transforms the data to have zero mean and a variance of 1.","c5d320f2":"#### 2.1.3. Data Engineering\nI will talk about these classes later!","bd4cdfbc":"One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction. It accepts only Numerical data as input. It takes a column which has categorical data, which has been label encoded and then splits the column into multiple columns. The numbers are replaced by 1s and 0s, depending on which column has what value.\n\nWe use Label Encode technique when the categorical feature is ordinal. In this case, retaining the order is important. Hence encoding should reflect the sequence.","4b5ee07e":"<a id=\"envprep\"><\/a>\n# 2. Environment Preparation","18cc489a":"# 3. Exploratory Data Analysis\nExploratory data analysis is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods","50f9a2ed":"### 9.3. Model Performance","81150cff":"<a id=\"introduction\"><\/a>\n# 1. Introduction","d65eff02":"It is important to keep your main functions in one place to stay organized.","957b9d8c":"#### 9.2.14. kNN","0f3bee9f":"### 3.4. Sex","b807c43c":"Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\n\nThe procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation.\n\nCross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n\nIt is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train\/test split.\n\nThe general procedure is as follows:\n\n1. Shuffle the dataset randomly.\n2. Split the dataset into k groups\n3. For each unique group:\n    * Take the group as a hold out or test data set\n    * Take the remaining groups as a training data set\n    * Fit a model on the training set and evaluate it on the test set\n    * Retain the evaluation score and discard the model\n4. Summarize the skill of the model using the sample of model evaluation scores\n\nImportantly, each observation in the data sample is assigned to an individual group and stays in that group for the duration of the procedure. This means that each sample is given the opportunity to be used in the hold out set 1 time and used to train the model k-1 times.\n\n#### Below we have an example of how to instantiate, but we will not be using it here.","2ae39a76":"### 4.5. One-hot Encode Embarked & Label Encode Title","7cff0bae":"OptBinning is a library written in Python implementing a rigorous and flexible mathematical programming formulation to solving the optimal binning problem for a binary, continuous and multiclass target type, incorporating constraints not previously addressed.\n\nIt is really useful and you can see details here: http:\/\/gnpalencia.org\/optbinning\/","545c816b":"It is Important to save the shape of your train and test data before you start coding. \nYou must have the same number of rows in your test data before your submission. ","bb12b992":"# Before you read","94217f57":"# 11. Credits & References\nhttps:\/\/www.kaggle.com\/sreevishnudamodaran\/ultimate-eda-fe-neural-network-model-top-2\n\nhttps:\/\/en.wikipedia.org\/wiki\/Pandas_(software)\n\nhttps:\/\/en.wikipedia.org\/wiki\/NumPy\n\nhttps:\/\/seaborn.pydata.org\/#\n\nhttps:\/\/en.wikipedia.org\/wiki\/Matplotlib\n\nhttps:\/\/machinelearningmastery.com\/one-hot-encoding-for-categorical-data\/\n\nhttps:\/\/etav.github.io\/python\/vif_factor_python.html\n\nhttps:\/\/towardsdatascience.com\/all-about-feature-scaling-bcc0ad75cb35\n\nhttps:\/\/machinelearningmastery.com\/k-fold-cross-validation\/","a480a339":"### 4.1. Merge Train & Test for Transformation\n","7ec2f682":"### 10.3. Submitting Using Voting","48fb9b5c":"#### 9.2.10. Passive Aggressive Classifier","c5760f3f":"And we can convert the bins to the event rate","59e7e17c":"### 6.1. Create Age Band","c6bf1723":"### 3.3. Pclass","733b9509":"#### 2.1.5. Settings","2aae50f0":"### 4.11. Derive the Ticket Frequency","8c219bc2":"### 4.6. Derive Family Size Feature","26eb7aba":"### 3.1 Age","58dd7c46":"#### 9.2.7. Logistic Regression","b4cc1ff4":"# 9. Model Development","de2e79fe":"# 5. Imputation of Missing Values","c1890abf":"#### VIF\n\nColinearity is the state where two variables are highly correlated and contain similiar information about the variance within a given dataset. To detect colinearity among variables, simply create a correlation matrix and find variables with large absolute values\n\nMulticolinearity on the other hand is more troublesome to detect because it emerges when three or more variables, which are highly correlated, are included within a model. To make matters worst multicolinearity can emerge even when isolated pairs of variables are not colinear.\n\nThe Variance Inflation Factor (VIF) is a measure of colinearity among predictor variables within a multiple regression. It is calculated by taking the the ratio of the variance of all a given model's betas divide by the variane of a single beta if it were fit alone.\n\n* VIF starts at 1 and has no upper limit\n* VIF = 1, no correlation between the independent variable and the other variables\n* VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others\n\n\nMulti-collinearity will not be a problem for most of the classification problems. Once when we have two identical columns, the model (e.g. Random Forest) will automatically \"drop\" one column at each split. So, let's just visualize the output.","68276556":"#### 9.2.15. SVC","4d69743c":"### 4.15. Remove Constant Columns","d76de51d":"<a id=\"libimport\"><\/a>\n### 2.1 Library Imports\nLibraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line.","f25d42e4":"<a id=\"datavisualization\"><\/a>\n#### 2.1.2. Data Visualization\n* Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK. Pyplot is a Matplotlib module which provides a MATLAB-like interface. Matplotlib is designed to be as usable as MATLAB, with the ability to use Python, and the advantage of being free and open-source.\n\n* Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.","946d4a30":"This notebook contains a lot of information from [this notebook](https:\/\/www.kaggle.com\/sreevishnudamodaran\/ultimate-eda-fe-neural-network-model-top-2). I made some changes and improved it, but if you like please also upvote the original notebook.  \n\nMy goal is to describe the step by step of how to create and submit a model, in addition to describing each of these steps. I will still improve. It is not a final version. Over time, I will also update [here](https:\/\/github.com\/mathanssen\/titanic).","94ca24c5":"### 4.3. Title","0fb10131":"# 8. Preparation of Train & Test Data","39bd6b01":"#### 9.2.5. Random Forest","a2ce1391":"Remembering: Embarked - Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n    \nSouthampton seems to be the most dangerous, while people who embarked in Cherbourg have the highest survival rate.","d455d99b":"### 4.9. Cleaning & Encoding of the Cabin","92cf5960":"#### 9.2.18. Decision Tree","0cff2443":"#### 9.2.6. Gaussian Process Classifier","c0bcf21e":"### 4.2. Encoding Sex","ac30f8c9":"#### 9.2.16. Linear SVC","4181cfa4":"We saw earlier that the number of parents and siblings is relevant to know the likelihood of survival. In this way, we will create variables that represent the size of the family and whether the person is alone on the ship.","63367430":"### 8.1. Split the data","347ce8da":"# 4. Feature Engineering & Data Cleaning","e7cf418f":"### 6.2. Obtain Features for Children & Seniors","3ce3492b":"### 9.2. Prediction","3cd4bc49":"### 3.7. Embarked","0c9c5fd9":"#### 9.2.19. Linear Discriminant Analysis","c9181ef3":"The basic intuition behind this feature is that people with longer names tends to be of a higher class and thus would have likely survived. ","01b8389f":"# 7. Checking Feature Importance by Correlation Analysis","60109374":"Once again, we are creating a variable influenced directly by the target. So, let's also create the variable in the dataset where we have our validation data to avoid overfitting.","70505636":"### 2.2. Utils","73aaf955":"### 9.4. Stack","c1d7c2e1":"### 10.1. Submiting Using Keras","f7b56644":"Remembering: Passenger Class (1 = 1st, 2 = 2nd, 3 = 3rd).\n\nWe can observe that the higher the class, the higher the likelihood of survival.","658178af":"### 10.4. Final Submission Adjustments","5b5e3440":"#### 9.2.21. Keras","2d716db9":"Remembering: Number of Siblings\/Spouses Aboard.\n\nPeople who don't have anybody aboard seem to have a low survival rate.","1d1588ad":"<span style=\"font-family: Segoe UI; font-size: 2.5em; font-weight: 300;\">THE TITANIC PROJECT<\/span>\n\n![](https:\/\/static1.squarespace.com\/static\/5006453fe4b09ef2252ba068\/5095eabce4b06cb305058603\/5095eabce4b02d37bef4c24c\/1352002236895\/100_anniversary_titanic_sinking_by_esai8mellows-d4xbme8.jpg)","16934742":"#### 9.2.17. NuSVC","bba7abbf":"### 4.14. Additional Derived Features from Feature Relationships","888a3053":"#### 9.2.3. Gradient Boosting Classifier","194d9ad2":"### 3.6. Parch","e7fae6b3":"\n# 10. Submission","7552ceea":"### 4.8. Extract FamilyName Feature from Name","08eaccaf":"### 4.10. Cleaning the Ticket","47b9e2df":"From the Name variable, we can extract the person's title. So we have data that shows us sex, position and wealth at the same time. The title is a variable that correlates with our dependent variable.","68f18f8d":"### 2.3. Data Imports","c4373907":"#### 9.2.4. Extra Trees Classifier","fa97aeaf":"<span style=\"font-family: Segoe UI; font-size: 2EM; font-weight: 300;\">Summary<\/span>\n* 1. Introduction\n* 2. Environment Preparation\n    - 2.1 Library Imports\n        - 2.2.1 Data Cleaning\n        - 2.2.2 Data Visualization\n        - 2.2.3 Data Engineering\n        - 2.2.4 Data Modelling\n        - 2.2.5 Settings\n    - 2.2 Utils\n    - 2.3 Data Imports\n* 3. A bit of Exploratory Data Analysis\n    - 3.1 Age\n    - 3.2 Fare\n    - 3.3 Pclass\n    - 3.4 Sex\n    - 3.5 SibSp\n    - 3.6 Parch\n    - 3.7 Embarked\n* 4. Feature Engineering & Data Cleaning\n    - 4.1 Merge Train & Test for Transformation\n    - 4.2 Encoding Sex\n    - 4.3 Title\n    - 4.4 Name Length\n    - 4.5 One-hot Encode Embarked & Label Encode Title\n    - 4.6 Family Size\n    - 4.7 Label Encoding Family Size\n    - 4.8 FamilyName\n    - 4.9 Cabin\n    - 4.10 Cleaning Ticket\n    - 4.11 Ticket Frequency\n    - 4.12 One-hot Encoding Ticket\n    - 4.13 Fare into Categorical Bins\n    - 4.14 Additional Derived Features from Feature Relationships\n    - 4.15 Remove Constant Columns\n* 5. Imputation of Missing Values\n* 6. Final Adjustments\n    - 6.1 Create Age Banc\n    - 6.2 Obtain Features for Children & Seniors\n    - 6.3 Standard Scaling Data\n    - 6.4 Split Data Back to Train & Test\n* 7. Checking feature Importance & Correlations\n* 8. Preparation of Train & Test Data\n    - 8.1 Split the Data\n    - 8.2 Cross Validation (K-Fold)\n* 9. Model Development\n    - 9.1 Model Evaluation\n    - 9.2 Prediction\n        - 9.2.1 AdaBoost\n        - 9.2.2 Bagging\n        - 9.2.3 Gradient Boosting\n        - 9.2.4 Extra Trees\n        - 9.2.5 Random Forest\n        - 9.2.6 Gaussian Process\n        - 9.2.7 Logistic Regression\n        - 9.2.8 Ridge\n        - 9.2.9 Perceptron\n        - 9.2.10 Passive Agressive\n        - 9.2.11 SGD\n        - 9.2.12 Gaussian Naive Bayes\n        - 9.2.13 Bernoulli\n        - 9.2.14 K-Nearest Neighbors\n        - 9.2.15 Support Vector Clustering\n        - 9.2.16 Linear SVC\n        - 9.2.17 NuSVC\n        - 9.2.18 Decision Tree\n        - 9.2.19 Linear Discriminant Analysis\n        - 9.2.20 XGBoost\n        - 9.2.21 Keras\n    - 9.3 Model Performance\n    - 9.4 Stack\n    - 9.5 Voting\n    - 9.6 Tunning Parameters\n* 10. Submission\n    - 10.1. Using Stack\n    - 10.2. Using Keras\n    - 10.3. Using Voting\n    - 10.4. Final Adjustments\n* 11. Credits","17d6761f":"#### 9.2.2. Bagging Classifier"}}