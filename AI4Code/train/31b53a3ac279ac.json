{"cell_type":{"b84e4f10":"code","04fa78b3":"code","286887bd":"code","a6c185b4":"code","cc8bf240":"code","4d0ccc6a":"code","30ba0fca":"code","9883c949":"code","8c9a5a61":"code","c2b39f1a":"code","ba255d51":"code","2fbd3b99":"code","09bc6c59":"code","6edd19da":"code","d81636fc":"code","ae1721ac":"code","07d3d575":"code","6f86f12c":"code","2ed136ac":"code","c40f916d":"code","a5afe238":"code","404d90b5":"code","6b018922":"code","6211c527":"code","f5c07337":"code","86d817a8":"code","cfac727f":"code","ea0f9aa2":"code","d60ac07e":"code","8fc6c70a":"code","5938c5a5":"code","0ab97db4":"code","2786b410":"code","e3aecc68":"code","08f98f5b":"code","25ceb396":"code","f03244fd":"code","6cf6b46b":"code","0f54b1f5":"code","af018892":"code","8c889cbb":"code","d68ad846":"code","31bda969":"code","d16fc9f9":"code","296f7f07":"markdown","5b208c79":"markdown","eb83cd1e":"markdown","8427fde0":"markdown","59a06dae":"markdown","fc054189":"markdown","bbbad357":"markdown","8f6ad2b1":"markdown","468d959f":"markdown","db400778":"markdown","4a1a03f0":"markdown","aa171a87":"markdown","b5943476":"markdown","7e3488da":"markdown","7e0ca0d4":"markdown","6ce61987":"markdown","e61d1631":"markdown","4ec4d474":"markdown"},"source":{"b84e4f10":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","04fa78b3":"df1 = pd.read_csv(\"..\/input\/autos.csv\", encoding=\"ISO-8859-1\")","286887bd":"df = df1.copy()\nprint(df1.shape)\ndf1.head()","a6c185b4":"df1.describe().T","cc8bf240":"df1.info()","4d0ccc6a":"df1.isna().sum()","30ba0fca":"df1[df1['dateCrawled'].isnull()].isna().sum()\n# There are 103701 rows with all NA values which we straight away remove.\ndf2 = df1.dropna(thresh=1)\nprint(df2.shape)\ndf2.isna().sum()","9883c949":"df3 = df2.dropna(thresh=12)\nprint(df3.shape)\ndf3.isna().sum()","8c9a5a61":"plt.subplots(figsize=(15,10))\nplt.subplot(321)\ndf3.seller.value_counts(100).plot(kind='bar', title=\"Seller Proportion\", fontsize=18)\nplt.xticks(rotation=0, fontsize=12)\nplt.yticks(rotation=0, fontsize=12)\n\nplt.subplot(322)\ndf3.offerType.value_counts(100).plot(kind='bar', title='Offer Type Proportion', fontsize=18)\nplt.xticks(rotation=0, fontsize=12)\nplt.yticks(rotation=0, fontsize=12)\nplt.show()\n\nplt.subplots(figsize=(15,10))\nplt.subplot(323)\ndf3.fuelType.value_counts(100).plot(kind='bar', title='Fuel Type Proportion', fontsize=18)\nplt.xticks(rotation=0, fontsize=12)\nplt.yticks(rotation=0, fontsize=12)\n\nplt.subplot(324)\ndf3.gearbox.value_counts(100).plot(kind='bar', title='Gearbox Proportion', fontsize=18)\nplt.xticks(rotation=0, fontsize=12)\nplt.yticks(rotation=0, fontsize=12)\nplt.show()\n\nplt.subplots(figsize=(15,10))\nplt.subplot(325)\ndf3.notRepairedDamage.value_counts(100).plot(kind='bar', title='Damage Not Repaired Proportion', fontsize=18)\nplt.xticks(rotation=0, fontsize=12)\nplt.yticks(rotation=0, fontsize=12)\n\nplt.subplot(326)\ndf3.nrOfPictures.value_counts(100).plot(kind='bar', title='Number Of Pictures Proportion', fontsize=18)\nplt.xticks(rotation=0, fontsize=12)\nplt.yticks(rotation=0, fontsize=12)\nplt.show()","c2b39f1a":"df4 = df3.drop(['seller','offerType','abtest','nrOfPictures','dateCrawled', 'dateCreated', 'lastSeen','name','postalCode'], axis=1)\ndf4.shape","ba255d51":"df4.kilometer = df4.kilometer.astype('int64')","2fbd3b99":"# Distribution of target variable\nplt.subplots(figsize=(15,15))\nplt.subplot(321)\nsns.distplot(df4.price)\nplt.subplot(322)\nsns.boxplot(df4.price)\n\n# Distribution of powerPS variable\nplt.subplot(323)\nsns.distplot(df4.powerPS)\nplt.subplot(324)\nsns.boxplot(df4.powerPS)\n\n# Distribution of kilometer variable\nplt.subplot(325)\nsns.distplot(df4.kilometer)\nplt.subplot(326)\nsns.boxplot(df4.kilometer)\n\nplt.show()","09bc6c59":"qnt = np.quantile(a=df4.price, q=[0.04,0.1,0.25,0.5,0.75,0.99,0.995,0.9999,0.99999,1])\nfor q in qnt:\n    print('{:.2f}'.format(q))\n","6edd19da":"qnt = np.quantile(a=df4.powerPS, q=[0.1,0.11,0.15,0.25,0.5,0.75,0.99,0.995,0.9999,0.99999,1])\nfor q in qnt:\n    print('{:.2f}'.format(q))\n\nprint(df4[(df4['powerPS'] > 408) | (df4['powerPS'] <1)].shape)","d81636fc":"qnt = np.quantile(a=df4.kilometer, q=[0.03,0.04,0.1,0.25,0.5,0.75,0.99,0.995,0.9999,0.99999,1])\nfor q in qnt:\n    print('{:.2f}'.format(q))","ae1721ac":"print(df4[df4['price'] > 47694.75].shape)\ndf5 = df4[(df4['price'] <= 47695) & (df4['price'] > 50) & (df4['powerPS'] > 5) & (df4['powerPS'] < 408)]\nprint(df5.shape)","07d3d575":"# Distribution of target variable\nplt.subplots(figsize=(15,15))\nplt.subplot(321)\nsns.distplot(df5.price)\nplt.subplot(322)\nsns.boxplot(df5.price)\n\n# Distribution of powerPS variable\nplt.subplots(figsize=(15,15))\nplt.subplot(323)\nsns.distplot(df5.powerPS)\nplt.subplot(324)\nsns.boxplot(df5.powerPS)\n\n# Distribution of kilometer variable\nplt.subplots(figsize=(15,15))\nplt.subplot(325)\nsns.distplot(df5.kilometer)\nplt.subplot(326)\nsns.boxplot(df5.kilometer)\n\nplt.show()","6f86f12c":"plt.subplots(figsize=(20,12))\nplt.subplot(221)\nsns.scatterplot(x=df5.powerPS, y=df5.price, hue=df5.vehicleType)\n\nplt.subplot(222)\nsns.scatterplot(x=df5.powerPS, y=df5.price, hue=df5.gearbox)\n\nplt.subplot(223)\nsns.scatterplot(x=df5.powerPS, y=df5.price, hue=df5.fuelType)\n\nplt.subplot(224)\nsns.scatterplot(x=df5.powerPS, y=df5.price, hue=df5.notRepairedDamage)\n\nplt.show()","2ed136ac":"print(df5.yearOfRegistration.unique())","c40f916d":"df6 = df5.copy()\ndf6.yearOfRegistration = df6.yearOfRegistration.astype('int64')\ndf6.kilometer = df6.kilometer.astype('int64')\ndf6.yearOfRegistration[((df6['yearOfRegistration'] > 2015) | (df6['yearOfRegistration'] < 2000)) & (df6['kilometer'] <= 10000)] = 2015\ndf6.yearOfRegistration[((df6['yearOfRegistration'] > 2015) | (df6['yearOfRegistration'] < 2000)) & (df6['kilometer'] <= 20000) & (df6['kilometer'] > 10000)] = 2014\ndf6.yearOfRegistration[((df6['yearOfRegistration'] > 2015) | (df6['yearOfRegistration'] < 2000)) & (df6['kilometer'] > 20000)] = 2013\ndf6.yearOfRegistration[((df6['yearOfRegistration'] > 2015) | (df6['yearOfRegistration'] < 2000)) & (df6['kilometer'] <= 40000) & (df6['kilometer'] > 30000)] = 2012\ndf6.yearOfRegistration[((df6['yearOfRegistration'] > 2015) | (df6['yearOfRegistration'] < 2000)) & (df6['kilometer'] <= 50000) & (df6['kilometer'] > 40000)] = 2011\ndf6.yearOfRegistration[((df6['yearOfRegistration'] > 2015) | (df6['yearOfRegistration'] < 2000)) & (df6['kilometer'] <= 60000) & (df6['kilometer'] > 50000)] = 2010\ndf6.yearOfRegistration[((df6['yearOfRegistration'] > 2015) | (df6['yearOfRegistration'] < 2000)) & (df6['kilometer'] <= 70000) & (df6['kilometer'] > 60000)] = 2009\ndf6.yearOfRegistration[((df6['yearOfRegistration'] > 2015) | (df6['yearOfRegistration'] < 2000)) & (df6['kilometer'] <= 80000) & (df6['kilometer'] > 70000)] = 2008\ndf6.yearOfRegistration[((df6['yearOfRegistration'] > 2015) | (df6['yearOfRegistration'] < 2000)) & (df6['kilometer'] <= 90000) & (df6['kilometer'] > 80000)] = 2007\ndf6.yearOfRegistration[((df6['yearOfRegistration'] > 2015) | (df6['yearOfRegistration'] < 2000)) & (df6['kilometer'] <= 100000) & (df6['kilometer'] > 90000)] = 2006\ndf6.yearOfRegistration[((df6['yearOfRegistration'] > 2015) | (df6['yearOfRegistration'] < 2000)) & (df6['kilometer'] <= 125000) & (df6['kilometer'] > 100000)] = 2005\ndf6.yearOfRegistration[((df6['yearOfRegistration'] > 2015) | (df6['yearOfRegistration'] < 2000)) & (df6['kilometer'] > 125000)] = 2004","a5afe238":"plt.subplots(figsize=(20,5))\nplt.subplot(121)\nsns.countplot(df6.yearOfRegistration)\nplt.xticks(rotation=90, fontsize=12)\nplt.yticks(fontsize=12)\nplt.xlabel(s='Year Of Registration', fontsize=15)\nplt.ylabel(s='Count', fontsize=15)\n\nplt.subplot(122)\nsns.scatterplot(x=df6.yearOfRegistration, y=df6.price)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.xlabel(s='Year Of Registration', fontsize=15)\nplt.ylabel(s='Price', fontsize=15)\nplt.show()","404d90b5":"plt.subplots(figsize=(15,10))\nplt.subplot(321)\ndf6.brand.value_counts(100)[:5].plot(kind='bar', title=\"Brand\", fontsize=18)\nplt.xticks(rotation=0, fontsize=12)\nplt.yticks(rotation=0, fontsize=12)\n\nplt.subplot(322)\ndf6.vehicleType.value_counts(100)[:5].plot(kind='bar', title='Vehicle Type', fontsize=18)\nplt.xticks(rotation=0, fontsize=12)\nplt.yticks(rotation=0, fontsize=12)\nplt.show()\n\nplt.subplots(figsize=(15,10))\nplt.subplot(323)\ndf6.model.value_counts(100)[:5].plot(kind='bar', title='Model', fontsize=18)\nplt.xticks(rotation=0, fontsize=12)\nplt.yticks(rotation=0, fontsize=12)\n\nplt.show()","6b018922":"print(df6.shape)\ndf6.notRepairedDamage = df6['notRepairedDamage'].fillna(method='ffill')\ndf6.isna().sum()","6211c527":"from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split","f5c07337":"# vehicleType column\n\n# filter data for model building\nX = df6[df6['vehicleType'].notna()].drop(['vehicleType','model'], axis=1)\ny = df6.vehicleType[df6['vehicleType'].notna()]\n\n# create test data with unknown vehicleType fields\nxt = df6[df6['vehicleType'].isna()].drop(['vehicleType','model'], axis=1)\n\n# one-hot encode the categorical variables\nX_dum = pd.get_dummies(X)\nxt_dum = pd.get_dummies(xt)\n\n# build a decision tree model\ndtree_veh = DecisionTreeClassifier().fit(X_dum,y)\n\n# predict on test data\nveh_pred = dtree_veh.predict(xt_dum)\n\n# fill the missing values with the predicted values\ndf6.vehicleType[df6['vehicleType'].isna()] = veh_pred\ndf6.vehicleType.isna().sum()","86d817a8":"# model column\n\n# filter data for model building\nX = df6[df6['model'].notna()].drop(['model'], axis=1)\ny = df6.model[df6['model'].notna()]\n\n# create test data with unknown model fields\nxt = df6[df6['model'].isna()].drop(['model'], axis=1)\n\n# one-hot encode the categorical variables\nX_dum = pd.get_dummies(X)\nxt_dum = pd.get_dummies(xt)\nxt_dum.drop('brand_sonstige_autos', axis=1, inplace=True)\n\n# build a decision tree model\ndtree_model = DecisionTreeClassifier().fit(X_dum,y)\n\n# predict on test data\nmodel_pred = dtree_model.predict(xt_dum)\n\n# fill the missing values with the predicted values\ndf6.model[df6['model'].isna()] = model_pred\ndf6.model.isna().sum()","cfac727f":"# gearbox column\n\n# filter data for model building\nX = df6[df6['gearbox'].notna()].drop(['gearbox'], axis=1)\ny = df6.gearbox[df6['gearbox'].notna()]\n\n# create test data with unknown gearbox fields\nxt = df6[df6['gearbox'].isna()].drop(['gearbox'], axis=1)\n\n# one-hot encode the categorical variables\nX_dum = pd.get_dummies(X)\nxt_dum = pd.get_dummies(xt)\nfor col in X_dum.columns:\n    if col not in xt_dum:\n        xt_dum[col] = np.zeros(len(xt_dum))\n        \n# build a decision tree model\ndtree_gear = DecisionTreeClassifier().fit(X_dum,y)\n\n# predict on test data\ngear_pred = dtree_gear.predict(xt_dum)\n\n# fill the missing values with the predicted values\ndf6.gearbox[df6['gearbox'].isna()] = gear_pred\ndf6.gearbox.isna().sum()","ea0f9aa2":"# fuelType column\n\n# filter data for model building\nX = df6[df6['fuelType'].notna()].drop(['fuelType'], axis=1)\ny = df6.fuelType[df6['fuelType'].notna()]\n\n# create test data with unknown fuelType fields\nxt = df6[df6['fuelType'].isna()].drop(['fuelType'], axis=1)\n\n# one-hot encode the categorical variables\nX_dum = pd.get_dummies(X)\nxt_dum = pd.get_dummies(xt)\nfor col in X_dum.columns:\n    if col not in xt_dum:\n        xt_dum[col] = np.zeros(len(xt_dum))\n        \n# build a decision tree model\ndtree_fuel = DecisionTreeClassifier().fit(X_dum,y)\n\n# predict on test data\nfuel_pred = dtree_fuel.predict(xt_dum)\n\n# fill the missing values with the predicted values\ndf6.fuelType[df6['fuelType'].isna()] = fuel_pred\ndf6.fuelType.isna().sum()","d60ac07e":"print(df6.shape)\ndf6.isna().sum()","8fc6c70a":"plt.subplots(figsize=(8,6))\nsns.heatmap(df6.corr(), annot=True, square=True, annot_kws={'fontsize':15})\nplt.show()","5938c5a5":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, scale\nfrom sklearn.metrics import mean_squared_error, accuracy_score, r2_score","0ab97db4":"X = df6.drop('price', axis=1)\ny = df6.price\n\nX_dum = pd.get_dummies(X)\nX_scaled = scale(X_dum)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=10)","2786b410":"linreg = LinearRegression().fit(X_train, y_train)\ny_pred = linreg.predict(X_test)\nprint(\"R2_Score for linear regression: {:.2f}\".format(linreg.score(X_test, y_test)))\nprint(\"RMSE score: {}\".format(np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_test))))\n\nmse_all = cross_val_score(linreg, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\nreg_cv = cross_val_score(linreg, X_train, y_train, cv=5)\nmean_mse_linreg = np.mean(mse_all)\nprint(\"mean MSE with 5k cross validation linear regression: {:.2f}\".format(mean_mse_linreg))\nprint(\"R2_score with 5k cross validation linear regression: {:.2f}\".format(np.mean(reg_cv)))","e3aecc68":"ridge = Ridge(alpha=0.05, normalize=True)\n\nridge.fit(X_train, y_train)\ny_pred = ridge.predict(X_test)\nprint(\"R2_score with Ridge regression: {:.2f}\".format(ridge.score(X_test, y_test)))\n\n# Perform 5-fold cross-validation: ridge_cv\nridge_cv = cross_val_score(ridge, X_train, y_train, cv=5)\n# Print the cross-validated scores\nprint(\"R2_score with 5k cross validation Ridge regression: {:.2f}\".format(np.mean(ridge_cv)))\n","08f98f5b":"lasso = Lasso(alpha=0.4, normalize=True)\n\n# Fit the regressor to the data\nlasso.fit(X_train,y_train)\n\n# Compute and print the coefficients\nlasso_coef = lasso.coef_.tolist()\nprint(lasso_coef)","25ceb396":"df6.skew()","f03244fd":"from scipy.stats import norm\nsns.distplot(df6.price, fit=norm)\nprint(df6.price.skew())\nprint(norm.fit(df6.price))","6cf6b46b":"from scipy import stats\n\nstats.probplot(df6.price, plot=plt)","0f54b1f5":"df7 = df6.copy()\ndf7.price = np.log(df6.price)","af018892":"sns.distplot(df7.price, fit=norm)\nprint(df7.price.skew())\nprint(norm.fit(df7.price))\nplt.show()\n\nstats.probplot(df7.price, plot=plt)\n\nplt.show()","8c889cbb":"# df7 = \n\nX_dum = pd.get_dummies(df7)\n\nX = X_dum.drop('price', axis=1)\ny = X_dum.price\n\n# X_scaled = scale(X_dum)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)","d68ad846":"ridge = Ridge(alpha=0.05, normalize=True)\n\nridge.fit(X_train, y_train)\ny_pred = ridge.predict(X_test)\nprint(\"R2_score with Ridge regression: {:.2f}\".format(ridge.score(X_test, y_test)))\n# print(\"Ridge best parameter: {:.2f}\".format(ridge.best_params_))\n# print(\"Ridge best R2_score: {:.2f}\".format(ridge.best_score_))\n\n# Perform 5-fold cross-validation: ridge_cv\nridge_cv = cross_val_score(ridge, X_train, y_train, cv=5)\n# Print the cross-validated scores\nprint(\"R2_score with 5k cross validation Ridge regression: {:.2f}\".format(np.mean(ridge_cv)))\n","31bda969":"ridge.score(X_train,y_train)","d16fc9f9":"lasso = Lasso(alpha=0.4, normalize=True)\n\n# Fit the regressor to the data\nlasso.fit(X_train,y_train)\n\n# Compute and print the coefficients\nlasso_coef = lasso.coef_.tolist()\nprint(lasso_coef)\ny_lasspred = lasso.predict(X_test)\nlasso.score(X_test,y_test)","296f7f07":"        There is not multicollinearity between the variables, however we can see that monthOfRegistration has very low       correlation with price, hence we will remove it while building the model.","5b208c79":"* It is always a good practice to have visualization of data before treating any of the noisy or missing data. But in our example we can identify a pattern that most of the columns have similar missing values, hence we first look into these missing values and remove if necessary and try to save the computation time required to go through entire dataset length.\n\n        There are plenty of mising values, it is not feasible to remove these missing values without understanding the importance of each feature to predict price of vehicle.\n        We see that many features have same number of missing values, hence we examine these features to find a pattern in missingness.","eb83cd1e":"        We have successfully removed the 1 or 2 missing values from df2 by removing 2 observations and stored the result in new dataframe df3.\n        df3 has 4 columns that have large amount of missing values, removing which result in huge loss of data.\n        \n# Exploratory Data Analysis\n        ","8427fde0":"        Lasso gives us the important feature set, by reducing the weights of non-important features to 0.","59a06dae":"    We will use df4 dataset for further exploratory data analysis","fc054189":"        Year of registration is an important feature since it clearly shows how old a vehicle is.","bbbad357":"        From above plot we can see that seller, offerType and nrOfPictures are very very sparse with more than 99.99% being of one particular cateogory. \n        Such variable will have no effect on model performance and hence we drop these features.\n        Also abtest variable does not provide any meaningful information to predict the price of the vehicle.\n        We also remove dateCrawled, dateCreated and lastSeen features which are less important for our problem to predict    price.","8f6ad2b1":"# Model Building\n        We have handled all the missing values, now we can check for multicollinearity. We handle this multicollinearity if correlation value is greater than 0.7 or less than -0.7","468d959f":"        The original distribution plot and box plot shows there are extreme extreme outliers that shadows the rest of the    datapoints in the plot. Hence we check these observations and then take further actions whether to remove or replace.","db400778":"# Missing Value Imputation with Decision Tree Classifier\n\n        The notRepairedDamage column is easy to impute with mode 'nein'.\n        We have 4 variables left now with missing values - vehicleType, model, gearbox, fuelType\n        To impute the categorical variable we use decision tree classifier with each column as predicted, one by one           starting with gearbox with least number of missing values","4a1a03f0":"    df2 dataset contains observations with atleast one non-missing value. \n    We can see that some features have 1 or 2 missing values. We can try to:\n        * remove these missing values and check how much data is lost\n        * if huge amount of data is lost we can try to impute whichever possible by using imputation techniques","aa171a87":"        The price data is still skewed to right which means the data majorly consists of lower segment vehicles. Hence for  computation we need to scale data using appropriate scaling measures.\n        \n         Lets see how price varied with power of the vehicle","b5943476":"        The 99.5th percentile of the price data is 47695, so we drop the rows with price greater than 47695 since it indicates there is noise in data or this data refers to a highly luxury vehicle category which does not represent the 99.5% of our data. Including this data in model will cause the model to necessarily fit to these noises and overfit on train data.\n        The number of observations are 1340 which we can remove since it forms part of only 0.5% of data.\n        \n        We can see that we have price starting from 0, which does not seem to be correct information, by looking the data we can understand that those vehicles having registration year in 1990s have very low price of order of 50-100. Hence we drop the rows with price less than 50.\n        \n        Many observations have power values either 0 or very high which does not seem feasible. Hence we choose the values between 10 and 408 which are 11th and 99.5th percentile respectively.\n        \n        kilometer feature has range from 5000 to 150000, with 65% of the values are 150000, this seems to be real value hence we do not modify this feature","7e3488da":"### Multilinear Regression with cross validation score","7e0ca0d4":"        We create train and test dataset from df6","6ce61987":"        We see that features like postalCode, yearOfRegistration, monthOfRegistration are not in correct format, hence we can change the data type of these features. But these changes can be applied only after imputing or treating the missing values.\n\n# Handling Missing Values\n        We find the number of missing values in each column.","e61d1631":"        For positive skew use log","4ec4d474":"        There is incorrect information in the data of yearOfRegistration as we can observe yearOfRegistration is after the   dateCrawled in some cases. We remove such observations as yearOfRegistration information cannot be reproduced to impute.\n        We can replace yearOfRegistration with previous few years based on our understanding of average km run per year and model.\n        Even the yearOfRegistration values which indicate too old model is very unlikely to be available now for sale. So    these are possible noise in the data, we use similar replacement technique to repalce yearOfRegistration values less         than 2000."}}