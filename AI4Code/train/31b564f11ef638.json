{"cell_type":{"f6c864ff":"code","6522a7e5":"code","899ab59a":"code","fea1cc85":"code","75c7c740":"code","812387f1":"code","77039ba2":"code","e861b6fa":"code","ae097077":"code","81a0dccc":"code","e798fecf":"code","a3e0dd1f":"code","d4d06687":"code","a17c8f6e":"code","f4731840":"code","d824c782":"markdown","5f66b2d7":"markdown","e55ca38a":"markdown","5cad71e3":"markdown","09288df6":"markdown","e3ca407e":"markdown","c895ec4e":"markdown","9511cee1":"markdown","daf1881e":"markdown","3973c2e1":"markdown","7c628579":"markdown","402c4404":"markdown","6c92f8a2":"markdown","3948b9ba":"markdown","728efded":"markdown","2c2253ed":"markdown","ee235c98":"markdown","d1209758":"markdown","e8e839d4":"markdown","8a37fc95":"markdown","6bdf53a1":"markdown","d57263d0":"markdown","35b5f8a2":"markdown"},"source":{"f6c864ff":"import numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv(\"\/kaggle\/input\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/sampleSubmission.csv\")","6522a7e5":"all_data_temp = pd.concat([train, test])\nall_data_temp","899ab59a":"all_data = pd.concat([train, test], ignore_index=True)\nall_data","fea1cc85":"from datetime import datetime\n\nall_data['date'] = all_data['datetime'].apply(lambda x: x.split()[0]) # Create date feature\nall_data['year'] = all_data['datetime'].apply(lambda x: x.split()[0].split('-')[0]) # Create year feature\nall_data['month'] = all_data['datetime'].apply(lambda x: x.split()[0].split('-')[1]) # Create month feature\nall_data['hour'] = all_data['datetime'].apply(lambda x: x.split()[1].split(':')[0]) # Create hour feature\nall_data[\"weekday\"] = all_data['date'].apply(lambda dateString : datetime.strptime(dateString,\"%Y-%m-%d\").weekday()) # Create weekday feature","75c7c740":"categorical_features = ['season', 'holiday', 'workingday', 'weather', 'weekday', 'month', 'year', 'hour']\n\nfor feature in categorical_features:\n    all_data[feature] = all_data[feature].astype(\"category\")","812387f1":"train = all_data[pd.notnull(all_data['count'])]\ntest = all_data[~pd.notnull(all_data['count'])]\ny = train['count']","77039ba2":"drop_features = ['count', 'casual', 'registered', 'datetime', 'date', 'datetime', 'windspeed', 'month']\n\nX_train = train.drop(drop_features, axis=1)\nX_test = test.drop(drop_features, axis=1)","e861b6fa":"X_train.info()","ae097077":"def rmsle(y_true, y_pred, convertExp=True):\n    # Apply exponential transformation function\n    if convertExp:\n        y_true = np.exp(y_true)\n        y_pred = np.exp(y_pred)\n        \n    # Convert missing value to zero after log transformation\n    log_true = np.nan_to_num(np.array([np.log(y+1) for y in y_true]))\n    log_pred = np.nan_to_num(np.array([np.log(y+1) for y in y_pred]))\n    \n    # Compute RMSLE\n    output = np.sqrt(np.mean((log_true - log_pred)**2))\n    return output","81a0dccc":"from sklearn.linear_model import LinearRegression\n\n# Step 1: Create Model\nlinear_reg_model = LinearRegression()\n\n# Step 2: Train Model\nlog_y = np.log1p(y)  # Log Transformation of Target Value y\nlinear_reg_model.fit(X_train, log_y) \n\n# Step 3 : Predict\npreds = linear_reg_model.predict(X_train)\n\n# Step 4 : Evaluate\nprint ('Linear Regression RMSLE:', rmsle(log_y, preds, True))","e798fecf":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\n\n# Step 1: Create Model\nridge_model = Ridge()\n\n# Step 2-1 : Create GridSearchCV Object\n# Hyper-parameter List\nridge_params = {'max_iter':[3000], 'alpha':[0.1, 1, 2, 3, 4, 10, 30, 100, 200, 300, 400, 800, 900, 1000]}\n# Evaluate Function for Cross-Validation (RMSLE score)\nrmsle_scorer = metrics.make_scorer(rmsle, greater_is_better=False) \n# Create GridSearchCV Object (with Ridge)\ngridsearch_ridge_model = GridSearchCV(estimator=ridge_model,\n                                      param_grid=ridge_params,\n                                      scoring=rmsle_scorer,\n                                      cv=5)\n\n# Step 2-2 : Perform Grid Search\nlog_y = np.log1p(y) # Log Transformation of Target Value y\ngridsearch_ridge_model.fit(X_train, log_y) # Train (Grid Search)\n\nprint('Best Parameter:', gridsearch_ridge_model.best_params_)\n\n# Step 3 : Predict\npreds = gridsearch_ridge_model.best_estimator_.predict(X_train)\n\n# Step 4 : Evaluate\nprint('Ridge Regression RMSLE:', rmsle(log_y, preds, True))","a3e0dd1f":"from sklearn.linear_model import Lasso\n\n# Step 1: Create Model\nlasso_model = Lasso()\n\n# Step 2-1 : Create GridSearchCV Object\n# Hyper-parameter List\nlasso_alpha = 1\/np.array([0.1, 1, 2, 3, 4, 10, 30, 100, 200, 300, 400, 800, 900, 1000])\nlasso_params = {'max_iter':[3000], 'alpha':lasso_alpha}\n# Create GridSearchCV Object (with Lasso)\ngridsearch_lasso_model = GridSearchCV(estimator=lasso_model,\n                                      param_grid=lasso_params,\n                                      scoring=rmsle_scorer,\n                                      cv=5)\n\n\n# Step 2-2 : Perform Grid Search\nlog_y = np.log1p(y)\ngridsearch_lasso_model.fit(X_train, log_y) # Train (Grid Search)\n\nprint('Best Parameter:', gridsearch_lasso_model.best_params_)\n\n# Step 3 : Predict\npreds = gridsearch_lasso_model.best_estimator_.predict(X_train)\n\n# Step 4 : Evaluate\nprint('Lasso Regression RMSLE:', rmsle(log_y, preds, True))","d4d06687":"from sklearn.ensemble import RandomForestRegressor\n\n# Step 1: Create Model\nrandomforest_model = RandomForestRegressor()\n\n# Step 2-1 : Create GridSearchCV Object\n# Hyper-parameter List\nrf_params = {'random_state':[42], 'n_estimators':[100, 120, 140]}\n# Create GridSearchCV Object (with Random Forest Regression)\ngridsearch_random_forest_model = GridSearchCV(estimator=randomforest_model,\n                                              param_grid=rf_params,\n                                              scoring=rmsle_scorer,\n                                              cv=5)\n\n# Step 2-2 : Perform Grid Search\nlog_y = np.log1p(y)\ngridsearch_random_forest_model.fit(X_train, log_y)\n\nprint('Best Parameter:', gridsearch_random_forest_model.best_params_)\n\n# \uc2a4\ud15d 3 : \uc608\uce21\npreds = gridsearch_random_forest_model.best_estimator_.predict(X_train)\n\n# \uc2a4\ud15d 4 : \ud3c9\uac00\nprint('Random Forest Regression RMSLE:', rmsle(log_y, preds, True))","a17c8f6e":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nrandomforest_preds = gridsearch_random_forest_model.best_estimator_.predict(X_test)\n\nfigure, axes = plt.subplots(ncols=2)\nfigure.set_size_inches(10, 4)\n\nsns.distplot(y, ax=axes[0], bins=50)\naxes[0].set_title('Train Data Distribution')\nsns.distplot(np.exp(randomforest_preds), ax=axes[1], bins=50)\naxes[1].set_title('Predicted Test Data Distribution');","f4731840":"submission['count'] = np.exp(randomforest_preds)\nsubmission.to_csv('submission.csv', index=False)","d824c782":"## Feature Engineering","5f66b2d7":"### This is a simple modeling notebook using Random Forest Regression. This model reaches the top 6.6%. If you think it's useful, please upvote ^^ ","e55ca38a":"### Evaluation score(RMSLE) function","5cad71e3":"# I would appreciate it if you upvote my notebook. Thank you!","09288df6":"### Random Forest Regression Model is the Best among four models!","e3ca407e":"- [Bike Sharing Demand Competition](https:\/\/www.kaggle.com\/c\/bike-sharing-demand)\n\n- [Modeling Reference Notebook](https:\/\/www.kaggle.com\/viveksrinivasan\/eda-ensemble-model-top-10-percentile)","c895ec4e":"### Compare train data vs predicted test data distribution ","9511cee1":"### Check final features and types","daf1881e":"## Train Model and Measure Model Performance","3973c2e1":"### submit final predictions","7c628579":"## Submit","402c4404":"### Create new features","6c92f8a2":"### Concatenate train and test data","3948b9ba":"### Drop useless features","728efded":"### Load Data","2c2253ed":"### Ridge Model (Apply Gridsearch)","ee235c98":"### Lasso Model (Apply Gridsearch)","d1209758":"### Separate train and test data. Assign train target value(y)","e8e839d4":"### I also shared [basic EDA notebook for everyone](https:\/\/www.kaggle.com\/werooring\/bike-sharing-demand-basic-eda-for-everyone)","8a37fc95":"### Linear regression model","6bdf53a1":"### Change categorical data type for memory reduction","d57263d0":"### Random Forest Regression Model (Apply Grid Search)","35b5f8a2":"# Bike Shring Demand Top 6.6% Solution for Everyone !!"}}