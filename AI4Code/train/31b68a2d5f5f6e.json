{"cell_type":{"1209a137":"code","07b2c14c":"code","14133f97":"code","bf77f6f6":"code","ce8a4357":"code","0aa4aaa6":"code","a0c6de99":"code","c02a160b":"code","e4434b95":"code","cc691182":"code","48fbfda2":"code","a4f50167":"code","d3a7d2b8":"code","16fa4eba":"code","fbc51224":"code","1bfb2e06":"code","64fafeff":"code","7148e8ab":"code","fdc403c3":"code","907d2338":"code","937a1851":"code","d6e3680a":"code","93691b2c":"code","d94a21bf":"code","add7ed5f":"code","bda9e0c0":"code","2e377b03":"code","a62e4710":"code","367e6331":"code","9a354c5e":"code","b4e780a4":"code","4e01ab9a":"code","5537a8c1":"code","fab83ee6":"code","ce056bd0":"code","44724699":"code","d726fde9":"code","80d3da77":"code","2403ea54":"code","c6e1a867":"code","bbbbd259":"code","322b95c2":"code","9128105f":"code","c0f2c8eb":"code","4300b91c":"code","8c3a6e02":"code","e7f571e0":"code","928d6a6c":"code","2b13673d":"code","f70ddb7c":"code","e700f6e3":"code","0e62d41b":"code","6097230e":"code","9dd7a69a":"code","ae8fac7d":"code","c3f3046e":"code","ae2807ff":"code","121db85a":"code","4813ed7a":"code","7a217dc9":"code","ac69b855":"code","c3b21d0d":"code","c021c482":"code","57ca1a3d":"code","9d69c43d":"code","90137661":"code","912de135":"code","db58f1ae":"code","4f01fe8b":"code","4081185b":"code","6f1b0866":"code","70bd6a31":"code","2c9a2024":"code","506e6f9d":"code","fb33a56e":"code","c74867b4":"markdown","2439ac94":"markdown","4dd32c25":"markdown","98984a91":"markdown","36ad09ec":"markdown"},"source":{"1209a137":"\"\"\"\nLets import all the libraries needed for our code\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score   \nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom math import sqrt\nfrom scipy.stats import norm, skew \nfrom sklearn.preprocessing import PolynomialFeatures \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn import linear_model\nfrom scipy import stats\nimport os\nprint(os.listdir(\"..\/input\"))","07b2c14c":"\"\"\"\nReading file from the specific folder\n\"\"\"\ntrain=pd.read_csv(\"..\/input\/train.csv\", parse_dates=['timestamp'])\nprint(\"Shape of dataset\",train.shape,\"\\n\")\nprint(\"Basic information of our data\",train.info(),\"\\n\")\nprint(\"Basic view of our rows\",train.head())\ntest=pd.read_csv(\"..\/input\/test.csv\", parse_dates=['timestamp'])\nmacro_cols = [\"balance_trade\", \"balance_trade_growth\", \"eurrub\", \"average_provision_of_build_contract\",\n\"micex_rgbi_tr\", \"micex_cbi_tr\", \"deposits_rate\", \"mortgage_value\", \"mortgage_rate\",\n\"income_per_cap\", \"rent_price_4+room_bus\", \"museum_visitis_per_100_cap\", \"apartment_build\"]\nmacro = pd.read_csv(\"..\/input\/macro.csv\", parse_dates=['timestamp'], usecols=['timestamp'] + macro_cols)\n","14133f97":"test.head()","bf77f6f6":"df = pd.concat([train, test])\ndf = pd.merge_ordered(df, macro, on='timestamp', how='left')\n\nprint(df.shape)","ce8a4357":"df.head()","0aa4aaa6":"print(pd.set_option(\"display.max_rows\",999))\n\nprint(\"Basic description of our data \",df.describe())","a0c6de99":"\"\"\"\nLets check how many null values are present in our dataset\n\"\"\"\nnull_columns=df.columns[df.isnull().any()]\n\ndf[null_columns].isnull().sum()","c02a160b":"\"\"\"\nLets fetch Year column from our Transaction timestamp \n\"\"\"\ndf[\"trans_year\"]=df.timestamp.dt.year\n\nmonth_year = (df.timestamp.dt.month + df.timestamp.dt.year * 100)\nmonth_year_cnt_map = month_year.value_counts().to_dict()\ndf['month_year_cnt'] = month_year.map(month_year_cnt_map)\n\n# Add week-year count\nweek_year = (df.timestamp.dt.weekofyear + df.timestamp.dt.year * 100)\nweek_year_cnt_map = week_year.value_counts().to_dict()\ndf['week_year_cnt'] = week_year.map(week_year_cnt_map)\n\n# Add month and day-of-week\ndf['month'] = df.timestamp.dt.month\ndf['dow'] = df.timestamp.dt.dayofweek\n\n# Remove timestamp column (may overfit the model in train)\ndf.drop(['timestamp'], axis=1, inplace=True)\n","e4434b95":"#df[df.build_year>300].build_year.min()\n\ndf=df[df.build_year!=20052009.0]\ndf.build_year=np.where(df.build_year<1691, np.nan,df.build_year)","cc691182":"\"\"\"\nLets replace NULL values in Buildyear as mean of the build_year in that subarea\n\"\"\"\ndf.build_year = df.groupby([\"sub_area\"])['build_year'].apply(lambda x: x.fillna(x.mean()))\ndf[\"build_year\"]=df[\"build_year\"].astype(\"int64\")\nprint(\"Lets see count of null value left in build_year now \",df.build_year.isnull().sum())","48fbfda2":"#Even after replacement with mean we have build_year as zero which is quite irrelevant \n#hence without any groupping lets replace these values with mean\n\n\ndf.build_year=np.where(df.build_year<1691, np.nan,df.build_year)\ndf.build_year.fillna(df.build_year.mean(),inplace=True)","a4f50167":"df.build_year.isnull().sum()","d3a7d2b8":"sns.scatterplot(df.index,df.build_year)","16fa4eba":"\"\"\"\nLets replace NULL values in max_floor as mean of the max_floor in that subarea\n\"\"\"\ndf.max_floor = df.groupby([\"sub_area\"])['max_floor'].apply(lambda x: x.fillna(x.mean()))\ndf[\"max_floor\"]=df[\"max_floor\"].astype(\"int64\")\nprint(\"Lets see count of null value left in max_floor now \",df.max_floor.isnull().sum())\n\n","fbc51224":"\"\"\"\nLets create bins as per building size\n\"\"\"\nbins=[-1,0,1,2,5,10,50,120]\nlabels=[\"Single storey\",\"One floor\",\"Only 2 floor\",\"2-5 floor\",\"6-10 floors\",\"11-50 floors\",\"50+ floors\"]\n\ndf[\"build_size\"]=pd.cut(df[\"max_floor\"],bins,labels=labels)\n\"\"\"\nLets convert the build_size as object type\n\"\"\"\ndf[\"build_size\"]=df[\"build_size\"].astype(\"object\")\nprint(\"Build size column overview \\n\",df[\"build_size\"].head())\n\ntrain[\"build_size\"]=pd.cut(train[\"max_floor\"],bins,labels=labels)\ntrain[\"build_size\"]=train[\"build_size\"].astype(\"object\")","1bfb2e06":"\"\"\"\nLets create a column that tells the age of the building\n\"\"\"\ndf[\"Building_Age\"]=0\ndf[\"Building_Age\"] = df[\"build_year\"].apply(lambda x: 2019-x)\nprint (\"Building age overview \\n\",df[\"Building_Age\"].head())\ndf[\"Building_Age\"]=df[\"Building_Age\"].astype(\"int64\")\n\n","64fafeff":"\"\"\"\nLets create a column that tells the type as per the age of the building\n\"\"\"\nbins=[0,2,5,10,20,50,100,500]\nlabels=[\"2017-18\",\"2017-2014\",\"2014-2009\",\"2009-1999\",\"1999-1969\",\"1969-1919\",\"1919& before\"]\ndf[\"building_agetype\"]=pd.cut(df[\"Building_Age\"],bins,labels=labels)\ndf[\"building_agetype\"]=df[\"building_agetype\"].astype(\"object\")\n\nprint (\"Building age overview \\n\",df[\"building_agetype\"].head())\n","7148e8ab":"\"\"\"\nLets try to replace max_floor on the basis of subarea groupped mean's\n\"\"\"\n\ndf.floor = df.groupby([\"sub_area\"])['floor'].apply(lambda x: x.fillna(x.mean()))\nprint (\"Lets see count of null value left in max_floor now \",df.floor.isnull().sum())","fdc403c3":"\"\"\"\nLets try to replace max_floor on the basis of subarea and build year groupped mean's\n\"\"\"\ndf.max_floor = df.groupby([\"sub_area\",\"build_year\"])['max_floor'].apply(lambda x: x.fillna(x.mean()))\n#print (df.max_floor.isnull().sum())\ndf.max_floor = df.groupby([\"build_year\"])['max_floor'].apply(lambda x: x.fillna(x.mean()))\n#print (df.max_floor.isnull().sum())\ndf.max_floor = df.groupby([\"sub_area\"])['max_floor'].apply(lambda x: x.fillna(x.mean()))\n\n\n\"\"\"\nSince all values should be integer \n\"\"\"\ndf[\"max_floor\"]=df[\"max_floor\"].astype(\"int64\")\nprint (\"Lets see count of null value left in max_floor now \",df.max_floor.isnull().sum())","907d2338":"\"\"\"\nLets try to replace preschool_quota,school_quota as its mean\n\"\"\"\ndf.preschool_quota = df[\"preschool_quota\"].fillna(df[\"preschool_quota\"].mean())\ndf.school_quota = df[\"school_quota\"].fillna(df[\"school_quota\"].mean())\n\n\"\"\"\nSince all values should be integer \n\"\"\"\n#df[\"preschool_quota\"]=df[\"preschool_quota\"].astype(\"int64\")\nprint (\"Lets see count of null value left in preschool_quota now \",df.preschool_quota.isnull().sum())\n\nprint (\"Lets see count of null value left in school_quota now \",df.school_quota.isnull().sum())","937a1851":"\"\"\"\nLets try to replace state on the basis of subarea and build year groupped mean's\n\"\"\"\ndf.state = df.groupby([\"sub_area\",\"build_year\"])['state'].apply(lambda x: x.fillna(x.mean()))\n\ndf.state = df.groupby([\"build_year\"])['state'].apply(lambda x: x.fillna(x.mean()))\ndf.state = df.groupby([\"sub_area\"])['state'].apply(lambda x: x.fillna(x.mean()))\n\n\"\"\"\nSince all values should be integer \n\"\"\"\ndf[\"state\"]=df[\"state\"].astype(\"int64\")\nprint (\"Lets see count of null value left in max_floor now \",df.state.isnull().sum())","d6e3680a":"\"\"\"\nWe could see that as per num_room we had approx 7k+ null rows so lets remove them\n\"\"\"\ndf.num_room.fillna(df.num_room.median(),inplace=True)\n\ndf[\"num_room\"]=df[\"num_room\"].astype(\"int64\")\nprint (\"Lets see count of null value left in num_room now \",df.num_room.isnull().sum())","93691b2c":"\"\"\"\nLets try to replace material on the basis of subarea median\n\"\"\"\n\ndf.material = df.groupby([\"sub_area\"])['material'].apply(lambda x: x.fillna(x.median()))\n\"\"\"\nSince all values are numeric itself  \n\"\"\"\ndf[\"material\"]=df[\"material\"].astype(\"int64\")\n\nprint (\"Lets see count of null value left in material now \",df.material.isnull().sum())","d94a21bf":"\"\"\"\nWe have assumed that below columns are of not much relevance for us and hence lets delete these columns from our dataset\n\n\nto_be_del_col=[\"kitch_sq\",\"cafe_sum_500_min_price_avg\",\"cafe_sum_500_max_price_avg\",\"cafe_avg_price_500\",\"id\",\n              \"cafe_sum_1000_min_price_avg\",\"cafe_sum_1000_max_price_avg\",\"cafe_avg_price_1000\",\"cafe_sum_1500_min_price_avg\"\n               ,\"cafe_sum_1500_max_price_avg\",\"cafe_avg_price_1500\",\"timestamp\",\"life_sq\",\"hospital_beds_raion\"\n              ]\n\ndf_old=df.copy(deep=True)\n#to_be_del=['cafe_sum_1000_max_price_avg', 'cafe_avg_price_1000',]\ndf.drop(to_be_del_col,inplace=True,axis=1)\n\"\"\"","add7ed5f":"\"\"\"\nLets round off the float values upto 2 decimal places \n\"\"\"\nfor col in df.columns:\n    if (df[col].dtype == 'float64'): \n        df[col] = round(df[col],2)\n        ","bda9e0c0":"\"\"\"\nIts not a relevant case ot have max_floor less than floor\nHence lets have only data where max_floor is greater than or equal to floor\n\"\"\"        \ndf[\"floor\"]=df['floor'].astype(\"int64\")\ndf[\"max_floor\"]=df['max_floor'].astype(\"int64\")\n","2e377b03":"flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\nsns.set_palette(flatui)\nsns.palplot(sns.color_palette())","a62e4710":"fig, ax = plt.subplots(figsize=(15,10))\nb = sns.boxplot(x=\"build_size\", y=\"price_doc\", data=train)\nplt.xlabel(\"Building Size\")\nplt.ylabel(\"Price \")\nplt.title(\"Price variation as per Building Type\")\nplt.show(b)","367e6331":"sns.countplot(\"trans_year\",hue=\"product_type\",data=df)\nplt.xlabel(\"Year of transaction\")\nplt.ylabel(\"Total Number of Transactions\")\nplt.title(\"Transaction count as per year and Product Type\")\n","9a354c5e":"sns.scatterplot(\"full_sq\",\"price_doc\",data=train)\nplt.xlabel(\"Full square\")\n\nplt.ylabel(\"Price\")\nplt.title(\"Price v\/s Full square\")\n","b4e780a4":"\"\"\"\nHence we can see that full_sq >300 is an outlier so lets remove this value and check our plot\n\"\"\"\n\n#The outliers should not be there for price_doc as well and hence removing it\ntrain_new=train[train[\"price_doc\"]<train[[\"price_doc\"]].quantile(.95).values[0]]\n\nsns.scatterplot(\"full_sq\",\"price_doc\",data=train_new)\nplt.xlabel(\"Full square\")\nplt.ylabel(\"Price\")\nplt.title(\"Price v\/s Full square\")","4e01ab9a":"df_invest=df[df[\"product_type\"]==\"Investment\"].groupby(\"sub_area\")[[\"product_type\"]].count()\ndf_owner=df[df[\"product_type\"]==\"OwnerOccupier\"].groupby(\"sub_area\")[[\"product_type\"]].count()\n\nfig, ax = plt.subplots(figsize=(15,30))\np1 = plt.barh(df_invest.index.values,df_invest.product_type)\np2 = plt.barh(df_owner.index.values,df_owner.product_type)\nplt.ylabel('Sub Area')\nplt.title('Number of investment and owner occupied properties as per sub area')\nplt.legend((p1[0], p2[0]), ('Investment', 'OwnerOccupier'))\n\nplt.show()","5537a8c1":"#df_2011=df[df[\"trans_year\"]==2011].groupby(\"sub_area\")[[\"id\"]].count()\n#df_2012=df[df[\"trans_year\"]==2012].groupby(\"sub_area\")[[\"id\"]].count()\ndf_2013=df[df[\"trans_year\"]==2013].groupby(\"sub_area\")[[\"trans_year\"]].count()\ndf_2014=df[df[\"trans_year\"]==2014].groupby(\"sub_area\")[[\"trans_year\"]].count()\ndf_2015=df[df[\"trans_year\"]==2015].groupby(\"sub_area\")[[\"trans_year\"]].count()\n\nfig, ax = plt.subplots(figsize=(15,30))\n#p2011 = plt.barh(df_2011.index.values,df_2011.id)\n#p2012 = plt.barh(df_2012.index.values,df_2012.id)\np2013 = plt.barh(df_2013.index.values,df_2013.trans_year)\np2014 = plt.barh(df_2014.index.values,df_2014.trans_year)\np2015 = plt.barh(df_2015.index.values,df_2015.trans_year)\nplt.ylabel('Subarea')\nplt.title('Price  by subarea and Transaction year')\nplt.legend((p2013[0], p2014[0],p2015[0]), ('2013', '2014','2015'))\n\n#plt.legend((p2011[0], p2012[0],p2013[0], p2014[0],p2015[0]), ('2011', '2012','2013', '2014','2015'))\n\nplt.show()","fab83ee6":"fig, ax = plt.subplots(figsize=(25,10))\na=sns.countplot(\"building_agetype\",data=df,orient=\"v\")\nplt.ylabel(\"Number of houses \")\nplt.xlabel(\"Building age \")\nplt.title(\"Count of houses per Building age group \")\n\n","ce056bd0":"\"\"\"\n#We removed some columns so lets see now how many columns have\n#null values and if there are any lets replace it with mode\nnull_columns=df.columns[df.isnull().any()]\n# basically just for the sake of keeping all columns and applying vif we replaced all null values with mode\n\nfor column in null_columns:\n    df[column].fillna(df[column].mode()[0], inplace=True)\n\"\"\"\n    \n    ","44724699":"\"\"\"\nSince XGBoost works only on numeric data lets convert our object type data to numeric form and then apply our model\n\n\"\"\"\n","d726fde9":"#Since there are so many columns and it takes a long time we have removed the below code ,\n#one can run and see for reference\n\n\"\"\"\ncorrelation_matrix = df.corr()\nsns.heatmap(correlation_matrix, annot=True)\nplt.show()\n\"\"\"","80d3da77":"rs = np.random.RandomState(0)\n#df = pd.DataFrame(rs.rand(10, 10))\ncorr = df[['green_part_5000', 'prom_part_5000', 'office_count_5000', 'office_sqm_5000','trc_count_5000', 'trc_sqm_5000', 'cafe_count_5000',\n 'cafe_sum_5000_min_price_avg', 'cafe_sum_5000_max_price_avg','cafe_avg_price_5000', 'cafe_count_5000_na_price',\n 'cafe_count_5000_price_500', 'cafe_count_5000_price_1000','cafe_count_5000_price_1500', 'cafe_count_5000_price_2500', 'cafe_count_5000_price_4000', 'cafe_count_5000_price_high',\n 'big_church_count_5000', 'church_count_5000', 'mosque_count_5000',\n 'leisure_count_5000', 'sport_count_5000', 'market_count_5000']].corr()\ncorr.style.background_gradient(cmap='coolwarm')\n","2403ea54":"\n\"\"\"\ndf.drop([\"trc_count_1500\",\"cafe_count_1500\",\"cafe_count_1500_na_price\",\"cafe_count_1500_price_500\",\"cafe_count_1500_price_1000\",\"cafe_count_1500_price_1500\",\"cafe_count_1500_price_2500\",\"cafe_count_1500_price_4000\",\"cafe_count_1500_price_high\",\"big_church_count_1500\",\"church_count_1500\",\"mosque_count_1500\",\"leisure_count_1500\",\"sport_count_1500\",\"market_count_1500\"],axis=1,inplace=True)\ndf.drop([\"culture_objects_top_25_raion\",\"culture_objects_top_25\",\"thermal_power_plant_raion\",\"incineration_raion\",\"oil_chemistry_raion\",\"radiation_raion\",\"railroad_terminal_raion\",\"big_market_raion\",\"nuclear_reactor_raion\",\"detention_facility_raion\",\"trc_count_1000\",\"cafe_count_1000\",\"cafe_count_1000_na_price\",\"cafe_count_1000_price_500\",\"cafe_count_1000_price_1000\",\"cafe_count_1000_price_1500\",\"cafe_count_1000_price_2500\",\"cafe_count_1000_price_4000\",\"cafe_count_1000_price_high\",\"big_church_count_1000\",\"church_count_1000\",\"mosque_count_1000\",\"leisure_count_1000\",\"sport_count_1000\",\"market_count_1000\",],axis=1,inplace=True)\n\ndf.drop([\"office_count_3000\",\"trc_count_3000\",\"cafe_count_3000\",\"cafe_sum_3000_min_price_avg\",\"cafe_sum_3000_max_price_avg\",\"cafe_avg_price_3000\",\"cafe_avg_price_3000\",\"cafe_count_3000_na_price\",\"cafe_count_3000_price_500\",\"cafe_count_3000_price_1000\",\"cafe_count_3000_price_1500\",\"cafe_count_3000_price_2500\",\"cafe_count_3000_price_4000\",\"cafe_count_3000_price_high\",\"big_church_count_3000\",\"church_count_3000\",\"mosque_count_3000\",\"leisure_count_3000\",\"sport_count_3000\",\"market_count_3000\"],axis=1,inplace=True)\ndf.drop([\"office_count_2000\",\"trc_count_2000\",\"cafe_count_2000\",\"cafe_sum_2000_min_price_avg\",\"cafe_sum_2000_max_price_avg\",\"cafe_avg_price_2000\",\"cafe_count_2000_na_price\",\"cafe_count_2000_price_500\",\"cafe_count_2000_price_1000\",\"cafe_count_2000_price_1500\",\"cafe_count_2000_price_2500\",\"cafe_count_2000_price_4000\",\"cafe_count_2000_price_high\",\"big_church_count_2000\",\"church_count_2000\",\"mosque_count_2000\",\"leisure_count_2000\",\"sport_count_2000\",\"market_count_2000\"],axis=1,inplace=True)\ndf.drop([\"male_f\",\"female_f\",\"young_all\",\"young_male\",\"young_female\",\"work_all\",\"work_male\",\"work_female\",\"ekder_all\",\n             \"ekder_male\",\"ekder_female\",\"0_6_all\",\"0_6_male\",\"0_6_female\",\"7_14_all\",\"7_14_male\",\"7_14_female\",\"0_17_all\",\n              \"0_17_male\",\"0_17_female\",\"16_29_all\",\"16_29_male\",\"16_29_female\",\"0_13_all\",\"0_13_male\",\"0_13_female\"],\n             axis=1,inplace=True)\n\ndf.drop([\"office_count_500\",\"office_sqm_500\",\"trc_count_500\",\"trc_sqm_500\",\"cafe_count_500\",\"cafe_count_500_na_price\",\"cafe_count_500_price_500\",\"cafe_count_500_price_1000\",\"cafe_count_500_price_1500\",\"cafe_count_500_price_2500\",\"cafe_count_500_price_4000\",\"cafe_count_500_price_high\",\"big_church_count_500\",\"mosque_count_500\",\"leisure_count_500\",\"sport_count_500\",\"market_count_500\"],axis=1,inplace=True)\ndf.drop([\"cafe_count_5000_na_price\",\"cafe_count_5000_price_500\",\"cafe_count_5000_price_1000\",\"cafe_count_5000_price_1500\",\"cafe_count_5000_price_2500\",\"cafe_count_5000_price_4000\",\"cafe_count_5000_price_high\",\"big_church_count_5000\",\"church_count_5000\",\"leisure_count_5000\",\"office_count_5000\",\"office_sqm_5000\"],axis=1,inplace=True)\n\n\"\"\"","c6e1a867":"correlation_matrix = df.corr()\nsns.heatmap(correlation_matrix, annot=True)\nplt.show()","bbbbd259":"a=list(df.select_dtypes(include=['object']).dtypes.index.values)\n\nfor i in a :\n    # use pd.concat to join the new columns with your original dataframe\n    df_xg = pd.concat([df,pd.get_dummies(df[i], prefix='sub_area',drop_first=True)],axis=1,)\n\n    # now drop the original 'country' column (you don't need it anymore)\n    #df_xg.drop([i],axis=1, inplace=True)","322b95c2":"df_xg.columns.values","9128105f":"df_xg[\"log_price\"]=np.log1p(df_xg[\"price_doc\"])\n                            \nnew_train=df_xg[df_xg.id<30474]\nnew_train.shape\nnew_test=df_xg[df_xg.id>30473]\nnew_test.shape","c0f2c8eb":"\nx=new_train.drop([\"price_doc\",\"log_price\"],axis=1).select_dtypes(exclude=['object'])\ny=new_train[\"log_price\"]\n\nx -= x.mean(axis=0)\nx \/= x.std(axis=0)\n\n\nx_test=new_test.drop([\"price_doc\",\"log_price\"],axis=1).select_dtypes(exclude=['object'])\ny_test=new_test[\"log_price\"]\n\nx_test -= x_test.mean(axis=0)\nx_test \/= x_test.std(axis=0)\n\nx_train,x_val,y_train,y_val=train_test_split(x, y, test_size=0.33, random_state=42)\n","4300b91c":"\"\"\"\nIts better to convert data into Dmatrix before applying xgb \n\"\"\"\ndtrain = xgb.DMatrix(data=x_train,label= y_train)\ndval = xgb.DMatrix(data=x_val, label=y_val)\ndtest = xgb.DMatrix(data=x_test, label=y_test)\n\n\"\"\"\nIntialising XGB model and then fitting train dataa\n\"\"\"\n\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 6,\n    'subsample': 1.0,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\n\n# Uncomment to tune XGB `num_boost_rounds`\npartial_model = xgb.train(xgb_params, dtrain, num_boost_round=1000, evals=[(dval, 'val')],\n                       early_stopping_rounds=20, verbose_eval=20)\n\nnum_boost_round = partial_model.best_iteration","8c3a6e02":"num_boost_round = partial_model.best_iteration\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_round)","e7f571e0":"ylog_pred = model.predict(dtest)\ny_pred = np.exp(ylog_pred) - 1\n\ndf_sub = pd.DataFrame({'id': new_test.id, 'price_doc': y_pred})\n\ndf_sub.to_csv('sub.csv', index=False)","928d6a6c":"params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5, 'alpha': 10}\n\ncv_results = xgb.cv(dtrain=dtrain, params=params, nfold=3,\n                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)","2b13673d":"cv_results.head()","f70ddb7c":"\"\"\"\nPlotting the feature importance graph\n\"\"\"\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 16))\nxgb.plot_importance(partial_model, max_num_features=50, height=0.5, ax=ax)","e700f6e3":"feature_important = partial_model.get_score(importance_type='weight')\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\n\n\"\"\"\nLets create a dataframe with all the details about feature importance as per our model \n\"\"\"\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n","0e62d41b":"#Fetching top 50 features for our model building \n(data.sort_values(by=['score'], ascending=False)[:50]).index.values\n\n","6097230e":"\"\"\"\nWe selected top 50 fetaures and then tried to fit linear model\n\"\"\"\n\n\nz=df[['full_sq', 'floor', 'max_floor', 'build_year', 'trans_year',\n       'num_room', 'state', 'railroad_km', 'metro_min_avto',\n       'cemetery_km', 'public_healthcare_km', 'radiation_km',\n       'kindergarten_km', 'green_zone_km', 'industrial_km', 'park_km',\n       'workplaces_km', 'swim_pool_km', 'mosque_km', 'nuclear_reactor_km',\n       'school_km', 'big_church_km', 'material',\n       'public_transport_station_min_walk', 'green_part_500',\n       'additional_education_km', 'metro_km_avto', 'university_km',\n       'ttk_km', 'cafe_count_5000', 'catering_km', 'metro_min_walk',\n       'water_km', 'area_m', 'theater_km', 'ice_rink_km', 'Building_Age',\n       'green_part_1500', 'shopping_centers_km', 'big_road2_km',\n       'fitness_km', 'stadium_km', 'church_synagogue_km',\n       'zd_vokzaly_avto_km', 'big_market_km', 'bus_terminal_avto_km',\n       'hospice_morgue_km', 'green_part_1000', 'ts_km',\n       'railroad_station_avto_km']].select_dtypes(exclude=['object'])\ny=df[\"price_doc\"]\n\nfrom sklearn.preprocessing import StandardScaler\nstandradize=True\nif standradize:\n    print('Standradizing the data..')\n    #inf values can result from squaring\n    scaler = StandardScaler()\n    \n    z= scaler.fit_transform(z)\n\n    print('Data Standradized!')\n    \nx_train,x_test,y_train,y_test=train_test_split(z, y, test_size=0.33, random_state=42)\n#wo_null1.columns.values","9dd7a69a":"\"\"\"\nFitting linear regression on our dataset\n\"\"\"\n\nmodel=LinearRegression()\n\nmodel.fit(x_train,y_train)","ae8fac7d":"pred=model.predict(x_test)\n\nprint(\"mse\",mean_squared_error(pred,y_test))\nprint(\"mse\",mean_absolute_error(pred,y_test))\n","c3f3046e":"\"\"\"\nLets try and analyse our target variable and understand if its skewed or normally distributed \n\"\"\"\n\nsns.distplot(train['price_doc'] , fit=norm);\n\n\"\"\"\nFinding mu and sigma after fitting it to a normalized form \n\"\"\"\n(mu, sigma) = norm.fit(train['price_doc'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Price distribution')\n","ae2807ff":"\"\"\"\nLets also draw the qqplot of the same , basically qqplot are \nQ\u2013Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability \ndistributions by plotting their quantiles against each other.\n\"\"\"\nfig = plt.figure()\nres = stats.probplot(train['price_doc'], plot=plt)\nplt.show()","121db85a":"\"\"\"\nWe could see that the price_doc value is right skewed\nand as (linear) models work well on normally distributed data , \nwe need to transform this variable and make it more normally distributed. \nLets try log transformation and understand if it improved model perfomance\n\"\"\"\n\nsns.distplot(new_train[\"log_price\"], fit=norm);\n\n\"\"\"\nLets try and analyse our  transformed price and understand if its skewed or normally distributed \n\"\"\"\n(mu, sigma) = norm.fit(new_train[\"log_price\"])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('Log Price distribution')","4813ed7a":"\"\"\"\nLets also draw the qqplot of the transformed price\n\"\"\"\nfig = plt.figure()\nres = stats.probplot(new_train[\"log_price\"], plot=plt)\nplt.show()","7a217dc9":"\"\"\"\nLets apply regression now on Logtransformmed data\n\"\"\"","ac69b855":"\nz=new_train[['full_sq', 'floor', 'max_floor', 'build_year', 'trans_year',\n       'num_room', 'state', 'railroad_km', 'metro_min_avto',\n       'cemetery_km', 'public_healthcare_km', 'radiation_km',\n       'kindergarten_km', 'green_zone_km', 'industrial_km', 'park_km',\n       'workplaces_km', 'swim_pool_km', 'mosque_km', 'nuclear_reactor_km',\n       'school_km', 'big_church_km', 'material',\n       'public_transport_station_min_walk', 'green_part_500',\n       'additional_education_km', 'metro_km_avto', 'university_km',\n       'ttk_km', 'cafe_count_5000', 'catering_km', 'metro_min_walk',\n       'water_km', 'area_m', 'theater_km', 'ice_rink_km', 'Building_Age',\n       'green_part_1500', 'shopping_centers_km', 'big_road2_km',\n       'fitness_km', 'stadium_km', 'church_synagogue_km',\n       'zd_vokzaly_avto_km', 'big_market_km', 'bus_terminal_avto_km',\n       'hospice_morgue_km', 'green_part_1000', 'ts_km',\n       'railroad_station_avto_km']].select_dtypes(exclude=['object'])\ny=new_train[\"log_price\"]\n\nfrom sklearn.preprocessing import StandardScaler\nstandradize=True\nif standradize:\n    print('Standradizing the data..')\n    #inf values can result from squaring\n    scaler = StandardScaler()\n    \n    z= scaler.fit_transform(z)\n\n    print('Data Standradized!')\n    \nx_train,x_test,y_train,y_test=train_test_split(z, y, test_size=0.33, random_state=42)\n    \n#x_train,x_test,y_train,y_test=train_test_split(z, y, test_size=0.33, random_state=42)\n#wo_null1.columns.values","c3b21d0d":"\"\"\"\nFitting linear regression on our dataset\n\"\"\"\n\nmodel=LinearRegression()\n\nmodel.fit(x_train,y_train)","c021c482":"pred=model.predict(x_test)\n\nprint(\"rmse\",sqrt(mean_squared_error(pred,y_test)))\nprint(\"mse\",mean_absolute_error(pred,y_test))\nprint(\"Rsquare values\",r2_score(pred,y_test))","57ca1a3d":"pred_price=np.expm1(pred)\ny_test_price=np.expm1(y_test)\n\nprint(\"mse\",mean_squared_error(pred_price,y_test_price))\nprint(\"mse\",mean_absolute_error(pred_price,y_test_price))\nprint(\"Rsquare values\",r2_score(pred_price,y_test_price))","9d69c43d":"\"\"\"\nLets try implementing Polynomial regression \n\"\"\"\n  \npoly = PolynomialFeatures(degree = 2) \nX_poly = poly.fit_transform(x_train) \n\npoly.fit(X_poly, y_train) \nlin2 = LinearRegression() \nlin2.fit(X_poly, y_train) ","90137661":"pred=lin2.predict(poly.fit_transform(x_test)) \nprint(\"Mean absolute error\",mean_absolute_error(pred,y_test))\nprint(\"Rsquare values\",r2_score(pred,y_test))","912de135":"\"\"\"\nLets try implementing elasticnet i.e combination of lasso and ridge regresion \nand see if there is any improvement in our model\n\"\"\"\n\nregr = ElasticNet(random_state=0)\nregr.fit(x_train, y_train)\nElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n      max_iter=1000, normalize=False, positive=False, precompute=False,\n      random_state=0, selection='cyclic', tol=0.0001, warm_start=False)\n#print(regr.coef_) \nprint(regr.intercept_) \npred=regr.predict(x_test)\nprint(\"Root Mean square error\",sqrt(mean_squared_error(pred,y_test)))\nprint(\"Rsquare values\",r2_score(pred,y_test))","db58f1ae":"#Again lets implement xgboost on selected features along with Log transformed price value","4f01fe8b":"\"\"\"\nWe will implement k fold cross validation with Linear model as its a better way to train\nBelow is the sample copied code for reference \n\"\"\"\n\nx_train, x_test, y_train, y_test = train_test_split(z,y,test_size=0.2)\nclf = linear_model.Lasso()\nclf.fit(x_train,y_train)\naccuracy = clf.score(x_test,y_test)\nprint(\"accuracy\",accuracy)\npred=clf.predict(x_test)\nprint(\"Mean absolute error\",mean_absolute_error(pred,y_test))\n\n\nscores = cross_val_score(clf, x_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\nrmse_scores = np.sqrt(-scores)\nprint(rmse_scores)","4081185b":"\"\"\"\nIts better to convert data into Dmatrix before applying xgb \n\"\"\"\ndtrain = xgb.DMatrix(data=x_train,label= y_train)\ndval = xgb.DMatrix(data=x_test, label=y_test)\n\n\"\"\"\nIntialising XGB model and then fitting train dataa\n\"\"\"\n\nxgb_params = {\n    'eta': 0.1,\n    'max_depth': 6,\n    'subsample': 1.0,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\n\n# Uncomment to tune XGB `num_boost_rounds`\npartial_model = xgb.train(xgb_params, dtrain, num_boost_round=1000, evals=[(dval, 'val')],\n                       early_stopping_rounds=20, verbose_eval=20)\n\nnum_boost_round = partial_model.best_iteration","6f1b0866":"\npred=partial_model.predict(dval)\nprint(\"Mean absolute error\",mean_absolute_error(pred,y_test))\nprint(\"rmse\",sqrt(mean_squared_error(pred,y_test)))\nprint(\"Rsquare values\",r2_score(pred,y_test))","70bd6a31":"\ntest1=new_test[['full_sq', 'floor', 'Building_Age', 'max_floor', 'trans_year',\n       'kindergarten_km', 'state', 'metro_min_avto', 'num_room',\n       'public_transport_station_km', 'park_km', 'green_zone_km',\n       'railroad_km', 'school_km', 'industrial_km', 'water_km',\n       'catering_km', 'metro_km_avto', 'additional_education_km',\n       'cemetery_km', 'build_year', 'big_road1_km', 'fitness_km',\n       'public_healthcare_km', 'material', 'mosque_km', 'metro_min_walk',\n       'big_market_km', 'hospice_morgue_km', 'radiation_km',\n       'big_road2_km', 'water_treatment_km', 'green_part_1000',\n       'thermal_power_plant_km', 'shopping_centers_km', 'area_m',\n       'swim_pool_km', 'stadium_km', 'market_shop_km',\n       'railroad_station_walk_km', 'ts_km', 'theater_km', 'preschool_km',\n       'office_km', 'green_part_500', 'power_transmission_line_km',\n       'nuclear_reactor_km', 'prom_part_5000', 'zd_vokzaly_avto_km',\n       'bus_terminal_avto_km']].select_dtypes(exclude=['object'])\n\nfrom sklearn.preprocessing import StandardScaler\nstandradize=True\nif standradize:\n    print('Standradizing the data..')\n    #inf values can result from squaring\n    scaler = StandardScaler()\n    \n    test1= scaler.fit_transform(test1)\n\n    print('Data Standradized!')\ndval1 = xgb.DMatrix(data=test1, label=y_test)\nprint(test1.shape)","2c9a2024":"print(new_test.shape)\ntest.shape","506e6f9d":"\"\"\"\npred1=partial_model.predict(dval1)\n#pred2=pd.DataFrame(pred1)\ndf_sub = pd.DataFrame({'id': test[\"id\"], 'price_doc': pred1})\n\ndf_sub.to_csv('sub.csv', index=False)\n\"\"\"","fb33a56e":"df_sub.shape","c74867b4":"<font color=#8F00FF><h3 > Sberbank Russian Housing Market <h3><\/font>","2439ac94":"<font color=#8F00FF>Lets do Some Exploratory data analysis and have a better understanding of our data<\/font>","4dd32c25":"<font color=#8F00FF>Lets Implement our model and see if we can predict price of houses <\/font>","98984a91":"<font color=#8F00FF>Lets Explore and see some Basic details about our dataset<\/font>","36ad09ec":"<font color=#8F00FF>Since we saw there are so many null values lets try and handle them <\/font>"}}