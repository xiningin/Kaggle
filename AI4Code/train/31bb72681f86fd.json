{"cell_type":{"55a49612":"code","c3150fa8":"code","3ea6d23d":"code","49cbb1d6":"code","481c8141":"code","3ec87c5b":"code","f9f42298":"code","f212aba0":"code","b0bfc7b6":"code","fb470b5c":"code","55819f58":"code","02ea85e7":"code","515fd4e2":"code","3957669a":"code","03c79d30":"code","d812a534":"code","7257fc21":"code","e869117a":"code","8e201ed7":"code","d347c800":"code","326d18f3":"code","db30522d":"code","c6924db3":"code","586610e0":"code","0d4baba9":"code","d91fc92d":"code","33b40881":"code","f3a8822d":"code","a8846b5f":"code","9fcdfc1b":"code","2b92f94d":"code","3397436f":"code","52b4fe85":"code","e6376ae5":"code","284f86b2":"code","ff21d695":"code","693b951a":"code","679c4e95":"code","eaacf501":"code","edbb621f":"code","2efc0624":"code","44505723":"code","a78c8c54":"code","864c370b":"code","ad373022":"code","41a99bd9":"code","dff51135":"code","5927c4ee":"code","0ffb8e48":"code","f796dde4":"code","9dafad0e":"code","7ef733f6":"code","031caea1":"code","004c8b91":"code","60ff421f":"code","15b1465f":"code","56a5b4fd":"code","a1cb6155":"code","a793bf41":"code","718a9ab9":"code","9586f01d":"code","7751977f":"code","2b4e8c67":"code","ce1f0edd":"code","eb15643a":"code","8628dce0":"code","95ed2c6d":"code","0546f1c8":"code","799914b4":"code","8b14a588":"code","ab0dce05":"code","b6e98087":"code","010e23af":"code","8ff9f5f1":"code","94a3fc4a":"code","4fcc543a":"code","bd73373f":"code","f3909bbc":"code","0913f013":"markdown","c45b4fe4":"markdown","59ad1dc9":"markdown","b196195a":"markdown","442012ca":"markdown","0473732d":"markdown"},"source":{"55a49612":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","c3150fa8":"df_train = pd.read_csv('..\/input\/train.csv')","3ea6d23d":"df_train.columns","49cbb1d6":"df_train['SalePrice'].describe()","481c8141":"sns.distplot(df_train['SalePrice']);","3ec87c5b":"#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","f9f42298":"#\u5c45\u4f4f\u9762\u79ef\u5e73\u65b9\u82f1\u5c3a\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","f212aba0":"#\u5730\u4e0b\u5ba4\u9762\u79ef\u5e73\u65b9\u82f1\u5c3a\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)    #'SalePrice'\u548c'TotalBsmtSF'\u7528pd.concat\u62fc\u63a5\u6210\u4e00\u4e2a\u5217\u8868\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","b0bfc7b6":"#\u6574\u4f53\u6750\u6599\u548c\u9970\u9762\u8d28\u91cf\nvar = 'OverallQual'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","fb470b5c":"#\u539f\u65bd\u5de5\u65e5\u671f\nvar = 'YearBuilt'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","55819f58":"var = 'Neighborhood'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\n#fig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","02ea85e7":"#correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, square=True,cmap='YlGnBu');","515fd4e2":"k = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index   #\u76f8\u5173\u5ea6\u6700\u5927\u7684\u5341\u4e2a\u5217\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values,cmap='YlGnBu')\nplt.show()\n","3957669a":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols], size = 2.5)\nplt.show();","03c79d30":"#missing data\ntotal = df_train.isnull().sum().sort_values(ascending=False)  #\u4ece\u5927\u5230\u5c0f\u6392\u5e8f\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","d812a534":"import pandas as pd\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\n#\u770b\u770b\u6570\u636e\u591a\u5927\u7684\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#ID\u5148\u7559\u7740\uff0c\u6682\u65f6\u4e0d\u7528\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#\u53bb\u6389ID\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#\u770b\u4e00\u4e0b\u73b0\u5728\u7684\u6570\u636e\u7684shape\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","7257fc21":"#\u53d1\u73b0\u79bb\u7fa4\u70b9\nfig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","e869117a":"#\u5e72\u6389\u79bb\u7fa4\u70b9\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","8e201ed7":"sns.distplot(train['SalePrice'] , fit=norm);\n\n(mu, sigma) = norm.fit(train['SalePrice'])   #\u6b63\u592a\u62df\u5408\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#\u5206\u5e03\u56fe\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#QQ\u56fe\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","d347c800":"#\u5bf9\u6570\u53d8\u6362log(1+x)\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#\u770b\u770b\u65b0\u7684\u5206\u5e03\nsns.distplot(train['SalePrice'] , fit=norm);\n\n# \u53c2\u6570\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#\u753b\u56fe\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#QQ\u56fe\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","326d18f3":"ntrain = train.shape[0]   #\u884c\u6570\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","db30522d":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","c6924db3":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","586610e0":"all_data[\"PoolQC\"][:5]","0d4baba9":"#\u6e38\u6cf3\u6c60\uff1f\u4e0a\u6d41\u793e\u4f1a\uff1f\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")","d91fc92d":"all_data[\"PoolQC\"][:5]","33b40881":"#\u6ca1\u6709\u7279\u5f81\u3002\u3002\u3002\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")","f3a8822d":"#\u901a\u9053\u7684\u5165\u53e3\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")","a8846b5f":"#\u6805\u680f\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")","9fcdfc1b":"#\u58c1\u7089\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","2b92f94d":"#\u5230\u8857\u9053\u7684\u8ddd\u79bb\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","3397436f":"#\u8f66\u5e93\u7684\u4e8b\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","52b4fe85":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","e6376ae5":"#\u5730\u4e0b\u5ba4\u7684\u4e8b\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","284f86b2":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","ff21d695":"#\u780c\u4f53\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","693b951a":"all_data['MSZoning'].mode()","679c4e95":"#\u4e00\u822c\u5206\u533a\u5206\u7c7b\uff0c\u7528\u4f17\u6570\u6765\u5427\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","eaacf501":"#Functional\u5bb6\u5ead\u529f\u80fd\u8bc4\u5b9a\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","edbb621f":"#\u7535\u529b\u7cfb\u7edf\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","2efc0624":"#\u53a8\u623f\u7684\u54c1\u8d28\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","44505723":"#\u5916\u90e8\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","a78c8c54":"#\u9500\u552e\u7c7b\u578b\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","864c370b":"#\u5efa\u7b51\u7c7b\u578b\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","ad373022":"all_data = all_data.drop(['Utilities'], axis=1)\n\nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","41a99bd9":"#\u6709\u4e9b\u5e76\u4e0d\u662f\u8fde\u7eed\u503c\uff0c\u7ed9\u4ed6\u4eec\u505a\u6210\u7c7b\u522b\u503c\u5427\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","dff51135":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))\nall_data.head()","5927c4ee":"#\u589e\u52a0\u4e00\u4e2a\u65b0\u7279\u5f81\u603b\u9762\u79ef\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","0ffb8e48":"from scipy.stats import norm, skew\n\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","f796dde4":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    ","9dafad0e":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","7ef733f6":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","031caea1":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","004c8b91":"n_folds = 5\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","60ff421f":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","15b1465f":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","56a5b4fd":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","a1cb6155":"GBoost = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","a793bf41":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             nthread = -1)\n","718a9ab9":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","9586f01d":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","7751977f":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","2b4e8c67":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","ce1f0edd":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","eb15643a":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","8628dce0":"score = rmsle_cv(model_lgb)\nprint(\"lightfm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","95ed2c6d":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","0546f1c8":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","799914b4":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","8b14a588":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","ab0dce05":"averaged_models.fit(train.values, y_train)\ny_pre_averaged_models =  averaged_models.predict(test.values)\n#y_pre_averaged_models = np.expm1(y_pre_averaged_models)","b6e98087":"stacked_averaged_models.fit(train.values, y_train)\ny_pre_stacked_averaged_models = stacked_averaged_models.predict(test.values)\n#y_pre_stacked_averaged_models = np.expm1(y_pre_stacked_averaged_models)","010e23af":"model_xgb.fit(train.values, y_train)\nxgb_pred = model_xgb.predict(test.values)\n#xgb_pred = np.expm1(xgb_train_pred)","8ff9f5f1":"model_lgb.fit(train.values, y_train)\nlgb_pred = model_lgb.predict(test.values)\n#lgb_pred = np.expm1(lgb_train_pred)","94a3fc4a":"ensemble = y_pre_averaged_models*0.5 + y_pre_stacked_averaged_models *0.5 \n","4fcc543a":"pre_df = pd.DataFrame()\npre_df[\"SalePrice\"] = np.exp(ensemble)-1\npre_df = pre_df[[\"SalePrice\"]]\n\n","bd73373f":"submission = pd.DataFrame()\nsubmission['Id'] = test_ID\nsubmission = pd.concat([submission['Id'],pre_df[\"SalePrice\"]],axis=1)\nsubmission","f3909bbc":"submission.to_csv('1.csv',index=False)","0913f013":"ElasticNet\u540c\u65f6\u4f7f\u7528l1\u548cl2","c45b4fe4":"## \u6837\u672c\u6b63\u592a\u5206\u5e03\u53d8\u6362","59ad1dc9":"KernelRidge\u5e26\u6709\u6838\u51fd\u6570\u7684\u5cad\u56de\u5f52","b196195a":"\u5bf9\u4e8e\u54b1\u7684\u8fd9\u4e9b\u7279\u5f81\u662f\u4e0d\u662f\u4e5f\u5f97\u53d8\u6362\u4e0b\u5440","442012ca":"## Box-Cox\u53d8\u6362\n\nBox-Cox \u53d8\u6362\u5728\u4e0a\u4e16\u7eaa\u516d\u5341\u5e74\u4ee3\u7531\u4e24\u4f4d\u82f1\u56fd\u7edf\u8ba1\u5b66\u5bb6 George E.P. Box \u548c David Cox \u63d0\u51fa\n\n\u5047\u8bbe\u6837\u672c\u91cc\u4e00\u5171\u6709 n \u4e2a\u6570\u636e\u70b9\uff0c\u5206\u522b\u662fy1 y2...yn,\u627e\u5230\u4e00\u4e2a\u5408\u9002\u7684\u51fd\u6570\u4f7f\u5f97\u6570\u636e\u70b9\u7ecf\u8fc7\u53d8\u6362\u4e4b\u540e\u80fd\u6574\u4f53\u7684\u6b63\u592a\u578b\u80fd\u591f\u6700\u597d\n\n\n<img src=\".\/img\/1.png\" alt=\"FAO\" width=\"290\" >\n<img src=\".\/img\/2.png\" alt=\"FAO\" width=\"190\" >\n\n\u5173\u952e\u70b9\u5728\u4e8e\u5982\u4f55\u627e\u5230\u4e00\u4e2a\u5408\u9002\u7684\u53c2\u6570\uff0c\u4e00\u822c\u60c5\u51b5\u4e0b0.15\u4e3a\u7ecf\u9a8c\u503c","0473732d":"make_pipeline\uff1a\u7ea7\u8054\u8d77\u6765\u53bb\u505a\u4e8b\nRobustScaler\uff1a\u66f4\u9002\u5408\u5904\u7406\u79bb\u7fa4\u70b9"}}