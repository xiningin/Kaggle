{"cell_type":{"c9abf6aa":"code","c938de6c":"code","8a5c5f02":"code","a64c3d36":"code","91b8949f":"code","69c8f9f1":"code","74020099":"code","ea11fb23":"code","d2fa020c":"code","aeaf11b1":"code","254379e4":"code","4d542fe6":"code","e829d8c2":"code","36566642":"code","89d9b4fa":"code","440e47cc":"code","83023453":"code","23bfd212":"code","8ef09f66":"markdown","140b8d61":"markdown","be3d29a3":"markdown","5f530339":"markdown","3cbced6f":"markdown","fc493cc6":"markdown","c19afc30":"markdown","1ccad469":"markdown","bd9c4972":"markdown"},"source":{"c9abf6aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c938de6c":"import pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")\ndf.head()","8a5c5f02":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","a64c3d36":"(df.isna().sum()).sort_values(ascending=True)","91b8949f":"df.shape\nfeature = df[['age','sex','cp','chol']]\nprint(feature)","69c8f9f1":"\nsns.set_theme(style=\"ticks\")\nsns.pairplot(feature)","74020099":"df.hist(bins = 30, figsize = (18, 25));","ea11fb23":"for col in df.select_dtypes(include=['float64','int64']):\n    plt.figure()\n    sns.displot(df[col],kind='kde',height=3)\n    plt.show()","d2fa020c":"df.corr().T\nplt.figure(figsize=(16, 6))\nsns.heatmap(df.corr(),annot=True);","aeaf11b1":"df.dtypes","254379e4":"print (df.corr()['output'].abs().sort_values())","4d542fe6":"y = df['output']\nriskyDF = df[y == 1]\nsafeDF = df[y == 0]\nfor col in df.select_dtypes(include=['float64','int64']):\n    plt.figure(figsize=(4,4))\n    sns.distplot(riskyDF[col],label='High Risk')\n    sns.distplot(safeDF[col],label='Low Risk')\n    plt.legend()\n    plt.show()","e829d8c2":"from sklearn.model_selection import train_test_split\nX = df.drop('output',axis=1)\ny = df['output']  \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 4)\nprint(\"Training Data set:\\n\",y.value_counts())\nprint(\"----------------------------\")\nprint(\"Test Data set:\\n\",X['sex'].value_counts())\n","36566642":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import RobustScaler","89d9b4fa":"preprocessor = make_pipeline(RobustScaler())\nRandomPipeline = make_pipeline(preprocessor,RandomForestClassifier(random_state=0))\nAdaPipeline = make_pipeline(preprocessor,AdaBoostClassifier(random_state=0))\nSVMPipeline = make_pipeline(preprocessor,SVC(random_state=0,probability=True))\nKNNPipeline = make_pipeline(preprocessor,KNeighborsClassifier())\nLRPipeline = make_pipeline(preprocessor,LogisticRegression())","440e47cc":"dict_of_models = {'RandomForest': RandomPipeline,\n'AdaBoost': AdaPipeline,\n'SVM': SVMPipeline,\n'KNN': KNNPipeline,\n'LR': LRPipeline}","83023453":"from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report, roc_curve\nfrom sklearn.model_selection import learning_curve\n\ndef evaluation(model):\n    model.fit(X_train, y_train)\n    # calculating the probabilities\n    y_pred_proba = model.predict_proba(X_test)\n\n    # finding the predicted valued\n    y_pred = np.argmax(y_pred_proba,axis=1)\n    print('Accuracy = ', accuracy_score(y_test, y_pred))\n    print('-')\n    print(confusion_matrix(y_test,y_pred))\n    print('-')\n    print(classification_report(y_test,y_pred))\n    print('-')\n    \n    N, train_score, val_score = learning_curve(model, X_train, y_train, \n                                               cv=4, scoring='f1', \n                                               train_sizes=np.linspace(0.1,1,10))\n    plt.figure(figsize=(12,8))\n    plt.plot(N, train_score.mean(axis=1), label='train score')\n    plt.plot(N, val_score.mean(axis=1), label='validation score')\n    plt.legend()\n\nfor name, model in dict_of_models.items():\n    print('---------------------------------')\n    print(name)\n    evaluation(model)","23bfd212":"RandomPipeline.fit(X_train, y_train)\ny_proba = RandomPipeline.predict_proba(X_test)\ny_pred = np.argmax(y_proba,axis=1)\n\nprint(\"Random Forest : \", accuracy_score(y_test, y_pred))\ny_pred_prob = RandomPipeline.predict_proba(X_test)[:,1]\n\nfpr,tpr,threshols=roc_curve(y_test,y_pred_prob)\n\nplt.plot(fpr,tpr,label='Random Forest Curve')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Random Forest ROC Curve\")\nplt.show()","8ef09f66":"# **Risk of Heart Attack Understanding based on Features**","140b8d61":"About this dataset\n\nAge : Age of the patient\n\nSex : Sex of the patient\n\nexang: exercise induced angina (1 = yes; 0 = no)\n\nca: number of major vessels (0-3)\n\ncp : Chest Pain type chest pain type\n\nValue 1: typical angina\nValue 2: atypical angina\nValue 3: non-anginal pain\nValue 4: asymptomatic\ntrtbps : resting blood pressure (in mm Hg)\n\nchol : cholestoral in mg\/dl fetched via BMI sensor\n\nfbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\nrest_ecg : resting electrocardiographic results\n\nValue 0: normal\nValue 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\nValue 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\nthalach : maximum heart rate achieved\n\ntarget : 0= less chance of heart attack 1= more chance of heart attack","be3d29a3":"# **Density Curve**","5f530339":"We can see that in the charts we have different types of columns, being categorical and numerical:\n\nCategorical columns: sex, exng, caa, cp, fbs, restecg, slp and thall.\n\nNumeric columns: age, trtbps, chol, thalachh and oldpeak.","3cbced6f":"# **Model Evaluation**","fc493cc6":"# **Correlation**","c19afc30":"# **Visualization**","1ccad469":"No none values observed","bd9c4972":"# **Dictionary of Models**"}}