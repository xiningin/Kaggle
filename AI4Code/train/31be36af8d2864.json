{"cell_type":{"f10053d2":"code","7d3a148f":"code","223f66ff":"code","42a5f2c3":"code","b7f0763e":"code","63664f25":"code","6be85c26":"code","d7dab4d8":"code","4d74fd90":"code","68cb4f57":"code","7332e6d3":"code","616bf1c4":"code","72e2c2e5":"code","b2b8457c":"code","a75b925c":"code","71908e77":"code","235631a9":"code","3944d2a4":"code","4968c7b4":"code","b78ea4c9":"code","1a3d05d8":"code","ebc636f5":"code","9b354453":"code","357cf3b9":"code","52d3e3c9":"code","a75f553e":"code","75439747":"code","eb44b5d0":"markdown","046a8df6":"markdown","71ab3f31":"markdown","d1182bed":"markdown","bf780e6f":"markdown","a9fa2d2a":"markdown","5c775db5":"markdown","6be653b2":"markdown","30de2edf":"markdown","c6a9cbf7":"markdown","7a967c98":"markdown","d8b90086":"markdown","4bea0469":"markdown","50e6a772":"markdown","7b713d78":"markdown"},"source":{"f10053d2":"import numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import LabelBinarizer\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","7d3a148f":"# list out the available files in the input path\nimport os\nprint(os.listdir(\"..\/input\"))","223f66ff":"!pip install tensorflow-gpu==2.0.0-beta1","42a5f2c3":"!pip install -q tensorflow_hub","b7f0763e":"import tensorflow as tf\nimport tensorflow_hub as hub","63664f25":"print(tf.__version__)","6be85c26":"train_dir=\"..\/input\/train\/train\"\ntest_dir=\"..\/input\/test\/test\"\ntrain = pd.read_csv('..\/input\/train.csv')\nsub_file = pd.read_csv(\"..\/input\/sample_submission.csv\")\ndata_folder = \"..\/input\"","d7dab4d8":"train.head()","4d74fd90":"sub_file.head()","68cb4f57":"def show_images(directory, df, is_train=True):\n    plt.figure(figsize=(15,15))\n    for i in range(10):\n        n = np.random.choice(df.shape[0], 1)\n        plt.subplot(5,5,i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(True)\n        image = plt.imread(os.path.join(directory, df[\"id\"][int(n)]))\n        plt.imshow(image)\n        if is_train:\n            label = df[\"has_cactus\"][int(n)]\n            plt.xlabel(label)\n    plt.show()\n","7332e6d3":"# train set\nshow_images(train_dir, train)","616bf1c4":"# test set\nshow_images(test_dir, sub_file, is_train=False)","72e2c2e5":"train[\"has_cactus\"].value_counts()","b2b8457c":"# 90% for train\npartial_train = train.sample(frac=0.9)\ntrain.drop(partial_train.index, axis=0, inplace=True)\n\n# 10% for validation\nvalid = train","a75b925c":"partial_train[\"has_cactus\"].value_counts()","71908e77":"valid[\"has_cactus\"].value_counts()","235631a9":"# account for skew in the labeled data\nlb = LabelBinarizer()\ny_train = lb.fit_transform(partial_train[\"has_cactus\"])\nclassTotals = y_train.sum(axis=0)\nclassWeight = classTotals.max() \/ classTotals","3944d2a4":"# convert the data-type of the labels to string to make it compatible with\n# ImageDataGenerator\npartial_train[\"has_cactus\"] = partial_train[\"has_cactus\"].astype(\"str\") \nvalid[\"has_cactus\"] = valid[\"has_cactus\"].astype(\"str\") \nsub_file[\"has_cactus\"] = sub_file[\"has_cactus\"].astype(\"str\")","4968c7b4":"# set up the data augmentation objects\ntrainAug = tf.keras.preprocessing.image.ImageDataGenerator(\n  horizontal_flip=True,\n  fill_mode=\"nearest\")\n\nvalAug = tf.keras.preprocessing.image.ImageDataGenerator()\n\n# define the ImageNet mean subtraction (in RGB order) and set the\n# the mean subtraction value for each of the data augmentation\n# objects\nmean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\ntrainAug.mean = mean\nvalAug.mean = mean\n\ntrainGen = trainAug.flow_from_dataframe(partial_train, directory=train_dir, \n    x_col=\"id\", y_col=\"has_cactus\", target_size=(224, 224), \n    class_mode=\"categorical\", batch_size=64, shuffle=True)\n\nvalGen = valAug.flow_from_dataframe(valid, directory=train_dir, \n    x_col=\"id\", y_col=\"has_cactus\", target_size=(224, 224), \n    class_mode=\"categorical\", batch_size=64)\n\ntestGen = valAug.flow_from_dataframe(sub_file, directory=test_dir, \n    x_col=\"id\", y_col=\"has_cactus\", target_size=(224, 224), \n    class_mode=\"categorical\", batch_size=64)","b78ea4c9":"# define the input dimension of the KerasLayer and then set its layers to\n# trainable to adapt to our dataset\nfeature_extractor_url = \"https:\/\/tfhub.dev\/google\/tf2-preview\/mobilenet_v2\/feature_vector\/2\"\nfeature_extractor_layer = hub.KerasLayer(feature_extractor_url,\n                                         input_shape=(224,224,3))\nfeature_extractor_layer.trainable = True","1a3d05d8":"model = tf.keras.Sequential([\n  feature_extractor_layer,\n  tf.keras.layers.Dense(2, activation=\"sigmoid\")\n])","ebc636f5":"model.compile(\n  optimizer=tf.keras.optimizers.Adam(),\n  loss='categorical_crossentropy',\n  metrics=['acc'])","9b354453":"H = model.fit_generator(\n    trainGen,\n    steps_per_epoch=partial_train.shape[0] \/\/ 64,\n    validation_data=valGen,\n    validation_steps=valid.shape[0] \/\/ 64,\n    epochs=5,\n    class_weight=classWeight,\n    verbose=1)","357cf3b9":"def plot_training(H, N):\n    plt.style.use(\"ggplot\")\n    plt.figure(figsize=(10,8))\n    plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n    plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n    plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n    plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n    plt.title(\"Training Loss and Accuracy\")\n    plt.xlabel(\"Epoch #\")\n    plt.ylabel(\"Loss\/Accuracy\")\n    plt.legend(loc=\"upper center\")","52d3e3c9":"plot_training(H, 5)","a75f553e":"# get the predictions from the network and map \n# the class-labels accordingly\npredIdxs = model.predict_generator(testGen,\n    steps=(sub_file.shape[0] \/\/ 64) + 1)\npredIdxs = np.argmax(predIdxs, axis=1)","75439747":"sub_file.has_cactus = predIdxs\nsub_file.to_csv('submission.csv', index=False)","eb44b5d0":"**Context**\n\nTo assess the impact of climate change on Earth's flora and fauna, it is vital to quantify how human activities such as logging, mining, and agriculture are impacting our protected natural areas. Researchers in Mexico have created the [VIGIA project](https:\/\/jivg.org\/research-projects\/vigia\/), which aims to build a system for autonomous surveillance of protected areas. A first step in such an effort is the ability to recognize the vegetation inside the protected areas. In this competition, you are tasked with creation of an algorithm that can identify a specific type of cactus in aerial imagery.\n\n**Provided data description**\n\nThis dataset contains a large number of 32 x 32 thumbnail images containing aerial photos of a columnar cactus (Neobuxbaumia tetetzo). Kaggle has resized the images from the original dataset to make them uniform in size. The file name of an image corresponds to its id.\n\nI will be using the fastai library for doing my experiments. I will be approaching the problem with a deep-learning based solution.","046a8df6":"### Installation and imports","71ab3f31":"### Data augmentation set up","d1182bed":"As we can see above, there is a class imabalance & we will handle this accordingly while training our model. We now split the available training set into additional training and validation sets.","bf780e6f":"### References:\n- [TensorFlow Hub with Keras](https:\/\/www.tensorflow.org\/beta\/tutorials\/images\/hub_with_keras)\n- [Fine-tuning with Keras and Deep Learning](https:\/\/www.pyimagesearch.com\/2019\/06\/03\/fine-tuning-with-keras-and-deep-learning\/)","a9fa2d2a":"### Model training","5c775db5":"A utility function to show 10 randomly selected images from the provided data split.","6be653b2":"### Inference on the test set and submission","30de2edf":"Let's check out the class distribution in the train set. ","c6a9cbf7":"### Transfer learning using `TF-Hub`\n\nWe start by downloading the headless MobileNetV2 model without its classification head. This model was trained on the ImageNet dataset.","7a967c98":"### Loading in the data files","d8b90086":"Let's now use the `Sequential` API of Keras to add a dense layer on top of the feature extraction layer. ","4bea0469":"Let's check the class distributions in these two newly created splits. ","50e6a772":"We get a decent accuracy of **99.65%** on the validation set. We now plot the training history to look for any sign of overfitting. ","7b713d78":"We now compile the model supplying the optimizer, loss function and the metrics we are interested in. "}}