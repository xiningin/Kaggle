{"cell_type":{"fdb19224":"code","ebaaf4da":"code","28941e89":"code","9ff62e28":"code","cc2bc4e6":"code","3c21ba53":"code","2806218c":"code","751c95b9":"code","3dc8d8b4":"code","72c1aba2":"code","43e5cb6c":"code","6756da37":"code","4bcc6ccf":"code","ae1c3ca5":"code","40df09fd":"code","dd57020a":"code","b4acf1ad":"code","e42c94eb":"code","7218cf47":"code","08cb1ea8":"code","a4d10438":"code","5388ac64":"code","0124e243":"code","6012be64":"code","7cbec704":"code","81e4995e":"code","6a63dc83":"code","e9441996":"code","fccd32f0":"code","251ef8b8":"code","1aca2367":"code","46f20980":"code","8205f4a6":"code","836bdb07":"code","e12c48a0":"code","95e03354":"code","37235ae6":"code","50062d21":"code","fdf6dc0e":"code","5028c317":"code","5f068205":"code","39916180":"code","69300adb":"code","6896ceb5":"code","0eb2e3f7":"code","e05eb83b":"code","70f2feef":"markdown","e3189a79":"markdown","f8ba508e":"markdown","eb1b690d":"markdown","bdcc4a28":"markdown","e094da63":"markdown","7253b72a":"markdown","ccc48add":"markdown","730f6251":"markdown","abee4673":"markdown","20938a45":"markdown","10466b88":"markdown"},"source":{"fdb19224":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score,classification_report, confusion_matrix\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nimport pandas_profiling as pp\nfrom sklearn.compose import ColumnTransformer\nimport  tensorflow as tf\nimport keras\nfrom tpot import TPOTClassifier","ebaaf4da":"df = pd.read_csv('..\/input\/learn-together\/train.csv')\ndf.head()","28941e89":"df_test = pd.read_csv('..\/input\/learn-together\/test.csv')\ndf_test.head()","9ff62e28":"print('Train size: ',df.shape)\nprint('Test size: ', df_test.shape)","cc2bc4e6":"df.info()","3c21ba53":"df.isnull().mean()","2806218c":"df.describe().T","751c95b9":"colormap = plt.cm.RdBu\nplt.figure(figsize=(50,35))\nplt.title('Pearson Correlation of Features', y=1.05, size=50)\nsns.heatmap(df.corr(),linewidths=0.1, vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","3dc8d8b4":"df.duplicated().sum() #No Duplicate Data","72c1aba2":"## Checking if we have a balance dataset\ntarget = df.Cover_Type.value_counts()\nsns.countplot(x='Cover_Type', data=df)\nplt.title('Class Distribution');\nprint(target) # Balanced Train Dataset.","43e5cb6c":"pp.ProfileReport(df)","6756da37":"#Soil_Type7 and  Soil_Type15 have zero values, so removing them. \ndf.drop(['Soil_Type7', 'Id','Soil_Type15'], axis=1, inplace=True)","4bcc6ccf":"df.Cover_Type.value_counts()","ae1c3ca5":"# separate intro train and test set\n\nX_train, X_test, y_train, y_test = train_test_split(\n    df.drop(['Cover_Type'], axis=1),  # just the features\n    df['Cover_Type'],  # the target\n    test_size=0.2,  # the percentage of obs in the test set\n    random_state=42)  # for reproducibility\n\nX_train.shape, X_test.shape","40df09fd":"plt.figure(figsize=(10,10))\nplt.scatter(y=df.Hillshade_9am, x=df.Hillshade_3pm)\nplt.xlabel(\"Hillshade_3pm\")\nplt.ylabel(\"Hillshade_9am\")\nplt.title(\"Hillshade_3pm VS Hillshade_9am\")","dd57020a":"plt.figure(figsize=(10,10))\nplt.scatter(y=df.Hillshade_Noon, x=df.Slope)\nplt.xlabel(\"Slope\")\nplt.ylabel(\"Hillshade_Noon\")\nplt.title(\"Slope VS Hillshade_Noon\")","b4acf1ad":"plt.figure(figsize=(10,10))\nplt.scatter(x=df.Hillshade_9am, y=df.Aspect)\nplt.ylabel(\"Aspect\")\nplt.xlabel(\"Hillshade_9am\")\nplt.title(\"Aspect VS Hillshade_9am\")","e42c94eb":"plt.figure(figsize=(10,10))\nplt.scatter(x=df.Hillshade_3pm, y=df.Aspect)\nplt.ylabel(\"Aspect\")\nplt.xlabel(\"Hillshade_3pm\")\nplt.title(\"Aspect VS Hillshade_3pm\")","7218cf47":"plt.figure(figsize=(10,10))\nplt.scatter(x=df.Vertical_Distance_To_Hydrology, y=df.Horizontal_Distance_To_Hydrology)\nplt.ylabel(\"Horizontal_Distance_To_Hydrology\")\nplt.xlabel(\"Vertical_Distance_To_Hydrology\")\nplt.title(\"Horizontal_Distance_To_Hydrology VS Vertical_Distance_To_Hydrology\")","08cb1ea8":"plt.figure(figsize=(12,12))\ncols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n       'Horizontal_Distance_To_Fire_Points', 'Cover_Type']\nsns.pairplot(df[cols][df.Cover_Type==4])","a4d10438":"#tpot = TPOTClassifier(generations=5,population_size=10,verbosity=2, n_jobs=-1)\n#tpot.fit(X_train, y_train)\n#print(tpot.score(X_test, y_test))\n#print(tpot.score(X_train, y_train))\n#tpot.export('tpot_tree_classification_pipeline.py')\n#!cat tpot_tree_classification_pipeline.py\n#tpot.evaluated_individuals_\n#tpot.fitted_pipeline_\n#print(classification_report(y_test, tpot.predict(X_test)))\n#print(confusion_matrix(y_test, tpot.predict(X_test)))","5388ac64":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom tpot.builtins import StackingEstimator\n\n\n# Average CV score on the training set was:0.8458153791211421\nexported_pipeline = make_pipeline(\n    StackingEstimator(estimator=GradientBoostingClassifier(learning_rate=0.5, max_depth=9, max_features=0.25, min_samples_leaf=17, min_samples_split=6, n_estimators=100, subsample=0.8)),\n    RandomForestClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.25, min_samples_leaf=18, min_samples_split=17, n_estimators=100)\n)\n\nexported_pipeline.fit(X_train, y_train)\nresults = exported_pipeline.predict(X_test)","0124e243":"print(exported_pipeline.score(X_test, y_test))\nprint(exported_pipeline.score(X_train, y_train))","6012be64":"print(classification_report(y_test, exported_pipeline.predict(X_test)))\nprint(confusion_matrix(y_test, exported_pipeline.predict(X_test)))","7cbec704":"result_final = exported_pipeline.predict(df_test.drop(['Soil_Type7', 'Id', 'Soil_Type15'], axis=1))\nresult_final_proba = exported_pipeline.predict_proba(df_test.drop(['Soil_Type7', 'Id', 'Soil_Type15'], axis=1))\n#df_test.drop(['Soil_Type7', 'Id', 'Soil_Type15'], axis=1, inplace=True)","81e4995e":"result_final_proba[0]","6a63dc83":"# Save test predictions to file\n#output = pd.DataFrame({'ID': df_test.Id,\n#                       'Cover_Type': result_final})\n#output.to_csv('submission.csv', index=False)","e9441996":"#pd.DataFrame(output).iloc[0]","fccd32f0":"#result_final_proba[0]","251ef8b8":"import seaborn as sns\ncols = ['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Horizontal_Distance_To_Roadways','Hillshade_9am','Hillshade_Noon','Hillshade_3pm','Horizontal_Distance_To_Fire_Points', 'Cover_Type']\nsns.pairplot(df[cols], hue=\"Cover_Type\")","1aca2367":"train = df.copy()\ndel df","46f20980":"test = df_test.copy()\ndel df_test","8205f4a6":"# train.head()\ntrain['HorizontalHydrology_HorizontalFire'] = (train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points'])\ntrain['Neg_HorizontalHydrology_HorizontalFire'] = (train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])\ntrain['HorizontalHydrology_HorizontalRoadways'] = (train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])\ntrain['Neg_HorizontalHydrology_HorizontalRoadways'] = (train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])\ntrain['HorizontalFire_Points_HorizontalRoadways'] = (train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])\ntrain['Neg_HorizontalFire_Points_HorizontalRoadways'] = (train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])\n\ntrain['Neg_Elevation_Vertical'] = train['Elevation']-train['Vertical_Distance_To_Hydrology']\ntrain['Elevation_Vertical'] = train['Elevation']+train['Vertical_Distance_To_Hydrology']\n\ntrain['mean_hillshade'] =  (train['Hillshade_9am']  + train['Hillshade_Noon'] + train['Hillshade_3pm'] )\n\ntrain['Mean_HorizontalHydrology_HorizontalFire'] = (train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points'])\ntrain['Mean_HorizontalHydrology_HorizontalRoadways'] = (train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])\ntrain['Mean_HorizontalFire_Points_HorizontalRoadways'] = (train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])\n\ntrain['MeanNeg_Mean_HorizontalHydrology_HorizontalFire'] = (train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])\ntrain['MeanNeg_HorizontalHydrology_HorizontalRoadways'] = (train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])\ntrain['MeanNeg_HorizontalFire_Points_HorizontalRoadways'] = (train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])\n\ntrain['Slope2'] = np.sqrt(train['Horizontal_Distance_To_Hydrology']**2+train['Vertical_Distance_To_Hydrology']**2)\ntrain['Mean_Fire_Hydrology_Roadways']=(train['Horizontal_Distance_To_Fire_Points'] + train['Horizontal_Distance_To_Hydrology'] + train['Horizontal_Distance_To_Roadways'])\ntrain['Mean_Fire_Hyd']=(train['Horizontal_Distance_To_Fire_Points'] + train['Horizontal_Distance_To_Hydrology'])\n\ntrain[\"Vertical_Distance_To_Hydrology\"] = abs(train['Vertical_Distance_To_Hydrology'])\n\ntrain['Neg_EHyd'] = train.Elevation-train.Horizontal_Distance_To_Hydrology\n\n\ntest['HorizontalHydrology_HorizontalFire'] = (test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points'])\ntest['Neg_HorizontalHydrology_HorizontalFire'] = (test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])\ntest['HorizontalHydrology_HorizontalRoadways'] = (test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])\ntest['Neg_HorizontalHydrology_HorizontalRoadways'] = (test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])\ntest['HorizontalFire_Points_HorizontalRoadways'] = (test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])\ntest['Neg_HorizontalFire_Points_HorizontalRoadways'] = (test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])\n\ntest['Neg_Elevation_Vertical'] = test['Elevation']-test['Vertical_Distance_To_Hydrology']\ntest['Elevation_Vertical'] = test['Elevation'] + test['Vertical_Distance_To_Hydrology']\n\ntest['mean_hillshade'] = (test['Hillshade_9am']  + test['Hillshade_Noon']  + test['Hillshade_3pm'] )\n\ntest['Mean_HorizontalHydrology_HorizontalFire'] = (test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points'])\ntest['Mean_HorizontalHydrology_HorizontalRoadways'] = (test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])\ntest['Mean_HorizontalFire_Points_HorizontalRoadways'] = (test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])\n\ntest['MeanNeg_Mean_HorizontalHydrology_HorizontalFire'] = (test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])\ntest['MeanNeg_HorizontalHydrology_HorizontalRoadways'] = (test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])\ntest['MeanNeg_HorizontalFire_Points_HorizontalRoadways'] = (test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])\n\ntest['Slope2'] = np.sqrt(test['Horizontal_Distance_To_Hydrology']**2+test['Vertical_Distance_To_Hydrology']**2)\ntest['Mean_Fire_Hydrology_Roadways']=(test['Horizontal_Distance_To_Fire_Points'] + test['Horizontal_Distance_To_Hydrology'] + test['Horizontal_Distance_To_Roadways'])\ntest['Mean_Fire_Hyd']=(test['Horizontal_Distance_To_Fire_Points'] + test['Horizontal_Distance_To_Hydrology'])\n\n\ntest['Vertical_Distance_To_Hydrology'] = abs(test[\"Vertical_Distance_To_Hydrology\"])\n\ntest['Neg_EHyd'] = test.Elevation-test.Horizontal_Distance_To_Hydrology","836bdb07":"train.head()","e12c48a0":"from sklearn.model_selection import train_test_split\nx = train.drop(['Cover_Type'], axis = 1)\n\ny = train['Cover_Type']\nprint( y.head() )\n\nx_train, x_test, y_train, y_test = train_test_split( x.values, y.values, test_size=0.05, random_state=42 )","95e03354":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n    \nfrom sklearn import decomposition\n\nscaler = StandardScaler()\nscaler.fit(x_train)\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)","37235ae6":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom tpot.builtins import StackingEstimator\n\n\n# Average CV score on the training set was:0.8458153791211421\nexported_pipeline = make_pipeline(\n    StackingEstimator(estimator=GradientBoostingClassifier(learning_rate=0.5, max_depth=9, max_features=0.25, min_samples_leaf=17, min_samples_split=6, n_estimators=100, subsample=0.8)),\n    RandomForestClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.25, min_samples_leaf=18, min_samples_split=17, n_estimators=100)\n)\n\nexported_pipeline.fit(x_train, y_train)\nresults = exported_pipeline.predict(x_test)","50062d21":"print(exported_pipeline.score(x_test, y_test))\nprint(exported_pipeline.score(x_train, y_train))","fdf6dc0e":"print(classification_report(y_test, exported_pipeline.predict(x_test)))\nprint(confusion_matrix(y_test, exported_pipeline.predict(x_test)))","5028c317":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#uncomment the commented code and uncomment the commented to perform gridsearchCV\nfrom xgboost import XGBClassifier as xgb\n\nclf = ExtraTreesClassifier(n_estimators=950, random_state=0)\nfrom sklearn.svm import LinearSVC\nfrom mlxtend.classifier import StackingCVClassifier\n\nc1 = ExtraTreesClassifier(n_estimators=500,bootstrap=True) \nc2= RandomForestClassifier(n_estimators=500,bootstrap=True)\nc3=xgb();\nmeta = LinearSVC()\nsclf = StackingCVClassifier(classifiers=[c1,c2,c3],use_probas=True,meta_classifier=meta)","5f068205":"sclf.fit(x_train, y_train)\nprint('Accuracy of classifier on training set: {:.2f}'.format(sclf.score(x_train, y_train) * 100))\nprint('Accuracy of classifier on test set: {:.2f}'.format(sclf.score(x_test, y_test) * 100))","39916180":"test.head()","69300adb":"test.columns","6896ceb5":"test.head()\n\nid = test['Id']\ntest.drop(['Id', 'Soil_Type7', 'Soil_Type15'] , inplace = True , axis = 1)\ntest = scaler.transform(test)","0eb2e3f7":"predictions = sclf.predict(test)","e05eb83b":"out = pd.DataFrame({'Id': id,'Cover_Type': predictions})\nout.to_csv('submission.csv', index=False)\nout.head(5)","70f2feef":"Now we should seperate the training set from the labels and name them x and y then we will split them into training and test sets to be able to see how well it would do on unseen data which will give anestimate on how well it will do when testing on Kaggle test data. I will use the convention of using 80% of the data as training set and 20% for the test set.","e3189a79":"No missing values. No categorical","f8ba508e":"## After several hyperparameter tuning and different classifications, below one is ging best results so far.","eb1b690d":"As you can see there are some important relations that the model can infere from these new features according to the plots and also the correlation matrix and the heatmap. I will now add these features to the training data and the test data. I have read many resources as this study, this grat course and from that great kernel.\n\nAlso it seems that the vertical distance contain some negative number and it gave me better performance when taken the absolute for the column. It is really important to notice that Tree based models only fits vertical and horizontal lines so it is very important to engineer some oblique or tilted features like slope and etc... .","bdcc4a28":"It is important to know if the number of points in the classes are balanced. If the data is skewed then we will not be able to use accuracy as a performance metric since it will be misleading but if it is skewed we may use F-beta score or precision and recall. Precision or recall or F1 score. the choice depends on the problem itself. Where high recall means low number of false negatives , High precision means low number of false positives and F1 score is a trade off between them. You can refere to this article for more about precision and recall http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_precision_recall.html","e094da63":"### Predicting types of trees in an area based on various geographic features\n\nForest Cover Types:\n\n1 - Spruce\/Fir 2 - Lodgepole Pine 3 - Ponderosa Pine 4 - Cottonwood\/Willow 5 - Aspen 6 - Douglas-fir 7 - Krummholz","7253b72a":"### Data Fields\n* 1. Elevation - Elevation in meters Aspect - Aspect in degrees azimuth Slope - Slope in degrees \n* 2. Horizontal_Distance_To_Hydrology - Horz Dist to nearest surface water features \n* 3. Vertical_Distance_To_Hydrology - Vert Dist to nearest surface water features \n* 4. Horizontal_Distance_To_Roadways - Horz Dist to nearest roadway \n* 5. Hillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice \n* 6. Hillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice \n* 7. Hillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice \n* 8. Horizontal_Distance_To_Fire_Points - Horz Dist to nearest wildfire ignition points \n* 9. Wilderness_Area (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation \n* 10. Soil_Type (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation \n* 11. Cover_Type (7 types, integers 1 to 7) - Forest Cover Type designation \n\nThe wilderness areas are:\n* 1 - Rawah Wilderness Area \n* 2 - Neota Wilderness Area \n* 3 - Comanche Peak Wilderness Area \n* 4 - Cache la Poudre Wilderness Area \n\nThe soil types are:\n* 1 Cathedral family - Rock outcrop complex, extremely stony. \n* 2 Vanet - Ratake families complex, very stony. \n* 3 Haploborolis - Rock outcrop complex, rubbly.\n* 4 Ratake family - Rock outcrop complex, rubbly. \n* 5 Vanet family - Rock outcrop complex complex, rubbly. \n* 6 Vanet - Wetmore families - Rock outcrop complex, stony. \n* 7 Gothic family. \n* 8 Supervisor - Limber families complex. \n* 9 Troutville family, very stony. \n* 10 Bullwark - Catamount families - Rock outcrop complex, rubbly. \n* 11 Bullwark - Catamount families - Rock land complex, rubbly. \n* 12 Legault family - Rock land complex, stony. \n* 13 Catamount family - Rock land - Bullwark family complex, rubbly. \n* 14 Pachic Argiborolis - Aquolis complex. \n* 15 unspecified in the USFS Soil and ELU Survey. \n* 16 Cryaquolis - Cryoborolis complex. \n* 17 Gateview family - Cryaquolis complex. \n* 18 Rogert family, very stony. \n* 19 Typic Cryaquolis - Borohemists complex. \n* 20 Typic Cryaquepts - Typic Cryaquolls complex. \n* 21 Typic Cryaquolls - Leighcan family, till substratum complex. \n* 22 Leighcan family, till substratum, extremely bouldery. \n* 23 Leighcan family, till substratum - Typic Cryaquolls complex. \n* 24 Leighcan family, extremely stony. \n* 25 Leighcan family, warm, extremely stony. \n* 26 Granile - Catamount families complex, very stony. \n* 27 Leighcan family, warm - Rock outcrop complex, extremely stony. \n* 28 Leighcan family - Rock outcrop complex, extremely stony. \n* 29 Como - Legault families complex, extremely stony. \n* 30 Como family - Rock land - Legault family complex, extremely stony. \n* 31 Leighcan - Catamount families complex, extremely stony. \n* 32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony. \n* 33 Leighcan - Catamount families - Rock outcrop complex, extremely stony. \n* 34 Cryorthents - Rock land complex, extremely stony. \n* 35 Cryumbrepts - Rock outcrop - Cryaquepts complex. \n* 36 Bross family - Rock land - Cryumbrepts complex, extremely stony. \n* 37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony. \n* 38 Leighcan - Moran families - Cryaquolls complex, extremely stony. \n* 39 Moran family - Cryorthents - Leighcan family complex, extremely stony. \n* 40 Moran family - Cryorthents - Rock land complex, extremely stony.\n\nChecking the data type of each attribute**","ccc48add":"Ckearly my model is overfitting. Will do some feature engineering and predict.","730f6251":"# Obeservations:\n1. Soil_Type7 and Soil_Type15 have zero values, so removing them.\n1. We have 10.5% and 12.5% zeros for Horizontal_Distance_To_Hydrology and Vertical_Distance_To_Hydrology. We need to check those zero values.\n1. Hillshade_9am and Hillshade_3pm are 80% correlated. Need to check.\n1. Hillshade_Noon and Slope are -50% correlated. Need to check.\n1. Hillshade_9am and Aspect are -58% correlated. Need to check.\n1. Hillshade_3PM and Aspect are 66% correlated. Need to check.\n1. Vertical_Distance_To_Hydrology and Horizontal_Distance_To_Hydrology are 63% correlated. Need to check.\n1. Soil_Types are highly cardinal(40 unique types). Need to check to reduce the cardinality.","abee4673":"### Feature Engineering","20938a45":"### Dimensions of Data\nWe have 15,120 rows and 56 columns in our training set while in our testing set we have 565,892 rows and 55 columns so the problem here is we have too many rows and our algorithms may take too long also in columns we have 56 this means that some algorithms can be distracted or suffer poor performance due to the curse of dimensionality.","10466b88":"### Data Description\nThe study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. You are asked to predict an integer classification for the forest cover type. The seven types are:\n\n* 1 - Spruce\/Fir\n* 2 - Lodgepole Pine\n* 3 - Ponderosa Pine\n* 4 - Cottonwood\/Willow\n* 5 - Aspen\n* 6 - Douglas-fir\n* 7 - Krummholz\n\nThe training set (15120 observations) contains both features and the Cover_Type. The test set contains only the features. You must predict the Cover_Type for every row in the test set (565892 observations)."}}