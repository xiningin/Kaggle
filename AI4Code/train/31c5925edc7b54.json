{"cell_type":{"b41af7f6":"code","a4a1fcb5":"code","a8d8e8c3":"code","5cd13d3c":"code","dbf77172":"code","480258ed":"code","6f307afd":"code","7bd394b3":"code","8688477f":"code","72ab3aea":"code","1e5ca43a":"code","0fb98ed9":"code","4b4d215a":"code","4666515e":"code","2eeed11a":"code","a239f3d2":"code","5f1d8af1":"code","acde008a":"code","9e483bfc":"code","f163e8f3":"code","e4fdf069":"code","b5ff2541":"code","ffc6e52a":"code","2dd0d050":"code","af23e0df":"code","ccc10707":"code","e0ef76f7":"code","600ef026":"code","c5afd427":"code","bccf27ba":"code","3569e177":"code","aa0c8936":"code","f666f725":"code","2d7968d4":"code","fed81c20":"code","c322e890":"code","4fbfba53":"code","025acdaa":"code","c8bce816":"code","2d752f40":"code","b4897105":"code","870d2fb8":"code","2f793fba":"code","6c3d1951":"code","a559b64e":"code","5fd17634":"code","5e1872c6":"code","c4914b74":"code","d51fd3c5":"code","2c3b84fa":"code","e7b4b715":"code","be6edefc":"code","cf2124de":"code","c38f86b3":"code","c8e6057d":"code","4a0dc355":"markdown","efda0a9f":"markdown","be0d9eee":"markdown","8ab3c24d":"markdown","1f10a470":"markdown","4f67133a":"markdown","69e6fddc":"markdown","1d7fa58c":"markdown","2d4b7925":"markdown","acdebaac":"markdown","b6a77df5":"markdown","74b38cf3":"markdown","4cd08564":"markdown","d15c9a68":"markdown","7139b973":"markdown","77fb9234":"markdown","4619329d":"markdown","1996f67b":"markdown","0436cde2":"markdown","fa04ca3a":"markdown","b23293e1":"markdown","ab4d5229":"markdown","34ed10b5":"markdown","5d792732":"markdown","56d9b93c":"markdown","c7fbcda9":"markdown","6f9afd35":"markdown","11ea996e":"markdown","f5b8f79c":"markdown","e79913e6":"markdown","853f304a":"markdown","d12f1019":"markdown"},"source":{"b41af7f6":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.metrics import f1_score, precision_score, recall_score, make_scorer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.utils import shuffle\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, BaggingClassifier\nfrom sklearn.svm import SVC\n\nimport keras.optimizers\nfrom keras.models import Sequential \nfrom keras.layers import Dense, GaussianNoise\nfrom keras.utils import np_utils\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n%matplotlib inline\nsns.set()\n\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.max_columns', 25)\n\nnp.set_printoptions(suppress=True)","a4a1fcb5":"train = pd.read_csv('..\/input\/treino.csv')\ntest = pd.read_csv('..\/input\/teste.csv')\ninfo = pd.read_csv('..\/input\/info_uso.csv')\nmachines = pd.read_csv('..\/input\/maquinas.csv')\nerrors = pd.read_csv('..\/input\/erros.csv')","a8d8e8c3":"info = info.merge(machines, on='maquina')\ninfo = info.merge(pd.get_dummies(errors, prefix=[''], prefix_sep='', columns=['erro']).groupby(['data', 'maquina'], as_index=False).sum(), how='left', on=['data', 'maquina'])\ninfo.fillna(0, inplace=True)\ninfo.data = pd.to_datetime(info.data, infer_datetime_format=True)","5cd13d3c":"def query_range(maq, a, b):\n    aux = info.query(\"maquina==@maq and @a<=data<@b\")\n    med = aux[['voltagem', 'rotacao', 'pressao', 'vibracao']].median()\n    tot = aux[['error1', 'error2', 'error3', 'error4', 'error5']].sum()\n    return pd.Series([\n        med['voltagem'],\n        med['rotacao'],\n        med['pressao'],\n        med['vibracao'],\n        tot['error1'],\n        tot['error2'],\n        tot['error3'],\n        tot['error4'],\n        tot['error5']\n    ])\n\ndef process(row):\n    maq = row['maquina']\n    a = row['data'] - pd.DateOffset(days=6)\n    b = row['data'] + pd.DateOffset(days=1)\n    return pd.concat([\n        query_range(maq, row['data'] - pd.DateOffset(days=6), row['data'] + pd.DateOffset(days=1)),\n        query_range(maq, row['data'] - pd.DateOffset(days=1), row['data'] + pd.DateOffset(days=1))\n    ], ignore_index=True)\n\ndef prepare(df):\n    df = df.merge(machines, on='maquina')\n    df.data = pd.to_datetime(df.data, infer_datetime_format=True)\n    df[['voltagem_w', 'rotacao_w', 'pressao_w', 'vibracao_w', 'error1_w', 'error2_w', 'error3_w', 'error4_w', 'error5_w',\n       'voltagem_d', 'rotacao_d', 'pressao_d', 'vibracao_d', 'error1_d', 'error2_d', 'error3_d', 'error4_d', 'error5_d']] = df.apply(process, axis=1)\n    return df\n\ntrain = prepare(train)\ntest = prepare(test)","dbf77172":"info.head(10)","480258ed":"train.head(10)","6f307afd":"fg = sns.FacetGrid(train, hue='falha', col='modelo', col_wrap=2, height=6.25, aspect=1, hue_order=['ok', 'comp1', 'comp2', 'comp3', 'comp4'], col_order=['model1', 'model2', 'model3', 'model4'])\nfg.map(sns.distplot, 'idade')\nfg.add_legend()","7bd394b3":"fg = sns.FacetGrid(train, hue='falha', col='modelo', col_wrap=2, height=6.25, aspect=1, hue_order=['ok', 'comp1', 'comp2', 'comp3', 'comp4'], col_order=['model1', 'model2', 'model3', 'model4'])\nfg.map(sns.distplot, 'voltagem_d')\nfg.add_legend()","8688477f":"fg = sns.FacetGrid(train, hue='falha', col='modelo', col_wrap=2, height=6.25, aspect=1, hue_order=['ok', 'comp1', 'comp2', 'comp3', 'comp4'], col_order=['model1', 'model2', 'model3', 'model4'])\nfg.map(sns.distplot, 'rotacao_d')\nfg.add_legend()","72ab3aea":"fg = sns.FacetGrid(train, hue='falha', col='modelo', col_wrap=2, height=6.25, aspect=1, hue_order=['ok', 'comp1', 'comp2', 'comp3', 'comp4'], col_order=['model1', 'model2', 'model3', 'model4'])\nfg.map(sns.distplot, 'pressao_d')\nfg.add_legend()","1e5ca43a":"fg = sns.FacetGrid(train, hue='falha', col='modelo', col_wrap=2, height=6.25, aspect=1, hue_order=['ok', 'comp1', 'comp2', 'comp3', 'comp4'], col_order=['model1', 'model2', 'model3', 'model4'])\nfg.map(sns.distplot, 'vibracao_d')\nfg.add_legend()","0fb98ed9":"plt.figure(figsize=(15,8))\nsns.barplot(data=train.loc[:, ['falha', 'error1_d', 'error2_d', 'error3_d', 'error4_d', 'error5_d']].melt(id_vars='falha'), x='falha', y='value', hue='variable', order=['ok', 'comp1', 'comp2', 'comp3', 'comp4'])","4b4d215a":"fg = sns.FacetGrid(train, hue='falha', col='modelo', col_wrap=2, height=6.25, aspect=1, hue_order=['ok', 'comp1', 'comp2', 'comp3', 'comp4'], col_order=['model1', 'model2', 'model3', 'model4'])\nfg.map(sns.distplot, 'voltagem_w')\nfg.add_legend()","4666515e":"fg = sns.FacetGrid(train, hue='falha', col='modelo', col_wrap=2, height=6.25, aspect=1, hue_order=['ok', 'comp1', 'comp2', 'comp3', 'comp4'], col_order=['model1', 'model2', 'model3', 'model4'])\nfg.map(sns.distplot, 'rotacao_w')\nfg.add_legend()","2eeed11a":"fg = sns.FacetGrid(train, hue='falha', col='modelo', col_wrap=2, height=6.25, aspect=1, hue_order=['ok', 'comp1', 'comp2', 'comp3', 'comp4'], col_order=['model1', 'model2', 'model3', 'model4'])\nfg.map(sns.distplot, 'pressao_w')\nfg.add_legend()","a239f3d2":"fg = sns.FacetGrid(train, hue='falha', col='modelo', col_wrap=2, height=6.25, aspect=1, hue_order=['ok', 'comp1', 'comp2', 'comp3', 'comp4'], col_order=['model1', 'model2', 'model3', 'model4'])\nfg.map(sns.distplot, 'vibracao_w')\nfg.add_legend()","5f1d8af1":"plt.figure(figsize=(15,8))\nsns.barplot(data=train.loc[:, ['falha', 'error1_w', 'error2_w', 'error3_w', 'error4_w', 'error5_w']].melt(id_vars='falha'), x='falha', y='value', hue='variable', order=['ok', 'comp1', 'comp2', 'comp3', 'comp4'])","acde008a":"def score_model(y_test, y_pred, ok=4):\n    miss = 0\n    total = 0\n    near_miss = 0\n    miss_failure = 0\n    total_failure = 0\n    aux = y_test.values\n    \n    for i in range(len(y_pred)):\n        if aux[i] != ok:\n            if y_pred[i] != aux[i]:\n                if y_pred[i] != ok:\n                    near_miss += 1\n                miss_failure += 1\n            total_failure += 1\n\n        if y_pred[i] != aux[i]:\n            miss += 1\n        total += 1\n\n    print('f1 score: {0:.5f}'.format(f1_score(y_test, y_pred, average='weighted')))\n    print('precision: {0:.5f}'.format(precision_score(y_test, y_pred, average='weighted')))\n    print('recall: {0:.5f}'.format(recall_score(y_test, y_pred, average='weighted')))\n    print('total missed', miss, 'of', total)\n    print('failure missed', miss_failure, 'of', total_failure, 'with', near_miss, 'near misses')","9e483bfc":"def hit_score(y_test, y_pred, ok=4):\n    miss = 0\n    total = 0\n    miss_failure = 0\n    total_failure = 0\n    aux = y_test.values\n    \n    for i in range(len(y_pred)):\n        # 4 = 'ok'\n        if aux[i] != ok:\n            if y_pred[i] != aux[i]:\n                miss_failure += 1\n            total_failure += 1\n\n        if y_pred[i] != aux[i]:\n            miss += 1\n        total += 1\n        \n    r1 = 1 - miss \/ total\n    r2 = 1 - miss_failure \/ total_failure\n    return 2 * r1 * r2 \/ (r1 + r2)","f163e8f3":"def prep_undersampling(df, ok=4):\n    df_ok = df[df.falha==ok]\n    df_fail = df[df.falha!=ok]\n    cnt = df_fail.shape[0]\n    return df_fail.append(df_ok.sample(cnt))","e4fdf069":"def prep_oversampling(df, df_y, ok=4):\n    df_ok = df[df_y==ok]\n    df_fail = df[df_y!=ok]\n    df_y_ok = df_y[df_y==ok]\n    df_y_fail = df_y[df_y!=ok]\n    cnt = df_ok.shape[0] \/\/ df_fail.shape[0]\n    return df_ok.append([df_fail] * cnt, ignore_index=True), df_y_ok.append([df_y_fail] * cnt, ignore_index=True)","b5ff2541":"f1 = make_scorer(f1_score, average='micro')\nhs = make_scorer(hit_score, greater_is_better=True)","ffc6e52a":"def prep_train(df, keepIndex=False):\n    if keepIndex == False:\n        df = df.drop(columns=['index'])\n    df = df.drop(columns=['data', 'maquina'])\n    \n    day = ['voltagem_d', 'rotacao_d', 'pressao_d', 'vibracao_d', 'error1_d', 'error2_d', 'error3_d', 'error4_d', 'error5_d']\n    week = ['voltagem_w', 'rotacao_w', 'pressao_w', 'vibracao_w', 'error1_w', 'error2_w', 'error3_w', 'error4_w', 'error5_w']\n    to_drop = ['modelo']\n    to_scale = ['idade'] + day + week\n    to_encode = []\n    \n    df = df.drop(columns=to_drop)\n        \n    for col in to_scale:\n        sc = StandardScaler()\n        #sc = MinMaxScaler()\n        sc.fit(df.loc[:, [col]])\n        df[col] = sc.transform(df.loc[:, [col]])\n        \n    return pd.get_dummies(df, columns=to_encode)","2dd0d050":"df = prep_train(train.copy(deep=True))\n\nencode = ['falha']\nfor col in encode:\n    le = LabelEncoder()\n    le.fit(df[col])\n    df[col] = le.transform(df[col])\n\ndf_y = df['falha']\ndf.drop(columns=['falha'], inplace=True)","af23e0df":"x_train, x_test, y_train, y_test = train_test_split(df, df_y, stratify=df_y, random_state=None, test_size=0.5)\n\nx_train, y_train = prep_oversampling(x_train, y_train, 4)\nx_train, y_train = shuffle(x_train, y_train)","ccc10707":"df.head()","e0ef76f7":"model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, subsample=1)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)\npd.DataFrame(data=(model.feature_importances_ * 100).round(5).reshape(1, len(df.columns)), columns=df.columns)","600ef026":"model = RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, class_weight='balanced')\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)\npd.DataFrame(data=(model.feature_importances_ * 100).round(5).reshape(1, len(df.columns)), columns=df.columns)","c5afd427":"model = RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=5, class_weight='balanced')\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)\npd.DataFrame(data=(model.feature_importances_ * 100).round(5).reshape(1, len(df.columns)), columns=df.columns)","bccf27ba":"model = ExtraTreesClassifier(n_estimators=75, criterion='gini', max_depth=8, class_weight='balanced', min_samples_leaf=0.01)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)\npd.DataFrame(data=(model.feature_importances_ * 100).round(5).reshape(1, len(df.columns)), columns=df.columns)","3569e177":"model = ExtraTreesClassifier(n_estimators=75, criterion='entropy', max_depth=8, class_weight='balanced', min_samples_leaf=0.01)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)\npd.DataFrame(data=(model.feature_importances_ * 100).round(5).reshape(1, len(df.columns)), columns=df.columns)","aa0c8936":"def create_model(features_count=1):\n    model = Sequential()\n    model.add(GaussianNoise(0.05, input_shape=(features_count,)))\n    model.add(Dense(8, activation='relu'))\n    model.add(Dense(16, activation='relu'))\n    model.add(Dense(24, activation='relu'))\n    model.add(Dense(5, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.RMSprop(lr=.001), metrics=['categorical_accuracy'])\n    return model","f666f725":"model = KerasClassifier(build_fn=create_model, features_count=len(df.columns), epochs=50, batch_size=250, verbose=0)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)","2d7968d4":"model = VotingClassifier(estimators=[\n    ('gb', GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, max_depth=5, subsample=0.5)),\n    ('rf1', RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, class_weight='balanced')),\n    ('rf2', RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=9, class_weight='balanced')),\n    ('et1', ExtraTreesClassifier(n_estimators=75, criterion='gini', max_depth=8, class_weight='balanced')),\n    ('et2', ExtraTreesClassifier(n_estimators=75, criterion='entropy', max_depth=9, class_weight='balanced'))\n], voting='soft', weights=[1.25, 1, 1, 1, 1])\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)","fed81c20":"model = VotingClassifier(estimators=[\n    ('gb', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, subsample=1)),\n    ('rf1', RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, class_weight='balanced')),\n    ('rf2', RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=9, class_weight='balanced')),\n    ('et1', ExtraTreesClassifier(n_estimators=75, criterion='gini', max_depth=8, class_weight='balanced')),\n    ('et2', ExtraTreesClassifier(n_estimators=75, criterion='entropy', max_depth=5, class_weight='balanced'))\n], voting='soft', weights=[1.25, 1.25, 0.75, 0.75, 1.25])\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)","c322e890":"model = VotingClassifier(estimators=[\n    ('gb', GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, max_depth=5, subsample=0.5)),\n    ('rf1', RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, class_weight='balanced')),\n    ('rf2', RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=9, class_weight='balanced')),\n    ('et1', ExtraTreesClassifier(n_estimators=75, criterion='gini', max_depth=8, class_weight='balanced')),\n    ('et2', ExtraTreesClassifier(n_estimators=75, criterion='entropy', max_depth=9, class_weight='balanced')),\n    ('nn', KerasClassifier(build_fn=create_model, features_count=len(df.columns), epochs=30, batch_size=250, verbose=0))\n], voting='soft', weights=[1.25, 1, 1, 1, 1, 2])\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)","4fbfba53":"model = VotingClassifier(estimators=[\n    ('gb', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, subsample=1)),\n    ('rf1', RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, class_weight='balanced')),\n    ('rf2', RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=9, class_weight='balanced')),\n    ('et1', ExtraTreesClassifier(n_estimators=75, criterion='gini', max_depth=8, class_weight='balanced')),\n    ('et2', ExtraTreesClassifier(n_estimators=75, criterion='entropy', max_depth=5, class_weight='balanced')),\n    ('nn', KerasClassifier(build_fn=create_model, features_count=len(df.columns), epochs=30, batch_size=250, verbose=0))\n], voting='soft', weights=[1.25, 1.25, 0.75, 0.75, 1.25, 1])\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)","025acdaa":"model = VotingClassifier(estimators=[\n    ('gb', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, subsample=1)),\n    ('rf1', RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, class_weight='balanced')),\n    ('rf2', RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=9, class_weight='balanced')),\n    ('et1', ExtraTreesClassifier(n_estimators=75, criterion='gini', max_depth=8, class_weight='balanced')),\n    ('et2', ExtraTreesClassifier(n_estimators=75, criterion='entropy', max_depth=5, class_weight='balanced')),\n    ('nn', KerasClassifier(build_fn=create_model, features_count=len(df.columns), epochs=30, batch_size=250, verbose=0))\n], voting='soft', weights=[1.5, 2, 1, 1, 2, 2])\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)","c8bce816":"class PredictProbaTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, clf=None):\n        self.clf = clf\n\n    def fit(self, x, y):\n        if self.clf is not None:\n            self.clf.fit(x, y)\n        return self\n\n    def transform(self, x):\n        if self.clf is not None:\n            return self.clf.predict_proba(x)\n        return x\n\n    def fit_transform(self, x, y):\n        return self.fit(x, y).transform(x)","2d752f40":"model = Pipeline([\n    ('stack', FeatureUnion([\n        ('0', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, max_depth=5, subsample=0.5))),\n        ('1', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, subsample=1.))),\n        ('2', PredictProbaTransformer(RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, class_weight='balanced'))),\n        ('3', PredictProbaTransformer(RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=9, class_weight='balanced'))),\n        ('4', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='gini', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n        ('5', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='entropy', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n        ('6', PredictProbaTransformer(KerasClassifier(build_fn=create_model, features_count=len(df.columns), epochs=30, batch_size=250, verbose=0))),\n    ])),\n    ('final', SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=2, max_iter=1500))\n])\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)","b4897105":"model = Pipeline([\n    ('stack', FeatureUnion([\n        ('0', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, max_depth=5, subsample=0.5))),\n        #('1', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, subsample=1.))),\n        ('2', PredictProbaTransformer(RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, class_weight='balanced'))),\n        ('3', PredictProbaTransformer(RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=8, class_weight='balanced'))),\n        ('4', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='gini', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n        ('5', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='entropy', max_depth=5, class_weight='balanced', min_samples_leaf=0.01))),\n        ('6', PredictProbaTransformer(KerasClassifier(build_fn=create_model, features_count=len(df.columns), epochs=50, batch_size=250, verbose=0))),\n    ])),\n    ('final', SVC(class_weight='balanced', kernel='sigmoid', gamma='auto', tol=1e-4, decision_function_shape='ovr', max_iter=1500))\n])\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)","870d2fb8":"model = Pipeline([\n    ('stack1', FeatureUnion([\n        ('0', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, max_depth=5, subsample=0.5))),\n        ('1', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, subsample=1.))),\n        ('2', PredictProbaTransformer(RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, class_weight='balanced'))),\n        ('3', PredictProbaTransformer(RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=9, class_weight='balanced'))),\n        ('4', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='gini', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n        ('5', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='entropy', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n        ('6', PredictProbaTransformer(KerasClassifier(build_fn=create_model, features_count=len(df.columns), epochs=30, batch_size=250, verbose=0))),\n    ])),\n    ('stack2', FeatureUnion([\n        ('0', PredictProbaTransformer(SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=2, max_iter=1500, probability=True))),\n        ('1', PredictProbaTransformer(SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=2, max_iter=1500, probability=True))),\n    ])),\n    ('final', SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=1, max_iter=1500))\n])\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)","2f793fba":"model = Pipeline([\n    ('stack1', FeatureUnion([\n        ('0', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, max_depth=5, subsample=0.5))),\n        ('1', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, subsample=1.))),\n        ('2', PredictProbaTransformer(RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, class_weight='balanced'))),\n        ('3', PredictProbaTransformer(RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=9, class_weight='balanced'))),\n        ('4', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='gini', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n        ('5', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='entropy', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n        ('6', PredictProbaTransformer(KerasClassifier(build_fn=create_model, features_count=len(df.columns), epochs=30, batch_size=250, verbose=0))),\n    ])),\n    ('stack2', FeatureUnion([\n        ('0', PredictProbaTransformer(SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=2, max_iter=1500, probability=True))),\n        ('1', PredictProbaTransformer(SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=2, max_iter=1500, probability=True))),\n    ])),\n    ('final', BaggingClassifier(base_estimator=SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=1, max_iter=1500), n_estimators=5, max_samples=0.2))\n])\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)","6c3d1951":"model = Pipeline([\n    ('stack1', FeatureUnion([\n        ('0', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, max_depth=5, subsample=0.5))),\n        ('1', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, subsample=1.))),\n        ('2', PredictProbaTransformer(RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, class_weight='balanced'))),\n        ('3', PredictProbaTransformer(RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=9, class_weight='balanced'))),\n        ('4', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='gini', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n        ('5', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='entropy', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n    ])),\n    ('stack2', FeatureUnion([\n        ('0', PredictProbaTransformer(SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=2, max_iter=1500, probability=True))),\n        ('1', PredictProbaTransformer(SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=2, max_iter=1500, probability=True))),\n        ('2', PredictProbaTransformer(KerasClassifier(build_fn=create_model, features_count=30, epochs=5, batch_size=250, verbose=0))),\n    ])),\n    ('final', BaggingClassifier(base_estimator=SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=1, max_iter=1500), n_estimators=3, max_samples=0.34))\n])\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)","a559b64e":"model = Pipeline([\n    ('stack1', FeatureUnion([\n        ('0', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, max_depth=5, subsample=0.5))),\n        ('1', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, subsample=1.))),\n        ('2', PredictProbaTransformer(RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, class_weight='balanced'))),\n        ('3', PredictProbaTransformer(RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=9, class_weight='balanced'))),\n        ('4', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='gini', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n        ('5', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='entropy', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n        ('6', PredictProbaTransformer(KerasClassifier(build_fn=create_model, features_count=len(df.columns), epochs=30, batch_size=250, verbose=0))),\n    ])),\n    ('stack2', FeatureUnion([\n        ('0', PredictProbaTransformer(SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=2, max_iter=1500, probability=True))),\n        ('1', PredictProbaTransformer(SVC(class_weight='balanced', kernel='sigmoid', gamma='auto', max_iter=1500, probability=True))),\n    ])),\n    ('final', SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=1, max_iter=1500))\n])\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)","5fd17634":"model = Pipeline([\n    ('stack1', FeatureUnion([\n        ('0', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, max_depth=5, subsample=0.5))),\n        ('1', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, subsample=1.))),\n        ('2', PredictProbaTransformer(RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, class_weight='balanced'))),\n        ('3', PredictProbaTransformer(RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=9, class_weight='balanced'))),\n        ('4', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='gini', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n        ('5', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='entropy', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n        ('6', PredictProbaTransformer(KerasClassifier(build_fn=create_model, features_count=len(df.columns), epochs=30, batch_size=250, verbose=0))),\n    ])),\n    ('stack2', FeatureUnion([\n        ('0', PredictProbaTransformer(SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=2, max_iter=1500, probability=True))),\n        ('1', PredictProbaTransformer(SVC(class_weight='balanced', kernel='sigmoid', gamma='auto', max_iter=1500, probability=True))),\n    ])),\n    ('final', BaggingClassifier(base_estimator=SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=1, max_iter=1500), n_estimators=5, max_samples=0.2))\n])\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)","5e1872c6":"model = Pipeline([\n    ('stack1', FeatureUnion([\n        ('0', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, max_depth=5, subsample=0.5))),\n        ('1', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, subsample=1.))),\n        ('2', PredictProbaTransformer(RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, class_weight='balanced'))),\n        ('3', PredictProbaTransformer(RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=9, class_weight='balanced'))),\n        ('4', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='gini', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n        ('5', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='entropy', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n    ])),\n    ('stack2', FeatureUnion([\n        ('0', PredictProbaTransformer(SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=2, max_iter=1500, probability=True))),\n        ('1', PredictProbaTransformer(SVC(class_weight='balanced', kernel='sigmoid', gamma='auto', max_iter=1500, probability=True))),\n        ('2', PredictProbaTransformer(KerasClassifier(build_fn=create_model, features_count=30, epochs=5, batch_size=250, verbose=0))),\n    ])),\n    ('final', BaggingClassifier(base_estimator=SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=1, max_iter=1500), n_estimators=3, max_samples=0.34))\n])\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)","c4914b74":"model = Pipeline([\n    ('stack1', FeatureUnion([\n        ('0', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, max_depth=5, subsample=0.5))),\n        ('1', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, subsample=1.))),\n        ('2', PredictProbaTransformer(RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, class_weight='balanced'))),\n        ('3', PredictProbaTransformer(RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=9, class_weight='balanced'))),\n        ('4', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='gini', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n        ('5', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='entropy', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n    ])),\n    ('stack2', FeatureUnion([\n        ('0', PredictProbaTransformer(SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=2, max_iter=1500, probability=True))),\n        ('1', PredictProbaTransformer(SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=2, max_iter=1500, probability=True))),\n        ('2', PredictProbaTransformer(KerasClassifier(build_fn=create_model, features_count=30, epochs=5, batch_size=250, verbose=0))),\n    ])),\n    ('final', SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=1, max_iter=1500))\n])\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)","d51fd3c5":"model = Pipeline([\n    ('stack1', FeatureUnion([\n        ('0', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, max_depth=5, subsample=0.5))),\n        ('1', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, subsample=1.))),\n        ('2', PredictProbaTransformer(RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, class_weight='balanced'))),\n        ('3', PredictProbaTransformer(RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=9, class_weight='balanced'))),\n        ('4', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='gini', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n        ('5', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='entropy', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n    ])),\n    ('stack2', FeatureUnion([\n        ('0', PredictProbaTransformer(SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=2, max_iter=1500, probability=True))),\n        ('1', PredictProbaTransformer(SVC(class_weight='balanced', kernel='sigmoid', gamma='auto', max_iter=1500, probability=True))),\n        ('2', PredictProbaTransformer(KerasClassifier(build_fn=create_model, features_count=30, epochs=5, batch_size=250, verbose=0))),\n    ])),\n    ('final', SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=1, max_iter=1500))\n])\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)","2c3b84fa":"model = Pipeline([\n    ('stack1', FeatureUnion([\n        ('0', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, max_depth=5, subsample=0.5))),\n        ('1', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, subsample=1.))),\n        ('2', PredictProbaTransformer(RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, class_weight='balanced'))),\n        ('3', PredictProbaTransformer(RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=9, class_weight='balanced'))),\n        ('4', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='gini', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n        ('5', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='entropy', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n    ])),\n    ('stack2', FeatureUnion([\n        ('0', PredictProbaTransformer(SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=2, max_iter=1500, probability=True))),\n        ('1', PredictProbaTransformer(SVC(class_weight='balanced', kernel='sigmoid', gamma='auto', max_iter=1500, probability=True))),\n        ('2', PredictProbaTransformer(KerasClassifier(build_fn=create_model, features_count=30, epochs=5, batch_size=250, verbose=0))),\n    ])),\n    ('final', SVC(class_weight='balanced', kernel='sigmoid', gamma=2, degree=1, max_iter=1500))\n])\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore_model(y_test, y_pred)","e7b4b715":"test_final = prep_train(test.copy(deep=True), True)\ntrain_final = prep_train(train.copy(deep=True))","be6edefc":"le = LabelEncoder()\nle.fit(train['falha'])\ntrain_final['falha'] = le.transform(train_final['falha'])","cf2124de":"x = train_final.drop(columns=['falha'])\ny = train_final['falha']\nx, y = prep_oversampling(x, y, 4)\nx, y = shuffle(x, y)\nx_t = test_final.drop(columns=['index'])","c38f86b3":"model = Pipeline([\n    ('stack1', FeatureUnion([\n        ('0', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, max_depth=5, subsample=0.5))),\n        ('1', PredictProbaTransformer(GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, subsample=1.))),\n        ('2', PredictProbaTransformer(RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, class_weight='balanced'))),\n        ('3', PredictProbaTransformer(RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=9, class_weight='balanced'))),\n        ('4', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='gini', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n        ('5', PredictProbaTransformer(ExtraTreesClassifier(n_estimators=75, criterion='entropy', max_depth=8, class_weight='balanced', min_samples_leaf=0.01))),\n    ])),\n    ('stack2', FeatureUnion([\n        ('0', PredictProbaTransformer(SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=2, max_iter=1500, probability=True))),\n        ('1', PredictProbaTransformer(SVC(class_weight='balanced', kernel='sigmoid', gamma='auto', max_iter=1500, probability=True))),\n        ('2', PredictProbaTransformer(KerasClassifier(build_fn=create_model, features_count=30, epochs=5, batch_size=250, verbose=0))),\n    ])),\n    ('final', SVC(class_weight='balanced', kernel='poly', gamma='auto', degree=1, max_iter=1500))\n])\nmodel.fit(x, y)\npred = model.predict(x_t)","c8e6057d":"sub = pd.DataFrame({'index': test_final['index'], 'falha': le.inverse_transform(pred)})\nsub.falha = sub.falha.str.replace('comp', 'peca')\nsub.to_csv(\"submission.csv\", index=False)","4a0dc355":"### Model 13 (a.k.a. 10 fixed)","efda0a9f":"### Model 1","be0d9eee":"### Model 12 (a.k.a. 9 fixed)","8ab3c24d":"### Model 15 (a.k.a. 14 fixed)","1f10a470":"### Model 6","4f67133a":"## 1 week before","69e6fddc":"# Model","1d7fa58c":"# Submit","2d4b7925":"## Trees","acdebaac":"# Setup","b6a77df5":"### Model 9","74b38cf3":"### Model 16","4cd08564":"## Voting","d15c9a68":"## Feature creation","7139b973":"### Model 4","77fb9234":"### Model 8","4619329d":"### Model 10","1996f67b":"### Model 3","0436cde2":"### ExtraTreesClassifier","fa04ca3a":"## Prepare train and test","b23293e1":"### Model 7","ab4d5229":"## 2 days before","34ed10b5":"### Model 5","5d792732":"### Model 2","56d9b93c":"## Dataframes","c7fbcda9":"## Neural Network","6f9afd35":"# Analysis","11ea996e":"### RandomForestClassifier","f5b8f79c":"### Model 11 (a.k.a. 8 fixed)","e79913e6":"### Model 14","853f304a":"## Stacking","d12f1019":"### GradientBoostingClassifier"}}