{"cell_type":{"19636f03":"code","5e084532":"code","5be45ee9":"code","7463215e":"code","ab07b388":"code","36d3b568":"code","df67fdf6":"code","eec017e5":"code","341f1b86":"code","6fc6022b":"code","bc3089e5":"code","42679f8d":"code","a883052f":"code","fbfc04f5":"code","e869a9ca":"code","5647dde4":"code","5f6c8762":"code","b03a4274":"code","49f61d3b":"code","d6e39d6c":"code","eb291868":"code","1be53436":"code","7290c842":"code","bdfbb369":"code","6e7d61cf":"code","3cfa17b3":"code","f8509648":"code","c3285ede":"code","7207fd05":"code","607d8eb3":"code","3974fe2f":"code","c86fed64":"code","d1ac322c":"code","a6bcb214":"code","68241e3b":"code","e20be3ea":"code","137d0c65":"code","712dad5a":"code","75e47314":"markdown","3e66615d":"markdown","352cf3d1":"markdown","0670fb4e":"markdown","06407c03":"markdown","0331a198":"markdown","06923a57":"markdown","75dffecb":"markdown","7b4c5dd0":"markdown","911a20b9":"markdown","01984a4d":"markdown"},"source":{"19636f03":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5e084532":"#Importing Libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n","5be45ee9":"#Importing three csv files \ndf= pd.read_csv(\"\/kaggle\/input\/machine-learning-24-hrs-hackathon\/train_SJC.csv\")\ndf_test=pd.read_csv(\"\/kaggle\/input\/machine-learning-24-hrs-hackathon\/Test_SJC.csv\")\nsample=pd.read_csv(\"\/kaggle\/input\/machine-learning-24-hrs-hackathon\/sample_submission.csv\")\ndf.head()","7463215e":"#Renaming column names that were unnamed. And dropping the first row of data that contained the name of features. \ndf= df.rename( columns={'Unnamed: 0':'ClaimNumber','Unnamed: 1': 'DateTimeOfAccident','Unnamed: 3':'Age','Unnamed: 4': 'Gender',\n                    'Unnamed: 5': 'MaritalStatus','Unnamed: 6':'DependentChildren','Unnamed: 8' : 'WeeklyWages',\n                    'Unnamed: 9' : 'PartTimeFullTime','Unnamed: 10' : 'HoursWorkedPerWeek', 'Unnamed: 12' : 'ClaimDescription',\n                    'Unnamed: 13': 'InitialIncurredClaimsCost','Unnamed: 14':'UltimateIncurredClaimCost'} )\ndf=df.iloc[1:]\n\ndf.head()","ab07b388":"#Checking for missing values\ndf.isnull().sum()","36d3b568":"#For Testing data as well \ndf_test['MaritalStatus'].fillna('U', inplace = True)\n","df67fdf6":"df_test.isnull().sum()","eec017e5":"#MaritalStatus columns has 22 missing values which can be replaced with U, which means \"Unknown\"\n\ndf['MaritalStatus'].fillna(value=\"U\",inplace=True)\ndf.isnull().sum()\n","341f1b86":"#Converting data type of columns \ndf=df.astype({\"WeeklyWages\": float})\ndf=df.astype({\"HoursWorkedPerWeek\": float})\ndf=df.astype({\"DaysWorkedPerWeek\": int})\ndf=df.astype({\"InitialIncurredClaimsCost\": int})\ndf=df.astype({\"UltimateIncurredClaimCost\": float})","6fc6022b":"#Checking Skewness of Data\nsns.distplot(df.WeeklyWages)","bc3089e5":"#When the data is skewed it is always good to replace with median or mode instead of mean \ndf['WeeklyWages'].fillna((df['WeeklyWages'].median()), inplace= True )\ndf.isnull().sum()","42679f8d":"#The distribution is not skewed \nsns.distplot(df['HoursWorkedPerWeek'])","a883052f":"#HoursWorkedPerWeek has 49 missing values which is replaced with the mean\n\ndf['HoursWorkedPerWeek'].fillna((df['HoursWorkedPerWeek'].mean()), inplace= True )\ndf.isnull().sum()","fbfc04f5":"df.describe()","e869a9ca":"df.info()","5647dde4":"#FIT and transform function is using from the labelencoder package\nimport sklearn.preprocessing as pre\nimport sklearn.model_selection as ms\nle=pre.LabelEncoder()\nfor x in df.select_dtypes(include='object').columns.tolist():\n    df[x]=le.fit_transform(df[x])","5f6c8762":"#Creating a subplot of Marital Status to check the number of Married, single and unknown \nplt.suptitle('Countplot of Marital Status: M - Married; S - Single; U - Unknown')\nplt.subplot(1, 2, 1)\ndf['MaritalStatus'].value_counts(dropna = False).plot(kind = 'bar', rot = 0);\n\nplt.subplot(1, 2, 2)\ndf['MaritalStatus'].value_counts(dropna = False).plot(kind = 'bar', rot = 0);","b03a4274":"#correlation matrix\ncor = round(df.corr(),2)\ncor","49f61d3b":"#Correlation matrix to check for multicollinearity \ndf.corr()","d6e39d6c":"#Checking the distribution of age in train and test data sets\nplt.suptitle('Distribution of Age')\n\nplt.subplot(1, 2, 1)\nsns.distplot(df['Age'], color = '#810f7c')\nplt.title('Training Set')\n\nplt.subplot(1, 2, 2)\nsns.distplot(df_test['Age'], color = '#8c96c6')\nplt.title('Testing Set');","eb291868":"#Checking the mix max and median values for the feature \"DependentChildren\"\ndf['DependentChildren'].max(), df['DependentChildren'].min(), df['DependentChildren'].median()","1be53436":"#Plotting into a bar chart of the feature DependentChildren\nplt.subplot(1, 2 , 1)\nsns.countplot(df['DependentChildren'])\nplt.subplot(1, 2 , 2)\nsns.countplot(df['DependentsOther']);","7290c842":"#To visualise the InitialIncurredClaimsCost in distplot \nplt.figure(figsize = (16, 10))\nplt.subplot(1, 2, 1)\nsns.distplot(df['InitialIncurredClaimsCost'])\nplt.subplot(1, 2, 2)\nplt.title('Log Scale')\nsns.distplot(np.log1p(df['InitialIncurredClaimsCost']));","bdfbb369":"# The log1p function applies log(1+x) to all elements of the column\ndf[\"LogUltimateIncurredClaimCost\"] = np.log1p(df[\"UltimateIncurredClaimCost\"])\ndf[\"LogInitialIncurredClaimsCost\"] = np.log1p(df[\"InitialIncurredClaimsCost\"])\n\n# plot distribution: claim costs (log)\nplt.subplots(figsize=(10, 6))\nsns.distplot(df.LogUltimateIncurredClaimCost, kde=False, label='Amount claimed originally',bins=100)\nsns.distplot(df.LogInitialIncurredClaimsCost, kde=False, label='Initial claim predicted amount', bins=100)\nplt.xlabel('claim costs (log)')\nplt.legend()\nplt.show()","6e7d61cf":"# List of features with to many different values\nnum_list_bins =['HoursWorkedPerWeek','WeeklyWages','HoursWorkedPerWeek','ClaimNumber']\n\n# plot binned plot boxplots for 'LogUltimateIncurredClaimCost'\nfor name in num_list_bins:\n    f, ax = plt.subplots(figsize=(14, 5))\n    df['bin_'] = pd.cut(df[name], 8)\n    sns.boxplot(x='bin_', y='LogUltimateIncurredClaimCost', data=df)\n    plt.xlabel(name)\n    plt.show()\n\ndf.drop(['bin_'],axis=1,inplace=True)\n","3cfa17b3":"#Now we have to treat the extreme outliers \n# Print the most expensive claim amounts\nexp = ['UltimateIncurredClaimCost']\nprint(df[exp].sort_values(by='UltimateIncurredClaimCost', ascending=False).head(8))","f8509648":"#Remove the four million sample and give max value of 1Mil\ndf['UltimateIncurredClaimCost'] = np.where(df['UltimateIncurredClaimCost'] > 1000000, 1000000., df['UltimateIncurredClaimCost']) * 1.000\ndf['UltimateIncurredClaimCost'].mean() ","c3285ede":"#By this we can successfully say that datetime features have too many outliers or errors so we can remove this two features\n#We don't need the datetime features\ndf.drop(['DateTimeOfAccident', 'DateReported'], axis = 1, inplace = True)\ndf_test.drop(['DateTimeOfAccident', 'DateReported'], axis = 1, inplace = True)","7207fd05":"#Standardization is done on the data \ndf['InitialIncurredClaimsCost'] = np.log1p(df['InitialIncurredClaimsCost'])\ndf_test['InitialIncurredCalimsCost'] = np.log1p(df_test['InitialIncurredCalimsCost'])\n\noutcome = np.log1p(df['UltimateIncurredClaimCost'])\n\ndf.drop(['ClaimNumber', 'ClaimDescription', 'UltimateIncurredClaimCost'], axis = 1, inplace = True)\ndf_test.drop(['ClaimNumber', 'ClaimDescription'], axis = 1, inplace = True)\n\ndf.shape , df_test.shape","607d8eb3":"df_test.columns\ndf.drop('LogUltimateIncurredClaimCost',axis=1, inplace=True)\ndf.drop('LogInitialIncurredClaimsCost',axis=1,inplace= True)","3974fe2f":"df.shape , df_test.shape","c86fed64":"df_test.columns","d1ac322c":"#Now we split the train and test data\nfrom sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(df, outcome, test_size = 0.2, random_state = 123456)\nprint(Xtrain.shape, ytrain.shape, Xtest.shape, ytest.shape)","a6bcb214":"from sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\n\nxgb = XGBRegressor( learning_rate = 0.01, \n                    n_estimators = 10000,\n                    max_depth = 3, \n                    min_child_weight = 0,\n                    gamma = 0, \n                    subsample = 0.7,\n                    colsample_bytree = 0.7,\n                    objective = 'reg:squarederror', \n                    nthread = 1,\n                    scale_pos_weight = 1, \n                    seed = 27,\n                    reg_alpha = 0.00006\n                    )\n","68241e3b":"#After initialising the model we fit the model with our train and test split\nxgb_model = xgb.fit(Xtrain, ytrain)\nxpreds = xgb_model.predict(Xtest)\nprint((f\"XGBOOST RMSE: {np.sqrt(mean_squared_error(ytest, xpreds))}\"))","e20be3ea":"print(mean_squared_error(ytest, xpreds))","137d0c65":"import sklearn.preprocessing as pre\nimport sklearn.model_selection as ms\nle=pre.LabelEncoder()\nfor x in df_test.select_dtypes(include='object').columns.tolist():\n    df_test[x]=le.fit_transform(df_test[x])","712dad5a":"xg_preds = xgb_model.predict(df_test)\nsample['UltimateIncurredClaimCost'] = np.expm1(xg_preds)\nsample.to_csv('Sample Submission1.csv' , index = False)\nsample.head()","75e47314":"#  **Exploratory Data Analysis**","3e66615d":" **Three Columns have missing values namely, MaritalStatus with 22 missing values, WeeklyWages with 56 missing values nad HoursWorkedPerWeek with 49 missing values. To Treat these missing values we either replace the NaN value with a new value or imputate the values using either mode,median or mean imputation techniques.**","352cf3d1":"**Here we see that Gender is negatively correlated to our target variable , MaritalStatus has close to zero correlation and Claim discription is again negatively correlated to our target variable**","0670fb4e":" **This dataset contains 36000 rows 15 columns. The main aim is to predict the outcome variable \"UltimateIncurredClaimCost\"**","06407c03":"# **Data Cleaning**","0331a198":"# Sample Submission Data","06923a57":"**Methods Followed:**\n* First step was to import the packages needed for the data cleaning and preprocessing as well as EDA.The three datasets were then imported and passed on to variables -> Df , Df_Test and sample\n* The second step was to rename the columns and see the top 5 rows of the dataset\n* After this was done, the dataset was checked for missing values and the values were handled using imputation methods like mode and median. \n* After imputation of values the Fit_transform function was used in order to change categorical data into numeric. Then exploratory data analysis was performed and the outliers were treated for the target variable.\n* The target variable was taken as UltimateIncurredClaimCost \n* This is a regression task therefore Xg boost regressor model was used and evaulated based on the RMSE scores. The RMSE score obtained was 0.7\n* Standardisation of data was done using the np.log1p function. \n* DateTimeOfAccident and Datereported features were removed as it had unexplainable values.\n* The value of the target variable had an outlier which was corrected and removed. \n* The train \/test split of the data was done. The variables ClaimNumber, ClaimDescription and UltimateIncurredClaimCost was removed from the features and the outcome had only UltimateIncurredClaimCost as it is the target variable. This was the passed into an XGBoostRegressor model with the objective to find the RootMeanSquareError(RMSE). The MSE is 0.5 and the RMSE was 0.7. \n","75dffecb":"**RMSE score is calucated and MSE score is calculated**","7b4c5dd0":"**Label Encoder is used to fit and transform the train data**","911a20b9":"# Train\/Test Split of Data is done","01984a4d":"# Model Building "}}