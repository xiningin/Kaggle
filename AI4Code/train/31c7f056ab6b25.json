{"cell_type":{"2a3e19cf":"code","52a5f92e":"code","cab13d11":"code","f10c57ea":"code","46863aa4":"code","22fbad23":"code","5515bf56":"code","efc2471f":"code","8bfbec92":"code","e55b85b2":"code","764ca6df":"code","5fbbb838":"code","559a0d5a":"code","2a7fb8aa":"code","0b920203":"code","7d8cf29c":"code","0c33047c":"code","97af75a7":"code","33a039e7":"code","2031912c":"code","359043b8":"code","76e48592":"code","bc5bf0f9":"code","3f6ead82":"code","cea367bb":"markdown","efe424a5":"markdown","4a481d28":"markdown","4e960b83":"markdown","c9cc8b9d":"markdown","055dd8a3":"markdown","db8e7ac9":"markdown","032c8657":"markdown","fd70b58a":"markdown","3be19187":"markdown","2b817e22":"markdown","797f0902":"markdown","26f44564":"markdown","102aa79a":"markdown","e8b4af0a":"markdown","87ba9d65":"markdown","1cdda4bc":"markdown","ae1a8acc":"markdown","8501bfc6":"markdown","ce501b98":"markdown","0cf938a0":"markdown","809b5b8f":"markdown"},"source":{"2a3e19cf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nfrom scipy.stats import skew, pearsonr\n\nfrom sklearn.impute import SimpleImputer","52a5f92e":"sns.set(font_scale=1.2)\n\nprint(\"Loading data from train.csv.\")\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain.info()\nm = train.shape[0]","cab13d11":"vars_leaky = ['MoSold', 'YrSold', 'SaleType', 'SaleCondition']","f10c57ea":"sns.distplot(train['SalePrice'])\nprint(\"Statistics of the target variable: \")\nprint(\"--------------------------------\")\nprint(train['SalePrice'].describe())\nprint(\"Skewness: %f\" % (skew(train['SalePrice'])))","46863aa4":"from sklearn.ensemble import IsolationForest\n\ndef study_outliers_with_isolation_forest():\n    #rng = np.random.RandomState(42)\n    corrs = train.corrwith(train['SalePrice'], method='spearman').sort_values(ascending=False)\n    clf = IsolationForest(max_samples=200, random_state=42, contamination=0.01)\n    Xtrain = train[corrs.index]\n\n    imp = SimpleImputer(strategy=\"median\")\n    Xtrain = imp.fit_transform(Xtrain)\n\n    # clrs = [\"blue\" if y == 1 else \"red\" for y in pred]\n    markers = {1: \".\", -1:\"X\", -2:\"X\"}\n    # palette = sns.color_palette()\n    # plt.scatter(train['GrLivArea'], train['SalePrice'], s=6, color=clrs, marker=\".\")\n\n    clf.fit(Xtrain)\n    pred = clf.predict(Xtrain)\n    y = clf.decision_function(Xtrain)\n\n    ncols = 4\n    nrows = (int)(corrs.index.size \/ ncols) + 1\n\n    sns.set(style='ticks', font_scale=0.6)\n\n    fig, axs = plt.subplots(nrows, ncols, num=\"outliers\", figsize=(4*ncols, 3*nrows), sharey=True)\n    axs = axs.flatten()\n    for i, var in enumerate(corrs.index[1:]):\n        ax = axs[i]\n        sns.scatterplot(x=train[var], y=train['SalePrice'], \\\n                        hue=pred, style=pred, palette=\"tab10\", \\\n                        markers=markers, hue_order=[1,-1,-2], ax=ax, legend=False)\n    #     sns.scatterplot(x=train[var], y=train['SalePrice'], \\\n    #                     hue=y, style=pred, \\\n    #                     markers=markers, \\\n    #                     palette=\"rocket\", ax=ax, legend=False)\n        if(i % nrows): ax.set_ylabel(\"\")\n        xlabel = ax.get_xlabel()\n        ax.text(0.95, 0.85, xlabel, transform=ax.transAxes, ha='right', fontsize=10)\n        ax.set_xlabel(\"\")\n        ticks_loc = ax.get_yticks().tolist()\n        ax.yaxis.set_major_locator(mpl.ticker.FixedLocator(ticks_loc))\n        ylabels = ['{:,.0f}'.format(y\/1000) + 'k' for y in ticks_loc]\n        ax.set_yticklabels(ylabels)\n        \nstudy_outliers_with_isolation_forest()\n","22fbad23":"# One way to look at missing features\nprint(\"10 Most sparse features:\")\nprint(\"--------------------------------\")\nprint((1.0 - train.count() \/ m).sort_values(ascending=False)[:10])\nprint()\n\n# I found this a very illustrative way of checking missing values\n# Ref: (House Prices: EDA, Pipelines, GridSearchCV)\nfig, axs = plt.subplots(2, 1, num=\"missing_train\", figsize=(10, 10))\nsns.heatmap(train.isna(), yticklabels=False, cbar=False, ax=axs[0])\naxs[0].set_xticks([])\n# Note that the figure above hides the labels of some features. One small update from myself is to only display features that have some (say, >1%) missing train:\nfeatures_with_missing_train = train.loc[:, train.count() < 0.99 * m]\nsns.heatmap(features_with_missing_train.isna(), yticklabels=False, cbar=False, ax=axs[1])\nfig.subplots_adjust(bottom=0.2, hspace=0.1)\n    \n# Select features with > 80% NaN\nvars_sparse = train.loc[:, train.count() < 0.2 * m]\n# Print out the values, are they important?\nprint(\"Feature with > 80% data missing: \")\nfor (col, val) in vars_sparse.iteritems():\n    print(\"------------\")\n    print(val.value_counts())\n","5515bf56":"def study_variable_correlations(data, limit_score=0.2):\n    '''\n    Compute the correlation coeffcients between features and the target y and plot the correlation heatmap.\n    \n    Parameters:\n    ---\n    data: The pandas DataFrame that contains the data.\n    limit_score: Only show features that strongly correlate with the target with a correlation coefficient larger than this value.\n    '''\n\n    # There are several metrics for measuring correlations. \n    # I will only use the Spearman correlation coefficients but keep the rest in a dataframe.\n    corr_p = data.corrwith(data['SalePrice'])\n    corr_k = data.corrwith(data['SalePrice'], method=\"kendall\")\n    corr_s = data.corrwith(data['SalePrice'], method=\"spearman\")\n    # Create pandas.dataFrame from pandas.Series\n    corrs = {'pearson':corr_p, 'kendall':corr_k, 'spearman':corr_s}\n    corrs = pd.DataFrame(corrs)\n\n    corrs = corrs[abs(corrs['spearman']) > limit_score]\n    # corrs = corrs.T.sort_values(by=0)\n    corrs.sort_values(by='spearman', inplace=True)\n    # Now move on to the correlation matrix\n    corrmat = data[corrs.index].corr(method='spearman')\n    # Trick 1: re-order the dataing data to follow the order of correlation\n    # Trick 2: Use a diverging color map rather than the default\n    #cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\n    cmap = plt.get_cmap(\"coolwarm\")\n    fig, ax = plt.subplots(1,1,figsize=(12,12), num=\"heatmap\")\n    sns.heatmap(corrmat, vmin=-0.8, vmax=0.8, square=True, cmap=cmap, ax=ax)\n    fig.subplots_adjust(left=0.2, bottom=0.2)\n\ncorrs = study_variable_correlations(train, 0.1)","efc2471f":"import json\nwith open('..\/input\/ordinal-feature-mapping\/ordinal.json',) as f:\n    ordinal_feature_encoder = json.load(f)\nprint(\"Ordinal features to be encoded: \")\nprint(\"----------------\")\nprint(ordinal_feature_encoder.keys())\nprint()\n\n# Encode\ntrain_derived = train.replace(ordinal_feature_encoder)\n\n# Check one ordinal feature after encoding.\n# Randomly pick 10 samples.\nnp.random.seed(26)\nfor i in np.random.randint(0, m, 10):\n    print(\"%s : %f\" % (train['ExterQual'][i], train_derived['ExterQual'][i]))","8bfbec92":"corrs = study_variable_correlations(train_derived, 0.4)","e55b85b2":"def pairplot_multi_variables(data, cols):\n    '''\n    Show paired scatter plot for selected features from the dataset.\n    \n    Parameters:\n    ---\n    data: The pd.DataFrame that contains the data.\n    cols: A list of features (columns) from the dataset to plot.\n    '''\n    \n    sns.set(style=\"ticks\", font_scale=0.6)\n    p = sns.pairplot(data[cols], height=1.5, corner=True, plot_kws={\"s\":5})\n    p.fig.set_size_inches(10., 10.)\n    # can use kind=\"hist\"\n    sns.despine()\n    # Set corner to True to reduce resource consumption\n    # Re-set tick labels for SalePrice\n    for ax in p.axes.reshape(-1):\n        if(ax == None): continue          \n        if(ax.get_xlabel() == 'SalePrice'):\n            ticks_loc = ax.get_xticks().tolist()\n            ax.xaxis.set_major_locator(mpl.ticker.FixedLocator(ticks_loc))\n            xlabels = ['{:,.0f}'.format(x\/1000) + 'k' for x in ticks_loc]\n            ax.set_xticklabels(xlabels)\n        if(ax.get_ylabel() == 'SalePrice'):\n            ticks_loc = ax.get_yticks().tolist()\n            ax.yaxis.set_major_locator(mpl.ticker.FixedLocator(ticks_loc))\n            ylabels = ['{:,.0f}'.format(y\/1000) + 'k' for y in ticks_loc]\n            ax.set_yticklabels(ylabels)\n    return p\n\np = pairplot_multi_variables(train, ['GarageYrBlt', 'YearBuilt', 'SalePrice'])","764ca6df":"train_derived['MSSubClass'] = train_derived['MSSubClass'].astype('object')","5fbbb838":"vars_cat = train_derived.select_dtypes('object')\n# Drop features that are sparse\nfor var in vars_sparse.columns: \n    if(var in vars_cat):\n        vars_cat = vars_cat.drop(var, axis=1)\n# Find the number of unique values in any feature (cardinality)\nratio_unique = vars_cat.nunique() \/ vars_cat.count()\ncount_unique = vars_cat.nunique()\n# sns.ecdfplot(ratio_unique)\n# The 10 features with the most unique values\n# One may want to avoid OneHot for very high cardinality columns\nprint(\"Features with largest number of unique values: \")\nprint(\"----------------\")\n# pandas guide says nlargest() is faster than sort_values().head(), so keep a good habit\nprint(count_unique.nlargest(10))","559a0d5a":"def boxplot_multi_variables(cols, height=2.5, max_features=16, fignum=\"boxplot_multi\", rank=False):\n    '''\n    Show the dependence of the target variable on multiple categorical variables.\n    \n    Parameters:\n    ---\n    cols: List of columns (features) to show. The total number of features must be smaller than max_features\n    max_features: The maximum number of features to show (default: 4x4).\n    height: Scale factor for the image size.\n    rank: If True, rank order the labels on the x axis by the mean value.\n    '''\n    \n    num_features = len(cols)\n    if (num_features > max_features):\n        raise ValueError(\"Too many features (%d, limit=%d) to show.\" % (num_features, max_features))\n    # Optimize the number of rows\/cols for plotting\n    nrows = int(np.sqrt(num_features))\n    if(nrows * (nrows + 1) < num_features): nrows += 1\n    ncols = int(np.ceil(num_features \/ nrows))\n    print(\"Plotting %d x %d grid for %d features ...\" % (nrows, ncols, num_features))\n    fig, axs = plt.subplots(nrows, ncols, num=fignum, figsize=(ncols*height, nrows*height*1.2), sharey=True)\n    for i, ax in enumerate(axs.flatten()):\n        if(i < num_features):\n            if(rank):\n                xy = train[[cols[i],'SalePrice']]\n                sorted_index = xy.groupby(cols[i]).mean().sort_values(by='SalePrice',ascending=False).index\n                sns.boxplot(data=train, x=cols[i], y=\"SalePrice\", ax=ax, order=sorted_index)\n            else:\n                sns.boxplot(data=train, x=cols[i], y=\"SalePrice\", ax=ax)\n        if(i % nrows): ax.set_ylabel(\"\")\n        else:\n            ticks_loc = ax.get_yticks().tolist()\n            ax.yaxis.set_major_locator(mpl.ticker.FixedLocator(ticks_loc))\n            ylabels = ['{:,.0f}'.format(y\/1000) + 'k' for y in ticks_loc]\n            ax.set_yticklabels(ylabels)\n        xlabel = ax.get_xlabel()\n        ax.text(0.95, 0.9, xlabel, transform=ax.transAxes, ha='right',fontsize=height*3)\n        ax.set_xlabel(\"\")\n    fig.subplots_adjust(hspace=0.2, wspace=0, top=0.9)\n    return fig, axs\n","2a7fb8aa":"# Check how many nominal variables we have.\nprint(vars_cat.columns)\nprint(\"Number of nominal features: %d\" % (vars_cat.columns.size))","0b920203":"#_, _ = boxplot_multi_variables(vars_cat.columns[:16], height=3.5, rank=True)\n_, _ = boxplot_multi_variables(count_unique.sort_values(ascending=False).index[:9], height=4.5, rank=True)","7d8cf29c":"_, _ = boxplot_multi_variables(count_unique.sort_values(ascending=False).index[9:], height=3.5, rank=True)","0c33047c":"from category_encoders.target_encoder import TargetEncoder\nfrom category_encoders.leave_one_out import LeaveOneOutEncoder","97af75a7":"te_features = ['Neighborhood', 'Exterior1st', 'Exterior2nd', 'MSSubClass']\n\nte = TargetEncoder(cols=te_features)\nlooe = LeaveOneOutEncoder(cols=te_features)\n\nx = te.fit_transform(train_derived, train_derived['SalePrice'])\nx2 = looe.fit_transform(train_derived, train_derived['SalePrice'])\n\nneighborhood = pd.DataFrame({'Neighborhood':train_derived['Neighborhood'], \\\n                             'TargetEncoder':x['Neighborhood'], \\\n                             'LeaveOneOutEncoder':x2['Neighborhood'], \\\n                             'SalePrice':train_derived['SalePrice']})\n\n# Mapping between neighborhood and the encoded value\nprint(neighborhood[['Neighborhood','TargetEncoder']].drop_duplicates().sort_values(by='TargetEncoder'))\n\n# Compare the re-labeled 'Neighborhood' feature using scatter plot\npairplot_multi_variables(neighborhood, neighborhood.columns)","33a039e7":"from sklearn.feature_selection import SelectKBest, chi2, f_regression\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nreg_rf = RandomForestRegressor()\n","2031912c":"# Re-define train_derived\ntrain_derived = train.copy()\n\n# Drop features that will not be used in modeling\n# Combines features that would be dropped:\ndrop_features = [\"Id\",\"Utilities\"]\ndrop_features += vars_sparse.columns.tolist()\ndrop_features += vars_leaky\n# drop_features += vars_collinear\nfor var in drop_features:\n    if(var in train_derived):\n        print(\"Drop feature: \", var)\n        train_derived = train_derived.drop(var, axis=1)\n\n# Hard code the ordinal features using a .json file\nwith open('..\/input\/ordinal-feature-mapping\/ordinal.json',) as f:\n    ordinal_feature_encoder = json.load(f)\ntrain_derived = train_derived.replace(ordinal_feature_encoder)\n\n# MSSubClass is a nominal (categorical) feature but is encoded with numerics\ntrain_derived['MSSubClass'] = train_derived['MSSubClass'].astype('object')\n\n# Prepare data for training\ny = train_derived['SalePrice']\nX = train_derived.drop('SalePrice', axis=1)\n\nnum_features = X.select_dtypes(['int', 'float']).columns\ncat_features = X.select_dtypes('object').columns\nord_features = pd.Index(ordinal_feature_encoder.keys()).join(X.columns, how='inner') # Note that some features have been dropped in X.","359043b8":"# Construct pipelines\n\n# Replace missing values with mean, and perform standard scaler to numerical features.\n# Note that the ordinal features are now numerical.\nppl_num = Pipeline(steps = \\\n    [('imp_num', SimpleImputer()), \\\n     ('scaler', StandardScaler())] \\\n)\n\n# Apply OneHotEncoder to the remaining categorical features.\nppl_cat = Pipeline(steps = \\\n    [('imp_cat', SimpleImputer(strategy='constant', fill_value='missing')), \\\n     ('ohe', OneHotEncoder(handle_unknown='ignore'))] \\\n)\n# Must set handle_unknown, otherwise when applying to test data error may occur:\n# \"found unknown categories in column ... during transform\"\n\nct = ColumnTransformer(transformers= \\\n    [('numeric', ppl_num, num_features), \\\n     ('categorical', ppl_cat, cat_features)] \\\n)\n","76e48592":"# Train\/Test split and perform the fit\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.3, random_state=120)\n\nppl_rf = Pipeline(steps = [('ct', ct), ('rf', reg_rf)])\nppl_rf.fit(X_train, y_train)\ny_pred = ppl_rf.predict(X_test)\n\n# First, check score to see if the fitted model is reasonable\nprint(\"RMSE score (RandomForest): %f\" % np.sqrt(mean_squared_error(y_test, y_pred)))\nprint(\"r2 score (RandomForest): %f\" % (ppl_rf.score(X_test, y_test)))","bc5bf0f9":"# SLOW CELL\nprint(\"Analyzing feature importance using permutation method: \")\n\nperm_result_train = permutation_importance(ppl_rf, X_train, y_train, n_repeats=10, random_state=24)\n\nperm_result_test = permutation_importance(ppl_rf, X_test, y_test, n_repeats=10, random_state=24)\nperm_sorted_idx = perm_result_test.importances_mean.argsort()[::-1]","3f6ead82":"def show_important_features(perm_result_train, perm_result_test, X):\n    '''\n    Display the most important features calculated from the permutation method.\n\n    Parameters:\n    ---\n    perm_result_train: The output from sklearn.inspection.permutation_importance on the training set\n    perm_result_test: On the test set\n    X: The training\/test data before applying any pipeline. It is used to extract the features.\n    '''\n    types = ['N\/A'] * X.columns.size\n    feature_importance = pd.DataFrame({'mean_train':perm_result_train.importances_mean, \\\n                                       'std_train':perm_result_train.importances_std, \\\n                                       'mean_test':perm_result_test.importances_mean, \\\n                                       'std_test':perm_result_test.importances_std, \\\n                                       'type':types}, \\\n                                      index=X.columns)\n    feature_importance.loc[num_features, 'type'] = 'numerical'\n    feature_importance.loc[cat_features, 'type'] = 'nominal'\n    feature_importance.loc[ord_features, 'type'] = 'ordinal'\n    feature_importance.sort_values(by='mean_train', ascending=False, inplace=True)\n\n    print(\"The top 10 Numerical features:\")\n    print(feature_importance[feature_importance['type']=='numerical'][:10])\n    print()\n    print(\"The top 5 Nominal features (nominal):\")\n    print(feature_importance[feature_importance['type']=='nominal'][:5])\n    print()\n    print(\"The top 5 Nominal features (ordinal):\")\n    print(feature_importance[feature_importance['type']=='ordinal'][:5])\n\n    sns.set(font_scale = 1.2)\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8), num='feature_importance')\n    sns.barplot(x = feature_importance['mean_train'][:20], \\\n                y = feature_importance.index[:20], \\\n                hue=feature_importance['type'][:20], \\\n                ax=ax1, orient='h', dodge=False)\n    \n    feature_importance.sort_values(by='mean_test', ascending=False, inplace=True)\n    \n    sns.barplot(x = feature_importance['mean_test'][:20], \\\n                y = feature_importance.index[:20], \\\n                hue=feature_importance['type'][:20], \\\n                ax=ax2, orient='h', dodge=False)\n    ax1.set_xscale(\"log\")\n    ax2.set_xscale(\"log\")\n\n    fig.subplots_adjust(left=0.2, wspace=0.30)\n    plt.show()\n\nshow_important_features(perm_result_train, perm_result_test, X)","cea367bb":"None of these seem to show strong correlations with the target. In particular, note that the 'Utilities' only have value for one of its categories. We might drop it from the dataset.","efe424a5":"## Feature Importance\n\nNot too bad! Now we are ready to evaluate feature importance from the random forest model.\n\nBut, how to extract feature importance from the fitted model? In simpler applications, the model has attributes such as coef_ and feature_importance_ that directly estimates the importance of any given feature. However there are two concerns: \n1. The feature_importance_ may not apply to the test set, because it is often estimated based solely on the training data which the model may overfit. \n2. It is hard to map the feature_importance_ metric to the original features if the data has gone through some transformations such as the OneHotEncoder for categorical features, which expands the feature space.\n\npermuation_importance() provides a very convenient way of evaluating the feature importance on a hold-out test set. However, the results may be degraded if many features are correlated.","4a481d28":"### Encoding High Cardinality Features\n\nNow let's revisit the problem of encoding high cardinality features such as the 'Neighborhood'. Here I tried a TargetEncoder and a LeaveOneOutEncoder, both re-labels the nominal categories with numerics based on how strong indicators these categories are.\n\nQuote from the reference:\n\u201cfeatures are replaced with a blend of posterior probability of the target given particular categorical value and the prior probability of the target over all the training data.\u201d\n\nReference: https:\/\/medium.com\/analytics-vidhya\/target-encoding-vs-one-hot-encoding-with-simple-examples-276a7e7b3e64\n\nOne caveat, however, is that this kind of encoding should be applied to each train\/cv split separately. Otherwise the trained model will under-estimate the cross validation error since the encoder itself already 'peeks' into the cv set. In other word, one should not apply the TargetEncoder to the whole dataset first and then perform train\/cv split.","4e960b83":"The above figure shows the 9 nominal features with highest cardinality. The features on the x-axis have been re-ordered by the mean of each category. From this figure it is easy to see:\n1. Whether or not there is some intrinsic correlations between any feature and the target.\n2. Where are the outliers and do they show any characteristics?\n\nClearly, several features, e.g., Neighborhood, Exterior1st, Exterior2nd, MSSubClass, do show strong correlations with the target value.\n\nNow showing the rest of features:","c9cc8b9d":"1. The output is apparently skewed (with a skewness value > 1). A common practice is to transform it to y = log(1+y). We will apply this transformation to skewed features later.\n\n## Outliers\n\nThis dataset has a few notable outliers. As noted in the second part of this notebook, https:\/\/www.kaggle.com\/shuiyaohuang\/house-price-from-50-to-top-4, different criteria for removing outliers lead to large fluctuations in the leaderboard score.\n\nHere I used a isolation forest to pick out outliers based on the numerical features of the training data. The results are shown in the following grid, with red crosses indicating outliers. Each panel shows the dependence between SalePrice and a numerical feature. ","055dd8a3":"## Correlations between numerical features\n\nNext, let's look at the numerical features and how they correlate with the target y. Heatmap is a useful tool to visualize correlations between numerical features. Here are a few considerations when I made it:\n1. The correlation matric to use. The default metric, the \"pearson\" correlation, has a few limitations, such as it measures the linear correlation, assumes normal distribution of variables and is sensitive to outliers. I prefer using rank based metrics such as the \"spearman\" correlation.\n2. Re-order the features on the axes according to their correlations with the target y. Furthermore, I dropped features that are only weakly correlated with the target instead of showing all ~30 features here. \n3. Use a diverging color map to help picking out strong correlations.","db8e7ac9":"Now we plot the heat map again, now including encoded ordinal features and raise the correlation coefficient limit to 0.4.","032c8657":"## Missing Data\n\nNext, let's take a look at features with lots of missing data. I will refer these features as 'sparse' features. \nOften people simply drops those features that are mostly NULL. Here, I will quickly examine them and discard them if they are not strong indicators of y. 'Sparse' features might contain some critical information. For example, what if some houses used to be hosted by very famous people or have some bad rumors around (though this sample that we are working on doesn't have these kinds of information)? A single feature like these will likely dominate its sale price even though most houses may not have these features.\n\nThe following code examines:\n1. The 10 most sparse features and what fraction of their values are missing.\n2. Features with over 80% entries missing, what are they, are they of any interest? \n3. Visualize the missing values\n\nI pick out 4 features to be dropped from the dataset (PoolQC, MiscFeature, Alley, Fence)","fd70b58a":"Here it is! We have a metric that measures the permutation importance of all the features on the train\/test dataset. In this notebook I have explored various topics of explorative data analysis using a house price dataset with more than 70 features and have made a plan of selecting and transforming features for further modeling with more sophisticated algorithms. Here is a summary of main conclusions and the pre-processing steps:\n1. The target distribution is skewed. Perform log(1+x) transformation before modeling.\n2. Drop several features from both the training and test set\n  - 'Id': non-informative\n  - 'Utilities': single-valued\n  - 'PoolQC', 'Alley', 'Fence', 'MiscFeature': Very sparse with more than 80% data missing.\n  - 'MoSold', 'YrSold', 'SaleType', 'SaleCondition': Likely leaky, maybe missing in the test set\n3. One feature, 'MSSubClass', is falsely interpreted as numerical. Treat it as categorical.\n4. Hard encode all ordinal features using input from a separate .json file.\n5. Correlation heatmap shows strong signs of co-linearity, e.g., among the quality metrics. \n6. Several nominal features have more than 10 categories, among these features, the 'Neighborhood' shows signs of correlation with the target. Encode it using TargetEncoder.\n7. Transform numerical features with a mean imputer and a standard scaler\n8. Transform nominal features except 'Neighborhood' with OneHot Encoder ignoring missing values.\n9. Train a random forest model and use it to evaluate feature importance on the test data using permutation.","3be19187":"Always check distribution of the dependent variable:","2b817e22":"## Summary","797f0902":"The following script rank orders and displays the most important features calculated from the permutation_importance()","26f44564":"Load the training set. Have a quick look at the data using DataFrame.info(). The training data has 1460 samples and 81 features with a mixture of numerical and categorical features. A few features have NULL values that we will deal with later. ","102aa79a":"As expected, some neighborhoods are more expensive than others!\n\n## A Simple Random Forest Model\n\nFinally, let's evaluate feature importance with a simple random forest model fit to the data.","e8b4af0a":"# House Price I: A Comprehensive Explorative Data Analysis\n\n### *Charlie Huang*\n\n***\n\nThe kaggle competition, *House Price - Advanced Regression Techniques*, has been a very popular playground for exploring various ways of analyzing, modeling and visualizing dataset. Given a sample of houses with a wide range of features (~80) and their sale prices, the main task is to train a model to predict the sale price of new houses. I started this project with a purpose of exercising my newly acquired knowledge of data engineering and machine learning and exploring deep functionalities of python packages such as pandas, scikit-learn and seaborn.\n\nThis is the first module of a comprehensive exploration of the dataset, focusing on understanding the characteristics of the independent variables and their relations to the dependent variable. There are several reasons for performing a explorative data analysis (EDA):\n- Build initial insights from the data and make better plans for more sophisticated modeling. \n- Detect anomalies within the data such as missing values, outliers and leaky features.\n- Select and transform features to help improve the performance of machine learning algorithms.\n- Help interpret and present the results from later analysis.\n\nHere is a list of topics to cover within this module:\n- Skewness\n- Correlations between variables\n- Data cleaning\n- Outliers, Missing values and leaky features\n- Encoding the categorical features: ordinal encoder, one-hot encoder, target encoder, leave-one-out encoder\n- Dependence of the target variable on numerical, nominal and ordinal features\n- Extract feature importance with a simple random forest model\n- Some pandas and seaborn tricks of handling and visualizing data\n\nNote that in this notebook, the \"features\" are often refered as \"variables\" and \"columns\".","87ba9d65":"## Encoding Ordinal Categorical Features\n\nNote however the heatmap does not contain categorical features. How do we know if a categorical feature is important to predicting y? Here I will divide categorical features into two classes:\n1. Ordinal features. In these feature the categories are naturally ordered. For example, the feature \"ExterQual\" has 5 categories, \"Ex\", \"Gd\", \"TA\", \"fa\", \"po\", there are clearly ranked by goodness. One can encode ordinal features as numerics that increases with the rank order. Then using a rank based correlation metric one can include them in the heatmap as well.\n2. Nominal features. In these features the categories are labeled without any particular order. For example, \"Neighborhood\" contains a list of names that do not seem to be related. Even though they might be ranked by things like the average income, the population density, etc., these information are simply not known from the data. Later we will see that \"Neighborhood\" is a relatively strong indicator of sale price, but it is difficult to visualize in a heatmap.\n\nThe following code hard codes the ordinal features from the data set. I keep the mappings between categories and their corresponding numerics in a separate .json file. The code loads the mappings into a python dictionary structure that can be directly applied to the pd.DataFrame. Here shows an example of a .json file of this kind:\n\n{\n    \"ExterQual\":\n    {\n\t\"Ex\": 0,\n\t\"Gd\": 1,\n\t\"TA\": 2,\n\t\"Fa\": 3,\n\t\"Po\": 4\t\n    },\n    \"ExterCond\":\n    {\n\t\"Ex\": 0,\n\t\"Gd\": 1,\n\t\"TA\": 2,\n\t\"Fa\": 3,\n\t\"Po\": 4\t\n    }\n}","1cdda4bc":"Among all the features from training set, some may not be applicable to the test set. In this dataset, YrSold, MoSold, SaleType, SaleCondition could be leaky features if we want to predict the sale price for houses that have not been sold yet. The trained model incorporates these information but the test data (houses not yet sold) do not have them.\nIn general, data leakage manifests in various ways and often leads to over-estimation of the expected model performance. \n\nReference: https:\/\/insidebigdata.com\/2014\/11\/26\/ask-data-scientist-data-leakage\/","ae1a8acc":"Now let's move on to the nominal categorical features. \n\nFirst of all, note that there is a 'numerical' feature, 'MSSubClass', that is actually a nominal feature. So we need to label it accordingly","8501bfc6":"The feature with highest cardinality here is the \"neighborhood\", which has 25 distinct categories. This is manageable with the popular OneHot encoder, though I will experiment with target encoder and leave one out encoder later on some of the nominal features.\n\n### Target response to nominal features\n\nNext let's visualize the dependence of the target variable on the nominal features using boxplots. Below is a script that display many of these features on a self-adjusting grid.","ce501b98":"## Multi-colinearity\n\nFrom the heatmap it is easy to pick out several features that are strongly rank correlated with each other:\n- The quality metrics (OverallQual, ExterQual, KitchenQual, BsmtQual, HeatingQC)\n- TotRmsAbvGrd and GrLivArea\n- BsmtFinSF1 and BsmtFinType1\n- GarageArea and GarageCars\n- YearBuilt and GarageYrBlt\n\nWhat should we do with the correlated features? In general, how does one deal with multi-collinearity? I have found people simply keep only one of the correlated features, or engineer new features that are combinations of the existing ones. One the one hand, multi-collinearity is known to cause large fluctuations in the values of the coefficients. On the other hand, it has also been suggested that removing multi-collinearity does not promise a significant improvement in prediction accuracy. Therefore, I did not spend much time engineering these features, at least before fitting the very first models.\n\nNevertheless, I use scatter plots to help examine the correlated features to see if I can find something interesting. Here I show one example plotting YearBuilt, GarageYrBlt and their relations with the sale price. For most houses the garage was built in the same year as the house, but there are exceptions where the garage was built later. Strangely, in rare cases, a garage was built way before the house itself (bugged data?).\n\nHere is a nice explanation on multi-collinearity in regression analysis.\nRef: https:\/\/statisticsbyjim.com\/regression\/multicollinearity-in-regression-analysis\/\n","0cf938a0":"The numerical features explain most of the trend in the dependent variable y in both the training and the test set. The rank order is consistent with the correlation heatmap shown earlier. Though many of the features here are correlated, we can identify some of the strongest indicators for the sale price are:\n- Quality (most notably the OverallQual)\n- Area (above ground, basement, garage, etc.)\n- Age (YearBuilt)\n\nThe 'Neighborhood' turns out as the most important nominal feature.","809b5b8f":"## Norminal Features\n\n### Cardinality\n \nOne thing to check is the cardinality of a categorical feature. Cardinality measures the number of unique categories for a feature. High cardinality variables, e.g., zip code, may pose serious challenges to training algorithms if they are encoded with dummy variables as it may drastically expands the feature space and thus introducing the curse of dimensionality. Alternatively, one might consider encode high cardinality variables with continuous numerics based on how they correlate with the target variable.\n\nReference: https:\/\/www.kdnuggets.com\/2016\/08\/include-high-cardinality-attributes-predictive-model.html\n\nThe following code displays the 10 nominal features with highest cardinality. "}}