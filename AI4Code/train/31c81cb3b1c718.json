{"cell_type":{"c00daed2":"code","5941422e":"code","0e1fdb22":"code","8907840e":"code","cb714d03":"code","8a0fe84d":"code","4baa8b98":"code","97a98c69":"code","4f40911c":"code","722d6959":"code","4896e30c":"code","e17256d6":"code","96024cde":"code","08fff552":"code","3aed07d4":"code","45111514":"code","8ce25586":"code","f15ef1a7":"code","fa62911a":"code","f6a333dc":"code","1d8a196d":"code","5b35214d":"code","1495e054":"code","aaa788c8":"code","5dcc405b":"code","09707ecc":"code","249c6841":"code","52cd67b4":"code","27af4328":"code","933b7d7a":"code","c8b386ce":"code","0dd27a08":"code","21754be8":"code","7fdd0ea2":"code","63f1cc99":"code","da8465d2":"code","1d736a5d":"code","7d8350a6":"code","3fc6070d":"code","437a8876":"markdown","945caf5f":"markdown","48209aa7":"markdown","475aeba7":"markdown","9e23ad6b":"markdown","77a953d1":"markdown","9c90de96":"markdown","853233b9":"markdown","522600cd":"markdown","ead51868":"markdown","abb16447":"markdown","b0e37d98":"markdown","3bdcadc0":"markdown","282937ad":"markdown","61dc7dd2":"markdown","93864a61":"markdown","cb60dabf":"markdown","e4422173":"markdown","de5a7505":"markdown","9db04687":"markdown","b14a05fe":"markdown","0c07db2e":"markdown","ce99cb06":"markdown","123c5bee":"markdown","85b7059a":"markdown"},"source":{"c00daed2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5941422e":"import plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\n\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nfrom sklearn import metrics\nfrom scipy import stats\n\nfrom copy import deepcopy\n\nimport h2o","0e1fdb22":"train_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain_df.shape, test_df.shape","8907840e":"house_price_profile = ProfileReport(train_df)","cb714d03":"house_price_profile","8a0fe84d":"# I want to thanks @masumrumi for sharing this amazing plot!\ndef plotting_3_chart(df, feature):\n    ## Importing seaborn, matplotlab and scipy modules. \n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    import matplotlib.gridspec as gridspec\n    from scipy import stats\n    import matplotlib.style as style\n    style.use('fivethirtyeight')\n\n    ## Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout=True, figsize=(12,8))\n    ## creating a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    #gs = fig3.add_gridspec(3, 3)\n\n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram')\n    ## plot the histogram. \n    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(df.loc[:,feature], plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(df.loc[:,feature], orient='v', ax = ax3 );","4baa8b98":"train_corr = train_df.drop('Id', axis=1).corr()\n\nfig, ax = plt.subplots(figsize=(10, 8))\n# mask\nmask = np.triu(np.ones_like(train_corr, dtype=np.bool))\n# adjust mask and df\ncorr = train_corr.iloc[1:,:-1].copy()\n# plot heatmap\nsns.heatmap(train_corr, mask=mask, annot=False, fmt=\".2f\", cmap='Blues',\n           vmin=-1, vmax=1, cbar_kws={\"shrink\": .8})\n# yticks\nplt.yticks(rotation=0)\nplt.show()","97a98c69":"data = pd.concat([train_df['SalePrice'], train_df['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=train_df['OverallQual'], y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","4f40911c":"data = pd.concat([train_df['SalePrice'], train_df['TotalBsmtSF']], axis=1)\nfig = px.scatter(data, x='TotalBsmtSF', y='SalePrice')\nfig.show()","722d6959":"data = pd.concat([train_df['SalePrice'], train_df['GrLivArea']], axis=1)\nfig = px.scatter(data, x='GrLivArea', y='SalePrice')\nfig.show()","4896e30c":"# Remove the Ids from train and test, as they are unique for each row and hence not useful for the model\ntrain_id = train_df['Id']\ntest_id = test_df['Id']\n\ntrain_df = train_df.drop(['Id'], axis=1)\ntest_df = test_df.drop(['Id'], axis=1)\n\ntrain_df.shape, test_df.shape","e17256d6":"# Remove outliers\ntrain_df = train_df.drop(train_df[(train_df['GrLivArea']>4500) & (train_df['SalePrice']<300000)].index)\ntrain_df.shape","96024cde":"# Split features and labels\ny = train_df['SalePrice'].reset_index(drop=True)\ntrain_features = train_df.drop('SalePrice', axis=1)\ntest_features = test_df.copy()\n\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)\nfeatures.shape","08fff552":"# Some of the non-numeric predictors are stored as numbers (based on description); convert them into strings \nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)","3aed07d4":"plotting_3_chart(pd.DataFrame(y), 'SalePrice')","45111514":"# Fix the target variable\ny = np.log1p(y)\n\nplotting_3_chart(pd.DataFrame(y), 'SalePrice')","8ce25586":"y.head()","f15ef1a7":"#missing data\ntotal = features.isnull().sum().sort_values(ascending=False)\npercent = ((features.isnull().sum() \/ features.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data = missing_data.reset_index()\nmissing_data.columns = ['Name', 'Total', 'Percent']\nmissing_data[:10]","fa62911a":"def handle_missing(features):\n    # Deleting vars with high missings (over 80%)\n    features = features.drop(features.loc[:, (features.isna().sum() \/ features.count()) > 0.8].columns, axis=1)\n    \n    # the data description states that NA refers to typical ('Typ') values\n    features['Functional'] = features['Functional'].fillna('Typ')\n    \n    # Replace the missing values in each of the columns below with their mode\n    features['Electrical'] = features['Electrical'].fillna(features['Electrical'].mode()[0])\n    features['KitchenQual'] = features['KitchenQual'].fillna(features['KitchenQual'].mode()[0])\n    features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\n    features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n    features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n    \n    # Based on the type of dwelling, we can obtain the type of 'MSZoning'\n    features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n        \n    # Group the by neighborhoods, and fill in missing value by the median LotFrontage of the neighborhood\n    features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n    \n    # Finding all the categorical columns from the data\n    categorical_columns = features.select_dtypes(exclude=['int64','float64']).columns\n    numerical_columns = features.select_dtypes(include=['int64','float64']).columns\n\n    # We have no particular intuition around how to fill in the rest of the categorical features\n    # So we replace their missing values with None\n    features[categorical_columns] = features[categorical_columns].fillna('None')\n        \n    # And we do the same thing for numerical features, but this time with 0s\n    features[numerical_columns] = features[numerical_columns].fillna(0)\n    \n    return features","f6a333dc":"features = handle_missing(features)\nfeatures.shape","1d8a196d":"# check there are no nulls\nfeatures.isna().sum().sum()","5b35214d":"def add_new_vars(features):\n    # simplified features\n    features['HasPool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    features['Has2ndFloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    features['HasGarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    features['HasBsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    features['HasFireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n    # Adding total sqfootage features \n    features['TotalSF'] = features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n    features['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                                   features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n    features['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                                  features['EnclosedPorch'] + features['ScreenPorch'])\n\n    # Add years scince remodel\n    features['YearsSinceRemodel'] = features['YrSold'].astype(int) - features['YearRemodAdd'].astype(int)\n\n    return features","1495e054":"features = add_new_vars(features)\nfeatures.shape","aaa788c8":"numerical_columns = features.select_dtypes(include=['int64','float64']).columns\n\nskewed_features = features[numerical_columns].apply(lambda x: stats.skew(x)).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew value' :skewed_features})\nskewness.head(10)","5dcc405b":"def fix_skew(features):\n    \"\"\"\n    This function takes in a dataframe and return fixed skewed dataframe\n    \"\"\"\n    ## Import necessary modules \n    from scipy.special import boxcox1p\n    from scipy.stats import boxcox_normmax\n    \n    ## Getting all the data that are not of \"object\" type. \n    numerical_columns = features.select_dtypes(include=['int64','float64']).columns\n\n    # Check the skew of all numerical features\n    skewed_features = features[numerical_columns].apply(lambda x: stats.skew(x)).sort_values(ascending=False)\n    high_skew = skewed_features[abs(skewed_features) > 0.5]\n    skewed_features = high_skew.index\n\n    # Perform the Box-Cox transformation\n    for column in skewed_features:\n        features[column] = boxcox1p(features[column], boxcox_normmax(features[column] + 1))\n        \n    return features","09707ecc":"features = fix_skew(features)\nfeatures.head()","249c6841":"features = pd.get_dummies(features).reset_index(drop=True)\nfeatures.shape","52cd67b4":"x = features.iloc[:len(y), :]\nx_test = features.iloc[len(y):, :]\nx.shape, y.shape, x_test.shape","27af4328":"import h2o\n\nh2o.init()","933b7d7a":"hf = h2o.H2OFrame(pd.concat([x, y], axis=1))\nx_test_hf = h2o.H2OFrame(x_test)\nhf.head()","c8b386ce":"predictors = hf.drop('SalePrice').columns\nresponse = 'SalePrice'","0dd27a08":"# Split into train and test\ntrain, valid = hf.split_frame(ratios=[.8], seed=1234)","21754be8":"from h2o.automl import H2OAutoML\n\n# Add a Stopping Creterias: max number of models and max time\n# We are going to exclude DeepLearning algorithms because they are too slow\naml = H2OAutoML(\n    max_models=20,\n    max_runtime_secs=300,\n    seed=1234,\n    # exclude_algos = [\"DeepLearning\"]\n)","7fdd0ea2":"# Train the model\naml.train(x=predictors,\n        y=response,\n        training_frame=hf,\n        # validation_frame=valid\n)","63f1cc99":"# View the AutoML Leaderboard\nlb = aml.leaderboard\nlb.head(rows=10)  # Print the first 5 rows","da8465d2":"submission_results = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")","1d736a5d":"predict = aml.predict(x_test_hf)\npredict = predict.as_data_frame()\npredict.head()","7d8350a6":"submission_results.iloc[:, 1] = np.floor(np.expm1(predict['predict']))\n\n# Fix outleir predictions\nq_low = submission_results['SalePrice'].quantile(0.0045)\nq_high = submission_results['SalePrice'].quantile(0.99)\nsubmission_results['SalePrice'] = submission_results['SalePrice'].apply(lambda x: x if x > q_low else x*0.77)\nsubmission_results['SalePrice'] = submission_results['SalePrice'].apply(lambda x: x if x < q_high else x*1.1)\n\nsubmission_results.head()","3fc6070d":"submission_results.to_csv('submission_results', index=False)","437a8876":"<a id='5'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">5. Submission<\/p>","945caf5f":"<a id='4.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 15px 50px;\">4.1 H2O AutoML<\/p>","48209aa7":"<a id='2'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. Data cleaning<\/p>","475aeba7":"<a id='2.3'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 15px 50px;\">2.3 Creating new variables<\/p>","9e23ad6b":"First, lets check the skew of the numeric variables","77a953d1":"<a id='2.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 15px 50px;\">2.1 Fix target<\/p>","9c90de96":"<a id='2.4'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 15px 50px;\">2.4 Transform existing variables<\/p>","853233b9":"<a id='2.2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 15px 50px;\">2.2 Correct and Complete<\/p>","522600cd":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">Table of Content<\/p>\n\n* [1. Data Exploration \ud83d\udcca](#1)\n* [2. Data cleaning \ud83d\udd27](#2)\n    * [2.1 Fix target](#2.1)\n    * [2.2 Correct and complete](#2.2)\n    * [2.3 Creating new variables](#2.3)\n    * [2.4 Transform existing variables](#2.4)\n* [3. Feature Engineering \u2699\ufe0f](#3)\n    * [3.1 Feature encoding](#3.1)\n* [4. Modeling: H2O \ud83c\udfc2](#4)\n    * [4.1 H2O AutoML \ud83d\udcdd](#4.1)\n* [5. Submission \ud83d\udcdd](#5)","ead51868":"Lets see how the Target var is related with other vars","abb16447":"<a id='3'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. Feature Engineering<\/p>\n\nIn this section we are going to engineer the viariables to improve the performance.","b0e37d98":"In this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential outliers in age and fare. However, since they are reasonable values, we will wait until after we complete our exploratory analysis to determine if we should include or exclude from the dataset. It should be noted, that if they were unreasonable values, for example age = 800 instead of 80, then it's probably a safe decision to fix now. However, we want to use caution when we modify data from its original value, because it may be necessary to create an accurate model.\n2. **Completing:** There are null values or missing data in the age, cabin, and embarked field. Missing values can be bad, because some algorithms don't know how-to handle null values and will fail. While others, like decision trees, can handle null values. Thus, it's important to fix before we start modeling, because we will compare and contrast several models. There are two common methods, either delete the record or populate the missing value using a reasonable input. It is not recommended to delete the record, especially a large percentage of records, unless it truly represents an incomplete record. Instead, it's best to impute missing values. A basic methodology for qualitative data is impute using mode. A basic methodology for quantitative data is impute using mean, median, or mean + randomized standard deviation. An intermediate methodology is to use the basic methodology based on specific criteria; like the average age by class or embark port by fare and SES. There are more complex methodologies, however before deploying, it should be compared to the base model to determine if complexity truly adds value. For this dataset, age will be imputed with the median, the cabin attribute will be dropped, and embark will be imputed with mode. Subsequent model iterations may modify this decision to determine if it improves the model\u2019s accuracy.\n3. **Creating:**  Feature engineering is when we use existing features to create new features to determine if they provide new signals to predict our outcome. For this dataset, we will create a title feature to determine if it played a role in survival.\n4. **Converting:** Last, but certainly not least, we'll deal with formatting. There are no date or currency formats, but datatype formats. Our categorical data imported as objects, which makes it difficult for mathematical calculations. For this dataset, we will convert object datatypes to categorical dummy variables.","3bdcadc0":"<a id='4'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">4. Modeling<\/p>","282937ad":"![h2o-automl-logo2.jpg](attachment:h2o-automl-logo2.jpg)\n\nRef: https:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html\n\nH2O\u2019s AutoML can be used for automating the machine learning workflow, which includes automatic training and tuning of many models within a user-specified time-limit. Stacked Ensembles \u2013 one based on all previously trained models, another one on the best model of each family \u2013 will be automatically trained on collections of individual models to produce highly predictive ensemble models which, in most cases, will be the top performing models in the AutoML Leaderboard.\n\nH2O offers a number of model explainability methods that apply to AutoML objects (groups of models), as well as individual models (e.g. leader model). Explanations can be generated automatically with a single function call, providing a simple interface to exploring and explaining the AutoML models.\n\n### Required Stopping Parameters\n\nOne of the following stopping strategies (time or number-of-model based) must be specified. When both options are set, then the AutoML run will stop as soon as it hits one of either of these limits.\n\n* **max_runtime_secs**: This argument specifies the maximum time that the AutoML process will run for, prior to training the final Stacked Ensemble models. The default is 0 (no limit), but dynamically sets to 1 hour if none of `max_runtime_secs` and `max_models` are specified by the user.\n\n* **max_models**: Specify the maximum number of models to build in an AutoML run, excluding the Stacked Ensemble models. Defaults to `NULL\/None`.","61dc7dd2":"As we can see, we have there are **34** columns with some missing values, and **4** of them are over 80% missings!\n\nSo we are going to delete them!\n","93864a61":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\">\ud83c\udfe1 House Pricing - EDA and Prediction | H2O AutoML<\/p>","cb60dabf":"For categorical variables where no such ordinal relationship exists, the integer encoding is not enough.\n\nIn fact, using this encoding and allowing the model to assume a natural ordering between categories may result in poor performance or unexpected results (predictions halfway between categories).\n\nIn this case, a one-hot encoding can be applied to the integer representation. This is where the integer encoded variable is removed and a new binary variable is added for each unique integer value.\n\n![one-hot-encoding.png](attachment:one-hot-encoding.png)","e4422173":"![house-pricing.jpeg](attachment:house-pricing.jpeg)","de5a7505":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">1. Data Exploration<\/p>","9db04687":"Lets look at the target variable and distribution!","b14a05fe":"### Recreating train and test sets","0c07db2e":"![skewness.png](attachment:skewness.png)","ce99cb06":"Now data is cleaned and we can start the next step in feature engineering.","123c5bee":"<a id='3.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 15px 50px;\">3.1 Feature encoding<\/p>","85b7059a":"Few important things to know about **skew**:\n- Skewness is the degree of distortion from the symmetrical bell curve or the normal distribution.\n- Symmetrical distribution must have a skewness of 0.\n- Skewness can be positive or negative.\n- Skewness means that the mean and median are greater or lower than the mode."}}