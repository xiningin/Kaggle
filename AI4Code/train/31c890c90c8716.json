{"cell_type":{"efa88c08":"code","02038f74":"code","46b77fab":"code","565267e9":"code","32694ce0":"code","2bea369e":"code","2a620abf":"code","962bfce1":"code","f63ebe9f":"code","3287cf97":"code","bbbb54a2":"code","aae3ba41":"code","43eb5221":"code","c1200197":"code","cc76ceb9":"code","62cd79fd":"code","9d8271f2":"code","3f6a0553":"markdown","81a50d89":"markdown","5d0e0831":"markdown","1c2d07e2":"markdown","7ed0c836":"markdown","ee43dbbd":"markdown","9f85d154":"markdown","4c7fb3d5":"markdown","43c7afd3":"markdown","8421713b":"markdown","f2d1e9d4":"markdown","263d23e3":"markdown","4d07303e":"markdown","5897b2b8":"markdown","1ded3644":"markdown","c5b6fda8":"markdown"},"source":{"efa88c08":"import os\nprint(os.listdir(\"..\/input\"))","02038f74":"# Import the libraries\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom learntools.core import *\n","46b77fab":"kc_data = pd.read_csv('..\/input\/kc_house_data.csv')\nkc_data.head()","565267e9":"# Information about the dataset\nkc_data.info()","32694ce0":"# Statistical summary of the dataset\nkc_data.describe().transpose()","2bea369e":"import plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.offline as ply\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nfrom plotly import tools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style= \"whitegrid\")\n\ncorr_mat = kc_data.corr()\nplt.figure(figsize=(30,15))\nsns.heatmap(corr_mat, cmap = 'BrBG', linecolor = 'white', linewidth = 1, annot=True)","2a620abf":"plt.figure(figsize=(12,5))\nsns.distplot(kc_data['price'])","962bfce1":"fig1 = go.Scattergl(x=kc_data['sqft_living'], y=kc_data['price'], mode='markers', name='sqft_living')\nfig2 = go.Scattergl(x=kc_data['bedrooms'], y=kc_data['price'], mode = 'markers', name = 'bedrooms')\nfig3 = go.Scattergl(x=kc_data['bathrooms'], y=kc_data['price'],mode = 'markers', name = 'bathrooms')\nfig4 = go.Scattergl(x=kc_data['grade'], y=kc_data['price'],mode = 'markers', name = 'grade')\nfig5 = go.Scattergl(x=kc_data['yr_built'], y=kc_data['price'],mode = 'markers', name = 'yr_built')\nfig6 = go.Scattergl(x=kc_data['lat'], y=kc_data['price'],mode = 'markers', name = 'lat')\nfig = tools.make_subplots(rows=2, cols=3, subplot_titles=('sqft_living vs Price', 'bedrooms vs Price',\n'bathrooms vs Price', 'grade vs Price', 'yr_built vs price', 'lat vs price'))\nfig.append_trace(fig1, 1, 1)\nfig.append_trace(fig2, 1, 2)\nfig.append_trace(fig3, 1, 3)\nfig.append_trace(fig4, 2, 1)\nfig.append_trace(fig5, 2, 2)\nfig.append_trace(fig6, 2, 3)\nfig['layout'].update(height=800, width=800, title='Price Subplots')\nply.iplot(fig)","f63ebe9f":"kc_df = kc_data.drop(kc_data[kc_data[\"bedrooms\"]>10].index )","3287cf97":"from sklearn.model_selection import train_test_split\ny = kc_df.price\nfeatures = ['bedrooms', 'bathrooms', 'sqft_living', 'grade', 'yr_built', 'lat']\nX = kc_df[features]\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\n\n","bbbb54a2":"from sklearn.linear_model import LinearRegression\n\nkc_lrmodel = LinearRegression()\nkc_lrmodel.fit(X_train, y_train)\n# Predicting the Test set results\ny_lrpred = kc_lrmodel.predict(X_test)\n","aae3ba41":"from sklearn.svm import SVR\n\nkc_svrmodel = SVR(kernel='rbf')\nkc_svrmodel.fit(X_train, y_train)\ny_svrpred = kc_svrmodel.predict(X_test)","43eb5221":"from sklearn.neighbors import KNeighborsRegressor\n\nkc_knnmodel = KNeighborsRegressor(n_neighbors=1)\nkc_knnmodel.fit(X_train,y_train)\ny_knnpred = kc_knnmodel.predict(X_test)","c1200197":"from sklearn.ensemble import RandomForestRegressor\n\nkc_rfmodel = RandomForestRegressor(n_estimators=20, random_state = 0)\nkc_rfmodel.fit(X_train, y_train)\ny_rfpred = kc_rfmodel.predict(X_test)\n","cc76ceb9":"from xgboost import XGBRegressor\n\nkc_xgbmodel = XGBRegressor()\nkc_xgbmodel.fit(X_train, y_train)\ny_xgbpred = kc_xgbmodel.predict(X_test) ","62cd79fd":"# Calculate Adjusted R Squared Value\nfrom sklearn import metrics\nlr_R = metrics.r2_score(y_test,y_lrpred)\nlr_a_R = 1 - (1-lr_R)*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1)\nprint('Adjusted R Squared Value for Linear Regression: ', round(lr_a_R, 3) )\n\nsvr_R = metrics.r2_score(y_test,y_svrpred)\nsvr_a_R = 1 - (1-svr_R)*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1)\nprint('Adjusted R Squared Value for SVR: ', round(svr_a_R, 3) )\n\nrf_R = metrics.r2_score(y_test,y_rfpred)\nrf_a_R = 1 - (1-rf_R)*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1)\nprint('Adjusted R Squared Value for Random Forest: ', round(rf_a_R, 3) )\n\nknn_R = metrics.r2_score(y_test,y_knnpred)\nknn_a_R = 1 - (1-knn_R)*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1)\nprint('Adjusted R Squared Value for KNN: ', round(knn_a_R, 3) )\n\nxgb_R = metrics.r2_score(y_test,y_xgbpred)\nxgb_a_R = 1 - (1-xgb_R)*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1)\nprint('Adjusted R Squared Value for XGBoost: ', round(xgb_a_R, 3) )\n","9d8271f2":"columns = ['bedrooms', 'bathrooms', 'sqft_living', 'grade', 'yr_built', 'lat']\nsample = pd.DataFrame([[4, 3.25, 3360, 10, 1994, 47.70]],\n                        columns = columns )\ncustomer = sc_X.transform(sample)\n\nlrpredictor = kc_lrmodel.predict(customer)\nprint('Prediction by Linear Regression is', lrpredictor)\n\nsvrpredictor = kc_svrmodel.predict(customer)\nprint('Prediction by SVR is',svrpredictor)\n\nrfpredictor = kc_rfmodel.predict(customer)\nprint('Prediction by Random Forest is', rfpredictor)\n\nknnpredictor = kc_knnmodel.predict(customer)\nprint('Prediction by KNN is',knnpredictor)\n\nxgbpredictor = kc_xgbmodel.predict(customer)\nprint('Prediction by XGBoost is',xgbpredictor)","3f6a0553":"## Data Preprocessing\nFrom output of kc_data.info() above, I can tell the dataset does not have null values. But as we can see from the visualization I need to remove some outliers with ","81a50d89":"## Measuring the models","5d0e0831":"## Explore the dataset by visualization\nUsually in this step I can find the characteristics of dataset through various visualization techniques. I drew a correlation matrix heat map to depict the different degrees of correlation among the variables. As to price, high positively correlated features include sqft_living, grade, sqft_above, and sqft_living15. There are two negatively correlated features id and zipcode, and they have a very low correlation with price as well.","1c2d07e2":"I bought the house on 840000, Random Forest win this single case. Real world is much more complicated, will keep exploring!","7ed0c836":"### A real sample prediction\n2015 I bought a house in Redmond, can't wait to try my models! Guess which model will win?","ee43dbbd":"In this report, I am going to load the dataset, explore it, and use Multiple Linear Regression, SVR, K Nearest Neighbor Regression, Random Forest Regression and XGBoost to do the prediction, finally apply the Adjusted R^2 to measure the models.","9f85d154":"## Linear Regression","4c7fb3d5":"## Create y and X, split data for training and testing ","43c7afd3":"## Load and understand the dataset\nThe dataset records houses sold which range from May 2014 to May 2015. It consists of 19 home features, 1 house ID, and 1 dependent variable which is the price.","8421713b":"In this subplot, y axis is price and x axis are sqft_living, bedrooms, bathrooms, grade, yr_built and lat. My first assumption for Linear Regression is that the features of the dataset have a linear relationship with those dependent variables. ","f2d1e9d4":"## XGBoost","263d23e3":"## K Nearest Neighbor Regression","4d07303e":"## Random Forest","5897b2b8":"XGBoost got the best score! sencond place was Random Forest.","1ded3644":"With distribution plot of price, I can see that most of the prices are under 1 million with few outliers, some even close to 8 million.","c5b6fda8":"## SVR"}}