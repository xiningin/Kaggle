{"cell_type":{"04c39074":"code","113834a9":"code","78bc1eb9":"code","febb5da7":"code","cdf16efe":"code","1fc130cc":"code","32461f4a":"code","24ba7600":"code","e7ad7c02":"code","8bf1825f":"code","139ae9a9":"code","7c012192":"code","da4b95e2":"code","f8c40ddb":"code","7cf13d45":"code","fda6fc3f":"code","230e5c72":"code","8705155d":"code","946b9692":"code","0245dfd5":"code","44c44157":"code","687ef669":"code","3747b9a9":"code","b1cf2430":"code","b3ef816f":"code","42498c86":"code","0db2d5c8":"code","93e5428d":"code","61b4cf15":"code","097c175b":"code","a165dad0":"code","6e70489b":"code","c753ffe6":"code","6c6d9f13":"code","d155880c":"code","ea6dca9d":"code","d26c89f7":"code","874be7f1":"code","d161cf5f":"code","54a5056d":"code","e40205d2":"code","0cf21ecf":"code","6fe8bdf7":"code","63d04d0a":"code","355cda88":"code","401586c2":"code","a11fecd0":"code","7c942faa":"code","9ac49b83":"code","6d1bb52a":"code","7d693e2c":"code","fa0da01b":"code","aa713ffb":"code","77b76bbe":"code","ae3a1857":"code","7e38b516":"code","e8c8f3be":"code","12245462":"code","3bd03d98":"code","7fb8c33c":"code","97a5bc29":"code","1a1f4fe3":"code","cd1c1121":"code","29859099":"code","81c77edc":"code","f7e7ff17":"code","7c55ae2a":"code","5f6c2ab8":"code","d9eb92c1":"code","9e3587fa":"code","133c582c":"code","7a92f868":"code","5ae56e9c":"code","aad92f3c":"code","b50d3539":"code","b405e317":"code","a9d18648":"code","9976c4ee":"code","3717bf90":"code","07caf9fc":"code","5000da7c":"markdown","792f9ea4":"markdown","75291149":"markdown","ddbb893c":"markdown","45ba4578":"markdown","b31148df":"markdown","602a9af5":"markdown","c8ceb484":"markdown","9f69972d":"markdown","260d019d":"markdown","296d1c29":"markdown","b42f419f":"markdown","b0f18500":"markdown","225164e0":"markdown","7f612075":"markdown","e9342d20":"markdown","f4674cdf":"markdown","e2dd09a0":"markdown"},"source":{"04c39074":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport missingno as msno\nimport os\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","113834a9":"df = pd.read_csv(\"..\/input\/data-science-day1-titanic\/DSB_Day1_Titanic_train.csv\")\ndf.head()","78bc1eb9":"# Defining Interquartile Range\nq1 = df[\"Age\"].quantile(0.25)\nq3 = df[\"Age\"].quantile(0.75)\niqr = q3 - q1\nup = q3 + 1.5 * iqr\nlow = q1 - 1.5 * iqr","febb5da7":"# less than the lower limit or greater than the upper limit\ndf[(df[\"Age\"] < low) | (df[\"Age\"] > up)]","cdf16efe":"# lets find the index of the outliers\ndf[(df[\"Age\"] < low) | (df[\"Age\"] > up)].index","1fc130cc":"# Lets find do I have any outliers ?\ndf[(df[\"Age\"] > up) | (df[\"Age\"] < low)].any(axis=None)","32461f4a":"# Lets add functionalty\n\ndef outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\noutlier_thresholds(df, \"Age\")\noutlier_thresholds(df, \"Fare\")","24ba7600":"# Lets add check outlier function for further needs (Return Boolean)\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\ncheck_outlier(df, \"Age\")","e7ad7c02":"def grab_col_names(dataframe, cat_th=10, car_th=20):\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n\n    return cat_cols, num_cols, cat_but_car\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","8bf1825f":"for col in num_cols:\n    print(col, check_outlier(df, col))","139ae9a9":"# Lets add function to grab the outliers\ndef grab_outliers(dataframe, col_name, index=False):\n    low, up = outlier_thresholds(dataframe, col_name)\n    if dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0] > 10:\n        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].head())\n    else:\n        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))])\n\n    if index:\n        outlier_index = dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].index\n        return outlier_index\n\ngrab_outliers(df, \"Age\", True)","7c012192":"sns.boxplot(df[\"Age\"])","da4b95e2":"# Dropping the outlier data points\ndef remove_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    df_without_outliers = dataframe[~((dataframe[col_name] < low_limit) | (dataframe[col_name] > up_limit))]\n    return df_without_outliers\n\nremove_outlier(df, \"Fare\").shape","f8c40ddb":"for col in [\"Age\", \"Fare\"]:\n    new_df = remove_outlier(df, col)\n\ndf.shape[0] - new_df.shape[0]","7cf13d45":"def replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\nremove_outlier(df, \"Age\").shape\nreplace_with_thresholds(df, \"Age\")","fda6fc3f":"# We can see that all the outlier data points have gone\nsns.boxplot(df[\"Age\"])","230e5c72":"df = sns.load_dataset('diamonds')\ndf = df.select_dtypes(include=['float64', 'int64'])\ndf = df.dropna()\ndf.head()","8705155d":"for col in df.columns:\n    print(col, check_outlier(df, col))","946b9692":"# The higher LOF score means the more normal\nclf = LocalOutlierFactor(n_neighbors=20)\nclf.fit_predict(df)\ndf_scores = clf.negative_outlier_factor_\nnp.sort(df_scores)[0:5] # selecting the worst 5 five scores","0245dfd5":"scores = pd.DataFrame(np.sort(df_scores))\nscores.plot(stacked=True, xlim=[0, 20], style='.-')\nplt.show()","44c44157":"th = np.sort(df_scores)[3]\ndf[df_scores < th]\ndf[df_scores < th].shape\ndf.describe([0.01, 0.05, 0.75, 0.90, 0.99]).T\ndf[df_scores < th].index","687ef669":"df[df_scores < th].drop(axis=0, labels=df[df_scores < th].index)","3747b9a9":"V1 = np.array([1, 3, 6, np.NaN, 7, 1, np.NaN, 9, 15])\nV2 = np.array([7, np.NaN, 5, 8, 12, np.NaN, np.NaN, 2, 3])\nV3 = np.array([np.NaN, 12, 5, 6, 14, 7, np.NaN, 2, 31])\nV4 = np.array([\"IT\", \"IT\", \"IK\", \"IK\", \"IK\", \"IK\", \"IT\", \"IT\", \"IT\"])\n\ndff = pd.DataFrame(\n    {\"salary\": V1,\n     \"V2\": V2,\n     \"V3\": V3,\n     \"departmant\": V4}\n)","b1cf2430":"# Lets catch the missing values\ndff.isnull().values.any()","b3ef816f":"# Catching the missing value counts for each columns\ndff.isnull().sum()","42498c86":"# Catching the not null data counts for each columns\ndff.notnull().sum()","0db2d5c8":"# Catching total missing value counts for all the dataset\ndff.isnull().sum().sum()","93e5428d":"# Catching the columns that have at least 1 misssing value \ndff[dff.isnull().any(axis=1)]","61b4cf15":"# Lets add functionality\n\ndef missing_values_table(dataframe, na_name=False):\n    # The columns name that contains missing value\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    # Number of missing data\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    # Ration of the missing data points over the dataset\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    # Missing dataframe\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n    if na_name:\n        return na_columns\n    \nmissing_values_table(dff, True)","097c175b":"df = pd.read_csv(\"..\/input\/data-science-day1-titanic\/DSB_Day1_Titanic_train.csv\")\ndf.isnull().any()","a165dad0":"# Dropping the missing data points\ndf.dropna()","6e70489b":"# Assigning mean, median value of the related column of the dataset\ndf[\"Age\"].fillna(0)\ndf[\"Age\"].fillna(df[\"Age\"].mean())\ndf[\"Age\"].fillna(df[\"Age\"].median())","c753ffe6":"df.apply(lambda x: x.fillna(x.mean()) if x.dtype != \"O\" else x, axis=0).head()\ndff = df.apply(lambda x: x.fillna(x.mean()) if x.dtype != \"O\" else x, axis=0)\ndff.isnull().sum().sort_values(ascending=False)","6c6d9f13":"dff[\"Embarked\"].fillna(dff[\"Embarked\"].mode()[0])\ndff[\"Embarked\"].fillna(dff[\"Embarked\"].mode()[0]).isnull().sum()\ndff[\"Embarked\"].fillna(\"missing\")","d155880c":"dff.apply(lambda x: x.fillna(x.mode()[0]) if (x.dtype == \"O\" and len(x.unique()) <= 10) else x, axis=0)\n\ndff.apply(lambda x: x.fillna(x.mode()[0]) if (x.dtype == \"O\" and len(x.unique()) <= 10) else x, axis=0).isnull().sum()","ea6dca9d":"# Using imputer to fill the missing data points\nfrom sklearn.impute import SimpleImputer\n\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')  # mean, median, most_frequent, constant\nimp_mean.fit(df)\nimp_mean.transform(df)","d26c89f7":"# Value Assignment in Categorical Variable Breakdown\n# Using predictive methods to fill missing data points\ndf = pd.read_csv(\"..\/input\/data-science-day1-titanic\/DSB_Day1_Titanic_train.csv\")\ndf[\"Age\"].fillna(df.groupby(\"Sex\")[\"Age\"].transform(\"mean\")).isnull().sum()","874be7f1":"# Using predictive methods to fill missing data points\ndf = pd.read_csv(\"..\/input\/data-science-day1-titanic\/DSB_Day1_Titanic_train.csv\")\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\nnum_cols = [col for col in num_cols if col not in \"PassengerId\"]\ndff = pd.get_dummies(df[cat_cols + num_cols], drop_first=True)\ndff.head()","d161cf5f":"# Scaling the dataset\nscaler = MinMaxScaler()\ndff = pd.DataFrame(scaler.fit_transform(dff), columns=dff.columns)\ndff.head()","54a5056d":"# Lets use KNN imputer for predictive filling the missing data points\n\nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=5)\ndff = pd.DataFrame(imputer.fit_transform(dff), columns=dff.columns)\ndff.head()","e40205d2":"# Lets use inverse transform to reach the raw dataset\ndff = pd.DataFrame(scaler.inverse_transform(dff), columns=dff.columns)\ndf[\"age_imputed_knn\"] = dff[[\"Age\"]]\ndf.head()","0cf21ecf":"# Lets check the null age values with computed age_imputed_knn\ndf.loc[df[\"Age\"].isnull(), [\"Age\", \"age_imputed_knn\"]]","6fe8bdf7":"msno.bar(df)\nplt.show()","63d04d0a":"msno.matrix(df)\nplt.show()","355cda88":"msno.heatmap(df)\nplt.show()","401586c2":"missing_values_table(df, True)\nna_cols = missing_values_table(df, True)\n\n\ndef missing_vs_target(dataframe, target, na_columns):\n    temp_df = dataframe.copy()\n    for col in na_columns:\n        temp_df[col + '_NA_FLAG'] = np.where(temp_df[col].isnull(), 1, 0)\n    na_flags = temp_df.loc[:, temp_df.columns.str.contains(\"_NA_\")].columns\n    for col in na_flags:\n        print(pd.DataFrame({\"TARGET_MEAN\": temp_df.groupby(col)[target].mean(),\n                            \"Count\": temp_df.groupby(col)[target].count()}), end=\"\\n\\n\\n\")\n\n\nmissing_vs_target(df, \"Survived\", na_cols)","a11fecd0":"df = pd.read_csv(\"..\/input\/data-science-day1-titanic\/DSB_Day1_Titanic_train.csv\")\n\ndf[\"Sex\"].head()","7c942faa":"le = LabelEncoder()\nle.fit_transform(df[\"Sex\"])[0:5]\nle.inverse_transform([0, 1])","9ac49b83":"# Lets add fuctionality\n\ndf[\"Age\"].fillna(df.groupby(\"Sex\")[\"Age\"].transform(\"mean\")).isnull().sum()\n\n# First defining the binary columns using categorical columns\nbinary_cols = [col for col in df.columns if df[col].dtype not in [int, float]\n               and df[col].nunique() == 2]","6d1bb52a":"\ndef label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe\n\nfor col in binary_cols:\n    label_encoder(df, col)\n    \ndf.head(10)","7d693e2c":"def one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\n\nohe_cols = [col for col in df.columns if 10 >= df[col].nunique() > 2]\none_hot_encoder(df, ohe_cols).head()","fa0da01b":"# 1. Target Frequency\n# 2. Traget Ratio\n# 3. Group by columns for target column\n\n# Lets use large dataset to understand better the rare encoding\ndf = pd.read_csv(\"..\/input\/home-credit-default-risk\/application_train.csv\")\n\ndef rare_analyser(dataframe, target, cat_cols):\n    for col in cat_cols:\n        print(col, \":\", len(dataframe[col].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO\": dataframe[col].value_counts() \/ len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")","aa713ffb":"df.TARGET.head()","77b76bbe":"def rare_encoder(dataframe, rare_perc):\n    temp_df = dataframe.copy()\n\n    rare_columns = [col for col in temp_df.columns if temp_df[col].dtypes == 'O'\n                    and (temp_df[col].value_counts() \/ len(temp_df) < rare_perc).any(axis=None)]\n\n    for var in rare_columns:\n        tmp = temp_df[var].value_counts() \/ len(temp_df)\n        rare_labels = tmp[tmp < rare_perc].index\n        temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])\n\n    return temp_df\n\nnew_df = rare_encoder(df, 0.01)","ae3a1857":"cat_cols, num_cols, cat_but_car = grab_col_names(df)\n# With the ouput, we can analyze the rare columns\nrare_analyser(new_df, \"TARGET\", cat_cols)","7e38b516":"rare_analyser(df, \"TARGET\", cat_cols)","e8c8f3be":"df = pd.read_csv(\"..\/input\/data-science-day1-titanic\/DSB_Day1_Titanic_train.csv\")\n\nscaler = StandardScaler()\n\ndf[\"Age_standard_scaler\"] = scaler.fit_transform(df[[\"Age\"]])\n\ndf.head()","12245462":"rscaler = RobustScaler()\n\ndf[\"Age_robuts_scaler\"] = rscaler.fit_transform(df[[\"Age\"]])\n\ndf.head()","3bd03d98":"mmscaler = MinMaxScaler()\n\ndf[\"Age_min_max_scaler\"] = mmscaler.fit_transform(df[[\"Age\"]])\n\ndf.head()","7fb8c33c":"df[\"Age_log\"] = np.log(df[\"Age\"])\n\ndf.head()","97a5bc29":"# If the cabin is Nan we will assign as 0 otherwise 1\n# We know that employees att Titanic dont have Cabin\n\ndf[\"NEW_CABIN_BOOL\"] = df[\"Cabin\"].notnull().astype('int')\n\n# So let's analyze them if they survive or not\n\ndf.groupby(\"NEW_CABIN_BOOL\").agg({\"Survived\": \"mean\"})","1a1f4fe3":"# Lets check the used method for Cabin column using proportion test\n\nfrom statsmodels.stats.proportion import proportions_ztest\n\ntest_stat, pvalue = proportions_ztest(count=[df.loc[df[\"NEW_CABIN_BOOL\"] == 1, \"Survived\"].sum(),\n                                             df.loc[df[\"NEW_CABIN_BOOL\"] == 0, \"Survived\"].sum()],\n\n                                      nobs=[df.loc[df[\"NEW_CABIN_BOOL\"] == 1, \"Survived\"].shape[0],\n                                            df.loc[df[\"NEW_CABIN_BOOL\"] == 0, \"Survived\"].shape[0]])\n\nprint('Test Stat = %.4f, p-value = %.4f' % (test_stat, pvalue))\n\n# Ho rejected. That means, there is no difference between the cabin breakdown for Survived target as statistically","cd1c1121":"#sibsp\t# of siblings \/ spouses aboard the Titanic\t\n#parch\t# of parents \/ children aboard the Titanic\n\n# Lets check the relation as alone or not using feature extraction\n\ndf.loc[((df['SibSp'] + df['Parch']) > 0), \"NEW_IS_ALONE\"] = \"NO\"\ndf.loc[((df['SibSp'] + df['Parch']) == 0), \"NEW_IS_ALONE\"] = \"YES\"\ndf.head()","29859099":"df.groupby(\"NEW_IS_ALONE\").agg({\"Survived\": \"mean\"})","81c77edc":"test_stat, pvalue = proportions_ztest(count=[df.loc[df[\"NEW_IS_ALONE\"] == \"YES\", \"Survived\"].sum(),\n                                             df.loc[df[\"NEW_IS_ALONE\"] == \"NO\", \"Survived\"].sum()],\n\n                                      nobs=[df.loc[df[\"NEW_IS_ALONE\"] == \"YES\", \"Survived\"].shape[0],\n                                            df.loc[df[\"NEW_IS_ALONE\"] == \"NO\", \"Survived\"].shape[0]])\n\nprint('Test Stat = %.4f, p-value = %.4f' % (test_stat, pvalue))\n\n# Ho rejected. That means, there is no difference between the cabin breakdown for Survived target as statistically","f7e7ff17":"# Lets check the title of the titanic crew and analyze\n\ndf[\"NEW_NAME_DR\"] = df[\"Name\"].apply(lambda x: len([x for x in x.split() if x.startswith(\"Dr\")]))\ndf.groupby(\"NEW_NAME_DR\").agg({\"Survived\": \"mean\"})","7c55ae2a":"# Lets use the same method for all title using regex\ndf['NEW_TITLE'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ndf[[\"NEW_TITLE\", \"Survived\", \"Age\"]].groupby([\"NEW_TITLE\"]).agg({\"Survived\": \"mean\", \"Age\": [\"count\", \"mean\"]})","5f6c2ab8":"\ndf[\"NEW_FAMILY_SIZE\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\ndf.loc[(df['Sex'] == 'male') & (df['Age'] <= 21), 'NEW_SEX_CAT'] = 'youngmale'\ndf.loc[(df['Sex'] == 'male') & ((df['Age'] > 21) & (df['Age']) < 50), 'NEW_SEX_CAT'] = 'maturemale'\ndf.loc[(df['Sex'] == 'male') & (df['Age'] > 50), 'NEW_SEX_CAT'] = 'seniormale'\ndf.loc[(df['Sex'] == 'female') & (df['Age'] <= 21), 'NEW_SEX_CAT'] = 'youngfemale'\ndf.loc[(df['Sex'] == 'female') & ((df['Age'] > 21) & (df['Age']) < 50), 'NEW_SEX_CAT'] = 'maturefemale'\ndf.loc[(df['Sex'] == 'female') & (df['Age'] > 50), 'NEW_SEX_CAT'] = 'seniorfemale'\n\ndf[\"NEW_AGExPCLASS\"] = df[\"Age\"] * df[\"Pclass\"]","d9eb92c1":"df.head()","9e3587fa":"df = pd.read_csv(\"..\/input\/data-science-day1-titanic\/DSB_Day1_Titanic_train.csv\")","133c582c":"# Feature Engineering\n\n\n# Cabin bool\ndf[\"NEW_CABIN_BOOL\"] = df[\"Cabin\"].notnull().astype('int')\n# Name count\ndf[\"NEW_NAME_COUNT\"] = df[\"Name\"].str.len()\n# name word count\ndf[\"NEW_NAME_WORD_COUNT\"] = df[\"Name\"].apply(lambda x: len(str(x).split(\" \")))\n# name dr\ndf[\"NEW_NAME_DR\"] = df[\"Name\"].apply(lambda x: len([x for x in x.split() if x.startswith(\"Dr\")]))\n# name title\ndf['NEW_TITLE'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n# family size\ndf[\"NEW_FAMILY_SIZE\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n# age_pclass\ndf[\"NEW_AGE_PCLASS\"] = df[\"Age\"] * df[\"Pclass\"]\n# is alone\ndf.loc[((df['SibSp'] + df['Parch']) > 0), \"NEW_IS_ALONE\"] = \"NO\"\ndf.loc[((df['SibSp'] + df['Parch']) == 0), \"NEW_IS_ALONE\"] = \"YES\"\n# age level\ndf.loc[(df['Age'] < 18), 'NEW_AGE_CAT'] = 'young'\ndf.loc[(df['Age'] >= 18) & (df['Age'] < 56), 'NEW_AGE_CAT'] = 'mature'\ndf.loc[(df['Age'] >= 56), 'NEW_AGE_CAT'] = 'senior'\n# sex x age\ndf.loc[(df['Sex'] == 'male') & (df['Age'] <= 21), 'NEW_SEX_CAT'] = 'youngmale'\ndf.loc[(df['Sex'] == 'male') & ((df['Age'] > 21) & (df['Age']) < 50), 'NEW_SEX_CAT'] = 'maturemale'\ndf.loc[(df['Sex'] == 'male') & (df['Age'] > 50), 'NEW_SEX_CAT'] = 'seniormale'\ndf.loc[(df['Sex'] == 'female') & (df['Age'] <= 21), 'NEW_SEX_CAT'] = 'youngfemale'\ndf.loc[(df['Sex'] == 'female') & ((df['Age'] > 21) & (df['Age']) < 50), 'NEW_SEX_CAT'] = 'maturefemale'\ndf.loc[(df['Sex'] == 'female') & (df['Age'] > 50), 'NEW_SEX_CAT'] = 'seniorfemale'\n\ndf.head()","7a92f868":"cat_cols, num_cols, cat_but_car = grab_col_names(df)\n\nnum_cols = [col for col in num_cols if \"PASSENGERID\" not in col]\n\ndf.shape","5ae56e9c":"#############################################\n# 2. Outliers (Ayk\u0131r\u0131 De\u011ferler)\n#############################################\n\n\nfor col in num_cols:\n    print(col, check_outlier(df, col))\n\n\nfor col in num_cols:\n    replace_with_thresholds(df, col)\n\n\nfor col in num_cols:\n    print(col, check_outlier(df, col))","aad92f3c":"#############################################\n# 3. Missing Values (Eksik De\u011ferler)\n#############################################\n\nmissing_values_table(df)\ndf.head()\n\n\ndf.drop(\"Cabin\", inplace=True, axis=1)\nmissing_values_table(df)\n\n\n\nremove_cols = [\"Ticket\", \"Name\"]\ndf.drop(remove_cols, inplace=True, axis=1)\ndf.head()\n\nmissing_values_table(df)","b50d3539":"\ndf[\"Age\"] = df[\"Age\"].fillna(df.groupby(\"NEW_TITLE\")[\"Age\"].transform(\"median\"))\nmissing_values_table(df)\n\ndf[\"NEW_AGE_PCLASS\"] = df[\"Age\"] * df[\"Pclass\"]\n\ndf.loc[(df['Age'] < 18), 'NEW_AGE_CAT'] = 'young'\ndf.loc[(df['Age'] >= 18) & (df['Age'] < 56), 'NEW_AGE_CAT'] = 'mature'\ndf.loc[(df['Age'] >= 56), 'NEW_AGE_CAT'] = 'senior'\n\ndf.loc[(df['Sex'] == 'male') & (df['Age'] <= 21), 'NEW_SEX_CAT'] = 'youngmale'\ndf.loc[(df['Sex'] == 'male') & ((df['Age'] > 21) & (df['Age']) < 50), 'NEW_SEX_CAT'] = 'maturemale'\ndf.loc[(df['Sex'] == 'male') & (df['Age'] > 50), 'NEW_SEX_CAT'] = 'seniormale'\ndf.loc[(df['Sex'] == 'female') & (df['Age'] <= 21), 'NEW_SEX_CAT'] = 'youngfemale'\ndf.loc[(df['Sex'] == 'female') & ((df['Age'] > 21) & (df['Age']) < 50), 'NEW_SEX_CAT'] = 'maturefemale'\ndf.loc[(df['Sex'] == 'female') & (df['Age'] > 50), 'NEW_SEX_CAT'] = 'seniorfemale'\n\nmissing_values_table(df)\n\ndf = df.apply(lambda x: x.fillna(x.mode()[0]) if (x.dtype == \"O\" and len(x.unique()) <= 10) else x, axis=0)\nmissing_values_table(df)","b405e317":"# Label Encoding\n\nbinary_cols = [col for col in df.columns if df[col].dtype not in [int, float]\n               and df[col].nunique() == 2]\n\nfor col in binary_cols:\n    df = label_encoder(df, col)","a9d18648":"# Rare Encoding\n\nrare_analyser(df, \"Survived\", cat_cols)\n\ndf = rare_encoder(df, 0.01)\ndf[\"NEW_TITLE\"].value_counts()\nrare_analyser(df, \"Survived\", cat_cols)","9976c4ee":"#############################################\n# One-Hot Encoding\n#############################################\n\n\nohe_cols = [col for col in df.columns if 10 >= df[col].nunique() > 2]\n\ndf = one_hot_encoder(df, ohe_cols)\ndf.head()\ndf.shape\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\n\nnum_cols = [col for col in num_cols if \"PassengerId\" not in col]\n\n\nrare_analyser(df, \"Survived\", cat_cols)\ndf.head()\n\n\n\n(df[\"Sex\"].value_counts() \/ len(df) < 0.01).any()\n(df[\"NEW_NAME_WORD_COUNT_9\"].value_counts() \/ len(df) < 0.01).any()\n\n\nuseless_cols = [col for col in df.columns if df[col].nunique() == 2 and\n                (df[col].value_counts() \/ len(df) < 0.01).any(axis=None)]","3717bf90":"#############################################\n# Standart Scaler\n#############################################\n\n\nnum_cols\n\nscaler = StandardScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])\n\ndf[num_cols].head()\n\n# son kontrol:\ndf.head()\ndf.shape\ndf.tail()","07caf9fc":"\n#############################################\n# Model\n#############################################\n\n\ny = df[\"Survived\"]\nX = df.drop([\"PassengerId\", \"Survived\"], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n\n# MODEL\nfrom sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier(random_state=46).fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\naccuracy_score(y_pred, y_test)","5000da7c":"## Label Encoding & Binary Encoding\n\nEncoding or continuization is the transformation of categorical variables to binary or numerical counterparts. An example is to treat male or female for gender as 1 or 0. Categorical variables must be encoded in many modeling methods (e.g., linear regression, SVM, neural networks)\n\n![image.png](https:\/\/womaneng.com\/wp-content\/uploads\/2018\/09\/onehotencoding.jpg)","792f9ea4":"## Advanced Analysis for Missing Data Points\n\n\ud83d\udc8e We will use missingo library for plotting and interpreting the figure.\n\n\ud83d\udc8e And, we will check the missing value correlations and the missing values relation between each other","75291149":"## One-Hot Encoding\n\nThough label encoding is straight but it has the disadvantage that the numeric values can be misinterpreted by algorithms as having some sort of hierarchy\/order in them. This ordering issue is addressed in another common alternative approach called \u2018One-Hot Encoding\u2019. In this strategy, each category value is converted into a new column and assigned a 1 or 0 (notation for true\/false) value to the column.\n\n![image.png](https:\/\/miro.medium.com\/max\/2000\/1*WHM-sZuVQBOZzZv64fMgow.png)\n\n__Advantages of one-hot encoding__\n- Does not assume the distribution of categories of the categorical variable.\n\n- Keeps all the information of the categorical variable.\n\n- Suitable for linear models.\n\n__Limitations of one-hot encoding__\n\n- Expands the feature space.\n\n- Does not add extra information while encoding.\n\n- Many dummy variables may be identical, and this can introduce redundant information.\n\nSource:\n\n[One-Hot Encoding](https:\/\/towardsdatascience.com\/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd)","ddbb893c":"__Multivariate Outlier Analysis (Local Outlier Factor)__","45ba4578":"In this notebook we will deeply analyze feature engineering topics as below.\n\n- Outliers\n- Missing Values\n- Encoding (Label Encoding, One-Hot Encoding, Rare Encoding)\n- Feature Scaling\n- Feature Extraction\n- Feature Interactions\n- End-to-End Application","b31148df":"# 3. Encoding (Label Encoding, One-Hot Encoding, Rare Encoding)","602a9af5":"# 2.Missing Values\n\nThe imputation method develops reasonable guesses for missing data. It\u2019s most useful when the percentage of missing data is low. If the portion of missing data is too high, the results lack natural variation that could result in an effective model.\n\nThe other option is to remove data. When dealing with data that is missing at random, related data can be deleted to reduce bias. Removing data may not be the best option if there are not enough observations to result in a reliable analysis. In some situations, observation of specific events or factors may be required.\n\n\ud83d\udc8e Direct removal of missing value observations from the data set and not examining the randomness will lose the statistical reliability of inferences and modelling studies (Alpar, 2011).\n\n![image.png](https:\/\/d35fo82fjcw0y8.cloudfront.net\/2016\/04\/03210550\/missing-values-.jpg)\n","c8ceb484":"__Solving Outliers Problem__\n\nWe will check dropping and capping methods in order to solve outlier problems","9f69972d":"# 1. Outliers\n\nAn outlier is an observation that lies an abnormal distance from other values in a random sample from a population. In a sense, this definition leaves it up to the analyst (or a consensus process) to decide what will be considered abnormal. Before abnormal observations can be singled out, it is necessary to characterize normal observations.\n\n\ud83d\udc49 __Trimming__: It excludes the outlier values from our analysis. By applying this technique our data becomes thin when there are more outliers present in the dataset. Its main advantage is its fastest nature.\n\n\ud83d\udc49 __Capping__: In this technique, we cap our outliers data and make the limit i.e, above a particular value or less than that value, all the values will be considered as outliers, and the number of outliers in the dataset gives that capping number.\n\nFor Example, if you\u2019re working on the income feature, you might find that people above a certain income level behave in the same way as those with a lower income. In this case, you can cap the income value at a level that keeps that intact and accordingly treat the outliers.\n\n\ud83d\udc49 __Treat outliers as a missing value__: By assuming outliers as the missing observations, treat them accordingly i.e, same as those of missing values.\n\n\ud83d\udc49 __Discretization__: In this technique, by making the groups we include the outliers in a particular group and force them to behave in the same manner as those of other points in that group. This technique is also known as Binning.\n\n__How to Detect Outliers ?__\n\n\ud83d\udc49 __For Normal distributions__: Use empirical relations of Normal distribution.\n\n       The data points which fall below mean-3*(sigma) or above mean+3*(sigma) are outliers.\n\nwhere mean and sigma are the average value and standard deviation of a particular column.\n\n\ud83d\udc49 __For Skewed distributions__: Use Inter-Quartile Range (IQR) proximity rule __(Box Plot).__\n\n    The data points which fall below Q1 \u2013 1.5 IQR or above Q3 + 1.5 IQR are outliers.\n\nwhere Q1 and Q3 are the 25th and 75th percentile of the dataset respectively, and IQR represents the inter-quartile range and given by Q3 \u2013 Q1.\n\n![image.png](https:\/\/miro.medium.com\/max\/1400\/1*NRlqiZGQdsIyAu0KzP7LaQ.png)\n\nSource:\n\n[Analytics Vidhya](https:\/\/www.analyticsvidhya.com\/blog\/2021\/05\/feature-engineering-how-to-detect-and-remove-outliers-with-python-code\/)","260d019d":"# 7. End-to-End Application\n\nWe will use Titanic dataset and we will use all the feature engineering methods. After all this process, we will use Random Forest Classifier to predict Survive or not.\n\n![image.png](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/f\/fd\/RMS_Titanic_3.jpg\/1200px-RMS_Titanic_3.jpg)","296d1c29":"# 4. Feature Scaling\n\n![image.png](https:\/\/miro.medium.com\/max\/2000\/1*yR54MSI1jjnf2QeGtt57PA.png)\n\nFeature scaling in machine learning is one of the most critical steps during the pre-processing of data before creating a machine learning model. Scaling can make a difference between a weak machine learning model and a better one.\nThe most common techniques of feature scaling are Normalization and Standardization.\nNormalization is used when we want to bound our values between two numbers, typically, between [0,1] or [-1,1]. While Standardization transforms the data to have zero mean and a variance of 1, they make our data unitless. Refer to the below diagram, which shows how data looks after scaling in the X-Y plane.\n\n![image.png](https:\/\/i.stack.imgur.com\/lggVP.png)\n\n__Type of feature scaling:__\n\n- __StandardScaler__: z = (x - u) \/ s\n\n- __RobustScaler__: value = (value \u2013 median) \/ (p75 \u2013 p25)\n\n- __MinMaxScaler__:  \n        \n        X_std = (X - X.min(axis=0)) \/ (X.max(axis=0) - X.min(axis=0))\n        X_scaled = X_std * (max - min) + min\n\n\n- __Logaritmic Scaler__: Taking the log of the value. But, if we have a negative values we couldn't take the log. So we need to be careful abaout it.\n\n\nSource:\n\n[Towards Data Science](https:\/\/towardsdatascience.com\/all-about-feature-scaling-bcc0ad75cb35)","b42f419f":"## Lets Code and Practice \ud83d\ude80\ud83d\udc68\ud83c\udffc\u200d\ud83d\udcbb","b0f18500":"## Rare Encoding\n\nRare labels are those that appear only in a tiny proportion of the observations in a dataset. Rare labels may cause some issues, especially with overfitting and generalization.\nThe solution to that problem is to group those rare labels into a new category like other or rare\u2014this way, the possible issues can be prevented.\n\n![image.png](https:\/\/miro.medium.com\/max\/2000\/1*wmgHrdrZ3fXvlYL5zHpt7A.png)\n\nThis way, categories that are new in the test set are treated as rare, and the model can know how to handle those categories as well, even though they weren\u2019t present in the train set.\n\nSource: \n\n[Rare Encoding](https:\/\/heartbeat.comet.ml\/hands-on-with-feature-engineering-techniques-encoding-categorical-variables-be4bc0715394)","225164e0":"# 5. Feature Extraction\n\nFeature extraction is a process of dimensionality reduction by which an initial set of raw data is reduced to more manageable groups for processing. A characteristic of these large data sets is a large number of variables that require a lot of computing resources to process.\n\n![image.png](https:\/\/www.shopfactory.com\/contents\/media\/feature-people.png)\n\n[Deep AI](https:\/\/deepai.org\/machine-learning-glossary-and-terms\/feature-extraction#:~:text=Feature%20extraction%20is%20a%20process,of%20computing%20resources%20to%20process.)","7f612075":"# Feature Engineering\n\nWhat is a feature and why we need the engineering of it? \n\nBasically, all machine learning algorithms use some input data to create outputs. This input data comprise features, which are usually in the form of structured columns. Algorithms require features with some specific characteristic to work properly. Here, the need for feature engineering arises. Feature engineering efforts mainly have two goals:\n\n- Preparing the proper input dataset, compatible with the machine learning algorithm requirements.\n\n- Improving the performance of machine learning models.\n\nThe important point is that machine learning algorithms desire structured dataset because of that reason feature engineering is a key indicator for data science life cyle. __Harward Business Review article__ stated that, \"Poor data quality is enemy number one to the widespread, profitable use of machine learning. The quality demands of machine learning are steep, and bad data can rear its ugly head twice both in the historical data used to train the predictive model and in the new data used by that model to make future decisions. To ensure you have the right data for machine learning, you must have an aggressive, well-executed quality program.\"\n\nBesides that, according to a survey in Forbes, data scientists spend 80% of their time on data preparation:\n\n![image.png](https:\/\/miro.medium.com\/max\/1400\/0*-dn9U8gMVWjDahQV.jpg)\n\n\n\nSource:\n\n[Harward Business Review](https:\/\/hbr.org\/2018\/04\/if-your-data-is-bad-your-machine-learning-tools-are-useless)\n\n[Towards Data Science](https:\/\/towardsdatascience.com\/feature-engineering-for-machine-learning-3a5e293a5114)\n\n[Forbes Survey](https:\/\/www.forbes.com\/sites\/gilpress\/2016\/03\/23\/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says\/?sh=28c8fe0c6f63)\n","e9342d20":"## We have defined the missing data points, so how can we solve the missing data problem ? \ud83d\ude4b\n\nThere are some approach to achieve this goal as below.\n\n- Dropping the missing data points\n\n- Assigning mean, median value of the related column of the dataset\n\n- Using imputer to fill the missing data points\n\n- Value Assignment in Categorical Variable Breakdown\n\n- Using predictive methods to fill missing data points","f4674cdf":"# 6. Feature Interactions\n\n![image.png](https:\/\/miro.medium.com\/max\/620\/1*SGai7lOKRn9YhM2p-8SChQ.jpeg)\n\nIf a machine learning model makes a prediction based on two features, we can decompose the prediction into four terms: a constant term, a term for the first feature, a term for the second feature and a term for the interaction between the two features.\nThe interaction between two features is the change in the prediction that occurs by varying the features after considering the individual feature effects.\n","e2dd09a0":"## Lets Code and Practice \ud83d\ude80\ud83d\udc68\ud83c\udffc\u200d\ud83d\udcbb"}}