{"cell_type":{"84c23e31":"code","82f94bcc":"code","2a92d65b":"code","c90605a4":"code","ca1a9df8":"code","3c73c108":"code","48f6d81b":"code","4d8d1fc4":"code","4a30df8f":"code","ac375ec8":"code","f797750a":"code","c1cd555a":"code","ed768b8d":"markdown","2ea7710a":"markdown","b76c2d5b":"markdown","ce93425a":"markdown","f1b14e21":"markdown","4b78a7fe":"markdown","ea688b08":"markdown","f6900418":"markdown","3b3989d1":"markdown"},"source":{"84c23e31":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport geopandas as gpd\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","82f94bcc":"train_set = pd.read_csv('..\/input\/TTiDS20\/train.csv')\nzips = gpd.read_file('..\/input\/TTiDS20\/zipcodes.csv')\ntest = pd.read_csv('..\/input\/TTiDS20\/test_no_target.csv')\nsubmission = pd.read_csv('..\/input\/TTiDS20\/sample_submission.csv')\nzips.loc[:, 'zipcode'] = zips['zipcode'].astype('int64')","2a92d65b":"train_set.head()","c90605a4":"# train_set.columns\ntrain_set['brand'].unique()","ca1a9df8":"zip_dict = {x: y for x, y in zip(zips['zipcode'], zips['city'])}","3c73c108":"train_set.loc[:, 'city'] = train_set['zipcode'].map(zip_dict)\ntest.loc[:, 'city'] = test['zipcode'].map(zip_dict)\ntrain_set","48f6d81b":"def fix_dates(string):\n    if len(string) == 1:\n        return '200'+string\n    if len(string) == 2:\n        return '19'+string\n    if len(string) == 4:\n        return string\n\ntrain_set.loc[:, 'registration_year'] = train_set['registration_year'].astype('str')    \ntrain_set.loc[:, 'registration_year'] = train_set['registration_year'].apply(fix_dates)\n\ntest.loc[:, 'registration_year'] = test['registration_year'].astype('str')    \ntest.loc[:, 'registration_year'] = test['registration_year'].apply(fix_dates)\ntrain_set","4d8d1fc4":"def replace_numeric_nans(df, strategy='median', constant=None):\n    for col in df.columns:\n        if df[col].isna().sum() > 0 and df[col].dtype != 'object':\n            if strategy == 'median':\n                median = df[col].median()\n                df.loc[:, col] = df[col].fillna(median)\n            if constant is not None and strategy == 'constant':\n                df.loc[:, col] = df[col].fillna(constant)\n    if strategy == 'median':\n        return (df, median)\n    else:\n        return (df, constant)\n\ndef replace_str_nans(df, strategy='unknown'):\n    for col in df.columns:\n        if df[col].isna().sum() > 0 and df[col].dtype == 'object':\n            if strategy == 'unknown':\n                #mode = df[col].mode()[0]\n                #print(mode)\n                df.loc[:, col] = df[col].fillna('unknown')\n    return df    \n\ntrain_set, med = replace_numeric_nans(train_set, 'median')\ntrain_set = replace_str_nans(train_set)\n\ntest, _ = replace_numeric_nans(test, 'constant', med)\ntest = replace_str_nans(test)\ntrain_set","4a30df8f":"from category_encoders import OneHotEncoder\n\ndef cat_encoder(df, columns, target=None, test=False, encoder=None):\n    if test:\n        return encoder.transform(df)\n    else:\n        encoder = OneHotEncoder(cols=columns, return_df=True)\n        encoder.fit(df, target)\n        df = encoder.transform(df)\n        return (df, encoder)\n\ncat_cols = ['type', 'registration_year', 'gearbox', 'model', 'fuel', 'brand']\nX = train_set.drop(['Unnamed: 0', 'zipcode', 'price', 'city'], axis=1)\ny = train_set['price']\ntest = test.drop(['Unnamed: 0', 'zipcode', 'city'], axis=1)\n\nX, encoder = cat_encoder(X, cat_cols, y, False, None)\ntest = cat_encoder(test, cat_cols, None, True, encoder)\nX","ac375ec8":"\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nfrom ignite.contrib.metrics.regression import MedianAbsolutePercentageError\n\nclass CarsDataset(Dataset):\n    def __init__(self, df, test=False):\n        super(CarsDataset, self).__init__()\n        self.test = test\n        if test:\n            self.df = df\n        else:\n            self.df = df.drop('price', axis=1)\n            self.target = df['price']\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, ind):\n        if self.test:\n            return self.df.values[ind]\n        else:\n            return (self.df.values[ind], self.target.values[ind])\n        \n\nclass CarsModel(nn.Module):\n    def __init__(self):\n        super(CarsModel, self).__init__()\n        self.lin1 = nn.Linear(385, 256)\n        self.norm1 = nn.BatchNorm1d(256)\n        self.lin2 = nn.Linear(256, 64)\n        self.lin3 = nn.Linear(64, 1)\n        #self.output = nn.Linear()\n        \n    def forward(self, x):\n        #x = x.view(-1, 12)\n        x = F.relu(self.lin1(x.float()))\n        x = self.norm1(x)\n        x = F.relu(self.lin2(x))\n        x = F.relu(self.lin3(x))\n        return x\n    \ndataset_train = CarsDataset(pd.concat([X, y], axis=1))\ndataset_test = CarsDataset(test, test=True)\n\ndataloader_train = DataLoader(dataset_train, batch_size=16, shuffle=True)\ndataloader_test = DataLoader(dataset_test, batch_size=16)\n#print(dataloader_train.dataset.df)\n#print(dataloader_test.dataset.df)\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    #y_true, y_pred = np.array(y_true), np.array(y_pred)\n    y_true, y_pred = y_true.detach().numpy(), y_pred.detach().numpy()\n    return torch.Tensor(np.mean(np.abs((y_true - y_pred) \/ y_true)), requires_grad=True)\n\ndef fit():\n    model = CarsModel()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.MSELoss()\n    epochs = 1\n    \n    \n    for epoch in range(epochs):\n        total_loss = 0\n        for features, target in tqdm(dataloader_train):\n            optimizer.zero_grad()\n            # = each\n            y = model(features)\n            loss = criterion(y.float(), target.view(-1, 1).float())\n            #print(loss.__dir__())\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print('epoch:', epoch, 'loss:', total_loss)\n    return model\n\nmodel = fit()\n\nmodel.eval()\n\npreds = []\nfor features in tqdm(dataloader_test):\n    #print(features)\n    y = model(features)\n    preds.append(y[0].detach().numpy())\nprint(preds)\n\"\"\"","f797750a":"from sklearn.model_selection import KFold, RepeatedKFold\nfrom sklearn.linear_model import LinearRegression, Ridge, SGDRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n\nkf = RepeatedKFold(n_splits=5, n_repeats=2)\n\nmodel_1 = GradientBoostingRegressor(n_estimators=200, max_depth=20, verbose=1)\nmodel_2 = RandomForestRegressor(n_estimators=100, verbose=2)\n\nfor train_index, test_index in kf.split(train_set):\n    print('Fitting Fold...')\n    \n    X_train, X_test = X.values[train_index], X.values[test_index]\n    y_train, y_test = y.values[train_index], y.values[test_index]\n    model_1.fit(X_train, y_train)\n    print(model_1.score(X_test, y_test))\n\npreds = model_1.predict(test.values)\npreds","c1cd555a":"submission.loc[:, 'Predicted'] = preds\nsubmission.to_csv('submission.csv', index=False)","ed768b8d":"As my experience show it is not the best way to replace categorical values with mode so I replaced it with another category ``unknown``. Numerical ones I replaced with median just not to create a scew.","2ea7710a":"That is how cities were mapped. Disclaimer: the column city is dropped eventually","b76c2d5b":"I also wanted to train a pytorch model with given data but I figured out there is not much time for it and did not do it.","ce93425a":"That is all I were able to come up with. Even though my results are not great I am grateful for opportunity to participate, practise. Moreover, with my tight schedule I practised also to make decisions fast :)","f1b14e21":"At first my idea was there is some spacial relationship, hence I tried to map zipcodes to cities given in another file.\nSeveral problems have arisen:\n* several town can have one zip code and vice versa\n* since (1) the geographical information about precise locationwould be meaningless without knowing it precisely. In a short timeframe I had I couldn't make it work as expected\n* the attempt to map city to zipcodes actually made the results far worse and I ended up not using it at all","4b78a7fe":"## Disclaimer:\n### My solution scored low, take each step of this notebook with a grain of salt\nP.S. I worked on weekend so did not have a time to come up with something better :)","ea688b08":"Even though dataset would become sparse I decided to OneHot encode categorical features","f6900418":"The ``registration_year`` column contained really diverse values (1999, 2009, 65, 9, etc). I figured out 200x would be in the place of single digit (not sure about 0 but ignored it with a little sceptisism), 19xx would be for two digits and relplaced them.","3b3989d1":"The faster way to get at least some results in short timeframe would do with basic models. Linear, Ridge did not work although something meaningful they showed with polynomial features with second degree. Still, the pick-and-go algorythms for me usually are Random Forests and Gradient Boosting (the one I used for solution)"}}