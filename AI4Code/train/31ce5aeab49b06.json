{"cell_type":{"20907145":"code","70440172":"code","54e253df":"code","60191a77":"code","bc776218":"code","35f98819":"code","71da2932":"code","67133e4e":"code","6404dd93":"code","cf3b1f57":"code","17e40744":"code","f010d41d":"code","f4626e4d":"code","ba0f24e4":"code","c8f93a60":"code","19429dd0":"code","2b891ff9":"code","2fae34b5":"code","80f7cbe5":"code","04df8ffc":"code","72b699b6":"code","91a0a9e2":"code","9960c9cd":"code","ab094174":"code","41aafc73":"code","b96ef95b":"markdown","131d807d":"markdown","0698d0d1":"markdown","f2daf1e0":"markdown","fad4eda7":"markdown","2e2e0caf":"markdown","c5d4b87e":"markdown"},"source":{"20907145":"import sys\nimport numpy as np\nimport pandas as pd\nimport gc\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Dropout\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nimport nltk","70440172":"data = pd.read_csv('..\/input\/all-trumps-twitter-insults-20152021\/trump_insult_tweets_2014_to_2021.csv')\n#data.head()\ndata.shape","54e253df":"text = ''\nfor i in range(10360):\n    text += data.tweet[i]\n    text += '\\n'","60191a77":"text = text.lower()","bc776218":"words = set(nltk.corpus.words.words())\ntext = \" \".join(w for w in nltk.wordpunct_tokenize(text) \\\n         if w.lower() in words or not w.isalpha())","35f98819":"del data\ngc.collect()","71da2932":"import re\ntext = re.sub(r'\\W+', ' ', text)","67133e4e":"vocab = sorted(list(set(text)))\nlen(vocab)","6404dd93":"char2id = dict((c, i) for i, c in enumerate(vocab))\n\ninput_len = len(text)\nvocab_len = len(vocab)\nprint (\"Total number of characters:\", input_len)\nprint (\"Total vocab:\", vocab_len)","cf3b1f57":"seq_length = 100\nX_data = []\ny_data = []","17e40744":"for i in range(0, input_len - seq_length, 1):\n    in_seq = text[i:i + seq_length]\n    out_seq = text[i + seq_length]\n    X_data.append([char2id[char] for char in in_seq])\n    y_data.append(char2id[out_seq])","f010d41d":"n_patterns = len(X_data)\nprint (\"Total Patterns:\", n_patterns)","f4626e4d":"X = np.reshape(X_data, (n_patterns, seq_length, 1))\nX = X\/float(vocab_len)","ba0f24e4":"from keras.utils import np_utils\n\ny = np_utils.to_categorical(y_data)","c8f93a60":"gc.collect()","19429dd0":"training = False\n\ndef create_model():\n    model = Sequential()\n    #model.add(Embedding(vocab_len,64))\n    model.add(LSTM (256,return_sequences=True))\n    model.add(Dropout(0.25))\n    model.add(LSTM (256))\n    #model.add(Dropout(0.25))\n    #model.add(Dense(64,activation='relu'))\n    model.add(Dense(y.shape[1], activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    return model","2b891ff9":"import gc\ngc.collect()","2fae34b5":"import pickle\n\n\nif training:\n    model = create_model()\n    model.fit(X, y, epochs=50, batch_size=1024)\n    model.save('trump_tweet.h5')\n    '''Pkl_Filename = \"Tweeter.pkl\"  \n\n    with open(Pkl_Filename, 'wb') as file:  \n        pickle.dump(model, file)'''\nelse:\n    from tensorflow import keras\n    model = create_model()\n    model.built = True\n    model.load_weights('..\/input\/trump-tweet-generation\/trump_tweet.h5')","80f7cbe5":"model.summary()","04df8ffc":"id2char = dict((i, c) for i, c in enumerate(vocab))","72b699b6":"start = np.random.randint(0, len(X_data) - 1)\npattern = X_data[start]\nprint(\"Random Data Seed:\")\nprint(\"\\\"\", ''.join([id2char[value] for value in pattern]), \"\\\"\")","91a0a9e2":"for i in range(250):\n    x = np.reshape(pattern, (1, len(pattern), 1))\n    x = x \/ float(vocab_len)\n    prediction = model.predict(x, verbose=0)\n    index = np.argmax(prediction)\n    result = id2char[index]\n    seq_in = [id2char[value] for value in pattern]\n\n    sys.stdout.write(result)\n\n    pattern.append(index)\n    pattern = pattern[1:len(pattern)]","9960c9cd":"tweet_2012 = 'it makes me feel so good to hit sleazebags back much better than seeing a psychiatrist which i never have'","ab094174":"new_pattern = [char2id[value] for value in tweet_2012]","41aafc73":"for i in range(250):\n    x = np.reshape(new_pattern, (1, len(new_pattern), 1))\n    x = x \/ float(vocab_len)\n    prediction = model.predict(x, verbose=0)\n    index = np.argmax(prediction)\n    result = id2char[index]\n    seq_in = [id2char[value] for value in pattern]\n\n    sys.stdout.write(result)\n\n    new_pattern.append(index)\n    new_pattern = new_pattern[1:len(pattern)]","b96ef95b":"**Let's use a tweet from Trump's account from 2012.**","131d807d":"**After training the model let's try to check it's performance by doing some text generation. Before generating some text we need to convert generated numbers back to readable character form.**","0698d0d1":"**Next we create a dictionary to map these characters to numbers that can be fed to model.**","f2daf1e0":"**A fun project to train a model over tweets of Donald Trump and try to create new tweets.\nAs a start, I will use simple LSTM model and try to choose better hyperparameters and improve performance over the time. As a conclusion for this small project will try to use Transformers to generate new tweet.**\n\n![trump.jpg](attachment:trump.jpg)","fad4eda7":"**Applying regex operation to remove some irrelevant and less frequent character occurances. Then we create a vocabulary to store unique character that we will be feeding to our model.**","2e2e0caf":"**Firstly let's start with random text from our data that we trained upon.**","c5d4b87e":"**I will be using only the tweet column from the dataset. Then will convert all the text to lowercase, as it improves performance and also it is fast to train with less values to predict.**"}}