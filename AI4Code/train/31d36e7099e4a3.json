{"cell_type":{"5024cc74":"code","467316f9":"code","44ea8eaf":"code","21dec7e6":"code","9962f7e5":"code","8bf7404b":"code","de2281d6":"code","969fdec1":"code","dc0929a3":"code","5664c7e4":"code","8742bc6b":"code","86e7884f":"code","ff0f65e3":"code","cdca8714":"code","85e216a1":"code","5ace7037":"code","a636e929":"code","588f976e":"code","5cff35b7":"code","0977f266":"code","563e5dbb":"code","ea730921":"code","2bef216f":"code","18c56f5a":"code","93f877eb":"code","0d939942":"code","588495d6":"code","0841a83b":"code","b2686dd2":"code","c3ca04c5":"code","c05ffe55":"code","b2b7fc27":"code","ce52bddc":"code","7cc223de":"markdown","28f87733":"markdown","1f37a303":"markdown","0b798650":"markdown","bc92a48b":"markdown","fe171485":"markdown","afc8de5a":"markdown"},"source":{"5024cc74":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')","467316f9":"df_train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ndf_train.head()","44ea8eaf":"df_test = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\ndf_test.head()","21dec7e6":"samp_sub = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nsamp_sub.head()","9962f7e5":"print(df_train.shape)\nprint(df_test.shape)\nprint(samp_sub.shape)","8bf7404b":"df_train.info()","de2281d6":"df_train.describe()","969fdec1":"sns.countplot(x=df_train.claim,palette='Set2')","dc0929a3":"print(df_train.isnull().sum())","5664c7e4":"# Fork of https:\/\/www.kaggle.com\/mrigendraagrawal\/tps-sep-eda-and-starter?scriptVersionId=73721669&cellId=21\n\nmissing_values = pd.DataFrame(df_train.isna().sum())\nmissing_values.rename(columns={0:'missing_value'},inplace=True)\ndef train_missing_perecentage(idx):\n    return (idx\/len(df_train))*100\nmissing_values['missing_value'] = missing_values.apply(train_missing_perecentage)\nfeatures = list(df_train.columns)\npercentage = []\nfor i in features:\n    percentage.append(float(missing_values.loc[str(i)]))\nmissing_values = pd.DataFrame({'Feature':features,'Percentage':percentage})","8742bc6b":"import plotly.express as px\npx.scatter(data_frame=missing_values,x='Feature',y='Percentage',template='plotly_dark')","86e7884f":"X = df_train.drop(['id', 'claim'], axis = 1)\nY = df_train.claim","ff0f65e3":"import optuna","cdca8714":"import lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score","85e216a1":"def objective(trial,data=X,target=Y):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.2, random_state=42)\n    param = {\n        'random_state': 42,\n        'n_estimators': 500,\n        'task': 'train',\n        'objective': 'binary',\n        'metric':'binary_logloss',\n        'reg_alpha': trial.suggest_categorical('reg_alpha', [1,10.0]),\n        'reg_lambda': trial.suggest_categorical('reg_lambda', [1e-1,1e-2]),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.4,0.6,0.8]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.6,0.8]),\n        'subsample_freq': trial.suggest_categorical('subsample_freq', [1,2]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [5e-3,2e-2]),\n        'max_depth': -1,\n        'num_leaves' : trial.suggest_categorical('num_leaves', [128,512]),\n        'min_child_weight' : trial.suggest_categorical('min_child_weight', [128,256]),\n        'min_child_samples': trial.suggest_categorical('min_child_samples', [20,100]),\n        'importance_type': 'gain'\n    }\n    model = lgb.LGBMRegressor(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=500)\n    \n    preds = model.predict(test_x)\n    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","5ace7037":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=20)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","a636e929":"study.trials_dataframe()","588f976e":"optuna.visualization.plot_optimization_history(study)","5cff35b7":"optuna.visualization.plot_parallel_coordinate(study)","0977f266":"optuna.visualization.plot_slice(study)","563e5dbb":"#Visualize parameter importances.\noptuna.visualization.plot_param_importances(study)","ea730921":"optuna.visualization.plot_edf(study)","2bef216f":"params = {\n        'random_state': 42,\n        'n_estimators': 1000,\n        'task': 'train',\n        'objective': 'binary',\n        'metric':'binary_logloss',\n        'reg_alpha': 1, \n        'reg_lambda': 0.1, \n        'colsample_bytree': 0.8, \n        'subsample': 0.8, \n        'subsample_freq': 2, \n        'learning_rate': 0.02, \n        'num_leaves': 512, \n        'min_child_weight': 128, \n        'min_child_samples': 100,\n        'max_depth': -1,\n        'importance_type': 'gain'\n    }","18c56f5a":"folds = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, valid_idx) in enumerate(folds.split(df_train)):\n    print(f'fold {fold} starting...')\n    fold_train = df_train.iloc[train_idx]\n    train_x = fold_train.drop(['id', 'claim'], axis = 1)\n    train_y = fold_train.claim\n    dtrain = lgb.Dataset(train_x,label=train_y)\n    \n    fold_valid = df_train.iloc[valid_idx]\n    valid_x = fold_valid.drop(['id', 'claim'], axis = 1)\n    valid_y = fold_valid.claim\n    dvalid = lgb.Dataset(valid_x,valid_y)\n    \n    model = lgb.train(params,\n                    train_set=dtrain, \n                    valid_sets=dvalid,\n                    early_stopping_rounds=100,\n                    verbose_eval=1000\n                    )\n    \n    oof = model.predict(valid_x)\n    score = roc_auc_score(valid_y,oof)\n    print(f\"Valid score for {fold} is: {score}\")\n    oof = pd.DataFrame({'id':valid_x.index,'claim':oof})\n    oof.to_csv(f'{fold}_oof.csv',index=False)\n    model.save_model(f'lightgbm_{fold}.txt')\n    print(f' fold {fold} completed')","93f877eb":"from tqdm import tqdm","0d939942":"df_test = df_test.drop(['id'], axis = 1)","588495d6":"for fold in tqdm(range(5)):\n    model = lgb.Booster(model_file=f'.\/lightgbm_{fold}.txt')\n    preds = model.predict(df_test)\n    submission = samp_sub.copy()\n    submission['claim'] = preds\n    submission.to_csv(f'submission_{fold}.csv',index=False)","0841a83b":"sub0 = pd.read_csv('.\/submission_0.csv')\nsub1 = pd.read_csv('.\/submission_1.csv')\nsub2 = pd.read_csv('.\/submission_2.csv')\nsub3 = pd.read_csv('.\/submission_3.csv')\nsub4 = pd.read_csv('.\/submission_4.csv')","b2686dd2":"import plotly.figure_factory as ff\nimport plotly.express as px","c3ca04c5":"hist_data = [sub0.claim, sub1.claim, sub2.claim, sub3.claim, sub4.claim]\n\ngroup_labels = ['sub0', 'sub1', 'sub2', 'sub3', 'sub4']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2, show_hist=False, show_rug=False)\nfig.show()","c05ffe55":"data=np.corrcoef([sub0.claim, sub1.claim, sub2.claim, sub3.claim, sub4.claim])\nfig = px.imshow(data,\n                x=group_labels,\n                y=group_labels\n               )\nfig.show()","b2b7fc27":"sub = sub1.copy()\nsub.loc[:, 'claim'] = (0.2 * sub0 + 0.2 * sub1 + 0.2 * sub2 + 0.2 * sub3 + 0.2 * sub4 )\nsub.to_csv('submission.csv', index=False)","ce52bddc":"sub.head()","7cc223de":"# Modeling","28f87733":"# Data","1f37a303":"# Inference","0b798650":"# Missing values","bc92a48b":"# Let's build our optimization function using optuna\n\nFork of https:\/\/www.kaggle.com\/hamzaghanmi\/lgbm-hyperparameter-tuning-using-optuna?scriptVersionId=53513594&cellId=14","fe171485":"# Training \n## (Let's Create an LGBMRegressor model with the best hyperparameters)\n'reg_alpha': 1, 'reg_lambda': 0.1, 'colsample_bytree': 0.8, 'subsample': 0.8, 'subsample_freq': 2, 'learning_rate': 0.02, 'num_leaves': 512, 'min_child_weight': 128, 'min_child_samples': 100","afc8de5a":"# Visualization for Hyperparameter Optimization Analysis"}}