{"cell_type":{"59e79bec":"code","aadcbe28":"code","15da3190":"code","4e439365":"code","a1215683":"code","52238004":"code","7c8cca55":"code","a22efbf7":"code","f215e0ef":"code","b5acde02":"code","b462a6fe":"code","ac260004":"code","c8fdd3db":"code","762d7997":"code","63f7adcd":"code","e48d615a":"code","47730609":"code","dee669ea":"code","00109d8f":"code","a094c9fa":"code","7be65b54":"code","a6560c40":"code","9e0ecb03":"code","7f0a0162":"code","e1b95b1f":"code","23bdfed4":"code","3eaf1ef0":"code","3a26b977":"code","f1ce500b":"code","fa80f2e0":"markdown","1756a44d":"markdown","653ee870":"markdown","85eba4d4":"markdown","8a527aa3":"markdown","1e373798":"markdown","c4f018a4":"markdown","558008bc":"markdown","adfa7601":"markdown","94363202":"markdown","e111f580":"markdown","3f70ecb2":"markdown","7d5a3724":"markdown","23253fdd":"markdown","3771ceb7":"markdown"},"source":{"59e79bec":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns","aadcbe28":"#Load Data\ndata = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndata.head()","15da3190":"sns.pairplot(data, hue='Outcome')","4e439365":"#First lets split the data\nX = data.iloc[:, :-1].values\ny = data.iloc[:, 8].values\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","a1215683":"#Checking for nulls\ndata.isnull()\ndata.isnull().sum()\ndata.eq(0).any().any()","52238004":"#Imputation\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(missing_values=0, strategy='mean', axis=0)\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.fit_transform(X_test)","7c8cca55":"from sklearn.preprocessing import StandardScaler\nstd = StandardScaler()\nX_train = std.fit_transform(X_train)\nX_test = std.transform(X_test)","a22efbf7":"def plot_corr(data, size):\n    corr = data.corr()\n    fig, ax = plt.subplots(figsize=(size,size))\n    ax.matshow(corr)\n    plt.xticks(range(len(corr.columns)), corr.columns)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n\nplot_corr(data, data.shape[1])\ndata.corr()","f215e0ef":"sns.heatmap(data.corr(), annot=True)","b5acde02":"#Fitting model\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\nclassifier.fit(X_train, y_train)","b462a6fe":"#Predicting test result\ny_pred = classifier.predict(X_test)\n\n#Predicting training result\ny_pred_train = classifier.predict(X_train)","ac260004":"#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ntrue_pred = cm[1][1] + cm[0][0]\nneg_pred = cm[1][0] + cm[0][1]\nprint(cm)\nprint('True prediction: ', true_pred, ' False prediction: ', neg_pred)\nprint('Accuracy of true prediction: ', (true_pred\/X_test.shape[0])*100, '%')\nprint('Accuracy of false prediction: ', (neg_pred\/X_test.shape[0])*100, '%')","c8fdd3db":"#Performance on training data\nfrom sklearn import metrics\nprint (\"Accuracy: ({0:.4f})\".format(metrics.accuracy_score(y_train, y_pred_train)))\n\n#Performance on testing data\nfrom sklearn import metrics\nprint (\"Accuracy: ({0:.4f})\".format(metrics.accuracy_score(y_test, y_pred)))","762d7997":"#Fitting model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\nknn.fit(X_train, y_train)","63f7adcd":"#Predicting test result\ny_pred_knn = knn.predict(X_test)\n\n#Predicting training result\ny_pred_train_knn = knn.predict(X_train)","e48d615a":"#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred_knn)\ntrue_pred = cm[1][1] + cm[0][0]\nneg_pred = cm[1][0] + cm[0][1]\nprint(cm)\nprint('True prediction: ', true_pred, ' False prediction: ', neg_pred)\nprint('Accuracy of true prediction: ', (true_pred\/X_test.shape[0])*100, '%')\nprint('Accuracy of false prediction: ', (neg_pred\/X_test.shape[0])*100, '%')","47730609":"#Performance on training data\nfrom sklearn import metrics\nprint (\"Accuracy: ({0:.4f})\".format(metrics.accuracy_score(y_train, y_pred_train_knn)))\n\n#Performance on testing data\nfrom sklearn import metrics\nprint (\"Accuracy: ({0:.4f})\".format(metrics.accuracy_score(y_test, y_pred_knn)))","dee669ea":"#Fitting model\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)","00109d8f":"#Predicting test result\ny_pred_tree = dt.predict(X_test)\n\n#Predicting training result\ny_pred_train_tree = dt.predict(X_train)","a094c9fa":"#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred_tree)\ntrue_pred = cm[1][1] + cm[0][0]\nneg_pred = cm[1][0] + cm[0][1]\nprint(cm)\nprint('True prediction: ', true_pred, ' False prediction: ', neg_pred)\nprint('Accuracy of true prediction: ', (true_pred\/X_test.shape[0])*100, '%')\nprint('Accuracy of false prediction: ', (neg_pred\/X_test.shape[0])*100, '%')","7be65b54":"#Display Tree\nfrom sklearn.tree import export_graphviz\nfrom sklearn.externals.six import StringIO \nfrom IPython.display import Image \nfrom pydot import graph_from_dot_data\nfeature_names = data.columns[:-1]\ndot_data = StringIO()\nexport_graphviz(dt, out_file=dot_data, feature_names=feature_names)\n(graph, ) = graph_from_dot_data(dot_data.getvalue())\nImage(graph.create_png())","a6560c40":"#Fitting model\nfrom sklearn.naive_bayes import GaussianNB   \nNB = GaussianNB()  \nNB.fit(X_train, y_train)   ","9e0ecb03":"#Predicting test result\ny_pred_nb = NB.predict(X_test)\n\n#Predicting training result\ny_pred_train_nb = NB.predict(X_train)","7f0a0162":"#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred_nb)\ntrue_pred = cm[1][1] + cm[0][0]\nneg_pred = cm[1][0] + cm[0][1]\nprint(cm)\nprint('True prediction: ', true_pred, ' False prediction: ', neg_pred)\nprint('Accuracy of true prediction: ', (true_pred\/X_test.shape[0])*100, '%')\nprint('Accuracy of false prediction: ', (neg_pred\/X_test.shape[0])*100, '%')","e1b95b1f":"#Performance on training data\nfrom sklearn import metrics\nprint (\"Accuracy: ({0:.4f})\".format(metrics.accuracy_score(y_train, y_pred_train_nb)))\n\n#Performance on testing data\nfrom sklearn import metrics\nprint (\"Accuracy: ({0:.4f})\".format(metrics.accuracy_score(y_test, y_pred_nb)))","23bdfed4":"from keras import Sequential\nfrom keras.layers import Dense\n\nclassifierNN = Sequential()\n\n#output = activation(dot(input, kernel) + bias)\n#kernel is the weight matrix. kernel initialization defines the way to set the initial random weights of Keras layers.\n#Random normal initializer generates tensors with a normal distribution.\n\n#First Hidden Layer\nclassifierNN.add(Dense(8, activation='relu', kernel_initializer='random_normal', input_dim=8))\n\n#Second Hidden Layer\nclassifierNN.add(Dense(8, activation='relu', kernel_initializer='random_normal'))\n\n#Output Layer\nclassifierNN.add(Dense(1, activation='sigmoid',  kernel_initializer='random_normal'))\n\n#Compiling the neural network\n#We use binary_crossentropy to calculate the loss function between the actual output and the predicted output.\n#Adam stands for Adaptive moment estimation. Adam is a combination of RMSProp + Momentum.\n#Momentum takes the past gradients into account in order to smooth out the gradient descent.\n#We use accuracy as the metrics to measure the performance of the model\nclassifierNN.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n\n\n#Fitting the data to the training dataset\n#We use a batch_size of 10. This implies that we use 10 samples per gradient update.3\n#We iterate over 100 epochs to train the model. An epoch is an iteration over the entire data set.\nclassifierNN.fit(X_train,y_train, batch_size=10, epochs=100)","3eaf1ef0":"eval_model=classifierNN.evaluate(X_train, y_train)\neval_model","3a26b977":"y_pred_NN=classifierNN.predict(X_test)\ny_pred_NN =(y_pred_NN>0.5)","f1ce500b":"#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred_NN)\ntrue_pred = cm[1][1] + cm[0][0]\nneg_pred = cm[1][0] + cm[0][1]\nprint(cm)\nprint('True prediction: ', true_pred, ' False prediction: ', neg_pred)\nprint('Accuracy of true prediction: ', (true_pred\/X_test.shape[0])*100, '%')\nprint('Accuracy of false prediction: ', (neg_pred\/X_test.shape[0])*100, '%')","fa80f2e0":"<div style=\"font-size:20px\">\n    **2.2 KNN**\n<\/div>\n* KNN is a non-parametric, lazy learning algorithm. \n* Needs high memory requirement\n* Sensitive to irrelevant features","1756a44d":"<div style=\"font-size:25px\">\n    **1.4. Multicollinearity and its impact**\n <\/div>\n    <br>\nMulticollinearity occurs in our dataset when we have features which are strongly dependent on each other.    \n* The main impact it will have is that it can cause the decision boundary to change which can have a huge impact on the result of our model.\n* In addition to that if we have multicollinearity in our dataset then we won\u2019t be able to use our weight vector to calculate the feature importance.","653ee870":"<div style=\"font-size:20px\">\n    **2.3 Decision Tree**\n<\/div>\n* In order to determine which of the three splits is better, we use impurity.\n* We ultimately decide on the split with the largest information gain.","85eba4d4":"If we look at the previous outputs, we can easily identify if there is unrelated patterns between columns.\nIn our case the data seem related","8a527aa3":"<div style=\"font-size:20px\">\n    **2.5 Neural Network**\n<\/div>\n* We will use keras to build our neural network.\n* There are two main types of models available in keras \u2014 Sequential and Model. we will use Sequential model to build our neural network.\n* We use Dense library to build input, hidden and output layers of a neural network.\n* We have 8 input features and one target variable. 2 Hidden layers. Each hidden layer will have 4 nodes.\n* ReLu will be the activation function for hidden layers. As this is a binary classification problem we will use sigmoid as the activation function.","1e373798":"It seems there is no null data in this dataset. But there is zeros\n\nIn such a case where there is nulls, the easiest way to solve this problem, is by dropping the rows or columns that contain null values (dataset.dropna())\n\nHowever it is not the best option, it can lead to loss of valuable information.\nSo we can use **Imputation**\nThis is simply the process of substituting the missing values of our dataset.","c4f018a4":"<div style=\"font-size:20px\">\n    **2.4 Naive Bayes**\n<\/div>\n* Naive Bayes Classifier is based on the Bayes\u2019 Theorem of conditional probability","558008bc":"<div style=\"font-size:25px\">\n    **1.2 Standarization**\n <\/div>\n    <br>\nWe transform our values such that the **mean** of the values is 0 and **standard deviation** is 1.\n\nRemeber the table above the head of data, we can see that for example, we have 2 numerical values **Pregnancies** and **Glucose**. They are not on the same scale and it will always be that glucose is greater than pregnancies, therefore the model will give more weightage to glucose which is not the ideal case as pregnancies is also a factor here. So to avoid that we perform standarization","adfa7601":"<div style=\"font-size:25px\">\n    ****Refrences****\n<\/div>\n<br>\n* Data preprocessing: https:\/\/towardsdatascience.com\/introduction-to-data-preprocessing-in-machine-learning-a9fa83a5dc9d\n* Logistic regression: https:\/\/towardsdatascience.com\/logistic-regression-python-7c451928efee\n* Testing model accuracy: https:\/\/medium.com\/@sachinkmr375\/testing-the-models-accuracy-in-ml-8385ee944e3f\n* Data preprocessing: https:\/\/medium.com\/@sachinkmr375\/selecting-an-algorithm-for-processing-training-data-e6114f91f6c5\n* KNN: https:\/\/towardsdatascience.com\/k-nearest-neighbor-python-2fccc47d2a55\n* Decision Tree: https:\/\/towardsdatascience.com\/decision-tree-in-python-b433ae57fb93\n* Naive Bayes: https:\/\/medium.com\/dataseries\/lets-build-your-first-naive-bayes-classifier-with-python-d31a5140e4bc\n* Neural Network: https:\/\/medium.com\/datadriveninvestor\/building-neural-network-using-keras-for-classification-3a3656c726c1","94363202":"<div style=\"font-size:25px\">\n    **1.3 Handling Categorical Variables**\n <\/div>\n    <br>\n    \nCategorical variables are that contain discrete values, like color of an item. They are divided into 2 types\n* **Ordnial categorical variables**  these are variables that can be orderd like size of a T-shirt \n* **Nominal categorical variables**  these are variables can't be orderd like color\n\nSo here we need to prerpocess them differently\n\nFor Ordinal we use Label Encoder from sklearn\nFor nominal we use One-Hot Encoding\n\nWe will not be doing it on this dataset","e111f580":"<div style=\"font-size:30px\">\n**1. Data Preprocessing**\n<\/div>\n<br>\nWe start by preprocessing the data and visualize it.\n\nFirst we import libraries and read it.","3f70ecb2":"<div style=\"font-size:30px\">\n    **2. Learning**\n <\/div>\n    <br>\n1. Logistic regression\n2. KNN\n3. Decision Tree\n4. Naive Bayes\n5. Neural Network","7d5a3724":"<div style=\"font-size:25px\">\n**1.1 Handling Nulls**\n<\/div>\n<br>\n\nDatasets can contain null or(zeros)  values, whatever is the kind of the problem, no model can handle this NULL.\n\nSo we start by checking which columns contains null values","23253fdd":"<div style=\"font-size:20px\">\n    **2.1 Logistic Regression**\n<\/div>\n* One of the limitations of Logistic Regression is the fact that it can only categorize data with two distinct classes. \n* When we train our model, we are in fact attempting to select the Sigmoid function whose shape best fits our data.","3771ceb7":"<div style=\"font-size:40px\">\n    ****Classification****\n    <\/div>\n    <br>\n\nThis notebook helps me get start, how to preproccess the data,\nhow to apply different algorithm, and test their accuracy, choose the best one.\n(Still undergo)\n\nExcuses for English mistakes.\n"}}