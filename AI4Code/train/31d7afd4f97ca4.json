{"cell_type":{"144355b5":"code","78782481":"code","73c64098":"code","e9f851ed":"code","dc99b7fa":"code","53bf4a76":"code","38584984":"code","29a72999":"code","1561e531":"code","58997255":"code","6721e9b5":"code","cb38b6fb":"code","38a212e6":"code","13f97e52":"code","ca76b076":"code","99503bc9":"code","6527582d":"code","0f23a950":"code","739d6f4a":"code","f3227416":"code","bebc3fef":"code","70369401":"code","580e5495":"code","70cd1132":"code","ca5a3c15":"code","028c6d9f":"code","9fd5ce12":"code","f8cbc599":"code","aa7ee855":"code","5b8808eb":"code","29716d4a":"code","81537dcd":"code","a61fc099":"code","4407faf6":"code","553a1a32":"code","1f780b53":"code","26caa30f":"code","863bc6ad":"code","f55ab8aa":"code","c95eabec":"code","55b78e25":"code","bcbccd4a":"code","b821c0c3":"code","63a78f1a":"code","85cc767c":"code","7738f50d":"code","bd93578c":"code","a18661a6":"code","73da6dcb":"code","da04d042":"code","bd666387":"code","cc4f2a35":"code","9cb20700":"code","44f11b8d":"code","bcd1574c":"code","4b02b9fb":"code","83c32008":"code","a580464d":"code","7dbf41df":"code","c74069c0":"code","773f417d":"code","f5627433":"code","ae25d0d6":"code","72890093":"code","5b458892":"code","52211cd3":"code","844b3e17":"code","ef9842de":"code","c400a8e3":"code","70091224":"code","5fefaba2":"code","4ff508ea":"code","cc494a31":"code","5ddfb48b":"code","06e02926":"code","c84c10f3":"code","412ec162":"code","c298dbcc":"code","5c326bb2":"code","10250a6e":"code","712e3f3e":"code","fe3809bc":"code","dd2d2327":"code","83d331a9":"code","77663fb2":"code","384d959f":"code","a03f291e":"code","091d805d":"code","de722ff7":"code","b2861dea":"code","202e28c0":"code","83144393":"code","51356aaf":"code","9288fdaf":"code","81faca63":"code","4d180903":"code","911bddb2":"code","f712aefc":"code","bc0d4ab9":"code","2367b72e":"code","860517fc":"markdown","f5624595":"markdown","1804dd32":"markdown","6661b75a":"markdown","6bd42e68":"markdown","d12e79bf":"markdown","5cafab9a":"markdown","381f1e5e":"markdown","2228ce2f":"markdown","744afd6f":"markdown","d368f902":"markdown","a23cf40a":"markdown","d2a4dc6c":"markdown","a80c032d":"markdown","d82e8605":"markdown","3e1032ad":"markdown","8c54c00f":"markdown","2c8ad2b7":"markdown","8f43a1dc":"markdown","1ce9561f":"markdown","dcf313e9":"markdown","0114407c":"markdown","607a4ae6":"markdown","2a82ee0f":"markdown","b90b88b6":"markdown","77e143dd":"markdown","3e13351c":"markdown","9ba8511c":"markdown","9d6cc8d6":"markdown","20383f6d":"markdown","e5ecf92d":"markdown","05852df6":"markdown","b67c7308":"markdown","595c1181":"markdown","dc6e7e31":"markdown","a7afe93d":"markdown","2a6e6c7f":"markdown","1c3dedeb":"markdown","7c865d44":"markdown","52146651":"markdown","d82f3616":"markdown","3799bed2":"markdown","4d10ba95":"markdown","6f3ea939":"markdown","db6a2cbc":"markdown","60c369f6":"markdown","8940fcf6":"markdown","c9890532":"markdown","1260a21b":"markdown","2aacf41c":"markdown","b7e2481b":"markdown","3c01f92e":"markdown","0dcd159a":"markdown","ac6f4e83":"markdown","21f0cbea":"markdown","9458eab9":"markdown","df9293b3":"markdown","74556fb3":"markdown","e1ca48ae":"markdown","4508ed9d":"markdown","5dec002b":"markdown","35ca4568":"markdown","dbae5282":"markdown","434cbd6b":"markdown","edd839fe":"markdown","5a716eef":"markdown","78eed392":"markdown","80a5f136":"markdown","32387f4e":"markdown","84cf07b4":"markdown","81665108":"markdown","18e5c589":"markdown","0dd11c8a":"markdown","a59eb355":"markdown","1588d231":"markdown","d74a2a80":"markdown","eb8aa230":"markdown","a6c5b1e2":"markdown","3da6520b":"markdown","bcfa65c5":"markdown","470b7c6a":"markdown","447fa6ab":"markdown","5e69be49":"markdown","b48091dd":"markdown","554f4881":"markdown","da71931e":"markdown","02b6b2f4":"markdown","76449126":"markdown","a938660e":"markdown","7cee60e1":"markdown","2997be54":"markdown","43a57e6c":"markdown","c6b51c4e":"markdown","cce5936f":"markdown","1757f087":"markdown"},"source":{"144355b5":"import re\nfrom pathlib import Path\nimport numpy as np \nimport pandas as pd\nimport pickle\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport matplotlib.pyplot as plt\n\nfrom skimage import io\n\n# tqdm is a library that enables you to visualize the progress of a for loop by displaying a configurable progress bar\nfrom tqdm.notebook import tqdm\ntqdm().pandas();\n\nprint('import complete')","78782481":"input_path = Path.cwd()\/'..\/input\/breast-histopathology-images'\nprint(input_path)","73c64098":"# Create List of Paths\npath_contents = [path for path in Path.iterdir(input_path)]\n# Print out the first 5 paths\npath_contents[0:5]","e9f851ed":"def get_files_folders(path,print_summary = True):\n    folders = []\n    files = []\n    for item in Path.iterdir(path):\n        if item.is_dir():\n            folders.append(item)\n        else:\n            files.append(item)\n    if print_summary:\n        print(f'There are {len(folders)} folders and {len(files)} files in folder {path.name}') \n    \n    return {'folders': folders, 'files': files, 'path': path} ","dc99b7fa":"sorted_paths = get_files_folders(input_path)","53bf4a76":"patient_1 = sorted_paths['folders'][0]\n\nfor p in Path.iterdir(patient_1):\n    print(p,'\\n')\n# In order to obtain the number of files and folders\nget_files_folders(patient_1);","38584984":"def verify_path_contents(input_path,n_folders,n_files,print_summary = True):\n    '''\n    loops through all the directories in a path and checks the number of folders and files in each directory\n    returns a dictionary containing a list of the matched paths and a list of the unmatched paths\n    '''\n    \n    matched_paths = []\n    unmatched_paths = []\n    \n    for path in Path.iterdir(input_path):\n        if path.is_dir():\n            folders = [sub_path for sub_path in Path.iterdir(path) if sub_path.is_dir()]\n            files = [sub_path for sub_path in Path.iterdir(path) if sub_path.is_file()]\n\n            if len(folders) != n_folders or len(files) != n_files:\n                unmatched_paths.append(path)\n            else:\n                matched_paths.append(path)\n        \n    if print_summary:\n        print(f'We have a total of {len(matched_paths)} matched paths, and a total of {len(unmatched_paths)} unmatched paths in {input_path.name}')\n        \n    return {'matched_paths':matched_paths,'unmatched_paths':unmatched_paths}","29a72999":"path_contents = verify_path_contents(input_path = input_path, n_folders = 2, n_files = 0)","1561e531":"# Let's check out our first directory\npatient_1_paths = get_files_folders(patient_1)","58997255":"p = {}\n# store the paths of each folder, by using it's name, in the dictionary above \nfor folder in patient_1_paths['folders']:\n    p[folder.name] = get_files_folders(folder)","6721e9b5":"# displaying the first 5 paths in each directory\ndisplay(p['0']['files'][0:5])\ndisplay(p['1']['files'][0:5])","cb38b6fb":"def extract_all_image_paths(input_path = Path.cwd()\/'..\/input\/breast-histopathology-images'):\n    image_paths = [image_path for image_path in Path.glob(input_path,pattern = '*\/*\/*.png')]\n    return image_paths","38a212e6":"image_paths = extract_all_image_paths()","13f97e52":"# retrieve the first 5 paths\nimage_paths[0:5]","ca76b076":"print(f'We have a total of {len(image_paths)} images.')","99503bc9":"# let's check that we correctly extracted our paths\ndisplay(image_paths[0:3])\ndisplay(image_paths[10000:10003])","6527582d":"# Extracts a dict of lists containing the informaion of each path\ndef extract_metadata(image_paths) -> dict:\n    path_data = {'path':[],'patient_id':[],'x_coord':[] ,'y_coord':[],'target':[]}\n    pattern = '\\\/(\\d+)_.+_x(\\d+)_y(\\d+)_.+(\\d)'\n    for image_path in tqdm(image_paths,total = 277524):\n        meta_data = re.search(pattern, str(image_path))\n        path_data['path'].append(image_path)\n        path_data['patient_id'].append(meta_data.group(1))\n        path_data['x_coord'].append(meta_data.group(2))\n        path_data['y_coord'].append(meta_data.group(3))\n        path_data['target'].append(meta_data.group(4))\n    return path_data","0f23a950":"# retrieve dict of path metadata\npath_data = extract_metadata(image_paths)","739d6f4a":"# extract and display the first 3 rows of each value\nfor value in path_data.values():\n    display(value[0:3]) ","f3227416":"# convert dictionary to pandas dataframe --> convert path_data dict to dataframe\ndf = pd.DataFrame.from_dict(path_data)","bebc3fef":"# prints the first five rows of the dataframe\ndf.head()","70369401":"# to get a better understanding of our dataframe we use the .info() method\ndf.info()","580e5495":"df.iloc[:,2:5]","70cd1132":"# loop through the desired columns and change the types\nfor col in df.iloc[:,2:5]:\n    df[col] = df[col].astype('int')","ca5a3c15":"df.info()","028c6d9f":"# Function below gets the the number of patches for each patient, \n# and gets the ratio of cancerous patches to non-cancerous patches for each patient\n\ndef get_image_count_and_cancer_ratios(path_df):\n    # Return the count of images per patient\n    s1 = path_df.groupby('patient_id')['target'].count()\n    # Return the ratio of cancerous to non-cancerous images per-patient\n    s2 = path_df.groupby('patient_id')['target'].mean()\n\n    # Merge the series into one dataframe that uses the same index\n    df_summary = pd.concat([s1,s2],axis = 1)\n    df_summary.columns = ['n_patches\/patient','cancer_ratio']\n    return df_summary","9fd5ce12":"df_summary = get_image_count_and_cancer_ratios(path_df = df)\ndisplay(df_summary)","f8cbc599":"# Lets get a statistical summary of our dataframe\ndisplay (df_summary.describe())","aa7ee855":"# set bin values for number of images\nbin_values = np.linspace(0,2400,25)\n# below we will group our data into bins in order to e\ndef get_binned_cancer_ratio_df(path_df,bin_values):\n    df_summary = get_image_count_and_cancer_ratios(path_df)\n    # use cut to determine which values fit into which bin\n    bins = pd.cut(df_summary['n_patches\/patient'], bin_values)\n    # group your dataframe by the bins\n    binned_df = df_summary.groupby(bins).median() \n    binned_df['non-cancer_ratio_median'] = binned_df['cancer_ratio'].apply(lambda x: 1-x)\n    binned_df['n_patients_per_bin'] = df_summary['n_patches\/patient'].groupby(bins).count()\n    # rename cancer ratio to cancer ratio median since we are taking the median now\n    binned_df.rename(columns={'cancer_ratio':'cancer_ratio_median'},inplace=True)\n    return binned_df","5b8808eb":"binned_df = get_binned_cancer_ratio_df(df,bin_values)\nbinned_df # fix the average number n_imgs\/patient","29716d4a":"def plot_patch_info(binned_df,bin_values):\n    \n    # get the the ranges for the x-axis since this is supposed to emulate a histogram\n    ranges = []\n    for i in range(len(bin_values)):\n        try: r = '('+str(int(bin_values[i]))+'-'+str(int(bin_values[i+1]))+')'\n        except: pass\n        ranges.append(r)\n    \n    # Number of Patients per Bin -> Bar Plot\n    trace_0 = go.Bar(name='Number of Patients Per Bin',x=ranges, y=binned_df['n_patients_per_bin'],marker_color='#ff6efa');\n    # Ratio of Cancerous Images -> Bar Plot\n    trace_1 = go.Bar(name='Cancerous Images', x=ranges, y=binned_df['cancer_ratio_median'],marker_color='#8634eb');\n    # Ratio of Non-Cancerous Images -> Bar Plot\n    trace_2 = go.Bar(name='Non-Cancerous Images', x=ranges, y=binned_df['non-cancer_ratio_median'],marker_color='#ff87c9')\n    # Number of Patches per Patient vs Cancer Ratio of Image -> Scatter Plot\n    trace_3 = go.Scatter(name='# Patches vs Cancer Ratio ',x = df_summary['n_patches\/patient'],y = df_summary['cancer_ratio'],mode='markers',marker=dict(size=16,color=df_summary['cancer_ratio'], colorscale=[[0.0, \"#ff87c9\"],[1.0, \"#8634eb\"]]))\n    \n    fig = make_subplots(rows=3, cols=1, \n                        shared_xaxes=True, \n                        vertical_spacing=0.05,\n                        subplot_titles=(\"Frequency of Number of Patches\",\n                                        \"Ratio of Cancerous to Non Cancerous Images as a Function of Number of Images per Patient\",\n                                        \"Number of Patches per Patient vs Cancer Ratio of Image\")\n                       )\n\n    # Change the bar mode\n    fig.update_layout(\n        barmode='stack',\n        bargap=0\n    )\n\n    fig.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Percentage\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Percentage\", row=3, col=1)\n\n    fig.update_xaxes(title_text=\"Number of Patches\", row=3, col=1)\n    \n    fig.update_layout(height=1800,legend={'traceorder':'normal'})\n    fig.update_xaxes(rangemode=\"tozero\")\n\n    fig.append_trace(trace_0,1,1)\n    fig.append_trace(trace_1,2,1)\n    fig.append_trace(trace_2,2,1)\n    fig.append_trace(trace_3,3,1)\n\n\n    fig.show()","81537dcd":"%%javascript\nIPython.OutputArea.auto_scroll_threshold = 130;","a61fc099":"plot_patch_info(binned_df,bin_values) # put image in next cell hidden in case image does not show ","4407faf6":"plt.figure(figsize = (50,50));\nio.imshow('..\/input\/plot-images\/patch_info.jpg');","553a1a32":"# read image from path\nimg = io.imread('..\/input\/breast-histopathology-images\/10253\/0\/10253_idx5_x1001_y1001_class0.png')\n# show the image after being read\nio.imshow(img);","1f780b53":"print(img.shape)","26caa30f":"img","863bc6ad":"# Accessing the first row\nfirst_row = img[0:1]\nprint('FIRST ROW:')\nprint(f'The shape of the first row: {first_row.shape}')\nprint(f'The first five pixels of the first row: \\n\\n{first_row[0:1,0:5]}\\n')\n# Accessing the first column\nfirst_column = img[:,0:1]\nprint('FIRST COLUMN:')\nprint(f'The shape of the first column: {first_column.shape}')\nprint(f'The first five pixels of the first row: \\n\\n{first_column[0:5,0:1]}\\n')\n# Accessing a specific pixel\npixel = img[19:20,16:17]\nprint('RGB VALUE 20th ROW, 17th COLUMN:')\nprint(f'The shape of the pixel: {pixel.shape}')\nprint(f'The first five pixels of the first row: \\n\\n{pixel}\\n')","f55ab8aa":"fig,axs = plt.subplots(1,3,figsize = (30,10));\n# Displaying the first row\naxs[0].imshow(first_row);\n# Displaying the first column\naxs[1].imshow(first_column);\n#Displaying the pixel\naxs[2].imshow(pixel);","c95eabec":"display(img[0:1].shape)\ndisplay(img[0:1].ndim)\ndisplay(img[0].shape)\ndisplay(img[0].ndim)","55b78e25":"display(img[0:1])\ndisplay(img[0])","bcbccd4a":"io.imshow(img[0]);","b821c0c3":"fig,axs = plt.subplots(1,3,figsize = (20,50));\naxs[0].imshow(img[:,0]);\naxs[1].imshow(img[:,0:1]);\n# using reshape to achieve the same result\naxs[2].imshow(img[:,0].reshape(50,1,3));","63a78f1a":"path_df = df","85cc767c":"def show_patches(target='Any',nrows = 2,ncols = 5, path_df = path_df):\n    '''\n    Function accepts type of target:\n    target = 0 --> Non-Cancerous Tissue\n    target = 1 --> Cancerous Tissue\n    path_df: dataframe containing paths\n    '''\n    if target == 'Any':\n        # if target is set to any retrieve both cancerous and non-cancerous cells\n        tissue_indices = np.random.choice(path_df.index, size=nrows*ncols, replace=False)\n    else:\n        # replace = False means that no duplication is allowed\n        tissue_indices = np.random.choice(path_df[path_df['target']==target].index, size=nrows*ncols, replace=False)\n    \n    fig,axes = plt.subplots(nrows=nrows,ncols=ncols,figsize = (ncols*6,nrows*6))\n            \n    for i,ax in enumerate(axes.flatten()):\n        idx = tissue_indices[i]\n        img = io.imread(path_df.loc[idx,'path'])\n        ax.imshow(img)","7738f50d":"show_patches(target=0,ncols=10)","bd93578c":"show_patches(target=1,ncols=10)","a18661a6":"def get_patient_df(p_id,df=df):\n    return df.loc[df['patient_id']== p_id,:] ","73da6dcb":"from matplotlib.colors import LinearSegmentedColormap\n\ndef scatter_patient_xy(nrows=3,ncols=3):\n    n_imgs = nrows*ncols\n    # get random patient ids to plot\n    p_ids = np.random.choice(df['patient_id'].unique(), size=n_imgs, replace=False)\n    \n    fig, axs = plt.subplots(nrows=nrows,ncols=ncols,figsize=(30,30))\n    \n    colors = ['#ff70db','#9334eb']\n    \n    # setting the point colors for the target values\n    cmap = LinearSegmentedColormap.from_list(name='',colors = colors, N=2)\n    \n    for i,row in enumerate(axs):\n        for j,ax in enumerate(row):\n            p_id = p_ids[i*nrows+j]\n            p_df = get_patient_df(p_id)\n            \n            x_coords = p_df['x_coord'].values\n            y_coords = p_df['y_coord'].values\n            \n            # get the min coordinate of each patient image\n            min_coord = min(x_coords.max(),y_coords.max())\n            # use min_coord to to set the size of the plotted square\n            s = int(4000\/min_coord*15)\n            \n            # used to determine the color of the point\n            targets = p_df['target'].values\n            ax.scatter(x_coords,y_coords,c=targets,cmap=cmap,s=s,marker='s')\n            ax.set_title('Patient: '+p_id)","da04d042":"scatter_patient_xy()","bd666387":"def get_tissue_image_array(patient_id,pred = False):\n    # get patient dataframe\n    path_df = get_patient_df(patient_id)\n    # get the max_coordinate to define the numpy array size, numpy array will be containing the rgb values for the image\n    max_coord = np.max((*path_df['x_coord'],*path_df['y_coord']))\n    # add 50 to the max_coord to get the final image dimension,50 here is the patch size\n    image_dimension = max_coord + 50\n    # create a 3 dimensional array with RGB values = 255, we set the type as uint since no RGB value can be > 255, thus increasing efficiency\n    grid = 255*np.ones(shape = (image_dimension,image_dimension,3)).astype('uint8')\n    mask = 255*np.ones(shape = (image_dimension,image_dimension,3)).astype('uint8')\n    for x,y,target,path in path_df[['x_coord','y_coord','target','path']].values:\n        img_array = io.imread(path)\n        # some patches have dimension less than 50 x 50 which would cause the code to break\n        try:\n            # replace values in grid array by the image array values\n            grid[y:y+50,x:x+50] = img_array \n            # if the image is cancerous add\n            if target != 0:\n                mask[y:y+50,x:x+50] = [0,0,255]\n        except: pass\n    # check if prediction is specified\n    if pred == False:\n        img = grid\n    else:\n        alpha = 0.8\n        # This is step is very important, multiplying the 2 values by a float value converts the arrays to float64, which is why convert them back to uint8\n        img  = (mask * (1.0 - alpha) + grid * alpha).astype('uint8')\n        \n    return img\n            ","cc4f2a35":"def plot_tissue_images(nrows=3,ncols=3,pred = False,df=df):\n    n_imgs = nrows*ncols\n    p_ids = np.random.choice(df['patient_id'].unique(), size=n_imgs, replace=False)\n    \n    fig,axs = plt.subplots(nrows=nrows,ncols=ncols,figsize=(30,30))\n    for i,row in enumerate(axs):\n        for j,ax in enumerate(row):\n            p_id = p_ids[i*nrows+j]\n            img = get_tissue_image_array(p_id,pred = pred)\n            ax.set_title(f'Breast Tissue Slice for patient {p_id}')\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.imshow(img)","9cb20700":"plot_tissue_images(pred = True)","44f11b8d":"# function that accepts number of patients and returns a dataframe containing all the images for each patient. \n# prioritizes patients with higher number of patches.\ndef get_patients(df,n_patients=10):\n    p_ids = df.groupby('patient_id')['patient_id'].count().sort_values(ascending=False).index[0:n_patients]\n    df =  df.loc[df['patient_id'].isin(p_ids)].reset_index(drop=True)\n    return df","bcd1574c":"sample_df= get_patients(df=df)\nsample_df.info()","4b02b9fb":"sample_df.head()","83c32008":"def get_img_arrays(df,):\n    # read each image array from corresponding path as grayscale and flatten the image array\n    df['img_array'] = df.progress_apply(lambda x : io.imread(x['path'],as_gray=True).flatten(),axis=1) # make sure to specify axis = 1\n    # get the shape of each image array and store it in the dataframe\n    df['array_shape'] = df.progress_apply(lambda x : x['img_array'].shape[0],axis=1) # make sure to specify axis = 1\n    return df","a580464d":"sample_df = get_img_arrays(df = sample_df)","7dbf41df":"sample_df.head()","c74069c0":"# get the count of the unique values in the column array_shape\nsample_df['array_shape'].value_counts() ","773f417d":"# get all images that do not have an array shape of 2500\nweird_imgs = sample_df[sample_df['array_shape'] != 2500] ","f5627433":"# use the show patches function that we created earlier to visualize those images\nshow_patches(nrows = 2,ncols = 5, path_df = weird_imgs)","ae25d0d6":"# get the shape of the dataframe before dropping the artifacts\nsample_df.shape ","72890093":"# drop images using indices of the filter\nsample_df.drop(weird_imgs.index,inplace=True) ","5b458892":"# get the shape of the dataframe before dropping the artifacts\nsample_df.shape ","52211cd3":"sample_df.head()","844b3e17":"# get the number of rows in the pandas dataframe in order to determine the number of rows in our numpy array\nnrows=sample_df.shape[0]\n# set the number of columns to 2500, which is the length of our array\nncols=2500\n# initialize the array using the information above\nimg_arrays= np.zeros((nrows,ncols))\nprint(img_arrays.shape)","ef9842de":"# add all the image arrays to the numpy array that we just initialized\nfor i,array in enumerate(sample_df['img_array']):\n    img_arrays[i,:] = array\ndisplay(img_arrays[0:5,:])","c400a8e3":"from sklearn.decomposition import PCA\nimages_pca = PCA()\n# fit the function to our image arrays\nimages_pca.fit(img_arrays);","70091224":"# explained get the cumalitive sum of the explained variance ratio for each principle component\nevr = np.cumsum(images_pca.explained_variance_ratio_)\n# plot the explained variance ratio\nfig = go.Figure(data=go.Scatter(y = evr,line=dict(color='#ff70db')))\nfig.update_layout(title='Explained Variance Ratio After PCA',\n                   xaxis_title='Number of Principle Components',\n                   yaxis_title='Cumalitive Explained Variance Ratio')\nfig.show()","5fefaba2":"# let's now apply PCA but only retrieve the first 150 components or the components that account for 80 percent of the variance\nimages_pca = PCA(0.8)\n# OR\nimages_pca = PCA(150)\n# fit the function to our image arrays\nimages_pca.fit(img_arrays);","4ff508ea":"# get the shape of the first image array\nsample_df['img_array'][0].shape","cc494a31":"# Apply PCA transformation to each row in the img_array column\n# hint: the transform function accepts a list, so if you wanted to feed it one value you would have to place that value in a list\nsample_df['pca_array'] = sample_df['img_array'].progress_apply(lambda x: images_pca.transform([x]).flatten()) ","5ddfb48b":"sample_df['pca_array'][0].shape","06e02926":"sample_df.head()","c84c10f3":"# visualizing the first 30 principle components\nfig, axes = plt.subplots(3, 10, figsize=(30, 10))\nfor i, ax in enumerate(axes.flat):\n    # return each of the components, and reshape them to 50x50\n    ax.imshow(images_pca.components_[i].reshape(50, 50), cmap='binary_r')","412ec162":"components = images_pca.transform(img_arrays)\nprojected = images_pca.inverse_transform(components)\n\n# Plot the results\nfig, ax = plt.subplots(2, 10, figsize=(30, 7))\nfor i in range(10):\n    ax[0, i].imshow(img_arrays[i].reshape(50, 50), cmap='binary_r')\n    ax[1, i].imshow(projected[i].reshape(50, 50), cmap='binary_r')\n    \nax[0, 0].set_ylabel('Before PCA')\nax[1, 0].set_ylabel('150-dim\\nReconstruction');","c298dbcc":"# get the count of each of the target values\nsample_df['target'].value_counts() ","5c326bb2":"# get the dataframe containing the negative target variables\nnegative = sample_df[sample_df['target'] == 0]\n# get the dataframe containing the positive target variables\npositive = sample_df[sample_df['target'] == 1]\n# get the shapes of each dataframe\ndisplay(negative.shape)\ndisplay(positive.shape)","10250a6e":"from sklearn.utils import resample\n# downsample the negative targets\nneg_downsampled = resample(negative,n_samples=positive.shape[0], random_state=42)\n# combine minority and downsampled majority\ndownsampled = pd.concat([positive, neg_downsampled])\n# check new class counts\ndownsampled['target'].value_counts()","712e3f3e":"# show the first 5 values of the dataframe\ndownsampled.head()","fe3809bc":"# let's extract our variables of interest and store them in a new dataframe\ndfd = downsampled.loc[:,['img_array','pca_array','target']]","dd2d2327":"dfd.head()","83d331a9":"# get the number of rows in the pandas dataframe in order to determine the number of rows in our numpy array\nnrows=dfd.shape[0]\n# set the number of columns to 150, which is the length of our array\nncols=150\n# initialize the array using the information above\npca_arrays= np.zeros((nrows,ncols))\nprint(pca_arrays.shape)","77663fb2":"# loop over the array and replace the data\nfor i,array in enumerate(dfd['pca_array']):\n    pca_arrays[i,:] = array","384d959f":"from sklearn.model_selection import train_test_split\n# split our data into training and testing data, and input data and target data\nX_train, X_test, y_train, y_test =  train_test_split(pca_arrays, dfd['target'], train_size=0.7, shuffle = True)","a03f291e":"# compare the shape of the train and test inputs\nprint(f'X_train Shape: {X_train.shape}')\nprint(f'X_test Shape: {X_test.shape}')\nprint(f'y_train Shape: {y_train.shape}')\nprint(f'y_test Shape: {y_test.shape}')","091d805d":"from sklearn.svm import SVC\nsvc_rbf = SVC(kernel = 'rbf',gamma = 'auto' )\nsvc_linear = SVC(kernel='linear',gamma = 'auto')","de722ff7":"# fitting our models\nsvc_rbf.fit(X = X_train,y = y_train);\nsvc_linear.fit(X = X_train,y = y_train);","b2861dea":"print(svc_linear.score(X_train,y_train))\nprint(svc_rbf.score(X_train,y_train))","202e28c0":"print(svc_linear.score(X_test,y_test))\nprint(svc_rbf.score(X_test,y_test))","83144393":"from sklearn.metrics import confusion_matrix\n\ntn, fp, fn, tp = confusion_matrix(y_true=y_test,y_pred=svc_linear.predict(X_test)).ravel()\n\nprint(f'training set: true negatives: {tn}')\nprint(f'training set: true positives: {tp}')\nprint(f'training set: false negatives: {fn}')\nprint(f'training set: false positives: {fp}')","51356aaf":"from sklearn.metrics import confusion_matrix\n\ntn, fp, fn, tp = confusion_matrix(y_true=y_test,y_pred=svc_rbf.predict(X_test)).ravel()\n\nprint(f'training set: true negatives: {tn}')\nprint(f'training set: true positives: {tp}')\nprint(f'training set: false negatives: {fn}')\nprint(f'training set: false positives: {fp}')","9288fdaf":"def confusion_vectors(predicted,actual):\n    '''\n    Function returns:\n        # 0 False Negative\n        # 1 True  Negative\n        # 3 True  Positive\n        # 4 False Positive\n    '''\n    # get the predicted values\n    evaluation = (predicted + 1)**2 - actual\n    return evaluation","81faca63":"# predict the values for each row, make sure to reshape since predict accepts a 2d vector \n# use [0] to return the first value since predict returns an array, and we are only predicting one row at a time\nsample_df['predicted']=sample_df['pca_array'].progress_apply(lambda x: svc_rbf.predict(x.reshape(1,150))[0])","4d180903":"# get the confusion vector values for each row, so that we can distinguish what type of error we got for each patch prediction\nsample_df['conf']=sample_df.progress_apply(lambda x: confusion_vectors(predicted=x['predicted'],actual=x['target']),axis=1)","911bddb2":"sample_df.head()","f712aefc":"def get_tissue_image_array_conf(patient_id,path_df):\n    # get patient dataframe\n    p_df = get_patient_df(patient_id,df=path_df)\n    # get the max_coordinate to define the numpy array size, numpy array will be containing the rgb values for the image\n    max_coord = np.max((*p_df['x_coord'],*p_df['y_coord']))\n    # add 50 to the max_coord to get the final image dimension,50 here is the patch size\n    image_dimension = max_coord + 50\n    # create a 3 dimensional array with RGB values = 255, we set the type as uint since no RGB value can be > 255, thus increasing efficiency\n    grid = 255*np.ones(shape = (image_dimension,image_dimension,3)).astype('uint8')\n    # initialize empty array for for the mask\n    mask = 255*np.ones(shape = (image_dimension,image_dimension,3)).astype('uint8')\n    # create a dict containing the values to be filled in for each corresponding case\n    # 0 is false negative, 3 is true positive, 4 is false positive\n    mask_color = {0:[255,0,0],3:[0,0,255],4:[0,255,0]}\n\n    for x,y,conf,path in p_df[['x_coord','y_coord','conf','path']].values:\n        img_array = io.imread(path)\n        # some patches have dimension less than 50 x 50 which would cause the code to break\n        try:\n            # replace values in grid array by the image array values\n            grid[y:y+50,x:x+50] = img_array \n            # check if conf is not a true negative\n            if conf != 1:\n                mask[y:y+50,x:x+50] = mask_color[conf]\n        except: pass\n\n    alpha = 0.8\n    # This is step is very important, multiplying the 2 values by a float value converts the arrays to float64, which is why convert them back to uint8\n    img  = (mask * (1.0 - alpha) + grid * alpha).astype('uint8')\n    \n    return img","bc0d4ab9":"def plot_tissue_images_conf(nrows=3,ncols=3,df=df):\n    n_imgs = nrows*ncols\n    p_ids = np.random.choice(df['patient_id'].unique(), size=n_imgs, replace=False)\n    \n    fig,axs = plt.subplots(nrows=nrows,ncols=ncols,figsize=(30,30))\n    for i,row in enumerate(axs):\n        for j,ax in enumerate(row):\n            p_id = p_ids[i*nrows+j]\n            img = get_tissue_image_array_conf(p_id,path_df=df)\n            ax.set_title(f'Breast Tissue Slice for patient {p_id}')\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.imshow(img)","2367b72e":"plot_tissue_images_conf(df=sample_df)","860517fc":"Not bad for a first try, and not bad considering the amount of data that we used. However, this is accuracy was tested on the same data that our model was initially trained on. So our model has already seen this data before. \n\nThe real question is: how well does our model do on data that it has not seen before? \n\nThat's what the training data is for...","f5624595":"So how do we extract the data that we want, now that we have all the paths?<br>\n\nThe answer is using **regex**.\n\nI know that regex may be daunting at first, but if this is your first time using regex I highly suggest you check out [regexr.com](https:\/\/regexr.com\/)","1804dd32":"### Setting model parameters","6661b75a":"### Visualizing our Confusion Matrix","6bd42e68":"## Healthy Tissue Patches Vs Cancerous Tissue Patches\nLet us now explore the visual differences between cancerous tissue cells, and healthy tissue cells. Usually partnering with a specialist is a good idea so that they can point the exact points of interest that differentiate the 2 from each other.","d12e79bf":"## Reading and Displaying an Image","5cafab9a":"### Let's take a closer look","381f1e5e":"The below function acts similar to a confusion matrix, however it returns numerical values for each input.\n*         0 False Negative\n*         1 True  Negative\n*         3 True  Positive\n*         4 False Positive\n\nThis is done by getting the predicted value, which is either 0 or 1, and then adding 1 to the value and the squaring it then subtracting it from the actual value. This is done so that we can distinguish the individual cases from each other.","2228ce2f":"### Splitting our Data\n\n* ***Training Data*** : Data that we will use to train our model\n* ***Testing Data*** : Data that we will use to test our model\n* ***Out of Sample Data*** : Data that we will use to further validate our testing. Usually this is taken before preprocessing the data.\n\nWe will be using sklearn's train_test_split to split our data. \n\nShuffle is set to False, this is because we already manually shuffled our data in the previous cell. \n\nFor simplicity's sake we will not be considering the Out of Sample data, and we will only be splitting our data to training and testing sets.","744afd6f":"From the graph above we can determine that the first 150 components account for over 80% percent of the variance in our data. We managed to reduce the number of dimensions of our image arrays from 2500 to 150, while still being able to maintain over 80% of the information, this a 96% reduction in the number of dimensions that we have!","d368f902":"Let's start by extracting our image arrays from our dataframe and storing them in a numpy array.","a23cf40a":"### Non-Cancerous Patches","d2a4dc6c":"So it seems that we need to drill down more to find our files, and it seems that each file contains the data for the patient with the corresponding patient id. <br>\n\nNext, let's take a look at the contents of the first folder.","a80c032d":"Let's drop these images as they add noise unnecessary noise to our model.","d82e8605":"## Training & Testing Split\n\nThere are a few things that we must consider before splitting our data. The first being the ratio of cancerous to non cancerous images that we have. We determined earlier that our data is not split uniformly. We have many more, non-cancerous images that non-cancerous images, which may possibly bias our model. \n\nThis may also pose a problem when evaluating our model. Let's take an extreme example where we have a dataset consisting of 100 points, 90 of them are cancerous and 10 of them are non-cancerous, let's also assume that our model classified our entire dataset as cancerous, this would give an accuracy of 90%, however this is not really representitive of what is really happening. In reality it is always misclassifying our non-cancerous data.\n\nSo how do we go about solving this?\n\nWe can use the train-test split function offered in sklearn model_selection, this function has the option to split our data such that we get equally distributed training and testing sets when grouped by the classes.\n\nFor more information, check out this [article](https:\/\/towardsdatascience.com\/what-to-do-when-your-classification-dataset-is-imbalanced-6af031b12a36), that talks about the considerations that need to be taken before splitting your data.","3e1032ad":"Now that we have set up our data, let's create a visual summary to help us draw some insights from our data.","8c54c00f":"#### An RGB image is 3 dimensional, while a grayscale image is 2 dimensional","2c8ad2b7":"Based on the code above we can see that the first patient_folder contains 2 folders and no files.\nIt's best practice to validate that all the other files and folders contain the same sub-folder stucture of 2 folders and 0 files.","8f43a1dc":"Now that we have the predicted values, we apply the confusion vector function that we created to each of the rows so that we can determine what type of error we got for each prediction.","1ce9561f":"### What do you think will happen if I try to display an image?\n### Let's take a look...","dcf313e9":"In our case we will be undersampling our Non-Cancerous Data, in order to make our target distribution more balanced. Ideally we would artificially oversample our data by rotating our cancerous images.","0114407c":"# Training Model","607a4ae6":"We will be using skimage as a library to process the images.","2a82ee0f":"# Training - Testing Split\n\nIn order to verify our model's performance we need to split our data to testing and training data.\nThe testing data will verify that our model is able to classify cases that it hasn't seen before. \n\nNormally, this step would be done before performing PCA, as the testing data would influence our PCA fit. But I will split it here for simplicities sake.","b90b88b6":"## Storing the image_path, patient_id, target and x & y coordinates\nWe have about 278,000 images. To feed the algorithm with image patches we will store the path of each image and create a dataframe containing all the paths. **This way we can load batches of images one by one without storing the individual pixel values of all images**. \n\nNow that we know our basic file structure which seems to be as follows:\n\nbreast-histopathology-images:<br>\n    -->patient_id:<br>\n           -->-->cancer_class:<br>\n                     -->-->-->image_files<br>\n                     \nNow we need to find a way to loop through all of our directories to extract the paths and labels. Some of the information that we will be interested in extracting includes:\n* patient_id\n* x_coordinate\n* y_coordinate\n* target\n* path_to_image","77e143dd":"### Comparing Images Before & After PCA","3e13351c":"### Insights\n1. The number of image patches per patient varies a lot! It seems that not all images have the same resolution, as the resolution of the image and the number of patches is directly proportional.\n2. Some patients appear to have over 70% of the image covered in cancer. Patients with a lower number of images tend to suffer from that the most, indicating that we are only receiving a fraction of the full picture.\n3. The classes of Cancerous vs Non-Cancerous patches are imbalanced. We may consider rebalancing the class at a later stage.","9ba8511c":"We can see above that inside our breast-histopathology folder we have a folder that labeled with each patients id. <br>","9d6cc8d6":"#### Confusion Matrix\nBut Accuracy alone is not a good metric to use and does not reflect the entire picture. \n\nFor that we will be using something known as the confusion matrix. The confusion matrix gives us the following results:\n\nTrue Postives: When a patch is cancerous and our model correctly classifies it as cancerous<br>\nTrue Negatives: When a patch is not cancerous and our model correctly classifies it as non-cancerous<br>\nFalse Positives: When a patch is non-cancerous but it is misslasified as cancerous <br>\nFalse Negatives: When a patch is cancerous but it is missclasified as non-cancerous\n\nWhen modeling cancer classification, we need to consider which of the above is more detrimental to the patient.\n\nIs it worse not having cancer and to be classified as having cancer? Or, is it worse having cancer and being told that you do not?\n\nThe latter case is much worse, as it will eventually lead to the death of the patient due to not being treated in the first place. \n\n\nBased on the above analogy, we would like to maximize the number of times that a paitient suffering from cancer is classified as such, and we would also like to minimize the amount of times that a patient sufferning from cancer is missclassified as not having cancer. \n\nIn other words, we are trying to maximize the True Positives, and Minimize the False Negatives, on the expense of sometimes incorrectly classifying a patient that does not suffer from cancer as cancerous.","20383f6d":"## Visualizing the Breast Tissue\nEarlier we extracted the coordinates of the cropped tissue cells, we can use those coordinates to reconstruct the whole breast tissue of the patient. This way we can explore how the diseased tissue looks when compared to the healthy tissue. \n\nWe can also explore the most common places that the cancer tends to occur in. It would be interesting to plot a heatmap of the  most common areas where the cancer appears. \n\nIf position of the crop has significance then perhaps we can use it as an input feature for our model.","e5ecf92d":"#### In case plot doesn't show above, try refreshing or unhide and run the lines of code below..","05852df6":"## Taking a sample of our data\nBelow we are going to take a smaller sample of our data. Ideally we would like to use our entire dataset, but for the sake of demonstration purposes and saving time we will be using a smaller sample.\n\nWe will be taking the top 10 patients in terms of number of patches for our sample.","b67c7308":"Recall that, each patient directory has 2 sub-folders, labeled 0 and 1 <br\/>\nFolder 0: Non-Invasive Ductal Carcinoma (IDC) <br\/>\nFolder 1 : Invasive Ductal Carcinoma (IDC) <br\/>\n\nLet's check out the folder contents of each directory.","595c1181":"### Insights\n1. Some patiets have as little as 63 patches\n1. The average cancer ratio for each patient is about 70% non-cancerous and 30% cancerous\n1. Our Cancer ratio varies just as much as our number of patches, is there any relation?","dc6e7e31":"Notice how our x & y coords and our targets are formatted as strings instead of ints","a7afe93d":"Let's double check that our function is extracting everything correctly","2a6e6c7f":"# Exploratory Data Analysis (EDA)\n\n## Let's create a visual summary of our data","1c3dedeb":"#### After Dropping Artifacts","7c865d44":"Let's add the predicted values to our dataframe so that we can compare them with our actual target values. ","52146651":"### Cancerous Patches","d82f3616":"#### Before Dropping Artifacts","3799bed2":"#### Notice how the image arrays now contain values between 0 and 1. This is because we read the image as grayscale image. Normally in Machine Learning if a we scale our data between 0 and 1, but since this step is already done for us we will be skipping it.","4d10ba95":"Now that we have our eigen vectors, let's compare how our images looked like before PCA and after PCA. What we are about to do is multiply each of one of our components by its corresponding eigen vector and sum it, think of it like a weighted sum.","6f3ea939":"Let's now visualize what some of the reconstructed tissue cells look like by using the function above. \n\nNote: The function can be improved by having edges on the mask, rather than overlaying a color on top. This will help us preserve the true image of the tissue cells.","db6a2cbc":"To simplify things we will create a function that slices our existing dataframe and retrieves the values associated with a patient id.","60c369f6":"Finally below we replot our patches, this time we use mask colors to indicate the following:\n* Green Mask: False Positives\n* Blue Mask: True Positives\n* Red Mask: False Negatives","8940fcf6":"Let's make sure that our changes where applied by checking the info one more time","c9890532":"### Verifying Directory Structure\nThe funcion below accepts the path to the directory that you would like to explore and accepts the number of folders and files that you would like to verify that each path in that directory contains.<br>\n\nIt returns the dictionary containing the paths of the matched and unmatched files.","1260a21b":"Now that we have extracted the paths to our images, let's check out how many images we have. The number of images is simply equal to the number of paths that we have.","2aacf41c":"### Extracting Metadata into DataFrame","b7e2481b":"### Navigating to Directory Containing Data Files","3c01f92e":"### Undersampling Our Data","0dcd159a":"#### Initialize Image Array","ac6f4e83":"####  Let's see if all of our images have the same shape:","21f0cbea":"### Drilling deeper into what an image is\nSo far we:\n* Read the image from a path\n* We then stored that image in a variable called img\n* We discovered that img is a python numpy array\n\nNow let's have a look at the raw image array","9458eab9":"#### Split the Data","df9293b3":"uint8 is used unsigned 8 bit integer. And that is the range of pixel. We can't have pixel value more than 2^8 -1. Therefore, for images uint8 type is used. Whereas double is used to handle very big numbers.","74556fb3":"### What is an image in python?\n\nWhen we do io.imread(path) we are converting this image into it's numerical form. An image in python is represented by a numpy array. (n x m x 3) in the case of an RGB Image. or (n x m x 4) in the case of an RGBA image or CMYK image. Finally it is also possible to have a shape as follows (n x m x 1) which indicates that we are dealing with a grayscale image.\n\nWe will be dealing with the first case.","e1ca48ae":"#### Shape of Array after applying PCA","4508ed9d":"### Fitting the model ","5dec002b":"#### Applying PCA on Image Arrays\nThe PCA function accepts a percentage between 0 and 1, for example n_components = 0.8, would mean return for me the eigen vectors that account for 80% percent of the variation from the in my image arrays.\n\nThe PCA function also accepts any number of absolute components that you specify, for example: we initially have 2500 components, if we want the first 150 components then we set n_components = 150.","35ca4568":"# Preparation & peek at the data structure\n\n\n## Loading packages","dbae5282":"### Something to be careful of\n\nIs `io.imshow(img[0:1])` ,the same as `io.imshow(img[0])`?\n\nIs `io.imshow(img[:,0:1])` ,the same as `io.imshow(img[:,0])`?\n\nThe answer is a big fat\n# NO!","434cbd6b":"### Reaching the image file paths\nNow that we know which paths match our structure criteria we will leave the other file for later, and continue exploring the matched files that we have.","edd839fe":"Now that we have the directory containing our files, we can proceed to exploring the underlying structure.","5a716eef":"Let's try and visualize how are function is doing by restitching the patches together for each patient.","78eed392":"### Accessing rows & columns of an image\nNow that we know that the image is basically a numpy array, we can access the rows and columns using numpy array slicing.\n\nIn other words,accessing the rows and columns in an image is as simple as accessing them in a numpy array.","80a5f136":"BIAS IN THE DATASET: for example if a patient has a cancerous spot but the spot is not there and therofore classified as non-cancerous.","32387f4e":"Finally we have reached the part to evaluate how well our model does. First let's check out our score on the data that we trained our models on. The cell below retrieves the accuracy for each of our models. Recall that Accuracy is: total correctly classified \/ total number of classifications.","84cf07b4":"### Plotting the First Row, Column, and specific Pixel","81665108":"### Analytics\n* How many patches do we have on average per patient?\n\n* On average how many of those patches are cancerous vs non-cancerous?\n\nLet's try and answer those questions by querying our dataframe.","18e5c589":"### Eigen Patches\nIf you've hearded eigen faces, this right here is the breast cancer version of that","0dd11c8a":"### Insights\u00b6\n* Sometimes we can find artifacts or incomplete patches, some images are also less than 50 x 50 pxs. \n* Patches with cancer look more violet and crowded than healthy ones. Is this really typical for cancer or is it more typical for ductal cells and tissue?\n* Though some of the healthy patches are very violet colored too!\n* Would be very interesting to hear what criteria are important for a [pathologist](https:\/\/en.wikipedia.org\/wiki\/Pathology).\n","a59eb355":"#### Initial shape of our image array","1588d231":"## Exploring the data structure\nWe start by exploring the file structure to gain an understanding of how everything is organised","d74a2a80":"Finally we will transfer our values from dictionaries into a pandas dataframe which will make our analysis much easier moving forward.","eb8aa230":"#### Explaining the Variance Ratio","a6c5b1e2":"From the images above we can conclude that our model is biased towards false positives. Now there are many reasons as to why that may be. \n\nWe can start by filtering by our images by the different confusion matrix cases and we can try to understand what is going on. But I leave that to you. \n\n\nI hope this notebook, gave you an idea of what happens behind the scenes of image classification using machine learning, there are still many ways we can improve it, such as using different image kernels to extract features, but that is a task for another day.","3da6520b":"Let's take a peak at the paths that we have and verify that everything makes sense.","bcfa65c5":"It seems that we have reached the files containing the tissue images of each patient, \nthe structure seems to be as follows:\n\nPatient_id\/xcoordinate_of_patch\/ycoordinate_of_patch\/class_of_cancer","470b7c6a":"Our images seem to be of 50 Pixels wide and 50 Pixels tall. The 3 represents the RGB space.","447fa6ab":"As expected most of our image arrays have a length of 2500, this is because our arrays where originally, 50 x 50 x 3 so when we convert them to grayscale we get an array of 50 x 50 x 1 and when we flatten it we get 2500 total values.\n\nNotice how we have some image arrays that don't have a length of 2500. Let's have a look at what those images look like.","5e69be49":"### Model Evaluation","b48091dd":"## Storing the Images\n\nNow that we know that our images are a bunch of arrays, how do we actually feed them to a machine learning model?","554f4881":"## Applying PCA on to the Images\nIn the above step we managed to read our image arrays into our numpy arrays, we also managed to reduce the total length of our image arrays from 7500, to 2500, by converting our images into grayscale. Now we would like to further reduce the number of dimensions by applying Principle Component Analysis onto our image arrays. ","da71931e":"#### Extracting Data into Array","02b6b2f4":"#### Increase the maximum number of rows that will be displayed in the output before a scroll bar is used\n","76449126":"### Insights\n* Sometimes we don't have the full tissue information. It seems that tissue patches have been discarded or lost during preparation.\n* Cancerous Tissue tends to appear in clusters rather than, being dispersed all over the place.\n","a938660e":"### Binary target visualisation per tissue slice \nBefore we will take a look at the whole tissue let's keep it a bit simpler by looking at the target structure in the x-y-space for a handful of patients:","7cee60e1":"### Understanding Directory Structure\nNext let's see count the number of files in the path above and the number of folders so that we make sure we aren't missing anything.","2997be54":"### Repatching the Actual Breast Tissue Image\nNow it's time to go one step deeper with our EDA. Instead of plotting the target values using the x-y coordinates, we now plot the images themselves on their respective x-y coordinates. This will help us visualize how the cancerous tissue looks like from a macro perspective.","43a57e6c":"Great, we managed to extract the patients containing the most images, giving us a total of 22259 images that we will be using to train.","c6b51c4e":"The reason the above happens is because, when you look at the shape of your image you will have 50 rows , and 3 columns, and no value for the color part of the image, which will cause imshow to take it as grayscale by default.","cce5936f":"The next steps to this model would be to replot the restitched patches from above, and correctly mask them with the right color depending on how they were classified. ","1757f087":"In order to randomly resample our data we will be using the sklearns.utils function resample. For a more in depth explanation about the parameters of resample, check out the documentation."}}