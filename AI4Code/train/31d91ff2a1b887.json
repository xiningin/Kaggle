{"cell_type":{"d623e607":"code","e1e94b17":"code","527ae30b":"code","a0f2ba27":"code","704eded4":"code","3c0fe6ba":"code","19fe41ac":"code","710654b8":"code","0543c5f7":"code","7d6504f3":"code","c298bb6b":"code","4a55f0a7":"code","6a6139d9":"code","76b68bda":"code","3233addd":"code","9436153b":"code","72f989e6":"code","90cddf20":"code","ef65c969":"code","3df8682a":"code","0a62a715":"markdown","dab6168b":"markdown","f9e021f6":"markdown","03188f4e":"markdown","f38ad912":"markdown","9dc6a8ad":"markdown","78fa5325":"markdown","70a6a79a":"markdown","ce22fe7c":"markdown"},"source":{"d623e607":"import pandas as pd \nimport glob\nimport os\nprint(os.listdir(\"..\/input\"))\n\nPATH = \"..\/input\"","e1e94b17":"filenames = glob.glob(os.path.join(PATH, \"*.csv\"))\nprint(filenames)\ndf = pd.concat((pd.read_csv(f) for f in filenames))\nprint(df.shape)\ndf.head()","527ae30b":"## I'll keep the external id for now. We drop the harvested ID. \ndf.drop(\"harvested_date\",axis=1,inplace=True)","a0f2ba27":"# print(df.external_author_id.nunique()) # 2489\n# df.external_author_id = pd.Categorical(df.external_author_id).codes\n# print(df.external_author_id.nunique())  # # 2490\n\n# # There's a mismatch - unknown where my bug is. Commenting out for now!","704eded4":"df.shape","3c0fe6ba":"df.dtypes","19fe41ac":"df.describe(include=\"all\")","710654b8":"df.shape[0] - df.drop_duplicates(subset=\"content\").shape[0]","0543c5f7":"df.drop_duplicates(subset=\"content\",inplace=True)\nprint(\"df without duplicated content:\",df.shape[0])","7d6504f3":"# how many unique authors?\n# df.author.value_counts().shape[0]\ndf.author.nunique()","c298bb6b":"df.author.value_counts().head() # Top authors have tens of thousands of tweets\n# df.author.value_counts(normalize=True).head() # top author are 1-2 % of all tweets ","4a55f0a7":"(df.author.value_counts()<5).sum() # a few hundred with only a few posts","6a6139d9":"df.language.value_counts()","76b68bda":"df_en = df.loc[df.language==\"English\"]\nprint(df_en.shape[0])","3233addd":"df_en.describe(include=\"all\")","9436153b":"df = df.loc[df.account_category != \"Unknown\" ]\ndf.account_category.value_counts(normalize=True)","72f989e6":"df_en.account_category.value_counts()\nprint(\"original Unknown counts (for english only tweets)\")\ndf_en = df_en.loc[df.account_category != \"Unknown\" ]\ndf_en.account_category.value_counts(normalize=True)","90cddf20":"## Model building & text features can go here:","ef65c969":"df.to_csv(\"russianTweet538Election.csv.gz\",index=False,compression=\"gzip\")","3df8682a":"df_en.sample(frac=0.25).to_csv(\"russianTweet538Election_eng_sample.csv.gz\",index=False,compression=\"gzip\")","0a62a715":"## 538 defined clusters\/Categories:\n\n* We'll drop the 8th, *\"Unknown\"* cluster (it's also the smallest).\n* we'll run a model (Externally?) to classify between the clusters.\n    * Presumably, 538 made the clusters based on simple word clusters\/topics\/LDA or similar, so this won't be very informative, but it's an easy thing to start with. ","dab6168b":"### 20%  duplicated posts!\n\n* A lot of duplicated posts (may be due to retweets or lazyness)\n* Let's drop these for now - it makes author identification\/resharing easier, but it's less interesting to us for some other purposes + makes it to oeasy","f9e021f6":"## Concatenate the dataframes\n* https:\/\/stackoverflow.com\/questions\/20906474\/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe\ndf = pd.concat((pd.read_csv(f) for f in all_files))\n\n> os.path.join(path, \"*.csv\")\n","03188f4e":"## Peek at the authors:\n* 2,848 authors\n* Relatively imbalanced (range from tens of thousands to a handful of tweets).\n* Unknown yet if there are  reallymultiple \"writers\" per authors or whether the same writers create content for multiple author.  (beyond the \"external id\" which doesn't necessarily capture the truth).\n    * Would be an interesting problem to practice *author identification* on! ","f38ad912":"## Initial EDA on the Russian tweets\n* Data from https:\/\/fivethirtyeight.com\/features\/why-were-sharing-3-million-russian-troll-tweets\/\n    * 538 and the center did a great job at getting, sharing and analyzing the initial data, so this will be just the basic for getting the data into a more amenable form. \n    \n    * In future - I'll add an additional sample of tweets from the same period or in English, to see if we can seperate  the distributions (not just between the groups\/hashtags)","9dc6a8ad":"### minor EDA\n","78fa5325":"## Let's look at (English) language\n* We may want to keep only English language tweets. (As detected by Twitter's algo presumably)  = A bit of noise. (Note the many tweets that are in very rare languages. More likely they were'nt identified correctly)\n*  Makes it much more relevant for any future \"Identify foreign propaganda\/russian spy\" type models\n\n* Interestingly, Russian is the second most common language! \n    * Would be interesting to leave the Russian tweets in , in future\n* We leave geography alone.","70a6a79a":"*  Replace the existing external author id number with a shorter one, using pandas categoricals (not  really needed, but it's a tiny bit more mem.efficient, and can save data corruption if opened in excel. Also, I like this minor code snippet).\n    * https:\/\/stackoverflow.com\/questions\/38088652\/pandas-convert-categories-to-numbers : Get categoricals mapping (str_id to int) \n\n* **BUG** - mismatch in # entities, disabling for now. ","ce22fe7c":"# Export the data"}}