{"cell_type":{"5c3a40c6":"code","a78f3851":"code","e94479ee":"code","1baabeb7":"code","1f95b76d":"code","6dbaeec4":"code","64350629":"code","e3267969":"code","80e61c78":"code","43c4c8cc":"code","e7825bf6":"code","0c86ffe8":"code","e2aebfa2":"code","07bb1ea3":"code","efe0d879":"code","fe0247ab":"code","575e63a3":"code","60370269":"code","0b3c7e92":"code","589e4ad2":"code","a8b3b657":"code","59a6dca7":"code","828a2ec0":"code","ddb213e8":"code","b30c1198":"code","217ac6f8":"code","0c0d2aac":"code","1ef27636":"code","07a9aae3":"code","a0b02add":"code","f231ec7c":"code","23c38f98":"markdown","40054134":"markdown","dee2b2ac":"markdown","2dabf26b":"markdown","64ce90a3":"markdown","f722114a":"markdown","1c1ad296":"markdown","84e25f3e":"markdown","dc4b5ad0":"markdown"},"source":{"5c3a40c6":"# For local time and timing the runtime of the whole notebook\n\nfrom datetime import datetime\nimport pytz\nimport datetime\nIST = pytz.timezone('Asia\/Kolkata')\nstart=datetime.datetime.now(IST).replace(microsecond=0)\ns_hrs=start.time()\nprint(\"Notebook started on:\",start.date(),\"at\",s_hrs,\"hrs\")","a78f3851":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n\nimport sys\n!cp ..\/input\/rapids\/rapids.0.19.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","e94479ee":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n!nvidia-smi\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","1baabeb7":"# Installing gdown library for importing data from Google Drive\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n!conda install -y gdown \nimport gdown \n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","1f95b76d":"# Importing files from Google Drive \nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n!gdown https:\/\/drive.google.com\/uc?id=15r0vqWk7JyUazZxCDAQg5Kb1qVPuIB1w #1.csv\n!gdown https:\/\/drive.google.com\/uc?id=1HFhEwWFd6dWVos-PVlQT-MqAksm6eZk1 #2.csv\n!gdown https:\/\/drive.google.com\/uc?id=1ZO1VqaqtuULqQy8QnLKfyJtxN37eOKN3 #3.csv\n!gdown https:\/\/drive.google.com\/uc?id=1xjX-F3iRYxDJ4KIpFO2s9xSBmLj0bvyX #4.csv\n!gdown https:\/\/drive.google.com\/uc?id=1EdI84fMIeSnV-wL90N5fg7gQNHyM43AF #5.csv\n!gdown https:\/\/drive.google.com\/uc?id=1swoBNbc590Y2iEyvuygahe9Eq6zJdWZZ #6.csv\n    \nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","6dbaeec4":"# Standard Imports\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n\nimport pandas\nimport os\nimport numpy \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","64350629":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n\nimport cudf as pd\n\nimport cuml\n\nimport cupy as np\n\nimport io, requests\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","e3267969":"# Assigning files\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n\naisles=pd.read_csv(\".\/1.csv\")\n\ndept=pd.read_csv(\".\/2.csv\")\n\nord_prior=pd.read_csv(\".\/3.csv\")\n\norders=pd.read_csv(\".\/5.csv\")\n\nprod=pd.read_csv(\".\/6.csv\")\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","80e61c78":"# Column names\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nprint(aisles.columns)\nprint(dept.columns)\nprint(orders.columns)\nprint(prod.columns)\nprint(ord_prior.columns)\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","43c4c8cc":"# Merging Aisles and Department tables into Product tables\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nprod_dep=pd.merge( prod, dept, how='left', on='department_id')\nprod_fin=pd.merge(prod_dep, aisles, how='left', on='aisle_id')\nprint(prod_fin.isnull().sum())\ndel prod, dept, prod_dep, aisles\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","e7825bf6":"# Confirming the dtypes before merging for compatibility\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nprint(\"Orders Prior Datatypes:\\n\",ord_prior.dtypes)\nprint(\"\\nProduct Datatypes:\\n\",prod_fin.dtypes)\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","0c86ffe8":"# Verifying the variables still in the memory\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n%whos DataFrame\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","e2aebfa2":"# Merging the Orders Prior and Product tables\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nord_prod=pd.merge(ord_prior, prod_fin, how='left', on='product_id')\ndel ord_prior, prod_fin\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","07bb1ea3":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nprint(ord_prod.head(10))\nprint(\"\\n\\n\")\nprint(orders.head())\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","efe0d879":"# Checking dtypes for compatibility and null values\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nprint(\"Orders Prior Datatypes:\\n\")\nprint(ord_prod.dtypes)\nprint(\"\\nOrders Datatypes:\\n\")\nprint(orders.dtypes)\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","fe0247ab":"# Merging Orders prior and Orders tables\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n\ndata=pd.merge(ord_prod, orders, how='left', on='order_id')\ndel ord_prod, orders\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","575e63a3":"# Removing Na Values from the final data table\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nprint(\"Number of NaN Values present in the dataset: \\n\")\nprint(data.isnull().sum())\nprint(\"\\n**************************************\")\nprint(\"           REMOVING NaN VALUES\")\nprint(\"**************************************\")\ndata.dropna(axis=0, inplace=True)\nprint(\"\\nNumber of NaN Values present in the dataset: \\n\")\nprint(data.isnull().sum())\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","60370269":"# Verifying the variables still in the memory\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n%who_ls\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","0b3c7e92":"# Verifying that all the columns required have been suceessfully merged\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nprint(data.columns)\nprint(\"\\n*******************\\n\")\nprint(data.dtypes)\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","589e4ad2":"# Getting the final table output\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n# data.head(10000).to_csv(\"Sample.csv\")\nprint(data.head(10000))\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","a8b3b657":"# Droping \"eval_set\" column from the dataset\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\ndata=data.drop(\"eval_set\",axis=1)\nprint(data)\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","59a6dca7":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\ndata=data.to_pandas()\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","828a2ec0":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n# data.pivot_table(df, values=['D', 'E'], index=['A', 'C'],aggfunc={'D': np.mean,'E': np.mean})   Syntax\n\ncord_id=(data.pivot_table(index=['department_id'], \n                          values=['order_id'], \n                          aggfunc={'order_id':'count'})).reset_index()\navord_hod=(data.pivot_table(index=['department_id'], \n                            values=['order_hour_of_day'], \n                            aggfunc={'order_hour_of_day':np.mean})).reset_index()\navord_dow=(data.pivot_table(index=['department_id'], \n                            values=['order_dow'], \n                            aggfunc={'order_dow':np.mean})).reset_index()\navord_dspo=(data.pivot_table(index=['department_id'], \n                             values=['days_since_prior_order'], \n                             aggfunc={'days_since_prior_order':np.mean})).reset_index()\navaddtc=(data.pivot_table(index=['department_id'], \n                                 values=['add_to_cart_order'], \n                                 aggfunc={'add_to_cart_order':np.mean})).reset_index()\nmaxaddtc=(data.pivot_table(index=['department_id'], \n                                  values=['add_to_cart_order'], \n                                  aggfunc={'add_to_cart_order':np.max})).reset_index()\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","ddb213e8":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\ndept_piv=pd.merge(cord_id, avord_hod, how='left', on='department_id')\ndept_piv=pd.merge(dept_piv, avord_dow, how='left', on='department_id')\ndept_piv=pd.merge(dept_piv, avord_dspo, how='left', on='department_id')\ndept_piv=pd.merge(dept_piv, avaddtc, how='left', on='department_id')\ndept_piv=pd.merge(dept_piv, maxaddtc, how='left', on='department_id')\n\ndel cord_id, avord_hod, avord_dow, avord_dspo, avaddtc, maxaddtc \n\n# dept_piv=dept_piv.to_pandas()\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","b30c1198":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n# df1.columns = ['Customer_unique_id', 'Product_type', 'Province'] \ndept_piv.columns=['dept_id','count_order_id','avg_order_hod','avg_ord_dow','avg_ord_dspo','avg_addtc','max_addtc']\ndept_piv\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","217ac6f8":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\ncorrMatrix = dept_piv.corr()\nfig_dims = (20, 10)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.heatmap(corrMatrix, annot=True)\nplt.show()\ndel corrMatrix\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","0c0d2aac":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nsns.pairplot(dept_piv)\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","1ef27636":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nr=len(dept_piv.columns)\nprint(r)\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","07a9aae3":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n\nfor col_1 in range (0,r):\n  for col_2 in range (col_1,r):\n    if col_1==col_2:\n     pass\n    else:\n        w=dept_piv.iloc[:,col_1]\n        z=dept_piv.iloc[:,col_2]\n        data_num=pandas.concat([w, z], axis=1)\n        print(data_num)\n        print(\"\\n\")\n      # Selecting the data for Scaling and Clustering\n        print(data_num.dtypes)\n        q=data_num.columns\n        print(data_num.head())\n        print(type(data_num))\n        data_num.iloc[:,0]=np.float64(data_num.iloc[:,0])\n        data_num.iloc[:,1]=np.float64(data_num.iloc[:,1])\n        print(data_num.dtypes)\n        print(data_num.head())\n        data_scaled=cuml.preprocessing.scale(pd.from_pandas(data_num), axis=0)\n      # Creating clusters for Visual representation\n        K=range(1,10)\n        wss = []\n        for k in K:\n            kmeans = cuml.cluster.KMeans(n_clusters=k,init=\"scalable-k-means++\")\n            means=kmeans.fit(data_scaled)\n            t= means.inertia_\n            wss.append(t)\n            print(\"K-Means processed for k=\",k)\n            print(\"For K =\",k,\", WSS =\",t,\";\\n\")\n        kwss=pd.DataFrame()\n        kwss[\"K\"]=K\n        kwss[\"WSS\"] = wss\n        print(kwss)\n        del kwss\n        x=q[0]\n        y=q[1]\n        plt.plot(K, wss, 'bx')\n        plt.xlabel('k')\n        plt.ylabel('Average distortion')\n        plt.title(str(x)+\" vs \"+str(y)+\" Elbow Plot\")\n        plt.show()\n        plt.figure(clear=True)\n        del K, wss\n        k=[]\n        Sil_Score=[]\n        print(\"Starting the Silhouette Measure calculation at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n        for i in range(2,11):\n            labels=cuml.cluster.KMeans(n_clusters=i).fit(data_scaled).labels_\n            g=cuml.metrics.cluster.silhouette_score(data_scaled,labels,metric=\"euclidean\")\n            k.append(i)\n            Sil_Score.append(g)\n            print (\"Silhoutte score for k= \"+str(i)+\" is \"+str(g)+\" at \"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n        km=pd.DataFrame()\n        km['K']=k\n        km['Silhouette Score']=Sil_Score\n        del k, Sil_Score\n        print(\"\\n\")\n        print(km,\"\\n\")\n        print(km.dtypes,\"\\n\")\n        Kmeans=km.sort_values(by=\"Silhouette Score\",ascending=False)\n        print(\"****************************************************\")\n        print(\"           Sorting the Silhouette Score\")\n        print(\"****************************************************\")\n        print(Kmeans)\n        print(\"\\nHighest K Values is for K=\"+str(Kmeans.iloc[0,0])+\", Silhouette Value=\"+str(Kmeans.iloc[0,1]))\n        a=Kmeans.iloc[0, 0]\n        del km, Kmeans\n        kmeans=cuml.cluster.KMeans(n_clusters=a, init=\"scalable-k-means++\")\n        kmeans=kmeans.fit(data_num)\n        kmeans.cluster_centers_\n        data_num['Clusters'] = kmeans.labels_\n        data_num['Clusters']\n        sns.scatterplot(x=x, y=y,hue = 'Clusters',  data=data_num)\n        plt.title(str(x)+\" vs \"+str(y))\n        plt.savefig(str(x)+' vs '+str(y)+'.jpg')\n        plt.figure(clear=True)\n      \nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","a0b02add":"# End time of the notebook runtime\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n\nend=datetime.datetime.now(IST).replace(microsecond=0)\ne_hrs=end.time()\nprint(\"Notebook ended on:\", end.date(), \"at\", e_hrs)","f231ec7c":"# Total runtime of the notebook\n\nprint(\"Code started on:\", start.date(), \"at\",s_hrs,\"hrs.\\nCode ended on:\", end.date(), \"at\",e_hrs,\"\\n\\nTotal duration of the code runtime:\",(end-start),\"hrs\")","23c38f98":"# Data Merging","40054134":"# Enable GPU Acceleration\n## Input Rapids Data File (For Kaggle):\n\n### Add Data\n### Search by URL\n### Paste the URL: https:\/\/www.kaggle.com\/cdeotte\/rapids\n### Add","dee2b2ac":"\n# For Jupyter Notebook\n\n%%time\n\ndir=\"G:\\\\IIM-IPBA\\\\Capstone Project\\\\Dataset\"\n\nos.chdir(dir)\n\naisles=pd.read_csv(\"1.csv\")\n\ndept=pd.read_csv(\"2.csv\")\n\norders=pd.read_csv(\"5.csv\")\n\nprod=pd.read_csv(\"6.csv\")\n\nord_prior=pd.read_csv(\"3.csv\")","2dabf26b":"# Department Id","64ce90a3":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n\n# Enable GPU Acceleration\n\n## For Google Colab\n\n!nvidia-smi\n\n## Install RAPIDS\n!git clone https:\/\/github.com\/rapidsai\/rapidsai-csp-utils.git\n!bash rapidsai-csp-utils\/colab\/rapids-colab.sh stable\n\nimport sys, os, shutil\n\nsys.path.append('\/usr\/local\/lib\/python3.7\/site-packages\/')\nos.environ['NUMBAPRO_NVVM'] = '\/usr\/local\/cuda\/nvvm\/lib64\/libnvvm.so'\nos.environ['NUMBAPRO_LIBDEVICE'] = '\/usr\/local\/cuda\/nvvm\/libdevice\/'\nos.environ[\"CONDA_PREFIX\"] = \"\/usr\/local\"\nfor so in ['cudf', 'rmm', 'nccl', 'cuml', 'cugraph', 'xgboost', 'cuspatial']:\n  fn = 'lib'+so+'.so'\n  source_fn = '\/usr\/local\/lib\/'+fn\n  dest_fn = '\/usr\/lib\/'+fn\n  if os.path.exists(source_fn):\n    print(f'Copying {source_fn} to {dest_fn}')\n    shutil.copyfile(source_fn, dest_fn)\n\nif not os.path.exists('\/usr\/lib64'):\n    os.makedirs('\/usr\/lib64')\nfor so_file in os.listdir('\/usr\/local\/lib'):\n  if 'libstdc' in so_file:\n    shutil.copyfile('\/usr\/local\/lib\/'+so_file, '\/usr\/lib64\/'+so_file)\n    shutil.copyfile('\/usr\/local\/lib\/'+so_file, '\/usr\/lib\/x86_64-linux-gnu\/'+so_file)\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","f722114a":"# All the merging has been completed\n\n# Grouping the data for various features using Pivot table","1c1ad296":"# All commands finished","84e25f3e":"# Standard Imports","dc4b5ad0":"# END"}}