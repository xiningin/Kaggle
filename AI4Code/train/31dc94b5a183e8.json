{"cell_type":{"b94d9064":"code","f037dbe6":"code","b5c4935c":"code","1cb405c2":"code","a59db095":"code","1024693e":"code","cee38328":"code","3ad9b83b":"code","12732af4":"code","48daccf8":"code","d4d1bfae":"code","f31c2859":"markdown"},"source":{"b94d9064":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f037dbe6":"\"\"\"loading the pre-processed training and testing dataset \"\"\"\n\ntrain_df  = pd.read_csv('\/kaggle\/input\/preprocessed\/train_cleaned.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/preprocessed\/test_cleaned.csv')","b5c4935c":"train_df.head()","1cb405c2":"# Converting the raw text  to numeric tensors using pretrained glove 100d word embeddings (word level tokenization)\n\nimport keras\nimport numpy as np\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n\ntokenizer = Tokenizer()                              \ntokenizer.fit_on_texts(pd.concat([train_df['text_cleaned'],test_df['text_cleaned']]))   # builds the word index \nword_index = tokenizer.word_index\nprint('The size of the vocabulary in the training and testing set is : %d'%(len(word_index)))\nvocab_size = max_features = len(word_index) \n\n# word_index.items()","a59db095":"# getting the 100d glove representation\nembedding_dict_100d={}\nwith open('\/kaggle\/input\/6b-glove\/glove.6B.100d.txt','r', encoding='utf8') as f:\n    for line in f:\n        values=line.split(' ')\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict_100d[word]=vectors\nf.close()\n\nprint('The size of the embedding index is : %d'%(len(embedding_dict_100d)))","1024693e":"# creating a glove word embedding matrix and setting the Embedding layer weights to the pretrained embedding matrix\n\nembedding_dim  = 100\n\nembedding_matrix = np.zeros((max_features+1, embedding_dim))\n\nfor word, i in word_index.items():\n  if i < max_features:\n    \n    try:\n      embedding_matrix[i] =embedding_dict_100d[word]\n    except KeyError:  \n      embedding_matrix[i] = np.zeros((1, embedding_dim))\nembedding_matrix.shape","cee38328":"# creating the model architecture\n\nfrom keras.models import Sequential\nfrom keras.layers import Embedding , SimpleRNN, Dense, LSTM, SpatialDropout1D\nfrom keras.initializers import Constant\nmaxlen = 25\n\nmodel = Sequential()\nmodel.add(Embedding(max_features+1,embedding_dim, embeddings_initializer=Constant(embedding_matrix),\n                    input_length=maxlen,\n                    trainable=False))\n\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(50, return_sequences=True))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(50))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","3ad9b83b":"# model is ready. next step is to prepare the data for feeding into the network.\n# setting batch_size, maximum length allowed for a sequence \nfrom keras.preprocessing import sequence\nfrom sklearn.model_selection import train_test_split\n\nbatch = 64\n\ntraining_samples = 7000\nvalidation_samples = 613\n\nsequences = tokenizer.texts_to_sequences(train_df['text_cleaned'])  # transform each tweets in to a sequence of integers\ndata = pad_sequences(sequences, maxlen=maxlen, truncating='post', padding='post')  # (longer sequence than threshold will be truncated and shorter sequence will be padded with zeros)\nlabels = np.asarray(train_df['target'])\n\nprint('Shape of label tensor: %d' %(labels.shape))\nprint('Shape of data tensor:', data.shape)\n\nx_train,x_val,y_train,y_val = train_test_split(data, labels, test_size = 0.050,random_state = 0, shuffle = True ) ","12732af4":"print('Shape of x_train tensor:', x_train.shape)\nprint('Shape of x_val tensor:', x_val.shape)\nprint('Shape of y_train tensor:', y_train.shape)\nprint('Shape of y_val tensor:', y_val.shape)","48daccf8":"#  training and evaluation \nfrom keras.optimizers import Adam\n\nmodel.compile(optimizer = Adam(lr=0.0001), loss='binary_crossentropy', metrics=['acc'] )\n\nhistory = model.fit(x_train, y_train,epochs=50  , batch_size=64, validation_data=(x_val, y_val),verbose=2)\nmodel.save_weights('pre_trained_glove_model.h5')","d4d1bfae":"test_sequences = tokenizer.texts_to_sequences(test_df['text_cleaned'])  # transform each tweets in to a sequence of integers\ntest_data = pad_sequences(test_sequences, maxlen=maxlen, truncating='post', padding='post')  # (longer sequence than threshold will be truncated and shorter sequence will be padded with zeros)\n\npredictions = model.predict(test_data)\npredictions = np.round(predictions).astype(int).reshape(3263)\n\n# Creating submission file \nsubmission = pd.DataFrame({'id' : test_df['id'], 'target' : predictions})\nsubmission.to_csv('final_submission.csv', index=False)\n\nsubmission.head()\n","f31c2859":"This notebook is inspired by \n1. http:\/\/jalammar.github.io\/illustrated-word2vec\/. Jay's blog does an incredible job of making visualization of the concepts on the word embeddings\n2. Deep learning Course 5 (Sequence Models) by Andrew Ng"}}