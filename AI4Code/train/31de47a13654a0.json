{"cell_type":{"4100f363":"code","f519b588":"code","6356102a":"code","ae5dc506":"code","c2c12269":"code","a90b19e6":"code","40e0bf2e":"code","be0415d9":"code","dbcfa7e6":"code","f2376513":"code","b30e0d7b":"code","8d919af1":"code","9b859f62":"code","faab7edd":"code","9960624f":"code","4836db5e":"code","60920f93":"code","831610d6":"code","ad623ae7":"code","bcce5983":"code","dfdfa242":"markdown","8a441007":"markdown","044a87a0":"markdown","2af787a8":"markdown","af64197c":"markdown","cfc9d84b":"markdown","eb8bc510":"markdown","e5e3539b":"markdown","ce3acc44":"markdown","33c486c3":"markdown","3d2a3dea":"markdown","3fd726c2":"markdown","ec44dc2b":"markdown","a8022bb1":"markdown","5c828f1c":"markdown","645812fc":"markdown","4f7f17d8":"markdown","7a37abbe":"markdown","7c359f48":"markdown","a25bcf1d":"markdown","f5c599ad":"markdown","563b9d6c":"markdown","e9a1f8f8":"markdown","e0a546a0":"markdown"},"source":{"4100f363":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # visualization\nimport matplotlib.pyplot as plt \n\n!pip install pandas-bokeh\nimport pandas_bokeh\npandas_bokeh.output_notebook()\npd.set_option('plotting.backend', 'pandas_bokeh')\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.models.widgets import DataTable, TableColumn\n\nfrom sklearn.preprocessing import OneHotEncoder \nfrom sklearn import preprocessing\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve, auc, plot_roc_curve\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f519b588":"heart = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndisplay(heart.head(3))\ndisplay(heart.describe())","6356102a":"heart[['age','output']].groupby(['output']).count().rename(columns = {'age':'count of output values'})","ae5dc506":"heart[[\"sex\", \"output\"]].groupby(['sex'], as_index=False).agg(['count', 'mean'])","c2c12269":"heart.corr()[['output']].multiply(100).T.applymap('{:.2f}%'.format)","a90b19e6":"f = plt.figure(figsize=(12, 12))\nplt.matshow(heart.corr(), fignum=f.number)\nplt.xticks(range(heart.select_dtypes(['number']).shape[1]), heart.select_dtypes(['number']).columns, fontsize=14, rotation=45)\nplt.yticks(range(heart.select_dtypes(['number']).shape[1]), heart.select_dtypes(['number']).columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\nplt.title('Correlation Matrix', fontsize=16);","40e0bf2e":"corr = heart.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","be0415d9":"sns.pairplot(heart, hue=\"output\")","dbcfa7e6":"def thall_rename(thall):\n    if thall==0:\n        return 'Unknown'\n    if thall==1:\n        return 'Fixed Defect'\n    if thall==2:\n        return 'Normal'\n    return 'Reversable Defect'\n\ndef output_rename(output):\n    if output==0:\n        return '0 - Non-critical Patient'\n    else:\n        return '1 - Heart Attack Diagnosis'\n\n###############\n# Data Prep \n###############\ndf = heart.copy()\ndf.thall = df.thall.apply(lambda x: thall_rename(x))\ndf.output = df.output.apply(lambda x: output_rename(x))\n\ndf = df.filter(['thalachh','age','thall','output'])#.groupby(['thall']).count()#.agg(['mean','count'])\n\n###############\n# Making the Plot \n###############\ndata_table = DataTable(\n    columns=[TableColumn(field=Ci, title=Ci) for Ci in df.columns],\n    source=ColumnDataSource(df),\n    width=300,\n    height=300,\n)\n\np_scatter = df.plot_bokeh.scatter(\n    x=\"age\",\n    y=\"thalachh\",\n    category=\"output\",\n    title=\"Correlation between Age\/Thalachh values to Heart Attack Diagnosis\",\n    show_figure=False,\n)\npandas_bokeh.plot_grid([[data_table, p_scatter]], plot_width=350, plot_height=450)","f2376513":"df = heart.copy()\ndf = df.filter(['thall','output'])\ndf.thall = df.thall.apply(lambda x: thall_rename(x))\ndf['1 - Heart Attack Diagnosis'] = df.output #df[df['output'] == 1].filter('output')\ndf = df.rename(columns = {'output': '0 - Non-critical Patient'})\ndf[\"0 - Non-critical Patient\"] = df[\"0 - Non-critical Patient\"].replace({0:1, 1:0})\ndf = df.groupby('thall').count()\n\n\np_stacked_bar = df.plot_bokeh.bar(\n    ylabel=\"Price per Unit [\u20ac]\",\n    title=\"Fruit prices per Year\",\n    stacked=True,\n    alpha=0.6)","b30e0d7b":"heart_categories = heart[['sex', 'cp', 'fbs', 'restecg', 'exng', 'slp', 'caa', 'thall']]\n\nenc = OneHotEncoder()\nenc.fit(heart_categories)\nonehotlabels = enc.transform(heart_categories).toarray()\n\n#enc.inverse_transform(onehotlabels) # To convert them back to the original shape\n\ncolumns = enc.get_feature_names(['sex', 'cp', 'fbs', 'restecg', 'exng', 'slp', 'caa', 'thall'])\nheart_categories = pd.DataFrame(onehotlabels, columns=columns)\n\nheart = heart.drop(['sex', 'cp', 'fbs', 'restecg', 'exng', 'slp', 'caa', 'thall'], axis=1)\nheart = heart.join(heart_categories)","8d919af1":"heart.head()","9b859f62":"#display(heart[heart.duplicated()])\nheart = heart.drop_duplicates()","faab7edd":"y = np.ravel(heart[['output']])\nX = heart.loc[:, heart.columns != 'output']\nX = SelectKBest(chi2, k=23).fit_transform(X, y)\n\ndisplay(heart.corr()[['output']].multiply(100).abs().sort_values(by = ['output']).applymap('{:.2f}%'.format).T)\n\ntemp = pd.DataFrame(X).merge(pd.DataFrame(y), left_index=True, right_index=True)\ndisplay(temp.corr()[['0_y']].multiply(100).abs().sort_values(by = ['0_y']).applymap('{:.2f}%'.format).T)","9960624f":"scaler = preprocessing.StandardScaler().fit(X)\nX = scaler.transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","4836db5e":"rfc1 = RandomForestClassifier()\nrfc2 = RandomForestClassifier(n_estimators=500)\nrfc3 = RandomForestClassifier(n_estimators=1000)\nrfc4 = RandomForestClassifier(criterion='entropy')\nrfc5 = RandomForestClassifier(n_estimators=500, criterion='entropy')\n\nlgc1 = LogisticRegression()\nlgc2 = LogisticRegression(penalty='none')\nlgc3 = LogisticRegression(solver='liblinear')\nlgc4 = LogisticRegression(solver='newton-cg')\nlgc5 = LogisticRegression(solver='saga', max_iter=500)\nlgc6 = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=.1, max_iter=500)\nlgc7 = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=.5, max_iter=500)\nlgc8 = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=.9, max_iter=500)\n\nsvm1 = svm.SVC()\nsvm2 = svm.SVC(C=.1)\nsvm3 = svm.SVC(C=.5)\nsvm4 = svm.SVC(C=2)\nsvm5 = svm.SVC(kernel = 'linear')\nsvm6 = svm.SVC(kernel = 'poly')\nsvm7 = svm.SVC(kernel = 'sigmoid')\n\ngbc1 = GradientBoostingClassifier()\ngbc2 = GradientBoostingClassifier(learning_rate = 0.05, max_depth=2, n_estimators=50)\ngbc3 = GradientBoostingClassifier(learning_rate = 0.05, max_depth=2, n_estimators=60)\ngbc4 = GradientBoostingClassifier(loss = 'exponential')\ngbc5 = GradientBoostingClassifier(loss = 'exponential', learning_rate = .05, max_depth=2, n_estimators = 50)\ngbc6 = GradientBoostingClassifier(loss = 'exponential', learning_rate = .05, max_depth=2, n_estimators = 60)\n\nmodels = [rfc1, rfc2, rfc3, rfc4, rfc5,\n          lgc1, lgc2, lgc3, lgc4, lgc5, lgc6, lgc7, lgc8,\n          svm1, svm2, svm3, svm4, svm5, svm6, svm7,\n          gbc1, gbc2, gbc3, gbc4, gbc5, gbc6]","60920f93":"'''\nCompare several models to see which performs the best.\n\nRETURNS:\n    model_df - pandas dataframe with the mean scores for each model.\n    \nPARAMETERS:\n    model_arr (REQ) - list of models to compare.\n    X_train (REQ) - independent variables for the training data\n    y_train (REQ) - dependent variables for the training data\n    num_of_folds (OPT) - number of times to perform KFold cross validation. Default is 5.\n'''\ndef compare_models(model_arr, X_train, y_train, num_of_folds = 5):\n    assert (model_arr is not None),\"Must provide a list of models to test.\"\n    assert (X_train is not None and isinstance(X_train,(np.ndarray))),\"Must provide X_train as a numpy array.\"\n    assert (y_train is not None and isinstance(y_train,(np.ndarray))),\"Must provide y_train as a numpy array.\"\n    from collections import defaultdict # Future thought: use defaultdict to avoid KeyError\n\n    kf = KFold(n_splits = num_of_folds)\n    accuracy_dict = {}\n    sensitivity_dict = {}\n    specificity_dict = {}\n    f1_dict = {}\n\n    for train_index, test_index in kf.split(X_train):\n\n        k_X_train = X_train[train_index]\n        k_y_train = y_train[train_index]\n        k_X_test = X_train[test_index]\n        k_y_test = y_train[test_index]\n\n        for model in model_arr:\n    \n            model.fit(k_X_train, k_y_train)\n            y_pred = model.predict(k_X_test)\n            accuracy = accuracy_score(y_pred, k_y_test)\n\n            cm = confusion_matrix(k_y_test, y_pred)\n            tn = cm[0][0]\n            fp = cm[0][1]\n            fn = cm[1][0]\n            tp = cm[1][1]\n            sensitivity = tp \/ (fn + tp)\n            specificity = tn \/ (fp + tn)\n            f1 = f1_score(k_y_test, y_pred)\n\n            try:\n                accuracy_dict[model] = np.append(accuracy_dict[model], accuracy)\n                sensitivity_dict[model] = np.append(sensitivity_dict[model], sensitivity)\n                specificity_dict[model] = np.append(specificity_dict[model], specificity)\n                f1_dict[model] = np.append(f1_dict[model], f1)\n            except KeyError:\n                accuracy_dict[model] = np.array(accuracy)\n                sensitivity_dict[model] = np.array(sensitivity)\n                specificity_dict[model] = np.array(specificity)\n                f1_dict[model] = np.array(f1)\n    \n    \n    # Compile the score dictionaries into a pandas dataframe.\n    \n    model_df = pd.DataFrame(columns = ['model', 'accuracy', 'sensitivity', 'specificity', 'f1'])\n    \n    for model in model_arr:\n        accuracy = accuracy_dict[model].mean() * 100\n        sensitivity = sensitivity_dict[model].mean() * 100\n        specificity = specificity_dict[model].mean() * 100\n        f1 = f1_dict[model].mean() * 100\n        \n        row = {'model' : model,\n               'accuracy' : accuracy,\n               'sensitivity' : sensitivity,\n               'specificity' : specificity,\n               'f1' : f1}\n                \n        model_df = model_df.append(row, ignore_index=True)\n    \n    return (model_df)\n\n\nmodel_df = compare_models(model_arr = models, X_train=X_train, y_train=y_train).sort_values('accuracy', ascending=False)\npd.set_option(\"precision\", 2)\ndisplay(model_df)\nmodel_df.describe()","831610d6":"model_list = model_df.model.tolist()\ntop_x_models = 3\n\nfor i in range(top_x_models):\n    model = model_list[i]\n    plot_roc_curve(model, X_test, y_test)  \n    plt.show() ","ad623ae7":"vc = VotingClassifier(estimators=[('svm', svm2), ('rf', rfc4), ('lr', lgc1), ('gb',gbc5)], voting='hard')\nvc.fit(X_train,y_train)\ny_pred = vc.predict(X_test)\naccuracy_score(y_pred, y_test)","bcce5983":"cm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=vc.classes_)\ndisp.plot() ","dfdfa242":"<a id=\"corr\"><\/a>\n## Correlation Between Variables","8a441007":"Pairplot shows a lot of information that it can be hard to process it all. Something that stood out to me is that instead of missing data we have 0's in some rows where that isn't valid for the column such as 'slp' and 'thall'. ","044a87a0":"Less females in the dataset, and they are significantly more likely to have a heart attack diagnosis.","2af787a8":"Correlation between the output and the independent variables:","af64197c":"<a id=\"feature_select\"><\/a>\n## Feature Selection","cfc9d84b":"Keeping the k=23 best predictors to reduce noise.","eb8bc510":"Variables that are categories: 'sex', 'cp', 'fbs', 'restecg', 'exng', 'slp', 'caa', 'thall'","e5e3539b":"Slight bias by having more datapoints with a heart attack diagnosis than not.","ce3acc44":"<a id=\"prep\"><\/a>\n# Data Preparation for Modeling\n<a id=\"1hot\"><\/a>\n## One Hot Encoding\nSince there are a lot of predictors that have numeric values, but are truly categories (e.g. Female [0] vs Male [1]), I'm going to use one hot encoding so that the machine learning algortithms don't try to use the order of the numbers as an attribute of significance (i.e. treat a higher number as more significant as a lower number). \n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html","33c486c3":"<a id=\"model\"><\/a>\n# Modeling\n<a id=\"bob_the_builder\"><\/a>\n## Building the models\n\nI tried several combinations of the following sklearn models: <br>\n    <li>RandomForestClassifier\n    <li>LogisticRegression\n    <li>SVC\n    <li>GradientBoostingClassifier","3d2a3dea":"There is one duplicate row so I removed that:","3fd726c2":"<a id=\"fin\"><\/a>\n## Fin! \nThank you for reading my notebook, feedback is much appreciated!","ec44dc2b":"<a id=\"americas_next_top_model\"><\/a>\n## Model Selection\nThere are _**a lot**_ of ways to measure how well a model is performing. I'm going to focus on the following:\n<li>Accuracy = # of Correct Predictions \/ Total Predictions <br>\n<li>Sensitivity (True Positive Rate) = TP \/ (FN + TP)  <br>\n<li>Specificity (True Negative Rate) = TN \/ (FP + TN) <br>\n<li>F1 Score = 2 * ( 1 \/ ((1\/precision) + (1\/recall)) )  <br>\n    \n<br>\nI used KFold cross validation with 5 folds to help account for the randomness in the data.\n   \n   ","a8022bb1":"<a id=\"split\"><\/a>\n## Splitting the data\nGoing with the classic kfold cross validation to split the data.\n\nKnowing nothing about the data collection process, it is not really safe to assume i.i.d. though so a future test could be using special techniques for grouped data - https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html#cross-validation-iterators-for-grouped-data","5c828f1c":"Plotting the ROC for the top 3 models:","645812fc":"# Table of Contents\n* [Analyzing the Data](#analyze)\n* [Visualization](#viz)\n    - [Correlation Between Variables](#corr)\n    - [Pandas Bokeh](#pandas_bokeh)\n* [Data Preperation](#prep)\n    - [One Hot Encoding](#1hot)\n    - [Standardization](#standard)\n    - [Feature Selection](#feature_select)\n* [Modeling](#model)\n    - [Building the models](#bob_the_builder)\n    - [Model Selection](#americas_next_top_model)\n* [Conclusion](#section-three)","4f7f17d8":"<a id=\"standardize\"><\/a>\n## Standarizing the Data\nUsing sklearn's StandardScaler to scale the data <br>\nhttps:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html","7a37abbe":"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/style.html","7c359f48":"From the heatmaps we can see that there is some multicollinearity between the variables, i.e. correlation between the independent variables. It is worth making note of so we can handle it with regularization in our models below. ","a25bcf1d":"Finally, lets combine the best models and have them vote to make a prediction: ","f5c599ad":"<a id=\"analyze\"><\/a>\n# Analyzing the Data\nFirst taking a look at some rows of the data and looking at the variable terms:\n1. age - age in years\n\n2. sex - sex <br>\n    1 = male <br>\n    0 = female\n\n3. cp - chest pain type <br>\n    0 = typical angina <br>\n    1 = atypical angina <br>\n    2 = non-anginal pain <br>\n    3 = asymptomatic\n\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n\n5. chol - serum cholestoral in mg\/dl\n\n6. fbs - fasting blood sugar > 120 mg\/dl  <br>\n    1 = true <br>\n    0 = false\n\n7. restecg - resting electrocardiographic results  <br>\n    0 = normal <br>\n    1 = having ST-T <br>\n    2 = hypertrophy\n\n8. thalach - maximum heart rate achieved\n\n9. exng - exercise induced angina <br>\n    1 = yes <br>\n    0 = no\n\n10. oldpeak - ST depression induced by exercise relative to rest\n\n11. slp - the slope of the peak exercise ST segment <br>\n    1 = upsloping<br>\n    2 = flat<br>\n    3 = downsloping\n\n12. caa - number of major vessels (0-3) colored by flourosopy\n\n13. thall <br>\n    1 = fixed defect<br>\n    2 = normal<br>\n    3 = reversable defect\n    \n14. output - the predicted attribute - diagnosis of heart disease (angiographic disease status) <br>\n    Value 0 = < diameter narrowing<br>\n    Value 1 = > 50% diameter narrowing","563b9d6c":"**Note:** I have grown as a data scientist since making this. Looking back I see I made some beginner mistakes such as standardizing and performing feature selection *before* splitting the data. For a real use case, it is recommended to perform these steps *after* splitting to avoid [leaking](https:\/\/machinelearningmastery.com\/data-leakage-machine-learning\/).","e9a1f8f8":"<a id=\"viz\"><\/a>\n# Visualization","e0a546a0":"<a id=\"pandas_bokeh\"><\/a>\n## Pandas Bokeh\nI'm a big fan of interactive plots so I love bokeh. To make things easier, there is a pandas-bokeh library available. See the documention here: https:\/\/github.com\/PatrikHlobil\/Pandas-Bokeh"}}