{"cell_type":{"c024297e":"code","c6f6231c":"code","70dcd68c":"code","755bb6fa":"code","157f4557":"code","53694894":"code","cf960c8f":"code","fc8ed7e4":"code","57e6228c":"code","eb146e8a":"code","5841efb3":"code","ee58b0a1":"code","7b810bcc":"code","cf4f4645":"code","da4adc5a":"code","695fadd7":"code","6c2593bf":"code","13d852c3":"code","cda8ebc9":"code","d7e9164c":"code","827f216c":"code","0d1c00c0":"code","4fa3b3cd":"code","09f59b7f":"code","2744a6de":"code","dc8f2e85":"code","aacf6890":"code","753afd8e":"code","ede929e2":"code","e4fb6997":"code","592abe5f":"markdown","8364c161":"markdown"},"source":{"c024297e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c6f6231c":"# Some libraries for working in python ................\nimport pandas as pd                 \nimport numpy as np\nimport matplotlib.pyplot as plt   \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import linear_model","70dcd68c":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","755bb6fa":"print(\"Train data shape:\", train.shape)\nprint(\"Test data shape:\", test.shape)","157f4557":"print(train.head())","53694894":"plt.style.use(style='ggplot')\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint (train.SalePrice.describe())","cf960c8f":"print (\"Skew is:\", train.SalePrice.skew())\nplt.hist(train.SalePrice, color='blue')\nplt.show()","fc8ed7e4":"target = np.log(train.SalePrice)\nprint (\"\\n Skew is:\", target.skew())\nplt.hist(target, color='blue')\nplt.show()","57e6228c":"numeric_features = train.select_dtypes(include=[np.number])\ncorr = numeric_features.corr()\nprint (corr['SalePrice'].sort_values(ascending=False)[:5], '\\n')\nprint (corr['SalePrice'].sort_values(ascending=False)[-5:])","eb146e8a":"plt.scatter(x=train['GarageArea'], y=target)\nplt.ylabel('Sale Price')\nplt.xlabel('Garage Area')\nplt.show()","5841efb3":"train = train[train['GarageArea'] < 1200]\n\nplt.scatter(x=train['GarageArea'], y=np.log(train.SalePrice))\nplt.xlim(-200,1600)     # This forces the same scale as before\nplt.ylabel('Sale Price')\nplt.xlabel('Garage Area')\nplt.show()","ee58b0a1":"nulls = pd.DataFrame(train.isnull().sum().sort_values(ascending=False)[:25])\nnulls.columns = ['Null Count']\nnulls.index.name = 'Feature'\n#nulls\nprint(nulls)","7b810bcc":"categoricals = train.select_dtypes(exclude=[np.number])\n#categoricals.describe()\nprint(categoricals.describe())","cf4f4645":"print (\"Original: \\n\")\nprint (train.Street.value_counts(), \"\\n\")","da4adc5a":"train['enc_street'] = pd.get_dummies(train.Street, drop_first=True)\ntest['enc_street'] = pd.get_dummies(test.Street, drop_first=True)","695fadd7":"print ('Encoded: \\n')\nprint (train.enc_street.value_counts())","6c2593bf":"condition_pivot = train.pivot_table(index='SaleCondition', values='SalePrice', aggfunc=np.median)\ncondition_pivot.plot(kind='bar', color='blue')\nplt.xlabel('Sale Condition')\nplt.ylabel('Median Sale Price')\nplt.xticks(rotation=0)\nplt.show()","13d852c3":"def encode(x): return 1 if x == 'Partial' else 0\ntrain['enc_condition'] = train.SaleCondition.apply(encode)\ntest['enc_condition'] = test.SaleCondition.apply(encode)\n\ncondition_pivot = train.pivot_table(index='enc_condition', values='SalePrice', aggfunc=np.median)\ncondition_pivot.plot(kind='bar', color='blue')\nplt.xlabel('Encoded Sale Condition')\nplt.ylabel('Median Sale Price')\nplt.xticks(rotation=0)\nplt.show()","cda8ebc9":"data = train.select_dtypes(include=[np.number]).interpolate().dropna()\n\n# sum(data.isnull().sum() != 0)\nprint(sum(data.isnull().sum() != 0))","d7e9164c":"y = np.log(train.SalePrice)\nX = data.drop(['SalePrice', 'Id'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)\nlr = linear_model.LinearRegression()\nmodel = lr.fit(X_train, y_train)","827f216c":"print(\"R^2 is: \\n\", model.score(X_test, y_test))","0d1c00c0":"predictions = model.predict(X_test)\n\nprint('RMSE is: \\n', mean_squared_error(y_test, predictions))\n\nactual_values = y_test\nplt.scatter(predictions, actual_values, alpha=.75,\n            color='b')  # alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Linear Regression Model')\nplt.show()","4fa3b3cd":"for i in range (-2, 3):\n    alpha = 10**i\n    rm = linear_model.Ridge(alpha=alpha)\n    ridge_model = rm.fit(X_train, y_train)\n    preds_ridge = ridge_model.predict(X_test)\n\n    plt.scatter(preds_ridge, actual_values, alpha=.75, color='b')\n    plt.xlabel('Predicted Price')\n    plt.ylabel('Actual Price')\n    plt.title('Ridge Regularization with alpha = {}'.format(alpha))\n    overlay = 'R^2 is: {}\\nRMSE is: {}'.format(\n                    ridge_model.score(X_test, y_test),\n                    mean_squared_error(y_test, preds_ridge))\n    plt.annotate(s=overlay,xy=(12.1,10.6),size='x-large')\n    plt.show()","09f59b7f":"from sklearn.model_selection import train_test_split , KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\nX_train, X_test, y_train, y_test = train_test_split(data.drop('SalePrice', axis=1), data['SalePrice'], test_size=0.2, random_state=42)","2744a6de":"X_train, X_test, y_train, y_test = train_test_split(data.drop('SalePrice', axis=1), data['SalePrice'], test_size=0.2, random_state=42)","dc8f2e85":"import xgboost\n\nxgb = xgboost.XGBRegressor(colsample_bytree=0.4,\n                 gamma=0,                 \n                 learning_rate=0.07,\n                 max_depth=3,\n                 min_child_weight=1.5,\n                 n_estimators=10000,                                                                    \n                 reg_alpha=0.75,\n                 reg_lambda=0.45,\n                 subsample=0.6,\n                 seed=42)\nxgb.fit(X_train,y_train)\ny_test_pred_2 = xgb.predict(X_test)\ny_train_pred_2= xgb.predict(X_train)","aacf6890":"print('MSE train: %.3f, test: %.3f' % (\n        mean_squared_error(y_train, y_train_pred_2),\n        mean_squared_error(y_test, y_test_pred_2)))\nprint('R^2 train: %.3f, test: %.3f' % (\n        r2_score(y_train, y_train_pred_2),\n        r2_score(y_test, y_test_pred_2)))\n","753afd8e":"submission = pd.DataFrame()\nsubmission['Id'] = test.Id\nfeats = test.select_dtypes(\n    include=[np.number]).drop(['Id'], axis=1).interpolate()\npredictions = model.predict(feats)\nfinal_predictions = np.exp(predictions)\n\nprint(\"Original predictions are: \\n\", predictions[:10], \"\\n\")\nprint(\"Final predictions are: \\n\", final_predictions[:10])\n","ede929e2":"submission['SalePrice'] = final_predictions\nprint(submission.head())","e4fb6997":"submission.to_csv('submission1.csv', index=False)\nprint('Submission saved!')\n","592abe5f":"# Final Prediction analysis","8364c161":"## XGBoost"}}