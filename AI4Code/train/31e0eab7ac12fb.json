{"cell_type":{"4f372b08":"code","82ed204b":"code","fc766090":"code","ea898476":"code","eace04d5":"code","5b3d92c5":"code","3d4b04e8":"code","82f8701b":"code","96bc1150":"code","77584b71":"code","d5389c70":"code","60c212b5":"code","e1f5ac62":"code","d1e39b3e":"code","78345af4":"code","b7385844":"code","8dc25905":"code","39114ea1":"code","1888550e":"code","00d985fd":"code","aaef327a":"code","01dd4ab6":"code","6228d7fb":"code","248411d9":"markdown","7295240e":"markdown","f124e1a4":"markdown","b037554a":"markdown","cfcdfb37":"markdown","0a69c359":"markdown","6b3c040e":"markdown","10f3522b":"markdown","4f659f09":"markdown","6cbb9d21":"markdown","b6877127":"markdown","dbba05d1":"markdown","1e3cc7c9":"markdown","f70282df":"markdown"},"source":{"4f372b08":"import numpy as np \nimport pandas as pd\npd.plotting.register_matplotlib_converters()\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(7)\n\n\n# reading data into dataframe - train dataset, test dataset\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col='PassengerId')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col='PassengerId')\n","82ed204b":"train.head()","fc766090":"train.shape, test.shape","ea898476":"train.info()","eace04d5":"# missing values in the train dataset\n\ndata_na = train.isna().sum()\ndata_na[data_na>0]","5b3d92c5":"# missing values in test dataset\n\ntest_na = test.isna().sum()\ntest_na[test_na>0]","3d4b04e8":"train.drop(['Name','Cabin','Ticket'], axis=1, inplace=True)\ntest.drop(['Name', 'Cabin', 'Ticket'], axis=1, inplace=True)","82f8701b":"fig, ((ax1, ax2),(ax3, ax4),(ax5, ax6)) = plt.subplots(3,2, figsize=(12,8))\nsns.set_style(\"darkgrid\")\nsns.distplot(train['Age'], color = 'green', kde=False, bins=30, ax=ax1)\nsns.distplot(test['Age'], color = 'purple', kde=False, bins=30, ax=ax2)\nsns.swarmplot(train['Sex'], train['Age'], palette='dark', ax=ax3)\nsns.swarmplot(test['Sex'], test['Age'], palette ='rocket', ax=ax4)\nsns.boxplot(train['Sex'], train['Age'], palette='dark', ax=ax5)\nsns.boxplot(test['Sex'], test['Age'], palette ='rocket', ax=ax6)\nax1.set_title('Distribution of \"Age\" in train dataset', color='red')\nax2.set_title('Distribution of \"Age\" in test dataset', color='red')\nax3.set_title('\"Age\" in train dataset, by \"Sex\"', color='red')\nax4.set_title('\"Age\" in test dataset, by \"Sex\"', color='red')\nplt.tight_layout()\nplt.show()\n","96bc1150":"# Concatenate train and test set to optimally fill missing values\ndf = pd.concat([train, test],sort=True).reset_index(drop=True)\ndf.index = df.index+1","77584b71":"\nfor i, x in enumerate(df.columns):\n    if (i%2 == 0):\n        f,ax = plt.subplots(1,2, figsize=(12,5))\n        sns.set_style('darkgrid')\n        plt.figure(figsize=(6,4))\n    sns.scatterplot(df[x], df['Age'], hue=df['Parch'], palette='dark', ax=ax[i%2])\n    ax[i%2].set_ylabel('Age')\n    ax[i%2].set_xlabel(str(x))\n    if (i%2 == 1):\n        plt.show()\n","d5389c70":"sns.set_style('whitegrid')\nplt.figure(figsize=(14,6))\nsns.scatterplot(y=df['SibSp'], x=df['Age'], hue=df['Parch'], palette='Set1')\nplt.legend()\nplt.show()","60c212b5":"df_new = df.groupby(['Parch','SibSp']).Age.describe().reset_index()\n\nprint('*'*75,'\\nStatistical Insight on feature \"Age\"\\n',  df.groupby(['Parch','SibSp']).Age.describe())\n","e1f5ac62":"sns.set_style('darkgrid')\ni=list(np.arange(60,220,20))\nplt.figure(figsize=(12,5))\nsns.scatterplot(x=df_new['mean'], y=df_new['50%'], hue=df_new['SibSp'], size=df_new['Parch'], sizes=i, palette='deep')\nplt.show()\n#\nplt.figure(figsize=(12,5))\nsns.boxplot(y=df['Age'], x=df['SibSp'], palette='deep')\nplt.show()\n#\nplt.figure(figsize=(14,5))\nsns.boxplot(y=df['Age'], x=df['Parch'], hue=df['SibSp'], palette='deep')\nplt.show()\n#\nplt.figure(figsize=(14,5))\nsns.boxplot(y=df['Age'], hue=df['Parch'], x=df['SibSp'], palette='deep')\nplt.show()","d1e39b3e":"df.columns","78345af4":"# Fill missing values\n\nfor i in ['Age', 'Fare']:\n    df[i] = df.groupby(['Parch', 'SibSp'])[i].apply(lambda x: x.fillna(x.median()))\n    df[i] = df.groupby(['SibSp'])[i].apply(lambda x: x.fillna(x.median()))          # to fill missing which cannot be grouped by Parch\ndf['Embarked'] = df.groupby(['SibSp'])['Embarked'].apply(lambda x: x.fillna(x.value_counts().index[0]))    # fill with modal value\n","b7385844":"# Split train and test data as original\ntrain = df.loc[:891]\ntest = df.loc[892:].drop('Survived', axis=1)","8dc25905":"train.shape, test.shape","39114ea1":"train.isnull().sum().sum(), test.isnull().sum().sum()","1888550e":"from sklearn.model_selection import train_test_split\nfrom category_encoders import TargetEncoder\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\n# Split train data into feature data X and target data y\nX = train.drop(['Survived'], axis=1)\ny = train['Survived']\n\n# Get 20% of validation data from train set\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\nX_train.Age.astype('float64')","00d985fd":"import itertools\nnum_col = ['Age', 'Fare']\ncat_col = list(set(X.columns)-set(num_col))\n\n# Generate interaction features with categorical columns\nfor c1, c2 in itertools.combinations(cat_col, 2):\n    print(c1,c2)\n    name = '_'.join([c1,c2])\n    X_train[name] = X_train[c1].map(str)+'_'+X_train[c2].map(str)\n    X_valid[name] = X_valid[c1].map(str)+'_'+X_valid[c2].map(str)\n    test[name] = test[c1].map(str)+'_'+test[c2].map(str)\n\nprint(X_train.columns)\n# Encode categorical variables\nnew_cat_col = [i for i in X_train.columns if X_train[i].dtype != 'float64']\n\nenc = TargetEncoder(cols=new_cat_col)\nX_train[new_cat_col] = enc.fit_transform(X_train[new_cat_col], y_train)\nX_valid[new_cat_col] = enc.transform(X_valid[new_cat_col])\ntest[new_cat_col] = enc.transform(test[new_cat_col])\n","aaef327a":"# Feature Selection\nselector = SelectKBest(f_classif, k=15)\nX_1 = selector.fit_transform(X_train, y_train)\nX_2 = pd.DataFrame(selector.inverse_transform(X_1), index=X_train.index, columns = X_train.columns)\nselect_col = X_2.columns[X_2.var() != 0]\n\nprint(select_col)","01dd4ab6":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=0)\nmodel.fit(X_train[select_col], y_train)\nprediction = model.predict(X_valid[select_col])\n\naccuracy = accuracy_score(y_valid, prediction)\nprint('Validation Accuracy :', accuracy)\nconf = confusion_matrix(y_valid, prediction)\nprint('Confusion matrix: ', conf)","6228d7fb":"# predicting test dataset\nX_full = pd.concat([X_train,X_valid], sort=True)\ny_full = pd.concat([y_train,y_valid], sort=True)\nmodel_full = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=0)\nmodel_full.fit(X_full[select_col], y_full)\nfinal_pred = model_full.predict(test[select_col])\nbinary = final_pred.astype('int64')\noutput = pd.DataFrame({'PassengerId':test.index, 'Survived':binary})\noutput.to_csv('submission.csv', index=False)","248411d9":"### Out of 10 features in train data, 3 have missing values. `Cabin` leads the list with more than half of its data missing (687\/891). `Age` has 177 missing values, quite a number. `Embarked` column has only 2 missing values.","7295240e":"# Step 4: Model Validation","f124e1a4":"### Visualizations are always great in Data Analytics! With this plot, few insights from visual observations can be stated as:\n### 1. If a traveller has 4 or 5 Sibling\/Spouse count, her\/his `Age` is confined within 1 to 18.\n### 2. If a traveller has 1 or 2 Parent\/Children count and 2 or 3 Sibling\/Spouse count, her\/his `Age` is less than 25\n### 3. Most importantly, if Parent\/Children count increases, then Sibling\/Spouse count decreses but `Age` increases.\n\n### With these observations, we can group data according to these critical features and take a statistically significant decision on imputing value of `Age`","b037554a":"# Step 1: Data extracting","cfcdfb37":"# Step 5: Final Modeling and Prediction\n\n### Concate train set and valid set to model with full data set. And save predictions to output file. ","0a69c359":"## Statistical Insights on `Age` feature\n\n* ### Above table has grouped the entire train data set primarily based on counts of Parent\/Children and secondarily based on counts of Sibling\/Spouse. Statistical details such as count of entries, mean, median(50%), quartiles (25% and 75%), standard deviation, minimum and maximum values of entires are presented as grouped.\n\n* ### Mean and median values are strongly correlated and are close to each other is almost every category.\n\n* ### As the count of either `SibSp` or `Parch` increases, standard deviation is smaller with a low range(= maximum - minimum). The less scatter of values and very low count of values might be the reason. Any decision on such groups may become unreliable than it seems.\n\n* ### For most of the groups, standard deviation is a low value. Hence it is advisable that missing values of `Age` can be filled with *mean* value of corresponding group.","6b3c040e":"### Similar to train dataset, column `Cabin` has a huge missing values (327\/418) in the test dataset. `Age` has considerable number of missing data (86\/418). And `Fare` has one missing value, an odd thing compared to train dataset.\n\n### Since feature `Cabin` has enormous number of missing values, the column can be dropped from both train and test datasets. By intuition, it can be said that `Age` is one of the important parameters which decide the survival rate. So `Age` column cannot be dropped at all. On the other hand, filling them with anonymous values may lead to noise in data. \n\n\n### By observing the data, it can be concluded that the `Name` of passenger and `Ticket` Number could not have any effect on survival. Hence we can simply drop those two columns from the datasets.","10f3522b":"# *EXPLORATORY DATA ANALYTICS* on survival prediction in Titanic!\n\n\n-\n### A notebook by **Rajkumar Lakshmanamoorthy** ([https:\/\/www.kaggle.com\/rajkumarl](http:\/\/))\n### using Dataset: [https:\/\/www.kaggle.com\/c\/titanic\/data](http:\/\/)\n","4f659f09":"## **Insight:** \n### `Age` follows some remarkable distribution in both train set and test set. And by intuition, it can be expressed that `Age` plays a major role in the rate of survival. The pattern of distribution of `Age` and its correlation with other features can yield a good solution to impute its missing values.","6cbb9d21":"## **Insight:**\n### From the plots above, we can easily understand that `Age` has good correlation with features `SibSp` and `Parch`. It also has a moderate correlation with `Embarked` feature. And other features fail to exhibit significant correlation with `Age`. Hence by inter-plotting these 3 features `Age`, `SibSp` and `Parch` we surely can generate an idea of how to impute the missing values of `Age`.","b6877127":"### `Age` feature has a quite good number of missing values in both train and test dataset.\n### Observing the `Age` feature through visualizations to get an idea how it is distributed and correlated!","dbba05d1":"# Step 3: Feature Engineering","1e3cc7c9":"# Step 2: Data Preprocessing","f70282df":"### `Survived` column is the target. And the remaining 10 columns form features. Out of 10, 5 are objects data type. Remaining are numericals."}}