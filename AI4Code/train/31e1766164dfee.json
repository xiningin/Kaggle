{"cell_type":{"2cc22e67":"code","9625a48f":"code","ffc8702a":"code","03647f5b":"code","4e6c1546":"code","579360f8":"code","531b172f":"code","188345c0":"code","7c074917":"code","7a3f1f46":"code","e97625fc":"code","41371d51":"code","cdb57d05":"code","84905c24":"code","17009133":"code","ee1310ec":"code","0f8f2c2a":"code","19c1566b":"code","36bf75fc":"code","e1e7c0d0":"code","95a8b500":"code","bd70ba46":"code","d65ea82e":"code","0d58d9e5":"code","dbb6aab2":"code","af9414dd":"code","abab33b7":"code","4d79fc84":"code","22afac1e":"code","f7828c49":"code","dbc3f2a2":"code","6b40f1b3":"code","db36c38d":"code","d99bd6fb":"code","6725d9ce":"code","e353765a":"code","138526eb":"code","6e453a9f":"code","be0242c5":"code","0ec5f102":"code","0d1a2487":"code","c06ce515":"code","5b1826c0":"code","95956350":"code","5899decf":"code","ea03469e":"code","7207796c":"code","26c56f1f":"code","3c4eef43":"markdown","1cc636b2":"markdown","35b574da":"markdown","2bb93b7e":"markdown","e5e245e6":"markdown","e59976ee":"markdown","6e37fab1":"markdown","0672f737":"markdown","01fbf896":"markdown","b609cad0":"markdown","efd017e6":"markdown","321e14f3":"markdown","61e72269":"markdown","e1b6aa36":"markdown","36567379":"markdown","7124affa":"markdown","b771e8b0":"markdown","4d2957b2":"markdown"},"source":{"2cc22e67":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, gc, warnings\nimport random\nimport datetime\n\nfrom tqdm.notebook import tqdm\n# matplotlib and seaborn for plotting\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# import plotly.offline as py\n# py.init_notebook_mode(connected=True)\n# from plotly.offline import init_notebook_mode, iplot\n# init_notebook_mode(connected=True)\n# import plotly.graph_objs as go\n# import plotly.offline as offline\n# offline.init_notebook_mode()\n\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport sklearn\n\nimport lightgbm as lgb\n\nimport pickle\n\nwarnings.filterwarnings('ignore')","9625a48f":"path = '..\/input\/work-df\/'\n# Input data files are available in the \"..\/input\/\" directory.\nfor dirname, _, filenames in os.walk(path):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ffc8702a":"# unimportant features (see importance below)\nunimportant_cols = ['wind_direction', 'wind_speed', 'sea_level_pressure']\ntarget = 'meter_reading'\n\ndef load_data(source='train', path=path):\n    ''' load and merge all tables '''\n    assert source in ['train', 'test']\n    \n    building = pd.read_csv(f'{path}\/building_metadata.csv', dtype={'building_id':np.uint16, 'site_id':np.uint8})\n    weather  = pd.read_csv(f'{path}\/weather_{source}_filled.csv', parse_dates=['timestamp'],\n                                                           dtype={'site_id':np.uint8, 'air_temperature':np.float16,\n                                                                  'cloud_coverage':np.float16, 'dew_temperature':np.float16,\n                                                                  'precip_depth_1_hr':np.float16},\n                                                           usecols=lambda c: c not in unimportant_cols)\n    df = pd.read_csv(f'{path}\/{source}.csv', dtype={'building_id':np.uint16, 'meter':np.uint8}, parse_dates=['timestamp'])\n    df = df.merge(building, on='building_id', how='left')\n    df = df.merge(weather, on=['site_id', 'timestamp'], how='left')\n    return df","03647f5b":"%%time\ntrain = load_data('train')\ntrain.sample(7)","4e6c1546":"%%time\ntest = load_data('test')\ntest.sample(7)","579360f8":"print(f'Training from {train.timestamp.min()} to {train.timestamp.max()}')","531b172f":"meter_arr = train[\"meter\"].unique()\nfor meter in meter_arr:\n    mask = train[\"meter\"] == meter\n    plt.figure(figsize=(20, 5))\n    sns.scatterplot(data = train[mask], x = \"meter_reading\", y = \"air_temperature\")\n    plt.xlabel(\"Meter: {}\".format(meter))\n    plt.show()","188345c0":"mask1 = train[\"meter\"] == 0\nmask2 = train[\"meter_reading\"] > 40000\nmask = np.logical_and(mask1, mask2)\nprint(train.shape)\ntrain[mask][\"meter_reading\"] = np.mean(train[mask1][\"meter_reading\"])\nprint(train.shape)","7c074917":"mask1 = train[\"meter\"] == 3\nmask2 = train[\"meter_reading\"] > 140000\nmask = np.logical_and(mask1, mask2)\nprint(train.shape)\ntrain[mask][\"meter_reading\"] = np.mean(train[mask1][\"meter_reading\"])\nprint(train.shape)","7a3f1f46":"mask = train[\"building_id\"] == 778\ntrain[mask][\"meter_reading\"] = 1","e97625fc":"mask = train[\"building_id\"] == 1088\ntrain[mask][\"meter_reading\"] = 1","41371d51":"# for our train and valudation dataset    return df\ndef correct_error_meter0_model_use(df):\n    out = df\n    new_values = out[\"meter_reading\"] * 0.2931\n    out.loc[out.meter == 0, \"meter_reading\"] = new_values\n    return out","cdb57d05":"# train = correct_error_meter0_model_use(train)","84905c24":"# target's log-log histogram:\nax = train.meter_reading.hist()\nax.set_yscale('log')\n\n# describe raw values first\ntrain.meter_reading.describe()","17009133":"# check the distribution in the types of meters\nmeters = train.groupby('building_id').meter.nunique()\nplt.title('Distribution of types of meters\\n{0:electricity, 1:water, 2:steam, 3:hotwater}') # from the official starter kernel\n_ = meters.hist()\n# from the graphs it looks like steam and hotwater are reversed (e.g.: 3:steam, 2:hotwater) but that shouldn't make any difference to the model","ee1310ec":"building_id = 1258  # a building with all 4 meters\nmeters = train[train['building_id'] == building_id].meter.nunique()\n\nfor meter in range(meters):\n    fig, ax = plt.subplots()\n    plt.title(f'Building {building_id} Meter {meter}')\n    ax2 = ax.twinx()\n    # plot meter_reading\n    idx = (train['building_id'] == building_id) & (train['meter'] == meter)\n    dates = matplotlib.dates.date2num(train.loc[idx, 'timestamp'])\n    ax2.plot_date(dates, train.loc[idx, 'meter_reading'], '-', label='meter_reading', alpha=0.8)\n    # plot air_temperature\n    dates = matplotlib.dates.date2num(train.loc[train['building_id'] == building_id, 'timestamp'])\n    ax.plot_date(dates, train.loc[train['building_id'] == building_id, 'air_temperature'], '.', color='tab:cyan', label='air_temperature')\n    ax.set_ylabel('air_temperature'); ax2.set_ylabel('meter_reading')\n    ax.legend(loc='upper left'); ax2.legend(loc='upper right')","0f8f2c2a":"meter = 1 # pick a meter\n\ntrain_sample = train[(train['building_id'] == building_id) & (train['meter'] == meter)]  # same train sample as above\n\ntest['meter_reading'] = 0.0\ntest_sample = test[(test['building_id'] == building_id) & (test['meter'] == meter)]  # and the same meter in the test set\n\nfig, ax = plt.subplots(figsize=(16,4))\nplt.title(f'Meter {meter}')\nax.xaxis.set_tick_params(rotation=30, labelsize=10)\nax2 = ax.twinx()\n\n# plot training sample\ndates = matplotlib.dates.date2num(train_sample['timestamp'])\nax2.plot_date(dates, train_sample['meter_reading'], '-', label='train', alpha=0.8)\nax.plot_date(dates, train_sample['air_temperature'], '.', color='tab:cyan', label='air_temperature_train')\n\n# plot test sample\ndates = matplotlib.dates.date2num(test_sample['timestamp'])\nax2.plot_date(dates, test_sample['meter_reading'], '*', label='test', alpha=0.8)\nax.plot_date(dates, test_sample['air_temperature'], '.', color='tab:cyan', label='air_temperature_test')\n\nax.set_ylabel('air_temperature'); ax2.set_ylabel('meter_reading')\nax.legend(loc='upper left'); ax2.legend(loc='upper right')\n\ndel train_sample; del test_sample; del dates","19c1566b":"# the counts above expose the missing data (Should we drop or refill the missing data?)\nprint(\"Ratio of available data (not NAN's):\")\ndata_ratios = train.count()\/len(train)\ndata_ratios","36bf75fc":"# Is the same happening in the test set? Yes\nprint(\"Ratio of available data (not NAN's):\")\ntest.count()\/len(test)","e1e7c0d0":"def ASHRAE3Preprocessor(df, data_ratios):\n    avgs = df.loc[:,data_ratios < 1.0].mean()\n    pu_le = LabelEncoder() # Asign to a categorical variable numerical values.\n    pu_le.fit(df[\"primary_use\"])\n    \n    df = df.fillna(avgs) # refill NAN with averages\n    df['primary_use'] = np.uint8(pu_le.transform(df['primary_use']))  # encode labels\n\n    # expand datetime into its components\n    df['hour'] = np.uint8(df['timestamp'].dt.hour)\n    df['day'] = np.uint8(df['timestamp'].dt.day)\n    df['weekday'] = np.uint8(df['timestamp'].dt.weekday)\n    df['month'] = np.uint8(df['timestamp'].dt.month)\n    df['year'] = np.uint8(df['timestamp'].dt.year-2000)\n\n    # parse and cast columns to a smaller type\n#     df.rename(columns={\"square_feet\": \"log_square_feet\"}, inplace=True)\n#     df['log_square_feet'] = np.float16(np.log(df['log_square_feet']))\n    df['year_built'] = np.uint8(df['year_built']-1900)\n    df['floor_count'] = np.uint8(df['floor_count'])\n\n    # remove redundant columns\n    for col in df.columns:\n        if col in ['timestamp', 'row_id']:\n            del df[col]\n\n    # extract target column\n    if 'meter_reading' in df.columns:\n        df['meter_reading'] = np.log1p(df['meter_reading']\/ df['square_feet']).astype(np.float32) # comp metric uses log errors \n    \n    return df","95a8b500":"train_transform = ASHRAE3Preprocessor(train, data_ratios)","bd70ba46":"train_transform.sample(7)","d65ea82e":"%%time\nfig, ax = plt.subplots(figsize=(16,8))\n# use a ranked correlation to catch nonlinearities\n# plot train in all columns except year, taking 100100 random samples, checking correlation using the method 'spearman'\ncorr = train_transform[[col for col in train_transform.columns if col != 'year']].sample(100100).corr(method='spearman')\n_ = sns.heatmap(corr, annot=True,\n                xticklabels=corr.columns.values,\n                yticklabels=corr.columns.values)","0d58d9e5":"# target = meter_reading\n# force the model to use the weather data instead of dates, to avoid overfitting to the past history\nfeatures = [col for col in train_transform.columns if col not in [target, 'year', 'month', 'day']]","dbb6aab2":"def fit_regressor(tr_idx, val_idx, features_arr, target_str):\n    # train\n    tr_x, tr_y = train_transform[features_arr].iloc[tr_idx], train_transform[target_str][tr_idx]\n    # evaluating (\"test\")\n    vl_x, vl_y = train_transform[features_arr].iloc[val_idx], train_transform[target_str][val_idx]\n    print({'train_transform size':len(tr_x), 'eval size':len(vl_x)})\n\n    tr_data = lgb.Dataset(tr_x, label=tr_y)\n    vl_data = lgb.Dataset(vl_x, label=vl_y)  \n    clf = lgb.LGBMRegressor(n_estimators=6000,\n                            learning_rate=0.28,\n                            feature_fraction=0.9,\n                            subsample=0.2,  # batches of 20% of the data\n                            subsample_freq=1,\n                            num_leaves=20,\n                            metric='rmse')\n    # Metric: Root Mean Square Error (RMSE), it tells you how concentrated the data is around the line of best fit.\n    clf.fit(tr_x, tr_y,\n            eval_set=[(vl_x, vl_y)],\n            early_stopping_rounds=50,\n            verbose=200)\n    return clf","af9414dd":"folds = 4\nseed = 42\nkf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed) # Provides train\/test indices to split data in train\/test sets.\n# oof_pred = np.zeros(train_transform.shape[0])  # out of fold predictions\nmodels = []\n\n## generating 4 train\/test pair of index_arrays, and analizing wich give the better results.\nfor tr_idx, val_idx in tqdm(kf.split(train_transform, train_transform['building_id']), total=folds): # train\/test indices\n    clf = fit_regressor(tr_idx, val_idx, features, target)\n    models.append(clf)\n\ngc.collect() # trigger a manual garbage collection process, cleans up a huge amount of objects.","abab33b7":"_ = lgb.plot_importance(models[1], importance_type='gain', figsize=(16,8))","4d79fc84":"# calculate the mean feature importance, so that we can update 'unimportant_cols' above\nfeature_importance = np.mean([m._Booster.feature_importance(importance_type='gain') for m in models], axis=0)\nsorted(zip(feature_importance, train_transform.columns), reverse=True)","22afac1e":"features = [col for col in train_transform.columns if col not in [target, 'year', 'month', 'day']]\ntr_idx = np.random.randint(0, 1552000, 1000000)\ntr_x, tr_y = train_transform[features].iloc[tr_idx], train_transform[target][tr_idx]","f7828c49":"dic = {\"real\": tr_y, \"prediction\": models[1].predict(tr_x)}\np_df = pd.DataFrame(data = dic)","dbc3f2a2":"actual = p_df[\"real\"].values\npredicted = p_df[\"prediction\"].values\nmse = sklearn.metrics.mean_squared_error(actual, predicted)\nrmse = np.sqrt(mse)\nprint(rmse)","6b40f1b3":"# the counts above expose the missing data (Should we drop or refill the missing data?)\nprint(\"Ratio of available data (not NAN's):\")\ndata_ratios = test.count()\/len(test)\ndata_ratios","db36c38d":"# load and pre-process test data\ntest_transform = ASHRAE3Preprocessor(test, data_ratios)\ntest_transform.sample(7)","d99bd6fb":"folds = 4\nmeter = 1 # pick a meter\nbuilding_id = 1258  # a building with all 4 meters\nfeatures = [col for col in train_transform.columns if col not in [target, 'year', 'month', 'day']]\n\ndef recover_timestamp(x):\n    ''' reassemble timestamp using date components '''\n    return datetime.datetime.strptime(f'{x.year}-{x.month}-{x.day} {x.hour}', '%y-%m-%d %H')\n\nfig, ax = plt.subplots(figsize=(16,4))\nplt.title(f'Building {building_id} Meter {meter} on all {folds} prediction folds')\nax.xaxis.set_tick_params(rotation=30, labelsize=10)\nax2 = ax.twinx()\n\ntrain_sample = train_transform[(train_transform['building_id'] == building_id) & (train_transform['meter'] == meter)]  # same training sample as before\ntest_sample = test_transform[(test_transform['building_id'] == building_id) & (test_transform['meter'] == meter)]   # and the same meter in the test set\n\n# plot training sample\ndates = matplotlib.dates.date2num(train_sample[['year', 'month', 'day', 'hour']].apply(recover_timestamp, axis=1))\nax2.plot_date(dates, train_sample['meter_reading'], '-', label='train', alpha=0.8)\nax.plot_date(dates, train_sample['air_temperature'], '.', color='tab:cyan', label='air_temperature_train')\n\n\n# plot prediction sample\ndates = matplotlib.dates.date2num(test_sample[['year', 'month', 'day', 'hour']].apply(recover_timestamp, axis=1))\nax.plot_date(dates, test_sample['air_temperature'], '.', color='tab:cyan', label='air_temperature')\nfor i,model in enumerate(models):\n    ax2.plot_date(dates, np.expm1(model.predict(test_sample[features])), '-', label=f'prediction{i}', alpha=0.4)\n\nax.set_ylabel('air_temperature'); ax2.set_ylabel('meter_reading (+prediction)')\nax.legend(loc='upper left'); ax2.legend(loc='upper right')\n_ = plt.show()","6725d9ce":"# # save the model to disk\n# filename = '.\/models_saved\/ASHRAE3_ID3_16082021_3.sav'\n# pickle.dump(models, open(filename, 'wb'))","e353765a":"# # load the model from disk\n# filename = '.\/models_saved\/ASHRAE3_ID3_16082021_3.sav'\n# models = pickle.load(open(filename, 'rb'))\n# models","138526eb":"# split test data into batches\nset_size = len(test_transform)\niterations = 50\nbatch_size = set_size \/\/ iterations\n\nprint(set_size, iterations, batch_size)\nassert set_size == iterations * batch_size","6e453a9f":"meter_reading = []\nfor i in tqdm(range(iterations)):\n    pos = i*batch_size\n    fold_preds = [model.predict(test_transform[features].iloc[pos : pos+batch_size]) for model in models]\n    meter_reading.extend(np.mean(fold_preds, axis=0)) # Using all the models and making the mean between each other.\n\nprint(len(meter_reading))\nassert len(meter_reading) == set_size","be0242c5":"# filename = '.\/models_saved\/meter_reading_1.csv'\n# out_df = pd.DataFrame(meter_reading)\n# out_df.to_csv(filename, index=False)","0ec5f102":"# for our outcome in validation\ndef correct_error_meter0_scoring_use(df):\n    out = df\n    new_values = out[\"meter_reading\"] * 3.4118\n    out.loc[out.meter == 0, \"meter_reading\"] = new_values\n    return out","0d1a2487":"# train = correct_error_meter0_scoring_use(train)","c06ce515":"meter_reading_exp = np.expm1(meter_reading) # Calculate exp(x) - 1 for all elements in the array.","5b1826c0":"meter_reading_exp = meter_reading_exp * test_transform['square_feet']","95956350":"dic = {\"pred\": meter_reading_exp}\ndf_pred_out = pd.DataFrame(data= dic)","5899decf":"# target's log-log histogram:\nax = df_pred_out.pred.hist()\nax.set_yscale('log')\n\n# describe raw values first\ndf_pred_out.pred.describe()","ea03469e":"np.clip([-1, 1,2,3,4], a_min=0, a_max=None)","7207796c":"submission = pd.read_csv(f'{path}\/sample_submission.csv')\nsubmission['meter_reading'] = np.clip(meter_reading_exp, a_min=0, a_max=None) # clip min at zero","26c56f1f":"submission.to_csv('submission.csv', index=False)\nsubmission.head(9)","3c4eef43":"# Save submision","1cc636b2":"# Train k folds","35b574da":"# Preproces data\n* The weather data sets are correctly filled in other scripts.","2bb93b7e":"## now let's see what's the expected prediction in the test set for the same building","e5e245e6":"## Feature importance","e59976ee":"# Load data and display samples","6e37fab1":"### Remove building_id = [1099, 1088]. It's outlayer.","0672f737":"## Save object data","01fbf896":"# Remove visual outlayers.","b609cad0":"# Check prediction in the same test sample","efd017e6":"# Checking in train data RMSE error.","321e14f3":"* meter:0, electricity: bigger than 8000 is outlayer.\n* meter:3, hotwater: bigger than 140000 is outlayer.","61e72269":"# Prepare data for submision.","e1b6aa36":"# Removing meter_reading out_layers for the different meters","36567379":"## display a single time series (notice measurement errors and discontinuities)","7124affa":"# Save\/ load the model to a file","b771e8b0":"# Test inferance and submission","4d2957b2":"# Algorithm ID3 regresion\n1. Calculate the initial system entropy based on the **objective** variable to predict.\n    * Entropy: Determine wich parameters are more important than others to have a better sort in the tree."}}