{"cell_type":{"dedd44e0":"code","4a62af9f":"code","6eefa772":"code","032c1f19":"code","47e1190d":"code","45187763":"code","fcdf3499":"code","89a81cc3":"code","375bcf21":"code","519065ec":"code","3aa86c5f":"code","be147a88":"code","d693855c":"code","1d0e7f13":"code","36942aeb":"code","8adc74a8":"code","1ef14267":"code","bbbcc378":"code","917cd6d4":"code","4339bfcc":"code","6caa8da1":"code","c140b903":"code","3c1b2f7d":"code","f3c049cb":"code","4051245d":"code","485e3cfe":"code","bd053411":"code","1474a195":"code","6c143eba":"code","25b4f78c":"code","40b6b2c7":"code","02ffc096":"code","513d0f2f":"code","b92d61b9":"code","60d120a5":"code","d44c60ba":"code","84e6f21b":"code","0bb2e3e8":"code","7290088c":"code","a9c91e48":"code","1311ef50":"code","2b41d256":"code","bde4b326":"code","b78ff064":"code","8ce4eec8":"code","3732b7db":"code","7bf65f24":"code","1937fad9":"code","4b18b9d6":"code","2b6ebb56":"code","0fd6462b":"code","fd44b286":"code","d5c9a680":"code","392cbe6c":"code","23e1c833":"code","784aa411":"code","e566f7ea":"code","b7a28564":"code","77a3fe2d":"code","76c7cc15":"markdown","3da1fdab":"markdown","eed91e30":"markdown","ab4044fc":"markdown","ce266c10":"markdown","206e8e9f":"markdown","4668e789":"markdown","823515d4":"markdown","e09b6717":"markdown","434c41e0":"markdown","9d1f5c6d":"markdown","a5e375fb":"markdown","0cdb19f7":"markdown","22f380be":"markdown","3127abc7":"markdown"},"source":{"dedd44e0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4a62af9f":"from fbprophet import Prophet","6eefa772":"train = pd.read_csv(\"\/kaggle\/input\/web-traffic-time-series-forecasting\/train_1.csv.zip\")","032c1f19":"train.head()","47e1190d":"all_pages = train['Page']","45187763":"first_page = all_pages[0]\nfirst_page","fcdf3499":"train_allT = train.set_index('Page').T.reset_index().rename(columns={'index':'Date'})\ntrain_allT.head()","89a81cc3":"df = pd.DataFrame(train_allT, columns = ['Date',first_page]) \ndf = df.rename(columns={'Date':'ds', first_page:'y'})","375bcf21":"df.head()","519065ec":"m = Prophet()\nm.fit(df)","3aa86c5f":"future = m.make_future_dataframe(periods=60)\nfuture.head()","be147a88":"forecast = m.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","d693855c":"result = forecast['yhat']","1d0e7f13":"result","36942aeb":"sub_result = result[-60:]","8adc74a8":"sub_result","1ef14267":"sub_result.shape","bbbcc378":"fig1 = m.plot(forecast)","917cd6d4":"fig2 = m.plot_components(forecast)","4339bfcc":"key = pd.read_csv(\"\/kaggle\/input\/web-traffic-time-series-forecasting\/key_1.csv.zip\")","6caa8da1":"key","c140b903":"listOfIds = key.index[key['Page'].str.contains(first_page)].values","3c1b2f7d":"listOfIds.shape","f3c049cb":"sub = pd.read_csv(\"\/kaggle\/input\/web-traffic-time-series-forecasting\/sample_submission_1.csv.zip\")","4051245d":"sub","485e3cfe":"sub['Visits'].loc[listOfIds.min():listOfIds.max()] = sub_result.values.round()\n#sub['Visits'].loc[0:59] = sub_result.values.round()","bd053411":"sub","1474a195":"def my_function():\n    for page in all_pages:\n        #print(page)\n        df = pd.DataFrame(train_allT, columns = ['Date',page]) \n        df = df.rename(columns={'Date':'ds', page:'y'})\n        m = Prophet()\n        m.fit(df)\n        future = m.make_future_dataframe(periods=60)\n        forecast = m.predict(future)\n        result = forecast['yhat']\n        sub_result = result[-60:]\n        listOfIds = key.index[key['Page'].str.contains(page)].values\n        sub['Visits'].loc[listOfIds.min():listOfIds.max()] = sub_result.values.round()\n        #print(sub)","6c143eba":"#my_function()","25b4f78c":"#sub.to_csv('submission.csv', index=False)","40b6b2c7":"import numpy as np\nimport pandas as pd\n\nprint('Reading data...')\nkey_1 = pd.read_csv('\/kaggle\/input\/web-traffic-time-series-forecasting\/key_2.csv.zip')\ntrain_1 = pd.read_csv('\/kaggle\/input\/web-traffic-time-series-forecasting\/train_2.csv.zip')\nss_1 = pd.read_csv('\/kaggle\/input\/web-traffic-time-series-forecasting\/sample_submission_2.csv.zip')\n\n","02ffc096":"print('Preprocessing...')\n# train_1.fillna(0, inplace=True)\n\nprint('Processing...')\nids = key_1.Id.values\npages = key_1.Page.values\n\n","513d0f2f":"print('key_1...')\nd_pages = {}\nfor id, page in zip(ids, pages):\n    d_pages[id] = page[:-11]\n   \n    \n\n","b92d61b9":"print('train_1...')\npages = train_1.Page.values\n# visits = train_1['2016-12-31'].values # Version 1 score: 60.6\n# visits = np.round(np.mean(train_1.drop('Page', axis=1).values, axis=1)) # Version 2 score: 64.8\n# visits = np.round(np.mean(train_1.drop('Page', axis=1).values[:, -14:], axis=1)) # Version 3 score: 52.5\n# visits = np.round(np.mean(train_1.drop('Page', axis=1).values[:, -7:], axis=1)) # Version 4 score: 53.7\n# visits = np.round(np.mean(train_1.drop('Page', axis=1).values[:, -21:], axis=1)) # Version 5, 6 score: 51.3\n# visits = np.round(np.mean(train_1.drop('Page', axis=1).values[:, -28:], axis=1)) # Version 7 score: 51.1\n# visits = np.round(np.median(train_1.drop('Page', axis=1).values[:, -28:], axis=1)) # Version 8 score: 47.1 \n# visits = np.round(np.median(train_1.drop('Page', axis=1).values[:, -35:], axis=1)) # Version 9 score: 46.6\n# visits = np.round(np.median(train_1.drop('Page', axis=1).values[:, -42:], axis=1)) # Version 10 score: 46.3\n# visits = np.round(np.median(train_1.drop('Page', axis=1).values[:, -49:], axis=1)) # Version 11 score: 46.2\n# visits = np.nan_to_num(np.round(np.nanmedian(train_1.drop('Page', axis=1).values[:, -49:], axis=1))) # Version 12 score: 45.7\nvisits = np.nan_to_num(np.round(np.nanmedian(train_1.drop('Page', axis=1).values[:, -56:], axis=1))) # scorer 41.8 #find medianen de sidste 56 dage og skift nan ud med 0\n\nd_visits = {}\nfor page, visits_number in zip(pages, visits):\n    d_visits[page] = visits_number\n    # for hver page i pages og visit i visits gem antal visits p\u00e5 page\n\nprint('Modifying sample submission...') # l\u00e6s submissionfilen ind\nss_ids = ss_1.Id.values\nss_visits = ss_1.Visits.values","60d120a5":"d_visits","d44c60ba":"d_pages","84e6f21b":"ss_ids","0bb2e3e8":"\n\nfor i, ss_id in enumerate(ss_ids):\n    ss_visits[i] = d_visits[d_pages[ss_id]] #s\u00e6t f\u00f8rste ss_id i d_pages-listen for at finde page-navn. s\u00e6t s\u00e5 page-navn i d_visits for at finde tal-v\u00e6rdien, \n    #som gemmes i stedet for ss_tal-v\u00e6rdien inkrementalt.\n\nprint('Saving submission...')\nsubm = pd.DataFrame({'Id': ss_ids, 'Visits': ss_visits})\nsubm.to_csv('submission.csv', index=False)","7290088c":"import warnings\nwarnings.filterwarnings('ignore')","a9c91e48":"print('Pre-processing and feature engineering train data...')\ntrain_flattened = pd.melt(train[list(train.columns[-49:])+['Page']], id_vars='Page', var_name='date', value_name='Visits')\ntrain_flattened['date'] = train_flattened['date'].astype('datetime64[ns]')\ntrain_flattened['weekend'] = ((train_flattened.date.dt.dayofweek) \/\/ 5 == 1).astype(float)\n","1311ef50":"# Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(50, 8))\nmean_group = train_flattened[['Page','date','Visits']].groupby(['date'])['Visits'].mean()\nplt.plot(mean_group)\nplt.title('Time Series - Average')\nplt.show()\nplt.close()","2b41d256":"times_series_means =  pd.DataFrame(mean_group).reset_index(drop=False)","bde4b326":"df_date_index = times_series_means[['date','Visits']].set_index('date')","b78ff064":"from statsmodels.tsa.stattools import adfuller\n# Run Dicky-Fuller test\nresult = adfuller(df_date_index)\n\n# Print test statistic\nprint(result[0])\n\n# Print p-value\nprint(result[1])","8ce4eec8":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n# Create figure\nfig, (ax1, ax2) = plt.subplots(2,1, figsize=(12,8))\n \n# Plot the ACF of savings on ax1\nplot_acf(df_date_index, zero=False, ax=ax1, lags=10)\n\n# Plot the PACF of savings on ax2\nplot_pacf(df_date_index, zero=False, ax=ax2, lags=10)\n\nplt.show()\nplt.close()","3732b7db":"# Create empty list to store search results\norder_aic_bic=[]\n\n# Loop over p values from 0-2\nfor p in range(3):\n  # Loop over q values from 0-2\n    for q in range(3):\n        try:\n            # create and fit ARMA(p,q) model\n            model = SARIMAX(df_date_index, order=(p,0,q), seasonal_order=(1,2,0,7))\n            results = model.fit()\n           \n\n            # Append order and results tuple\n            order_aic_bic.append((p,q, results.aic, results.bic))\n            print(p,q,results.aic, results.bic)\n            \n        except:\n            print(p, q, None, None)","7bf65f24":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n# Create and fit model\nmodel = SARIMAX(df_date_index, order=(2,0,1), trend='c')\nresults = model.fit()\n\n# Create the 4 diagostics plots\nresults.plot_diagnostics()\nplt.show()\nplt.close()\n\n# Print summary\nprint(results.summary())\n","1937fad9":"# Import seasonal decompose\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Perform additive decomposition\ndecomp = seasonal_decompose(df_date_index, \n                            freq=7)\n\n# Plot decomposition\ndecomp.plot()\nplt.show()\nplt.close()","4b18b9d6":"!pip install pmdarima","2b6ebb56":"import pmdarima as pm","0fd6462b":"# Create auto_arima model\nmodel1 = pm.auto_arima(df_date_index,\n                      seasonal=True, m=7,\n                      d=0, D=1, \n                 \t  max_p=2, max_q=2,\n                      trace=True,\n                      error_action='ignore',\n                      suppress_warnings=True)\n                       \n# Print model summary\nprint(model1.summary())","fd44b286":"# Import model class\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# Create model object\nmodel = SARIMAX(df_date_index, \n                order=(2,0,1), \n                seasonal_order=(1,1,1,7), \n                trend='c')\n# Fit model\nresults = model.fit()","d5c9a680":"# Plot common diagnostics\nresults.plot_diagnostics()\nplt.show()\nplt.close()","392cbe6c":"# Create forecast object\nforecast_object = results.get_forecast(steps=90)\n\n# Extract prediction mean\nmean = forecast_object.predicted_mean\n\n# Extract the confidence intervals\nconf_int = forecast_object.conf_int()\n\n# Extract the forecast dates\ndates = mean.index","23e1c833":"\ndf_date_index.index = pd.to_datetime(df_date_index.index)","784aa411":"# Print last predicted mean\nprint(mean.iloc[-1])\n\n# Print last confidence interval\nprint(conf_int.iloc[-1])","e566f7ea":"## Validating Forecast\npred = results.get_prediction(start=pd.to_datetime('2016-12-01'), dynamic=False)\npred_ci = pred.conf_int()\nax = df_date_index['2016':].plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nplt.legend()","b7a28564":"y_forecasted = pred.predicted_mean\ny_truth = df_date_index['2016-10-01':]\nmse = ((y_forecasted - y_truth) ** 2).mean()\nprint('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))\n#The MSE is a measure of the quality of an estimator\u200a\u2014\u200ait is always non-negative, \n#and the smaller the MSE, the closer we are to finding the line of best fit.","77a3fe2d":"pred_uc = results.get_forecast(steps=100)\npred_ci = pred_uc.conf_int()\nax = df_date_index.plot(label='observed', figsize=(14, 7))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nplt.legend()","76c7cc15":"Transpose for at vende matricen, reset_index for at starte fra nul, s\u00e5 datoerne ikke bliver index, og omd\u00f8b index til Date, s\u00e5 vi kan se at de er datoer","3da1fdab":"Lav lister af id'er og page-navne med dato p\u00e5","eed91e30":"Id'erne er en kombination af dato og page","ab4044fc":"L\u00e6s ind","ce266c10":"En god strategi kunne v\u00e6re at starte med kun at arbejde p\u00e5 en row af gangen, da hver row er en time series for en specifik artikel p\u00e5 wikipedia. Desuden kunne en god baseline v\u00e6re medianen per time series, s\u00e5 vi kunne lave et loop, som tager en timeseries af gangen, finder medianen, og s\u00e6tter v\u00e6rdien ind","206e8e9f":"Til at predicte med, kunne vi pr\u00f8ve: ARIMA, Prophet, RandomForestRegressor eller XGBoost (eller noget helt syvende, men vi kender ARIMA og RandomForest, og vi har snakket lidt om XGBoost. Prophet er noget lignende ARIMA, som facebook har udviklet). Husk at redeg\u00f8re for, hvordan den algoritme I anvender virker.","4668e789":"Ud fra plottet kan vi se, at der er nogle outliers, men vi kunne jo godt pr\u00f8ve bare at levere denne simple l\u00f8sning for at se, hvad det giver","823515d4":"Det der f\u00f8rst og fremmest g\u00f8r denne opgave sv\u00e6r, er faktisk at den er sv\u00e6r at forst\u00e5. Hvad er det der skal afleveres, hvordan stemmer submission-id'erne overens med predictions? Hvorfor ligger datoerne p\u00e5 den anden led etc.","e09b6717":"Find liste af id'er vi skal bruge","434c41e0":"Hvis vi unders\u00f8ger, hvor lange tr\u00e6ningsfilerne er, viser det sig at de er 145063 rows, og n\u00e5r vi ser, hvad der skal predictes er det 62 dage (fra 13. september til og med 13. november), det giver 8993906 entries, hvilket stemmer overens med l\u00e6ngden i key og submission-filerne. Det betyder, at hver page skal angive sin prediction for hver af de 62 dage. Desv\u00e6rre er det ikke s\u00e5dan, at de f\u00f8rste 62 entries svarer til de 62 dage for den f\u00f8rste page i s\u00e6ttet, s\u00e5 vi bliver n\u00f8dt til at finde koblingen mellem page og submission_id f\u00f8r vi kan aflevere noget.\nDe 145063 er individuelle time series for hver page der er blevet bes\u00f8gt","9d1f5c6d":"Kolonnerne skal hedde ds og y hvis vi vil bruge prophet","a5e375fb":"pseudokoden ville v\u00e6re lidt a la:\n>  foreach page in pages \n\n>     for (i=0; i<=61; i++)","0cdb19f7":"Her er resultatet for den ene time series, det samme skal s\u00e5 g\u00f8res med alle de andre, og s\u00e5 skal vi have dem placeret de rigtige steder i submission-filen","22f380be":"Resultatat viser hele datas\u00e6ttet inklusive de data vi har tr\u00e6net ud fra, derfor er s\u00e6ttet 610 langt. Men vi skal kun bruge de sidste 60 entries","3127abc7":"fjern dato-delen fra alle page-navne"}}