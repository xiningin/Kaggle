{"cell_type":{"5dc6c7ac":"code","9f71ea9a":"code","a8e498ca":"code","7b6e988c":"code","5d8aa696":"code","7b7b42ee":"code","25f83f66":"code","b60f45ed":"code","34ca049f":"code","a112eee9":"code","0cc08e2a":"code","4c48b7ee":"code","6c6ae174":"code","90af479d":"code","dc50544f":"code","6f3e8a13":"code","82e3d849":"code","1c46ac6f":"code","17e582f1":"code","c0890b7a":"code","465cf6e0":"code","ba4a661c":"code","8d705cf9":"code","b4bb0c5e":"code","61e546fc":"code","75c73072":"code","73da325c":"code","f742aacf":"code","a9f0140e":"code","ca3de178":"code","ac69ad75":"code","f70c1f10":"code","aedb9462":"code","b23bb247":"code","2e8465fc":"code","123964b3":"markdown","ff733a77":"markdown","69bfec18":"markdown","0eb7f54f":"markdown","cfc6fa51":"markdown","0d0f2098":"markdown","9b45dab1":"markdown","2be1a8a8":"markdown","56324f67":"markdown","4dbc1cb8":"markdown","7b7d2bdd":"markdown"},"source":{"5dc6c7ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9f71ea9a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option(\"display.max_columns\" , None)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nfrom collections import defaultdict\nfrom sklearn.model_selection import train_test_split\ndevice = (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","a8e498ca":"np.random.seed(0)\ntorch.manual_seed(0)","7b6e988c":"BASE_PATH = \"\/kaggle\/input\/digit-recognizer\/\"\n# BASE_PATH = \".\"\ntrain = pd.read_csv(os.path.join(BASE_PATH , \"train.csv\") , low_memory=False)\ntest = pd.read_csv(os.path.join(BASE_PATH , \"test.csv\") , low_memory=False)\nsub = pd.read_csv(os.path.join(BASE_PATH , \"sample_submission.csv\") , low_memory=False)","5d8aa696":"display(train.head())\ndisplay(test.tail())","7b7b42ee":"TARGET = \"label\"","25f83f66":"train , valid = train_test_split(train , test_size=0.2 , random_state = 0 ,stratify=train[TARGET])","b60f45ed":"train[TARGET].value_counts(normalize=True).sort_index()","34ca049f":"valid[TARGET].value_counts(normalize=True).sort_index()","a112eee9":"train.reset_index(drop=True , inplace=True)\nvalid.reset_index(drop=True , inplace=True)","0cc08e2a":"train.shape , valid.shape","4c48b7ee":"X_train , y_train = train.loc[: , \"pixel0\":\"pixel783\"] , train[TARGET]\nX_valid , y_valid = valid.loc[: , \"pixel0\":\"pixel783\"] , valid[TARGET]","6c6ae174":"X_train = X_train \/ 255\nX_valid = X_valid \/ 255\ntest = test \/ 255","90af479d":"X_train = X_train.values.reshape([-1,28*28*1])#28 by 28 picels with 1 color channel\nX_valid = X_valid.values.reshape([-1,28*28*1])\nX_test = test.values.reshape([-1,28*28*1])","dc50544f":"X_train.shape , X_valid.shape , X_test.shape","6f3e8a13":"def plot_sample_image(n):\n    image = X_train[n].reshape([28,28])\n    plt.imshow(image , cmap=\"Greys\")\n    plt.title(y_train[n] , fontsize=17)\n    \nplot_sample_image(n=930)","82e3d849":"class MNISTDataset:\n    def __init__(self,features,targets):\n        self.features = features\n        self.targets = targets\n    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self , item):\n        x = self.features[item] #For Features\n        y = self.targets[item] #For Targets\n        return x , y\n    \nclass TestDataset:\n    def __init__(self,features):\n        self.features = features\n        \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self,item):\n        x = self.features[item]#For Features\n        return x , item","1c46ac6f":"train_dataset = MNISTDataset(X_train , y_train)\nvalid_dataset = MNISTDataset(X_valid , y_valid)\ntest_dataset = TestDataset(X_test)","17e582f1":"def create_dataloader(dataset,batch_size,num_workers):\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        num_workers=num_workers\n    )\n    return dataloader","c0890b7a":"train_loader = create_dataloader(train_dataset,batch_size=200,num_workers=4)\nvalid_loader = create_dataloader(valid_dataset,batch_size=200,num_workers=4)\ntest_loader = create_dataloader(test_dataset,batch_size=200,num_workers=4)","465cf6e0":"x , y = next(iter(train_loader))","ba4a661c":"# X_train[0].reshape([-1,28*28]).shape","8d705cf9":"in_features = X_train.shape[1]\nnum_targets = len(y_train.unique())","b4bb0c5e":"class Model(nn.Module):\n    def __init__(self,in_features,num_targets):\n        super(Model , self).__init__()\n        \n        #Linear Function 1: 784 --> 200\n        self.fc1 = nn.Linear(in_features , 200)\n        #Non-Linearity 1\n        self.relu1 = nn.ReLU()\n        \n        #Linear Function 2: 200 --> 250\n        self.fc2 = nn.Linear(200 , 250)\n        #Non-Linearuty 2\n        self.tanh2 = nn.Tanh()\n        \n        #Linear Function 3: 250 --> 200\n        self.fc3 = nn.Linear(250,200)\n        #Non-Linearity 3\n        self.elu3 = nn.ELU()\n        \n        #Output Layer 200 --> 10\n        self.output = nn.Linear(200 , num_targets)\n        \n    def forward(self,x):\n        x = self.fc1(x)\n        x = self.relu1(x)\n        \n        x = self.fc2(x)\n        x = self.tanh2(x)\n        \n        x = self.fc3(x)\n        x = self.elu3(x)\n        \n        x = self.output(x)\n        return x","61e546fc":"model = Model(in_features,num_targets)\nmodel = model.to(device)\nlr = 0.02\noptimizer = torch.optim.Adam(model.parameters(),lr=lr)\ncriterion = nn.CrossEntropyLoss().to(device)","75c73072":"def train_epoch(model,dataloader,loss_fn,optimizer):\n    global inputs\n    model.train()\n    losses = []\n    accuracy = 0\n    model = model.float()\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        \n        inputs , targets = data\n        \n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        \n        inputs = inputs.view(-1 ,28*28)\n        \n        outputs = model(inputs.float())\n        \n        loss = loss_fn(outputs , targets)\n        losses.append(loss.item())\n        _ , preds = torch.max(outputs , dim=1)\n        acc = torch.sum(preds == targets)\n        accuracy += acc.detach().cpu().numpy()\n        \n        loss.backward()\n        optimizer.step()\n        \n    return np.mean(losses) , accuracy \/ len(train)\n        \n        \ndef eval_model(model,dataloader,loss_fn,optimizer):\n    model.eval()\n    losses = []\n    accuracy = 0\n    model = model.float()\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs , targets = data\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        inputs = inputs.view(-1,28*28)\n        outputs = model(inputs.float())\n        loss = loss_fn(outputs, targets)\n        losses.append(loss.item())\n        _ , preds = torch.max(outputs , dim=1)\n        acc = torch.sum(preds == targets)\n        accuracy += acc.detach().cpu().numpy()\n        \n        loss.backward()\n        optimizer.step()\n        \n        \n    return np.mean(losses) , accuracy \/ len(valid)","73da325c":"def Run_Test(model , dataloader):\n    model.eval()\n    predictions = []\n    indeces = []\n    model = model.float()\n    \n    with torch.no_grad():\n        for data in dataloader:\n            inputs , index = data\n            inputs = inputs.to(device)\n            index = index.to(device)\n            inputs = inputs.view(-1,28*28)\n            outputs = model(inputs.float())\n            _ , preds = torch.max(outputs , dim=1)\n            preds = preds.detach().cpu().numpy()\n            predictions.extend(preds)\n            indeces.extend(index.detach().cpu().numpy())\n            \n    return predictions , indeces","f742aacf":"EPOCHS = 20","a9f0140e":"history = defaultdict(list)\nfor epoch in range(EPOCHS):\n    print(f\"Epoch {epoch+1}\/{EPOCHS}\")\n    train_loss , train_acc = train_epoch(model , train_loader, criterion , optimizer)\n    val_loss , val_acc = eval_model(model , valid_loader, criterion , optimizer)\n    \n    history[\"train_loss\"].append(train_loss)\n    history[\"train_acc\"].append(train_acc)\n    history[\"val_loss\"].append(val_loss)\n    history[\"val_acc\"].append(val_acc)\n    \n    print(f\"Train Loss {train_loss} Accuracy {train_acc}\")\n    print(f\"Val Loss {val_loss} Accuracy {val_acc}\")\n    print()","ca3de178":"plt.plot(history[\"train_loss\"] , color=\"blue\" , label=\"Train_Loss\")\nplt.plot(history[\"val_loss\"] ,color=\"red\" ,label=\"Val_Loss\")\nplt.xlabel(\"Epochs\" , fontsize=17)\nplt.ylabel(\"Loss\" , fontsize=17)\nplt.legend()\nplt.show()","ac69ad75":"plt.plot(history[\"train_acc\"] , color=\"blue\" , label=\"Train_Accuracy\")\nplt.plot(history[\"val_acc\"] ,color=\"red\" ,label=\"Val_Accuracy\")\nplt.xlabel(\"Epochs\" , fontsize=17)\nplt.ylabel(\"Accuracy\" , fontsize=17)\nplt.legend()\nplt.show()","f70c1f10":"predictions , indeces = Run_Test(model ,  test_loader)","aedb9462":"sub[\"Label\"] = predictions\nsub[\"indeces\"] = indeces","b23bb247":"sub.set_index(\"indeces\" , inplace=True)\nsub.index.name = None","2e8465fc":"sub.to_csv(\"submission.csv\" , index=False)","123964b3":"We can see that our train and validation data are well stratified to have similiar distributions","ff733a77":"In out model we would not be using a Convolutional Layer","69bfec18":"### Split Training and Validation Dataset","0eb7f54f":"### Model Class","cfc6fa51":"### This notebooks was guided by this [DATAI's](https:\/\/www.kaggle.com\/kanncaa1) notebook that can be found [HERE](https:\/\/www.kaggle.com\/kanncaa1\/pytorch-tutorial-for-deep-learning-lovers)","0d0f2098":"### Import Necessary Packages","9b45dab1":"### Normalizaion","2be1a8a8":"##### Let's reshape pixels to shape displayed by images","56324f67":"### Create PyTorch Dataset Class Object","4dbc1cb8":"### Upvote if you find this notebook helpful and drop comments if you have questions or see something wrong\n![image.png](attachment:image.png)","7b7d2bdd":"Dividing by 255 works exactly just like a MinMax Scaler,\nThe Min-Max Scaler Formula is\n\n$$X_{new} = \\frac{X - X_{min}}{X_{max}-X_{min}}$$\n\nThe minimum pixel is 0 while the maximum pixel is 255\n\nSo take a pixel 16 for example and apply the formula on it you get\n\n$$X_{new} = \\frac{16 - 0}{255-0}$$\n\nThen you can see that this is just the same as dividing by the maximum which is 255"}}