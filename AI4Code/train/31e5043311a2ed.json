{"cell_type":{"04e54ca0":"code","c434ccd0":"code","5687445e":"code","1fca5b33":"code","9df002b1":"code","d64cfe54":"code","a2d90f27":"code","96ba563e":"code","7b3d8584":"code","18d6e774":"code","6173a7b8":"code","20ab5e98":"code","414ec87d":"code","21d8be70":"code","8ae4e8c7":"markdown","1cca50d7":"markdown","c51b4cd0":"markdown","08b8de86":"markdown","b61a286b":"markdown","5babf201":"markdown"},"source":{"04e54ca0":"!pip install pytorch-lightning-spells geffnet","c434ccd0":"from pathlib import Path\nfrom typing import Callable, Union, Tuple, List\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nfrom albumentations import SmallestMaxSize, CenterCrop, Compose\nfrom albumentations.pytorch.transforms import ToTensor\nimport pytorch_lightning as pl\nfrom pytorch_lightning_spells.callbacks import CutMixCallback, MixUpCallback, SnapMixCallback\nfrom torch.utils.data import Dataset, DataLoader\n\ntorch.multiprocessing.set_sharing_strategy('file_system')\n\nOFFSET = np.asarray([0.485, 0.456, 0.406])[:, np.newaxis, np.newaxis]\nSCALE = np.asarray([0.229, 0.224, 0.225])[:, np.newaxis, np.newaxis]\nTRANSFORMATIONS = Compose([\n    SmallestMaxSize(448),\n    CenterCrop(448, 448),\n    ToTensor(normalize=dict(\n        mean=OFFSET[:,0,0], std=SCALE[:,0,0])\n    )\n])","5687445e":"def load_image(filepath: Path) -> Image.Image:\n    image = np.array(Image.open(filepath).convert('RGB'))\n    return image\n\n\ndef load_transform_image(filepath):\n    image = load_image(filepath)\n    return TRANSFORMATIONS(image=image)[\"image\"]\n\n\nclass CassavaDataset(Dataset):\n    def __init__(self, folder: Union[Path, str], df: pd.DataFrame):\n        super().__init__()\n        self._df = df\n        self._folder = Path(folder)\n\n    def __len__(self):\n        return len(self._df)\n\n    def __getitem__(self, idx: int):\n        item = self._df.iloc[idx]\n        image = load_transform_image(\n            self._folder \/ item.image_id)\n        return [image, torch.tensor(item.label, dtype=torch.int64)]","1fca5b33":"data_loader = DataLoader(\n    CassavaDataset(\n        Path(\"\/kaggle\/input\/cassava-leaf-disease-classification\/train_images\/\"),\n        pd.read_csv(\"\/kaggle\/input\/cassava-leaf-disease-classification\/train.csv\")\n    ),\n    shuffle=True,\n    batch_size=16,\n    num_workers=0,\n    drop_last=False\n)","9df002b1":"mixup_cb = MixUpCallback(alpha=0.5, softmax_target=True)","d64cfe54":"batch, targets = next(iter(data_loader))\nbatch_packed = [batch, targets]\nmixup_cb.on_train_batch_start(None, None, batch_packed, None, None)\nbatch_new, targets_new = batch_packed\nfig, ax = plt.subplots(nrows=4, ncols=4, figsize=(12, 12))\ncount=0\nfor row in ax:\n    for col in row:\n        col.imshow(\n            ((batch_new[count].numpy() * SCALE + OFFSET).transpose(1,2,0) * 255.).astype(np.uint8)\n        )\n        col.set_axis_off()\n        col.set_title(\n            f\"L1: {targets_new[count][0]:.0f} ({targets_new[count][2]*100:.0f}%) \"\n            f\"| L2: {targets_new[count][1]:.0f} \"\n        )\n        count += 1\nplt.show()","a2d90f27":"cutmix_cb = CutMixCallback(alpha=0.9, minmax=[0.2, 0.8], softmax_target=True)","96ba563e":"batch, targets = next(iter(data_loader))\nbatch_packed = [batch, targets]\ncutmix_cb.on_train_batch_start(None, None, batch_packed, None, None)\nbatch_new, targets_new = batch_packed\nfig, ax = plt.subplots(nrows=4, ncols=4, figsize=(12, 12))\ncount=0\nfor row in ax:\n    for col in row:\n        col.imshow(\n            ((batch_new[count].numpy() * SCALE + OFFSET).transpose(1,2,0) * 255.).astype(np.uint8)\n        )\n        col.set_axis_off()\n        col.set_title(\n            f\"L1: {targets_new[count][0]:.0f} ({targets_new[count][2]*100:.0f}%) \"\n            f\"| L2: {targets_new[count][1]:.0f} \"\n        )\n        count += 1\nplt.show()","7b3d8584":"import geffnet\nfrom fastcore.basics import patch_to\n\n@patch_to(geffnet.gen_efficientnet.GenEfficientNet)\ndef extract_features(self, input_tensor):\n    return self.features(input_tensor)\n\n@patch_to(geffnet.gen_efficientnet.GenEfficientNet)\ndef get_fc(self):\n    return self.classifier","18d6e774":"# Load a baseline model\nmodel_dict = torch.load(\"\/kaggle\/input\/cassava-model\/full_b4_41341_3290_randaug.pth\")\nmodel = geffnet.tf_efficientnet_b4_ns(pretrained=False, drop_rate=0.4, as_sequential=False)\nmodel.classifier = torch.nn.Linear(model.classifier.in_features, 5)\nmodel.load_state_dict(model_dict[\"model_states\"])\n_ = model.cuda()","6173a7b8":"snapmix_cb = SnapMixCallback(model, image_size=(448, 448), minmax = (0.2, 0.8), alpha=0.9, cutmix_bbox=True)","20ab5e98":"batch, targets = next(iter(data_loader))\nbatch_packed = [batch, targets]\nsnapmix_cb.on_train_batch_start(None, None, batch_packed, None, None)\nbatch_new, targets_new = batch_packed\nfig, ax = plt.subplots(nrows=4, ncols=4, figsize=(12, 12))\ncount=0\nfor row in ax:\n    for col in row:\n        col.imshow(\n            ((batch_new[count].numpy() * SCALE + OFFSET).transpose(1,2,0) * 255.).astype(np.uint8)\n        )\n        col.set_axis_off()\n        col.set_title(\n            f\"L1: {targets_new[count][0]:.0f} ({targets_new[count][2]*100:.0f}%) \"\n            f\"| L2: {targets_new[count][1]:.0f} ({targets_new[count][3]*100:.0f}%)\"\n        )\n        count += 1\nplt.show()","414ec87d":"snapmix_cb = SnapMixCallback(model, image_size=(448, 448), minmax = (0.2, 0.8), alpha=0.9, cutmix_bbox=False)","21d8be70":"batch, targets = next(iter(data_loader))\nbatch_packed = [batch, targets]\nsnapmix_cb.on_train_batch_start(None, None, batch_packed, None, None)\nbatch_new, targets_new = batch_packed\nfig, ax = plt.subplots(nrows=4, ncols=4, figsize=(12, 12))\ncount=0\nfor row in ax:\n    for col in row:\n        col.imshow(\n            ((batch_new[count].numpy() * SCALE + OFFSET).transpose(1,2,0) * 255.).astype(np.uint8)\n        )\n        col.set_axis_off()\n        col.set_title(\n            f\"L1: {targets_new[count][0]:.0f} ({targets_new[count][2]*100:.0f}%) \"\n            f\"| L2: {targets_new[count][1]:.0f} ({targets_new[count][3]*100:.0f}%)\"\n        )\n        count += 1\nplt.show()","8ae4e8c7":"### CutMix-style Bounding Boxes","1cca50d7":"Because the nature of this dataset, the mixed up image is hard to read. We can infer that Mix Up augmentation is probablly not the best choice for this dataset.","c51b4cd0":"## SnapMix\n\nReference: [SnapMix: Semantically Proportional Mixing for Augmenting Fine-grained Data](https:\/\/arxiv.org\/abs\/2012.04846)","08b8de86":"### SnapMix-style Bounding Boxes\n\n1. Randomly generate two bounding boxes (instead of one as in CutMix). \n2. The first bounding mox is used to extract a patch from the source image.\n3. The extracted patch is resized to the size of the second bounding box.\n4. The resized patch is put into the target image at the second bounding box.","b61a286b":"## MixUp","5babf201":"## CutMix"}}