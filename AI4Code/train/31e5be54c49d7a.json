{"cell_type":{"7469210f":"code","10088439":"code","681c086a":"code","e13acdcb":"code","04e77fc7":"code","dfa4aea8":"code","024f1c96":"code","cea015fe":"code","87cf4921":"code","ec8fd7de":"code","9577a1ee":"code","26dc3864":"code","947fcd2c":"code","a0c696ca":"code","ff559962":"code","46404e21":"code","34eba513":"code","d5adc91d":"code","7f3382da":"code","0c0ec565":"code","ee7338c1":"code","8f5e8ff1":"code","119f2bc3":"code","120e4e15":"code","26b72613":"code","eeb68bd4":"code","e096d2fe":"code","9a7dfcef":"code","4a24b7b4":"code","841df96a":"code","5aacf066":"code","0aad8c65":"code","8b3f2084":"code","73d67d79":"code","80ce7a0f":"code","3589d60f":"code","199a2b6f":"code","74b11bb5":"code","825ef955":"code","08f1f6cd":"code","224b976c":"code","ad8ddc2f":"code","6ea3f94a":"code","fc5c02dd":"code","62bb3d3d":"code","d0c1243b":"code","d31ae3a4":"code","831ef03c":"code","47b1745e":"code","396de9d1":"code","2bb86f21":"code","27ce5923":"markdown","7bbc7297":"markdown","01b66175":"markdown","3bdca270":"markdown","4a3c445b":"markdown","563b5fec":"markdown","acf93dc5":"markdown","6eca4209":"markdown","b26ea762":"markdown","814ac852":"markdown","30084f40":"markdown","8a1b2835":"markdown","7786f575":"markdown","d82292f8":"markdown","5ace1bed":"markdown","83b00fb4":"markdown","9884fbde":"markdown","3b3fb6c4":"markdown","97bdae54":"markdown","aef6f006":"markdown","daacf20d":"markdown","b79ab647":"markdown"},"source":{"7469210f":"#importing libraries\n\nimport pandas as pd\nimport numpy as np\n\n# uploading the file from the local drive\n\n#final_jobs = pd.read_csv(\"C:\\\\Users\\\\KANDIRAJU\\\\Downloads\\\\neat_Data-20190423T142731Z-001\\\\neat_Data\\\\Combined_Jobs_Final.csv\")\nfinal_jobs = pd.read_csv(\"..\/input\/Combined_Jobs_Final.csv\")\n# listing out the first 5 rows of the data set\n\nfinal_jobs.head()","10088439":"# Listing out all the columns that are present in the data set.\n\nlist(final_jobs) ","681c086a":"print(final_jobs.shape)\nfinal_jobs.isnull().sum()","e13acdcb":"#subsetting only needed columns and not considering the columns that are not necessary\ncols = list(['Job.ID']+['Slug']+['Title']+['Position']+ ['Company']+['City']+['Employment.Type']+['Education.Required']+['Job.Description'])\nfinal_jobs =final_jobs[cols]\nfinal_jobs.columns = ['Job.ID','Slug', 'Title', 'Position', 'Company','City', 'Empl_type','Edu_req','Job_Description']\nfinal_jobs.head() ","04e77fc7":"# checking for the null values again.\nfinal_jobs.isnull().sum()","dfa4aea8":"#selecting NaN rows of city\nnan_city = final_jobs[pd.isnull(final_jobs['City'])]\nprint(nan_city.shape)\nnan_city.head()","024f1c96":"nan_city.groupby(['Company'])['City'].count() ","cea015fe":"#replacing nan with thier headquarters location\nfinal_jobs['Company'] = final_jobs['Company'].replace(['Genesis Health Systems'], 'Genesis Health System')\n\nfinal_jobs.ix[final_jobs.Company == 'CHI Payment Systems', 'City'] = 'Illinois'\nfinal_jobs.ix[final_jobs.Company == 'Academic Year In America', 'City'] = 'Stamford'\nfinal_jobs.ix[final_jobs.Company == 'CBS Healthcare Services and Staffing ', 'City'] = 'Urbandale'\nfinal_jobs.ix[final_jobs.Company == 'Driveline Retail', 'City'] = 'Coppell'\nfinal_jobs.ix[final_jobs.Company == 'Educational Testing Services', 'City'] = 'New Jersey'\nfinal_jobs.ix[final_jobs.Company == 'Genesis Health System', 'City'] = 'Davennport'\nfinal_jobs.ix[final_jobs.Company == 'Home Instead Senior Care', 'City'] = 'Nebraska'\nfinal_jobs.ix[final_jobs.Company == 'St. Francis Hospital', 'City'] = 'New York'\nfinal_jobs.ix[final_jobs.Company == 'Volvo Group', 'City'] = 'Washington'\nfinal_jobs.ix[final_jobs.Company == 'CBS Healthcare Services and Staffing', 'City'] = 'Urbandale'","87cf4921":"final_jobs.isnull().sum()","ec8fd7de":"#The employement type NA are from Uber so I assume as part-time and full time\nnan_emp_type = final_jobs[pd.isnull(final_jobs['Empl_type'])]\nprint(nan_emp_type)\n\n","9577a1ee":"#replacing na values with part time\/full time\nfinal_jobs['Empl_type']=final_jobs['Empl_type'].fillna('Full-Time\/Part-Time')\nfinal_jobs.groupby(['Empl_type'])['Company'].count()\nlist(final_jobs)","26dc3864":"final_jobs[\"pos_com_city_empType_jobDesc\"] = final_jobs[\"Position\"].map(str) + \" \" + final_jobs[\"Company\"] +\" \"+ final_jobs[\"City\"]+ \" \"+final_jobs['Empl_type']+\" \"+final_jobs['Job_Description']\nfinal_jobs.pos_com_city_empType_jobDesc.head()","947fcd2c":"#removing unnecessary characters between words separated by space between each word of all columns to make the data efficient\nfinal_jobs['pos_com_city_empType_jobDesc'] = final_jobs['pos_com_city_empType_jobDesc'].str.replace('[^a-zA-Z \\n\\.]',\" \") #removing unnecessary characters\nfinal_jobs.pos_com_city_empType_jobDesc.head()","a0c696ca":"#converting all the characeters to lower case\nfinal_jobs['pos_com_city_empType_jobDesc'] = final_jobs['pos_com_city_empType_jobDesc'].str.lower() \nfinal_jobs.pos_com_city_empType_jobDesc.head()","ff559962":"final_all = final_jobs[['Job.ID', 'pos_com_city_empType_jobDesc']]\n# renaming the column name as it seemed a bit complicated\nfinal_all = final_jobs[['Job.ID', 'pos_com_city_empType_jobDesc']]\nfinal_all = final_all.fillna(\" \")\n\nfinal_all.head()","46404e21":"print(final_all.head(1))","34eba513":"pos_com_city_empType_jobDesc = final_all['pos_com_city_empType_jobDesc']\n#removing stopwords and applying potter stemming\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nstemmer =  PorterStemmer()\nstop = stopwords.words('english')\nonly_text = pos_com_city_empType_jobDesc.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\nonly_text.head()\n","d5adc91d":"only_text = only_text.apply(lambda x : filter(None,x.split(\" \")))\nprint(only_text.head())","7f3382da":"only_text = only_text.apply(lambda x : [stemmer.stem(y) for y in x])\nprint(only_text.head())","0c0ec565":"only_text = only_text.apply(lambda x : \" \".join(x))\nprint(only_text.head())","ee7338c1":"#adding the featured column back to pandas\nfinal_all['text']= only_text\n# As we have added a new column by performing all the operations using lambda function, we are removing the unnecessary column\n#final_all = final_all.drop(\"pos_com_city_empType_jobDesc\", 1)\n\nlist(final_all)\nfinal_all.head()","8f5e8ff1":"# in order to save this file for a backup\n#final_all.to_csv(\"job_data.csv\", index=True)","119f2bc3":"\n#initializing tfidf vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n#from sklearn.feature_extraction.text import CountVectorizer\n\ntfidf_vectorizer = TfidfVectorizer()\n\ntfidf_jobid = tfidf_vectorizer.fit_transform((final_all['text'])) #fitting and transforming the vector\ntfidf_jobid","120e4e15":"#Consider a  new data set and  taking the datasets job view, position of interest, experience of the applicant into consideration for creating a query who applied for job\njob_view = pd.read_csv(\"..\/input\/Job_Views.csv\")\njob_view.head()\n","26b72613":"#subsetting only needed columns and not considering the columns that are not necessary as we did that earlier.\njob_view = job_view[['Applicant.ID', 'Job.ID', 'Position', 'Company','City']]\n\njob_view[\"pos_com_city\"] = job_view[\"Position\"].map(str) + \"  \" + job_view[\"Company\"] +\"  \"+ job_view[\"City\"]\n\njob_view['pos_com_city'] = job_view['pos_com_city'].str.replace('[^a-zA-Z \\n\\.]',\"\")\n\njob_view['pos_com_city'] = job_view['pos_com_city'].str.lower()\n\njob_view = job_view[['Applicant.ID','pos_com_city']]\n\njob_view.head()\n","eeb68bd4":"#Experience\nexper_applicant = pd.read_csv(\"..\/input\/Experience.csv\")\nexper_applicant.head()","e096d2fe":"#taking only Position\nexper_applicant = exper_applicant[['Applicant.ID','Position.Name']]\n\n#cleaning the text\nexper_applicant['Position.Name'] = exper_applicant['Position.Name'].str.replace('[^a-zA-Z \\n\\.]',\"\")\n\nexper_applicant.head()\n#list(exper_applicant)","9a7dfcef":"exper_applicant['Position.Name'] = exper_applicant['Position.Name'].str.lower()\nexper_applicant.head(10)","4a24b7b4":"exper_applicant =  exper_applicant.sort_values(by='Applicant.ID')\nexper_applicant = exper_applicant.fillna(\" \")\nexper_applicant.head(20)\n","841df96a":"#adding same rows to a single row\nexper_applicant = exper_applicant.groupby('Applicant.ID', sort=False)['Position.Name'].apply(' '.join).reset_index()\nexper_applicant.head(20)","5aacf066":"#Position of interest\npoi =  pd.read_csv(\"..\/input\/Positions_Of_Interest.csv\", sep=',')\npoi = poi.sort_values(by='Applicant.ID')\npoi.head()","0aad8c65":"# There is no need of application and updation becuase there is no deadline mentioned in the website ( assumption) hence we are droping unimportant attributes\npoi = poi.drop('Updated.At', 1)\npoi = poi.drop('Created.At', 1)\n\n#cleaning the text\npoi['Position.Of.Interest']=poi['Position.Of.Interest'].str.replace('[^a-zA-z \\n\\.]',\"\")\npoi['Position.Of.Interest']=poi['Position.Of.Interest'].str.lower()\npoi = poi.fillna(\" \")\npoi.head(20)","8b3f2084":"poi = poi.groupby('Applicant.ID', sort=True)['Position.Of.Interest'].apply(' '.join).reset_index()\npoi.head()","73d67d79":"#merging jobs and experience dataframes\nout_joint_jobs = job_view.merge(exper_applicant, left_on='Applicant.ID', right_on='Applicant.ID', how='outer')\nprint(out_joint_jobs.shape)\nout_joint_jobs = out_joint_jobs.fillna(' ')\nout_joint_jobs = out_joint_jobs.sort_values(by='Applicant.ID')\nout_joint_jobs.head()","80ce7a0f":"#merging position of interest with existing dataframe\njoint_poi_exper_view = out_joint_jobs.merge(poi, left_on='Applicant.ID', right_on='Applicant.ID', how='outer')\njoint_poi_exper_view = joint_poi_exper_view.fillna(' ')\njoint_poi_exper_view = joint_poi_exper_view.sort_values(by='Applicant.ID')\njoint_poi_exper_view.head()","3589d60f":"#combining all the columns\n\njoint_poi_exper_view[\"pos_com_city1\"] = joint_poi_exper_view[\"pos_com_city\"].map(str) + joint_poi_exper_view[\"Position.Name\"] +\" \"+ joint_poi_exper_view[\"Position.Of.Interest\"]\n\njoint_poi_exper_view.head()","199a2b6f":"final_poi_exper_view = joint_poi_exper_view[['Applicant.ID','pos_com_city1']]\nfinal_poi_exper_view.head()","74b11bb5":"final_poi_exper_view.columns = ['Applicant_id','pos_com_city1']\nfinal_poi_exper_view.head()","825ef955":"final_poi_exper_view = final_poi_exper_view.sort_values(by='Applicant_id')\nfinal_poi_exper_view.head()","08f1f6cd":"final_poi_exper_view['pos_com_city1'] = final_poi_exper_view['pos_com_city1'].str.replace('[^a-zA-Z \\n\\.]',\"\")\nfinal_poi_exper_view.head()\n","224b976c":"final_poi_exper_view['pos_com_city1'] = final_poi_exper_view['pos_com_city1'].str.lower()\nfinal_poi_exper_view.head()","ad8ddc2f":"\nfinal_poi_exper_view = final_poi_exper_view.reset_index(drop=True)\nfinal_poi_exper_view.head()","6ea3f94a":"#taking a user\nu = 6945\nindex = np.where(final_poi_exper_view['Applicant_id'] == u)[0][0]\nuser_q = final_poi_exper_view.iloc[[index]]\nuser_q","fc5c02dd":"#creating tf-idf of user query and computing cosine similarity of user with job corpus\nfrom sklearn.metrics.pairwise import cosine_similarity\nuser_tfidf = tfidf_vectorizer.transform(user_q['pos_com_city1'])\noutput = map(lambda x: cosine_similarity(user_tfidf, x),tfidf_jobid)\n","62bb3d3d":"output2 = list(output)","d0c1243b":"#getting the job id's of the recommendations\ntop = sorted(range(len(output2)), key=lambda i: output2[i], reverse=True)[:50]\nrecommendation = pd.DataFrame(columns = ['ApplicantID', 'JobID'])\ncount = 0\nfor i in top:\n    recommendation.set_value(count, 'ApplicantID',u)\n    recommendation.set_value(count,'JobID' ,final_all['Job.ID'][i])\n    count += 1\n","d31ae3a4":"recommendation","831ef03c":"#getting the job ids and their data\nnearestjobs = recommendation['JobID']\njob_description = pd.DataFrame(columns = ['JobID','text'])\nfor i in nearestjobs:\n    index = np.where(final_all['Job.ID'] == i)[0][0]    \n    job_description.set_value(count, 'JobID',i)\n    job_description.set_value(count, 'text', final_all['text'][index])\n    count += 1\n    ","47b1745e":"#printing the jobs that matched the query\njob_description","396de9d1":"job_description.to_csv(\"recommended_content.csv\")","2bb86f21":"final_all.to_csv(\"job_data.csv\", index=False)","27ce5923":"### Experience\nWe take experience of all the applicants who applied for the job and we are comaring the point of interest with the jobs that are present in our previous data.","7bbc7297":"#### combining the columns of position,company,city,emp_type and position","01b66175":"Here comes the important concept call **Stop words**. Stop words are natural language words which have very little meaning, such as \"and\", \"the\", \"a\", \"an\", and similar words.\nWe use NLP where NLTK ( Natural Language Toolkit ) is used for stopwords.\nText may contain stop words like \u2018the\u2019, \u2018is\u2019, \u2018are\u2019. Stop words can be filtered from the text to be processed. There is no universal list of stop words in nlp research, however the nltk module contains a list of stop words.","3bdca270":"### Position of Interest","4a3c445b":"# Using vector space model ( Cosine similarity )\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.pairwise.cosine_similarity.html","563b5fec":" From the above list we see that there are lot of NaN values, perform data cleansing for each and every column","acf93dc5":"In the above code, we separated each letter in a word separated by comma, now, in this step, we join the words(x)\n","6eca4209":"pos_com city, position_name","b26ea762":"select random row of 6945 ","814ac852":"Splitting each word in a row separated by space.","30084f40":"#   corpus ","8a1b2835":"Here **stemming** is basically used to remove the suffixes and common words that repeat and separated by commas. for y in x means, for each word(y) in the total list(x)","7786f575":"same applicant has 3 applications 100001 in sigle line","d82292f8":"  We can import stopwords from nltk.corpus as below. With that, We exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n  ##  NLTK stands for Natural Languague Toolkit\nIt removes stop words such as the, is, and etc..","5ace1bed":"### Merging","83b00fb4":"We see that there are only 9 companies cities that are having NaN values so I manually adding their head quarters with the help of google search\n","9884fbde":"# User query Corpus\nTake another dataset called job views.","3b3fb6c4":"There is a difference between fit (unique words are created \/ vectorization) and fit_transform.\nfit means fit transform and transform( adding the row internally adding 1 in the users input query)\nex : innovate is not present then then it will not create ( loosing the efficiency)","97bdae54":"# TF-IDF ( Term Frequency - Inverse Document Frequency ) \nThis method is also called as Normalization.\nTF - How many times a particular word appears in a single doc. \nIDF - This downscales words that appear a lot across documents.","aef6f006":"cosine similarity score","daacf20d":"## concatenating the columns ( job corups)\n","b79ab647":"The next package used in here is stemming, The idea of stemming is a sort of normalizing method. Many variations of words carry the same meaning, other than when tense is involved. So in order to clean up the space, we use stemming method and the one of the packages used here is PorterStemmer"}}