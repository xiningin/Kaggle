{"cell_type":{"d6257187":"code","25a95765":"code","823ded39":"code","0c5c7a20":"code","d13639b2":"code","78953d3d":"code","8b76ad5d":"code","f24a962f":"code","3b638634":"code","c36ba424":"code","4f6f1591":"code","fc9bbc20":"code","56f37663":"code","a79f67c4":"code","36e6dd9d":"code","3abe5447":"code","4ab4ab14":"markdown","056666e6":"markdown"},"source":{"d6257187":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np","25a95765":"def show_history(history):\n    print(history.history.keys())\n\n    # summarize history for accuracy\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()","823ded39":"mnist = tf.keras.datasets.mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train \/ 255.0, x_test \/ 255.0\n\nx_train = x_train.reshape((x_train.shape[0], x_train.shape[1], x_train.shape[2], 1))\nx_test = x_test.reshape((x_test.shape[0], x_test.shape[1], x_test.shape[2], 1))\nprint(x_train.shape)\n","0c5c7a20":"model = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28,1)),\n  tf.keras.layers.Dense(256, activation='relu'),\n  tf.keras.layers.Dense(256, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nprint(model.summary())","d13639b2":"history = model.fit(x_train, y_train, batch_size=32, epochs=2, validation_split=0.2, verbose=0)","78953d3d":"show_history(history)","8b76ad5d":"evaluate = model.evaluate(x_test,  y_test, verbose=0)\nprint(\"Accuracy test of = \" + str(evaluate[1]))","f24a962f":"import keras\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv2D, Flatten, Dense, BatchNormalization\n\n\nmodelConv = Sequential()\nmodelConv.add(Conv2D(32, (3, 3), activation='relu', input_shape=x_train.shape[1:]))\nmodelConv.add(BatchNormalization())\nmodelConv.add(Conv2D(32, (3, 3)))\nmodelConv.add(BatchNormalization())\nmodelConv.add(Flatten())\nmodelConv.add(Dense(32, activation='relu'))\nmodelConv.add(Dense(10, activation='softmax'))\n\n\nmodelConv.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nprint(modelConv.summary())","3b638634":"from tensorflow.keras.callbacks import ModelCheckpoint\n\ncheckpointer = ModelCheckpoint(filepath=\"best_weights.hdf5\", \n                               monitor = 'val_accuracy',\n                               verbose=1, \n                               save_best_only=True)\n\nhistory = modelConv.fit(x_train, y_train, \n                        batch_size=32, \n                        epochs=10, \n                        #callbacks=[checkpointer],\n                        validation_split=0.2)","c36ba424":"show_history(history)","4f6f1591":"modelConv.evaluate(x_test,  y_test, verbose=2)[1]","fc9bbc20":"print(modelConv.layers[0])\n\nprint(modelConv.layers)\n","56f37663":"layer_outputs = [layer.output for layer in modelConv.layers[:12]] # Extracts the outputs of the top 12 layers\nactivation_model = tf.keras.models.Model(inputs=modelConv.input, outputs=layer_outputs) # Creates a model that will return these outputs, given the model input","a79f67c4":"activations = activation_model.predict(x_test[:1])\nfirst_layer_activation = activations[0]\nprint(first_layer_activation.shape)","36e6dd9d":"plt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')","3abe5447":"layer_names = []\nfor layer in modelConv.layers[:12]:\n    layer_names.append(layer.name) # Names of the layers, so you can have them as part of your plot\n    \nimages_per_row = 16\n\nfor layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n    n_features = layer_activation.shape[-1] # Number of features in the feature map\n    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n    n_cols = n_features \/\/ images_per_row # Tiles the activation channels in this matrix\n    display_grid = np.zeros((size * n_cols, images_per_row * size))\n    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n        for row in range(images_per_row):\n            channel_image = layer_activation[0,\n                                             :, :,\n                                             col * images_per_row + row]\n            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n            channel_image \/= channel_image.std()\n            channel_image *= 64\n            channel_image += 128\n            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n            display_grid[col * size : (col + 1) * size, # Displays the grid\n                         row * size : (row + 1) * size] = channel_image\n    scale = 1. \/ size\n    plt.figure(figsize=(scale * display_grid.shape[1],\n                        scale * display_grid.shape[0]))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n","4ab4ab14":"Ref: https:\/\/towardsdatascience.com\/visualizing-intermediate-activation-in-convolutional-neural-networks-with-keras-260b36d60d0","056666e6":"## Refs \n\nhttps:\/\/www.tensorflow.org\/install\n\nhttps:\/\/www.tensorflow.org\/tutorials\/keras\/classification\n    "}}