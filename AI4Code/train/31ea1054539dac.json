{"cell_type":{"eff73f0a":"code","3442c9bf":"code","6bec9ae3":"code","2951e610":"code","0f549301":"code","2bf4ae83":"code","8075ceef":"code","53865f77":"code","e2199b69":"code","c4442e89":"code","e38b3f30":"code","a29a997a":"code","1accc63f":"code","e98f01de":"code","763003ed":"markdown","9ac1831e":"markdown","48cbffcd":"markdown","c5598ae8":"markdown","f63e1d48":"markdown","7093c0cd":"markdown","18772924":"markdown","3cdfa599":"markdown","15069eb5":"markdown","e926c1da":"markdown","43fce2fb":"markdown","ecdac212":"markdown","1bb81491":"markdown","595ef91b":"markdown","e6416225":"markdown","7defbaa2":"markdown","4fb2cbf2":"markdown","9fc579d6":"markdown","4dd39efb":"markdown"},"source":{"eff73f0a":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler","3442c9bf":"data = pd.read_csv('..\/input\/social\/Social_Network_Ads.csv')\nprint('Dataset :',data.shape)\ndata.info()\ndata[0:10]","6bec9ae3":"cnt_pro = data['Purchased'].value_counts()\nplt.figure(figsize=(6,4))\nsns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Kelas', fontsize=12)\nplt.xticks(rotation=90)\nplt.show();","2951e610":"sns.set_style(\"whitegrid\")\nsns.pairplot(data,hue=\"Purchased\",size=3);\nplt.show()","0f549301":"data = data[['User ID','Gender','Age','EstimatedSalary','Purchased']] #Subsetting the data\ncor = data.corr() #Calculate the correlation of the above variables\nsns.heatmap(cor, square = True) #Plot the correlation as heat map","2bf4ae83":"#Convert sting to numeric\nGender  = {'Male': 1,'Female': 0} \n  \n# traversing through dataframe \n# Gender column and writing \n# values where key matches \ndata.Gender = [Gender[item] for item in data.Gender] \nprint(data)","8075ceef":"from sklearn.model_selection import train_test_split\nY = data['Purchased']\nX = data.drop(columns=['Purchased'])\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=9)","53865f77":"print('X train shape: ', X_train.shape)\nprint('Y train shape: ', Y_train.shape)\nprint('X test shape: ', X_test.shape)\nprint('Y test shape: ', Y_test.shape)","e2199b69":"# We define the number of trees in the forest in 100. \n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n\n# We define the model\nrfcla = RandomForestClassifier(n_estimators=100,random_state=9,n_jobs=-1)\n\n# We train model\nrfcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict5 = rfcla.predict(X_test)","c4442e89":"test_acc_rfcla = round(rfcla.fit(X_train,Y_train).score(X_test, Y_test)* 100, 2)\ntrain_acc_rfcla = round(rfcla.fit(X_train, Y_train).score(X_train, Y_train)* 100, 2)","e38b3f30":"# The confusion matrix\nrfcla_cm = confusion_matrix(Y_test, Y_predict5)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(rfcla_cm, annot=True, linewidth=0.7, linecolor='black', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('Random Forest Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","a29a997a":"model1 = pd.DataFrame({\n    'Model': ['Random Forest'],\n    'Train Score': [train_acc_rfcla],\n    'Test Score': [test_acc_rfcla]\n})\nmodel1.sort_values(by='Test Score', ascending=False)","1accc63f":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y_test, Y_predict5)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","e98f01de":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nimport matplotlib.pyplot as plt\n\ndisp = plot_precision_recall_curve(rfcla,X_train, Y_train)\ndisp.ax_.set_title('2-class Precision-Recall curve: '\n                   'AP={0:0.2f}'.format(average_precision))","763003ed":"Build Model use Random Forest","9ac1831e":"# Accuracy\nis closeness of the measurements to a specific value. Accuracy has two definitions:\n\n1. More commonly, it is a description of systematic errors, a measure of statistical bias; low accuracy causes a difference between a result and a \"true\" value. ISO calls this trueness.\n1. Alternatively, ISO defines[[1](https:\/\/en.wikipedia.org\/wiki\/Accuracy_and_precision)] accuracy as describing a combination of both types of observational error above (random and systematic), so high accuracy requires both high precision and high trueness.","48cbffcd":"# it\u2019s time for a practical example","c5598ae8":"# Confusion matrix\nConfusion Matrix is commonly used for a summarization of prediction results\non a classification problem.The number of correct and incorrect predictions \nis summarized with counting values and each value broken down for each class. \nEach of them is the key to the confusion matrix. It shows the classification \nmodel is confused when it makes predictions, at this point in here it gives us \ninsight not only into the errors being made by a classifier but also show the \ntypes of errors that are being made [[3](https:\/\/www.geeksforgeeks.org\/confusion-matrix-machine-learning\/)]. ","f63e1d48":"In this work, we tried to classifying salary using Random Forest classification. Predictor variable:\n\n1.  Gender         \n1.  Age              \n1.  EstimatedSalary  \n1.  Purchased                     \n","7093c0cd":"**VISUALIZING THE DATA\n**","18772924":"# Precision and Recall\nPrecision is a description of random errors, a measure of statistical variability. In simpler terms, given a set of data points from repeated measurements of the same quantity, the set can be said to be accurate if their average is close to the true value of the quantity being measured, while the set can be said to be precise if the values are close to each other. While Recall is defined as the fraction of relevant documents retrieved compared to the total number of relevant documents (true positives divided by true positives+false negatives).","3cdfa599":"# Random forest classification","15069eb5":"As you can see above, we obtain the heatmap of correlation among the variables. The color palette in the side represents the amount of correlation among the variables. The lighter shade represents a high correlation.","e926c1da":"# Plotting Heatmap\nHeatmap can be defined as a method of graphically representing numerical data where individual data points contained in the matrix are represented using different colors. The colors in the heatmap can denote the frequency of an event, the performance of various metrics in the data set, and so on. Different color schemes are selected by varying businesses to present the data they want to be plotted on a heatmap [2].","43fce2fb":"![https:\/\/admin-me.com\/wp-content\/uploads\/2018\/11\/2018-November-14th-Salary-1024x683.jpg](https:\/\/admin-me.com\/wp-content\/uploads\/2018\/11\/2018-November-14th-Salary-1024x683.jpg)","ecdac212":"![https:\/\/miro.medium.com\/max\/592\/1*i0o8mjFfCn-uD79-F1Cqkw.png](https:\/\/miro.medium.com\/max\/592\/1*i0o8mjFfCn-uD79-F1Cqkw.png)","1bb81491":"# Result","595ef91b":"# Measurement\nWhen the classification process was already done. This work evaluated\nthe results using the Confusion Matrix.","e6416225":"Import Library","7defbaa2":"Read Dataset","4fb2cbf2":"# Working of Random forest classification\nRandom forest is a supervised learning algorithm that creates a forest randomly. This forest, is a set of decision trees, most of the times trained with the bagging method. The essential idea of bagging is to average many noisy but approximately impartial models, and therefore reduce the variation. Each tree is constructed using the following algorithm:\n\n* Let $N$ be the number of test cases, $M$ is the number of variables in the classifier.\n* Let $m$ be the number of input variables to be used to determine the decision in a given node; $m<M$.\n* Choose a training set for this tree and use the rest of the test cases to estimate the error.\n* For each node of the tree, randomly choose $m$ variables on which to base the decision. Calculate the best partition of the training set from the $m$ variables.\n\n\n**Advantages Random Forest:**\n* runtimes are quite fast\n* Are able to deal with unbalanced and missing data\n\n\n\n**Random Forest is a prediction ensemble, in which:\n* The training dataset :  divided into sub datasets.\n* Each sub dataset is processed to generate a prediction model.\n* Each prediction model is applied to each test data\n* A prediction is by using the \u201cmaximum voting\u201d method.\n\n**To illustrate the concept, you can see below:\n**","9fc579d6":"# SPLITING DATA\nData for training and testing\nTo select a set of training data that will be input in the Machine Learning algorithm, to ensure that the classification algorithm training can be generalized well to new data. For this study using a sample size of 10% ( aims to reduce the overfitting effect).","4dd39efb":"To understand the random forest model, we must first learn about the decision tree, the basic building block of a random forest. We need to talk about trees before we can get into forests.   A decision tree is a flowchart-like tree structure where an internal node represents feature, the branch represents a decision rule, and each leaf node represents the outcome. The decision tree analyzes a set of data to construct a set of rules or questions, which are used to predict a class, i.e., the goal of decision tree is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. In this sense the decision tree selects the best attribute using to divide the records, converting that attribute into a decision node and dividing the data set into smaller subsets, to finally start the construction of the tree repeating this process recursively.\n\nExample\n![https:\/\/cdn-images-1.medium.com\/max\/824\/0*J2l5dvJ2jqRwGDfG.png](https:\/\/cdn-images-1.medium.com\/max\/824\/0*J2l5dvJ2jqRwGDfG.png)\n\n\nPrinciple of the decision tree\u2019s algorithm\n\n* Root Node: This attribute is used for dividing the data into two or more sets. The feature attribute in this node is selected based on Attribute Selection Techniques.\n* Branch or Sub-Tree: A part of the entire decision tree is called branch or sub-tree.\n* Splitting: Dividing a node into two or more sub-nodes based on if-else conditions.\n* Decision Node: After splitting the sub-nodes into further sub-nodes, then it is called as the decision node.\n* Leaf or Terminal Node: This is the end of the decision tree where it cannot be split into further sub-nodes.\n* Pruning: Removing a sub-node from the tree is called pruning.\n\n\n![https:\/\/cdn-images-1.medium.com\/max\/688\/0*pb-1ufHK-OmR8k7r.png](https:\/\/cdn-images-1.medium.com\/max\/688\/0*pb-1ufHK-OmR8k7r.png)\n\n\n\nSource: https:\/\/www.kdnuggets.com\/"}}