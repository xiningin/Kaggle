{"cell_type":{"883ccf6d":"code","45d47e7f":"code","64f6b5da":"code","a6d70d6b":"code","e1277abb":"code","6c6a4c37":"code","1f3b8322":"code","e4de8db1":"code","2830bbe2":"code","97ab6f4c":"code","f9aa7565":"code","0cd9fe49":"code","6ea043e6":"code","7baad20c":"code","c1ffeb36":"code","11874801":"code","9740706d":"code","8e97e071":"code","e488ab63":"code","c496b452":"code","852fdf57":"code","4b417b32":"code","b6d0d4ec":"code","ddd763b8":"markdown","6f9fcc0b":"markdown","21061e8a":"markdown","85c382f1":"markdown","b066df06":"markdown","dfa7c9ff":"markdown","d4f84533":"markdown","07470669":"markdown","c48dc34e":"markdown","83cfe9a9":"markdown","5d988ea8":"markdown","2b21a0bd":"markdown","bb662267":"markdown"},"source":{"883ccf6d":"!pip3 install face_recognition\nimport face_recognition","45d47e7f":"import os\nimport sys\nimport random\nimport warnings\nfrom pylab import imshow, show, get_cmap\n\nimport numpy as np\nimport pandas as pd\nfrom numpy import random\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom itertools import chain\nimport skimage\nfrom PIL import Image\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.util import crop, pad\nfrom skimage.morphology import label\n\nfrom keras.models import Model, load_model,Sequential\nfrom keras.layers import Input, Dense, UpSampling2D, Flatten, Reshape\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\nfrom keras import backend as K\n\nimport tensorflow as tf\n\nIMG_WIDTH = 128\nIMG_HEIGHT = 128\nIMG_CHANNELS = 3\nINPUT_SHAPE=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\nD_INPUT_SHAPE=[192]\nTRAIN_PATH = '..\/input\/lagdataset_200\/LAGdataset_200\/'\n\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage')\nseed = 42\nrandom.seed = seed\nnp.random.seed = seed","64f6b5da":"def FaceCrop(image):\n    face_locations = face_recognition.face_locations(image)\n    top, right, bottom, left = face_locations[0]\n    image = image[top:bottom,left:right]\n    return image","a6d70d6b":"%%time\ntrain_ids = next(os.walk(TRAIN_PATH))[2]\nX_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nfinal_train_ids = []\nmissing_count = 0\nprint('Getting train images ... ')\nsys.stdout.flush()\nfor n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n    path = TRAIN_PATH + id_+''\n    try:\n        img = imread(path)\n        img = FaceCrop(img)\n        img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n        X_train[n-missing_count] = img\n        final_train_ids.append(id_)\n    except:\n        missing_count += 1\n        \nprint(\"Total missing: \"+ str(missing_count))\nX_train = X_train[0:X_train.shape[0]-missing_count]","e1277abb":"for n in range(0,5):\n    imshow(X_train[n])\n    plt.show()","6c6a4c37":"X_train = X_train.astype('float32') \/ 255.\nX_train_noisy = X_train + 0.1 * np.random.normal(size=X_train.shape)\n\nX_train_noisy = np.clip(X_train_noisy, 0., 1.)","1f3b8322":"def Encoder():\n    inp = Input(shape=INPUT_SHAPE)\n    x = Conv2D(128, (4, 4), activation='elu', padding='same',name='encode1')(inp)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode2')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode3')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same',name='encode4')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same',name='encode5')(x)\n    x = Conv2D(16, (2, 2), activation='elu', padding='same',name='encode6')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same',name='encode7')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same',name='encode8')(x)\n    x = Conv2D(16, (2, 2), activation='elu', padding='same',name='encode9')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same',name='encode10')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same',name='encode11')(x)\n    x = Conv2D(16, (2, 2), activation='elu', padding='same',name='encode12')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(32, (4, 4), activation='elu', padding='same',name='encode13')(x)\n    x = Conv2D(16, (3, 3), activation='elu', padding='same',name='encode14')(x)\n    x = Conv2D(16, (2, 2), activation='elu', padding='same',name='encode15')(x)\n    x = Conv2D(3, (3, 3), activation='elu', padding='same',name='encode16')(x)\n    x = Flatten()(x)\n    x = Dense(256, activation='elu',name='encode17')(x)\n    encoded = Dense(D_INPUT_SHAPE[0], activation='sigmoid',name='encode18')(x)\n    return Model(inp, encoded)\n\nencoder = Encoder()\nencoder.summary()","e4de8db1":"def Decoder():\n    inp = Input(shape=D_INPUT_SHAPE, name='decoder')\n    x = Dense(D_INPUT_SHAPE[0], activation='elu', name='decode1')(inp)\n    x = Dense(192, activation='elu', name='decode2')(x)\n    x = Reshape((8, 8, 3))(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode3')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode4')(x)\n    x = Conv2DTranspose(64, (3, 3), strides=(2, 2), activation='elu', padding='same', name='decodetrans1')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode5')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode6')(x)\n    x = Conv2DTranspose(64, (3, 3), strides=(2, 2), activation='elu', padding='same', name='decodetrans2')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode7')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode8')(x)\n    x = Conv2DTranspose(64, (3, 3), strides=(2, 2), activation='elu', padding='same', name='decodetrans3')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same', name='decode9')(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same', name='decode10')(x)\n    x = Conv2DTranspose(128, (3, 3), strides=(2, 2), activation='elu', padding='same', name='decodetrans4')(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same', name='decode11')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode12')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode13')(x)\n    x = Conv2D(16, (1, 1), activation='elu', padding='same', name='decode14')(x)\n    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same', name='decode15')(x)\n    return Model(inp, decoded)\n\ndecoder = Decoder()\ndecoder.summary()","2830bbe2":"def Autoencoder():\n    inp = Input(shape=INPUT_SHAPE)\n    x = Conv2D(128, (4, 4), activation='elu', padding='same',name='encode1')(inp)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode2')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode3')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same',name='encode4')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same',name='encode5')(x)\n    x = Conv2D(16, (2, 2), activation='elu', padding='same',name='encode6')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same',name='encode7')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same',name='encode8')(x)\n    x = Conv2D(16, (2, 2), activation='elu', padding='same',name='encode9')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same',name='encode10')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same',name='encode11')(x)\n    x = Conv2D(16, (2, 2), activation='elu', padding='same',name='encode12')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(32, (4, 4), activation='elu', padding='same',name='encode13')(x)\n    x = Conv2D(16, (3, 3), activation='elu', padding='same',name='encode14')(x)\n    x = Conv2D(16, (2, 2), activation='elu', padding='same',name='encode15')(x)\n    x = Conv2D(3, (3, 3), activation='elu', padding='same',name='encode16')(x)\n    x = Flatten()(x)\n    x = Dense(256, activation='elu',name='encode17')(x)\n    encoded = Dense(D_INPUT_SHAPE[0], activation='sigmoid',name='encode18')(x)\n    x = Dense(D_INPUT_SHAPE[0], activation='elu', name='decode1')(encoded)\n    x = Dense(192, activation='elu', name='decode2')(x)\n    x = Reshape((8, 8, 3))(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode3')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode4')(x)\n    x = Conv2DTranspose(64, (3, 3), strides=(2, 2), activation='elu', padding='same', name='decodetrans1')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode5')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode6')(x)\n    x = Conv2DTranspose(64, (3, 3), strides=(2, 2), activation='elu', padding='same', name='decodetrans2')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode7')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode8')(x)\n    x = Conv2DTranspose(64, (3, 3), strides=(2, 2), activation='elu', padding='same', name='decodetrans3')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same', name='decode9')(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same', name='decode10')(x)\n    x = Conv2DTranspose(128, (3, 3), strides=(2, 2), activation='elu', padding='same', name='decodetrans4')(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same', name='decode11')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode12')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode13')(x)\n    x = Conv2D(16, (1, 1), activation='elu', padding='same', name='decode14')(x)\n    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same', name='decode15')(x)\n    return Model(inp, decoded)\n\nmodel = Autoencoder()\nmodel.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\nmodel.summary()","97ab6f4c":"learning_rate_reduction = ReduceLROnPlateau(monitor='loss', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5,\n                                            min_lr=0.00001)\nfilepath = \"Face_Auto_Model.h5\"\ncheckpoint = ModelCheckpoint(filepath,\n                             save_best_only=True,\n                             monitor='loss',\n                             mode='min')\n\nearly_stopping = EarlyStopping(monitor='loss',\n                              patience=3,\n                              verbose=1,\n                              mode='min',\n                              restore_best_weights=True)","f9aa7565":"D_train_noise = random.random((X_train.shape[0], D_INPUT_SHAPE[0]))\n\nrandom_decoder = Decoder()\nrandom_decoder.compile(optimizer='adam', loss='mean_squared_error')","0cd9fe49":"%%time \nrandom_decoder.fit(D_train_noise, X_train,\n          epochs=5, \n          batch_size=32,\n         callbacks=[learning_rate_reduction, checkpoint, early_stopping])","6ea043e6":"D_test_noise = random.random((100, D_INPUT_SHAPE[0]))\n\nTest_imgs = random_decoder.predict(D_test_noise)","7baad20c":"plt.figure(figsize=(20, 4))\nfor i in range(5):\n    plt.subplot(2, 10, i + 1)\n    plt.imshow(Test_imgs[i].reshape(INPUT_SHAPE))\n    plt.axis('off')\n \nplt.tight_layout()\nplt.show()","c1ffeb36":"%%time \nmodel.fit(X_train_noisy, X_train,\n          epochs=70,\n          batch_size=50,\n         callbacks=[learning_rate_reduction, checkpoint, early_stopping])","11874801":"decoded_imgs = model.predict(X_train_noisy)","9740706d":"plt.figure(figsize=(20, 4))\nfor i in range(5):\n    # original\n    plt.subplot(2, 10, i + 1)\n    plt.imshow(X_train[i].reshape(INPUT_SHAPE))\n    plt.axis('off')\n \n    # reconstruction\n    plt.subplot(2, 10, i + 1 + 10)\n    plt.imshow(decoded_imgs[i].reshape(INPUT_SHAPE))\n    plt.axis('off')\n \nplt.tight_layout()\nplt.show()","8e97e071":"model.save('Face_Auto_Model.hdf5')\nmodel.save_weights(\"Face_Auto_Weights.hdf5\")","e488ab63":"encoder = Encoder()\ndecoder = Decoder()\n\nencoder.load_weights(\"Face_Auto_Weights.hdf5\", by_name=True)\ndecoder.load_weights(\"Face_Auto_Weights.hdf5\", by_name=True)","c496b452":"Encoder_predicts = encoder.predict(X_train)","852fdf57":"func = K.function([decoder.input, K.learning_phase()],\n                        [decoder.output])\n\nrand_vecs = np.random.normal(0.0, 1.0, (50, D_INPUT_SHAPE[0]))\n\nx_mean = np.mean(Encoder_predicts, axis=0)\nx_stds = np.std(Encoder_predicts, axis=0)\nx_cov = np.cov((Encoder_predicts - x_mean).T)\ne, v = np.linalg.eig(x_cov)\n\nprint(x_mean)\nprint(x_stds)\nprint(x_cov)","4b417b32":"e_list = e.tolist()\ne_list.sort(reverse=True)\nplt.clf()\nplt.bar(np.arange(e.shape[0]), e_list, align='center')\nplt.draw()\n\nx_vecs = x_mean + np.dot(v, (rand_vecs * e).T).T\ny_faces = func([x_vecs, 0])[0]","b6d0d4ec":"plt.figure(figsize=(50, 20))\nfor i in range(50):\n    plt.subplot(5, 10, i + 1)\n    plt.imshow(y_faces[i])\n    plt.axis('off')","ddd763b8":"# Read in the Faces\n\nFor preprocessing, the face recognition package will be used to find the bounding box around the face in the image and cut out the surrounding areas. Since the faces are taken from different areas and radically different hairstyles, limiting the area to just the face makes it a little easier on our model and focus on the most important features.","6f9fcc0b":"# Sample the Random Decoder","21061e8a":"# Create the Models\n\nWe will create three models, the encoder, the decoder, and the autoencoder which is a combination of the 2. Make sure to keep the names of the layers consistent with the autoencoder as we will be setting the weights by_name after training the autoencoder.","85c382f1":"# Sample the Autoencoder Model","b066df06":"# Train the Autoencoder\n\nNow to train the autoencoder proper. Standard autoencoder training procedure here except that we will not use any validation splits. The loss will use the ReduceLROnPlateau a few times before it is over. Takes around 1 hour.","dfa7c9ff":"# Sample New Faces\n\nHere is a selection of the new random faces.","d4f84533":"# Add Noise\n\nIt is usually a good idea to add some noise to the training images when making an autoencoder.","07470669":"The result is the most average image the model could make. In a fairly uniform dataset like this one, we get a pretty clear face as a result with all the important features.","c48dc34e":"# Checkpoints\n\nGood to have some checkpoints for the models. The autoencoder really only benefits from ReduceLROnPlateau, the other checkpoints are just standard. ","83cfe9a9":"# Train a Decoder on Random Data\n\nFirst thing, just for fun, let's quickly see what happens when we train just the decoder on random noise. By training the decoder on random noise we force the model to make average predictions on everything so we can see the most common features throughout the dataset.","5d988ea8":"# Generate New Autoencoded Faces\n\nIn order to generate new faces, we will use PCA on the encoded results to make new \"random\" data that is still normally distributed in a similar way as the actual face results. I used some code found in this repository to get this part working correctly: https:\/\/github.com\/HackerPoet\/FaceEditor","2b21a0bd":"# Making Faces Using an Autoencoder\n\nAutoencoders learn to compress data into a smaller frame and then reconstruct that data from that frame. When a computer encodes data this way, it is basically simplifying the data into what features it finds to be the most useful. This notebook will train an autoencoder on faces, then use PCA to create new encoded data that looks similar enough to our training data to create artificial faces based on the features that the neural network found was important.","bb662267":"# Results\n\nThe results are pretty good, farly clear faces with a lot of variety between them. We can automatically make more or manually adjust features in the array to get a feel for key features that the neural network found to be the most important. \n\nIf you enjoyed this notebook, please like, comment, and check out some of my other notebooks on Kaggle: \n\nMaking AI Dance Videos: https:\/\/www.kaggle.com\/valkling\/how-to-teach-an-ai-to-dance\n\nImage Colorization: https:\/\/www.kaggle.com\/valkling\/image-colorization-using-autoencoders-and-resnet\/notebook\n\nStar Wars Steganography: https:\/\/www.kaggle.com\/valkling\/steganography-hiding-star-wars-scripts-in-images"}}