{"cell_type":{"7aee350a":"code","5b04f062":"code","72418747":"code","58c85d9b":"code","ad39b3ef":"code","57219a5b":"code","7c5d5bf9":"code","85733900":"code","0a4706cb":"code","23148661":"code","79c1d6cf":"code","4cd641ba":"code","14ade17b":"code","3c6d9288":"code","d7c19a8d":"code","e133cabe":"code","bfa523d8":"markdown","4e6d4a3b":"markdown","09a11069":"markdown","dc06d0cf":"markdown","c7f488a8":"markdown","549a3c70":"markdown","b323295f":"markdown","67a6b7ee":"markdown","3a9bc9e3":"markdown","c2e4b862":"markdown","c5421ccc":"markdown","79f030e4":"markdown","d766a2ad":"markdown"},"source":{"7aee350a":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5b04f062":"#importing other libraries\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics","72418747":"data=pd.read_csv('..\/input\/speed-dating-experiment\/Speed Dating Data.csv', encoding=\"ISO-8859-1\")\ndata.head()","58c85d9b":"print(pd.crosstab(data.gender,data.match))\nprint(pd.crosstab(data.gender,data.met))\nprint(pd.crosstab(data.gender,data.dec))","ad39b3ef":"#so for females the match ratio is:\nF_match_ratio=690\/3494\nprint(\"For females the match ratio is\",F_match_ratio*100)\n#Ratio of meeting a male for a female:\nF_met=1-2018\/(2018+183+1791+1+2+1+2+1)\nprint(\"Ratio of meeting a male for a female\",F_met*100)\n#decision_of_female_accpeting_male\nF_Dec=2655\/(2655+1529)\nprint(\"Decision_of_female_accpeting_male\",F_Dec*100)\n#so for males the match ratio is:\nM_match_ratio=690\/3504\nprint(\"For males the match ratio is:\",M_match_ratio*100)\n#Ratio of meeting a Female for a male:\nM_met=1-2029\/(2029+168+1806+1)\nprint(\"Ratio of meeting a Female for a male:\",M_met*100)\n#decision_of_male_accpeting_female\nM_Dec=2205\/(2205+1989)\nprint(\"Decision_of_male_accpeting_female\",M_Dec*100)","57219a5b":"plt.hist(data.age.values)\nplt.xlabel('Age')\nplt.ylabel('Frequency')","7c5d5bf9":"for column in data.columns:\n    if(data[column].isnull().sum())>1000:\n        data.drop([column],axis=1,inplace=True)","85733900":"data.shape","0a4706cb":"data.dropna(inplace=True)\ndata.shape","23148661":"data.dtypes","79c1d6cf":"data.drop(['career','from','field'],axis=1,inplace=True)\ndata.shape","4cd641ba":"plt.subplots(figsize=(20,15))\nax = plt.axes()\nax.set_title(\"Correlation Heatmap\")\ncorr = data.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)","14ade17b":"#prepare the data\nX=data[['like','dec']]\ny=data['match']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","3c6d9288":"model = LogisticRegression(random_state=0)\nlrc = model.fit(X_train, y_train)\npredict_train_lrc = lrc.predict(X_train)\npredict_test_lrc = lrc.predict(X_test)\nprint('Training Accuracy:', metrics.accuracy_score(y_train, predict_train_lrc))\nprint('Production Accuracy:', metrics.accuracy_score(y_test, predict_test_lrc))","d7c19a8d":"model = RandomForestClassifier()\nrf_model = model.fit(X_train, y_train)\npredict_train_rf = rf_model.predict(X_train)\npredict_test_rf = rf_model.predict(X_test)\nprint('Training Accuracy:', metrics.accuracy_score(y_train, predict_train_rf))\nprint('Production Accuracy:', metrics.accuracy_score(y_test, predict_test_rf))","e133cabe":"model = GradientBoostingClassifier()\nxgb_model = model.fit(X_train, y_train)\npredict_train_xgb = xgb_model.predict(X_train)\npredict_test_xgb = xgb_model.predict(X_test)\nprint('Training Accuracy:', metrics.accuracy_score(y_train, predict_train_xgb))\nprint('Production Accuracy:', metrics.accuracy_score(y_test, predict_test_xgb))","bfa523d8":"**RANDOM FOREST**","4e6d4a3b":"Good to see some columns being dropped!!!","09a11069":"**Shortening the dataframe by deleting rows with null value**","dc06d0cf":"**XG_BOOST**","c7f488a8":"**Dropping columns which has null values summation greater than 1000 as they were of no or very little use**","549a3c70":"**LOGISTIC REGRESSION**","b323295f":"From data above we can infer that For example, men (gender = 1) seem to have a preference for attractive partners (attr1_1) while women (gender = 0) seem to have a preference for ambitious partners (amb1_1)","67a6b7ee":"**Looks like Random Forest has given me the best result**","3a9bc9e3":"Dropping categorival data to lower the complexity or they can also be transformed to OneHoEncoder but i am not using as they were many categories","c2e4b862":"So majority of Frequency lies between mid twenties to late twenties","c5421ccc":"**Let us now verify the datatypes of our data and see if everything is numerical data**","79f030e4":"> MODELS","d766a2ad":"Now, Some rows were dropped as well"}}