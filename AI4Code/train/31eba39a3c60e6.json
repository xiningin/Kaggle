{"cell_type":{"d66d467e":"code","f164bba6":"code","c78e44dc":"code","c90ba145":"code","dba72ea0":"code","adca8a17":"code","d1cf18d1":"code","d80bbd2d":"code","a7d99efd":"code","01510151":"code","5c48fd44":"code","a77cd152":"code","82761517":"code","b21c8fd4":"code","a854b2fa":"code","71290350":"code","5e1e22e3":"code","ede9da5d":"code","a4b12fe6":"code","e23e6e42":"code","73f4bb02":"code","214e425b":"code","14774970":"code","2ff1df8c":"code","787935bc":"markdown","a83cb241":"markdown","4b11f5dc":"markdown","476cb23c":"markdown","470197d7":"markdown"},"source":{"d66d467e":"#Importing the necessary modules\n#Importando as bibliotecas necessarias\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier","f164bba6":"#Converting the dataset to dataframe\n#Convertendo o dataset para um dataframe\n\n#Local Files\n#Arquivos Locais\n#train = pd.read_csv(\"input\/train.csv\") \n#test = pd.read_csv(\"input\/test.csv\")\n\n#Kaggle Kernel Files\n#Arquivos do Kernel do Kaggle\ntrain = pd.read_csv(\"..\/input\/train.csv\") \ntest = pd.read_csv(\"..\/input\/test.csv\") ","c78e44dc":"#It shows the first 5 rows of the dataframe\n#Mostra as 5 primeiras linhas do dataframe\ntrain.head()","c90ba145":"#Look that the 'test' dataframe doesn't have a 'Survived' column, it's what we want to predict\n#Observe que o dataframe de \"test\" nao possui a coluna \"Survived\", pois isso e o que queremos prever futuramente.\ntest.head()","dba72ea0":"#It removes the specific columns, like 'Name', 'Ticket', 'Cabin', in this case\n#If you don't want to create a new dataframe you need to set the 'inplace' parameter as 'True'\n'''\nRemove as colunas especificadas, no caso quando desejar remover mais de uma coluna de uma so vez,\ndeve-se utilizar uma lista como primeiro argumento.\nPoderia alterar\/remover diretamente no dataset, sem precisar atribuir novamente a um dataframe.\nDeveria utilizar entao: train.drop([\"Name\", \"Ticket\", \"Cabin\"], axis=1, inplace=True)\n'''\n\ntrain = train.drop([\"Name\", \"Ticket\", \"Cabin\"], axis=1)\ntest = test.drop([\"Name\", \"Ticket\", \"Cabin\"], axis=1)","adca8a17":"#Now it shows the new dataframe without the dropped columns\n#We need to apply the changes in both 'train' and 'test' datasets\n#Imprime novamente o \"head\", mas agora apos o .drop, nao ha mais as colunas \"Name\", \"Ticket\", \"Cabin\"\n#Deve-se sempre executar os \"mesmas\" manipulacoes de dados para \"train\" e \"test\", se nao teriamos um erro\ntrain.head()","d1cf18d1":"test.head()","d80bbd2d":"#Applies the one_hot_encoding for the 'Sex' and 'Embarked' features\n#Aplica one_hot_encoding para a feature de \"Sex\" e \"Embarked\"\n#one_hot_train = pd.get_dummies(train)\n#one_hot_test = pd.get_dummies(test)\nnew_data_train = pd.get_dummies(train)\nnew_data_test = pd.get_dummies(test)","a7d99efd":"#Now we can see that the 'Sex' and 'Embarked' are now numerical columns.\n#Observamos agora as colunas que nao eram numericas, seguindo o one_hot_encoding.\nnew_data_train.head()","01510151":"#Checks if there is a NaN value for the training data, 'train' data.\n#Verifica e agrupa a quantidade de valores nulos(NaN) para \"train\" data\nnew_data_train.isnull().sum().sort_values(ascending=False).head(10)","5c48fd44":"#We will use the mean 'Age' of the dataset for the NaN values\n#Atribui a media da coluna \"Age\" para os valores nulos(NaN)\nnew_data_train[\"Age\"].fillna(new_data_train[\"Age\"].mean(), inplace=True)\nnew_data_test[\"Age\"].fillna(new_data_test[\"Age\"].mean(), inplace=True)","a77cd152":"#Checks if there is a NaN value for the testing data, 'test' data\n#Verifica e agrupa a quantidade de valores nulos(NaN) para \"test\" data\nnew_data_test.isnull().sum().sort_values(ascending=False).head(10)","82761517":"#We will use the mean 'Fare' for the NaN values\n#Atribui a media da Coluna \"Fare\" para os valores nulos(NaN)\nnew_data_test[\"Fare\"].fillna(new_data_test[\"Fare\"].mean(), inplace=True)","b21c8fd4":"#Splitting the 'features' and 'targets' for the model, as X and y\n#Separando \"features\" e \"targets\" para o modelo, X e y respectivamente\nX = new_data_train.drop(\"Survived\", axis=1)\ny = new_data_train[\"Survived\"]","a854b2fa":"#We will use a Decision Tree Model as the Machine Learning Algorithm\n#Utilizaremos Decision Tree, como algoritmo de Machine Learning\ntree = DecisionTreeClassifier(max_depth = 10, random_state = 0)\ntree.fit(X, y)","71290350":"#tree.score(X, y)","5e1e22e3":"#Import the necessary modules for the model\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split ","ede9da5d":"#It's already done, without the 'Survived' columns and with all the features prepared\n#J\u00e1 esta to normalizado, sem a coluna \"Survived\" e a feature \"Sex\" ja tratada pelo\"one-hot-encoded\"\nX.head()","a4b12fe6":"#Test Data\n#Xtest = new_data_test.drop([\"Survived\"], axis=1)\nXtest = new_data_test\nXtest.head()","e23e6e42":"Xtrain, Xvalidation, Ytrain, Yvalidation = train_test_split(X, y, test_size=0.2, random_state=True)","73f4bb02":"#Model\nmodel = RandomForestClassifier(n_estimators=100,\n                               max_leaf_nodes=12,\n                               max_depth=12,\n                               random_state=0)\nmodel.fit(Xtrain, Ytrain)\n#model.score(Xtrain, Ytrain)","214e425b":"#Prediction\nfrom sklearn.metrics import accuracy_score\nYprediction = model.predict(Xvalidation)\naccuracy_score(Yvalidation, Yprediction)","14774970":"#Submission\n#We create a new dataframe for the submission\nsubmission = pd.DataFrame()\n\nsubmission[\"PassengerId\"] = Xtest[\"PassengerId\"]\nsubmission[\"Survived\"] = model.predict(Xtest)\n\n#We save the submission as a '.csv' file\nsubmission.to_csv(\"submission.csv\", index=False)","2ff1df8c":"submission.head()","787935bc":"## Decision Tree Results\nThe decision tree model has an 0.77990 accuracy in the competition","a83cb241":"> ## Random Forests Model","4b11f5dc":"# Kaggle Titanic Competition ","476cb23c":"## Decision Tree Model","470197d7":"# Final Results\nIt has an  .80382 of accuracy in the real test of the competition at Kaggle.  \nIf you have found this Kernel useful, feel free to use it."}}