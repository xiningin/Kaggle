{"cell_type":{"da8cf0a8":"code","2eccc7ed":"code","62fd74da":"code","358c0291":"code","dd591356":"code","a5d0bb17":"code","fcbaa4f5":"code","789811f6":"code","a94d7e91":"code","89785271":"code","3af9abec":"code","721743a2":"code","46a353dc":"code","092e17e9":"code","23a1e2ac":"code","2d21bdd2":"code","02837dc2":"code","380da863":"code","619828f2":"code","2311565a":"code","bf01c0fe":"code","b0cc44fa":"code","1e4a6926":"code","1b8c29a0":"code","e6acdbcd":"code","b29b6179":"code","fff8df27":"code","c16f5919":"code","1779b629":"code","4245e410":"code","5266bc3c":"code","8e49458a":"markdown","5f853946":"markdown","9afe87d4":"markdown","b4cbb449":"markdown","22981f8f":"markdown","fb908726":"markdown"},"source":{"da8cf0a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \npd.set_option('display.max_columns',5000)\npd.set_option('display.max_rows',100)\npd.set_option('display.width',10000)\n# Any results you write to the current directory are saved as output.","2eccc7ed":"import pandas as pd\nsample_submission = pd.read_csv(\"..\/input\/ieee-fraud-detection\/sample_submission.csv\")\ntest_identity = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_identity.csv\")\ntest_transaction = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_transaction.csv\")\ntrain_identity = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_identity.csv\")\ntrain_transaction = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_transaction.csv\")","62fd74da":"### columns begining with id in test identity are not in same format as train identity so this needs\n### to be changed \ntmp = {}\nfor col in [rec for rec in test_identity.columns if rec.startswith('id')]:\n    #print(col.replace('-','_'))\n    tmp[col] = col.replace('-','_')\n\ntest_identity.rename(columns=tmp,inplace=True)","358c0291":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","dd591356":"import gc\n\ndef id_split(dataframe):\n    dataframe['device_name'] = dataframe['DeviceInfo'].str.split('\/', expand=True)[0]\n    dataframe['device_version'] = dataframe['DeviceInfo'].str.split('\/', expand=True)[1]\n\n    dataframe['OS_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[0]\n    dataframe['version_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[1]\n\n    dataframe['browser_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[0]\n    dataframe['version_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[1]\n\n    dataframe['screen_width'] = dataframe['id_33'].str.split('x', expand=True)[0]\n    dataframe['screen_height'] = dataframe['id_33'].str.split('x', expand=True)[1]\n\n    #dataframe['id_34'] = dataframe['id_34'].str.split(':', expand=True)[1]\n    #dataframe['id_23'] = dataframe['id_23'].str.split(':', expand=True)[1]\n\n    dataframe.loc[dataframe['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n    dataframe.loc[dataframe['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n    dataframe.loc[dataframe['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n    dataframe.loc[dataframe['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n    dataframe.loc[dataframe['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n    dataframe.loc[dataframe['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n    dataframe.loc[dataframe['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n    dataframe.loc[dataframe['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n\n    dataframe.loc[dataframe.device_name.isin(dataframe.device_name.value_counts()[dataframe.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n    dataframe['had_id'] = 1\n    gc.collect()\n    \n    return dataframe","a5d0bb17":"train_identity = id_split(train_identity)\ntest_identity = id_split(test_identity)","fcbaa4f5":"test_identity","789811f6":"train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint('Data was successfully merged!\\n')\n\ndel train_identity, train_transaction, test_identity, test_transaction\n\nprint(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\nprint(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')\n\ngc.collect()","a94d7e91":"train.set_index(keys='TransactionID_x',inplace=True)\ntest.set_index(keys='TransactionID_x',inplace=True)","89785271":"useful_features = ['TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1',\n                   'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13',\n                   'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M2', 'M3',\n                   'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V17',\n                   'V19', 'V20', 'V29', 'V30', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V40', 'V44', 'V45', 'V46', 'V47', 'V48',\n                   'V49', 'V51', 'V52', 'V53', 'V54', 'V56', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V69', 'V70', 'V71',\n                   'V72', 'V73', 'V74', 'V75', 'V76', 'V78', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V87', 'V90', 'V91', 'V92',\n                   'V93', 'V94', 'V95', 'V96', 'V97', 'V99', 'V100', 'V126', 'V127', 'V128', 'V130', 'V131', 'V138', 'V139', 'V140',\n                   'V143', 'V145', 'V146', 'V147', 'V149', 'V150', 'V151', 'V152', 'V154', 'V156', 'V158', 'V159', 'V160', 'V161',\n                   'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V169', 'V170', 'V171', 'V172', 'V173', 'V175', 'V176', 'V177',\n                   'V178', 'V180', 'V182', 'V184', 'V187', 'V188', 'V189', 'V195', 'V197', 'V200', 'V201', 'V202', 'V203', 'V204',\n                   'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V219', 'V220',\n                   'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V231', 'V233', 'V234', 'V238', 'V239',\n                   'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V249', 'V251', 'V253', 'V256', 'V257', 'V258', 'V259', 'V261',\n                   'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276',\n                   'V277', 'V278', 'V279', 'V280', 'V282', 'V283', 'V285', 'V287', 'V288', 'V289', 'V291', 'V292', 'V294', 'V303',\n                   'V304', 'V306', 'V307', 'V308', 'V310', 'V312', 'V313', 'V314', 'V315', 'V317', 'V322', 'V323', 'V324', 'V326',\n                   'V329', 'V331', 'V332', 'V333', 'V335', 'V336', 'V338', 'id_01', 'id_02', 'id_03', 'id_05', 'id_06', 'id_09',\n                   'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_17', 'id_19', 'id_20', 'id_30', 'id_31', 'id_32', 'id_33',\n                   'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'device_name', 'device_version', 'OS_id_30', 'version_id_30',\n                   'browser_id_31', 'version_id_31', 'screen_width', 'screen_height', 'had_id']","3af9abec":"cols_to_drop = [col for col in train.columns if col not in useful_features]\ncols_to_drop.remove('isFraud')\ncols_to_drop.remove('TransactionDT')","721743a2":"train = train.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop, axis=1)","46a353dc":"train","092e17e9":"columns_a = ['TransactionAmt', 'id_02', 'D15']\ncolumns_b = ['card1', 'card4', 'addr1']\n\nfor col_a in columns_a:\n    for col_b in columns_b:\n        for df in [train, test]:\n            df[f'{col_a}_to_mean_{col_b}'] = df[col_a] \/ df.groupby([col_b])[col_a].transform('mean')\n            df[f'{col_a}_to_std_{col_b}'] = df[col_a] \/ df.groupby([col_b])[col_a].transform('std')","23a1e2ac":"from sklearn.preprocessing import LabelEncoder\n\n# New feature - log of transaction amount.\ntrain['TransactionAmt_Log'] = np.log(train['TransactionAmt'])\ntest['TransactionAmt_Log'] = np.log(test['TransactionAmt'])\n\n# New feature - decimal part of the transaction amount.\ntrain['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)\n\n# New feature - day of week in which a transaction happened.\ntrain['Transaction_day_of_week'] = np.floor((train['TransactionDT'] \/ (3600 * 24) - 1) % 7)\ntest['Transaction_day_of_week'] = np.floor((test['TransactionDT'] \/ (3600 * 24) - 1) % 7)\n\n# New feature - hour of the day in which a transaction happened.\ntrain['Transaction_hour'] = np.floor(train['TransactionDT'] \/ 3600) % 24\ntest['Transaction_hour'] = np.floor(test['TransactionDT'] \/ 3600) % 24\n\n# Some arbitrary features interaction\nfor feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n\n    f1, f2 = feature.split('__')\n    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n\n    le = LabelEncoder()\n    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n    train[feature] = le.transform(list(train[feature].astype(str).values))\n    test[feature] = le.transform(list(test[feature].astype(str).values))\n\n# Encoding - count encoding for both train and test\nfor feature in ['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'id_36']:\n    train[feature + '_count_full'] = train[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n    test[feature + '_count_full'] = test[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n\n# Encoding - count encoding separately for train and test\nfor feature in ['id_01', 'id_31', 'id_33', 'id_36']:\n    train[feature + '_count_dist'] = train[feature].map(train[feature].value_counts(dropna=False))\n    test[feature + '_count_dist'] = test[feature].map(test[feature].value_counts(dropna=False))","2d21bdd2":"%%time\n\nfor col in train.columns:\n    if train[col].dtype == 'object':\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values))","02837dc2":"%%time\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","380da863":"X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT'], axis=1)\ny = train.sort_values('TransactionDT')['isFraud']\n\nX_test = test.drop(['TransactionDT'], axis=1)\n\ndel train, test\ngc.collect()","619828f2":"X_test.index","2311565a":"from sklearn.model_selection import KFold\nimport lightgbm as lgb\n\nparams = {'num_leaves': 491,\n          'min_child_weight': 0.03454472573214212,\n          'feature_fraction': 0.3797454081646243,\n          'bagging_fraction': 0.4181193142567742,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.006883242363721497,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3899927210061127,\n          'reg_lambda': 0.6485237330340494,\n          'random_state': 47,\n         }","bf01c0fe":"X_temp = X.iloc[:10000]\ny_temp = y.iloc[:10000]\n","b0cc44fa":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\n\nNFOLDS = 2\nfolds = KFold(n_splits=NFOLDS)\n\ncolumns = X.columns\nsplits = folds.split(X, y)\ny_preds = np.zeros(X_test.shape[0])\ny_oof = np.zeros(X.shape[0])\nscore = 0\n\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = columns\n  \nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dvalid = lgb.Dataset(X_valid, label=y_valid)\n\n    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=200, early_stopping_rounds=500)\n    \n    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n    \n    y_pred_valid = clf.predict(X_valid)\n    y_oof[valid_index] = y_pred_valid\n    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n    \n    score += roc_auc_score(y_valid, y_pred_valid) \/ NFOLDS\n    y_preds += clf.predict(X_test) \/ NFOLDS\n    \n    del X_train, X_valid, y_train, y_valid\n    gc.collect()\n    \nprint(f\"\\nMean AUC = {score}\")\nprint(f\"Out of folds AUC = {roc_auc_score(y, y_oof)}\")","1e4a6926":"sub = pd.DataFrame()\nsub['TransactionID'] = X_test.index\nsub['isFraud'] = y_preds\nprint(sub)\nsub.to_csv(\"submission_lgb.csv\", index=False)\n\n","1b8c29a0":"del sub,clf\ngc.collect()","e6acdbcd":"# from sklearn.model_selection import KFold\n# from sklearn.metrics import roc_auc_score\n# from sklearn.model_selection import train_test_split\n# import xgboost as xgb\n# from sklearn.model_selection import GridSearchCV\n\n# #X_train,X_test,y_train,y_test = train_test_split(X,y)\n# model = xgb.XGBClassifier()\n\n# param_value = {\"max_depth\": [10,30,50],\n#               \"min_child_weight\" : [1,3,6],\n#               \"n_estimators\": [100,200,300],\n#               \"learning_rate\": [0.05, 0.1,0.16]}\n\n# grid_search = GridSearchCV(model,param_grid=param_value,cv=3,verbose=10,n_jobs=-1,scoring='roc_auc')\n\n# grid_search.fit(X_temp,y_temp)\n\n# print(grid_search.best_estimator_)\n# print(grid_search.best_score_)\n# print(grid_search.best_estimator_)\n\n# # XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n# #               colsample_bynode=1, colsample_bytree=1, gamma=0,\n# #               learning_rate=0.05, max_delta_step=0, max_depth=10,\n# #               min_child_weight=6, missing=None, n_estimators=200, n_jobs=1,\n# #               nthread=None, objective='binary:logistic', random_state=0,\n# #               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n# #               silent=None, subsample=1, verbosity=1)\n# # 0.8412369177483182\n# # XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n# #               colsample_bynode=1, colsample_bytree=1, gamma=0,\n# #               learning_rate=0.05, max_delta_step=0, max_depth=10,\n# #               min_child_weight=6, missing=None, n_estimators=200, n_jobs=1,\n# #               nthread=None, objective='binary:logistic', random_state=0,\n# #               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n# #               silent=None, subsample=1, verbosity=1)\n\n\n","b29b6179":"import xgboost as xgb\nxgb_model = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=10,\n              learning_rate=0.05, max_delta_step=0, max_depth=5,\n              min_child_weight=6, missing=None, n_estimators=1000, n_jobs=1,\n              nthread=None, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n              silent=None, subsample=1, verbosity=1,early_stopping_rounds=10)\nxgb_model.fit(X_temp,y_temp)","fff8df27":"from sklearn.metrics import roc_auc_score\ny_pred = xgb_model.predict_proba(X)[:,1]\nprint(roc_auc_score(y,y_pred))","c16f5919":"sub = pd.DataFrame()\nsub['TransactionID'] = X_test.index\nsub['isFraud'] = y_preds\nsub.to_csv(\"submission_xgboost.csv\", index=False)","1779b629":"del sub\ngc.collect()","4245e410":"from matplotlib import pyplot as plt\n\nxgb.plot_tree(xgb_model,num_trees=0, rankdir='LR')\nplt.rcParams['figure.figsize'] = [50, 10]\nplt.show()","5266bc3c":"# from matplotlib import pyplot as plt\n# import seaborn as sns\n# feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\n# feature_importances.to_csv('feature_importances.csv')\n\n# plt.figure(figsize=(16, 16))\n# sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\n# plt.title('50 TOP feature importance over {} folds average'.format(folds.n_splits));","8e49458a":"Let's reduce the memory usage ","5f853946":"Drop columns which are not usefull","9afe87d4":"Using XGBoost arguments identified by Gridsearch","b4cbb449":"Load the files ","22981f8f":"Apply Xgboost","fb908726":"Let's get done with the shenanigans "}}