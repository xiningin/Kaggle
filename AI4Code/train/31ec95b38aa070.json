{"cell_type":{"1af1a6ee":"code","eba95cac":"code","166bfe1f":"code","13170e3c":"code","4565ac84":"code","9feca09b":"code","ed83bffa":"code","78205197":"code","69af568b":"code","19a93b59":"code","861c5b52":"code","f900fcaa":"code","744d24c0":"code","97ac0d29":"code","8c9e74e9":"code","f4554508":"code","7f862b26":"code","209b7c42":"code","9858f406":"code","8d3c5af1":"code","00d80d16":"code","19f144f9":"code","2fb82542":"code","dd41046e":"code","71022667":"code","69cb6986":"code","015b1448":"code","bd3d3093":"code","76c6726f":"code","1a0a1ab2":"code","a62696f6":"code","dc91098e":"code","5cd06093":"code","47f750f1":"code","45f47ac9":"code","34cd7b8f":"code","048f4009":"code","1efcb498":"code","076e5943":"code","9381d40c":"code","d8bdd2ce":"code","f18e02fa":"code","3b0b9a1c":"code","f5bddb2c":"code","5f4fa5bd":"code","c7e968b9":"code","7fe4b150":"code","566f0c2a":"code","1d9794e9":"code","66ab5ff2":"code","f03b0446":"code","fc28d614":"code","8c691b68":"code","1538752c":"code","bb88f6a7":"code","6f0d07ba":"code","48b9fc40":"code","7773a384":"code","7d0c7a19":"code","3e016655":"code","a58acd56":"code","5ce5ca25":"code","f28d4e2d":"code","f7dacbf9":"code","2d67bec0":"code","07b582f2":"markdown","79a0b979":"markdown","4b86bcf9":"markdown","a5370ab2":"markdown","5aba39c4":"markdown","8fb12a8c":"markdown","746b8c69":"markdown","33632b94":"markdown","7c342dfb":"markdown","3235b9f0":"markdown","3cd84e7d":"markdown","238457de":"markdown","3d484b70":"markdown","812e596b":"markdown","67314eea":"markdown","43a66063":"markdown","d616951e":"markdown","3ea9404c":"markdown","1d05e3ff":"markdown"},"source":{"1af1a6ee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Display all columns in the output\npd.options.display.max_columns = None\npd.options.display.max_rows = None\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eba95cac":"# Import libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom scipy.stats import skew\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor","166bfe1f":"submission_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission_df.head()","13170e3c":"train_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain_df.head()","4565ac84":"# Load test dataset\ntest_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_df.head()","9feca09b":"# Check shapes of the train and test dataframes\nprint('Train dataframe shape:',train_df.shape)\nprint('Test dataframe shape:',test_df.shape)","ed83bffa":"# Train features and target variable\ntrain_target = train_df['SalePrice']\ntrain_df = train_df.drop('SalePrice', axis=1)","78205197":"# Check shapes of the train and test dataframes\nprint('Train target shape:',train_target.shape)\nprint('Train features dataframe shape:',train_df.shape)\nprint('Test features dataframe shape:',test_df.shape)","69af568b":"# Basic description\ntrain_df.info()","19a93b59":"# Describe the numerical data\ntrain_df.describe()","861c5b52":"# Check for duplicated values\nprint('Duplicates in Train:', train_df.duplicated().sum())\nprint('Duplicates in Test:', test_df.duplicated().sum())","f900fcaa":"# Check shapes of the train and test dataframes\nprint('Train dataframe shape:',train_df.shape)\nprint('Test dataframe shape:',test_df.shape)","744d24c0":"# Check for missing value percentage in train dataset\nmissing_train_df = 100 * train_df.isnull().mean()\nmissing_train_df[missing_train_df.values > 0].sort_values(ascending=False)","97ac0d29":"# Check for missing value percentage in test dataset\nmissing_test_df = 100 * test_df.isnull().mean()\nmissing_test_df[missing_test_df.values > 0].sort_values(ascending=False)","8c9e74e9":"# Remove features with >30% missing values from train and test dataframes\nfeatures_to_drop = ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu']\n\ntrain_df.drop(features_to_drop, axis=1, inplace=True)\ntest_df.drop(features_to_drop, axis=1, inplace=True)","f4554508":"# Check shapes of the train and test dataframes\nprint('Train dataframe shape:',train_df.shape)\nprint('Test dataframe shape:',test_df.shape)","7f862b26":"# Numeric features\nnumeric_feats = train_df.dtypes[train_df.dtypes != \"object\"].index\n# Categorical features\ncat_feats = train_df.dtypes[train_df.dtypes == \"object\"].index\n\nprint('Numerical features length:', len(numeric_feats))\nprint('Categorical features length:', len(cat_feats))","209b7c42":"# Impute categorical features missing values with \"Unknown\" in train and test dataframes\ntrain_df_cat = train_df.loc[:, cat_feats].fillna('Unknown')\ntest_df_cat = test_df.loc[:, cat_feats].fillna('Unknown')","9858f406":"# Check for missing value percentage in train dataset\nmissing_train_df = 100 * train_df.loc[:, numeric_feats].isnull().mean()\nmissing_train_df[missing_train_df.values > 0].sort_values(ascending=False)","8d3c5af1":"# Check for missing value percentage in test dataset\nmissing_test_df = 100 * test_df.loc[:, numeric_feats].isnull().mean()\nmissing_test_df[missing_test_df.values > 0].sort_values(ascending=False)","00d80d16":"# Use KNNImputer to impute missing values in train dataset\nfor col in missing_train_df[missing_train_df.values > 0].index:\n    train_df[f'{col}_missing'] = train_df[col].isnull()\n\nimputer = KNNImputer(n_neighbors=5)\ntrain_df_imputed = pd.DataFrame(imputer.fit_transform(train_df.loc[:, numeric_feats]))\ntrain_df_imputed.columns = train_df.loc[:, numeric_feats].columns\ntrain_df_imputed.head()","19f144f9":"# Use KNNImputer to impute missing values in test dataset\nfor col in missing_test_df[missing_test_df.values > 0].index:\n    test_df[f'{col}_missing'] = test_df[col].isnull()\n\nimputer = KNNImputer(n_neighbors=5)\ntest_df_imputed = pd.DataFrame(imputer.fit_transform(test_df.loc[:, numeric_feats]))\ntest_df_imputed.columns = test_df.loc[:, numeric_feats].columns\ntest_df_imputed.head()","2fb82542":"# Concat cat and numeric imputed datasets\ntrain_df = pd.concat([train_df_imputed, train_df_cat], axis=1)\ntest_df = pd.concat([test_df_imputed, test_df_cat], axis=1)","dd41046e":"# Check for missing value percentage in train dataset\nmissing_train_df = 100 * train_df.isnull().mean()\nmissing_train_df[missing_train_df.values > 0].sort_values(ascending=False)","71022667":"# Check for missing value percentage in test dataset\nmissing_test_df = 100 * test_df.isnull().mean()\nmissing_test_df[missing_test_df.values > 0].sort_values(ascending=False)","69cb6986":"train_df_cat = train_df.select_dtypes(include='object')\ntest_df_cat = test_df.loc[:, train_df_cat.columns]\ntrain_df_cat.head()","015b1448":"test_df_cat.head()","bd3d3093":"# Use Pandas.get_dummies to encode categorical features to numerical features\ntrain_cat_encoded = pd.get_dummies(train_df_cat, drop_first=True)\ntrain_cat_encoded.head()","76c6726f":"# Use Pandas.get_dummies to encode categorical features to numerical features\ntest_cat_encoded = pd.get_dummies(test_df_cat, drop_first=True)\ntest_cat_encoded.head()","1a0a1ab2":"# Concat original train_df and train_cat_encoded and then drop cat columns. Same for test_df\ntrain_df = pd.concat([train_df, train_cat_encoded], axis=1)\ntrain_df.drop(train_df_cat.columns, axis=1, inplace=True)","a62696f6":"train_df.head()","dc91098e":"# Concat original test_df and test_cat_encoded and then drop cat columns.\ntest_df = pd.concat([test_df, test_cat_encoded], axis=1)\ntest_df.drop(test_df_cat.columns, axis=1, inplace=True)","5cd06093":"test_df.head()","47f750f1":"# Check shapes of the train and test dataframes\nprint('Train dataframe shape:',train_df.shape)\nprint('Test dataframe shape:',test_df.shape)","45f47ac9":"train_df.nunique().sort_values()","34cd7b8f":"test_df.nunique().sort_values()","048f4009":"plt.figure(figsize=(10, 6))\nsns.histplot(train_target, kde=True)\nplt.title('Histogram for Sale price')\nplt.show()","1efcb498":"plt.figure(figsize=(10, 6))\nsns.histplot(np.log1p(train_target), kde=True)\nplt.title('Histogram for log(Sale price+1)')\nplt.show()","076e5943":"y_train = np.log1p(train_target)","9381d40c":"skew_threshold = 0.75\n\nskewed_feats = train_df[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > skew_threshold]\nskewed_feats = skewed_feats.index\nskewed_feats","d8bdd2ce":"# Take log transformation for train and test datasets\n\ntrain_df[skewed_feats] = np.log1p(train_df[skewed_feats])\ntest_df[skewed_feats] = np.log1p(test_df[skewed_feats])","f18e02fa":"common_feats = set(train_df.columns).intersection(set(test_df.columns))\nprint(len(common_feats))","3b0b9a1c":"X_train = train_df.loc[:, common_feats]\nX_test = test_df.loc[:, common_feats]\ny_train = y_train","f5bddb2c":"X_trn, X_val, y_trn, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n\nprint(X_trn.shape, y_trn.shape)\nprint(X_val.shape, y_val.shape)","5f4fa5bd":"def prepare_submission(model, file_name, test_df = None):\n    if test_df is None:\n        test_df = X_test\n        \n    # Prepare for submission using model\n    y_test_preds = np.expm1(model.predict(test_df))\n\n    # print(test_df['Id'][:5], y_test_preds[:5])\n\n    submission_df['SalePrice'] = y_test_preds\n\n    submission_df.to_csv(file_name, index=False)","c7e968b9":"# KNeighborsRegressor\nknn = KNeighborsRegressor(n_neighbors=5)\n\nknn.fit(X_trn, y_trn)\n\n# y_val_preds = knn.predict(X_val)\n\nprint(knn.score(X_val, y_val))","7fe4b150":"# Prepare for submission\nprepare_submission(knn, 'submission_knn_1.csv')","566f0c2a":"# Linear regression\nlr = LinearRegression()\n\nlr.fit(X_trn, y_trn)\n\nprint(lr.score(X_val, y_val))","1d9794e9":"#prepare_submission(lr, 'submission_lr_1.csv')\n#prepare_submission(lr, 'submission_lr_2.csv')\nprepare_submission(lr, 'submission_lr_3.csv')","66ab5ff2":"# RandomForestRegressor\nrf = RandomForestRegressor(random_state=42)\n\nrf.fit(X_trn, y_trn)\n\n# y_val_preds = knn.predict(X_val)\n\nprint(rf.score(X_val, y_val))","f03b0446":"prepare_submission(rf, 'submission_rf_1.csv')","fc28d614":"feats_imp = pd.DataFrame({'Column':X_trn.columns, 'Score': rf.feature_importances_})\nfeats_imp.sort_values(by='Score', ascending=False)","8c691b68":"scaler = MinMaxScaler(feature_range=(0,10))\nX_trn_scaled = scaler.fit_transform(X_trn)\nX_val_scaled = scaler.transform(X_val)","1538752c":"X_trn_scaled_df = pd.DataFrame(X_trn_scaled, columns=X_trn.columns)\nX_val_scaled_df = pd.DataFrame(X_val_scaled, columns=X_val.columns)","bb88f6a7":"X_trn_scaled_df.head()","6f0d07ba":"X_val_scaled_df.head()","48b9fc40":"sel = VarianceThreshold(threshold=1.0)\nX_trn_vt = sel.fit_transform(X_trn)","7773a384":"# Features selected\ncols_selected = X_trn.columns[sel.get_support()]\ncols_selected","7d0c7a19":"# Build the DataFrame\nX_trn_vt_df = pd.DataFrame(X_trn_vt, columns=cols_selected)\nX_trn_vt_df.head()","3e016655":"X_val_vt_df = pd.DataFrame(sel.transform(X_val), columns=cols_selected)\nX_val_vt_df.head()","a58acd56":"X_test_vt_df = pd.DataFrame(sel.transform(X_test), columns=cols_selected)\nX_test_vt_df.head()","5ce5ca25":"# Let's build the KNN regressor again\nknn_1 = KNeighborsRegressor(n_neighbors=5)\nknn_1.fit(X_trn_vt_df, y_trn)\n\nprint('KNN score:', knn_1.score(X_val_vt_df, y_val))","f28d4e2d":"# Linear regression\nlr_1 = LinearRegression()\nlr_1.fit(X_trn_vt_df, y_trn)\n\nprint('Linear Regression score:', lr_1.score(X_val_vt_df, y_val))","f7dacbf9":"# Random Forest Regressor\nrf_1 = RandomForestRegressor(random_state=42)\nrf_1.fit(X_trn_vt_df, y_trn)\n\nprint('Random Forest score:', rf_1.score(X_val_vt_df, y_val))","2d67bec0":"prepare_submission(rf_1, 'Submission_RF_2.csv', X_test_vt_df)","07b582f2":"## Check skewness of numeric variables","79a0b979":"Now, our target variable is more normally distributed, which will help in our data analysis.","4b86bcf9":"So, we have to submit \"SalePrice\" for each \"Id\" of the given house.","a5370ab2":"## Load datasets","5aba39c4":"### Train dataset information\n\n- We have 1460 observations with 80 features and 1 target variable.\n- There are features with missing values\n- There are 37 numerical features and 43 categorical features\n- Our target variable is in numeric as expected.","8fb12a8c":"## Handle Categorical features","746b8c69":"We have at least 2 unique values in each feature, which is okay. If it'd have only 1 unique value in a feature, then we could easily drop the feature because 1 unique value doesn't add any importance in the analysis.","33632b94":"## Check target variable","7c342dfb":"## Data preparation","3235b9f0":"### Check for features with missing values","3cd84e7d":"### Check for duplicates in train and test dataframes","238457de":"## Basic analysis on the datasets","3d484b70":"The above model gave us good score around 0.9, but it is not good model. If you look at the model features, there are 227 features which makes the model too complex and it might overfit the data.\n\nSo, we need to trim the features to lesser number, so that our model is simple and yet predictive.\n\nWhat are the measures we can take to remove features?\n- VarianceThreshold\n- SelectKBest\n- Feature importances from tree ensemble models\n- _More?? (Comment to suggest more on this)_","812e596b":"### Train dataset description for numerical features","67314eea":"### Normalization","43a66063":"Test dataset has more features with missing values than train dataset.","d616951e":"As we can see that our target variable is skewed towards right, we can use log transformation to make it more normal.","3ea9404c":"## Check for nunique values for each column","1d05e3ff":"## VarianceThreshold\n\nLet's take a look at what VarianceThreshold can offer in this case.\n\nThis method filters the features based on the variance of each feature. If the variance is low or close to zero, then the feature is almost constant and it will not improve the performance of the model."}}