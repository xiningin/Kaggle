{"cell_type":{"4b58245d":"code","21f3674f":"code","78c1c49d":"code","55de7557":"code","7650e25a":"code","69c96355":"code","fb68034e":"code","4702df4d":"code","7fc7cded":"code","cb122f6b":"code","0c59cbc3":"code","b671537c":"code","6fb68414":"code","da19098f":"code","3200cc92":"code","86d8edfb":"code","e4d85824":"code","9b9b7ec4":"code","9c3dd6c1":"code","2ee94f30":"code","ee486366":"code","ce0f38e6":"code","cbbffa4a":"code","d5a88b38":"code","4f993cbb":"code","0e084352":"code","2037d831":"code","d082389c":"code","cf367ec5":"code","b259189f":"code","b793980b":"code","802ebd12":"code","d5a8ac05":"code","e5deb6ea":"code","f0f6bfec":"code","27fdd2c4":"markdown","b63d7587":"markdown","9b3180f0":"markdown","d71f3e2e":"markdown","a89d7f4d":"markdown","e0817714":"markdown","a4fb259f":"markdown","072e95e3":"markdown","9be723d2":"markdown","65e2b647":"markdown","0a52f23a":"markdown","90bc111d":"markdown","dfce48bb":"markdown","839e937a":"markdown","d29d82d6":"markdown","cbd5b6ec":"markdown","82fd9dfe":"markdown","cffd974a":"markdown","2c6fb99e":"markdown","c491afa8":"markdown","aae492ee":"markdown","f6ee57d7":"markdown","662fc024":"markdown","6cfee45b":"markdown"},"source":{"4b58245d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom keras.layers import Dense, GRU, Embedding\nfrom keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","21f3674f":"dataset = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")","78c1c49d":"dataset.head()","55de7557":"data = dataset[\"text\"].values.tolist()\ntarget = dataset[\"target\"].values.tolist()","7650e25a":"cutoff = int(len(data)*0.8)\nx_train , x_test = data[0:cutoff] , data[cutoff:]\ny_train , y_test = target[0:cutoff] , target[cutoff:]","69c96355":"x_train =  [x.lower() for x in x_train]\nx_test = [y.lower() for y in x_test]","fb68034e":"x_train[100]","4702df4d":"num_words = 10000\n\ntokenizer = Tokenizer(num_words=num_words)","7fc7cded":"tokenizer.fit_on_texts(x_train)\ntokenizer.fit_on_texts(x_test)","cb122f6b":"tokenizer.word_index # you can clear the stopwords or useless words.","0c59cbc3":"x_train_tokens = tokenizer.texts_to_sequences(x_train)","b671537c":"x_train[500]","6fb68414":"print(x_train_tokens[500])","da19098f":"x_test_tokens = tokenizer.texts_to_sequences(x_test)","3200cc92":"num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\nnum_tokens = np.array(num_tokens)","86d8edfb":"num_tokens.sum() # total tokens","e4d85824":"np.mean(num_tokens) # tokens length average","9b9b7ec4":"np.max(num_tokens) # the longest token length","9c3dd6c1":"max_tokens = np.mean(num_tokens) + 2*np.std(num_tokens)\nmax_tokens = int(max_tokens)\nprint(max_tokens)","2ee94f30":"np.sum(num_tokens < max_tokens ) \/ len(num_tokens)","ee486366":"x_train_pad = pad_sequences(x_train_tokens,maxlen = max_tokens)","ce0f38e6":"x_test_pad = pad_sequences(x_test_tokens,maxlen=max_tokens)","cbbffa4a":"x_test_pad.shape","d5a88b38":"x_train_pad.shape","4f993cbb":"np.array(x_train_tokens[400])","0e084352":"np.array(x_train_pad[400])","2037d831":"model = Sequential()","d082389c":"embedding_size=50","cf367ec5":"model.add(Embedding(input_dim=num_words,\n                   output_dim=embedding_size,\n                   input_length=max_tokens,\n                   name=\"embedding_layer\"))","b259189f":"model.add(GRU(16,return_sequences=True)) # \"return_sequences=True\" means give output to next layer as sequence\nmodel.add(GRU(8,return_sequences=True))\nmodel.add(GRU(4,return_sequences=False)) # Dense layer doesn't understand sequences. Because of this we'll turn return_sequences False.\nmodel.add(Dense(1,activation=\"sigmoid\"))","b793980b":"model.summary()","802ebd12":"opt = Adam(1e-3)","d5a8ac05":"model.compile(optimizer=opt,loss=\"binary_crossentropy\",metrics=[\"accuracy\"])","e5deb6ea":"model.fit(np.array(x_train_pad),np.array(y_train),epochs=15,batch_size=256,validation_split = 0.2)","f0f6bfec":"model_result = model.evaluate(np.array(x_test_pad),np.array(y_test))","27fdd2c4":"* We'll use the Gated Recurrent Unit (GRU)to train our model. GRU is a RNN model,with that we can train better the sequence datasets like texts, audios etc.\n* I will not use LSTM, because GRU is faster than LSTM and the problem is not complicated.","b63d7587":"* 26 is ideal token length to give as input to our model.","9b3180f0":"<a id=7><\/a>\n### Testing Model\n\n* We've seen the accuracy of our model seems very good, but we don't know yet the accuracy on unseen data.Let's test it!","d71f3e2e":"* Implement tokenization","a89d7f4d":"<a id=4><\/a>\n### Tokenization\n* With Tokenization process we can split all comments to words and every word would convert to vector. Because the computers are not able to understand texts, they tend to understand numbers,vectors etc.\n* To Tokenization we'll use keras.\n* First we have to determine the number of words,which will entering our model. Then we'll use the Tokenizer method in keras, to do tokenization process.\n* We don't need all words of dataset to train our model. Some words are useless, they can be typo like \"computr\" or \"helo\". Because of this we select most common 10000 words to get better results.","e0817714":"# Deep Learning for NLP (Guide for Beginners)\n\n**Hello Guys!** \n\n**Here I try to explain the important points of Deep Learning for NLP with this competition as beginner as i am.**\n\n**The Competition is about Detection of Fake Tweets about Disasters.**\n\n**Enjoy!**","a4fb259f":"* We've to determine our parameters to update our weigths.\n* I'll prefer to use Adam optimizer with 0.003 learning rate and for loss function binary classification.","072e95e3":"<a id=6><\/a>\n### Building Model\n\n* We're finally able to create our model!\n* First we'have to create Embedding Layer convert the tokens to vectors.\n* Embedding Layer build a matrix for this vectors and we've to give a length for this vector length.","9be723d2":"* Now we have to convert this texts to vectors for computer to understand.","65e2b647":"* Then we have to split our data to 4 parts. x_train and x_test are for training and testing the model. y_train and y_test are for evaluation the model.That means model performance on unseen data. \n* I split both of them to %80 - %20.","0a52f23a":"* I prefer to build 3 GRU Layer to do best practices you can try another number of layer or units. This is just experiment!\n* And finally to predict binary(fake or not) we'll create Dense Layer with sigmoid activation function\n* Sigmoid activation function scales our predict between 0-1 and this is preferable for binary classification.","90bc111d":"* We have successfully splitted. You can se below","dfce48bb":"* We convert the words to lowercase, because computer can't understand same \"Hello\" and \"hello\"","839e937a":"#### Accuracy is not bad for Beginners!","d29d82d6":"* We determine the spesific shape ","cbd5b6ec":"<a id=3><\/a>\n### Splitting Dataset\n* We have just imported dataset and as we can see id,keyword and location columns is useless for this practice.Because we try to learn text the information and try to predict the target. So we keep just text and target columns and convert them to list.","82fd9dfe":"* We'll add 0's to %97 of tokens and throw away some words the rest of our tokens.","cffd974a":"<a id='1'><\/a>\n### Import Libraries\n\n* First of all we have to import necessary libraries which are keras modules comes first.","2c6fb99e":"<a id=2> <\/a>\n### Import Dataset\n* I imported just train.csv to better understanding this concepts , so we keep moving with train.csv.","c491afa8":"## Contents\n\n1. [Import Libraries](#1)\n1. [Import Dataset](#2)\n1. [Splitting Dataset](#3)\n1. [Tokenization](#4)\n1. [Preprocessing](#5)\n1. [Building Model](#6)\n1. [Testing Model](#7)\n\n\n","aae492ee":"<a id=5><\/a>\n### Preprocessing","f6ee57d7":"* Everything has already been prepared. We can train our model!","662fc024":"#### Sample Tweet:\n![](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/tweet_screenshot.png)\n                       ","6cfee45b":"* We can't give as input different shape of tokens to our GRU or any RNN layer. Because of this we need to convert all tokens to shape same.\n* We determine a spesific token's shape for our model. If the tokens smaller than our shape we'll give 0's the beginnig of the token to equalize the shapes. If tokens longer than our spesific shape, we'll cut the shape as necessery."}}