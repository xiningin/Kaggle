{"cell_type":{"677bd718":"code","a1cd3f5a":"code","fd8ffe1c":"code","1b48471a":"code","8a7f4df1":"code","304b168c":"code","48f2221e":"code","81f04ffb":"code","dbf5f0ab":"code","bca7d77d":"code","00730ab1":"code","72b5870e":"code","9ff89473":"code","8ee5bd0e":"code","f69d05e0":"code","e3317cb1":"code","806c1a34":"code","a78ffb72":"code","9080eb18":"code","39cee34b":"code","281e5ffd":"code","88de99a4":"code","3cd8666b":"code","e2581da0":"code","f0c4a38d":"code","63ebaa28":"code","a3045d3b":"code","d0c9addf":"code","65b1add5":"code","52b0b225":"code","8df8f5c1":"code","bec55044":"code","4d48b08b":"code","02387250":"code","14c762e6":"code","267be83b":"code","389b7447":"code","2d71e7dc":"code","eca8dc58":"code","b693b73b":"code","b8dd4423":"code","eb7d3073":"code","9b7c9e48":"code","77186078":"code","c3ed8a39":"code","ebf849b4":"code","d9ae7ad5":"code","4828d78b":"code","df7cc78c":"markdown","ecb20404":"markdown","b85e16ed":"markdown","24ef41a5":"markdown","7c7b29f7":"markdown","1127ad43":"markdown","9c90814c":"markdown","dd9c2cf6":"markdown","e7846de9":"markdown","28324628":"markdown","29e15f3e":"markdown","6ed8507a":"markdown","e5b9a5f1":"markdown","d11694f6":"markdown","bc4bf762":"markdown","74fbdf6e":"markdown","e8c1a961":"markdown","2501dda9":"markdown","ec841bab":"markdown","6a1e7aa8":"markdown","9de425b6":"markdown","3d290dbf":"markdown","1dace8ce":"markdown","90485556":"markdown","51290a78":"markdown","dc915282":"markdown","8a917993":"markdown","1acb35a5":"markdown","aceb8dfe":"markdown","f3e16f36":"markdown","440cbaa7":"markdown","8542bf03":"markdown","eb49f26a":"markdown","f36de619":"markdown","3a6e1e8a":"markdown","34b3212b":"markdown","2aa6fa11":"markdown","2f50b516":"markdown","0be39eba":"markdown","799cd8c5":"markdown","13bdc6c2":"markdown","09de8dab":"markdown","d015bccd":"markdown","4f213a7f":"markdown","37311e13":"markdown","3c172782":"markdown","6ba3be62":"markdown","46438e72":"markdown","5741f9bf":"markdown","fd0d8502":"markdown","c0190cb5":"markdown","8f446c91":"markdown","a92cfcd3":"markdown","a0e73be1":"markdown","e1afdd15":"markdown","0ca95421":"markdown","64d197fe":"markdown","59a535a8":"markdown","9fd0c8fa":"markdown","2af8e889":"markdown","8b84353a":"markdown","c1a66596":"markdown","bde497d2":"markdown","8afff472":"markdown","85e41f29":"markdown","d6a0bdd2":"markdown","f56e9375":"markdown","8495e59a":"markdown","49f0343b":"markdown"},"source":{"677bd718":"from absl import flags\nfrom absl.flags import FLAGS\nimport numpy as np\nimport tensorflow as tf\n#import visualkeras\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import (\n    Add,\n    Concatenate,\n    Conv2D,\n    Input,\n    Lambda,\n    LeakyReLU,\n    MaxPool2D,\n    UpSampling2D,\n    ZeroPadding2D,\n    BatchNormalization,\n)\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.losses import (\n    binary_crossentropy,\n    sparse_categorical_crossentropy\n)","a1cd3f5a":"import time\nfrom absl import app, flags, logging\nimport cv2\n#from yolov3_tf2.models import YoloV3\n#from yolov3_tf2.dataset import transform_images, load_tfrecord_dataset\n#from yolov3_tf2.utils import draw_outputs","fd8ffe1c":"#from absl import app, flags, logging\n#from absl.flags import FLAGS\n#import numpy as np\n#from yolov3_tf2.models import YoloV3\n#from yolov3_tf2.utils import load_darknet_weights","1b48471a":"from tensorflow.keras.callbacks import (\n    ReduceLROnPlateau,\n    EarlyStopping,\n    ModelCheckpoint,\n    TensorBoard\n)\n#from yolov3_tf2.models import (\n    #YoloV3, YoloV3Tiny, YoloLoss,\n   # yolo_anchors, yolo_anchor_masks,\n   # yolo_tiny_anchors, yolo_tiny_anchor_masks)\n    \n#from yolov3_tf2.utils import freeze_all\n#import yolov3_tf2.dataset as dataset","8a7f4df1":"max_boxes=100\niou_threshold=0.5\nscore_threshold=0.5\npath_data=''\npath_val_data=''\nbatch_size_flag=8\nweights_num_classes=None\nlearning_rate_flag=1e-3\nepochs_flag=2\nvideo_output_format = 'XVID'\npath_files= r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\data\\coco.names'\npath_weights= r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\checkpoints\\yolov3.tf'\nresize_img= 416\npath_img= r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\data\\girl.png'\npath_video= r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\data\\video.mp4'\ntiny_flag=False\ntfrecord_flag = None\nvideo_path_output=r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\outputs\\video-output.mp4'\npath_output= r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\outputs\\output.jpg'\nnum_classes=80\n\n\ntransfer_flag =  None\n                #['none', 'darknet', 'no_output', 'frozen', 'fine_tune'],\n                  #'none: Training from scratch, '\n                  #'darknet: Transfer darknet, '\n                  #'no_output: Transfer all but output, '\n                  #'frozen: Transfer and freeze all, '\n                  #'fine_tune: Transfer all and freeze darknet only'\n                \n                \n                \nmode_flag = 'fit'\n#['fit', 'eager_fit', 'eager_tf'],\n                  #'fit: model.fit, '\n                  #'eager_fit: model.fit(run_eagerly=True), '\n                  #'eager_tf: custom GradientTape')\n#flags.DEFINE_enum('transfer', 'none',","304b168c":"yolo_anchors = np.array([(10, 13), (16, 30), (33, 23), (30, 61), (62, 45),\n                         (59, 119), (116, 90), (156, 198), (373, 326)],\n                        np.float32) \/ 416\nyolo_anchor_masks = np.array([[6, 7, 8], [3, 4, 5], [0, 1, 2]])\n\nyolo_tiny_anchors = np.array([(10, 14), (23, 27), (37, 58),\n                              (81, 82), (135, 169),  (344, 319)],\n                             np.float32) \/ 416\nyolo_tiny_anchor_masks = np.array([[3, 4, 5], [0, 1, 2]])","48f2221e":"max_boxes=100\niou_threshold=0.5\nscore_threshold=0.5\npath_data=''\npath_val_data=''\nbatch_size_flag=8\nweights_num_classes=None\nlearning_rate_flag=1e-3\nepochs_flag=2\npath_files= r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\data\\coco.names'\npath_weights= r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\checkpoints\\yolov3.tf'\nresize_img= 416\npath_img= r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\data\\girl.png'\ntiny_flag=False\ntfrecord_flag = None\npath_output= r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\outputs\\output.jpg'\nnum_classes=80\n\n\ntransfer_flag =  None\n                #['none', 'darknet', 'no_output', 'frozen', 'fine_tune'],\n                  #'none: Training from scratch, '\n                  #'darknet: Transfer darknet, '\n                  #'no_output: Transfer all but output, '\n                  #'frozen: Transfer and freeze all, '\n                  #'fine_tune: Transfer all and freeze darknet only'\n                \n                \n                \nmode_flag = 'fit'\n#['fit', 'eager_fit', 'eager_tf'],\n                  #'fit: model.fit, '\n                  #'eager_fit: model.fit(run_eagerly=True), '\n                  #'eager_tf: custom GradientTape')\n#flags.DEFINE_enum('transfer', 'none',","81f04ffb":"def Darknet(name=None):\n    x = inputs = Input([None, None, 3])\n    x = DarknetConv(x, 32, 3)\n    x = DarknetBlock(x, 64, 1)\n    x = DarknetBlock(x, 128, 2)  # skip connection\n    x = x_36 = DarknetBlock(x, 256, 8)  # skip connection\n    x = x_61 = DarknetBlock(x, 512, 8)\n    x = DarknetBlock(x, 1024, 4)\n    return tf.keras.Model(inputs, (x_36, x_61, x), name=name)","dbf5f0ab":"def DarknetBlock(x, filters, blocks):\n    x = DarknetConv(x, filters, 3, strides=2)\n    for _ in range(blocks):\n        x = DarknetResidual(x, filters)\n    return x","bca7d77d":"def DarknetConv(x, filters, size, strides=1, batch_norm=True):\n    if strides == 1:\n        padding = 'same'\n    else:\n        x = ZeroPadding2D(((1, 0), (1, 0)))(x)  # top left half-padding\n        padding = 'valid'\n    x = Conv2D(filters=filters, kernel_size=size,\n               strides=strides, padding=padding,\n               use_bias=not batch_norm, kernel_regularizer=l2(0.0005))(x)\n    if batch_norm:\n        x = BatchNormalization()(x)\n        x = LeakyReLU(alpha=0.1)(x)\n    return x","00730ab1":"def DarknetResidual(x, filters):\n    prev = x\n    x = DarknetConv(x, filters \/\/ 2, 1)\n    x = DarknetConv(x, filters, 3)\n    x = Add()([prev, x])\n    return x","72b5870e":"def YoloConv(filters, name=None): \n    def yolo_conv(x_in):\n        if isinstance(x_in, tuple):\n            inputs = Input(x_in[0].shape[1:]), Input(x_in[1].shape[1:])\n            x, x_skip = inputs\n\n            # concat with skip connection\n            x = DarknetConv(x, filters, 1)\n            x = UpSampling2D(2)(x)\n            x = Concatenate()([x, x_skip])\n        else:\n            x = inputs = Input(x_in.shape[1:])\n\n        x = DarknetConv(x, filters, 1)\n        x = DarknetConv(x, filters * 2, 3)\n        x = DarknetConv(x, filters, 1)\n        x = DarknetConv(x, filters * 2, 3)\n        x = DarknetConv(x, filters, 1)\n        return Model(inputs, x, name=name)(x_in)\n    return yolo_conv","9ff89473":"def YoloOutput(filters, anchors, classes, name=None):\n    def yolo_output(x_in):\n        x = inputs = Input(x_in.shape[1:])\n        x = DarknetConv(x, filters * 2, 3)\n        x = DarknetConv(x, anchors * (classes + 5), 1, batch_norm=False)\n        x = Lambda(lambda x: tf.reshape(x, (-1, tf.shape(x)[1], tf.shape(x)[2],\n                                            anchors, classes + 5)))(x)\n        return tf.keras.Model(inputs, x, name=name)(x_in)\n    return yolo_output","8ee5bd0e":"def _meshgrid(n_a, n_b):\n\n    return [\n        tf.reshape(tf.tile(tf.range(n_a), [n_b]), (n_b, n_a)),\n        tf.reshape(tf.repeat(tf.range(n_b), n_a), (n_b, n_a))\n    ]","f69d05e0":"def yolo_boxes(pred, anchors, classes):\n    # pred: (batch_size, grid, grid, anchors, (x, y, w, h, obj, ...classes))\n    grid_size = tf.shape(pred)[1:3]\n    box_xy, box_wh, objectness, class_probs = tf.split(\n        pred, (2, 2, 1, classes), axis=-1)\n\n    box_xy = tf.sigmoid(box_xy)\n    objectness = tf.sigmoid(objectness)\n    class_probs = tf.sigmoid(class_probs)\n    pred_box = tf.concat((box_xy, box_wh), axis=-1)  # original xywh for loss\n\n    # !!! grid[x][y] == (y, x)\n    grid = _meshgrid(grid_size[1],grid_size[0])\n    grid = tf.expand_dims(tf.stack(grid, axis=-1), axis=2)  # [gx, gy, 1, 2]\n\n    box_xy = (box_xy + tf.cast(grid, tf.float32)) \/ \\\n        tf.cast(grid_size, tf.float32)\n    box_wh = tf.exp(box_wh) * anchors\n\n    box_x1y1 = box_xy - box_wh \/ 2\n    box_x2y2 = box_xy + box_wh \/ 2\n    bbox = tf.concat([box_x1y1, box_x2y2], axis=-1)\n\n    return bbox, objectness, class_probs, pred_box","e3317cb1":"def yolo_nms(outputs, anchors, masks, classes):\n    max_boxes = 100\n    iou_threshold = 0.5\n    score_threshold = 0.5\n    # boxes, conf, type\n    b, c, t = [], [], []\n\n    for o in outputs:\n        b.append(tf.reshape(o[0], (tf.shape(o[0])[0], -1, tf.shape(o[0])[-1])))\n        c.append(tf.reshape(o[1], (tf.shape(o[1])[0], -1, tf.shape(o[1])[-1])))\n        t.append(tf.reshape(o[2], (tf.shape(o[2])[0], -1, tf.shape(o[2])[-1])))\n\n    bbox = tf.concat(b, axis=1)\n    confidence = tf.concat(c, axis=1)\n    class_probs = tf.concat(t, axis=1)\n\n    scores = confidence * class_probs\n\n    dscores = tf.squeeze(scores, axis=0)\n    scores = tf.reduce_max(dscores,[1])\n    bbox = tf.reshape(bbox,(-1,4))\n    classes = tf.argmax(dscores,1)\n    selected_indices, selected_scores = tf.image.non_max_suppression_with_scores(\n        boxes=bbox,\n        scores=scores,\n        max_output_size=max_boxes,\n        iou_threshold=iou_threshold,\n        score_threshold=score_threshold,\n        soft_nms_sigma=0.5\n    )\n    \n    num_valid_nms_boxes = tf.shape(selected_indices)[0]\n\n    selected_indices = tf.concat([selected_indices,tf.zeros(max_boxes-num_valid_nms_boxes, tf.int32)], 0)\n    selected_scores = tf.concat([selected_scores,tf.zeros(max_boxes-num_valid_nms_boxes,tf.float32)], -1)\n\n    boxes=tf.gather(bbox, selected_indices)\n    boxes = tf.expand_dims(boxes, axis=0)\n    scores=selected_scores\n    scores = tf.expand_dims(scores, axis=0)\n    classes = tf.gather(classes,selected_indices)\n    classes = tf.expand_dims(classes, axis=0)\n    valid_detections=num_valid_nms_boxes\n    valid_detections = tf.expand_dims(valid_detections, axis=0)\n\n    return boxes, scores, classes, valid_detections","806c1a34":"def YoloV3(size=None, channels=3, anchors=yolo_anchors,\n           masks=yolo_anchor_masks, classes=80, training=False):\n    x = inputs = Input([size, size, channels], name='input')\n\n    x_36, x_61, x = Darknet(name='yolo_darknet')(x)\n\n    x = YoloConv(512, name='yolo_conv_0')(x)\n    output_0 = YoloOutput(512, len(masks[0]), classes, name='yolo_output_0')(x)\n\n    x = YoloConv(256, name='yolo_conv_1')((x, x_61))\n    output_1 = YoloOutput(256, len(masks[1]), classes, name='yolo_output_1')(x)\n\n    x = YoloConv(128, name='yolo_conv_2')((x, x_36))\n    output_2 = YoloOutput(128, len(masks[2]), classes, name='yolo_output_2')(x)\n\n    if training:\n        return Model(inputs, (output_0, output_1, output_2), name='yolov3')\n\n    boxes_0 = Lambda(lambda x: yolo_boxes(x, anchors[masks[0]], classes),\n                     name='yolo_boxes_0')(output_0)\n    boxes_1 = Lambda(lambda x: yolo_boxes(x, anchors[masks[1]], classes),\n                     name='yolo_boxes_1')(output_1)\n    boxes_2 = Lambda(lambda x: yolo_boxes(x, anchors[masks[2]], classes),\n                     name='yolo_boxes_2')(output_2)\n\n    outputs = Lambda(lambda x: yolo_nms(x, anchors, masks, classes),\n                     name='yolo_nms')((boxes_0[:3], boxes_1[:3], boxes_2[:3]))\n\n    return Model(inputs, outputs, name='yolov3')","a78ffb72":"def YoloLoss(anchors, classes=80, ignore_thresh=0.5):\n    def yolo_loss(y_true, y_pred):\n        # 1. transform all pred outputs\n        # y_pred: (batch_size, grid, grid, anchors, (x, y, w, h, obj, ...cls))\n        pred_box, pred_obj, pred_class, pred_xywh = yolo_boxes(\n            y_pred, anchors, classes)\n        pred_xy = pred_xywh[..., 0:2]\n        pred_wh = pred_xywh[..., 2:4]\n\n        # 2. transform all true outputs\n        # y_true: (batch_size, grid, grid, anchors, (x1, y1, x2, y2, obj, cls))\n        true_box, true_obj, true_class_idx = tf.split(\n            y_true, (4, 1, 1), axis=-1)\n        true_xy = (true_box[..., 0:2] + true_box[..., 2:4]) \/ 2\n        true_wh = true_box[..., 2:4] - true_box[..., 0:2]\n\n        # give higher weights to small boxes\n        box_loss_scale = 2 - true_wh[..., 0] * true_wh[..., 1]\n\n        # 3. inverting the pred box equations\n        grid_size = tf.shape(y_true)[1]\n        grid = tf.meshgrid(tf.range(grid_size), tf.range(grid_size))\n        grid = tf.expand_dims(tf.stack(grid, axis=-1), axis=2)\n        true_xy = true_xy * tf.cast(grid_size, tf.float32) - \\\n            tf.cast(grid, tf.float32)\n        true_wh = tf.math.log(true_wh \/ anchors)\n        true_wh = tf.where(tf.math.is_inf(true_wh),\n                           tf.zeros_like(true_wh), true_wh)\n\n        # 4. calculate all masks\n        obj_mask = tf.squeeze(true_obj, -1)\n        # ignore false positive when iou is over threshold\n        best_iou = tf.map_fn(\n            lambda x: tf.reduce_max(broadcast_iou(x[0], tf.boolean_mask(\n                x[1], tf.cast(x[2], tf.bool))), axis=-1),\n            (pred_box, true_box, obj_mask),\n            tf.float32)\n        ignore_mask = tf.cast(best_iou < ignore_thresh, tf.float32)\n\n        # 5. calculate all losses\n        xy_loss = obj_mask * box_loss_scale * \\\n            tf.reduce_sum(tf.square(true_xy - pred_xy), axis=-1)\n        wh_loss = obj_mask * box_loss_scale * \\\n            tf.reduce_sum(tf.square(true_wh - pred_wh), axis=-1)\n        obj_loss = binary_crossentropy(true_obj, pred_obj)\n        obj_loss = obj_mask * obj_loss + \\\n            (1 - obj_mask) * ignore_mask * obj_loss\n        # TODO: use binary_crossentropy instead\n        class_loss = obj_mask * sparse_categorical_crossentropy(\n            true_class_idx, pred_class)\n\n        # 6. sum over (batch, gridx, gridy, anchors) => (batch, 1)\n        xy_loss = tf.reduce_sum(xy_loss, axis=(1, 2, 3))\n        wh_loss = tf.reduce_sum(wh_loss, axis=(1, 2, 3))\n        obj_loss = tf.reduce_sum(obj_loss, axis=(1, 2, 3))\n        class_loss = tf.reduce_sum(class_loss, axis=(1, 2, 3))\n\n        return xy_loss + wh_loss + obj_loss + class_loss\n    return yolo_loss","9080eb18":"YOLOV3_LAYER_LIST = [\n    'yolo_darknet',\n    'yolo_conv_0',\n    'yolo_output_0',\n    'yolo_conv_1',\n    'yolo_output_1',\n    'yolo_conv_2',\n    'yolo_output_2',\n]\n\nYOLOV3_TINY_LAYER_LIST = [\n    'yolo_darknet',\n    'yolo_conv_0',\n    'yolo_output_0',\n    'yolo_conv_1',\n    'yolo_output_1',\n]","39cee34b":"def load_darknet_weights(model, weights_file, tiny=False):\n    wf = open(weights_file, 'rb')\n    major, minor, revision, seen, _ = np.fromfile(wf, dtype=np.int32, count=5)\n\n    if tiny:\n        layers = YOLOV3_TINY_LAYER_LIST\n    else:\n        layers = YOLOV3_LAYER_LIST\n\n    for layer_name in layers:\n        sub_model = model.get_layer(layer_name)\n        for i, layer in enumerate(sub_model.layers):\n            if not layer.name.startswith('conv2d'):\n                continue\n            batch_norm = None\n            if i + 1 < len(sub_model.layers) and \\\n                    sub_model.layers[i + 1].name.startswith('batch_norm'):\n                batch_norm = sub_model.layers[i + 1]\n\n            logging.info(\"{}\/{} {}\".format(\n                sub_model.name, layer.name, 'bn' if batch_norm else 'bias'))\n\n            filters = layer.filters\n            size = layer.kernel_size[0]\n            in_dim = layer.get_input_shape_at(0)[-1]\n\n            if batch_norm is None:\n                conv_bias = np.fromfile(wf, dtype=np.float32, count=filters)\n            else:\n                # darknet [beta, gamma, mean, variance]\n                bn_weights = np.fromfile(\n                    wf, dtype=np.float32, count=4 * filters)\n                # tf [gamma, beta, mean, variance]\n                bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]\n\n            # darknet shape (out_dim, in_dim, height, width)\n            conv_shape = (filters, in_dim, size, size)\n            conv_weights = np.fromfile(\n                wf, dtype=np.float32, count=np.product(conv_shape))\n            # tf shape (height, width, in_dim, out_dim)\n            conv_weights = conv_weights.reshape(\n                conv_shape).transpose([2, 3, 1, 0])\n\n            if batch_norm is None:\n                layer.set_weights([conv_weights, conv_bias])\n            else:\n                layer.set_weights([conv_weights])\n                batch_norm.set_weights(bn_weights)\n\n    assert len(wf.read()) == 0, 'failed to read all data'\n    wf.close()","281e5ffd":"def broadcast_iou(box_1, box_2):\n    # box_1: (..., (x1, y1, x2, y2))\n    # box_2: (N, (x1, y1, x2, y2))\n\n    # broadcast boxes\n    box_1 = tf.expand_dims(box_1, -2)\n    box_2 = tf.expand_dims(box_2, 0)\n    # new_shape: (..., N, (x1, y1, x2, y2))\n    new_shape = tf.broadcast_dynamic_shape(tf.shape(box_1), tf.shape(box_2))\n    box_1 = tf.broadcast_to(box_1, new_shape)\n    box_2 = tf.broadcast_to(box_2, new_shape)\n\n    int_w = tf.maximum(tf.minimum(box_1[..., 2], box_2[..., 2]) -\n                       tf.maximum(box_1[..., 0], box_2[..., 0]), 0)\n    int_h = tf.maximum(tf.minimum(box_1[..., 3], box_2[..., 3]) -\n                       tf.maximum(box_1[..., 1], box_2[..., 1]), 0)\n    int_area = int_w * int_h\n    box_1_area = (box_1[..., 2] - box_1[..., 0]) * \\\n        (box_1[..., 3] - box_1[..., 1])\n    box_2_area = (box_2[..., 2] - box_2[..., 0]) * \\\n        (box_2[..., 3] - box_2[..., 1])\n    return int_area \/ (box_1_area + box_2_area - int_area)","88de99a4":"def draw_outputs(img, outputs, class_names):\n    boxes, objectness, classes, nums = outputs\n    boxes, objectness, classes, nums = boxes[0], objectness[0], classes[0], nums[0]\n    wh = np.flip(img.shape[0:2])\n    for i in range(nums):\n        x1y1 = tuple((np.array(boxes[i][0:2]) * wh).astype(np.int32))\n        x2y2 = tuple((np.array(boxes[i][2:4]) * wh).astype(np.int32))\n        img = cv2.rectangle(img, x1y1, x2y2, (255, 0, 0), 2)\n        img = cv2.putText(img, '{} {:.4f}'.format(\n            class_names[int(classes[i])], objectness[i]),\n            x1y1, cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 255), 2)\n    return img","3cd8666b":"def draw_labels(x, y, class_names):\n    img = x.numpy()\n    boxes, classes = tf.split(y, (4, 1), axis=-1)\n    classes = classes[..., 0]\n    wh = np.flip(img.shape[0:2])\n    for i in range(len(boxes)):\n        x1y1 = tuple((np.array(boxes[i][0:2]) * wh).astype(np.int32))\n        x2y2 = tuple((np.array(boxes[i][2:4]) * wh).astype(np.int32))\n        img = cv2.rectangle(img, x1y1, x2y2, (255, 0, 0), 2)\n        img = cv2.putText(img, class_names[classes[i]],\n                          x1y1, cv2.FONT_HERSHEY_COMPLEX_SMALL,\n                          1, (0, 0, 255), 2)\n    return img","e2581da0":"def freeze_all(model, frozen=True):\n    model.trainable = not frozen\n    if isinstance(model, tf.keras.Model):\n        for l in model.layers:\n            freeze_all(l, frozen)","f0c4a38d":"def mainB(_argv):\n\n    yolo = YoloV3(classes=num_classes)\n    yolo.summary()\n    logging.info('model created')\n\n    load_darknet_weights(yolo, path_weights)\n    logging.info('weights loaded')\n\n    img = np.random.random((1, 320, 320, 3)).astype(np.float32)\n    output = yolo(img)\n    logging.info('sanity check passed')\n\n    yolo.save_weights(path_output)\n    logging.info('weights saved')\n\n\nif __name__ == '__main__':\n    try:\n        app.run(mainB)\n    except SystemExit:\n        pass","63ebaa28":"def mainA(_argv):\n    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n    for physical_device in physical_devices:\n        tf.config.experimental.set_memory_growth(physical_device, True)\n\n    if tiny_flag:\n        yolo = YoloV3Tiny(classes=num_classes)\n    else:\n        yolo = YoloV3(classes=num_classes)\n\n    yolo.load_weights(path_weights).expect_partial()\n    logging.info('weights loaded')\n\n    class_names = [c.strip() for c in open(path_files).readlines()]\n    logging.info('classes loaded')\n\n    if tfrecord_flag:\n        dataset = load_tfrecord_dataset(\n            tfrecord_flag, path_files, resize_img)\n        dataset = dataset.shuffle(512)\n        img_raw, _label = next(iter(dataset.take(1)))\n    else:\n        img_raw = tf.image.decode_image(\n            open(path_img, 'rb').read(), channels=3)\n\n    img = tf.expand_dims(img_raw, 0)\n    img = transform_images(img, resize_img)\n\n    t1 = time.time()\n    boxes, scores, classes, nums = yolo(img)\n    t2 = time.time()\n    logging.info('time: {}'.format(t2 - t1))\n\n    logging.info('detections:')\n    for i in range(nums[0]):\n        logging.info('\\t{}, {}, {}'.format(class_names[int(classes[0][i])],\n                                           np.array(scores[0][i]),\n                                           np.array(boxes[0][i])))\n\n    img = cv2.cvtColor(img_raw.numpy(), cv2.COLOR_RGB2BGR)\n    img = draw_outputs(img, (boxes, scores, classes, nums), class_names)\n    cv2.imwrite(path_output, img)\n    logging.info('output saved to: {}'.format(path_output))\n\n\nif __name__ == '__main__':\n    try:\n        app.run(mainB)\n    except SystemExit:\n        pass","a3045d3b":"#Flags del modelo original sustituidas en funciones posteriores\n\n#flags.DEFINE_string('classes', '.\/data\/coco.names', 'path to classes file')\n#flags.DEFINE_string('weights', '.\/checkpoints\/yolov3.tf',\n                    #'path to weights file')\n#flags.DEFINE_boolean('tiny', False, 'yolov3 or yolov3-tiny')\n#flags.DEFINE_integer('size', 416, 'resize images to')\n#flags.DEFINE_string('video', '.\/data\/video.mp4',\n                    #'path to video file or number for webcam)')\n#flags.DEFINE_string('output', None, 'path to output video')\n#flags.DEFINE_string('output_format', 'XVID', 'codec used in VideoWriter when saving video to file')\n#flags.DEFINE_integer('num_classes', 80, 'number of classes in the model')","d0c9addf":"def mainC(_argv):\n    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n    for physical_device in physical_devices:\n        tf.config.experimental.set_memory_growth(physical_device, True)\n\n    if tiny_flag:\n        yolo = YoloV3Tiny(classes=num_classes)\n    else:\n        yolo = YoloV3(classes=num_classes)\n\n    yolo.load_weights(path_weights)\n    logging.info('weights loaded')\n\n    class_names = [c.strip() for c in open(path_files).readlines()]\n    logging.info('classes loaded')\n\n    times = []\n\n    try:\n        vid = cv2.VideoCapture(int(path_video))\n    except:\n        vid = cv2.VideoCapture(path_video)\n\n    out = None\n\n    if video_path_output:\n        # by default VideoCapture returns float instead of int\n        width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        fps = int(vid.get(cv2.CAP_PROP_FPS))\n        codec = cv2.VideoWriter_fourcc(*video_output_format) ## Atento aqui a este asteristo\n        out = cv2.VideoWriter(video_path_output, codec, fps, (width, height))\n\n    while True:\n        _, img = vid.read()\n\n        if img is None:\n            logging.warning(\"Empty Frame\")\n            time.sleep(0.1)\n            continue\n\n        img_in = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img_in = tf.expand_dims(img_in, 0)\n        img_in = transform_images(img_in, resize_img)\n\n        t1 = time.time()\n        boxes, scores, classes, nums = yolo.predict(img_in)\n        t2 = time.time()\n        times.append(t2-t1)\n        times = times[-20:]\n\n        img = draw_outputs(img, (boxes, scores, classes, nums), class_names)\n        img = cv2.putText(img, \"Time: {:.2f}ms\".format(sum(times)\/len(times)*1000), (0, 30),\n                          cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 255), 2)\n        if video_path_output:\n            out.write(img)\n        cv2.imshow('output', img)\n        if cv2.waitKey(1) == ord('q'):\n            break\n\n    cv2.destroyAllWindows()\n\n\nif __name__ == '__main__':\n    try:\n        app.run(mainC)\n    except SystemExit:\n        pass","65b1add5":"#Flags del modelo original sustituidas en la siguiente funcion\n\n#flags.DEFINE_string('dataset', '', 'path to dataset')\n#flags.DEFINE_string('val_dataset', '', 'path to validation dataset')\n#flags.DEFINE_boolean('tiny', False, 'yolov3 or yolov3-tiny')\n#flags.DEFINE_string('weights', '.\/checkpoints\/yolov3.tf',\n                    #'path to weights file')\n#flags.DEFINE_string('classes', '.\/data\/coco.names', 'path to classes file')\n#flags.DEFINE_enum('mode', 'fit', ['fit', 'eager_fit', 'eager_tf'],\n              #    'fit: model.fit, '\n              #    'eager_fit: model.fit(run_eagerly=True), '\n              #    'eager_tf: custom GradientTape')\n\n#transfer_flag =  ['none', 'darknet', 'no_output', 'frozen', 'fine_tune'],\n                  #'none: Training from scratch, '\n                  #'darknet: Transfer darknet, '\n                  #'no_output: Transfer all but output, '\n                  #'frozen: Transfer and freeze all, '\n                  #'fine_tune: Transfer all and freeze darknet only')\n#flags.DEFINE_integer('size', 416, 'image size')\n#flags.DEFINE_integer('epochs', 2, 'number of epochs')\n#flags.DEFINE_float('learning_rate', 1e-3, 'learning rate')\n#flags.DEFINE_integer('num_classes', 80, 'number of classes in the model')\n#flags.DEFINE_integer('weights_num_classes', None, 'specify num class for `weights` file if different, '\n                     #'useful in transfer learning with different number of classes')","52b0b225":"def mainD(_argv):\n    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n    for physical_device in physical_devices:\n        tf.config.experimental.set_memory_growth(physical_device, True)\n\n    if tiny_flag:\n        model = YoloV3Tiny(resize_img, training=True,\n                           classes=num_classes)\n        anchors = yolo_tiny_anchors\n        anchor_masks = yolo_tiny_anchor_masks\n    else:\n        model = YoloV3(resize_img, training=True, classes=num_classes)\n        anchors = yolo_anchors\n        anchor_masks = yolo_anchor_masks\n\n    if path_data:\n        train_dataset = dataset.load_tfrecord_dataset(\n            path_data, path_files, resize_img)\n    else:\n        train_dataset = load_fake_dataset()\n    train_dataset = train_dataset.shuffle(buffer_size=512)\n    train_dataset = train_dataset.batch(batch_size_flag)\n    train_dataset = train_dataset.map(lambda x, y: (\n        transform_images(x, resize_img),\n        transform_targets(y, anchors, anchor_masks, resize_img)))\n    train_dataset = train_dataset.prefetch(\n        buffer_size=tf.data.experimental.AUTOTUNE)\n\n    if path_val_data:\n        val_dataset = load_tfrecord_dataset(\n            path_val_data, path_files, resize_img)\n    else:\n        val_dataset = load_fake_dataset()\n    val_dataset = val_dataset.batch(batch_size_flag)\n    val_dataset = val_dataset.map(lambda x, y: (\n        transform_images(x, resize_img),\n        transform_targets(y, anchors, anchor_masks, resize_img)))\n\n    # Configure the model for transfer learning\n    if transfer_flag == 'none':\n        pass  # Nothing to do\n    elif transfer_flag in ['darknet', 'no_output']:\n        # Darknet transfer is a special case that works\n        # with incompatible number of classes\n\n        # reset top layers\n        if tiny_flag:\n            model_pretrained = YoloV3Tiny(\n                resize_img, training=True, classes=weights_num_classes or num_classes)\n        else:\n            model_pretrained = YoloV3(\n                resize_img, training=True, classes=weights_num_classes or num_classes)\n        model_pretrained.load_weights(path_weights)\n\n        if transfer_flag == 'darknet':\n            model.get_layer('yolo_darknet').set_weights(\n                model_pretrained.get_layer('yolo_darknet').get_weights())\n            freeze_all(model.get_layer('yolo_darknet'))\n\n        elif transfer_flag == 'no_output':\n            for l in model.layers:\n                if not l.name.startswith('yolo_output'):\n                    l.set_weights(model_pretrained.get_layer(\n                        l.name).get_weights())\n                    freeze_all(l)\n\n    else:\n        # All other transfer require matching classes\n        model.load_weights(path_weights)\n        if transfer_flag == 'fine_tune':\n            # freeze darknet and fine tune other layers\n            darknet = model.get_layer('yolo_darknet')\n            freeze_all(darknet)\n        elif transfer_flag == 'frozen':\n            # freeze everything\n            freeze_all(model)\n\n    optimizer = tf.keras.optimizers.Adam(lr=learning_rate_flag)\n    loss = [YoloLoss(anchors[mask], classes=num_classes)\n            for mask in anchor_masks]\n\n    if mode_flag == 'eager_tf':\n        # Eager mode is great for debugging\n        # Non eager graph mode is recommended for real training\n        avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n        avg_val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\n\n        for epoch in range(1, epochs_flag + 1):\n            for batch, (images, labels) in enumerate(train_dataset):\n                with tf.GradientTape() as tape:\n                    outputs = model(images, training=True)\n                    regularization_loss = tf.reduce_sum(model.losses)\n                    pred_loss = []\n                    for output, label, loss_fn in zip(outputs, labels, loss):\n                        pred_loss.append(loss_fn(label, output))\n                    total_loss = tf.reduce_sum(pred_loss) + regularization_loss\n\n                grads = tape.gradient(total_loss, model.trainable_variables)\n                optimizer.apply_gradients(\n                    zip(grads, model.trainable_variables))\n\n                logging.info(\"{}_train_{}, {}, {}\".format(\n                    epoch, batch, total_loss.numpy(),\n                    list(map(lambda x: np.sum(x.numpy()), pred_loss))))\n                avg_loss.update_state(total_loss)\n\n            for batch, (images, labels) in enumerate(val_dataset):\n                outputs = model(images)\n                regularization_loss = tf.reduce_sum(model.losses)\n                pred_loss = []\n                for output, label, loss_fn in zip(outputs, labels, loss):\n                    pred_loss.append(loss_fn(label, output))\n                total_loss = tf.reduce_sum(pred_loss) + regularization_loss\n\n                logging.info(\"{}_val_{}, {}, {}\".format(\n                    epoch, batch, total_loss.numpy(),\n                    list(map(lambda x: np.sum(x.numpy()), pred_loss))))\n                avg_val_loss.update_state(total_loss)\n\n            logging.info(\"{}, train: {}, val: {}\".format(\n                epoch,\n                avg_loss.result().numpy(),\n                avg_val_loss.result().numpy()))\n\n            avg_loss.reset_states()\n            avg_val_loss.reset_states()\n            model.save_weights(\n                'checkpoints\/yolov3_train_{}.tf'.format(epoch))\n    else:\n        model.compile(optimizer=optimizer, loss=loss,\n                      run_eagerly=(mode_flag == 'eager_fit'))\n\n        callbacks = [\n            ReduceLROnPlateau(verbose=1),\n            EarlyStopping(patience=3, verbose=1),\n            ModelCheckpoint('checkpoints\/yolov3_train_{epoch}.tf',\n                            verbose=1, save_weights_only=True),\n            TensorBoard(log_dir='logs')\n        ]\n\n        history = model.fit(train_dataset,\n                            epochs=epochs_flag,\n                            callbacks=callbacks,\n                            validation_data=val_dataset)\n\n\nif __name__ == '__main__':\n    try:\n        app.run(mainA)\n    except SystemExit:\n        pass","8df8f5c1":"@tf.function\ndef transform_targets_for_output(y_true, grid_size, anchor_idxs):\n    # y_true: (N, boxes, (x1, y1, x2, y2, class, best_anchor))\n    N = tf.shape(y_true)[0]\n\n    # y_true_out: (N, grid, grid, anchors, [x1, y1, x2, y2, obj, class])\n    y_true_out = tf.zeros(\n        (N, grid_size, grid_size, tf.shape(anchor_idxs)[0], 6))\n\n    anchor_idxs = tf.cast(anchor_idxs, tf.int32)\n\n    indexes = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n    idx = 0\n    for i in tf.range(N):\n        for j in tf.range(tf.shape(y_true)[1]):\n            if tf.equal(y_true[i][j][2], 0):\n                continue\n            anchor_eq = tf.equal(\n                anchor_idxs, tf.cast(y_true[i][j][5], tf.int32))\n\n            if tf.reduce_any(anchor_eq):\n                box = y_true[i][j][0:4]\n                box_xy = (y_true[i][j][0:2] + y_true[i][j][2:4]) \/ 2\n\n                anchor_idx = tf.cast(tf.where(anchor_eq), tf.int32)\n                grid_xy = tf.cast(box_xy \/\/ (1\/grid_size), tf.int32)\n\n                # grid[y][x][anchor] = (tx, ty, bw, bh, obj, class)\n                indexes = indexes.write(\n                    idx, [i, grid_xy[1], grid_xy[0], anchor_idx[0][0]])\n                updates = updates.write(\n                    idx, [box[0], box[1], box[2], box[3], 1, y_true[i][j][4]])\n                idx += 1\n\n    # tf.print(indexes.stack())\n    # tf.print(updates.stack())\n\n    return tf.tensor_scatter_nd_update(\n        y_true_out, indexes.stack(), updates.stack())","bec55044":"def transform_targets(y_train, anchors, anchor_masks, size):\n    y_outs = []\n    grid_size = size \/\/ 32\n\n    # calculate anchor index for true boxes\n    anchors = tf.cast(anchors, tf.float32)\n    anchor_area = anchors[..., 0] * anchors[..., 1]\n    box_wh = y_train[..., 2:4] - y_train[..., 0:2]\n    box_wh = tf.tile(tf.expand_dims(box_wh, -2),\n                     (1, 1, tf.shape(anchors)[0], 1))\n    box_area = box_wh[..., 0] * box_wh[..., 1]\n    intersection = tf.minimum(box_wh[..., 0], anchors[..., 0]) * \\\n        tf.minimum(box_wh[..., 1], anchors[..., 1])\n    iou = intersection \/ (box_area + anchor_area - intersection)\n    anchor_idx = tf.cast(tf.argmax(iou, axis=-1), tf.float32)\n    anchor_idx = tf.expand_dims(anchor_idx, axis=-1)\n\n    y_train = tf.concat([y_train, anchor_idx], axis=-1)\n\n    for anchor_idxs in anchor_masks:\n        y_outs.append(transform_targets_for_output(\n            y_train, grid_size, anchor_idxs))\n        grid_size *= 2\n\n    return tuple(y_outs)","4d48b08b":"def transform_images(x_train, size):\n    x_train = tf.image.resize(x_train, (size, size))\n    x_train = x_train \/ 255\n    return x_train\n\n\n# https:\/\/github.com\/tensorflow\/models\/blob\/master\/research\/object_detection\/g3doc\/\n#using_your_own_dataset.md#conversion-script-outline-conversion-script-outline\n\n# Commented out fields are not required in our project\nIMAGE_FEATURE_MAP = {\n    # 'image\/width': tf.io.FixedLenFeature([], tf.int64),\n    # 'image\/height': tf.io.FixedLenFeature([], tf.int64),\n    # 'image\/filename': tf.io.FixedLenFeature([], tf.string),\n    # 'image\/source_id': tf.io.FixedLenFeature([], tf.string),\n    # 'image\/key\/sha256': tf.io.FixedLenFeature([], tf.string),\n    'image\/encoded': tf.io.FixedLenFeature([], tf.string),\n    # 'image\/format': tf.io.FixedLenFeature([], tf.string),\n    'image\/object\/bbox\/xmin': tf.io.VarLenFeature(tf.float32),\n    'image\/object\/bbox\/ymin': tf.io.VarLenFeature(tf.float32),\n    'image\/object\/bbox\/xmax': tf.io.VarLenFeature(tf.float32),\n    'image\/object\/bbox\/ymax': tf.io.VarLenFeature(tf.float32),\n    'image\/object\/class\/text': tf.io.VarLenFeature(tf.string),\n    # 'image\/object\/class\/label': tf.io.VarLenFeature(tf.int64),\n    # 'image\/object\/difficult': tf.io.VarLenFeature(tf.int64),\n    # 'image\/object\/truncated': tf.io.VarLenFeature(tf.int64),\n    # 'image\/object\/view': tf.io.VarLenFeature(tf.string),\n}","02387250":"def parse_tfrecord(tfrecord, class_table, size):\n    x = tf.io.parse_single_example(tfrecord, IMAGE_FEATURE_MAP)\n    x_train = tf.image.decode_jpeg(x['image\/encoded'], channels=3)\n    x_train = tf.image.resize(x_train, (size, size))\n\n    class_text = tf.sparse.to_dense(\n        x['image\/object\/class\/text'], default_value='')\n    labels = tf.cast(class_table.lookup(class_text), tf.float32)\n    y_train = tf.stack([tf.sparse.to_dense(x['image\/object\/bbox\/xmin']),\n                        tf.sparse.to_dense(x['image\/object\/bbox\/ymin']),\n                        tf.sparse.to_dense(x['image\/object\/bbox\/xmax']),\n                        tf.sparse.to_dense(x['image\/object\/bbox\/ymax']),\n                        labels], axis=1)\n\n    paddings = [[0, FLAGS.yolo_max_boxes - tf.shape(y_train)[0]], [0, 0]]\n    y_train = tf.pad(y_train, paddings)\n\n    return x_train, y_train","14c762e6":"def load_tfrecord_dataset(file_pattern, class_file, size=416):\n    LINE_NUMBER = -1  # TODO: use tf.lookup.TextFileIndex.LINE_NUMBER\n    class_table = tf.lookup.StaticHashTable(tf.lookup.TextFileInitializer(\n        class_file, tf.string, 0, tf.int64, LINE_NUMBER, delimiter=\"\\n\"), -1)\n\n    files = tf.data.Dataset.list_files(file_pattern)\n    dataset = files.flat_map(tf.data.TFRecordDataset)\n    return dataset.map(lambda x: parse_tfrecord(x, class_table, size))","267be83b":"def load_fake_dataset():\n    x_train = tf.image.decode_jpeg(\n        open(path_img, 'rb').read(), channels=3)\n    x_train = tf.expand_dims(x_train, axis=0)\n\n    labels = [\n        [0.18494931, 0.03049111, 0.9435849,  0.96302897, 0],\n        [0.01586703, 0.35938117, 0.17582396, 0.6069674, 56],\n        [0.09158827, 0.48252046, 0.26967454, 0.6403017, 67]\n    ] + [[0, 0, 0, 0, 0]] * 5\n    y_train = tf.convert_to_tensor(labels, tf.float32)\n    y_train = tf.expand_dims(y_train, axis=0)\n\n    return tf.data.Dataset.from_tensor_slices((x_train, y_train))","389b7447":"yolo = YoloV3(classes=num_classes)\nyolo.summary()","2d71e7dc":"#Cargamos manualmente los pesos\n\nyolo.load_weights(r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\checkpoints\\yolov3.tf')\nprint(\"Cargado modelo desde disco.\")","eca8dc58":"im = cv2.imread(r'C:\\Users\\pablo.cumbrera\\Pictures\\Books\\Image detection YOLO\\panda.jpg')\nprint(im.shape)\nimage = tf.image.resize(im,(416,416))\/255\nprint(\"After resizing for Input\",image.shape)","b693b73b":"import matplotlib.pyplot as plt\nplt.imshow(image)","b8dd4423":"prediction = yolo.predict(np.array([image]))\nprint(prediction)","eb7d3073":"#Localizaci\u00f3n\n\nx1=(prediction[0][0][0][0])\ny1=(prediction[0][0][0][1])\nx2=(prediction[0][0][0][2])\ny2=(prediction[0][0][0][3])\n\nh=y2-y1\nw=x2-x1\n\n\nprint(x1)\nprint(y1)\nprint(x2)\nprint(y2)\nprint(h)\nprint(w)\n\nimport matplotlib.patches as patches\nfrom PIL import Image\n\nplt.imshow(im)\n\nax = plt.gca()\n\nrect = patches.Rectangle((x1*944 #x1\n                          ,y2*10), #y2'\n                 w*944,#w\n                 h*944, #h\n                 linewidth=2,\n                 edgecolor='cyan',\n                 fill = False)\n\n\nax.add_patch(rect)\n\nplt.show()","9b7c9e48":"#algunas proporciones (calculos propios)\n\ny2*44\nx1*78.44\n530*0.7849\np1=1\/0.7849\np1\n944*0.4407\np2=1\/0.4407\np2","77186078":"path_img = r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\data\\panda.jpg'\nmainA(path_img)\nfrom IPython.display import Image\nImage(filename=r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\outputs\\output.jpg') ","c3ed8a39":"#path_img = r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\data\\perro-casa.jpg'\n#mainA(path_img)\n#from IPython.display import Image\n#Image(filename=r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\outputs\\output.jpg') ","ebf849b4":"#path_img = r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\data\\kyoto-gion.jpg'\n#mainA(path_img)\n#from IPython.display import Image\n#Image(filename=r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\outputs\\output.jpg') ","d9ae7ad5":"path_img = r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\data\\delhi-por-carretera-la-india.jpg'\nmainA(path_img)\nfrom IPython.display import Image\nImage(filename=r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\outputs\\output.jpg') ","4828d78b":"#path_video= r'C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\data\\video2.mp4'\n#mainC(path_video)\nfrom IPython.display import Video\n#Video(r\"C:\\Users\\pablo.cumbrera\\Downloads\\yolov3-tf2-master\\yolov3-tf2-master\\outputs\\video-output.mp4\")","df7cc78c":"#### 1. Definici\u00f3n de funciones del modelo\n___","ecb20404":"- Librerias para definir el entrenamiento","b85e16ed":"Para su funcionamiento, YOLO divide la imagen en un grid 13x13, siendo cada una de estas divisiones encargadas de predecir 5 bounding boxes (845 en total) prediciendo cuadros de identificaci\u00f3n y probabilidades por cada regi\u00f3n. Las cajas son ponderadas a partir de las probabilidades predichas. El algoritmo aprende representaciones generalizables de los objetos, permitiendo un bajo error de detecci\u00f3n para entradas nuevas (stardart threshold=30%) diferentes al conjunto de datos de entrenamiento.\n\nTodas estas predicciones 845, se realizaron al mismo tiempo. Por ello el algoritmo recibe el nombre de 'You Only Look Once'.\n\nEn la imagen a continuaci\u00f3n podemos observar estos pasos.\n1. Grid 2.Bounding Boxes 3.Non max supression","24ef41a5":"- 2. B","7c7b29f7":"- Compilaci\u00f3n del Modelo\n- Prueba y evaluaci\u00f3n del Modelo\n- Conclusiones","1127ad43":"- 1. F","9c90814c":"#### 4. Definici\u00f3n de la funci\u00f3n de detecci\u00f3n\n___","dd9c2cf6":"- 6. B","e7846de9":"#### 7. Definiciones de las funciones de Data Set\n____","28324628":"- 1. K","29e15f3e":"- 1. G","6ed8507a":"- 1. B","e5b9a5f1":"#### 5. Definici\u00f3n de las funciones de detecci\u00f3n de video\n___","d11694f6":"- 3. A","bc4bf762":"- 1. A","74fbdf6e":"####  Introducci\u00f3n\n___","e8c1a961":"#### 2. Definici\u00f3n de las funciones de utilidad\n___","2501dda9":"![1_PSFl5og1c9HIKXlMIJV8-Q.png](attachment:c9a26e2f-adec-48e4-9d11-0ae1ded8d8c8.png)","ec841bab":"### \u00cdndice\n\n- Introducci\u00f3n\n- Librerias (Modelo, utilidad, conversi\u00f3n y entrenamiento)","6a1e7aa8":"- Librerias para definir la conversi\u00f3n","9de425b6":"- Librerias para definir la detecci\u00f3n","3d290dbf":"- 1. I","1dace8ce":"#### Fuente:\n- Contenido bibliogr\u00e1fico:\n    - https:\/\/pjreddie.com\/media\/files\/papers\/YOLOv3.pdf\n    \n    - https:\/\/blog.zenggyu.com\/en\/post\/2018-12-16\/an-introduction-to-evaluation-metrics-for-object-detection\/\n                         \n- Contenido t\u00e9cnico:\n    - https:\/\/github.com\/zzh8829\/yolov3-tf2 - \n        Autor: 'Zihao Zhang' Email: 'zzh8829@gmail.com',","90485556":"<center>\n\n##### Pablo de la Asunci\u00f3n Cumbrera Conde","51290a78":"- 1. J","dc915282":"- 7. D","8a917993":"- 2. D","1acb35a5":"- Librerias para definir el modelo","aceb8dfe":"- 7. F","f3e16f36":"- 6. A","440cbaa7":"- 1. C","8542bf03":"- 7. C","eb49f26a":"#### Prueba y evaluaci\u00f3n del modelo\n___","f36de619":"- 2. E","3a6e1e8a":"- 7. A","34b3212b":"- 1. L","2aa6fa11":"- Probaremos el modelo en las siguientes imagenes en complejidad creciente llamando a las funciones","2f50b516":"- 5. B","0be39eba":"#### Compilaci\u00f3n del Modelo\n___","799cd8c5":"- 0. A","13bdc6c2":"- 2. C","09de8dab":"#### Librer\u00edas\n___","d015bccd":"#### YOLO Training\n\n- El modelo ha sido pre entrenado con el Data Set COCO (Common Objects in Context): https:\/\/cocodataset.org\/\n\n    - 121,408 Imagenes\n    - 883,331 anotaciones de objetos\n    - 80 clases\n    - 640x480 median image ratio","4f213a7f":"#### Arquitectura del modelo:\n\n- El modelo original tiene 53 capas convolucionales (Darknet 53)\n\n- El modelo Tiny se compone de 24 capas convolucionales y 2 capas full conected.\n \n- El output es un tensor 13x13x125 (13x13 matriz en que divide la imagen) y 125 son los datos que contienen cada una de las 5 bounding boxes de cada celda:\n    - [x, y, w, h] Which are: x [0,1] , y [0,1], height and width for the bounding box rectangle\n    - Probability distribution over the classes (Probability that there is an object in the cell, and which class is it\n    - Confidence Score","37311e13":"![Architecture-of-Tiny-YOLO-CNN.png](attachment:20ebae41-b27a-4ce2-8360-cb5e840ad314.png)","3c172782":"- 1. D","6ba3be62":"##########################################################################################################################\n____","46438e72":"<center>\n    \n    \n ## Replicating YOLOv2 CNN Architecture\n    \n___","5741f9bf":"#### \u00bfQu\u00e9 es YOLO?","fd0d8502":"- Detecci\u00f3n de video","c0190cb5":"- 2. A","8f446c91":"- 7. B ","a92cfcd3":"#### 3. Definici\u00f3n de la funcion de conversi\u00f3n\n___","a0e73be1":"- 1. H","e1afdd15":"- 2. F","0ca95421":"- Probamos nuestro modelo de la forma \"manual\"","64d197fe":"- 4. A","59a535a8":"- En Yolov3 las tres salidas que observamos en la imagen se corresponden con 13x13, 26x26 y 52x52 respectivamente para tres diferentes escalas.","9fd0c8fa":"YOLO (You Only Look Once)es un modelo de Deep Learning de c\u00f3digo abierto para detecci\u00f3n de objetos en tiempo real, el cual hace uso de una \u00fanica red neuronal convolucional para detectar objetos en im\u00e1genes.\n\nEn la imagen podemos ver una versi\u00f3n reducida del algoritmo (Tiny YOLO).","2af8e889":"- 1. E","8b84353a":"#### 6. Definici\u00f3n de la funci\u00f3n de entrenamiento\n___","c1a66596":"- Compilamos el modelo llam\u00e1ndolo desde las funciones anteriores de \"1. Modelo\"","bde497d2":"- 0. Definici\u00f3n de funciones miscel\u00e1neas para variar par\u00e1metros\n- 1. Definici\u00f3n de las funciones del modelo\n- 2. Definici\u00f3n de las funciones de utilidad\n- 3. Definici\u00f3n de la funci\u00f3n de conversi\u00f3n\n- 4. Definici\u00f3n de la funci\u00f3n de detecci\u00f3n\n- 5. Definici\u00f3n de la funci\u00f3n de detecci\u00f3n de video\n- 6. Definici\u00f3n de la funci\u00f3n de entrenamiento\n- 7. Definici\u00f3n de las funciones Data Set (nombre original)","8afff472":"#### \u00bfC\u00f3mo funciona YOLO?","85e41f29":"#### 0. Definici\u00f3n funciones miscel\u00e1neas para variar par\u00e1metros\n___","d6a0bdd2":"- 3. B","f56e9375":"- 7. E","8495e59a":"- 5. A","49f0343b":"![yoloo.jpg](attachment:82ff89e8-658b-48f7-bb1c-7c7d51e773c1.jpg)"}}