{"cell_type":{"fa4101fa":"code","2eed197a":"code","49f4708c":"code","fe9f8703":"markdown"},"source":{"fa4101fa":"import matplotlib.pyplot as plt\n\n\ndef draw_training_info_plots(_history):\n    \"\"\"\u041d\u0430\u0440\u0438\u0441\u043e\u0432\u0430\u0442\u044c \u0433\u0440\u0430\u0444\u0438\u043a\u0438 \u043f\u043e\u0442\u0435\u0440\u044c \u043d\u0430 \u044d\u0442\u0430\u043f\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438\"\"\"\n    acc = _history.history['acc']\n    val_acc = _history.history['val_acc']\n    loss = _history.history['loss']\n    val_loss = _history.history['val_loss']\n\n    epochs_plot = range(1, len(acc) + 1)\n    plt.plot(epochs_plot, acc, 'bo', label='Training acc')\n    plt.plot(epochs_plot, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.legend()\n    plt.figure()\n\n    plt.plot(epochs_plot, loss, 'bo', label='Training loss')\n    plt.plot(epochs_plot, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend()\n    plt.show()\n\n    if 'lr' in _history.history:\n        learning_rate = _history.history['lr']\n        plt.plot(epochs_plot, learning_rate, 'b', label='Learning rate')\n        plt.title('Learning rate')\n        plt.xlabel('epoch')\n        plt.ylabel('learning rate')\n        plt.legend()\n        plt.show()\n    return\n\n\ndef print_model(model):\n    \"\"\"\u041d\u0430\u043f\u0435\u0447\u0430\u0442\u0430\u0442\u044c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e \u043c\u043e\u0434\u0435\u043b\u0435\"\"\"\n    print('MODEL TRAINABLE LAYERS:')\n    for i, layer in enumerate(model.layers):\n        print(i, ')', layer, layer.trainable)\n\n    print('MODEL.SUMMARY:')\n    model.summary()\n    return\n","2eed197a":"from typing import Tuple, Callable\n\nimport keras\nfrom keras import layers\nfrom keras import models\nfrom keras.applications import VGG16, Xception, ResNet50, NASNetMobile, MobileNetV2, MobileNet\nfrom keras.engine.training import Model\n\n\ndef get_vgg16_fine_tune_model(_num_classes) -> Tuple[Model, Model, int, Callable]:\n    \"\"\"Fine tuning \u0432\u0435\u0440\u0445\u043d\u0435\u0433\u043e \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u043e\u0433\u043e \u0431\u043b\u043e\u043a\u0430 VGG16 \u0441 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u043c Dense-\u0441\u043b\u043e\u0435\u043c\"\"\"\n    image_size = 224\n    channels_count = 3\n    conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, channels_count))\n    conv_base.trainable = True\n\n    for i, layer in enumerate(conv_base.layers):\n        if i < 15:\n            layer.trainable = False\n        else:\n            layer.trainable = True\n\n    model = models.Sequential()\n    model.add(conv_base)\n\n    # add a global spatial average pooling layer\n    model.add(layers.GlobalAveragePooling2D())\n\n    # let's add a fully-connected layer\n    model.add(layers.Dense(1024, activation='relu'))\n\n    # and a logistic layer -- let's say we have N classes\n    model.add(layers.Dense(_num_classes, activation='softmax'))\n\n    return model, conv_base, image_size, keras.applications.vgg16.preprocess_input\n\n\ndef get_vgg16_full_tune_model(_num_classes) -> Tuple[Model, Model, int, Callable]:\n    \"\"\"\u041f\u043e\u043b\u043d\u043e\u0435 \u0434\u043e\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 VGG16\"\"\"\n    image_size = 224\n    channels_count = 3\n    conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, channels_count))\n\n    # \u0414\u043e\u043e\u0431\u0443\u0447\u0430\u0435\u043c \u0432\u0441\u044e \u0441\u0435\u0442\u044c.\n    conv_base.trainable = True\n    for i, layer in enumerate(conv_base.layers):\n        layer.trainable = True\n\n    model = models.Sequential()\n    model.add(conv_base)\n\n    # add a global spatial average pooling layer\n    model.add(layers.GlobalAveragePooling2D())\n\n    # let's add a fully-connected layer\n    model.add(layers.Dense(1024, activation='relu'))\n\n    # and a logistic layer -- let's say we have N classes\n    model.add(layers.Dense(_num_classes, activation='softmax'))\n\n    return model, conv_base, image_size, keras.applications.vgg16.preprocess_input\n\n\ndef get_vgg16_fine_tune_model_concatenated(_num_classes) -> Tuple[Model, Model, int, Callable]:\n    \"\"\"Fine tuning \u0432\u0435\u0440\u0445\u043d\u0435\u0433\u043e \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u043e\u0433\u043e \u0431\u043b\u043e\u043a\u0430 VGG16 \u0441 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u043c Dense-\u0441\u043b\u043e\u0435\u043c.\n    \u041f\u0440\u044f\u043c\u043e\u0435 \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 \u0441\u043b\u043e\u0435\u0432 VGG16 \u0432 \u0438\u0442\u043e\u0433\u043e\u0432\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c\"\"\"\n    image_size = 224\n    channels_count = 3\n    initial_model: Model = VGG16(weights='imagenet', include_top=False,\n                                 input_shape=(image_size, image_size, channels_count))\n\n    # \u0414\u043e\u043e\u0431\u0443\u0447\u0430\u0435\u043c \u0432\u0435\u0440\u0445\u043d\u0438\u0435 \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u044b\u0435 \u0441\u043b\u043e\u0438.\n    initial_model.trainable = True\n    for i, layer in enumerate(initial_model.layers):\n        if i < 15:\n            layer.trainable = False\n        else:\n            layer.trainable = True\n\n    # \u0412\u043a\u043b\u044e\u0447\u0430\u0435\u043c \u0441\u043b\u043e\u0438 VGG16 \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0432 \u0438\u0442\u043e\u0433\u043e\u0432\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c.\n    initial_model_output = initial_model.output\n    x = layers.GlobalAveragePooling2D()(initial_model_output)\n    x = layers.Dense(1024, activation='relu')(x)\n    predictions = layers.Dense(_num_classes, activation='softmax')(x)\n\n    model = Model(initial_model.input, predictions)\n\n    return model, initial_model, image_size, keras.applications.vgg16.preprocess_input\n\n\ndef get_xception_fine_tune_model(_num_classes) -> Tuple[Model, Model, int, Callable]:\n    \"\"\"Fine tuning \u0432\u0435\u0440\u0445\u043d\u0435\u0433\u043e \u0431\u043b\u043e\u043a\u0430 Xception\"\"\"\n    # It should have exactly 3 inputs channels, and width and height should be no smaller than 71.\n    # E.g. (150, 150, 3) would be one valid value.\n    image_size = 299\n    channels_count = 3\n    conv_base = Xception(weights='imagenet', include_top=False, input_shape=(image_size, image_size, channels_count))\n    conv_base.trainable = True\n\n    for i, layer in enumerate(conv_base.layers):\n        if i < 115:\n            layer.trainable = False\n        else:\n            layer.trainable = True\n\n    model = models.Sequential()\n    model.add(conv_base)\n\n    # add a global spatial average pooling layer\n    model.add(layers.GlobalAveragePooling2D())\n\n    # let's add a fully-connected layer\n    model.add(layers.Dense(1024, activation='relu'))\n\n    # and a logistic layer -- let's say we have N classes\n    model.add(layers.Dense(_num_classes, activation='softmax'))\n\n    return model, conv_base, image_size, keras.applications.xception.preprocess_input\n\n\ndef get_resnet50_feature_extraction_model(_num_classes) -> Tuple[Model, Model, int, Callable]:\n    \"\"\"Feature extraction ResNet50\"\"\"\n    image_size = 224\n    channels_count = 3\n    conv_base = ResNet50(weights='imagenet', include_top=False, input_shape=(image_size, image_size, channels_count))\n\n    # \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u0432\u0441\u044e \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u0443\u044e \u043e\u0441\u043d\u043e\u0432\u0443.\n    conv_base.trainable = True\n\n    model = models.Sequential()\n    model.add(conv_base)\n\n    # add a global spatial average pooling layer\n    model.add(layers.GlobalAveragePooling2D())\n\n    # let's add a fully-connected layer\n    model.add(layers.Dense(1024, activation='relu'))\n\n    # and a logistic layer -- let's say we have N classes\n    model.add(layers.Dense(_num_classes, activation='softmax'))\n\n    return model, conv_base, image_size, keras.applications.resnet50.preprocess_input\n\n\ndef get_nasnetmobile_full_tune_model(_num_classes) -> Tuple[Model, Model, int, Callable]:\n    \"\"\"Fine tuning \u0432\u0435\u0440\u0445\u043d\u0435\u0433\u043e \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u043e\u0433\u043e \u0431\u043b\u043e\u043a\u0430 VGG16 \u0441 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u043c Dense-\u0441\u043b\u043e\u0435\u043c\"\"\"\n    image_size = 224\n    channels_count = 3\n    conv_base = NASNetMobile(weights='imagenet', include_top=False,\n                             input_shape=(image_size, image_size, channels_count))\n    conv_base.trainable = True\n\n    # for i, layer in enumerate(conv_base.layers):\n    #     if i < 15:\n    #         layer.trainable = False\n    #     else:\n    #         layer.trainable = True\n\n    model = models.Sequential()\n    model.add(conv_base)\n\n    # add a global spatial average pooling layer\n    model.add(layers.GlobalAveragePooling2D())\n\n    # let's add a fully-connected layer\n    model.add(layers.Dense(1024, activation='relu'))\n\n    # and a logistic layer -- let's say we have N classes\n    model.add(layers.Dense(_num_classes, activation='softmax'))\n\n    return model, conv_base, image_size, keras.applications.nasnet.preprocess_input\n\n\ndef get_mobilenet_full_tune_model(_num_classes) -> Tuple[Model, Model, int, Callable]:\n    \"\"\"\u0414\u043e\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0432\u0441\u0435\u0445 \u0441\u043b\u043e\u0435\u0432 MobileNet\"\"\"\n    image_size = 224\n    channels_count = 3\n    conv_base = MobileNet(weights='imagenet', alpha=1.0, include_top=False,\n                          input_shape=(image_size, image_size, channels_count))\n\n    # \u0414\u043e\u043e\u0431\u0443\u0447\u0430\u0435\u043c \u0432\u0441\u044e \u0441\u0435\u0442\u044c.\n    conv_base.trainable = True\n    for i, layer in enumerate(conv_base.layers):\n        layer.trainable = True\n\n    model = models.Sequential()\n    model.add(conv_base)\n\n    # add a global spatial average pooling layer\n    model.add(layers.GlobalAveragePooling2D())\n\n    # let's add a fully-connected layer\n    model.add(layers.Dense(1024, activation='relu'))\n\n    # and a logistic layer -- let's say we have N classes\n    model.add(layers.Dense(_num_classes, activation='softmax'))\n\n    return model, conv_base, image_size, keras.applications.mobilenet_v2.preprocess_input\n\n\ndef get_mobilenetv2_full_tune_model_alpha_1(_num_classes) -> Tuple[Model, Model, int, Callable]:\n    \"\"\"\u0414\u043e\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0432\u0441\u0435\u0445 \u0441\u043b\u043e\u0435\u0432 MobileNetV2, alpha=1.0\"\"\"\n    image_size = 224\n    channels_count = 3\n    conv_base = MobileNetV2(weights='imagenet', alpha=1.0, include_top=False,\n                            input_shape=(image_size, image_size, channels_count))\n\n    # \u0414\u043e\u043e\u0431\u0443\u0447\u0430\u0435\u043c \u0432\u0441\u044e \u0441\u0435\u0442\u044c.\n    conv_base.trainable = True\n    for i, layer in enumerate(conv_base.layers):\n        layer.trainable = True\n\n    model = models.Sequential()\n    model.add(conv_base)\n\n    # add a global spatial average pooling layer\n    model.add(layers.GlobalAveragePooling2D())\n\n    # let's add a fully-connected layer\n    model.add(layers.Dense(1024, activation='relu'))\n\n    # and a logistic layer -- let's say we have N classes\n    model.add(layers.Dense(_num_classes, activation='softmax'))\n\n    return model, conv_base, image_size, keras.applications.mobilenet_v2.preprocess_input\n\n\ndef get_mobilenetv2_full_tune_model_alpha_1_4(_num_classes) -> Tuple[Model, Model, int, Callable]:\n    \"\"\"\u0414\u043e\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0432\u0441\u0435\u0445 \u0441\u043b\u043e\u0435\u0432 MobileNetV2, alpha=1.4\"\"\"\n    # If imagenet weights are being loaded, alpha can be one of `0.35`, `0.50`, `0.75`, `1.0`, `1.3` or `1.4` only.\n    image_size = 224\n    channels_count = 3\n    conv_base = MobileNetV2(weights='imagenet', alpha=1.4, include_top=False,\n                            input_shape=(image_size, image_size, channels_count))\n\n    # \u0414\u043e\u043e\u0431\u0443\u0447\u0430\u0435\u043c \u0432\u0441\u044e \u0441\u0435\u0442\u044c.\n    conv_base.trainable = True\n    for i, layer in enumerate(conv_base.layers):\n        layer.trainable = True\n\n    model = models.Sequential()\n    model.add(conv_base)\n\n    # add a global spatial average pooling layer\n    model.add(layers.GlobalAveragePooling2D())\n\n    # let's add a fully-connected layer\n    model.add(layers.Dense(1024, activation='relu'))\n\n    # and a logistic layer -- let's say we have N classes\n    model.add(layers.Dense(_num_classes, activation='softmax'))\n\n    return model, conv_base, image_size, keras.applications.mobilenet_v2.preprocess_input\n\n\ndef get_mobilenetv2_full_tune_model_alpha_1_4_concatenated(_num_classes) -> Tuple[Model, Model, int, Callable]:\n    \"\"\"\u0414\u043e\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0432\u0441\u0435\u0445 \u0441\u043b\u043e\u0435\u0432 MobileNetV2, alpha=1.4. \u0421\u043b\u043e\u0438 MobileNetV2 \u0432\u043a\u043b\u044e\u0447\u0435\u043d\u044b \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e.\"\"\"\n    # If imagenet weights are being loaded, alpha can be one of `0.35`, `0.50`, `0.75`, `1.0`, `1.3` or `1.4` only.\n    image_size = 224\n    channels_count = 3\n    initial_model: Model = MobileNetV2(weights='imagenet', alpha=1.4, include_top=False,\n                                       input_shape=(image_size, image_size, channels_count))\n\n    # \u0414\u043e\u043e\u0431\u0443\u0447\u0430\u0435\u043c \u0432\u0441\u044e \u0441\u0435\u0442\u044c.\n    initial_model.trainable = True\n    for i, layer in enumerate(initial_model.layers):\n        layer.trainable = True\n\n    # \u0412\u043a\u043b\u044e\u0447\u0430\u0435\u043c \u0441\u043b\u043e\u0438 MobileNetV2 \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e \u0432 \u0438\u0442\u043e\u0433\u043e\u0432\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c.\n    initial_model_output = initial_model.output\n    x = layers.GlobalAveragePooling2D()(initial_model_output)\n    x = layers.Dense(1024, activation='relu')(x)\n    predictions = layers.Dense(_num_classes, activation='softmax')(x)\n\n    model = Model(initial_model.input, predictions)\n\n    return model, initial_model, image_size, keras.applications.mobilenet_v2.preprocess_input\n","49f4708c":"import functools\nimport os\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import plot_model\n\n# ----------------------------------------------------------------------------------------------------------------------\n# \u0412\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438:\ndef print_model_info(_conv_base, _model):\n    \"\"\"\u041d\u0430\u043f\u0435\u0447\u0430\u0442\u0430\u0442\u044c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u043e\u0439 \u043e\u0441\u043d\u043e\u0432\u0435 \u0438 \u0432\u0441\u0435\u0439 \u043c\u043e\u0434\u0435\u043b\u0435\"\"\"\n    print('CONV_BASE.SUMMARY:')\n    _conv_base.summary()\n\n    print('CONV_BASE TRAINABLE LAYERS:')\n    for i, layer in enumerate(_conv_base.layers):\n        print(i, ')', layer, layer.trainable)\n\n    print('MODEL.SUMMARY:')\n    _model.summary()\n    return\n\n\n# ----------------------------------------------------------------------------------------------------------------------\ndef get_callbacks_list(_early_stopping_patience, _reduce_lr_on_plateau_factor, _reduce_lr_on_plateau_patience):\n    \"\"\"\u041f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043a\u043e\u043b\u043b\u0431\u0435\u043a\u0438 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438\"\"\"\n    return [\n        keras.callbacks.EarlyStopping(\n            monitor='val_acc',\n            patience=_early_stopping_patience\n        ),\n        keras.callbacks.ModelCheckpoint(\n            verbose=1,\n            filepath='best_model.h5',\n            monitor='val_loss',\n            save_best_only=True\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            verbose=1,\n            monitor='val_loss',\n            factor=_reduce_lr_on_plateau_factor,\n            patience=_reduce_lr_on_plateau_patience\n        ),\n    ]\n\n\n# ----------------------------------------------------------------------------------------------------------------------\n# \u0414\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u0438 \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438:\n# train_dir = 'D:\/ML Datasets\/stanford-car-dataset-by-classes-folder\/car_data\/train'\n# validation_dir = 'D:\/ML Datasets\/stanford-car-dataset-by-classes-folder\/car_data\/test'\ntrain_dir = '..\/input\/car_data\/car_data\/train'\nvalidation_dir = '..\/input\/car_data\/car_data\/test'\n# train_dir = '..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/train'\n# validation_dir = '..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/test'\n\n# ----------------------------------------------------------------------------------------------------------------------\n# \u0413\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b:\nbatch_size = 80\ninit_lr = 0.0001\nmomentum = 0.9\nepochs = 45\n# optimazer = keras.optimizers.SGD(lr=init_lr, momentum=momentum)\noptimazer = keras.optimizers.Adam(lr=init_lr)\n\n# \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u043a\u043e\u043b\u043b\u0431\u0435\u043a\u043e\u0432:\nearly_stopping_patience = 10\nreduce_lr_on_plateau_factor = 0.2\nreduce_lr_on_plateau_patience = 3\n\n# \u0412\u044b\u0431\u0440\u0430\u043d\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c:\nmodel_function = get_resnet50_feature_extraction_model\n\n# ----------------------------------------------------------------------------------------------------------------------\n# \u0423\u0437\u043d\u0430\u0435\u043c \u0447\u0438\u0441\u043b\u043e \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0437\u0430\u0434\u0430\u0447\u0438 \u043f\u043e \u0447\u0438\u0441\u043b\u043e \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u0439 \u0432 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u043f\u0430\u043f\u043a\u0435:\nnum_classes = len(os.listdir(train_dir))\n\n# \u041f\u043e\u043b\u0443\u0447\u0438\u043c \u043f\u043e\u043b\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c, \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u0443\u044e \u043e\u0441\u043d\u043e\u0432\u0443, \u0440\u0430\u0437\u043c\u0435\u0440 \u0438\u0437\u043e\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438 \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0438\u043d\u0433\u0430 \u0432\u0445\u043e\u0434\u043d\u044b\u0445 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439:\nmodel, conv_base, image_size, preprocess_function = model_function(num_classes)\n\n# \u0420\u0430\u0441\u043f\u0435\u0447\u0430\u0442\u0430\u0435\u043c \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438:\nprint_model_info(conv_base, model)\n\n# \u041d\u0430\u0440\u0438\u0441\u0443\u0435\u043c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0441 \u0440\u0430\u0441\u043f\u0435\u0447\u0430\u0442\u043a\u043e\u0439 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438:\nplot_model(conv_base, show_shapes=True, to_file='conv_base.png')\nplot_model(model, show_shapes=True, to_file='model.png')\n\n# ----------------------------------------------------------------------------------------------------------------------\n# \u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\u044b:\n# \u0420\u0430\u0441\u0448\u0438\u0440\u044f\u0435\u043c \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435.\n# \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e preprocess_function, \u0431\u0435\u0437 \u043a\u0430\u043a\u043e\u0439-\u043b\u0438\u0431\u043e \u0438\u043d\u043e\u0439 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043f\u0438\u043a\u0441\u0435\u043b\u043e\u0432 \u0438 \u043a\u0430\u043d\u0430\u043b\u043e\u0432.\ntrain_image_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_function,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\ntrain_generator = train_image_datagen.flow_from_directory(\n    train_dir,\n    target_size=(image_size, image_size),\n    batch_size=batch_size,\n    class_mode='categorical'\n)\ntrain_images_count = len(train_generator.filenames)\n\n# \u041f\u0440\u043e\u0432\u0435\u0440\u043e\u0447\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0435 \u0440\u0430\u0441\u0448\u0438\u0440\u044f\u0435\u043c.\nvalidation_image_datagen = ImageDataGenerator(preprocessing_function=preprocess_function)\nvalidation_generator = validation_image_datagen.flow_from_directory(\n    validation_dir,\n    target_size=(image_size, image_size),\n    batch_size=batch_size,\n    class_mode='categorical'\n)\nvalidation_images_count = len(validation_generator.filenames)\n\n# ----------------------------------------------------------------------------------------------------------------------\n# \u041c\u0435\u0442\u0440\u0438\u043a\u0430 \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0442\u043e\u043f-5 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438\ntop5_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=5)\ntop5_acc.__name__ = 'top5_acc'\n\n# \u041a\u043e\u043c\u043f\u0438\u043b\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c:\nmodel.compile(loss='categorical_crossentropy', optimizer=optimazer, metrics=['accuracy', top5_acc])\n\n# ----------------------------------------------------------------------------------------------------------------------\n# \u0422\u0440\u0435\u043d\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c:\ntrain_steps = len(train_generator.filenames) \/\/ batch_size\nvalidation_steps = len(validation_generator.filenames) \/\/ batch_size\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=train_steps,\n    epochs=epochs,\n    validation_data=validation_generator,\n    validation_steps=validation_steps,\n    callbacks=get_callbacks_list(early_stopping_patience, reduce_lr_on_plateau_factor, reduce_lr_on_plateau_patience)\n)\n\n# ----------------------------------------------------------------------------------------------------------------------\n\n# \u0412\u044b\u0432\u0435\u0434\u0435\u043c \u0440\u0435\u0439\u0442 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445:\nvalidation_score = model.evaluate_generator(validation_generator, steps=validation_steps)\nprint('Validation loss: ', validation_score[0])\nprint('Validation acc:  ', validation_score[1])\n\n# \u0421\u0444\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u0433\u0440\u0430\u0444\u0438\u043a\u0438 \u043f\u043e\u0442\u0435\u0440\u044c \u043d\u0430 \u044d\u0442\u0430\u043f\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438:\ndraw_training_info_plots(history)\n","fe9f8703":"\u0414\u043e\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0432\u0441\u0435\u0439 \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u043e\u0439 \u043e\u0441\u043d\u043e\u0432\u044b ResNet50 \u0441 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440\u043e\u043c Adam."}}