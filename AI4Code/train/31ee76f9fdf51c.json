{"cell_type":{"9a7c7708":"code","e03b31c7":"code","84cefb33":"code","8f1ebcc8":"code","cacf889d":"code","efd04b39":"code","32b6e3bc":"code","cd7c1e35":"code","c2e8a42f":"code","d016c3fa":"code","476b2fad":"code","04efffeb":"code","d718869f":"code","93990a91":"code","de3833de":"code","db70ca92":"code","4055ec82":"code","462e0db2":"code","c811a0e4":"code","1d0a3de4":"code","93bc7719":"code","5f77cb30":"code","c0e589d7":"code","f72c17a4":"code","2f4d47ff":"code","9cdcc3e7":"code","59bceba5":"code","e127ed22":"markdown","05faf234":"markdown","ccd864fb":"markdown","91f3e617":"markdown","02c04fd1":"markdown","1938f46e":"markdown","5f2b57e7":"markdown","3f60b613":"markdown","2ba21b16":"markdown","e6222f8d":"markdown"},"source":{"9a7c7708":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom wordcloud import WordCloud\nfrom gensim.models import Word2Vec\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier","e03b31c7":"df_train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","84cefb33":"df_train.head()","8f1ebcc8":"df_test.head()","cacf889d":"train_na_count = df_train.isna().sum()\nsns.barplot(x=train_na_count.values, y=train_na_count.index)","efd04b39":"test_na_count = df_test.isna().sum()\nsns.barplot(x=test_na_count.values, y=test_na_count.index)","32b6e3bc":"sns.countplot(x=df_train['target'])","cd7c1e35":"def make_wordcloud(text):\n    wordcloud = WordCloud(width = 1000, height = 1000,\n                background_color ='white',\n                stopwords = STOPWORDS,\n                min_font_size = 10).generate(\" \".join(text.values))\n    plt.figure(figsize=(13,13))\n    plt.imshow(wordcloud)","c2e8a42f":"make_wordcloud(df_train['text'])","d016c3fa":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport re\n\n\nwl = WordNetLemmatizer()\nSTOPWORDS = stopwords.words('english')","476b2fad":"# utility function for preprocessing the texts\ndef preprocess_text(texts):\n    corpus = list()\n    for text in texts:\n        text = re.sub(r'https?:\/\/\\S+|www\\.\\S+','',text)    # removing website link if any present in the text \n        text = re.sub(r'[^a-zA-Z]', ' ', text)             # keeping only alphabetic characters\n        text = text.lower()\n        text = text.split()\n        \n        text = [wl.lemmatize(word) for word in text if not word in STOPWORDS]  # lemmatizing the words using wordnet lemmatizer\n        text = \" \".join(text)\n        \n        corpus.append(text)\n    \n    return corpus   ","04efffeb":"df_train['processed_text'] = preprocess_text(df_train['text'])\ndf_test['processed_text'] = preprocess_text(df_test['text'])","d718869f":"sample_df = df_train.sample(n=20).reset_index(drop=True)\nfor i in range(10):\n    print(\"-\"*100)\n    print()\n    print(f\"BEFORE: {sample_df.loc[i, 'text']}\")\n    print()\n    print(f\"AFTER: {sample_df.loc[i, 'processed_text']}\")\n    print()","93990a91":"make_wordcloud(df_train['processed_text'])","de3833de":"make_wordcloud(df_test['processed_text'])","db70ca92":"# tokenizing the processed text using word_tokenize of nltk\ndf_train['tokenized_text'] = df_train['processed_text'].apply(lambda x: word_tokenize(x))\ndf_test['tokenized_text'] = df_test['processed_text'].apply(lambda x: word_tokenize(x))","4055ec82":"corpus = list(df_train['tokenized_text']) + list(df_test['tokenized_text'])","462e0db2":"wv_model = Word2Vec(corpus, vector_size=150, window=3, min_count=2)\n\nwv_model.train(corpus,total_examples=len(corpus),epochs=10)   # training for 10 epochs","c811a0e4":"# averaging the vectors of each word present in a sentence\n\nvector_list = wv_model.wv.key_to_index\ndef word_embedding(token_list):\n    if len(token_list) < 1:\n        return np.random.rand(150)\n    else:\n        vectorized = [wv_model.wv[word] if word in vector_list else np.random.rand(150) for word in token_list]\n    \n    sum_vec = np.sum(vectorized,axis=0)\n    return sum_vec\/len(vectorized)","1d0a3de4":"embedding_train = df_train['tokenized_text'].apply(lambda x: word_embedding(x))\nembedding_test = df_test['tokenized_text'].apply(lambda x: word_embedding(x))\n\nembedding_train = np.array([x for x in embedding_train])\nembedding_test = np.array([x for x in embedding_test])","93bc7719":"X_train, X_val, y_train, y_val = train_test_split(embedding_train, df_train['target'], test_size=0.2, random_state=42) # train-test-split","5f77cb30":"model1 = RandomForestClassifier()\n\nmodel1.fit(X_train, y_train)","c0e589d7":"from sklearn.metrics import accuracy_score\n\npred = model1.predict(X_val)\n\nprint(f\"The accuracy score is {accuracy_score(y_val, pred)*100}\")  ","f72c17a4":"model = RandomForestClassifier()\nmodel.fit(embedding_train, df_train['target'])","2f4d47ff":"final_predict = model.predict(embedding_test)","9cdcc3e7":"df_submission = pd.DataFrame({'id':df_test['id'].values, 'target':final_predict})\ndf_submission.head()","59bceba5":"df_submission.to_csv('submission.csv', index=False)","e127ed22":"### You can observe how preprocessing the text kept the only important terms of the sentences and excluded all those insignificant terms.","05faf234":"## Why this notebook?\nI saw most of the kernel approached the problem using advanced Deep Learning techniques like **LSTM**, **GRU** and state of the art NLP architecture like **Transformer**. I do agree implementing those methods increases the accuracy and perfomance of the model but for many beginners who have just started their NLP journey, it is quite hard for them to grasp it. So, I have tried to keep this notebook as simple as i could.\n\nI have performed basic text preprocessing techniques and used **Word2Vec** for word embedding. For training i have used RandomForestClassifier.","ccd864fb":"### The wordcloud of text before preprocessing shows words like 'https', 'co', 'like', 'new', etc are the most frequently occuring words.","91f3e617":"### The validation accuracy is 75% which is not that bad. Perfoming some hyperparameter optimization would obviously increase the performace, but as i said earlier, i tried to keep this notebook as beginner friendly as i could, so its up to you to tweak some hyperparameters if you wish.","02c04fd1":"## At last, if you have good knowledge of Deep Learning, do go for the advance approach, but if you are beginners you can play with this notebook to further improve the accuracy.\n## Happy Learning!! :D","1938f46e":"# Word Embedding","5f2b57e7":"I have used gensim ***Word2Vec*** for embedding the tokens. You can even use the pretrained Word2Vec model, but for the scope of this notebook i preferred to train a new vectorizing model in the provided sentence corpus. If you want to learn more about Word2Vec model, i highly suggest you to read the article on it by [Jay Alammar](https:\/\/jalammar.github.io\/illustrated-word2vec\/).","3f60b613":"## importing the basic libraries","2ba21b16":"### Both training and test data has many missing values in location columns. Now lets check how balanced the training set is. The plot below shows the data is balanced.","e6222f8d":"## Lets observe the missing values in the dataset"}}