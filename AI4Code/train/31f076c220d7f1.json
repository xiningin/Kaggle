{"cell_type":{"86432460":"code","731ffdf4":"code","756403f5":"code","39cc7164":"code","bb23d87d":"code","4836149b":"code","fe950326":"code","be25e02c":"code","35b02c90":"code","03cf34e5":"code","0e7a940c":"code","6cbcee8a":"code","fb66646b":"code","d43f6015":"code","0e1a753c":"code","2ed2c4f5":"code","5b58a73d":"code","58a6fcc2":"code","232b0efb":"code","87214c0b":"code","d34601da":"code","081302ae":"code","bc8de5d5":"code","d5aceb30":"code","5d3e17d0":"code","84b37fa5":"code","ca8c8c0c":"code","4ba2cb58":"code","d2f84823":"code","f9f28b60":"code","c24a8381":"code","b8dc5f9d":"code","e72a3187":"code","b8bf3ff0":"code","d29f786c":"code","004628f0":"code","e787d45f":"code","eab64d25":"code","21254163":"code","7c8ca419":"code","672abfc1":"code","4b6d0da0":"code","b88a0bde":"code","d698c5e2":"code","1726f5ea":"code","b797242d":"code","0a2001ff":"code","6381041a":"code","93417f02":"code","c606070c":"code","0b436cfa":"code","a1202292":"code","50a52336":"code","573d2de8":"code","03f6c045":"code","7847c38d":"code","07f7f7a2":"code","82f8e77e":"code","a3548b3b":"code","432736bc":"code","5b61b4d7":"markdown","47d1abb4":"markdown","c9c5f5a1":"markdown","5566b789":"markdown","8b4ce884":"markdown","320b656f":"markdown","61fc7cee":"markdown","eb8f1073":"markdown","07ba0dae":"markdown","ef0241e0":"markdown","7559b4a4":"markdown","83dfe9e9":"markdown","df3b61fa":"markdown","e6836365":"markdown","04c8c700":"markdown","ce337535":"markdown","f64a3540":"markdown","ed3fef6c":"markdown","d3259cbc":"markdown","b944630c":"markdown","d06441e4":"markdown","6880d0c3":"markdown","8ef0fca9":"markdown","a2c27e45":"markdown","d8619aa2":"markdown","1c405044":"markdown","06459bf0":"markdown","9b5c6679":"markdown","330920f7":"markdown","9d692474":"markdown","426af676":"markdown","cfbe370a":"markdown","14b6449f":"markdown","8a4ad8e0":"markdown","3975eed8":"markdown","2ad207ce":"markdown","9df58b33":"markdown","547a39ea":"markdown"},"source":{"86432460":"import pandas as pd\nimport matplotlib\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.model_selection import train_test_split as split\nfrom sklearn.metrics import confusion_matrix, SCORERS, classification_report, accuracy_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport warnings\nimport time","731ffdf4":"warnings.simplefilter('ignore')","756403f5":"data=pd.read_csv('..\/input\/train.csv')\ndata.head()","39cc7164":"data.columns","bb23d87d":"data.shape","4836149b":"val_exsplore=(data.agg(['min','max','dtype',lambda df: df.nunique(),pd.unique])\n                  .transpose()\n                  .rename(index=str, columns={'<lambda>':'nunique'})\n             )","fe950326":"pd.set_option('display.max_colwidth', 100)\nval_exsplore.sort_values(by=['nunique'],ascending = False)","be25e02c":"# Check for NaN values\nnan_sum=pd.DataFrame(data.isnull().sum())\nnan_sum[nan_sum[0]>0]","35b02c90":"# replace nan values with 0\ndata_clean = data.copy()\ndata_clean.fillna(value=0,inplace=True)","03cf34e5":"house_labels=pd.DataFrame(data_clean\n                                    .groupby(['idhogar'])\n                                    .Target\n                                    .nunique()\n                         )\nhouse_labels[house_labels['Target']>1].size","0e7a940c":"data_clean[data_clean['parentesco1']==1][['idhogar','Target']]\n","6cbcee8a":"#size of targets of heads of households\ndata_clean[data_clean['parentesco1']==1][['idhogar','Target']].shape[0]","fb66646b":"#number of unique housholds\ndata_clean['idhogar'].nunique()","d43f6015":"data_clean['idhogar'].nunique()-data_clean[data_clean['parentesco1']==1][['idhogar','Target']].shape[0]","0e1a753c":"data_household_ishead=pd.pivot_table(data_clean, index='idhogar',aggfunc = sum, values = ['parentesco1'])\ndata_household_nohead=data_household_ishead[data_household_ishead['parentesco1']==0].index","2ed2c4f5":"missing_labels=data_clean[data_clean['idhogar'].isin(data_household_nohead)][['idhogar','Target']].drop_duplicates()\nall_house_labels=pd.concat([data_clean[data_clean['parentesco1']==1][['idhogar','Target']],missing_labels], axis=0)\nall_house_labels.rename(index=str, columns={'Target': 'Target_new'}, inplace = True)\nall_house_labels.head()","5b58a73d":"house_labels_new=pd.DataFrame(all_house_labels\n                                    .groupby(['idhogar'])\n                                    .Target_new\n                                    .nunique()\n                         )\nhouse_labels_new[house_labels_new['Target_new']>1].size","58a6fcc2":"data_clean = pd.merge(data_clean,all_house_labels, on='idhogar')","232b0efb":"data_clean.shape","87214c0b":"labels = (pd\n            .DataFrame(data_clean\n                            .Target_new\n                            .value_counts()\n                      )\n            .sort_index()\n         )\nlabels['Target_all_%']=((labels['Target_new']\/(labels['Target_new'] .sum()))\n                                                            .round(3)\n                       )\nlabels","d34601da":"labels['house_target']=all_house_labels['Target_new'].value_counts().sort_index()\nlabels['house_target_%']=(labels['house_target']\/labels['house_target'].sum()).round(3)\n#  .plot.bar())\nlabels","081302ae":"names = pd.Series(['extreme', 'moderate', 'vulnerable', 'non vulnerable'])\nlabels.set_index(names,inplace=True)\nlabels['house_target'].plot(kind='bar')","bc8de5d5":"(pd\n     .concat([labels,pd.DataFrame(labels.sum()).transpose()], axis=0)\n     .rename(index={0: 'Total'})\n)","d5aceb30":"def is_adult(s):\n    if (s<=64) & (s>=19):\n        return 1\n    return 0\n    \ndef is_minor(s):\n    if s<19:\n        return 1\n    return 0\n    \ndef is_senior(s):\n    if s>64:\n        return 1\n    return 0\n    \ndata['is_adult']=data['age'].apply(is_adult)\ndata['is_minor']=data['age'].apply(is_minor)\ndata['is_senior']=data['age'].apply(is_senior)","5d3e17d0":"agg_ages=pd.pivot_table(data, index='idhogar', values = ['is_minor','is_adult','is_senior'], aggfunc = sum) \nagg_ages['all_ages']=agg_ages[['is_minor','is_adult','is_senior']].sum(axis=1)\nagg_ages.head(10)","84b37fa5":"agg_ages['dependency_our']=agg_ages[['is_minor','is_senior']].sum(axis=1)\/agg_ages['is_adult']\n# agg_ages.head(20)\n\nagg_ages['dependency_our'].replace(np.inf, 10, inplace=True)","ca8c8c0c":"agg_ages.head()","4ba2cb58":"(data_clean.drop(\n                    axis=1, \n                    columns=['Id','hogar_nin','hogar_adul','hogar_mayor',\n                        'hogar_total','dependency','qmobilephone','age','agesq','Target'],\n                    inplace =True\n                )\n)","d2f84823":"# replace string values with yes with 1 and no with 0\ndata_clean.edjefe=data_clean.edjefe.replace(['yes'], 1, inplace=True)\ndata_clean.edjefe=data_clean.edjefe.replace(['no'], 0, inplace=True)\ndata_clean.edjefa=data_clean.edjefa.replace(['yes'], 1, inplace=True)\ndata_clean.edjefa=data_clean.edjefa.replace(['no'], 0, inplace=True)","f9f28b60":"data_household_max=pd.pivot_table(data_clean, index='idhogar',aggfunc = max, values = ['v2a1','hacdor','rooms','hacapo',\n                    'v14a',\n                    'refrig',\n                    'v18q',\n                    'v18q1',\n                    'r4h1',\n                    'r4h2',\n                    'r4h3',\n                    'r4m1',\n                    'r4m2',\n                    'r4m3',\n                    'r4t1',\n                    'r4t2',\n                    'r4t3',\n                    'tamhog',\n                    'tamviv',\n                    'hhsize',\n                    'paredblolad',\n                    'paredzocalo',\n                    'paredpreb',\n                    'pareddes',\n                    'paredmad',\n                    'paredzinc',\n                    'paredfibras',\n                    'paredother',\n                    'pisomoscer',\n                    'pisocemento',\n                    'pisoother',\n                    'pisonatur',\n                    'pisonotiene',\n                    'pisomadera',\n                    'techozinc',\n                    'techoentrepiso',\n                    'techocane',\n                    'techootro',\n                    'cielorazo',\n                    'abastaguadentro',\n                    'abastaguafuera',\n                    'abastaguano',\n                    'public',\n                    'planpri',\n                    'noelec',\n                    'coopele',\n                    'sanitario1',\n                    'sanitario2',\n                    'sanitario3',\n                    'sanitario5',\n                    'sanitario6',\n                    'energcocinar1',\n                    'energcocinar2',\n                    'energcocinar3',\n                    'energcocinar4',\n                    'elimbasu1',\n                    'elimbasu2',\n                    'elimbasu3',\n                    'elimbasu4',\n                    'elimbasu5',\n                    'elimbasu6',\n                    'epared1',\n                    'epared2',\n                    'epared3',\n                    'etecho1',\n                    'etecho2',\n                    'etecho3',\n                    'eviv1',\n                    'eviv2',\n                    'eviv3',\n                    'dis',\n                    'male',\n                    'female',\n                    'estadocivil1',\n                    'estadocivil2',\n                    'estadocivil3',\n                    'estadocivil4',\n                    'estadocivil5',\n                    'estadocivil6',\n                    'estadocivil7',\n                    'parentesco1',\n                    'parentesco2',\n                    'parentesco3',\n                    'parentesco4',\n                    'parentesco5',\n                    'parentesco6',\n                    'parentesco7',\n                    'parentesco8',\n                    'parentesco9',\n                    'parentesco10',\n                    'parentesco11',\n                    'parentesco12',\n                    'edjefe',\n                    'edjefa',\n                    'meaneduc',\n                    'bedrooms',\n                    'overcrowding',\n                    'tipovivi1',\n                    'tipovivi2',\n                    'tipovivi3',\n                    'tipovivi4',\n                    'tipovivi5',\n                    'computer',\n                    'television',\n                    'mobilephone',\n                    'lugar1',\n                    'lugar2',\n                    'lugar3',\n                    'lugar4',\n                    'lugar5',\n                    'lugar6',\n                    'area1',\n                    'area2',\n                    'SQBescolari',\n                    'SQBage',\n                    'SQBhogar_total',\n                    'SQBedjefe',\n                    'SQBhogar_nin',\n                    'SQBovercrowding',\n                    'SQBdependency',\n                    'SQBmeaned','Target_new']\n                                 )","c24a8381":"data_household_sum=pd.pivot_table(data_clean, index='idhogar',aggfunc = sum, values = ['escolari',\n                                                                                        'rez_esc',\n                                                                                        'instlevel1',\n                                                                                        'instlevel2',\n                                                                                        'instlevel3',\n                                                                                        'instlevel4',\n                                                                                        'instlevel5',\n                                                                                        'instlevel6',\n                                                                                        'instlevel7',\n                                                                                        'instlevel8',\n                                                                                        'instlevel9']\n                                 )","b8dc5f9d":"data_houshold=(data_household_max\n                                .join(data_household_sum)\n                                .join(agg_ages)\n              )\ndata_houshold.head()","e72a3187":"data_houshold = pd.concat([data_houshold,(pd.DataFrame(agg_ages['dependency_our']))], axis=1, sort=False)","b8bf3ff0":"data_houshold.shape","d29f786c":"feature_names = ['v18q','mobilephone','refrig','computer','television']","004628f0":"for feature in feature_names:\n    data_houshold.groupby(['Target_new',feature]).size().unstack().plot(kind='bar', stacked=True)","e787d45f":"region = data_houshold[['lugar1','lugar2','lugar3','lugar4','lugar5','lugar6','Target_new']].copy()\nregion['lugar2'] = region['lugar2'].replace(1,2)\nregion['lugar3'] = region['lugar3'].replace(1,3)\nregion['lugar4'] = region['lugar4'].replace(1,4)\nregion['lugar5'] = region['lugar5'].replace(1,5)\nregion['lugar6'] = region['lugar6'].replace(1,6)\nregion['Region']= (region[['lugar1','lugar2','lugar3','lugar4','lugar5','lugar6']].max(axis =1)\n      .replace([1,2,3,4,5,6],['Central','Chorotega','Pac\u00c3\u0192\u00c2\u00adfico central','Brunca','Huetar Atl\u00c3\u0192\u00c2\u00a1ntica','Huetar Norte']))\nregion['Target_new']=region['Target_new'].replace([1,2,3,4],['extreme', 'moderate', 'vulnerable', 'non vulnerable'])\nregion.groupby(['Region','Target_new']).size().groupby(level=0).apply(lambda x: 100 * x \/ x.sum()).unstack().plot(kind='bar',stacked=True)\n","eab64d25":"param_grid_log_reg_L1 = {'Feature_selection__estimator__C': [0.1, 1, 10],\n                          'clf__C' : [0.1, 1, 10], \n                          'Feature_selection__threshold': [0.05, 0.1, 0.2]\n                         }\n\nparam_grid_log_reg_PCA = {'Feature_extraction__n_components': [30, 60, 90, 120],\n                          'clf__C' : [0.1, 1, 10]\n                         }\n\nparam_grid_log_reg = {'clf__C' : [0.01, 0.1, 1, 10, 100]}\n\nparam_grid_RF_L1 = {'Feature_selection__estimator__C': [0.1, 1, 10],\n                    'Feature_selection__threshold': [0.05, 0.1, 0.2], \n                    'clf__min_samples_split': [2,8,15,20]\n                   }\n\nparam_grid_RF_PCA = {'Feature_extraction__n_components': [30, 60, 90, 120],\n                     'clf__min_samples_split': [2,8,15,20]\n                    }\n\nparam_grid_RF = {'clf__min_samples_split': [2,8,15,20]}","21254163":"max_iter_param=100\n\n#Scaler\nminmax_scaler = MinMaxScaler()\n\n#Feature selection - 2 labels\nLasso_log_reg2 = LogisticRegression(penalty='l1',\n                                    class_weight='balanced',\n                                    solver='liblinear'\n                                   )\n                                    \nFeature_selection2=SelectFromModel(Lasso_log_reg2)                \n\n#Feature selection - 4 labels\nLasso_log_reg4 =LogisticRegression(penalty='l1' ,\n                                   max_iter = max_iter_param,\n                                   multi_class='multinomial',\n                                   class_weight='balanced', \n                                   solver='saga'\n                                  )\n\nFeature_selection4=SelectFromModel(Lasso_log_reg4)     \n\n#Feature Extraction\nPCA_features=PCA()\n\n#Logistic regression classifier - 2 labels\nlog_reg_clf2=LogisticRegression(solver='lbfgs',\n                                class_weight='balanced'\n                               )\n\n#Logistic regression classifier - 4 labels\nlog_reg_clf4=LogisticRegression(solver='saga',\n                                max_iter = max_iter_param,\n                                multi_class='multinomial',\n                                class_weight='balanced'\n                               )\n\n#Random forest classifier\nRF_clf = RandomForestClassifier(class_weight='balanced' ,\n                                n_estimators=100,\n                                random_state=123\n                               )\n\n","7c8ca419":"#Pipelines - 2 labels\n\nLog_reg_pipe2_L1 = Pipeline(steps=[('Scaler', minmax_scaler),\n                                   ('Feature_selection', Feature_selection2),\n                                   ('clf',log_reg_clf2)\n                                  ]\n                            )\n\nLog_reg_pipe2_PCA = Pipeline(steps=[('Scaler', minmax_scaler),\n                                     ('Feature_extraction', PCA_features),\n                                     ('clf',log_reg_clf2)\n                                   ]\n                            ) \n\nLog_reg_pipe2= Pipeline (steps=[('Scaler', minmax_scaler),\n                                ('clf',log_reg_clf2)\n                               ]\n                        )\n\nRF_pipe_2_L1 = Pipeline(steps=[('Scaler', minmax_scaler),\n                               ('Feature_selection', Feature_selection2),\n                               ('clf',RF_clf)\n                                  ]\n                            )\n\n","672abfc1":"#Pipelines - 4 labels\nLog_reg_pipe4_L1 = Pipeline(steps=[('Scaler', minmax_scaler),\n                                   ('Feature_selection', Feature_selection4),\n                                   ('clf',log_reg_clf4)\n                                  ]\n                            )\n\nLog_reg_pipe4_PCA = Pipeline(steps=[('Scaler', minmax_scaler),\n                                    ('Feature_extraction', PCA_features),\n                                    ('clf',log_reg_clf4)\n                                   ]\n                            ) \n\nLog_reg_pipe4= Pipeline (steps=[('Scaler', minmax_scaler),\n                                ('clf',log_reg_clf4)\n                               ]\n                        )\n\nRF_pipe_4_L1 = Pipeline(steps=[('Scaler', minmax_scaler),\n                               ('Feature_selection', Feature_selection4),\n                               ('clf',RF_clf)\n                                  ]\n                            )","4b6d0da0":"#pipelines for 2 or 4 labels\nRF_pipe_PCA = Pipeline(steps=[('Scaler', minmax_scaler),\n                              ('Feature_extraction', PCA_features),\n                              ('clf',RF_clf)\n                             ]\n                      )\n                                   \n\nRF_pipe = Pipeline(steps=[('Scaler', minmax_scaler),\n                          ('clf',RF_clf)\n                         ]\n                  )","b88a0bde":"#Runs the GridSearchCV fit and finds the best classifer:\ndef fit_model (pipe, grid, X_train, y_train, num_cv, scoring_param):\n    best_clf=GridSearchCV(pipe, grid, cv=num_cv,scoring = scoring_param)\n    print ('Begin GridSearchCV fit')\n    t0 = time.time()\n    best_clf.fit(X_train, y_train)\n    t1 = time.time()\n    h, m ,s=time_convert(t1-t0)\n    print('GridSearchCV ended. Elapsed time: {0:.0f} hours, {1:.0f} minutes and {2:.0f} seconds'.format(h,m,s))\n    return best_clf\n\n#Returns DF with relevant columns (parameter, mean_test_score and rank_test_score)\n#from cv_results_ of the after GridSearchCV fit, sorted by test score rank\ndef cv_results (clf) :\n    df_cv_results = pd.DataFrame(clf.cv_results_)\n    param_list=[i for i in list(df_cv_results.columns) if 'param_' in i]\n    df_cv_results_filter=df_cv_results[param_list+['mean_test_score','rank_test_score']].sort_values(by=['rank_test_score'])\n    return df_cv_results_filter\n\n\n#Returns DF with best classifer params and train\/test scores of for each pipeline:\ndef results_test_df(rdf,name,clf,X_test,y_test,scoring_param):\n    if scoring_param == 'roc_auc':\n        rdf=rdf.append ({ 'Model':name,\n                          'Best_params':clf.best_params_,\n                          'Best_Train_Score':clf.best_score_.round(5),\n                          'Best_Test_Score':roc_auc_score(y_true=y_test, y_score=clf.predict(X_test)).round(5)}\n                          ,ignore_index = True\n                        )\n    elif scoring_param == 'accuracy':\n        rdf=rdf.append ({ 'Model':name,\n                          'Best_params': clf.best_params_,\n                          'Best_Train_Score':clf.best_score_.round(5),\n                          'Best_Test_Score':accuracy_score(y_true=y_test, y_pred=clf.predict(X_test)).round(5)}\n                          ,ignore_index = True\n                       )\n    return rdf\n\ndef time_convert (t):\n    h,m1=divmod(t, 3600)\n    m,s=divmod(m1, 60) \n    return h, m ,s\n\ndef find_best(model_dict, X_train,y_train,X_test,y_test,num_cv,scoring_param,df_init,df_all_best_results):\n    for name, (pipe,grid) in model_dict.items():\n        print ('Model name:',name)\n        best_clf = fit_model (pipe, grid, X_train,y_train, num_cv,scoring_param)\n        best_clf_cv_results=cv_results(best_clf)\n        display(best_clf_cv_results)\n        df_all_best_results = df_all_best_results.append(results_test_df(df_init,name,best_clf,X_test,y_test, scoring_param))\n        print('======================================================')\n    return df_all_best_results\n","d698c5e2":"#feature dictionary - 2 labels\nmodel_dict2={'Feature selection and logistic regression':(Log_reg_pipe2_L1, param_grid_log_reg_L1),\n            'PCA and logistic regression':               (Log_reg_pipe2_PCA, param_grid_log_reg_PCA),\n            'All features and logistic regression':      (Log_reg_pipe2, param_grid_log_reg),\n            'Feature selection and random forest':       (RF_pipe_2_L1,param_grid_RF_L1),\n            'PCA and random forest':                     (RF_pipe_PCA,param_grid_RF_PCA),\n            'All features and random forest':            (RF_pipe , param_grid_RF)\n            }\n\n#feature dictionary - 4 labels\nmodel_dict4={'Feature selection and logistic regression':(Log_reg_pipe4_L1, param_grid_log_reg_L1),\n            'PCA and logistic regression':               (Log_reg_pipe4_PCA, param_grid_log_reg_PCA),\n            'All features and logistic regression':      (Log_reg_pipe4, param_grid_log_reg),\n            'Feature selection and random forest':       (RF_pipe_4_L1,param_grid_RF_L1),\n            'PCA and random forest':                     (RF_pipe_PCA,param_grid_RF_PCA),\n            'All features and random forest':            (RF_pipe , param_grid_RF)\n            }\n\n#init dataframe for best scores of each pipline\ndf_init=pd.DataFrame(columns=['Model','Best_params','Best_Train_Score','Best_Test_Score'])\n\n#number of cross valisation folds\nnum_cv = 10\n\n#scoring parameter - 2 labels\nscoring_param2 = 'roc_auc'\n\n#scoring parameter - 4 labels\nscoring_param4 = 'accuracy'\n\n#Columns display definition\npd.set_option('display.max_colwidth', 0)","1726f5ea":"data_houshold_2=data_houshold.copy()","b797242d":"#Replace the label values to create 2 labels\ndata_houshold_2['Target_new']=data_houshold_2['Target_new'].replace([2,3,4], [1,1,0])","0a2001ff":"data_houshold_2['Target_new'].value_counts()","6381041a":"X_train2, X_test2, y_train2, y_test2 = split(data_houshold_2.drop(axis=1, columns=['Target_new']), \n                                             data_houshold_2['Target_new'], \n                                             test_size =0.3, random_state=123)","93417f02":"\nX_train, X_test, y_train, y_test = X_train2, X_test2, y_train2, y_test2\ndf_all_best_results=pd.DataFrame()\n\n\ndf_all_best_results2=find_best(model_dict2, X_train,y_train,X_test,y_test,num_cv,scoring_param2,df_init,df_all_best_results)\n","c606070c":"data_houshold_2_i=data_houshold.copy()","0b436cfa":"data_houshold_2_i['Target_new']=data_houshold_2_i['Target_new'].replace([2,3,4], [1,0,0])","a1202292":"data_houshold_2_i['Target_new'].value_counts()","50a52336":"X_train2_i, X_test2_i, y_train2_i, y_test2_i = split(data_houshold_2_i.drop(axis=1, columns=['Target_new']), \n                                                     data_houshold_2_i['Target_new'], \n                                                     test_size =0.3, random_state=123)","573d2de8":"\ndf_all_best_results=pd.DataFrame()\nX_train, X_test, y_train, y_test = X_train2_i, X_test2_i, y_train2_i, y_test2_i\n\ndf_all_best_results2_i=find_best(model_dict2, X_train,y_train,X_test,y_test,num_cv,scoring_param2,df_init,df_all_best_results)\n\n","03f6c045":"data_houshold['Target_new'].value_counts()","7847c38d":"X_train4, X_test4, y_train4, y_test4 = split(data_houshold.drop(axis=1, columns=['Target_new']), \n                                             data_houshold['Target_new'], \n                                             test_size =0.3, random_state=123\n                                            )","07f7f7a2":"df_all_best_results=pd.DataFrame()\nX_train, X_test, y_train, y_test = X_train4, X_test4, y_train4, y_test4\n\ndf_all_best_results4=find_best(model_dict4, X_train,y_train,X_test,y_test,num_cv,scoring_param4,df_init,df_all_best_results)\n","82f8e77e":"df_all_best_results2.sort_values(['Best_Test_Score','Best_Train_Score'],ascending = False)","a3548b3b":"df_all_best_results2_i.sort_values(['Best_Test_Score','Best_Train_Score'],ascending = False)","432736bc":"df_all_best_results4.sort_values(['Best_Test_Score','Best_Train_Score'],ascending = False)","5b61b4d7":"## <font color=green>2 Labels - imbalanced data<\/font> ","47d1abb4":"After consideration we decided to replace nan values with 0. ","c9c5f5a1":"The labels of this data are more balanced and they represent non_vulnerable (0) and vulnerable\/poor (1) households. <br>\nWe'll split the data to train and test:","5566b789":"### Exploration and Preprocessing","8b4ce884":"Now our data is clean and aggregated by household.<br>\nWe will create 3 datasets for predicting different labels.<br>\n\n1. **data_houshold_2 - 2 Labels of balanced data:** <br>\n    1 = extreme poverty ==> 1<br>\n    2 = moderate poverty ==> 1 <br>\n    3 = vulnerable households ==> 1 <br>\n    4 = non vulnerable households ==> 0 <br>\n    <br>\n2. **data_houshold_2_i - 2 Labels of imbalanced data:** <br>\n    1 = extreme poverty ==> 1<br>\n    2 = moderate poverty ==> 1 <br>\n    3 = vulnerable households ==> 0 <br>\n    4 = non vulnerable households ==> 0<br>\n    <br>\n3. **data_houshold - 4 Labels of imbalanced data:** <br>\n    1 = extreme poverty <br>\n    2 = moderate poverty <br>\n    3 = vulnerable households <br>\n    4 = non vulnerable households <br>","320b656f":"Aggregate the data by houshold","61fc7cee":"## <font color=red>2 Labels - balanced data<\/font> ","eb8f1073":"**Define scaler, feature extraction and selection models and classifiers:**","07ba0dae":"**Define the piplines:**","ef0241e0":"**The hyper-parameter grids for each configuration:**","7559b4a4":"So there are 15 households with no head of household in the data. We will finde their lables and concatenate to what we have","83dfe9e9":"Logistic regression looks like the better method (with C=1). Seems like keeping all the features is be as good as PCA.","df3b61fa":"# Fitting the models","e6836365":"## <font color=green>2 Labels - imbalanced data<\/font> ","04c8c700":"Define some helpful parameters:","ce337535":"for multiclass, random forest classifier performs better than logistic regresstion, with feature selection (Threshold = 0.2) having the best test score.","f64a3540":"Drop columns that don't contibute to the household data","ed3fef6c":"## <font color=blue>4 labels of imbalanced data<\/font>","d3259cbc":"## <font color=blue>4 labels of imbalanced data<\/font>","b944630c":"We will find the labels of the heads of houshold:","d06441e4":"We note some inconsistencies with houshole lables. <br>\nSome households have more than one lable. In those cases the lable of the head of the household label for the houshold","6880d0c3":"The data for this project is taken from a Kaggle competition for poverty levels prediction of Costa Rican housholds, hosted by the The Inter-American Development Bank: <br>\nhttps:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction\n<br>\n<br>\nThe training data conatains 9558 rows, each row represting a person. There are 143 columns, including the person ID, its household identifer and the target columns (our labels). Some of the features refer to the person and some are aggregated per houshold. \n<br>\nFor example: \nAge, is male\/female if owns a tablet and number of years of education refer to person. <br>\nNumber people under 12, rent, the matirial the house is made of and the region -  refer to a household. <br>\n<br>\nThe prediction should be on the houshold level, i.e. all the persons under the same household should have the same poverty level.\n<br>\nThe poverty level are:<br>\n1 = extreme poverty <br>\n2 = moderate poverty <br>\n3 = vulnerable households <br>\n4 = non vulnerable households <br>\n","8ef0fca9":"As mentioned, the data should be aggregated to houshold level, since the prediction should be per household.<br>\nSo we explored each column to understand the data range and understand how to aggregate.","a2c27e45":"Again, using all features or extracting with PCA seem like equal methods, combined with logistic regression as classifier, with C=1, and they preform the best on the test data. ","d8619aa2":"We note issues in calculating dependency. The dependency in the data was not according to the promised calculations, so we calculated our own dependecy columns, based on the age data of the members of each housholds.","1c405044":"For Each dataset we have a all the best estimators from each type of pipeline. we will compare them to each other on the test data.","06459bf0":"Split the data to train and test","9b5c6679":"How the target lables are split in the data:","330920f7":"Join the new target labels into the data_clean table","9d692474":"## <font color=red>2 Labels - balanced data<\/font> ","426af676":"**Helpful functions:**","cfbe370a":"# Conclusions","14b6449f":"Since we need to lable in the household label, we look at the target labels groupd by houshold","8a4ad8e0":"### Overview","3975eed8":"## Costa Rican Household Poverty Level Prediction","2ad207ce":"We see that the label ratio is similar on the person and on the household level.","9df58b33":"Most features consist of binary data but some features have int\/float type data that is much larger than 1, so data will be scaled with min-max scaler.<br> \n\nFor each dataset we will run 2 methods of dimensionality reduction:\n* Feature selection with lasso logistic regression\n* Feature extraction with PCA\n  \nand 2 classifiers: \n* Logistic regression\n* Random forest\n \nWe will run a grid search cross validation (CV=10, since the data size is fairly small, using accuracy score for multiclass and roc-auc for the 2 class datasets), with diffrent parameters to find the best classifires for each dataset.<br>\n<br>\n<br>\nWe will also look at logistic regression and random forest calssifiers, with all the features","547a39ea":"The labels of this data are not balanced, but the split makes more sense, since the lables now represent poor (1) and not-poor (0) households <br>\nWe'll split the data to train and test"}}