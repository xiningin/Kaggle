{"cell_type":{"f81d66b9":"code","8a20c796":"code","d1f7d680":"code","d26d3504":"code","cea823fa":"code","e84efd68":"code","b8866374":"code","0a5898c1":"code","a14eb3b5":"code","e27d5517":"code","96ac2d71":"code","73d22340":"code","f3bd067b":"code","5b3c1f40":"code","378ab2cb":"markdown","3b103cbc":"markdown","bd7df30a":"markdown","de76787e":"markdown","86bdeedc":"markdown","285ae123":"markdown","6fba8883":"markdown","4193ea89":"markdown","1fbad31e":"markdown","9e76112c":"markdown","b0a588fb":"markdown","0a406c23":"markdown"},"source":{"f81d66b9":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, balanced_accuracy_score, roc_auc_score, log_loss\nimport keras\nfrom keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D, Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Sequential","8a20c796":"train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ny = train['label'].values\nX = train.drop(['label'], axis=1).values","d1f7d680":"X.shape","d26d3504":"y.shape","cea823fa":"def normalise_data(x):\n    return x \/ 255.0","e84efd68":"def reshape(x, model='cnn'):\n    if model == 'mlp':\n        return x.reshape(x.shape[0], -1)  # -1 flattens rest of dimensions: no change\n    else:\n        # model == 'cnn'\n        return x.reshape(x.shape[0], 28, 28, 1)  # 1 for greyscale, 3 for rgb","b8866374":"def one_hot(y):\n    return keras.utils.to_categorical(y, np.max(y) + 1)","0a5898c1":"def model_performance(model, x_train, x_test, y_test):\n    predictions = model.predict(x_test)  # same as predict_proba in softmax output\n    y_pred = np.argmax(np.round(predictions), axis=1)\n    y_test_og = np.argmax(y_test, axis=1)\n\n    accuracy = accuracy_score(y_test_og, y_pred)\n    balanced_accuracy = balanced_accuracy_score(y_test_og, y_pred)\n    # auc = roc_auc_score(y_test_og, predictions)  # need 1-vs-all approach for auc-roc curve\n    loss = log_loss(y_test_og, predictions)\n    report = classification_report(y_test_og, y_pred)\n    matrix = confusion_matrix(y_test_og, y_pred)\n\n    tp = sum(np.diagonal(matrix))\n    fp = np.sum(matrix, axis=0) - tp\n    tn = 0  # must be computed per class\n    fn = np.sum(matrix, axis=1) - tp\n\n    print(f'training cases={x_train.shape[0]}, test cases={y_test.shape[0]}, possible outcomes={y_test.shape[1]}')\n    print(f'accuracy={accuracy:.2f}%, balanced_accuracy={balanced_accuracy:.2f}%, loss={loss:.3f}')\n    # print(f'auc={auc:.3f}')\n    print(report)","a14eb3b5":"def mlp(x, y):\n    \"\"\"Multilayer Perceptron\"\"\"\n    # Initialise MLP\n    # Input layer with nodes=number of features in the dataset\n    model = Sequential()\n    # Hidden layer, one hidden layer is sufficient for the large majority of problems\n    model.add(Dense(512, activation='relu', input_shape=(x.shape[1],)))\n    model.add(Dropout(0.2))  # apply dropout to input, randomly setting a fraction rate of input units to 0 at each\n    # update during training time, which helps prevent overfitting\n    # Hidden layer, size between input and output layers\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.2))\n    # Output layer, one node unless 'softmax' in multi-class problems\n    model.add(Dense(y.shape[1], activation='softmax'))\n    # Compile\n    model.compile(loss=keras.losses.categorical_crossentropy,  # 'sparse_categorical_crossentropy' doesn't require oh\n                  optimizer=keras.optimizers.RMSprop(),\n                  metrics=['accuracy'])  # more metric history available https:\/\/keras.io\/metrics\/\n    return model","e27d5517":"def cnn(x, y):\n    \"\"\"Convolutional Neural Network\"\"\"\n    # Initialise CNN\n    # Input layer\n    model = Sequential()\n    # Hidden layer\n    model.add(Conv2D(64, (3, 3), input_shape=(x.shape[1], x.shape[2], 1)))  # 64 filters (output space), 3x3 convolution\n    # BatchNormalization() aids with overfitting, according to authors and Andrew Ng it should be applied immediately\n    # before activation function (non-linearity)\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))  # rectified linear unit (fire or not)\n    model.add(MaxPooling2D(pool_size=(2, 2)))  # maximum value for each patch on feature map reduced by 2x2 pool_size\n    # Hidden layer\n    model.add(Conv2D(64, (3, 3)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    # Output Layer\n    model.add(Flatten())  # flattens the input\n    model.add(Dense(64))  # regular densely connected NN layer, no activation function means linear activation\n    model.add(Dense(y.shape[1]))  # can also do e.g. model.add(Dense(64, activation='tanh'))\n    model.add(BatchNormalization())\n    model.add(Activation('softmax'))  # softmax activation function as output, turns into weights that sum to 1\n    # Compile\n    model.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer=keras.optimizers.Adam(),\n                  metrics=['accuracy'])\n    return model","96ac2d71":"def plot_model_history(metric):\n    plt.figure()\n    plt.plot(history.history[metric])\n    plt.plot(history.history[f'val_{metric}'])\n    plt.title(f'model {metric}')\n    plt.ylabel(f'{metric}')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","73d22340":"ml_model = cnn","f3bd067b":"# Load the train test data\ntrain = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ny = train['label'].values\nX = train.drop(['label'], axis=1).values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=38)\n\n# Normalise and shape data\nX_train, X_test = normalise_data(X_train), normalise_data(X_test)\nX_train, X_test = reshape(X_train, ml_model.__name__), reshape(X_test, ml_model.__name__)\ny_train, y_test = one_hot(y_train), one_hot(y_test)\n\n# Model\nmodel = ml_model(X_train, y_train)\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=64, epochs=10)\n\n# Validate\nmodel.summary()\ntest_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=64, verbose=1)\nprint(f'test loss={test_loss}, test accuracy={test_accuracy}')\nplot_model_history('loss')\nplot_model_history('accuracy')\nmodel_performance(model, X_train, X_test, y_test)","5b3c1f40":"# Load and manipulate data\nx_test = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\nx_test = x_test.values\nx_test = normalise_data(x_test)\nx_test = reshape(x_test, ml_model.__name__)\n\n# Predict\ny_test = model.predict(x_test)\n\n# Manipulate submission format\nsubmission = pd.DataFrame({'Label': np.argmax(y_test, axis=1)})\nsubmission['ImageId'] = submission.index + 1\nsubmission = submission[['ImageId', 'Label']]\nsubmission.to_csv('\/kaggle\/working\/digit-recognizer-submission.csv', index=False)","378ab2cb":"### CNN (Convolutional Neural Network)","3b103cbc":"## Runner code","bd7df30a":"## Fit to competition data\n\nFirst retrain with 95% of training data (leave 5% to test)","de76787e":"Image created using keras.utils import plot_model and pydot.\n\n![mlp.png](attachment:mlp.png)","86bdeedc":"**MLP (Multilayer Perceptron)**\n\nOverfitting is a common issue in deep learning models and despite using Dropout layers this is particularly evident in the loss plot. To minimise this the dropout should be increased or regularization layers introduced.\n\n![mlp_accuracy.png](attachment:mlp_accuracy.png)\n\n**CNN (Convolutional Neural Network)**\n\nThe CNN outperforms the MLP without overfitting. BatchNormalization layers are included when training with more data.\n\n![cnn_accuracy.png](attachment:cnn_accuracy.png)","285ae123":"# MNIST Digits \n\nSelect whether to run a simple MLP or CNN network. The main code is the runner code near the bottom of the kernel.\n\n- Quick exploration of digit data\n- Use Keras to classify the digits\n- MLP architecture\n- CNN architecture","6fba8883":"## Define model to use","4193ea89":"## Define functions","1fbad31e":"## Quick data shape exploration","9e76112c":"### MLP (Multilayer Perceptron)","b0a588fb":"This final CNN achieves ~98%, to improve the final accuracy we can expand the dataset via data augmentation e.g. image shifting, rotation, scaling, flipping, colour changing.","0a406c23":"Image created using keras.utils import plot_model and pydot (excludes BatchNormalization layers). \n\n![cnn.png](attachment:cnn.png)"}}