{"cell_type":{"7e5eeb4f":"code","72c027e2":"code","a2b6a668":"code","00eae460":"code","4d0603d9":"code","64de00ce":"code","44e81623":"code","4cfaa43f":"code","80181d18":"code","fb239a0f":"code","96e8533e":"code","707fb1a5":"code","2bf4d7ee":"code","9b428714":"code","9a8eb1e0":"code","5e504840":"code","1139b599":"code","60eef41d":"code","0612cc45":"code","8d1f7d77":"code","8bb923dd":"markdown","c30fff63":"markdown"},"source":{"7e5eeb4f":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\n\nfrom sklearn import svm, neighbors, linear_model, neural_network\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA, TruncatedSVD, KernelPCA\n\nfrom sklearn.mixture import GaussianMixture as GMM\nfrom sklearn.metrics import silhouette_score\n\n\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn import preprocessing\nfrom sklearn import svm, neighbors, linear_model\nimport gc\nwarnings.filterwarnings('ignore')\n\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import Matern, RationalQuadratic\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.decomposition import FastICA, TruncatedSVD, PCA\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cat\n\nfrom tqdm import *","72c027e2":"%%time\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","a2b6a668":"train_columns = [c for c in train_df.columns if c not in ['id','target','wheezy-copper-turtle-magic']]\n\nmagic_variance_over2 = {}\nfor magic in sorted(train_df['wheezy-copper-turtle-magic'].unique()):\n    temp = train_df.loc[train_df['wheezy-copper-turtle-magic']==magic]\n    std = temp[train_columns].std()\n    magic_variance_over2[magic] = list(std.index.values[np.where(std >2)])","00eae460":"class hist_model(object):\n    \n    def __init__(self, bins=50):\n        self.bins = bins\n        \n    def fit(self, X):\n        \n        bin_hight, bin_edge = [], []\n        \n        for var in X.T:\n            # get bins hight and interval\n            bh, bedge = np.histogram(var, bins=self.bins)\n            bin_hight.append(bh)\n            bin_edge.append(bedge)\n        \n        self.bin_hight = np.array(bin_hight)\n        self.bin_edge = np.array(bin_edge)\n   \n\n    def predict(self, X):\n        \n        scores = []\n        for obs in X:\n            obs_score = []\n            for i, var in enumerate(obs):\n                # find wich bin obs is in\n                bin_num = (var > self.bin_edge[i]).argmin()-1\n                obs_score.append(self.bin_hight[i, bin_num]) # find bin hitght\n            \n            scores.append(np.mean(obs_score))\n        \n        return np.array(scores)","4d0603d9":"from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import Lasso, LassoLars","64de00ce":"random_state = 42\ndebug = True\ndebug = False","44e81623":"svnu_params = {'probability':True, 'kernel':'poly','degree':4,'gamma':'auto','nu':0.4,'coef0':0.08, 'random_state':4}\nsvnu2_params = {'probability':True, 'kernel':'poly','degree':2,'gamma':'auto','nu':0.4,'coef0':0.08, 'random_state':4}\nsvc_params = {'probability':True,'kernel':'poly','degree':4,'gamma':'auto', 'random_state':4}\nlr_params = {'solver':'liblinear','penalty':'l1','C':0.05,'n_jobs':-1, 'random_state':42}\nmlp16_params = {'activation':'relu','solver':'lbfgs','tol':1e-06, 'hidden_layer_sizes':(16, ), 'random_state':42}\nmlp128_params = {'activation':'relu','solver':'lbfgs','tol':1e-06, 'hidden_layer_sizes':(128, ), 'random_state':42}\ngnb_params = {}","4cfaa43f":"def get_oofs(random_state):\n    oof_nusvc = np.zeros(len(train_df))\n    preds_nusvc = np.zeros(len(test_df))\n\n    oof_nusvc2 = np.zeros(len(train_df))\n    preds_nusvc2 = np.zeros(len(test_df))\n\n    oof_qda = np.zeros(len(train_df))\n    preds_qda = np.zeros(len(test_df))\n\n    oof_svc = np.zeros(len(train_df))\n    preds_svc = np.zeros(len(test_df))\n    \n    oof_knn = np.zeros(len(train_df))\n    preds_knn = np.zeros(len(test_df))\n    \n    oof_lr = np.zeros(len(train_df))\n    preds_lr = np.zeros(len(test_df))\n    \n    oof_gnb = np.zeros(len(train_df))\n    preds_gnb = np.zeros(len(test_df))\n    \n    cols = [c for c in train_df.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\n\n    for i in tqdm_notebook(range(512)):\n\n        # each magic\n        train = train_df[train_df['wheezy-copper-turtle-magic'] == i]\n        test = test_df[test_df['wheezy-copper-turtle-magic'] == i]\n\n        # for oof\n        train_idx_origin = train.index\n        test_idx_origin = test.index\n\n\n        # start point\n\n        # new cols\n        cols = magic_variance_over2[i]\n\n        X_train = train.reset_index(drop=True)[cols].values\n        y_train = train.reset_index(drop=True).target\n\n        X_test = test[cols].values\n\n        # vstack\n        data = np.vstack([X_train, X_test])\n        \n        # PCA\n        data = KernelPCA(n_components=len(cols), kernel='cosine', random_state=random_state).fit_transform(data)\n        \n        # Bad\n        '''\n        gmm_pred = np.zeros((len(data), 5))\n        for j in range(5):\n            gmm = GMM(n_components=4, random_state=random_state + j, max_iter=1000).fit(data)\n            gmm_pred[:, j] += gmm.predict(data)\n        '''\n          \n        # original\n        gmm = GMM(n_components=5, random_state=random_state, max_iter=1000).fit(data)\n        gmm_pred = gmm.predict_proba(data)\n        gmm_score = gmm.score_samples(data)\n        gmm_label = gmm.predict(data)\n        \n        hist = hist_model(); hist.fit(data)\n        hist_pred = hist.predict(data).reshape(-1, 1)\n\n        data = np.hstack([data, gmm_pred])\n\n        # HOXI\n        data = np.hstack([data, gmm_pred])\n        data = np.hstack([data, gmm_pred])\n        data = np.hstack([data, gmm_pred])\n        \n        # Add Some Features\n        data = np.hstack([data, gmm_pred])\n        data = np.hstack([data, hist_pred, gmm_score.reshape(-1, 1)])\n        data = np.hstack([data, gmm_score.reshape(-1, 1)])\n        data = np.hstack([data, gmm_score.reshape(-1, 1)])\n\n        # STANDARD SCALER\n        data = StandardScaler().fit_transform(data)\n\n        # new train\/test\n        X_train = data[:X_train.shape[0]]\n        X_test = data[X_train.shape[0]:]\n\n        fold = StratifiedKFold(n_splits=5, random_state=random_state)\n        for tr_idx, val_idx in fold.split(X_train, gmm_label[:X_train.shape[0]]):\n            \n            # NuSVC 1\n            clf = svm.NuSVC(**svnu_params)\n            clf.fit(X_train[tr_idx], y_train[tr_idx])\n            oof_nusvc[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n            preds_nusvc[test_idx_origin] += clf.predict_proba(X_test)[:,1] \/ fold.n_splits\n\n            # NuSVC 2\n            clf = svm.NuSVC(**svnu2_params)\n            clf.fit(X_train[tr_idx], y_train[tr_idx])\n            oof_nusvc2[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n            preds_nusvc2[test_idx_origin] += clf.predict_proba(X_test)[:,1] \/ fold.n_splits\n\n\n            # qda 3\n            clf = QuadraticDiscriminantAnalysis(reg_param=0.111)\n            clf.fit(X_train[tr_idx], y_train[tr_idx])\n            oof_qda[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n            preds_qda[test_idx_origin] += clf.predict_proba(X_test)[:,1] \/ fold.n_splits\n\n            # SVC 4\n            clf = svm.SVC(**svc_params)\n            clf.fit(X_train[tr_idx], y_train[tr_idx])\n            oof_svc[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n            preds_svc[test_idx_origin] += clf.predict_proba(X_test)[:,1] \/ fold.n_splits\n            \n            # knn 8\n            clf = KNeighborsClassifier(n_neighbors=16)\n            clf.fit(X_train[tr_idx], y_train[tr_idx])\n            oof_knn[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n            preds_knn[test_idx_origin] += clf.predict_proba(X_test)[:,1] \/ fold.n_splits   \n            \n            # LR 5\n            clf = linear_model.LogisticRegression(**lr_params)\n            clf.fit(X_train[tr_idx], y_train[tr_idx])\n            oof_lr[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n            preds_lr[test_idx_origin] += clf.predict_proba(X_test)[:,1] \/ fold.n_splits\n            \n            # GNB\n            #clf = GaussianNB(**gnb_params)\n            #clf.fit(X_train[tr_idx], y_train[tr_idx])\n            #oof_gnb[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n            #preds_gnb[test_idx_origin] += clf.predict_proba(X_test)[:,1] \/ fold.n_splits\n            \n    oof_train = pd.DataFrame()\n\n    oof_train['nusvc'] = oof_nusvc\n    oof_train['nusvc2'] = oof_nusvc2\n    oof_train['qda'] = oof_qda\n    oof_train['svc'] = oof_svc\n    oof_train['knn'] = oof_knn\n    oof_train['lr'] = oof_lr\n    #oof_train['gnb'] = oof_gnb\n    \n    oof_test = pd.DataFrame()\n\n    oof_test['nusvc'] = preds_nusvc\n    oof_test['nusvc2'] = preds_nusvc2\n    oof_test['qda'] = preds_qda\n    oof_test['svc'] = preds_svc\n    oof_test['knn'] = preds_knn\n    oof_test['lr'] = preds_lr\n    #oof_test['gnb'] = preds_gnb\n\n    print('nusvc', roc_auc_score(train_df['target'], oof_nusvc))\n    print('nusvc2', roc_auc_score(train_df['target'], oof_nusvc2))\n    print('qda', roc_auc_score(train_df['target'], oof_qda))\n    print('svc', roc_auc_score(train_df['target'], oof_svc))\n    print('knn', roc_auc_score(train_df['target'], oof_knn))\n    print('lr', roc_auc_score(train_df['target'], oof_lr))\n    #print('gnb', roc_auc_score(train_df['target'], oof_gnb))\n    \n    return oof_train, oof_test","80181d18":"def get_oofs_2(random_state):\n    oof_nusvc = np.zeros(len(train_df))\n    preds_nusvc = np.zeros(len(test_df))\n\n    oof_nusvc2 = np.zeros(len(train_df))\n    preds_nusvc2 = np.zeros(len(test_df))\n\n    oof_qda = np.zeros(len(train_df))\n    preds_qda = np.zeros(len(test_df))\n\n    oof_svc = np.zeros(len(train_df))\n    preds_svc = np.zeros(len(test_df))\n    \n    oof_knn = np.zeros(len(train_df))\n    preds_knn = np.zeros(len(test_df))\n\n    oof_lr = np.zeros(len(train_df))\n    preds_lr = np.zeros(len(test_df))\n \n    oof_gnb = np.zeros(len(train_df))\n    preds_gnb = np.zeros(len(test_df))\n    \n    cols = [c for c in train_df.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\n\n    for i in tqdm_notebook(range(512)):\n\n        # each magic\n        train = train_df[train_df['wheezy-copper-turtle-magic'] == i]\n        test = test_df[test_df['wheezy-copper-turtle-magic'] == i]\n\n        # for oof\n        train_idx_origin = train.index\n        test_idx_origin = test.index\n\n\n        # start point\n\n        # new cols\n        cols = magic_variance_over2[i]\n\n        X_train = train.reset_index(drop=True)[cols].values\n        y_train = train.reset_index(drop=True).target\n\n        X_test = test[cols].values\n\n        # vstack\n        data = np.vstack([X_train, X_test])\n\n        # PCA\n        data = KernelPCA(n_components=len(cols), kernel='cosine', random_state=random_state).fit_transform(data)\n\n        # Bad\n        '''\n        gmm_pred = np.zeros((len(data), 5))\n        for j in range(5):\n            gmm = GMM(n_components=4, random_state=random_state + j, max_iter=1000).fit(data)\n            gmm_pred[:, j] += gmm.predict(data)\n        '''\n            \n        # original\n        gmm = GMM(n_components=5, random_state=random_state, max_iter=1000, init_params='random').fit(data)\n        gmm_pred = gmm.predict_proba(data)\n        gmm_score = gmm.score_samples(data)\n        gmm_label = gmm.predict(data)\n        \n        hist = hist_model(); hist.fit(data)\n        hist_pred = hist.predict(data).reshape(-1, 1)\n\n        data = np.hstack([data, gmm_pred])\n        \n        # HOXI\n        data = np.hstack([data, gmm_pred])\n        data = np.hstack([data, gmm_pred])\n        data = np.hstack([data, gmm_pred])\n        \n        # Add Some Features\n        data = np.hstack([data, gmm_pred])\n        data = np.hstack([data, hist_pred, gmm_score.reshape(-1, 1)])\n        data = np.hstack([data, gmm_score.reshape(-1, 1)])\n        data = np.hstack([data, gmm_score.reshape(-1, 1)])\n\n        # STANDARD SCALER\n        data = StandardScaler().fit_transform(data)\n\n        # new train\/test\n        X_train = data[:X_train.shape[0]]\n        X_test = data[X_train.shape[0]:]\n\n        fold = StratifiedKFold(n_splits=5, random_state=random_state)\n        for tr_idx, val_idx in fold.split(X_train, gmm_label[:X_train.shape[0]]):\n            \n            # NuSVC 1\n            clf = svm.NuSVC(**svnu_params)\n            clf.fit(X_train[tr_idx], y_train[tr_idx])\n            oof_nusvc[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n            preds_nusvc[test_idx_origin] += clf.predict_proba(X_test)[:,1] \/ fold.n_splits\n\n            # NuSVC 2\n            clf = svm.NuSVC(**svnu2_params)\n            clf.fit(X_train[tr_idx], y_train[tr_idx])\n            oof_nusvc2[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n            preds_nusvc2[test_idx_origin] += clf.predict_proba(X_test)[:,1] \/ fold.n_splits\n\n\n            # qda 3\n            clf = QuadraticDiscriminantAnalysis(reg_param=0.111)\n            clf.fit(X_train[tr_idx], y_train[tr_idx])\n            oof_qda[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n            preds_qda[test_idx_origin] += clf.predict_proba(X_test)[:,1] \/ fold.n_splits\n\n            # SVC 4\n            clf = svm.SVC(**svc_params)\n            clf.fit(X_train[tr_idx], y_train[tr_idx])\n            oof_svc[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n            preds_svc[test_idx_origin] += clf.predict_proba(X_test)[:,1] \/ fold.n_splits\n            \n            # knn 8\n            clf = KNeighborsClassifier(n_neighbors=16)\n            clf.fit(X_train[tr_idx], y_train[tr_idx])\n            oof_knn[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n            preds_knn[test_idx_origin] += clf.predict_proba(X_test)[:,1] \/ fold.n_splits   \n            \n            # LR 5\n            clf = linear_model.LogisticRegression(**lr_params)\n            clf.fit(X_train[tr_idx], y_train[tr_idx])\n            oof_lr[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n            preds_lr[test_idx_origin] += clf.predict_proba(X_test)[:,1] \/ fold.n_splits\n            \n            # GNB\n            #clf = GaussianNB(**gnb_params)\n            #clf.fit(X_train[tr_idx], y_train[tr_idx])\n            #oof_gnb[train_idx_origin[val_idx]] = clf.predict_proba(X_train[val_idx])[:,1]\n            #preds_gnb[test_idx_origin] += clf.predict_proba(X_test)[:,1] \/ fold.n_splits\n            \n    oof_train = pd.DataFrame()\n\n    oof_train['nusvc'] = oof_nusvc\n    oof_train['nusvc2'] = oof_nusvc2\n    oof_train['qda'] = oof_qda\n    oof_train['svc'] = oof_svc\n    oof_train['knn'] = oof_knn\n    oof_train['lr'] = oof_lr\n    #oof_train['gnb'] = oof_gnb\n    \n    oof_test = pd.DataFrame()\n\n    oof_test['nusvc'] = preds_nusvc\n    oof_test['nusvc2'] = preds_nusvc2\n    oof_test['qda'] = preds_qda\n    oof_test['svc'] = preds_svc\n    oof_test['knn'] = preds_knn\n    oof_test['lr'] = preds_lr\n    #oof_test['gnb'] = preds_gnb\n\n    print('nusvc', roc_auc_score(train_df['target'], oof_nusvc))\n    print('nusvc2', roc_auc_score(train_df['target'], oof_nusvc2))\n    print('qda', roc_auc_score(train_df['target'], oof_qda))\n    print('svc', roc_auc_score(train_df['target'], oof_svc))\n    print('knn', roc_auc_score(train_df['target'], oof_knn))\n    print('lr', roc_auc_score(train_df['target'], oof_lr))\n    print('gnb', roc_auc_score(train_df['target'], oof_gnb))\n    \n    return oof_train, oof_test","fb239a0f":"oof_train_1, oof_test_1 = get_oofs(1)\noof_train_2, oof_test_2 = get_oofs(2)\noof_train_3, oof_test_3 = get_oofs_2(1)\noof_train_4, oof_test_4 = get_oofs_2(2)","96e8533e":"x_train_second_layer = oof_train_1 + oof_train_2 + oof_train_3 + oof_train_4\nx_test_second_layer = oof_test_1 + oof_test_2 + oof_test_3 + oof_test_4\nprint('Ensemble', roc_auc_score(train_df['target'], x_train_second_layer.mean(1)))","707fb1a5":"submit = pd.read_csv('..\/input\/sample_submission.csv')\nsubmit[\"target\"] = x_test_second_layer.mean(1)\nsubmit.to_csv(\"submission0.csv\", index=False)","2bf4d7ee":"def time_decorator(func):\n    \n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        print(\"\\nStartTime: \", datetime.now() + timedelta(hours=9))\n        start_time = time.time()\n        \n        df = func(*args, **kwargs)\n        \n        print(\"EndTime: \", datetime.now() + timedelta(hours=9))  \n        print(\"TotalTime: \", time.time() - start_time)\n        return df\n        \n    return wrapper\n\nclass SklearnWrapper(object):\n    def __init__(self, clf, params=None, **kwargs):\n        \"\"\"\n        params['random_state'] = kwargs.get('seed', 0)\n        self.clf = clf(**params)\n        self.is_classification_problem = True\n        \"\"\"\n        if 'seed' in kwargs:\n            params['random_state'] = kwargs.get('seed', 0)\n        self.clf = clf(**params)\n        self.is_classification_problem = True\n    #@time_decorator\n    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n        if len(np.unique(y_train)) > 30:\n            self.is_classification_problem = False\n            \n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        if self.is_classification_problem is True:\n            return self.clf.predict_proba(x)[:,1]\n        else:\n            return self.clf.predict(x)\n    \nclass LgbmWrapper(object):\n    def __init__(self, params=None, **kwargs):\n        self.param = params\n        if 'seed' in kwargs:\n            self.param['seed'] = kwargs.get('seed', 0)\n        self.num_rounds = kwargs.get('num_rounds', 1000)\n        self.early_stopping = kwargs.get('ealry_stopping', 100)\n\n        self.eval_function = kwargs.get('eval_function', None)\n        self.verbose_eval = kwargs.get('verbose_eval', 100)\n        self.best_round = 0\n        self.feature_importance = pd.DataFrame()\n        \n    #@time_decorator\n    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n        \"\"\"\n        x_cross or y_cross is None\n        -> model train limted num_rounds\n        \n        x_cross and y_cross is Not None\n        -> model train using validation set\n        \"\"\"\n        if isinstance(y_train, pd.DataFrame) is True:\n            y_train = y_train[y_train.columns[0]]\n            if y_cross is not None:\n                y_cross = y_cross[y_cross.columns[0]]\n\n        if x_cross is None:\n            dtrain = lgb.Dataset(x_train, label=y_train, silent= True)\n            train_round = self.best_round\n            if self.best_round == 0:\n                train_round = self.num_rounds\n                \n            self.clf = lgb.train(self.param, train_set=dtrain, num_boost_round=train_round)\n            del dtrain   \n        else:\n            dtrain = lgb.Dataset(x_train, label=y_train, silent=True)\n            dvalid = lgb.Dataset(x_cross, label=y_cross, silent=True)\n            self.clf = lgb.train(self.param, train_set=dtrain, num_boost_round=self.num_rounds, valid_sets=[dtrain, dvalid],\n                                  feval=self.eval_function, early_stopping_rounds=self.early_stopping,\n                                  verbose_eval=self.verbose_eval)\n            \n            try:\n                self.feature_importance = pd.DataFrame()\n                self.feature_importance[\"Feature\"] = x_train.columns\n                self.feature_importance[\"Importance\"] = self.clf.feature_importance()\n            except:\n                pass\n            \n            self.best_round = max(self.best_round, self.clf.best_iteration)\n            del dtrain, dvalid\n            \n        gc.collect()\n    \n    def get_importance_df(self):\n        return self.feature_importance\n    \n    def predict(self, x):\n        return self.clf.predict(x, num_iteration=self.clf.best_iteration)\n    \n    def plot_importance(self):\n        lgb.plot_importance(self.clf, max_num_features=50, height=0.7, figsize=(10,30))\n        plt.show()\n        \n    def get_params(self):\n        return self.param","9b428714":"#@time_decorator\ndef get_oof(clf, x_train, y_train, x_test, eval_func, **kwargs):\n    nfolds = kwargs.get('NFOLDS', 5)\n    kfold_shuffle = kwargs.get('kfold_shuffle', True)\n    kfold_random_state = kwargs.get('kfold_random_state', 0)\n    stratified_kfold_ytrain = kwargs.get('stratifed_kfold_y_value', None)\n    inner_predict = kwargs.get('inner_predict', True)\n    export_feature_importance = kwargs.get('export_feature_importance', True)\n    ntrain = x_train.shape[0]\n    ntest = x_test.shape[0]\n    \n    kf_split = None\n    if stratified_kfold_ytrain is None:\n        kf = KFold(n_splits=nfolds, shuffle=kfold_shuffle, random_state=kfold_random_state)\n        kf_split = kf.split(x_train)\n    else:\n        kf = StratifiedKFold(n_splits=nfolds, shuffle=kfold_shuffle, random_state=kfold_random_state)\n        kf_split = kf.split(x_train, stratified_kfold_ytrain)\n        \n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n\n    cv_sum = 0\n    \n    # before running model, print model param\n    # lightgbm model and xgboost model use get_params()\n    \"\"\"\n    try:\n        if clf.clf is not None:\n            print(clf.clf)\n    except:\n        print(clf)\n        print(clf.get_params())\n    \"\"\"\n    feature_importance_df = pd.DataFrame()\n    for i, (train_index, cross_index) in enumerate(kf_split):\n        x_tr, x_cr = None, None\n        y_tr, y_cr = None, None\n        if isinstance(x_train, pd.DataFrame):\n            x_tr, x_cr = x_train.iloc[train_index], x_train.iloc[cross_index]\n            y_tr, y_cr = y_train.iloc[train_index], y_train.iloc[cross_index]\n        else:\n            x_tr, x_cr = x_train[train_index], x_train[cross_index]\n            y_tr, y_cr = y_train[train_index], y_train[cross_index]\n\n        clf.train(x_tr, y_tr, x_cr, y_cr)\n        \n        if isinstance(clf, LgbmWrapper) is True:\n            feature_importance_df = pd.concat([feature_importance_df, clf.get_importance_df()], axis=0)\n    \n        oof_train[cross_index] = clf.predict(x_cr)\n        if inner_predict is True:\n            oof_test += clf.predict(x_test)\n        \n        cv_score = eval_func(y_cr, oof_train[cross_index])\n        \n        #print('Fold %d \/ ' % (i+1), 'CV-Score: %.6f' % cv_score)\n        cv_sum = cv_sum + cv_score\n        \n        del x_tr, x_cr, y_tr, y_cr\n        \n    gc.collect()\n    \n    score = cv_sum \/ nfolds\n    #print(\"Average CV-Score: \", score)\n    #print(\"OOF CV-Score: \", eval_func(y_train, oof_train))\n    \n    if export_feature_importance is True:\n        print(\"Export Feature Importance\")\n        filename = '{}_cv{:.6f}'.format(datetime.now().strftime('%Y%m%d_%H%M%S'), score)\n        if os.path.isdir(\"importance\/\") is True:\n            feature_importance_df.to_csv('importance\/importance_{}.csv'.format(filename),index=False)\n        else:\n            feature_importance_df.to_csv('importance_{}.csv'.format(filename),index=False)\n            \n    if inner_predict is True:\n        oof_test = oof_test\/nfolds\n    else:\n        # Using All Dataset, retrain\n        clf.train(x_train, y_train)\n        oof_test = clf.predict(x_test)\n\n    return oof_train, oof_test, score","9a8eb1e0":"x_train_second_layer1 = pd.DataFrame(x_train_second_layer)\nx_test_second_layer1 = pd.DataFrame(x_test_second_layer)","5e504840":"import time\nfrom datetime import datetime, timedelta,date\nimport warnings\nimport itertools\nfrom functools import wraps\nimport functools\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn import preprocessing\nfrom sklearn import svm, neighbors, linear_model\nimport gc\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.mixture import GaussianMixture as GMM","1139b599":"SEED = 0\n\nparam = {\n        #'bagging_freq': 5,\n        #'bagging_fraction': 0.8,\n        'min_child_weight':6.790,\n        \"subsample_for_bin\":50000,\n        'bagging_seed': 0,\n        'boost_from_average':'true',\n        'boost': 'gbdt',\n        'feature_fraction': 0.450,\n        'bagging_fraction': 0.343,\n        'learning_rate': 0.025,\n        'max_depth': 10,\n        'metric':'auc',\n        'min_data_in_leaf': 78,\n        'min_sum_hessian_in_leaf': 8, \n        'num_leaves': 18,\n        'num_threads': 8,\n        'tree_learner': 'serial',\n        'objective': 'binary', \n        'verbosity': 1,\n        'lambda_l1': 7.961,\n        'lambda_l2': 7.781\n        #'reg_lambda': 0.3,\n    }\n\nmlp16_params = {'activation':'relu','solver':'lbfgs','tol':1e-06, 'hidden_layer_sizes':(16, ), 'random_state':42}\nknn_params ={'n_neighbors':17, 'p':2.9,'n_jobs':-1}\n\nlgbm_meta_model = LgbmWrapper(params=param, num_rounds = 2000, ealry_stopping=100)\nmlp_meta_model = SklearnWrapper(neural_network.MLPClassifier,mlp16_params)\nknn_meta_model = SklearnWrapper(neighbors.KNeighborsClassifier,knn_params)\n\n\nthird_number = 4\noof_train_5 = pd.DataFrame()\noof_test_5 = pd.DataFrame()\n\n# lgbm\nthird_oof = np.zeros(len(train_df))\nthird_pred = np.zeros(len(test_df))\n\n# mlp\nthird_oof1 = np.zeros(len(train_df))\nthird_pred1 = np.zeros(len(test_df))\n\n# knn\nthird_oof2 = np.zeros(len(train_df))\nthird_pred2 = np.zeros(len(test_df))\n\nthird_oof3 = np.zeros(len(train_df))\nthird_pred3 = np.zeros(len(test_df))\n\n#for SEED in np.arange(third_number):\nsecond_oof, second_pred, second_score = get_oof(lgbm_meta_model, x_train_second_layer1, train_df['target'], x_test_second_layer1, eval_func=roc_auc_score, NFOLDS=5, kfold_random_sate= SEED )\nsecond_oof1, second_pred1, second_score1 = get_oof(mlp_meta_model, x_train_second_layer1, train_df['target'], x_test_second_layer1, eval_func=roc_auc_score, NFOLDS=5, kfold_random_sate= SEED )\nsecond_oof2, second_pred2, second_score2 = get_oof(knn_meta_model, x_train_second_layer1, train_df['target'], x_test_second_layer1, eval_func=roc_auc_score, NFOLDS=5, kfold_random_sate= SEED )\n\nthird_oof += second_oof\nthird_pred += second_pred\nprint(second_score)\nthird_oof1 += second_oof1\nthird_pred1 += second_pred1\nprint(second_score1)\nthird_oof2 += second_oof2\nthird_pred2 += second_pred2\nprint(second_score2)\nprint(\"\")\n    \noof_train_5['lgb'] = third_oof * 4\noof_test_5['lgb'] = third_pred * 4\noof_train_5['mlp'] = third_oof1 * 4\noof_test_5['mlp'] = third_pred1 * 4\noof_train_5['knn'] = third_oof2 * 4\noof_test_5['knn'] = third_pred2 * 4","60eef41d":"x_train_second_layer = oof_train_1 + oof_train_2 + oof_train_3 + oof_train_4  \nx_test_second_layer = oof_test_1 + oof_test_2 + oof_test_3 + oof_test_4 \n\nx_train_second_layer = pd.concat([x_train_second_layer,oof_train_5],axis=1)\nx_test_second_layer = pd.concat([x_test_second_layer,oof_test_5],axis=1)\n                                     \nprint('Ensemble', roc_auc_score(train_df['target'], x_train_second_layer.mean(1)))","0612cc45":"x_train_second_layer.corr()","8d1f7d77":"submit = pd.read_csv('..\/input\/sample_submission.csv')\nsubmit[\"target\"] = x_test_second_layer.mean(1)\nsubmit.to_csv(\"submission.csv\", index=False)","8bb923dd":"\uc544\ub798 SEED \uc218\uc815 \ub41c \uac70\ub85c \ubc14\uafd4\uc57c \ud568. ","c30fff63":"- V1 : LGBM STACKING \n- V2 : LGBM, MLP16 STACKING\n- V3 : V2 + pred 5 score 3\n- v4 : v3 + gnb"}}