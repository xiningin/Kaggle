{"cell_type":{"1f05838b":"code","5a71a480":"code","dbdf9ba9":"code","f18dcc44":"code","4bb41ca6":"code","967a8a25":"code","01791f3c":"code","f9fa6a98":"code","a38b8fc9":"code","d2151008":"code","fba9adae":"code","ae4db753":"code","dd1c6636":"code","0cc320d9":"code","e0fbcb9e":"code","4e24213d":"code","1ca4d902":"code","2eb5cd51":"markdown","57282cd6":"markdown","0821b534":"markdown","8b103af3":"markdown","df2aa9c0":"markdown","bed0691a":"markdown","27917d5c":"markdown","b42a8ba4":"markdown","6fbd5183":"markdown","a2e4ee1f":"markdown","6cffde45":"markdown","f6a24165":"markdown","c8f25878":"markdown","9ba7c7ee":"markdown","8dd0c7ba":"markdown","12c9fab8":"markdown","8817edce":"markdown","5ee53a4c":"markdown","408fc641":"markdown"},"source":{"1f05838b":"## Keras for deep learning\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers import Bidirectional\nfrom keras.models import Sequential\n\n## Scikit learn for mapping metrics\nfrom sklearn.metrics import mean_squared_error\n\n#for logging\nimport time\n\n##matrix math\nimport numpy as np\nimport math\n\n##plotting\nimport matplotlib.pyplot as plt\n\n##data processing\nimport pandas as pd","5a71a480":"def load_data(filename, sequence_length):\n    \"\"\"\n    Loads the bitcoin data\n    \n    Arguments:\n    filename -- A string that represents where the .csv file can be located\n    sequence_length -- An integer of how many days should be looked at in a row\n    \n    Returns:\n    X_train -- A tensor of shape (2400, 49, 35) that will be inputed into the model to train it\n    Y_train -- A tensor of shape (2400,) that will be inputed into the model to train it\n    X_test -- A tensor of shape (267, 49, 35) that will be used to test the model's proficiency\n    Y_test -- A tensor of shape (267,) that will be used to check the model's predictions\n    Y_daybefore -- A tensor of shape (267,) that represents the price of bitcoin the day before each Y_test value\n    unnormalized_bases -- A tensor of shape (267,) that will be used to get the true prices from the normalized ones\n    window_size -- An integer that represents how many days of X values the model can look at at once\n    \"\"\"\n    #Read the data file\n    raw_data = pd.read_csv(filename, dtype = float).values\n    \n    #Change all zeros to the number before the zero occurs\n    for x in range(0, raw_data.shape[0]):\n        for y in range(0, raw_data.shape[1]):\n            if(raw_data[x][y] == 0):\n                raw_data[x][y] = raw_data[x-1][y]\n    \n    #Convert the file to a list\n    data = raw_data.tolist()\n    \n    #Convert the data to a 3D array (a x b x c) \n    #Where a is the number of days, b is the window size, and c is the number of features in the data file\n    result = []\n    for index in range(len(data) - sequence_length):\n        result.append(data[index: index + sequence_length])\n    \n    #Normalizing data by going through each window\n    #Every value in the window is divided by the first value in the window, and then 1 is subtracted\n    d0 = np.array(result)\n    dr = np.zeros_like(d0)\n    dr[:,1:,:] = d0[:,1:,:] \/ d0[:,0:1,:] - 1\n    \n    #Keeping the unnormalized prices for Y_test\n    #Useful when graphing bitcoin price over time later\n    start = 2400\n    end = int(dr.shape[0] + 1)\n    unnormalized_bases = d0[start:end,0:1,20]\n    \n    #Splitting data set into training (First 90% of data points) and testing data (last 10% of data points)\n    split_line = round(0.9 * dr.shape[0])\n    training_data = dr[:int(split_line), :]\n    \n    #Shuffle the data\n    np.random.shuffle(training_data)\n    \n    #Training Data\n    X_train = training_data[:, :-1]\n    Y_train = training_data[:, -1]\n    Y_train = Y_train[:, 20]\n    \n    #Testing data\n    X_test = dr[int(split_line):, :-1]\n    Y_test = dr[int(split_line):, 49, :]\n    Y_test = Y_test[:, 20]\n\n    #Get the day before Y_test's price\n    Y_daybefore = dr[int(split_line):, 48, :]\n    Y_daybefore = Y_daybefore[:, 20]\n    \n    #Get window size and sequence length\n    sequence_length = sequence_length\n    window_size = sequence_length - 1 #because the last value is reserved as the y value\n    \n    return X_train, Y_train, X_test, Y_test, Y_daybefore, unnormalized_bases, window_size","dbdf9ba9":"def initialize_model(window_size, dropout_value, activation_function, loss_function, optimizer):\n    \"\"\"\n    Initializes and creates the model to be used\n    \n    Arguments:\n    window_size -- An integer that represents how many days of X_values the model can look at at once\n    dropout_value -- A decimal representing how much dropout should be incorporated at each level, in this case 0.2\n    activation_function -- A string to define the activation_function, in this case it is linear\n    loss_function -- A string to define the loss function to be used, in the case it is mean squared error\n    optimizer -- A string to define the optimizer to be used, in the case it is adam\n    \n    Returns:\n    model -- A 3 layer RNN with 100*dropout_value dropout in each layer that uses activation_function as its activation\n             function, loss_function as its loss function, and optimizer as its optimizer\n    \"\"\"\n    #Create a Sequential model using Keras\n    model = Sequential()\n\n    #First recurrent layer with dropout\n    model.add(Bidirectional(LSTM(window_size, return_sequences=True), input_shape=(window_size, X_train.shape[-1]),))\n    model.add(Dropout(dropout_value))\n\n    #Second recurrent layer with dropout\n    model.add(Bidirectional(LSTM((window_size*2), return_sequences=True)))\n    model.add(Dropout(dropout_value))\n\n    #Third recurrent layer\n    model.add(Bidirectional(LSTM(window_size, return_sequences=False)))\n\n    #Output layer (returns the predicted value)\n    model.add(Dense(units=1))\n    \n    #Set activation function\n    model.add(Activation(activation_function))\n\n    #Set loss function and optimizer\n    model.compile(loss=loss_function, optimizer=optimizer)\n    \n    return model","f18dcc44":"def fit_model(model, X_train, Y_train, batch_num, num_epoch, val_split):\n    \"\"\"\n    Fits the model to the training data\n    \n    Arguments:\n    model -- The previously initalized 3 layer Recurrent Neural Network\n    X_train -- A tensor of shape (2400, 49, 35) that represents the x values of the training data\n    Y_train -- A tensor of shape (2400,) that represents the y values of the training data\n    batch_num -- An integer representing the batch size to be used, in this case 1024\n    num_epoch -- An integer defining the number of epochs to be run, in this case 100\n    val_split -- A decimal representing the proportion of training data to be used as validation data\n    \n    Returns:\n    model -- The 3 layer Recurrent Neural Network that has been fitted to the training data\n    training_time -- An integer representing the amount of time (in seconds) that the model was training\n    \"\"\"\n    #Record the time the model starts training\n    start = time.time()\n\n    #Train the model on X_train and Y_train\n    model.fit(X_train, Y_train, batch_size= batch_num, nb_epoch=num_epoch, validation_split= val_split)\n\n    #Get the time it took to train the model (in seconds)\n    training_time = int(math.floor(time.time() - start))\n    return model, training_time","4bb41ca6":"def test_model(model, X_test, Y_test, unnormalized_bases):\n    \"\"\"\n    Test the model on the testing data\n    \n    Arguments:\n    model -- The previously fitted 3 layer Recurrent Neural Network\n    X_test -- A tensor of shape (267, 49, 35) that represents the x values of the testing data\n    Y_test -- A tensor of shape (267,) that represents the y values of the testing data\n    unnormalized_bases -- A tensor of shape (267,) that can be used to get unnormalized data points\n    \n    Returns:\n    y_predict -- A tensor of shape (267,) that represnts the normalized values that the model predicts based on X_test\n    real_y_test -- A tensor of shape (267,) that represents the actual prices of bitcoin throughout the testing period\n    real_y_predict -- A tensor of shape (267,) that represents the model's predicted prices of bitcoin\n    fig -- A branch of the graph of the real predicted prices of bitcoin versus the real prices of bitcoin\n    \"\"\"\n    #Test the model on X_Test\n    y_predict = model.predict(X_test)\n\n    #Create empty 2D arrays to store unnormalized values\n    real_y_test = np.zeros_like(Y_test)\n    real_y_predict = np.zeros_like(y_predict)\n\n    #Fill the 2D arrays with the real value and the predicted value by reversing the normalization process\n    for i in range(Y_test.shape[0]):\n        y = Y_test[i]\n        predict = y_predict[i]\n        real_y_test[i] = (y+1)*unnormalized_bases[i]\n        real_y_predict[i] = (predict+1)*unnormalized_bases[i]\n\n    #Plot of the predicted prices versus the real prices\n    fig = plt.figure(figsize=(10,5))\n    ax = fig.add_subplot(111)\n    ax.set_title(\"Bitcoin Price Over Time\")\n    plt.plot(real_y_predict, color = 'green', label = 'Predicted Price')\n    plt.plot(real_y_test, color = 'red', label = 'Real Price')\n    ax.set_ylabel(\"Price (USD)\")\n    ax.set_xlabel(\"Time (Days)\")\n    ax.legend()\n    \n    return y_predict, real_y_test, real_y_predict, fig","967a8a25":"def price_change(Y_daybefore, Y_test, y_predict):\n    \"\"\"\n    Calculate the percent change between each value and the day before\n    \n    Arguments:\n    Y_daybefore -- A tensor of shape (267,) that represents the prices of each day before each price in Y_test\n    Y_test -- A tensor of shape (267,) that represents the normalized y values of the testing data\n    y_predict -- A tensor of shape (267,) that represents the normalized y values of the model's predictions\n    \n    Returns:\n    Y_daybefore -- A tensor of shape (267, 1) that represents the prices of each day before each price in Y_test\n    Y_test -- A tensor of shape (267, 1) that represents the normalized y values of the testing data\n    delta_predict -- A tensor of shape (267, 1) that represents the difference between predicted and day before values\n    delta_real -- A tensor of shape (267, 1) that represents the difference between real and day before values\n    fig -- A plot representing percent change in bitcoin price per day,\n    \"\"\"\n    #Reshaping Y_daybefore and Y_test\n    Y_daybefore = np.reshape(Y_daybefore, (-1, 1))\n    Y_test = np.reshape(Y_test, (-1, 1))\n\n    #The difference between each predicted value and the value from the day before\n    delta_predict = (y_predict - Y_daybefore) \/ (1+Y_daybefore)\n\n    #The difference between each true value and the value from the day before\n    delta_real = (Y_test - Y_daybefore) \/ (1+Y_daybefore)\n\n    #Plotting the predicted percent change versus the real percent change\n    fig = plt.figure(figsize=(10, 6))\n    ax = fig.add_subplot(111)\n    ax.set_title(\"Percent Change in Bitcoin Price Per Day\")\n    plt.plot(delta_predict, color='green', label = 'Predicted Percent Change')\n    plt.plot(delta_real, color='red', label = 'Real Percent Change')\n    plt.ylabel(\"Percent Change\")\n    plt.xlabel(\"Time (Days)\")\n    ax.legend()\n    plt.show()\n    \n    return Y_daybefore, Y_test, delta_predict, delta_real, fig","01791f3c":"def binary_price(delta_predict, delta_real):\n    \"\"\"\n    Converts percent change to a binary 1 or 0, where 1 is an increase and 0 is a decrease\/no change\n    \n    Arguments:\n    delta_predict -- A tensor of shape (267, 1) that represents the predicted percent change in price\n    delta_real -- A tensor of shape (267, 1) that represents the real percent change in price\n    \n    Returns:\n    delta_predict_1_0 -- A tensor of shape (267, 1) that represents the binary version of delta_predict\n    delta_real_1_0 -- A tensor of shape (267, 1) that represents the binary version of delta_real\n    \"\"\"\n    #Empty arrays where a 1 represents an increase in price and a 0 represents a decrease in price\n    delta_predict_1_0 = np.empty(delta_predict.shape)\n    delta_real_1_0 = np.empty(delta_real.shape)\n\n    #If the change in price is greater than zero, store it as a 1\n    #If the change in price is less than zero, store it as a 0\n    for i in range(delta_predict.shape[0]):\n        if delta_predict[i][0] > 0:\n            delta_predict_1_0[i][0] = 1\n        else:\n            delta_predict_1_0[i][0] = 0\n    for i in range(delta_real.shape[0]):\n        if delta_real[i][0] > 0:\n            delta_real_1_0[i][0] = 1\n        else:\n            delta_real_1_0[i][0] = 0    \n\n    return delta_predict_1_0, delta_real_1_0","f9fa6a98":"def find_positives_negatives(delta_predict_1_0, delta_real_1_0):\n    \"\"\"\n    Finding the number of false positives, false negatives, true positives, true negatives\n    \n    Arguments: \n    delta_predict_1_0 -- A tensor of shape (267, 1) that represents the binary version of delta_predict\n    delta_real_1_0 -- A tensor of shape (267, 1) that represents the binary version of delta_real\n    \n    Returns:\n    true_pos -- An integer that represents the number of true positives achieved by the model\n    false_pos -- An integer that represents the number of false positives achieved by the model\n    true_neg -- An integer that represents the number of true negatives achieved by the model\n    false_neg -- An integer that represents the number of false negatives achieved by the model\n    \"\"\"\n    #Finding the number of false positive\/negatives and true positives\/negatives\n    true_pos = 0\n    false_pos = 0\n    true_neg = 0\n    false_neg = 0\n    for i in range(delta_real_1_0.shape[0]):\n        real = delta_real_1_0[i][0]\n        predicted = delta_predict_1_0[i][0]\n        if real == 1:\n            if predicted == 1:\n                true_pos += 1\n            else:\n                false_neg += 1\n        elif real == 0:\n            if predicted == 0:\n                true_neg += 1\n            else:\n                false_pos += 1\n    return true_pos, false_pos, true_neg, false_neg","a38b8fc9":"def calculate_statistics(true_pos, false_pos, true_neg, false_neg, y_predict, Y_test):\n    \"\"\"\n    Calculate various statistics to assess performance\n    \n    Arguments:\n    true_pos -- An integer that represents the number of true positives achieved by the model\n    false_pos -- An integer that represents the number of false positives achieved by the model\n    true_neg -- An integer that represents the number of true negatives achieved by the model\n    false_neg -- An integer that represents the number of false negatives achieved by the model\n    Y_test -- A tensor of shape (267, 1) that represents the normalized y values of the testing data\n    y_predict -- A tensor of shape (267, 1) that represents the normalized y values of the model's predictions\n    \n    Returns:\n    precision -- How often the model gets a true positive compared to how often it returns a positive\n    recall -- How often the model gets a true positive compared to how often is hould have gotten a positive\n    F1 -- The weighted average of recall and precision\n    Mean Squared Error -- The average of the squares of the differences between predicted and real values\n    \"\"\"\n    precision = float(true_pos) \/ (true_pos + false_pos)\n    recall = float(true_pos) \/ (true_pos + false_neg)\n    F1 = float(2 * precision * recall) \/ (precision + recall)\n    #Get Mean Squared Error\n    MSE = mean_squared_error(y_predict.flatten(), Y_test.flatten())\n\n    return precision, recall, F1, MSE","d2151008":"X_train, Y_train, X_test, Y_test, Y_daybefore, unnormalized_bases, window_size = load_data(\"Bitcoin Data.csv\", 50)\nprint X_train.shape\nprint Y_train.shape\nprint X_test.shape\nprint Y_test.shape\nprint Y_daybefore.shape\nprint unnormalized_bases.shape\nprint window_size","fba9adae":"model = initialize_model(window_size, 0.2, 'linear', 'mse', 'adam')\nprint model.summary()","ae4db753":"model, training_time = fit_model(model, X_train, Y_train, 1024, 100, .05)\n\n#Print the training time\nprint \"Training time\", training_time, \"seconds\"","dd1c6636":"y_predict, real_y_test, real_y_predict, fig1 = test_model(model, X_test, Y_test, unnormalized_bases)\n\n#Show the plot\nplt.show(fig1)","0cc320d9":"Y_daybefore, Y_test, delta_predict, delta_real, fig2 = price_change(Y_daybefore, Y_test, y_predict)\n\n#Show the plot\nplt.show(fig2)","e0fbcb9e":"delta_predict_1_0, delta_real_1_0 = binary_price(delta_predict, delta_real)\n\nprint delta_predict_1_0.shape\nprint delta_real_1_0.shape","4e24213d":"true_pos, false_pos, true_neg, false_neg = find_positives_negatives(delta_predict_1_0, delta_real_1_0)\nprint \"True positives:\", true_pos\nprint \"False positives:\", false_pos\nprint \"True negatives:\", true_neg\nprint \"False negatives:\", false_neg","1ca4d902":"precision, recall, F1, MSE = calculate_statistics(true_pos, false_pos, true_neg, false_neg, y_predict, Y_test)\nprint \"Precision:\", precision\nprint \"Recall:\", recall\nprint \"F1 score:\", F1\nprint \"Mean Squared Error:\", MSE","2eb5cd51":"## Getting the Statistics","57282cd6":"![alt text](http:\/\/2.bp.blogspot.com\/-wuinSTn-X4A\/UwHmmceDQqI\/AAAAAAAAJFo\/5EjPg-LpAJc\/s1600\/Sivakumar_Vellingiri_Normal_Forms_Poster.Jpeg \"Logo Title Text 1\")\n\n# Step 1 - Data Processing\n\n- The data is inputed as a .csv file.\n- Lets time-series transform data from (num days x num features) to (num days- window size x num days per sample x num features) where window size is 50\n- Normalization via dividing each value in the window by the first value of the window and then subtracting one.i.e [4,3,2] into [0, -0.25, -0.5]. .\n- The unnormalized bases are kept to compare the model's predictions of prices with the true prices. \n- The first 90% of the data is used in training the model, and the last 10% will be used to test the model. \n- A list of the prices before each day Y_test is drawn from will be compiled in order to generate statistics about the model's predictions\n\n======================================================================================================================\n\nThe columns of data and their definitions are as follows: \n- Annual Hash Growth: Growth in the total network computations over the past 365 days\n- Block Height: The total number of blocks in the blockchain\n- Block Interval: Average amount of time between blocks\n- Block Size: The storage size of each block (i.e. megabytes)\n- BlockChain Size: The storage size of the blockchain (i.e. gigabytes)\n- Daily Blocks: Number of blocks found each day\n- Chain Value Density: The value of bitcoin's blockchain, in terms of dollars per megabyte\n- Daily Transactions: The number of transactions included in the blockchain per day\n- Difficulty: The minimum proof-of-work threshold required for a bitcoin miner to mine a block\n- Fee Percentage: Average fee paid as a percentage of transaction volume\n- Fee Rate: Average fee paid per transaction\n- Two-Week Hash Growth: Growth in the total network computations over the past 14 days\n- Hash Rate: The number of block solutions computed per second by all miners\n- Market Capitalization: The market value of all bitcoin in circulation\n- Metcalfe's Law - TX: A variant of Metcalfe's Law in which price is divided by n log n number of daily transactions\n- Metcalfe's Law - UTXO: A variant of Metcalfe's Law in which price is divided by n log n number of unspent transaction outputs\n- Miner Revenue Value: The amount of dollars earned by the mining network\n- Miner Revenue: The amount of bitcoin earned by the mining network, in the form of block rewards and transaction fees\n- Money Supply: The amount of bitcoin in circulation\n- Output Value: The dollar value of all outputs sent over the network\n- Output Volume: The amount of Bitcoin sent over the network\n- Bitcoin Price: The amount of dollars a single bitcoin is worth\n- Quarterly Hash Growth: Growth in the total network computations in the past 90 days\n- Total Transactions: The running total number of transactions processed by the Bitcoin network\n- Transaction Amount: The average amount of bitcoin moved per transaction\n- Fees Value: The dollar value of mining fees\n- Transaction Fees: The amount of bitcoin paid to miners in fees\n- Transaction Size: The average data size of a transaction\n- Transaction Value: The average dollar value moved in each transaction\n- Transactions per Block: The number of transactions in each block\n- Average UTXO Amount: The average amount of bitcoin contained in each unspent transaction output\n- UTXO Growth: The net number of unspent transaction outputs created\n- UTXO Set Size: The total number of unspent transaction outputs\n- Average UTXO Value: The average dollar value of each uspent transaction output\n- Velocity - Daily: The proportion of the money supply transacted each day\n- Velocity - Quarterly: The proportion of the money supply transacted each day, computed on a rolling-quarter basis\n- Velocity of Money: How many times the money supply changes hands in a given year","0821b534":"# Step 6 - Process the Percent Change in Price\n\n- The percent change in price will be processsed such that an increase in price is represented by a 1, and a decrease\/no change is represented by a 0. These binary values will be stored in arrays delta_predict_1_0 and delta_real_1_0. \n\n- This will be done by looping through the values of the real and predicted percent change arrays. If a value is greater than 0, a 1 is stored in a new array. Otherwise, a 0 is stored in the new array.\n\n- This process is very useful to understand how well the model did, and can be used to gather statistics about the model's performance.","8b103af3":"## Getting Binary Version of Percent Change","df2aa9c0":"# 9 - Putting It All Together\n\nApplying all the methods defined above and analyzing results.","bed0691a":"## Loading the Data","27917d5c":"## Comparing Predictions and True Data","b42a8ba4":"![alt text](http:\/\/d3kbpzbmcynnmx.cloudfront.net\/wp-content\/uploads\/2015\/09\/rnn.jpg \"Logo Title Text 1\")\n![alt text](https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/preview\/media\/scenario-tdsp-biomedical-recognition\/lstm-cell.png \"Logo Title Text 1\")\n![alt text](http:\/\/d3kbpzbmcynnmx.cloudfront.net\/wp-content\/uploads\/2015\/09\/bidirectional-rnn.png \"Logo Title Text 1\")\n![alt text](https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2017\/05\/Comparison-of-Adam-to-Other-Optimization-Algorithms-Training-a-Multilayer-Perceptron.png \"Logo Title Text 1\")\n\n\nBidirectional RNNs are based on the idea that the output at time t may not only depend on the previous elements in the sequence, but also future elements. For example, to predict a missing word in a sequence you want to look at both the left and the right context. Bidirectional RNNs are quite simple. They are just two RNNs stacked on top of each other. The output is then computed based on the hidden state of both RNNs.","6fbd5183":"# Step 4 - Testing the Model\n\n- The models given x values of testing data & will predict normalized prices (y_predict)\n- Then, both the predicted values and the real values will be unnormalized and stored in separate arrays. \n- The values are unnormalized by looping through the predicted and true values. \n- 1 is added to each value, and then the result is multiplied by a corresponding number in the unnormalized_bases array. \n- In other words, the unnormalization processs is the exact reverse of the normalization process\n- Finally, a plot is created of the unnormalized real values and the unnormalized predicted values. ","a2e4ee1f":"## Plotting Percent Change","6cffde45":"## Training the Model","f6a24165":"# 8 - Calculating Statistics\n\n![alt text](https:\/\/www.researchgate.net\/profile\/Alexandros_Karatzoglou\/publication\/221515860\/figure\/fig1\/AS:339586132791298@1457975051470\/Figure-1-Mean-Squared-Error-formula-used-to-evaluate-the-user-model.ppm \"Logo Title Text 1\")\n\n![alt text](https:\/\/image.slidesharecdn.com\/qconrio-machinelearningforeveryone-150826200704-lva1-app6892\/95\/qcon-rio-machine-learning-for-everyone-51-638.jpg?cb=1440698161 \"Logo Title Text 1\")\n\n\nPutting everything together and getting statistics about the model. Statistics being calculated include:\n- Precision: How often the model gets a true positive compared to how often it returns a positive\n- Recall: How often the model gets a true positive compared to how often it should have gotten a positive\n- F1 Score: The weighted average of recall and precision\n- Mean Squared Error: The average of the squares of the differences between predicted and real values","c8f25878":"# Step 5 - Evaluating Change in Price\n\n- Lets plot the model's predicted change in price each day against the real change in price daily\n- The percent increases of the predicted values and the real values are calculated by subtracting the value from the day before from the predicted\/real value then dividing the result by 1+the value from the day before. \n- The predicted change in price is stored in delta_predict, while the real change in price is stored in delta_real.\n- These two tensors are then graphed together to visualize the difference between predicted and real change in price for bitcoin throughout the testing period. \n- The plot will represent the percent change in bitcoin price each day. ","9ba7c7ee":"## Initializing the Model","8dd0c7ba":"# 7 - Comparing Predictions and Real Data\n\nThe binary categories computed in the previous cell is now used to compare predicted and real data. It will be used to find the number of:\n- True positives\n- False positives\n- True negatives\n- False negatives\nThese can then be used to further calculate statistics of the model's performance. \n\nThis will be done by looping through both binary arrays at once and getting the corresponding values. If the real value is a 1 and the predicted value is a 1, that index will be counted as a true positive. If the real value is a 1 and the predicted value is a 0, that index will be counted as a false negative. If the real value is a 0 and the predicted value is a 0, that index will be counted as a true negative. If the real value is a 0 and the predicted value is a 1, that index will be counted as a false positive.","12c9fab8":"# Step 3 - Training the Model\n\n- The model will be fitted to the training dat with a batch_size of 1024. \n- Additionally, 100 epochs will be performed to give the model time to adjust its weights and biases to fit the training data.\n- 5% of the training data will be used as the validation set.  \n- The model will train by minimizing the loss (mean squared error) of its training data. -- - The validation set is useful when attempting to identify signs of overfitting. \n- If the validation loss begins to consistently and rapidly increase, the model has overfitted to the training data, and changes should be made to the model. \n","8817edce":"# Step 2 - Building the Model\n\n- We'll use a 3layer RNN with  20% dropout at each layer to reduce overfitting to the training data. \n- This model will have 515,579 trainable parameters throughout all of its layers. \n- The model uses the AdamOptimizer as its optimization function.\n- The loss function used in this model is mean squared error. \n- A linear activation function is used in this model to determine the output of each neuron in the model. The linear activation function is simply defined as f(x) = x.\n- The model will use Keras's Sequential model with Bidirectional LSTM layers.","5ee53a4c":"## Testing the Model","408fc641":" # A Deep Learning Approach to Predicting Cryptocurrency Prices (Ethereum, Bitcoin, etc.)\n \n##### We will Implement a recurrent neural network to predict bitcoin prices\n\n![alt text](https:\/\/dashee87.github.io\/images\/bitcoin_ether_training_test.png \"Logo Title Text 1\")\n "}}