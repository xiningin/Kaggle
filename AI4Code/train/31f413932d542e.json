{"cell_type":{"c9c47ec1":"code","d0fe6c63":"code","d0c0e845":"code","c4ffcde3":"code","d5e24eb9":"code","de397d3c":"code","75612b04":"code","c0604588":"code","90e19a34":"code","e7772bf1":"code","c2d441e6":"code","526688d7":"code","5cfd1a8f":"code","5e4cb4c4":"code","eca6555b":"code","6c168df7":"code","9fcd4d93":"code","11bd407a":"code","823dfc7d":"code","6cb3669c":"code","aded422b":"code","418fb491":"code","b064a8ae":"code","2e3e9935":"code","0b03abf6":"code","afc2bf19":"code","4d20ef7b":"code","fd0e8e06":"code","0c25e203":"code","6e0184ad":"code","c4e6925d":"code","b7fed22e":"code","7560079a":"code","a929e3ed":"code","a4c0e09a":"code","79d8907f":"code","d71c2293":"code","4bb224d5":"code","286c8149":"code","81596f56":"code","6addbc43":"code","377897cf":"code","93027605":"code","179095b3":"code","7d37bfba":"code","ff32c394":"code","b616f997":"code","1a474b41":"code","2f6d7d39":"code","93915fca":"code","f695dc69":"code","47e9b753":"code","1b326667":"code","0dad0e69":"code","13d05125":"code","2dd2022b":"code","b76d0ed6":"code","7fbd4a4c":"code","383f8667":"code","c3562552":"code","25986fa4":"code","19e07be1":"code","53fda0fb":"markdown","2327574a":"markdown","f910e13d":"markdown","ae64ab5a":"markdown","dbff3897":"markdown","5dd1f9b0":"markdown","ed5b292e":"markdown","e3c25a53":"markdown","66535ace":"markdown","c407edf8":"markdown","5cf867ea":"markdown","b539c921":"markdown","0a4336f0":"markdown","a5b1cef2":"markdown","e1174287":"markdown","53cf6f6c":"markdown","8df32ad0":"markdown","62400e62":"markdown","45b5671c":"markdown","2d9192b7":"markdown","54eb8bff":"markdown"},"source":{"c9c47ec1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pds.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom statistics import mode\nfrom scipy.special import boxcox1p\nfrom sklearn.preprocessing import LabelEncoder,RobustScaler\nfrom sklearn.linear_model import Ridge, Lasso,ElasticNet\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom catboost import Pool, CatBoostRegressor, cv\nimport xgboost as xgb\nimport lightgbm as lgb\nimport sys\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n# Any results you write to the current directory are saved as output.","d0fe6c63":"train=pd.read_csv(\"..\/input\/train.csv\")\ntest=pd.read_csv(\"..\/input\/test.csv\")","d0c0e845":"train.shape,test.shape","c4ffcde3":"train.describe()","d5e24eb9":"train_ID = train['Id']\ntest_ID = test['Id']\n\n# Now drop the 'Id' colum since it's unnecessary for the prediction process\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","de397d3c":"sns.regplot(train[\"GrLivArea\"],y=train[\"SalePrice\"],fit_reg=True)\nplt.show()\n","75612b04":"# Removing two very extreme outliers in the bottom right hand corner\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n# Re-check graph\nsns.regplot(x=train['GrLivArea'], y=train['SalePrice'], fit_reg=True)\nplt.show()\n","c0604588":"(mu,sigma)=norm.fit(train.SalePrice)\nsns.distplot(train.SalePrice,fit=norm)\nplt.legend([\"$\\mu=$ {:.2f} and $\\sigma=$ {:.2f}\".format(mu,sigma)],loc=\"best\")","90e19a34":"train.SalePrice = np.log1p(train.SalePrice)","e7772bf1":"(mu,sigma)=norm.fit(train.SalePrice)\nsns.distplot(train.SalePrice,fit=norm)\nplt.legend([\"$\\mu=$ {:.2f} and $\\sigma=$ {:.2f}\".format(mu,sigma)],loc=\"best\")","c2d441e6":"train_nS=train.shape[0]\ntest_nS=test.shape[0] # shpaes of train and tests for sperating them back\n\ntrain_y=train.SalePrice.values\nfull_data=pd.concat((train,test)).reset_index(drop=True) #concating the train and test sets\n\nfull_data.drop([\"SalePrice\"],axis=1,inplace=True) #dropping the target values\n\nfull_data.shape","526688d7":"missing_data_rank=(full_data.isnull().sum()\/len(full_data))*100\nprint(\"total number of columns with values misiing : {}\".format(missing_data_rank[missing_data_rank>0].count()))\nmissed =pd.DataFrame({\"Missing Percentage\": missing_data_rank[missing_data_rank>0].sort_values(ascending =False)})\n\n","5cfd1a8f":"missed_features=list(missed.index)","5e4cb4c4":"full_data.head(30)","eca6555b":"full_data.GarageQual.unique()","6c168df7":"# All columns where missing values can be replaced with 'None'\nfor col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'MSSubClass'):\n    full_data[col] = full_data[col].fillna('None')","9fcd4d93":"\n# All columns where missing values can be replaced with 0\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n    full_data[col] = full_data[col].fillna(0)","11bd407a":"\n\n# All columns where missing values can be replaced with the mode (most frequently occurring value)\nfor col in ('MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Functional', 'Utilities'):\n    full_data[col] = full_data[col].fillna(full_data[col].mode()[0])\n\n# Imputing LotFrontage with the median (middle) value\nfull_data['LotFrontage'] = full_data.groupby('Neighborhood')['LotFrontage'].apply(lambda x: x.fillna(x.median()))","823dfc7d":"full_data['TotalSF'] = full_data['TotalBsmtSF'] + full_data['1stFlrSF'] + full_data['2ndFlrSF']\n","6cb3669c":"missing_data=(full_data.isnull().sum()\/len(full_data))*100\nprint(\"total number of columns with values misiing : {}\".format(missing_data[missing_data>0].count()))\nmissed =pd.DataFrame({\"Missing Percentage\": missing_data[missing_data>0].sort_values(ascending =False)})\n","aded422b":"full_data.info()","418fb491":"# Converting those variables which should be categorical, rather than numeric\nfor col in ('MSSubClass', 'OverallCond', 'YrSold', 'MoSold'):\n    full_data[col] = full_data[col].astype(str)\n    \nfull_data.info()","b064a8ae":"# Applying a log(1+x) transformation to all skewed numeric features\nnumeric_feats = full_data.dtypes[full_data.dtypes != \"object\"].index\n\n# Compute skewness\nskewed_feats = full_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(15)","2e3e9935":"# Check on number of skewed features above 75% threshold\nskewness = skewness[abs(skewness) > 0.75]\nprint(\"Total number of features requiring a fix for skewness is: {}\".format(skewness.shape[0]))","0b03abf6":"# Now let's apply the box-cox transformation to correct for skewness\nskewed_features = skewness.index\nlam = 0.15\nfor feature in skewed_features:\n    full_data[feature] = boxcox1p(full_data[feature], lam)","afc2bf19":"full_data = full_data.drop(['Street', 'Utilities', 'Condition2', 'RoofMatl', 'Heating', 'PoolQC'], axis=1)","4d20ef7b":"#highlyrepeated_values= [col for col in full_data.select_dtypes(exclude=['number']) if 1 - sum(full_data[col] == mode(full_data[col]))\/len(full_data) < 0.03]\n# Dropping these columns from both datasets\n#full_data = full_data.drop(['Street', 'Utilities', 'Condition2', 'RoofMatl', 'Heating', 'PoolQC'], axis=1)","fd0e8e06":"full_data.info()","0c25e203":"obj_features=list(full_data.select_dtypes(include=\"object\").columns)\nlen(obj_features)","6e0184ad":"# List of columns to Label Encode\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\n# Process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(full_data[c].values)) \n    full_data[c] = lbl.transform(list(full_data[c].values))\n\n# Check on data shape        \nprint('Shape all_data: {}'.format(full_data.shape))","c4e6925d":"full_data.info()","b7fed22e":"full_data=pd.get_dummies(full_data)","7560079a":"full_data.shape","a929e3ed":"full_data.i","a4c0e09a":"\n# Now to return to separate train\/test sets for Machine Learning\ntrain_x = full_data[:train_nS]\ntest_x= full_data[train_nS:]","79d8907f":"# Defining two rmse_cv functions\n\ndef rmse_cv(model):\n    \n    rmse=np.sqrt(-cross_val_score(model, train_x,train_y,scoring=\"neg_mean_squared_error\",cv=10))\n    return (rmse)","d71c2293":"\nalphas = [0.05, 0.1, 0.3, 1, 3, 5,7, 10, 15, 30]\n#alphas=np.arange(0.05,30,0.05)\n# Iterate over alpha's\ncv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas]","4bb224d5":"print(cv_ridge)\ncv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Validation\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Rmse\")\n","286c8149":"# 5 looks like the optimal alpha level, so let's fit the Ridge model with this value\n#model_ridge = Ridge(alpha = 10)\nmodel_ridge = Ridge(alpha = 7)","81596f56":"alphas = [0.01, 0.005, 0.001, 0.0002,0.0003,0.0004,0.0005,0.0001]\n#alphas=np.arange(0.0001,0.01,0.0005)\n# Iterate over alpha's\ncv_lasso = [rmse_cv(Lasso(alpha = alpha,random_state=1)).mean() for alpha in alphas]\n\n# Plot findings\ncv_lasso = pd.Series(cv_lasso, index = alphas)\ncv_lasso.plot(title = \"Validation\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Rmse\")\nprint(cv_lasso)","6addbc43":"# Initiating Lasso model\nmodel_lasso = make_pipeline(RobustScaler(), Lasso(alpha = 0.0004))","377897cf":"# Setting up list of alpha's\nalphas = [0.01, 0.005, 0.001, 0.00055,0.0006, 0.0001]\n#alphas=np.arange(0.0001,1,0.0004)\n# Iterate over alpha's\ncv_elastic = [rmse_cv(ElasticNet(alpha = alpha)).mean() for alpha in alphas]\n\n# Plot findings\ncv_elastic = pd.Series(cv_elastic, index = alphas)\ncv_elastic.plot(title = \"Validation\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Rmse\")\nprint(cv_elastic)","93027605":"# Initiating ElasticNet model\nmodel_elasticnet = make_pipeline(RobustScaler(), ElasticNet(alpha = 0.0006))","179095b3":"# Setting up list of alpha's\nalphas = [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n\n# Iterate over alpha's\ncv_krr = [rmse_cv(KernelRidge(alpha = alpha)).mean() for alpha in alphas]\n\n# Plot findings\ncv_krr = pd.Series(cv_krr, index = alphas)\ncv_krr.plot(title = \"Validation\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"Rmse\")\nprint(cv_krr)","7d37bfba":"# Initiatiing KernelRidge model\nmodel_krr = make_pipeline(RobustScaler(), KernelRidge(alpha=7, kernel='polynomial', degree=2.65, coef0=6.9))","ff32c394":"# Initiating Gradient Boosting Regressor\nmodel_gbr = GradientBoostingRegressor(n_estimators=1200, \n                                      learning_rate=0.05,\n                                      max_depth=4, \n                                      max_features='sqrt',\n                                      min_samples_leaf=15, \n                                      min_samples_split=10, \n                                      loss='huber',\n                                      random_state=5)\ncv_gbr=rmse_cv(model_gbr).mean()\ncv_gbr","b616f997":"\n\n# Initiating XGBRegressor\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.2,\n                             learning_rate=0.025,\n                             max_depth=3,\n                             n_estimators=1550)\ncv_xgb = rmse_cv(model_xgb).mean()\ncv_xgb","1a474b41":"# Initiating LGBMRegressor model\nmodel_lgb = lgb.LGBMRegressor(objective='regression',\n                              num_leaves=4,\n                              learning_rate=0.05, \n                              n_estimators=1080,\n                              max_bin=75, \n                              bagging_fraction=0.80,\n                              bagging_freq=5, \n                              feature_fraction=0.232,\n                              feature_fraction_seed=9, \n                              bagging_seed=9,\n                              min_data_in_leaf=6, \n                              min_sum_hessian_in_leaf=11)\ncv_lgb = rmse_cv(model_lgb).mean()\ncv_lgb","2f6d7d39":"# Fitting all models with rmse_cv function, apart from CatBoost\ncv_ridge = rmse_cv(model_ridge).mean()\ncv_lasso = rmse_cv(model_lasso).mean()\ncv_elastic = rmse_cv(model_elasticnet).mean()\ncv_krr = rmse_cv(model_krr).mean()\ncv_gbr = rmse_cv(model_gbr).mean()\ncv_xgb = rmse_cv(model_xgb).mean()\ncv_lgb = rmse_cv(model_lgb).mean()","93915fca":"\n\n# Creating a table of results, ranked highest to lowest\nresults = pd.DataFrame({\n    'Model': ['Ridge',\n              'Lasso',\n              'ElasticNet',\n              'Kernel Ridge',\n              'Gradient Boosting Regressor',\n              'XGBoost Regressor',\n              'Light Gradient Boosting Regressor',\n              ],\n    'Score': [cv_ridge,\n              cv_lasso,\n              cv_elastic,\n              cv_krr,\n              cv_gbr,\n              cv_xgb,\n              cv_lgb]})\n\n# Build dataframe of values\nresult_df = results.sort_values(by='Score', ascending=True).reset_index(drop=True)\nresult_df.head(8)\n\n","f695dc69":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)\n","47e9b753":"\n#Averaged base models score\naveraged_models = AveragingModels(models = (model_elasticnet, model_gbr, model_krr, model_lasso))\nscore = rmse_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\n","1b326667":"n_folds = 5\ndef rmsle_cv(model):\n    #kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train_x.values, train_y, scoring=\"neg_mean_squared_error\", cv = 10))\n    return(rmse)","0dad0e69":"#Stacking averaged Models Class\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n    \n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)\n\n\n\n","13d05125":"stacked_averaged_models = StackingAveragedModels(base_models = (model_elasticnet, model_gbr, model_krr),meta_model = model_lasso)\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n","2dd2022b":"#define a rmsle evaluation function\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","b76d0ed6":"#Final Training and Prediction\nstacked_averaged_models.fit(train_x.values, train_y)\nstacked_train_pred = stacked_averaged_models.predict(train_x.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test_x.values))\nprint(rmsle(train_y, stacked_train_pred))","7fbd4a4c":"model_xgb.fit(train_x, train_y)\nxgb_train_pred = model_xgb.predict(train_x)\nxgb_pred = np.expm1(model_xgb.predict(test_x))\nprint(rmsle(train_y, xgb_train_pred))\n","383f8667":"\nmodel_lgb.fit(train_x, train_y)\nlgb_train_pred = model_lgb.predict(train_x)\nlgb_pred = np.expm1(model_lgb.predict(test_x.values))\nprint(rmsle(train_y, lgb_train_pred))\n\n","c3562552":"\n\nprint('RMSLE score on train data:')\nprint(rmsle(train_y,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","25986fa4":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15\n#ensemble=xgb_pred","19e07be1":"# Create stacked model\n#stacked = (lasso_pred + elastic_pred + ridge_pred + xgb_pred + lgb_pred + krr_pred + gbr_pred) \/ 7\n# Setting up competition submission\nsub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble #stacked\nsub.to_csv('house_price_predictions.csv',index=False)","53fda0fb":"we can see there are few outliers in the dataset we can go ahead and remove them as these would affect over model prediction.","2327574a":"now looking at the target variable, we donnot need the ID as it is not usefull for the model prediction we can drop it.","f910e13d":"We can see that only few features have high rank of missing values in them and rest are prety good enough with less than 10%.\nNow lets dive into data and look how can we fill up this missed values: there are two ways in deeling the missing values:\n1. We can drop the row for the values misisng in them, tough this is not the ideal choice if we havel less traning data and droping data can lead the model to ineffecinet.\n2. Than droping the row and loosing the data, we can fill up or impute the mising values with the appropriate values like using mean, median and mode values.\nSo am going to keep the missing rows and fill them with appropriate values.","ae64ab5a":"\n\nBox Cox Transformation of (highly) skewed features\n\nSkewed features are a formality when dealing with real-world data. Transformation techniques can help to stabilize variance, make data more normal distribution-like and improve the validity of measures of association.\n\nThe problem with the Box-Cox Transformation is estimating lambda. This value will depend on the existing data, and as such should be considered when performing cross validation on out of sample datasets.\n","dbff3897":"Now looking at the missing values and imputing with the appropriate values. Now will  find the percenatge of missing values in each features.","5dd1f9b0":"\nB. Ensemble methods (Gradient tree boosting)\n\nBoosting is an ensemble technique in which the predictors are not made independently, but sequentially.\n\nThis technique employs the logic in which the subsequent predictors learn from the mistakes of the previous predictors. Therefore, the observations have an unequal probability of appearing in subsequent models and ones with the highest error appear most. The predictors can be chosen from a range of models like decision trees, regressors, classifiers etc. Because new predictors are learning from mistakes committed by previous predictors, it takes less time\/iterations to reach close to actual predictions. But we have to choose the stopping criteria carefully or it could lead to overfitting on training data. Gradient Boosting is an example of a boosting algorithm, and these are what i'll be applying to the current data next.\n 5. Gradient Boosting\n\nFor the Gradient Boosting algorithm I will use 'huber' as the loss function as this is robust to outliers. The other parameters on display originate from other kernels tackling this challenge, followed by trial and error to refine them to this specific dataset. Again, applying GridSearchCV will help to define a better set of parameters than those currently on display.\n","ed5b292e":"Ridge Regression:","e3c25a53":"I thing few of the features should be categorical but are as numeric in data, so I am going to change them to categorical values.","66535ace":"now taking a look at the target values for any kind of skewness  present in them.","c407edf8":"\nLabel encoding\n\nThis step build on the previous step whereby all text data will become numeric. This is a requirement for Machine Learning, that is, only numerical data can be fed into a predictive model. There are many other encoding techniques available, some of which more powerful than Label Encoding which does incur the risk of falsely ranking variables, e.g. coding three locations into 0, 1 and 2 might imply that 2 is a higher value than 0, which is incorrect as the numbers just represent different categories (locations). This is a simple approach, however, and therefore I'm going to stick with it for the current kernel.","5cf867ea":"**XGB**","b539c921":"ElasticNet:","0a4336f0":"**LGB**","a5b1cef2":"First we will make the target values seperate from the Training values and combine th train and test sets to make all the work together rather doing seperate for each once.","e1174287":"Now our target values looks well distributed.","53cf6f6c":"Now find any features which is highly represented i.e the values in the features are same to the  extend of 97%, this values donot play any role in the model prediction so we will drop those features. ","8df32ad0":"As we alredy change the right skewed target values to normal distribution, now i am going to cange alll the numeric data to normal distribution if any data is skewd internally.","62400e62":"Now we cleaned up our missing values and its time to move to the categorical values as our models can work with only numeric data, now we change our categorical values to numeric by labeling them. \n","45b5671c":"\n3. Stacking algorithms\n\nI've ran eight models thus far, and they've all performed pretty well. I'm now quite keen to explore stacking as a means of achieving an even higher score. In a nutshell, stacking uses as a first-level (base) the predictions of a few basic classifiers and then uses another model at the second-level to predict the output from the earlier first-level predictions. Stacking can be beneficial as combining models allows the best elements of their predictive power on the given challenged to be pooled, thus smoothing over any gaps left from an individual model and increasing the likelihood of stronger overall model performance.\n\nOk, let's get model predictions and then stack the results!\n","2d9192b7":"\n 4. Kernel ridge regression\n\nOK, this is not strictly a generalized linear model. Kernel ridge regression (KRR) combines Ridge Regression (linear least squares with l2-norm regularization) with the 'kernel trick'. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.\n","54eb8bff":"From the above plot we can see that the data is bit right skewed. we can make it to normal by applying the log for the target values."}}