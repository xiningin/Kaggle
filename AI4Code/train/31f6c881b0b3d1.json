{"cell_type":{"e5e922c1":"code","7de3a4b0":"code","3f9e81ac":"code","9657c908":"code","5c3bceff":"code","617493a1":"code","c16412b7":"code","01b7f677":"code","df9bd12d":"code","12d12c3e":"code","d7e4fb16":"code","d59d93ac":"code","10828bea":"code","360159e5":"code","bee0b686":"code","307b1c40":"code","2cb27985":"code","0dc2f364":"code","3386edd1":"code","9cca32de":"code","c261f20a":"code","10007170":"code","23b886d3":"code","65eea0a4":"code","31a2bdcf":"code","d6e324a3":"markdown","c2106212":"markdown"},"source":{"e5e922c1":"import warnings\nfrom catboost import CatBoostRegressor\nimport pandas as pd\nfrom lightgbm import LGBMRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.impute import KNNImputer\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import GridSearchCV, cross_validate, RandomizedSearchCV, validation_curve\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor,ExtraTreesRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nimport numpy as np\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)","7de3a4b0":"from shutil import copyfile\ncopyfile(src=\"\/kaggle\/input\/helpers\/eda.py\", dst=\"..\/working\/eda.py\")\ncopyfile(src=\"\/kaggle\/input\/helpers\/data_prep.py\", dst=\"..\/working\/data_prep.py\")\nfrom eda import *\nfrom data_prep import *","3f9e81ac":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","9657c908":"# Concat with Train and test datasets:\ndf = train.append(test).reset_index(drop=True)\ndf.head()","5c3bceff":"# Check the target values in terms of distrbution.\n# There is slightly positively skewed\n\nplt.hist(train[\"SalePrice\"], bins=20)\nplt.show()","617493a1":"#Check df:\ncheck_df(df)\n\n# Pick out the Categoric and numeric values\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","c16412b7":"# Analyses categoric but cardinal values:\ncat_but_car\n\n# It shows how many different unique values is inclued \ndf[\"Neighborhood\"].nunique()","01b7f677":"# Features Engineering\n# Those are created via high_corr socres: \n\ndf[\"GrLivArea*GarageArea\"] = df[\"GrLivArea\"] * df[\"GarageArea\"]\ndf[\"GrLivArea*LotArea\"] = df[\"GrLivArea\"] * df[\"LotArea\"]\ndf.replace([np.inf, -np.inf], 0, inplace=True)","df9bd12d":"# Categorical values analysis:\n\nfor col in cat_cols:\n    cat_summary(df, col)","12d12c3e":"# Numerical Values analysis:\n\ndf[num_cols].describe([0.10, 0.30, 0.50, 0.70, 0.80, 0.99]).T","d7e4fb16":"corrmat = train.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.5]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\nplt.show()","d59d93ac":"# Rare encoding\nrare_analyser(df, \"SalePrice\", cat_cols)","10828bea":"df = rare_encoder(df, 0.01)","360159e5":"# Determine low entropy\n\nuseless_cols = [col for col in cat_cols if df[col].nunique() == 1 or\n                (df[col].nunique() == 2 and (df[col].value_counts() \/ len(df) <= 0.01).any(axis=None))]\n\ncat_cols = [col for col in cat_cols if col not in useless_cols]\n\nfor col in useless_cols:\n    df.drop(col, axis=1, inplace=True)","bee0b686":"# one_hot_encoding:\ncat_cols = cat_cols + cat_but_car\ndf = one_hot_encoder(df, cat_cols, drop_first=True)","307b1c40":"# Remove outlier:\noutlier=[]\nfor col in num_cols:\n    if check_outlier(df, col,q1=0.25,q3=0.75) ==True:\n        outlier.append(col)\n    print(col, check_outlier(df, col,q1=0.25,q3=0.75))\n\n\nfor col in outlier:\n    replace_with_thresholds(df,col)","2cb27985":"# Determine missing value:\nmissing_values_table(df)","0dc2f364":"# Filling\nna_cols = [col for col in df.columns if df[col].isnull().sum() > 0 and \"SalePrice\" not in col]\ndf[na_cols] = df[na_cols].apply(lambda x: x.fillna(x.median()), axis=0)","3386edd1":"# Num_cols i\u00e7inden \u00e7\u0131karal\u0131m:\nnum_cols.remove(\"SalePrice\") #Target de\u011fi\u015fkeni \u00e7\u0131kard\u0131k\nnum_cols.remove(\"Id\") #Id de\u011fi\u015fkeni \u00e7\u0131kard\u0131k\n\nfor col in num_cols:\n    transformer = MinMaxScaler().fit(df[[col]])\n    df[col] = transformer.transform(df[[col]])","9cca32de":"train_df = df[df['SalePrice'].notnull()]\n\ntest_df = df[df['SalePrice'].isnull()].drop(\"SalePrice\", axis=1)","c261f20a":"y = np.log1p(train_df['SalePrice'])\n\nX = train_df.drop([\"Id\", \"SalePrice\"], axis=1)","10007170":"##################\n# Base Models\n##################\n\nmodels = [('LR', LinearRegression()),\n          #(\"Ridge\", Ridge()),\n          (\"Lasso\", Lasso()),\n          (\"ElasticNet\", ElasticNet()),\n          ('KNN', KNeighborsRegressor()),\n          ('CART', DecisionTreeRegressor()),\n          ('RF', RandomForestRegressor()),\n          ('SVR', SVR()),\n          ('GBM', GradientBoostingRegressor()),\n          #(\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n          (\"LightGBM\", LGBMRegressor()),\n          (\"CatBoost\", CatBoostRegressor(verbose=False))]\n\nfor name, regressor in models:\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n","23b886d3":"##############################\n# Hyperparameter Optimization\n##############################\n\nlgbm_model = LGBMRegressor(random_state=46)\n\nrmse = np.mean(np.sqrt(-cross_val_score(lgbm_model,\n                                        X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n\n\nlgbm_params = {\"learning_rate\": [0.01, 0.1],\n               \"n_estimators\": [500, 1500],\n               \"colsample_bytree\": [0.5, 0.7, 1]}\n\n\nlgbm_gs_best = GridSearchCV(lgbm_model,\n                            lgbm_params,\n                            cv=3,\n                            n_jobs=-1,\n                            verbose=True).fit(X, y)\n","65eea0a4":"\nfinal_model = lgbm_model.set_params(**lgbm_gs_best.best_params_).fit(X, y)\n\nrmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n\nrmse","31a2bdcf":"#######################################\n# Feature Selection\n#######################################\n\ndef plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')\n        \nplot_importance(final_model, X, 30)","d6e324a3":"**MODELLING**","c2106212":"**Competition Description**\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n**Executive Summary**\n\nI started this competition by just focusing on getting a good understanding of the dataset. The EDA is detailed and many visualizations are included. This version also includes modeling:\n\n* XGBoost model\n* LGBM model"}}