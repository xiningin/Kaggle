{"cell_type":{"e2e3ebd0":"code","d2fa7061":"code","738907e9":"code","aa9c896c":"code","838a6f22":"code","196bb6c6":"code","f30d1551":"code","4c4c1068":"code","f169f533":"code","229f8d3a":"code","dcfd2860":"code","11abc04d":"code","7678f923":"code","e1907333":"code","e95a6723":"code","70ec20bd":"code","05abcfd9":"code","634e8882":"code","7552f944":"code","53284392":"code","4abd73ce":"code","c5e3767e":"code","1d2983e7":"code","372dc9ea":"code","f2f7c81d":"code","c36851ff":"code","175137ac":"code","3a576c57":"code","f4237c9d":"code","81987dd9":"code","dcc35399":"code","8d1e6931":"code","434f3e2f":"code","49f726d7":"code","9b92a741":"code","f1b666d3":"code","e07dcb51":"code","08161481":"code","e80afca6":"code","ba449fd2":"code","246af427":"code","4bfe789d":"code","cce31c44":"code","6fb835e8":"code","91af9a20":"code","99fdd7f0":"code","e81a815b":"code","bb19f252":"code","c2aeccca":"code","ac24d33b":"code","b0b3b4ba":"code","e3dbc8bd":"code","7aad7e3a":"code","70042fd7":"code","88e12ba5":"code","7a69d5de":"code","553fed56":"code","a7a8bbc5":"code","e4cd4179":"code","4ef64cd4":"code","dbb369ef":"code","0280e01e":"code","22d2d13f":"code","f5d7d81c":"code","70a56d44":"code","e839c6ec":"code","51ae0cc1":"code","46a7c856":"code","40b52223":"code","f7a3ca1c":"markdown","01012bd2":"markdown","1b02813b":"markdown","11c67248":"markdown","f298ad71":"markdown","403c287b":"markdown","994872d3":"markdown","3b9415bc":"markdown","47c277e5":"markdown","5a246725":"markdown","9c4042f0":"markdown","bef5bfa4":"markdown","c3691822":"markdown","24563249":"markdown","36b9fa22":"markdown","e8134e9b":"markdown","3b873021":"markdown","41274ed3":"markdown","a9e0c4b8":"markdown","7af1ed4e":"markdown","904f12b5":"markdown","22b11a86":"markdown","60196bbf":"markdown","8a2d5d33":"markdown","d460ce68":"markdown","77e02805":"markdown","15b6ff37":"markdown","a00b1de2":"markdown","2e8a95d7":"markdown","db6e0b16":"markdown","8537b8bd":"markdown","b5289a9a":"markdown","046d9f53":"markdown","73ba4677":"markdown","7a629886":"markdown","a76eeba6":"markdown","13bb2897":"markdown","6f5ef5ff":"markdown","4b2206d8":"markdown"},"source":{"e2e3ebd0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nfrom IPython.display import display\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 15)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nRANDOM_SEED = np.random.seed(0)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d2fa7061":"df = pd.read_csv('\/kaggle\/input\/california-so2-measures\/California_SO2_Measures.csv', parse_dates = ['Date'])\nprint('Set Memory Usage = {:.2f} MB'.format(df.memory_usage().sum() \/ 1024**2))\ndisplay(df)","738907e9":"df.dtypes","aa9c896c":"def draw_missing_data_table(df):\n    '''\n    Docstring: Returns a datarframe with percent of missing\/nan values per feature\/column\n    \n    Parameters:\n    ------------\n    df: dataframe object\n    \n    Returns:\n    ------------\n    Dataframe containing missing value information\n    '''\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent of NaNs'])\n    return missing_data","838a6f22":"draw_missing_data_table(df)","196bb6c6":"print(df.UNITS.unique())\nprint(df.Source.unique())","f30d1551":"df.drop(['Source','UNITS','Site ID','AQS_PARAMETER_CODE','SITE_LONGITUDE','SITE_LATITUDE'],axis=1,inplace=True)","4c4c1068":"df","f169f533":"df['DAILY_OBS_COUNT'].value_counts()","229f8d3a":"fig, ax = plt.subplots(figsize=(25, 6))\n\nsns.barplot(x=df['COUNTY'].value_counts().index,y=df['COUNTY'].value_counts(),ax=ax,capsize=.2)\n\nax.set_title('County Frequency Sample', fontsize=15)\nax.set_ylabel('Count',fontsize=15)","dcfd2860":"fig, ax = plt.subplots(figsize=(25, 6))\n\nsns.barplot(x=df['Site Name'].value_counts().index,y=df['Site Name'].value_counts(),ax=ax,capsize=.2)\n\nax.set_title('Site Name Frequency Sample', fontsize=15)\nax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha=\"right\")\nax.set_ylabel('Count',fontsize=15)","11abc04d":"fig, ax = plt.subplots(figsize=(25, 6))\n\nsns.barplot(x=\"COUNTY\",\n            y='Daily Max 1-hour SO2 Concentration', \n            data= df.groupby(['COUNTY'] ,as_index=False)['Daily Max 1-hour SO2 Concentration'].mean(),\n            ax=ax,\n            capsize=.2)\nax.set_title('Average Daily Max 1-hour SO2 Concentration for observed Counties', fontsize=15)","7678f923":"df","e1907333":"df.loc[df.Date==pd.to_datetime(\"2019-10-22\")]","e95a6723":"pd.set_option('display.max_rows', 28)\ndisplay(df.pivot_table(index=['COUNTY','Site Name']))\npd.set_option('display.max_rows', 10)","70ec20bd":"COUNTIES = df.COUNTY.unique()\n\nfig, ax = plt.subplots(nrows=int(len(COUNTIES)),figsize=(20, 100))\n\nfor i,county in enumerate(COUNTIES):\n    t = df[['Date','Daily Max 1-hour SO2 Concentration','Site Name']].loc[df.COUNTY==county]\n    pt = t.pivot_table(index=['Date'],columns=['Site Name'],values=['Daily Max 1-hour SO2 Concentration'])\n    \n    pt.plot(ax=ax[i])\n\n    ax[i].text(.5,.9,county,\n        horizontalalignment='center',\n        transform=ax[i].transAxes,fontsize=15)","05abcfd9":"COUNTIES = df.COUNTY.unique()\n\nfig, ax = plt.subplots(nrows=int(len(COUNTIES)),figsize=(16, 60))\n\nfor i,county in enumerate(COUNTIES):\n    t = df[['Date','Daily Max 1-hour SO2 Concentration','Site Name']].loc[df.COUNTY==county]\n    pt = t.pivot_table(index=['Date'],columns=['Site Name'],values=['Daily Max 1-hour SO2 Concentration'])\n    pt = pt.resample(rule = \"M\").mean().ffill()\n    pt.plot(ax=ax[i])\n    \n    #ax[i].set_title(county, fontsize=25)\n    ax[i].text(.5,.9,county,\n        horizontalalignment='center',\n        transform=ax[i].transAxes,fontsize=15)","634e8882":"sacramento = df.loc[df['COUNTY']=='Sacramento']\nsacramento","7552f944":"sacramento.drop(['COUNTY','Site Name'],axis=1,inplace=True)\nsacramento.set_index('Date',inplace=True)","53284392":"t = sacramento[['Daily Max 1-hour SO2 Concentration','DAILY_AQI_VALUE']]","4abd73ce":"fig, ax = plt.subplots(figsize=(20, 5))\n\nt.plot(ax=ax)\nax.set_title('Changes in AQI and So2',fontsize=20)\nax.set_ylabel('Concentration',fontsize=15)","c5e3767e":"t.corr(method='pearson')","1d2983e7":"t = t.resample(rule = \"M\").mean().ffill()","372dc9ea":"fig, ax = plt.subplots(figsize=(16, 5))\n\n#t = sacramento[['Daily Max 1-hour SO2 Concentration','DAILY_AQI_VALUE']]\nt.plot(ax=ax)\nax.set_title('Changes in AQI and So2',fontsize=20)\nax.set_ylabel('Concentration',fontsize=15)","f2f7c81d":"from statsmodels.tsa.seasonal import seasonal_decompose","c36851ff":"plt.rc('figure',figsize=(16,12))\nplt.rc('font',size=15)\n\nresult = seasonal_decompose(sacramento['Daily Max 1-hour SO2 Concentration'],model='additive')\nfig = result.plot()","175137ac":"seasonally_adjusted = result.observed-result.seasonal","3a576c57":"plt.rc('figure',figsize=(16,5))\nplt.rc('font',size=15)\n\nresult.observed.plot(color=['blue'],alpha=0.5)\nseasonally_adjusted.plot(color=['black'])","f4237c9d":"from statsmodels.tsa.stattools import adfuller","81987dd9":"def adf_check(timeseries):\n    \n    result = adfuller(timeseries)\n    print('Augmented Dickey Fuller Test')\n    labels = ['ADF test statistic','p-value','# of lags','No. of obs used']\n    \n    for value,label in zip(result,labels):\n        print(label+\" : \"+str(value))\n        \n    if result[1] <= 0.05:\n        print('String evidence against null hypothesis')\n        print('Reject null hypothesis')\n        print('Data has no Unit root and is Stationary')\n        \n    else:\n        print('Weak evidence against null hypothesis')\n        print('Fail to reject null hypothesis')\n        print('Data has a unit root, it is not stationary')","dcc35399":"adf_check(sacramento['Daily Max 1-hour SO2 Concentration'])","8d1e6931":"from statsmodels.graphics.tsaplots import plot_acf,plot_pacf","434f3e2f":"acf = plot_acf(sacramento['Daily Max 1-hour SO2 Concentration'], lags=50)\nprint(\"Concave shade is the Confidence Interval~95%\")","49f726d7":"pacf = plot_pacf(sacramento['Daily Max 1-hour SO2 Concentration'], lags=50)","9b92a741":"from fbprophet import Prophet\nfrom pandas.tseries.offsets import DateOffset","f1b666d3":"pt = df.loc[df['COUNTY']=='Sacramento']","e07dcb51":"t = pd.DataFrame()\nt['ds'] = pt['Date']\nt['y'] = pt['Daily Max 1-hour SO2 Concentration']","08161481":"t.reset_index(inplace=True,drop=True)\nt.set_index('ds',inplace=True)","e80afca6":"future_dates = [t.index[-1] + DateOffset(days=x) for x in range(1,14)] #adding 14 days(2 weeks) to the dataset","ba449fd2":"t.tail()","246af427":"future_dates","4bfe789d":"future_df = pd.DataFrame(index=future_dates,columns=t.columns)\n\nfinal_df = pd.concat([t,future_df])\nfinal_df.index.names = ['ds']\nfinal_df.reset_index(inplace=True)\n\ndisplay(final_df)","cce31c44":"import altair as alt\n#alt.renderers.enable('notebook')","6fb835e8":"def fit_predict_model(dataframe, interval_width = 0.99, changepoint_range = 0.8):\n    m = Prophet(daily_seasonality = False, yearly_seasonality = False, weekly_seasonality = False,\n                seasonality_mode = 'additive', \n                interval_width = interval_width,\n                changepoint_range = changepoint_range)\n    m = m.fit(dataframe)\n    forecast = m.predict(dataframe)\n    forecast['fact'] = dataframe['y'].reset_index(drop = True)\n    return forecast\n    \npred = fit_predict_model(final_df)","91af9a20":"def detect_anomalies(forecast):\n    forecasted = forecast[['ds','trend', 'yhat', 'yhat_lower', 'yhat_upper', 'fact']].copy()\n    #forecast['fact'] = df['y']\n\n    forecasted['anomaly'] = 0\n    forecasted.loc[forecasted['fact'] > forecasted['yhat_upper'], 'anomaly'] = 1\n    forecasted.loc[forecasted['fact'] < forecasted['yhat_lower'], 'anomaly'] = -1\n\n    #anomaly importances\n    forecasted['importance'] = 0\n    forecasted.loc[forecasted['anomaly'] ==1, 'importance'] = \\\n        (forecasted['fact'] - forecasted['yhat_upper'])\/forecast['fact']\n    forecasted.loc[forecasted['anomaly'] ==-1, 'importance'] = \\\n        (forecasted['yhat_lower'] - forecasted['fact'])\/forecast['fact']\n    \n    return forecasted\n\npred = detect_anomalies(pred)","99fdd7f0":"def plot_anomalies(forecasted):\n    interval = alt.Chart(forecasted).mark_area(interpolate=\"basis\", color = '#7FC97F').encode(\n    x=alt.X('ds:T',  title ='date'),\n    y='yhat_upper',\n    y2='yhat_lower',\n    tooltip=['ds', 'fact', 'yhat_lower', 'yhat_upper']\n    ).interactive().properties(\n        title='Anomaly Detection'\n    )\n\n    fact = alt.Chart(forecasted[forecasted.anomaly==0]).mark_circle(size=15, opacity=0.7, color = 'Black').encode(\n        x='ds:T',\n        y=alt.Y('fact', title='Concentration'),    \n        tooltip=['ds', 'fact', 'yhat_lower', 'yhat_upper']\n    ).interactive()\n\n    anomalies = alt.Chart(forecasted[forecasted.anomaly!=0]).mark_circle(size=30, color = 'Red').encode(\n        x='ds:T',\n        y=alt.Y('fact', title='Concentration'),    \n        tooltip=['ds', 'fact', 'yhat_lower', 'yhat_upper'],\n        size = alt.Size( 'importance', legend=None)\n    ).interactive()\n\n    return alt.layer(interval, fact, anomalies)\\\n              .properties(width=870, height=450)\\\n              .configure_title(fontSize=20)\n              \nplot_anomalies(pred)","e81a815b":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nfrom sklearn.preprocessing import MinMaxScaler\ntf.random.set_seed(0)","bb19f252":"dataset = sacramento[['Daily Max 1-hour SO2 Concentration','DAILY_AQI_VALUE']]\ndisplay(dataset)","c2aeccca":"shift_difference = dataset['Daily Max 1-hour SO2 Concentration']-dataset['Daily Max 1-hour SO2 Concentration'].shift(1)\nshift_difference.isnull().sum()","ac24d33b":"features_considered = ['Daily Max 1-hour SO2 Concentration', 'DAILY_AQI_VALUE']","b0b3b4ba":"features = sacramento[features_considered]\nfeatures.head()","e3dbc8bd":"dataset = features.values","7aad7e3a":"def multivariate_data(dataset, target, start_index, end_index, history_size,\n                      target_size, step, single_step=False):\n    data = []\n    labels = []\n\n    start_index = start_index + history_size\n    if end_index is None:\n        end_index = len(dataset) - target_size\n\n    for i in range(start_index, end_index):\n        indices = range(i-history_size, i, step)\n        data.append(dataset[indices])\n\n        if single_step:\n            labels.append(target[i+target_size])\n        else:\n            labels.append(target[i:i+target_size])\n\n    return np.array(data), np.array(labels)","70042fd7":"# I wil be considering past 100 days of the past (our acf gave 6)\npast_history = 100\n\n# Predicting 4 days into the future\nfuture_target = 4\n\n# Training and Validation Split (Data.len=243)\nTRAIN_SPLIT = 220\n\nSTEP=1\n\nx_train_multi, y_train_multi = multivariate_data(dataset = dataset, \n                                                 target = dataset[:, 0], \n                                                 start_index = 0,\n                                                 end_index = TRAIN_SPLIT, \n                                                 history_size = past_history,\n                                                 target_size = future_target, \n                                                 step = STEP)\n\nx_val_multi, y_val_multi = multivariate_data(dataset = dataset, \n                                             target = dataset[:, 0],\n                                             start_index = TRAIN_SPLIT-past_history, \n                                             end_index = None, \n                                             history_size = past_history,\n                                             target_size = future_target, \n                                             step = STEP)","88e12ba5":"print ('Single window of past history : {}'.format(x_train_multi[0].shape))\nprint ('\\n Target temperature to predict : {}'.format(y_train_multi[0].shape))","7a69d5de":"x_train_multi.shape,y_train_multi.shape","553fed56":"x_val_multi.shape,y_val_multi.shape","a7a8bbc5":"BUFFER_SIZE = 1000\nBATCH_SIZE = 128\n\ntrain_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\ntrain_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n\nval_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\nval_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()","e4cd4179":"multi_step_model = tf.keras.models.Sequential()\nmulti_step_model.add(tf.keras.layers.LSTM(8,\n                                          return_sequences=True,\n                                          input_shape=(100,2)))\nmulti_step_model.add(tf.keras.layers.LSTM(8, activation='relu'))\nmulti_step_model.add(tf.keras.layers.Dense(4))\n\nmulti_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')","4ef64cd4":"EPOCHS = 10\nEVALUATION_INTERVAL = 10\n\nmulti_step_history = multi_step_model.fit(train_data_multi, epochs=EPOCHS,\n                                          steps_per_epoch=EVALUATION_INTERVAL,\n                                          validation_data=val_data_multi,\n                                          validation_steps=50\n                                         )","dbb369ef":"def plot_train_history(history, title):\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(len(loss))\n\n    plt.figure(figsize=(12,5))\n\n    plt.plot(epochs, loss, 'b', label='Training loss')\n    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n    plt.title(title)\n    plt.legend()\n\n    plt.show()","0280e01e":"plot_train_history(multi_step_history, 'Multi-Step Training and validation loss')","22d2d13f":"def create_time_steps(length):\n    return list(range(-length, 0))\n\ndef multi_step_plot(history, true_future, prediction):\n    plt.figure(figsize=(12, 6))\n    num_in = create_time_steps(len(history))\n    num_out = len(true_future)\n\n    plt.plot(num_in, np.array(history[:, 1]), label='History')\n    plt.plot(np.arange(num_out)\/STEP, np.array(true_future), 'bo',\n           label='True Future')\n    if prediction.any():\n        plt.plot(np.arange(num_out)\/STEP, np.array(prediction), 'ro',\n                 label='Predicted Future')\n    plt.legend(loc='upper left')\n    plt.show()","f5d7d81c":"for x, y in val_data_multi.take(3):\n    multi_step_plot(x[0], y[0], multi_step_model.predict(x)[0])","70a56d44":"!git clone https:\/\/github.com\/cknd\/pyESN.git","e839c6ec":"from pyESN.pyESN import ESN ","51ae0cc1":"data = sacramento['Daily Max 1-hour SO2 Concentration'].values","46a7c856":"n_reservoir= 500\nsparsity=0.2\nrand_seed=23\nspectral_radius = 1.2\nnoise = .0005\n\n\nesn = ESN(n_inputs = 1,\n      n_outputs = 1, \n      n_reservoir = n_reservoir,\n      sparsity=sparsity,\n      random_state=rand_seed,\n      spectral_radius = spectral_radius,\n      noise=noise)\n\ntrainlen = 193\nfuture = 1\nfutureTotal= 50\npred_tot=np.zeros(futureTotal)\n\nfor i in range(0,futureTotal,future):\n    pred_training = esn.fit(np.ones(trainlen),data[i:trainlen+i])\n    prediction = esn.predict(np.ones(future))\n    pred_tot[i:i+future] = prediction[:,0]","40b52223":"plt.figure(figsize=(15, 8))\nplt.plot(range(0,trainlen+futureTotal),data[0:trainlen+futureTotal],'b',label=\"Data\", alpha=0.6)\n#plt.plot(range(0,trainlen),pred_training,'.g',  alpha=0.3)\nplt.plot(range(trainlen,trainlen+futureTotal),pred_tot,'k',  alpha=0.8, label='Free Running ESN')\n\nlo,hi = plt.ylim()\nplt.plot([trainlen,trainlen],[lo+np.spacing(1),hi-np.spacing(1)],'k:', linewidth=4)\n\nplt.title(r'Ground Truth and Echo State Network Output', fontsize=20)\nplt.xlabel(r'Time (Days)', fontsize=15,labelpad=10)\nplt.ylabel(r'Concentration', fontsize=15,labelpad=10)\nplt.legend(fontsize='large', loc='best')\nsns.despine()","f7a3ca1c":"<font size=\"3\"> Looking Statically at the observed Window (from 1-1-19:31-10-19) ```Contra Costa County``` reports **highest levels of Average So2 Concentration**","01012bd2":"<font size=\"3\"> ```Oakland West``` reports more frequently than any other site in the Dataset","1b02813b":"# Forecasting\n\n<font size=\"3\"> For forecasting we'll use Recurrent Neural Networks","11c67248":"<font size=\"3\">If we see PACF plot there are many instances where correlation is above upper confidence band as PACF calculates correlations of lags of time series with residuals and our series itself is linear combination of residual and its lagged values.Hence we can get good correlation for near as well as past lags.","f298ad71":"- <font size=\"3\">The timeline is different for every County since the observation taken on a single day belongs to a specific county and site name\n- <font size=\"3\">Since the Timeline of each County is different we're gonna need different scales on the y-axis to visualize the timeseries trends in So2 concentration","403c287b":"<font size=\"3\"> With low Resolution we can see the overall trend exhibited by each county and site over the the months. This help us to analyze which months had more Concentration and devise out probable reasons according to that (like Vehicular Pollution, Chemical Factors, Industries)","994872d3":"- <font size=\"3\"> We cannot visualize the change in So2 concentration just by plotting the data. Since at a particular date multiple readings from a county and site are recorded. What we do here is visualize the change in So2 concentration by county and for a particular county see the changes in sites where the reading is recorded.","3b9415bc":"# Interpretations of the above Sub-plots by County\n\n- <font size=\"3\">```Alameda County```: Oakland West was the single site name in this county. Daily So2 concentrations in the county varies vividly. The county sees local maxima at the end of every month except for the months of April and September, where we see two sudden spikes of significant magnitude. The So2 concentration saw global maxima in March. A possible reason could be increased vehicular movement due to some sort of festival or event happening in the city. As we saw major sources of So2 include IC vehicles, petrol refineries, and chemical factories.\n    \n- <font size=\"3\">```Contra Costa```: The So2 activity in this county is monitored by 6 different sites namely Bethel Island, Concord, Crockett, Martinez, Richmond, San Pablo. Concord area sees a spike in the concentration in late January and mid-September, Bethel Island sees a spike in the 1st week of August. All other sites report concentration to be close to 0, which signifies excellent air quality with little or no sign of sulfur in the air.\n \n- <font size=\"3\">```Fresno``` - This county only has a single site, Garland. It reports fairly similar behavior in the concentration till mid-April and after that, the trend is increasing linearly with numerous local maximas and global maxima at the start of July.\n    \n- <font size=\"3\">```Humboldt``` - Jacobs is the only site reporting the concentration of So2. It reports ~0.5 till the month of April with few curves between. Starting May it drops substantially and gets below 0 till mid of June. This behavior is a likely cause of mass activity that took place in the county, like vacations or refineries coming to a halt.\n    \n- <font size=\"3\">```Imperial``` - Calexico Ethel-street is the sole site in this county where the observations are made. The trend here is quite flat and troughs and crests occur in every month observed.\n    \n- <font size=\"3\">```Inyo``` - Owens Valley Lab sees a pretty flat line till June and sudden spike in the concentration in mid-August, post that it goes to the original flat line estimate\n    \n- <font size=\"3\">```Los Angeles``` - 3 sites report the So2 concentration in Los Angeles: Hastings, Long Beach, and North-Main Street. All three of the sites report a sharp spike towards the end of January. Long Beach shows more So2 activity with again spiking at March-end and in multiple days of April. So2 activity fades out after May-June.\n    \n- <font size=\"3\">```Riverside``` - Rubidoux is the sole site in this county where the observations are made. The trend here is quite flat and troughs and crests occur in every month observed.\n    \n- <font size=\"3\">```Sacramento```- Only one site reports the So2 concentration activity in the county which is Sacramento-Del Paso Manor. It varies quite similarly with troughs and crests, stretching till May. After that, the trend tends to increase linearly with more local maxima getting a global maxima at the end of July. After July, due to less activity the slope decreases.\n    \n- <font size=\"3\">```San Bernadino```- Fontana, Trona, Victorville. We can see breakpoints in the observation that's because the readings weren't taken for that missing time frame for a particular datum. Fontana sees a flat trend throughout the observations while Victorville sees a linear slope increasing trend. Trona, however, observed regular sharp spikes throughout with an increasing trend.  ","47c277e5":"# Trend Analysis of Daily So2 concentration","5a246725":"## Autocorrelation\n\n<font size=\"3\"> From autocorrelation we can see the correlation of a partcular timestep with the previous timesteps followed. This will help us analyze if on a particular day the reading closely resemble to the day before","9c4042f0":"---","bef5bfa4":"# Sacramento County","c3691822":"---","24563249":"<font size=\"3\">Since p<0.05 We reject the H0 and the Timeseries is Stationary. Time series are stationary if they do not have trend or seasonal effects so we can proceed with this finding","36b9fa22":"<font size=\"3\"> Lets also look at the plot when sampled Monthly.","e8134e9b":"<font size=\"3\">Seasonally adjusted series contain the remainder component as well as the trend-cycle. Therefore, they are not \u201csmooth\u201d, and \u201cdownturns\u201d or \u201cupturns\u201d can be misleading. If the purpose is to look for turning points in a series, and interpret any changes in direction, then it is better to use the trend-cycle component rather than the seasonally adjusted data.","3b873021":"<font size=\"3\"> There aren't many Outliers in this dataset and also the ones which are ,they are relatively close to the likelihood of the rest. So we can proceed without removing them.","41274ed3":"# First Impressions of the Dataset\n\n- <font size='3'>The Data is ***Sampled Daily***\n- <font size='3'>The data contains Observations from 15 **Counties** and different **Sites** in them (the observatories) denoted by **SiteID**.\n- <font size='3'>This data belongs to central repository of **Air Quality System** (source) \n- <font size='3'>Observation are measured on **ppb scale** (parts per billion)\n- <font size='3'>Alongwith **Daily Concentration of So2** other timeseries feature given is the **AQI**","a9e0c4b8":"<font size=\"3\">Since there's only one Site in the Sacramento County, I'll drop county and site name from the dataframe above","7af1ed4e":"## Using Echo State Networks\n\n<font size=\"3\">Echo state networks are a relatively new invention, it is essentially a recurrent neural network with a loosely connected hidden layer, called a \u2018reservoir\u2019 which works surprisingly well in the presence of chaotic time series. \n    \n<font size=\"3\">In order for the ESN principle to work, the reservoir must have the echo state property (ESP), which relates asymptotic properties of the excited reservoir dynamics to the driving signal. Intuitively, the ESP states that the reservoir will asymptotically wash out any information from initial conditions. The ESP is guaranteed for additive-sigmoid neuron reservoirs, if the reservoir weight matrix (and the leaking rates) satisfy certain algebraic conditions in terms of singular values.","904f12b5":"# Outlier Detection\n\n- <font size=\"3\"> Outliers are sudden changes in the dataset (timeseries) which probably weren't part of the distribution and their Likelihood is small. If they exists in large quantities (not larger than the observed data) then they hamper the predictions and in-turn the model.\n- <font size=\"3\"> So its best to identify outliers and remove them before they create havoc in Modelling process","22b11a86":"<font size=\"3\"> We can see here that orginal observations and Seasonally adjusted one's are similar, which can be explained by the fact that we doesn't have much date (multiple years) to actually see seasonality change.","60196bbf":"- <font size=\"3\"> Echo state networks works pretty well in Stock Data, which tends to predict for the next hour or next day also called short-term prediction\n- <font size=\"3\"> ESN has a disadvantage of loosing track if predicted for longer term with relaively less data\n- <font size=\"3\"> ESN implemented with Differential Neural Computers (helps in saving long-range dependencies or past states) can prove really good for long-term predictions","8a2d5d33":"<font size=\"3\"> We can observe here that 10-5-19,29-5-19,4-6-19,6-6-19,31-7-19 are Outliers (from the above plots).","d460ce68":"<font size=\"3\"> Since we saw a good correlation between AQI and So2 concentrations, I'll be using both the features to predict the timeseries of So2 since both of them are standalone feature with different semantics and its evident that increase in AQI might result an increase in So2 levels in the atmosphere.","77e02805":"---","15b6ff37":"### ETS (Error-Trend-Seasonality) Decompose","a00b1de2":"- <font size=\"3\">Source has nothing to do with So2 concentration and AQI values\n- <font size=\"3\">SiteID also contributes nothing to So2 concentration\n- <font size=\"3\">Since County has been given, there seems no need for latitude and longitude","2e8a95d7":"| Feature | Description|\n| --- | --- |\n| Date | Date of the Observation |\n| --- | --- |\n| Source | Source of the data |\n| --- | --- |\n| Site ID | ID of the site |\n| --- | --- |\n| POC | Parameter Occurence Code(convenient method for tracking multiple measurements of a single parameter at one site.) |\n| --- | --- |\n| Daily Max 1-hour SO2 Concentration | Daily Max 1-hour SO2 Concentration |\n| --- | --- |\n| UNITS | Unit of Measurement eg ppm,ppb |\n| --- | --- |\n| DAILY_AQI_VALUE | Air quality index value (daily sampled) |\n| --- | --- |\n| Site Name | Name of the site |\n| --- | --- |\n| DAILY_OBS_COUNT | Number of daily Observation taken |\n| --- | --- |\n| PERCENT_COMPLETE | (not known) |\n| --- | --- |\n| AQS_PARAMETER_CODE | AQS Parameter Code (Internal) |\n| --- | --- |\n| COUNTY | The Observed County |\n| --- | --- |\n| SITE_LATITUDE\/LONGITUDE | Site's location coordinates|","db6e0b16":"- <font size=\"3\">Most of the observations belong to ```Contra Costa```, ```Santa Barbara``` and ```San Bernadino``` (top 3) ","8537b8bd":"<font size=\"3\">As per above plot we have good correlation upto 6th lag, this is the lag after which plot cuts the upper confidence interval. Order q of series obtained by the plot is 6, which is correct as we had defined our series with linear combination of residuals upto lag 6.\nThus this proves that ACF correctly predicted order of our MA(6) series.","b5289a9a":"<font size=\"3\"> This clearly means the reading stay correlated upto 6 days (and highly correlated upto 2 days)","046d9f53":"---","73ba4677":"### So2 when Sampled Daily,visualized","7a629886":"## Check for stationarity - Augmented Dickey-Fuller test\n\n<font size=\"3\">The Augmented Dickey-Fuller test is a type of statistical test called a unit root test.\n\n<font size=\"3\">The intuition behind a unit root test is that it determines how strongly a time series is defined by a trend.\n\n<font size=\"3\">There are a number of unit root tests and the Augmented Dickey-Fuller may be one of the more widely used. It uses an autoregressive model and optimizes an information criterion across multiple different lag values.\n\n<font size=\"3\">The null hypothesis of the test is that the time series can be represented by a unit root, that it is not stationary (has some time-dependent structure). The alternate hypothesis (rejecting the null hypothesis) is that the time series is stationary.\n\n<font size=\"3\">Null Hypothesis (H0): If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.\n<font size=\"3\">Alternate Hypothesis (H1): The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.\n<font size=\"3\">We interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary).\n\n- <font size=\"3\">**p-value > 0.05**: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n- <font size=\"3\">**p-value <= 0.05**: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.","a76eeba6":"### Checking for a missing sequence","13bb2897":"<font size=\"3\">There's essentially no Timestep missing in Sacramento subset, since the only reported NaN belongs to the first observation.","6f5ef5ff":"<font size=\"3\">We can surely see a **strong positive correlation** between AQI and So2 concentrations. Since the date object is sorted we can see a relationship between the two using a lineplot.","4b2206d8":"---"}}