{"cell_type":{"eeba4c62":"code","bd787181":"code","36419a13":"code","00573e36":"code","08bcb78c":"code","e18671f2":"code","eb3b9667":"code","26c1cc27":"code","1bd9213e":"code","a3f5fec6":"code","cca28cfe":"code","8c57ccd0":"code","8e429aba":"code","55c1fd66":"code","f291d576":"code","8499ab2d":"code","05e6d039":"code","b706f76b":"code","964c729d":"code","b362437d":"code","72be994b":"code","0f1de6b4":"code","807f6c73":"code","ad74663a":"code","b0efa84e":"code","ca1de2a6":"code","23f6e9ce":"code","8862ba75":"code","08869f3c":"code","7e3b212b":"code","8a87d05a":"code","884603b8":"code","701f0982":"code","6ffa7a36":"code","cb5aed12":"code","984e835e":"code","be469c02":"code","7318e8c3":"code","4e2e40bd":"code","f774a7c8":"code","0b4c1bcd":"code","3549db53":"code","6b993cae":"code","0403dd64":"code","d0963ea1":"code","f63390f5":"code","d322e516":"code","5f6eea2b":"code","117a2697":"code","8d86d97d":"code","ec10e2ef":"code","b98de89b":"code","d7845c71":"code","3a1b5c10":"markdown","6cde5c6f":"markdown","5a08da9d":"markdown","83e037fc":"markdown"},"source":{"eeba4c62":"# # AI is a big umbrella that covers everything which makes Machine intelligent\n# # AI -> ML -> Deep Learning\n\n# AI is giving artificial human int. to m\/ch\n# like, doing the whole task, recognizing if person dance, move or anything or drive a car, anything\n\n# It can be narrow and broad.\n# Broad have a lot scope, like all the task (driving car) done by the system\n# narrow is limited to some scope\n\n\n","bd787181":"# MACHINE LEARNING:\n\n# comes under AI\n# given data to machine, it is expected to learn and work on own, provided some training.\n# Training in machine learning entails giving a lot of data to the algorithm \n# and allowing it to learn more about the processed information.\n","36419a13":"# 3 different learning styles:\n# supervised\n# unsupervised\n# semi-supervised\n\n# reinforcement","00573e36":"# 1. Supervised Learning\n# Input data is called training data and has a known label or result such as spam\/not-spam or a stock price at a time.\n\n# A model is prepared through a training process in which it is required to make \n# predictions and is corrected when those predictions are wrong. \n# The training process continues until the model achieves a desired level of accuracy on the training data.\n\n# Example problems are classification and regression.\n\n# Example algorithms include: Logistic Regression and the Back Propagation Neural Network.","08bcb78c":"# Linear Regression\n\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# Generating some 'random' data\nnp.random.seed(0)\nX = np.random.randint(low=0, high=50, size=(10))         # Array of 100 values \n\n# Generate 100 dependent terms\ny = np.random.randint(low=0, high=50, size=(10)) * 0.98        # Actual values of Y\n\n# Create pandas dataframe to store our X and y values\ndf = pd.DataFrame(\n    {'X': X,\n     'y': y}\n)\n\n# Show the first five rows of our dataframe\ndf.head()","e18671f2":"# Now, we need mean of x and y values:\n\nmean_x = np.mean(X)\nmean_y = np.mean(y)\nprint(mean_x, mean_y)","eb3b9667":"# Now, for slope, 'm'\nnumerator = 0\ndenomenator = 0\n\nfor i in range(len(df)):\n  numerator += (X[i]-mean_x)*(y[i]-mean_y)\n  denomenator += (X[i]-mean_x)**2\n\nslope = numerator\/denomenator\nprint(slope)\n# line_equation:\n# y = slope * X + c\n# that is, mean_y = slope * mean_x + c\n# finding c \n\nc = mean_y - (slope*mean_x)\nprint(c)\n","26c1cc27":"# for plotting from start to end for y pred\n\nmax_x = np.max(X)+10\nmin_x = np.min(X) - 10\n\nx = np.linspace(min_x, max_x)\ny_line = (slope * x) + c\nprint(y_line)\n\nplt.plot(x,y_line, color=\"red\", label=\"Regression line\")\nplt.scatter(X,y, color=\"green\", label=\"Plot of data\" )\n\nplt.xlabel(\"x values\")\nplt.ylabel(\"y values\")\nplt.legend()\nplt.show()","1bd9213e":"# get y_pred only for our x values\n\nmax_x = np.max(X)+10\nmin_x = np.min(X) - 10\n\ny_line = (slope * X) + c\nprint(y_line)\nplt.plot(X,y_line, color=\"red\", label=\"Regression line\")\nplt.scatter(X,y, color=\"green\", label=\"Plot of data\" )\n\nplt.xlabel(\"x values\")\nplt.ylabel(\"y values\")\nplt.legend()\nplt.show()","a3f5fec6":"# R Square\n\n# ss_t = total sum of Square\n# ss_r = total sum of sq of residuals\n\nss_t=0\nss_r=0\n\nfor i in range(len(df)):\n  ss_t += (y_line[i] - mean_y) ** 2\n  ss_r += (y[i] - mean_y) ** 2\n\naccuracy = 1 - (ss_t \/ ss_r)\nprint(accuracy)","cca28cfe":"#  R value close to 1 depicts that our regression \n#  model is close enough to the actual value and good\n","8c57ccd0":"\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nX = X.reshape((len(df),1))\n\n# model creation:\nreg = LinearRegression()\n\n# Fitting the training data\nreg = reg.fit(X,y)\n\n# PRedicting y values using model (here, y_pred is same as our y_line)\nY_pred = reg.predict(X)\n\n# R sq value calculation\naccuracy = 1 - reg.score(X,y)\n\nprint(accuracy)\n\n","8e429aba":"# # For more than two independent variables\n\n# predictors = ['TV', 'Radio']\n# X = advert[predictors]\n# y = advert['Sales']\n\n# # Initialise and fit model\n# lm = LinearRegression()\n# model = lm.fit(X, y)","55c1fd66":"import pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","f291d576":"titanic_data = pd.read_csv('..\/input\/titanic-dataset\/titanic.csv')\ntitanic_data.head()","8499ab2d":"len(titanic_data)","05e6d039":"# analyzing the data","b706f76b":"sns.countplot(x=\"Survived\", data = titanic_data)","964c729d":"sns.countplot(x=\"Survived\", hue = \"Sex\", data = titanic_data)","b362437d":"sns.countplot(x=\"Survived\", hue = \"Pclass\", data = titanic_data)","72be994b":"titanic_data[\"Age\"].plot.hist()","0f1de6b4":"titanic_data[\"Fare\"].plot.hist()","807f6c73":"# Since fare size is only from 0 to 100, we can change the scale by setting the fig and bin size\ntitanic_data[\"Age\"].plot.hist(bins = 20, figsize = (10,5) )","ad74663a":"titanic_data.info()","b0efa84e":" sns.countplot(x=\"Parents\/Children Aboard\", data = titanic_data)","ca1de2a6":" sns.countplot(x=\"Siblings\/Spouses Aboard\", data = titanic_data)","23f6e9ce":"# Data Wrangling\n\n# removing the unnecessary values\n","8862ba75":"titanic_data.isnull()","08869f3c":"titanic_data.isnull().sum()","7e3b212b":"# the dataset is already clean, otherwise, it would be getting some doifferent colors showing the null values\n\nsns.heatmap(titanic_data.isnull(), yticklabels = False)","8a87d05a":"# to fix null, we can add some value, like mean or 0 or 1 or mode, or can drop them\n\nsns.boxplot(x=\"Pclass\", y=\"Age\", data=titanic_data)","884603b8":"#  titanic_data.drop('col_name', axis=1, inplace=True)\n\n# axis=0 for row, and 1for column","701f0982":"# drop na values:\n\n# titanic_data.dropna(inplace=True)","6ffa7a36":"# to convert the \"sex\" column to words:\npd.get_dummies(titanic_data['Sex'])","cb5aed12":"# to convert the \"sex\" column to words:\nsex = pd.get_dummies(titanic_data['Sex'], drop_first=True)\ntitanic_data.head(5)\n\n# similar can be done for the column with 3 classes, like","984e835e":"p_class = pd.get_dummies(titanic_data['Pclass'], drop_first=True)\ntitanic_data.head(5)","be469c02":"titanic_data=pd.concat([titanic_data, sex,p_class], axis=1 )","7318e8c3":"titanic_data","4e2e40bd":"titanic_data.info()","f774a7c8":"titanic_data.drop(['Sex', 'Pclass'], axis=1, inplace= True)","0b4c1bcd":"titanic_data.head()","3549db53":"titanic_data.drop(['Name'], axis=1, inplace= True)","6b993cae":"titanic_data.head()","0403dd64":"X = titanic_data.drop(['Survived'], axis=1)\ny = titanic_data['Survived']","d0963ea1":"from sklearn.model_selection import train_test_split","f63390f5":"\n# If you don't specify the random_state in the code, then every\n# time you run(execute) your \n# code a new random value is generated and the train and test \n# datasets would have different values each time.\n\n# However, if a fixed value is assigned like random_state = 0 or 1 or 42 or \n# any other integer then no matter how many times you execute \n# your code the result would be the same .i.e, same values in\n# train and test datasets.\n\n# Random state ensures that the splits that you generate are \n# reproducible. Scikit-learn uses random permutations to generate the splits. \n# The random state that you provide is used as a seed to the random number generator. \n# This ensures that the random numbers are generated in the same order.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n","d322e516":"from sklearn.linear_model import LogisticRegression\nlog_reg_model = LogisticRegression(solver='liblinear')\n","5f6eea2b":"log_reg_model.fit(X_train, y_train)","117a2697":"predictions = log_reg_model.predict(X_test)","8d86d97d":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions))","ec10e2ef":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, predictions))","b98de89b":"from sklearn.metrics import accuracy_score\na = accuracy_score(y_test, predictions)\na","d7845c71":"accuracy = a * 100\naccuracy","3a1b5c10":"**Linear Regression using scikit learn**","6cde5c6f":"Training the data","5a08da9d":"# Logistic Regression","83e037fc":"# **Linear Regression**"}}