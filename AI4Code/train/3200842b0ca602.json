{"cell_type":{"2fb9b011":"code","0c2dd984":"code","783fdd46":"code","a7d76f04":"code","230c5681":"code","bc2786b6":"code","5885af26":"code","3e9e600b":"code","b8747cfe":"code","ff67a65b":"code","72f91187":"code","35b57301":"code","139936cc":"code","145fe229":"code","b28847f0":"code","eb59efec":"code","d64a8070":"code","0076a7c4":"code","412b815c":"markdown","8164667b":"markdown","da023991":"markdown","f8eb6fff":"markdown","8a47922c":"markdown","8d5ee55b":"markdown","fccd7914":"markdown","d8ac2223":"markdown","a59c76c1":"markdown","82fcf996":"markdown","a3d7ae19":"markdown","83457f6d":"markdown","15e3d47d":"markdown","14b9dfad":"markdown","afea4c99":"markdown"},"source":{"2fb9b011":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nimport os\n\nfrom pandas import Series, DataFrame\nfrom pylab import rcParams\nfrom sklearn import preprocessing\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import classification_report\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","0c2dd984":"met_df = pd.read_csv('\/kaggle\/input\/did-it-rain-in-seattle-19482017\/seattleWeather_1948-2017.csv')\nprint(met_df.head()); print(); print()\nmet_df.info()","783fdd46":"met_df.describe(include = 'all')","a7d76f04":"met_df.isna().sum()","230c5681":"P_median = met_df.PRCP.median()\nR_mode   = met_df.RAIN.mode()[0]\n\nmet_df.PRCP.fillna(P_median, inplace = True)\nmet_df.RAIN.fillna(R_mode, inplace = True)\n\nmet_df.isna().sum()","bc2786b6":"from sklearn.preprocessing import LabelEncoder\nRAIN_encode = LabelEncoder().fit_transform(met_df.RAIN)\nRAIN_encode","5885af26":"met_df['RAIN'] = RAIN_encode\n\nmet_df.describe(include = 'all')","3e9e600b":"%matplotlib inline\nrcParams['figure.figsize'] = 6, 5\nsb.set_style('whitegrid')\n\nsb.pairplot(met_df, palette = 'husl', hue = 'RAIN')\nplt.show()","b8747cfe":"sb.heatmap(met_df.corr(), vmin=-1, vmax=1, annot=True, cmap = 'RdBu_r')\nplt.show()","ff67a65b":"sb.scatterplot(x = 'TMIN', y ='TMAX', data = met_df, hue = 'RAIN')\nplt.show()","72f91187":"fig, axis = plt.subplots(1, 2,figsize=(10,4))\nsb.boxplot(x = 'RAIN', y ='TMAX', data = met_df, ax = axis[0], showfliers = False)\nsb.boxplot(x = 'RAIN', y ='TMIN', data = met_df, ax = axis[1], showfliers = False)\nplt.show()","35b57301":"met_df.drop(['TMIN', 'PRCP','DATE'], inplace = True, axis=1)\nmet_df.head()","139936cc":"X_train, X_test, Y_train, Y_test = train_test_split(met_df.drop('RAIN', axis=1),\n                                                   met_df['RAIN'], test_size=0.2, random_state=10)                             \n","145fe229":"all_classifiers = {'Ada Boost': AdaBoostClassifier(),\n                 'Random Forest': RandomForestClassifier(n_estimators=50, min_samples_leaf=1, min_samples_split=2, max_depth=4),\n                 'Gaussian NB': GaussianNB(),\n                 'Logistic Regression': LogisticRegression(solver='liblinear'),#fit_intercept=True,\n                 'Decision Tree' : DecisionTreeClassifier(),\n                  'SVC': SVC()} #probability = False ","b28847f0":"ML_name = []\nML_accuracy = []\nfor Name,classifier in all_classifiers.items():\n    classifier.fit(X_train,Y_train)\n    Y_pred = classifier.predict(X_test)\n    ML_accuracy.append(metrics.accuracy_score(Y_test,Y_pred)) \n    ML_name.append(Name) ","eb59efec":"rcParams['figure.figsize'] = 8, 4\nplt.barh(ML_name, ML_accuracy, color = 'brown')\nplt.xlabel('Accuracy Score', fontsize = '14')\nplt.ylabel('Machine Learning Algorithms', fontsize = '14')\nplt.xlim([0.65, 0.685])\nplt.show()","d64a8070":"criteri       = ['gini', 'entropy']\nmin_samp_lf   = [1, 2, 5, 10]\nmin_samp_splt = [2, 4, 8, 12]\nmaxim_depth   = [2, 4, 8, 12, None]\n\nmax_score = 0\n\nfor c in criteri:\n    for ml in min_samp_lf:\n        for ms in min_samp_splt:\n            for md in maxim_depth:\n                MLA = DecisionTreeClassifier(criterion=c, min_samples_leaf=ml, min_samples_split=ms, max_depth=md)\n                MLA.fit(X_train,Y_train)\n                Y_pred = MLA.predict(X_test)\n                if metrics.accuracy_score(Y_test,Y_pred) > max_score:\n                    max_score, c_best, l_best, s_best, d_best = metrics.accuracy_score(Y_test,Y_pred), c, ml, ms, md\n\nprint('maximum accuracy score, criterion, min_samples_leaf, min_samples_split, max_depth:')\nprint(max_score, c_best, l_best, s_best, d_best)","0076a7c4":"learning_R    = [1, 2, 3]\nrandom_st     = [None, 20]\nn_estimat     = [50, 100]\n\nmax_score = 0\n\nfor lr in learning_R:\n    for rs in random_st:\n        for ne in n_estimat:\n            MLA = AdaBoostClassifier(random_state=rs, learning_rate=lr, n_estimators=ne)\n            MLA.fit(X_train,Y_train)\n            Y_pred = MLA.predict(X_test)\n            if metrics.accuracy_score(Y_test,Y_pred) > max_score:\n                max_score, r_best, l_best, n_best = metrics.accuracy_score(Y_test,Y_pred), rs, lr, ne\n\nprint('maximum accuracy score, random_state, learning_rate, n_estimators:')\nprint(max_score, r_best, l_best, n_best)","412b815c":"### Model Validation:\n\nSee Part 2.\n\n----\n\n## Discussion:\n\n- It seems that tuning hyper-parameters for various MLAs would give us an accuracy score of ~ 68.1%, meaning that our MLAs can predict the rain correctly in 68% of times for test datasets.\n\n- This is better than the baseline: we should be able to predict rain by 50% accuracy only by tossing a coin. Moreover, there are 57% of instances of not rain and 43% instances of rain. So, if we always select not rain, we would get a score of 57%. So far, we improved the accuracy score by 11.1%.\n\n- In part 2, we will utilize advanced feature engineering and subject-matter expertise to increase the accuracy score.\n\n- Note that the limited number of predictors and the highly uncertain and non-linear nature of weather make it very difficult to increase the accuracy score to higher than ~85%. If it was possible, there would not be a need for super complex numerical weather prediction (NWP) models that use thousands of processes and mathematical models and are now very common for weather forecasts with high accuracy.\n\n- I saw that other solutions claimed they reached an accuracy of higher than 90%. This is an artifact and mistake because they did not drop the 'PRCP' variable from the predictor list when using MLAs.\n","8164667b":"## Data Cleaing:\n\n### Step 1: Correcting wrong values or outliers:","da023991":"- Ax expected, TMIN and TMAX are highliy correlated, so we drop TMIN that has lower correlation with RAIN.","f8eb6fff":"### Ada Boost Classifier:","8a47922c":"### Reading the data:","8d5ee55b":"# An Analysis by an Atmospheric Scientist regarding Rain Forecast in Seattle\n\n## Objective:\n\n- The customer would like to know the likelihood of predicting whether it rains or not on a specific day ins Seattle by using MLAs. The input dataset (seattleWeather_1948-2017.csv) provides information on a few Meteorological variables in Seattle from 1948 to 2017.\n\n- The input variables (predictors) are Date, PRCP, TMAX, and TMIN. The last three are numerical continuous variables.\n\n- The outcome or dependent variable is RAIN (boolean).\n\n- This is part 1: I will only use the available predictors (as they are) and will drop the ones that are not independent.\n\n- In part 2, I will perform advanced feature engineering and will extract new variables based on the available predictors in order to improve the accuracy score.\n----\n\nNote that the other solutions to this problem, that I found on this website, did a terrible mistake and kept the variable PRCP (precipitation amount) to predict RAIN (binary rain: did it rain or not). This is cheating: your model would provide a perfect answer when the outcome is included among the predictors. When your model knows the amount of precipitation on a specific day, it can certainly say whether it rains or not. This motivated me to provide a better methodology.\n\n\n### Importing important libraries:","fccd7914":"### Tuning models with hyper parameters:\n\n- Let's see if we can improve a model performance by changing the hyper parameters.\n- We are not going to test all the models, but just a few of them.\n\n\n### Decision Tree:\n","d8ac2223":"### Step 2: Imputing missing values:\n\nThere are only three missing data points for each PRCP and RAIN. So, we use median for PRCP and mode for RAIN to fill in the gaps.","a59c76c1":"### Using a variety of MLAs to get the best results\n\n- The outcome is binary, so we can use Logistic Regression, Decision Tree, or Naive Bayes. \n- We also use ensemble algorithms (such as Random Forest) to see if the score can be improved.","82fcf996":"The description and unit of each variable:\n- DATE = the date of the observation\n- PRCP = the amount of precipitation, in inches\n- TMAX = the maximum temperature for that day, in degrees Fahrenheit\n- TMIN = the minimum temperature for that day, in degrees Fahrenheit\n- RAIN = TRUE if rain was observed on that day, FALSE if it was not","a3d7ae19":"## Implementing MLAs:\n\n### Spliting the data into test and train sets:\n","83457f6d":"### Making sure all predictors are independent\n\n### Insightful plots","15e3d47d":"- Here, each TMAX and TMIN is grouped based on RAIN.\n- Again, we see that TMAX is a better predictor of RAIN.\n\n- We should also drop 'PRCP' variable: if we know the amount of precipitation on each day, we can certainly say whether it rains or not on that day.","14b9dfad":"### Step 3: Converting boolean variable to dummy variable:\n- We should change RAIN from True\/False to 1\/0.\n- We then replace the new variable with the original one.","afea4c99":"The data description makes sense, and the mean, min, and max values of each variable is reasonable meaning there should not be a mistake in the data (such as a very large temperature of 200 F)."}}