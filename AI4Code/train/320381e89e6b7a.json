{"cell_type":{"fa5edff7":"code","7d5381c6":"code","aab8b808":"code","d4722b37":"code","4ad68bdc":"code","9cf8d672":"code","4b025c5b":"code","527e9268":"code","4b70d8b9":"code","4b234dd1":"code","9d345c85":"code","dbd7d46d":"code","3497c521":"code","c94d9134":"code","ed04cd01":"code","901dd47f":"code","5ed94e40":"code","6cdac145":"code","2bdac6ca":"code","8e471979":"code","7b43e219":"code","085b5d62":"code","61239e89":"code","7a76eaee":"markdown","d7cad90a":"markdown","76ccbf0d":"markdown","942b5047":"markdown","773dcf37":"markdown","463a3d46":"markdown","e21c189f":"markdown","9b0dad6d":"markdown","ea688bf6":"markdown","0dc42859":"markdown","4f2aa5b1":"markdown","cb9aebfa":"markdown","b5b5b860":"markdown","9b6c8ca8":"markdown","90b6fef8":"markdown","e81f6a4e":"markdown","b0ed18a2":"markdown","4135d214":"markdown","93108f14":"markdown","e1ecefed":"markdown","fe88d9ec":"markdown"},"source":{"fa5edff7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","7d5381c6":"train_path = \"..\/input\/tabular-playground-series-jun-2021\/train.csv\"\ntrain = pd.read_csv(train_path)\ntrain.head()","aab8b808":"train.drop(\"id\", axis=1, inplace=True)","d4722b37":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny = le.fit_transform(train[\"target\"])\nX = train.drop(\"target\", axis=1).values","4ad68bdc":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                      test_size=0.2, \n                                                      stratify=y,\n                                                      random_state=42)","9cf8d672":"X_train.shape, y_train.shape, X_valid.shape, y_valid.shape ","4b025c5b":"train.iloc[:, :-1] = train.iloc[:, :-1].astype(\"int16\")","527e9268":"def draw_plot_2d(decompose=None, \n                 subset=None,\n                 X_train=X_train, \n                 y_train=y_train):\n    \n    if subset is not None:\n        X_train = X_train[subset, :]\n        y_train = y_train[subset]\n    \n    if decompose is None:\n        decompose_2d = X_train\n    else:\n        decompose_2d = decompose.fit_transform(X_train)\n\n    plt.figure(figsize=(15, 8), dpi=100)\n    sns.scatterplot(x=decompose_2d[:, 0], \n                    y=decompose_2d[:, 1],\n                    hue=[le.classes_[i] for i in y_train]);","4b70d8b9":"pca = PCA(n_components=2, random_state=42)\ndraw_plot_2d(pca)","4b234dd1":"sample_ids = np.random.choice(X_train.shape[0], 10000)","9d345c85":"from sklearn.decomposition import KernelPCA\n\nkernel = KernelPCA(n_components=2, kernel=\"rbf\", n_jobs=-1, copy_X=False)\ndraw_plot_2d(decompose=kernel, subset=sample_ids)","dbd7d46d":"from sklearn.manifold import TSNE\n\ntsne = TSNE()\ndraw_plot_2d(decompose=tsne, subset=sample_ids)","3497c521":"from sklearn.manifold import Isomap\n\niso = Isomap()\ndraw_plot_2d(decompose=iso, subset=sample_ids)","c94d9134":"from sklearn.manifold import LocallyLinearEmbedding\n\nlle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\ndraw_plot_2d(decompose=lle, subset=sample_ids)","ed04cd01":"from sklearn.metrics import log_loss\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef train_rf_with_decompose(decompose=None, \n                            subset=None,\n                            X_train=X_train,\n                            y_train=y_train):\n    \n    if subset is not None:\n        X_train = X_train[subset, :]\n        y_train = y_train[subset]\n    \n    if decompose is None:\n        # if no decomposition, we use the original one\n        X_train_transformed = X_train\n        X_valid_transformed = X_valid\n    else:\n        # transform training set and valid set\n        X_train_transformed = decompose.transform(X_train)\n        X_valid_transformed = decompose.transform(X_valid)\n    \n    # train a random forest\n    rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, \n                                random_state=42)\n    rf.fit(X_train_transformed, y_train)\n    pred = rf.predict_proba(X_valid_transformed)\n    \n    return log_loss(y_valid, pred)","901dd47f":"train_rf_with_decompose()","5ed94e40":"pca_full = PCA(n_components=75).fit(X_train)\n\nplt.plot(pca_full.explained_variance_ratio_.cumsum())\nplt.hlines(0.95, 0.1, 51, \"black\", \"--\")\nplt.vlines(50, 0.05, 0.95, \"black\", \"--\")\nplt.xlabel(\"number of principal components\")\nplt.ylabel(\"Cumulative explained variance ration\")\nplt.xlim(0.5, 80)\nplt.ylim(0.1, 1);","6cdac145":"train_rf_with_decompose(decompose=pca_full)","2bdac6ca":"X_train = pca_full.transform(X_train)\nX_valid = pca_full.transform(X_valid)","8e471979":"rf_final = RandomForestClassifier(n_estimators=500, max_depth=15,\n                                  n_jobs=-1, random_state=42).fit(X_train, y_train)","7b43e219":"pred = rf_reduce.predict_proba(X_valid)\nlog_loss(y_valid, pred)","085b5d62":"test = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/test.csv\")\ntest = test.iloc[:, 1:].values\ntest_preds = rf_final.predict_proba(test)","61239e89":"sub = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")\nsub.iloc[:, 1:] = test_preds\nsub.to_csv(\"submission.csv\", index=False)","7a76eaee":"We usually decide the number of component by finding the \"elbow\" of explained variance.","d7cad90a":"**What I've done in this notebook**\n\n* Applying various dimension reduction techniques and visualize the reduced data\n    - PCA\n    - kernel PCA\n    - t-SNE\n    - Isomap\n    - LLE\n* Train a random forest to evaluate performances after PCA","76ccbf0d":"## Principal component analysis","942b5047":"**outline of this note**\n\n","773dcf37":"In this section, I will use several dimensionality reduction techniques to transform our data set into 2D space and visualize the data points to see if these techniques can give us some insights about the data.","463a3d46":"split data set for evaluating performance","e21c189f":"# Visualizing the data with dimensionality reduction techniques","9b0dad6d":"Unfortunately, since this is a synthesized dataset, these images don't really tell us anything.","ea688bf6":"# Can dimension reduction improve or hurt our prediction performance?","0dc42859":"Here's a helper for visualizing the data:","4f2aa5b1":"Wow! We use a 33% smaller dataset and get a slighly better performance. Let's apply PCA to our training and validation set.","cb9aebfa":"## t-sne","b5b5b860":"# Locally linear embeding (LLE)","9b6c8ca8":"Now that we have a smaller data set, we can train a more complex model.","90b6fef8":"Make prediction on the test set","e81f6a4e":"## Isomap","b0ed18a2":"# Import the data and preprocessing","4135d214":"Train a random forest with the original dataset:","93108f14":"I will use a subset of data (sample of rows) in the following four techniques because they either use tons of memory (kernel PCA) or are time-consuming (the others).","e1ecefed":"## Kernel PCA","fe88d9ec":"## Use PCA as preliminary reduction"}}