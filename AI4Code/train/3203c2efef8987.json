{"cell_type":{"df3e545b":"code","8af1c854":"code","8e119c46":"code","dfb5bf21":"code","adf35207":"code","40ab3239":"code","eeefe434":"code","e9721043":"code","e4a81803":"code","5fd723b9":"code","3c70b3cb":"code","e37fd9c8":"code","1f5b3d75":"code","a429162e":"code","87a3eb68":"code","5f6dda64":"code","1e53b50a":"code","fd665510":"code","20a1d350":"code","0c3ac265":"code","e591d45b":"code","2a919936":"code","b2f2b092":"code","02b7cfe6":"code","b9c528c2":"code","242d5b81":"markdown","90369994":"markdown","0e1ca241":"markdown","967aa45a":"markdown","bca9738e":"markdown","f87dfd05":"markdown","e7d5de6a":"markdown","0523e005":"markdown","14cbe180":"markdown","ad4ab878":"markdown","7ae74720":"markdown","fcd5f5e7":"markdown","df5e58f7":"markdown","312eebaf":"markdown","8584034d":"markdown","9ac5b539":"markdown","7fb7a21d":"markdown","11ae5c7e":"markdown","dabe76de":"markdown","7456b1f9":"markdown","ddf06c73":"markdown","6fadd393":"markdown","13455bde":"markdown","be86faba":"markdown","17dcdb2d":"markdown","ef65dffa":"markdown","51a893ba":"markdown","d7185e5d":"markdown","b4430892":"markdown","63f34621":"markdown","e0967670":"markdown","d068d477":"markdown"},"source":{"df3e545b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# plotly\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# import warnings\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.","8af1c854":"# Importing data\ntrain = pd.read_csv('..\/input\/sign_mnist_train.csv')\ntest = pd.read_csv('..\/input\/sign_mnist_test.csv')","8e119c46":"#train.label = train.label.apply(lambda x: str(x))\n#train.label = train.label.apply(lambda x: str(x).replace('0','A') if 0 in train.label else str(x))\n#train.label = train.label.apply(lambda x: str(x).replace('1','B') if 1 in train.label else str(x))\n#train.label = train.label.apply(lambda x: str(x).replace('2','C') if 2 in train.label else str(x))\n\nplt.figure(figsize=(10,10))\nfor i in range(9):   \n    \n    plt.subplot(3,3,i+1)\n    plt.imshow(train.drop(['label'], axis=1).values[i].reshape(28,28), cmap='gray')\n    plt.axis('off')","dfb5bf21":"a = train[train.label==0]\nb = train[train.label==1]\nc = train[train.label==2]\nnew_train = pd.concat([a,b,c],axis=0, ignore_index=True)\nprint('shape & labels of train: {}, {}'.format(new_train.shape,new_train.label.unique()))\n\na_test = test[test.label==0]\nb_test = test[test.label==1]\nc_test = test[test.label==2]\nnew_test = pd.concat([a_test,b_test,c_test],axis=0, ignore_index=True)\nprint('shape & labels of test : {}, {}'.format(new_test.shape,new_test.label.unique()))","adf35207":"x_train = new_train.drop(['label'],axis=1).values\/255\ny_train = new_train.label.values.reshape(-1,1)\nx_test = new_test.drop(['label'], axis=1).values\/255\ny_test = new_test.label.values.reshape(-1,1)","40ab3239":"bar = go.Bar(x=new_train.label.value_counts().index,\n       y=new_train.label.value_counts().values,\n       marker = dict(color = 'rgba(15, 100, 111)'))\niplot([bar])","eeefe434":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(max_depth=9, random_state=42)\ndtc.fit(x_train, y_train)","e9721043":"y_pred = dtc.predict(x_test)\nprint('score:',dtc.score(x_test,y_test))","e4a81803":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\nplt.figure(figsize=(12,12))\nsns.heatmap(cm, annot=True, linecolor='black', linewidths=1, cmap='Greens', fmt='.1f')\nplt.xticks(np.arange(3), ('A', 'B', 'C'),fontsize = 15)\nplt.yticks(np.arange(3), ('A', 'B', 'C'),fontsize = 15, rotation=0)\nplt.show()\n","5fd723b9":"from sklearn.preprocessing import label_binarize\ny_true_roc = label_binarize(y_test,classes=[0,1,2])\ny_pred_roc = label_binarize(y_pred, classes=[0,1,2])","3c70b3cb":"fpr = {} #  false positive rate\ntpr = {} #  true positive rate\nroc_auc = {}\nfrom sklearn.metrics import roc_curve, auc\nfor i in range(y_true_roc.shape[1]):\n    fpr[i], tpr[i], _ = roc_curve(y_pred_roc[:, i], y_true_roc[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])","e37fd9c8":"colors = ['red','orange','blue']\nlabels = []\nliste = ['A','B','C']\nplt.figure(figsize=(10,30))\nfor i in range(y_true_roc.shape[1]):\n    plt.subplot(3,1,i+1)\n    labels.append('ROC curve for sign {} & Area = {:f}'.format(liste[i], roc_auc[i])) \n    plt.plot(fpr[i], tpr[i], color = colors[i], label=labels[i])\n    plt.legend(loc=(.17, .45), prop={'size':15})\n    plt.ylim([0.0, 1.03])\n    plt.xlim([0.0, 1.03])\n    plt.xlabel('False Positive Rate', fontsize=15)\n    plt.ylabel('True Positive Rate', fontsize =15)\n    plt.title('ROC curves & AUC scores'.format(i+1), fontsize=15)\n    \nplt.show()","1f5b3d75":"from sklearn.svm import SVC\nsvc = SVC(kernel='rbf')\nsvc.fit(x_train, y_train)\ny_pred2 = svc.predict(x_test)\nprint('score:',svc.score(x_test,y_test))","a429162e":"y_pred2 = pd.DataFrame(y_pred2.T)\nytest = pd.DataFrame(y_test)\nconf = pd.concat([ytest,y_pred2],ignore_index=True,axis=1)\nsum(conf[0]-conf[1])","87a3eb68":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred2)\n\nplt.figure(figsize=(12,12))\nsns.heatmap(cm, annot=True, linecolor='black', linewidths=1, cmap='Greens', fmt='.1f')\nplt.xticks(np.arange(3), ('A', 'B', 'C'),fontsize = 15)\nplt.yticks(np.arange(3), ('A', 'B', 'C'),fontsize = 15, rotation=0)\nplt.show()","5f6dda64":"y_true_roc2 = label_binarize(y_test,classes=[0,1,2])\ny_pred_roc2 = label_binarize(y_pred2, classes=[0,1,2])","1e53b50a":"fpr2 = {} # false positive rate\ntpr2 = {} #  true positive rate\nroc_auc2 = {}\nfrom sklearn.metrics import roc_curve, auc\nfor i in range(y_true_roc2.shape[1]):\n    fpr2[i], tpr2[i], _ = roc_curve(y_pred_roc2[:, i], y_true_roc2[:, i])\n    roc_auc2[i] = auc(fpr2[i], tpr2[i])","fd665510":"colors = ['red','orange','blue']\nlabels = []\nliste = ['A','B','C']\nplt.figure(figsize=(10,30))\nfor i in range(y_true_roc2.shape[1]):\n    plt.subplot(3,1,i+1)\n    labels.append('ROC curve for class {} & Area = {:f}'.format(liste[i], roc_auc2[i])) \n    plt.plot(fpr2[i], tpr2[i], color = colors[i],label=labels[i])\n    plt.legend(loc=(.2, .45), prop={'size':15})\n    plt.ylim([0.0, 1.03])\n    plt.xlim([0.0, 1.03])\n    plt.xlabel('False Positive Rate', fontsize=15)\n    plt.ylabel('True Positive Rate', fontsize =15)\n    plt.title('ROC curves & AUC scores'.format(i+1), fontsize=15)\nplt.show()\n","20a1d350":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(categorical_features='all')\ny_test_encoded = ohe.fit_transform(y_test).toarray()\ny_train_encoded = ohe.fit_transform(y_train).toarray()\n","0c3ac265":"### building ANN function\n# importing libraries\nfrom keras.models import Sequential     # initializing neural network library\nfrom keras.layers import Dense, Dropout # building layers\n\n# feed-forward neural network classifier is assigned as \"model\".\nmodel = Sequential()  \n\n# we use dropout in the ratio of 0.25 to prevent overfitting.\nmodel.add(Dropout(0.25)) \n\n# 8 units for the first layer, also the input shape must be given in this line. \n# ReLU activation function is more useful than tanh function due to vanishing gradient problem.\n# weights are initialized as \"random uniform\".\nmodel.add(Dense(8, activation='relu', kernel_initializer='random_uniform', input_dim = x_train.shape[1])) \n# 16 nodes for the second layer\nmodel.add(Dense(16, activation='relu', kernel_initializer='random_uniform'))\n# since we have 10 outputs, in the last layer we need to enter 10 nodes. The output of the softmax function can be used to represent a categorical distribution. \nmodel.add(Dense(3, activation='softmax', kernel_initializer='random_uniform'))\n\n# we compile our model by using \"adadelta\" optimizer. \n# since we have categorical outputs, loss function must be the cross entropy. if you use grid search, you need to use \"sparse_categoricalentropy\".\nmodel.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# fit the model with below batch size and number of epochs.\n# verbose integers 0,1,2 sets the appearance of progress bar. \"2\" shows just a line.\nhistory = model.fit(x_train, y_train_encoded, validation_data=(x_test, y_test_encoded), epochs = 20, batch_size = 100, verbose = 2)\n","e591d45b":"loss = go.Scatter(y= history.history['val_loss'], x=np.arange(0,20), mode = \"lines+markers\", name='Test Loss') \naccuracy = go.Scatter(y= history.history['val_acc'], x=np.arange(0,20), mode = \"lines+markers\", name='Test Accuracy') \nlayout = dict(title = 'Test Loss & Accuracy Visualization',\n              xaxis= dict(title= 'Epochs',ticklen= 5,zeroline= True),\n              yaxis= dict(title= 'Loss & Accuracy',ticklen= 5,zeroline= True))\ndata = [loss, accuracy]\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","2a919936":"y_pred3 = model.predict(x_test)\n\n# Find the column indices of maximum values which corresponds to predicted digits.\n# An alternative method to do this is to convert the matrix into a dataframe first, then find maximum column indices with \"idxmax\".\ny_pred3 = np.argmax(y_pred3, axis = 1) \n\n# Create the confusion matrix.\nconfusion__matrix = confusion_matrix(y_test, y_pred3) \n\n# Plot\nplt.figure(figsize=(12,12))\nsns.heatmap(confusion__matrix, annot=True, linewidths=0.2, cmap=\"Blues\",linecolor=\"black\",  fmt= '.1f')\nplt.xlabel(\"Predicted Labels\", fontsize=15)\nplt.xticks(np.arange(3), ('A', 'B', 'C'),fontsize = 15)\nplt.yticks(np.arange(3), ('A', 'B', 'C'),fontsize = 15, rotation=0)\nplt.ylabel(\"True Labels\", fontsize=15)\nplt.title(\"Confusion Matrix\", color = 'red', fontsize = 20)\nplt.show()","b2f2b092":"y_true_roc3 = label_binarize(y_test,classes=[0,1,2])\ny_pred_roc3 = label_binarize(y_pred3, classes=[0,1,2])","02b7cfe6":"fpr3 = {} # false positive rate\ntpr3 = {} #  true positive rate\nroc_auc3 = {}\nfrom sklearn.metrics import roc_curve, auc\nfor i in range(y_true_roc3.shape[1]):\n    fpr3[i], tpr3[i], _ = roc_curve(y_pred_roc3[:, i], y_true_roc3[:, i])\n    roc_auc3[i] = auc(fpr3[i], tpr3[i])","b9c528c2":"colors = ['red','orange','blue']\nlabels = []\nliste = ['A','B','C']\nplt.figure(figsize=(10,30))\nfor i in range(y_true_roc3.shape[1]):\n    plt.subplot(3,1,i+1)\n    labels.append('ROC curve for class {} & Area = {}'.format(liste[i], roc_auc3[i])) \n    plt.plot(fpr3[i], tpr3[i], color = colors[i],label=labels[i])\n    plt.legend(loc=(.2, .45), prop={'size':15})\n    plt.ylim([0.0, 1.03])\n    plt.xlim([0.0, 1.03])\n    plt.xlabel('False Positive Rate', fontsize=15)\n    plt.ylabel('True Positive Rate', fontsize =15)\n    plt.title('ROC curves & AUC scores'.format(i+1), fontsize=15)\nplt.show()\n","242d5b81":"# 2.1 Creating Model\n\n*  I prefer choosing the kernel function as \"RBF\"","90369994":"# Preprocessing","0e1ca241":"# **It's a must for evaluation of classification models**\n * Consider you have 2 classes; first one has 99 samples, second has 1 sample.\n * Even when the model predict the latter class as wrong, the accuracy will decrease maximum 1%. In case all the predictions for former class is true, the accuracy becomes **%99**. \n * Then by constructing a confusion matrix, we can see how many of a class predicted as another classes, or for images such as handwritten digits we can see which digits are confused with each other like 5 and 6 having close shapes.","967aa45a":"![img](https:\/\/i.imgur.com\/vanvaM1.png[\/img])","bca9738e":"# 1.3. ROC Curve & AUC Score\n* ROC curves typically feature true positive rate(**TPR**) on the y axis, and false positive rate(**FPR**) on the x axis\n* Ideal point of a ROC curve is on top left where TP rate is 1 and FP rate is 0.\n* AUC score equals the area under the ROC curve. When it equals 1, then the classification is done without any errors.","f87dfd05":"* I won't explain the whole code but remember that normally, the outputs -y_train and y_test here- must be **one-hot encoded**. But if you use **\"sparse_categorical_crossentropy\"** instead of cross_entropy, you don't need one-hot encoded output values.","e7d5de6a":"# **3. Multilayer Perceptron**","0523e005":"**Counts of the Labels**","14cbe180":"* In numeric data, the same method is applied to data. But that time our model will assign some random thresholds that seperates the classes.\n* This the instead of the methods like entropy and Gini, impurity is measured by** \"squared errors\".**\n* Note that in every subpartition, **boundaries are parallel to each other and axes** and no non-linear boundaries can be observed in a decision tree. It can be seen in figure below.\n![](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_tree_regression_0011.png)","ad4ab878":"- To find ROC curve in multiclass problems, we need binarized labels.","7ae74720":"# 2.2 Confusion Matrix","fcd5f5e7":"* After preeceding step, what we're going to do next is to choose a kernel function. Here, I'll show you **Radial Basis Function** which was derived from **Gaussian function**.\n\n\n\n![](http:\/\/www.wikizeroo.net\/index.php?q=aHR0cHM6Ly93aWtpbWVkaWEub3JnL2FwaS9yZXN0X3YxL21lZGlhL21hdGgvcmVuZGVyL3N2Zy9jMTZmZDZjNTE1NDEyZjk2YTU3NTA2MTAzODk2MTc4ZDBlOGFmNzdk)\n\n\n* There are 2 important parameters to choose here. The first one is **C**:\n\n>  **C** is the parameter which  trades off correct classification of training examples against maximization of the decision function\u2019s margin.\n\n> A lower C will encourage a** larger margin**, therefore a simpler decision function, at the cost of training accuracy. In other words C behaves as a** regularization parameter** in the SVM.\n\n\n\n\n* The second one is **Sigma**:\n\n\n> Sigma in the RBF is the same sigma in** Gaussian Distrbution** in which sigma stands for the **Standard Deviation**.\n\n>  Here the sigma takes on the same task by determining the dispersion of every single training example and how far the influence of them can reach.\n\n\nThe best values for both parameters can be found by using **Grid Search Cross Validation**.","df5e58f7":"# **1. Decision Tree Classification**","312eebaf":"# A quick look at the images","8584034d":"**Nonlinear SVMs**\n\n\nIf the datapoints cannot be seperable like in the below figure, then you will have to use a **kernel function k** which is the **projects the data points into an another space**.  \n\n\n\n![](http:\/\/openclassroom.stanford.edu\/MainFolder\/courses\/MachineLearning\/exercises\/ex8materials\/ex8a_nofill.png)\n","9ac5b539":"* Now let's try different C and sigma values for a better comprehension.\n> In the next figure, **C = 1**     &    **Sigma = 0.25**\n![img](https:\/\/i.imgur.com\/rYKLTyL.jpg[\/img])\n> This time** C = 1** again & **Sigma = 0.5**\n![img](https:\/\/i.imgur.com\/mDOOYDX.jpg[\/img])\n> And lastly, **C = 0.5 **& **Sigma = 0.5** to observe the change in C\n![img](https:\/\/i.imgur.com\/XEtbtm2.jpg[\/img])\n\n\n*  For all simulation, visit  [SVM simulation of Stanford University](https:\/\/cs.stanford.edu\/people\/karpathy\/svmjs\/demo\/)\n\nLet's start creating model","7fb7a21d":"# 1.2 Confusion Matrix\n* A confusion matrix gives us the number of correct and incorrect predictions of a classification model compared to the actual outcomes. \n* Size of a confusion matrix is NxN, where N is the number of classes..\n* Columns (x axis in heatmap) are based while comparing predictions.\n","11ae5c7e":"# INTRODUCTION\n* In this kernel, a multilabel classification will be made with three methods which are \"Decision Tree\", \"SVC\" and \"MLP\" respectively. \n* Mathematics behind the methods will be explained briefly.\n* After every method,** Confusion Matrices** and **ROC Curves** will be constructed evaluation of classifications.\n* In order that running of this kernel doesen't take too much time, only first three letters of our data which are A, B and C letters of **American Sign Language** data.\n* Let's pass the preprocessing part quickly and see the methods.","dabe76de":"# 3.4 ROC Curve & AUC Score","7456b1f9":"# END","ddf06c73":"# 3.1 Creating Model","6fadd393":"\n![](http:\/\/www.wikizeroo.net\/index.php?q=aHR0cDovL3VwbG9hZC53aWtpbWVkaWEub3JnL3dpa2lwZWRpYS9jb21tb25zL3RodW1iLzcvNzIvU1ZNX21hcmdpbi5wbmcvNjE3cHgtU1ZNX21hcmdpbi5wbmc)","13455bde":"# 3.2 Test Loss & Accuracy Visualization\n* In below figure, the visualization of change in test_loss is shown. We can learn from this graph how many epochs are enough for our model where it started not to show a significant decreasing in loss after a certain epoch. Since the model parameters are assigned as random, it will change in every run of code.\n* You can also see the change in the loss and the accuracy interactively by holding mouse on scatter points","be86faba":"# 2.3. ROC Curve & AUC Score","17dcdb2d":"Decision tree classification is a very simple way of classification to understand. It ask questions to dataset regarding the decisions and their impurities. The data is repeatedly partitioned using predictor values that do the best job of separating the data into relatively homogeneous partitions.\n* The purpose is to decrease entropy to zero and so have homogen leaf nodes. \n* Since it makes the partition based on questions, it can work with **non-numeric** data as well. The methods such as **\"KNN\" and \"SVM\"** don't have that ability because of them being based on the **distance** between data points. \n* A good identification tree refers to **a small tree (minimum cost**) and** homogeneous sets** after subpartition.\n* In non-numeric data, some features matter and some of them don't, because they don't reduce the impurity after subpartitions. \n\n> * In first node in below figure, the tree was able to classify the survived ones on the right-hand side, so the entropy zero since there is a homogeneous classification.\n* In former leaf, there is still an impurity so tree need another questions to ask to make classification and make entropy zero.\n\n\n\n![](http:\/\/cdn-images-1.medium.com\/max\/1200\/1*XMId5sJqPtm8-RIwVVz2tg.png)\nIn images;\n![img](https:\/\/i.imgur.com\/CGapxfs.png[\/img])\n\n","ef65dffa":"# 3.3 Confusion Matrix","51a893ba":"* The accuracy is 1? It's hard to believe. Let's validate it by substracting predicted labels from true labels!","d7185e5d":"The reason of why I prefer MLP to CNN here is to compare 2 methods which can be especially competitive to each other in such a pattern recognition, SVM and MLP. \n* Multilayer perceptron is a conventional type of artifical neural networks which includes at least 1 hidden layer in addition to input and output layers.\n * Let me tell about it over a logistic regression which can also be said to be a simple, single layer perceptron.\n> * We have a flatten input consisting of pixels of an image. The density of these pixels in summing part will be determined by the **weights** of them. Each of them can be thought as **signals** received by neuron in our brain.  \n> * All these singular signals forms a big signal and before it's delivered to the brain and got an answer i.e. output, a threshold will decide wheter the signal is strong enough to pass to another neuron or not.\n> * This threshold will be step function, and to pull the threshold to zero, we add a value called **bias** which represents the threshold actually. So our function becomes; **   z = w*x + b**\n> * The next step is insert the z into a **sigmoid function**. We need to do it since while implying gradient descent method in order to update our weights and bias, we need a **continuous function** and step function doesen't work for a** descrete step function**.\n> * After finishing forward propagation part, we determine and measure a **loss function to minimize** our error, also to update our parameters. Generally in multilabel classifications **cross entropy** is used as a loss function in which **y** refers to our actual values and** p** refers to the predicted values.\n![](http:\/\/wiki.fast.ai\/images\/math\/8\/a\/a\/8aa1e513366a2046bee816f7a0f8dd1c.png) \n> * Then we optimize our loss function iteratively to find a better accuracy.\n> * In MLP, since there will be more inputs and outputs at the beginning and end of every layer, the parameters will be updated with respect to **chain rule**.","b4430892":"# 1.1. Creating Model","63f34621":"# **2. Support Vector Classifier (SVC)**\nSupport vector classifier is a classifier using kernel functions transforming data into a higher (infinite) dimensional space that has maximum margin distance from points to the hyperplane.  The SVM was found to be competitive with neural networks on pattern recognition, so it's a really strong algorithm.\n\nIn the graph below, There can be seen linearly seperable 2 classes. The red line in the middle is called an **\"hyperplane\"**. \n\n* A hyperplane is determined with respect to the maximum and equal distances between the closest data points which are called **support vectors** and the hyperplane.\n* After some algebra and calculations, the problem of finding the widest margins turns into an optimization problem, in which** \"w\" is aimed to be minimized**. You can watch the whole tutorial as a proof until the kernel functions are found.\n\n[Formulation of SVMs](https:\/\/www.youtube.com\/watch?v=_PwhiWxHK8o&list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe_4hSi&index=18&t=0s)","e0967670":"**Node :** Any branch after partition\n\n**Leaf Node :** Decisions of the last node of a tree. A leaf node is said to be ***homogeneous*** if all of its training examples are belonging to the same class.\n\n**Entropy :** The measure of the disorder in a system. After each partition we need a way to measure** *homogenity or class purity***. Maximum value 1 refers to a high entropy meaning lots of disorder. Minimum value 0 means homogenityThe formula in which \"*p*\" is the probability of each label is;\n![](http:\/\/www.wikizeroo.net\/index.php?q=aHR0cHM6Ly93aWtpbWVkaWEub3JnL2FwaS9yZXN0X3YxL21lZGlhL21hdGgvcmVuZGVyL3N2Zy9kNjQyZDQ2NjMxZGVjYmY2ODM1NTc4ZGE1NmJmYTA1ZTVkNWYzMzI3)\n\n\n**Gini Impurity** :Gini plays the same role as entropy. Mostly, it's a default parameter of measuring disorder in models. Since unlike entropy,  it doesen't require logarithmic calculations that causes a model to work slower.\n![](http:\/\/www.wikizeroo.net\/index.php?q=aHR0cHM6Ly93aWtpbWVkaWEub3JnL2FwaS9yZXN0X3YxL21lZGlhL21hdGgvcmVuZGVyL3N2Zy9lM2M3MmE4ZTcwNWMyZmRkZjJmZDU1MjI0MWM1NzlhNmMxNDZhZjdm)\n\nIt can be said that there are no differences between them, as seen in the figure below. The blue line looks like a perfect symmetric distribution represents** gini impurity** and the [right skewed](https:\/\/www.itl.nist.gov\/div898\/handbook\/eda\/section3\/eda35b.htm) red line is for **entropy**. They don't increase or decrease accuracy and the performance significantly. Only matters 2% whether you use** *Entropy or Gini***. For a more detailed comparison check the link below.\n\n[Theoritical Comparison of Gini Impurity & Entropy](https:\/\/www.unine.ch\/files\/live\/sites\/imi\/files\/shared\/documents\/papers\/Gini_index_fulltext.pdf)\n![](http:\/\/1.bp.blogspot.com\/-H7hF1zl2aS0\/U__H2vDXUyI\/AAAAAAAAAV8\/CB51ojLz0eI\/s1600\/3.png)","d068d477":"* You remember the our purpose in the above graph was to **maximize**  ![](http:\/\/www.wikizeroo.net\/index.php?q=aHR0cHM6Ly93aWtpbWVkaWEub3JnL2FwaS9yZXN0X3YxL21lZGlhL21hdGgvcmVuZGVyL3N2Zy9hYzU1M2M5NGFlYzk5NmRmZmEzMzY5YWRjZjI4NTMxOTE3OGE0NzRm) so, minimize ![](http:\/\/www.wikizeroo.net\/index.php?q=aHR0cHM6Ly93aWtpbWVkaWEub3JnL2FwaS9yZXN0X3YxL21lZGlhL21hdGgvcmVuZGVyL3N2Zy81YjZmMjdhODkyZjMwNTNlZjBiZmUyNzNmODhmMTgzNTFhMWExOGFj)\n\n* When there is an optimization problem of a function which has **constraints** like this where **y** is a scalar when the sample is positive **y=1** and **y=0** if it's negative;\n![](http:\/\/www.wikizeroo.net\/index.php?q=aHR0cHM6Ly93aWtpbWVkaWEub3JnL2FwaS9yZXN0X3YxL21lZGlhL21hdGgvcmVuZGVyL3N2Zy83MTgxY2Y4ZTcyM2IyNGFkNmQ4NWI3M2I5NTEzZGNhYWMwZDMxMDIz)\nthen we have to use the method of **Lagrange Multiplier**.\n* After some derivatives and substitutions, our problem becomes a maximization problem as below where the **c** is the Lagrance multiplier,\n\n![](http:\/\/www.wikizeroo.net\/index.php?q=aHR0cHM6Ly93aWtpbWVkaWEub3JnL2FwaS9yZXN0X3YxL21lZGlhL21hdGgvcmVuZGVyL3N2Zy85Yzg5ODUxZmEyZmNkOWM5MjBhYTA4OWEyYThkNzU3ODRhODRkNjIz)\n* Once the vector c^0 = (c1,...,cN) which is the solution the above maximization problem is found, hyper plane has the following expansion;\n\n![img](https:\/\/i.imgur.com\/iRfZlsn.jpg[\/img])\n* Then decision function can be written as\n![img](https:\/\/i.imgur.com\/mOEb5ey.jpg[\/img])\n* If we try to make a nonlinear classification that points cannot be seperated linearly, then we will need the transformed data points which are represented with **Phi**.  In our new optimization problem our samples are replaced with the transformed ones, which forms the kernel function.\n![](http:\/\/www.wikizeroo.net\/index.php?q=aHR0cHM6Ly93aWtpbWVkaWEub3JnL2FwaS9yZXN0X3YxL21lZGlhL21hdGgvcmVuZGVyL3N2Zy9mYWUwYmVkZGU5MjlhNDI0NzY0MjYzMTQwZjM4YzdlNmU2NDJmMzM4)\n![](http:\/\/www.wikizeroo.net\/index.php?q=aHR0cHM6Ly93aWtpbWVkaWEub3JnL2FwaS9yZXN0X3YxL21lZGlhL21hdGgvcmVuZGVyL3N2Zy82NTIxYjlkM2UwMDliY2E0MDU1MmJiOTRkMjA0YTRkYTFmMmFmNGZl)\n* Then the new decision function expansion is;\n![img](https:\/\/i.imgur.com\/VWOq9nd.jpg[\/img])\n"}}