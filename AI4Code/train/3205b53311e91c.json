{"cell_type":{"ebea9046":"code","a93d6451":"code","915fb675":"code","15d2d581":"code","110140b7":"code","0bd26daf":"code","ac62ad61":"code","bc35c5f4":"code","e0c1e1c1":"code","07216d0e":"code","55ae423e":"code","db8d9de2":"code","a68bef73":"code","bc4f36ac":"code","0b3e1513":"code","f21efa5f":"code","c2a61e35":"code","04a66b23":"code","833e5a5a":"code","85e0747b":"code","23a4944c":"code","c94d64ad":"code","48633a51":"markdown","301cef61":"markdown","9649f89a":"markdown","2cc16f96":"markdown","5d07e77f":"markdown","48b165b4":"markdown","5315fdf3":"markdown","e4281343":"markdown","99126547":"markdown","b1c21044":"markdown","a52a8203":"markdown","576b44ae":"markdown","709199e5":"markdown","3d01b92e":"markdown","096e041e":"markdown","805ab67e":"markdown","df544329":"markdown","13918810":"markdown","e1f52089":"markdown","d3dc6c15":"markdown","6f17750e":"markdown","aebcf9ac":"markdown","06b4ad76":"markdown"},"source":{"ebea9046":"import numpy as np \nimport pandas as pd \nimport json\nimport pandas.io.json as pdjson\n\nimport ast\n\nimport os\nprint(os.listdir(\"..\/input\"))","a93d6451":"#path = 'C:\\\\Users\\\\Andre\\\\code\\\\Kaggle\\\\2. Google Analytics Customer Revenue Prediction\\\\train.csv'\npath = '..\/input\/train_v2.csv'\ndata1 = pd.read_csv(path, sep=',', dtype={'fullVisitorId': 'str'}, nrows=100)\ndel(path)\n\n# load the competition test data\n#path = 'C:\\\\Users\\\\Andre\\\\code\\\\Kaggle\\\\2. Google Analytics Customer Revenue Prediction\\\\test.csv'\npath = '..\/input\/test_v2.csv'\ndata2 = pd.read_csv(path, sep=',', dtype={'fullVisitorId': 'str'}, nrows=100)\ndel(path)\n\nprint('data1 shape:', data1.shape)\ndata1.head()","915fb675":"print('data2 shape:', data2.shape)\ndata2.head()","15d2d581":"data = data1.append(data2, ignore_index=True)\ndel(data1, data2)\nprint(' data shape:', data.shape)","110140b7":"data.customDimensions[0]","0bd26daf":"data.device[0]","ac62ad61":"data.geoNetwork[0]","bc35c5f4":"data.hits[0]","e0c1e1c1":"data.totals[0]","07216d0e":"data.trafficSource[0]","55ae423e":"# Any results you write to the current directory are saved as output.\n\ndef parse(csv_path, nrows=None):\n\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n\n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows)\n    \n    device_list=df['device'].tolist()\n    \n    #deleting unwanted columns before normalizing\n    for device in device_list:\n        del device['browserVersion'],device['browserSize'],device['flashVersion'],device['mobileInputSelector'],device['operatingSystemVersion'],device['screenResolution'],device['screenColors']\n    df['device']=pd.Series(device_list)\n    \n    geoNetwork_list=df['geoNetwork'].tolist()\n    for network in geoNetwork_list:\n        del network['latitude'],network['longitude'],network['networkLocation'],network['cityId']\n    df['geoNetwork']=pd.Series(geoNetwork_list)\n    \n    df['hits']=df['hits'].apply(ast.literal_eval)\n    df['hits']=df['hits'].str[0]\n    df['hits']=df['hits'].apply(lambda x: {'index':np.NaN,'value':np.NaN} if pd.isnull(x) else x)\n    \n    df['customDimensions']=df['customDimensions'].apply(ast.literal_eval)\n    df['customDimensions']=df['customDimensions'].str[0]\n    df['customDimensions']=df['customDimensions'].apply(lambda x: {'index':np.NaN,'value':np.NaN} if pd.isnull(x) else x)\n    \n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource','hits','customDimensions']\n\n    for column in JSON_COLUMNS:\n        column_as_df = pdjson.json_normalize(df[column])\n        column_as_df.columns = [f\"{column}_{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    \n    return df\nprint(\"The 'parse' function to flatten JSON columns have been created\")","db8d9de2":"data1 = parse('..\/input\/train_v2.csv', nrows=100000)\ndata2 = parse(\"..\/input\/test_v2.csv\",nrows=100000)\n\nprint('data1 shape: ', data1.shape)\nprint('data2 shape: ', data2.shape)\n\ndata = data1.append(data2, sort=True)\ndel(data1, data2)\n\nprint('number of unique columns in data1 + data2:', data.shape)","a68bef73":"jsonlist=[]\nfor i in range(len(data.columns)):   # for each column\n    if (isinstance(data.iloc[1,i], list) ):  # see if some element 1 is a list\n        jsonlist.append( data.columns[i] )   # if yes, then save name to list\nprint(jsonlist)","bc4f36ac":"print(\"Printout for each column's number of unique values (incl. nans)\\n\")\nfor col in data.columns:\n    try:\n        print(col, ':', data[col].nunique(dropna=False))\n    except TypeError:\n        a=data[col].astype('str')\n        #print(a)\n        print( col, ':', a.nunique(dropna=False), ' >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> LIST')\n# Clean workspace\ndel(col)","0b3e1513":"print('Data shape before dropping constant columns:', data.shape)\n\nprint('\\nColumns being dropped:')\n\nfor col in data.columns:\n    try:\n        if (data[col].nunique(dropna=False) == 1):\n            del(data[col])\n            print(col)\n    except TypeError:\n        a=data[col].astype('str')\n        if (a.nunique(dropna=False) == 1):\n            del(data[col])\n            print(col)\ndel(col)\n\nprint('\\ndata shape is now:', data.shape)","f21efa5f":"data.head()","c2a61e35":"print(\"Printout for each column's number of unique values (incl. nans)\\n\")\nfor col in data.columns:\n    try:\n        print(col, ':', data[col].nunique(dropna=False))\n    except TypeError:\n        a=data[col].astype('str')\n        #print(a)\n        print( col, ':', a.nunique(dropna=False), ' >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> LIST')\n# Clean workspace\ndel(col)","04a66b23":"print('number of unique values in column:', \n      data['hits_product'].astype('str').nunique(dropna=False), '\\n' )\nprint( data['hits_product'].iloc[0] )","833e5a5a":"print('data shape:', data.shape)\ndata = data.drop(labels=['hits_product'], axis=1)\nprint('Removed hits_product')\nprint('data shape:', data.shape)","85e0747b":"print('number of unique values in column:', \n      data['hits_promotion'].astype('str').nunique(dropna=False), '\\n' )\nprint( data['hits_promotion'].iloc[1] )","23a4944c":"print('data shape:', data.shape)\ndata = data.drop(labels=['hits_promotion'], axis=1)\nprint('Removed hits_promotion')\nprint('data shape:', data.shape)","c94d64ad":"print(\"Printout for each column's number of unique values (incl. nans)\\n\")\nfor col in data.columns:\n    try:\n        print(col)\n    except TypeError:\n        a=data[col].astype('str')\n        #print(a)\n        print( col)\n# Clean workspace\ndel(col)","48633a51":"\n# Final comments\nWe have completed the data cleaning notebook. In the next notebook we shall reload observations, but immediately delete columns that do not feature in the list as given above, in order to save memory.  \n\n*Acknowledgements*  \nSpecial thank you to the following authors for their insightful kernels:  \nhttps:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields  \nhttps:\/\/www.kaggle.com\/codlife\/pre-processing-for-huge-train-data-with-chunksize  \nhttps:\/\/www.kaggle.com\/usmanabbas\/flatten-hits-and-customdimensions  \n\nI welcome comments and suggestions for improvement!  \n\nPart 2 shall be about Visualization, Exploratory Data Analysis (EDA), and Feature Engineering.  \nEach feature in the shall be explored and discussed.  \nPart 2 : https:\/\/www.kaggle.com\/andredanielvolschenk\/gstore-part-2-visuals-eda-feature-engineering","301cef61":"Let us look at the JSON subcolumns in `data1`...  \nLets first look at the first observation in `customDimensions`:","9649f89a":"There are many JSON columns with only 2 unique values (including nans!)  \nThese can be coded as categorical variables to save space! We will remember this for later.  \nWe only have 2 JSON lists that contain more than 2 unique entries: `hits_product` and `hits_promotion`.  \nLets consider each individually to see what subcolumns they have!  \n\nFor `hits_product`we consider observation 0:","2cc16f96":"Some useless columns:  \n'cityId', 'latitude', 'longitude', 'networkLocation'.  \n\nNext we consider `hits`:","5d07e77f":"This JSON blob has 2 subcolumns: index and value  \nThese can be kept in for now.  \n\nNow lets look at `device`:","48b165b4":"Lets see which JSON columns remain:","5315fdf3":"Before flattening these, we can immediately look into which columns we can delete.  \nLets look at the number of unique values per column. We want to see the number of unique values *including* nans, because nans *may* indicate something unique about the column.  \nWe also indicate which columns are a JSON blob in the form of a list:","e4281343":"This is again a lot of columns and data, but these are abstracted in some other existing columns that relate to promotions, namely:  \n`hits_promotionActionInfo.promoIsClick`, and `hits_promotionActionInfo.promoIsView`  \n\nLet us delete this column:","99126547":"Next we look at `hits_promotion`. we consider observation 1:","b1c21044":"That is a lot of columns !  \nIn fact, these are subcolumns within subcolumns! These JSON blobs are deeply embedded!  \nFor now we will keep them all  \n\nNext we look at `totals`:","a52a8203":"# Columns with constant values:\n\nLooks like there are quite a few features with only 1 value (including nans!) in the entire dataset.  \nThese columns will obviously not contribute to predictive power, so they may be removed.  \n\nLets remove useless columns and see the new shape of our data:","576b44ae":"We now we have 106 features remaining.  \nIn the next notebook, we will continue reducing the number of columns under consideration, and we shall finally start using all the available observations.  \n\nLets printout our columns one last time so that we can re-use them for the next notebook:","709199e5":"These are all important to keep, so we keep them.  \n\n# Parsing JSON\nNow we finally declare the function to parse the JSON blobs, keeping in mind the columns we can already delete, and keeing in mind that `customDimensions`, and `hits` are encoded differently:","3d01b92e":"Now lets read in `data1` and `data2` with 100'000 rows each, for a total of 200'000 observations.  \nWe then merge these to form `data`:","096e041e":"@author: Andr\u00e9 Dani\u00ebl VOLSCHENK  \n\nKaggle project {Google Analytics Customer Revenue Prediction}  \nkaggle.com\/andredanielvolschenk  \n\n\n# Problem Statement\n\nDESCRIPTION  \nThe 80\/20 rule has proven true for many businesses\u2013only a small percentage of customers produce most of the revenue. As such, marketing teams are challenged to make appropriate investments in promotional strategies.  \nRStudio has partnered with Google Cloud and Kaggle to demonstrate the business impact that thorough data analysis can have.  \nIn this competition, you\u2019re challenged to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer. Hopefully, the outcome will be more actionable operational changes and a better use of marketing budgets for those companies who choose to use data analysis on top of GA data.  \n\nEVALUATION  \nSubmissions are scored on the root mean squared error.  \n\nSubmission File  \nFor each fullVisitorId in the test set, you must predict the natural log of their total revenue in PredictedLogRevenue. The submission file should contain a header and have the following format:  \n\n| fullVisitorId                        | PredictedLogRevenue  \n|-----------------------------------------------------------\n| 0000000259678714014 | 0  \n| 0000049363351866189 | 0  \n| 0000053049821714864 | 0  \netc.  \n\nDATA  \n\nWhat files do I need?  \nYou will need to download train.csv and test.csv. These contain the data necessary to make predictions for each fullVisitorId listed in sample_submission.csv.  \n\nAll information below pertains to the data in both CSV and BigQuery format.  \n\nWhat should I expect the data format to be?  \nBoth train.csv and test.csv contain the columns listed under Data Fields. Each row in the dataset is one visit to the store. Because we are predicting the log of the total revenue per user, be aware that not all rows in test.csv will correspond to a row in the submission, but all unique fullVisitorIds will correspond to a row in the submission.  \n\nIMPORTANT: Due to the formatting of fullVisitorId you must load the Id's as strings in order for all Id's to be properly unique!\nThere are multiple columns which contain JSON blobs of varying depth. In one of those JSON columns, totals, the sub-column transactionRevenue contains the revenue information we are trying to predict. This sub-column exists only for the training data.  \n\nWhat am I predicting?  \nWe are predicting the natural log of the sum of all transactions per user.   \n\nFile Descriptions  \n\ntrain.csv - the training set - contains the same data as the BigQuery rstudio_train_set.  \ntest.csv - the test set - contains the same data as the BigQuery rstudio_test_set.  \nsampleSubmission.csv - a sample submission file in the correct format. Contains all fullVisitorIds in test.csv.  \n\nData Fields  \n\n| Data name                      | Description\n|--------------------------------------------------------------\n|  fullVisitorId                     | A unique identifier for each user of the Google Merchandise Store.\n| channelGrouping            | The channel via which the user came to the Store.\n| date                                   | The date on which the user visited the Store.\n| device                                | The specifications for the device used to access the Store.\n| geoNetwork                      | This section contains information about the geography of the user.\n| sessionId                           | A unique identifier for this visit to the store.\n| socialEngagementType   | Engagement type, either \"Socially Engaged\" or \"Not Socially Engaged\".\n| totals                                  | This section contains aggregate values across the session.\n| trafficSource                     | This section contains information about the Traffic Source from which the session originated.\n| visitId                                 | An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId\n| visitNumber                      |  The session number for this user. If this is the first session, then this is set to 1\n| visitStartTime                  |  The timestamp (expressed as POSIX time)\n\nA more complete description of each column is given: https:\/\/support.google.com\/analytics\/answer\/3437719?hl=en\n\nRemoved Data Fields  \nSome fields were censored to remove target leakage. The major censored fields are listed below.  \nhits - This row and nested fields are populated for any and all types of hits. Provides a record of all page visits.  \ncustomDimensions - This section contains any user-level or session-level custom dimensions that are set for a session. This is a repeated field and has an entry for each dimension that is set.  \ntotals - Multiple sub-columns were removed from the totals field.  \n\n# Import libraries\nLets import libraries and see what datafiles we have in our environment.","805ab67e":"We will keep these for now, too.  \n\nFinally we can look at `trafficSource`:","df544329":"Lets look at what our data looks like now:","13918810":"First lets see which columns are still JSON blobs in the form of lists:  ","e1f52089":"Note: train_v2.csv has 1'708'337 observations and test_v2.csv has 401'589 observations.  \nThat is a LOT of data!!!  \n\n# Load\n\nRecall that,  due to the formatting of fullVisitorId you must load the Id's as strings in order for all Id's to be properly unique!  \nLets just load 100 rows to take a quick look  \n\nSince `train_v2` and `test_v2`are so big, we will only load 1000 observations for now.  \n\nLets see what data1 looks like:","d3dc6c15":"These are a lot of additional columns !  \nBasically this column encodes the products shown in the GStore search result. We will leave this out, as this is a lot of information for what is likely minimal gain, if any.  ","6f17750e":"Some columns here really wont add usefull information to our prediction, namely: 'browserVersion', 'browserSize', 'operatingSystemVersion', 'mobileInputSelector', 'flashVersion', 'screenColors', 'screenResolution'.  \n\nNow lets consider `geoNetwork`:","aebcf9ac":"Lets see what `data2` looks like:","06b4ad76":"From both `data1` and `data2` we can see that there are multiple columns which contain JSON blobs of varying depth.  \n\nJSON is 'JavaScript Object Notation'  \nJSON is unstructured data, in-that each row in the table does not have the same number of column (when sub-columns are expanded).\n\nWe could write a function soon to just flatten all JSON columns that have embedded sub-columns...\n\nLets first make a list of the JSON columns:\n* `customDimensions`\n* `device`\n* `geoNetwork`\n* `hits`\n* `totals`\n* `trafficSource`\n\n\nWe have one problem to adress first: The `hits` and `customDimensions` columns encode its Jason blobs differently. It has the `[` and `]` characters around its blob. Furhermore, it uses single quotes `'` instead of double quotes `\"`.  They will have to receive special consideration.  \n\n# Explore JSON fields\nFirst we merge `data1` and `data2`..."}}