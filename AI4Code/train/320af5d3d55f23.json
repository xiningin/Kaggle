{"cell_type":{"2cf2113e":"code","744b144e":"code","426a35b4":"code","808894d0":"code","ced87483":"code","4087ea35":"code","77cbe2e9":"code","7034b524":"code","32ce5a6a":"code","8e024424":"code","7110d9b9":"code","bec9c62a":"code","de01b05c":"code","387df22e":"code","4998dd7d":"markdown","9329d3c6":"markdown","f039357f":"markdown","cc6c8656":"markdown","5eb5adbd":"markdown","932f0fd3":"markdown"},"source":{"2cf2113e":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport os\nimport gc\n\nimport re\nfrom nltk.corpus import stopwords\n#import distance\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup","744b144e":"import pandas as pd\ndf = pd.read_csv(\"..\/input\/question-pairs-dataset\/questions.csv\",nrows=100000)","426a35b4":"df.head()","808894d0":"df.shape[0]","ced87483":"df.info()","4087ea35":"d=df.groupby('is_duplicate')['id'].count().plot.bar()\nd=df['is_duplicate'].value_counts()\nd","77cbe2e9":"print(\"Percentage of duplicate question {}%\".format(round(df['is_duplicate'].mean()*100,2)))\nprint(\"Percentage of non-duplicates questions {}%\".format(100-round(df['is_duplicate'].mean()*100,2)))","7034b524":"qids=pd.Series(df['qid1'].tolist()+df['qid2'].tolist())\nunique_qs=len(np.unique(qids))\nqs_morethanone=np.sum(qids.value_counts()>1)\n\nprint(\"Unique questions {}\\n\".format(unique_qs))\nprint(\"Question more than one {}\\n\".format(qs_morethanone))\nprint(\"max value of repition {}\\n\".format(max(qids.value_counts())))\n\nq_vals=qids.value_counts()\n\nq_vals=q_vals.values","32ce5a6a":"\nx = [\"unique_questions\" , \"Repeated Questions\"]\ny =  [unique_qs , qs_morethanone]\n\nplt.figure(figsize=(10, 6))\nplt.title (\"Plot representing unique and repeated questions  \")\nsns.barplot(x,y)\nplt.show()","8e024424":"pair_duplicates=df[['id','qid1','qid2']].groupby(['qid1','qid2']).count().reset_index()\nnum=pair_duplicates.shape[0]-df.shape[0]\npair_duplicates.shape[0]","7110d9b9":"plt.figure(figsize=(20, 10))\n\nplt.hist(qids.value_counts(), bins=160)\n\nplt.yscale('log', nonposy='clip')\n\nplt.title('Log-Histogram of question appearance counts')\n\nplt.xlabel('Number of occurences of question')\n\nplt.ylabel('Number of questions')\n\nprint ('Maximum number of times a single question is repeated: {}\\n'.format(max(qids.value_counts()))) ","bec9c62a":"nan_rows=df[df.isnull().any(1)]\nprint(nan_rows)","de01b05c":"df=df.fillna('')\nnan_rows=df[df.isnull().any(1)]\nprint(nan_rows)","387df22e":"if os.path.isfile('df_fe_without_preprocessing_train1.csv'):\n    df=pd.read_csv('df_fe_without_preprocessing_train.csv',encoding='latin-1')\nelse:\n    df['freq_qid1']=df.groupby('qid1')['qid1'].transform('count')\n    df['freq_qid2']=df.groupby('qid2')['qid2'].transform('count')\n    df['q1len']=df['question1'].str.len()\n    df['q2len']=df['question2'].str.len()\n    df['q1_n_words']=df['question1'].apply(lambda row: len(row.split(\" \")))\n    df['q2_n_words']=df['question2'].apply(lambda row: len(row.split(\" \")))\n    \n    def normalized_word_common(row):\n        w1=set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n        w2=set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))\n        return 1.0*len(w1 & w2)\n    df['word_Common']=df.apply(normalized_word_common,axis=1)\n    \n    def normalized_word_common(row):\n        w1=set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n        w2=set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))\n        return 1.0*(len(w1)+len(w2))\n    df['word_total']=df.apply(normalized_word_common,axis=1)\n    \n    def normalized_word_share(row):\n        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n        return 1.0 * len(w1 & w2)\/(len(w1) + len(w2))\n    df['word_share'] = df.apply(normalized_word_share, axis=1)\n    \n    df['freq_q1+q2'] = df['freq_qid1']+df['freq_qid2']\n    df['freq_q1-q2'] = abs(df['freq_qid1']-df['freq_qid2'])\n    df.to_csv('df_fe_without_preprocessing_train.csv',index=False)\ndf.head()\n    ","4998dd7d":"<h2>3.3 Basic Feature Extraction (before cleaning) <\/h2>","9329d3c6":"Let us now construct a few features like:\n - ____freq_qid1____ = Frequency of qid1's\n - ____freq_qid2____ = Frequency of qid2's \n - ____q1len____ = Length of q1\n - ____q2len____ = Length of q2\n - ____q1_n_words____ = Number of words in Question 1\n - ____q2_n_words____ = Number of words in Question 2\n - ____word_Common____ = (Number of common unique words in Question 1 and Question 2)\n - ____word_Total____ =(Total num of words in Question 1 + Total num of words in Question 2)\n - ____word_share____ = (word_common)\/(word_Total)\n - ____freq_q1+freq_q2____ = sum total of frequency of qid1 and qid2 \n - ____freq_q1-freq_q2____ = absolute difference of frequency of qid1 and qid2 ","f039357f":"<h3> 3.2.4 Number of occurrences of each question <\/h3>","cc6c8656":"<h3>Number of Unique of questions<h3\/>","5eb5adbd":"<h3> 3.2.1 Distribution of data points among output classes<\/h3>\n- Number of duplicate(smilar) and non-duplicate(non similar) questions","932f0fd3":"<h3>3.2.3 Checking for Duplicates <\/h3>"}}