{"cell_type":{"680bd01f":"code","b421f1f8":"code","5324a17f":"code","00031d0f":"code","719fbf3f":"code","36675b0f":"code","f6e7a771":"code","1d222861":"code","33f74076":"code","4349452f":"code","111908ec":"code","079d3348":"code","606060d5":"code","f2693a27":"code","116cd0c4":"code","a48c48fe":"code","362ac255":"code","cde339e6":"code","65d8f65f":"code","31bcfe46":"code","47535fa7":"code","7e5307a0":"code","e76eaaba":"code","3a62ca83":"code","4124c4ac":"code","9952dede":"code","f7bef108":"code","e0023310":"code","2f6dae52":"code","86be9825":"code","e836fbf5":"code","d229c5fe":"code","1fd2eb00":"code","362386e1":"code","8ca9f0c2":"code","81b53804":"code","eb32920b":"code","a01281e2":"code","80f73178":"code","c0ffb1f0":"code","782f8584":"code","ae5698fb":"code","3b007056":"code","c30a7d47":"code","0e8dd086":"code","76494b8f":"code","d2d4b4e3":"markdown","b3bdd45f":"markdown","fff59773":"markdown","083e4cf3":"markdown","188e3134":"markdown","661dbd44":"markdown","e6420145":"markdown","cb66006a":"markdown","edc62a52":"markdown"},"source":{"680bd01f":"\nimport numpy as np \nimport pandas as pd \nimport matplotlib\nfrom matplotlib import pyplot as plt \nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression # Logistic regression\nfrom sklearn.ensemble import RandomForestClassifier  # Random forest\nfrom sklearn.model_selection import train_test_split # Splits arrays or matrices into random train and test subsets\nfrom sklearn.metrics import classification_report \nfrom sklearn.model_selection import cross_val_score\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b421f1f8":"# Getting the train data\ntrain_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain_data.head() # Showing the train dataset","5324a17f":"# Getting the test data\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest_data.head() # Showing the test dataset","00031d0f":"# List with feature describtions\nvariabel_lst = [['PassengerId', 'Id for each passenger'],\n                ['Survival', 'Has the passenger survived. 0 = no, 1 = yes'], \n               ['Pclass', 'The passenger travel class. 1., 2., or 3. class'], \n               ['Name', 'Passengers name'], \n               ['Sex', 'Passengers sex'],\n               ['Age', 'Passengers age'], \n               ['Siblings\/Spouses Aboard', 'Amount of siblings and spouses traveling with the passenger'], \n               ['Parch', 'Amount of parents and children traveling with the passenge'], \n                ['Ticket', 'Passengers ticket number'],\n               ['Fare', 'Passengers ticket Fare'],\n               ['Cabin', 'Passengers cabin number'],\n               ['Embarked', 'Passengers embarkation. S,C or Q']] \n    \nvariabel_df = pd.DataFrame(variabel_lst, columns =['Feature', 'Describtion']) \npd.options.display.max_colwidth = 100\nvariabel_df","719fbf3f":"# We will start the machine learning process by identifing the different data types:\ntrain_data.info() ","36675b0f":"# We can see that the training dataset has 12 features and 891 entries.\n# We can furthermore see that 7 features are numerical datatypes 5 object datatypes","f6e7a771":"# Basic descriptive statistics of the train dataset:\ntrain_data.describe()","1d222861":"# Calculations of % of women who survived\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)*100\n\nprint(rate_women, \"% of women who survived:\")","33f74076":"# Calculations of % of men who survivedmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)*100\n\nprint(rate_men, \"% of men who survived\")","4349452f":"# Now we are checking the relevant features for any missing values, because machine learning models do not likes NaN values. ","111908ec":"# One way to check individual features:\ntrain_data['Ticket'].isnull().values.any() #Check for missing values in the 'Ticket' of the train set \ntest_data['Ticket'].isnull().values.any() #Check for missing values in the 'Ticket' of the test set","079d3348":"# More advanced way to check for NaN's in the train-dataset - inspiration from (Donges, 2018):\ntotal = train_data.isnull().sum().sort_values(ascending=False)\nmissing_data = pd.concat([total], axis=1, keys=['Total'])\nmissing_data.head()","606060d5":"# We can see that the 'Cabin', 'Age' and 'Embarked'-features has missing values.\n# The 'Cabin' feature includes too many missing values, that we choose to drop it","f2693a27":"# Exclude the 'Cabin'-feature:\ntrain_data = train_data.drop(['Cabin'], axis=1)\ntest_data = test_data.drop(['Cabin'], axis=1)","116cd0c4":"#Because 'Age' contains NaN values, we will replace the missing values with the average of the Age variable\ntrain_data['Age_filled']=train_data.Age.fillna(train_data.Age.mean()) #Fill the NaN's with the mean value\ntrain_data.Age_filled\n\n#And of course we have to do the same replacement with the test data\ntest_data['Age_filled']=test_data.Age.fillna(train_data.Age.mean()) #Fill the NaN's with the mean value from the train_data\ntest_data.Age_filled","a48c48fe":"# Check if all missing values are filled:\ntrain_data['Age_filled'].isnull().values.any()","362ac255":"# Find most common value in 'Embarked'\ntrain_data['Embarked'].value_counts()","cde339e6":"#Because 'Embarked' contains 2 NaN values, we will replace the missing values with the most common value 'S'\nmost_common = 'S'\n\ntrain_data['Embarked']=train_data.Age.fillna(most_common) #Fill the NaN's with 'S'\ntrain_data.Embarked\n\n#And of course we have to do the same replacement with the test data\ntest_data['Embarked']=test_data.Embarked.fillna(most_common)\ntest_data.Embarked","65d8f65f":"# Check if all missing values are filled:\nprint(train_data['Embarked'].isnull().values.any())\nprint(test_data['Embarked'].isnull().values.any())","31bcfe46":"# We will now change the data type for each relevant feature into a categorical type and furthermore do a coding of it, in order to get the value in an integer format.\n\n# First the train_data:","47535fa7":"Ticket_cat =train_data['Ticket'].astype('category') # Converting 'Ticket' into a categorical dtype in a new column\ntrain_data['Ticket_cat_codes']= Ticket_cat.cat.codes # Coding the 'Ticket' variable in a new column\ntrain_data[['Ticket','Ticket_cat_codes']] # The final train_data for the 'Ticket' variable","7e5307a0":"Sex_cat = train_data['Sex'].astype(\"category\")\ntrain_data['Sex_cat_codes'] = Sex_cat.cat.codes\ntrain_data[['Sex','Sex_cat_codes']]","e76eaaba":"Age_filled_cat = train_data['Age_filled'].astype(\"category\")\ntrain_data['Age_filled_cat_codes'] = Age_filled_cat.cat.codes\ntrain_data[['Age_filled','Age_filled_cat_codes']]","3a62ca83":"Fare_cat = train_data['Fare'].astype(\"category\")\ntrain_data['Fare_cat_codes'] = Fare_cat.cat.codes\ntrain_data[['Fare','Fare_cat_codes']]","4124c4ac":"train_data['Embarked_filled']=pd.Categorical(train_data['Embarked'])\ndef setCategory(train_data):\n    if train_data['Embarked_filled'] == 'S':\n        return 'Southampton'\n    elif train_data['Embarked_filled'] == 'C':\n        return 'Cherbourg'\n    elif train_data['Embarked_filled'] == 'Q':\n        return 'Queenstown'\ntrain_data['Embarked_filled'] = train_data.apply(setCategory, axis =1) #Replacing letters with names","9952dede":"Pclass_cat = train_data['Pclass'].astype(\"category\")\ntrain_data['Pclass_cat_codes'] = Pclass_cat.cat.codes\ntrain_data[['Pclass', 'Pclass_cat_codes']]","f7bef108":"# And now we will do the same with the test_data:","e0023310":"Ticket_cat = test_data['Ticket'].astype('category')\ntest_data['Ticket_cat_codes']= Ticket_cat.cat.codes\ntest_data[['Ticket', 'Ticket_cat_codes']]","2f6dae52":"Sex_cat = test_data['Sex'].astype(\"category\")\ntest_data['Sex_cat_codes'] = Sex_cat.cat.codes\ntest_data[['Sex','Sex_cat_codes']]","86be9825":"Age_filled_cat = test_data['Age_filled'].astype(\"category\")\ntest_data['Age_filled_cat_codes'] = Age_filled_cat.cat.codes\ntest_data[['Age_filled','Age_filled_cat_codes']]","e836fbf5":"Fare_cat = test_data['Fare'].astype(\"category\")\ntest_data['Fare_cat_codes'] = Fare_cat.cat.codes\ntest_data[['Fare','Fare_cat_codes']]","d229c5fe":"test_data['Embarked_filled']=pd.Categorical(test_data['Embarked'])\ndef setCategory(test_data):\n    if test_data['Embarked_filled'] == 'S':\n        return 'Southampton'\n    elif test_data['Embarked_filled'] == 'C':\n        return 'Cherbourg'\n    elif test_data['Embarked_filled'] == 'Q':\n        return 'Queenstown'\ntest_data['Embarked_filled'] = test_data.apply(setCategory, axis =1)","1fd2eb00":"Pclass_cat = test_data['Pclass'].astype(\"category\")\ntest_data['Pclass_cat_codes'] = Pclass_cat.cat.codes\ntest_data[['Pclass','Pclass_cat_codes']]","362386e1":"# New feature that includes familysize \ntrain_data['family_size'] = train_data['SibSp'] + train_data['Parch'] +1 # two features plus the passenger","8ca9f0c2":"family_size_cat = train_data['family_size'].astype(\"category\")\ntrain_data['family_size_cat_codes'] = family_size_cat.cat.codes\ntrain_data[['family_size','family_size_cat_codes']]","81b53804":"# Horizontal bar plot of how the 'Pclass' feature has something to do with the chance of survival - (Donges, 2018): \nsns.barplot(x='Pclass', y='Survived', data=train_data)","eb32920b":"train_data.head()","a01281e2":"train_data['Survived'].isnull().values.any()","80f73178":"features= [\"Sex_cat_codes\",\"Fare_cat_codes\", \"Age_filled_cat_codes\", \"Pclass_cat_codes\", \"Ticket_cat_codes\"].copy()\n\ny = train_data[\"Survived\"].copy() # target array \/ dependent variable\nX = train_data[features].copy() # features matrix","c0ffb1f0":"# Model: Train-Test-Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=123)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","782f8584":"# Logistic Regression:\nclf = LogisticRegression(max_iter=500)\nclf.fit(X_train, y_train)  # Fit the model to our data\n\nY_pred = clf.predict(X_test)\n\n\nLR_acc = round(clf.score(X_train, y_train) * 100, 2)\n","ae5698fb":"# Random Forest: \nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\n\nRF_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, y_train)\nRF_acc = round(random_forest.score(X_train, y_train) * 100, 2)","3b007056":"# Which is the best Model? - (Donges, 2018)\n\nresults = pd.DataFrame({\n    'Model type': ['Logistic Regression', \n              'Random Forest'],\n    'Score': [LR_acc, \n              RF_acc]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(9)","c30a7d47":"# K-Fold Cross validation - (Donges, 2018)\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"average accuracy:\", scores.mean()*100, \"%\")\nprint(\"Standard Deviation:\", scores.std()*100, \"%\")","0e8dd086":"# From the results above, we can see that the model has an accuacy of 82% and a standard deviation of 5%. ","76494b8f":"# The features importances based on random forest measurements - (Donges, 2018)\nimportances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(6)","d2d4b4e3":"# Bibliography","b3bdd45f":"*Jakob*\n# Machine learning model","fff59773":"# Data correlation","083e4cf3":"# Data exploration","188e3134":"*Sanna*\n# Converting features","661dbd44":"*Sanna*\n# Getting the data","e6420145":"# Creating new feature","cb66006a":"*Jakob*\n# Data Cleaning","edc62a52":"Donges, 2018. Achieved from: https:\/\/towardsdatascience.com\/predicting-the-survival-of-titanic-passengers-30870ccc7e8 \n\nVanderPlas (2016). Achieved from: \n* https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.02-introducing-scikit-learn.html \n* https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.01-what-is-machine-learning.html\n* https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.02-introducing-scikit-learn.html"}}