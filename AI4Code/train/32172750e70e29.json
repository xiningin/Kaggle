{"cell_type":{"57da6b15":"code","c54d99cc":"code","5043f5e1":"code","30da800f":"code","2844e1f4":"code","13dfca4b":"code","cad4e052":"code","1beab44e":"code","08817c3f":"code","5b299a31":"markdown","d535a9a5":"markdown","3f1c3403":"markdown","91cab573":"markdown","8929cf92":"markdown","11bff279":"markdown","07c4f089":"markdown","76bb9307":"markdown","dcea9a89":"markdown","e0e6a34c":"markdown"},"source":{"57da6b15":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom xgboost import XGBClassifier","c54d99cc":"df = pd.read_csv('..\/input\/mlcourse\/telecom_churn.csv')","5043f5e1":"df.head()","30da800f":"state_enc = LabelEncoder()\ndf['State'] = state_enc.fit_transform(df['State'])\ndf['International plan'] = (df['International plan'] == 'Yes').astype('int')\ndf['Voice mail plan'] = (df['Voice mail plan'] == 'Yes').astype('int')\ndf['Churn'] = (df['Churn']).astype('int')","2844e1f4":"X_train, X_test, y_train, y_test = train_test_split(df.drop('Churn', axis=1), df['Churn'],test_size=0.3, stratify=df['Churn'], random_state=17)","13dfca4b":"params = {\n    'objective': 'binary:logistic',\n    'max_depth': 3,\n    'learning_rate': 1.0,\n    'n_estimators': 50\n}","cad4e052":"xgb_model = XGBClassifier(**params).fit(X_train, y_train)","1beab44e":"preds_prob = xgb_model.predict(X_test)","08817c3f":"predicted_labels = preds_prob > 0.5\nprint(\"Accuracy and F1 on the test set are: {:.2} and {:.2}\".format(\n    round(accuracy_score(y_test, predicted_labels), 3),\n    round(f1_score(y_test, predicted_labels), 3)))","5b299a31":"Boosting is a sequential technique which works on the principle of an ensemble. It combines a set of weak learners and delivers improved prediction accuracy. At any instant t, the model outcomes are weighed based on the outcomes of previous instant t-1. The outcomes predicted correctly are given a lower weight and the ones miss-classified are weighted higher. Note that a weak learner is one which is slightly better than random guessing. For example, a decision tree whose predictions are slightly better than 50%.","d535a9a5":"XGBoost is one of the most popular machine learning algorithm these days. Regardless of the type of prediction task at hand; regression or classification.","3f1c3403":"Divide the data into training and test samples with a ratio of 7: 3","91cab573":"Let's calculate the percentage of correct algorithm responses in the test sample.","8929cf92":"Forecasts for the test sample","11bff279":"**Initializing parameters**\n\nbinary classification ('objective': 'binary:logistic')\nlimiting the depth of trees ('max_depth':3)\nwe don't want extra output ('silent':1)\nwe will perform 10 iterations of boosting\nthe gradient descent step is quite large ('eta':1) - the algorithm will learn quickly and \"aggressively\" (better results will be obtained if you reduce the eta and increase the number of iterations)","07c4f089":"\n# Import Necessary Libraries.","76bb9307":"# Don't forget to upvote\ud83d\udcc8 if you like\ud83d\udc4d\ud83c\udffb it","dcea9a89":"**Training of the classifier**\n\n\u0422\u0443\u0442 \u043c\u044b \u043f\u0440\u043e\u0441\u0442\u043e \u043f\u0435\u0440\u0435\u0434\u0430\u0435\u043c \u0441\u043b\u043e\u0430\u0432\u0442\u044c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432, \u0434\u0430\u043d\u043d\u044b\u0435 \u0438 \u0447\u0438\u0441\u043b\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0439.","e0e6a34c":"We will simply number the States, and make the international plan (international roaming), Voice mail plan (voice mail) and target Churn attributes binary."}}