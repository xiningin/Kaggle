{"cell_type":{"6e221899":"code","6ad5251b":"code","1c8ada79":"code","db536644":"code","201a651e":"code","417950ad":"code","292b4bd0":"code","1c8eda60":"code","220e674d":"code","01b16c9b":"code","e975e803":"code","8c40642f":"code","478b240b":"code","069c0c88":"code","e7e2e805":"code","216de89a":"code","e684a441":"code","49ab310a":"code","8ffc984a":"code","082af535":"code","5155f094":"code","31a1b94a":"code","363c224d":"code","e11b8d11":"code","de01020b":"code","a662ed0a":"code","95a245db":"code","bfa62f2b":"code","cd58d4e3":"code","e5fa36aa":"code","a6e2de43":"code","4ec4f048":"code","04b84523":"code","6655f93e":"code","5a84a270":"code","53d994a4":"code","acfe8509":"code","2aa9e95c":"code","1d563dbe":"code","86fe15cc":"code","1ee4f1b9":"code","07e2a693":"code","d9b8e68d":"code","04ca1241":"code","92e39898":"code","d35937e9":"code","4d13b3dd":"code","74d9948d":"code","56923b17":"code","46aa5eac":"code","ecc74115":"code","31055d26":"code","407557ca":"code","2b01f578":"code","2ba7e0f6":"code","b679ffdc":"code","5fc12f16":"code","82c12bdb":"code","8581d5b2":"code","1a755f73":"code","48105578":"code","96fd4937":"code","d7f81733":"code","760f8c4e":"code","05235f84":"code","052d1b83":"code","551dbb07":"code","9ba91bd5":"code","d91cdd9f":"code","6272cc48":"code","0d53c5be":"code","1ecbc1b1":"code","0e8acea5":"markdown","1eb62db2":"markdown","dc2369d7":"markdown","99c220ec":"markdown","bd5a8577":"markdown","a9ca392a":"markdown","311412aa":"markdown","4eefb0a4":"markdown","0aeccf56":"markdown","81f87f5e":"markdown","a9ae7786":"markdown","f0274dc1":"markdown","e96f861f":"markdown","77a596d8":"markdown","d5f3cc41":"markdown","12828c59":"markdown","75bd61a1":"markdown","b6375b89":"markdown","a3a1ae89":"markdown","4e299469":"markdown","a37f7cac":"markdown","873a6033":"markdown","6c9eaa33":"markdown","f05086dd":"markdown","5beeab45":"markdown","b1b37698":"markdown","056c36aa":"markdown","8c6fe364":"markdown","a780d22b":"markdown","f5aef118":"markdown","2fe5bf12":"markdown","baee4ba0":"markdown","8204800f":"markdown","fdfcbb29":"markdown"},"source":{"6e221899":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport os\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom IPython.display import display, HTML\n\nfrom imputer import Imputer\nimport lightgbm as lgb\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, StandardScaler, RobustScaler\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier","6ad5251b":"print(os.listdir(\"..\/input\"))","1c8ada79":"data_train = pd.read_csv('..\/input\/application_train.csv')","db536644":"#print( list(data_train.columns) ) ","201a651e":"data_train.info()","417950ad":"index = 0\nfor i in data_train.drop(columns='SK_ID_CURR').columns:\n    if np.dtype(data_train[i]) == 'float64' and len(data_train[i].dropna())\/data_train.shape[0] > 0.6:\n        #if sum(data_train[i].dropna() == data_train[i].dropna().astype('int64')):\n            index +=1\n            plt.subplot(4, 5, index)\n            #sns.boxplot(x = data_train.TARGET, y = data_train[i].dropna()[abs(stats.zscore(data_train[i].dropna())) < 3])\n            curr_feature_1 = data_train[i][data_train.TARGET == 1].dropna()\n            curr_feature_0 = data_train[i][data_train.TARGET == 0].dropna()\n            sns.distplot(curr_feature_1[abs(stats.zscore(curr_feature_1)) < 3]) #to reduce most of outliers\n            sns.distplot(curr_feature_0[abs(stats.zscore(curr_feature_0)) < 3])\n\nplt.subplots_adjust(top=3, bottom=0, left=0, right=3, hspace=0.25, wspace=0.55)\nplt.show()","292b4bd0":"index = 0\nfor i in data_train.drop(columns=['SK_ID_CURR', 'TARGET']).columns:\n    if np.dtype(data_train[i]) == 'int64' and len(data_train[i].dropna())\/data_train.shape[0] > 0.6 and len(data_train[i].unique()) > 50:\n        index +=1\n        plt.subplot(1, 3, index)\n        curr_feature_1 = data_train[i][data_train.TARGET == 1].dropna()\n        curr_feature_0 = data_train[i][data_train.TARGET == 0].dropna()\n        #sns.boxplot(x = data_train.TARGET, y = data_train[i].dropna()[data_train.TARGET == 0])\n        #sns.boxplot(x = data_train.TARGET, y = data_train[i].dropna()[data_train.TARGET == 1])\n        sns.distplot(curr_feature_1)\n        sns.distplot(curr_feature_0)\nplt.subplots_adjust(top=1, bottom=0, left=0, right=3, hspace=0.25, wspace=0.55)\nplt.show()","1c8eda60":"plt.subplot(211)\nsns.distplot(np.log(data_train.AMT_INCOME_TOTAL[data_train.TARGET == 0] + 1))\nplt.subplot(212)\nsns.distplot(np.log(data_train.AMT_INCOME_TOTAL[data_train.TARGET == 1] + 1))\nplt.show()","220e674d":"sum(np.log(data_train.AMT_INCOME_TOTAL + 1) > 14) #outliers","01b16c9b":"index = 0\nfor i in data_train.drop(columns=['SK_ID_CURR', 'TARGET']).columns:\n    if np.dtype(data_train[i]) == 'O' and len(data_train[i].dropna())\/data_train.shape[0] > 0.6:\n        tab = pd.crosstab(data_train.TARGET, data_train[i], margins=True)\n        display(HTML((tab\/tab.loc[tab.index[-1]]).to_html()))","e975e803":"index = 0\nfor i in data_train.drop(columns=['SK_ID_CURR', 'TARGET']).columns:\n    if np.dtype(data_train[i]) == 'int64' and len(data_train[i].dropna())\/data_train.shape[0] > 0.6 and len(data_train[i].unique()) < 100:\n        tab = pd.crosstab(data_train.TARGET, data_train[i], margins=True)\n        display(HTML((tab\/tab.loc[tab.index[-1]]).to_html()))","8c40642f":"def chi_test(data, feature, target = 'TARGET', group_classes=False):\n    \n    if sum(pd.isna(data[feature])):\n        data[feature].replace(np.nan, 'Unknown', inplace=True)\n\n    cnt_table = pd.crosstab(data[target], data[feature])#to check if there are enough observations in each class\n    \n    if group_classes:\n        tab = pd.crosstab(data[target], data[feature], margins=True)\n        tab = tab\/tab.loc[tab.index[-1]]\n        labels = {}\n        for i in cnt_table.columns:\n            if tab[i][1] > tab['All'][1]:\n                labels[i] = 'High risk'\n            else:\n                labels[i] = 'Low risk'\n        cnt_bi_table = pd.crosstab(data[target], data[feature].replace(labels))\n        chi = stats.chi2_contingency(cnt_bi_table)\n        display(HTML(pd.crosstab(data[target], data[feature].replace(labels), margins=True).to_html()))\n        print( { 'Chi-square statisitc': chi[0],\n           'p-value': chi[1], \n          'df': chi[2]} )\n        return labels\n    else:\n        chi = stats.chi2_contingency(cnt_table)                           \n        display(HTML(pd.crosstab(data[target], data[feature], margins=True).to_html()))\n        print( { 'Chi-square statisitc': chi[0],\n           'p-value': chi[1], \n          'df': chi[2]} )","478b240b":"chi_test(data_train[data_train.CODE_GENDER != 'XNA'], 'CODE_GENDER')","069c0c88":"inc_labels = chi_test(data_train, 'NAME_INCOME_TYPE', group_classes=True)","e7e2e805":"hsng_labels = chi_test(data_train, 'NAME_HOUSING_TYPE', group_classes=True)","216de89a":"occup_labels = chi_test(data_train, 'OCCUPATION_TYPE', group_classes=True)","e684a441":"orgn_labels = chi_test(data_train, 'ORGANIZATION_TYPE', group_classes=True)","49ab310a":"data_train.shape","8ffc984a":"print( sum(data_train.FLAG_OWN_CAR == \"Y\") )\nprint( data_train.OWN_CAR_AGE.dropna().shape )","082af535":"data_train.OWN_CAR_AGE.fillna(value = 0, inplace=True)","5155f094":"print( data_train.OCCUPATION_TYPE.unique() )\ndata_train.OCCUPATION_TYPE.fillna(value = 'Unknown', inplace=True)","31a1b94a":"print( sum(data_train.FLAG_MOBIL == 0) )\nprint( sum(data_train.FLAG_MOBIL == 1) )","363c224d":"print( sum(data_train.FLAG_CONT_MOBILE == 0) )\nprint( sum(data_train.FLAG_CONT_MOBILE == 1) )","e11b8d11":"data_train.drop(columns=['FLAG_MOBIL', 'FLAG_CONT_MOBILE'], inplace=True)","de01020b":"data_train.shape","a662ed0a":"data_train['DOC_COUNT'] = data_train.FLAG_DOCUMENT_2\n\nfor i in range(3, 22):\n    data_train['DOC_COUNT'] = data_train['DOC_COUNT'] + data_train['FLAG_DOCUMENT_'+str(i)]","95a245db":"msno.matrix(data_train.iloc[:,0:42])\nplt.show()","bfa62f2b":"msno.matrix(data_train.iloc[:,42:89])\nplt.show()","cd58d4e3":"msno.matrix(data_train.iloc[:,89:])\nplt.show()","e5fa36aa":"for i in data_train.iloc[:, 42:56].columns:\n    data_train[i] = -pd.isna(data_train[i])\n\nfor i in data_train.iloc[:, 84:89].columns:\n    data_train[i] = -pd.isna(data_train[i])","a6e2de43":"data_train['HOUSE_INFO'] = (data_train.iloc[:, 84:89].sum(axis=1) + data_train.iloc[:, 42:56].sum(axis=1))\/19","4ec4f048":"#for i in data_train.iloc[:, 114:120].columns:\n#   data_train[i].fillna(data_train[i].median(), inplace = True)","04b84523":"data_train.drop(columns=data_train.iloc[:, 94:114].columns, inplace=True)\ndata_train.drop(columns=data_train.iloc[:, 42:89].columns, inplace=True)\ndata_train.drop(columns='EXT_SOURCE_1', inplace=True)","6655f93e":"sns.distplot(data_train.EXT_SOURCE_3.dropna())\nsns.distplot(data_train.EXT_SOURCE_3.fillna(data_train.EXT_SOURCE_3.dropna().median()))\nplt.show()","5a84a270":"data_train.info()","53d994a4":"to_be_scaled = ['DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_ID_PUBLISH']\nfor i in data_train.columns:\n    if data_train[i].dtype == 'float64':\n        to_be_scaled.append(i)","acfe8509":"#data_train.head()","2aa9e95c":"for i in data_train.columns:\n    if data_train[i].dtype == 'O':\n        print( [i, data_train[i].unique()] )","1d563dbe":"sum(data_train.CODE_GENDER == 'XNA')","86fe15cc":"data_train.CODE_GENDER.replace('XNA', 'F', inplace=True)\ndata_train.NAME_INCOME_TYPE.replace(inc_labels, inplace=True)\ndata_train.OCCUPATION_TYPE.replace(occup_labels, inplace=True)\ndata_train.ORGANIZATION_TYPE.replace(orgn_labels, inplace=True)\ndata_train.WEEKDAY_APPR_PROCESS_START.replace({'WEDNESDAY': 'Week', 'MONDAY': 'Week', 'THURSDAY': 'Week', 'SUNDAY': 'Weekend',\n                                               'SATURDAY': 'Weekend', 'FRIDAY': 'Week', 'TUESDAY': 'Week'}, inplace=True)","1ee4f1b9":"data_train.head()","07e2a693":"binarizer = LabelBinarizer()","d9b8e68d":"for i in data_train.columns:\n    if data_train[i].dtype == 'O' and len(data_train[i].unique()) == 2:\n        data_train[i] = binarizer.fit_transform(data_train[i])","04ca1241":"#data_train.head()","92e39898":"encoder = LabelEncoder()","d35937e9":"sum(data_train.NAME_TYPE_SUITE.isnull())","4d13b3dd":"data_train.NAME_TYPE_SUITE = encoder.fit_transform(data_train.NAME_TYPE_SUITE.replace(np.nan, 'Unknown'))","74d9948d":"print( encoder.classes_ )\nsum(data_train.NAME_TYPE_SUITE == 7)","56923b17":"data_train.NAME_TYPE_SUITE.replace('Unknown', np.nan, inplace=True)","46aa5eac":"data_train.NAME_EDUCATION_TYPE = encoder.fit_transform(data_train.NAME_EDUCATION_TYPE)","ecc74115":"encoder.classes_ #ordered","31055d26":"data_train = pd.get_dummies(data_train)","407557ca":"data_train.shape","2b01f578":"impute = Imputer()","2ba7e0f6":"%%time\ndata_train_cl = pd.DataFrame(impute.knn(X=data_train, column='EXT_SOURCE_3', k = 3), columns=data_train.columns)","b679ffdc":"sns.distplot(data_train.EXT_SOURCE_3.dropna())\nsns.distplot(data_train_cl.EXT_SOURCE_3)\nplt.show()","5fc12f16":"%%time\nfor i in data_train.columns:\n    if sum(data_train[i].isnull()):\n        data_train_cl = pd.DataFrame(impute.knn(X=data_train_cl, column=i, k = 3), columns=data_train.columns)","82c12bdb":"data_train_cl.head()","8581d5b2":"std = RobustScaler()\nfor i in to_be_scaled:\n    data_train_cl[i] = std.fit_transform(pd.DataFrame(data_train_cl[i], columns=[i]))","1a755f73":"X_train, X_test, y_train, y_test = train_test_split(data_train_cl.drop(columns='TARGET'), \n                                                    data_train_cl.TARGET, test_size=0.2, random_state=23, stratify=data_train_cl.TARGET)","48105578":"%%time\nresults = {}\nfor i in [0.001, 0.01, 0.1, 1, 2]:\n    lgreg = LogisticRegression(C=i, class_weight='balanced', penalty='l2', max_iter=1000)\n    lgreg.fit(X=X_train, y=y_train)\n    pred = lgreg.predict(X_test)\n    results[i] = roc_auc_score(y_test, pred)","96fd4937":"results","d7f81733":"forest = RandomForestClassifier(max_depth=10, n_estimators=200, class_weight='balanced', n_jobs=-1)","760f8c4e":"%%time\nforest.fit(X_train, y_train)","05235f84":"pred2 = forest.predict(X_test)","052d1b83":"roc_auc_score(y_test, pred2)","551dbb07":"# for validation lgb\nX_train_v, X_test_v, y_train_v, y_test_v = train_test_split(X_train, \n                                                    y_train, test_size=0.2, random_state=23)","9ba91bd5":"train_data = lgb.Dataset(X_train_v, label=y_train_v)\ntest_data = lgb.Dataset(X_test_v, label=y_test_v)","d91cdd9f":"parameters = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting': 'gbdt',\n    'num_leaves': 50,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 30,\n    'learning_rate': 0.05,\n    'verbose': 0\n}\n","6272cc48":"model = lgb.train(parameters,\n                       train_data,\n                       valid_sets=test_data,\n                       num_boost_round=5000,\n                       early_stopping_rounds=100)","0d53c5be":"pred4= model.predict(X_test)","1ecbc1b1":"roc_auc_score(y_test, pred4)","0e8acea5":"Well, according to graphs (taking into account that they are in the interval of 3 sd), some of them are visualised in inappropriate way, except those that are actually continious. So, some of the variables show different distribution between classes like EXT_SOURCE_2(3), DAYS_BIRTH, and probably AMT_INCOME_TOTAL which is quite right-skewed and has outliers, so I need to look a bit closer or apply some transformation. ","1eb62db2":"## Data preprocessing","dc2369d7":"## Model Building","99c220ec":"Firstly, I'd like to see distributions of continious variables in dataset according to their class and find variables that have significant difference between classes and can have a bigger impact inside the model.","bd5a8577":"Well, as for me, imputations that made using mean\/median are not suitable in that case because they violate the distribution. Probably, it would be better to use another strategy, for example, KNN to fill these gaps taking into account what values have nearest objects in a multidimensional space, but firstly, it's better to encode categorical features in order to perform imputation function faster. ","a9ca392a":"Here we see the difference in about 4%, not much, but still it can be included in the model in such division, because it has too many classes.","311412aa":"In this block, I'll perform chi-square test for those variables that seem to have a significant association with TARGET.","4eefb0a4":"The last five columns shows the number of enquiries to Credit Bureau and number of enquiries of each next column excludes those that were already marked in previous one. Nans make up 14% in each column. I'll fill them later.","0aeccf56":"### Random Forest","81f87f5e":"Well, although p-value is significant, the difference in TARGET = 1 between groups is about 3%, therefore I don't think that this feature should be present in model with such division.","a9ae7786":"Well, we see that almost all features from the second plot have too many missings. All these features are connected with the information about apartments. As it would be inaccurate to use features with such number of missings, probably it would be better to create one feature from them, that would represent the percentage of known information about client's apartments.","f0274dc1":"Next subset of features is about means of communications provided by client. There are very imbalanced classes inside each feature, so it should be taken into account during train-test split. Moreover some features are presented by only one class that significantly prevail and some of them can be dropped.","e96f861f":"So, here we see that while the groups High risk and Low risk have almost the same number of observatoins (158801 and 148710) the class TARGET=1 is almost 60% frequent than TARGET=0, therefore probably such division can be informative in model. The same can be done with the next feature NAME_EDUCATION_TYPE, but as far as this feature is ordinal, I think it'd better to put them in right order while encoding. ","77a596d8":"### Logistic regression","d5f3cc41":"Now, let's take a look at the varibles of type object,  I'll create crosstables that will show the percentege of TARGET = 1 and TARGET = 0 inside each class of presented variables in order to indicate class or set of them that can be associated with TARGET = 1. However, on the stage of inferential statistics it should be taken into account that there are some very unbalanced classes presented in the tables below.","12828c59":"## EDA\nFirst of all, I'm going to deal with the main dataset \"application_train.csv\".  As most of descriptive staistics and visualization (e. g. distributions of single variables, number of nans, etc.) are presented on [Kaggle](https:\/\/www.kaggle.com\/c\/home-credit-default-risk\/data#), I'll use it without reduplicating the same information in this notebook.","75bd61a1":"### Gradient boosting","b6375b89":"Testing on left-off set","a3a1ae89":"The same is with binary features that are presnted below.","4e299469":"So, there are some varibles and classes that have a higher percenatge of TARGET = 1. Well, probably it would be better to redefine multinominal features into two classes, for example, \"Higher risk\" and \"Lower risk\", what will also allow us not to enlarge the dimensionality with dummy encoding, and then make sure that difference in percentages are statistically significant (e.g. apply chi-square criteria).","a37f7cac":"Well, obviously nans are produced by absence of the car, then we can just impute 0 instead of nans. ","873a6033":"### Dealing with NA","6c9eaa33":"The next subset (second plot below) of features has too many nans (>50%) and it would be better to see how these nans distributed in our data. ","f05086dd":"Now dropping all columns that have (>=50%) of values missing and those that was combined into one (DOC_COUNT).","5beeab45":"In this feature there is also a significant difference in numbers between High risk TARGET=1 and Low risk TARGET = 1, moreover such grouping in this feature again reduces quite a big number of classes (58).","b1b37698":"The are quite a lot of features that have too many missing values. Let's see what can be done about it. The first feature OWN_CAR_AGE has for about 66% values missing.  As there is also FLAG_OWN_CAR feature that represents if client have a car, we will see if nans can be explained by absence of the car.","056c36aa":"The next feature is OCCUPATION_TYPE and we don't know the origin of nans. I suppose that it can be two things: client refused telling it or he doesn't have a job, but still both of these tells us some information about client, therefore I will replace nans by 'Unknown'.  After cleaning data from missing values I'll encode this feature.","8c6fe364":"Another subset of features is about provided documents and almost all of them have an enormous dominance by one class, so I think it would make sense to combine them to feature which will represent the number of provided documents.","a780d22b":"### Standardization","f5aef118":"### Encoding","2fe5bf12":"## Loading packages and data","baee4ba0":"The last column with a significant number of gaps is EXT_SOURCE_3, that has 16% of values missing, therefore I must find an appropriate way to fill them.","8204800f":"### Imputation","fdfcbb29":"## Hypotheses testing"}}