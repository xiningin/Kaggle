{"cell_type":{"de942043":"code","bd1fa406":"code","94527bba":"code","033f9b24":"code","f9b8d4cb":"code","4530642d":"code","1c944bc5":"code","9b08e2aa":"code","a848e446":"code","0ff0872b":"code","7b43d339":"code","77ffa655":"code","acfd4c38":"code","af7e0905":"code","e84f564d":"code","6b9c59c8":"code","117f2384":"code","360c5fe9":"code","a497b017":"code","7ff194e9":"markdown"},"source":{"de942043":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bd1fa406":"# CORE\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport matplotlib.pyplot as plt\nimport time\nimport math\n\n# warnings\npd.options.mode.chained_assignment = None  # default='warn'\n\n# ML\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC","94527bba":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nprint(\"Taille du dataset: {}\".format(train.shape))\ntrain","033f9b24":"#  CHECK SI ON A DES DOUBLONS\nnb_avant = len(train)\nprint('nb avant:', nb_avant)\ntrain.drop_duplicates(keep='first', inplace=True) # supprimer les doublons\ntrain.reset_index(drop=True, inplace=True) # reset les indices apres avoir retirer les doublons\nnb_apres = len(train)\nprint('nb_apres:', nb_apres)\nif(nb_avant == nb_apres):\n    print(\"Pas de doublon\")\nelse:\n    print(\"{} doublons supprim\u00e9s\".format(nb_avant-nb_apres))","f9b8d4cb":"def fill_age(x):\n    # replace NA in \"Age\"\n    classe = x[\"Pclass\"]\n    sexe = x[\"Sex\"]\n    age = x[\"Age\"]\n    if pd.isna(age):\n        return int(train[(train[\"Pclass\"]==classe) & (train[\"Sex\"]==sexe)][\"Age\"].mean())\n    else:\n        return int(age)","4530642d":"def pre_process(dataset):\n    # drop Cabin column\n    dataset.drop(columns=[i for i in [\"Cabin\"] if i in list(dataset.columns)],\n             inplace=True) \n    \n    # Fill NA in Age (cf fill_age)\n    dataset.loc[:,\"Age\"] = dataset[[\"Pclass\",\"Sex\",\"Age\"]].apply(fill_age,axis=1)\n    \n    #drop NA in Embarked\n    dataset = dataset.dropna(subset = [\"Embarked\"])\n    \n    # Replace NA in Fare by the mean of the Fare for the same Pclass\n    na_Fare = list(dataset[dataset[\"Fare\"].isna()].index)\n    if na_Fare:\n        class_na_Fare = list(dataset.loc[dataset[dataset[\"Fare\"].isna()].index,(\"Pclass\")])[0]\n        dataset.loc[na_Fare,(\"Fare\")] = train.loc[(train[\"Pclass\"] == class_na_Fare),\"Fare\"].mean()\n            \n    # Mapping values in Sex\n    dataset.loc[:,\"Sex\"] = dataset[\"Sex\"].replace({\"male\":1,\"female\":0})\n    \n    # Drop Name & Ticket (useless)\n    dataset.drop(columns=[\"Name\",\"Ticket\"],inplace=True)\n    \n    \n    # Using LabelEncoder\n    temp = dataset[[\"Embarked\"]]\n    list_var_qual = list(temp.columns)\n    print()\n    print(\"Mapping from LabelEncoder()\")\n    for i in list_var_qual:\n        le = LabelEncoder()\n        dataset.loc[:,i] = le.fit_transform(dataset[i])\n        le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n        print(i, le_name_mapping)\n    print()\n    return dataset","1c944bc5":"print(\"Train :\")\ntrain_clean = pre_process(train)\n\nprint(\"Test :\")\ntest_clean = pre_process(test)","9b08e2aa":"x_train, x_test, y_train, y_test = train_test_split(train_clean.drop(columns=[\"PassengerId\",\"Survived\"]), train_clean[\"Survived\"], test_size=0.25, random_state=42)","a848e446":"x_train","0ff0872b":"# Normalisation\nss=StandardScaler()\nss.fit(x_train)\n# x_train_values = x_train.values\n# x_test_values = x_test.values\nx_train_values=ss.transform(x_train)\nx_test_values=ss.transform(x_test)","7b43d339":"def fitAndPredict(model):\n    \"\"\"The following code makes faster to evaluate a model \n    automating the fit and accuracy process\"\"\"\n    \n    model.fit(x_train_values, y_train)\n    prediction = model.predict(x_test_values)\n    return accuracy_score(y_test, prediction)","77ffa655":"# Try different models\nmodel1 = LogisticRegression(solver='liblinear', random_state=0)\nmodel2 = LogisticRegression(solver='liblinear',class_weight=\"balanced\",random_state=0)\nmodel3 = BaggingClassifier(random_state=0)\nmodel4 = GradientBoostingClassifier()\nmodel5 = RandomForestClassifier()\nmodel6 = SGDClassifier()\nmodel7 = SVC()\n\nmodels = [model1, model2, model3, model4, model5]\nfor i,model in enumerate(models):\n    print(\"Model \", i,\":\", model)\n    print(\"ACC: \", fitAndPredict(model))","acfd4c38":"# GradientBoostingClassifier() is the best so we try to tune its parameters\nmodel = GradientBoostingClassifier(min_samples_split=20, min_samples_leaf=60, max_depth=3)\nfitAndPredict(model)","af7e0905":"test_clean","e84f564d":"testdf_values = ss.fit_transform(test_clean.drop(columns=\"PassengerId\").values)\noutput = model.predict(testdf_values)","6b9c59c8":"output","117f2384":"submission = pd.DataFrame({\n        \"PassengerId\": test_clean[\"PassengerId\"],\n        \"Survived\": output\n    })","360c5fe9":"submission","a497b017":"submission.to_csv('\/kaggle\/working\/submission.csv', index=False)","7ff194e9":"# ====================="}}