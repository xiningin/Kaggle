{"cell_type":{"fae890e0":"code","dba54b52":"code","d9a3b23e":"code","d7603b1a":"code","30034be5":"code","e55d0b9b":"code","e4e9fa8e":"code","c06d6033":"code","9b3e2b74":"code","3bde92a7":"code","dabff8ba":"code","779ce2fa":"code","a4f326d8":"code","9ba062b4":"code","296488c2":"code","8b51a4b1":"code","fb304643":"code","e65e7d3b":"code","6a33423d":"code","30580ac7":"code","3e6bfd6a":"code","17c59495":"code","fbb05e69":"code","a39fc238":"markdown","87468b42":"markdown","a7f552bc":"markdown","9aaf3dc3":"markdown","dcc106f3":"markdown","e2c89e90":"markdown","5b512e1d":"markdown","2bdc90bc":"markdown","8016466f":"markdown","079a2676":"markdown","fc079b6b":"markdown"},"source":{"fae890e0":"import gc\nimport itertools\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import StratifiedKFold\n\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras.layers import Input, BatchNormalization\nfrom tensorflow.keras.layers import Dense, Dropout, Multiply\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nnp.random.seed(42)\ntf.random.set_seed(42)","dba54b52":"train_df = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")\ntrain_df.set_index('id', inplace=True)\nprint(f\"train_df: {train_df.shape}\")\ntrain_df.head()","d9a3b23e":"test_df = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\")\ntest_df.set_index('id', inplace=True)\nind = test_df.index.tolist()\nprint(f\"test_df: {test_df.shape}\")\ntest_df.head()","d7603b1a":"mode = 'add'\n\nif mode == 'add':\n    \n    features = test_df.columns.tolist() \n\n    for col in tqdm(features):\n        train_df[col+'_bin'] = train_df[col].apply(lambda x: 1 if np.cbrt(x)>0 else 0) #1\n        test_df[col+'_bin'] = test_df[col].apply(lambda x: 1 if np.cbrt(x)>0 else 0) #1\n    \n    print('Calc train features...')\n\n    train_df[\"mean\"] = train_df[features].mean(axis=1)\n    train_df[\"std\"] = train_df[features].std(axis=1)\n    train_df[\"min\"] = train_df[features].min(axis=1)\n    train_df[\"max\"] = train_df[features].max(axis=1)\n    train_df[\"median\"] = train_df[features].median(axis=1)\n\n\n    \n    print('Calc test features...')\n    test_df[\"mean\"] = test_df[features].mean(axis=1)\n    test_df[\"std\"] = test_df[features].std(axis=1)\n    test_df[\"min\"] = test_df[features].min(axis=1)\n    test_df[\"max\"] = test_df[features].max(axis=1)\n    test_df[\"median\"] = test_df[features].median(axis=1)\n\n\n    features = test_df.columns.tolist()\n\n    features.extend(['mean', 'std', 'min', 'max', 'median'])\n\n    print(f\"train_df: {train_df.shape} \\ntest_df: {test_df.shape}\")\n\nelse:\n    pass","30034be5":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\n\ndef PCA_transform(train, test):\n    pca = PCA(n_components=16, whiten = False)\n    train = train.drop(columns=['target']).values\n    test = test.values\n    train = pca.fit_transform(train)\n    test = pca.transform(test)\n    return train, test\n\ndef Q_transform(train, test):\n    qtf = QuantileTransformer(random_state=21, n_quantiles=2000, output_distribution='normal')\n    train = train.drop(columns=['target']).values\n    test = test.values\n    train = qtf.fit_transform(train)\n    test = qtf.transform(test)\n    return train, test\n\ndef svd_whiten(train, test):\n    train = train.drop(columns=['target']).values\n    test = test.values\n    \n    U, s, Vt = np.linalg.svd(train, full_matrices=False)\n    train_arr = np.dot(U, Vt)\n    \n    U, s, Vt = np.linalg.svd(test, full_matrices=False)\n    test_arr = np.dot(U, Vt)\n    # U and Vt are the singular matrices, and s contains the singular values.\n    # Since the rows of both U and Vt are orthonormal vectors, then U * Vt\n    # will be white\n    return train_arr, test_arr\n\ndef scale_data(train, test):\n    feats_to_scale = ['f{}' for e in range(100)]\n    feats_to_add = ['f{}_bin' for e in range(100)] + ['mean', 'std', 'min', 'max', 'median']\n    \n    \n    train = train.drop(columns=['target']).values\n    test = test.values\n    \n    scaler = MinMaxScaler(feature_range=(0, 1))\n    train_arr = scaler.fit_transform(train)\n    test_arr = scaler.transform(test)\n\n    return train_arr, test_arr","e55d0b9b":"mode = None\n\nif mode == 'PCA':\n    targets = train_df.target.values\n    train_arr, test_arr = PCA_transform(train_df, test_df)\nelif mode == 'Quantile':\n    targets = train_df.target.values\n    train_arr, test_arr = Q_transform(train_df, test_df)\nelif mode == 'SVD':\n    targets = train_df.target.values\n    train_arr, test_arr = svd_whiten(train_df, test_df)\nelif mode == 'SCALE':\n    targets = train_df.target.values\n    train_arr, test_arr = scale_data(train_df, test_df)\nelse:\n    targets = train_df.target.values\n    train_arr = train_df.drop(columns=['target']).values\n    test_arr = test_df.values","e4e9fa8e":"print(train_arr.shape, test_arr.shape)","c06d6033":"print(train_arr.max(), train_arr.min(), test_arr.max(), test_arr.min())","9b3e2b74":"train_df = pd.DataFrame(train_arr)\ntest_df = pd.DataFrame(test_arr)\nfeatures = train_df.shape[1]\nprint(f\"Num features: {train_df.shape[1]}\")","3bde92a7":"def plot_confusion_matrix(cm, classes):\n\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix', fontweight='bold', pad=15)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], 'd'),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontweight='bold')\n    plt.xlabel('Predicted label', fontweight='bold')\n    plt.tight_layout()","dabff8ba":"from tensorflow.keras.layers import Reshape, Conv2D, GlobalMaxPooling2D, UpSampling2D, MaxPooling2D, Flatten, AveragePooling2D\n\ndef dnn_model():\n    x_input = Input(shape=(features,))\n    \n    x1 = Dense(units=386, activation='swish')(x_input)\n    x1 = BatchNormalization()(x1)\n    x2 = Dropout(rate=0.45)(x1)\n    \n    x2 = Dense(units=192, activation='swish')(x2)\n    x2 = BatchNormalization()(x2)\n    x3 = Dropout(rate=0.35)(x2)\n    \n    x3 = Dense(units=98, activation='swish')(x3)\n    x3 = BatchNormalization()(x3)\n    x3 = Dropout(rate=0.25)(x3)\n    \n    x4 = Dense(units=192, activation='swish')(x3)\n    x4 = BatchNormalization()(x4)\n    x4 = Multiply()([x2, x4])\n    x4 = Dropout(rate=0.35)(x4)\n    \n    x5 = Dense(units=386, activation='swish')(x4)\n    x5 = BatchNormalization()(x5)\n    x5 = Multiply()([x1, x5])\n    x5 = Dropout(rate=0.45)(x5)\n    \n    x_e1 = Concatenate()([x3, x5])\n    x = Reshape((22, 22, 1))(x_e1)\n    \n    x = Conv2D(16, kernel_size=(3, 3), padding='same', activation='swish')(x)#16\n    x = AveragePooling2D((2, 2), padding='same')(x)\n    x = Conv2D(8, kernel_size=(3, 3), padding='same', activation='swish')(x)#8\n    x = AveragePooling2D((2, 2), padding='same')(x)\n    x = Conv2D(8, kernel_size=(3, 3), padding='same', activation='swish')(x)#8\n    \n    enc = AveragePooling2D((2, 2), padding='same')(x)\n    \n    flat = Flatten()(enc)\n    \n    x = Concatenate()([x_e1, flat])\n    \n    x = Dense(units=128, activation='swish')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(rate=0.25)(x)\n    \n    x_output = Dense(units=1, activation='sigmoid')(x)\n\n    model = Model(inputs=x_input, outputs=x_output, \n                  name='DNN_Model')\n    return model","779ce2fa":"model = dnn_model()","a4f326d8":"from tensorflow.keras.utils import plot_model\nplot_model(\n    model, \n    to_file='DNN_CNN_Model.png', \n    show_shapes=True,\n    show_layer_names=True\n)","9ba062b4":"import plotly\nimport plotly.graph_objs as go\nimport numpy as np\nimport random\nimport umap\n\nSAVE_FLAG = False\nfile_out = '.\/default.html'\n\nto_vis = pd.DataFrame(train_arr)\nto_vis['label'] = targets\nto_vis = to_vis.sample(n=5000)\nlabels = to_vis.label.tolist()\n\nprint(to_vis.label.value_counts())\n\nX_emb = to_vis.drop(columns=['label'])\n\nmetric_umap = 'correlation'\nadapted_embedded = umap.UMAP(n_neighbors=25,\n                      n_components=3,\n                      min_dist=0.5,\n                      metric=metric_umap).fit_transform(X_emb)\n\nadapted_embedded = pd.DataFrame(adapted_embedded, columns=['V1', 'V2', 'V3'])\nadapted_embedded['to_view'] = ''\nadapted_embedded['labels'] = labels\nlabels_list = adapted_embedded.labels.value_counts().index.tolist()\n\n\n\ndef plot_tsne(df:pd.DataFrame, label_list, to_save = False, add_custom_legend = False, file_out = 'default.html'):\n    list_of_plots = []\n    for i, lab in enumerate(label_list):\n        adapted_embedded_f = df[df.labels == lab]\n        color = random.randint(0, 0xFFFFFF)\n        \n        x1 = adapted_embedded_f['V1'].values\n        y1 = adapted_embedded_f['V2'].values\n        z1 = adapted_embedded_f['V3'].values\n        \n\n        plot_f = go.Scatter3d(\n                x=x1,\n                y=y1,\n                z=z1,\n                hovertext = adapted_embedded_f.to_view.tolist(),\n                mode='markers',\n                marker=dict(\n                    size=4,\n                    color=color,            \n                    symbol='circle',\n                    line=dict(\n                            color=color,\n                            width=1\n                        )\n                ),\n                name=lab,\n            )\n        list_of_plots.append(plot_f)\n    \n    fig = go.Figure(data=list_of_plots)\n    \n    if add_custom_legend:\n        fig.update_layout(\n        legend=dict(\n                x=0,\n                y=1,\n                traceorder=\"reversed\",\n                title_font_family=\"Calibri\",\n                font=dict(\n                    family=\"Courier\",\n                    size=12,\n                    color=\"black\"\n                ),\n                bgcolor=\"White\",\n                bordercolor=\"Black\",\n                borderwidth=1\n                )\n            )\n        \n    plotly.offline.iplot(fig)\n    if to_save:\n        plotly.offline.plot(fig, filename=file_out)\n        \nplot_tsne(adapted_embedded, labels_list, to_save=SAVE_FLAG, file_out = file_out)","296488c2":"FOLD = 5\nVERBOSE = 0\nSEEDS = [2021, 2025]\nBATCH_SIZE = 2048\nCLASS_WEIGHT_FLAG = False\n\ncounter = 0\noof_score = 0\ny_pred_final_dnn = np.zeros((test_df.shape[0], 1))\ny_pred_meta_dnn = np.zeros((train_df.shape[0], 1))\n\n\nfor sidx, seed in enumerate(SEEDS):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(train_arr, targets)):\n        counter += 1\n        \n        #define train and validation data by indices\n        train_x, train_y = train_arr[train], targets[train]\n        val_x, val_y = train_arr[val], targets[val]\n        \n        #count zero and one label ratios\n        zero = np.unique(train_y, return_counts=True)[1][0]\n        one = np.unique(train_y, return_counts=True)[1][1]\n        total = len(train_y)\n\n        weight_for_0 = (1 \/ zero)*(total)\/2.0 \n        weight_for_1 = (1 \/ one)*(total)\/2.0\n\n        class_weight = {0: weight_for_0, 1: weight_for_1}\n\n        print('Weight for class 0: {:.2f}'.format(weight_for_0))\n        print('Weight for class 1: {:.2f}'.format(weight_for_1))\n        \n        #init and compile model\n        model = dnn_model()\n        model.compile(optimizer=Adam(learning_rate=1e-2), \n                      loss=\"binary_crossentropy\", \n                      metrics=['AUC'])\n\n        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, \n                               patience=7, verbose=VERBOSE)\n        \n        chk_point = ModelCheckpoint(f'.\/Keras_DNN_Model_{counter}C.h5', \n                                    monitor='val_loss', verbose=VERBOSE, \n                                    save_best_only=True, mode='min')\n\n        es = EarlyStopping(monitor=\"val_loss\", patience=30, \n                           verbose=VERBOSE, mode=\"min\", \n                           restore_best_weights=True)\n        \n        if CLASS_WEIGHT_FLAG == False:\n            class_weight = None\n        else:\n            pass\n        \n        #train model\n        model.fit(train_x, train_y, \n                  validation_data=(val_x, val_y), \n                  epochs=900,\n                  verbose=VERBOSE,\n                  class_weight=class_weight,\n                  batch_size=BATCH_SIZE, \n                  callbacks=[lr, chk_point, es])\n        \n        #load model and perform tests\n        model = load_model(f'.\/Keras_DNN_Model_{counter}C.h5')\n        \n        y_pred = model.predict(val_x, batch_size=BATCH_SIZE)\n        y_pred_meta_dnn[val] += y_pred\n        y_pred_final_dnn += model.predict(test_arr, batch_size=BATCH_SIZE)\n        \n        score = roc_auc_score(val_y, y_pred)\n        oof_score += score\n        seed_score += score\n        print(\"\\nSeed-{} | Fold-{} | OOF Score: {}\\n\".format(seed, idx, score))\n        \n        del model, y_pred\n        del train_x, train_y\n        del val_x, val_y\n        gc.collect()\n    \n    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_meta_dnn = y_pred_meta_dnn \/ float(len(SEEDS))\ny_pred_final_dnn = y_pred_final_dnn \/ float(counter)\noof_score \/= float(counter)","8b51a4b1":"print(\"Aggregate OOF Score: {}\".format(oof_score))","fb304643":"y_pred_meta = np.mean(y_pred_meta_dnn, axis=1)\ny_pred = (y_pred_meta>0.5).astype(int)\nprint(classification_report(targets, y_pred))","e65e7d3b":"cnf_matrix = confusion_matrix(targets, y_pred, labels=[0, 1])\nnp.set_printoptions(precision=2)\nplt.figure(figsize=(12, 5))\nplot_confusion_matrix(cnf_matrix, classes=[0, 1])","6a33423d":"def avg_results(l1, l2):\n    en_l = []\n    for i, e in enumerate(l1):\n        r = (0.3 * e) + (0.7 * l2[i])\n        en_l.append(r)\n        \n    return en_l\n\nl1 = y_pred_final_dnn.ravel()\nl2 = pd.read_csv('..\/input\/simple-nn-with-good-results-tps-nov-21\/submission.csv').target.tolist()\n\nens = avg_results(l1, l2)\n\ndf_vis = pd.DataFrame()\ndf_vis['model1'] = l1\ndf_vis['model2'] = l2\ndf_vis['model_ens'] = ens","30580ac7":"#DNN\/CNN model prob dist\ndf_vis.model1.hist(bins=1000)","3e6bfd6a":"#Simple NN model prob dist\ndf_vis.model2.hist(bins=1000)","17c59495":"#Ens model prob dist\ndf_vis.model_ens.hist(bins=1000)","fbb05e69":"submit_df = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv\")\nsubmit_df['target'] = ens\nsubmit_df.to_csv(\"submission.csv\", index=False)\nsubmit_df.head()","a39fc238":"# Training and testing\n\nClass weighting implemented mechanism is controled by CLASS_WEIGHT_FLAG, by default it's off (data is almost balanced).","87468b42":"# Some weighted average ensemble\n","a7f552bc":"# Import libraries","9aaf3dc3":"# Keras Model\n\nModel structure was enhanced by an additional branch of CONV2D which was to extract additional features concatenated then with Enc\/Dec outputs.","dcc106f3":"# Feature Engineering\nI was basing on DLASTSTARK feature engineering proposal but also added some statistics as : *'mean', 'std', 'min', 'max', 'median'*. During many tests found it worth to consider.","e2c89e90":"# Load source datasets","5b512e1d":"# Intro\n\n**This notebook is highly based on amazing work done by DLASTSTARK, JAVIER VALLEJOS and PRIYANSHU CHAUDHARY, you may find link below:**\n\n* https:\/\/www.kaggle.com\/dlaststark\/tps-1121-dnn-v4\n* \nhttps:\/\/www.kaggle.com\/javiervallejos\/simple-nn-with-good-results-tps-nov-21\n* https:\/\/www.kaggle.com\/chaudharypriyanshu\/understanding-neural-net\n\n\n    \n**Please give them fully deserved interest!** \ud83d\udc4f\n\n![](https:\/\/media.giphy.com\/media\/I4wGMXoi2kMDe\/giphy-downsized-large.gif)","2bdc90bc":"# Create Submission","8016466f":"# Helper Function","079a2676":"# UMAP VIS\nSome sample visualisation by umap reduction in 3D space, found it useful to compare scaling methods.\nIf you are not familiar with umap check it out here: https:\/\/umap-learn.readthedocs.io\/en\/latest\/basic_usage.html","fc079b6b":"# Scaling \/ Dim Reduction \/ Whitening\nBelow you may switch between dim reduction \/ scaling \/ whitening methods prepared above. By default I did not use them.\nIf you are novice you may find some interesting information below:\n\n1. PCA: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html\n2. Quantile Transformation: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.QuantileTransformer.html\n3. MinMaxScaler: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html\n4. Whitening: https:\/\/en.wikipedia.org\/wiki\/Whitening_transformation"}}