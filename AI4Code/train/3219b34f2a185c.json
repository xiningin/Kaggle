{"cell_type":{"fbdd991a":"code","4f132511":"code","44292683":"code","4ed7c76d":"code","72517bc6":"code","f69abe14":"code","12f9291c":"code","31719fc4":"code","838e0738":"code","94dbc9a8":"code","37c8722d":"code","37b6dd4f":"code","3f5c0ec4":"code","81613a19":"code","6c6b4ed3":"code","a2a0691c":"code","af8498a5":"code","8945af21":"code","b1e26305":"code","697316bd":"code","8ea3dbf3":"code","a43cd28d":"code","ca4fe921":"code","9f162736":"code","5ed91c6d":"markdown","781b8bd0":"markdown","6e2ee4fa":"markdown","c221b3cc":"markdown","efa96f7d":"markdown","47b718de":"markdown","6819a1f5":"markdown","3100d449":"markdown","f141e347":"markdown","eaeebdd3":"markdown","df86262e":"markdown","3d9d685d":"markdown","0b31ed1c":"markdown","6dbcc5a8":"markdown","4d5506f6":"markdown","3beb69ca":"markdown","3b985f98":"markdown","b6f7576a":"markdown","6c09c400":"markdown","4c4de661":"markdown","922bcbe6":"markdown","43184454":"markdown","5d4d1e3e":"markdown","d3c9ed03":"markdown","7cae9c4d":"markdown","6dad2d57":"markdown","da8e2ae3":"markdown"},"source":{"fbdd991a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nimport re\nimport nltk \nfrom nltk.corpus import stopwords \nfrom nltk.stem.porter import PorterStemmer \nfrom nltk.util import ngrams\nfrom wordcloud import WordCloud, ImageColorGenerator, get_single_color_func\nfrom collections import defaultdict\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n\n\nplt.style.use('ggplot')\nstop=set(stopwords.words('portuguese'))\n\nimport string\nimport warnings\nwarnings.simplefilter('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4f132511":"#Importing MBA-SP Dataset\ndf1 = pd.read_json('..\/input\/dsasp\/result.json')\n#Importing MBA-General Dataset\ndf2 = pd.read_json('..\/input\/chat-geral\/result.json')\n\n#Grouping Datasets\ndata = df1.append(df2)\n\ndata.head(5)","44292683":"#Isolating messages key\ndf = pd.DataFrame()\ndf['message'] = data.messages\ndf['name'] = data.name\n\ndf.reset_index(drop=True)","4ed7c76d":"#Transforming the dataset to a message list\nx = []\ny = []\n\nfor index, row in enumerate(df['message']):\n    if row['type'] == 'message':            #Filter type=message  \n        if type(row['text']) == str:        #Filter only str types   \n            if len(row['text']) > 0:        #Removing empty results\n                x.append(str(row['text']))  #Appending\n                y.append(index)\n\ndf_names = df['name'].iloc[y].reset_index(drop=True)\n\ndataset = pd.DataFrame({'name': df_names,\n                        'message': x})\n\ndataset","72517bc6":"# extracting the number of examples of each class\nmba_sp = dataset[dataset['name'] == 'MBA USP SP']\nmba_general = dataset[dataset['name'] == '2021 MBA USP Data Science e Analytics (ESALQ)']","f69abe14":"# bar plot of the 3 classes\nplt.rcParams['figure.figsize'] = (7, 5)\nplt.bar(10,mba_sp.count(),3, label=\"MBA - SP\", color='blue')\nplt.bar(15,mba_general.count(),3, label=\"MBA - GENERAL\", color='green')\nplt.legend()\nplt.ylabel('messsages')\nplt.title('total messages')\nplt.show()","12f9291c":"def length(text):    \n    '''a function which returns the length of text'''\n    return len(text)\n\ndataset['length'] = dataset['message'].apply(length)\n\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nbins = 300\n\nplt.hist(mba_sp['message'].apply(length), bins=bins, label='MBA SP', color = 'blue')\nplt.hist(mba_general['message'].apply(length), bins=bins, label='MBA GENERAL', color='green')\n\nplt.xlabel('length')\nplt.ylabel('messages')\n\nplt.legend(loc='upper right')\nplt.xlim(0,150)\nplt.grid()\nplt.show()","31719fc4":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\nbins = 50\n\n\nax1.hist(mba_sp['message'].str.len(),color='blue', bins=bins)\nax1.set_title('MBA - SP')\n\nax2.hist(mba_general['message'].str.len(),color='green', bins=bins)\nax2.set_title('MBA - GENERAL')\n\nfig.suptitle('total characters per message')\n\nplt.show()","838e0738":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\nbins=50\n\nax1.hist(mba_sp['message'].str.split().map(lambda x: len(x)),color='blue', bins=bins)\nax1.set_title('MBA - SP')\n\nax2.hist(mba_general['message'].str.split().map(lambda x: len(x)),color='green', bins=bins)\nax2.set_title('MBA - GENERAL')\n\nfig.suptitle('total words per message')\nplt.show()","94dbc9a8":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n\nsns.distplot(mba_sp['message'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x)),ax=ax1,color='blue')\nax1.set_title('MBA - SP')\n\nsns.distplot(mba_general['message'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x)),ax=ax2,color='red')\nax2.set_title('MBA - GERAL')\n\nfig.suptitle('average word length per message')","37c8722d":"def create_corpus(target):\n    corpus=[]\n    \n    for x in target['message'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","37b6dd4f":"corpus=create_corpus(mba_sp)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\n\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y=zip(*top)\nplt.suptitle('MBA - SP StopWords')\nplt.bar(x,y, color='blue')","3f5c0ec4":"corpus=create_corpus(mba_general)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]\n\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y=zip(*top)\nplt.suptitle('MBA - GENERAL StopWords')\nplt.bar(x,y, color='green')","81613a19":"plt.figure(figsize=(16,5))\ncorpus=create_corpus(mba_sp)\n\ndic=defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True) \n\n        \nx,y=zip(*top)\nplt.suptitle('MBA - SP Punctuations')\nplt.bar(x,y, color = 'blue')","6c6b4ed3":"plt.figure(figsize=(16,5))\ncorpus=create_corpus(mba_general)\n\ndic=defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True) \n        \nx,y=zip(*top)\nplt.suptitle('MBA - GENERAL Punctuations')\nplt.bar(x,y, color='green')","a2a0691c":"plt.figure(figsize=(16,5))\ncounter=Counter(create_corpus(dataset))\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y,y=x)","af8498a5":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","8945af21":"plt.figure(figsize=(16,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(dataset['message'])[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","b1e26305":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","697316bd":"plt.figure(figsize=(16,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(dataset['message'])[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","8ea3dbf3":"#clean stopwords and stemming\n\nstopwords=stopwords.words('portuguese')\nremovewords = ['hahahaha', 'hahaha', 'gente', 'ofici', 'meno', 'quase', 'hahahahaha', 'grupo', 'pessoal', 'Algu\u00e9m']\nstopwords.append(removewords)\n\ncorpus = []\ntotal = len(dataset)\n\nfor i in range(total):  \n    description = re.sub('[^a-jl-zA-JL-Z\u00e1\u00e0\u00e2\u00e3\u00e9\u00e8\u00ea\u00ed\u00ef\u00f3\u00f4\u00f5\u00f6\u00fa\u00e7\u00f1\u00c1\u00c0\u00c2\u00c3\u00c9\u00c8\u00cd\u00cf\u00d3\u00d4\u00d5\u00d6\u00da\u00c7\u00d1]', ' ', dataset['message'][i])  \n    description = description.lower()\n    description = description.split()\n    \n    #ps = PorterStemmer()   \n    #description = [ps.stem(word) for word in description if not word in stopwords and len(word)>4]\n    \n    description = [word for word in description if not word in stopwords and len(word)>4]\n    corpus.extend(description)\n\nnp.array(corpus[:100])","a43cd28d":"class SimpleGroupedColorFunc(object):\n    \"\"\"Create a color function object which assigns EXACT colors\n       to certain words based on the color to words mapping\n\n       Parameters\n       ----------\n       color_to_words : dict(str -> list(str))\n         A dictionary that maps a color to the list of words.\n\n       default_color : str\n         Color that will be assigned to a word that's not a member\n         of any value from color_to_words.\n    \"\"\"\n\n    def __init__(self, color_to_words, default_color):\n        self.word_to_color = {word: color\n                              for (color, words) in color_to_words.items()\n                              for word in words}\n\n        self.default_color = default_color\n\n    def __call__(self, word, **kwargs):\n        return self.word_to_color.get(word, self.default_color)","ca4fe921":"class GroupedColorFunc(object):\n    \"\"\"Create a color function object which assigns DIFFERENT SHADES of\n       specified colors to certain words based on the color to words mapping.\n\n       Uses wordcloud.get_single_color_func\n\n       Parameters\n       ----------\n       color_to_words : dict(str -> list(str))\n         A dictionary that maps a color to the list of words.\n\n       default_color : str\n         Color that will be assigned to a word that's not a member\n         of any value from color_to_words.\n    \"\"\"\n\n    def __init__(self, color_to_words, default_color):\n        self.color_func_to_words = [\n            (get_single_color_func(color), set(words))\n            for (color, words) in color_to_words.items()]\n\n        self.default_color_func = get_single_color_func(default_color)\n\n    def get_color_func(self, word):\n        \"\"\"Returns a single_color_func associated with the word\"\"\"\n        try:\n            color_func = next(\n                color_func for (color_func, words) in self.color_func_to_words\n                if word in words)\n        except StopIteration:\n            color_func = self.default_color_func\n\n        return color_func\n\n    def __call__(self, word, **kwargs):\n        return self.get_color_func(word)(word, **kwargs)","9f162736":"# Since the text is small collocations are turned off and text is lower-cased\n\ntext = \" \".join(s for s in corpus)\n\nwc = WordCloud(collocations=False,\n                      background_color=\"black\",\n                      stopwords = removewords,\n                      width=1000,\n                      height=1000,\n                      max_words=150).generate(text.lower())\n\ncolor_to_words = {\n    # will be colored with a red single color function\n    '#ff0000': ['moletom']\n}\n\n# Words that are not in any of the color_to_words values\n# will be colored with a grey single color function\ndefault_color = 'white'\n\n# Create a color function with single tone\n# grouped_color_func = SimpleGroupedColorFunc(color_to_words, default_color)\n\n# Create a color function with multiple tones\ngrouped_color_func = GroupedColorFunc(color_to_words, default_color)\n\n# Apply our color function\nwc.recolor(color_func=grouped_color_func)\n\n# Plot\nfig, ax = plt.subplots(figsize=(20,20))\nax.imshow(wc, interpolation='bilinear')\nax.set_axis_off()\n \nplt.imshow(wc);","5ed91c6d":"* # N-gram analysis (N=2)\n","781b8bd0":"\nThis study aims to analyze data obtained through Telegram groups from the MBA course in DSA at USP ESALQ, comparing the group from s\u00e3o paulo and the general group and understanding the main issues discussed.\n\nIn the end we will understand if the sweatshirt is being a topic of common interest among the groups.\n\nSo let's Go!","6e2ee4fa":"Luiz de Queiroz College of Agriculture (Esalq\/USP) is located at the Luiz de Queiroz Campus in the city of Piracicaba, and is currently considered a Center of Excellence for Undergraduate and Graduate programs in Agricultural, Environmental, Biological and Applied Social Sciences, acknowledged for  its outstanding scientific and technical performance. Its academic community is comprised of 800 faculty and staff members along with nearly 3,400 undergraduate and graduate students. Its total area (3,825.4 hectares) corresponds to 50% of the total area of USP.\n\nESALQ offers 7 undergraduate programs and 15 graduate programs (one international), in addition to one inter-institutional and two inter-unit programs, in its 12 departments and more than 130 laboratories. It houses a reference library in Agricultural Sciences in Latin America, 4 experimental stations located in Anhembi, Anhumas, Itatinga and Piracicaba \u2013 \u201cAre\u00e3o\u201d Experimental Station, as w ell as an enterprise incubator, ESALQTec.\n\nESALQ has graduated 15,000 students. It is the first Brazilian higher education institution to graduate more than 11,000 Agricultural Engineering. ESALQ is a part of the international scene due to agreements with foreign institutions, exchanging students and faculty members, and offering double degree programs in Agriculture and in Food Science with French institutions.\n\nPicture by Gerhard Waller (USP\/ESALQ - DvComun)\n\n[ESALQ](www.en.esalq.usp.br\/)","c221b3cc":"# 2. Download Datasets ","efa96f7d":"# 3. Transform Data ","47b718de":"# Dedicated to Agricultural, Environmental, Biological and Applied Social Sciences for more than 100 years","6819a1f5":"Natural Language Processing, usually shortened as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language.\nThe ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable.\nMost NLP techniques rely on machine learning to derive meaning from human languages.\n\nIn fact, a typical interaction between humans and machines using Natural Language Processing could go as follows:\n\n1. A human talks to the machine\n2. The machine captures the audio\n3. Audio to text conversion takes place\n4. Processing of the text\u2019s data\n5. Data to audio conversion takes place\n6. The machine responds to the human by playing the audio file\n\n[Natural Language Processing](https:\/\/becominghuman.ai\/a-simple-introduction-to-natural-language-processing-ea66a1747b32)","3100d449":"**By. [Jo\u00e3o Campista](https:\/\/www.linkedin.com\/in\/joaocampista\/)**","f141e347":"# 5. Data Cleaning","eaeebdd3":"# Natural Language","df86262e":"# 4. Exploratory Data Analysis (EDA)","3d9d685d":"* # Punctuations Analysis","0b31ed1c":"![Esalq](https:\/\/www.esalq.usp.br\/sites\/default\/files\/em-foco\/banner-edificio-central.jpg)","6dbcc5a8":"* # Common words","4d5506f6":"* # Average word length in a message","3beb69ca":"Natural language refers to the way we, humans, communicate with each other.\n\nNamely, speech and text.\n\nWe are surrounded by text.\n\nThink about how much text you see each day:\n\n* Signs\n* Menus\n* Email\n* SMS\n* Web Pages\n* and so much more\u2026\n* The list is endless.\n\nNow think about speech.\n\nWe may speak to each other, as a species, more than we write. It may even be easier to learn to speak than to write.\n\nVoice and text are how we communicate with each other.\n\nGiven the importance of this type of data, we must have methods to understand and reason about natural language, just like we do for other types of data.\n\n[Natural Language](https:\/\/machinelearningmastery.com\/natural-language-processing\/)","3b985f98":"# Natural Language Processing","b6f7576a":"As a conclusion of this study, we can see that the group is anxious for the new sweatshirt.","6c09c400":"* #  Total messages","4c4de661":"* # Number of characters in messages","922bcbe6":"* # Total words per message","43184454":"# 6. Word Cloud & Results","5d4d1e3e":"# Motivations","d3c9ed03":"# 1. Import libraries","7cae9c4d":"* # StopWords Analysis","6dad2d57":"# 7. Conclusion","da8e2ae3":"* # N-gram analysis (N=3)"}}