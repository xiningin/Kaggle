{"cell_type":{"340484ef":"code","896dd985":"code","53879e7e":"code","8d4b822b":"code","aae48b98":"code","ddddb9a2":"code","4e915b6b":"code","1b8d5159":"code","c52b0505":"code","b0e77538":"code","f96765e3":"code","65a09fed":"code","9f155d63":"code","481c159d":"code","e2b3c073":"code","99fb689c":"code","2d7da4e6":"code","017e6d33":"code","15da6196":"code","7f4f0e0d":"code","3ed9a3e8":"code","35b7ee94":"code","fdeabb92":"code","34f7e618":"code","8e3f9d3a":"code","f48d3295":"code","b4346d46":"code","5b96d8e2":"code","a2518631":"code","ef58f11a":"code","84358e7c":"code","5a707a95":"code","489fe1ec":"code","932165a9":"code","4b3dddd5":"markdown","efb4a907":"markdown","7fe01c00":"markdown","ee254bc9":"markdown","e791d6ef":"markdown","692b393e":"markdown","62545460":"markdown","5ff1c143":"markdown","28908266":"markdown","d2b6d24d":"markdown","a256208b":"markdown","9bde20fb":"markdown","45110d1b":"markdown","2e11bede":"markdown","48cf0226":"markdown","b413b82f":"markdown","4a5cd3a7":"markdown"},"source":{"340484ef":"!pip install -U catalyst transformers > \/dev\/null","896dd985":"# Python \nimport os\nimport warnings\nimport logging\nfrom typing import Mapping, List\nfrom pprint import pprint\n\n# Numpy and Pandas \nimport numpy as np\nimport pandas as pd\n\n# PyTorch \nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Transformers \nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\n\n# Catalyst\nfrom catalyst.dl import SupervisedRunner\nfrom catalyst.dl.callbacks import AccuracyCallback, F1ScoreCallback, OptimizerCallback\nfrom catalyst.dl.callbacks import CheckpointCallback, InferCallback\nfrom catalyst.utils import set_global_seed, prepare_cudnn","53879e7e":"MODEL_NAME = 'distilbert-base-uncased' # pretrained model from Transformers\nLOG_DIR = \".\/logdir_amazon_reviews\"    # for training logs and tensorboard visualizations\nNUM_EPOCHS = 3                         # smth around 2-6 epochs is typically fine when finetuning transformers\nBATCH_SIZE = 72                        # depends on your available GPU memory (in combination with max seq length)\nMAX_SEQ_LENGTH = 256                   # depends on your available GPU memory (in combination with batch size)\nLEARN_RATE = 5e-5                      # learning rate is typically ~1e-5 for transformers\nACCUM_STEPS = 4                        # one optimization step for that many backward passes\nSEED = 17                              # random seed for reproducibility","8d4b822b":"FP16_PARAMS = None","aae48b98":"%%capture\n# if Your machine doesn't support FP16, comment these 4 lines below\n!git clone https:\/\/github.com\/NVIDIA\/apex \n!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .\/apex \n!rm -rf .\/apex\nFP16_PARAMS = dict(opt_level=\"O1\") ","ddddb9a2":"# to reproduce, download the data and customize this path\nPATH_TO_DATA = '..\/input\/amazon-pet-product-reviews-classification\/'","4e915b6b":"train_df = pd.read_csv(PATH_TO_DATA + 'train.csv', index_col='id').fillna('')\nvalid_df = pd.read_csv(PATH_TO_DATA + 'valid.csv', index_col='id').fillna('')\ntest_df = pd.read_csv(PATH_TO_DATA + 'test.csv', index_col='id').fillna('')","1b8d5159":"train_df.head()","c52b0505":"# target distribution\ntrain_df['label'].value_counts(normalize=True)","b0e77538":"# statistics of text length (in words)\ntrain_df['text'].apply(lambda s: len(s.split())).describe()","f96765e3":"class TextClassificationDataset(Dataset):\n    \"\"\"\n    Wrapper around Torch Dataset to perform text classification\n    \"\"\"\n    def __init__(self,\n                 texts: List[str],\n                 labels: List[str] = None,\n                 label_dict: Mapping[str, int] = None,\n                 max_seq_length: int = 512,\n                 model_name: str = 'distilbert-base-uncased'):\n        \"\"\"\n        Args:\n            texts (List[str]): a list with texts to classify or to train the\n                classifier on\n            labels List[str]: a list with classification labels (optional)\n            label_dict (dict): a dictionary mapping class names to class ids,\n                to be passed to the validation data (optional)\n            max_seq_length (int): maximal sequence length in tokens,\n                texts will be stripped to this length\n            model_name (str): transformer model name, needed to perform\n                appropriate tokenization\n\n        \"\"\"\n\n        self.texts = texts\n        self.labels = labels\n        self.label_dict = label_dict\n        self.max_seq_length = max_seq_length\n\n        if self.label_dict is None and labels is not None:\n            # {'class1': 0, 'class2': 1, 'class3': 2, ...}\n            # using this instead of `sklearn.preprocessing.LabelEncoder`\n            # no easily handle unknown target values\n            self.label_dict = dict(zip(sorted(set(labels)),\n                                       range(len(set(labels)))))\n\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        # suppresses tokenizer warnings\n        logging.getLogger(\n            \"transformers.tokenization_utils\").setLevel(logging.FATAL)\n\n        # special tokens for transformers\n        # in the simplest case a [CLS] token is added in the beginning\n        # and [SEP] token is added in the end of a piece of text\n        # [CLS] <indexes text tokens> [SEP] .. <[PAD]>\n        self.sep_vid = self.tokenizer.vocab[\"[SEP]\"]\n        self.cls_vid = self.tokenizer.vocab[\"[CLS]\"]\n        self.pad_vid = self.tokenizer.vocab[\"[PAD]\"]\n\n    def __len__(self):\n        \"\"\"\n        Returns:\n            int: length of the dataset\n        \"\"\"\n        return len(self.texts)\n\n    def __getitem__(self, index) -> Mapping[str, torch.Tensor]:\n        \"\"\"Gets element of the dataset\n\n        Args:\n            index (int): index of the element in the dataset\n        Returns:\n            Single element by index\n        \"\"\"\n\n        # encoding the text\n        x = self.texts[index]\n        x_encoded = self.tokenizer.encode(\n            x,\n            add_special_tokens=True,\n            max_length=self.max_seq_length,\n            return_tensors=\"pt\",\n        ).squeeze(0)\n\n        # padding short texts\n        true_seq_length = x_encoded.size(0)\n        pad_size = self.max_seq_length - true_seq_length\n        pad_ids = torch.Tensor([self.pad_vid] * pad_size).long()\n        x_tensor = torch.cat((x_encoded, pad_ids))\n\n        # dealing with attention masks - there's a 1 for each input token and\n        # if the sequence is shorter that `max_seq_length` then the rest is\n        # padded with zeroes. Attention mask will be passed to the model in\n        # order to compute attention scores only with input data\n        # ignoring padding\n        mask = torch.ones_like(x_encoded, dtype=torch.int8)\n        mask_pad = torch.zeros_like(pad_ids, dtype=torch.int8)\n        mask = torch.cat((mask, mask_pad))\n\n        output_dict = {\n            \"features\": x_tensor,\n            'attention_mask': mask\n        }\n\n        # encoding target\n        if self.labels is not None:\n            y = self.labels[index]\n            y_encoded = torch.Tensor(\n                [self.label_dict.get(y, -1)]\n            ).long().squeeze(0)\n            output_dict[\"targets\"] = y_encoded\n\n        return output_dict","65a09fed":"train_dataset = TextClassificationDataset(\n    texts=train_df['text'].values.tolist(),\n    labels=train_df['label'].values.tolist(),\n    label_dict=None,\n    max_seq_length=MAX_SEQ_LENGTH,\n    model_name=MODEL_NAME\n)\n\nvalid_dataset = TextClassificationDataset(\n    texts=valid_df['text'].values.tolist(),\n    labels=valid_df['label'].values.tolist(),\n    label_dict=train_dataset.label_dict,\n    max_seq_length=MAX_SEQ_LENGTH,\n    model_name=MODEL_NAME\n)\n\ntest_dataset = TextClassificationDataset(\n    texts=test_df['text'].values.tolist(),\n    labels=None,\n    label_dict=None,\n    max_seq_length=MAX_SEQ_LENGTH,\n    model_name=MODEL_NAME\n)","9f155d63":"NUM_CLASSES = len(train_dataset.label_dict)","481c159d":"train_df.loc[1]","e2b3c073":"pprint(train_dataset[1])","99fb689c":"train_val_loaders = {\n    \"train\": DataLoader(dataset=train_dataset,\n                        batch_size=BATCH_SIZE, \n                        shuffle=True),\n    \"valid\": DataLoader(dataset=valid_dataset,\n                        batch_size=BATCH_SIZE, \n                        shuffle=False)    \n}","2d7da4e6":"class DistilBertForSequenceClassification(nn.Module):\n    \"\"\"\n    Simplified version of the same class by HuggingFace.\n    See transformers\/modeling_distilbert.py in the transformers repository.\n    \"\"\"\n\n    def __init__(self, pretrained_model_name: str, num_classes: int = None):\n        \"\"\"\n        Args:\n            pretrained_model_name (str): HuggingFace model name.\n                See transformers\/modeling_auto.py\n            num_classes (int): the number of class labels\n                in the classification task\n        \"\"\"\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(\n            pretrained_model_name, num_labels=num_classes)\n\n        self.distilbert = AutoModel.from_pretrained(pretrained_model_name,\n                                                    config=config)\n        self.pre_classifier = nn.Linear(config.dim, config.dim)\n        self.classifier = nn.Linear(config.dim, num_classes)\n        self.dropout = nn.Dropout(config.seq_classif_dropout)\n\n    def forward(self, features, attention_mask=None, head_mask=None):\n        \"\"\"Compute class probabilities for the input sequence.\n\n        Args:\n            features (torch.Tensor): ids of each token,\n                size ([bs, seq_length]\n            attention_mask (torch.Tensor): binary tensor, used to select\n                tokens which are used to compute attention scores\n                in the self-attention heads, size [bs, seq_length]\n            head_mask (torch.Tensor): 1.0 in head_mask indicates that\n                we keep the head, size: [num_heads]\n                or [num_hidden_layers x num_heads]\n        Returns:\n            PyTorch Tensor with predicted class probabilities\n        \"\"\"\n        assert attention_mask is not None, \"attention mask is none\"\n        distilbert_output = self.distilbert(input_ids=features,\n                                            attention_mask=attention_mask,\n                                            head_mask=head_mask)\n        # we only need the hidden state here and don't need\n        # transformer output, so index 0\n        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n        # we take embeddings from the [CLS] token, so again index 0\n        pooled_output = hidden_state[:, 0]  # (bs, dim)\n        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n        logits = self.classifier(pooled_output)  # (bs, dim)\n\n        return logits","017e6d33":"model = DistilBertForSequenceClassification(pretrained_model_name=MODEL_NAME,\n                                            num_classes=NUM_CLASSES)","15da6196":"criterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARN_RATE)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)","7f4f0e0d":"os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"    # can be changed in case of multiple GPUs onboard\nset_global_seed(SEED)                       # reproducibility\nprepare_cudnn(deterministic=True)           # reproducibility","3ed9a3e8":"%%time\n# here we specify that we pass masks to the runner. So model's forward method will be called with\n# these arguments passed to it. \nrunner = SupervisedRunner(\n    input_key=(\n        \"features\",\n        \"attention_mask\"\n    )\n)\n\n\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=train_val_loaders,\n    callbacks=[\n        AccuracyCallback(num_classes=NUM_CLASSES),\n#         F1ScoreCallback(activation='Softmax'), # throws a tensor shape mismatch error\n        OptimizerCallback(accumulation_steps=ACCUM_STEPS)\n    ],\n    fp16=FP16_PARAMS,\n    logdir=LOG_DIR,\n    num_epochs=NUM_EPOCHS,\n    verbose=True\n)","35b7ee94":"!nvidia-smi","fdeabb92":"torch.cuda.empty_cache()","34f7e618":"!nvidia-smi","8e3f9d3a":"from catalyst.dl.utils import plot_metrics","f48d3295":"# # doesn't work :(\n# plot_metrics(\n#     logdir=LOG_DIR,\n#     step='batch',\n#     metrics=['accuracy01']\n# )","b4346d46":"test_loaders = {\n    \"test\": DataLoader(dataset=test_dataset,\n                        batch_size=BATCH_SIZE, \n                        shuffle=False) \n}","5b96d8e2":"runner.infer(\n    model=model,\n    loaders=test_loaders,\n    callbacks=[\n        CheckpointCallback(\n            resume=f\"{LOG_DIR}\/checkpoints\/best.pth\"\n        ),\n        InferCallback(),\n    ],   \n    verbose=True\n)","a2518631":"predicted_probs = runner.callbacks[0].predictions['logits']","ef58f11a":"sample_sub_df = pd.read_csv(PATH_TO_DATA + 'sample_submission.csv',\n                           index_col='id')","84358e7c":"train_dataset.label_dict","5a707a95":"sample_sub_df['label'] = predicted_probs.argmax(axis=1)\nsample_sub_df['label'] = sample_sub_df['label'].map({v:k for k, v in train_dataset.label_dict.items()})","489fe1ec":"sample_sub_df.head()","932165a9":"sample_sub_df.to_csv('distillbert_submission.csv')","4b3dddd5":"# The model\n\nIt's going to be a slightly simplified version of [`DistilBertForSequenceClassification`](https:\/\/github.com\/huggingface\/transformers\/blob\/master\/transformers\/modeling_distilbert.py#L547) by HuggingFace.\nWe need only predicted probabilities as output, nothing more - we don't need neither loss to be output nor hidden states or attentions (as in the original implementation).\n\nA good overview of DistilBERT is done in [this great post](https:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/) by Jay Alammar.","efb4a907":"## Torch Dataset\n\nThis is left for user to be defined. Catalyst will take care of the rest. ","7fe01c00":"**Finally, we define standard PyTorch loaders. This dictionary will be fed to Catalyst.**","ee254bc9":"Let's create a Torch loader for the test set and launch `infer` to actually make predictions fot the test set. First, we load the best model checkpoint, then make inference with this model.","e791d6ef":"**Create Torch Datasets with train, validation, and test data.**","692b393e":"We need to predict original class names (strings), so we are using inverted class name dictionary to map indices of classes with highest predicted probability to actual class names. ","62545460":"# Plot metrics\n\n<img src=\"https:\/\/habrastorage.org\/webt\/ki\/ib\/hy\/kiibhyp373r65zriwruroiqitky.jpeg\" width=30% \/>\n\nThere are at least 4 ways to monitor training:\n\n### 1. Good old tqdm\nThere above it's set with a flag `verbose` in `runner.train`. Actually, it's not that bad :)\n\n<img src='https:\/\/habrastorage.org\/webt\/ta\/1s\/98\/ta1s988ghabz412weaq0lgs_cke.png'> \n\n\n### 2. Weights & Biases\n\nBefore launching training, you can run [Weighs & Biases](https:\/\/app.wandb.ai\/) inititialization for this project. Execute `wandb init` in a separate terminal window (from the same directory where this notebook is running). `wandb` will ask your API key from https:\/\/app.wandb.ai\/authorize and project name. The rest will be picked up by Catalyst's `SupervisedWandbRunner` (so you'll need to import this instead of `SupervisedRunner`). \nFollowing the links printed above (smth. like  https:\/\/app.wandb.ai\/yorko\/catalyst-nlp-bert) we can keep track of loss and metrics.\n\n### 3. Tensorboard\nDuring training, logs are written to `LOG_DIR` specified above. \nSimiltaneously with training, you can run `tensorboard --logdir $LOG_DIR` (in another terminal tab, in case of training on a server, I also had to add a `--bin_all` flag),\nand you'll get a nice dashboard. Here we see how accuracy and loss change during training.\n\n<img src=\"https:\/\/habrastorage.org\/webt\/2a\/sx\/mo\/2asxmoizgcpf2fnhjjkfhvf70aw.png\" width=50% \/>\n\n### 4. Offline metric plotting\n\nIf your training is pretty fast and\/or you're not interested in tracking training progress, you can just plot losses and metrics once the training is done. Looks like it won't work in Kernels though but try it locally.","5ff1c143":"Now that we have predicted probabilities, let's finally create a submission file.","28908266":"**Setup**","d2b6d24d":"# Inference for the test set","a256208b":"**Additionaly, we install [Nvidia Apex](https:\/\/github.com\/NVIDIA\/apex) to reuse AMP - automatic mixed-precision training.**\n\nThe idea is that we can use float16 format for faster training, only switching tio float32 when necessary. \nHere we'll only need to tell Catalyst to use fp16.","9bde20fb":"One of the training dataset instances:","45110d1b":"To run Deep Learning experiments, Catalyst resorts to the [`Runner`](https:\/\/catalyst-team.github.io\/catalyst\/api\/dl.html#catalyst.dl.core.runner.Runner) abstraction, in particular, to [`SupervisedRunner`](https:\/\/catalyst-team.github.io\/catalyst\/api\/dl.html#module-catalyst.dl.runner.supervised).\n\n`SupervisedRunner` implements the following methods:\n - `train` - starts the training process of the model\n - `predict_loader` - makes a prediction on the whole loader with the specified model\n - `infer` - makes the inference on the model\n \nTo train the model within this interface you pass the following to the `train` method:\n - model (`torch.nn.Module`) \u2013 PyTorch model to train\n - criterion (`nn.Module`) \u2013 PyTorch criterion function for training\n - optimizer (`optim.Optimizer`) \u2013 PyTorch optimizer for training\n - loaders (dict) \u2013 dictionary containing one or several `torch.utils.data.DataLoader` for training and validation\n - logdir (str) \u2013 path to output directory. There Catalyst will write logs, will dump the best model and the actual code to train the model\n - callbacks \u2013 list of Catalyst callbacks\n - scheduler (`optim.lr_scheduler._LRScheduler`) \u2013 PyTorch scheduler for training\n - ...\n \nIn our case we'll pass the created `DistilBertForSequenceClassification` model, cross-entropy criterion, Adam optimizer, scheduler and data loaders that we created earlier. Also, we'll be tracking accuracy and thus will need `AccuracyCallback`. To perform batch accumulation, we'll be using `OptimizationCallback`.\n\nThere are many more useful [callbacks](https:\/\/catalyst-team.github.io\/catalyst\/api\/dl.html#module-catalyst.dl.callbacks.checkpoint) implemented, also check out [Catalyst examples](https:\/\/github.com\/catalyst-team\/catalyst\/tree\/master\/examples\/notebooks).","2e11bede":"We infer the number of classes from the training set.","48cf0226":"# <center> Text classification with DistillBert and Catalyst\n    \n<img src='https:\/\/habrastorage.org\/webt\/ne\/n_\/ow\/nen_ow49hxu8zrkgolq1rv3xkhi.png'>\n    \nHere we classify Amazon product reviews with the `DistillBert` model from [transformers](https:\/\/github.com\/huggingface\/transformers). We use [catalyst](https:\/\/github.com\/catalyst-team\/catalyst) to run the experiment. We reuse many powerful techniques for faster model training, almost all of them are handled by Catalyst:\n\n1. **Gradient accumulation.** Doing one optimization step for several bachward steps. Well explained in [this post](https:\/\/medium.com\/huggingface\/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255) by HuggingFace\n1. **Mixed-precision training.** Handled by [Nvidia Apex](https:\/\/github.com\/NVIDIA\/apex) and reused by Catalyst\n1. **Learning rate schedule.** Standard thing when training deep neural networks, Catalysts handles lot of them","b413b82f":"## Model training\n\nFirst we specify optimizer and scheduler (pure PyTorch). Then Catalyst stuff.","4a5cd3a7":"**Dataset**\n\nAmazon product reviews - [competition](https:\/\/www.kaggle.com\/c\/amazon-pet-product-reviews-classification).\nGiven text of a review, we need to classify it into one of 6 categories: dogs, cats, fish aquatic pets, birds, and two others."}}