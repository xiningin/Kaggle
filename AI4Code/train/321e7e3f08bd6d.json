{"cell_type":{"04c40ff7":"code","b85c679e":"code","9dfb175c":"code","a79ca494":"code","c6329557":"code","51b7eb8c":"code","6a3a0ec9":"code","6dde5321":"code","15c98cbc":"code","035f26fc":"code","280dafc2":"code","0bb8c034":"code","d9bd33da":"code","44c46515":"code","35854d2f":"code","cd530f36":"code","f7cb9d06":"code","84eb9a88":"code","586f4441":"code","79d2ca2e":"code","40869435":"code","8569bea3":"code","edc0f4ff":"code","b6a413e7":"code","c0f94983":"code","d6ca156c":"code","a43d9025":"code","216fca66":"code","cc80a7e1":"code","f4f5cd2c":"code","c1b43733":"code","df6eaa3a":"code","22628762":"code","8e22e683":"code","f7084270":"code","88c234a0":"code","d56f95fe":"code","be93e42b":"code","e55ce323":"code","852c0b41":"code","6ec11b91":"code","122a79af":"markdown","6cfabb81":"markdown","83be5dd3":"markdown","7c1de802":"markdown","1235f8a6":"markdown","5ad3f9b4":"markdown","247d8dc0":"markdown","a34896d1":"markdown","241f1d43":"markdown","4b9984fa":"markdown","7e881011":"markdown","d7fa9455":"markdown","d739d921":"markdown","93ac8dd7":"markdown","015ae74b":"markdown","02768ca5":"markdown","90281b74":"markdown","11cea00e":"markdown","68a3e6f0":"markdown"},"source":{"04c40ff7":"%config Completer.use_jedi = False","b85c679e":"sns.set(rc={'figure.figsize':(18,10)})\nsns.set_style({'axes.facecolor':'white', 'grid.color': '.8', 'font.family':'Times New Roman'})","9dfb175c":"# Colors\n\ncyan = '#00FFD1'\nred = '#FF007D'\nprussian = '#0075FF'\ngreen = '#EEF622'\nyellow = '#FFF338'\nviolet = '#9B65FF'\norange = '#FFA500'\nblue = '#00EBFF'\nvermillion = '#FF6900'\nred2 = '#FF2626'\nseagreen = '#28FFBF'\ngreen2 = '#FAFF00'\nnavyblue = '#04009A'\ndarkgreen = '#206A5D'\nlightgreen = '#CCF6C8'\npink = '#F35588'\nmauve = '#BAABDA'\nlightblue = '#1CC5DC'\nmustard = '#FDB827'\ndeeppurple = '#723881'\n\ncolor_list = [cyan,red,prussian,green,violet,orange,yellow,blue,vermillion,red2,seagreen,green2,navyblue,darkgreen,lightgreen,pink,mauve,lightblue,mustard,deeppurple]\nplt.rcParams['axes.prop_cycle'] = plt.cycler(color=color_list)","a79ca494":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport random","c6329557":"df = pd.read_csv('..\/input\/vehicle-dataset-from-cardekho\/car data.csv')","51b7eb8c":"df.head()","6a3a0ec9":"df.shape","6dde5321":"print(df['Fuel_Type'].unique())\nprint(df['Seller_Type'].unique())\nprint(df['Transmission'].unique())\nprint(df['Owner'].unique())\nprint(df['Year'].unique())","15c98cbc":"df.isnull().sum() ","035f26fc":"df.describe()","280dafc2":"df.columns","0bb8c034":"# dropping car names\nfinal_dataset = df[['Year', 'Selling_Price', 'Present_Price', 'Kms_Driven',\n       'Fuel_Type', 'Seller_Type', 'Transmission', 'Owner']] ","d9bd33da":"final_dataset.head()","44c46515":"final_dataset['Current_yr'] = 2020\nfinal_dataset['No_yr'] = final_dataset['Current_yr']-final_dataset['Year']","35854d2f":"final_dataset.head()","cd530f36":"# dropping Year and Current year\nfinal_dataset.drop(['Year','Current_yr'],axis=1,inplace=True)","f7cb9d06":"final_dataset.head()","84eb9a88":"final_dataset = pd.get_dummies(final_dataset,drop_first=True) ","586f4441":"final_dataset.head()","79d2ca2e":"sns.pairplot(final_dataset,palette=random.choice(color_list));","40869435":"fig, ax = plt.subplots(figsize=(18,14)) \ncorrmat= final_dataset.corr()\ntop_corr_features=corrmat.index #will take top corelation features\n\nmask = np.triu(final_dataset[top_corr_features].corr())\nsns.heatmap(final_dataset[top_corr_features].corr(),cmap='BrBG',linewidths=1.5,ax=ax,annot=True,center=0,square=True,mask=mask);","8569bea3":"final_dataset.head()","edc0f4ff":"X = final_dataset.iloc[:,1:] # independant features \ny = final_dataset.iloc[:,0] # dependant feature","b6a413e7":"X.head()","c0f94983":"y.head()","d6ca156c":"# finding out ordering of important features\nfrom sklearn.ensemble import ExtraTreesRegressor\nmodel = ExtraTreesRegressor()\nmodel.fit(X,y)","a43d9025":"print(model.feature_importances_)","216fca66":"feat_importances = pd.Series(model.feature_importances_, index = X.columns)\nfeat_importances.nlargest(5).plot(kind = 'barh',color=color_list) # top 5\nplt.title('Top 5',fontsize=30)\nplt.show()","cc80a7e1":"from sklearn.model_selection import train_test_split\ntrain_X, test_X, train_y, test_y = train_test_split(X,y,test_size = 0.2)","f4f5cd2c":"print(train_X.shape)\nprint(test_X.shape)\nprint(train_y.shape)\nprint(test_y.shape)","c1b43733":"from sklearn.ensemble import RandomForestRegressor\nrf_random = RandomForestRegressor() ","df6eaa3a":"# Hyperparameters\nimport numpy as np\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n\n# Number of features to consider at every step\nmax_features = ['auto','sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5,30,num = 6)]\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2,5,10,15,100]\n\n# Miimum number of samples required at each leaf node\nmin_samples_leaf = [1,2,5,10]\n\n# Crosshecking\nprint(n_estimators)\nprint(max_depth)","22628762":"# helps us to find out the best parameters considering how many estimators should be \n# there how many max features and depth should be there\nfrom sklearn.model_selection import RandomizedSearchCV ","8e22e683":"#remember to take in the form of key value pairs\nrandom_grid = {'n_estimators': n_estimators,\n              'max_features': max_features,\n              'max_depth': max_depth,\n              'min_samples_split': min_samples_split,\n              'min_samples_leaf': min_samples_leaf}\nprint(random_grid)","f7084270":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\n\n# Initializing a random forest regressor\nrf = RandomForestRegressor() #this we have passed as an argument below","88c234a0":"rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, scoring='neg_mean_squared_error', n_iter=10, cv=5, verbose=2, random_state=42, n_jobs=1)","d56f95fe":"rf_random.fit(train_X, train_y)","be93e42b":"predictions = rf_random.predict(test_X)","e55ce323":"predictions","852c0b41":"sns.distplot(test_y-predictions,color=random.choice(color_list));","6ec11b91":"plt.scatter(test_y,predictions,color=random.choice(color_list));","122a79af":"Explanation:\n\nFuel_type: there were 3 categories ['Petrol' 'Diesel' 'CNG'] : CNG column has been dropped to prevent dummy variable trap","6cfabb81":"This notebook was done on following this video by Krish Naik as a tutorial - https:\/\/youtu.be\/p_tpQSY1aTs\n","83be5dd3":"Here since we get this close gaussian distribution graph that means that the distance is very minimal","7c1de802":"We can use liner regression lasso ... but here for him RF gave better results\n\nWhen using random forest we dont hve to scaale the values since random forest uses decision tree and in that its usually not required","1235f8a6":"#### Categorical and Numerical","5ad3f9b4":"Verbose is for displaying everything below","247d8dc0":"These are the various parameters that we've taken","a34896d1":"#### Corelation","241f1d43":"Since the graph looks like normal distribution it shows us that the model that we created is giving us very good results","4b9984fa":"Here alse the scatter plot shows us that the values of y test and prediction are almost in a line itself, therefore this model is good","7e881011":"These are all the different decision trees that we have selected for hyperparameters","d7fa9455":"Hence the first(present price) then the fifth(fuel type diesel)\nthe n seventh.... have the most importnce","d739d921":"# Random Forest","93ac8dd7":"### One hot encoding","015ae74b":"* Categorical - ['Fuel_Type', 'Seller_Type', 'Transmission', 'Owner'] \n* Numerical - ['Selling_Price', 'Present_Price', 'Kms_Driven' ]","02768ca5":"To compare predictions we'll use : (below)","90281b74":"#### Checking missing or null values","11cea00e":"Here the selling price price will be a dependant feature and the rest will be independant features","68a3e6f0":"## Prediction"}}