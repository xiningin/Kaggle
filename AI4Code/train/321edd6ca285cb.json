{"cell_type":{"c141d696":"code","443be3d7":"code","2b22788f":"code","7ba7bee6":"code","1d571a4f":"code","e4c77a9d":"code","a7b4d606":"code","05a8cfdc":"code","92215feb":"code","15a84342":"code","8b49d8bb":"code","8c868eee":"code","619306b6":"code","52c56f47":"code","fcb525c3":"code","19142b59":"markdown","bd6c3fe8":"markdown","20e1af22":"markdown","474a647e":"markdown","0a2c1cec":"markdown","55d65557":"markdown","12c8e869":"markdown"},"source":{"c141d696":"import copy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport time\n\nimport torch\nimport torch.optim as optim\nfrom torch.quantization import convert\nfrom torch import nn\n\nimport torchvision\nimport torchvision.models.quantization as models\nfrom torchvision import transforms, datasets\n\nimport warnings\nwarnings.filterwarnings('ignore')","443be3d7":"data_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize(224),\n        transforms.RandomCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(224),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}","2b22788f":"data_dir = '\/kaggle\/input\/hymenoptera-data\/hymenoptera_data'\n\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\n\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n                                              shuffle=True, num_workers=8)\n              for x in ['train', 'val']}\n\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","7ba7bee6":"def imshow(inp, title=None, ax=None, figsize=(5, 5)):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    if ax is None:\n        fig, ax = plt.subplots(1, figsize=figsize)\n    ax.imshow(inp)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    if title is not None:\n        ax.set_title(title)","1d571a4f":"# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs, nrow=4)\n\nfig, ax = plt.subplots(1, figsize=(10, 10))\nimshow(out, title=[class_names[x] for x in classes], ax=ax)","e4c77a9d":"def train_model(model, criterion, optimizer, scheduler, num_epochs=25, device='cpu'):\n    \"\"\"\n    Support function for model training.\n\n    Args:\n    model: Model to be trained\n    criterion: Optimization criterion (loss)\n    optimizer: Optimizer to use for training\n    scheduler: Instance of ``torch.optim.lr_scheduler``\n    num_epochs: Number of epochs\n    device: Device to run the training on. Must be 'cpu' or 'cuda'\n    \"\"\"\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    # Each epoch has a training and validation phase\n    for phase in ['train', 'val']:\n        if phase == 'train':\n            model.train()  # Set model to training mode\n        else:\n            model.eval()   # Set model to evaluate mode\n\n        running_loss = 0.0\n        running_corrects = 0\n\n        # Iterate over data.\n        for inputs, labels in dataloaders[phase]:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward\n            # track history if only in train\n            with torch.set_grad_enabled(phase == 'train'):\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                loss = criterion(outputs, labels)\n\n                # backward + optimize only if in training phase\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n\n        # statistics\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        \n    if phase == 'train':\n        scheduler.step()\n\n    epoch_loss = running_loss \/ dataset_sizes[phase]\n    epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n\n    print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n        phase, epoch_loss, epoch_acc))\n\n    # deep copy the model\n    if phase == 'val' and epoch_acc > best_acc:\n        best_acc = epoch_acc\n        best_model_wts = copy.deepcopy(model.state_dict())\n\n    print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","a7b4d606":"def visualize_model(model, rows=3, cols=3):\n    was_training = model.training\n    model.eval()\n    current_row = current_col = 0\n    fig, ax = plt.subplots(rows, cols, figsize=(cols*2, rows*2))\n\n    with torch.no_grad():\n        for idx, (imgs, lbls) in enumerate(dataloaders['val']):\n            imgs = imgs.cpu()\n            lbls = lbls.cpu()\n\n            outputs = model(imgs)\n            _, preds = torch.max(outputs, 1)\n\n            for jdx in range(imgs.size()[0]):\n                imshow(imgs.data[jdx], ax=ax[current_row, current_col])\n                ax[current_row, current_col].axis('off')\n                ax[current_row, current_col].set_title('predicted: {}'.format(class_names[preds[jdx]]))\n\n                current_col += 1\n                if current_col >= cols:\n                    current_row += 1\n                    current_col = 0\n                if current_row >= rows:\n                    model.train(mode=was_training)\n                    return\n    model.train(mode=was_training)","05a8cfdc":"model_fe = models.resnet18(pretrained=True, progress=True, quantize=True)\nnum_ftrs = model_fe.fc.in_features","92215feb":"def create_combined_model(model_fe):\n    # Step 1. Isolate the feature extractor.\n    model_fe_features = nn.Sequential(\n        model_fe.quant,  # Quantize the input\n        model_fe.conv1,\n        model_fe.bn1,\n        model_fe.relu,\n        model_fe.maxpool,\n        model_fe.layer1,\n        model_fe.layer2,\n        model_fe.layer3,\n        model_fe.layer4,\n        model_fe.avgpool,\n        model_fe.dequant,  # Dequantize the output\n    )\n\n    # Step 2. Create a new \"head\"\n    new_head = nn.Sequential(\n        nn.Dropout(p=0.5),\n        nn.Linear(num_ftrs, 2),\n    )\n\n    # Step 3. Combine, and don't forget the quant stubs.\n    new_model = nn.Sequential(\n        model_fe_features,\n        nn.Flatten(1),\n        new_head,\n    )\n    \n    return new_model","15a84342":"new_model = create_combined_model(model_fe)\nnew_model = new_model.to('cpu')\n\ncriterion = nn.CrossEntropyLoss()\n\n# Note that we are only training the head.\noptimizer_ft = optim.SGD(new_model.parameters(), lr=0.01, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)","8b49d8bb":"new_model = train_model(new_model, criterion, optimizer_ft, exp_lr_scheduler,\n                        num_epochs=25, device='cpu')\n\nvisualize_model(new_model)\nplt.tight_layout()","8c868eee":"# notice `quantize=False`\nmodel = models.resnet18(pretrained=True, progress=True, quantize=False)\nnum_ftrs = model.fc.in_features\n\n# Step 1 - use models from torchvision\/models\/quantization, which have a member method fuse_model\nmodel.train()\nmodel.fuse_model()\n\n# Step 2 - create_combined_model function used in the previous section\nmodel_ft = create_combined_model(model)\n\n# Use default QAT configuration\nmodel_ft[0].qconfig = torch.quantization.default_qat_qconfig\n\n# Step 3 - torch.quantization.prepare_qat, which inserts fake-quantization modules\nmodel_ft = torch.quantization.prepare_qat(model_ft, inplace=True)","619306b6":"for param in model_ft.parameters():\n    param.requires_grad = True\n\n# We can fine-tune on GPU if available\nmodel_ft.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Note that we are training everything, so the learning rate is lower\n# Notice the smaller learning rate\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.1)\n\n# Decay LR by a factor of 0.3 every several epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.3)\n\nmodel_ft_tuned = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                              num_epochs=25, device=device)","52c56f47":"model_ft_tuned.cpu()\n\nmodel_quantized_and_trained = convert(model_ft_tuned, inplace=False)","fcb525c3":"visualize_model(model_quantized_and_trained)\n\nplt.ioff()\nplt.tight_layout()\nplt.show()","19142b59":"<h1 id=\"train\" style=\"color:orange; background:purple;\"> \n    <center>Training\n        <a class=\"anchor-link\" href=\"#train\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","bd6c3fe8":"<h1 id=\"show\" style=\"color:orange; background:purple;\"> \n    <center>Show Images\n        <a class=\"anchor-link\" href=\"#show\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","20e1af22":"<div>\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/842582\/1437912\/367d8ddf581a52984850ae2a50995c42\/dataset-cover.png\" \/>\n<\/div>","474a647e":"<h1 id=\"load\" style=\"color:orange; background:purple;\"> \n    <center>Data Loading\n        <a class=\"anchor-link\" href=\"#load\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","0a2c1cec":"<h1 id=\"finetuning\" style=\"color:orange; background:purple;\"> \n    <center>Finetuning the Quantizable Model\n        <a class=\"anchor-link\" href=\"#finetuning\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","55d65557":"<h1 id=\"augmentations\" style=\"color:orange; background:purple;\"> \n    <center>Augmentations\n        <a class=\"anchor-link\" href=\"#augmentations\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","12c8e869":"<h1 id=\"quantized\" style=\"color:orange; background:purple;\"> \n    <center>Quantized Feature Extraction\n        <a class=\"anchor-link\" href=\"#quantized\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>"}}