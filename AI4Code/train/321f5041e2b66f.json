{"cell_type":{"b4db4778":"code","b29c7be6":"code","8783eb2c":"code","eb9cf17c":"code","e9ba8098":"code","c1c22ec8":"code","617b79d5":"code","9f072608":"code","863b588f":"code","7dcd5d14":"code","39127ae4":"code","a8aa5dd6":"code","934d3158":"code","4fe0a5f6":"code","01e8e382":"code","9cbcba42":"code","c8dc74e9":"code","dbe49bb5":"code","80b85c1e":"code","34599947":"code","3e4a5d29":"code","a28705f0":"code","b56a3372":"code","7f4f83e4":"code","c70fe40c":"code","286dfe9f":"code","b17a3809":"code","a454c2fe":"code","a96978a1":"code","d79cd5d7":"code","0d10f15a":"code","fc70eb37":"code","08db5f15":"code","4d56f429":"code","01314994":"code","e0e97620":"code","465859e7":"code","f5411d29":"code","c173b109":"code","07999371":"code","d71793ae":"code","ef243665":"code","072e96cd":"code","63d8e804":"code","a168ae8f":"code","77f2fb8a":"code","9be1a594":"code","e7d90794":"code","135898cb":"code","27e91b52":"code","cc48eec2":"code","76dfdc47":"code","6715101e":"code","cd7d8f78":"code","d988bb40":"code","99a4e116":"code","bbab1145":"markdown","4fc7cd3e":"markdown","c7dbbea1":"markdown","1b63d7da":"markdown","010bfee9":"markdown","379f0908":"markdown","454a802d":"markdown","4191629f":"markdown","fca4db74":"markdown","8605a748":"markdown","ec2c89e0":"markdown","a78a58a1":"markdown","cd7e0f4d":"markdown","6be65370":"markdown","27a07ffd":"markdown","7f8cef46":"markdown","74b806d2":"markdown"},"source":{"b4db4778":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom scipy import stats\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf","b29c7be6":"df = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv')\ndf.head()\n","8783eb2c":"df.describe()","eb9cf17c":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error as MSE","e9ba8098":"df['date_time_2'] = df['date_time'].astype('datetime64[ns]').astype(np.int64)\/10**9\n#test['date_time'] = test['date_time'].astype('datetime64[ns]').astype(np.int64)\/10**9","c1c22ec8":"df['date_time'] = pd.to_datetime(df['date_time'])\ndf['year'] = df['date_time'].dt.year\ndf['month'] = df['date_time'].dt.month\ndf['week'] = df['date_time'].dt.week\ndf['day'] = df['date_time'].dt.day\ndf['dayofweek'] = df['date_time'].dt.dayofweek\ndf['time'] = df['date_time'].dt.date - df['date_time'].dt.date.min()\ndf['hour'] = df['date_time'].dt.hour\ndf['time'] = df['time'].apply(lambda x : x.days)\ndf.drop(columns = 'date_time', inplace = True)","617b79d5":"df.head(5)","9f072608":"x=df.drop(['target_benzene','target_nitrogen_oxides', 'target_carbon_monoxide'], axis=1)\ny_bz=df['target_benzene']\ny_no=df['target_nitrogen_oxides']\ny_co=df['target_carbon_monoxide']","863b588f":"from sklearn.model_selection import RandomizedSearchCV\n\n# # Number of trees in random forest\n# n_estimators = [int(x) for x in np.linspace(start = 300, stop = 5000, num = 10)]\n# # Number of features to consider at every split\n# max_features = ['auto', 'sqrt']\n# # Maximum number of levels in tree\n# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n# max_depth.append(None)\n# # Minimum number of samples required to split a node\n# min_samples_split = [2, 3, 6, 10, 12]\n# # Minimum number of samples required at each leaf node\n# min_samples_leaf = [1, 3, 2, 4, 10, 7]\n# # Method of selecting samples for training each tree\n# bootstrap = [True, False]\n# # Create the random grid\n# random_grid = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf,\n#                'bootstrap': bootstrap}\n\n\nrandom_grid = {'n_estimators': [2911],\n 'min_samples_split': [3],\n 'min_samples_leaf': [2],\n 'max_features': ['auto'],\n 'max_depth': [60],\n 'bootstrap': [True]}\n\n\nprint(random_grid)\n","7dcd5d14":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 500, cv = 10, verbose=2, random_state=42, n_jobs = -1, scoring = 'neg_mean_squared_error')\n# Fit the random search model\n","39127ae4":"import time\n\nt_start = time.time()\nrf_random.fit(x,y_bz)\nt_stop = time.time()\n\nprint('Time elapsed: {:.3f} seconds'.format(t_stop - t_start))","a8aa5dd6":"rf_random.best_score_\n\n# best neg mean squared error with no dates -1.728175972337029\n\n#{'n_estimators': 2000,\n# 'min_samples_split': 5,\n# 'min_samples_leaf': 2,\n# 'max_features': 'auto',\n# 'max_depth': 80,\n# 'bootstrap': True}\n\n#new benzene with dates- -1.55\n# {'n_estimators': 2911,\n#  'min_samples_split': 3,\n#  'min_samples_leaf': 2,\n#  'max_features': 'auto',\n#  'max_depth': 60,\n#  'bootstrap': True}\n","934d3158":"rf_random.best_params_","4fe0a5f6":"rf_random.best_estimator_.feature_importances_","01e8e382":"import pandas as pd\nimport matplotlib.pyplot as plt\n# Create a pd.Series of features importances\nimportances_rf = pd.Series(rf_random.best_estimator_.feature_importances_,\nindex = x.columns)\n# Sort importances_rf\nsorted_importances_rf = importances_rf.sort_values()\n# Make a horizontal bar plot\nsorted_importances_rf.plot(kind='barh', color='blue')\nplt.show()","9cbcba42":"final_rf_bz = rf_random.best_estimator_","c8dc74e9":"# # Number of trees in random forest\n# n_estimators = [int(x) for x in np.linspace(start = 500, stop = 4000, num = 6)]\n# # Number of features to consider at every split\n# max_features = ['auto', 'sqrt']\n# # Maximum number of levels in tree\n# max_depth = [int(x) for x in np.linspace(5, 110, num = 8)]\n# max_depth.append(None)\n# # Minimum number of samples required to split a node\n# min_samples_split = [2, 3, 6, 10, 12]\n# # Minimum number of samples required at each leaf node\n# min_samples_leaf = [1, 3, 2, 4, 10, 7]\n# # Method of selecting samples for training each tree\n# bootstrap = [True, False]\n# # Create the random grid\n# random_grid_co = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf,\n#                'bootstrap': bootstrap}\n# print(random_grid_co)\n\n\nrandom_grid_co = {'n_estimators': [500],\n 'min_samples_split': [6],\n 'min_samples_leaf': [1],\n 'max_features': ['sqrt'],\n 'max_depth': [50],\n 'bootstrap': [True]}\n\n\nprint(random_grid_co)\n\n\n\n","dbe49bb5":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf_co = RandomForestRegressor()\n\nrf_random_co = RandomizedSearchCV(estimator = rf_co, param_distributions = random_grid_co, n_iter = 400, cv = 5, verbose=2, random_state=42, n_jobs = -1,scoring = 'neg_mean_squared_error')\n# Fit the random search model\n","80b85c1e":"x_bz=x.join(y_bz.to_frame())\nx_bz","34599947":"import time\n\nt_start = time.time()\nrf_random_co.fit(x_bz,y_co)\nt_stop = time.time()\n\nprint('Time elapsed: {:.3f} seconds'.format(t_stop - t_start))","3e4a5d29":"rf_random_co.best_score_\n\n#neg mean squared error with Benzen and parameter tuning and no dates - -0.3161621132509786\n# {'n_estimators': 2000,\n#  'min_samples_split': 3,\n#  'min_samples_leaf': 2,\n#  'max_features': 'sqrt',\n#  'max_depth': 95,\n#  'bootstrap': True}\n\n#neg mean squared error with Benzen and parameter tuning and dates - -0.280873\n\n# {'n_estimators': 500,\n#  'min_samples_split': 6,\n#  'min_samples_leaf': 1,\n#  'max_features': 'sqrt',\n#  'max_depth': 50,\n#  'bootstrap': True}","a28705f0":"rf_random_co.best_params_","b56a3372":"rf_random_co.best_estimator_.feature_importances_","7f4f83e4":"import pandas as pd\nimport matplotlib.pyplot as plt\n# Create a pd.Series of features importances\nimportances_rf = pd.Series(rf_random_co.best_estimator_.feature_importances_,\nindex = x_bz.columns)\n# Sort importances_rf\nsorted_importances_rf = importances_rf.sort_values()\n# Make a horizontal bar plot\nsorted_importances_rf.plot(kind='barh', color='blue')\nplt.show()","c70fe40c":"final_rf_co = rf_random_co.best_estimator_","286dfe9f":"# from sklearn.model_selection import RandomizedSearchCV\n# # Number of trees in random forest\n# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 3500, num = 10)]\n# # Number of features to consider at every split\n# max_features = ['auto', 'sqrt']\n# # Maximum number of levels in tree\n# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n# max_depth.append(None)\n# # Minimum number of samples required to split a node\n# min_samples_split = [2, 5, 10]\n# # Minimum number of samples required at each leaf node\n# min_samples_leaf = [1, 2, 4]\n# # Method of selecting samples for training each tree\n# bootstrap = [True, False]\n# # Create the random grid\n# random_grid_no = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf,\n#                'bootstrap': bootstrap}\n# print(random_grid_no)\n\n\nrandom_grid_no = {'n_estimators': [200],\n 'min_samples_split': [2],\n 'min_samples_leaf': [1],\n 'max_features': ['auto'],\n 'max_depth': [80],\n 'bootstrap': [True]}\n\nprint(random_grid_no)","b17a3809":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf_no = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random_no = RandomizedSearchCV(estimator = rf_no, param_distributions = random_grid_no, n_iter = 400, cv = 5, verbose=2, random_state=42, n_jobs = -1,scoring = 'neg_mean_squared_error')\n# Fit the random search model\n","a454c2fe":"x_bz_co=x_bz.join(y_co.to_frame())\nx_bz_co","a96978a1":"import time\n\nt_start = time.time()\nrf_random_no.fit(x_bz_co,y_no)\nt_stop = time.time()\n\nprint('Time elapsed: {:.3f} seconds'.format(t_stop - t_start))","d79cd5d7":"rf_random_no.best_score_\n\n\n\n#neg mean squared error with bz\/co and parameter tuning and no dates - -8843.46\n# {'n_estimators': 3500,\n#  'min_samples_split': 2,\n#  'min_samples_leaf': 2,\n#  'max_features': 'auto',\n#  'max_depth': 70,\n#  'bootstrap': True}\n\n\n#neg mean squared error with bz\/co and parameter tuning and dates - -8079.27\n# {'n_estimators': 200,\n#  'min_samples_split': 2,\n#  'min_samples_leaf': 1,\n#  'max_features': 'auto',\n#  'max_depth': 80,\n#  'bootstrap': True}","0d10f15a":"rf_random_no.best_params_","fc70eb37":"rf_random_no.best_estimator_.feature_importances_","08db5f15":"import pandas as pd\nimport matplotlib.pyplot as plt\n# Create a pd.Series of features importances\nimportances_rf = pd.Series(rf_random_no.best_estimator_.feature_importances_,\nindex = x_bz_co.columns)\n# Sort importances_rf\nsorted_importances_rf = importances_rf.sort_values()\n# Make a horizontal bar plot\nsorted_importances_rf.plot(kind='barh', color='blue')\nplt.show()","4d56f429":"final_rf_no = rf_random_no.best_estimator_","01314994":"test","e0e97620":"test_no_dates = test","465859e7":"test_no_dates.head(5)","f5411d29":"test_no_dates['date_time_2'] = test_no_dates['date_time'].astype('datetime64[ns]').astype(np.int64)\/10**9\n\ntest_no_dates['date_time'] = pd.to_datetime(test_no_dates['date_time'])\ntest_no_dates['year'] = test_no_dates['date_time'].dt.year\ntest_no_dates['month'] = test_no_dates['date_time'].dt.month\ntest_no_dates['week'] = test_no_dates['date_time'].dt.week\ntest_no_dates['day'] = test_no_dates['date_time'].dt.day\ntest_no_dates['dayofweek'] = test_no_dates['date_time'].dt.dayofweek\ntest_no_dates['time'] = test_no_dates['date_time'].dt.date - test_no_dates['date_time'].dt.date.min()\ntest_no_dates['hour'] = test_no_dates['date_time'].dt.hour\ntest_no_dates['time'] = test_no_dates['time'].apply(lambda x : x.days)\ntest_no_dates.drop(columns = 'date_time', inplace = True)","c173b109":"test_no_dates.describe()","07999371":"test_y_bz=final_rf_bz.predict(test_no_dates)","d71793ae":"test_y_bz=pd.DataFrame(test_y_bz)\ntest_y_bz.columns=['test_benzene']\ntest_y_bz","ef243665":"test_no_dates_bz=test_no_dates.join(test_y_bz)","072e96cd":"test_no_dates_bz","63d8e804":"test_y_co=final_rf_co.predict(test_no_dates_bz)","a168ae8f":"test_y_co=pd.DataFrame(test_y_co)\ntest_y_co.columns=['test_carbon_monoxide']\ntest_y_co","77f2fb8a":"test_no_dates_co_bz=test_no_dates_bz.join(test_y_co)","9be1a594":"test_no_dates_co_bz","e7d90794":"test_y_no=final_rf_no.predict(test_no_dates_co_bz)","135898cb":"test_y_co","27e91b52":"test_y_no","cc48eec2":"submit=test_y_co\n#submit.columns=['target_carbon_monoxide']\nsubmit=submit.join(test_y_bz, how='inner')\n#submit.columns=['target_carbon_monoxide','target_benzene']\nsubmit=submit.join(pd.DataFrame(test_y_no), how='inner')\nsubmit.columns=['target_carbon_monoxide','target_benzene','target_nitrogen_oxides']\nsubmit","76dfdc47":"test2 = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv')","6715101e":"test2","cd7d8f78":"submission=test2['date_time'].to_frame().join(submit, how='inner')","d988bb40":"submission","99a4e116":"submission.to_csv('20210711_RF.csv',index=False)","bbab1145":"#### Nitrous","4fc7cd3e":"#### Join all Predicted columns together","c7dbbea1":"# July Tabular PlayGround - Basline Models","1b63d7da":"##### Look into the Random Search's Best Model","010bfee9":"## Final Predictions for test data\n\n* The data frame names are weird - but the code works","379f0908":"#### Predict Benzene first","454a802d":"#### Import Supervised Learning Models","4191629f":"##### Look into the Random Search's Best Model","fca4db74":"#### Import required libraries and training data","8605a748":"#### Add the predicted Carbon Monoxide values to test X","ec2c89e0":"#### Add the predicted Benzene values to test X","a78a58a1":"### Predict Using trained models","cd7e0f4d":"#### Finally Predict Nitrous","6be65370":"### Benzene","27a07ffd":"### Carbon Monoxide","7f8cef46":"#### Predict Carbon Monoxide next","74b806d2":"### Create new Features based on Dates\n\nBorrowed from https:\/\/www.kaggle.com\/junhyeok99\/automl-pycaret"}}