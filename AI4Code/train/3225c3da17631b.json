{"cell_type":{"d6436d17":"code","e96fba25":"code","aaa78f6d":"code","d232d3bc":"code","96e3a24a":"code","58ed1af2":"code","75609b39":"markdown","f954ba27":"markdown","f030478f":"markdown","11fd0dd8":"markdown","3cf830fa":"markdown","ee34da78":"markdown","37aa8a31":"markdown","d48964c8":"markdown","3d448f09":"markdown","a0e9d652":"markdown","1b46877a":"markdown"},"source":{"d6436d17":"# Import the necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom numpy import mean\nfrom sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\n#from mlxtend.classifier import StackingCVClassifier\nimport warnings\nwarnings.simplefilter(action =\"ignore\")","e96fba25":"data = pd.read_csv('..\/input\/breast-cancer-dataset-clean-data\/data.csv')\ndata['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})\nX = data.loc[:, data.columns != 'diagnosis']\ny = data['diagnosis']","aaa78f6d":"#Visualizing Multidimensional Relationships\nplt.style.use('fivethirtyeight')\nsns.set_style(\"white\")\nsns.pairplot(data[[data.columns[30], data.columns[1],data.columns[2],data.columns[3],\n                     data.columns[4], data.columns[5]]], hue = 'diagnosis' , size=3)","d232d3bc":"def get_models():\n    models = dict()\n    models['Random Forest'] = RandomForestClassifier(criterion='entropy', n_estimators=250, min_samples_leaf=1, min_samples_split=2, random_state=42)\n    models['Decision Tree'] = DecisionTreeClassifier(criterion='entropy', max_depth=10, max_features='log2', min_samples_split=3, random_state=42)\n    models['Extra Trees'] = ExtraTreesClassifier(criterion='gini', n_estimators=350, max_depth=15, min_samples_split=2, random_state=42)        \n    models['xb'] = xgb.XGBClassifier(learning_rate = 0.01, max_depth = 10, min_child_weight = 3, subsample = 0.5, colsample_bytree = 0.7, n_estimators = 100)\n    return models\n\ndef evaluate_model(model, X, y):\n    kfold = model_selection.KFold(n_splits=10, random_state=42)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=kfold, n_jobs=-1, error_score='raise')\n    #scores = cross_val_score(model, X, y, scoring='precision', cv=kfold, n_jobs=-1, error_score='raise')\n    return scores","96e3a24a":"print('Accuracy Score of individual models in Cross Validation:')\n#get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print('Accuracy Score of %s = %0.2f' % (name, mean(scores)*100))","58ed1af2":"kfold = model_selection.KFold(n_splits=10, random_state=42)\nestimators = [\n    ('rf', RandomForestClassifier(criterion='entropy', n_estimators=250, min_samples_leaf=1, min_samples_split=2, random_state=42)),\n    ('dt', DecisionTreeClassifier(criterion='entropy', max_depth=10, max_features='log2', min_samples_split=3, random_state=42)),\n    ('et', ExtraTreesClassifier(criterion='gini', n_estimators=350, max_depth=15, min_samples_split=2, random_state=42)),\n    ('xb', xgb.XGBClassifier(learning_rate = 0.01, max_depth = 10, min_child_weight = 3, subsample = 0.5, colsample_bytree = 0.7, n_estimators = 100))\n\n]\nXg = xgb.XGBClassifier(learning_rate = 0.01, max_depth = 10, min_child_weight = 3, subsample = 0.5, colsample_bytree = 0.7, n_estimators = 100)\nensemble = StackingClassifier(estimators=estimators, final_estimator=Xg, cv = kfold)\n#clf= StackingCVClassifier(classifiers=[rF, dT, eT], meta_classifier= rF, random_state=42, cv=kfold)\nen_acc = cross_val_score(ensemble, X, y, scoring='accuracy', cv=kfold, n_jobs=-1, error_score='raise')\nprint('Accuracy of Ensemble Model = %.4f' % (mean(en_acc)*100))\n#en_f1 = cross_val_score(ensemble, X, y, scoring='f1_macro', cv=kfold, n_jobs=-1, error_score='raise')\n#print('F1-score of Ensemble Model = %.3f' % (mean(en_f1)))","75609b39":"The **benefit** of stacking is that it can harness the capabilities of a range of well-performing models on a classification or regression task and make predictions that have better performance than any single model in the ensemble.\n\n***Eventually, this notebook concludes that the Stacking ensemble learning approach can be a substantial paradigm in machine learning studies to improve models accuracy and reduce the error rate.***","f954ba27":"Now, Let's get our hands dirty by exploring and implementing a Stacking ensemble learning model.","f030478f":"# 2. Load libraries and read the data","11fd0dd8":"# 1. Dataset Description\n\nThe Breast Cancer datasets is available UCI machine learning repository maintained by the University of California, Irvine. The dataset contains 569 samples of malignant and benign tumor cells.\n\nThe first two columns in the dataset store the unique ID numbers of the samples and the corresponding diagnosis (M=malignant, B=benign), respectively. The columns 3-32 contain 30 real-value features that have been computed from digitized images of the cell nuclei, which can be used to build a model to predict whether a tumor is benign or malignant.\n\n* 1= Malignant (Cancerous) - Present (M)\n* 0= Benign (Not Cancerous) -Absent (B)\n\nThe dataset can be found on UCI Machine Learning Repository: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29 \n\nI have cleaned the data beforehand and then used it accordingly. Perhaps, you can check [THIS](https:\/\/www.kaggle.com\/pankajbhowmik\/breast-cancer-analysis-prediction\/notebook) notebook for further interest.","3cf830fa":"**Reference**\n\n[1] Pankaj Bhowmik et al. \"Cardiotocography Data Analysis to Predict Fetal Health Risks with Tree-Based Ensemble Learning\", International Journal of Information Technology and Computer Science(IJITCS), Vol.13, No.5, pp.30-40, 2021. DOI: 10.5815\/ijitcs.2021.05.03 link: http:\/\/www.mecs-press.org\/ijitcs\/ijitcs-v13-n5\/v13n5-3.html \n\n[2] Susan Yuhou Xia, \u201cUsing a Stacking Model Ensemble Approach to Predict Rare Events,\u201d Conference Talks, SciPy 2019, 18th annual Scientific Computing with Python Conference, in Austin, Texas, USA. [Online]. Available: youtube.com\/watch?v=6oD5K0x1k7c&t=551s","ee34da78":"# 0. Stacking Ensemble: Overview\n\nStacked ensemble learning is a layered-structure machine learning approach that combines the predictive competency of diverse base learning algorithms to attain higher prediction accuracy compared to the individual weak learners [1]. Stacking is an ensemble learning technique to combine multiple classification models via a meta-classifier. It uses a meta-learning algorithm to learn how to best combine the predictions from two or more base machine learning algorithms. \n\n![SEL](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingClassifier_files\/stackingclassification_overview.png)","37aa8a31":"# 4. Implement Stacking Ensemble Model\n\nThe scikit-learn Python machine learning library provides an implementation of Stacking ensemble for machine learning.","d48964c8":"# 3. List of Individual Algorithms\n\nHere, we choose four tree-based algorithms for this notebook.\n\n1. Random Forest Classifier\n1. Decision Tree Classifier\n1. Extra Trees Classifier and\n1. XGBoost Classifier\n\nEach algorithm will be evaluated using the known optimal hyperparameters. *[**You can further optimize the parameters of models with Grid search or Randomize Search or so on**]*. The function *get_models()* below creates the models we wish to evaluate.","3d448f09":"# EDA","a0e9d652":"Each model will be evaluated using k-fold cross-validation.\nThe *evaluate_model()* function below takes a model instance and returns a list of scores from 10-fold cross-validation.","1b46877a":"**Cross Validation Accuracy Comparison**\n\n* Random Forest Classifier = 95.96%\n* Decision Tree Classifier = 92.79%\n* Extra Trees Classifier = 96.31%\n* XGBoost Classifier = 94.38%\n* **Ensemble Learning Model = 96.66%**\n\nAlthough the individual classifier models showed reasonable outcomes, the Stacking ensemble learning approach has boosted the performance of the final model. The Stacked ensemble model outweighed the individual models, not significantly though comaparing with Extra Trees Classifier.\n\n ***Feel free to try other machine learning algorithms and switch the meta learner as well, to see whether the outcome changes.**"}}