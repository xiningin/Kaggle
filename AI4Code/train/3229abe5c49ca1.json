{"cell_type":{"93864e85":"code","b375a3c3":"code","13fe549f":"code","2bbd5dfe":"code","4cc8ff13":"code","a4692110":"code","c0423c13":"code","f0c1a0d0":"code","48aa00ab":"code","d32b6a6d":"code","398d1b88":"code","7e4531f3":"code","c4614fb2":"code","d1d631e4":"code","d19893ef":"code","09add2df":"code","61e775d6":"code","4db997fd":"code","847f8516":"code","c0cfd4c0":"code","ecdf1463":"code","cbc383d6":"code","6bea1316":"code","074b41bb":"code","5a551d00":"code","12ddf43c":"code","777c7d0e":"markdown","4c2ed35f":"markdown","d695c3ef":"markdown","232d52fb":"markdown","aeec1f09":"markdown","104482ed":"markdown","867e6686":"markdown","508bbd3e":"markdown","550560ca":"markdown","941d814f":"markdown","23bc5056":"markdown","6188a37b":"markdown"},"source":{"93864e85":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf","b375a3c3":"tf.__version__","13fe549f":"data = pd.read_csv('..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')\ndata.shape","2bbd5dfe":"# let's check the head of the dataset\ndata.head()","4cc8ff13":"data.isnull().sum()","a4692110":"# imputing missing values\n# imputing missing values\ndata['LoanAmount']=data['LoanAmount'].fillna(data['LoanAmount'].mean())\ndata['Credit_History']=data['Credit_History'].fillna(data['Credit_History'].median())\ndata.dropna(inplace=True)","c0423c13":"data.isnull().sum().sum()","f0c1a0d0":"# splitting the dependent and independent variables\nX = data.iloc[: , 1:-1].values\nY = data.iloc[: ,-1].values","48aa00ab":"print(X)\nprint(Y)","d32b6a6d":"Y = np.where(Y=='Y',1,Y)\nY = np.where(Y=='N',0,Y)\nprint(Y)","398d1b88":"Y = Y.astype('int')\nY.dtype","7e4531f3":"X[: ,2] = np.where(X[:, 2]=='3+',3,X[: ,2])\nX[:, 2]","c4614fb2":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nX[:, 0] = le.fit_transform(X[:, 0])\nX[:, 1] = le.fit_transform(X[:, 1])\nX[:, 3] = le.fit_transform(X[:, 3])\nX[:, 4] = le.fit_transform(X[:, 4])","d1d631e4":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [10])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))","d19893ef":"X = X.astype('int')\nX.dtype","09add2df":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)","61e775d6":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","4db997fd":"# Initializing ANN\nann = tf.keras.models.Sequential()","847f8516":"# adding input and first hidden layer\nann.add(tf.keras.layers.Dense(units=8, activation='relu'))","c0cfd4c0":"# adding second hidden layer\nann.add(tf.keras.layers.Dense(units=6, activation='relu'))","ecdf1463":"# adding third hidden layer\nann.add(tf.keras.layers.Dense(units=6, activation='relu'))","cbc383d6":"# adding output layer\nann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))","6bea1316":"# compiling ANN\nann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","074b41bb":"# training ANN on training set\nann.fit(X_train, y_train, batch_size = 32, epochs = 150)","5a551d00":"y_pred = ann.predict(X_test)\ny_pred = (y_pred > 0.5)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","12ddf43c":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","777c7d0e":"### Splitting the dataset into training and test data","4c2ed35f":"### checking for missing values","d695c3ef":"There are 7 columns which contain missing values.","232d52fb":"The dataset contains 614 rows and 13 columns.","aeec1f09":"## Building the ANN","104482ed":"#### Feature Scaling","867e6686":"## Encoding categorical data","508bbd3e":"## Data Preprocessing","550560ca":"### Importing the dataset","941d814f":"Loan_Status is the target column","23bc5056":"## Importing the libraries","6188a37b":"# Loan Prediction using Artificial Neural Network"}}