{"cell_type":{"63f7e01b":"code","357d2913":"code","d3b8abe5":"code","d7acc4da":"code","6cf4b443":"code","d77334eb":"code","15c7e091":"code","eb7b831d":"code","0b231fc5":"code","eeeae1d2":"code","899a4aea":"code","9a84c007":"code","6b91ebbb":"code","38f1923e":"code","44391ae8":"code","8e28f03e":"code","fa4dad02":"code","9dcb9e2d":"code","c5de4aeb":"code","00b296f7":"code","25f951f3":"code","c10e2f24":"code","90eca3c7":"code","5ab2f380":"code","e925e2f6":"code","da61b1eb":"code","e5432463":"code","02485d0a":"code","ee596680":"code","22233ef6":"code","56b13771":"code","bd3c8cb5":"markdown","e8f4881b":"markdown","fc40e1cb":"markdown","da105a9c":"markdown","e34d816d":"markdown","6c866658":"markdown","9cfe06d0":"markdown","b2563ab4":"markdown","a90c83dc":"markdown","a5374dcb":"markdown","e5c795f6":"markdown","d9d08caf":"markdown","353becd3":"markdown","ff509489":"markdown","877431f6":"markdown","e04f3371":"markdown","ba9d23e5":"markdown","dfef808a":"markdown","f14f551c":"markdown","4e3520f4":"markdown","bf479e23":"markdown","1c7808dc":"markdown"},"source":{"63f7e01b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","357d2913":"#for preprocessing\nimport re\nimport random\nfrom string import punctuation\nimport nltk\nfrom nltk.corpus import stopwords\n\n#for building ml models\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix\n\n\n#for building a neural network\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch import optim","d3b8abe5":"#read small chunk of data\ndata = pd.read_csv(\"..\/input\/enron-email-dataset\/emails.csv\", chunksize=10000)\ndf_chunk = next(data)\ndf_chunk.head()","d7acc4da":"df_chunk.tail()","6cf4b443":"df_chunk['file'][0]","d77334eb":"df_chunk['message'][1].split(\"\\n\")","15c7e091":"df_chunk['message'][1].split(\"\\n\")[15:]","eb7b831d":"#extract author of email\nextract_name = re.compile(r'^([a-z0-9\\-]*)\/')\ndf_chunk['name'] = df_chunk['file'].str.extract(extract_name)\ndf_chunk.head()","0b231fc5":"def extract_text(Series):\n    \"\"\"\n    returns the string of content of the email in the \"message\" column while discarding those that contains the headers specified below.\n    \n    Series is the \"message\" column of the enron email dataset.\n    \n    \"\"\"\n    \n    result = []\n    \n    headers = ['Message-ID:', 'Date:', 'From:', 'To:', 'Subject:', 'Mime-Version:', 'Content-Type',\n              'Content-Transfer-Encoding:', 'X-From:', 'X-To:', 'X-cc:', 'X-bcc:', 'X-Folder:', \n              'X-Origin:', 'X-FileName:', 'Cc:', 'Bcc:', '-----Original Message-----',\n              '----------------------', \"Sent:\", \"cc:\"]\n    \n    for row, message in enumerate(Series):\n        strings = message.split(\"\\n\")\n        accepted_strings = [string for string in strings if all(header not in string for header in headers)]\n        result.append(\" \".join(accepted_strings))\n            \n    return result","eeeae1d2":"#extract content of email\ndf_chunk['text'] = extract_text(df_chunk['message'])\ndf_chunk.head()","899a4aea":"def clean_text(Series):\n    \"\"\"\n    returns the string of cleaned text of the message of the dataset.\n    \n    Series is the series of string containing text that wants to be cleaned\n    \n    \"\"\"\n    \n    result = []\n    sw = stopwords.words(\"english\")\n    strings = Series.str.lower()\n    \n    for string in strings:\n        new_string = []\n        words = string.split(\" \")\n        \n        for word in words:\n            word = word.strip(punctuation) \n            \n            if word in sw:\n                continue\n            if re.search(r'[\\W\\d]',word):\n                continue\n                \n            new_string.append(word)\n                \n        new_string = \" \".join(new_string)\n        \n        result.append(new_string)\n    \n    return result","9a84c007":"#cleaning the text column\ndf_chunk['text'] = clean_text(df_chunk['text'])\ndf_chunk.head()","6b91ebbb":"df_chunk['name'].value_counts()","38f1923e":"#grouping and combining the text according to the author's name\ndf_grouped = df_chunk.groupby(\"name\")['text'].apply(' '.join).reset_index()\ndf_grouped.head()","44391ae8":"vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)\nfeatures = vectorizer.fit_transform(df_grouped['text'])","8e28f03e":"#read and preprocess the whole dataset\ndf = pd.read_csv(\"..\/input\/enron-email-dataset\/emails.csv\")\ndf['name'] = df['file'].str.extract(extract_name)\ndf['text'] = extract_text(df['message'])\ndf['text'] = clean_text(df['text'])\ndf.head()","fa4dad02":"df_grouped = df.groupby(\"name\")['text'].apply(' '.join).reset_index()\ndf_grouped.head()","9dcb9e2d":"poi_names = ['lay-k', 'skilling-j', 'forney-j', 'delainey-d']\ndf_grouped['poi'] = 0\n\nfor idx, name in enumerate(df_grouped['name']):\n    if name in poi_names:\n        df_grouped.loc[idx, 'poi'] = 1\n        \nprint(\"Number of POI : {}\".format(sum(df_grouped['poi'])))","c5de4aeb":"df_grouped.head()","00b296f7":"# save the processed dataframe in order to save time for the next time you want to use it\n# df_grouped.to_csv(\"df_grouped.csv\", index=False)","25f951f3":"df_grouped = pd.read_csv(\"..\/input\/groupeddata\/df_grouped.csv\")","c10e2f24":"df_grouped[df_grouped['poi']==1]","90eca3c7":"poi_idx = df_grouped.index[df_grouped['poi']==1].to_list()\n\n#select rows that are not POI, then shuffle the index\nshuffle_idx = [idx for idx in df_grouped.index.to_list() if idx not in poi_idx]\nrandom.shuffle(shuffle_idx)\n\n#select 70% as training data, then add 3 more data which are POIs\ntrain_idx = shuffle_idx[:round(len(shuffle_idx)*0.7)]\ntrain_idx = train_idx + poi_idx[:3]\n\n#select the rest as test data, adding 1 more data which is POI\ntest_idx = shuffle_idx[round(len(shuffle_idx)*0.7):]\ntest_idx = test_idx + poi_idx[3:]","5ab2f380":"df_train = df_grouped.loc[train_idx, :].reset_index(drop=True)\ndf_test = df_grouped.loc[test_idx, :].reset_index(drop=True)\ndf_train.head()","e925e2f6":"vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)\nfeatures_train = vectorizer.fit_transform(df_train['text']).toarray()\nfeatures_test = vectorizer.transform(df_test['text']).toarray()","da61b1eb":"#decision tree\ndt_clf = DecisionTreeClassifier()\ndt_clf.fit(features_train, df_train['poi'])\ndt_pred = dt_clf.predict(features_test)\n\n#gaussian naive-bayes\nnb_clf = GaussianNB()\nnb_clf.fit(features_train, df_train['poi'])\nnb_pred = nb_clf.predict(features_test)\n\n#knn\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(features_train, df_train['poi'])\nknn_pred = knn_clf.predict(features_test)\n\n#adaboost\nab_clf = GradientBoostingClassifier()\nab_clf.fit(features_train, df_train['poi'])\nab_pred = ab_clf.predict(features_test)\n\npreds = {\"Decision Tree\":dt_pred,\n         \"Naive-bayes\":nb_pred,\n         \"K-Nearest Neighbor\":knn_pred,\n         \"Adaboost\":ab_pred}","e5432463":"#print the confusion matrix for the predictions of each models\nfor pred in preds:\n    print(\"\\n\", pred)\n    print(pd.DataFrame(confusion_matrix(df_test['poi'], preds[pred]), \n                 columns=[\"0(Predicted)\", \"1(Predicted)\"], index=[\"0(Actual)\", \"1(Actual)\"]))","02485d0a":"print(\"Size of input layer of neural network : {}\".format(features_train.shape[1]))","ee596680":"### I tried using pytorch, but it still isnt working, so I used the sklearn implementation instead\n# nn_input = torch.from_numpy(features_train)\n\n# model = nn.Sequential(nn.Linear(features_train.shape[1], 1000),\n#                       nn.ReLU(),\n#                       nn.Linear(1000, 100),\n#                       nn.ReLU(),\n#                       nn.Linear(100, 2),\n#                       nn.LogSoftmax(dim=0))\n# model = model.float()\n\n# criterion = nn.NLLLoss()\n# optimizer = optim.SGD(model.parameters(), lr=0.003)","22233ef6":"# epochs = 5\n# for e in range(epochs):\n#     running_loss = 0\n#     for idx in range(len(df_train['poi'])):\n    \n#         optimizer.zero_grad()\n#         output = model(nn_input[idx].float())\n#         print(\"output : {}, target : {}\".format(output, df_train.loc[idx, 'poi']))\n#         loss = criterion(output, df_train.loc[idx, 'poi'])\n#         loss.backward()\n#         optimizer.step()\n        \n#         running_loss += loss.item()\n#     else:\n#         print(f\"Training loss: {running_loss\/len(df_train['poi'])}\")","56b13771":"nn_clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n                     hidden_layer_sizes=(100, 10, 2), random_state=1)\nnn_clf.fit(features_train, df_train['poi'])\nnn_pred = nn_clf.predict(features_test)\nprint(\"Neural Network\")\nprint(pd.DataFrame(confusion_matrix(df_test['poi'], nn_pred), \n                 columns=[\"0(Predicted)\", \"1(Predicted)\"], index=[\"0(Actual)\", \"1(Actual)\"]))","bd3c8cb5":"In order to extract the content of the emails, I want to see the format with which the emails are displayed in the dataset.","e8f4881b":"The neural network still are not able to detect the POI. This seems to show that the available emails are not enough to accurately detect the prescence of POIs. ","fc40e1cb":"<a id=\"split\"\/>**SPLITTING DATA INTO TRAIN AND TEST**","da105a9c":"Now I will clean the text of the emails through stopwords removal, punctuations removal, making all letters lowercase, and removing numbers, email addresses, and links.","e34d816d":"It seems that the \"message\" column contains several sections of the email containing metadata and content of the emails, separated by \"\\n\". So next, I will use regex to extract the author and content of the emails.","6c866658":"Now with a clear idea on how to process the dataset, I will use the methods above to process to whole data.","9cfe06d0":"For the models, I will use several common classification algorithms : decision tree, naive-bayes, k-nearest neighbor, and adaboost.","b2563ab4":"<a id=\"test\"\/>**TESTING USING SMALL CHUNK OF DATA**","a90c83dc":"1. <a href=\"#test\">Testing using small chunk of data<\/a>\n2. <a href=\"#whole\">Using the whole dataset<\/a>\n3. <a href=\"#split\">Splitting data into train and test<\/a>\n4. <a href=\"#models\">Building the models<\/a>\n5. <a href=\"#conclusion\">Conclusion<\/a>","a5374dcb":"<a id=\"models\"\/>**BUILDING THE MODELS**","e5c795f6":"Turns out that there are only 4 POI from the list of 158 people in the dataset. At first glance, only 4 people does not seems enough to capture the patterns that might show that someone is a POI. But, there is no possible way to arbitralily add more POIs, since the number of POI are fixed and I cannot access the emails that are not present in the dataset. Therefore, I will try to work with what I have.","d9d08caf":"**ENRON EMAIL POI CLASSIFIER**\n\n\"The Enron scandal, publicized in October 2001, led to the bankruptcy of the Enron Corporation, an American energy company based in Houston, Texas, and the de facto dissolution of Arthur Andersen, which was one of the five largest audit and accountancy partnerships in the world. In addition to being the largest bankruptcy reorganization in American history at that time, Enron was cited as the biggest audit failure.\"\n\nsource : https:\/\/en.wikipedia.org\/wiki\/Enron_scandal\n\nThe Enron Corpus is a database of over 600,000 emails generated by 158 employees of the Enron Corporation in the years leading up to the company's collapse in December 2001. The corpus was generated from Enron email servers by the Federal Energy Regulatory Commission (FERC) during its subsequent investigation.\n\nI will try to use the emails to find out whether or not it is useful in identifying POIs (Person of Interests) in the scandal. \n","353becd3":"From the result of the ML models predictions, it looks like the emails of the 4 POI in the dataset are not enough to accurately used to detect the prescence of POIs. I didnt try to mess around with the hyperparameters of the models yet, and I will try to do it when I have the time later. Any corrections, criticisms, or ideas will be greatly appreciated. Thank you for reading !","ff509489":"Splitting the data into training and testing set presents a problem, since the number of POIs are only 4. If we randomly split the dataset, there is a chance that all of the POIs are only present in the training set or testing set. So in order to prevent that, I will choose 70% of the data which are not POI as training data, then manually add 3 of the 4 POIs to the training data. Then the rest of the data will be used as testing data.","877431f6":"In order to use the text as a feature to train the ML models, we first need to vectorise the words of the text. There are several methods available, but I will use the Tf-idf Vectoriser which is described here : https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html.","e04f3371":"Now I would like to group the text according to the name of the authors, and then append them all together. This is because the input to the ML models will be all of the words written by each person.","ba9d23e5":"From the result we can see that the classifiers cannot detect the prescence of the POI in the test set. All of the classifiers predicted that the actual POI is not a POI. I will try another approach, using multi-layer perceptron classifier.","dfef808a":"Next I want to add the target feature, which is whether or not the person is a POI or not. In order to determine which person are POIs, I will use the information in the link below.","f14f551c":"POI source : http:\/\/usatoday30.usatoday.com\/money\/industries\/energy\/2005-12-28-enron-participants_x.htm","4e3520f4":"<a id=\"conclusion\"\/>**CONCLUSION**","bf479e23":"The dataset size is huge, containing over 600,000 emails. In order to mess around with the data first, I will only use 10000 rows of the dataset.","1c7808dc":"<a id=\"whole\"\/>**USING THE WHOLE DATASET**"}}