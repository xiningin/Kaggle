{"cell_type":{"88bbed27":"code","7ecc2699":"code","73eda545":"code","0e1f6ab1":"code","a8081625":"code","210ec72c":"code","cb68b580":"code","7f7476f4":"code","d91c4147":"code","b61c9f73":"code","8c1b5f7d":"code","096ffc29":"code","95ef6228":"code","fa3f62ce":"markdown","e2429f32":"markdown","2bbe574b":"markdown","c10ae74c":"markdown","ccae9553":"markdown","134c9dc3":"markdown","640bbd9c":"markdown","a17521a5":"markdown","25dcec3b":"markdown","68e515e5":"markdown","c59684d5":"markdown","438f2dbe":"markdown","42e527df":"markdown","f51a3978":"markdown","50a1ca2a":"markdown","75d8e363":"markdown","b9e6a4e0":"markdown","79a69fb5":"markdown","875fb222":"markdown","22dcdf38":"markdown","339f5f1b":"markdown"},"source":{"88bbed27":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as implt\nfrom PIL import Image \nimport seaborn as sns\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","7ecc2699":"train_dataset='..\/input\/covid19-image-dataset\/Covid19-dataset\/train'\ntest_dataset='..\/input\/covid19-image-dataset\/Covid19-dataset\/test'\n\ntrain_covid='..\/input\/covid19-image-dataset\/Covid19-dataset\/train\/Covid'\ntrain_normal='..\/input\/covid19-image-dataset\/Covid19-dataset\/train\/Normal'\ntrain_viral_pneumonia='..\/input\/covid19-image-dataset\/Covid19-dataset\/train\/Viral Pneumonia'\n\ntest_covid='..\/input\/covid19-image-dataset\/Covid19-test\/train\/Covid'\ntest_normal='..\/input\/covid19-image-dataset\/Covid19-test\/train\/Normal'\ntest_viral_pneumonia='..\/input\/covid19-image-dataset\/Covid19-dataset\/test\/Viral Pneumonia'\n\n","73eda545":"# plot some samples\ncovid1=implt.imread(train_covid+'\/01.jpeg')\nplt.subplot(1,2,1)\nplt.title('Covid Sample')\nplt.imshow(covid1)\nplt.show()","0e1f6ab1":"img_size=50\ncovid_train=[]\nnormal_train=[]\nviral_pneumonia_train=[]\n\n\ncovid_labels=np.ones(111)\nnormal_labels=np.zeros(70)\nviral_pneumonia_labels=np.full((70),2)\n\nfor i in os.listdir(train_covid):\n     if os.path.isfile(train_dataset + \"\/Covid\/\" + i):\n            covid=Image.open(train_dataset+'\/Covid\/'+i).convert('L') # converting grey scale\n            covid=covid.resize((img_size,img_size),Image.ANTIALIAS) # resizing to 50,50\n            covid=np.asarray(covid)\/255 # bit format\n            covid_train.append(covid)\nfor i in os.listdir(train_normal):\n     if os.path.isfile(train_dataset + \"\/Normal\/\" + i):\n            normal=Image.open(train_dataset+'\/Normal\/'+i).convert('L') # converting grey scale\n            normal=normal.resize((img_size,img_size),Image.ANTIALIAS) # resizing to 50,50\n            normal=np.asarray(normal)\/255 # bit format\n            normal_train.append(normal)\nfor i in os.listdir(train_viral_pneumonia):\n     if os.path.isfile(train_dataset + \"\/Viral Pneumonia\/\" + i):\n            viral_pneumonia=Image.open(train_dataset+'\/Viral Pneumonia\/'+i).convert('L') # converting grey scale\n            viral_pneumonia=viral_pneumonia.resize((img_size,img_size),Image.ANTIALIAS) # resizing to 50,50\n            viral_pneumonia=np.asarray(viral_pneumonia)\/255 # bit format\n            viral_pneumonia_train.append(viral_pneumonia)\n\nX_train=np.concatenate((covid_train,normal_train,viral_pneumonia_train),axis=0)\nY_train=np.concatenate((covid_labels,normal_labels,viral_pneumonia_labels),axis=0).reshape(X_train.shape[0],1)\nprint('X train shape:',X_train.shape)\nprint('Y train shape:',Y_train.shape)","a8081625":"# Reshape\nX_train = X_train.reshape(-1,50,50,1)\nprint(\"x_train shape: \",X_train.shape)","210ec72c":"# Label Encoding\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nY_train = to_categorical(Y_train, num_classes = 3)","cb68b580":"# Split the train and the validation set for the fitting\nfrom sklearn.model_selection import train_test_split\nX_train,X_val,Y_train,Y_val=train_test_split(X_train,Y_train,test_size=0.1,random_state=2)\nprint(\"x_train shape\",X_train.shape)\nprint(\"x_test shape\",X_val.shape)\nprint(\"y_train shape\",Y_train.shape)\nprint(\"y_test shape\",Y_val.shape)","7f7476f4":"from sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nmodel = Sequential()\n#\nmodel.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (50,50,1)))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(filters = 16, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n# fully connected\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3, activation = \"softmax\"))\n","d91c4147":"# Define the optimizer\noptimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)","b61c9f73":"# Compile the model\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","8c1b5f7d":"epochs = 200  # for better result increase the epochs\nbatch_size = 10","096ffc29":"# data augmentation\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # dimesion reduction\n        rotation_range=5,  # randomly rotate images in the range 5 degrees\n        zoom_range = 0.1, # Randomly zoom image 10%\n        width_shift_range=0.1,  # randomly shift images horizontally 10%\n        height_shift_range=0.1,  # randomly shift images vertically 10%\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(X_train)","95ef6228":"history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_val,Y_val), steps_per_epoch=X_train.shape[0] \/\/ batch_size)","fa3f62ce":"* This dataset contains Images from The University of Montreal.\n* We have 3 classes which are names normal(0),covid(1),viral pneumonia(2) # digits 0,1,2\n* I will try to classificate images by use train and test dataset.\n* I am going to use CNN model this analysis.\n","e2429f32":"# Label Encoding","2bbe574b":"# Fit the Model","c10ae74c":"# Compile Model \n* categorical crossentropy\n* We make binary cross entropy at previous parts and in machine learning tutorial\n* At this time we use categorical crossentropy. That means that we have multi class","ccae9553":"# Processing Dataset","134c9dc3":"# Reshape\n* Train and test images (50 x 50)\n* We reshape all data to 50x50x1 3D matrices.\n* Keras needs an extra dimension in the end which correspond to channels. Our images are gray scaled so it use only one channel.","640bbd9c":"* conv => max pool => dropout => conv => max pool => dropout => fully connected (2 layer)\n* Dropout: Dropout is a technique where randomly selected neurons are ignored during training","a17521a5":"# Define optimizer\n* Adam optimizer: Change the learning rate","25dcec3b":"# Normalization,Reshape and label Encoding","68e515e5":"# Data Augmentation","c59684d5":"* To avoid overfitting problem, we need to expand artificially our handwritten digit dataset\n* Alter the training data with small transformations to reproduce the variations of digit.\n* For example, the number is not centered The scale is not the same (some who write with big\/small numbers) The image is rotated.","438f2dbe":"* Say you have a dataset of 10 examples (or samples). You have a batch size of 2, and you've specified you want the algorithm to run for 3 epochs. Therefore, in each epoch, you have 5 batches (10\/2 = 5). Each batch gets passed through the algorithm, therefore you have 5 iterations per epoch.","42e527df":"# Load Dataset","f51a3978":"# Epochs and Batch Size\n","50a1ca2a":"# Create Model ","75d8e363":"* We split the data into train and test sets.\n* test size is 10%.\n* train size is 90%","b9e6a4e0":"# Train test split","79a69fb5":"# Content\n* import libraries\n* Load Dataset\n* Processing Dataset\n* Normalization,Reshape and label Encoding\n* Train-test split\n* Implementing with Keras","875fb222":"* Encode labels to one hot vectors\n\n* 2 => [0,0,1,0,0,0,0,0,0,0]\n* 4 => [0,0,0,0,1,0,0,0,0,0]","22dcdf38":"# Normalization\n* We perform a grayscale normalization to reduce the effect of illumination's differences.\n* If we perform normalization, CNN works faster.\n\n* 'We have already made normalization in Processing Step'","339f5f1b":"# Hello everyone.\n## I have never work with Coronavirus Dataset because I hate this virus and it have terrible effect on the world.\n## The first time I worked with dataset that related Corona virus."}}