{"cell_type":{"5487a5e8":"code","90088bf2":"code","834e9de5":"code","103f2c9a":"code","b5b0ee96":"code","8fda896a":"code","4f58660f":"code","831b5316":"code","28dae39b":"code","7ac714ec":"code","b1c3481d":"code","7a38c4fc":"code","4f776928":"code","e589ed8d":"code","e1b29887":"code","92e9c38b":"code","13b84836":"code","bebfa50b":"code","d8f85b4f":"code","78665ffa":"code","5cddae88":"code","27d29690":"code","45b52b28":"code","23ee29ae":"code","876b34a0":"code","21f53913":"code","91b55b63":"code","e90a36e5":"code","38a10cbc":"code","064c554f":"code","d4f37203":"code","dc19730a":"code","e95bd1cf":"code","8ec1abce":"code","356d837e":"code","760eb867":"code","ec64e6f8":"code","d91e3180":"code","88c27b9d":"code","a05ce208":"code","a87ae07c":"code","0b075513":"code","c1dbefab":"code","b31d866a":"code","63b72b1d":"code","6889d019":"code","94314dad":"code","688bc675":"code","3a01fdeb":"code","449504b5":"code","be4648ec":"code","01450301":"code","4908e5d3":"code","e3ce1516":"code","18ac1038":"code","848a9aec":"markdown","bfb23ecb":"markdown","815e5a76":"markdown","d90c12b5":"markdown","9b7e5a32":"markdown","70ed462d":"markdown","d0e84914":"markdown","67f2c60e":"markdown","5879d139":"markdown","00b20e20":"markdown","c3143047":"markdown","7785daf4":"markdown","feebcdd3":"markdown","e94ed5e7":"markdown","79532ad7":"markdown","43de3c9d":"markdown","bddf4f40":"markdown","c7a57df1":"markdown","a889dc18":"markdown","d8a76e62":"markdown","e18108bf":"markdown","e5edf096":"markdown","87967871":"markdown","d814e71d":"markdown","006668ed":"markdown","61f62b78":"markdown","53221ca2":"markdown","bea27227":"markdown","e8cf45fb":"markdown","5e82a2ce":"markdown","5a6a929a":"markdown","e5256980":"markdown","51ef93a4":"markdown","003b284a":"markdown","c2628632":"markdown","ab52d255":"markdown","ee001159":"markdown","69a5a430":"markdown","f3c6cb6d":"markdown","c42dc678":"markdown","98b60c33":"markdown","27a906d9":"markdown"},"source":{"5487a5e8":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n\n# Handle table-like data and matrices\nimport numpy as np\nimport pandas as pd \n\n# Modelling Algorithms\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis , QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score\n\n# Modelling Helpers\nfrom sklearn.preprocessing import Imputer , Normalizer , scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import GridSearchCV\n\n# Visualisation\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\n# Configure visualisations\n%matplotlib inline\nmpl.style.use( 'ggplot' )\nsns.set_style( 'white' )\nparams = { \n    'axes.labelsize': \"large\",\n    'xtick.labelsize': 'x-large',\n    'legend.fontsize': 20,\n    'figure.dpi': 150,\n    'figure.figsize': [25, 7]\n}\nplt.rcParams.update(params)","90088bf2":"# Center all plots\nfrom IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n<\/style>\n\"\"\");","834e9de5":"train = pd.read_csv('..\/input\/train.csv' ) # Train\ntest = pd.read_csv('..\/input\/test.csv' ) # Test\ntest_df = test.copy()","103f2c9a":"# Explore Train Data\ntrain.head()","b5b0ee96":"train.shape","8fda896a":"train.describe()","4f58660f":"train.info()","831b5316":"train.dtypes","28dae39b":"test.head()","7ac714ec":"# Test data doesn't have 'Survived' column and that's what we have to predict","b1c3481d":"# Let's look at the figures and Understand the Survival Ratio\ntrain.Survived.value_counts()","7a38c4fc":"# Look at the percentage\ntrain.Survived.value_counts(normalize=True)","4f776928":"sns.factorplot(x='Survived' , kind='count' , data=train , palette=['r','g'] , size=3 , aspect=.6)","e589ed8d":"pd.crosstab(train.Pclass , train.Survived , margins=True)","e1b29887":"train[['Pclass' , 'Survived']].groupby('Pclass').mean()","92e9c38b":"# Let's Plot graph to better Visualize","13b84836":"sns.factorplot(x='Pclass' , data=train , col='Survived' , kind='count' , size=5 , aspect=.8)","bebfa50b":"pd.crosstab(train.Sex , train.Survived , margins=True)","d8f85b4f":"# To view percentage, add normalize='index'\npd.crosstab(train.Sex , train.Survived , normalize='index')","78665ffa":"# Let's plot some graphs to visualize","5cddae88":"sns.factorplot(x='Sex' , y='Age' , data=train , hue='Survived' , kind='violin' , palette=['r','g'] , split=True)","27d29690":"sns.factorplot(x='Sex' , data=train , hue='Survived' , kind='count' , palette=['r','g'])","45b52b28":"# Age is a continuous variable.. Therefore , let's Analyze the Age variable by plotting Distribution graphs before and after filling Null values","23ee29ae":"sns.kdeplot(train.Age , shade=True , color='r')","876b34a0":"print('Median : ' + str(train.Age.median()) + '  Mean : ' + str(train.Age.mean()))","21f53913":"# For this case mean and median are both close, so we can fill with any of these, but I'll go with median.","91b55b63":"print(train.Age.count())\n# Train\ntrain['Age'].fillna(train.Age.median() , inplace=True)","e90a36e5":"print(train.Age.count())  # Null values filled","38a10cbc":"# Now plot kde plot.\nsns.kdeplot(train['Age'] , shade=True , color='r')","064c554f":"sns.factorplot(x='Sex',y='Age' , col='Pclass', data=train , hue='Survived' , kind = 'box', palette=['r','g'])","d4f37203":"# Understanding Box Plot :\n\n# The bottom line indicates the min value of Age.\n# The upper line indicates the max value.\n# The middle line of the box is the median or the 50% percentile.\n# The side lines of the box are the 25 and 75 percentiles respectively.","dc19730a":"sns.factorplot(x='Embarked' , y ='Fare' , kind='bar', data=train , hue='Survived' , palette=['r','g'])","e95bd1cf":"print(train.Embarked.count())\n# Fill NaN values\ntrain['Embarked'].fillna(train['Embarked'].mode()[0] ,inplace=True)","8ec1abce":"train.Embarked.count() # filled the values with Mode.","356d837e":"pd.crosstab([train.Sex,train.Survived] , [train.Pclass,train.Embarked] , margins=True)","760eb867":"sns.violinplot(x='Embarked' , y='Pclass' , data=train , hue='Survived' , palette=['r','g'])","ec64e6f8":"train[['SibSp' , 'Survived']].groupby('SibSp').mean()","d91e3180":"# Similarly let's examine Parch","88c27b9d":"train[['Parch','Survived']].groupby('Parch').mean()","a05ce208":"# create New Attribute\n\n# Train\ntrain['Alone']=0\ntrain.loc[(train['SibSp']==0) & (train['Parch']==0) , 'Alone'] = 1\n\n# Similarly for test\ntest['Alone']=0\ntest.loc[(test['SibSp']==0) & (test['Parch']==0) , 'Alone'] = 1","a87ae07c":"# Check\ntrain.head()\n# test.head()","0b075513":"drop_features = ['PassengerId' , 'Name' , 'SibSp' , 'Parch' , 'Ticket' , 'Cabin']\n# Train\ntrain.drop(drop_features , axis=1, inplace = True)\n# Test\ntest.drop(drop_features , axis=1 , inplace = True)","c1dbefab":"train.head()","b31d866a":"test.info()","63b72b1d":"# Fill NaN values in test\ntest['Age'].fillna(test['Age'].median() , inplace=True)\ntest['Fare'].fillna(test['Fare'].median() , inplace=True)\n\n# Check\ntest.info()","6889d019":"# Create a function to Map all values\n\ndef map_all(frame):\n    # Map Sex\n    frame['Sex'] = frame.Sex.map({'female': 0 ,  'male': 1}).astype(int)\n    \n    # Map Embarked\n    frame['Embarked'] = frame.Embarked.map({'S' : 0 , 'C': 1 , 'Q':2}).astype(int)\n    \n    # Map Age\n    # Age varies from 0.42 to 80, therefore 5 categories map to range of 16.\n    frame.loc[frame.Age <= 16 , 'Age'] = 0\n    frame.loc[(frame.Age >16) & (frame.Age<=32) , 'Age'] = 1\n    frame.loc[(frame.Age >32) & (frame.Age<=48) , 'Age'] = 2\n    frame.loc[(frame.Age >48) & (frame.Age<=64) , 'Age'] = 3\n    frame.loc[(frame.Age >64) & (frame.Age<=80) , 'Age'] = 4\n    \n    # Map Fare\n    # Fare varies from 0 to 512 with and we will map it depending upon the quartile variation.\n    # Look at train.describe() above, you will see 25% -> 7.91 , 50% -> 14.453 , 75% -> 31\n    frame.loc[(frame.Fare <= 7.91) , 'Fare'] = 0\n    frame.loc[(frame.Fare > 7.91) & (frame.Fare <= 14.454) , 'Fare'] = 1\n    frame.loc[(frame.Fare > 14.454) & (frame.Fare <= 31) , 'Fare'] = 2\n    frame.loc[(frame.Fare > 31) , 'Fare'] = 3","94314dad":"train.head()","688bc675":"map_all(train)\ntrain.head()","3a01fdeb":"# Similarly for test\nmap_all(test)\ntest.head()","449504b5":"x_train,x_test,y_train,y_test=train_test_split(train.drop('Survived',axis=1),train.Survived,test_size=0.20,random_state=66)","be4648ec":"models = [LogisticRegression(),LinearSVC(),SVC(kernel='rbf'),KNeighborsClassifier(),RandomForestClassifier(),\n        DecisionTreeClassifier(),GradientBoostingClassifier(),GaussianNB() , LinearDiscriminantAnalysis() , \n        QuadraticDiscriminantAnalysis()]\n\nmodel_names=['LogisticRegression','LinearSVM','rbfSVM','KNearestNeighbors','RandomForestClassifier','DecisionTree',\n             'GradientBoostingClassifier','GaussianNB', 'LinearDiscriminantAnalysis','QuadraticDiscriminantAnalysis']\n\naccuracy = []\n\nfor model in range(len(models)):\n    clf = models[model]\n    clf.fit(x_train,y_train)\n    pred = clf.predict(x_test)\n    accuracy.append(accuracy_score(pred , y_test))\n    \ncompare = pd.DataFrame({'Algorithm' : model_names , 'Accuracy' : accuracy})\ncompare","01450301":"    sns.factorplot(x='Accuracy',y='Algorithm' , data=compare , kind='point' , size=3 , aspect = 3.5)","4908e5d3":"params_dict={'criterion':['gini','entropy'],'max_depth':[5.21,5.22,5.23,5.24,5.25,5.26,5.27,5.28,5.29,5.3]}\nclf_dt=GridSearchCV(estimator=DecisionTreeClassifier(),param_grid=params_dict,scoring='accuracy', cv=5)\nclf_dt.fit(x_train,y_train)\npred=clf_dt.predict(x_test)\nprint(accuracy_score(pred,y_test))\nprint(clf_dt.best_params_)","e3ce1516":"#now lets try KNN.\n#lets try to tune n_neighbors. the default value is 5. so let us vary from say 1 to 50.\nno_of_test=[i+1 for i in range(50)]\n#no_of_test\nparams_dict={'n_neighbors':no_of_test}\nclf_knn=GridSearchCV(estimator=KNeighborsClassifier(),param_grid=params_dict,scoring='accuracy')\nclf_knn.fit(x_train,y_train)\npred=clf_knn.predict(x_test)\nprint(accuracy_score(pred,y_test))\nprint(clf_knn.best_params_)","18ac1038":"pred = clf_dt.predict(test)\n\nd = {'PassengerId' : test_df.PassengerId , 'Survived' : pred}\nanswer = pd.DataFrame(d)\nanswer.to_csv('Prediction.csv' , index=False)","848a9aec":" ### Let's Start With Importing Required Libraries -->","bfb23ecb":"* **Cabin contains a lot of Null values, so I'm going to drop it.**\n* **Names, PassengerId and Ticket Number doesn't help in finding Probability of Survival, so I'll be Dropping them too.**\n* **Also, we have created Alone feature and therefore I'll be Dropping SibSp and Parch too..**","815e5a76":"* Let's create a New Attribute __'Alone'__ , which would be True, if he\/she is travelling Alone.","d90c12b5":"## Convert the Categorical Variables into Numeric\n\nThis is Done because Modelling Algos cannot process Categorical String Variables.\n\n* Sex Attribute has (Male\/Female) , which will be mapped to 0\/1.\n* Divide Age into 5 categories and Map them with 0\/1\/2\/3\/4.\n* Divide Fare into 4 categories and Map them to 0\/1\/2\/3.\n* Embarked Attribute has (S\/C\/Q) , which will be mapped to 0\/1\/2.\n* Alone Attribute is already mapped.\n* Pclass Attribute is already mapped.\n","9b7e5a32":"**It seems that there individuals having 1 or 2 siblings\/spouses had the highest Probability of Survival, followed by individuals who were Alone.**","70ed462d":"## Let's try to tune Parameters :","d0e84914":"**Remember! Age has Null values , so we need to fill it before proceeding further.**","67f2c60e":"So, We have 891 rows and 12 columns","5879d139":"It seems that **Female Survival Probability (74%)** is almost *thrice* that of **Men (18%)** . Or we can say Females were more likely to Survive.","00b20e20":"## SibSp ->\nLet's Examine Survival on the basis of number of Siblings.","c3143047":"# Features\n* PassengerId : The id given to each traveller on the boat.\n* Pclass : The Passenger class. It has three possible values: 1,2,3 (first, second and third class).\n* Name : The Name of the passeger.\n* Sex : The Gender of the Passenger.\n* Age : The Age of the Passenger.\n* SibSp : The number of siblings and spouses traveling with the passenger.\n* Parch : number of parents and children traveling with the passenger.\n* Ticket : The ticket number of the Passenger.\n* Fare : The ticket Fare of the passenger\n* Cabin : The cabin number.\n* Embarked : This describe three areas of the Titanic from which the people embark. Three possible values S,C,Q (Southampton,     Cherbourg, Queenstown).\n\n\n*Qualitative Features (Categorical) : PassengerId , Pclass , Survived , Sex , Ticket , Cabin , Embarked.*\n\n*Quantitative Features (Numerical) : SibSp , Parch , Age , Fare.*\n\n### Survival is the Target Variable.","7785daf4":"# Examine Features :","feebcdd3":"## Embarked ->\nLet's examine Survival based on point of embarkation.","e94ed5e7":"#### We see that Age, Cabin and Embarked have Null values.","79532ad7":"**Fill the Age with it's Median, and that is because, for a dataset with great Outliers, it is advisable to fill the Null values with median.**","43de3c9d":"**We can see that those who paid high were likely to Survive.**","bddf4f40":"We can see that Males have **Surviving Density** less than Females (Bulged) . And majority of those who survived belonged to the category of Age limit 20-30 . Same is true for the Death Scenario.","c7a57df1":"So, there is *62.96%* Survival chance for **1st Class**. This clearly shows us that **First Class People were given priority first**.","a889dc18":"**It seems that individuals with  1,2 or 3 family members had a greater Probability of Survival, followed by individuals who were Alone.**","d8a76e62":"## Pclass ->\nLet's examine Survival based on Pclass.","e18108bf":"**We can clearly see that 3rd Class People were alloted the least priority and they died in large numbers.**","e5edf096":"# Titanic Survival In-depth Analysis.    [ Accuracy: 0.85 ]","87967871":"* **Looking at Parch and SibSp, we can see that individuals having Family Members had a slightly greater chance of Survival.**","d814e71d":"### We have Explored all the Important features , now let's proceed to Feature Engineering.","006668ed":"### Let's look at Test data","61f62b78":"# Now, Let's Apply Models :","53221ca2":"**We have a few Null values in Test (Age , Fare) , let's fill it up.**","bea27227":"## Well, DecisionTree did a great job there, with the highest accuracy [85%] .","e8cf45fb":"# Feature Engineering :","5e82a2ce":"* You can also view the Notebook on the link below.\n*  Github  Link -> **https:\/\/github.com\/Chinmayrane16\/Titanic-Survival-In-Depth-Analysis\/blob\/master\/Titanic-Survival.ipynb**\n* Do Upvote if you like it : )","5a6a929a":"**We can see that those who embarked at C with First Class ticket had a good chance of Survival.\nWhereas for S, it seems that all classes had nearly equal probability of Survival.\nAnd for Q, third Class seems to have Survived and Died with similar probabilities.**","e5256980":"## Fare ->\nlet's examine Survival on the basis of Fare.","51ef93a4":"**Let's look at all the columns and examine Null Values**","003b284a":"\n\n# THANK YOU :)","c2628632":"## Sex ->\nLet's examine Survival based on Gender","ab52d255":"## Parch ->\nExamine Survival on basis of number of Parents\/Children.","ee001159":"## Generate CSV file based on DecisionTree Classifier.","69a5a430":"## Age ->\nLet's examine Survival bassed on Age.","f3c6cb6d":"**Remember! Embarked has Null values, The best way to fill it would be by most occured value (Mode) .**","c42dc678":"**We can see that the plot has peak close to 30. So, we can infer that majority of people on Titanic had Age close to 30.**","98b60c33":"# Extract Train and Test Data\n* Specify the location to the dataset and import them","27a906d9":"### So, out of 891 examples only 342 (38%) survived and rest all died."}}