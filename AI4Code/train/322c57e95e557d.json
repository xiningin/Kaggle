{"cell_type":{"10d174ea":"code","cd2e662c":"code","9fbc872e":"code","86780458":"code","ac9794e2":"code","2bbb3110":"code","947cc758":"code","5d7e2f28":"code","9bec1220":"code","737324d5":"code","fa7941d6":"code","35deedcb":"code","6d80b108":"code","d4fba006":"code","0c4a6213":"code","3d36335b":"code","b33ea9fb":"code","1b2ee8c1":"code","834d8248":"code","f93f3da6":"code","c4415159":"markdown","9e9f6be3":"markdown","a364e5fa":"markdown","3e3e52f5":"markdown","ac510aa4":"markdown","4e8e710d":"markdown"},"source":{"10d174ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler , LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import mode\n\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras.utils import to_categorical\n\nfrom matplotlib import ticker\nimport time\nimport warnings\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cd2e662c":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/test.csv')","9fbc872e":"train.head()","86780458":"print(f'Number of rows in train data: {train.shape[0]}')\nprint(f'Number of columns in train data: {train.shape[1]}')\nprint(f'No of missing values in train data: {sum(train.isna().sum())}')","ac9794e2":"train.describe()","2bbb3110":"test.head()","947cc758":"print(f'Number of rows in test data: {test.shape[0]}')\nprint(f'Number of columns in test data: {test.shape[1]}')\nprint(f'No of missing values in test data: {sum(test.isna().sum())}')","5d7e2f28":"train.drop([\"Id\"] , axis = 1 , inplace = True)\ntest.drop([\"Id\"] , axis = 1 , inplace = True)","9bec1220":"TARGET = 'Cover_Type'\nFEATURES = [col for col in train.columns if col not in ['Id', TARGET]]\nRANDOM_STATE = 12 ","737324d5":"target_df = pd.DataFrame(train[TARGET].value_counts()).reset_index()\ntarget_df.columns = [TARGET, 'count']\nfig = px.bar(data_frame =target_df, \n             x = 'Cover_Type',\n             y = 'count' , \n             color = \"count\",\n             color_continuous_scale=\"sunset\") \nfig.show()\ntarget_df.sort_values(by =TARGET , ignore_index = True)","fa7941d6":"train = train.drop(index = int(np.where(train[\"Cover_Type\"] == 5 )[0]))\ntrain = train.drop(labels = [\"Soil_Type7\" , \"Soil_Type15\"] ,axis = 1)\nFEATURES.remove('Soil_Type7')\nFEATURES.remove('Soil_Type15')","35deedcb":"train[\"mean\"] = train[FEATURES].mean(axis=1)\ntrain[\"std\"] = train[FEATURES].std(axis=1)\ntrain[\"min\"] = train[FEATURES].min(axis=1)\ntrain[\"max\"] = train[FEATURES].max(axis=1)\n\ntest[\"mean\"] = test[FEATURES].mean(axis=1)\ntest[\"std\"] = test[FEATURES].std(axis=1)\ntest[\"min\"] = test[FEATURES].min(axis=1)\ntest[\"max\"] = test[FEATURES].max(axis=1)\n\nFEATURES.extend(['mean', 'std', 'min', 'max'])","6d80b108":"scaler = StandardScaler()\nfor col in FEATURES:\n    train[col] = scaler.fit_transform(train[col].to_numpy().reshape(-1,1))\n    test[col] = scaler.transform(test[col].to_numpy().reshape(-1,1))\n    \nX = train[FEATURES].to_numpy().astype(np.float32)\ny = train[TARGET].to_numpy().astype(np.float32)\nX_test = test[FEATURES].to_numpy().astype(np.float32)\n\ndel train, test","d4fba006":"catb_params = {\n    \"objective\": \"MultiClass\",\n    \"task_type\": \"GPU\",\n}\n\ncatb_predictions = []\ncatb_scores = []\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X = X, y = y)):\n\n    print(10*\"=\", f\"Fold={fold+1}\", 10*\"=\")\n    start_time = time.time()\n    x_train = X[train_idx, :]\n    x_valid = X[valid_idx, :]\n    y_train = y[train_idx]\n    y_valid = y[valid_idx]\n    \n    model = CatBoostClassifier(**catb_params)\n    model.fit(x_train, y_train,\n          early_stopping_rounds=200,\n          eval_set=[(x_valid, y_valid)],\n          verbose=0)\n    \n    preds_valid = model.predict(x_valid)\n    acc = accuracy_score(y_valid,  preds_valid)\n    catb_scores.append(acc)\n    run_time = time.time() - start_time\n    print(f\"Fold={fold+1}, acc: {acc:.8f}, Run Time: {run_time:.2f}\")\n    test_preds = model.predict(X_test)\n    catb_predictions.append(test_preds)\n    \nprint(\"Mean Accuracy:\", np.mean(catb_scores))","0c4a6213":"xgb_params = {'objective': 'multi:softmax',\n              'eval_metric': 'mlogloss',\n              'tree_method': 'gpu_hist',\n              'predictor': 'gpu_predictor',\n             }\n\nxgb_predictions = []\nxgb_scores = []\n\nxgb_predictions = []\nxgb_scores = []\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X = X, y = y)):\n\n    print(10*\"=\", f\"Fold={fold+1}\", 10*\"=\")\n    start_time = time.time()\n    x_train = X[train_idx, :]\n    x_valid = X[valid_idx, :]\n    y_train = y[train_idx]\n    y_valid = y[valid_idx]\n    \n    model = XGBClassifier(**xgb_params)\n    model.fit(x_train, y_train,\n          early_stopping_rounds=200,\n          eval_set=[(x_valid, y_valid)],\n          verbose=0)\n    preds_valid = model.predict(x_valid)\n    acc = accuracy_score(y_valid,  preds_valid)\n    xgb_scores.append(acc)\n    run_time = time.time() - start_time\n    print(f\"Fold={fold+1}, acc: {acc:.8f}, Run Time: {run_time:.2f}\")\n    test_preds = model.predict(X_test)\n    xgb_predictions.append(test_preds)\n    \nprint(\"Mean Accuracy:\", np.mean(xgb_scores))","3d36335b":"LEARNING_RATE = 0.0001\nBATCH_SIZE = 2048\nEPOCHS = 100\nVALIDATION_RATIO = 0.05\n\nLE = LabelEncoder()\ny = to_categorical(LE.fit_transform(y))\nX_train , X_valid ,y_train ,y_valid  = train_test_split(X,y , test_size = VALIDATION_RATIO , random_state=RANDOM_STATE)\n\n\ndef load_model(): \n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(2048, activation = 'swish', input_shape = [X.shape[1]]),\n        tf.keras.layers.Dense(1024, activation ='swish'),\n        tf.keras.layers.Dense(512, activation ='swish'),\n        tf.keras.layers.Dense(6, activation='softmax'),\n    ])\n    model.compile(\n        optimizer= tf.keras.optimizers.Adam(learning_rate = LEARNING_RATE),\n        loss='categorical_crossentropy',\n        metrics=['acc'],\n    )\n    return model\n    \n    \nearly_stopping = callbacks.EarlyStopping(\n        patience=10,\n        min_delta=0,\n        monitor='val_loss',\n        restore_best_weights=True,\n        verbose=0,\n        mode='min', \n        baseline=None,\n    )\nplateau = callbacks.ReduceLROnPlateau(\n            monitor='val_loss', \n            factor=0.2, \n            patience=4, \n            verbose=0,\n            mode='min')\n\nnn_model = load_model()\nhistory = nn_model.fit(  X_train , y_train,\n                validation_data = (X_valid , y_valid),\n                batch_size = BATCH_SIZE, \n                epochs = EPOCHS,\n                callbacks = [early_stopping , plateau],\n              )\nnn_preds = nn_model.predict(X_test , batch_size=BATCH_SIZE)","b33ea9fb":"submission = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")","1b2ee8c1":"catb_submission = submission.copy()\ncatb_submission['Cover_Type'] = np.squeeze(mode(np.column_stack(catb_predictions),axis = 1)[0]).astype('int')\ncatb_submission.to_csv(\"catboost.csv\",index=None)\ncatb_submission.head()","834d8248":"xgb_submission = submission.copy()\nxgb_submission['Cover_Type'] = np.squeeze(mode(np.column_stack(xgb_predictions),axis = 1)[0]).astype('int')\nxgb_submission.to_csv(\"xgboost.csv\",index=None)\nxgb_submission.head()","f93f3da6":"nn_submission = submission.copy()\nnn_submission[\"Cover_Type\"] = LE.inverse_transform(np.argmax((nn_preds), axis=1)).astype(int)\nnn_submission.to_csv(\"nnet.csv\" , index= False)\nnn_submission.head()","c4415159":"# Modeling","9e9f6be3":"# Neural Network","a364e5fa":"# Import Libraries","3e3e52f5":"# Feature Engineering","ac510aa4":"# Xgboost Classifier","4e8e710d":"# Catboost Classifier"}}