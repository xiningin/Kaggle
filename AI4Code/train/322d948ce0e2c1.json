{"cell_type":{"5b19dbc7":"code","feef4d1b":"code","397787ae":"code","d933d1ac":"code","0cfaf94e":"code","4664e703":"code","87bd4afa":"code","09cb0c78":"code","91468baf":"code","57e67669":"code","9419c070":"code","957a3800":"code","6baace09":"code","c57e9d09":"code","22451dc3":"code","59bdd508":"code","0820d2c0":"code","1caa78b5":"code","b50ac425":"code","4d45aeaf":"code","0e8accde":"code","bd4b9d6b":"code","048ad9b0":"code","1c48d045":"code","13690151":"code","37019314":"code","10ab7ff7":"code","dc47ff74":"code","1f4f8b9f":"code","edcfd4a4":"code","ef91a06d":"code","17062a27":"code","8b1383f8":"code","ef609dd3":"code","f9aa425f":"code","c4258c41":"code","55485c1c":"code","9b41a3f3":"markdown"},"source":{"5b19dbc7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","feef4d1b":"train=pd.read_csv('..\/input\/train.csv')\ntest=pd.read_csv('..\/input\/test.csv')\nsample=pd.read_csv('..\/input\/sample_submission.csv')","397787ae":"comb=train.append(test,ignore_index=True)","d933d1ac":"comb.shape","0cfaf94e":"train['length']=train['tweet'].apply(len)","4664e703":"train.head()","87bd4afa":"train.describe()","09cb0c78":"train[train['length']==274]['tweet'].iloc[0]","91468baf":"train['length'].plot(bins=50,kind='hist')","57e67669":"train['tweet'].iloc[0]\ntrain.head()","9419c070":"import re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport nltk\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n","957a3800":"def remove_pattern(input_txt,pattern):\n    r=re.findall(pattern,input_txt)\n    for i in r:\n        input_txt = re.sub(i,'',input_txt)\n    return input_txt","6baace09":"comb['tidy_tweet'] = np.vectorize(remove_pattern)(comb['tweet'],\"@[\\w]*\")","c57e9d09":"comb.head()","22451dc3":"comb['tidy_tweet']=comb['tidy_tweet'].str.replace(\"[^a-zA-Z#]\",\" \")","59bdd508":"comb.head()","0820d2c0":"comb['tidy_tweet'] = comb['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))","1caa78b5":"comb.head()","b50ac425":"#bow_transformer = CountVectorizer(analyzer=text_process).fit(train['tweet'])","4d45aeaf":"tokenized_tweet = comb['tidy_tweet'].apply(lambda x:x.split())","0e8accde":"tokenized_tweet.head()","bd4b9d6b":"from nltk.stem.porter import *\nstemmer = PorterStemmer()\ntokenized_tweet = tokenized_tweet.apply(lambda x:[stemmer.stem(i) for i in x])","048ad9b0":"tokenized_tweet.head()","1c48d045":"for i in range(len(tokenized_tweet)):\n    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\ncomb['tidy_tweet'] = tokenized_tweet","13690151":"comb.head()","37019314":"all_words = ' '.join([text for text in comb['tidy_tweet']])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800,height=500,random_state=21,max_font_size=110).generate(all_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud,interpolation =\"bilinear\")\nplt.axis('off')\nplt.show()","10ab7ff7":"normal_words = ' '.join([text for text in comb['tidy_tweet'][comb['label'] ==0]])\nwordcloud=WordCloud(width=800,height=500,random_state=21,max_font_size=110).generate(normal_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud,interpolation='bilinear')\nplt.axis('off')\nplt.show()","dc47ff74":"racist_words = ' '.join([text for text in comb['tidy_tweet'][comb['label']==1]])\nwordcloud=WordCloud(width=800,height=500,random_state=21,max_font_size = 110).generate(racist_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud,interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","1f4f8b9f":"def hashtag_extract(x):\n    hashtag = []\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\",i)\n        hashtag.append(ht)\n        \n    return hashtag","edcfd4a4":"ht_regular = hashtag_extract(comb['tidy_tweet'][comb['label']==0])\nht_negative = hashtag_extract(comb['tidy_tweet'][comb['label']==1])\nht_regular = sum(ht_regular,[])\nht_negative = sum(ht_negative,[])\n#print(ht_regular)","ef91a06d":"a = nltk.FreqDist(ht_regular)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 10 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","17062a27":"a = nltk.FreqDist(ht_negative)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 10 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","8b1383f8":"from sklearn.feature_extraction.text import CountVectorizer\nbow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\nbow = bow_vectorizer.fit_transform(comb['tidy_tweet'])","ef609dd3":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\ntfidf = tfidf_vectorizer.fit_transform(comb['tidy_tweet'])","f9aa425f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom xgboost import XGBRegressor\n\ntrain_bow = bow[:31962,:]\ntest_bow = bow[31962:,:]\n\n\nxtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'], random_state=42, test_size=0.3)\n\nlreg = LogisticRegression()\n#lreg.fit(xtrain_bow, ytrain) \n\n#prediction = lreg.predict_proba(xvalid_bow) \n#prediction_int = prediction[:,1] >= 0.3  \n#prediction_int = prediction_int.astype(np.int)\n\n#f1_score(yvalid, prediction_int)","c4258c41":"train_tfidf=tfidf[:31962,:]\ntest_tfidf=tfidf[31962:,:]\nxtrain_tfidf = train_tfidf[ytrain.index]\nxvalid_tfidf = train_tfidf[yvalid.index]\nlreg.fit(xtrain_tfidf,ytrain)\nprediction = lreg.predict_proba(xvalid_tfidf)\nprediction_int = prediction[:,1] >=0.3\nprediction_int = prediction_int.astype(np.int)\nf1_score(yvalid,prediction_int)","55485c1c":"test_pred= lreg.predict_proba(test_tfidf)\ntest_pred_int=test_pred[:,1] >=0.3\ntest_pred_int = test_pred_int.astype(np.int)\ntest['label']=test_pred_int\nsubmission = test[['id','label']]\nsubmission.to_csv('123.csv',index=False)","9b41a3f3":"This notebook contain Sentiment analysis of tweets on a Analtics vidya [Practice problem](https:\/\/datahack.analyticsvidhya.com\/contest\/practice-problem-twitter-sentiment-analysis\/).This is basically a baseline kernel for that problem and if you want more information about the competititon you can visit the link and participate in that competition."}}