{"cell_type":{"39056fd4":"code","13aa9104":"code","f7272676":"code","3255ff4f":"code","8c5a94bf":"code","35ce0320":"code","b9670791":"code","f739285a":"code","534f2819":"code","65378ffa":"code","27dcdf7a":"code","db32e153":"markdown","75fdeade":"markdown","7a1b6069":"markdown"},"source":{"39056fd4":"# LSTM for sequence classification in the IMDB dataset\nimport numpy\nfrom keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence","13aa9104":"# load the dataset but only keep the top n words, zero the rest\ntop_words = 5000\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\nX_train","f7272676":"# truncate and pad input sequences\nmax_review_length = 500\nX_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\nX_test = sequence.pad_sequences(X_test, maxlen=max_review_length)","3255ff4f":"X_train.shape","8c5a94bf":"print(X_train[1])\nprint(type(X_train[1]))\nprint(len(X_train[1]))","35ce0320":"# create the model\nembedding_vecor_length = 64\nmodel = Sequential()\nmodel.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\nmodel.add(LSTM(30, dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","b9670791":"model.summary()","f739285a":"model.fit(X_train, y_train, epochs=10, batch_size=32,validation_data=(X_test, y_test))","534f2819":"# Final evaluation of the model\nscores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","65378ffa":"# LSTM Model with dropout Another Parameter\n\nfrom keras.layers import Dropout\nmodel = Sequential()\nmodel.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","27dcdf7a":"model.fit(X_train, y_train, epochs=3, batch_size=64)\n# Final evaluation of the model\nscores = model.evaluate(X_test, y_test, verbose=0)\nprint(scores)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","db32e153":"# The IMDB movie review sentiment classification problem. Each movie review is a variable sequence of words and the sentiment of each movie review must be classified. The Large Movie Review Dataset (often referred to as the IMDB dataset) contains 25,000 highly-polar movie reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given movie review has a positive or negative sentiment. Use LSTM for sequence classification in the IMDB dataset. Perform the following.\n- a) Load dataset but keep only top 5000 words. \n- b) Build LSTM model.\n- c) Train the network and perform hyper parameter tuning.\n- d) Evaluate the model.\n","75fdeade":"# Model Description\n- The first layer is the Embedded layer that uses 64 length vectors to represent each word. The next layer is the LSTM layer with 30 memory units (smart neurons)and dropout layer(0.2). Finally, because this is a classification problem we use a Dense output layer with a single neuron and a sigmoid activation function to make 0 or 1 predictions for the two classes (good and bad) in the problem.\n- Because it is a binary classification problem, log loss is used as the loss function (binary_crossentropy in Keras). The efficient ADAM optimization algorithm is used. The model is fit for only 2 epochs because it quickly overfits the problem. A large batch size of 64 reviews is used to space out weight updates.","7a1b6069":"We can now define, compile and fit our LSTM model.\n\nThe first layer is the Embedded layer that uses 32 length vectors to represent each word. The next layer is the LSTM layer with 100 memory units (smart neurons). Finally, because this is a classification problem we use a Dense output layer with a single neuron and a sigmoid activation function to make 0 or 1 predictions for the two classes (good and bad) in the problem.\n\nBecause it is a binary classification problem, log loss is used as the loss function (binary_crossentropy in Keras). The efficient ADAM optimization algorithm is used. The model is fit for only 2 epochs because it quickly overfits the problem. A large batch size of 64 reviews is used to space out weight updates."}}