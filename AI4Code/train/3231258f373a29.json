{"cell_type":{"5e5f94e2":"code","b7924cb8":"code","02905ae6":"code","3be39131":"code","8a99d406":"code","88b3af54":"code","83ca5f4a":"code","4ad4517c":"code","5cd73692":"code","cb423ab0":"code","a6644bff":"code","c5060284":"code","ab82dbb9":"code","1c83e957":"code","f2e3988c":"code","75d1a9f0":"code","d2f6638a":"code","51910594":"markdown","3d008029":"markdown","1ec0ec53":"markdown","045f5584":"markdown","a16af105":"markdown","d9b34889":"markdown","c4296ace":"markdown","e9529784":"markdown","79e6b047":"markdown","3bac13be":"markdown","4355049c":"markdown","10c3d545":"markdown","6f01db8b":"markdown"},"source":{"5e5f94e2":"import pandas as pd              # pandas es ideal para administrar conjuntos de datos como un solo objeto\nimport numpy as np               # numpy tiene excelentes operaciones de matriz \/ matem\u00e1ticas en matrices\n\nimport matplotlib.pyplot as plt  # figuras y gr\u00e1ficos\nimport seaborn as sns            # un complemento elegante para matplotlib\n\n# Este comando crea las figuras y gr\u00e1ficos dentro del notebook Jupyter\n%matplotlib inline     ","b7924cb8":"'''\nEsto llama a todas las funciones de preprocesamiento.\nEl preprocesamiento debe hacerse exactamente igual en los conjuntos de datos de entrenamiento y prueba.\n'''\n\ndef preprocessData(df):\n    \n    # Convierta el campo de embarked de categ\u00f3rico (\"S\", \"C\", \"Q\")\n    # para numerico (0,1,2)\n    df = convertEmbarked(df)\n    \n    # Convierta sex. Female = 0, Male = 1\n    df = convertSex(df)\n\n    df = addFamilyFactor(df)\n    \n    df = addTitles(df)\n\n    # Remover columnas irrelevantes y no numericas (features)\n    df = df.drop(['Name', 'Cabin', 'PassengerId', 'Ticket'], axis=1) \n\n    # Reemplazar valores faltantes (NaN) with the mean value for that field\n    df = replaceWithMean(df)\n\n    return df","02905ae6":"'''\nconvierte el campo sex a numerico\n'''\ndef convertSex(df):\n    \n    \n    # Cree una nueva columna llamada 'Gender' que es un map de la columna \"Sex\" en valores enteros \n    '''escriba su codigo'''\n    # Ahora elimine la columna \"sex\" ya que la hemos reemplazado por la columna 'gender'\n    '''escriba su codigo'''\n    \n    return df\n    ","3be39131":"'''\nScikit-learn solo puede manejar n\u00fameros.\nAs\u00ed que reemplacemos los valores de texto para la columna 'Embarked' con n\u00fameros. \nPor ejemplo, el puerto de embarque etiquetado 'S' es 0, 'C' es 1, y 'Q' es 2.\n'''\ndef convertEmbarked(df):\n    \n    if ('Embarked' in df.columns) :  # \n        \n        # valor faltante, llene na con el valor m\u00e1s frecuente \n        if (len(df[df[\"Embarked\"].isnull()]) > 0):\n\n            # Necesitamos deshacernos de los valores faltantes\n            # http:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.mode.html\n            # Si desea imputar valores faltantes con la moda en un dataframe de datos df, puede hacerlo:\n            df.loc[df[\"Embarked\"].isnull(), 'Embarked'] = df[\"Embarked\"].dropna().mode().iloc[0]\n\n        ports = list(enumerate(np.unique(df[\"Embarked\"])))  # Obtenemos la lista ID de puertos \u00fanicos\n        port_dict = { name: i for i, name in ports } # Cree un diccionario de las diferentes ID de puerto\n        df[\"Embarked\"] = df[\"Embarked\"].map( lambda x: port_dict[x]).astype(int)  # Reasignar los ID de puerto a n\u00fameros\n        \n    return df","8a99d406":"def addTitles(df):\n    \n    # extraemos el t\u00edtulo de cada nombre \n    combined = df['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n    \n    # construimos un mapa de t\u00edtulos \n    Title_Dictionary = {\n                        \"Capt\":       1,\n                        \"Col\":        1,\n                        \"Major\":      1,\n                        \"Jonkheer\":   3,\n                        \"Don\":        3,\n                        \"Sir\" :       3,\n                        \"Dr\":         2,\n                        \"Rev\":        1,\n                        \"the Countess\":3,\n                        \"Dona\":       3,\n                        \"Mme\":        0,\n                        \"Mlle\":       0,\n                        \"Ms\":         0,\n                        \"Mr\" :        0,\n                        \"Mrs\" :       0,\n                        \"Miss\" :      0,\n                        \"Master\" :    1,\n                        \"Lady\" :      3\n\n                        }\n    \n    # mapeamos cada t\u00edtulo\n    df['Title'] = combined.map(Title_Dictionary)\n    \n    return df","88b3af54":"'''\nReemplace los valores faltantes (NaN) con el valor medio para esa columna\n'''\ndef replaceWithMean(df):\n    \n    '''escriba su codigo'''\n    return df","83ca5f4a":"'''\nexisten 2 variables de tipo \"family\" en el dataset. podemos combinarlas en una variable.\n'''\ndef addFamilyFactor(df):\n    # Agregar una categor\u00eda llamada FamilyFactor\n    # \u00bfQuiz\u00e1s las personas con familias m\u00e1s grandes ten\u00edan una mayor probabilidad de rescate?\n    # Si solo agrego los dos juntos, entonces la nueva categor\u00eda es solo una transformaci\u00f3n lineal y\n    # realmente no agregar\u00e1 nueva informaci\u00f3n. Entonces agrego y luego elevo al cuadrado el valor. \n    '''escriba su codigo'''\n    \n    return df","4ad4517c":"'''\n\nLea los datos de entrenamiento del archivo csv path='..\/input\/train.csv'\n\n'''\ntrain_df = '''escriba su codigo'''\n\n# Obtenga la informaci\u00f3n b\u00e1sica para los datos en este archivo\ntrain_df.info()","5cd73692":"# Configure un grafico con 3 subgraficos una al lado de la otra \nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(15,5))\n\n# Cuente cu\u00e1ntas personas se embarcaron en cada ubicaci\u00f3n\n# countplot es para categorial data, barplot para data cuantitativa \nsns.countplot(x=\"Embarked\", data=train_df, ax=axis1)\naxis1.set_title(\"# pasajeros por sitio de embarque\")\n\n# Comparaci\u00f3n de sobrevivientes versus muertes en funci\u00f3n del embarque \n'''escriba su codigo'''\naxis2.set_title(\"Sobrevivientes versus muertes\")\n\n# agrupar por embarked, y obtenga la media de los pasajeros sobrevivientes para cada valor en  Embarked\nembark_perc = train_df[[\"Embarked\", \"Survived\"]].groupby(['Embarked'],as_index=False).mean()\nsns.barplot(x='Embarked', y='Survived', data=embark_perc,order=['S','C','Q'],ax=axis3)\naxis3.set_title(\"Supervivencia versus embarque\")","cb423ab0":"train_df = preprocessData(train_df)","a6644bff":"train_df.describe()","c5060284":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\n\n# Los datos ya est\u00e1n listos para funcionar. As\u00ed que vamos al entrenamiento\n# \ntrain_data = train_df.values\ntrain_features = train_data[0::,1::]   # los features a usar por el modelo de prediccion (e.g. age, family size)\ntrain_result = train_data[0::,0]       # Lo que predice el modelo (i.e. survived)\n\nprint('Entrenamiento con el modelo. Por favor espera ...')\n\n# Adaboost usando un mont\u00f3n de modelos RandomForest\n#  \n# Zhu, H. Zou, S. Rosset, T. Hastie, \u201cMulti-class AdaBoost\u201d, 2009.\n# para mas informacion, http:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_adaboost_multiclass.html\nmodel = AdaBoostClassifier(RandomForestClassifier(n_estimators = 1000),\n                         algorithm=\"SAMME\",\n                         n_estimators=500)\n\n\n# Ajustar los datos de entrenamiento al modelo Adaboost\nmodel = model.fit(train_features, train_result)\n\nprint ('Okay. Termin\u00e9 de entrenar al modelo.')","ab82dbb9":"from sklearn.metrics import accuracy_score\n\nprint ('Accuracy = {:0.2f}%'.format(100.0 * accuracy_score(train_result, model.predict(train_features))))","1c83e957":"from sklearn.model_selection import cross_val_score\n\n# C\u00e1lculo de cross validation del modelo de entrenamiento\nprint ('C\u00e1lculo de cross validation del modelo de entrenamiento. Por favor espera ...')\n\n# Cross-validation con k-fold de 5. Por lo tanto, esto dividir\u00e1 aleatoriamente los datos de entrenamiento en dos conjuntos.\n# Luego ajusta un modelo a un conjunto y lo prueba contra el otro para obtener una precisi\u00f3n.\n# Lo har\u00e1 5 veces y devolver\u00e1 la precisi\u00f3n promedio.\nscores = cross_val_score(model, train_features, train_result, cv=5)\nprint ( 'En promedio, este modelo es correcto {:0.2f}% (+\/- {:0.2f}%) .'.format(\n        scores.mean() * 100.0, scores.std() * 2 * 100.0))","f2e3988c":"# Importe los datos de test en un dataframe Pandas \n'''escriba su codigo'''     # cargue los datos de test usando el path '..\/input\/test.csv'\n\ntest_df.info()","75d1a9f0":"# obtiene primero los ID de los pasajeros, ya que son eliminados por la funci\u00f3n de preprocesamiento\ntestIds = test_df['PassengerId']\n\ntest_df = preprocessData(test_df)","d2f6638a":"print('Predecir la supervivencia a partir de los datos de test. POR FAVOR ESPERA... ', end='') \n\ntest_predictions = model.predict(test_df.values)\n\nprint('FIN')","51910594":"### Pre-processing\n","3d008029":"#### Grados y Titulos\nEl campo `Names` de nuestros datos se incluye t\u00edtulos. Entonces, tal vez alguien como _Duff Gordon, Lady. (Lucille Christiana Sutherland) _ obtendr\u00e1 un bote salvavidas antes de _Dooley, Sr. Patrick_ . Por supuesto, eso es una suposici\u00f3n, pero probablemente sea bueno saber un poco sobre el tr\u00e1gico evento. Si ese es el caso, analicemos el campo `Names` para los t\u00edtulos y agreguemos una nueva columna llamada `T\u00edtles` que tiene un valor num\u00e9rico basado en la importancia que creemos tiene la persona.\n","1ec0ec53":"# Titanic Machine Learning\n### \n### \n#### Aqu\u00ed est\u00e1 el conjunto de datos de  Titanic de [Kaggle](https:\/\/www.kaggle.com\/c\/titanic). Usaremos Scikit-Learn's Adaptive Boost [adaboost](http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#adaboost).\n\n  \nEl dataset Titanic es un buen conjunto para comenzar a aprender data science. Tiene tipos de datos mixtos (num\u00e9ricos y no num\u00e9ricos). Le faltan valores y datos err\u00f3neos para manejar e incluso con las t\u00e9cnicas de an\u00e1lisis de datos m\u00e1s simples, puede obtener una precisi\u00f3n de predicci\u00f3n superior al 70% sin mucho esfuerzo.","045f5584":"### Cross-validation\nSu precisi\u00f3n muestra que obtuvo un modelo * realmente * buen, \u00bfverdad?\n\nBueno, no tan r\u00e1pido\n\nLa precisi\u00f3n es realmente enga\u00f1osa. Recuerde, est\u00e1 ajustando la curva a los datos. Espera un buen ajuste a esos datos. A veces, puede ajustar los datos tan bien que el modelo solo funciona realmente en ese conjunto de datos espec\u00edfico. Esto se llama _ overfitting_. Entonces, para probar realmente la verdadera precisi\u00f3n de su nuevo modelo, debe probar el modelo con datos nuevos (es decir, datos en los que no entren\u00f3 el modelo). \n\n[Cross validation](http:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html) le da una idea de qu\u00e9 tan bien el modelo predice nuevos datos. El m\u00f3dulo scikit-learn `model_selection` tiene una funci\u00f3n llamada` cross_val_score` que devuelve la precisi\u00f3n del modelo. Lo hace dividiendo aleatoriamente sus datos en dos conjuntos (tambi\u00e9n conocidos como k-folds). Se usar\u00e1 un conjunto para entrenar al modelo. El otro conjunto se usar\u00e1 para probar qu\u00e9 tan preciso es ese modelo. Esto nos ayuda a identificar el sobreajuste. Muchos modelos pueden ajustarse perfectamente al conjunto de datos durante el entrenamiento, pero deben ser tan espec\u00edficos para ese conjunto de datos que fallan cuando ven nuevos datos.","a16af105":"## Importamos librerias python \n\n\nPrimero, solo necesitamos importar las librerias de Python para cargar y preprocesar conjuntos de datos ([pandas](http:\/\/pandas.pydata.org\/), [numpy](http:\/\/numpy.org)) y graficar datos ([matplotlib](http:\/\/matplotlib.org) and [seaborn](https:\/\/pypi.python.org\/pypi\/seaborn)). Estrictamente hablando, no se necesita graficar para ejecutar el c\u00f3digo; sin embargo, un buen an\u00e1lisis exige que inspeccionemos visualmente los datos antes de ejecutarlos a trav\u00e9s de modelos. Cuanto mejor podamos preprocesar los datos (es decir, determinar posibles tendencias, identificar posibles variables significativas, eliminar \/ manejar los valores faltantes), mejor ser\u00e1 nuestro modelo.","d9b34889":"## Funciones de preprocesamiento\n\nEstas funciones se utilizan para preprocesar los datos (que se almacenan en un dataframe de Pandas, `df`). Seg\u00fan el [sitio web de Kaggle] (https:\/\/www.kaggle.com\/c\/titanic\/data), los datos se ven as\u00ed:\n\n| Variable Name  | Description                        | Type |\n| -------------- | ---------------------------------- | ---- |\n| survival       | Survival  (**Our model tries to predict this based on the other variables.**)                         | number  |\n|                | Values: (0 = No; 1 = Yes)                  | \n| pclass         | Passenger Class                    | number  |\n|                | Values: (1 = 1st; 2 = 2nd; 3 = 3rd)  |\n| name           | Name                               | string |\n| sex            | Sex                                | string |\n|                | Values: (female, male) | \n| age            | Age                                | number |\n| sibsp          | Number of Siblings\/Spouses Aboard  | number |\n| parch          | Number of Parents\/Children Aboard  | number |\n| ticket         | Ticket Number                      | string |\n| fare           | Passenger Fare                     | number |\n| cabin          | Cabin                              | string |\n|embarked        | Port of Embarkation                | character |\n|                | Values: (C = Cherbourg; Q = Queenstown; S = Southampton) |\n\nPor ejemplo, en una funci\u00f3n, reemplazamos los valores de texto para el campo 'Embarked' con n\u00fameros. Por ejemplo, el puerto de embarque etiquetado 'S' se convierte en 0, 'C' se convierte en 1 y 'Q' se convierte en 2.\nM\u00e1s adelante en este notebook, utilizaremos el [paquete scikit-learn] (http:\/\/scikit-learn.org) para crear modelos para los datos. Scikit-learn asume que sus datos son todos n\u00fameros-- * no puede leer texto *. Por lo tanto, debemos sustituir las asignaciones de n\u00fameros para los campos de texto. Por ejemplo, para el campo sex, podemos reemplazar los valores 0 para mujeres y 1 para hombres; o, podemos tener 34 para hombres y 123 para mujeres (o -7.8 para hombres y 999.123 para mujeres). El n\u00famero no importa siempre que sea consistente: los hombres obtienen un n\u00famero y las mujeres obtienen un n\u00famero diferente. Esto debe hacerse para un campo no num\u00e9rico.\n\nTambi\u00e9n tenemos funciones que reemplazan los datos faltantes (NaN) con el valor media o promedio dentro de ese campo de datos. A veces incluso eliminamos campos si tienen demasiados valores perdidos o si no es posible que est\u00e9n asociados con la predicci\u00f3n. Por ejemplo, el nombre de la persona probablemente no predice si sobrevivi\u00f3, pero su edad probablemente s\u00ed.","c4296ace":"# [AdaBoost (Adaptive Boost)](https:\/\/en.wikipedia.org\/wiki\/AdaBoost)\n##\n\nCreemos un clasificador AdaBoost para modelar los datos de entrenamiento. Adaboost es un meta-algoritmo para machine learning. Por lo tanto, no es un modelo de machine learning real, sino una forma de combinar modelos de machine learning. AdaBoost toma un conjunto de otros clasificadores  (por ejemplo, regresi\u00f3n, \u00e1rboles b, random forests, redes neuronales) y los combina de manera ponderada para crear un nuevo clasificador. Por lo tanto, es como tomar varios modelos predictivos d\u00e9biles y combinarlos para formar un supermodelo.\n\nPor ejemplo, supongamos que tenemos 5 redes neuronales que predicen una salida binaria (0 o 1). Entonces, cada una de las 5 redes neuronales produce una predicci\u00f3n para nosotros, pero cada una est\u00e1 configurada de manera diferente (diferentes capas, diferentes entradas, etc.). Pero, digamos que ninguna de estas redes neuronales son realmente buenos predictores. Adaboost es una forma de combinar las 5 redes neuronales en un solo modelo que ofrece una mejor predicci\u00f3n que cualquier modelo unitario (la suma es mayor que las partes).\n\nPara los clasificadores binarios, una tasa de error del 50% es como lanzar una moneda: es lo peor que puede hacer, ya que es completamente aleatorio (si es peor que el 50%, entonces debe adivinar lo contrario de lo que el modelo predice !). Cualquier modelo que tenga una tasa de error cercana al 50% (por ejemplo, 43%) se considera un modelo predictivo \"d\u00e9bil\" weak learner. Cualquier modelo que est\u00e9 cerca de un error de predicci\u00f3n del 0% (por ejemplo, 9%) se considera un modelo predictivo \"fuerte\".\n","e9529784":"### Use Seaborn (matplotlib wrapper) para mostrar imagenes de nuestros datos sin procesar\n\nlos datos sin procesar de la variable 'Embarked'. notese si embarked es 'C' (Cherbourg), se obtiene mas del 50% de chance sobrevivir mientras que los otros dos puertos tienen menos que el 40% de chance de sobrevivir. No es una gran correlaci\u00f3n, pero podr\u00eda ser \u00fatil cuando se combina con otras caracter\u00edsticas. Por lo tanto, el sitio de embarque podr\u00eda ser una caracter\u00edstica \u00fatil para usar en nuestro modelo de predicci\u00f3n.\n\n### (Practica )\n\nPuede probar este enfoque en las otras variables (por ejemplo, sex, age) para ver c\u00f3mo influyen en la tasa de supervivencia. Este es un m\u00e9todo para seleccionar qu\u00e9 caracter\u00edsticas (campos \/ columnas) usar\u00e1 en el modelo final.\n\nRecuerde que [adaptive boosting](https:\/\/en.wikipedia.org\/wiki\/AdaBoost) el m\u00e9todo es bueno para tomar clasificadores d\u00e9biles(weak learners) y combinarlos (de forma ponderada) para crear modelos de predicci\u00f3n s\u00f3lidos. Tantas variables mal correlacionadas pueden ser tan \u00fatiles como una fuertemente correlacionada.","79e6b047":"### Tiempo de Prediccion\n\nAhora use los datos de test y prediga la salida (survived) seg\u00fan el modelo que hemos creado a partir de los datos de entrenamiento. Sorprendentemente, esto solo toma una l\u00ednea. Scikit-learn hace el resto por usted","3bac13be":"### Accuracy\nEntonces, \u00bfqu\u00e9 tan bueno es este modelo? Bueno, calcule qu\u00e9 tan bien nuestras predicciones coincidieron con los valores conocidos. Esto se llama precisi\u00f3n (o por el contrario error).","4355049c":"\n\n### Obtenga los datos de prueba\nAhora lea en los datos de prueba. Esos son los datos donde no conocemos el resultado (es decir, no hay un campo 'Survived'). Usaremos el modelo entrenado en esos datos para predecir el valor y enviarlo para kaggle.","10c3d545":"### Leer los datos de entrenamiento\nLas siguientes celdas leen los datos de entrenamiento del archivo de datos separados por comas `train.csv`. Estamos usando pandas para almacenar los datos como un objeto porque pandas es realmente bueno en la manipulaci\u00f3n de datos y muchas otras bibliotecas (numpy \/ scikit \/ matplotlib) funcionan bien con los objetos pandas.\n","6f01db8b":"### Pre-process de la Data\nNecesitamos hacer algo con los valores faltantes, las cadenas de texto y los campos irrelevantes.\n"}}