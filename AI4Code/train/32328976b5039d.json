{"cell_type":{"b2620fda":"code","651e4d40":"code","ef1ab025":"code","40e2211f":"code","5f33cad6":"code","005cf2cf":"code","2fc06ba6":"code","d270fa8f":"code","5debdfe5":"code","94b869e0":"code","990f948c":"code","8835aa7b":"code","2e9669b6":"code","9b0f339b":"code","15514d38":"code","58b2ba3d":"code","25d2d0e1":"code","5e6b73c2":"code","ab72abe1":"code","636accca":"code","3fe26558":"code","ce40abb7":"code","d5ff2d3a":"code","b989a23a":"code","e16f8692":"code","48ae4c8d":"markdown","44b8ecd9":"markdown","abf663d3":"markdown","794d96b1":"markdown","4c41c70a":"markdown","b48650e0":"markdown","00d498f7":"markdown","f1b09e27":"markdown","f23be3bc":"markdown","2f47f2b2":"markdown","1d68436f":"markdown","d4b1f003":"markdown","877461a7":"markdown","d35885cb":"markdown"},"source":{"b2620fda":"import requests\nimport numpy as np\nimport matplotlib.pyplot as plt","651e4d40":"# Some labels are defined to be used for plotting the data\nlabels = [\n    'Sepal length (cm)', \n    'Sepal width (cm)', \n    'Petal length (cm)', \n    'Petal width (cm)'\n]\nclasses = [\n    'Iris-setosa', \n    'Iris-versicolor', \n    'Iris-virginica'\n]","ef1ab025":"# The Iris dataset is retrieved from UCI\nurl = \"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/iris\/iris.data\"\ntext = requests.get(url).text","40e2211f":"# Data is converted to numpy arrays\n# Replacing class labels for an numeric id (0, 1, 2)\nfor i, c in enumerate(classes):\n    text = text.replace(c, str(i))\n\n# Converting to a numerical matrix\ndata = np.array([\n    [\n        float(item) \n        for i, item in enumerate(line.split(\",\"))\n    ]\n    for line in text.strip().split(\"\\n\") # Extract observations\n])\n\n# Array of classes\nX = data[:, :-1]\ny = np.zeros((len(data), 3))\nfor i in range(len(data)):\n        y[i, int(data[i, -1])] = 1","5f33cad6":"# Normalizing the X matrix\nX = (X - X.mean(axis=0)) \/ X.std(axis=0)","005cf2cf":"# Visualizing the Iris dataset\nfig, ax = plt.subplots(ncols=2, figsize=(16, 9))\nax[0].imshow(X, aspect='auto')\nax[0].set_ylabel('Examples')\nax[0].set_xlabel('Inputs')\nax[1].imshow(y, aspect='auto')\nax[1].set_xlabel('Output')","2fc06ba6":"# For example, layers can be specified as follows:\n\nD = [4, 8, 5, 3]","d270fa8f":"# W and b are initialized with random values\ndef initial_params(D):\n    W = [np.random.rand(D[i], D[i + 1]) for i, l in enumerate(D[:-1])]\n    b = [np.random.rand(D[i + 1]) for i, l in enumerate(D[:-1])]\n    return W, b","5debdfe5":"W, b = initial_params(D)\n\nprint('Dimensions of W:')\nfor i in range(len(W)):\n    print('  Layer %d: ' % (i + 1), W[i].shape)\nprint('Dimensions of b:')\nfor i in range(len(b)):\n    print('  Layer %d: ' % (i + 1), b[i].shape)","94b869e0":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))","990f948c":"# Sigmoid with scalar values\nx = 5\nsigmoid(x)","8835aa7b":"# Sigmoid with matrix values\nx = np.array([[2, 5], [-3, 2]])\nsigmoid(x)","2e9669b6":"def forward(X, W, b, D, func):\n    aux = X.copy()\n    a = []\n    z = []\n    for i in range(len(D) - 1):\n        z.append(aux @ W[i] + b[i])\n        a.append(func(z[i]))\n        aux = a[i]\n    y_hat = a[-1]\n    return y_hat, a, z","9b0f339b":"y_hat, a, z = forward(X, W, b, D, sigmoid)\n# Showing the firsts rows\ny_hat[:10, :]","15514d38":"def cost(y, y_hat):\n    n = y.shape[0]\n    d = y.shape[1]\n    diff = y_hat - y\n    c = (1 \/ (n * d)) * \\\n        np.ones((n, 1)).T @ \\\n        (diff * diff) @ \\\n        np.ones((d, 1))\n    return c[0, 0] # Only the scalar value","58b2ba3d":"cost(y, y_hat)","25d2d0e1":"# So, for this example we need the derivative of the sigmoid function\n\ndef d_sigmoid(X):\n    return sigmoid(X) * (1 - sigmoid(X))","5e6b73c2":"x = 5\nd_sigmoid(x)","ab72abe1":"x = np.array([[2, 5], [-3, 2]])\nd_sigmoid(x)","636accca":"def backpropagation(y, y_hat, W, b, a, z, d_func, nabla=0.1):\n    n = y.shape[0]\n    d = y.shape[1]\n    delta = (2 \/ (n * d)) * (y_hat - y)\n    delta = delta.reshape(n, d)\n    for i in reversed(range(len(W))):\n        delta = delta * d_func(z[i])\n        if i == 0:\n            aux = X\n        else:\n            aux = a[i - 1]\n        W[i] = W[i] - nabla * aux.T @ delta\n        b[i] = b[i] - nabla * np.ones(n).T @ delta\n        if i > 0:\n            delta = delta @ W[i].T\n    return W, b","3fe26558":"W, b = backpropagation(y, y_hat, W, b, a, z, d_sigmoid)\ny_hat, a, z = forward(X, W, b, D, sigmoid)\ncost(y, y_hat)","ce40abb7":"def gradient_descent(X, y, D, nabla, max_iterations, min_error):\n    W, b = initial_params(D)\n    y_hat, a, z = forward(X, W, b, D, sigmoid)\n    c = [cost(y, y_hat)]\n    k = 0\n    while (k < max_iterations and c[-1] > min_error):\n        W, b = backpropagation(y, y_hat, W, b, a, z, d_sigmoid, nabla)\n        y_hat, a, z = forward(X, W, b, D, sigmoid)\n        c.append(cost(y, y_hat))\n        k += 1\n    return W, b, c","d5ff2d3a":"D = [4, 8, 3]\nW, b, c = gradient_descent(X, y, D, 1, 3e4, 1e-2)\ny_hat, a, z = forward(X, W, b, D, sigmoid)\nc[-1]","b989a23a":"# Visualizing the values of the cost function\nplt.figure(figsize=(16, 9))\nplt.scatter(range(len(c)), c, s=3)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')","e16f8692":"fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,6))\nfor i in range(3):\n    ax[i].scatter(range(150), y[:, i], color='blue', s=5, label='Real')\n    ax[i].scatter(range(150), y_hat[:, i], color='orange', s=5, label='Predicted')\n    ax[i].set_xlabel(classes[i])\nax[0].legend()","48ae4c8d":"## References\n\n* Fisher, R.A. (1936). Iris Data Set. UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science. https:\/\/archive.ics.uci.edu\/ml\/datasets\/iris\n* Deisenroth, M.P., Faisal, A.A., & Ong, C.S. (2019). _Mathematics for Machine Learning_. Cambridge University Press. pp. 187-194.","44b8ecd9":"## Gradient Descent Optimization\n\nGiven forward and backpropagation functions, a gradient descent algorithm can\nadjust values of $\\mathbf{W}$ and $\\mathbf{b}$ near to the optimal values. Below\nis the gradient descent function in which $\\kappa$ indicates the maximun number of\niterations and $\\epsilon$ represents the maximun accepted error. $\\mathbf{rnd}$\nis a function that returns matrices of random numbers.","abf663d3":"## Backpropagation function\n\nThe backprogration function adjust the values of $\\mathbf{W}$ and $\\mathbf{b}$ from a set of predicted outputs and the real outputs.","794d96b1":"## Partial derivatives\n\nThe derivatives of elements in the Jacobian are shown below.\n\n$$\\frac{\\partial{C}}{\\partial{\\mathbf{a}^{{(m)}}}} = \\frac{2}{nd^{(m)}} (\\mathbf{a}^{(m)} - \\mathbf{y})$$\n\n$$\\frac{\\partial{\\mathbf{a}^{{(l)}}}}{\\partial{\\mathbf{a}^{{(l-1)}}}} = \\frac{\\partial{\\mathbf{a}^{{(l)}}}}{\\partial{\\mathbf{z}^{{(l)}}}} \n\\frac{\\mathbf{z}^{{(l)}}}{\\partial{\\mathbf{a}^{{(l-1)}})}}$$\n\n$$\\frac{\\partial{\\mathbf{a}^{{(l)}}}}{\\partial{\\mathbf{z}^{{(l)}}}} = \\phi^{(l)}{'}(\\mathbf{z}^{(l)})$$\n\n$$\\frac{\\partial{\\mathbf{z}^{{(l)}}}}{\\partial{\\mathbf{a}^{{(l-1)}}}} = \\mathbf{W}^{(l)}$$\n\n$$\\frac{\\mathbf{z}^{{(l)}}}{\\partial{\\mathbf{W}^{{(l)}}}} = \\mathbf{a}^{(l-1)}$$\n\n$$\\frac{\\mathbf{z}^{{(l)}}}{\\partial{\\mathbf{b}^{{(l)}}}} = \\mathbf{1}_{d^{{l}}}$$\n\n$\\phi^{(l)}{'}$ is the derivative of the function $\\phi^{(l)}$.","4c41c70a":"## Optimal parameters\n\nThe optimal values of parameters $\\mathbf{W}$ and $\\mathbf{b}$ are those at the\nminimum of the function $C$\n\n$$\\mathbf{J}_C = \\mathbf{0}$$\n\nWhere $\\mathbf{J}_C$ is the Jacobian of the function $C$\n\n$$\\mathbf{J}_C = [\\partial{C}\/\\partial{\\mathbf{W}}, \\partial{C}\/\\partial{\\mathbf{b}}]$$\n\nFor the last layer, applying the chain rule for derivatives, the elements of the Jacobian are:\n\n$$\\frac{\\partial{C}}{\\partial{\\mathbf{W}^{{(m)}}}} = \n\\frac{\\partial{C}}{\\partial{\\mathbf{a}^{{(m)}}}} \n\\frac{\\partial{\\mathbf{a}^{{(m)}}}}{\\partial{\\mathbf{z}^{{(m)}}}} \n\\frac{\\mathbf{z}^{{(m)}}}{\\partial{\\mathbf{W}^{{(m)}}}}$$\n\n$$\\frac{\\partial{C}}{\\partial{\\mathbf{b}^{{(m)}}}} = \n\\frac{\\partial{C}}{\\partial{\\mathbf{a}^{{(m)}}}} \n\\frac{\\partial{\\mathbf{a}^{{(m)}}}}{\\partial{\\mathbf{z}^{{(m)}}}} \n\\frac{\\mathbf{z}^{{(m)}}}{\\partial{\\mathbf{b}^{{(m)}}}}$$\n\nIn general, the elements for the Jacobian for other layers are\n\n$$\\frac{\\partial{C}}{\\partial{\\mathbf{W}^{{(l)}}}} = \n\\frac{\\partial{C}}{\\partial{\\mathbf{a}^{{(m)}}}} \n\\frac{\\partial{\\mathbf{a}^{{(m)}}}}{\\partial{\\mathbf{a}^{{(m-1)}}}}\n\\dots\n \\frac{\\partial{\\mathbf{a}^{{(l+1)}}}}{\\partial{\\mathbf{a}^{{(l)}}}}\n\\frac{\\partial{\\mathbf{a}^{{(l)}}}}{\\partial{\\mathbf{z}^{{(l)}}}} \n\\frac{\\mathbf{z}^{{(l)}}}{\\partial{\\mathbf{W}^{{(l)}}}}$$\n\n$$\\frac{\\partial{C}}{\\partial{\\mathbf{b}^{{(l)}}}} = \n\\frac{\\partial{C}}{\\partial{\\mathbf{a}^{{(m)}}}} \n\\frac{\\partial{\\mathbf{a}^{{(m)}}}}{\\partial{\\mathbf{a}^{{(m-1)}}}}\n\\dots\n\\frac{\\partial{\\mathbf{a}^{{(l+1)}}}}{\\partial{\\mathbf{a}^{{(l)}}}}\n\\frac{\\partial{\\mathbf{a}^{{(l)}}}}{\\partial{\\mathbf{z}^{{(l)}}}} \n\\frac{\\mathbf{z}^{{(l)}}}{\\partial{\\mathbf{b}^{{(l)}}}}$$","b48650e0":"## Network structure\n\nThe multilayer perceptron is a network composed of $m$ layers with fully\nconnected nodes. The layers are specified by a vector in which each element\nindicates the number of nodes by layer. $d^{(1)}$ is the number of nodes for the\ninput layer and $d^{(m)}$ is the number of nodes for the output layer.\n\n$$\\mathbf{D} = [d^{(1)}, d^{(2)}, \\dots, d^{(m)}]$$","00d498f7":"Each layer operates on inputs by these sequential functions:\n\n$$\\mathbf{z}\u207d\u02e1\u207e = \\mathbf{a}\u207d\u02e1 \u207b \u00b9\u207e \\mathbf{W}\u207d\u02e1\u207e + \\mathbf{b}\u207d\u02e1\u207e$$\n\n$$\\mathbf{a}\u207d\u02e1\u207e = \u03d5\u207d\u02e1\u207e(\\mathbf{z}\u207d\u02e1\u207e)$$\n\nNotice that $\\mathbf{a}\u207d\u00b9\u207e = \\mathbf{X}\u2099 \\times d\u207d\u00b9\u207e$ and\n$\\mathbf{\u0177}\u2099 \\times d\u207d\u1d50\u207e = \\mathbf{a}\u207d\u1d50\u207e$. Besides that,\n$\\mathbf{W}\u207d\u02e1\u207e$ is a matrix of coefficients of size $d\u207d\u02e1 \u207b \u00b9\u207e\u00d7\nd\u207d\u02e1\u207e$ and $b\u207d\u02e1\u207e$ is a vector of constants of size $d\u207d\u02e1\u207e$ for the linear\nfunction $\\mathbf{z}\u207d\u02e1\u207e$. $\\mathbf{W}\u207d\u02e1\u207e$, $\\mathbf{b}\u207d\u02e1\u207e$, and\n$\\mathbf{z}\u207d\u02e1\u207e$ are defined for $l = {2, \u2026, m}$. $\u03d5\u207d\u02e1\u207e$ is a\nnon-linear function that transforms $\\mathbf{z}\u207d\u02e1\u207e$.","f1b09e27":"When the forward function is applied the first time, wihtout any optimization, the output values are completely misleaded.","f23be3bc":"## Cost Function\n\nIn scalar notation, a cost function is defined as the square of the difference between the real and the predicted outputs.\n\n$$C = \\frac{1}{nd^{(m)}} \\sum_{i=1}^{d^{(m)}} \\sum_{j=1}^{n} (\\hat{y}_{i, j} - y_{i, j})^2$$\n\nThe cost function in vectorial notation is presented below. Observe that\n$\\otimes$ is the element-wise product of matrices. \n\n$$C = \\frac{1}{nd^{(m)}} \\mathbf{1}_{n}^T [(\\mathbf{\\hat{y}} - \\mathbf{y}) \\otimes (\\mathbf{\\hat{y}} - \\mathbf{y})] \\mathbf{1}_{d^{(m)}}$$","2f47f2b2":"When $y$ and $\\hat{y}$ are more similar, the value of the cost function will be near to zero.","1d68436f":"For this example, the sigmoid function is used as activator $\\phi\u207d\u02e1\u207e$.\n\n$$S(x) = \\frac{1}{1 + e^{-x}}$$","d4b1f003":"## Forward function\n\nThe function `forward` transforms input\n$\\mathbf{X}$ to the output $\\mathbf{\\hat{y}}$, i.e. the predicted values.","877461a7":"## Working dataset\n\nIris dataset (Fished, 1936) is used to develop this topic. Its attributes are:\n\n* sepal length in cm\n* sepal width in cm\n* petal length in cm\n* petal width in cm\n* class:\n    - Iris Setosa\n    - Iris Versicolour\n    - Iris Virginica","d35885cb":"# Multilayer Perceptron\n\nIn this notebook a model is deduced and implemented for the\nmultilayer perceptron (MLP), a feedforward neural network. In the first section,\nMLP's structure is shown, including the algorithm to transform the input\n$\\mathbf{X}$ to the output $\\mathbf{\\hat{y}}$, i.e. the predicted values. In the\nsecond section, considerations to optimize MLP's parameters are defined. The\nbackpropagation algorithm indicates how to adjust values for coefficients in\neach network connection. Finally, a gradient descent algorithm is drawn to\nintegrate forward and backpropagation operations."}}