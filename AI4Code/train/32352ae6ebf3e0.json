{"cell_type":{"4e31ae36":"code","3a91677e":"code","780423fd":"code","b7d31f14":"code","12d0564e":"code","5e85c1c1":"code","08e9c751":"code","af8c44f1":"code","36aecdbb":"code","507b73db":"code","a180e6f8":"code","b4cd3585":"code","ff52e8e2":"code","5ace8e3a":"code","de1270fb":"code","944ff584":"code","2cd638e1":"code","ac0d1ed8":"code","226b863e":"code","f5e2e87a":"code","99d05880":"code","0516db0b":"code","e7794441":"code","fda36243":"code","60e59735":"code","3dc6faca":"markdown","945615a9":"markdown","dd34d15a":"markdown","9c47a2c1":"markdown","02e63b58":"markdown","2b5c16ed":"markdown","83087e2d":"markdown","a48b3559":"markdown","3a0fb870":"markdown","73eca7fb":"markdown","62ebb0b5":"markdown","4ece4361":"markdown","6a8c444f":"markdown","6e4ddab8":"markdown","d2a90c20":"markdown"},"source":{"4e31ae36":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib\nfrom sklearn.ensemble import BaggingRegressor\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.\n\n\n#Customizations\nsns.set() #matplotlib defaults\n\n#Any tweaks that normally go in .matplotlibrc, etc should explicitly go here\nplt.rcParams['figure.figsize'] = (12,8)\n%config InlineBackend.figure_format = 'retina'\n","3a91677e":"#Read the data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain.tail()\n#concatenate train and test\nall_data = pd.concat([train.loc[:,'MSSubClass':'SaleCondition'],test.loc[:,'MSSubClass':'SaleCondition']])","780423fd":"from scipy.stats import skew\n\ndef num_cat_separate(df):\n    \"\"\"\n    seperate features into numerica and categorical features based on dtypes\n    return: df of column names\n    -------\n    df: DataFrame, contains columns of numeric and categorical data\n    \"\"\"\n    nuemric_features = df.dtypes[df.dtypes !='object']\n    categorical_features = all_data.dtypes[df.dtypes =='object']\n    return nuemric_features, categorical_features\n\ndef remove_skewness(df,features_to_check ,threshold=None):\n    \"\"\"\n    log1p transform the skewed featrues if the skewness is greater than sk_threshold\n    \"\"\"\n    df_copy = df.copy()\n    transformed_feats = df[features_to_check].apply(lambda x: skew(x.dropna()))\n    if threshold is not None:\n        transformed_feats = transformed_feats[transformed_feats > threshold]\n    transformed_feats = transformed_feats.index\n    \n    print(f'The features that are log1p transformed, with threshold = {threshold}: {transformed_feats}')\n    df_copy[transformed_feats] = np.log1p(df[transformed_feats])\n    return df_copy\n    \n\ndef normalize(df):\n    df_stats = df.describe()\n    df_stats = df_stats.transpose()\n    normed_df = (df-df_stats.loc[:,'mean']) \/df_stats.loc[:,'std']\n    return normed_df\n# def error(actual, hat):\n#     return np.sqrt(np.sum(np.square(np.log(actual)-np.log(hat)))\/len(actual))\ndef rmse(actual, hat):\n    return np.sqrt(np.sum(np.square(actual-hat))\/len(actual))\n\n\ndef rmse_cv(model,X_train,y,scoring ='neg_mean_squared_error',  **kwargs):\n    rmse = np.sqrt(-cross_val_score(model, X_train,y,scoring= scoring,**kwargs))\n    return rmse","b7d31f14":"#separate numeric, categorical features\nnumeric_features, categorical_features = num_cat_separate(all_data)\n\n#set categorical data into dummy\nall_data = pd.get_dummies(all_data)\n\n#log transform to remove skewness\nall_data = remove_skewness(all_data, numeric_features.index,threshold=.75)\n\n#fill missing data with their mean\nall_data = all_data.fillna(all_data.mean())\n\n#prepare data for learning\nX_train = all_data[:train.shape[0]]\nX_test = all_data[train.shape[0]:]\ny=train.SalePrice\n\n\ny_log = np.log1p(y)\n\n#normalize the data for lasso and ridge\nnormed_all_data = normalize(all_data)\nnormed_X_train = normed_all_data[:train.shape[0]]\nnormed_X_test = normed_all_data[train.shape[0]:]\n","12d0564e":"from sklearn.model_selection import train_test_split\ntrain_X, valid_X, train_y, valid_y = train_test_split(X_train,y_log, test_size =.3, random_state=0)","5e85c1c1":"from sklearn.metrics import mean_squared_error\ndef rmse(y_actual,y_pred):\n    return np.sqrt(mean_squared_error(y_actual,y_pred))","08e9c751":"from sklearn.linear_model import Ridge, Lasso, LassoCV, RidgeCV\nfrom sklearn.model_selection import cross_val_score\n","af8c44f1":"alpha= 10.64636132589743 #pre-determined alpha with cv that is not shown\nalphas = np.linspace(.9*alpha, 1.1*alpha,20)\nmodel_Ridge = RidgeCV(alphas=alphas, cv=4 )\nmodel_Ridge.fit(X_train,y_log)\ny_log_hat = model_Ridge.predict(X_train)\nprint('The error of Ridege on the training set is:',rmse(y_log,y_log_hat))\nprint('optimized Ridge alpha is', model_Ridge.alpha_)\n","36aecdbb":"Ridge_coef = pd.Series(model_Ridge.coef_, index = X_train.columns)\nplt.plot(Ridge_coef)\nplt.xticks([])\n\nimp_coef = pd.concat([Ridge_coef.sort_values().head(10),\n                     Ridge_coef.sort_values().tail(10)])\n\nplt.figure()\nplt.figure(figsize=(8.0, 10.0))\nimp_coef.plot(kind = \"barh\")\nplt.title(\"20 most important coefficients in the Ridge Model\")","507b73db":"alphas = np.logspace(-5,-3, num=20)\nprint(alphas)\n\ncv_lasso = [ rmse_cv(Lasso(alpha = alpha,max_iter=50000), X_train,y_log,cv=5).mean() for alpha in alphas]\ncv = pd.Series(cv_lasso, index= alphas)\nplt.figure(figsize=(6,4))\nplt.plot(cv,'bo')\nminimum = cv[cv==cv.min()]\nalpha = minimum.index.values[0]\nprint('alpha = ',alpha,'\\n rmse = ',minimum.values[0])","a180e6f8":"#fine tune the alpha with LassoCV\n\nalphas =np.linspace(.75*alpha,1.25*alpha,20)\n\nmodel_LassoCV = LassoCV(alphas=alphas, max_iter=10000, cv=4 )\nmodel_LassoCV = model_LassoCV.fit(X_train,y_log)\nrmse_cv(model_LassoCV,X_train=X_train,y=y_log,cv=5).mean()","b4cd3585":"alpha = cv[cv==cv.min()].index.values[0]\nprint('a coarse search of penalization strength, alpha, is:',alpha)","ff52e8e2":"#fine tune the alpha for LassoCV\nalphas =np.linspace(.75*alpha,1.25*alpha,10)\n\nmodel_LassoCV = LassoCV(alphas=alphas, max_iter=50000, cv=8 )\nmodel_LassoCV = model_LassoCV.fit(X_train,y_log)\n\nalpha = model_LassoCV.alpha_\n\ny_log_pred = model_LassoCV.predict(X_train)\nprint('alpha:',alpha,'\\n cv rmse score:',rmse(y_log_pred, y_log))","5ace8e3a":"lasso_submission = model_LassoCV.predict(X_test)\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission.SalePrice = np.expm1(lasso_submission)\nsubmission.to_csv('lasso_submission.csv',index=False)","de1270fb":"from xgboost import XGBRegressor #import a the sklearn wrapped xgboost regressor\n","944ff584":"num_round=530; #num_round was pre-determined with cv, not shown here\nn_trees = 40;\nxgb_regr = XGBRegressor(max_depth= 3, learning_rate= .1 ,n_estimators= num_round, colsample_bytree=.8 ) \n#colsample_bytree is used to sample the columns used to construct each tree, \n#add randomness to the model with colsample_bytree <1\n\nbagging_xgb = BaggingRegressor(xgb_regr,max_samples=.8, n_estimators=n_trees, n_jobs=-1) \n#set n_of xgb estimators to 40, and n_jobs =-1 to use all cpu for paralellism.\n\nprint(xgb_regr)\nprint(bagging_xgb)","2cd638e1":"bagging_xgb.fit(train_X,train_y)","ac0d1ed8":"\nprint('training bagging R^2 score:', bagging_xgb.score(train_X, train_y))\nprint('validation bagging R^2 score:', bagging_xgb.score(valid_X, valid_y))\n","226b863e":"print('rmse score of training set:',rmse(train_y, bagging_xgb.predict(train_X)))\nprint('rmse score of validating set:',rmse(valid_y, bagging_xgb.predict(valid_X)))","f5e2e87a":"bagging_xgb.fit(X_train,y_log) #X_train, y_log is the entire training dataset\n","99d05880":"print('training bagging R^2 score:', bagging_xgb.score(X_train, y_log))\nprint('rmse score of training set:',rmse(y_log, bagging_xgb.predict(X_train)))","0516db0b":"bagging_xgb_all_y_pred = bagging_xgb.predict(X_test)\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission.SalePrice = np.expm1(bagging_xgb_all_y_pred)\nsubmission.to_csv('bagging_40_xgb.csv',index=False)","e7794441":"Lasso_pred = model_LassoCV.predict(X_test)\nbagging_pred = bagging_xgb.predict(X_test)\nRidge_pred = model_Ridge.predict(X_test)\n\ny_log_preds = [Lasso_pred,Ridge_pred,bagging_pred]\n\nw1=.5;\nw2=.25;\nw3=1-w1-w2\nweights = [w1,w2,w3]\n\n# ensemble = np.exp1m(np.dot(y_log_preds,weights))","fda36243":"submission = pd.read_csv('..\/input\/sample_submission.csv')\n\nensemble_1 = [w*y for w,y in zip(weights, y_log_preds)]\nensemble_1 = sum(ensemble_1)\n\nsubmission.SalePrice = np.expm1(ensemble_1)\nsubmission.to_csv('ensemble_1.csv',index=False)\n\n\nensemble_2 = np.expm1(y_log_preds)\nensemble_2 = sum([w*y for w,y in zip(weights,ensemble_2)])\nsubmission.SalePrice = ensemble_2\nsubmission.to_csv('ensemble_2.csv',index=False)\n# submission.SalePrice = np.dot(np.expm1(y_log_preds),weights)\n# submission.to_csv('ensemble_2.csv',index=False)","60e59735":"weights = [.5,0,.5]\nensemble_3 = [w*y for w,y in zip(weights, y_log_preds)]\nensemble_3 = sum(ensemble_3)\nsubmission.SalePrice = np.expm1(ensemble_3)\nsubmission.to_csv('ensemble_3.csv',index=False)\n\nensemble_4 = np.expm1(y_log_preds)\nensemble_4 = sum([w*y for w,y in zip(weights,ensemble_4)])\nsubmission.SalePrice = ensemble_4\nsubmission.to_csv('ensemble_4.csv',index=False)","3dc6faca":"### 2.1 Some function definitions for preprocessing","945615a9":"## bagging 50 different trees","dd34d15a":"\n\n# This notebook is inspired by:\nhttps:\/\/www.kaggle.com\/apapiu\/regularized-linear-models  by  Alexandru Papiu (@apapiu, GitHub)\n\nhttps:\/\/www.kaggle.com\/skyjiao\/xgboost-bagging","9c47a2c1":"### 3.1.2 LassoCV","02e63b58":"## re-train with the entire training set and submit","2b5c16ed":"### 2.2 Apply the preprocessing 1-5","83087e2d":"# Use bagging gradient boost + lasso + ridge to predict house prices","a48b3559":"## 3.2 Bagging gradient boost with XGBRegressor ","3a0fb870":"### 3.1.1 Ridge","73eca7fb":"## I used ensemble method that combines 3 different models:\n1. bagging xgboost\n2. lasso\n3. ridge\n### The outputs in this kernel receive the following scores:\n    1. ensemble_1: 0.11929\n    2. ensemble_2: 0.11937\n    3. ensemble_3: 0.12003\n    4. ensemble_4: 0.12012","62ebb0b5":"## 2. Data Prepocessing:\n### We only do the following simple preprocessing:\n1. separate numerical , categorical features with dtypes\n2. use one hot encoding for categorical\n3. impute NaN data with their means\n4. log1p transform highly skewed features (threshold =.75) and the target y\n5. normalize the features to N(0,1) ,\n","4ece4361":"### 2.3 train\/validation split","6a8c444f":"# 3. Models","6e4ddab8":"# Submit an ensemble of bagging gradient boost, lasso and ridge","d2a90c20":"## 3.1 Lasso and Ridge"}}