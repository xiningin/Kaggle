{"cell_type":{"4f0373dd":"code","4efe9951":"code","7163206d":"code","d93ca65d":"code","86bf4199":"code","8dd49382":"code","b6d398c0":"code","55d78558":"code","b5b77afd":"code","c3a5ecc8":"code","f4c51ed4":"code","63cfed94":"code","8668e7f4":"code","45703de1":"code","2592032d":"code","f64891d2":"code","c239623f":"code","60dc63e2":"code","f2d949f9":"code","99a6f433":"code","c273b6f5":"code","209f5693":"code","3e848a39":"code","689ac2f3":"code","07ce5bbe":"code","ccaf1320":"code","10ae9440":"code","626986ff":"code","e598e337":"code","64b7b4ba":"code","c1afb901":"code","690fdcc1":"code","35b39753":"code","80c83749":"code","7f7dcf57":"code","9723e1df":"code","dc88a8b2":"code","aaa63ebe":"code","c4cb1568":"code","f9270138":"code","a81692ee":"code","c3ebe21a":"code","46196f19":"code","8077ebdb":"code","f7617f41":"code","8499fb5a":"code","b28ce969":"code","54d0e8c9":"code","e72166d2":"code","35f1d318":"code","1d32f244":"code","b06b6031":"code","64d55e60":"code","3916091a":"code","210a708f":"code","06c8505b":"code","dacb2624":"code","22de552e":"code","c952ac6a":"code","51453e57":"code","fba7f31d":"code","389b1542":"code","f92c07ea":"code","6fc32c3d":"code","f4b50c94":"code","24592e1f":"code","24e5b7d9":"code","4fa6a29e":"code","8119e148":"code","88b03374":"code","786c30ee":"code","49053d22":"code","d683e069":"code","5fa7a8c4":"code","1a6b5eca":"code","e1aae563":"code","b9a2e9b0":"code","f9c183a6":"code","132576b6":"code","8a779af9":"code","6bbf1425":"code","ea99fae9":"code","9c7edeb5":"code","ffac1cf3":"code","bdf04d61":"code","6b6664e2":"code","23ee1034":"code","084b8790":"code","c10f3590":"code","5725eed5":"code","484d625f":"code","cbb10118":"code","adfb6610":"code","05546928":"markdown","9fdb0d3b":"markdown","52c8af3d":"markdown","f42c1665":"markdown","d59c65fa":"markdown","e0540884":"markdown","ef3ccf9d":"markdown","5f1820a1":"markdown","a650cf52":"markdown","58e1ec98":"markdown","bb153e46":"markdown","c165a29f":"markdown","0f27799d":"markdown","2b36bc78":"markdown","bfad0f76":"markdown","30d05ac2":"markdown","5583148f":"markdown","55c4bfba":"markdown","38093054":"markdown","eae24f91":"markdown","df2ed2aa":"markdown","862f4137":"markdown","a6d03e99":"markdown","c1e51e4c":"markdown","78938157":"markdown","cd4b4f85":"markdown","7720bd92":"markdown","dee78a25":"markdown","bc16d475":"markdown","4ea4361b":"markdown","f23e0e7e":"markdown","981af229":"markdown","b135480a":"markdown","f90b2445":"markdown","f1d32d5d":"markdown","a215aead":"markdown","ad40893f":"markdown","618b9175":"markdown","649f11dc":"markdown","d43b1815":"markdown","e4fe9351":"markdown","05be19da":"markdown","f39ac9ee":"markdown","69dc688c":"markdown","4d937b99":"markdown"},"source":{"4f0373dd":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport warnings\nfrom collections import Counter\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve, learning_curve\nfrom sklearn.ensemble import (RandomForestClassifier, \n                              AdaBoostClassifier, \n                              GradientBoostingClassifier,\n                              ExtraTreesClassifier, \n                              VotingClassifier)\n\nfrom lightgbm import LGBMClassifier","4efe9951":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","7163206d":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","d93ca65d":"train.head()","86bf4199":"test.head()","8dd49382":"print(f\" Train shape: {train.shape} \\n Test shape: {test.shape}\")","b6d398c0":"test_id = test['PassengerId']","55d78558":"train.drop(['PassengerId'], axis = 1, inplace = True)\ntest.drop(['PassengerId'], axis = 1, inplace = True)","b5b77afd":"print(f\" Train shape: {train.shape} \\n Test shape: {test.shape}\")","c3a5ecc8":"corr = train.corr()\ncorr_index = corr.index\ncorr_index","f4c51ed4":"plt.figure(figsize = (12, 7))\nsns.heatmap(corr, annot = True)\nplt.show()","63cfed94":"# ignore the warnings\nwarnings.filterwarnings(\"ignore\")\n\nfig, axes = plt.subplots(3, 2, figsize = (24, 29))\nsns.scatterplot(ax = axes[0, 0], x = 'Age', y = 'Survived', data = train, hue = 'Survived')\nsns.histplot(ax = axes[0, 1], x = train['Age'], kde = True)\nsns.distplot(ax = axes[1, 0], x = train['Survived'])\nsns.barplot(ax = axes[1, 1], x = \"Pclass\", y = 'Survived', data = train)\nsns.barplot(ax = axes[2, 0], x = \"Sex\", y = 'Survived', data = train)\nplt.show()","8668e7f4":"sns.catplot(ax = axes[2, 0], x = \"Parch\", y = 'Survived', data = train, kind = \"bar\")","45703de1":"sns.catplot(y = 'Survived', x = 'SibSp',data = train, kind=\"bar\")","2592032d":"graph = sns.FacetGrid(train, col='Survived')\ngraph = graph.map(sns.distplot, \"Age\")","f64891d2":"graph = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 0) & (train[\"Age\"].notnull())], color=\"Red\", shade = True)\ngraph = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 1) & (train[\"Age\"].notnull())], ax =graph, color=\"Green\", shade= True, alpha = 0.3)\ngraph.set_xlabel(\"Age\")\ngraph.set_ylabel(\"Frequency\")\ngraph = graph.legend([\"Not Survived\",\"Survived\"])","c239623f":"sns.catplot(ax = axes[2, 1], x = 'Pclass', y = 'Survived', hue = 'Sex', data = train, kind = 'bar')","60dc63e2":"sns.catplot(x = \"Embarked\", y = \"Survived\", kind = 'bar', data = train)","f2d949f9":"sns.catplot(x = \"Embarked\", y = \"Survived\", kind = 'bar',  hue = 'Pclass', data = train)","99a6f433":"fig, axes = plt.subplots(1, 2, figsize = (20, 7))\nsns.boxplot(ax = axes[0], x = \"Pclass\", y =\"SibSp\", data = train)\nsns.boxplot(ax = axes[1], x = \"Sex\", y = \"Age\", data = train)\nplt.show()","c273b6f5":"sns.catplot(x = \"Sex\",  y = \"Age\", data = train, hue = \"Pclass\", kind = \"box\")","209f5693":"sns.catplot(x = \"SibSp\",  y = \"Age\", data = train, kind = \"box\")","3e848a39":"sns.catplot(x = \"SibSp\",  y = \"Age\", data = train, kind = \"box\", hue = \"Survived\")","689ac2f3":"y_train = train['Survived']\ntrain.drop(\"Survived\", axis = 1, inplace = True)\nall_data = pd.concat([train, test]).reset_index(drop = True)","07ce5bbe":"y_train.head()","ccaf1320":"def check_missing_values(df):\n    col_na = df.columns[df.isnull().any()]\n    col_na_df = df[col_na].isnull().sum()\n    percentage_df = (df.isnull().sum() \/ len(df)) * 100\n    percentage_col = (df[col_na].isnull().sum() \/ len(df)) * 100\n     \n    return percentage_df, pd.DataFrame({\"Missing Value Count\" : col_na_df, \"Percentage of NaN values\" : percentage_col})\n    ","10ae9440":"percentage_df, percentage_col = check_missing_values(all_data)","626986ff":"percentage_df","e598e337":"percentage_col","64b7b4ba":"plt.figure(figsize = (12, 9))\nsns.barplot(x = percentage_col.index, y = \"Percentage of NaN values\",  data = percentage_col)\nplt.xticks(rotation = 45)\nplt.title = \"Missing Values Visualization\"\nplt.show()","c1afb901":"all_data['Cabin'].replace({np.nan : \"X\"}, inplace = True)\nall_data['Cabin'].head()","690fdcc1":"all_data['Age'].replace({np.nan : all_data['Age'].mean()}, inplace = True)","35b39753":"all_data['Fare'].fillna(method = 'bfill', inplace = True)","80c83749":"percentage_df, percentage_col = check_missing_values(all_data)","7f7dcf57":"percentage_df","9723e1df":"percentage_col","dc88a8b2":"sns.heatmap(all_data.isnull())\n# confirming we dont have missing values","aaa63ebe":"all_data.dtypes","c4cb1568":"all_data.head()","f9270138":"titles = [title[1][:-1] for title in all_data['Name'].str.split(\" \")]\ntitles[:5]","a81692ee":"all_data['Title'] = titles\nall_data.head()","c3ebe21a":"plt.figure(figsize = (12, 7))\nsns.countplot(x = all_data[\"Title\"])\nplt.xticks(rotation = 90)\nplt.show()","46196f19":"unwanted_titles = [title for title in all_data['Title'].unique() if title not in ['Mr', 'Mrs', 'Miss', 'Dr', 'Ms']]\nunwanted_titles[:5]","8077ebdb":"all_data['Title'].replace(to_replace = unwanted_titles, value = ['unpopular']*len(unwanted_titles), inplace = True)\nall_data['Title'].unique()","f7617f41":"sns.barplot(x = all_data[:len(train)]['Title'], y = y_train)","8499fb5a":"all_data[\"Cabin_Embarked\"] = all_data['Cabin'] + all_data['Embarked']","b28ce969":"all_data['FamilySize'] = all_data['Parch'] + all_data['SibSp']\nall_data.head()","54d0e8c9":"sns.barplot(x = all_data[:len(train)]['FamilySize'], y = y_train)","e72166d2":"all_data.dtypes","35f1d318":"for col in all_data.select_dtypes(include = ['object']):\n    all_data[col] = all_data[col].astype(\"category\")","1d32f244":"all_data.dtypes","b06b6031":"all_data.drop(['Name', 'Ticket'], axis = 1, inplace = True)","64d55e60":"all_data.head()","3916091a":"all_data = pd.get_dummies(all_data, columns = ['Sex'], prefix = \"Sex\", drop_first = True)\nall_data = pd.get_dummies(all_data, columns = ['Title'], prefix = \"Tit\", drop_first = True)\nall_data = pd.get_dummies(all_data, columns = ['Cabin'], prefix = \"Cab\", drop_first = True)\nall_data = pd.get_dummies(all_data, columns = [\"Cabin_Embarked\"], prefix = \"Cab_Emb\", drop_first = True)\nall_data = pd.get_dummies(all_data, columns = ['Embarked'], prefix = \"Emb\", drop_first = True)","210a708f":"all_data.head()","06c8505b":"all_data.columns","dacb2624":"feature_selection_df = all_data[:len(train)]\nfeature_selection_df","22de552e":"from sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_selection import chi2\nfrom sklearn.inspection import permutation_importance","c952ac6a":"infor_gain = mutual_info_classif(feature_selection_df, y_train)\nfeature_importance = pd.Series(infor_gain, index = feature_selection_df.columns).sort_values(ascending = False)\nfeature_importance = feature_importance[:30]\nfeature_importance.plot(kind = \"barh\", color = \"blue\")","51453e57":"scores = chi2(feature_selection_df, y_train)\nchi_square_scores = pd.Series(scores[0], index = feature_selection_df.columns).sort_values(ascending = False)\nchi_square_scores = chi_square_scores[:20]\nchi_square_scores.plot(kind = \"barh\", color = \"blue\")","fba7f31d":"from sklearn.ensemble import RandomForestClassifier","389b1542":"cls = RandomForestClassifier(random_state = 42)\ncls.fit(feature_selection_df, y_train)","f92c07ea":"importance = cls.feature_importances_\nfeatures_importance = pd.Series(importance, index = feature_selection_df.columns).sort_values(ascending = False)\nfeatures_importance[:20].plot(kind = \"barh\", color = \"blue\")","6fc32c3d":"selected_feature = [*features_importance[:20].index, *chi_square_scores[:4].index, *feature_importance[:30].index]\nimportant_features = set(selected_feature)\nlen(important_features)","f4b50c94":"all_data = all_data[important_features]\nall_data.head()","24592e1f":"from sklearn.preprocessing import MinMaxScaler","24e5b7d9":"scaler = MinMaxScaler()\nall_data['Fare'] = scaler.fit_transform(all_data['Fare'].values.reshape(-1, 1))","4fa6a29e":"all_data[\"Age\"] = scaler.fit_transform(all_data['Age'].values.reshape(-1, 1))","8119e148":"all_data[['Fare', 'Age']].head()","88b03374":"train_data =  all_data[:len(train)]\ntest_data = all_data[len(train):]","786c30ee":"print(f\"Train data: {train_data.shape}\")\nprint(f\"test data: {test_data.shape}\")\nprint(f\"Original Test data: {test.shape}\")\nprint(f\"Y_train: {y_train.shape}\")","49053d22":"log_reg = LogisticRegression(penalty='l2',C = 10 ,random_state = 20)\nsvc = svm = SVC(random_state = 20)\ndecision_tree = DecisionTreeClassifier(random_state=20)\nrand_forest = RandomForestClassifier(random_state=20)\ngrad_desc = GradientBoostingClassifier(random_state=20)\nknn = KNeighborsClassifier()\nLin_Disc = LinearDiscriminantAnalysis()\nmlp = MLPClassifier(random_state=20)\nxtra = ExtraTreesClassifier(random_state=20)\nlgb_classifier = LGBMClassifier()\nadaboost = AdaBoostClassifier(DecisionTreeClassifier(random_state=20),random_state=20 ,learning_rate=0.1)","d683e069":"models = [log_reg, svc, decision_tree, rand_forest, grad_desc, knn, Lin_Disc, mlp, xtra, lgb_classifier, adaboost]\nmodels_name = ['log_reg', 'svc', 'decision_tree', 'rand_forest', 'grad_desc', 'knn', \n               'Lin_Disc', 'mlp', 'xtra', 'lgb_classifier', 'adaboost']","5fa7a8c4":"kfold = StratifiedKFold(n_splits = 5)","1a6b5eca":"def model_perf(model, X_train, y_train):\n    cv_scores = cross_val_score(model, X = X_train, y = y_train, n_jobs = 4, cv = kfold, scoring = 'accuracy')\n    mean = cv_scores.mean()\n    std = cv_scores.std()\n    \n    return mean, std","e1aae563":"def run_all(models, models_name):\n    \n    models_mean = []\n    models_std = []\n    \n    for model in models:\n        mean, std = model_perf(model, train_data, y_train)\n        models_mean.append(mean)\n        models_std.append(std)\n        \n    results = pd.DataFrame({\"Model_Name\" : models_name, \"Mean_score\" : models_mean, \"STD_score\" : models_std})\n    \n    return results","b9a2e9b0":"results = run_all(models, models_name)","f9c183a6":"results","132576b6":"def plot_perf(results):\n    graph = sns.barplot(x = results['Mean_score'], y = results['Model_Name'])\n    graph.set_title(\"Simple Models Cross Validation Performance in Accuracy\")\n    graph.set_xlabel(\"Accuracy Score\")","8a779af9":"plot_perf(results)","6bbf1425":"models_grid = {\n    \"SVC\" : {\n        \"model\" : SVC(),\n        \"parameters\" : {\n            'C': [0.1, 1, 10, 100, 267, 1000], \n            'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n            'kernel': ['rbf']\n        }\n    },\n    \n    \"GradientBoosting\" : {\n        \"model\" : GradientBoostingClassifier(),\n        \"parameters\" : {\n            'loss' : [\"deviance\"],\n            'n_estimators' : [100,200,300],\n            'learning_rate': [0.1, 0.05, 0.01],\n            'max_depth': [4, 8],\n            'min_samples_leaf': [100,150],\n            'max_features': [0.3, 0.1] \n        }\n    },\n    \n    \"LGBClassifier\" : {\n        \"model\" : LGBMClassifier(),\n        \"parameters\" : {\n            \"boosting_type\" : [\"gbdt\", \"dart\", \"goss\", \"rf\"],\n            \"num_leaves\" : [5, 20, 30, 40, 100],\n            \"max_depth\" : [0, 1, 5, 10, 20],\n            \"learning_rate\" : [0.01, 0.001, 0.1],\n            \"n_estimators\" : [100, 150, 200, 300]\n        }\n    },\n    \n    \"RandomForest\" : {\n        \"model\" : RandomForestClassifier(),\n        \"parameters\" : {\n            \"max_depth\": [None],\n            \"max_features\": [1, 3, 10],\n            \"min_samples_split\": [2, 3, 10],\n            \"min_samples_leaf\": [1, 3, 10],\n            \"bootstrap\": [False, True],\n            \"n_estimators\" :[100,300, 500],\n            \"criterion\": [\"gini\"]\n        }\n    },\n    \n    \"MLP\" : {\n        \"model\" : MLPClassifier(),\n        \"parameters\" : {\n            \"hidden_layer_sizes\" : [(100,), (200, ), (150, )],\n            \"activation\" : [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n            \"solver\" : [\"lbfgs\", \"sgd\", \"adam\"]\n        }\n    },\n    \n    \"KNN\" : {\n        \"model\" : KNeighborsClassifier(),\n        \"parameters\" : {\n            \"n_neighbors\" : [5, 10, 20, 27],\n            \"weights\" : [\"uniform\", \"distance\"],\n            \"algorithm\" : [\"auto\", \"ball_tree\", \"brute\"]\n        }\n    },\n    \n     \"LogisticRegression\" : {\n         \"model\" : LogisticRegression(),\n         \"parameters\" : {\n             \"penalty\" : [\"l1\", \"l2\", \"elasticnet\"],\n             \"solver\" : [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\"]\n         }\n     },\n    \n    \"DecisionTree\" : {\n        \"model\" : DecisionTreeClassifier(),\n        \"parameters\" : {\n            \n        }\n    }\n}","ea99fae9":"scores = []\nparams = []\nbest_estimators = []\nfor estimator_info in models_grid.keys():\n    estimator = models_grid[estimator_info]\n    grid_srch = GridSearchCV(estimator = estimator['model'], param_grid = estimator['parameters'], \n                             scoring = 'accuracy', n_jobs = 4, cv = kfold)\n    grid_srch.fit(train_data, y_train)\n    scores.append(grid_srch.best_score_)\n    params.append(grid_srch.best_params_)\n    best_estimators.append(grid_srch.best_estimator_)","9c7edeb5":"params","ffac1cf3":"performace_results = pd.DataFrame({\"Model_Name\" : models_grid.keys(), \"Model_score\" : scores, \"Model_params\" : params})","bdf04d61":"performace_results","6b6664e2":"def learning_curve_generator(estimator, X, y, title, cv,  train_sizes=np.linspace(.1, 1.0, 5), n_jobs=-1, ylim=None):\n    if ylim is not None:\n        plt.ylim(*ylim)\n    \n    train_sizes_abs, train_scores, test_scores = learning_curve(estimator, \n                                                                X, \n                                                                y, \n                                                                cv=cv, \n                                                                train_sizes=train_sizes,\n                                                                n_jobs=n_jobs)\n    \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(test_scores, axis = 1)\n    test_scores_mean = np.mean(test_scores, axis = 1)\n    test_scores_std = np.std(test_scores, axis = 1)\n    \n#     graph\n    plt.figure()\n    \n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    \n    plt.plot(train_sizes, train_scores_mean, 'o-', color = 'r', label = \"Train Scores\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color = 'g', label = \"Test Scores\/ Cross Val Scores\")\n    \n    plt.title = title\n    plt.xlabel(title + \" Training Samples\")\n    plt.ylabel(\"Scores\")\n    plt.grid()\n    plt.legend(loc = \"best\")\n    plt.show()","23ee1034":"for estimator_name, estimator in zip(models_grid.keys(), best_estimators):\n    title = estimator_name + \" Learning Curves\"\n    learning_curve_generator(estimator, train_data, y_train, title, cv=kfold)","084b8790":"best_estimators","c10f3590":"best_models = []\nfor estimator_name, estimator in zip(models_grid.keys(), best_estimators):\n    print(estimator_name)\n    if estimator_name in [\"GradientBoosting\", \"RandomForest\", \"SVC\", \"LogisticRegression\", \"LGBClassifier\"]:\n        if estimator_name == 'SVC':\n            estimator = SVC(C= 100, gamma=0.01, kernel ='rbf', probability=True)\n    else:\n        continue\n    model = estimator.fit(train_data, y_train)\n    best_models.append((estimator_name, model))","5725eed5":"best_models","484d625f":"voting_classifier = VotingClassifier(estimators = best_models, voting='soft', n_jobs=4)\n\nvoting_classifier = voting_classifier.fit(train_data, y_train)","cbb10118":"submission_predictions = pd.Series(voting_classifier.predict(test_data), name=\"Survived\")\n\nindex = test.index\nsubmission_df = pd.DataFrame({\"Survived\" : submission_predictions, \"PassengerId\" : test_id})\nsubmission_df.to_csv(\"submission_final.csv\", index=False)","adfb6610":"submission_df","05546928":"I'll be using soft voting technique. This is how it works:\n\n1. Finds the probability of all the model predictions both positive probability and negative probailities.\n\n2. Finds the average of bothe the positive probabilities and negative probabilities.\n\n3. Determines which is greater. If mean of positive probabilities is greater then the prediction is 1 and vice versa.\n\n[Read here for more information](https:\/\/towardsdatascience.com\/how-voting-classifiers-work-f1c8e41d30ff)","9fdb0d3b":"The models I choose to use are:\n\n- GradientBoosting\n- LGBClassifier\n- Logistic Regression\n- RandomForest\n- MLP\n- Decision Tree","52c8af3d":"## Remove unusefull Features\n\nPassengerId  features are not important, so lets get rid of them","f42c1665":"## Correlation Between Numberic Features and Survival","d59c65fa":"# Data Preprocessing","e0540884":"# Titanic Classification Competition\n\n## Methodology\n1. Load Data\n    - Load from memory\n    - Shape inspection\n    - Delete unuseful features\n    - Familiarization of data\n2. Exploratory Data Analysis\n3. Outlier Detection and Removal\n4. Data Preprocessing\n    - Check and Visualiza Missing Values\n    - Fix DataTypes\n    - Imputing Missing Values\n5. Feature Engineering\n6. Label or One hot encoding\n7. Modeling\n    - Simple models\n        - Logistic Regression\n        - SVM\n        - KNN\n        - Naive Bayes\n        - Light XGB\n        - XGBoosting\n    - Ensembling\n8. Making Predictions","ef3ccf9d":"# Feature Engineering\n\n\nThe ``Name`` feature can be of use to us. It contains the title of an individual passenger. Lets see whow it relates to survivability. First we need to filter it out. Lets get started...","5f1820a1":"## Best Performing Models\n\nLets get the trained best performing models","a650cf52":"### Checking Data Types","58e1ec98":"# 2. Exploratory Data Analysis (EDA)","bb153e46":"Basing on the features importance metrics above, will select the following features to work with.","c165a29f":"- SVC model is doing quite well. Its not underfitting or overfittin. I think it will do well with more data.\n\n- GradientBoosting classifier is doing great(awesome) and its overfitting since it does very bad on training sample and very poorly on test samples.\n\n- LGBoost will do well with more data.\n\n- Adaboost is undefitting since it does 100% accuracy on training and 75% on testing.\n\n- Random Forest did well in generalizing predictions since the training and testing accuracies are not that off.\n\n- Decision tree looks to be overfitting.","0f27799d":"## Carbin and Embarked\n\nI think we can get some information from the cabin and the point from which the individual embarked on the journey.","2b36bc78":"Thanks for reading. This was my first ever publication on kaggle. If you think I did a good job give this notebook an upvote.Thanks\n\nHappy coding!!","bfad0f76":"## Visualizing Overfitting or Underfitting\n\nThis graphs will provide us with a good insight to help us discover whether or not our models are overfitting.","30d05ac2":"# Base Models\n\nI will use the following base or simple models\n\n- Logistic Regression\n- SVC\n- Decision Tree\n- Random Forest\n- Gradient Descent\n- K-Nearest Neighbors\n- Linear Discriminant Analysis\n- MLP Classifier\n- Extra Trees Classifier\n- LGBM Classifier","5583148f":"# Modeling \n\nFinaly we are ready to do some modeling :)\n\nBut before that, lets fo ahead and uncombine the training and testing data since we combined them earlier.","55c4bfba":"## Familiarization Of Dataset","38093054":"## Correlation Test\n\nThere are not strong correlations between independent variables(no multicollinearity)","eae24f91":"Most adults from 20-30 were traveling alone, while children had thier siblings or Sp for adulsts","df2ed2aa":"|Feature|Description|\n|:-|:-|\n|PassengeId|This is just the Identification number given to every passender, I dont think the ID has a determining factor in survival but, it can be used to trace who got a ticke before the other.|\n|Pclass|This is for the socio-economic status (SES) 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower|\n|Name|This is just the name of the individual passengers who were on the Titanic at the moment of the crash. I dont think this is a usefull column and I plan on droping it. But, the titles in the name can be usefull so its better to extract a feature from the titles.|\n|Sex|This stores the gender of the individual passengers, we need to label encode this since its a binary value. One hot encoding is not compulsory in this process.|\n|Age|e is in Years, Fractional if Age less than One (1).If the Age is estimated, it is in the form xx.5|\n|sibsp|This stores the family relationship like **Spouce** or **Sibling**|\n|Parch|This stores the number of children on parents on board of each individual passenger|\n|Ticket|This feature stores the ticke number, this could tell us who bought a ticke before the other|\n|Fare|Passenger Fare in British pound|\n|Cabin|The cabin in which each individual passenger was in|\n|Embarked|Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)|\n|Survived|This is the target variable I'll be predicting. Survival (0 = No; 1 = Yes)|\n\n[Reference](http:\/\/campus.lakeforest.edu\/frank\/FILES\/MLFfiles\/Bio150\/Titanic\/TitanicMETA.pdf)","862f4137":"Some of the title in here are not common, lets group those to avoid model complexity.","a6d03e99":"## Hyperparameter Tunning Of The Best Models","c1e51e4c":"#### Run all simple models","78938157":"Looks like we have all the Features with thier right data types, great! You can confirm from the dataframe output below.","cd4b4f85":"Basing on my analysis, we'll use the following models:\n\n- SVC\n- GradientBoosting classifier\n- LGBoost\n- RandomForest\n- Logistic Regression","7720bd92":"### Simple Interpretation\n\nA postive correlation means when one value increases the other also increases for bivariate data.\n\n#### Fare and Parch\n\nThese bivariate data has a positive correlation Parch is the total number of children or parent on board for each individual meaning. This makes sense cause, if you were to travel with your family, you will probably carry more laguage hence more on your fare.\n\nAge and Survived have a negative correlation, meaning as one goes up the other goes down.\n\n[Reference](https:\/\/www.investopedia.com\/terms\/p\/positive-correlation.asp)","dee78a25":"### Relationship and Care\n\nFrom the data analysis and visualization, its clear that:\n\n1. Survival probability for people with 3 children or parent(s) on board was the most highest.\n2. At the same time, people with 3 siblings or spouce have the second lowest survival probability of about 20%\n\nThis shows a relationship between total number of relatives or spouce on board and survival probability. Lets great a feature to depict this relationship. Lets add total ``Parch`` and ``SibSp`` together for each passenger to create a feature called ``FamilySize``","bc16d475":"## Simple Models","4ea4361b":"Most passengers from town C are more survived but why is that? lets find out with the next plot","f23e0e7e":"# Feature Selection","981af229":"Okay, we can see that most first class passenger came from town C and first class passengers are more likely to survive.","b135480a":"From the above data frame, we can see that the Cabin, Fare and Age features have missing values. Cabin has the most missing values in all.\n\nLets replace missing cabins with X, for age, we can use mean for fare we can decide to drop it since its only one or just use the mean. I'll use the mean in this case.","f90b2445":"# Loading Data and Inspection","f1d32d5d":"# Introduction\n\nThis kernel notebook is my first public Kaggle notebook. My aim is to build a much of simple classifiers and see how well it performs. I'll then perform ensembling of the best simple models to option a better prediction if I need to. Lets get started...","a215aead":"## Voting \n\nNow that we have a set of classifiers lets take a vote.\n\nFor more reading on how voting works read the following docs:\n\n[Official docs](hestimator_namelearn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingClassifier.html)\n\n[Video resource](https:\/\/www.youtube.com\/watch?v=EFk21H6Q1ew)","ad40893f":"## Chi-square","618b9175":"## Fixing Data Types\n\nLets convert object types to categorical types then one hot encode them.","649f11dc":"## Random Forest Embedded Features Selection","d43b1815":"- This shows that you are more likely to not survive if your age is around 25-35. \n\n- On the other hand Children are more likely to survive compared to not survive around ages 0-13","e4fe9351":"## Information Gain","05be19da":"# One Hot Encoding\n\nFor one hot encoding, I'll encode the categoricall features: \n\n- Sex\n- Cabin\n- Embarked\n- Title","f39ac9ee":"#### Drop the ``Ticket`` and Name column\n\nI dont find this column important and think I should just drop it.\n\nFor the ``Name`` column, we have already extracted a feature from it so we can drop it as well.","69dc688c":"## Shape of dataset","4d937b99":"## Check For Missing Values"}}