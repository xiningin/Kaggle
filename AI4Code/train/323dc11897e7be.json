{"cell_type":{"81844b80":"code","9abeb2b3":"code","43f0ceeb":"code","39594eef":"code","8f4a0f9c":"code","2d092a99":"code","09811d70":"code","0c3ad238":"code","86a96fe5":"code","2adee1a4":"code","7781b108":"code","064bcbe7":"code","efc35256":"code","ee1837e9":"code","2645a40e":"code","da1c2023":"code","fcd5f25a":"code","75900210":"code","7ab47727":"code","901cab98":"code","82c93e91":"code","1a63b7a7":"code","b1e15edc":"code","088e92d4":"code","7921ec8b":"code","902ff500":"code","1489fb4f":"code","e38a7555":"code","ad0125dc":"code","daf9bb4e":"code","22c37c65":"code","63301b80":"code","63ac3345":"code","8827533f":"code","b16c5547":"code","b49998e3":"code","b592c44b":"code","7e10adb7":"code","4afa64c5":"code","f3f13158":"code","418a1071":"code","17114359":"code","e64bce51":"code","72e421d6":"code","56ba229b":"code","4a387ded":"code","8c41fc6f":"code","ee8afadd":"markdown","38fc5b6d":"markdown","3ff2e092":"markdown","499ebf04":"markdown","7b482dad":"markdown","52ff5752":"markdown","57f45823":"markdown","9bb60bef":"markdown","ed991750":"markdown","8417cbbb":"markdown","0ad59282":"markdown","c3715587":"markdown","64f6ac87":"markdown","b9f787a0":"markdown","56c5bbdd":"markdown","6a4fd2a3":"markdown","dedf6af9":"markdown","d9597f60":"markdown","2766fd43":"markdown","5cfc0ec9":"markdown","db21bd14":"markdown","5d340494":"markdown","7cf662ee":"markdown","08629919":"markdown","6781de0b":"markdown","f61f5c33":"markdown","57bca19a":"markdown","28e805b3":"markdown","5daf4e0f":"markdown","551ff493":"markdown","db255050":"markdown","388d77bd":"markdown","e52022e8":"markdown","02b5031e":"markdown","0d253d9c":"markdown","3e424b80":"markdown","1b955ddf":"markdown","6d9f53b0":"markdown","0d4fc2b6":"markdown","d3569630":"markdown","c60530f3":"markdown","8b6356cc":"markdown","a5f80317":"markdown","fdf5fd0b":"markdown","a453324b":"markdown","fc48ff24":"markdown","54c774fe":"markdown","d2c0320e":"markdown","5ed88b31":"markdown","5d944503":"markdown","41613325":"markdown","c5bfc915":"markdown","17a643fd":"markdown","9100d17d":"markdown","b10b2ab1":"markdown","0292ecc3":"markdown","5cded388":"markdown","7df124e4":"markdown","f75c9861":"markdown","6a726f0c":"markdown","72a90d17":"markdown","c4711fe7":"markdown","d7dbed10":"markdown","56e6f9de":"markdown","26f1f39c":"markdown","c04988c1":"markdown","8db1b071":"markdown"},"source":{"81844b80":"#Basic imports\nimport numpy as np\nimport pandas as pd\n\n#sklearn imports\nfrom sklearn.decomposition import PCA #Principal Component Analysis\nfrom sklearn.manifold import TSNE #T-Distributed Stochastic Neighbor Embedding\nfrom sklearn.cluster import KMeans #K-Means Clustering\nfrom sklearn.preprocessing import StandardScaler #used for 'Feature Scaling'\n\n#plotly imports\nimport plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot","9abeb2b3":"#df is our original DataFrame\ndf = pd.read_csv(\"..\/input\/covtype.csv\")","43f0ceeb":"X = df.copy()","39594eef":"X.isnull().sum()","8f4a0f9c":"X.head()","2d092a99":"X[\"Distance_To_Hydrology\"] = ( (X[\"Horizontal_Distance_To_Hydrology\"] ** 2) + (X[\"Vertical_Distance_To_Hydrology\"] ** 2) ) ** (0.5)","09811d70":"X.drop([\"Horizontal_Distance_To_Hydrology\",\"Vertical_Distance_To_Hydrology\"], axis=1, inplace=True)","0c3ad238":"X.head()","86a96fe5":"X['Cover_Type'].replace({1:'Spruce\/Fir', 2:'Lodgepole Pine', 3:'Ponderosa Pine', 4:'Cottonwood\/Willow', 5:'Aspen', 6:'Douglas-fir', 7:'Krummholz'}, inplace=True)","2adee1a4":"X.head()","7781b108":"#We use pandas's 'get_dummies()' method\nX = pd.get_dummies(X)","064bcbe7":"X.head()","efc35256":"#numer is the DataFrame that holds all of X's numerical variables\nnumer = X[[\"Elevation\",\"Aspect\",\"Slope\",\"Horizontal_Distance_To_Roadways\",\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\",\"Horizontal_Distance_To_Fire_Points\",\"Distance_To_Hydrology\"]]","ee1837e9":"#cater is the DataFrame that holds all of X's categorical variables\ncater = X[[\"Wilderness_Area1\",\"Wilderness_Area2\",\"Wilderness_Area3\",\"Wilderness_Area4\",\"Soil_Type1\",\"Soil_Type2\",\"Soil_Type3\",\"Soil_Type4\",\"Soil_Type5\",\"Soil_Type6\",\"Soil_Type7\",\"Soil_Type8\",\"Soil_Type9\",\"Soil_Type10\",\"Soil_Type11\",\"Soil_Type12\",\"Soil_Type13\",\"Soil_Type14\",\"Soil_Type15\",\"Soil_Type16\",\"Soil_Type17\",\"Soil_Type18\",\"Soil_Type19\",\"Soil_Type20\",\"Soil_Type21\",\"Soil_Type22\",\"Soil_Type23\",\"Soil_Type24\",\"Soil_Type25\",\"Soil_Type26\",\"Soil_Type27\",\"Soil_Type28\",\"Soil_Type29\",\"Soil_Type30\",\"Soil_Type31\",\"Soil_Type32\",\"Soil_Type33\",\"Soil_Type34\",\"Soil_Type35\",\"Soil_Type36\",\"Soil_Type37\",\"Soil_Type38\",\"Soil_Type39\",\"Soil_Type40\",\"Cover_Type_Aspen\",\"Cover_Type_Cottonwood\/Willow\",\"Cover_Type_Douglas-fir\",\"Cover_Type_Krummholz\",\"Cover_Type_Lodgepole Pine\",\"Cover_Type_Ponderosa Pine\",\"Cover_Type_Spruce\/Fir\"]]","2645a40e":"numer.head()","da1c2023":"cater.head()","fcd5f25a":"#Initialize our scaler\nscaler = StandardScaler()","75900210":"#Scale each column in numer\nnumer = pd.DataFrame(scaler.fit_transform(numer))","7ab47727":"numer.columns = [\"Elevation_Scaled\",\"Aspect_Scaled\",\"Slope_Scaled\",\"Horizontal_Distance_To_Roadways_Scaled\",\"Hillshade_9am_Scaled\",\"Hillshade_Noon_Scaled\",\"Hillshade_3pm_Scaled\",\"Horizontal_Distance_To_Fire_Points_Scaled\",\"Distance_To_Hydrology_Scaled\"]","901cab98":"X = pd.concat([numer, cater], axis=1, join='inner')","82c93e91":"X.head()","1a63b7a7":"#Initialize our model\nkmeans = KMeans(n_clusters=3)","b1e15edc":"#Fit our model\nkmeans.fit(X)","088e92d4":"#Find which cluster each data-point belongs to\nclusters = kmeans.predict(X)","7921ec8b":"#Add the cluster vector to our DataFrame, X\nX[\"Cluster\"] = clusters","902ff500":"#plotX is a DataFrame containing 5000 values sampled randomly from X\nplotX = pd.DataFrame(np.array(X.sample(5000)))\n\n#Rename plotX's columns since it was briefly converted to an np.array above\nplotX.columns = X.columns","1489fb4f":"#PCA with one principal component\npca_1d = PCA(n_components=1)\n\n#PCA with two principal components\npca_2d = PCA(n_components=2)\n\n#PCA with three principal components\npca_3d = PCA(n_components=3)","e38a7555":"#This DataFrame holds that single principal component mentioned above\nPCs_1d = pd.DataFrame(pca_1d.fit_transform(plotX.drop([\"Cluster\"], axis=1)))\n\n#This DataFrame contains the two principal components that will be used\n#for the 2-D visualization mentioned above\nPCs_2d = pd.DataFrame(pca_2d.fit_transform(plotX.drop([\"Cluster\"], axis=1)))\n\n#And this DataFrame contains three principal components that will aid us\n#in visualizing our clusters in 3-D\nPCs_3d = pd.DataFrame(pca_3d.fit_transform(plotX.drop([\"Cluster\"], axis=1)))","ad0125dc":"PCs_1d.columns = [\"PC1_1d\"]\n\n#\"PC1_2d\" means: 'The first principal component of the components created for 2-D visualization, by PCA.'\n#And \"PC2_2d\" means: 'The second principal component of the components created for 2-D visualization, by PCA.'\nPCs_2d.columns = [\"PC1_2d\", \"PC2_2d\"]\n\nPCs_3d.columns = [\"PC1_3d\", \"PC2_3d\", \"PC3_3d\"]","daf9bb4e":"plotX = pd.concat([plotX,PCs_1d,PCs_2d,PCs_3d], axis=1, join='inner')","22c37c65":"plotX[\"dummy\"] = 0","63301b80":"#Note that all of the DataFrames below are sub-DataFrames of 'plotX'.\n#This is because we intend to plot the values contained within each of these DataFrames.\n\ncluster0 = plotX[plotX[\"Cluster\"] == 0]\ncluster1 = plotX[plotX[\"Cluster\"] == 1]\ncluster2 = plotX[plotX[\"Cluster\"] == 2]","63ac3345":"#This is needed so we can display plotly plots properly\ninit_notebook_mode(connected=True)","8827533f":"#Instructions for building the 1-D plot\n\n#trace1 is for 'Cluster 0'\ntrace1 = go.Scatter(\n                    x = cluster0[\"PC1_1d\"],\n                    y = cluster0[\"dummy\"],\n                    mode = \"markers\",\n                    name = \"Cluster 0\",\n                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),\n                    text = None)\n\n#trace2 is for 'Cluster 1'\ntrace2 = go.Scatter(\n                    x = cluster1[\"PC1_1d\"],\n                    y = cluster1[\"dummy\"],\n                    mode = \"markers\",\n                    name = \"Cluster 1\",\n                    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),\n                    text = None)\n\n#trace3 is for 'Cluster 2'\ntrace3 = go.Scatter(\n                    x = cluster2[\"PC1_1d\"],\n                    y = cluster2[\"dummy\"],\n                    mode = \"markers\",\n                    name = \"Cluster 2\",\n                    marker = dict(color = 'rgba(0, 255, 200, 0.8)'),\n                    text = None)\n\ndata = [trace1, trace2, trace3]\n\ntitle = \"Visualizing Clusters in One Dimension Using PCA\"\n\nlayout = dict(title = title,\n              xaxis= dict(title= 'PC1',ticklen= 5,zeroline= False),\n              yaxis= dict(title= '',ticklen= 5,zeroline= False)\n             )\n\nfig = dict(data = data, layout = layout)\n\niplot(fig)","b16c5547":"#Instructions for building the 2-D plot\n\n#trace1 is for 'Cluster 0'\ntrace1 = go.Scatter(\n                    x = cluster0[\"PC1_2d\"],\n                    y = cluster0[\"PC2_2d\"],\n                    mode = \"markers\",\n                    name = \"Cluster 0\",\n                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),\n                    text = None)\n\n#trace2 is for 'Cluster 1'\ntrace2 = go.Scatter(\n                    x = cluster1[\"PC1_2d\"],\n                    y = cluster1[\"PC2_2d\"],\n                    mode = \"markers\",\n                    name = \"Cluster 1\",\n                    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),\n                    text = None)\n\n#trace3 is for 'Cluster 2'\ntrace3 = go.Scatter(\n                    x = cluster2[\"PC1_2d\"],\n                    y = cluster2[\"PC2_2d\"],\n                    mode = \"markers\",\n                    name = \"Cluster 2\",\n                    marker = dict(color = 'rgba(0, 255, 200, 0.8)'),\n                    text = None)\n\ndata = [trace1, trace2, trace3]\n\ntitle = \"Visualizing Clusters in Two Dimensions Using PCA\"\n\nlayout = dict(title = title,\n              xaxis= dict(title= 'PC1',ticklen= 5,zeroline= False),\n              yaxis= dict(title= 'PC2',ticklen= 5,zeroline= False)\n             )\n\nfig = dict(data = data, layout = layout)\n\niplot(fig)","b49998e3":"#Instructions for building the 3-D plot\n\n#trace1 is for 'Cluster 0'\ntrace1 = go.Scatter3d(\n                    x = cluster0[\"PC1_3d\"],\n                    y = cluster0[\"PC2_3d\"],\n                    z = cluster0[\"PC3_3d\"],\n                    mode = \"markers\",\n                    name = \"Cluster 0\",\n                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),\n                    text = None)\n\n#trace2 is for 'Cluster 1'\ntrace2 = go.Scatter3d(\n                    x = cluster1[\"PC1_3d\"],\n                    y = cluster1[\"PC2_3d\"],\n                    z = cluster1[\"PC3_3d\"],\n                    mode = \"markers\",\n                    name = \"Cluster 1\",\n                    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),\n                    text = None)\n\n#trace3 is for 'Cluster 2'\ntrace3 = go.Scatter3d(\n                    x = cluster2[\"PC1_3d\"],\n                    y = cluster2[\"PC2_3d\"],\n                    z = cluster2[\"PC3_3d\"],\n                    mode = \"markers\",\n                    name = \"Cluster 2\",\n                    marker = dict(color = 'rgba(0, 255, 200, 0.8)'),\n                    text = None)\n\ndata = [trace1, trace2, trace3]\n\ntitle = \"Visualizing Clusters in Three Dimensions Using PCA\"\n\nlayout = dict(title = title,\n              xaxis= dict(title= 'PC1',ticklen= 5,zeroline= False),\n              yaxis= dict(title= 'PC2',ticklen= 5,zeroline= False)\n             )\n\nfig = dict(data = data, layout = layout)\n\niplot(fig)","b592c44b":"#plotX will hold the values we wish to plot\nplotX = pd.DataFrame(np.array(X.sample(5000)))\nplotX.columns = X.columns","7e10adb7":"#Set our perplexity\nperplexity = 50","4afa64c5":"#T-SNE with one dimension\ntsne_1d = TSNE(n_components=1, perplexity=perplexity)\n\n#T-SNE with two dimensions\ntsne_2d = TSNE(n_components=2, perplexity=perplexity)\n\n#T-SNE with three dimensions\ntsne_3d = TSNE(n_components=3, perplexity=perplexity)","f3f13158":"#This DataFrame holds a single dimension,built by T-SNE\nTCs_1d = pd.DataFrame(tsne_1d.fit_transform(plotX.drop([\"Cluster\"], axis=1)))\n\n#This DataFrame contains two dimensions, built by T-SNE\nTCs_2d = pd.DataFrame(tsne_2d.fit_transform(plotX.drop([\"Cluster\"], axis=1)))\n\n#And this DataFrame contains three dimensions, built by T-SNE\nTCs_3d = pd.DataFrame(tsne_3d.fit_transform(plotX.drop([\"Cluster\"], axis=1)))","418a1071":"TCs_1d.columns = [\"TC1_1d\"]\n\nPCs_1d.columns = [\"PC1_1d\"]\n\n#\"TC1_2d\" means: 'The first component of the components created for 2-D visualization, by T-SNE.'\n#And \"TC2_2d\" means: 'The second component of the components created for 2-D visualization, by T-SNE.'\nTCs_2d.columns = [\"TC1_2d\",\"TC2_2d\"]\n\nTCs_3d.columns = [\"TC1_3d\",\"TC2_3d\",\"TC3_3d\"]","17114359":"plotX = pd.concat([plotX,TCs_1d,TCs_2d,TCs_3d], axis=1, join='inner')","e64bce51":"plotX[\"dummy\"] = 0","72e421d6":"cluster0 = plotX[plotX[\"Cluster\"] == 0]\ncluster1 = plotX[plotX[\"Cluster\"] == 1]\ncluster2 = plotX[plotX[\"Cluster\"] == 2]","56ba229b":"#Instructions for building the 1-D plot\n\n#trace1 is for 'Cluster 0'\ntrace1 = go.Scatter(\n                    x = cluster0[\"TC1_1d\"],\n                    y = cluster0[\"dummy\"],\n                    mode = \"markers\",\n                    name = \"Cluster 0\",\n                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),\n                    text = None)\n\n#trace2 is for 'Cluster 1'\ntrace2 = go.Scatter(\n                    x = cluster1[\"TC1_1d\"],\n                    y = cluster1[\"dummy\"],\n                    mode = \"markers\",\n                    name = \"Cluster 1\",\n                    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),\n                    text = None)\n\n#trace3 is for 'Cluster 2'\ntrace3 = go.Scatter(\n                    x = cluster2[\"TC1_1d\"],\n                    y = cluster2[\"dummy\"],\n                    mode = \"markers\",\n                    name = \"Cluster 2\",\n                    marker = dict(color = 'rgba(0, 255, 200, 0.8)'),\n                    text = None)\n\ndata = [trace1, trace2, trace3]\n\ntitle = \"Visualizing Clusters in One Dimension Using T-SNE (perplexity=\" + str(perplexity) + \")\"\n\nlayout = dict(title = title,\n              xaxis= dict(title= 'TC1',ticklen= 5,zeroline= False),\n              yaxis= dict(title= '',ticklen= 5,zeroline= False)\n             )\n\nfig = dict(data = data, layout = layout)\n\niplot(fig)","4a387ded":"#Instructions for building the 2-D plot\n\n#trace1 is for 'Cluster 0'\ntrace1 = go.Scatter(\n                    x = cluster0[\"TC1_2d\"],\n                    y = cluster0[\"TC2_2d\"],\n                    mode = \"markers\",\n                    name = \"Cluster 0\",\n                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),\n                    text = None)\n\n#trace2 is for 'Cluster 1'\ntrace2 = go.Scatter(\n                    x = cluster1[\"TC1_2d\"],\n                    y = cluster1[\"TC2_2d\"],\n                    mode = \"markers\",\n                    name = \"Cluster 1\",\n                    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),\n                    text = None)\n\n#trace3 is for 'Cluster 2'\ntrace3 = go.Scatter(\n                    x = cluster2[\"TC1_2d\"],\n                    y = cluster2[\"TC2_2d\"],\n                    mode = \"markers\",\n                    name = \"Cluster 2\",\n                    marker = dict(color = 'rgba(0, 255, 200, 0.8)'),\n                    text = None)\n\ndata = [trace1, trace2, trace3]\n\ntitle = \"Visualizing Clusters in Two Dimensions Using T-SNE (perplexity=\" + str(perplexity) + \")\"\n\nlayout = dict(title = title,\n              xaxis= dict(title= 'TC1',ticklen= 5,zeroline= False),\n              yaxis= dict(title= 'TC2',ticklen= 5,zeroline= False)\n             )\n\nfig = dict(data = data, layout = layout)\n\niplot(fig)","8c41fc6f":"#Instructions for building the 3-D plot\n\n#trace1 is for 'Cluster 0'\ntrace1 = go.Scatter3d(\n                    x = cluster0[\"TC1_3d\"],\n                    y = cluster0[\"TC2_3d\"],\n                    z = cluster0[\"TC3_3d\"],\n                    mode = \"markers\",\n                    name = \"Cluster 0\",\n                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),\n                    text = None)\n\n#trace2 is for 'Cluster 1'\ntrace2 = go.Scatter3d(\n                    x = cluster1[\"TC1_3d\"],\n                    y = cluster1[\"TC2_3d\"],\n                    z = cluster1[\"TC3_3d\"],\n                    mode = \"markers\",\n                    name = \"Cluster 1\",\n                    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),\n                    text = None)\n\n#trace3 is for 'Cluster 2'\ntrace3 = go.Scatter3d(\n                    x = cluster2[\"TC1_3d\"],\n                    y = cluster2[\"TC2_3d\"],\n                    z = cluster2[\"TC3_3d\"],\n                    mode = \"markers\",\n                    name = \"Cluster 2\",\n                    marker = dict(color = 'rgba(0, 255, 200, 0.8)'),\n                    text = None)\n\ndata = [trace1, trace2, trace3]\n\ntitle = \"Visualizing Clusters in Three Dimensions Using T-SNE (perplexity=\" + str(perplexity) + \")\"\n\nlayout = dict(title = title,\n              xaxis= dict(title= 'TC1',ticklen= 5,zeroline= False),\n              yaxis= dict(title= 'TC2',ticklen= 5,zeroline= False)\n             )\n\nfig = dict(data = data, layout = layout)\n\niplot(fig)","ee8afadd":"# Visualizing High Dimensional Clusters","38fc5b6d":"<a id=\"T-SNE_1D\"><\/a>\n### 1-D Visualization:","3ff2e092":"Now that we have `X[\"Distance_To_Hydrology\"]`, and because there's nothing extra special about Vertical or Horizontal Distances to Hydrology, we can drop the original two columns:","499ebf04":"(The reason we converted `X.sample(5000)` to a numpy array, then back to a pandas DataFrame, is so that the indices of the resulting DataFrame, `plotX`, are *'renumbered'* 0-4999. )","7b482dad":"## T-SNE Remarks:\n\n\nThe T-SNE algorithm did a fairly decent job in visualizing the clusters, too. But, there were a few noticable differences when comparing it's resulting plots to PCA's resulting plots. \n\nOne major difference between the plots produced by PCA and T-SNE is that T-SNE's plots seemed to have it's clusters overlapping with eachother more so than in PCA's plots. For example, if you look at the [**2-D plot**](#PCA_2D) fomed from PCA, you see three distinct sections of the data-points with strict, visible borders separating each colour into groups. Whereas, if you look at the [**2-D**](#T-SNE_2D) plot formed from T-SNE, you, again, see three sections formed within the data-points, but this time, datapoints between each cluster seem to 'intermingle' and overlap more.\n\nThe other major difference between the plots created by PCA and the plots created by T-SNE, is the shape. Because both PCA and T-SNE perform dimensionality reduction in very different ways (and with different objectives), the resulting shape or distibution of the points produced by the algorithms will almost always be very different.\n\nBear in mind that the plots resulting from the T-SNE algorithm are quite variable, in that they depend very heavily on the value chosen for `perplexity`.","52ff5752":"<a id=\"8\"><\/a>\n# Conclusion:\n\nSo there you have it: two interesting methods to view clusters formed on high-dimensional data.\nOne method was the standard and reliable PCA algorithm, and the other method was the somewhat more interesting and exotic T-SNE algorithm.\n\nBoth algorithms definitely have their own strengths and weaknesses when it comes to performing this task, and I'd imagine that the effectiveness of each algorithm depends largely on the type of data being given. So, in the end, it's largely up to the user which algorithm he or she prefers to use when visualizing clusterings on high-dimensional data.","57f45823":"<a id=\"6\"><\/a>\n# **Method #1:** *Principal Component Analysis* (PCA):","9bb60bef":"<a id=\"PCA_1D\"><\/a>\n### 1-D Visualization:","ed991750":"<a id=\"1\"><\/a>\n# Introduction:","8417cbbb":"## PCA Remarks:\n\nAs we can see from the plots above: if you have data that is highly *clusterable*, then PCA is a pretty good way to view the clusters formed on the original data. Also, it would seem that visualizing the clusters is more effective when the clusters are visualized using more principle components, rather than less. For example, the 2-D plot did a better job of providing a clear visual representation of the clusters than the 1-D plot; and the 3-D plot did a better job than the 2-D plot!","0ad59282":"But first, we will create a seperate, smaller DataFrame, `plotX`, to plot our data with. The reason we create a smaller DataFrame is so that we can plot our data faster, and so that our plots do not turn out looking too messy or over-crowded.","c3715587":"<a id=\"5\"><\/a>\n# Clustering:","64f6ac87":"<a id=\"T-SNE_3D\"><\/a>\n### 3-D Visualization:","b9f787a0":"**Time to build our clusters.**","56c5bbdd":"The next plot displays the three clusters on the two *principal components* created for 2-D visualization:","6a4fd2a3":"Now that we have our clusters, we can begin visualizing our data!","dedf6af9":"We initialize our PCA models:","d9597f60":"Now we divide our DataFrame, `plotX`, into three new DataFrames. \n\nEach of these new DataFrames will hold all of the values contained in exacltly one of the clusters. For example, all of the values contained within the DataFrame, `cluster0` will belong to 'cluster 0', and all the values contained in DataFrame, `cluster1` will belong to 'cluster 1', etc.","2766fd43":"Once again, we create a sub-DataFrame called `plotX` that will hold a sample of the data from `X` for the purpose of visualization.","5cfc0ec9":"Note: And just like before, we will use this algorithm to visualize our data in [**1-D**](#T-SNE_1D), [**2-D**](#T-SNE_2D), and [**3-D**](#T-SNE_3D) space!","db21bd14":"Any missing values?","5d340494":"Now we can re-merge our two DataFrames into a new, scaled `X`.","7cf662ee":"<a id=\"T-SNE_2D\"><\/a>\n### 2-D Visualization:","08629919":"Okay. Now that we have our separate numerical DataFrame, it's time to feature-scale it:","6781de0b":"<img src=\"https:\/\/digmet.files.wordpress.com\/2014\/12\/step2-nsa-netvizz.png\" width=\"650px\" height=\"650px\"\/> ","f61f5c33":"We concatenate these newly created DataFrames to `plotX` so that they can be used by `plotX` as columns.","57bca19a":"## Contents\n1. [Introduction:](#1)\n1. [Imports:](#2)\n1. [Read the Data:](#3)\n1. [Exploration\/Engineering:](#4)\n1. [Clustering:](#5)\n1. [**Method #1:** *Principal Component Analysis* (PCA):](#6)\n1. [**Method #2:** *T-Distributed Stochastic Neighbor Embedding* (T-SNE):](#7)\n1. [Conclusion:](#8)\n1. [Closing Remarks:](#9)","28e805b3":"## T-SNE Visualizations:","5daf4e0f":"And we create one new column for `plotX` so that we can use it for 1-D visualization.","551ff493":"We'll rename the columns to show that they've been scaled:","db255050":"Rename the columns of these newly created DataFrames:","388d77bd":"The plot below displays our three original clusters on the single dimension created by T-SNE for 1-D visualization:","e52022e8":"This is not a particularly important section of the Kernel as the bulk of the interesting work will be done in the next few sections. Feel free to skim this part, if you want.","02b5031e":"Next up, we have to decide what level of `perplexity` we would like to use for our T-SNE algorithm. The `perplexity` is a hyperparameter used in the T-SNE algorithm that greatly determines how the data returned from the algorithm is distributed.\n\nTo see the role that `perplexity` plays in shaping the distibution of the data through T-SNE, check out this clearly written, and interactive [article](https:\/\/distill.pub\/2016\/misread-tsne\/) by some of the Engineers\/Scientists at [Google Brain](https:\/\/ai.google\/research\/teams\/brain).\n\nI have found, through a few trials, that `perplexity = 50` works fairly well for this data, but am convinced that there probably exists a more ideal value for `perplexity` between the values of `30` and `50`. If you're up for the challenge, feel free to fork this Kernel and try to find the value for `perplexity` that best displays the clusters formed on the original data.","0d253d9c":"And now we can 'one-hot-encode' this column:","3e424b80":"<a id=\"PCA_3D\"><\/a>\n### 3-D Visualization:","1b955ddf":"<a id=\"4\"><\/a>\n# Exploration\/Engineering:","6d9f53b0":"We build our new DataFrames:","0d4fc2b6":"(Note that, above, we performed our PCA's on data that *excluded* the `Cluster` variable.)","d3569630":"Now, before we get into clustering our data, we just need to do one more thing: [feature-scale](https:\/\/en.wikipedia.org\/wiki\/Feature_scaling#Standardization) our [numerical variables](https:\/\/www.dummies.com\/education\/math\/statistics\/types-of-statistical-data-numerical-categorical-and-ordinal\/).\n\nWe need to do this because, while each of our categorical variables hold values of either 0 or 1, some of our numerical variables hold values like 2596 and 2785. If we were to leave our data like this, then K-Means Clustering would not give us such a nice result, since K-Means Clustering measures the [euclidean distance](https:\/\/en.wikipedia.org\/wiki\/Euclidean_distance) between data-points. This means that, if we were to leave our numeical variables un-scaled, then most of the distance measured between points would be attributed to the larger numerical variables, rather than any of the categorical variables.\n\nTo fix this problem we will scale all of our numerical variables through the use of sklearn's [StandardScaler](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html) tool. This tool allows us to scale each numerical variable such that each numerical variable's mean becomes 0, and it's variance becomes 1. This is a good way to make sure that all of the numerical variables are on roughly the same scale that the categorical (binary) variables are on.","c60530f3":"<a id=\"9\"><\/a>\n# Closing Remarks:\n\nI learned about quite alot in the making of this kernel -- about clusterability, perplexity, how to use plotly, the importance of feature-engineering, and much more. In all honesty, this was a ton of fun to make and has only further deepened my interest in [unsupervised learning](https:\/\/en.wikipedia.org\/wiki\/Unsupervised_learning) and data visualization. I hope to make more kernels like this in the future and to continue to sharpen my skills in this area.\n\nIf you've got any feedback for me: please leave a comment below, as I'd love to hear what you've got to say. And if you found this kernel to be interesting or useful to you, please consider giving it an upvote - I'd appreciate it very much :)\n\nTill next time!\n*-Josh*","8b6356cc":"The next plot displays the three clusters on the two dimensions created by T-SNE for 2-D visualization:","a5f80317":"And we create one new column for `plotX` so that we can use it for 1-D visualization.","fdf5fd0b":"<a id=\"PCA_2D\"><\/a>\n### 2-D visualization:","a453324b":"<a id=\"3\"><\/a>\n# Read the data:","fc48ff24":"We concatenate these newly created DataFrames to `plotX` so that they can be used by `plotX` as columns.","54c774fe":"<a id=\"7\"><\/a>\n# **Method #2:** *T-Distributed Stochastic Neighbor Embedding* (T-SNE):","d2c0320e":"First, we construct a new DataFrame, `X` that we can modify. `X` will begin as a 'copy' of the original DataFrame, `df`.","5ed88b31":"(Note that, above, we performed our T-SNE algorithms on data that *exluded* the `Cluster` variable.)","5d944503":"In this notebook we will be exploring two different methods that can be used to visualize [clusters](https:\/\/en.wikipedia.org\/wiki\/Cluster_analysis) that were formed on high-dimensional data (data with more than three dimensions).\n\nFirst, we will clean our data so that it's in a proper format for clustering, then, we will divide the data into three different clusters using [K-Means Clustering](https:\/\/en.wikipedia.org\/wiki\/K-means_clustering). After that, we will go ahead and visualize our three clusters using our two methods: [Principal Component Analysis](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis) (PCA), and [T-Distributed Stochastic Neighbor Embedding](https:\/\/en.wikipedia.org\/wiki\/T-distributed_stochastic_neighbor_embedding) (T-SNE).\n\nThe data we will be using will be the [Forest Cover Type Dataset](https:\/\/www.kaggle.com\/uciml\/forest-cover-type-dataset).","41613325":"Our first method for visualization will be [Principal Component Analysis](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis) (PCA). \n\nPCA is an algorithm that is used for [dimensionality reduction](https:\/\/en.wikipedia.org\/wiki\/Dimensionality_reduction) - meaning, informally, that it can take in a DataFrame with many columns and return a DataFrame with a *reduced* number of columns that still retains much of the information from the columns of the original DataFrame. The columns of the DataFrame produced from the PCA procedure are called *Principal Components*. We will use these principal components to help us visualize our clusters in 1-D, 2-D, and 3-D space, since we cannot easily visualize the data we have in higher dimensions. For example, we can use two principal components to visualize the clusters in 2-D space, or three principal components to visualize the clusters in 3-D space.","c5bfc915":"This last plot below displays our clusters on the three dimensions created by T-SNE for 3-D visualization:","17a643fd":"<a id=\"2\"><\/a>\n# Imports:","9100d17d":"Now we divide our DataFrame, `plotX`, into three new DataFrames.\n\nEach of these new DataFrames will hold all of the values contained in exacltly one of the clusters. For example, all of the values contained within the DataFrame, `cluster0` will belong to 'cluster 0', and all the values contained in DataFrame, `cluster1` will belong to 'cluster 1', etc.","b10b2ab1":"If we look at the columns: `X[\"Horizontal_Distance_To_Hydrology\"]` and `X[Vertical_Distance_To_Hydrology\"]`, we see that we can create from them, a new column `X[Distance_To_Hydrology]`, which measures the shortest distance to Hydrology. We can calculate the values of this column through using the equation from the [Pythagorean Theorem](https:\/\/en.wikipedia.org\/wiki\/Pythagorean_theorem).","0292ecc3":"In this kernel, we will be visualizing only three different clusters on our data. I chose three because I found it to be a good number of clusters to help us visualize our data in a non-complicated way.","5cded388":"We initialize our T-SNE models:","7df124e4":"This last plot below displays our clusters on the three *principal components* created for 3-D visualization:","f75c9861":"The plot below displays our three original clusters on the single *principal component* created for 1-D visualization:","6a726f0c":"Sweet! No missing values. That saves us quite a bit of work.","72a90d17":"Now, to visualize our data, we will build three DataFrames from `plotX` using the 'PCA' algorithm. \n\nThe *first* DataFrame will hold the results of the PCA algorithm with only one principal component. This DataFrame will be used to visualize our clusters in *one dimension* ([**1-D**](#PCA_1D)).\n\nThe *second* DataFrame will hold the two principal components returned by the PCA algorithm with `n_components=2`. This DataFrame will aid us in our visualization of these clusters in *two dimensions* ([**2-D**](#PCA_2D)).\n\nAnd the *third* DataFrame will hold the results of the PCA algorithm that returns three principal components. This DataFrame will allow us to visualize the clusters in *three dimensional space* ([**3-D**](#PCA_3D)).","c4711fe7":"Our next method for visualizing our clusters is [T-Distributed Stochastic Neighbor Embedding](https:\/\/en.wikipedia.org\/wiki\/T-distributed_stochastic_neighbor_embedding) (T-SNE).\n\nHere is a good [video](https:\/\/www.youtube.com\/watch?v=wvsE8jm1GzE) by Google that gives a quick overview of what the algorithm does. And here is a [video](https:\/\/www.youtube.com\/watch?v=NEaUSP4YerM) that gives a helpful and simplified explanation of how the algorithm does what it does, if you're interested.\n\nIn short, T-SNE is an interesting and complicated machine learning algorithm that can help us visualize high-dimensional data. It is a method for performing dimensionality reduction, and it is for this reason that we can use it to help us visualize our three clusters that were built on high-dimensional data.","d7dbed10":"Rename the columns of these newly created DataFrames:","56e6f9de":"But, to make sure we scale only our numerical variables -- and not our categorical variables --, we'll split our current DataFrame, `X`, into two other DataFrames: `numer` and `cater`; feature-scale. `numer`, then recombine the two DataFrames together again into a DataFrame that is suitable for clustering.","26f1f39c":"## PCA Visualizations:","c04988c1":"Next, if you take a look at the values contained within `X['Cover_Type']`, you'll notice that it contains numerically-encoded [categorical data](https:\/\/en.wikipedia.org\/wiki\/Categorical_variable). If we head over to the column descriptions on the [Forest Cover Type Dataset](https:\/\/www.kaggle.com\/uciml\/forest-cover-type-dataset) page, it says that:\n\n> *1 = \"Spruce\/Fir\", 2 = \"Lodgepole Pine\", 3 = \"Ponderosa Pine\", 4 = \"Cottonwood\/WIllow\", 5 = \"Aspen\", 6 = \"Douglas-fir\", and 7 = \"Krummholz\".*\n\nWe'll relabel our data so that the values in `X['Cover_Type']` are more descriptive of what's really contained within it. We'll also do it so that we can easily apply a [one-hot-encoding](https:\/\/www.kaggle.com\/dansbecker\/using-categorical-data-with-one-hot-encoding) to it, afterwards - so that `X['Cover_Type']` will be properly encoded along with the rest of the categorical data in `X`.","8db1b071":"We build our new DataFrames to help us visualize our data in 1-D, 2-D, and 3-D space:"}}