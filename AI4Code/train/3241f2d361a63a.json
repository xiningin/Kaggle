{"cell_type":{"5b8b2ff6":"code","1540ae45":"code","4c439e59":"code","8d4cbfd0":"code","77b90c84":"code","30f84924":"code","7e678bc0":"code","1417ad77":"code","b9305632":"code","2e8546fa":"code","e68c0b36":"code","9bdbae7b":"code","fe56f247":"code","5dcc2869":"code","88a39162":"code","9729f2cb":"code","64cbaf2e":"code","2cb2c33f":"code","0c65cc8d":"code","a1be29d8":"code","a5cdfc04":"code","1df0266f":"code","a254b7bd":"code","ae73fea2":"code","da7efce2":"code","6bc254c9":"code","b52d5ea8":"code","9e7367cd":"code","91fb7be4":"code","7a9dd91a":"code","21edcf83":"code","aa6057e6":"markdown","8206c9e8":"markdown","e7bd169b":"markdown","1bc87471":"markdown","3a6ee6e9":"markdown","dc279733":"markdown","d077ee86":"markdown","c8d807d9":"markdown","99660726":"markdown","6029df09":"markdown","c9f5a355":"markdown","d7b3e066":"markdown","86ddd2da":"markdown","7088e3c3":"markdown","4b200e0b":"markdown","3e459c71":"markdown","19982859":"markdown","055073bb":"markdown"},"source":{"5b8b2ff6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\n\nimport keras\nimport numpy\nfrom keras.preprocessing.image import ImageDataGenerator","1540ae45":"import keras\nimport numpy\nfrom keras.preprocessing.image import ImageDataGenerator","4c439e59":"train_datagen = ImageDataGenerator(\n        rescale=1.\/255, validation_split=0.2)\n#         shear_range=0.2,\n#         zoom_range=0.2,\n#         horizontal_flip=True)\n","8d4cbfd0":"train_data = '..\/input\/state-farm-distracted-driver-detection\/train'\ntest_data = '..\/input\/state-farm-distracted-driver-detection\/test'\ntrain_generator = train_datagen.flow_from_directory(\n        train_data,\n        target_size=(224, 224),\n        batch_size=32,\n        class_mode='categorical',\n        subset='training')\n\nval_generator = train_datagen.flow_from_directory(\n        train_data,\n        target_size=(224,224),\n        batch_size=32,\n        class_mode='categorical',\n        subset='validation')\n","77b90c84":"from PIL import Image\n\nac_labels=  [\"c0: safe driving\",\n\"c1: texting - right\",\n\"c2: talking on the phone - right\",\n\"c3: texting - left\",\n\"c4: talking on the phone - left\",\n\"c5: operating the radio\",\n\"c6: drinking\",\n\"c7: reaching behind\",\n\"c8: hair and makeup\",\n\"c9: talking to passenger\"]\n","30f84924":"imgs, labels = next(train_generator)","7e678bc0":"import functools\n\ndef list_counts(start_dir):\n    lst = sorted(os.listdir(start_dir))\n    out = [(fil, len(os.listdir( os.path.join(start_dir, fil)))) for fil in lst if os.path.isdir(os.path.join(start_dir,fil))]\n    return out\n\nout = list_counts(train_data)\nlabels, counts = zip(*out)\nprint(\"Total number of images : \",functools.reduce(lambda a,b : a+b, counts))\nout","1417ad77":"\nimport matplotlib.pyplot as plt\nimport pandas as pd\n# Pretty display for notebooks\n%matplotlib inline\n\n\ny = np.array(counts)\nwidth = 1\/1.5\nN = len(y)\nx = range(N)\n\nfig = plt.figure(figsize=(20,15))\nay = fig.add_subplot(211)\n\nplt.xticks(x, labels, size=15)\nplt.yticks(size=15)\n\nay.bar(x, y, width, color=\"blue\")\n\nplt.title('Bar Chart',size=25)\nplt.xlabel('classname',size=15)\nplt.ylabel('Count',size=15)\n\nplt.show()\n\n\n\ndef showImages(imgs ,inlabels=None, single=True):\n    if single:\n        aim = (imgs * 255 ).astype(np.uint8)\n        img = Image.fromarray(aim)\n        if labels is not None:\n            print(\"Label : \", ac_labels[np.argmax(inlabels)])\n        plt.imshow(img)\n        plt.show()\n    else:\n        for i,img in enumerate(imgs):\n            lbl = None\n            if inlabels is not None:\n                lbl = labels[i]\n            showImages(img, lbl)\n\nind = 1\nshowImages(imgs[:ind], inlabels=labels[:ind], single = False)\n","b9305632":"from keras.layers import ZeroPadding2D, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\nfrom keras.layers import GlobalAveragePooling2D, MaxPooling2D\nfrom keras.models import Model, Sequential\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import regularizers","2e8546fa":"input_layer = Input(shape=(224,224, 3))\n\nconv = Conv2D(filters=8, kernel_size=2)(input_layer)\nconv = Conv2D(filters=16, kernel_size=2, activation='relu')(conv)\nconv = Conv2D(filters=32, kernel_size=2, activation='relu')(conv)\nconv = MaxPooling2D()(conv)\n\nconv = Conv2D(filters=64, kernel_size=2, activation='relu')(conv)\nconv = Conv2D(filters=128, kernel_size=2, activation='relu')(conv)\nconv = Conv2D(filters=512, kernel_size=2, activation='relu')(conv)\n\nconv = GlobalAveragePooling2D()(conv)\ndense = Dense(units=500, activation='relu')(conv)\ndense = Dropout(0.1)(dense)\ndense = Dense(units=100, activation='relu')(dense)\ndense = Dropout(0.1)(dense)\noutput = Dense(units=10, activation='softmax')(dense)\n\nmodel = Model(inputs=input_layer, outputs = output)","e68c0b36":"model.summary()","9bdbae7b":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","fe56f247":"#model.load_weights('best_model_1.hdf5')\ncheckpoint = ModelCheckpoint('best_model_1.hdf5', save_best_only=True, verbose=1)\n\nhistory = model.fit_generator(train_generator, steps_per_epoch=len(train_generator),\n                    epochs=10,\n                   validation_data = val_generator,\n                   validation_steps=len(val_generator),\n                    callbacks=[checkpoint] )","5dcc2869":"plt.plot(history.history['val_loss'])\nplt.show()\nplt.plot(history.history['loss'])","88a39162":"\n!ls ..\/input\/state-farm-distracted-driver-detection\/\nimport os\n# model.load_weights('..\/input\/best_model_1.hdf5')\n#Test Images\nbatch_index = 0\nfiles_list = os.listdir(\"..\/input\/state-farm-distracted-driver-detection\/test\/\")\ndef load_test_images(batch_size=32, src='..\/input\/state-farm-distracted-driver-detection\/test\/'):\n    global batch_index, files_list\n    imgs_list = files_list[batch_index: batch_index+batch_size]\n    batch_index += len(imgs_list)\n    batch_imgs = []\n    for img_name in imgs_list:\n        img = Image.open(src+img_name)\n        im = img.resize((224,224))\n        batch_imgs.append(np.array(im)\/255.)\n#     plt.imshow()\n#     plt.show()\n    return np.array(batch_imgs)\n","9729f2cb":"#Test Images write\nimport sys\npreds_list = np.array([])\nbatch_index=0\nbatch_size = 32\nwhile True:\n    tst_imgs = load_test_images(batch_size=batch_size)\n    if(tst_imgs.shape[0] <= 0  ):\n        print(\"Batchsize is less : \",batch_index)\n        break\n    preds = model.predict(tst_imgs)\n    print(\"\\r {},  batch_size : {}, nth_batch\/all_batch : {}\/{}\".format(preds_list.shape,batch_size, batch_index, len(files_list)),end=\"\")    \n    sys.stdout.flush()\n    if len(preds_list) == 0:\n        preds_list = np.array(preds)\n    else:\n        preds_list = np.append(preds_list, preds, axis=0)\n","64cbaf2e":"titles = \"img,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9\".split(\",\")\nnames = pd.DataFrame(files_list[:len(preds_list)])\nnames.columns=[\"img\"]\ndf = pd.DataFrame(preds_list)\ndf.columns=titles[1:]\ndf['img']=names['img']\ndf = df[titles]\ndf.tail()\ndf.to_csv('sub.csv',index=False)\n","2cb2c33f":"indices = [1,24]\nfor index in indices:\n#     display(df.iloc[index])\n    cls = np.argmax(list(df.iloc[index][1:]))\n    print(\"label : \",ac_labels[cls])\n    im_test = Image.open('..\/input\/state-farm-distracted-driver-detection\/test\/'+df.iloc[index]['img'])\n    plt.imshow(np.array(im_test))\n    plt.show()","0c65cc8d":"\ndef load_VGG16(weights_path=None, no_top=True):\n\n    input_shape = (224, 224, 3)\n\n    #Instantiate an empty model\n    img_input = Input(shape=input_shape)   # Block 1\n    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n\n    # Block 2\n    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n\n    # Block 3\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n\n    # Block 4\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n\n    # Block 5\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n    x = GlobalAveragePooling2D()(x)\n    vmodel = Model(img_input, x, name='vgg16')\n    if weights_path is not None:\n        print(\"Weights have been loaded.\")\n        vmodel.load_weights(weights_path)\n\n    return vmodel\n","a1be29d8":"vgg_model_raw = load_VGG16('..\/input\/vgg16-weights\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')\n\nvgg_model = vgg_model_raw.output\n#vgg_model = Flatten()(vgg_model)\nvgg_model = Dense(5000, activation='relu',kernel_regularizer=regularizers.l2(0.00001))(vgg_model)\n#vgg_model = Dropout(0.1)(vgg_model)\n#vgg_model = Dense(1000, activation='relu')(vgg_model)\nvgg_model = Dropout(0.1)(vgg_model)\nvgg_model = Dense(500, activation='relu',kernel_regularizer=regularizers.l2(0.00001))(vgg_model)\nvgg_model = Dropout(0.1)(vgg_model)\nvgg_model = Dense(10, activation='softmax')(vgg_model)\nvgg_m = Model(inputs=vgg_model_raw.input, outputs= vgg_model)","a5cdfc04":"vgg_m.layers[16].get_weights()","1df0266f":"vgg_m.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.SGD(0.001), metrics=['accuracy'])\n","a254b7bd":"vgg_m.summary()","ae73fea2":"\ncheckpoint = ModelCheckpoint('vgg_model.h5', save_best_only=True, verbose=1)\n\nhistory = vgg_m.fit_generator(train_generator, steps_per_epoch=len(train_generator),\n                   epochs=6,\n                   validation_data = val_generator,\n                   validation_steps=len(val_generator),\n                   callbacks=[checkpoint] )\n\n","da7efce2":"plt.plot(history.history['val_loss'])\nplt.show()\nplt.plot(history.history['loss'])","6bc254c9":"!ls\nimport os\n#model.load_weights('..\/input\/best_model_1.hdf5')\n#Test Images\nbatch_index = 0\n#files_lst = os.listdir(\"..\/input\/state-farm-distracted-driver-detection\/test\")\nfiles_list = os.listdir(\"..\/input\/state-farm-distracted-driver-detection\/test\")\ndef load_test_images(batch_size=32, src='..\/input\/state-farm-distracted-driver-detection\/test\/'):\n    global batch_index, files_list\n    imgs_list = files_list[batch_index: batch_index+batch_size]\n    batch_index += len(imgs_list)\n    batch_imgs = []\n    for img_name in imgs_list:\n        img = Image.open(src+img_name)\n        im = img.resize((224,224))\n        batch_imgs.append(np.array(im)\/255.)\n#     plt.imshow()\n#     plt.show()\n    return np.array(batch_imgs)\n","b52d5ea8":"#Test Images write\nimport sys\npreds_list = np.array([])\nbatch_index=0\nbatch_size = 32\n\nmm_raw = load_VGG16()\n\nmm_model = mm_raw.output\nmm_model = Dense(5000, activation='relu',kernel_regularizer=regularizers.l2(0.00001))(mm_model)\nmm_model = Dropout(0.1)(mm_model)\nmm_model = Dense(500, activation='relu',kernel_regularizer=regularizers.l2(0.00001))(mm_model)\nmm_model = Dropout(0.1)(mm_model)\nmm_model = Dense(10, activation='softmax')(mm_model)\nmm = Model(inputs=vgg_model_raw.input, outputs= vgg_model)\n\nmm.load_weights('vgg_model.h5')\n\n\n\nwhile True:\n    tst_imgs = load_test_images(batch_size=batch_size)\n    if(tst_imgs.shape[0] <= 0  ):\n        print(\"Batchsize is less : \",batch_index)\n        break\n    preds = mm.predict(tst_imgs)\n    print(\"\\r {},  batch_size : {}, nth_batch\/all_batch : {}\/{}\".format(preds_list.shape,batch_size, batch_index, len(files_list)),end=\"\")    \n    sys.stdout.flush()\n    if len(preds_list) == 0:\n        preds_list = np.array(preds)\n    else:\n        preds_list = np.append(preds_list, preds, axis=0)\n","9e7367cd":"\ntitles = \"img,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9\".split(\",\")\nnames = pd.DataFrame(files_list[:len(preds_list)])\nnames.columns=[\"img\"]\ndf = pd.DataFrame(preds_list)\ndf.columns=titles[1:]\ndf['img']=names['img']\ndf = df[titles]\ndf.tail()","91fb7be4":"df.to_csv('sub_VGG16.csv',index=False)","7a9dd91a":"indices = [10,24]\nfor index in indices:\n#     display(df.iloc[index])\n    cls = np.argmax(list(df.iloc[index][1:]))\n    print(\"label : \",ac_labels[cls])\n    im_test = Image.open('..\/input\/state-farm-distracted-driver-detection\/test\/'+df.iloc[index]['img'])\n    plt.imshow(np.array(im_test))\n    plt.show()\n","21edcf83":"!cat sub_VGG16.csv | head -10","aa6057e6":"- In the following code block, we have added 3 Dense layers with dropout layers on top of VGG16 CNN model.","8206c9e8":"## Dataset Exploration\n- Following code block explores and reveals the dataset","e7bd169b":"## Data Visualization\nFollowing code block. displays images and their respective labels of training\/ validation dataset","1bc87471":"#### Loading test dataset into memory and predict on test images\n- Following code block contains a method, that loads all test images into memory by batches, each batch with 32 images when it is called each time","3a6ee6e9":"## Convert the predicted output to submittable output file","dc279733":"### Compile CNN from scratch Model","d077ee86":"- Following method gives a batch of 32 images at each call, just like a generator","c8d807d9":"In the following code, we are loading only 32 images at once into memory and performing all the preprocessing operations on loaded images. The flow_from_directory loads a defined set of images from the location of images, instead of loading all images at once into memory.\n\n- train_generator contains training set\n- val_generator contains validation set","99660726":"<a id=\"step0\"><\/a>\n## Step 0: Import Datasets\n\n### Import Driver Dataset\n\nIn the following code cell, we create a instance of ImageDataGenerator which does all preprocessing operations on images that we are going to feed to our CNN","6029df09":"### (IMPLEMENTATION) Train the Model\n\nThe model is trained in the code cell below. Model checkpointing is used to save the model that attains the best validation loss.\n","c9f5a355":"# Machine Learning Engineer Nanodegree\n\n## Capstone Project\n\n## Project: Write an Algorithm for Distracted Driver Detection \n\n---\n\n\n\n>**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n\n\n\n---\n\n### The Road Ahead\n\nThe notebook is broken into separate steps as shown below.\n\n* [Step 0](#step0): Import Datasets\n* [Step 1](#step1): Create and train a CNN to Classify Driver Images (from Scratch)\n* [Step 2](#step3): Train a CNN with Transfer Learning (Using Fine-tuned VGG16)\n* [Step 3](#step4): Kaggle Results\n\n\n","d7b3e066":"---\n<a id='step1'><\/a>\n## Step 1: Create a CNN to Classify Driver Images (from Scratch)\n\n\n\nA CNN is created to classify driver images.  At the end of the code cell block, the layers of the model are summarized by executing the line:\n    \n        model.summary()\n\nWe have created 6 convolutional layers with 1 max pooling layer and 1 GlobalAveragePooling in between. Filters were increased from 8 to 512 in total convolutional layers. Also dropout was used along with Global average pooling layer before using the fully connected layer. Number of nodes in the last fully connected layer were setup as 10 along with softmax activation function. ReLU activation function was used for all other layers.\n\n6 convolutional layers were used to learn hierarchy of high level features. Max pooling layer is added to reduce the dimensionality. Global Average Pooling layer is added to reduce the dimensionality as well as the matrix to row vector. This is because fully connected layer only accepts row vector. Dropout layers were added to reduce overfitting and ensure that the network generalizes well. The last fully connected layer with softmax activation function is added to obtain probabilities of the prediction.\n","86ddd2da":"### Compile the vgg16 model with categorical crossentropy as loss function and SGD as optimizer","7088e3c3":"- Create a Checkpoint to save best weights, when loss in improvised\n- Train the model for 6 epochs","4b200e0b":"## Predictions by scratch CNN on some sample images\n- Seems like the predictions are not so accurate because of less training","3e459c71":"---\n<a id=\"step2\"><\/a>\n##  Step 2: Train a CNN with Transfer Learning (Using Fine-tuned VGG16 Model)\n\n- In the following steps, we are going to load a VGG16 model, without top Fully connected layers, with its trained weights\n- Then we are going add Fully Connected Layers on top of GlobalAveragePooling layer with Dropout layers in between\n- The following code block contains a method load_VGG16, which loads VGG16 model with weights loaded when weights file location is given","19982859":"- In the following code block, we are plotting the loss value of training and validation to see whether the model is learning or not","055073bb":"- Following code block predicts on test images by loading 32 images a batch using load_test_images function"}}