{"cell_type":{"8a9852aa":"code","d35c81ae":"code","46199f26":"code","b4dde1d8":"code","fef2807a":"code","1464ac6b":"code","d1f94f5c":"code","6dbe9936":"code","32e47740":"code","a4e07bc4":"code","dd494d09":"code","e62407ca":"code","4450c90d":"code","a93a2e06":"code","1c26a380":"code","b3f321ee":"code","1265ec3b":"code","a25b9eb4":"code","d6691eda":"code","3fa767b9":"code","404a7066":"code","f8a64b6f":"code","894a529d":"markdown","a7d430fe":"markdown","656aef3b":"markdown","08a15338":"markdown","0010bd01":"markdown","31b6e867":"markdown","08002770":"markdown","52aac781":"markdown","af8d4854":"markdown","fe8d011c":"markdown","d9ebcf45":"markdown","238e1242":"markdown","de283108":"markdown","d7f3e079":"markdown","4a00a6eb":"markdown","5f133c5c":"markdown","de087afc":"markdown","500b7bcd":"markdown","4c944629":"markdown","947ecb80":"markdown","d6b6357d":"markdown","831f27ce":"markdown","343bd810":"markdown","d0c5beed":"markdown"},"source":{"8a9852aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d35c81ae":"import pandas as pd\nimport numpy as np\n\n# Import Plotting Libararies\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Import Data Preprocessing Libraries \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Machine Learning Models\nfrom sklearn import svm  \nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb\n\n# Model Evaluation Libraries\nfrom sklearn.metrics import classification_report, confusion_matrix","46199f26":"train =  pd.read_csv('\/kaggle\/input\/imdb-dataset-of-50k-movie-translated-urdu-reviews\/imdb_urdu_reviews_train.csv')\ntest =   pd.read_csv('\/kaggle\/input\/imdb-dataset-of-50k-movie-translated-urdu-reviews\/imdb_urdu_reviews_test.csv')\nprint('Shape of Training Set ' , train.shape,'\\nShape of Testing Set ', test.shape)","b4dde1d8":"data =  pd.concat([train, test]).reset_index(drop=True)\nprint(data.shape)","fef2807a":"df =  data.copy()","1464ac6b":"df.head()","d1f94f5c":"sns.countplot( x = 'sentiment', data = df );","6dbe9936":"le = LabelEncoder()\nle.fit(df['sentiment'])\ndf['encoded_sentiments'] = le.transform(df['sentiment'])","32e47740":"df.head()","a4e07bc4":"X_train, X_test, Y_train, Y_test = train_test_split(df['review'], df['encoded_sentiments'], test_size = 0.30, random_state = 7)","dd494d09":"print('Shape of X_train', X_train.shape)\nprint('Shape of X_test', X_test.shape)\nprint('Shape of Y_train', Y_train.shape)\nprint('Shape of Y_test', Y_test.shape)","e62407ca":"max_feature_num = 50000\nvectorizer = TfidfVectorizer(max_features=max_feature_num)\ntrain_vecs = vectorizer.fit_transform(X_train)\ntest_vecs = TfidfVectorizer(max_features=max_feature_num, vocabulary=vectorizer.vocabulary_).fit_transform(X_test)","4450c90d":"\ndef SVM_classifier(train_vecs, Y_train, test_vecs, Y_test):\n    # Training\n    SVM = svm.LinearSVC(max_iter=100)\n    SVM.fit(train_vecs, Y_train)\n\n    # Testing\n    test_predictionSVM = SVM.predict(test_vecs)\n    return classification_report(test_predictionSVM, Y_test), confusion_matrix(test_predictionSVM, Y_test)","a93a2e06":"def LR_classifier(train_vecs, Y_train, test_vecs, Y_test):\n    # Training\n    LR = LogisticRegression()\n    LR.fit(train_vecs, Y_train)\n\n    # testing\n    test_predictionLR = LR.predict(test_vecs)\n    return classification_report(test_predictionLR, Y_test) , confusion_matrix(test_predictionLR, Y_test)","1c26a380":"def DT_classifier(train_vecs, Y_train, test_vecs, Y_test):\n    # Training\n    DT = DecisionTreeClassifier(max_depth = 9, random_state = 23 )\n    DT.fit(train_vecs, Y_train)\n\n    # Testing\n    test_predictionDT = DT.predict(test_vecs)\n    return classification_report(test_predictionDT, Y_test), confusion_matrix(test_predictionDT, Y_test) ","b3f321ee":"def XGB_classifier(train_vecs, Y_train, test_vecs, Y_test):\n    # Training\n    XGB = xgb.XGBClassifier(colsample_bytree = 0.2, learning_rate = 0.01, n_estimators = 100)\n    XGB.fit(train_vecs, Y_train)\n\n    # Testing\n    test_predictionXGB = XGB.predict(test_vecs)\n    return classification_report(test_predictionXGB, Y_test), confusion_matrix(test_predictionXGB, Y_test)  ","1265ec3b":"def RF_classifier(train_vecs, Y_train, test_vecs, Y_test):\n    # Training\n    RF = RandomForestClassifier(n_estimators = 450, max_depth=9, random_state=43)\n    RF.fit(train_vecs, Y_train)\n\n    # Testing\n    test_predictionRF = RF.predict(test_vecs)\n    return classification_report(test_predictionRF, Y_test), confusion_matrix(test_predictionRF, Y_test)\n","a25b9eb4":"class_report , conf_matrix = SVM_classifier(train_vecs, Y_train, test_vecs, Y_test)\nprint('Results of SVM Classifier on TF-IDF Vectorizer')\nprint(class_report)\nprint(conf_matrix)","d6691eda":"class_report , conf_matrix = LR_classifier(train_vecs, Y_train, test_vecs, Y_test)\nprint('Results of Logistic Regression Classifier on TF-IDF Vectorizer')\nprint(class_report)\nprint(conf_matrix)","3fa767b9":"class_report , conf_matrix = DT_classifier(train_vecs, Y_train, test_vecs, Y_test)\nprint('Results of Decision Tree Classifier on TF-IDF Vectorizer')\nprint(class_report)\nprint(conf_matrix)","404a7066":"class_report , conf_matrix = XGB_classifier(train_vecs, Y_train, test_vecs, Y_test)\nprint('Results of Xgboost Classifier on TF-IDF Vectorizer')\nprint(class_report)\nprint(conf_matrix)","f8a64b6f":"class_report , conf_matrix = RF_classifier(train_vecs, Y_train, test_vecs, Y_test)\nprint('Results of Random Forest Classifier on TF-IDF Vectorizer')\nprint(class_report)\nprint(conf_matrix)","894a529d":"### Import Libraries","a7d430fe":"Lets make a copy of the dataset to move forward so we didnt have to load dataset again and again.","656aef3b":"We now check the shape of Taining and Testing Splits.","08a15338":"##### Random Forest Classifer","0010bd01":"##### Label Encoding\n\nNow, we will encode the label column using label Encoder from Sklearn.","31b6e867":"##### Split Dataset into Training and Testing","08002770":"we can see that label Encoder have assigned the 1 to potive sentiment and 0 to negative sentiment.","52aac781":"##### Logistic Regression","af8d4854":"Now we will check the distribution of classes in dataset.","fe8d011c":"Lets see, how data looks like.","d9ebcf45":"### Read Data","238e1242":"### Future Work\n\nIn coming days, I will preprocess this data using UrduHack Library and will retrain the models.\nI will also build Word2Vector and Glove Embeddings for this data and train Deep Learning Models.","de283108":"we have two columns in dataset which are review and sentiment:\n* Review is review by some user\n* Sentiment is the label for the review.","d7f3e079":"### Conclusion\n\nWe have applied multiple algorithms on this dataset and achieved maximum accuracy of 88 percent using SVM. SVM is very powerful model and normally works good on text data. The tree based algortihms didnt work good on this data.","4a00a6eb":"##### Xgboost Classifier","5f133c5c":"### Results - Machine Learning Models on TF-IDF Vectorizer Features","de087afc":"##### TF-IDF Vectorizer\nTF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.","500b7bcd":"### Machine Learning Modeling\nWe will apply multiple algorithms to this TF-IDF features set and conclude what is best accuracy we achieved using any Machine Learning Model.","4c944629":"Concatenate Train and Testing Set, So we can handle data easily.","947ecb80":"##### Support Vector Machines","d6b6357d":"##### Decision Tree Classifier","831f27ce":"We can see that there are only two classes in our dataset: \n* Positive means the review holds positive sentiment.\n* Negative means the review holds negative sentiment.\n\nAlso the class is very balanced. So, it will be easy for us to build any model.\n","343bd810":"We have now 50,000 records available in our dataset. The size of dataset is good and we can build very good predictive model using this data.","d0c5beed":"### Data Preprocessing"}}