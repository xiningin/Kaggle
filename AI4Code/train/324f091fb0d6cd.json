{"cell_type":{"92d3a79c":"code","fe566aef":"code","90544bb6":"code","d5a77274":"code","fd3b999a":"code","3d55e050":"code","082668ca":"code","b89e5b94":"code","7f3cff48":"code","b18d6f6b":"code","da9942de":"code","3a24b0f6":"code","bab3f764":"code","ad09c5e5":"code","cb49206f":"code","ed3dd171":"code","43ff50e2":"code","ca2d2a36":"code","812264d8":"code","4783bef6":"code","6e481d54":"code","5a025a0e":"code","3fae6894":"code","ef52a6bb":"code","a82c5fe4":"code","2eb4141f":"code","5f808444":"code","a501f90d":"code","13b64f39":"code","0eb91f22":"markdown","5b4ed51c":"markdown"},"source":{"92d3a79c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fe566aef":"import plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom catboost import CatBoostClassifier\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.model_selection import train_test_split\nfrom pylab import plot, show, subplot, specgram, imshow, savefig\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n%matplotlib inline","90544bb6":"train = pd.read_csv(\"\/kaggle\/input\/janatahack-crosssell-prediction\/train.csv\")\ntrain.head()","d5a77274":"test = pd.read_csv(\"\/kaggle\/input\/janatahack-crosssell-prediction\/test.csv\")\ntest.head()","fd3b999a":"sub = pd.read_csv('\/kaggle\/input\/janatahack-crosssell-prediction\/sample_submission.csv')\nsub.head()","3d55e050":"train.info()","082668ca":"train.isna().sum()","b89e5b94":"train.describe()","7f3cff48":"train.dtypes","b18d6f6b":"print(train.shape)\nprint(test.shape)","da9942de":"def plot_feature_importance(importance,names,model_type):\n\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n\n    #Define size of bar plot\n    plt.figure(figsize=(10,8))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + ' FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","3a24b0f6":"le = LabelEncoder()\ntrain[\"Gender\"] = le.fit_transform(train[\"Gender\"])\ntrain[\"Vehicle_Age\"] = le.fit_transform(train[\"Vehicle_Age\"])\ntrain[\"Vehicle_Damage\"] = le.fit_transform(train[\"Vehicle_Damage\"])\n\ntest[\"Gender\"] = le.fit_transform(test[\"Gender\"])\ntest[\"Vehicle_Age\"] = le.fit_transform(test[\"Vehicle_Age\"])\ntest[\"Vehicle_Damage\"] = le.fit_transform(test[\"Vehicle_Damage\"])","bab3f764":"rf_model = RandomForestClassifier().fit(train.drop([\"id\", \"Response\"],axis=1),train[\"Response\"])\nplot_feature_importance(rf_model.feature_importances_,train.drop([\"id\", \"Response\"],axis=1).columns,'RANDOM FOREST')","ad09c5e5":"train.dtypes","cb49206f":"train.head()","ed3dd171":"sns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.heatmap(data=train.drop(\"id\", axis=1).corr().round(2), annot = True)\nplt.show()","43ff50e2":"sns.set(rc={'figure.figsize':(15,8)})\nsns.lineplot(x='Age', y='Vintage', data=train)\nplt.show()","ca2d2a36":"sns.set(rc={'figure.figsize':(19,8)})\nsns.distplot(train['Age'], kde=True)\nplt.show()","812264d8":"def show_donut_plot(col): #donut plot function\n    \n    rating_data =train.groupby(col)[['id']].count().head(10)\n    plt.figure(figsize = (12, 8))\n    plt.pie(rating_data[['id']], autopct = '%1.0f%%', startangle = 140, pctdistance = 1.1, shadow = True)\n\n    # create a center circle for more aesthetics to make it better\n    gap = plt.Circle((0, 0), 0.5, fc = 'white')\n    fig = plt.gcf()\n    fig.gca().add_artist(gap)\n    \n    plt.axis('equal')\n    \n    cols = []\n    for index, row in rating_data.iterrows():\n        cols.append(index)\n    plt.legend(cols)\n    \n    plt.title('Donut Plot: Response Proportion for Cross-Sale ', loc='center')\n    plt.show()","4783bef6":"show_donut_plot('Response')","6e481d54":"print(train[train.Response == 1].shape)\nprint(train[train.Response == 0].shape)","5a025a0e":"from sklearn.model_selection import StratifiedKFold,KFold\n# Set up folds\nK = 5\nkf = KFold(n_splits = K, random_state = 7, shuffle = True)\nskf = StratifiedKFold(n_splits = K, random_state = 7, shuffle = True)","3fae6894":"MAX_ROUNDS = 1000\nOPTIMIZE_ROUNDS = False\n#LEARNING_RATE = 0.1","ef52a6bb":"train.columns","a82c5fe4":"X = train.drop(columns=['id','Response'],axis=1)\ny = train['Response']\nX_test = test.drop(columns='id',axis=1)\ny_valid_pred = 0*y\ny_test_pred = 0\naccuracy = 0\nresult={}\n#specifying categorical variables indexes\ncat_columns = ['Gender','Vehicle_Age','Vehicle_Damage', 'Driving_License', 'Previously_Insured']\n#fitting catboost classifier model\nj=1\nmodel = CatBoostClassifier(n_estimators=MAX_ROUNDS,verbose=False)\nfor i, (train_index, test_index) in enumerate(kf.split(train)):\n\n#for train_index, test_index in skf.split(X, y):  \n    # Create data for this fold\n    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n    X_train, X_valid = X.iloc[train_index,:], X.iloc[test_index,:]\n    print( \"\\nFold \", j)\n    #print( \"\\nFold \", i)\n    \n    # Run model for this fold\n    if OPTIMIZE_ROUNDS:\n        fit_model = model.fit( X_train, y_train, \n                               eval_set=[X_valid, y_valid],cat_features=cat_columns,\n                               use_best_model=True\n                             )\n        print( \"  N trees = \", model.tree_count_ )\n    else:\n        fit_model = model.fit( X_train, y_train,cat_features=cat_columns )\n        \n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n    y_valid_pred.iloc[test_index] = pred.reshape(-1)\n    print(accuracy_score(y_valid,pred))\n    accuracy+=accuracy_score(y_valid,pred)\n    # Accumulate test set predictions\n    y_test_pred += fit_model.predict(X_test)\n    result[j]=fit_model.predict(X_test)\n    j+=1\nresults = y_test_pred \/ K  # Average test set predictions\nprint(accuracy\/5)","2eb4141f":"d = pd.DataFrame()\nfor i in range(1, 6):\n    d = pd.concat([d,pd.DataFrame(result[i])],axis=1)\nd.columns=['1','2','3','4','5']","5f808444":"re = d.mode(axis=1)[0]","a501f90d":"sub.Response = re\nsub.to_csv('submission.csv',index = False)","13b64f39":"sub","0eb91f22":"We can infer from the dataset that nearly 85% of the customers are interested in Vehicle insurance.","5b4ed51c":"***Top 5 features are,***\n* Vintage\n* Annual_Premium\n* Age\n* Region_Code\n* Vehicle_Damage"}}