{"cell_type":{"357144a0":"code","8060b225":"code","64d2a6ee":"code","7ab45bec":"code","11f12fe0":"code","626c1ba4":"code","3ac96764":"code","b8170756":"code","84291c4c":"code","352e13ea":"code","e908c6bf":"code","5b52f6c4":"code","cd0fdaf4":"code","a5c74fe5":"code","55817d9e":"code","87a36f9c":"code","01e1471d":"code","cce1b82d":"code","6ed733fb":"code","dbb0fa19":"code","717fe14c":"code","6bf8e9e7":"code","5ee9bc4f":"code","cb222a2d":"code","5970f617":"code","6d2afaf4":"code","b34a63a0":"code","85d9fa4b":"code","21a7d268":"code","8a99dfe0":"markdown","f6cebbd1":"markdown","9fcc3f86":"markdown","44fcb733":"markdown","dca90b19":"markdown","f506d49c":"markdown","5b17c1a2":"markdown","1706678b":"markdown","6a4b509b":"markdown","67d27971":"markdown","816d3ee7":"markdown","bcf15b1a":"markdown","2e4318ef":"markdown","06db761c":"markdown","2bf29546":"markdown","e1ce7c60":"markdown","6883bc68":"markdown","288d5405":"markdown","efa3411e":"markdown","87d722f6":"markdown","502a0caf":"markdown","c51f03f6":"markdown","4077cfd7":"markdown","9c5aa610":"markdown","6968ff75":"markdown","ba769f11":"markdown","db25bb5e":"markdown","b398c3ed":"markdown","ed3c0d49":"markdown","bb4d7704":"markdown","52a1d1b6":"markdown"},"source":{"357144a0":"import pandas as pd\nimport numpy as np","8060b225":"data=pd.read_csv(\"..\/input\/paysim1\/PS_20174392719_1491204439457_log.csv\")","64d2a6ee":"data.info()","7ab45bec":"data.isnull().sum()","11f12fe0":"from scipy import stats","626c1ba4":"z = np.abs(stats.zscore(data[['amount','oldbalanceOrg','newbalanceOrig','oldbalanceDest','newbalanceDest']]))\nprint(z)","3ac96764":"threshold = 3\nprint(np.where(z > 3))","b8170756":"print(z[84][4])","84291c4c":"data_new = data[(z < 3).all(axis=1)]","352e13ea":"data.shape","e908c6bf":"data_new.shape","5b52f6c4":"dummy_type=pd.get_dummies(data_new['type'])\ndummy_type.info()","cd0fdaf4":"data_new=pd.concat([data_new,dummy_type],axis=1)","a5c74fe5":"data_new['isFraud'].value_counts()","55817d9e":"fraud= data_new[data_new.isFraud == 1]\nlegit= data_new[data_new.isFraud == 0]","87a36f9c":"data_new.groupby('isFraud').mean()","01e1471d":"from random import sample\nlegit_sample = legit.sample(n=6485)","cce1b82d":"balanced_data= pd.concat([legit_sample, fraud], axis= 0)","6ed733fb":"balanced_data['isFraud'].value_counts()","dbb0fa19":"import seaborn as sns\ncorrMatrix=balanced_data.corr()\n\nax = sns.heatmap(\n    corrMatrix, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","717fe14c":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()","6bf8e9e7":"from sklearn.model_selection import train_test_split\n\nX = balanced_data[['step','amount','oldbalanceOrg','oldbalanceDest','CASH_IN','CASH_OUT','DEBIT','PAYMENT','TRANSFER']]\nY = balanced_data[\"isFraud\"]\n\nx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.30, random_state = 0)","5ee9bc4f":"lr.fit(x_train,y_train)","cb222a2d":"lr.score(x_train,y_train)","5970f617":"lr.score(x_test,y_test)","6d2afaf4":"x = data_new[['step','amount','oldbalanceOrg','oldbalanceDest','CASH_IN','CASH_OUT','DEBIT','PAYMENT','TRANSFER']]\ny = data_new[\"isFraud\"]\n\nX_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size = 0.30, random_state = 0)","b34a63a0":"lr.fit(X_train,Y_train)","85d9fa4b":"lr.score(X_train,Y_train)","21a7d268":"lr.score(X_test,Y_test)","8a99dfe0":" The first array contains the list of row numbers and second array respective column numbers, which mean z[84][4] have a Z-score higher than 3.","f6cebbd1":"First of all, we will try to find out if there is any NULL or NaN values in the dataset features","9fcc3f86":"## Now let us check the correlation amoung the features for feature selection","44fcb733":"### For this task, we will be using Logistic Regression model.\n\nLogistic regression is a statistical analysis method used to predict a data value based on prior observations of a data set. Logistic regression has become an important tool in the discipline of machine learning. The approach allows an algorithm being used in a machine learning application to classify incoming data based on historical data. As more relevant data comes in, the algorithm should get better at predicting classifications within data sets.\n\nA logistic regression model predicts a dependent data variable by analyzing the relationship between one or more existing independent variables.","dca90b19":"As we can see, there are no null values present in the dataset, we can move further towards the outliers.","f506d49c":"## Data Cleaning","5b17c1a2":"Now, we are going to split the data set into train and test data with a divion of 70% of 30% for train and test respectively","1706678b":"Lets have a look at the basic information about the features in dataset, so accordingly we will apply manipulations to them in further steps","6a4b509b":"### Veryfying Data Distributon and Balancing dataset","67d27971":"#        Executing the process for proactive detection of fraud","816d3ee7":"So, above code removed 272031 rows from the dataset i.e. outliers have been removed.","bcf15b1a":"### Treating Missing values or NaN values","2e4318ef":"At the beginning, we will import all the basic libraries which all data science tasks need, further we will add more libraries whenever we encounter the need of them","06db761c":"For this task, we are going to use the Z -Score method.","2bf29546":"Lets find out the score of our model over training and testing data set respectively","e1ce7c60":"we saw how one can detect the outlier using Z-score but now we want to remove or filter the outliers and get the clean data. This can be done with just one line code as we have already calculated the Z-score.","6883bc68":"Looking the code and the output above, it is difficult to say which data point is an outlier. Let\u2019s try and define a threshold to identify an outlier.","288d5405":"### Converting useful categorical data into numeric data","efa3411e":"Now as we can see, we have successfully balanced the dataset","87d722f6":"We can see, we might be having low accuracy rate with the balanced data.\nFor once, lets also have a look at the unbalanced data with the same method","502a0caf":"### Treating Outliers ","c51f03f6":"As we can clearly see, the unbalanced data have a better accuracy rate. We can use any of them depending upon the needs","4077cfd7":"Now let us fit the model with training data set","9c5aa610":"Now, Let us import the dataset","6968ff75":"As we can see, the data is highly imbalanced, so we will first have to balane the data","ba769f11":"So first of all, what is Z-Score method -\n\nThe Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured.\nThe intuition behind Z-score is to describe any data point by finding their relationship with the Standard Deviation and Mean of the group of data points. Z-score is finding the distribution of data where mean is 0 and standard deviation is 1 i.e. normal distribution.\n\nWe will use Z-score function defined in scipy library to detect the outliers.","db25bb5e":"## Fraud Detection model","b398c3ed":"It is crucial to have balanced class distribution, i.e., there should be no significant difference between positive and negative classes (commonly negative classes are more than positives in the life science field). The models trained on datasets with imbalanced class distribution tend to be biased and show poor performance toward minor class.","ed3c0d49":"We will take all the quantitative features for Z-Score function, except for \"step\" which is unrelated to the happening of fraud case, \"isFraud\" which is target variable and \"isFlaggedFraud\" which we will not be using in the model as it is not helpful to us.","bb4d7704":"As we can see, \"oldbalanceOrg\" and \"newbalanceOrig\" are higly correlated, similarly \"oldbalanceDest\" and \"newbalanceDest\" are also highly correlated, so we will only take one from each pair as a predictor.  ","52a1d1b6":"Now, the outliers have been removed, lets compare both datasets of before and after the removal of outliers respectively"}}