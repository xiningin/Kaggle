{"cell_type":{"8e7a26bb":"code","c64b8a3c":"code","e226c111":"code","a16e421f":"code","320ef281":"code","c05d36b0":"code","41a960f0":"code","d081cdfb":"code","411903bf":"code","253af122":"code","4fc0adcb":"code","880de5af":"code","832d7471":"code","34bb5d68":"code","a1b4fb7e":"code","a2bfee8b":"code","7edb69aa":"code","b2a62348":"code","798f27d0":"code","0bd4da79":"code","af1f50df":"code","b5c62c02":"code","63dca3ef":"code","afeeed1a":"code","2d29794b":"code","8c47637b":"code","0e1e45d9":"code","8696f014":"code","0cde67d6":"code","024943b0":"code","ec1f0193":"code","107ec6eb":"code","2a2e59ba":"code","24db50a6":"code","4cd6e378":"code","5c05be7c":"code","c2eb2b68":"code","3437fb14":"code","2144505f":"code","e3d34386":"markdown","dd3b78d7":"markdown","53cd8365":"markdown","f218c9b7":"markdown","5594c845":"markdown","522b8001":"markdown","95966f07":"markdown","110c2d32":"markdown"},"source":{"8e7a26bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c64b8a3c":"# if you keras is not using tensorflow as backend set \"KERAS_BACKEND=tensorflow\" use this command\nfrom keras.utils import np_utils \nfrom keras.datasets import mnist \nimport seaborn as sns\nfrom keras.initializers import RandomNormal","e226c111":"%matplotlib notebook\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\n# this function is used to update the plots for each epoch and error\ndef plt_dynamic(x, vy, ty, ax, colors=['b']):\n    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n    ax.plot(x, ty, 'r', label=\"Train Loss\")\n    plt.legend()\n    plt.grid()\n    fig.canvas.draw()","a16e421f":"# the data, shuffled and split between train and test sets \n(X_train, y_train), (X_test, y_test) = mnist.load_data()","320ef281":"print(\"Shape of data point :\", X_train.shape)","c05d36b0":"print(\"Number of training examples :\", X_train.shape[0], \"and each image is of shape (%d, %d)\"%(X_train.shape[1], X_train.shape[2]))\nprint(\"Number of testing examples :\", X_test.shape[0], \"and each image is of shape (%d, %d)\"%(X_test.shape[1], X_test.shape[2]))","41a960f0":"# if you observe the input shape its 2 dimensional vector\n# for each image we have a (28*28) vector\n# we will convert the (28*28) vector into single dimensional vector of 1 * 784 \n\nX_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]) \nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])","d081cdfb":"# after converting the input images from 3d to 2d vectors\n\nprint(\"Number of training examples :\", X_train.shape[0], \"and each image is of shape (%d)\"%(X_train.shape[1]))\nprint(\"Number of training examples :\", X_test.shape[0], \"and each image is of shape (%d)\"%(X_test.shape[1]))","411903bf":"# An example data point\nprint(X_train[0])","253af122":"# if we observe the above matrix each cell is having a value between 0-255\n# before we move to apply machine learning algorithms lets try to normalize the data\n# X => (X - Xmin)\/(Xmax-Xmin) = X\/255\n\nX_train = X_train\/255\nX_test = X_test\/255","4fc0adcb":"# example data point after normlizing\nprint(X_train[0])","880de5af":"# here we are having a class number for each image\nprint(\"Class label of first image :\", y_train[0])\n\n# lets convert this into a 10 dimensional vector\n# ex: consider an image is 5 convert it into 5 => [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n# this conversion needed for MLPs \n\nY_train = np_utils.to_categorical(y_train, 10) \nY_test = np_utils.to_categorical(y_test, 10)\n\nprint(\"After converting the output into a vector : \",Y_train[0])","832d7471":"from keras.models import Sequential\nfrom keras.layers import Dense,Activation ","34bb5d68":"# some model parameters\n\noutput_dim = 10\ninput_dim = X_train.shape[1]\n\nbatch_size = 128 \nnb_epoch = 20","a1b4fb7e":"# start building a model\nmodel = Sequential()\n\n# The model needs to know what input shape it should expect. \n# For this reason, the first layer in a Sequential model \n# (and only the first, because following layers can do automatic shape inference)\n# needs to receive information about its input shape. \n# you can use input_shape and input_dim to pass the shape of input\n\n# output_dim represent the number of nodes need in that layer\n# here we have 10 nodes\n\nmodel.add(Dense(output_dim, input_dim=input_dim, activation='softmax'))","a2bfee8b":"# Before training a model, you need to configure the learning process, which is done via the compile method\n\n# It receives three arguments:\n# An optimizer. This could be the string identifier of an existing optimizer , https:\/\/keras.io\/optimizers\/\n# A loss function. This is the objective that the model will try to minimize., https:\/\/keras.io\/losses\/\n# A list of metrics. For any classification problem you will want to set this to metrics=['accuracy'].  https:\/\/keras.io\/metrics\/\n\n\n# Note: when using the categorical_crossentropy loss, your targets should be in categorical format \n# (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros except \n# for a 1 at the index corresponding to the class of the sample).\n\n# that is why we converted out labels into vectors\n\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Keras models are trained on Numpy arrays of input data and labels. \n# For training a model, you will typically use the  fit function\n\n# fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, \n# validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, \n# validation_steps=None)\n\n# fit() function Trains the model for a fixed number of epochs (iterations on a dataset).\n\n# it returns A History object. Its History.history attribute is a record of training loss values and \n# metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n\n# https:\/\/github.com\/openai\/baselines\/issues\/20\n\nhistory = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","7edb69aa":"score = model.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","b2a62348":"# Multilayer perceptron\n\nmodel_sigmoid = Sequential()\nmodel_sigmoid.add(Dense(512, activation='sigmoid', input_shape=(input_dim,)))\nmodel_sigmoid.add(Dense(128, activation='sigmoid'))\nmodel_sigmoid.add(Dense(output_dim, activation='softmax'))\n\nmodel_sigmoid.summary()","798f27d0":"model_sigmoid.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_sigmoid.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","0bd4da79":"score = model_sigmoid.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","af1f50df":"w_after = model_sigmoid.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","b5c62c02":"# Multilayer perceptron\n\n# https:\/\/arxiv.org\/pdf\/1707.09725.pdf#page=95\n# for relu layers\n# If we sample weights from a normal distribution N(0,\u03c3) we satisfy this condition with \u03c3=\u221a(2\/(ni). \n# h1 =>  \u03c3=\u221a(2\/(fan_in) = 0.062  => N(0,\u03c3) = N(0,0.062)\n# h2 =>  \u03c3=\u221a(2\/(fan_in) = 0.125  => N(0,\u03c3) = N(0,0.125)\n# out =>  \u03c3=\u221a(2\/(fan_in+1) = 0.120  => N(0,\u03c3) = N(0,0.120)\n\nmodel_relu = Sequential()\nmodel_relu.add(Dense(512, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_relu.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_relu.add(Dense(output_dim, activation='softmax'))\n\nmodel_relu.summary()","63dca3ef":"model_relu.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_relu.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","afeeed1a":"score = model_relu.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","2d29794b":"w_after = model_relu.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","8c47637b":"model_relu = Sequential()\nmodel_relu.add(Dense(512, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_relu.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_relu.add(Dense(output_dim, activation='softmax'))\n\nprint(model_relu.summary())\n\nmodel_relu.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_relu.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","0e1e45d9":"score = model_relu.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","8696f014":"w_after = model_relu.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","0cde67d6":"# Multilayer perceptron\n\n# https:\/\/intoli.com\/blog\/neural-network-initialization\/ \n# If we sample weights from a normal distribution N(0,\u03c3) we satisfy this condition with \u03c3=\u221a(2\/(ni+ni+1). \n# h1 =>  \u03c3=\u221a(2\/(ni+ni+1) = 0.039  => N(0,\u03c3) = N(0,0.039)\n# h2 =>  \u03c3=\u221a(2\/(ni+ni+1) = 0.055  => N(0,\u03c3) = N(0,0.055)\n# h1 =>  \u03c3=\u221a(2\/(ni+ni+1) = 0.120  => N(0,\u03c3) = N(0,0.120)\n\nfrom keras.layers.normalization import BatchNormalization\n\nmodel_batch = Sequential()\n\nmodel_batch.add(Dense(512, activation='sigmoid', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_batch.add(BatchNormalization())\n\nmodel_batch.add(Dense(128, activation='sigmoid', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_batch.add(BatchNormalization())\n\nmodel_batch.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_batch.summary()","024943b0":"model_batch.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_batch.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","ec1f0193":"score = model_batch.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","107ec6eb":"w_after = model_batch.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","2a2e59ba":"# https:\/\/stackoverflow.com\/questions\/34716454\/where-do-i-call-the-batchnormalization-function-in-keras\n\nfrom keras.layers import Dropout\n\nmodel_drop = Sequential()\n\nmodel_drop.add(Dense(512, activation='sigmoid', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_drop.add(BatchNormalization())\nmodel_drop.add(Dropout(0.5))\n\nmodel_drop.add(Dense(128, activation='sigmoid', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_drop.add(BatchNormalization())\nmodel_drop.add(Dropout(0.5))\n\nmodel_drop.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_drop.summary()","24db50a6":"model_drop.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","4cd6e378":"score = model_drop.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","5c05be7c":"w_after = model_drop.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","c2eb2b68":"from keras.optimizers import Adam,RMSprop,SGD\ndef best_hyperparameters(activ):\n\n    model = Sequential()\n    model.add(Dense(512, activation=activ, input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\n    model.add(Dense(128, activation=activ, kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\n    model.add(Dense(output_dim, activation='softmax'))\n\n\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n    \n    return model","3437fb14":"# https:\/\/machinelearningmastery.com\/grid-search-hyperparameters-deep-learning-models-python-keras\/\n\nactiv = ['sigmoid','relu']\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = KerasClassifier(build_fn=best_hyperparameters, epochs=nb_epoch, batch_size=batch_size, verbose=0)\nparam_grid = dict(activ=activ)\n\n# if you are using CPU\n# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n# if you are using GPU dont use the n_jobs parameter\n\ngrid = GridSearchCV(estimator=model, param_grid=param_grid)\ngrid_result = grid.fit(X_train, Y_train)","2144505f":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","e3d34386":"# Hyper-parameter tuning of Keras models using Sklearn ","dd3b78d7":"# MLP + ReLU + ADAM ","53cd8365":"# Softmax classifier ","f218c9b7":"# **Keras -- MLPs on MNIST**","5594c845":"# MLP + Sigmoid activation + SGDOptimizer ","522b8001":"# MLP + Batch-Norm on hidden Layers + AdamOptimizer <\/2> ","95966f07":"# MLP + Dropout + AdamOptimizer ","110c2d32":"# MLP + ReLU +SGD"}}