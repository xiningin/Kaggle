{"cell_type":{"5ff4cae1":"code","97d04db6":"code","6c11ba89":"code","52b50b05":"code","525e0db9":"code","43104e48":"code","f4746c43":"code","3918777c":"code","5e5a5ef2":"code","26298d66":"code","ca4618e8":"code","5a60eea6":"code","db34d73c":"code","9f3018ca":"code","d311f65e":"code","b6f7990d":"code","83415742":"code","10499d10":"code","aced8f80":"code","01a6c382":"code","c0833458":"code","5c2b12a7":"code","dab27bcf":"code","864ba268":"code","9e325c07":"code","1d5dd3c2":"code","66c5507d":"code","f647d45f":"code","61b0ab83":"code","88c08d2d":"code","e0a660fd":"code","a0b70b07":"code","da94dbde":"code","d23b077c":"code","bec8a8bc":"code","7c6d54ee":"code","855ec31e":"code","fd91f589":"code","24d17d25":"code","8dfe141b":"code","dfe37e48":"code","8db191e0":"code","a611f0bd":"code","d172239e":"code","d1edc7eb":"code","93628b50":"code","069707aa":"code","6d37aee5":"code","e4d1618f":"code","213d97b9":"markdown","da242674":"markdown","cd1da3af":"markdown","f5522de0":"markdown","20cf9744":"markdown","beca816e":"markdown","8c8e506b":"markdown","3a6170d7":"markdown","b48d5752":"markdown","671768dd":"markdown","059caf8c":"markdown","d7e0061f":"markdown","3d456d78":"markdown","95571d5a":"markdown","c684e3df":"markdown","339648d1":"markdown","68da179d":"markdown","70c875e6":"markdown","1bff2584":"markdown","4df854df":"markdown","7cdcac54":"markdown","624a2cc9":"markdown","0d99bf3a":"markdown","75b76331":"markdown","edb62477":"markdown","d4abf16e":"markdown","94400754":"markdown","c9533a16":"markdown"},"source":{"5ff4cae1":"'''\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n'''","97d04db6":"import numpy as np\nimport pandas as pd\nimport sklearn as skl\nimport scipy  as scp\nimport seaborn as sns\nimport matplotlib.pyplot as plt","6c11ba89":"df_draw_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv');\ndf_draw_test  = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv');","52b50b05":"print(f\"Shape of train\\t: {df_draw_train.shape}\\nShape of test\\t: {df_draw_test.shape}\")","525e0db9":"df_draw_train.describe()","43104e48":"df_draw_test.describe()","f4746c43":"df_draw_train.columns","3918777c":"Dtypes = df_draw_train.dtypes\nd = {};\nd['Column']=Dtypes.index;\nd['Type']  =Dtypes.values;\nd['Values']=[];\nfor idx, Dtype in zip(Dtypes.index, Dtypes):\n    if Dtype == np.object:\n        d['Values'].append(set(df_draw_train[idx].values));\n    else:\n        d['Values'].append([]);\nDTYPES = pd.DataFrame(d);\nDTYPES.head()","5e5a5ef2":"train = df_draw_train.copy();\ntest  = df_draw_test.copy();","26298d66":"trainIds = train['Id'].values;\ntestIds  = test ['Id'].values;","ca4618e8":"rates = train.isnull().sum()\/train.shape[0]*100;","5a60eea6":"ax = rates.plot.barh(figsize=(10,20))","db34d73c":"for idx, rate in zip(rates.index, rates):\n    if rate > 0.0:\n        print('{:>20}'.format(idx), end = ': ');\n        print('{:<20}'.format(rate), end = '%\\n');","9f3018ca":"threshold = 30.0;\nlist_dropped_columns = [];\nfor idx, rate in zip(rates.index, rates):\n    if rate > threshold:\n        print('{:>20}'.format(idx), end = ': ');\n        print('{:<20}'.format(rate), end = '%\\n');\n        list_dropped_columns.append(idx);\nprint(list_dropped_columns)","d311f65e":"train_01 = train.drop(axis = 1, columns = list_dropped_columns);\ntest_01  = test.drop( axis = 1, columns = list_dropped_columns);\nprint(train_01.shape);\nprint(test_01.shape);","b6f7990d":"Dtypes = train_01.dtypes\nobject_columns = [];\ncontinous_columns = [];\nmedians={};\nmodes = {};\nfor col, Dtype in zip(Dtypes.index, Dtypes):\n    if Dtype != np.object:\n        continous_columns.append(col);\n        medians[col] = train_01[col].median();\n    else:\n        object_columns.append(col);\n        modes[col] = train_01[col].mode().iloc[0];\n\ndel medians['SalePrice'];\n\"\"\"\nprint(medians);\nprint(\"-------\")\nprint(modes);\n\"\"\"","83415742":"print(continous_columns);","10499d10":"train_01[continous_columns] = train_01[continous_columns].transform(lambda x:x.fillna(x.median()));","aced8f80":"for col in continous_columns[:-1]:\n    test_01[col]=test_01[col].fillna(medians[col]);","01a6c382":"train_01[continous_columns].isnull().sum().sum()","c0833458":"test_01[continous_columns[:-1]].isnull().sum().sum()","5c2b12a7":"print(object_columns);","dab27bcf":"train_01[object_columns]=train_01[object_columns].fillna(train_01[object_columns].mode().iloc[0]);","864ba268":"train_01.isnull().sum().sum()","9e325c07":"for col in object_columns:\n    test_01[col]=test_01[col].fillna(modes[col]);","1d5dd3c2":"test_01.isnull().sum().sum()","66c5507d":"print(f\"Shape of train 01: {train_01.shape}\");\nprint(f\"Shape of test  01: {test_01 .shape}\");","f647d45f":"train_02, test_02 = train_01.copy(), test_01.copy();","61b0ab83":"train_02[continous_columns] = train_02[continous_columns].astype('float64');\ntest_02[continous_columns[:-1]]=test_02[continous_columns[:-1]].astype('float64');","88c08d2d":"train_02 = pd.get_dummies(train_02);\ntest_02  = pd.get_dummies(test_02);","e0a660fd":"train_02.columns.values.shape","a0b70b07":"train_02,test_02 = train_02.align(test_02, join='outer', axis=1, fill_value=0.0);\ndel test_02['SalePrice']\nprint(f\"train shape: {train_02.shape} - test shape: {test_02.shape}\");","da94dbde":"print(f\"Shape of train 01: {train_02.shape}\");\nprint(f\"Shape of test  01: {test_02 .shape}\");","d23b077c":"q_low = train_02['GrLivArea'].quantile(0.01)\nq_high = train_02['GrLivArea'].quantile(0.99)\ntrain_03 = train_02[(train_02['GrLivArea'] > q_low) & (train_02['GrLivArea'] < q_high)];\n#test_03 = test_02[(test_02['GrLivArea'] > q_low) & (test_02['GrLivArea'] < q_high)]\ntest_03=test_02.copy();\nprint(train_03.shape);\nprint(test_03.shape);","bec8a8bc":"from sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV","7c6d54ee":"para_pca = {\n    \"pca__n_components\": [80],\n    \"svc__kernel\":['linear', 'rbf', 'poly'],\n    \"svc__degree\":[8,9,10,11,12,13],\n    \"svc__C\":[20.0],\n    \"svc__epsilon\": [0.05],\n    \"svc__gamma\": [0.0005]\n};","855ec31e":"scaler = ('scaler', StandardScaler());\n#scaler = ('scaler', RobustScaler());\n#scaler = ('scaler', MinMaxScaler(feature_range=(0,1)));","fd91f589":"pipe = Pipeline([scaler, ('pca', PCA()), ('svc', SVR())])","24d17d25":"grid_search_cv = GridSearchCV(pipe, para_pca, n_jobs=-1)","8dfe141b":"del train_03['Id'];\ny_train=train_03['SalePrice'].values;\ndel train_03['SalePrice'];\nX_train=train_03.values;","dfe37e48":"print(f\"shape x train: {X_train.shape}\\nshape y train: {y_train.shape}\")","8db191e0":"del test_03['Id'];\nprint(f\"shape test: {test_03.shape}\")","a611f0bd":"X_test = test_03.values;","d172239e":"grid_search_cv.fit(X_train, y_train)\nyhat = grid_search_cv.predict(X_test)\nyhat","d1edc7eb":"d = {'Id': testIds, 'SalePrice': yhat}","93628b50":"testIds.shape","069707aa":"df = pd.DataFrame(d);\n","6d37aee5":"df.to_csv('submission.csv', index=False);","e4d1618f":"grid_search_cv.best_params_","213d97b9":"Check datatype","da242674":"Shape of train and test set","cd1da3af":"## 1.1. Get Id","f5522de0":"## 0.1. Import libraries","20cf9744":"## 1.3. Transform datatype","beca816e":"Continous Variables","8c8e506b":"Align test","3a6170d7":"## 1.2. Check and solve miss values","b48d5752":"Read csv files","671768dd":"Remove unsatisfied columns","059caf8c":"The new dimension","d7e0061f":"Using quantile from 1% to 99%","3d456d78":"# 2. Feature Engineering","95571d5a":"Check missing values on train_01, test_01 with continous_columns","c684e3df":"Fill miss values of continous varibles by median","339648d1":"Try setting a threshold and find the list of unsatisfied columns","68da179d":"# 1. Data preprocessing","70c875e6":"1.2.1. The rate of miss values","1bff2584":"## 0.3. Description","4df854df":"## 1.5. Establish data transform pipeline","7cdcac54":"# 3. Modeling","624a2cc9":"## 1.4. Filter outliers","0d99bf3a":"## 0.2. Load draw data","75b76331":"Check category variables","edb62477":"Check amount of miss values (don't show full-values columns)","d4abf16e":"With discrete variables of a column, using mode of that column.","94400754":"Solve discrete variables with getting dummies varibles","c9533a16":"# 0. Preparation"}}