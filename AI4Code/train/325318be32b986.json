{"cell_type":{"3e52c97d":"code","554f01af":"code","5a6b9c5f":"code","c17103d5":"code","97a6cfb2":"code","17b1c5b9":"code","456a5f4e":"code","63acbc5f":"code","061fd608":"code","27b47d9b":"code","cdfc025d":"code","d1ece926":"code","7d43deb0":"code","504d5cc4":"code","3d7f92e6":"code","921b58af":"code","87853e8b":"code","fa8269bc":"code","071102a8":"code","b24e7430":"markdown","139203c3":"markdown","968c74f6":"markdown","6cade56c":"markdown","8f5078c5":"markdown","8bb11d6b":"markdown","37229e81":"markdown","e96fabcd":"markdown","39de77b3":"markdown","1a655adc":"markdown","c3a96718":"markdown","c578b447":"markdown","9c706cdd":"markdown","ef2c994f":"markdown","0f693351":"markdown","8356b91c":"markdown"},"source":{"3e52c97d":"# Imports for Deep Learning\nfrom keras.layers import Conv2D, Dense, Dropout, Flatten\nfrom keras.models import Sequential, load_model\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# Ensure consistency across runs\nfrom numpy.random import seed\nimport random\nseed(2)\nfrom tensorflow import set_random_seed\nset_random_seed(2)\n\n# Imports to view data\nimport cv2\nfrom glob import glob\n\n# Metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Visualization\nfrom keras.utils import print_summary\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# Utils\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom os import getenv\nimport time\nimport itertools\n\n# Image Preprocessing\nfrom skimage.filters import sobel, scharr","554f01af":"# Set global variables\nTRAIN_DIR = '..\/input\/asl-alphabet\/asl_alphabet_train\/asl_alphabet_train'\nTEST_DIR = '..\/input\/asl-alphabet\/asl_alphabet_test\/asl_alphabet_test'\nCUSTOM_TEST_DIR = '..\/input\/asl-alphabet-test\/asl-alphabet-test'\nCLASSES = [folder[len(TRAIN_DIR) + 1:] for folder in glob(TRAIN_DIR + '\/*')]\nCLASSES.sort()\n\nTARGET_SIZE = (64, 64)\nTARGET_DIMS = (64, 64, 3) # add channel for RGB\nN_CLASSES = 29\nVALIDATION_SPLIT = 0.1\nBATCH_SIZE = 64\n\n# Model saving for easier local iterations\nMODEL_DIR = '..\/input\/aslalphabetcnnmodel1'\nMODEL_PATH = MODEL_DIR + '\/cnn-model.h5'\nMODEL_WEIGHTS_PATH = MODEL_DIR + '\/cnn-model.weights.h5'\nMODEL_SAVE_TO_DISK = getenv('KAGGLE_WORKING_DIR') != '\/kaggle\/working'\n\nprint('Save model to disk? {}'.format('Yes' if MODEL_SAVE_TO_DISK else 'No'))","5a6b9c5f":"def plot_one_sample_of_each(base_path):\n    cols = 5\n    rows = int(np.ceil(len(CLASSES) \/ cols))\n    fig = plt.figure(figsize=(16, 20))\n    \n    for i in range(len(CLASSES)):\n        cls = CLASSES[i]\n        img_path = base_path + '\/' + cls + '\/**'\n        path_contents = glob(img_path)\n    \n        imgs = random.sample(path_contents, 1)\n\n        sp = plt.subplot(rows, cols, i + 1)\n        plt.imshow(cv2.imread(imgs[0]))\n        plt.title(cls)\n        sp.axis('off')\n\n    plt.show()\n    return","c17103d5":"plot_one_sample_of_each(TRAIN_DIR)","97a6cfb2":"plot_one_sample_of_each(CUSTOM_TEST_DIR)","17b1c5b9":"def preprocess_image(image):\n    '''Function that will be implied on each input. The function\n    will run after the image is resized and augmented.\n    The function should take one argument: one image (Numpy tensor\n    with rank 3), and should output a Numpy tensor with the same\n    shape.'''\n    sobely = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5)\n    return sobely\n\ndef make_generator(options):\n    '''Creates two generators for dividing and preprocessing data.'''\n    validation_split = options.get('validation_split', 0.0)\n    preprocessor = options.get('preprocessor', None)\n    data_dir = options.get('data_dir', TRAIN_DIR)\n\n    augmentor_options = {\n        'samplewise_center': True,\n        'samplewise_std_normalization': True,\n    }\n    if validation_split is not None:\n        augmentor_options['validation_split'] = validation_split\n    \n    if preprocessor is not None:\n        augmentor_options['preprocessing_function'] = preprocessor\n    \n    flow_options = {\n        'target_size': TARGET_SIZE,\n        'batch_size': BATCH_SIZE,\n        'shuffle': options.get('shuffle', None),\n        'subset': options.get('subset', None),\n    }\n\n    data_augmentor = ImageDataGenerator(**augmentor_options)\n    return data_augmentor.flow_from_directory(data_dir, **flow_options)","456a5f4e":"def load_model_from_disk():\n    '''A convenience method for re-running certain parts of the\n    analysis locally without refitting all the data.'''\n    model_file = Path(MODEL_PATH)\n    model_weights_file = Path(MODEL_WEIGHTS_PATH)\n                      \n    if model_file.is_file() and model_weights_file.is_file():\n        print('Retrieving model from disk...')\n        model = load_model(model_file.__str__())\n                      \n        print('Loading CNN model weights from disk...')\n        model.load_weights(model_weights_file)\n        return model\n    \n    return None\n\nCNN_MODEL = load_model_from_disk()\nREPROCESS_MODEL = (CNN_MODEL is None)\n\nprint('Need to reprocess? {}'.format(REPROCESS_MODEL))","63acbc5f":"def build_model(save=False):\n    print('Building model afresh...')\n    \n    model = Sequential()\n    \n    model.add(Conv2D(64, kernel_size=4, strides=1, activation='relu', input_shape=TARGET_DIMS))\n    model.add(Conv2D(64, kernel_size=4, strides=2, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Conv2D(128, kernel_size=4, strides=1, activation='relu'))\n    model.add(Conv2D(128, kernel_size=4, strides=2, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Conv2D(256, kernel_size=4, strides=1, activation='relu'))\n    model.add(Conv2D(256, kernel_size=4, strides=2, activation='relu'))\n    model.add(Flatten())\n    model.add(Dropout(0.5))\n    model.add(Dense(512, activation='relu'))\n    model.add(Dense(N_CLASSES, activation='softmax'))\n\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    if save: model.save(MODEL_PATH)\n        \n    return model\n\nif REPROCESS_MODEL:\n    CNN_MODEL = build_model(save=MODEL_SAVE_TO_DISK)\n\nprint_summary(CNN_MODEL)","061fd608":"def make_generator_for(subset):\n    '''Create a generator for the training or validation set.'''\n    generator_options = dict(\n        validation_split=VALIDATION_SPLIT,\n        shuffle=True,\n        subset=subset,\n        preprocessor=preprocess_image,\n    )\n    return make_generator(generator_options)\n\n\ndef fit_model(model, train_generator, val_generator, save=False):\n    '''Fit the model with the training and validation generators.'''    \n    history = model.fit_generator(train_generator, epochs=5, validation_data=val_generator)\n    \n    if save: model.save_weights(MODEL_WEIGHTS_PATH)\n    \n    return history\n\n\nCNN_TRAIN_GENERATOR = make_generator_for('training')\nCNN_VAL_GENERATOR = make_generator_for('validation')\n\nHISTORY = None\nif REPROCESS_MODEL:\n    start_time = time.time()\n    HISTORY = fit_model(CNN_MODEL, CNN_TRAIN_GENERATOR, CNN_VAL_GENERATOR, save=MODEL_SAVE_TO_DISK)\n    print('Fitting the model took ~{:.0f} second(s).'.format(time.time() - start_time))\n\n\ncolumns=['Dimension 1', 'Dimension 2', 'Dimension 3', 'Dimension 4']\npd.DataFrame(data=[x.shape for x in CNN_MODEL.weights], columns=columns)","27b47d9b":"if HISTORY:\n    print('Final Accuracy: {:.2f}%'.format(HISTORY.history['acc'][4] * 100))\n    print('Validation set accuracy: {:.2f}%'.format(HISTORY.history['val_acc'][4] * 100))","cdfc025d":"%%HTML\n<div align=\"middle\">\n    <video width=\"80%\" controls>\n        <source src=\"https:\/\/s3-us-west-2.amazonaws.com\/danrasband-w207\/A.mp4\" type=\"video\/mp4\">\n    <\/video>\n<\/div>","d1ece926":"def plot_confusion_matrix(cm, classes,\n                      normalize=False,\n                      title='Confusion matrix',\n                      cmap=plt.cm.Blues):\n    '''\n    Plot a confusion matrix heatmap using matplotlib. This code was obtained from\n    the scikit-learn documentation:\n\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n    '''\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    return\n\n\ndef plot_confusion_matrix_with_default_options(y_pred, y_true, classes):\n    '''Plot a confusion matrix heatmap with a default size and default options.'''\n    cm = confusion_matrix(y_true, y_pred)\n    with sns.axes_style('ticks'):\n        plt.figure(figsize=(16, 16))\n        plot_confusion_matrix(cm, classes)\n        plt.show()\n    return","7d43deb0":"def evaluate_model(generator):\n    start_time = time.time()\n    evaluations = CNN_MODEL.evaluate_generator(generator)\n    for i in range(len(CNN_MODEL.metrics_names)):\n        print(\"{}: {:.2f}%\".format(\n            CNN_MODEL.metrics_names[i], evaluations[i] * 100))\n    print('Took {:.0f} seconds to evaluate this set.'.format(\n        time.time() - start_time))\n\n    start_time = time.time()\n    predictions = CNN_MODEL.predict_generator(generator)\n    print('Took {:.0f} seconds to get predictions on this set.'.format(\n        time.time() - start_time))\n\n    y_pred = np.argmax(predictions, axis=1)\n    y_true = generator.classes\n    return dict(y_pred=y_pred, y_true=y_true)\n\n\ndef evaluate_validation_dataset():\n    gen_options = dict(\n        validation_split=0.1,\n        data_dir=TRAIN_DIR,\n        shuffle=False,\n        subset='validation',\n        preprocessor=preprocess_image,\n    )\n    val_gen = make_generator(gen_options)\n    return evaluate_model(val_gen)\n\n\ndef evaluate_test_dataset():\n    gen_options = dict(\n        validation_split=0.0,\n        data_dir=CUSTOM_TEST_DIR,\n        shuffle=False,\n        preprocessor=preprocess_image,\n    )\n    test_gen = make_generator(gen_options)\n    return evaluate_model(test_gen)","504d5cc4":"CNN_VALIDATION_SET_EVAL = evaluate_validation_dataset()","3d7f92e6":"print(classification_report(**CNN_VALIDATION_SET_EVAL, target_names=CLASSES))","921b58af":"with sns.axes_style('ticks'):\n    plot_confusion_matrix_with_default_options(**CNN_VALIDATION_SET_EVAL, classes=CLASSES)","87853e8b":"CNN_TEST_SET_EVAL = evaluate_test_dataset()","fa8269bc":"print(classification_report(**CNN_TEST_SET_EVAL, target_names=CLASSES))","071102a8":"with sns.axes_style('ticks'):\n    plot_confusion_matrix_with_default_options(**CNN_TEST_SET_EVAL, classes=CLASSES)","b24e7430":"### Conclusion\n\nWhile the convolutional neural network used here does in fact provide a strong predictor for the ASL Alphabet data set, it doesn't work quite as well in more \"real-world\" environments. Further work is required to determine better preprocessing techniques or to experiment with different neural network setups.","139203c3":"## Data Processing Set-Up\n\nIn the next snippet, I make a generator for use by Keras. The `make_generator` function is versatile enough to be used for setting up a generator for training, validation, prediction, and testing.","968c74f6":"And the following is a random sampling of the \"real-world\" test set:","6cade56c":"Below is an attempt to evaluate our model against the ASL Alphabet Test data set, which is more like a real-world data set.\n\nFirst, here are some helper methods for plotting the confusion matrices:","8f5078c5":"# The data\n\nThere are 2 data sets utilized in this notebook:\n\n1. [ASL Alphabet](https:\/\/www.kaggle.com\/grassknoted\/asl-alphabet) - This data set is the basis for the model.\n2. [ASL Alphabet Test](https:\/\/www.kaggle.com\/danrasband\/asl-alphabet-test\/home) - This data set was made specifically for validating the model created using the above data set, and is intended to be used to improve the feature engineering and modeling process to make it more versatile in \"the wild\" with less contrived images.","8bb11d6b":"You can see below that most predictions go pretty well, but there are some issues present:","37229e81":"### Re-evaluate the Validation Set\n\nHere I run an evaluation against a non-shuffled validation set to once again check performance and see how bad things get confused:","e96fabcd":"# Intro\n\nThe [ASL Alphabet](https:\/\/www.kaggle.com\/grassknoted\/asl-alphabet) data set provides 87,000 images of the ASL alphabet. This notebook aims to take a first step at building a model around that data that is sufficiently versatile to handle images of the ASL alphabet with different hands and different backgrounds. This project is part of an assignment for the W207 Applied Machine Learning class in the UC Berkeley MIDS (Master of Information and Data Science) program.","39de77b3":"## Model Specification\n\nThe model used here is taken from a [Kaggle kernel called Running Kaggle Kernels with a GPU](https:\/\/www.kaggle.com\/grassknoted\/asl-alphabet), and is an example of a convolutional neural network. It is made up of 12 layers, as is diagrammed below.","1a655adc":"## Model Fitting\n\nHere we fit the model using the 87,000 images: 78,300 (90% of the data) for training and 8,700 (the remaining 10%) for validation.","c3a96718":"### Next Steps\n\nThe most logical next step for this project is to experiment with preprocessing techniques. An interesting idea is to use [Mask R-CNNs](https:\/\/arxiv.org\/abs\/1703.06870) to find the hand in the image and basically crop, resize, and resample the image to pull out that part of the image and leave out the background and other interference. There are projects, such as [matterport\/Mask_RCNN](https:\/\/github.com\/matterport\/Mask_RCNN), that are worth exploring.","c578b447":"### Sample Images\n\nThe following shows one example of each image class:","9c706cdd":"### Set Up Global Variables\n\nThe following variables will be used throughout. The `MODEL_` variables are helpful for iterating over the notebook without re-fitting the model all the time, but aren't useful in Kaggle.","ef2c994f":"### Import Libraries","0f693351":"Now, looking at the ASL Alphabet Test data set, we can see that our initial model is still too tightly fitted to the original data set:","8356b91c":"### Validation Against Real-World Data\n\nThe data provided in the ASL Alphabet data set is very much contrived. It's obvious that the images are made with one person's hand, in basically one environment. Because if this, it seemed like a good idea to validate that the models were not overfitting to images in this controlled environment. Below you can see a video compilation of all the \"A\" images."}}