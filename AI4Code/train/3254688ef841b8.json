{"cell_type":{"5a138032":"code","e21e34b4":"code","c4699091":"code","472a69dd":"code","fb054245":"code","4c1eef97":"code","eb24fd04":"code","d6089598":"code","fe94afc5":"code","12d4d908":"code","8a6e2082":"code","e5c52bcc":"code","71f91e7c":"code","e2833afe":"code","b6a2ae90":"code","7c7f0a28":"code","c5c5362f":"code","c41144b1":"code","800c0fa9":"code","fe6976ab":"code","37e25d54":"code","ea7169f3":"code","118a5a9a":"code","1c14c71c":"code","37507a16":"code","de53c92a":"code","fb8fc195":"code","f26f00a0":"code","c260c6ff":"code","85bd5a5b":"code","82983ace":"code","19b5ddfe":"code","13847df5":"code","6284e149":"code","ed17ca8b":"code","62b239c5":"code","1a53dbc6":"code","038c96b9":"code","5ea95ce6":"code","77af75e4":"code","8badd0cb":"code","2f14c942":"code","b0c78f92":"code","97338896":"code","86644669":"code","1f44f531":"code","91719781":"code","b2ea740a":"code","9e81959c":"code","21aaa9c8":"code","62105397":"code","96e48de4":"code","653457ce":"code","dc1bb7fc":"code","818f6b2a":"code","3430b31a":"code","4e44de70":"code","3b05519a":"code","b32184b4":"code","c168b807":"code","9a4966b0":"code","7991211f":"code","d7cb8825":"code","8c333798":"code","4bbee6f0":"code","fb03381f":"code","da2cbf56":"code","fa1355b8":"code","4787837e":"code","66bf8689":"code","464c16b8":"code","37a5e40d":"code","40b1d369":"code","ce60d637":"code","db662553":"code","3b7d60dc":"code","fd313fc0":"code","13f9bce5":"code","825970d9":"code","70115a12":"code","93317db5":"code","b32c818b":"code","adfeabc8":"code","519fec61":"code","79114285":"code","a27cf0f4":"code","84fe45e7":"code","35ca2f79":"code","1a6a9a0e":"code","f8749b37":"code","177d30b9":"code","fbcbac57":"code","478dcc1c":"code","2185bb05":"code","75b79fdf":"code","6fc1dc70":"code","85dda610":"code","026d631b":"code","2b6f3923":"code","03fa565f":"markdown","47c4e319":"markdown","72567eae":"markdown","58b82811":"markdown","b266a43e":"markdown","b3384a1d":"markdown","d747b3bc":"markdown","7b874927":"markdown","ba24d18f":"markdown","9e777fd8":"markdown","154a8ca7":"markdown","aaf6315e":"markdown","dc47d513":"markdown","609af0b6":"markdown","d67feabc":"markdown","b6a9f6fe":"markdown","6036ea41":"markdown","a674d3f9":"markdown","90a6ed7b":"markdown","dbaddc7f":"markdown","61d0bc9c":"markdown","05d4e2e0":"markdown","8b023fd4":"markdown","72123300":"markdown","4a6c6b38":"markdown","d357b5dd":"markdown","ed3ac8ae":"markdown","392f7451":"markdown","7a9ef7c6":"markdown","9cadcdfb":"markdown","5504a175":"markdown","7d859c4b":"markdown","dea611ea":"markdown","669039c0":"markdown","71f63381":"markdown","79f286bd":"markdown","98d9c0a7":"markdown","24ab0fda":"markdown","01a048c5":"markdown","c005cd49":"markdown","82ee973f":"markdown","cede37ff":"markdown","49e6212c":"markdown","d711ba53":"markdown","88bad89f":"markdown","c835ab95":"markdown","a9038319":"markdown","508c6b06":"markdown","8a43b0e1":"markdown","43993879":"markdown","33ba07a6":"markdown","778de6b9":"markdown","597946bd":"markdown","58c34ad5":"markdown","290b2999":"markdown","9e8b6c76":"markdown","15c52330":"markdown","0c617f70":"markdown","e6204d5d":"markdown","ee870e9c":"markdown","7a67d239":"markdown","71a6f9ee":"markdown","6efc8767":"markdown","7572936a":"markdown","00d54b6d":"markdown","dc2007dc":"markdown","261e3af6":"markdown","fd0c1684":"markdown","937b0e97":"markdown","25509ea9":"markdown","970358c0":"markdown","a9c6b70a":"markdown","bbb60370":"markdown","6c8778c4":"markdown","7aadff30":"markdown","16ca078d":"markdown","8d29017f":"markdown","5f88d915":"markdown","1d4dff71":"markdown","726db412":"markdown","0e522d5a":"markdown","5aadc775":"markdown"},"source":{"5a138032":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.api import OLS\nfrom sklearn.model_selection import train_test_split\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e21e34b4":"column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndf = pd.read_csv('..\/input\/boston-house-prices\/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n","c4699091":"df.head(5)","472a69dd":"df.describe()","fb054245":"X=df['LSTAT']\ny=df['MEDV']","4c1eef97":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)","eb24fd04":"X_train = np.array(X).reshape((len(X), 1))\ny_train = np.array(y).reshape((len(y), 1))\n","d6089598":"reg = LinearRegression().fit(X_train, y_train)\n","fe94afc5":"print(\"Coefficient B1 is :\" ,reg.coef_)\nprint(\"Intercept B0 is :\" ,reg.intercept_)","12d4d908":"est = smf.ols('MEDV ~ LSTAT', df).fit()\nest.summary().tables[1]","8a6e2082":" print(sm.OLS(y_train, X_train).fit().summary())","e5c52bcc":"model_min = est.conf_int(alpha=0.05)[0]\nmodel_max = est.conf_int(alpha=0.05)[1]\n\nprint(model_min)\nprint('')\nprint(model_max)","71f91e7c":"reg = LinearRegression().fit(X_train, y_train)","e2833afe":"reg.score(X_train,y_train)","b6a2ae90":"reg.coef_\n","7c7f0a28":"X_ex=np.array([[10]])\nreg.predict(X_ex)","c5c5362f":"y_pred=reg.predict(X_train)\n","c41144b1":"import matplotlib.pyplot as plt\nplt.scatter(X_train, y_train,  color='black',marker='*')\nplt.plot(X_train, y_pred, color='blue', linewidth=3)","800c0fa9":"plt.scatter(X_train, y_train,  color='black',marker='*')","fe6976ab":"\nfig, axs = plt.subplots(1, 4)\nfig.set_figheight(5)\nfig.set_figwidth(20)\n\nX_train_log = np.log(1 + X_train)\nX_train_sqr=X_train**2\nX_train_log_sqr=X_train_log**(2)\n# let's convert X -> log(x) , so that it get's closer to the linear behaviour\naxs[0].set_title('LOG(X) vs Y')\naxs[1].set_title('X^2 vs Y')\naxs[2].set_title('LOG(X)^2 vs Y')\naxs[3].set_title('LOG(X)^(1\/2) vs Y')\naxs[0].scatter(X_train_log, y_train,  color='black',marker='*')\naxs[1].scatter(X_train_sqr,y_train,color='y')\naxs[2].scatter(X_train_log_sqr,y_train,color='b')\naxs[3].scatter(X_train_log**(1\/2),y_train,color='r')\n\nfig, axs = plt.subplots(1, 1)\nfig.set_figheight(5)\nfig.set_figwidth(20\/\/4)\naxs.set_title('X vs Y')\naxs.scatter(X_train, y_train,  color='black',marker='*')","37e25d54":"X_train=X_train_log**(1\/2)","ea7169f3":"import seaborn as sns\nsns.residplot(x=X_train,y=y_train)","118a5a9a":"sns.set(style=\"whitegrid\")\n\n# let's see if taking the log of the output helps\ny_train_log = np.log(1 + y_train)\n# Plot the residuals after fitting a linear model\nsns.residplot(y=y_train_log, x=X_train, lowess=True, color=\"b\")","1c14c71c":"sns.residplot(y=y_train**(1\/2),x= X_train, lowess=True, color=\"b\")","37507a16":"y_train_log = np.log(1 + y_train)\ny_train_log_sqrt=y_train_log**(1\/2)\nsns.residplot(y=(y_train_log_sqrt), x=X_train, lowess=True, color=\"b\")","de53c92a":"plt.scatter(X_train, y_train,  color='black',marker='*')","fb8fc195":"calc=est.outlier_test()\nprint(calc)","f26f00a0":"\nlist1=[]\nfor i in range(len(calc)):\n    if calc['student_resid'].iloc[i]>3 or calc['student_resid'].iloc[i]<-3:\n        list1.append(i)\ndf=df.drop(list1,axis=0)","c260c6ff":"X=df['LSTAT']\ny=df['MEDV']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\nX_train = np.array(X).reshape((len(X), 1))\ny_train = np.array(y).reshape((len(y), 1))\nX_train_log = np.log(1 + X_train)\nX_train=X_train_log**(1\/2)\nreg = LinearRegression().fit(X_train, y_train)\nprint(\"Coefficient B1 is :\" ,reg.coef_)\nprint(\"Intercept B0 is :\" ,reg.intercept_)","85bd5a5b":"print(reg.score(X_train,y_train))","82983ace":"plt.scatter(x=X_train,y=y_train)","19b5ddfe":"X=df['LSTAT']\ny=df['MEDV']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\nX_train = np.array(X).reshape((len(X), 1))\ny_train = np.array(y).reshape((len(y), 1))\nX_train_log = np.log(1 + X_train)\nX_train=X_train_log**(1\/2)\nreg = LinearRegression().fit(X_train, y_train)\nprint(\"Coefficient B1 is :\" ,reg.coef_)\nprint(\"Intercept B0 is :\" ,reg.intercept_)\nprint(reg.score(X_train,y_train))","13847df5":"X_test = np.array(X).reshape((len(X), 1))\ny_test = np.array(y).reshape((len(y), 1))\nX_test_log = np.log(1 + X_test)\nX_test=X_test_log**(1\/2)\n","6284e149":"print(reg.score(X_test,y_test))","ed17ca8b":"df.head(5)","62b239c5":"sns.scatterplot(x=df['CRIM'],y=df['MEDV'],data=df)","1a53dbc6":"sns.scatterplot(x=df['ZN'],y=df['MEDV'],data=df)","038c96b9":"sns.scatterplot(x=df['INDUS'],y=df['MEDV'],data=df)","5ea95ce6":"sns.scatterplot(x=df['CHAS'],y=df['MEDV'],data=df)","77af75e4":"sns.scatterplot(x=df['NOX'],y=df['MEDV'],data=df)","8badd0cb":"sns.scatterplot(x=df['RM'],y=df['MEDV'],data=df)","2f14c942":"sns.scatterplot(x=df['AGE'],y=df['MEDV'],data=df)","b0c78f92":"sns.scatterplot(x=df['DIS'],y=df['MEDV'],data=df)","97338896":"sns.scatterplot(x=df['TAX'],y=df['MEDV'],data=df)","86644669":"sns.scatterplot(x=df['PTRATIO'],y=df['MEDV'],data=df)","1f44f531":"sns.scatterplot(x=df['B'],y=df['MEDV'],data=df)","91719781":"sns.scatterplot(x=df['LSTAT'],y=df['MEDV'],data=df)","b2ea740a":"X=df.drop(columns=['MEDV'])\ny=df['MEDV']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\nreg = LinearRegression().fit(X_train, y_train)\nfor i in range(len(X.columns)):\n    print(\"Coefficient B1 is :\" ,X.columns[i],reg.coef_[i])\n    print('')\nprint(\"Intercept B0 is :\" ,reg.intercept_)","9e81959c":"X=df.drop(columns=['MEDV'])\ny=df['MEDV']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\nreg = LinearRegression().fit(X_train, y_train)","21aaa9c8":"y_pred=reg.predict(X_train)","62105397":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nimport math\nnum_data = X_train.shape[0]\nmse = mean_squared_error(y_train,y_pred)\nrse = math.sqrt(mse\/(num_data-2))\nmae=mean_absolute_error(y_train,y_pred)\nprint(rse,' is the residual standard error')\nprint(mae,' is the mean absolute error')","96e48de4":"print(reg.score(X_train,y_train),' is  the R^2 statistic obtained for the multiple regression ')","653457ce":" print(sm.OLS(y_train, X_train).fit().summary())","dc1bb7fc":"df=df.drop(columns=['INDUS','NOX','RAD','TAX','ZN'])","818f6b2a":"X=df.drop(columns=['MEDV'])\ny=df['MEDV']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\nreg = LinearRegression().fit(X_train, y_train)","3430b31a":"y_pred=reg.predict(X_train)","4e44de70":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nimport math\nnum_data = X_train.shape[0]\nmse = mean_squared_error(y_train,y_pred)\nrse = math.sqrt(mse\/(num_data-2))\nmae=mean_absolute_error(y_train,y_pred)\nprint(rse,' is the residual standard error')\nprint(mae,' is the mean absolute error')","3b05519a":"print(reg.score(X_train,y_train),' is  the R^2 statistic obtained for the multiple regression ')","b32184b4":"df['LS*AGE']=df['LSTAT']*df['AGE']","c168b807":"X=df.drop(columns=['MEDV'])\ny=df['MEDV']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\nreg = LinearRegression().fit(X_train, y_train)\ny_pred=reg.predict(X_train)\nnum_data = X_train.shape[0]\nmse = mean_squared_error(y_train,y_pred)\nrse = math.sqrt(mse\/(num_data-2))\nmae=mean_absolute_error(y_train,y_pred)\nprint(rse,' is the residual standard error')\nprint(mae,' is the mean absolute error')","9a4966b0":" print(sm.OLS(y_train, X_train).fit().summary())","7991211f":"\nfor col in X_train.columns:\n    fig, axs = plt.subplots(1, 4)\n    fig.set_figheight(5)\n    fig.set_figwidth(20)\n    var_train_log = np.log(1 + X_train[str(col)])\n    var_train_sqr= X_train[str(col)]**2\n    var_train_log_sqr=var_train_log**(2)\n    # let's convert X -> log(x) , so that it get's closer to the linear behaviour\n    print('This is for columns named',col)\n    axs[0].set_title('LOG(X) vs Y')\n    axs[1].set_title('X^2 vs Y')\n    axs[2].set_title('LOG(X)^2 vs Y')\n    axs[3].set_title('LOG(X)^(1\/2) vs Y')\n    axs[0].scatter(var_train_log, y_train,  color='black',marker='*')\n    axs[1].scatter(var_train_sqr,y_train,color='y')\n    axs[2].scatter(var_train_log_sqr,y_train,color='b')\n    axs[3].scatter(var_train_log**(1\/2),y_train,color='r')\n    fig, axs = plt.subplots(1, 1)\n    fig.set_figheight(5)\n    fig.set_figwidth(20\/\/4)\n    axs.set_title(str(col)+' vs Y')\n    axs.scatter(X_train[str(col)], y_train,  color='black',marker='*')","d7cb8825":"df['CRIM']=df['CRIM']**2\ndf['AGE']=(np.log(1 + df['AGE']))**(1\/2)\ndf['DIS']=(np.log(1 + df['DIS']))**(1\/2)\ndf['LSTAT']=(np.log(1 + df['LSTAT']))**(1\/2)","8c333798":"X=df.drop(columns=['MEDV'])\ny=df['MEDV']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\nreg = LinearRegression().fit(X_train, y_train)\ny_pred=reg.predict(X_train)\nnum_data = X_train.shape[0]\nmse = mean_squared_error(y_train,y_pred)\nrse = math.sqrt(mse\/(num_data-2))\nmae=mean_absolute_error(y_train,y_pred)\nprint(rse,' is the residual standard error')\nprint(mae,' is the mean absolute error')","4bbee6f0":"y_pred=reg.predict(X_train)","fb03381f":"print(reg.score(X_train,y_train),' is  the R^2 statistic obtained for the multiple regression ')","da2cbf56":"sns.residplot(x=X['CRIM'],y=y)","fa1355b8":"sns.residplot(x=X['CRIM'],y=y**2)","4787837e":"y_log = np.log(1 + y)\nsns.residplot(x=X['CRIM'],y=y_log)","66bf8689":"df.columns","464c16b8":"sns.residplot(x=X['RM'],y=y)","37a5e40d":"sns.residplot(x=X['RM'],y=y**2)","40b1d369":"sns.residplot(x=X['RM'],y=y_log)","ce60d637":"sns.residplot(x=X['DIS'],y=y)","db662553":"sns.residplot(x=X['DIS'],y=y_log)","3b7d60dc":"sns.residplot(x=X['DIS'],y=y**2)","fd313fc0":"#est= sm.OLS(y_train, X_train).fit()\n#calc=est.outlier_test()\n#print(calc)\n#list1=[]\n#for i in range(len(calc)):\n#    if calc['student_resid'].iloc[i]>3 or calc['student_resid'].iloc[i]<-3:\n#        list1.append(i)\n#df=df.drop(list1,axis=0)","13f9bce5":"X=df.drop(columns=['MEDV'])\ny=df['MEDV']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\nreg = LinearRegression().fit(X_train, y_train)\ny_pred=reg.predict(X_train)\nnum_data = X_train.shape[0]\nmse = mean_squared_error(y_train,y_pred)\nrse = math.sqrt(mse\/(num_data-2))\nmae=mean_absolute_error(y_train,y_pred)\nprint(rse,' is the residual standard error')\nprint(mae,' is the mean absolute error')\ny_pred=reg.predict(X_train)\nprint(reg.score(X_train,y_train),' is  the R^2 statistic obtained for the multiple regression ')","825970d9":"from mpl_toolkits import mplot3d\nfig = plt.figure()\nax = plt.axes(projection='3d')\nax.scatter3D(df['LSTAT'], df['MEDV'] , df['RM']  , c=df['LSTAT'], cmap='Greens');","70115a12":"def f(x, y):\n    return (np.array(x *-0.3226  + y *-0.0035 ))\n\ny=df['LSTAT']\nx=df['RM']\n\nX, Y = np.meshgrid(x, y)\nZ = f(X, Y)\n\nfig = plt.figure()\nax = plt.axes(projection='3d')\nax.contour3D(X, Y, Z, 50, cmap='binary')\nax.set_ylabel('LSTAT')\nax.set_xlabel('RM')\nax.set_zlabel('MEDV');","93317db5":"y_pred_test=reg.predict(X_test)","b32c818b":"from sklearn import metrics\nprint(metrics.mean_absolute_error(y_train, y_pred),'for training dataset')\nprint(metrics.mean_absolute_error(y_test, y_pred_test),'for test dataset')","adfeabc8":"print(metrics.mean_squared_error(y_train, y_pred),'for training daatset')\nprint(metrics.mean_squared_error(y_test, y_pred_test),'for test dataset')","519fec61":"from math import sqrt\nprint(sqrt(metrics.mean_squared_error(y_train, y_pred)),'for training dataset')\nprint(sqrt(metrics.mean_squared_error(y_test, y_pred_test)),'for testing dataset')","79114285":"print(metrics.r2_score(y_train, y_pred),'for training dataset')\nprint(metrics.r2_score(y_test, y_pred_test),'for test datset')","a27cf0f4":"corr = df.corr()\nplt.figure(figsize=(20,20))\nsns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='Greens')","84fe45e7":"df['New']=df['DIS']*df['PTRATIO']","35ca2f79":"X=df.drop(columns=['MEDV'])\ny=df['MEDV']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\nreg = LinearRegression().fit(X_train, y_train)\ny_pred=reg.predict(X_train)\nnum_data = X_train.shape[0]\nmse = mean_squared_error(y_train,y_pred)\nrse = math.sqrt(mse\/(num_data-2))\nmae=mean_absolute_error(y_train,y_pred)\nprint(rse,' is the residual standard error')\nprint(mae,' is the mean absolute error')\ny_pred=reg.predict(X_train)\nprint(reg.score(X_train,y_train),' is  the R^2 statistic obtained for the multiple regression ')","1a6a9a0e":"df=df.drop(columns=['New'])\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)","f8749b37":"X_train.head()","177d30b9":"from statsmodels.stats.outliers_influence import variance_inflation_factor    \n\ndef calculate_vif_(X, thresh=5.0):\n    variables = list(range(X.shape[1]))\n    dropped = True\n    while dropped:\n        dropped = False\n        vif = [variance_inflation_factor(X.iloc[:, variables].values, ix)\n               for ix in range(X.iloc[:, variables].shape[1])]\n\n        maxloc = vif.index(max(vif))\n        if max(vif) > thresh:\n            print('dropping \\'' + X.iloc[:, variables].columns[maxloc] +\n                  '\\' at index: ' + str(maxloc))\n            del variables[maxloc]\n            dropped = True\n\n    print('Remaining variables:')\n    print(X.columns[variables])\n    return X.iloc[:, variables]\n\nX_train=calculate_vif_(X_train)","fbcbac57":"X_train","478dcc1c":"reg = LinearRegression().fit(X_train, y_train)\ny_pred=reg.predict(X_train)\nnum_data = X_train.shape[0]\nmse = mean_squared_error(y_train,y_pred)\nrse = math.sqrt(mse\/(num_data-2))\nmae=mean_absolute_error(y_train,y_pred)\nprint(rse,' is the residual standard error')\nprint(mae,' is the mean absolute error')\ny_pred=reg.predict(X_train)\nprint(reg.score(X_train,y_train),' is  the R^2 statistic obtained for the multiple regression ')","2185bb05":"X=df.drop(columns=['MEDV'])\ny=df['MEDV']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\nreg = LinearRegression().fit(X_train, y_train)\ny_pred=reg.predict(X_train)\nnum_data = X_train.shape[0]\nmse = mean_squared_error(y_train,y_pred)\nrse = math.sqrt(mse\/(num_data-2))\nmae=mean_absolute_error(y_train,y_pred)\nprint(rse,' is the residual standard error')\nprint(mae,' is the mean absolute error')\ny_pred=reg.predict(X_train)\nprint(reg.score(X_train,y_train),' is  the R^2 statistic obtained for the multiple regression ')","75b79fdf":"df.columns","6fc1dc70":"df.head(5)","85dda610":"from plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go","026d631b":"# Creating trace1\ntrace1 = go.Scatter(\n                    x = df['B'],\n                    y = df['PTRATIO'],\n                    mode = \"markers\",\n                    name = \"value=MEDV\",\n                    marker = dict(color = 'rgba(16, 112, 2, 0.8)'),\n                    text= df['MEDV'])\n# Creating trace2\n\n\n    \ndata = [trace1]\nlayout = dict(title = 'The percentage of black people in the city vs the pupil to teacher ratio',\n              xaxis= dict(title= 'BLACKS%',ticklen= 5,zeroline= False),yaxis=dict(title='PTRAIO')\n             )\nfig = dict(data = data, layout = layout)\niplot(fig)","2b6f3923":"# Creating trace1\ntrace1 = go.Scatter(\n                    x = df['CRIM'],\n                    y = df['B'],\n                    mode = \"markers\",\n                    name = \"value=MEDV\",\n                    marker = dict(color = 'rgba(80, 26, 80, 0.8)'),\n                    text= df['CRIM'])\n# Creating trace2\n\n\n    \ndata = [trace1]\nlayout = dict(title = 'The crime rate vs The percentage of black people in the city ',\n              xaxis= dict(title= 'Crime Rate',ticklen= 5,zeroline= False),yaxis=dict(title='Blacks%')\n             )\nfig = dict(data = data, layout = layout)\niplot(fig)","03fa565f":"# POTENTIAL PROBLEMS\n\n* Non Linearity of the response-predictor relationships\n* Correlation of error terms\n* Non constant variance of error terms\n* Outliers\n* High Leverage Points\n* Colinearity\n","47c4e319":"* Might just code this variable to become a categorical variable  , where threshold =400","72567eae":"## # **INSIGHTS **\n1. The data shows some linearity , wrt to the X\n2. There not a very bad variation in the data like an outlier (but it's working abnormally at both ends (when x->0 and x->35)","58b82811":"## 1) Non Linearity of data \n### Creating a residual plot , so as to reconsider our approach to observe the distribution of data","b266a43e":"# 2) CORRELATION B\/W ERROR  TERMS ","b3384a1d":"# NOW WE SEE HOW MUCH IMPACT HAVE OUR ACTION BROUGHT TO INITIAL OBSERVATIONS","d747b3bc":"# 3) NON CONSTANT VARIANCE OF ERROR TERMS","7b874927":"# Let's train a general multiple regression model without working on the present state of the data","ba24d18f":"# PLOTTING A REGRESSION LINE THROUGH THE DATA GIVES US INSIGHT ABOUT , HOW'S THE ACTUAL DISTRIBUTION OF THE DATA AND THE BEHAVIOUR OF THE ACTUAL FUNCTION F(x) ","9e777fd8":"# We'll be using Non Linear Transformations , Interaction Terms, Colinearity Detection, Outlier Removal ,Non Linearity of Data , Non Constant Variance of Error Terms etc","154a8ca7":"# NOW WE GO AHEAD AND CALCULATE THE ACCURACY OF OUR MODEL , FROM HERE WE'LL BE MOVING TO MULTIPLE LINEAR REGRESSION AND COMPARE IT WITH THE SIMPLE LINEAR REGRESSION","aaf6315e":"# EXPERIMENTSSSSSSSSSS","dc47d513":"1. CRIM - Per capita crime rate in the neighbourhood","609af0b6":"# NOW LET'S CALCULATE THE RSE OF THE FIT NORMALLY WITHOUT ANY CLEANING , SCALING , INTERACTION, NON LINEAR TRANSFORMATION ETC","d67feabc":"# We can clearly see that the residual plot  has no shape , i.e it has linearity wrt the residuals for entire data , we could have seen a pattern if this wasn't the case . Nevertheless we'll try some other variations of the Y","b6a9f6fe":"## By seeing the above 5 graphs , we come to the conclusion that graph 4  LOG(X)^(1\/2) vs Y is the most linear in nature and we'll further use it to build the model","6036ea41":"**I.e 95% of the times 1 unit change in LSTAT would change the value of MEDV by at min -> (-1.028) and at max -> (-0.873)**","a674d3f9":"# Introducing an interaction columns \n\nLet's try to introduce a columns which is dependant on LSTAT*AGE","90a6ed7b":"# EXPERIMENT FAILED :( - The new interaction term does not add any significant value to the present model","dbaddc7f":"# NOW LET'S GET STARTED WITH MULTIPLE LINEAR REGRESSION","61d0bc9c":"# FEATURE SCALING (NON LINEAR TRANSFORMATIONS FOR -CRIM , AGE, DIS,LSTAT)","05d4e2e0":"* Usually categorical variable do not come encoded in the data , but here they are finely coded \n* We'll be keeping the variable in the dataset","8b023fd4":"There is one more assumption that we make while creatign linear models - \"We assume the error terms have a constant variance\". Going back to the same residual plot , we can rather see that , the residual vary by large and weird amounts at different intervals.","72123300":"# 6)COLINEARITY - In simple linear regression there is no other variable to consider colinearity with , so we just pass on this part as well","4a6c6b38":"# The same accuracy is achieved for test set as well, which means we worked well :)","d357b5dd":"# Training a Simple Linear Reg , LSTAT(X) VS MEDV(Y)","ed3ac8ae":"# THAT DIDNT GO VERY WELL, THE R2 STATISTIC WENT FROM 80% TO 50% ON REMOVAL OF COLINEAR VARIBALE ACCORDING TO VIF \n\n# EXPERIMENT FAILED :(","392f7451":"# INTERACTION TERMS\n\n1. **The use of interaction terms has been seen in the case of LSTAT WITH AGE , but that was a random interaction term , with no intuition behind it , we now further persue the topic in order to experiment with out regression model **","7a9ef7c6":"# We have several variables let's first try to fit a simple linear regression model using Lstat as predictor and MEDV are response","9cadcdfb":"Intuitively the higher the LSTAT the higher the MEDV (Beacuse if a considerable amount of population does not belong to a backward soci-economic backgroud , they might tend to live in costlier households)","5504a175":"We can actually see that there is a trend that is somewhat linear but not perfectly linear , so let's try playing with the variable X , and see if we can get a graph , which is somewhat near to linear behaviour","7d859c4b":"# Although we dropped by 3% in our calcuation of R2 statistic , i don't mind dropping 5 columns even if there's a loss of 3%","dea611ea":"INTERACTION TERM  1) The PTRATIO and DIS might have the potential to be an interaction term, as they don't have much power by themselves but together in my opinion could be stronger variables , for eg if the pupil teacher ratio is high and weighted sitance is low , then the medv values might actually be very high","669039c0":"INDUS: proportion of non-retail business acres per town","71f63381":"## 1) ARE 3D PLOTS USEFUL ??\n","79f286bd":"# EXPLORATORY DATA ANALYSIS","98d9c0a7":"# We'll drop the columns where p>0.05 , indicating that there is no linear relationship bw the dropped variables and the output-\n\n* INDUS \n* NOX\n* RAD \n* TAX\n* ZN\n","24ab0fda":"# What if we want to see what would be the estimation for a random instance, let's say lstat=10 . Here we got 25.05 units of MEDV , if LSTAT =10","01a048c5":"# I tried buch of combinations , so as to reduce the effect of variance , but it does not work :(","c005cd49":"## Let's visulaize LSTAT and LS * AGE wrt to the MEDV value","82ee973f":"# We see that there are no leverage points present in this data , so for now i won't be removing anything from the dataframe , a leverage point is basically an outlier which lies on the X axis","cede37ff":"* Introducing Interaction Terms\n* Intoduction Non Transformations\n","49e6212c":"* Doesn't show any linear behaviour , we might just drop it","d711ba53":"# After fitting the data using multiple linear regression , we get a R2 statistic of 77% which isn't great considering we got 70% in simple linear regression itself ,we'll work towards improving the model , from here forward.","88bad89f":"* CRIM: per capita crime rate by town\n* ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n* INDUS: proportion of non-retail business acres per town\n* CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n* NOX: nitric oxides concentration (parts per 10 million)\n* 20.2. Load the Dataset 124\n* RM: average number of rooms per dwelling\n* AGE: proportion of owner-occupied units built prior to 1940\n* DIS: weighted distances to \ufb01ve Boston employment centers\n* RAD: index of accessibility to radial highways\n* TAX: full-value property-tax rate per 10,000 dollars\n* PTRATIO: pupil-teacher ratio by town 12.\n* B: 1000(Bk\u22120.63)2 where Bk is the proportion of blacks by town 13. \n* LSTAT: perc lower status of the population\n* MEDV: Median value of owner-occupied homes in 1000 dollars\n* We can see that the input attributes have a mixture of units.\n","c835ab95":"* **As one would predict the crime rate is very high from some houses where the value is very low , simplifying it we can say that  - if the neighbourhood has a low MEDV then they might\/might not have per capita crime rate very very high**\n* **But it doesn't really ring a bell to have a per capital crime rate of 60-80 , that might just be some leverage points that we should rather skip**\n* Coefficient B1 is : CRIM -0.09262839331882253(In a multiple linear reg model ) which means that as MEDV increases by 1 unit the crime rate decreases by 9 times per capita","a9038319":"# 5)LEVERAGE POINT DETECTION","508c6b06":"# Mean Absolute Error \n* Average of the difference between the Original Values and the Predicted Values.\n* Do not gives any idea of the direction of the error i.e. whether we are under predicting the data or over predicting the data.\n* Smaller the MAE, better is the model.","8a43b0e1":"**I have used df as a whole dataframe and not in terms of X_train and y_train**","43993879":"* Another useful variable , we'll be keeping in the \n* Non-linear scaling could severly help the variable","33ba07a6":"**So here we can see the rows where the data lies outside the acceptable permit of studentized_intervals**\n\n**Let's remove this data**","778de6b9":"# Removing colinear variables","597946bd":"* We encoded this variable earlier as well , certainly did a good job after scaling","58c34ad5":"# Although the R2 statistic of the graph has increased to 80% it's going to be very hard for me to explain such tremendous mathematically scaled graphs in terms of references :) \n","290b2999":"# ADVANCED VISUALIZATIONS ","9e8b6c76":"**Heteroscedasticity - If the value of the variance of the error term  (y - y^) might increase or decrease with the variation in y , such actions lead to shrinkage of evaluation of responses at the high or low ranges**","15c52330":"* This variable probably makes no sense \n* And it might be harmful to keep it in linear model , as this variable is clearly far away from linear \"","0c617f70":"* A good variable , which won't require any scaling as it's behaviour is very close to linear","e6204d5d":"* This variable also shows some useful info that we can use for further analysis\n* We'll be usin the variable\n* Although might try some feature scaling on the variable","ee870e9c":"**The linear regression model assumes that there is a linear relationship b\/w the predictors and the response . If the true relationship is far from linear , then all the insights that we get from data are questionable** - Gareth James\n","7a67d239":"The 95% confidence interval is \n\n [(B1-2*(S.E)) , (B1+2*(S.E))] and similar for the B0 ","71a6f9ee":"* CRIM: per capita crime rate by town\n*  ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n* INDUS: proportion of non-retail business acres per town\n* CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n* NOX: nitric oxides concentration (parts per 10 million)\n* 20.2. Load the Dataset 124\n* RM: average number of rooms per dwelling\n* AGE: proportion of owner-occupied units built prior to 1940\n* DIS: weighted distances to \ufb01ve Boston employment centers\n* RAD: index of accessibility to radial highways\n* TAX: full-value property-tax rate per 10,000 dollars\n* PTRATIO: pupil-teacher ratio by town 12.\n* B: 1000(Bk\u22120.63)2 where Bk is the proportion of blacks by town 13.\n* LSTAT: perc lower status of the population\n* MEDV: Median value of owner-occupied homes in 1000 dollars\nWe can see that the input attributes have a mixture of units.","6efc8767":"**The error terms are randomly distributed and show no pattern while we observe the residual plots , so we do not fix the data for anything here**","7572936a":"# RMSE\n* Because the MSE is squared, its units do not match that of the original output. RMSE is the square root of MSE.\n* Since the MSE and RMSE both square the residual, they are similarly affected by outliers.","00d54b6d":"* Seems like a very good variable and shows linear behaviour\n* So we'll probably not even require scaling for the variable","dc2007dc":"* **WE SEE THAT THE COEFFICIENT HAS CHANGED FROM -0.95 INITIALLY TO -0.40 , ALSO THE INTERCEPT HAS CHANGED FROM 35 TO 84 WHICH IS SIGNIFICANT CHANGE TBH, BUT WHILE TAKING INFERENCES WE MUST CONSIDER THE FACT THAT X=LOG(X)^(1\/2)**\n* **WE CAN ALSO SEE THAT THERE IS A LOT OF CHANGE IN THE STATISTIC OF R^2 WHICH WENT FROM 54% TO 70% , WUHUUU WE JUST INCREASED EXPLANABILITY**","261e3af6":"ZN: proportion of residential land zoned for lots over 25,000 sq.ft.","fd0c1684":"# Interpreting t-statistics and their respective p-values\n\n\n## MY INTERPRETATION OF T-Statistic - How many standard deviations are we away from the 0\n\n                                    NULL HYPOTHESES : B1=0\n                                    ALTERNATE HYPOTHESIS : B1 not = 0\n\n**We can see that the t-statistics are sufficiently high in order to say that the B1 and B0 are significantly far away from the 0**\n\n**Also the p - values of t-statistic are low enough (<0.05) in order to back up the fact that this has not taken place by chance**\n\n# **SO WE REJECT NULL HYPOTHESIS AND SELECT THE ALTERNATE HYPOTHESIS**","937b0e97":"# FROM THIS POINT FORWARD I'LL PREPROCESS DATA WITH RESPECT TO SOME STATISTICAL TECHNIQUES , WE'LL PROBABLY SEE A LOT OF CHANGE IN RESULTS ","25509ea9":"* MEDV - (Median House Value)\n* LSTAT - (Percent Of Households with low socio-econmic status)","970358c0":"# 4)OUTLIER DETECTION ","a9c6b70a":"# It doesn't actually make data seem more linear and uncorrelated wrt to X","bbb60370":"# Interpreting coefficients\n### ** I'll be using the notation y= B0 + B1(X)**\n* Coefficient B1 is : [[-0.95004935]]\n\n* Intercept B0 is : [34.55384088]\n\nIf the LSTAT changes by 1% (or rather 1 unit)then the value is value of households in the area is decreasing by 0.95 units (which is as per what we assumed it to be , the no. of poor households when increase , will eventually decrease the value of houses)\n\nIf the LSTAT is 0 then the  value is value of hosueholds in the area is 34.55 units","6c8778c4":"# NON CONSTANT VARIANCE OF ERROR TERMS\n\n","7aadff30":"# For calculating outliers , generally we calc studentized residuals , which if >3 or <-3 , is considered an outlier","16ca078d":"**Return the coefficient of determination R^2 of the prediction.**\n\nR^2 -> The LSTAT is able to explain about 54% of the variability in the MEDV column","8d29017f":"* This variable shows some beahviour and pattern which might be useful \n* So , we won't be dropping this column","5f88d915":"# R_squared ","1d4dff71":"* Doesn't look like a good variable \n* Don't know how to handle this feature tbh","726db412":"# Each record in the database describes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970. The attributes are de\ufb01ned as follows (taken from the UCI Machine Learning Repository1): ","0e522d5a":"# Line Charts","5aadc775":"# Mean Squared Error \n* Takes the average of the square of the difference between the original values and the predicted values.\n* As we take square of the error, the effect of larger errors(sometimes outliers) become more pronounced then smaller error. Model will be penalized more for making predictions that differ greatly from the corresponding actual value."}}