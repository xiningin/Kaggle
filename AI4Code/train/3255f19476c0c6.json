{"cell_type":{"a8f6d3fa":"code","90d85b1c":"code","5c2b201e":"code","91dfa07e":"code","fef719b2":"code","ac3f2674":"code","e44790db":"code","67684bf9":"code","7df820c8":"code","73ee47ed":"code","dcaed3a6":"code","ed0a5c79":"code","4d7553be":"code","0e8ad54f":"code","d68e984a":"code","0c23c4a2":"code","6f86382e":"code","03f2dc3e":"code","ca2e23ea":"code","30501cb1":"code","3382a41f":"code","9adb9ad6":"code","6b2b2373":"code","f6fb1049":"code","ebb78763":"code","ad8f2a6c":"code","d3d7457c":"code","fc1588de":"code","db9803e9":"code","bc991603":"code","f99785a0":"code","b9c61f1c":"code","23b2d51f":"code","fb821981":"code","436084c6":"markdown","d2638dd9":"markdown","3c0622cc":"markdown"},"source":{"a8f6d3fa":"pip install transformers ","90d85b1c":"from transformers import BertTokenizer, TFBertForSequenceClassification\nfrom transformers import InputExample, InputFeatures","5c2b201e":"model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")","91dfa07e":"model.summary()","fef719b2":"import tensorflow as tf\nimport pandas as pd","ac3f2674":"trainDF=pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntestDF=pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","e44790db":"trainDF.head()","67684bf9":"trainDF.isna().sum()","7df820c8":"trainDF # there are mentions , hashtags ","73ee47ed":"trainDF['text'] = trainDF.text.str.lower() #set all the tweets to lower case\ntestDF['text']=testDF.text.str.lower()","dcaed3a6":"import re","ed0a5c79":"trainDF.text = trainDF.text.apply(lambda x: re.sub(r'https?:\\\/\\\/\\S+', '', x)) # remove links with http,https\ntrainDF.text=trainDF.text.apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x)) #remove links without http,https","4d7553be":"testDF.text = testDF.text.apply(lambda x: re.sub(r'https?:\\\/\\\/\\S+', '', x))\ntestDF.text=testDF.text.apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))","0e8ad54f":"trainDF.text =trainDF.text.apply(lambda x: re.sub(r'{link}', '', x)) #romove ref(links,video)\ntrainDF.text =trainDF.text.apply(lambda x: re.sub(r\"\\[video\\]\", '', x))","d68e984a":"testDF.text =testDF.text.apply(lambda x: re.sub(r'{link}', '', x)) \ntestDF.text =testDF.text.apply(lambda x: re.sub(r\"\\[video\\]\", '', x))","0c23c4a2":"trainDF.text = trainDF.text.apply(lambda x: re.sub(r'&[a-z]+;', '', x)) #remove &\ntrainDF.text =trainDF.text.apply(lambda x: re.sub(r\"[^a-z\\s\\(\\-:\\)\\\\\\\/\\];='#]\", '', x)) #remove non letter charcters \ntrainDF.text =trainDF.text.apply(lambda x: re.sub(r'@mention', '', x)) # remove @","6f86382e":"testDF.text = testDF.text.apply(lambda x: re.sub(r'&[a-z]+;', '', x)) #remove &\ntestDF.text =testDF.text.apply(lambda x: re.sub(r\"[^a-z\\s\\(\\-:\\)\\\\\\\/\\];='#]\", '', x)) #remove non letter charcters \ntestDF.text =testDF.text.apply(lambda x: re.sub(r'@mention', '', x)) # remove @","03f2dc3e":"from nltk.tokenize import TweetTokenizer\ntoken = TweetTokenizer()\ntrainDF['tokens'] = trainDF['text'].apply(token.tokenize)\ntestDF['tokens'] = testDF['text'].apply(token.tokenize)","ca2e23ea":"import string","30501cb1":"PUNCUATION_LIST = list(string.punctuation)\ndef remove_punctuation(word_list):\n    \"\"\"Remove punctuation tokens from a list of tokens\"\"\"\n    return [w for w in word_list if w not in PUNCUATION_LIST]\ntrainDF['tokens'] = trainDF['tokens'].apply(remove_punctuation)\ntestDF['tokens'] = testDF['tokens'].apply(remove_punctuation)","3382a41f":"from sklearn.model_selection import train_test_split","9adb9ad6":"X_train, X_test = train_test_split(trainDF,test_size=0.2)","6b2b2373":"def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): \n  train_InputExamples = train.apply(lambda x: InputExample(guid=None, \n                                                          text_a = x[DATA_COLUMN], \n                                                          text_b = None,\n                                                          label = x[LABEL_COLUMN]), axis = 1)\n\n  validation_InputExamples = test.apply(lambda x: InputExample(guid=None, \n                                                          text_a = x[DATA_COLUMN], \n                                                          text_b = None,\n                                                          label = x[LABEL_COLUMN]), axis = 1)\n  \n  return train_InputExamples, validation_InputExamples\n\n  train_InputExamples, validation_InputExamples = convert_data_to_examples(train, \n                                                                           test, \n                                                                           'DATA_COLUMN', \n                                                                           'LABEL_COLUMN')\n  \ndef convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n    features = [] \n\n    for e in examples:\n        input_dict = tokenizer.encode_plus(\n            e.text_a,\n            add_special_tokens=True,\n            max_length=max_length, \n            return_token_type_ids=True,\n            return_attention_mask=True,\n            pad_to_max_length=True, \n            truncation=True\n        )\n\n        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n\n        features.append(\n            InputFeatures(\n                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n            )\n        )\n\n    def gen():\n        for f in features:\n            yield (\n                {\n                    \"input_ids\": f.input_ids,\n                    \"attention_mask\": f.attention_mask,\n                    \"token_type_ids\": f.token_type_ids,\n                },\n                f.label,\n            )\n\n    return tf.data.Dataset.from_generator(\n        gen,\n        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n        (\n            {\n                \"input_ids\": tf.TensorShape([None]),\n                \"attention_mask\": tf.TensorShape([None]),\n                \"token_type_ids\": tf.TensorShape([None]),\n            },\n            tf.TensorShape([]),\n        ),\n    )\n\n\nDATA_COLUMN = 'DATA_COLUMN'\nLABEL_COLUMN = 'LABEL_COLUMN'","f6fb1049":"InputExample(guid=None,\n             text_a = \"Hello, world\",\n             text_b = None,\n             label = 1)","ebb78763":"train_InputExamples, validation_InputExamples = convert_data_to_examples(X_train,X_test,'text', 'target')\n\ntrain_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\ntrain_data = train_data.shuffle(100).batch(32).repeat(2)\n\nvalidation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\nvalidation_data = validation_data.batch(32)","ad8f2a6c":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n\nmodel.fit(train_data, epochs=3, validation_data=validation_data,batch_size=16)","d3d7457c":"pred_sentences=testDF['text'].values.tolist()","fc1588de":"!export CUDA_VISIBLE_DEVICES=1 ","db9803e9":"target=[]","bc991603":"for i in pred_sentences:\n  tf_batch = tokenizer(i, max_length=128, padding=True, truncation=True, return_tensors='tf')\n  tf_outputs = model(tf_batch)\n  tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n  labels = ['0','1']\n  label = tf.argmax(tf_predictions, axis=1)\n  label = label.numpy()\n  target.append(label)","f99785a0":"import numpy as np","b9c61f1c":"result = pd.DataFrame({'id':np.array(testDF['id']),'target':np.array(target).astype(int).reshape(-1)})","23b2d51f":"result","fb821981":"result.to_csv(\"submission4.csv\",index=False)","436084c6":"# Data Download from Kaggle","d2638dd9":"# Data Preprocessing","3c0622cc":"# Data Tokenization "}}