{"cell_type":{"26b70bcd":"code","acadf044":"code","927796f8":"code","11a27449":"code","b6fc3be2":"code","b6f125d5":"code","28cf683a":"code","f6c5f7d9":"code","677ae6b3":"code","c9c870a1":"code","5103670b":"code","37c0fa28":"code","c2658540":"code","4878d2af":"code","4aa6f6ee":"code","bbed5ea0":"code","f16a3263":"code","00648537":"code","4a250443":"code","dd6e755a":"code","2956dbe6":"code","ff62ee12":"code","e74309a7":"code","333b8282":"code","5229706c":"code","a19578d4":"code","95b127df":"code","fb25086b":"code","fd15d768":"code","3cdf24aa":"code","05ae18dd":"code","b2cf4423":"code","aaa163a1":"code","119b1958":"code","15c28f0a":"code","7e4fe41c":"code","74faf896":"code","af122708":"code","60c9e0e6":"code","56bf140c":"code","5f6ee935":"code","49a43629":"code","8e437f99":"code","3718c7d6":"code","c6bf7502":"code","29d78f6f":"code","209d0b64":"code","4b87e9c6":"code","be12f063":"markdown","2c6c433c":"markdown","a6ea04b3":"markdown","3e596bb4":"markdown","3b518c88":"markdown","859479d4":"markdown","653a4a38":"markdown"},"source":{"26b70bcd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","acadf044":"#Installing pyspark thorugh pip command\n!pip install pyspark","927796f8":"import os\nimport pandas as pd\nimport numpy as np\n\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import udf, col\n\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.mllib.evaluation import RegressionMetrics\n\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\nfrom pyspark.ml.evaluation import RegressionEvaluator","11a27449":"import seaborn as sns\nimport matplotlib.pyplot as plt","b6fc3be2":"# setting random seed for notebook reproducability\nrand_seed=23\nnp.random.seed=rand_seed\nnp.random.set_state=rand_seed","b6f125d5":"#checking number of cores\nimport multiprocessing\n\nmem_bytes = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')  # e.g. 4015976448\nmem_gib = mem_bytes\/(1024.**3)  # e.g. 3.74\nprint(\"RAM: %f GB\" % mem_gib)\nprint(\"CORES: %d\" % multiprocessing.cpu_count())","28cf683a":"## Creating spark session\nspark = SparkSession.builder.master(\"local[*]\")\\# number of cores to use locally\n                            .appName(\"Lending Club\")\\\n                            .getOrCreate()#Gets an existing SparkSession or, if there is no existing one,creates a new one based on the options set in this builder.\n","f6c5f7d9":"spark","677ae6b3":"sc = spark.sparkContext\nsc","c9c870a1":"sqlContext = SQLContext(sc)\nsqlContext","5103670b":"#Loading accepted loans\n\n# File location and type\nfile_location = \"\/kaggle\/input\/lending-club\/accepted_2007_to_2018q4.csv\/accepted_2007_to_2018Q4.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf1 = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\n","37c0fa28":"#loading rejected loans\n\n# File location and type\nfile_location_2 = \"\/kaggle\/input\/lending-club\/rejected_2007_to_2018q4.csv\/rejected_2007_to_2018Q4.csv\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf2 = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location_2)\n\n","c2658540":"df2.show(5)","4878d2af":"#number of records\nprint('Number of accepted loans ',df1.count())\nprint('Number of rejected loans ',df2.count())","4aa6f6ee":"df2.printSchema()","bbed5ea0":"#checking missing values in df2\n#isnan function is used to check for Not a Number\", it's usually the result of a mathematical operation that doesn't make sense eg 0\/0\n#isNull() function is used to find null values (nothing or no value)\ndf2.select([F.count(F.when(F.isnan(c) | col(c).isNull(), c)).alias(c) for c in df2.columns]).show()","f16a3263":"def fill_avg(df, colname): \n    return df.select(colname).agg(F.avg(colname))","00648537":"# fill_avg will return a dataframe(only 1 col Risk_Score) with all rows having mean value so just fetching first row\nrisk_avg=fill_avg(df2,\"Risk_Score\").first()[0]\nrisk_avg","4a250443":"#removing random strings in risk score col, extract will give null where there is string\ndf2=df2.withColumn(\"Risk_Score\",F.regexp_extract(col(\"Risk_Score\"),\"\\\\d+\",0))# 0 is position from which to start extraction","dd6e755a":"df2=df2.withColumn(\"Risk_Score\",df2.Risk_Score.cast(DoubleType()))","2956dbe6":"df2=df2.fillna(risk_avg,subset=\"Risk_Score\")","ff62ee12":"df2.filter(df2.Risk_Score.isNull()).count()","e74309a7":"df2.select('Debt-To-Income Ratio').show(100)","333b8282":"#removing %\ndf2=df2.withColumn(\"Debt-To-Income Ratio\",F.regexp_replace(col(\"Debt-To-Income Ratio\"),\"%\",\"\"))\ndf2=df2.withColumn(\"Debt-To-Income Ratio\",df2['Debt-To-Income Ratio'].cast(DoubleType()))\n","5229706c":"df2.printSchema()","a19578d4":"scatter_df=df2.select(['Amount Requested','Risk_Score','Debt-To-Income Ratio']).limit(100000).toPandas()\nplt.figure(figsize=(15,6))\nsns.scatterplot(scatter_df['Amount Requested'],scatter_df['Risk_Score'])\nplt.xlim(0,50000)#catering the outliers\nplt.show()","95b127df":"plt.figure(figsize=(15,6))\nsns.scatterplot(scatter_df['Debt-To-Income Ratio'],scatter_df['Risk_Score'])\nplt.xlim(0,1000000)\nplt.show()","fb25086b":"df2.filter(df2['Loan Title'].isNull()).count()","fd15d768":"df2=df2.withColumn('Loan Title',F.upper(\"Loan Title\"))#converting all values into upper case\n","3cdf24aa":"df2=df2.withColumn(\"Loan Title\",F.regexp_replace(col(\"Loan Title\"),\"_\",\" \"))# removing \"_\" as it was duplicating values","05ae18dd":"df2.groupby('Loan Title').count().orderBy('count',ascending=False).show()","b2cf4423":"#finding mode of loan title\ndf2.groupby('Loan Title').count().orderBy('count',ascending=False).first()[0]","aaa163a1":"#imputting null values with mode\ndf2=df2.fillna(\"DEBT CONSOLIDATION\",subset=\"Loan Title\")","119b1958":"df2.filter(df2['Loan Title'].isNull()).count()","15c28f0a":"Loan_title=df2.groupby('Loan Title').count().orderBy('count',ascending=False).toPandas()\n","7e4fe41c":"df2.count()","74faf896":"Loan_title['count']=Loan_title['count']\/27648741\nLoan_title[:10]","af122708":"plt.figure(figsize=(15,6))\nsns.barplot(x=Loan_title['Loan Title'][:10],y=Loan_title['count'][:10])\nplt.xticks(rotation=90)\nplt.show()","60c9e0e6":"df2.show()","56bf140c":"#checking missing value %\ndf2.filter(df2['Employment Length'].isNull()).count()\/df2.count()","5f6ee935":"df2=df2.withColumn('Employment Length',F.regexp_extract(col('Employment Length'),\"\\\\d+\",0))","49a43629":"df2.select('Employment Length').show(10)","8e437f99":"df2.groupby('Employment Length').count().orderBy('count',ascending=False).show()","3718c7d6":"#Replacing empty value with  1 in Employment Length col\ndf2=df2.withColumn('Employment Length',F.when(col('Employment Length')==\"\",1).otherwise(col('Employment Length')))","c6bf7502":"#No empty value here, majority of requestors have employment length 1 or less than 1\ndf2.groupby('Employment Length').count().orderBy('count',ascending=False).show()","29d78f6f":"#imputting missing value with mode\ndf2=df2.withColumn(\"Employment Length\",df2['Employment Length'].cast(DoubleType()))\ndf2=df2.fillna(1,subset=\"Employment Length\")","209d0b64":"df2.filter(col('Employment Length').isNull()).count()","4b87e9c6":"emp_length=df2.groupby('Employment Length').count().orderBy('count',ascending=False).toPandas()\nemp_length['count']=emp_length['count']\/df2.count()\nplt.figure(figsize=(15,6))\nsns.barplot(x=emp_length['Employment Length'],y=emp_length['count'])\nplt.xticks(rotation=90)\nplt.show()\n#We can see clearly see here majority of loans are rejected for those having employment length <=1","be12f063":"### Loan Title Analysis","2c6c433c":"### Employment Length Analysis","a6ea04b3":"### **Amount Requested vs Risk Score**","3e596bb4":"### Here we will be analysing only Rejected loans as this notebook is desgined to learn basics of pyspark and Rejected loans has few columns which can be easily catered and analysis can be done along with pyspark","3b518c88":"Here we can see most(~44%) of the loans rejected are from debt consolidation as these customers already have high debts on them so it is bound for them to get their loan rejected","859479d4":"### Here we complete the analysis . Hope this notebook helps you get started with  data preprocessing techniques in Pyspark and visualization through seaborn\/matpotlib. We will be covering ML basics in Pyspark in next notebook","653a4a38":"### Debt to income ratio vs Risk Score"}}