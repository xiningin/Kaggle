{"cell_type":{"8286c190":"code","4067c900":"code","e4665fd9":"code","38156921":"code","d4f576bd":"code","f29f4901":"code","d7074c21":"code","a3046289":"code","8ba22587":"code","128320b0":"code","3ee02fc7":"code","76ac55e9":"code","b2f83687":"code","f2025f89":"code","96b22030":"code","f141d3f5":"code","76fd6505":"code","71b1a270":"code","a02c867e":"code","ff9c5875":"code","5d60e445":"code","4869e69d":"code","4f585546":"code","c905208e":"code","a8527e09":"code","7eecf56b":"code","f4a08c53":"code","36a905fc":"code","688ff31b":"code","013580d4":"code","72d7d628":"code","964a1f54":"code","8e2d104a":"code","61ed995b":"code","f2b79d05":"code","804168a2":"code","5e7b4e5b":"code","6baa5c60":"code","cb5f8f09":"code","e7fc90ea":"code","25014146":"code","3d20cd8f":"code","1fc594b5":"code","bb4715ab":"code","bf25244a":"code","48148f11":"code","94940d2c":"code","975fa77d":"code","297c1c47":"code","24e2cf6b":"code","d6220652":"code","d57ccebf":"code","b820b1ec":"code","afa5e337":"code","990df4d9":"code","a9879912":"code","4a8a559a":"code","92657e12":"code","522257ce":"code","94e7238a":"code","5870db34":"code","9207d6d6":"code","ab42daaa":"code","8370d853":"code","4d966680":"code","63c5f6ce":"code","29433c3e":"markdown","15b24296":"markdown","1ecbb491":"markdown","302c449b":"markdown","3ea89cbe":"markdown","532f35c1":"markdown","7d4657b9":"markdown","8af58c6e":"markdown","9715a30f":"markdown","a57dee1b":"markdown","6fb21be9":"markdown","3b442d74":"markdown","ad28bdc7":"markdown","6e5c0a14":"markdown","598870ad":"markdown","6df89999":"markdown","305faedf":"markdown","4335aa8f":"markdown","3653dae0":"markdown","e7c419b1":"markdown","c59c1822":"markdown","25c8a4a4":"markdown","ff55970f":"markdown","297efe43":"markdown","078e83ca":"markdown","b2a031c7":"markdown","9a5a542d":"markdown","68f57fde":"markdown","1f953511":"markdown","75cf1552":"markdown","a289a9b2":"markdown","b5588729":"markdown","7fe01912":"markdown","83e84df2":"markdown","17440d19":"markdown","1ab8f53b":"markdown","6bbd8610":"markdown","56567c5c":"markdown","7a62d7b1":"markdown","1357cd1d":"markdown","d05dcb43":"markdown","320443d9":"markdown","2725119b":"markdown","f3e0c9d3":"markdown","397f08e7":"markdown","8216c9c0":"markdown","4f34867b":"markdown","0eecb4a3":"markdown","6a8e1b12":"markdown","504640b0":"markdown","e47ca2d7":"markdown","0f22b176":"markdown"},"source":{"8286c190":"import numpy as np                    # Linear Algebra\nimport pandas as pd                   # Data processing \nimport matplotlib.pyplot as plt       # Visualizations\nimport seaborn as sns                 # Visualizations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix \nimport warnings                       # Hide warning messages\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline                    ","4067c900":"# Reading the file \ndf = pd.read_csv(\"..\/input\/advertising.csv\") ","e4665fd9":"df.head(10) # Checking the 1st 10 rows of the data","38156921":"df.info() # gives the information about the data","d4f576bd":"df.duplicated().sum() # displays duplicate records","f29f4901":"df.columns # displays column names","d7074c21":"df.select_dtypes(include = ['object']).columns # Displays categorical variables which are detected by python ","a3046289":"# Assigning columns as numerical variables\nnumeric_cols = ['Daily Time Spent on Site', 'Age', 'Area Income', 'Daily Internet Usage' ]","8ba22587":"# Assigning columns as categorical variables\nCategorical_cols = [ 'Ad Topic Line', 'City', 'Male', 'Country', 'Clicked on Ad' ]","128320b0":"df[numeric_cols].describe()\n# Decribe method is used to give statistical information on the numerical columns","3ee02fc7":"df[Categorical_cols].describe(include = ['O'])\n# Decribe method is used to give statistical information on the categorical columns","76ac55e9":"pd.crosstab(df['Country'], df['Clicked on Ad']).sort_values(1,0, ascending = False).head(10)","b2f83687":"pd.crosstab(index=df['Country'],columns='count').sort_values(['count'], ascending=False).head(10)","f2025f89":"df.isnull().sum() # Number of missing values in each column","96b22030":"# Extract datetime variables using timestamp column\ndf['Timestamp'] = pd.to_datetime(df['Timestamp']) \n# Converting timestamp column into datatime object in order to extract new features\ndf['Month'] = df['Timestamp'].dt.month \n# Creates a new column called Month\ndf['Day'] = df['Timestamp'].dt.day     \n# Creates a new column called Day\ndf['Hour'] = df['Timestamp'].dt.hour   \n# Creates a new column called Hour\ndf[\"Weekday\"] = df['Timestamp'].dt.dayofweek \n# Creates a new column called Weekday with sunday as 6 and monday as 0\n# Other way to create a weekday column\n#df['weekday'] = df['Timestamp'].apply(lambda x: x.weekday()) # Monday 0 .. sunday 6\n# Dropping timestamp column to avoid redundancy\ndf = df.drop(['Timestamp'], axis=1) # deleting timestamp","f141d3f5":"df.head() # verifying if the variables are added to our main data frame","76fd6505":"# Visualizing target variable Clicked on Ad\nplt.figure(figsize = (14, 6)) \nplt.subplot(1,2,1)            \nsns.countplot(x = 'Clicked on Ad', data = df)\nplt.subplot(1,2,2)\nsns.distplot(df[\"Clicked on Ad\"], bins = 20)\nplt.show()","71b1a270":"# Jointplot of daily time spent on site and age\nsns.jointplot(x = \"Age\", y= \"Daily Time Spent on Site\", data = df) ","a02c867e":"# Creating a pairplot with hue defined by Clicked on Ad column\nsns.pairplot(df, hue = 'Clicked on Ad', vars = ['Daily Time Spent on Site', 'Age', 'Area Income', 'Daily Internet Usage'], palette = 'husl')","ff9c5875":"plots = ['Daily Time Spent on Site', 'Age', 'Area Income','Daily Internet Usage']\nfor i in plots:\n    plt.figure(figsize = (14, 6))\n    plt.subplot(1,2,1)\n    sns.boxplot(df[i])\n    plt.subplot(1,2,2)\n    sns.distplot(df[i],bins= 20)    \n    plt.title(i)    \n    plt.show()","5d60e445":"print('oldest person was of:', df['Age'].max(), 'Years')\nprint('Youngest person was of:', df['Age'].min(), 'Years')\nprint('Average age was of:', df['Age'].mean(), 'Years')","4869e69d":"f,ax=plt.subplots(2,2, figsize=(20,10))\nsns.violinplot(\"Male\",\"Age\", hue= \"Clicked on Ad\", data=df,ax=ax[0,0],palette=\"spring\")\nax[0,0].set_title('Gender and Age vs Clicked on Ad or not')\nax[0,0].set_yticks(range(0,80,10))\nsns.violinplot(\"Weekday\",\"Age\", hue=\"Clicked on Ad\", data=df,ax=ax[0,1],palette=\"summer\")\nax[0,1].set_title('Weekday and Age vs Clicked on Ad or not')\nax[0,1].set_yticks(range(0,90,10))\nsns.violinplot(\"Male\",\"Daily Time Spent on Site\", hue=\"Clicked on Ad\", data=df,ax=ax[1,0],palette=\"autumn\")\nax[1,0].set_title('Gender and Daily time spent vs (Clicked on ad or not)')\n#ax[1,0].set_yticks(range(0,120,10))\nsns.violinplot(\"Weekday\",\"Daily Time Spent on Site\", hue=\"Clicked on Ad\", data=df,ax=ax[1,1],palette=\"winter\")\nax[1,1].set_title('Weekday and Daily time spent vs (Clicked on ad or not)')\n#ax[1,1].set_yticks(range(0,120,10))\nplt.show()","4f585546":"fig = plt.figure(figsize = (12,10))\nsns.heatmap(df.corr(), cmap='Blues', annot = True) # Degree of relationship i.e correlation using heatmap","c905208e":"f,ax=plt.subplots(1,2,figsize=(14,5))\ndf['Month'][df['Clicked on Ad']==1].value_counts().sort_index().plot(ax=ax[0])\nax[0].set_title('Months Vs Clicks')\nax[0].set_ylabel('Count of Clicks')\npd.crosstab(df[\"Clicked on Ad\"], df[\"Month\"]).T.plot(kind = 'Bar',ax=ax[1])\n#df.groupby(['Month'])['Clicked on Ad'].sum() # alternative code\nplt.tight_layout()\nplt.show()","a8527e09":"f,ax=plt.subplots(1,2,figsize=(14,5))\npd.crosstab(df[\"Clicked on Ad\"], df[\"Hour\"]).T.plot(style = [], ax = ax[0])\npd.pivot_table(df, index = ['Weekday'], values = ['Clicked on Ad'],aggfunc= np.sum).plot(kind = 'Bar', ax=ax[1]) # 0 - Monday\nplt.tight_layout()\nplt.show()","7eecf56b":"df.groupby('Clicked on Ad')['Clicked on Ad', 'Daily Time Spent on Site', 'Age', 'Area Income', \n                            'Daily Internet Usage'].mean()","f4a08c53":"df.groupby(['Male','Clicked on Ad'])['Clicked on Ad'].count().unstack()","36a905fc":"hdf = pd.pivot_table(df, index = ['Hour'], columns = ['Male'], values = ['Clicked on Ad'], \n                     aggfunc= np.sum).rename(columns = {'Clicked on Ad':'Clicked'})\n\ncm = sns.light_palette(\"green\", as_cmap=True)\nhdf.style.background_gradient(cmap=cm)  # Sums all 1's i.e clicked for each hour","688ff31b":"f,ax=plt.subplots(1,2,figsize=(14,5))\nsns.set_style('whitegrid')\nsns.countplot(x='Male',hue='Clicked on Ad',data=df,palette='bwr', ax = ax[0]) # Overall distribution of Males and females count\ntable = pd.crosstab(df['Weekday'],df['Clicked on Ad'])\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, ax=ax[1], grid = False) # 0 - Monday\nax[1].set_title('Stacked Bar Chart of Weekday vs Clicked')\nax[1].set_ylabel('Proportion by Day')\nax[1].set_xlabel('Weekday')\nplt.tight_layout()\nplt.show()","013580d4":"sns.factorplot(x=\"Weekday\", y=\"Age\", col=\"Clicked on Ad\", data=df, kind=\"box\",size=5, aspect=2.0) ","72d7d628":"sns.factorplot('Month', 'Clicked on Ad', hue='Male', data = df)\nplt.show()","964a1f54":"for i in numeric_cols:\n    stat = df[i].describe()\n    print(stat)\n    IQR = stat['75%'] - stat['25%']\n    upper = stat['75%'] + 1.5 * IQR\n    lower = stat['25%'] - 1.5 * IQR\n    print('The upper and lower bounds for suspected outliers are {} and {}.'.format(upper, lower))","8e2d104a":"# Importing train_test_split from sklearn.model_selection family\nfrom sklearn.model_selection import train_test_split","61ed995b":"# Assigning Numerical columns to X & y only as model can only take numbers\nX = df[['Daily Time Spent on Site', 'Age', 'Area Income', 'Daily Internet Usage', 'Male']]\ny = df['Clicked on Ad']","f2b79d05":"# Splitting the data into train & test sets \n# test_size is % of data that we want to allocate & random_state ensures a specific set of random splits on our data because \n#this train test split is going to occur randomly\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) \n# We dont have to use stratify method in train_tst_split to handle class distribution as its not imbalanced and does contain equal number of classes i.e 1's and 0's\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","804168a2":"# Import LogisticRegression from sklearn.linear_model family\nfrom sklearn.linear_model import LogisticRegression","5e7b4e5b":"# Instantiate an instance of the linear regression model (Creating a linear regression object)\nlogreg = LogisticRegression()\n# Fit the model on training data using a fit method\nmodel = logreg.fit(X_train,y_train)\nmodel","6baa5c60":"# The predict method just takes X_test as a parameter, which means it just takes the features to draw predictions\npredictions = logreg.predict(X_test)\n# Below are the results of predicted click on Ads\npredictions[0:20]","cb5f8f09":"# Importing classification_report from sklearn.metrics family\nfrom sklearn.metrics import classification_report\n\n# Printing classification_report to see the results\nprint(classification_report(y_test, predictions))","e7fc90ea":"# Importing a pure confusion matrix from sklearn.metrics family\nfrom sklearn.metrics import confusion_matrix\n\n# Printing the confusion_matrix\nprint(confusion_matrix(y_test, predictions))","25014146":"new_df = df.copy() # just to keep the original dataframe unchanged","3d20cd8f":"# Creating pairplot to check effect of datetime variables on target variable (variables which were created)\npp = sns.pairplot(new_df, hue= 'Clicked on Ad', vars = ['Month', 'Day', 'Hour', 'Weekday'], palette= 'husl')","1fc594b5":"# Dummy encoding on Month column\nnew_df = pd.concat([new_df, pd.get_dummies(new_df['Month'], prefix='Month')], axis=1) \n# Dummy encoding on weekday column\nnew_df = pd.concat([new_df, pd.get_dummies(new_df['Weekday'], prefix='Weekday')], axis=1)","bb4715ab":"# Creating buckets for hour columns based on EDA part\nnew_df['Hour_bins'] = pd.cut(new_df['Hour'], bins = [0, 5, 11, 17, 23], \n                        labels = ['Hour_0-5', 'Hour_6-11', 'Hour_12-17', 'Hour_18-23'], include_lowest= True)","bf25244a":"# Dummy encoding on Hour_bins column\nnew_df = pd.concat([new_df, pd.get_dummies(new_df['Hour_bins'], prefix='Hour')], axis=1)","48148f11":"# Feature engineering on Age column\nplt.figure(figsize=(25,10))\nsns.barplot(new_df['Age'],df['Clicked on Ad'], ci=None)\nplt.xticks(rotation=90)","94940d2c":"# checking bins\nlimit_1 = 18\nlimit_2 = 35\n\nx_limit_1 = np.size(df[df['Age'] < limit_1]['Age'].unique())\nx_limit_2 = np.size(df[df['Age'] < limit_2]['Age'].unique())\n\nplt.figure(figsize=(15,10))\n#sns.barplot(df['age'],df['survival_7_years'], ci=None)\nsns.countplot('Age',hue='Clicked on Ad',data=df)\nplt.axvspan(-1, x_limit_1, alpha=0.25, color='green')\nplt.axvspan(x_limit_1, x_limit_2, alpha=0.25, color='red')\nplt.axvspan(x_limit_2, 50, alpha=0.25, color='yellow')\n\nplt.xticks(rotation=90)","975fa77d":"# Creating Bins on Age column based on above plots\nnew_df['Age_bins'] = pd.cut(new_df['Age'], bins=[0, 18, 30, 45, 70], labels=['Young','Adult','Mid', 'Elder'])","297c1c47":"sns.countplot('Age_bins',hue='Clicked on Ad',data= new_df) # Verifying the bins by checking the count","24e2cf6b":"# Dummy encoding on Age column\nnew_df = pd.concat([new_df, pd.get_dummies(new_df['Age_bins'], prefix='Age')], axis=1) ","d6220652":"# Dummy encoding on Column column based on EDA\nnew_df = pd.concat([new_df, pd.get_dummies(new_df['Country'], prefix='Country')], axis=1)","d57ccebf":"# Remove redundant and no predictive power features\nnew_df.drop(['Country', 'Ad Topic Line', 'City', 'Day', 'Month', 'Weekday', \n             'Hour', 'Hour_bins', 'Age', 'Age_bins'], axis = 1, inplace = True)\nnew_df.head() # Checking the final dataframe","b820b1ec":"X = new_df.drop(['Clicked on Ad'],1)\ny = new_df['Clicked on Ad']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","afa5e337":"# Standarizing the features\nfrom  sklearn.preprocessing  import StandardScaler\nstdsc = StandardScaler()\nX_train_std = stdsc.fit_transform(X_train)\nX_test_std = stdsc.transform(X_test)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","990df4d9":"import  statsmodels.api  as sm\nfrom scipy import stats\n\nX2   = sm.add_constant(X_train_std)\nest  = sm.OLS(y_train, X2)\nest2 = est.fit()\nprint(est2.summary())","a9879912":"# Applying logistic regression model to training data\nlr = LogisticRegression(penalty=\"l2\", C= 0.1, random_state=42)\nlr.fit(X_train_std, y_train)\n# Predict using model\nlr_training_pred = lr.predict(X_train_std)\nlr_training_prediction = accuracy_score(y_train, lr_training_pred)\n\nprint( \"Accuracy of Logistic regression training set:\",   round(lr_training_prediction,3))","4a8a559a":"#Creating K fold Cross-validation \nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nscores = cross_val_score(lr, # model\n                         X_train_std, # Feature matrix\n                         y_train, # Target vector\n                         cv=kf, # Cross-validation technique\n                         scoring=\"accuracy\", # Loss function\n                         n_jobs=-1) # Use all CPU scores\nprint('10 fold CV accuracy: %.3f +\/- %.3f' % (np.mean(scores), np.std(scores)))","92657e12":"from sklearn.model_selection import cross_val_predict\nprint('The cross validated score for Logistic Regression Classifier is:',round(scores.mean()*100,2))\ny_pred = cross_val_predict(lr,X_train_std,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,y_pred),annot=True,fmt='3.0f',cmap=\"winter\")\nplt.title('Confusion_matrix', y=1.05, size=15)","522257ce":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(criterion='gini', n_estimators=400,\n                             min_samples_split=10,min_samples_leaf=1,\n                             max_features='auto',oob_score=True,\n                             random_state=42,n_jobs=-1)\nrf.fit(X_train_std,y_train)\n# Predict using model\nrf_training_pred = rf.predict(X_train_std)\nrf_training_prediction = accuracy_score(y_train, rf_training_pred)\n\nprint(\"Accuracy of Random Forest training set:\",   round(rf_training_prediction,3))","94e7238a":"kf = KFold(n_splits=10, shuffle=True, random_state=42)\nscores = cross_val_score(rf, # model\n                         X_train_std, # Feature matrix\n                         y_train, # Target vector\n                         cv=kf, # Cross-validation technique\n                         scoring=\"accuracy\", # Loss function\n                         n_jobs=-1) # Use all CPU scores\nprint('10 fold CV accuracy: %.3f +\/- %.3f' % (np.mean(scores), np.std(scores)))","5870db34":"from sklearn.model_selection import cross_val_predict\nprint('The cross validated score for Random Forest Classifier is:',round(scores.mean()*100,2))\ny_pred = cross_val_predict(rf,X_train_std,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,y_pred),annot=True,fmt='3.0f',cmap=\"winter\")\nplt.title('Confusion_matrix', y=1.05, size=15)","9207d6d6":"print (\"\\n\\n ---Logistic Regression Model---\")\nlr_auc = roc_auc_score(y_test, lr.predict(X_test_std))\n\nprint (\"Logistic Regression AUC = %2.2f\" % lr_auc)\nprint(classification_report(y_test, lr.predict(X_test_std)))\n\nprint (\"\\n\\n ---Random Forest Model---\")\nrf_roc_auc = roc_auc_score(y_test, rf.predict(X_test_std))\n\nprint (\"Random Forest AUC = %2.2f\" % rf_roc_auc)\nprint(classification_report(y_test, rf.predict(X_test_std)))","ab42daaa":"# Create ROC Graph\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, lr.predict_proba(X_test_std)[:,1])\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, rf.predict_proba(X_test_std)[:,1])\n\n\nplt.figure()\n\n# Plot Logistic Regression ROC\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % lr_auc)\n\n# Plot Random Forest ROC\nplt.plot(rf_fpr, rf_tpr, label='Random Forest Classifier (area = %0.2f)' % rf_roc_auc)\n\n\n# Plot Base Rate ROC\nplt.plot([0,1], [0,1],label='Base Rate')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Graph')\nplt.legend(loc=\"lower right\")\nplt.show()","8370d853":"columns = X.columns\ntrain = pd.DataFrame(np.atleast_2d(X_train_std), columns=columns) # Converting numpy array list into dataframes","4d966680":"# Get Feature Importances\nfeature_importances = pd.DataFrame(rf.feature_importances_,\n                                   index = train.columns,\n                                    columns=['importance']).sort_values('importance', ascending=False)\nfeature_importances = feature_importances.reset_index()\nfeature_importances.head(10)","63c5f6ce":"sns.set(style=\"whitegrid\")\n\n# Initialize the matplotlib figure\nf, ax = plt.subplots(figsize=(13, 7))\n\n# Plot the Feature Importance\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=\"importance\", y='index', data=feature_importances[0:10],\n            label=\"Total\", color=\"b\")","29433c3e":"# Check for Missing Values","15b24296":"From the stacked bar chart it seems that there more chances of user clicking on a ad if its a thursday!","1ecbb491":"# Building a Basic Model","302c449b":"# Summarizing Categorical Variables","3ea89cbe":"# Load Data ","532f35c1":"# Test Models Performance","7d4657b9":"Line chart showing the count of clicks for each month. Grouped bar chart shows distribution of target variable across 7 months. 2nd Month seems to be the best for clicking on a Ad.","8af58c6e":"# Data type and length of the variables","9715a30f":"Heatmap gives us better understanding of relationship between each feature. Correlation is measured between __-1__ and __1__. Higher the absolute value, higher is the degree of correlation between the variables. We expect daily internet usage and daily time spent on site to be more correlated with our target variable. Also, none of our explantory variables seems to correlate with each other which indicates there is no collinearity in our data. ","a57dee1b":"Distribution of clicks by gender. It seems that more number of females have clicked on ad.","6fb21be9":"Line chart here indicates that user tends to click on a Ad later in a day or probably early in the morning. It is expected based on the age feature that most people are working so it seems appropriate as they either find time early or late in the day. Also sunday seems to be effective for clicking on a ad from the bar chart.","3b442d74":"# Duplicates Checkup","ad28bdc7":"# Visualize Target Variable ","6e5c0a14":"There dont seems to be any effect of month, day, weekday and hour on the target variable. ","598870ad":"# Examine the data","6df89999":"## Results for Basic Model","305faedf":"# Basic model building based on the actual data","4335aa8f":"As we have many different cities (__Unique__) and also not many people belonging to a same city(__freq__). So, it probably means that this feature is having no or very less predictive power. However we have less diversity with country feature so we have to further investigate it.","3653dae0":"# Load Libraries","e7c419b1":"# Random Forest Model","c59c1822":"### Investing Country Variable","25c8a4a4":"# Extracting Datetime Variables\n\nUtilizing timestamp feature to better understand the pattern when a user is clicking on a ad.","ff55970f":"We can see that more people aged between 30 to 40 are spending more time on site daily.","297efe43":"# Recommendations","078e83ca":"# Performance Metrics","b2a031c7":"Average profile of a user who will click on a ad or not.","9a5a542d":"As the __mean__ and __median__(50% percentile) are very similar to each other which indicates that our data is not skewed and we do not require any data transformations.We shall confirm this by visualizing as well.","68f57fde":"# ROC Graph","1f953511":"We can see that the feature __Male(Gender)__ does not contribute to the model (i.e., see x4) so we can actually remove that variable from our model. After removing the variable if the __Adjusted R-squared has not changed__ from the previous model. Then we could conclude that the feature indeed was not contributing to the model. Looks like the contributing features for the model are:\n\n- Daily Time Spent on site\n- Daily Internet Usage\n- Age\n- Country\n- Area income","75cf1552":"# Correlation Between Variables","a289a9b2":"# Predictions","b5588729":"Above are the dominant features our model is predicting so our target population are the people:\n\n- Who Spends less time on the internet\n- Who spends less time on the website\n- Who has lower income\n- Who are older than our average sample (mean around 40 years old)","7fe01912":" <h1><center>Ad Click Prediction<\/center><\/h1>","83e84df2":"## Clicked Vs Not Clicked","17440d19":"# Numerical and Categorical Variables Identification","1ab8f53b":"# Building Logistic Regression Model","6bbd8610":"Now we need to see how far our predictions met the actual test data (y_test) by performing evaluations using classification report & confusion matrix on the target variable and the predictions.\n\n**confusion matrix** is used to evaluate the model behavior from a matrix. Below is how a confusion matrix looks like:\n\n                    Predicted No   Predicted Yes\n    Actual No          TN                 FP \n\n    Actual Yes         FN                 TP   \n\nTP- True Positive \tTN- True Negative \nFP- False Positive \tFN- False Negative \n\nTrue Positive is the proportion of positives that are correctly identified. Similarly, True Negative is the proportion of negatives that are correctly identified. False Positive is the condition where we predict a result that is actually doesn't fulfill. Similarly, False Negative is the condition where the prediction failed, when it was actually successful.\n\nIf we want to calculate any specific value, we can do it from confusion matrix directly.\n\n**classification_report** will basically tell us the precision, recall value's accuracy, f1 score & support. This way we don't have to read it ourself from a confusion matrix.\n\n**precision** is the fraction of retrieved values that are relevant to the data. The precision is the ratio of tp \/ (tp + fp).\n\n**recall** is the fraction of successfully retrieved values that are relevant to the data. The recall is the ratio of          tp \/ (tp + fn). \n\n**f1-score** is the harmonic mean of precision and recall. Where an fscore reaches its best value at 1 and worst score at 0.\n\n**support** is the number of occurrences of each class in y_test.","56567c5c":"# Introduction\n\nThe goal of the project is to __Predict who is likely going to click on the Ad__ on a website based on the features of a user. Following are the features involved in this dataset which is obtained from Kaggle.\n\n|           Feature               |                  Description                           |\n|---------------------------------|--------------------------------------------------------|\n|1. __Daily Time Spent on a Site__   | Time spent by the user on a site in minutes.        |\n|2. __Age__                          | Customer's age in terms of years.                   |\n|3. __Area Income__                  | Average income of geographical area of consumer.    |\n|4. __Daily Internet Usage__         | Avgerage minutes in a day consumer is on the internet.|\n|5. __Ad Topic Line__                | Headline of the advertisement.                      | \n|6. __City__                         | City of the consumer.                               |\n|7. __Male__                         | Whether or not a consumer was male.                 |\n|8. __Country__                      | Country of the consumer.                            |\n|9. __Timestamp__                    | Time at which user clicked on an Ad or the closed window.|\n|10. __Clicked on Ad__               | 0 or 1 is indicated clicking on an Ad.              |\n\n\n\nThis notebook will contain exploratory data analysis along with classification models related to this project. \n\nSteps involved in this Notebook\n\n- [Getting to know about the Data](#Examine-the-data)\n- [Extract New features](#Extracting-Datetime-Variables)\n- [Check distribution of target variable](#Visualize-Target-Variable )\n- [Understand Relationship between variables](#Distribution-and-Relationship-Between-Variables)\n- [Identifying Potential Outliers](#Identifying-Potential-Outliers-using-IQR)\n- [Building a basic model](#Basic-model-building-based-on-the-actual-data)\n- [feature engineering](#Feature-Engineering)\n- [Building Logistic Regression Model](#Building-Logistic-Regression-Model )\n- [Random Forest Model](#Random-Forest-Model)\n- [Models Performances on Test Data](#Test-Models-Performance)\n- [Feature Importances](#Random Forest Feature Importances)\n","7a62d7b1":"# Extracted Features Visualizations","1357cd1d":"Comparison of users who have clicked on ad or not in terms of age and weekday. It is clear that people with higher age tend to click on a ad.","d05dcb43":"So from the plot we can see that the number of users who click on a ad and who do not are equal in numbers i.e 500,  that makes it very interesting.","320443d9":"Pairplot represents the relationship between our target feature\/variable and explanatory variables. It provides the possible direction of the relationship between the variables. We can see that people who spend less time on site and have less income and are aged more relatively are tend to click on ad. ","2725119b":"# Summarizing Numerical Variables","f3e0c9d3":"Distribution by each hour and by gender. Overall females tend to click on a Ad more often than males.","397f08e7":"# Feature Engineering","8216c9c0":"We can observe that random forest has higher accuracy compared to logistic regression model in both test and train data sets.","4f34867b":"# Distribution and Relationship Between Variables ","0eecb4a3":"We can clearly see that daily interent usage and daily time spent on a site has 2 peaks (Bi-model in statistical terms). It indicates that there are two different groups present in our data. We dont expect the users to be normally distributed as there are people who spend more time on internet\/website and people who spend less time. Some regularly use the website and some less often so they are perfectly alright.  ","6a8e1b12":"The results from evaluation are as follows:\n\n**Confusion Matrix:** \n\nThe users that are predicted to click on commercials and the actually clicked users were 144, the people who were predicted not to click on the commercials and actually did not click on them were 156.\n\nThe people who were predicted to click on commercial and actually did not click on them are 6, and the users who were not predicted to click on the commercials and actually clicked on them are 24.\n\nWe have only a few mislabelled points which is not bad from the given size of the dataset.\n\n**Classification Report:**\n\nFrom the report obtained, the precision & recall are 0.91 which depicts the predicted values are 91% accurate. Hence the probability that the user can click on the commercial is 0.91 which is a good precision value to get a good model.  ","504640b0":"# Random Forest Feature Importances","e47ca2d7":"# Identifying Potential Outliers using IQR","0f22b176":"It seems that users are from all over the world with maximum from france and czech republic with a count of 9 each."}}