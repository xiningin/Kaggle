{"cell_type":{"f040f9eb":"code","51cec63c":"code","d96763e0":"code","f1fa5ada":"code","49336a3b":"code","5aee634c":"code","daa0a58f":"code","cbd17108":"code","5b2ddc5b":"code","e86dee6c":"code","bf990ea7":"code","0ed44690":"code","28cab5bf":"code","e6269e39":"code","9e8d1060":"code","84386f39":"code","08e9b4f0":"code","adf06d62":"code","31d57151":"code","919aec8a":"code","d8aabca5":"code","0fe56e10":"code","96b73bad":"code","1129041e":"code","a5e3984b":"code","312ba033":"code","3e5ddc57":"code","9b52bdd4":"code","e66b5079":"code","a97ac827":"markdown","a61157fc":"markdown","01efb157":"markdown","cd9db376":"markdown","760e3a8a":"markdown","4d5bedbe":"markdown","a7e1eb59":"markdown","a77f8a49":"markdown","46b3b760":"markdown","af5bb874":"markdown","193f55f4":"markdown","533e74ff":"markdown","d061e3f0":"markdown","e2fc2727":"markdown"},"source":{"f040f9eb":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom keras.datasets import fashion_mnist\n\nfrom sklearn.model_selection import KFold\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.optimizers import SGD","51cec63c":"(trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n\nprint('Train: X=%s, y=%s' % (trainX.shape, trainy.shape)) \nprint('Test: X=%s, y=%s' % (testX.shape, testy.shape))","d96763e0":"plt.figure(figsize=(10,10))\nfor i in range(9):\n    # define subplot\n    plt.subplot(330 + 1 + i)\n    # plot raw pixel data\n    plt.imshow(trainX[i], cmap=plt.get_cmap('gray')) \n# show the figure\nplt.xticks()\nplt.yticks()\nplt.xlabel('')\nplt.ylabel('')\nplt.show()","f1fa5ada":"def load_dataset():\n    (trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n    \n    trainX = trainX.reshape(trainX.shape[0], 28, 28, 1)\n    testX = testX.reshape(testX.shape[0], 28, 28, 1)\n    \n    trainY = to_categorical(trainY, num_classes=10)\n    testY = to_categorical(trainY, num_classes=10)\n    \n    return trainX, trainY, testX, testY\n\ndef prep_pixels(trainX, testX):\n    trainX = trainX.astype('float32')\n    testX = testX.astype('float32')\n    \n    trainX = trainX \/ 255.0\n    testX = testX \/ 255.0\n    \n    return trainX, testX","49336a3b":"def define_model():\n    model = Sequential()\n    model.add(Conv2D(256, (3,3), padding='same', activation='relu', \n                     kernel_initializer='he_uniform', input_shape=(28,28,1)))\n    model.add(MaxPooling2D((2,2)))\n    model.add(Flatten())\n    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform')) \n    model.add(Dense(10, activation='softmax'))\n\n    opt = SGD(lr=0.01, momentum=0.9)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\ndef evaluate_model(X, y, n_folds=5):\n    scores, histories, models = list(), list(), list()\n    \n    kfold = KFold(n_folds, shuffle=True, random_state=1)\n    \n    for train_ix, test_ix in kfold.split(X):\n        model = define_model()\n        \n        X_train, y_train, X_test, y_test = X[train_ix], y[train_ix], X[test_ix], y[test_ix]\n        history = model.fit(X_train, y_train, epochs=10, \n                            batch_size=32, validation_data=(X_test,y_test), verbose=1)\n        \n        _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n        print('> %.3f' % (accuracy * 100.0))\n        \n        # append scores\n        scores.append(accuracy)\n        histories.append(history)\n        models.append(model)\n        \n    return scores, histories, models       ","5aee634c":"def summarize_diagnostics(histories):\n    plt.figure(figsize=(10,5))\n    for i in range(len(histories)):\n    # plot loss\n        plt.subplot(211)\n        plt.title('Cross Entropy Loss')\n        plt.plot(histories[i].history['loss'], color='blue', label='train') \n        plt.plot(histories[i].history['val_loss'], color='orange', label='test')\n\n        # plot accuracy\n        plt.subplot(212)\n        plt.title('Classification Accuracy') \n        plt.plot(histories[i].history['accuracy'], color='blue', label='train') \n        plt.plot(histories[i].history['val_accuracy'], color='orange', label='test')\n    plt.show()","daa0a58f":"def summarize_performance(scores):\n# print summary\n    print('Accuracy: mean=%.3f std=%.3f, n=%d' % (np.mean(scores)*100, np.std(scores)*100,\n      len(scores)))\n  # box and whisker plots of results\n    plt.figure(figsize=(10,5))\n    plt.boxplot(scores)\n    plt.show()","cbd17108":"def run_test_harness():\n    # load dataset\n    trainX, trainY, testX, testY = load_dataset()\n\n    trainX, testX = prep_pixels(trainX, testX)\n\n    scores, histories, models = evaluate_model(trainX, trainY)\n    \n    return scores, histories, models\n\n\n# entry point, run the test harness\nscores, histories, models = run_test_harness()","5b2ddc5b":"summarize_diagnostics(histories)\n# summarize estimated performance\nsummarize_performance(scores)","e86dee6c":"data = testX.astype('float32')\/100\nlabels = testy","bf990ea7":"def predict_data(models, data):\n    probs = list()\n    for model in models:\n        prob = model.predict(data.reshape(data.shape[0], 28, 28, 1))\n        probs.append(prob)\n\n    sum_probs = np.sum(probs, axis=0)\n    most_predicted_class = np.argmax(sum_probs, axis=1)\n\n    return most_predicted_class","0ed44690":"from sklearn.metrics import accuracy_score\npredictions = predict_data(models, data)\n\naccuracy_score(predictions, labels)","28cab5bf":"from keras.datasets import cifar10\n\n(trainX, trainy), (testX, testy) = cifar10.load_data()\n\nprint('Train: X=%s, y=%s' % (trainX.shape, trainy.shape)) \nprint('Test: X=%s, y=%s' % (testX.shape, testy.shape))","e6269e39":"plt.figure(figsize=(10,10))\nfor i in range(9):\n    # define subplot\n    plt.subplot(330 + 1 + i)\n    # plot raw pixel data\n    plt.imshow(trainX[i], cmap=plt.get_cmap('gray')) \n# show the figure\nplt.xticks()\nplt.yticks()\nplt.xlabel('')\nplt.ylabel('')\nplt.show()","9e8d1060":"def load_dataset():\n    (trainX, trainY), (testX, testY) = cifar10.load_data()\n    \n    trainX = trainX.reshape(trainX.shape[0], 32, 32, 3)\n    testX = testX.reshape(testX.shape[0], 32, 32, 3)\n    \n    trainY = to_categorical(trainY, num_classes=10)\n    testY = to_categorical(testY, num_classes=10)\n    \n    return trainX, trainY, testX, testY","84386f39":"X_train, y_train, X_test, y_test = load_dataset()\n\nX_train, X_test = prep_pixels(X_train, X_test)","08e9b4f0":"def define_model():\n    model = Sequential()\n    model.add(Conv2D(32, (3,3), activation='relu', kernel_initializer='he_uniform', \n                    padding='same', input_shape=(32, 32, 3)))\n    model.add(Conv2D(32, (3,3), activation='relu', kernel_initializer='he_uniform', \n                    padding='same'))\n    model.add(MaxPooling2D((2,2)))\n    \n    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', \n                    padding='same'))\n    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', \n                    padding='same'))\n    model.add(MaxPooling2D((2,2)))\n    \n    model.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', \n                    padding='same'))\n    model.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', \n                    padding='same'))\n    model.add(MaxPooling2D((2,2)))\n    \n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    \n    model.add(Dense(10, activation='softmax'))\n    \n    opt = SGD(lr=0.01, momentum=0.9)\n    \n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n    \n    return model","adf06d62":"from keras.callbacks import ReduceLROnPlateau\nmodel = define_model()\nrlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, \n                              min_delta=1E-7)\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=64, \n                    verbose=1, callbacks=[rlrop], validation_data=(X_test, y_test))","31d57151":"_, accuracy = model.evaluate(X_test, y_test, verbose=0)","919aec8a":"def summarize_diagnostics(history):\n    plt.figure(figsize=(10,5))\n    plt.subplot(211)\n    plt.title('Cross Entropy Loss')\n    plt.plot(history.history['loss'], color='blue', label='train') \n    plt.plot(history.history['val_loss'], color='orange', label='test')\n\n        # plot accuracy\n    plt.subplot(212)\n    plt.title('Classification Accuracy') \n    plt.plot(history.history['accuracy'], color='blue', label='train') \n    plt.plot(history.history['val_accuracy'], color='orange', label='test')\n    plt.show()","d8aabca5":"summarize_diagnostics(history)\nprint(accuracy)","0fe56e10":"from keras.layers import Dropout\n\ndef define_model():\n    model = Sequential()\n    model.add(Conv2D(32, (3,3), activation='relu', kernel_initializer='he_uniform', \n                    padding='same', input_shape=(32, 32, 3)))\n    model.add(Conv2D(32, (3,3), activation='relu', kernel_initializer='he_uniform', \n                    padding='same'))\n    model.add(MaxPooling2D((2,2)))\n    model.add(Dropout(0.2))\n    \n    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', \n                    padding='same'))\n    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', \n                    padding='same'))\n    model.add(MaxPooling2D((2,2)))\n    model.add(Dropout(0.2))\n    \n    model.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', \n                    padding='same'))\n    model.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', \n                    padding='same'))\n    model.add(MaxPooling2D((2,2)))\n    model.add(Dropout(0.2))\n    \n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dropout(0.2))\n    \n    model.add(Dense(10, activation='softmax'))\n    \n    opt = SGD(lr=0.01, momentum=0.9)\n    \n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n    \n    return model","96b73bad":"model = define_model()\nrlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, \n                              min_delta=1E-7)\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=64, \n                    verbose=1, callbacks=[rlrop], validation_data=(X_test, y_test))","1129041e":"_, accuracy = model.evaluate(X_test, y_test, verbose=0)\nsummarize_diagnostics(history)\nprint(accuracy)","a5e3984b":"from keras.preprocessing.image import ImageDataGenerator\n\n# create data generator\ndatagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1,\n      horizontal_flip=True)\n\n# prepare iterator\nit_train = datagen.flow(X_train, y_train, batch_size=64)\n\nsteps = int(X_train.shape[0] \/ 64)\n\n\nmodel = define_model()\nrlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, \n                              min_delta=1E-7)\n\nhistory = model.fit_generator(it_train, steps_per_epoch=steps, epochs=60,\n      validation_data=(X_test, y_test), callbacks=[rlrop], verbose=1)\n","312ba033":"_, accuracy = model.evaluate(X_test, y_test, verbose=0)\nsummarize_diagnostics(history)\nprint(accuracy)","3e5ddc57":"from keras.layers import BatchNormalization\n\ndef define_model():\n    model = Sequential()\n    model.add(Conv2D(32, (3,3), activation='relu', kernel_initializer='he_uniform', \n                    padding='same', input_shape=(32, 32, 3)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, (3,3), activation='relu', kernel_initializer='he_uniform', \n                    padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((2,2)))\n    \n    model.add(Dropout(0.2))\n    \n    \n    \n    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', \n                    padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, (3,3), activation='relu', kernel_initializer='he_uniform', \n                    padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((2,2)))\n    \n    model.add(Dropout(0.3))\n    \n    \n    \n    model.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', \n                    padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform', \n                    padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((2,2)))\n    \n    model.add(Dropout(0.4))\n    \n    \n    \n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    model.add(BatchNormalization())\n    \n    model.add(Dropout(0.5))\n    \n    \n    \n    model.add(Dense(10, activation='softmax'))\n    \n    opt = SGD(lr=0.01, momentum=0.9)\n    \n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n    \n    return model","9b52bdd4":"# create data generator\ndatagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1,\n      horizontal_flip=True)\n\n# prepare iterator\nit_train = datagen.flow(X_train, y_train, batch_size=64)\n\nsteps = int(X_train.shape[0] \/ 64)\n\n\nmodel = define_model()\nrlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, \n                              min_delta=1E-7)\n\nhistory = model.fit_generator(it_train, steps_per_epoch=steps, epochs=60,\n      validation_data=(X_test, y_test), callbacks=[rlrop], verbose=1)","e66b5079":"_, accuracy = model.evaluate(X_test, y_test, verbose=0)\nsummarize_diagnostics(history)\nprint(accuracy)","a97ac827":"<h3><center>Fashion-MNIST<\/center><\/h3>\n\n![image.png](attachment:image.png)\n\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nThe Fashion-MNIST clothing classification problem is a new standard dataset used in computer vision and deep learning. Although the dataset is relatively simple, it can be used as the basis for learning and practicing how to develop, evaluate, and use deep convolutional neural networks for image classification from scratch.\n    <\/div>","a61157fc":"<h3><center>2. Load & Preprcoess data<\/center><\/h3>","01efb157":"<h3><center>3. Baseline Model<\/center><\/h3>","cd9db376":"Model was not able to learn the data and instead overfitted the training Data","760e3a8a":"<h3><center>4. Improvement using Dropout + Data Augmentation<\/center><\/h3>","4d5bedbe":"<h3><center>1. Explore DataSet<\/center><\/h3>","a7e1eb59":"<h3><center>3. Improvement using Dropout<\/center><\/h3>","a77f8a49":"<h3><center>Classify Small Photos of Objects<\/center><\/h3>\n\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\n# The CIFAR-10 small photo classification problem is a standard dataset used in computer vision and deep learning. Although the dataset is effectively solved, it can be used as the basis for learning and practicing how to develop, evaluate, and use convolutional deep learning neural networks for image classification from scratch.<br><br>\n    \nThe dataset is comprised of 60,000 32 \u00d7 32 pixel color photographs of objects from 10 classes, such as frogs, birds, cats, ships, etc. The class labels and their standard associated integer values are listed below.\n    <\/div>\n    \n![image.png](attachment:image.png)","46b3b760":"<h3><center>4. Predicting Data using all models<\/center><\/h3>","af5bb874":"<h3><center>1. Explore Dataset<\/center><\/h3>","193f55f4":"<h3><center>2. Prepare Baseline Model<\/center><\/h3>","533e74ff":"<h3><center>5. Improvement using Dropout + Data Augmentation + Batch Normalization<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nWe can increase the regularization by changing the dropout from a fixed pattern to an increasing pattern.\nFinally, we can change dropout from a fixed amount at each level, to instead using an increasing trend with less dropout in early layers and more dropout in later layers, increasing 10% each time from 20% after the first block to 50% at the fully connected layer. This type of increasing dropout with the depth of the model is a common pattern. It is effective as it forces layers deep in the model to regularize more than layers closer to the input.<\/div>","d061e3f0":"<h3><center>PART II<\/center><\/h3>","e2fc2727":"\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nThe Fashion-MNIST dataset is proposed as a more challenging replacement dataset for the MNIST dataset. It is a dataset comprised of 60,000 small square 28 \u00d7 28 pixel grayscale images of items of 10 types of clothing, such as shoes, t-shirts, dresses, and more. The mapping of all 0-9 integers to class labels is listed below\n    <\/div>\n    \n![image.png](attachment:image.png)\n    \n    "}}