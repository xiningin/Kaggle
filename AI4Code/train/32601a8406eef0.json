{"cell_type":{"e76308e4":"code","dde353c8":"code","88724909":"code","01ad27e3":"code","6458dc85":"code","aeb45638":"code","279259a4":"code","e85c6e65":"code","7d2456fa":"code","c7009a70":"code","988f3307":"code","9b5bab11":"code","030aac7e":"code","ccc1f24b":"code","f14eb6c3":"code","719f77d9":"code","862067b3":"code","df4e34ed":"code","13771c3e":"code","05f50312":"code","08625369":"code","30652a66":"code","e27c7400":"code","082c6590":"code","6058e108":"code","735529e2":"code","8bbebcb2":"code","2ab983f8":"code","788f7e2e":"code","bce5bb51":"code","d487daa3":"code","edf1009b":"code","7ab90225":"code","3f701468":"code","a7007b90":"code","e93effcd":"code","85142477":"code","ec90b8e3":"code","6fb8f1f2":"code","a99036e2":"code","7a8a013c":"code","44ac1797":"code","0d6ee77b":"code","f76b31b0":"code","1b9e5724":"code","af7b34db":"code","f18c9893":"code","be321bb7":"code","1a748382":"code","1ef3fdad":"code","ce6b74a0":"code","c16d3007":"code","7ac144db":"code","d36c2c7b":"code","745651e9":"code","0618753b":"code","f979b6b5":"code","3f04ad5a":"code","e9ed8e6c":"code","2e291f04":"code","53e81759":"code","5b025f21":"code","0461279f":"code","09ed98fb":"code","d75ee598":"code","162631c3":"code","bd05bd23":"code","200c47c7":"code","3ddfe940":"code","d673cc3f":"code","b2d521a8":"code","be516773":"code","a9ee9c19":"code","f74d1132":"code","ef8487ec":"code","d5fd0cd1":"code","cd57d7d3":"code","5f2393f7":"code","ef4a088c":"code","976d9110":"code","5cd5ec78":"code","cfef9097":"code","2576d653":"code","6f019326":"code","ec29443d":"code","297f262a":"code","34b551bc":"code","97db9d39":"markdown","1abd806a":"markdown","8fa423a4":"markdown"},"source":{"e76308e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","dde353c8":"import warnings\nwarnings.filterwarnings('ignore')\nfrom tqdm import tqdm_notebook\nimport re\nfrom bs4 import BeautifulSoup\nimport os\nimport re\nimport gc\nimport sys\nimport time\nimport json\nimport random\nimport unicodedata\nimport multiprocessing\nfrom functools import partial, lru_cache\n\nimport emoji\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.externals import joblib\nfrom tqdm import tqdm, tqdm_notebook\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom nltk import TweetTokenizer\nfrom nltk.stem import PorterStemmer, SnowballStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\nimport html\nfrom sklearn.feature_extraction.text import TfidfVectorizer","88724909":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook as tqdm\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport os\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nfrom transformers import *\n\nnp.set_printoptions(suppress=True)\nprint(tf.__version__)","01ad27e3":"GOOGLE_PATH = \"..\/input\/google-quest-challenge\/\"\nSTACK_PATH = \"..\/input\/stackexchange123\/StackexchangeExtract\/\"\n\ntrain = pd.read_csv(GOOGLE_PATH+\"train.csv\")\ntest = pd.read_csv(GOOGLE_PATH+\"test.csv\")\nsample_submission = pd.read_csv(GOOGLE_PATH+\"sample_submission.csv\")\n\ntrain.shape, test.shape","6458dc85":"target_columns = sample_submission.columns[1:].values\ntarget_columns.shape","aeb45638":"def url_id_ex(url):\n    try:\n        ids = int(url.split(\"\/\")[-2])\n    except:\n        ids = int(url.split(\"\/\")[-1])\n    return ids\n\n# category_type\ntrain[\"category_type\"] = train[\"url\"].apply(lambda x : x.split(\".\")[0].split(\"\/\")[-1])\ntest[\"category_type\"] = test[\"url\"].apply(lambda x : x.split(\".\")[0].split(\"\/\")[-1])\n\ntrain[\"quser_id\"] = train[\"question_user_page\"].apply(lambda x: int(x.split(\"\/\")[-1]))\ntrain[\"auser_id\"] = train[\"answer_user_page\"].apply(lambda x: int(x.split(\"\/\")[-1]))\ntrain[\"url_id\"] = train[\"url\"].apply(url_id_ex)\n\ntest[\"quser_id\"] = test[\"question_user_page\"].apply(lambda x: int(x.split(\"\/\")[-1]))\ntest[\"auser_id\"] = test[\"answer_user_page\"].apply(lambda x: int(x.split(\"\/\")[-1]))\ntest[\"url_id\"] = test[\"url\"].apply(url_id_ex)\n\ntrain.category_type.replace(\"programmers\", \"softwareengineering\", inplace=True)\ntest.category_type.replace(\"programmers\", \"softwareengineering\", inplace=True)","279259a4":"def final_dataframe(files_path, df):\n    listofdir = list(os.listdir(files_path))\n    listofdir.remove('dataset-metadata.json')\n    final_list = []\n    \n    posts_columns = None\n    q_users_columns = None\n    a_users_columns = None\n    \n    for file in tqdm_notebook(listofdir):\n        temp_df = df[df.category_type == file]\n        \n        temp_users = pd.read_csv(STACK_PATH+file+\"\/user_df.csv\")\n        temp_posts = pd.read_csv(STACK_PATH+file+\"\/posts_df.csv\")\n        temp_users_columns = temp_users.columns.values\n        \n        posts_columns = temp_posts.columns.values\n        temp_df = pd.merge(temp_df, temp_posts, left_on=\"url_id\", right_on=\"Id\", how=\"left\")\n        \n        del temp_posts\n        \n        temp_users = temp_users.add_prefix(\"q_\")\n        q_users_columns = temp_users.columns.values\n        temp_df = pd.merge(temp_df, temp_users, left_on=\"quser_id\", right_on=\"q_Id\", how=\"left\")\n        \n        temp_users.columns = temp_users_columns\n        \n        temp_users = temp_users.add_prefix(\"a_\")\n        a_users_columns = temp_users.columns.values\n        temp_df = pd.merge(temp_df, temp_users, left_on=\"auser_id\", right_on=\"a_Id\", how=\"left\")\n        del temp_users\n        \n        temp_df = temp_df.to_dict(\"records\")\n        final_list.extend(temp_df)\n        del temp_df\n        \n    total_columns_dic = {\n        \"posts_columns\":posts_columns,\n        \"q_users_columns\":q_users_columns,\n        \"a_users_columns\":a_users_columns\n    }\n    \n    final_df = pd.DataFrame(final_list)\n    del final_list\n        \n    return final_df, total_columns_dic","e85c6e65":"!cat \/proc\/meminfo | grep Mem","7d2456fa":"%%time\nfiles_path = STACK_PATH\ntrain_final, total_columns_dic = final_dataframe(files_path, train)\ntest_final, total_columns_dic = final_dataframe(files_path, test)","c7009a70":"train_final.shape, test_final.shape","988f3307":"stackof_train = train[train.category == \"STACKOVERFLOW\"].copy()\nstackof_test = test[test.category == \"STACKOVERFLOW\"].copy()\nstackof_train.shape , stackof_test.shape","9b5bab11":"train_final = train_final.append(stackof_train)\ntest_final = test_final.append(stackof_test)\ntrain_final.shape, test_final.shape","030aac7e":"test_columns = test.columns.values.tolist()\ntrain_columns = train.columns.values.tolist()\ntrain_final = pd.merge(train, train_final, left_on=train_columns, right_on=train_columns, how=\"left\")\ntest_final = pd.merge(test, test_final, left_on=test_columns, right_on=test_columns, how=\"left\")","ccc1f24b":"train_final.shape , test_final.shape","f14eb6c3":"!cat \/proc\/meminfo | grep Mem","719f77d9":"from pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\"\n    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\n    \"\"\"\n\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n\n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            continue\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            #df[col] = df[col].astype(\"category\")\n            pass\n        \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df\n\nfrom bs4 import BeautifulSoup\ndef removing_html_tags(raw_html):\n    cleantext = BeautifulSoup(raw_html, \"lxml\").text\n    return cleantext\n\ndef replace_urls(text):\n    text = re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', ' web ', text, flags=re.MULTILINE)\n    return text\n\ndef clean_AboutMe(text):\n    text = removing_html_tags(text)\n    text = replace_urls(text)\n    return text\n\ndef clean_Name(text):\n    text = str(text)\n    text = re.sub(r' ', '_', text, flags=re.MULTILINE)\n    text = re.sub(r',',' ', text, flags=re.MULTILINE)\n    return text\n\ndef clean_Class(text):\n    text = str(text)\n    text = re.sub(r',', ' ',text, flags=re.MULTILINE)\n    return text\n\ndef log_transform_apply(value):\n    if value == 0:\n        return value\n    elif value < 1:\n        value = np.log1p(abs(value))\n        return -value\n    else:\n        return np.log1p(value)","862067b3":"train_final_targets = train_final[target_columns].copy()\ntrain_final_targets[\"qa_id\"] = train_final[\"qa_id\"]\ntrain_final.drop(columns=target_columns, inplace=True)","df4e34ed":"train_final = reduce_mem_usage(train_final)\ntest_final = reduce_mem_usage(test_final)","13771c3e":"# User AboutMe\ntrain_final[\"q_AboutMe_nan\"] = train_final[\"q_AboutMe\"].isnull().astype(int)\ntrain_final[\"a_AboutMe_nan\"] = train_final[\"a_AboutMe\"].isnull().astype(int)   \n\ntest_final[\"q_AboutMe_nan\"] = test_final[\"q_AboutMe\"].isnull().astype(int)\ntest_final[\"a_AboutMe_nan\"] = test_final[\"a_AboutMe\"].isnull().astype(int) \n\ntrain_final[\"q_AboutMe\"].fillna(\"nane\", inplace=True)\ntrain_final[\"a_AboutMe\"].fillna(\"nane\", inplace=True)\n\ntest_final[\"q_AboutMe\"].fillna(\"nane\", inplace=True)\ntest_final[\"a_AboutMe\"].fillna(\"nane\", inplace=True)\n\ntrain_final[\"q_AboutMe\"] = train_final[\"q_AboutMe\"].apply(clean_AboutMe)\ntrain_final[\"a_AboutMe\"] = train_final[\"a_AboutMe\"].apply(clean_AboutMe)\n\ntest_final[\"q_AboutMe\"] = test_final[\"q_AboutMe\"].apply(clean_AboutMe)\ntest_final[\"a_AboutMe\"] = test_final[\"a_AboutMe\"].apply(clean_AboutMe)\n\n# User TagBased\n\ntrain_final[\"q_TagBased_nan\"] = train_final[\"q_TagBased\"].isnull().astype(int)\ntrain_final[\"a_TagBased_nan\"] = train_final[\"a_TagBased\"].isnull().astype(int)   \n\ntest_final[\"q_TagBased_nan\"] = test_final[\"q_TagBased\"].isnull().astype(int)\ntest_final[\"a_TagBased_nan\"] = test_final[\"a_TagBased\"].isnull().astype(int) \n\n\ntrain_final[\"q_TagBased\"].fillna(\"nane\", inplace=True)\ntrain_final[\"a_TagBased\"].fillna(\"nane\", inplace=True)\n\ntest_final[\"q_TagBased\"].fillna(\"nane\", inplace=True)\ntest_final[\"a_TagBased\"].fillna(\"nane\", inplace=True)\n\ntrain_final[\"q_TagBased\"] = train_final[\"q_TagBased\"].apply(clean_Class)\ntrain_final[\"a_TagBased\"] = train_final[\"a_TagBased\"].apply(clean_Class)\n\ntest_final[\"q_TagBased\"] = test_final[\"q_TagBased\"].apply(clean_Class)\ntest_final[\"a_TagBased\"] = test_final[\"a_TagBased\"].apply(clean_Class)\n\n# User Name\n\ntrain_final[\"q_Name_nan\"] = train_final[\"q_Name\"].isnull().astype(int)\ntrain_final[\"a_Name_nan\"] = train_final[\"a_Name\"].isnull().astype(int)   \n\ntest_final[\"q_Name_nan\"] = test_final[\"q_Name\"].isnull().astype(int)\ntest_final[\"a_Name_nan\"] = test_final[\"a_Name\"].isnull().astype(int) \n\n\ntrain_final[\"q_Name\"].fillna(\"nane\", inplace=True)\ntrain_final[\"a_Name\"].fillna(\"nane\", inplace=True)\n\ntest_final[\"q_Name\"].fillna(\"nane\", inplace=True)\ntest_final[\"a_Name\"].fillna(\"nane\", inplace=True)\n\ntrain_final[\"q_Name\"] = train_final[\"q_Name\"].apply(clean_Name)\ntrain_final[\"a_Name\"] = train_final[\"a_Name\"].apply(clean_Name)\n\ntest_final[\"q_Name\"] = test_final[\"q_Name\"].apply(clean_Name)\ntest_final[\"a_Name\"] = test_final[\"a_Name\"].apply(clean_Name)\n\n# User Class\n\ntrain_final[\"q_Class\"].fillna(\"0\", inplace=True)\ntrain_final[\"a_Class\"].fillna(\"0\", inplace=True)\n\ntest_final[\"q_Class\"].fillna(\"0\", inplace=True)\ntest_final[\"a_Class\"].fillna(\"0\", inplace=True)\n\ntrain_final[\"q_Class\"] = train_final[\"q_Class\"].apply(clean_Class)\ntrain_final[\"a_Class\"] = train_final[\"a_Class\"].apply(clean_Class)\n\ntest_final[\"q_Class\"] = test_final[\"q_Class\"].apply(clean_Class)\ntest_final[\"a_Class\"] = test_final[\"a_Class\"].apply(clean_Class)\n\n# User Views\ntrain_final[\"q_Views_nan\"] = train_final[\"q_Views\"].isnull().astype(int)\ntrain_final[\"a_Views_nan\"] = train_final[\"a_Views\"].isnull().astype(int)   \n\ntest_final[\"q_Views_nan\"] = test_final[\"q_Views\"].isnull().astype(int)\ntest_final[\"a_Views_nan\"] = test_final[\"a_Views\"].isnull().astype(int) \n\ntrain_final[\"q_Views\"].fillna(0, inplace=True)\ntrain_final[\"a_Views\"].fillna(0, inplace=True)\n\ntest_final[\"q_Views\"].fillna(0, inplace=True)\ntest_final[\"a_Views\"].fillna(0, inplace=True)\n\n# User UpVotes\n#train_final[\"q_Views_nan\"] = train_final[\"q_Views\"].isnull().astype(int)\n#train_final[\"a_Views_nan\"] = train_final[\"a_Views\"].isnull().astype(int)   \n\n#test_final[\"q_Views_nan\"] = test_final[\"q_Views\"].isnull().astype(int)\n#test_final[\"a_Views_nan\"] = test_final[\"a_Views\"].isnull().astype(int)\n\n# User UpVotes\n\ntrain_final[\"q_UpVotes_nan\"] = train_final[\"q_UpVotes\"].isnull().astype(int)\ntrain_final[\"a_UpVotes_nan\"] = train_final[\"a_UpVotes\"].isnull().astype(int)   \n\ntest_final[\"q_UpVotes_nan\"] = test_final[\"q_UpVotes\"].isnull().astype(int)\ntest_final[\"a_UpVotes_nan\"] = test_final[\"a_UpVotes\"].isnull().astype(int) \n\ntrain_final[\"q_UpVotes\"].fillna(0, inplace=True)\ntrain_final[\"a_UpVotes\"].fillna(0, inplace=True)\n\ntest_final[\"q_UpVotes\"].fillna(0, inplace=True)\ntest_final[\"a_UpVotes\"].fillna(0, inplace=True)\n\n# User DownVotes\ntrain_final[\"q_DownVotes_nan\"] = train_final[\"q_DownVotes\"].isnull().astype(int)\ntrain_final[\"a_DownVotes_nan\"] = train_final[\"a_DownVotes\"].isnull().astype(int)   \n\ntest_final[\"q_DownVotes_nan\"] = test_final[\"q_DownVotes\"].isnull().astype(int)\ntest_final[\"a_DownVotes_nan\"] = test_final[\"a_DownVotes\"].isnull().astype(int) \n\ntrain_final[\"q_DownVotes\"].fillna(0, inplace=True)\ntrain_final[\"a_DownVotes\"].fillna(0, inplace=True)\n\ntest_final[\"q_DownVotes\"].fillna(0, inplace=True)\ntest_final[\"a_DownVotes\"].fillna(0, inplace=True)\n\nuser_drop_cols = [\"q_Id\", \"q_DisplayName\", \"q_UserId\", \"a_Id\", \"a_DisplayName\", \"a_UserId\"]","05f50312":"def clean_Tags(text):\n    text = str(text)\n    text = re.sub(r'><', '> <', text, flags=re.MULTILINE)\n    text = re.sub(r'>', '', text, flags=re.MULTILINE)\n    text = re.sub(r'<', '', text, flags=re.MULTILINE)\n    text = ''.join([i for i in text if not i.isdigit()])\n    return text","08625369":"# Posts PostTypeId\ntrain_final[\"PostTypeId_nan\"] = train_final[\"PostTypeId\"].isnull().astype(int)\ntest_final[\"PostTypeId_nan\"] = test_final[\"PostTypeId\"].isnull().astype(int) \n\ntrain_final[\"PostTypeId\"].fillna(1.0, inplace=True)\ntest_final[\"PostTypeId\"].fillna(1.0, inplace=True)\n\n# Posts Score\ntrain_final[\"Score_nan\"] = train_final[\"Score\"].isnull().astype(int)\ntest_final[\"Score_nan\"] = test_final[\"Score\"].isnull().astype(int) \n\ntrain_final[\"Score\"].fillna(0, inplace=True)\ntest_final[\"Score\"].fillna(0, inplace=True)\n\ntrain_final[\"Score\"] = train_final[\"Score\"].apply(log_transform_apply)\ntest_final[\"Score\"] = test_final[\"Score\"].apply(log_transform_apply)\n\n# Posts ViewCount\ntrain_final[\"ViewCount_nan\"] = train_final[\"ViewCount\"].isnull().astype(int)\ntest_final[\"ViewCount_nan\"] = test_final[\"ViewCount\"].isnull().astype(int) \n\ntrain_final[\"ViewCount\"].fillna(0, inplace=True)\ntest_final[\"ViewCount\"].fillna(0, inplace=True)\n\ntrain_final[\"ViewCount\"] = np.log1p(abs(train_final[\"ViewCount\"]))\ntest_final[\"ViewCount\"] = np.log1p(abs(test_final[\"ViewCount\"]))\n\n# Posts Tags\ntrain_final[\"Tags_nan\"] = train_final[\"Tags\"].isnull().astype(int)\ntest_final[\"Tags_nan\"] = test_final[\"Tags\"].isnull().astype(int) \n\ntrain_final[\"Tags\"].fillna(\"<nanetag>\", inplace=True)\ntest_final[\"Tags\"].fillna(\"<nanetag>\", inplace=True)\n\ntrain_final[\"Tags\"] = train_final[\"Tags\"].apply(clean_Tags)\ntest_final[\"Tags\"] = test_final[\"Tags\"].apply(clean_Tags)\n\n\n# Posts AnswerCount\ntrain_final[\"AnswerCount_nan\"] = train_final[\"AnswerCount\"].isnull().astype(int)\ntest_final[\"AnswerCount_nan\"] = test_final[\"AnswerCount\"].isnull().astype(int) \n\ntrain_final[\"AnswerCount\"].fillna(1, inplace=True)\ntest_final[\"AnswerCount\"].fillna(1, inplace=True)\n\n\n# Posts CommentCount\ntrain_final[\"CommentCount_nan\"] = train_final[\"CommentCount\"].isnull().astype(int)\ntest_final[\"CommentCount_nan\"] = test_final[\"CommentCount\"].isnull().astype(int) \n\ntrain_final[\"CommentCount\"].fillna(0, inplace=True)\ntest_final[\"CommentCount\"].fillna(0, inplace=True)\n\n# Posts FavoriteCount\ntrain_final[\"FavoriteCount_nan\"] = train_final[\"FavoriteCount\"].isnull().astype(int)\ntest_final[\"FavoriteCount_nan\"] = test_final[\"FavoriteCount\"].isnull().astype(int) \n\ntrain_final[\"FavoriteCount\"].fillna(0, inplace=True)\ntest_final[\"FavoriteCount\"].fillna(0, inplace=True)\n\ntrain_final[\"FavoriteCount\"] = train_final[\"FavoriteCount\"].apply(log_transform_apply)\ntest_final[\"FavoriteCount\"] = test_final[\"FavoriteCount\"].apply(log_transform_apply)\n\n\nposts_drop_cols = ['Id', 'AcceptedAnswerId', 'OwnerUserId', 'LastActivityDate', 'ParentId', 'ClosedDate', 'LastEditorDisplayName', 'OwnerDisplayName', 'CommunityOwnedDate']","30652a66":"def get_code_html(text, body):\n    if text == np.nan:\n        body = str(body)\n        code_list = []\n        codes_list1 = re.findall(':\\n\\n.*?\\n\\n\\n',body, flags=re.DOTALL)\n        codes_list2 = re.findall('.\\n\\n(.*?)\\n\\n\\n',body, flags=re.DOTALL)\n        codes_list3 = re.findall('{(.*?)}',body, flags=re.DOTALL)\n        code_list.extend(codes_list1)\n        code_list.extend(codes_list2)\n        code_list.extend(codes_list3)\n        if len(codes_list) > 0:\n            code = '<#next#>'.join(map(str, codes_list))\n            return code\n        else:\n            return \"NONE\"\n    else:\n        text = str(text)\n        codes_list = re.findall('<code>(.*?)<\/code>',text, flags=re.DOTALL)\n        if len(codes_list) > 0:\n            code = '<#next#>'.join(map(str, codes_list))\n            return code\n        else:\n            return \"NONE\"\n    \ndef get_code_replace(text, code):\n    text = str(text)\n    code = str(code)\n    if code != \"NONE\":\n        codes_list = code.split(\"<#next#>\")\n        codes_list = sorted(codes_list, key=len,reverse=True)\n        for i in codes_list:\n            i = re.escape(i)\n            text = re.sub(f\"{i}\", ' [CODE] ', text, flags=re.DOTALL)\n        return text\n    else:\n        return text      \n    \ndef get_blockquote_html(text):\n    text = str(text)\n    codes_list = re.findall('<blockquote>(.*?)<\/blockquote>',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n    \ndef get_slsldolel(text):\n    text = str(text)\n    codes_list = re.findall('\\\\\\\\\\$(.*?)\\\\\\\\\\$',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n\ndef get_slsldolel_replace(text):\n    text = str(text)\n    codes_list = re.findall('\\\\\\\\\\$.*?\\\\\\\\\\$',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            i = re.escape(i)\n            text = re.sub(f\"{i}\", ' [FORMULA] ', text)\n        return text\n    else:\n        return text\n\ndef get_doldol(text):\n    text = str(text)\n    codes_list = re.findall('\\$\\$(.*?)\\$\\$',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n\ndef get_doldol_replace(text):\n    text = str(text)\n    codes_list = re.findall('\\$\\$.*?\\$\\$',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            i = re.escape(i)\n            text = re.sub(f\"{i}\", ' [FORMULA] ', text)\n        return text\n    else:\n        return text\n\ndef get_spdol(text):\n    text = str(text)\n    codes_list = re.findall(' \\$(.*?) \\$',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n    \ndef get_spdol_replace(text):\n    text = str(text)\n    codes_list = re.findall(' \\$.*? \\$',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            i = re.escape(i)\n            text = re.sub(f\"{i}\", ' [FORMULA] ', text)\n        return text\n    else:\n        return text\n\ndef get_dol(text):\n    text = str(text)\n    codes_list = re.findall('\\$(.*?)\\$',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n    \ndef get_dol_replace(text):\n    text = str(text)\n    codes_list = re.findall('\\$.*?\\$',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            i = re.escape(i)\n            text = re.sub(f\"{i}\", ' [FORMULA] ', text)\n        return text\n    else:\n        return text\n\ndef get_code1(text):\n    text = str(text)\n    codes_list = re.findall(':\\n\\n(.*?)\\n\\n\\n',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' . '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n\ndef get_code1_replace(text):\n    text = str(text)\n    codes_list = re.findall(':\\n\\n.*?\\n\\n\\n',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            i = re.escape(i)\n            text = re.sub(f\"{i}\", ' [CODE] ', text)\n        return text\n    else:\n        return text\n\ndef get_code2(text):\n    text = str(text)\n    codes_list = re.findall('.\\n\\n(.*?)\\n\\n\\n',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' . '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n\ndef get_code2_replace(text):\n    text = str(text)\n    codes_list = re.findall('.\\n\\n.*?\\n\\n\\n',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            if i.count(\"\\n\") > 4:\n                i = re.escape(i)\n                text = re.sub(f\"{i}\", ' [CODE] ', text)\n            else:\n                pass\n        return text\n    else:\n        return text\n    \ndef get_code3(text):\n    text = str(text)\n    codes_list = re.findall('{(.*?)}',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' . '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n\ndef get_code3_replace(text):\n    text = str(text)\n    codes_list = re.findall('{.*?}',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            if len(i) > 10:\n                i = re.escape(i)\n                text = re.sub(f\"{i}\", ' [CODE] ', text)\n            else:\n                pass\n        return text\n    else:\n        return text","e27c7400":"# question_body_code\ntrain_final[\"question_body_code\"] = train_final.apply(lambda x: get_code_html(x[\"Body\"], x[\"question_body\"]), axis=1)\ntrain_final[\"question_body_clean\"] = train_final.apply(lambda x: get_code_replace(x[\"question_body\"], x[\"question_body_code\"]), axis=1)\n\ntest_final[\"question_body_code\"] = test_final.apply(lambda x: get_code_html(x[\"Body\"], x[\"question_body\"]), axis=1)\ntest_final[\"question_body_clean\"] = test_final.apply(lambda x: get_code_replace(x[\"question_body\"], x[\"question_body_code\"]), axis=1)\n\n# question_body_slsldolel\ntrain_final[\"question_body_slsldolel\"] = train_final[\"question_body\"].apply(get_slsldolel)\ntrain_final[\"question_body_clean\"] = train_final[\"question_body_clean\"].apply(get_slsldolel_replace)\n\ntest_final[\"question_body_slsldolel\"] = test_final[\"question_body\"].apply(get_slsldolel)\ntest_final[\"question_body_clean\"] = test_final[\"question_body_clean\"].apply(get_slsldolel_replace)\n\n# question_body_doldol\ntrain_final[\"question_body_doldol\"] = train_final[\"question_body\"].apply(get_doldol)\ntrain_final[\"question_body_clean\"] = train_final[\"question_body_clean\"].apply(get_doldol_replace)\n\ntest_final[\"question_body_doldol\"] = test_final[\"question_body\"].apply(get_doldol)\ntest_final[\"question_body_clean\"] = test_final[\"question_body_clean\"].apply(get_doldol_replace)\n\n# question_body_spdol\ntrain_final[\"question_body_spdol\"] = train_final[\"question_body\"].apply(get_spdol)\ntrain_final[\"question_body_clean\"] = train_final[\"question_body_clean\"].apply(get_spdol_replace)\n\ntest_final[\"question_body_spdol\"] = test_final[\"question_body\"].apply(get_spdol)\ntest_final[\"question_body_clean\"] = test_final[\"question_body_clean\"].apply(get_spdol_replace)\n\n# question_body_dol\ntrain_final[\"question_body_dol\"] = train_final[\"question_body\"].apply(get_dol)\ntrain_final[\"question_body_clean\"] = train_final[\"question_body_clean\"].apply(get_dol_replace)\n\ntest_final[\"question_body_dol\"] = test_final[\"question_body\"].apply(get_dol)\ntest_final[\"question_body_clean\"] = test_final[\"question_body_clean\"].apply(get_dol_replace)\n\ntrain_final[\"question_body_all\"] = list(map(lambda a,b,c,d,e : str(a) + ' ' + str(b) + ' ' + str(c) +' '+ str(d) +' '+ str(e),train_final[\"question_body_code\"],train_final[\"question_body_slsldolel\"],train_final[\"question_body_doldol\"],train_final[\"question_body_spdol\"], train_final[\"question_body_dol\"]))\ntest_final[\"question_body_all\"] = list(map(lambda a,b,c,d,e : str(a) + ' ' + str(b) + ' ' + str(c) +' '+ str(d) +' '+ str(e),test_final[\"question_body_code\"],test_final[\"question_body_slsldolel\"],test_final[\"question_body_doldol\"],test_final[\"question_body_spdol\"], test_final[\"question_body_dol\"]))","082c6590":"def get_code1(text):\n    text = str(text)\n    codes_list = re.findall(':\\n\\n(.*?)\\n\\n\\n',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' . '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n\ndef get_code1_replace(text):\n    text = str(text)\n    codes_list = re.findall(':\\n\\n.*?\\n\\n\\n',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            i = re.escape(i)\n            text = re.sub(f\"{i}\", ' [CODE] ', text)\n        return text\n    else:\n        return text\n\ndef get_code2(text):\n    text = str(text)\n    codes_list = re.findall('.\\n\\n(.*?)\\n\\n\\n',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' . '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n\ndef get_code2_replace(text):\n    text = str(text)\n    codes_list = re.findall('.\\n\\n.*?\\n\\n\\n',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            if i.count(\"\\n\") > 4:\n                i = re.escape(i)\n                text = re.sub(f\"{i}\", ' [CODE] ', text)\n            else:\n                pass\n        return text\n    else:\n        return text\n    \ndef get_code3(text):\n    text = str(text)\n    codes_list = re.findall('{(.*?)}',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        code = ' . '.join(map(str, codes_list))\n        return code\n    else:\n        return \"NONE\"\n\ndef get_code3_replace(text):\n    text = str(text)\n    codes_list = re.findall('{.*?}',text, flags=re.DOTALL)\n    if len(codes_list) > 0:\n        #code = ' '.join(map(str, codes_list))\n        for i in codes_list:\n            if len(i) > 10:\n                i = re.escape(i)\n                text = re.sub(f\"{i}\", ' [CODE] ', text)\n            else:\n                pass\n        return text\n    else:\n        return text","6058e108":"# answer_code1\n\ntrain_final[\"answer_code1\"] = train_final[\"answer\"].apply(get_code1)\ntrain_final[\"answer_clean\"] = train_final[\"answer\"].apply(get_code1_replace)\n\ntest_final[\"answer_code1\"] = test_final[\"answer\"].apply(get_code1)\ntest_final[\"answer_clean\"] = test_final[\"answer\"].apply(get_code1_replace)\n\n# answer_code2\n\ntrain_final[\"answer_code2\"] = train_final[\"answer\"].apply(get_code2)\ntrain_final[\"answer_clean\"] = train_final[\"answer_clean\"].apply(get_code2_replace)\n\ntest_final[\"answer_code2\"] = test_final[\"answer\"].apply(get_code2)\ntest_final[\"answer_clean\"] = test_final[\"answer_clean\"].apply(get_code2_replace)\n\n# answer_code3\n\ntrain_final[\"answer_code3\"] = train_final[\"answer\"].apply(get_code3)\ntrain_final[\"answer_clean\"] = train_final[\"answer_clean\"].apply(get_code3_replace)\n\ntest_final[\"answer_code3\"] = test_final[\"answer\"].apply(get_code3)\ntest_final[\"answer_clean\"] = test_final[\"answer_clean\"].apply(get_code3_replace)\n\ntrain_final[\"answer_code\"] = list(map(lambda a,b,c : str(a) + ' ' + str(b) + ' ' + str(c),train_final[\"answer_code1\"],train_final[\"answer_code2\"],train_final[\"answer_code3\"]))\ntest_final[\"answer_code\"] = list(map(lambda a,b,c : str(a) + ' ' + str(b) + ' ' + str(c),test_final[\"answer_code1\"],test_final[\"answer_code2\"],test_final[\"answer_code3\"]))\n\n# question_body_slsldolel\ntrain_final[\"answer_slsldolel\"] = train_final[\"answer\"].apply(get_slsldolel)\ntrain_final[\"answer_clean\"] = train_final[\"answer_clean\"].apply(get_slsldolel_replace)\n\ntest_final[\"answer_slsldolel\"] = test_final[\"answer\"].apply(get_slsldolel)\ntest_final[\"answer_clean\"] = test_final[\"answer_clean\"].apply(get_slsldolel_replace)\n\n# question_body_doldol\ntrain_final[\"answer_doldol\"] = train_final[\"answer\"].apply(get_doldol)\ntrain_final[\"answer_clean\"] = train_final[\"answer_clean\"].apply(get_doldol_replace)\n\ntest_final[\"answer_doldol\"] = test_final[\"answer\"].apply(get_doldol)\ntest_final[\"answer_clean\"] = test_final[\"answer_clean\"].apply(get_doldol_replace)\n\n# question_body_spdol\ntrain_final[\"answer_spdol\"] = train_final[\"answer\"].apply(get_spdol)\ntrain_final[\"answer_clean\"] = train_final[\"answer_clean\"].apply(get_spdol_replace)\n\ntest_final[\"answer_spdol\"] = test_final[\"answer\"].apply(get_spdol)\ntest_final[\"answer_clean\"] = test_final[\"answer_clean\"].apply(get_spdol_replace)\n\n# question_body_dol\ntrain_final[\"answer_dol\"] = train_final[\"answer\"].apply(get_dol)\ntrain_final[\"answer_clean\"] = train_final[\"answer_clean\"].apply(get_dol_replace)\n\ntest_final[\"answer_dol\"] = test_final[\"answer\"].apply(get_dol)\ntest_final[\"answer_clean\"] = test_final[\"answer_clean\"].apply(get_dol_replace)\n\ntrain_final[\"answer_all\"] = list(map(lambda a,b,c,d,e : str(a) + ' ' + str(b) + ' ' + str(c) +' '+ str(d)+' '+ str(e),train_final[\"answer_code\"],train_final[\"answer_slsldolel\"],train_final[\"answer_doldol\"],train_final[\"answer_spdol\"],train_final[\"answer_dol\"]))\ntest_final[\"answer_all\"] = list(map(lambda a,b,c,d,e : str(a) + ' ' + str(b) + ' ' + str(c) +' '+ str(d)+' '+ str(e),test_final[\"answer_code\"],test_final[\"answer_slsldolel\"],test_final[\"answer_doldol\"],test_final[\"answer_spdol\"], test_final[\"answer_dol\"]))","735529e2":"droped_columns = []\ndroped_columns.extend(user_drop_cols)\ndroped_columns.extend(posts_drop_cols)\nprint(len(droped_columns))\n\ntrain_final.drop(columns=droped_columns, inplace=True)\ntest_final.drop(columns=droped_columns, inplace=True)","8bbebcb2":"train_final.shape, test_final.shape, train_final_targets.shape","2ab983f8":"def url_replace(text):\n    text = re.sub(\"(?:(?:https?|ftp):\\\/\\\/)?[\\w\/\\-?=%.]+\\.[\\w\/\\-?=%.]+\", '[URL]' ,text, flags=re.DOTALL)\n    return text\n\ndef url_count(text):\n    count = len(re.findall('(?:(?:https?|ftp):\\\/\\\/)?[\\w\/\\-?=%.]+\\.[\\w\/\\-?=%.]+',text))\n    return count","788f7e2e":"CUSTOM_TABLE = str.maketrans(\n    {\n        \"\\xad\": None,\n        \"\\x7f\": None,\n        \"\\ufeff\": None,\n        \"\\u200b\": None,\n        \"\\u200e\": None,\n        \"\\u202a\": None,\n        \"\\u202c\": None,\n        \"\u2018\": \"'\",\n        \"\u2019\": \"'\",\n        \"`\": \"'\",\n        \"\u201c\": '\"',\n        \"\u201d\": '\"',\n        \"\u00ab\": '\"',\n        \"\u00bb\": '\"',\n        \"\u0262\": \"G\",\n        \"\u026a\": \"I\",\n        \"\u0274\": \"N\",\n        \"\u0280\": \"R\",\n        \"\u028f\": \"Y\",\n        \"\u0299\": \"B\",\n        \"\u029c\": \"H\",\n        \"\u029f\": \"L\",\n        \"\u0493\": \"F\",\n        \"\u1d00\": \"A\",\n        \"\u1d04\": \"C\",\n        \"\u1d05\": \"D\",\n        \"\u1d07\": \"E\",\n        \"\u1d0a\": \"J\",\n        \"\u1d0b\": \"K\",\n        \"\u1d0d\": \"M\",\n        \"\u039c\": \"M\",\n        \"\u1d0f\": \"O\",\n        \"\u1d18\": \"P\",\n        \"\u1d1b\": \"T\",\n        \"\u1d1c\": \"U\",\n        \"\u1d21\": \"W\",\n        \"\u1d20\": \"V\",\n        \"\u0138\": \"K\",\n        \"\u0432\": \"B\",\n        \"\u043c\": \"M\",\n        \"\u043d\": \"H\",\n        \"\u0442\": \"T\",\n        \"\u0455\": \"S\",\n        \"\u2014\": \"-\",\n        \"\u2013\": \"-\",\n    }\n)\n\nWORDS_REPLACER = [\n    (\"sh*t\", \"shit\"),\n    (\"s**t\", \"shit\"),\n    (\"f*ck\", \"fuck\"),\n    (\"fu*k\", \"fuck\"),\n    (\"f**k\", \"fuck\"),\n    (\"f*****g\", \"fucking\"),\n    (\"f***ing\", \"fucking\"),\n    (\"f**king\", \"fucking\"),\n    (\"p*ssy\", \"pussy\"),\n    (\"p***y\", \"pussy\"),\n    (\"pu**y\", \"pussy\"),\n    (\"p*ss\", \"piss\"),\n    (\"b*tch\", \"bitch\"),\n    (\"bit*h\", \"bitch\"),\n    (\"h*ll\", \"hell\"),\n    (\"h**l\", \"hell\"),\n    (\"cr*p\", \"crap\"),\n    (\"d*mn\", \"damn\"),\n    (\"stu*pid\", \"stupid\"),\n    (\"st*pid\", \"stupid\"),\n    (\"n*gger\", \"nigger\"),\n    (\"n***ga\", \"nigger\"),\n    (\"f*ggot\", \"faggot\"),\n    (\"scr*w\", \"screw\"),\n    (\"pr*ck\", \"prick\"),\n    (\"g*d\", \"god\"),\n    (\"s*x\", \"sex\"),\n    (\"a*s\", \"ass\"),\n    (\"a**hole\", \"asshole\"),\n    (\"a***ole\", \"asshole\"),\n    (\"a**\", \"ass\"),\n]\n\nWORDS_REPLACER_2 = [\n    (\"ain't\", 'is not'),\n     (\"aren't\", 'are not'),\n     (\"can't\", 'cannot'),\n     (\"'cause\", 'because'),\n     (\"could've\", 'could have'),\n     (\"couldn't\", 'could not'),\n     (\"didn't\", 'did not'),\n     (\"doesn't\", 'does not'),\n     (\"don't\", 'do not'),\n     (\"hadn't\", 'had not'),\n     (\"hasn't\", 'has not'),\n     (\"haven't\", 'have not'),\n     (\"he'd\", 'he would'),\n     (\"he'll\", 'he will'),\n     (\"he's\", 'he is'),\n     (\"how'd\", 'how did'),\n     (\"how'd'y\", 'how do you'),\n     (\"how'll\", 'how will'),\n     (\"how's\", 'how is'),\n     (\"i'd\", 'i would'),\n     (\"i'd've\", 'i would have'),\n     (\"i'll\", 'i will'),\n     (\"i'll've\", 'i will have'),\n     (\"i'm\", 'i am'),\n     (\"i've\", 'i have'),\n     (\"i'd\", 'i would'),\n     (\"i'd've\", 'i would have'),\n     (\"i'll\", 'i will'),\n     (\"i'll've\", 'i will have'),\n     (\"i'm\", 'i am'),\n     (\"i've\", 'i have'),\n     (\"isn't\", 'is not'),\n     (\"it'd\", 'it would'),\n     (\"it'd've\", 'it would have'),\n     (\"it'll\", 'it will'),\n     (\"it'll've\", 'it will have'),\n     (\"it's\", 'it is'),\n     (\"let's\", 'let us'),\n     (\"ma'am\", 'madam'),\n     (\"mayn't\", 'may not'),\n     (\"might've\", 'might have'),\n     (\"mightn't\", 'might not'),\n     (\"mightn't've\", 'might not have'),\n     (\"must've\", 'must have'),\n     (\"mustn't\", 'must not'),\n     (\"mustn't've\", 'must not have'),\n     (\"needn't\", 'need not'),\n     (\"needn't've\", 'need not have'),\n     (\"o'clock\", 'of the clock'),\n     (\"oughtn't\", 'ought not'),\n     (\"oughtn't've\", 'ought not have'),\n     (\"shan't\", 'shall not'),\n     (\"sha'n't\", 'shall not'),\n     (\"shan't've\", 'shall not have'),\n     (\"she'd\", 'she would'),\n     (\"she'd've\", 'she would have'),\n     (\"she'll\", 'she will'),\n     (\"she'll've\", 'she will have'),\n     (\"she's\", 'she is'),\n     (\"should've\", 'should have'),\n     (\"shouldn't\", 'should not'),\n     (\"shouldn't've\", 'should not have'),\n     (\"so've\", 'so have'),\n     (\"so's\", 'so as'),\n     (\"this's\", 'this is'),\n     (\"that'd\", 'that would'),\n     (\"that'd've\", 'that would have'),\n     (\"that's\", 'that is'),\n     (\"there'd\", 'there would'),\n     (\"there'd've\", 'there would have'),\n     (\"there's\", 'there is'),\n     (\"here's\", 'here is'),\n     (\"they'd\", 'they would'),\n     (\"they'd've\", 'they would have'),\n     (\"they'll\", 'they will'),\n     (\"they'll've\", 'they will have'),\n     (\"they're\", 'they are'),\n     (\"they've\", 'they have'),\n     (\"to've\", 'to have'),\n     (\"wasn't\", 'was not'),\n     (\"we'd\", 'we would'),\n     (\"we'd've\", 'we would have'),\n     (\"we'll\", 'we will'),\n     (\"we'll've\", 'we will have'),\n     (\"we're\", 'we are'),\n     (\"we've\", 'we have'),\n     (\"weren't\", 'were not'),\n     (\"what'll\", 'what will'),\n     (\"what'll've\", 'what will have'),\n     (\"what're\", 'what are'),\n     (\"what's\", 'what is'),\n     (\"what've\", 'what have'),\n     (\"when's\", 'when is'),\n     (\"when've\", 'when have'),\n     (\"where'd\", 'where did'),\n     (\"where's\", 'where is'),\n     (\"where've\", 'where have'),\n     (\"who'll\", 'who will'),\n     (\"who'll've\", 'who will have'),\n     (\"who's\", 'who is'),\n     (\"who've\", 'who have'),\n     (\"why's\", 'why is'),\n     (\"why've\", 'why have'),\n     (\"will've\", 'will have'),\n     (\"won't\", 'will not'),\n     (\"won't've\", 'will not have'),\n     (\"would've\", 'would have'),\n     (\"wouldn't\", 'would not'),\n     (\"wouldn't've\", 'would not have'),\n     (\"y'all\", 'you all'),\n     (\"y'all'd\", 'you all would'),\n     (\"y'all'd've\", 'you all would have'),\n     (\"y'all're\", 'you all are'),\n     (\"y'all've\", 'you all have'),\n     (\"you'd\", 'you would'),\n     (\"you'd've\", 'you would have'),\n     (\"you'll\", 'you will'),\n     (\"you'll've\", 'you will have'),\n     (\"you're\", 'you are'),\n     (\"you've\", 'you have'),\n     ('what\u201ds', 'what is'),\n     ('what\"s', 'what is'),\n     ('its', 'it is'),\n     (\"what's\", 'what is'),\n     (\"'ll\", 'will'),\n     (\"n't\", 'not'),\n     (\"'re\", 'are'),\n     (\"ain't\", 'is not'),\n     (\"aren't\", 'are not'),\n     (\"can't\", 'cannot'),\n     (\"'cause\", 'because'),\n     (\"could've\", 'could have'),\n     (\"couldn't\", 'could not'),\n     (\"didn't\", 'did not'),\n     (\"doesn't\", 'does not'),\n     (\"don't\", 'do not'),\n     (\"hadn't\", 'had not'),\n     (\"hasn't\", 'has not'),\n     (\"haven't\", 'have not'),\n     (\"he'd\", 'he would'),\n     (\"he'll\", 'he will'),\n     (\"he's\", 'he is'),\n     (\"how'd\", 'how did'),\n     (\"how'd'y\", 'how do you'),\n     (\"how'll\", 'how will'),\n     (\"how's\", 'how is'),\n     (\"i'd\", 'i would'),\n     (\"i'd've\", 'i would have'),\n     (\"i'll\", 'i will'),\n     (\"i'll've\", 'i will have'),\n     (\"i'm\", 'i am'),\n     (\"i've\", 'i have'),\n     (\"i'd\", 'i would'),\n     (\"i'd've\", 'i would have'),\n     (\"i'll\", 'i will'),\n     (\"i'll've\", 'i will have'),\n     (\"i'm\", 'i am'),\n     (\"i've\", 'i have'),\n     (\"isn't\", 'is not'),\n     (\"it'd\", 'it would'),\n     (\"it'd've\", 'it would have'),\n     (\"it'll\", 'it will'),\n     (\"it'll've\", 'it will have'),\n     (\"it's\", 'it is'),\n     (\"let's\", 'let us'),\n     (\"ma'am\", 'madam'),\n     (\"mayn't\", 'may not'),\n     (\"might've\", 'might have'),\n     (\"mightn't\", 'might not'),\n     (\"mightn't've\", 'might not have'),\n     (\"must've\", 'must have'),\n     (\"mustn't\", 'must not'),\n     (\"mustn't've\", 'must not have'),\n     (\"needn't\", 'need not'),\n     (\"needn't've\", 'need not have'),\n     (\"o'clock\", 'of the clock'),\n     (\"oughtn't\", 'ought not'),\n     (\"oughtn't've\", 'ought not have'),\n     (\"shan't\", 'shall not'),\n     (\"sha'n't\", 'shall not'),\n     (\"shan't've\", 'shall not have'),\n     (\"she'd\", 'she would'),\n     (\"she'd've\", 'she would have'),\n     (\"she'll\", 'she will'),\n     (\"she'll've\", 'she will have'),\n     (\"she's\", 'she is'),\n     (\"should've\", 'should have'),\n     (\"shouldn't\", 'should not'),\n     (\"shouldn't've\", 'should not have'),\n     (\"so've\", 'so have'),\n     (\"so's\", 'so as'),\n     (\"this's\", 'this is'),\n     (\"that'd\", 'that would'),\n     (\"that'd've\", 'that would have'),\n     (\"that's\", 'that is'),\n     (\"there'd\", 'there would'),\n     (\"there'd've\", 'there would have'),\n     (\"there's\", 'there is'),\n     (\"here's\", 'here is'),\n     (\"they'd\", 'they would'),\n     (\"they'd've\", 'they would have'),\n     (\"'they're\", 'they are'),\n     (\"they'll\", 'they will'),\n     (\"they'll've\", 'they will have'),\n     (\"they're\", 'they are'),\n     (\"they've\", 'they have'),\n     (\"to've\", 'to have'),\n     (\"wasn't\", 'was not'),\n     (\"we'd\", 'we would'),\n     (\"we'd've\", 'we would have'),\n     (\"we'll\", 'we will'),\n     (\"we'll've\", 'we will have'),\n     (\"we're\", 'we are'),\n     (\"we've\", 'we have'),\n     (\"weren't\", 'were not'),\n     (\"what'll\", 'what will'),\n     (\"what'll've\", 'what will have'),\n     (\"what're\", 'what are'),\n     (\"what's\", 'what is'),\n     (\"what've\", 'what have'),\n     (\"when's\", 'when is'),\n     (\"when've\", 'when have'),\n     (\"where'd\", 'where did'),\n     (\"where's\", 'where is'),\n     (\"where've\", 'where have'),\n     (\"who'll\", 'who will'),\n     (\"who'll've\", 'who will have'),\n     (\"who's\", 'who is'),\n     (\"who've\", 'who have'),\n     (\"why's\", 'why is'),\n     (\"why've\", 'why have'),\n     (\"will've\", 'will have'),\n     (\"won't\", 'will not'),\n     (\"won't've\", 'will not have'),\n     (\"would've\", 'would have'),\n     (\"wouldn't\", 'would not'),\n     (\"wouldn't've\", 'would not have'),\n     (\"y'all\", 'you all'),\n     (\"y'all'd\", 'you all would'),\n     (\"y'all'd've\", 'you all would have'),\n     (\"y'all're\", 'you all are'),\n     (\"y'all've\", 'you all have'),\n     (\"you'd\", 'you would'),\n     (\"you'd've\", 'you would have'),\n     (\"you'll\", 'you will'),\n     (\"you'll've\", 'you will have'),\n     (\"you're\", 'you are'),\n     (\"you've\", 'you have')\n    ]\n\nmispell_dict = {\"aren't\" : \"are not\",\n                \"can't\" : \"cannot\",\n                \"couldn't\" : \"could not\",\n                \"couldnt\" : \"could not\",\n                \"didn't\" : \"did not\",\n                \"doesn't\" : \"does not\",\n                \"doesnt\" : \"does not\",\n                \"don't\" : \"do not\",\n                \"hadn't\" : \"had not\",\n                \"hasn't\" : \"has not\",\n                \"haven't\" : \"have not\",\n                \"havent\" : \"have not\",\n                \"he'd\" : \"he would\",\n                \"he'll\" : \"he will\",\n                \"he's\" : \"he is\",\n                \"i'd\" : \"I would\",\n                \"i'd\" : \"I had\",\n                \"i'll\" : \"I will\",\n                \"i'm\" : \"I am\",\n                \"isn't\" : \"is not\",\n                \"it's\" : \"it is\",\n                \"it'll\":\"it will\",\n                \"i've\" : \"I have\",\n                \"let's\" : \"let us\",\n                \"mightn't\" : \"might not\",\n                \"mustn't\" : \"must not\",\n                \"shan't\" : \"shall not\",\n                \"she'd\" : \"she would\",\n                \"she'll\" : \"she will\",\n                \"she's\" : \"she is\",\n                \"shouldn't\" : \"should not\",\n                \"shouldnt\" : \"should not\",\n                \"that's\" : \"that is\",\n                \"thats\" : \"that is\",\n                \"there's\" : \"there is\",\n                \"theres\" : \"there is\",\n                \"they'd\" : \"they would\",\n                \"they'll\" : \"they will\",\n                \"they're\" : \"they are\",\n                \"theyre\":  \"they are\",\n                \"they've\" : \"they have\",\n                \"we'd\" : \"we would\",\n                \"we're\" : \"we are\",\n                \"weren't\" : \"were not\",\n                \"we've\" : \"we have\",\n                \"what'll\" : \"what will\",\n                \"what're\" : \"what are\",\n                \"what's\" : \"what is\",\n                \"what've\" : \"what have\",\n                \"where's\" : \"where is\",\n                \"who'd\" : \"who would\",\n                \"who'll\" : \"who will\",\n                \"who're\" : \"who are\",\n                \"who's\" : \"who is\",\n                \"who've\" : \"who have\",\n                \"won't\" : \"will not\",\n                \"wouldn't\" : \"would not\",\n                \"you'd\" : \"you would\",\n                \"you'll\" : \"you will\",\n                \"you're\" : \"you are\",\n                \"you've\" : \"you have\",\n                \"'re\": \" are\",\n                \"wasn't\": \"was not\",\n                \"we'll\":\" will\",\n                \"didn't\": \"did not\",\n                \"tryin'\":\"trying\",\n                \"\u2018\": \"'\", \n                \"\u20b9\": \"e\", \n                \"\u00b4\": \"'\",\n                \"\u00b0\": \"\", \n                \"\u20ac\": \"e\", \n                \"\u2122\": \"tm\", \n                \"\u221a\": \" sqrt \",\n                \"\u00d7\": \"x\", \n                \"\u00b2\": \"2\", \n                \"\u2014\": \"-\",\n                \"\u2013\": \"-\",\n                \"\u2019\": \"'\",\n                \"_\": \"-\",\n                \"`\": \"'\", \n                '\u201c': '\"',\n                '\u201d': '\"',\n                '\u201c': '\"',\n                \"\u00a3\": \"e\", \n                '\u221e': 'infinity',\n                '\u03b8': 'theta',\n                '\u00f7': '\/',\n                '\u03b1': 'alpha', \n                '\u2022': '.',\n                '\u00e0': 'a', \n                '\u2212': '-', \n                '\u03b2': 'beta', \n                '\u2205': '', \n                '\u00b3': '3', \n                '\u03c0': 'pi',\n                '\\u200b': ' ', \n                '\u2026': ' ... ', \n                '\\ufeff': '', \n                '\u0915\u0930\u0928\u093e': '', \n                '\u0939\u0948': '',  \n                \n               }\n\n\nWORDS_REPLACER_3 = [(k, v) for k, v in mispell_dict.items()]\n\nWORDS_REPLACER_4 = [('automattic', 'automatic'),\n         ('sweetpotato', 'sweet potato'),\n         ('statuscode', 'status code'),\n         ('applylayer', 'apply layer'),\n         ('aligator', 'alligator'),\n         ('downloands', 'download'),\n         ('dowloand', 'download'),\n         ('thougths', 'thoughts'),\n         ('helecopter', 'helicopter'),\n         ('telugul', 'telugu'),\n         ('unconditionaly', 'unconditionally'),\n         ('coompanies', 'companies'),\n         ('lndigenous', 'indigenous'),\n         ('evluate', 'evaluate'),\n         ('suggstion', 'suggestion'),\n         ('thinkning', 'thinking'),\n         ('concatinate', 'concatenate'),\n         ('constitutionals', 'constitutional'),\n         ('moneyback', 'money back'),\n         ('civilazation', 'civilization'),\n         ('paranoria', 'paranoia'),\n         ('rightside', 'right side'),\n         ('methamatics', 'mathematics'),\n         ('natual', 'natural'),\n         ('brodcast', 'broadcast'),\n         ('pleasesuggest', 'please suggest'),\n         ('intitution', 'institution'),\n         ('experinces', 'experiences'),\n         ('reallyreally', 'really'),\n         ('testostreone', 'testosterone'),\n         ('musceles', 'muscle'),\n         ('bacause', 'because'),\n         ('peradox', 'paradox'),\n         ('probabity', 'probability'),\n         ('collges', 'college'),\n         ('diciplined', 'disciplined'),\n         ('completeted', 'completed'),\n         ('lunchshould', 'lunch should'),\n         ('battlenet', 'battle net'),\n         ('dissapoint', 'disappoint'),\n         ('resultsnew', 'results new'),\n         ('indcidents', 'incidents'),\n         ('figuire', 'figure'),\n         ('protonneutron', 'proton neutron'),\n         ('tecnical', 'technical'),\n         ('patern', 'pattern'),\n         ('unenroll', 'un enroll'),\n         ('proceedures', 'procedures'),\n         ('srategy', 'strategy'),\n         ('mordern', 'modern'),\n         ('prepartion', 'preparation'),\n         ('throuhout', 'throught'),\n         ('academey', 'academic'),\n         ('instituitions', 'institutions'),\n         ('abadon', 'abandon'),\n         ('compitetive', 'competitive'),\n         ('hypercondriac', 'hypochondriac'),\n         ('spiliting', 'splitting'),\n         ('physchic', 'psychic'),\n         ('flippingly', 'flipping'),\n         ('likelyhood', 'likelihood'),\n         ('armsindustry', 'arms industry'),\n         (' turorials', 'tutorials'),\n         ('photostats', 'photostat'),\n         ('sunconcious', 'subconscious'),\n         ('chemistryphysics', 'chemistry physics'),\n         ('secondlife', 'second life'),\n         ('histrorical', 'historical'),\n         ('disordes', 'disorders'),\n         ('differenturl', 'differential'),\n         ('councilling', ' counselling'),\n         ('sugarmill', 'sugar mill'),\n         ('relatiosnhip', 'relationship'),\n         ('fanpages', 'fan pages'),\n         ('agregator', 'aggregator'),\n         ('switc', 'switch'),\n         ('smatphones', 'smartphones'),\n         ('headsize', 'head size'),\n         ('pendrives', 'pen drives'),\n         ('biotecnology', 'biotechnology'),\n         ('borderlink', 'border link'),\n         ('furnance', 'furnace'),\n         ('competetion', 'competition'),\n         ('distibution', 'distribution'),\n         ('ananlysis', ' analysis'),\n         ('textile\uff1f', 'textile'),\n         ('howww', 'how'),\n         ('strategybusiness', 'strategy business'),\n         ('spectrun', 'spectrum'),\n         ('propasal', 'proposal'),\n         ('appilcable', 'applicable'),\n         ('accountwhat', ' account what'),\n         ('algorithems', ' algorithms'),\n         ('protuguese', ' Portuguese'),\n         ('exatly', 'exactly'),\n         ('disturbence', 'disturbance'),\n         ('govrnment', 'government'),\n         ('requiremnt', 'requirement'),\n         ('vargin', 'virgin'),\n         ('lonleley', 'lonely'),\n         ('unmateralistic', 'materialistic'),\n         ('dveloper', 'developer'),\n         ('dcuments', 'documents'),\n         ('techonologies', 'technologies'),\n         ('morining', 'morning'),\n         ('samsing', 'Samsung'),\n         ('engeeniring', 'engineering'),\n         ('racetrac', 'racetrack'),\n         ('physian', 'physician'),\n         ('theretell', 'there tell'),\n         ('tryto', 'try to'),\n         ('teamfight', 'team fight'),\n         ('recomend', 'recommend'),\n         ('spectables', 'spectacles'),\n         ('emtional', 'emotional'),\n         ('engeenerring', 'engineering'),\n         ('optionsgood', 'options good'),\n         ('primarykey', 'primary key'),\n         ('foreignkey', 'foreign key'),\n         ('concieved', 'conceived'),\n         ('leastexpensive', 'least expensive'),\n         ('foodtech', 'food tech'),\n         ('electronegetivity', 'electronegativity'),\n         ('polticians', 'politicians'),\n         ('distruptive', 'disruptive'),\n         ('currrent', 'current'),\n         ('hidraulogy', 'hydrology'),\n         ('californa', 'California'),\n         ('electrrical', 'electrical'),\n         ('navigationally', 'navigation'),\n         ('whwhat', 'what'),\n         ('bcos', 'because'),\n         ('vaccancies', 'vacancies'),\n         ('articels', 'articles'),\n         ('boilng', 'boiling'),\n         ('hyperintensity', 'hyper intensity'),\n         ('rascism', 'racism'),\n         ('messenging', 'messaging'),\n         ('cleaniness', 'cleanliness'),\n         ('vetenary', 'veterinary'),\n         ('investorswhat', 'investors what'),\n         ('chrestianity', 'Christianity'),\n         ('apporval', 'approval'),\n         ('repaire', 'repair'),\n         ('biggerchance', 'bigger chance'),\n         ('manufacturering', 'manufacturing'),\n         ('buildertrend', 'builder trend'),\n         ('allocatively', 'allocative'),\n         ('subliminals', 'subliminal'),\n         ('mechnically', 'mechanically'),\n         ('binaurial', 'binaural'),\n         ('naaked', 'naked'),\n         ('aantidepressant', 'antidepressant'),\n         ('geunine', 'genuine'),\n         ('quantitaive', 'quantitative'),\n         ('paticipated', 'participated'),\n         ('repliedjesus', 'replied Jesus'),\n         ('baised', 'biased'),\n         ('worldreport', 'world report'),\n         ('eecutives', 'executives'),\n         ('paitents', 'patients'),\n         ('telgu', 'Telugu'),\n         ('nomeniculature', 'nomenclature'),\n         ('crimimaly', 'criminally'),\n         ('resourse', 'resource'),\n         ('procurenent', 'procurement'),\n         ('improvemet', 'improvement'),\n         ('metamers', 'metamer'),\n         ('tautomers', 'tautomer'),\n         ('knowwhen', 'know when'),\n         ('whatdoes', 'what does'),\n         ('pletelets', 'platelets'),\n         ('pssesive', 'possessive'),\n         ('oxigen', 'oxygen'),\n         ('ethniticy', 'ethnicity'),\n         ('situatiation', 'situation'),\n         ('ecoplanet', 'eco planet'),\n         ('situatio', 'situation'),\n         ('dateing', 'dating'),\n         ('hostress', 'hostess'),\n         ('initialisation', 'initialization'),\n         ('hydrabd', 'Hyderabad'),\n         ('deppresed', 'depressed'),\n         ('dwnloadng', 'downloading'),\n         ('expirey', 'expiry'),\n         ('engeenering', 'engineering'),\n         ('hyderebad', 'Hyderabad'),\n         ('automatabl', 'automatable'),\n         ('architetureocasions', 'architectureoccasions'),\n         ('restaraunts', 'restaurants'),\n         ('recommedations', 'recommendations'),\n         ('intergrity', 'integrity'),\n         ('reletively', 'relatively'),\n         ('priceworthy', 'price worthy'),\n         ('princples', 'principles'),\n         ('reconigze', 'recognize'),\n         ('paticular', 'particular'),\n         ('musictheory', 'music theory'),\n         ('requied', 'required'),\n         ('netural', 'natural'),\n         ('fluoresent', 'fluorescent'),\n         ('girlfiend', 'girlfriend'),\n         ('develpment', 'development'),\n         ('eridicate', 'eradicate'),\n         ('techologys', 'technologies'),\n         ('hybridyzation', 'hybridization'),\n         ('ideaa', 'ideas'),\n         ('tchnology', 'technology'),\n         ('appropiate', 'appropriate'),\n         ('respone', 'response'),\n         ('celebreties', 'celebrities'),\n         ('exterion', 'exterior'),\n         ('uservoice', 'user voice'),\n         ('effeciently', 'efficiently'),\n         ('torquise', 'turquoise '),\n         ('governmentand', 'government and'),\n         ('eletricity', 'electricity'),\n         ('coulums', 'columns'),\n         ('nolonger', 'no longer'),\n         ('wheras', 'whereas'),\n         ('infnite', 'infinite'),\n         ('decolourised', 'no color'),\n         ('onepiece', 'one piece'),\n         ('assignements', 'assignments'),\n         ('celebarted', 'celebrated'),\n         ('pharmacistical', 'pharmaceutical'),\n         ('jainsingle', 'Jain single'),\n         ('asssistance', 'assistance'),\n         ('glases', 'glasses'),\n         ('polymorpism', 'polymorphism'),\n         ('amerians', 'Americans'),\n         ('masquitos', 'mosquitoes'),\n         ('interseted', 'interested'),\n         ('thehighest', 'the highest'),\n         ('etnicity', 'ethnicity'),\n         ('anopportunity', 'anopportunity'),\n         ('multidiscipline', 'multi discipline'),\n         ('smartchange', 'smart change'),\n         ('collegefest', 'college fest'),\n         ('disdvantages', 'disadvantages'),\n         ('successfcators', 'success factors'),\n         ('sustitute', 'substitute'),\n         ('caoching', 'coaching'),\n         ('bullyed', 'bullied'),\n         ('comunicate', 'communicate'),\n         ('prisioner', 'prisoner'),\n         ('tamilnaadu', 'Tamil Nadu'),\n         ('methodologyies', 'methodologies'),\n         ('tranfers', 'transfers'),\n         ('truenorth', 'true north'),\n         ('backdonation', 'back donation'),\n         ('oreals', 'ordeals'),\n         ('browsec', 'browser'),\n         ('solarwinds', 'solar winds'),\n         ('susten', 'sustain'),\n         ('carnegi', 'Carnegie'),\n         ('doesent', \"doesn't\"),\n         ('automtotive', 'automotive'),\n         ('nimuselide', 'nimesulide'),\n         ('subsciption', 'subscription'),\n         ('quatrone', 'Quattrone'),\n         ('qatalyst', 'catalyst'),\n         ('vardamana', 'Vardaman'),\n         ('suplements', 'supplements'),\n         ('repore', 'report'),\n         ('pikettys', 'Piketty'),\n         ('paramilltary', 'paramilitary'),\n         ('aboutlastnight', 'about last night'),\n         ('vidyapeth', 'Vidyapeeth'),\n         ('extraterrestial', 'extraterrestrial'),\n         ('powerloom', 'power loom'),\n         ('zonbie', 'zombie'),\n         ('cococola', 'Coca Cola'),\n         ('hameorrhage', 'hemorrhage'),\n         ('abhayanand', 'Abhay Anand'),\n         ('romedynow', 'remedy now'),\n         ('couster', 'counter'),\n         ('encouaged', 'encouraged'),\n         ('toprepare', 'to prepare'),\n         ('eveteasing', 'eve teasing'),\n         ('roulete', 'roulette'),\n         ('sorkar', 'Sarkar'),\n         ('waveboard', 'wave board'),\n         ('acclerate', 'accelerate'),\n         ('togrow', 'to grow'),\n         ('felatio', 'fellatio'),\n         ('baherain', 'Bahrain'),\n         ('teatment', 'treatment'),\n         ('iwitness', 'eye witness'),\n         ('autoplaying', 'autoplay'),\n         ('twise', 'twice'),\n         ('timeskip', 'time skip'),\n         ('disphosphorus', 'diphosphorus'),\n         ('implemnt', 'implement'),\n         ('proview', 'preview'),\n         ('pinshoppr', 'pin shoppe'),\n         ('protestng', 'protesting'),\n         ('chromatographymass', 'chromatography mass'),\n         ('ncache', 'cache'),\n         ('dowloands', 'downloads'),\n         ('biospecifics', 'bio specifics'),\n         ('conforim', 'conform'),\n         ('dreft', 'draft'),\n         ('sinhaleseand', 'Sinhalese'),\n         ('swivl', 'swivel'),\n         ('officerjms', 'officers'),\n         ('refrigrant', 'refrigerant'),\n         ('kendras', 'Kendra'),\n         ('alchoholism', 'alcoholism'),\n         ('dollor', 'dollar'),\n         ('jeyalalitha', 'Jayalalitha'),\n         ('bettner', 'better'),\n         ('itemstream', 'timestream'),\n         ('notetaking', 'note taking'),\n         ('cringworthy', 'cringeworthy'),\n         ('easyday', 'easy day'),\n         ('scenessex', 'scenes sex'),\n         ('vivavideo', 'via video'),\n         ('washboth', 'wash both'),\n         ('textout', 'text out'),\n         ('createwindow', 'create window'),\n         ('calsium', 'calcium'),\n         ('biofibre', 'bio fibre'),\n         ('emailbesides', 'email besides'),\n         ('kathhi', 'Kathi'),\n         ('cenre', 'center'),\n         ('polyarmory', 'polyamory'),\n         ('superforecasters', 'super forecasters'),\n         ('blogers', 'bloggers'),\n         ('medicalwhich', 'medical which'),\n         ('iiving', 'living'),\n         ('pronouciation', 'pronunciation'),\n         ('youor', 'you or'),\n         ('thuderbird', 'Thunderbird'),\n         ('oneside', 'one side'),\n         ('spearow', 'Spearow'),\n         ('aanythign', 'anything'),\n         ('inmaking', 'in making'),\n         ('datamining', 'data mining'),\n         ('greybus', 'grey bus'),\n         ('onmeter', 'on meter'),\n         ('biling', 'billing'),\n         ('fidlago', 'Fidalgo'),\n         ('edfice', 'edifice'),\n         ('microsolutions', 'micro solutions'),\n         ('easly', 'easily'),\n         ('eukarotic', 'eukaryotic'),\n         ('accedental', 'accidental'),\n         ('intercasts', 'interests'),\n         ('oppresive', 'oppressive'),\n         ('generalizably', 'generalizable'),\n         ('tacometer', 'tachometer'),\n         ('loking', 'looking'),\n         ('scrypt', 'script'),\n         ('usafter', 'us after'),\n         ('everyweek', 'every week'),\n         ('hopesthe', 'hopes the'),\n         ('openflow', 'OpenFlow'),\n         ('checkride', 'check ride'),\n         ('springdrive', 'spring drive'),\n         ('emobile', 'mobile'),\n         ('dermotology', 'dermatology'),\n         ('somatrophin', 'somatropin'),\n         ('saywe', 'say we'),\n         ('multistores', 'multistory'),\n         ('bolognaise', 'Bolognese'),\n         ('hardisk', 'harddisk'),\n         ('penisula', 'peninsula'),\n         ('refferring', 'referring'),\n         ('freshere', 'fresher'),\n         ('pokemkon', 'Pokemon'),\n         ('nuero', 'neuro'),\n         ('whosampled', 'who sampled'),\n         ('researchkit', 'research kit'),\n         ('speach', 'speech'),\n         ('acept', 'accept'),\n         ('indiashoppe', 'Indian shoppe'),\n         ('todescribe', 'to describe'),\n         ('hollywod', 'Hollywood'),\n         ('whastup', 'whassup'),\n         ('kjedahls', 'Kjeldahl'),\n         ('lancher', 'launcher'),\n         ('stalkees', 'stalkers'),\n         ('baclinks', 'backlinks'),\n         ('instutional', 'institutional'),\n         ('wassap', 'Wassup'),\n         ('methylethyl', 'methyl ethyl'),\n         ('fundbox', 'fund box'),\n         ('keypoints', 'key points'),\n         ('particually', 'particularly'),\n         ('loseit', 'lose it'),\n         ('gowipe', 'go wipe'),\n         ('autority', 'authority'),\n         ('prinicple', 'principle'),\n         ('complaince', 'compliance'),\n         ('itnormal', 'it normal'),\n         ('forpeople', 'for people'),\n         ('chaces', 'chances'),\n         ('yearhow', 'year how'),\n         ('fastcomet', 'fast comet'),\n         ('withadd', 'with add'),\n         ('omnicient', 'omniscient'),\n         ('tofeel', 'to feel'),\n         ('becauseof', 'because of'),\n         ('laungauage', 'language'),\n         ('combodia', 'Cambodia'),\n         ('bhuvneshwer', 'Bhubaneshwar'),\n         ('cognito', 'Cognito'),\n         ('thaelsemia', 'thalassemia'),\n         ('meritstore', 'merit store'),\n         ('masterbuate', 'masturbate'),\n         ('planethere', 'planet here'),\n         ('mostof', 'most of'),\n         ('shallowin', 'shallow in'),\n         ('wordwhen', 'word when'),\n         ('biodesalination', 'desalination'),\n         ('tendulkars', 'Tendulkar'),\n         ('kerja', 'Kerja'),\n         ('sertifikat', 'certificate'),\n         ('indegenous', 'indigenous'),\n         ('lowpage', 'low page'),\n         ('asend', 'ascend'),\n         ('leadreship', 'leadership'),\n         ('openlab', 'open lab'),\n         ('foldinghome', 'folding home'),\n         ('sachins', 'Sachin'),\n         ('pleatue', 'plateau'),\n         ('passwor', 'password'),\n         ('manisfestation', 'manifestation'),\n         ('valryian', 'valerian'),\n         ('chemotaxic', 'chemotaxis'),\n         ('condesending', 'condescending'),\n         ('spiltzvilla', 'splitsville'),\n         ('mammaliaforme', 'mammaliaform'),\n         ('instituteagra', 'institute agra'),\n         ('learningand', 'learning and'),\n         ('ramamurthynagar', 'Ramamurthy Nagar'),\n         ('glucoses', 'glucose'),\n         ('imitaion', 'imitation'),\n         ('awited', 'awaited'),\n         ('realvision', 'real vision'),\n         ('simslot', 'sim slot'),\n         ('yourr', 'your'),\n         ('pacjage', 'package'),\n         ('branchth', 'branch'),\n         ('magzin', 'magazine'),\n         ('frozon', 'frozen'),\n         ('codescomputational', 'code computational'),\n         ('tempratures', 'temperatures'),\n         ('neurophaphy', 'neuropathy'),\n         ('freezone', 'free zone'),\n         ('speices', 'species'),\n         ('compaitable', 'compatible'),\n         ('sensilization', 'sensitization'),\n         ('tuboscope', 'tube scope'),\n         ('gamechangers', 'game changer'),\n         ('windsheild', 'windshield'),\n         ('explorerie', 'explorer'),\n         ('cuccina', 'Cucina'),\n         ('earthstone', 'hearthstone'),\n         ('vocabs', 'vocab'),\n         ('previouse', 'previous'),\n         ('oneview', 'one view'),\n         ('relance', 'reliance'),\n         ('waterstop', 'water stop'),\n         ('imput', 'input'),\n         ('survivers', 'survivors'),\n         ('benedryl', 'Benadryl'),\n         ('requestparam', 'request param'),\n         ('typeadd', 'type add'),\n         ('autists', 'artists'),\n         ('forany', 'for any'),\n         ('inteview', 'interview'),\n         ('aphantasia', 'Phantasia'),\n         ('lisanna', 'Lisanne'),\n         ('civilengineering', 'civil engineering'),\n         ('austrailia', 'Australia'),\n         ('alchoholic', 'alcoholic'),\n         ('adaptersuch', 'adapter such'),\n         ('sphilosopher', 'philosopher'),\n         ('calenderisation', 'calendarization'),\n         ('smooking', 'smoking'),\n         ('pemdulum', 'pendulum'),\n         ('analsyis', 'analysis'),\n         ('psycholology', 'psychology'),\n         ('ubantu', 'ubuntu'),\n         ('emals', 'emails'),\n         ('questionth', 'questions'),\n         ('jawarlal', 'Jawaharlal'),\n         ('svaldbard', 'Svalbard'),\n         ('prabhudeva', 'Prabhudeva'),\n         ('robtics', 'robotics'),\n         ('umblock', 'unblock'),\n         ('professionaly', 'professionally'),\n         ('biovault', 'bio vault'),\n         ('bibal', 'bible'),\n         ('higherstudies', 'higher studies'),\n         ('lestoil', 'less oil'),\n         ('biteshow', 'bike show'),\n         ('humanslike', 'humans like'),\n         ('purpse', 'purpose'),\n         ('barazilian', 'Brazilian'),\n         ('gravitional', 'gravitational'),\n         ('cylinderical', 'cylindrical'),\n         ('peparing', 'preparing'),\n         ('healthequity', 'health equity'),\n         ('appcleaner', 'app cleaner'),\n         ('instantq', 'instant'),\n         ('abolisihed', 'abolished'),\n         ('kwench', 'quench'),\n         ('prisamatic', 'prismatic'),\n         ('bhubneshwar', 'Bhubaneshwar'),\n         ('liscense', 'license'),\n         ('cyberbase', 'cyber base'),\n         ('safezone', 'safe zone'),\n         ('deactivat', 'deactivate'),\n         ('salicyclic', 'salicylic'),\n         ('cocacola', 'coca cola'),\n         ('noice', 'noise'),\n         ('examinaton', 'examination'),\n         ('pharmavigilance', 'pharmacovigilance'),\n         ('sixthsense', 'sixth sense'),\n         ('musiclly', 'musically'),\n         ('khardushan', 'Kardashian'),\n         ('chandragupt', 'Chandragupta'),\n         ('bayesians', 'bayesian'),\n         ('engineeringbut', 'engineering but'),\n         ('caretrust', 'care trust'),\n         ('girlbut', 'girl but'),\n         ('aviations', 'aviation'),\n         ('joinee', 'joiner'),\n         ('tutior', 'tutor'),\n         ('tylenal', 'Tylenol'),\n         ('neccesity', 'necessity'),\n         ('kapsule', 'capsule'),\n         ('prayes', 'prayers'),\n         ('depositmobile', 'deposit mobile'),\n         ('settopbox', 'set top box'),\n         ('meotic', 'meiotic'),\n         ('accidentially', 'accidentally'),\n         ('offcloud', 'off cloud'),\n         ('keshavam', 'Keshava'),\n         ('domaincentral', 'domain central'),\n         ('onetaste', 'one taste'),\n         ('lumpsum', 'lump sum'),\n         ('medschool', 'med school'),\n         ('digicard', 'Digi card'),\n         ('abroadus', 'abroad'),\n         ('campusexcept', 'campus except'),\n         ('aptittude', 'aptitude'),\n         ('neutrions', 'neutrinos'),\n         ('onepaper', 'one paper'),\n         ('remidies', 'remedies'),\n         ('convinient', 'convenient'),\n         ('financaily', 'financially'),\n         ('postives', 'positives'),\n         ('nikefuel', 'Nike fuel'),\n         ('ingrediants', 'ingredients'),\n         ('aspireat', 'aspirate'),\n         ('firstand', 'first'),\n         ('mohammmad', 'Mohammad'),\n         ('mutliple', 'multiple'),\n         ('dimonatization', 'demonization'),\n         ('cente', 'center'),\n         ('marshmellow', 'marshmallow'),\n         ('citreon', 'Citroen'),\n         ('theirony', 'the irony'),\n         ('slienced', 'silenced'),\n         ('identifiy', 'identify'),\n         ('energ', 'energy'),\n         ('distribuiton', 'distribution'),\n         ('devoloping', 'developing'),\n         ('maharstra', 'Maharastra'),\n         ('siesmologist', 'seismologist'),\n         ('geckoos', 'geckos'),\n         ('placememnt', 'placement'),\n         ('introvercy', 'introvert'),\n         ('nuerosurgeon', 'neurosurgeon'),\n         ('realsense', 'real sense'),\n         ('congac', 'cognac'),\n         ('plaese', 'please'),\n         ('addicition', 'addiction'),\n         ('othet', 'other'),\n         ('howwill', 'how will'),\n         ('betablockers', 'beta blockers'),\n         ('phython', 'Python'),\n         ('concelling', 'counseling'),\n         ('einstine', 'Einstein'),\n         ('takinng', 'taking'),\n         ('birtday', 'birthday'),\n         ('prefessor', 'professor'),\n         ('dreamscreen', 'dream screen'),\n         ('satyabama', 'Satyabhama'),\n         ('faminism', 'feminism'),\n         ('noooooooooo', 'no'),\n         ('certifaction', 'certification'),\n         ('smalll', 'small'),\n         ('sterlization', 'sterilization'),\n         ('athelete', 'athlete'),\n         ('comppany', 'company'),\n         ('handlebreakup', 'handle a breakup'),\n         ('wellrounded', 'well rounded'),\n         ('breif', 'brief'),\n         ('engginering', 'engineering'),\n         ('genrally', 'generally'),\n         ('forgote', 'forgot'),\n         ('compuny', 'the company'),\n         ('wholeseller', 'wholesaler'),\n         ('conventioal', 'conventional'),\n         ('healther', 'healthier'),\n         ('realitic', 'realistic'),\n         ('israil', 'Israel'),\n         ('morghulis', 'Margulis'),\n         ('begineer', 'beginner'),\n         ('unwaiveringly', 'unwavering'),\n         ('writen', 'written'),\n         ('gastly', 'ghastly'),\n         ('obscurial', 'obscure'),\n         ('permanetly', 'permanently'),\n         ('bday', 'birthday'),\n         ('studing', 'studying'),\n         ('blackcore', 'black core'),\n         ('macbok', 'MacBook'),\n         ('realted', 'related'),\n         ('resoning', 'reasoning'),\n         ('servicenow', 'service now'),\n         ('medels', 'medals'),\n         ('hairloss', 'hair loss'),\n         ('messanger', 'messenger'),\n         ('masterbate', 'masturbate'),\n         ('oppurtunities', 'opportunities'),\n         ('newzealand', 'new zealand'),\n         ('offcampus', 'off campus'),\n         ('lonliness', 'loneliness'),\n         ('percentilers', 'percentiles'),\n         ('caccount', 'account'),\n         ('imrovement', 'improvement'),\n         ('cashbacks', 'cashback'),\n         ('inhand', 'in hand'),\n         ('baahubali', 'bahubali'),\n         ('diffrent', 'different'),\n         ('strategywho', 'strategy who'),\n         ('meetme', 'meet me'),\n         ('wealthfront', 'wealth front'),\n         ('masterbation', 'masturbation'),\n         ('successfull', 'successful'),\n         ('lenght', 'length'),\n         ('increse', 'increase'),\n         ('mastrubation', 'masturbation'),\n         ('intresting', 'interesting'),\n         ('quesitons', 'questions'),\n         ('fullstack', 'full stack'),\n         ('harambe', 'Harambee'),\n         ('criterias', 'criteria'),\n         ('rajyasabha', 'Rajya Sabha'),\n         ('techmahindra', 'tech Mahindra'),\n         ('messeges', 'messages'),\n         ('intership', 'internship'),\n         ('benifits', 'benefits'),\n         ('dowload', 'download'),\n         ('dellhi', 'Delhi'),\n         ('traval', 'travel'),\n         ('prepration', 'preparation'),\n         ('engineeringwhat', 'engineering what'),\n         ('habbit', 'habit'),\n         ('diference', 'difference'),\n         ('permantley', 'permanently'),\n         ('doesnot', 'does not'),\n         ('thebest', 'the best'),\n         ('addmision', 'admission'),\n         ('gramatically', 'grammatically'),\n         ('dayswhich', 'days which'),\n         ('intrest', 'interest'),\n         ('seperatists', 'separatists'),\n         ('plagarism', 'plagiarism'),\n         ('demonitize', 'demonetize'),\n         ('explaination', 'explanation'),\n         ('numericals', 'numerical'),\n         ('defination', 'definition'),\n         ('inmortal', 'immortal'),\n         ('elasticsearch', 'elastic search')\n    ]\n\n\n\nREGEX_REPLACER = [\n    (re.compile(pat.replace(\"*\", \"\\*\"), flags=re.IGNORECASE), repl)\n    for pat, repl in WORDS_REPLACER\n]\n\nREGEX_REPLACER_2 = [\n    (re.compile(pat.replace(\"*\", \"\\*\"), flags=re.IGNORECASE), repl)\n    for pat, repl in WORDS_REPLACER_2\n]\n\nREGEX_REPLACER_3 = [\n    (re.compile(pat.replace(\"*\", \"\\*\"), flags=re.IGNORECASE), repl)\n    for pat, repl in WORDS_REPLACER_3\n]\n\nREGEX_REPLACER_4 = [\n    (re.compile(pat.replace(\"*\", \"\\*\"), flags=re.IGNORECASE), repl)\n    for pat, repl in WORDS_REPLACER_4\n]\n\n\"\"\"\nWORDS_REPLACER_5 = [('[\"code\"]', '[\"CODE\"]'),\n                    ('[\"formula\"]', '[\"FORMULA\"]')\n                   ]\n\nREGEX_REPLACER_5 = [\n    (re.compile(pat.replace(\"*\", \"\\*\"), flags=re.IGNORECASE), repl)\n    for pat, repl in WORDS_REPLACER_5\n]\n\"\"\"\n\nRE_SPACE = re.compile(r\"\\s\")\nRE_MULTI_SPACE = re.compile(r\"\\s+\")\n\nsymbols_to_isolate = '.,?!-;*\u2026:\u2014()[]%#$&_\/@\uff3c\u30fb\u03c9+=^\u2013>\\\\\u00b0<~\u2022\u2260\u2122\u02c8\u028a\u0252\u221e\u00a7{}\u00b7\u03c4\u03b1\u2764\u263a\u0261|\u00a2\u2192\u0336`\u2765\u2501\u2523\u252b\u2517\uff2f\u25ba\u2605\u00a9\u2015\u026a\u2714\u00ae\\x96\\x92\u25cf\u00a3\u2665\u27a4\u00b4\u00b9\u2615\u2248\u00f7\u2661\u25d0\u2551\u25ac\u2032\u0254\u02d0\u20ac\u06e9\u06de\u2020\u03bc\u2712\u27a5\u2550\u2606\u02cc\u25c4\u00bd\u02bb\u03c0\u03b4\u03b7\u03bb\u03c3\u03b5\u03c1\u03bd\u0283\u272c\uff33\uff35\uff30\uff25\uff32\uff29\uff34\u263b\u00b1\u264d\u00b5\u00ba\u00be\u2713\u25fe\u061f\uff0e\u2b05\u2105\u00bb\u0412\u0430\u0432\u2763\u22c5\u00bf\u00ac\u266b\uff23\uff2d\u03b2\u2588\u2593\u2592\u2591\u21d2\u2b50\u203a\u00a1\u2082\u2083\u2767\u25b0\u2594\u25de\u2580\u2582\u2583\u2584\u2585\u2586\u2587\u2199\u03b3\u0304\u2033\u2639\u27a1\u00ab\u03c6\u2153\u201e\u270b\uff1a\u00a5\u0332\u0305\u0301\u2219\u201b\u25c7\u270f\u25b7\u2753\u2757\u00b6\u02da\u02d9\uff09\u0441\u0438\u02bf\u2728\u3002\u0251\\x80\u25d5\uff01\uff05\u00af\u2212\ufb02\ufb01\u2081\u00b2\u028c\u00bc\u2074\u2044\u2084\u2320\u266d\u2718\u256a\u25b6\u262d\u272d\u266a\u2614\u2620\u2642\u2603\u260e\u2708\u270c\u2730\u2746\u2619\u25cb\u2023\u2693\u5e74\u220e\u2112\u25aa\u2599\u260f\u215b\uff43\uff41\uff53\u01c0\u212e\u00b8\uff57\u201a\u223c\u2016\u2133\u2744\u2190\u263c\u22c6\u0292\u2282\u3001\u2154\u00a8\u0361\u0e4f\u26be\u26bd\u03a6\u00d7\u03b8\uffe6\uff1f\uff08\u2103\u23e9\u262e\u26a0\u6708\u270a\u274c\u2b55\u25b8\u25a0\u21cc\u2610\u2611\u26a1\u2604\u01eb\u256d\u2229\u256e\uff0c\u4f8b\uff1e\u0295\u0250\u0323\u0394\u2080\u271e\u2508\u2571\u2572\u258f\u2595\u2503\u2570\u258a\u258b\u256f\u2533\u250a\u2265\u2612\u2191\u261d\u0279\u2705\u261b\u2669\u261e\uff21\uff2a\uff22\u25d4\u25e1\u2193\u2640\u2b06\u0331\u210f\\x91\u2800\u02e4\u255a\u21ba\u21e4\u220f\u273e\u25e6\u266c\u00b3\u306e\uff5c\uff0f\u2235\u2234\u221a\u03a9\u00a4\u261c\u25b2\u21b3\u25ab\u203f\u2b07\u2727\uff4f\uff56\uff4d\uff0d\uff12\uff10\uff18\uff07\u2030\u2264\u2215\u02c6\u269c\u2601'\nsymbols_to_delete = '\\n\ud83c\udf55\\r\ud83d\udc35\ud83d\ude11\\xa0\\ue014\\t\\uf818\\uf04a\\xad\ud83d\ude22\ud83d\udc36\ufe0f\\uf0e0\ud83d\ude1c\ud83d\ude0e\ud83d\udc4a\\u200b\\u200e\ud83d\ude01\u0639\u062f\u0648\u064a\u0647\u0635\u0642\u0623\u0646\u0627\u062e\u0644\u0649\u0628\u0645\u063a\u0631\ud83d\ude0d\ud83d\udc96\ud83d\udcb5\u0415\ud83d\udc4e\ud83d\ude00\ud83d\ude02\\u202a\\u202c\ud83d\udd25\ud83d\ude04\ud83c\udffb\ud83d\udca5\u1d0d\u028f\u0280\u1d07\u0274\u1d05\u1d0f\u1d00\u1d0b\u029c\u1d1c\u029f\u1d1b\u1d04\u1d18\u0299\u0493\u1d0a\u1d21\u0262\ud83d\ude0b\ud83d\udc4f\u05e9\u05dc\u05d5\u05dd\u05d1\u05d9\ud83d\ude31\u203c\\x81\u30a8\u30f3\u30b8\u6545\u969c\\u2009\ud83d\ude8c\u1d35\u035e\ud83c\udf1f\ud83d\ude0a\ud83d\ude33\ud83d\ude27\ud83d\ude40\ud83d\ude10\ud83d\ude15\\u200f\ud83d\udc4d\ud83d\ude2e\ud83d\ude03\ud83d\ude18\u05d0\u05e2\u05db\u05d7\ud83d\udca9\ud83d\udcaf\u26fd\ud83d\ude84\ud83c\udffc\u0b9c\ud83d\ude16\u1d20\ud83d\udeb2\u2010\ud83d\ude1f\ud83d\ude08\ud83d\udcaa\ud83d\ude4f\ud83c\udfaf\ud83c\udf39\ud83d\ude07\ud83d\udc94\ud83d\ude21\\x7f\ud83d\udc4c\u1f10\u1f76\u03ae\u03b9\u1f72\u03ba\u1f00\u03af\u1fc3\u1f34\u03be\ud83d\ude44\uff28\ud83d\ude20\\ufeff\\u2028\ud83d\ude09\ud83d\ude24\u26fa\ud83d\ude42\\u3000\u062a\u062d\u0643\u0633\u0629\ud83d\udc6e\ud83d\udc99\u0641\u0632\u0637\ud83d\ude0f\ud83c\udf7e\ud83c\udf89\ud83d\ude1e\\u2008\ud83c\udffe\ud83d\ude05\ud83d\ude2d\ud83d\udc7b\ud83d\ude25\ud83d\ude14\ud83d\ude13\ud83c\udffd\ud83c\udf86\ud83c\udf7b\ud83c\udf7d\ud83c\udfb6\ud83c\udf3a\ud83e\udd14\ud83d\ude2a\\x08\u2011\ud83d\udc30\ud83d\udc07\ud83d\udc31\ud83d\ude46\ud83d\ude28\ud83d\ude43\ud83d\udc95\ud835\ude0a\ud835\ude26\ud835\ude33\ud835\ude22\ud835\ude35\ud835\ude30\ud835\ude24\ud835\ude3a\ud835\ude34\ud835\ude2a\ud835\ude27\ud835\ude2e\ud835\ude23\ud83d\udc97\ud83d\udc9a\u5730\u7344\u8c37\u0443\u043b\u043a\u043d\u041f\u043e\u0410\u041d\ud83d\udc3e\ud83d\udc15\ud83d\ude06\u05d4\ud83d\udd17\ud83d\udebd\u6b4c\u821e\u4f0e\ud83d\ude48\ud83d\ude34\ud83c\udfff\ud83e\udd17\ud83c\uddfa\ud83c\uddf8\u043c\u03c5\u0442\u0455\u2935\ud83c\udfc6\ud83c\udf83\ud83d\ude29\\u200a\ud83c\udf20\ud83d\udc1f\ud83d\udcab\ud83d\udcb0\ud83d\udc8e\u044d\u043f\u0440\u0434\\x95\ud83d\udd90\ud83d\ude45\u26f2\ud83c\udf70\ud83e\udd10\ud83d\udc46\ud83d\ude4c\\u2002\ud83d\udc9b\ud83d\ude41\ud83d\udc40\ud83d\ude4a\ud83d\ude49\\u2004\u02e2\u1d52\u02b3\u02b8\u1d3c\u1d37\u1d3a\u02b7\u1d57\u02b0\u1d49\u1d58\\x13\ud83d\udeac\ud83e\udd13\\ue602\ud83d\ude35\u03ac\u03bf\u03cc\u03c2\u03ad\u1f78\u05ea\u05de\u05d3\u05e3\u05e0\u05e8\u05da\u05e6\u05d8\ud83d\ude12\u035d\ud83c\udd95\ud83d\udc45\ud83d\udc65\ud83d\udc44\ud83d\udd04\ud83d\udd24\ud83d\udc49\ud83d\udc64\ud83d\udc76\ud83d\udc72\ud83d\udd1b\ud83c\udf93\\uf0b7\\uf04c\\x9f\\x10\u6210\u90fd\ud83d\ude23\u23fa\ud83d\ude0c\ud83e\udd11\ud83c\udf0f\ud83d\ude2f\u0435\u0445\ud83d\ude32\u1f38\u1fb6\u1f41\ud83d\udc9e\ud83d\ude93\ud83d\udd14\ud83d\udcda\ud83c\udfc0\ud83d\udc50\\u202d\ud83d\udca4\ud83c\udf47\\ue613\u5c0f\u571f\u8c46\ud83c\udfe1\u2754\u2049\\u202f\ud83d\udc60\u300b\u0915\u0930\u094d\u092e\u093e\ud83c\uddf9\ud83c\uddfc\ud83c\udf38\u8521\u82f1\u6587\ud83c\udf1e\ud83c\udfb2\u30ec\u30af\u30b5\u30b9\ud83d\ude1b\u5916\u56fd\u4eba\u5173\u7cfb\u0421\u0431\ud83d\udc8b\ud83d\udc80\ud83c\udf84\ud83d\udc9c\ud83e\udd22\u0650\u064e\u044c\u044b\u0433\u044f\u4e0d\u662f\\x9c\\x9d\ud83d\uddd1\\u2005\ud83d\udc83\ud83d\udce3\ud83d\udc7f\u0f3c\u3064\u0f3d\ud83d\ude30\u1e37\u0417\u0437\u25b1\u0446\ufffc\ud83e\udd23\u5356\u6e29\u54e5\u534e\u8bae\u4f1a\u4e0b\u964d\u4f60\u5931\u53bb\u6240\u6709\u7684\u94b1\u52a0\u62ff\u5927\u574f\u7a0e\u9a97\u5b50\ud83d\udc1d\u30c4\ud83c\udf85\\x85\ud83c\udf7a\u0622\u0625\u0634\u0621\ud83c\udfb5\ud83c\udf0e\u035f\u1f14\u6cb9\u522b\u514b\ud83e\udd21\ud83e\udd25\ud83d\ude2c\ud83e\udd27\u0439\\u2003\ud83d\ude80\ud83e\udd34\u02b2\u0448\u0447\u0418\u041e\u0420\u0424\u0414\u042f\u041c\u044e\u0436\ud83d\ude1d\ud83d\udd91\u1f50\u1f7b\u03cd\u7279\u6b8a\u4f5c\u6226\u7fa4\u0449\ud83d\udca8\u5706\u660e\u56ed\u05e7\u2110\ud83c\udfc8\ud83d\ude3a\ud83c\udf0d\u23cf\u1ec7\ud83c\udf54\ud83d\udc2e\ud83c\udf41\ud83c\udf46\ud83c\udf51\ud83c\udf2e\ud83c\udf2f\ud83e\udd26\\u200d\ud835\udcd2\ud835\udcf2\ud835\udcff\ud835\udcf5\uc548\uc601\ud558\uc138\uc694\u0416\u0459\u041a\u045b\ud83c\udf40\ud83d\ude2b\ud83e\udd24\u1fe6\u6211\u51fa\u751f\u5728\u4e86\u53ef\u4ee5\u8bf4\u666e\u901a\u8bdd\u6c49\u8bed\u597d\u6781\ud83c\udfbc\ud83d\udd7a\ud83c\udf78\ud83e\udd42\ud83d\uddfd\ud83c\udf87\ud83c\udf8a\ud83c\udd98\ud83e\udd20\ud83d\udc69\ud83d\udd92\ud83d\udeaa\u5929\u4e00\u5bb6\u26b2\\u2006\u26ad\u2686\u2b2d\u2b2f\u23d6\u65b0\u2700\u254c\ud83c\uddeb\ud83c\uddf7\ud83c\udde9\ud83c\uddea\ud83c\uddee\ud83c\uddec\ud83c\udde7\ud83d\ude37\ud83c\udde8\ud83c\udde6\u0425\u0428\ud83c\udf10\\x1f\u6740\u9e21\u7ed9\u7334\u770b\u0281\ud835\uddea\ud835\uddf5\ud835\uddf2\ud835\uddfb\ud835\ude06\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\uddee\ud835\uddf9\ud835\uddf6\ud835\ude07\ud835\uddef\ud835\ude01\ud835\uddf0\ud835\ude00\ud835\ude05\ud835\uddfd\ud835\ude04\ud835\uddf1\ud83d\udcfa\u03d6\\u2000\u04af\u057d\u1d26\u13a5\u04bb\u037a\\u2007\u0570\\u2001\u0269\uff59\uff45\u0d66\uff4c\u01bd\uff48\ud835\udc13\ud835\udc21\ud835\udc1e\ud835\udc2b\ud835\udc2e\ud835\udc1d\ud835\udc1a\ud835\udc03\ud835\udc1c\ud835\udc29\ud835\udc2d\ud835\udc22\ud835\udc28\ud835\udc27\u0184\u1d28\u05df\u146f\u0ed0\u03a4\u13e7\u0be6\u0406\u1d11\u0701\ud835\udc2c\ud835\udc30\ud835\udc32\ud835\udc1b\ud835\udc26\ud835\udc2f\ud835\udc11\ud835\udc19\ud835\udc23\ud835\udc07\ud835\udc02\ud835\udc18\ud835\udfce\u051c\u0422\u15de\u0c66\u3014\u13ab\ud835\udc33\ud835\udc14\ud835\udc31\ud835\udfd4\ud835\udfd3\ud835\udc05\ud83d\udc0b\ufb03\ud83d\udc98\ud83d\udc93\u0451\ud835\ude25\ud835\ude2f\ud835\ude36\ud83d\udc90\ud83c\udf0b\ud83c\udf04\ud83c\udf05\ud835\ude6c\ud835\ude56\ud835\ude68\ud835\ude64\ud835\ude63\ud835\ude61\ud835\ude6e\ud835\ude58\ud835\ude60\ud835\ude5a\ud835\ude59\ud835\ude5c\ud835\ude67\ud835\ude65\ud835\ude69\ud835\ude6a\ud835\ude57\ud835\ude5e\ud835\ude5d\ud835\ude5b\ud83d\udc7a\ud83d\udc37\u210b\ud835\udc00\ud835\udc25\ud835\udc2a\ud83d\udeb6\ud835\ude62\u1f39\ud83e\udd18\u0366\ud83d\udcb8\u062c\ud328\ud2f0\uff37\ud835\ude47\u1d7b\ud83d\udc42\ud83d\udc43\u025c\ud83c\udfab\\uf0a7\u0411\u0423\u0456\ud83d\udea2\ud83d\ude82\u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4\u0ac0\u1fc6\ud83c\udfc3\ud835\udcec\ud835\udcfb\ud835\udcf4\ud835\udcee\ud835\udcfd\ud835\udcfc\u2618\ufd3e\u032f\ufd3f\u20bd\\ue807\ud835\udc7b\ud835\udc86\ud835\udc8d\ud835\udc95\ud835\udc89\ud835\udc93\ud835\udc96\ud835\udc82\ud835\udc8f\ud835\udc85\ud835\udc94\ud835\udc8e\ud835\udc97\ud835\udc8a\ud83d\udc7d\ud83d\ude19\\u200c\u041b\u2012\ud83c\udfbe\ud83d\udc79\u238c\ud83c\udfd2\u26f8\u516c\u5bd3\u517b\u5ba0\u7269\u5417\ud83c\udfc4\ud83d\udc00\ud83d\ude91\ud83e\udd37\u64cd\u7f8e\ud835\udc91\ud835\udc9a\ud835\udc90\ud835\udc74\ud83e\udd19\ud83d\udc12\u6b22\u8fce\u6765\u5230\u963f\u62c9\u65af\u05e1\u05e4\ud835\ude6b\ud83d\udc08\ud835\udc8c\ud835\ude4a\ud835\ude6d\ud835\ude46\ud835\ude4b\ud835\ude4d\ud835\ude3c\ud835\ude45\ufdfb\ud83e\udd84\u5de8\u6536\u8d62\u5f97\u767d\u9b3c\u6124\u6012\u8981\u4e70\u989d\u1ebd\ud83d\ude97\ud83d\udc33\ud835\udfcf\ud835\udc1f\ud835\udfd6\ud835\udfd1\ud835\udfd5\ud835\udc84\ud835\udfd7\ud835\udc20\ud835\ude44\ud835\ude43\ud83d\udc47\u951f\u65a4\u62f7\ud835\udde2\ud835\udff3\ud835\udff1\ud835\udfec\u2981\u30de\u30eb\u30cf\u30cb\u30c1\u30ed\u682a\u5f0f\u793e\u26f7\ud55c\uad6d\uc5b4\u3138\u3153\ub2c8\u035c\u0296\ud835\ude3f\ud835\ude54\u20b5\ud835\udca9\u212f\ud835\udcbe\ud835\udcc1\ud835\udcb6\ud835\udcc9\ud835\udcc7\ud835\udcca\ud835\udcc3\ud835\udcc8\ud835\udcc5\u2134\ud835\udcbb\ud835\udcbd\ud835\udcc0\ud835\udccc\ud835\udcb8\ud835\udcce\ud835\ude4f\u03b6\ud835\ude5f\ud835\ude03\ud835\uddfa\ud835\udfee\ud835\udfed\ud835\udfef\ud835\udff2\ud83d\udc4b\ud83e\udd8a\u591a\u4f26\ud83d\udc3d\ud83c\udfbb\ud83c\udfb9\u26d3\ud83c\udff9\ud83c\udf77\ud83e\udd86\u4e3a\u548c\u4e2d\u53cb\u8c0a\u795d\u8d3a\u4e0e\u5176\u60f3\u8c61\u5bf9\u6cd5\u5982\u76f4\u63a5\u95ee\u7528\u81ea\u5df1\u731c\u672c\u4f20\u6559\u58eb\u6ca1\u79ef\u552f\u8ba4\u8bc6\u57fa\u7763\u5f92\u66fe\u7ecf\u8ba9\u76f8\u4fe1\u8036\u7a23\u590d\u6d3b\u6b7b\u602a\u4ed6\u4f46\u5f53\u4eec\u804a\u4e9b\u653f\u6cbb\u9898\u65f6\u5019\u6218\u80dc\u56e0\u5723\u628a\u5168\u5802\u7ed3\u5a5a\u5b69\u6050\u60e7\u4e14\u6817\u8c13\u8fd9\u6837\u8fd8\u267e\ud83c\udfb8\ud83e\udd15\ud83e\udd12\u26d1\ud83c\udf81\u6279\u5224\u68c0\u8ba8\ud83c\udfdd\ud83e\udd81\ud83d\ude4b\ud83d\ude36\uc950\uc2a4\ud0f1\ud2b8\ub93c\ub3c4\uc11d\uc720\uac00\uaca9\uc778\uc0c1\uc774\uacbd\uc81c\ud669\uc744\ub835\uac8c\ub9cc\ub4e4\uc9c0\uc54a\ub85d\uc798\uad00\ub9ac\ud574\uc57c\ud569\ub2e4\uce90\ub098\uc5d0\uc11c\ub300\ub9c8\ucd08\uc640\ud654\uc57d\uae08\uc758\ud488\ub7f0\uc131\ubd84\uac08\ub54c\ub294\ubc18\ub4dc\uc2dc\ud5c8\ub41c\uc0ac\uc6a9\ud83d\udd2b\ud83d\udc41\u51f8\u1f70\ud83d\udcb2\ud83d\uddef\ud835\ude48\u1f0c\ud835\udc87\ud835\udc88\ud835\udc98\ud835\udc83\ud835\udc6c\ud835\udc76\ud835\udd7e\ud835\udd99\ud835\udd97\ud835\udd86\ud835\udd8e\ud835\udd8c\ud835\udd8d\ud835\udd95\ud835\udd8a\ud835\udd94\ud835\udd91\ud835\udd89\ud835\udd93\ud835\udd90\ud835\udd9c\ud835\udd9e\ud835\udd9a\ud835\udd87\ud835\udd7f\ud835\udd98\ud835\udd84\ud835\udd9b\ud835\udd92\ud835\udd8b\ud835\udd82\ud835\udd74\ud835\udd9f\ud835\udd88\ud835\udd78\ud83d\udc51\ud83d\udebf\ud83d\udca1\u77e5\u5f7c\u767e\\uf005\ud835\ude40\ud835\udc9b\ud835\udc72\ud835\udc73\ud835\udc7e\ud835\udc8b\ud835\udfd2\ud83d\ude26\ud835\ude52\ud835\ude3e\ud835\ude3d\ud83c\udfd0\ud835\ude29\ud835\ude28\u1f7c\u1e51\ud835\udc71\ud835\udc79\ud835\udc6b\ud835\udc75\ud835\udc6a\ud83c\uddf0\ud83c\uddf5\ud83d\udc7e\u14c7\u14a7\u152d\u1403\u1427\u1426\u1473\u1428\u14c3\u14c2\u1472\u1438\u146d\u144e\u14c0\u1423\ud83d\udc04\ud83c\udf88\ud83d\udd28\ud83d\udc0e\ud83e\udd1e\ud83d\udc38\ud83d\udc9f\ud83c\udfb0\ud83c\udf1d\ud83d\udef3\u70b9\u51fb\u67e5\u7248\ud83c\udf6d\ud835\udc65\ud835\udc66\ud835\udc67\uff2e\uff27\ud83d\udc63\\uf020\u3063\ud83c\udfc9\u0444\ud83d\udcad\ud83c\udfa5\u039e\ud83d\udc34\ud83d\udc68\ud83e\udd33\ud83e\udd8d\\x0b\ud83c\udf69\ud835\udc6f\ud835\udc92\ud83d\ude17\ud835\udfd0\ud83c\udfc2\ud83d\udc73\ud83c\udf57\ud83d\udd49\ud83d\udc32\u0686\u06cc\ud835\udc6e\ud835\uddd5\ud835\uddf4\ud83c\udf52\ua725\u2ca3\u2c8f\ud83d\udc11\u23f0\u9244\u30ea\u4e8b\u4ef6\u0457\ud83d\udc8a\u300c\u300d\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600\u71fb\u88fd\u30b7\u865a\u507d\u5c41\u7406\u5c48\u0413\ud835\udc69\ud835\udc70\ud835\udc80\ud835\udc7a\ud83c\udf24\ud835\uddf3\ud835\udddc\ud835\uddd9\ud835\udde6\ud835\udde7\ud83c\udf4a\u1f7a\u1f08\u1f21\u03c7\u1fd6\u039b\u290f\ud83c\uddf3\ud835\udc99\u03c8\u0541\u0574\u0565\u057c\u0561\u0575\u056b\u0576\u0580\u0582\u0564\u0571\u51ac\u81f3\u1f40\ud835\udc81\ud83d\udd39\ud83e\udd1a\ud83c\udf4e\ud835\udc77\ud83d\udc02\ud83d\udc85\ud835\ude2c\ud835\ude31\ud835\ude38\ud835\ude37\ud835\ude10\ud835\ude2d\ud835\ude13\ud835\ude16\ud835\ude39\ud835\ude32\ud835\ude2b\u06a9\u0392\u03ce\ud83d\udca2\u039c\u039f\u039d\u0391\u0395\ud83c\uddf1\u2672\ud835\udf48\u21b4\ud83d\udc92\u2298\u023b\ud83d\udeb4\ud83d\udd95\ud83d\udda4\ud83e\udd58\ud83d\udccd\ud83d\udc48\u2795\ud83d\udeab\ud83c\udfa8\ud83c\udf11\ud83d\udc3b\ud835\udc0e\ud835\udc0d\ud835\udc0a\ud835\udc6d\ud83e\udd16\ud83c\udf8e\ud83d\ude3c\ud83d\udd77\uff47\uff52\uff4e\uff54\uff49\uff44\uff55\uff46\uff42\uff4b\ud835\udff0\ud83c\uddf4\ud83c\udded\ud83c\uddfb\ud83c\uddf2\ud835\uddde\ud835\udded\ud835\uddd8\ud835\udde4\ud83d\udc7c\ud83d\udcc9\ud83c\udf5f\ud83c\udf66\ud83c\udf08\ud83d\udd2d\u300a\ud83d\udc0a\ud83d\udc0d\\uf10a\u10da\u06a1\ud83d\udc26\\U0001f92f\\U0001f92a\ud83d\udc21\ud83d\udcb3\u1f31\ud83d\ude47\ud835\uddf8\ud835\udddf\ud835\udde0\ud835\uddf7\ud83e\udd5c\u3055\u3088\u3046\u306a\u3089\ud83d\udd3c'\nsymbols_to_delete2 = '\"#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~' + '\u201c\u201d\u2019'\n\n\nisolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}\nremove_dict1 = {ord(c):f'' for c in symbols_to_delete2}\n\nNMS_TABLE = dict.fromkeys(\n    i for i in range(sys.maxunicode + 1) if unicodedata.category(chr(i)) == \"Mn\"\n)\n\nHEBREW_TABLE = {i: \"\u05d0\" for i in range(0x0590, 0x05FF)}\nARABIC_TABLE = {i: \"\u0627\" for i in range(0x0600, 0x06FF)}\nCHINESE_TABLE = {i: \"\u662f\" for i in range(0x4E00, 0x9FFF)}\nKANJI_TABLE = {i: \"\u30c3\" for i in range(0x2E80, 0x2FD5)}\nHIRAGANA_TABLE = {i: \"\u30c3\" for i in range(0x3041, 0x3096)}\nKATAKANA_TABLE = {i: \"\u30c3\" for i in range(0x30A0, 0x30FF)}\n\nTABLE = dict()\nTABLE.update(CUSTOM_TABLE)\nTABLE.update(NMS_TABLE)\n# Non-english languages\nTABLE.update(CHINESE_TABLE)\nTABLE.update(HEBREW_TABLE)\nTABLE.update(ARABIC_TABLE)\nTABLE.update(HIRAGANA_TABLE)\nTABLE.update(KATAKANA_TABLE)\nTABLE.update(KANJI_TABLE)\n\n\n\nEMOJI_REGEXP = emoji.get_emoji_regexp()\n\nUNICODE_EMOJI_MY = {\n    k: f\" EMJ {v.strip(':').replace('_', ' ')} \"\n    for k, v in emoji.UNICODE_EMOJI_ALIAS.items()\n}\n\ndef my_demojize(string: str) -> str:\n    def replace(match):\n        return UNICODE_EMOJI_MY.get(match.group(0), match.group(0))\n    \n    return re.sub(\"\\ufe0f\", \"\", EMOJI_REGEXP.sub(replace, string))\n\ndef normalize(text: str) -> str:\n    \n    text = text.replace(\"[CODE]\", \" ACODEA \")\n    text = text.replace(\"[FORMULA]\", \" AFORMULAA \")\n    #text = text.replace(\"[]\", \" [URL] \")\n    \n    text = html.unescape(text)\n    \n    text = text.lower()\n    \n    text = my_demojize(text)\n    \n    # replacing urls with \"url\" string\n    text = re.sub('(?:(?:https?|ftp):\\\/\\\/)?[\\w\/\\-?=%.]+\\.[\\w\/\\-?=%.]+', '[WEB]' ,text)\n    text = text.replace(\"[WEB]\", \" AWEBA \")\n    \n    text = RE_SPACE.sub(\" \", text)\n    text = unicodedata.normalize(\"NFKD\", text)\n    text = text.translate(TABLE)\n    text = RE_MULTI_SPACE.sub(\" \", text).strip()\n    \n    # remove some unimportent symbles\n    text = text.translate(remove_dict)\n    \n    # remove some unimportent symbles\n    text = text.translate(remove_dict1)\n    \n    text = text.translate(isolate_dict)\n    \n\n    # Replacing and mispell\n    \n    for pattern, repl in REGEX_REPLACER:\n        text = pattern.sub(repl, text)\n    \n    for pattern, repl in REGEX_REPLACER_2:\n        text = pattern.sub(repl, text)\n            \n    for pattern, repl in REGEX_REPLACER_3:\n        text = pattern.sub(repl, text)\n        \n    # isolated_characters\n    #text = text.translate(isolate_dict)\n    \n    for pattern, repl in REGEX_REPLACER_4:\n        text = pattern.sub(repl, text)\n    \"\"\"\n    for pattern, repl in REGEX_REPLACER_5:\n        text = pattern.sub(repl, text)\n    \"\"\"\n    \n    \n        \n    text = RE_MULTI_SPACE.sub(\" \", text).strip()\n    \n    text = text.replace(\" acodea \", \" [CODE] \")\n    text = text.replace(\" aformulaa \", \" [FORMULA] \")\n    text = text.replace(\" AWEBA \", \" [WEB] \")\n    \n    text = RE_MULTI_SPACE.sub(\" \", text).strip()\n    \n    return text","bce5bb51":"%%time\n\ntrain_final[\"question_body_clean1\"] = train_final.question_body_clean.apply(normalize)\ntrain_final[\"answer_clean1\"]  = train_final.answer_clean.apply(normalize)\ntrain_final[\"question_title_clean1\"] = train_final.question_title.apply(normalize)\n\ntest_final[\"question_body_clean1\"] = test_final.question_body_clean.apply(normalize)\ntest_final[\"answer_clean1\"]  = test_final.answer_clean.apply(normalize)\ntest_final[\"question_title_clean1\"] = test_final.question_title.apply(normalize)\n\ntrain_final[\"question_body_clean1\"].fillna(\"please see figure below\", inplace=True)\ntest_final[\"question_body_clean1\"].fillna(\"please see figure below\", inplace=True)\n\ntrain_final[\"answer_clean1\"].fillna(\"please see figure below\", inplace=True)\ntest_final[\"answer_clean1\"].fillna(\"please see figure below\", inplace=True)\n\ntrain_final[\"question_title_clean1\"].fillna(\"please see figure below\", inplace=True)\ntest_final[\"question_title_clean1\"].fillna(\"please see figure below\", inplace=True)","d487daa3":"%%time\n\ndef replace_specialtokens(text):\n    text = text.replace(\" [CODE] \", \" code \")\n    text = text.replace(\" [FORMULA] \", \" formula \")\n    text = text.replace(\" [WEB] \", \" web \")\n    return text\n\ntrain_final[\"question_body_clean2\"] = train_final.question_body_clean1.apply(replace_specialtokens)\ntrain_final[\"answer_clean2\"]  = train_final.answer_clean1.apply(replace_specialtokens)\ntrain_final[\"question_title_clean2\"] = train_final.question_title_clean1.apply(replace_specialtokens)\n\ntest_final[\"question_body_clean2\"] = test_final.question_body_clean1.apply(replace_specialtokens)\ntest_final[\"answer_clean2\"]  = test_final.answer_clean1.apply(replace_specialtokens)\ntest_final[\"question_title_clean2\"] = test_final.question_title_clean1.apply(replace_specialtokens)","edf1009b":"train_final[\"question_body_all_clean\"] = train_final.question_body_all.apply(normalize)\ntrain_final[\"answer_all_clean\"]  = train_final.answer_all.apply(normalize)\n\ntest_final[\"question_body_all_clean\"] = test_final.question_body_all.apply(normalize)\ntest_final[\"answer_all_clean\"]  = test_final.answer_all.apply(normalize)","7ab90225":"from sklearn.decomposition import PCA\nfrom scipy.sparse import vstack\nfrom sklearn.feature_extraction.text import CountVectorizer","3f701468":"def text_metadata(train, test):\n    \n    a_AboutMe_text = train[\"a_AboutMe\"].apply(normalize)\n    q_AboutMe_text = train[\"q_AboutMe\"].apply(normalize)\n    all_text = pd.concat([a_AboutMe_text, q_AboutMe_text])\n\n    word_vectorizer = TfidfVectorizer(\n                sublinear_tf=True,\n                strip_accents='unicode',\n                analyzer='word',\n                token_pattern=r'\\w{1,}',\n                stop_words='english',\n                ngram_range=(1,1))\n    \n    word_vectorizer.fit(all_text)\n    \n    q_AboutMe_cols =  [f\"q_AboutMe_PCA_{i}\" for i in range(1,101)] \n    a_AboutMe_cols =  [f\"a_AboutMe_PCA_{i}\" for i in range(1,101)]\n    \n    q_AboutMe_text = word_vectorizer.transform(q_AboutMe_text)\n    a_AboutMe_text = word_vectorizer.transform(a_AboutMe_text)\n    \n    tq_AboutMe_text = word_vectorizer.transform(test[\"q_AboutMe\"].apply(normalize))\n    ta_AboutMe_text = word_vectorizer.transform(test[\"a_AboutMe\"].apply(normalize))\n    \n    new = vstack([q_AboutMe_text, a_AboutMe_text])\n    \n    pca = PCA(n_components=100)\n    pca.fit(new.toarray())\n    \n    q_AboutMe_text = pca.transform(q_AboutMe_text.toarray())\n    a_AboutMe_text = pca.transform(a_AboutMe_text.toarray())\n    \n    tq_AboutMe_text = pca.transform(tq_AboutMe_text.toarray())\n    ta_AboutMe_text = pca.transform(ta_AboutMe_text.toarray())\n    \n    \n    train[q_AboutMe_cols] = pd.DataFrame(q_AboutMe_text, columns=q_AboutMe_cols, index=train.index)\n    test[q_AboutMe_cols] = pd.DataFrame(tq_AboutMe_text, columns=q_AboutMe_cols, index=test.index)\n    \n    train[a_AboutMe_cols] = pd.DataFrame(a_AboutMe_text, columns=a_AboutMe_cols, index=train.index)\n    test[a_AboutMe_cols] = pd.DataFrame(ta_AboutMe_text, columns=a_AboutMe_cols, index=test.index)\n    \n    \n    # Tages\n    \n    tags_all_text = train[\"Tags\"]\n\n    word_vectorizer = CountVectorizer()\n    \n    word_vectorizer.fit(tags_all_text)\n    \n    tags_cols =  [\"Tags_\" + sub for sub in word_vectorizer.get_feature_names()] \n    \n    train[tags_cols] = pd.DataFrame(word_vectorizer.transform(train[\"Tags\"]).toarray(), columns=tags_cols, index=train.index)\n    test[tags_cols] = pd.DataFrame(word_vectorizer.transform(test[\"Tags\"]).toarray(), columns=tags_cols, index=test.index)\n    \n    \n    \n    return train, test, q_AboutMe_cols, a_AboutMe_cols, tags_cols\n    ","a7007b90":"%%time\n\ntrain_final, test_final, q_AboutMe_cols, a_AboutMe_cols, tags_cols = text_metadata(train_final, test_final)","e93effcd":"train_final.shape, test_final.shape","85142477":"def linear_based_models(train, test, cat_cols, num_cols):\n    \n    # Column std MinMax Scalling\n    std = MinMaxScaler()\n    train[num_cols] = std.fit_transform(train[num_cols])\n    test[num_cols] = std.transform(test[num_cols])\n    \n    # One Hot Encoder\n    train = pd.get_dummies(train, columns=cat_cols, prefix=cat_cols)\n    test = pd.get_dummies(test, columns=cat_cols, prefix=cat_cols)\n    \n    rem = list(set(train.columns).intersection(set(test.columns)))\n    \n    train = train[rem]\n    test = test[rem]\n    \n\n    return train, test\n\nnum_cols = ['AnswerCount', 'CommentCount', 'FavoriteCount','PostTypeId', 'Score','ViewCount','a_DownVotes','a_UpVotes', 'a_Views','q_DownVotes', 'q_UpVotes', 'q_Views']\ncat_cols = ['category', 'category_type']\ntrain_final, test_final = linear_based_models(train_final, test_final, cat_cols, num_cols)","ec90b8e3":"train_final.shape, test_final.shape","6fb8f1f2":"train_final = pd.merge(train_final, train_final_targets, left_on=\"qa_id\", right_on=\"qa_id\", how=\"left\")","a99036e2":"train_final.shape, test_final.shape","7a8a013c":"train_df = train_final\ntest_df = test_final\n\ntarget_columns = train_df.columns.values.tolist()[-30:]","44ac1797":"category_features = [col for col in train_df.columns if col.startswith('category_')]\ntags_cols = [col for col in train_df.columns if col.startswith('Tags_')]\na_AboutMe_cols = [col for col in train_df.columns if col.startswith('a_AboutMe_')]\nq_AboutMe_cols = [col for col in train_df.columns if col.startswith('q_AboutMe_')]\nnum_cols = ['AnswerCount', 'CommentCount', 'FavoriteCount','PostTypeId', 'Score','ViewCount','a_DownVotes','a_UpVotes', 'a_Views','q_DownVotes', 'q_UpVotes', 'q_Views','AnswerCount_nan', 'CommentCount_nan', 'FavoriteCount_nan', 'Score_nan','ViewCount_nan','a_DownVotes_nan','a_UpVotes_nan', 'a_Views_nan','q_DownVotes_nan', 'q_UpVotes_nan', 'q_Views_nan']","0d6ee77b":"a_AboutMe_cols = ['a_AboutMe_PCA_64',\n 'a_AboutMe_PCA_19',\n 'a_AboutMe_PCA_40',\n 'a_AboutMe_PCA_6',\n 'a_AboutMe_PCA_77',\n 'a_AboutMe_PCA_35',\n 'a_AboutMe_PCA_100',\n 'a_AboutMe_PCA_57',\n 'a_AboutMe_PCA_76',\n 'a_AboutMe_PCA_16',\n 'a_AboutMe_PCA_67',\n 'a_AboutMe_PCA_36',\n 'a_AboutMe_PCA_25',\n 'a_AboutMe_PCA_26',\n 'a_AboutMe_PCA_7',\n 'a_AboutMe_PCA_50',\n 'a_AboutMe_PCA_32',\n 'a_AboutMe_PCA_60',\n 'a_AboutMe_PCA_54',\n 'a_AboutMe_PCA_84',\n 'a_AboutMe_PCA_66',\n 'a_AboutMe_PCA_88',\n 'a_AboutMe_PCA_61',\n 'a_AboutMe_PCA_23',\n 'a_AboutMe_PCA_37',\n 'a_AboutMe_PCA_1',\n 'a_AboutMe_PCA_21',\n 'a_AboutMe_PCA_20',\n 'a_AboutMe_PCA_55',\n 'a_AboutMe_PCA_86',\n 'a_AboutMe_PCA_2',\n 'a_AboutMe_PCA_3',\n 'a_AboutMe_PCA_99',\n 'a_AboutMe_PCA_18',\n 'a_AboutMe_PCA_78',\n 'a_AboutMe_PCA_51',\n 'a_AboutMe_PCA_53',\n 'a_AboutMe_PCA_96',\n 'a_AboutMe_PCA_15',\n 'a_AboutMe_PCA_11',\n 'a_AboutMe_PCA_89',\n 'a_AboutMe_PCA_82',\n 'a_AboutMe_PCA_13',\n 'a_AboutMe_PCA_44',\n 'a_AboutMe_PCA_28',\n 'a_AboutMe_PCA_41',\n 'a_AboutMe_PCA_68',\n 'a_AboutMe_PCA_42',\n 'a_AboutMe_PCA_27',\n 'a_AboutMe_PCA_73',\n 'a_AboutMe_PCA_95',\n 'a_AboutMe_PCA_85',\n 'a_AboutMe_PCA_49',\n 'a_AboutMe_PCA_33',\n 'a_AboutMe_PCA_48',\n 'a_AboutMe_PCA_59',\n 'a_AboutMe_PCA_46',\n 'a_AboutMe_PCA_65',\n 'a_AboutMe_PCA_75',\n 'a_AboutMe_PCA_63',\n 'a_AboutMe_PCA_4',\n 'a_AboutMe_PCA_52',\n 'a_AboutMe_PCA_5',\n 'a_AboutMe_PCA_17',\n 'a_AboutMe_PCA_92',\n 'a_AboutMe_PCA_47',\n 'a_AboutMe_PCA_80',\n 'a_AboutMe_PCA_14',\n 'a_AboutMe_PCA_98',\n 'a_AboutMe_PCA_34',\n 'a_AboutMe_PCA_83',\n 'a_AboutMe_PCA_58',\n 'a_AboutMe_PCA_94',\n 'a_AboutMe_PCA_69',\n 'a_AboutMe_PCA_45',\n 'a_AboutMe_PCA_31',\n 'a_AboutMe_PCA_91',\n 'a_AboutMe_PCA_12',\n 'a_AboutMe_PCA_70',\n 'a_AboutMe_PCA_8',\n 'a_AboutMe_PCA_39',\n 'a_AboutMe_PCA_74',\n 'a_AboutMe_PCA_43',\n 'a_AboutMe_PCA_62',\n 'a_AboutMe_PCA_10',\n 'a_AboutMe_PCA_9',\n 'a_AboutMe_PCA_22',\n 'a_AboutMe_PCA_30',\n 'a_AboutMe_PCA_24',\n 'a_AboutMe_PCA_87',\n 'a_AboutMe_PCA_79',\n 'a_AboutMe_PCA_81',\n 'a_AboutMe_PCA_90',\n 'a_AboutMe_PCA_93',\n 'a_AboutMe_PCA_38',\n 'a_AboutMe_PCA_72',\n 'a_AboutMe_PCA_29',\n 'a_AboutMe_PCA_56',\n 'a_AboutMe_PCA_97',\n 'a_AboutMe_PCA_71']\n\nq_AboutMe_cols = ['q_AboutMe_PCA_93',\n 'q_AboutMe_PCA_50',\n 'q_AboutMe_PCA_70',\n 'q_AboutMe_PCA_65',\n 'q_AboutMe_PCA_85',\n 'q_AboutMe_PCA_71',\n 'q_AboutMe_PCA_18',\n 'q_AboutMe_PCA_69',\n 'q_AboutMe_PCA_51',\n 'q_AboutMe_PCA_79',\n 'q_AboutMe_PCA_31',\n 'q_AboutMe_PCA_99',\n 'q_AboutMe_PCA_40',\n 'q_AboutMe_PCA_92',\n 'q_AboutMe_PCA_86',\n 'q_AboutMe_PCA_34',\n 'q_AboutMe_PCA_2',\n 'q_AboutMe_PCA_64',\n 'q_AboutMe_PCA_1',\n 'q_AboutMe_PCA_72',\n 'q_AboutMe_PCA_32',\n 'q_AboutMe_PCA_29',\n 'q_AboutMe_PCA_5',\n 'q_AboutMe_PCA_7',\n 'q_AboutMe_PCA_67',\n 'q_AboutMe_PCA_96',\n 'q_AboutMe_PCA_82',\n 'q_AboutMe_PCA_35',\n 'q_AboutMe_PCA_55',\n 'q_AboutMe_PCA_39',\n 'q_AboutMe_PCA_27',\n 'q_AboutMe_PCA_4',\n 'q_AboutMe_PCA_66',\n 'q_AboutMe_PCA_57',\n 'q_AboutMe_PCA_38',\n 'q_AboutMe_PCA_12',\n 'q_AboutMe_PCA_76',\n 'q_AboutMe_PCA_20',\n 'q_AboutMe_PCA_89',\n 'q_AboutMe_PCA_28',\n 'q_AboutMe_PCA_30',\n 'q_AboutMe_PCA_98',\n 'q_AboutMe_PCA_100',\n 'q_AboutMe_PCA_61',\n 'q_AboutMe_PCA_3',\n 'q_AboutMe_PCA_37',\n 'q_AboutMe_PCA_81',\n 'q_AboutMe_PCA_97',\n 'q_AboutMe_PCA_49',\n 'q_AboutMe_PCA_91',\n 'q_AboutMe_PCA_43',\n 'q_AboutMe_PCA_90',\n 'q_AboutMe_PCA_94',\n 'q_AboutMe_PCA_58',\n 'q_AboutMe_PCA_36',\n 'q_AboutMe_PCA_8',\n 'q_AboutMe_PCA_46',\n 'q_AboutMe_PCA_25',\n 'q_AboutMe_PCA_13',\n 'q_AboutMe_PCA_10',\n 'q_AboutMe_PCA_87',\n 'q_AboutMe_PCA_21',\n 'q_AboutMe_PCA_62',\n 'q_AboutMe_PCA_11',\n 'q_AboutMe_PCA_42',\n 'q_AboutMe_PCA_33',\n 'q_AboutMe_PCA_74',\n 'q_AboutMe_PCA_26',\n 'q_AboutMe_PCA_6',\n 'q_AboutMe_PCA_68',\n 'q_AboutMe_PCA_54',\n 'q_AboutMe_PCA_73',\n 'q_AboutMe_PCA_17',\n 'q_AboutMe_PCA_44',\n 'q_AboutMe_PCA_52',\n 'q_AboutMe_PCA_47',\n 'q_AboutMe_PCA_56',\n 'q_AboutMe_PCA_15',\n 'q_AboutMe_PCA_59',\n 'q_AboutMe_PCA_88',\n 'q_AboutMe_PCA_22',\n 'q_AboutMe_PCA_77',\n 'q_AboutMe_PCA_84',\n 'q_AboutMe_PCA_75',\n 'q_AboutMe_PCA_63',\n 'q_AboutMe_PCA_60',\n 'q_AboutMe_PCA_41',\n 'q_AboutMe_PCA_53',\n 'q_AboutMe_PCA_45',\n 'q_AboutMe_PCA_14',\n 'q_AboutMe_PCA_16',\n 'q_AboutMe_PCA_24',\n 'q_AboutMe_PCA_19',\n 'q_AboutMe_PCA_83',\n 'q_AboutMe_PCA_9',\n 'q_AboutMe_PCA_48',\n 'q_AboutMe_PCA_23',\n 'q_AboutMe_PCA_78',\n 'q_AboutMe_PCA_95',\n 'q_AboutMe_PCA_80']","f76b31b0":"meta_cols = [*category_features, *num_cols]","1b9e5724":"len(meta_cols)","af7b34db":"def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_lenght):\n    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n\n    def return_id(str1, str2, lenght):\n\n        inputs = tokenizer.encode_plus(\n            str1, str2,\n            add_special_tokens = True,\n            max_length = lenght,\n            pad_to_max_length = True,\n            return_token_type_ids = True,\n            return_attention_mask = True,\n            truncation_strategy = \"longest_first\"\n        )\n\n        input_ids = inputs[\"input_ids\"]\n        attention_mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n\n        return [input_ids, attention_mask, token_type_ids]\n\n\n    input_ids_q , attention_mask_q, token_type_ids_q = return_id(title, question, max_sequence_lenght)\n    input_ids_a , attention_mask_a, token_type_ids_a = return_id(answer,None, max_sequence_lenght)\n\n    return [\n            input_ids_q , attention_mask_q, token_type_ids_q,\n            input_ids_a , attention_mask_a, token_type_ids_a\n    ]","f18c9893":"def compute_input_arrays(df, columns, meta_cols, q_AboutMe_cols, a_AboutMe_cols, tokenizer, max_sequence_length):\n    input_ids_q , attention_mask_q, token_type_ids_q = [], [], []\n    input_ids_a , attention_mask_a, token_type_ids_a = [], [], []\n    meta_features = []\n    q_AboutMe_features = []\n    a_AboutMe_features = []\n\n    total_cols = [*columns, *meta_cols, *q_AboutMe_cols, *a_AboutMe_cols]\n\n    #i = 0\n\n    for _, instance in tqdm(df[total_cols].iterrows()):\n\n    \n        t, q, a, qc, ac = instance.question_title_clean2, instance.question_body_clean2, instance.answer_clean2,instance.question_body_all_clean, instance.answer_all_clean\n        t = str(t) \n        q = str(q) + \" [SEP] \"+ str(qc)\n        a = str(a) + \" [SEP] \"+ str(ac)\n\n        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n\n        input_ids_q.append(ids_q)\n        attention_mask_q.append(masks_q)\n        token_type_ids_q.append(segments_q)\n\n        input_ids_a.append(ids_a)\n        attention_mask_a.append(masks_a)\n        token_type_ids_a.append(segments_a)\n\n        meta_data = instance[meta_cols].values.tolist()\n        q_AboutMe_data = instance[q_AboutMe_cols].values.tolist()\n        a_AboutMe_data = instance[a_AboutMe_cols].values.tolist()\n\n        meta_features.append(meta_data)\n        q_AboutMe_features.append(q_AboutMe_data)\n        a_AboutMe_features.append(a_AboutMe_data)\n        #i = i+1\n        #if i == 100:\n        #    break\n\n    return [np.asarray(input_ids_q, dtype=np.int32), \n            np.asarray(attention_mask_q, dtype=np.int32), \n            np.asarray(token_type_ids_q, dtype=np.int32),\n            np.asarray(input_ids_a, dtype=np.int32), \n            np.asarray(attention_mask_a, dtype=np.int32), \n            np.asarray(token_type_ids_a, dtype=np.int32),\n            np.asarray(meta_features, dtype=np.float32),\n            np.asarray(q_AboutMe_features, dtype=np.float32),\n            np.asarray(a_AboutMe_features, dtype=np.float32)\n            ]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","be321bb7":"def create_model_soft(BERT_PATH):\n    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n\n    meta_features_layer = tf.keras.layers.Input((len(meta_cols),), dtype=tf.float32)\n    q_AboutMe_features_layer = tf.keras.layers.Input((len(q_AboutMe_cols),), dtype=tf.float32)\n    a_AboutMe_features_layer = tf.keras.layers.Input((len(a_AboutMe_cols),), dtype=tf.float32)\n    \n    config = BertConfig() # print(config) to see settings\n    config.output_hidden_states = False # Set to True to obtain hidden states\n    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n    \n    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n    # pretrained model has been downloaded manually and uploaded to kaggle. \n    bert_model = TFBertModel.from_pretrained(BERT_PATH+'bert-base-uncased-tf_model.h5',config=config )\n    \n    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    \n    x = tf.keras.layers.Concatenate()([q, q_AboutMe_features_layer, a, a_AboutMe_features_layer, meta_features_layer, ])\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x = tf.keras.layers.Dense(21, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn, meta_features_layer, q_AboutMe_features_layer, a_AboutMe_features_layer], outputs=x)\n    \n    return model","1a748382":"def create_model_hard(BERT_PATH):\n    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n\n    meta_features_layer = tf.keras.layers.Input((len(meta_cols),), dtype=tf.float32)\n    q_AboutMe_features_layer = tf.keras.layers.Input((len(q_AboutMe_cols),), dtype=tf.float32)\n    a_AboutMe_features_layer = tf.keras.layers.Input((len(a_AboutMe_cols),), dtype=tf.float32)\n    \n    config = BertConfig() # print(config) to see settings\n    config.output_hidden_states = False # Set to True to obtain hidden states\n    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n    \n    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n    # pretrained model has been downloaded manually and uploaded to kaggle. \n    bert_model = TFBertModel.from_pretrained(BERT_PATH+'bert-base-uncased-tf_model.h5',config=config )\n    \n    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    \n    x = tf.keras.layers.Concatenate()([q, q_AboutMe_features_layer, a, a_AboutMe_features_layer, meta_features_layer, ])\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x = tf.keras.layers.Dense(8, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn, meta_features_layer, q_AboutMe_features_layer, a_AboutMe_features_layer], outputs=x)\n    \n    return model","1ef3fdad":"input_categories = [\"question_title_clean2\", \"question_body_clean2\", \"answer_clean2\", \"question_body_all_clean\", \"answer_all_clean\"]\n\nMAX_SEQUENCE_LENGTH = 512","ce6b74a0":"BERT_PATH = '..\/input\/bert-base-uncased-huggingface-transformer\/'","c16d3007":"tokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')","7ac144db":"#outputs = compute_output_arrays(train_df, target_columns)\n#inputs = compute_input_arrays(train_df, input_categories,meta_cols, q_AboutMe_cols, a_AboutMe_cols ,tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arrays(test_df, input_categories,meta_cols,q_AboutMe_cols, a_AboutMe_cols , tokenizer, MAX_SEQUENCE_LENGTH)","d36c2c7b":"model_weights_path = '..\/input\/quest-bert-soft-hard-models\/'","745651e9":"soft_target_columns = [\n #'question_asker_intent_understanding',\n 'question_body_critical',\n 'question_conversational',\n 'question_expect_short_answer',\n 'question_fact_seeking',\n 'question_has_commonly_accepted_answer',\n 'question_interestingness_others',\n 'question_interestingness_self',\n 'question_multi_intent',\n #'question_not_really_a_question',\n 'question_opinion_seeking',\n 'question_type_choice',\n #'question_type_compare',\n #'question_type_consequence',\n 'question_type_definition',\n 'question_type_entity',\n 'question_type_instructions',\n 'question_type_procedure',\n 'question_type_reason_explanation',\n #'question_type_spelling',\n 'question_well_written',\n #'answer_helpful',\n 'answer_level_of_information',\n #'answer_plausible',\n #'answer_relevance',\n #'answer_satisfaction',\n 'answer_type_instructions',\n 'answer_type_procedure',\n 'answer_type_reason_explanation',\n 'answer_well_written']","0618753b":"soft_test_predictions = []\n\nfor i in range(5):\n    j = 1\n    model_path = f'{model_weights_path}bert-{i}-{j}.h5'\n    model1 = create_model_soft(BERT_PATH)\n    model1.load_weights(model_path)\n    soft_test_predictions.append(model1.predict(test_inputs, batch_size=2))","f979b6b5":"len(soft_test_predictions)","3f04ad5a":"soft_test_predictions[0].shape","e9ed8e6c":"soft_test_predictions = np.mean(soft_test_predictions, axis=0)","2e291f04":"soft_test_predictions.shape","53e81759":"hard_target_columns  = [\n'question_asker_intent_understanding',\n#  'question_body_critical',\n#  'question_conversational',\n#  'question_expect_short_answer',\n#  'question_fact_seeking',\n#  'question_has_commonly_accepted_answer',\n#  'question_interestingness_others',\n#  'question_interestingness_self',\n#  'question_multi_intent',\n'question_not_really_a_question',\n#  'question_opinion_seeking',\n#  'question_type_choice',\n'question_type_compare',\n'question_type_consequence',\n#  'question_type_definition',\n#  'question_type_entity',\n#  'question_type_instructions',\n#  'question_type_procedure',\n#  'question_type_reason_explanation',\n# 'question_type_spelling',\n#  'question_well_written',\n'answer_helpful',\n#  'answer_level_of_information',\n'answer_plausible',\n'answer_relevance',\n'answer_satisfaction',\n#  'answer_type_instructions',\n#  'answer_type_procedure',\n#  'answer_type_reason_explanation',\n#  'answer_well_written'\n ]\n\nhard_test_predictions = []\n\nfor i in range(1,5):\n    print(i)\n    model_path = f'{model_weights_path}hard-bert-{i}.h5'\n    model1 = create_model_hard(BERT_PATH)\n    model1.load_weights(model_path)\n    hard_test_predictions.append(model1.predict(test_inputs, batch_size=2))","5b025f21":"hard_test_predictions[0].shape","0461279f":"hard_test_predictions = np.mean(hard_test_predictions, axis=0)","09ed98fb":"hard_test_predictions.shape","d75ee598":"sample_submission[soft_target_columns] = soft_test_predictions","162631c3":"sample_submission[hard_target_columns] = hard_test_predictions","bd05bd23":"def question_type_spelling_hard(test):\n    if test[\"category_type_english\"] == 1 or test[\"category_type_ell\"] == 1:\n        \n        if test[\"Tags_pronunciation\"] == 1:\n            return 0.666667\n        elif test[\"Tags_spelling\"] == 1:\n            return 0.666667\n        else:\n            return 0.555555\n    else:\n        return 0.00000","200c47c7":"sample_submission[\"question_type_spelling\"] = test_df.apply(question_type_spelling_hard, 1)","3ddfe940":"def question_type_compare_hard(text):\n    if text == np.nan:\n        return 0.00000\n    else:\n        text = str(text)\n        ls = text.split(\" \")\n        if \"vs\" in ls:\n            return 1.000000\n        elif (\"between\" or \"difference\") in ls:\n            return 0.666667\n        elif (\"means\" or \"better\") in ls:\n            return 0.333333\n        else:\n            return 0.000000","d673cc3f":"#sample_submission[\"question_type_compare\"] = test_df.question_title_clean2.apply(question_type_compare_hard)","b2d521a8":"sample_submission.nunique(axis=0)","be516773":"sample_submission.isna().sum().sum()","a9ee9c19":"sample_submission.fillna(0.5, inplace=True)","f74d1132":"sample_submission.iloc[:,1:].max().max()","ef8487ec":"sample_submission.iloc[:,1:].min().min()","d5fd0cd1":"#sample_submission.to_csv('submission.csv', index=False)","cd57d7d3":"sample_submission.head()","5f2393f7":"sample_submission.to_csv('submission.csv', index=False)","ef4a088c":"TARGET_COLUMNS = target_columns","976d9110":"TARGET_COLUMNS","5cd5ec78":"\"\"\"\n\nfrom sklearn.preprocessing import MinMaxScaler\n    \ndef postprocessing(oof_df):\n   \n    scaler = MinMaxScaler()\n    \n    # type 1 column [0, 0.333333, 0.5, 0.666667, 1]\n    # type 2 column [0, 0.333333, 0.666667]\n    # type 3 column [0.333333, 0.444444, 0.5, 0.555556, 0.666667, 0.777778, 0.8333333, 0.888889, 1]\n    # type 4 column [0.200000, 0.266667, 0.300000, 0.333333, 0.400000, \\\n    # 0.466667, 0.5, 0.533333, 0.600000, 0.666667, 0.700000, \\\n    # 0.733333, 0.800000, 0.866667, 0.900000, 0.933333, 1]\n    \n    # comment some columns based on oof result\n    \n    ################################################# handle type 1 columns\n    type_one_column_list = [\n       'question_conversational', \\\n       'question_has_commonly_accepted_answer', \\\n       'question_not_really_a_question', \\\n       'question_type_choice', \\\n       'question_type_compare', \\\n       'question_type_consequence', \\\n       'question_type_definition', \\\n       'question_type_entity', \\\n       'question_type_instructions', \n    ]\n    \n    oof_df[type_one_column_list] = scaler.fit_transform(oof_df[type_one_column_list])\n    \n    tmp = oof_df.copy(deep=True)\n    \n    for column in type_one_column_list:\n        \n        oof_df.loc[tmp[column] <= 0.16667, column] = 0\n        oof_df.loc[(tmp[column] > 0.16667) & (tmp[column] <= 0.41667), column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.41667) & (tmp[column] <= 0.58333), column] = 0.500000\n        oof_df.loc[(tmp[column] > 0.58333) & (tmp[column] <= 0.73333), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.73333), column] = 1\n    \n    \n    \n    ################################################# handle type 2 columns      \n#     type_two_column_list = [\n#         'question_type_spelling'\n#     ]\n    \n#     for column in type_two_column_list:\n#         if sum(tmp[column] > 0.15)>0:\n#             oof_df.loc[tmp[column] <= 0.15, column] = 0\n#             oof_df.loc[(tmp[column] > 0.15) & (tmp[column] <= 0.45), column] = 0.333333\n#             oof_df.loc[(tmp[column] > 0.45), column] = 0.666667\n#         else:\n#             t1 = max(int(len(tmp[column])*0.0013),2)\n#             t2 = max(int(len(tmp[column])*0.0008),1)\n#             thred1 = sorted(list(tmp[column]))[-t1]\n#             thred2 = sorted(list(tmp[column]))[-t2]\n#             oof_df.loc[tmp[column] <= thred1, column] = 0\n#             oof_df.loc[(tmp[column] > thred1) & (tmp[column] <= thred2), column] = 0.333333\n#             oof_df.loc[(tmp[column] > thred2), column] = 0.666667\n    \n    \n    \n    ################################################# handle type 3 columns      \n    type_three_column_list = [\n       'question_interestingness_self', \n    ]\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    oof_df[type_three_column_list] = scaler.fit_transform(oof_df[type_three_column_list])\n    tmp[type_three_column_list] = scaler.fit_transform(tmp[type_three_column_list])\n    \n    for column in type_three_column_list:\n        oof_df.loc[tmp[column] <= 0.385, column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.385) & (tmp[column] <= 0.47), column] = 0.444444\n        oof_df.loc[(tmp[column] > 0.47) & (tmp[column] <= 0.525), column] = 0.5\n        oof_df.loc[(tmp[column] > 0.525) & (tmp[column] <= 0.605), column] = 0.555556\n        oof_df.loc[(tmp[column] > 0.605) & (tmp[column] <= 0.715), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.715) & (tmp[column] <= 0.8), column] = 0.833333\n        oof_df.loc[(tmp[column] > 0.8) & (tmp[column] <= 0.94), column] = 0.888889\n        oof_df.loc[(tmp[column] > 0.94), column] = 1\n        \n        \n        \n    ################################################# handle type 4 columns      \n    type_four_column_list = [\n        'answer_satisfaction'\n    ]\n    scaler = MinMaxScaler(feature_range=(0.2, 1))\n    oof_df[type_four_column_list] = scaler.fit_transform(oof_df[type_four_column_list])\n    tmp[type_four_column_list] = scaler.fit_transform(tmp[type_four_column_list])\n    \n    for column in type_four_column_list:\n        \n        oof_df.loc[tmp[column] <= 0.233, column] = 0.200000\n        oof_df.loc[(tmp[column] > 0.233) & (tmp[column] <= 0.283), column] = 0.266667\n        oof_df.loc[(tmp[column] > 0.283) & (tmp[column] <= 0.315), column] = 0.300000\n        oof_df.loc[(tmp[column] > 0.315) & (tmp[column] <= 0.365), column] = 0.333333\n        oof_df.loc[(tmp[column] > 0.365) & (tmp[column] <= 0.433), column] = 0.400000\n        oof_df.loc[(tmp[column] > 0.433) & (tmp[column] <= 0.483), column] = 0.466667\n        oof_df.loc[(tmp[column] > 0.483) & (tmp[column] <= 0.517), column] = 0.500000\n        oof_df.loc[(tmp[column] > 0.517) & (tmp[column] <= 0.567), column] = 0.533333\n        oof_df.loc[(tmp[column] > 0.567) & (tmp[column] <= 0.633), column] = 0.600000\n        oof_df.loc[(tmp[column] > 0.633) & (tmp[column] <= 0.683), column] = 0.666667\n        oof_df.loc[(tmp[column] > 0.683) & (tmp[column] <= 0.715), column] = 0.700000\n        oof_df.loc[(tmp[column] > 0.715) & (tmp[column] <= 0.767), column] = 0.733333\n        oof_df.loc[(tmp[column] > 0.767) & (tmp[column] <= 0.833), column] = 0.800000\n        oof_df.loc[(tmp[column] > 0.883) & (tmp[column] <= 0.915), column] = 0.900000\n        oof_df.loc[(tmp[column] > 0.915) & (tmp[column] <= 0.967), column] = 0.933333\n        oof_df.loc[(tmp[column] > 0.967), column] = 1\n    \n    \n    ################################################# round to i \/ 90 (i from 0 to 90)\n    oof_values = oof_df[TARGET_COLUMNS].values\n    DEGREE = len(oof_df)\/\/45*9\n#     if degree:\n#         DEGREE = degree\n#     DEGREE = 90\n    oof_values = np.around(oof_values * DEGREE) \/ DEGREE  ### 90 To be changed\n    oof_df[TARGET_COLUMNS] = oof_values\n    \n    return oof_df\n\"\"\"","cfef9097":"#sample_submission_post = postprocessing(sample_submission)","2576d653":"#sample_submission_post.shape","6f019326":"#for column in TARGET_COLUMNS:\n#    print(sample_submission_post[column].value_counts())","ec29443d":"#sample_submission_post","297f262a":"#sample_submission_post","34b551bc":"#sample_submission_post.to_csv('submission.csv', index=False)","97db9d39":"## Text cleaning","1abd806a":"## META DATA","8fa423a4":"# Model"}}