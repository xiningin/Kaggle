{"cell_type":{"2eb3b421":"code","4e7bbf71":"code","1e76d5b7":"code","c9fcee5c":"code","484dbe33":"code","de7bdfbd":"code","ac55eaf6":"code","51fc0471":"code","5322d8a4":"code","d432d592":"code","447d7c57":"code","b939847c":"code","215e6909":"code","a19e90d7":"code","4a6ad08f":"code","29dc557a":"code","d2c04a20":"code","62f248b1":"code","d32b1fe4":"code","dd12758f":"code","d3d25474":"code","37ce6bb3":"code","c47fd725":"code","b7362054":"code","128daf14":"code","a4edacbd":"code","98a3426e":"code","41c0992f":"code","8b456bc9":"code","2cfe717d":"code","d14316ef":"code","cccda3c5":"code","d8eb45bd":"code","d6759f4d":"code","bfb762e1":"code","f0aa45aa":"code","ec05e79f":"code","d0c8a116":"code","ce2e1f3b":"code","1dfd0fae":"code","cb7dbacc":"code","0bad6a8a":"code","8df2786f":"markdown","c6a2b770":"markdown","7be54abe":"markdown","f63445ec":"markdown","5c793e96":"markdown","b767d2d0":"markdown","c941eac6":"markdown","d8ac803d":"markdown","d587ba89":"markdown","e8f7d73d":"markdown","423bedb3":"markdown","50f8db61":"markdown","d49664d3":"markdown","4001751d":"markdown","9ea24a37":"markdown","c0849490":"markdown","f4ab92de":"markdown","a07c91f4":"markdown","39fe9068":"markdown","10f10116":"markdown","8c3a97c6":"markdown","eb0eb55f":"markdown","3527e139":"markdown","3834efc7":"markdown","8be5d7fd":"markdown","dfbc3b33":"markdown","d9bad47c":"markdown","ca363780":"markdown","873cbfa1":"markdown","deb4370f":"markdown","014f2853":"markdown","5654cdb2":"markdown","3ca9f694":"markdown","0b9664df":"markdown","8f185fae":"markdown","fa4f0d30":"markdown","61df3eba":"markdown","4ca4cc3e":"markdown","89a2af90":"markdown","dd67b653":"markdown","b138f668":"markdown","776ea1c4":"markdown","01c762e0":"markdown","afa39ed5":"markdown","910ea2dd":"markdown","f6f5f47e":"markdown","160376c6":"markdown","a96a5be3":"markdown"},"source":{"2eb3b421":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom lightgbm import LGBMClassifier\nfrom optuna.samplers import TPESampler\nimport optuna.integration.lightgbm as lightgbm\nfrom sklearn.metrics import roc_auc_score\nimport optuna\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","4e7bbf71":"data = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\ndata.dropna(subset=['RainTomorrow'], inplace=True)\ndata['RainTomorrow'] = pd.factorize(data['RainTomorrow'])[0]\ntarget = data['RainTomorrow']\ndata = data.drop(['RainTomorrow'], axis=1)","1e76d5b7":"data.info()","c9fcee5c":"cat = ['Location', \n       'WindGustDir',\n       'WindDir9am',\n       'WindDir3pm',\n       'RainToday',\n       'Date']       \ndig = [i for i in list(data) if i not in cat]","484dbe33":"percent_nan = data.isna().sum()\/data.shape[0]\npercent_nan","de7bdfbd":"for ind, d in zip(percent_nan.index, percent_nan):\n  if ind in dig and d != 0:\n    data[ind].fillna(data[ind].mean(), inplace=True)\n\ndata.fillna(method='ffill', inplace=True)","ac55eaf6":"for c in cat:\n  data['{}_dig'.format(c)] = pd.factorize(data[c])[0]","51fc0471":"cat_dig = ['{}_dig'.format(c) for c in cat]\nfeatures = dig + cat_dig","5322d8a4":"data[features].hist(figsize=(20, 15), bins=20)","d432d592":"data[features].corrwith(target)","447d7c57":"sns.pairplot(data[dig])","b939847c":"drop_features = []\n\nfor feature, corr in zip(data[features].corrwith(target).index, data[features].corrwith(target).values):\n    if np.abs(corr) < 0.05:\n        drop_features.append(feature)","215e6909":"for d in dig:\n  data[d] = (data[d] - data[d].mean()) \/ data[d].std()","a19e90d7":"data[dig].hist(figsize=(20, 15), bins=20)","4a6ad08f":"new_features = [feature for feature in features if feature not in drop_features]","29dc557a":"xtrain, xtest, ytrain, ytest = train_test_split(\n    data[new_features], target, random_state=100, test_size=0.2, shuffle=False) # shuffle False because data is time series.","d2c04a20":"clf = xgb.XGBClassifier(seed=123)\nclf.fit(xtrain, ytrain)\nclf.score(xtest, ytest)","62f248b1":"xtrain_old, xtest_old, ytrain_old, ytest_old = train_test_split(\n    data[features], target, random_state=100, test_size=0.2, shuffle=False)\n\nclf_old = xgb.XGBClassifier(seed=123)\nclf_old.fit(xtrain_old, ytrain_old)\nclf_old.score(xtest_old, ytest_old)","d32b1fe4":"lgb_model = LGBMClassifier(random_state=123)\nlgb_model.fit(xtrain, ytrain)\nlgb_model.score(xtest, ytest)","dd12758f":"lgb_train = lightgbm.Dataset(xtrain, ytrain)\nlgb_eval = lightgbm.Dataset(xtest, ytest)\n\ndef create_model(trial):\n    params = {\n            'num_leaves': trial.suggest_int('num_leaves', 32, 512),\n            'boosting_type': 'gbdt',\n            'objective': 'binary',\n            'metric': 'auc',\n            'learning_rate': trial.suggest_uniform('learning_rate', 0.05, 0.5),\n            'max_depth': trial.suggest_int('max_depth', 3, 18),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n            'bagging_freq': trial.suggest_int('bagging_freq', 1, 8),\n            'min_child_samples': trial.suggest_int('min_child_samples', 4, 80),\n            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n            'n_estimators': trial.suggest_int('n_estimators', 200, 600),\n            'random_state': 123\n        }\n    model = LGBMClassifier(**params)\n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(xtrain, ytrain)\n    preds = model.predict_proba(xtest)[:, 1]\n    score = roc_auc_score(ytest, preds)\n    return score","d3d25474":"sampler = TPESampler(seed=123)\nstudy = optuna.create_study(direction='maximize', sampler=sampler)\nstudy.optimize(objective, n_trials=100) # you can increase n_trials number for better result ","37ce6bb3":"params = study.best_params \nprint(params)","c47fd725":"new_lgb = LGBMClassifier(**params)\nnew_lgb.fit(xtrain, ytrain)\nnew_lgb.score(xtest, ytest)","b7362054":"select_model = LGBMClassifier(**params)\nselector = RFECV(select_model, step=1, cv=5, verbose=10, min_features_to_select=6)\nselector.fit(xtrain, ytrain)","128daf14":"selector.ranking_","a4edacbd":"final_features = [new_features[i] for i in range(len(selector.support_)) if selector.support_[i] == True]","98a3426e":"xtrain_final, xtest_final, ytrain_final, ytest_final = train_test_split(\n    data[final_features], target, random_state=100, test_size=0.3, shuffle=False)\n\nfinal_lgb = LGBMClassifier(**params)\nfinal_lgb.fit(xtrain_final, ytrain_final)\nfinal_lgb.score(xtest_final, ytest_final)","41c0992f":"k = KNeighborsClassifier(n_neighbors=7)\ng = GaussianNB()\nrf = RandomForestClassifier()\n\nestimators = [\n    ('k', k), ('g', g), ('l', final_lgb), ('rf', rf)\n]","8b456bc9":"k.fit(xtrain, ytrain)\nk.score(xtest, ytest)","2cfe717d":"g.fit(xtrain, ytrain)\ng.score(xtest, ytest)","d14316ef":"rf.fit(xtrain, ytrain)\nrf.score(xtest, ytest)","cccda3c5":"vote = VotingClassifier(voting='hard', estimators=estimators)\nvote.fit(xtrain, ytrain)\nvote.score(xtest, ytest)","d8eb45bd":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras import models\nfrom keras import layers\nfrom keras.optimizers import Adam\nimport numpy as np","d6759f4d":"device_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","bfb762e1":"k = 30 # for best result, you may increase k value\n\nr_xtrain = [xtrain.iloc[i:i+k] for i in range(xtrain.shape[0]-k-1)]\nr_ytrain = [ytrain.iloc[i+k] for i in range(ytrain.shape[0]-k-1)]\n\nr_xtest = [xtest.iloc[i:i+k] for i in range(xtest.shape[0]-k-1)]\nr_ytest = [ytest.iloc[i+k] for i in range(xtest.shape[0]-k-1)]\n\nr_xtrain = np.array(r_xtrain)\nr_ytrain = np.array(r_ytrain)\n\nr_xtest = np.array(r_xtest)\nr_ytest = np.array(r_ytest)","f0aa45aa":"with tf.device('\/cpu:0'):\n    model = models.Sequential()\n\n    model.add(layers.GRU(32, input_shape=(None, r_xtrain.shape[-1]), recurrent_dropout=0.2))\n    model.add(layers.Dense(1))\n    model.compile(optimizer=Adam(amsgrad=True), loss='mse', metrics='accuracy')\n    history = model.fit(x=r_xtrain, \n                        y=r_ytrain,\n                        epochs=15,\n                        validation_data=(r_xtest, r_ytest)\n                        )","ec05e79f":"def history_plt(history):\n    loss = history.history['loss']\n    acc = history.history['accuracy']\n    val_loss = history.history['val_loss']\n    val_acc = history.history['val_accuracy']\n\n    epochs = range(1, len(loss) + 1)\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n    ax[0].plot(epochs, acc, label='acc', color='b')\n    ax[0].plot(epochs, val_acc, label='val_acc', color='r')\n    ax[0].legend()\n    ax[1].plot(epochs, loss, label='loss', color='b')\n    ax[1].plot(epochs, val_loss, label='val_loss', color='r')\n    ax[1].legend()\n    plt.show()","d0c8a116":"history_plt(history)","ce2e1f3b":"np.max(history.history['val_accuracy'])","1dfd0fae":"with tf.device('\/cpu:0'):\n    model = models.Sequential()\n\n    model.add(layers.Conv1D(16, 3, input_shape=(None, r_xtrain.shape[-1]), activation='relu', kernel_regularizer='l1_l2'))\n    model.add(layers.Conv1D(32, 3, activation='relu', kernel_regularizer='l1_l2'))\n\n    model.add(layers.BatchNormalization())\n    model.add(layers.MaxPool1D())\n    \n    model.add(layers.Conv1D(64, 3, activation='relu', kernel_regularizer='l1_l2'))\n    model.add(layers.Conv1D(128, 3, activation='relu', kernel_regularizer='l1_l2'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.MaxPool1D())\n    \n    model.add(layers.Dense(1))\n    model.compile(optimizer=Adam(amsgrad=True), loss='mse', metrics='accuracy')\n    history = model.fit(x=r_xtrain, \n                        y=r_ytrain,\n                        epochs=15,\n                        validation_data=(r_xtest, r_ytest)\n                        )","cb7dbacc":"history_plt(history)","0bad6a8a":"np.max(history.history['val_accuracy'])","8df2786f":"We can't increase our result by tuning parameters. But difference is really small.","c6a2b770":"Before select the models, create some new features or chose them from existing, we need to analysis the data we got. Lets make it consistently.","7be54abe":"Further, combine them.","f63445ec":"Lets create two list with categorical and not features.\n\n***object* - means non digital format**\n\n***float64* - digital**","5c793e96":"## 2.3 Data standartization","b767d2d0":"Looking at the plots, we can continue train further to achieve better results.","c941eac6":"Also we can go through feature selection procedure. \n\n* Create parameters dictionary\n* Using optuna we choose the better one combination of them","d8ac803d":"## 2.1 Data reading and primary processing","d587ba89":"### In conclusion, I want to say there are many means and opportunities to improve the quality of the model, and each of them is effective in some specific task.","e8f7d73d":"This in cell for activate check gpu's availability.","423bedb3":"Plots the history process.","50f8db61":"To find out insights in our data, lets visualize them.","d49664d3":"Further, we will work with *Nan* values.","4001751d":"Let's train the model for the last time and see the final result.","9ea24a37":"## 2. Exploratory data analysis ","c0849490":"Lets create a simple 1-layer RNN.","f4ab92de":"## 4. Ensemble of models","a07c91f4":"Look at feature importanse:","39fe9068":"The next step is chose and prepare model for predict target.\n\nFirstly i will take the best algorithms for kaggle competitors - XGBCLassifier().","10f10116":"## 1. Importing tools\n\nFirstly, lets import all tools we need.","8c3a97c6":"### Good luck to all!","eb0eb55f":"## 3. Model selection","3527e139":"## 5. Recurrent Neural Network (RNN)","3834efc7":"All machine learning algorithms prefer data with same scale, so lets subtract from each column it mean value and devide by standard deviation.","8be5d7fd":"## 3.1 Parameters selection","dfbc3b33":"Lets check all of these models.","d9bad47c":"For reccurent nn we need to prepare sequences of data, where k - length of sequence in days.","ca363780":"## 3.2 Features selection","873cbfa1":"### That's all.","deb4370f":"We will change the not a number value in digit columns for their mean, cat - nearest value in the same column.","014f2853":"In the follow cells we read the data and subsets them on targets table, which self-explanatory, and on another data, with whom we would treaking.\n\nAlso, before subset we drop all unknown target values and factorize it on 0 if tomorrow none rainy day and 1 otherwise.","5654cdb2":"As we can see, training ensemble of different models also didnt give improve in this particular problem.","3ca9f694":"So, if we look on new digital columns distribution, we can see, that each mean value is equal to zero and std is equal to unit.","0b9664df":"But i will choose *LGBMClassifier*, cause it more lighter, faster and accurate than *XGBClassifier*.","8f185fae":"## 6. Convolutional Neural Network (CNN)","fa4f0d30":"As we can see, categorical columns mostly had uniform distribution.\n\nDigital columns had nearly bell-shaped distribution, but some of them was heavy-tail.","61df3eba":"From this matrix we can take information about features with smallest or highest negative\/positive correlation with target variable. We may drop som of them, like *'Location_dig', 'Temp9am'* and so on.","4ca4cc3e":"As we can see, result dose not improve. So we come back to previous features set.","89a2af90":"## 2.2 Data visualization","dd67b653":"Using this plot, we can see, that some of time based features had a high positive correlation with themselfs. So, we may want drop one of them from each pair.","b138f668":"Let's create 3 any different models (of your choice).\n\nAnd list of tupple to them.","776ea1c4":"# **Rain in Australia**\n## **Predict next-day rain in Australia**","01c762e0":"Separate data for train and test subsets. Also i create the new features list without *drop_features*","afa39ed5":"Not bad for first result. You can compare it with old features list, may be more is better?","910ea2dd":"Later we manually selected the features, which seemed promising for us.\n\nLet's give this job for RFECV algorithm.","f6f5f47e":"Our next step is factorize the cat. features list to digit format.","160376c6":"I will use cap equal to 0.05. It just heuristic and you may save them in data. Different decision can give different advantages, for example, the fewer features, the higher the models speed and vice versa.","a96a5be3":"    * XGBClassifier with all parameters: 0.8624\n    * XGBClassifier without some dropped parameters: 0.8483\n    * LGBMClassifier: 0.8643\n    * LGBMClassifier with parameters tuning: 0.8642\n    * LGBMClassifier with parameters tuning and feature selection: 0.8587\n    * RandomForestClassifier: 0.8607\n    * KNeighborsClassifier: 0.8435\n    * GaussianNB: 0.8399\n    * Ensemble of [RFC, KNC, GNB, LGBM]: 0.8611\n    * Reccurent Neural Network: 0.8127\n    * Convolutional Neural Network: 0.7979"}}