{"cell_type":{"f2180319":"code","2b1cc4af":"code","86fd0049":"code","232edd5f":"code","cd0b0b6b":"code","ae0f627b":"code","9cbb1b2e":"code","1c72a038":"code","de7ee583":"code","03b8fbfc":"code","92d2469c":"code","7b59bba8":"code","56822bea":"code","46a16c98":"code","45b3252e":"code","9e01c113":"code","6ded57d1":"code","4da20c11":"code","6ae3b31f":"code","263a9c09":"code","55e23493":"code","cf5ecf7d":"code","a68f3df2":"code","5a183209":"code","5a9f67b8":"code","0642ced9":"code","717408f7":"code","70b032c9":"code","063028ec":"code","78e55e62":"code","810da173":"markdown","2855c1bd":"markdown","f0b15a9a":"markdown","e297c964":"markdown","3f2de4ef":"markdown","83bb2e80":"markdown","2db9dd9c":"markdown","a46ddd9e":"markdown","d90cc4d8":"markdown","24789df4":"markdown","b3b6b7b4":"markdown","04007ab3":"markdown","df0e1d7c":"markdown","603520c5":"markdown","be80778f":"markdown","e1b2afb8":"markdown","49db443b":"markdown","9c3405e5":"markdown","0c308ca1":"markdown","c4213115":"markdown"},"source":{"f2180319":"import numpy as np # linear algebra\nimport datatable as dt\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', None)\nimport matplotlib.pyplot as plt # for visualization \nimport seaborn as sns\nimport plotly.graph_objects as go # can resize graphs\nimport plotly.express as px # for graphing volumes traded\nimport gc # to collect garbage\nimport traceback\nimport gresearch_crypto # required to use their API\n\nimport pandas_datareader as web # Getting web data\n\nimport datetime\nimport time\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, RobustScaler\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import r2_score\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom catboost import CatBoostRegressor, Pool\nfrom lightgbm import LGBMRegressor\nfrom numpy import nan\nimport math # to use sqrt function\nimport sys\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2b1cc4af":"# Set graph style and font\n# sns.set()\n# Change the axes' title and label size to 18 & 16 by default and default figure size, and make title bold\n# Axes formatter limit will only display scientific notation if it's > 10^7 (or 10 million JPY) or < 10^-5\n# plt.rcParams.update({'axes.titleweight': 'bold','figure.figsize': (16,10),'axes.titlesize': 18,'axes.labelsize': 16, \n#                      'legend.fontsize': 12, 'xtick.labelsize': 12, 'ytick.labelsize': 12, 'font.family': 'serif', \n#                      'axes.formatter.limits':'-5, 7'}) ","86fd0049":"# For Kaggle only\npath = '\/kaggle\/input\/g-research-crypto-forecasting\/'\norig_df_train = pd.read_csv(path + 'train.csv')\nsupp_df_train = pd.read_csv(path + 'supplemental_train.csv')","232edd5f":"# The right join is outer as I'm stacking supplemental data on top of main data\nmerged_df = orig_df_train.merge(supp_df_train, how = 'outer')\n# merged_df","cd0b0b6b":"# Delete unused variables to save some memory\n# del orig_df_train, supp_df_train","ae0f627b":"# Reduce size by converting to float\nmerged_df[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', \\\n           'Target']] = merged_df[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']].astype(np.float32)\nprint(merged_df.shape)\n\n# Fill in missing 'target' values with 0\n# merged_df['Target'] = merged_df['Target'].fillna(0) # Fill in missing values\n\n# Drop any rows with missing values\nmerged_df = merged_df.dropna(how = 'any')\nprint(merged_df.shape)\n\n# Convert UNIX timestamp to human time\nmerged_df['date'] = pd.to_datetime(merged_df['timestamp'],unit='s')","9cbb1b2e":"# Assign human time as index\nmerged_df.set_index('date', inplace = True)\n# merged_df.describe(include = 'all')","1c72a038":"# Group data by the hour to reduce the rows\nhourly_df = merged_df.groupby('Asset_ID').resample('H').mean()\nhourly_df['date'] = pd.to_datetime(hourly_df['timestamp'],unit='s')\nhourly_df = hourly_df.sort_index()\n# hourly_df.sort_values('date', axis = 0)\nhourly_df","de7ee583":"hourly_df.index","03b8fbfc":"asset_details = pd.read_csv(path + 'asset_details.csv')\n# Add the asset symbols\nasset_details['Symbol'] = ['BCH', 'BNB', 'BTC', 'EOS', 'ETC', 'ETH', 'LTC', 'XMR', 'TRX', 'XLM', 'ADA', 'MIOTA', 'MKR', 'DOGE']\n\n# Sort value from smallest to highest\nasset_details.sort_values('Asset_ID', inplace = True)\nasset_weight_dict = {asset_details['Asset_ID'].tolist()[idx]: asset_details['Weight'].tolist()[idx] for idx in range(len(asset_details))}\nasset_name_dict = {asset_details['Asset_ID'].tolist()[idx]: asset_details['Asset_Name'].tolist()[idx] for idx in range(len(asset_details))}\nasset_details","92d2469c":"# Create 2 data frame dictionaries to store all the data frames by the minutes and hours\nminutes = {coin : pd.DataFrame for coin in asset_details['Asset_ID']} # by the minute\nhourly = {hourly_coin : pd.DataFrame for hourly_coin in asset_details['Asset_ID']}\n# Extract data for each asset and set time as index\nfor ID, symbol in zip(asset_details['Asset_ID'], asset_details['Symbol']):\n    minutes[symbol] = merged_df[:][merged_df['Asset_ID'] == ID]\n    minutes[symbol]['date'] = pd.to_datetime(minutes[symbol]['timestamp'],unit='s') # Convert from Unix timestamp to human time\n    minutes[symbol].set_index('date', inplace = True)  # Necessary in order to use resampling\n    hourly[symbol] = minutes[symbol].resample('H').mean().replace(np.NaN, 0)\n    hourly[symbol].reset_index(inplace = True) # Reset index in order to be able to sort\n    hourly[symbol]['date'] = pd.to_datetime(hourly[symbol]['timestamp'],unit='s')\n    hourly[symbol] = hourly[symbol].sort_index()\n    minutes[symbol].reset_index(inplace = True) # Reset index in order to be able to sort and work with the training function","7b59bba8":"# Check to see if it's sorted correctly\nhourly['BNB']","56822bea":"# See the rows with NaN values\nhourly['ADA'].isna().sum()","46a16c98":"minutes['BNB']","45b3252e":"# Function to draw candlestick charts\ndef candlestick_chart(data, title):\n    candlestick = go.Figure(data = [go.Candlestick(x =data.index, \n                                               open = data[('Open')], \n                                               high = data[('High')], \n                                               low = data[('Low')], \n                                               close = data[('Close')])])\n    candlestick.update_xaxes(title_text = 'Time',\n                             rangeslider_visible = True)\n\n    candlestick.update_layout(\n    title = {\n        'text': '{:} Candlestick Chart'.format(title),\n        'y':0.90,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'} , \n    template=\"plotly_white\")\n\n    candlestick.update_yaxes(title_text = 'Price in USD', ticksuffix = '$')\n    return candlestick","9e01c113":"# Plot BTC price for last 1000 points\nbtc_hourly = candlestick_chart(hourly['BTC'][-1000:], 'BTC price over time')\nbtc_hourly.show()","6ded57d1":"def ohlc_chart(data,title):\n    ohlc = go.Figure(data = [go.Ohlc(x =data.index, \n                                               open = data[('Open')], \n                                               high = data[('High')], \n                                               low = data[('Low')], \n                                               close = data[('Close')])])\n    ohlc.update_xaxes(title_text = 'Time',\n                             rangeslider_visible = True)\n\n    ohlc.update_layout(\n    title = {\n        'text': '{:} OHLC Chart'.format(title),\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n        template=\"plotly_white\")\n\n    ohlc.update_yaxes(title_text = 'Price in USD', ticksuffix = '$')\n    return ohlc","4da20c11":"# Chart for ETH for last 200 rows\n# ohlc_chart(dfDict['ETH'][-200:], title = \"Ethereum(ETH) price over last 200 minutes\")","6ae3b31f":"def vol_traded(data ,title,color):\n    area = px.area(data_frame=data,\n               x = data.index ,\n               y = \"Volume\",\n               markers = True)\n    area.update_traces(line_color=color)\n    area.update_xaxes(\n        title_text = 'Time',\n        rangeslider_visible = True)\n    area.update_yaxes(title_text = 'Number of trades every minute')\n    area.update_layout(showlegend = True,\n        title = {\n            'text': '{:} Volume Traded'.format(title),\n            'y':0.94,\n            'x':0.5,\n            'xanchor': 'center',\n            'yanchor': 'top'},\n        template=\"plotly_white\")\n    return area","263a9c09":"# vol_traded(dfDict['ETH'][-50:], \"Ethereum (ETH) volume over last 50 minutes\", color = \"Red\")","55e23493":"DEVICE = \"GPU\" #or \"CPU\"\n\nSEED = 25\n\n# CV PARAMS\nFOLDS = 5\nGROUP_GAP = 130\nMAX_TEST_GROUP_SIZE = 180\nMAX_TRAIN_GROUP_SIZE = 280\n\n# LOAD STRICT? YES=1 NO=0 | see: https:\/\/www.kaggle.com\/julian3833\/proposal-for-a-meaningful-lb-strict-lgbm\nLOAD_STRICT = True\n\n# WHICH YEARS TO INCLUDE? YES=1 NO=0\nINC2021 = 0\nINC2020 = 0\nINC2019 = 0\nINC2018 = 0\nINC2017 = 0\nINCCOMP = 1\nINCSUPP = 0\n\n# HYPER PARAMETERS\nLEARNING_RATE = [0.09, 0.09, 0.09, 0.09, 0.09]\nN_ESTIMATORS = [1000, 1000, 1000, 1000, 1000]\nMAX_DEPTH = [10, 10, 10, 10, 10]","cf5ecf7d":"# Two features from the competition tutorial\ndef upper_shadow(df): return df['High'] - np.maximum(df['Close'], df['Open'])\ndef lower_shadow(df): return np.minimum(df['Close'], df['Open']) - df['Low']\n\n# A utility function to build features from the original df\ndef get_features(df):\n    df_feat = df[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']].copy()\n    df_feat['upper_Shadow'] = upper_shadow(df_feat)\n    df_feat['lower_Shadow'] = lower_shadow(df_feat)\n    df_feat[\"high_div_low\"] = df_feat[\"High\"] \/ df_feat[\"Low\"]\n    df_feat[\"open_sub_close\"] = df_feat[\"Open\"] - df_feat[\"Close\"]\n    return df_feat","a68f3df2":"def corr(a, b, w):\n    cov = lambda x, y: np.sum(w * (x - np.average(x, weights=w)) * (y - np.average(y, weights=w))) \/ np.sum(w)\n    return cov(a, b) \/ np.sqrt(cov(a, a) * cov(b, b))\n\n# Catboost Version\ndef get_catb_metric(w):\n    class WCorrMetric(object):\n        def get_final_error(self, error, weight): return error \n        def is_max_optimal(self): return True\n        def evaluate(self, approxes, target, weight):\n            approx = approxes[0]\n            weight_sum = 1.0\n            error_sum = corr(target, approx, w)\n            return error_sum, weight_sum","5a183209":"def build_model(fold, weight = 1.0):\n\n    model = CatBoostRegressor(iterations = N_ESTIMATORS[fold], depth = MAX_DEPTH[fold], \\\n                              learning_rate = LEARNING_RATE[fold], task_type = \"GPU\" if DEVICE == 'GPU' else None, \\\n                              eval_metric = get_catb_metric(weight))\n    return model","5a9f67b8":"from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n# https:\/\/github.com\/getgaurav2\/scikit-learn\/blob\/d4a3af5cc9da3a76f0266932644b884c99724c57\/sklearn\/model_selection\/_split.py#L2243\nclass GroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Provides train\/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_size : int, default=None\n        Maximum size for a single training set.\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import GroupTimeSeriesSplit\n    >>> groups = np.array(['a', 'a', 'a', 'a', 'a', 'a',\\\n                           'b', 'b', 'b', 'b', 'b',\\\n                           'c', 'c', 'c', 'c',\\\n                           'd', 'd', 'd'])\n    >>> gtss = GroupTimeSeriesSplit(n_splits=3)\n    >>> for train_idx, test_idx in gtss.split(groups, groups=groups):\n    ...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n    ...     print(\"TRAIN GROUP:\", groups[train_idx],\\\n                  \"TEST GROUP:\", groups[test_idx])\n    TRAIN: [0, 1, 2, 3, 4, 5] TEST: [6, 7, 8, 9, 10]\n    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a']\\\n    TEST GROUP: ['b' 'b' 'b' 'b' 'b']\n    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] TEST: [11, 12, 13, 14]\n    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b']\\\n    TEST GROUP: ['c' 'c' 'c' 'c']\n    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\\\n    TEST: [15, 16, 17]\n    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b' 'c' 'c' 'c' 'c']\\\n    TEST GROUP: ['d' 'd' 'd']\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_size=None\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_size = max_train_size\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train\/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n        group_test_size = n_groups \/\/ n_folds\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n            for train_group_idx in unique_groups[:group_test_start]:\n                train_array_tmp = group_dict[train_group_idx]\n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n            train_end = train_array.size\n            if self.max_train_size and self.max_train_size < train_end:\n                train_array = train_array[train_end -\n                                          self.max_train_size:train_end]\n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n            yield [int(i) for i in train_array], [int(i) for i in test_array]\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n# modified code for group gaps; source\n# https:\/\/github.com\/getgaurav2\/scikit-learn\/blob\/d4a3af5cc9da3a76f0266932644b884c99724c57\/sklearn\/model_selection\/_split.py#L2243\nclass PurgedGroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Allows for a gap in groups to avoid potentially leaking info from\n    train into test if the model has windowed or lag features.\n    Provides train\/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_group_size : int, default=Inf\n        Maximum group size for a single training set.\n    group_gap : int, default=None\n        Gap between train and test\n    max_test_group_size : int, default=Inf\n        We discard this number of groups from the end of each train split\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_group_size=np.inf,\n                 max_test_group_size=np.inf,\n                 group_gap=None,\n                 verbose=False\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_group_size = max_train_group_size\n        self.group_gap = group_gap\n        self.max_test_group_size = max_test_group_size\n        self.verbose = verbose\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train\/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        group_gap = self.group_gap\n        max_test_group_size = self.max_test_group_size\n        max_train_group_size = self.max_train_group_size\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n\n        group_test_size = min(n_groups \/\/ n_folds, max_test_group_size)\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n\n            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n                train_array_tmp = group_dict[train_group_idx]\n\n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n\n            train_end = train_array.size\n\n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n\n            test_array  = test_array[group_gap:]\n\n\n            if self.verbose > 0:\n                    pass\n\n            yield [int(i) for i in train_array], [int(i) for i in test_array]","0642ced9":"import numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\ndef plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n    cmap_cv = plt.cm.coolwarm\n    jet = plt.cm.get_cmap('jet', 256)\n    seq = np.linspace(0, 1, 256)\n    _ = np.random.shuffle(seq)   # inplace\n    cmap_data = ListedColormap(jet(seq))    \n    for ii, (tr, tt) in enumerate(list(cv.split(X=X, y=y, groups=group))):\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0        \n        ax.scatter(range(len(indices)), [ii + .5] * len(indices), c=indices, marker='_', lw=lw, cmap=cmap_cv, vmin=-.2, vmax=1.2)\n    ax.scatter(range(len(X)), [ii + 1.5] * len(X), c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n    ax.scatter(range(len(X)), [ii + 2.5] * len(X), c=group, marker='_', lw=lw, cmap=cmap_data)\n    yticklabels = list(range(n_splits)) + ['target', 'day']\n    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels, xlabel='Sample index', ylabel=\"CV iteration\", ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n    return ax\n\ndef plot_importance(importances, features_names, PLOT_TOP_N = 20, figsize=(12, 20)):\n    try: plt.close()\n    except: pass\n    importance_df = pd.DataFrame(data=importances, columns=features_names)\n    sorted_indices = importance_df.median(axis=0).sort_values(ascending=False).index\n    sorted_importance_df = importance_df.loc[:, sorted_indices]\n    plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n    _, ax = plt.subplots(figsize=figsize)\n    ax.grid()\n    ax.set_xscale('log')\n    ax.set_ylabel('Feature')\n    ax.set_xlabel('Importance')\n    plt.title('Feature Importances')\n    sns.boxplot(data=sorted_importance_df[plot_cols], orient='h', ax=ax)\n    plt.show()\n    \n# asset_id = 0\n# df = load_training_data_for_asset(asset_id) # not loading because I use my the original data 'merged_df' (too big)\n# Using hourly_df, if this is still too big, I'll just use each asset df, e.g. hourly['BNB']\ndf = minutes['BTC']  # Change here to see different asset and timeline (minutes, hourly)\ndf_proc = get_features(df)\ndf_proc['date'] = df['date'].copy() \ndf_proc['y'] = df['Target']\ndf_proc = df_proc.dropna(how=\"any\")\nX = df_proc.drop(\"y\", axis=1)\ny = df_proc[\"y\"]\ngroups = pd.factorize(X['date'].dt.day.astype(str) + '_' + X['date'].dt.month.astype(str) + '_' + X['date'].dt.year.astype(str))[0]\nX = X.drop(columns = 'date')\n\nfig, ax = plt.subplots(figsize = (12, 6))\ncv = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap = GROUP_GAP, max_train_group_size=MAX_TRAIN_GROUP_SIZE, max_test_group_size=MAX_TEST_GROUP_SIZE)\nplot_cv_indices(cv, X, y, groups, ax, FOLDS, lw=20)","717408f7":"# USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit\nVERBOSE = 0\n\n# Train per asset, so I need to use hourly df of each asset\ndef get_Xy_and_model_for_asset(asset_id, symbol):\n#     df = load_training_data_for_asset(asset_id) # No need because my data is separate\n    df = minutes[symbol]   # modified to work with the hourly dataframe by asset\n    orig_close = df['Close'].copy()\n    df_proc = get_features(df)\n    df_proc['date'] = df['date'].copy()\n    df_proc['y'] = df['Target']\n    df_proc = df_proc.dropna(how=\"any\")\n    X = df_proc.drop(\"y\", axis=1)\n    y = df_proc[\"y\"]\n    groups = pd.factorize(X['date'].dt.day.astype(str) + '_' + X['date'].dt.month.astype(str) + '_' + X['date'].dt.year.astype(str))[0]\n    X = X.drop(columns = 'date')\n    oof_preds = np.zeros(len(X))\n    importances, scores, models = [], [], []\n    for fold, (train_idx, val_idx) in enumerate(PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap = GROUP_GAP, max_train_group_size = MAX_TRAIN_GROUP_SIZE, max_test_group_size = MAX_TEST_GROUP_SIZE).split(X, y, groups)):\n        # GET TRAINING, VALIDATION SET\n        x_train, x_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        # DISPLAY FOLD INFO\n        print('#' * 25); print('#### FOLD', fold + 1)\n        print('#### Training N_ESTIMATORS %s | MAX_DEPTH %s | LEARNING_RATE %s' % (N_ESTIMATORS[fold], MAX_DEPTH[fold], LEARNING_RATE[fold]))\n\n        model = build_model(fold, weight = np.array([asset_weight_dict[asset_id]] * len(y_val)))\n\n        # TRAIN\n        model.fit( x_train, y_train, eval_set = [(x_val, y_val)], early_stopping_rounds = 50, verbose = VERBOSE)\n\n        # PREDICT OOF\n        pred = model.predict(x_val)\n        models.append(model)       \n        \n        # REPORT RESULTS\n        try: mse = mean_squared_error(np.nan_to_num(y_val), np.nan_to_num(pred))\n        except: mse = 0.0\n\n        scores.append(mse)\n        oof_preds[val_idx] = pred\n        importances.append(model.get_feature_importance())\n        \n        w_score = corr(np.nan_to_num(y_val), np.nan_to_num(pred.flatten()), np.array([asset_weight_dict[asset_id]] * len(y_val)))\n        print('#### FOLD %i OOF MSE %.3f | WCORR: %.3f' % (fold + 1, mse, w_score))\n        \n    df = df_proc\n    df['oof_preds'] = np.nan_to_num(oof_preds)\n    df['Close'] = orig_close\n    print('\\n\\n' + ('-' * 80) + '\\n' + 'Finished training %s. Results:' % asset_name_dict[asset_id])\n    print('Model: r2_score: %s | pearsonr: %s | wcorr: %s ' % (r2_score(df['y'], df['oof_preds']), pearsonr(df['y'], df['oof_preds'])[0], corr(df['y'].values, df['oof_preds'].values, np.array([asset_weight_dict[asset_id]] * len(df['y'].values)))))\n    print('Predictions std: %s | Target std: %s' % (df['oof_preds'].std(), df['y'].std()))\n    try: plt.close()\n    except: pass   \n    df2 = df.reset_index(drop = True).set_index('date')\n    fig = plt.figure(figsize = (12, 6))\n    # fig, ax_left = plt.subplots(figsize = (12, 6))\n    ax_left = fig.add_subplot(111)\n    ax_left.set_facecolor('azure')    \n    ax_right = ax_left.twinx()\n    ax_left.plot(df2['y'].rolling(3 * 30 * 24 * 60).corr(df2['oof_preds']).iloc[::24 * 60], color = 'crimson', label = \"Target WCorr\")\n    ax_right.plot(df2['Close'].iloc[::24 * 60], color = 'darkgrey', label = \"%s Close\" % asset_name_dict[asset_id])   \n    plt.legend()\n    plt.grid()\n    plt.xlabel('Time')\n    plt.title('3 month rolling pearsonr for %s' % (asset_name_dict[asset_id]))\n    plt.show()\n    \n    plot_importance(np.array(importances), list(X.columns), PLOT_TOP_N = 20)\n    \n    return scores, oof_preds, models, y\n\nmodels, scores, targets, oof_preds = {}, {}, {}, {}\n\n# Traing all assets at once, modified to work with my hourly df for each model\nfor asset_id, symbol in zip(asset_details['Asset_ID'], asset_details['Symbol']):\n    print(f\"Training model for {symbol:<6} (ID={asset_id:<2})\")\n    cur_scores, cur_oof_preds, cur_models, cur_targets = get_Xy_and_model_for_asset(asset_id, symbol)\n    scores[asset_id], oof_preds[asset_id], models[asset_id], targets[asset_id] = np.mean(cur_scores), cur_oof_preds, cur_models, cur_targets","70b032c9":"# COMPUTE OVERALL OOF MSE\nprint('Overall MEAN OOF MSE %s' % np.mean(list(scores.values())))\n\n# SAVE OOF TO DISK \ny_pred, y_true, weights = [], [], []\nfor asset in oof_preds:\n    df_oof = pd.DataFrame(dict(asset_id = asset, oof_preds=oof_preds[asset]))\n    df_oof.to_csv(str(asset) + '_oof.csv',index=False)\n    y_pred += oof_preds[asset].tolist()\n    y_true += targets[asset].tolist() \n    weights += ([asset_weight_dict[asset]] * len(oof_preds[asset].tolist()))\n    print('%s score: %s' % (asset_name_dict[asset], corr(np.nan_to_num(np.array(y_true).flatten()), np.nan_to_num(np.array(y_pred).flatten()), np.nan_to_num(np.array(weights).flatten()))))\n    \nprint('Overall score %s' % corr(np.nan_to_num(np.array(y_true).flatten()), np.nan_to_num(np.array(y_pred).flatten()), np.nan_to_num(np.array(weights).flatten())))","063028ec":"all_df_test = []\n\nenv = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\nfor i, (df_test, df_pred) in enumerate(iter_test):\n    for j , row in df_test.iterrows():\n        try:\n            if row['Asset_ID'] in models:\n                cur_models = models[row['Asset_ID']]\n                x_test = get_features(row)\n                y_pred = np.mean(np.concatenate([np.expand_dims(model.predict([x_test]), axis = 0) for model in cur_models], axis = 0), axis = 0)\n            else: y_pred = 0.0\n        except: \n            y_pred = 0.0\n            traceback.print_exc()\n        df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = y_pred\n    all_df_test.append(df_test)\n    env.predict(df_pred)","78e55e62":"all_df_test","810da173":"# Time Series CV","2855c1bd":"## OHLC charts","f0b15a9a":"## Candlestick charts","e297c964":"# Training","3f2de4ef":"## See the CV visually","83bb2e80":"# Save the results for submission to Kaggle","2db9dd9c":"# Preprocessing","a46ddd9e":"# Configure the model","d90cc4d8":"# Feature engineering","24789df4":"## Area plots (volumes traded)","b3b6b7b4":"## The model","04007ab3":"## Main training function","df0e1d7c":"# EDA","603520c5":"# Loading in data","be80778f":"# Variables to change","e1b2afb8":"# Dividing each asset into its own dataframe","49db443b":"## Competition metric","9c3405e5":"# Calculate OOF MSE","0c308ca1":"## Merging main data with supplemental data together","c4213115":"Fill in the target column with 0 if nan."}}