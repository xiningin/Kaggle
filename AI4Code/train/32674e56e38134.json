{"cell_type":{"6bb5a379":"code","3001b013":"code","1024c0df":"code","8ed396a3":"code","acfed2ea":"code","20848dde":"code","81428b23":"code","d4f00e6f":"code","8cf8d97b":"code","8e9d85ab":"code","1ca348de":"code","63d1243f":"code","7569d5a9":"code","9e7425a6":"code","dafc1578":"code","c159ed7f":"code","7f5927c5":"code","869741ea":"code","5f0f2643":"code","da298181":"code","bedab4f1":"code","6a4333fe":"markdown","6f04fba3":"markdown","76183979":"markdown","13242009":"markdown"},"source":{"6bb5a379":"import numpy as np \nimport pandas as pd \nimport os, random, sys, time, re\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.utils.data as D\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom transformers import *\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForMaskedLM","3001b013":"# ====================================================\n# Directory settings\n# ====================================================\nOUTPUT_DIR = '.\/'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n\nTRAIN_PATH = '..\/input\/commonlitreadabilityprize'\nTEST_PATH = '..\/input\/commonlitreadabilityprize'","1024c0df":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    debug=False\n    seed=7117\n    n_folds=5\n    model_name='roberta-base'\n    max_sequence_length=220\n    batch_size=12\n    epochs=15\n    lr=2.5e-5\n    scheduler='ConstantScheduleWithWarmup' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts', 'ConstantScheduleWithWarmup']\n    #factor=0.2 # ReduceLROnPlateau\n    #patience=4 # ReduceLROnPlateau\n    #eps=1e-6 # ReduceLROnPlateau\n    #T_max=10 # CosineAnnealingLR\n    #T_0=10 # CosineAnnealingWarmRestarts\n    #n_epochs=100\n    #min_lr=1e-6\n    \nif CFG.debug:\n    CFG.epochs = 1\n    train = train.sample(n=1000, random_state=CFG.seed).reset_index(drop=True)","8ed396a3":"\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n# seed=CFG.seed\n\n# error log\nsys.stderr = open('err.txt', 'w')","acfed2ea":"# ====================================================\n# Utils\n# ====================================================\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=CFG.seed)","20848dde":"DATA_PATH = \"..\/input\/commonlitreadabilityprize\/\"\n\n# model_path = '..\/input\/distilbertbaseuncased'\n# model_path = '..\/input\/pretrained-albert-pytorch\/albert-base-v2'\n# model_path = '..\/input\/roberta-transformers-pytorch\/distilroberta-base'\n# model_path = '..\/input\/roberta-transformers-pytorch\/roberta-base'\n# model_path = '..\/input\/bart-models-hugging-face-model-repository\/bart-base'\nmodel_path = '..\/input\/pretrainedrobertabase'\n\n# VOCAB_PATH = '..\/input\/roberta-transformers-pytorch\/roberta-base'\n# VOCAB_PATH = '..\/input\/pretrained-albert-pytorch\/albert-base-v2'\n# VOCAB_PATH = '..\/input\/pretrained-albert-pytorch\/albert-base-v1'\n# VOCAB_PATH = '..\/input\/distilbertbaseuncased'\n# VOCAB_PATH = '..\/input\/bart-models-hugging-face-model-repository\/bart-base'\nVOCAB_PATH = '..\/input\/pretrainedrobertabase'","81428b23":"if CFG.model_name == 'roberta-base':\n    model_name = '..\/input\/pretrainedrobertabase'\n","d4f00e6f":"train_csv = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'), index_col='id')\ntrain_csv['excerpt'] = train_csv['excerpt'].replace('\\n', '')\n\ntest_csv = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'), index_col='id')","8cf8d97b":"subm = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'), index_col='id')\n\ny = (train_csv.target.values > 0).astype(int)\ncv = StratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)","8e9d85ab":"tokenizer = RobertaTokenizer.from_pretrained(\n            '..\/input\/roberta-transformers-pytorch\/roberta-base', model_max_length=CFG.max_sequence_length)\ndef get_tokens(text):\n    tokens = tokenizer.encode_plus(text, max_length=CFG.max_sequence_length, truncation=True, return_attention_mask=True, return_token_type_ids=True)\n    return tokens\n    \ntrain_csv['token'] = train_csv.excerpt.apply(get_tokens)\ntest_csv['token'] = test_csv.excerpt.apply(get_tokens)","1ca348de":"train_csv['token'].head()","63d1243f":"test_csv['token'].head()","7569d5a9":"class LitDataset(D.Dataset):\n    \n    def __init__(self, token, target):\n        self.token = token\n        self.target = target\n        \n    def __len__(self):\n        return self.token.shape[0]\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.token[idx].input_ids), \\\n                torch.tensor(self.token[idx].attention_mask), torch.tensor(self.token[idx].token_type_ids), self.target[idx]\n    \ndef collate_fn(batch):\n    ids, attns, token_type, targets = zip(*batch)\n    ids = pad_sequence(ids, batch_first=True).to(DEVICE)\n    attns = pad_sequence(attns, batch_first=True).to(DEVICE)\n    token_type = pad_sequence(token_type, batch_first=True).to(DEVICE)\n    targets = torch.tensor(targets).float().to(DEVICE)\n    return ids, attns, token_type, targets\ndef collate_fn_test(batch):\n    ids, attns, token_type, idxs = zip(*batch)\n    ids = pad_sequence(ids, batch_first=True).to(DEVICE)\n    attns = pad_sequence(attns, batch_first=True).to(DEVICE)\n    token_type = pad_sequence(token_type, batch_first=True).to(DEVICE)\n    return idxs, ids, attns, token_type\n","9e7425a6":"class CommonLitModel(nn.Module):\n    def __init__(\n        self, \n        model_name, \n        config,  \n        multisample_dropout=False,\n        output_hidden_states=False\n    ):\n        super(CommonLitModel, self).__init__()\n        self.config = config\n        self.roberta = AutoModel.from_pretrained(\n            model_name, \n            output_hidden_states=output_hidden_states\n        )\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        if multisample_dropout:\n            self.dropouts = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        #self.regressor = nn.Linear(config.hidden_size*2, 1)\n        self.regressor = nn.Linear(config.hidden_size, 1)\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.regressor)\n \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n \n    def forward(\n        self, \n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None\n#         labels=None\n    ):\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n        sequence_output = outputs[1]\n        sequence_output = self.layer_norm(sequence_output)\n \n        # max-avg head\n        # average_pool = torch.mean(sequence_output, 1)\n        # max_pool, _ = torch.max(sequence_output, 1)\n        # concat_sequence_output = torch.cat((average_pool, max_pool), 1)\n \n        # multi-sample dropout\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.regressor(dropout(sequence_output))\n            else:\n                logits += self.regressor(dropout(sequence_output))\n        \n        logits \/= len(self.dropouts)\n        \n        return logits","dafc1578":"ds = LitDataset(train_csv.token, train_csv.target)\ntest_ds = LitDataset(test_csv.token, test_csv.index)\n\ntloader = D.DataLoader(test_ds, batch_size=CFG.batch_size,\n                       shuffle=False, collate_fn = collate_fn_test, num_workers=0)","c159ed7f":"### Table for results\nheader = r'''\n            Train         Validation\nEpoch |  MSE  |  RMSE |  MSE  |  RMSE | Time, m\n'''\n#          Epoch         metrics            time\nraw_line = '{:6d}' + '\\u2502{:7.3f}'*4 + '\\u2502{:6.2f}'","7f5927c5":"    # ====================================================\n    # scheduler \n    # ====================================================\n    def get_scheduler(optimizer):\n        if CFG.scheduler=='ReduceLROnPlateau': # epoch\n            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n        elif CFG.scheduler=='CosineAnnealingLR': # epoch\n            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n        elif CFG.scheduler=='CosineAnnealingWarmRestarts': # epoch\n            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n        elif CFG.scheduler == 'ConstantScheduleWithWarmup':\n            scheduler = get_constant_schedule_with_warmup(optimizer, 35)\n        return scheduler","869741ea":"@torch.no_grad()\ndef validation_fn(model, loader, loss_fn):\n    tloss = []\n    for texts, attns, token_type, target in loader:\n#         outputs = model(texts, attention_mask=attns)\n        outputs = model(\n            input_ids=texts,\n            attention_mask=attns,\n            token_type_ids=token_type\n        )\n        loss = loss_fn(outputs.squeeze(-1), target)\n        tloss.append(loss.item())\n    tloss = np.array(tloss).mean()\n    return tloss\n\ndef oof_preds(ds, tloader, cv, y, epochs = CFG.epochs):\n    \n    loss_fn = torch.nn.MSELoss()\n    \n#     for train_idx, valid_idx in cv.split(range(len(ds)), y):\n    for fold, (train_idx, valid_idx) in enumerate(cv.split(range(len(ds)), y)):\n        \n        train_ds = D.Subset(ds, train_idx)\n        loader = D.DataLoader(train_ds, batch_size=CFG.batch_size,\n                              shuffle=True, collate_fn = collate_fn,num_workers=0)\n        \n        valid_ds = D.Subset(ds, valid_idx)\n        vloader = D.DataLoader(valid_ds, batch_size=CFG.batch_size,\n                      shuffle=False, collate_fn = collate_fn,num_workers=0)\n        \n#         model = get_model.from_pretrained( \n#                           model_path, num_labels=1).to(DEVICE);\n        config = AutoConfig.from_pretrained(model_path)\n        config.update({'num_labels': 1})\n        model = CommonLitModel(model_path, config=config)\n        model = model.to(DEVICE)\n        \n        optimizer = optim.AdamW(model.parameters(), CFG.lr,\n                                betas=(0.9, 0.999), weight_decay=1e-1)\n#         scheduler = get_constant_schedule_with_warmup(optimizer, 35)\n        scheduler = get_scheduler(optimizer)\n        b_loss = np.inf\n        \n        print(header)\n        for epoch in range(1, epochs+1):      \n            start_time = time.time()\n            tloss = []          \n            model.train()\n            \n            for texts, attns, token_type, target in loader:\n                optimizer.zero_grad()\n#                 outputs = model(texts, attention_mask=attns)\n                outputs = model(\n                    input_ids=texts,\n                    attention_mask=attns,\n                    token_type_ids=token_type\n                )\n                loss = loss_fn(outputs.squeeze(-1), target)\n                tloss.append(loss.item())\n                loss.backward()\n                optimizer.step()\n                scheduler.step()\n            tloss = np.array(tloss).mean()\n            vloss = validation_fn(model, vloader, loss_fn)\n            tmetric = tloss**.5\n            vmetric = vloss**.5\n            print(raw_line.format(epoch,tloss,tmetric,vloss,vmetric,(time.time()-start_time)\/60**1))\n            del loss, outputs\n            \n            if vmetric <= b_loss:\n                b_loss = vmetric\n                torch.save(model.state_dict(), f\"fold{fold}_best.pth\")\n        \n#         model = get_model.from_pretrained(model_path, num_labels=1)\n        model = CommonLitModel(model_path, config=config)\n        model.load_state_dict(torch.load(f\"fold{fold}_best.pth\"), strict=False)\n        model.to(DEVICE)\n        \n        model.eval();\n        # Get prediction for test set\n        ids, preds = [], [] \n        with torch.no_grad():\n            for batch_ids, texts, attn, token_type in tloader:\n#                 outputs = model(texts, attention_mask=attn)\n                outputs = model(\n                    input_ids=texts,\n                    attention_mask=attn,\n                    token_type_ids=token_type\n                )\n                ids += batch_ids\n                preds.append(outputs.detach().squeeze(-1).cpu().numpy())\n            \n        # Save prediction of test set\n        preds = np.concatenate(preds)\n        subm.loc[ids, 'target']  =  subm.loc[ids, 'target'].values + preds \/ N_FOLDS\n        \n        del model, vloader, loader, train_ds, valid_ds\n        torch.cuda.empty_cache()\n        ","5f0f2643":"oof_preds(ds, tloader, cv, y, epochs = CFG.epochs)","da298181":"subm.to_csv('submission.csv')","bedab4f1":"subm.head()","6a4333fe":"## Directory Settings","6f04fba3":"## Data Loading, CV Split","76183979":"## Utils","13242009":"## CFG"}}