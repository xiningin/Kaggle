{"cell_type":{"f90dae03":"code","03e14758":"code","5c1c7d51":"code","0076a193":"code","8fbb1ea7":"code","fc27f8f1":"code","1b2ba5b4":"code","4565dbfe":"code","2644a6e2":"code","e28d6bbe":"code","2ebd39d2":"code","b5f54576":"code","d7b94cce":"code","215f794d":"code","ee9d1f12":"code","711c4699":"code","e119b893":"code","ced73087":"code","24dbb5c4":"code","751912cc":"code","bb6233ba":"code","9dbbb172":"code","48ffd2fd":"code","b42f6521":"code","feeb0a9e":"code","dcae2000":"code","013d1bf1":"code","a0a21fed":"code","d68a9395":"code","e3941dc7":"code","64eb30ab":"code","0dbb2a71":"code","04325dd3":"code","746767ef":"code","a1da396f":"markdown","45678ae7":"markdown","a4af5a08":"markdown","67e24ef0":"markdown","597b4338":"markdown","6811d143":"markdown","38ca5913":"markdown","c54015f7":"markdown","29f5a5ca":"markdown","ff3a7a10":"markdown","e4301fd0":"markdown","509858fd":"markdown","5f1fe9bd":"markdown","7913ffd8":"markdown","9c78f2de":"markdown","29182b12":"markdown","5b3cc7c4":"markdown","20cf535d":"markdown","8ea255d9":"markdown","ca7ebed1":"markdown","ed9e2f24":"markdown","fa7d6f04":"markdown","702715a3":"markdown","9454437c":"markdown","d1b175f7":"markdown"},"source":{"f90dae03":"#Data Manipulation Library:\nimport pandas as pd\n\n#Importing Data Manipulation Library:\nimport pandas as pd\n\n#Importing Scientific computing library:\nimport numpy as np\n\n#Importing Plotting libraries:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Importing Text Analysis Libraries:\nfrom textblob import TextBlob\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import PorterStemmer\n!pip install textblob \nfrom nltk.corpus import stopwords\nimport nltk\nnltk.download()\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as vader\nprint('Libraries Imported')","03e14758":"data2=pd.read_csv(\"https:\/\/dvn-cloud.s3.amazonaws.com\/10.7910\/DVN\/DPQMQH\/17352493abb-cf8c4a43d6c3?response-content-disposition=attachment%3B%20filename%2A%3DUTF-8%27%27india-news-headlines.csv&response-content-type=text%2Fcsv&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20201025T140932Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3599&X-Amz-Credential=AKIAIEJ3NV7UYCSRJC7A%2F20201025%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=e573a0dbc02a5078107335d8c103375e43b9f3fa6514b5543d56561e69b0fd9c\")\ndata2.head()","5c1c7d51":"data2.tail()","0076a193":"data2.columns","8fbb1ea7":"data2=data2.rename(columns={'publish_date':'Date'})\ndata2.head()","fc27f8f1":"data2['Date']=pd.to_datetime(data2['Date'],format='%Y%m%d')\ndata2=data2.drop('headline_category',axis=1)\ndata2.head(5)","1b2ba5b4":"data2=data2[data2['Date']>='2015-10-19']\ndata2.sort_values(by='Date')","4565dbfe":"data2['headline_text']=data2.groupby(['Date']).transform(lambda x: ' '.join(x))\ndata2=data2.drop_duplicates()\ndata2.reset_index()","2644a6e2":"\nprint('Dimension of dataset:{}'.format(data2.shape),'\\n',70*'-')\nprint('Number of duplicated values:{}'.format(data2.duplicated().sum()),'\\n',70*'-')\nprint('rows contain null values:\\n{}'.format(data2.isnull().sum()),'\\n',70*'-')\nprint('Schema of the dataset:\\n')\nprint(data2.info(),'\\n',70*'-')","e28d6bbe":"df2=data2","2ebd39d2":"df2['word_count'] = df2['headline_text'].apply(lambda x: len(str(x).split(\" \")))\ndf2[['headline_text','word_count']].head()","b5f54576":"df2['char_count'] = df2['headline_text'].str.len()\ndf2[['headline_text','char_count']].head()","d7b94cce":"def avg_word(sentence):\n  words = sentence.split()\n  return (sum(len(word) for word in words)\/len(words))\n\ndf2['avg_word'] = df2['headline_text'].apply(lambda x: avg_word(x))\ndf2[['headline_text','avg_word']].head()","215f794d":"stop = stopwords.words('english')\n\ndf2['stopwords'] = df2['headline_text'].apply(lambda x: len([x for x in x.split() if x in stop]))\ndf2[['headline_text','stopwords']].head()","ee9d1f12":"df2['hastags'] = df2['headline_text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\ndf2[['headline_text','hastags']].head()","711c4699":"df2['numerics'] = df2['headline_text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\ndf2[['headline_text','numerics']].head()","e119b893":"df2['upper'] = df2['headline_text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\ndf2[['headline_text','upper']].head()","ced73087":"df2['headline_text'] = df2['headline_text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ndf2['headline_text'].head()","24dbb5c4":"df2['headline_text'] = df2['headline_text'].str.replace('[^\\w\\s]','')\ndf2['headline_text'].head()","751912cc":"stop = stopwords.words('english')\ndf2['headline_text'] = df2['headline_text'].apply(\n    lambda x: \" \".join(x for x in x.split() if x not in stop))\ndf2['headline_text'].head()","bb6233ba":"freq = pd.Series(' '.join(df2['headline_text']).split()).value_counts()[:10]\nfreq","9dbbb172":"freq = list(freq.index)\ndf2['headline_text'] = df2['headline_text'].apply(\n    lambda x: \" \".join(x for x in x.split() if x not in freq))\ndf2['headline_text'].head()","48ffd2fd":"freq = pd.Series(' '.join(df2['headline_text']).split()).value_counts()[-10:]\nfreq","b42f6521":"freq = list(freq.index)\ndf2['headline_text'] = df2['headline_text'].apply(\n    lambda x: \" \".join(x for x in x.split() if x not in freq))\ndf2['headline_text'].head()","feeb0a9e":"df2.shape","dcae2000":"Data=df2","013d1bf1":"\nTextBlob(str(Data['headline_text'])).words","a0a21fed":"st = PorterStemmer()\nData['headline_text'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))","d68a9395":"Data['sentiment'] = Data['headline_text'].apply(lambda x: TextBlob(x).sentiment[0] )\nData[['headline_text','sentiment']].head()","e3941dc7":"Data1=Data[['Date','headline_text','sentiment']].reset_index()\nData1=Data1.drop('index',axis=1)\nData1.head()","64eb30ab":"x1=Data1[Data1['sentiment']>=0.5]\nx2=Data1[Data1['sentiment']<0.5]","0dbb2a71":"Data1['sentiment']=Data1['sentiment'].astype(float)\nData1.sentiment[Data1.sentiment>0]=1\nData1.sentiment[Data1.sentiment<0]=-1;","04325dd3":"Data1.head(20)","746767ef":"plt.figure(figsize=(10,5))\nax=sns.countplot(Data1['sentiment'],palette='Set3')\nax.set_xticklabels(['Negative','Neutral','Positive']);","a1da396f":"### **ANALYZING THE HEADLINES DATASET**","45678ae7":"### **REMOVING RARE WORDS TO GET RID OF NOISE**","a4af5a08":"### **PRE-PROCESSING DATASET**","67e24ef0":"### **REMOVING COMMON WORDS**","597b4338":"### **FINAL DATASET TABLE**","6811d143":"### **NUMBER OF CHARACTERS IN EACH STATEMENTS FROM EACH ROW**","38ca5913":"#### **DATA SOURCE: https:\/\/dataverse.harvard.edu\/dataset.xhtml?persistentId=doi:10.7910\/DVN\/DPQMQH**","c54015f7":"### **REMOVING PUNCTUATIONS**","29f5a5ca":"### **CONVERTING EVERY WORDS IN THE COLUMN TO LOWER CASE**","ff3a7a10":"### **IMPORTING DATASET**","e4301fd0":"### **NUMBER OF WORDS IN EACH ROW:**","509858fd":"### **REMOVING STOPWORDS**","5f1fe9bd":"### **CALCULATING THE NUMBER OF STOPWORDS**","7913ffd8":"### **NUMBER OF NUMERICS IN EACH STATEMENTS**","9c78f2de":"### **TOKENIZATION**","29182b12":"### **THE LAST AND FINAL RESULT: VISUALIZING OUR RESULT BY DISTRIBUTING IT AS NEGATIVE, NEUTRAL AND POSITIVE SENTIMENTS BASED ON THEIR SENTIMENTAL POLARITY WE ACQUIRED FROM ALL OF THE ABOVE ANALYSING TECHNIQUES**","5b3cc7c4":"### **NUMBER OF UPPERCASE WORDS IN EACH HEADLINES FROM EACH ROW**","20cf535d":"### **NOW, LETS PREPROCESS THE DATA**","8ea255d9":"### **From the above visualization it is very much clear that most of the headline topics published by TOI has neutral sentiments, followed by positive and negative sentiments.**","ca7ebed1":"### **ASSIGNING VALUE 1 TO ALL THOSE SENTIMENTS WHOSE POLARITY IS GREATER THAN 0 , AND VALUE -1 TO ALL THOSE SENTIMENTS WHOSE POLARITY IS SMALLER THAN 0 WHILE LEAVING SENTIMENTS WHOSE POLARITY IS EQUAL TO ZERO AS IT IS**","ed9e2f24":"### **FINALLY! LETS PERFORM SENTIMENTAL ANALYSIS FOR EACH ROWS IN A HEADLINE_TEXT COLUMN**","fa7d6f04":"### **NUMBER OF SPECIAL CHARACTERS IN EACH STATEMENTS**","702715a3":"### **IMPORTING LIBRARIES**","9454437c":"### **CALCULATING AVERAGE WORD LENGTH**","d1b175f7":"### **STEMMING**"}}