{"cell_type":{"4beb2403":"code","d57d5579":"code","a9bf23ad":"code","cee585f6":"code","08879767":"code","421fa2b9":"code","a37d3438":"code","97e5e688":"code","69eef548":"code","ec763237":"code","f1a626b7":"code","8f3f9742":"code","c11ad875":"code","fc132a6f":"code","62257812":"code","6d52edc8":"code","703ee7c9":"code","2c817caa":"code","ede813e8":"code","a4d51704":"code","c24536c9":"code","5c4488ef":"code","bd5e2a5a":"code","46b6f3d8":"code","c2a65a45":"code","09ef2497":"code","b1bce79a":"code","73d8b2f7":"code","063133da":"code","658bc359":"code","b861fade":"code","05204ad0":"code","b0261e21":"code","06f08e39":"code","7c6c642c":"code","1a4f429b":"code","7dd869ca":"code","577f7a86":"code","ad76311e":"code","700806d5":"code","38578dc0":"code","df5d5ab9":"code","d2911977":"code","ef44bb89":"code","352d36f4":"code","8cdd048f":"code","df1d1ef1":"code","d187c0e6":"code","3da49c93":"code","6e22ab66":"code","b4379f2c":"code","5bd9c8c5":"code","5b294c0e":"code","df724ee4":"code","02eaa4d9":"code","fa6312cf":"code","2b5858cc":"code","1cd37c90":"code","4b771418":"code","53daf46a":"code","ca5c779e":"code","1d7aefb4":"code","5ae88793":"code","62c0ec69":"code","128db067":"code","04fc053d":"code","96ccb068":"code","b36e70fc":"markdown","e8616d73":"markdown","3d39770e":"markdown","35b4313c":"markdown","fc16e8f5":"markdown","2c26daa0":"markdown","5b3943c2":"markdown","7b239fbe":"markdown","999e6359":"markdown","ef4c230f":"markdown","ed37d867":"markdown","42af3868":"markdown","3510e39f":"markdown","180225e7":"markdown","fa3ae1b1":"markdown","dfa1c994":"markdown","19d8d510":"markdown","a04350b6":"markdown","a54347ea":"markdown","6f0f6944":"markdown","540b1f78":"markdown","1b37d0f4":"markdown","fb790419":"markdown","305a1f23":"markdown","ed269414":"markdown","cdd62429":"markdown"},"source":{"4beb2403":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d57d5579":"mnist_df = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\nmnist_df.head()","a9bf23ad":"mnist_df.shape","cee585f6":"import matplotlib as mpl\nimport matplotlib.pyplot as plt","08879767":"for val in (10, 25, 1000, 2500):\n    sample_digit = mnist_df.iloc[val][1:] #taking all pixel values\n    sample_digit_mat = sample_digit.values.reshape(28,28)\n    plt.imshow(sample_digit_mat, cmap=\"binary\")\n    plt.show()","421fa2b9":"# to confirm the values\nfor val in (10, 25, 1000, 2500):\n    print( mnist_df.iloc[val][0])","a37d3438":"mnist_df.isnull().any().describe()","97e5e688":"import seaborn as sns\nplt.ioff()\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(data=mnist_df, x=\"label\")","69eef548":"from sklearn.model_selection import train_test_split","ec763237":"X = mnist_df.drop([\"label\"], axis=1)\ny = mnist_df[\"label\"]","f1a626b7":"#Verify it with its shape\nprint(X.shape)\nprint(y.shape)","8f3f9742":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","c11ad875":"from sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(random_state = 42, n_jobs=-1)\nsgd_clf_log = SGDClassifier(random_state = 42, n_jobs=-1, loss=\"log\")","fc132a6f":"sgd_clf.fit(X_train, y_train)\nsgd_clf_log.fit(X_train, y_train)","62257812":"from sklearn.metrics import accuracy_score\n\ny_pred_sgd_clf = sgd_clf.predict(X_test)\ny_pred_sgd_clf_log = sgd_clf_log.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred_sgd_clf))\nprint(accuracy_score(y_test, y_pred_sgd_clf_log))","6d52edc8":"from sklearn.metrics import confusion_matrix\n\ny_test_pred_sgd = sgd_clf.predict(X_test)\nconfusion_matrix(y_test, y_test_pred_sgd)","703ee7c9":"from sklearn.metrics import precision_score, recall_score, f1_score\n\nprint(precision_score(y_test, y_test_pred_sgd, average=\"weighted\"))\nprint(recall_score(y_test, y_test_pred_sgd, average=\"weighted\"))\nprint(f1_score(y_test, y_test_pred_sgd, average=\"weighted\"))","2c817caa":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators=500, max_leaf_nodes=30, random_state=42, n_jobs=-1, verbose=1)\nrfc.fit(X_train, y_train)","ede813e8":"from sklearn.metrics import accuracy_score\n\ny_test_pred_rfc = rfc.predict(X_test)\naccuracy_score(y_test, y_test_pred_rfc)","a4d51704":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV","c24536c9":"param_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [5, 8, 13]}]\n\nknn_clf = KNeighborsClassifier(n_jobs=-1)\n\ngrid_search = GridSearchCV(knn_clf, param_grid, cv=3, verbose=3)\ngrid_search.fit(X_train, y_train)","5c4488ef":"print(grid_search.best_params_)\nprint(grid_search.best_score_)","bd5e2a5a":"from sklearn.model_selection import cross_val_score\n\naccuracies = []\nfor k in np.arange(4,13):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X_train, y_train, cv=3, n_jobs=-1, verbose=3)\n    accuracies.append(scores.mean())\n    \nprint(accuracies)","46b6f3d8":"plt.plot(np.arange(4,13), accuracies)\nplt.xlabel('k in kNN')\nplt.ylabel('Accuracy')\nplt.show()","c2a65a45":"from sklearn.decomposition import PCA\n\npca = PCA()\npca.fit(X_train)","09ef2497":"cumsum = np.cumsum(pca.explained_variance_ratio_)\n#print(cumsum)\nd = np.argmax(cumsum >= 0.95) + 1 # to maintain 95% variance\nprint(d)","b1bce79a":"#better way is to specify the variance we need to preserve\npca = PCA(n_components=0.95)\nX_train_reduced = pca.fit_transform(X_train)","73d8b2f7":"X_train_reduced.shape","063133da":"from sklearn.svm import SVC\n\nsvm_clf = SVC(kernel=\"rbf\", random_state=42, verbose=3)\nsvm_clf.fit(X_train, y_train)","658bc359":"y_test_pred_svm = svm_clf.predict(X_test)\naccuracy_score(y_test, y_test_pred_svm)","b861fade":"params_grid_svm = {'C':[0.1, 1, 10, 100]}","05204ad0":"grid_search_svm = GridSearchCV(SVC(kernel=\"rbf\", random_state=42), param_grid=params_grid_svm, verbose=3)\ngrid_search_svm.fit(X_train, y_train)","b0261e21":"print(grid_search_svm.best_params_)\nprint(grid_search_svm.best_score_)","06f08e39":"from sklearn.ensemble import VotingClassifier\n\nsgd_clf = SGDClassifier(random_state = 42)\nrf_clf = RandomForestClassifier(n_estimators=500, random_state=42, n_jobs=-1, verbose=1)\nknn_clf = KNeighborsClassifier(n_neighbors=5, weights=\"distance\")\nsvm_clf = SVC(kernel=\"rbf\", C=10, probability=True, random_state=42)\n\nvoting_clf = VotingClassifier(estimators=[(\"sgd\", sgd_clf), (\"rfc\", rf_clf), (\"knn\", knn_clf), (\"svc\", svm_clf)])\nvoting_clf.fit(X_train, y_train)","7c6c642c":"for clf in (sgd_clf, rf_clf, knn_clf, svm_clf, voting_clf):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))","1a4f429b":"voting_clf = VotingClassifier(estimators=[(\"rfc\", rf_clf), (\"knn\", knn_clf), (\"svc\", svm_clf)], voting=\"soft\")\nvoting_clf.fit(X_train, y_train)","7dd869ca":"y_pred_votclf = voting_clf.predict(X_test)\naccuracy_score(y_test, y_pred_votclf)","577f7a86":"from sklearn.svm import SVC\n\nsvm_clf = SVC(kernel=\"rbf\", C=10, random_state=42, verbose=3)\nsvm_clf.fit(X_train_reduced, y_train)","ad76311e":"svm_clf.score(pca.transform(X_test), y_test)","700806d5":"pca = PCA(n_components=0.95)\n#X_train_reduced = pca.fit_transform(X_train)","38578dc0":"svm_clf.fit(pca.fit_transform(X), y)","df5d5ab9":"val = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\npred = svm_clf.predict(pca.transform(val))\n# ImageId,Label\n\nval['Label'] = pd.Series(pred)\nval['ImageId'] = val.index +1\nsub = val[['ImageId','Label']]","d2911977":"sub.to_csv('submission_updated1.csv', index=False)","ef44bb89":"import tensorflow as tf\nfrom tensorflow import keras","352d36f4":"from keras.preprocessing.image import ImageDataGenerator","8cdd048f":"X_train_scaled = X_train\/255\nX_test_scaled = X_test\/255","df1d1ef1":"#reshaping for data augmentation\nX_train_scaled = X_train_scaled.values.reshape(-1, 28, 28, 1)\nX_test_scaled = X_test_scaled.values.reshape(-1, 28, 28, 1)","d187c0e6":"datagen = ImageDataGenerator(\n        rotation_range=10,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        zoom_range=0.1)","3da49c93":"datagen.fit(X_train_scaled) # we do data augmentation of training images, leaving the test images untouched","6e22ab66":"training_set = datagen.flow(X_train_scaled, y_train)","b4379f2c":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense, Dropout\n\nmodel = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=5, input_shape=(28, 28, 1), activation=\"relu\"))\nmodel.add(Conv2D(32, kernel_size=5, activation=\"relu\"))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\nmodel.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\nmodel.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(Dropout(0.4))\n\nmodel.add(Flatten())\nmodel.add(Dense(300, activation = \"relu\"))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(10, activation = \"softmax\"))","5bd9c8c5":"# now compiling and training my CNN\n# we mention our sparse_categorical as it works on integers\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate = 0.001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])","5b294c0e":"#callbacks_es = keras.callbacks.EarlyStopping(patience=10)\ncallbacks_cp = keras.callbacks.ModelCheckpoint(filepath=\"\/kaggle\/working\/mykeras_model.h5\", save_best_only=True)","df724ee4":"history = model.fit(training_set, validation_data = (X_test_scaled, y_test), epochs=30, verbose=2, callbacks=[callbacks_cp])","02eaa4d9":"val = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","fa6312cf":"val_scaled = val\/255\nval_scaled = val_scaled.values.reshape(-1, 28, 28, 1)\nval_scaled.shape","2b5858cc":"pred = model.predict(val_scaled)\n\nresult = np.argmax(pred, axis=1)\nresult1 = pd.Series(result, name=\"Label\")\n#result['ImageId'] = val.index +1\nsub = pd.concat([pd.Series(range(1, 28001), name=\"ImageId\"), result1], axis=1)","1cd37c90":"sub.to_csv('submission_updated4.csv', index=False)","4b771418":"# trying to tune more hyperparameters using keras tuner","53daf46a":"!pip install -q -U keras-tuner","ca5c779e":"import kerastuner as kt","1d7aefb4":"def model_builder(hp):\n    model = keras.models.Sequential()\n\n    model.add(Conv2D(32, kernel_size=5, input_shape=(28, 28, 1), activation=\"relu\"))\n    model.add(Conv2D(32, kernel_size=5, activation=\"relu\"))\n    model.add(MaxPooling2D(pool_size=2))\n\n    model.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\n    model.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\n    model.add(MaxPooling2D(pool_size=2, strides=2))\n\n    model.add(Flatten())\n    hp_units = hp.Int('units', min_value=150, max_value=300, step=50)\n    model.add(Dense(units = hp_units, activation = \"relu\"))\n    model.add(Dense(10, activation = \"softmax\"))\n\n    hp_lr = hp.Choice('learning_rate', values=[1e-1, 1e-2, 1e-3])\n    my_optim = keras.optimizers.Adam(learning_rate = hp_lr)\n    model.compile(optimizer=my_optim, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n    \n    return model","5ae88793":"tuner = kt.Hyperband(model_builder,\n                     objective='val_accuracy',\n                     max_epochs=10)","62c0ec69":"stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\ntuner.search(training_set, validation_data = (X_test_scaled, y_test), epochs=30, verbose=2, callbacks=[stop_early])","128db067":"best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]","04fc053d":"print(best_hps.get('learning_rate'))","96ccb068":"print(best_hps.get('units'))","b36e70fc":"Let us try to increase accuracy of knn by hyperparameter tuning of k value","e8616d73":"# Importing Dataset","3d39770e":"Let us try with some better classification model with higher accuracy and f1 score","35b4313c":"1. SGD Classifier","fc16e8f5":"They are equal so everything seems almost fine and we are good to go with our EDA and learning models.","2c26daa0":"# Splitting of Training and Validation datasets","5b3943c2":"# Trying to go beyond 98.5% - 99% accuracy with CNN","7b239fbe":"3. K-Nearest Neighbors Classifier","999e6359":"There is no null or missing value present in the training dataset","ef4c230f":"4. SVM Classification","ed37d867":"Visualizng accuracy on different k values in KNN, we can see it stays same from k 3 to 6 which is around 96.3% and then it drops sharply for higher k","42af3868":"Confusion Matrix to look into false positive and false negative predictions","3510e39f":"**GridSearchCV to try out KNN at different hyperparameters**","180225e7":"# Building an ensemble with the above classifiers","fa3ae1b1":"# EDA","dfa1c994":"# Constructing a digit from pixel values","19d8d510":"We have used Keras Tuner to find out the best hyperparameters for our CNN. It found out 300 units in the Dense layer and learning rate of Adam optimizer = 0.001","a04350b6":"Now pred has probability for each class, but we need to consider the max one","a54347ea":"2. Random Forest Classifier","6f0f6944":"Checking missing or null values","540b1f78":"# Classification Models","1b37d0f4":"We can visualize that dataset has similar count of values for all digits from 0 to 9","fb790419":"Prediction and Recall scores","305a1f23":"**GridSearchCV to try out SVC at different hyperparameters of C**","ed269414":"From the confusion matrix we can see a lot of digits as false positives and negatives","cdd62429":"It is given that pixelx is located on row i and column j of a 28 x 28 matrix\nSo, we get 28 x 28 = 784 after excluding labels (actual number values)"}}