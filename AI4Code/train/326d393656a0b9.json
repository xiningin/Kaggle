{"cell_type":{"b36510dd":"code","2101253e":"code","b1371c75":"code","b9ae5a0c":"code","befac203":"code","ab186e15":"code","6b24062e":"code","bc322bb8":"code","9f983f08":"code","192793ff":"code","ba8747fa":"code","1d0e04f1":"code","3b680840":"code","cb62de76":"code","5e5d27f8":"code","4612d80b":"code","e70af0a1":"code","bc31ce5c":"code","af2ab86d":"code","b9cdf280":"code","2c5f8b94":"code","2593dfd7":"code","ebaa8caa":"code","0f79892b":"code","662d7de8":"code","4dc8d4ed":"code","6edf7921":"code","12569667":"code","0e0120e8":"code","dd465d28":"code","2286810d":"code","8141db8b":"code","9d893a2a":"code","dbd42c8c":"code","86cc5969":"code","eeeaea03":"code","f0778042":"code","cb478c8c":"code","dce682b0":"code","ffedb0d2":"code","7ce99a82":"code","6a205d67":"code","c5f0fddc":"code","37b94558":"code","9daeb9bd":"code","8c8e2ab9":"code","7208d552":"code","d04ec1ae":"code","fdd88095":"code","02ad7680":"code","274e7a72":"code","e955f4ce":"code","52f7d1cc":"code","d8728b6b":"code","b9a342d5":"code","95353c23":"code","e457ca1d":"code","0c09a9f3":"markdown","96d42898":"markdown","9492a9cb":"markdown","ab4311ef":"markdown","72448b58":"markdown","eac00d8b":"markdown","68c1d6bd":"markdown"},"source":{"b36510dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2101253e":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport time\n\nfrom tqdm import tqdm\nfrom datetime import datetime\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, recall_score, average_precision_score, f1_score\nfrom sklearn.metrics import log_loss\n\nfrom imblearn.over_sampling import SMOTE\n\nfrom catboost import CatBoostClassifier","b1371c75":"# Importing tensorflow\nimport tensorflow as tf\nfrom tensorflow.keras import layers","b9ae5a0c":"# \uc635\uc158 \uc124\uc815\npd.options.display.max_columns = 50\n\n# Random seed fixed\nnp.random.seed(2)\ntf.compat.v1.set_random_seed(2)","befac203":"# \uc815\uc0c1 \uc0c1\ud0dc \ub370\uc774\ud130\uc14b \ubd88\ub7ec\uc624\uae30\ndata = pd.read_csv(\"\/kaggle\/input\/swfc-project-phm-data\/train\/01_M01_DC_train.csv\")\ndata","ab186e15":"# \uace0\uc7a5 \uc0c1\ud0dc \ub370\uc774\ud130\uc14b \ubd88\ub7ec\uc624\uae30\ndata_fault = pd.read_csv(\"\/kaggle\/input\/swfc-project-phm-data\/train\/train_faults\/01_M01_train_fault_data.csv\")\ndata_fault","6b24062e":"# \uc815\uc0c1 \ub370\uc774\ud130\uc14b, \uace0\uc7a5 \ub370\uc774\ud130\uc14b\uc744 \ud558\ub098\uc758 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ubcc0\ud658\nall_data = pd.merge(data.drop('Tool', axis = 1), data_fault.drop('Tool', axis = 1), how = 'left', on = 'time')\ndel data, data_fault\nall_data","bc322bb8":"# \uace0\uc7a5(fault) \ub370\uc774\ud130 \ub2e8\uc21c\ud654 -> fault \uc885\ub958\uc5d0 \uad00\uacc4\uc5c6\uc774 0(\uc815\uc0c1)\/1(\uace0\uc7a5)\ub85c \ub77c\ubca8\ub9c1\nfault_index = all_data[all_data['fault_name'].notnull()].index\nfault_index\nall_data['fault'] = 0\nall_data.loc[fault_index, 'fault'] = 1\nall_data = all_data.drop('fault_name', axis=1)\ndisplay(all_data.shape, all_data['fault'].value_counts())","9f983f08":"# time \uce7c\ub7fc 0 \uae30\uc900\uc73c\ub85c \uc815\ub9ac\nall_data['time'] = all_data['time'] - all_data.loc[0, 'time']\nall_data","192793ff":"# \uce7c\ub7fc\ubcc4 Data Type \ud655\uc778\nall_data.dtypes","ba8747fa":"# Data Description\uacfc \uc77c\uce58\ud558\ub3c4\ub85d \uce7c\ub7fc \ub370\uc774\ud130 \ud0c0\uc785 \ubcc0\ud658\ncat_cols = ['stage', 'Lot', 'recipe', 'recipe_step', 'fault']\n\nfor cols in cat_cols :\n    all_data[cols] = all_data[cols].astype('object')\n\nall_data.dtypes","1d0e04f1":"# categorical column value_counts \ud655\uc778\nfor cols in cat_cols :\n    display(all_data[cols].value_counts()[:5]) # \uc0c1\uc704 5\uac1c\ub9cc","3b680840":"# \uce7c\ub7fc\ubcc4 \ud1b5\uacc4\ub7c9 \ud655\uc778\nall_data.loc[:, all_data.dtypes != 'object'].describe().T","cb62de76":"# \uce7c\ub7fc \ubcc4 NaN \ud655\uc778\nall_data.isnull().sum()","5e5d27f8":"# recipe_step, FIXTURESHUTTERPOSITION \uce7c\ub7fc\uc758 NaN \ud589 \ud655\uc778\ndisplay(all_data[all_data['recipe_step'].isnull()], all_data[all_data['FIXTURESHUTTERPOSITION'].isnull()])\n\n# all_data.iloc[1242940:1242950, :] # \uadfc\ucc98 \ub370\uc774\ud130 \uac12\uacfc \ud568\uaed8 \ud655\uc778","4612d80b":"# recipe_step, FIXTURESHUTTERPOSITION \uce7c\ub7fc\uc758 \uacb0\uce21 \ud589 \uc81c\uac70\nall_data = all_data.dropna(subset = ['recipe_step', 'FIXTURESHUTTERPOSITION'])\n\n# \uc778\ub371\uc2a4 \ucd08\uae30\ud654\nall_data = all_data.reset_index(drop = True)\n\ndisplay(all_data.shape, all_data.isnull().sum().sum())","e70af0a1":"# \uce7c\ub7fc \ub370\uc774\ud130 \ud0c0\uc785 \ubcc0\ud658(\ucd95\uc18c) --> RAM \uc5ec\uc720 \uacf5\uac04 \ud655\ubcf4\uc704\ud574\nfor cols in all_data.columns.values :\n    if all_data[cols].dtypes == 'int64' :\n        if (cols == 'runnum') or (cols == 'time'):\n            continue\n        all_data[cols] = all_data[cols].astype('int16')\n    elif all_data[cols].dtypes == 'float64' :\n        all_data[cols] = all_data[cols].astype('float16')\n\nall_data.dtypes","bc31ce5c":"# \uce74\ud14c\ucf54\ub9ac \uce7c\ub7fc Label Encoding\nfor cols in cat_cols :\n    le = LabelEncoder()\n    all_data[cols] = le.fit_transform(list(all_data[cols])).astype('int16')\n\nall_data.dtypes","af2ab86d":"# \ub370\uc774\ud130 \ud655\uc778\nall_data","b9cdf280":"# \uc0c1\uad00\ubd84\uc11d - Heatmap \uc774\uc6a9\ndata_corr = all_data.corr()\ndata_corr = data_corr.apply(lambda x : round(x, 3))\nplt.figure(figsize=(16, 16))\nsns.heatmap(data_corr, annot=True,cmap='Blues')\nplt.title('Heatmap', fontsize=20)\nplt.show()","2c5f8b94":"all_data_2 = all_data.copy() # \ub4a4\uc5d0\uc11c \uc774\uc6a9\n\ndel data_corr\nall_data_2.shape","2593dfd7":"# Scaling\nscaler = StandardScaler()\nall_data_sc = all_data.iloc[:, :-1]\nall_data_sc = scaler.fit_transform(all_data_sc)\nall_data.iloc[:, :-1] = all_data_sc\n\ndel all_data_sc\nall_data","ebaa8caa":"# \ube60\ub978 \ud559\uc2b5\uc744 \uc704\ud574 \ub370\uc774\ud130 \ucd95\uc18c (\uc57d 3,000,000\uac1c \uc911 150,000\uac1c\ub9cc \uc774\uc6a9)\ndf_fault = all_data[all_data['fault'] == 1]\ndf_normal = all_data[all_data['fault'] == 0].sample(150000-8, random_state=2)\n\nall_data_train = pd.concat([df_normal, df_fault])\n\ndel df_fault, df_normal\nall_data_train","0f79892b":"# \ud559\uc2b5\uc744 \uc704\ud574 \ub79c\ub364\ud558\uac8c 50,000\uac1c\uc758 '\uc815\uc0c1' \ub370\uc774\ud130 \ucd94\ucd9c\ndf_train = all_data_train[all_data_train['fault'] == 0].sample(50000, random_state=2)\ndf_train.tail(3)","662d7de8":"# \ubf51\ud78c \uc0d8\ud50c\ub4e4\uc744 all_data\uc5d0\uc11c \uc81c\uac70\nindex_list = df_train.index\nall_data_train = all_data_train.drop(index_list)\nall_data_train.shape","4dc8d4ed":"# Label column \uc81c\uac70\ndf_train = df_train.drop('fault',axis=1)\ndf_train.shape","6edf7921":"## GAN\n# generator \uc815\uc758\ndef make_generator_model():\n\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(16, use_bias=False, input_shape=(23, ))) ## noise dim\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Dense(16))\n    assert model.output_shape == (None, 16) # Note: None is the batch size\n    \n    \n    model.add(layers.Dense(32))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Dense(32))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Dense(23))\n    assert model.output_shape == (None, 23)\n    \n    \n    return model","12569667":"# discriminator \uc815\uc758\ndef make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(16, use_bias=False,\n                                    input_shape=[1, 23]))\n   \n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.2))\n\n    model.add(layers.Dense(32, use_bias=True))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.2))\n\n   \n    model.add(layers.Dense(1, activation='sigmoid')) # 0~1 \uc0ac\uc774 \uacb0\uacfc\uac12 \ucd9c\ub825\n    # model.add(layers.Softmax()) # softmax\ub3c4 \uc0ac\uc6a9 \uac00\ub2a5\n\n    return model","0e0120e8":"# \ubaa8\ub378 \uc120\uc5b8\ngenerator = make_generator_model()\ndiscriminator = make_discriminator_model()","dd465d28":"# cross_entropy \uc774\uc6a9\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)","2286810d":"# discriminator_loss \ud568\uc218 \uc815\uc758\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss","8141db8b":"bce = tf.keras.losses.BinaryCrossentropy()\nloss = bce([1., 1., 1., 1.], [1., 1., 1., 1.])\nprint('Loss: ', loss.numpy())","9d893a2a":"# generator_loss \ud568\uc218 \uc815\uc758\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","dbd42c8c":"# Optimizer\ub85c\ub294 Adam \uc774\uc6a9\ngenerator_optimizer = tf.keras.optimizers.Adam(0.001)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(0.001)","86cc5969":"# Parameter \uc124\uc815\nEPOCHS = 100 # \ubc18\ubcf5 \ud69f\uc218\nnoise_dim = 23 # noise\uc758 \ucc28\uc6d0\n# num_examples_to_generate = 16\nBATCH_SIZE = 64","eeeaea03":"# train_step \ud568\uc218 \uc815\uc758\n@tf.function\ndef train_step(data_frame):\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n    data_frame = tf.reshape(data_frame,(1, 23))\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_data_frame = generator(noise, training = True)\n        \n        real_output = discriminator(data_frame, training = True)\n        fake_output = discriminator(generated_data_frame, training = True)\n        \n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n     \n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n    \n    return (gen_loss, disc_loss)","f0778042":"# train \ud568\uc218 \uc815\uc758\nhistory = dict()\nhistory['gen_loss'] = []\nhistory['dis_loss'] = []\n\ndef train(dataset, epochs):\n    for epoch in range(epochs):\n        start = time.time()\n\n        for batch in dataset:\n            gen_loss,dis_loss = train_step(batch)\n            \n        history['gen_loss'].append(gen_loss)\n        history['dis_loss'].append(dis_loss)\n        print ('Time for epoch {} is {} sec '.format(epoch + 1, time.time()-start))","cb478c8c":"# \ud559\uc2b5\uc744 \uc704\ud574 Data Frame\uc5d0\uc11c \uac12\ub9cc \uac00\uc838\uc624\uae30\nx_train = df_train.values\nx_train","dce682b0":"# GAN \ud559\uc2b5\ntrain(x_train, EPOCHS)","ffedb0d2":"# generator loss\nplt.figure(figsize=(18, 6))\nplt.plot(history['gen_loss'])\nplt.show()","7ce99a82":"# discriminator_loss\nplt.figure(figsize=(18, 6))\nplt.plot(history['dis_loss'])\nplt.show()","6a205d67":"# test data \uc124\uc815 (100,000\uac1c\uc758 \ub370\uc774\ud130 \uc911 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ud55c 30,000\uac1c\ub97c \uc81c\uc678\ud55c 70,000\uac1c\uc758 \ub370\uc774\ud130)\ny_test = all_data_train['fault']\nx_test = all_data_train.drop('fault', axis = 1).values.reshape(-1, 1, 23)","c5f0fddc":"# discriminator\ub85c test data\uc5d0 \ub300\ud55c \uc608\uce21\uac12 \uc0dd\uc131\ny_pred = discriminator.predict(x_test, workers=-1)\ny_pred","37b94558":"# Output value\uc758 \ubc94\uc704 \ud655\uc778\nplt.figure(figsize = (18, 6))\nplt.plot(range(len(y_pred)),y_pred.reshape(-1, 1))\nplt.ylabel('y_pred')\nplt.show()","9daeb9bd":"# \uc801\uc808\ud55c cutoff value(probability threshold) \uac80\uc0c9\nprob = [0.2, 0.3, 0.4, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]\nfor p in prob:\n    pred_value =[1 if i > p else 0 for i in y_pred]\n    f1 = f1_score(y_test, pred_value)\n    acc = accuracy_score(y_test, pred_value)\n    preci = average_precision_score(y_test, pred_value)\n    recall = recall_score(y_test, pred_value)\n    print(\"prob = {:.2f} :\\nF1-score = {:.2f} Accuracy = {:.2f} Precision = {:.2f} Recall = {:.2f}\".format(p, f1, acc, preci, recall))\n    print(pred_value.count(0), pred_value.count(1))\n\n# \uc124\ube44 \uace0\uc7a5\uacfc \uad00\ub828\ud558\uc5ec\uc11c\ub294 Recall \uac12\uc774 \uac00\uc7a5 \uc911\uc694.","8c8e2ab9":"del all_data, x_train, x_test, df_train, y_test \ngc.collect()","7208d552":"## \uc88b\uc740 \ubc29\ubc95\uc740 \uc544\ub2c8\uc9c0\ub9cc, Fault \ub370\uc774\ud130\uac00 \ub108\ubb34 \ubd80\uc871\ud558\uc5ec \uc815\uc0c1\uc801\uc73c\ub85c \ubaa8\ub378 \ud559\uc2b5\uc774 \ub418\uc9c0\uc54a\uc544 \uc784\uc758\ub85c \ub370\uc774\ud130\ub97c \ub9cc\ub4e4\uc5b4\uc11c \ubd84\uc11d\n## \uc6b0\uc120 SMOTE \uc54c\uace0\ub9ac\uc998\uc744 \uc774\uc6a9\ud558\uc5ec \ub9ce\uc740\uc591\uc758 \uace0\uc7a5 \ub370\uc774\ud130\ub97c \uc0dd\uc131\ud55c\ub2e4\uc74c \uc774\ub97c \uc774\uc6a9\ud558\uc5ec \ubaa8\ub378 \ud559\uc2b5 \ubc0f \ud3c9\uac00 \n## \ubaa8\ub378\uc740 \ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd\uc774 \ube44\uad50\uc801 \ubd88\ud544\uc694\ud558\uace0, GPU\ub97c \uc774\uc6a9\ud574 \ube60\ub974\uac8c \ud559\uc2b5\ud560 \uc218 \uc788\ub294 Catboost \ubaa8\ub378 \uc774\uc6a9\n## Cross Validation \uacfc\uc815\uc744 \uac70\uccd0 Feature Importance\ub97c \ubd84\uc11d\ud558\uc5ec, \uc5b4\ub5a4 Feature\uac00 Fault\ub97c \uc608\uce21\ud558\ub294\ub370 \uac00\uc7a5 \ud070 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294\uc9c0 \ud655\uc778","d04ec1ae":"# \uc0ac\uc6a9\ud560 \ub370\uc774\ud130 \ucd94\ucd9c (1,500,000 \uac1c)\ndf_fault = all_data_2[all_data_2['fault'] == 1]\ndf_normal = all_data_2[all_data_2['fault'] == 0].sample(1500000-8, random_state=10)\n\nall_data_3 = pd.concat([df_normal, df_fault])\nall_data_3 = all_data_3.reset_index(drop = True)\n\nall_data_3","fdd88095":"# SMOTE \ubaa8\ub378 \uc124\uc815\nsm = SMOTE(sampling_strategy='auto', k_neighbors=3, random_state=10)\nsm","02ad7680":"# X \/ Y \ubd84\ub9ac\nX_train = all_data_3.iloc[:, :-1]\ny_train = all_data_3.iloc[:, -1]\nX_train","274e7a72":"# SMOTE\ub97c \ud1b5\ud574 \ub370\uc774\ud130 \ubcf5\uc81c (Oversampling)\nX_train_sm, y_train_sm = sm.fit_sample(X_train, list(y_train))\ny_train_sm = pd.Series(y_train_sm)\ny_train_sm.name = 'fault'\n\nprint('SMOTE \uc801\uc6a9 \uc804 \ud559\uc2b5\uc6a9 \ud53c\ucc98\/\ub808\uc774\ube14 \ub370\uc774\ud130 \uc138\ud2b8: ', X_train.shape, y_train.shape)\nprint('SMOTE \uc801\uc6a9 \ud6c4 \ud559\uc2b5\uc6a9 \ud53c\ucc98\/\ub808\uc774\ube14 \ub370\uc774\ud130 \uc138\ud2b8: ', X_train_sm.shape, len(y_train_sm))\nprint('SMOTE \uc801\uc6a9 \ud6c4 \ub808\uc774\ube14 \uac12 \ubd84\ud3ec: \\n', y_train_sm.value_counts())","e955f4ce":"# \ucd1d \uc57d 3,000,000\uac1c\uc758 \ub370\uc774\ud130\nall_data_3 = pd.concat([X_train_sm, y_train_sm], axis=1)\n\ndel all_data_2\nall_data_3","52f7d1cc":"# X \/ Y \ub2e4\uc2dc \ubd84\ub9ac\nX_train = all_data_3.iloc[:, :-1]\ny_train = all_data_3.iloc[:, -1]\n\ndel all_data_3\nX_train","d8728b6b":"# tuner \ud568\uc218 \uc815\uc758\ndef timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod(\n            (datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' %\n              (thour, tmin, round(tsec, 2)))","b9a342d5":"# Catboost Cross Validation\n\nfolds = 5\n\ncat_cv_sum = 0\ncat_pred = []\ncat_fpred = []\n\navreal = y_train\n\nfeature_importances = pd.DataFrame()\n\ntrain_time = timer(None)\nkf = KFold(n_splits=folds, random_state=10, shuffle=True)\nfor i, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n    start_time = timer(None)\n    Xtrain, Xval = X_train.iloc[train_index], X_train.iloc[val_index]\n    ytrain, yval = y_train.iloc[train_index], y_train.iloc[val_index]\n\n    model = CatBoostClassifier(loss_function = 'CrossEntropy',\n                               eval_metric = 'Recall',\n                               task_type = \"GPU\",\n                               random_state = 10,\n                               iterations = 300,\n                               verbose = 100\n                               )\n    \n    model.fit(Xtrain, ytrain, eval_set=(Xval, yval), early_stopping_rounds=100)\n    \n    feature_importances = feature_importances.append(pd.Series(model.feature_importances_), ignore_index=True)\n              \n    cat_scores_val = model.predict(Xval)\n    cat_recall = recall_score(yval, cat_scores_val)\n    print('\\n Fold %02d cat Recall: %.6f' % ((i + 1), cat_recall))\n\n    del Xtrain, Xval\n    gc.collect()\n\n    timer(start_time)\n\n    cat_pred = cat_fpred\n    cat_cv_sum = cat_cv_sum + cat_recall\n\ntimer(train_time)\n\ncat_cv_score = (cat_cv_sum \/ folds)\n\nprint('\\n Average cat Recall:\\t%.6f' % cat_cv_score)\ncat_score = round(cat_cv_score, 6)","95353c23":"# Feature Importnce data frame \uc0dd\uc131\nfeature_importances.columns = model.feature_names_\nfeature_importances = feature_importances.sum()\nfeature_importances = pd.DataFrame(feature_importances.sort_values(ascending = False))\nfeature_importances = feature_importances.reset_index()\nfeature_importances.columns = ['Feature', 'Importance']\nfeature_importances","e457ca1d":"# Feature Importances \ud655\uc778\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances, x='Importance', y='Feature')\nplt.title('Feature Importancess', fontsize=20)\nplt.show()","0c09a9f3":"## Simple EDA \/ Data Preprocessing","96d42898":"## Anomaly Detection using GAN","9492a9cb":"## Library Importing","ab4311ef":"## Feature Importance Analysis using Catboost","72448b58":"## Data Join","eac00d8b":"## Data Load","68c1d6bd":"## SWFC \uc0dd\uc0b0\uc18c\ud504\ud2b8\uc6e8\uc5b4 3\ubc18 \ub370\uc774\ud130 \ucc98\ub9ac \ud504\ub85c\uc81d\ud2b8\n## Etching Data Anomaly Detection & Feature Importance Analysis\n## with GAN and Catboost"}}