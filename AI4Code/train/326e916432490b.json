{"cell_type":{"bce99b77":"code","458a225b":"code","87c4d0cb":"code","57b40c00":"code","b308f120":"code","053ae0d1":"code","2d99ec1a":"code","f08fd8eb":"code","83367a42":"code","eeb2ff0e":"code","fc69c4d9":"code","41ff1410":"code","17f5b5bf":"code","96f36411":"code","449b92e5":"code","7ddc53fe":"code","8ddc653f":"code","88d1870e":"code","973f97bf":"code","e9670908":"code","c7996203":"code","aa3778f7":"code","b172ac0d":"code","fb6997bb":"code","7116830e":"code","d52f9237":"code","c7f88c0c":"code","bb7f47d6":"code","a911556c":"code","febccdd3":"code","d8609c90":"code","1f2951ad":"code","89266b40":"code","818c490b":"markdown","dc298e90":"markdown","262ed04e":"markdown","1577883b":"markdown","b5e631d3":"markdown","ee341ad9":"markdown","ce93bbb3":"markdown","9154d319":"markdown","b16fa6c5":"markdown","6a493954":"markdown","5fdc9e07":"markdown","1387728d":"markdown","754eae32":"markdown","fce5d41c":"markdown"},"source":{"bce99b77":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport plotly.offline as py\nimport plotly.express as px\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","458a225b":"df = pd.read_csv('..\/input\/\/data-lukas2\/datasets_684561_1212154_Rank_by_Country_V6.csv', encoding='ISO-8859-2')\ndf.head()","87c4d0cb":"fig,axes = plt.subplots(1,1,figsize=(20,5))\nsns.heatmap(df.isna(),yticklabels=False,cbar=False,cmap='viridis')\nplt.show()","57b40c00":"# filling missing values with NA\ndf[['Legal Status', 'Adults without an account with Mobile phone and internet  (%), 2017', 'Adults without an account and without Mobile phone and internet  (%), 2017', 'Adults without an account owning a mobile phone (in millions)', 'Adults without an account owning a mobile phone (in millions).1']] = df[['Legal Status', 'Adults without an account with Mobile phone and internet  (%), 2017', 'Adults without an account and without Mobile phone and internet  (%), 2017', 'Adults without an account owning a mobile phone (in millions)', 'Adults without an account owning a mobile phone (in millions).1']].fillna('NA')","b308f120":"df.plot(subplots=True, figsize=(10, 10), sharex=False, sharey=False)\nplt.show()","053ae0d1":"plt.style.use('fivethirtyeight')\nsns.countplot(df['Continent'],linewidth=3,palette=\"Set2\",edgecolor='black')\nplt.show()","2d99ec1a":"df['Rank'].hist(figsize=(10,5), bins=20)","f08fd8eb":"sns.countplot(x=\"Legal Status\",data=df,palette=\"GnBu_d\",edgecolor=\"black\")\nplt.xticks(rotation=45)\nplt.yticks(rotation=45)\n# changing the font size\nsns.set(font_scale=1)","83367a42":"ax = df['Continent'].value_counts().plot.barh(figsize=(14, 6))\nax.set_title('Continent Distribution', size=18)\nax.set_ylabel('Continent', size=14)\nax.set_xlabel('Rank', size=14)","eeb2ff0e":"import matplotlib.ticker as ticker\nax = sns.distplot(df['Rank'])\nplt.xticks(rotation=45)\nax.xaxis.set_major_locator(ticker.MultipleLocator(2))","fc69c4d9":"from category_encoders import OneHotEncoder\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n\ncols_selected = ['Continent']\nohe = OneHotEncoder(cols=cols_selected, use_cat_names=True)\ndf_t = ohe.fit_transform(df[cols_selected+['Rank']])\n\n#scaler = MaxAbsScaler()\nX = df_t.iloc[:,:-1]\ny = df_t.iloc[:, -1].fillna(df_t.iloc[:, -1].mean()) \/ df_t.iloc[:, -1].max()\n\nmdl = Ridge(alpha=0.1)\nmdl.fit(X,y)\n\npd.Series(mdl.coef_, index=X.columns).sort_values().head(10).plot.barh()","41ff1410":"fig = px.pie(df,\n             values=\"Rank\",\n             names=\"Continent\",\n             template=\"seaborn\")\nfig.update_traces(rotation=90, pull=0.05, textinfo=\"percent+label\")\nfig.show()","17f5b5bf":"fig = px.bar(df[['Continent', 'Rank']].sort_values('Rank', ascending=False), \n             y=\"Rank\", x=\"Continent\", color='Continent', \n             log_y=True, template='ggplot2', title='Mobile Phone & Internet by Continent')\nfig.show()","96f36411":"df.groupby(['Rank'])['Continent'].value_counts(normalize=True)","449b92e5":"df[['Rank','Continent']].head()","7ddc53fe":"Feature = df[['Rank','Continent']]\nFeature = pd.concat([Feature,pd.get_dummies(df['Continent'])], axis=1)\nFeature.drop(['Continent'],\naxis = 1,inplace=True)\nFeature.head()","8ddc653f":"X = Feature\nX[0:5]","88d1870e":"y = df['Rank'].values\ny[0:5]","973f97bf":"def foo():\n    X= preprocessing.StandardScaler().fit(X).transform(X)\n    X[0:5]\n    return self.partial_fit(X,y)","e9670908":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report,confusion_matrix\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)","c7996203":"error_rate = []\n\n# Will take some time\nfor i in range(1,35):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i!=y_test))","aa3778f7":"plt.figure(figsize=(10,6))\nplt.plot(range(1,35),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","b172ac0d":"    ##for k=9....acc to the above graph\n    \n    knn = KNeighborsClassifier(n_neighbors=9)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    print('CONFUSION_MATRIX :\\n')\n    print(confusion_matrix(pred_i,y_test))\n    print('\\n')\n    print('REPORT :\\n')\n    print(classification_report(pred_i,y_test))","fb6997bb":"print('ACCURACY :')\n\nmetrics.accuracy_score(pred_i,y_test)","7116830e":"from sklearn.tree import DecisionTreeClassifier\ndc = DecisionTreeClassifier()\nclassification = dc.fit(X_train,y_train)\nprediction = dc.predict(X_test)\nprediction","d52f9237":"print('CONFUSION_MATRIX :\\n')\n\nprint(confusion_matrix(prediction,y_test))","c7f88c0c":"print('REPORT :\\n')\nprint(classification_report(prediction,y_test))","bb7f47d6":"print('ACCURACY :\\n')\nmetrics.accuracy_score(prediction,y_test)","a911556c":"from sklearn import svm\nsvc = svm.SVC(kernel='rbf')\nsvc.fit(X_train,y_train)\ny_pred = svc.predict(X_test)\ny_pred","febccdd3":"print('CLASSIFICATION_REPORT :\\n')\nprint(metrics.classification_report(y_pred,y_test))","d8609c90":"from sklearn.linear_model import LogisticRegression\nlinear = LogisticRegression(fit_intercept=True)\nlinear.fit(X_train,y_train)","1f2951ad":"y_pred = linear.predict(X_test)\ny_pred","89266b40":"print('CLASSIFICATION_REPORT :\\n');\nprint(metrics.classification_report(y_pred,y_test))","818c490b":"Feature before Encoding","dc298e90":"Kaggle Notebook Runner: Mar\u00edlia Prata   @mpwolke","262ed04e":"Oh Lord! I suck. Maybe next time.","1577883b":"The original range below was (1,40). I changed to fit with this data.","b5e631d3":"#Codes from  Ashutosh Varma  https:\/\/www.kaggle.com\/ashutoshvarma\/coursera-ibm-project","ee341ad9":"SVM - Support Vector Machine","ce93bbb3":"Data Standardization give data zero mean and unit variance (technically should be done after train test split )","9154d319":"Use one hot encoding technique to convert categorical varables to binary variables and append them to the feature Data Frame","b16fa6c5":"Feature Selection","6a493954":"Logistic Regression","5fdc9e07":"Classification (I my case just trying to make it).","1387728d":"#OH Encoding","754eae32":"KNN","fce5d41c":"Decision Tree"}}