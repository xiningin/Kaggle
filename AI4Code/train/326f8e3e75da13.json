{"cell_type":{"4cea9da9":"code","dc4e4819":"code","4d32bb03":"code","2f28db30":"code","2a98c863":"code","f4c175fb":"markdown","2bd408bc":"markdown"},"source":{"4cea9da9":"plato = open(\"..\/input\/the-republic-by-plato\/1497.txt\", \"r\").read()\nprint (plato[:53])\n\n# http:\/\/www.gutenberg.org\/cache\/epub\/730\/pg730.txt\noliver_twist = open(\"..\/input\/dickens\/dickens\/pg730.txt\", \"r\").read()\nprint (oliver_twist[:64])\n\nchristmas_carol = open(\"..\/input\/dickens\/dickens\/pg19337.txt\", \"r\").read()\nprint (christmas_carol[:69])\n\ntext = oliver_twist","dc4e4819":"import string\n\n# turn a doc into clean tokens\ndef clean_doc(doc):\n    # replace '--' with a space ' '\n    doc = doc.replace('--', ' ')\n    # split into tokens by white space\n    tokens = doc.split()\n    # remove punctuation from each token\n    tokens = [' ' if w in string.punctuation else w for w in tokens]\n    # remove remaining tokens that are not alphabetic\n    tokens = [word for word in tokens if word.isalpha()]\n    # make lower case\n    tokens = [word.lower() for word in tokens]\n    return tokens\n \ntokens = clean_doc(text)\n\nnumber_of_unique_tokens = len(set(tokens))\n\nprint('Total Tokens: %d' % len(tokens))\nprint('Unique Tokens: %d' % number_of_unique_tokens)\nprint('These are the first 200 tokens: %s' % tokens[:200])\n\n# A key design decision is how long the input sequences should be. \n# They need to be long enough to allow the model to learn the context for the words to predict. \n# This input length will also define the length of seed text used to generate new sequences \n# when we use the model.\n# There is no correct answer. With enough time and resources, we could explore the ability of \n# the model to learn with differently sized input sequences.\n\nsequence_length = 2\n\n# organize into sequences of tokens of input words plus one output word\nlength = sequence_length + 1\nsequences = list()\nfor i in range(length, len(tokens)):\n    # select sequence of tokens\n    seq = tokens[i-length:i]\n    # convert into a line\n    line = ' '.join(seq)\n    # store\n    sequences.append(line)\n\nprint ('Total Sequences: %d' % len(sequences))\nprint ('This is the first sequence: {0}'.format(sequences[0]))","4d32bb03":"import numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\n \ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(sequences)\nsequences = tokenizer.texts_to_sequences(sequences)\n# vocab_size = len(tokenizer.word_index) + 1\nvocab_size = number_of_unique_tokens + 1\n \nsequences0 = np.array(sequences)\nX, y = sequences0[:,:-1], sequences0[:,-1]\ny = to_categorical(y, num_classes=vocab_size)\n\n# The learned embedding needs to know how many dimensions will be used to represent each word. \n# That is, the size of the embedding vector space. That is, the size of the embedding vector space.\n# Common values are 50, 100, and 300. Consider testing smaller or larger values.\ndimensions_to_represent_word = 100\n \nmodel = Sequential()\nmodel.add(Embedding(vocab_size, sequence_length, input_length=sequence_length))\n# We will use a two LSTM hidden layers with 100 memory cells each. \n# More memory cells and a deeper network may achieve better results.\nmodel.add(LSTM(100, return_sequences=True))\nmodel.add(LSTM(100))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(vocab_size, activation='softmax'))\nprint(model.summary())\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Training may take a few hours on modern hardware without GPUs. \n# You can speed it up with a larger batch size and\/or fewer training epochs.\n# model.fit(X, y, batch_size=128, epochs=100)\nmodel.fit(X, y, batch_size=128, epochs=100)","2f28db30":"print (X.shape)\nprediction = model.predict(X[0].reshape(1,sequence_length))\nprint (prediction.shape)\nprint (prediction)","2a98c863":"test = ['thank you',\n'welcome to',\n'when there',\n'more than',\n'it cannot',\n'is that',\n'although this',\n'do you',\n'I was',\n'the only',\n'a great']\n\nfor t in test:\n    example = tokenizer.texts_to_sequences([t])\n    prediction = model.predict(np.array(example))\n    predicted_word = np.argmax(prediction)\n    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))  # https:\/\/stackoverflow.com\/a\/43927939\/246508\n    print (\"{0} -> {1}\".format(t, reverse_word_map[predicted_word]))","f4c175fb":"Inspired from:\n\nhttps:\/\/machinelearningmastery.com\/how-to-develop-a-word-level-neural-language-model-in-keras\n\nhttps:\/\/www.kaggle.com\/fuzzyfroghunter\/statistical-language-modeling","2bd408bc":"Can you generate predictions on the next word?"}}