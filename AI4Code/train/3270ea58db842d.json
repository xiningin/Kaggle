{"cell_type":{"67e6b478":"code","674d73a0":"code","2679da83":"code","77433aee":"code","1faad84a":"code","a268e28d":"code","03b0cdb4":"code","756e8557":"code","82462f88":"code","cd2f89a6":"code","2789a472":"code","9f04eefe":"code","51cf01a0":"code","e1265edd":"code","aca0a17f":"code","60acbe41":"code","775d859e":"code","63c475e0":"code","0af07594":"code","18c43308":"code","f12bb637":"code","a4106aeb":"code","e2a30cc8":"code","476b9fa9":"code","736e37b5":"code","95576bf9":"code","10a59725":"code","e35f3324":"code","bb7ddbc1":"code","fbdb5701":"code","6bbed190":"code","ddf4a82f":"code","d7b7c8aa":"code","78412f7c":"code","be5e4d35":"code","114ef805":"code","15b817c4":"code","6ce46299":"code","2918a1ea":"code","17cd72c5":"code","ed9e8cb9":"code","e259d27e":"code","0426d163":"code","175bc983":"code","7f662aaf":"code","810e863c":"code","6a664ee2":"code","1ba7f392":"code","7c90fab9":"code","917653ee":"code","17b1e0d3":"code","85a138b8":"code","b32e05d2":"code","fafdfd33":"code","6a1a5e15":"code","818dadbb":"code","caba615b":"code","9f801092":"code","03656d74":"code","e5ded123":"code","74c96e75":"code","dd9b9b59":"code","184646c5":"code","516428e3":"code","d13973cc":"code","77b7d932":"code","a1794b82":"code","e4925aa3":"code","82557045":"code","3ba93c21":"code","d71c9327":"code","4ebc8480":"code","8169fc34":"code","92e9b42c":"code","4fc0cf8d":"code","27bb43e1":"code","5c09b3b0":"code","7ab7fe35":"code","f9f20b65":"code","c8bf7708":"code","f9ff69e9":"code","7e3935eb":"code","3d0a833c":"code","b84e66ca":"code","f604405d":"code","7dd70103":"code","0f3b91dc":"code","2729b201":"code","88c7adb6":"code","e3ba7a3e":"code","da96faaf":"code","3ce30326":"code","30bbaaf6":"code","49a86f8e":"code","5f9f5007":"code","f4f2bcae":"code","849bfd94":"code","366d2ca8":"code","1449e109":"code","27d09ce2":"code","639c43d8":"code","96995559":"code","a5016cd0":"code","b14cad3b":"code","db8b5f02":"code","d81e678a":"code","42a146e4":"code","0ab22314":"code","788a4806":"code","85be1618":"code","3aebe12e":"code","a6deac89":"code","38b390db":"code","b5d96ea0":"code","ffead406":"code","49e3fa64":"code","967d3ca1":"code","6ee1b808":"code","a96952b8":"code","9878fb38":"code","ca534fc1":"code","2baa55d9":"code","7c3caddb":"code","b05a5471":"code","992aa55e":"code","bc4d799c":"code","be6b6ceb":"code","ca3f3a21":"code","66e92d25":"markdown","710b40bb":"markdown","8b5f7853":"markdown","167c1d0a":"markdown","c5993678":"markdown","6e7e7676":"markdown","ca48831d":"markdown","626eb46b":"markdown","14091fec":"markdown","7989a1c4":"markdown","710866ad":"markdown","5377f93f":"markdown","b73c06d3":"markdown","f1d0cb0b":"markdown","8567f182":"markdown","aa0e44fa":"markdown","fe311c5d":"markdown","76f423db":"markdown","055fd8c7":"markdown","a0dc5535":"markdown","e8b3aee8":"markdown","f580556d":"markdown","9fe6e403":"markdown","56e0148e":"markdown","14504bb1":"markdown","8645bca5":"markdown","29bb5d4a":"markdown","8d676feb":"markdown","50cd503e":"markdown","2896b07a":"markdown","ceaeab79":"markdown","20687134":"markdown"},"source":{"67e6b478":"import pandas as pd\nimport numpy as np","674d73a0":"# Load the data\nhr_df = pd.read_csv( '\/kaggle\/input\/hr-data-for-analytics\/HR_comma_sep.csv' )","2679da83":"hr_df.columns","77433aee":"hr_df.info()","1faad84a":"#missings\nhr_df.isnull().any().sum()","a268e28d":"hr_df.describe().T","03b0cdb4":"hr_df.tail()","756e8557":"# Encoding Categorical Features\nnumerical_features = ['satisfaction_level', 'last_evaluation', 'number_project',\n     'average_montly_hours', 'time_spend_company']\n\ncategorical_features = ['Work_accident','promotion_last_5years', 'sales', 'salary']","82462f88":"# An utility function to create dummy variable\ndef create_dummies( df, colname ):\n    col_dummies = pd.get_dummies(df[colname], prefix=colname)\n    col_dummies.drop(col_dummies.columns[0], axis=1, inplace=True)\n    df = pd.concat([df, col_dummies], axis=1)\n    df.drop( colname, axis = 1, inplace = True )\n    return df","cd2f89a6":"for c_feature in categorical_features:\n    hr_df = create_dummies( hr_df, c_feature )","2789a472":"hr_df.head()","9f04eefe":"#Splitting the data\n\nfeature_columns = hr_df.columns.difference( ['left'] )\n#feature_columns1 = feature_columns","51cf01a0":"feature_columns","e1265edd":"from sklearn.model_selection import train_test_split\n\n\ntrain_X, test_X, train_y, test_y = train_test_split( hr_df[feature_columns],\n                                                  hr_df['left'],\n                                                  test_size = 0.3,\n                                                  random_state = 123 )","aca0a17f":"# Building Models\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit( train_X, train_y)","60acbe41":"logreg.predict(train_X)   #by default, it use cut-off as 0.5","775d859e":"list( zip( feature_columns, logreg.coef_[0] ) )","63c475e0":"logreg.intercept_","0af07594":"#Predicting the test cases\nhr_test_pred = pd.DataFrame( { 'actual':  test_y,\n                            'predicted': logreg.predict( test_X ) } )","18c43308":"\nhr_test_pred = hr_test_pred.reset_index()","f12bb637":"#Comparing the predictions with actual test data\nhr_test_pred.sample( n = 10 )","a4106aeb":"# Creating a confusion matrix\n\nfrom sklearn import metrics\n\ncm = metrics.confusion_matrix( hr_test_pred.actual,\n                            hr_test_pred.predicted, [1,0] )\ncm","e2a30cc8":"import matplotlib.pyplot as plt\nimport seaborn as sn\n%matplotlib inline","476b9fa9":"sn.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"Left\", \"No Left\"] , yticklabels = [\"Left\", \"No Left\"] )\nplt.ylabel('True label')\nplt.xlabel('Predicted label')","736e37b5":"score = metrics.accuracy_score( hr_test_pred.actual, hr_test_pred.predicted )\nround( float(score), 2 )","95576bf9":"#How good the model is?\npredict_proba_df = pd.DataFrame( logreg.predict_proba( test_X ) )\npredict_proba_df.head()","10a59725":"hr_test_pred = pd.concat( [hr_test_pred, predict_proba_df], axis = 1 )","e35f3324":"hr_test_pred.columns = ['index', 'actual', 'predicted', 'Left_0', 'Left_1']","bb7ddbc1":"auc_score = metrics.roc_auc_score( hr_test_pred.actual, hr_test_pred.Left_1  )\nround( float( auc_score ), 2 )","fbdb5701":"sn.distplot( hr_test_pred[hr_test_pred.actual == 1][\"Left_1\"], color = 'b' )\nsn.distplot( hr_test_pred[hr_test_pred.actual == 0][\"Left_1\"], color = 'g' )","6bbed190":"# Finding the optimal cutoff probability\nfpr, tpr, thresholds = metrics.roc_curve( hr_test_pred.actual,\n                                     hr_test_pred.Left_1,\n                                     drop_intermediate = False )\n\nplt.figure(figsize=(6, 4))\nplt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate or [1 - True Negative Rate]')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","ddf4a82f":"print(thresholds[0:10])\nprint(fpr[0:10])\nprint(tpr[0:10])","d7b7c8aa":"tpr[np.abs(tpr - 0.7).argmin()]","78412f7c":"cutoff_prob = thresholds[(np.abs(tpr - 0.7)).argmin()]","be5e4d35":"round( float( cutoff_prob ), 2 )","114ef805":"#Predicting with new cut-off probability\nhr_test_pred['new_labels'] = hr_test_pred['Left_1'].map( lambda x: 1 if x >= 0.3 else 0 )","15b817c4":"metrics.accuracy_score( hr_test_pred.actual, hr_test_pred['new_labels'])","6ce46299":"hr_test_pred[0:10]","2918a1ea":"\ncm = metrics.confusion_matrix( hr_test_pred.actual,\n                          hr_test_pred.new_labels, [1,0] )\nsn.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"Left\", \"No Left\"] , yticklabels = [\"Left\", \"No Left\"] )\nplt.ylabel('True label')\nplt.xlabel('Predicted label')","17cd72c5":"import sklearn.tree as dt","ed9e8cb9":"from sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV","e259d27e":"train_X.shape","0426d163":"param_grid = {'max_depth': np.arange(2, 12),\n             'max_features': np.arange(10,18)}","175bc983":"train_y.shape","7f662aaf":"tree = GridSearchCV(DecisionTreeClassifier(), param_grid, cv = 10,verbose=1,n_jobs=-1)\ntree.fit( train_X, train_y )","810e863c":"tree.best_score_","6a664ee2":"tree.best_estimator_","1ba7f392":"tree.best_params_","7c90fab9":"train_pred = tree.predict(train_X)","917653ee":"print(metrics.classification_report(train_y, train_pred))","17b1e0d3":"test_pred = tree.predict(test_X)","85a138b8":"print(metrics.classification_report(test_y, test_pred))","b32e05d2":"clf_tree = DecisionTreeClassifier( max_depth = 9, max_features=17)\nclf_tree.fit( train_X, train_y )","fafdfd33":"train_X.columns","6a1a5e15":"clf_tree.feature_importances_","818dadbb":"list(zip(train_X.columns,clf_tree.feature_importances_ ))","caba615b":"tree_test_pred = pd.DataFrame( { 'actual':  test_y,\n                            'predicted': clf_tree.predict( test_X ) } )","9f801092":"tree_test_pred.sample( n = 10 )","03656d74":"metrics.accuracy_score( tree_test_pred.actual, tree_test_pred.predicted )","e5ded123":"tree_cm = metrics.confusion_matrix( tree_test_pred.predicted,\n                                 tree_test_pred.actual,\n                                 [1,0] )\nsn.heatmap(tree_cm, annot=True,\n         fmt='.2f',\n         xticklabels = [\"Left\", \"No Left\"] , yticklabels = [\"Left\", \"No Left\"] )\n\nplt.ylabel('True label')\nplt.xlabel('Predicted label')","74c96e75":"metrics.roc_auc_score( tree_test_pred.actual, tree_test_pred.predicted )","dd9b9b59":"import os     \nos.environ[\"PATH\"] += os.pathsep + 'C:\/Program Files (x86)\/Graphviz2.38\/bin\/'","184646c5":"#!pip install --upgrade pip\n#!pip install pydotplus","516428e3":"# Exporting the tree output in the form opendocument\n#export_graphviz( clf_tree,\n#              out_file = \"hr_tree.odt\",\n#              feature_names = train_X.columns )","d13973cc":"# Converting open document file to jpg imanage\n\n#import pydotplus as pdot\n\n#chd_tree_graph = pdot.graphviz.graph_from_dot_file( 'hr_tree.odt' )","77b7d932":"#chd_tree_graph.write_jpg( 'hr_tree.jpg' )","a1794b82":"# Viewing the image in the notebook (display the image)\n#from IPython.display import Image\n#Image(filename='hr_tree.jpg')","e4925aa3":"import sklearn.ensemble as en","82557045":"dir(en)","3ba93c21":"from sklearn.ensemble import BaggingClassifier","d71c9327":"bagclm = BaggingClassifier(oob_score=True, n_estimators=100, verbose=0, n_jobs=-1)\nbagclm.fit(train_X, train_y)","4ebc8480":"bagclm.predict(train_X)","8169fc34":"bagclm.oob_score_","92e9b42c":"y_pred = pd.DataFrame( { 'actual':  test_y,\n                            'predicted': bagclm.predict( test_X) } )","4fc0cf8d":"print(metrics.accuracy_score( y_pred.actual, y_pred.predicted ))\nprint(metrics.roc_auc_score( y_pred.actual, y_pred.predicted ))","27bb43e1":"tree_bg = metrics.confusion_matrix( y_pred.predicted,\n                                 y_pred.actual,\n                                 [1,0] )\nsn.heatmap(tree_bg, annot=True,\n         fmt='.2f',\n         xticklabels = [\"Left\", \"No Left\"] , yticklabels = [\"Left\", \"No Left\"] )\n\nplt.ylabel('True label')\nplt.xlabel('Predicted label')","5c09b3b0":"pargrid_bagging = {'n_estimators': [20,50,100,200,250,300,350,400]}\n\ngscv_bagging = GridSearchCV(estimator=BaggingClassifier(), \n                        param_grid=pargrid_bagging, \n                        cv=5,\n                        verbose=1, n_jobs=-1)","7ab7fe35":"gscv_results = gscv_bagging.fit(train_X, train_y)","f9f20b65":"gscv_results.best_params_","c8bf7708":"gscv_results.best_score_","f9ff69e9":"y_pred = pd.DataFrame( { 'actual':  test_y,\n                            'predicted': gscv_results.predict( test_X) } )","7e3935eb":"\nprint(metrics.accuracy_score( y_pred.actual, gscv_results.predict( test_X)))\nprint(metrics.roc_auc_score( y_pred.actual, gscv_results.predict( test_X)))","3d0a833c":"#gscv_results.feature_importances_","b84e66ca":"from sklearn.ensemble import RandomForestClassifier","f604405d":"pargrid_rf = {'n_estimators': [50, 60, 70, 80, 90, 100],\n                  'max_features': [5,6,7,8,9,10,11,12]}\n\n#from sklearn.grid_search import GridSearchCV\ngscv_rf = GridSearchCV(estimator=RandomForestClassifier(), \n                        param_grid=pargrid_rf, \n                        cv=10,\n                        verbose=True, n_jobs=-1)\n\ngscv_results = gscv_rf.fit(train_X, train_y)","7dd70103":"gscv_results.best_params_","0f3b91dc":"gscv_rf.best_score_","2729b201":"radm_clf = RandomForestClassifier(oob_score=True,n_estimators=80, max_features=7, n_jobs=-1)\nradm_clf.fit( train_X, train_y )","88c7adb6":"radm_test_pred = pd.DataFrame( { 'actual':  test_y,\n                            'predicted': radm_clf.predict( test_X ) } )","e3ba7a3e":"print(metrics.accuracy_score( radm_test_pred.actual, radm_test_pred.predicted ))\nprint(metrics.roc_auc_score( radm_test_pred.actual, radm_test_pred.predicted ))","da96faaf":"tree_cm = metrics.confusion_matrix( radm_test_pred.predicted,\n                                 radm_test_pred.actual,\n                                 [1,0] )\nsn.heatmap(tree_cm, annot=True,\n         fmt='.2f',\n         xticklabels = [\"Left\", \"No Left\"] , yticklabels = [\"Left\", \"No Left\"] )\n\nplt.ylabel('True label')\nplt.xlabel('Predicted label')","3ce30326":"print(radm_clf.feature_importances_)\nprint(np.argsort(radm_clf.feature_importances_))","30bbaaf6":"indices = np.argsort(radm_clf.feature_importances_)[::-1]","49a86f8e":"indices = np.argsort(radm_clf.feature_importances_)[::-1]\nfeature_rank = pd.DataFrame( columns = ['rank', 'feature', 'importance'] )\nfor f in range(train_X.shape[1]):\n  feature_rank.loc[f] = [f+1,\n                         train_X.columns[indices[f]],\n                         radm_clf.feature_importances_[indices[f]]]\nsn.barplot( y = 'feature', x = 'importance', data = feature_rank )","5f9f5007":"from sklearn.ensemble import AdaBoostClassifier","f4f2bcae":"pargrid_ada = {'n_estimators': [100, 200,250,300,350,400],\n               'learning_rate': [10 ** x for x in range(-1, 3)]}","849bfd94":"from sklearn.model_selection import GridSearchCV\ngscv_ada = GridSearchCV(estimator=AdaBoostClassifier(), \n                        param_grid=pargrid_ada, \n                        cv=5,\n                        verbose=1, n_jobs=-1)","366d2ca8":"gscv_ada.fit(train_X, train_y)","1449e109":"gscv_ada.best_params_","27d09ce2":"gscv_ada.best_score_","639c43d8":"clf_ada = gscv_ada.best_estimator_","96995559":"ad=clf_ada.fit(train_X, train_y )","a5016cd0":"print(metrics.accuracy_score(test_y,ad.predict(test_X)))\nprint(metrics.roc_auc_score(test_y,ad.predict(test_X)))","b14cad3b":"from sklearn.model_selection import cross_val_score","db8b5f02":"print(pd.Series(cross_val_score(clf_ada, test_X, test_y, cv=10)))\n\nprint(pd.Series(cross_val_score(clf_ada, test_X, test_y, cv=10)).describe()[['min', 'mean', 'max', 'std']])","d81e678a":"from sklearn.ensemble import GradientBoostingClassifier","42a146e4":"pargrid_gbm = {'n_estimators': [350,400,450,500],\n               'learning_rate': [10 ** x for x in range(-3, 1)],\n                'max_features': [5,6,7,8,9,10]}","0ab22314":"from sklearn.model_selection import GridSearchCV\ngscv_gbm = GridSearchCV(estimator=GradientBoostingClassifier(), \n                        param_grid=pargrid_gbm, \n                        cv=5,\n                        verbose=True, n_jobs=-1)","788a4806":"gscv_gbm.fit(train_X, train_y)","85be1618":"gscv_gbm.best_params_","3aebe12e":"gbm = gscv_gbm.best_estimator_","a6deac89":"gscv_gbm.best_score_","38b390db":"gbm.fit(train_X, train_y )","b5d96ea0":"print(metrics.accuracy_score(test_y,gbm.predict(test_X)))\nprint(metrics.roc_auc_score(test_y,gbm.predict(test_X)))","ffead406":"print(pd.Series(cross_val_score(gbm, test_X, test_y, cv=10)))\nprint(pd.Series(cross_val_score(gbm, test_X, test_y, cv=10)).describe()[['min', 'mean', 'max']])","49e3fa64":"from xgboost import XGBClassifier","967d3ca1":"pargrid_xgbm = {'n_estimators': [200, 250, 300, 400, 500],\n               'learning_rate': [10 ** x for x in range(-3, 1)],\n                'max_features': [5,6,7,8,9,10]}","6ee1b808":"#from sklearn.model_selection import GridSearchCV\ngscv_xgbm = GridSearchCV(estimator=XGBClassifier(), \n                        param_grid=pargrid_xgbm, \n                        cv=5,\n                        verbose=True, n_jobs=-1)","a96952b8":"gscv_xgbm.fit(train_X, train_y)","9878fb38":"gscv_xgbm.best_params_","ca534fc1":"xgbm = gscv_xgbm.best_estimator_","2baa55d9":"gscv_gbm.best_score_","7c3caddb":"xgbm.fit(train_X, train_y)","b05a5471":"print(metrics.accuracy_score(test_y,xgbm.predict(test_X)))\nprint(metrics.roc_auc_score(test_y,xgbm.predict(test_X)))","992aa55e":"print(pd.Series(cross_val_score(xgbm, test_X, test_y, cv=10)))\n\nprint(pd.Series(cross_val_score(xgbm, test_X, test_y, cv=10)).describe()[['min', 'mean', 'max']])","bc4d799c":"from sklearn.ensemble import VotingClassifier","be6b6ceb":"voting_clf = VotingClassifier(estimators = [('logreg',logreg), ('radm_clf',radm_clf), ('xgbm',xgbm)], voting = 'hard')\nvoting_clf.fit(train_X, train_y)","ca3f3a21":"print(metrics.accuracy_score(test_y,voting_clf.predict(test_X)))\nprint(metrics.roc_auc_score(test_y,voting_clf.predict(test_X)))","66e92d25":"Overall test accuracy is 78%. But it is not a good measure. The result is very high as there are lots of cases which are no left and the model has predicted most of them as no left. <br>\nThe objective of the model is to indentify the people who will leave, so that the company can intervene and act.<br>\nThis might be the case as the default model assumes people with more than 0.5 probability will not leave the company","710b40bb":"### DATA ATRRIBUTES\n\nsatisfaction_level: Employee satisfaction level <br>\nlast_evaluation: Last evaluation  <br>\nnumber_project: Number of projects  <br>\naverage_montly_hours: Average monthly hours <br>\ntime_spend_company: Time spent at the company <br>\nWork_accident: Whether they have had a work accident <br>\npromotion_last_5years: Whether they have had a promotion in the last 5 years <br>\ndepartment: Department <br>\nsalary: Salary <br>\nleft: Whether the employee has left <br>","8b5f7853":"#### Fine Tuning the parameters","167c1d0a":"## Feature Importance","c5993678":"### PREDICTIVE MODEL: Build a model to predict if an employee will leave the company","6e7e7676":"#### Ada Boosting","ca48831d":"## What is ensembling?\n\n**Ensemble learning (or \"ensembling\")** is the process of combining several predictive models in order to produce a combined model that is more accurate than any individual model.\n\n- **Regression:** take the average of the predictions\n- **Classification:** take a vote and use the most common prediction, or take the average of the predicted probabilities\n\nFor ensembling to work well, the models must have the following characteristics:\n\n- **Accurate:** they outperform the null model\n- **Independent:** their predictions are generated using different processes\n\n**The big idea:** If you have a collection of individually imperfect (and independent) models, the \"one-off\" mistakes made by each model are probably not going to be made by the rest of the models, and thus the mistakes will be discarded when averaging the models.\n\nThere are two basic **methods for ensembling:**\n\n- Manually ensemble your individual models\n- Use a model that ensembles for you","626eb46b":"# BUILDING RANDOM FOREST MODEL","14091fec":"### Generate Rules from Decision Trees\n\n#### To create a decision tree visualization graph.\n- Install GraphViz (As per the OS and version you are using)\n- pip install pydotplus\n- Add the path to environmental variables\n- Note: The notebook needs a restart.","7989a1c4":"### Boosting","710866ad":"### Building Final Decision Tree Model","5377f93f":"## Estimating feature importance\n\nBagging increases **predictive accuracy**, but decreases **model interpretability** because it's no longer possible to visualize the tree to understand the importance of each feature.\n\nHowever, we can still obtain an overall summary of **feature importance** from bagged models:\n\n- **Bagged regression trees:** calculate the total amount that **MSE** is decreased due to splits over a given feature, averaged over all trees\n- **Bagged classification trees:** calculate the total amount that **Gini index** is decreased due to splits over a given feature, averaged over all trees","b73c06d3":"### Hetrogenous induction Algo - Voting Classifier","f1d0cb0b":"#### Gradient Boosting","8567f182":"### Building Decision Tree Model","aa0e44fa":"---\n**How does bagging work (for decision trees)?**\n\n1. Grow B trees using B bootstrap samples from the training data.\n2. Train each tree on its bootstrap sample and make predictions.\n3. Combine the predictions:\n    - Average the predictions for **regression trees**\n    - Take a vote for **classification trees**\n\nNotes:\n\n- **Each bootstrap sample** should be the same size as the original training set.\n- **B** should be a large enough value that the error seems to have \"stabilized\".\n- The trees are **grown deep** so that they have low bias\/high variance.\n\nBagging increases predictive accuracy by **reducing the variance**, similar to how cross-validation reduces the variance associated with train\/test split (for estimating out-of-sample error) by splitting many times an averaging the results.","fe311c5d":"The summary statistics for Work_accident, left and promotion_last_5years does not make sense, as they are categorical variables","76f423db":"### Tuning n_estimators\n\nOne important tuning parameter is **n_estimators**, which is the number of trees that should be grown. It should be a large enough value that the error seems to have \"stabilized\".","055fd8c7":"<b> Note: <\/b>\nAs per the model, the most important features which influence whether to leave the company,in descending order, are\n\n- satisfaction_level\n- number_project\n- time_spend_company\n- last_evaluation\n- average_montly_hours\n- work_accident","a0dc5535":"## Comparing Random Forests with decision trees\n\n**Advantages of Random Forests:**\n\n- Performance is competitive with the best supervised learning methods\n- Provides a more reliable estimate of feature importance\n- Allows you to estimate out-of-sample error without using train\/test split or cross-validation\n\n**Disadvantages of Random Forests:**\n\n- Less interpretable\n- Slower to train\n- Slower to predict","e8b3aee8":"----\n## Estimating out-of-sample error\n\nFor bagged models, out-of-sample error can be estimated without using **train\/test split** or **cross-validation**!\n\nOn average, each bagged tree uses about **two-thirds** of the observations. For each tree, the **remaining observations** are called \"out-of-bag\" observations.","f580556d":"The model is predicting the probability of him leaving the company is only 0.027, which is very low.","9fe6e403":"### HR - Attrition Analytics -  Exploratory Analysis & Predictive Modeling\n> Human Resources are critical resources of any organiazation. Organizations spend huge amount of time and money to hire <br>\n> and nuture their employees. It is a huge loss for companies if employees leave, especially the key resources.  <br>\n> So if HR can predict weather employees are at risk for leaving the company, it will allow them to identify the attrition  <br>\n> risks and help understand and provie necessary support to retain those employees or do preventive hiring to minimize the  <br>\n> impact to the orgranization.","56e0148e":"---\nWhy are we learning about ensembling?\n\n- Very popular method for improving the predictive performance of machine learning models\n\n- Provides a foundation for understanding more sophisticated models","14504bb1":"Random Forests is a **slight variation of bagged trees** that has even better performance:\n\n- Exactly like bagging, we create an ensemble of decision trees using bootstrapped samples of the training set.\n- However, when building each tree, each time a split is considered, a **random sample of m features** is chosen as split candidates from the **full set of p features**. The split is only allowed to use **one of those m features**.\n    - A new random sample of features is chosen for **every single tree at every single split**.\n    - For **classification**, m is typically chosen to be the square root of p.\n    - For **regression**, m is typically chosen to be somewhere between p\/3 and p.\n\nWhat's the point?\n\n- Suppose there is **one very strong feature** in the data set. When using bagged trees, most of the trees will use that feature as the top split, resulting in an ensemble of similar trees that are **highly correlated**.\n- Averaging highly correlated quantities does not significantly reduce variance (which is the entire goal of bagging).\n- By randomly leaving out candidate features from each split, **Random Forests \"decorrelates\" the trees**, such that the averaging process can reduce the variance of the resulting model.","8645bca5":"---\n# Model $Ensembles$\n\n\n> Ensemble methods combine multiple classifiers (using _model averaging_ or _voting_) which may differ in algorithms, input features, or input samples. Statistical analyses showed that ensemble methods yield better classification performances and are also less prone to overfitting. Different methods, e.g., bagging or boosting, are used to construct the final classification decision based on weighted votes.","29bb5d4a":"### Feature importance from the Random Forest Model","8d676feb":"# Bagging\n\nThe primary weakness of **decision trees** is that they don't tend to have the best predictive accuracy. This is partially due to **high variance**, meaning that different splits in the training data can lead to very different trees.\n\n**Bagging** is a general purpose procedure for reducing the variance of a machine learning method, but is particularly useful for decision trees. Bagging is short for **bootstrap aggregation**, meaning the aggregation of bootstrap samples.\n\nWhat is a **bootstrap sample**? A random sample with replacement:","50cd503e":"How to calculate **\"out-of-bag error\":**\n\n1. For every observation in the training data, predict its response value using **only** the trees in which that observation was out-of-bag. Average those predictions (for regression) or take a vote (for classification).\n2. Compare all predictions to the actual response values in order to compute the out-of-bag error.\n\nWhen B is sufficiently large, the **out-of-bag error** is an accurate estimate of **out-of-sample error**.","2896b07a":"#### Xtreme Gradient Boosting","ceaeab79":"### Bagged decision trees (with B=10)","20687134":"### Tuning max_features\n\nThe other important tuning parameter is **max_features**, which is the number of features that should be considered at each split."}}