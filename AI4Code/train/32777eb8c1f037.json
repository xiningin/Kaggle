{"cell_type":{"32d39fdc":"code","39af6a7d":"code","bf62a408":"code","9a9ad224":"code","a072fa08":"code","9efbdd42":"code","e91aa053":"code","d58c35ba":"code","b2034bc6":"code","713c24ad":"code","0628afc2":"code","87a0ea33":"code","b604b05a":"code","050579b5":"code","22eecc25":"code","d87bec19":"code","5cd28aaf":"code","a3b998d5":"code","6e077ee9":"code","9a04308d":"code","f95d8ad3":"code","8ad78ddf":"code","07c5f678":"code","e8ac33f6":"code","759616a3":"code","d51804fe":"code","ebbb63ac":"code","e2917db8":"code","bc94498d":"code","0674418e":"code","432c1f08":"code","5f504b3d":"code","faca3f67":"code","416b2739":"code","636155b6":"code","e69c23a2":"code","d5c6b29c":"code","be9df6d3":"code","e472154d":"markdown","fff0aa2a":"markdown","12516737":"markdown","08533e0e":"markdown","53a748f8":"markdown","65747eac":"markdown","b81dcdf3":"markdown","2da4732e":"markdown","0cf84d07":"markdown","5a6bac87":"markdown","4cedb899":"markdown"},"source":{"32d39fdc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","39af6a7d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom  matplotlib.pyplot import subplot\n%matplotlib inline\n\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import SGDClassifier,LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix,mean_squared_error","bf62a408":"red_wine = pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","9a9ad224":"#Let's check how the data is distributed\nred_wine.head()","a072fa08":"#Information about the data columns\nred_wine.info()","9efbdd42":"red_wine.describe()","e91aa053":"red_wine.quality.value_counts()","d58c35ba":"red_wine.isnull().sum()","b2034bc6":"#fixed acidity does not give any specification to classify the quality.\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'fixed acidity', data = red_wine)","713c24ad":"#Relationship between each variables\nplt.figure(figsize=(20,10))\nsns.heatmap(red_wine.corr(), annot=True,cmap='Reds')\nplt.show()","0628afc2":"#a downing trend in the volatile acidity as the quality goes higher \nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'volatile acidity', data = red_wine)","87a0ea33":"#pie plot showing quality\nplt.figure(1, figsize=(8,8))\nred_wine['quality'].value_counts().plot.pie(autopct=\"%1.1f%%\")","b604b05a":"#histogram\nsns.countplot(red_wine['quality'])","050579b5":"#Composition of citric acid go higher as quality of the wine goes higher\nfig = plt.figure(figsize = (10,6))\nsns.violinplot(x = 'quality', y = 'citric acid', data = red_wine)","22eecc25":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'residual sugar', data = red_wine)","d87bec19":"#Composition of chloride goes down as quality of the wine goes higher\nfig = plt.figure(figsize = (10,6))\nsns.boxenplot(x = 'quality', y = 'chlorides', data = red_wine)","5cd28aaf":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'free sulfur dioxide', data = red_wine)","a3b998d5":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'total sulfur dioxide', data = red_wine)","6e077ee9":"#Sulphates level goes higher with the quality of wine\nfig = plt.figure(figsize = (10,6))\nsns.violinplot(x = 'quality', y = 'sulphates', data = red_wine)","9a04308d":"#Alcohol level goes higher as quality of wine increases\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'alcohol', data = red_wine)","f95d8ad3":"#Making binary classificaion for the response variable.\n#Dividing wine as good and bad by giving the limit for the quality\nbins = (2, 6.5, 8)\ngroup_names = ['bad', 'good']\nred_wine['quality'] = pd.cut(red_wine['quality'], bins = bins, labels = group_names)","8ad78ddf":"#Now lets assign a labels to our quality variable\nlabel_quality = LabelEncoder()","07c5f678":"#Bad becomes 0 and good becomes 1 \nred_wine['quality'] = label_quality.fit_transform(red_wine['quality'])","e8ac33f6":"red_wine['quality'].value_counts()","759616a3":"sns.countplot(red_wine['quality'])","d51804fe":"#Now seperate the dataset as response variable and feature variabes\nX = red_wine.drop('quality', axis = 1)\ny = red_wine['quality']","ebbb63ac":"#Train and Test splitting of data \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","e2917db8":"#Applying Standard scaling to get optimized result\nsc = StandardScaler()","bc94498d":"X_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","0674418e":"rfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train, y_train)\npred_rfc = rfc.predict(X_test)\n#Let's see how our model performed\nprint(classification_report(y_test, pred_rfc))\nprint(accuracy_score(y_test,pred_rfc))","432c1f08":"svm = SVC()\nsvm.fit(X_train, y_train)\npred_svm = svm.predict(X_test)\nprint(classification_report(y_test, pred_svm))\nprint(accuracy_score(y_test,pred_svm))","5f504b3d":"sgd = SGDClassifier(penalty=None)\nsgd.fit(X_train, y_train)\npred_sgd = sgd.predict(X_test)\nprint(classification_report(y_test, pred_sgd))\nprint(accuracy_score(y_test,pred_sgd))","faca3f67":"xgb = XGBClassifier(max_depth=3,n_estimators=200,learning_rate=0.5)\nxgb.fit(X_train,y_train)\npred_xgb = xgb.predict(X_test)\nprint(classification_report(y_test, pred_xgb))\nprint(accuracy_score(y_test,pred_xgb))","416b2739":"#for SGD\nprint(confusion_matrix(y_test, pred_sgd))\n#for randomforest\nprint(confusion_matrix(y_test, pred_rfc))\n#for SVM\nprint(confusion_matrix(y_test, pred_svm))\n#for XGB\nprint(confusion_matrix(y_test, pred_xgb))","636155b6":"x=red_wine.drop('quality', axis = 1)\ny= red_wine['quality']","e69c23a2":"scaler = MinMaxScaler(feature_range = (0,1))\nscaler.fit_transform(x)\n\nx.head()","d5c6b29c":"x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=0)","be9df6d3":"classifier_log = LogisticRegression()\nmodel = classifier_log.fit(x_train,y_train)\n\ny_pred_log = classifier_log.predict(x_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_pred_log, y_test)*100)","e472154d":"## Confusion Matrix","fff0aa2a":"# Let's visualize the data columns in the dataset","12516737":"## Random Forest","08533e0e":"# Importing the dataset","53a748f8":"## Logistic Regression","65747eac":"# **This Notebook is for Beginners...**\n\nHello everyone... This is my Kaggle first notebook.Hope u find this notebook useful. Here I have used the Red Wine Quality Dataset to analyse and predict using few ML techniques.\n\nIf you like this notebook, then please do upvote!\n\nHappy learning friends...","b81dcdf3":"## XGB","2da4732e":"# Preprocessing the data","0cf84d07":"The dataset has no missing values and all fields are numeric.","5a6bac87":"## SGD","4cedb899":"## SVM "}}