{"cell_type":{"af7b2a45":"code","3bb600bd":"code","0063cf44":"code","f08763c4":"code","2e7a771a":"code","dfbe3281":"code","11ee632a":"code","1f4ecede":"code","63d9cd7a":"code","59f484c6":"code","8db1e807":"code","2e1030f1":"code","7600d858":"code","25631c28":"code","878f1a3e":"code","8dde104c":"code","c0692de9":"markdown","0776baf0":"markdown","3182d674":"markdown","143eb46d":"markdown","8f88b9ed":"markdown","793ff9eb":"markdown"},"source":{"af7b2a45":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\n\nfrom sklearn.metrics import classification_report, confusion_matrix","3bb600bd":"dfs = []\n\nfor label in ['0', '1', '2', '3']:\n    dfs.append(pd.read_csv('..\/input\/emg-4\/' + label + '.csv'))","0063cf44":"for df in dfs:\n    df.columns = list(range(len(df.columns)))","f08763c4":"data = pd.concat([df for df in dfs], axis=0).reset_index(drop=True)","2e7a771a":"data","dfbe3281":"print(\"Total missing values:\", data.isna().sum().sum())","11ee632a":"y = data[64].copy()\nX = data.drop(64, axis=1).copy()","1f4ecede":"scaler = StandardScaler()\n\nX = scaler.fit_transform(X)","63d9cd7a":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=100)","59f484c6":"X_train.shape","8db1e807":"inputs = tf.keras.Input(shape=(X.shape[1],))\nx = tf.keras.layers.Dense(1024, activation='relu')(inputs)\nx = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(1024, activation='relu')(x)\nx = tf.keras.layers.Dropout(0.2)(x)\noutputs = tf.keras.layers.Dense(4, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs, outputs)\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nbatch_size = 32\nepochs = 50\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=batch_size,\n    epochs=epochs,\n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau()\n    ]\n)","2e1030f1":"plt.figure(figsize=(16, 10))\n\nplt.plot(range(epochs), history.history['loss'], label=\"Training Loss\")\nplt.plot(range(epochs), history.history['val_loss'], label=\"Validation Loss\")\n\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Loss Over Time\")\n\nplt.show()","7600d858":"model.evaluate(X_test, y_test)","25631c28":"y_true = np.array(y_test)\ny_pred = np.array(list(map(lambda x: np.argmax(x), model.predict(X_test))))","878f1a3e":"cm = confusion_matrix(y_true, y_pred)\n\nplt.figure(figsize=(6, 6))\nsns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()","8dde104c":"print(classification_report(y_true, y_pred))","c0692de9":"# Getting Started","0776baf0":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/8HLUmsMV2ho","3182d674":"# Results","143eb46d":"# Splitting\/Scaling","8f88b9ed":"# Training","793ff9eb":"# Task for Today  \n\n***\n\n## Hand Gesture Classification  \n\nGiven *tabular data about four hand gestures*, let's try to classify the **gesture** of a given example.\n\nWe will use a TensorFlow ANN to make our predictions."}}