{"cell_type":{"13773c1a":"code","f8ceca69":"code","08f1c59b":"code","b08d347c":"code","a599674b":"code","99f4d6b6":"code","31d1a4c3":"code","6a1db40b":"code","93bfd370":"code","52f03b9e":"code","4001d75f":"code","b1ca8ad4":"code","166cecb3":"code","b235efdb":"code","e7ffeb92":"markdown","34ed5002":"markdown","f5e41763":"markdown","e474aaf8":"markdown","088dbe63":"markdown"},"source":{"13773c1a":"!pip install timm","f8ceca69":"import os\nimport math\nimport random\nimport time\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\n\nimport cv2\nimport albumentations\nfrom albumentations.pytorch import ToTensorV2\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nimport timm\nfrom sklearn.metrics import mean_squared_error\n\nimport gc\ngc.enable()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","08f1c59b":"def get_train_file_path(id):\n    return f\"{train_img_path}\/{id}.jpg\"","b08d347c":"TTA_LIST = ['transforms_test','tta_flip','tta_shift_scale_rotate','tta_rotate']\nDIM = 384\nseed = 42\ntrain_img_path = '..\/input\/petfinder-pawpularity-score\/train'\n\ndevice = 'cuda'\nbackbone = 'swin_large_patch4_window12_384'\n\ndata = pd.read_csv('..\/input\/pawpular-models\/pawpularity_folds.csv')\ndata['file_path'] = data['Id'].apply(get_train_file_path)","a599674b":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)","99f4d6b6":"def get_valid_transforms():\n    return albumentations.Compose(\n        [\n          albumentations.Resize(DIM,DIM),\n          albumentations.Normalize(\n              mean=[0.485, 0.456, 0.406],\n              std=[0.229, 0.224, 0.225],\n          ),\n          ToTensorV2(p=1.0)\n        ]\n    )","31d1a4c3":"def get_inference_transforms(image_size):\n    \"\"\"Performs Augmentation on test dataset.\n    Returns the transforms for inference in a dictionary which can hold TTA transforms.\n\n    Args:\n        image_size (int, optional): [description]. Defaults to AUG.image_size.\n\n    Returns:\n        Dict[str, albumentations.Compose]: [description]\n    \"\"\"\n    transforms_dict = {\n        \"transforms_test\": albumentations.Compose(\n            [\n                albumentations.Resize(image_size, image_size),\n                albumentations.Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225]\n                ),\n                ToTensorV2(p=1.0),\n            ]\n        ),\n        \"tta_flip\": albumentations.Compose(\n            [\n                albumentations.HorizontalFlip(p=1),\n                albumentations.Resize(image_size, image_size),\n                albumentations.Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225],\n                    max_pixel_value=255.0,\n                    p=1.0,\n                ),\n                ToTensorV2(p=1.0),\n            ]\n        ),\n        \n        \"tta_rotate\": albumentations.Compose(\n            [\n                albumentations.Rotate(limit=180, p=1),\n                albumentations.Resize(image_size, image_size),\n                albumentations.Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225],\n                    max_pixel_value=255.0,\n                    p=1.0,\n                ),\n                ToTensorV2(p=1.0),\n            ]\n        ),\n        \n        \"tta_shift_scale_rotate\": albumentations.Compose(\n            [\n                albumentations.ShiftScaleRotate(\n                    shift_limit=0.1, scale_limit=0.1, rotate_limit=45, p=1\n                ),\n                albumentations.Resize(image_size, image_size),\n                albumentations.Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225],\n                    max_pixel_value=255.0,\n                    p=1.0,\n                ),\n                ToTensorV2(p=1.0),\n            ]\n        ),\n        \n        \"tta_hue_saturation_value\": albumentations.Compose(\n            [\n                albumentations.HueSaturationValue(\n                    hue_shift_limit=0.2,\n                    sat_shift_limit=0.2,\n                    val_shift_limit=0.2,\n                    p=1,\n                ),\n                albumentations.Resize(image_size, image_size),\n                albumentations.Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225],\n                    max_pixel_value=255.0,\n                    p=1.0,\n                ),\n                ToTensorV2(p=1.0),\n            ]\n        ),\n        \n        \"tta_random_brightness_contrast\": albumentations.Compose(\n            [\n                albumentations.RandomBrightnessContrast(\n                    brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=1\n                ),\n                albumentations.Resize(image_size, image_size),\n                albumentations.Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225],\n                    max_pixel_value=255.0,\n                    p=1.0,\n                ),\n                ToTensorV2(p=1.0),\n            ]\n        )\n    }\n    \n    return transforms_dict","6a1db40b":"class PawpularityDataset(Dataset):\n    def __init__(self, df, transforms=None):\n        self.df = df\n        self.file_names = df['file_path'].values\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path = self.file_names[index]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n            \n        return img#,self.df.loc[index,'Pawpularity'],self.df.loc[index,'Id']","93bfd370":"class PawpularityModel(nn.Module):\n    def __init__(self, backbone, pretrained=True):\n        super(PawpularityModel, self).__init__()\n        self.backbone = timm.create_model(backbone, pretrained=pretrained)\n        self.n_features = self.backbone.head.in_features\n        self.backbone.reset_classifier(0)\n\n        self.fc = nn.Sequential(\n            nn.Linear(self.n_features, self.n_features\/\/2),\n            nn.Tanh(),\n            nn.Dropout(0.2),\n            nn.Linear(self.n_features\/\/2, 256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, 1)\n        )\n\n    def forward(self, images):\n        features = self.backbone(images)\n        output = self.fc(features)           \n        return output","52f03b9e":"def oof_out(dataloader,model,device):\n    model.eval()\n    fin_out = []\n    #fin_tar = []\n    #fin_id = []\n  \n    with torch.no_grad():\n        #,target,ids\n        for batch_num, (image) in enumerate(dataloader):\n            image = image.to(device)\n            \n            output = model(image)\n            output = output.sigmoid().squeeze(1).detach().cpu().numpy()\n            #target = target.detach().cpu().numpy()\n    \n            fin_out.append(output*100)\n            #fin_tar.append(target)\n            #fin_id.append(ids)\n        \n    return np.concatenate(fin_out)#,np.concatenate(fin_tar),np.concatenate(fin_id)","4001d75f":"def make_oofs():\n    oof_id = []\n    oof_fold = []\n    oof_pred = []\n    oof_tar=[]\n\n    model = PawpularityModel(backbone=backbone)\n    model.to(device)\n    model.eval()\n    \n    transform = get_inference_transforms(DIM)\n\n    for fold in tqdm(range(5)):\n        set_seed(seed+fold)\n        \n        valid = data[data['kfold']==fold].reset_index(drop=True)\n        \n        tta_preds = 0\n        for i,tta in enumerate(TTA_LIST):\n            # Defining DataSet            \n            valid_dataset = PawpularityDataset(valid,transform[tta])   \n\n            valid_loader = torch.utils.data.DataLoader(\n                valid_dataset,\n                batch_size=32,\n                pin_memory=True,\n                drop_last=False,\n                num_workers=4\n            )\n            \n            set_seed(seed+fold)\n            \n            model_path = f\"..\/input\/pawpular-models\/12\/swin_baseline_11_fold_{fold}.pth\"\n            model.load_state_dict(torch.load(model_path,map_location=device))\n        \n            # THE ENGINE LOOP\n            valid_out = oof_out(valid_loader, model, device)\n            tta_preds = tta_preds + valid_out\n            \n\n        tta_preds = tta_preds\/len(TTA_LIST)\n\n        ### Storing OOFS\n        oof_id.append(valid['Id'].values)\n        oof_tar.append(valid['Pawpularity'].values)\n        oof_pred.append(tta_preds)\n        oof_fold.append([fold]*len(valid['Id'].values))\n\n    return oof_pred,oof_tar,oof_id,oof_fold","b1ca8ad4":"oof_pred,oof_tar,oof_id,oof_fold = make_oofs()","166cecb3":"# COMPUTE OVERALL OOF AUC\noof = np.concatenate(oof_pred); true = np.concatenate(oof_tar);\nid = np.concatenate(oof_id); folds = np.concatenate(oof_fold)\nRMSE = mean_squared_error(true,oof,squared=False)\nprint('Overall OOF RMSE with TTA =',RMSE)","b235efdb":"df_oof = pd.DataFrame(dict(\n    id = id, target=true, pred = oof, fold=folds))\ndf_oof.to_csv('swin_009_oof_tta.csv',index=False)\ndf_oof.head()","e7ffeb92":"# Dataset","34ed5002":"# Config","f5e41763":"# Engine","e474aaf8":"# Model","088dbe63":"# Utils"}}