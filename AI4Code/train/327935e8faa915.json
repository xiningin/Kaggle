{"cell_type":{"4e4590dd":"code","fb6f7c90":"code","f3ad4a3e":"code","aba40f05":"code","4fead5d7":"code","ee09cc83":"code","d208b11a":"code","74765b69":"code","99f500dd":"code","80bfadfe":"code","a79e2f83":"code","b41bdc54":"code","fd5cb3cf":"code","bc83234e":"code","cb41da39":"code","220466f0":"code","7bdbcbb8":"code","6a15af46":"code","c67eb75a":"code","47304de9":"code","4e803a48":"code","f58bb83d":"code","185e6f5c":"code","07f582c7":"code","eabb63ca":"code","e298ee8e":"code","08074856":"code","3495d351":"code","2c0f0198":"code","9cb25e0e":"code","9ce692b4":"code","60551ab8":"code","9c56b07a":"code","85068971":"code","848518d1":"code","36d4c5b5":"code","5518508b":"code","6f405835":"code","78406dc6":"code","a3060dfe":"code","1760b50c":"code","39e6752a":"code","d7d2ecd1":"code","246a3425":"markdown","f3e1e8b2":"markdown","e51d4214":"markdown","ebc42721":"markdown","8b97e9cb":"markdown","38226d97":"markdown","4f70b15d":"markdown","d788641a":"markdown","fca1151f":"markdown","7494cb80":"markdown","99acbdbd":"markdown","c84622b4":"markdown","664bccd0":"markdown","115ba295":"markdown","80e8a64c":"markdown","eb66ce36":"markdown","5a7606c1":"markdown","254d22a6":"markdown","451c0720":"markdown","3f23d4c2":"markdown","d51051d5":"markdown","943d0104":"markdown"},"source":{"4e4590dd":"#Import libs\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier,ExtraTreesClassifier\nfrom collections import Counter\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\nimport warnings\nwarnings.filterwarnings('ignore')","fb6f7c90":"df=pd.read_csv('..\/input\/lingspam-dataset\/messages.csv')\ndf.head()","f3ad4a3e":"# converting all messages to lower case\n\ndf['message'] = df['message'].str.lower()","aba40f05":"# check data once \ndf.head()","4fead5d7":"# checing null values \ndf.isnull().sum()","ee09cc83":"df.fillna(df['subject'].mode().values[0],inplace=True)","d208b11a":"# let's once again \ndf.isnull().sum()","74765b69":"df['sub_mssg']=df['subject']+df['message']\ndf.head()","99f500dd":"df['sub_mssg'].describe()","80bfadfe":"df['length']=df['sub_mssg'].apply(len)\ndf.head()","a79e2f83":"#now i'm going to drop un-necessary features \ndf.drop('subject',axis=1,inplace=True)","b41bdc54":"# check it once \ndf.head()","fd5cb3cf":"lb=df['label'].value_counts().index.tolist()\nval=df['label'].value_counts().values.tolist()\nexp=(0.025,0)\nclr=('orange','blue')\nplt.figure(figsize=(10,5),dpi=140)\nplt.pie(x=val,explode=exp,labels=lb,colors=clr,autopct='%2.0f%%',pctdistance=0.5, shadow=True,radius=0.9)\nplt.legend([\"0 = NO SPAM\",'1 = SPAM'])\nplt.show()","bc83234e":"df['message'][0]","cb41da39":"import re","220466f0":"def decontact(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","7bdbcbb8":"mssg=decontact(df['message'][70])\nmssg","6a15af46":"#REPLACING NUMBERS\ndf['sub_mssg']=df['sub_mssg'].str.replace(r'\\d+(\\.\\d+)?', 'numbers')\ndf['sub_mssg'][0]","c67eb75a":"#CONVRTING EVERYTHING TO LOWERCASE\ndf['sub_mssg']=df['sub_mssg'].str.lower()\n#REPLACING NEXT LINES BY 'WHITE SPACE'\ndf['sub_mssg']=df['sub_mssg'].str.replace(r'\\n',\" \") \n# REPLACING EMAIL IDs BY 'MAILID'\ndf['sub_mssg']=df['sub_mssg'].str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$','MailID')\n# REPLACING URLs  BY 'Links'\ndf['sub_mssg']=df['sub_mssg'].str.replace(r'^http\\:\/\/[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(\/\\S*)?$','Links')\n# REPLACING CURRENCY SIGNS BY 'MONEY'\ndf['sub_mssg']=df['sub_mssg'].str.replace(r'\u00a3|\\$', 'Money')\n# REPLACING LARGE WHITE SPACE BY SINGLE WHITE SPACE\ndf['sub_mssg']=df['sub_mssg'].str.replace(r'\\s+', ' ')\n\n# REPLACING LEADING AND TRAILING WHITE SPACE BY SINGLE WHITE SPACE\ndf['sub_mssg']=df['sub_mssg'].str.replace(r'^\\s+|\\s+?$', '')\n#REPLACING CONTACT NUMBERS\ndf['sub_mssg']=df['sub_mssg'].str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$','contact number')\n#REPLACING SPECIAL CHARACTERS  BY WHITE SPACE \ndf['sub_mssg']=df['sub_mssg'].str.replace(r\"[^a-zA-Z0-9]+\", \" \")","47304de9":"#CONVRTING EVERYTHING TO LOWERCASE\ndf['message']=df['message'].str.lower()\n#REPLACING NEXT LINES BY 'WHITE SPACE'\ndf['message']=df['message'].str.replace(r'\\n',\" \") \n# REPLACING EMAIL IDs BY 'MAILID'\ndf['message']=df['message'].str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$','MailID')\n# REPLACING URLs  BY 'Links'\ndf['message']=df['message'].str.replace(r'^http\\:\/\/[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(\/\\S*)?$','Links')\n# REPLACING CURRENCY SIGNS BY 'MONEY'\ndf['message']=df['message'].str.replace(r'\u00a3|\\$', 'Money')\n# REPLACING LARGE WHITE SPACE BY SINGLE WHITE SPACE\ndf['message']=df['message'].str.replace(r'\\s+', ' ')\n\n# REPLACING LEADING AND TRAILING WHITE SPACE BY SINGLE WHITE SPACE\ndf['message']=df['message'].str.replace(r'^\\s+|\\s+?$', '')\n#REPLACING CONTACT NUMBERS\ndf['message']=df['message'].str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$','contact number')\n#REPLACING SPECIAL CHARACTERS  BY WHITE SPACE \ndf['message']=df['message'].str.replace(r\"[^a-zA-Z0-9]+\", \" \")","4e803a48":"df['sub_mssg'][0]","f58bb83d":"df.head()","185e6f5c":"from tqdm import tqdm\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n# removing stopwords \nstop = stopwords.words('english')\ndf['Cleaned_Text'] = df['sub_mssg'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n","07f582c7":"df.head()","eabb63ca":"df.drop('message',axis=1,inplace=True)","e298ee8e":"df.drop('sub_mssg',axis=1,inplace=True)","08074856":"df.head()","3495d351":"df.isnull().sum()","2c0f0198":"df['lgth_clean']=df['Cleaned_Text'].apply(len)\ndf.head()","9cb25e0e":"original_length=sum(df['length'])\nafter_cleaning=sum(df['lgth_clean'])","9ce692b4":"print(\"original_length\",original_length)\nprint('after_cleaning',after_cleaning)","60551ab8":"# 1. Convert text into vectors using TF-IDF\n# 2. Instantiate MultinomialNB classifier\n# 3. Split feature and label\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nfrom sklearn.pipeline import Pipeline","9c56b07a":"tvec = TfidfVectorizer()\nlr = LogisticRegression(solver = \"lbfgs\")","85068971":"X = df.Cleaned_Text\nY = df.label\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, random_state = 225,stratify=Y)","848518d1":"model = Pipeline([('vectorizer',tvec),('classifier',lr)])\n\nmodel.fit(X_train,Y_train)\n\n\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = model.predict(X_test)\n\nconfusion_matrix(y_pred,Y_test)","36d4c5b5":"print(\"Accuracy : \", accuracy_score(y_pred,Y_test))\nprint(\"Precision : \", precision_score(y_pred,Y_test, average = 'weighted'))\nprint(\"Recall : \", recall_score(y_pred,Y_test, average = 'weighted'))\n","5518508b":"knc = KNeighborsClassifier()\nmodel_1 = Pipeline([('vectorizer',tvec),('classifier',knc)])\nmodel_1.fit(X_train,Y_train)\n\n\ny_pred = model_1.predict(X_test)\n\nprint(confusion_matrix(y_pred,Y_test))\nprint(\"Accuracy : \", accuracy_score(y_pred,Y_test))\nprint(\"Precision : \", precision_score(y_pred,Y_test, average = 'weighted'))\nprint(\"Recall : \", recall_score(y_pred,Y_test, average = 'weighted'))","6f405835":"abc = AdaBoostClassifier()\nmodel_3 = Pipeline([('vectorizer',tvec),('classifier',abc)])\nmodel_3.fit(X_train,Y_train)\n\n\ny_pred = model_3.predict(X_test)\n\nprint(confusion_matrix(y_pred,Y_test))\nprint(\"Accuracy : \", accuracy_score(y_pred,Y_test))\nprint(\"Precision : \", precision_score(y_pred,Y_test, average = 'weighted'))\nprint(\"Recall : \", recall_score(y_pred,Y_test, average = 'weighted'))","78406dc6":"mnb = MultinomialNB()\nmodel_5 = Pipeline([('vectorizer',tvec),('classifier',mnb)])\nmodel_5.fit(X_train,Y_train)\n\n\ny_pred = model_5.predict(X_test)\n\nprint(confusion_matrix(y_pred,Y_test))\nprint(\"Accuracy : \", accuracy_score(y_pred,Y_test))\nprint(\"Precision : \", precision_score(y_pred,Y_test, average = 'weighted'))\nprint(\"Recall : \", recall_score(y_pred,Y_test, average = 'weighted'))","a3060dfe":"gbc = GradientBoostingClassifier()\nmodel_6= Pipeline([('vectorizer',tvec),('classifier',gbc)])\nmodel_6.fit(X_train,Y_train)\n\n\ny_pred = model_6.predict(X_test)\nprint(confusion_matrix(y_pred,Y_test))\nprint(\"Accuracy : \", accuracy_score(y_pred,Y_test))\nprint(\"Precision : \", precision_score(y_pred,Y_test, average = 'weighted'))\nprint(\"Recall : \", recall_score(y_pred,Y_test, average = 'weighted'))","1760b50c":"from sklearn.ensemble import RandomForestClassifier as RFC\nrfc = RFC(random_state=42)\nmodel_7 = Pipeline([('vectorizer',tvec),('classifier',rfc)])\n\nmodel_7.fit(X_train,Y_train)\n\ny_pred = model_7.predict(X_test)\nprint(confusion_matrix(y_pred,Y_test))\nprint(\"Accuracy : \", accuracy_score(y_pred,Y_test))\nprint(\"Precision : \", precision_score(y_pred,Y_test, average = 'weighted'))\nprint(\"Recall : \", recall_score(y_pred,Y_test, average = 'weighted'))","39e6752a":"result=model_7.predict(['your microsoft account has been compromised ,you must update before or else your account going to close click to update'])\nresult","d7d2ecd1":"result=model_7.predict(['Today we want to inform you that the application period for 15.000 free Udacity Scholarships in Data Science is now open! Please apply by November 16th, 2020 via https:\/\/www.udacity.com\/bertelsmann-tech-scholarships.'])\nresult","246a3425":"## Testing Model","f3e1e8b2":"Now it's looking perfect and move on to next step's .","e51d4214":"## Random Forest Classifier","ebc42721":"## Use case\nYou were recently hired in start up company and you were asked to build a system to identify spam emails.\n","8b97e9cb":"## Data Cleansing","38226d97":"## Logistic Regression","4f70b15d":"Now message looking perfect .","d788641a":"## Importing Libraries","fca1151f":"## Feature Engineering ","7494cb80":"## KNeighbors Classifier","99acbdbd":"#  EOF DONE CLASSIFICATION OF SPAM CLASSIFICATION ","c84622b4":"From here we can observe that data is missing here .","664bccd0":"To get clarity about mail i'm going to merge both subject and message .","115ba295":"From here we can observe that Random forest classifier working well compared to all other algorithms","80e8a64c":"## Training Model","eb66ce36":"## Naive Bayes","5a7606c1":"## Ada Boost Classifier","254d22a6":"## Preprocessing Email Messages :","451c0720":"Here 0 is spam and 1 is normal message.","3f23d4c2":"## Gradient Boosting Classifier","d51051d5":"# Spam Detection","943d0104":"## Data Visualization "}}