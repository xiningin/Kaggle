{"cell_type":{"8ddfa6aa":"code","541d55a6":"code","2ddea4fd":"code","c8f839a0":"code","73f3acfb":"code","11bcd367":"code","cec543e5":"code","0a7c7707":"code","0c5bc902":"code","b6102dec":"code","8e80096d":"code","93cbdd98":"code","02f7fe62":"code","206ee547":"code","cfa25fb2":"code","9beae997":"code","1f365fdc":"code","c83599ab":"code","b69bc6c8":"code","d3c5d75d":"code","6720bba8":"code","a8aa9b41":"code","ce20b820":"code","b441061b":"code","d3050e9c":"code","29a14be5":"code","9b53cfd1":"code","f4ecba5b":"code","d522d6b0":"code","455379a5":"code","3ff8ce75":"code","728d74d4":"code","cb70b1a9":"code","82a830f6":"code","1a797610":"code","dad18ffa":"code","145ace31":"code","2c4a30e1":"code","e1e5cac7":"code","4b8bae3b":"markdown","51fc2c15":"markdown","90c98f5d":"markdown","29ba73c7":"markdown","6bf3677f":"markdown","f43e9e3d":"markdown","05973c03":"markdown","4071e24a":"markdown","f3b9a8a8":"markdown","9a460edb":"markdown","eef45fee":"markdown","5dd46295":"markdown","31ee4d74":"markdown","0560c538":"markdown","880a2b3a":"markdown","aca56d77":"markdown","9aa5cded":"markdown","cc2b8af0":"markdown","108c23b2":"markdown","3b86784d":"markdown","762f1376":"markdown","85a89b72":"markdown","cdd04747":"markdown","bdb10c72":"markdown","c6108232":"markdown","8574f549":"markdown","47ca26b3":"markdown","9291a3b8":"markdown","354ff5b2":"markdown","a21b5e4c":"markdown","ae90b3e7":"markdown","5feec96c":"markdown","1dd85f7b":"markdown","d75eef0a":"markdown","2547314a":"markdown","ee549866":"markdown","288f5be1":"markdown","527edc78":"markdown","5e8dabd3":"markdown","e804294b":"markdown","bedea60e":"markdown","eb997007":"markdown","a88b481f":"markdown"},"source":{"8ddfa6aa":"import os\nimport re\nimport math\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport spacy\nnlp = spacy.load('en')\nnlp.remove_pipe('parser')\nnlp.remove_pipe('ner')\n\nfrom wordcloud import WordCloud\nfrom collections import Counter","541d55a6":"warnings.filterwarnings('ignore')\npd.set_option('display.max_colwidth', -1)\n\nSEED = 13\nrandom.seed(SEED)\nnp.random.seed(SEED)","2ddea4fd":"file_job_titles = '..\/input\/cityofla\/CityofLA\/Additional data\/job_titles.csv'\njob_titles = pd.read_csv(file_job_titles, header=None, names=['job_title'])\njob_titles = job_titles['job_title'].tolist()","c8f839a0":"file_data_dic = '..\/input\/cityofla\/CityofLA\/Additional data\/kaggle_data_dictionary.csv'\ndata_dictionary = pd.read_csv(file_data_dic)","73f3acfb":"dir_job_bulletins = '..\/input\/cityofla\/CityofLA\/Job Bulletins'\ndata_list = []\nfor filename in os.listdir(dir_job_bulletins):\n    with open(os.path.join(dir_job_bulletins, filename), 'r', errors='ignore') as f:\n        data_list.append([filename, ''.join(f.readlines())])\njobs = pd.DataFrame(data_list, columns=['file', 'job_description'])","11bcd367":"jobs = jobs[jobs['file'] != 'Vocational Worker  DEPARTMENT OF PUBLIC WORKS.txt']","cec543e5":"def merge_jobs_data(jobs, extracted_data):\n    \"\"\" Add the extracted_data to the current jobs DataFrame\n\n        param jobs: Current jobs DataFrame\n        param extracted_data: Series with DataFrame inside to extract\n        return jobs: Merged DataFrame\n    \"\"\" \n    jobs['temp'] = extracted_data\n    for index, row in jobs.iterrows():\n        extracted_data = row['temp']\n        if isinstance(extracted_data, pd.DataFrame):\n            for c in extracted_data.columns:\n                jobs.loc[index, c] = extracted_data[c][0]\n    jobs = jobs.drop('temp', axis=1) \n    return jobs\n\ndef extract_text_by_regex(text, regex_dictionary, flags=re.DOTALL):\n    \"\"\" Extract values by regular expressions\n\n        param text: String to extract the values\n        param regex_dictionary: Dictionary with the names and regular expressions to extract\n        return result: Series with the first extracted values\n    \"\"\" \n    regex_dictionary = pd.DataFrame(regex_dictionary, columns=['name', 'regexpr'])\n    result = regex_dictionary.copy()\n    result['text'] = np.NaN\n    for index,row in regex_dictionary.iterrows():\n        find_reg = re.findall(row['regexpr'], text, flags)\n        extracted_text = find_reg[0].strip() if find_reg else np.NaN\n        result.loc[index, 'text'] = extracted_text\n    return result.set_index('name')[['text']].T \n\ndef extract_text_by_regex_index(text, regex_dictionary):\n    \"\"\" Extract values by regular expressions\n    \n        Search for the index of the first occurrence of the regular expression \n        and extract the text to the next regular expression.\n\n        param text: String to extract the values\n        param regex_dictionary: Dictionary with the names and regular expressions to extract\n        return result: Series with the first extracted values\n    \"\"\" \n    regex_dictionary = pd.DataFrame(regex_dictionary, columns=['name', 'regexpr'])\n\n    result = regex_dictionary.copy()\n    result['text'] = np.NaN\n    for index,row in regex_dictionary.iterrows():\n        find_text = re.search(row['regexpr'], text)\n        find_text = find_text.span(0)[0] if find_text else np.nan\n        result.loc[index, 'start'] = find_text\n    result.dropna(subset=['start'], inplace=True)\n    result['end'] = result['start'].apply(lambda x: np.min(result[result['start'] > x]['start'])).fillna(len(text))\n    \n    for index,row in result.iterrows():\n        extracted_text = text[int(row['start']):int(row['end'])]\n        find_reg = re.findall(row['regexpr']+'(.*)', extracted_text, re.DOTALL)\n        extracted_text = find_reg[0].strip() if find_reg else np.NaN\n        result.loc[index, 'text'] = extracted_text\n    return result.set_index('name')[['text']].T \n\ndef nlp_transformation(data, token_pos=None):\n    \"\"\" Use NLP to transform the text corpus to cleaned sentences and word tokens\n\n        param data: List with sentences, which should be processed.\n        param token_pos: List with the POS-Tags to filter (Default: None = All POS-Tags)\n        return processed_tokens: List with the cleaned and tokenized sentences\n    \"\"\"    \n    def token_filter(token):\n        \"\"\" Keep tokens who are alphapetic, in the pos (part-of-speech) list and not in stop list\n            \n        \"\"\"    \n        if token_pos:\n            return not token.is_stop and token.is_alpha and token.pos_ in token_pos\n        else:\n            return not token.is_stop and token.is_alpha\n    \n    data = [re.compile(r'<[^>]+>').sub('', x) for x in data] #Remove HTML-tags\n    processed_tokens = []\n    data_pipe = nlp.pipe(data)\n    for doc in data_pipe:\n        filtered_tokens = [token.lemma_.lower() for token in doc if token_filter(token)]\n        processed_tokens.append(filtered_tokens)\n    return processed_tokens","0a7c7707":"regex_dictionary = [('metadata', r''), \n                      ('salary', r'(?:ANNUAL SALARY|ANNUALSALARY)'),\n                      ('duties', r'(?:DUTIES)'),\n                      ('requirements', r'(?:REQUIREMENTS\/MINIMUM QUALIFICATIONS|REQUIREMENT\/MINIMUM QUALIFICATION|REQUIREMENT|REQUIREMENTS|REQUIREMENT\/MIMINUMUM QUALIFICATION)'),\n                      ('where_to_apply', r'(?:WHERE TO APPLY|HOW TO APPLY)'),\n                      ('application_deadline', r'(?:APPLICATION DEADLINE|APPLICATION PROCESS)'),\n                      ('selection_process', r'(?:SELECTION PROCESS|SELELCTION PROCESS)'),\n                      ]\nextracted_data = jobs['job_description'].dropna().apply(lambda x: extract_text_by_regex_index(x, regex_dictionary))\njobs = merge_jobs_data(jobs, extracted_data)","0c5bc902":"regex_dictionary = [('job_title', r'(.*?)(?=\\n)'), \n                      ('class_code', r'(?:Class Code:|Class  Code:)\\s*(\\d\\d\\d\\d)'),\n                      ('open_date', r'(?:Open Date:|Open date:)\\s*(\\d\\d-\\d\\d-\\d\\d)'),\n                      ('revised', r'(?:Revised:|Revised|REVISED:)\\s*(\\d\\d-\\d\\d-\\d\\d)')\n                      ]\nextracted_data = jobs['metadata'].dropna().apply(lambda x: extract_text_by_regex(x, regex_dictionary))\njobs = merge_jobs_data(jobs, extracted_data)\njobs['open_date'] = pd.to_datetime(jobs['open_date'], infer_datetime_format=True)\njobs['revised'] = pd.to_datetime(jobs['revised'], infer_datetime_format=True)","b6102dec":"# Extract the first salary\nregex_dictionary = [('salary_first', r'(\\$(?:\\d{1,3})(?:\\,\\d{3})*(?:\\.\\d{2})* to \\$(?:\\d{1,3})(?:\\,\\d{3})*(?:\\.\\d{2})*|\\$(?:\\d{1,3})(?:\\,\\d{3})*(?:\\.\\d{2})* \\(flat-rated\\))'), \n                      ('salary_additional', r'(?:\\n)(.*)(?:NOTES)'),\n                      ('salary_notes', r'(?:NOTES:)(.*)'),\n                      ]\nextracted_data = jobs['salary'].dropna().apply(lambda x: extract_text_by_regex(x, regex_dictionary, re.DOTALL|re.IGNORECASE))\njobs = merge_jobs_data(jobs, extracted_data)\n# Extract from the first salary the values\nregex_dictionary = [('salary_from', r'\\$((?:\\d{1,3})(?:\\,\\d{3})*(?:\\.\\d{2})*).*'), \n                      ('salary_to', r'(?:to \\$)((?:\\d{1,3})(?:\\,\\d{3})*(?:\\.\\d{2})*).*'),\n                      ('salary_flatrated', r'(flat-rated)')\n                      ]\nextracted_data = jobs['salary_first'].dropna().apply(lambda x: extract_text_by_regex(x, regex_dictionary, re.DOTALL|re.IGNORECASE))\njobs = merge_jobs_data(jobs, extracted_data)\njobs['salary_from'] = jobs['salary_from'].dropna().apply(lambda x: float(x.replace(',', '')))\njobs['salary_to'] = jobs['salary_to'].dropna().apply(lambda x: float(x.replace(',', '')))\njobs['salary_flatrated'] = jobs['salary_flatrated'].dropna().apply(lambda x: True)\njobs.drop('salary_first', axis=1, inplace=True)","8e80096d":"regex_dictionary = [('duties_text', r''), \n                      ('duties_notes', r'(?:NOTE:|NOTES:)'),\n                      ]\nextracted_data = jobs['duties'].dropna().apply(lambda x: extract_text_by_regex_index(x, regex_dictionary))\njobs = merge_jobs_data(jobs, extracted_data)","93cbdd98":"regex_dictionary = [('requirements_text', r''), \n                         ('requirements_notes', r'(?:PROCESS NOTES|NOTES:|NOTE:|PROCESS NOTE)'),\n                         ('requirements_certifications', r'(?:SELECTIVE CERTIFICATION|SELECTIVE CERTIFICATION:)'),\n                      ]\nextracted_data = jobs['requirements'].dropna().apply(lambda x: extract_text_by_regex_index(x, regex_dictionary))\njobs = merge_jobs_data(jobs, extracted_data)","02f7fe62":"regex_dictionary = [('where_to_apply_text', r''), \n                         ('where_to_apply_notes', r'(?:NOTE:)'),\n                      ]\nextracted_data = jobs['where_to_apply'].dropna().apply(lambda x: extract_text_by_regex_index(x, regex_dictionary))\njobs = merge_jobs_data(jobs, extracted_data)","206ee547":"regex_dictionary = [('application_deadline_text', r''), \n                         ('application_deadline_notes', r'(?:NOTE:)'),\n                         ('application_deadline_review', r'(?:QUALIFICATIONS REVIEW|EXPERT REVIEW COMMITTEE)'),\n                      ]\nextracted_data = jobs['application_deadline'].dropna().apply(lambda x: extract_text_by_regex_index(x, regex_dictionary))\njobs = merge_jobs_data(jobs, extracted_data)","cfa25fb2":"regex_dictionary = [('selection_process_text', r''), \n                         ('selection_process_notes', r'(?:NOTES:)'),\n                         ('selection_process_notice', r'(?:NOTICE:|Notice:)'),\n                      ]\nextracted_data = jobs['selection_process'].dropna().apply(lambda x: extract_text_by_regex_index(x, regex_dictionary))\njobs = merge_jobs_data(jobs, extracted_data)","9beae997":"def get_exam_type(text):\n    \"\"\" Extract the exam type from the text\n\n        param text: String to extract the values\n        return result: String with the exam_type code\n    \"\"\" \n    regex_dic = {'OPEN_INT_PROM':r'BOTH.*INTERDEPARTMENTAL.*PROMOTIONAL', \n                 'INT_DEPT_PROM':r'INTERDEPARTMENTAL.*PROMOTIONAL', \n                 'DEPT_PROM':r'DEPARTMENTAL.*PROMOTIONAL',\n                 'OPEN':r'OPEN.*COMPETITIVE.*BASIS'\n                }\n    result = np.nan\n    for key, value in regex_dic.items():\n        regex = value\n        regex_find = re.findall(regex, text, re.DOTALL)\n        if regex_find:\n            result = key\n            break\n    return result\n\njobs['exam_type'] = jobs['selection_process'].dropna().apply(get_exam_type)","1f365fdc":"def get_driver_license(text):\n    \"\"\" Extract the driver license from the text\n\n        param text: String to extract the values\n        return result: String if a driver license is needed\n    \"\"\" \n    regex_dic = {'P':r'(may[^\\.]*requir[^\\.]*driver[^\\.]*license)', \n                 'R':r'(requir[^\\.]*driver[^\\.]*license)|(driver[^\\.]*license[^\\.]*requir)', \n                }\n    result = np.nan\n    for key, value in regex_dic.items():\n        regex = value\n        regex_find = re.findall(regex, text, re.IGNORECASE)\n        if regex_find:\n            result = key\n            break\n    return result\n\njobs['driver_license'] = jobs['job_description'].dropna().apply(get_driver_license)","c83599ab":"def extrac_salary(text):\n    reg_expr = r'(\\$(?:\\d{1,3})(?:\\,\\d{3})*(?:\\.\\d{2})* to \\$(?:\\d{1,3})(?:\\,\\d{3})*(?:\\.\\d{2})*|\\$(?:\\d{1,3})(?:\\,\\d{3})*(?:\\.\\d{2})* \\(flat-rated\\))'\n    result = re.findall(reg_expr, text, re.DOTALL|re.IGNORECASE)\n    return pd.Series(result)\n\nsalary = jobs['salary'].dropna().apply(extrac_salary)\nsalary.columns = ['salary_{}'.format(x) for x in salary.columns]\nfor col in salary.columns:\n    jobs[col] = salary[col]","b69bc6c8":"def extract_requirements(text):\n    \"\"\" Extract the main and seconday requirements from the text\n\n        param text: String to extract the values\n        return result: DataFrame with the main and seconday requirements\n    \"\"\" \n    reg_expr_set = r'^\\d(?=\\.)'\n    reg_expr_subset = r'^[a-z](?=\\.)'\n    find = re.finditer(reg_expr_set, text, re.MULTILINE|re.IGNORECASE)\n    result = [(x.group(0), x.span(0)[0]) for x in find]\n    result = pd.DataFrame(result, columns=['set', 'start'])\n    result['end'] = result['start'].apply(lambda x: np.min(result[result['start'] > x]['start'])).fillna(len(text))\n    for index,row in result.iterrows():\n        extracted_text = text[int(row['start']):int(row['end'])].strip()\n        result.loc[index, 'set_text'] = extracted_text\n\n        find_subset = re.finditer(reg_expr_subset, extracted_text, re.MULTILINE|re.IGNORECASE)\n        result_subset = [(x.group(0).lower(), x.span(0)[0]) for x in find_subset]\n        result_subset = pd.DataFrame(result_subset, columns=['subset', 'start'])\n        result_subset['end'] = result_subset['start'].apply(lambda x: np.min(result_subset[result_subset['start'] > x]['start'])).fillna(len(extracted_text))\n        for index_sub, row_sub in result_subset.iterrows():\n            extracted_sub_text = extracted_text[int(row_sub['start']):int(row_sub['end'])].strip()\n            result.loc[index, row_sub['subset']] = extracted_sub_text\n            result.loc[index, 'set_text'] = result.loc[index, 'set_text'].replace(extracted_sub_text, '').strip()\n    result.drop(['start', 'end'], axis=1, inplace=True) \n    return result\n\ntemp = jobs['requirements_text'].dropna().apply(extract_requirements)\n\nrequirements = pd.DataFrame()\nfor i, value in temp.iteritems(): \n    if isinstance(value, pd.DataFrame):\n        value['file'] = jobs.loc[i, 'file']\n        requirements = requirements.append(value, ignore_index=True)\n        \nrequirements = requirements.melt(id_vars=['file', 'set', 'set_text'], var_name='subset', value_name='subset_text')\nrequirements = requirements[(requirements['subset'] == 'a') | ((requirements['subset'] != 'a') & (~requirements['subset_text'].isnull()))]  \nrequirements['subset'] = requirements['subset'].apply(lambda x: x.upper())","d3c5d75d":"def get_school_type(text):\n    \"\"\" Extract the school type from the text\n\n        param text: String to extract the values\n        return result: String with the schol_type code\n    \"\"\" \n    regex_dic = {'COLLEGE OR UNIVERSITY':'college or university', \n                 'HIGH SCHOOL':'high school', \n                 'APPRENTICESHIP':'apprenticeship'\n                }\n    result = np.nan\n    for key, value in regex_dic.items():\n        regex = value\n        regex_find = re.findall(regex, text, re.DOTALL|re.IGNORECASE)\n        if regex_find:\n            result = key\n            break\n    return result\n\nrequirements['school_type'] = requirements['set_text'].dropna().apply(get_school_type)","6720bba8":"def get_education_years(text):\n    \"\"\" Extract the education years from the text\n\n        param text: String to extract the values\n        return result: String with the schol_type code\n    \"\"\" \n    regex_dic = {1:'one', 2:'two', 3:'three', 4:'four', 5:'five',\n                6:'six', 7:'seven', 8:'eight', 9:'nine'}\n    result = np.nan\n    for key, value in regex_dic.items():\n        regex = value+'[\/s-]year.*(college or university|high school|apprenticeship)'\n        regex_find = re.findall(regex, text, re.DOTALL|re.IGNORECASE)\n        if regex_find:\n            result = key\n            break\n    return result\n\nrequirements['education_years'] = requirements['set_text'].dropna().apply(get_education_years)","a8aa9b41":"def get_full_part_time(text):\n    \"\"\" Extract the full time \/ part time from the text\n\n        param text: String to extract the values\n        return result: String with the full time \/ part time\n    \"\"\" \n    regex_dic = {'FULL_TIME':'full-time', \n                 'PART_TIME':'part-time', \n                }\n    result = np.nan\n    for key, value in regex_dic.items():\n        regex = value\n        regex_find = re.findall(regex, text, re.DOTALL|re.IGNORECASE)\n        if regex_find:\n            result = key\n            break\n    return result\n\nrequirements['full_time_part_time'] = requirements['set_text'].dropna().apply(get_full_part_time)","ce20b820":"def get_experience_length(text):\n    \"\"\" Extract the experience length from the text\n\n        param text: String to extract the values\n        return result: String with the experience length\n    \"\"\" \n    regex_dic = {1:'one', 2:'two', 3:'three', 4:'four', 5:'five',\n                6:'six', 7:'seven', 8:'eight', 9:'nine'}\n    result = np.nan\n    for key, value in regex_dic.items():\n        regex = r'(?!.*(college or university|high school|apprenticeship)).+'+value+'[\\s-]year.*(full[\\s-]time|part[\\s-]time)'\n        regex_find = re.findall(regex, text, re.DOTALL|re.IGNORECASE)\n        if regex_find:\n            result = key\n            break\n    return result\n\nrequirements['experience_length'] = requirements['set_text'].dropna().apply(get_experience_length)","b441061b":"def get_exp_job_class_title(text, job_titles=job_titles):\n    \"\"\" Check if the text has a job from the job_titles list\n\n        param text: String to extract the values\n        param job_titles: List with possible job titles\n        return result: String with the job title\n    \"\"\" \n    result = np.nan\n    for x in job_titles:\n        if x.lower() in text.lower():\n            result = x\n            break\n    return result\n\nrequirements['exp_job_class_title'] = requirements['set_text'].dropna().apply(get_exp_job_class_title)","d3050e9c":"drop_cols = ['job_description', 'metadata', 'salary', 'duties', \n        'requirements', 'where_to_apply', \n        'application_deadline', 'selection_process']\nexample = jobs[jobs['file'] == 'SYSTEMS ANALYST 1596 102717.txt'].drop(drop_cols, axis=1).iloc[0,:].dropna()\nfor idx in example.index:\n    print('\\033[42m'+idx+':'+'\\033[0m')\n    print(example[idx])","29a14be5":"data_dictionary","9b53cfd1":"# Initialize empty DataFrame with 'Field Name' as Columns\nsubmission = pd.DataFrame(columns=data_dictionary['Field Name'].values)\n\n# FILE_NAME\nsubmission['FILE_NAME'] = jobs['file']\n# JOB_CLASS_TITLE\nsubmission['JOB_CLASS_TITLE'] = jobs['job_title']\n# JOB_CLASS_NO\nsubmission['JOB_CLASS_NO'] = jobs['class_code']\n# EXAM_TYPE\nsubmission['EXAM_TYPE'] = jobs['exam_type']\n# OPEN_DATE\nsubmission['OPEN_DATE'] = jobs['open_date']\n# ENTRY_SALARY_GEN\nsubmission['ENTRY_SALARY_GEN'] = jobs['salary_0']\n# ENTRY_SALARY_DWP\nsubmission['ENTRY_SALARY_DWP'] = jobs['salary_1']\n# DRIVERS_LICENSE_REQ\nsubmission['DRIVERS_LICENSE_REQ'] = jobs['driver_license']\n# JOB_DUTIES\nsubmission['JOB_DUTIES'] = jobs['duties_text']\n\n# Merge with Requirements\nsubmission = pd.merge(submission, requirements, left_on='FILE_NAME', right_on='file', how='left')\nsubmission['REQUIREMENT_SET_ID'] = submission['set']\nsubmission['REQUIREMENT_SUBSET_ID'] = submission['subset']\nsubmission['EDUCATION_YEARS'] = submission['education_years']\nsubmission['SCHOOL_TYPE'] = submission['school_type']\nsubmission['EXP_JOB_CLASS_FUNCTION'] = submission['subset_text']\nsubmission['FULL_TIME_PART_TIME'] = submission['full_time_part_time']\nsubmission['EXPERIENCE_LENGTH'] = submission['experience_length']\nsubmission['EXP_JOB_CLASS_TITLE'] = submission['exp_job_class_title']\nsubmission.drop(requirements.columns, axis=1, inplace=True)","f4ecba5b":"submission[submission['FILE_NAME'] == 'SYSTEMS ANALYST 1596 102717.txt'].T","d522d6b0":"def validate_submission(submission, data_dictionary):\n    \"\"\" Makes some validations and creates a summary of the submission file\n\n        param submission: DataFrame ob the Submission file\n        param data_dictionary: DataFrame with the data dictionary to validate the submission file\n        return result: DataFrame with a summary of the validation\n    \"\"\" \n    result = pd.DataFrame(index=data_dictionary['Field Name'].values)\n    for col in data_dictionary['Field Name'].values:\n        result.loc[col, 'Unique Values'] = len(submission[col].dropna().unique())\n        result.loc[col, 'Values'] = len(submission[col].dropna())\n        result.loc[col, 'Null Values'] = len(submission[submission[col].isnull()][col])\n        result.loc[col, 'Accept Null Values'] = data_dictionary[data_dictionary['Field Name'] == col]['Accepts Null Values?'].values[0]\n        result.loc[col, 'Data Type'] = submission[col].dtype\n        result.loc[col, 'Expected Data Type'] = data_dictionary[data_dictionary['Field Name'] == col]['Data Type'].values[0]\n        result.loc[col, 'Check Values'] = ('Okay' if (result.loc[col, 'Accept Null Values'] == 'Yes') or (result.loc[col, 'Null Values'] == 0) else 'No Null Values allowed')\n        result.loc[col, 'Check Data Type'] = ('Okay' \n                                              if ((result.loc[col, 'Data Type'] == 'object') and (result.loc[col, 'Expected Data Type'] == 'String'))\n                                              or ((result.loc[col, 'Data Type'] == 'float64') and (result.loc[col, 'Expected Data Type'] == 'Float'))\n                                              or ((result.loc[col, 'Data Type'] == 'datetime64[ns]') and (result.loc[col, 'Expected Data Type'] == 'Date'))\n                                              or ((result.loc[col, 'Data Type'] == 'int64') and (result.loc[col, 'Expected Data Type'] == 'Integer'))\n                                              else 'Check Data Type')\n    return result\n\nvalidate_submission(submission, data_dictionary)","455379a5":"temp = jobs.fillna('Missing')\ntemp = temp.applymap(lambda x: x if x == 'Missing' else 'Available')\nfigsize_width = 12\nfigsize_height = len(temp.columns)*0.5\nplt_data = pd.DataFrame()\nfor col in temp.columns:\n    temp_col = temp.groupby(col).size()\/len(temp.index)\n    temp_col = pd.DataFrame({col:temp_col})\n    plt_data = pd.concat([plt_data, temp_col], axis=1)\n    \nax = plt_data.T.plot(kind='barh', stacked=True, figsize=(figsize_width, figsize_height))\n\n# Annotations\nlabels = []\nfor i in plt_data.index:\n    for j in plt_data.columns:\n        label = '{:.2%}'.format(plt_data.loc[i][j])\n        labels.append(label)\npatches = ax.patches\nfor label, rect in zip(labels, patches):\n    width = rect.get_width()\n    if width > 0:\n        x = rect.get_x()\n        y = rect.get_y()\n        height = rect.get_height()\n        ax.text(x + width\/2., y + height\/2., label, ha='center', va='center')\n\nplt.xlabel('Frequency')\nplt.title('Missing values')\nplt.xticks(np.arange(0, 1.05, 0.1))\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()","3ff8ce75":"plt_data = (jobs.groupby(jobs['open_date'].dropna().dt.strftime('%Y-%m')).size())\nplt_data = pd.DataFrame(plt_data)\nplt_data.plot(kind='bar', figsize=(15, 5))\nplt.xlabel('Month')\nplt.ylabel('Quantity')\nplt.title('Distribution over time')\nplt.legend('')\nplt.show()","728d74d4":"plt_data = jobs[['salary_from', 'salary_to']]\nplt_data.plot(kind='box', showfliers=True, vert=False, figsize=(12, 3), grid=True)\nplt.xticks(range(0, 300001, 25000))\nplt.xlabel('Salary')\nplt.title('Salary Distribution')\nplt.show()\nplt_data.describe()","cb70b1a9":"plt_data = jobs[['salary_from', 'salary_to']]\nplt_data.plot(kind='hist', bins=1000, density=True, histtype='step', cumulative=True, figsize=(15, 7), lw=2, grid=True)\nplt.xlabel('Salary')\nplt.ylabel('Cumulative')\nplt.title('Cumulative histogram for salary')\nplt.legend(loc='upper left')\nplt.xlim([25000, 200000])\nplt.xticks(range(25000, 200001, 10000))\nplt.yticks(np.arange(0, 1.05, 0.05))\nplt.show()","82a830f6":"plt_data_duties = jobs['duties_text'].astype(str).apply(lambda x: len(x.split()))\nplt_data_requirements = jobs['requirements_text'].astype(str).apply(lambda x: len(x.split()))\nplt_data = pd.DataFrame([plt_data_duties, plt_data_requirements]).T\n\nplt_data.plot(kind='box', showfliers=False, vert=False, figsize=(12, 3), grid=True)\nplt.xticks(range(0, 201, 10))\nplt.xlabel('Words')\nplt.title('Word count')\nplt.show()","1a797610":"plt_data = jobs['exam_type'].fillna('None')\nplt_data = plt_data.groupby(plt_data).size()\nplt_data.plot(kind='pie', figsize=(10, 5), autopct='%.2f')\nplt.title('Exam Type')\nplt.ylabel('')\nplt.show()","dad18ffa":"plt_data = jobs['driver_license'].fillna('None')\nplt_data = plt_data.groupby(plt_data).size()\nplt_data.plot(kind='pie', figsize=(10, 5), autopct='%.2f')\nplt.title('Driver License')\nplt.ylabel('')\nplt.show()","145ace31":"pos_tags = ['NOUN', 'VERB', 'PROPN', 'ADJ']\nplot_cols = 2\nplot_rows = math.ceil(len(pos_tags) \/ plot_cols)\naxisNum = 0\nplt.figure(figsize=(7*plot_cols, 4*plot_rows))\nfor pos_tag in pos_tags:\n    plt_data = nlp_transformation(jobs['duties_text'].dropna(), [pos_tag])\n    plt_data = [j for i in plt_data for j in i]\n    plt_data=Counter(plt_data)\n    wordcloud = WordCloud(margin=0, max_words= 40, random_state=SEED).generate_from_frequencies(plt_data)\n    axisNum += 1\n    ax = plt.subplot(plot_rows, plot_cols, axisNum)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    title = 'Duties ({})'.format(pos_tag)\n    plt.title(title)\n    plt.axis(\"off\")\nplt.show()    ","2c4a30e1":"pos_tags = ['NOUN', 'VERB', 'PROPN', 'ADJ']\nplot_cols = 2\nplot_rows = math.ceil(len(pos_tags) \/ plot_cols)\naxisNum = 0\nplt.figure(figsize=(7*plot_cols, 4*plot_rows))\nfor pos_tag in pos_tags:\n    plt_data = nlp_transformation(jobs['requirements_text'].dropna(), [pos_tag])\n    plt_data = [j for i in plt_data for j in i]\n    plt_data=Counter(plt_data)\n    wordcloud = WordCloud(margin=0, max_words= 40, random_state=SEED).generate_from_frequencies(plt_data)\n    axisNum += 1\n    ax = plt.subplot(plot_rows, plot_cols, axisNum)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    title = 'Requirements ({})'.format(pos_tag)\n    plt.title(title)\n    plt.axis(\"off\")\nplt.show()    ","e1e5cac7":"plt_data = requirements['exp_job_class_title'].dropna().tolist()\nplt_data=Counter(plt_data)\nplt.figure(figsize=(10, 10))\n\nwordcloud = WordCloud(margin=0, random_state=SEED).generate_from_frequencies(plt_data)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title('Requirements (Job class title)')\nplt.axis(\"off\")\nplt.show() ","4b8bae3b":"### Salary  \n* Salary from\n* Salary to\n* Flat-rated\n* Additional informations\n* Notes","51fc2c15":"## Table of Content  \n1. [Introduction](#introduction)\n2. [Preparation](#preparation)\n3. [Data Extraction](#data_extraction)  \n4. [Submission](#submission)\n5. [Exploratory Data Analysis (EDA)](#eda)","90c98f5d":"### Wordcloud (Requirements - Job class title)  \nThe most frequently found job class titles in the requirement text.","29ba73c7":"## 1. Introduction <a id=\"introduction\"><\/a> \nThis notebook uses regular expression to extract the relevant data we need for the submission. some usefull links to work with regular expression are:\n* [https:\/\/regexr.com\/](https:\/\/regexr.com\/) : A JavaScript Engine with explaination of the regular expressions.  \n* [https:\/\/pythex.org\/](https:\/\/pythex.org\/) : A implementation of the python `re.findall()` function, to test regular expression with the different flags.  \n* [https:\/\/docs.python.org\/3\/library\/re.html](https:\/\/docs.python.org\/3\/library\/re.html): Link to the python `re` library with the explanation of functions and parameters.  \n* [https:\/\/docs.python.org\/3\/howto\/regex.html](https:\/\/docs.python.org\/3\/howto\/regex.html): Informations about how to use the `re` library.  \n\nIn the section [Preparation](#preparation) we load all necesary libraries and files  \n* `job_titles`: List with all available job titles  \n* `data_dictionary`: Description how to create the submission file. We will use this later, to create a template for the `submission`.\n* `jobs`: DataFrame with the plain text of all job bulletins. This will be our working dataset.  \n* `submission`: Later we will create this DataFrame for the final submission file.  \n\nOne of the main tasks is to extract the data. This happens in the [Data Extraction](#data_extraction), where we will extract the data step by step using regular expressions. For example, in the first step, the `'Requirements'` section is extracted and then in the second step, the Details from this (`School type, Education years, ...`).  \nThe next Step in the [Submission](#submission) is to create the final submission file. We will use the `data_dictionary` to create a empty DataFrame with all necessary columns and fill them in the required format. In addition, there is a validation function that gives an overview of where potential errors exist.  \nAfter we have our data, we use [Exploratory Data Analysis (EDA)](#eda) to get some insight into the data. There we can see how the distribution of the salary is or which words occur most frequently in the 'Duties' or 'Requirements'.","6bf3677f":"For now, we will remove the file `'Vocational Worker  DEPARTMENT OF PUBLIC WORKS.txt'`, because it contains a completely different format.","f43e9e3d":"### Wordcloud (Requirements)  \nThe most frequently found noun, verbs, proper noun and adjective in the requirement text.","05973c03":"### Print Example  \nLet's have a look on an example to see which information has been extracted.  \n(Expand the `output` to see the result)","4071e24a":"### Open Date  \nThis plot should show when new jobs should be occupied.","f3b9a8a8":"* **Driver License**","9a460edb":"### Missing Values  \nHere we can check where data is available and which information is less frequently available.  \nSince certain information needs to be checked and the data extraction may need to be adjusted, this plot may still change.","eef45fee":"**Create Submission file**","5dd46295":"* **Education Years**","31ee4d74":"### Application deadline\n* Text\n* Notes\n* Review","0560c538":"## 2. Preparations <a id=\"preparation\"><\/a>","880a2b3a":"### Exam Type  \nOPEN_INT_PROM = 'Open or Competitive Interdepartmental Promotional'  \nINT_DEPT_PROM = 'Interdepartmental Promotional'  \nDEPT_PROM = 'Departmental Promotional'  \nOPEN = 'Exam open to anyone'  \nNone = Not defined","aca56d77":"### Global Parameters","9aa5cded":"### Wordcount","cc2b8af0":"### Other Details  \n* **Exam Type**","108c23b2":"### Salary  \nHere the distribution of the salary can be checked.  \n50% of jobs will earn at least \\$80,000.  \nThe maximum annual salary is currently \\$280,000.","3b86784d":"* **Experience Length**","762f1376":"### Where to apply\n* Text\n* Notes","85a89b72":"### Metadata  \n* Job title\n* Class code\n* Open Date\n* Revised","cdd04747":"### Requirements Details \n* **Main requirements**  \n* **Secondary requirements**  \n\nExtracting the information from the requirements will create several rows per file. Because of this we first save them in a separate DataFrame `requirements`.","bdb10c72":"#### Kaggle Data Dictionary","c6108232":"**Examples of the Submission file**  \nAs an example we use the 'SYSTEMS ANALYST 1596 102717.txt', which was also given by the Kaggle Team as an example submission. This allows us to check our result with their result.  \n(Expand the `output` to see the result)","8574f549":"### Load Data","47ca26b3":"* **Salary Informations**","9291a3b8":"### Duties\n* Text\n* Notes","354ff5b2":"* **School Type**","a21b5e4c":"#### Job Bulletins","ae90b3e7":"#### Job Titles","5feec96c":"### Requirements\n* Text\n* Notes\n* Certifications","1dd85f7b":"**Validation of Submission**  \nA function has been defined, to validate out submission against the specifications and return a summary of the result.  \nThe following information are currently available:\n* **Index:** The Name of the Column in the `submission` DataFrame.\n* **Unique Values:** Number of unique values.  \n* **Values:** Number of filled values.  \n* **Null Values:** Number of null-values.  \n* **Accept Null Values:** Does the column accept null-values? (Yes\/No).  \n* **Data Type:** Data type of the column.  \n* **Expected Data Type:** Expected data type of the column.  \n* **Check Values:** Check the null-value constraint.\n* **Check Data Type:** Check the data type constraint.","d75eef0a":"### Upper Sections  \n* Metadata\n* Salary\n* Duties\n* Requirements\n* Where to apply\n* Application deadline\n* Selection Process\n","2547314a":"### Wordcloud (Duties)  \nThe most frequently found verbs in the duties text.","ee549866":"## 3. Data Extraction <a id=\"data_extraction\"><\/a>  \nWe will use regular expressions to extract the relevant information.  \nWe will first divide the text into general parts (metadata, salary, ...) and then extract details from them (metadata -> job title, class code, open date, ...).","288f5be1":"## 4. Submission <a id=\"submission\"><\/a>  \nHere we will create the submission file. \nFirst of all we can take a look off the definition from the Kaggle data dictionary.  \n(Expand the `output` to see the result)   \nThen we create an empty DataFrame `submission` based on the given columns and fill this with our extracted data.  \nFinally, we will display an example and run the validation.","527edc78":"## 5. Exploratory Data Analysis (EDA) <a id=\"eda\"><\/a>","5e8dabd3":"### Selection process\n* Text\n* Notes\n* Notice","e804294b":"### Libraries","bedea60e":"* **Job class title of Internal City job**","eb997007":"### Driver License \nP = 'Posible'  \nR = 'Required'  \nNone = Not defined","a88b481f":"* **Full Time \/ Part Time**"}}