{"cell_type":{"22b9bae6":"code","ad71d32e":"code","08561ad4":"code","6fd47119":"code","0244d28b":"code","e0f3d3cb":"code","f44667af":"code","96924100":"code","e1e8f7d9":"code","d37f83fa":"code","8d2b3efc":"code","f3b80d48":"code","e6358499":"code","5656da04":"code","25f9ba37":"code","3fdf1519":"code","296e028b":"code","40bd3f84":"code","3fb88118":"code","1f6e89a7":"code","760c4326":"code","e41c4421":"code","3e6bdb2f":"code","a126a3c3":"code","129f329c":"code","5bbd779e":"code","3743f197":"code","018b1675":"code","2414fb13":"code","afedbcae":"code","b94b0a3b":"code","3799758a":"code","f9ff73d3":"code","e9888b65":"code","df79b963":"code","3844996c":"code","c69109fe":"code","45cfc30b":"markdown","1a833bb6":"markdown","d4f26da8":"markdown","d1234bad":"markdown","0ef8302c":"markdown","a613dc2c":"markdown","781b3d98":"markdown","99f818ae":"markdown","9a0f889d":"markdown","f6772cda":"markdown","be6b438d":"markdown","a518b7f4":"markdown","29613851":"markdown","709486f6":"markdown","3234b029":"markdown"},"source":{"22b9bae6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ad71d32e":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nimport sklearn.metrics as metrics\nimport os","08561ad4":"data = pd.read_csv('\/kaggle\/input\/pizza-price-prediction\/pizza_v2.csv')","6fd47119":"data.head()","0244d28b":"data.isnull().sum()","e0f3d3cb":"data.describe","f44667af":"data.info()","96924100":"data.head()","e1e8f7d9":"data['price_rupiah'] = data['price_rupiah'].apply(lambda x : x.replace(\"Rp\",\"\").replace(\",\",\".\"))\n# remove rp and comma and store it in new price column\ndata['diameter'] = data['diameter'].apply(lambda x : x.replace(\" inch\",\"\"))\n#remove inch and store it in new column","d37f83fa":"data.head()","8d2b3efc":"data[\"diameter\"] = data[\"diameter\"].astype(float)\ndata[\"price_rupiah\"] = data[\"price_rupiah\"].astype(float)","f3b80d48":"data.info()","e6358499":"data.head()","5656da04":"cat_col = [col for col in data.columns if data[col].dtype == 'object']\nnum_col = [col for col in data.columns if data[col].dtype != 'object']","25f9ba37":"for col in cat_col:\n    print(f'A {col} has {data[col].unique()} list of values and the number is {data[col].nunique()}\\n unique values \\n')","3fdf1519":"data.info()","296e028b":"#OneHot won't be a good idea coz there are too less rows\nen = LabelEncoder()\n#catCols = ['company','topping','variant','size','extra_sauce','extra_cheese','extra_mushrooms']\nfor cols in cat_col:\n    data[cols] = en.fit_transform(data[cols])","40bd3f84":"data.head()","3fb88118":"data.info()","1f6e89a7":"Y = data[\"price_rupiah\"]\nX = data.drop(\"price_rupiah\",axis = 1)","760c4326":"X.head()","e41c4421":"Y.head()","3e6bdb2f":"Y=pd.DataFrame(Y)","a126a3c3":"Y.head()","129f329c":"# Break off validation set from training data\nX_train_full, X_valid_full, Y_train, Y_valid = train_test_split(X, Y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","5bbd779e":"X_train_full.head()","3743f197":"Y_train.head()","018b1675":"sns.pairplot(data)","2414fb13":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\n# Define the model\nmodel = XGBRegressor(n_estimators=1000, learning_rate=0.09) \n\n# Fit the model\nmodel.fit(X_train_full, Y_train) \n\n# Get predictions\npredictions = model.predict(X_valid_full) \n\n# Calculate MAE\nmae = mean_absolute_error(predictions, Y_valid)\nprint(\"Mean Absolute Error:\" , mae)\n\n#Calculating R2\nr2 =  metrics.r2_score(Y_valid, predictions)\nprint(\"R2 score :\", r2)","afedbcae":"predictions = pd.DataFrame(predictions, columns=Y_valid.columns)\npredictions","b94b0a3b":"Y_valid","3799758a":"Y_valid.reset_index(drop=True, inplace=True)\nfinal = pd.concat([predictions,Y_valid],axis=1)\nfinal.columns = ['Predictions', 'Y_valid']\nfinal","f9ff73d3":"# Define the model\nmodel2 = XGBRegressor(n_estimators=1000, learning_rate=0.01,max_depth=10) \n\n# Fit the model\nmodel2.fit(X_train_full, Y_train) \n\n# Get predictions\npredictions2 = model2.predict(X_valid_full) \n\n# Calculate MAE\nmae2 = mean_absolute_error(predictions2, Y_valid)\nprint(\"Mean Absolute Error:\" , mae2)\n\n#Calculating R2\nr22 =  metrics.r2_score(Y_valid, predictions2)\nprint(\"R2 score :\", r22)","e9888b65":"Y_train.shape","df79b963":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators=1000)\n\nrf.fit(X_train_full, np.array(Y_train).ravel())","3844996c":"rf.score(X_valid_full, Y_valid)","c69109fe":"metrics.r2_score(rf.predict(X_valid_full), Y_valid)","45cfc30b":"### Checking for categories under each categorical feature\n","1a833bb6":"# Test-Train Split","d4f26da8":"### Whole dataset is in numerical form now!!\n### \n### \n","d1234bad":"### Encoding categorical features","0ef8302c":"# Analyzing Preparing the Dataset","a613dc2c":"# Importing Necessary Libraries","781b3d98":"### No Missing Values","99f818ae":"# Splitting into X and Y","9a0f889d":"# XGB Regressor","f6772cda":"Only graph which seems interesting is price vs diameter which is quite what we expect naturally.","be6b438d":"### Convering Object data like Price and Diameter to numerical ","a518b7f4":"Seems that XGBReg is doing far better.","29613851":"# Improved XGB egressor","709486f6":"### All object values","3234b029":"# Reading the Dataset"}}