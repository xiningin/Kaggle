{"cell_type":{"16b72c18":"code","224b3f26":"code","e062079f":"code","bf43d8f2":"code","e674754e":"code","83722bea":"code","7b31c53a":"code","6d48500b":"code","e4c6d0a9":"code","8c9a9c9b":"code","080b55d9":"code","14116639":"code","8a76a000":"code","c1543a17":"code","73c1f4d1":"code","4a1c4bd0":"code","7c41f343":"code","b3a4e150":"code","1c292f9b":"code","e9c3fdfb":"code","61ceb9f7":"code","84c22c09":"code","6cb58581":"code","40643606":"code","99d782ff":"code","d79c812f":"code","3e98b2b5":"code","2a316656":"code","75495489":"markdown"},"source":{"16b72c18":"import numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly_express as px\n\n%matplotlib inline\n# sns.set_palette(['#FF1744', '#666666'], 2)\n# sns.set_style(\"whitegrid\")\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.linear_model import SGDClassifier\nfrom xgboost import XGBClassifier\n\nimport lightgbm as lgb\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance","224b3f26":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","e062079f":"print(train.shape, test.shape)","bf43d8f2":"train.head()","e674754e":"train.info()","83722bea":"# train.descirbe()","7b31c53a":"cols = []\nfor i in train.columns:\n    cols.append(i)\ncols.remove('id')\ncols.remove('target')\n\ncols_corr = []\nfor i in cols:\n    cols_corr.append(train[[i, 'target']].corr()[i]['target'])\n    \ncorr_df = pd.DataFrame()\ncorr_df['col'] = cols\ncorr_df['corr'] = cols_corr\n\nplt.figure(figsize=(20, 5))\nplt.bar(corr_df['col'], corr_df['corr'])\nplt.xticks([])\nplt.show()","6d48500b":"train['wheezy-copper-turtle-magic'].value_counts().head()","e4c6d0a9":"scaler = StandardScaler()\n\ndis_cols = [i for i in train.columns if i not in ['id', 'target', 'wheezy-copper-turtle-magic']]   \ntrain[cols] = scaler.fit_transform(train[cols])\ntest[cols] = scaler.fit_transform(test[cols])","8c9a9c9b":"train['wheezy-copper-turtle-magic'] = train['wheezy-copper-turtle-magic'].astype('category')\ntest['wheezy-copper-turtle-magic'] = test['wheezy-copper-turtle-magic'].astype('category')","080b55d9":"train = train.sample(10000)","14116639":"# train = train[['id', 'wheezy-copper-turtle-magic', 'target']]\n# test = test[['id', 'wheezy-copper-turtle-magic']]","8a76a000":"# train = pd.concat([train, pd.get_dummies(train['wheezy-copper-turtle-magic'], prefix='magic', drop_first=True)], axis=1).drop(['wheezy-copper-turtle-magic'], axis=1)\n# test = pd.concat([test, pd.get_dummies(test['wheezy-copper-turtle-magic'], prefix='magic', drop_first=True)], axis=1).drop(['wheezy-copper-turtle-magic'], axis=1)","c1543a17":"# for i in train.columns:\n#     if train[i].dtype=='object':\n#         print(i)","73c1f4d1":"# plt.figure(figsize=(50, 50))\n# sns.heatmap(train.corr(), annot=True)","4a1c4bd0":"X = train.drop(['id', 'target'], axis=1)\ny = train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)","7c41f343":"model = GaussianNB()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\nprint(accuracy_score(pred, y_test))\nprint(confusion_matrix(pred, y_test))","b3a4e150":"predictions = model.predict(test.drop(['id'], axis=1))\ndf = pd.read_csv('..\/input\/sample_submission.csv')\ndf['target'] = predictions\ndf.to_csv('submission.csv', index=False)\ndf.head(10)","1c292f9b":"# model = SVC(gamma='scale')\n# model.fit(X_train, y_train)\n# pred = model.predict(X_test)\n# print(accuracy_score(pred, y_test))\n# print(confusion_matrix(pred, y_test))","e9c3fdfb":"# model = LogisticRegression()\n# model.fit(X_train, y_train)\n# pred = model.predict(X_test)\n# print(accuracy_score(pred, y_test))\n# print(confusion_matrix(pred, y_test))","61ceb9f7":"# model = KNeighborsClassifier()\n# model.fit(X_train, y_train)\n# pred = model.predict(X_test)\n# print(accuracy_score(pred, y_test))\n# print(confusion_matrix(pred, y_test))","84c22c09":"# model = DecisionTreeClassifier()\n# model.fit(X_train, y_train)\n# pred = model.predict(X_test)\n# print(accuracy_score(pred, y_test))\n# print(confusion_matrix(pred, y_test))","6cb58581":"# feature = train.columns\n# importance = model.feature_importances_\n# indices = np.argsort(importance)\n\n# plt.rcParams['figure.figsize'] = (12, 50)\n# plt.barh(range(len(indices)), importance[indices])\n# plt.yticks(range(len(indices)), feature[indices])\n# plt.xlabel('Relative Importance')\n# plt.show()","40643606":"# model = RandomForestClassifier()\n# model.fit(X_train, y_train)\n# pred = model.predict(X_test)\n# print(accuracy_score(pred, y_test))\n# print(confusion_matrix(pred, y_test))","99d782ff":"# model = SGDClassifier()\n# model.fit(X_train, y_train)\n# pred = model.predict(X_test)\n# print(accuracy_score(pred, y_test))\n# print(confusion_matrix(pred, y_test))","d79c812f":"# model = XGBClassifier()\n# model.fit(X_train, y_train)\n# pred = model.predict(X_test)\n# print(accuracy_score(pred, y_test))\n# print(confusion_matrix(pred, y_test))","3e98b2b5":"# params = {\n#     'task': 'train',\n#     'boosting_type': 'gbdt',\n#     'objective': 'binary',\n#     'metric': 'auc',\n#     'learning_rate': 0.05,\n#     'num_leaves': 63,\n#     'feature_fraction': 0.7,\n# }\n\n# lgbtrain = lgb.Dataset(X_train, label=y_train)\n# gbmdl = lgb.train(params, lgbtrain, num_boost_round=20000)\n# pred = gbmdl.predict(X_test,num_iteration=gbmdl.best_iteration)\n# print(accuracy_score(pred, y_test))\n# print(confusion_matrix(pred, y_test))","2a316656":"# type(pred, y_test)","75495489":"**This is an anonymized, binary classification dataset found on a USB stick that washed ashore in a bottle.   \nThere was no data dictionary with the dataset, but this poem was handwritten on an accompanying scrap of paper:**\n>> *Silly column names abound,   \n>> but the test set is a mystery.   \n>> Careful how you pick and slice,    \n>> or be left behind by history.   *"}}