{"cell_type":{"2a7ef085":"code","1f707fcd":"code","81f61b9c":"code","ab45e9e3":"code","a3c8f5d0":"code","550896d0":"code","c925ad4c":"code","92a188ed":"code","73faabf1":"code","e230ac16":"markdown","1919d80c":"markdown","3e380dd1":"markdown","aae4e4bd":"markdown","3ddae109":"markdown","648389a1":"markdown","3b7ea43c":"markdown","eedbc503":"markdown"},"source":{"2a7ef085":"# importing python dependencies\n\nimport pandas as pd\nimport numpy as np\nfrom pprint import pprint\nimport io\n\n# read csv file, adding back name of features (column headings)\npath = '..\/input\/zoo-data\/zoo.csv'\ndataset = pd.read_csv(path,\n                      names=['animal_name','hair','feathers','eggs','milk',\n                             'airbone','aquatic','predator','toothed','backbone',\n                             'breathes','venomous','fins','legs','tail','domestic','catsize','class',])\n\ndataset.head()","1f707fcd":"# remove animal names column, as bad feature to split data on\ndataset=dataset.drop('animal_name',axis=1)","81f61b9c":"# calculate entropy\ndef compute_entropy(samples):\n    if len(samples) < 2:\n        return 0\n    freq = np.array(samples.value_counts(normalize=True))\n    return -(freq * np.log2(freq + 1e-6)).sum()\n\n# calculate information gain\ndef info_gain(data, attr, target):\n    values = data[attr].value_counts(normalize=True)\n    split_ent = 0\n    for value, count in values.iteritems():\n        idx = data[attr] == value\n        sub_ent = compute_entropy(target[idx])\n        split_ent += count * sub_ent\n    ent = compute_entropy(target)\n    return ent - split_ent","ab45e9e3":"# tree implementation\nclass TreeNode:\n    # recursively defined data structure to store tree\n    # each node contains other nodes as children\n    def __init__(self, node=\"\", min_sample=1, default_decision=None):\n        self.children = {} # sub nodes\n        self.decision = None # undecided\n        self.split_feat = None # splitting feature\n        self.name = node\n        self.default_decision = default_decision\n        self.min_sample = min_sample\n\n    # pretty print function\n    def pretty_print(self, prefix=''):\n        if self.split_feat is not None:\n            for each, value in self.children.items():\n                value.pretty_print(f\"{prefix}:When {self.split_feat} is {each}\")\n        else:\n            print(f\"{prefix}:{self.decision}\")\n\n    # predict function\n    def predict(self, data):\n        if self.decision is not None:\n            # uncomment to debug, view more details\n            # print(\"Decision:\", self.decision)\n            return self.decision\n        else:\n            attr_val = data[self.split_feat]\n            if self.children.get(attr_val) is None:\n                return self.decision\n            child = self.children[attr_val]\n            # uncomment to debug, view more details\n            # print(\"Testing \", self.split_feat, \"->\", attr_val)\n            return child.predict(data)\n\n    # fit data to model\n    def fit(self, data, target):\n        # function accepts training dataset\n        # from which a tree is built to make decisions or to make children nodes\n        # set prediction to mode if decision is none\n        if self.default_decision is None:\n            self.default_decision = target.mode()[0]\n        # uncomment to debug, view more details\n        # print(self.name, \"received\", len(data), \"samples\")\n        # if length of data is less than specified min sample, make decision\n        if len(data) < self.min_sample:\n        # if data is empty, arbitrary decision is made\n            if len(data) == 0:\n                self.decision = self.default_decision\n                # uncomment to debug, view more details\n                # print(\"Decision: \", self.decision)\n                # else mode is used\n            else:\n                self.decision = target.mode()[0]\n                # uncomment to debug, view more details\n                # print(\"Decision: \", self.decision)\n            return\n        else:\n            unique_values = target.unique()\n            # if only 1 unique prediction in target, make decision the target\n            if len(unique_values) == 1:\n                self.decision = unique_values[0]\n                # uncomment to debug, view more details\n                # print(\"Decision: \", self.decision)\n                return\n            else:\n                # select split feature\n                info_gain_max = 0\n                for attr in data.keys(): # examine each attribute\n                    attr_ig = info_gain(data, attr, target)\n                    if attr_ig > info_gain_max:\n                        info_gain_max = attr_ig\n                        self.split_feat = attr\n                # uncomment to debug, view more details\n                # print(f\"Split by {self.split_feat}, IG: {info_gain_max:.2f}\")\n                self.children = {}\n                for value in data[self.split_feat].unique():\n                    idx = data[self.split_feat] == value\n                    self.children[value] = TreeNode(node=self.name + \":\" + self.split_feat + \"==\" + str(value),\n                                                    min_sample=self.min_sample,\n                                                    default_decision=self.default_decision)\n                    self.children[value].fit(data[idx], target[idx])","a3c8f5d0":"# Test tree building\n\nfrom sklearn.model_selection import train_test_split\n\ndata = dataset.drop([\"class\"], axis=1)\ntarget = dataset[\"class\"]\n\nX_train, X_test, Y_train, Y_test = train_test_split(data, target, train_size=0.7, random_state=1)\n\n# min samples can be adjusted based on tolerance of data samples\nclf = TreeNode(min_sample=1)\nclf.fit(X_train, Y_train)","550896d0":"# view decision tree and the features it splits on\nclf.pretty_print()","c925ad4c":"# measuring accuracy and validating performance\n\ndef accuracy(data, targ):\n    count = 0\n    for (i, ct), target in zip(data.iterrows(), targ):\n        prediction = clf.predict(ct)\n        if prediction == target:\n            count += 1\n    return round(count\/len(targ), 4)\n\nprint(\"Training accuracy: \", accuracy(X_train, Y_train))\nprint(\"Validating accuracy: \", accuracy(X_test, Y_test))","92a188ed":"# view issue with prediction\n\nprint(sorted(X_train['legs'].unique()))\nprint(sorted(X_test['legs'].unique()))","73faabf1":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# used to inspect accuracy as min sample restriction increases\ntrain_sample_accuracy = []\ntest_sample_accuracy = []\nbest_param = 0\nfor i in range(0,51):\n    clf = TreeNode(min_sample=i)\n    clf.fit(X_train, Y_train)\n    train_sample_accuracy.append(accuracy(X_train, Y_train))\n    test_sample_accuracy.append(accuracy(X_test, Y_test))\n\nmerged = [train_sample_accuracy] + [test_sample_accuracy]\n#sns.lineplot(data=(merged)) - won't work in kaggle :(\nsns.lineplot(data=np.array(train_sample_accuracy), label=\"Training Set\")\nsns.lineplot(data=np.array(test_sample_accuracy), label=\"Test set\")\nplt.show()","e230ac16":"From investigating the unique values in the number of legs column, we can see that the model is tested on an animal with 5 legs which was not present in the training set and is likely why an error has been produced.","1919d80c":"Making the ID3 Decision Tree algorithm using entropy from scratch. The zoo data is from http:\/\/archive.ics.uci.edu\/ml\/datasets\/zoo.","3e380dd1":"Begin by first importing any useful or frequently used dependencies, followed by reading the zoo.csv file.","aae4e4bd":"Reusable functions are created for the calculation of entropy and information gain as noted by the ID3 algorithm.\n\nEntropy is the measure of the amount of uncertainty and information gain is the measure of the difference in entropy from before to after the data is split on an attribute\/feature. https:\/\/en.wikipedia.org\/wiki\/ID3_algorithm","3ddae109":"Unsurprisingly, the more minimum samples that are required to generate a split will lower the accuracy of the model. Although accuracy is reduced, this feature is still useful when you want to discard attributes with very little samples.","648389a1":"We can also visualise the effects of adjusting the min_sample parameter","3b7ea43c":"The animal_name column will be removed as it is simply an identifier which will not provide useful information to this implementation.","eedbc503":"# Creating Decision Tree Classifier from Scratch"}}