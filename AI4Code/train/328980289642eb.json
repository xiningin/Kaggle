{"cell_type":{"f90fb0b9":"code","ce7fad5a":"code","5696ae3d":"code","164e82d1":"code","2743352f":"code","a9b993e9":"code","c0daf773":"code","6ae43c19":"code","c2a5f38d":"code","9427d93d":"code","8be8f266":"code","21f21a7f":"code","9c34f106":"code","3a76428c":"code","695fcf3a":"code","d02496b3":"code","46d5abf9":"markdown","d1cdd482":"markdown","80639c16":"markdown","0ff2526d":"markdown","5f1e1a13":"markdown","639c8d37":"markdown","425e7889":"markdown","899fd44e":"markdown","309bd83d":"markdown","f4c74346":"markdown","b186a227":"markdown","1c96dc9f":"markdown","a6e83c4e":"markdown","8ac0a19a":"markdown","0dc0ba29":"markdown","a138cc81":"markdown","88b401e5":"markdown"},"source":{"f90fb0b9":"import pandas as pd\n\n# What do we say to python warnings? NOT TODAY\nimport warnings\nwarnings.filterwarnings('ignore')","ce7fad5a":"df_raw = pd.read_csv(\"\/kaggle\/input\/ashrae-global-thermal-comfort-database-ii\/ashrae_db2.01.csv\", low_memory=False)","5696ae3d":"df_raw.info()","164e82d1":"target = \"Thermal sensation acceptability\"","2743352f":"df = df_raw[df_raw[target].isna() == False]\ndf.info()","a9b993e9":"to_drop = [col for col in df.columns if ((\"(F)\" in col) or (\"(fpm)\" in col) or (\"Thermal\" in col) or (\"Air movement\" in col)) and (col != target)] + [\"Database\", \"Publication (Citation)\", \"Data contributor\"]\nto_drop","c0daf773":"df.drop(to_drop, axis=1, inplace=True)\ndf.info()","6ae43c19":"df.isna().sum() \/ len(df) * 100","c2a5f38d":"# Get columns with less than 25% of data missing\ns = (df.isna().sum() \/ len(df) * 100 < 25)\nto_keep = list(s[s].index)\nto_keep","9427d93d":"df = df[to_keep]\ndf.info()","8be8f266":"df.isna().sum()*100\/len(df)","21f21a7f":"from pandas_profiling import ProfileReport","9c34f106":"df.describe()","3a76428c":"profile = ProfileReport(df, title=\"ASHRAE thermal comfort data\")","695fcf3a":"profile","d02496b3":"def select_features(df, target, th):\n    \"\"\"\n    Select features.\n    \"\"\"\n    # Select rows with our target value\n    proc_df = df[df[target].isna() == False]\n    \n    # Remove useless columns\n    to_drop = [col for col in proc_df.columns if ((\"(F)\" in col) or (\"(fpm)\" in col) or (\"Thermal\" in col) or (\"Air movement\" in col)) and (col != target)] + [\"Database\", \"Publication (Citation)\", \"Data contributor\"]\n    proc_df = proc_df.drop(to_drop, axis=1)\n    \n    # Remove columns with a lot of missing values\n    # Get columns with less than 30% of data missing\n    s = (proc_df.isna().sum() \/ len(proc_df) * 100 < th)\n    to_keep = list(s[s].index)\n    proc_df = proc_df[to_keep]\n    \n    return proc_df","46d5abf9":"# Notebook goal\nThe goal of this notebook is to select the target and the features to be used in a predictive model (to be used in the following notebooks). Here will be shown only one of the many possibilities in machine learning, feel free to experiment and play around with the data on your own. We will be focusing on the techniques and not on the model performance.","d1cdd482":"First, we can checkout the summary metrics with the method `describe`:","80639c16":"# (Optional) One function to do it all\nThis part is completely optional. The only purpose of this function is to be used in the following notebooks.","0ff2526d":"And we keep only the rows that has our target value, it makes no sense to have missing values in the feature we are predicting.","5f1e1a13":"And now we create a profile with `pandas_profiling`. This will create an interactive report that you can browse. It takes some time to render it.","639c8d37":"That reduced our data set a lot, but we are going to make a good use of these rows ;)","425e7889":"# Select features\nNext, we are going to drop some columns: all the target candidates that were not chosen and all the temperature columns in Farenheit (we are going to use Celsius). In addition, columns related to publication and data contributor will also be dropped. We are using a quite long list comprehension to create this list but it is really simple, check it out:","899fd44e":"Seems like there a lot of columns with a lor of missing values. We are not going to write them down, we can select them with a few lines of code:","309bd83d":"# Introduction\n\nThis is a serie of notebooks thar should be visited in order, they are all linked in the table of content. In this notebook we are going to load the data and transform it to be used in a predictive model in the following notebooks.\n\n### Content table\n- **Preprocessing pt. 1: data transformation & EDA** (you are here)\n    - [Load data](#Load-data)\n    - [Select target](#Select-target)\n    - [Select features](#Select-features)\n    - [Exploration](#Exploration)\n- [Preprocessing pt. 2: encoding categorical variables](https:\/\/www.kaggle.com\/ponybiam\/classification-preprocessing-pt-2)\n- [Preprocessing pt. 3: handling numerical features](https:\/\/www.kaggle.com\/ponybiam\/classification-preprocessing-pt-3)  \n- [Feature engineering pt. 1: simple features](https:\/\/www.kaggle.com\/ponybiam\/classification-feature-engineering-pt-1)\n- [Feature engineering pt. 2: clustering & PCA](https:\/\/www.kaggle.com\/ponybiam\/classification-feature-engineering-pt-2)\n- [Feature engineering pt. 3: target encoding](https:\/\/www.kaggle.com\/ponybiam\/classification-feature-engineering-pt-3)","f4c74346":"# Select target\nThe are several candidates that could be used as target in a classification model:\n\n- **Thermal sensation**: ASHRAE thermal sensation vote, from -3 (cold) to +3 (hot)\n- **Thermal sensation acceptability**: unnaceptable (0) or acceptable (1)\n- **Thermal preference**: cooler, no changes or warmer\n- **Air movement acceptability**: unnaceptable (0) or acceptable (1)\n- **Air movement preference**: less, no changes or more\n- **Thermal comfort**: from 1 (very un comfortable) to 6 (very comfortable)\n\nThese 6 features are related, we are going to predict `thermal sensation acceptability` and exclude the others. This feature is a *binary class target*, meaning that it only takes two values: `True` (or 1, or positive) and `False` (or 0 or negative). A binary classification makes the metrics more intuitive so we are going on with these one to create a clear example on a classification model.","b186a227":"Now that we have the columns we want, what about the missing values? If a column is mostly missing values maybe using it for a predictive model is not the best idea. We are going to set a threshold of 25% of missing; if percentage of missing is bigger, we drop it. This threshold is a personal criteria, another value could be used and it would be fine too.","1c96dc9f":"# Load data\nWe are going to load the whole dataset and in the following sections select the target and some features. As usual, we are using [Pandas](https:\/\/pandas.pydata.org\/) to work with DataFrames.","a6e83c4e":"And we drop the columns we selected in the list `to_drop`:","8ac0a19a":"First thing we can notice is that we have some redundant columns: the same features in celsius and farenheits. There ir also information about the data source and several features that could be used as target, depending on the goal of your model.","0dc0ba29":"# Exploration\nFor the data exploration we are going to use [*Pandas profiling*](https:\/\/pandas-profiling.github.io\/pandas-profiling\/docs\/master\/rtd\/), a really usefull tool that automate a basic data exploration. You can always plot and inspect the data manually, this library is just a tool to do it quickly and simple. You don't always have to use it but is good to know it and keep it in mind.","a138cc81":"Notice that here we are selecting the columnt **to keep** (the oposite as before). So we are not droping thme, we are selecting them:","88b401e5":"Let's inspect the features, datatypes and null values:"}}