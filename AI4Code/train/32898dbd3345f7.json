{"cell_type":{"bdb87d43":"code","bc5e6903":"code","ce07c914":"code","59bdc77c":"code","1c8e08bc":"code","b5a41f33":"code","c1f59ee6":"code","134c6e9f":"code","949a32e3":"code","5d097cea":"code","3db28a4c":"code","17cf0bbf":"code","3be79483":"code","14a2babc":"code","72049562":"code","18c8dfb3":"code","ea523b66":"code","06ecbcc8":"code","5085867c":"code","c1453183":"code","ea7ea77a":"code","de3c3d97":"code","2080cd8d":"code","e87f37fc":"code","4310cf0f":"code","53ee910e":"code","ad8b3148":"code","3af48c1f":"code","15e55721":"code","581848b5":"code","d2c56ec5":"code","0821d0f0":"code","b0b2d6e9":"code","e6bb4fed":"code","21612b95":"code","495b0f47":"code","8b6c4b07":"code","ca9eceb1":"code","57dc884b":"code","295e8e87":"code","fbd37c03":"code","55a12e2c":"code","c7e8bb44":"code","626ba237":"code","90dc6108":"code","1cfd5c44":"code","4473de7a":"code","14702535":"code","facb809f":"code","b096f487":"code","88a2fa03":"code","c4c9bef6":"code","bea97ded":"code","c6d739cc":"code","1748fa2a":"code","8865b56e":"code","cf0be882":"markdown","11d2461a":"markdown","d8ae96a3":"markdown","78bce00e":"markdown","c0fb3d36":"markdown","95e91575":"markdown","fbb6e386":"markdown","dd0f925b":"markdown","f05e87ee":"markdown","c8987b27":"markdown","2b44ed49":"markdown","3e316328":"markdown","e09caf00":"markdown","0e9bd5f1":"markdown","4d5de727":"markdown","ae2c6471":"markdown","1756b85a":"markdown","523a1b3f":"markdown","f5ad327b":"markdown"},"source":{"bdb87d43":"import re\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy.sparse import hstack\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nfrom nltk.corpus import stopwords\nfrom sklearn.svm import LinearSVC\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn import metrics, svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn import utils\nfrom sklearn.svm import SVR\n#from sklearn import svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer \nfrom sklearn.feature_extraction.text import TfidfVectorizer\n","bc5e6903":"pd.options.mode.chained_assignment = None \ndf = pd.read_csv(\"..\/input\/winemag-data_first150k.csv\", nrows=50000,index_col=0)\ndf.head()","ce07c914":"#drop columns not needed\ndf = df.drop(['designation','province','region_1','region_2','winery'], axis = 1)\n\n#We will now test for duplictes in the dataset to ensure that we are using unique reviews\ndf[df.duplicated('description',keep=False)].head()\n\n#We will now remove the dulicates based on the descripton column \ndf = df.drop_duplicates('description')\ndf.head()","59bdc77c":"#Return a sum count of rows with missing data\ndf.apply(lambda x: sum(x.isnull()),axis=0) ","1c8e08bc":"#fill in missing price with mean values\ndf['price'].fillna(df['price'].mean(), inplace=True)\n\n#Return a sum count of rows with missing data\ndf.apply(lambda x: sum(x.isnull()),axis=0) ","b5a41f33":" #Drop rows with missing\/invalid data\ndf.dropna(axis='rows',inplace=True)\n\n#Return a sum count of rows with missing data\ndf.apply(lambda x: sum(x.isnull()),axis=0) ","c1f59ee6":"df['description'][0]","134c6e9f":"#Get rid of the less useful parts like symbols and digits\ndescription =  re.sub('[^a-zA-Z]',' ',df['description'][0])\ndescription","949a32e3":"#All the words should be in same case so lowercase the words and remove trailing whitespaces\ndescription = description.lower().strip()\ndescription","5d097cea":"#convert string to a list of words\ndescription_words = description.split() \n\n#iterate over each word and include it if it is not stopword \ndescription_words = [word for word in description_words if not word in stopwords.words('english')]\ndescription_words","3db28a4c":"ps = PorterStemmer()\ndescription_words=[ps.stem(word) for word in description_words]\ndescription_words","17cf0bbf":"#Now the description is clean the cleaned list of words can be converted to string and pushed to the dataset\ndf['description'][0]=' '.join(description_words)\ndf['description'][0]","3be79483":"#Now to clean other rows too one can iterate over all rows of the dataset and clean each\nstopword_list = stopwords.words('english')\nps = PorterStemmer()\nfor i in range(1,len(df['description'])):\n    try:\n        description = re.sub('[^a-zA-Z]',' ',df['description'][i])\n        description = description.lower().strip()\n        description_words = description.split()\n        description_words = [word for word in description_words if not word in stopword_list]\n        description_words = [ps.stem(word) for word in description_words]\n        df['description'][i] = ' '.join(description_words)\n    except:\n        pass","14a2babc":"#Displaying all the descriptions after cleaning\nfor i in range(len(df['description'])):\n    try:\n        print(str(i+1)+\".\",df['description'][i],\"\\n\")\n    except:\n        pass","72049562":"#We will test for a correlation between the price of wine and its rating\nprint(\"Pearson Correlation:\", pearsonr(df.price, df.points))\nprint(sm.OLS(df.points, df.price).fit().summary())\nsns.lmplot(y = 'price', x='points', data=df)","18c8dfb3":"fig, ax = plt.subplots(figsize = (20,7))\nchart = sns.boxplot(x='country',y='points', data=df, ax = ax)\nplt.xticks(rotation = 90)\nplt.show()","ea523b66":"df.country.value_counts()","06ecbcc8":"country=df.groupby('country').filter(lambda x: len(x) >100)\ndf1 = pd.DataFrame({col:vals['points'] for col,vals in country.groupby('country')})\nmeds = df1.median()\nmeds.sort_values(ascending=False, inplace=True)\n\nfig, ax = plt.subplots(figsize = (20,7))\nchart = sns.boxplot(x='country',y='points', data=country, order=meds.index, ax = ax)\nplt.xticks(rotation = 90)\n\nplt.show()","5085867c":"df2 = pd.DataFrame({col:vals['price'] for col,vals in country.groupby('country')})\nmeds2 = df2.median()\nmeds2 = meds2.sort_values(ascending=False)\n\nplt.rcParams['figure.figsize']=15,8 \nmeds2.plot(\"bar\")\nplt.title('Bar Chart Showing Median Wine Prices from Highest to Lowest')\nplt.xlabel('Country')\nplt.ylabel('Median Wine Price')\nplt.show()","c1453183":"#Medians for the above Barplot\nprint(meds2)","ea7ea77a":"df = df.groupby('variety').filter(lambda x: len(x) >100)\nlist = df.variety.value_counts().index.tolist()\nfig4, ax4 = plt.subplots(figsize = (20,7))\nsns.countplot(x='variety', data=df, order = list, ax=ax4)\nplt.xticks(rotation = 90)\nplt.show()","de3c3d97":"df = df.groupby('variety').filter(lambda x: len(x) >200)\n\ndf3 = pd.DataFrame({col:vals['points'] for col,vals in df.groupby('variety')})\nmeds3 = df3.median()\nmeds3.sort_values(ascending=False, inplace=True)\n\nfig3, ax3 = plt.subplots(figsize = (20,7))\nchart = sns.boxplot(x='variety',y='points', data=df, order=meds3.index, ax = ax3)\nplt.xticks(rotation = 90)\nplt.show()","2080cd8d":"df4 = pd.DataFrame({col:vals['points'] for col,vals in df.groupby('variety')})\nmean1 = df4.mean()\nmean1 = mean1.sort_values(ascending=False)\n\nplt.rcParams['figure.figsize']=15,8 \nmean1.plot(\"bar\")\nplt.title('Bar Chart Showing Median Wine Prices from Highest to Lowest')\nplt.xlabel('Variety')\nplt.ylabel('Median Wine Price')\nplt.show()","e87f37fc":"#Mean for the above Barplot\nprint(mean1)","4310cf0f":"#We will now test the variations in price\ndf5 = pd.DataFrame({col:vals['price'] for col,vals in df.groupby('variety')})\nmean2 = df5.mean()\nmean2.sort_values(ascending=False, inplace=True)\n\nfig3, ax3 = plt.subplots(figsize = (20,7))\nchart = sns.barplot(x='variety',y='price', data=df, order=mean2.index, ax = ax3)\nplt.xticks(rotation = 90)\nplt.show()","53ee910e":"X = df.drop(['country','points', 'variety'], axis = 1)\ny = df.variety\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","ad8b3148":"wine =df.variety.unique().tolist()\nwine.sort()\nwine","3af48c1f":"#Split wine varieties with space and make new list\noutput = set()\nfor x in df.variety:\n    x = x.lower()\n    x = x.split()\n    for y in x:\n        output.add(y)\n\nvariety_list =sorted(output)\nvariety_list","15e55721":"extras = ['',' ',\"\",\" \",'.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', 'cab',\"%\"]\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\nstop.update(variety_list)\nstop.update(extras)","581848b5":"vect = CountVectorizer(stop_words = stop)\nX_train_dtm = vect.fit_transform(X_train.description)\nprice = X_train.price.values[:,None]\nX_train_dtm = hstack((X_train_dtm, price))\n\nX_test_dtm = vect.transform(X_test.description)\nprice_test = X_test.price.values[:,None]\nX_test_dtm = hstack((X_test_dtm, price_test))\n# X_test_dtm","d2c56ec5":"models = {}\nfor z in wine:\n    model = LogisticRegression()\n    y = y_train == z\n    model.fit(X_train_dtm, y)\n    models[z] = model\n\ntesting_probs = pd.DataFrame(columns = wine)","0821d0f0":"for variety in wine:\n    testing_probs[variety] = models[variety].predict_proba(X_test_dtm)[:,1]\n    \npredicted_wine = testing_probs.idxmax(axis=1)\n\ncomparison = pd.DataFrame({'actual':y_test.values, 'predicted':predicted_wine.values})   \n# comparison = pd.DataFrame({'actual':'Malbec', 'predicted':predicted_wine.values})   \n\nprint('Accuracy Score:',accuracy_score(comparison.actual, comparison.predicted)*100,\"%\")\ncomparison","b0b2d6e9":"filtered = df.groupby('variety').filter(lambda x: len(x) >= 500) #taking only the highest occuring to reduce size and keeping distribution in mind.","e6bb4fed":"#Making a new column that is encoded version of variety\nfiltered['variety_id'] = filtered['variety'].factorize()[0]\ncategory_id_df = filtered[['variety', 'variety_id']].drop_duplicates().sort_values('variety_id')\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['variety_id', 'variety']].values)\n\nfiltered.head()","21612b95":"tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='UTF-8', ngram_range=(1, 2), stop_words='english') \n\nfeatures = tfidf.fit_transform(filtered.description).toarray() #Removing Stop words from descriptions \nlabels = filtered.variety_id #Varity Numberical values saved as labels\n\nmodel = LinearSVC()\nX_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, filtered.index, test_size=0.30, random_state=0) #70\/30 Split\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\nconf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=category_id_df.variety.values, yticklabels=category_id_df.variety.values)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","495b0f47":"X = df.drop('country', axis=1)  \ny = df['points']\n","8b6c4b07":"X = category_id_df.drop('variety_id', axis=1)  \ny = category_id_df['variety_id'] \ncategory_id_df.head()","ca9eceb1":"#Droping the duplicates\ndf[df.duplicated('description',keep=False)].sort_values('description').head(5)\n","57dc884b":"df.head()","295e8e87":"#Dropping all duplicated based and description and missing prices\n\ndf = df.drop_duplicates('description')\ndf = df[pd.notnull(df.price)]\ndf.shape","fbd37c03":"from scipy.stats import pearsonr\nimport statsmodels.api as sm\nprint(\"Pearson Correlation:\", pearsonr(df.price, df.points))\nprint(sm.OLS(df.points, df.price).fit().summary())\nsns.lmplot(y = 'price', x='points', data=df)","55a12e2c":"fig, ax = plt.subplots(figsize = (10,7))\nchart = sns.boxplot(x='country',y='points', data=df, ax = ax)\nplt.xticks(rotation = 90)\nplt.show()","c7e8bb44":"df.country.value_counts()[:]","626ba237":"df6 = pd.DataFrame({col:vals['price'] for col,vals in country.groupby('country')})\nmeds2 = df6.median()\nmeds2.sort_values(ascending=False, inplace=True)\n\nfig, ax = plt.subplots(figsize = (20,5))\nchart = sns.barplot(x='country',y='price', data=country, order=meds2.index, ax = ax)\nplt.xticks(rotation = 90)\nplt.show()","90dc6108":"df = df.reset_index()","1cfd5c44":"X = df.drop(['country','description','variety'], axis = 1)\ny = df.price\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","4473de7a":"X = X.as_matrix().astype(np.float)\ny = y.as_matrix().astype(np.float)","14702535":"df.apply(lambda x: sum(x.isnull()),axis=0)","facb809f":"classifier = svm.SVR(kernel='linear') # We set a SVM classifier, the default SVM Classifier (Kernel = Radial Basis Function)","b096f487":"lab_enc = preprocessing.LabelEncoder()\ntraining_scores_encoded = lab_enc.fit_transform(y_train)\nprint(training_scores_encoded)\nprint(utils.multiclass.type_of_target(y_train))\nprint(utils.multiclass.type_of_target(y_train.astype('int')))\nprint(utils.multiclass.type_of_target(training_scores_encoded))","88a2fa03":"classifier.fit(X_train, y_train) # Then we train our model, with our balanced data train.","c4c9bef6":"from sklearn.model_selection import train_test_split  \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.60) ","bea97ded":"from sklearn.svm import SVR  \nsvclassifier = SVR(kernel='linear')  \nsvclassifier.fit(X_train, y_train) ","c6d739cc":"y_pred = svclassifier.predict(X_test) ","1748fa2a":"from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\nclf = RandomForestRegressor(n_estimators=10)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint('Accuracy Score:',clf.score(X_test, y_test) *100,\"%\")","8865b56e":"y_test\n","cf0be882":"**There's clear variation in price which may help in predicting the wine type.**","11d2461a":"## Support Vector Machine Algorithm","d8ae96a3":"**We will now remove the countries that have less than 100 observations**","78bce00e":"## Data Cleaning\n* Dropping unwanted columns\n* Removing Duplicate records\n* Removing Duplicate Descriptions\n* Filling in missing values \n* Dropping rows with null data","c0fb3d36":"**Above is a countplot chart containing all wine varieties with more than 200 observations and their respective point distributions. Sangiovese Grosso appears to have the highest median score of all wines. **","95e91575":"**There seems to be the label name in the description**\nIf you read the descriptions, the reviewers often times say \"Cabertnet\", \"Pinot\", \"Red\", etc. in the review itself, and these words need to be taken off if so as to create a model that doesn't rely on the probability that a word in the description that matches the label. I however included tokenized versions of the feature labels as parts of the stopwords used in analysis.","fbb6e386":"**We will now find the average wine price by using the median from highest to lowest to test for any price distortions due to outliers**","dd0f925b":"### Stemming words\n\nStemming reduce each word to its root form in order to remove the differences between inflected forms of a word. Example: \"running\", \"runs\", \"runned\" become \"run\"","f05e87ee":"**We will now find sort the prices from highest to lowest to identify distortion**","c8987b27":"**We will be using the wine price and description**","2b44ed49":"**We can also see that there are odd plots as a result of low sample size per country**","3e316328":"**We can see here that the accuracy of the model is 58.97%. There is room for improvement in this case where if other featres were included then maybe the accuracy would go up **","e09caf00":"**We will now test the accuracy of the model**","0e9bd5f1":"## Cleaning Description\n* Remove symbols and digits\n* Change all words to lowercase and remove trailing whitespaces\n* Remove stop words\n* Stemming words","4d5de727":"**We can see that for each point increase in rating the price goes up by an average of  $1.04. However there seems to be some odd points in the plot which may be due to low sample size of some countries****","ae2c6471":"## LOGISTIC REGRESSION","1756b85a":"**There's a large variety of wines in the dataset. However, there's an exponential decline in the number of observations for each wine type so we'll be dropping any wine types with less than 200 observations, for the reason that there's not enough data in these buckets to generate an accuarte model for predicting their respective wine type**","523a1b3f":"### Drop the stopwords\n\nThe next step is to to remove the **stop words**. Stop words are irrelevant as they occur frequently in the data example 'a', 'the','is','in' etc. In order to save both space and time, these words are dropped .","f5ad327b":"**We can see that there a significant correlation between the cost and ratings of the wine of an average of  $1.04 for each point increase**"}}