{"cell_type":{"b7a0a0fd":"code","a29efd52":"code","96ee9e0f":"code","2e538489":"code","2a7cd481":"code","df5f329e":"code","3def7918":"code","2afac939":"code","fc997a72":"code","2ce1a3a4":"code","9b4037b9":"code","ee8223a4":"code","1d79c9f1":"code","70385221":"code","1199ebfe":"code","ed7d46ba":"code","68537b81":"code","5a81b5c1":"markdown","01a009ae":"markdown","6c8019a0":"markdown","998edd18":"markdown","aa9588fe":"markdown","c6bcc1ee":"markdown","34f1fc71":"markdown","1cb6c41a":"markdown","7c2643e3":"markdown","05eccfd1":"markdown","674eeca1":"markdown","d4afea3b":"markdown","b7cb9eb2":"markdown"},"source":{"b7a0a0fd":"!pip install -qq torchflare","a29efd52":"!pip install -qq nb_black","96ee9e0f":"%load_ext lab_black","2e538489":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport pandas as pd\n\nimport torchflare.callbacks as cbs\nimport torchflare.metrics as metrics\nfrom torchflare.experiments import Experiment\nfrom torchflare.datasets import TextDataloader\n\nimport transformers\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","2a7cd481":"tokenizer = transformers.BertTokenizer.from_pretrained(\n    \"bert-base-uncased\", do_lower_case=True\n)","df5f329e":"train_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")","3def7918":"train_df.head()","2afac939":"train_df, valid_df = train_test_split(train_df, test_size=0.3, random_state=42)","fc997a72":"train_dl = TextDataloader.from_df(\n    df=train_df,\n    tokenizer=tokenizer,\n    max_len=185,\n    input_col=\"excerpt\",\n    label_cols=\"target\",\n).get_loader(batch_size=32, shuffle=True)\n\nvalid_dl = TextDataloader.from_df(\n    df=valid_df,\n    tokenizer=tokenizer,\n    max_len=185,\n    input_col=\"excerpt\",\n    label_cols=\"target\",\n).get_loader(batch_size=64, shuffle=False)","2ce1a3a4":"class BERTClass(nn.Module):\n    def __init__(self):\n        super(BERTClass, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n        self.fc = nn.Linear(768, 1)\n\n    def forward(self, input_ids, token_type_ids, attention_mask):\n        _, output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            return_dict=False,\n        )\n        output = self.fc(output)\n        return output","9b4037b9":"model = BERTClass()","ee8223a4":"def rmse(op, y):\n    return torch.sqrt(nn.MSELoss()(op, y.float()))","1d79c9f1":"param_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\"]\noptimizer_parameters = [\n    {\n        \"params\": [\n            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n        ],\n        \"weight_decay\": 0.001,\n    },\n    {\n        \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n        \"weight_decay\": 0.0,\n    },\n]\ncallbacks = [\n    cbs.ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_dir=\".\/\"),\n    cbs.ReduceLROnPlateau(mode=\"min\", patience=2),\n]","70385221":"exp = Experiment(num_epochs=4, fp16=False, device=\"cuda\", seed=42)\n\n# Compiling the experiment\nexp.compile_experiment(\n    model=model,\n    optimizer=\"AdamW\",\n    optimizer_params=dict(\n        model_params=optimizer_parameters, lr=3e-4\n    ),  # used model_params argument for custom optimizer params.\n    callbacks=callbacks,\n    criterion=rmse,\n    metrics=None,\n    main_metric=\"loss\",\n)","1199ebfe":"exp.fit_loader(train_dl, valid_dl)","ed7d46ba":"exp.history","68537b81":"print(f\"Mean VAL RMSE  : {sum(exp.history['train_loss'])\/len(exp.history['Epoch'])}\")","5a81b5c1":"### ***Again this goes without saying, If you like this workflow and want to see more of it please star [TorchFlare](https:\/\/github.com\/Atharva-Phatak\/torchflare).***","01a009ae":"* ***Defining BERT Model.***","6c8019a0":"* ***Defining Callbacks and metrics and some optimizer params.***","998edd18":"#### ***Downloading our tokenizer.***","aa9588fe":"# ***Introducing TorchFlare***\n\n![logo](https:\/\/raw.githubusercontent.com\/Atharva-Phatak\/torchflare\/main\/docs\/static\/images\/TorchFlare.gif)\n\n\n***TorchFlare*** is a simple, beginner-friendly and an easy-to-use PyTorch Framework train your models without much effort.\nIt provides an almost Keras-like experience for training\nyour models with all the callbacks, metrics, etc\n\n\n### ***Features***\n* _A high-level module for Keras-like training._\n* _Off-the-shelf Pytorch style Datasets\/Dataloaders for standard tasks such as **Image classification, Image segmentation,\n  Text Classification**, etc_\n* _**Callbacks** for model checkpoints, early stopping, and much more!_\n* _**Metrics** and much more._\n* _**Reduction** of the boiler plate code required for training your models._\n\n***\n* **Github**: https:\/\/github.com\/Atharva-Phatak\/torchflare\n* **Docs**: https:\/\/atharva-phatak.github.io\/torchflare\/\n\n**Show some love to [TorchFlare](https:\/\/github.com\/Atharva-Phatak\/torchflare) by giving start if you like the library.**\n\n**If you find bug or have feature requests or create awesome kernels\/notebooks using TorchFlare, open up a issue in repo.**","c6bcc1ee":"* Note you wont be possibly be able to plot history this since we upgraded the Pillow version and Kaggle kernel has pip resolve issues(matplotlib compat issues). I will try update the requirements for torchflare to make it compatible with Kaggle Kernels. \n","34f1fc71":"* ***Using TextDataloader functionality from TorchFlarefor easy creation of text dataloaders.***","1cb6c41a":"#### ***Using simple train_test_split for data splitting.***\n* Will add another kernel showing how to do crossval with torchflare.","7c2643e3":"#### Let's take a experiment History. ","05eccfd1":"* ***Defining the loss function.***","674eeca1":"* For some reason installation takes some time, I guess because torchflare is based on latest versions. ","d4afea3b":"#### ***Importing Libraries***","b7cb9eb2":"#### ***Defining experiment in TorchFlare is easy***\n1. First define some constant params like epochs , seeds, etc\n2. Compile your experiment with model, optimizer, callbacks, etc\n3. Fit your experiment on traininig and validation loaders."}}