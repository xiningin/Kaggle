{"cell_type":{"c52ea18b":"code","340c5968":"code","63ca1c67":"code","fc1bea58":"code","86af99c3":"code","326a91af":"code","fd2069c2":"code","f598f019":"code","fa499b2c":"code","c22ce59e":"code","40f814c3":"code","d62bd01b":"code","edf50fdc":"code","9a78e691":"code","7c437975":"code","9aba9172":"code","dd216f72":"code","08e74ef4":"code","02c99ad2":"code","88bd5bda":"code","40fa6466":"code","58dc3666":"code","e010d08c":"code","803e6c6a":"code","cb56d4e9":"code","bb56859f":"code","9c2511f2":"code","f79da00a":"code","403fe1c6":"code","6da75e04":"code","2e3d34f2":"code","353ca948":"code","c5b0a10f":"code","d2b3845e":"code","c83314d4":"code","9e2ee81b":"code","001f220d":"code","2ca7add1":"code","752bd1c9":"code","5af19cdc":"markdown","daf14f33":"markdown","517c3d99":"markdown","efd1a870":"markdown","e6fd4133":"markdown","09dcfff7":"markdown","04717041":"markdown","558e134e":"markdown","6d6d67ad":"markdown","a7f6d18c":"markdown","9d80f0ee":"markdown","a0eac74d":"markdown","87f405aa":"markdown","57b3143a":"markdown","22be26bb":"markdown","4f61b0d0":"markdown","fc7037fb":"markdown","b2dc852e":"markdown","1a7c3cf7":"markdown"},"source":{"c52ea18b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","340c5968":"# Import data using Pandas\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# Let's look at training data\ntrain.head(5)","63ca1c67":"# Check datatypes\ntrain.dtypes","fc1bea58":"train.describe(include=\"all\").T","86af99c3":"# Count for missing value in training data\nprint(pd.isnull(train).sum())","326a91af":"# Barplot of survivors by sex\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train)\n\n# Print percentages of females vs. males that survive\nprint(\"Percentage of females who survived:\", train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of males who survived:\", train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)","fd2069c2":"# Pclass = Passenger class\n# Barplot of survivors by passenger class\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train)\n\n# Print percentage of survivors by Pclass\nprint(\"Percentage of Pclass = 1 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Pclass = 2 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Pclass = 3 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)","f598f019":"# People with relatives (parents and children)\nsns.barplot(x=\"Parch\", y=\"Survived\", data=train)\n\n# Print percentage of survivors by Parch\nprint(\"Percentage of Parch = 1 who survived:\", train[\"Survived\"][train[\"Parch\"] == 1].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Parch = 2 who survived:\", train[\"Survived\"][train[\"Parch\"] == 2].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Parch = 3 who survived:\", train[\"Survived\"][train[\"Parch\"] == 3].value_counts(normalize = True)[1]*100)\n# print(\"Percentage of Parch = 4 who survived:\", train[\"Survived\"][train[\"Parch\"] == 4].value_counts(normalize = True)[1]*100) # <-- No Parch = 4\nprint(\"Percentage of Parch = 5 who survived:\", train[\"Survived\"][train[\"Parch\"] == 5].value_counts(normalize = True)[1]*100)","fa499b2c":"# Age feature\n# Fill missing values with -0.5\ntrain[\"Age\"] = train[\"Age\"].fillna(-0.5)\ntest[\"Age\"] = test[\"Age\"].fillna(-0.5)\n\n# Create bins for age ranges separation\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\n\n# Label of age ranges\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\n\n# Assign labels to bins\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = labels)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = labels)\n\n# Plot barplot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train)","c22ce59e":"# Cabin feature\n\ntrain[\"CabinBool\"] = (train[\"Cabin\"].notnull().astype('int'))\ntest[\"CabinBool\"] = (test[\"Cabin\"].notnull().astype('int'))\n\n# Calculate percentages of CabinBool vs. survived\nprint(\"Percentage of CabinBool = 1 who survived:\", train[\"Survived\"][train[\"CabinBool\"] == 1].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of CabinBool = 0 who survived:\", train[\"Survived\"][train[\"CabinBool\"] == 0].value_counts(normalize = True)[1]*100)\n\n# Plot barplot of CabinBool vs. survival\nsns.barplot(x=\"CabinBool\", y=\"Survived\", data=train)\nplt.show()","40f814c3":"# Look at test data\ntest.describe(include=\"all\").T","d62bd01b":"# Cabin feature is not very useful --> We will drop it\ntrain = train.drop(['Cabin'], axis = 1)\ntest = test.drop(['Cabin'], axis = 1)","edf50fdc":"# We can drop Ticket feature too\ntrain = train.drop(['Ticket'], axis = 1)\ntest = test.drop(['Ticket'], axis = 1)","9a78e691":"# Embarked feature\n# print(train['Embarked'].unique) # S, C, Q\n\nprint(\"Number of people embarking in Southampton (S):\")\nsouthampton = train[train[\"Embarked\"] == \"S\"].shape[0]\nprint(southampton)\n\nprint(\"Number of people embarking in Cherbourg (C):\")\ncherbourg = train[train[\"Embarked\"] == \"C\"].shape[0]\nprint(cherbourg)\n\nprint(\"Number of people embarking in Queenstown (Q):\")\nqueenstown = train[train[\"Embarked\"] == \"Q\"].shape[0]\nprint(queenstown)","7c437975":"# Replacing the missing values in the Embarked feature with S\ntrain = train.fillna({\"Embarked\": \"S\"})","9aba9172":"# Age feature\n# Create a combined group of both datasets\ncombine = [train, test]\n\n# Extract a title for each Name in the train and test datasets\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])","dd216f72":"# Replace various titles with more common title\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Capt', 'Col',\n    'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n    \n    dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","08e74ef4":"# Map each of the title groups to a numerical value\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 6}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain.head()","02c99ad2":"# Try to predict the missing Age values from the most common age for their Title.\n\n# fill missing age with mode age group for each title\nmr_age = train[train[\"Title\"] == 1][\"AgeGroup\"].mode() #Young Adult\nmiss_age = train[train[\"Title\"] == 2][\"AgeGroup\"].mode() #Student\nmrs_age = train[train[\"Title\"] == 3][\"AgeGroup\"].mode() #Adult\nmaster_age = train[train[\"Title\"] == 4][\"AgeGroup\"].mode() #Baby\nroyal_age = train[train[\"Title\"] == 5][\"AgeGroup\"].mode() #Adult\nrare_age = train[train[\"Title\"] == 6][\"AgeGroup\"].mode() #Adult\n\nage_title_mapping = {1: \"Young Adult\", 2: \"Student\", 3: \"Adult\", 4: \"Baby\", 5: \"Adult\", 6: \"Adult\"}\n\n# train = train.fillna({\"Age\": train[\"Title\"].map(age_title_mapping)})\n# test = test.fillna({\"Age\": test[\"Title\"].map(age_title_mapping)})\n\nfor x in range(len(train[\"AgeGroup\"])):\n    if train[\"AgeGroup\"][x] == \"Unknown\":\n        train[\"AgeGroup\"][x] = age_title_mapping[train[\"Title\"][x]]\n        \nfor x in range(len(test[\"AgeGroup\"])):\n    if test[\"AgeGroup\"][x] == \"Unknown\":\n        test[\"AgeGroup\"][x] = age_title_mapping[test[\"Title\"][x]]","88bd5bda":"# Map each Age value to a numerical value\nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ntrain['AgeGroup'] = train['AgeGroup'].map(age_mapping)\ntest['AgeGroup'] = test['AgeGroup'].map(age_mapping)\n\ntrain.head(10)\n\n# Dropping the Age feature for now, might change\n# train = train.drop(['Age'], axis = 1)\n# test = test.drop(['Age'], axis = 1)","40fa6466":"# Drop the name feature since it contains no more useful information.\ntrain = train.drop(['Name'], axis = 1)\ntest = test.drop(['Name'], axis = 1)","58dc3666":"# Map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(sex_mapping)\ntest['Sex'] = test['Sex'].map(sex_mapping)\n\ntrain.head()","e010d08c":"# Map each Embarked value to a numerical value\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)\n\ntrain.head()","803e6c6a":"# Dropping the Age feature for now, might change\ntrain = train.drop(['Age'], axis = 1)\ntest = test.drop(['Age'], axis = 1)","cb56d4e9":"# Fare feature --> Separate into logical groups\n# Fill in missing Fare value in test set based on mean fare for that Pclass \nfor x in range(len(test[\"Fare\"])):\n    if pd.isnull(test[\"Fare\"][x]):\n        pclass = test[\"Pclass\"][x] #Pclass = 3\n        test[\"Fare\"][x] = round(train[train[\"Pclass\"] == pclass][\"Fare\"].mean(), 4) # Round from pclass = 3's fare\n        \n# Map Fare values into groups of numerical values\n\n\"\"\"pd.qcut = Discretize variable into equal-sized buckets based on rank or based on sample quantiles. \nFor example 1000 values for 10 quantiles would produce a Categorical object indicating quantile membership for each data point.\"\"\"\n\ntrain['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\ntest['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])\n\n# Drop Fare values (we use FareBand instead)\ntrain = train.drop(['Fare'], axis = 1)\ntest = test.drop(['Fare'], axis = 1)\n\n# Check train data\ntrain.head()","bb56859f":"# Check test data\ntest.head()","9c2511f2":"# Train and validate value split\nfrom sklearn.model_selection import train_test_split\n\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.22, random_state = 0)","f79da00a":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)","403fe1c6":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","6da75e04":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","2e3d34f2":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)","353ca948":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_perceptron)","c5b0a10f":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","d2b3845e":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","c83314d4":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","9e2ee81b":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","001f220d":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","2ca7add1":"# Let's compare the accuracies of each model!\n\nmodels = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)","752bd1c9":"#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = gbk.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","5af19cdc":"* ## People with recorded cabin number are more likely to survive\n* ## \"people with recorded cabin numbers are of higher socioeconomic class, and thus more likely to survive\"","daf14f33":"## 7. Submission file creation","517c3d99":"## Summary\n- We have 891 passengers\n- Age count = 714 --> 19.8% missing (must fix)\n- Cabin count = 204 --> 76% missing (must fix!!!)\n- embarked = 889 --> 0.22% missing (can be neglected)","efd1a870":"## 6. Test different models and choose the best one\nSplitting the Training Data\n* We will use part of our training data (22% in this case) to test the accuracy of our different models.","e6fd4133":"## Summary of test data\n- 418 passenger\n- fare count 417 --> 0.24% missing (1 value is missing)\n- cabin count 91 --> 78.23% misssing","09dcfff7":"## 3. Data analysis","04717041":"* ## People with < 4 relatives are likely to survive than people with >= 4 relative\n* ## But people with no relatives are less likely to survive compared to people with 1-3 relatives","558e134e":"## 2. Import data","6d6d67ad":"## First-class is more likely to survive than second, and third class","a7f6d18c":"## 5. Data cleaning","9d80f0ee":"## Testing different models\n* Gaussian Naive Bayes\n* Logistic Regression\n* Support Vector Machines\n* Perceptron\n* Decision Tree Classifier\n* Random Forest Classifier\n* KNN or k-Nearest Neighbors\n* Stochastic Gradient Descent\n* Gradient Boosting Classifier\n\nFor each model, we set the model, fit it with 80% of our training data, predict for 20% of the training data and check the accuracy.","a0eac74d":"## We can see that the counts = describe value. Thus, there are no NaN value (only missing value)","87f405aa":"## Procedure\n1. Import libraries\n2. Import data\n3. Data analysis (look for missing\/corrupted values)\n4. Data visualization (maybe of each feature)\n5. Data cleaning\n6. Test different models and choose the best\n7. kaggle --> create submission file","57b3143a":"## Gradient Boosting Classifier has the highest score, we will choose this one.","22be26bb":"### Survivors are most likely female --> Sex feature is important in prediction","4f61b0d0":"## 4. Data visualization","fc7037fb":"## 1. Import Libraries","b2dc852e":"## Majority of people embarked in Southampton --> So, we will replace missing value with S.","1a7c3cf7":"## Baby is the most likely to survive."}}