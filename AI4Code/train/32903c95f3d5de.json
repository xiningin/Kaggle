{"cell_type":{"40e5f999":"code","d41495f6":"code","2cad04e2":"code","96b574d8":"code","287cd174":"code","b78076ef":"code","27f93864":"code","cd5e0cc9":"code","8bae75c2":"code","1870ea86":"code","ce8aa3d9":"code","32256180":"code","0537382e":"code","802721fe":"code","68c662b8":"code","54da60e3":"code","b096a7ea":"code","121d3078":"code","ecd3bf6d":"code","5be7c1ad":"code","b628700e":"code","30396a57":"code","26c748b4":"code","fad6a26f":"code","219c81cf":"code","b08fe476":"code","a26139aa":"code","dbfdd387":"code","4c027d29":"code","febe9ce2":"code","a87dd589":"code","06817826":"code","3d2a6ec2":"code","b53d0b0f":"code","40449b67":"markdown","33f047cd":"markdown"},"source":{"40e5f999":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom pylab import rcParams\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d41495f6":"#Read dataframe\ndata = pd.read_csv('\/kaggle\/input\/column_2C_weka.csv')\n","2cad04e2":"#Let's see the shape of our data\ndata.shape","96b574d8":"data.head()","287cd174":"data.tail()","b78076ef":"data.info()","27f93864":"data.describe()","cd5e0cc9":"data.rename(columns={\"class\": \"class1\"},inplace=True)","8bae75c2":"data.class1.value_counts()","1870ea86":"color=['red' if _=='Abnormal' else 'green' for _ in data.class1];\npd.plotting.scatter_matrix(data, alpha = 0.5,figsize = (20, 20),color=color,marker='*',s=200);","ce8aa3d9":"#update class as 0's and 1's ==> 0 if normal 1 otherwise\ndata.class1 = [1 if _=='Abnormal' else 0 for _ in data.class1]","32256180":"#Drop dependent variable from x values\nx_values=data.drop('class1',axis=1)\n#Seperate independent values from features\ny_values=data.class1","0537382e":"#Normalize the data between 0 and 1 \nx_values=(x_values-x_values.min())\/(x_values.max()-x_values.min())","802721fe":"x_values.info()","68c662b8":"y_values.value_counts()","54da60e3":"x_values.shape","b096a7ea":"y_values.shape","121d3078":"# Create train and test samples \nx_train, x_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.2, random_state=42)","ecd3bf6d":"x_train.info()","5be7c1ad":"y_train.shape","b628700e":"y_train.value_counts()","30396a57":"x_test.info()","26c748b4":"y_test.shape","fad6a26f":"y_test.value_counts()","219c81cf":"# Define the sigmoid function\ndef sigmoid(z):\n    y_head=1\/(1+np.exp(-z))\n    return y_head","b08fe476":"# initialize weight and Bias\n\ndef initialize_weight_bias(dimension):\n    weight=np.full((dimension,1),0.01)\n    bias = 0.0\n    return weight, bias","a26139aa":"# Define Cost Function\n\ndef cost_function(y_train,y_head):\n\n    loss_function=((1-y_train)*np.log(1-y_head)+y_train*np.log(y_head))*-1\n    cost=np.sum(loss_function)\/y_train.shape[0]\n    \n    return cost","dbfdd387":"#Define derivative of cost function with respect to weight and biss\ndef gradients(x_train,y_train,y_head_train):\n    weight_derivative=np.dot(x_train.T,(y_head_train-y_train.values.reshape(-1,1)))\/x_train.shape[0]\n    bias_derivative=np.sum((y_head_train-y_train.values.reshape(-1,1)))\/y_train.shape[0]\n    return weight_derivative, bias_derivative","4c027d29":"#Define forward and backward propagation method. \n\ndef forward_and_backward_propagation(x_train,y_train,learning_rate,number_of_iterations): \n    cost_list=[]\n    cost_list_by10=[]\n    dimension=x_train.shape[1] # just for plotting purposes to decrease complexity of the plot\n    weight,bias=initialize_weight_bias(dimension) # initialize weight and bias only once. \n    \n    \n    # performs forward and backward propagation n times (n defined by user as number of iterations. )\n  \n    for i in range(number_of_iterations):\n        z=np.dot(x_train,weight)+bias\n        y_head_train=sigmoid(z)\n        cost=cost_function(y_train.values.reshape(-1,1),y_head_train) # calls cost function to calculate the cost\/penalty\n        weight_derivative, bias_derivative=gradients(x_train,y_train,y_head_train) # calls gradient function to calculate the derivative of weight & bias\n       \n        weight=weight-weight_derivative*learning_rate # update weight\n        bias=bias-bias_derivative*learning_rate #update bias\n        cost_list.append(cost) # append the new cost into the cost_list   \n        if i%10==0:\n            cost_list_by10.append(cost)\n    \n\n    # Draw line chart\n    plt.plot(cost_list_by10)\n    rcParams['figure.figsize'] = 6,6 # set the size of the plot\n    plt.xlabel('iteration\/5: actual iteration is 10 times what is denoted ') \n    plt.ylabel('cost')\n    plt.title('cost w.r.t iteration')\n          \n    return weight,bias","febe9ce2":"# This method acts like a main engine: logistic regression is performed by calling this method.\n# Associated methods are called automatically. \n\ndef main_engine(x_train,y_train,x_test,y_test, learning_rate,number_of_iterations):\n    prediction_accuracy_train,parameters_train=sub_engine(x_train,y_train, learning_rate,number_of_iterations)\n    \n    z_test=np.dot(x_test,parameters_train[\"weight_learnt\"])+ parameters_train[\"bias_learnt\"]   #calculate z value ==> sum(wi * xi) + bias\n    y_head_test=sigmoid(z_test) # z values are converted into values between 0 and 1 to represent probability. \n    y_head_test=[1 if each_test>0.5 else 0 for each_test in y_head_test] #if y_head value is greater than 0.5 return 1 otherwise 0. \n    prediction_accuracy_test=(1-np.mean(np.abs(y_test-y_head_test)))*100 #calculate prediction accuracy (%)\n    return prediction_accuracy_train,prediction_accuracy_test","a87dd589":"#Method is used for hyper parameter fine-tuning\n\ndef sub_engine(x_train,y_train, learning_rate,number_of_iterations):\n    weight,bias=forward_and_backward_propagation(x_train,y_train,learning_rate,number_of_iterations)\n    parameters = {\"weight_learnt\": weight,\"bias_learnt\": bias}\n    \n    z_train=np.dot(x_train,weight)+bias #calculate z value ==> sum(wi * xi) + bias\n    y_head_train=sigmoid(z_train) # z values are converted into values between 0 and 1 to represent probability. \n    y_head_train=[1 if each>0.5 else 0 for each in y_head_train] #if y_head value is greater than 0.5 return 1 otherwise 0. \n    prediction_accuracy=(1-np.mean(np.abs(y_train-y_head_train)))*100 #calculate prediction accuracy (%)\n    return prediction_accuracy,parameters","06817826":"prediction_accuracy_train,prediction_accuracy_test=main_engine(x_train,y_train,x_test,y_test, 8,550)\nprint(\"prediction_accuracy_train is:\",prediction_accuracy_train)\nprint(\"prediction_accuracy_test is:\", prediction_accuracy_test)","3d2a6ec2":"#Determine the best performing hyper-parameters\n#Method comes up with two hyper parameter suggestions, namely number of iterations & \n#learning rate to maximize prediction accuracy\nprediction_list=[]\niteration_list=[]\nlearning_rate_list=[]\n\nfor iteration in range(100,1000,25):\n    for learn_rate in range(1,10,1):\n        prediction_accuracy_check,parameters_check=sub_engine(x_train,y_train, learn_rate,iteration)\n        \n        prediction_list.append(prediction_accuracy_check)\n        iteration_list.append(iteration)\n        learning_rate_list.append(learn_rate)\n        \nlearning_rate_list=pd.DataFrame(learning_rate_list)\nprediction_list=pd.DataFrame(prediction_list)\niteration_list=pd.DataFrame(iteration_list)\nhyper_parameters_df=pd.concat([iteration_list, learning_rate_list,prediction_list], axis=1, sort=False) \nhyper_parameters_df.columns = ['iteration_no', 'learning_rate','prediction_acc']\nprint(f'Max prediction accuracy can be reached with the following hyper parameters based on the train data:\\n when random state setting is 42\\n\\n'\n      ,hyper_parameters_df.loc[hyper_parameters_df.prediction_acc.idxmax()])\n      \n","b53d0b0f":"from sklearn.linear_model import LogisticRegression\nlog_res=LogisticRegression(solver='liblinear')\nlog_res.fit(x_train,y_train)\nprint('prediction accuracy of Logistic Regression model w\/ sci-kit learn is',log_res.score(x_test, y_test)*100)","40449b67":"# Conclusion\nLogistics Regression Model of Sci-kit learn predicts y values with **77% accuracy** (data=[x_test,y_test],random_state=42)\n\nPrediction accuracy of manually coded logistic regression model is **87.09%** \n\nThe prediction accuracy of manually coded regression model outperformed that of sci-kit learn model with different random state settings as well.\n\nThis difference in prediction accuracy between two different calculation methods does not make sense in the first glance therefore manual coding of the regression model needs to be validated. \n\n","33f047cd":"## Logistic Regression with Sckit-learn"}}