{"cell_type":{"1b4617fa":"code","ec180426":"code","9804109a":"code","baa73712":"code","f24a26ab":"code","d1d2cf51":"code","860a8c38":"code","876717e3":"code","f7e830ff":"code","9dc30c2a":"code","ae95a0bc":"code","8ccd141a":"code","3959e2f2":"code","83f3b52f":"code","e939a391":"code","538bfcf9":"code","f6c6c635":"code","a95e81c7":"code","a67b540b":"code","2851befd":"code","88bd3d30":"code","2f2161aa":"code","628160f2":"code","6626e783":"markdown","26dce0f5":"markdown","f8317982":"markdown","0cbeb9a7":"markdown","aa2e33b9":"markdown","660ebbe4":"markdown","37746868":"markdown","9ef2c265":"markdown","26e1bdee":"markdown","40312a5a":"markdown","f9614994":"markdown","5e38ea45":"markdown","e1acba41":"markdown","dee828df":"markdown","ec5a96fe":"markdown","17421c7b":"markdown"},"source":{"1b4617fa":"# Libraries\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib.pylab import rcParams\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM\nfrom tensorflow.python.client import device_lib\n\n# To eliminate chained assignment warnings.\npd.options.mode.chained_assignment = None\n\n# For plotting in the notebook itself.\n%matplotlib inline\n\n# Changing the style and size of plots.\nsns.set(style='darkgrid', palette='muted')\nrcParams['figure.figsize'] = 16, 8\n\n# Generating pseudi-random number for repeatable outputs.\nnp.random.seed(1)\ntf.random.set_seed(1)\n\n# Printing the tensorslow version.\nprint('Tensorflow version:', tf.__version__)","ec180426":"# Checking for available hardware for model training and printing it.\ndef get_available_devices():\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos]\n\nprint(get_available_devices())","9804109a":"# Importing CSV file in DataFrame and printing first few values.\ndf = pd.read_csv('..\/input\/fandata.csv')\ndf.head()","baa73712":"# Interactive plotting of amplitude vs time.\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df.time, y=df.amplitude,\n                    mode='lines',\n                    name='close'))\nfig.update_layout(showlegend=True)\nfig.show()","f24a26ab":"# 80% of data used for training & rest for testing purpose.\ntrain_size = int(len(df) * 0.8)\ntest_size = len(df) - train_size\ntrain, test = df.iloc[0:train_size], df.iloc[train_size:len(df)]\n\n# Printing the shapes for train & test data (rows, columns).\nprint(train.shape, test.shape)","d1d2cf51":"# First, fitting weights to the scaler.\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler = scaler.fit(train[['amplitude']])\n\n# Second, transforming train & test data using the scaler.\ntrain['amplitude'] = scaler.transform(train[['amplitude']])\ntest['amplitude'] = scaler.transform(test[['amplitude']])","860a8c38":"# Function for data-reshaping.\ndef create_dataset(X, y, time_steps = 1):\n    Xs, ys = [], []\n    \n    # Looping within the lenght of dataframe minus time steps.\n    for i in range(len(X) - time_steps):\n        Xs.append(X.iloc[i:(i + time_steps)].values)       \n        ys.append(y.iloc[i + time_steps])\n    return np.array(Xs), np.array(ys)","876717e3":"# Defining time steps for our model.\ntime_steps = 30\n\n# Creating datasets for training & testing purpose.\nX_train, y_train = create_dataset(train[['amplitude']], train.amplitude, time_steps)\nX_test, y_test = create_dataset(test[['amplitude']], test.amplitude, time_steps)\n\n# Printing the shape of training data.\nprint(X_train.shape, y_train.shape)","f7e830ff":"# Storing the number of timesteps & features.\ntimesteps = X_train.shape[1]\nnum_features = X_train.shape[2]\n\n# Printing the same for verification.\ntimesteps, num_features","9dc30c2a":"# Clearing any backend session for a brand new training.\ntf.keras.backend.clear_session()\n\n# Defining the neural network topology or in simple terms, the design network.\n# Creating an instance of a Sequential model.\nmodel = Sequential([\n    \n    # Adding LSTM Layers.\n    LSTM(50, input_shape = (timesteps, num_features), return_sequences=True),\n    LSTM(50, input_shape = (timesteps, num_features), return_sequences=True),\n    LSTM(50, input_shape = (timesteps, num_features), return_sequences=True),\n    LSTM(50, input_shape = (timesteps, num_features), return_sequences=True),\n    LSTM(50, input_shape = (timesteps, num_features), return_sequences=True),\n    LSTM(50, input_shape = (timesteps, num_features), return_sequences=True),\n    LSTM(50, input_shape = (timesteps, num_features), return_sequences=True),\n    LSTM(50, input_shape = (timesteps, num_features), return_sequences=True),\n    LSTM(50, input_shape = (timesteps, num_features), return_sequences=True),\n    LSTM(50, input_shape = (timesteps, num_features), return_sequences=True),\n    LSTM(50, input_shape = (timesteps, num_features), return_sequences=True),\n    \n    # Adding the final feed-forward layer to complete the autoencoder.\n    Dense(num_features)                 \n])\n\n# Compiling model with loss & optimizer parameter.\nmodel.compile(loss='mae', optimizer='adam')\n\n# Printing model summary for more information.\nmodel.summary()","ae95a0bc":"# Creating the keras callback.\nes = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3, mode = 'min')\n\n# Fitting the model to training data.\nhistory = model.fit(\n    \n    # Passing training data.\n    X_train, y_train,\n    \n    # Setting initial number of epochs.\n    epochs = 50,\n    \n    # Setting batch size hyper-parameter.\n    batch_size = 72,\n    \n    # Using 10% of training data for validation & rest 90% for actual training.\n    validation_split = 0.1,\n    \n    # Parsing the callback array while training.\n    callbacks = [es],\n    \n    # Not shuffling the data, since order for time-series data matters.\n    shuffle = False\n)","8ccd141a":"# Plotting training and validation loss\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.legend();","3959e2f2":"# Evaluating the model for test data in MAE\nmodel.evaluate(X_test, y_test)","83f3b52f":"# Calling predictions for training data\nX_train_pred = model.predict(X_train)\n\n# Calculating MAE for the predictions done on training data and storing that in a Pandas DataFrame.\ntrain_mae_loss = pd.DataFrame(np.mean(np.abs(X_train_pred - X_train), axis = 1), columns = ['Error'])","e939a391":"# Looking at the distribution of error column for training data\nsns.distplot(train_mae_loss, bins = 50, kde = True);","538bfcf9":"# Calling predictions for testing data.\nX_test_pred = model.predict(X_test)\n\n# Calculating MAE for the predictions done on testing data.\ntest_mae_loss = np.mean(np.abs(X_test_pred - X_test), axis = 1)","f6c6c635":"# Looking at the distribution of errors for testing data\nsns.distplot(test_mae_loss, bins = 50, kde = True);","a95e81c7":"# Setting the loss threshold for anamoly detection.\nTHRESHOLD = 0.055\n\n# Creating a new pandas dataframe for storing the following.\ntest_score_df = pd.DataFrame(test[time_steps:])\ntest_score_df['loss'] = test_mae_loss\ntest_score_df['threshold'] = THRESHOLD\ntest_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold\ntest_score_df['amplitude'] = test[time_steps:].amplitude","a67b540b":"# Printing first few values.\ntest_score_df.head()","2851befd":"# Checking the number of anamolies.\ntest_score_df.anomaly.value_counts()","88bd3d30":"# Interactive plotting for test values with an overlay line showing threshold.\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=test[time_steps:].time, y=test_score_df.loss,\n                    mode='lines',\n                    name='Test Loss'))\nfig.add_trace(go.Scatter(x=test[time_steps:].time, y=test_score_df.threshold,\n                    mode='lines',\n                    name='Threshold'))\nfig.update_layout(showlegend=True)\nfig.show()","2f2161aa":"# Creating a new DataFrame with only the anamolies & printing few values to verify.\nanomalies = test_score_df.query('anomaly == True')\nanomalies.head()","628160f2":"# Plotting the inversely transformed data with an overlay of anomalies points for test data.\n# Inverse transform is done to obtain the original values.\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=test[time_steps:].time, y=scaler.inverse_transform([test[time_steps:].amplitude]).flatten(),\n                    mode='lines',\n                    name='Amplitudes'))\nfig.add_trace(go.Scatter(x=anomalies.time, y=scaler.inverse_transform([anomalies.amplitude]).flatten(),\n                    mode='markers',\n                    name='Anomaly'))\nfig.update_layout(showlegend=True)\nfig.show()","6626e783":"Hear, we are creating a function for reshaping the data. This allows us to reuse the data-reshaping code again without repetition.\n\nAlso, we are returning the numpy array of the reshaped data since TensorFlow only uses the numpy arrays for modelling.","26dce0f5":"## Task 2: Load and Inspect the Time - Series Data","f8317982":"Here's the list of libraries being used for our project.\n<ul>\n<li>Numpy: is the core library for scientific computing in Python.<\/li>\n<li>TensorFlow: is a free and open-source software library for machine learning applications such as neural networks.<\/li>\n<li>Pandas: is a software library written for the Python programming language for data manipulation and analysis using DataFrames.<\/li>\n<li>Seaborn: is a Python data visualization library based on matplotlib and provides a high-level interface for plots.<\/li>\n<li>Matplotlib: is a comprehensive library for creating static, animated, and interactive visualizations.<\/li>\n<li>Plotly: is the interactive graphing library for Python.<\/li>\n<li>Sklearn: is mainly used for range of supervised & unsupervised learning but in our case, \"sklearn.preprocessing\" package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.<\/li>\n<\/ul>","0cbeb9a7":"When working with neural networks it is always good to scale data to a range between 0 and 1 & hence MinMaxScaler is used for tranforming data before model training.","aa2e33b9":"### Limitations of data used:\n\n<ul>\n<li>The data was obtained from a Fan running at different RPM (speed) for prototyping of this Deep learning model.\nAlthough, this model can be trained using actual Wind Turbine data from birth to death for actual predictions.<\/li>\n<li>Only one threshold was set for anamoly detection, but more can be set.<\/li>\n<li>The model was trained on 1-axis vibration, but it can also be trained using 3-axis vibration data.<\/li>\n<\/ul>","660ebbe4":"## Task 7: Plot Metrics and Evaluate the Model","37746868":"## Task 6: Train the LSTM Autoencoder model","9ef2c265":"## Task 1: Import Libraries","26e1bdee":"## Task 5: Build an LSTM Autoencoder","40312a5a":"We can outperform state-of-the-art time series anomaly detection algorithms and feed-forward neural networks by using long-short term memory (LSTM) networks.\n\nBased on recent research (the 2012 Stanford publication titled <a href = \"http:\/\/cs229.stanford.edu\/proj2012\/BussetiOsbandWong-DeepLearningForTimeSeriesModeling.pdf\">Deep Learning for Time Series Modeling<\/a> by Enzo Busseti, Ian Osband, and Scott Wong), we have skipped experimenting with deep feed-forward neural networks and directly jump to experimenting with a deep, recurrent neural network because it uses LSTM layers. Using LSTM layers is a way to introduce memory to neural networks that makes them ideal for analyzing time-series and sequence data.\n\n### Specifications of our model:\n<ul>\n<li>Using a sequential model because this allows to add as many layers as required.<\/li>\n<li>Then, add 11 LSTM layer as first layer with 50 internal neurons, input shape is of timesteps by number of features, and want the layer to return a sequence of the predicted future time steps.<\/li>\n<li>Then finalize with a normal, fully connected feed-forward layer to bring down the dimensions to the number of features.<\/li>\n<li>Finally, compile the model with two parameters:\n    <ul>\n    <li><code>loss = mae<\/code>, which means that the training error during training and validation is measured using the \u201cmean absolute error\u201d measure.<\/li>\n        <li><code>optimizer = adam<\/code>, which is a gradient decent parameter updater.<\/li>\n    <\/ul>\n<\/li>\n<\/ul>\n\n>Note: An autoencoder also called as <i>bottleneck<\/i> is a neural network that learns to copy its input to its output. It has an internal (hidden) layer that describes a code used to represent the input, and it is constituted by two main parts: an encoder that maps the input into the code, and a decoder that maps the code to a reconstruction of the original input.","f9614994":"## Task 4: Creating Training and Test Splits","5e38ea45":">NOTE: We are stopping the training of the model until there is no \"minimum\" improvement in validation loss for 3 successive epochs.","e1acba41":"## Task 8: Detecting Anomalies","dee828df":"# Predictive Maintenance for Wind Turbine Rotor\n\n## The aim of this project is to build a Deep Learning Keras model to predict the anomalies in Time-Series data using TensorFlow through LSTM autoencoder.\n\n### Getting to know the columns:\n\n<ul>\n<li>time: Time stamp of each data point.<\/li>\n<li>amplitude: Acceleration values in metre per square second.<\/li>\n<\/ul>","ec5a96fe":"While dealing with LSTM layers for time-series data, the input data needs to be reshaped to contain window of timesteps before training. So the data is supposed to be in the 3 dimensional shape consisting the number of samples, time steps & features.\n\n>For example:\nIf we have 3000 sample values, after re-shaping, instead of 3000 samples per dimension (per vibration axis) we have 300 batches of length 10. This way what we want to do is given the last 10 time steps of the signal predict the future 10.\n\n>NOTE: For 1-axis vibrational data, features = 1, for 2-axis vibrational data, features = 2 & for 3-axis vibrational data, features = 3.","17421c7b":"## Task 3: Data Preprocessing"}}