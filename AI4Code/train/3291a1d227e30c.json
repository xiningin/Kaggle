{"cell_type":{"de94cfa6":"code","c01863bd":"code","2e9d3f84":"code","a806e996":"code","df05e407":"code","4c335049":"code","4e9c1e66":"code","559b9bb6":"code","3a01b507":"code","6c6605fe":"code","4c9a4bd2":"code","0e3be95d":"code","e8cf4bbb":"code","c1cb9ebb":"code","927106cd":"code","e325cd5a":"code","db3000ec":"code","9d860056":"code","d33e4ec8":"code","c41ca0d2":"code","baa226d0":"code","b2f0f695":"code","4b3051eb":"code","82c1cb09":"code","d08ea0ab":"code","ab01fa0f":"code","83be3f87":"code","5a3bcd88":"code","850aa117":"code","61dd5f21":"code","1365292d":"code","563d9f70":"code","50cda489":"markdown","abfb1e43":"markdown","42a83ae0":"markdown","fb5e91c2":"markdown","6f4a129b":"markdown","d38d25c5":"markdown","294f2193":"markdown","4dee9cfb":"markdown","f560a4a5":"markdown","fe16ee98":"markdown","025c710f":"markdown","90e93d45":"markdown","cb9ea683":"markdown","cdd37b32":"markdown","0cdc4210":"markdown","d98b7345":"markdown","2e233f03":"markdown","78c74e62":"markdown","d3f92a24":"markdown","5113544a":"markdown","d47a4438":"markdown","8257a586":"markdown","92006c1f":"markdown","6e702b85":"markdown","1fd14e73":"markdown"},"source":{"de94cfa6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport matplotlib.pyplot as plt\nfrom catboost import CatBoostClassifier,Pool\nfrom sklearn.model_selection import train_test_split,cross_val_predict,StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom bayes_opt import BayesianOptimization\nfrom tqdm import tqdm_notebook as tqdm\nimport lightgbm as lgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler,RobustScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n\n\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n#print(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c01863bd":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test_bqCt9Pv.csv')","2e9d3f84":"#Lets have a look at the data\n\ntrain.head()","a806e996":"#Lets see how skewed the data is wrt the number of data points belonging to each class.\n\ntrain['loan_default'].value_counts()","df05e407":"branchList = train['branch_id'].unique()\nbranchSupId = train.groupby('branch_id')['supplier_id'].unique()\n\nbranchSupIdList = []\nanomalousBranch = []\n\nfor bra in range(len(branchList)):\n    branchId = branchList[bra]\n    branchSupIdList.append(branchSupId[branchId])\n\nfor i in range(len(branchSupIdList)):\n  for j in range(len(branchSupIdList)):\n    if(i != j):\n      #print(len(list(set(branchSupIdList[i]).intersection(set(branchSupIdList[j])))))\n      if ((len(list(set(branchSupIdList[i]).intersection(set(branchSupIdList[j]))))) != 0):  \n        if (len(list(set(branchSupIdList[i]).intersection(set(branchSupIdList[j]))))) >= 3:  \n          #Both branches in the same locality.\n          continue\n        else:\n          anomalousBranch.append(branchList[i])\n      else:\n        #Disjoint Branches\n        continue\n    else:\n      continue  ","4c335049":"def isBranchAnomalous(x):\n  if (x in anomalousBranch):\n    return 1\n  else:\n    return 0","4e9c1e66":"def CIBIL_norm(x):\n    a=''\n    if((x=='A-Very Low Risk') or (x=='B-Very Low Risk') or (x=='C-Very Low Risk') or (x=='D-Very Low Risk')):\n        a = 'Very Low Risk'\n    elif((x=='M-Very High Risk')):\n        a = 'Very Very High Risk'\n    elif((x=='L-Very High Risk')):\n        a='Very High Risk'\n    elif((x=='E-Low Risk') or (x=='F-Low Risk') or (x=='G-Low Risk')):\n        a = 'Low Risk'\n    elif((x=='H-Medium Risk') or (x=='I-Medium Risk')):\n        a = 'Medium Risk'\n    elif((x=='J-High Risk') or (x=='K-High Risk')):\n        a = 'High Risk'\n    elif((x=='Not Scored: No Activity seen on the customer (Inactive)') or (x=='Not Scored: No Updates available in last 36 months')):\n        a = 'Inactive'\n    elif((x=='Not Scored: Only a Guarantor')):\n        a='Guarantor'\n    elif((x=='Not Scored: More than 50 active Accounts found')):\n        a='SuperActive'\n    else:\n        a='Others'\n    return a","559b9bb6":"def CIBIL_other(x):\n    a=''\n    if((x=='A-Very Low Risk') or (x=='B-Very Low Risk') or (x=='C-Very Low Risk') or (x=='D-Very Low Risk')):\n        a = 'Very Low Risk'\n    elif((x=='M-Very High Risk')):\n        a = 'Very Very High Risk'\n    elif((x=='L-Very High Risk')):\n        a='Very High Risk'\n    elif((x=='E-Low Risk') or (x=='F-Low Risk') or (x=='G-Low Risk')):\n        a = 'Low Risk'\n    elif((x=='H-Medium Risk') or (x=='I-Medium Risk')):\n        a = 'Medium Risk'\n    elif((x=='J-High Risk') or (x=='K-High Risk')):\n        a = 'High Risk'\n    elif((x=='Not Scored: No Activity seen on the customer (Inactive)') or (x=='Not Scored: No Updates available in last 36 months')):\n        a = 'Inactive'\n    elif((x=='Not Scored: Only a Guarantor')):\n        a='Guarantor'\n    elif((x=='Not Scored: More than 50 active Accounts found')):\n        a='SuperActive'\n    elif((x=='No Bureau History Available') or (x=='Not Scored: Sufficient History Not Available') or (x=='Not Scored: Not Enough Info available on the customer')):  \n        a='NoHistory'\n    else:\n        a='Others'\n    return a","3a01b507":"def CIBIL_trend(x):\n    a=''\n    if(x==300):\n        a='Very Poor'\n    elif((x>300) and (x<=550)):\n        a='Poor'\n    elif((x>550) and (x<=650)):\n        a='Fair'\n    elif((x>650) and (x<=750)):\n        a='Good'\n    elif((x>750) and (x<=900)):\n        a='Excellent'\n    else:\n        a='Others'\n    return a","6c6605fe":"def NumIds(x):\n    a=''\n    if(x==1):\n        a = 'One'\n    elif(x==2):\n        a='Two'\n    elif(x==3):\n        a='Three'\n    else:\n        a='Four'\n    return a","4c9a4bd2":"def calcAge(x):\n    year = int(x.split('-')[2])\n    if(year<=19):\n        age = 20-year\n    else:\n        age = 100 + (20-year)\n    return age","0e3be95d":"def PrimaDefault(x):\n    a=''\n    if(x==-1):\n        a='First'\n    elif(x==0):\n        a='Great'\n    elif(x<=0.2):\n        a='Normal'\n    elif(x<=0.4):\n        a='Bothersome'\n    elif(x<=0.6):\n        a='Trouble'\n    elif(x<=0.8):\n        a='Danger'\n    else:\n        a='High Alert'\n    return a\n\ndef SecDefault(x):\n    a=''\n    if(x==-1):\n        a='First'\n    elif(x==0):\n        a='Great'\n    elif(x<=0.2):\n        a='Normal'\n    elif(x<=0.4):\n        a='Bothersome'\n    elif(x<=0.6):\n        a='Trouble'\n    elif(x<=0.8):\n        a='Danger'\n    else:\n        a='High Alert'\n    return a\n\ndef TotDefault(x):\n    a=''\n    if(x==-1):\n        a='First'\n    elif(x==0):\n        a='Great'\n    elif(x<=0.2):\n        a='Normal'\n    elif(x<=0.4):\n        a='Bothersome'\n    elif(x<=0.6):\n        a='Trouble'\n    elif(x<=0.8):\n        a='Danger'\n    else:\n        a='High Alert'\n    return a\n","e8cf4bbb":"def PrimaDefaultLastSix(x):\n    a=''\n    if(x==0):\n        a='Great'\n    elif(x==1):\n        a='Normal'\n    elif(x==2):\n        a='Bothersome'\n    elif(x>=3):\n        a='Trouble'\n    else:\n        a='Others'\n    return a","c1cb9ebb":"def CurrOutstandingBal(x):\n    a=''\n    if (x==0):\n        a='Good'\n    elif((x<0) and (x!=-1)):\n        a='Very Good'\n    elif(x>0 and x<=1):\n        a='Both'\n    elif(x>1):\n        a='Problematic'\n    else:\n        a='Other'\n    return a","927106cd":"def AvgAcctAge(x):\n    year = int(x.split(\" \")[0].split(\"y\")[0])\n    month = int(x.split(\" \")[1].split(\"m\")[0])\n    time_int = (12*year) + month\n    return time_int","e325cd5a":"def oneMonth(x):\n    if(int(x.split('-')[1])==10):\n        return 1\n    else:\n        return 0","db3000ec":"oneHot = train[['Aadhar_flag','PAN_flag','VoterID_flag','Driving_flag','Passport_flag']]\noneHot['sum'] = oneHot['Aadhar_flag'] + oneHot['PAN_flag'] + oneHot['VoterID_flag'] + oneHot['Driving_flag'] + oneHot['Passport_flag'] \ntrain['NumIDs'] = oneHot['sum']\n\noneHotTest = test[['Aadhar_flag','PAN_flag','VoterID_flag','Driving_flag','Passport_flag']]\noneHotTest['sum'] = oneHotTest['Aadhar_flag'] + oneHotTest['PAN_flag'] + oneHotTest['VoterID_flag'] + oneHotTest['Driving_flag'] + oneHotTest['Passport_flag'] \ntest['NumIDs'] = oneHotTest['sum']","9d860056":"#Simply Calling the functions defined above for feature engineering.\n\n#Train set dataframe manipulation\n\ntrain['NumIDsCnt'] = train['NumIDs'].apply(NumIds)\ntrain['IDsCount'] = np.where(train['NumIDs']>1,1,0)\n\ntrain['Age']=train['Date.of.Birth'].apply(calcAge)\n\ntrain['isStudent'] = np.where(train['Age']<=25,1,0)\ntrain['isSenior'] = np.where(train['Age']>=60,1,0)\n\ntrain['Employment.Type'] = np.where(train['Employment.Type'].isnull(),'Unemployed',train['Employment.Type'])\n\ntrain['leftover'] = train['asset_cost'] - train['disbursed_amount']\ntrain['loanRatio'] = (train['disbursed_amount']\/train['asset_cost'])*100\n\ntrain['CIBIL_Descr'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].apply(CIBIL_norm)\ntrain['CIBIL_Other'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].apply(CIBIL_other)\ntrain['CIBIL_Trend'] = train['PERFORM_CNS.SCORE'].apply(CIBIL_trend)\n\ntrain['PriOverduePercentage'] = np.where(train['PRI.NO.OF.ACCTS'] != 0,train['PRI.OVERDUE.ACCTS']\/train['PRI.NO.OF.ACCTS'],-1) \ntrain['SecOverduePercentage'] = np.where(train['SEC.NO.OF.ACCTS'] != 0,train['SEC.OVERDUE.ACCTS']\/train['SEC.NO.OF.ACCTS'],-1) \n\ntrain['PrimaDefaultRemark'] = train['PriOverduePercentage'].apply(PrimaDefault)\ntrain['SecoDefaultRemark'] = train['SecOverduePercentage'].apply(SecDefault)\ntrain['totalDefaultPercent'] = np.where((train['PRI.NO.OF.ACCTS'] + train['SEC.NO.OF.ACCTS']) != 0,(train['PRI.OVERDUE.ACCTS'] + train['SEC.OVERDUE.ACCTS'])\/(train['PRI.NO.OF.ACCTS'] + train['SEC.NO.OF.ACCTS']),-1)    \ntrain['TotaDefaultRemark'] = train['totalDefaultPercent'].apply(TotDefault) \n\ntrain['AcctsLastSixRemarks'] = train['DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS'].apply(PrimaDefaultLastSix)\n\ntrain['PRIcritRatio'] = np.where(train['PRI.DISBURSED.AMOUNT'] != 0,train['PRI.CURRENT.BALANCE']\/train['PRI.DISBURSED.AMOUNT'],-1)\ntrain['SECcritRatio'] = np.where(train['SEC.DISBURSED.AMOUNT'] != 0,train['SEC.CURRENT.BALANCE']\/train['SEC.DISBURSED.AMOUNT'],-1)\n\ntrain['TOT.DISBURSED.AMOUNT'] = train['PRI.DISBURSED.AMOUNT'] + train['SEC.DISBURSED.AMOUNT']\ntrain['TOT.CURRENT.BALANCE'] = train['PRI.CURRENT.BALANCE'] + train['SEC.CURRENT.BALANCE']\ntrain['TOTcritRatio'] = np.where(train['TOT.DISBURSED.AMOUNT'] != 0,train['TOT.CURRENT.BALANCE']\/train['TOT.DISBURSED.AMOUNT'],-1)\n\ntrain['PriRatioRemark'] = train['PRIcritRatio'].apply(CurrOutstandingBal)\ntrain['SecRatioRemark'] = train['SECcritRatio'].apply(CurrOutstandingBal)\ntrain['TotRatioRemark'] = train['TOTcritRatio'].apply(CurrOutstandingBal)\n\ntrain[\"AvgAcctAge\"] = train['AVERAGE.ACCT.AGE'].apply(AvgAcctAge)\ntrain['CredAcctAge'] = train['CREDIT.HISTORY.LENGTH'].apply(AvgAcctAge)\n\ntrain['OneMonthDef'] = train['DisbursalDate'].apply(oneMonth)\n\ntrain['PERFORM_CNS.SCORE'] = np.where(train['PERFORM_CNS.SCORE']==11,0,train['PERFORM_CNS.SCORE'])\ntrain['PERFORM_CNS.SCORE'] = np.where(train['PERFORM_CNS.SCORE']==14,0,train['PERFORM_CNS.SCORE'])\ntrain['PERFORM_CNS.SCORE'] = np.where(train['PERFORM_CNS.SCORE']==15,0,train['PERFORM_CNS.SCORE'])\ntrain['PERFORM_CNS.SCORE'] = np.where(train['PERFORM_CNS.SCORE']==16,0,train['PERFORM_CNS.SCORE'])\ntrain['PERFORM_CNS.SCORE'] = np.where(train['PERFORM_CNS.SCORE']==17,0,train['PERFORM_CNS.SCORE'])\ntrain['PERFORM_CNS.SCORE'] = np.where(train['PERFORM_CNS.SCORE']==18,0,train['PERFORM_CNS.SCORE'])\n\ntrain['isBranchAnomalous'] = train['branch_id'].apply(isBranchAnomalous)\n\n#Test Set dataframe manipulation\n\n\ntest['isBranchAnomalous'] = test['branch_id'].apply(isBranchAnomalous)\n\ntest['NumIDsCnt'] = test['NumIDs'].apply(NumIds)\ntest['IDsCount'] = np.where(test['NumIDs']>1,1,0)\n\ntest['Age']=test['Date.of.Birth'].apply(calcAge)\n\ntest['isStudent'] = np.where(test['Age']<=25,1,0)\ntest['isSenior'] = np.where(test['Age']>=60,1,0)\n\ntest['Employment.Type'] = np.where(test['Employment.Type'].isnull(),'Unemployed',test['Employment.Type'])\n\ntest['leftover'] = test['asset_cost'] - test['disbursed_amount']\ntest['loanRatio'] = (test['disbursed_amount']\/test['asset_cost'])*100\n\ntest['CIBIL_Descr'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].apply(CIBIL_norm)\ntest['CIBIL_Other'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].apply(CIBIL_other)\ntest['CIBIL_Trend'] = test['PERFORM_CNS.SCORE'].apply(CIBIL_trend)\n\ntest['PriOverduePercentage'] = np.where(test['PRI.NO.OF.ACCTS'] != 0,test['PRI.OVERDUE.ACCTS']\/test['PRI.NO.OF.ACCTS'],-1) \ntest['SecOverduePercentage'] = np.where(test['SEC.NO.OF.ACCTS'] != 0,test['SEC.OVERDUE.ACCTS']\/test['SEC.NO.OF.ACCTS'],-1) \n\ntest['PrimaDefaultRemark'] = test['PriOverduePercentage'].apply(PrimaDefault)\ntest['SecoDefaultRemark'] = test['SecOverduePercentage'].apply(SecDefault)\ntest['totalDefaultPercent'] = np.where((test['PRI.NO.OF.ACCTS'] + test['SEC.NO.OF.ACCTS']) != 0,(test['PRI.OVERDUE.ACCTS'] + test['SEC.OVERDUE.ACCTS'])\/(test['PRI.NO.OF.ACCTS'] + test['SEC.NO.OF.ACCTS']),-1)    \ntest['TotaDefaultRemark'] = test['totalDefaultPercent'].apply(TotDefault) \n\ntest['AcctsLastSixRemarks'] = test['DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS'].apply(PrimaDefaultLastSix)\n\ntest['PRIcritRatio'] = np.where(test['PRI.DISBURSED.AMOUNT'] != 0,test['PRI.CURRENT.BALANCE']\/test['PRI.DISBURSED.AMOUNT'],-1)\ntest['SECcritRatio'] = np.where(test['SEC.DISBURSED.AMOUNT'] != 0,test['SEC.CURRENT.BALANCE']\/test['SEC.DISBURSED.AMOUNT'],-1)\n\ntest['TOT.DISBURSED.AMOUNT'] = test['PRI.DISBURSED.AMOUNT'] + test['SEC.DISBURSED.AMOUNT']\ntest['TOT.CURRENT.BALANCE'] = test['PRI.CURRENT.BALANCE'] + test['SEC.CURRENT.BALANCE']\ntest['TOTcritRatio'] = np.where(test['TOT.DISBURSED.AMOUNT'] != 0,test['TOT.CURRENT.BALANCE']\/test['TOT.DISBURSED.AMOUNT'],-1)\n\ntest['PriRatioRemark'] = test['PRIcritRatio'].apply(CurrOutstandingBal)\ntest['SecRatioRemark'] = test['SECcritRatio'].apply(CurrOutstandingBal)\ntest['TotRatioRemark'] = test['TOTcritRatio'].apply(CurrOutstandingBal)\n\ntest[\"AvgAcctAge\"] = test['AVERAGE.ACCT.AGE'].apply(AvgAcctAge)\ntest['CredAcctAge'] = test['CREDIT.HISTORY.LENGTH'].apply(AvgAcctAge)\n\ntest['OneMonthDef'] = test['DisbursalDate'].apply(oneMonth)\n\ntest['PERFORM_CNS.SCORE'] = np.where(test['PERFORM_CNS.SCORE']==11,0,test['PERFORM_CNS.SCORE'])\ntest['PERFORM_CNS.SCORE'] = np.where(test['PERFORM_CNS.SCORE']==14,0,test['PERFORM_CNS.SCORE'])\ntest['PERFORM_CNS.SCORE'] = np.where(test['PERFORM_CNS.SCORE']==15,0,test['PERFORM_CNS.SCORE'])\ntest['PERFORM_CNS.SCORE'] = np.where(test['PERFORM_CNS.SCORE']==16,0,test['PERFORM_CNS.SCORE'])\ntest['PERFORM_CNS.SCORE'] = np.where(test['PERFORM_CNS.SCORE']==17,0,test['PERFORM_CNS.SCORE'])\ntest['PERFORM_CNS.SCORE'] = np.where(test['PERFORM_CNS.SCORE']==18,0,test['PERFORM_CNS.SCORE'])\n\n","d33e4ec8":"train.drop(columns=['Date.of.Birth','DisbursalDate','UniqueID'],inplace=True)\ntest.drop(columns=['Date.of.Birth','DisbursalDate','UniqueID'],inplace=True)","c41ca0d2":"def scoreOfModel(clf,X,y,flag,shuffleBool=False,nFolds=5):\n    score = 0\n    finalPreds = np.zeros(112392)\n    trainPreds = np.zeros(233154)\n    folds = StratifiedKFold(n_splits=nFolds, shuffle=shuffleBool, random_state=42)\n    #train_pred = cross_val_predict(clf, X, y, cv=12,method='predict_proba')\n    for fold_, (trn_idx, val_idx) in tqdm(enumerate(folds.split(X,stratCol))):\n        X_train,X_val = X.loc[trn_idx,:],X.loc[val_idx,:]\n        y_train,y_val = y[trn_idx],y[val_idx]\n        clf.fit(X_train,y_train)\n        yPreds = clf.predict_proba(X_val)\n        yPredsTweaked = yPreds[:,1]\n        trainPreds[val_idx] = yPredsTweaked\n        score += roc_auc_score(y_val,yPredsTweaked)\n        p = clf.predict_proba(x_test)\n        #Adding the probabilities of belonging to the class \"1\".\n        for k in range(len(p)):\n            finalPreds[k] += p[k][1]\n        print(\"**********\"+ str(score\/(1+fold_)) + \"******************Iteration \"+str(fold_)+\" Done****************\")    \n    return str(score\/nFolds),(trainPreds),(finalPreds\/nFolds)  \n","baa226d0":"labelEnc = ['AVERAGE.ACCT.AGE','CREDIT.HISTORY.LENGTH',\n'branch_id', 'supplier_id','manufacturer_id','State_ID','PERFORM_CNS.SCORE.DESCRIPTION','Current_pincode_ID',\n                                          'isStudent','isSenior','Employment.Type',\n                                                  'CIBIL_Trend','AcctsLastSixRemarks','OneMonthDef',\n                                                  'NumIDsCnt','CIBIL_Descr','CIBIL_Other',\n                                                  'PrimaDefaultRemark','SecoDefaultRemark','TotaDefaultRemark',\n                                                  'PriRatioRemark','SecRatioRemark','TotRatioRemark',  'Employee_code_ID'\n                                                    ]\n","b2f0f695":"X = train.drop(columns=['loan_default'])\ny = train['loan_default']","4b3051eb":"data = pd.concat([X, test], axis = 0)\n\nX_newLGB = data.copy()\n#test_newLGB = test.copy()\nfor col in labelEnc:\n  le = LabelEncoder()\n  data[col] = le.fit_transform(data[col])\n  #X_newLGB[col] = le.fit_transform(data[col])\n  #test_newLGB[col] = le.transform(test[col])\n\ndata['is_test'] = np.zeros(345546)\n\n#(data.iloc[:233154,:])['is_test'] = 0\ndata.iloc[233154:,-1] = 1\n\ntrain_examples = train.shape[0]\n\ndata_x = data.drop('is_test', axis=1)\ndata_y = data['is_test']\n\nis_test_probs = cross_val_predict(RandomForestClassifier(max_depth = 7,n_estimators=200), data_x, data_y, method='predict_proba')[:train_examples]\n\nis_test_Probs = is_test_probs[:,1]\n\nfrom scipy.stats import rankdata\n\ndata.iloc[:233154,-1] = rankdata(is_test_Probs)\nbins = np.histogram(data.iloc[:233154,-1])[1][:-1]\n#train['is_test_bins'] = np.digitize(X_newLGB['is_test'], bins)\nstratCol = np.digitize(data.iloc[:233154,-1], bins)","82c1cb09":"x_train = data.iloc[:233154,:]\nx_test = data.iloc[233154:,:]","d08ea0ab":"catFeatures = ['AVERAGE.ACCT.AGE','CREDIT.HISTORY.LENGTH','branch_id', 'supplier_id','manufacturer_id','State_ID','PERFORM_CNS.SCORE.DESCRIPTION','Current_pincode_ID',\n                                          'isStudent','isSenior','Employment.Type',\n                                                  'CIBIL_Trend','AcctsLastSixRemarks','OneMonthDef',\n                                                  'NumIDsCnt','CIBIL_Descr','CIBIL_Other',\n                                                  'PrimaDefaultRemark','SecoDefaultRemark','TotaDefaultRemark',\n                                                  'PriRatioRemark','SecRatioRemark','TotRatioRemark',  'Employee_code_ID' \n                                         ]","ab01fa0f":"# catClf1 = CatBoostClassifier(learning_rate = 0.02147,iterations = 9997, l2_leaf_reg = 9985,scale_pos_weight = 3.662,eval_metric='AUC',\n#                             silent = True,cat_features=catFeatures)\n\ncatClf2 = CatBoostClassifier(learning_rate = 0.03185,iterations = 2000, l2_leaf_reg = 999.6,scale_pos_weight = 1.915,eval_metric='AUC',\n                            silent = True,cat_features=catFeatures)\n\ncatClf3 = CatBoostClassifier(learning_rate = 0.03998,iterations = 1497, l2_leaf_reg = 49.97,scale_pos_weight = 2.207,eval_metric='AUC',\n                            silent = True,cat_features=catFeatures)\n\n# catClf4 = CatBoostClassifier(learning_rate = 0.02838,iterations = 5174, l2_leaf_reg = 6311,scale_pos_weight = 3.926,eval_metric='AUC',\n#                             silent = True,cat_features=catFeatures)\n\n# catClf5 = CatBoostClassifier(learning_rate = 0.02373,iterations = 3174, l2_leaf_reg = 2739,scale_pos_weight = 2.228,eval_metric='AUC',\n#                             silent = True,cat_features=catFeatures)\n\n# catClf6 = CatBoostClassifier(learning_rate = 0.02,iterations = 5336, l2_leaf_reg = 7763,scale_pos_weight = 2.012,eval_metric='AUC',\n#                             silent = True,cat_features=catFeatures)\n\n# catClf7 = CatBoostClassifier(learning_rate = 0.03624,iterations = 5995, l2_leaf_reg = 9994,scale_pos_weight = 0.8615,eval_metric='AUC',\n#                             silent = True,cat_features=catFeatures)\n\ncatClf8 = CatBoostClassifier(learning_rate = 0.03365,iterations = 2001, l2_leaf_reg = 9943,scale_pos_weight = 4.617,eval_metric='AUC',\n                            silent = True,cat_features=catFeatures)\n\n# catClf9 = CatBoostClassifier(learning_rate = 0.03132,iterations = 9985, l2_leaf_reg = 9988,scale_pos_weight = 0.6724,eval_metric='AUC',\n#                            silent = True,cat_features=catFeatures)\n\n#catClf10 = CatBoostClassifier(learning_rate = 0.0379,iterations = 2001, l2_leaf_reg = 5651,scale_pos_weight = 3.447,eval_metric='AUC',\n#                            silent = True,cat_features=catFeatures)\n\n#catClf11 = CatBoostClassifier(learning_rate = 0.02852,iterations = 2015, l2_leaf_reg = 2005,scale_pos_weight = 1.301,eval_metric='AUC',\n#                            silent = True,cat_features=catFeatures)\n\n# catClf12 = CatBoostClassifier(learning_rate = 0.02705,iterations = 6150, l2_leaf_reg = 9998,scale_pos_weight = 4.336,eval_metric='AUC',\n#                             silent = True,cat_features=catFeatures)\n\n# catClf13 = CatBoostClassifier(learning_rate = 0.02706,iterations = 9996, l2_leaf_reg = 6891,scale_pos_weight = 0.596,eval_metric='AUC',\n#                             silent = True,cat_features=catFeatures)\n\n# catClf14 = CatBoostClassifier(learning_rate = 0.0396,iterations = 5582, l2_leaf_reg = 2002,scale_pos_weight = 2.781,eval_metric='AUC',\n#                             silent = True,cat_features=catFeatures)\n\n# catClf15 = CatBoostClassifier(learning_rate = 0.03604,iterations = 9958, l2_leaf_reg = 10000,scale_pos_weight = 3.879,eval_metric='AUC',\n#                             silent = True,cat_features=catFeatures)\n\n# catClf16 = CatBoostClassifier(learning_rate = 0.02844,iterations = 7360, l2_leaf_reg = 6280,scale_pos_weight = 0.6643,eval_metric='AUC',\n#                             silent = True,cat_features=catFeatures)\n\n# catClf17 = CatBoostClassifier(learning_rate = 0.03624,iterations = 5995, l2_leaf_reg = 9994,scale_pos_weight = 0.8615,eval_metric='AUC',\n#                             silent = True,cat_features=catFeatures)\n\n# catClf18 = CatBoostClassifier(learning_rate = 0.03098,iterations = 2002, l2_leaf_reg = 6313,scale_pos_weight = 5.341,eval_metric='AUC',\n#                             silent = True,cat_features=catFeatures)\n\n# catClf19 = CatBoostClassifier(learning_rate = 0.02764,iterations = 7314, l2_leaf_reg = 7162,scale_pos_weight = 5.475,eval_metric='AUC',\n#                             silent = True,cat_features=catFeatures)\n","83be3f87":"#scr_catClf1,trainPredsProbas1,catClfPreds1 = scoreOfModel(catClf1,x_train,y,3)\nscr_catClf2,trainPredsProbas2,catClfPreds2 = scoreOfModel(catClf2,x_train,y,3)\nscr_catClf3,trainPredsProbas3,catClfPreds3 = scoreOfModel(catClf3,x_train,y,3)\n#scr_catClf4,trainPredsProbas4,catClfPreds4 = scoreOfModel(catClf4,x_train,y,3)\n#scr_catClf5,trainPredsProbas5,catClfPreds5 = scoreOfModel(catClf5,x_train,y,3)\n#scr_catClf6,trainPredsProbas6,catClfPreds6 = scoreOfModel(catClf6,x_train,y,3)\n#scr_catClf7,trainPredsProbas7,catClfPreds7 = scoreOfModel(catClf7,x_train,y,3)\nscr_catClf8,trainPredsProbas8,catClfPreds8 = scoreOfModel(catClf8,x_train,y,3)\n#scr_catClf9,trainPredsProbas9,catClfPreds9 = scoreOfModel(catClf9,x_train,y,3)\n#scr_catClf10,trainPredsProbas10,catClfPreds10 = scoreOfModel(catClf10,x_train,y,3)\n#scr_catClf11,trainPredsProbas11,catClfPreds11 = scoreOfModel(catClf11,x_train,y,3)\n# scr_catClf12,trainPredsProbas12,catClfPreds12 = scoreOfModel(catClf12,x_train,y,3)\n# scr_catClf13,trainPredsProbas13,catClfPreds13 = scoreOfModel(catClf13,x_train,y,3)\n# scr_catClf14,trainPredsProbas14,catClfPreds14 = scoreOfModel(catClf14,x_train,y,3)\n# scr_catClf15,trainPredsProbas15,catClfPreds15 = scoreOfModel(catClf15,x_train,y,3)\n# scr_catClf16,trainPredsProbas16,catClfPreds16 = scoreOfModel(catClf16,x_train,y,3)\n# scr_catClf17,trainPredsProbas17,catClfPreds17 = scoreOfModel(catClf17,x_train,y,3)\n# scr_catClf18,trainPredsProbas18,catClfPreds18 = scoreOfModel(catClf18,x_train,y,3)\n# scr_catClf19,trainPredsProbas19,catClfPreds19 = scoreOfModel(catClf19,x_train,y,3)","5a3bcd88":"stackedDF = pd.DataFrame({#'One' : trainPredsProbas1,\n                          'Two' : trainPredsProbas2,'Three' : trainPredsProbas3, \n                          # 'Four' : trainPredsProbas4, 'Five' : trainPredsProbas5, 'Six' : trainPredsProbas6,\n                          #'Seven':trainPredsProbas7,\n                           'Eight':trainPredsProbas8,\n                            #'Nine':trainPredsProbas9,\n                          #'Ten':trainPredsProbas10,'Eleven':trainPredsProbas11\n                            #,'Twelve':trainPredsProbas12,\n                          #'Thirteen':trainPredsProbas13,'Fourteen':trainPredsProbas14,\n                          #'Fifteen':trainPredsProbas15,'Sixteen':trainPredsProbas16,'Seventeen':trainPredsProbas17,\n                          #'Eighteen':trainPredsProbas18,'Nineteen':trainPredsProbas19\n                         })\n\nstackedTest = pd.DataFrame({#'One' : catClfPreds1,\n                            'Two' : catClfPreds2, 'Three' : catClfPreds3,\n                            #'Four' : catClfPreds4,'Five' : catClfPreds5, 'Six' : catClfPreds6,\n                           #'Seven':catClfPreds7,\n                           'Eight':catClfPreds8,\n                            #'Nine':catClfPreds9,\n                         # 'Ten':catClfPreds10,'Eleven':catClfPreds11\n                            #,'Twelve':catClfPreds12,\n                          #'Thirteen':catClfPreds13,'Fourteen':catClfPreds14,\n                          #'Fifteen':catClfPreds15,'Sixteen':catClfPreds16,'Seventeen':catClfPreds17,\n                          #'Eighteen':catClfPreds18,'Nineteen':catClfPreds19\n                         })\n","850aa117":"Stacker = LogisticRegression(C = 0.003728,solver='liblinear')","61dd5f21":"LRprobas = np.zeros(112392)\nfolds = StratifiedKFold(n_splits=5, shuffle=False, random_state=42)\nfor fold_, (trn_idx, val_idx) in tqdm(enumerate(folds.split(stackedDF,y))):\n    X_train,X_val = stackedDF.loc[trn_idx,:],stackedDF.loc[val_idx,:]\n    y_train,y_val = y[trn_idx],y[val_idx]\n    \n    Stacker.fit(X_train,y_train)\n    \n    #LRpreda = Stacker.predict_proba(X_val)\n    #LRtrainprobas[val_idx] = LRpreda[:,1]\n    #LRpredaT = LRpreda[:,1]\n    #LRscore = LRscore + roc_auc_score(y_val,LRpredaT)\n    LRpreds = Stacker.predict_proba(stackedTest)\n    LRprobas = LRprobas + LRpreds[:,1]\n\nLRprobas = LRprobas\/5","1365292d":"sub = pd.read_csv('..\/input\/sample_submission_24jSKY6.csv')\n\nsub['loan_default'] = LRprobas","563d9f70":"sub.to_csv('Submission.csv',index = False)","50cda489":"**Applying the functions defined above, and thus, the final steep in actually implementing the features planned above.**","abfb1e43":"**READING THE FILES**","42a83ae0":"**Basic Overview of the things done in the kernel before jumping into the coding part - \n**\n\n1. FEATURE ENGINEERING - \n      *   **Anomalous Branch** - Keeps track of the branches, from where, certain loans have been sanctioned and then the buy has been done at a showroom  which is far from that bank,possibly even in a **different state or city**. This is tracked by seeing the usual showrooms from where buys take place if a loan is sanctioned from a particular branch. Certain anomalies detected in this list have been tracked in this feature.\n      *   The super-messy Perform_CNS Score categorical data have been **re-binned** to give a cleaner idea of the CIBIL scores. There have been 2 new binnings made. One on the basis of some background knowledge about banking, and the way banks segregate the users and the second according to the data provided in the dataset.\n      *   The number of ID Proofs a person has submitted at the time of taking the loan - Assumption being, **the more number of IDs shown, the more the credibility of the borrower.**\n      *   The number of Primary and Secondary accounts a person already has defaulted, overall as well as over the last 6 months.\n      *   The borrower's age, his\/her average account age,i.e, on an average how much time he\/she takes to give back all the lent money.\n      *   Whether the borrower is a **\"Student\"** or a **\"Senior Citizen\"** from the age and the employment status.\n      *   Since the model was suppposed to predict who would be defaulting in the **FIRST MONTH** of taking loan, so, keeping track of which all users defaulted in the **first month only**, rather than who all defaulted over-all in the train set makes more sense as the model would then be able to recognize the trends and behaviour more easily.\n      \n2. MISSING DATA HANDLING -\n      *   The missing \"Employment\" were treated as **\"Unemployed\"** as of that moment.\n      *   The UNEMPLOYED borrowers were categorized into \"Students\" and \"Senior Citizens\" taking a hint from their ages.\n      \n3. STRATIFICATION - \n\n      *   Stratification done on the basis of **similarity between the train and test set**, rather than doing on the basis of the classes.\n    \n4. TRAINING -\n\n      *   **Five-Fold Cross Validation** was used and the predictions on the test set were taken over the model trained on each fold and were finally averaged over all the folds to get the final prediction over the test set.\n      *   Heavy **Parameter Tuning done on CatBoost Classifier**, LightGBM, Random Forest and XGBoost, with CatBoost out-performing the rest. Hence, finally CatBoost was used for submission.\n      *   **Stacking** was done, with 20 CatBoost models and a meta learner (Logistic Regression) was used.\n     \n","fb5e91c2":"**Stacking the base models**","6f4a129b":"The logic behind this feature is that, if a person takes a loan from a particular branch, in normal cases, we would expect him to buy the vehicle from a showroom\/retailer which is located in the same city ( or the same state in worst case scenario ). \n\nSo, my assumption was that every branch serves to customers who then go to one of the showrooms of a disjoint set, i.e, ideally, there should be a set of showrooms from where if a customer is buying a vehicle, then he must be getting it funded from a particular branch.\n\nThough this seemed to me to be somewhat logical assumption, it *didnt* really turn out to be that good a differentiator.","d38d25c5":"**Feature representing the number of identity cards given by the customer.**","294f2193":"**CIBIL features are made from a bit of background knowledge. This is the usual score used by financial institutions in order to decide whether to lend money to a person or not.**","4dee9cfb":"![](https:\/\/github.com\/rajat5ranjan\/AV-LTFS-Data-Science-FinHack-ML-Hackathon\/raw\/2853f792147b4305cad1b40d75893dab112e6611\/ltfs.jpg)","f560a4a5":"Training the Stacker with **5 fold CV stratified**.","fe16ee98":"Certain Scores on CNS Score like 11,14,15,16,17,18 were all made 0 as they all were corresponding to cases with less\/no history of the borrower being available.","025c710f":"Keeping track of the number of primary default accounts in the last 6 months and assigning a remark to that.","90e93d45":"For the final submission, I have stacked all the 20 base models with the hyper-parameters which I have mentioned in the comments below as well. \n\nDue to the restriction on the maximum time a kernel can run for getting committed on Kaggle, I am unable to run a complete stack of 20 models here.\n\nHere I have trained only 3 models and stacked on those.\n\nHence, this result can be expected to be a bit *sub-optimal* than the max scores which I have actually achieved in the hackathon.","cb9ea683":"**Using \"similarity between the train and test columns\" as the stratification. **\n\nInstead of using the class labels as stratification,using the similarity between the train and test set as a parameter for stratification tends to give a better model, considering that the model gets an idea about how similar\/dissimilar the data points in train and test set are.","cdd37b32":"**Making a function for easier Training and Cross-Validation - Using 5 fold stratified cross-validation**","0cdc4210":"**Remarks on the basis of the number of Primary and Secondary Defaulted accounts of that person.**","d98b7345":"Defining a new feature based on the Number of outstanding Balance accounts the customer has. \n\nThe idea behind this being, **more the number of accounts a customer has with outstanding balance**, **the less reliable** he would be expected to be.","2e233f03":"**Categorical Features**","78c74e62":"Hello Everyone !\n\nThis kernel consists of my work for the **AV - LTFS Hackathon** where we were supposed to predict the loan defaulters in the first month of EMI payment.\n\nI have tried some feature engineering first up, followed by parameter tuning of CatBoost and then a 1-Layer Stacking of the different base models.\n\nOther than CatBosst, XGBoost,LightGBM,RF,NNs were also tried, but they were giving sub-optimal results.\n\nOn hind sight, a bit more extensive feature engineering would have helped in boosting the score further up.\n\nThis kernel gets a \n\n**CV Score - 0.6752**\n\n**Public LB Score - 0.6636**       ( Rank - 53rd \/ 1352 )\n\n**Private LB Score - 0.667127**    ( Rank - 47th \/ 1352 )\n\n(AUC-ROC Metric)","d3f92a24":"The number of ID proofs submitted by a person while taking a loan.  - > Thought behind this is, **More the number of ID proofs a person submits, more is the chance of that person being a genuine person** and not someone who is intentionally going to default in EMI Payments.","5113544a":"**Training different base models with different hyper-paramter settings for stacking.**","d47a4438":"* The 'PERFORM_CNS.SCORE.DESCRIPTION' column has a lot of bins and there are many different kinds of bins which essentially represent more or less the same set of people\/distribution of customers. \n\n**Ex - Different types of \"High Risk\", \"Low Risk\" etc.**","8257a586":"**Now that we have reached the end of the kernel, I am assuming you liked the kernel, since you didnt close it mid-way.**\n\n**If you did like it, please UPVOTE the kernel. That keeps me going !**\n\n**Any suggestions and criticism are welcome.**\n\n**Cheers !**","92006c1f":"**Defining a new Dataframe with the prediction values from our previous base models and then, we will use this DataFrame to train a meta-learner ( Logistic Regression in this kernel ) to get a boost in the prediction levels.**","6e702b85":"**Age Calculation from the DOB column**","1fd14e73":"**FEATURE ENGINEERING - MAKING UP AS MANY NEW INNOVATIVE FEATURES AS I COULD COME UP WITH**"}}