{"cell_type":{"87ae91a1":"code","a1d7e1be":"code","d794af3d":"code","3be8466c":"code","75f02c7e":"code","d4a7f995":"code","19545c69":"code","bc0f8535":"code","725dc072":"code","86ef2e45":"code","e1a19323":"code","ec9a8a31":"code","c7fd2a23":"code","b227bf92":"code","42fa5a3b":"code","bd9a2191":"code","a5e8a391":"code","4883bc4c":"code","e38cf6ef":"code","1eb81190":"code","5a7ee91a":"code","b01a44af":"code","d405c44e":"code","1c4ab950":"code","e3da622d":"code","bef30af1":"code","bad0a121":"code","63b135f2":"code","274cd28e":"code","aeeb3029":"code","ac02ccf6":"code","786348c5":"code","850110e8":"code","7cb97939":"code","9b78bd2f":"code","0783e65d":"code","2a1007f8":"code","fe263452":"code","0b315f68":"code","211c6a9b":"code","942c1dc1":"code","89331235":"markdown","7c1307b1":"markdown","c064dbd3":"markdown","d4823f3c":"markdown","4f9d939b":"markdown","b986b9f5":"markdown","5235e86e":"markdown","d1a70f02":"markdown","8f56f27d":"markdown","4de7635c":"markdown","da501b6c":"markdown","67017627":"markdown","b18a5ead":"markdown","df02f7f9":"markdown","97dfca05":"markdown","68162526":"markdown","0307e6b8":"markdown","bb8bf1e8":"markdown","4f38ad71":"markdown","698bb3bd":"markdown","9284193c":"markdown","6ba7fb0c":"markdown","9bf58878":"markdown","de689d54":"markdown","c64d3217":"markdown","a63491fa":"markdown"},"source":{"87ae91a1":"# Uncomment below lines to download the dataset directly from Kaggle\n## To Install Kaggle package in case it is not already present\n# !pip install kaggle\n## Download dataset\n# !kaggle datasets download -d xhlulu\/medal-emnlp","a1d7e1be":"#Importing the Required Python Packages\nimport os\nimport shutil\nimport zipfile\nimport pandas as pd\nimport matplotlib.pyplot as plt","d794af3d":"print(os.getcwd())","3be8466c":"#Reading the current path in a variable\npath = os.getcwd()\nprint(path)","75f02c7e":"#Creating a folder by name Train\nos.mkdir(os.path.join(path, 'Train'))","d4a7f995":"#Creating a folder by name Test\nos.mkdir(os.path.join(path, 'Test'))","19545c69":"#Creating a folder by name Data\nos.mkdir(os.path.join(path, 'Data'))","bc0f8535":"#Creating a folder by name Validation\nos.mkdir(os.path.join(path, 'Validation'))","725dc072":"#Creating a folder by name Images\nos.mkdir(os.path.join(path, 'Images'))","86ef2e45":"#Moving the file to Data folder\nsource = os.path.join(path, 'NLP_Dataset.zip')\ndestination = os.path.join(path, 'Data', 'NLP_Dataset.zip')\nshutil.move(source, destination)","e1a19323":"#Check the contents of Data folder\nprint(os.listdir(os.path.join(path, 'Data')))","ec9a8a31":"#Unzip the dataset file\nwith zipfile.ZipFile(destination, 'r') as zip_ref:\n    zip_ref.extractall(os.path.join(path, 'Data'))","c7fd2a23":"#Check the contents of Data folder\nprint(os.listdir(os.path.join(path, 'Data')))","b227bf92":"#Check the contents of pretrain_subset folder\nprint(os.listdir(os.path.join(path, 'Data', 'pretrain_subset')))","42fa5a3b":"# Deleting full_data.csv and moving train, test and valid files to Data folder.\nsource = os.path.join(path, 'Data', 'pretrain_subset')\ndestination = os.path.join(path, 'Data')\nshutil.move(os.path.join(source, 'train.csv'), os.path.join(destination, 'train.csv'))\nshutil.move(os.path.join(source, 'test.csv'), os.path.join(destination, 'test.csv'))\nshutil.move(os.path.join(source, 'valid.csv'), os.path.join(destination, 'valid.csv'))\nos.remove(os.path.join(destination, 'full_data.csv'))\n\n#Check the contents of Data folder\nprint(os.listdir(os.path.join(path, 'Data')))","bd9a2191":"#Creating a path variable directly to the dataset\ndata_path = os.path.join(path, 'Data', 'train.csv')\nprint(data_path)","a5e8a391":"# Loading the data in a dataframe.\ntextDF = pd.read_csv(data_path)","4883bc4c":"#Checking the shape of Dataframe\ntextDF.shape","e38cf6ef":"textDF.drop(textDF.index[1000000:], inplace = True)\ntextDF.shape","1eb81190":"#Checking the first 5 rows of the dataframe\ntextDF.head(5)","5a7ee91a":"#Checking last 5 rows of the dataframe\ntextDF.tail(5)","b01a44af":"#Checking the summary statistics of the dataframe\ntextDF.describe(include = 'all')","d405c44e":"#Checking the datatypes of dataframe columns\ntextDF.dtypes","1c4ab950":"# Checking the unique Abstract_id to see if Abstract_id can be converted to Index\ntextDF['ABSTRACT_ID'].nunique()","e3da622d":"duplicate = textDF[textDF.duplicated()]\nduplicate.head(5)","bef30af1":"# Lets check for null values if any\ntextDF.isnull().values.any()","bad0a121":"# Lets look at one row of the dataset in detail\npd.set_option('display.max_colwidth', -1)\ntextDF.head(1)","63b135f2":"# Lets check the Abbreviations of first 10 rows of the dataset alongwith their labels\nsplit_text = [ t.split(' ') for t in textDF[:10]['TEXT']]\nlabel = [t for t in textDF[:10]['LABEL']]\nlocation = [t for t in textDF[:10]['LOCATION']]","274cd28e":"for i in range(0,10):\n    print(label[i], ' -- ', split_text[i][location[i]])","aeeb3029":"# Checking the unique Abstract_id\ntextDF['ABSTRACT_ID'].nunique()","ac02ccf6":"# Checking the shape of the Dataset\ntextDF.shape","786348c5":"duplicate = textDF[textDF['ABSTRACT_ID'].duplicated(keep = False)]","850110e8":"duplicate.sort_values(by = ['ABSTRACT_ID']).head(5)","7cb97939":"textDF.to_csv('Train\/train.csv', index = False)","9b78bd2f":"# Loading valid.csv\nvalid = pd.read_csv(os.path.join(path, 'Data', 'valid.csv'))\n#Loading test.csv\ntest = pd.read_csv(os.path.join(path, 'Data', 'test.csv'))","0783e65d":"# Check the shape of valid dataset\nvalid.shape","2a1007f8":"valid.drop(valid.index[200000:], inplace = True)\nvalid.shape","fe263452":"#Save this updated Valid.csv to Validation folder\nvalid.to_csv('Validation\/valid.csv', index = False)","0b315f68":"# Check the shape of test dataset\ntest.shape","211c6a9b":"test.drop(test.index[200000:], inplace = True)\ntest.shape","942c1dc1":"#Save this updated test.csv to Test folder\ntest.to_csv('Test\/test.csv', index = False)","89331235":"## Step# 1: Downloading Data from Data Sources.\n\n            Data being used in this problem will be downloaded using Kaggle API from the below location:\n\n            https:\/\/www.kaggle.com\/xhlulu\/medal-emnlp","7c1307b1":"### So, based on the above results for duplicates, it can be seen that a single Text might contain more than 1 Abbreviation at diffrent places. Thus, multiple row for multiple Abbreviations are present.","c064dbd3":"Thus, we don't have any Null values in the dataset.","d4823f3c":"It can be seen here that there are some Abstract_ID's which are not unique. Lets find those abstracts and check what is the main differences","4f9d939b":"From the above analysis, the relationship between Location, Label and Text columns are clearly visible. ","b986b9f5":"Hence, none of the rows are duplicates.","5235e86e":"### Let's save the above trainDF in a csv file inside Train folder for further use.","d1a70f02":"# Abbreviation Disambiguation in Medical Texts - Data Wrangling & EDA\n\nThis Notebook is used to list down:\n\n1. How the Data for Test and Train sets was collected i.e, Data Sources.\n2. How the data has been organized for Training and Testing the model.\n3. Data Cleaning.\n4. Exploratory Data Analysis (EDA).","8f56f27d":"### Once the dataset has been downloaded lets check the directory contents","4de7635c":"## Step# 4: EDA","da501b6c":"Moving the NLP_Dataset.zip file into the Data folder and then unzipping the file","67017627":"## Step# 3: Cleaning Data","b18a5ead":"### As per dataset specifications, location column signifies the word count after which the Abbreviation occurs and its Label is provided in Lable column.","df02f7f9":"Hence, the Abstract_ID's are not all unique. So, lets check the duplicates in the dataset.","97dfca05":"### The Data contains 3 million rows but my System won't be able to work with such a huge dataset hence, will take the first 1 Million rows only for this project.","68162526":"### Lets have a look at our data.","0307e6b8":"    The Data for this problem will have the following Folder structure:\n\n    Work_Dir\n    |\n    |\n    |----> Data\n    |\n    |\n    |----> Train\n    |\n    |\n    |----> Test\n    |\n    |\n    |----> Validation\n    |\n    |\n    |----> Images\n    \n    Thus, creating the required Folder structure.","bb8bf1e8":"### The Data contains 1 million rows so lets reduce this data to 20% of train data i.e, 20k records.","4f38ad71":"Printing the Current Working Directory","698bb3bd":"### At this point the Data Wrangling has been completed and the resulting data is now ready for EDA","9284193c":"## Step# 2: Organizing data into Logical folder structure from where the system will read the data and process it.\n","6ba7fb0c":"Pretrain_subset folder contains the dataset already divided into 3 different files- Train, test and valid. We will use the the data in this folder due to system memory restrictions.","9bf58878":"### Let us again check the number of unique ABSTRACT_ID in Dataset","de689d54":"### Lets load the valid.csv and test.csv as well","c64d3217":"### The Data contains 1 million rows so lets reduce this data to 20% of train data i.e, 20k records.","a63491fa":"Data was successfully extracted and a new folder 'pretrain_subset' was also created, Let's check the contents of that folder."}}