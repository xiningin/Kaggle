{"cell_type":{"6a842807":"code","2c5c5cbc":"code","5a58ea3a":"code","187e784b":"code","9ab829df":"code","ad2cac3f":"code","10bc33e4":"code","8f656d43":"code","fe223b9c":"code","6552f60d":"code","82006595":"code","b27c7b04":"code","b0a41f5c":"code","1feeac13":"code","57c7bd97":"code","6f5a2302":"code","851864a2":"code","f22eb090":"code","706250fc":"code","6e4f810e":"code","475ba271":"code","d4c59fb1":"code","2c812b3a":"code","1b47230b":"code","36ce0183":"code","8613eb50":"code","02d87a5b":"code","6cb11e3a":"code","e78039f8":"code","0f75e941":"code","588f57c7":"code","699b9621":"code","cef6cf8b":"code","c78a1506":"code","436db688":"code","646e99d9":"markdown","ce1f6833":"markdown","b47b5709":"markdown","5a12663a":"markdown","d2bf74c6":"markdown","226d0857":"markdown","6f3cdb44":"markdown","a46aa11d":"markdown","c7716302":"markdown","a0ca1a50":"markdown","b117ada9":"markdown","0b761490":"markdown","c21cff18":"markdown","6a767c4a":"markdown","eaf09ed4":"markdown","7e489de5":"markdown","0004cf78":"markdown","fdec8a1d":"markdown","2c6e0231":"markdown","eff27956":"markdown","4694d2ac":"markdown","d2d87bf4":"markdown","1e260ac8":"markdown","08c154bd":"markdown","e4e5617c":"markdown","e41df4ea":"markdown","bef29d11":"markdown","cf48d317":"markdown","e86ece88":"markdown","39524fbc":"markdown"},"source":{"6a842807":"from warnings import filterwarnings\nfilterwarnings(\"ignore\")","2c5c5cbc":"pip install skompiler","5a58ea3a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mping\nimport seaborn as sns\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster import hierarchy\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import make_blobs,make_moons\nfrom yellowbrick.cluster import KElbowVisualizer\nimport time\nfrom matplotlib.colors import ListedColormap\nfrom skompiler import skompile\nfrom joblib import dump, load","187e784b":"pd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 1000)\npd.set_option('display.width', 1000)","9ab829df":"df = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")","ad2cac3f":"df.head()","10bc33e4":"df.drop(\"Unnamed: 32\",axis=1,inplace=True)\ndf.drop(\"diagnosis\",axis=1,inplace=True)","8f656d43":"df.shape","fe223b9c":"scaler = StandardScaler()","6552f60d":"scaled_X = scaler.fit_transform(df)","82006595":"scaled_X","b27c7b04":"covariance_matrix = np.cov(scaled_X, rowvar=False)","b0a41f5c":"eigen_values, eigen_vectors = np.linalg.eig(covariance_matrix)","1feeac13":"sorted_key = np.argsort(eigen_values)[::-1][:2]","57c7bd97":"eigen_values, eigen_vectors = eigen_values[sorted_key], eigen_vectors[:, sorted_key]","6f5a2302":"principal_components=np.dot(scaled_X,eigen_vectors)","851864a2":"principal_components","f22eb090":"plt.figure(figsize=(8,6))\nplt.scatter(principal_components[:,0],principal_components[:,1])\nplt.xlabel('First principal component')\nplt.ylabel('Second Principal Component')\nplt.grid(False)\nplt.show()","706250fc":"df = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")","6e4f810e":"df.drop(\"Unnamed: 32\",axis=1,inplace=True)\ndf.drop(\"diagnosis\",axis=1,inplace=True)","475ba271":"df.shape","d4c59fb1":"scaler = StandardScaler()","2c812b3a":"scaled_X = scaler.fit_transform(df)","1b47230b":"scaled_X","36ce0183":"principal_components = PCA(n_components=2)\nfitted_principal_components = principal_components.fit_transform(scaled_X)","8613eb50":"component_df = pd.DataFrame(data=fitted_principal_components,columns=[\"First Component\",\"Second Component\"])","02d87a5b":"component_df.head(10)","6cb11e3a":"principal_components.explained_variance_ratio_","e78039f8":"principal_components = PCA(n_components=5)\nfitted_principal_components = principal_components.fit_transform(scaled_X)","0f75e941":"component_df = pd.DataFrame(data=fitted_principal_components,columns=[\"First Component\",\"Second Component\",\n                                                                     \"Third Component\",\"Fourth Component\",\"Fifth Component\"\n                                                                     ])","588f57c7":"component_df.head(10)","699b9621":"principal_components.explained_variance_ratio_","cef6cf8b":"np.sum(principal_components.explained_variance_ratio_)","c78a1506":"principal_components = PCA().fit(scaled_X)","436db688":"plt.plot(np.cumsum(principal_components.explained_variance_ratio_))\nplt.grid(False)\nplt.show()","646e99d9":"In order to see all rows and columns, we will increase max display numbers of dataframe.","ce1f6833":" Dot product of original data and eigen_vectors are the principal component values. This is the \"projection\" step of the original points on to the Principal Component.","b47b5709":"Let's get num_components of Eigen Values and Eigen Vectors.","5a12663a":"For a real world example, we will use *Breast Cancer Wisconsin* dataset. Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. \n\nIt can be downloaded from [here](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+(Diagnostic)).\n\nWe will understand the dataset first.","d2bf74c6":" PCA seeks a small number of dimensions that are as interesting as possible, where the concept of interesting is measured by the amount that the observations vary along each dimension. Each of the dimensions found by PCA is a linear combination of the p features.","226d0857":"Now we will get Eigen Vectors and Eigen Values.","6f3cdb44":"The first principal component of a set of features X1, X2, . . . , Xp is the normalized linear combination of the features\n\n![Screen%20Shot%202021-10-12%20at%2015.08.12.png](attachment:Screen%20Shot%202021-10-12%20at%2015.08.12.png)","a46aa11d":"Let's execute the algorithm.","c7716302":"## Content\n","a0ca1a50":"- **The Elements of  Statistical Learning** - Trevor Hastie,  Robert Tibshirani, Jerome Friedman -  Data Mining, Inference, and Prediction (Springer Series in Statistics) \n\n- **An Introduction to Statistical Learning** - Trevor Hastie,  Robert Tibshirani, Daniela Witten, Gareth James\n\n- [**Understanding Principal Component Analysis**](https:\/\/towardsdatascience.com\/understanding-principal-component-analysis-ddaf350a363a)\n\n- [**Principal Component Analysis for Dimensionality Reduction in Python**](https:\/\/machinelearningmastery.com\/principal-components-analysis-for-dimensionality-reduction-in-python\/)\n\n- [**Principal Component Analysis explained visually**](https:\/\/setosa.io\/ev\/principal-component-analysis\/)","b117ada9":"Principal Component Analysis (PCA) is by far the most popular dimensionality reduction algorithm. First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.\n\n When faced with a large set of correlated variables, principal components allow us to summarize this set with a smaller number of representative variables that collectively explain most of the variability in the original set.\n \n Principal Components Analysis(PCA)refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data. PCA is an unsupervised approach, since it involves only a set of features X1, X2, . . . , Xp, and no associated response Y .\n \n Suppose that we wish to visualize n observations with measurements on a set of p features, X1, X2, . . . , Xp, as part of an exploratory data analysis. We could do this by examining two-dimensional scatterplots of the data, each of which contains the n observations\u2019 measurements on two of the features. If p is large, then it will certainly not be possible to look at all of them; moreover, most likely none of them will be informative since they each contain just a small fraction of the total information present in the data set. In particular, we would like to find a low-dimensional representation of the data that captures as much of the information as possible. For instance, if we can obtain a two-dimensional representation of the data that captures most of the information, then we can plot the observations in this low-dimensional space.\n \n***\n\nThere are two important outcames of PCA:\n\n- It reduces number of dimensions in data.\n\n- It shows which features explain the most variance in the data.","0b761490":"For a real world example, we will use *Breast Cancer Wisconsin* dataset. Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. \n\nIt can be downloaded from [here](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+(Diagnostic)).\n\nWe will understand the dataset first.","c21cff18":"### Theory","6a767c4a":"### Manual Implementation of PCA","eaf09ed4":"Let's try to find optimal component number.","7e489de5":"### Model","0004cf78":"Let's look how much these components explains our data.","fdec8a1d":"As of step 1, we will calculate covariance matrix.","2c6e0231":"**Created by Berkay Alan**\n\n**Principal Components Analysis(PCA)**\n\n**13 October 2021**\n\n**For more Tutorial:** https:\/\/github.com\/berkayalan","eff27956":"## Importing Libraries","4694d2ac":"![image.png](attachment:image.png)\n\n<div class=\"alert alert-block alert-info\">\n<b>Reference 6:<\/b>  Image is taken from *Dataiku Knowledge Base* website.\n<\/div>","d2d87bf4":"First we need to standardize the data.","1e260ac8":"## Resources","08c154bd":"that has the largest variance.\n\n Since we are only interested in variance, we assume that each of the variables in X has been centered to have mean zero (that is, the col- umn means of X are zero). We then look for the linear combination of the sample feature values of the form\n\n![Screen%20Shot%202021-10-12%20at%2021.38.40.png](attachment:Screen%20Shot%202021-10-12%20at%2021.38.40.png)\n\nthat has largest sample variance.\n\n***\n\n**Steps of PCA**\n\n1. After Getting original data, calculate covariance matrix.\n\n2. Calculate EigenVectors.\n\n3. Sort EigenVectors by Eigenvalues.\n\n4. Choose N largest EigenValues.\n\n5. Project Original data onto EigenVectors.","e4e5617c":"Let's look how much these components explains our data.","e41df4ea":"If you want to learn more about Unsupervised Learning - Clustering - Principal Components Analysis(PCA), you can check my [Github Notebook.](https:\/\/github.com\/berkayalan\/Data-Science-Tutorials\/tree\/master\/Unsupervised%20Learning%20-%20Clustering%20-%20Principal%20Components%20Analysis(PCA))","bef29d11":"First we need to standardize the data.","cf48d317":"Now we will sort EigenVectors by Eigenvalues.","e86ece88":"## Principal Components Analysis(PCA) (Theory - Model)","39524fbc":"**Principal Components Analysis(PCA)**\n\n***\n\n- Principal Components Analysis(PCA) (Theory - Manual Implementation of PCA - Model)"}}