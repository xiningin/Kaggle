{"cell_type":{"f4e37ae5":"code","cc56bcfb":"code","f5860de3":"code","eb2f419b":"code","7e914368":"code","e580d141":"code","ac8a6d7e":"code","0b98a118":"code","a7367671":"code","107001ef":"code","94b12d5c":"code","a4740b7d":"code","816ad409":"code","703f1ed6":"code","6b3302f4":"code","a0c3812f":"code","299db04e":"code","518f14b6":"code","fc785189":"code","816feb74":"code","b46713f7":"code","72cb5a18":"code","af074589":"code","688247cf":"code","bd344d3a":"code","436d3f80":"markdown","e7ce28d0":"markdown","5172cb92":"markdown","bd196a00":"markdown","0543cc5f":"markdown","c5fcf457":"markdown","3ef77f77":"markdown","342a2612":"markdown","6876aa45":"markdown","aff602d6":"markdown","fdcb6c95":"markdown","bb151272":"markdown","51a31a3f":"markdown","9f746e99":"markdown","f3b46b17":"markdown","760cfac3":"markdown","8a9007ce":"markdown","df423543":"markdown","b7fb7093":"markdown","aef9346d":"markdown","115ffb08":"markdown","b7389c1f":"markdown","dadab929":"markdown","94642a42":"markdown","74c5b4d4":"markdown"},"source":{"f4e37ae5":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport string\nimport pandas as pd\n%matplotlib inline","cc56bcfb":"mails = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\",encoding = 'latin-1')\nmails.head()","f5860de3":"# drop the null columns namely Unnamed: 2, Unnamed: 3 and Unnamed: 4\n\nmails.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1,inplace=True)\nmails.head()","eb2f419b":"# renaming the columns v1 and v2 as labels and message\n\nmails.rename(columns = {'v1':'labels', 'v2':'message'}, inplace=True)\nmails.head()","7e914368":"# count of labels\n\nmails['labels'].value_counts()","e580d141":"mails.shape","ac8a6d7e":"# now, we will see if our dataset contains duplicates, we will drop the duplicates\n\nmails.drop_duplicates(inplace=True)","0b98a118":"# after droping duplicates let's see the shape of dataset again\n\nmails.shape","a7367671":"# check for any null values in the dataset\n\nmails.isnull().sum()","107001ef":"# mapping the labels as 0 or 1\n# 0 for ham and 1 for spam\n\nmails['label'] = mails['labels'].map({'ham': 0, 'spam': 1})\nmails.head()","94b12d5c":"# now, labels column is of no use so we will drop the labels columns\n\nmails.drop(['labels'], axis=1, inplace=True)\nmails.head()","a4740b7d":"def process_text(text):\n    '''\n    What will be covered:\n    1. Remove punctuation\n    2. Remove stopwords\n    3. Return list of clean text words\n    '''\n    \n    #1\n    #nopunc = [char for char in text if char not in string.punctuation]\n    nopunc = []\n    for char in text:\n        if char not in string.punctuation:\n            nopunc.append(char)\n    nopunc = ''.join(nopunc)\n    \n    #2\n    clean_words = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n    \n    #3\n    return clean_words","816ad409":"#show the tokenization \n\nmails['message'].head().apply(process_text)","703f1ed6":"spam_words = ' '.join(list(mails[mails['label'] == 1]['message']))\nspam_wc = WordCloud(width = 512,height = 512).generate(spam_words)\nplt.figure(figsize = (10, 8), facecolor = 'k')\nplt.imshow(spam_wc)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()","6b3302f4":"ham_words = ' '.join(list(mails[mails['label'] == 0]['message']))\nham_wc = WordCloud(width = 512,height = 512).generate(ham_words)\nplt.figure(figsize = (10, 8), facecolor = 'k')\nplt.imshow(ham_wc)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()","a0c3812f":"# Convert a collection of text documents to a matrix of token counts\n# message_bow stands for bag of words\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nmessage_bow = CountVectorizer(analyzer=process_text).fit_transform(mails['message'])","299db04e":"# split the data into 80% training and 20% testing\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(message_bow, mails['label'], test_size=0.20, random_state=0)","518f14b6":"#shape of message_bow\n\nmessage_bow.shape","fc785189":"# Create and train the naive Bayes classifier\n# The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification)\n\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB().fit(X_train, y_train)","816feb74":"# print the predictions\nprint(classifier.predict(X_train))\n\n# print the actual values\nprint(y_train.values)","b46713f7":"# Evaluate the model on the training data set\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\npred = classifier.predict(X_train)\nprint(classification_report(y_train, pred))\nprint()\nprint('Confusion Matrix:\\n',confusion_matrix(y_train, pred))\nprint()\nprint('Accuracy : ',accuracy_score(y_train, pred))","72cb5a18":"# print the predictions\nprint(classifier.predict(X_test))\n\n# print the actual values\nprint(y_test.values)","af074589":"# Evaluate the model on the testing data set\n\npred = classifier.predict(X_test)\nprint(classification_report(y_test, pred))\nprint()\nprint('Confusion Matrix:\\n',confusion_matrix(y_test, pred))\nprint()\nprint('Accuracy : ',accuracy_score(y_test, pred))","688247cf":"#msg1 = [[\"Congratulations, your entry into our contest last month made you a WINNER! goto our website to claim your price! You have 24 hours to claim.\"]]\n#words = process_text(msg1)\n#features = message_bow(words)\n\n#print(classifier.predict(words))","bd344d3a":"#spam2 = 'Your mobile number has won 1,615,000 million pounds in Apple iPhone UK. For claim email your name, country, occupation.'\n#process_text(spam2)\n#print(classifier.predict(spam2))\n\n#spam3 = 'Our Summer Sale is live from 15th to 17th june! Get 40% off on select products. visit the store now.'\n#process_text(spam3)\n#print(classifier.predict(spam3))\n\n#ham1 = 'Hey there! I am using kaggle'\n#process_text(ham1)\n#print(classifier.predict(ham1))\n\n#ham2 = 'Did you here about the new tv show?'\n#process_text(ham2)\n#print(classifier.predict(ham2))\n\n#ham3 = 'I am free after 11:00 in morning, meet you soon!'\n#process_text(ham3)\n#print(classifier.predict(ham3))","436d3f80":"### Checking for the shape of the data after dropping duplicates<a id=\"9\"><\/a>","e7ce28d0":"## 1. Import libraries<a id=\"1\"><\/a>","5172cb92":"Thus, there is no null value in the data.","bd196a00":"## 9. Model Evaluation<a id = \"19\"><\/a>","0543cc5f":"## 6. Feature Extraction<a id = \"16\"><\/a>\nThe CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.","c5fcf457":"### Renaming columns <a id=\"5\"><\/a>","3ef77f77":"### Counting values in `label`<a id=\"6\"><\/a>","342a2612":"### WordCloud for ham messages<a id='15'><\/a>","6876aa45":"## 3. Exploratory Data Analysis<a id=\"3\"><\/a>","aff602d6":"### Checking shape of the data<a id=\"7\"><\/a>","fdcb6c95":"## 5. Data Visualization<a id = \"13\"><\/a>\n\nNow, we will see frequently used words using WordCloud.","bb151272":"### Dropping `label` column<a id=\"11\"><\/a>","51a31a3f":"# Are you getting spam messages in your inbox???\n<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcTLLx3jilwIExEUyOBnj_pRKGS0E-IEel_jTVE8eSzVgBsYeH1Q\" width=\"500px\" align=\"left\">\n\n","9f746e99":"## 2. Load the data<a id=\"2\"><\/a>","f3b46b17":"### Dropping duplicates<a id=\"8\"><\/a>","760cfac3":"### Mapping `label`<a id=\"10\"><\/a>","8a9007ce":"**What are Spam messages ? **\n\nSpam messages are unsolicited, usually commercial messages (such as e-mails, text messages, or Internet postings) sent to a large number of recipients or posted in a large number of places.\n\n**Problem Statement : ** Accurately classifing spam messages by building a predictive model. \n\n**Solution : ** By doing data preprocessing and predictive analysis using naive bayes, we can achieve the goal of accurately classifing spam messages.\n\n**Agenda :**\n1. [Import libraries](#1)\n2. [Load the data](#2)\n3. [Exploratory Data Analysis](#3)\n    * [Dropping null data](#4)\n    * [Renaming columns](#5)    \n    * [Counting values in `label`](#6)\n    * [Checking shape of the dataset](#7)\n    * [Dropping duplicates](#8)\n    * [Checking shape of the dataset after dropping duplicates](#9)\n    * [Mapping `label`](#10)\n    * [Dropping `label` column](#11)\n4. [Data Preprocessing](#12)\n    * [Removing Punctuations](#12)\n    * [Removing Stopwords](#12)\n5. [Data Visualization](#13)\n    * [WordCloud for spam messages](#14)\n    * [WordCloud for ham messages](#15)\n6. [Feature Extraction](#16)\n7. [Train and Test split](#17)\n8. [Predictive Analysis](#18)\n9. [Model Evaluation](#19)\n10. [Connecting predictions with inputs to the model](#20)","df423543":"## 4. Data Preprocessing<a id = \"12\"><\/a>\nPreprocessing is one of the major steps when we are dealing with any kind of text models. During this stage we have to look at the distribution of our data, what techniques are needed and how deep we should clean.\n\nThis step never has a one hot rule, and totally depends on the problem statement. Few mandatory preprocessing are converting to lowercase, removing punctuation, removing stop words and lemmatization\/stemming. In our problem statement it seems like the basic preprocessing steps will be sufficient.\n\nWe will tokenize or preprocess our dataset using some techniques. Tokenization basically refers to splitting up a larger body of text into smaller lines, words or even creating words.\n> Punctuations are the unnecessary symbols that are in our corpus documents.\n\n> Stop words are the most commonly occurring words which don\u2019t give any additional value to the document vector. in-fact removing these will increase computation and space efficiency.\n","b7fb7093":"## 7. Train and Test split<a id = \"17\"><\/a>","aef9346d":"As we see our dataset had contained 403 duplicates.","115ffb08":"## 10. Connecting predictions with inputs to the model<a id = \"20\"><\/a>","b7389c1f":"### WordCloud for spam messages<a id='14'><\/a>","dadab929":"## 8. Predictive analysis<a id = \"18\"><\/a>","94642a42":"### Conclusion : \nThus, using naive bayes algorithm we can classify ham and spam messages. \nAccuracy on train data = 99% and Accuracy on test data = 95%","74c5b4d4":"### Dropping null data <a id=\"4\"><\/a>"}}