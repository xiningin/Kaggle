{"cell_type":{"56ae90ee":"code","5d91b8c1":"code","e3d023c7":"code","13da03ec":"code","8d6f08b9":"code","adc0f3c9":"code","b03ad32a":"code","2b470507":"code","42da15fd":"code","fed55016":"code","815d1787":"code","80c7aa2d":"code","0603ceaf":"code","1e8a848e":"markdown","57f76385":"markdown","21ea7b82":"markdown","900796de":"markdown","716fe55e":"markdown","2bf6d33a":"markdown","00c7280b":"markdown","6f4a46b5":"markdown","86573e48":"markdown","e024fa39":"markdown","0d862677":"markdown"},"source":{"56ae90ee":"!pip install datasets --no-index --find-links=file:\/\/\/kaggle\/input\/coleridge-packages\/packages\/datasets\n!pip install ..\/input\/coleridge-packages\/seqeval-1.2.2-py3-none-any.whl\n!pip install ..\/input\/coleridge-packages\/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ..\/input\/coleridge-packages\/transformers-4.5.0.dev0-py3-none-any.whl","5d91b8c1":"import os\nimport re\nimport json\nimport time\nimport datetime\nimport random\nimport glob\nimport importlib\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nrandom.seed(123)\nnp.random.seed(456)","e3d023c7":"# copy my_seqeval.py to the working directory because the input directory is non-writable\n!cp \/kaggle\/input\/coleridge-packages\/my_seqeval.py .\/","13da03ec":"MAX_LENGTH = 48 # max no. words for each sentence.\nOVERLAP = 16 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n\nMAX_SAMPLE = None # set a small number for experimentation, set None for production.","8d6f08b9":"train_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train.csv'\npaper_train_folder = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\n\ntrain = pd.read_csv(train_path)\ntrain = train[:MAX_SAMPLE]\nprint(f'No. raw training rows: {len(train)}')","adc0f3c9":"train = train.groupby('Id').agg({\n    'pub_title': 'first',\n    'dataset_title': '|'.join,\n    'dataset_label': '|'.join,\n    'cleaned_label': '|'.join\n}).reset_index()\n\nprint(f'No. grouped training rows: {len(train)}')","b03ad32a":"papers = {}\nfor paper_id in train['Id'].unique():\n    with open(f'{paper_train_folder}\/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper","2b470507":"validation_labels = ['adni', \"alzheimer's disease neuroimaging initiative (adni)\",\n                     'alzheimers disease neuroimaging initiative', 'alzheimer s disease neuroimaging initiative adni ',\n                     \"ADNI|Alzheimer's Disease Neuroimaging Initiative (ADNI)\"\n#                     'rural-urban continuum codes', 'rural urban continuum codes',\n#                     'baccalaureate and beyond', 'baccalaureate and beyond longitudinal study',\n                    ]\n\n# Actual filtering is done below","42da15fd":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\ndef totally_clean_text(txt):\n    txt = clean_text(txt)\n    txt = re.sub(' +', ' ', txt)\n    return txt\n\ndef clean_training_text(txt):\n    \"\"\"\n    similar to the default clean_text function but without lowercasing.\n    \"\"\"\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n\ndef shorten_sentences(sentences):\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > MAX_LENGTH:\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences\n\ndef find_sublist(big_list, small_list):\n    all_positions = []\n    for i in range(len(big_list) - len(small_list) + 1):\n        if small_list == big_list[i:i+len(small_list)]:\n            all_positions.append(i)\n    \n    return all_positions\n\ndef tag_sentence(sentence, labels): # requirement: both sentence and labels are already cleaned\n    sentence_words = sentence.split()\n    \n    if labels is not None and any(re.findall(f'\\\\b{label}\\\\b', sentence)\n                                  for label in labels): # positive sample\n        nes = ['O'] * len(sentence_words)\n        for label in labels:\n            label_words = label.split()\n\n            all_pos = find_sublist(sentence_words, label_words)\n            for pos in all_pos:\n                nes[pos] = 'B'\n                for i in range(pos+1, pos+len(label_words)):\n                    nes[i] = 'I'\n\n        return True, list(zip(sentence_words, nes))\n        \n    else: # negative sample\n        nes = ['O'] * len(sentence_words)\n        return False, list(zip(sentence_words, nes))","fed55016":"cnt_pos, cnt_neg = 0, 0 # number of sentences that contain\/not contain labels\nner_data = []\ncount_valid = 0\n\npbar = tqdm(total=len(train))\nfor i, id, dataset_label, cleaned_label in train[['Id', 'dataset_label', 'cleaned_label']].itertuples():\n    \n    # Initial filter of adni labels\n    if not \"adni\" in dataset_label.lower() and not \"disease neuroimaging\" in dataset_label.lower() and not \"disease neuroimaging\" in cleaned_label.lower() and not \"adni\" in cleaned_label.lower():\n        # paper\n        paper = papers[id]\n        valid_occurrence = False\n        \n        text_1 = '. '.join(section['text'] for section in paper).lower()\n        text_2 = totally_clean_text(text_1)\n        # Second filter of adni occurrences in actual text\n        for valid_label in validation_labels:\n            if valid_label in text_1 or valid_label in text_2:\n                valid_occurrence = True\n                count_valid += 1\n                break\n        \n        if not valid_occurrence:\n            # labels\n            labels = dataset_label.split('|')\n            labels = [clean_training_text(label) for label in labels]\n\n            # sentences\n            sentences = set([clean_training_text(sentence) for section in paper \n                         for sentence in section['text'].split('.') \n                        ])\n            sentences = shorten_sentences(sentences) # make sentences short\n            sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n\n            # positive sample\n            for sentence in sentences:                    \n                is_positive, tags = tag_sentence(sentence, labels)\n                if is_positive:\n                    cnt_pos += 1\n                    ner_data.append(tags)\n                elif any(word in sentence.lower() for word in ['data', 'study']): \n                    ner_data.append(tags)\n                    cnt_neg += 1\n\n            # process bar\n            pbar.update(1)\n            pbar.set_description(f\"Training data size: {cnt_pos} positives + {cnt_neg} negatives\")\n    else:\n        count_valid += 1\n        \nprint(len(train))\nprint(count_valid)\n# shuffling\nrandom.shuffle(ner_data) # 'Alzheimers Disease Neuroimaging Initiative', 'ADNI', 'Alzheimer s Disease Neuroimaging Initiative ADNI'","815d1787":"with open('train_ner.json', 'w') as f:\n    for row in ner_data:\n        words, nes = list(zip(*row))\n        row_json = {'tokens' : words, 'tags' : nes}\n        json.dump(row_json, f)\n        f.write('\\n')","80c7aa2d":"!python ..\/input\/kaggle-ner-utils\/kaggle_run_ner.py \\\n--model_name_or_path \"allenai\/scibert_scivocab_cased\" \\\n--train_file '.\/train_ner.json' \\\n--validation_file '.\/train_ner.json' \\\n--num_train_epochs 1 \\\n--per_device_train_batch_size 16 \\\n--per_device_eval_batch_size 16 \\\n--save_steps 15000 \\\n--output_dir '.\/output' \\\n--report_to 'none' \\\n--seed 123 \\\n--do_train ","0603ceaf":"# Disregard below: output can be saved with quick save through advanced settings in the\n# saving popup screen\n\n# Saving locally to manually make Kaggle dataset (only in case save&commit doesn't work \n# because of GPU memory overload)\n# !zip -r output.zip \".\/output\"\n# from IPython.display import FileLink\n# FileLink(r'output.zip')","1e8a848e":"# Transform data to NER format","57f76385":"Group by publication, training labels should have the same form as expected output.","21ea7b82":"# Fine-tune a BERT model for NER","900796de":"# Import","716fe55e":"# Load data","2bf6d33a":"After the tuning finishes, we should find our model in '.\/output'.","00c7280b":"# Filtering validation labels from training data","6f4a46b5":"# Hyper-parameters","86573e48":"## Install packages","e024fa39":"write data to file.","0d862677":"This notebook shows how to fine-tune a BERT model (from huggingface) for our dataset recognition task.\n\nNote that internet is needed during the training phase (for downloading the bert-base-cased model). Internet can be turned off during prediction."}}