{"cell_type":{"1751cfc9":"code","e511d05f":"code","a50d2715":"code","cfff892b":"code","545571dd":"code","410426e9":"code","5ba692e7":"code","11d1caa4":"code","b8510b52":"code","6824d335":"code","6e29b7ab":"markdown","2374a45e":"markdown","a4a32c2b":"markdown","48a064e3":"markdown","0a4a2129":"markdown","ab22aee4":"markdown"},"source":{"1751cfc9":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics\nimport gc\nimport xgboost as xgb\n\npd.set_option('display.max_columns', 200)","e511d05f":"train_df = pd.read_csv('..\/input\/train.csv', engine='python')\ntest_df = pd.read_csv('..\/input\/test.csv', engine='python')","a50d2715":"import subprocess\nprint((subprocess.check_output(\"lscpu\", shell=True).strip()).decode())","cfff892b":"MAX_TREE_DEPTH = 8\nTREE_METHOD = 'hist'\nITERATIONS = 1000\nSUBSAMPLE = 0.6\nREGULARIZATION = 0.1\nGAMMA = 0.3\nPOS_WEIGHT = 1\nEARLY_STOP = 10\n\nparams = {'tree_method': TREE_METHOD, 'max_depth': MAX_TREE_DEPTH, 'alpha': REGULARIZATION,\n          'gamma': GAMMA, 'subsample': SUBSAMPLE, 'scale_pos_weight': POS_WEIGHT, 'learning_rate': 0.05, \n          'silent': 1, 'objective':'binary:logistic', 'eval_metric': 'auc', 'silent':True, \n          'verbose_eval': False}","545571dd":"%%time\nnfold = 5\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2019)\n\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\n\ntarget = 'target'\npredictors = train_df.columns.values.tolist()[2:]\n\ni = 1\nfor train_index, valid_index in skf.split(train_df, train_df.target.values):\n    print(\"\\nFold {}\".format(i))\n    xg_train = xgb.DMatrix(train_df.iloc[train_index][predictors].values,\n                           train_df.iloc[train_index][target].values,                           \n                           )\n    xg_valid = xgb.DMatrix(train_df.iloc[valid_index][predictors].values,\n                           train_df.iloc[valid_index][target].values,                           \n                           )   \n\n    \n    clf = xgb.train(params, xg_train, ITERATIONS, evals=[(xg_train, \"train\"), (xg_valid, \"eval\")],\n                early_stopping_rounds=EARLY_STOP, verbose_eval=False)\n    oof[valid_index] = clf.predict(xgb.DMatrix(train_df.iloc[valid_index][predictors].values)) \n    \n    predictions += clf.predict(xgb.DMatrix(test_df[predictors].values)) \/ nfold\n    i = i + 1\n\nprint(\"\\n\\nCV AUC: {:<0.2f}\".format(metrics.roc_auc_score(train_df.target.values, oof)))","410426e9":"!nvidia-smi","5ba692e7":"MAX_TREE_DEPTH = 8\nTREE_METHOD = 'gpu_hist'\nITERATIONS = 1000\nSUBSAMPLE = 0.6\nREGULARIZATION = 0.1\nGAMMA = 0.3\nPOS_WEIGHT = 1\nEARLY_STOP = 10\n\nparams = {'tree_method': TREE_METHOD, 'max_depth': MAX_TREE_DEPTH, 'alpha': REGULARIZATION,\n          'gamma': GAMMA, 'subsample': SUBSAMPLE, 'scale_pos_weight': POS_WEIGHT, 'learning_rate': 0.05, \n          'silent': 1, 'objective':'binary:logistic', 'eval_metric': 'auc',\n          'n_gpus': 1}","11d1caa4":"%%time\nnfold = 5\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2019)\n\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\n\ntarget = 'target'\npredictors = train_df.columns.values.tolist()[2:]\n\ni = 1\nfor train_index, valid_index in skf.split(train_df, train_df.target.values):\n    print(\"\\nFold {}\".format(i))\n    xg_train = xgb.DMatrix(train_df.iloc[train_index][predictors].values,\n                           train_df.iloc[train_index][target].values,                           \n                           )\n    xg_valid = xgb.DMatrix(train_df.iloc[valid_index][predictors].values,\n                           train_df.iloc[valid_index][target].values,                           \n                           )   \n\n    \n    clf = xgb.train(params, xg_train, ITERATIONS, evals=[(xg_train, \"train\"), (xg_valid, \"eval\")],\n                early_stopping_rounds=EARLY_STOP, verbose_eval=False)\n    oof[valid_index] = clf.predict(xgb.DMatrix(train_df.iloc[valid_index][predictors].values)) \n    \n    predictions += clf.predict(xgb.DMatrix(test_df[predictors].values)) \/ nfold\n    i = i + 1\n\nprint(\"\\n\\nCV AUC: {:<0.2f}\".format(metrics.roc_auc_score(train_df.target.values, oof)))","b8510b52":"sub_df = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsub_df[\"target\"] = predictions\nsub_df[:10]","6824d335":"sub_df.to_csv(\"xgboost_gpu.csv\", index=False)","6e29b7ab":"# Accelerating XGboost with GPU\n\nThis kernel uses the Xgboost models, running on CPU and GPU. With the GPU acceleration, we gain a ~8.5x performance improvement on an NVIDIA K80 card compared to the 2-core virtual CPU available in the Kaggle VM (1h 8min 46s vs. 8min 20s).\n\nThe gain on a NVIDIA 1080ti card compared to an Intel i7 6900K 16-core CPU is ~6.6x.\n\nTo turn GPU support on in Kaggle, in notebook settings, set the **GPU beta** option to \"GPU on\".\n\n## Notebook  Content\n1. [Loading the data](#0) <br>    \n1. [Training the model on CPU](#1)\n1. [Training the model on GPU](#2)\n1. [Submission](#3)\n","2374a45e":"<a id=\"1\"><\/a> \n## 2. Training the model on CPU","a4a32c2b":"<a id=\"2\"><\/a>\n## 3. Training the model on GPU","48a064e3":"We now train the model with a K80 GPU available in Kaggle. Xgboost provides out of the box support for single GPU training. On a local workstation, a GPU-ready xgboost docker image can be obtained from https:\/\/hub.docker.com\/r\/rapidsai\/rapidsai\/.\n\nAll we need to change is to set: `TREE_METHOD = 'gpu_hist'`","0a4a2129":"<a id=\"3\"><\/a>\n## 4. Submission","ab22aee4":"<a id=\"0\"><\/a>\n## 1. Loading the data"}}