{"cell_type":{"dd874c28":"code","4700e0f4":"code","861498c1":"code","613a7c1c":"code","01eb8134":"code","6fb892cc":"code","72f8134d":"code","6065ed7a":"code","be16f71d":"code","b3156ebe":"code","00c0a195":"code","9c0af341":"code","c52d7ad9":"code","01d1fc79":"code","44bc6b8a":"code","70276ac6":"code","06a8de55":"code","69f9d8ee":"code","a1f7e2b3":"code","f8e6342b":"code","aafb1a8f":"code","d27f56ab":"code","44f1cc96":"code","31853145":"code","761b8e26":"code","67fbfdd7":"code","7a7681e2":"code","bc52da6b":"code","d834e7f1":"code","69c7fafc":"code","77330574":"code","e18515db":"code","a55787b8":"code","9ff57616":"code","48cad9b3":"code","32b9df2d":"code","ec805f0a":"code","802b1cb2":"code","332148d4":"code","b1239785":"code","fac2c9ff":"code","d5fccd23":"code","33813bda":"code","1dfd4444":"code","58b52de8":"code","dc771238":"code","23ac13d8":"code","d27c1e39":"code","cf095bac":"code","f76b7906":"code","adff6f09":"code","71bddf1b":"code","13409790":"code","af180a4a":"markdown","24ba7994":"markdown","9d3f9795":"markdown","324fdc59":"markdown"},"source":{"dd874c28":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.metrics import recall_score,precision_score,f1_score, accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom matplotlib import gridspec\nimport warnings\nwarnings.filterwarnings('ignore')\n","4700e0f4":"card=pd.read_csv('creditcard.csv')\ncard.head()","861498c1":"print(card.shape)","613a7c1c":"print(card.info())","01eb8134":"time = card['Time'].values\nsns.distplot(time)","6fb892cc":"from matplotlib import gridspec\n# distribution of anomalous features\nfeatures = card.iloc[:,0:10].columns\n\nplt.figure(figsize=(12,8*4))\ngs = gridspec.GridSpec(7, 4)\n#gs = gridspec.GridSpec(30, 1)\nfor i, c in enumerate(card[features]):\n ax = plt.subplot(gs[i])\n sns.distplot(card[c][card.Class == 1], bins=30)\n sns.distplot(card[c][card.Class == 0], bins=30)\n ax.set_xlabel('')\n ax.set_title('histogram of feature: ' + str(c))\nplt.show()","72f8134d":"print(\"Class as pie chart:\")\nfig, ax = plt.subplots(1, 1)\nax.pie(card.Class.value_counts(),autopct='%1.1f%%', labels=['Genuine','Fraud'], colors=['red','r'])\nplt.axis('equal')\nplt.ylabel('')","6065ed7a":"print(\"Time variable\")\ncard[\"Time_Hr\"] = card[\"Time\"]\/3600 # convert to hours\nprint(card[\"Time_Hr\"].tail(5))\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex = True, figsize=(6,3))\nax1.hist(card.Time_Hr[card.Class==0],bins=48,color='g',alpha=0.5)\nax1.set_title('Genuine')\nax2.hist(card.Time_Hr[card.Class==1],bins=48,color='r',alpha=0.5)\nax2.set_title('Fraud')\nplt.xlabel('Time (hrs)')\nplt.ylabel('# transactions')","be16f71d":"plt.figure()\nplt.yscale('log')\nsns.set_context({\"figure.figsize\": (10, 8)})\ng = sns.boxplot(data = card, x = 'Class', y = 'Amount')\nplt.title(\"Distribution of Transaction Amount\", fontsize=25)\nplt.xlabel('Class', fontsize=20); plt.xticks(fontsize=15)\nplt.ylabel('Amount', fontsize=20); plt.yticks(fontsize=15)","b3156ebe":"from sklearn.model_selection import train_test_split","00c0a195":"y = card['Class'].values\nx = card.drop(['Class'],axis=1).values\nx_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.2, train_size=0.8, random_state = 42)\n","9c0af341":"def split_data(card, drop_list):\n    card = card.drop(drop_list,axis=1)\n    print(card.columns)","c52d7ad9":"from sklearn.linear_model import LogisticRegression\nClassifier = LogisticRegression(random_state = 0)\nClassifier.fit(x_train, y_train)\nplt.scatter(data = card, x = 'Amount', y = 'Class')\nfig=plt.plot(x,lw=4,c='orange',label='regression line')\nplt.xlabel('Amount',fontsize=20)\nplt.ylabel('Class',fontsize=20)\nplt.show()","01d1fc79":"\nfeature_names = card.iloc[:, 1:30].columns\ntarget = card.iloc[:1, 30: ].columns\nprint(feature_names)\nprint(target)","44bc6b8a":"from sklearn.naive_bayes import GaussianNB\n","70276ac6":"clf = GaussianNB()\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_train)\n","06a8de55":"clf.score(x_test, y_test)","69f9d8ee":"prc= precision_score(y_train, y_pred, average='binary')\nrecall= recall_score(y_train, y_pred, average='binary')","a1f7e2b3":"print(prc)\nprint(recall)","f8e6342b":"f_src= f1_score(y_train, y_pred, average='binary')\nprint(f_src)","aafb1a8f":"acc = accuracy_score(y_train, y_pred )\nprint(acc)","d27f56ab":"conf = confusion_matrix(y_train, y_pred)\n\ncard_conf = pd.DataFrame(conf, index = ['True (positive)', 'True (negative)'])\ncard_conf.columns = ['Predicted (positive)', 'Predicted (negative)']\n\nsns.heatmap(card_conf, annot=True, fmt=\"d\")\n","44f1cc96":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\nclf = clf.fit(x_train, y_train)\ny_pred = clf.predict(x_train)\n","31853145":"from IPython.display import Image\nfrom sklearn.tree import export_graphviz\n","761b8e26":"clf.score(x_test, y_test)","67fbfdd7":"prc = precision_score(y_train, y_pred, average='binary')\nrecall = recall_score(y_train, y_pred, average='binary')\n\nprint(prc)\nprint(recall)","7a7681e2":"f_src= f1_score(y_train, y_pred, average='binary')\nprint(f_src)","bc52da6b":"acc = accuracy_score(y_train, y_pred )\nprint(acc)","d834e7f1":"\nconf = confusion_matrix(y_train, y_pred)\n\ncard_conf = pd.DataFrame(conf, index = ['True (positive)', 'True (negative)'])\ncard_conf.columns = ['Predicted (positive)', 'Predicted (negative)']\n\nsns.heatmap(card_conf, annot=True, fmt=\"d\")\n","69c7fafc":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf.fit(x,y)\ny_pred = clf.predict(x_train)","77330574":"clf.score(x_test, y_test)","e18515db":"prc = precision_score(y_train, y_pred, average='binary')\nrecall = recall_score(y_train, y_pred, average='binary')\n\nprint(prc)\nprint(recall)","a55787b8":"f_src= f1_score(y_train, y_pred, average='binary')\nprint(f_src)","9ff57616":"acc = accuracy_score(y_train, y_pred )\nprint(acc)","48cad9b3":"cm2 = confusion_matrix(y_train,y_pred)\n\ncard_cm2 = pd.DataFrame(cm2, index = ['True(positive)', 'True(negative)'])\ncard_cm2.columns = ['Predicted (positive)', 'Predicted (negative)']\n\nsns.heatmap(card_cm2, annot=True, fmt=\"d\")\n","32b9df2d":"# Correlation matrix\ncorrmat = card.corr()\nfig = plt.figure(figsize = (9, 5))\nsns.heatmap(corrmat, vmax = .8, square = True)\nplt.show()","ec805f0a":"'''from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_curve\n\n#Plot ROC\nfpr, tpr, thresholds = roc_curve(y_test.ravel(), y_pred)\nacc = accuracy_score(fpr,tpr)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% acc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()'''\n\n#not possible becuase of incosistent number of sample and variable.\n#please check it from where you have copied the code.\n#or remove it\n","802b1cb2":"features = np.array(card.columns[:-1])\ny = card['Class'].values\nX = card.drop(['Class'],axis=1).values\nX_train,X_test,y_train,y_test = train_test_split(X, y,  train_size=0.33, random_state = 42)","332148d4":"params_rf = { \n    'n_estimators': [5],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [5, 8, 15, 25, 30, None],\n    'min_samples_leaf' : [0.1, 2, 5, 10],\n    'min_samples_split': [0.1, 2, 5, 10, 15, 60],\n    'random_state': [42]\n }\nparams_lr = {\n    'penalty': ['l1', 'l2'],\n    'C': [0.08, 0.09, 0.1, 0.11, 0.12],\n    'random_state': [78]\n}\n","b1239785":"from sklearn.model_selection import KFold\nkf = KFold(n_splits = 5)","fac2c9ff":"# function to display scores for classifier clf, with training data x_tr, y_tr and test data\n# x_te, y_te\ndef scores(clf, x_tr, y_tr, x_te, y_te):\n    clf.fit(x_tr, y_tr)\n    pred = clf.predict(x_te)\n    print('Precision:',precision_score(y_te, pred))\n    print('Recall:',recall_score(y_te, pred))\n    print('F1:',f1_score(y_te, pred))\n    print('Confusion Matrix (tn, fp, fn, tp):',confusion_matrix(y_te, pred).ravel())","d5fccd23":"def get_metafeatures(clf, x, y, kf):\n    meta_feat = np.zeros((len(x),))\n    for train_index, test_index in kf.split(x, y):\n        X_tr, X_te = x[train_index], x[test_index]\n        y_tr, y_te = y[train_index], y[test_index]\n        clf.fit(X_tr, y_tr)\n        y_pred = clf.predict(X_te)\n        meta_feat[test_index] = y_pred\n    return meta_feat.reshape(-1, 1)","33813bda":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV","1dfd4444":"from sklearn import preprocessing\nfrom sklearn import utils","58b52de8":"clf_rf = RandomForestClassifier()\nrand_rf = RandomizedSearchCV(clf, params_rf, scoring = 'f1', cv=kf) ","dc771238":"rand_rf.fit(X_train, y_train)","23ac13d8":"print('RandomForest Best estimator:')\nprint(rand_rf.best_estimator_)\nprint('RandomForest Best score:')\nprint(rand_rf.best_score_)","d27c1e39":"# best RF classifier after hyperparameter tuning based on F1 score\nb_rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=8, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=2, min_samples_split=5,\n            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n            oob_score=False, random_state=42, verbose=0,\n            warm_start=False) \npred_train_rf = get_metafeatures(b_rf, X_train, y_train, kf) \npred_test_rf = get_metafeatures(b_rf, X_test, y_test, kf)\nscores(b_rf, X_train, y_train, X_test, y_test)","cf095bac":"clf_lr = LogisticRegression()","f76b7906":"rand_lr = RandomizedSearchCV(clf_lr, params_lr, scoring = 'f1', cv=kf)","adff6f09":"rand_lr.fit(X_train, y_train)","71bddf1b":"print('LR Best estimator:')\nprint(rand_lr.best_estimator_)\nprint('LR Best score:')\nprint(rand_lr.best_score_ )","13409790":"# best LR classifier after hyperparameter tuning based on F1 score\nb_lr = LogisticRegression(C=0.11, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l1', random_state=78, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\npred_train_lr = get_metafeatures(b_lr, X_train, y_train, kf)\npred_test_lr = get_metafeatures(b_lr, X_test, y_test, kf)\nscores(b_lr, X_train, y_train, X_test, y_test)","af180a4a":"### Logistic Regression Hyperparameter tuning using RandomizedSearchCV with 5-fold\n","24ba7994":"### RandomForestClassifier Hyperparameter tuning using RandomizedSearchCV with 5-fold","9d3f9795":"## Hyperparameter tuning ","324fdc59":"#### Hypterparameters to tune base models using RandomizedSearchedCV\n"}}