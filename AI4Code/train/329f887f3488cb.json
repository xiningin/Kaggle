{"cell_type":{"4ca86118":"code","f0c8b3a9":"code","54ca097b":"code","4d7b621a":"code","07b11b5e":"code","6c2d51e6":"code","dbffe1b7":"code","16388057":"code","44923515":"code","fa256a3b":"code","75a8ef49":"code","fff45747":"code","5bb92eb6":"code","c03bed00":"code","a5c528b3":"code","a7a54a7b":"code","30e6fe10":"code","bdb030bf":"code","d4e1f234":"code","a52fc73a":"code","f637e314":"code","e06dd968":"code","83f1b52c":"code","872af722":"code","6463644e":"code","68919dc1":"code","f648c461":"markdown","980ed955":"markdown","1ff39553":"markdown","466617c9":"markdown","34425c4d":"markdown","3f53cf37":"markdown","ce4a9177":"markdown","364e2161":"markdown","1c0a4d25":"markdown"},"source":{"4ca86118":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n#from dataprep.eda import plot\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f0c8b3a9":"train_data=pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_data=pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\nsample=pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\n","54ca097b":"train_data.head()","4d7b621a":"train_X=train_data.text.values\ntrain_y=train_data.target.values\ntest_X=test_data.text.values\n\n","07b11b5e":"import re\nREPLACE_BY_SPACE_RE = re.compile('[\/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef preprocess(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n    text = text.lower()# lowercase text\n    text = REPLACE_BY_SPACE_RE.sub('',text)# replace REPLACE_BY_SPACE_RE symbols by space in text\n    text = BAD_SYMBOLS_RE.sub('', text)# delete symbols which are in BAD_SYMBOLS_RE from text\n    text = ' '.join([x for x in text.split() if x and x not in STOPWORDS]) # delete stopwords from text\n    return text\ntrain_X=[preprocess(x) for x in train_X]","6c2d51e6":"train_X,val_X,train_y,val_y=train_test_split(train_X,train_y,shuffle=True)\nprint(train_X[:5])\nprint(\"--------------------------\")\nprint(train_y[:5])","dbffe1b7":"train_X=np.array(train_X)\ntrain_X.shape,train_X.shape","16388057":"vocab_size=1000\ntrun='post'\nmax_length=150\ntokenizer=Tokenizer(num_words=vocab_size,oov_token='<OOV>')\ntokenizer.fit_on_texts(train_X)\nword_index=tokenizer.word_index\nseq=tokenizer.texts_to_sequences(train_X)\npadded=pad_sequences(seq,maxlen=max_length,truncating=trun,padding='post')\n\nval_seq=tokenizer.texts_to_sequences(val_X)\nval_padded=pad_sequences(val_seq,maxlen=max_length,truncating=trun,padding='post')","44923515":"#word_index.items()","fa256a3b":"model_emb=tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size,20,input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D() ,\n    tf.keras.layers.Dense(64,activation='elu'),\n    tf.keras.layers.Dense(128,activation='elu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(264,activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(264,activation='elu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(264,activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(64,activation='relu'),\n    tf.keras.layers.Dense(1,activation='sigmoid')\n])\nmodel.summary()","75a8ef49":"model_emb.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\nhistory=model_emb.fit(padded,train_y,epochs=20,validation_data=(val_padded,val_y))","fff45747":"def want_plot_call_me(history,string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string,'val_'+string])\n    plt.show()\n    \nwant_plot_call_me(history,'accuracy')\nwant_plot_call_me(history,'loss')","5bb92eb6":"def predict_and_sub(testt,model,name):\n    '''\n        This guy will save model into csv.\n        testt: test set\n        model: NN model\n        name: file name\n    '''\n    test=[preprocess(x) for x in testt]\n    test=np.array(test)\n    test_seq=tokenizer.texts_to_sequences(test)\n    test_padded=pad_sequences(test_seq,maxlen=max_length,truncating=trun,padding='post')  \n    pred=model.predict(test_padded).squeeze()\n    pred=[1 if x>0.5 else 0 for x in pred]\n    pred=np.array(pred)\n    sample.target=pred\n    sample.to_csv(name,index=False)\n    ","c03bed00":"predict_and_sub(test_X,model_emb,\"model_emb.csv\")","a5c528b3":"model_cnv=tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size,20,input_length=max_length),\n    tf.keras.layers.Conv1D(64,5,activation='relu'),\n    tf.keras.layers.GlobalAveragePooling1D() ,\n    tf.keras.layers.Dense(64,activation='elu'),\n    tf.keras.layers.Dense(128,activation='elu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(264,activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(264,activation='elu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(264,activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(64,activation='relu'),\n    tf.keras.layers.Dense(1,activation='sigmoid')\n])\nmodel.summary()","a7a54a7b":"model_cnv.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\nhistory=model_cnv.fit(padded,train_y,epochs=20,validation_data=(val_padded,val_y))","30e6fe10":"want_plot_call_me(history,'accuracy')\nwant_plot_call_me(history,'loss')","bdb030bf":"predict_and_sub(test_X,model_cnv,\"model_cnv.csv\")","d4e1f234":"model_lstm=tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size,25,input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(64,activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(64,activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(264,activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(64,activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(64,activation='relu'),\n    tf.keras.layers.Dense(1,activation='sigmoid')\n])\nmodel.summary()","a52fc73a":"model_lstm.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\nhistory=model_lstm.fit(padded,train_y,epochs=20,validation_data=(val_padded,val_y))","f637e314":"want_plot_call_me(history,'accuracy')\nwant_plot_call_me(history,'loss')","e06dd968":"predict_and_sub(test_X,model_lstm,\"model_lstm.csv\")","83f1b52c":"model_gru=tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size,20,input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),\n    tf.keras.layers.Dense(64,activation='elu'),\n    tf.keras.layers.Dense(128,activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(128,activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(264,activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(512,activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(264,activation='elu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(264,activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(64,activation='relu'),\n    tf.keras.layers.Dense(1,activation='sigmoid')\n])\nmodel.summary()","872af722":"model_gru.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\nhistory=model_gru.fit(padded,train_y,epochs=20,validation_data=(val_padded,val_y))","6463644e":"want_plot_call_me(history,'accuracy')\nwant_plot_call_me(history,'loss')","68919dc1":"predict_and_sub(test_X,model_gru,\"model_gru.csv\")","f648c461":"# Embedding network","980ed955":"# Lets do submission part","1ff39553":"## Uncomment below and see majiiicccc ","466617c9":"# dot head() will show dataframe upto five rows, niiiiceeee","34425c4d":"# Turn for CONV1d boi","3f53cf37":"**We have words but our NN can't really understand them , ughh**\n\n> WHYYYYYYYYYY ???, so much drama NN.\n\n**so we do need to tokenize them and convert into sequences**\n\n*I am TENSORFLOW, i got your back..*\n","ce4a9177":"# why not The Great LLLLLSSSSTTTMMMMM","364e2161":"## Ahan, from Dataframe to Numpy array.\n> dot values will have all the credit\n","1c0a4d25":"*Okie, remove all bullshit from text*"}}