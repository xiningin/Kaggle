{"cell_type":{"cf9b9359":"code","17d4394f":"code","e65571b9":"code","b0b212dd":"code","a343b8cd":"code","8fe4e810":"code","f39b13af":"code","d6dad2a9":"code","49f3a90b":"code","46413fe5":"code","78b6b6b0":"code","0daa895a":"code","3935d04c":"code","1c96e1aa":"code","57dd9aaf":"code","f2a8ccc5":"code","52461b6c":"code","6810dfa6":"code","7fdf2322":"code","00ae8afc":"code","a8c63719":"code","d4de2fab":"code","736573c6":"code","64e07831":"code","3ae18597":"code","50d1aef1":"code","5271d3d7":"code","6278173f":"code","73d85005":"code","269be3a1":"code","36c6f846":"code","45b31cea":"code","4f10f583":"code","9ff50b38":"code","c7c5e5f5":"code","24c1b354":"code","09746a61":"code","4aef0d2e":"code","d4d18bd2":"code","5fb433e5":"code","4031a993":"code","5d69ed80":"code","b1b4649e":"code","bd6d2377":"code","7e5de4d6":"code","a52c04a2":"code","61162afc":"code","31534d86":"code","415778a2":"code","6ecd8586":"code","72c04e64":"code","ae5f8ff3":"code","81a4781c":"code","f188dc32":"code","07a4b364":"markdown","08b83d93":"markdown","3efc4053":"markdown","fb8c5b82":"markdown","b2e99ad0":"markdown","fc946656":"markdown","1c96f591":"markdown","484ec3dd":"markdown","b421cbfe":"markdown","3db8c067":"markdown","307aaa6f":"markdown","209b1555":"markdown","d5ddfada":"markdown","2b26a262":"markdown","fd774359":"markdown","9ec99fb7":"markdown","44c09576":"markdown","70f1fdcd":"markdown","65b16dea":"markdown"},"source":{"cf9b9359":"## import libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, RobustScaler, MaxAbsScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","17d4394f":"train = pd.read_csv(\"\/kaggle\/input\/datasetshr\/train.csv\")\nprint(train.shape)\ntrain.head()","e65571b9":"test = pd.read_csv(\"\/kaggle\/input\/datasetshr\/test.csv\")\nprint(test.shape)\ntest.head()","b0b212dd":"df = pd.concat([train,test])\nprint(df.shape)\ndf.head()","a343b8cd":"df.info()","8fe4e810":"## percentage of null values in every field\nNull_per = pd.Series((df.isnull().sum()*100)\/len(df))\nNull_per = round(Null_per.sort_values(ascending=False),2)\nNull_per","f39b13af":"print(df['education'].unique())\nprint(df['previous_year_rating'].unique())","d6dad2a9":"df[df['education'].isnull()]","49f3a90b":"df.groupby(['department','education'])['education'].count().plot(kind='bar')","46413fe5":"#lets remove single quote from values in any field specially education\ndf = df.replace({'\\'': ' '}, regex=True)","78b6b6b0":"## seems bachelor's degree would be apt for missing value in education\ndf['education'] = df['education'].fillna(\"Bachelor s\")\ndf.isnull().sum()","0daa895a":"df[df['previous_year_rating'].isnull()]","3935d04c":"## lets impute missing value in previous_year_rating \ndf['previous_year_rating'] = df['previous_year_rating'].fillna(0)\ndf.isnull().sum()","1c96e1aa":"df.groupby(['is_promoted','previous_year_rating','awards_won?'])['previous_year_rating'].count().plot(kind='bar')","57dd9aaf":"df.groupby(['is_promoted','awards_won?'])['awards_won?'].count().plot(kind='bar')","f2a8ccc5":"df.groupby(['is_promoted','KPIs_met >80%'])['KPIs_met >80%'].count().plot(kind='bar')","52461b6c":"df.groupby(['is_promoted','gender'])['gender'].count().plot(kind='bar')","6810dfa6":"df['gender'] = df['gender'].map({'m':1,'f':0})","7fdf2322":"df.groupby(['is_promoted','no_of_trainings'])['no_of_trainings'].count().plot(kind='bar')","00ae8afc":"df.groupby(['is_promoted','education'])['education'].count().plot(kind='bar')","a8c63719":"df['education'] = df['education'].map({'Bachelor s':0,'Below Secondary':1,'Master s & above':2})","d4de2fab":"df.groupby(['is_promoted','department'])['department'].count().plot(kind='bar')","736573c6":"df['department'] = df['department'].map({'Analytics':0,'Finance':1,'HR':2,'Legal':3,'Operations':4,'Procurement':5,'R&D':6,'Sales & Marketing':7,'Technology':8})","64e07831":"df.groupby(['is_promoted','recruitment_channel'])['recruitment_channel'].count().plot(kind='bar')","3ae18597":"df['recruitment_channel'] = df['recruitment_channel'].map({'other':0,'referred':1,'sourcing':2})","50d1aef1":"df['region'] = df['region'].str.replace(\"region_\",'')","5271d3d7":"## binning of length of service columns\nbins = [0,3,6,10,15,25,70]\nlabels = ['LessThan3','LessThan6','LessThan10','LessThan15','LessThan25','MoreThan25']\ndf['len_of_serv_bins'] = pd.cut(df['length_of_service'],bins=bins,labels=labels)\ndf.head()","6278173f":"df.groupby(['is_promoted','len_of_serv_bins'])['len_of_serv_bins'].count().plot(kind='bar')","73d85005":"## binning of age columns\nbins = [15,25,30,35,45,100]\nlabels = ['LessThan25','LessThan30','LessThan35','LessThan45','MoreThan45']\ndf['age_bins'] = pd.cut(df['age'],bins=bins,labels=labels)\ndf.head()","269be3a1":"df.groupby(['is_promoted','age_bins'])['age_bins'].count().plot(kind='bar')","36c6f846":"df['age_bins'] = df['age_bins'].map({'LessThan25':0,'LessThan30':1,'LessThan35':2,'LessThan45':3,'MoreThan45':5})","45b31cea":"df['len_of_serv_bins'] = df['len_of_serv_bins'].map({'LessThan3':0,'LessThan6':1,'LessThan10':2,'LessThan15':3,'LessThan25':4,'MoreThan25':5})","4f10f583":"df['sum_metric'] = df['awards_won?']+df['KPIs_met >80%'] + df['previous_year_rating']\ndf['tot_score'] = df['avg_training_score'] * df['no_of_trainings']","9ff50b38":"df.drop(['age','length_of_service'],axis=1,inplace=True)","c7c5e5f5":"df.head()\ndf['region'] = df['region'].astype(int)\ndf['len_of_serv_bins'] = df['len_of_serv_bins'].astype(int)\ndf['age_bins'] = df['age_bins'].astype(int)","24c1b354":"train = df[:54808]\ntest = df[54808:]","09746a61":"train.drop(['employee_id'],axis=1,inplace=True)","4aef0d2e":"X = train.drop('is_promoted',axis=1)\ny = train['is_promoted']","d4d18bd2":"X.info()","5fb433e5":"test_c = test.copy()\ntest = test.drop(['is_promoted','employee_id'],axis=1)","4031a993":"# from sklearn.decomposition import PCA\n# pca = PCA(n_components=2)\n# pca.fit(X)\n# X_pca = pca.transform(X)\n# test_pca = pca.transform(test)\n# X=np.column_stack((X,X_pca))\n# test=np.column_stack((test,test_pca))","5d69ed80":"# scale = StandardScaler()\n# X = scale.fit_transform(X)\n# test = scale.transform(test)","b1b4649e":"# #lets print stats after smote\n# print(\"counts of label '1':\",sum(y==1))\n# print(\"counts of label '0':\",sum(y==0))","bd6d2377":"# #perform oversampling using smote\n# import six\n# import sys\n# import joblib\n# sys.modules['sklearn.externals.six'] = six\n# sys.modules['sklearn.externals.joblib'] = joblib\n# from imblearn.over_sampling import SMOTE\n\n\n# sm = SMOTE(random_state=1)\n# X, y = sm.fit_sample(X, y)","7e5de4d6":"# #lets print stats after smote\n# print(\"counts of label '1':\",sum(y==1))\n# print(\"counts of label '0':\",sum(y==0))","a52c04a2":"from sklearn.metrics import f1_score\nrf = RandomForestClassifier(n_estimators=2000,max_depth=20,min_samples_split=5,max_features='sqrt')\nrf.fit(X,y)\ny_pred_rf = rf.predict(test).astype(int)\nprint(\"Accuracy_Score:\",rf.score(X,y))\n#print(\"F1_Score:\",f1_score(y,y_pred_rf, average='weighted'))","61162afc":"#xgb = XGBClassifier(n_estimators=2000,learning_rate=0.1,reg_lambda=0.3,gamma=8,subsample=0.2)\n#xgb = XGBClassifier(learning_rate =0.1, n_estimators=494, max_depth=5,subsample = 0.70, \n#                                                              scale_pos_weight = 2.5,updater =\"grow_histmaker\",base_score  = 0.2,)\nxgb = XGBClassifier(base_score=0.5, gamma=0.3, learning_rate=0.1, max_delta_step=0, max_depth=5,\n                                                            missing=None, n_estimators=494, nthread=15,\n                                                           objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n                                                           scale_pos_weight=2.5,  silent=True, subsample=1)\n\n\nxgb.fit(X,y)\ny_pred_xgb = xgb.predict(test).astype(int)\nprint(\"Accuracy_Score:\",xgb.score(X,y))","31534d86":"#xgb = XGBClassifier(n_estimators=2000,learning_rate=0.1,reg_lambda=0.3,gamma=8,subsample=0.2)\nxgb1 = XGBClassifier(learning_rate =0.1, n_estimators=494, max_depth=5,subsample = 0.70, \n                                                              scale_pos_weight = 2.5,updater =\"grow_histmaker\",base_score  = 0.2)\n\nxgb1.fit(X,y)\ny_pred_xgb1 = xgb1.predict(test).astype(int)\nprint(\"Accuracy_Score:\",xgb1.score(X,y))","415778a2":"#lgb = LGBMClassifier(n_estimators=1000,learning_rate=0.095,reg_lambda=0.4,gamma=10,subsample=0.2)  ### .4911 f1 score highest\n#lgb = LGBMClassifier(n_estimators=1000,learning_rate=0.095,reg_lambda=0.4,gamma=10,subsample=0.2,colsample_bytree = 0.3,min_child_weight=3)   ##.497297 f1 score\nlgb = LGBMClassifier(subsample_freq = 2, objective =\"binary\",importance_type = \"gain\",verbosity = -1, max_bin = 60,num_leaves = 300,boosting_type = 'dart',learning_rate=0.15, n_estimators=494, max_depth=5, scale_pos_weight=2.5) \n\n\nlgb.fit(X,y)\ny_pred_lgb = lgb.predict(test).astype(int)   \nprint(\"Accuracy_Score:\",lgb.score(X,y))","6ecd8586":"#lgb = LGBMClassifier(n_estimators=1000,learning_rate=0.095,reg_lambda=0.4,gamma=10,subsample=0.2)  ### .4911 f1 score highest\n#lgb = LGBMClassifier(n_estimators=1000,learning_rate=0.095,reg_lambda=0.4,gamma=10,subsample=0.2,colsample_bytree = 0.3,min_child_weight=3)   ##.497297 f1 score\nlgb1 = LGBMClassifier(  bagging_fraction=0.9, feature_fraction=0.9, subsample_freq = 2, objective =\"binary\",importance_type = \"gain\",verbosity = -1, max_bin = 60,num_leaves = 300,boosting_type = 'dart',learning_rate=0.15, n_estimators=494, max_depth=5, scale_pos_weight=2.5)\n\n\nlgb1.fit(X,y)\ny_pred_lgb1 = lgb1.predict(test).astype(int)   \nprint(\"Accuracy_Score:\",lgb1.score(X,y))","72c04e64":"#catb = CatBoostClassifier(n_estimators=1000,learning_rate=0.095,random_state=2)\n#catb = CatBoostClassifier(learning_rate=0.15, n_estimators=494, max_depth=5, scale_pos_weight=2.5,random_strength= None)\ncatb = CatBoostClassifier(learning_rate=0.15, n_estimators=494, max_depth=5, scale_pos_weight=2.5,\n                                                random_strength= 0.157)                                                                                       \ncatb.fit(X,y)\ny_pred_catb = catb.predict(test).astype(int)\nprint(\"Accuracy_Score:\",catb.score(X,y))","ae5f8ff3":"#catb = CatBoostClassifier(n_estimators=1000,learning_rate=0.095,random_state=2)\ncatb1 = CatBoostClassifier(learning_rate=0.15, n_estimators=494, max_depth=5, scale_pos_weight=2.5,random_strength= None)\n                                                                              \ncatb1.fit(X,y)\ny_pred_catb1 = catb1.predict(test).astype(int)\nprint(\"Accuracy_Score:\",catb1.score(X,y))","81a4781c":"d=pd.DataFrame()\nd=pd.concat([d,pd.DataFrame(catb1.predict(test).astype(int)),pd.DataFrame(xgb.predict(test).astype(int)),pd.DataFrame(lgb.predict(test).astype(int))],axis=1)\nd.columns=['1','2','3']\n\nre=d.mode(axis=1)[0]\nre.head()","f188dc32":"submission = pd.DataFrame({\n        \"employee_id\": test_c[\"employee_id\"],\n        \"is_promoted\": re             ### prediting using ensemble model\n    })\n\nsubmission.to_csv('HR_submission.csv', index=False)","07a4b364":"### Perform PCA for dimensionality reduction","08b83d93":"### XGBoost-2","3efc4053":"### separate out train and test data now","fb8c5b82":"### LightBoost-2","b2e99ad0":"### Lets train Using Random Forest","fc946656":"#### Exploratory Data Analysis - Univariate, Bivariate, Multivariate Analysis","1c96f591":"#### Loading the train data","484ec3dd":"#### Loading the test data","b421cbfe":"#### Sacling train and test values","3db8c067":"#### Handling class imbalance using smote","307aaa6f":"### LightBoost-1","209b1555":"## HR Analytics - AV\n\nYour client is a large MNC and they have 9 broad verticals across the organisation. One of the problem your client is facing is around identifying the right people for promotion (only for manager position and below) and prepare them in time. Currently the process, they are following is:\n\nThey first identify a set of employees based on recommendations\/ past performance\nSelected employees go through the separate training and evaluation program for each vertical. These programs are based on the required skill of each vertical\nAt the end of the program, based on various factors such as training performance, KPI completion (only employees with KPIs completed greater than 60% are considered) etc., employee gets promotion\nFor above mentioned process, the final promotions are only announced after the evaluation and this leads to delay in transition to their new roles. Hence, company needs your help in identifying the eligible candidates at a particular checkpoint so that they can expedite the entire promotion cycle.\n\nThey have provided multiple attributes around Employee's past and current performance along with demographics. Now, The task is to predict whether a potential promotee at checkpoint in the test set will be promoted or not after the evaluation process.","d5ddfada":"### Feature engineering","2b26a262":"### CATBoost-2","fd774359":"### CATBoost -1","9ec99fb7":"### XGBoost -1","44c09576":"### Lets get the Mode of all Model predictions - Ensemble","70f1fdcd":"#### concate both datasets for further processing - missing value treatment etc","65b16dea":"### Store the prediction "}}