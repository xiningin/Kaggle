{"cell_type":{"dbd6a68f":"code","26b06c06":"code","3978734d":"code","0daaa7e5":"code","e3efcde3":"code","e5b0a73b":"code","ac8d4649":"code","731cdf41":"code","8dc1d225":"code","6ffbceab":"code","b4a1c523":"code","7ee44fff":"code","cf80b426":"code","4a9568ca":"code","9d6cea8f":"code","b50bfe51":"code","3489effd":"code","15a57dd0":"code","124ae76d":"code","c0a6e418":"code","5d9c6ebe":"code","c12a7925":"code","3650fe27":"code","4373e32d":"code","84d2b7c4":"code","a7460843":"code","f94b592e":"code","148d0b2c":"code","47f70200":"code","a0a9a98c":"code","1272da8a":"code","71c7f86f":"code","a098334d":"code","dd9a4404":"code","58bc02f3":"code","f42a745a":"code","4babcebf":"code","c456a4a5":"code","30c9cab6":"code","a12191fe":"code","df8a2250":"code","48ffd11e":"code","ab7916e2":"code","086080d4":"code","969a5b71":"code","fc934e24":"code","89ce105f":"code","35432dcd":"code","9b0600f3":"code","ed919a33":"code","05923d8c":"code","584bf37e":"code","65b09a4e":"code","624bcbca":"code","04c77ede":"code","13b20500":"markdown","5f34b50a":"markdown","df1de9ef":"markdown","28fad33d":"markdown","99706c33":"markdown","4bb23c5f":"markdown","5992c7ee":"markdown","e4488925":"markdown","dc780dd8":"markdown","34e16b61":"markdown","623576db":"markdown","40a6bb38":"markdown","4640dd87":"markdown","85175ea6":"markdown","026ac3be":"markdown","06054dc6":"markdown","83cb2280":"markdown","a67ac225":"markdown","ee0651cf":"markdown","7e85344d":"markdown","936ad5e3":"markdown","4c355284":"markdown","42ade422":"markdown"},"source":{"dbd6a68f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","26b06c06":"from datetime import datetime","3978734d":"df = pd.read_csv('..\/input\/incident-response-log\/incident_event_log.csv')","0daaa7e5":"df.head()","e3efcde3":"df.describe()","e5b0a73b":"df.shape","ac8d4649":"columns = df.columns","731cdf41":"df.replace('?', np.nan, inplace=True)\n","8dc1d225":"df.isna().sum()","6ffbceab":"def basic_data(column):\n    print('nunique: ', column.nunique())\n    print('\\nunique: ', column.unique())\n    print('\\nNull values: ', column.isna().sum())","b4a1c523":"columns_to_change_to_datetime = ['closed_at', 'opened_at' ]\n\nfor i in columns_to_change_to_datetime:\n    df[i] = df[i].astype('datetime64[ns]')","7ee44fff":"df.dtypes","cf80b426":"# life of a ticket in seconds\ndf['ticket_life'] = (df['closed_at'] - df['opened_at']).dt.total_seconds()","4a9568ca":"# tickets opened after closing\n# Filtering out the rows where the opened_at > closed_at\n\nnegative_life = df[df.ticket_life<0 ]\nnegative_life.shape","9d6cea8f":"perc = negative_life.shape[0]\/df.shape[0]*100\nperc","b50bfe51":"negative_life.head()","3489effd":"# tickets with negative life\nnegative_life[negative_life['reopen_count']<1].head()","15a57dd0":"# total tickets with negative life and reopen count = 0\nnegative_life[negative_life['reopen_count']<1].shape","124ae76d":"negative_life[negative_life.reassignment_count > 1].shape","c0a6e418":"df[df.reassignment_count<1].shape","5d9c6ebe":"# df[df.number.sum()>0].shape","c12a7925":"len(df.number)","3650fe27":"df.number.nunique()","4373e32d":"df.groupby(by='number').get_group(df.number[0])","84d2b7c4":"df = df[df.ticket_life>0]","a7460843":"df.shape","f94b592e":"df.sample()","148d0b2c":"print('percentage of missing values'.upper())\ndf.isna().sum()\/df.shape[0]*100","47f70200":"drop = ['caused_by', 'rfc', 'vendor','cmdb_ci','problem_id']\ndf.drop(drop,axis=1,inplace=True)","a0a9a98c":"df.shape","1272da8a":"temp = df[['opened_at', 'sys_created_at']]\ntemp = temp.dropna().astype('datetime64[ns]')","71c7f86f":"temp['gap'] = temp.sys_created_at - temp.opened_at","a098334d":"import datetime\ntemp_mean = temp[temp.gap > datetime.timedelta(hours=0,minutes=0,seconds=0)].gap.mean()\ntemp_mean","dd9a4404":"# df['sys_created_at'].fillna(value=(df.opened_at+temp_mean)).mean()\nfrom datetime import timedelta\n\ndf['sys_created + temp_mean'] = df.opened_at + timedelta(minutes = 9, seconds=52.656746788)","58bc02f3":"df.sys_created_at.fillna(value = df['sys_created + temp_mean'],inplace=True)","f42a745a":"temp2 = df[['opened_at', 'resolved_at']]","4babcebf":"temp2a = temp2.dropna()","c456a4a5":"temp2a.resolved_at = temp2a.resolved_at.astype('datetime64[ns]')","30c9cab6":"temp2a['new'] = temp2a.resolved_at - temp2a.opened_at","a12191fe":"temp2a.new.mean()","df8a2250":"import datetime\ndf['new2'] = df['opened_at'] + datetime.timedelta(days=5,hours = 5, minutes=34, seconds=59.468314879)","48ffd11e":"df['resolved_at'].fillna(value = df.new2,inplace=True)","ab7916e2":"df.isna().sum()","086080d4":"# creating a neww class for missing values  for caller id\ndf['caller_id'].fillna(value='caller 4340',inplace=True)\n\n# creating a neww class for missing values for opened by\ndf['opened_by'].fillna(value = 'Opened by 4341', inplace=True)\n\n# creating a neww class for missing values for sys created by\ndf['sys_created_by'].fillna(value='Created by 4342', inplace=True)\n\n# creating a neww class for missing values for location\ndf['location'].fillna('Location 4343', inplace=True)\n\n# creating a neww class for missing values for category\ndf['category'].fillna('Category 4344',inplace=True)\n\n# creating a neww class for missing values for subcategory\ndf['subcategory'].fillna(value='Subcategory 4345',inplace=True)\n\n# creating a neww class for missing values for symptoms\ndf['u_symptom'].fillna(value='Symptom 4346', inplace=True)\n\n# creating a neww class for missing values for assignment group\ndf['assignment_group'].fillna(value='Group 4347',inplace=True)\n\n# creating a neww class for missing values for resolver\ndf['assigned_to'].fillna(value='Resolver 4348',inplace=True)\n\n# creating a neww class for missing values for problem id\n# df['problem_id'].fillna(value='Promlem ID 4349',inplace=True)","969a5b71":"# replacing with mode\ndf['closed_code'].fillna(df['closed_code'].mode()[0],inplace=True)\ndf['resolved_by'].fillna(df['resolved_by'].mode()[0],inplace=True)","fc934e24":"df.isna().sum()","89ce105f":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle","35432dcd":"columns_to_be_label_encoded = ['active',\n                         'made_sla',\n                         'contact_type',\n                         'knowledge',\n                         'u_priority_confirmation',\n                         'notify',\n                         'incident_state',\n                         'caller_id',\n                         'opened_by',\n                         'sys_created_by',\n                         'sys_updated_by',\n                         'location',\n                         'category',\n                         'subcategory',\n                         'u_symptom',\n                         'assignment_group',\n                         'assigned_to',\n                         'closed_code',\n                         'resolved_by',\n                         'number']","9b0600f3":"for i in columns_to_be_label_encoded:\n    df[i] = le.fit_transform(df[i])\n    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n    print(i.upper(),'\\n', le_name_mapping)\n    print('*'*150)","ed919a33":"df.dtypes","05923d8c":"columns_to_be_ordinal_encoded = ['impact','urgency','priority']\nfrom sklearn.preprocessing import OrdinalEncoder\noe = OrdinalEncoder()\n\nfor i in columns_to_be_ordinal_encoded:\n    df[i] = oe.fit_transform(df[i].values.reshape(-1,1))\n#     le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n#     print(i, le_name_mapping)","584bf37e":"df.dtypes","65b09a4e":"columns_for_obj_to_datetime = ['sys_created_at',\n                              'sys_updated_at',\n                              'resolved_at']","624bcbca":"for i in columns_for_obj_to_datetime:\n    df[i] = df[i].astype('datetime64[ns]')","04c77ede":"df.dtypes","13b20500":"### using ordinal Encoding for the ordinal Data ","5f34b50a":"As we can see that when a ticket is reopened, the open date of the ticket is not necessarily changed. Hence this is not the reson for negative life of the ticket. ","df1de9ef":"### Avg time between opening and resolved time","28fad33d":"#### Adding mean obtained above to the opened time to fill the null values in the sys_created_at","99706c33":"## Fillna","4bb23c5f":"#### Filling null values of a few columns with mode","5992c7ee":"## Encoding the features","e4488925":"## dropping colmns with more than 90% missing values","dc780dd8":"# functions to use later","34e16b61":"### Filling null values in other columns","623576db":"## Changing the datatype for the opening and closing columns","40a6bb38":"#### Adding the mean obtained above to the open time to fill the null values in the resolved at column","4640dd87":"Using Label encoder to encode the Nominal features","85175ea6":"### Avg time b\/w open time and sys_creted_at","026ac3be":" 38.6% of tickets have negative life. We will handle these tickets seperately and not use the same in the model building as this might affect the performance of the model.","06054dc6":"#### Creating new class for the missing columns","83cb2280":"# Genuine Tickets","a67ac225":"## replacing the '?' values with np.nan","ee0651cf":"## Ticekts those are opened after the closing date","7e85344d":"Now, since all the null values are replaced, we can move ahead with encoding the features","936ad5e3":"Now the data is ready for analysis. We perform analysis in the next notebook","4c355284":"## Negative life tickets with reopen count less than 1","42ade422":"## Converting the datatype of the datetime columns"}}