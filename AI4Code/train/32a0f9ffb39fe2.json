{"cell_type":{"32247567":"code","c550bbc7":"code","bbd58a6d":"code","5d473dbd":"code","b7404841":"code","d307db0c":"code","e4ba4de7":"code","62e3eecd":"code","fac15a63":"code","be7d9b98":"code","f28a328b":"code","ff5771ac":"code","ad199350":"code","d049367f":"code","31190ced":"code","93ad20eb":"code","059e70f6":"code","854aa8d6":"code","e5499939":"code","784972cd":"code","e25a50ef":"markdown","47873c3e":"markdown","2370844f":"markdown","fb00318b":"markdown","ec0ca45b":"markdown","a223d789":"markdown","bb3ee4dc":"markdown","e5e5e57f":"markdown","703a8dfb":"markdown","2f6423f2":"markdown","8dafa6ed":"markdown","3f1dffa7":"markdown","ca00f61b":"markdown"},"source":{"32247567":"from sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","c550bbc7":"# import the files\ntrain_data = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest_data = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","bbd58a6d":"# set the train data and the labels\nX = train_data\ny = train_data.label\nX.drop(['label'], axis=1, inplace=True) # drop\nX = X \/ 255 # normalize\nX = X.values.reshape(-1,28,28,1) # reshape 28x28\n\nX_test = test_data \nX_test = X_test \/ 255 # normalize\nX_test = X_test.values.reshape(-1,28,28,1) # reshape","5d473dbd":"img = np.squeeze(X[1])\nplt.imshow(img, cmap='gray')\nplt.show()\nprint(y[1])","b7404841":"# split the train data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=0)","d307db0c":"from tensorflow.keras.utils import to_categorical\n\nX_train_re = X_train.reshape(-1, 784)\nX_valid_re = X_valid.reshape(-1, 784)","e4ba4de7":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\n\nmodel0 = Sequential()\n\nmodel0.add(Dense(units=256, input_shape=(784,)))\nmodel0.add(Activation('relu'))\nmodel0.add(Dense(units=100))\nmodel0.add(Activation('relu'))\nmodel0.add(Dense(units=10))\nmodel0.add(Activation('softmax'))\n\nmodel0.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])","62e3eecd":"model0.summary()","fac15a63":"print(X_train_re.shape)\nprint(y_train.shape)\nprint(X_valid_re.shape)\nprint(y_valid.shape)","be7d9b98":"y_train = to_categorical(y_train, 10)\ny_valid = to_categorical(y_valid, 10)","f28a328b":"model0.fit(X_train_re, y_train, batch_size=1000, epochs=10, verbose=1)","ff5771ac":"model0.evaluate(X_valid_re, y_valid)","ad199350":"# predict and submit\nX_test_re = X_test.reshape(-1, 784)\npreds = model0.predict(X_test_re)\nfinal_preds = np.argmax(preds, axis=1)\n\noutput = pd.DataFrame({'ImageId': range(1,len(X_test)+1), 'Label': final_preds})\noutput.to_csv(\"submission.csv\", index=False)","d049367f":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D","31190ced":"# set up the model\n\nmodel1 = Sequential()\n\nmodel1.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu', input_shape=(28,28,1)))\nmodel1.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu', input_shape=(28,28,1)))\n\nmodel1.add(MaxPool2D(pool_size=(2,2)))\nmodel1.add(Dropout(0.3))\nmodel1.add(Flatten())\nmodel1.add(Dense(256, activation=\"relu\"))\nmodel1.add(Dropout(0.3))\nmodel1.add(Dense(10, activation=\"softmax\"))\n\nmodel1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])","93ad20eb":"model1.summary()","059e70f6":"# split the train data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=0)","854aa8d6":"print(X_train.shape)\nprint(X_valid.shape)\nprint(y_train.shape)\nprint(y_valid.shape)","e5499939":"# fit and evaluate\nmodel1.fit(X_train, y_train, epochs=10)\nmodel1.evaluate(X_valid, y_valid)","784972cd":"# predict and submit\npreds = model1.predict(X_test)\nfinal_preds = np.argmax(preds, axis=1)\n\noutput = pd.DataFrame({'ImageId': range(1,len(X_test)+1), 'Label': final_preds})\noutput.to_csv(\"submission.csv\", index=False)","e25a50ef":"Let's try to take one of the data and output it as an image.","47873c3e":"Converts MNIST data into trainable sequences.","2370844f":"The correct answer rate went up to 99.0%.<br>\nIt can be seen that the computation in the convolutional layer has a higher correct answer rate than the total binding layer alone.<br>\nThis may be due to the fact that we are good at capturing the features of partial arrays, which are features of convolution.","fb00318b":"Case 2.Classification by multi-layered perceptron using convolution","ec0ca45b":"It is divided into training data and evaluation data. <br>The size of the data for evaluation should be 10% of the total.","a223d789":"Load the training data and test data. <br>\nThe `.csv` file is located in the same folder as `.ipynb`.","bb3ee4dc":"We will let them learn.","e5e5e57f":"We change the array so that it is easier to compute in all coupled layers.","703a8dfb":"**Classification of MNIST data by the fully connected layer and convolutional neural network, respectively**\n\nCase 1.Classification by multi-layer perceptron with the fully connected layer\u3000<br>","2f6423f2":"The first thing to do is to load the main library.","8dafa6ed":"It turned out to be about 85% correct.<br>\nFinally, let them learn the data for submission and convert it to a csv file.","3f1dffa7":"The computational model in this case uses the\n1. the fully connected layer\n2. the activation function is a ReLU function\n3. the optimization method is SGD","ca00f61b":"The model is outlined below.<br>\n\n1. two layers of convolution<br>\n2. The activation function is based on the ReLU function.<br>\n3. all bonding layers are two layers<br>\n4. two drop-outs (coefficient 0.3)<br>\n5. the optimization method uses ADAM<br>"}}