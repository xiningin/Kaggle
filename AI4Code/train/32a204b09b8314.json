{"cell_type":{"c8fcd4eb":"code","2edbccfa":"code","6f14205e":"code","82c1966c":"code","97aeaa87":"code","84198d02":"code","8265fa80":"code","9f33f77d":"code","986653cf":"code","428d8338":"code","e4e64866":"code","27123b90":"code","f7d0da7f":"code","f182e490":"code","69b210ec":"code","8a520487":"code","d71bef79":"code","dc7c9cf7":"code","ec9220d1":"code","814461e0":"code","85592ca6":"code","00288883":"code","cb89f793":"code","2fe42a53":"code","f1c83558":"code","c459344d":"code","5384c1e8":"code","39ee905d":"code","026a17c4":"code","36261e93":"code","c4e53c70":"code","160fefba":"code","955ddf5e":"code","14a6c73f":"code","8ece1089":"code","3e110402":"code","2ce1c9dd":"code","1429ebc6":"code","2b67638b":"code","432b9367":"code","40516f2c":"code","f2430bba":"code","abdf0ac5":"code","55483c98":"code","60e12219":"code","80401a3f":"code","e6522847":"code","2231f19a":"code","e2e8b3ab":"code","f5723c14":"code","13ca0904":"code","04f5ffad":"code","803b18eb":"code","c263acab":"code","442ceb2b":"code","b240a9bd":"code","f00e06fb":"code","9f2785ff":"code","f301c1f1":"code","9ccdf873":"code","0c8a9e06":"code","f14ecd36":"code","c9156680":"code","2daa46fc":"code","9e36153d":"code","85d7a53e":"code","125bbec5":"code","c1cce940":"code","dd9226aa":"code","1a671005":"code","d099300d":"code","c3b1eb3c":"code","149635d7":"code","da5ac0f5":"code","39a734e9":"code","1aff2324":"code","aab9fd79":"code","94901f8a":"code","e7c5d2be":"code","5a783735":"code","d63b22f6":"code","834c2f9d":"code","3dbb94a3":"code","b48f8465":"code","85aa2b92":"code","7b34cc16":"code","3d128a0d":"code","56b8f26a":"code","fb8a4987":"code","19b0e2c9":"code","e286f633":"code","888034f7":"code","a20a2c39":"code","a3840a04":"code","904df740":"code","5096edb4":"code","dad0b3c6":"code","94e0e723":"code","47de314b":"code","2f848cc2":"code","08a3aa40":"markdown","bee081be":"markdown","361eab93":"markdown","b8b808c1":"markdown","0a98999e":"markdown","6fa82c1d":"markdown","7a4bbeba":"markdown","feb1b6a6":"markdown","b60de1c0":"markdown","8218045b":"markdown","0ced1a4d":"markdown","1aaab288":"markdown","de7c8dff":"markdown","523c4c3a":"markdown","1700e871":"markdown","5584b398":"markdown","b02c6581":"markdown","2f68dd4c":"markdown","fc1ba36a":"markdown","b83f701b":"markdown","7eab5cf5":"markdown","ecdbb36b":"markdown","e1350bae":"markdown","dbc7ed2e":"markdown","b6ee7cdf":"markdown","465d4556":"markdown","55497158":"markdown","19a33cc9":"markdown","9f6e4a57":"markdown","bbbb01bf":"markdown","cac035f6":"markdown","bf108fcb":"markdown","678702d9":"markdown","c06e3227":"markdown","e0ae7bfc":"markdown","59e1a838":"markdown","f2e9a785":"markdown","39715ce5":"markdown","888485e6":"markdown","6e4cc2a6":"markdown","c534a4f5":"markdown","45cc083b":"markdown","3b265b1e":"markdown","a2f4b123":"markdown","309b5675":"markdown","5fb5f0ed":"markdown","bc91de29":"markdown","76c67221":"markdown","03b4622d":"markdown","ae92544b":"markdown","71202aa8":"markdown","b2867668":"markdown","e1269791":"markdown","68fb7025":"markdown","64037943":"markdown","5ab4fc8d":"markdown","ed20bfe8":"markdown","97bb1e2b":"markdown","eeb6a09e":"markdown","3145dd7e":"markdown","a63a4afd":"markdown","e690c0a0":"markdown","fee3f7b3":"markdown","f334fd44":"markdown","a476efe6":"markdown","d6edb7a6":"markdown","f734be6e":"markdown","713b5e55":"markdown","f990aba5":"markdown","b55813fd":"markdown","4b5cf4c9":"markdown","a4fe26ce":"markdown","9ecff349":"markdown","14c70474":"markdown","d295906f":"markdown","6524167c":"markdown","fbba811a":"markdown","c8c15d39":"markdown","c6b36ea8":"markdown","cc82ff72":"markdown","8d525bbb":"markdown","945e70c8":"markdown","c9ca03ad":"markdown"},"source":{"c8fcd4eb":"# Basic Libraries\nimport numpy as np\nimport pandas as pd\nimport time\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2edbccfa":"# Visualization\nimport matplotlib.pyplot as plt\nplt.style.use(\"fivethirtyeight\")\nimport seaborn as sns\n\n# statiscics\nimport scipy\n\n# Data preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Grid search\nfrom sklearn.model_selection import GridSearchCV\n\n# StratifiedKFold\nfrom sklearn.model_selection import StratifiedKFold\n\n# Learning curve\nfrom sklearn.model_selection import learning_curve\n\n# Validation curve\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_curve, auc\nfrom scipy import interp\n\n# Dimension reduction\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\n# Classification method\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\n\n# Imbalanced data preprocessing\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\n\n# Validation\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score","6f14205e":"# train data\ntrain = pd.read_csv(\"\/kaggle\/input\/aps-failure-at-scania-trucks-data-set\/aps_failure_training_set.csv\")\n# test data\ntest = pd.read_csv(\"\/kaggle\/input\/aps-failure-at-scania-trucks-data-set\/aps_failure_test_set.csv\")","82c1966c":"# datacheck\ntrain.head()","97aeaa87":"# data size\ntrain.shape","84198d02":"# null value\ntrain.isnull().sum().sum()","8265fa80":"# data info\ntrain.dtypes","9f33f77d":"# train data, Reload 'na'as Null value\ntrain = pd.read_csv(\"\/kaggle\/input\/aps-failure-at-scania-trucks-data-set\/aps_failure_training_set.csv\", na_values=\"na\")\n# test data\ntest = pd.read_csv(\"\/kaggle\/input\/aps-failure-at-scania-trucks-data-set\/aps_failure_test_set.csv\", na_values=\"na\")\n\n# or train.replace('nan', np.nan) + type change","986653cf":"# null chack\ncol = train.iloc[:,1:].columns\nnull_ratio = train.iloc[:,1:].isnull().sum().values \/ train.shape[0]*100\n\n# Check by visualization\nplt.figure(figsize=(20,8))\nplt.plot(col, null_ratio)\nplt.xlabel(\"variables\")\nplt.ylabel(\"ratio(%)\")\nplt.xticks(rotation=90, fontsize=10)\nplt.title(\"Null ratio\")\nplt.legend()","428d8338":"# data type check\ntrain.dtypes","e4e64866":"plt.figure(figsize=(10,6))\nsns.countplot(train[\"class\"])\nplt.title(\"neg:{0} \/ pos:{1}\".format(train[\"class\"].value_counts()[0], train[\"class\"].value_counts()[1]))","27123b90":"# define function\ndef class_flg(x):\n    if x[\"class\"] == 'pos':\n        res = 1\n    else:\n        res = 0\n    return res\n\ntrain[\"class\"] = train.apply(class_flg, axis=1)\n# test data\ntest[\"class\"] = test.apply(class_flg, axis=1)","f7d0da7f":"# min and max and mean and std\ncol = train.iloc[:,1:].columns\nmean = train.iloc[:,1:].mean()\nmin_ = train.iloc[:,1:].min()\nmax_ = train.iloc[:,1:].max()\nstd = train.iloc[:,1:].std()","f182e490":"# Visualization by plot\nplt.figure(figsize=(25,8))\n\nplt.plot(col, mean, linewidth=5, color=\"blue\", label='mean') #mean\nplt.fill_between(col, mean+std, mean-std, alpha=0.15, color='green', label='\u00b11\u03c3') # \u00b11\u03c3\nplt.plot(col, min_, linewidth=5, color='blue', linestyle='--', label='max-min') # min\nplt.plot(col, max_, linewidth=5, color='blue', linestyle='--') # max\nplt.xlabel(\"variables\")\nplt.ylabel(\"values\")\nplt.yscale(\"log\")\nplt.xticks(rotation=90, fontsize=10)\nplt.title(\"variables range\")\nplt.legend()","69b210ec":"train_mean = train.groupby(\"class\").mean().T\ntrain_std = train.groupby(\"class\").std().T\n\n# Visualization by plot\nplt.figure(figsize=(25,8))\n\nplt.plot(train_mean.index, train_mean[1], linewidth=5, color=\"red\", label='pos') #mean\nplt.fill_between(train_mean.index, train_mean[1]+train_std[1], train_mean[1]-train_std[1], alpha=0.15, color='orange', label='\u00b11\u03c3') # \u00b11\u03c3\n\nplt.plot(train_mean.index, train_mean[0], linewidth=5, color=\"blue\", label='nag') #mean\nplt.fill_between(train_mean.index, train_mean[0]+train_std[0], train_mean[0]-train_std[0], alpha=0.15, color='green', label='\u00b11\u03c3') # \u00b11\u03c3\n\nplt.xlabel(\"variables\")\nplt.ylabel(\"values\")\nplt.yscale(\"log\")\nplt.xticks(rotation=90, fontsize=10)\nplt.title(\"variables range\")\nplt.legend()","8a520487":"# Null data are tempolary filled mean value.\nmatrix = train.iloc[:,1:].iloc[1:].corr()\nplt.figure(figsize=(15,15))\nsns.heatmap(matrix, vmax=1, vmin=-1, cmap='bwr', square=True, annot=False, center=0, yticklabels=False, xticklabels=False)","d71bef79":"# skew\ncol = train.iloc[:,1:].columns\n# Roop, calculate with drop na values.\nskew = []\nfor i in col:\n    sk = scipy.stats.skew(train[i].dropna())\n    skew.append(sk)","dc7c9cf7":"# kurtosis\n# Roop, calculate with drop na values.\nkurt = []\nfor i in col:\n    ku = scipy.stats.kurtosis(train[i].dropna())\n    kurt.append(ku)","ec9220d1":"# check with graph\nfig, ax = plt.subplots(1, 2, figsize=(20,6))\nsns.distplot(skew, ax=ax[0], kde=False, bins=100)\nax[0].set_xlabel(\"Skewness\")\nax[0].set_ylabel(\"Frequency\")\nax[0].set_title(\"Skewness\")\nsns.distplot(kurt, ax=ax[1], kde=False, bins=100)\nax[1].set_xlabel(\"Kurtosis\")\nax[1].set_ylabel(\"Frequency\")\nax[1].set_title(\"Kurtosis\")","814461e0":"# Data\nlabel = train[\"class\"]\nX = train.iloc[:,1:]\ncol = train.iloc[:,1:].columns\n\n# Scaling\n# Create instance\nsc = StandardScaler()\n# Fitting\nsc.fit(X)\n# Transform\nX_std = sc.fit_transform(X)","85592ca6":"## test data\nX_test_std = sc.fit_transform(test.iloc[1:])\nY_test = test[\"class\"]","00288883":"# Create data frame\ntrain_std = pd.DataFrame(X_std, columns=col)\ntrain_std[\"class\"] = label\ntrain_std.head()","cb89f793":"# Create test data frame\ntest_std = pd.DataFrame(X_test_std, columns=test.iloc[1:].columns)\ntest_std[\"class\"] = Y_test\ntest_std.head()","2fe42a53":"# Null value\nnull_df = pd.DataFrame({\"variables\":train.iloc[:,1:].columns,\n                        \"null_ratio\":null_ratio})\nnull_over15_col = null_df[null_df[\"null_ratio\"]>15][\"variables\"]","f1c83558":"# Result columns\nnull_over15_col","c459344d":"# null ratio df\nnull_ratio_df = pd.DataFrame({\"variables\":train_std[null_over15_col].isnull().sum().index, \n                             \"null_ratio\":train_std[null_over15_col].isnull().sum()\/len(train_std)*100}).sort_values(by=\"null_ratio\", ascending=False)\n\n# over 15% null value df\nnull_df = train_std[null_ratio_df[\"variables\"]]\n\n# distribution check\ncol = null_df.columns\nfig, ax = plt.subplots(4, 7, figsize=(25, 20))\nplt.subplots_adjust(hspace=0.5)\nfor i in range(len(col)):\n    if i <= 6:\n        sns.distplot(null_df[col[i]], ax=ax[0,i], kde=False)\n        ax[0,i].set_title(\"Null ratio(%):\\n{}\".format(null_ratio_df[null_ratio_df[\"variables\"]==col[i]][\"null_ratio\"].values))\n    if i > 6 and i <= 13:\n        sns.distplot(null_df[col[i]], ax=ax[1,i-7], kde=False)\n        ax[1,i-7].set_title(\"Null ratio(%):\\n{}\".format(null_ratio_df[null_ratio_df[\"variables\"]==col[i]][\"null_ratio\"].values))\n    if i > 13 and i <= 20:\n        sns.distplot(null_df[col[i]], ax=ax[2,i-14], kde=False)\n        ax[2,i-14].set_title(\"Null ratio(%):\\n{}\".format(null_ratio_df[null_ratio_df[\"variables\"]==col[i]][\"null_ratio\"].values))\n    if i > 20 and i <= 27:\n        sns.distplot(null_df[col[i]], ax=ax[3,i-21], kde=False)\n        ax[3,i-21].set_title(\"Null ratio(%):\\n{}\".format(null_ratio_df[null_ratio_df[\"variables\"]==col[i]][\"null_ratio\"].values))","5384c1e8":"# Columns of drop\ndrop_col = null_over15_col.values\ndrop_col = np.delete(drop_col, np.where((drop_col == 'bl_000') & (drop_col == 'bk_000')))\n# Checking\ndrop_col","39ee905d":"# Drop over15% null columns, and fill mean values\ntrain_std.drop(drop_col, axis=1, inplace=True)\n\n# Roop fill mean\nfor i in train_std.columns:\n    mean = train_std[i].mean()\n    train_std[i].fillna(mean, inplace=True)\n    \ntrain_std.head()","026a17c4":"# Drop over15% null columns, and fill mean values\ntest_std.drop(drop_col, axis=1, inplace=True)\n\n# Roop fill mean\nfor i in test_std.columns:\n    mean = test_std[i].mean()\n    test_std[i].fillna(mean, inplace=True)\n    \ntest_std.head()","36261e93":"train_std.corr().isnull().sum()","c4e53c70":"# Source of error that occurred during analysis\ntrain_std[\"cd_000\"].var()","160fefba":"train_std.drop(\"cd_000\", axis=1, inplace=True)\n# test data\ntest_std.drop(\"cd_000\", axis=1, inplace=True)","955ddf5e":"# Difine variables\nX = train_std.iloc[:,:-1]\nY = train_std[\"class\"]","14a6c73f":"# Calculation of distiortions\ndistortions = []\nfor i in range(1,20):\n    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=10)\n    km.fit(X)\n    distortions.append(km.inertia_)\n    \n# Plotting \nplt.figure(figsize=(10,6))\nplt.plot(range(1,20), distortions, marker='o')\nplt.xlabel(\"Number of clusters\")\nplt.xticks(range(1,20))\nplt.ylabel(\"Distortion\")","8ece1089":"# Clustering n=8\nkmeans = KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=100, random_state=10)","3e110402":"# Fitting\nkmeans.fit(X)\n# output\ncluster = kmeans.labels_","2ce1c9dd":"# test data fit transform and labels\ncluster_test = kmeans.fit_predict(test_std.iloc[:,:-1])","1429ebc6":"eigen_vals = sorted(np.linalg.eigvals(X.corr()), reverse=True)\n\n# plot\nfig, ax = plt.subplots(2, 1, figsize=(20,10))\nplt.subplots_adjust(hspace=0.4)\nax[0].plot(eigen_vals, 's-')\nax[0].set_xlabel(\"factor\")\nax[0].set_ylabel(\"eigenvalue\")\n\nax[1].plot(eigen_vals, 's-')\nax[1].set_xlabel(\"factor\")\nax[1].set_ylabel(\"eigenvalue\")\nax[1].set_ylim([0,10])\nax[1].set_title(\"Scale up\")","2b67638b":"# Create instance, n=10\npca = PCA(n_components=10)\n\n# Fitting\npca_result = pca.fit_transform(X)\npca_result = pd.DataFrame(pca_result, columns=[\"pca1\",\"pca2\",\"pca3\",\"pca4\",\"pca5\",\"pca6\",\"pca7\",\"pca8\",\"pca9\",\"pca10\"])\n\npca_result.head()","432b9367":"# Visualization by heatmap\nplt.figure(figsize=(20,20))\nsns.heatmap(pca.components_.T, vmax=1, vmin=-1, cmap='bwr', square=False, annot=False, center=0, yticklabels=X.columns, xticklabels=pca_result.columns)\nplt.xlabel(\"pca\")\nplt.ylabel(\"variables\")","40516f2c":"# Visualization by plot\nx = pca_result[\"pca1\"]\ny = pca_result[\"pca2\"]\ncolor = Y\n\nplt.figure(figsize=(10,6))\nplt.scatter(x, y, c=color, alpha=0.5)\nplt.xlabel(\"PCA1\")\nplt.ylabel(\"PCA2\")\nplt.colorbar()","f2430bba":"# create dataframe\npca_result[\"class\"] = Y\n\nfig, ax = plt.subplots(1,2,figsize=(20,6))\nsns.distplot(pca_result[pca_result[\"class\"]==1][\"pca1\"], label=\"pos\", ax=ax[0])\nsns.distplot(pca_result[pca_result[\"class\"]==0][\"pca1\"], label=\"neg\", ax=ax[0])\nax[0].legend()\n\nsns.distplot(pca_result[pca_result[\"class\"]==1][\"pca2\"], label=\"pos\", ax=ax[1])\nsns.distplot(pca_result[pca_result[\"class\"]==0][\"pca2\"], label=\"neg\", ax=ax[1])\nax[1].legend()","abdf0ac5":"# create dataframe\npca_result[\"cluster\"] = cluster\n\n# Visualization by plot\nx = pca_result[\"pca1\"]\ny = pca_result[\"pca2\"]\n\nplt.figure(figsize=(10,6))\nplt.scatter(x, y, c=cluster, alpha=0.5, cmap=\"Set1\")\nplt.xlabel(\"PCA1\")\nplt.ylabel(\"PCA2\")\nplt.colorbar()","55483c98":"# pivot count \npivot = pd.pivot_table(data=pca_result, index=\"class\", columns=\"cluster\", values=\"pca1\", aggfunc=\"count\", fill_value=0)\npivot.columns = [\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\"]\npivot.reset_index()","60e12219":"# Characteristics of each cluster\n# Create dataframe\ncluster_name = [\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\"]\npca_label = [\"pca1\",\"pca2\",\"pca3\",\"pca4\",\"pca5\",\"pca6\",\"pca7\",\"pca8\",\"pca9\",\"pca10\"]\n\ncluster_stats = pd.DataFrame({\"cluster\":range(0,7)})\ncluster_pos_mean = pd.merge(cluster_stats, pca_result[pca_result[\"class\"]==1].groupby(\"cluster\").mean()[pca_label].reset_index(),\n                            left_on=\"cluster\", right_on=\"cluster\", how=\"left\").drop(\"cluster\", axis=1)\ncluster_pos_std = pd.merge(cluster_stats, pca_result[pca_result[\"class\"]==1].groupby(\"cluster\").std()[pca_label].reset_index(),\n                            left_on=\"cluster\", right_on=\"cluster\", how=\"left\").drop(\"cluster\", axis=1)\ncluster_neg_mean = pd.merge(cluster_stats, pca_result[pca_result[\"class\"]==0].groupby(\"cluster\").mean()[pca_label].reset_index(),\n                            left_on=\"cluster\", right_on=\"cluster\", how=\"left\").drop(\"cluster\", axis=1)\ncluster_neg_std = pd.merge(cluster_stats, pca_result[pca_result[\"class\"]==0].groupby(\"cluster\").std()[pca_label].reset_index(),\n                            left_on=\"cluster\", right_on=\"cluster\", how=\"left\").drop(\"cluster\", axis=1)\n\n# Change name of columns\ncluster_pos_mean.index = cluster_name\ncluster_pos_std.index = cluster_name\ncluster_neg_mean.index = cluster_name\ncluster_neg_std.index = cluster_name","80401a3f":"# Visualization\nfig, ax = plt.subplots(2,5,figsize=(20,10))\nplt.subplots_adjust(hspace=0.5, wspace=0.4)\n\nfor i in range(len(cluster_pos_mean.columns)):\n    if i <5:\n        ax[0,i].plot(cluster_pos_mean.index, cluster_pos_mean[pca_label[i]], color=\"red\", label=\"pos\")\n        ax[0,i].fill_between(cluster_pos_mean.index, cluster_pos_mean[pca_label[i]]+cluster_pos_std[pca_label[i]],\n                             cluster_pos_mean[pca_label[i]]-cluster_pos_std[pca_label[i]], color=\"orange\", alpha=0.3, label=\"\u00b11\u03c3\")\n        ax[0,i].plot(cluster_neg_mean.index, cluster_neg_mean[pca_label[i]], color=\"blue\", label=\"neg\")\n        ax[0,i].fill_between(cluster_neg_mean.index, cluster_neg_mean[pca_label[i]]+cluster_neg_std[pca_label[i]],\n                             cluster_neg_mean[pca_label[i]]-cluster_neg_std[pca_label[i]], color=\"green\", alpha=0.3, label=\"\u00b11\u03c3\")\n        ax[0,i].set_title(pca_label[i])\n        ax[0,i].set_xlabel(\"Cluster\")\n        ax[0,i].set_ylabel(\"Standarlized values\")\n        ax[0,i].tick_params(axis='x', labelrotation=90)\n        ax[0,i].legend(ncol=2)\n    else:\n        ax[1,i-5].plot(cluster_pos_mean.index, cluster_pos_mean[pca_label[i]], color=\"red\", label=\"pos\")\n        ax[1,i-5].fill_between(cluster_pos_mean.index, cluster_pos_mean[pca_label[i]]+cluster_pos_std[pca_label[i]],\n                               cluster_pos_mean[pca_label[i]]-cluster_pos_std[pca_label[i]], color=\"orange\", alpha=0.3, label=\"\u00b11\u03c3\")\n        ax[1,i-5].plot(cluster_neg_mean.index, cluster_neg_mean[pca_label[i]], color=\"blue\", label=\"neg\")\n        ax[1,i-5].fill_between(cluster_neg_mean.index, cluster_neg_mean[pca_label[i]]+cluster_neg_std[pca_label[i]],\n                               cluster_neg_mean[pca_label[i]]-cluster_neg_std[pca_label[i]], color=\"green\", alpha=0.3, label=\"\u00b11\u03c3\")\n        ax[1,i-5].set_title(pca_label[i])\n        ax[1,i-5].set_xlabel(\"Cluster\")\n        ax[1,i-5].set_ylabel(\"Standarlized values\")\n        ax[1,i-5].tick_params(axis='x', labelrotation=90)\n        ax[1,i-5].legend(ncol=2)","e6522847":"# Difine variables\nX = train_std.iloc[:,:-1]\nY = train_std[\"class\"]\n\n# Data preprocessing, oversampling method\n# create instance\nros = RandomOverSampler(sampling_strategy=\"auto\", random_state=10)\n\n# train test data split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n\n# Apply to data\nX_resampled, y_resampled = ros.fit_sample(X_train, y_train)","2231f19a":"# Create instance\nforest  = RandomForestClassifier(n_estimators=10, random_state=10)","e2e8b3ab":"# parameters\nparam_range = [10,15,20]\nleaf = [70, 75, 80, 85]\ncriterion = [\"entropy\", \"gini\", \"error\"]\nparam_grid = [{\"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n\n# Optimization by Grid search, scoring is f1 score\ngs = GridSearchCV(estimator=forest, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\ngs = gs.fit(X_resampled, y_resampled)","f5723c14":"print(\"gs best:%.3f\" % gs.best_score_)\nprint(\"gs params:{}\".format(gs.best_params_))","13ca0904":"# Prediction\ngs_best = gs.best_estimator_\n\ny_pred = gs_best.predict(X_test)","04f5ffad":"# Scores\nprint(\"Confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))","803b18eb":"# Create best model of random forest classifier\nforest  = RandomForestClassifier(n_estimators=10, random_state=10, criterion=\"entropy\", max_depth=15, max_leaf_nodes=60)\nforest.fit(X_resampled, y_resampled)\n\nimportance = forest.feature_importances_\n\nindices = np.argsort(importance)[::-1]\n\n# Due to the large number of items, only the top 30 were written.\nfor i in range(30):\n    print(\"%2d) %-*s %f\" %(i+1, 10, X.columns[indices[i]], importance[indices[i]]))","c263acab":"# Visualization with paret0 graph\nforest_importance1 = pd.DataFrame({})\nvariables = []\nfeature_importance1 = []\nfor i in range(len(indices)):\n    col = X.columns[indices[i]]\n    impor = importance[indices[i]]\n    variables.append(col)\n    feature_importance1.append(impor)\nforest_importance1[\"variables\"] = variables\nforest_importance1[\"feature_importance1\"] = feature_importance1\nforest_importance1[\"feature_importance1\"] = forest_importance1[\"feature_importance1\"]*100\nforest_importance1[\"cumsum\"] = forest_importance1[\"feature_importance1\"].cumsum()\n\n\n# Graph\nfig, ax1 = plt.subplots(figsize=(20,8))\nax1.bar(forest_importance1[\"variables\"], forest_importance1[\"feature_importance1\"], label=\"importance\")\nax1.grid()\nax1.set_xlabel(\"variables\")\nax1.tick_params(axis='x', rotation=90, labelsize=10)\nax1.set_ylabel(\"importance(%)\")\nplt.legend(loc='lower left')\nax2 = ax1.twinx()\nax2.plot(forest_importance1[\"variables\"], forest_importance1[\"cumsum\"], color=\"red\", label=\"ratio\")\nax2.set_ylim([0,110])\nax2.set_ylabel(\"Ratio(%)\")\nplt.legend(loc=\"upper right\")","442ceb2b":"# combine cluster label\ntrain_std[\"cluster\"] = cluster\n# Create variable data and label data by removing the cluster data of c1 and c5 (0th and 4th in the label).\n\n# test data\ntest_std[\"cluster\"] = cluster_test","b240a9bd":"# Difine variables\nX = train_std.query(\"cluster!=0 & cluster!=4\").iloc[:,:-2]\nY = train_std.query(\"cluster!=0 & cluster!=4\")[\"class\"]\n\n# train test data split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n\n# Create instance\nforest  = RandomForestClassifier(n_estimators=10, random_state=10)\n\n# parameters\nparam_range = [5, 10,15]\nleaf = [60, 65, 70, 75]\ncriterion = [\"entropy\", \"gini\", \"error\"]\nparam_grid = [{\"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n\n# Optimization by Grid search, scoring is f1\ngs = GridSearchCV(estimator=forest, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\ngs = gs.fit(X_train, y_train)\n\nprint(\"gs best:%.3f\" % gs.best_score_)\nprint(\"gs params:{}\".format(gs.best_params_))","f00e06fb":"# Prediction\ngs_best = gs.best_estimator_\n\ny_pred = gs_best.predict(X_test)\n\n# Scores\nprint(\"Confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))","9f2785ff":"# Create best model of random forest classifier\nforest  = RandomForestClassifier(n_estimators=10, random_state=10, criterion=\"entropy\", max_depth=15, max_leaf_nodes=60)\nforest.fit(X_train, y_train)\n\nimportance = forest.feature_importances_\n\nindices = np.argsort(importance)[::-1]\n\n# Due to the large number of items, only the top 30 were written.\nfor i in range(30):\n    print(\"%2d) %-*s %f\" %(i+1, 10, X.columns[indices[i]], importance[indices[i]]))","f301c1f1":"# Visualization with paret0 graph\nforest_importance2 = pd.DataFrame({})\nvariables = []\nfeature_importance2 = []\nfor i in range(len(indices)):\n    col = X.columns[indices[i]]\n    impor = importance[indices[i]]\n    variables.append(col)\n    feature_importance2.append(impor)\nforest_importance2[\"variables\"] = variables\nforest_importance2[\"feature_importance2\"] = feature_importance2\nforest_importance2[\"feature_importance2\"] = forest_importance2[\"feature_importance2\"]*100\nforest_importance2[\"cumsum\"] = forest_importance2[\"feature_importance2\"].cumsum()\n\n\n# Graph\nfig, ax1 = plt.subplots(figsize=(20,8))\nax1.bar(forest_importance2[\"variables\"], forest_importance2[\"feature_importance2\"], label=\"importance\")\nax1.grid()\nax1.set_xlabel(\"variables\")\nax1.tick_params(axis='x', rotation=90, labelsize=10)\nax1.set_ylabel(\"importance(%)\")\nplt.legend(loc='lower left')\nax2 = ax1.twinx()\nax2.plot(forest_importance2[\"variables\"], forest_importance2[\"cumsum\"], color=\"red\", label=\"ratio\")\nax2.set_ylim([0,110])\nax2.set_ylabel(\"Ratio(%)\")\nplt.legend(loc=\"upper right\")","9ccdf873":"forest_importance = pd.merge(forest_importance1.drop(\"cumsum\", axis=1), forest_importance2.drop(\"cumsum\", axis=1),\n                             left_on=\"variables\", right_on=\"variables\", how='left')\nforest_importance","0c8a9e06":"count = []\nratio = []\nfor i in range(0,11):\n    r = i*0.1\n    c = forest_importance[(forest_importance[\"feature_importance1\"]<=i*0.1) & (forest_importance[\"feature_importance2\"]<=i*0.1)][\"variables\"].count()\n    count.append(c)\n    ratio.append(r)\n    \npd.DataFrame({\"Both ratio(%)<\":ratio,\n              \"count\":count})","f14ecd36":"# Variables\nX = train_std.iloc[:,:-2]\nY = train_std[\"class\"]\n\n# Roop\nThreshold = []\ncol_count = []\naccuracy = []\nprecision = []\nrecall = []\nf1_ = []\n\nfor i in range(0,10):\n    # Create instance, parameters are default.\n    forest  = RandomForestClassifier(n_estimators=10, random_state=10)\n    \n    # threshold\n    thre = \"> 0.\" + str(i) +\"%\"\n\n    col = forest_importance[(forest_importance[\"feature_importance1\"] >= i*0.1) | (forest_importance[\"feature_importance2\"] >= i*0.1)][\"variables\"].values\n    col_c = len(col)\n    # Select data\n    X = train_std[col]\n    y = train_std[\"class\"]\n    # With over sampling\n    # Data preprocessing, oversampling method\n    # create instance\n    ros = RandomOverSampler(sampling_strategy=\"auto\", random_state=10)\n\n    # train test data split\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n\n    # Apply to data\n    X_resampled, y_resampled = ros.fit_sample(X_train, y_train)\n    \n    # Fitting\n    forest.fit(X_resampled, y_resampled)\n    \n    # Prediction\n    y_pred = forest.predict(X_test)\n\n    # Scores\n    acc = accuracy_score(y_true=y_test, y_pred=y_pred).round(3)\n    pre = precision_score(y_true=y_test, y_pred=y_pred).round(3)\n    rec = recall_score(y_true=y_test, y_pred=y_pred).round(3)\n    f1 = f1_score(y_true=y_test, y_pred=y_pred).round(3)\n    \n    # list append\n    Threshold.append(thre)\n    col_count.append(col_c)\n    accuracy.append(acc)\n    precision.append(pre)\n    recall.append(rec)\n    f1_.append(f1)\n\n# create dataframe\npd.DataFrame({\"Threshold\":Threshold,\n              \"Col_count\":col_count,\n             \"accuracy\":accuracy,\n             \"precision\":precision,\n             \"recall\":recall,\n             \"f1_score\":f1_})","c9156680":"# Variables\nX = train_std.iloc[:,:-2]\nY = train_std[\"class\"]\n\n# Roop\nThreshold = []\ncol_count = []\naccuracy = []\nprecision = []\nrecall = []\nf1_ = []\n\nfor i in range(0,10):\n    # Create instance, parameters are default.\n    forest  = RandomForestClassifier(n_estimators=10, random_state=10)\n    \n    # threshold\n    thre = \"> 0.\" + str(i) +\"%\"\n\n    col = forest_importance[(forest_importance[\"feature_importance1\"] >= i*0.1) | (forest_importance[\"feature_importance2\"] >= i*0.1)][\"variables\"].values\n    col_c = len(col)\n    # Select data\n    X = train_std.query(\"cluster!=0 & cluster!=4\")[col]\n    y = train_std.query(\"cluster!=0 & cluster!=4\")[\"class\"]\n\n    # train test data split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n    \n    # Fitting\n    forest.fit(X_train, y_train)\n    \n    # Prediction\n    y_pred = forest.predict(X_test)\n\n    # Scores\n    acc = accuracy_score(y_true=y_test, y_pred=y_pred).round(3)\n    pre = precision_score(y_true=y_test, y_pred=y_pred).round(3)\n    rec = recall_score(y_true=y_test, y_pred=y_pred).round(3)\n    f1 = f1_score(y_true=y_test, y_pred=y_pred).round(3)\n    \n    # list append\n    Threshold.append(thre)\n    col_count.append(col_c)\n    accuracy.append(acc)\n    precision.append(pre)\n    recall.append(rec)\n    f1_.append(f1)\n\n# create dataframe\npd.DataFrame({\"Threshold\":Threshold,\n              \"Col_count\":col_count,\n             \"accuracy\":accuracy,\n             \"precision\":precision,\n             \"recall\":recall,\n             \"f1_score\":f1_})","2daa46fc":"best_col = forest_importance[(forest_importance[\"feature_importance1\"] >= 0.6) | (forest_importance[\"feature_importance2\"] >= 0.6)][\"variables\"].values","9e36153d":"# Difine variables\nX = pca_result.iloc[:,:-2]\nY = pca_result[\"class\"]\n\n# train test data split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n\n# Data preprocessing, oversampling method\n# create instance\nros = RandomOverSampler(sampling_strategy=\"auto\", random_state=10)\n\n# Apply to data\nX_resampled, y_resampled = ros.fit_sample(X_train, y_train)","85d7a53e":"# Create instance\nforest  = RandomForestClassifier(n_estimators=10, random_state=10)\n\n# parameters\nparam_range = [20, 25, 30]\nleaf = [85, 90, 95, 100]\ncriterion = [\"entropy\", \"gini\", \"error\"]\nparam_grid = [{\"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n\n# Optimization by Grid search, scoring is f1\ngs = GridSearchCV(estimator=forest, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\ngs = gs.fit(X_resampled, y_resampled)\n\nprint(\"gs best:%.3f\" % gs.best_score_)\nprint(\"gs params:{}\".format(gs.best_params_))","125bbec5":"# Prediction\ngs_best = gs.best_estimator_\n\ny_pred = gs_best.predict(X_test)\n\n# Scores\nprint(\"Confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))","c1cce940":"# Create best model of random forest classifier\nforest  = RandomForestClassifier(n_estimators=10, random_state=10, criterion=\"entropy\", max_depth=15, max_leaf_nodes=60)\nforest.fit(X_resampled, y_resampled)\n\nimportance = forest.feature_importances_\n\nindices = np.argsort(importance)[::-1]\n\n# Due to the large number of items, only the top 30 were written.\nfor i in range(10):\n    print(\"%2d) %-*s %f\" %(i+1, 10, X.columns[indices[i]], importance[indices[i]]))","dd9226aa":"# Visualization with paret0 graph\nforest_importance = pd.DataFrame({})\nvariables = []\nfeature_importance = []\nfor i in range(len(indices)):\n    col = X.columns[indices[i]]\n    impor = importance[indices[i]]\n    variables.append(col)\n    feature_importance.append(impor)\nforest_importance[\"variables\"] = variables\nforest_importance[\"feature_importance\"] = feature_importance\nforest_importance[\"feature_importance\"] = forest_importance[\"feature_importance\"]*100\nforest_importance[\"cumsum\"] = forest_importance[\"feature_importance\"].cumsum()\n\n\n# Graph\nfig, ax1 = plt.subplots(figsize=(20,8))\nax1.bar(forest_importance[\"variables\"], forest_importance[\"feature_importance\"], label=\"importance\")\nax1.grid()\nax1.set_xlabel(\"variables\")\nax1.tick_params(axis='x', rotation=90, labelsize=10)\nax1.set_ylabel(\"importance(%)\")\nplt.legend(loc='lower left')\nax2 = ax1.twinx()\nax2.plot(forest_importance[\"variables\"], forest_importance[\"cumsum\"], color=\"red\", label=\"ratio\")\nax2.set_ylim([0,110])\nax2.set_ylabel(\"Ratio(%)\")\nplt.legend(loc=\"upper right\")","1a671005":"# Difine variables\nX = train_std.iloc[:,:-2]\nY = train_std[\"class\"]\n\n# Data preprocessing, oversampling method\n# create instance\nrus = RandomUnderSampler(sampling_strategy=\"auto\", random_state=10)\n\n# train test data split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n\n# Apply to data\nX_resampled, y_resampled = rus.fit_sample(X_train, y_train)","d099300d":"# Create instance\nforest  = RandomForestClassifier(n_estimators=10, random_state=10)\n\n# parameters\nparam_range = [5, 10,15,20]\nleaf = [60, 65, 70, 75]\ncriterion = [\"entropy\", \"gini\", \"error\"]\nparam_grid = [{\"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n\n# Optimization by Grid search, scoring is f1 score\ngs = GridSearchCV(estimator=forest, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\ngs = gs.fit(X_resampled, y_resampled)\n\nprint(\"gs best:%.3f\" % gs.best_score_)\nprint(\"gs params:{}\".format(gs.best_params_))","c3b1eb3c":"# Prediction\ngs_best = gs.best_estimator_\n\ny_pred = gs_best.predict(X_test)\n\n# Scores\nprint(\"Confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))","149635d7":"# Difine variables\nX = train_std.iloc[:,:-2]\nY = train_std[\"class\"]\n\n# Data preprocessing, oversampling method\n# create instance\nsmote = SMOTE(sampling_strategy=\"auto\", random_state=10)\n\n# train test data split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n\n# Apply to data\nX_resampled, y_resampled = smote.fit_sample(X_train, y_train)","da5ac0f5":"# Create instance\nforest  = RandomForestClassifier(n_estimators=10, random_state=10)\n\n# parameters\nparam_range = [10,15,20]\nleaf = [80, 85, 90, 95]\ncriterion = [\"entropy\", \"gini\", \"error\"]\nparam_grid = [{\"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n\n# Optimization by Grid search, scoring is f1 score\ngs = GridSearchCV(estimator=forest, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\ngs = gs.fit(X_resampled, y_resampled)\n\nprint(\"gs best:%.3f\" % gs.best_score_)\nprint(\"gs params:{}\".format(gs.best_params_))","39a734e9":"# Prediction\ngs_best = gs.best_estimator_\n\ny_pred = gs_best.predict(X_test)\n\n# Scores\nprint(\"Confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))","1aff2324":"# Difine variables\nX = train_std.iloc[:,:-2]\nY = train_std[\"class\"]\n\n# Data preprocessing, oversampling method\n# create instance\nnem = NearMiss(sampling_strategy=\"auto\")\n\n# train test data split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n\n# Apply to data\nX_resampled, y_resampled = nem.fit_sample(X_train, y_train)","aab9fd79":"# Create instance\nforest  = RandomForestClassifier(n_estimators=10, random_state=10)\n\n# parameters\nparam_range = [5, 10,15,20]\nleaf = [60, 65, 70, 75]\ncriterion = [\"entropy\", \"gini\", \"error\"]\nparam_grid = [{\"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n\n# Optimization by Grid search, scoring is f1 score\ngs = GridSearchCV(estimator=forest, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\ngs = gs.fit(X_resampled, y_resampled)\n\nprint(\"gs best:%.3f\" % gs.best_score_)\nprint(\"gs params:{}\".format(gs.best_params_))","94901f8a":"# Prediction\ngs_best = gs.best_estimator_\n\ny_pred = gs_best.predict(X_test)\n\n# Scores\nprint(\"Confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\nprint(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\nprint(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))","e7c5d2be":"class k_fold_cross_val:\n    def __init__(self, X_train, y_train, estimator, cv):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.estimator = estimator\n        self.cv = cv\n        \n    def cross_val_kfold(self):\n        kfold = StratifiedKFold(n_splits=self.cv, random_state=10)\n        self.kfold = kfold\n        \n        scores = []\n        for train_idx, test_idx in self.kfold.split(self.X_train, self.y_train):\n            self.estimator.fit(self.X_train[train_idx], self.y_train.values[train_idx])\n            score = self.estimator.score(self.X_train[test_idx], self.y_train.values[test_idx])\n            scores.append(score)\n            print(\"Class: %s, Acc: %.3f\" % (np.bincount(self.y_train.values[train_idx]), score))\n            self.scores = scores\n            \n    def score(self):\n        scores = cross_val_score(estimator=self.estimator, X=self.X_train, y=self.y_train, cv=self.cv, n_jobs=1)\n        print(\"CV accuracy scores: %s\" % self.scores)\n        print(\"CV accuracy: %.3f +\/- %.3f\" % (np.mean(self.scores), np.std(self.scores)))\n        \n    def draw_roc_curve(self, X_test, y_test):\n        self.X_test = X_test\n        self.y_test = y_test\n        \n        mean_tpr=0\n        mean_fpr=np.linspace(0,1,100)\n        plt.figure(figsize=(10,6))\n        for train_idx, test_idx in self.kfold.split(self.X_train, self.y_train):\n            proba = self.estimator.fit(self.X_train[train_idx], self.y_train.values[train_idx]).predict_proba(self.X_train[test_idx])\n            fpr, tpr, thresholds = roc_curve(y_true=self.y_train.values[test_idx], y_score=proba[:,1], pos_label=1)\n            mean_tpr += interp(mean_fpr, fpr, tpr)\n            mean_tpr[0] = 0\n            roc_auc = auc(fpr, tpr)\n            plt.plot(fpr, tpr, lw=1, label=\"ROC fold (area=%.2f)\" %(roc_auc))\n        \n        # Line\n        plt.plot([0,1], [0,1], linestyle='--', color=(0.6,0.6,0.6), label=\"random guessing\")\n        # plot mean of fpr, tpr roc_auc\n        mean_tpr \/= self.cv\n        mean_tpr[-1] = 1.0\n        mean_auc = auc(mean_fpr, mean_tpr)\n        plt.plot(mean_fpr, mean_tpr, 'k--', label=\"mean ROC (area = %.2f)\" % mean_auc, color=\"blue\")\n        # Line\n        plt.plot([0,0,1], [0,1,1], lw=2, linestyle=':', color=\"black\", label='perfect performance')\n        plt.xlabel(\"false positive rate\")\n        plt.ylabel(\"true positive rate\")\n        plt.title(\"Receiver Operator Characteristic\")\n        plt.legend()","5a783735":"def draw_learning_curve(estimator, X_train, y_train):\n    # learning curve\n    train_sizes, train_scores, test_scores = learning_curve(estimator=estimator, X=X_train, y=y_train, train_sizes=np.linspace(0.1,1,10), cv=10, n_jobs=1)\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n\n    # plot\n    plt.figure(figsize=(10,6))\n    # train data\n    plt.plot(train_sizes, train_mean, color=\"blue\", marker='o', markersize=5, label='training accuracy')\n    plt.fill_between(train_sizes, train_mean+train_std, train_mean-train_std, color=\"blue\", alpha=0.15)\n    # val data\n    plt.plot(train_sizes, test_mean, color=\"green\", marker='s', linestyle='--', markersize=5, label='validation accuracy')\n    plt.fill_between(train_sizes, test_mean+test_std, test_mean-test_std, color=\"green\", alpha=0.15)\n\n    plt.grid()\n    plt.xlabel(\"Number of trainig samples\")\n    plt.ylabel(\"Accuracy\")\n    plt.ylim([0.8,1.0])\n    plt.title(\"Learning curve\")\n    plt.legend()","d63b22f6":"def draw_validation_curve(estimator, X_train, y_train, param_name, param_range, xscale):\n    # validation curve\n    train_scores, test_scores = validation_curve(estimator=estimator, X=X_train, y=y_train, param_name=param_name, param_range=param_range, cv=10)\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    \n    # plot\n    plt.figure(figsize=(10,6))\n    # train data\n    plt.plot(param_range, train_mean, color=\"blue\", marker='o', markersize=5, label='training accuracy')\n    plt.fill_between(param_range, train_mean+train_std, train_mean-train_std, color=\"blue\", alpha=0.15)\n    # val data\n    plt.plot(param_range, test_mean, color=\"green\", marker='s', linestyle='--', markersize=5, label='validation accuracy')\n    plt.fill_between(param_range, test_mean+test_std, test_mean-test_std, color=\"green\", alpha=0.15)\n\n    plt.grid()\n    plt.xlabel(\"{}\".format(param_name))\n    if xscale==\"log\":\n        plt.xscale(\"log\")\n    else:\n        pass\n    plt.ylabel(\"Accuracy\")\n    plt.ylim([0.8,1.0])\n    plt.title(\"Validation curve\")\n    plt.legend()","834c2f9d":"def confmat_roccurve(X_test, y_test, y_pred, estimator):\n    # create confusion matrix\n    confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n    # visualiazation confusion matrix\n    fig, ax = plt.subplots(1,2,figsize=(18,6))\n    \n    ax[0].matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n    for i in range(confmat.shape[0]):\n        for j in range(confmat.shape[1]):\n            ax[0].text(x=j, y=i, s=confmat[i,j], va=\"center\", ha=\"center\")\n            \n    ax[0].set_xlabel(\"predicted label\")\n    ax[0].set_ylabel(\"true label\")\n    ax[0].set_title(\"confusion matrix\")\n    # Score\n    print(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\n    print(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\n    print(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\n    print(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))\n    \n    # visualization roc curve\n    y_score = estimator.predict_proba(X_test)[:,1]\n    fpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_score)\n    ax[1].plot(fpr, tpr, label=\"roc curve (area = %.3f)\" % auc(fpr, tpr), color=\"blue\")\n    ax[1].plot([0,1], [0,1], linestyle='--', color=(0.6,0.6,0.6), label='random')\n    ax[1].plot([0,0,1], [0,1,1], linestyle=':', color=\"black\", label='perfect performance')\n    ax[1].set_xlabel(\"false positive rate\")\n    ax[1].set_ylabel(\"true positive rate\")\n    ax[1].set_title(\"Receiver Operator Characteristic\")\n    ax[1].legend()","3dbb94a3":"best_col","b48f8465":"# Selected parameters, feature importance threshold >=0.6\ncol = best_col\n# Variables\nX = train_std[col]\ny = train_std[\"class\"]\n\n# train test data split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n\n# Apply over sampling method\nros = RandomOverSampler(sampling_strategy=\"auto\", random_state=10)\nX_resampled, y_resampled = ros.fit_sample(X_train, y_train)\n\n# Study best parameter by Cross validation\n# Instance\nlgb_ = lgb.LGBMClassifier()\n\n# prameters\nmax_depth = [5, 10, 15]\nmin_samples_leaf = [1,3,5,7]\nmin_samples_split = [4,6, 8, 10]\n\nparam_grid = [{\"max_depth\":max_depth,\n               \"min_samples_leaf\":min_samples_leaf, \"min_samples_split\":min_samples_split}]\n\n# Optimization by Grid search\ngs = GridSearchCV(estimator=lgb_, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\ngs = gs.fit(X_resampled, y_resampled)\n\nprint(\"gs best:%.3f\" % gs.best_score_)\nprint(\"gs params:{}\".format(gs.best_params_))","85aa2b92":"# best params\ngs_best_all = gs.best_estimator_\n\n# Cross validation\ncv = k_fold_cross_val(X_resampled.values, y_resampled, gs_best_all, 5)\ncv.cross_val_kfold()","7b34cc16":"# cross val score\ncv.score()","3d128a0d":"# learning curve\ndraw_learning_curve(gs_best_all, X_resampled, y_resampled)","56b8f26a":"# validation curve\ndraw_validation_curve(gs_best_all, X_resampled, y_resampled, \"max_depth\", param_range, \"\")","fb8a4987":"# cv training roc curve\ncv.draw_roc_curve(X_resampled, y_resampled)","19b0e2c9":"# test data prediction\ny_pred_all = gs_best_all.predict(X_test)\n\n# Confusion matrix and ROC curve\nconfmat_roccurve(X_test, y_test, y_pred_all, gs_best_all)","e286f633":"# Selected parameters, feature importance threshold >=0.6\ncol = best_col\n# Variables\nX = train_std.query(\"cluster!=0 & cluster!=4\")[col]\ny = train_std.query(\"cluster!=0 & cluster!=4\")[\"class\"]\n\n# train test data split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n\n# Apply over sampling method\nros = RandomOverSampler(sampling_strategy=\"auto\", random_state=10)\nX_resampled, y_resampled = ros.fit_sample(X_train, y_train)\n\n# Study best parameter by Cross validation\n# Instance\nlgb_ = lgb.LGBMClassifier()\n\n# prameters\nmax_depth = [5, 10, 15]\nmin_samples_leaf = [1,3,5,7]\nmin_samples_split = [4,6, 8, 10]\n\nparam_grid = [{\"max_depth\":max_depth,\n               \"min_samples_leaf\":min_samples_leaf, \"min_samples_split\":min_samples_split}]\n\n# Optimization by Grid search\ngs = GridSearchCV(estimator=lgb_, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\ngs = gs.fit(X_resampled, y_resampled)\n\nprint(\"gs best:%.3f\" % gs.best_score_)\nprint(\"gs params:{}\".format(gs.best_params_))","888034f7":"# best params\ngs_best_cluster = gs.best_estimator_\n\n# Cross validation\ncv = k_fold_cross_val(X_resampled.values, y_resampled, gs_best_cluster, 5)\ncv.cross_val_kfold()","a20a2c39":"# cross val score\ncv.score()","a3840a04":"# learning curve\ndraw_learning_curve(gs_best_cluster, X_resampled, y_resampled)","904df740":"# validation curve\ndraw_validation_curve(gs_best_cluster, X_resampled, y_resampled, \"max_depth\", param_range, \"\")","5096edb4":"# cv training roc curve\ncv.draw_roc_curve(X_resampled, y_resampled)","dad0b3c6":"# test data prediction\ny_pred_cluster = gs_best_cluster.predict(X_test)\n\n# Confusion matrix and ROC curve\nconfmat_roccurve(X_test, y_test, y_pred_cluster, gs_best_cluster)","94e0e723":"# Selected parameters, feature importance threshold >=0.6\ncol = best_col\n# Variables\nX_Test = test_std[col]\ny_Test = test_std[\"class\"]\n\n# test data prediction\ny_Test_pred_all = gs_best_all.predict(X_Test)","47de314b":"# Variables\nX_Test_cluster = test_std.query(\"cluster!=0 & cluster!=4\")[col]\ny_Test_cluster = test_std.query(\"cluster!=0 & cluster!=4\")[\"class\"]\n\n# test data prediction\ny_Test_pred_cluster = gs_best_cluster.predict(X_Test_cluster)","2f848cc2":"# Scores\n# All data prediction\nprint(\"-\"*30, \"all data\", \"-\"*30)\nprint(\"Confusion_matrix = \\n\", confusion_matrix(y_true=y_Test, y_pred=y_Test_pred_all))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_Test, y_pred=y_Test_pred_all))\nprint(\"precision = %.3f\" % precision_score(y_true=y_Test, y_pred=y_Test_pred_all))\nprint(\"recall = %.3f\" % recall_score(y_true=y_Test, y_pred=y_Test_pred_all))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_Test, y_pred=y_Test_pred_all))\n\n# with cluster data prediction\nprint(\"-\"*30, \"cluster data\", \"-\"*30)\nprint(\"Confusion_matrix = \\n\", confusion_matrix(y_true=y_Test_cluster, y_pred=y_Test_pred_cluster))\nprint(\"accuracy = %.3f\" % accuracy_score(y_true=y_Test_cluster, y_pred=y_Test_pred_cluster))\nprint(\"precision = %.3f\" % precision_score(y_true=y_Test_cluster, y_pred=y_Test_pred_cluster))\nprint(\"recall = %.3f\" % recall_score(y_true=y_Test_cluster, y_pred=y_Test_pred_cluster))\nprint(\"f1_score = %.3f\" % f1_score(y_true=y_Test_cluster, y_pred=y_Test_pred_cluster))","08a3aa40":"Many variables object, to analys data distribution etc, need to numerical values.<br>\nThe conjecture is likely to be the letter na, which is included in place of the Null value.","bee081be":"## Under sampling\nConfirm the results of Under sampling method under the same conditions as Oversampling, which deals with all data.","361eab93":"### Scree plot","b8b808c1":"We confirmed the sampling method for imbalanced data under the same conditions. When comparing with f1 score to see the balance between precision and recall, the over sampling method gave the best results for this data. On the other hand, undersampling and its derivative, the Near Miss method, have high recall values but low precision values. This may be due to the negative power of negatives being reduced by reducing the number of negative classes and should be avoided for the present data.","0a98999e":"Next, the results of the classification performance with random forest model when the variables are dropped for each threshold are compared.<br>\nFrom the results of the overall data and the results of the cluster results, the optimum variable may change depending on the importance, so the condition was set to \"or\".","6fa82c1d":"### Some data have zero variance. Eliminate it because it does not affect the analysis result and becomes a computational risk.","7a4bbeba":"Next, the importance of the principal component axis was similarly analyzed using the results of PCA in which the number of variables was reduced by dimension reduction.","feb1b6a6":"From the results of cluster analysis, it is known that the two clusters contain almost no positive label. By removing this cluster, the number of positive and negative classes was brought closer, and a similar analysis was performed.","b60de1c0":"Most of the two principal component axes are overlapped when plotted by class label, but the regions with a large number of positive classes and the regions with a dense and solid negative class are separated.","8218045b":"### Confirming of featrue importance, with over sampling method.","0ced1a4d":"# Classifier model building direction based on the results so far","1aaab288":"Check the distribution of variables numerically<br>\nThe number of variables is 171 which is very large, so first check the distribution of the variables numerically to get the characteristics.\n\nReplace the class labels with numbers for later analysis. 1: positive, 0: negative.","de7c8dff":"Therefore, we first standardized the data and confirmed the distribution, especially for those with a null ratio exceeding 15%. Looking at the distribution, we consider variables for which the data have variance and there is likely to be a sufficient number of samples. <br>\nAs for the other Null values, the ones with the remaining importance in the subsequent importance analysis of variables were adopted, and it was decided to fill them with the average value that has the least statistical influence.","523c4c3a":"# Comparing sampling method\nhttps:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/over_sampling.html<br>\nhttps:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/under_sampling.html<br>","1700e871":"Tempolaly, 'na' change into Null value.<br>\nAfter understanding the whole data, consider the processing of null values.","5584b398":"Next, we analyze the importance of features using a completely different approach. Classification prediction is performed in a random forest, and the importance of the feature is extracted using the information gain that is the feature of the decision tree.","b02c6581":"## Comparison of each prediction result","2f68dd4c":"## SMOTE\nNest, confirming SMOTE method","fc1ba36a":"# Feature importance analysis with using Random forest","b83f701b":"## Parameters","7eab5cf5":"From the analysis results so far, it was found that it is important to build a model using the whole data. In that case, it is necessary to apply the sampling method to the imbalaced data. From now on, we will confirm the effects of some of these methods and consider the best one. The prediction uses a random forest model.\n\nThe confirmed method is as follows\n- Over sampling (Already confirmed)\n- Under sampling\n- SMOTE method\n- NearMiss method","ecdbb36b":"# Libraries","e1350bae":"# NearMiss method\nFinaly, confirming NearMiss method","dbc7ed2e":"### k-means clustering","b6ee7cdf":"About the result of this random forest classification, it was found that about 1\/4 of the variables could explain more than over 90% of the classification, and 2\/3 of the variables could explain almost 100%.","465d4556":"Execution of random forest classifier","55497158":"# System failure classification","19a33cc9":"Those with both importances below the respective thresholds were counted. As a result, there are 13 variables with both importances being 0. Also, 27 variables correspond to the 0.1% threshold and 42 variables correspond to the 0.2% threshold.","9f6e4a57":"The results fit both rows to the training data, but did not successfully detect positives to the test data. The result was that there was a strong tendency for over-learning.<br>\nMy hypothesis and the countermeasure as a cause are<br>\n- Isn't the variable of the test data not contained in the training data in the high-dimensional space and there are many elements outside? Therefore, it is possible that even important variables were reduced when the variables were reduced. \u21d2 Countermeasure: Lower the variable reduction threshold.\n- There are still many variables, and some variables in the training data that have a strong influence are too strong in the test data. \u21d2Countermeasure: Equalize the influence of variables by principal component analysis.\n\n\nFor future trials, I will try the above two.","bbbb01bf":"Based on the analysis results up to this point, we decided to take the following policies regarding the handling of variable data in classification.<br>\n\n(1) Reduce the number of unimportant variables, with threshold feature importance 0.6%. I can expect an effect on PCA as well, but this time I decided to proceed with variable reduction by importance. (Maybe we can expect further improvement by combining them ...)<br>\n(2) Cluster analysis is performed on the reduced variables to create a cluster group in which the positive and negative classes are relatively well balanced.<br>\n(3) Train the predictive model on both the whole data and the refined cluster data and combine the results.For the model that handles the entire data, the sampling method for Imbalanced data is over sampling used.<br>","cac035f6":"## Preprocessing","bf108fcb":"When plotting the average value for each class, some variables have almost the same numerical values, but most of them can be confirmed depending on the size of the values. From this, it can be seen that the classifying property of the class label can be expected. On the other hand, there are quite a few variables.<br>\nIt would be nice to be able to infer each mechanical relationship here, but there is no such information. Therefore, it was decided to reduce the dimensionality and convert the data into a state in which it is easy to understand the characteristics of the data.","678702d9":"# EDA","c06e3227":"Analysis, classification and forecasting notebooks<br>\nClassification method<br>\nhttps:\/\/www.kaggle.com\/urayukitaka\/notebook-classification-method<br>\nRegression method<br>\nhttps:\/\/www.kaggle.com\/urayukitaka\/notebook-regression-method<br>\nDimension reduction method<br>\nhttps:\/\/www.kaggle.com\/urayukitaka\/notebook-dimension-reduction<br>\nStatistical test<br>\nhttps:\/\/www.kaggle.com\/urayukitaka\/notebook-statistical-test-with-avocado-price<br>","e0ae7bfc":"Over sampling method","59e1a838":"As a training of a good practical theme using machine learning,\nIdentifies and predicts the cause of air pressure system failure.","f2e9a785":"Whole data","39715ce5":"### This notebook performs classification system failure by LGBM\n### EDA and data preprocessing are confirmed by variable importance with random forest and clustering and PCA. As a result, apply the method that seems to be good for the training data to the test data and confirm.\n### In addition, for the imbalance of the data, confirm the effect by methods such as oversampling and undersampling.","888485e6":"# Prediction model : I use LGBM classifier","6e4cc2a6":"# Dataloading and basic data checks","c534a4f5":"There is a big problem in classifying. That is, there is a large bias between the positive and negative classes. In such a case, complement using the oversampling method or the undersampling method, and balance each class to create a model. <br>\nThis time, only the oversampling method was used because only the features of the feature quantity are important. Also, for reference, as a result of cluster analysis, clusters to which many negative classes belong can be enclosed. Therefore, we tried to construct a model with data samples excluding the clusters.\n\nFor other methods of imbalanced data, the effect was verified when constructing the prediction model after this importance analysis.","45cc083b":"No null value, but, instead of null, there are filled 'na'.","3b265b1e":"Although the relationship of the feature amount differs for each principal component, the strength of the variables related to the data information can be seen from this figure. It can also be seen that there are variables that are not related to the main components so far.","a2f4b123":"# Detect unimportant features.","309b5675":"It is easier to understand the distribution of each plotted principal component axis. Regarding the pca1 axis, the positive class is biased toward the larger value, and the negative class is clustered around 0. The axis of pca2 is widely distributed from negative values to positive values in the positive class, but is biased near 0 in the negative class.","5fb5f0ed":"It can be seen that the distortion decreases as the number of clusters increases. In particular, it can be seen that the number of clusters is greatly reduced to 4 and then gradually decreases. This time, we decided to proceed with the analysis, assuming that the number of clusters where this curve is asymptotic is up to 8.","bc91de29":"It can be seen that many of the variables have high correlation. If the contents of variables and their technical relationships are clear, variables can be selected, but this time there is no such prior information. Therefore, it is necessary to extract relevant variables using data analysis.","76c67221":"Some variables have very high Null value percentages. How to use this information? Needs consideration.","03b4622d":"As a result of confirming the eigenvalue using the variance value, it is highly possible that even two can explain a considerable part of the entire data, and even if there are 10 variables, the entire data can be expressed with considerable information. I understand.","ae92544b":"Looking at the results using either learning data, it can be seen that reducing the variables does not uniformly reduce the performance. Looking at both results, it was found that the results with the threshold of 0.6% were the best.","71202aa8":"# Feature importance analysis with using Random forest of PCA variables","b2867668":"# Classification prediction model","e1269791":"Next, check the correlation of each variable with a heat map","68fb7025":"### Correlation","64037943":"# Feature importance analysis with using Random forest and clustering result","5ab4fc8d":"## Verification with whole data and clustering result","ed20bfe8":"The results of the principal component analysis showed no significant difference in the classification performance from the analysis results using the entire data. But the important thing is that some information is reduced when selecting the number of principal components. It is very useful to be able to get similar results despite this, because it is easy to understand the whole picture for analysis.<br>\n\nAlthough it is only about 1%, the tendency of over-fitting of test data is smaller than that of training data. This can be seen as the number of variables has decreased and the model has become more robust. This PCA-based method is more likely to be more suitable for actual operation and prediction of unknown data.","97bb1e2b":"> ### Class count check","eeb6a09e":"# Approach for extracting important features","3145dd7e":"As a result of confirming the distribution, it was decided to leave bl_000 and bk_000 because the ratio of null values is 40 to 50% and there is more than half of the data, and the data is also scattered and likely to have information. ..\nOther variables were excluded because the number of data is small and they are biased to almost one.","a63a4afd":"It is not an organized graph, but you can see the size and width of variables. The absolute value of each value can range from fairly large numbers to small ones. Even if you use a logarithmic graph, it has changed significantly, so be careful when handling each scale.","e690c0a0":"# Test data prediction Create ensemble prediction","fee3f7b3":"The results for the training data provide a solid extraction of the positive class. But, precision score was low because the whole data was out of balance. Although this is a problem, it can be evaluated in that the recall value is high and the positive class can be classified for the time being.<br>\nFrom the result of this model, the degree of importance of the feature amount is obtained.","f334fd44":"### 1st whole data prediction","a476efe6":"Both importance count the number of each ratio.","d6edb7a6":"From the results, we can see the improvement of precision value. On the other hand, the recall value has dropped, but the overall score is good. The distribution of the degree of importance of variables has changed from the previous result. The line with more than 90% has almost half the number of variables, and it is asymptotic to 100% around 4\/5. In other words, variables that gave results that were not so important with overall training became more important in the cluster with a high concentration of the positive class, and became variables necessary for classification. It was possible to improve the classification performance by combining cluster analysis.<br>\nAlso, if there are variables that are found to be insignificant in both the overall result and this result, it is likely that they can be excluded from the model if their contents are confirmed and a logical explanation is given.","f734be6e":"Distributions of data were confirmed. First, it was found that there were only skewness values greater than 0, and the distribution had long right tails and was biased to the left. It was also confirmed that all the values were very large and the bias was strong.\nIt can be seen that there are many kurtosis near 0 and not a sharp distribution, but some kurtosis has data with a large sharp distribution.","713b5e55":"### Difference by label","f990aba5":"### 2nd with cluster analysis data prediction","b55813fd":"Next, when the results of cluster analysis were plotted, it was confirmed that the clusters could be divided even on the principal component axis. <br>\nIs this cluster dividing the positive and negative classes for the class label? confirmed. As a result, cluster 1 and cluster 5 can form clusters to which a considerable number of negative classes belong. This is the sharp point on the left side of the plot graph. (Note: yellow and red areas in the plot)","4b5cf4c9":"## Direction\nIn order to search for important features from such a large number of variables, analysis was performed using the following three approaches. <br>\n1) PCA and cluster analysis<br>\n2) Importance check by random forest classifier<br>\n\n### From these results, we decided to make a comprehensive judgment and decide the important features.","a4fe26ce":"# Data prepcocessing","9ecff349":"Check with cluster numbers, by elbow method.","14c70474":"## Difine class and function, k-fold cross validation, learning and validation curve, confusion matrix, roc auc curve","d295906f":"In order to determine the number of principal components, we confirmed the eigenvalue decay of the variance by scree plot.","6524167c":"## Verification with whole data","fbba811a":"### Skerness and kurtosis","c8c15d39":"### Understanding distribution and relationship of each data","c6b36ea8":"## PCA","cc82ff72":"As a result of confirming each principal component other than pca1 and pca2, the characteristics of the principal component are classified for each cluster. The points of interest are clusters 1 and 5, which mostly belonged to the negative class earlier, but by looking at each axis, it can be seen that other clusters can have similar values. This means that it is not possible to make a judgment with only one variable, suggesting that it is necessary to make a positive \/ negative judgment that includes the relationship between each variable. <br>\n- On the other hand, clusters 4, 6 and 7 are clusters to which a large number of positive classes belong, and it can be confirmed that their values are also characteristic. In fact, understanding the characteristics of these clusters will help analyze the mechanical mechanisms.","8d525bbb":"Next, I also confirmed how other variables differed in the cluster..<br>\nHowever, cluster 8 is small in number and is an outlier, so it is excluded.","945e70c8":"cluster data","c9ca03ad":"Data set<br>\nhttps:\/\/www.kaggle.com\/uciml\/aps-failure-at-scania-trucks-data-set<br>\n\nContent<br>\nThe training set contains 60000 examples in total in which 59000 belong to the negative class and 1000 positive class. The test set contains 16000 examples. There are 171 attributes per record.<br>\n\nThe attribute names of the data have been anonymized for proprietary reasons. It consists of both single numerical counters and histograms consisting of bins with different conditions. Typically the histograms have open-ended conditions at each end. For example, if we measuring the ambient temperature \"T\" then the histogram could be defined with 4 bins where:<br>\n\nThe attributes are as follows: class, then anonymized operational data. The operational data have an identifier and a bin id, like \"Identifier_Bin\". In total there are 171 attributes, of which 7 are histogram variables. Missing values are denoted by \"na\".<br>\n\nAcknowledgements<br>\nThis file is part of APS Failure and Operational Data for Scania Trucks. It was imported from the UCI ML Repository.<br>\n\nInspiration<br>\nThe total cost of a prediction model the sum of Cost_1 multiplied by the number of Instances with type 1 failure and Cost_2 with the number of instances with type 2 failure, resulting in a Total_cost. In this case Cost_1 refers to the cost that an unnecessary check needs to be done by an mechanic at an workshop, while Cost_2 refer to the cost of missing a faulty truck, which may cause a breakdown. Cost_1 = 10 and Cost_2 = 500, and Total_cost = Cost_1*No_Instances + Cost_2*No_Instances.<br>"}}