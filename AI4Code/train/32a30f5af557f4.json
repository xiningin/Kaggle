{"cell_type":{"7f5379c3":"code","62815a60":"code","0f7c427e":"code","66e9e720":"code","5c9c24b1":"code","f3bea2ac":"code","a44c87bf":"markdown","ce43132a":"markdown","6b81caab":"markdown","0c9024f2":"markdown","cb406b3e":"markdown","33618f5b":"markdown"},"source":{"7f5379c3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","62815a60":"data = pd.read_csv(\"..\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")\ndata.head()","0f7c427e":"data[\"Date\"] = data[\"Date\"].astype(\"datetime64[ns]\")\n#selecting a chunk of data\nsydney_feb = data[(data[\"Date\"].dt.year == 2016) & (data[\"Location\"] == \"Sydney\")]\n","66e9e720":"\nplt.scatter(sydney_feb[\"Date\"],sydney_feb[\"MaxTemp\"])","5c9c24b1":"def weighted_average(series,beta):\n    v = [0]\n    n = series.shape[0]\n    for i in range(n):\n        v.append(beta*v[-1] + (1-beta)*series.iloc[i])\n        \n    return v[1:]","f3bea2ac":"fig, ax = plt.subplots(1,3,figsize=(18,4))\nbeta = [0.9,0.5,0.98]\nfor i in range(3):\n    ax[i].scatter(sydney_feb[\"Date\"],sydney_feb[\"MaxTemp\"])\n    a1 = weighted_average(sydney_feb[\"MaxTemp\"],beta[i])\n    ax[i].plot(sydney_feb[\"Date\"],a1,c=\"r\")\n    ax[i].set_ylim(0,sydney_feb[\"MaxTemp\"].max()+5)\n    ax[i].set_title(\"Beta = {}\".format(beta[i]))\n    ","a44c87bf":"### We are going to look over the sydney's 2017 Temprature Stats.","ce43132a":"# Bias correction\n\n### Notice something strange? Yup, that trail start from 0.\n\n> #### It's because we started from $V_0 = 0$.\n\n## #ToDo","6b81caab":"### Here, $ \\beta $ is a parameter that decides decaying range let's say.\n\n#### The higher the $\\beta$ is the higher the decaying window is, for example if $\\beta = 0.9$ then it calculates average of past 10 points,\n\nlets say window = $ w $\n\n$ w \\approx 1 \/ (1 - \\beta) $\n\nfor $ \\beta = 0.9 $,<br>\n$ w \\approx 1 \/ (1 - 0.9)$ <br>\n$ w \\approx 10 $\n\nfor $ \\beta = 0.5 $,<br>\n$ w \\approx 1 \/ (1 - 0.5)$ <br>\n$ w \\approx 2 $\n\n#### For $ \\beta = 2 $, it calculate the average of two points at an iteration.","0c9024f2":"### Calculating moving average is inefficient in both ways (memory and time), it requires to calculate average of window with every iteration.\n\n### To resolve this, we can use Exponentially Weighted Averages, it will not be as precise as traditional way but it gives great estimations, besides it's very efficient.\n\n#### To start with, let's say we have:\n\n> $ V_{0} = 0 $\n\n> $ \\theta_1,\\theta_2,\\theta_3 ... \\theta_n = Datapoints$\n\n> $ \\beta = 0.9$  (We will discuss about $\\beta$ later.)\n\n#### Furthermore,\n\n\n> $ V_{1} = \\beta V_{0} + (1 - \\beta)\\theta_1$ <br>\n> $ V_{2} = \\beta V_{1} + (1 - \\beta)\\theta_2$ <br>\n> $ V_{3} = \\beta V_{2} + (1 - \\beta)\\theta_3$\n\n> And so on...\n\n#### So we can formulate:\n\n# $ V_t = \\beta V_{t-1} + (1 - \\beta)\\theta_t $\n\n## It calculate decaying average from past data points:\n\n#### It can be seen by pluging $V_2$ in $V_3$:\n\n ## $  V_{3} = (1 - \\beta)\\theta_3 + \\beta ((1 - \\beta)\\theta_2 + \\beta V_{1})$\n \nHere, $V_1$ can be replaced too.","cb406b3e":"# Exponentially Weighted Averages\n\n\nIn statistics, moving average is generally calculated to smooth out noise in time series data, but it can be helpfull in deep learning too, Simply, the gradient update in each iteration can be smoothen out with each step ofcourse.\n\nJust like this:\n![](https:\/\/eloquentarduino.github.io\/wp-content\/uploads\/2020\/04\/SGD.jpg)","33618f5b":"## In this quick overview, we are going to observe what Exponentially Weighted Averages is and why it is more efficient than regular moving average.\n\n![](https:\/\/i.ytimg.com\/vi\/NxTFlzBjS-4\/maxresdefault.jpg)\n\n> Exponentially Weighted Averages are used in various optimization algorithms such as Adam"}}