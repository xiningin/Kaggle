{"cell_type":{"4e360a27":"code","fad7ec00":"code","84e96f59":"code","43f0cc0a":"code","fd0bac68":"code","19b280a4":"code","ac7336dd":"code","339124c6":"code","cbb3144b":"markdown","129e182a":"markdown","ca863337":"markdown","c878c85e":"markdown","2c022035":"markdown"},"source":{"4e360a27":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\nd0 = pd.read_csv('..\/input\/Sales_Transactions_Dataset_Weekly.csv')\n# save the labels into a variable l. Creating lables just for visulization purpose after doing PCA\nl = d0['Product_Code']\n# Drop the label feature and store the sales data in d. Separating the lable data\nd = d0.drop(\"Product_Code\",axis=1)\nd.drop(d.columns[0:54], axis=1, inplace=True) # this is a normalized data","fad7ec00":"raw=d0[d0.columns[1:53]] # this is a raw data\nraw.head() # how our data looks after seperating the lables","84e96f59":"print(d.shape)\nprint(l.shape)","43f0cc0a":"# initializing the pca\nfrom sklearn import decomposition\npca = decomposition.PCA()","fd0bac68":"# configuring the parameteres\n# the number of components = 2\npca.n_components = 2\npca_data = pca.fit_transform(raw)\n\n# pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape = \", pca_data.shape)","19b280a4":"pca.n_components = 52\n\npca_data_pca = pca.fit_transform(raw)\npercentage_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_);\n\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\n\nplt.clf()\nplt.plot(cum_var_explained,linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()","ac7336dd":"# configuring the parameteres\n# the number of components = 2\npca.n_components = 2\npca_data = pca.fit_transform(d)\n\n# pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape = \", pca_data.shape)","339124c6":"pca.n_components = 52\n\npca_data_pca = pca.fit_transform(d)\npercentage_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_);\n\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\n\nplt.clf()\nplt.plot(cum_var_explained,linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()","cbb3144b":"a) we can use PCA for the raw data as we can see from the above graph,  the no. of componets are analyzed based on the business goal and how variance is explained for each principle componets. \n\nb) I Believe whether to use the data or not depends upon our business goal, yes for example, if we want to analyze the the sales data for the overall product then its is sufficient and reduced dimensions will be 10(approx.), however if want to analyze the sales and provide new product introduction or we want to generate business targets for the following year then we may look for 99% of the variance explained. In the latter case the reduced dimensions will be 36-38 approximately.","129e182a":"The above graph is based on the normalized data from the sales data, where the variance for 2 principle components is approximaley 40 % which is quite low and 100% variance is explained only if we consider the data for all the given 52 weeks, its not feasible to consider normalized values rather than raw data. I mean raw data provides effective varaince and we can reduce diminiosns easily.\n\nIn raw data,for example consider 2 variables or components: The approximate variance is 93.2% which is best used for model\nIn normalized data, for example consider 2 variables or components: The approximate varaince is 40% which is not good for effective results\n\n__The best from raw data is  approx 95% variance explaination is captured with 10 variables and reduced dimensions in this case are 42.__","ca863337":"### Raw data","c878c85e":"### Normalized Data","2c022035":"# PCA Raw data and Normalized Data"}}