{"cell_type":{"137b3140":"code","53b19b84":"code","da8f9e72":"code","139bad11":"code","c4deb551":"code","ed137fa6":"code","65320f8b":"code","2a049d8d":"markdown","fcd8cda7":"markdown","e0c3efda":"markdown","2f5451b1":"markdown","ebc36d4f":"markdown","3a9c9f21":"markdown"},"source":{"137b3140":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport warnings\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scikitplot.metrics import plot_confusion_matrix, plot_roc\nfrom tqdm import tqdm\n\nwarnings.filterwarnings('ignore')","53b19b84":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","da8f9e72":"import os\nos.environ['NEPTUNE_API_TOKEN'] = 'your_long_api_token_goes_here'\n","139bad11":"import neptune\n\nneptune.init(project_qualified_name='jakub-czakon\/santander')","c4deb551":"NAME = 'Magic Parameters'\n\nN_SPLITS = 15\nSEED = 1234\n\nTRAIN_PARAMS = {\n        'num_boosting_rounds': 1000000,\n        'early_stopping_rounds' : 4000\n        }\n\nMODEL_PARAMS = {'bagging_freq': 5,\n         'bagging_fraction': 0.335,\n         'boost_from_average':'false',\n         'boost': 'gbdt',\n         'feature_fraction': 0.041,\n         'learning_rate': 0.1,\n         'max_depth': -1,\n         'metric':'auc',\n         'min_data_in_leaf': 80,\n         'min_sum_hessian_in_leaf': 10.0,\n         'num_leaves': 13,\n         'num_threads': 8,\n         'tree_learner': 'serial',\n         'objective': 'binary',\n         'verbosity': 1,\n                     }\n\nparams = {**MODEL_PARAMS, **TRAIN_PARAMS}","ed137fa6":"def neptune_monitor(prefix):\n    def callback(env):\n        for name, loss_name, loss_value, _ in env.evaluation_result_list:\n            channel_name = '{}{}_{}'.format(prefix, name, loss_name)\n            neptune.send_metric(channel_name, x=env.iteration, y=loss_value)\n    return callback\n\n\ndef plot_prediction_distribution(y_true, y_pred, ax):\n    df = pd.DataFrame({'prediction': y_pred, 'ground_truth': y_true})\n    \n    sns.distplot(df[df['ground_truth'] == 0]['prediction'], label='negative', ax=ax)\n    sns.distplot(df[df['ground_truth'] == 1]['prediction'], label='positive', ax=ax)\n\n    ax.legend(prop={'size': 16}, title = 'Labels')","65320f8b":"with neptune.create_experiment(name=NAME,\n                               params=params):\n\n    folds = StratifiedKFold(n_splits=N_SPLITS, shuffle=False, random_state=SEED)\n    \n    features = [c for c in train_df.columns if c not in ['ID_code', 'target']]\n    oof, predictions = np.zeros(len(train_df)), np.zeros(len(test_df))\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, train_df['target'].values)):\n        print(\"Fold {}\".format(fold_))\n        trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], \n                                         label=train_df['target'].iloc[trn_idx])\n        val_data = lgb.Dataset(train_df.iloc[val_idx][features], \n                    label=train_df['target'].iloc[val_idx])\n\n        monitor = neptune_monitor(prefix='fold{}_'.format(fold_))\n        clf = lgb.train(MODEL_PARAMS, trn_data, \n                        TRAIN_PARAMS['num_boosting_rounds'], \n                        valid_sets = [trn_data, val_data], \n                        verbose_eval=5000, \n                        early_stopping_rounds = TRAIN_PARAMS\n                        ['early_stopping_rounds'],\n                        callbacks=[monitor])\n        oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n        predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n    roc_auc_oof = roc_auc_score(train_df['target'], oof)\n    print(\"CV score: {:<8.5f}\".format(roc_auc_oof))\n    neptune.send_metric('roc_auc', roc_auc_oof)\n\n    preds = pd.DataFrame(oof, columns=['pos_preds'])\n    preds['neg_preds'] = 1.0 - preds['pos_preds']\n    fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(24, 6))\n    plot_prediction_distribution(train_df['target'], preds['pos_preds'], ax=ax1);\n    plot_roc(train_df['target'], preds[['neg_preds','pos_preds']], ax=ax2);\n    plot_confusion_matrix(train_df['target'], oof>0.5, ax=ax3);\n    fig.savefig('model_diagnostics.png') \n    neptune.send_image('model_diagnostics', 'model_diagnostics.png')\n\npd.DataFrame({\"ID_code\": train_df.ID_code.values, 'target':oof}).to_csv(\"oof_{}.csv\".format(NAME), index=False)\npd.DataFrame({\"ID_code\": test_df.ID_code.values, 'target':predictions}).to_csv(\"submission_{}.csv\".format(NAME), index=False)\n","2a049d8d":"## Step 3\n\nCreate an experiment and run training\n\nIn order to log stuff to neptune you need to create an experiment:\n\n    with neptune.create_experiment():\n\nand then simply log stuff like metrics or images to neptune:\n\n        neptune.send_metric('roc_auc', roc_auc_oof)\n        ...\n        neptune.send_image('model_diagnostics', 'model_diagnostics.png')\n\n**Optional (but cool)**\n\nPrepare stuff for custom logging. \n 1. **Lightgbm monitoring**:\n     I like to monitor my lightgbm training and compare the learning curves, so I want to create a `neptune_monitor` callback and  look at the charts as it trains.\n 1. **Model diagnoscs**:\n    I want to have a clear(er) picture of the situation so I log confusion matrix, ROC AUC curve and prediction distrubitions after every run. ","fcd8cda7":"and loading the data.","e0c3efda":"## Step 2\n\nDefine hyperparameters. Put everything you care about in one dictionary.","2f5451b1":"# Step 4\n\nGo to the Experiment in Neptune -> https:\/\/ui.neptune.ml\/jakub-czakon\/santander\/e\/SAN1-59\/charts.\nAnd see your training:\n\n![image](https:\/\/gist.githubusercontent.com\/jakubczakon\/f754769a39ea6b8fa9728ede49b9165c\/raw\/c8425bb2244200dcb86b8cf850db87696acc0322\/kaggel_kernel1.png)\n\nIf you log more experiments you can compare them and stuff:\n\n![image](https:\/\/gist.githubusercontent.com\/jakubczakon\/f754769a39ea6b8fa9728ede49b9165c\/raw\/c8425bb2244200dcb86b8cf850db87696acc0322\/kaggle_kernel2.png)\n\n\n\n\n","ebc36d4f":"# Neptune tracking example\n\nI will use the parametrers from the  ['Magic Parameters' kernel](https:\/\/www.kaggle.com\/sandeepkumar121995\/magic-parameters)\n\nTo get a better picture of what Neptune is, go to this [Medium blog post](http:\/\/bit.ly\/2HtXtMH).\n\nLet's start by importing the usual stuff.","3a9c9f21":"## Step 0\nGo to [neptune.ml](http:\/\/bit.ly\/2FndEZO) and register.\nIt is absolutely free, no card or anything required.\n\n## Step 1\nInitialize Neptune. Set the project name and authorization.\n\nThe recommended way is to create the`NEPTUNE_API_TOKEN` environment variable and pass your account token there."}}