{"cell_type":{"d1fe9304":"code","863e285c":"code","be8012ae":"code","55b28272":"code","5d6cd89e":"code","d772d524":"code","835a3c47":"code","7dcf5019":"code","7aa00d5c":"code","ebd21daf":"code","bcba103a":"code","fcf86dbf":"code","a1feafc0":"code","7026a859":"code","e50c2bec":"code","422c57ea":"code","ed484368":"code","2bac40d3":"code","7cbcbf3e":"code","46d9fb8b":"code","58437548":"code","f7adc748":"code","38ac5339":"code","5353c44f":"code","c9d2f301":"code","f0bc2332":"code","7272f205":"markdown"},"source":{"d1fe9304":"# Code you have previously used to load data\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor","863e285c":"train=pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')\ntest=pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')\ntrain.head()","be8012ae":"train.info()","55b28272":"train.shape","5d6cd89e":"sns.countplot(train['SalePrice'],label=\"Count\")","d772d524":"y = train.SalePrice\n# Create X\nfeatures = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nX = train[features]","835a3c47":"X","7dcf5019":"# Generate the Profiling Report\n\nimport pandas_profiling as pp\nprofile = pp.ProfileReport(\n    X, title=\"OSIC Pulmonary Fibrosis Progression\", html={\"style\": {\"full_width\": True}}, sort=\"None\"\n)\nprofile","7aa00d5c":"X.info()","ebd21daf":"# Split into validation (test) and training data\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)","bcba103a":"X","fcf86dbf":"y","a1feafc0":"from sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nmodel_1 = make_pipeline(StandardScaler(), LinearRegression())\n\nprint(model_1.fit(X_train,y_train))\n\nprint(model_1.score(X_test,y_test))\n\nfrom sklearn.metrics import mean_absolute_error\ny_pred = model_1.predict(X_test)\nprint(mean_absolute_error(y_test, y_pred))","7026a859":"from sklearn.ensemble import RandomForestRegressor\nmodel_2 = make_pipeline(StandardScaler(), RandomForestRegressor())\n\nprint(model_2.fit(X_train,y_train))\n\nprint(model_2.score(X_test,y_test))\n\nfrom sklearn.metrics import mean_absolute_error\ny_pred = model_2.predict(X_test)\nprint(mean_absolute_error(y_test, y_pred))","e50c2bec":"from sklearn.tree import DecisionTreeRegressor\nmodel_3 = make_pipeline(StandardScaler(), DecisionTreeRegressor())\n\nprint(model_3.fit(X_train,y_train))\n\nprint(f'score Model:',model_3.score(X_test,y_test))\n\nfrom sklearn.metrics import mean_absolute_error\ny_pred = model_3.predict(X_test)\nprint(f'Mean_Absolute_Error:',mean_absolute_error(y_test, y_pred))","422c57ea":"from sklearn import neighbors\nmodel_4 = make_pipeline(StandardScaler(), neighbors.KNeighborsRegressor())\n\nprint(model_4.fit(X_train,y_train))\n\nprint(f'score Model:',model_4.score(X_test,y_test))\n\nfrom sklearn.metrics import mean_absolute_error\ny_pred = model_4.predict(X_test)\nprint(f'Mean_Absolute_Error:',mean_absolute_error(y_test, y_pred))","ed484368":"import xgboost as xgb\nmodel_5 = make_pipeline(StandardScaler(),xgb.XGBRegressor())\n\nprint(model_5.fit(X_train,y_train))\n\nprint(f'score Model:',model_5.score(X_test,y_test))\n\nfrom sklearn.metrics import mean_absolute_error\ny_pred = model_5.predict(X_test)\nprint(f'Mean_Absolute_Error:',mean_absolute_error(y_test, y_pred))","2bac40d3":"from sklearn import linear_model\nmodel_6 = make_pipeline(StandardScaler(),linear_model.ARDRegression())\n\nprint(model_6.fit(X_train,y_train))\n\nprint(f'score Model:',model_6.score(X_test,y_test))\n\nfrom sklearn.metrics import mean_absolute_error\ny_pred = model_6.predict(X_test)\nprint(f'Mean_Absolute_Error:',mean_absolute_error(y_test, y_pred))","7cbcbf3e":"from sklearn import linear_model\nmodel_7 = make_pipeline(StandardScaler(),linear_model.BayesianRidge())\n\nprint(model_7.fit(X_train,y_train))\n\nprint(f'score Model:',model_7.score(X_test,y_test))\n\nfrom sklearn.metrics import mean_absolute_error\ny_pred = model_7.predict(X_test)\nprint(f'Mean_Absolute_Error:',mean_absolute_error(y_test, y_pred))","46d9fb8b":"from sklearn.svm import SVC\nmodel_8 = make_pipeline(StandardScaler(),SVC())\n\nprint(model_8.fit(X_train,y_train))\n\nprint(f'score Model:',model_8.score(X_test,y_test))\n\nfrom sklearn.metrics import mean_absolute_error\ny_pred = model_8.predict(X_test)\nprint(f'Mean_Absolute_Error:',mean_absolute_error(y_test, y_pred))","58437548":"models = pd.DataFrame({\n    'Model': ['Linear Regression','Random Forest Regressor','Decision Tree Regressor','K Neighbors Regressor', 'XGB Regressor', \n              'ARD Regression', 'Bayesian Ridge','SVC'],\n\n    'Score': [model_1.score(X_test,y_test)*100,\n              model_2.score(X_test,y_test)*100,\n              model_3.score(X_test,y_test)*100, \n              model_4.score(X_test,y_test)*100,\n              model_5.score(X_test,y_test)*100, \n              model_6.score(X_test,y_test)*100,\n              model_7.score(X_test,y_test)*100,\n              model_8.score(X_test,y_test)*100]})\nmodels.sort_values(by='Score', ascending=True)","f7adc748":"test","38ac5339":"test_X = test[features]","5353c44f":"features","c9d2f301":"test_pred = model_5.predict(test_X)\n","f0bc2332":"output = pd.DataFrame({'Id': test.Id,\n                       \n                       'SalePrice': test_pred})\noutput.to_csv('submission.csv', index=False)\noutput.head()","7272f205":"# 8 Algorithms Regression\n### We used 7 algorithms regression\n* Linear Regression\n* Random Forest Regressor\n* Decision Tree Regressor\n* XGB Regressor\n* ARD Regression\n* KNeighborsRegressor\n* BayesianRidge\n* SVC\n\n\n<img src=\"https:\/\/miro.medium.com\/max\/1024\/1*Juv1bpp5--0Fl8cA4EmTPw.jpeg\" width=\"800px\">\n\n### Data Description\n\n#### File descriptions\n* train.csv - the training set\n* test.csv - the test set\n* data_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here\n* sample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms\n\n\n#### Dataset Link\n\n\n[Hsere ](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course)\n"}}