{"cell_type":{"77be5a67":"code","930e89ed":"code","8604830a":"code","09e011a5":"code","e7feec25":"code","28f88e0d":"code","e3c4ea08":"code","3dcbdf69":"code","47e1e584":"code","37766375":"code","1e56ceaa":"code","3a60c4cb":"code","0c13344e":"code","5a7d5690":"code","7951edd3":"code","4db65755":"code","c9d03dff":"code","542e699e":"code","78c4b30a":"code","74853431":"code","a65487b3":"code","187408de":"code","e66efcc4":"code","dc08ced0":"code","2a206639":"code","6040f21d":"code","4f88c284":"code","ffdae926":"code","dc483014":"code","26de56b9":"code","7236ca62":"code","09a75a5b":"code","5980876a":"code","bceeb75b":"code","4f0dfdfa":"code","d984fea0":"code","6318f2b9":"code","48d03c91":"code","eae90cc4":"code","330d1119":"code","a0ee0355":"code","39ce374b":"code","cb7a7544":"code","dab4e638":"code","29a52f0c":"code","14e65e66":"code","9a7529fa":"code","166edcb9":"code","d1e2fa1c":"code","f18642f2":"code","fac12649":"code","728ed989":"code","30652c4a":"code","44ebf4a3":"code","6b444ab0":"code","2a77b18f":"code","a0977752":"code","f56f04c8":"code","69bf9767":"code","34f3ede5":"code","7c291339":"code","9b91bd32":"code","911c36ef":"code","fcf97a13":"markdown","7b3a5bbb":"markdown","9e1b2bbc":"markdown","949bde5a":"markdown","655dc7db":"markdown","7484e4ad":"markdown","8a5bbecd":"markdown","5026238c":"markdown","48f868ea":"markdown","c3a5a9ce":"markdown","19ea75c4":"markdown","efd86bd9":"markdown","75cc4648":"markdown","d258f40c":"markdown","2ba08bb2":"markdown","e50f8cff":"markdown","d7854998":"markdown","3830a1b4":"markdown","f628d754":"markdown","dba2befe":"markdown","939c0843":"markdown","a7b7b8fc":"markdown","688d6c1a":"markdown","cd633f56":"markdown","89e99392":"markdown","c3553a94":"markdown","a67b0d9c":"markdown","5ece00c6":"markdown","24a817eb":"markdown","f5e86de7":"markdown","80e9e775":"markdown","7ae00c47":"markdown","381932b2":"markdown","1a454a18":"markdown","e1708f40":"markdown","71c1149b":"markdown","11886f8a":"markdown","dfcd275d":"markdown","10d5c5ec":"markdown","d8081ef7":"markdown","908b7659":"markdown","d300680b":"markdown","7953c85e":"markdown","3e846694":"markdown","77db61dd":"markdown","aaec7e93":"markdown","23cf5481":"markdown","7d5cf6f1":"markdown","39f9ceb3":"markdown","efb591b6":"markdown","9889b045":"markdown","5829e9be":"markdown","fbf833fb":"markdown","9a558baf":"markdown"},"source":{"77be5a67":"## Import & install needed packages & libraries\nimport numpy as np # algebra\nimport pandas as pd # data frames & processing, CSV file I\/O \n\n# Visualization, graphs & plots\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go # plots & graphics \nfrom plotly.subplots import make_subplots \nimport seaborn as sns # plots & graphics\n\n# SpaCy\nimport spacy\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\nfrom thinc.neural.optimizers import Adam # for hyperparameter tuning\nfrom thinc.neural import Model # for hyperparameter tuning\n\n# Utilities & helpers\nfrom tqdm import tqdm_notebook as tqdm # progress bar\nimport time\nimport os\nimport re # Regex library\nimport random\nimport timeit # measuring execution time\nfrom wordcloud import WordCloud\nimport warnings\nwarnings.filterwarnings('ignore') #Ignore \"future\" warnings and Data-Frame-Slicing warnings.","930e89ed":"# Load data: Go to parent directory (..\/), re-dicrect to train & test.csv\nDATA_BASE_PATH = '..\/input\/tweet-sentiment-extraction\/' \n\n# Read training & test data as pandas-dataframe (ending _df)\ntrain_df = pd.read_csv(DATA_BASE_PATH + 'train.csv') \ntest_df = pd.read_csv(DATA_BASE_PATH + 'test.csv')","8604830a":"print(f\"Train-Data shape is: {train_df.shape}\") \nprint(f\"Test-Data shape is: {test_df.shape}\")\n# Link for knowledge about Python 3.6+ new f-string formatting:\n# https:\/\/realpython.com\/python-f-strings\/","09e011a5":"# First glimps of the train data\ntrain_df.head()","e7feec25":"# First glimps of the test data\ntest_df.head()","28f88e0d":"# Checking for missing values\nprint(f\"Missing values in TRAIN_data: \\n\" + \"-\" * 25)\nprint(f\"{train_df.isna().sum()}\")\n\nprint(f\"\\n \\n\" + \"Missing values in TEST_data: \\n\" + \"-\" * 25)\nprint(f\"{test_df.isna().sum()}\")\n\n# Show all rows containing missing values:\n# Reminder using pandas: sum(axis = 1) sums up along the X axis\nprint(f\"\\n \\n {train_df[train_df.isna().sum(axis = 1) >= 1]}\")   ","e3c4ea08":"train_df.dropna(inplace = True, how = 'any')","3dcbdf69":"train_df.sample(15)","47e1e584":"train_df.loc[[9710]] # cheese","37766375":"train_df.loc[[1305]] # live long","1e56ceaa":"train_absolute_count = train_df[\"sentiment\"].value_counts()\ntrain_relative_count = train_df[\"sentiment\"].value_counts(normalize = True)\ntest_relative_count = test_df[\"sentiment\"].value_counts(normalize = True)\n\n# Create figure and add traces\nfig = make_subplots(1,3, subplot_titles = ('TRAIN data absolute amount',\n                                           'TRAIN data relative amount', \n                                           'TEST data relative amount'))\nfor i in fig['layout']['annotations']:\n            i['font'] = dict(size = 13)\n  \nfig.add_trace(go.Bar(x = train_absolute_count.index, y = train_absolute_count.values, \n                     marker_color = ['blue','green','red'], name= ''), row = 1, col = 1)\n\nfig.add_trace(go.Bar(x = train_relative_count.index, y = train_relative_count.values,                     \n                     marker_color = ['blue','green','red'], name= ''), row = 1, col = 2)\n\nfig.add_trace(go.Bar(x = test_relative_count.index, y = test_relative_count.values,\n                     marker_color = ['blue','green','red'], name= ''), row = 1, col = 3)\n\n\ntitle_text = \"Absolut and relative distribution of sentiments in train and test data\"\nfig.update_layout(title={'text': title_text})\n\n# Define default go-layout for future use\ndefault_layout =  go.Layout(  \n    title = {                    \n        'y': 0.95,\n        'x': 0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'\n    },\n    titlefont = {\n     'size' : 15, \n     'color': 'black'\n    },\n    font = {\n      'size' : 10 \n    })\n\nfig.update_layout(default_layout)\n\nfig.show()","3a60c4cb":"cache = \"\"\"\ntrain_max_len = max(train_df[\"TEXT_number_of_words\"])\nbins = np.linspace(0, train_max_len, train_max_len) # np.linspace takes following arguments: startpoint, endpoint, number of steps\n\n# Cache tbd removed?\ncache = plt.hist(train_df[\"TEXT_number_of_words\"], bins, alpha=0.5, label = 'Number of words in \"TEXT\"')\ncache = plt.hist(train_df[\"SELECTED_TEXT_number_of_words\"], bins, alpha=0.5, label = 'Number of words in \"SELECTED_TEXT\"')\ncache = plt.legend(loc = 'upper right')\ncache = plt.style.use('seaborn-deep')\nplt.gcf().set_size_inches(15, 7)  # Wow, there's really no set_size_cm ...but yea, a conversion via a pre-defined tuple could help here.\ncache = plt.show()\n\"\"\"","0c13344e":"# Create columns with number of words in 'text' and 'selected_Text'\ntrain_df[\"TEXT_number_of_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))  \ntrain_df[\"SELECTED_TEXT_number_of_words\"] = train_df[\"selected_text\"].apply(lambda x: len(str(x).split()))\ntrain_max_len = max(train_df[\"TEXT_number_of_words\"])\n\nfig = plt.figure(figsize=(18,8))\n# Number of bins shall equal the max-length in train_df['text']\nbins = np.linspace(0, train_max_len, train_max_len) \n# sns.distplot is a nice combination of sns.hist and sns.kdplot\nplot1 = sns.distplot(train_df['TEXT_number_of_words'], \n                     bins = bins, \n                     label = 'TEXT_number_of_words')\nplot1 = sns.distplot(train_df['SELECTED_TEXT_number_of_words'], \n                     bins =  bins,  \n                     label = 'SELECTED_TEXT_number_of_words')  \ncache = plt.legend() \n\nfig.suptitle('Distribution of number of words', fontsize = 20)\n\n# Defining default parameter for plt.rc for later re-use.\nparams = {\n    'figure.titlesize': 22, # fontsize of plot title \/ fig.suptitle\n    'axes.titlesize': 14,   # fontsize of the axes title\n    'axes.labelsize': 11,   # fontsize of the x and y labels    \n    'xtick.labelsize': 11,  # fontsize of the tick labels\n    'ytick.labelsize': 11,  # fontsize of the tick labels\n    'legend.fontsize': 12,  # fontsize for legends (plt.legend(), fig.legend())\n}\n\nplt.rcParams.update(params)","5a7d5690":"sentiments = ['neutral', 'negative', 'positive']\nfig,ax = plt.subplots(nrows = 1, ncols = len(sentiments), figsize = (20,5))\n\nfor i, sentiment in enumerate(sentiments):\n    plot2 = sns.kdeplot(train_df[train_df['sentiment'] == sentiment]['TEXT_number_of_words'],\n                        ax = ax[i],\n                        label = 'TEXT wordcount').set_title(sentiment) \n    plot2 = sns.kdeplot(train_df[train_df['sentiment'] == sentiment]['SELECTED_TEXT_number_of_words'],  \n                        ax = ax[i],\n                        label = 'SELECTED_TEXT wordcount') \n    ax[i].set_xlabel('Number of words')\n    cache = plt.legend() \n\nfig.suptitle('Words in TEXT and SELECTED_TEXT', fontsize = 20)\n\nparams_3_figs = {\n    'figure.titlesize': 18, # Fontsize of plot title \/ fig.suptitle\n    'axes.titlesize': 14,   # Fontsize of the axes title\n    'axes.labelsize': 14,   # Fontsize of the x and y labels    \n    'xtick.labelsize': 14,  # Fontsize of the tick labels\n    'ytick.labelsize': 14,  # Fontsize of the tick labels\n    'legend.fontsize': 14,  # Fontsize for legends (plt.legend(), fig.legend())\n}\n\nplt.rcParams.update(params_3_figs) # Params defined above and re-used.","7951edd3":"grid = sns.FacetGrid(train_df, col = 'sentiment', height = 8)\nsns_default_font_scale = 1.6 # setting default font scale for sns for later use\ngrid.map(sns.lineplot, 'TEXT_number_of_words', 'SELECTED_TEXT_number_of_words', estimator = 'mean', ci = None)\ngrid.add_legend() \nplt.subplots_adjust(top = 0.8)\nsns.set(font_scale = sns_default_font_scale) \ncache = grid.fig.suptitle('Mean number of words in \"SELECTED_TEXT\" over number of words in \"TEXT\"', fontsize = 24)","4db65755":"# Creating column with the difference of number of words in 'text' and 'selected text'\ntrain_df['Diff_len_text_selected_text'] = train_df['TEXT_number_of_words'] - train_df['SELECTED_TEXT_number_of_words']","c9d03dff":"grid = sns.FacetGrid(train_df, col = 'sentiment', height = 8)\nsns.set(font_scale = sns_default_font_scale) \ngrid.map(sns.lineplot, 'TEXT_number_of_words', 'Diff_len_text_selected_text', ci = None)\ngrid.add_legend()\nplt.subplots_adjust(top = 0.8)\ncache = grid.fig.suptitle('Difference of len(TEXT) and len(SELECTED_TEXT) over number of words in \"TEXT\"', fontsize = 24)","542e699e":"fig_box = go.Figure()\n\nfor _, sentiment in enumerate(sentiments):\n    # CHANGE HERE! You can change \"Diff_len_text_selected_text\" in below line to \n    # \"TEXT_number_of_words\" or \"SELECTED_TEXT_number_of_words\" to get the respective boxplots.  \n    fig_box.add_trace(go.Box(y = train_df[train_df['sentiment'] == sentiment]['Diff_len_text_selected_text'], name = sentiment)) \n\ntitle_text = 'Boxplot diagram difference in len(text) and len(selected_text)'\nfig_box.update_layout(title = {'text': title_text})\nfig_box.update_layout(default_layout)\n          \nfig_box.show()","78c4b30a":"## Defining functions for using RegEx\ndef startsOrEndsWithSpecialCharacter(rowString): \n    '''\n    Returns a boolean for whether a given string starts OR ends with an unexpected characters\n    '''\n    # Check beginning of the string\n    pattern = '^[\\\/_,.|;:#*~+-?!].*'\n    value =  0 if (re.match(pattern, rowString) == None) else 1\n    # Check ending: expected interpunctuation at the end of the selected text (! ? .) is allowed.  \n    pattern = '.*[\\\/_,|;:#*~+-]$' \n    value = value | (0 if (re.match(pattern, rowString) == None) else 1)\n    # Check ending for white spaces before ending on a spec. character. E.g. \"hi .\"\n    value = value | endsWithAppendedSpecialCharactersAndWhitespace(rowString) \n    # print(re.match(pattern, rowString)) # for analysis, if needed\n    return value","74853431":"# Some additional functions for extending the analysis if needed.\n\n# looking for stuff like ', so happy'\ndef startsWithPrependedSpecialCharactersAndWhitespace(rowString):   \n    pattern = '(^[\\\/_|,;.:#*~+-!?]\\s+.*)'\n    # print(re.match(pattern, rowString)) \n    return 0 if (re.match(pattern, rowString) == None) else 1\n\n# looking for stuff like ',so happy'\ndef startsWithPrependedSpecialCharactersNoWhitespace(rowString): \n    pattern = '(^[\\\/_|,;.:#*~+-](?!\\s+).*)'\n    # print(re.match(pattern, rowString))\n    return 0 if (re.match(pattern, rowString) == None) else 1\n\n# looking for stuff like 'so happy,') \n# expected interpunctuation at the end of the selected text (! ? .) is allowed\ndef endsWithAppendedSpecialCharactersNoWhitespace(rowString):  \n    pattern = '.*(<\\s)[\\\/_|,;:#*~+-]$'  \n    # print(re.match(pattern, rowString))\n    return 0 if (re.match(pattern, rowString) == None) else 1\n\n# looking for stuff like 'so happy ,') \ndef endsWithAppendedSpecialCharactersAndWhitespace(rowString):  \n    pattern = '(.*\\s+)?[\\\/_,.|;:#*~+-?!]$'\n    # print(re.match(pattern, rowString))\n    return 0 if (re.match(pattern, rowString) == None) else 1","a65487b3":"startsOrEndsWithSpecialCharacter_series = []\nstartsOrEndsWithSpecialCharacter_series = train_df[\"selected_text\"].apply(lambda x: startsOrEndsWithSpecialCharacter(x))\nstartsOrEndsWithSpecialCharacter_series = startsOrEndsWithSpecialCharacter_series[startsOrEndsWithSpecialCharacter_series > 0]\nstartsOrEndsWithSpecialCharacter_len = len((startsOrEndsWithSpecialCharacter_series[startsOrEndsWithSpecialCharacter_series == 1]))\nprint(f\"There are {startsOrEndsWithSpecialCharacter_len} tweets\"\n      f\"(= {startsOrEndsWithSpecialCharacter_len\/train_df.shape[0] * 100:.2}%) that start or end with a special character pattern. \\n\"\n      \"Some examples look like this:\")\ntrain_df.loc[startsOrEndsWithSpecialCharacter_series.index].sample(10)[[\"textID\", \"text\", \"selected_text\", \"sentiment\"]]","187408de":"nonNeutralSentiment_df = train_df[train_df[\"sentiment\"] != \"neutral\"]\nnonNeutralSentimentsStartEndWithSpecialChar = nonNeutralSentiment_df.loc[nonNeutralSentiment_df.index.intersection(startsOrEndsWithSpecialCharacter_series[startsOrEndsWithSpecialCharacter_series == 1].index)]\nprint(f\"There are {len(nonNeutralSentiment_df)} non-neutral sentiment tweets\" \n      f\" and {len(nonNeutralSentimentsStartEndWithSpecialChar) \/ len(nonNeutralSentiment_df) * 100:.1f}%\"\n      f\" END with a special character\")","e66efcc4":"## Loading the spaCy model\n# Model options: small, med, large, see: https:\/\/spacy.io\/models\/en\nspaCy_model = 'en_core_web_lg' \nnlp = spacy.load(spaCy_model)","dc08ced0":"## Creating spaCy vocab set\nspaCy_vocab_list = (list(nlp.vocab.strings))\n# set ensures that all values are unique. All words converted to lower case.\nspaCy_vocab_set = set([word.lower() for word in spaCy_vocab_list])","2a206639":"print(f\"The loaded spaCy vocab {spaCy_model} contains unique lower case words: {len(spaCy_vocab_set)}\")","6040f21d":"def remove_special_characters(str1):\n    '''\n    Takes a string, removes(substituted by \"\") all special characters and URLS and returns it.\n    '''\n    url_pattern = 'http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    # Remove URLs with re.sub(pattern, replacement, string)\n    cache = re.sub(url_pattern, ' ', str(str1))  \n    # Removing spec. characters\n    character_pattern = '\\||\\,|\\;|\\.|\\:|\\#|\\*|\\~|\\+|\\-|\\\/|\\_|\\?|\\!|\\\"|\\'|\\`|\\(|\\)|\\=|\\&|\\%|\\$|\\\u00a7' \n    return re.sub(character_pattern, ' ', str(cache))","4f88c284":"# Create new columns for clean data\ntrain_df['clean_text'] = train_df['text'].str.lower().apply(remove_special_characters)\ntrain_df['clean_selected_text'] = train_df['selected_text'].str.lower().apply(remove_special_characters)\ntest_df['clean_text'] = test_df['text'].str.lower().apply(remove_special_characters)\n\n# Create sets with unique words and update them \ntrain_text_vocab_set = set()\ntrain_selected_text_vocab_set  = set()\ntest_text_vocab_set  = set()\n\n# Apply set.update to fill the sets\ntrain_df['clean_text'].str.lower().str.split().apply(train_text_vocab_set.update)\ncache = train_df['clean_selected_text'].str.lower().str.split().apply(train_selected_text_vocab_set.update)\ncache = test_df['clean_text'].str.lower().str.split().apply(test_text_vocab_set.update)","ffdae926":"print(\n    f\"The used spaCy model contains unique words: {len(spaCy_vocab_set)}  \\n\"\n    f\"TRAIN-datas 'text' column contains unique words: {(len(train_text_vocab_set))}  \\n\"\n    f\"TRAIN-datas 'selected_text' column contains unique words: {len(train_selected_text_vocab_set)} \\n\"\n    f\"TEST-datas 'selected_text' column contains unique words: {len(test_text_vocab_set)} \\n\"\n)","dc483014":"# Calculate the fraction of words contained in both sets for relevant pairs\nfraction_shared_words_train_text_to_spaCy = len(train_text_vocab_set.intersection(spaCy_vocab_set)) \/ (len(train_text_vocab_set))\nfraction_shared_words_train_selected_text_spaCy = len(train_selected_text_vocab_set.intersection(spaCy_vocab_set)) \/ (len(train_selected_text_vocab_set))\nfraction_shared_words_test_text_spaCy = len(test_text_vocab_set.intersection(spaCy_vocab_set)) \/ (len(test_text_vocab_set))\nfraction_shared_words_test_text_train_text = len(test_text_vocab_set.intersection(train_text_vocab_set)) \/ (len(test_text_vocab_set))","26de56b9":"fig = go.Figure([go.Bar(x = [\"train['text'] & spaCy\", \"train['selected_text'] & spaCy\", \"test['text'] & spaCy\", \"test['text'] & train['text']\"], \n                        y = [fraction_shared_words_train_text_to_spaCy, fraction_shared_words_train_selected_text_spaCy, fraction_shared_words_test_text_spaCy, fraction_shared_words_test_text_train_text], \n                        marker_color = ['blue','green','red'])])\n\ntitle_text = 'Fraction of words contained in both sets for relevant pairs'\nfig.update_layout(title={'text': title_text})\nfig.update_layout(default_layout)\n \nfig.show()","7236ca62":"print(f\"Only {len(train_selected_text_vocab_set.intersection(train_text_vocab_set)) \/ (len(train_selected_text_vocab_set)):.4f}%\"\n      f\" of 'selected_text' is fully available in 'text',.. giving us {len(train_selected_text_vocab_set - train_text_vocab_set)} words which are cut off: \\n\")\nprint(train_selected_text_vocab_set - train_text_vocab_set)","09a75a5b":"## Create lists, which contain only the not-shared words but which preserve the number of occurences of those words\ntrain_clean_text_word_list = ' '.join([i for i in train_df['clean_text']]).split()  \ntrain_clean_selected_text_word_list = ' '.join([i for i in train_df['clean_selected_text']]).split()  \ntest_clean_text_word_list = ' '.join([i for i in test_df['clean_text']]).split()  \n\n# Calculate differences \nnot_shared_train_text_to_spaCy = [word for word in train_clean_text_word_list if ((word in train_text_vocab_set) and (word not in spaCy_vocab_set))]\nnot_shared_train_selected_text_to_spaCy = [word for word in train_clean_selected_text_word_list if ((word in train_selected_text_vocab_set) and (word not in spaCy_vocab_set))]\nnot_shared_test_text_to_spaCy = [word for word in test_clean_text_word_list if ((word in test_text_vocab_set) and (word not in spaCy_vocab_set))]\nnot_shared_test_text_to_train_text = [word for word in test_clean_text_word_list if ((word in test_text_vocab_set) and (word not in train_text_vocab_set))]","5980876a":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[30, 15])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(not_shared_train_text_to_spaCy ))\ntitleFontsize = 20\n\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Not shared words: train_text and spaCy vocab',fontsize = titleFontsize);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(not_shared_train_selected_text_to_spaCy))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Not shared words: train_selected_text and spaCy vocab',fontsize = titleFontsize)","bceeb75b":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[30, 15])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(not_shared_train_text_to_spaCy ))\ntitleFontsize = 20\n\nwordcloud3 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(not_shared_test_text_to_spaCy))\nax1.imshow(wordcloud3)\nax1.axis('off')\nax1.set_title('Not shared words: test_text and spaCy vocab',fontsize=titleFontsize);\n\nwordcloud4 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(not_shared_test_text_to_train_text))\nax2.imshow(wordcloud4)\nax2.axis('off')\nax2.set_title('Not shared words: Test_text to train_text',fontsize=titleFontsize)","4f0dfdfa":"def jaccard_score (str1, str2):\n    '''\n    Returns the Jaccard Score (intersection over union) of two strings.\n    '''\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    \n    return float(len(c) \/ (len(a) + len(b) - len(c)))  ","d984fea0":"train_df[\"Jaccard_Score_text_selected_text\"] = train_df.apply(lambda x: jaccard_score(str(x[\"text\"]), str(x[\"selected_text\"])), axis = 1) \n[avg_Jaccard_neu_train, avg_Jaccard_neg_train, avg_Jaccard_pos_train] = [train_df[train_df[\"sentiment\"] == sentiment][\"Jaccard_Score_text_selected_text\"].mean() for sentiment in [\"neutral\", \"negative\", \"positive\"]]\navg_Jaccard_train = pd.Series(data = {'Avg Jaccard neutral': avg_Jaccard_neu_train, 'Avg Jaccard negative': avg_Jaccard_neg_train, \"Avg Jaccard positive\": avg_Jaccard_pos_train}, name = 'Jaccard Score per sentiment' )\n\nprint(f\"Overall avg. Jaccard Score: {(avg_Jaccard_neu_train + avg_Jaccard_neg_train + avg_Jaccard_pos_train) \/ 3}\")\nprint(avg_Jaccard_train)","6318f2b9":"# Code for creating a submission_df for base-line score: simply taking text as selected text\nexample = \"\"\"submission_df = pd.read_csv( data_base_path + 'sample_submission.csv')\nsubmission_df['selected_text'] = test_df['text']\nsubmission_df.to_csv(\"submission.csv\", index=False)\ndisplay(submission_df.head(10))\"\"\"","48d03c91":"## Split the data into training and validation set.\ntrain_val_split = 0.9\n\n# Create a copy of the train_df to work with and not change train_df itself\ntrain_df_copy = train_df.copy() \n# Setting a random state for reproducable splits\ntrain_set_df = train_df_copy.sample(frac = train_val_split, random_state = 42) \nval_set_df = train_df_copy.drop(train_set_df.index)\nval_set_df.drop(['clean_text', 'clean_selected_text'], axis='columns', inplace = True)\n\n# Get val_set for each sentiment\nval_set_pos_df = val_set_df[val_set_df['sentiment'] == 'positive'].copy()\nval_set_neg_df = val_set_df[val_set_df['sentiment'] == 'negative'].copy()\nval_set_neu_df = val_set_df[val_set_df['sentiment'] == 'neutral'].copy()","eae90cc4":"print(\n    f\"Number of tweets in val_set_neu_df: {len(val_set_pos_df)}  \\n\"\n    f\"Number of tweets in val_set_neg_df: {(len(val_set_neg_df))}  \\n\"\n    f\"Number of tweets in val_set_pos_df: {len(val_set_neu_df)}\")","330d1119":"def get_training_data(sentiment, splitAtLength = 0):\n    '''\n    Returns the training data in spaCy-required format for the given sentiment.\n    If 'splitAtLength' is > 0, two none-empty arrays are returned: \n    first array containing the train_data with a 'text' containing more words than 'splitAtLength' \n    and the second arraycontaining train_data with 'text' containing number of words up to 'splitAtLength'.\n    If splitAtLength is 0 or None, only one none-empty array \n    containing all train_data is returned, the second array is empty.\n    \n            Parameters:\n                    sentiment (str): sentiment for which train_data needs to be returned\n                    splitAtLength (int, optional): determins if and where the train_data is split\n\n            Returns:\n                    train_data (array): Returns the train data in an array, or array of arrays, if splitAtLength > 0.\n    '''\n    train_data_long = []\n    train_data_short = []    \n    \n    for idx in train_set_df.index:\n        if train_set_df.at[idx, 'sentiment'] == sentiment:\n            text = train_set_df.at[idx,'text']\n            len_text = len(text.split())            \n            selected_text = train_set_df.at[idx,'selected_text']\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            # create the train data in spaCy-required format. We can choose any \"dummy_label\" here\n            # as we are anyway training just ONE model per sentiment:\n            # all labels would be identical (e.g. positive) anyway.\n            if (splitAtLength == None) or (splitAtLength == 0):\n                   train_data_long.append((text, {\"entities\": [[start, end, \"dummy_label\"]]}))\n            elif len(len_text) >= splitAtLength:\n                   train_data_long.append((text, {\"entities\": [[start, end, \"dummy_label\"]]}))\n            elif len(len_text) < splitAtLength:\n                   train_data_short.append((text, {\"entities\": [[start, end, \"dummy_label\"]]}))\n            else: print(\"something is wrong in getting training data\")  \n            \n    return [train_data_long, train_data_short]","a0ee0355":"def get_validation_data(sentiment, model_type, splitAtLength = None):\n    '''\n    Returns the validation data used in training function\n           \n           Parameters:\n                    sentiment (str): sentiment for which validation data needs to be returned\n                    model_type (str): determins if data for short or long model is needed\n                    splitAtLength (int, optional): determins if and where the data is split\n\n           Returns:\n                    train_data (array): Returns the train data in an array, or array of arrays, if splitAtLength > 0.\n    '''\n    if ((splitAtLength is None) or (splitAtLength == 0)):\n        val_set_new_df = val_set_df[val_set_df['sentiment'] == sentiment].copy()  \n        return val_set_new_df\n    \n    if (splitAtLength > 0) and (model_type == \"short\"): # tweets with length up to splitAtLength\n        val_set_new_df = val_set_df[val_set_df[\"text\"].str.split().str.len() < splitAtLength].copy()\n        val_set_new_df = val_set_new_df[val_set_new_df['sentiment'] == sentiment].copy()\n        return val_set_new_df\n    \n    if (splitAtLength > 0) and (model_type == \"long\"): # tweets with length > splitAtLength\n        val_set_new_df = val_set_df[val_set_df[\"text\"].str.split().str.len() >= splitAtLength].copy()\n        val_set_new_df = val_set_new_df[val_set_new_df['sentiment'] == sentiment].copy()\n        return val_set_new_df","39ce374b":"def get_model_output_path(sentiment, splitAtLength = None, train_val_split = 0.90):\n    '''\n    Creates an easy to understand path for saving the model based on  the models parameters\n    '''\n    model_out_paths = []\n    if splitAtLength == None or splitAtLength == 0:\n        model_out_paths.append('models\/model_'\n                               + str(sentiment)\n                               + '_splitAtLength_Longer_than'\n                               + str(splitAtLength)\n                               + '_train_val_split_'\n                               + str(train_val_split))\n        model_out_paths.append(None)\n    elif splitAtLength != None:\n        model_out_paths.append('models\/model_' \n                               + str(sentiment)\n                               + '_splitAtLength_Longer_than'\n                               + str(splitAtLength)\n                               + '_train_val_split_'\n                               + str(train_val_split))\n        model_out_paths.append('models\/model_'\n                               + str(sentiment)\n                               + '_splitAtLength_Up_To'\n                               + str(splitAtLength) + '_train_val_split_' \n                               + str(train_val_split))\n    return model_out_paths","cb7a7544":"def save_model(output_dir, model , new_model_name, rank):\n    '''\n    Saves the model to an easy to understand path\n    \n        Parameters:\n            output_dir (str): easy to understand path derived from models parameters\n            model (model): the trained NLP model from spaCy            \n            new_model_name (str): name for the model, NOT used for loading later\n            rank (str): determins if the model is the best, 2nd best or 3rd best and adds it to save-path.\n    '''\n    output_dir = f'..\/working\/{output_dir}_{rank}'\n    if output_dir is not None:        \n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        model.meta[\"name\"] = new_model_name\n        model.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)  ","dab4e638":"# create disctionary for logging grid-search results.\nresults = {}","29a52f0c":"def train(sentiment, output_dir, epochs = 1, model = None, optimizer = None, dropout = 0.5, splitAtLength = 0, fill_no_predictions_with_text = False):\n    '''\n    Load the model, set up the pipeline and train the entity recognizer\n    '''\n    # define aborting conditions\n    if epochs == 0 or epochs == None:\n        return\n    if output_dir == [] or output_dir == None:\n        return\n\n    # extract info if this model shall be for short or for long tweets from \"output_dir\"\n    if \"Longer_than\" in output_dir:\n        model_type = \"long\"\n        # for long model, get 1st entry in array from get_training_data (used below)\n        short_data = 0 \n    elif \"Up_To\" in output_dir:\n        model_type = \"short\" \n         # for short model, get 2nd entry in array from get_training_data (used below)\n        short_data = 1\n        \n    # get train data relevant for this training session\n    train_data = get_training_data(sentiment, splitAtLength = 0)[short_data]\n    \n    # get validation data relevant for this training session\n    val_data = get_validation_data(sentiment, model_type, splitAtLength = splitAtLength)\n   \n    if output_dir == None:\n        return \n   \n    if model is not None:\n        nlp = spacy.load(output_dir)  # load existing spaCy model to continue training\n        print(f'Loaded model {model}')\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n    \n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last = True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n        print(\"Getting NER-pipe in spaCy model.\")\n    \n    # add all labels available in train_data\n    # we could adjust train_data to get more\/different labels\n    # this will be explained in detail later.\n    for _, annotations in train_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER       \n        if model is None:\n            nlp.begin_training()\n            optimizer = nlp.begin_training() if optimizer is None else optimizer\n        else:\n            nlp.resume_training() \n            optimizer = nlp.resume_training() if optimizer is None else optimizer\n\n        # initialize values to avoid \"referenced before assignment\" issues.\n        best_Jaccard_score, second_best_Jaccard_score, third_best_Jaccard_score = [0,0,0]\n        last_update_best_model, last_update_2nd_best_model, last_update_3rd_best_model = [0,0,0]\n        improvement_best, improvement_2ndbest, improvement_3rdbest = [0,0,0]\n        \n       \n        # actual train-step\n        for itn in tqdm(range(epochs)):\n            random.shuffle(train_data)\n            # batch up the examples using spaCy's minibatch\n            # compounding(start batch size, end batch size,  compounding factor).\n            batches = minibatch(train_data, size = compounding(1.0, 100.0, 1.15) )    \n            losses = {}\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(texts,  # batch of texts\n                                    annotations,  # batch of annotations\n                                    drop = dropout,   # makes it harder to memorize data\n                                    sgd = optimizer,\n                                    losses=losses, \n                                    )\n            \n           # test model on validattion_df and measure time\n            start = time.time()\n            [avg_pred_jaccard, number_no_predictions] = val_predictions_and_calc_Jaccard(\n                sentiment = sentiment,\n                model = nlp,\n                val_df = val_data,               \n                fill_no_predictions_with_text = fill_no_predictions_with_text)\n            ende = time.time()\n            print(\"Losses\", losses)\n            print(f'Avg. Jaccard Score for sentiment \"{sentiment}\" is: {avg_pred_jaccard:.4f},'                  \n                  f' Number of empty predictions: {number_no_predictions}')\n            \n            # keep track of top 3 models & save them                      \n            if avg_pred_jaccard > best_Jaccard_score:\n                        improvement_best = avg_pred_jaccard - best_Jaccard_score\n                        best_Jaccard_score = avg_pred_jaccard\n                        save_model(output_dir,\n                                   model = nlp, \n                                   new_model_name = output_dir.split('\/')[-1],\n                                   rank = 'best')\n                        last_update_best_model = itn + 1  \n            elif avg_pred_jaccard > second_best_Jaccard_score:\n                        improvement_2ndbest = avg_pred_jaccard - second_best_Jaccard_score\n                        second_best_Jaccard_score = avg_pred_jaccard\n                        save_model(output_dir,\n                                   model = nlp,\n                                   new_model_name = output_dir.split('\/')[-1],\n                                   rank = 'second_best')\n                        last_update_2nd_best_model = itn + 1     \n            elif avg_pred_jaccard > third_best_Jaccard_score:\n                        improvement_3rdbest = avg_pred_jaccard - third_best_Jaccard_score\n                        third_best_Jaccard_score = avg_pred_jaccard\n                        save_model(output_dir,\n                                   model = nlp,\n                                   new_model_name = output_dir.split('\/')[-1],\n                                   rank = 'third_best')  \n                        last_update_3rd_best_model = itn + 1   \n            else: print(\"Model didn't perform better, therefore not saved.\")    \n                    \n    \n    if best_Jaccard_score > 0: # only if some progress was made:\n        print(f'\\n \\n \\nBest model reached {best_Jaccard_score:.4f} and was updated' \n              f' (+{improvement_best:.4f}) last in {last_update_best_model}th epoch \\n'\n              f'2nd best model reached {second_best_Jaccard_score:.4f} and was updated'\n              f' (+{improvement_2ndbest:.4f}) last in {last_update_2nd_best_model}th epoch\\n'\n              f'3rd best model reached {third_best_Jaccard_score:.4f} and was updated' \n              f' (+{improvement_3rdbest:.4f})last in {last_update_3rd_best_model}th epoch')    \n\n    log = str(splitAtLength) + '_' + str(model_type)\n    results[log] = {'best_Jaccard_score': best_Jaccard_score,\n                                  'sentiment': sentiment,\n                                  'splitAtLength': splitAtLength,                              \n                                  'model_type': model_type}","14e65e66":"def make_predictions(text, model, fill_no_predictions_with_text):\n    '''\n    Predicts entities based on the given model.\n    Set fill_no_predictions_with_text to TRUE for creating a valid (high scoring) submission.\n    If fill_no_predictions_with_text is false; all no-predictions will be marked with\n    \"NO-PREDICTION\" for fruther evaluation.\n    '''\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        ent_array.append([start, end, ent.label_])\n    if fill_no_predictions_with_text:\n        selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    else:\n        selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else \"NO-PREDICTION\"\n    return selected_text","9a7529fa":"def val_predictions_and_calc_Jaccard (sentiment, model, val_df, fill_no_predictions_with_text):    \n    val_df['prediction']  = val_df[\"text\"].apply(str).apply(lambda x: make_predictions(x, model, fill_no_predictions_with_text))\n    val_df['pred_jaccard'] = val_df.apply(lambda x: jaccard_score (str(x['selected_text']), str(x['prediction'])), axis = 1)\n    return [val_df['pred_jaccard'].mean(), np.sum(val_df[\"prediction\"] == \"NO-PREDICTION\")]           ","166edcb9":"## Hyperparameters\n'''\nsplitAtLength: Allows to create a model per sentiment for tweets longer than splitAtLength words\nand another model for shorter tweets --> so two models per sentiment.\nsplitAtLength = 0 (default): means NO split at all --> 1 model per sentiment.\nsplitAtLength_list: Can contain one or multiple values which will be iterated over.\nUse this for grid search for optimal length!\nMedian for positive and negative tweets is 12, so half of the tweets contain less than \n(or equal to) 12 words and the other half contains more.\n'''\nsplitAtLength_list = [0] \n\n# Epochs for long and short model\nepochs_long = 30\nepochs_short = 30\n\ndropout_rate = 0.2\n\n'''\nIf fill_no_predictions_with_text is FALSE: all rows without a prediction will be marked with\n\"NO-PREDICTION\" for further evaluation.\n'''\nfill_no_predictions_with_text = True \n\n# Optimizer options\nops = Model.ops\nlearn_rate = 0.0015 # default: 0.001\nL2 = 1e-5 # L2 regularisation penatly. Default: 1e-6\nmax_grad_norm = 1.0 # avoiding exploding gradients. Default: 1\noptimizer = Adam(ops, learn_rate, L2 = L2) \noptimizer.max_grad_norm = max_grad_norm\n\n# Determin for which sentiments to train a model for.\n# Neutral sentiment is skipped here, as we can't get a better score\n# for neutral sentiment than just using text als selected_text.\n\ntrain_for_sentiments = ['negative', 'positive']\n\n## Start the training\nfor splitAtLength in splitAtLength_list:\n    for sentiment in train_for_sentiments:\n        # Gget model path and determin whether to train two or one model per sentiment based on \n        # the value of  'splitAtLength'. returns [model_path_long, []] if splitAtLength = 0. \n        # Returns two paths if splitAtLength > 0\n        [model_path_long, model_paths_short]  = get_model_output_path(\n            sentiment,\n            splitAtLength = splitAtLength,\n            train_val_split = train_val_split) \n\n        train(sentiment, \n              output_dir = model_path_long, \n              epochs = epochs_long, \n              model = None,\n              optimizer = optimizer,\n              dropout = dropout_rate, \n              splitAtLength = splitAtLength, \n              fill_no_predictions_with_text = fill_no_predictions_with_text)\n        \n        # Train for short model will only be executed if model_paths_short != [] \n        train(sentiment, \n              output_dir = model_paths_short, \n              epochs = epochs_short, \n              model = None,\n              optimizer = optimizer, \n              dropout = dropout_rate,\n              splitAtLength = splitAtLength, \n              fill_no_predictions_with_text = fill_no_predictions_with_text)","d1e2fa1c":"def load_best_models(splitAtLength):\n    '''\n    Loads the best models, predicting for each row in \"target_df\" and putting result in \"target_column\"\n    '''\n    BASE_PATH = f'..\/working\/'\n    BASE_PATH_BEST_LONG_POS = BASE_PATH + get_model_output_path(\"positive\", splitAtLength, train_val_split)[0] + \"_best\"\n    BASE_PATH_BEST_SHORT_POS = BASE_PATH + get_model_output_path(\"positive\", splitAtLength, train_val_split)[1] + \"_best\" if get_model_output_path(\"positive\", splitAtLength, train_val_split)[1] else \"no-model\"\n    BASE_PATH_BEST_LONG_NEG = BASE_PATH + get_model_output_path(\"negative\", splitAtLength, train_val_split)[0] + \"_best\"\n    BASE_PATH_BEST_SHORT_NEG = BASE_PATH + get_model_output_path(\"negative\", splitAtLength, train_val_split)[1] + \"_best\" if get_model_output_path(\"positive\", splitAtLength, train_val_split)[1] else \"no-model\"\n        \n    model_long_best_pos = spacy.load(BASE_PATH_BEST_LONG_POS) if os.path.isdir(BASE_PATH_BEST_LONG_POS) else None\n    model_short_best_pos = spacy.load(BASE_PATH_BEST_SHORT_POS) if os.path.isdir(BASE_PATH_BEST_SHORT_POS) else None\n    model_long_best_neg = spacy.load(BASE_PATH_BEST_LONG_NEG) if os.path.isdir(BASE_PATH_BEST_LONG_NEG) else None\n    model_short_best_neg = spacy.load(BASE_PATH_BEST_SHORT_NEG) if os.path.isdir(BASE_PATH_BEST_SHORT_NEG) else None\n    print(f'Models loaded:\\nModel_pos_long_best: {model_long_best_pos != None} \\n'\n          f'Model_pos_short_best: {model_short_best_pos != None} \\n'\n          f'Model_neg_long_best: {model_long_best_neg != None}\\n'\n          f'Model_neg_short_best: {model_short_best_neg != None}')\n    return [model_long_best_pos, model_short_best_pos, model_long_best_neg, model_short_best_neg]","f18642f2":"def fill_dataframe_with_predictions(splitAtLength, target_df, target_column, fill_no_predictions_with_text):\n    '''\n    Loads the best models, predicting for each row in \"target_df\" and putting result in \"target_column\"\n    '''\n    \n    [model_long_best_pos, model_short_best_pos, model_long_best_neg, model_short_best_neg] = load_best_models(splitAtLength)\n                 \n    # making it easier to deal with splitAtLength = None\n    if splitAtLength is None:\n        splitAtLength = 0\n            \n    target_df[target_column] = \"EMPTY\"\n        \n    for idx in target_df.index:\n        text = target_df.at[idx,'text']\n        sentiment = target_df.at[idx,'sentiment']\n        if sentiment == 'neutral':  \n            target_df.at[idx, target_column] = target_df.at[idx, 'text']\n            # positive sentiment    \n        elif sentiment == 'positive' and len(text.split()) > splitAtLength:\n            target_df.at[idx, target_column] = make_predictions(\n                                                                text,\n                                                                model_long_best_pos,\n                                                                fill_no_predictions_with_text) if model_long_best_pos != None else text  \n        elif sentiment == 'positive' and len(text.split()) <= splitAtLength:\n            target_df.at[idx, target_column] = make_predictions(text,\n                                                                model_short_best_pos,\n                                                                fill_no_predictions_with_text) if model_short_best_pos != None else text\n            # negative sentiment      \n        elif sentiment == 'negative' and len(text.split()) > splitAtLength:\n            target_df.at[idx, target_column] = make_predictions(text,\n                                                                model_long_best_neg,\n                                                                fill_no_predictions_with_text) if model_long_best_neg != None else text\n        elif sentiment == 'negative' and len(text.split()) <= splitAtLength:\n            target_df.at[idx, target_column] = make_predictions(text,\n                                                                model_short_best_neg,\n                                                                fill_no_predictions_with_text) if model_short_best_neg != None else text\n        else:\n            print('something is wrong with fillSubmissionDf()')\n    return target_df","fac12649":"## Fill val_set with our predictions\nval_set_with_preds_df = fill_dataframe_with_predictions(splitAtLength,\n                                                        val_set_df.copy(),\n                                                        target_column = 'pred_selected_text',\n                                                        fill_no_predictions_with_text = False)","728ed989":"## Evaluate our results: compare our models result (incl. empty predictions) with simply using text\nval_set_with_preds_df['Jaccard_prediction'] = \"EMPTY\"\nval_set_with_preds_df['Jaccard_prediction'] = val_set_with_preds_df.apply(lambda x: jaccard_score (str(x['selected_text']), str(x['pred_selected_text'])), axis = 1)\nval_set_with_preds_df['Jaccard_text_sel_text'] = val_set_with_preds_df.apply(lambda x: jaccard_score (str(x['text']), str(x['selected_text'])), axis = 1)\navg_Jaccard_val_set_pred_incl_empty_mean = val_set_with_preds_df[\"Jaccard_prediction\"].mean()\n[avg_Jaccard_neu_pred_val, avg_Jaccard_neg_pred_val, avg_Jaccard_pos_pred_val] = [val_set_with_preds_df[val_set_with_preds_df[\"sentiment\"] == sentiment][\"Jaccard_prediction\"].mean() for sentiment in [\"neutral\", \"negative\", \"positive\"]]\navg_Jaccard_val_set_pred = pd.Series(data = {'Avg Jaccard neutral': avg_Jaccard_neu_pred_val,\n                                             'Avg Jaccard negative': avg_Jaccard_neg_pred_val,\n                                             \"Avg Jaccard positive\": avg_Jaccard_pos_pred_val},\n                                     name = 'Jaccard Score per sentiment using our predictions' )\n\n\nnumber_no_preds = np.sum(val_set_with_preds_df[\"pred_selected_text\"] == \"NO-PREDICTION\")\n\n## Check empty predictions\nprint(f'We have {number_no_preds} (={number_no_preds\/val_set_with_preds_df.shape[0] * 100:.1f}%) empty predictions!')  \n\nprint(f\"\\nUsing our predictions (incl. empty predictions): \\n\" + \"-\" * 25)\nprint(f\"Overall avg. Jaccard Score: {avg_Jaccard_val_set_pred_incl_empty_mean:.3f}\")\nprint(avg_Jaccard_val_set_pred)\n\n[avg_Jaccard_neu_using_text_val, avg_Jaccard_neg_using_text_val, avg_Jaccard_pos_using_text_val] = [val_set_with_preds_df[val_set_with_preds_df[\"sentiment\"] == sentiment][\"Jaccard_text_sel_text\"].mean() for sentiment in [\"neutral\", \"negative\", \"positive\"]]\navg_Jaccard_val_set_using_text_mean = val_set_with_preds_df[\"Jaccard_text_sel_text\"].mean()\navg_Jaccard_val_set_using_text= pd.Series(data = {'Avg Jaccard neutral': avg_Jaccard_neu_using_text_val, \n                                                  'Avg Jaccard negative': avg_Jaccard_neg_using_text_val, \n                                                  \"Avg Jaccard positive\": avg_Jaccard_pos_using_text_val}, \n                                          name = 'Jaccard Score per sentiment using our text as selected_text' )\n\nprint(f\"\\n \\n\" + \"Simply using text as selected_text: \\n\" + \"-\" * 25)\nprint(f\"Overall avg. Jaccard Score: {avg_Jaccard_val_set_using_text_mean:.3f}\")\nprint(avg_Jaccard_val_set_using_text)","30652c4a":"val_set_with_preds_full_df = val_set_with_preds_df[['text', 'selected_text', 'pred_selected_text', 'Jaccard_text_sel_text', 'Jaccard_prediction', 'sentiment', 'TEXT_number_of_words', 'SELECTED_TEXT_number_of_words']].copy()","44ebf4a3":"## Check only our preds and drop no predictions to evaluate quality of our predictions \nval_set_only_non_empty_preds_df = val_set_with_preds_df[val_set_with_preds_df[\"pred_selected_text\"] != \"NO-PREDICTION\"]\n[avg_Jaccard_neu_only_preds, avg_Jaccard_neg_only_preds, avg_Jaccard_pos_only_preds] = \\\n    [val_set_only_non_empty_preds_df[val_set_only_non_empty_preds_df[\"sentiment\"] == sentiment][\"Jaccard_prediction\"].mean() for sentiment in [\"neutral\", \"negative\", \"positive\"]]\n\navg_Jaccard_val_set_only_preds = pd.Series(data = {'Avg Jaccard neutral only preds': avg_Jaccard_neu_only_preds,\n                                             'Avg Jaccard negative only preds': avg_Jaccard_neg_only_preds, \n                                             \"Avg Jaccard positive only preds\": avg_Jaccard_pos_only_preds}, \n                                           name = 'Jaccard Score per sentiment using our predictions' )\navg_Jaccard_val_set_only_preds_mean = val_set_only_non_empty_preds_df[\"Jaccard_prediction\"].mean() \n\nprint(f\"Using our predictions without empty predictions: \\n\" + \"-\" * 25)\nprint(f\"Overall avg. Jaccard Score: {avg_Jaccard_val_set_only_preds_mean:.3f}\")\nprint(avg_Jaccard_val_set_only_preds)","6b444ab0":"## Use our predictions where available. Use TEXT if our model only gave us an empty prediction.\nval_set_with_preds_full_df['Jaccard_prediction_filled'] = 'empty'\nfor idx in val_set_with_preds_full_df.index:\n    text = val_set_with_preds_full_df.at[idx,'text']\n    pred_selected_text = val_set_with_preds_full_df.at[idx,'pred_selected_text']\n    Jaccard_own_pred = val_set_with_preds_full_df.at[idx,'Jaccard_prediction']\n    Jaccard_text_as_sel_text= val_set_with_preds_full_df.at[idx,'Jaccard_text_sel_text']\n    if pred_selected_text != 'NO-PREDICTION':\n        val_set_with_preds_full_df.at[idx,'Jaccard_prediction_filled'] = Jaccard_own_pred\n    elif pred_selected_text == 'NO-PREDICTION':\n        val_set_with_preds_full_df.at[idx,'Jaccard_prediction_filled'] = Jaccard_text_as_sel_text\n    else:\n        print('something wrong with predicting filled Jaccard Scores')\n[avg_Jaccard_neu_pred_filled_val, avg_Jaccard_neg_pred_filled_val, avg_Jaccard_pos_pred_filled_val] = \\\n    [val_set_with_preds_full_df[val_set_with_preds_full_df[\"sentiment\"] == sentiment][\"Jaccard_prediction_filled\"].mean() for sentiment in [\"neutral\", \"negative\", \"positive\"]]\n\navg_Jaccard_val_set_pred = pd.Series(data = {'Avg Jaccard neutral filled': avg_Jaccard_neu_pred_filled_val,\n                                             'Avg Jaccard negative filled': avg_Jaccard_neg_pred_filled_val, \n                                             \"Avg Jaccard positive filled\": avg_Jaccard_pos_pred_filled_val}, \n                                             name = 'Jaccard Score per sentiment using our predictions' )\nval_set_with_preds_full_df_mean = val_set_with_preds_full_df[\"Jaccard_prediction_filled\"].mean()\nprint(f\"Using our predictions and fill up empty predictions with 'text': \\n\" + \"-\" * 25)\nprint(f\"Overall avg. Jaccard Score: {val_set_with_preds_full_df_mean:.3f}\")\nprint(avg_Jaccard_val_set_pred)","2a77b18f":"## Drop neutral sentiments and calc. difference for number of words and Jaccard\nval_set_with_preds_no_neutral_df = val_set_with_preds_full_df[val_set_with_preds_full_df[\"sentiment\"] != \"neutral\"]\nval_set_with_preds_no_neutral_df['PREDICTIONS_number_of_words'] = val_set_with_preds_df['pred_selected_text'].str.split().str.len()\n\nval_set_with_preds_no_neutral_df['Diff_Jaccard_pred_vs_text'] = val_set_with_preds_no_neutral_df.apply  (lambda x: np.subtract((x['Jaccard_prediction']), (x['Jaccard_text_sel_text'])), axis = 1)\nval_set_with_preds_no_neutral_df['Diff_len_pred_sel_text'] = val_set_with_preds_no_neutral_df.apply(lambda x: np.subtract((x['PREDICTIONS_number_of_words']), (x['SELECTED_TEXT_number_of_words'])), axis = 1)\nval_set_without_no_preds = val_set_with_preds_no_neutral_df[val_set_with_preds_no_neutral_df[\"pred_selected_text\"] != \"NO-PREDICTION\"]","a0977752":"val_set_without_no_preds.nsmallest(5, 'Diff_Jaccard_pred_vs_text')","f56f04c8":"val_set_without_no_preds.nlargest(5, 'Diff_Jaccard_pred_vs_text')","69bf9767":"## Evaluate avg. difference of number of words for worst predictions\navg_Diff_number_of_words = val_set_without_no_preds['Diff_len_pred_sel_text'].abs().mean()\nn_preds = 100\navg_sel_text_number_of_words_worst = val_set_without_no_preds.nsmallest(n_preds, 'Diff_Jaccard_pred_vs_text')['SELECTED_TEXT_number_of_words'].mean()\n# Check for avg number of words in wrost and best predictions (compared to simply using text)\navg_Diff_number_of_words_worst = val_set_without_no_preds.nsmallest(n_preds, 'Diff_Jaccard_pred_vs_text')['Diff_len_pred_sel_text'].abs().mean()\navg_Diff_number_of_words_best = val_set_without_no_preds.nlargest(n_preds, 'Diff_Jaccard_pred_vs_text')['SELECTED_TEXT_number_of_words'].abs().mean()\nnumber_diff_len_greater_5 = len(val_set_without_no_preds[val_set_without_no_preds[\"Diff_len_pred_sel_text\"].abs() > 5])\nfraction_number_diff_len_greater_5 = number_diff_len_greater_5 \/ val_set_without_no_preds.shape[0]\n\n\nprint(f'The avg. difference of number of words between the selected_text and our prediction is {avg_Diff_number_of_words:.1f}.\\n'\n      f'The avg. difference of number of words for our WORST {n_preds} predictions is {avg_Diff_number_of_words_worst\/avg_Diff_number_of_words:.1f}x higher: {avg_Diff_number_of_words_worst:.1f}!\\n\\n'\n      f'We have {number_diff_len_greater_5} rows (= {fraction_number_diff_len_greater_5 * 100:.1f}%) with a difference bigger than 5 words!\\n\\n'\n      f'Avg. number of words in \"selected_text\" for our WORST predictions is: {avg_Diff_number_of_words_worst:.1f},' \n      f\" while it's only {avg_Diff_number_of_words_best} for our best predictions.\")","34f3ede5":"fig = plt.figure(figsize = (18,8) )\naxes = plt.axes()\nplot1 = sns.lineplot(x = 'TEXT_number_of_words', y = 'Diff_Jaccard_pred_vs_text',  label = 'Difference Jaccard pred to text', data = val_set_without_no_preds, ci = None)\nplot1 = sns.lineplot(x = 'TEXT_number_of_words', y = 'Jaccard_prediction', label = 'Jaccard for our predictions', data = val_set_without_no_preds, ci = None)\nplot1 = sns.lineplot(x = 'TEXT_number_of_words', y = 'Jaccard_text_sel_text', label = 'Jaccard for text as selected_text', data = val_set_without_no_preds, ci = None)\naxes.set(ylabel = 'Jaccard Score', xticks = range(1,32))\naxes.lines[0].set_linestyle(\"--\")\ncache = plt.legend() \nfig.suptitle('Jaccard Scores over text length: Our predictions vs simply taking text', fontsize = 20)\nsns.set(font_scale = sns_default_font_scale) ","7c291339":"def make_final_predictions(splitAtLength, target_df, target_column):\n    '''\n    Loads the best models, predicting for each row in \"target_df\" and putting result in \"target_column\"\n    OPTIMIZE HERE for better predictions!\n    '''\n    \n    [model_pos_long_best, model_pos_short_best, model_neg_long_best, model_neg_short_best] = load_best_models(splitAtLength)\n                 \n    # Making it easier to deal with splitAtLength = None\n    if splitAtLength is None:\n        splitAtLength = 0\n            \n    target_df[target_column] = \"EMPTY\"\n        \n    for idx in target_df.index:\n        text = target_df.at[idx,'text']\n        sentiment = target_df.at[idx,'sentiment']\n        \n        if sentiment == 'neutral':  \n            target_df.at[idx, target_column] = target_df.at[idx, 'text']\n            # positive sentiment    \n        elif sentiment == 'positive' and len(text.split()) > splitAtLength:\n            target_df.at[idx, target_column] = make_predictions(text,\n                                                                model_pos_long_best,\n                                                                fill_no_predictions_with_text = True) if model_pos_long_best != None else text  \n        elif sentiment == 'positive' and len(text.split()) <= splitAtLength:\n            target_df.at[idx, target_column] = make_predictions(text,\n                                                                model_pos_short_best,\n                                                                fill_no_predictions_with_text = True) if model_pos_short_best != None else text\n            # negative sentiment      \n        elif sentiment == 'negative' and len(text.split()) > splitAtLength:\n            target_df.at[idx, target_column] = make_predictions(text,\n                                                                model_neg_long_best,\n                                                                fill_no_predictions_with_text = True) if model_neg_long_best != None else text\n        elif sentiment == 'negative' and len(text.split()) <= splitAtLength:\n            target_df.at[idx, target_column] = make_predictions(text,\n                                                                model_neg_short_best,\n                                                                fill_no_predictions_with_text = True) if model_neg_short_best != None else text\n        else:\n            print('something is wrong with make_final_predictions()')\n    return target_df","9b91bd32":"# Get final predictions - do optimizations in hidden code block above\nfilled_test_df = make_final_predictions(splitAtLength, \n                                        test_df.copy(),\n                                        target_column = \"selected_text\")","911c36ef":"# Create submission_csv\nsubmission_df = pd.read_csv(DATA_BASE_PATH + 'sample_submission.csv')\nsubmission_df['selected_text'] = filled_test_df[\"selected_text\"]\nsubmission_df.to_csv(\"submission.csv\", index = False)\ndisplay(submission_df.head(10))","fcf97a13":"## 3.3 Which words in our data are *NOT* in a vocab?\n","7b3a5bbb":"### Checking for bad predictions","9e1b2bbc":"### Quick Intro\n\nThis is my first Kaggle competition! After doing the [kaggle.com spaCy-course for NLP](https:\/\/www.kaggle.com\/learn\/natural-language-processing) I felt inspired to try out my new knowledge in this competition.\nAdditionally there is an almost \"ready for use\" code example for using spaCy NAMED ENTITIY RECOGNIZER: https:\/\/spacy.io\/usage\/training#ner\n\nPlease feel free to give me some hints for improvements in all categories: making code more \"pythonic\", better plots, ideas for post-processing, etc.\n\nWarning: Following text may contain Star Trek references.\n\n\nFinal edit:\nAdded some insights, finalized wording, optimized section order slightly.","949bde5a":"That actually looks quite good. But before we create our final submission file, we should investigate one more thing:","655dc7db":"## 2.5 Let's check how the difference of length relates to the length of the tweet's text","7484e4ad":"**Findings:**  \n* In above plot we can see that for almost all lengths of the tweet our non-empty predictions are better than simply taking text. Sadly this does not allow us to draw a clever conclusion on which tweet-length to split our models or for which tweet-length simply taking ```text``` as prediction is an improvement for our overall performance.\n* The big jump on the right end of the plot is because we only have two tweets longer than 30 words and we were quite right with our predictions for them.","8a5bbecd":"We got **between 15% and 40% EMPTY PREDICTIONS**! That's the core reason why **spaCy NER model does not perform well on this task**. We can't score over 66,X% on the Leaderboard so far.  \n\nBut what about the quality of our (non-empty) predictions? If we now drop the empty-predictions and just focus on the rows, where we actually made a prediction, what's our models' performance then?","5026238c":"## 2.4 How is len(```selected_text```) related to len(```text```) per sentiment?\n","48f868ea":"# Table of Contents\n1. **Preparations**\n   - 1.1 Importing dependencies & loading data-sets\n   - 1.2 First glimps on data and dealing with missing values\n2. **Explorative Data Analysis**\n   - 2.1 Distribution of sentiments in train and test data.\n   - 2.2 Distribution of number of words in train data\n   - 2.3 Distribution of number of words per sentiment in train data\n   - 2.4 How is len(selected_text) related to len(text) per sentiment?\n   - 2.5 Check how the difference of length relates to the length of the tweets \n3. **Content & language analysis**\n   - 3.1 Interpunctuation & special characters\n   - 3.2 Slang, wording, typos & unusual words-analysis\n   - 3.3 Which words in our data are *NOT* in a vocab?\n   - 3.4 Jaccard Scores for taking \"text\" as \"selected_text\" in train-data\n   - 3.5 Submission: Setting the Baseline Score: Jaccard Scores for using \"TEXT\" as \"SELECTED_TEXT\"\n4. **Modelling & predicting with spaCy**\n   - 4.1 Modelling with spaCy\n   - 4.2 Evaluating the achieved predictions\n5. **Conclusion**\n6. **Creating submission_df to hand in our solution!**","c3a5a9ce":"**Findings for large spaCy vocab:**\n* Train ```text``` and train ```selected_text``` have rougly 85% common words with spaCys big vocab. That still makes roughly 15% of words NOT contained in the biggest spaCy vocab.\n* The test ```text``` has a higher overlap: almost 93% of all words are also available in spaCys vocab, leaving just ~7% of \"new\" words.\n* If we now check the bar on the right: TEST and TRAIN ```text``` seem to be quite different, as they have only ~75% overlap!\n\nOne last step before we focus on the words, which are NOT in the spaCy vocab:    \nWhat happens, if we check for the fraction of the *cleaned* dataset of ```train['selected_text']``` which is also in ```train[text]```?  \nShould be 100%, as ```selected_text``` is derived from ```text```, right?    \nNo! As we saw earlier, the ```selected_text``` is sometimes strangely cut off. Let's check that quickly before we move on.","19ea75c4":"## 2.1 Distribution of sentiments in train and test data","efd86bd9":"# 5 Conclusion\nspaCy seemed to be a cool idea for my first try in a competition, but the overall model performance stays behind my expectations. Overall (without post-processing) we can achieve roughly a score of 66.4 %, which is not too impressive.\n\nMy journey was the following:\n* First I tried to train just one model for positive AND negative sentiment, using \"positive\" and \"negative\" as the \"entity\" part of the labels (reminder: label consists out of [start, end, entity]). Sadly the performance of this single model was only roughly 62%.\n* Then I split up the models: one model for positive and one for negative sentiment. The performance increased up to 64,4% and I saved the top 3 models per sentiment.\n* Next, I tried to split it up even more: I introduced ```splitAtLength``` and split the tweets basend on the number of words: I trained one model per sentiment for the tweets with a length up to ```splitAtLength``` and one model for tweets longer than ```splitAtLength```. I did a grid search for all values for ```splitAtLength``` between 2 and 28 in steps of two. Sadly this could not outperform the 64,4%.\n* In a next step, I created my own entities: I created bins\/buckets in steps of 3, 5 or 10 words and added it to the sentiments. For bins of 5, the data then looked like  \n        \"this is a short tweet\"                        entitiy:  \"positive[0-5]\"  \n        \"this is a tweet with more than 6 words!!\"     entitiy:  \"positive[6-10]\"\n\nUsing those bins\/buckets was basically a generalized version for the here published ```splitAtLength```. But again: the best performance stayed behind the 64,4%. If somebody is interested in the (still a little messy) code for creating and using those bins, just let me know and I will gladly clean it up and share it.\n\nWe have seen and analyzed a lot about the given data and it's quite clear that not a human was creating the ```selected_text``` column. It feels much more like we are trying to reverse-engineer and re-build a not-too-good-working existing AI-algorithm (incl. post-processing-war) which created the faulty, noisy, cut-off ```selected_text``` column and which clearly needs to be taught some proper Star Trek phrases (see chapter 2, Top list of failed ```selected_text``` examples).\n\nFurther improvements can be realized with the following:  \n-Using Cross Validation to enhancing training and selection of the best model.\n-Merging the top 3 models (Ensemble!)  \n-Switching to a more suiteable model, like BERT + custom heads. Probably a transformer with some custom-heads will win this competition.\n-Using post-processing, especially for interpunctuation.\n\nMy next step will propably be to try out some Tensorflow & Keras BERT models.\nThanks a lot for reading and please feel free to share your thoughts on the models, the code (I am still learning) and my analysis.\n\nSo, whats left? The prediction for our submission of course. So let's load the needed modules:    \n\n<span style=\"color:darkgreen ;font-family: Impact; font-size:13;\"> from  <\/span> coffee  <span style=\"color:darkgreen ;font-family: Impact; font-size:13;\"> import  <\/span> *****   \n\nLet's do this!","75cc4648":"**Quick explanation**:  \nThe KDE (kernel density estimation) is an *estimate* (based on our train_df as a sample) for the probability density function (PDF).  \nThe probability density function \"can be interpreted as providing a relative likelihood that the value of the random variable would equal that sample\" (perfect wording, found on Wikipedia)\n\n**Findings:**\n* The plotted KDEs show:\n* For **neutral**-sentiment the (estimated) probability for each length for ```text``` and ```selected_text``` is almost the same.\n* For **positive** and **negative**-sentiment, the KDE is strongly right-skewed (having a long right trail) and short ```selected_text```s do have the highest estimated probability.\n\nThis is not yet telling us the *correlation* between len(```text```) and len(```selected_text```), but shows that neutral is behaving very differently from negative and positive, while positive and negative KDE is quite similar.  \nSo let's dig deeper:\n* How is the len(```selected_text```) related to len(```text```) per sentiment?\n* Let's use the difference between len(```text```) and len(```selected_text```) as a measure of correlation to gain another point of view.\n\n<font size=\"0.6\">\nSide note for statistical purists:   \nUsing a KDE or PDF above is actually not the perfect solution. As we have a discret (and not a continuous) variable (length of text), we should go for a probability *distribution* function rather than for a *density* function. You could do that with searborn using \"sns.distplot(train_df, kde = False)\". As the graphical result is optically harder to interprete, I allowed myself to use the less perfect solution using a PDF here.\n<\/font>","d258f40c":"In the next step we calculate the fraction of words which are available in both, our data *and* the spaCy model, aswell as the fraction which is available in train *and* test ```text```.","2ba08bb2":"**Findings:**\n* While overall only 2251 tweets are affected by starting or ending with a special (and unexpected) character, we can still see that most of them are in non-neutral sentiment tweets. Roughly 8% of all non-neutral sentiment tweets are affected!\n* Special characters and interpunctuation patterns might very well have a big impact on our model creation and overall score - most certainly they will be a big focus in the post processing.\n\nBtw: There is a good notebook counting \"lonely\" interpunctuation-characters here:\n[What The No*ise?](https:\/\/www.kaggle.com\/debanga\/what-the-no-ise) by Zenfiy\n","e50f8cff":"## Starting the training!\nIn the following part we can set the hyperparameters and start the training.","d7854998":"**Quick explanation**:  \nThis ```sns.lineplot``` shows the *mean difference* of len(```selected_text```) and len(```text```) over the tweets len(```text```). For more info see: [Docs](https:\/\/seaborn.pydata.org\/generated\/seaborn.lineplot.html).\n\n**Findings:**\n* For **neutral**-sentiment the mean difference in length is very small and again underlining our initial assumption that ```text``` might be a good estimate for ```selected_text```. However, we can see again that for tweets with around 22-29 words we have more bumps and a bigger mean difference in length.\n\n* For **positive** and **negative**-sentiment, the mean difference in length is strongly scaling with len(```text```): So we can expect that we have to predict more often shorter ```selected_text```for longer tweets when the sentiment is not **neutral**. Furthermore we can see much more bumps in the mean len(```selected_text```).\n\nTo get a better feeling of how the difference in length is distributed, let's check the distribution with a boxplot diagram.","3830a1b4":"**Positive** values in the column ```Diff_Jaccard_pred_vs_text``` mean: our prediction is **better** than simply taking ```text``` as ```selected_text```.  \n**Negative** values in the column ```Diff_Jaccard_pred_vs_text``` mean: our prediction is **worse** than simply taking ```text``` as ```selected_text```.  \n**Zero** values in the column ```Diff_Jaccard_pred_vs_text``` mean: our prediction is **equal to** simply taking ```text``` as ```selected_text```.\n\nSo let's see some rows to get a clue what to look for.","f628d754":"# 1. Preparations\n## 1.1 Importing dependencies & loading data-sets\n<span style=\"color:darkgreen ;font-family: Impact; font-size:13;\"> from  <\/span> coffee  <span style=\"color:darkgreen ;font-family: Impact; font-size:13;\"> import  <\/span> ***** ","dba2befe":"Okay, on average: IF we have a prediction, our prediction is significantly better than simply taking ```text```. So, let's decide how to deal with the empty predictions: We can use ```text``` as prediction wherever our model didn't predict anything. What's our overall score if we fill the empty predictions with ```text```? That's definitely an improvement and results in the following scores.","939c0843":"**Findings:**\n* Most ```selected_text``` entries consist of less than 10 words.\n* The most frequently observed length of ```selected_text``` is just one word.\n* Most tweets ```text``` length is between 5 and 25 words.\n* As many other notebooks described, the ```text``` length in TEST-data is similar to what we see in train-data.","a7b7b8fc":"** Findings:**\n\n**Performance\/speed:**\ntqdm is providing a nice progress bar and we can see the following: \n* The initial epoch (called iteration by tqdm by default) takes the longest: model needs to be loaded and set up. Afterwards the needed time per iteration is decreasing. One of the reasons for this is, that spaCys compounding (see train function) is increasing the batch size each iteration.\n* CPU training is quite fast: ~20-25 (excl. roughly 5 sec. of validation) seconds per epoch, leaving us roughly with 25 min for 50 epochs (incl. validation) per sentiment.\n\n**Predictions:**\n* We always save the top 3 models which we get during the training. There is (given a train_val_split = 0.9) always a 3 digit number of NO-PREDICTIONs (= empty predictions). This we will further investigate in the following parts and is the core issue for now.\n* We get most of the good model update-results (= updated & saved models) in the early (<50) epochs of the training, and not after 100+ epochs.\n\nIn the following we are going to evaluate the achieved predictions in more detail.\n","688d6c1a":"### First orientation: Let's get some hints on what to look for in our EDA.\n\nLet's run the line below a few times and check some rows to get some hints on what to look for & learn about the data.\nBased on this, we can structure our further analysis.","cd633f56":"After defining the structure of what we want to look at, let's start with analysis no.1.","89e99392":"**Findings:**\n* If we use the (hidden) above code for creating a submission_df using ```text``` as ```selected_text```, we get our Baseline-Score.  \n* We get as <span style=\"color:red\">Baseline a Jaccard Score of 59,4% for our submissions! <\/span> That's actually a lot!  \nFunfact: I am quite sure we can create an overfitting model which performs worse than that!","c3553a94":"So part of our task is to teach an algorithm the following realtionship, which is not clearly understandable for humans:\n<img width=\"100\">![IHAT.PNG](attachment:IHAT.PNG)","a67b0d9c":"# Competition description\nWith all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company or a brand positively or negatively. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.  \nThe objective in this competition is to construct a model that can look at a given tweet and the already assigned sentiment and figure out what word or phrase best supports the sentiment. What words in tweets support a positive, negative, or neutral sentiment? How can you help make that determination using machine learning tools?\n\n### What should I expect the data format to be?\nEach row in the dataset contains the text of a tweet and a sentiment label. In the training set we are provided with a word or phrase drawn from the tweet (selected_text) that determins and encapsulates the provided sentiment.\n\n### What am I predicting?\nWe are attempting to predict the word or phrase from each tweet that determines and exemplifies the provided sentiment. The word or phrase should include all characters within that span (i.e. including commas, spaces, etc.). It's only one span per tweet: the word or phrase starts somewhere and ends somewhere without being interrupted in between.\n\nFor example:\n\n> 2,  \"very good\"  \n> 5,  \"I am neutral about this\"  \n> 6,  \"bad\"  \n> 8,  \"if you say so!\"  \n> etc.  ","5ece00c6":"In the next steps we are going to create a lot of (hidden) helper functions.\n\nPerformance-related side note:  \nUsing  ```for idx in train_set_df.index```  \nto iterate over train data is roughly 8 to 9 times faster than using \n```for index, row in train_set_df.iterrows()```.","24a817eb":"# 3 Content & language analysis\n### What we are going to look for:\n1) Interpunctuation & special characters. Lets find out what their impact is.  \n2) Slang, wording & typos - let's find words that are uncommon in a typical web-dictonary. ","f5e86de7":"**Quick explanation**:  \n![](https:\/\/i2.wp.com\/flowingdata.com\/wp-content\/uploads\/2008\/02\/box-plot-explained.gif?w=1090)\n[Source for picture](https:\/\/flowingdata.com\/2008\/02\/15\/how-to-read-and-use-a-box-and-whisker-plot\/)\n\nMedian: The median is dividing the data into two equally sized halves, so that half of the tweets are shorter than the median and half are longer.\n\n**Findings:**\n* For **neutral**-sentiment: The median is 0 and so is the upper quartil. We have quite some outliers but our hypothesis of ```text``` beeing a good estimator for ```selected_text``` is again supported.\n\nFor **positive** and **negative**-sentiment: \n1.  The boxplots are almost identical: Half of the difference between len(```selected_text```) and len(```text```) is smaller than 9 words, the other half is bigger.\n2. 25% of the tweets have a difference in length bigger than 15 (upper quartile).\n3. 25% of the tweets have a difference in length smaller than 4 (lower quartile).\n\nWe have seen and learned a lot about the given dataset which we can use for building the model and for post processing.\nAs a next step, let's go for some content and language analysis.\n\nNote: if you want to see the boxplots for ```text``` and ```selected_text```, you just need to expand above code cell and do a minor copy-paste change, as indicated in the comment.","80e9e775":"I first created a histo using matplotlib, but it took me many lines of code and looked ugly. If you want to see it: comment-in the hidden code. **Using seaborn** seems to be the way to go: fast and nice.\nCheck out the following links for <span style=\"color:orange\"> **Seaborn Tutorials & examples**: <\/span>  \n\nhttps:\/\/www.kaggle.com\/kralmachine\/seaborn-tutorial-for-beginners  \nhttps:\/\/www.geeksforgeeks.org\/kde-plot-visualization-with-pandas-and-seaborn\/    \nhttps:\/\/seaborn.pydata.org\/generated\/seaborn.kdeplot.html  ","7ae00c47":"## 3.4 Jaccard Scores for using \"TEXT\" as \"SELECTED_TEXT\" in TRAIN-data","381932b2":"### Let's check where our (non-empty) predictions were significantly worse than simply taking ```text```. Maybe there is a pattern?\nFor this analysis we are going to drop neutral values again, as we are using ```text``` for predicting them later anyway.","1a454a18":"**Findings:**\n\n* For our **worst** predictions (compared to simply using ```text```) the difference in number of words between ```selected_Text``` and our prediction ```pred_selected_text``` is much higher than average! So for those we either predict far too few words, or far too many, but we don't get the (roughly) right number.\n* So one of the major issues in our model is not only predicting wrong interpunctuation, but not getting the right number of words and the right segments.\n* Our 100 worst predictions were made for ```selected_text``` with (on average) 11-14 words, while our best predictions have only ~1 word as ```selected_text```. That's reason enough to check if we can find a relationship between our model's performance and the number of words of the tweet.\n\nLet's plot the Jaccard Scores for simply using ```text``` and for our predictions over the number of words in a tweet.","e1708f40":"**Findings:**\n* For **neutral**-sentiment the Jaccard Score is very close to 1. As we are already around scoring 98% if we simply take ```text``` as our prediction, we don't even need to train a model for this in a first step.\n* For **positive** and **negative**-sentiment the Jaccard Score is 31% and 33% respectively - here we should focus our work and check the performance of the models first.","71c1149b":"## 3.2 Slang, wording & typos\nThe idea in the next few lines of code is to compare our words in ```text``` and ```selected_text``` to a big existing vocab from spaCy. With this we can identify unusual words and on our way to achieve this we can see how many unique words (stripped from interpunctuation) our dataset contains.\n\nBut: why spaCy? \n1. The [vocab from the large spaCy model](https:\/\/spacy.io\/models\/en#en_core_web_lg) consists of the corpus from [OntoNotes](https:\/\/catalog.ldc.upenn.edu\/LDC2013T19)\/CommonCrawl containing data from blogs, news, comments and webblogs. For this task this seems to be the better solution than going for a vocab derived from perfectly written and reviewd Wikipedia-articels.\n2. We are anyway going to use a spaCy model later ;).\n\nLet's have a look if special-language is common in ```selected_text```: let's check, which words in selected_text are NOT available in spaCys biggest dictionary.","11886f8a":"# 4 Model\nI did the [kaggle.com spaCy-course for NLP](https:\/\/www.kaggle.com\/learn\/natural-language-processing) and the course inspired me to try out spaCy for this competition.\nAdditionally there is an almost \"ready for use\" code example for using spaCy NAMED ENTITIY RECOGNIZER: https:\/\/spacy.io\/usage\/training#ner\n\nIn this section we are modelling a solution for the given task using spaCy NAMED ENTITIY RECOGNIZER as also done here (thanks a lot for additional inspriation!):\n\n* https:\/\/www.kaggle.com\/rohitsingh9990\/ner-training-using-spacy-0-628-lb  \n* https:\/\/www.kaggle.com\/tanulsingh077\/twitter-sentiment-extaction-analysis-eda-and-model#Modelling  \n* SpaCy-Docs and source of most code-snippets: https:\/\/spacy.io\/usage\/training#ner\n\nLet's start with splitting our data into a train and a validation set.","dfcd275d":"In the following we evaluate our results: Let's check for\n\n* number of empty predictions (no predictions).\n* average Jaccard Score of our best models (incl. empty predictions) compared to avg Jaccard of simply using text.\n* performance of our models' predictions (excl. empty predictions)","10d5c5ec":"**Quick explanation**:  \nThe ```sns.lineplot``` shows the *mean* len(```selected_text```) over len(```text```). For more info see: [Docs](https:\/\/seaborn.pydata.org\/generated\/seaborn.lineplot.html).\n\n**Findings:**\n* For **neutral**-sentiment the mean len(```selected_text```) is strongly scaling with len(```text```). We can see that the slope is mostly constant and roughly 1. This underlines our assumption, that ```selected_text``` is mostly equal to ```text``` for **neutral**-sentiment.\n* For **positive** and **negative**-sentiment, the mean len(```selected_text```) is also increasing with len(```text```), but with a slope much smaller than 1. We can also see much more jumps in the mean len(```selected_text```) for longer tweets. In the next step, let's check out these jumps: let's first plot the difference in length for ```text``` and ```selected_text``` and then plot the **distribution** of the difference in length. Most probably the jumps derive from having a wide range of len(```selected_text```) for longer tweets.","d8081ef7":"# 6 Creating submission_df to hand in our solution!","908b7659":"# 2 Exploratory Data Analysis\n#### As there are already some very good EDA-focused notebooks, I try to minimize EDA here to what is *new* and necessary and remove some stuff which I did along the path.\nHere my top favorites for a more complete EDA:  \n[ EDA and Preprocessing for BERT](https:\/\/www.kaggle.com\/parulpandey\/eda-and-preprocessing-for-bert) by Parul  \n[Complete EDA & Baseline model [0.708 LB] ](https:\/\/www.kaggle.com\/shahules\/complete-eda-baseline-model-0-708-lb) by Shahules786  \n[Twitter sentiment Extaction-Analysis,EDA and Model ](Twitter sentiment Extaction-Analysis,EDA and Model) by MrKnowNothing","d300680b":"## 3.5 Setting the Baseline Score: Jaccard Scores for using \"TEXT\" as \"SELECTED_TEXT\" in TEST-data","7953c85e":"### Some quick findings:\n1. ```selected_text```  of **neutral** tweets seems to be **longer** than for positive or negative tweets.\n2. ``` selected_text```  of **neutral** tweets seems to have a very **high overlap** \/ Jaccard Score with ``` text```. They also seem to have a higher overlap than positive and negative tweets.\n3. ``` selected_text```  **contains interpunctuation** in general - removing it would not be a good idea. \n4. Some ``` selected_text```  entries **contain unexpected interpunctuation**.   \n> E.g. line 9079: ``` seletect_text```  = *\". Probably spelt it wrong lol.\"*\n5. Some ``` selected_text```  entries **contain unexpected chars** and\/or are **cut off**:   \n> E.g. line 751: ``` seletect_text```  = *\"t sucks\"*\n\n\n### So what are we going to look for:\n1. Distribution of **sentiments** in train and test data.\n2. Distribution of **number of words in train**_df.\n3. Distribution of **number of words per sentiment** category.\n4. **Difference between len(``` text``` ) and len(``` selected text``` )** per sentiment: As we can see at that least **neutral** sentiments look rather special (see finding no.2).\n5. ```selected_Text```  **Content & Language Analysis**: interpunctuations, special characters, slang and unique data-set words which are *not* available in a big vocab\/dictionary\n6. **Avg. Jaccard Score** for each sentiment if we use the complete ``` text```  as ``` selected_text```  for training data --> this will give us a lower boundary for our model's estimations.\n7. **Avg. public Jaccard Score** for each sentiment if we use the complete ``` text```  as``` selected_text``` --> this will give us a Baseline Jaccard Score (lower boundary) for our model's estimations.\n\n\n### Due to many funny\/sad or surprising rows, I felt the urge to start a short top list:","3e846694":"## 4.2 Evaluating the achieved predictions","77db61dd":"**Findings:**\n* Of course there can't be a notebook without word-clouds if it's really about NLP.\n* The first three WordClouds compare ```train[text]```, ```train['selected_text']``` and ```test[text]``` to (large) spaCy vocab. Quite some interesting words are unique in our datasets: followfriday, sanctuarysunday, Musicmonday, starwarsday, twittierville, tweetup.\n* The rightmost WordCloud compares: ```test[text]``` to  ```train[text]``` and again we find some interesting unique words.\n* We can see that most of the unique words are about starwars or some other typical hashtag-stuff.\n\nThanks again [Parul](https:\/\/www.kaggle.com\/parulpandey\/eda-and-preprocessing-for-bert) for sharing a perfect blue-print for Word-Clouds!\n\nIn the next section we deal with the Jaccard Score.\n\nBefore we go:\n\nRemember:\n* The computation of the Jaccard Score happens at the word level.\n* The order of words doesn't matter. Same applies for upper or lower case: \"yes\" is the same as \"YES\".\n* Interpunctuation counts: \"YES!\" is not the same as \"YES\"\n* The overall evaluation metric of this competition is the mean Jaccard Score over all the entries.\n\nLets do two things:  \n1) Let's calculate the Jaccard Score in *train-data* for each sentiment which we can obtain if we simply use ```text``` as ```selected_text```.  \n2) Defining the Baseline Score: Let's check which Jaccard Score we get, when we use ```test[text]``` as ```test[selected_text]``` for our test_df and submit this as our solution.","aaec7e93":"Nothing missing in test_data.  \nOnly 2 missing values (in the same row) in train data.\nAs just one row (#314) of the data contained missing values, in-place dropping of that row is ok.","23cf5481":"Let's clean the data(remove special characters and interpunctuation) before we try to identify unique words, so that \"*...what?*\" is not matched as \"unique\" but is the same as \"*what*\".","7d5cf6f1":"### Checking for missing values","39f9ceb3":"**Findings:**\n* **Neutral**: 40,5%  \n* **Positive**: 31,2%  \n* **Negative**: 28,3%**  \n\n* **TEST and TRAIN data** share **almost identical distribution** concerning the sentiments.","efb591b6":"### 3.1 Interpunctuation & special characters \nLet's catch some examples for strange interpunctuaion in ```selected text``` with a RegEx-using function.\n\n","9889b045":"![stock.jpg](attachment:stock.jpg) \n\nI guess Spock's sentiment would be rather \"neutral\" (or maybe even negative?) if he sees his most memorable quote split into two.  \n\u201cLive long and prosper.\u201d - First spoken in Star Trek, season 2, episode 1 (\u201cAmok Time,\u201d 1968)","5829e9be":"## 1.2 First glimps of data and dealing with missing values","fbf833fb":"## 2.2 Distribution of number of words in train data","9a558baf":"## 2.3 Distribution of number of words per sentiment in train data"}}