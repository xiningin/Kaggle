{"cell_type":{"ed09061f":"code","2f7301c8":"code","6ec78f9f":"code","93e20ca9":"code","0ba94b35":"code","557d2fa8":"code","e0dafd5f":"code","63fa33ff":"code","dcd7444f":"code","2a57e0a3":"code","155aa1a9":"code","df61243e":"code","b545d4bc":"code","4863f0b6":"code","f2ca6058":"code","2e467b5c":"code","e80298c8":"code","64370ed4":"code","b37c4bdb":"code","87c6904a":"code","52e05109":"code","17293d5a":"code","3fd31b97":"code","67c57bd9":"code","9fd11c0c":"code","b8954851":"code","a1cf0f05":"code","954bd445":"code","84759987":"code","8ef54df3":"code","f832fc38":"code","c7a65878":"code","79dcadf5":"code","27cfd7e3":"code","41a8971e":"code","c0b7d492":"markdown","23cb8648":"markdown","aa8cf300":"markdown","8cc87ba0":"markdown","ad9d00d6":"markdown","cfc64675":"markdown","4249fb45":"markdown","c02a8925":"markdown","ad95062a":"markdown","923199ca":"markdown","afba06f6":"markdown","301c7a71":"markdown","52d04c92":"markdown","f57b748f":"markdown","035a7531":"markdown","c7e0ca17":"markdown","6bd67ff7":"markdown","156e0f62":"markdown","96287c01":"markdown","f35ea1ad":"markdown","b2139cd0":"markdown","48176ead":"markdown","c4763955":"markdown","63393260":"markdown","42723c35":"markdown"},"source":{"ed09061f":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression","2f7301c8":"length = 200\nperiods = 5\ny = pd.Series(np.sin(np.pi*np.arange(0, length)\/(length\/periods\/2))).rename('sinusoid')\ny = y + np.random.randn(len(y))*0.0001 # minimum amount of noise only necessary for showing effect of overfitting\n\npx.line(y, markers=True)","6ec78f9f":"test_size = round(length\/periods*1.25)\ntrain = y[:-test_size].rename('train')\ntest = y[-test_size:].rename('test')","93e20ca9":"px.line(pd.concat([train, test], axis=1), markers=True)","0ba94b35":"onestep_fit_model = LinearRegression()\n\nX = train[:-1].values.reshape(-1, 1)\ny = train.shift(-1)[:-1].values\n\nonestep_fit_model.fit(X, y)\n\nprint(f'Fitted a={onestep_fit_model.coef_} b={onestep_fit_model.intercept_}')","557d2fa8":"def recursive_predict(train, model, steps):\n  X = train[-1:].values.reshape(-1, 1)\n  prediction = np.array([])\n  for i in range(steps):\n    y = model.predict(X)\n    X = y.reshape(-1, 1)\n    prediction = np.append(prediction, y[0])\n  return pd.Series(data=prediction, index=train.index[-1:].values + 1 + range(0, steps))","e0dafd5f":"onestep_fit_prediction = recursive_predict(train, onestep_fit_model, len(test)).rename('1-step fit prediction')\nprint(f'MSE for 1-step fit: {mean_squared_error(test, onestep_fit_prediction):.2f}')","63fa33ff":"px.line(pd.concat([train, test, onestep_fit_prediction], axis=1), markers=True)","dcd7444f":"zero_prediction = pd.Series(data=np.zeros(len(test)), index=test.index).rename('zero prediction')\nprint(f'MSE for zero prediction: {mean_squared_error(test, zero_prediction):.2f}')","2a57e0a3":"px.line(pd.concat([train, test, zero_prediction], axis=1), markers=True)","155aa1a9":"still_bad_prediction = recursive_predict(train[:-50], onestep_fit_model, len(test)).rename('still bad prediction')\nprint(f'MSE for bad but better case: {mean_squared_error(train[-50:], still_bad_prediction):.2f}')","df61243e":"px.line(pd.concat([train[:-50], train[-50:].rename('test'), still_bad_prediction], axis=1), markers=True)","b545d4bc":"def recursive_fit(train, model, max_steps, diagnose_fit=False, fit_only_once=False):\n  prediction = None\n  if diagnose_fit:\n    all_training_data = pd.DataFrame()\n  for i in range(1, max_steps+1):\n    # create complete training set for i-step ahead prediction (1, 2, ..., i)\n    recursive_train = pd.DataFrame()\n    p_start = 0\n    # create training data for each single step\n    for j in range(0, i):\n      train_step_length = len(train) - j - 1\n\n      if (j == 0):\n        train_step = pd.DataFrame({'y': train.head(train_step_length).values, 'target': train.tail(train_step_length).values})\n      else:\n        # use previous predictions after first step\n        train_step = pd.DataFrame({'y': prediction[p_start:(p_start+train_step_length)], 'target': train.tail(train_step_length).values})\n        train_step.index = train_step.index + j\n        p_start = p_start + train_step_length + 1\n      train_step['forecast_step'] = (j+1)\n      recursive_train = pd.concat([recursive_train, train_step])\n    # train model\n    X = np.array(recursive_train['y']).reshape(-1, 1)\n\n    if not fit_only_once or i==1:\n        model.fit(X=X, y=recursive_train['target'])\n\n    if diagnose_fit:\n      print(f'Fit intercept={model.intercept_} coef={model.coef_}')\n\n    # predict training set\n    prediction = model.predict(X=X)\n\n    if diagnose_fit:\n      recursive_train['prediction'] = prediction\n      recursive_train['recursion_step'] = i\n      all_training_data = pd.concat([all_training_data, recursive_train])\n  \n  if diagnose_fit:\n    all_training_data['t'] = all_training_data.index.values\n    all_training_data['trace_id'] = all_training_data['t']-all_training_data['forecast_step']+2\n    return model, all_training_data\n  else:\n    return model","4863f0b6":"recursive_fit_model = recursive_fit(train, LinearRegression(), len(test))\nprint(f'Fitted a={recursive_fit_model.coef_} b={recursive_fit_model.intercept_}')","f2ca6058":"recursive_fit_prediction = recursive_predict(train, recursive_fit_model, len(test)).rename('recursive fit prediction')\nprint(f'MSE for recursive fit: {mean_squared_error(test, recursive_fit_prediction):.2f}')","2e467b5c":"px.line(pd.concat([train, test, recursive_fit_prediction], axis=1), markers=True)","e80298c8":"errors = [mean_squared_error(test, onestep_fit_prediction),\n          mean_squared_error(test, zero_prediction),\n          mean_squared_error(test, recursive_fit_prediction)]\nfig = px.bar(x=['1-step fit','zero prediction','recursive fit'], y=errors, text=np.round(errors, 2))\nfig.update_layout(font=dict(size=18), height=400, xaxis_title='', yaxis_title='MSE')\nfig.show()","64370ed4":"px.line(pd.concat([test, onestep_fit_prediction, zero_prediction, recursive_fit_prediction], axis=1), markers=True)","b37c4bdb":"from sklearn.base import BaseEstimator, RegressorMixin\n\nclass OverfittingModel(BaseEstimator, RegressorMixin):\n  def __init__(self, fallback_estimator):\n    self.overfit_dict = dict()\n    self.fallback_estimator = fallback_estimator\n\n  def fit(self, X, y):\n    # save X,y for later overfit\n    X_tuples = [tuple(x) for x in X]\n    self.overfit_dict = dict(zip(X_tuples, y))\n    # fit fallback estimator for unknown values\n    self.fallback_estimator.fit(X, y)\n    return self\n\n\n  def predict(self, X):\n    prediction = self.fallback_estimator.predict(X)\n    # replace prediction[i] where X[i] is in overfit_dict\n    prediction = np.array([self.overfit_dict.get(tuple(x), y) for x, y in zip(X, prediction)])\n    return prediction\n","87c6904a":"overfit_model = recursive_fit(train, OverfittingModel(LinearRegression()), len(test))","52e05109":"in_sample_train = train[:-len(test)].rename('train')\nin_sample_test = train[-len(test):].rename('test')\nin_sample_prediction = recursive_predict(in_sample_train, overfit_model, len(test)).rename('in-sample w\/ overfit')\nprint(f'MSE for overfitted in-sample prediction: {mean_squared_error(in_sample_test, in_sample_prediction):.4f}')","17293d5a":"px.line(pd.concat([in_sample_train, in_sample_test, in_sample_prediction], axis=1), markers=True)","3fd31b97":"out_of_sample_prediction = recursive_predict(train, overfit_model, len(test)).rename('out-of-sample w\/ overfit')\nprint(f'MSE for overfitted out-of-sample prediction: {mean_squared_error(out_of_sample_prediction, test):.4f}')","67c57bd9":"px.line(pd.concat([train, test, out_of_sample_prediction], axis=1), markers=True)","9fd11c0c":"debug_y = pd.Series([(1+i)**i for i in range(0,8)])\ndebug_train = debug_y[:-3].rename('train')\ndebug_test = debug_y[-3:].rename('test')","b8954851":"debug_model, debug_fit_data = recursive_fit(debug_train, LinearRegression(), 4, diagnose_fit=True)","a1cf0f05":"debug_fit_data.loc[debug_fit_data['recursion_step'].isin([1,2])]","954bd445":"recursive_model, recursive_fit_data = recursive_fit(train, LinearRegression(), 50, diagnose_fit=True)","84759987":"def plot_recursion_steps(fit_data, recursion_steps, dynamic_color_map=True):\n    # dynamic_color_map=True will show the latest forecast_step in one color (yellow) across all subplots\n    # dynamic_color_map=False will show the same forecast_step in one color across all subplots\n    if dynamic_color_map:\n        color_col = 'forecast_step'\n        color_map = {}\n        trendline = 'ols'\n    else:\n        fs_min = fit_data['forecast_step'].min()\n        fs_max = fit_data['forecast_step'].max()\n        fs_count = fs_max-fs_min+1\n        color_map = dict(list(zip([str(x) for x in range(fs_min, fs_max+2)], px.colors.sample_colorscale('turbo', samplepoints=fs_count+1))))\n        fit_data['forecast_step_str'] = fit_data['forecast_step'].astype('str')\n        color_col = 'forecast_step_str'\n        trendline = None\n\n    fig = px.scatter(\n        fit_data.loc[fit_data['recursion_step'].isin(recursion_steps)],\n        x='y', y='target', color=color_col,\n        trendline=trendline, trendline_color_override='black',\n        facet_col='recursion_step', facet_col_wrap=4,\n        labels={'y': 'y(t)', 'target': 'y(t+1)'},\n        color_discrete_map = color_map,\n        height=1000\n    )\n    if dynamic_color_map:\n        fig.update(layout_coloraxis_showscale=False)\n    return fig","8ef54df3":"plot_recursion_steps(recursive_fit_data, [1,2,3,4, 6,8,10,12, 16,20,24,28, 34,40,46,50])","f832fc38":"import plotly.graph_objects as go\n\ndef plot_traces(fit_data, recursion_step):\n    prediction_plot = px.line(fit_data.loc[fit_data['recursion_step']==recursion_step], x='t', y='prediction', line_group ='trace_id', line_dash_sequence=['dot'], color_discrete_sequence=['#00CC96'])\n    target_plot = px.line(fit_data.loc[fit_data['recursion_step']==1], x='t', y='target')\n    combined_plot = go.Figure(data=prediction_plot.data + target_plot.data)\n    return combined_plot","c7a65878":"plot_traces(recursive_fit_data, 50)","79dcadf5":"debug_model, single_fit_data = recursive_fit(train, LinearRegression(), 50, diagnose_fit=True, fit_only_once=True)","27cfd7e3":"plot_recursion_steps(single_fit_data, [1,2,3,4, 6,8,10,12, 16,20,24,28, 34,40,46,50])","41a8971e":"plot_traces(single_fit_data, 50)","c0b7d492":"## Standard Training of a one-step model","23cb8648":"Due to overfitting, the model has never seen the errors it would make on unseen data, and thus it could not adjust the parameters appropriately during training.","aa8cf300":"Note, that the regression line now only represents the best linear fit of the recursively generated data. The model used in each step is always the same with $a \\approx 1, b \\approx 0$.\n\nYou can see that recursively generating forecasts from the first model and then training a model on the generated traces would be an option as well. It would yield the zero prediction strategy: $a \\approx 0, b \\approx 0$","8cc87ba0":"Let's do a simple example. The time series to predict is a sine wave.","ad9d00d6":"# Perils of recursive forecasting\n\nRecursive prediction of time series seems natural at first. You learn how to predict one step ahead and use each of these predictions to predict the next step. This way, you can predict any number of steps. Great, isn't it?\n\nHowever, there is something strange about it. Imagine a rooky firefighter learning to climb the turntable ladder. His instructor shows him how to climb from the first rung to the second. After the apprentice has successfully practiced this, the instructor says, \"Wonderful, you're well prepared to climb all the way to the top in an emergency case! Just repeat what you have learned until you reach the basket.\"\n\nThe rooky is left with the vague feeling that climbing 75 feet to the top is different than just taking the first step.\n\nWhen it comes to recursive forecasting, it's a similar situation. The model learned to forecast one step ahead, but forecasting multiple steps ahead can be quite a different thing.","cfc64675":"While this test case was obviously chosen to represent the worst case, the zero prediction would be better for almost all possible test cases.","4249fb45":"# Consequences of overfit in recursive training","c02a8925":"The drastic improvement is due to the simplicity of the example and the fact that the AR(1) model is completely inappropriate for the data.\n\nHowever, when using recursive forecasting models you could try to check, if recursive training improves your model.\n\nKeep in mind that depending on your data size, the number of forecasting steps and model complexity the training time might be significantly higher.\n\nUnfortunately, besides increased training time, there is an even worse problem if your model even slightly tends to overfit.","ad95062a":"That does not look like a good prediction.\n\nIntuitively, a model that simply predicts zero would be much better.","923199ca":"To demonstrate the effect of overfitting during recursive training, we will use a model with memory that perfectly reproduces all training data. For data it has not seen before, it will use a fallback estimator model.","afba06f6":"# Recursive Fit: Explorative Analysis\n\nHere is some code to explore the recursive fit a little deeper.\n\nFirst a small example so you can check how predictions for each step are reused in the following training step.","301c7a71":"The overfitting model perfectly reproduces everything in-sample when used recursively.","52d04c92":"Now, let's try to forecast recursively ahead with this model...","f57b748f":"A good guess for the model would be to use the current value $y_t$ for the forecast, as the next value is always quite close to the current. So this model would have $a=1$ and $b=0$.","035a7531":"Awesome, the best prediction strategy for the AR(1) model is to approach 0 gradually, thus still exploiting the proximity to the start value initially.\n\nIn visual summary...","c7e0ca17":"It would be super easy to predict this time series, but to illustrate the problem, we use an inappropriate model: a linear model that can only use the current value to forecast the next value. In mathematical terms $y_{t+1}=a y_{t} + b$, which basically is an AR(1) process.\n\nWe will try to forecast 1.25 periods of the wave ahead and split into training and test accordingly...","6bd67ff7":"As expected, the model learns this strategy and predicts $y_{t+1} \\approx y_{t}$.","156e0f62":"Back to our original example and a nice visualization of the training steps...","96287c01":"Fixing this in a real life forecasting scenario is not trivial.\n\nThe perfect solution would be to train every recursive step on different data. For a forecast horizon of 24 you would cut the training data in 24 parts, train the one-step estimator on the first part, predict the second part and so on. However, you might not have sufficient training data for this strategy and get underperforming models.\n\nYou have to be creative then, use fewer cuts, add noise sampled from out-of-sample predictions, etc. Cutting the training data in only 4 parts and recursively fit 4 steps may not be ideal for a 24 month forecast, but if it makes the difference?!\n\nAnother option you can always try is replacing the recursive strategy with direct multi-steps models (DMS). So you would train one model to predict $y_{t+1}$, another model to predict $y_{t+2}$ and so on. Some benefits:\n\n- Errors do not add up compared to the recursive strategy\n- No need to forecast additional variables that might be useful but hard to forecast, e.g. stock levels\n- Short-term models can use different variables than the long-term models, e.g. today's sales and stock is important for the short-term models, but general trends and product lifecycle are more important for the long-term models","f35ea1ad":"## Recursive Training of a one-step model\n\nTraining the model recursively on its own predictions will show that there is even a better strategy than simply predicting zero.\n\nFirst, the model will be trained on the normal training data. The result would be a good one-step estimator. This estimator is used to predict the whole training set. These one-step predictions can be appended to the training set and a good two-step predictor can be trained. This will be repeated for the number of steps to be predicted.","b2139cd0":"# Practical Advice","48176ead":"We can tell the `recursive_fit` function to only fit the model once in the beginning and recursively forecast. This is a simulation of the forecasts a traditional fit would create.","c4763955":"You can see how in the initial step the data is almost perfectly fit by the regression line. Thereafter, with each training step it becomes clearer that the more distant prediction points (in yellow) are harder to reach and therefore the parameters have to be adjusted (note the rotation of the black regression line). Short term accuracy is sacrificed in favor of the overall forecast accuracy.","63393260":"In fact, this reduced the error by more than half (```1.26``` vs. ```0.51```).\n\nHowever, the model could not learn this strategy, as it was never trained to be used recursively.","42723c35":"However, out-of-sample we are back to where we started originally as if we did not train recursively..."}}