{"cell_type":{"218fe20d":"code","37f4d5b4":"code","5b6a7422":"code","e2f875fb":"code","5d0f1d59":"code","3d505a1a":"code","28dd5c83":"code","fb054b25":"code","097c87e9":"code","7e821cb3":"code","9ab36e29":"code","0721945b":"code","1203a0e9":"code","73109b5e":"code","97315912":"code","b3d47ad5":"code","9ae35184":"code","f7c62b4a":"code","2558f64f":"code","2d32eeb6":"code","e398eaa0":"code","50d37504":"code","84e89906":"code","54579d96":"code","fb95addf":"code","8385c208":"code","c3c21814":"code","2bf1ec33":"code","9a3ac48b":"code","7fa416fc":"code","5595a3ae":"code","0398cc57":"code","3b59f0a2":"code","70b6fb49":"code","3b1dc3d5":"code","25b64611":"code","adc43416":"code","44361a7b":"code","7bace22f":"code","e5ff1f19":"code","f1a210bc":"code","62b39425":"code","11b588fe":"code","2060d2e9":"code","fec05270":"code","3651e7b2":"code","a1cec754":"code","3da5969e":"code","ec060834":"code","ef1b2ca8":"code","d01544eb":"code","30ab3db5":"code","678ee65e":"code","084bc885":"code","ecb1aff1":"code","8ac7cddc":"code","b697fdb1":"code","583db85d":"code","11d14d0c":"code","72551b2d":"code","aef6c542":"code","84cda282":"code","51a32060":"code","b6c157ba":"code","79f6706d":"code","c66a2817":"code","2f9b2cfb":"code","43281777":"code","55afe8ed":"code","2fcac2d4":"code","cc246872":"code","3c70c1e7":"code","7ddedc3d":"code","126a7c56":"code","30b5a221":"code","fd3e0370":"code","4cb95ed2":"code","d90506bf":"code","4708763c":"code","5fe85911":"code","3df4be4a":"code","3c0af259":"code","ea725a32":"code","251fd601":"code","a771971e":"code","20442f2a":"code","81586364":"code","f227e825":"code","499db1ef":"code","b5b0819f":"code","29433702":"code","2eb04464":"code","0877307f":"code","2050ad4c":"code","c9f84386":"code","9263fec4":"code","8500ce21":"code","a450bcef":"code","26f154b9":"markdown","dd35f95e":"markdown","e4942136":"markdown","b0a28046":"markdown","f709caa5":"markdown","3cd37233":"markdown","a0c11295":"markdown","a09b1f7d":"markdown","13d1bb9a":"markdown","31968454":"markdown","8d7cc437":"markdown","0376a81a":"markdown","f4ea00c0":"markdown","52e2a278":"markdown","c2138901":"markdown","9e5947c2":"markdown","133ee9c8":"markdown","f9d76a5c":"markdown","2513cd60":"markdown","474103dc":"markdown","74bb82fb":"markdown","bac7c2b3":"markdown","5c0c47a0":"markdown","d505e335":"markdown","22cc5bd0":"markdown","2a2a212e":"markdown","f7d5c3e1":"markdown","a880fcf1":"markdown","0c24ff50":"markdown","d1faeeec":"markdown","2a3b0de1":"markdown","8e6b207e":"markdown","8626656a":"markdown","f35ebd8f":"markdown","01963f9d":"markdown","72c3272d":"markdown","242f165e":"markdown","946c146b":"markdown","ed0eb742":"markdown","9729c812":"markdown","7bd96e85":"markdown","429a5adf":"markdown","52bb4e8a":"markdown","409a5ecc":"markdown","d4700c29":"markdown","f220830d":"markdown","bc837374":"markdown","eeedfc69":"markdown","3beec538":"markdown","bf0b7fb4":"markdown","0e7a9652":"markdown","f6f664b6":"markdown","ef827a49":"markdown","34a56b4d":"markdown"},"source":{"218fe20d":"import pandas as pd \nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport re\nimport string\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import classification_report\nfrom nltk.corpus import stopwords\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom gensim.models import Word2Vec\nfrom numpy import asarray\nfrom numpy import zeros\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom nltk.tokenize import RegexpTokenizer\nimport plotly\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud,STOPWORDS\n# general imports\nimport math\nfrom bs4 import BeautifulSoup\nimport tensorflow as tf\nimport numpy as np\nimport skimage\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score\nimport missingno as msno\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nimport warnings\nwarnings.filterwarnings('ignore')","37f4d5b4":"#Lets load the first csv file \ndf = pd.read_csv('..\/input\/google-play-store-apps\/googleplaystore.csv')    \ndf.shape","5b6a7422":"#First look at data\ndf.sample(5)","e2f875fb":"pd.set_option('display.float_format', lambda x: '%.5f' % x)","5d0f1d59":"#Lets check Missing values\ndf.isna().sum()","3d505a1a":"#Lets see % wise top contributors to missing values \ndef missing_values(dff):\n    print (round((dff.isnull().sum() * 100\/ len(dff)),2).sort_values(ascending=False))\n\nmissing_values(df)","28dd5c83":"#chek datatypes\ndf.info()","fb054b25":"#Describe the numerical data\ndf.describe()","097c87e9":"#Variable : Size\n# Replace \"Varies with device\" with a fixed numeric value \"999999\"\ndf['Size'].replace(to_replace =\"Varies with device\",value =\"999999\", inplace=True) \n# Replace \"1000+\" with \"1000\" \ndf['Size'].replace(to_replace =\"1,000+\",value =\"1000\", inplace=True) \n","7e821cb3":"#Replace small k with capital K\ndf[\"Size\"] = df[\"Size\"].replace({'k':'K'}, regex=True)\n#Replace small m with capital M\ndf[\"Size\"] = df[\"Size\"].replace({'m':'M'}, regex=True) ","9ab36e29":"#Multiply the values ending with K  by 1000 and values ending with M ith 1000000  \ndf.Size = (df.Size.replace(r'[KM]+$', '', regex=True).astype(float) * \\\n          df.Size.str.extract(r'[\\d\\.]+([KM]+)', expand=False)\n          .fillna(1)\n          .replace(['K','M'], [10**3, 10**6]).astype(int))\n","0721945b":"df[\"Size\"].sample(5)","1203a0e9":"#Varibable : Type\n# Drop rows with any NaN in the selected columns only\ndf = df.dropna(how='any', subset=['Type'])\ndf.Type.isna().sum()\n# No missing values for Type column","73109b5e":"# Get names of indexes for which column Type has value 0\nindexNames = df[ df['Type'] == '0' ].index\n\n# Delete these row indexes from dataFrame\ndf.drop(indexNames , inplace=True)\n\ndf[\"Type\"].value_counts()","97315912":"#So we have only two categories in \"Type\" column. Lets convert to numeric ones for our analysis\ndf[\"Type\"] = df[\"Type\"].map({\"Free\": 0, \"Paid\":1})\ndf[\"Type\"].value_counts()","b3d47ad5":"df[\"Type\"].sample(5)","9ae35184":"#Variable : Rating\n# Rating Null Values : Fill Null values with value= \"No Rating\" \ndf['Rating'].fillna(\"No Rating\", inplace=True)\n","f7c62b4a":"#Variable : Current ver\n# Current Ver :  Null Values : Fill Null values with value= \"No Current Ver\" \ndf['Current Ver'].fillna(\"No Current Ver\", inplace=True)\n","2558f64f":"#Variable : Andriod ver\n# Android Ver :  Null Values : Fill Null values with value= \"No Android Ver\" \ndf['Android Ver'].fillna(\"No Android Ver\", inplace=True)\n","2d32eeb6":"df.isnull().sum()","e398eaa0":"# Remove $ & ,and Converting price to float\ndf['Price'] = df['Price'].apply(lambda x: x.replace('$', '').replace(',', '')\n                                if isinstance(x, str) else x).astype(float)\ndf['Price'].apply(type).value_counts()\ndf.info()","50d37504":"# Remove + & , and Converting Installs to float \ndf['Installs'] = df['Installs'].apply(lambda x: x.replace('+', '').replace(',', '')\n                                if isinstance(x, str) else x).astype(float)\ndf['Installs'].apply(type).value_counts()","84e89906":"#Convert Reviews from object to numerical int\ndf['Reviews']  = df['Reviews'].astype('int')","54579d96":"#check skewness for Reviews\ng = sns.distplot(df[\"Reviews\"], color=\"m\", label=\"Skewness : %.2f\"%(df[\"Reviews\"].skew()))\ng = g.legend(loc=\"best\")","fb95addf":"# Apply log transformation to reduce skewness distribution\ndf['Reviews_log'] = df['Reviews'].map(lambda i: np.log(i) if i > 0 else 0)\n","8385c208":"#check skewness for post log transformation on Reviews\ng = sns.distplot(df[\"Reviews_log\"], color=\"m\", label=\"Skewness : %.2f\"%(df[\"Reviews_log\"].skew()))\ng = g.legend(loc=\"best\")","c3c21814":"#check skewness for Installs\ng = sns.distplot(df[\"Installs\"], color=\"y\", label=\"Skewness : %.2f\"%(df[\"Installs\"].skew()))\ng = g.legend(loc=\"best\")","2bf1ec33":"# Apply log transformation to reduce skewness distribution\ndf['Installs_log'] = df['Installs'].map(lambda i: np.log(i) if i > 0 else 0)","9a3ac48b":"#check skewness for post log transformation on Installs\ng = sns.distplot(df[\"Installs_log\"], color=\"y\", label=\"Skewness : %.2f\"%(df[\"Installs_log\"].skew()))\ng = g.legend(loc=\"best\")","7fa416fc":"df.sample(5)","5595a3ae":"# Lets explore variables : 'Type','Content Rating' \ncat_col = ['Type','Content Rating']\nplt.figure(figsize=(10, 12))\ncount = 1\nfor cols in cat_col:\n    plt.subplot(2, 2, count)\n    df[cols].value_counts().plot.pie(shadow=True,autopct='%1.1f%%')\n    count +=1\n    plt.subplot(2, 2, count)\n    sns.countplot(cols, data=df)\n    fig=plt.gcf()\n    fig.set_size_inches(15,8)\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=1)\n    plt.xticks(rotation=45)\n    count+=1","0398cc57":"# Lets explore other categorical variables : 'Category', 'Rating', 'Android Ver'\ncat_col_1 = ['Category', 'Rating', 'Android Ver']\n\nfor cols in cat_col_1:\n    plt.figure(figsize=(18,7))\n    sns.countplot(x=cols, order = df[cols].value_counts().index, data=df, palette=\"hls\")\n    plt.ylabel(\"Count\", fontsize=15) # Seting the ytitle and size\n    plt.xticks(fontsize=8)\n    plt.yticks(fontsize=12)\n    plt.xticks(rotation=45)\n    plt.show()","3b59f0a2":"#Catplot Reviews+Content Rating+Type\nsns.catplot(x='Content Rating',y='Reviews',kind='point',data=df,hue='Type', palette=\"hls\")\nfig=plt.gcf()\nfig.set_size_inches(15,8)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=1)\nplt.xticks(rotation=45)\nplt.show()","70b6fb49":"#Biveriate Analysis of Rating Vs reviews & hue= \"Type\"\nsns.catplot(data=df, x='Rating', y='Reviews', kind='bar', hue=\"Type\");\nfig=plt.gcf()\nfig.set_size_inches(15,8)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=1)\nplt.xticks(rotation=45)\nplt.show()","3b1dc3d5":"#Biveriate Analysis of Android Ver Vs reviews & hue=\"Type\"\nsns.catplot(data=df, x='Android Ver', y='Reviews', kind='bar', hue=\"Type\", palette=\"hls\");\nfig=plt.gcf()\nfig.set_size_inches(15,8)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=1)\nplt.xticks(rotation=45)\nplt.show()","25b64611":"#Biveriate Analysis of Category Vs reviews & hue=\"Type\"\nsns.catplot(data=df, x='Category', y='Reviews', kind='bar', hue=\"Type\");\nfig=plt.gcf()\nfig.set_size_inches(25,8)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=1)\nplt.xticks(rotation=45)\nplt.show()","adc43416":"# Explore Size Vs Type using kde plots\nplt.figure(figsize=(8, 6))\ng = sns.kdeplot(df[\"Size\"].round(5)[(df[\"Type\"] == 0) & (df[\"Size\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(df[\"Size\"].round(5)[(df[\"Type\"] == 1) & (df[\"Size\"].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Size\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Type=0\",\"Type=1\"])","44361a7b":"sns.heatmap(df.corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':12})\nfig=plt.gcf()\nfig.set_size_inches(12,8)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.show()","7bace22f":"#Lets explore \"Last Updated\" i.e. Date field \ndf['month'] = pd.DatetimeIndex(df['Last Updated']).month\ndf['weekday'] = pd.DatetimeIndex(df['Last Updated']).weekday\ndf.sample(5)","e5ff1f19":"# Lets see how weekday analysis looks like\nplt.rcParams['figure.figsize'] = (14,6)\ndf['weekday'].hist(bins=7,color='orange',range = (-.5,6.5),rwidth=.8)\nplt.xticks(range(7),'Mon Tue Wed Thu Fri Sat Sun'.split());\nplt.ylabel('Counts')\nplt.title('Count of \"Last Updated\" by Weekday')\nplt.show()","f1a210bc":"# Lets see how month analysis looks like\nsns.countplot(x = 'month', data = df)","62b39425":"#Creating an categorical variable to handle with the Price variable\ninterval = (-0.5, 0.5, 50.00, 300.00, 450.00)\ncats = ['zero', 'low', 'medium', 'high',]\ndf[\"Price_cat\"] = pd.cut(df.Price, interval, labels=cats)","11b588fe":"df[[\"Price\",\"Price_cat\"]].sample(5)","2060d2e9":"#Catplot Price_cat+Reviews+Content Rating\nsns.catplot(x='Price_cat',y='Reviews',kind='point',data=df, palette=\"hls\", hue=\"Content Rating\")\nfig=plt.gcf()\nfig.set_size_inches(25,8)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.xticks(rotation=45)\nplt.show()","fec05270":"#Creating an categorical variable to handle with the Size variable\ninterval = (8400, 100000, 1000000, 10000000, 1000000000)\ncats = ['small', 'medium', 'high', 'Very high']\ndf[\"Size_cat\"] = pd.cut(df.Size, interval, labels=cats)\n","3651e7b2":"df[[\"Size\",\"Size_cat\"]].sample(5)","a1cec754":"#Catplot Size_cat+Reviews+Content Rating\nsns.catplot(x='Size_cat',y='Reviews',kind='point',data=df, palette=\"hls\", hue=\"Content Rating\")\nfig=plt.gcf()\nfig.set_size_inches(25,8)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.xticks(rotation=45)\nplt.show()\n","3da5969e":"#Size_cat+Price+Content Rating\nsns.catplot(x='Size_cat',y='Price',kind='point',data=df, palette=\"hls\", hue=\"Content Rating\")\nfig=plt.gcf()\nfig.set_size_inches(25,8)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.xticks(rotation=45)\nplt.show()","ec060834":"#Lets load the first csv file \ntweet = pd.read_csv('..\/input\/google-play-store-apps\/googleplaystore_user_reviews.csv')  ","ef1b2ca8":"print('There are {} rows and {} columns'.format(tweet.shape[0],tweet.shape[1]))","d01544eb":" tweet.sample(10)","30ab3db5":"#Lets check Missing values\ntweet.isna().sum()","678ee65e":"#Lets see % wise top contributors to missing values \ndef missing_values(dff):\n    print (round((dff.isnull().sum() * 100\/ len(dff)),2).sort_values(ascending=False))\nmissing_values(tweet)","084bc885":"tweet.dropna(thresh=2, inplace=True) ","ecb1aff1":"#Lets check Missing values\ntweet.isna().sum()","8ac7cddc":"#Display remaining Null values \ntweet[tweet.isna().any(axis=1)]","b697fdb1":"#Lets inject text = \"Neutral\" for Sentiment=\"Neutral\"\ntweet.loc[(tweet.Translated_Review.isnull()),'Translated_Review'] ='Neutral'","583db85d":"#Lets check Missing values\ntweet.isna().sum()","11d14d0c":"print('There are {} rows and {} columns in tweet'.format(tweet.shape[0],tweet.shape[1]))","72551b2d":"#Explore Categorical Variable - Sentiment\ncat_col = ['Sentiment']\nplt.figure(figsize=(10, 6))\ncount = 1\nfor cols in cat_col:\n    plt.subplot(1, 2, count)\n    tweet[cols].value_counts().plot.pie(explode=[0,0.1,0.1],shadow=True,autopct='%1.1f%%')\n    count +=1\n    plt.subplot(1, 2, count)\n    sns.countplot(cols, data=tweet)\n    count+=1","aef6c542":"tweet['Sentiment'].value_counts()","84cda282":"#Lets explore numerical variables - Sentiment_Polarity & Sentiment_Subjectivity\nnum_col = ['Sentiment_Polarity', 'Sentiment_Subjectivity']\n\nplt.figure(figsize=(10, 8))\ncount = 1\nfor cols in num_col:\n    plt.subplot(2, 2, count)\n    sns.boxplot(x='Sentiment', y= cols, data= tweet)\n    count +=1\n    \n    plt.subplot(2, 2, count)\n    g = sns.kdeplot(tweet[cols][(tweet[\"Sentiment\"] == \"Negative\") & (tweet[cols].notnull())], color=\"Red\", shade = True)\n    g = sns.kdeplot(tweet[cols][(tweet[\"Sentiment\"] == \"Positive\") & (tweet[cols].notnull())], ax =g, color=\"Green\", shade= True)\n    g.set_xlabel(cols)\n    g.set_ylabel(\"Frequency\")\n    g = g.legend([\"Negative\",\"Positive\"])\n    count+=1","51a32060":"# Scattrplot between Sentiment_Polarity Vs Sentiment_Subjectivity with hue= sentiment\nsns.set(style='whitegrid', palette=\"deep\", font_scale=1.1, rc={\"figure.figsize\": [5, 3]})\n(sns\n .FacetGrid(tweet, hue='Sentiment', size=6)\n .map(plt.scatter, 'Sentiment_Polarity', 'Sentiment_Subjectivity')\n .add_legend()\n .set(\n    title='Sentiment_Polarity Vs Sentiment_Subjectivity',\n    xlabel='Sentiment_Polarity',\n    ylabel='Sentiment_Subjectivity'\n))","b6c157ba":"# Separate scatter plots for Sentiment_Polarity Vs Sentiment_Subjectivity across varous sentiments\nsns.set(style='whitegrid', palette=\"BrBG\", font_scale=1.1, rc={\"figure.figsize\": [7, 4]})\n(tweet\n .pipe(sns.FacetGrid, \n       col='Sentiment', \n       col_wrap=4, \n       aspect=.5, \n       size=6)\n .map(plt.scatter, 'Sentiment_Polarity', 'Sentiment_Subjectivity', s=20)\n .fig.subplots_adjust(wspace=.2, hspace=.2)\n)","79f6706d":"#Explore variable  - App - Top 15\nplt.figure(figsize=(8, 4))\ntweet['App'].value_counts()[0:15].plot.barh(color=\"g\").set_title('Count of top 15 Apps')","c66a2817":"#Explore variable  - App - Bottom 15\nplt.figure(figsize=(8, 4))\ntweet['App'].value_counts()[-15:].plot.barh(color=\"r\").set_title('Count of bottom 15 Apps')","2f9b2cfb":"#Explore Variable - Translated_Review\n#Number of characters in tweets\nfig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(12,5))\ntweet_len=tweet[tweet['Sentiment']=='Negative']['Translated_Review'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('Negative Review')\ntweet_len=tweet[tweet['Sentiment']=='Positive']['Translated_Review'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Positive Review')\ntweet_len=tweet[tweet['Sentiment']=='Neutral']['Translated_Review'].str.len()\nax3.hist(tweet_len,color='magenta')\nax3.set_title('Neutral Review')\nfig.suptitle('Characters in Review')\nplt.show()","43281777":"#Number of words in a Reviews\nfig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(10,5))\ntweet_len_neg=tweet[tweet['Sentiment']=='Negative']['Translated_Review'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len_neg,color='red')\nax1.set_title('Negative Review')\ntweet_len_pos=tweet[tweet['Sentiment']=='Positive']['Translated_Review'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len_pos,color='green')\nax2.set_title('Positive Review')\ntweet_len_neutral=tweet[tweet['Sentiment']=='Neutral']['Translated_Review'].str.split().map(lambda x: len(x))\nax3.hist(tweet_len_neutral,color='magenta')\nax3.set_title('Neutral Review')\nfig.suptitle('Characters in Review')\nplt.show()","55afe8ed":"# Explore length of reviews Vs Sentiment using kde plots\nplt.figure(figsize=(14, 8))\ng = sns.kdeplot(tweet_len_neg, color=\"Red\", shade = True)\ng = sns.kdeplot(tweet_len_pos, ax =g, color=\"Green\", shade= True)\ng = sns.kdeplot(tweet_len_neutral, ax =g, color=\"magenta\", shade= True)\n\ng.set_xlabel(\"Word Length\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Negative\",\"Positive\",\"Neutral\"])","2fcac2d4":"#Average word length in a review\nfig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(10,5))\nword=tweet[tweet['Sentiment']=='Negative']['Translated_Review'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('Negative')\nword=tweet[tweet['Sentiment']=='Positive']['Translated_Review'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Positive')\nword=tweet[tweet['Sentiment']=='Neutral']['Translated_Review'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax3,color='magenta')\nax3.set_title('Neutral')\nfig.suptitle('Average word length in each review')","cc246872":"#Common stopwords in reviews - First we will analyze review with class Negative\ndef create_corpus(target):\n    corpus=[]\n    \n    for x in tweet[tweet['Sentiment']==target]['Translated_Review'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus\n\ncorpus=create_corpus(\"Negative\")\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n\nx,y=zip(*top)\nplt.figure(figsize=(5, 3))\nplt.bar(x,y,color=\"red\")","3c70c1e7":"#Lets analyze review with class Positive\ncorpus=create_corpus(\"Positive\")\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n\nx,y=zip(*top)\nplt.figure(figsize=(5, 3))\nplt.bar(x,y,color=\"green\")","7ddedc3d":"#Lets analyze review with class Neutral\ncorpus=create_corpus(\"Neutral\")\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n\nx,y=zip(*top)\nplt.figure(figsize=(5, 3))\nplt.bar(x,y,color=\"magenta\")","126a7c56":"#Analyzing punctuations - Lets check class Negative\nplt.figure(figsize=(5,3))\ncorpus=create_corpus(\"Negative\")\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color=\"red\")","30b5a221":"#Analyzing punctuations - Lets check class Positive\nplt.figure(figsize=(5,3))\ncorpus=create_corpus(\"Positive\")\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color=\"green\")","fd3e0370":"#Analyzing punctuations - Lets check class Neutral\nplt.figure(figsize=(5,3))\ncorpus=create_corpus(\"Neutral\")\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color=\"magenta\")","4cb95ed2":"def create_corpus_generic():\n    corpus=[]\n    \n    for x in tweet['Translated_Review'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus\n\ncorpus_generic=create_corpus_generic()\n","d90506bf":"# Identify common words in generic corpus\ncounter=Counter(corpus_generic)\nmost=counter.most_common()\n\nx=[]\ny=[]\n\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)\nplt.figure(figsize=(10,8))\nsns.barplot(x=y,y=x)","4708763c":"#Ngram analysis\n#we will do a bigram (n=2) analysis over the tweets.\n#Let's check the most common bigrams in tweets.\ndef get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\nplt.figure(figsize=(10,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(tweet['Translated_Review'])[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","5fe85911":"#Step 1 : Removing urls\n\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ntweet['tweet_NoURL']=tweet['Translated_Review'].apply(lambda x : remove_URL(x))\ntweet[['tweet_NoURL']].head(10)\n","3df4be4a":"#Step 2 : Removing HTML tags\n\ntweet['tweet_NoURL']=tweet['tweet_NoURL'].apply(str)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ntweet['tweet_NoHTML']=tweet['tweet_NoURL'].apply(lambda x : remove_html(x))\ntweet['tweet_NoHTML'].head(10)","3c0af259":"#Step 3 : Romoving Emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ntweet['tweet_NoEMOJI']=tweet['tweet_NoHTML'].apply(lambda x: remove_emoji(x))\ntweet[['tweet_NoURL','tweet_NoHTML','tweet_NoEMOJI']].head(10)","ea725a32":"#Step 4 :Removing punctuations\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ntweet['tweet_NoPUNKT']=tweet['tweet_NoEMOJI'].apply(lambda x : remove_punct(x))\ntweet[['tweet_NoURL','tweet_NoHTML','tweet_NoEMOJI','tweet_NoPUNKT']].head(10)","251fd601":"#Step 5: transform to lowercase\ntweet['tweet_tolower']= tweet['tweet_NoPUNKT'].str.lower()\ntweet[['tweet_NoPUNKT','tweet_tolower']].head(10)","a771971e":"#Step 6: remove stopwords 'n punctuation\nsw = stopwords.words('english')\n\ndef transform_text(s):\n    \n    # remove html\n    html=re.compile(r'<.*?>')\n    s = html.sub(r'',s)\n    \n    # remove numbers\n    s = re.sub(r'\\d+', '', s)\n    \n    # remove punctuation\n    # remove stopwords\n    tokens = nltk.word_tokenize(s)\n    \n    new_string = []\n    for w in tokens:\n        # remove words with len = 2 AND stopwords\n        if len(w) > 2 and w not in sw:\n            new_string.append(w)\n \n    s = ' '.join(new_string)\n    s = s.strip()\n\n    exclude = set(string.punctuation)\n    s = ''.join(ch for ch in s if ch not in exclude)\n    \n    return s.strip()\n\ntweet['tweet_sw'] = tweet['tweet_tolower'].apply(transform_text)\ntweet[['tweet_NoPUNKT','tweet_tolower', 'tweet_sw']].head(10)","20442f2a":"\n#Step 7: lemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndef lemmatizer_text(s):\n    tokens = nltk.word_tokenize(s)\n    \n    new_string = []\n    for w in tokens:\n        lem = lemmatizer.lemmatize(w, pos=\"v\")\n        # exclude if lenght of lemma is smaller than 2\n        if len(lem) > 2:\n            new_string.append(lem)\n    \n    s = ' '.join(new_string)\n    return s.strip()\n\n\ntweet['tweet_lm'] = tweet['tweet_sw'].apply(lemmatizer_text)\n","81586364":"#Step 8: transform to lowercase, select only alphabets and remove stopwords\ndef create_corpus(df):\n    corpus=[]\n    #corpus_new=[]\n    for tweet in tqdm(df['tweet_lm']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word.lower() not in stop))]\n        corpus.append(words)\n        #words_new = (word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word.lower() not in stop)))\n        #corpus_new.append(words_new)\n    return corpus\n\ncorpus=create_corpus(tweet)","f227e825":"#Find max length of a row in the corpus\nMAX_LEN = max(len(x) for x in corpus)\nprint(\"MAX_LEN = \", MAX_LEN)","499db1ef":"#see the word cloud with treated text\n# -ve wordcloud\ndf_neg = tweet[tweet['Sentiment']==\"Negative\"]['tweet_lm']\n\nwordcloud1 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_neg))\n\nplt.figure(1,figsize=(10, 8))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.show()\n\n","b5b0819f":"# +ve wordcloud\ndf_pos = tweet[tweet['Sentiment']==\"Positive\"]['tweet_lm']\n\n\nwordcloud1 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_pos))\n\nplt.figure(1,figsize=(10, 8))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.show()","29433702":"# Neutral wordcloud\ndf_neu = tweet[tweet['Sentiment']==\"Neutral\"]['tweet_lm']\n\n\nwordcloud1 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_neu))\n\nplt.figure(1,figsize=(10, 8))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.show()\n","2eb04464":"#label encode the target variable\nlabel_encoder = LabelEncoder()\ntweet['sentiment_encoded'] = label_encoder.fit_transform(tweet['Sentiment'])\n\n\nx = tweet['tweet_lm'].values\ny = tweet['sentiment_encoded'].values\n\nX_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.15)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","0877307f":"#Vectorize the data\nvectorizer = CountVectorizer(binary=False, ngram_range=(1, 2))\n#vectorizer = TfidfVectorizer(binary=True, ngram_range=(1, 2))\nvectorizer.fit(X_train)\nX_train_onehot = vectorizer.transform(X_train)\nX_test_onehot = vectorizer.transform(X_test)\n\n#print shape post encoding \nprint(X_train_onehot.shape)\nprint(X_test_onehot.shape)","2050ad4c":"# Generic function for model building\ndef fit_and_test(classifier, X_train, y_train, X_test, y_test, only_return_accuracy=False):\n  classifier.fit(X_train, y_train)\n  y_hat = classifier.predict(X_test)\n  print('accuracy:', accuracy_score(y_test, y_hat))\n  if not only_return_accuracy:\n    print('f1_score:', f1_score(y_test, y_hat,average='micro'))","c9f84386":"#SGDRegressor\nsgd = SGDClassifier() # 92.19%\nfit_and_test(sgd, X_train_onehot, y_train, X_test_onehot, y_test)\n","9263fec4":"from sklearn.feature_extraction.text import CountVectorizer\n#from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nNGRAM_RANGE = (1, 2)\nTOP_K = 30000\nTOKEN_MODE = 'word'\nMIN_DOC_FREQ = 1\n\ndef ngram_vectorize(texts, labels):\n    kwargs = {\n        'ngram_range' : NGRAM_RANGE,\n        'dtype' : 'int32',\n        'strip_accents' : 'unicode',\n        'decode_error' : 'replace',\n        'analyzer' : TOKEN_MODE,\n        'min_df' : MIN_DOC_FREQ,\n    }\n    # Learn Vocab from train texts and vectorize train and val sets\n    tfidf_vectorizer = TfidfVectorizer(**kwargs)\n    transformed_texts = tfidf_vectorizer.fit_transform(texts)\n    \n    print(\"Size of vocabulary = \", transformed_texts.shape[1]) #will have size of vocaulary\n    \n    # Select best k features, with feature importance measured by f_classif\n    # Set k as 20000 or (if number of ngrams is less) number of ngrams   \n    selector = SelectKBest(f_classif, k=min(TOP_K, transformed_texts.shape[1]))\n    selector.fit(transformed_texts, labels)\n    transformed_texts = selector.transform(transformed_texts).astype('float32')\n    return transformed_texts\n\n\n# Vectorize the data\n\nvect_data = ngram_vectorize(tweet['tweet_lm'], tweet['sentiment_encoded'])\n\nvect_data.shape[0]\n\nX = vect_data.toarray()\ny = (np.array(tweet['sentiment_encoded']))\n\n# Here we split data to training and testing parts\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=13)\nprint(\"Train dataset shape: {0}, \\nTest dataset shape: {1}\".format(X_train.shape, X_test.shape))\n\n","8500ce21":"# Get last layer units and activation function\ndef get_last_layer_units_and_activation(num_classes):\n    if num_classes == 2:\n        activation = 'sigmoid'\n        units = 1\n    else:\n        activation = 'softmax'\n        units = num_classes\n    return units, activation\n\nDROPOUT_RATE = 0.2\nUNITS = 128\nNUM_CLASSES = 3\nLAYERS = 4\ninput_shape = X_train.shape[1:]\n\nop_units, op_activation = get_last_layer_units_and_activation(NUM_CLASSES)\n\nmodel = Sequential()\n# Applies Dropout to the input\nmodel.add(Dropout(rate=DROPOUT_RATE, input_shape=input_shape))\nfor _ in range(LAYERS-1):\n    model.add(Dense(units=UNITS, activation='relu'))\n    model.add(Dropout(rate=DROPOUT_RATE))\n    \nmodel.add(Dense(units=op_units, activation=op_activation))\nmodel.summary()\n\nLEARNING_RATE = 1e-4\n\n# Compile model with parameters\nif NUM_CLASSES == 2:\n    loss = 'binary_crossentropy'\nelse:\n    loss = 'sparse_categorical_crossentropy'\noptimizer = tf.keras.optimizers.Adam(lr=LEARNING_RATE)\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n\nEPOCHS = 30\nBATCH_SIZE = 512\n","a450bcef":"# Create callback for early stopping on validation loss. If the loss does\n# not decrease on two consecutive tries, stop training\ncallbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)]\n\n# Train and validate model\n# To start training, call the model.fit method\u2014the model is \"fit\" to the training data.\n# Note that fit() will return a History object which we can use to plot training vs. validation accuracy and loss.\nhistory = model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_test, y_test), verbose=1, batch_size=BATCH_SIZE, callbacks=callbacks)\n","26f154b9":"Positive words like great, good, love, best, better, tanks ,amaze can be seen","dd35f95e":"Approach - 1 : Classical SGD Algorithm ","e4942136":"Android Ver = \"Varies with device\" has got maximum no of reviews in free as well as paid","b0a28046":"* Step 1 : Data cleaning starts","f709caa5":"Bowmasters app is at the top","3cd37233":"Most of the reviews are given for low zero price i.e. (Free apps)\n","a0c11295":"* Lets delete the missing values if all four columns \n* (Translated_Review,sentiment, sentiment polarity, sentiment subjetivity)are having null values using thresh\ni.e. Lets delete the missing values if no of Null values > 1 in a row","a09b1f7d":"* Overall Sentiment is Positive  \n* 64.1% (23998) sentiment is positive\n* 22.1% (8271) is negative\n* 13.8% (5163) is Neutral","13d1bb9a":"* Category = Social & communication have maximum reviews\n* game and video player also have good amount of reviews\n* Almost no reviews for Medical which is strange\n* Most of the reviews are present for free apps\n* Paid reviews are present for Game, weather and Family","31968454":"Average word length more for positive than other categories","8d7cc437":"All Null values are imputed\/removed","0376a81a":"41% (i.e. 26863 out of 64295) of the data in four columns is missing ","f4ea00c0":"> ****If you have liked my Kernel, PLEASE UPVOTE","52e2a278":"Neutral  words like work, need, use app, keep can be seen","c2138901":"Hello Friends,\n\nI have divided this kernel into two parts \n\n* Part 1 :  EDA for \"googleplaystore.csv\" and draw insights from the data\n* Part 2 :  Sentiment Analysis of the spreadsheet \"googleplaystore_user_reviews.csv\" and prediction  ","9e5947c2":"> **Please UPVote if you have liked my Kernel :)**","133ee9c8":"* Negative : 0-30 word length observed for 6900+ reviews\n* Positive : 0-20 word length observed for 14200+ reviews\n* Neutral  : 0-6 word length observed for 2700+ reviews","f9d76a5c":"In Overlapping kde plot - we can see that the density is higher for neutral for length (0-25)","2513cd60":"Data Cleaning starts","474103dc":"We can see skewness reduced to -0.04 ","74bb82fb":"Part 1 :  EDA for \"googleplaystore.csv\" and draw insights from the data","bac7c2b3":"Lets explore numerical variables - Sentiment_Polarity & Sentiment_Subjectivity","5c0c47a0":"* No of reviews for free as well as paid apps are highest for \"Everyone 10+\"\n* Under paid apps - \"Mature 17+\" content rating is at No. 2","d505e335":"1. Sentiment_Polarity :\n    * For positive - review polarity is highest (median : 0.37)\n    * For Negative - review polarity is lowest  (median : -0.12)\n    * For Neutral  - Its  0\n2. Sentiment_Subjectivity :\n    * For positive - review polarity is higher (median : 0.58)\n    * For Negative - review polarity is slightly lower than positive  (median : 0.55)\n    * For Neutral  - Many outliers can be seen","22cc5bd0":"Medium size category are having highet no of reviews\n","2a2a212e":"* Frequency of updates is way more on week days than week ends which is strange \n* It is highest on Thursday and lowest on Sunday","f7d5c3e1":"A Clear separation among positive, negative and neutral can be seen","a880fcf1":"Data cleaning completed","0c24ff50":"Text processing completed","d1faeeec":"* ****> We achieved 88% accuracy with simple Neural Network with N-gram=(1,2) and selecting 30k top features","2a3b0de1":"* Many (1474) missing values in rating column\n* Current Ver (8) & Andrioid Ver (3)\n* Type & Content Rating have 1 each","8e6b207e":"Part 2 : Sentiment Analysis of the spreadsheet \"googleplaystore_user_reviews.csv\" and prediction","8626656a":"* No of installs has strong positive correlation (0.64) with no of reviews which is kind of obvious\n* Type is positively correlated with Typw i.e. As Type becomes 1 (paid), price increases","f35ebd8f":"Lets inject text = \"Neutral\" for Sentiment=\"Neutral\"","01963f9d":"> ****End of Part 1 :  EDA for \"googleplaystore.csv\" and draw insights from the data","72c3272d":"* Category :\n    1. Family,Game & Tools Type Apps have highest count, whereas beauty, comics, parenting have lowest count.\n    2. It means people are more concerned about Family and gaming apps which is as expected.\n    3. Medical comes at No. 4.\n    4. Dating apps count are above Education which is interesting\n* Rating : \"No rating is at top followed by \"4.4,4.3 & 4.5 by count, 1.4, 1.5, 1.2 are lowest which means overall rating is good and peopl are happay about them.\n* Android Ver: 4.1 version has highest count whereas 2.2.7.1.1. has lowest count, \"Varies with device\" is at no. 4 which is interesting ver. category","242f165e":"Lets have a look at the cleaned data ","946c146b":"Lets do a correlation plot for entire dataframe ","ed0eb742":"No missing values , Lets begin with EDA","9729c812":"Step 2 : Exploratory Data analysis (EDA)","7bd96e85":"* Negative : 0-250 character length observed for 7500+ reviews\n* Positive : 0-125 character length observed for 14000+ reviews\n* Neutral  : 0-30 character length observed for 2700+ reviews","429a5adf":"* Apps with high size are having highest price\n* teen content related apps are having high price under high size category ","52bb4e8a":"Approach - 2 : Simple Neural Networks, N-gram=(1,2) and using TfidfVectorizer with Top 30k features","409a5ecc":"Still 5 missing values in column Translated review","d4700c29":"* July has highest updates against September whch has lowest. What can be the reason? Summer time ?  ","f220830d":"* We can see there are NAN values for aall four columns (Translated_Review,sentiment, sentiment polarity, sentiment subjetivity)on multiple rows. \n* Let see missing values in entire dataframe","bc837374":"Negative  words like waste time, bad, hate, frustrate can be seen","eeedfc69":"> ****So We recommend Approach 1 i.e. SGD algorithm which gave us 92% accuracy in prediting Positive,Negative and Neutral Sentiments  ","3beec538":"Frequency of free reviews is slightly higher for low size than paid reviews","bf0b7fb4":"Rating = 4.5 has got maximum no of reviews for free as well as paid apps\nRating = 4.6 stands at No. 2","0e7a9652":"> ****We achieved 92.23% accuracy with classical SGD algorithm","f6f664b6":"* Type : 92.6% apps are free Vs 7.4% are paid\n* Content rating : 80.4% are for everyone 11.1% for teen, Adults only and unrated are 0%","ef827a49":"26863 rows deleted","34a56b4d":"* 75% of the data lies below 4.5 rating\n* min rating is 1\n* maximum rating is 19 - seems to be an outlier"}}