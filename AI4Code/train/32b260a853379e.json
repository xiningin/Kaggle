{"cell_type":{"3ea88d52":"code","24a22aa9":"code","0d7d400c":"code","446609c4":"code","1dbbe89e":"code","8b74288f":"code","df048925":"code","b04e7581":"code","e3ba0dcb":"code","a82fd547":"code","6eb934f8":"code","bf2feb44":"code","9a8ec496":"code","526386fe":"code","e9633bbe":"code","b966b3f4":"code","d4fab7d8":"code","2cb643f9":"code","b4d5d8c0":"code","b0662970":"code","a0977d2d":"code","15cc8160":"code","cdd48d4d":"code","f4ab1541":"code","08ee11df":"code","11f204be":"code","9e561877":"code","0525dc0a":"code","8244392e":"code","be39bedc":"code","c51adda4":"code","5d63aec6":"code","3a041ecd":"code","dec8f28e":"code","ec12183c":"code","c156b52d":"code","4faf9f4e":"code","9be986cc":"code","415eb3b7":"code","f34bf52d":"code","8463530b":"code","96f9ae2b":"code","a132cf4a":"code","254d7c65":"code","27fe835e":"code","8bdefcf4":"code","2da891a8":"code","5e2d9bca":"code","984bd956":"code","dc2331d9":"code","682f193b":"code","97d4f044":"code","979e1c41":"code","c5d19cf9":"code","fd5225e2":"code","580dcd48":"code","6d536b42":"code","c7daffa7":"code","b977411b":"code","b35c0aeb":"code","6948674b":"code","42f92f18":"code","d4a2ce31":"code","f2fc7e11":"code","77a82731":"code","bf60afd3":"code","606a9d24":"code","50bd967e":"code","17e6b619":"code","7a2e0488":"code","5d52dd83":"code","aeae5e3a":"code","f71e30c3":"code","3bbfcf28":"code","2968f7ff":"markdown","e3b09251":"markdown","452daffb":"markdown","ce3f5eba":"markdown","b852ab54":"markdown","5e1e706f":"markdown","aef5ec86":"markdown","5f41f082":"markdown","a57bcf4c":"markdown","0f800192":"markdown","ceda10ba":"markdown","e108faf2":"markdown","983ef793":"markdown","93eedddf":"markdown","5053d37a":"markdown","f709712d":"markdown","4228d267":"markdown","56cf8304":"markdown","f0683a39":"markdown","bfec2c18":"markdown","b784124e":"markdown","c6d8e3d6":"markdown","c65f1311":"markdown","8e5b2efd":"markdown","f8624807":"markdown","ac429d60":"markdown","9d238640":"markdown","49a28b16":"markdown","b8f1427a":"markdown","ab819c90":"markdown","313f9e15":"markdown","d41565d5":"markdown","0d40be00":"markdown","99b4d778":"markdown"},"source":{"3ea88d52":"#from google.colab import drive\n#drive.mount('\/content\/drive')","24a22aa9":"#!pip install -U tensorflow-addons","0d7d400c":"import keras\nimport tensorflow as tf\nfrom keras import applications\nfrom keras import optimizers\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator, img_to_array,load_img\nfrom keras.layers import (Input,GlobalAveragePooling2D, Multiply, Flatten,Dense, Dropout, Conv2D, BatchNormalization, MaxPooling2D, MaxPool2D)\nfrom keras.applications.resnet50 import preprocess_input\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint # Se importa para guardar el mejor modelo\nfrom tensorflow.keras import Model,layers\nimport tensorflow_addons as tfa\nfrom PIL import Image\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport os\n%pylab inline","446609c4":"# Esta parche s\u00f3lo es necesario si se ejcuta en Colab.\n#from google.colab.patches import cv2_imshow # Colab no admite cv2.imshow y ha creado este parche","1dbbe89e":"# All rellevant imports\nimport tensorflow as tf\n\nprint(\"TF version   : \", tf.__version__)\n# we'll need GPU!\nprint(\"GPU available: \", tf.config.list_physical_devices('GPU'))\n\n# keras version is 2.4.3\nimport keras\nprint(\"Keras version   : \", keras.__version__)","8b74288f":"# Deficici\u00f3n del directorio contenedor de im\u00e1genes\n#path = '\/content\/drive\/MyDrive\/archive-2_COPY\/train'\n#path = '\/content\/drive\/MyDrive\/Pr\u00e1ctica_final\/archive-2\/train'\npath = \"..\/input\/kaggle-plant-pathology-2021-modificat\/train\"\n\n# Se muestran 5 im\u00e1genes de cada clase\n# Se cuentan las im\u00e1genes por carpeta\n\n# Obtenci\u00f3n de los  directorios\nfolders = ([name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))]) \ncount=0\nfor folder in folders:\n    # Obtenci\u00f3n del contenido de los directorios\n    contents = os.listdir(os.path.join(path,folder))\n    print(\"**************\")\n    print (\"La carpeta {} contiene {} im\u00e1genes \".format(folder, len(contents)))\n    count_i= len(contents)\n    count += count_i\n    for i, image in enumerate(contents):\n        if i<5:\n            fullpath = os.path.join(path,folder,image)\n            image = cv2.imread(fullpath)\n            # Se escalan las im\u00e1genes para reducir su tama\u00f1o\n            scale_percent = 60 # Porcentaje del original\n            width = int(image.shape[1] * scale_percent \/ 100)\n            height = int(image.shape[0] * scale_percent \/ 100)\n            dim = (width, height)\n            resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n            im = Image.open(fullpath)\n            #cv2_imshow(resized)\n            plt.imshow(resized)\n            plt.show()\nprint(\"**********************************\")\nprint (\"El n\u00famero total de im\u00e1genes es {}\".format(count))","df048925":"# Se crea un alista vac\u00eda para alamcenar los nombres de los fotos\ndata = []\n# Se recorren los directorios y se extraen los nombres\nfor r, d, f in os.walk(path):\n    for file in f:\n        if \".jpg\" in file:\n            #data.append((r.split('\/')[-1],os.path.join(r.split('\/')[-1],file)))\n            data.append((r.split('\/')[-1],file))\n# Se crea un dataframe con los nombres de las fotos y el nombre de las carpeta contenedora\ndf_l = pd.DataFrame(data,columns=['labels','file'])\ncount_dict = df_l.labels.value_counts()\nclasses = list(count_dict.index)\nclasses_count = list(count_dict.values)\nprint(\"N\u00famero de etiquetas: \",len(classes))\nprint(\"-------------------------------------------\")\ndf_l.head()","b04e7581":"def get_label_info(img_labels):\n    \"\"\"\n    Se obtiene el n\u00famero de imagenes por directorio y se calcula el porcentaje \n    Argumentos:\n      - img_labels: Dataframe con las im\u00e1genes y sus etiquetas\n    Return: Nuevo dataframe con la cantidad y el porcentaje por etiqueta\n    \"\"\"\n    df_l_count = df_l.groupby(by='labels').count().reset_index()\n    df_l_count.columns = ['labels', 'count']\n    \n    df_l_count['%'] = np.round((df_l_count['count'] \/ df_l.shape[0]), 2) * 100\n    df_l_count = df_l_count.set_index('labels').sort_values(by='count', ascending=False)\n\n    return df_l_count\n\nget_label_info(df_l)","e3ba0dcb":"def plot_label_counts(img_labels):\n    '''\n    La funci\u00f3n crea una diagrama de barras con las columnas\n    Fuente: \"https:\/\/www.kaggle.com\/ted0071\/plant2021-preprocessing\" \n    '''\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.set_style(\"whitegrid\")\n    palette = sns.color_palette(\"Blues_r\", 12)\n    sns.countplot(\n      x='labels', \n      palette=palette,\n      data=img_labels,\n      order=img_labels['labels'].value_counts().index,\n      );\n    plt.ylabel(\"N\u00ba de im\u00e1genes\", size=20);\n    plt.xlabel(\"Label\/Enferemedad\", size=20)\n    plt.xticks(rotation=45) \n    fig.tight_layout()\n    plt.show()\n    \nplot_label_counts(df_l)    ","a82fd547":"# Se copia usando Linux\n# %cd \/content\/drive\/MyDrive\/Pr\u00e1ctica_final\n#\u00a0%cp -av archive-2 archive-2_COPY","6eb934f8":"'''\nfrom PIL import Image\n#path = '\/content\/drive\/MyDrive\/Pr\u00e1ctica_final\/archive-2\/train'\n#path_2 = '\/content\/drive\/MyDrive\/Pr\u00e1ctica_final\/archive-2_COPY\/train'\n#path = \"..\/input\/kaggle-plant-pathology-2021-modificat\/train\"\n\n# Obtenci\u00f3n de los  directorios\nfolders = ([name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))]) \nfolders_2 = ([name for name in os.listdir(path_2) if os.path.isdir(os.path.join(path_2, name))]) \n\nfor folder in folders:\n    # Obtenci\u00f3n del contenido de los directorios\n    contents = os.listdir(os.path.join(path,folder))\n\n    for i, image in enumerate(contents):\n        fullpath = os.path.join(path,folder,image)\n        fullpath_2 = os.path.join(path_2,folder,image)\n\n        img = Image.open(fullpath) # image extension *.png,*.jpg\n        new_width  = 150\n        new_height = 150\n        img = img.resize((new_width, new_height), Image.ANTIALIAS)\n        img.save(fullpath_2)\n'''","bf2feb44":"'''\npath = '\/content\/drive\/MyDrive\/Pr\u00e1ctica_final\/archive-2\/test'\npath_2 = '\/content\/drive\/MyDrive\/Pr\u00e1ctica_final\/archive-2_COPY\/test'\n\ndirs = os.listdir(path) \n\nfor item in dirs:\n  #if os.path.isfile(path+item):\n    fullpath = os.path.join(path,item)\n    fullpath_2 = os.path.join(path_2,item)\n    #destiny = os.path.join(path_2,item)\n    im = Image.open(fullpath)\n    #f, e = os.path.splitext(path+item)\n    imResize = im.resize((150,150), Image.ANTIALIAS)\n    imResize.save(fullpath_2)\n'''","9a8ec496":"'''\nruta = '\/content\/drive\/MyDrive\/Pr\u00e1ctica_final\/archive-2_COPY\/test\/00z8ig2jyH.jpg'\nimg = Image.open(ruta) # image extension *.png,*.jpg\n\nwidth, height = img.size\nprint(width, height)\n'''","526386fe":"# Se define el directorio que contiene las im\u00e1genes de entrenamiento\npath_train =\"..\/input\/kaggle-plant-pathology-2021-modificat\/train\"\n","e9633bbe":"# Se accede al directorio y se leen las im\u00e1genes\n# Se crea la partici\u00f3n de Train y validaci\u00f3n\n# Genera lotes de datos de im\u00e1genes tensores con aumento de datos en tiempo real.\n# Los datos se repiten en bucle (por lotes).\ndata_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1\/255.0, \n                                                                zoom_range=0.2, # Se hace zoom\n                                                                width_shift_range=0.1, # Se modifica el ancho\n                                                                height_shift_range = 0.1, # Se modifica el largo\n                                                                rotation_range=15, # Se rotan\n                                                                fill_mode=\"nearest\", # Se rellenan algunos puntos\n                                                                horizontal_flip=True, # Se gira sobre el eje horizontal\n                                                                validation_split=0.3) # Se define el set de validaci\u00f3n\n# Se crea el conjunto train\ntrain_data_loader = data_generator.flow_from_directory(path_train,\n                                                     target_size=(150,150), # Se ajusta el tama\u00f1o de la im\u00e1gen\n                                                     batch_size=64, # Se define el tama\u00f1o del lote\n                                                     shuffle=True, # Elecci\u00f3n aleatoria de fotos\n                                                     class_mode='categorical', # Es una clasificai\u00f3n multilabel\n                                                     subset='training') \n\n","b966b3f4":"# Se crea el conjunto validaci\u00f3n\nval_data_loader =  data_generator.flow_from_directory(path_train,\n                                                    target_size=(150,150), # Se ajusta el tama\u00f1o de la im\u00e1gen\n                                                    batch_size=64, # Se define el tama\u00f1o del lote\n                                                    shuffle=False, # Elecci\u00f3n No aleatoria de fotos\n                                                    class_mode='categorical', # Es una clasificai\u00f3n multilabel\n                                                    subset='validation')  \n\n         \n                  ","d4fab7d8":"# Lista con las etiquetas de las\u00e1g imenes\ndict_classes = train_data_loader.class_indices\n# Tama\u00f1o de las i\u00e1gmenes\ninput_shape=train_data_loader.image_shape","2cb643f9":"dict_classes","b4d5d8c0":"fig = plt.figure(figsize=(15, 10))\nnpics= 16\ncount = 1\nfor i in range(npics):\n    x,y = train_data_loader.next()\n    image = x[0]\n    #print (image.shape)\n    label = y[0]  \n    ax = fig.add_subplot(npics\/4 , 4, count, xticks=[],yticks=[])\n    ax.set_title(label, fontsize=10)  \n    plt.imshow(image)\n    count = count + 1  \n\nplt.tight_layout()\nplt.show()","b0662970":"from keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","a0977d2d":"f1_score = tfa.metrics.F1Score(num_classes=6, average='macro')","15cc8160":"# Funci\u00f3n tomada de los ejemplos del libro \"Libro-Deep-Learning\" \n# Plot del training loss y el accuracy\ndef plot_prediction(n_epochs, mfit):\n    '''\n    La funci\u00f3n resprenta la exactitud y la p\u00e9rdida del modelo\n    Los argumentos son:\n    n_epochs: N\u00famero de \u00e9pocas para la reprentaci\u00f3n del eje x\n    mfit: resultado del modelo ejecutado para obtener los valores de accuracy y loss.\n\n    La funci\u00f3n devuelve un diagrama con los resultados\n\n    '''\n    N = n_epochs\n    plt.style.use(\"ggplot\")\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,6))\n    fig.suptitle('Training Loss, Accuracy , F1-Score')\n    \n    ax1.plot(np.arange(0, N), mfit.history[\"accuracy\"], label=\"train\")\n    ax1.plot(np.arange(0, N), mfit.history[\"val_accuracy\"], label=\"val\")\n\n    ax1.set_title(\"Accuracy\")\n    ax1.set_xlabel(\"Epoch #\")\n    ax1.set_ylabel(\"Accuracy\")\n    ax1.legend(loc=\"lower right\")\n    \n    ax2.plot(np.arange(0, N), mfit.history[\"loss\"], label=\"train\")\n    ax2.plot(np.arange(0, N), mfit.history[\"val_loss\"], label=\"val\")\n\n    ax2.set_title(\"Loss\")\n    ax2.set_xlabel(\"Epoch #\")\n    ax2.set_ylabel(\"Loss\")\n    ax2.legend(loc=\"upper right\")\n\n    #ax3.plot(np.arange(0, N), mfit.history[\"f1_m\"], label=\"train_f1_score\")\n    #ax3.plot(np.arange(0, N), mfit.history[\"val_f1_m\"], label=\"val_f1_score\")\n    ax3.plot(np.arange(0, N), mfit.history[\"f1_score\"], label=\"train_f1_score\")\n    ax3.plot(np.arange(0, N), mfit.history[\"val_f1_score\"], label=\"val_f1_score\")\n\n    ax3.set_title(\"F1_Score\")\n    ax3.set_xlabel(\"Epoch #\")\n    ax3.set_ylabel(\"Fi_Score\")\n    ax3.legend(loc=\"upper right\")\n    \n    plt.show()","cdd48d4d":"# Se definen las variables necesarias para los modelos\nBATCH_SIZE = 16\nEPOCH = 15\nOPTIMIZER = tf.keras.optimizers.Adam(learning_rate=0.00001)\nOPTIMIZER_2 = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\nsteps_per_epoch_training = train_data_loader.n\/\/train_data_loader.batch_size\nsteps_per_epoch_validation = val_data_loader.n\/\/val_data_loader.batch_size","f4ab1541":"#path_save= '\/content\/drive\/MyDrive'\npath_save= '\"..\/kaggle\/working\/checkpoint_model_save'\n\n\ncheckpoint=ModelCheckpoint(path_save,\n                          monitor='val_loss',\n                          save_best_only=True,\n                          verbose=1)\nearlystop=EarlyStopping(monitor='val_loss',\n                       patience=10,\n                       verbose=1,\n                       restore_best_weights=True)\n\ncallbacks=[checkpoint,earlystop]","08ee11df":"Callbacks = [EarlyStopping(monitor='val_loss', patience=7, verbose=1 )]","11f204be":"from tensorflow.keras.applications.inception_v3 import InceptionV3","9e561877":"base_inceptionV3 = InceptionV3(\n                    #weights='\/content\/drive\/MyDrive\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',\n                    weights='..\/input\/inceptionv3\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',\n                    include_top=False, \n                    pooling='avg', \n                    input_shape=(150, 150, 3))\n\nbase_inceptionV3.layers[0].trainable = True \n\nmodel_inceptionV3 = Sequential()\nmodel_inceptionV3.add(base_inceptionV3)\nmodel_inceptionV3.add(layers.Flatten())\nmodel_inceptionV3.add(layers.Dropout(0.2))\nmodel_inceptionV3.add(layers.Dense(1024, activation='relu'))\nmodel_inceptionV3.add(layers.Dropout(0.2))\nmodel_inceptionV3.add(layers.Dense(6, activation='softmax'))\n\nmodel_inceptionV3.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy', f1_score])\n#base_inceptionV3.summary()\n\n","0525dc0a":"history = model_inceptionV3.fit(train_data_loader, \n                            validation_data=val_data_loader, \n                            batch_size=BATCH_SIZE,\n                            epochs=EPOCH,\n                            steps_per_epoch=steps_per_epoch_training,\n                            validation_steps=steps_per_epoch_validation,\n                            callbacks=Callbacks)","8244392e":"plot_prediction(len(history.epoch), history)","be39bedc":"#model_inceptionV3.save('\/content\/drive\/MyDrive\/Plant_Pathology_InceptionV3_model.h5')\nmodel_inceptionV3.save('\/kaggle\/working\/Plant_Pathology_InceptionV3_model_complet.h5')\n#model_inceptionV3.save('\/input\/Plant_Pathology_InceptionV3_model_complet.h5')","c51adda4":"base_inceptionV3_2 = InceptionV3(\n                    weights='..\/input\/inceptionv3\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',\n                    #weights='imagenet',\n                    include_top=False,\n                    #pooling='avg', \n                    input_shape=input_shape)\n\nfor layer in base_inceptionV3_2.layers:\n     layer.trainable = True\n\n#last_layer = base_inceptionV3_2.get_layer('mixed7') \n#last_output = last_layer.output\n\n\nx = base_inceptionV3_2.output\nx = GlobalAveragePooling2D()(x)\n#x = layers.Dense(256, activation='relu')(x)\n#x = Dropout(0.5)(x)\n\nx = layers.Dense  (6, activation='softmax')(x)           \n\nmodel_inceptionV3_2 = Model(base_inceptionV3_2.input, x)\nmodel_inceptionV3_2.compile(optimizer = OPTIMIZER, \n              loss = 'categorical_crossentropy', \n              metrics=['accuracy', f1_score])\n    \n#model_inceptionV3_2.summary()","5d63aec6":"history = model_inceptionV3_2.fit_generator(train_data_loader, \n                            validation_data=val_data_loader, \n                            ##batch_size=BATCH_SIZE,\n                            epochs=EPOCH,\n                            steps_per_epoch=steps_per_epoch_training,\n                            validation_steps=steps_per_epoch_validation,\n                            callbacks=Callbacks)","3a041ecd":"plot_prediction(len(history.epoch), history)","dec8f28e":"model_inceptionV3_2.save('\/kaggle\/working\/Plant Pathology_inceptionV3_2.h5')\n#model_inceptionV3_2.save('\/input\/Plant Pathology_inceptionV3_2.h5')","ec12183c":"base_inceptionV3_3 = InceptionV3(\n                    weights='..\/input\/inceptionv3\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',\n                    #weights='imagenet',\n                    include_top=False,\n                    #pooling='avg', \n                    input_shape=input_shape)\n\n\nx = base_inceptionV3_3.output\nx = GlobalAveragePooling2D()(x)\n#x = layers.Dense(256, activation='relu')(x)\n#x = Dropout(0.5)(x)\n\nx = layers.Dense  (6, activation='softmax')(x)           \n\nmodel_inceptionV3_3 = Model(base_inceptionV3_3.input, x)\nmodel_inceptionV3_3.compile(optimizer = OPTIMIZER, \n              loss = 'categorical_crossentropy', \n              metrics=['accuracy', f1_score])\n    \n#model_inceptionV3_2.summary()","c156b52d":"for layer in model_inceptionV3_3.layers[:275]:\n    layer.trainable = False\nfor layer in model_inceptionV3_3.layers[275:]:\n    layer.trainable = True\n    \nmodel_inceptionV3_3.compile(optimizer = OPTIMIZER, \n                            loss = 'categorical_crossentropy', \n                            metrics=['accuracy', f1_score])","4faf9f4e":"history_2 = model_inceptionV3_3.fit_generator(train_data_loader, \n                            validation_data=val_data_loader, \n                            ##batch_size=BATCH_SIZE,\n                            epochs=EPOCH,\n                            steps_per_epoch=steps_per_epoch_training,\n                            validation_steps=steps_per_epoch_validation,\n                            callbacks=Callbacks)","9be986cc":"plot_prediction(len(history_2.epoch), history_2)","415eb3b7":"model_inceptionV3_3.save('\/kaggle\/working\/Plant Pathology_inceptionV3_3_2.h5')\n#model_inceptionV3_3.save('\/input\/Plant Pathology_inceptionV3_3_2.h5')","f34bf52d":"data_generator_2 = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=keras.applications.resnet50.preprocess_input, \n                                                                zoom_range=0.2, \n                                                                width_shift_range=0.1, \n                                                                height_shift_range = 0.1, \n                                                                rotation_range=15,\n                                                                fill_mode=\"nearest\",\n                                                                horizontal_flip=True,\n                                                                validation_split=0.3)\n\ntrain_data_loader_2 = data_generator_2.flow_from_directory(path_train,\n                                                     target_size=(224,224),\n                                                     batch_size=64,\n                                                     shuffle=True,\n                                                     class_mode='categorical',\n                                                     subset='training') ","8463530b":"val_data_loader_2 =  data_generator_2.flow_from_directory(path_train,\n                                                    target_size=(224,224),\n                                                    batch_size=64,\n                                                    shuffle=False,\n                                                    class_mode='categorical',\n                                                    subset='validation')  ","96f9ae2b":"# Se comprueba el tama\u00f1o de la imagen\ntrain_data_loader_2.image_shape","a132cf4a":"#Se confirma la normalizaci\u00f3n de los datos\nbatchX, batchy = train_data_loader_2.next()\nprint('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))\n\n#Se confirma la normalizaci\u00f3n de los datos\nbatchX, batchy = val_data_loader_2.next()\nprint('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))","254d7c65":"input_shape=train_data_loader_2.image_shape\ninput_shape","27fe835e":"from tensorflow.keras.applications import ResNet50\n\n# Se utilizan los pesos de Imagenet y la red es una ResNet50.\n# La opci\u00f3n include_top=False permite extraer caracter\u00edsticas eliminando las \u00faltimas capas densas. \n# Se crea el modelo base\nbase_model = ResNet50(weights='imagenet', \n                      include_top = False, \n                      input_shape=train_data_loader_2.image_shape)\n\n#base_model.summary()\n\nbase_model.layers[0].trainable = False \n\n#pretrained_model.summary()","8bdefcf4":"# Se crea un nuevo modelo \nnum_classes = 6\n\n# Baseline (ResNet50 + Flattern)\nx = base_model.output\nx = Flatten()(x)\n# Clasificaci\u00f3n\nx = Dense(1024, activation='relu')(x)\nx = Dropout(0.5)(x)  \npreds = Dense(num_classes, activation ='softmax')(x)\n\nmodel_ResNet50 = Model(inputs=base_model.input, outputs=preds)\n\n#print(model_ResNet50.summary())\n\nmodel_ResNet50.compile(optimizer = OPTIMIZER, \n              loss = 'categorical_crossentropy', \n              metrics=['accuracy', f1_score])","2da891a8":"history_ResNet50 = model_ResNet50.fit_generator(train_data_loader_2, \n                            validation_data=val_data_loader_2, \n                            ##batch_size=BATCH_SIZE,\n                            epochs=EPOCH,\n                            steps_per_epoch=steps_per_epoch_training,\n                            validation_steps=steps_per_epoch_validation,\n                            callbacks=callbacks)","5e2d9bca":"plot_prediction(len(history_ResNet50.epoch), history_ResNet50)","984bd956":"model_ResNet50.save('\/kaggle\/working\/Plant Pathology_ResNet50.h5')\n#model_ResNet50.save('\/input\/Plant Pathology_ResNet50.h5')","dc2331d9":"base_model = ResNet50(include_top=False, \n                      input_shape=train_data_loader_2.image_shape,\n                      weights=\"imagenet\")\n\noutput = base_model.layers[-1].output\noutput = keras.layers.Flatten()(output)\n\nrestnet = Model(base_model.input, output)\n\n#restnet.summary()","682f193b":"restnet.trainable = True\nset_trainable = False\nfor layer in restnet.layers:\n    if layer.name in ['res5c_branch2b', 'res5c_branch2c', 'activation_97']:\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False\nlayers = [(layer, layer.name, layer.trainable) for layer in restnet.layers]\npd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])","97d4f044":"model_finetuned = Sequential()\nmodel_finetuned.add(restnet)\nmodel_finetuned.add(Flatten())\nmodel_finetuned.add(Dense(1024, activation='relu'))\nmodel_finetuned.add(Dropout(0.5))\nmodel_finetuned.add(Dense(num_classes, activation='softmax'))\n\nmodel_finetuned.compile(optimizer = OPTIMIZER, \n              loss = 'categorical_crossentropy', \n              metrics=['accuracy', f1_score])\n#model.summary()","979e1c41":"history_ResNet50_3 = model_finetuned.fit_generator(train_data_loader_2, \n                            validation_data=val_data_loader_2, \n                            ##batch_size=BATCH_SIZE,\n                            epochs=EPOCH,\n                            steps_per_epoch=steps_per_epoch_training,\n                            validation_steps=steps_per_epoch_validation,\n                            callbacks=callbacks)","c5d19cf9":"plot_prediction(len(history_ResNet50_3.epoch), history_ResNet50_3)","fd5225e2":"model_finetuned.save('\/kaggle\/working\/Plant Pathology_ResNet50_3.h5')\n#model_ResNet50_2.save('\/input\/Plant Pathology_ResNet50_2_2.h5')","580dcd48":"f1_score_inceptionV3 = model_inceptionV3.evaluate(val_data_loader,verbose=1)\nprint ('f1 score: ', f1_score_inceptionV3[2])\n\nf1_score_inceptionV3_2 = model_inceptionV3_2.evaluate(val_data_loader,verbose=1)\nprint ('f1 score: ', f1_score_inceptionV3_2[2])\n\nf1_score_inceptionV3_3 = model_inceptionV3_3.evaluate(val_data_loader,verbose=1)\nprint ('f1 score: ', f1_score_inceptionV3_3[2])\n\n# Este \u00faltimo modelo se sustituy\u00f3 por f1_score_model_ResNet50_3.\n#f1_score_model_ResNet50_2 = model_ResNet50_2.evaluate(val_data_loader_2,verbose=1)\n#print ('f1 score: ', f1_score_model_ResNet50_2[2])","6d536b42":"f1_score_model_ResNet50 = model_ResNet50.evaluate(val_data_loader_2,verbose=1)\nprint ('f1 score: ', f1_score_model_ResNet50[2])\n\n# Este \u00faltimo modelo se sustituy\u00f3 por f1_score_model_ResNet50_3.\n#f1_score_model_ResNet50_3 = model.evaluate(val_data_loader_2,verbose=1)\n#print ('f1 score: ', f1_score_model_ResNet50_3[2])","c7daffa7":"f1_score_model_ResNet50_3 = model_finetuned.evaluate(val_data_loader_2,verbose=1)\nprint ('f1 score: ', f1_score_model_ResNet50_3[2])","b977411b":"# Se predicie\nprediction = model_ResNet50.predict(val_data_loader_2, steps=val_data_loader_2.n\/\/val_data_loader_2.batch_size +1)","b35c0aeb":"# Se redondea el resultadopara comparar los enteros\nprediction_ci_50=np.argmax(prediction,axis=1)\nprint('Predicciones redondeadas: \\n', prediction_ci_50)","6948674b":"# Para obtener el nombre de las etiquetas\ntrue_classes = val_data_loader_2.classes\nclass_labels = list(val_data_loader_2.class_indices.keys())","42f92f18":"# Se importan m\u00e9tricas\nfrom sklearn.metrics import (precision_score, recall_score, roc_curve,\n                             confusion_matrix, precision_recall_curve, auc,\n                             classification_report, f1_score,\n                             precision_recall_fscore_support, accuracy_score)","d4a2ce31":"# Se obtienen el reporte de las m\u00e9tricas\nreport = classification_report(true_classes, prediction_ci_50, target_names=class_labels)\nprint(report) ","f2fc7e11":"# Se crea la matriz de confusi\u00f3n y se representa\n\ncm=confusion_matrix(val_data_loader_2.labels, prediction_ci_50)\ndf_cm = pd.DataFrame(cm, columns=np.unique(class_labels), index = np.unique(class_labels))\n\nplt.figure(figsize=(12, 12))\nsns.heatmap(df_cm, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","77a82731":"# Se define el directorio contenedor de las im\u00e1genes test\ntest_dir = '..\/input\/kaggle-plant-pathology-2021-modificat\/test\/'\n# Se crea un dataframe con el nombre de las im\u00e1genes\ntest_df = pd.DataFrame()\ntest_df['image'] = os.listdir(test_dir)","bf60afd3":"# Como se hizo anteriormente, se extraen las fotos del directorio test.\n# En esta ocasi\u00f3n no se genran im\u00e1genes y se accede a las im\u00e1genes mediante \n# flow_from_dataframe.\ntest_set = data_generator_2.flow_from_dataframe(dataframe=test_df,\n                                    directory=test_dir,\n                                    x_col=\"image\",\n                                    y_col=None,\n                                    batch_size=64,\n                                    seed=42,\n                                    shuffle=False,\n                                    class_mode=None,\n                                    target_size=(224,224))\n","606a9d24":"# Se predice utilizando el modelo seleciionado\npred_ResNet50= model_ResNet50.predict(test_set).tolist()","50bd967e":"# Se redondea la predicci\u00f3n a un n\u00famero entero\nfor i in range(len(pred_ResNet50)):\n    pred_ResNet50[i] = np.argmax(pred_ResNet50[i])","17e6b619":"res = []\n \nfor i in range(len(pred_ResNet50)):\n    try:\n        res.append(most_common([pred_ResNet50[i]]))\n    except:\n        res.append(pred_ResNet50[i])","7a2e0488":"def get_key(val):\n    '''\n    Esta funci\u00f3n compara la etiqueta predicha con el valor del\n    diccionario y devuelve la llave o nombre de la etiqueta\n    '''\n    for key, value in dict_classes.items():\n        if val == value:\n            return key","5d52dd83":"# Se extrae del array creado anteriormente el elemento corresondiente a la etiqueta\nfor i in range(len(res)):\n    res[i] = get_key(res[i])","aeae5e3a":"# Se crea una columna nueva y se a\u00f1aden las nombres de las etiquetas\ntest_df['labels'] = res\ntest_df","f71e30c3":"# Se extrael el Dataframe a un archivo .csv\ntest_df.to_csv('test.csv', index=False)","3bbfcf28":"# Se visualiza el resultado \nplot_label_counts(test_df) ","2968f7ff":"### A continuaci\u00f3n se elege el modelo con mejor rendimiento (aquel que tiene la m\u00e1xima F1 sobre el conjunto de validaci\u00f3n).\n","e3b09251":"Esta red requiere un preprocesado de ls datos distinto.","452daffb":"## Modelos","ce3f5eba":"## flow_from_directory","b852ab54":"## Importaci\u00f3n de librer\u00edas relevantes","5e1e706f":"Se revisana algunas de las im\u00e1genes creadas","aef5ec86":"## Definici\u00f3n de la estrategia de validaci\u00f3n y de aumento de datos sobre el conjunto de entrenamiento","5f41f082":"El modelo probado ofrece buenos resultados en las m\u00e9tricas.","a57bcf4c":"## An\u00e1lisis exploratorio de los datos","0f800192":"## Visualizaci\u00f3n","ceda10ba":"Se crea una funci\u00f3n para obtener el n\u00famero de im\u00e1genes y el porcentaje como dataframe","e108faf2":"## __1. inception_v3__","983ef793":"Se seleccionan las capas que se van a entrenar. A diferencia de la celda anterior, con este c\u00f3digo podemos hacer un mejor fine tuning del entrenamineto.\n\nSe decide entrenar una parte del modelo. Lo hacemos \"descongelado\" algunos de los \u00faltimos bloques de convoluci\u00f3n mientras mantenemos congelados los primeros bloques de conv. Esto nos ayudar\u00e1 a aprender caracter\u00edsticas muy gen\u00e9ricas utilizando las primeras capas. Las capas superiores de los modelos preentrenados se podr\u00e1n entrenar o afinar.","93eedddf":"Se preparan los datos como se ha hecho con los conjuntos train y validation ","5053d37a":"## RestNet50","f709712d":"# Ejercicio 3","4228d267":"Se hace un fine tuning del modelo y se entrenan s\u00f3lo las \u00faltimas 50 capas","56cf8304":"## RestNet50_2","f0683a39":"Mediante est\u00e1 m\u00e9trica creada se calcular\u00e1 F1-Score.\nSe usar\u00e1 esta en lugar de las anteririores.","bfec2c18":"# Ejercicio 2 ","b784124e":"## __3. inception_v3_3 Fine tuning 2__","c6d8e3d6":"## M\u00e9tricas","c65f1311":"### Se calculan las predicciones del conjunto de test proporcionado.","8e5b2efd":"## Fin","f8624807":"La clase [ImageDataGenerator](https:\/\/keras.io\/api\/preprocessing\/image\/) nos permite cargar las im\u00e1genes, aplicar diferentes normalizaciones, t\u00e9cnicas de aumentaci\u00f3n (random flip, rotation, etc) as\u00ed como definir el porcentaje de los datos que queremos usar para validaci\u00f3n. Adem\u00e1s, esta clase contiene el m\u00e9todo flow_from_directory; que resulta muy \u00fatil cuando tenemos las im\u00e1genes organizadas por directorios (que corresponden a las diferentes categor\u00edas).  \n\nLa estrateg\u00eda de aumento de datos se ha incluido en este apartado, dado que tiene m\u00e1s sentido generar los conjuntos de validaci\u00f3n y entrenemineto mientras se aumentas los datos disponibles.","ac429d60":"### Ejercicio 4","9d238640":"El dataset est\u00e1 bastante desbalanceado","49a28b16":"Se comprueba el tama\u00f1o de las im\u00e1genes","b8f1427a":"\nSe crea un dataframe pra listar que contienen las im\u00e1genes y loas etiquetas.  \n\nNo se conoce el nombre de las enferemedades pero dado que las im\u00e1genes est\u00e1m clasificadas por carpeta se otorgar\u00e1 el nombre de la carpeta como la etiqueta de la im\u00e1gen.","ab819c90":"# Ejercicio 1","313f9e15":"Como parte del preprocesao se pens\u00f3 en ajustar desde el principio el tama\u00f1o de las im\u00e1genes. El c\u00f3digo que se muestra a continuaci\u00f3ncopia el directorio original con las subcarpeta y los archivos.  \n\nEl c\u00f3digo es bash Linux y se puede ejecutar directamente desde el notebook.\n\nImportante: Si no se desea crear una copia de los archivos se debe omitir este paso.","d41565d5":"## __2. inception_v3_2_Fine tuning__","0d40be00":"Una vez creada la copia se modifican las im\u00e1genes. \n\nEste paso no es necesario. Se prob\u00f3 para acelerar la carga de las im\u00e1genes en el modelo mientras se usaba Colab.","99b4d778":"El mejor f1_Score (avg) se ha obtenido con el modelo __ResNet50__"}}