{"cell_type":{"2189b4e7":"code","2fb5b065":"code","ee768cd8":"code","89fa7255":"code","b70f6c06":"code","ca5bb3ae":"code","6aeba9c2":"code","03853385":"code","9223ea29":"code","2c0bae23":"code","b8acd567":"code","596ddda5":"code","4fd0b017":"code","12c231a1":"code","ebe777ca":"code","ee2c384c":"markdown","e40b180d":"markdown","794e2bef":"markdown"},"source":{"2189b4e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2fb5b065":"df = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv')\n# df_test = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')","ee768cd8":"df = df.drop(columns = 'id')\n# df_test = df_test.drop(columns = 'id')","89fa7255":"x = df.drop(columns = 'target')\ny = df.target","b70f6c06":"# import optuna\n# from sklearn.model_selection import train_test_split\n# import lightgbm as lgb\n# from sklearn import metrics\n\n# def objective(trial):\n    \n#     train_x, test_x, train_y, test_y = train_test_split(x,y, test_size=0.2)\n \n#     params = {\n#         'objective': 'binary',\n#         'metric': 'auc',\n#         #####################'n_estimators': trial.suggest_categorical(\"n_estimators\", [10000]),\n#         'learning_rate': trial.suggest_float(\"learning_rate\", 0.001, 0.3),\n#         'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n#         'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n#         'num_leaves': trial.suggest_int('num_leaves', 2, 100),\n#         'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0),\n#         'bagging_fraction': trial.suggest_float(\"bagging_fraction\", 0.2, 0.95, step=0.1),\n#         'subsample': trial.suggest_float(\"subsample\", 0.2, 0.9, step=0.1),\n#         'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n#     }\n    \n#     model = lgb.LGBMClassifier(**params,device = 'gpu')\n#     model.fit(train_x,train_y,eval_set=[(test_x,test_y)], early_stopping_rounds=50)\n\n#     preds = model.predict_proba(test_x)[:,1]\n#     roc_auc_score = metrics.roc_auc_score(test_y,preds)\n    \n#     return roc_auc_score\n\n\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=50)\n \n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', study.best_trial.params)","ca5bb3ae":"# # Number of finished trials: 50\n#  'learning_rate': 0.1508067112491576, 'lambda_l1': 2.4697757781805905, 'lambda_l2': 9.083741442132819e-08, 'num_leaves': 23, 'feature_fraction': 0.6677259987024685, 'bagging_fraction': 0.6000000000000001, 'subsample': 0.8, 'min_child_samples': 35}","6aeba9c2":"##create folds\nfrom sklearn import model_selection\ndf[\"kfold\"] = -1\n\ndf = df.sample(frac=1).reset_index(drop=True)\n\nkf = model_selection.StratifiedKFold(n_splits=5, shuffle=False, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X=df, y=df.target.values)):\n    print(len(train_idx), len(val_idx))\n    df.loc[val_idx, 'kfold'] = fold","03853385":"params ={'device':['gpu'],'learning_rate': 0.1508067112491576, 'lambda_l1': 2.4697757781805905, 'lambda_l2': 9.083741442132819e-08, 'num_leaves': 23, 'feature_fraction': 0.6677259987024685, 'bagging_fraction': 0.6000000000000001, 'subsample': 0.8, 'min_child_samples': 35}","9223ea29":"import joblib\nimport lightgbm as lgb\nimport xgboost as xg\nimport pandas as pd\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn import tree\ndef run(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    df_train = df_train.drop(columns = 'kfold')\n    df_valid = df_valid.drop(columns = 'kfold')\n    x_train = df_train.drop('target', axis=1).values\n    y_train = df_train.target.values\n    x_valid = df_valid.drop('target', axis=1).values\n    y_valid = df_valid.target.values\n    clf = lgb.LGBMClassifierclf = lgb.LGBMClassifier(device = 'gpu',\n                            learning_rate=0.1508067112491576,\n                            n_estimators= 900,\n                            lambda_l1=  2.4697757781805905, lambda_l2 =  9.083741442132819e-08,subsample= 0.4,\n                            num_leaves= 23, feature_fraction= 0.6677259987024685,\n                            bagging_fraction=  0.6000000000000001, min_child_samples= 35)\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict_proba(x_valid)[:,1]\n    roc_auc_score = metrics.roc_auc_score(y_valid,y_pred)\n    print(f\"Fold={fold}, roc_auc_score={roc_auc_score}\")\n    File_name = 'model_lgb' + str(fold)\n    joblib.dump(\n    clf,File_name)\nfor i in range(5):\n    run(fold = i)","2c0bae23":"model_0_lgb= joblib.load('.\/model_lgb0')\nmodel_1_lgb =joblib.load('.\/model_lgb1')\nmodel_2_lgb= joblib.load('.\/model_lgb2')\nmodel_3_lgb= joblib.load('.\/model_lgb3')\nmodel_4_lgb= joblib.load('.\/model_lgb4')","b8acd567":"df_test = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')\ndf_test = df_test.drop(columns = 'id')","596ddda5":"y_final_3_lgb = model_3_lgb.predict_proba(df_test)[:,1]\ny_final_0_lgb = model_0_lgb.predict_proba(df_test)[:,1]\ny_final_1_lgb = model_1_lgb.predict_proba(df_test)[:,1]\ny_final_2_lgb = model_2_lgb.predict_proba(df_test)[:,1]\ny_final_4_lgb = model_4_lgb.predict_proba(df_test)[:,1]","4fd0b017":"y_final_avg = (y_final_0_lgb + y_final_1_lgb +y_final_2_lgb + y_final_3_lgb + y_final_4_lgb)\/5","12c231a1":"submission['target'] = y_final_avg ","ebe777ca":"submission.to_csv('pred_csv_lgb_avg.csv',index = False)","ee2c384c":"## Ensemble","e40b180d":"## scores = 0.85566\n","794e2bef":"### Train.py"}}