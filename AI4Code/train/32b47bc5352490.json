{"cell_type":{"e607aa81":"code","74019bc1":"code","f9a9f5d0":"code","5bdf6e6d":"code","19814552":"code","833ce47b":"code","6e241353":"code","80d1409b":"code","a328acd4":"code","0c7b4abc":"code","92f85772":"code","5dbfe413":"code","76f812c0":"code","fba2f0b1":"markdown","1c99e1ea":"markdown","86fbad74":"markdown","7690eaf8":"markdown","ef163e46":"markdown","2f5291f1":"markdown","97353a64":"markdown","8c75aa8f":"markdown","26d42638":"markdown","d4cb5808":"markdown","b6ded7c9":"markdown","9e3b1428":"markdown","7ec2bbfb":"markdown","da001135":"markdown","8db6118e":"markdown","43b63cac":"markdown"},"source":{"e607aa81":"# Make necessary imports\n\n# for array operations\nimport numpy as np \n# TensorFlow framework\nimport tensorflow as tf\n# PyTorch framework\nimport torch\n# for pretty printing\nfrom pprint import pprint","74019bc1":"# pipeline module from the HF's transformers library\nfrom transformers import pipeline\n\n# download and cache a suitable pre-trained model\nclassifier = pipeline('sentiment-analysis')","f9a9f5d0":"texts = ['One of the best products I have ever used',\n            'They got thrilled with the movie', \n            'My car starts to underperform', \n            'I love to take icecreams along with milkshakes',\n            'Nobody enters my restaurant after that horror incident']\nclassifier(texts)","5bdf6e6d":"checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'","19814552":"from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n\n# build a tokenizer\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# convert raw text into number inputs\ninputs = tokenizer(texts, truncation=True, padding=True, return_tensors='tf')\npprint(inputs) ","833ce47b":"# build model \nmodel = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n# calculate outputs\noutputs = model(inputs)\noutputs","6e241353":"scores = tf.math.softmax(outputs.logits).numpy()\nscores","80d1409b":"model.config.id2label","a328acd4":"# obtain position of max score\nids = np.argmax(scores, axis=-1)\n# obtain the max score itself\nvalues = np.max(scores, axis=-1)\n# calculate the labels for each input\nlabels = list(map(lambda i: model.config.id2label[i], ids))\n# print results as similar to a pipeline\nfor i in range(len(texts)):\n    print(dict([('label',labels[i]), ('score',values[i])]))","0c7b4abc":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# build a tokenizer\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# convert raw text into tensor inputs\ninputs = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')\npprint(inputs) ","92f85772":"# build model\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n# make predictions\noutputs = model(**inputs)\noutputs","5dbfe413":"# convert logits into prob scores using softmax\nscores = torch.nn.functional.softmax(outputs.logits, dim=-1).detach().numpy()\nscores","76f812c0":"# calculate the max score position\nids = np.argmax(scores, axis=-1)\n# calculate the max score itself\nvalues = np.max(scores, axis=-1)\n# identify the setiment labels \nlabels = list(map(lambda i: model.config.id2label[i], ids))\n# print the results as similar to pipeline\nfor i in range(len(texts)):\n    print({'label':labels[i], 'score':values[i]})","fba2f0b1":"### In TensorFlow","1c99e1ea":"![Image](https:\/\/raw.githubusercontent.com\/RajkumarGalaxy\/dataset\/master\/Images\/pipeline_1.jpg)\n\n> Image by [Bekky Bekks](https:\/\/unsplash.com\/@bekkybekks)\n### What is inside a transformer pipeline?","86fbad74":"**Pipeline** module in the **transformers** library offers a quick lauch of NLP tasks without any preprocessing or model building activities. \n\nLooking at the processes inside a pipeline may help us explore the libraries and customize as per our problem needs. \n\nLet's discuss the Sentiment Analysis task for the sake of simplicity.","7690eaf8":"# Effortless NLP using HuggingFace's Tranformers Ecosystem","ef163e46":"Using multiple text inputs at a time - ","2f5291f1":"# Sentiment Analysis without Pipeline\n\nWhen we call Sentiment Analysis Pipeline, it fetches a particular checkpoint of pre-trained distilBERT. Tokenizer and Model are formulated based on that checkpoint. Let's bring that checkpoint to light!","97353a64":"Key reference: [HuggingFace's NLP Course](https:\/\/huggingface.co\/course)","8c75aa8f":"### Thank you for your valuable time!","26d42638":"That's awesome! We could reproduce the exact results.","d4cb5808":"Yeah! We got the same output as obtained with Pipeline.","b6ded7c9":"**Things to consider:**\n\n> While calling the 'pipeline', it downloads something in four steps. What are they?\n\n> We have just entered texts. It outputs sentiments as humans expect. How?\n\nPipeline is a high-level wrapper that downloads a compatible tokenizer, a pre-trained model, a model head and other post-processing elements to deliver the expected output just with the inputs, but no other efforts. Further, pipeline does not expect its users to have knowledge in any frameworks such as PyTorch and TensorFlow.\n\nIn this article, however, we are going to reproduce the results of pipeline with separate modules and APIs with our effort. We will explore the approaches for both PyTorch and TensorFlow.","9e3b1428":"We get probability scores. How to interpret them as Negative or Positive label? How was the model configured in this regard?","7ec2bbfb":"The outputs are logits. They need to be mapped to a probability distribution using softmax.","da001135":"#### ------------------------------------------------ \n#### *Articles So Far In This Series*\n#### -> [[NLP Tutorial] Finish Tasks in Two Lines of Code](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-finish-tasks-in-two-lines-of-code)\n#### -> [[NLP Tutorial] Unwrapping Transformers Pipeline](https:\/\/www.kaggle.com\/rajkumarl\/nlp-unwrapping-transformers-pipeline)\n#### -> [[NLP Tutorial] Exploring Tokenizers](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-exploring-tokenizers)\n#### -> [[NLP Tutorial] Fine-Tuning in TensorFlow](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-fine-tuning-in-tensorflow) \n#### -> [[NLP Tutorail] Fine-Tuning in Pytorch](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-fine-tuning-in-pytorch) \n#### -> [[NLP Tutorail] Fine-Tuning with Trainer API](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-fine-tuning-with-trainer-api) \n#### ------------------------------------------------ ","8db6118e":"# Sentiment Analysis Using Pipeline\n\nSentiment analysis is a binary text classification task in which the text is generally classified into one of the labels - positive and negative. The pipeline module downloads a suitable pre-trained transformer model, preprocesses the input text and outputs the sentiment label and its probability score value.","43b63cac":"### In PyTorch"}}