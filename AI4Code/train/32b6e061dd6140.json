{"cell_type":{"3fd779aa":"code","c4ae0c29":"code","9d940f74":"code","8e536072":"code","433ed5fc":"code","e5601820":"code","c8d80fca":"code","a238e5aa":"code","5f6e4cd4":"code","c79c5fc3":"code","bb435ff5":"code","c1fe7e0a":"code","697bd4e2":"code","4ac4e69b":"code","412909be":"code","27ee25f8":"code","823aadb9":"code","7b55e7e0":"markdown","34d96e28":"markdown","cc401213":"markdown","fa4a2aa9":"markdown","3977bd96":"markdown","081f6121":"markdown","53583b87":"markdown","2ae29e7d":"markdown","406e8006":"markdown","868bd0d5":"markdown","14234ff8":"markdown","86ba0b1b":"markdown","9a8002f8":"markdown","640cb38d":"markdown","07c57677":"markdown","2e9e6df3":"markdown","31e3723b":"markdown","b63aebbc":"markdown"},"source":{"3fd779aa":"## Kaggle's kernel have outdated version of Seaborn library(0.10, as of when uploading this notebook), we need 0.11 or above for smooth implementation of the notebook.\n## Make sure you run this cell if you do not have seaborn version 0.11 or above.\n!pip install seaborn --upgrade","c4ae0c29":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.__version__","9d940f74":"pcos = pd.read_csv('..\/input\/pcos-dataset\/PCOS_data.csv')","8e536072":"pcos","433ed5fc":"for i in ['AMH(ng\/mL)', 'II    beta-HCG(mIU\/mL)']:\n    pcos[i] = pd.to_numeric(pcos[i], errors='coerce')\npcos = pcos.drop(['Sl. No', 'Patient File No.', 'Unnamed: 44'], axis =1)","e5601820":"target = pcos.columns[:1].to_list()\nfeatures = pcos.columns[1:].to_list()\nprint(\"Total number of Features:\", len(features))","c8d80fca":"pcos.isnull().sum()","a238e5aa":"pcos = pcos.dropna()","5f6e4cd4":"continous=[\n'PRL(ng\/mL)', 'FSH\/LH', \n'II    beta-HCG(mIU\/mL)', '  I   beta-HCG(mIU\/mL)',\n'BP _Diastolic (mmHg)', 'BP _Systolic (mmHg)',\n'Avg. F size (L) (mm)', 'Avg. F size (R) (mm)',\n'TSH (mIU\/L)', 'RBS(mg\/dl)',\n'Vit D3 (ng\/mL)','Cycle length(days)'\n]\n\nf, axes = plt.subplots(6, 2, figsize=(16,25))\nk = 0\nfor i in range(0,6):\n    for j in range(0,2):\n        sns.kdeplot(data=pcos, x=continous[k], hue=\"PCOS (Y\/N)\", ax = axes[i][j])\n        k = k + 1","c79c5fc3":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nnum = 30\n\nbestfeatures = SelectKBest(score_func=chi2, k=num)\nfit = bestfeatures.fit(pcos[features], pcos[target])\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(pcos.columns)\n\nfeatureScores = pd.concat([dfcolumns, dfscores], axis=1)\nfeatureScores.columns = ['Feature','Score']\nfeatureScores = featureScores.sort_values(by='Score', ascending = False)\nfeatureScores = featureScores[featureScores.Feature != target[0]]\nfeatureScores = featureScores.reset_index(drop = True)\nfeatureScores[:num]","bb435ff5":"new_features = featureScores['Feature'].to_list()\nnew_features = new_features[:num]","c1fe7e0a":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nnumerical_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n\npreprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, new_features)])","697bd4e2":"from sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\n\ntrain, test = train_test_split(pcos, test_size = 0.2, random_state = 0)\n\nobservations = pd.DataFrame()\n\nclassifiers = [\n    'Linear SVM', \n    'Radial SVM',\n    'LogisticRegression', \n    'RandomForestClassifier', \n    'KNeighborsClassifier', \n    'Gaussian Naive Bayes'\n]\n\nmodels = [\n    svm.SVC(kernel='linear'), \n    svm.SVC(kernel='rbf'), \n    LogisticRegression(), \n    RandomForestClassifier(n_estimators=200, random_state=0),\n    KNeighborsClassifier(),\n    GaussianNB()\n]\n\nj = 0\nfor i in models:\n    model = i\n    cv = KFold(n_splits=5, random_state=0, shuffle=True)\n    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)])\n    observations[classifiers[j]] = (cross_val_score(pipe, train[new_features], np.ravel(train[target]), scoring='accuracy', cv=cv))\n    j = j+1","4ac4e69b":"mean = pd.DataFrame(observations.mean(), index= classifiers)\nobservations = pd.concat([observations,mean.T])\nobservations.index=['Fold 1','Fold 2','Fold 3','Fold 4','Fold 5','Mean']\nobservations.T.sort_values(by=['Mean'], ascending = False)","412909be":"from sklearn.metrics import confusion_matrix\n\nran_model = RandomForestClassifier(n_estimators=200, random_state=0)\nran_pipe = Pipeline(steps=[('preprocessor', preprocessor), ('model', ran_model)])\nran_pipe.fit(train[new_features], np.ravel(train[target]))\npred = ran_pipe.predict(test[new_features])","27ee25f8":"plt.figure(dpi = 100)\nplt.title(\"Confusion Matrix\")\ncf_matrix = confusion_matrix(np.ravel(test[target]), pred)\ncf_hm = sns.heatmap(cf_matrix, annot=True, cmap = 'rocket_r')","823aadb9":"import sklearn.metrics as metrics\n\nfpr, tpr, threshold = metrics.roc_curve(test[target], pred)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.figure(dpi = 100)\nplt.title('ROC curve for Random Forest Classifier')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate(sensitivity)')\nplt.xlabel('False Positive Rate(specificity)')\nplt.show()","7b55e7e0":"We can see that patients who had PCOS have similar trends as the patients without PCOS. These distributions are not really useful from the point of view of finding features that can help us differentiate between a patient who is diagnosed with PCOS and a patient who isn't.  \n\nSo inorder to find important features we will take help from Statistics. ","34d96e28":"## Explore","cc401213":"As the amount of **missing values is very low** (negligible), we can directly delete them.","fa4a2aa9":"Some columns were wrongly interpreted as datatype \"object\" rather than a real number. Therefore, we will convert them into the apppropiate numeric datatype.  \nColumns like `Sl. No, Patient File No., Unnamed: 44` are of no use to us, so we will remove them.","3977bd96":"<a href=\"https:\/\/imgur.com\/T5FEMnP\"><img src=\"https:\/\/i.imgur.com\/T5FEMnP.png\" title=\"source: imgur.com\" \/><\/a>","081f6121":"## Visualization\n\nThe dataset contains columns which has continous as well as discrete observations. So let's see if we can derive any useful insights from the columsn which have continous values.","53583b87":"## Overview\n### Description :  \n**Polycystic Ovary Syndrome (PCOS)** is a medical condition which causes hormonal disorder in women in their childbearing years. The hormonal imbalance leads to a delayed or even absent menstrual cycle. Women with PCOS majorly suffer from excessive weight gain, facial hair growth, acne, hair loss, skin darkening and irregular periods leading to infertility in rare cases. The existing methodologies and treatments are insufficient for early-stage detection and prediction. To deal with this problem, we propose a system which can help in early detection and prediction of PCOS treatment from an optimal and minimal set of parameters. To detect whether a woman is suffering from PCOS, 5 different machine learning classifiers like Random Forest, SVM, Logistic Regression, Gaussian Na\u00efve Bayes, K Neighbours have been used. Out of the 41 features from the dataset, top 30 features were identified using CHI SQUARE method and used in the feature vector. We also compared the results of each classifier and it has been observed that the accuracy of the Random Forest Classifier is the highest and the most reliable. The dataset used is available on KAGGLE and owned by *Prasoon Kottarathil*.\n\n### **Dataset link:** https:\/\/www.kaggle.com\/prasoonkottarathil\/polycystic-ovary-syndrome-pcos\n### **Target:**  \n`PCOS (Y\/N)`: Whether the person has diagnosed with PCOS.    \n\n### **Features:**  \n**1. Physical Parameters:**  \n`\nAge (yrs)  \nWeight (Kg)  \nHeight(Cm)  \nBlood Group  \nPulse rate(bpm)  \nRR (breaths\/min)  \nCycle(R\/I)  \nCycle length(days)  \nMarraige Status (Yrs)\nHip(inch)  \nWaist(inch)  \nWaist:Hip Ratio  \nNo. of abortions `  \n\n**2. Physical Symptoms:**  \n`Pregnant(Y\/N)  \nWeight gain(Y\/N)  \nhair growth(Y\/N)  \nSkin darkening (Y\/N)  \nHair loss(Y\/N)  \nPimples(Y\/N)  \nFast food (Y\/N)  \nReg.Exercise(Y\/N)`.\n\n**3. Medical Parameters:**  \n`BMI`: Body mass index (BMI) is a measure of body fat based on height and weight that applies to adult men and women.  \n`Hb(g\/dl)`: Hemoglobin(a protein in your red blood cells).  \n`I beta-HCG(mIU\/mL), II beta-HCG(mIU\/mL)`: Human chorionic gonadotropin is a hormone for the maternal recognition of pregnancy.  \n`FSH(mIU\/mL)`: FSH helps manage the menstrual cycle and stimulates the ovaries to produce eggs.  \n`LH(mIU\/mL)`: LH helps control the menstrual cycle. It also triggers the release of an egg from the ovary.  \n`FSH\/LH`: are gonadotropins because they stimulate the gonads - in males, the testes, and in females, the ovaries.  \n`TSH(mIU\/L)`: TSH stands for thyroid stimulating hormone. A TSH test is a blood test that measures this hormone.  \n`AMH(ng\/mL)`: Within the ovaries, AMH helps in the early development of follicles.  \n`PRL(ng\/mL)`:  PRL or lactogenic hormone. Prolactin is mainly used to help women produce milk after childbirth.  \n`Vit D3(ng\/mL)`: Vitamin D.  \n`PRG(ng\/mL)`: Progesterone is an endogenous steroid and progestogen sex hormone involved in the menstrual cycle, pregnancy.  \n`RBS(mg\/dl)`: Random blood sugar (RBS) measures blood glucose regardless of when you last ate.  \n`BP_Systolic (mmHg)`: Systolic pressure, the force of the blood against the artery walls as your heart beats.  \n`BP_Diastolic (mmHg)`: Diastolic pressure, the blood pressure between heartbeats.  \n`Follicle No. (L), Follicle No. (R)`: Ovarian follicles are small sacs filled with fluid that are found inside a woman's ovaries.  \n`Endometrium (mm)`: The endometrium is the innermost lining layer of the uterus.  ","2ae29e7d":"## Feature Importance\nAs we have see above, we have 41 features. We could use all of them but it could happen that all of them are not useful or there can be a chance of overfitting. We also saw that visualisation did not help us in finding important features. Hence we will use the Chi Square method to determine important features.   \nChi square method will calculate a score. The score calculated tells us how important that feature is.  \nWe will use let's say **top 30** most important features.  \nWe will use **[SelectKBest](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.SelectKBest.html)** and **[chi-squared](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.chi2.html)** to find the feature importance.  \n\n#### What is a Chi-Square statistic?\nA chi-square (\u03c72) statistic is a test that measures how expectations compare to actual observed data (or model results). The data used in calculating a chi-square statistic must be random, raw, mutually exclusive, drawn from independent variables, and drawn from a large enough sample.\n\n#### What does a Chi-Square statistic tell you?\nThere are two main kinds of chi-square tests: the test of independence, which asks a question of relationship, such as, **\"Is there a relationship between gender and SAT scores?\"**; and the goodness-of-fit test, which asks something like **\"If a coin is tossed 100 times, will it come up heads 50 times and tails 50 times?\"**","406e8006":"## Evaluation\n#### Here we have, scores of the 5 folds along with their mean.","868bd0d5":"Let's check if there's any missing values, if yes, remove them.","14234ff8":"## Loading the Data","86ba0b1b":"## Transforming the data\nWe will use combination of [ColumnTransformer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.compose.ColumnTransformer.html) with [Pipeline](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html) to carry out the necessary transformation on our data.","9a8002f8":"### Table of Contents\n1. [Importing Libraries](#Importing-Libraries)\n2. [Overview](#Overview)\n3. [Loading the Data](#Loading-the-Data)\n4. [Explore](#Explore)\n5. [Visualization](#Visualization)\n6. [Feature Importance](#Feature-Importance)\n7. [Transforming the data](#Transforming-the-data)\n8. [Training Models](#Training-Models)\n9. [Result](#Result)","640cb38d":"[Go to Table of Contents](#Table-of-Contents)","07c57677":"#### Let's see how our Random Forest model performed using Confusion Matrix and ROC Curve.","2e9e6df3":"## Result","31e3723b":"## Importing Libraries","b63aebbc":"## Training Models  \n### Classifiers used: \n1. [Random Forest Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html)\n2. [Support Vector Machine(SVM)](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html)\n     1. SVM with Linear kernel.\n     2. SVM with Radial kernel.\n3. [Logistic Regression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)\n4.  [KNeighborsClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html)\n5. [Gaussian Naive Bayes](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.GaussianNB.html)\n\n### Evaluation Metrics: [K-Folds cross-validator](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.KFold.html)"}}