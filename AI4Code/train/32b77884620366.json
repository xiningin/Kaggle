{"cell_type":{"efe4c4c6":"code","e8d70f58":"code","dc5bb5a4":"code","2832c17c":"code","3be29dee":"code","db02ae45":"code","440bbc2a":"code","03a4c668":"code","74beb197":"code","aabefdea":"code","5d9d76ca":"code","2aa2c278":"code","87b3cadc":"code","50d583fc":"code","2b5cfe71":"code","9971c13f":"code","349d032b":"code","f61ea652":"code","8ef99481":"code","9eefad68":"code","aabe2b1e":"code","e6e4e054":"code","6add44a0":"code","6e2d222e":"code","e2165ef1":"code","5e1facc8":"code","29a924ab":"code","965fa9e0":"code","688860d5":"code","e86bf103":"code","94f941dd":"code","14c2493b":"code","9c127496":"code","e8d40334":"code","51a141be":"code","ea14eb9e":"code","83355939":"code","dca45ead":"code","55cd498c":"code","17b6ee6d":"code","8fb1e8ec":"code","19cb4d9f":"code","76d384ec":"code","16ffc348":"code","7424e99b":"code","87a27632":"code","cb9f9618":"code","7346bab8":"code","7e1113a3":"code","e111a3d4":"code","c2ce253d":"code","d7bbae3c":"code","1c04a7cf":"code","17628be7":"code","e9fb5ac0":"code","81b9f333":"code","8ef4f4e5":"code","0ed4e8f0":"code","1bc38f6e":"code","ef9b2dce":"code","d18756a1":"code","61351edc":"code","4e927b1f":"code","f4b31f7f":"code","20fd48f6":"code","3fd6f2af":"code","de793e5f":"code","9f498fca":"code","b4b0cf62":"code","91164343":"code","0f365dbb":"code","820bb025":"code","2fa84fa2":"code","3a3296a0":"code","9d12421f":"code","96b0f987":"code","391b2b44":"code","49a0e5f9":"code","31ca91f5":"code","6b72a0e5":"markdown","f5388500":"markdown","a8a002af":"markdown","4cd6c416":"markdown","0a9e09d2":"markdown","7b56f8b1":"markdown","4cd1cd87":"markdown","771ff35c":"markdown","385196ae":"markdown","3ba81349":"markdown","789187cc":"markdown","d291553c":"markdown","056ad9e5":"markdown","41e75128":"markdown","d677c98e":"markdown","6b235c8a":"markdown","63fc97ed":"markdown","f8f1cea9":"markdown","e370a56b":"markdown","ac4d5b41":"markdown","0d7dfdc6":"markdown","013f2c5e":"markdown","4e050546":"markdown","ab5984a6":"markdown","1d998c4b":"markdown","a8f2500c":"markdown","a1c4445f":"markdown","dc4a8cfe":"markdown","1b3ac9b1":"markdown","52e6a2a4":"markdown","d161a7ee":"markdown","aff72238":"markdown","0ac5b0db":"markdown","593118ea":"markdown","8a0767e1":"markdown","148f2bf1":"markdown","ffbfe3aa":"markdown","99096e99":"markdown","aeee7946":"markdown","5d8a3cdd":"markdown","c085f115":"markdown","a3ee1476":"markdown","2ca5ee51":"markdown","ed66fe72":"markdown","4d4baaa9":"markdown","6681dd3f":"markdown","9e753528":"markdown","ef4ce3f7":"markdown"},"source":{"efe4c4c6":"import os\nimport numpy as np\nimport pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nmpl.rc('axes', labelsize=20)\nmpl.rc('xtick', labelsize=18)\nmpl.rc('ytick', labelsize=18)\nimport seaborn as sns","e8d70f58":"#!pip install datawig\n#!pip install fancyimpute\n#!pip install delayed\n#!pip install xlrd\n#!pip install openpyxl\n#!pip install lightgbm","dc5bb5a4":"# Data Cleaning Libraries...\n\nfrom sklearn.impute import KNNImputer\n#from impyute.imputation.cs import mice\nimport datawig\n\n# importing the MICE from fancyimpute library\nfrom fancyimpute import IterativeImputer as MICE\n# Calling Estimators for IterativeImputer class from Scikit library\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import RandomForestRegressor","2832c17c":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(\"Data path: \", os.path.join(dirname, filename))\n        # Import dataset\n        data = pd.read_excel(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3be29dee":"data.head(10)","db02ae45":"data = data.iloc[8:, :]","440bbc2a":"data.head(10)","03a4c668":"# Derive column names from first two rows.\n# construct list of column names.\nlst_colnames = []; dict_ctr = {}\nfor val1, val2 in zip(data.iloc[0, :].to_dict().values(), data.iloc[1, :].to_dict().values()):\n    # check if 2nd row entry is NaN, if not then check for corresponding 1st row entry. \n    #If not Nan add both else add 2nd row entry to last entry made from 1st row.\n    if pd.notna(val2):\n        if pd.notna(val1):\n            # keeping counter of every col-names\n            dict_ctr[val1.strip() + \"-\" + val2.strip()] = 0\n            lst_colnames.append(val1.strip() + \"-\" + val2.strip())\n        else:\n            #check if names we're adding are same.\n            to_be_added = (lst_colnames[-1].split(\"-\")[0]).strip() + \"-\" + val2.strip()\n            if to_be_added not in lst_colnames:\n                # keeping counter of every col-names\n                dict_ctr[to_be_added] = 0\n                lst_colnames.append(to_be_added)\n            else:\n                dict_ctr[to_be_added] += 1\n                lst_colnames.append(to_be_added + str(dict_ctr[to_be_added]))\n    else:\n        lst_colnames.append(val1.strip())","74beb197":"# Redefine column names of data frame\ndata = data.iloc[2:,:]\ndata.columns = lst_colnames","aabefdea":"lst_colnames","5d9d76ca":"data.head(7)","2aa2c278":"# See description of data\ndata.describe(include = 'all')","87b3cadc":"# Check number of NaN values in the data frame\nprint(data.isnull().sum().sort_values(ascending = False)) # or print(data.isna().sum())","50d583fc":"# Shape of the data frame\nprint(\"Shape of Data: \", data.shape)","2b5cfe71":"# Percentage of Null values in the data frame.\nprint(\"Percentage of NaN in the data frame: %.3f\"%(data.isnull().sum().sum()*100 \/ np.product(data.shape)))","9971c13f":"# Check whether these two Mw columns are same (after removing NaN from Mw and the corresponding rows in Mw1).\n(data[data[\"MAGNITUDE-Mw\"].notnull()])[\"MAGNITUDE-Mw\"] == (data[data[\"MAGNITUDE-Mw\"].notnull()])[\"MAGNITUDE-Mw1\"]","349d032b":"# Efficient way to check.\n(data[data[\"MAGNITUDE-Mw\"].notnull()])[\"MAGNITUDE-Mw\"].equals((data[data[\"MAGNITUDE-Mw\"].notnull()])[\"MAGNITUDE-Mw1\"])","f61ea652":"#Drop columns...\n# Remove two columns name is 'C' and 'D'\n#df.drop(['C', 'D'], axis = 1)\n# Remove three columns as index base\n#df.drop(df.columns[[0, 4, 2]], axis = 1, inplace = True)\n# df.drop(columns =['C', 'D'])\n# Remove all columns between column index 1 to 3\n#df.drop(df.iloc[:, 1:3], inplace = True, axis = 1)\n# Remove all columns between column name 'B' to 'D'\n#df.drop(df.ix[:, 'B':'D'].columns, axis = 1)\n# Remove all columns between column name 'B' to 'D'\n#df.drop(df.loc[:, 'B':'D'].columns, axis = 1)\nlabel = data['MAGNITUDE-Mw1']\ndata.drop(columns = ['INTENSITY-MME', \n                     'INTENSITY-MMI', \n                     'INTENSITY-MM', \n                     'ORIGIN TIME-(IST)', \n                     'LOCATION', \n                     'MAGNITUDE-Mb', \n                     'ORIGIN TIME-(UTC)', \n                     'MAGNITUDE-Mw', \n                     'MAGNITUDE-Mw1', \n                     'MAGNITUDE-Mb1', \n                     'REFERENCE', \n                     'MAGNITUDE-Ms', \n                     'MAGNITUDE-ML', \n                     'Sl. No.'], inplace = True)\n# inplace Job is to transform at address location and return None, rather returning a transformed copy of the same.","8ef99481":"data","9eefad68":"# Replace zero in both Month and Date column with nan values.\n#Method 1: data = (data.replace({'MONTH':{0: np.nan, \" \":np.nan, \" \":np.nan}, \n#                      'DATE':{0: np.nan, \" \":np.nan, \" \":np.nan}})).iloc[2:, :]\n# Method 2: data[['MONTH', 'DATE']] = data[['MONTH', 'DATE']].replace({0: np.nan})\n# Method 3: data.replace(to_replace = [0, \"\", \" \"], value = np.nan, inplace=True)\n\n# Convert 0, one-unit length string or empty string to NaN.\ndata = (data.replace({'MONTH':{0: np.nan, \" \":np.nan, \" \":np.nan}, \n                      'DATE':{0: np.nan, \" \":np.nan, \" \":np.nan}})).iloc[2:, :]# Remove first two rows...","aabe2b1e":"data.head(8)","e6e4e054":"#reset the index of the dataframe...\ndata.reset_index(drop = True, inplace = True)\ndata.head(10)","6add44a0":"# Plotting Year column to see if it's in Non-decreasing order.\nplt.figure(figsize = (10,8))\nplt.plot(np.arange(0, len(data)), data.YEAR, color = 'b', linestyle = '-', linewidth = 2, label = 'Year')\nplt.ylabel(\"Year\"); plt.xlabel(\"Index\"); plt.title(\"Year values as we go down in DataFrame\")\nplt.grid(); plt.legend(); plt.show()\nprint(\"\\n\\nZooming into the range from 1750 and onwards...\")\nplt.figure(figsize = (10,8))\nplt.plot(np.arange(0, len(data)), data.YEAR, color = 'b', linestyle = '-', linewidth = 2, label = 'Year')\nplt.ylabel(\"Year\"); plt.xlabel(\"Index\"); plt.ylim(1750, 2050); plt.title(\"Year values from 1750 onwards\")\nplt.grid(); plt.legend(); plt.show()","6e2d222e":"# Impute the NaN values in Month and Date column...\ndef month_date_imputer(df_mm_dd_yy):\n    \"\"\"\n    Takes in month, date and year column and impute the NaN values \n    in month as well as date col according to the strategy given above.\n    Output:\n        Imputed Month & Date Columns\n    \"\"\"\n    # iterate through each row and select 'Name' and 'Age' column respectively.\n    #for index, row in df.iterrows():\n    #    print (row[\"Name\"], row[\"Age\"])\n    # iterate through each row and select 'Name' and 'Percentage' column respectively.\n    #for row in df.itertuples(index = True, name ='Pandas'):\n    #    print (getattr(row, \"Name\"), getattr(row, \"Percentage\"))\n    # 0th and 2nd index column respectively.\n    #for i in range(len(df)) :\n    #  print(df.iloc[i, 0], df.iloc[i, 2])\n    if pd.isna(df_mm_dd_yy.iloc[0,:].MONTH):\n        df_mm_dd_yy.iloc[0,:].MONTH = 1\n    if pd.isna(df_mm_dd_yy.iloc[0,:].DATE):\n        df_mm_dd_yy.iloc[0,:].DATE = 1\n    prev_row = df_mm_dd_yy.iloc[0,:]\n    # Iterate over the NaN Month values...\n    for row in (df_mm_dd_yy[df_mm_dd_yy['MONTH'].isnull()]).itertuples():\n        # current row for the nan month and it's previous row \n        cur_row, prev_row = df_mm_dd_yy.iloc[row.Index, :], df_mm_dd_yy.iloc[row.Index - 1, :]\n        # check if year for current idx and prev idx are same.\n        if cur_row.YEAR == prev_row.YEAR:\n            if cur_row.MONTH == 12:\n                df_mm_dd_yy.loc[row.Index, \"MONTH\"] = 12\n            else:\n                df_mm_dd_yy.loc[row.Index, \"MONTH\"] = prev_row.MONTH + 1\n        else:\n            df_mm_dd_yy.loc[row.Index, \"MONTH\"] = 1\n    # Iterate over NaN date Values...\n    for row in (df_mm_dd_yy[df_mm_dd_yy['DATE'].isnull()]).itertuples():\n        # current row for the nan month and it's previous row \n        cur_row, prev_row = df_mm_dd_yy.iloc[row.Index, :], df_mm_dd_yy.iloc[row.Index - 1, :]\n        # check if year for current idx and prev idx are same.\n        if (cur_row.YEAR == prev_row.YEAR) and (cur_row.MONTH == prev_row.MONTH):\n            if cur_row.DATE == 31:\n                if cur_row.MONTH == 12:\n                    df_mm_dd_yy.loc[row.Index] = np.array([cur_row.YEAR + 1, 1, 1])\n                else: \n                    df_mm_dd_yy.loc[row.Index] = np.array([cur_row.YEAR, cur_row.MONTH + 1, 1])\n            else:\n                df_mm_dd_yy.loc[row.Index, \"DATE\"] = prev_row.DATE + 1\n        else:\n            df_mm_dd_yy.loc[row.Index, \"DATE\"] = 1\n    return df_mm_dd_yy","e2165ef1":"pd.options.mode.chained_assignment = None\ndata[[\"YEAR\", \"MONTH\", \"DATE\"]] = month_date_imputer(data[[\"YEAR\", \"MONTH\", \"DATE\"]])","5e1facc8":"# Check for Number of Null Values in all columns...\ndata.isnull().sum()","29a924ab":"data.dtypes","965fa9e0":"# Convert dtypes of YEAR, MONTH, and DATE to int32.\n# Convert dtype of LAT and LONG to float32 after removing the special characters.\n\n#data[[\"YEAR\", \"MONTH\", \"DATE\"]] = data[[\"YEAR\", \"MONTH\", \"DATE\"]].apply(pd.to_numeric) \n# Assign a numeric dtype which may be float or int\ndata = data.astype({\"YEAR\":\"int32\", \"MONTH\":\"int32\", \"DATE\":\"int32\"}) \n# allows to convert the dtype according to user","688860d5":"#Transform LAT and LONG columns.\ndef to_numeric(x):\n    \"\"\"\n    convert the given string to numeric while removing the unwanted characters.\n    \"\"\"\n    try:\n        return np.float32(x)# return after removing the white spaces.\n    except:\n        return np.float32(\"\".join([k for k in x if k not in ['N', 'S', chr(32), 'W', 'E', chr(176), chr(65392)]]))\n        ","e86bf103":"#Tranforming both Lat and Long.\ndata['LAT (N)'] = data['LAT (N)'].apply(to_numeric)\ndata['LONG (E)'] = data['LONG (E)'].apply(to_numeric)","94f941dd":"data.head()","14c2493b":"# Time to transform the Depth col.\n# Since a lot of entries are zero, we need to convert them to Nan and impute it using certain methods.\ndata = data.replace({\"DEPTH (km)\":{0:np.nan, \" \":np.nan, \"\":np.nan}})","9c127496":"data.isna().sum() #or data.isnull().sum()","e8d40334":"# We'll try above 3 imputation methods\n\n# define KNNimputer\nimputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\ndata_transformed_KNN = imputer.fit_transform(data)","51a141be":"data_transformed_KNN = pd.DataFrame(data_transformed_KNN, columns = ['YEAR', 'MONTH', 'DATE', 'LAT (N)', 'LONG (E)', 'DEPTH (km)'])","ea14eb9e":"data_transformed_KNN.head(9)","83355939":"plt.figure(figsize = (12,8))\n# reset the default parameters by calling set_theme():\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})#sns.axes_style(\"darkgrid\")\nsns.set_context(\"paper\")#\"talk\", \"poster\"\nsns.scatterplot(x = np.arange(len(data_transformed_KNN)), y = data_transformed_KNN['DEPTH (km)'], linewidth = 0)\nplt.title(\"Imputation using KNN\"); plt.show()","dca45ead":"# Split the data into two for training and testing.\ndf_train, df_test = data[data[\"DEPTH (km)\"].notnull()], data[data[\"DEPTH (km)\"].isnull()]\n#datawig.utils.random_split(data)","55cd498c":"df_test","17b6ee6d":"#Initialize a SimpleImputer model\n#Fit DataWig imputer and predict\nimputer = datawig.SimpleImputer(\n    input_columns=['YEAR', 'MONTH', 'DATE', 'LAT (N)', 'LONG (E)'], # column(s) containing information about the column we want to impute\n    output_column='DEPTH (km)', # the column we'd like to impute values for\n    output_path = 'imputer_model' # stores model data and metrics\n    )\n\n#Fit an imputer model on the train data\nimputer.fit(train_df=df_train)#, num_epochs=50\n\n#Impute missing values and return original dataframe with predictions\nimputed = imputer.predict(df_test)","8fb1e8ec":"imputed.drop(columns = ['DEPTH (km)'], inplace = True)","19cb4d9f":"imputed.rename(columns = {'DEPTH (km)_imputed': 'DEPTH (km)'}, inplace=True)","76d384ec":"imputed.tail(10)","16ffc348":"imputed.head(10)","7424e99b":"data_imputed_datawig = df_train.combine_first(imputed)","87a27632":"# Plot the depth profile...\nplt.figure(figsize = (12,8))\nsns.scatterplot(x = np.arange(len(data_imputed_datawig)), \n                y = data_imputed_datawig[\"DEPTH (km)\"], linewidth=0)#alpha = 0.7\nplt.title(\"Depth Imptation using Data-Wig\"); plt.show()","cb9f9618":"# Plot the depth profile...\nplt.figure(figsize = (12,8))\nsns.scatterplot(x = np.arange(len(data_imputed_datawig)), \n                y = data_imputed_datawig[\"DEPTH (km)\"], linewidth=0)#alpha = 0.7\nplt.ylim(-50, 1000); plt.title(\"Depth Imptation using Data-Wig with limit on y-axis\"); plt.show()","7346bab8":"# Plot the depth profile...\nplt.figure(figsize = (12,8))\nsns.scatterplot(x = np.arange(len(df_train)), \n                y = df_train[\"DEPTH (km)\"], linewidth=0)#alpha = 0.7\nplt.title(\"Original Training Data - Depth Profile\"); plt.show()","7e1113a3":"# calling the  MICE class\nmice_imputer = MICE()\n# imputing the missing value with mice imputer\ndata_imputed_MICE = mice_imputer.fit_transform(data)\n# printing dataframe\ndata_imputed_MICE = pd.DataFrame(data_imputed_MICE, columns = ['YEAR', 'MONTH', 'DATE', 'LAT (N)', 'LONG (E)', 'DEPTH (km)'])","e111a3d4":"data_imputed_MICE.head(7)","c2ce253d":"# Plot the depth profile...\nplt.figure(figsize = (12,8))\nsns.scatterplot(x = np.arange(len(data_imputed_MICE)), \n                y = data_imputed_MICE['DEPTH (km)'], linewidth=0)#alpha = 0.7\nplt.title(\"MICE Imputation (from fancyimpute library)\"); plt.show()","d7bbae3c":"# Estimate the score after iterative imputation of the missing values\n# with different estimators\nestimators = [\n    BayesianRidge(),\n    ExtraTreesRegressor(n_estimators=50, random_state=0),\n    RandomForestRegressor(n_estimators=50, random_state=0)\n]\ndata_imputed_iterative = {}\nestimator_names = ['bayesian_ridge', 'extra_trees_regressor', 'random_forest_regressor']\nfor name, impute_estimator in zip(estimator_names, estimators):\n    data_imputed_iterative[name] = IterativeImputer(random_state=0, estimator=impute_estimator,\n                                                    imputation_order='random').fit_transform(data.values)\n    data_imputed_iterative[name] = pd.DataFrame(data_imputed_iterative[name], columns = ['YEAR', \n                                                                                         'MONTH', \n                                                                                         'DATE', \n                                                                                         'LAT (N)', \n                                                                                         'LONG (E)', \n                                                                                         'DEPTH (km)'])","1c04a7cf":"# Plot the depth profile...\nfor name in estimator_names:\n    plt.figure(figsize = (12,8))\n    sns.scatterplot(x = np.arange(len(data_imputed_iterative[name])), \n                    y = (data_imputed_iterative[name])[\"DEPTH (km)\"], linewidth=0)#alpha = 0.7\n    plt.title(f\"Iterative Imputation with {name} estimator.\")\nplt.show()","17628be7":"# append the label column.\nfor key in data_imputed_iterative.keys():\n    (data_imputed_iterative[key])['Magnitude'] = label\ndata_transformed_KNN['Magnitude'] = label","e9fb5ac0":"# Remove the DUPLICATE Rows --- LAST Pre-processing.\nfor key in data_imputed_iterative.keys():\n    data_imputed_iterative[key] = (data_imputed_iterative[key])[~data_imputed_iterative[key].duplicated(keep = 'last')]\ndata_transformed_KNN = data_transformed_KNN[~data_transformed_KNN.duplicated(keep = 'last')]","81b9f333":"# Create folder imputed_data in \/kaggle\/output\n!mkdir .\/imputed_data","8ef4f4e5":"# Save all the transformed datasets as excel file.\nfor key in data_imputed_iterative.keys():\n    (data_imputed_iterative[key]).to_excel(\"imputed_data\/Imputed_iterative_\"+key+\".xlsx\", engine = \"openpyxl\")\ndata_transformed_KNN.to_excel(\"imputed_data\/Imputed_KNN.xlsx\")\nprint(\"Successfully Saved!\")","0ed4e8f0":"# Import data-sets.\ndata_dict = {}\nfor file in os.listdir(os.path.join(\"imputed_data\", \"\")):\n    data_dict[file.split(\".\")[0]] = pd.read_excel(\"imputed_data\/\"+file, engine='openpyxl')","1bc38f6e":"#Import all the required models...\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n#from xgboost import XGBClassifier\n#from sklearn.ensemble import VotingClassifier","ef9b2dce":"# Usually 0.5 is taken as the threshold when decisions are made between [0, 1]. Now we'll just scale it to the actual range of Magnitude.\nmagnitude = (data_dict[\"Imputed_iterative_bayesian_ridge\"]['Magnitude'])[data_dict[\"Imputed_iterative_bayesian_ridge\"]['Magnitude'].notnull()]\nmagnitude.hist(); plt.title(\"Magnitude of Earthquake\"); plt.xlabel(\"Magnitude\"); plt.ylabel(\"Frequency\"); plt.show()","d18756a1":"#0.5*(magnitude.max() - magnitude.min()) + magnitude.min()\nT = 0.5*(magnitude.max() - magnitude.min()) + magnitude.min() # Also magnitude.mean() or .median() can be used.\nprint(\"The Threshold for Magnitude that classify it as Earthquake and Non-Earthquake: \", T)","61351edc":"# Let's first sparate out the test data (i.e., for which Magnitude is nan).\ntrain_val_dict, test_dict = {}, {}\n#req_cols = list(data_dict[\"Imputed_iterative_bayesian_ridge\"].columns)[1:-1]\nfor key in data_dict.keys():\n    train_val_dict[key] = ((data_dict[key])[data_dict[key]['Magnitude'].notnull()]).drop(columns = [\"Unnamed: 0\"])\n    test_dict[key] = ((data_dict[key])[data_dict[key]['Magnitude'].isnull()]).drop(columns = [\"Unnamed: 0\"])","4e927b1f":"# get a list of models to evaluate\ndef get_models():\n    \"\"\"\n    Return three different models each of different versions (i.e., with different parameters) \n    \"\"\"\n    models_param = [{\"n_neighbors\": [50, 80, 120, 150, 180]}, \n                    {\"max_depth\": [3, 5, 7, 9, 11]}, \n                    {\"n_estimators\": [10, 25, 50, 100, 150]}]\n    models = [KNeighborsClassifier(),\n              DecisionTreeClassifier(random_state=23),\n              LGBMClassifier(random_state = 23)\n             ]\n    return models, models_param\n\n\n# evaluate a give model using cross-validation\ndef evaluate_model(model, param_grid):\n    \"\"\"\n    Train model with GridSearchCV mode on the given parameters.\n    \"\"\"\n    cv = StratifiedKFold(n_splits=5, random_state=23, shuffle=True)\n    return GridSearchCV(model, param_grid, scoring=\"roc_auc\", n_jobs=-1, cv=cv, verbose=1)","f4b31f7f":"# Run in Loop and Store the results for each Data.\nfinal_results = {}\nfor data_key in train_val_dict.keys():\n    # Call get_model() to get Models and Their Parameters \n    models, models_param = get_models()\n    # Initialize new dictionary for each DataSet.\n    final_results[data_key] = {}\n    for model, param in zip(models, models_param):\n        labels = (train_val_dict[data_key][\"Magnitude\"] >= T).astype(np.int32)\n        if model.__class__.__name__ == \"KNeighborsClassifier\":\n            # Rescaling data for KNN within 0 and 1.\n            data = train_val_dict[data_key].iloc[:, :-1].apply(lambda x: (x - min(x))\/(max(x) - min(x)))\n        else:\n            data = train_val_dict[data_key].iloc[:, :-1]\n        grid_model = evaluate_model(model, param)\n        grid_result = grid_model.fit(data, labels)\n        final_results[data_key][model.__class__.__name__] = grid_result","20fd48f6":"# Helper Functions...\ndef d_to_arr(d, key):\n    return np.array([dict_ele[key] for dict_ele in d])\n\ndef to_dataframe(data_scores):\n    final_df = []\n    for data_name in data_scores.keys():\n        df = pd.DataFrame.from_dict(data_scores[data_name]).assign(DataName=data_name)\n        final_df.append(df)\n    return pd.concat(final_df, axis = 0, ignore_index = True)    ","3fd6f2af":"# Plotting ROC scores for KNN Model with every parameter configuration for each DATASet.\ndata_scores_knn = {data_name:\n               {\"mean_test_score\":final_results[data_name][\"KNeighborsClassifier\"].cv_results_[\"mean_test_score\"], \n                \"params\":d_to_arr(final_results[data_name][\"KNeighborsClassifier\"].cv_results_[\"params\"], \"n_neighbors\"),\n                \"std_test_score\":final_results[data_name][\"KNeighborsClassifier\"].cv_results_[\"std_test_score\"]} \n                for data_name in final_results.keys()}","de793e5f":"# Transformed DataFrame\ndf_knn = to_dataframe(data_scores_knn)\n\n# Plot Grouped BAR plot...\nsns.set_theme(style=\"whitegrid\"); sns.set_context(\"paper\")\n\nplt.figure(figsize = (17, 6))\n# class v \/ s fare barplot\nsns.set(font_scale = 1.1)\nax = sns.barplot(x = 'DataName', y = 'mean_test_score', hue = 'params', \n            data = df_knn, palette = sns.color_palette(\"bright\"))\nax.set_title(\"For KNeighborsClassifier with K = 50, 80, 120, 150, and 180; with 4 different dataset.\")\nax.set_xlabel(\"Dataset obtained through different imputation methods\")\n# Show the plot\nplt.show()","9f498fca":"# best for each data.\nbest_results_knn = pd.DataFrame([[data_name, final_results[data_name][\"KNeighborsClassifier\"].best_score_, \n                     final_results[data_name][\"KNeighborsClassifier\"].best_params_] \n                    for data_name in final_results.keys()], columns = [\"DataSet name\", \"Best Score\", \"Best Parameters\"])\n\nbest_results_knn","b4b0cf62":"# Plotting ROC scores for Decision Tree with every parameter configuration for each DATASet.\"LGBMClassifier\"\ndata_scores_dt = {data_name:\n               {\"mean_test_score\":final_results[data_name][\"DecisionTreeClassifier\"].cv_results_[\"mean_test_score\"], \n                \"params\":d_to_arr(final_results[data_name][\"DecisionTreeClassifier\"].cv_results_[\"params\"], \"max_depth\"),\n                \"std_test_score\":final_results[data_name][\"DecisionTreeClassifier\"].cv_results_[\"std_test_score\"]} \n                for data_name in final_results.keys()}","91164343":"# Transformed DataFrame\ndf_dt = to_dataframe(data_scores_dt)\n\n# Plot Grouped BAR plot...\nsns.set_theme(style=\"whitegrid\"); sns.set_context(\"paper\")\n\nplt.figure(figsize = (17, 6))\n# class v \/ s fare barplot\nsns.set(font_scale = 1.1)\nax = sns.barplot(x = 'DataName', y = 'mean_test_score', hue = 'params', \n            data = df_dt, palette = sns.color_palette(\"bright\"))\nax.set_title(\"For DecisionTreeClassifier with max_depth = 3, 5, 7, 9, 11; with 4 different dataset.\")\nax.set_xlabel(\"Dataset obtained through different imputation methods\")\n# Show the plot\nplt.show()","0f365dbb":"# best for each data.\nbest_results_dt = pd.DataFrame([[data_name, final_results[data_name][\"DecisionTreeClassifier\"].best_score_, \n                     final_results[data_name][\"DecisionTreeClassifier\"].best_params_] \n                    for data_name in final_results.keys()], columns = [\"DataSet name\", \"Best Score\", \"Best Parameters\"])\n\nbest_results_dt","820bb025":"# Plotting ROC scores for Ensemble Model (LightGBM) with every parameter configuration for each DATASet.\ndata_scores_lgbm = {data_name:\n               {\"mean_test_score\":final_results[data_name][\"LGBMClassifier\"].cv_results_[\"mean_test_score\"],\n                \"params\":d_to_arr(final_results[data_name][\"LGBMClassifier\"].cv_results_[\"params\"], \"n_estimators\"),\n                \"std_test_score\":final_results[data_name][\"LGBMClassifier\"].cv_results_[\"std_test_score\"]} \n                for data_name in final_results.keys()}","2fa84fa2":"# Transformed DataFrame\ndf_lgbm = to_dataframe(data_scores_lgbm)\n\n# Plot Grouped BAR plot...\nsns.set_theme(style=\"whitegrid\"); sns.set_context(\"paper\")\n\nplt.figure(figsize = (17, 6))\n# class v \/ s fare barplot\nsns.set(font_scale = 1.1)\nax = sns.barplot(x = 'DataName', y = 'mean_test_score', hue = 'params', \n            data = df_lgbm, palette = sns.color_palette(\"bright\"))\nax.set_title(\"For LightGBM Classifier with n_estimators = 10, 25, 50, 100, 150; with 4 different dataset.\")\nax.set_xlabel(\"Dataset obtained through different imputation methods\")\n# Show the plot\nplt.show()","3a3296a0":"# best for each data.\nbest_results_lgbm = pd.DataFrame([[data_name, final_results[data_name][\"LGBMClassifier\"].best_score_, \n                     final_results[data_name][\"LGBMClassifier\"].best_params_] \n                    for data_name in final_results.keys()], columns = [\"DataSet name\", \"Best Score\", \"Best Parameters\"])\n\nbest_results_lgbm","9d12421f":"# Final Data -- Choose one of the four dataset.\nfinal_data = train_val_dict[\"Imputed_iterative_random_forest_regressor\"].iloc[:, :-1]\nlabels_final = (train_val_dict[\"Imputed_iterative_random_forest_regressor\"][\"Magnitude\"] >= T).astype(np.int32)","96b0f987":"# Perform feature Importance Analysis using RandomForestClassifier on final_data.\n# define the model\nfeature_imp_model = RandomForestClassifier()\n# fit the model\nfeature_imp_model.fit(final_data, labels_final)\n# get importance\nimportance = feature_imp_model.feature_importances_","391b2b44":"# summarize feature importance\nfor i,v in zip(list(final_data.columns), importance):\n    print( ' Feature: %s, Score: %.5f ' % (i,v))\n# plot feature importance\nplt.figure(figsize = (16, 7)); plt.title(\"Feature importance v\/s Their Importance Score.\")\nplt.bar(list(final_data.columns), importance); plt.ylabel(\"Importance Score\"); plt.show()","49a0e5f9":"# Adding Latitude and Longitude ratio as new feature column.\nnew_data = final_data.assign(LAT_LONG_Ratio = lambda x: x[\"LAT (N)\"]\/x[\"LONG (E)\"])","31ca91f5":"# define an instance of LightGBM model\nlgbm = LGBMClassifier(n_estimators = 50, random_state = 23)\n# fit the model with 5-fold Cross-validation\n# define cv splitter\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=23)#, shuffle=True, random_state=23\n# Load the model and Data to instance of splitter and fit.\nscores = cross_val_score(lgbm, new_data, labels_final, cv=cv, n_jobs = -1,  scoring=\"roc_auc\", verbose=1)\n\n# print the scores.\nprint(f\"Scores for 5 folds: {scores}\\nAverage score = {np.mean(scores)}\")","6b72a0e5":"## Task 5\nConsider **test results** of the **best model** from above analysis. Report the input features that was used to achieve this. Try to improvise the test results by applying **feature processing** (You may come up with additional features by processing original ones).\\\nReport the new set of features that was used and also report the improvements in test results that was achieved. Please use appropriate metrics to report the results.","f5388500":"### Decision Trees as Predcitor.","a8a002af":"# Objective","4cd6c416":"__Seeing both the plots we can `drop` this method of imputation as of now.__\n\n**Again we can see the `datawig()` model is not completely trained and `earlystopping` is being applied. So it's better to avoid this imputation method.**","0a9e09d2":"## Dataset Overview","7b56f8b1":"https:\/\/scikit-learn.org\/stable\/modules\/impute.html#iterative-imputer\n\nhttps:\/\/machinelearningmastery.com\/iterative-imputation-for-missing-values-in-machine-learning\/","4cd1cd87":"__Indeed, the imputation results we obtained through MICE of `facyimpute` package is same as `IterativeImputer` with `Bayesian Ridge` as estimator. Plus, in the documentation of scikit (user-guide) it's clearly mentioned that the Iterative Imputation is purely inspired from MICE algorithm.__","771ff35c":"#### DataWig","385196ae":"Use __KNN__, __Decision Trees__ and __Ensemble Learning__ to build classifiers for predicting labels as Magnitude($M_w$). \n\n1. Use an appropriate threshold, $T$, that seem fit between $[4, 5]$ inclusive. For $M_w < T$, label becomes $0$ (no earthquake) and for $M_w \\geq T$ becomes $1$ (earthquake). \n2. Use appropriate features as input from the dataset that you seem fit.\n3. Use `roc_auc()` score as evaluation metric.","3ba81349":"###### Analysis:\n\n__Seeing above results we infer that, MORE or LESS every Imputation worked well and are at par with each other.__\n\n1. __The one that gave the best performance with all the 3 models is `Random Forest Regressor` embedded as estimator in `Iterative Imputer`.__\n2. __Secondly, with `Grid Search` and `Cross-validation` with 5 folds (that split TRAIN and VAL into 80%:20% = 4:1) we obtained our best parameters for each model.__\n3. __Among all the `Ensemble` classifier that's the `LightGBM` performed the best.__","789187cc":"***Answer:***\n1. __According to the 5-fold CV results, the best model is none other than `LightGBM` classifier with ROC-AUC score $= 82.2$.__\n2. Input Features used = `YEAR`, `MONTH`, `DATE`, `LAT (N)`, `LONG (E)`, and `DEPTH (km)`","d291553c":"## Transforming the DATE, MONTH and YEAR column.","056ad9e5":"1. __So it's evident from the above fact to drop the Mw and keep the Mw1.__\n2. __Plus we'll drop all other MAGNITUDE columns as we need to predict Mw1.__\n","41e75128":"***Answer***:\n\nThe best possible values of parameters according to the `GridSearch()` implemented are as follows:\n1. k-Neighbors Classifier: $k = n\\_neighbors = 120$ for all four dataset.\\\nReasoning:\n    * When the value of $K$ or the number of neighbors is too low, the algorithm picks only the data-points that are closest to the given data sample, thus forming a very complex decision boundary. Such a model fails to generalize well on the test data set, thereby showing poor results. \n    * The problem can be mitigated by tweaking `n_neighbors` parameter. As we increase the number of neighbors, the model starts to generalize well, but increasing the value too much would again drop the performance.\n2. Decision Tree: Depth of Tree $= max\\_depth = 7$ for all four dataset.\\\nReasoning:\n    * If we restrict our decision trees to have a lower depth then we couldn't have enough splits to reach the pure leaf node where decisions are taken and hence pre-mature pruning causes model to underfit.\n    * And if we allow our tree to grow deeper, our model will become more complex since we are having enough splits which captures more information this causes overfitting as our model will fit perfectly for the training data and will not be able to generalize well on test set.\n3. LightGBM: Number of Estimators = Number of Decision Trees $= n\\_estimators = 50$ for all four dataset.\\\nReasoning:\n    * Here again we're facing the same scenario. Taking less than $50$ trees doesn't allow the whole model to completely understand the data i.e., the mis-classification problem with each sequential weak learner is not completely eradicated. And taking $\\geq 50$ allowed it to understand the data more closely and hence overfit.","d677c98e":"# Tasks\n\nUse __KNN__, __Decision Trees__ and __Ensemble Learning__ to build classifiers for predicting labels as Magnitude (*Mw*). Use an appropriate threshold that seem fit between $[4, 5]$ inclusive. Consider a threshold of $T$. For *Mw* $< T$, label becomes $0$ (no earthquake) and for *Mw* $\\geq T$ becomes $1$ (earthquake). Use appropriate features as input from the dataset that you seem fit. Please mention what threshold you used and what is the train-test split size that you used in the submission.","6b235c8a":"The following is a (false positive) warning. The potential for false positives is addressed in the docs on indexing, if you'd like to read further. You can safely disable this new warning with the following assignment.\nhttps:\/\/stackoverflow.com\/questions\/20625582\/how-to-deal-with-settingwithcopywarning-in-pandas","63fc97ed":"## Cleaning Latitude and Longitude Column\n\nAll entries in **LAT (N)** and **LONG (E)** columns are `string` type (i.e., `object` type) out of which most of them are just plain string of `float` type values while other cantains some special character like **N**, **S**, **E**, **W**, **-**, **\u25e6** ($= degree$), and **\" \"** (i.e, space character) which are pre-processed to obtain `float32` values.","f8f1cea9":"## Task 1\n\nPlot __ROC__ for both these classifiers for $K$ as parameter in __KNN__, **pre-prune depth** as a parameter in __Decision Tree__ and __number of estimators__ as parameter in __Ensemble learning__.","e370a56b":"**Remove first 8 rows to remove the title of sheet and other additional information that are basically subtitles.**","ac4d5b41":"__Define the label column according to a particular threshold.__","0d7dfdc6":"## Task 2\n\nWhich is the better classifier for this data amongst the three? Give Reasoning.","013f2c5e":"__Since 33% is a considerable amount, removing the irrelevant columns would help for predicting *Mw*.__\n\n1. __Since, no. of data samples is 52989 which is comparable or significantly more in no. to the no. of missing values in the columns upto ORIGIN-TIME-(UTC), we'll drop these columns.__\n2. __Further, LAT and LONG will compensate for LOCATION attribute. And Sl. No. is of no relevance.__","4e050546":"__Answer:__\n\nAs we discussed above the `LightGBM` is the best classifier amongst the three.\\\n**Reasoning**:\n\n1. Since this is a boosting classifier it consists of iteratively (sequentially) learning weak classifiers with respect to a distribution with a motive to rectify the mistakes commited by the previous classifier i.e., basically tries to correctly classify the mis-classified sample from previous learner. \n2. And in this process more weight is being conferred to the mis-classified data while a successive learner (classifier) tries to learn. \n3. Finally, combining them generates strong classifier where all the learners decisions are respected through a voting criteria.","ab5984a6":"### KNN as Predictor.","1d998c4b":"__Redefine the dataset by dropping the Magnitude Column and splitting it into Train, Dev, and Test.__","a8f2500c":"# Import Essential Libraries...","a1c4445f":"## Renaming the columns...\n\nRename the columns (i.e., to merge two rows that consists of column names). Below is the table for column names.","dc4a8cfe":"##### Analysis...\n\n1. __With addition of *Ratio column* for dataset imputed with `Random Forest regressor` method embedded in `Iterative Imputation` wrapper, we got a decrement in `ROC_AUC` score from `0.822354` to `0.820956` which is almost 0.17% decrement.__\n2. __New features: `YEAR`, `MONTH`, `DATE`, `LAT (N)`, `LONG (E)`, `DEPTH (km)`, `LAT_LONG_Ratio`.__\n3. **Feel free to `fork` this note-book and play around any sections you want. Also `comment down` if anyone comes up with `feature-engneering` method that help models perform better.**","1b3ac9b1":"* To decide the threshold $T$ for `Magnnitude` column, we scaled up the $[0, 1]$ interval to the \u201crange\u201d of magnitue column (i.e., $\\max(mag ni t ud e) - \\min(mag ni t ud e)$) and shifted it by a $constant = \\min(mag ni t ud e)$.\n\n* Now as $0.5$ is a safe choice in $[0, 1]$ interval, correspondingly we chose that value in scaled and shifted interval as our threshold (i.e., $= 5.3914$). \n\n* Again our choice for such threshold is motivated by the fact that distribution of `Magnitude` is **mostly gaussian with a little skew towards left (positively skewed)**. So choosing a value in between would do the needful.\n\n$$T = 0.5\\left(\\max(magnitude)-\\min(magnitude)\\right) + \\min(magnitude)= 5.3914$$","52e6a2a4":"1. Since shape of the dataset $= 52989 \\times 20$ and keeping an eye on percentage of `NaN` cells in the dataframe (i.e., $32. 916 \\%$) we couldn\u2019t remove all the rows or cols with NaN cells as that could cause whole data wipe out and hence a significant loss. So what we can do, we can drop those columns thath contains a significant amount of `NaN` such as:<br>\n`INTENSITY-MME`, `INTENSITY-MMI`, `INTENSITY-MM`, `ORIGIN TIME-(IST)`, `LOCATION`, `MAGNITUDE-Mb`, and `ORIGIN TIME-(UTC)` whose `NaN` counts are over $50\\%$.\n2. Now coming to the columns whose existence doesn\u2019t at all matters or have any relevance are the ones that provide extra information about each data point. Since our motive is to predict the $M_w$ (i.e., the magnitude of earthquake), `Sl. No.` , and `REFERENCE` have no role in helping out for prediction.\n3. Further, columns like `MAGNITUDE-Mb1` , `MAGNITUDE-Ms` , and `MAGNITUDE-ML` are related to magnitude and these features are obtained only when earthquake occurs. So it doesn\u2019t make sense to use these if we were to predict earthquake in future or at some location.\n4. Now, `MAGNITUDE-Mw` and `MAGNITUDE-Mw1` are the same columns. The only aspect at where they differ is the number of `NaN` cells. If we remove the rows from DataFrame corresponding to `NaN` cells of `MAGNITUDE-Mw` and check for equality of both columns then we got `True` flag. So, it\u2019s better to keep the `MAGNITUDE-Mw1` column for our prediction purpose and drop the other one.","d161a7ee":"#### KNN","aff72238":"__Install the required dependencies...__","0ac5b0db":"__Last Pre-Pocessing Step...__\n\nRemove duplicate rows present in the dataframe, if any. And finally save all four dataframes.","593118ea":"## Task 4\nIf you have to choose only a subset of **two** features to predict earthquake, which ones would it be? Give Reasoning. [*Hint: You may use nodes of estimated Decision Tree or other techniques*]","8a0767e1":"# EDA (Exploratory Data Analysis)","148f2bf1":"## Task 3\n\nWhat could be the best possible values of the parameters for respective\nclassifier based on the ROC curves? Give Reasoning.","ffbfe3aa":"### Ensemble Learning as Predictor.","99096e99":"You are given the enclosed data files in the csv file. They are data about occurrences of earth quakes in a geographical region. The meta data is :\n\n* **Sl. No.**: Serial Number\n* **Year, Month, Day**: Date of a particular earthquake as per UTC (Coordinated Universal Time)\n* **Origin Time** of earthquake in UTC and IST (Indian Standard Time) in $[Hour: Minute: seconds]$ format \n* **Magnitude of Earthquake**: There are a different way to represent the magnitude of an earthquake. For your study, you can consider $M_w$, since we are deriving other types from $M_w$ only.\n* GPS Location in terms of Latitude(Lat) and Longitude(Long) of earthquake.\n* **Depth**: Depth of occurrence of an earthquake in kilometre\n* **Location**: Name of a region where an earthquake took place\n* **Source**: The agency from which we have gathered the data, for e.g. IMD=Indian Meteorological Department, Min. of Earth Science, Government of India\\\nA sample row :\n\n| 52165 | 2016 | 7   | 7   | 22:24:02 | 3.3 | 3.3 | 3.164855 | 2.438576 | 3.019937 | 26.8 N | 89.5 E | 40  | Jalpaiguri,West Bengal | IMD |\n|-------|------|-----|-----|----------|-----|-----|----------|----------|----------|--------|--------|-----|------------------------|-----|\n\n<br>read as:<br>\n**\"A 3.3 magnitude earthquake occurred on 7th July 2016 at 22:24:02 (UTC). The location of earthquake event was Jalpaiguri, West Bengal area with GPS location 26.8 N 89.5 E at a depth of 40km published by the source IMD\"**","aeee7946":"#### Iterative Imputer (from `scikit-learn`)","5d8a3cdd":"#### MICE (from `fancyimpute`)","c085f115":"### Imputation with KNN, MICE (from `fancyimpute`), DataWig, and IterativeImputer (of Scikit-Learn) with 3 diff estimators.","a3ee1476":"__Let's transform the Year, Date, and Month columns.__\n\n*Method of Transformation...*\n1. Remove __First Two Rows__ as it contains negative year.\n2. Replace '0' in MONTH and DATE column with `NaN`, since there are no zero months and date.\n3. Since the values in year column are in chronological order, we devised a method of imputaton for both date and month.\n    1. Put $Month=1$ if no months have been registered for the same year, else put $value=+1$ to the the value from preceding month.\n    2. If preceding $month = 12$ for the same year, keep the current month as 12 and make one step increment in date. Follow the same procedure for date column too.\n    3. If $date = 31$ and $month = 12$, then change change the year itself with one step increment putting the date as `01\/01\/incremented year`.\n    4. While, if $date = 31$ and $month < 12$, then the date would be `01\/current_month + 1\/current_year`.","2ca5ee51":"## Cleaning and Imputing Depth Column\n\n__Overview:__ Converted \"0\", and \" \" to `NaN` and then used `KNN`, `MICE` and it\u2019s different versions, and `DataWig` to impute **Depth** Column.\n* With `KNNImputer()` and `IterativeImputer()` class from `scikit-learn` package we implemented $4$ different versions of imputaion with $3$ versions of `IterativeImputer()`.\n* `KNNImputer()` imputes new sample by finding the samples in the training set \u201cclosest\u201d (in terms of \u201cEuclidean\u201d distance) to it and averages these nearby points to fill in the value.\n* While the `IterativeImputer()` refers to a process where each feature is modeled as a function of the other features, e.g. a regression problem where missing values are predicted.\n    * Each feature is imputed sequentially, one after the other that allows to use the prior imputed values as a part of a model in predicting subsequent features.\n    * Since this process is repeated multiple times it is iterative in nature, allowing ever improved estimates of missing values to be calculated as missing values across all features are estimated.\n    * This approach may be generally referred to as fully conditional specification (**FCS**) or multivariate imputation by chained equations (**MICE**).\n* Different regression algorithms can be used to estimate the missing values for each feature, and hence we used 3 different regressors, namely:\n    1. `BayesianRidge()`, the default one.\n    2. `ExtraTreesRegressor(n_estimators=50, random_state=0)`.\n    3. `RandomForestRegressor(n_estimators=50, random_state=0)`.\n* `DataWig()` is a neural-network model that predict the imputed values using other features. This method proved to be inefficient for our dataframe and got earlystopped at $5^{th}$ epoch due to divergence.","ed66fe72":"## Type conversion for Year, Month and Date\n\nSince **YEAR** is of `object`, and **MONTH** and **DATE** are of `float64` type, we typecast them all to `Int32` format.","4d4baaa9":"__Define Models and Evaluation Criteria...__","6681dd3f":"## Deciding which columns to drop and which to keep - DATA cleaning","9e753528":"__So as per above analysis, we can assert that `YEAR`, `LAT (N)` and `LONG (E)` are the features among all that best explains the Magnitude.__\n\n***Reasoning:***\n1. According to Longitude and Latidude, we can have information about Earthquake prone location or we can put it in this way that at given (Longitude, Latitude) the tectonic plate exhibit such motions that decide about the magnitude of earthquake and it's degree of severeness.\n2. Normally, at locations where tectonic plates collide (convergent boundary) or emerge (divergent boundary), we can witness earthquake, not all locations do that.\n3. `YEAR` record the time-stamp or time-rate of change of plates boundary position which informs us regarding the tectonic activity which may occur in future (resulting in Earth-Quake). If time-rate is high then for sure in future there will be earthquakes and it's a earthquake prone zone. So given the time-rate and co-ordinates, one can predict is this a earth-quake prone zone or not and accordingly can predict when next the future earthquake will happen.","ef4ce3f7":"##### __Train the Models and obtain their scores for every possible parameter for each model.__\n\nFor each dataset (**total 4**), we performed `GridSearch()` with `StratifiedKFold()` with $K = 5$ as our cross-validation. We took $5$ parameters for each of the $3$ models (shown below) and trained it for $5$ times in each fold, which totals to $25$ run for $5$ folds. And $75$ runs for all $3$ models. $75 \\times 4 = 300$ runs for all $4$ datasets.\n\n| $n\\_neighbors$ (KNN) | $max\\_depth$ (Decision Tree) | $n\\_estimators$ (LightGBM) |\n|--------------------|----------------------------|--------------------------|\n| 50                 | 3                          | 10                       |\n| 80                 | 5                          | 25                       |\n| 120                | 7                          | 50                       |\n| 150                | 9                          | 100                      |\n| 180                | 11                         | 150                      |"}}