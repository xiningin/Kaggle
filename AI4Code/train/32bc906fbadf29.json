{"cell_type":{"86058691":"code","cf693df3":"code","c8afc115":"code","1fa0d91b":"code","9fe1f536":"code","761172e9":"code","3a4e7feb":"code","c6778485":"code","c2096077":"code","8922009c":"code","4390f785":"code","87b0ea4f":"code","fb8329a7":"code","50dfb4b8":"code","3fafc512":"code","aa964cf8":"markdown","566446ff":"markdown","4acce50e":"markdown","84d2c2eb":"markdown","4b0d5b2c":"markdown","572704be":"markdown","8646cde6":"markdown","d71948e7":"markdown","e5819849":"markdown"},"source":{"86058691":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cf693df3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sys\nimport scipy","c8afc115":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.sample(5)","1fa0d91b":"data.columns","9fe1f536":"data.shape","761172e9":"# random_state helps assure that you always get the same output when you split the data\n# this helps create reproducible results and it does not actually matter what the number is\n# frac is percentage of the data that will be returned\ndata=data.sample(frac=0.2, random_state=1)\n","3a4e7feb":"data.shape","c6778485":"284807\/5","c2096077":"# plot the histogram of each parameter\ndata.hist(figsize=[20,20])\nplt.show()","8922009c":"# determine the number of fraud cases\nfraud=data[data['Class']==1]\nvalid=data[data.Class==0]\noutlier_fraction=len(fraud)\/len(valid)\nprint(outlier_fraction)\nprint('Fraud Cases: ',len(fraud))\nprint('Valid Cases: ',len(valid))\n","4390f785":"# correlation matrix\ncorrmat =data.corr()\nfig=plt.figure(figsize=[10,10])\nsns.heatmap(corrmat,vmax=.8,square=True)\nplt.show()\n","87b0ea4f":"# get the columns from the dataframe\ncolumns = data.columns.tolist()\n# filter the columns to remove the data we do not want\ncolumns = [c for c in columns if c not in ['Calss']]\n\n# store the variable we will be predicting on which is class\ntarget = 'Class'\n\n# X includes everything except our class column\nX = data[columns]\n# Y includes all the class labels for each sample\n# this is also one-dimensional\nY = data[target]\n\n# print the shapes of X and Y\nprint(X.shape)\nprint(Y.shape)\n\n","fb8329a7":"from sklearn.metrics import accuracy_score , classification_report\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor","50dfb4b8":"# define a random state\nstate=1\n\n# define the outlier detection methods\nclassifiers = {\n    # contamination is the number of outliers we think there are\n    'Isolation Forest': IsolationForest(max_samples = len(X),\n                                       contamination = outlier_fraction,\n                                       random_state = state),\n    # number of neighbors to consider, the higher the percentage of outliers the higher you want to make this number\n    'Local Outlier Factor': LocalOutlierFactor( n_neighbors = 20,\n                                               contamination = outlier_fraction)\n}\n","3fafc512":"\nn_outliers = len(fraud)\n\n\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    \n    # fit the data and tag outliers\n    if clf_name == 'Local Outlier Factor':\n        y_pred = clf.fit_predict(X)\n        scores_pred = clf.negative_outlier_factor_\n    else:\n        clf.fit(X)\n        scores_pred = clf.decision_function(X)\n        y_pred = clf.predict(X)\n        \n    # reshape the prediction values to 0 for valid and 1 for fraud\n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1\n\n    # calculate the number of errors\n    n_errors = (y_pred != Y).sum()\n    \n    # classification matrix\n    print('{}: {}'.format(clf_name, n_errors))\n    print(accuracy_score(Y, y_pred))\n    print(classification_report(Y, y_pred))","aa964cf8":"You can see a lot of the values are close to 0 . Most of them are fairly unrelated. The lighter squares signify a stronger correlation.","566446ff":"## Applying Algorithms","4acce50e":"## Imports","84d2c2eb":"## Organizing the Data\n","4b0d5b2c":"Looking at precision for fraudulent cases (1) lets us know the percentage of cases that are getting correctly labeled. 'Precision' accounts for false-positives. 'Recall' accounts for false-negatives. Low numbers could mean that we are constantly calling clients asking them if they actually made the transaction which could be annoying.\n\nGoal: To get better percentages.\n\nOur Isolation Forest method (which is Random Forest based) was able to produce a better result. Looking at the f1-score 26% (or approx. 30%) of the time we are going to detect the fraudulent transactions.","572704be":"You can see most of the V's are clustered around 0 with some or no outliers. Notice we have very few fraudulent cases over valid cases in our class histogram.","8646cde6":"Hello!\n\nI am really excited about machine-learning and decided to take on this project as the first of many to get more comfortable the models used.\n\nThis project covers credit card fraud and is meant to look at a dataset of transactions and predict whether it is fraudulent or not. I learned alot of this from Eduonix Learning Solutions.\n","d71948e7":"## Fit the Model\n","e5819849":"## Data Importing\n"}}