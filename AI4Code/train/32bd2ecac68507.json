{"cell_type":{"dd774925":"code","bb01ba4e":"code","5f0faa83":"code","986929ce":"code","03b6cb5e":"code","3bbe5396":"code","41812bca":"code","303b5908":"code","b09610cd":"code","5fe4b415":"code","78badc88":"code","2c093db1":"code","58a08153":"code","b6a7105d":"code","76f476f0":"code","d6d0a965":"code","6b192de3":"code","3b526085":"code","58f21c73":"code","48fa21e2":"code","beadd1fb":"code","3b219938":"code","ad34ee6d":"code","1d0e6a0b":"code","380438df":"code","fcbd547c":"code","061c251e":"code","9db76def":"code","b736a722":"code","bc5f7d5b":"code","4130cd58":"code","2bec72a2":"code","279c9937":"code","de6327f0":"code","3fb36c8e":"code","5930206f":"code","9133faa7":"code","173374a6":"code","2a98b2ca":"code","85d97382":"code","117d065b":"code","ecf440b0":"code","1c5998f7":"code","bf23d6c9":"markdown","8572a146":"markdown","27bdd13d":"markdown","909e9512":"markdown","ce6ca7ef":"markdown","829310c5":"markdown","566e19ae":"markdown","8e95064b":"markdown","c8c16b24":"markdown","2ae754a6":"markdown","f030b68c":"markdown","454aefbc":"markdown","b03d6f0b":"markdown","95a3df42":"markdown","e2d1deb7":"markdown","73397530":"markdown","144dc0a6":"markdown","22315dae":"markdown","6d3d90e9":"markdown","7c73cc12":"markdown","c39ec230":"markdown","41c853dd":"markdown","1974f7bb":"markdown","dbb976e5":"markdown","d0e16750":"markdown","d8430fad":"markdown","fbe3040d":"markdown","7c8a42d3":"markdown","318faf0c":"markdown","f2f5995f":"markdown","95e1bbc9":"markdown","eb6654c9":"markdown","3fd8215e":"markdown","5a55a2b9":"markdown","90fee3dc":"markdown","a0a16a26":"markdown","60ae7af1":"markdown","df6b94b5":"markdown","4192b698":"markdown","2e41f305":"markdown","1c03ee33":"markdown","6536fbf3":"markdown","47aec2f4":"markdown","dc2e2632":"markdown","4ef769d4":"markdown","cba970e9":"markdown","eba84a35":"markdown"},"source":{"dd774925":"import numpy as np\nimport pandas as pd\nfrom pandas import Series, DataFrame\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","bb01ba4e":"df = pd.read_csv('..\/input\/titanic\/train.csv')\ndf","5f0faa83":"print('PassengerId unique values : %d' % (len(df['PassengerId'].unique())))\nprint('Name unique values : %d' % (len(df['Name'].unique())))\nprint('Ticket unique values : %d' % (len(df['Ticket'].unique())))","986929ce":"df = df.drop(columns=['PassengerId', 'Ticket'])","03b6cb5e":"df.isna().sum()","3bbe5396":"plt.figure(figsize=(6,4))\nsns.countplot(x='Survived',data=df)\nplt.title('Distribution of passenger survivability')","41812bca":"plt.figure(figsize=(6,4))\nsns.countplot(x='Pclass',data=df)\nplt.title('Distribution of passenger ticket class')","303b5908":"plt.figure(figsize=(6,4))\nsns.countplot(x='Sex',data=df)\nplt.title('Distribution of passenger sex')","b09610cd":"plt.figure(figsize=(6,4))\nsns.countplot(x='SibSp',data=df)\nplt.title('Distribution of passenger siblings\/spouse relationship')","5fe4b415":"plt.figure(figsize=(6,4))\nsns.countplot(x='Parch',data=df)\nplt.title('Distribution of passenger parents\/children relationship')","78badc88":"plt.figure(figsize=(6,4))\nsns.countplot(x='Embarked',data=df)\nplt.title('Distribution of passenger embarkation port')","2c093db1":"f, (ax1, ax2) = plt.subplots(figsize=(12,4), nrows=1, ncols=2)\nsns.boxplot(df['Age'].dropna(), ax=ax1)\nsns.distplot(df['Age'].dropna(), ax=ax2)\nf.suptitle('Distribution of passenger age')","58a08153":"f, (ax1, ax2) = plt.subplots(figsize=(12,4), nrows=1, ncols=2)\nsns.boxplot(df['Fare'].dropna(), ax=ax1)\nsns.distplot(df['Fare'].dropna(), ax=ax2)\nf.suptitle('Distribution of passenger fare')","b6a7105d":"df[['Age','Fare']].describe()","76f476f0":"f, (ax1, ax2) = plt.subplots(figsize=(12,4), nrows=1, ncols=2)\nsns.boxplot(x='Pclass',y='Fare',data=df, ax=ax1)\nsns.boxplot(x='Embarked',y='Fare',data=df, ax=ax2)\nax1.title.set_text('Distribution of ticket fare for each class')\nax2.title.set_text('Distribution of ticket fare for each port')","d6d0a965":"cols = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\nfig = plt.figure(figsize=(15,10))\nfor c,i in zip(cols, range(1,6)):\n    ax = fig.add_subplot(2,3,i)\n    sns.countplot(x='Survived',hue=c,data=df)\n    ax.set_title(c)\n    ax.legend(loc=\"upper right\") \nfig.tight_layout(pad=4.0) \nfig.suptitle('Passenger survivability based on each categorical variable')","6b192de3":"f, (ax1, ax2) = plt.subplots(figsize=(12,4), nrows=1, ncols=2)\nsns.boxplot(x='Survived',y='Age',data=df, ax=ax1)\nsns.boxplot(x='Survived',y='Fare',data=df, ax=ax2)\nax1.title.set_text('Passenger survivability based on age')\nax2.title.set_text('Passenger survivability based on fare')","3b526085":"df[(df['Name'].str.contains('Duff Gordon')) | (df['Name'].str.contains('Countess')) | (df['Fare'] == df['Fare'].max())]","58f21c73":"df.isna().sum()","48fa21e2":"df = df.drop(columns='Cabin')","beadd1fb":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\ndf['Embarked'] = imp.fit_transform(np.array(df['Embarked']).reshape(-1,1))","3b219938":"from sklearn.impute import KNNImputer\ntemp = df.copy()\ntemp = pd.concat([temp,pd.get_dummies(temp[['Sex','Embarked']])],axis=1)\ntemp = temp.drop(columns=['Sex','Embarked'])\n\nimputer = KNNImputer(n_neighbors=8)\nwel = DataFrame(imputer.fit_transform(temp.drop(columns=['Name'])))\ndf1 = df.copy()\ndf1['Age'] = wel[2]\ndf1","ad34ee6d":"df1[df1['Name'].str.contains('Sage,')]","1d0e6a0b":"c = df['Name'].str.split(', ').str[1]\ntitle = c.str.split('.').str[0]\nTitle = title.unique()\n\nfig = plt.figure(figsize=(15,15))\nfor c,i in zip(Title, range(1,18)):\n    df0=df[df['Name'].str.contains(c)].dropna()\n    ax = fig.add_subplot(5,5,i)\n    sns.boxplot(df0['Age'])\n    ax.set_title(c)\nfig.tight_layout(pad=4.0)\nfig.suptitle('Age distribution among passenger title name')","380438df":"def age_pred(title):\n    return (df[df['Name'].str.contains(title)]['Age'].median())\n\ndef age_predict(df):\n    c = df['Name'].str.split(', ').str[1]\n    title = c.str.split('.').str[0]\n    df['title'] = title\n    df.loc[df['Age'].isna(),'Age'] = df[df['Age'].isna()]['title'].apply(age_pred)\n    del df['title']\n    return(df)","fcbd547c":"df2 = age_predict(df)\ndf2","061c251e":"df2.isna().sum()","9db76def":"data = df2.drop(columns=['Name'])\ndata = pd.concat([data,pd.get_dummies(data[['Sex','Embarked']])],axis=1)\ndata = data.drop(columns=['Sex','Embarked'])\ndata","b736a722":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nX = data.iloc[:,1:12]\ny = data.iloc[:,0]","bc5f7d5b":"from sklearn.tree import DecisionTreeClassifier\nparam_grid = {'max_depth' : [4, 5, 6, 7, 8], 'ccp_alpha' : [0, 0.001, 0.002, 0.003, 0.004, 0.005]}\ngrid = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\ngrid.fit(X, y)\nprint('Best hyperparameter: ', grid.best_params_)\nprint('Best cross validation score: ', grid.best_score_)","4130cd58":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(grid, X, y, display_labels=['Unsurvived','Survived'], cmap=plt.cm.Blues)","2bec72a2":"from sklearn.tree import plot_tree\nplt.figure(figsize=(20,12))\nplot_tree(DecisionTreeClassifier(**grid.best_params_).fit(X,y), feature_names=data.drop(columns='Survived').columns, \n          class_names=['Unsurvived','Survived'], filled=True, fontsize=9)\nplt.show()","279c9937":"from sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\npipe = Pipeline([(\"scaler\", StandardScaler()), (\"svc\", SVC())])\nparam_grid = {'svc__C': [0.3, 0.4, 0.5, 0.6, 0.7], 'svc__gamma' : [0.01, 0.05, 0.1, 0.5, 1]}\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\ngrid.fit(X,y)\nprint('Best hyperparameter: ', grid.best_params_)\nprint('Best cross validation score: ', grid.best_score_)","de6327f0":"plot_confusion_matrix(grid, X, y, display_labels=['Unsurvived','Survived'], cmap=plt.cm.Blues)","3fb36c8e":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(random_state=0,n_jobs=4)\nparam_grid = {'n_estimators': [200, 300, 400], 'max_depth' : [8, 9, 10]}\ngrid = GridSearchCV(forest, param_grid=param_grid, cv=5)\ngrid.fit(X,y)\nprint('Best hyperparameter: ', grid.best_params_)\nprint('Best cross validation score: ', grid.best_score_)","5930206f":"plot_confusion_matrix(grid, X, y, display_labels=['Unsurvived','Survived'], cmap=plt.cm.Blues)","9133faa7":"forest.fit(X, y)\nindices = np.argsort(forest.feature_importances_)[::-1]\ncc = DataFrame({'feature score':Series(forest.feature_importances_),'features':Series(X.columns)})    \nplt.figure(figsize=(8,6))\nsns.barplot(x='feature score',y='features',data=cc.head(50).sort_values(by='feature score',ascending=False))","173374a6":"new_col = np.array(Series(X.columns[indices]).head(7))\nX_new = X[new_col]\ngrid.fit(X_new,y)\nprint('Best hyperparameter: ', grid.best_params_)\nprint('Best cross validation score: ', grid.best_score_)","2a98b2ca":"dt = pd.read_csv('..\/input\/titanic\/test.csv')\ndt","85d97382":"dt.isna().sum()","117d065b":"dt = dt.drop(columns=['PassengerId','Ticket','Cabin'])\ndt = age_predict(dt)\ndt.loc[dt['Fare'].isna(),'Fare'] = df[df['Pclass'] == dt[dt['Fare'].isna()]['Pclass'].values[0]]['Fare'].median()\ndt","ecf440b0":"dt.isna().sum()","1c5998f7":"data_test = dt.drop(columns=['Name'])\ndata_test = pd.concat([data_test,pd.get_dummies(data_test[['Sex','Embarked']])],axis=1)\ndata_test = data_test.drop(columns=['Sex','Embarked'])\nX_test = data_test[new_col]\n\ny_pred = grid.predict(X_test)\nfinal = pd.concat([dt, Series(y_pred,name='Survived')], axis=1)\nfinal","bf23d6c9":"### Hypothesis based on EDA","8572a146":"If we recall again the history of Titanic, in the final hours of the shipwreck, the lifeboats began to be launched with orders of women and children first. This explains why did many of the female passengers survived more than the male passengers. We can also roughly assume that class 1 passengers were prioritized over the other classes. It might be that many important and prominent persons\/families were in the class 1, hence they were prioritized to the lifeboats. Several names can be noticed here.","27bdd13d":"Finally, we have predicted which passengers would have survived the Titanic shipwreck using our best predictive model based on the training data.","909e9512":"Some of the columns are the primary key of the data (all the values are unique for each record). These columns are PassengerId and Name. The Ticket column also has a lot of unique values (681) and might not contain information needed for prediction task since it is only act as an identifier. We should drop PassengerId and Ticket columns but keep the Name column for later purpose.","ce6ca7ef":"## Dropping Some Columns","829310c5":"Passenger fare might be associated with ticket class since the class represents economic grade among the passengers as can be seen below. We also try to find relationship between the passenger fare and embarkation port. ","566e19ae":"## Implementing to the Test Data","8e95064b":"For the first model, we will use decision tree classifier from sklearn package. Feature scaling is not necessarily needed for tree classifier. We will perform hyperparameter tuning using grid search + cross validation method.","c8c16b24":"Our decision tree model can be visualized as can be seen from the above figure. We can see that the first feature used for splitting the data is passenger sex. This confirms that passenger sex is important factor for determining the survivability of the passenger. Other features that appear quite often in this tree plot are Age, Pclass, and Fare.","2ae754a6":"## Building Predictive Models","f030b68c":"## Handling Missing Values","454aefbc":"It turns out that Embarked column has very low importance score compared to the other columns. Hence, we can drop this column and refit the model.","b03d6f0b":"In this notebook, we are going to analyze and build machine learning model to predict which passengers would have survived the Titanic shipwreck disaster. The given dataset consists of several information of every passenger in the ship, e.g. ticket number, ticket class, sex, age, port of embarkation etc. \n\nThe aim of this project is to build a model which can predict which passengers would have survived the shipwreck. There are only 2 target values: survived (1) or not survived (0), hence it is a binary classification problem. Apart from the modelling objective, we can also find the factor(s) which mostly affect the survivability of the passenger. ","95a3df42":"Before we can implement our model, missing values from test data have to be handled in the same manner as in training data:\n- Drop Cabin column\n- Impute NA values in Age column by the median value of its corresponding passenger title name\n\nUnfortunately, there is one NA value in Fare column. Judging by its number of NA values, we can 'safely' fill them with the median value of Fare column from training data.","e2d1deb7":"Another approach which might be more sensible is filling the missing age values using median value of the corresponding passenger title name. Every passenger's name contains title such as 'Miss', 'Mr', 'Mrs' etc. This title information can be used to infer the age of the corresponding passenger. We use median value because the age distributions among passenger title are quite skewed. ","73397530":"For the next model, we will use random forest classifier from sklearn package. Feature scaling is not necessarily needed as random forest is basically an ensemble of tree classifier. Hyperparameter tuning was done using grid search + cross validation method.","144dc0a6":"### Predictive model: Random Forest","22315dae":"Finally, we get the best accuracy score 0.831 (0.27% higher than SVC score) for random forest classifier with hyperparameters max_depth = 8 and n_estimators = 200.","6d3d90e9":"Several columns have missing values: Age, Cabin, Embarked. We will deal with that later.","7c73cc12":"For random forest classifier, our best accuracy in cross validation scheme is 0.827 which is slightly lower than the SVC ones. We can improve this score by employing feature selection (dropping unnecessary features). This can easily be done because random forest provide straightforward method for feature selection by calculating feature importances.","c39ec230":"Several remarks can be drawn from here:\n- The average age of Titanic passenger is about 30. The age distribution has little skewness to right.\n- The passenger fare distribution is quite extreme. 75% of the fare values are between 0 - 31. The remaining fare has very high value up to 512 (not sure what currency used for this fare. It can be Dollar or Pound). ","41c853dd":"Before building the predictive model, training data must be prepared. We no longer need Name column for modelling purpose so we can drop it. Categorical columns (Sex and Embarked) must be encoded into dummy variables because sklearn package cannot handle string categorical data. In this classification task, we will use accuracy as the metric.","1974f7bb":"### Improvement: Random Forest with feature selection","dbb976e5":"### Strategy for Age column: median age based on the passenger title name","d0e16750":"We can see that the Cabin column has so much NA values, more than 75% of the records to be precise. Filling out this numerous NA values will be worthless, hence we should drop this column.","d8430fad":"### Distribution plot of each categorical and discrete variable","fbe3040d":"Finally, all of the NA values have been handled.","7c8a42d3":"### Relation between several variables","318faf0c":"## Exploratory Data Analysis (EDA)","f2f5995f":"After handling missing values, we proceed to the prediction step. We choose our best predictive model which is random forest classifier with tuned hyperparameters (max_depth = 8 and n_estimators = 200). Feature selection also performed to the test data.","95e1bbc9":"Next we can try to find factor(s) which affect the survivability of the passenger by plotting Survived column values with each feature.","eb6654c9":"## Importing Dataset","3fd8215e":"Several remarks:\n- It seems that survivability for passenger class 1 (which is high economic grade) is higher than the other class.  \n- Survivability of female passenger is clearly higher than male passenger.\n- Other variables have no clear effect to the passenger survivability, but passenger age seems has quite little effect (younger passenger tend to survive).","5a55a2b9":"## Introduction","90fee3dc":"For support vector classifier, our best accuracy in cross validation scheme is 0.828 with hyperparameters C = 0.6 and gamma = 0.1. Also shown here the confusion matrix that results from implementing SVC to the training data. The SVC score is slightly higher (around 0.5%) than the tree classifier. ","a0a16a26":"One drawback of KNN imputation on this data is several passengers from same family whose age is unknown are imputed with same age. Example can be seen for Sage family in the table below.","60ae7af1":"### Strategy for Age column: KNN imputation","df6b94b5":"Several remarks can be drawn from here:\n- More than half of total passengers were not survived.\n- Most of the passengers take class 3 ticket .\n- Most of the passengers are male.\n- Most of the passengers aboard the ship individually (without siblings).\n- Most of the passengers aboard the ship from Southampton.","4192b698":"There are only 2 NA values in Embarked column which is a categorical variable hence we can 'safely' fill them with the most frequent value of the column.","2e41f305":"### Predictive model: Decision Tree","1c03ee33":"For the next model, we will use support vector classifier (SVC) from sklearn package. Feature scaling is needed for this classifier and we will use StandardScaler. Hyperparameter tuning was done using grid search + cross validation method.","6536fbf3":"### Distribution plot of each continuous numerical variable.","47aec2f4":"### Predictive model: Support Vector Machine","dc2e2632":"For decision tree classifier, our best accuracy in cross validation scheme is 0.824 with hyperparameters ccp_alpha = 0.003 and max_depth = 6. Also shown here the confusion matrix that results from implementing tree classifier to the training data.","4ef769d4":"Age columns has so many NA values (approximately 20% of the records). However, it is not wise to drop this column because passenger age might contain crucial information about passenger survivability. We will try to fill the missing values using nearest neighbor (KNN) imputation.","cba970e9":"Several remarks:\n- Passenger class of 1, 2, 3 can be associated with high, medium, low grade respectively based on its fare.\n- It seems that passengers who aboard the ship from Cherbourg were charged with higher fare.","eba84a35":"### Data preparation"}}