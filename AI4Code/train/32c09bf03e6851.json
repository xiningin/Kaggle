{"cell_type":{"de2e6229":"code","3e0d72b6":"code","bf30a858":"code","fb1d6c07":"code","eb74f628":"code","cf4d3041":"code","164ca3ab":"code","3bf0da45":"code","0edeb620":"code","241fd586":"code","4f3f568a":"code","3de06259":"code","84bc7188":"code","65aa8c2a":"code","74d7544e":"code","81ee708f":"code","03ede85b":"code","2427ab66":"code","1418f6e4":"code","1d9be538":"code","1667208c":"code","d285ca96":"code","1582dd34":"code","e71e08c5":"code","3b6c75ac":"code","d99d64dc":"code","a9ea3b5d":"code","f485aec9":"code","4c7b86e9":"code","70d060a8":"code","8b5fcd35":"code","64aa31db":"code","4a9f8598":"code","f6558d90":"code","4ea1d238":"code","097c89f9":"code","7b8a72de":"code","c26880dc":"code","013c829d":"code","3137b4fb":"code","c8fa7491":"code","8d8ad72b":"code","ee513e76":"code","f2eb5116":"code","99cbf484":"code","21e7cf8e":"code","c63d3ff4":"code","907c6cab":"code","6907e864":"code","a223a14a":"code","6d3dc791":"code","8b12903b":"code","138f62e2":"code","fe74f06a":"code","030b2a2c":"code","426b2fab":"code","46cf7138":"code","43202afb":"code","2ac580ad":"code","77fbe82d":"code","e9d93e5f":"code","db57206f":"code","bd0cc4ad":"markdown","1d78a42b":"markdown","6ccd5120":"markdown","84b6b547":"markdown","94b024db":"markdown","beca6e7f":"markdown","e5d0b36b":"markdown","f34ec2e3":"markdown","ed734f7d":"markdown","8452386f":"markdown","ae427637":"markdown","0436dd62":"markdown","8a6e3893":"markdown","f79cd31a":"markdown","587069be":"markdown","43546585":"markdown","53ca0946":"markdown","f6402907":"markdown","09f5cf1d":"markdown","cfbda292":"markdown","6fd503ab":"markdown","cd6179c1":"markdown","5448c70d":"markdown"},"source":{"de2e6229":"import time\nimport os\nimport sys\nimport gc\nimport pickle\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nsys.version_info","3e0d72b6":"os.getcwd()","bf30a858":"# Path to the dataset\n_START_PATH = os.path.join(\"..\", \"input\", \"competitive-data-science-predict-future-sales\")\n\nitems = pd.read_csv(os.path.join(_START_PATH, \"items.csv\"))\nshops = pd.read_csv(os.path.join(_START_PATH, \"shops.csv\"))\ncats = pd.read_csv(os.path.join(_START_PATH, \"item_categories.csv\"))\ntrain = pd.read_csv(os.path.join(_START_PATH, \"sales_train.csv\"))\n\n# set index to ID to avoid droping it later\ntest  = pd.read_csv(os.path.join(_START_PATH, \"test.csv\")).set_index('ID')","fb1d6c07":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min() - 5000, train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)","eb74f628":"train.item_cnt_day.describe()","cf4d3041":"print(\"For the column: 'item_cnt_day':\")\nprint(\"Min:\", train.item_cnt_day.min())\nprint(\"Max:\", train.item_cnt_day.max())\nprint(\"Range:\", train.item_cnt_day.max() - train.item_cnt_day.min())","164ca3ab":"train = train[train.item_price < 100000]\ntrain = train[train.item_cnt_day < 1001]","3bf0da45":"print(\"For the column: 'item_price':\")\nprint(\"Min:\", train.item_price.min())\nprint(\"Max:\", train.item_price.max())\nprint(\"Range:\", train.item_price.max() - train.item_price.min())","0edeb620":"_cond_1 = (train.shop_id == 32)\n_cond_2 = (train.item_id == 2973)\n_cond_3 = (train.date_block_num == 4)\n_cond_4 = (train.item_price > 0)\n\nmedian = train[_cond_1 & _cond_2 & _cond_3 & _cond_4].item_price.median()\ntrain.loc[train.item_price < 0, 'item_price'] = median\n\n# Using mean instead of the median does not improve the results\n# mean_val = train[_cond_1 & _cond_2 & _cond_3 & _cond_4].item_price.mean()\n# train.loc[train.item_price<0, 'item_price'] = mean_val","241fd586":"# Shop Name: \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11\n\n# Shop Name: \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n\n# Shop Name: \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58","4f3f568a":"# Check the Column Names of the shops Dataframe\nprint(f\"Column Names of the shops Dataframe: {shops.columns.tolist()}\")\n\n# Check the Column Names of the cats Dataframe\nprint(f\"Column Names of the cats Dataframe: {cats.columns.tolist()}\")","3de06259":"# We correct the name of the city the store is located in, with the below name\nshops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\n\n# We correct the name of the city, with the below name\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\n\n# In general, we remove the '!' character from the city name\nshops[\"city\"] = shops.city.apply(lambda x: str.replace(x, '!', ''))\n\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]","84bc7188":"cats['split'] = cats['item_category_name'].str.split('-')\ncats['type'] = cats['split'].map(lambda x: x[0].strip())\ncats['type_code'] = LabelEncoder().fit_transform(cats['type'])\n\n# If the subtype is nan then reuse the type as the subtype as well\ncats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\n\nprint(cats.head())\n\n# We will only be using the columns: item_category_id, type_code and subtype_code\n# for our model\ncats = cats[['item_category_id','type_code', 'subtype_code']]","65aa8c2a":"# We drop the column item_name as already have all the important information for prediction\n# in the item_category_id column\nitems.drop(['item_name'], axis=1, inplace=True)","74d7544e":"items.head()","81ee708f":"# The column item_id has the exact same values in the exact same positions as the default index column\nall(items.index == items.item_id)","03ede85b":"items = items.set_index(\"item_id\")","2427ab66":"len(test) == test.shape[0]","1418f6e4":"# Set of unique items in test set\na = set(test.item_id)\n\n# Set of unique items which are common to the test and train set\nb = set(test.item_id).intersection(set(train.item_id))\n\n# Number of unique items in the test set\nc = len(list(set(test.item_id)))\n\n# Total number of rows in the test set\nd = test.shape[0]\n\nlen(list(a - b)), c, d","1d9be538":"ts = time.time()  # Start time of this cell's execution\n\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \ntemp = matrix\n# Converting the list of numpy arrays into a DataFrame\nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\n\n# Setting the type for the various columns of the DataFrame\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\n\nmatrix.sort_values(cols,inplace=True)\n\n# Time taken to complete this operation\ntime.time() - ts","1667208c":"# temp","d285ca96":"# np.vstack(temp)","1582dd34":"matrix.head()","e71e08c5":"train['revenue'] = train['item_price'] * train['item_cnt_day']","3b6c75ac":"ts = time.time()  # Start time of this cell's execution\n\ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0, 20) # We clip target in this range as given in the challenge description\n                                .astype(np.float16))\n\ntime.time() - ts  # Time taken for this cell to execute","d99d64dc":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","a9ea3b5d":"ts = time.time()  # Start time of this cell's execution\n\nmatrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True)  # 34th month\n\ntime.time() - ts  # Time taken for this cell to execute","f485aec9":"ts = time.time()  # Start time of this cell's execution\n\nmatrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\n# matrix = pd.merge(matrix, items, on=['item_id'], how='left')\n\nmatrix = pd.merge(matrix, items, left_on=['item_id'], right_index=True, how='left')\n\nmatrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\n\ntime.time() - ts  # Time taken for this cell to execute","4c7b86e9":"LAG_LIST = [1, 2, 3, 6, 12]\n# LAG_LIST = [1, 2, 3, 4, 5, 12]","70d060a8":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","8b5fcd35":"ts = time.time()  # Start time of this cell's execution\n\nmatrix = lag_feature(matrix, LAG_LIST, 'item_cnt_month')\n\ntime.time() - ts  # Time taken for this cell to execute","64aa31db":"ts = time.time()  # Start time of this cell's execution\n\ngroup = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_avg_item_cnt')\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)\n\ntime.time() - ts  # Time taken for this cell to execute","4a9f8598":"ts = time.time()  # Start time of this cell's execution\n\ngroup = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, LAG_LIST, 'date_item_avg_item_cnt')\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\n\ntime.time() - ts  # Time taken for this cell to execute","f6558d90":"ts = time.time()  # Start time of this cell's execution\n\ngroup = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, LAG_LIST, 'date_shop_avg_item_cnt')\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)\n\ntime.time() - ts  # Time taken for this cell to execute","4ea1d238":"ts = time.time()  # Start time of this cell's execution\n\ngroup = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\nmatrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)\n\ntime.time() - ts  # Time taken for this cell to execute","097c89f9":"ts = time.time()  # Start time of this cell's execution\n\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_cat_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmatrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_cat_avg_item_cnt')\nmatrix.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)\n\ntime.time() - ts  # Time taken for this cell to execute","7b8a72de":"ts = time.time()  # Start time of this cell's execution\n\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_type_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'type_code'], how='left')\nmatrix['date_shop_type_avg_item_cnt'] = matrix['date_shop_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_type_avg_item_cnt')\nmatrix.drop(['date_shop_type_avg_item_cnt'], axis=1, inplace=True)\n\ntime.time() - ts  # Time taken for this cell to execute","c26880dc":"ts = time.time()  # Start time of this cell's execution\n\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nmatrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_subtype_avg_item_cnt')\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\n\ntime.time() - ts  # Time taken for this cell to execute","013c829d":"ts = time.time()  # Start time of this cell's execution\n\ngroup = matrix.groupby(['date_block_num', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'city_code'], how='left')\nmatrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_city_avg_item_cnt')\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\n\ntime.time() - ts  # Time taken for this cell to execute","3137b4fb":"ts = time.time()  # Start time of this cell's execution\n\ngroup = matrix.groupby(['date_block_num', 'item_id', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'city_code'], how='left')\nmatrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_item_city_avg_item_cnt')\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\n\ntime.time() - ts  # Time taken for this cell to execute","c8fa7491":"ts = time.time()  # Start time of this cell's execution\n\ngroup = matrix.groupby(['date_block_num', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_type_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'type_code'], how='left')\nmatrix['date_type_avg_item_cnt'] = matrix['date_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_type_avg_item_cnt')\nmatrix.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)\n\ntime.time() - ts  # Time taken for this cell to execute","8d8ad72b":"ts = time.time()  # Start time of this cell's execution\n\ngroup = matrix.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_subtype_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'subtype_code'], how='left')\nmatrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_subtype_avg_item_cnt')\nmatrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)\n\ntime.time() - ts  # Time taken for this cell to execute","ee513e76":"ts = time.time()  # Start time of this cell's execution\n\ngroup = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n\ngroup = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2,3,4,5,6]\nmatrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    matrix['delta_price_lag_' + str(i)] = \\\n        (matrix['date_item_avg_item_price_lag_' + str(i)] - matrix['item_avg_item_price']) \/ matrix['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_' + str(i)]:\n            return row['delta_price_lag_' + str(i)]\n    return 0\n    \nmatrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\nmatrix['delta_price_lag'].fillna(0, inplace=True)\n\n\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_' + str(i)]\n\nmatrix.drop(fetures_to_drop, axis=1, inplace=True)\n\ntime.time() - ts  # Time taken for this cell to execute","f2eb5116":"ts = time.time()  # Start time of this cell's execution\ngroup = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) \/ matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'delta_revenue')\n\nmatrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\ntime.time() - ts  # Time taken for this cell to execute","99cbf484":"matrix['month'] = matrix['date_block_num'] % 12","21e7cf8e":"days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\n\nmatrix['days'] = matrix['month'].map(days).astype(np.int8)","c63d3ff4":"ts = time.time()  # Start time of this cell's execution\n\ncache = {}\nmatrix['item_shop_last_sale'] = -1\nmatrix['item_shop_last_sale'] = matrix['item_shop_last_sale'].astype(np.int8)\nfor idx, row in matrix.iterrows():    \n    key = str(row.item_id)+' '+str(row.shop_id)\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        matrix.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n        cache[key] = row.date_block_num\n\ntime.time() - ts  # Time taken for this cell to execute","907c6cab":"ts = time.time()  # Start time of this cell's execution\n\ncache = {}\nmatrix['item_last_sale'] = -1\nmatrix['item_last_sale'] = matrix['item_last_sale'].astype(np.int8)\nfor idx, row in matrix.iterrows():    \n    key = row.item_id\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        if row.date_block_num>last_date_block_num:\n            matrix.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n            cache[key] = row.date_block_num\n\ntime.time() - ts  # Time taken for this cell to execute","6907e864":"ts = time.time()  # Start time of this cell's execution\n\nmatrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmatrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\n\ntime.time() - ts  # Time taken for this cell to execute","a223a14a":"ts = time.time()  # Start time of this cell's execution\n\nmatrix = matrix[matrix.date_block_num > 11]\n\ntime.time() - ts  # Time taken for this cell to execute","6d3dc791":"ts = time.time()  # Start time of this cell's execution\n\ndef fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = fill_na(matrix)\n\ntime.time() - ts  # Time taken for this cell to execute","8b12903b":"matrix.columns","138f62e2":"matrix.info()","fe74f06a":"matrix.to_pickle('data.pkl')\ndel matrix\ndel cache\ndel group\ndel items\ndel shops\ndel cats\ndel train\n# leave test for submission\ngc.collect();","030b2a2c":"data = pd.read_pickle('data.pkl')","426b2fab":"data = data[[\n    'date_block_num',\n    'shop_id',\n    'item_id',\n    'item_cnt_month',\n    'city_code',\n    'item_category_id',\n    'type_code',\n    'subtype_code',\n    'item_cnt_month_lag_1',\n    'item_cnt_month_lag_2',\n    'item_cnt_month_lag_3',\n    'item_cnt_month_lag_6',\n    'item_cnt_month_lag_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_2',\n    'date_item_avg_item_cnt_lag_3',\n    'date_item_avg_item_cnt_lag_6',\n    'date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1',\n    'date_shop_avg_item_cnt_lag_2',\n    'date_shop_avg_item_cnt_lag_3',\n    'date_shop_avg_item_cnt_lag_6',\n    'date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    'date_shop_cat_avg_item_cnt_lag_1',\n    #'date_shop_type_avg_item_cnt_lag_1',\n    #'date_shop_subtype_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    #'date_type_avg_item_cnt_lag_1',\n    #'date_subtype_avg_item_cnt_lag_1',\n    'delta_price_lag',\n    'month',\n    'days',\n    'item_shop_last_sale',\n    'item_last_sale',\n    'item_shop_first_sale',\n    'item_first_sale',\n]]","46cf7138":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","43202afb":"del data\ngc.collect();","2ac580ad":"ts = time.time()\n\nmodel = XGBRegressor(\n    gamma=0.125,\n    reg_alpha=0.170,\n    reg_lambda=0.171,\n    booster='gbtree',\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.875, # 0.8\n    eta=0.15,  # 0.3    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\ntime.time() - ts","77fbe82d":"os.getcwd()","e9d93e5f":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\n_SUBMISSION_PATH = os.path.join(\".\")\nsubmission.to_csv(os.path.join(_SUBMISSION_PATH, 'xgb_submission_1.csv'), index=False)\n\n# save predictions for an ensemble\n_PICKLE_PATH = os.path.join(\".\")\npickle.dump(Y_pred, open(os.path.join(_PICKLE_PATH, 'xgb_train_1.pickle'), 'wb'))\npickle.dump(Y_test, open(os.path.join(_PICKLE_PATH, 'xgb_test_1.pickle'), 'wb'))","db57206f":"plot_features(model, (10,14))","bd0cc4ad":"## Trend features","1d78a42b":"## Mean encoded features","6ccd5120":"Number of days in a month. There are no leap years.","84b6b547":"## Shops\/Items\/Cats features","94b024db":"## Final preparations\nBecause of the using 12 as lag value drop first 12 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set).","beca6e7f":"Last month shop revenue trend","e5d0b36b":"Validation strategy is 34 month for the test set, 33 month for the validation set and 13-33 months for the train.","f34ec2e3":"Months since the first sale for each shop\/item pair and for item only.","ed734f7d":"Producing lags brings a lot of nulls.","8452386f":"Months since the last sale for each shop\/item pair and for item only. I use programing approach.\n\n<i>Create HashTable with key equals to {shop_id,item_id} and value equals to date_block_num. Iterate data from the top. Foreach row if {row.shop_id,row.item_id} is not present in the table, then add it to the table and set its value to row.date_block_num. if HashTable contains key, then calculate the difference beteween cached value and row.date_block_num.<\/i>","ae427637":"Get the price trend for the last six months.","0436dd62":"## Target lags","8a6e3893":"## Shops\/Cats\/Items preprocessing\nObservations:\n* Each shop_name starts with the city name.\n    - For example, for the entry in the column ```shop_name```:\n        1. ```\"\u0412\u043e\u043b\u0436\u0441\u043a\u0438\u0439 \u0422\u0426 \"\"\u0412\u043e\u043b\u0433\u0430 \u041c\u043e\u043b\u043b\"\"\"```\n            * ```\"\u0412\u043e\u043b\u0436\u0441\u043a\u0438\u0439 \u0422\u0426``` is the name of the city\n            * ```\u0412\u043e\u043b\u0433\u0430 \u041c\u043e\u043b\u043b``` is the name of the shop\n* Each category contains type and subtype in its name.\n    - For example, for the entry in the column ```item_category_name```:\n        1. ```\u0418\u0433\u0440\u043e\u0432\u044b\u0435 \u043a\u043e\u043d\u0441\u043e\u043b\u0438 - PSVita```\n            * ```\u0418\u0433\u0440\u043e\u0432\u044b\u0435 \u043a\u043e\u043d\u0441\u043e\u043b\u0438``` is the type\n            * ```PSVita``` is the subtype","f79cd31a":"## Monthly sales\nTest set is a product of some shops and some items within 34 month. There are 5100 items * 42 shops = 214200 pairs. 363 items are new as compared to the train. Hence, for most of the items in the test set, the target value should be zero. \nOn the other hand, the train set contains only pairs which were sold or returned in the past.\n\nHence, the main idea here is to calculate the monthly sales and **extend it with zero sales** for each of the unique pair observed within the month. This way the train data will be similar to test data.","587069be":"## Test set\nAppend test pairs to the matrix.","43546585":"# Part 2, xgboost","53ca0946":"Several shops are duplicates of each other (according to their name). Hence we fixed this in the train and test set.","f6402907":"Aggregate the train set by using the shop\/item pairs to calculate target aggregates, then perform <b>clip(0,20)<\/b> on the target value. This way the train target will be similar to the test predictions.\n\n**NOTE:** We use floats instead of ints for ```item_cnt_month``` to avoid downcasting it after concatination with the test set later. If it would be **int16**, after concatination with NaN values it becomes **int64**, but **float16** becomes **float16** even with **NaNs**.","09f5cf1d":"There is one item with price below zero. Fill it with median.\nTry mean at the moment","cfbda292":"Select perfect features","6fd503ab":"## Outliers\n\nThere are items with strange prices and sales. After detailed exploration we decided to remove items with price > 100000 and sales > 1001 (1000 also works fine here).\n\nNote:\n1. ```item_cnt_day``` - The number of products sold in a day. We are trying to predict a monthly amount of this measure.\n1. ```item_price``` - The current price of an item","cd6179c1":"# Part 1, perfect features","5448c70d":"## Special features"}}