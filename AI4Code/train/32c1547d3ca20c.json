{"cell_type":{"3c7d8200":"code","46a7232d":"code","16087807":"code","5dab5fab":"code","0648f1ac":"code","0a7ee7fb":"code","6ae62b8e":"code","14e3914b":"code","8efc1fa2":"code","7cc9c676":"code","bfc01a9b":"code","bd138431":"code","fc488a68":"code","55780ae7":"code","057372a0":"code","06a78fa4":"code","3f8c43e8":"code","6db26753":"code","8098451b":"code","3e8c9b5a":"code","b5f8786e":"code","d1365a98":"code","2485de96":"code","5058025f":"code","0e28f27c":"code","ff7a2b87":"markdown","11fe4d22":"markdown","6976b04a":"markdown","0c4de9a8":"markdown","ecfc354a":"markdown","b8d573c3":"markdown","53e52022":"markdown","a830a181":"markdown","0e8a940c":"markdown","a2a940ec":"markdown","b0e403a7":"markdown","8d786251":"markdown","aec00ca8":"markdown","81934d6e":"markdown","590fb733":"markdown","f27c15a9":"markdown","dfe10c3e":"markdown","c584b19f":"markdown","775e609c":"markdown","755a958c":"markdown","01bb70ae":"markdown","71057f40":"markdown","2c13b7bd":"markdown","77050e8a":"markdown","8d5fe79f":"markdown"},"source":{"3c7d8200":"## Data Analysis\n## Main Aim is to understand about the data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.pandas.set_option('display.max_columns',None)","46a7232d":"dataset = pd.read_csv('..\/input\/mobile-price-classification\/train.csv')\nprint(dataset.shape)\ndataset.head()","16087807":"dataset.info()","5dab5fab":"## Since we are going to predict price, lets analyse based on price_range \n\ndataset['price_range'].value_counts()","0648f1ac":"sns.countplot(dataset['price_range'])\nplt.xlabel('price_range')\n","0a7ee7fb":"for i in dataset.columns:\n    if len(dataset[i].unique()) < 100 and i != 'price_range' :\n        print(i,'-----', len(dataset[i].unique()))\n    ","6ae62b8e":"###lets plot fo these discreate variables\ndiscreate_numerical = []\nfor i in dataset.columns:\n    if len(dataset[i].unique()) < 50:\n        discreate_numerical.append(i)\n\n        ","14e3914b":"plt.figure(figsize=(25,50))\nfor i in enumerate(discreate_numerical):\n    plt.subplot(8,2,i[0]+1)\n    sns.barplot(dataset[i[1]],dataset['price_range'])\n    plt.xlabel(i[1])","8efc1fa2":"plt.figure(figsize=(18,36))\nfor i in enumerate(discreate_numerical):\n    plt.subplot(8,2,i[0]+1)\n    sns.boxplot(dataset[i[1]],dataset['price_range'])\n    plt.xlabel(i[1])","7cc9c676":"numerical_fet = []\n\nfor i in dataset.columns:\n    if i  not in discreate_numerical:\n        numerical_fet.append(i)\n","bfc01a9b":"plt.figure(figsize=(18,36))\nfor i in enumerate(numerical_fet):\n    plt.subplot(8,2,i[0]+1)\n    sns.histplot(dataset[i[1]],kde=True)\n    plt.xlabel(i[1])","bd138431":"plt.figure(figsize=(25,25))\nsns.heatmap(dataset.corr(),annot=True)","fc488a68":"y = dataset['price_range']\ndataset.drop(['price_range'],axis=1,inplace=True)","55780ae7":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix\n\nx_tr,x_te,y_tr,y_te = train_test_split(dataset,y,test_size=0.3,random_state=42)\n\ncla = RandomForestClassifier()\ncla.fit(x_tr,y_tr)\nacuracy = accuracy_score(y_te,cla.predict(x_te))\ncm =confusion_matrix(y_te,cla.predict(x_te))\nprint(acuracy)\nsns.heatmap(cm,annot=True)","057372a0":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nselc_ten = SelectKBest(chi2,k=10).fit(x_tr,y_tr)","06a78fa4":"print(selc_ten.scores_)\nfor i in range(len(dataset.columns)):\n    print(dataset.columns[i],' :    ',selc_ten.scores_[i] )","3f8c43e8":"X_tr = selc_ten.transform(x_tr)\nX_te = selc_ten.transform(x_te)","6db26753":"clf_2 = RandomForestClassifier()","8098451b":"clf_2.fit(x_tr,y_tr)","3e8c9b5a":"accuracy_score(y_te,clf_2.predict(x_te))","b5f8786e":"cm1 = confusion_matrix(y_te,clf_2.predict(x_te))\nsns.heatmap(cm1,annot=  True)","d1365a98":"from sklearn.feature_selection import RFECV\n\nclf_3 = RandomForestClassifier()\nrfecv = RFECV(estimator=clf_3,cv=5, step=1,scoring='accuracy')\nrfecv.fit(x_tr,y_tr)","2485de96":"print(rfecv.n_features_)","5058025f":"print(x_tr.columns[rfecv.support_])","0e28f27c":"plt.figure()\nplt.xlabel('features')\nplt.ylabel('accuracy')\nplt.plot(range(1,len(rfecv.grid_scores_)+1),rfecv.grid_scores_)\n","ff7a2b87":"here we can see that we are having 15 discreate numerical variable ","11fe4d22":"1. Data Analysis\n2. Feature Engineering\n3. Feature Selection\n4. Model Building\n5. Model Deployment","6976b04a":"Here we are getting accuracy of 0.865 and we can also see that many features affect the prediction","0c4de9a8":"from the above model we can see that unwanted features giving us  type 1 and type 2 errors are minimizsed ","ecfc354a":"###### Now lets predict for test data","b8d573c3":"we can see the values is not numerical, it is classified into four categories having count of 500 each","53e52022":"we are having 21 columns all are \n###### Numerical values \n######  Having NO Null values","a830a181":"Lets see wat can we do if we want to increase more","0e8a940c":"###### Recursive feature elimination with cross validation and random forest classification","a2a940ec":"## Life Cycle of Data Science ","b0e403a7":"battery_power,ram,mobile_wt,px_height,px_width,fc,pc,sc_w,talk_time,int_memory\n\nthe above 10 features affect the model most ","8d786251":"selecting most important features from the list based on their value","aec00ca8":"we can see from the above the box plot for every features varies huge this indicates most of the features are providing information for result","81934d6e":"here it took 5 features to make the model to be optimized","590fb733":"lets count the unique values of all columns to find the discreate numerical variable","f27c15a9":"Since most of the values independent of each other lets get to feature selection","dfe10c3e":"1. 'battery_power' -battery_power\n2. 'blue' -bluetooth\n3. 'clock_speed' -clock_speed\n4. 'dual_sim' -sim\n5. 'fc' - Front Screen\n6. 'four_g'- 4G Connection\n7. 'int_memory'- Internal Memory\n8. 'm_dep'- Slimness\n9. 'mobile_wt' -Weight\n10. 'n_cores'- Processor\n11. 'pc'- Primary camera\n12. 'px_height' -Pixel hight\n13. 'px_width' - Pixel Width\n14. 'ram' - ram size\n15. 'sc_h'- screen hight\n16. 'sc_w'- screen width\n17. 'talk_time'- talk time\n18. 'three_g'- 3G\n19. 'touch_screen'- Touch facility\n20. 'wifi' - wifi\n21. 'price_range'- price","c584b19f":"here we can see most of the features are categorised in numbers","775e609c":"\n\n#### Recursive feature elimination with cross validation and random forest classification\nIn previous method we found how many features we needed most by our own choice,\n\nBut in this method we can find how many features can give best accuracy and the choice is made by cross validation","755a958c":"here we can see at 5 we are getting maximum accuracy, so we will be choosing 5 features for prediction","01bb70ae":"###### Lets analyse the corelation between these pics","71057f40":"Here we can see only px_height shows a right skewed distribution","2c13b7bd":" We can see here that Ram is highly co related with Price","77050e8a":"Lets do analysis for Numerical features","8d5fe79f":"here im getting accuracy .875 increased when compared to preivious"}}