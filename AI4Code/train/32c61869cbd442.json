{"cell_type":{"cadea9e2":"code","fdd16bb1":"code","d28186d6":"code","86015694":"code","54a6a3b8":"code","bd416c52":"code","7529fea8":"code","287ed13c":"code","bb4a5792":"code","53de8bd1":"code","9b989b70":"code","9eeb0552":"code","1547e1a1":"code","689bc556":"code","d02b7067":"code","0d2508a5":"code","8fb428c9":"code","855352f2":"code","4baf009a":"code","619022e9":"code","d4292cc7":"code","47e96fd8":"code","a3c504b2":"code","40df7435":"code","a40a3f7f":"code","d080faa8":"code","1f0dd1bd":"code","6fafc5b9":"code","2f3a46c6":"code","b5ea7f71":"code","93763f51":"code","dfd5b53d":"code","b716bd58":"code","f686fa4a":"code","f76dd7fc":"code","72088884":"code","9f44762b":"code","cad5f048":"code","80f45ba3":"code","fcd77cf2":"code","ce0061c2":"code","b24283a3":"code","1ae9c731":"code","b09ff39a":"code","1cc1208d":"code","7c87359c":"code","e3aed07b":"code","b863e61b":"code","205b39f3":"code","f9dcba7d":"code","571c1d6d":"code","7d7eb03f":"code","c6080c4c":"code","fd6d7783":"code","39aed411":"code","f4420ede":"code","60b59641":"code","481189c0":"code","cc907961":"code","d679a605":"code","776357c6":"code","20e0adf2":"code","8a999ddb":"code","bcff6977":"code","9965970e":"code","61549b8a":"code","e315bd5b":"code","406bbff9":"code","ba84fcc6":"code","fae265e3":"code","04a752a7":"code","981c6d3d":"markdown","1261312d":"markdown","94624006":"markdown","819c8b98":"markdown","1c2fab12":"markdown","d721c8a9":"markdown","d185890f":"markdown","d99af8e0":"markdown","6d1dc8bd":"markdown","ecc58aa0":"markdown","d3caa82d":"markdown","c4c12a9a":"markdown","c1abb162":"markdown","9a37f64e":"markdown","7a15593b":"markdown","7673216b":"markdown"},"source":{"cadea9e2":"!pip install -qq kaggle","fdd16bb1":"!mkdir ~\/.kaggle","d28186d6":"!cp kaggle.json ~\/.kaggle\/","86015694":"! chmod 600 ~\/.kaggle\/kaggle.json","54a6a3b8":"!kaggle datasets download -d kryvokhyzha\/refacefakedetectionimages8","bd416c52":"!unzip refacefakedetectionimages8.zip","7529fea8":"!pip install -qq av\n!pip install -qq torchsummary\n!pip install -qq linformer\n!pip install -qq vit_pytorch","287ed13c":"!pip install -qq albumentations==1.1.0","bb4a5792":"!pip install facenet-pytorch > \/dev\/null 2>&1\n!apt install zip > \/dev\/null 2>&1","53de8bd1":"!nvidia-smi","9b989b70":"from google.colab import drive\ndrive.mount('\/content\/drive')","9eeb0552":"!ls \/content\/drive\/MyDrive\/dl-creator-school\/","1547e1a1":"import os\nimport glob\nimport json\nimport cv2\nimport multiprocessing as mp\n\nimport pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.nn.functional as F\nimport torchvision\n\nfrom torch import nn, optim\nfrom torch.utils.data import sampler, DataLoader, Dataset\nfrom torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingLR, ReduceLROnPlateau, StepLR\nfrom torch.utils import data\nfrom torchvision import transforms, models\nfrom torchvision.models import resnet101\nfrom torchsummary import summary\n\nfrom albumentations import Normalize, Compose, Resize, CenterCrop, HorizontalFlip, Rotate, VerticalFlip, RandomCrop, Downscale, RandomBrightnessContrast, GaussianBlur, HueSaturationValue\nfrom albumentations.pytorch import ToTensorV2\n\nfrom facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization, training\n\nfrom linformer import Linformer\nfrom vit_pytorch.efficient import ViT\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\n\nfrom typing import List, Dict, Tuple, Union, Optional\nfrom pathlib import Path","689bc556":"import matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nplt.rcParams['figure.dpi'] = 150","d02b7067":"if torch.cuda.is_available():\n    device = 'cuda:0'\n#     torch.set_default_tensor_type('torch.cuda.FloatTensor')\n#     torch.multiprocessing.set_start_method('spawn')\nelse:\n    device = 'cpu'\nprint(f'Running on device: {device}')","0d2508a5":"# !ls ..\/input\/reface-fake-det-faces","8fb428c9":"PATH2PROJECT = Path('')\nPATH2DRIVE = Path('\/content\/drive\/MyDrive\/dl-creator-school\/')\n\nPATH2DATA = PATH2PROJECT \/ 'reface-fake-detection-result'\nPATH2TRAIN = PATH2DATA \/ 'train'\nPATH2TEST = PATH2DATA \/ 'test'\nPATH2SUBMISSIONS = Path('') \/ 'submissions'\nPATH2CHECKOUTS = Path('') \/ 'checkouts'","855352f2":"try: PATH2SUBMISSIONS.mkdir()\nexcept: pass\ntry: PATH2CHECKOUTS.mkdir()\nexcept: pass","4baf009a":"SEED = 42\nVAL_SIZE = 0.2","619022e9":"N_FACES = 6\n\nBATCH_SIZE = 32\nNUM_WORKERS = mp.cpu_count()\n\nWARM_UP_EPOCHS = 5\nWARM_UP_LR = 3e-3\nFINE_TUNE_EPOCHS = 20\nFINE_TUNE_LR = 5e-4\n\nH, W = 112, 112 #224, 224\nDELTA = 10\nMEAN = [0.485, 0.456, 0.406]\nSTD = [0.229, 0.224, 0.225]\n\nTHRESHOLD = 0.5\nEPSILON = 1e-7","d4292cc7":"meta_df = pd.read_csv(PATH2DATA \/ 'train.csv')\nmeta_df.shape","47e96fd8":"meta_df.label.value_counts(normalize=True)","a3c504b2":"meta_df['path'] = meta_df['filename'].apply(lambda x: str(PATH2TRAIN \/ x.split('.')[0]))","40df7435":"meta_df.sample(n=5, random_state=SEED)","a40a3f7f":"meta_df = meta_df[meta_df['path'].map(lambda x: os.path.exists(x))]\nmeta_df.shape","d080faa8":"# try:\n#     valid_meta_df = pd.read_csv(PATH2PROJECT \/ 'trainreface' \/ 'valid_meta_df.csv')\n# except:\nvalid_meta_df = pd.DataFrame(columns=['filename', 'label', 'path'])\nr = []\n# for row_idx, row in tqdm(train_df.iterrows()):\nfor row_idx in tqdm(meta_df.index):\n    row = meta_df.loc[row_idx]\n    img_dir = row['path']\n    face_paths = glob.glob(f'{img_dir}\/*.png')\n\n    if len(face_paths) >= 4: # Satisfy the minimum requirement for the number of faces\n        r.append(row)\n\nvalid_meta_df = valid_meta_df.append(r, ignore_index=True)\n# valid_meta_df.to_csv(PATH2PROJECT \/ 'trainreface' \/ 'valid_meta_df.csv', index=False)\nvalid_meta_df.shape","1f0dd1bd":"valid_meta_df.head()","6fafc5b9":"folders = os.listdir(PATH2TEST)\nX_test = pd.DataFrame({'path': [str(PATH2TEST\/folder) for folder in folders], 'filename': folders})\nlen(X_test)","2f3a46c6":"submission = pd.read_csv(PATH2PROJECT \/ 'sample_submission.csv')\nsubmission.shape","b5ea7f71":"submission['path'] = submission['filename'].apply(lambda x: str(PATH2TEST\/x.split('.')[0]))","93763f51":"X_train, X_val, y_train, y_val = train_test_split(\n    valid_meta_df['path'].to_numpy(),\n    valid_meta_df['label'].to_numpy(),\n    test_size=VAL_SIZE,\n    random_state=SEED, \n    stratify=valid_meta_df['label']\n)","dfd5b53d":"np.mean(y_train), np.mean(y_val)","b716bd58":"assert not set(X_train.tolist()) & set(X_val.tolist()), 'intersection is not empty'","f686fa4a":"def calculate_f1(preds, labels):\n    '''\n    Parameters:\n        preds: The predictions.\n        labels: The labels.\n\n    Returns:\n        f1 score\n    '''\n    return f1_score(labels, (np.array(preds) >= THRESHOLD).astype(np.uint8), average='micro')\n\n\ndef train_the_model(\n    model,\n    criterion,\n    optimizer,\n    scheduler,\n    epochs,\n    train_dataloader,\n    val_dataloader,\n    best_val_loss=1e7,\n):\n    '''\n    Parameters:\n        model: The model needs to be trained.\n        criterion: Loss function.\n        optimizer: The optimizer.\n        epochs: The number of epochs\n        train_dataloader: The dataloader used to generate training samples.\n        val_dataloader: The dataloader used to generate validation samples.\n        best_val_loss: The initial value of the best val loss (default: 1e7.)\n\n    Returns:\n        losses: All computed losses.\n        val_losses: All computed val_losses.\n        loglosses: All computed loglosses.\n        f1_scores: All computed f1_scores.\n        val_f1_scores: All computed val_f1_scores.\n        best_val_loss: New value of the best val loss.\n        best_model_state_dict: The state_dict of the best model.\n        best_optimizer_state_dict: The state_dict of the optimizer corresponds to the best model.\n    '''\n\n    losses = np.zeros(epochs)\n    val_losses = np.zeros(epochs)\n    f1_scores = np.zeros(epochs)\n    val_f1_scores = np.zeros(epochs)\n    best_model_state_dict = None\n    best_optimizer_state_dict = None\n\n    for i in tqdm(range(epochs)):\n        batch_losses = []\n        train_pbar = tqdm(train_dataloader)\n        train_pbar.desc = f'Epoch {i+1}'\n        classifier.train()\n\n        all_labels = []\n        all_preds = []\n\n        for i_batch, sample_batched in enumerate(train_pbar):\n            # Zero gradients\n            optimizer.zero_grad()\n            \n            # Make prediction.\n            y_pred = classifier(sample_batched['faces'].to(device))\n\n            all_labels.extend(sample_batched['label'].numpy().tolist())\n            all_preds.extend(y_pred.squeeze(dim=-1).detach().cpu().numpy().tolist())\n\n            # Compute loss.\n            loss = criterion(y_pred, sample_batched['label'].to(device))\n            batch_losses.append(loss.item())\n\n            # Perform a backward pass, and update the weights.\n            loss.backward()\n            optimizer.step()\n\n            # Display some information in progress-bar.\n            train_pbar.set_postfix({\n                'loss': batch_losses[-1]\n            })\n\n        # Compute scores.\n        f1_scores[i] = calculate_f1(all_preds, all_labels)\n\n        # Compute batch loss (average).\n        losses[i] = np.array(batch_losses).mean()\n\n\n        # Compute val loss\n        val_batch_losses = []\n        val_pbar = tqdm(val_dataloader)\n        val_pbar.desc = 'Validating'\n        classifier.eval()\n\n        all_labels = []\n        all_preds = []\n\n        for i_batch, sample_batched in enumerate(val_pbar):\n            # Make prediction.            \n            y_pred = classifier(sample_batched['faces'].to(device))\n\n            all_labels.extend(sample_batched['label'].numpy().tolist())\n            all_preds.extend(y_pred.squeeze(dim=-1).detach().cpu().numpy().tolist())\n\n            # Compute val loss.\n            val_loss = criterion(y_pred, sample_batched['label'].to(device))\n            val_batch_losses.append(val_loss.item())\n\n            # Display some information in progress-bar.\n            val_pbar.set_postfix({\n                'val_loss': val_batch_losses[-1]\n            })\n\n        # Compute val scores.\n        val_f1_scores[i] = calculate_f1(all_preds, all_labels)\n\n        val_losses[i] = np.array(val_batch_losses).mean()\n        print(f'loss: {losses[i]} | val loss: {val_losses[i]} | f1: {f1_scores[i]} | val f1: {val_f1_scores[i]}')\n        \n        # step of lr scheduler\n        scheduler.step(val_losses[i])\n        \n        # Update the best values\n        if val_losses[i] < best_val_loss:\n            best_val_loss = val_losses[i]\n            \n            print('Found a better checkpoint!')\n            best_model_state_dict = classifier.state_dict()\n            best_optimizer_state_dict = optimizer.state_dict()\n            state = {\n                'state_dict': best_model_state_dict,\n                'warmup_optimizer': best_optimizer_state_dict,\n                'best_val_loss': best_val_loss,\n            }\n            torch.save(state, 'best-checkout.pth')\n            \n    return losses, val_losses, f1_scores, val_f1_scores, best_val_loss, best_model_state_dict, best_optimizer_state_dict\n\n\ndef visualize_results(\n    losses,\n    val_losses,\n    f1_scores,\n    val_f1_scores\n):\n    '''\n    Parameters:\n        losses: A list of losses.\n        val_losses: A list of val losses.\n        f1_scores: A list of f1 scores.\n        val_f1_scores: A list of val f1 scores.\n    '''\n\n    fig = plt.figure(figsize=(16, 8))\n    ax = fig.add_axes([0, 0, 1, 1])\n\n    ax.plot(np.arange(1, len(losses) + 1), losses)\n    ax.plot(np.arange(1, len(val_losses) + 1), val_losses)\n    ax.set_xlabel('epoch', fontsize='xx-large')\n    ax.set_ylabel('loss', fontsize='xx-large')\n    ax.legend(\n        ['loss', 'val loss'],\n        loc='upper right',\n        fontsize='xx-large',\n        shadow=True\n    )\n    plt.show()\n\n    fig = plt.figure(figsize=(16, 8))\n    ax = fig.add_axes([0, 0, 1, 1])\n\n    ax.plot(np.arange(1, len(f1_scores) + 1), f1_scores)\n    ax.plot(np.arange(1, len(val_f1_scores) + 1), val_f1_scores)\n    ax.set_xlabel('epoch', fontsize='xx-large')\n    ax.set_ylabel('f1 score', fontsize='xx-large')\n    ax.legend(\n        ['f1', 'val f1'],\n        loc='upper left',\n        fontsize='xx-large',\n        shadow=True\n    )\n    plt.show()","f76dd7fc":"class FaceDataset(Dataset):\n    def __init__(self, img_dirs, labels, n_faces=1, preprocess=None):\n        self.img_dirs = img_dirs\n        self.labels = labels\n        self.n_faces = n_faces\n        self.preprocess = preprocess\n\n    def __len__(self):\n        return len(self.img_dirs)\n    \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_dir = self.img_dirs[idx]\n        label = self.labels[idx]\n        face_paths = glob.glob(f'{img_dir}\/*.png')\n\n        if len(face_paths) >= self.n_faces:\n            sample = sorted(np.random.choice(face_paths, self.n_faces, replace=False))\n        else:\n            sample = sorted(np.random.choice(face_paths, self.n_faces, replace=True))\n            \n        faces = []\n        for face_path in sample:\n            face = cv2.imread(face_path, 1)\n            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n            faces.append(face)\n            \n        if self.preprocess is not None:\n            d = {f'image{i-1}': faces[i] for i in range(1, self.n_faces)}\n            d['image'] = faces[0]\n            faces = list(self.preprocess(**d).values())\n\n        return {'faces': torch.stack(faces).permute(1, 0, 2, 3), 'label': torch.tensor([label], dtype=torch.float)}#{'faces': np.concatenate(faces, axis=-1).transpose(2, 0, 1), 'label': np.array([label], dtype=float)}","72088884":"train_transforms = Compose([\n    Resize(H+DELTA, W+DELTA),\n    # Downscale(scale_min=0.5, scale_max=0.9, p=0.3),\n    RandomCrop(H, W),\n    HorizontalFlip(p=0.5),\n    RandomBrightnessContrast(brightness_limit=0, contrast_limit=0.2, p=0.3),\n    HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.3),\n    GaussianBlur(blur_limit=(3, 7), p=0.3),\n    Normalize(mean=MEAN, std=STD, p=1),\n    ToTensorV2()\n], additional_targets={f'image{i}': 'image' for i in range(0, N_FACES-1)})\n\nval_transforms = Compose([\n    Resize(H+DELTA, W+DELTA),\n    CenterCrop(H, W),\n    Normalize(mean=MEAN, std=STD, p=1),\n    ToTensorV2()\n], additional_targets={f'image{i}': 'image' for i in range(0, N_FACES-1)})\n\ntest_transforms = Compose([\n    Resize(H+DELTA, W+DELTA),\n    CenterCrop(H, W),\n    Normalize(mean=MEAN, std=STD, p=1),\n    ToTensorV2()\n], additional_targets={f'image{i}': 'image' for i in range(0, N_FACES-1)})","9f44762b":"train_dataset = FaceDataset(\n    img_dirs=X_train,\n    labels=y_train,\n    n_faces=N_FACES,\n    preprocess=train_transforms\n)\nval_dataset = FaceDataset(\n    img_dirs=X_val,\n    labels=y_val,\n    n_faces=N_FACES,\n    preprocess=val_transforms\n)\ntest_dataset = FaceDataset(\n    img_dirs=X_test['path'].values,\n    labels=[0]*len(X_test['path']),\n    n_faces=N_FACES,\n    preprocess=test_transforms\n)\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=NUM_WORKERS,\n#     generator=torch.Generator(device='cuda'),\n#     num_workers=0,\n#     pin_memory=False,\n)\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n#     generator=torch.Generator(device='cuda'),\n#     num_workers=0,\n#     pin_memory=False,\n)\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n#     generator=torch.Generator(device='cuda'),\n#     num_workers=0,\n#     pin_memory=False,\n)","cad5f048":"plt.figure(figsize=(10,10))\nfor ii,img in enumerate(next(iter(train_dataloader))['faces'][0].permute(1, 0, 2, 3)):\n    plt.subplot(2,2,ii+1)\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = img.numpy().transpose((1, 2, 0))\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if ii == 3:\n        break","80f45ba3":"next(iter(train_dataloader))['faces'].shape","fcd77cf2":"class DeepfakeClassifierResnet(nn.Module):\n    def __init__(self, encoder, in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False, linear_size=2048, num_classes=1):\n        super(DeepfakeClassifierResnet, self).__init__()\n        self.encoder = encoder\n        \n        # Modify input layer.\n        self.encoder.conv1 = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            bias=bias,\n        )\n        \n        self.encoder.fc = nn.Linear(linear_size * 1, num_classes)\n\n    def forward(self, x):\n        return torch.sigmoid(self.encoder(x))\n    \n    def freeze_all_layers(self):\n        for param in self.encoder.parameters():\n            param.requires_grad = False\n\n    def freeze_middle_layers(self):\n        self.freeze_all_layers()\n        \n        for param in self.encoder.conv1.parameters():\n            param.requires_grad = True\n            \n        for param in self.encoder.fc.parameters():\n            param.requires_grad = True\n\n    def unfreeze_all_layers(self):\n        for param in self.encoder.parameters():\n            param.requires_grad = True","ce0061c2":"class DeepfakeClassifierInception(nn.Module):\n    def __init__(self, encoder, in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=3, bias=False, linear_size=512, num_classes=1):\n        super(DeepfakeClassifierInception, self).__init__()\n        self.encoder = encoder\n        \n        # Modify input layer.\n        self.encoder.conv2d_1a.conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            bias=bias,\n        )\n        \n        self.encoder.logits = nn.Linear(linear_size * 1, num_classes)\n\n    def forward(self, x):\n        return torch.sigmoid(self.encoder(x))\n    \n    def freeze_all_layers(self):\n        for param in self.encoder.parameters():\n            param.requires_grad = False\n\n    def freeze_middle_layers(self):\n        self.freeze_all_layers()\n        \n        for param in self.encoder.conv2d_1a.conv.parameters():\n            param.requires_grad = True\n            \n        for param in self.encoder.logits.parameters():\n            param.requires_grad = True\n\n    def unfreeze_all_layers(self):\n        for param in self.encoder.parameters():\n            param.requires_grad = True","b24283a3":"class DeepfakeClassifierR3D18(nn.Module):\n    def __init__(self, encoder, linear_size=512, num_classes=1):\n        super(DeepfakeClassifierR3D18, self).__init__()\n        self.encoder = encoder\n        \n        # Modify output layer.\n        num_features = self.encoder.fc.in_features\n        self.encoder.fc = nn.Linear(num_features, num_classes)\n\n    def forward(self, x):\n        return torch.sigmoid(self.encoder(x))\n    \n    def freeze_all_layers(self):\n        for param in self.encoder.parameters():\n            param.requires_grad = False\n\n    def freeze_middle_layers(self):\n        self.freeze_all_layers()\n            \n        for param in self.encoder.fc.parameters():\n            param.requires_grad = True\n\n    def unfreeze_all_layers(self):\n        for param in self.encoder.parameters():\n            param.requires_grad = True","1ae9c731":"class FocalLoss(nn.Module):\n    def __init__(self, gamma=2, sample_weight=None):\n        super().__init__()\n        self.gamma = gamma\n        self.sample_weight = sample_weight\n\n    def forward(self, logit, target):\n        target = target.float()\n        max_val = (-logit).clamp(min=0)\n        loss = logit - logit * target + max_val + \\\n               ((-max_val).exp() + (-logit - max_val).exp()).log()\n\n        invprobs = F.logsigmoid(-logit * (target * 2.0 - 1.0))\n        loss = (invprobs * self.gamma).exp() * loss\n        if len(loss.size())==2:\n            loss = loss.sum(dim=1)\n        if self.sample_weight is not None:\n            loss = loss * self.sample_weight\n        return loss.mean()","b09ff39a":"# efficient_transformer = Linformer(\n#     dim=128,\n#     seq_len=49+1,  # 7x7 patches + 1 cls-token\n#     depth=12,\n#     heads=8,\n#     k=64\n# )\n\n# classifier = ViT(\n#     dim=128,\n#     image_size=224,\n#     patch_size=32,\n#     num_classes=1,\n#     transformer=efficient_transformer,\n#     channels=3,\n# ).to(device)\n# classifier.train()","1cc1208d":"encoder_r3d_18 = models.video.r3d_18(\n    pretrained=True,\n)\n\nclassifier = DeepfakeClassifierR3D18(encoder=encoder_r3d_18, linear_size=512, num_classes=1)\n\nclassifier.to(device);\nclassifier.train();","7c87359c":"# x = torch.zeros(1, 3, N_FACES, H, W)\n# y= classifier(x)\n# print(y.shape)","e3aed07b":"# encoder_facenet = InceptionResnetV1(\n#     classify=True,\n#     pretrained='casia-webface',\n#     num_classes=1\n# )\n\n# classifier = DeepfakeClassifierInception(encoder=encoder_facenet, in_channels=3*N_FACES, num_classes=1)\n\n# classifier.to(device);\n# classifier.train();","b863e61b":"# encoder_resnet = resnet101(pretrained=True)\n\n# classifier = DeepfakeClassifierResnet(encoder=encoder_resnet, in_channels=3*N_FACES, num_classes=1)\n\n# classifier.to(device);\n# classifier.train();","205b39f3":"criterion = nn.BCELoss()#FocalLoss()","f9dcba7d":"losses = np.zeros(WARM_UP_EPOCHS + FINE_TUNE_EPOCHS)\nval_losses = np.zeros(WARM_UP_EPOCHS + FINE_TUNE_EPOCHS)\nf1_scores = np.zeros(WARM_UP_EPOCHS + FINE_TUNE_EPOCHS)\nval_f1_scores = np.zeros(WARM_UP_EPOCHS + FINE_TUNE_EPOCHS)\n\nbest_val_loss = 1e7","571c1d6d":"classifier.freeze_middle_layers()\nwarmup_optimizer = optim.Adam(filter(lambda p: p.requires_grad, classifier.parameters()), lr=WARM_UP_LR)\nwarmup_scheduler = ReduceLROnPlateau(warmup_optimizer, mode='min', factor=0.1, patience=5, threshold=0.0001, threshold_mode='abs', verbose=True)","7d7eb03f":"summary(classifier, input_size=(3, N_FACES, H, W))","c6080c4c":"losses[:WARM_UP_EPOCHS], val_losses[:WARM_UP_EPOCHS], \\\nf1_scores[:WARM_UP_EPOCHS], val_f1_scores[:WARM_UP_EPOCHS], \\\nbest_val_loss, \\\nbest_model_state_dict, best_optimizer_state_dict \\\n= train_the_model(\n    model=classifier,\n    criterion=criterion,\n    optimizer=warmup_optimizer,\n    scheduler=warmup_scheduler,\n    epochs=WARM_UP_EPOCHS,\n    train_dataloader=train_dataloader,\n    val_dataloader=val_dataloader,\n    best_val_loss=best_val_loss,\n)\n\n# Save the best checkpoint.\nif best_model_state_dict is not None:\n    state = {\n        'state_dict': best_model_state_dict,\n        'warmup_optimizer': best_optimizer_state_dict,\n        'best_val_loss': best_val_loss,\n    }\n    torch.save(state, 'best-checkout-warmup.pth')","fd6d7783":"visualize_results(\n    losses=losses[:WARM_UP_EPOCHS],\n    val_losses=val_losses[:WARM_UP_EPOCHS],\n    f1_scores=f1_scores[:WARM_UP_EPOCHS],\n    val_f1_scores=val_f1_scores[:WARM_UP_EPOCHS]\n)","39aed411":"# state = torch.load(PATH2PROJECT \/ 'trainreface' \/ 'temp-best-checkout-resnet101.pth', map_location=lambda storage, loc: storage)\nstate = torch.load('best-checkout-warmup.pth', map_location=lambda storage, loc: storage)\n# state = torch.load('best-checkout.pth', map_location=lambda storage, loc: storage)\nbest_val_loss = state['best_val_loss']\nclassifier.load_state_dict(state['state_dict'])\n# classifier.to(device)","f4420ede":"classifier.unfreeze_all_layers()\nfinetune_optimizer = optim.Adam(filter(lambda p: p.requires_grad, classifier.parameters()), lr=FINE_TUNE_LR)\nfinetune_scheduler = ReduceLROnPlateau(finetune_optimizer, mode='min', factor=0.1, patience=5, threshold=0.0001, threshold_mode='abs', verbose=True)","60b59641":"# losses[WARM_UP_EPOCHS:WARM_UP_EPOCHS+FINE_TUNE_EPOCHS], val_losses[WARM_UP_EPOCHS:WARM_UP_EPOCHS+FINE_TUNE_EPOCHS], \\\n# f1_scores[WARM_UP_EPOCHS:WARM_UP_EPOCHS+FINE_TUNE_EPOCHS], val_f1_scores[WARM_UP_EPOCHS:WARM_UP_EPOCHS+FINE_TUNE_EPOCHS], \\\n# best_val_loss, best_val_logloss, \\\n# best_model_state_dict, best_optimizer_state_dict \\\n_, _, \\\n_, _, \\\nbest_val_loss, \\\nbest_model_state_dict, best_optimizer_state_dict \\\n= train_the_model(\n    model=classifier,\n    criterion=criterion,\n    optimizer=finetune_optimizer,\n    scheduler=finetune_scheduler,\n    epochs=FINE_TUNE_EPOCHS,\n    train_dataloader=train_dataloader,\n    val_dataloader=val_dataloader,\n    best_val_loss=best_val_loss,\n)\n\n# Save the best checkpoint.\nif best_model_state_dict is not None:\n    state = {\n        'state_dict': best_model_state_dict,\n        'finetune_optimizer': best_optimizer_state_dict,\n        'best_val_loss': best_val_loss,\n    }\n\n    torch.save(state, 'best-checkout-finetune.pth')","481189c0":"# state = torch.load(PATH2PROJECT \/ 'trainreface' \/ 'temp-best-checkout-resnet101.pth', map_location=lambda storage, loc: storage)\nstate = torch.load('best-checkout-finetune.pth', map_location=lambda storage, loc: storage)\n# state = torch.load('best-checkout.pth', map_location=lambda storage, loc: storage)\nbest_val_loss = state['best_val_loss']\nclassifier.load_state_dict(state['state_dict'])\n# classifier.to(device)","cc907961":"# from IPython.display import FileLink\n# FileLink('best-checkout.pth')","d679a605":"# visualize_results(\n#     losses=losses,\n#     val_losses=val_losses,\n#     loglosses=loglosses,\n#     val_loglosses=val_loglosses,\n#     f1_scores=f1_scores,\n#     val_f1_scores=val_f1_scores\n# )","776357c6":"def inference(classifier, test_dataloader):\n    classifier.eval()\n    \n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for _, batch in enumerate(tqdm(test_dataloader, total=len(test_dataloader))):\n            # Make prediction.\n            y_pred = classifier(batch['faces'].to(device))\n\n            all_preds.extend(y_pred.squeeze(dim=-1).detach().cpu().numpy().tolist())\n            all_labels.extend(batch['label'].squeeze(dim=-1).numpy().tolist())\n    return all_preds, all_labels","20e0adf2":"test_prediction, _ = inference(classifier, test_dataloader)\nlen(test_prediction)","8a999ddb":"val_prediction, val_labels = inference(classifier, val_dataloader)\nlen(val_prediction)","bcff6977":"X_test['score'] = test_prediction","9965970e":"thresholds = np.linspace(0, 1, len(np.unique(val_prediction)))\nf1_scores = [f1_score(val_labels, (np.array(val_prediction) > t).astype(np.uint8), average='micro') for t in tqdm(thresholds)]\nt_best = thresholds[np.argmax(f1_scores)]\nprint('Best threshold: ', t_best)\nprint('Best F1-Score: ', np.max(f1_scores))","61549b8a":"from IPython.display import FileLink","e315bd5b":"X_test['label'] = (X_test['score'] > t_best).astype(int)\nsubmission_result = submission[['filename', 'path']].merge(X_test[['path', 'label']], on='path', how='left').fillna(0)[['filename', 'label']]\nsubmission_result['label'] = submission_result['label'].astype(int)\n\nassert submission_result.shape[0] == submission.shape[0]\n\nsubmission_result.to_csv('submission_result_th.csv', index=False)\n\nFileLink('submission_result_th.csv')","406bbff9":"X_test['label'] = (X_test['score'] > 0.5).astype(int)\nsubmission_result = submission[['filename', 'path']].merge(X_test[['path', 'label']], on='path', how='left').fillna(0)[['filename', 'label']]\nsubmission_result['label'] = submission_result['label'].astype(int)\n\nassert submission_result.shape[0] == submission.shape[0]\n\nsubmission_result.to_csv('submission_result_05.csv', index=False)\n\nFileLink('submission_result_05.csv')","ba84fcc6":"!cp submission_result_th.csv \/content\/drive\/MyDrive\/dl-creator-school\/submission_result_th.csv","fae265e3":"!cp submission_result_05.csv \/content\/drive\/MyDrive\/dl-creator-school\/submission_result_05.csv","04a752a7":"!cp best-checkout-finetune.pth \/content\/drive\/MyDrive\/dl-creator-school\/best-checkout-finetune.pth","981c6d3d":"### Remove corrupt videos or ones in what cannot detect any faces","1261312d":"## Install external modules and load our data","94624006":"## Training metadata","819c8b98":"## Dataset and Dataloaders","1c2fab12":"## Clean data","d721c8a9":"**Task [[kaggle](https:\/\/www.kaggle.com\/c\/reface-fake-detection)]:** recognize fake videos. You need to train the binary classifier to distinguish real videos from fake ones (the provided fake data is the result of the technologies developed in Reface).\n\n****\n\n### What I should get?\n\nIn order to complete this stage, you should meet one of 2 conditions below:\n+ either make a solution with a minimum target metric value of 0.92475\n+ or be in the top 30 of all competitors.\n\n****\n\n### Evaluation\n\nThe evaluation metric for this competition is F1-Score, average='micro'. The F1 score, commonly used in information retrieval, measures accuracy using the statistics precision p and recall r. Precision is the ratio of true positives (tp) to all predicted positives (tp + fp). Recall is the ratio of true positives to all actual positives (tp + fn).\n\nThe F1 metric weights recall and precision equally, and a good retrieval algorithm will maximize both precision and recall simultaneously. Thus, moderately good performance on both will be favored over extremely good performance on one and poor performance on the other.\n\nMore information you can find at sklearn docs:\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html\n\n****\n\n### Submission\n\nFor each filename in the test set, you must predict either this file is fake video (label 1) or this file is real video (label 0). The file should contain a header and have the following format:\n\n```\nfilename,label\n004582.mp4,1\n003603.mp4,0\n```","d185890f":"## Define training hyperparameters","d99af8e0":"### Remove videos in which do not have enough faces","6d1dc8bd":"## Stratified split data on test and validation","ecc58aa0":"## Models","d3caa82d":"## Helper functions","c4c12a9a":"## Modules importing","c1abb162":"## Training","9a37f64e":"## Settings","7a15593b":"## Inference and submission","7673216b":"## Problem"}}