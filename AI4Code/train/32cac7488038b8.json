{"cell_type":{"a1428c3e":"code","f43bb223":"code","687f2e9c":"code","0c4ecc17":"code","52c5fd44":"code","f106ef9e":"code","de5dbbb7":"code","c0470dd5":"code","d400188e":"code","9ed8ed3f":"code","3e12c2a4":"code","b05404d5":"code","2ac4fdac":"code","efb3e82e":"code","cf96c9b9":"markdown","c986405c":"markdown","707e4ed4":"markdown","6cf504b2":"markdown","abde5ae3":"markdown","636af044":"markdown","5d46bc98":"markdown","5934b842":"markdown","4545a485":"markdown","b3d5d329":"markdown","56442de9":"markdown","4b7e9398":"markdown","3b6a7b1f":"markdown","7dbf73f2":"markdown","f0b0cd01":"markdown","88b1602a":"markdown","48037015":"markdown","5b211ba8":"markdown","0c50e4bf":"markdown","8bcffc92":"markdown"},"source":{"a1428c3e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))","f43bb223":"import nltk","687f2e9c":"from nltk.corpus import product_reviews_1","0c4ecc17":"product_reviews_1.fileids()","52c5fd44":"# Will read raw text from this file\nproduct_review_raw = product_reviews_1.raw('Apex_AD2600_Progressive_scan_DVD player.txt')\nproduct_review_raw[:750] \n#We are setting upper limit otherwise it will product the big output with lots of scrolling ","f106ef9e":"# Will break down file in sentences\nproduct_review_sents = product_reviews_1.sents('Apex_AD2600_Progressive_scan_DVD player.txt')\nproduct_review_sents","de5dbbb7":"# Will break down file in words\nproduct_review_words = product_reviews_1.words('Apex_AD2600_Progressive_scan_DVD player.txt')\nproduct_review_words","c0470dd5":"from nltk.corpus import stopwords\nstoplist = stopwords.words('english')\nprint(stoplist)","d400188e":"print(f'word length with stopwords {len(product_review_words)}')\nproduct_review_wo_stopwords = [word for word in product_review_words if not word in stoplist]\nprint(f'word length without stopwords {len(product_review_wo_stopwords)}')","9ed8ed3f":"from nltk.tokenize import sent_tokenize, word_tokenize\n\nprint(f'Word Tokens - \\n{sent_tokenize(product_review_raw[750:1250])}\\n\\n\\n')\nprint(f'Sentence Tokens - \\n{word_tokenize(product_review_raw[750:1250])}')","3e12c2a4":"from nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nsample_sentence = 'A middle-aged woman entered the room, her hands full of hamburger meat as she formed a patty'\nporter_stemmer = PorterStemmer()\nword_lemmatizer = WordNetLemmatizer()\n\nfor w in word_tokenize(sample_sentence):\n    print(f'Actual Word - {w}')\n    print(f'Stem - {porter_stemmer.stem(w)}')\n    print(f'Lemma - {word_lemmatizer.lemmatize(w)}\\n')","b05404d5":"sample_sentence_words = word_tokenize(sample_sentence)\nnltk.pos_tag(sample_sentence_words)","2ac4fdac":"from sklearn.feature_extraction.text import CountVectorizer\n# initialize sample document\nsample_documents = ['Must have a subject and a verb','Must express a complete thought','Must only have one clause']\n# instantiate\nvectorizer = CountVectorizer()\nvectorizer.fit(sample_documents)\n# summarize\nprint(f':: vector vocabulary - {vectorizer.vocabulary_}\\n')\n# encode document\nvector = vectorizer.transform(sample_documents)\n# summarize encoded vector\nprint(f':: vector shape - {vector.shape}\\n')\nprint(f':: vector list - {vector.toarray()}')","efb3e82e":"from sklearn.feature_extraction.text import TfidfVectorizer\n# initialize sample document\nsample_documents = ['Must have a subject and a verb','Must express a complete thought','Must only have one clause']\n# instantiate\nvectorizer = TfidfVectorizer()\nvectorizer.fit(sample_documents)\n# summarize\nprint(f':: vector vocabulary - {vectorizer.vocabulary_}\\n')\n# encode document\nvector = vectorizer.transform(sample_documents)\n# summarize encoded vector\nprint(f':: vector shape - {vector.shape}\\n')\nprint(f':: vector list - {vector.toarray()}')","cf96c9b9":"So if you cross check with our calculated vector list then you will get that both are same, just position is different because of key positions in the dictionary, CountVectorizer lists keys in the dictionary in alphabetical order. \n\nThis approach is very basic, but has some limitations like it gives importance to words on the basis of their occurrence count, mostly resulting in higher importance to most common and un-important words like 'the', 'is', 'and' etc, so is not very preferred approach for feature extraction. This limitations is handled by TF-IDF method. ","c986405c":"Once we know the file name then we can read from that file in desired way, for eg- ","707e4ed4":"### <a id='bow'>A. Bag of Words<\/a>\nBag of words is one of the simplest approaches of feature extraction, here we simply keep the frequency count of all unique words and consider it as a feature. Example: \n\nSuppose we have below sentences (also referred as documents):\n\n> 1. Must have a subject and a verb.\n> 2. Must express a complete thought.\n> 3. Must only have one clause.\n\nFeature extraction we need to perform are:\n\n**1. Identify Unique words**\n    Unique words from all documents are:\n    **must, have, a, subject, and, verb, express, complete, thought, only, one, clause**\n    \n**2. Perform Vectorization**\n    we need to find the frequency count of each unique word and if it is not there then we need to put 0. For eg vector for first document can be formed as: \n\n> must - 1 <br\/>\n> have - 1 <br\/>\n> subject - 1 <br\/>\n> and - 1 <br\/>\n> verb - 1 <br\/>\n> express - 0 <br\/>\n> complete - 0 <br\/>\n> thought - 0 <br\/>\n> only - 0 <br\/>\n> one - 0 <br\/>\n> clause - 0 <br\/>\n\nSo, it will become\n> 1. [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n\nsimilar way doucument2 and document3 will become:\n> 2. [1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0]\n> 3. [1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n\nscikit-learn library provides CountVectorizer class to perform this action","6cf504b2":"### <a id='stopwords'>B. Stopwords<\/a>\n\nStopwords are extra words that don't have any useful meaning they are there just for the sake of sentence formation. They are not really helpful because they can't be categorized, so in NLP projects we prefer their elimination.","abde5ae3":"**Interpretation: **\n\ndictionary - \n> {'and': 0, 'clause': 1, 'complete': 2, 'express': 3, 'have': 4, 'must': 5, 'one': 6, 'only': 7, 'subject': 8, 'thought': 9, 'verb': 10} <br\/>\n\ndocument 1 - \n> 'Must have a subject and a verb'<br\/>\n\nvector - \n> [0.50461134  0.  0.  0.  0.38376993  0.29803159  0.  0.  0.50461134  0.  0.50461134]\n\n> and - 0.50461134<br\/>\n> clause - 0.<br\/>\n> complete - 0. <br\/>\n> express - 0.<br\/>\n> have - 0.38376993<br\/>\n> must - 0.29803159<br\/>\n> one - 0. <br\/>\n> only - 0. <br\/>\n> subject - 0.50461134<br\/>\n> thought - 0. <br\/>\n> verb - 0.50461134<br\/>\n\nExcluding 0, 'must' have the lowest weight because it is most frequent in all documents and this is what IDF does.","636af044":"We had so many stopwords, so it is somewhat useful to eliminate stopwords before performing any actual NLP operation.","5d46bc98":"All these concepts can be grasped better while working on the actual problem. This is it for now ;)","5934b842":"Let's check the difference between product_reviews_1 length with or without stop_words","4545a485":"There are many libraries out there like NLTK, TextBlob, SpaCy, Pattern etc that we can use.But, here we are going to prefer NLTK since it is used most commonly and will be good to start with, once we get the grasp of all fundamental operations, we can explore and understand the significance of other libraries too.","b3d5d329":"### <a id='tf-idf'>B. Term Frequency \u2013 Inverse Document Frequency (TF \u2013 IDF)<\/a>\n\nIt is the most popular method to perform feature extraction. To understand better let's understand TF and IDF separately.\n\n**Term Frequency: **Simply finds out the frequency of a word in document.<br\/>\n**Inverse Document Frequency:** Assigns a lower weight to the words which appear most frequently. It basically depicts the rarity of the word in all documents.\n\n![](https:\/\/mungingdata.files.wordpress.com\/2017\/11\/equation.png?w=430&h=336) <br\/>\nSimilar to CountVectorizer, we can import TfidfVectorizer class from scikit-learn library.","56442de9":"### <a id='tokenization'>C. Tokenization<\/a>\nA 'Token' is nothing but a single entity of whole entity we are referreing to. We can perform sentence and word split in below way:","4b7e9398":"**Steps -** \n\n1. <a href='#import-libs' target='_self'>Importing Libraries<\/a>\n1. <a href='#preprocessing' target='_self'>Basics (Preprocessing)<\/a>\n    1. <a href='#corpora' target='_self'>NLTK Corpora<\/a>\n    1. <a href='#stopwords' target='_self'>Stopwords<\/a>\n    1. <a href='#tokenization' target='_self'>Tokenization<\/a>\n    1. <a href='#stem-lemma' target='_self'>Stemming & Lemmatization<\/a>\n    1. <a href='#post' target='_self'>Part of Speech Tagging<\/a>\n1. <a href='#feature-extraction' target='_self'>Feature Extraction (Vectorization)<\/a>\n    1. <a href='#bow' target='_self'>Bag of Words<\/a>\n    1. <a href='#tf-idf' target='_self'>TF-IDF<\/a>","3b6a7b1f":"### <a id='corpora'>A. NLTK Corpora<\/a>\nOne of the best thing about NLTK is that it provides many sample text datasets (Corpora) where each dataset is called Corpus; we can directly import any desired dataset directly from NLTK. Here we are going to import product_reviews data set but you can pick any of the available dataset from http:\/\/www.nltk.org\/nltk_data\/","7dbf73f2":"## <a id='import-libs'>1. Importing Libraries<\/a>","f0b0cd01":"## <a id='feature-extraction'>3. Feature Extraction<\/a>\nWe can not use text directly to train our models. We need to convert it in the form of features, only then it can be used to train any model for desired outcome and we know very well that most of the models respond to the numeric features very well. So we need to bring all these text representations in the form of numbers.\n\nThere are two popular approaches to extract features from texts: \n1. Count the number of occurrece of each word in a document. \n2. Calculate the frequency of each word occurrence out of all word in a document.\n\nFew most commonly used techniqus to perform feature extraction are:<br\/>\n**1. Bag of Words**<br\/>\n**2. TF-IDF (Term Frequency - Inverse Document Frequency)**","88b1602a":"**Natural Language Processing (NLP)** is one of the fastest growing parts of Artificial intelligence. One must have a good command over NLP to process text-based data sets. I recently started on this and after doing some research got to know that below concepts needs to be understood very well before starting a journey on advance NLP computations. Here we will only focus on text preprocessing and feature extraction and later will solve some interesting problem using same.","48037015":"### <a id='stem-lemma'>D. Stemming and Lemmatization<\/a>\nThey both are used for text normalization. Stemming basically removes the redundancy by bringing everything in its simple form for example 'dancing' & 'dancer' becomes 'dance' in this. On the other hand, Lemmatization does the morphological analysis and keeps part of speech into consideration. This can be better understood by examples :\n\nLet's consider below sentence and perform <br\/>\n**Because I had to catch the train, and as we were short on time, I forgot to pack my toothbrush for our vacation.**","5b211ba8":"## <a id='preprocessing'>2. Basics (Preprocessing)<\/a>","0c50e4bf":"### <a id='post'>E. Part of Speech Tagging<\/a>\nAlso know as POS Taggin or POST. Why POST requried ? \nBecause same sentence or paragraph can have the same word in different grammatically contexts and it is not a good idea to consider the second occurrence as redundancy, so as a solution we prefer tagging each word with its Part of Speech to make it grammatically unique. Consider below example, here all **above** words are not grammatically same. \n\n1. The heavens are **above**. (Adverb)\n\n2. The moral code of conduct is **above** the civil code of conduct. (Proposition)\n\n3. Our blessings come from **above**. (Noun)","8bcffc92":"Each dataset contains text in text files and to read any file we need to know its name."}}