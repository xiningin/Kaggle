{"cell_type":{"f6435e72":"code","976a6be1":"code","79771540":"code","18899a33":"code","e1bb14e0":"code","9836d0f2":"code","ae0c5bab":"code","dbe07dd3":"code","9feba83c":"code","0762d1a3":"code","68dafe80":"code","70f3420e":"code","d8fe3ddb":"code","7d06fbf4":"code","501e0254":"code","fdffadf6":"code","0149c23d":"code","de03f210":"code","1380b370":"code","4d473a56":"code","2bb3ee44":"code","70fc6fd5":"code","f1da7c40":"code","32206a6e":"code","103350ab":"code","f1a59ef1":"code","71a68fc1":"code","3cf48791":"code","cfabdbda":"code","3b1a1623":"markdown"},"source":{"f6435e72":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","976a6be1":"import torch\nimport math\nfrom os import listdir\nimport numpy as np\nfrom torch.autograd import Variable","79771540":"from PIL import Image\nfrom os.path import join","18899a33":"from torchvision.transforms import Compose, RandomCrop, ToTensor, ToPILImage, CenterCrop, Resize","e1bb14e0":"from torch.utils.data import DataLoader, Dataset","9836d0f2":"torch.autograd.set_detect_anomaly(True)","ae0c5bab":"def is_image_file(filename):\n    return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])\n\n\ndef calculate_valid_crop_size(crop_size, upscale_factor):\n    return crop_size - (crop_size % upscale_factor)\n\n\ndef train_hr_transform(crop_size):\n    return Compose([\n        RandomCrop(crop_size),\n        ToTensor(),\n    ])\n\n\ndef train_lr_transform(crop_size, upscale_factor):\n    return Compose([\n        ToPILImage(),\n        Resize(crop_size \/\/ upscale_factor, interpolation=Image.BICUBIC),\n        ToTensor()\n    ])\n\n\ndef display_transform():\n    return Compose([\n        ToPILImage(),\n        Resize(400),\n        CenterCrop(400),\n        ToTensor()\n    ])","dbe07dd3":"class TrainDatasetFromFolder(Dataset):\n    def __init__(self, dataset_dir, crop_size, upscale_factor):\n        super(TrainDatasetFromFolder, self).__init__()\n        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]\n        crop_size = calculate_valid_crop_size(crop_size, upscale_factor)\n        self.hr_transform = train_hr_transform(crop_size)\n        self.lr_transform = train_lr_transform(crop_size, upscale_factor)\n\n    def __getitem__(self, index):\n        hr_image = self.hr_transform(Image.open(self.image_filenames[index]))\n        lr_image = self.lr_transform(hr_image)\n        return lr_image, hr_image\n\n    def __len__(self):\n        return len(self.image_filenames)","9feba83c":"UPSCALE_FACTOR = 4\nCROP_SIZE = 88","0762d1a3":"mean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])","68dafe80":"train_set = TrainDatasetFromFolder('\/kaggle\/input\/div2k\/DIV2K_train_HR\/DIV2K_train_HR', crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)\n# val_set = ValDatasetFromFolder('DIV2K_valid_HR', upscale_factor=UPSCALE_FACTOR)\ntrain_loader = DataLoader(dataset=train_set, num_workers=4, batch_size=64, shuffle=True)\n# val_loader = DataLoader(dataset=val_set, num_workers=4, batch_size=1, shuffle=False)","70f3420e":"import matplotlib.pyplot as plt\n%matplotlib inline","d8fe3ddb":"from torch import nn, optim","7d06fbf4":"class ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.prelu = nn.PReLU()\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(channels)\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = self.bn1(residual)\n        residual = self.prelu(residual)\n        residual = self.conv2(residual)\n        residual = self.bn2(residual)\n\n        return x + residual","501e0254":"class UpsampleBLock(nn.Module):\n    def __init__(self, in_channels, up_scale):\n        super(UpsampleBLock, self).__init__()\n        self.conv = nn.Conv2d(in_channels, in_channels * up_scale ** 2, kernel_size=3, padding=1)\n        self.pixel_shuffle = nn.PixelShuffle(up_scale)\n        self.prelu = nn.PReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.pixel_shuffle(x)\n        x = self.prelu(x)\n        return x","fdffadf6":"class Generator(nn.Module):\n    def __init__(self, scale_factor):\n        upsample_block_num = int(math.log(scale_factor, 2))\n\n        super(Generator, self).__init__()\n        self.block1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=9, padding=4),\n            nn.PReLU()\n        )\n        self.block2 = ResidualBlock(64)\n        self.block3 = ResidualBlock(64)\n        self.block4 = ResidualBlock(64)\n        self.block5 = ResidualBlock(64)\n        self.block6 = ResidualBlock(64)\n        self.block7 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64)\n        )\n        block8 = [UpsampleBLock(64, 2) for _ in range(upsample_block_num)]\n        block8.append(nn.Conv2d(64, 3, kernel_size=9, padding=4))\n        self.block8 = nn.Sequential(*block8)\n\n    def forward(self, x):\n        block1 = self.block1(x)\n        block2 = self.block2(block1)\n        block3 = self.block3(block2)\n        block4 = self.block4(block3)\n        block5 = self.block5(block4)\n        block6 = self.block6(block5)\n        block7 = self.block7(block6)\n        block8 = self.block8(block1 + block7)\n\n        return (torch.tanh(block8) + 1) \/ 2","0149c23d":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2),\n\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(512, 1024, kernel_size=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(1024, 1, kernel_size=1)\n        )\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        return torch.sigmoid(self.net(x).view(batch_size))","de03f210":"from torchvision.models.vgg import vgg16","1380b370":"class GeneratorLoss(nn.Module):\n    def __init__(self):\n        super(GeneratorLoss, self).__init__()\n        vgg = vgg16(pretrained=True)\n        loss_network = nn.Sequential(*list(vgg.features)[:31]).eval()\n        for param in loss_network.parameters():\n            param.requires_grad = False\n        self.loss_network = loss_network\n        self.mse_loss = nn.MSELoss()\n        self.tv_loss = TVLoss()\n\n    def forward(self, out_labels, out_images, target_images):\n        # Adversarial Loss\n        adversarial_loss = torch.mean(1 - out_labels)\n        # Perception Loss\n        perception_loss = self.mse_loss(self.loss_network(out_images), self.loss_network(target_images))\n        # Image Loss\n        image_loss = self.mse_loss(out_images, target_images)\n        # TV Loss\n        tv_loss = self.tv_loss(out_images)\n        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss + 2e-8 * tv_loss\n","4d473a56":"class TVLoss(nn.Module):\n    def __init__(self, tv_loss_weight=1):\n        super(TVLoss, self).__init__()\n        self.tv_loss_weight = tv_loss_weight\n\n    def forward(self, x):\n        batch_size = x.size()[0]\n        h_x = x.size()[2]\n        w_x = x.size()[3]\n        count_h = self.tensor_size(x[:, :, 1:, :])\n        count_w = self.tensor_size(x[:, :, :, 1:])\n        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n        return self.tv_loss_weight * 2 * (h_tv \/ count_h + w_tv \/ count_w) \/ batch_size\n\n    @staticmethod\n    def tensor_size(t):\n        return t.size()[1] * t.size()[2] * t.size()[3]","2bb3ee44":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","70fc6fd5":"netG = Generator(UPSCALE_FACTOR)\nnetD = Discriminator()\ngenerator_criterion = GeneratorLoss()","f1da7c40":"generator_criterion = generator_criterion.to(device)\nnetG = netG.to(device)\nnetD = netD.to(device)","32206a6e":"optimizerG = optim.Adam(netG.parameters(), lr=0.0002)\noptimizerD = optim.Adam(netD.parameters(), lr=0.0002)","103350ab":"results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}","f1a59ef1":"from tqdm import tqdm\nimport os","71a68fc1":"N_EPOCHS = 150","3cf48791":"for epoch in range(1, N_EPOCHS + 1):\n    train_bar = tqdm(train_loader)\n    running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}\n\n    netG.train()\n    netD.train()\n    for data, target in train_bar:\n        g_update_first = True\n        batch_size = data.size(0)\n        running_results['batch_sizes'] += batch_size\n        \n        real_img = Variable(target)\n        if torch.cuda.is_available():\n            real_img = real_img.cuda()\n        z = Variable(data)\n        if torch.cuda.is_available():\n            z = z.cuda()\n        \n        ############################\n        # (1) Update D network: maximize D(x)-1-D(G(z))\n        ###########################\n        fake_img = netG(z)\n\n        netD.zero_grad()\n        real_out = netD(real_img).mean()\n        fake_out = netD(fake_img).mean()\n        d_loss = 1 - real_out + fake_out\n        d_loss.backward(retain_graph=True)\n        optimizerD.step()\n\n        ############################\n        # (2) Update G network: minimize 1-D(G(z)) + Perception Loss + Image Loss + TV Loss\n        ###########################\n        ###### Was causing Runtime Error ######\n        fake_img = netG(z)\n        fake_out = netD(fake_img).mean()\n        #######################################\n        netG.zero_grad()\n        g_loss = generator_criterion(fake_out, fake_img, real_img)\n        g_loss.backward()\n\n        fake_img = netG(z)\n        fake_out = netD(fake_img).mean()\n\n        optimizerG.step()\n\n        # loss for current batch before optimization \n        running_results['g_loss'] += g_loss.item() * batch_size\n        running_results['d_loss'] += d_loss.item() * batch_size\n        running_results['d_score'] += real_out.item() * batch_size\n        running_results['g_score'] += fake_out.item() * batch_size\n\n        train_bar.set_description(desc='[%d\/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n            epoch, N_EPOCHS, running_results['d_loss'] \/ running_results['batch_sizes'],\n            running_results['g_loss'] \/ running_results['batch_sizes'],\n            running_results['d_score'] \/ running_results['batch_sizes'],\n            running_results['g_score'] \/ running_results['batch_sizes']))\n\n    netG.eval()\n    out_path = 'training_results\/SRF_' + str(UPSCALE_FACTOR) + '\/'\n    if not os.path.exists(out_path):\n        os.makedirs(out_path)","cfabdbda":"torch.save(netG.state_dict(), \"super_res_gen.pth\")","3b1a1623":"## Model\n\nWe are going to define the model here."}}