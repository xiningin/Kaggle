{"cell_type":{"85e3808f":"code","64e3a7d7":"code","fb20783d":"code","9ed36d2a":"code","7a49bfdb":"code","12cea47b":"code","5e315595":"code","e033fd44":"code","2ff8066b":"code","e03a3116":"code","a0cfb666":"code","c1c7729a":"code","19f46b91":"code","7a24a256":"code","622bce01":"code","827cbc13":"code","d2e5da64":"code","51581ddd":"code","40d2ae0b":"code","ac16e4fc":"code","0b885a72":"code","5cdff1a6":"code","3acd94cc":"code","8d94d699":"code","4544932b":"code","05e2dd6d":"code","18d46a86":"code","5f5a07dc":"code","d142eadd":"code","cb8ca3b1":"code","f37b344d":"code","4ff6809a":"code","8867f933":"code","2f62ab01":"code","8dc7e243":"code","bf3de3a1":"code","0c034a10":"code","ce9c2df4":"code","b68f7601":"code","47118a2f":"code","36160012":"code","bb103aca":"code","8626cd62":"code","5d2566b1":"code","ddca55ad":"code","98b3b2a3":"code","76ac8b16":"code","85b66a20":"code","970f0b0f":"code","07a30c55":"code","7523fc35":"code","0f233057":"code","b4b20e46":"code","c0ee516b":"code","a1f28dd5":"code","9ce0e3b4":"code","afaad39b":"code","104eb280":"code","1b8ad410":"code","9f5c3ea4":"code","91cb9899":"code","f735a9ad":"code","1a7ff48a":"code","3d5607db":"code","dbdf2dd6":"code","ee1c1a45":"code","d252ce30":"code","114f4fac":"code","fa2ebb39":"code","261c1bf1":"code","ce21bbce":"code","597f56ae":"code","5a3d909e":"code","ebbecb2e":"code","3f0c676b":"code","e96dd1a0":"code","667fc36f":"code","08ca4df5":"code","4515553e":"code","54bec0c2":"code","a517e915":"code","756553be":"markdown","be9ea0bf":"markdown","57c390ad":"markdown","3c677a8f":"markdown","f8a4b773":"markdown","4fe15d6a":"markdown","f3c65c19":"markdown","17365a85":"markdown","93b28353":"markdown","af29dd5a":"markdown","4d3ebc9c":"markdown","c8a07521":"markdown","927549f4":"markdown","d5e982d1":"markdown","918045a3":"markdown","0bbbf6cb":"markdown","25983820":"markdown","4f293f4c":"markdown","512f3d0e":"markdown","51fe02ab":"markdown","a0ef22c1":"markdown","1ea71639":"markdown","3aee93ba":"markdown","ba28fa63":"markdown","74728ba3":"markdown","bce0468d":"markdown","0d7254ca":"markdown","957f9501":"markdown","a96befb1":"markdown","b3668cb0":"markdown","fd24490d":"markdown","54329164":"markdown","300f58a1":"markdown","a696dd51":"markdown","0641db53":"markdown","0a71f03c":"markdown","5e25b772":"markdown","2ab2328a":"markdown","843a7f32":"markdown","f5e45cfa":"markdown","37b5735e":"markdown","6a69769c":"markdown","8fc88543":"markdown","d4f6aced":"markdown","dee263e0":"markdown","e1328480":"markdown","cb791737":"markdown","841a80b3":"markdown","2c956d0e":"markdown","e9a5087d":"markdown","52432188":"markdown","0f8b18a0":"markdown","be99919d":"markdown","ad50b499":"markdown","8957b90d":"markdown","efe86a36":"markdown","36f65585":"markdown","3fbfd866":"markdown","63f6b855":"markdown","489c923f":"markdown","df2dcdfb":"markdown","bbc7fb18":"markdown","0f3152b4":"markdown","2ec70591":"markdown","65cf31a6":"markdown","1f8649fc":"markdown","990c3308":"markdown","49cd8cc6":"markdown","3f6f1fd4":"markdown","15587ca2":"markdown","e7fd6476":"markdown","d4292e94":"markdown","c68306a1":"markdown","53d33b6d":"markdown","402eb17d":"markdown","9d30f049":"markdown","8ad90acf":"markdown","3e6bd465":"markdown","cc508efa":"markdown","9294d96a":"markdown","7e6c3aa0":"markdown","44d1d15c":"markdown","d62def55":"markdown","70a80c0b":"markdown","c209e629":"markdown","1946a725":"markdown","68c2069e":"markdown","54e13599":"markdown","980c577c":"markdown","f4da331b":"markdown","02b2066f":"markdown","3a4750b7":"markdown","c61c3fd8":"markdown","f01cdd83":"markdown","1556719b":"markdown","04e98abb":"markdown","1aba65bc":"markdown","0fdcdad4":"markdown","4f94f126":"markdown"},"source":{"85e3808f":"import numpy as np\nprint('numpy version\\t:',np.__version__)\nimport pandas as pd\nprint('pandas version\\t:',pd.__version__)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom scipy import stats\n\n# Regular expressions\nimport re\n\n# seaborn : advanced visualization\nimport seaborn as sns\nprint('seaborn version\\t:',sns.__version__)\n\npd.options.mode.chained_assignment = None #set it to None to remove SettingWithCopyWarning\npd.options.display.float_format = '{:.2f}'.format #set it to convert scientific noations such as 4.225108e+11 to 422510842796.00\n#pd.set_option('display.max_columns', 100) # to display all the columns\n\nnp.set_printoptions(suppress=True,formatter={'float_kind':'{:f}'.format})\n\nimport os\n\nimport warnings\nwarnings.filterwarnings('ignore') # if there are any warning due to version mismatch, it will be ignored","64e3a7d7":"print(os.listdir(\"..\/input\/lending-club\"))","fb20783d":"loan = pd.read_csv('..\/input\/lending-club\/accepted_2007_to_2018q4.csv\/accepted_2007_to_2018Q4.csv',dtype='object')\nprint(loan.shape)","9ed36d2a":"loan.head(2)","7a49bfdb":"NA_col = loan.isnull().sum()\nNA_col = NA_col[NA_col.values >(0.3*len(loan))]\nplt.figure(figsize=(20,4))\nNA_col.plot(kind='bar')\nplt.title('List of Columns & NA counts where NA values are more than 30%')\nplt.show()","12cea47b":"def removeNulls(dataframe, axis =1, percent=0.3):\n    '''\n    * removeNull function will remove the rows and columns based on parameters provided.\n    * dataframe : Name of the dataframe  \n    * axis      : axis = 0 defines drop rows, axis =1(default) defines drop columns    \n    * percent   : percent of data where column\/rows values are null,default is 0.3(30%)\n              \n    '''\n    df = dataframe.copy()\n    ishape = df.shape\n    if axis == 0:\n        rownames = df.transpose().isnull().sum()\n        rownames = list(rownames[rownames.values > percent*len(df)].index)\n        df.drop(df.index[rownames],inplace=True) \n        print(\"\\nNumber of Rows dropped\\t: \",len(rownames))\n    else:\n        colnames = (df.isnull().sum()\/len(df))\n        colnames = list(colnames[colnames.values>=percent].index)\n        df.drop(labels = colnames,axis =1,inplace=True)        \n        print(\"Number of Columns dropped\\t: \",len(colnames))\n        \n    print(\"\\nOld dataset rows,columns\",ishape,\"\\nNew dataset rows,columns\",df.shape)\n\n    return df","5e315595":"loan = removeNulls(loan, axis =1,percent = 0.3)","e033fd44":"loan = removeNulls(loan, axis =0,percent = 0.3)","2ff8066b":"unique = loan.nunique()\nunique = unique[unique.values == 1]\nprint(unique.index)","e03a3116":"loan.drop(labels = list(unique.index), axis =1, inplace=True)\nprint(\"Now we are left with\",loan.shape ,\"rows & columns.\")","a0cfb666":"print(loan.emp_length.unique())\nloan.emp_length.fillna('Self-Employed',inplace=True)\nprint(loan.emp_length.unique())","c1c7729a":"print(loan.term.unique())\nloan.term = loan.term.str.extract('(\\d+)').astype(float)\nloan.term.fillna(0,inplace=True)\nprint(loan.term.unique())","19f46b91":"if 'id'in loan.columns:\n    del loan['id']\n    print(\"ID deleted\")\nif 'member_id'in loan.columns:\n    del loan['member_id']\n    print(\"memeber ID deleted\")\nif 'url' in loan.columns:\n    del loan['url']\n    print(\"URL deleted\")\nif 'zip_code' in loan.columns:\n    del loan['zip_code']\n    print(\"Zip Code deleted\")","7a24a256":"print(\"Now we are left with\",loan.shape ,\"rows & columns.\")","622bce01":"numeric_columns = ['loan_amnt','funded_amnt','funded_amnt_inv','installment','int_rate','annual_inc','dti','open_acc','mort_acc','total_acc','delinq_2yrs','inq_last_6mths','pub_rec','fico_range_low','fico_range_low']\n\nloan[numeric_columns] = loan[numeric_columns].apply(pd.to_numeric)","827cbc13":"loan.tail(3)","d2e5da64":"(loan.purpose.value_counts()*100)\/len(loan)","51581ddd":"del_loan_purpose = (loan.purpose.value_counts()*100)\/len(loan)\ndel_loan_purpose = del_loan_purpose[(del_loan_purpose < 0.75) | (del_loan_purpose.index == 'other')]\n\nloan.drop(labels = loan[loan.purpose.isin(del_loan_purpose.index)].index, inplace=True)\nprint(\"Now we are left with\",loan.shape ,\"rows & columns.\")\n\nprint(loan.purpose.unique())","40d2ae0b":"(loan.loan_status.value_counts()*100)\/len(loan)","ac16e4fc":"del_loan_status = (loan.loan_status.value_counts()*100)\/len(loan)\ndel_loan_status = del_loan_status[(del_loan_status < 1.5)]\n\nloan.drop(labels = loan[loan.loan_status.isin(del_loan_status.index)].index, inplace=True)\nprint(\"Now we are left with\",loan.shape ,\"rows & columns.\")\n\nprint(loan.loan_status.unique())","0b885a72":"loan['loan_income_ratio'] = loan['loan_amnt']\/loan['annual_inc']","5cdff1a6":"loan['issue_month'],loan['issue_year'] = loan['issue_d'].str.split('-', 1).str\nloan[['issue_d','issue_month','issue_year']].head()","3acd94cc":"months_order = [\"Jan\", \"Feb\", \"Mar\", \"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\nloan['issue_month'] = pd.Categorical(loan['issue_month'],categories=months_order, ordered=True)","8d94d699":"bins = [0, 5000, 10000, 15000, 20000, 25000,40000]\nslot = ['0-5000', '5000-10000', '10000-15000', '15000-20000', '20000-25000','25000 and above']\nloan['loan_amnt_range'] = pd.cut(loan['loan_amnt'], bins, labels=slot)","4544932b":"bins = [0, 25000, 50000, 75000, 100000,1000000]\nslot = ['0-25000', '25000-50000', '50000-75000', '75000-100000', '100000 and above']\nloan['annual_inc_range'] = pd.cut(loan['annual_inc'], bins, labels=slot)","05e2dd6d":"bins = [0, 7.5, 10, 12.5, 15,20]\nslot = ['0-7.5', '7.5-10', '10-12.5', '12.5-15', '15 and above']\nloan['int_rate_range'] = pd.cut(loan['int_rate'], bins, labels=slot)","18d46a86":"def univariate(df,col,vartype,hue =None):\n    \n    '''\n    Univariate function will plot the graphs based on the parameters.\n    df      : dataframe name\n    col     : Column name\n    vartype : variable type : continuos or categorical\n                Continuos(0)   : Distribution, Violin & Boxplot will be plotted.\n                Categorical(1) : Countplot will be plotted.\n    hue     : It's only applicable for categorical analysis.\n    \n    '''\n    sns.set(style=\"darkgrid\")\n    \n    if vartype == 0:\n        fig, ax=plt.subplots(nrows =1,ncols=3,figsize=(20,8))\n        ax[0].set_title(\"Distribution Plot\")\n        sns.distplot(df[col],ax=ax[0])\n        ax[1].set_title(\"Violin Plot\")\n        sns.violinplot(data =df, x=col,ax=ax[1], inner=\"quartile\")\n        ax[2].set_title(\"Box Plot\")\n        sns.boxplot(data =df, x=col,ax=ax[2],orient='v')\n    \n    if vartype == 1:\n        temp = pd.Series(data = hue)\n        fig, ax = plt.subplots()\n        width = len(df[col].unique()) + 6 + 4*len(temp.unique())\n        fig.set_size_inches(width , 7)\n        ax = sns.countplot(data = df, x= col, order=df[col].value_counts().index,hue = hue) \n        if len(temp.unique()) > 0:\n            for p in ax.patches:\n                ax.annotate('{:1.1f}%'.format((p.get_height()*100)\/float(len(loan))), (p.get_x()+0.05, p.get_height()+20))  \n        else:\n            for p in ax.patches:\n                ax.annotate(p.get_height(), (p.get_x()+0.32, p.get_height()+20)) \n        del temp\n    else:\n        exit\n        \n    plt.show()","5f5a07dc":"loan[\"loan_amnt\"].describe()","d142eadd":"univariate(df=loan,col='loan_amnt',vartype=0)","cb8ca3b1":"univariate(df=loan,col='int_rate',vartype=0)","f37b344d":"loan[\"annual_inc\"].describe()","4ff6809a":"q = loan[\"annual_inc\"].quantile(0.995)\nloan = loan[loan[\"annual_inc\"] < q]\nloan[\"annual_inc\"].describe()","8867f933":"univariate(df=loan,col='annual_inc',vartype=0)","2f62ab01":"univariate(df=loan,col='loan_status',vartype=1)","8dc7e243":"univariate(df=loan,col='purpose',vartype=1,hue='loan_status')","bf3de3a1":"loan.home_ownership.unique()","0c034a10":"# Remove rows where home_ownership'=='OTHER', 'NONE', 'ANY'\nrem = ['OTHER', 'NONE', 'ANY']\nloan.drop(loan[loan['home_ownership'].isin(rem)].index,inplace=True)\nloan.home_ownership.unique()","ce9c2df4":"univariate(df=loan,col='home_ownership',vartype=1,hue='loan_status')","b68f7601":"year_wise =loan.groupby(by= [loan.issue_year])[['loan_status']].count()\nyear_wise.rename(columns={\"loan_status\": \"count\"},inplace=True)\nax =year_wise.plot(figsize=(20,8))\nyear_wise.plot(kind='bar',figsize=(20,8),ax = ax)\nplt.show()","47118a2f":"univariate(df=loan,col='term',vartype=1,hue='loan_status')","36160012":"loan.head(3)","bb103aca":"plt.figure(figsize=(16,12))\nsns.boxplot(data =loan, x='purpose', y='loan_amnt', hue ='loan_status')\nplt.title('Purpose of Loan vs Loan Amount')\nplt.show()","8626cd62":"loan_correlation = loan.corr()\nloan_correlation","5d2566b1":"f, ax = plt.subplots(figsize=(14, 9))\nsns.heatmap(loan_correlation, \n            xticklabels=loan_correlation.columns.values,\n            yticklabels=loan_correlation.columns.values,annot= True)\nplt.show()","ddca55ad":"loanstatus=loan.pivot_table(index=['loan_status','purpose','emp_length'],values='loan_amnt',aggfunc=('count')).reset_index()\nloanstatus=loan.loc[loan['loan_status']=='Charged Off']","98b3b2a3":"ax = plt.figure(figsize=(30, 18))\nax = sns.boxplot(x='emp_length',y='loan_amnt',hue='purpose',data=loanstatus)\nax.set_title('Employment Length vs Loan Amount for different pupose of Loan',fontsize=22,weight=\"bold\")\nax.set_xlabel('Employment Length',fontsize=16)\nax.set_ylabel('Loan Amount',color = 'b',fontsize=16)\nplt.show()","76ac8b16":"def crosstab(df,col):\n    '''\n    df : Dataframe\n    col: Column Name\n    '''\n    crosstab = pd.crosstab(df[col], df['loan_status'],margins=True)\n    crosstab['Probability_Charged Off'] = round((crosstab['Charged Off']\/crosstab['All']),3)\n    crosstab = crosstab[0:-1]\n    max1 = max(crosstab['Probability_Charged Off'])\n    maxx = crosstab.loc[crosstab['Probability_Charged Off']==max1]\n    \n    return crosstab,maxx","85b66a20":"# Probability of charge off\ndef bivariate_prob(df,col,stacked= True):\n    '''\n    df      : Dataframe\n    col     : Column Name\n    stacked : True(default) for Stacked Bar\n    '''\n    # get dataframe from crosstab function\n    plotCrosstab,maxx = crosstab(df,col)\n    \n    linePlot = plotCrosstab[['Probability_Charged Off']]      \n    barPlot =  plotCrosstab.iloc[:,0:2]\n    ax = linePlot.plot(figsize=(20,8), marker='o',color = 'r')\n    ax2 = barPlot.plot(kind='bar',ax = ax,rot=1,secondary_y=True,stacked=stacked)\n    ax.set_title(df[col].name.title()+' vs Probability Charge Off',fontsize=20,weight=\"bold\")\n    ax.set_xlabel(df[col].name.title(),fontsize=14)\n    ax.set_ylabel('Probability of Charged off',color = 'b',fontsize=14)\n    ax2.set_ylabel('Number of Applicants',color = 'g',fontsize=14)\n    plt.show()","970f0b0f":"filter_states = loan.addr_state.value_counts()\nfilter_states = filter_states[(filter_states < 10)]\n\nloan_filter_states = loan.drop(labels = loan[loan.addr_state.isin(filter_states.index)].index)","07a30c55":"states,maxx = crosstab(loan_filter_states,'addr_state')\ndisplay(states.tail(20))\n\nprint(\"maximum\")\ndisplay(maxx)\n\nbivariate_prob(df=loan_filter_states,col ='addr_state')","7523fc35":"purpose,maxx = crosstab(loan,'purpose')\ndisplay(purpose)\n\nprint(\"maximum\")\ndisplay(maxx)\n\nbivariate_prob(df=loan,col='purpose',stacked=False)","0f233057":"grade,maxx = crosstab(loan,'grade')\ndisplay(grade)\n\nprint(\"maximum\")\ndisplay(maxx)\n\nbivariate_prob(df =loan,col ='grade',stacked=False)\nbivariate_prob(df =loan,col ='sub_grade')","b4b20e46":"annual_inc_range,maxx = crosstab(loan,'annual_inc_range')\ndisplay(annual_inc_range)\n\nprint(\"maximum\")\ndisplay(maxx)\n\nbivariate_prob(df=loan,col ='annual_inc_range')","c0ee516b":"int_rate_range,maxx = crosstab(loan,'int_rate_range')\ndisplay(int_rate_range)\n\nprint(\"maximum\")\ndisplay(maxx)\n\nbivariate_prob(df =loan,col ='int_rate_range')","a1f28dd5":"emp_length,maxx = crosstab(loan,'emp_length')\ndisplay(emp_length)\n\nprint(\"maximum\")\ndisplay(maxx)\n\nbivariate_prob(df=loan,col='emp_length')","9ce0e3b4":"len(loan.columns)","afaad39b":"loan_status_grouped = loan.groupby('loan_status').size().sort_values(ascending=False)\/len(loan) * 100\nloan_status_grouped","104eb280":"loan = loan[loan.loan_status != 'Current']\n#loan['loan_status'] = loan['loan_status'].replace({'Fully Paid':'Paid'})\n#loan['loan_status'] = loan['loan_status'].replace({'Charged Off':'Default'})\nprint(loan['loan_status'].value_counts()\/len(loan))","1b8ad410":"univariate(df=loan,col='loan_status',vartype=1)","9f5c3ea4":"loan.tail()","91cb9899":"liststr=\"installment\tfunded_amnt\tfunded_amnt_inv\tverification_status\tissue_d\tsub_grade\temp_title\tpymnt_plan\ttitle\taddr_state\tearliest_cr_line\tinitial_list_status\tout_prncp\tout_prncp_inv\ttotal_pymnt\ttotal_pymnt_inv\ttotal_rec_prncp\ttotal_rec_int\ttotal_rec_late_fee\trecoveries\tcollection_recovery_fee\tlast_pymnt_d\tlast_pymnt_amnt\tlast_credit_pull_d\tcollections_12_mths_ex_med\tapplication_type\tacc_now_delinq\ttot_coll_amt\ttot_cur_bal\ttotal_rev_hi_lim\tacc_open_past_24mths\tavg_cur_bal\tbc_open_to_buy\tbc_util\tchargeoff_within_12_mths\tdelinq_amnt\tmo_sin_old_il_acct\tmo_sin_old_rev_tl_op\tmo_sin_rcnt_rev_tl_op\tmo_sin_rcnt_tl\tmths_since_recent_bc\tmths_since_recent_inq\tnum_accts_ever_120_pd\tnum_actv_bc_tl\tnum_actv_rev_tl\tnum_bc_sats\tnum_bc_tl\tnum_il_tl\tnum_op_rev_tl\tnum_rev_accts\tnum_rev_tl_bal_gt_0\tnum_sats\tnum_tl_120dpd_2m\tnum_tl_30dpd\tnum_tl_90g_dpd_24m\tnum_tl_op_past_12m\tpct_tl_nvr_dlq\tpercent_bc_gt_75\tpub_rec_bankruptcies\ttax_liens\ttot_hi_cred_lim\ttotal_bal_ex_mort\ttotal_bc_limit\ttotal_il_high_credit_limit\thardship_flag\tdisbursement_method\tdebt_settlement_flag\tissue_month\tissue_year\tissue_month\tissue_year\tloan_amnt_range\tannual_inc_range\tint_rate_range\"\nnot_required_columns = liststr.split(\"\\t\")\nprint('Number of rows to be deleted =',len(not_required_columns))\nloan.drop(not_required_columns, axis=1, inplace=True)\nprint(\"Now we are left with\",loan.shape ,\"rows & columns.\")","f735a9ad":"print(loan.shape)\nloan.head()","1a7ff48a":"print(\"We have following categorical data:\")\nprint(loan['purpose'].unique())\nprint(loan['loan_status'].unique())\nprint(loan['home_ownership'].unique())","3d5607db":"loan_correlation = loan.corr()\nloan_correlation","dbdf2dd6":"f, ax = plt.subplots(figsize=(14, 9))\nsns.heatmap(loan_correlation, \n            xticklabels=loan_correlation.columns.values,\n            yticklabels=loan_correlation.columns.values,annot= True)\nplt.show()","ee1c1a45":"loan['open_acc_ratio'] = loan['open_acc']\/loan['total_acc']","d252ce30":"loan[\"log_annual_income\"] = np.log(loan['annual_inc'])","114f4fac":"#loan.emp_length = loan.emp_length.str.extract('(\\d+)').astype(float)\n#loan.emp_length.fillna(loan.emp_length.median(),inplace=True)\n\nloan['delinq_2yrs_cat'] = 'no'\nloan.loc[loan['delinq_2yrs']> 0,'delinq_2yrs_cat'] = 'yes'\n\nloan['inq_last_6mths_cat'] = 'no'\nloan.loc[loan['inq_last_6mths']> 0,'inq_last_6mths_cat'] = 'yes'\n\nloan['pub_rec_cat'] = 'no'\nloan.loc[loan['pub_rec']> 0,'pub_rec_cat'] = 'yes'","fa2ebb39":"print(loan.delinq_2yrs.unique())\nprint(loan.delinq_2yrs_cat.unique())\nprint(loan.inq_last_6mths.unique())\nprint(loan.inq_last_6mths_cat.unique())\nprint(loan.pub_rec.unique())\nprint(loan.pub_rec_cat.unique())","261c1bf1":"#original = ['annual_inc','open_acc','mort_acc']\ndrop_original = ['annual_inc','delinq_2yrs','inq_last_6mths','pub_rec']\nloan.drop(drop_original, axis=1, inplace=True)\nprint(\"Now we are left with\",loan.shape ,\"rows & columns.\")","ce21bbce":"# Drop\ndrop = ['last_fico_range_high','last_fico_range_low']\nloan.drop(drop, axis=1, inplace=True)\nr,c=loan.shape\nprint(f\"The number of rows {r}\\nThe number of columns {c}\")\nloan.dropna(axis=0, how = 'any', inplace = True)\nr1,c1=loan.shape\nprint(f\"The difference between earlier and dropped Nan rows: {r-r1}\")","597f56ae":"# average fico score from range\n#loan[\"fico_score\"] = (loan['fico_range_high'] + loan['fico_range_low'])\/2\nloan['fico_score'] = loan[['fico_range_low', 'fico_range_high']].mean(axis=1)\ndrop = ['fico_range_low','fico_range_high']\nloan.drop(drop, axis=1, inplace=True)\nloan['fico_score']","5a3d909e":"univariate(df=loan,col='fico_score',vartype=0)","ebbecb2e":"univariate(df=loan,col='fico_score',vartype=1)","3f0c676b":"fico,maxx = crosstab(loan,'fico_score')\ndisplay(fico)\n\nprint(\"maximum\")\ndisplay(maxx)\n\nbivariate_prob(df=loan,col='fico_score',stacked=True)","e96dd1a0":"fico.dtypes","667fc36f":"#plt.subplots()\nlinePlot = fico['Probability_Charged Off']\nbarPlot =  fico.iloc[:,0:2]\nfig, ax1 = plt.subplots()\nax1 = linePlot.plot(figsize=(20,8), marker='o',color = 'r') #linestyle='-'\n#ax2 = barPlot.plot(kind='bar',rot=1,secondary_y=True,stacked=True)\nax1.set_title(loan['fico_score'].name.title()+' vs Probability Charge Off',fontsize=20,weight=\"bold\")\nax1.set_xlabel(loan['fico_score'].name.title(),fontsize=14)\nax1.set_ylabel('Probability of Charged off',color = 'b',fontsize=14)\n#ax2.set_ylabel('Number of Applicants',color = 'g',fontsize=14)\nplt.show()","08ca4df5":"loan['emp_length'] = loan['emp_length'].replace({'1 year':1,'10+ years':'10','2 years':2,'3 years':3,\"4 years\":4,\"5 years\":5,\n                                                 \"6 years\":6,\"7 years\":7,\"8 years\":8,\"9 years\":9,\"< 1 year\":0})\n\n#a_dataframe.drop(a_dataframe[a_dataframe.B > 3].index, inplace=True)\n\nloan.drop(loan[loan['emp_length']==\"Self-Employed\"].index,inplace = True)\nloan['emp_length'] = loan['emp_length'].apply(pd.to_numeric)\n\nunivariate(df=loan,col='emp_length',vartype=1)","4515553e":"loan.head()","54bec0c2":"loan.dtypes","a517e915":"loan.to_csv('cleanedData.csv', index=False)","756553be":"#### 1. Location vs Probability Charge Off","be9ea0bf":"#### 2. Purpose of Loan vs Probability Charge Off","57c390ad":"Dropped Policy code","3c677a8f":"__List of Columns & NA counts where NA values are more than 30%__","f8a4b773":"## II. Import Libraries and set required parameters","4fe15d6a":"### Continuous Variables","f3c65c19":"<font color='blue'><b>Insights<\/b>: As the interest rate is increasing the probability that person will default is increasing with highest of 18% at 15% & above bracket.<\/font>      ","17365a85":"#### 5. Remove irrelevant columns.\n\nTill now we have removed the columns based on the count & statistics.\nNow let's look at each column from any business perspective if that is required or not for our analysis such as Unique ID's, URL.\nAs last 2 digits of zip code is masked 'xx', we can remove that as well.","93b28353":"<font color='blue'><b>Insights<\/b>: As we move from Grade A to G, probability that person will charged off is increasing.<\/font>      ","af29dd5a":"#### Term into float data","4d3ebc9c":"<font color='blue'><b>Insights<\/b>: As the annual income is decreasing the probability that person will default is increasing with highest of 14% at (0 to 25000) salary bracket.<\/font>      ","c8a07521":"#### 2. We will remove all the columns not required for our model","927549f4":"#### Get some insights","d5e982d1":"#### 4. Employment Term : Remove 'n\/a' value with 'self-employed'.\n\nThere are some values in term which are <b>'n\/a'<\/b>, we assume that are <b>'self-employed'<\/b>. Because for 'self-employed' applicants, emp-lenth is 'Not Applicable'","918045a3":"#### 3. HeatMap: All continuos variables","0bbbf6cb":"#### 1. Remove columns where NA values are more than or equal to 30%","25983820":"__<font color='green'>UDF :: removeNulls(dataframe, axis, percent)<\/font>__\n\n###### removeNulls(dataframe, axis, percent) will drop the columns\/rows from the dataset based on the parameter values.","4f293f4c":"There are no rows where NA values are more than or equal to 30%","512f3d0e":"## V. Derived Metrics","51fe02ab":"__<font color='green'><u><b>UDF: <\/b>crosstab<\/u><\/font>__\n##### 'crosstab' function will be used to get the summarized table for each column(passed as a variable) along with probability of charge off w.r.t that column.","a0ef22c1":"<font color='blue'><b>Insights<\/b>: We can see from the above plot that there are 20+ columns in the dataset where all the values are NA.<\/font> \n\nAs we can see there are <b>2.2M<\/b> rows & <b>145<\/b> columns in the dataset, it will be very difficult to look at each column one by one & find the NA or missing values.\nSo let's find out all columns where missing values are more than certain percentage, let's say <b>30%<\/b>. We will remove those columns as it is not feasable to impute missing values for those columns.\n\nWe will create a UDF (User Defined function) for this.","1ea71639":"<a id='univariate'><\/a>","3aee93ba":"<font color='blue'><b>Insights<\/b>: There are multiple States\/Provinces with high probability of charge-off,highest being Albama (AL) at 14%<\/font>   ","ba28fa63":"#### 2. Extract Year & Month from Issue date","74728ba3":"<a id='library'><\/a>","bce0468d":"<font color='blue'><b>Insights<\/b>: 38.8% of applicants are living in rented home whereas 50.2% applicants have mortagaged their home.<\/font> ","0d7254ca":"#### 3. Grade\/Subgrade vs Probability Charge Off","957f9501":"#### 8. Loan Term","a96befb1":"## <a id=\"saved\"> <\/a> XI. Cleaned Data Saved","b3668cb0":"#### 5. Purpose of loan","fd24490d":"5. Our business case doesn't require last fico scores, Lets take in only fico scores at the time of loan origination. <br>\nLets also drop any nan values still present.","54329164":"#### 2. Interest Rate","300f58a1":"<a id='prerequisite'><\/a>","a696dd51":"<font color='blue'><b>Insights<\/b>: Applicants who has taken the Loan for 'small business' has the highest probabilty of charge off of 14%. So bank should take extra caution like take some asset or guarentee while approving the loan for purpose of 'small business'<\/font>   ","0641db53":"##### Handling Outlier\n###### Max value is 110000000 which is far more than mean value, so we will remove the outliers from Annual Income.\n\nRemove Outliers (values from 99 to 100%)","0a71f03c":"#### 4. Categorizing some more features","5e25b772":"#### 2. Correlation Matrix : All Continuous(Numeric) Variables","2ab2328a":"<a id=\"business\"> <\/a>\n\n## IX. Business Problem\n\n- In developing the model we need to think about the business problem we are trying to solve. I have identified two different scenarios:\n\n    1. In the first scenario, the investor (assuming he\/she has access to our same data) wants to predict the risk of charged off before lending the money to a borrower. The metrics associated to activity in the Loan Club are not known because the customer is still a prospect borrower.\n    2. In the second scenario, Loan Club wants to predict probability for a borrower to charge off while he\/she is \"Current\", to prevent the charge off from happening or try to minimise damage.\n- The problem therefore becomes: How well can we predict that a prospect customer will charge off at some point in the future?","843a7f32":"#### 6. Cast all continuous variables to numeric\nCast all continuous variables to numeric so that we can find a correlation between them","f5e45cfa":"#### 4. Annual Income Range vs Probability Charge Off","37b5735e":"#### 6. Home Ownership wise Loan","6a69769c":"### Continuous Variables\nIn case of continuous variables, we need to understand the central tendency and spread of the variable.These are measured using various statistical metrics visualization methods such as Boxplot,Histogram\/Distribution Plot, Violin Plot etc.","8fc88543":"<a id='derived'><\/a>","d4f6aced":"#### 3. Change order of months from Jan to Dec, currently it's in alphabetical order(A-Z)","dee263e0":"#### 1. Remove current from loan status as we are first predicting if the person will charged off before lending loan.","e1328480":"<font color='blue'><b>Insights<\/b>: Most of the applicants earns between 40000 to 90000 USD annually.<\/font> ","cb791737":"<font color='blue'><b>Insights<\/b>: loan applicants are increasing year on year, maximum loan applicants received loans in 2018.<\/font> ","841a80b3":"#### 5. Create Bins for range of Annual Income","2c956d0e":"#### 8. Loan Status : Drop records where values are less than 1.5%\nAs we can see, Other than ['Current','Fully Paid' & Charged off] other loan_status are not relevent for our analysis.","e9a5087d":"### Categorical Variables","52432188":"#### 6. Create Bins for range of Interest rates","0f8b18a0":"Distribution and Count plot of the Fico Scores","be99919d":"<font color='blue'><b>Insights<\/b>: Most of the loan amounts are distributed between 8000 to 20000 USD.<\/font> ","ad50b499":"#### 4. Create Bins for range of Loan Amount","8957b90d":"* Lets take mean of the ranges.\n  I'm not sure if its good to take mean or not.","efe86a36":"** We will now derive some new columns based on our business understanding that will be helpful in our analysis. **","36f65585":"Let's compute the ratio of the number of open credit lines in the borrower's credit file divided by the total number of credit lines currently in the borrower's credit file.","3fbfd866":"#### 1. Loan amount to Annual Income ratio","63f6b855":"<font color='blue'><b>Insights<\/b>: 61.7% of the applicants applied loan for paying their other loans(Debt Consolidation).<\/font> ","489c923f":"## VII. Bivariate\/Multivariate Analysis\nBivariate\/Multivariate Analysis finds out the relationship between two\/two or more variables.We can perform Bivariate\/Multivariate analysis for any combination of categorical and continuous variables. The combination can be: Categorical & Categorical, Categorical & Continuous and Continuous & Continuous.","df2dcdfb":"#### 1. Purpose of Loan vs Loan Amount for each Loan Status","bbc7fb18":"__<font color='green'>UDF :: univariate(df,col,vartype,hue)<\/font>__\n\n###### Univariate function will plot the graphs based on the parameter values.","0f3152b4":"<font color='blue'><b>Insights<\/b>: 5% of the applicants Charged off.<\/font> ","2ec70591":"#### 5. Interest rate Range vs Probability Charge Off","65cf31a6":"<a id='sourcing'><\/a>","1f8649fc":"## I. Prerequisite\n\n1. Place 'loan.csv' input file at __\"..\/input\"__ directory before running this code.\n\n2. Please make sure that you have following python libraries imported\/installed at your system:\n\n    * numpy version\t: 1.12.1 or higher\n    * pandas version\t: 0.20.3 or higher\n    * seaborn version\t: 0.8.0 or higher","990c3308":"#### 4. Loan Status","49cd8cc6":"#### 4. Employment Length vs Loan Amount for different pupose of Loan","3f6f1fd4":"<a id='bivariate_prob'><\/a>","15587ca2":"#### 6. Employment Length vs Probability Charge Off","e7fd6476":"#### 7. Year wise Loan","d4292e94":"#### 1. Loan Amount","c68306a1":"#### 3. Annual Income","53d33b6d":"## III. Data Sourcing","402eb17d":"<font color='blue'><b>Insights<\/b>: 109389 loan applications had 660 as their Fico Scores. 26% of people were charged off from this fico scores.<\/font>      ","9d30f049":"### Categorical Variables vs Probability of Charged Off\n\nThe main motive of this use case to find what parameters are impacting the most on Loan Status that is if a applicant will successfully complete the loan term or will charge off.\n\nSo we will be using a new term now <b>Probability of Charged Off<\/b> that will be equal to :\n\n\\begin{equation*}\n{Probability\\:of\\:Charged\\:Off =\\:}\\frac{Number\\:of\\:Applicants\\:who\\:charged\\:off}{Total\\:No.\\:of\\:Applicants}\n\\end{equation*}\n\n\nWe will calculate this probability w.r.t each column in bivariate analysis & will see how the Probability of Charged Off changes with these columns.\n\nWe will create a user defined function for this.","8ad90acf":"### <a id=\"toc\"> <\/a> <u>Table of Contents<\/u>\n\n[I. Prerequisite](#prerequisite)\n\n[II. Import Libraries and set required parameters](#library)\n\n[III. Data Sourcing](#sourcing)\n\n[IV. Data Cleaning and Manipulation](#cleaning)\n\n[V. Derived Metrics](#derived)\n\n[VI. Univariate Analysis](#univariate)\n\n[VII. Bivariate\/Multivariate Analysis](#bivariate)\n\n[VII. Bivariate\/Multivariate Analysis with Probability of Charged off](#bivariate_prob)\n\n[IX. Business UseCase](#business)\n\n[X. Re-cleaning](#preparingModel)\n\n[XI. Cleaned Data Saved](#saved)\n\n[XII. Conclusion](#saved)","3e6bd465":"### Target Variable\n* <font color='blue'><b>Loan Status<\/b><\/font>\n\n### Top-5 Major variables to consider for loan prediction: \n1. <font color='blue'><b>Purpose of Loan<\/b><\/font>\n2. <font color='blue'><b>Employment Length<\/b><\/font>\n3. <font color='blue'><b>Grade<\/b><\/font>\n4. <font color='blue'><b>Interest Rate<\/b><\/font>\n5. <font color='blue'><b>Term<\/b><\/font>","cc508efa":"__<font color='green'><u><b>UDF: <\/b>bivariate_prob<\/u><\/font>__\n##### 'bivariate_prob' function will be used to plot count of values for each column(passed as a variable) stacked across 'loan_status' along with probability of charged off as a line chart.","9294d96a":"<font color='blue'><b>Insights<\/b>: Applicants who are self employed & less than 1 year of experience are more probable of charged off.. <\/font> ","7e6c3aa0":"<font color='blue'><b>Insights<\/b>: It is clear from the Heatmap that how <b>'loan_amnt','funded_amnt' & 'funded_amnt_inv'<\/b> are closely <b>interrelated<\/b>.So we can take any one column out of them for our analysis.<\/font> ","44d1d15c":"***\n## <a id = \"conclusion\"> <\/a> <font color='blue'><b>XII. Next Step build model<\/b><\/font> \n### Conclusion","d62def55":"<a id='preparingModel'><\/a>\n## X. Cleaning & Feature Engineering Data to build a model for our first scenario.","70a80c0b":"6. Actually making employment Length a continuous value instead of categorical feature for better accuracy of model. (TRY)","c209e629":"#### 2. Remove rows where NA values are more than or equal to 30%\n(Note: NA columns has already been removed in step 1, so we may not find any <b>rows<\/b> where 30% values are NA)","1946a725":"<a id='bivariate'><\/a>","68c2069e":"<font color='blue'><b>Insights<\/b>: 70% of applicants applied loan for 36 months term period.<\/font> ","54e13599":"- delinq_2yrs: The number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past.\n- inq_last_6mths: The number of inquiries in past 6 months (excluding auto and mortgage inquiries)\n- pub_rec: Number of derogatory public records","980c577c":"## VIII. Bivariate\/Multivariate Analysis with Probability of Charge off","f4da331b":"#### 3. Remove columns where number of unique value is only 1.\n\nLet's look at no of unique values for each column.We will remove all columns where number of unique value is only 1 because that will not make any sense in the analysis","02b2066f":"# ALRS: Automatic Loan Risk Analysis\n***\n***","3a4750b7":"## IV. Data Cleaning and Manipulation","c61c3fd8":"<font color='blue'><b>Insights<\/b>: Majority of the applicants has Fico Score between 660 to 715 <\/font> ","f01cdd83":"### Categorical Variables\nFor categorical variables, we\u2019ll use frequency table to understand distribution of each category. It can be be measured using two metrics, Count and Count% against each category. Countplot or Bar chart can be used as visualization.","1556719b":"#### 7. Purpose of loan : Drop records where values are less than 0.75%\nWe will analyse only those categories which contain more than 0.75% of records.\nAlso, we are not aware what comes under 'Other' we will remove this category as well.","04e98abb":"#### 3. Adding our three more features","1aba65bc":"<font color='blue'><b>Insights<\/b>: Most of the loans interest rates are distributed between 10% to 16%.<\/font> ","0fdcdad4":"<a id='cleaning'><\/a>","4f94f126":"## VI. Univariate Analysis"}}