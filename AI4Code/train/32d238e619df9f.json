{"cell_type":{"b9f9da5f":"code","c0f31dd8":"code","4266fc52":"code","9bf1579c":"code","7c48145f":"code","abf20261":"code","6f0e2803":"code","36e1a807":"code","e9febf3c":"code","01be8c10":"code","7ba3ce6f":"code","d2fe8cb2":"code","54abafbb":"code","e01ea325":"code","0dc6cbe1":"code","10803fa3":"code","ce36a2e0":"code","3050cdaa":"code","ea04e5bf":"code","44156d38":"code","a6234029":"code","a640436a":"code","7d412a8a":"code","a6c70464":"code","2e36145d":"code","32085f8f":"code","78e8a4fe":"code","3444049f":"code","4ec2936e":"code","357647f3":"code","0e336693":"code","c202aaa7":"code","a419a814":"code","cf72237d":"code","7142fba7":"code","5c4eb1b3":"code","c6a201e5":"code","1bf154dc":"code","d7a1a8d3":"code","477252cd":"code","e1171aed":"code","64ddfb79":"code","e7406203":"code","84a56d54":"code","56ee9c94":"code","fe87eec6":"code","e7baa140":"code","0ee70b01":"code","b02e6911":"markdown","f42a644d":"markdown","2f0c477f":"markdown","8c284157":"markdown","eb953d3f":"markdown","a106ae2c":"markdown","f7678e06":"markdown","98115b5c":"markdown","12ef48f9":"markdown","8e5838d9":"markdown","b585f333":"markdown","20bc6753":"markdown","9cfb3f2d":"markdown","444eaad7":"markdown","ea47c059":"markdown","879b1fb0":"markdown","a3b426d6":"markdown","fd626b80":"markdown","f1916d49":"markdown","774db3ba":"markdown","e0db10fa":"markdown","b2379383":"markdown","b0367425":"markdown","0f8405a5":"markdown","7565df80":"markdown","d1c85daf":"markdown","1c849a29":"markdown","cda5b064":"markdown","65873636":"markdown"},"source":{"b9f9da5f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c0f31dd8":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n#KNNImputer\nfrom sklearn.impute import KNNImputer\n\nfrom numpy import loadtxt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.preprocessing import MinMaxScaler","4266fc52":"train_data = pd.read_csv(r\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data = pd.read_csv(r\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","9bf1579c":"def object_cols(df):\n    return list(df.select_dtypes(include='object').columns)\n\ndef numerical_cols(df):\n    return list(df.select_dtypes(exclude='object').columns)","7c48145f":"print(\"Number of non-numeric data columns = {0} and number of numeric data columns = {1}\".format(len(object_cols(train_data)),len(numerical_cols(train_data)) ))","abf20261":"train_data.head()","6f0e2803":"train_data.describe()","36e1a807":"def missing_values(df):\n    total_nans_df = pd.DataFrame(df.isnull().sum(), columns=['Values'])\n    total_nans_df = total_nans_df.reset_index()\n    total_nans_df.columns = ['Columns', 'Values']\n    total_nans_df['% Missing Values'] = 100*total_nans_df['Values']\/df.shape[0]\n    total_nans_df = total_nans_df[total_nans_df['% Missing Values'] > 0 ]\n    total_nans_df = total_nans_df.sort_values(by=['% Missing Values'])\n\n    plt.rcdefaults()\n    plt.figure(figsize=(10,5))\n    ax = sns.barplot(x=\"Columns\", y=\"% Missing Values\", data=total_nans_df)\n    ax.set_ylim(0, 100)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n    plt.show()","e9febf3c":"missing_values(train_data) #checking % emptiness of train data columns","01be8c10":"missing_values(test_data) #checking % emptiness of test data columns","7ba3ce6f":"# function to drop columns with missing that values based on a predefine cutoff criteria\ndef drop_columns_with_missing_values(df, cutoff):\n    \"\"\"Drop columns with missing values greater than the specified cut-off %\n    \n    Parameters:\n    -----------\n    df     : pandas dataframe\n    cutoff : % missing values\n    \n    Returns:\n    ---------\n    Returns clean dataframe\n    \"\"\"\n    # create a dataframe for missing values by column\n    total_nans_df = pd.DataFrame(df.isnull().sum(), columns=['values'])\n    total_nans_df = total_nans_df.reset_index()\n    total_nans_df.columns = ['cols', 'values']\n    \n    # calculate % missing values\n    total_nans_df['% missing values'] = 100*total_nans_df['values']\/df.shape[0]\n    \n    total_nans_df = total_nans_df[total_nans_df['% missing values'] >= cutoff ]\n    \n    # get columns to drop\n    cols = list(total_nans_df['cols'])\n    print('Features with missing values greater than specified cutoff : ', cols)\n    print('Shape before dropping: ', df.shape)\n    new_df = df.drop(labels=cols, axis=1)\n    print('Shape after dropping: ',new_df.shape)\n    \n    return new_df","d2fe8cb2":"train_df = drop_columns_with_missing_values(train_data, 80)\ntest_df = drop_columns_with_missing_values(test_data, 80)","54abafbb":"cols = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', \n        'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\n\nvalues = {col: 'None' for col in cols}\n\nnew_train_df = train_df.fillna(value=values)\nnew_test_df = test_df.fillna(value=values)","e01ea325":"other_cols = [col for col in object_cols(new_train_df) if col not in cols]\nprint(other_cols)\n\nvalues = {col:  new_train_df[col].mode()[0] for col in other_cols}\n\nnew_train_df = new_train_df.fillna(value=values)\n\nvalues = {col:  new_test_df[col].mode()[0] for col in other_cols}\n\nnew_test_df = new_test_df.fillna(value=values)","0dc6cbe1":"imp = KNNImputer(missing_values=np.nan, n_neighbors=7)\nimp.fit(new_train_df[numerical_cols(new_train_df)])\nnew_train_df[numerical_cols(new_train_df)] = imp.transform(new_train_df[numerical_cols(new_train_df)])","10803fa3":"imp = KNNImputer(missing_values=np.nan, n_neighbors=7)\nimp.fit(new_test_df[numerical_cols(new_test_df)])\nnew_test_df[numerical_cols(new_test_df)] = imp.transform(new_test_df[numerical_cols(new_test_df)])","ce36a2e0":"print(new_train_df.isnull().any() if new_train_df.isnull().any == True else \"There is no any empty value in train data\")","3050cdaa":"print(new_test_df.isnull().any() if new_test_df.isnull().any == True else \"There is no any empty value in test data\")","ea04e5bf":"corr = new_train_df[numerical_cols(new_train_df)].corr()\n\nfig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(corr[['SalePrice']].sort_values(by=['SalePrice'],ascending=False),ax=ax, vmin = -1, cmap='coolwarm',annot=True)","44156d38":"fig = sns.pairplot(data = new_train_df[numerical_cols(new_train_df)], \n                   x_vars = [\"OverallQual\",\"GrLivArea\", \"GarageCars\",\"GarageArea\",\"TotalBsmtSF\", \"1stFlrSF\"], \n                   y_vars = [\"OverallQual\",\"GrLivArea\", \"GarageCars\",\"GarageArea\",\"TotalBsmtSF\", \"1stFlrSF\"]\n                   )","a6234029":"#OverallQual\nplt.rcdefaults()\nplt.figure(figsize=(10,5))\nax = sns.barplot(x=\"OverallQual\", y=\"SalePrice\", data=new_train_df)\nax.set_ylim(0, max(new_train_df[\"SalePrice\"]))\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nplt.show()","a640436a":"#GrLivArea\nplt.rcdefaults()\nplt.figure(figsize=(10,5))\nax = plt.scatter(x=\"GrLivArea\", y=\"SalePrice\", data=new_train_df)\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()","7d412a8a":"#GarageCars\nplt.rcdefaults()\nplt.figure(figsize=(10,5))\nax = sns.barplot(x=\"GarageCars\", y=\"SalePrice\", data=new_train_df)\nax.set_ylim(0, max(new_train_df[\"SalePrice\"]))\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nplt.show()","a6c70464":"#KitchenAbvGr\nplt.rcdefaults()\nplt.figure(figsize=(10,5))\nax = sns.barplot(x=\"KitchenAbvGr\", y=\"SalePrice\", data=new_train_df)\nax.set_ylim(0, max(new_train_df[\"SalePrice\"]))\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nplt.show()","2e36145d":"#EnclosedPorch\nplt.rcdefaults()\nplt.figure(figsize=(10,5))\nax = plt.scatter(x=\"EnclosedPorch\", y=\"SalePrice\", data=new_train_df)\nplt.xlabel(\"EnclosedPorch\")\nplt.ylabel(\"SalePrice\")\nplt.show()","32085f8f":"#MSSubClass\nplt.rcdefaults()\nplt.figure(figsize=(10,5))\nax = sns.barplot(x=\"MSSubClass\", y=\"SalePrice\", data=new_train_df)\nax.set_ylim(0, max(new_train_df[\"SalePrice\"]))\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nplt.show()","78e8a4fe":"correlation = new_train_df[numerical_cols(new_train_df)].corr()\n\ncorrelation_df = pd.DataFrame(correlation['SalePrice'].sort_values(ascending=False))\n\ncorrelation_df = correlation_df.reset_index()\n\ncorrelation_df.columns = ['Column', 'Correlation']\n\ncols_to_drop = list(correlation_df[correlation_df['Correlation'] < 0]['Column'])\ncols_to_drop","3444049f":"new_train_df = new_train_df.drop(labels=cols_to_drop, axis=1)\nnew_test_df = new_test_df.drop(labels=cols_to_drop, axis=1)","4ec2936e":"new_train_df['train']  = 1 # if this pointer's value is 1 that means the related data is a train data\nnew_test_df['train']  = 0\n\ndf = pd.concat([new_train_df, new_test_df], axis=0,sort=False)","357647f3":"encoding_columns = [\n    'ExterQual',\n    'ExterCond',\n    'GarageCond',\n    'GarageQual',\n    'FireplaceQu',\n    'KitchenQual',\n    'CentralAir',\n    'HeatingQC',\n    'BsmtFinType2',\n    'BsmtFinType1',\n    'BsmtExposure',\n    'BsmtCond',\n    'BsmtQual'\n]","0e336693":"encoding_values = {\n    'Ex': 5,\n    'Gd': 4,\n    'TA': 3,\n    'Fa': 2,\n    'Po': 1,\n    'None': 0,\n    'Av':   3,\n    'Mn':   2,\n    'No':   1,\n    'GLQ':  6,\n    'ALQ':  5,\n    'BLQ':  4,\n    'Rec':  3,\n    'LwQ':  2,\n    'Unf':  1,\n    'N':    0,\n    'Y':    1\n}","c202aaa7":"for col in encoding_columns:\n    df[col] = df[col].map(encoding_values)","a419a814":"train_df_to_normalize =  df[df['train'] == 1] # getting train data which we labeled by 1\ntrain_df_to_normalize = train_df_to_normalize.drop(labels=['train'], axis=1) # dropping 'train' column which we added\n\n\ntest_df_to_normalize = df[df['train'] == 0] # getting test data which we labeled by 0\ntest_df_to_normalize = test_df_to_normalize.drop(labels=['SalePrice'], axis=1) # dropping 'SalePrice' column\ntest_df_to_normalize = test_df_to_normalize.drop(labels=['train'], axis=1) # dropping 'train' column which we added","cf72237d":"list_of_norm_cols = numerical_cols(test_df_to_normalize)\nprint(list_of_norm_cols)","7142fba7":"x_train_norm =  train_df_to_normalize[list_of_norm_cols].values #returns a numpy array\nmin_max_scaler = MinMaxScaler()\nx_scaled_train = min_max_scaler.fit_transform(x_train_norm)\ntrain_df_to_normalize[list_of_norm_cols] = x_scaled_train\n\nx_test_norm =  test_df_to_normalize[list_of_norm_cols].values #returns a numpy array\nmin_max_scaler = MinMaxScaler()\nx_scaled_test = min_max_scaler.fit_transform(x_test_norm)\ntest_df_to_normalize[list_of_norm_cols] = x_scaled_test","5c4eb1b3":"test_df_to_normalize","c6a201e5":"train_df_to_normalize['train']  = 1 # if this pointer's value is 1 that means the related data is a train data\ntest_df_to_normalize['train']  = 0\n\n\ndf = pd.concat([train_df_to_normalize, test_df_to_normalize], axis=0,sort=False)","1bf154dc":"one_hot_enc = [col for col in object_cols(new_train_df) if col not in encoding_columns ] # filtering rest of the categorical columns\n\none_hot_df = pd.get_dummies(df[one_hot_enc], drop_first=True) # one hot encoding\n\ndf_final = pd.concat([df, one_hot_df], axis=1, sort=False) # combining one-hot-encoded columns by our dataframe\n\ndf_final = df_final.drop(labels=one_hot_enc, axis=1) # droping columns which we used in one hot encoding","d7a1a8d3":"train_df_final =  df_final[df_final['train'] == 1] # getting train data which we labeled by 1\ntrain_df_final = train_df_final.drop(labels=['train'], axis=1) # dropping 'train' column which we added\n\n\ntest_df_final = df_final[df_final['train'] == 0] # getting test data which we labeled by 0\ntest_df_final = test_df_final.drop(labels=['SalePrice'], axis=1) # dropping 'SalePrice' column\ntest_df_final = test_df_final.drop(labels=['train'], axis=1) # dropping 'train' column which we added","477252cd":"y= train_df_final['SalePrice']\nX = train_df_final.drop(labels=['SalePrice'], axis=1)","e1171aed":"# split data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=50)","64ddfb79":"# kfold = KFold(n_splits=5, shuffle=True, random_state=7)\n# params = {\n#     'n_estimators': [250,500,1000],\n#     'learning_rate': [0.1, 0.2, 0.25, 0.275 ,0.3],\n#     'max_depth' : [2,4,8],\n#     'max_leaf_nodes' : [8, 16, 24, 32]\n    \n\n    \n# }\n\n# grad_boost = GradientBoostingRegressor(criterion = 'mse', max_features = 'auto')\n# clf = GridSearchCV(grad_boost,param_grid=params, verbose=0, cv=kfold, n_jobs=-1)\n# clf.fit(X, y)\n# print(clf.best_score_)\n# print(clf.best_params_)","e7406203":"model = GradientBoostingRegressor(criterion = 'mse', max_features = 'auto', max_depth = 2, n_estimators = 1000, learning_rate = 0.25, \n                                 max_leaf_nodes = 24)\n\nmodel.fit(X_train, y_train)\n# make predictions for test data and evaluate\npredictions = model.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test.values, predictions))\nprint(\"RMSE:\", (rmse))\n# Fit model using each importance as a threshold\nthresholds = np.sort(model.feature_importances_)\nbest_threshold = X_train.shape[1]\nbest_score = rmse\nthresh_list = list()\nn_features_list = list()\nRMSE_list = list()\nfor thresh in thresholds:\n    # select features using threshold\n    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n    select_X_train = selection.transform(X_train)\n    \n    # train model\n    selection_model = GradientBoostingRegressor(criterion = 'mse', max_features = 'auto', max_depth = 2, n_estimators = 1000, \n                                                learning_rate = 0.25, \n                                                max_leaf_nodes = 24)\n    selection_model.fit(select_X_train, y_train)\n    # eval model\n    select_X_test = selection.transform(X_test)\n    predictions = selection_model.predict(select_X_test)\n    score = np.sqrt(mean_squared_error(y_test, predictions))\n    if score < best_score:\n        best_score = score\n        best_threshold = select_X_train.shape[1]\n    \n    \n    thresh_list.append(thresh)\n    n_features_list.append(select_X_train.shape[1])\n    RMSE_list.append(score)\n    print(\"Thresh={}, n={}, RMSE: {}\".format(thresh, select_X_train.shape[1], score))\nprint('Best RMSE: {}, n={}'.format(best_score, best_threshold))","84a56d54":"min_rmse = min(RMSE_list) # findig the best RMSE to draw with another color at our graph\nbest_n_features = n_features_list[RMSE_list.index(min_rmse)] # and related best n_features value\n\n#deleting those elements from list\nRMSE_list.remove(min_rmse)\nn_features_list.remove(best_n_features)\n\nplt.plot(best_n_features, min_rmse, marker='o', color='g')\nplt.plot(n_features_list, RMSE_list, color='y')\n\nplt.xlabel(\"Number of Fatures\")\nplt.ylabel(\"RMSE\")\nplt.show()","56ee9c94":"feature_importance = pd.DataFrame(pd.Series(model.feature_importances_, index=X_train.columns, \n                               name='Feature_Importance').sort_values(ascending=False)).reset_index()\nselected_features = feature_importance.iloc[0:best_threshold]['index']\nselected_features = list(selected_features)","fe87eec6":"# now use the selected features  and fit the model on X and Y\nnew_X = X[selected_features]\nnew_test = test_df_final[selected_features]\n\nmodel.fit(new_X, y)","e7baa140":"# Now lets make predictions on the test dataset for submission\nsubmission_predictions = model.predict(new_test)","0ee70b01":"# prepare a csv file for submission\nsub_df = pd.DataFrame(submission_predictions)\nsub_df['Id'] = test_df['Id']\nsub_df.columns = ['SalePrice', 'Id']\nsub_df = sub_df[['Id', 'SalePrice']]\n\nsub_df.to_csv('submission.csv', index=False)","b02e6911":"Now we will normalize our numerical columns by using MinMaxScaler. Firstly we are seperating our dataframe to original form which is train data and test data.","f42a644d":"object_cols method will help us to find non-numeric columns and numerical_cols method will find the numeric features. Before we start analysis let's learn some fundamental things about our data. ","2f0c477f":"We have finished our preprocessing operations. Now we will seperate our dataframes to train and test dataframe","8c284157":"# Data Processing","eb953d3f":"Before we start let's drop some columns. We know that we have some features which has negative correlation coefficient. Those columns will be dropped.","a106ae2c":"Rest of the categorical columns will be encoded by One-Hot-Encoding","f7678e06":"Now we will combine our dataframes again to implement One-Hot-Encoding","98115b5c":"For the numeric values i have used KNNImputer for n=7 neighbors","12ef48f9":"Now we can continue with some visualizations. Let's start with a heat map to see the correlation between numeric features and target feature which is 'SalePrice'","8e5838d9":"Same as KitchenAbvGr, MSSubClass feature different distributions at different data points. We can not say \"The higher MSSubClass value causes higher SalePrice\". We have examine our data by using some basic plots. Now let's continue with data processing. ","b585f333":"From the heatmap we can easily say that some features such as OverallQual, GrLivArea, GarageCars are affecting SalePrice positively and some features such as KitchenAbvGr, EnclosedPorch is affecting negatively. Let's take a look to the top 3 features which has the highest and lowest correlation. Before we dive in to the features let's draw a pairplot to see the correlation between the features which has the highest 6 correlation coefficient according to the SalePrice","20bc6753":"We still have some null values. For the non-numeric data types this values will be filled by the most frequent item of that column","9cfb3f2d":"Let's draw a line graph to see the change at the RMSE according to the n_features for x = number of features and y = RMSE","444eaad7":"Now we will start encoding operations. According to the 'Data Description File' some columns has values such as 'Y', 'N', 'Ex', 'None' etc. Those values will be replaced with new numeric values in a scale of (0,10). First we will combine our train and test data frames in one dataframe to implement every operation to both of them.","ea47c059":"Let's check the number of null values in colums on a plot by using excellent work of Charles Modingva.","879b1fb0":"And the best estimator of this Regressor is {'learning_rate': 0.25, 'max_depth': 2, 'max_leaf_nodes': 24, 'n_estimators': 1000} with a score of 0.8696218712064999","a3b426d6":"The cell below helps us to finding the best features which will cause the lowest RMSE according to the GradientBoostingRegressor","fd626b80":"As we can predict the higher OverallQual causes higher SalePrice. But there is an exception. OverallQual 1 and OverallQual 2 has nearly same SalePrice values","f1916d49":"As we can see we have a stack at 0 and the other values are distributed independently.","774db3ba":"Let's check our normalization results","e0db10fa":"There is a stack between data points (0,2500) for GrLivArea and also after 3000 we can see some individual data points which has very high sale prices ","b2379383":"There is no any relation between KitchenAbvGr and SalePrice. The highest and the lowest values of KitchenAbvGr has almost same SalePrice values. ","b0367425":"I have used a Gradient Boosting Regressor from sklearn.ensemble. The code below helps you to find the best parameters of a given Gradient Boosting Regressor by using GridSearch with a k-fold Cross Validation for k = 5 ","0f8405a5":"Now let's check the number of empty values in columns of both train and test data","7565df80":"# A Closure Look to The Features","d1c85daf":"According to the 'Data Description File' there are some columns which the null values will be replaced by 'None'. Let's fill that empty values with 'None'","1c849a29":"I have used a predefined method to drop columns which has null values greater than a given threshold (Thank you so much for that method Charles Modingva).","cda5b064":"I expected to see a barplot just like we drawed for 'OverallQual' but it looks like GarageCars 3 houses are more expensive than GarageCars 4 houses.","65873636":"Before we start we will define some methods to use in our work"}}