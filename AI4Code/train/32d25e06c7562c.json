{"cell_type":{"271480af":"code","4cf85452":"code","fd44d35f":"code","0032b9e9":"code","28918347":"code","38079761":"code","bbd49de6":"code","c2395676":"code","9eec021a":"code","b809ee4b":"code","25ab5bbb":"code","df47806f":"code","9efcbed5":"code","fb90d2c1":"code","54ea442e":"code","596672ac":"code","b889a9ba":"code","2dc0f50c":"code","fbaa96d5":"code","52de556f":"code","dbe1e3d2":"code","abf4ac17":"code","8c67abf7":"code","e789d4e9":"code","42d0c78d":"code","2593b475":"code","e9f3e66e":"code","ec23b870":"code","e47b10fd":"code","fba6e406":"code","7ec8d13a":"code","25162939":"code","7f231bbf":"code","3af7e75d":"code","5609e01a":"code","e3180ae2":"code","1a245dab":"code","ac8ca3de":"code","69901128":"code","d25a971b":"code","bcec5e88":"code","2be6aac0":"code","95ab0347":"code","7e22ba7f":"code","391130da":"code","c33742fa":"code","e897e759":"code","e0e82dc1":"markdown","be5ead61":"markdown","14e9ef48":"markdown","b51f4a63":"markdown","b5d2da9b":"markdown"},"source":{"271480af":"# import the libraries as shown below\nfrom tensorflow.keras.layers import Input, Lambda, Dense, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\nfrom tensorflow.keras.models import Sequential\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt","4cf85452":"# re-size all the images to this\nIMAGE_SIZE = [224, 224]","fd44d35f":"# Import the VGG16 library as shown below and add preprocessing layer to the front of VGG\n# Here we will be using imagenet weights\n#include_top = false is basically saying that first layer and last layer is sset by user according to problem and classes they have \n\nvgg16 = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)","0032b9e9":"# don't train existing weights\nfor layer in vgg16.layers:\n    layer.trainable = False","28918347":"# useful for getting number of output classes\nfolders = glob('..\/input\/cotton-disease-dataset\/Cotton Disease\/train\/*')","38079761":"folders","bbd49de6":"len(folders)","c2395676":"# our layers - you can add more if you want\nx = Flatten()(vgg16.output)","9eec021a":"prediction = Dense(len(folders), activation='softmax')(x)\n\n# create a model object\nmodel = Model(inputs=vgg16.input, outputs=prediction)","b809ee4b":"# view the structure of the model\nmodel.summary()","25ab5bbb":"# tell the model what cost and optimization method to use\nmodel.compile(\n  loss='categorical_crossentropy',\n  optimizer='adam',\n  metrics=['accuracy']\n)","df47806f":"# Use the Image Data Generator to import the images from the dataset\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True)\n# We can't scale down to test set iamge\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)","9efcbed5":"# Make sure you provide the same target size as initialied for the image size\ntraining_set = train_datagen.flow_from_directory('..\/input\/cotton-disease-dataset\/Cotton Disease\/train',\n                                                 target_size = (224, 224),\n                                                 batch_size = 32,\n                                                 class_mode = 'categorical')","fb90d2c1":"test_set = test_datagen.flow_from_directory('..\/input\/cotton-disease-dataset\/Cotton Disease\/test',\n                                            target_size = (224, 224),\n                                            batch_size = 32,\n                                            class_mode = 'categorical')","54ea442e":"\n# Run the cell. It will take some time to execute\nrunnning = model.fit_generator(\n  training_set,\n  validation_data=test_set,\n  epochs=20,\n  steps_per_epoch=len(training_set),\n  validation_steps=len(test_set)\n)","596672ac":"import matplotlib.pyplot as plt\n# plot the loss\nplt.plot(runnning.history['loss'], label='train loss')\nplt.plot(runnning.history['val_loss'], label='val loss')\nplt.legend()\nplt.show()\nplt.savefig('LossVal_loss')\n\n# plot the accuracy\nplt.plot(runnning.history['accuracy'], label='train acc')\nplt.plot(runnning.history['val_accuracy'], label='val acc')\nplt.legend()\nplt.show()\nplt.savefig('AccVal_acc')","b889a9ba":"\nfrom tensorflow.keras.models import load_model\n\nmodel.save('model_vgg16.h5')\n","2dc0f50c":"y_pred = model.predict(test_set)","fbaa96d5":"y_pred","52de556f":"import numpy as np\ny_pred = np.argmax(y_pred, axis=1)","dbe1e3d2":"y_pred","abf4ac17":"from tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image","8c67abf7":"model=load_model('model_vgg16.h5')","e789d4e9":"img=image.load_img('..\/input\/cotton-disease-dataset\/Cotton Disease\/val\/diseased cotton plant\/dd (107)_iaip.jpg',target_size=(224,224))","42d0c78d":"x=image.img_to_array(img)\nx","2593b475":"x.shape","e9f3e66e":"\nx=x\/255","ec23b870":"from keras.applications.imagenet_utils import preprocess_input","e47b10fd":"import numpy as np\nx=np.expand_dims(x,axis=0)\nimg_data=preprocess_input(x)\nimg_data.shape","fba6e406":"model.predict(img_data)\n","7ec8d13a":"a=np.argmax(model.predict(img_data), axis=1)","25162939":"a","7f231bbf":"a==0","3af7e75d":"img=image.load_img('..\/input\/cotton-disease-dataset\/Cotton Disease\/test\/fresh cotton leaf\/d (122)_iaip.jpg',target_size=(224,224))","5609e01a":"x=image.img_to_array(img)\nx","e3180ae2":"x.shape","1a245dab":"\nx=x\/255","ac8ca3de":"import numpy as np\nx=np.expand_dims(x,axis=0)\nimg_data=preprocess_input(x)\nimg_data.shape","69901128":"model.predict(img_data)\n","d25a971b":"a=np.argmax(model.predict(img_data), axis=1)","bcec5e88":"a","2be6aac0":"a==0","95ab0347":"test_image = image.load_img('..\/input\/cotton-disease-dataset\/Cotton Disease\/test\/fresh cotton leaf\/d (122)_iaip.jpg',\n                            target_size = (224,224))\ntest_image = image.img_to_array(test_image)\ntest_image=test_image\/255\ntest_image = np.expand_dims(test_image, axis = 0)\nresult = model.predict(test_image)","7e22ba7f":"test_image.shape","391130da":"result","c33742fa":"result=np.argmax(model.predict(test_image), axis=1)","e897e759":"result","e0e82dc1":"# Visualization of loss and accuracy","be5ead61":"# fit the model","14e9ef48":"# Adding the last layer to model","b51f4a63":"# Data Augmentation Apply for setting image ","b5d2da9b":"# Test The Model using Different Images "}}