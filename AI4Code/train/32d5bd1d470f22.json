{"cell_type":{"a735f70f":"code","3699ca9d":"code","35180ee3":"code","7790c824":"code","747c622e":"code","ca6ca167":"code","a21bce5c":"code","83b1b7df":"code","877848db":"code","3c6c4aa7":"code","04f45e98":"code","35bbd835":"code","f854555b":"code","f04f5b18":"markdown","1757e474":"markdown","93e9b420":"markdown","90638aee":"markdown","e22bc67f":"markdown","d327b76d":"markdown","ed1e58d8":"markdown","3861e505":"markdown","98cc2978":"markdown","6a9dac86":"markdown","208e3071":"markdown","1f86a4ee":"markdown","b0d96848":"markdown","5976528b":"markdown","15959777":"markdown","12386624":"markdown","d7a8cf6b":"markdown","41e21896":"markdown","db64a39b":"markdown","a4b31d12":"markdown","29130845":"markdown","9ee3b08b":"markdown","d681fb3f":"markdown","965124aa":"markdown","eddf70f3":"markdown","91dae07d":"markdown","64cb65e1":"markdown","f8244ebd":"markdown","78359931":"markdown","affa9658":"markdown","51013573":"markdown","b9d2cb11":"markdown","3db292e6":"markdown","874d92a5":"markdown","f5c4f139":"markdown","76bb072c":"markdown","14829b25":"markdown","b20e8ebf":"markdown"},"source":{"a735f70f":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = 15, 6\n\nfrom sklearn.preprocessing import LabelEncoder\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.cluster import KMeans","3699ca9d":"# Upload data\nmovies = pd.read_csv('..\/input\/movies.csv', encoding = \"ISO-8859-1\")\n\n# Have a look on the data structure and types\nmovies.head()","35180ee3":"# Check if there are missing observations\nmovies.info()","7790c824":"for index in ['budget', 'gross', 'runtime', 'score', 'votes', 'year']:\n    print(index, 'min =', movies[index].min(), '&', index, 'max =', movies[index].max())","747c622e":"len(movies[movies['budget'] == 0])","ca6ca167":"labelencoder_X = LabelEncoder()\nmovies.loc[:, 'company'] = labelencoder_X.fit_transform(movies.loc[:, 'company'])\nmovies.loc[:, 'country'] = labelencoder_X.fit_transform(movies.loc[:, 'country'])\nmovies.loc[:, 'director'] = labelencoder_X.fit_transform(movies.loc[:, 'director'])\nmovies.loc[:, 'genre'] = labelencoder_X.fit_transform(movies.loc[:, 'genre'])\nmovies.loc[:, 'name'] = labelencoder_X.fit_transform(movies.loc[:, 'name'])\nmovies.loc[:, 'rating'] = labelencoder_X.fit_transform(movies.loc[:, 'rating'])\nmovies.loc[:, 'star'] = labelencoder_X.fit_transform(movies.loc[:, 'star'])\nmovies.loc[:, 'writer'] = labelencoder_X.fit_transform(movies.loc[:, 'writer'])","a21bce5c":"# Break down (released date) into (year, month & day)\nmovies['rel_year'] = movies['released'].apply(lambda x: x[:4]).astype(int)\nmovies['rel_month'] = 0\nmovies['rel_day'] = 0\n\n# Update 'rel_month' and 'rel_day' columns with the corresponding values from 'released' feature\nfor index in range(0, len(movies)):\n    if len(movies.released[index]) == 10:\n        movies.loc[index, 'rel_month'] = movies.loc[index, 'released'][5:7]\n        movies.loc[index, 'rel_day'] = movies.loc[index, 'released'][8:10]\n        \n# For 'released' observations with length less than 10, we can update their corresponding 'rel_month' & 'rel_day' with median\nmonth_avg = movies[movies['rel_month'] != 0]['rel_month'].median()\nday_avg = movies[movies['rel_day'] != 0]['rel_day'].median()\n\n# Update observations with length less than 10 with the average of each feature\nfor index in range(0, len(movies)):\n    if len(movies.released[index]) != 10:\n        movies.loc[index, 'rel_month'] = month_avg\n        movies.loc[index, 'rel_day'] = day_avg\n\n# Convert 'rel_month' & 'rel_day' types to (int)\nmovies['rel_month'] = movies['rel_month'].astype(int)\nmovies['rel_day'] = movies['rel_day'].astype(int)\n\n# Delete 'released' feature since it has no importance now\ndel(movies['released'])\n\n# Replace zero 'budget' observations with 'budget' median\nbudget_avg = movies['budget'].median()\n\nfor index in range(0, len(movies)):\n    if movies.budget[index] == 0:\n        movies.loc[index, 'budget'] = budget_avg\n\n# Create a copied dataset to to group some of its features' values for a better visualization through different cross tables\ndata = movies.copy()\n\n# Group the values of year feature\nfor number in range(1980, 2020, 4):\n    data.loc[(data['year'] > number) & (data['year'] <= (number + 4)), 'year'] = number + 4","83b1b7df":"# Create lists that hold features according to each needed computation (total, sum or average)\ntotal = ['company', 'country', 'director', 'genre', 'name', 'star', 'writer', 'rating']\nsumming = ['budget', 'gross', 'votes']\naver = ['runtime', 'score']\n\n# Display each feature's progress over consecutive time intervals (4 years) using the relevant computation\nfor index in range(0, len(total)):\n    print(\"Progress of '%s' over time:\\n\" % total[index])\n    for year in data.year.unique():\n        print('Total unique %s in the interval ending in %i is: %i' % (total[index], year, data[data['year'] == year][total[index]].nunique()))\n    print('\\n')\n\nprint('----------------------------------------------------------------------------\\n')\n\nfor index in range(0, len(summing)):\n    print(\"Progress of '%s' over time:\\n\" % summing[index])\n    for year in data.year.unique():\n        print('The sum of %s in the interval ending in %i is: %i million' % (summing[index], year, data[data['year'] == year][summing[index]].sum() \/ 1000000))\n    print('\\n')\n\nprint('----------------------------------------------------------------------------\\n')\n\nfor index in range(0, len(aver)):\n    print(\"Progress of '%s' over time:\\n\" % aver[index])\n    for year in data.year.unique():\n        print('%s median in the interval ending in %i is: %i' % (aver[index], year, data[data['year'] == year][aver[index]].median()))\n    print('\\n')","877848db":"# Set the dimensions of the headmap\nplt.figure(figsize=(15,12))\n\n# Set a title and plot the heatmap\nplt.title('Correlation of Features', fontsize=20)\nsns.heatmap(data.corr().astype(float).corr(),vmax=1.0, annot=True)\nplt.show()","3c6c4aa7":"# Define the function with it's parameters (below is the description of each parameter):\n\n# data: The data that will be used to train the model in order to form the clusters\n# features: Which features from the dataset will be considered in the clustering\n# display_clusters: Specify two features to check how clustering between them looks like\n# clusters: If 0, the model will calculate the best number of clusters to be used. If a value has been set manually, the user can decide the number of clusters s\/he wants to cluster upon\n# predict: If (True), the model will check which cluster each observation in the sample dataset (new_data) belongs to\n# display_methods: If (True), the elbow and dendrogram graphs \n# centroids: Display the centroid of each cluster\n\ndef clusters (data = pd.DataFrame(), features = [], display_clusters = [], clusters = 0, predict = False, display_methods = False, centroids = False):\n    \n    # Create a list of features from the provided data's columns' in case the features are not provided as argument\n    if (len(features) == 0):\n        features = []\n        for index in range(0, len(data.columns)):\n                features.append(data.columns[index])\n\n    # Consider only the specified features if the user placed a value for it while calling the function \n    else:\n        data = data.loc[:, features]\n    \n    # Create 'X' object\n    X = data.values\n    \n    # List to store the within clusters sum of squares (wcss) for each number of clusters to check at which number of clusers the value of wcss will not be decreased significantly anymore\n    wcss = []\n    \n    # Set the range that will be used in several steps below in case of manually defining number of clusters \n    if clusters != 0:\n        clust_range = range(1, clusters + 1)\n    \n    # Set the range that will be used in several steps below in case the model will detect the best number of clusters\n    else:\n        clust_range = range(1, 11)\n\n    # Detect the value of wcss corresponding to each number of clusters within the specified range above \n    for i in clust_range:\n        kmeans = KMeans(n_clusters = i, init = 'random', max_iter = 300, n_init = 10, random_state=0)\n        kmeans.fit(X)\n        wcss.append(kmeans.inertia_)\n\n    # List that will store the differences in wcss percentages' changes in order to determine the best number of clusters in which any number of clusters after it won't drop the wcss value significantly\n    difference = []\n    \n    # determine the best number of clusters (in case it's not set manually)\n    if (clusters == 0): \n        for i in range(0, 8):\n            difference.append((1 - (wcss[i + 1] \/ wcss[i])) - (1 - (wcss[i + 2] \/ wcss[i + 1])))\n        clusters = difference.index(max(difference)) + 2\n    \n    # Create an object with the corresponding cluster for each observation\n    kmeans = KMeans(n_clusters = clusters, init = 'random', max_iter = 300, n_init = 10, random_state = 0)\n    y_kmeans = kmeans.fit_predict(X)\n    \n    # List of colors that will differentiate each cluster\n    color = ['red', 'blue', 'green', 'brown', 'blueviolet', 'black', 'lightgrey', 'olive', 'peru', 'yellow']\n    \n    # Display a graph that will show the clusters associated with the observations of any specified two features    \n    if (len(display_clusters) == 2):\n        for feature in features:\n            if display_clusters[0] == feature:\n                for feature in features:\n                    if display_clusters[1] == feature:\n                        for cluster in range(0, clusters):\n                            plt.scatter(X[y_kmeans == cluster, features.index(display_clusters[0])], X[y_kmeans == cluster, features.index(display_clusters[1])], s = 50, c= color[cluster], label = str(cluster))\n                        if (centroids):\n                            plt.scatter(kmeans.cluster_centers_[:, features.index(display_clusters[0])], kmeans.cluster_centers_[:, features.index(display_clusters[1])], s = 100, c= 'orange', label = 'Centroids')\n                        plt.title('Clustering %s & %s' % (display_clusters[0], display_clusters[1]))\n                        plt.xlabel(display_clusters[0])\n                        plt.ylabel(display_clusters[1])\n                        plt.legend()\n                        plt.show()\n    \n    # Display the elbow method and dendrogram graphically (if display_methods = True)\n    if (display_methods):\n        \n        plt.plot(clust_range, wcss)\n        plt.title('The Elbow Method')\n        plt.xlabel('Number of Clusters')\n        plt.ylabel('WCSS')\n        plt.show()\n\n        dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n        plt.title('Dendrogram')\n        plt.ylabel('Euclidean distances')\n        plt.show()\n\n    # Predict the cluster of each observation in the sample dataset 'new_data' (if predict = True)\n    if (predict):\n        global new_data\n        new_data = data.copy()\n        new_data['cluster'] = kmeans.predict(new_data.values)\n        display(new_data)\n        print('\\nThe above dataset is stored under the name of \"new_data\"')\n    ","04f45e98":"clusters(data = movies, features = ['budget', 'year'], display_clusters = ['budget', 'year'], display_methods = True)\n","35bbd835":"clusters(data = movies, display_clusters = ['gross', 'year'], clusters = 4, centroids = True)","f854555b":"clusters(data = movies, features = ['gross', 'votes', 'writer'], clusters = 5, predict = True)","f04f5b18":"#### Let's start off the journey of finding the answers by uploading the data and have a look over the different features:","1757e474":"#### According to the above counting, there are no missing observation within our movie dataset. Only \"budget\" observations with zero value needs to be updated.","93e9b420":"* **1. Features Attributes & Exploration:**\n  * 1.1 Attributes.<br>\n  * 1.2 Project Question.<br>\n* **2. Features Engineering:**\n  * 2.1 Features Exploration.<br>\n  * 2.2 Features Engineering.<br>\n* **3. Analysis of Features Over Time:**\n  * 3.1 Features Changes over Time.<br>\n  * 3.2 Features Correlation with Time (year).<br>\n  * 3.3 Analysis Conclusion.<br>\n* **4. Observations Clustering:**\n  * 4.1 Define a Clustering Function.<br>\n  * 4.2 Test the Function.<br>","90638aee":"# 4. Observations Clustering","e22bc67f":"#### Lastly, create a dataset with the corresponding cluster for each observation according to the specified features:","d327b76d":"---","ed1e58d8":"# 1. Features Attributes & Exploration","3861e505":"---","98cc2978":"# Table of Contents:","6a9dac86":"#### As a conclusion, to answer a question like whether movie industry is dying or not, if we decide to measure the progress of the industry according to the changes of any of the available features over time, we will find that almost all features are booming with different rates over time. This can be conceived as a flourishing sign for the movie industry. ","208e3071":"## 3.2 Features Correlation with Time (year):","1f86a4ee":"## 2.2 Features Engineering:","b0d96848":"---","5976528b":"## 2.1 Features Exploration:","15959777":"---","12386624":"#### It's time to test the function with different scenarios:","d7a8cf6b":"---","41e21896":"## 1.2 Project Question:","db64a39b":"## 1.1 Attributes:","a4b31d12":"# 2. Features Engineering","29130845":"#### As seen above, we can use the function to display the needed visualization or predict the clusters according to our needs.","9ee3b08b":"#### In the below scenario, there was a manual specification to the number of clusters we wanted to fit our observations to and displayed the centroid of each cluster:","d681fb3f":"#### From the above simple analysis, we can conclude the following:<br>\n\n1- Number of movies' making companies is increasing over time.<br>\n2- Number of movies' countries of origin is increasing over time.<br>\n3- Directors are increasing slowly each new interval.<br>\n4- The progress of movies genre was slightly decreased but started to slightly increase again as of 2008.<br>\n5- According to the provided data, number of movies released in each interval has been almost the same since 1992.<br>\n6- Number of main actor\/actress (stars) for each interval hasn't been dramatically changing over different intervals.<br>\n7- The percentage of writers writing each interval's movies has been decreasing with a very low rate since 2000.<br>\n8- Number of ratings in each interval has been almost consistent.<br>\n9- Total budget and gross (as well as profit) were increasing with a few billions of dollars over time.<br>\n10- Number of users' votes was dramatically increasing up till the last interval (2012-2016) in which it started to decrease.<br>\n11- The average duration of movies (runtime) were slightly increasing.<br>\n12- Average scores on IMDb given by users for each interval's movies was almost steady.<br>","965124aa":"---","eddf70f3":"## 4.1 Define a Clustering Function:","91dae07d":"#### Here, we specified the dataset and features that will be involved in the clustering process, features that we want to visually check their corresponding clusters, as well as visualizing the graphs for elbow method and dendrogram used to determine the best number of clusters:","64cb65e1":"## 3.1 Features Changes over Time:","f8244ebd":"## Importing Libraries:","78359931":"#### By having a look over the data, we can conclude that some amendments can further smooth the analysis, like breaking down released date (which might be used in the last step \"clustering\"), replace zero budgets with median, and create a new copied dataset that will have a grouped 'year' feature as a 4 years intervals which will be used shortly in the analysis","affa9658":"# 3. Analysis of Features Over Time","51013573":"#### Let's now define a function that can help in clustering the observations according to any features we decide:","b9d2cb11":"#### Another side to look at the progress could be through checking the correlation between year and different original quantitative features (features that havn't been converted \"encoded\" from categorical to quantitative) as displayed below (they all have a positive correlation).","3db292e6":"## 4.2 Test the Function:","874d92a5":"#### Is the movie industry dying? How can we categorize movies according to different features?","f5c4f139":"#### Now, let's encode our categorical data to be able to use it in the upcoming analysis:","76bb072c":"#### Having a look on the progress of different features over time might seem a good idea","14829b25":"## 3.3 Analysis Conclusion:","b20e8ebf":"#### There are 6820 movies in the dataset (220 movies per year, 1986-2016). Each movie has the following attributes:\n\n* budget: the budget of a movie. Some movies don't have this, so it appears as 0<br>\n* company: the production company<br>\n* country: country of origin<br>\n* director: the director<br>\n* genre: main genre of the movie<br>\n* gross: revenue of the movie<br>\n* name: name of the movie<br>\n* rating: rating of the movie (R, PG, etc.)<br>\n* released: release date (YYYY-MM-DD)<br>\n* runtime: duration of the movie<br>\n* score: IMDb user rating<br>\n* votes: number of user votes<br>\n* star: main actor\/actress<br>\n* writer: writer of the movie<br>\n* year: year of release<br>"}}