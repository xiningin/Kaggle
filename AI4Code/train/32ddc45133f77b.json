{"cell_type":{"660983c3":"code","4a4e467c":"code","a081b7f1":"code","3cda7193":"code","38c62ed7":"code","d33f17a5":"code","a5e81a03":"code","053a2c32":"code","c95665e3":"code","04e69e0a":"code","281a5dc8":"code","d2b31704":"code","8fd32b2d":"code","6f553090":"code","5802c0cb":"code","036d2d21":"code","576b8d8a":"code","c56c8dc2":"code","da462705":"code","8ec18495":"code","e444caa3":"code","8f1db293":"code","cd7e31f0":"code","bd9a0158":"code","a58be2ea":"code","9d78fa6e":"code","b5d5f37e":"code","641933c2":"code","678c37d9":"code","3c2ebf79":"code","8f88b6c0":"code","fb128371":"code","5d47a8d8":"code","75f19ebe":"code","e1ff7d00":"code","96bd9c73":"code","e27af74c":"code","04ac6a2a":"code","87cfa553":"code","c39d2116":"code","76e41100":"code","c80271c5":"code","c43c8b1e":"code","3f8237c2":"code","4d9260f8":"markdown","c7df76ab":"markdown","77640ea7":"markdown","923abc51":"markdown","1d470861":"markdown","e7d83c26":"markdown","9e2e0f7c":"markdown"},"source":{"660983c3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4a4e467c":"train_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","a081b7f1":"train_df.info()","3cda7193":"train_df['text'][5700]","38c62ed7":"text = np.asarray(train_df['text']) # Converting to numpy array","d33f17a5":"target = np.asarray(train_df['target'])","a5e81a03":"from keras.preprocessing.text import Tokenizer","053a2c32":"max_vocab = 10000 # Defining max_vocab and max_len variable's\nmax_len = 500","c95665e3":"tokenizer = Tokenizer(num_words=max_vocab)\ntokenizer.fit_on_texts(text)\nsequences = tokenizer.texts_to_sequences(text)","04e69e0a":"from keras.preprocessing.sequence import pad_sequences","281a5dc8":"word_index = tokenizer.word_index\ndata = pad_sequences(sequences, maxlen=max_len)","d2b31704":"data # Data is converted to vector","8fd32b2d":"word_index","6f553090":"train_samples = int(len(text)*0.8) # Using 80% of data for training ","5802c0cb":"train_samples","036d2d21":"text_train = data[:train_samples]\ntarget_train = target[:train_samples]","576b8d8a":"text_test = data[train_samples:len(text)-2] # Using 20% data for evaluation, will be rebuilding model with full data\ntarget_test = target[train_samples:len(text)-2]","c56c8dc2":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout,GRU, SimpleRNN","da462705":"embedding_mat_columns=32\nmodel = Sequential()\nmodel.add(Embedding(input_dim=max_vocab,\n output_dim=embedding_mat_columns,\n input_length=max_len))\nmodel.add(LSTM(units=embedding_mat_columns))\nmodel.add(Dropout(0.5)) # Adding Dropout layers to avoid overfitting\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy',\n metrics=['acc'])\nmodel.summary()","8ec18495":"from tensorflow.keras.callbacks import EarlyStopping","e444caa3":"early_stop = EarlyStopping(monitor='val_loss',patience=2) # Using Early Stop to stop training","8f1db293":"model.fit(text_train, target_train, epochs=10, batch_size=24, validation_data=(text_test,target_test), callbacks=[early_stop])","cd7e31f0":"losses = pd.DataFrame(model.history.history)","bd9a0158":"losses[['loss','val_loss']].plot()","a58be2ea":"acc = model.evaluate(text_test, target_test)\nprint(\"Test loss is {0:.2f} accuracy is {1:.2f} \".format(acc[0],acc[1]))","9d78fa6e":"model.fit(data, target, epochs=5, batch_size=24,)","b5d5f37e":"test_df","641933c2":"test_text = np.asarray(test_df['text'])","678c37d9":"tokenizer = Tokenizer(num_words=max_vocab)\ntokenizer.fit_on_texts(test_text)\ntest_sequences = tokenizer.texts_to_sequences(test_text)","3c2ebf79":"word_index = tokenizer.word_index\ntest_data = pad_sequences(sequences, maxlen=max_len)","8f88b6c0":"pred_label = model.predict(test_data)","fb128371":"pred_label","5d47a8d8":"pred_label[3000]","75f19ebe":"test_df['text'][3000]","e1ff7d00":"submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","96bd9c73":"probabilities = pd.DataFrame(data=pred_label)","e27af74c":"submission['probabilities'] = probabilities","04ac6a2a":"submission = submission.drop('target',axis=1)","87cfa553":"submission[submission['probabilities'] < 0.5] =  0  # If probability is less than 0.5 it will return 0","c39d2116":"submission[submission['probabilities'] > 0.5] = 1","76e41100":"submission","c80271c5":"submission['target'] = submission['probabilities']\nsubmission = submission.drop('probabilities', axis=1)","c43c8b1e":"submission['target'] = submission['target'].astype('int64')","3f8237c2":"submission.to_csv('.\/submission.csv', index=False)","4d9260f8":"**Building the model**","c7df76ab":"**Processing data using Tokenizer**","77640ea7":"The model has accuracy of around 76%","923abc51":"The model has over-fitting due to lack of data","1d470861":"**Predicting the test data**","e7d83c26":"**Retraining the model with full dataset**","9e2e0f7c":"**Preparing for Submission**"}}