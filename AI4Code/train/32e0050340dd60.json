{"cell_type":{"0ad0fb8f":"code","da21dffa":"code","82bc6a11":"code","99d2be4c":"code","d8fece26":"code","72f3e099":"code","98478dbf":"code","86e8e2bb":"code","fafc3d4a":"code","c1007133":"code","600ac3b1":"code","9c95c31b":"code","751df112":"code","62a397c6":"code","4ff19187":"code","399f4044":"code","60c004a7":"code","cdbbae07":"code","ca1bf092":"code","a1be590a":"code","60d1cf90":"code","1fabb013":"code","070f8cad":"code","822d0711":"code","8d749913":"code","49379bcc":"code","b34bfcf9":"code","7a6e0250":"code","5f9eb832":"code","768a6129":"markdown","79665fdb":"markdown","2ce275f9":"markdown","4d9a60cb":"markdown","b2f9a4e7":"markdown","e1cc47be":"markdown","38364430":"markdown"},"source":{"0ad0fb8f":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","da21dffa":"df = pd.read_csv(\"\/kaggle\/input\/health-care-data-set-on-heart-attack-possibility\/heart.csv\")\ndf.head()","82bc6a11":"df.shape","99d2be4c":"df.isna().sum()","d8fece26":"sns.countplot(df['age'])","72f3e099":"sns.countplot(df['target'], label='count')","98478dbf":"df.describe()","86e8e2bb":"df.info()","fafc3d4a":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))","c1007133":"from sklearn import model_selection\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier","600ac3b1":"normalizedData = scaler.fit_transform(df)\nX = normalizedData[:,0:13]\nY = normalizedData[:,13]","9c95c31b":"print(X)","751df112":"kfold = model_selection.KFold(n_splits=10, random_state=7)\ncart = DecisionTreeClassifier()\nnum_trees = 100\nmodel = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=7)\nresults = model_selection.cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","62a397c6":"from sklearn.ensemble import AdaBoostClassifier\nseed = 10\nnum_trees = 60\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\nmodel = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\nresults = model_selection.cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","4ff19187":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\n\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\n# create the sub models\nestimators = []\nmodel1 = LogisticRegression()\nestimators.append(('logistic', model1))\nmodel2 = DecisionTreeClassifier()\nestimators.append(('cart', model2))\nmodel3 = SVC()\nestimators.append(('svm', model3))\n# create the ensemble model\nensemble = VotingClassifier(estimators)\nresults = model_selection.cross_val_score(ensemble, X, Y, cv=kfold)\nprint(results.mean())","399f4044":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=1\/3, random_state = 21)","60c004a7":"def models(x_train, y_train):\n    \n    #Logistic Regression\n    from sklearn.linear_model import LogisticRegression\n    log = LogisticRegression(random_state = 0)\n    log.fit(x_train, y_train)\n    \n    #Decision Tree\n    from sklearn.tree import DecisionTreeClassifier\n    tree = DecisionTreeClassifier(criterion='entropy', random_state = 0)\n    tree.fit(x_train, y_train)\n    \n    #Support Vector Machines\n    from sklearn.svm import SVC\n    svm = SVC(kernel='linear')\n    svm.fit(x_train,y_train)\n    \n    #Naive Bayes\n    from sklearn.naive_bayes import GaussianNB \n    gnb = GaussianNB() \n    gnb.fit(x_train, y_train) \n    \n    #Bagging Classifier\n    from sklearn.neighbors import KNeighborsClassifier  \n    knn = KNeighborsClassifier(n_neighbors=10, algorithm='kd_tree', metric='minkowski', p=5)  \n    knn.fit(x_train, y_train)  \n    from sklearn.ensemble import BaggingClassifier\n    bag = BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)\n    bag.fit(x_train, y_train)\n    \n    print(\"Logistic Regression Training Accuracy:\", log.score(x_train, y_train))\n    print(\"Decision Tree Classifier Training Accuracy:\", tree.score(x_train, y_train))\n    print(\"SVM Training Accuracy:\", svm.score(x_train, y_train))\n    print(\"Naive Bayes Training Accuracy:\", gnb.score(x_train, y_train))\n    print(\"Bagging Classifier Training Accuracy:\", knn.score(x_train, y_train))\n    \n    return log, tree, svm, gnb, bag","cdbbae07":"model = models(x_train, y_train)","ca1bf092":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nprint(\"Logistic Regression: \")\nprint()\nprint(classification_report(y_test, model[0].predict(x_test)))\nprint(accuracy_score(y_test, model[0].predict(x_test)))","a1be590a":"print(\"Decision Tree Classifier: \")\nprint()\nprint(classification_report(y_test, model[1].predict(x_test)))\nprint(accuracy_score(y_test, model[1].predict(x_test)))","60d1cf90":"print(\"Support Vector Machines Classifier: \")\nprint()\nprint(classification_report(y_test, model[2].predict(x_test)))\nprint(accuracy_score(y_test, model[2].predict(x_test)))","1fabb013":"print(\"Naive Bayes Classifier: \")\nprint()\nprint(classification_report(y_test, model[3].predict(x_test)))\nprint(accuracy_score(y_test, model[3].predict(x_test)))","070f8cad":"print(\"Bagging Classifier: \")\nprint()\nprint(classification_report(y_test, model[4].predict(x_test)))\nprint(accuracy_score(y_test, model[4].predict(x_test)))","822d0711":"print(\"Test Accuracy: \")\nprint(\"Logistic Regression:\", accuracy_score(y_test, model[0].predict(x_test))*100)\nprint(\"Decision Tree Classifier:\", accuracy_score(y_test, model[1].predict(x_test))*100)\nprint(\"Support Vector Machines (SVM):\", accuracy_score(y_test, model[2].predict(x_test))*100)\nprint(\"Naive Bayes:\", accuracy_score(y_test, model[3].predict(x_test))*100)\nprint(\"Bagging Classifier: \", accuracy_score(y_test, model[4].predict(x_test))*100)","8d749913":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# create the sub models\nestimators = []\nmodel1 = LogisticRegression()\nestimators.append(('logistic', model1))\nmodel2 = DecisionTreeClassifier()\nestimators.append(('decision', model2))\nmodel3 = SVC()\nestimators.append(('svm', model3))\nmodel4 = GaussianNB()\nestimators.append(('naive', model4))\nmodel5 = BaggingClassifier()\nestimators.append(('bag', model5))\n# create the ensemble model\nensemble = VotingClassifier(estimators)\nresults = model_selection.cross_val_score(ensemble, x_test, y_test)\nprint(results.mean()*100)","49379bcc":"logistic = accuracy_score(y_test, model[0].predict(x_test))*100\ndecision = accuracy_score(y_test, model[1].predict(x_test))*100\nsvm = accuracy_score(y_test, model[2].predict(x_test))*100\nnaive = accuracy_score(y_test, model[3].predict(x_test))*100\nbag = accuracy_score(y_test, model[4].predict(x_test))*100\nvoting = results.mean()*100\nli = [logistic, decision, svm, naive, bag, voting]\n\nplt.bar(['Logistic', 'Decision', 'SVM', 'Naive', 'Bag', 'Voting'], li)\nplt.xlabel('Models')\nplt.ylabel('Accuracy')\nplt.title('Final Accuracy of the individual models')\nplt.legend()\nplt.show()","b34bfcf9":"import keras\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense, Activation, Flatten, BatchNormalization, Conv2D, MaxPooling2D, Dropout\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nmodel = Sequential()\n\nmodel.add(Dense(8))\nmodel.add(Dropout(0.5))\nmodel.add(Activation('relu'))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics=['accuracy'])\n\nprint('<< Compiling Model >>')\n\nhistory_1 = model.fit(x_train,y_train ,batch_size = 32 ,epochs = 300)\ny_pred_1 = model.predict(x_test)\ny_pred_1 = (y_pred_1 > 0.5)","7a6e0250":"plt.plot(history_1.history['accuracy'], color='red')\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Accuracy'], loc='upper left')\nplt.show()","5f9eb832":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred_1)\nprint(cm)\n\nprint(accuracy_score(y_test, y_pred_1))\nann = 100*accuracy_score(y_test,y_pred_1)\nprint('percentage Accuracy : ',ann)","768a6129":"More updates soon.\n\nPlease leave an Upvote.","79665fdb":"First Ensemble with Cross-Val scores","2ce275f9":"Preparing Models","4d9a60cb":"Finally, an ANN (Artificial Neural Network) to compare all of them","b2f9a4e7":"Second Ensemble","e1cc47be":"Performing Cross-Validation and Ensemble Learning using AdaBoostClassifier","38364430":"Model Accuracy of ANN"}}