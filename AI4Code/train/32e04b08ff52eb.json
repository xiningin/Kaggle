{"cell_type":{"677f4910":"code","359b19fd":"code","24b2c2cf":"code","8243d4dd":"code","6c57683f":"code","a673e101":"code","95ed57a5":"code","90426e23":"code","ed0b7d87":"code","cb7dd0d7":"code","ebeeafd4":"code","fdf35092":"code","9cbb9722":"code","58ee866e":"code","e25e3eb0":"code","ed1419e7":"code","93bab8b3":"code","2bfbd3f2":"code","1c0d457b":"code","af10b24f":"code","1a9a667e":"code","22b7f2e2":"code","ad315f33":"code","c75da21a":"code","16958fb7":"code","700e64b2":"code","24e97f7c":"code","7f8b6eb5":"code","7fb0455f":"code","a697e697":"code","54514c87":"code","a3857835":"markdown","a7218311":"markdown","0f25e1e5":"markdown","3374faff":"markdown","e4a1be91":"markdown","925bd592":"markdown","4a620358":"markdown","ea7314e0":"markdown","a27f5b1e":"markdown","081ed919":"markdown","dcfd66ed":"markdown","2b1738fa":"markdown","a0aa8aaa":"markdown"},"source":{"677f4910":"# This notebook is a systematic display of application of Naive Bayes Algortihm on Sentiment Analysis\n\n# There is a comparative study in the end as to how the accuracy changes as we change the \"Smoothing Parameter\" in Naive Bayes\n\n# The notebook represents the results for \"amazon_cells_labelled\" data-set. Results for other data-set can alos be produced in the same way and provides almost same results.\n\n# Everything is written simply using Python, without use of any fancy Libraries","359b19fd":"import numpy as np \nimport re\nimport math\nimport random\nimport matplotlib.pyplot as plt","24b2c2cf":"data_set = []\nlabels = []\ndata_path1 = '\/kaggle\/input\/sentiment-labelled-sentences-data-set\/sentiment labelled sentences\/sentiment labelled sentences\/amazon_cells_labelled.txt'\ndata_path2 = 'imdb_labelled.txt'\ndata_path3 = 'yelp_labelled.txt'\nwith open(data_path1) as dp:\n    line = dp.readline()\n    while line:\n        data_set.append(line.strip())\n        line = dp.readline()","8243d4dd":"def createData(dataset):\n    d = [i[:-3] for i in dataset]\n    d = [item.lower() for item in d]\n    for i in range(len(d)):\n        d[i] = re.sub('[^A-Za-z0-9-\\s]+', '', d[i])\n    l = [i[-1:] for i in dataset]\n    for i in range(0, len(l)): \n        l[i] = int(l[i])\n    dataset = list(zip(d , l))\n    \n    return dataset","6c57683f":"dataset = createData(data_set)","a673e101":"def classDistribution(dataset):\n    \n    dataset_class_1 = []\n    dataset_class_0 = []\n    \n    for i in range(len(dataset)):\n        if dataset[i][1] == 1:\n            dataset_class_1.append(dataset[i])\n        else:\n            dataset_class_0.append(dataset[i])\n            \n    dataset_class_1 = random.sample(dataset_class_1,len(dataset_class_1))\n    \n    dataset_class_0 = random.sample(dataset_class_0,len(dataset_class_0))\n    \n    return dataset_class_0, dataset_class_1","95ed57a5":"dataset_class_0, dataset_class_1 = classDistribution(dataset)","90426e23":"def train_test_split(dataset, value):\n    index = int(value*len(dataset))\n    train = dataset[:index]\n    test = dataset[index:]\n    \n    return train, test","ed0b7d87":"train_1, test_1 = train_test_split(dataset_class_1,0.8)\ntrain_0, test_0 = train_test_split(dataset_class_0,0.8)","cb7dd0d7":"def generate_token(data):\n    c_words = {}               # Vocabulary of words in a class\n    total_words = 0            # Total words in a class\n    for i in range(len(data)):\n        for word in data[i][0].split(\" \"):\n            total_words += 1\n            if(word in c_words):\n                c_words[word] +=1\n            else:\n                c_words[word] = 1\n    return c_words,total_words","ebeeafd4":"c_words_1,total_words_1 = generate_token(train_1)\nc_words_0,total_words_0 = generate_token(train_0)","fdf35092":"train = train_1 + train_0\ntrain_ran = random.sample(train,len(train))\n\ntest = test_1 + test_0\ntest_ran = random.sample(test,len(test))","9cbb9722":"train_data = []\ntrain_label = []\nfor i in range(len(train_ran)):\n    train_data.append(train_ran[i][0])\n    train_label.append(train_ran[i][1])\n","58ee866e":"test_data = []\ntest_label = []\nfor i in range(len(test_ran)):\n    test_data.append(test_ran[i][0])\n    test_label.append(test_ran[i][1])","e25e3eb0":"def create_dict(data):\n    \n    vocab = {}\n    total_words = 0\n    \n    for i in range(len(data)):\n        for word in data[i][0].split(\" \"):\n            total_words += 1\n            if(word in vocab):\n                vocab[word] +=1\n            else:\n                vocab[word] = 1\n    return vocab,total_words","ed1419e7":"vocab,total_words = create_dict(train)","93bab8b3":"def generate_prior(train_class,data):\n    maxL = math.log(len(train_class)\/(len(data)))\n    \n    return maxL","2bfbd3f2":"maxL_0 = generate_prior(train_0,train)\nmaxL_1 = generate_prior(train_1,train)","1c0d457b":"def prediction(maxL_0,maxL_1,tokens_0,tokens_1,test):\n    \n    y_prime = []\n    accuracy = 0\n    \n    for i in range(len(test)):\n        \n        pred_0 = maxL_0\n        pred_1 = maxL_1\n        \n        for word in test[i][0].split(\" \"):\n            word = word.lower()\n            if tokens_0.__contains__(word):\n                pred_0 += tokens_0[word]\n            if tokens_1.__contains__(word):    \n                pred_1 += tokens_1[word]\n    \n        if pred_0 > pred_1:\n            y_prime.append(0)\n        else: \n            y_prime.append(1)\n        \n        if y_prime[i] == test[i][1]:\n            accuracy += 1\n    \n    total_accuracy = accuracy\/len(test)\n    \n    return total_accuracy","af10b24f":"def calculate_MAP(vocab_data,vocab_class,total_words_class,m):\n    MAP_tokens = {}\n    vocab_len = len(vocab_data)\n    \n    for word in vocab_data:\n        if vocab_class.__contains__(word):\n            MAP_tokens[word] = math.log((vocab_class[word] + m)\/(total_words_class + (vocab_len*m)))\n        else:\n            if m != 0:\n                MAP_tokens[word] = math.log((m)\/(total_words_class + (vocab_len*m)))\n    return MAP_tokens","1a9a667e":"maxL_tokens_0 = calculate_MAP(vocab,c_words_0,total_words_0,0)\nmaxL_tokens_1 = calculate_MAP(vocab,c_words_1,total_words_1,0)","22b7f2e2":"MAP_tokens_0 = calculate_MAP(vocab,c_words_0,total_words_0,1)\nMAP_tokens_1 = calculate_MAP(vocab,c_words_1,total_words_1,1)","ad315f33":"def kfold_cross_validation(train0,train1,k,m,q):\n    \n    train_0 = random.sample(train0,len(train0))\n    train_1 = random.sample(train1,len(train1))\n    \n    fold_break = int(len(train_0)\/k)\n    \n    MAP_curve_accuracy = np.zeros((10, 10))\n    \n    MAP_smooth_accuracy = []\n    \n    sd_maxL = []\n    sd_MAP = []\n\n    cv_test_0 = []\n    cv_test_1 = []\n    cv_train_0 = []\n    cv_train_1 = []\n\n    for i in range(k):\n        if i == 0:\n            cv_test_0 = train_0[(i*fold_break):((i+1)*fold_break)]\n            cv_train_0 = train_0[(i+1)*fold_break:]\n\n            cv_test_1 = train_1[i*fold_break:(i+1)*fold_break]\n            cv_train_1 = train_1[(i+1)*fold_break:]\n        if i == (k-1):\n            cv_test_0 = train_0[i*fold_break:(i+1)*fold_break]\n            cv_train_0 = train_0[:(i)*fold_break]\n\n            cv_test_1 = train_1[i*fold_break:(i+1)*fold_break]\n            cv_train_1 = train_1[:(i)*fold_break]\n        else:\n            cv_test_0 = train_0[i*fold_break:(i+1)*fold_break]\n            cv_train_0 = train_0[:(i)*fold_break] + train_0[(i+1)*fold_break:]\n\n            cv_test_1 = train_1[i*fold_break:(i+1)*fold_break]\n            cv_train_1 = train_1[:(i)*fold_break] + train_1[(i+1)*fold_break:]\n            \n            \n        # Now we have cv_test_0, cv_train_0, cv_test_1, cv_train_1\n        if q == 1:\n            for l in range(10):\n\n                new_train_1 = cv_train_1[:int(((l+1)*len(cv_train_1)\/10))]\n                new_train_0 = cv_train_0[:int(((l+1)*len(cv_train_0)\/10))]\n\n                vocab,total_words = create_dict(new_train_1 + new_train_0)\n\n                vocab_1,total_words_1 = create_dict(new_train_1)\n                vocab_0,total_words_0 = create_dict(new_train_0)\n\n                maxL_0 = generate_prior(new_train_0,new_train_1+new_train_0)\n                maxL_1 = generate_prior(new_train_1,new_train_1+new_train_0)\n\n                MAP_tokens_0 = calculate_MAP(vocab,vocab_0,total_words_0,m)\n                MAP_tokens_1 = calculate_MAP(vocab,vocab_1,total_words_1,m)\n\n                MAP_curve_accuracy[i][l] = prediction(maxL_0,maxL_1,MAP_tokens_0,MAP_tokens_1,cv_test_1+cv_test_0)\n                \n            \n        if q == 2:\n            new_train_1 = cv_train_1\n            new_train_0 = cv_train_0\n\n            vocab,total_words = create_dict(new_train_1 + new_train_0)\n\n            vocab_1,total_words_1 = create_dict(new_train_1)\n            vocab_0,total_words_0 = create_dict(new_train_0)\n\n            maxL_0 = generate_prior(new_train_0,new_train_1+new_train_0)\n            maxL_1 = generate_prior(new_train_1,new_train_1+new_train_0)\n\n            MAP_tokens_0 = calculate_MAP(vocab,vocab_0,total_words_0,m)\n            MAP_tokens_1 = calculate_MAP(vocab,vocab_1,total_words_1,m)\n\n            MAP_smooth_accuracy.append(prediction(maxL_0,maxL_1,MAP_tokens_0,MAP_tokens_1,cv_test_1+cv_test_0))\n    \n    if q == 2:\n        mean = np.mean(MAP_smooth_accuracy,axis=0)\n        sd = np.std(MAP_smooth_accuracy,axis=0)\n        return mean,sd\n    \n    if q ==1:\n        mean = np.mean(MAP_curve_accuracy, axis=0)\n        sd = np.std(MAP_curve_accuracy, axis=0)\n        return mean,sd","c75da21a":"mean_MAP,sd_MAP = kfold_cross_validation(dataset_class_0, dataset_class_1,10,1,1)","16958fb7":"mean_MLE,sd_MLE = kfold_cross_validation(dataset_class_0, dataset_class_1,10,0,1)","700e64b2":"def plot_errorbar(mean_MAP,sd_MAP,mean_MLE,sd_MLE):\n    \n    index = np.arange(10)\n\n    x = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n    \n    y1 = mean_MAP\n    yerr1 = sd_MAP\n    y2 = mean_MLE\n    yerr2 = sd_MLE\n    \n    plt.figure(200)\n    plt.errorbar(x,y1,yerr1)\n    plt.errorbar(x,y2,yerr2)\n    plt.title(\"Relationship between accuracy and train-set size\")\n    plt.gca().legend(('MAP(m=1)','MLE(m=0)'))\n    plt.xlabel('Function of train-set size')\n    plt.ylabel('Averages of accuracy')\n    plt.xticks(x)\n    plt.tick_params(axis=\"x\", labelsize=9)\n    fig = plt.gcf()\n    fig.set_size_inches(10,7)\n    #plt.savefig('pp1_5.png')\n    plt.show()","24e97f7c":"plot_errorbar(mean_MAP,sd_MAP,mean_MLE,sd_MLE)","7f8b6eb5":"smooth_mean = []\nsmooth_sd = []\nfor i in range(10):\n    s_mean,s_sd = kfold_cross_validation(dataset_class_0, dataset_class_1,10,(i\/10),2)\n    smooth_mean.append(s_mean)\n    smooth_sd.append(s_sd)\nfor j in range(10):\n    s_mean,s_sd = kfold_cross_validation(dataset_class_0, dataset_class_1,10,(j+1),2)\n    smooth_mean.append(s_mean)\n    smooth_sd.append(s_sd)","7fb0455f":"def single_error_plot(y,yerr):\n    # example variable error bar values\n    x = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,6,7,8,9,10]\n    plt.figure(300)\n    plt.errorbar(x,y,yerr)\n    plt.title(\"Relationship between accuracy and smoothing parameter(m)\")\n    plt.legend(('m'))\n    plt.xlabel('Function of m')\n    plt.ylabel('Averages of accuracy')\n    plt.xticks(x)\n    plt.tick_params(axis=\"x\", labelsize=9)\n    fig = plt.gcf()\n    fig.set_size_inches(14,11)\n    plt.savefig('pp1_2.png')\n    plt.show()","a697e697":"single_error_plot(smooth_mean,smooth_sd)","54514c87":"# The model successfully achieves an accuracy of around 80% with any value of m > 1","a3857835":"Organizing Data","a7218311":"Class Distribution","0f25e1e5":"MAP Calculation","3374faff":"Train and Test Split","e4a1be91":"Prior Calculation","925bd592":"Cross validation accuracy and SD as a function of the smoothing parameter","4a620358":"Creating Dictionary","ea7314e0":"Token Calculation","a27f5b1e":"k Fold Cross Validation","081ed919":"Creating train and test dataset","dcfd66ed":"ErrorBar","2b1738fa":"Reading Data","a0aa8aaa":"Prediction Function"}}