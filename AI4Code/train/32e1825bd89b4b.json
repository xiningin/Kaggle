{"cell_type":{"35929921":"code","cf5886c9":"code","6f850546":"code","5de608e7":"code","1f53b3ac":"code","68591bd1":"code","05455e01":"code","eb4860be":"code","fa770495":"code","90c19233":"code","227b1dd0":"code","9503230c":"code","0d66a522":"code","c220c991":"code","32ee1cfa":"code","624bb6da":"code","a61c117b":"code","d3a1eed4":"code","82bbc8e6":"code","c7a6b10f":"code","c2fa54fc":"code","545c750f":"code","e1261848":"code","447e67ba":"code","7bda11f4":"code","1c2a5911":"code","2f846ac9":"code","ec0efc5c":"code","e88db8d3":"code","8692883f":"code","fcd5da7c":"code","82a706c5":"code","88a1715f":"code","91d6fd5f":"code","b1b563fe":"code","41f7bd6d":"code","9f0db260":"code","88130441":"code","e882f877":"code","a6a58669":"code","c14a9853":"code","52ba337d":"code","852fbe86":"code","84791aec":"code","932ee6ec":"code","235b33d0":"code","3166f1eb":"code","a3cb6f21":"code","f7e42e97":"code","8bea91f4":"code","03efd01b":"code","9fdfda8c":"code","de038b98":"code","b04b565c":"code","bb820294":"code","fda1ee4d":"code","763e3989":"code","6f5c71cb":"code","96a76a4b":"code","aee11254":"code","10b6c76c":"code","02c8c2ac":"code","468c84de":"code","fb993c5e":"code","e1be19c3":"code","4b5614a9":"code","24c37359":"code","97ad3d96":"code","f85d56f5":"code","3cc3c1b0":"code","4abba143":"code","99d70e50":"code","acf929f0":"code","3cefea9e":"code","198670f4":"code","44c7ae4b":"code","29b523b2":"code","1fa130d3":"code","3994198c":"code","334bac57":"code","bc97e127":"code","c24fd44a":"code","67994e0d":"code","40cc62b8":"code","172929e2":"markdown","7585a78b":"markdown","96c42118":"markdown","d5bbcf4e":"markdown","9578c775":"markdown","2cb9fb5d":"markdown","d3716ca4":"markdown","71d863ad":"markdown","7df3df5b":"markdown","4e94bbfe":"markdown","38c51398":"markdown","2463aff6":"markdown","6138d9d2":"markdown","2e486511":"markdown","811cb26b":"markdown","31889b0c":"markdown","6493f723":"markdown","7d77e551":"markdown","bf6f286a":"markdown","d1ce2725":"markdown","1b488e23":"markdown","65bc0042":"markdown","9121bba7":"markdown","140d7710":"markdown","9638999a":"markdown","1ad0295b":"markdown","ab6a9ad8":"markdown","575a1783":"markdown"},"source":{"35929921":"# To support both python 2 and python 3\nfrom __future__ import division, print_function, unicode_literals\n\n# 1.0 Clear ipython memory\n%reset -f\n\n# Common imports\nimport numpy as np\nimport pandas as pd\nimport os\nimport gc\nimport time\nimport datetime as dt\n\n# to make this notebook's output stable across runs\n#Somehow this is not happening as o\/p of models is not consistent\nnp.random.seed(42)\n\n# Ignore useless warnings (see SciPy issue #5998)\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")","cf5886c9":"# 1.1 Working with imbalanced data\n# http:\/\/contrib.scikit-learn.org\/imbalanced-learn\/stable\/generated\/imblearn.over_sampling.SMOTE.html\n# Check imblearn version number as:\n#   import imblearn;  imblearn.__version__\nfrom imblearn.over_sampling import SMOTE, ADASYN\n","6f850546":"# 1.2 Class for applying multiple data transformation jobs\nfrom sklearn.compose import ColumnTransformer as ct\n# Scale numeric data\nfrom sklearn.preprocessing import StandardScaler as ss\n# One hot encode data--Convert to dummy\nfrom sklearn.preprocessing import OneHotEncoder as ohe\n","5de608e7":"# 1.3 Dimensionality reduction\nfrom sklearn.decomposition import PCA\n\n# 1.4 Data splitting and model parameter search\nfrom sklearn.model_selection import train_test_split\n#from sklearn.model_selection import GridSearchCV\n#from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n\n# 1.5 Modeling modules\n# conda install -c anaconda py-xgboost\n#from xgboost.sklearn import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\n\n\n\n# 1.6 Model pipelining\n#from sklearn.pipeline import Pipeline\n#from sklearn.pipeline import make_pipeline\n\n\n# 1.7 Model evaluation metrics\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc, roc_curve\n\n# 1.8\nimport matplotlib.pyplot as plt\n#from xgboost import plot_importance\n\n# 1.9 Needed for Bayes optimization\n#from sklearn.model_selection import cross_val_score\n\n# 1.10 Install as: pip install bayesian-optimization\n#     Refer: https:\/\/github.com\/fmfn\/BayesianOptimization\n#from bayes_opt import BayesianOptimization\n\n\n# 1.11 Find feature importance of ANY BLACK BOX estimator\n#      See note at the end of this code for explanation\n#      Refer: https:\/\/eli5.readthedocs.io\/en\/latest\/blackbox\/permutation_importance.html\n#      Install as:\n#      conda install -c conda-forge eli5\n#import eli5\n#from eli5.sklearn import PermutationImportance\n","1f53b3ac":"# 1.12 Model building\n#     Install h2o as: conda install -c h2oai h2o=3.22.1.2\nimport h2o\nfrom h2o.estimators.deeplearning import H2ODeepLearningEstimator","68591bd1":"transactions = pd.read_csv(\"..\/input\/online-shopping-fraud\/datasetForFinalAssignment.csv\")\ntransactions.info()","05455e01":"validate=pd.read_csv(\"..\/input\/online-fraud-sample\/datasetForFinalTest.csv\")\nvalidate.info()","eb4860be":"#columns = transactions.columns\n#columns\ntransactions.head()","fa770495":"\"\"\"\nIn summary, we may utilize the following variable going forward into statistical analysis and model building:\nnumberOfTimesDeviceUsed, \ntimeBetween, \nsex, \nbrowser, \nSource to arrive at estimated values of target variable 'class'.\"\n\"\"\"\n","90c19233":"X = transactions.copy() # independent variables data\ny = X['class']\nX.info()","227b1dd0":"X.drop(columns = ['class'], inplace = True)\nX.drop(columns = ['Column 1','user_id','device_id','ip_address','signup_time','purchase_time'], inplace = True)\nX.info()","9503230c":"X_val = validate.copy()","0d66a522":"X_val.drop(columns = ['Column 1','user_id','device_id','ip_address','signup_time','purchase_time'], inplace = True)\nX_val.info()\n","c220c991":"X.head()","32ee1cfa":"#Define the transformation function using columnTransformer, OHE and StandardScaler\ndef transform(categorical_columns,numerical_columns,df):\n    #  Create a tuple of processing tasks:\n    #  (taskName, objectToPerformTask, columns-upon-which-to-perform)\n    # 9.1 One hot encode categorical columns\n    cat = ('categorical', ohe() , categorical_columns  )\n    # 9.2 Scale numerical columns\n    num = ('numeric', ss(), numerical_columns)\n    # 9.3 Instantiate columnTransformer object to perform task\n    #     It transforms X separately by each transformer\n    #     and then concatenates results.\n    col_trans = ct([cat, num])\n    # 9.4 Learn data\n    col_trans.fit(df)\n    # 9.5 Now transform df\n    df_transAndScaled = col_trans.transform(df)\n    # 9.6 Return transformed data and also transformation object\n    return df_transAndScaled, col_trans\n","624bb6da":"#Define the columns for transformations\ncategorical_columns = ['source', 'browser','sex']\nnumerical_columns = ['signup_time-purchase_time', 'purchase_value','age','N[device_id]']","a61c117b":"X.source.unique() # ['Direct', 'SEO', 'Ads'] 3 values --> columns 0,1,2","d3a1eed4":"X.browser.unique() # ['Chrome', 'FireFox', 'IE', 'Safari', 'Opera'] 5 values  --> columns 3,4,5,6,7","82bbc8e6":"X.sex.unique() # ['M', 'F'] 2 values --> columns 8,9","c7a6b10f":"#Define the columns for post transformation dataframe - makes referencing and understanding easier\ncolumns = ['source_Direct','source_SEO','source_Ads','browser_Chrome','browser_FireFox','browser_IE','browser_Safari','browser_Opera','sex_M','sex_F'] + numerical_columns\ncolumns","c2fa54fc":"# 10.0 Transform X dataset\nX_transAndScaled, _  = transform(categorical_columns, numerical_columns, X)\n\n# 10.1\nX_transAndScaled.shape # (74691, 14)","545c750f":"X_transAndScaled = pd.DataFrame(X_transAndScaled, index=X.index, columns=columns)\nX_transAndScaled.head()","e1261848":"# 11.0 Transform X_val dataset\nXval_transAndScaled, _  = transform(categorical_columns, numerical_columns, X_val)\n\n# 10.1\nXval_transAndScaled.shape           # (13413, 14)","447e67ba":"#Provide the column names for additional dummy variables\nXval_transAndScaled = pd.DataFrame(Xval_transAndScaled, index=X_val.index, columns=columns)\nXval_transAndScaled.head()","7bda11f4":"# 12. Split data into train\/test\n#     train-test split. save the indices of split set\nX_train,X_test, y_train, y_test ,indicies_tr,indicies_test = train_test_split(\n                                                                      X_transAndScaled,    # Predictors\n                                                                      y,                # Target\n                                                                      np.arange(X_transAndScaled.shape[0]),\n                                                                      test_size = 0.3,   # split-ratio\n                                                                      random_state=1\n                                                                     )","1c2a5911":"X_train.shape","2f846ac9":"#Generate the image of test dataset pre-split using indicies_test.\n#This will be used to capture the unscaled values of purchase_value for computing cost of model\nX_cost = X.iloc[indicies_test]\nX_cost.purchase_value.head()","ec0efc5c":"#Using Garbage Collect to clear memory\ndel X_transAndScaled\ndel indicies_tr\ndel indicies_test\ngc.collect()","e88db8d3":"type(y_train)\n#y_train.info()","8692883f":"#12.0 Checking the extent of 'class' imbalance\nnp.sum(y_train)\/len(y_train)       # 0.09431363923263776","fcd5da7c":"# 12.1  Process X_train data with SMOTE\nsm = SMOTE(random_state=42)\nX_res, y_res = sm.fit_sample(X_train, y_train)\ntype(X_res)       # No longer pandas dataframe\n                  #  but we will convert to pandas dataframe\n\n","82a706c5":"# 12.2 Check\nX_res.shape                    # (94756, 14)","88a1715f":"#Using y_res for most of the classifiers\n#Using y_onehot for NeuralNetworks\ny_onehot = pd.get_dummies(y_res)\ny_onehot.info()","91d6fd5f":"np.sum(y_res)\/len(y_res)       # 0.5 ,earlier ratio was 0.09381634565728822","b1b563fe":"#y_res = y_res.reshape(len(y_res),1)\ny_res = pd.DataFrame(y_res)\ntype(y_res)","41f7bd6d":"X_res = pd.DataFrame(X_res,columns=columns)\ntype(X_res)","9f0db260":"def modelCost(test_y,model_y,df):\n    #falsePositive: Cost is $8*count\n    #non-fraudulent transactions (test_y '0') predicted as fraudulent by model (model_y '1')\n    falsePositiveCost = df.purchase_value[(test_y==0) & (model_y==1)].count()*8\n    print(\"falsePositive {:.0f}\".format(df.purchase_value[(test_y==0) & (model_y==1)].count()))\n    print(\"falsePositiveCost ${:.0f}\".format(falsePositiveCost))\n    #falseNegative: Cost is sum of purchase_value\n    #fraudulent transactions (test_y '1') predicted as non-fraudulent by model (model_y '0')\n    falseNegativeCost = df.purchase_value[(test_y==1) & (model_y==0)].sum()\n    print(\"falseNegative {:.0f}\".format(df.purchase_value[(test_y==1) & (model_y==0)].count()))\n    print(\"falseNegativeCost ${:.0f}\".format(falseNegativeCost))\n    totalCost = falsePositiveCost + falseNegativeCost\n    print(\"totalCost ${:.0f}\".format(totalCost))\n    return totalCost\n","88130441":"y0s=np.zeros(y_test.size)\ntotalCost_0s = modelCost(y_test, y0s, X_cost)","e882f877":"y1s=np.ones(y_test.size)\ntotalCost_1s = modelCost(y_test, y1s, X_cost)","a6a58669":"#Running basic regression first to setup all checking and evaluation functions\nlog_reg = LogisticRegression(random_state=42)\nstart = time.time()\nlog_reg.fit(X_res, y_res)\nend = time.time()\n(end - start) #0.31 seconds","c14a9853":"y_logreg = log_reg.predict(X_test)\nlog_reg.score(X_test, y_test) #0.920653338093538 Not bad for a start!","52ba337d":"totalCost_logreg = modelCost(y_test, y_logreg, X_cost)\ntotalCost_logreg # $35238","852fbe86":"########## Logistic Regression With L1 Penalty ##########\n# logistic regression with L1 penalty\nstart = time.time()\nlogreg_l1 = LogisticRegression(C=0.1, penalty='l1')\nlogreg_l1.fit(X_res, y_res)\nend = time.time()\nprint(\"LOGISTIC REGRESSION - L1 penalty took {:.2f}s\".format(end - start)) #0.57s","84791aec":"logreg_l1.coef_\ny_logregl1 = logreg_l1.predict(X_test)\nlogreg_l1.score(X_test, y_test) #0.9206087111745805 not much improvement","932ee6ec":"totalCost_logregl1 = modelCost(y_test, y_logregl1, X_cost)\ntotalCost_logregl1 # $35262 -- Cost degrades!","235b33d0":"########## Logistic Regression With L2 Penalty ##########\n# logistic regression with L2 penalty\nstart = time.time()\nlogreg_l2 = LogisticRegression(C=0.1, penalty='l2')\nlogreg_l2.fit(X_res, y_res)\nend = time.time()\nprint(\"LOGISTIC REGRESSION - L2 penalty took {:.2f}s\".format(end - start)) #0.46s","3166f1eb":"logreg_l2.coef_\ny_logregl2 = logreg_l2.predict(X_test)\nlogreg_l2.score(X_test, y_test) #0.920653338093538 not much improvement","a3cb6f21":"totalCost_logregl2 = modelCost(y_test, y_logregl2, X_cost)\ntotalCost_logregl2 # $35291 -- Cost degrades significantly!","f7e42e97":"########## DecisionTreeClassifier ##########\n#from sklearn.tree import DecisionTreeClassifier\nstart = time.time()\ntreeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\ntreeclf.fit(X_res, y_res)\nend = time.time()\nprint(\"DecisionTreeClassifier took {:.2f}s\".format(end - start)) #0.27s","8bea91f4":"y_treeclf = treeclf.predict(X_test)\ntreeclf.score(X_test, y_test) #0.9201178150660478","03efd01b":"totalCost_treeclf = modelCost(y_test, y_treeclf, X_cost)\ntotalCost_treeclf # $34890 -- Cost improves slightly from $35238 for logisticRegression!","9fdfda8c":"########## RandomForestClassifier ##########\n#from sklearn.ensemble import RandomForestClassifier\nstart = time.time()\n#rfclf = RandomForestClassifier(n_estimators=200, max_features=5, oob_score=True, random_state=1)\nrfclf = RandomForestClassifier(n_estimators=200, max_features=5, random_state=1)\nrfclf.fit(X_res, y_res)\nend = time.time()\nprint(\"DecisionTreeClassifier took {:.2f}s\".format(end - start)) #44.71s","de038b98":"y_rfclf = rfclf.predict(X_test)\nrfclf.score(X_test, y_test) #0.9364512674044984","b04b565c":"totalCost_rfclf = modelCost(y_test, y_rfclf, X_cost)\ntotalCost_rfclf # $38259 -- Cost degrades majorly from $34890 for DecisionTreeClassifier!","bb820294":"X_res.info()","fda1ee4d":"y_onehot.info()","763e3989":"start = time.time()\nNN_2l = Sequential()\nNN_2l.add(Dense(input_dim=14, output_dim=100))\nNN_2l.add(Dense(output_dim=2))\nNN_2l.add(Activation(\"softmax\"))\n\nNN_2l.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\nNN_2l.fit(X_res, y_onehot)\n\nend = time.time()\nprint(\"2layer NeuralNetwork took {:.2f}s\".format(end - start)) #6.9s\n","6f5c71cb":"y_NN2l = NN_2l.predict_classes(X_test)\nprint(\"\\n\\naccuracy\", np.sum(y_NN2l == y_test) \/ float(len(y_test)))","96a76a4b":"totalCost_NN2l = modelCost(y_test, y_NN2l, X_cost)\ntotalCost_NN2l # $33575 -- Cost improvement from $34890 for DecisionTreeClassifier!","aee11254":"start = time.time()\nNN_3l = Sequential()\nNN_3l.add(Dense(input_dim=14, output_dim=100))\nNN_3l.add(Dense(output_dim=100))\nNN_3l.add(Dense(output_dim=2))\nNN_3l.add(Activation(\"softmax\"))\n\nNN_3l.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\nNN_3l.fit(X_res, y_onehot)\n\nend = time.time()\nprint(\"3layer NeuralNetwork took {:.2f}s\".format(end - start)) #8.4s\n","10b6c76c":"y_NN3l = NN_3l.predict_classes(X_test)\nprint(\"\\n\\naccuracy\", np.sum(y_NN3l == y_test) \/ float(len(y_test)))","02c8c2ac":"totalCost_NN3l = modelCost(y_test, y_NN3l, X_cost)\ntotalCost_NN3l # $33501 -- Cost improvement from $33575 for 2layer NN!","468c84de":"#from keras.layers import Dense, Activation, Dropout","fb993c5e":"start = time.time()\nNN_ReLu = Sequential()\nNN_ReLu.add(Dense(100, input_shape=(14,)))\nNN_ReLu.add(Activation('relu'))\nNN_ReLu.add(Dropout(0.2))\nNN_ReLu.add(Dense(100))\nNN_ReLu.add(Activation('relu'))\nNN_ReLu.add(Dropout(0.2))\nNN_ReLu.add(Dense(2))\nNN_ReLu.add(Activation(\"softmax\"))\n\nNN_ReLu.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\nNN_ReLu.fit(X_res, y_onehot)\n\nend = time.time()\nprint(\"ReLu NeuralNetwork took {:.2f}s\".format(end - start)) #10.56s\n","e1be19c3":"y_NNReLu = NN_ReLu.predict_classes(X_test)\nprint(\"\\n\\naccuracy\", np.sum(y_NNReLu == y_test) \/ float(len(y_test)))","4b5614a9":"totalCost_NNReLu = modelCost(y_test, y_NNReLu, X_cost)\ntotalCost_NNReLu # $35503 -- Cost degrades from $33501 for 3layer NN!","24c37359":"# 13.0 Preparing to model data with deeplearning\n#      H2o requires composite data with both predictors\n#      and target\ndf = np.hstack((X_res,y_res))\ndf.shape            #\n\n\n# 13.1 Start h2o\nh2o.init()\n\n# 13.2 Transform data to h2o dataframe\ndf = h2o.H2OFrame(df, column_names=columns+['class'])\n#df = h2o.H2OFrame(df)\ndf.columns\n","97ad3d96":"len(df.columns)    # 15\n","f85d56f5":"df.shape           # (94842, 15)","3cc3c1b0":"len(columns) #14 class is extra in df","4abba143":"# 14. Get list of predictor column names and target column names\n#     Column names are given by H2O when we converted array to\n#     H2o dataframe\nX_columns = df.columns[0:14]        # Only column names. No data\nX_columns       # C1 to C18\n","99d70e50":"y_columns = df.columns[14]\ny_columns\n\n","acf929f0":"# 14.1 For classification, target column must be factor\n#      Required by h2o\ndf['class'] = df['class'].asfactor()\n\n# 15. Build a deeplearning model on balanced data\n#     http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-science\/deep-learning.html\ndl_model = H2ODeepLearningEstimator(epochs=1000,\n                                    distribution = 'bernoulli',                 # Response has two levels\n                                    missing_values_handling = \"MeanImputation\", # Not needed by us\n                                    variable_importances=True,\n                                    nfolds = 2,                           # CV folds\n                                    fold_assignment = \"Stratified\",       # Each fold must be sampled carefully\n                                    keep_cross_validation_predictions = True,  # For analysis\n                                    balance_classes=False,                # SMOTE is not provided by h2o\n                                    standardize = True,                   # z-score standardization\n                                    activation = 'RectifierWithDropout',  # Default dropout is 0.5\n                                    hidden = [100,100],                  # ## more hidden layers -> more complex interactions\n                                    stopping_metric = 'logloss',\n                                    loss = 'CrossEntropy')\n\n","3cefea9e":"# 16.1 Train our model\nstart = time.time()\ndl_model.train(X_columns,\n               y_columns,\n               training_frame = df)\n\n\nend = time.time()\n(end - start)\/60\n","198670f4":"\n# 16.2 Get model summary\nprint(dl_model)\n","44c7ae4b":"X_test_h2o = h2o.H2OFrame(X_test)\ntype(X_test_h2o)","29b523b2":"y_h20DL = dl_model.predict(X_test_h2o[: , 0:13])\ny_h20DL[:, 0]","1fa130d3":"#y_h20predict = y_h20DL['predict'].as_data_frame().as_matrix()\ny_h20predict = y_h20DL['predict'].as_data_frame().values\ntype(y_h20predict)\n#y_h20predict0 = y_h20predict[:,0]","3994198c":"print(\"\\n\\naccuracy\", np.sum(y_h20predict[:,0] == y_test) \/ float(len(y_test)))","334bac57":"totalCost_h20predict = modelCost(y_test, y_h20predict[:,0], X_cost)\ntotalCost_h20predict # $35503 -- Cost degrades from $33501 for 3layer NN!","bc97e127":"'''\nNeed a way to include totalCost in the optimization objective. \nwithout it modelling results have an elemnt of chance as \nany small increase in falseNegative disproportionately increases \nthe costs without the model being aware of the magnitude of cost error!!\n\nFor this reason, I am comparing the results obtained on test dataset after running all 9 models.\nModels are evaluated basis how well the falseNegative are addressed.\n\nA comparison of all models tried here, sorted by the falseNegative ascendingly:\n'''","c24fd44a":"Xval_transAndScaled.head()","67994e0d":"\"\"\" PLaceholder code to try out Bayesian Optimization\npara_set = {\n           'learning_rate':  (0, 1),                 # any value between 0 and 1\n           'n_estimators':   (50,300),               # any number between 50 to 300\n           'max_depth':      (3,10),                 # any depth between 3 to 10\n           'n_components' :  (20,30)                 # any number between 20 to 30\n            }\n\ndef xg_eval(learning_rate,n_estimators, max_depth,n_components):\n    # 12.1 Make pipeline. Pass parameters directly here\n    pipe_xg1 = make_pipeline (ss(),                        # Why repeat this here for each evaluation?\n                              PCA(n_components=int(round(n_components))),\n                              XGBClassifier(\n                                           silent = False,\n                                           n_jobs=2,\n                                           learning_rate=learning_rate,\n                                           max_depth=int(round(max_depth)),\n                                           n_estimators=int(round(n_estimators))\n                                           )\n                             )\n\n    # 12.2 Now fit the pipeline and evaluate\n    cv_result = cross_val_score(estimator = pipe_xg1,\n                                X= X_train,\n                                y = y_train,\n                                cv = 2,\n                                n_jobs = 2,\n                                scoring = 'f1'\n                                ).mean()             # take the average of all results\n\n\n    # 12.3 Finally return maximum\/average value of result\n    return cv_result\n    \nxgBO = BayesianOptimization(\n                             xg_eval,     # Function to evaluate performance.\n                             para_set     # Parameter set from where parameters will be selected\n                             )\n\ngp_params = {\"alpha\": 1e-5}      # Initialization parameter for gaussian\n\nstart = time.time()\nxgBO.maximize(init_points=5,    # Number of randomly chosen points to\n                                 # sample the target function before\n                                 #  fitting the gaussian Process (gp)\n                                 #  or gaussian graph\n               n_iter=25,        # Total number of times the\n               #acq=\"ucb\",       # ucb: upper confidence bound\n                                 #   process is to be repeated\n                                 # ei: Expected improvement\n               # kappa = 1.0     # kappa=1 : prefer exploitation; kappa=10, prefer exploration\n              **gp_params\n               )\nend = time.time()\n\nxgBO.res\n\ntype(xgBO.res) #if list the following line will not work\n#xgBO.res['max']\nxgBO.max # using the function directly to get the parameters\n\"\"\"","40cc62b8":"\"\"\" Plceholder code to try out pipeline\n\nclf = Pipeline([\n        (\"kpca\", KernelPCA(n_components=2)),\n        (\"log_reg\", LogisticRegression())\n    ])\n\nparam_grid = [{\n        \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n        \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n    }]\n\ngrid_search = GridSearchCV(clf, param_grid, cv=3)\ngrid_search.fit(X, y)\nprint(grid_search.best_params_)\n\"\"\"\n","172929e2":"# Check-1: All y's predicted as 1 (i.e. fraudulent transactions)","7585a78b":"signup_time-purchase_time: provides the information of timeElapsed between signup and purchase. This is already provided so no need to compute timeBetween\n\nN[device_id]: also provided so no need to compute numberOfTimesDeviceUsed","96c42118":"# Check-0: All y's predicted as 0 (i.e. non-fraudulent transactions)","d5bbcf4e":"Our customer is an e-commerce site that sells wholesale electronics. You have been contracted to build\n\na model that predicts whether a given transaction is fraudulent or not. You only have information\n\nabout each user\u2019s first transaction on the company's website. If you fail to identify a fraudulent\n\ntransaction, the company loses money equivalent to the price of the fraudulently purchased product. If\n\nyou incorrectly flag a real transaction as fraudulent, it inconveniences the customers\n\nwhose valid transactions are flagged\u2014a cost your client values at $8.\n\nSo, the task is to build a model that's predictive but also minimizes to cost to the company not only by correctly flagging and identifying fraudulent transactions but also to minimize the cost of wrong predictions since each wrong prediction costs the company $8.\n\nYou can use any model that we learned about in the class or you might have learnt about else where. The evaluation criteria will be total cost to company for the test data provided; for example if your model identifies 100 cases as fraudulent while they are normal transactions, this would cost the company $8 * 100  = $800.\n\nThere are 2 files included in this assignment:\n\n- training data to build your models:\n\ndatasetForFinalAssignment.csv\n- test data to apply your models to predict if the transaction id fraudulent or not.\n\ndatasetForFinalTest.csv","9578c775":"Step 1: Define BayesianOptimization function.\n            It broadly acts as follows\"\n            s1. Gets a dictionary of parameters that specifies\n                possible range of values for each one of\n                the parameters. [Our set: para_set ]\n            s2. Picks one value for each one of the parameters\n                (from the specified ranges as in (s1)) evaluate,\n                a loss-function that is given to it, say,\n                accuracy after cross-validation.\n                [Our function: xg_eval() ]\n            s3. Depending upon the value of accuracy returned\n                by the evaluator and also past values of accuracy\n                returned, this function, creates gaussian\n                processes and picks up another set of parameters\n                from the given dictionary of parameters\n            s4. The parameter set is then fed back to (s2) above\n                for evaluation\n            s5. (s2) t0 (s4) are repeated for given number of\n                iterations and then final set of parameters\n                that optimizes objective is returned","2cb9fb5d":"#### Data Processing with SMOTE","d3716ca4":"##### What percentage of all the transactions in our dataset were fraudulent transactions?\n\nPercentage of all the transactions in our dataset that were fraudulent = 9.38 %","71d863ad":"\n##### *Step1*  -- Load the Dataset into a dataframe called transactions\n#####       -- Load the Validation Dataset into a dataframe called validate\n","7df3df5b":"##### Step5 -- Making copies of test and validation data\n","4e94bbfe":"# Model-1: LogisticRegression","38c51398":"### Ready for Modeling\n##### X_res, y_res (or y_onehot)\n### For testing model accuracy\n##### X_test, y_test\n### For model validation\n##### Xval_transAndScaled \n##### y_prediction.to_csv(\"submissionFirstNameLastName.csv\")\n### For model selection need the costing function:\n##### calculate the cost of wrong response: $8*wrongFraudulent + purchase value of allFraudsMissed","2463aff6":"\nModel\t            falsePositive\tfalseNegative\tfalsePositiveCost\tfalseNegativeCost\ttotalCost\nCheck-all1s\t        20263\t        0\t            162104\t            0\t                162104\ntotalCost_treeclf\t1140\t        695         \t9120\t            25831\t            34951\ntotalCost_logregl1\t1131\t        698\t            9048\t            25946\t            34994\ntotalCost_logreg\t1125\t        699\t            9000\t            25957\t            34957\ntotalCost_logregl2\t1124\t        700\t            8992\t            26007\t            34999\ntotalCost_NN2l\t    1087\t        712\t            8696\t            26584\t            35280\ntotalCost_NN3l\t    883\t            768\t            7064\t            28891\t            35955\ntotalCost_h20   \t3142\t        852\t            25136\t            31579\t            56715\ntotalCost_NNReLu\t602\t            868\t            4816\t            32656\t            37472\ntotalCost_rfclf\t    497\t            940\t            3976\t            35374\t            39350\nCheck-all0s\t        0\t            2145\t        0\t                79288\t            79288\n\nObservations:\n1) treeclf ==> best fit. however, can look into range of classifiers with falseNegative <= 700 and logregl2 is also good.\n2) Surprisingly, none of the fancy models like randomforest \/ neural networks \/ deep learning are of much use here.\n3) This could also be due to the fact that two of the parameters: signup_time-purchase_time and N[device_id] have a significant impact in fraudulent transactions.\n4) purchase_value though impacting the cost has lesser role to play in classifying a transaction is fraudulent OR not.\n5) Given the high cost associated with falseNegative, I would recommend to adopt treeclf algorithm.\n","6138d9d2":"# Model-7: 3-layer NeuralNetwork","2e486511":"##### Customer loses \\$162104 as now all non-fraudulent transactions are incorrectly classified as fraudulent\n##### So our models should predict costs that are better than \\$79288 for the model to be of some value","811cb26b":"# Model-9: h2o -- deeplearning","31889b0c":"##### Customer loses $76393 from fraudulent transactions incorrectly classified as non-fraudulent","6493f723":"##### Step6 -- Dealing with Categorical and Numerical Variables\n##### calling OneHotEncoder and StandardScaler within the ColumnTransformer\n","7d77e551":"# Model-8: Regularized NeuralNetwork","bf6f286a":"### Insight from data exploration:\n\n   - Time between signup and purchase and how many times the same device was used are important, we'll explore the relationship between target variable and the new features that we created *timeBetween* and *numberOfTimesDeviceUsed* in detail\n   - Looks like the categorical values are sampled similarly for both values of target variable, no concern for sampling bias ","d1ce2725":"# Model-2: LogisticRegression With L1 Penalty","1b488e23":"# Final Project: Build a predictive model for determining if a customer transaction is fraudulent\n\n","65bc0042":"# Model-6: 2-layer NeuralNetwork","9121bba7":"# Model-4: DecisionTreeClassifier","140d7710":"# Model-5: RandomForestClassifier","9638999a":"# Model-10: xgBoost using Bayesian Optimization","1ad0295b":"##### Restoring the index and columns names\n##### Restoring index helps in correctly compute the cost using unscaled purchase_value","ab6a9ad8":"# Concluding: ","575a1783":"# Model-3: LogisticRegression With L2 Penalty"}}