{"cell_type":{"d02bd6cb":"code","acd36f84":"code","4466c246":"code","b3503ec0":"code","19f6180e":"code","5207a06c":"code","b5fadba1":"code","a6c1bac6":"code","668e0041":"code","597abb24":"code","a06486d4":"code","d8d232c5":"code","f3464cd9":"code","44a4c3d2":"code","1dfa1827":"code","0a195eb2":"code","565bdc8c":"code","d790a90d":"code","afefb847":"code","29328a6c":"code","51ef800b":"code","88937974":"code","fccbf81f":"code","cd74fda8":"code","bed06f46":"code","8bc042a4":"code","626d9f03":"code","ac4b143a":"code","1055527f":"code","7d57fef5":"code","30f8984f":"code","14bf7470":"code","31734fd5":"code","e0e3df88":"code","a6ac83aa":"code","a360611c":"code","cc1222d1":"code","5a914b84":"code","ef5b456e":"code","4aa89099":"code","b024c4eb":"markdown","89049201":"markdown","7929bb1a":"markdown","08ec0adb":"markdown","3213baa9":"markdown","925ba122":"markdown","bb5c74e8":"markdown","23f73fab":"markdown","c8de5c86":"markdown","7f69a658":"markdown","24e417c3":"markdown","76fa1e04":"markdown","07d27aba":"markdown","b4ddaf77":"markdown","80242c5d":"markdown","0301795c":"markdown","496b98ed":"markdown","d6bb3479":"markdown","f050642b":"markdown","4e3b4a93":"markdown","d6b6b66d":"markdown","9e100105":"markdown","115e01c4":"markdown","538432a0":"markdown","0055393f":"markdown","c5a80b02":"markdown","5239c990":"markdown","934718ad":"markdown","d5e1b1a9":"markdown","d36fd68b":"markdown","2afe58da":"markdown","607bd0b6":"markdown","ce8872f5":"markdown","2441a1ec":"markdown","ccde42b1":"markdown"},"source":{"d02bd6cb":"import pandas as pd\nimport numpy as np\n\ndef describe(df):\n    return pd.DataFrame(np.array([\n        df.columns.values,\n        df.dtypes.values,\n        df.count().values,\n        (df.fillna(0).count() - df.count()).values,\n        df.min().values,\n        df.max().values,\n        df.nunique().values,\n    ]).T, columns=[\n        'Column',\n        'Data Type',\n        'Non-Null Count',\n        'Null Count',\n        'Min',\n        'Max',\n        'Distinct Values'\n    ])","acd36f84":"train_df = pd.read_csv('..\/input\/learn-together\/train.csv')\ndescribe(train_df)","4466c246":"test_df = pd.read_csv('..\/input\/learn-together\/test.csv')\ndescribe(test_df)","b3503ec0":"def test_train_scale(train, test):\n    v = []\n    for col_name in train.columns.values:\n        if col_name not in train or col_name not in test:\n            continue\n\n        max_train = train[col_name].max()\n        max_test = test[col_name].max()\n        max_diff = abs(max_train-max_test)\n        numerator = max(max_train, max_test)\n        denominator = min(max_train, max_test)\n        if max_train < 0 and max_test < 0:\n            numerator = min(max_train, max_test) * -1\n            denominator = max(max_train, max_test) * -1\n        elif max_train < 0:\n            lift = abs(max_train)\n            numerator = max(max_train+lift, max_test+lift)\n            denominator = min(max_train+lift, max_test+lift)\n        elif max_test < 0:\n            lift = abs(max_test)\n            numerator = max(max_train+lift, max_test+lift)\n            denominator = min(max_train+lift, max_test+lift)\n        max_percent_diff = round(numerator if denominator == 0 else numerator\/denominator, 2)\n\n        min_train = train[col_name].min()\n        min_test = test[col_name].min()\n        min_diff = abs(min_train-min_test)\n        numerator = max(min_train, min_test)\n        denominator = min(min_train, min_test)\n        if min_train < 0 and min_test < 0:\n            numerator = min(min_train, min_test) * -1\n            denominator = max(min_train, min_test) * -1\n        elif min_train < 0:\n            lift = abs(min_train)\n            numerator = max(min_train+lift, min_test+lift)\n            denominator = min(min_train+lift, min_test+lift)\n        elif min_test < 0:\n            lift = abs(min_test)\n            numerator = max(min_train+lift, min_test+lift)\n            denominator = min(min_train+lift, min_test+lift)\n        min_percent_diff = round(numerator if denominator == 0 else numerator\/denominator, 2)\n\n        range_train = max_train - min_train\n        range_test = max_test - min_test\n        range_diff = abs(range_train-range_test)\n        numerator = max(range_train, range_test)\n        denominator = min(range_train, range_test)\n        if range_train < 0 and range_test < 0:\n            numerator = min(range_train, range_test) * -1\n            denominator = max(range_train, range_test) * -1\n        elif range_train < 0:\n            lift = abs(range_train)\n            numerator = max(range_train+lift, range_test+lift)\n            denominator = min(range_train+lift, range_test+lift)\n        elif range_test < 0:\n            lift = abs(range_test)\n            numerator = max(range_train+lift, range_test+lift)\n            denominator = min(range_train+lift, range_test+lift)\n        range_percent_diff = round(numerator if denominator == 0 else numerator\/denominator, 2)\n\n        v.append([\n            col_name, \n            max_train, \n            max_test, \n            max_diff, \n            max_percent_diff, \n            min_train, \n            min_test, \n            min_diff, \n            min_percent_diff, \n            range_train, \n            range_test, \n            range_diff, \n            range_percent_diff\n        ])\n\n    return pd.DataFrame(np.array(v), columns=[\n        'Column',\n        'Max (train)',\n        'Max (test)',\n        'Max (diff)',\n        'Max (% diff)',\n        'Min (train)',\n        'Min (test)',\n        'Min (diff)',\n        'Min (% diff)',\n        'Range (train)',\n        'Range (test)',\n        'Range (diff)',\n        'Range (% diff)',\n    ])","19f6180e":"test_train_scale(train_df, test_df)","5207a06c":"target_column = train_df.columns.values[-1]\ntarget_column","b5fadba1":"feature_columns = train_df.columns.values[1:-1]\nfeature_columns","a6c1bac6":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\nfrom statistics import mean\n\nskf = StratifiedKFold(n_splits=10)\n\nX = train_df[feature_columns]\ny = train_df[target_column]\n\nscores = []\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    classifier = RandomForestClassifier(n_estimators=100, random_state=8675309)\n    classifier.fit(X_train, y_train)\n    predictions = classifier.predict(X_test)\n    scores.append(f1_score(y_test, predictions, average='micro'))\n    \nmean(scores)","668e0041":"train_df['Soil_Type'] = sum([train_df['Soil_Type' + str(n)] * n for n in range(1, 41)])\ntrain_df['Soil_Type'].hist()","597abb24":"test_df['Soil_Type'] = sum([test_df['Soil_Type' + str(n)] * n for n in range(1, 41)])\ntest_df['Soil_Type'].hist()","a06486d4":"feature_columns","d8d232c5":"alt_feature_columns = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n       'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am',\n       'Hillshade_Noon', 'Hillshade_3pm',\n       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1',\n       'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4',\n       'Soil_Type']\n\nskf = StratifiedKFold(n_splits=10)\n\nX = train_df[alt_feature_columns]\ny = train_df[target_column]\n\nscores = []\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    classifier = RandomForestClassifier(n_estimators=100, random_state=8675309)\n    classifier.fit(X_train, y_train)\n    predictions = classifier.predict(X_test)\n    scores.append(f1_score(y_test, predictions, average='micro'))\n    \nmean(scores)","f3464cd9":"train_df['Wilderness_Area'] = sum([train_df['Wilderness_Area' + str(n)] * n for n in range(1, 4)])\ntrain_df['Wilderness_Area'].hist()","44a4c3d2":"test_df['Wilderness_Area'] = sum([test_df['Wilderness_Area' + str(n)] * n for n in range(1, 4)])\ntest_df['Wilderness_Area'].hist()","1dfa1827":"alt_feature_columns = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n       'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am',\n       'Hillshade_Noon', 'Hillshade_3pm',\n       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area',\n       'Soil_Type']\n\nskf = StratifiedKFold(n_splits=10)\n\nX = train_df[alt_feature_columns]\ny = train_df[target_column]\n\nscores = []\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    classifier = RandomForestClassifier(n_estimators=100, random_state=8675309)\n    classifier.fit(X_train, y_train)\n    predictions = classifier.predict(X_test)\n    scores.append(f1_score(y_test, predictions, average='micro'))\n    \nmean(scores)","0a195eb2":"ELU_CODES = (\n    2702,\n    2703,\n    2704,\n    2705,\n    2706,\n    2717,\n    3501,\n    3502,\n    4201,\n    4703,\n    4704,\n    4744,\n    4758,\n    5101,\n    5151,\n    6101,\n    6102,\n    6731,\n    7101,\n    7102,\n    7103,\n    7201,\n    7202,\n    7700,\n    7701,\n    7702,\n    7709,\n    7710,\n    7745,\n    7746,\n    7755,\n    7756,\n    7757,\n    7790,\n    8703,\n    8707,\n    8708,\n    8771,\n    8772,\n    8776,\n)\n\nlen(ELU_CODES)","565bdc8c":"train_df['ELU_Code'] = train_df['Soil_Type'].apply(lambda x: ELU_CODES[x-1])\ntrain_df['ELU_Code'].hist()","d790a90d":"test_df['ELU_Code'] = test_df['Soil_Type'].apply(lambda x: ELU_CODES[x-1])\ntest_df['ELU_Code'].hist()","afefb847":"train_df['ELU_1'] = train_df['ELU_Code'].apply(lambda code: int(str(code)[0]))\ntrain_df['ELU_1'].hist()","29328a6c":"test_df['ELU_1'] = test_df['ELU_Code'].apply(lambda code: int(str(code)[0]))\ntest_df['ELU_1'].hist()","51ef800b":"train_df['ELU_2'] = train_df['ELU_Code'].apply(lambda code: int(str(code)[1]))\ntrain_df['ELU_2'].hist()","88937974":"test_df['ELU_2'] = test_df['ELU_Code'].apply(lambda code: int(str(code)[1]))\ntest_df['ELU_2'].hist()","fccbf81f":"train_df['ELU_3'] = train_df['ELU_Code'].apply(lambda code: int(str(code)[2:]))\ntrain_df['ELU_3'].hist()","cd74fda8":"test_df['ELU_3'] = test_df['ELU_Code'].apply(lambda code: int(str(code)[2:]))\ntest_df['ELU_3'].hist()","bed06f46":"alt_feature_columns = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n       'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am',\n       'Hillshade_Noon', 'Hillshade_3pm',\n       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area',\n       'Soil_Type', 'ELU_1', 'ELU_2', 'ELU_3']\n\nskf = StratifiedKFold(n_splits=10)\n\nX = train_df[alt_feature_columns]\ny = train_df[target_column]\n\nscores = []\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    classifier = RandomForestClassifier(n_estimators=100, random_state=8675309)\n    classifier.fit(X_train, y_train)\n    predictions = classifier.predict(X_test)\n    scores.append(f1_score(y_test, predictions, average='micro'))\n    \nmean(scores)","8bc042a4":"def direction(azimuth):\n    if azimuth < 0 or azimuth > 360:\n        raise Exception(f'Azimuth {azimuth} out of bounds')\n    if azimuth > 337.5 or azimuth <= 22.5:\n        return 1 # 'N'\n    if azimuth >= 22.5 and azimuth < 67.5:\n        return 2 # 'NE'\n    if azimuth >= 67.5 and azimuth < 112.5:\n        return 3 # 'E'\n    if azimuth >= 112.5 and azimuth < 157.5:\n        return 4 # 'SE'\n    if azimuth >= 157.5 and azimuth < 202.5:\n        return 5 # 'S'\n    if azimuth >= 202.5 and azimuth < 247.5:\n        return 6 # 'SW'\n    if azimuth >= 247.5 and azimuth < 292.5:\n        return 7 # 'W'\n    if azimuth >= 292.5 and azimuth < 337.5:\n        return 8 # 'NW'\n    raise Exception(f'Azimuth {azimuth} out of bounds')","626d9f03":"train_df['Direction'] = train_df['Aspect'].apply(lambda a: direction(a))\ntrain_df['Direction']","ac4b143a":"test_df['Direction'] = test_df['Aspect'].apply(lambda a: direction(a))\ntest_df['Direction']","1055527f":"alt_feature_columns = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n       'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am',\n       'Hillshade_Noon', 'Hillshade_3pm',\n       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area',\n       'Soil_Type', 'ELU_1', 'ELU_2', 'ELU_3', 'Direction']\n\nskf = StratifiedKFold(n_splits=10)\n\nX = train_df[alt_feature_columns]\ny = train_df[target_column]\n\nscores = []\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    classifier = RandomForestClassifier(n_estimators=100, random_state=8675309)\n    classifier.fit(X_train, y_train)\n    predictions = classifier.predict(X_test)\n    scores.append(f1_score(y_test, predictions, average='micro'))\n    \nmean(scores)","7d57fef5":"import math\n\ntrain_df['Distance_To_Hydrology'] = (train_df['Horizontal_Distance_To_Hydrology']**2 + train_df['Vertical_Distance_To_Hydrology']**2).apply(lambda n: math.sqrt(n))\ntest_df['Distance_To_Hydrology'] = (test_df['Horizontal_Distance_To_Hydrology']**2 + test_df['Vertical_Distance_To_Hydrology']**2).apply(lambda n: math.sqrt(n))","30f8984f":"alt_feature_columns = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n       'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am',\n       'Hillshade_Noon', 'Hillshade_3pm',\n       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area',\n       'Soil_Type', 'ELU_1', 'ELU_2', 'ELU_3', 'Direction', 'Distance_To_Hydrology']\n\nskf = StratifiedKFold(n_splits=10)\n\nX = train_df[alt_feature_columns]\ny = train_df[target_column]\n\nscores = []\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    classifier = RandomForestClassifier(n_estimators=100, random_state=8675309)\n    classifier.fit(X_train, y_train)\n    predictions = classifier.predict(X_test)\n    scores.append(f1_score(y_test, predictions, average='micro'))\n    \nmean(scores)","14bf7470":"tree_count = len(classifier.estimators_)\n\nimportances = [0 for _ in range(len(feature_columns))]\n\nfor tree in classifier.estimators_:\n    for i, importance in enumerate(tree.feature_importances_):\n        importances[i] += importance\n\nfor i, feature_column in enumerate(alt_feature_columns):\n    print(f'{feature_column}: {importances[i]\/tree_count}')","31734fd5":"from sklearn.ensemble import ExtraTreesClassifier\n\nskf = StratifiedKFold(n_splits=10)\n\nX = train_df[alt_feature_columns]\ny = train_df[target_column]\n\nscores = []\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    classifier = ExtraTreesClassifier(n_estimators=100, random_state=8675309)\n    classifier.fit(X_train, y_train)\n    predictions = classifier.predict(X_test)\n    scores.append(f1_score(y_test, predictions, average='micro'))\n    \nmean(scores)","e0e3df88":"from sklearn.ensemble import GradientBoostingClassifier\n\nskf = StratifiedKFold(n_splits=10)\n\nX = train_df[alt_feature_columns]\ny = train_df[target_column]\n\nscores = []\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    classifier = GradientBoostingClassifier(n_estimators=100, random_state=8675309)\n    classifier.fit(X_train, y_train)\n    predictions = classifier.predict(X_test)\n    scores.append(f1_score(y_test, predictions, average='micro'))\n    \nmean(scores)","a6ac83aa":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\nX = train_df[alt_feature_columns]\ny = train_df[target_column]\n\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)\n\nskf = StratifiedKFold(n_splits=10)\nscores = []\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    classifier = SVC(random_state=8675309, gamma='scale')\n    classifier.fit(X_train, y_train)\n    predictions = classifier.predict(X_test)\n    scores.append(f1_score(y_test, predictions, average='micro'))\n    \nmean(scores)","a360611c":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nX = train_df[alt_feature_columns]\ny = train_df[target_column]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8675309, stratify=y)\n\nclassifier = GridSearchCV(\n    n_jobs=3,\n    verbose=True,\n    cv=5,\n    scoring=lambda e, x, y: f1_score(y, e.predict(x), average='micro'),\n    estimator=RandomForestClassifier(random_state=8675309),\n    param_grid={\n        'n_estimators': [250], # [250, 100, 50],\n        'criterion': ['entropy'], # ['gini', 'entropy'],\n        'min_samples_split': [2], # [2, 6, 12],\n        'min_samples_leaf': [1], # [1, 2, 4],\n        'max_depth': [None], # [None, 10, 100],\n        'bootstrap': [False], # [True, False],\n    },\n)\nclassifier.fit(X_train, y_train)\nprint(f'Best parameters: {classifier.best_params_}\\nBest score: {classifier.best_score_}')","cc1222d1":"classifier = classifier.best_estimator_\n\ntest_df['Cover_Type'] = classifier.predict(test_df[alt_feature_columns])\ntest_df[['Id', 'Cover_Type']].sample(10)","5a914b84":"submission = test_df[['Id', 'Cover_Type']]\nsubmission.to_csv('submission.csv', index=False)","ef5b456e":"import os\n\nos.listdir('.')","4aa89099":"with open('submission.csv') as f:\n    for _ in range(10):\n        print(f.readline(), end='')","b024c4eb":"## Initial Observations","89049201":"Now let's look at the ranges of values. To do this, we'll first create a helper function that finds the min, max, range, and percent differences for each for every column in our data.","7929bb1a":"## Distance to Hydrology\n\nAnd now let's calculate the actual distance to hydrology.","08ec0adb":"Next we will try to improve on our score by doing a bit of feature engineering.","3213baa9":"0.7849867724867725, a little better.","925ba122":"Looking at this, what have we learned about our data?\n\n 1. There are 54 feature columns (we won't count the `Id` column and `Cover_Type` is our target)\n 1. There are 4 yes\/no columns for wilderness area\n 1. There are 40 yes\/no columns for soil type that consume most of our distinct columns\n 1. `Soil_Type7` and `Soil_Type15` sare completely missing from our training data\n 1. All of the data is represented as integer data\n 1. There is no null data, so even missing data is filled in with at least zero values\n 1. The colums are orders of magnitude different (thousands vs. hundreds vs. tens) so we'll need to standardize and\/or normalize if we end up using an algorithm sensitive to scale\n 1. We have to deal with negative values in `Vertical_Distance_To_Hydrology`\n 1. `Hillshade_Noon` has a lower bound of 99, which might be problematic","bb5c74e8":"# Feature Engineering","23f73fab":"Let's now take a look at our testing data.","c8de5c86":"# Submission Time","7f69a658":"0.7841269841269841, which is a slight improvement.","24e417c3":"https:\/\/en.wikipedia.org\/wiki\/Azimuth\n\nFrom north\n\n* North\t0\u00b0\t\n* North-northeast\t22.5\u00b0\n* Northeast\t45\u00b0\n* East-northeast\t67.5\u00b0\n* East\t90\u00b0\n* East-southeast\t112.5\u00b0\n* Southeast\t135\u00b0\n* South-southeast\t157.5\u00b0\n* South\t180\u00b0\n* South-southwest\t202.5\u00b0\n* Southwest\t225\u00b0\n* West-southwest\t247.5\u00b0\n* West\t270\u00b0\n* West-northwest\t292.5\u00b0\n* Northwest\t315\u00b0\n* North-northwest\t337.5\u00b0\n","76fa1e04":"# GradientBoostingClassifier","07d27aba":"And now we can build a model and get a baseline for performance.","b4ddaf77":"## ELU Codes\n\nThe soil types map to ELU codes, which have a meaning of their own. Let's first just get the ELU codes documented.","80242c5d":"# SVC\n\nThe SVM classifer will be sensitive to the data being different scale, so let's scale our data and then use the classifer.[](http:\/\/)","0301795c":"And test it out.","496b98ed":"## Soil Type\n\nThe soil type is spread across 40 columns. This isn't a big deal for random forests, but might be problematic for neural networks. Let's first just put all of the soil types in a single 'Soil_Type' column.","d6bb3479":"## Aspect\n\nAzimuth can be tied to a direction. We can try to make the 0-360 range of azimuth be a directional. I'm not a big fan of this since it is actually reducing the amount of information that the model has available to use, but it is worth a shot.**","f050642b":"# Other Models\n\nLet's play with a few other models. Our current best score is 0.7861111111111111.","4e3b4a93":"And let's try out our new column.","d6b6b66d":"## ExtraTreesClassifier","9e100105":"# Hyperparameters\n\nRandomForest seemed to be the best. Let's tweak some hyperparamters now to see if we can get any improvement.\n\nSince this can take a while to run I just left in the best parameters that were found and left my search parameters as comments.","115e01c4":"Let's try out our new ELU columns.","538432a0":"I ended up with a mean score of 0.7805555555555556.","0055393f":"Each place in the code has some significance. We'll split the ELU into component parts. I had trouble determining exactly what each digit means, but we can get away with just splitting them out. Note that in most literature I found the thousands and hundreds place digit stand alone. The tens and ones place were combined.\n\n* [Reference](https:\/\/www.fws.gov\/northeast\/planning\/Lake%20Umbagog\/DraftCCP\/APP\/APPENDIX_L.pdf)\n","c5a80b02":"Let's see if there is any affect of using the single soil type instead of the multiple ones.","5239c990":"# Exploratory Data Analysis","934718ad":"And now we load and take a look at the training data.","d5e1b1a9":"Let's begin by loading the data and just seeing what we have to work with. I'm not a huge fan of describe, even when it is transposed, so for this I'll do my own describe. Since I'll be doing this for the test and training data let's first make a helper function.","d36fd68b":"0.7805555555555556, which is the same score... not too surprising.","2afe58da":"## Importance","607bd0b6":"## Wilderness Area\n\nWilderness area is also spread across a few columns. We can combine them into one column also.","ce8872f5":"# Baseline Model\n\nFirst, let's create a baseline model using the features as-is and see how well it performs.\n\nBegin by identifying the target column and the feature columns.","2441a1ec":"Let's try our new wilderness area column out.","ccde42b1":"Nothing too surprising here. We have the same columns as our training data aside from `Cover_Type`. Other interesting observations:\n\n 1. `Soil_Type7` and `Soil_Type15` do exist in this dataset\n 1. There are 565,892 testing data entries and 15,120 so we are projecting our training data out on about 37x\n 1. `Hillshade_3pm` maxes out at 248 in the training data, which is lower than the testing data."}}