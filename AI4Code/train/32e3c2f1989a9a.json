{"cell_type":{"24712f91":"code","84f705d0":"code","ca0e235b":"code","b2c9a304":"code","719d6ac1":"code","eb33c4b5":"code","27871b41":"code","28b0ca48":"code","1c75ec47":"code","c662902c":"code","f02d2ced":"code","612bb5f6":"code","df42158d":"code","d4ca24c0":"code","b30b3d73":"code","121e5139":"code","61a9ad0d":"code","1719dc05":"code","699d858d":"markdown"},"source":{"24712f91":"# pip install tensorflow-text (preprocessing for BERT inputs)\n# pip install -q tf-models-official\n# pip install --upgrade tensorflow_hub","84f705d0":"pip install tensorflow-text","ca0e235b":"pip install -q tf-models-official","b2c9a304":"# -- Import Libraries -- \nimport os\nimport numpy as np\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nfrom official.nlp import optimization\nfrom nltk.corpus import stopwords\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom tensorflow.keras import layers, losses, preprocessing\n\ntf.get_logger().setLevel('ERROR')","719d6ac1":"# -- Global Variables -- \nTRAIN_PATH = '..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv'  \nTEST_PATH = '..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv' \nCLASSES = ['Extremely Negative', 'Negative', 'Positive', 'Extremely Positive', 'Neutral']\nBATCH_SIZE = 128\nEPOCHS = 16\nLEARNING_RATE = 1e-05 #small gradient steps to prevent forgetting in transfer learning.\n\n# MODEL_NAME = 'bert-base-uncased'\ntfhub_handle_encoder = 'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-512_A-8\/1'\ntfhub_handle_preprocess = 'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3'","eb33c4b5":"# -- Load Data -- \ntrain_data = pd.read_csv(TRAIN_PATH, encoding='L1')\ntest_data = pd.read_csv(TEST_PATH,encoding='L1')\n\ntrain_data.head()","27871b41":"# -- Data distribution -- \nfig = plt.figure(figsize=(16, 4))\n\nx=train_data.Sentiment.value_counts()\n\nplt.bar(x=x.index,\n        height=x.values,\n        width=0.5)\n\nplt.title('Data Distribution')\n\nplt.show()","28b0ca48":"# -- Pre-processing -- \n\n# drop empty tweets, and unclassified tweets.\ntrain_data.OriginalTweet.dropna()\ntrain_data.Sentiment.dropna()\ntest_data.OriginalTweet.dropna()\ntest_data.Sentiment.dropna()\n\n# remove stop-words\nsw_nltk = stopwords.words('english')\nfunc = lambda text : \" \".join([word for word in str(text).split() if word.lower() not in sw_nltk])\ntrain_data['OriginalTweet'] = train_data['OriginalTweet'].apply(func)\n\n# TODO: remove Urls and HTML links","1c75ec47":"# -- Split Data to train, validation and test -- \ntrain_X, val_X, train_y, val_y = model_selection.train_test_split(train_data['OriginalTweet'],\n                                                                  train_data['Sentiment'], \n                                                                  test_size=0.3)\n\ntest_X, test_y = test_data['OriginalTweet'],test_data['Sentiment']","c662902c":"# -- convert labels to one hot --\nlabel_encoder = LabelEncoder()\n\nvec = label_encoder.fit_transform(train_y)\ntrain_y = tf.keras.utils.to_categorical(vec)\n\nvec = label_encoder.fit_transform(val_y)\nval_y = tf.keras.utils.to_categorical(vec)\n\nvec = label_encoder.fit_transform(test_y)\ntest_y = tf.keras.utils.to_categorical(vec)","f02d2ced":"# -- Creating the Model for Fine Tuning -- \ndef bert_text_classification():\n\n    # - text input -\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n        \n    # - preprocessing layer - \n    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n        \n    # - encoding - \n    encoder_inputs = preprocessing_layer(text_input)\n    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n        \n    # - output -\n    outputs = encoder(encoder_inputs)\n        \n    # - classifier layer -\n    net = outputs['pooled_output']\n    net = tf.keras.layers.Dropout(0.2)(net)\n    net = tf.keras.layers.Dense(5, activation='softmax', name='classifier')(net)\n    \n    model = tf.keras.Model(text_input, net)\n    return model\n        \nmodel = bert_text_classification()     ","612bb5f6":"# running the model on some random text, of course that doesn't mean anything, since we haven't trained the model yet, \n# it just so we will be able to print the model structure \ntest_text = ['some random tweet']\nbert_raw_result = model(tf.constant(test_text))\n\n# -- Model structure -- \ntf.keras.utils.plot_model(model)","df42158d":"# -- Loss -- \nloss = tf.keras.losses.CategoricalCrossentropy()\n\n# -- Optimizer -- \n# will use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). \ntrain_data_size = len(train_X)\nsteps_per_epoch = int(train_data_size\/BATCH_SIZE)\nnum_train_steps = steps_per_epoch * EPOCHS\nnum_warmup_steps = int(0.1*num_train_steps\/BATCH_SIZE)\n\noptimizer = optimization.create_optimizer(init_lr=LEARNING_RATE,\n                                          num_train_steps=num_train_steps,\n                                          num_warmup_steps=num_warmup_steps,\n                                          optimizer_type='adamw')\n\n# -- compile the model --\nmodel.compile(optimizer=optimizer,\n              loss=loss,\n              metrics=['accuracy'])","d4ca24c0":"# -- Fine Tuning the Model --\nhistory = model.fit(x=train_X,\n                    y=train_y,\n                    validation_data=(val_X, val_y),\n                    epochs=EPOCHS,\n                    validation_steps=1,\n                    verbose=1,\n                    batch_size=BATCH_SIZE)","b30b3d73":"# -- Plot training and validation loss and accuracy --\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n    \nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(12,8))\n\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n    \nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Loss - Cross Entropy')\nplt.xlabel('epoch')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\n    \nplt.show()","121e5139":"# -- Testing --\nloss, acc = model.evaluate(x=test_X,\n                           y=test_y)\nprint(\"test loss: \", loss, \", test acc: \", 100*acc, \"%\")","61a9ad0d":"# -- Save the Model -- \nmodel.save('classifier_model')","1719dc05":"# -- Confusion matrix -- \nplt.title('confusion matrix - train data')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\n\npreds = model.predict(train_X)\n\n# Convert the one-hot vectors to labels\ntrain_y_labels = tf.argmax(train_y, axis = 1)\npreds_labels = tf.argmax(preds, axis = 1)\n\ncm_train = tf.math.confusion_matrix(train_y_labels,\n                                    preds_labels,5,\n                                    dtype=tf.dtypes.float32)\n\n# Normalize the confusion matrix so that each row sums to 1.\ncm_train = cm_train\/cm_train.numpy().sum(axis=1)[:, tf.newaxis]\n\nsns.heatmap(data=cm_train,\n            annot=True,\n            xticklabels=CLASSES,\n            yticklabels=CLASSES)\n\nplt.show()","699d858d":"# **Covid19 Text-Classification - NLP Bert - TensorFlow**\n\nreferences:\n- Huggingface Bert model: https:\/\/huggingface.co\/bert-base-uncased\n- https:\/\/www.tensorflow.org\/tutorials\/text\/classify_text_with_bert\n- https:\/\/www.tensorflow.org\/hub\/api_docs\/python\/hub\/KerasLayer\n"}}