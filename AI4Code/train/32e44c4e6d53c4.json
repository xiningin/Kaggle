{"cell_type":{"ea8fcf16":"code","6229d433":"code","10c87644":"code","2ac20455":"code","a0e1e5f2":"code","a4b1d3ea":"code","47e0d7fe":"code","c3ff3b67":"code","bdbb23ab":"code","1b3708de":"code","ab8444e4":"code","1e4d962e":"code","b8134211":"code","907a5d75":"code","cb38a0ca":"code","ca2236c1":"code","2894e756":"code","bb0f2d13":"code","2f4795f5":"code","4768dc73":"code","41b0f87f":"code","88e7c4bc":"code","079544d5":"code","447d5188":"code","17b13f91":"code","972ac71f":"code","0d0f5e6b":"code","807289ff":"code","15ab6617":"code","37104f0c":"code","a9cc9f24":"markdown","539c1829":"markdown","94549293":"markdown","cf34e17b":"markdown","d7a72acd":"markdown","b66061e0":"markdown","2ae1e2a3":"markdown","86ff4534":"markdown","a477288c":"markdown","971dc07b":"markdown","14935b59":"markdown","da7d5a8a":"markdown"},"source":{"ea8fcf16":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfor dirname, _, filenames in os.walk('\/kaggle\/output'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6229d433":"import os\nimport re\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nfrom gensim.models.phrases import Phrases, Phraser\n\nimport spacy\nfrom nltk.corpus import stopwords\n\n# for plotting\nimport pyLDAvis\nimport pyLDAvis.gensim\nimport matplotlib.pyplot as plt\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","10c87644":"# # import nltk\n# # nltk.download('stopwords')\n# #python -m spacy download en\n!wget http:\/\/mallet.cs.umass.edu\/dist\/mallet-2.0.8.zip\n!unzip mallet-2.0.8.zip\n# os.listdir('\/kaggle\/working')","2ac20455":"IN_PATH = '\/kaggle\/input\/enron-dataset-cleaned'\nFILE_NAME = 'emails.csv'\nOUT_PATH = '\/kaggle\/working'\n\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\nmallet_path = os.path.join(OUT_PATH, '.\/mallet-2.0.8\/bin\/mallet')\n\n\nemails = pd.read_csv(os.path.join(IN_PATH, FILE_NAME))\n# emails_subset = emails[:10000]\nemails_subset = emails.sample(frac=0.02, random_state=1)","a0e1e5f2":"def parse_raw_message(raw_message):\n    '''\n        Funtion for cleanning each email..\n    '''\n    lines = raw_message.split('\\n')\n    email = {}\n    message = ''\n    keys_to_extract = ['from', 'to']\n    for line in lines:\n        if ':' not in line:\n            message += line.strip()\n            email['body'] = message\n        else:\n            pairs = line.split(':')\n            key = pairs[0].lower()\n            val = pairs[1].strip()\n            if key in keys_to_extract:\n                email[key] = val\n    return email\n\ndef map_to_list(emails, key):\n    '''\n        Helper Function for parse_into_emails to wrap things up!\n    '''\n    results = []\n    for email in emails:\n        if key not in email:\n            results.append('')\n        else:\n            results.append(email[key])\n    return results\n\ndef parse_into_emails(messages):\n    '''\n        Function for cleaning all emails and returning them as a dictionary\n    '''\n    emails = [parse_raw_message(message) for message in messages]\n    return {\n        'body': map_to_list(emails, 'body'),\n        'to': map_to_list(emails, 'to'),\n        'from_': map_to_list(emails, 'from')\n    }\n\n\ndef sent_to_words(sentences):\n    '''\n        # tokenize - break down each sentence into a list of words\n    '''\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n        \n\n# remove stop_words, make bigrams and lemmatize\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent))\n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","a4b1d3ea":"## Before preprocessing\nprint(emails_subset.shape)\nemails_subset.head()","47e0d7fe":"## After preprocessing\nemail_df = pd.DataFrame(parse_into_emails(emails_subset.message))\nprint(email_df.shape)\nemail_df.head()","c3ff3b67":"print(email_df.iloc[3]['body'])","bdbb23ab":"# Convert the body of the emails to a list\ndata = email_df.body.values.tolist()\n\n# Convert the list of sentence into list of words <> Tokenizing\ndata_words = list(sent_to_words(data))\n\nprint(data_words[3])","1b3708de":"## Creating models for <> bigram and trigram \nbigram = Phrases(data_words, min_count=5, threshold=100)\ntrigram = Phrases(bigram[data_words], threshold=100)\n\n## Faster way to get a sentence clubbed as a trigram\/bigram\nbigram_mod = Phraser(bigram)\ntrigram_mod = Phraser(trigram)\n\n## Trigram example\nprint(trigram_mod[bigram_mod[data_words[3]]])","ab8444e4":"# Remove Stop Words\ndata_words_nostops = remove_stopwords(data_words)\n\n# Form Bigrams\ndata_words_bigrams = make_bigrams(data_words_nostops)\n\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[3])","1e4d962e":"'''\nEach time we now see a token in a text, information on its frequency is paired with it.\nA word\/token like contract could then be represented as (6, 3) \u2014 >(token_id, token_count).\\\n'''\n\n## Creating Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n## Create Corpus\ntexts = data_lemmatized\n\n## Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n","b8134211":"lda_model = gensim.models.ldamodel.LdaModel(\n    corpus=corpus,    # Stream of document vectors or sparse matrix of shape (num_terms, num_documents)\n    id2word=id2word,  # It is used to determine the vocabulary size, as well as for debugging and topic printing.\n    num_topics=6,    # The number of requested latent topics to be extracted from the training corpus.\n    random_state=100, # Useful for reproducibility.\n    update_every=1,   # Set to 0 for batch learning, > 1 for online iterative learning.\n    chunksize=100,    # Number of documents to be used in each training chunk.\n    passes=10,        # Number of passes through the corpus during training.\n    alpha='auto',     # auto: Learns an asymmetric prior from the corpus\n    per_word_topics=True \n    # If True, the model also computes a list of topics, sorted in descending order of most likely topics for each word,\n    # along with their phi values multiplied by the feature-length (i.e. word count)\n)","907a5d75":"print(lda_model.print_topics())# The weights reflect how important a keyword is to that topic.\n","cb38a0ca":"doc_lda = lda_model[corpus]\n\n# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","ca2236c1":"# Visualize the topics\npyLDAvis.enable_notebook(sort=True)\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n\npyLDAvis.display(vis)","2894e756":"ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=6, id2word=id2word)","bb0f2d13":"# Show Topics\nprint(ldamallet.show_topics(formatted=False))","2f4795f5":"# Compute Coherence Score\ncoherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_ldamallet = coherence_model_ldamallet.get_coherence()\nprint('\\nCoherence Score: ', coherence_ldamallet)","4768dc73":"##  Converting lda mallet to lda model for visualizing with pyLDAvis\nmodel = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)\n\n# Visualize the topics with mallet model\npyLDAvis.enable_notebook(sort=True)\nvis = pyLDAvis.gensim.prepare(model, corpus, id2word)\npyLDAvis.display(vis)","41b0f87f":"# def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n#     \"\"\"\n#     Compute c_v coherence for various number of topics\n\n#     Parameters:\n#     ----------\n#     dictionary : Gensim dictionary\n#     corpus : Gensim corpus\n#     texts : List of input texts\n#     limit : Max num of topics\n\n#     Returns:\n#     -------\n#     model_list : List of LDA topic models\n#     coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n#     \"\"\"\n#     coherence_values = []\n#     model_list = []\n#     for num_topics in tqdm(range(start, limit, step)):\n#         model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n#         model_list.append(model)\n#         coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n#         coherence_values.append(coherencemodel.get_coherence())\n\n#     return model_list, coherence_values","88e7c4bc":"# # run\n# model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)","079544d5":"# # Show graph\n# limit=40; start=2; step=6;\n# x = range(start, limit, step)\n# plt.plot(x, coherence_values)\n# plt.xlabel(\"Num Topics\")\n# plt.ylabel(\"Coherence score\")\n# plt.legend((\"coherence_values\"), loc='best')\n# plt.show()","447d5188":"# # Print the coherence scores\n# for m, cv in zip(x, coherence_values):\n#     print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))","17b13f91":"# Select the model and print the topics\n# optimal_model = model_list[4]\noptimal_model = ldamallet\nmodel_topics = optimal_model.show_topics(formatted=False)\nprint(optimal_model.print_topics(num_words=8))","972ac71f":"def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)","0d0f5e6b":"df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']","807289ff":"df_dominant_topic.head(10)","15ab6617":"df_dominant_topic.shape","37104f0c":"df_dominant_topic[:300].to_csv('final.csv')","a9cc9f24":"## Mallet for Topic Modeling","539c1829":"## Analysing Data","94549293":"## Preprocessing for LDA","cf34e17b":"## Downloads","d7a72acd":"## Utils","b66061e0":"## Imports","2ae1e2a3":"checking for which number of topics best suits?","86ff4534":"## Config","a477288c":"Since Coherence Score of Mattel is good we go with Mattel.","971dc07b":"# Step-2 : LDA && Topic Modeling","14935b59":"## Building LDA Model","da7d5a8a":"# Step-1 : Initial Setup"}}