{"cell_type":{"d8b0dbb7":"code","cb11827a":"code","9ebefd21":"code","3d9539f9":"code","19c55dc7":"code","76dc5099":"code","a683b3a4":"code","a5d4a74d":"code","a3cc3a69":"code","a093ed1d":"code","1d75c1b7":"code","e5d6ab01":"code","f99e8590":"code","e6317d73":"code","bef01f32":"code","785a2c14":"code","6920ec60":"code","a2eeb762":"code","a4003d43":"code","4a951ecc":"code","e812a153":"code","13a1afc7":"code","8a047899":"code","b6469bfa":"code","e4c26999":"code","5f9c6f72":"code","49f0740f":"code","8d8ace54":"code","414b0755":"code","3d133f4a":"code","35ed8c17":"code","5061dbe5":"code","c9c80e13":"markdown","da834b68":"markdown","242d1b1b":"markdown","d6ad84df":"markdown","15eb5aa0":"markdown","36c076b2":"markdown","9a9903c9":"markdown","dde7e014":"markdown","cec1c89a":"markdown","cee369e9":"markdown","ddbf30f1":"markdown","dcd3392f":"markdown","bc08787c":"markdown","af60c822":"markdown","1ac77e1d":"markdown","0fa3d987":"markdown","0edc5cdd":"markdown","9b1f3f85":"markdown"},"source":{"d8b0dbb7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf","cb11827a":"df_comments = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\ndf_comments.head()","9ebefd21":"df_toxic = df_comments.loc[:,[\"worker\",\"more_toxic\"]]\ndf_toxic[\"Target\"] = 1\n\n\ndf_normal = df_comments.loc[:,[\"worker\",\"less_toxic\"]]\ndf_normal[\"Target\"] = 0\n\n\ndf_toxic.columns = [\"worker\",\"Text\",\"Target\"]\ndf_normal.columns = [\"worker\",\"Text\",\"Target\"]\n\n\ndf_final = pd.concat([df_toxic,df_normal],axis=0)\n\ndf_final.shape\n\n\n","3d9539f9":"df_final","19c55dc7":"df_final[\"Text\"] = df_final[\"Text\"].apply(lambda x: x.lower())\ndf_final","76dc5099":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import LSTM,Dense,Dropout,Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer","a683b3a4":"import re\n\npattern = \"[,!\\?\\.\\n=]*\\s+\"\ndf_final[\"Text\"] = df_final[\"Text\"].apply(lambda x: re.split(pattern,x))\nTc = Tokenizer()\ndf_final[\"Text\"].iloc[3]","a5d4a74d":"def remove_puntuations(words):\n    words = [re.sub(\"[\\n\\.\\?!,:]\",\"\", x) for x in words if x != \"\"]\n    return words\n\ndf_final[\"Text\"] = df_final[\"Text\"].apply(remove_puntuations)\ndf_final[\"Text\"].iloc[3]","a3cc3a69":"sent = df_final[\"Text\"].iloc[3]\nsentences = list(df_final[\"Text\"])\n\ntc = Tokenizer()\nTc.fit_on_texts(sentences)\nTc.texts_to_sequences(sent)","a093ed1d":"Tc.texts_to_sequences([\"hey\",\"cricketer\", \"Down\", \"To\", \"Earth\", \"Still\", \"You\", \"Leave\", \"Wicket\"])","1d75c1b7":"\ndef generate_seq(sent,length=30):\n    seq = [x[0] if len(x) == 1 else 0 for x in sent]\n    if len(seq) < length:\n        res = [0 for x in range(length - len(seq))]\n        res.extend(seq)\n    elif len(seq) > length:\n        res = seq[:length]\n    else:\n        res = list(seq)\n    return res","e5d6ab01":"x = Tc.texts_to_sequences(sent)\nprint(generate_seq(x,length=30),len(generate_seq(x,length=30)))","f99e8590":"seq = [[i] for i in range(51)]\n\n### You should get everything except 50\nprint(generate_seq(seq,length=50))","e6317d73":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(df_final[\"Text\"],df_final[\"Target\"],train_size=0.7,random_state=30)","bef01f32":"def prepare_training(X_train,y_train,tokenizer=Tc):\n    label = np.zeros((y_train.shape[0],2))\n    X_train = X_train.apply(lambda x: tokenizer.texts_to_sequences(x))\n    X_train = X_train.apply(lambda x: generate_seq(x,length=300))\n    X_train = list(X_train)\n    X_train = np.asarray(X_train)\n    for i,y in enumerate(y_train):\n        if y == 0:\n            label[i,0] = 1\n        else:\n            label[i,1] = 1 \n            \n    return X_train,label\n    ","785a2c14":"train_x, train_y = prepare_training(X_train,y_train,tokenizer=Tc)\nval_x, val_y = prepare_training(X_test,y_test,tokenizer=Tc)","6920ec60":"num_words = max(Tc.word_index.values()) + 1\nmodel = Sequential()\nmodel.add(Embedding(num_words,200,input_length=100,trainable=False))\nmodel.add(LSTM(256,input_shape=(None,100,200)))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(2,activation=\"softmax\"))\n\nmodel.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=\"categorical_accuracy\")\nmodel.summary()","a2eeb762":"history = model.fit(train_x,train_y,validation_data=(val_x,val_y),batch_size=300,epochs=50)","a4003d43":"df_final = pd.concat([df_toxic,df_normal],axis=0)\ndf_final","4a951ecc":"plt.plot(range(len(history.history['loss'])),history.history[\"loss\"])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"loss\")","e812a153":"df_final[\"Text\"] = df_final[\"Text\"].apply(lambda x: x.lower())\npuntuations = \"[\\.\\?\\n!=#\\$,;]+\\s*\"\ndf_final[\"Text\"] = df_final[\"Text\"].apply(lambda x: re.sub(puntuations,\" \",x))\ndf_final","13a1afc7":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nX_train,X_test,y_train,y_test = train_test_split(df_final[\"Text\"],df_final[\"Target\"],random_state=30,train_size=0.8)\n\n### Intanciate vectorizer\nvc = TfidfVectorizer()\n\nX_train = vc.fit_transform(X_train)\n\n\nx_test = vc.transform(X_test)","8a047899":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV","b6469bfa":"rf = RandomForestClassifier(random_state=300,max_depth=100)\n\nmodel = rf.fit(X_train,y_train)\n\n","e4c26999":"from sklearn.metrics import confusion_matrix","5f9c6f72":"y_train_pred = rf.predict(X_train)\ncf = confusion_matrix(y_train,y_train_pred)\ncf","49f0740f":"def metric(cf):\n    acc = (cf[0,0]+cf[1,1])\/np.sum(cf)\n    si = (cf[1,1]\/(cf[1,0] + cf[1,1]))\n    spec = (cf[0,0]\/(cf[0,1] + cf[0,0]))\n    print(f\"acc= {acc} recall= {si} specificity={spec}\")\n\nmetric(cf)","8d8ace54":"\nX_test = vc.transform(X_test)\ny_test_pred = rf.predict(X_test)\ncf = confusion_matrix(y_test,y_test_pred)\ncf","414b0755":"metric(cf)","3d133f4a":"df_test = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\ndf_test.shape","35ed8c17":"df_test[\"text\"] = df_test[\"text\"].apply(lambda x: x.lower())\ndf_test[\"text\"] = df_test[\"text\"].apply(lambda x: re.sub(puntuations,\" \",x))\nX_final = vc.transform(df_test[\"text\"])\ny_final = rf.predict(X_final)","5061dbe5":"df_submission = df_test.loc[:,[\"text\"]]\ndf_submission[\"score\"] = y_final\n\ndf_submission.to_csv(\"submission.csv\")","c9c80e13":"### Let us still go for running on model on test set","da834b68":"### Let us use random forest classifier ","242d1b1b":"### Let us evaluate","d6ad84df":"### Let us experiment on large sequence","15eb5aa0":"### Let us label the data","36c076b2":"### As the word cricketer is not there in corpus it has assigned empty token\n\n#### It is always important to have a fixed length sequence in RNN let us do preprocessing\n\n- Padding will at the start (Pre padding)\n- Truncation will be post","9a9903c9":"### Build LSTM","dde7e014":"### Let us use deep Learning Approach for the model\n\n## Step 1:\n#### Converting text to lower Case","cec1c89a":"### Let us vectorize the text","cee369e9":"### Let us experiment on sample sequence","ddbf30f1":"### Let us prepare vocubalory for training","dcd3392f":"### Let us build LSTM on top of data\n\n#### Let us split the data to train and validation","bc08787c":"### Let us remove puntuations in every word","af60c822":"#### Let us prepare the data","1ac77e1d":"### LSTM did not give good results\n1. Vanishing and exploding gradient problem","0fa3d987":"### model still overfitting","0edc5cdd":"### Import Deep Learning Libraries","9b1f3f85":"# Let us try on unseen words "}}