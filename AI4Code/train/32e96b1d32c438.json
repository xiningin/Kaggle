{"cell_type":{"34a05c1e":"code","d9f8a9a7":"code","e53f170f":"code","b1cf01c0":"code","eafda75a":"code","4f310c92":"code","676b1be9":"code","52d487cc":"code","46bbf1c9":"code","1a910432":"code","766ff972":"code","83e7a336":"code","f858b874":"code","5e79833c":"code","c22b9858":"code","a52e4a42":"code","3a19ba20":"code","f7c3bf5c":"code","46480e61":"code","bbf1b3dc":"code","933bd965":"code","19205fe2":"code","9d5ac950":"code","7e3e0b72":"code","8182cf81":"code","1bdb2655":"code","5fec8400":"code","3aa0fbd8":"code","9652abf4":"code","930f903d":"code","c12f3646":"code","6015914f":"code","de1884cc":"code","616e4863":"code","c178d9b7":"code","eb948444":"code","71f0edb0":"code","81d3aaed":"code","349b3972":"code","980d89c8":"code","bc179f58":"markdown","ad975a55":"markdown","67440b69":"markdown","6a1d97d4":"markdown","cdee2e91":"markdown","739ac92a":"markdown","2eb9c3e0":"markdown","c7427a62":"markdown","63652762":"markdown","132c9c0c":"markdown","1dfbd840":"markdown","1c491a32":"markdown","a5ce9d4e":"markdown","23a2c078":"markdown","1ffa42d6":"markdown","39fbb0ae":"markdown","248082e7":"markdown","6cbf8940":"markdown","d6f6e3fe":"markdown","848c998c":"markdown","f43a6d91":"markdown","7ffa4f2d":"markdown","f918f2ca":"markdown","8fcf7175":"markdown","35957e33":"markdown","26c142a2":"markdown","6b35e3c7":"markdown","1ae9ceea":"markdown","7cef523b":"markdown","8f5cb00e":"markdown","e33b1f2a":"markdown","e266ff82":"markdown","f4ce9625":"markdown","41cff24b":"markdown","cd185f11":"markdown","06e3be16":"markdown","684a32ca":"markdown","d257cd0d":"markdown","1bd1c65b":"markdown","a7e7697e":"markdown","04f88fe1":"markdown","46032f2c":"markdown","eff2e58f":"markdown","c1b89b5d":"markdown","721075b3":"markdown","48b9815d":"markdown","631a6239":"markdown"},"source":{"34a05c1e":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d9f8a9a7":"import sys\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sbn\nfrom scipy.stats import ks_2samp,wasserstein_distance,energy_distance\nfrom sklearn.pipeline import Pipeline,FeatureUnion\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport random\nfrom imblearn.over_sampling import RandomOverSampler,SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.combine import SMOTEENN\n\nfrom sklearn.metrics import precision_recall_curve,auc\nfrom sklearn.model_selection import cross_val_score,StratifiedKFold,train_test_split,GridSearchCV,ParameterGrid\n\n\n# Show plot output in the notebook\n%matplotlib inline\n\nsbn.set_style(\"darkgrid\")","e53f170f":"credit_card_data=pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","b1cf01c0":"credit_card_data.head(30)","eafda75a":"descriptive_stats=credit_card_data.describe()\ndescriptive_stats","4f310c92":"df_info=credit_card_data.info()\ndf_info","676b1be9":"# check more formally if there is any missing data\n# isnull() checks each entry, and the following any() check across rows and columns\ncredit_card_data.isnull().any().any()","52d487cc":"# Create the dataframe for the target\nlabels=credit_card_data[['Class']]\n\n# Create the dataframe for the features\nfeatures=credit_card_data.drop(['Class'],axis=1)","46bbf1c9":"labels['Class'].value_counts()","1a910432":"number_of_frauds=labels['Class'].value_counts()[1]\nnumber_of_nonfrauds=len(labels['Class'])-number_of_frauds\n\nfig,ax=plt.subplots(1,1,figsize=(10,10))\nsbn.countplot(labels['Class'],ax=ax)\nax.set_title('Number of examples in each class (logarithmic scale)',fontsize=16)\nax.set_yscale('log')\nax.text(0-0.4,number_of_nonfrauds+10000,'Number of non-frauds: '+str(number_of_nonfrauds),fontsize=14)\nax.text(1-0.4,number_of_frauds+150,'Number of frauds: '+str(number_of_frauds),fontsize=14)\n\n\nplt.show()","766ff972":"fraud_ratio=number_of_frauds\/(len(labels['Class']))\nprint('Fraudulent examples comprise the {0:.4f} % of the total examples'.format(100*fraud_ratio))","83e7a336":"descriptive_stats[['Time']]","f858b874":"time_in_hours=(features[['Time']]\/(60.0*60.0)).astype(int)\ntime_in_hours.describe()","5e79833c":"# Take the value counts from the time_in_hours and convert to dataframe.\n# The index of the new dataframe now corresponds to the hour\ntransactions_per_hour=time_in_hours['Time'].value_counts(sort=False).to_frame()\n\n# Reset the index to the dataframe so that the previous index becomes column\ntransactions_per_hour.reset_index(inplace=True)\n\n# Change the name of the columns for better comprehension\ntransactions_per_hour.columns=['Hour','Transactions']\n\ntransactions_per_hour.head(10)","c22b9858":"transactions_info=transactions_per_hour[['Transactions']].describe()\ntransactions_info","a52e4a42":"fig,axes=plt.subplots(1,2,figsize=(10,5))\n\nax=axes[0]\n\nax.hist(features['Time']\/(60*60),bins=range(48))\nax.set_xticks([0,8,16,24,32,40,47])\nax.set_xlim([-1,49])\nax.set_title('Number of transactions per hour',fontsize=16)\nax.set_xlabel('Time [hours]',fontsize=14)\nax.set_ylabel('Counts' ,fontsize=14)\n\nax=axes[1]\nax.hist(transactions_per_hour['Transactions'],bins=10)\nax.set_title('Counts of the hourly transaction number',fontsize=16)\nax.set_xlabel('Hourly transactions',fontsize=14)\n\n\nplt.subplots_adjust(wspace = 0.5)\nplt.show()","3a19ba20":"fig = plt.figure(figsize=(15, 10))\ngrid = plt.GridSpec(3, 3, wspace=0.4, hspace=0.3)\n\nax1=fig.add_subplot(grid[0, 0])\nax2=fig.add_subplot(grid[1, 0])\nax3=fig.add_subplot(grid[2, 0])\nax4=fig.add_subplot(grid[:,1:])\n\n\nax=ax1\n\nax.hist(features['Time']\/(60*60),bins=range(48))\nax.set_xticks([0,8,16,24,32,40,47])\nax.set_xlim([-1,49])\nax.set_title('Number of transactions per hour',fontsize=14)\nax.set_ylabel('Counts' ,fontsize=14)\n\nax=ax2\n\nax.hist(features[labels['Class']==0]['Time']\/(60*60),bins=range(48))\nax.set_xticks([0,8,16,24,32,40,47])\nax.set_xlim([-1,49])\nax.set_title('Number of non-fraudulent transactions per hour',fontsize=14)\nax.set_ylabel('Counts' ,fontsize=14)\n\nax=ax3\nax.hist(features[labels['Class']==1]['Time']\/(60*60),bins=range(48))\nax.set_xticks([0,8,16,24,32,40,47])\nax.set_xlim([-1,49])\nax.set_title('Number of fraudulent transactions per hour',fontsize=14)\nax.set_ylabel('Counts' ,fontsize=14)\nax.set_xlabel('Time [hours]',fontsize=14)\n\nax=ax4\nsbn.distplot(features[labels['Class']==0]['Time']\/(60*60),\n             kde=True,hist=True,\n             ax=ax,label='No Fraud',\n             color='green',\n             norm_hist=True\n             )\nsbn.distplot(features[labels['Class']==1]['Time']\/(60*60),\n             kde=True,\n             hist=True,\n             norm_hist=True,\n             ax=ax,label='Fraud',color='red')\n\n# sbn.distplot(features['Time']\/(60*60),kde=True,hist=False,ax=ax,label='Total')\nax.set_xlabel('Time [hours]',fontsize=14)\nax.set_title('Distribution of frauds and non-frauds vs hours',fontsize=15)\n# Use log scale to emphasize big (relative) differences\nax.set_yscale('log')\nplt.show()","f7c3bf5c":"# Get the frauds\ntime_of_frauds=credit_card_data[credit_card_data['Class']==1][['Time']]\n# Calculate the difference with previous row and add it as an additional column\ntime_of_frauds['Time difference']=time_of_frauds['Time'].diff()\ntime_of_frauds.head()","46480e61":"fig,axes=plt.subplots(1,2,figsize=(10,5))\n\n# Set the bin edges to correspond to every 10 seconds\nbins=[10*i for i in range(int(len(time_of_frauds)\/10))]\n\n# Select time difference up to which we zoom in\nseconds_to_zoom=100\n# Set the corresponding bins every 10 seconds\nbins_zoom=[10*i for i in range(int(seconds_to_zoom\/10)+1)]\n\nax=axes[0]\nsbn.distplot(time_of_frauds['Time difference'].dropna(),ax=ax,norm_hist=False,kde=False,bins=bins)\nax.set_xlabel('Time difference [seconds]',fontsize=13)\nax.set_ylabel('Counts of fraudulent transactions',fontsize=13)\nax.set_title('Time difference distribution between\\n consequtive frauds',fontsize=16)\nax.set_xticks(bins[::5])\n\nax=axes[1]\nsbn.distplot(time_of_frauds['Time difference'].dropna(),ax=ax,norm_hist=False,kde=False,bins=bins_zoom)\nax.set_xlabel('Time difference [seconds]',fontsize=13)\nax.set_ylabel('Counts of fraudulent transactions',fontsize=13)\nax.set_title('Time difference distribution between\\n consequtive frauds (zoomed in)',fontsize=16)\nax.set_xlim([0,100])\nax.set_xticks(bins_zoom)\n\n\nplt.subplots_adjust(wspace = 0.5)\n\nplt.show()","bbf1b3dc":"# Get the non - frauds\ntime_of_nonfrauds=credit_card_data[credit_card_data['Class']==0][['Time']]\n# Calculate the difference with previous row and add it as an additional column\ntime_of_nonfrauds['Time difference']=time_of_nonfrauds['Time'].diff()\ntime_of_nonfrauds.head()","933bd965":"fig,axes=plt.subplots(1,2,figsize=(10,5))\n\n# Set the bin edges to correspond to every 10 seconds\nbins=[10*i for i in range(int(len(time_of_frauds)\/10))]\n\n# Select time difference up to which we zoom in\nseconds_to_zoom=100\n# Set the corresponding bins every 10 seconds\nbins_zoom=[10*i for i in range(int(seconds_to_zoom\/10)+1)]\n\nax=axes[0]\nsbn.distplot(time_of_nonfrauds['Time difference'].dropna(),ax=ax,norm_hist=False,kde=False,bins=bins)\nax.set_xlabel('Time difference [seconds]',fontsize=13)\nax.set_ylabel('Counts of non fraudulent transactions',fontsize=13)\nax.set_title('Time difference distribution between\\n consequtive non frauds',fontsize=16)\nax.set_xticks(bins[::5])\n\nax=axes[1]\nsbn.distplot(time_of_nonfrauds['Time difference'].dropna(),ax=ax,norm_hist=False,kde=False,bins=bins_zoom)\nax.set_xlabel('Time difference [seconds]',fontsize=13)\nax.set_ylabel('Counts of non fraudulent transactions',fontsize=13)\nax.set_title('Time difference distribution between\\n consequtive non frauds (zoomed in)',fontsize=16)\nax.set_xlim([0,100])\nax.set_xticks(bins_zoom)\n\n\nplt.subplots_adjust(wspace = 0.5)\n\nplt.show()","19205fe2":"# Get the non - frauds\ntime_diff=credit_card_data[['Time','Class']]\n# Calculate the difference with previous row and add it as an additional column\ntime_diff['Time difference']=time_diff['Time'].diff()\ntime_diff[['Time difference']].describe()","9d5ac950":"fig,ax=plt.subplots(1,1)\nsbn.distplot(time_diff[time_diff['Class']==0]['Time difference'].dropna(),\n             kde=False,bins=[5*i for i in range(7)],\n             label='Non-fraud')\nsbn.distplot(time_diff[time_diff['Class']==1]['Time difference'].dropna(),\n             kde=False,color='green',\n             bins=[5*i for i in range(7)],\n             label='Fraud')\nax.set_yscale('log')\nax.set_xlabel('Time difference [seconds]',fontsize=13)\nax.set_ylabel('Counts of transactions',fontsize=13)\nax.set_title('Type of transactions vs. time difference \\nbetween consequtive transactions',fontsize=16)\nax.legend()\nplt.show()","7e3e0b72":"features[['Amount']].describe()","8182cf81":"fig,axes=plt.subplots(1,2,figsize=(10,6))\n\nax=axes[0]\n\nsbn.distplot(credit_card_data[credit_card_data['Class']==0]['Amount'],\n             ax=ax,\n             kde=False,\n             norm_hist=False,\n             bins=20,\n             color='green',\n             label='No Fraud')\n\nsbn.distplot(credit_card_data[credit_card_data['Class']==1]['Amount'],\n             ax=ax,\n             kde=False,\n             norm_hist=False,\n             bins=20,\n             color='red',\n             label='Fraud')\n\nax.set_yscale('log')\nax.set_title('Histogram of \"Amount\" for each class',fontsize=16)\nax.set_xlabel('Amount',fontsize=13)\nax.set_ylabel('Counts',fontsize=13)\n\nax=axes[1]\n\nsbn.distplot(credit_card_data[credit_card_data['Class']==0]['Amount'],\n             ax=ax,\n             kde=False,\n             norm_hist=True,\n             bins=20,\n             color='green',\n             label='No Fraud')\n\nsbn.distplot(credit_card_data[credit_card_data['Class']==1]['Amount'],\n             ax=ax,\n             kde=False,\n             norm_hist=True,\n             bins=20,\n             color='red',\n             label='Fraud')\n\nax.set_yscale('log')\nax.set_title('Histogram of \"Amount\" for each class',fontsize=16)\nax.set_xlabel('Amount',fontsize=13)\nax.set_ylabel('Density',fontsize=13)\n\n\nplt.subplots_adjust(wspace = 0.5)\n\nplt.show()","1bdb2655":"print('No Fraud')\nfeatures[labels['Class']==0][['Amount']].describe()","5fec8400":"print('Fraud')\nfeatures[labels['Class']==1][['Amount']].describe()","3aa0fbd8":"fig = plt.figure(figsize=(15, 10))\ngrid = plt.GridSpec(2, 2, wspace=0.4, hspace=0.3)\n\n\nax1=fig.add_subplot(grid[0, :])\nax2=fig.add_subplot(grid[1, 0])\nax3=fig.add_subplot(grid[1, 1])\n\n\nsbn.scatterplot(x='Time',y='Amount',data=credit_card_data,ax=ax1,alpha=0.35)\nax1.set_title('Amount vs time for all transactions',fontsize=16)\nax1.set_xlabel('Time [seconds]',fontsize=13)\nax1.set_ylabel('Amount',fontsize=13)\n\n\n\nsbn.scatterplot(x='Time',y='Amount',data=credit_card_data[credit_card_data['Class']==0],ax=ax2,color='green',alpha=0.35)\nax2.set_title('Amount vs time for normal transactions',fontsize=16)\nax2.set_xlabel('Time [seconds]',fontsize=13)\nax2.set_ylabel('Amount',fontsize=13)\n\nsbn.scatterplot(x='Time',y='Amount',data=credit_card_data[credit_card_data['Class']==1],ax=ax3,color='red',alpha=0.35)\nax3.set_title('Amount vs time for fraudulent transactions',fontsize=16)\nax3.set_xlabel('Time [seconds]',fontsize=13)\nax3.set_ylabel('Amount',fontsize=13)\n\nplt.show()\n","9652abf4":"anonymised_features=features.drop(['Time','Amount'],axis=1)","930f903d":"anonymised_features.describe()","c12f3646":"anonymised_features=features.drop(['Time','Amount'],axis=1)\n\nplt.figure(figsize=(12,28*4))\ngrid = plt.GridSpec(28, 1)\n\nks_distances=[]\nemd_distances=[]\ned_distances=[]\n\nfor idx,feature in enumerate(anonymised_features.columns):\n    \n    plt.subplot(grid[idx])\n        \n    sbn.distplot(anonymised_features[labels['Class']==0][feature],\n                 kde=True,\n                 color='green',\n                 label='No Fraud',bins=30)\n    \n    sbn.distplot(anonymised_features[labels['Class']==1][feature],\n                 kde=True,\n                 color='red',\n                 label='Fraud',bins=30)\n    \n    \n    ks=ks_2samp(anonymised_features[labels['Class']==1][feature].values,\n                anonymised_features[labels['Class']==0][feature].values)\n        \n    emd=wasserstein_distance(anonymised_features[labels['Class']==1][feature].values,\n                             anonymised_features[labels['Class']==0][feature].values)\n    \n    ed=energy_distance(anonymised_features[labels['Class']==1][feature].values,\n                       anonymised_features[labels['Class']==0][feature].values)\n    \n    ks_distances.append(ks[0])\n    emd_distances.append(emd)\n    ed_distances.append(ed)\n        \n    plt.title(feature+': KS: {0:.2f}, EMD: {1:.2f}, ED: {2:.2f}'.format(ks[0],emd,ed),fontsize=20)\n    plt.xlabel(feature,fontsize=18)\n    plt.legend()\n    \nplt.subplots_adjust(hspace = 0.5)\n\nplt.show()","6015914f":"fig,axes=plt.subplots(3,1,figsize=(15,20),sharex=False)\n\nax=axes[0]\n\nsbn.barplot(x=np.arange(28),y=ks_distances,ax=ax)\nax.set_title('Kolmogorov-Smirnov Statistic',fontsize=16)\nax.set_xticklabels(anonymised_features.columns.values)\n\n\nax=axes[1]\n\nsbn.barplot(x=np.arange(28),y=emd_distances,ax=ax)\nax.set_title('Wasserstein distance',fontsize=16)\nax.set_xticklabels(anonymised_features.columns.values)\n\nax=axes[2]\n\nsbn.barplot(x=np.arange(28),y=ks_distances,ax=ax)\nax.set_title('Energy distance',fontsize=16)\n\nax.set_xticklabels(anonymised_features.columns.values)\n\n\nplt.show()","de1884cc":"class ColumnSelector(BaseEstimator, TransformerMixin):\n    \n    \"\"\"\n    (Transformer)\n    Class that implements selection of specific columns.\n    The desired column or columns are passed as an argument to the constructor\n\n\n    \"\"\"\n\n    def __init__(self, cols):\n\n        \"\"\"\n        :param cols: desired columns to keep\n        :return: the transformed dataframe\n\n        \"\"\"\n\n        self.cols = cols\n\n    def fit(self, X, y=None):\n\n        \"\"\"\n        :param X: dataframe\n        :param y: none\n        :return: self\n\n        \"\"\"\n\n        return self\n\n    def transform(self, X):\n\n        \"\"\"\n\n        :param X: dataframe\n        :return: the dataframe with only the selected cols\n\n        \"\"\"\n\n        # First check if X is a pandas DataFrame\n\n        assert isinstance(X, pd.DataFrame)\n\n        try:\n\n            # Return the desired columns if all of them exist in the dataframe X\n            return X[self.cols]\n\n        except KeyError:\n\n            # Find which are the missing columns, i.e. desired cols to keep that do not exist in the dataframe\n            missing_cols = list(set(self.cols) - set(X.columns))\n\n            raise KeyError(\"The columns: %s do not exist in the data\" % missing_cols)\n            \n            \nclass Scaler(BaseEstimator, TransformerMixin):\n    \n    \"\"\"\n    (Transformer)\n    Class that implements scaling.\n    \n    method: string, either 'normalize' or 'standardize'\n        \n    \"\"\"\n\n    def __init__(self, method):\n\n        self.method = method\n\n    def fit(self, X, y=None):\n\n        return self\n\n    def transform(self, X):\n\n        if self.method == 'normalize':\n\n            return (X - X.min()) \/ (X.max() - X.min())\n\n        elif self.method == 'standardize':\n\n            return (X - X.mean()) \/ X.std()\n        ","616e4863":"class DataPrep(object):\n    \n    \"\"\"\n    (Transformer)\n    Prepare data and implement pipeline\n\n\n    columns_to_keep: list of which columns of the dataframe to keep\n    normalization_method:string, 'normalize' or 'standardize' denoting the scaling type\n    \n    \"\"\"\n\n    ### Pass the desired arguments to the constructor\n\n    def __init__(self, columns_to_keep,normalization_method):\n        self.columns_to_keep = columns_to_keep\n        self.normalization_method=normalization_method\n\n    def pipeline_creator(self):\n        \n        \"\"\"\n        \n        The pipelines are \"trivial\", but could be extended by adding more functionalities (e.g. select which cols to \n        normalize, deal with categorical cols...)\n        \n        \"\"\"\n        \n        #Data Selection\n\n        data_select_pipeline = Pipeline([\n\n            ('Columns to keep', ColumnSelector(self.columns_to_keep)),\n\n        ])\n\n    \n        norm_pipeline = Pipeline([\n            \n            ('Custom Scaling', Scaler(self.normalization_method))\n        ])\n        \n        \n        preprocess_pipeline=Pipeline([\n            \n            ('Data selection',data_select_pipeline),\n            ('Data scaling',norm_pipeline)\n\n        ])\n        \n        \n        return preprocess_pipeline","c178d9b7":"class MyEstimator(BaseEstimator):\n    \n    \"\"\"\n    (Estimator)\n    \n    Class that implements our custom estimator\n    \n    \"\"\"\n\n    def __init__(self,sampling_method,learning_rate,batch_size,num_epochs,keep_prob,model_dir,model_name):\n        \n        \"\"\"\n        sampling_method:string, 'over' ,'under', 'over_SMOTE' ,'both'\n        learning_rate: float, the learning rate of the algorithm\n        batch_size:the batch size to feed each step of optimization\n        num_epochs: int, iteration over the dataset\n        keep_prob: probability to keep nodes at the dropout layer (if any)\n        model_dir: where to save output for tensorboard visualization\n        \n        \n        \"\"\"\n        \n        self.sampling_method = sampling_method        \n        self.learning_rate = learning_rate\n        self.batch_size=batch_size\n        self.num_epochs=num_epochs\n        self.model_dir=model_dir\n        self.keep_prob=keep_prob\n        self.model_name=model_name\n    \n#     def make_tf_dataset(self,X,y,num_epochs,batch_size):\n        \n#         dataset=tf.data.Dataset.from_tensor_slices({'features':X,'labels':y})\n#         dataset.shuffle(X.shape[1])\n#         dataset.batch(batch_size)\n#         dataset.repeat(num_epochs)\n        \n#         return dataset\n    \n    def resample_dataset(self,X,y):\n        \n        \"\"\"\n        Method that implements the resampling. X and y correspond to data after train\/test split\n        sampling_strategy=1 denotes that the ratio of classes will be equal in the resampled datasets\n        \n        Beware, in oversampling, huge datasets could result\n        \n        \"\"\"\n        \n        if self.sampling_method=='over':\n            \n            sampler=RandomOverSampler(sampling_strategy=0.03,random_state=0)\n            X_resampled,y_resampled=sampler.fit_resample(X,y)\n            \n        elif self.sampling_method=='under':\n            \n            sampler=RandomUnderSampler(sampling_strategy=0.03,random_state=0)\n            X_resampled,y_resampled=sampler.fit_resample(X,y)\n            \n        elif self.sampling_method=='over_SMOTE':\n            \n            sampler=SMOTE(sampling_strategy=1,random_state=0)\n            X_resampled, y_resampled = sampler.fit_resample(X, y)\n            \n        elif self.sampling_method=='both':\n            \n            sampler=SMOTEENN(sampling_strategy=0.03,random_state=0)\n            X_resampled, y_resampled = sampler.fit_resample(X, y['Class'].values)\n        \n        else:\n            \n            print('No resampling is used!')\n            \n            X_resampled=X\n            y_resampled=y\n        \n        \n        return X_resampled, y_resampled\n        \n        \n    \n        \n    def fit(self,X,y):\n        \n        \"\"\"\n        \n        The fit method should implement training.\n        We resample the data, create the graph, run the training for num_epochs and for each step we feed in\n        batch_size of examples. We finally save the model for usage by predict and score methods\n        \n        \n        \"\"\"\n        \n        #for debugging\n        print('\\n Number of input examples for fit: '+str(X.shape[0]))\n        \n        # undersampling\/oversampling\n        \n        X_resampled, y_resampled=self.resample_dataset(X,y)\n        \n        y_resampled=np.asarray(y_resampled)\n        y_resampled=np.reshape(y_resampled,(-1,))\n        \n        # Had to use float16 in order not to run out of memory; maybe it is not an issue if you run python script not in Jupyter\n        X_resampled=X_resampled.astype(np.float16)\n        y_resampled=y_resampled.astype(np.int16)\n        \n#         print(X_resampled.shape)\n#         print(X_resampled.dtype)\n\n#         print(y_resampled.shape)\n        \n        print('\\n Number of examples after resampling: '+str(X_resampled.shape[0]))\n        \n        \n        # Create the graph (placeholders, variables, ops...) and then train the model for num_epochs\n        \n        # Reset graph so no overlapping names occur accross multiple runs\n        tf.reset_default_graph()\n\n        \"\"\"\n        \n        Create placeholders for the input. Every row of features correspond to the same row at output\n        The dimensions can be transposed, and this will affect how the matrix multiplication is done, so a good practice\n        would be to keep track of the dimensions, which we will append as comments in the code\n        \n        As a note, the values of the placeholders are not saved with the tf.train.Saver().save() method\n        \n        \"\"\"\n        # Features placeholder will have shape (batch_size,number_of_features)\n        \n        features=tf.placeholder(tf.float16,[None,X_resampled.shape[1]],name='features')\n        \n        # Labels placeholder will have shape(number_of_features,1)\n        \n        labels=tf.placeholder(tf.int32,[None,],name='labels')\n        \n        # We create a placeholder for the keep probability of the dropout layer.\n        # !!! SET TO 1 DURING DEV\/TEST\n        \n        prob_to_keep=tf.placeholder(tf.float16,name='keep_prob')\n        \n        '''\n        As an example algorithm, we use a neural net (NN) with multiple layers. \n        Logistic regression can be modeled with a NN of 1 (hidden) layer with 1 node\n        \n        By using more hidden layers, we let the network learn more complex functions of the input \n        than just a linear combination of it\n        \n        '''\n        \n        # Define the number of hidden layers and nodes. \n        # As a rule, we keep the ratio of nodes between consequtive hidden layers fixed\n        \n        ratio=1.5\n        \n        # The input nodes regard the input layer and correspond to the number of features\n        \n        input_nodes=X_resampled.shape[1]\n        \n        # Nodes in the first hidden layer\n        \n        hidden_nodes_1=8\n        \n        # Nodes in the second hidden layer\n        \n        hidden_nodes_2=round(hidden_nodes_1*ratio)\n        \n        # Nodes in the third hidden layer\n        \n        hidden_nodes_3=round(hidden_nodes_2*ratio)\n        \n        # Output nodes: we have class 0 and class 1\n        # With one output node, the output of the network will yield the probability of class=1, i.e. the probability to have fraud.\n        # Since the classes are mutually exclusive, the probability of no fraud will be 1-P(fraud)\n        \n        output_nodes=2\n        \n        # Construct hidden layer 1\n        \n        with tf.name_scope('Hidden_Layer_1'):\n        \n            W1 = tf.get_variable('W1',[input_nodes,hidden_nodes_1],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float16)\n            b1 = tf.get_variable('B1',[hidden_nodes_1],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float16)\n            \n            # out_1 will be matrix multiplication (batch_size,number_of_features)*(input_nodes,hidden_nodes_1)\n            # out_1 will be of shape (batch_size,hidden_nodes_1)\n            \n            # We use ReLU non-linearities to speed-up learning\n            out_1=tf.nn.relu(tf.matmul(features,W1)+b1,name='out_1')\n            \n            \n            tf.summary.histogram('Weights',W1)\n            tf.summary.histogram('Biases',b1)\n            tf.summary.histogram('Activations',out_1)\n            \n        \n        # Construct hidden layer 2\n         \n        with tf.name_scope('Hidden_Layer_2'):\n            \n            W2=tf.get_variable('W2',[hidden_nodes_1,hidden_nodes_2],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float16)\n            \n            b2 = tf.get_variable('B2',[hidden_nodes_2],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float16)\n\n            \n            # out_2 will be matrix multiplication (batch_size,hidden_nodes_1)*(hidden_nodes_1,hidden_nodes_2)\n            # out_2 will be of shape (batch_size,hidden_nodes_2)\n\n            out_2=tf.nn.relu(tf.matmul(out_1,W2)+b2,name='out_2')\n            \n            out_2=tf.nn.dropout(out_2,prob_to_keep,name='out_2_dropout')\n        \n            tf.summary.histogram('Weights',W2)\n            tf.summary.histogram('Biases',b2)\n            tf.summary.histogram('Activations',out_2)\n            \n        # Construct hidden layer 3 (comment out if needed)\n        '''\n        with tf.name_scope('Hidden_Layer_3'):\n            \n            W3=tf.get_variable('W3',[hidden_nodes_2,hidden_nodes_3],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float16)\n            \n            b3 = tf.get_variable('B3',[hidden_nodes_3],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float16)\n\n            # out_3 will be matrix multiplication (batch_size,hidden_nodes_2)*(hidden_nodes_2,hidden_nodes_3)\n            # out_3 will be of shape (batch_size,hidden_nodes_3)\n            \n            out_3=tf.nn.relu(tf.matmul(out_2,W3)+b3,name='out_3')\n            \n            out_3=tf.nn.dropout(out_3,prob_to_keep,name='out_3_dropout')\n            \n            \n            tf.summary.histogram('Weights',W3)\n            tf.summary.histogram('Biases',b3)\n            tf.summary.histogram('Activations',out_3)\n        '''    \n        \n        # construct hidden layer 4 (modify the dimensions accordingly)\n        \n        with tf.name_scope('Output_Layer'):\n            \n            W4 = tf.get_variable('W4',[hidden_nodes_2,output_nodes],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float16)\n            b4 = tf.get_variable('B4',[output_nodes],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float16)\n\n            \n            # out_4 will be matrix multiplication (batch_size,hidden_nodes_3)*(hidden_nodes_3,2)\n            # out_4 will be of shape (batch_size,2)\n            \n            # We do not apply any non-linearity to the output, as this will be taken care by the loss operation\n\n            out_4=tf.add(tf.matmul(out_2,W4),b4,name='out_4')\n            \n            \n            tf.summary.histogram('Weights',W4)\n            tf.summary.histogram('Biases',b4)\n            tf.summary.histogram('Activations',out_4)\n               \n            \n        with tf.name_scope('Loss'):\n            \n            loss=tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(labels=labels,logits=out_4),name='loss')\n            \n            # We decide to calculate and keep only the loss. \n            # Accuracy is not so good a metric when we have imbalanced datasets\n            \n            tf.summary.scalar('loss',loss)\n        \n       \n        optimizer=tf.train.AdamOptimizer(learning_rate=self.learning_rate,epsilon=0.05,name='optimizer')\n        train_op=optimizer.minimize(loss,name='training_objective')\n                      \n        # Define an operation to initialize global variables    \n        init=tf.global_variables_initializer()\n        \n        merged_summary_op=tf.summary.merge_all()\n        \n        with tf.Session() as sess:\n            \n            # Initialize the global variables\n            \n            sess.run(init)\n            \n            print('\\n Training has started...')\n            \n            \n            # Define operation to write to tensorboard\n            \n            summary_writer = tf.summary.FileWriter(self.model_dir, graph=tf.get_default_graph())\n            \n            \n            # Train the network for num_epochs (iterations over dataset)\n            \n            for epoch in range(self.num_epochs):\n                \n                # For each epoch, reset the training cost\n                \n                batch_cost=0\n                \n                num_batches=int(X_resampled.shape[0]\/self.batch_size)\n                \n                # Each optimization step will be done using a batch of the data of size batch_size\n                \n                for batch in range(num_batches):                    \n                    \n                    batch_x=X_resampled[batch*self.batch_size:(batch+1)*self.batch_size,:]\n                    batch_y=y_resampled[batch*self.batch_size:(batch+1)*self.batch_size]\n                    \n                                    \n                    _,temp_cost,summary=sess.run([train_op,loss,merged_summary_op],\n                                                 feed_dict={features:batch_x,labels:batch_y,prob_to_keep:self.keep_prob})\n                    \n                    # Calculate an average cost over the number of batches\n                    \n                    batch_cost+=temp_cost\/num_batches\n                                        \n                    # Write all the selected variables for every iteration \n                    summary_writer.add_summary(summary=summary, global_step=epoch * num_batches + batch )\n                \n                    to_string='Minibatch:'+str(batch)+'\/'+str(num_batches)\n                \n                    sys.stdout.write('\\r'+to_string)\n                \n                # Print cost (training loss) at regular intervals\n                if epoch % 10 ==0:\n                    \n                    print('\\nTraining cost at epoch {} : {}'.format(epoch,batch_cost))\n                    \n            \n            print('\\n Optimization finished! \\n')\n        \n        \n        \n        # Save the model. We have to do that inside the session.\n        # We decide to keep the last iteration\n        \n            saver=tf.train.Saver()\n            print('\\n Saving trained model...\\n')\n            saver.save(sess,'.\/'+self.model_name)\n        \n        \n    def predict(self,X):\n        \n        \"\"\"\n        Predict the output probabilities from a trained model. First we restore the model and load any desired tensors\n        and nodes. We add on top the softmax layer, create the corresponding op and run the session\n        \n        \"\"\"\n        \n        with tf.Session() as sess:\n            \n            \n        # this loads the graph\n            saver = tf.train.import_meta_graph(self.model_name+'.meta')\n        \n        # this gets all the saved variables\n            saver.restore(sess,tf.train.latest_checkpoint('.\/'))\n            \n        # get the graph\n        \n            graph=tf.get_default_graph()\n            \n            # get placeholder tensors to feed new values\n            features=graph.get_tensor_by_name('features:0')\n#             labels=graph.get_tensor_by_name('labels:0')\n            keep_prob=graph.get_tensor_by_name('keep_prob:0')\n            \n            # get the desired operation to restore. this will be the output of the last layer\n            op_to_restore=graph.get_tensor_by_name('Output_Layer\/out_4:0')\n            \n            # For prediction the keep_prob of the dropout layer will be equal to 1, i.e. no dropout\n            logits=sess.run(op_to_restore,feed_dict={features:X,keep_prob:1.0})\n            \n            # The output of this operation needs to be passed to a softmax layer in order to get as output probabilities\n            # Define the necessary op\n            softmax=tf.nn.softmax(logits)\n            \n            # Run the op\n            probabilities=sess.run(softmax)\n            \n            # The output will be of shape (num_examples,2)\n            # The first column corresponds to P(Class=0) , that is no fraud\n            # The second column corresponds to P(Class=1), that is fraud\n            \n            return probabilities\n            \n    \n    \n    \n    '''\n\n    Our custom estimator needs to implement a score method that will be used \n    to select the best custom score using grid search\n    \n    For such imbalanced dataset that we have, a good metric, \n    as alredy discussed, will be the area under the precision-recall curve\n        \n    '''\n    \n    def score(self,X,y):\n        \n        \"\"\"\n        \n        Scorer function to be used for selection of the best parameters.\n        Our scorer function will calculate the area under the precison recall curve\n        \n        \n        \"\"\"\n        \n        print('\\n Scoring using '+str(X.shape[0])+' examples')\n        \n        # Get the probabilities for each class\n        probs=self.predict(X)\n        \n        # Get the probabilities for fraud for each example\n        fraud_probs=probs[:,1]\n        \n        print('\\n Got probabilities for '+str(X.shape[0])+' examples')\n        \n        # Define the operation to get the area under the precision-recall curve\n        \n        with tf.name_scope('Scoring'):\n        \n            area=tf.metrics.auc(labels=y,\n                            predictions=fraud_probs,\n                            curve='PR',\n                            summation_method='careful_interpolation',\n                            name='AUCPRC')\n        \n        # Define an initialization operation on the global AND local variables\n        init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n        \n        # Create session and run\n        \n        with tf.Session() as sess:\n            \n            # Initialize the variables\n            sess.run(init)\n            \n            # Run the area node to calculate AUCPRC\n            out=sess.run(area)\n        \n        return out[1]\n    \n                ","eb948444":"# use 'under' for resampling to get results quicker, as here the dataset is smaller\n\nparams={'sampling_method':['under'],'learning_rate':[0.003,0.01],'batch_size':[128],\n        'num_epochs':[100],'keep_prob':[0.35,0.7],'cols':['full','discard'],'cv_splits':[3]}\n\nparams=ParameterGrid(params)\nscores_runs=[]\n\nfor run,item in enumerate(params):\n    \n    if item['cols']=='full':\n        \n        cols=features.columns\n        pipeline_data=DataPrep(cols,'standardize').pipeline_creator()\n        \n    elif item['cols']=='discard':\n        # drop the anonymised features with the smallest wasserstein distance.\n        cols=features.drop(['V28','V27','V26','V25','V24','V23','V22','V20','V21','V19','V15','V13','V6'],axis=1).columns\n        pipeline_data=DataPrep(cols,'standardize').pipeline_creator()\n\n    \n    folds=StratifiedKFold(n_splits=item['cv_splits'],shuffle=True,random_state=42)\n        \n    sampling_method=item['sampling_method']\n    learning_rate=item['learning_rate']\n    batch_size=item['batch_size']\n    num_epochs=item['num_epochs']\n    keep_prob=item['keep_prob']\n    \n    full_pipeline=Pipeline([\n    \n    ('data prep',pipeline_data),\n    ('estimator',MyEstimator(sampling_method,learning_rate,batch_size,num_epochs,keep_prob,model_dir='.\/tmp_over'+str(run),\n    model_name='over-model-run'+str(run)))\n\n    ])\n        \n    scores=cross_val_score(full_pipeline,features,labels,cv=folds.split(features,labels))\n    \n    scores_runs.append(scores)\n    \n    np.save('scores'+str(run),scores)\n    ","71f0edb0":"# The parameters used when undersampling\nparams_under={'sampling_method':['under'],'learning_rate':[0.003,0.01],'batch_size':[128],\n        'num_epochs':[100],'keep_prob':[0.35,0.7],'cols':['full','discard'],'cv_splits':[3]}\n\n\nparams=ParameterGrid(params_under)\n\n# make a dataframe containing as entries the parameters dictionary for each run\nresults=pd.DataFrame([item for item in params])","81d3aaed":"scores=[]\n\nfor run,item in enumerate(params):\n    run_scores=np.load('scores'+str(run)+'.npy')\n    scores.append(run_scores)\n\n    \nscores_array=np.array(scores)\nscores_array","349b3972":"results['cv_1']=scores_array[:,0]\nresults['cv_2']=scores_array[:,1]\nresults['cv_3']=scores_array[:,2]","980d89c8":"results['mean_score']=results[['cv_1','cv_2','cv_3']].mean(axis=1)\nresults","bc179f58":"It is evident that all fraudulent transactions (```Class=1```) regard smaller amounts than non-fraudulent transactions. Some more descriptive statistics can be obtained for the ```Amount``` in each class:","ad975a55":"# Modeling of Fraud\/No Fraud classifier","67440b69":"We see that for the given credit card system, for consequtive transactions that have a time difference larger than 20 seconds, the latter transaction cannot be fraudulent. This is highly dependent on these two consequtive days and cannot suggest for sure that a transaction occuring >20 seconds from the last one cannot be a fraudulent one on any given day. There is a trend however where the ratio of fraudulent to non-fraudulent transactions is higher as the time interval between transactions get shorter. Fraudulent and non-fraudulent transactions have similar distributions, but the one of normal transactions contains more outlier information. Should we decide to use this engineered feature, we need to bear in mind that shuffling the dataset ** before ** creating it is prohibited, since the results depend on the time difference.","6a1d97d4":"It would be useful to create separate dataframes for the features and for the target variable. The target variable is the class (no fraud, i.e. ```Class=0``` or fraud, i.e. ```Class=1```). The rest of the columns correspond to the features of each transaction.","cdee2e91":"The majority of transactions (75%) include amounts up to 77.165. There are however some outliers which \"push\" the mean to higher values. Let us look into the distribution of the ```Amount``` feature for each class separately:","739ac92a":"As can be seen from the two plots, the majority of transactions occur from hour 8 to hour 22 and from hour 32 (hour 8 of the second day) to hour 46 (hour 23 of the second day). This maximum number of hourly transactions is around 8000 as is suggested by the right plot. There is still however a significant amount of transactions that occur outside these hours. In order to see if there is any correlation between the number of hourly transactions and the nature of these kind of transactions (fraud \/ not fraud), we can input additional information in the histograms.","2eb9c3e0":"## Transaction amount\n\nThe second feature for which a clear description exists is ```Amount```. Let us begin by getting some descriptive statistics for this column.","c7427a62":"Consequently, we have information now regarding the amount of transactions per hour and the distribution of the hourly number of transactions","63652762":"The majority corresponds to non-fraud cases (```Class=0```). The corresponding visualization helps to understand the magnitude of each class type:","132c9c0c":"Based on these distances, we can decide to keep or discard certain features from predicting the target variable. The reason is because the corresponding feature values do not differentiate between ```Class=1``` and ```Class=0```: for each value of one feature if the calculated distances are small, the probability of obtaining either of the classes is similar, which means that the outcome for the ```Class``` variable is more stronly affected by other features than this one. Deciding to discard the feature in question, we are less likely to teach the \"noise\" to the model and thus less likely to overfit the dataset.","1dfbd840":"We can plot a histogram along with a fitted distribution for each of these features, and for each class separately.","1c491a32":"According to the [description](https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud) of the dataset, the ```Time``` feature corresponds to the seconds elapsed from the first transaction in the dataset (row 0), while the ```Amount``` feature corresponds to the amount of the transaction.","a5ce9d4e":"After investigation of the ```Time``` and ```Amount``` features, we continue with an analysis of the anonymised features (```V1-V28```). As is mentioned in the description, these features have resulted from PCA in a higher-dimensional feature space that contained personalized information about each transaction. Unfortunately, such additional information is not provided and we are only given the PCA version of the features.","23a2c078":"As can be seen, we have 284,807 entries (rows) and 31 features (columns). All of the data is numerical, either of type ```float64``` or ```int64```.\n\nAs can be seen from ```df_info```, there is no missing data, which can be more formally checked as following:","1ffa42d6":"Working in seconds might be a bit difficult to interpret and visualize, so it would be useful to transform the seconds to hours for our analysis. Of course, for modeling the fraud detector\/classifier the values in seconds carry much more information (for example, fraudulent transactions might take place a few seconds apart and this is not captured by the hour-resolution we set for the moment).","39fbb0ae":"By inspection of the plots, it is evident that there are certain features, for which the distributions for each ```Class``` are very close to each other. For example, the distributions of ```V23``` are much more similar than the corresponding ones of ```V18```. Another way to compare two distributions visually would be Q-Q plots.\nMore formally, this dissimilarity can be quantified by calculating the distance between the distributions. The calculated statistical distances using Kolmogorov-Smirnov statistic (KS), Wasserstein Distance (or Earth's mover distance, EMD) and Energy Distance (ED,Cramer-von Mises) are annotated on top of each plot. Specifically, since the KS is a statistical test, the p-value is also important. For all cases, the p value is very small and thus, we only need to focus on statistic value to say whethere two samples come from the same distribution. \n\nWe should also mention that we are mostly interested in the relative difference between the distributions of the features. Using the values for just one feature we cannot say with certainty if this feature contributes well to specifying the target variable. However, by comparing the distance metrics for one feature with those of another feature, we can conclude whether one of the two features contributes more or less to predicting the output. As an example, we consider feature ```V28``` with ```KS=0.37, EMD=0.3, ED=0.33```. Although by visual inspection it seems that the two distributions are similar, we cannot say if this is a small or large distance. By looking however at ```V16```, with ```KS=0.69, EMD=4.18, ED=1.94```, we can conclude that ```V16``` is a \"stronger\" predictor than ```V28``` regarding ```Class```.\n\nBelow, we plot the distances for each feature","248082e7":"We check the first few lines of the dataframe, and get some descriptive statistics for each column:","6cbf8940":"Another piece of information we can collect is to check for the time elapsed between fraudulent transactions. For this, we use the original dataset, with the time in seconds.","d6f6e3fe":"Create new columns (3 for each cross-validation split) and assign to each of them the corresponding AUCPRC score","848c998c":"Unfortunately, there is no additional information gain regarding ```Class```, when we \"combine\" ```Amount``` and ```Time```. Small-amount transactions (which are also the majority in both cases) for each class happen along the full timespan while the higher-amount transactions for both types, do not have a specific relation with respect to time.","f43a6d91":"As a final notice, we point out that ```Time``` and ```Amount``` occur at different scales. More specifically, ```Time``` (in seconds) ranges from 0 to 175,000 while ```Amount``` ranges from 0 to 25,000, that is we have an order of magnitude difference. Consequently, both of these features, need to be scaled appropriately if they are to be used as input to a learning algorithm.","7ffa4f2d":"As can be seen, features ```V1-V28``` have ```mean=0```, ```std``` can range from 1.95 to 0.33, with the ```min``` and ```max``` taking values in a larger range.","f918f2ca":"---","8fcf7175":"It would be intersting to combine ```Amount```, ```Time``` and ```Class```. We can plot ```Amount``` as a function of ```Time``` for each ```Class```. In this case, scatter plots are most appropriate:","35957e33":"For the ```Pipeline``` implementation, we create the following custom classes, which inherit from suitable parent classes. All the classes that implement ```fit``` and ```transform``` methods are transformers and are used in the first stages of the ```Pipeline```. Classes that implement ```fit```, ```predict``` and ```score``` methods are the estimators which occur at the end of the ```Pipeline```.","26c142a2":"To tackle the problem of class imbalance, we are going to use techniques such as oversampling, undersampling or a combination of those. \n\nWith oversampling, we sample more data from the minority class (here ```Class=1```) either randomly with replacement, or by generating new examples using a nearest neighbors algorithm with the SMOTE technique. With undersampling, we take a small sample from the majority class (here ```Class=0```) in order to match the scale of the number of examples in the minority class. The drawback here is that we train the classifier with a smaller training set, where some critical information might have been removed. A combination of both techniques can be applied as follows: we apply SMOTE and then we clean-up some of the noisy-generated examples (connecting outliers to inliers for example).\n\nWe should also mention that is important to ** first ** split the dataset into train and validation sets and perform resampling ** afterwards **. The reason is that we would like to train on the artificially augmented dataset, but we want to optimize with respect to data that the algorithm has not seen. Moreover, the kind of data that we optimize our algorithm on, should be of the same distribution as data on which the algorithm will eventually be used. As a result, we cannot use the augmented dataset for validation since:\n\n1) Any artificially created data during resampling will be seen again during validation\n\n2) We will artificially change the ratio of frauds\/non-frauds and make it bigger. However, this ratio does not (hopefully) resemble real-world situations.","6b35e3c7":"The majority of normal transactions occur within a very limited time span, which is approximately 10 seconds. However, the last two plots do not suggest a correlation between the time difference of two random transactions and their class. A better interpretation is that given a specific class of transaction (fraud\/no fraud), it is most probable (suggested by the current data) that a similar class of transaction will occur within the first 10 seconds. Moreover, for fraudulent transactions, there is also a finite probability that another fraudulent transaction will take place in a broader timespan. Perhaps a more informative feature would combine additional data for the transaction, such as the place, encrypted id of the user, etc. which unfortunately is not available in this dataset.","1ae9ceea":"More intuition can be gained by visualizing the aforementioned information:","7cef523b":"Create an ```Numpy``` array containing all the scores of every run for easy access","8f5cb00e":"The maximum value of fraudulent transactions is 2,125.87, while for normal transactions is 25,961.16. Moreover, most fraudulent transactions (75%) occur at similar values like normal ones (105.89 for ```Class=1``` and 77.05 for ```Class=0```). The main conclusion from the ```Amount``` variable is that for higher values, it is less probable for a transaction to be categorized as fraud.","e33b1f2a":"In the rest of this notebook, there is a detailed implementation of a Neural Net (NN) algorithm using the low-level ```Tensorflow``` API. \n\nWe create custom estimators, that are wrapped by a ```Scikit-learn```- respective custom estimator, that is used to implement a ```Pipeline```. This ```Pipeline``` is \"fed\" data and:\n\n1) Preprocess it, by selecting columns and applying some scaling\n\n2) Trains\/tests the model on the given data\n\nIn order to scan a representative range of some hyperparameters, we implement a grid-search procedure. This procedure does the following:\n\n1) Splits the data into train and validation examples defined by ```folds```. We are going to use ```StratifiedKFold``` splitting of the data, so that we keep the ratio of frauds\/non-frauds the same in train and validation sets. If we used random splitting, then in some training splits, 0 cases of fraud would appear. In that case, the algorithm would not learn anything useful.\n\n2) The train part of the data is passed to the ```fit``` method of our custom ```Scikit-learn``` estimator, where it is first resampled, using one of the afore-mentioned techniques. The ```fit``` method, implements the NN using the low-level or high-level ```Tensorflow``` API and trains the model by fetching batches of the * resampled * data. After the training is done, the ```fit``` method saves the model for later usage.\n\n3) The validation part of the dataset (which does not contribute to resampling), is passed to the ```score``` method of the ```Scikit-learn``` custom estimator. This method restores the trained model, and calculates the metric on the validation data. For the specific case, the metric that we have chosen is the Area Under the Precision-Recall Curve (AUCPRC). Another suitable metric could be the F1-score. As a reminder, precision measures how good the model is at predicting the positive class (here, representing frauds). At the same time, recall is a measure of how much we can trust our model when it predicts the positive class, i.e. how well it performs when we actually have a positive class. Both these measures need to be high. However, as our model will output probabilities (using ```softmax```), whether an example is positive (```Class=1```) or negative (```Class=0```) will also depend on the probability threshold that we choose to decide whether an example belongs to either class. By continuously modifying this threshold we can generate couples of operating points of precision and recall. These points form a curve, and we want its area to be as close to 1.0 as possible, meaning high precision and high recall.\n\n4) The process is repeated for every split scheme of the data and for every configuration of hyperparameters that we have chosen. The output is then comprised of AUCPRC scores for each split and each parameter configuration. \n","e266ff82":"Let us collect the results from the saved ```.npy``` files and create a dataframe:","f4ce9625":"After an analysis of the data, we have concluded the following:\n\n1) Highly imbalanced dataset\n\n2) Given features need to be scaled. Some of them could be discarded\n\nThe high-imbalanced nature comes from the fact that fraudulent transactions are much more rare than normal transactions. This high imbalance is the most important attribute of this dataset, that determines the classification methodology\/procedure. More specifically, the metric on which we evaluate our classifiers should no longer be accuracy, as we could achieve a very high accuracy, simply by predicting every time ```no fraud```. We are now interested in predicting correctly the fraudulent cases, and when we predict fraud, we need to be as sure as possible that this is fraud. We are less interested in cases where we predict fraud but it is not a case of fraud in reality (false positive). The metric on which we evaluate therefore, should capture these facts. We need high \n\n* precision = True Positives \/ (True Positives + False Positives) (how good we detect positive class, i.e. fraud)\n* recall = True Positives \/ (True Positives + False Negatives) (how good quality is our positive class prediction)\n\nWe have chosen to use as a metric the area under the Precision-Recall curve. High precision and recall for positive class should yield high area under the curve. One important notice here is that we evaluate on unseen data, that is data that have splitted * before * doing any resampling of the dataset. This is done to ensure that we optimize on data that come from distributions we would expect to see in reality. \n\nIt would not make sense therefore, to use this metric on the training set and compare it with the corresponding one on the validation set. For the training set, we measure during training the loss. The lower the loss, the better we have learnt the training set. However, this does not mean that it would perform well on the validation set too. We have two separate jobs: do well on the training set and optimize w.r.t. the validation set (orthogonal training). Early stopping (stopping the training in order to achieve high area under the curve) does not fall into this category. Low loss and bad metric would mean that the model does well on the training set but is too specific to it; it does not generalize well to new examples. One solution would be to make the model simpler, introduce regularization (here done with dropout layers), get more examples (change from undersampling to over-sampling or over-sampling with SMOTE or a combination of both - too computationally expensive).\n\n\nFinally regarding the importance of the features, the method followed here is by no means the only one. Alternatives would include:\n\n* We could explore the correlation coefficients between the output and the values of the features (show the linearity of the relations)\n* We could start building a model including all of the features and at each step we could decide which feature to remove in order to get a better performance\n* Same as before, but starting from one feature and deciding which one to add in order to boost performance\n* Ridge regression would take some of the feature coefficients to 0. Based on the remaining coefficients, we could build more complex models.\n* Decision tree algorithms have inherently the notion of feature importance","41cff24b":"## Time of the transaction\n\nLet us first look into the ```Time``` feature. From the descriptive stats we get some useful information, which we repeat here for convenience:","cd185f11":"What about the time difference between transactions in general?","06e3be16":"To get to know our dataset better, let us first check the amount of examples corresponding to each class:","684a32ca":"Before beginning the analysis, we import the necessary libraries\/packages that will be required.","d257cd0d":"It is evident that the majority of fraudulent transactions occur in a very short time period, that is in the range of 10 seconds between each other. Fewer fraudulent transactions occur over a wider time span. Does the time difference distribution for normal transactions exhibit a specific behavior itself?","1bd1c65b":"Based on the analysis above, we summarize the main points so far:\n\n* Highly imbalanced dataset (minority ```Class=1```, approx. 0.17% of total examples)\n* ```Time``` and ```Amount``` feature columns can play a role in predicting the ```Class``` but need to be appropriately scaled\n* Some of the PCA features could be discarded when predicting the target variable. Scaling can also be applied to these features\n\n\nIn the following, we are going to consider all the given features and decide through testing if some ought to be discarded when predicting the ```Class```. Moreover, we assume, since no further information is given, that the transactions are independent, i.e. we do not know whether they come from the same user, location, credit card, etc.\nIn this way, we can treat the ```Time``` attribute merely as a numerical feature, without interpreting its temporal meaning. More specifically, the temporal order of the examples will not be treated further. As a result, we will also not use any additional engineered features obtained by using the ```Time``` values.","a7e7697e":"It is clear that during both days fraudulent transactions occur. What is more, the peak of the fraudulent transactions occurs for a much smaller timespan than for normal transactions. Also, we see that we have the maximum number of frauds around hour 10 of the first day and around hour 2 of the second day. Overall, with respect to the ```Time``` feature, we could say that fraudulent transactions follow a more uniform distribution relative to the normal transactions, meaning that change in magnitude of the normal transactions is much stronger, while the number of fraudulent transactions acquires values in a much smaller range most of time.\n\nAs a result, during off-peak times (for example at night or very early in the morning), when a normal transaction seldom occurs, there is a higher probability that a fradulent transaction takes place.\n","04f88fe1":"We can have a look at the ```min, max, std``` and ```mean``` values of these features, which we isolate in a separate dataframe called ```anonymised_features```.","46032f2c":"# Some final thoughts...","eff2e58f":"# Anonymised features V1-V28","c1b89b5d":"# Exploratory Data Analysis","721075b3":"First, we load the data:","48b9815d":"The amount of transactions is more or less balanced between the two days (median of the time distribution is 23 hours, which corresponds to the last hour of the first day, since the counting starts from 0).\n\nWe can view the number of transactions by taking the counts for each hour:","631a6239":"The fact that the number of examples in ```Class=0``` is much higher (3 orders of magnitude) suggests that we are dealing with a highly imbalanced dataset. Therefore, care should be taken in order to incorporate this information in our subsequent models.\n\nFor the rest of the features, let us deal first with those for which a description is provided. \nThese are ```Time```, and ```Amount```. "}}