{"cell_type":{"bb2ed734":"code","e6e91f00":"code","c2c761f6":"code","900867d0":"code","3912280c":"code","e555e082":"code","f61fc666":"code","6fa7b299":"code","752b6380":"code","953ec2a7":"code","39035c97":"code","8c1c52d2":"code","31bb7d7a":"code","129ec7c5":"code","626a6cf5":"code","0bb6adb8":"code","1a82877a":"code","56469afd":"code","6ebbc355":"code","5f4a8cc0":"code","66189c0c":"code","a7b18f89":"code","237c058f":"code","4cc9ac25":"code","5fd52b5e":"code","b04895ff":"code","722b055f":"code","79ea1c88":"code","c4d27ec8":"code","060f015a":"code","9e6ba169":"code","c0feb98d":"code","191a2e95":"code","621af424":"code","8d8e3fc7":"code","2b2d38bd":"code","d8ef80c7":"code","1c29717d":"markdown","bec3df17":"markdown","1fcf51c8":"markdown","64a02c95":"markdown","df512646":"markdown","87e5738a":"markdown"},"source":{"bb2ed734":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e6e91f00":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.utils import class_weight\nimport warnings\nwarnings.filterwarnings('ignore')\n","c2c761f6":"train_df=pd.read_csv('\/kaggle\/input\/heartbeat\/mitbih_train.csv',header=None)\ntest_df=pd.read_csv('\/kaggle\/input\/heartbeat\/mitbih_test.csv',header=None)","900867d0":"train_df","3912280c":"#train_df.reset_index(drop=True)","e555e082":"#before decreasing freqency\ndf_new=train_df.iloc[:,0:149]\ndf_new = df_new.loc[~df_new.index.duplicated(keep='first')]\ndf_new","f61fc666":"# pre-processing step: interpolation and smoothing\nfrom scipy.interpolate import interp1d\nfrom scipy.interpolate import CubicSpline","6fa7b299":"#plot ecg the first row\n\nlst = [float(item) for item in df_new.iloc[0]]\narray = np.reshape(lst, (-1,1))\nplt.figure(figsize=(50,10))\n\nplt.plot(array)\nplt.ylabel('y_axis')\nplt.xlabel('before decreasing frequency')\nplt.show()","752b6380":"len(df_new.columns)","953ec2a7":"df_new.shape","39035c97":"label=train_df[187]\nlabel","8c1c52d2":"m =150\nx, y  = df_new.shape\ndf_final = pd.concat([df_new, pd.DataFrame(np.zeros((x, m - y)), columns=range(y, m))], axis=1)\ndf_final[150]=label\ndf_final","31bb7d7a":"test_new=test_df.iloc[:,0:150]\ntest_new=test_new.loc[~test_new.index.duplicated(keep='first')]\ntest_new","129ec7c5":"label_test=test_df[187]\nlabel_test","626a6cf5":"k =150\nx, y  = test_new.shape\ntest_final = pd.concat([test_new, pd.DataFrame(np.zeros((x, k - y)), columns=range(y, k))], axis=1)\ntest_final[150]=label_test\ntest_final","0bb6adb8":"df_final[150]=df_final[150].astype(int)\nequilibre=df_final[150].value_counts()\nprint(equilibre)","1a82877a":"from sklearn.utils import resample\ndf_1=df_final[df_final[150]==1]\ndf_2=df_final[df_final[150]==2]\ndf_3=df_final[df_final[150]==3]\ndf_4=df_final[df_final[150]==4]\ndf_0=(df_final[df_final[150]==0]).sample(n=20000,random_state=42)\n\ndf_1_upsample=resample(df_1,replace=True,n_samples=20000,random_state=123)\ndf_2_upsample=resample(df_2,replace=True,n_samples=20000,random_state=124)\ndf_3_upsample=resample(df_3,replace=True,n_samples=20000,random_state=125)\ndf_4_upsample=resample(df_4,replace=True,n_samples=20000,random_state=126)\n\ntrain_df_new=pd.concat([df_0,df_1_upsample,df_2_upsample,df_3_upsample,df_4_upsample])","56469afd":"train_df_new.reset_index(drop=True)","6ebbc355":"equilibre=train_df_new[150].value_counts()\nprint(equilibre)\n","5f4a8cc0":"plt.figure(figsize=(20,10))\nmy_circle=plt.Circle( (0,0), 0.7, color='white')\nplt.pie(equilibre, labels=['n','q','v','s','f'], colors=['red','green','blue','skyblue','orange'],autopct='%1.1f%%')\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.show()","66189c0c":"#original dataset first row\nplt.plot(train_df.iloc[0,:149])","a7b18f89":"#padding with 0 to the length of 200 to match with other dataset\n#plt.plot(df_final.iloc[0,:150])","237c058f":"c=train_df_new.groupby(150,group_keys=False).apply(lambda train_df_new : train_df_new.sample(1))","4cc9ac25":" #plt.plot(c.iloc[0,:150]","5fd52b5e":"def add_gaussian_noise(signal):\n    noise=np.random.normal(0,0.05,150)\n    return (signal+noise)\n","b04895ff":"tempo=c.iloc[0,:150]\nbruiter=add_gaussian_noise(tempo)\n\nplt.subplot(2,1,1)\nplt.plot(c.iloc[0,:150])\n\nplt.subplot(2,1,2)\nplt.plot(bruiter)\n\nplt.show()\n\n","722b055f":"target_train=train_df_new[150]\ntarget_test=test_final[150]\ny_train=to_categorical(target_train)\ny_test=to_categorical(target_test)","79ea1c88":"X_train=train_df_new.iloc[:,:150].values\nX_test=test_final.iloc[:,:150].values\n#for i in range(len(X_train)):\n#    X_train[i,:149]= add_gaussian_noise(X_train[i,:149])\nX_train = X_train.reshape(len(X_train), X_train.shape[1],1)\nX_test = X_test.reshape(len(X_test), X_test.shape[1],1)","c4d27ec8":"len(X_train[1])","060f015a":"!pip install keras-multi-head","9e6ba169":"\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils import class_weight\nfrom keras.callbacks import LearningRateScheduler, ModelCheckpoint\nfrom sklearn.preprocessing import StandardScaler\n\nimport os\nimport math\nimport matplotlib.pyplot as plt\nimport glob\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.layers import Dense, Dropout, Reshape, Conv1D, BatchNormalization, Activation, AveragePooling1D, GlobalAveragePooling1D, Lambda, Input, Concatenate, Add, UpSampling1D, Multiply\nfrom keras.models import Model\nfrom keras.objectives import mean_squared_error\nfrom keras import backend as K\nfrom keras.losses import binary_crossentropy, categorical_crossentropy\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau,LearningRateScheduler\nfrom keras.initializers import random_normal\nfrom keras.optimizers import Adam, RMSprop, SGD\nfrom keras.callbacks import Callback\nfrom keras import metrics\n\nfrom sklearn.metrics import cohen_kappa_score, f1_score\nfrom sklearn.model_selection import KFold, train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')","c0feb98d":"import keras\nfrom keras_multi_head import MultiHead\nfrom keras import layers,models\nfrom keras import optimizers\n\n# define model\n\nmodel = models.Sequential()\nmodel.add(layers.Conv1D(128, kernel_size=5, activation='relu', input_shape=(X_train.shape[1],1),padding='same'))\n \nmodel.add(MultiHead([\n    keras.layers.Conv1D(filters=64,kernel_size=3,padding='same'),\n    keras.layers.Conv1D(filters=64,kernel_size=5, padding='same'),\n    keras.layers.Conv1D(filters=64,kernel_size=7,padding='same'),\n], name='Multi-CNNs'))\nmodel.add(BatchNormalization())  \nmodel.add(keras.layers.Flatten(name='Flatten'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation = 'relu',name='Dense-1'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(5, activation='softmax', name='Dense-2'))\nmodel.build()\nmodel.summary()\nn_obs, feature, depth = X_train.shape\nbatch_size=250\n\ndef exp_decay(epoch):\n    initial_lrate = 0.001\n    k = 0.75\n    t = n_obs\/\/(10000 * batch_size)  # every epoch we do n_obs\/batch_size iteration\n    lrate = initial_lrate * math.exp(-k*t)\n    return lrate\n\nlrate = LearningRateScheduler(exp_decay)\nadam = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999)\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])","191a2e95":"history = model.fit(X_train, y_train, \n          epochs=100, \n          batch_size=batch_size,\n          validation_data=(X_test, y_test),\n          shuffle=True)","621af424":"def evaluate_model(history,X_test,y_test,model):\n    scores = model.evaluate((X_test),y_test, verbose=0)\n    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n    \n    print(history)\n    fig1, ax_acc = plt.subplots()\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Model - Accuracy')\n    plt.legend(['Training', 'Validation'], loc='lower right')\n    plt.show()\n    \n    fig2, ax_loss = plt.subplots()\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Model- Loss')\n    plt.legend(['Training', 'Validation'], loc='upper right')\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.show()\n    target_names=['0','1','2','3','4']\n    \n    y_true=[]\n    for element in y_test:\n        y_true.append(np.argmax(element))\n    prediction_proba=model.predict(X_test)\n    prediction=np.argmax(prediction_proba,axis=1)\n    cnf_matrix = confusion_matrix(y_true, prediction)\n    ","8d8e3fc7":"evaluate_model(history,X_test,y_test,model)\ny_pred=model.predict(X_test)","2b2d38bd":"print(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1)))","d8ef80c7":"\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(10, 10))\nplot_confusion_matrix(cnf_matrix, classes=['N', 'S', 'V', 'F', 'Q'],normalize=True,\n                      title='Confusion matrix, with normalization')\nplt.show()\n","1c29717d":"I take one sample per class and i store it in a datafrmae in order to have an exmeple. ","bec3df17":"**Import dataset and library**","1fcf51c8":"**Network**","64a02c95":"In this part i will speak o n what i do to transform data. ","df512646":"this is a test for transferring model from this dataset to predict labels in a private dataset.","87e5738a":"I use a fonction ( will depend of the version) where i add a noise to the data to generilize my train."}}