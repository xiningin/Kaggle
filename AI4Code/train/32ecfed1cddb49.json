{"cell_type":{"1fc984a3":"code","9bf1f383":"code","1b954098":"code","1627ba56":"code","93fe6618":"code","97516bf7":"code","67fdd11d":"code","fc1a4f1c":"code","263bda6c":"code","97ef7837":"code","651bc1ab":"code","ca0b6f58":"code","be9ef9af":"code","fcab424d":"code","03981c81":"code","362a3f15":"code","ec146549":"code","2f6f5259":"code","334635c1":"code","ff330177":"code","96187c08":"code","ddb10e6a":"code","41474cdf":"code","a423a9c1":"code","1d7570ad":"code","f55f3d3b":"code","7db6b65b":"code","1b8df736":"code","001acdb3":"code","cdf3b0a0":"code","d29f27b9":"code","8b4641dd":"code","c10178de":"code","395afe0e":"code","20531dec":"code","0fd81a65":"code","089a56e1":"code","6b42b3e4":"code","eb2ddcdf":"code","15c18cc5":"code","6e7f7d60":"code","3853273c":"code","80343e16":"code","481fbb70":"code","cf33fbb1":"code","4bba9395":"code","92f46fed":"code","452cc968":"code","7b39545a":"code","79428bab":"code","f6029088":"code","c25cd486":"code","26d17e02":"code","4ccae1e0":"code","43a6c683":"code","6a9566d2":"code","c02441ac":"markdown","4549059f":"markdown","7d14f475":"markdown","b0eb1a6c":"markdown","0312b4b9":"markdown","b01a24ca":"markdown","8c5d2293":"markdown","3abd5478":"markdown","3a67bfe5":"markdown","36bd44de":"markdown","826aa5b2":"markdown","88504fe5":"markdown","907a4967":"markdown","e454d887":"markdown","8ce5ae86":"markdown","d446f4a8":"markdown","878f123e":"markdown","a7e7b2da":"markdown","bcbebb15":"markdown","9a0bfe42":"markdown","e206eade":"markdown","dd89ba0d":"markdown","62c3b85f":"markdown","1e46d872":"markdown","4db15843":"markdown","dec778d3":"markdown","0488a53f":"markdown","48665944":"markdown","556bd4ca":"markdown","ccbbe760":"markdown","d3b11e03":"markdown","5ee121ac":"markdown","a9a7cd71":"markdown","edc34047":"markdown","c7e68f78":"markdown","7b45ec3a":"markdown","7d053477":"markdown","24203715":"markdown","81ca11d7":"markdown","aa38ae1d":"markdown","e716fdfd":"markdown","650cac07":"markdown"},"source":{"1fc984a3":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport geopandas as gpd\nfrom geopandas.tools import geocode\n\nimport folium \nfrom folium.plugins import HeatMap\n\nimport matplotlib.pyplot as plt\nfrom matplotlib_venn import venn2","9bf1f383":"#GAMES_DAILY\tsteamid\tappid\tplaytime_forever\nplaytime_data_link_1 = \"..\/input\/steam-playtime-complete\/games_1_sample.csv\"\nplaytime_data_link_2 = \"..\/input\/steam-playtime-complete\/games_2_sample.csv\"\nplaytime = pd.read_csv(playtime_data_link_1)\nplaytime.append(pd.read_csv(playtime_data_link_2))\nplaytime_sample_multiplier = 33\n#GAMES_PUBLISHERS\tappid\tPublisher\nproduct_publishers_data_link = \"..\/input\/steam-games-publishers\/game_publishers.csv\"\nproduct_publishers = pd.read_csv(product_publishers_data_link)\nproduct_publishers = product_publishers.dropna()\n#GAMES_DEVELOPERS\tappid\tDeveloper\nproduct_developers_data_link = \"..\/input\/game-developers\/game_developers.csv\"\nproduct_developers = pd.read_csv(product_developers_data_link)\n#GAMES_GENRES\tappid\tGenre\ngenres_data_link = \"..\/input\/steam-games-genres\/game_genres.csv\"\ngenres = pd.read_csv(genres_data_link)\n#APP_ID_INFO\tappid\tTitle\tType\tPrice\tRelease_Date\tRating\tRequired_Age\tIs_Multiplayer\nproducts_data_link = \"..\/input\/app-id-info\/app_id_info.csv\"\nproducts = pd.read_csv(products_data_link)","1b954098":"# slice products by type\nproduct_types = products.Type.unique()\ngames = products[products[\"Type\"] == \"game\"]\ndlcs = products[products[\"Type\"] == \"dlc\"]\nmods = products[products[\"Type\"] == \"mod\"]\nprint(product_types)","1627ba56":"product_publishers = product_publishers.dropna()\nplaytime_games = playtime[playtime[\"appid\"].isin(games.appid)]\nplaytime_games_groups = playtime_games.groupby(['appid'])\nplaytime_dlcs = playtime[playtime[\"appid\"].isin(dlcs.appid)]\nplaytime_dlcs_groups = playtime_dlcs.groupby(['appid'])","93fe6618":"# Making tables for Games product type\nproduct_developers_games = product_developers[product_developers[\"appid\"].isin(games.appid)]\ndeveloper_games_groups = product_developers_games.groupby([\"Developer\"])\ndevelopers_games = pd.DataFrame({\"Developer\":product_developers_games.Developer.unique()})\ndevelopers_games[\"Owners\"] = product_developers_games.appid.apply(lambda x: len(playtime_games_groups.groups[x]) * playtime_sample_multiplier if x in playtime_games_groups.groups.keys() else 0)\ndevelopers_games[\"Releases\"] = product_developers_games.Developer.apply(lambda x: developer_games_groups.get_group(x).size)\ndevelopers_games = developers_games[developers_games[\"Owners\"]  > 0]","97516bf7":"# Making tables for DLCs product type\nproduct_developers_dlcs = product_developers[product_developers[\"appid\"].isin(dlcs.appid)]\ndeveloper_dlcs_groups = product_developers_dlcs.groupby([\"Developer\"])\ndevelopers_dlcs = pd.DataFrame({\"Developer\":product_developers_dlcs.Developer.unique()})\ndevelopers_dlcs[\"Owners\"] = product_developers_dlcs.appid.apply(lambda x: len(playtime_dlcs_groups.groups[x]) * playtime_sample_multiplier if x in playtime_dlcs_groups.groups.keys() else 0)\ndevelopers_dlcs[\"Releases\"] = product_developers_dlcs.Developer.apply(lambda x: developer_dlcs_groups.get_group(x).size)\ndevelopers_dlcs = developers_dlcs.dropna()\ndevelopers_dlcs = developers_dlcs[developers_dlcs[\"Owners\"]  > 0]","67fdd11d":"plt.figure(figsize=(15,10))\nsns.kdeplot(np.log10(developers_games['Releases']), np.log10(developers_games['Owners']), cmap=\"Blues\", shade=True, shade_lowest=False, cbar = True, cbar_kws = {\"label\":\"Games (density)\"}, alpha = 0.5) #, \"shrink\":0.7\nsns.kdeplot(np.log10(developers_dlcs['Releases']), np.log10(developers_dlcs['Owners']), cmap=\"Greens\", shade=True, shade_lowest=False, cbar = True, cbar_kws = {\"label\":\"DLCs (density)\"}, alpha = 0.5)\nplt.xlabel(\"Releases (log10)\", fontsize = 14)\nplt.ylabel(\"Owners (log10)\", fontsize = 14)\nplt.title(\"Games Sales\", fontsize = 14)","fc1a4f1c":"developers_dlcs[developers_dlcs[\"Owners\"] > 10000].sort_values(\"Releases\", ascending = False).head()","263bda6c":"# Making tables for Games product type for publishers\nproduct_publishers_games = product_publishers[product_publishers[\"appid\"].isin(games.appid)]\npublishers_games_groups = product_publishers_games.groupby([\"Publisher\"])\npublishers_games = pd.DataFrame({\"Publisher\":product_publishers_games.Publisher.unique()})\npublishers_games[\"Owners\"] = product_publishers_games.appid.apply(lambda x: len(playtime_games_groups.groups[x]) * playtime_sample_multiplier if x in playtime_games_groups.groups.keys() else 0)\npublishers_games[\"Releases\"] = product_publishers_games.Publisher.apply(lambda x: len(publishers_games_groups.groups[x]))\npublishers_games = publishers_games[publishers_games[\"Owners\"]  > 0]","97ef7837":"# Making tables for DLCs of publishers\nproduct_publishers_dlcs = product_publishers[product_publishers[\"appid\"].isin(dlcs.appid)]\npublishers_dlcs_groups = product_publishers_dlcs.groupby([\"Publisher\"])\npublishers_dlcs = pd.DataFrame({\"Publisher\":product_publishers_dlcs.Publisher.unique()})\npublishers_dlcs[\"Owners\"] = product_publishers_dlcs.appid.apply(lambda x: len(playtime_dlcs_groups.groups[x]) * playtime_sample_multiplier if x in playtime_dlcs_groups.groups.keys() else 0)\npublishers_dlcs[\"Releases\"] = product_publishers_dlcs.Publisher.apply(lambda x: len(publishers_dlcs_groups.groups[x]))\npublishers_dlcs = publishers_dlcs.dropna()\npublishers_dlcs = publishers_dlcs[publishers_dlcs[\"Owners\"]  > 0]","651bc1ab":"publishers_dlcs.nlargest(10, \"Releases\")","ca0b6f58":"plt.figure(figsize=(15,10))\n\nkde2 = sns.kdeplot(np.log10(developers_games['Releases']), np.log10(developers_games['Owners']), cmap=\"Blues\", shade=True, shade_lowest=False, cbar = True, cbar_kws = {\"label\":\"Developers (density)\"}, alpha = 0.5) #, \"shrink\":0.7\nsns.kdeplot(np.log10(publishers_games['Releases']), np.log10(publishers_games['Owners']), cmap=\"Reds\", shade=True, shade_lowest=False, cbar = True, cbar_kws = {\"label\":\"Publishers (density)\"}, alpha = 0.5)\n#kde2.set(xlabel='Owners (log10)', ylabel='Releases (log10)', title = \"Game Sales\")\nplt.xlabel(\"Releases (log10)\", fontsize = 14)\nplt.ylabel(\"Owners (log10)\", fontsize = 14)\nplt.title(\"Games Sales\", fontsize = 14)","be9ef9af":"v = venn2([set(product_developers_games.Developer), set(product_publishers_games.Publisher)], set_colors=(\"purple\", \"skyblue\"), alpha = 0.7)\nv.get_label_by_id('A').set_text('Developers')\nv.get_label_by_id('B').set_text('Publishers')\n\nfig = plt.gcf()\nfig.set_size_inches(12.5, 6.5)\nfig.savefig('test2png.png', dpi=100)\n\nplt.show()","fcab424d":"# To display dables in a tidier manner:\nfrom IPython.core.display import HTML\n#._repr_html_()\ndef multi_table(table_list):\n    ''' Acceps a list of IpyTable objects and returns a table which contains each IpyTable in a cell\n    '''\n    return HTML(\n        '<table><tr style=\"background-color:white;\">' + \n        ''.join(['<td>' + table.to_html(index=False) + '<\/td>' for table in table_list]) +\n        '<\/tr><\/table>'\n    )","03981c81":"playtime_groups = playtime.groupby(['appid'])","362a3f15":"product_developers[\"Owners\"] = product_developers.appid.apply(lambda x: len(playtime_groups.get_group(x) * playtime_sample_multiplier) if x in playtime_groups.groups.keys() else 0)\nproduct_developers = product_developers[product_developers[\"Owners\"] > 0]\nproduct_developers[\"Price\"] = product_developers.appid.apply(lambda x: (products.loc[products[\"appid\"] == x, [\"Price\"]].iloc[0].Price))\nproduct_developers[\"Revenue\"] = product_developers.appid.apply(lambda x: (products.loc[products[\"appid\"] == x, [\"Price\"]].iloc[0].Price) * (product_developers.loc[product_developers[\"appid\"] == x, [\"Owners\"]].iloc[0].Owners))","ec146549":"product_publishers[\"Owners\"] = product_publishers.appid.apply(lambda x: len(playtime_groups.get_group(x) * playtime_sample_multiplier) if x in playtime_groups.groups.keys() else 0)\nproduct_publishers = product_publishers[product_publishers[\"Owners\"] > 0]\nproduct_publishers[\"Price\"] = product_publishers.appid.apply(lambda x: (products.loc[products[\"appid\"] == x, [\"Price\"]].iloc[0].Price))\nproduct_publishers[\"Revenue\"] = product_publishers.appid.apply(lambda x: (products.loc[products[\"appid\"] == x, [\"Price\"]].iloc[0].Price) * (product_publishers.loc[product_publishers[\"appid\"] == x, [\"Owners\"]].iloc[0].Owners))","2f6f5259":"dev_gr = product_developers.groupby([\"Developer\"])\ndev_gr.size().sort_values(ascending=False).head()","334635c1":"product_developers['Total_Revenue'] = product_developers['Revenue'].groupby(product_developers['Developer']).transform('sum')\nproduct_developers['Total_Owners'] = product_developers['Owners'].groupby(product_developers['Developer']).transform('sum')\nproduct_developers = product_developers[product_developers[\"Total_Owners\"] > 100]\n\nproduct_publishers['Total_Revenue'] = product_publishers['Revenue'].groupby(product_publishers['Publisher']).transform('sum')\nproduct_publishers['Total_Owners'] = product_publishers['Owners'].groupby(product_publishers['Publisher']).transform('sum')\nproduct_publishers = product_publishers[product_publishers[\"Total_Owners\"] > 100]","ff330177":"product_developers[\"Top_Product_Value\"] = product_developers['Revenue'].groupby(product_developers['Developer']).transform('max')\nproduct_publishers[\"Top_Product_Value\"] = product_publishers['Revenue'].groupby(product_publishers['Publisher']).transform('max')","96187c08":"publishers = pd.DataFrame({\"Publisher\":product_publishers.Publisher.unique()})\ndevelopers = pd.DataFrame({\"Developer\":product_developers.Developer.unique()})\n\ndeveloper_groups = product_developers.groupby([\"Developer\"])\ndevelopers[\"Releases\"] = developers.Developer.apply(lambda x: developer_groups.get_group(x).size)\n\ndevelopers[\"Total_Owners\"] = developers.Developer.apply(lambda x: developer_groups.get_group(x).iloc[0].Total_Owners)\ndevelopers[\"Entity_value\"] = developers.Developer.apply(lambda x: developer_groups.get_group(x).iloc[0].Total_Revenue)\n\npublisher_groups = product_publishers.groupby([\"Publisher\"])\npublishers[\"Releases\"] = publishers.Publisher.apply(lambda x: publisher_groups.get_group(x).size)\npublishers[\"Total_Owners\"] = publishers.Publisher.apply(lambda x: publisher_groups.get_group(x).iloc[0].Total_Owners)\npublishers[\"Entity_value\"] = publishers.Publisher.apply(lambda x: publisher_groups.get_group(x).iloc[0].Total_Revenue)","ddb10e6a":"top_app_ids_dev = developers.Developer.apply(lambda x: product_developers.loc[product_developers.groupby(['Developer']).get_group(x)[\"Revenue\"].idxmax()][\"appid\"])\ndevelopers[\"Top_Product_Name\"] = top_app_ids_dev.apply(lambda x: products[products.appid == int(x)][\"Title\"].item()) #a heavy error - the equals item was of wrong type (str)\ndevelopers[\"Top_Product_Value\"] = developers.Developer.apply(lambda x: developer_groups.get_group(x).iloc[0].Top_Product_Value)","41474cdf":"top_app_ids_pub = publishers.Publisher.apply(lambda x: product_publishers.loc[product_publishers.groupby(['Publisher']).get_group(x)[\"Revenue\"].idxmax()][\"appid\"])\npublishers[\"Top_Product_Name\"] = top_app_ids_pub.apply(lambda x: products[products.appid == int(x)][\"Title\"].item()) #a heavy error - the equals item was of wrong type (str)\npublishers[\"Top_Product_Value\"] = publishers.Publisher.apply(lambda x: publisher_groups.get_group(x).iloc[0].Top_Product_Value)","a423a9c1":"top_member_count = 10","1d7570ad":"high_prod_developers = developers.nlargest(top_member_count, \"Releases\")\nhigh_value_developers = developers.nlargest(top_member_count, \"Entity_value\")\nhigh_sale_dev = developers.nlargest(top_member_count, \"Total_Owners\")\nhigh_value_developers[\"Value_Perc\"] = high_value_developers[\"Top_Product_Value\"]\/high_value_developers[\"Entity_value\"]\nhigh_value_developers['Top_Product_Value_Perc'] = high_value_developers[\"Entity_value\"] ** (high_value_developers[\"Value_Perc\"])\nmulti_table([high_prod_developers[['Developer','Releases']], high_value_developers[['Developer','Entity_value']], high_sale_dev[['Developer','Total_Owners']]])","f55f3d3b":"sns.set(style=\"whitegrid\")\nf, ax1 = plt.subplots(figsize=(8, 8))\nplt.xlim(left=10, right=10e7)\n# First plot total value\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=\"Entity_value\", y=\"Developer\", data=high_value_developers, label=\"Total\", color=\"b\", orient='h', ax=ax1)\nax1.set_title('Top seller developers')\nax1.set(ylabel=\"Developer\",xlabel=\"Value (log)\")\nax1.set_xscale(\"log\")\n\nax1.grid(True,which=\"both\",ls=\"--\",c='gray')\nax1.legend(ncol=2, loc=\"lower right\", frameon=True)\n\n\n# Plot earnings from highest bought product on top\nax2 = ax1.twinx()\nsns.set_color_codes(\"muted\")\nsns.barplot(x=\"Top_Product_Value_Perc\", y=\"Developer\", data=high_value_developers, label=\"Best product (%)\", color=\"b\", orient='h', ax=ax2)\nax2.legend(ncol=2, loc=\"lower center\", frameon=True)\n\nax2.set(ylabel=\"\",xlabel=\"\")\nax2.set_yticklabels(high_value_developers.Top_Product_Name)\nplt.show()\n\n#perhaps could show the most valuable product on the right side?","7db6b65b":"high_pub_publishers = publishers.nlargest(top_member_count, \"Releases\")\nhigh_value_publishers = publishers.nlargest(top_member_count, \"Entity_value\")\nhigh_sale_pub = publishers.nlargest(top_member_count, \"Total_Owners\")\nhigh_value_publishers[\"Value_Perc\"] = high_value_publishers[\"Top_Product_Value\"]\/high_value_publishers[\"Entity_value\"]\nhigh_value_publishers['Top_Product_Value_Perc'] = high_value_publishers[\"Entity_value\"] ** (high_value_publishers[\"Value_Perc\"])\nmulti_table([high_pub_publishers[['Publisher','Releases']], high_value_publishers[['Publisher','Entity_value']], high_sale_pub[['Publisher','Total_Owners']]])","1b8df736":"f, ax3 = plt.subplots(figsize=(8, 8))\nsns.set(style=\"whitegrid\")\nplt.xlim(left=10, right=10e7)\n# First plot total value\n\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=\"Entity_value\", y=\"Publisher\", data=high_value_publishers, label=\"Total\", color=\"r\", orient='h', ax=ax3)\nax3.legend(ncol=2, loc=\"lower right\", frameon=True)\nax3.set_title('Top seller publishers')\n\nax3.set(ylabel=\"Publisher\",xlabel=\"Value (log)\")\nax3.set_xscale(\"log\")\nax3.grid(True,which=\"both\",ls=\"--\",c='gray')  \n\n# Plot earnings from highest bought product on top\nax4 = ax3.twinx()\nsns.set_color_codes(\"muted\")\nsns.barplot(x=\"Top_Product_Value_Perc\", y=\"Publisher\", data=high_value_publishers, label=\"Best product (%)\", color=\"r\", orient='h', ax = ax4)\nax4.legend(ncol=2, loc=\"lower center\", frameon=True)\n\nax4.set(ylabel=\"\",xlabel=\"Value (log10)\")\nax4.set_xscale(\"log\")\nax4.set(ylabel=\"\",xlabel=\"\")\nax4.set_yticklabels(high_value_publishers.Top_Product_Name)\n\nplt.show()","001acdb3":"products['Playtime'] = playtime['playtime_forever'].groupby(playtime['appid']).transform('sum')","cdf3b0a0":"products[\"Release_Date\"] = pd.to_datetime(products[\"Release_Date\"])\nproducts[\"Lifetime_Days\"] = products.apply(lambda x: pd.Timedelta(pd.to_datetime(\"2016-12-31\") - x[\"Release_Date\"]).days, axis = 1)\n#developers[\"Playtime\"] = product_developers.apply(lambda x: ((products.loc[products[\"appid\"] == x.appid, [\"Playtime\"]]) \/ (products.loc[products[\"appid\"] == x.appid, [\"Lifetime_Days\"]])).sum(), axis = 1)\n\nproducts[\"Owners\"] = products.appid.apply(lambda x: len(playtime_groups.get_group(x)) if x in playtime_groups.groups.keys() else 0)\n#products = products[products.Owners > 0]","d29f27b9":"products_non_zero = products[products.Playtime > 0]\np2000plus =  products_non_zero[products_non_zero[\"Release_Date\"] > '2008-1-1']","8b4641dd":"f10, ax10 = plt.subplots(figsize=(20, 10))\nsns.scatterplot(x=\"Release_Date\", y=\"Playtime\", hue=\"Type\", sizes=(100, 600), alpha=.5, palette=\"muted\", ax=ax10, data=p2000plus) #size=\"weight\",\nax10.set_xlim([pd.to_datetime(\"2008-01-01\"), pd.to_datetime(\"2017-01-01\")])\nax10.set(ylabel=\"Playtime\",xlabel=\"Release Date\")","c10178de":"f10, ax10 = plt.subplots(figsize=(20, 10))\nsns.scatterplot(x=\"Release_Date\", y=\"Playtime\", hue=\"Type\", sizes=(100, 600), alpha=.5, palette=\"muted\", ax=ax10, data=p2000plus) #size=\"weight\",\nax10.set_xlim([pd.to_datetime(\"2012-01-01\"), pd.to_datetime(\"2016-01-01\")])\nax10.set(ylabel=\"Playtime\",xlabel=\"Release Date (zoomed)\")","395afe0e":"#games monthly means:\ngames_timed = games.copy()\n#games_timed = products[products.Playtime > 0]\ntime_condition_1 = (pd.to_datetime(games_timed.Release_Date) > pd.to_datetime('2009-1-1'))\ntime_condition_2 = (pd.to_datetime(games_timed.Release_Date) < pd.to_datetime('2016-1-1'))\ngames_timed =  games_timed[time_condition_1 & time_condition_2]\ngames_timed.index = pd.to_datetime(games_timed['Release_Date'])\ngames_monthly_means = games_timed.groupby(pd.Grouper(freq='M')).mean()\ngames_monthly_means[\"Type\"] = \"Game\"\ngames_monthly_means = games_monthly_means.reset_index()","20531dec":"#dlc monthly means:\ndlcs_timed = dlcs.copy()\n#games_timed = products[products.Playtime > 0]\ntime_condition_1 = (pd.to_datetime(dlcs_timed.Release_Date) > pd.to_datetime('2009-1-1'))\ntime_condition_2 = (pd.to_datetime(dlcs_timed.Release_Date) < pd.to_datetime('2016-1-1'))\ndlcs_timed =  dlcs_timed[time_condition_1 & time_condition_2]\ndlcs_timed.index = pd.to_datetime(dlcs_timed['Release_Date'])\ndlcs_monthly_means = dlcs_timed.groupby(pd.Grouper(freq='M')).mean()\ndlcs_monthly_means[\"Type\"] = \"DLC\"\ndlcs_monthly_means = dlcs_monthly_means.reset_index()","0fd81a65":"#dlc monthly means:\nmods_timed = mods.copy()\n#games_timed = products[products.Playtime > 0]\ntime_condition_1 = (pd.to_datetime(mods_timed.Release_Date) > pd.to_datetime('2009-1-1'))\ntime_condition_2 = (pd.to_datetime(mods_timed.Release_Date) < pd.to_datetime('2016-1-1'))\nmods_timed =  mods_timed[time_condition_1 & time_condition_2]\nmods_timed.index = pd.to_datetime(mods_timed['Release_Date'])\nmods_monthly_means = mods_timed.groupby(pd.Grouper(freq='M')).mean()\nmods_monthly_means[\"Type\"] = \"Mod\"\nmods_monthly_means = mods_monthly_means.reset_index()","089a56e1":"products_timed_means = games_monthly_means.append([dlcs_monthly_means, mods_monthly_means])\nproducts_timed_means = products_timed_means.dropna(subset=[\"Price\"])\nproducts_timed_means['Month'] = products_timed_means.index","6b42b3e4":"ax11 = sns.lmplot(x=\"Month\", y=\"Price\", hue = \"Type\", data=products_timed_means, height=6, aspect=2)\nax11.set(ylabel=\"Price\",xlabel=\"Monthly average since 2009\")","eb2ddcdf":"products_rating_plot = products[(products.Rating >= 0) & (products.Owners > 10) & (products.Playtime > 100)]","15c18cc5":"fig, (ax20, ax21, ax22) = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(20,5))\nax20.set_ylim(30,100) \n\nsns.regplot(x=\"Price\", y=\"Rating\", data=products_rating_plot, ax=ax20)\nsns.regplot(x=np.log10(products_rating_plot[\"Playtime\"]), y=products_rating_plot.Rating, order=1, color=\"g\", ax = ax21)\nax21.set(ylabel=\"Rating\",xlabel=\"Playtime (log10)\")\nsns.regplot(x=np.log10(products_rating_plot[\"Owners\"]), y=products_rating_plot.Rating, data=products_rating_plot, color=\"r\", ax = ax22)\nax22.set(ylabel=\"Rating\",xlabel=\"Owners (log10)\")","6e7f7d60":"genres_types = genres.Genre.unique()\ngenres_groups = genres.groupby(\"Genre\")\ngenres_desc = genres_groups.describe()\nkey = ('appid','count')\ngenres_desc = pd.DataFrame(genres_desc[key])\ngenres_desc = genres_desc.sort_values(key, ascending=False)","3853273c":"sns.set(style=\"whitegrid\")\nf, ax = plt.subplots(figsize=(8, 8))\n# First plot total value\nsns.set_color_codes(\"pastel\")\nent_val_graph = sns.barplot(x=key, y=genres_desc.index, data=genres_desc, label=\"Total\", color=\"b\", orient='h')\nsns.set_color_codes(\"muted\")\n\nax.set(ylabel=\"\",xlabel=\"Product Amount\")\nax.set_xscale(\"log\")\nax.grid(True,which=\"both\",ls=\"--\",c='gray')  \nplt.show()","80343e16":"import networkx as nx","481fbb70":"genre_intersections = pd.DataFrame(columns=[\"Genre_A\",\"Genre_B\",\"Weight\"])\nkeys = [key for key, _ in genres_groups]\nfor j in range(len(keys)):\n    A_list = []\n    B_list = []\n    W_list = []\n    for i in range(j+1, len(genres_groups)):\n        weight = len(set(genres_groups.get_group(keys[j])[\"appid\"]).intersection(set(genres_groups.get_group(keys[i])[\"appid\"])))\n        if weight > 0:\n            A_list.append(keys[j])\n            B_list.append(keys[i])\n            W_list.append(weight)\n\n    df = pd.DataFrame({\"Genre_A\":A_list, \"Genre_B\":B_list, \"Weight\":W_list})\n    genre_intersections = genre_intersections.append(df)\n\ngenre_intersections[\"N_Weight\"] = ((genre_intersections.Weight \/ (genre_intersections.Weight).max()) * 4) +0.2","cf33fbb1":"G = nx.from_pandas_edgelist(genre_intersections,'Genre_A','Genre_B', edge_attr='N_Weight')\ndurations = [i['N_Weight'] for i in dict(G.edges).values()]\nlabels = [i for i in dict(G.nodes).keys()]\nlabels = {i:i for i in dict(G.nodes).keys()}\n\nfig, ax = plt.subplots(figsize=(12,15))\nmargin=0.01\nfig.subplots_adjust(left=margin, right=1.0-margin)\npos = nx.spring_layout(G)\nnx.draw_networkx_nodes(G, pos, ax = ax, node_color='#ACE7FF', labels=True)\nnx.draw_networkx_edges(G, pos, width=durations, edge_color='#A79AFF',ax=ax)\n_ = nx.draw_networkx_labels(G, pos, labels, ax=ax)","4bba9395":"# This dataset is provided in GeoPandas\nworld_filepath = gpd.datasets.get_path('naturalearth_lowres')\nworld = gpd.read_file(world_filepath)","92f46fed":"# Real data imports:\n#PLAYER_SUMMARIES\tsteamid\tloccountrycode\ttimecreated\nplayers_data_link = \"..\/input\/steam-player-summaries\/players_filtered.csv\"\nplayers = pd.read_csv(players_data_link).drop(['gameserverip', 'cityid', 'timecreated'], axis = 1).dropna(subset=[\"loccityid\"])","452cc968":"# creating an embed function to visualize maps in all browsers\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='700px')","7b39545a":"#prepare locale and coordinates, to be able to extract data from users locale, because steam uses its own id system. Not all entries have 'coordinates' data, so have to account for that later on...\nimport json \n\ndef flatten_to_countries(json):\n    output = []\n    for country, val_c in json.items():\n        if 'coordinates' in val_c.keys():\n            row = {'country_ID' : country, 'Country': val_c['name'],'coordinates': val_c['coordinates']}\n            output.append(row)\n        else:\n            row = {'country_ID' : country, 'Country': val_c['name'],'coordinates': ''}\n    return output\n\ndef flatten_to_states(json):\n    output = []\n    for country, val_c in json.items():\n        for state, val_s in json[country]['states'].items():\n            if 'coordinates' in val_s.keys():\n                row = {'state_ID':state, 'State': val_s['name'], 'country_ID':country, 'coordinates':val_s['coordinates']}\n            else:\n                row = {'state_ID':state, 'State': val_s['name'], 'country_ID':country, 'coordinates':''}\n            output.append(row)\n\n    return output\n\ndef flatten_to_cities(json):\n    output = []\n    for country, val_c in json.items():\n        for state, val_s in json[country]['states'].items():\n            for city, val_city in val_s['cities'].items():\n                row = {'city_ID':city, 'City': val_city['name'].lower(), 'country_ID':country, 'state_ID':state}\n                output.append(row)\n    return output\n\nst_countries_path = \"..\/input\/steam-countries\/steam_countries.min.json\"\n\nwith open(st_countries_path) as f:\n    json_countries = json.load(f)\n    flat_countries = flatten_to_countries(json_countries)\n    flat_states = flatten_to_states(json_countries)\n    flat_cities = flatten_to_cities(json_countries)\n    \n    countries_df = pd.DataFrame(flat_countries)\n    states_df = pd.DataFrame(flat_states)\n    cities_df = pd.DataFrame(flat_cities)","79428bab":"#prepare data for filling in missing coordinates\nworld_cities_pop_df = pd.read_csv(\"..\/input\/world-cities-database\/worldcitiespop.csv\", low_memory=False)\n#drop all rows which do not have a population in them. There are multiple rows like that for some reason in \"worldcitiespop.csv\"\nworld_cit_only_pop = world_cities_pop_df.dropna(subset=['Population'])\nworld_cities_pop_df = world_cities_pop_df[~world_cities_pop_df.City.isin(world_cit_only_pop.City)]\n#add only unique values, so that we wont miss anything when joining into steam id's\nworld_cities_pop_df = pd.concat([world_cit_only_pop,world_cities_pop_df])","f6029088":"countries_df['Latitude'] = countries_df.coordinates.apply(lambda x: float(x.split(',')[0]) if x is not \"\" else None)\ncountries_df['Longitude'] = countries_df.coordinates.apply(lambda x: float(x.split(',')[1]) if x is not \"\" else None)\nstates_df['Latitude'] = states_df.coordinates.apply(lambda x: float(x.split(',')[0]) if x is not \"\" else None)\nstates_df['Longitude'] = states_df.coordinates.apply(lambda x: float(x.split(',')[1]) if x is not \"\" else None)\n# merge steam ID table with world city coordinates\ncities_merged_df = cities_df.merge(world_cities_pop_df[['City','Population',\"Latitude\",'Longitude']], on=\"City\", how=\"inner\")\ncities_merged_df[\"City\"] = cities_merged_df[\"City\"].str.capitalize()","c25cd486":"players[\"loccityid\"] = players[\"loccityid\"].astype(int)\ncities_merged_df[\"city_ID\"] = cities_merged_df[\"city_ID\"].astype(int)","26d17e02":"players_groups = players.groupby([\"loccityid\"])\ncities_merged_df[\"Players\"] = players_groups.apply(lambda grp: grp.size)\ncities_merged_df = cities_merged_df.dropna()\ncities_merged_df[\"Gamer_Weight\"] = 1.0 * cities_merged_df['Players'] \/ cities_merged_df['Population']","4ccae1e0":"cities_gdf = gpd.GeoDataFrame(cities_merged_df, geometry=gpd.points_from_xy(cities_merged_df.Latitude, cities_merged_df.Longitude))\nLTs = cities_gdf[cities_gdf.country_ID == \"LT\"].sort_values(by=\"Players\", ascending=False).iloc[:100]\nm_1 = folium.Map(location=[54.899298, 23.888495], tiles='Stamen Toner', zoom_start=7)\ntop_1000_cities_gdf = cities_gdf.sort_values(by=\"Players\", ascending=False).iloc[:1000]\ntop_1000_cities_gdf = top_1000_cities_gdf.append(LTs)\nfor i in range(0, len(top_1000_cities_gdf)):\n    f_html = 'City: {}<br>Population: {:d}<br>Players: {:d}<br>Gamer Weight: {:.1%}'.format(top_1000_cities_gdf.iloc[i]['City'], int(top_1000_cities_gdf.iloc[i]['Population']), int(top_1000_cities_gdf.iloc[i]['Players']), top_1000_cities_gdf.iloc[i]['Gamer_Weight'])\n    popup = folium.Popup(\n        f_html,\n        max_width=200,\n        min_width=200)\n    folium.CircleMarker(\n        location=[top_1000_cities_gdf.iloc[i]['geometry'].x, top_1000_cities_gdf.iloc[i]['geometry'].y],\n        radius=((top_1000_cities_gdf.iloc[i][\"Players\"]\/1000)**(1\/2)), #\n        color='#3186cc',\n        fill=True,\n        fill_color='#3186cc',\n        popup = popup\n        ).add_to(m_1)","43a6c683":"m_1\n#embed_map(m_1, \"m_1.html\") #for some reason embeding no longer works","6a9566d2":"LT_Players = LTs.nlargest(10, \"Players\")\nLT_Gamer_Weight = LTs.nlargest(10, \"Gamer_Weight\")\nmulti_table([LT_Players[['City','Players']], LT_Gamer_Weight[['City','Gamer_Weight']]])","c02441ac":"Let's take products that are newer than the platform to make it more consistent and filter out zero playtimes (zero here means player owns the product, but has never played it. It is interesting data by itself, but not for the scope of this notebook).","4549059f":"### Top Developer Studios in Market","7d14f475":"Top product values here are shown in percentiles, same as explained bellow chart \"Top seller developers\".","b0eb1a6c":"## Product value\n\n* Draw popularity of products (total playtime) adjusted by the developer's release-time\n\n\n* See mean game price monthly historical changes data\n\n","0312b4b9":"Let's see average prices per release per month","b01a24ca":"We can find some interesting trends here, although some nationalities obviously tend to not reveal their data too much (or at all).\n\nWhat would be top 10 cities\/towns in Lithuania by gamer count and gamer percentage?\n\nHint: those are quite tiny cities\/towns.","8c5d2293":"$\nx_p = h_e^\\frac{h_t}{h_e}\n$\n, where $x_p$ is drawn value of the product, $h_e$ - total entity value, $h_t$ - top product value.","3abd5478":"This clearly confirms the suspicion: we can also see clear playtime patterns around Winter holiday times at years end - seems like a time to avoid games releases during that time! And it makes sense, why release when so many other games have discounts! Althouth it somewhat discipates in later years...","3a67bfe5":"# An exploratory attempt to tap into Steam user tendencies (data obtained in 2016)\n**109 MILLION GAMERS***\n\n**716 MILLION GAMES***\n\n**1.1 MILLION YEARS OF PLAYTIME***\n\nReference:\n\nDatabase retrieved reformated and re-uploaded to Kaggle from the [researcher's published site][1], where you can download their raw SQL. \n\nA dataset collected and analyzed for the **2016** ACM Internet Measurement Conference article by Mark O'Neill, Justin Wu, Elham Vaziripour, and Daniel Zappala\nsee [the paper][2].\n\n[1]: https:\/\/steam.internet.byu.edu\/\n[2]: https:\/\/steam.internet.byu.edu\/oneill-condensing-steam.pdf\n\n*the numbers were claimed by the authors of publication, although as we see later - data exploration reveals otherwise\n### I would love to work with newer data, so hit me up with suggestions :)\n\n","36bd44de":"The provided data was in MySQL script format, which introduced a weird problem - SQL Workbench could not export large tables into CSV, so I came up with a solution to write a python script.\n\nIt simply takes out data in chunks and appends to a file. Since SQL queries slow down logarithmically when requesting further in the table via \"LIMIT x, y\", a colleague proposed to make a copy of a table and keep deleting after an amount of requests (I chose 350k rows) were saved into csv.","826aa5b2":"* Publisher","88504fe5":"# Conclusions\nBear in mind the data is 4 years old, which is equivalent to barely usable - since then we've had a whole new section of games emerge. I mean, of course, VR\/AR. Steam also had major tech and policy updates since.\n1. With publishers it seems that more releases reward with more owners proportionally, but developers mostly are concentrated on a smaller amount of releases and cases where they would have a lot are rare.\n2. Most developers on steam self-publish.\n3. Most top rated developers and even publishers received a considerably large part of their revenue from a single title.\n4. Developers and publishers avoid releasing games on world-wide holiday seasons.\n5. Higher priced products tend to also be also higher rated.\n6. DLC monthly-mean prices are increasing over time quite steadily.","907a4967":"Interestingly enough, higher rated games do not have a meaningful tendency to be played more. An obvious correlation here is Owners vs Price, but it is good to make a sanity check once in a while - it makes sense. It is interesting though, that more playtime does not mean a better (rated) game, on the other hand more purchases often do! Another new insight could be the positive correlation of Rating vs Price. This could be useful when making pricing plans for your games!","e454d887":"### Top Publishers in Market","8ce5ae86":"Instead, let's compare game-only releases of developers vs publishers.","d446f4a8":"Looks like Telltale was first by the amount of products released. RIP Telltale :(","878f123e":"## World of Steam\n* Show global map of all users, draw bigger markers on cities, where registered user amount is larger\n* Find highest percentage of gamers vs population and plot a chloroplest map to visualize it - this doesn't work, because population data mismatch gamer data. A lot of cities get 1000's of percentages, which of course is inaccurate to say the least...","a7e7b2da":"We see here that publishers do not mark their products as DLCs almost all of the time. So sadly, we cannot evaluate differences, even if there aren't any.","bcbebb15":"### Show genre relation network\n\n\nSince each product can have multiple genres selected by the publisher, this is a nice opportunity to visualize the relational network between genres.","9a0bfe42":"Original idea was to show a heatmap, where heavier areas would be shown where there's a higher 'Gamer weight'. Sadly world citizen data is inconsistent compared to player data, thus showing in some towns as much as 6000% steam users related to population, which of course is impossible. I've left the value for anyone who's interested, though the sizes are adjusted via player count value in town.\n\nIf you are viewing in github the map will not be displayed. You can go to Kaggle and see the [original notebook and view the map](https:\/\/www.kaggle.com\/danieliusv\/steam-game).","e206eade":"* Developer","dd89ba0d":"Let's see how DLC's sales relate to Game sales visually.","62c3b85f":"### Developers","1e46d872":"As we could have already guessed Indie, Action and Adventure makes up the major triangle. Somewhat interesting to see two clusters of products formed:\n1. Games\n2. Education\/training\n3. Accounting is there too, wait what?! Looks like Accounting + Racing is an unexplored field yet...","4db15843":"Holek has provided us with most of the coordinates, but sadly not all, so we will have to try matching the names of the cities to extract coordinates from world map data. Let's try filling missing city coordinates.","dec778d3":"I wanted to see how much of a relative impact did the top product had, but since the X axis here is in log scale, plotting top product value also in logarithmic tricks the eye. Therefore, I transformed product value scale for better representation. The result is quite decent - now you can see the bar size as a percentage of the total bar size. The transformation used here is pretty simple - just raise the value in power of the fraction needed:","0488a53f":"Sadly seaborn does not provide a slope value. There was a [big discussion](https:\/\/stackoverflow.com\/questions\/33490833\/display-regression-equation-in-seaborn-regplot) after requesting such feature in 2015, but the author opposed. \nIt is obvious however that all prices are going up - as it should be. The question is - do they go up faster than normal inflation? There is an interesting trend seen here, that DLC prices are increasing way faster, by today (2020), they must have defintely gone past regular game price. In no surprise we can se that some DLCs that come out today are priced higher than base game. This must be a natural mechanism of leveraging DLC-based monetization model to increase product price as well as overall gaining popularity of the pricing model.","48665944":"We can see that more than half of developers self-publish games. There are 783 publishers, who do not develop themselves, instead only release other developer games on Steam.","556bd4ca":"## Data\nImporting, appending some which was split between tables in the DB.","ccbbe760":"Another interesting point to pay attention is that even three studios who worked on CoD:MW3 are in top 10. We can ignore these, since because multiple studios worked on the same project, we cannot attribute 100% of value delivered to any of them and we cannot discern from this data how sizable their input was.","d3b11e03":"* Self-published developers - in the overlapping circle:","5ee121ac":"## Further Exploration\nA lot of interesting insights could be done, but the scope of this notebook is at an end. Here I list some of the ideas that I decided not to implement in this case due to time constraints, but perhaps could do later, i.e. if I obtained new data from Steam some time in the future.\n\n#### DLC demand\n* Count additional products for each title (app id) and add a column into a dataframe.\n* How many DLCs per product count changes historically?\n\n\n#### Kickstarter success on Steam\n* Find and generate kickstarter game publishers who has their game on Steam. There is data to identify which products were started through KS available on Kaggle.\n* Calculate and plot their value and playtime, check genre distributions.\n\n\n#### Genre value\n* Most valuable game genre on Steam.\n* Most selective audience is of the G2 genre - uniquely concentrated around L0 location.\n* Highest non-English language-genre pair is L1-G3.\n* Plot \"Weighted owners\" of each genre.\n\n#### Genre clusters\n* Find the global value of each Genre \/ Tag which (products * owners * price).\n* Show how global value changed of top 3 most common genres over time.\n* Find most common genre * tags combo.\n* Show sum global values of top 5 most common combos.\n\n#### Game genre popularity and dispersion\n* Filter owners of each game genre and show top 3 genres world heatmaps.\n* Find the genre with smallest dispersion of it's users.\n* Show top language of users for each genre (do same excluding and including English to compare).","a9a7cd71":"## Top Market Players\nFor each dev and publisher:\n* count released products (top 10 highest producers\/publishers)\n* calculate total entity value (top 10 highest all-time earners)\n* calculate the average price of their products (top 10 luxurious)","edc34047":"### What rating tendencies are there?\n* vs price\n* vs units sold (or visa versa)\n* vs total cumulative playtime","c7e68f78":"First, let's prepare and tidy up our data a bit:\n* filter out unusable\/missing data and change locale cell to a proper geocode \/ coordinates point\n* group players by city ID, count their number and a column to city","7b45ec3a":"* As seen from this nice overlay KDE, there's a slight correlation between having more releases and more sales both as a developer and publisher.\n* More publishers also tend to have more game releases compared to developers alone, which completely makes sense! Developers tend to work more on a single product for a long time.\n* There is a lot overlap too, especially in the center, these are likely self-publishing developers.","7d053477":"Then let's extract steam locale IDs. We do this by using a dataset (json format) shared in github by a user  Holek. Sadly, the json has multiple levels (and weird dictionary format too), so to form proper dataframes, we'll need to flatten them into lists first.","24203715":"**All tables are fully exported, except \"playtime\" data, it's a sample of 3%**\n\nInside the database researchers provide three tables about player play-times. One table, is the initial crawl, another - a subsequent larger crawl. The third one is a daily crawl of a selected subset of players. In this notebook we would need the first and subsequent data, but these tables are so large, that kaggle\/jupyter won't run for various reasons. When I tried, I got the error \"out of memory\" and my PC build has got some serious power! Therefore, I took a sample of 3%, which barely scratches the surface, but the conclusions could still be accurate enough, just have to multiply them by 33.","81ca11d7":"Since I am from Lithuania, I am interested in the data there. Numbers here are tiny and boring, probably not a lot of players input their city... I also added a 1000 top-population world cities.","aa38ae1d":"There was definitely a boom of DLCs being played during 2013 compared to games. Let's zoom in on 2012-2016 to confirm.","e716fdfd":"## Genres available\n* Filter and find a list of unique genres\n* Find which genres have most products per title","650cac07":"## Releases vs Sales\nDo more product releases mean exponential popularity or a more linear one?\n* Make a dataframe on each Publisher \/ Developer;\n* Count how many products and titles each has, how many units users own;\n* Does Released title count increase Owner count linearly or is there a hotspot?\n* How many developers on steam self-publish?"}}