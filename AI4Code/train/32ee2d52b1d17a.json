{"cell_type":{"e111a1d3":"code","7f18431a":"code","57bd1067":"code","39257945":"code","84768e37":"code","13cdcb30":"code","c37517d4":"code","6aa3132d":"code","c38277df":"code","47ef8142":"code","d5d56bcb":"code","b164d149":"code","b421fe28":"code","dfb8fb7c":"code","0f6405cc":"code","6859c333":"code","603c5d29":"code","cf2e32b4":"code","8ac6aa21":"code","9afd5a8d":"code","74129351":"code","50fd2b2d":"code","c3d9fc25":"code","f45a3e56":"code","ca6d8724":"code","ebdf0666":"code","8ef06250":"code","1c91a706":"code","5f5d5663":"code","cbf521b3":"code","d89a49a6":"code","7dee6b96":"code","7732d884":"code","222b8a96":"code","86a1a2bc":"markdown","648c662a":"markdown","e55dedc5":"markdown","b218f749":"markdown","f4ee704d":"markdown","6685efe3":"markdown","efeeea9b":"markdown"},"source":{"e111a1d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re # reguliar expression\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7f18431a":"train_data = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_data  =pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain_data.head(10)","57bd1067":"train_data.dtypes","39257945":"train_data[train_data['target']==1][\"text\"]","84768e37":"train_data[train_data['target']==0][\"text\"]","13cdcb30":"import re\ndef  clean_text(df, text_field, new_text_field_name):\n    df[new_text_field_name] = df[text_field].str.lower()\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)|^rt|http.+?\", \"\", elem))  \n    # remove numbers\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n    return df\n\ndata_clean = clean_text(train_data, 'text', 'text_clean')\ndata_clean_tr = clean_text(train_data, 'text', 'text_clean')\ndata_clean_ts = clean_text(test_data, 'text', 'text_clean')","c37517d4":"data_clean.head()","6aa3132d":"import nltk\nfrom nltk.corpus import stopwords\nprint(stopwords.words('english'))","c38277df":"#Cleaning text\nimport nltk.corpus\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\ndata_clean_tr['text_clean'] = data_clean_tr['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ndata_clean_ts['text_clean'] = data_clean_ts['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n\ndata_clean.head()","47ef8142":"#text tokenization\nimport nltk \nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize, word_tokenize\ndata_clean['text_tokens'] = data_clean['text_clean'].apply(lambda x: word_tokenize(x))\ndata_clean.head()","d5d56bcb":"# Stemming words with NLTK\nfrom nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize\ndef word_stemmer(text):\n    stem_text = [PorterStemmer().stem(i) for i in text]\n    return stem_text\ndata_clean['text_clean_tokens'] = data_clean['text_tokens'].apply(lambda x: word_stemmer(x))\ndata_clean.head()","b164d149":"# text lemmatisation \nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\ndef word_lemmatizer(text):\n    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n    return lem_text\ndata_clean['text_clean_tokens'] = data_clean['text_tokens'].apply(lambda x: word_lemmatizer(x))\ndata_clean['text_clean_tokens'] = data_clean['text_clean_tokens'].astype(str)\ndata_clean.head()","b421fe28":"#Tokenization vs Stemming & Lemmatization\nX1 = data_clean['text_tokens'][0] \nX2 = data_clean['text_clean_tokens'][0]\nprint(X1,X2, sep='\\n')","dfb8fb7c":"# example of a disaster tweet\ndata_clean[data_clean_tr[\"target\"] == 1][\"text_clean\"].values[1]","0f6405cc":"# example of what is NOT a disaster tweet.\ndata_clean[data_clean_tr[\"target\"] == 0][\"text_clean\"].values[1]","6859c333":"X = data_clean_tr.iloc[ : ,5]\ny = data_clean_tr.iloc[ : ,4]","603c5d29":"# Splitting the training data into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","cf2e32b4":"#Text Vectorization using TfidfVectorizer \/\/Convert a collection of raw documents to a matrix of TF-IDF features.\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nX_train = vectorizer.fit_transform(X_train)\nX_test = vectorizer.transform(X_test)","8ac6aa21":"# Training the Logistic Regression model on the Training set\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(C = 0.1)\nclassifier.fit(X_train, y_train)","9afd5a8d":"#Explain the linear model\nimport shap\nexplainer = shap.LinearExplainer(classifier, X_train, feature_dependence=\"independent\")\nshap_values = explainer.shap_values(X_test)\nX_test_array = X_test.toarray() # we need to pass a dense version for the plotting functions","74129351":"#Summarize the effect of all the features\nshap.summary_plot(shap_values, X_test_array, feature_names=vectorizer.get_feature_names())","50fd2b2d":"from sklearn.metrics import classification_report\ny_pred = classifier.predict(X_test)\nprint(classification_report(y_test, y_pred))","c3d9fc25":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm","f45a3e56":"#Confusion Matrix Visualisation\nimport matplotlib.pyplot as plt \nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(classifier, X_test, y_test) \nplt.show()","ca6d8724":"from sklearn import feature_extraction\ncount_vectorizer = feature_extraction.text.TfidfVectorizer()\n\n## let's get counts for the first 5 tweets in the data\nexample_train_vectors = count_vectorizer.fit_transform(data_clean_tr[\"text\"][0:5])","ebdf0666":"## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\nprint(example_train_vectors[0].todense().shape)\nprint(example_train_vectors[0].todense())","8ef06250":"train_vectors = count_vectorizer.fit_transform(data_clean_tr[\"text_clean\"])\n\n## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n# that the tokens in the train vectors are the only ones mapped to the test vectors - \n# i.e. that the train and test vectors use the same set of tokens.\ntest_vectors = count_vectorizer.transform(data_clean_ts[\"text_clean\"])","1c91a706":"# Training the Logistic Regression model on all the Training set\nfrom sklearn.ensemble import AdaBoostClassifier\nclassifier = AdaBoostClassifier(n_estimators=100, base_estimator= None,learning_rate=1, random_state = 1)\nclassifier.fit(train_vectors, data_clean_tr[\"target\"])","5f5d5663":"from sklearn import model_selection\nscores = model_selection.cross_val_score(classifier, train_vectors, train_data[\"target\"], cv=3, scoring=\"f1\")\nscores","cbf521b3":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","d89a49a6":"y_pred  = classifier.predict(test_vectors)","7dee6b96":"sample_submission[\"target\"] = y_pred","7732d884":"sample_submission.to_csv(\"submission.csv\", index=False)\n","222b8a96":"sample_submission.head()","86a1a2bc":"# Text preproceesing","648c662a":"# Text Vectorization","e55dedc5":"# text lemmatisation ","b218f749":"# text tokenization","f4ee704d":"* Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.\n* Text preprocessing includes both Stemming as well as Lemmatization. Many times people find these two terms confusing. Some treat these two as same. Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words.\n**Examples of lemmatization:-> better : good**","6685efe3":"# Cleaning text","efeeea9b":"# Stemming words with NLTK"}}