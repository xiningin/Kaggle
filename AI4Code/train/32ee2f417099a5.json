{"cell_type":{"5f7bd508":"code","3aeabcf0":"code","a73d597d":"code","23fb8038":"code","da1733ec":"code","f0b92b94":"code","ce477691":"code","6e34c7ec":"code","36d91d5c":"code","c9b96aaa":"code","cdd7bcb5":"code","9107b3ba":"code","75eb78fe":"code","882a7ace":"code","470bc873":"code","3f068dcd":"code","49dd0d6d":"code","56b06395":"code","2f7fa175":"code","b17182d3":"code","892943be":"code","83224a82":"code","adb6a68e":"code","21312b9f":"code","f13a3deb":"markdown","f39be6da":"markdown","f3aee7d5":"markdown","882af79d":"markdown","10c29cb3":"markdown","6c53960c":"markdown","eac7a7de":"markdown","da5d1f34":"markdown","ee2de075":"markdown","4ef04ed3":"markdown","62680b22":"markdown","930f3a4e":"markdown","7acf4239":"markdown","1e76c5f3":"markdown"},"source":{"5f7bd508":"!pip install xmltodict\n\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport xmltodict\nimport random\nfrom os import listdir\nfrom os.path import isfile, join\nimport torchvision \nimport torch \nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n","3aeabcf0":"def getImageNames():\n    image_names = []\n    for dirname, _, filenames in os.walk('\/kaggle\/input'):\n        for filename in filenames:\n            fullpath = os.path.join(dirname, filename)\n            extension = fullpath[len(fullpath) - 4:]\n            if extension != '.xml':\n                image_names.append(filename)\n    return image_names\n\n\ndef get_path(image_name):\n    \n    #CREDIT: kaggle.com\/dohunkim\n    \n    home_path = '\/kaggle\/input\/medical-masks-dataset\/'\n    image_path = home_path + 'images\/' + image_name\n    \n    if image_name[-4:] == 'jpeg':\n        label_name = image_name[:-5] + '.xml'\n    else:\n        label_name = image_name[:-4] + '.xml'\n    \n    label_path = home_path + 'labels\/' + label_name\n        \n    return  image_path, label_path","a73d597d":"\ndef parse_xml(label_path):\n    \n    #CREDIT: kaggle.com\/dohunkim\n    \n    x = xmltodict.parse(open(label_path , 'rb'))\n    item_list = x['annotation']['object']\n    \n    # when image has only one bounding box\n    if not isinstance(item_list, list):\n        item_list = [item_list]\n        \n    result = []\n    \n    for item in item_list:\n        name = item['name']\n        bndbox = [(int(item['bndbox']['xmin']), int(item['bndbox']['ymin'])),\n                  (int(item['bndbox']['xmax']), int(item['bndbox']['ymax']))]       \n        result.append((name, bndbox))\n    \n    size = [int(x['annotation']['size']['width']), \n            int(x['annotation']['size']['height'])]\n    \n    return result, size\n\n\ndef visualize_image(image_name, bndbox=True):\n    \n    #CREDIT: kaggle.com\/dohunkim\n    \n    \n    image_path, label_path = get_path(image_name)\n    \n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    if bndbox:\n        labels, size = parse_xml(label_path)\n        thickness = int(sum(size)\/400.)\n        \n        for label in labels:\n            name, bndbox = label\n            \n            if name == 'good':\n                cv2.rectangle(image, bndbox[0], bndbox[1], (0, 255, 0), thickness)\n            elif name == 'bad':\n                cv2.rectangle(image, bndbox[0], bndbox[1], (255, 0, 0), thickness)\n            else: # name == 'none'\n                cv2.rectangle(image, bndbox[0], bndbox[1], (0, 0, 255), thickness)\n    \n    plt.figure(figsize=(20, 20))\n    plt.subplot(1, 2, 1)\n    plt.axis('off')\n    plt.title(image_name)\n    plt.imshow(image)\n    plt.show()","23fb8038":"image_names = getImageNames()","da1733ec":"NUM_OF_IMGS_TO_VISUALIZE = 3\n\nfor i in range(NUM_OF_IMGS_TO_VISUALIZE):\n    visualize_image(image_names[i])","f0b92b94":"def cropImage(image_name):\n    image_path, label_path = get_path(image_name)\n    \n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    \n    labels, size = parse_xml(label_path)\n    \n    cropedImgLabels = []\n\n    for label in labels:\n        name, bndbox = label\n        \n        \n        croped_image = image[bndbox[0][1]:bndbox[1][1], bndbox[0][0]:bndbox[1][0]]\n        \n        label_num = 0\n        \n        if name == \"good\":\n            label_num = 0\n        elif name == \"bad\":\n            label_num = 1\n        else:\n            label_num = 2\n        \n        cropedImgLabel = [croped_image, label_num]\n        \n        cropedImgLabels.append(cropedImgLabel)\n        \n    return cropedImgLabels\n        \n        \n        \n    ","ce477691":"def createDirectory(dirname):\n    try:\n        os.mkdir(dirname)\n    except FileExistsError:\n        print(\"Directory \" + dirname + \" already exists.\")","6e34c7ec":"dir_name = 'train\/'\nlabel_0_dir = dir_name + \"0\/\"\nlabel_1_dir = dir_name + \"1\/\"\n#label_2_dir = dir_name + \"2\/\"\nmodels_dir = \"models\/\"\n\n\ncreateDirectory(dir_name)\ncreateDirectory(label_0_dir)\ncreateDirectory(label_1_dir)\n#createDirectory(label_2_dir)\ncreateDirectory(models_dir)","36d91d5c":"label_0_counter = 0\nlabel_1_counter = 0\n#label_2_counter = 0\n\nfor image_name in image_names:\n    cropedImgLabels = cropImage(image_name)\n    \n    for cropedImgLabel in cropedImgLabels:\n        \n        label = cropedImgLabel[1]\n        img = cropedImgLabel[0]\n        \n        if label == 0:\n            croped_img_name = str(label_0_counter) + \".jpg\"\n            cv2.imwrite(label_0_dir + croped_img_name, img)\n            label_0_counter += 1\n        elif label == 1:\n            croped_img_name = str(label_1_counter) + \".jpg\"\n            cv2.imwrite(label_1_dir + croped_img_name, img)\n            label_1_counter += 1\n        #else:\n            #croped_img_name = str(label_2_counter) + \".jpg\"\n            #cv2.imwrite(label_2_dir + croped_img_name, img)\n            #label_2_counter += 1","c9b96aaa":"filenames_label_0 = [f for f in listdir(label_0_dir) if isfile(join(label_0_dir, f))]\nfilenames_label_1 = [f for f in listdir(label_1_dir) if isfile(join(label_1_dir, f))]\n#onlyfiles_2 = [f for f in listdir(label_2_dir) if isfile(join(label_2_dir, f))]","cdd7bcb5":"print(\"Total number of images: \" + str(len(filenames_label_0) + len(filenames_label_1)))\nprint(\"Number of images labeled 0: \" + str(len(filenames_label_0)))\nprint(\"Number of images labeled 1: \" + str(len(filenames_label_1)))\n#print(\"Number of images labeled 2: \" + str(len(onlyfiles_2)))","9107b3ba":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","75eb78fe":"model = models.resnet50(pretrained=True)","882a7ace":"for layer, param in model.named_parameters():\n    \n    if 'layer4' not in layer:\n        param.requires_grad = False\n\nmodel.fc = torch.nn.Sequential(torch.nn.Linear(2048, 512),\n                                 torch.nn.ReLU(),\n                                 torch.nn.Dropout(0.2),\n                                 torch.nn.Linear(512, 2),\n                                 torch.nn.LogSoftmax(dim=1))","470bc873":"train_transforms = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n    ])","3f068dcd":"dataset = datasets.ImageFolder(dir_name, transform = train_transforms)\n\ndataset_size = len(dataset)\ntrain_size = int(dataset_size * 0.6)\nval_size = int(dataset_size * 0.2)\ntest_size = dataset_size - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n\nprint('Dataset size: ', len(dataset))\nprint('Train set size: ', len(train_dataset))\nprint('Validation set size: ', len(val_dataset))\nprint('Test set size: ', len(test_dataset))","49dd0d6d":"BATCH_SIZE = 20\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset,\n                                          batch_size=BATCH_SIZE,\n                                          shuffle=True)\n\nval_loader = torch.utils.data.DataLoader(val_dataset,\n                                          batch_size=BATCH_SIZE,\n                                          shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(test_dataset,\n                                          batch_size=BATCH_SIZE,\n                                          shuffle=True)","56b06395":"LEARNING_RATE = 0.001\n\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)","2f7fa175":"#model.cuda()\nmodel.to(device)","b17182d3":"total_epoch = 20\n\nbest_epoch = 0\ntraining_losses = []\nval_losses = []\n\n\nfor epoch in range(total_epoch):\n    \n    epoch_train_loss = 0\n    \n    for X, y in train_loader:\n        \n        X, y = X.cuda(), y.cuda()\n        \n        optimizer.zero_grad()\n        result = model(X)\n        loss = criterion(result, y)\n        epoch_train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n      \n    training_losses.append(epoch_train_loss)\n    \n    \n    epoch_val_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for X, y in val_loader:\n            \n            X, y = X.cuda(), y.cuda()\n             \n            result = model(X)\n            loss = criterion(result, y)\n            epoch_val_loss += loss.item()\n            _, maximum = torch.max(result.data, 1)\n            total += y.size(0)\n            correct += (maximum == y).sum().item()\n            \n    val_losses.append(epoch_val_loss)\n    accuracy = correct\/total\n    print(\"EPOCH:\", epoch, \", Training Loss:\", epoch_train_loss, \", Validation Loss:\", epoch_val_loss, \", Accuracy: \", accuracy)\n    \n    \n    if min(val_losses) == val_losses[-1]:\n        best_epoch = epoch\n        checkpoint = {'model': model,\n                            'state_dict': model.state_dict(),\n                            'optimizer' : optimizer.state_dict()}\n\n        torch.save(checkpoint, models_dir + '{}.pth'.format(epoch))\n        print(\"Model saved\")","892943be":"plt.plot(range(total_epoch), training_losses, label='Training')\nplt.plot(range(total_epoch), val_losses, label='Validation')\nplt.legend()","83224a82":"def load_checkpoint(filepath):\n    checkpoint = torch.load(filepath)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['state_dict'])\n    for parameter in model.parameters():\n        parameter.requires_grad = False\n    \n    return model.eval()\n\n\nfilepath = models_dir + str(best_epoch) + \".pth\"\nloaded_model = load_checkpoint(filepath)\n\ntrain_transforms = transforms.Compose([\n                                       transforms.Resize((224,224)),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n                                       ])","adb6a68e":"correct = 0\ntotal = 0\n    \nwith torch.no_grad():\n    for X, y in test_loader:\n\n        X, y = X.cuda(), y.cuda()\n\n        result = loaded_model(X)\n        _, maximum = torch.max(result.data, 1)\n        total += y.size(0)\n        correct += (maximum == y).sum().item()\n\naccuracy = correct\/total\n\nprint(\"\\n\")\nprint(\"------------\")\nprint(\"Accuracy: \" + str(accuracy))\nprint(\"------------\")\nprint(\"\\n\")","21312b9f":"!pip install cvlib\nimport cvlib as cv\nfrom PIL import Image\n\ncap = cv2.VideoCapture(0)\n\nfont_scale=1\nthickness = 2\nred = (0,0,255)\ngreen = (0,255,0)\nblue = (255,0,0)\nfont=cv2.FONT_HERSHEY_SIMPLEX\n\n#File must be downloaded\nface_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n\n\nwhile(cap.isOpened()):\n    ret, frame = cap.read()\n    if ret == True:\n\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        faces = face_cascade.detectMultiScale(gray, 1.4, 4)\n        \n        for (x, y, w, h) in faces:\n            \n            cv2.rectangle(frame, (x, y), (x+w, y+h), blue, 2)\n            \n            croped_img = frame[y:y+h, x:x+w]\n            pil_image = Image.fromarray(croped_img, mode = \"RGB\")\n            pil_image = train_transforms(pil_image)\n            image = pil_image.unsqueeze(0)\n            \n            \n            result = loaded_model(image)\n            _, maximum = torch.max(result.data, 1)\n            prediction = maximum.item()\n\n            \n            if prediction == 0:\n                cv2.putText(frame, \"Masked\", (x,y - 10), font, font_scale, green, thickness)\n                cv2.rectangle(frame, (x, y), (x+w, y+h), green, 2)\n            elif prediction == 1:\n                cv2.putText(frame, \"No Mask\", (x,y - 10), font, font_scale, red, thickness)\n                cv2.rectangle(frame, (x, y), (x+w, y+h), red, 2)\n        \n        cv2.imshow('frame',frame)\n        \n        if (cv2.waitKey(1) & 0xFF) == ord('q'):\n            break\n    else:\n        break\n\ncap.release()\ncv2.destroyAllWindows()","f13a3deb":"# Training and Validation","f39be6da":"# Real Time Medical Mask Detection","f3aee7d5":"# Loss and Optimizer Functions","882af79d":"# Create Necessary Directories","10c29cb3":"# Demonstration\n\n### https:\/\/gifyu.com\/image\/lCJ0","6c53960c":"# Loading the Existing Model","eac7a7de":"# Initialize Fully Connected Layers","da5d1f34":"# Write Cropped Images","ee2de075":"# Shuffle and Split the Data","4ef04ed3":"# Pretrained Resnet50 Model","62680b22":"# Detection\nRun the following code block on an actual computer","930f3a4e":"# Initialize Loaders","7acf4239":"# Test","1e76c5f3":"# Visualization of Training and Validation Losses"}}