{"cell_type":{"83c17fdb":"code","1eadb06b":"code","a934af18":"markdown"},"source":{"83c17fdb":"#MODEL_TEST file\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns  # visualization tool\n\n\n        \nData=pd.read_csv(\"..\/input\/ce-475\/ce_475.csv\") \n\n#understanding data\nData.head()\nData.tail()\nData.columns\nData.info()\nData.corr()\nf,ax = plt.subplots(figsize=(9, 9))\nsns.heatmap(Data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.show()\n\n\n\ncolumn = ['x1','x2','x3','x4','x5','x6','Y']\ndata = Data[column]\n\nX = Data.iloc[0:100,1:7].values\ny = Data.iloc[0:100,7:8].values\n\nx1 = X[:,0]\nx2 = X[:,1]\nx3 = X[:,2]\nx4 = X[:,3]\nx5 = X[:,4]\nx6 = X[:,5]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\nX_trainOpt = X_train[:,[0,1,2,3,5]]\nX_testOpt = X_test[:,[0,1,2,3,5]]\n\n# -------------------- Linear Regression --------------------\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train,y_train)\ny_pred = regressor.predict(X_test)\nprint(\"Multi Linear predic\")\nprint(y_pred)\n\n\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10)\naccuracies.mean()\naccuracies.std()\nprint(accuracies)\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nmse = mean_squared_error(y_test,y_pred)\nrmse = sqrt(mean_squared_error(y_test, y_pred))\nprint(\"mean\")\nprint(accuracies.mean())\nprint(\"standard deviation\")\nprint(accuracies.std())\nprint(\"mean_squared_error\")\nprint(mse)\nprint(\"R mean_squared_error\")\nprint(rmse)\n\n\n# -------------------- Linear Regression --------------------\n\n# -------------------- Linear Regression (backward)--------------------\n\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_trainOpt,y_train)\ny_pred = regressor.predict(X_testOpt)\nprint(\"Multi Linear predic with backward   \")\nprint(y_pred)\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10)\naccuracies.mean()\naccuracies.std()\nprint(accuracies)\n\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nmse = mean_squared_error(y_test,y_pred)\nrmse = sqrt(mean_squared_error(y_test, y_pred))\nprint(\"mean\")\nprint(accuracies.mean())\nprint(\"standard deviation\")\nprint(accuracies.std())\nprint(\"mean_squared_error\")\nprint(mse)\nprint(\"R mean_squared_error\")\nprint(rmse)\n# -------------------- Linear Regression(backward) --------------------\n\n\n# -------------------- Decision Tree ----------------\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state = 0)\nregressor.fit(X_train,y_train)\ny_pred = regressor.predict(X_test)\nprint(\"Decision Tree Regression\")\nprint(y_pred)\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10)\naccuracies.mean()\nprint(accuracies)\naccuracies.std()\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nmse = mean_squared_error(y_test,y_pred)\nrmse = sqrt(mean_squared_error(y_test, y_pred))\nprint(\"mean\")\nprint(accuracies.mean())\nprint(\"standard deviation\")\nprint(accuracies.std())\nprint(\"mean_squared_error\")\nprint(mse)\nprint(\"R mean_squared_error\")\nprint(rmse)\n# -------------------- Decision Tree ----------------\n\n\n\n# -------------------- \u200aPolynomial Regression ----------------\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree = 4)\nX_poly = poly_reg.fit_transform(X_train)\nlin_reg = LinearRegression()\nlin_reg.fit(X_poly,y_train)\ny_pred = lin_reg.predict(poly_reg.fit_transform(X_test))\nprint(\"Polynomial Regression\")\nprint(y_pred)\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10)\naccuracies.mean()\naccuracies.std()\nprint(accuracies)\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nmse = mean_squared_error(y_test,y_pred)\nrmse = sqrt(mean_squared_error(y_test, y_pred))\nprint(\"mean\")\nprint(accuracies.mean())\nprint(\"standard deviation\")\nprint(accuracies.std())\nprint(\"mean_squared_error\")\nprint(mse)\nprint(\"R mean_squared_error\")\nprint(rmse)\n\n # -------------------- \u200aPolynomial Regression ----------------\n\n\n# -------------------- Random Forest ----------------\n\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 10000, criterion = \"mse\",max_features = \"auto\",random_state = 0)\nregressor.fit(X_train,y_train)\ny_pred = regressor.predict(X_test)\nprint(\"Random Forest Regressor\")\nprint(y_pred)\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10)\naccuracies.mean()\naccuracies.std()\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nmse = mean_squared_error(y_test,y_pred)\nrmse = sqrt(mean_squared_error(y_test, y_pred))\nprint(accuracies)\nprint(\"mean\")\nprint(accuracies.mean())\nprint(\"standard deviation\")\nprint(accuracies.std())\nprint(\"mean_squared_error\")\nprint(mse)\nprint(\"R mean_squared_error\")\nprint(rmse)\n# -------------------- Random Forest ----------------\n\n# -------------------- Lasso Regression ----------------\n\nfrom sklearn.linear_model import Lasso\nregressor = Lasso()\nregressor.fit(X_train,y_train)\ny_pred = regressor.predict(X_test)\nprint(\"Lasso\")\nprint(y_pred )\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv = 10)\naccuracies.mean()\naccuracies.std()\nprint(accuracies)\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nmse = mean_squared_error(y_test,y_pred)\nrmse = sqrt(mean_squared_error(y_test, y_pred))\nprint(\"mean\")\nprint(accuracies.mean())\nprint(\"standard deviation\")\nprint(accuracies.std())\nprint(\"mean_squared_error\")\nprint(mse)\nprint(\"R mean_squared_error\")\nprint(rmse)\n\n# -------------------- Lasso Regression ----------------\n\n# -------------------- xgboost Regression(bonus) ----------------\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\n\nclassifer = XGBClassifier()\nclassifer.fit(X_train,y_train)\ny_pred = classifer.predict(X_test)\n\n\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifer, X = X_train, y = y_train, cv = 3)\naccuracies.mean()\naccuracies.std()\n\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nmse = mean_squared_error(y_test,y_pred)\nrmse = sqrt(mean_squared_error(y_test, y_pred))\nprint(\"mean\")\nprint(accuracies.mean())\nprint(\"standard deviation\")\nprint(accuracies.std())\nprint(\"mean_squared_error\")\nprint(mse)\nprint(\"R mean_squared_error\")\nprint(rmse)\n\nxgb = XGBClassifier()\nxgb_params = {\"n_estimators\":[100,500,1000],\n              \"subsample\":[0.6,0.8,1],\n              \"max_depth\":[3,5,7],\n              \"learning_rate\":[0.1,0.001,0.01]}\n\n\nxgb_cv_model = GridSearchCV(xgb,xgb_params, cv = 2,\n                            n_jobs=-1, verbose = 2).fit(X_train,y_train)\n\n\nprint(xgb_cv_model.best_params_)\n\nxgb_tuned = xgb = XGBClassifier(learning_rate = 0.1,\n                                max_depth = 3, \n                                n_estimators = 500, \n                                subsample =0.6 ).fit(X_train,y_train)\ny_pred = xgb_tuned.predict(X_test)\n\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifer, X = X_train, y = y_train, cv = 3)\naccuracies.mean()\naccuracies.std()\n\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nmse = mean_squared_error(y_test,y_pred)\nrmse = sqrt(mean_squared_error(y_test, y_pred))\nprint(\"mean\")\nprint(accuracies.mean())\nprint(\"standard deviation\")\nprint(accuracies.std())\nprint(\"mean_squared_error\")\nprint(mse)\nprint(\"R mean_squared_error\")\nprint(rmse)\n\n#GRAPH\nimport matplotlib.pyplot as plt\n\nplt.subplot(231)\nplt.scatter(x1,y,s=180,marker = '.')\nplt.title(\"x1-y\")\nplt.show()\n\nplt.subplot(232)\nplt.scatter(x2,y,s=180,marker = '.')\nplt.title(\"x2-y\")\nplt.show()\n\nplt.subplot(233)\nplt.scatter(x3,y,s=180,marker = '.')\nplt.title(\"x3-y\")\nplt.show()\n\nplt.subplot(234)\nplt.scatter(x4,y,s=180,marker = '.')\nplt.title(\"x4-y\")\nplt.show()\n\nplt.subplot(235)\nplt.scatter(x5,y,s=180,marker = '.')\nplt.title(\"x5-y\")\nplt.show()\n\nplt.subplot(236)\nplt.scatter(x6,y,s=180,marker = '.')\nplt.title(\"x6-y\")\nplt.show()\n","1eadb06b":"#main_file\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\nData = pd.read_csv(\"..\/input\/ce-475\/ce_475.csv\")\ncolumn = ['x1','x2','x3','x4','x5','x6','Y']\ndata = Data[column]\n\nX = data.values[:,:-1]\ny = data.values[:,6]\nx_train = data.values[:100,:6]\ny_train = data.values[:100,6:7]\nx_test = data.values[100:,:6]\n\n# -------------------- Linear Regression --------------------\n\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(x_train,y_train)\ny_predMLR = regressor.predict(x_test)\nprint(\"Multi Linear\")\nprint(y_predMLR)\n\n# -------------------- Linear Regression --------------------\n\n#elimination for backward\nimport statsmodels.api as sm\n\nX_trainOPT = np.append(arr = np.ones((100,1)).astype(int), values = x_train, axis = 1)\nx_opt = X_trainOPT[:,[0,1,2,3,4,5,6]]\nregressor_OLS = sm.OLS(endog = y_train, exog = x_opt).fit()\nregressor_OLS.summary()\n\nprint(regressor_OLS.summary())\n\nX_trainOPT = np.append(arr = np.ones((100,1)).astype(int), values = x_train, axis = 1)\nx_opt = X_trainOPT[:,[0,1,2,3,5,6]]\nregressor_OLS = sm.OLS(endog = y_train, exog = x_opt).fit()\nregressor_OLS.summary()\nprint(regressor_OLS.summary())\n\nX_trainOPT = np.append(arr = np.ones((100,1)).astype(int), values = x_train, axis = 1)\nx_opt = X_trainOPT[:,[0,1,2,3,5]]\nregressor_OLS = sm.OLS(endog = y_train, exog = x_opt).fit()\nregressor_OLS.summary()\nprint(regressor_OLS.summary())\n#stay the same\n\n\n# -------------------- Linear Regression(backward) --------------------\n\nfrom sklearn.linear_model import LinearRegression\nx_opt2 = X_trainOPT[:,[1,2,3,6]] #x4 and x6 have large p-value\nX_testOpt = x_test[:,[0,1,2,5]]\nreg = LinearRegression()\nreg.fit(x_opt2,y_train) \ny_predBML = reg.predict(X_testOpt)\nprint(\"Multi linear with backward\")\nprint(y_predBML)\n# -------------------- Linear Regression (backward)--------------------\n\n# -------------------- Lasso Regression --------------------\n\nfrom sklearn.linear_model import Lasso\nregressor = Lasso(alpha=0.1)\nregressor.fit(x_train,y_train)\ny_predLasso = regressor.predict(x_test)\nprint(\"Lasso\")\nprint(y_predLasso)\n# -------------------- Lasso Regression --------------------\n\n# -------------------- \u200aPolynomial Regression ----------------\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree = 4)\nX_poly = poly_reg.fit_transform(x_train)\nlin_reg = LinearRegression()\nlin_reg.fit(X_poly,y_train)\ny_predPReg = lin_reg.predict(poly_reg.fit_transform(x_test))\nprint(\"PolynomialFeatures\")\nprint(y_predPReg)\n# -------------------- \u200aPolynomial Regression ----------------\n  \n# -------------------- Decision Tree ----------------\n\nfrom sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state = 0)\nregressor.fit(x_train,y_train)\ny_predDT = regressor.predict(x_test)\nprint(\"DecisionTreeRegressor\")\nprint(y_predDT)\n# -------------------- Decision Tree ----------------\n\n# -------------------- Random Forest ----------------\n\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 10000, criterion = \"mse\",max_features = \"auto\",random_state = 0)\nregressor.fit(x_train,y_train)\ny_predRFR = regressor.predict(x_test)\nprint(\"RandomForestRegressor\")\nprint(y_predRFR)\n# -------------------- Random Forest ----------------\n\n\n# -------------------- xgboost Regression(bonus) ----------------\nfrom xgboost import XGBClassifier\n\nclassifer = XGBClassifier(learning_rate = 0.1,\n                                max_depth = 3, \n                                n_estimators = 500, \n                                subsample =0.6 ).fit(x_train,y_train)\n\ny_predxg = classifer.predict(x_test)\nprint(\"xgboost_predict\")\nprint(y_predxg)\n\n\n# -------------------- xgboost Regression(bonus) ----------------\n\n","a934af18":"# CONTENT: <br>\n***\n1. [Overview](#Overview)<br>\n2. [Strategy](#Strategy)<br>\n3. [Data Understanding](#Data_Understanding) <br>\n4. [Libraries](#Libraries) <br>\n5. [Model Decision](#Model_Decision)<br>\n6. [Model Development](#Model-Development) <br>\n7. [Comparison of Models](#Comparison_of_Models) <br>\n8. [Estimates of Models](#Estimates_of_Models) <br>\n9. [Conclusion](#Conclusion) <br> \n10. [Source Code](#Source_Code) <br>\n\n\n\n<h1>Overview<\/h1>\n\n<p>If we look at the overall purpose of the project, we are asked to use the basics of machine learning and apply their algorithms. This project is an example of supervised machine learning. We need to create models according to the sample data given. After selecting the model that makes the best prediction among these models, we will estimate the expected data. If we examine this data, it consists of 120 lines. Of these, 100 are complete data, but 20 are missing the expected values. Data has 6 properties. There is a y value obtained at the end of the 6 properties.<\/p>\n\n<h1>Strategy<\/h1>\n\n<p>The most important thing in Supervised Machine learning is data. It is very important that we understand the data correctly and choose the appropriate model for it. My first step in this project was to understand the data. My second step was to determine what kind of strategy I would make based on the data I studied. I wrote two different codes for this project. The first and first code helped me to choose the model. Two different reasons for writing code, data. I used 100 complete data in my model selection and model testing program. So I compared the values I predicted with their actual values. After I finished selecting the Model, I wrote the main coding program. Here I trained the model with the first hundred data and estimated the y values with the next 20 data.<\/p>\n\n<h1>Data Understanding<\/h1>\n\n<p>In the first phase of studying and understanding data, we will use Python programming language and libraries in general.<\/p>\n\n1. First we load the data and run the first code, \"data.head() \" function. This function shows us the first rows of the data. This allows us to see the properties of the data.\n![1](https:\/\/imagizer.imageshack.com\/img924\/9356\/R8iTXy.png)\n1. The Tail function shows the end of the data as opposed to the head function. As you can see here, there are differences with head. The missing data gave us a good information about the data.\n![2](https:\/\/imageshack.com\/i\/polpvqexp)\n1. With the Columns function, we can easily see how many features there are.\n![3](https:\/\/imageshack.com\/i\/pnivtUaHp)\n\n1. The Info function is actually sufficient for us alone. Gives a wide range of information about Data. We can see how many columns, rows and missing data there are. We can also see the types of data.\n![4](https:\/\/imageshack.com\/i\/poB4rLozp)\n\n1. The correlation matrix is one of the important pre-processing stages to be used when deciding on data, and the relationship between this matrix and the columns can be seen.\n![5](https:\/\/imageshack.com\/i\/poYIF4zmp)\n\n1. We're finally doing the visualization in the data review. We visualize correlation matrices through the Seaborn library. In this way, we can see the relationship of the columns with each other more easily.\n![6](https:\/\/imageshack.com\/i\/pmhzK5Thp)\n\n<h1>Libraries<\/h1>\n<p>\n After reviewing the data, we started to learn about which models and which libraries to use. While we can think of some libraries in advance, there is a possibility that we can add some of them in the instant of coding. Here, I'll brief you on all the libraries I've used in my code.<br>\n\n\u25cf numpy: The Numpy library is a python library written to facilitate scientific computing. While a machine learning, image processing and artificial intelligence work, computational operations are frequently performed.<br>\n\u25cf pandas: Data science projects begin with the discovery and clearing of data, and these processes are the most time-consuming parts of projects.<br>\n\u25cf warnings: We called this library to turn off the alerts.<br>\n\u25cf sklearn.linear_model \u2192 LinearRegression:  We used this library for the Linear regression model.<br>\n\u25cf statsmodels.api: We called this library to use the ols method. The Ols method shows us the p values of X values. So we can get better results by subtracting these x values. The ols method also shows us the error results of the model we use.<br>\n\u25cf sklearn.linear_model \u2192 Lasso: We used this library for the lasso regression model.<br>\n\u25cf sklearn.preprocessing \u2192 PolynomialFeatures: We used this library for the Polynomial Regression model.<br>\n\u25cf sklearn.tree\u2192 DecisionTreeRegressor: We used this library for the Decision Tree Regression model.<br>\n\u25cf sklearn.ensemble\u2192 RandomForestRegressor: We used this library for the Random Forest Regression model.<br>\n\u25cf xgboost\u2192 XGBClassifier: We used this library for the Xgboost Regression model.<br>\n\u25cf sklearn.model_selection \u2192 train_test_split: I used this library to separate the data from the model test file as train and test.<br>\n\u25cf sklearn.metrics \u2192 mean_squared_error: I used this library to find a Mean squared error.<br>\n\u25cf math \u2192 sqrt : I used this library to find Sqrt errors.<br>\n<\/p>\n\n<h1>Model Decision<\/h1>\n\n<p>\nI used 5 models in the project, I tried a newly developed model as an extra:\n \n<h3> 1. Linear Regression <\/h3> \n  \nIt is formulized in the form of Yi = \u03b20 + \u03b21Xi. Here the goal is to find the \u03b2 coefficients that will produce results closest to the Y value.\n\n<h3>2. Lasso Regression<\/h3>\n\n    \nLasso regression (Least Absolute Shrinkage and Selection Operator Regression) is\nanother regulated variant of linear regression:   is added to our cost function as the editing term. An important characteristic of Lasso regression is that it excludes the weights of the least important attributes. In other words, Lasso regression automatically applies \u201cattribute selection\u201d and gives a discrete model as output .\n\n<h3>3. Polynomial Regression<\/h3>\n\n    \nIt is a method of analysis that is used when the relationship between variables is not linear. \u00df0 + \u00df1 X1 + \u00df2 X22+.... + \u00dfi Xni. Our goal is to find the \u03b2 coefficients again.\n\n<h3>4. Decision Tree Regression<\/h3>\n\n    \nWe can divide decision trees by classification and regression. The decision tree regression does the following in summary: It divides the arguments into ranges according to information gain. When asked for a value from this range during prediction, he says the average in this range in response. For this reason, decision tree regression is discrete, not continuous, like other regression models. That is, it produces the same results for the desired estimates in a certain range.\n\n<h3> 5. Random Forest Regression <\/h3>\n\n  \nRandom Forest is one of the popular machine learning models because it can be applied to both regression and classification problems, providing good results without hyper-parameter estimation.\n\n<h3> 6. Xgboost regression   <\/h3>\n\n    \nI wanted to put this model in your project as an extra. The above 5 models are classic machine learning models. Xgboost is a new and popular model. XGBoost is a decision-tree based and gradient-boosting ML system. Gradient boost combines bagging-bootstrap aggregation and feature randomness to prevent overfitting, while bias-variation provides trade-offs.\n    \n<\/p>\n\n\n<h1>Model Development<\/h1>\n\n\n\nWe applied enhancement techniques to get better results in the models we applied. These are like the following;\n    \n<h3>K Fold Cross Validation:<\/h3>\n    \nCross-validation is a technique used in model selection to better predict the error of the test performed in a machine learning model. The idea behind cross-validation is to create sample observation sections, known as validation sets, from the training data set. After placing a model in training data, its performance is measured against each new set of validation, and then a better assessment of how the model will perform is obtained when it is desired to predict new observations. The number of episodes to be made depends on the number of observations in the sample dataset and the decision to balance the bias variance, with more divisions leading to a smaller fallacy and varying depending on the greater variance.\n    \n<h3>Backward Elimination:<\/h3>\n    \n Each variable has an effect on the system. Some variables have a high impact on the system, while others have a low impact. The elimination of arguments that have little impact on the system allows us to build a better model. We can create better models using the reverse Elimination Method.\n \n![bacward1](https:\/\/imageshack.com\/i\/pm1c4HyYp) \n![backward2](https:\/\/imageshack.com\/i\/pmMzIGlMp)\n![backward3](https:\/\/imageshack.com\/i\/pnt5jOnHp)\n\n    \nThe p value values of x4 and x6 are high according to the above results. Properties with a value of p value above 0.05 are eliminated because they affect our model badly.\n    \n<h3>Grid Search CV:<\/h3>\n\nThe most appropriate parameters for the model are determined and the GridSearchCV method is used for this. This method uses the parameters that give the best results by trying the ranges that we specify for the individual parameters.\n\n\n\n<h1>Comparison of Models<\/h1>\n\n\n\nAll models have been tested in the model test file and these models have been tested according to some rules. These are respectively as follows.\n1. Mean value\n2. Standard deviation\n3. Mean squared error \n4. R mean squared error\nhowever, the value we consider is R mean squared error.\n\n ![](https:\/\/imageshack.com\/i\/po6whtyIp)\n\nThe above table shows the error values of the models we applied. This difference varies with the data's compatibility with the models. According to the above values, our model selection will be in the following order.\n1. Random forest regression\n2. Decision tree regression\n3. Linear regression\n4. Lasso regression\n5. Xgboost regression\n6. Polynomial regression\n\n\n\n<h1>Estimates of Models<\/h1>\n\n\nEstimates of Models\nBelow is the missing data that the project wants us to estimate. The data for all models are estimated and written, but the closest estimate should be made according to the reliability ranking we found in the model comparison above.\n    \n![](https:\/\/imageshack.com\/i\/poXcdetMp)\n\n<h1>Conclusion<\/h1>\n\n\n<p>\nFirst of all, by seeing the data in the project, I realized how to understand whether this is supervised or unsupervised. I learned to choose which models to use in this project, which I learned was supervised. I understood how the models worked and decided whether this way of working was appropriate to my efficiency and I was able to make the models better. After making the models I learned how to choose, during this selection I figured out which features to choose. Based on these characteristics, I was in order to guess. The most difficult part of the project was the strategy. Although I had difficulty in separating the data at first, I made progress in the project by applying the models we learned in the laboratory course. In addition, I learned new models and methods from the internet and tried to apply them in the project. <\/p>\n\n<h1>Source code<\/h1>\n"}}