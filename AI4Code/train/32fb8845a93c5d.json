{"cell_type":{"23cc023b":"code","d08cc6a7":"code","23ae8e8d":"code","45d77ae4":"code","8707b110":"code","f03e0c99":"code","3a43491f":"code","9d3adbcd":"code","116bad9e":"code","9029885e":"code","9958bd2a":"code","ac506c06":"code","5d9f93a9":"code","78b6034f":"code","a2c232cc":"code","1cac51d1":"code","66cfda64":"code","da2eb427":"code","be94ece9":"code","ea348aaf":"code","a05aa1bf":"code","e70bdb4f":"code","b42507bf":"code","86c267f6":"code","1c719362":"code","067575b2":"code","34302c11":"code","952328d5":"code","f60c81ba":"code","f6d1380c":"code","2919cafa":"code","4170292c":"code","3cf1302b":"code","ff46ae9b":"code","b04f30f4":"code","113e00a5":"code","e3efcdc8":"code","b994141d":"code","da7c1651":"markdown","14941027":"markdown","470f16aa":"markdown","a787c441":"markdown","f2124ab5":"markdown","41c0b008":"markdown","44867c59":"markdown","db1defda":"markdown","afdd4f5b":"markdown","89834342":"markdown","bce5a5ab":"markdown","a7aaeb6f":"markdown","c52c994f":"markdown","00b60037":"markdown","19f59a62":"markdown","4105f1a3":"markdown","afb5a100":"markdown","c8d7956c":"markdown","d918c7c2":"markdown","6964252a":"markdown","c404b4e8":"markdown","4bd874e9":"markdown","23833551":"markdown","2eca626b":"markdown"},"source":{"23cc023b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.offline as py\nfrom plotly import tools, subplots\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d08cc6a7":"df = pd.read_excel('\/kaggle\/input\/covid19\/Kaggle_Sirio_Libanes_ICU_Prediction.xlsx')\ndf.head()","23ae8e8d":"# Data processing, metrics and modeling\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom datetime import datetime\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\nfrom sklearn import metrics\n# Lgbm\nimport lightgbm as lgb\nimport catboost\nfrom catboost import Pool\nimport xgboost as xgb\n\n# Suppr warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")","45d77ae4":"# Find correlations with the target and sort\ncorrelations = df.corr()['ICU'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","8707b110":"%%time\ncorrs = df.corr()\nplt.figure(figsize = (10, 6))\n# Heatmap of correlations\nsns.heatmap(corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = False, vmax = 0.8)\nplt.title('Clustermap');","f03e0c99":"#fill in mean for floats\nfor c in df.columns:\n    if df[c].dtype=='float16' or  df[c].dtype=='float32' or  df[c].dtype=='float64':\n        df[c].fillna(df[c].mean())\n\n#fill in -999 for categoricals\ndf = df.fillna(-999)\n# Label Encoding\nfor f in df.columns:\n    if df[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(df[f].values))\n        df[f] = lbl.transform(list(df[f].values))\n        \nprint('Labelling done.')","3a43491f":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)","9d3adbcd":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","116bad9e":"from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","9029885e":"from sklearn.kernel_ridge import KernelRidge\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","9958bd2a":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","ac506c06":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\n#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df.values)\n    rmse= np.sqrt(-cross_val_score(model, df.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","5d9f93a9":"#ntrain = train.shape[0]\n#ntest = test.shape[0]\ny_train = df.ICU.values\n#all_data = pd.concat((train, test)).reset_index(drop=True)\n#all_data.drop(['SalePrice'], axis=1, inplace=True)\n#print(\"all_data size is : {}\".format(all_data.shape))","78b6034f":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","a2c232cc":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","1cac51d1":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","66cfda64":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","da2eb427":"from sklearn.metrics import mean_squared_error\nstacked_averaged_models.fit(df.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(df.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(df.values))\nprint(rmsle(y_train, stacked_train_pred))","be94ece9":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","ea348aaf":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","a05aa1bf":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","e70bdb4f":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","b42507bf":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","86c267f6":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","1c719362":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)","067575b2":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","34302c11":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","952328d5":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","f60c81ba":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","f6d1380c":"stacked_averaged_models.fit(df.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(df.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(df.values))\nprint(rmsle(y_train, stacked_train_pred))","2919cafa":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","4170292c":"import xgboost as xgb\nmodel_xgb.fit(df, y_train)\nxgb_train_pred = model_xgb.predict(df)\nxgb_pred = np.expm1(model_xgb.predict(df))\nprint(rmsle(y_train, xgb_train_pred))","3cf1302b":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","ff46ae9b":"import lightgbm as lgb\nmodel_lgb.fit(df, y_train)\nlgb_train_pred = model_lgb.predict(df)\nlgb_pred = np.expm1(model_lgb.predict(df.values))\nprint(rmsle(y_train, lgb_train_pred))","b04f30f4":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","113e00a5":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","e3efcdc8":"#train_PATIENT_VISIT_IDENTIFIER = train['PATIENT_VISIT_IDENTIFIER']\ntest_PATIENT_VISIT_IDENTIFIER = df['PATIENT_VISIT_IDENTIFIER']","b994141d":"sub = pd.DataFrame()\nsub['PATIENT_VISIT_IDENTIFIER'] = test_PATIENT_VISIT_IDENTIFIER\nsub['ICU'] = ensemble\nsub.to_csv('submission.csv',index=False)","da7c1651":"Define a rmsle evaluation function","14941027":"It is thought that age and underlying chronic conditions (or comorbidities) increase the risk of severe COVID-19, but the differential effect of various comorbidities remains unclear, and current guidance is largely based on expert consensus rather than observational data.\n\nSince many patients admitted to ICU with COVID-19 survive, predictors for death may differ from those for ICU admission.\n\nExclusion criteria included: studies of exclusively paediatric or pregnant patients, due to the varying presentation of COVID-19 in these groups, insufficient data on symptoms\/comorbidities on admission in either severe or non-severe disease groups (or ICU and non-ICU groups), coronavirus strains other than COVID-19 and studies not written in English, because of practical limitations with translation.\n\nTo allow comparability between studies for meta-analysis, these were grouped into a single predictor (cardiovascular disease). Smoking was not included in meta-analyses. Gender was compared between groups using the Chi-square test in STATA. Random effects models were used to account for between study heterogeneity, which was estimated with Tau-squared. This provided a pooled odds ratio (pOR), 95% confidence intervals and a p value, for each symptom or comorbidity. A p value of\u2009<\u20090.05 was used as a marker for evidence of significant association.https:\/\/link.springer.com\/article\/10.1007\/s00038-020-01390-7","470f16aa":"#Stacking Averaged models Score","a787c441":"#Final Training and Prediction","f2124ab5":"![](https:\/\/media.springernature.com\/lw685\/springer-static\/image\/art%3A10.1007%2Fs00038-020-01390-7\/MediaObjects\/38_2020_1390_Fig2_HTML.png?as=webp)\n\nJain, V., Yuan, J. Predictive symptoms and comorbidities for severe COVID-19 and intensive care unit admission: a systematic review and meta-analysis. Int J Public Health 65, 533\u2013546 (2020). https:\/\/doi.org\/10.1007\/s00038-020-01390-7\nhttps:\/\/link.springer.com\/article\/10.1007\/s00038-020-01390-7","41c0b008":"#Stacking averaged Models Class","44867c59":"#Label Encoding.\n\nIt assigns each unique value to a different integer.","db1defda":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcR931EBP_DZtjpmZrpKJME9Ccq4lTBZxyaCdg&usqp=CAU)pt-br.facebook.br","afdd4f5b":"#Codes from Serigne https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/notebook","89834342":"A total of seven symptoms were included in the model for severe disease and six for ICU admission, as well as four comorbidities in both. The most prevalent symptoms in the severe group were cough (70.5%), fever (64.1%) and fatigue (44.5%); in the ICU group these were cough (67.2%), fever (62.9%) and dyspnoea (61.2%). The most prevalent comorbidities in the severe group were hypertension (25.4%) and diabetes (16.8%) and in the ICU group were hypertension (40.5%) and cardiovascular disease (CVD) (24.1%).\n\nMen were 1.55 times more likely than women to be admitted to ICU (95% CI 1.02\u20132.36) but not at a significantly increased risk of being in the severe group. Dyspnoea was the only symptom significantly associated with both severe disease (pOR 3.70, 95% CI 1.83\u20137.46) and ICU admission (pOR 6.55, 95% CI 4.28\u201310.0). Cough was associated with severe disease (pOR 1.63, 95% CI 1.03\u20132.60), but not ICU admission. The remaining symptoms analysed were not associated with either outcome. Chronic obstructive pulmonary disease (COPD), CVD and hypertension were the comorbidities significantly predictive for both severe disease and ICU admission. The pORs for severe disease were as follows: COPD (6.42, 95% CI 2.44\u201316.9), CVD (2.70, 95% CI 1.52\u20134.80) and hypertension (1.97, 95% CI 1.40\u20132.77). COPD, CVD and hypertension were more strongly associated with ICU admission, compared with severe disease, with pORs of 17.8 (95% CI 6.56\u201348.2), 4.44 (95% CI 2.64\u20137.47), and 3.65 (95% CI 2.22\u20135.99), respectively. In contrast, diabetes was of borderline significance in predicting severe disease (pOR 3.12, 95% CI 1.0\u20139.75), but not ICU admission (pOR 2.72, 95% CI 0.70\u201310.6), although the Tau-squared value for the latter was unusually high implying a high level of heterogeneity. https:\/\/link.springer.com\/article\/10.1007\/s00038-020-01390-7","bce5a5ab":"#Base models scores","a7aaeb6f":"The snippet above takes a long time to run. ","c52c994f":"#Ensembling StackedRegressor, XGBoost and LightGBM","00b60037":"#Ensembling StackedRegressor, XGBoost and LightGBM","19f59a62":"#StackedRegressor:","4105f1a3":"The median age and gender of severe and non-severe disease, and ICU and non-ICU admitted, patients, after aggregating all studies. The median age was 62.4 years for ICU-admitted patients compared to 46 years for non-ICU patients, and 49.4 years for severe compared to 41.7 years for non-severe disease patients. https:\/\/link.springer.com\/article\/10.1007\/s00038-020-01390-7","afb5a100":"#Ensemble Prediction","c8d7956c":"Figure illustrates the process for selection of papers \n![](https:\/\/media.springernature.com\/lw685\/springer-static\/image\/art%3A10.1007%2Fs00038-020-01390-7\/MediaObjects\/38_2020_1390_Fig1_HTML.png?as=webp)\nJain, V., Yuan, J. Predictive symptoms and comorbidities for severe COVID-19 and intensive care unit admission: a systematic review and meta-analysis. Int J Public Health 65, 533\u2013546 (2020). https:\/\/doi.org\/10.1007\/s00038-020-01390-7","d918c7c2":"#Stacking models\n\nAveraged base models class","6964252a":"#Predictive symptoms and comorbidities for severe COVID-19 and ICU admission\n\nJain, V., Yuan, J. Predictive symptoms and comorbidities for severe COVID-19 and intensive care unit admission: a systematic review and meta-analysis. Int J Public Health 65, 533\u2013546 (2020). https:\/\/doi.org\/10.1007\/s00038-020-01390-7\n\nThis study identifies specific symptoms and comorbidities predicting severe COVID-19 and intensive care unit (ICU) admission.\n\n****Results****: Seven studies (including 1813 COVID-19 patients) were included. ICU patients were older (62.4 years) than non-ICU (46 years), with a greater proportion of males. Dyspnoea was the only symptom predictive for severe disease (pOR 3.70, 95% CI 1.83\u20137.46) and ICU admission (pOR 6.55, 95% CI 4.28\u201310.0). COPD was the strongest predictive comorbidity for severe disease (pOR 6.42, 95% CI 2.44\u201316.9) and ICU admission (pOR 17.8, 95% CI 6.56\u201348.2), followed by cardiovascular disease and hypertension\n\n****Conclusions****: Dyspnoea was the only symptom predictive for severe COVID-19 and ICU admission. Patients with COPD, cardiovascular disease and hypertension were at higher risk of severe illness and ICU admission.https:\/\/link.springer.com\/article\/10.1007\/s00038-020-01390-7","c404b4e8":"#Codes from  my friend Crisl\u00e2nio Macedo https:\/\/www.kaggle.com\/caesarlupum\/brazil-against-the-advance-of-covid-19\/notebook?select=h2oai-pystacknet-af571e0 ","4bd874e9":"#Implications for clinical practice\/public health\n\nBy identifying the symptoms and comorbidities predictive for severe disease and ICU admission, clinicians can better stratify the risk of individual patients, as early as their initial contact with health services\n\nThese can also be formalised within risk stratification tools to aid clinical decision-making, such as the CURB65 tool for community-acquired pneumonia. Anticipation of future demand, based on local population characteristics, may enable more timely planning and resource mobilisation.\n\nIdentifying those at high-risk will aid the public health response in controlling the spread of disease. Knowledge on the differing prevalence and risk of various conditions may help to focus and tailor public health efforts. For instance, for COPD, which is less common in the general population and very strongly associated with ICU admission, a more targeted and intensive health protection strategy may be warranted, compared to other conditions (such as hypertension) which are more difficult to target due to their higher prevalence in the general population.\n\nIn ICU-admitted patients, who represent the more severe end of the spectrum of clinical severity, the difference in effect sizes for COPD and the other included comorbidities was large, suggesting that COPD patients are particularly vulnerable to very severe (or critical) disease.Future research must build on these findings by investigating factors related to disease severity, including a wide range of comorbidities and the effect of various potential confounding factors. This will aid clinical assessment, risk stratification and resource allocation and allow public health interventions to be targeted at the most vulnerable. https:\/\/link.springer.com\/article\/10.1007\/s00038-020-01390-7","23833551":"Das War's Kaggle Notebook Runner: Mar\u00edlia Prata  @mpwolke ","2eca626b":"Key findings: The most prevalent conditions in the severe group were hypertension and diabetes and in the ICU group were hypertension and CVD. Males were not at an increased risk of severe disease, but 1.55 times more likely to have an ICU admission compared to females. Dyspnoea was the only symptom significantly associated with disease severity and ICU admission, alongside various comorbidities (COPD, CVD and hypertension). All of these factors were more strongly associated with ICU admission than disease severity. Patients with dyspnoea were 6.6 times more likely to have an ICU admission compared to those without dyspnoea. Although COPD was relatively uncommon, even in ICU patients, it was by far the most strongly predictive comorbidity for ICU admission. Those with CVD and hypertension were 4.4 and 3.7 times more likely to have an ICU admission, respectively, compared to patients without the comorbidity.\n\nThe studies included in that paper were all from China, so the generalisability of findings to other countries and populations is not clear. The Chinese may differ to other populations in terms of their health-seeking behaviour, symptom reporting, prevalence of different comorbidities, as well as their access to high-quality health services. https:\/\/link.springer.com\/article\/10.1007\/s00038-020-01390-7"}}