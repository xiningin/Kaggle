{"cell_type":{"b2dc0eaf":"code","98fbe706":"code","df233e98":"code","32486762":"code","910419ae":"code","0812b5a2":"code","3293ac44":"code","55422ddf":"markdown","e5fca9d3":"markdown","b0badc82":"markdown","d29767ad":"markdown","2c996f1c":"markdown","e7a15402":"markdown","d8b7a034":"markdown"},"source":{"b2dc0eaf":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","98fbe706":"import os\n\nimport dataset\nimport engine\nimport torch\nimport transformers\nimport warnings\n\nimport pandas as pd\nimport numpy as np\nimport torch.nn as nn\n\nfrom model import JigsawModel\n\nfrom sklearn import metrics\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nwarnings.filterwarnings(\"ignore\")","df233e98":"class config:\n    MAX_LEN = 192\n    TRAIN_BATCH_SIZE = 64\n    VALID_BATCH_SIZE = 4\n    EPOCHS = 1\n    LEARNING_RATE = 0.5e-5\n    BERT_PATH = \"..\/input\/bert-base-multilingual-uncased\/\"\n    MODEL_PATH = \"model.bin\"\n    TOKENIZER = transformers.BertTokenizer.from_pretrained(\n        BERT_PATH,\n        do_lower_case=True\n    )\n    JIGSAW_DATA_PATH = \"..\/input\/jigsaw-multilingual-toxic-comment-classification\/\"\n    TRAINING_FILE_1 = os.path.join(\n        JIGSAW_DATA_PATH, \n        \"jigsaw-toxic-comment-train.csv\"\n    )\n    TRAINING_FILE_2 = os.path.join(\n        JIGSAW_DATA_PATH, \n        \"jigsaw-unintended-bias-train.csv\"\n    )\n    VALIDATION_FILE = os.path.join(\n        JIGSAW_DATA_PATH, \n        \"validation.csv\"\n    )","32486762":"MX = JigsawModel(config.BERT_PATH)\n\ndf_train1 = pd.read_csv(\n    config.TRAINING_FILE_1, \n    usecols=[\"comment_text\", \"toxic\"]\n).fillna(\"none\")\n\ndf_train2 = pd.read_csv(\n    config.TRAINING_FILE_2, \n    usecols=[\"comment_text\", \"toxic\"]\n).fillna(\"none\")\n\ndf_valid = pd.read_csv(config.VALIDATION_FILE)\n\ndf_train = pd.concat([df_train1, df_train2], axis=0).reset_index(drop=True)\ndf_train = df_train.sample(frac=1).reset_index(drop=True).head(200000)\n\ndf_train = df_train.reset_index(drop=True)\ndf_valid = df_valid.reset_index(drop=True)\n\ntrain_targets = df_train.toxic.values\nvalid_targets = df_valid.toxic.values","910419ae":"def run():\n    train_dataset = dataset.JigsawTraining(\n        comment_text=df_train.comment_text.values,\n        targets=train_targets,\n        config=config\n    )\n\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n          train_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=True)\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config.TRAIN_BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=2\n    )\n\n    valid_dataset = dataset.JigsawTraining(\n        comment_text=df_valid.comment_text.values,\n        targets=valid_targets,\n        config=config\n    )\n\n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n          valid_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=False)\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=config.VALID_BATCH_SIZE,\n        sampler=valid_sampler,\n        drop_last=False,\n        num_workers=1\n    )\n\n    device = xm.xla_device()\n    model = MX.to(device)\n    \n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {\n            'params': [\n                p for n, p in param_optimizer if not any(\n                    nd in n for nd in no_decay\n                )\n            ], \n            'weight_decay': 0.001\n        },\n        {\n            'params': [\n                p for n, p in param_optimizer if any(\n                    nd in n for nd in no_decay\n                )\n            ],\n            'weight_decay': 0.0\n        },\n    ]\n\n    num_train_steps = int(\n        len(df_train) \/ config.TRAIN_BATCH_SIZE \/ xm.xrt_world_size() * config.EPOCHS\n    )\n    optimizer = AdamW(\n        optimizer_parameters, \n        lr=config.LEARNING_RATE * xm.xrt_world_size()\n    )\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_train_steps\n    )\n\n    best_auc = 0\n    for epoch in range(config.EPOCHS):\n        para_loader = pl.ParallelLoader(train_data_loader, [device])\n        engine.train_fn(\n            para_loader.per_device_loader(device), \n            model, \n            optimizer, \n            device, \n            scheduler\n        )\n        \n        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n        outputs, targets = engine.eval_fn(\n            para_loader.per_device_loader(device), \n            model, \n            device\n        )\n\n        targets = np.array(targets) >= 0.5\n        auc = metrics.roc_auc_score(targets, outputs)\n        print(f'[xla:{xm.get_ordinal()}]: AUC={auc}')\n        if auc > best_auc:\n            xm.save(model.state_dict(), config.MODEL_PATH)\n            best_auc = auc","0812b5a2":"def _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = run()","3293ac44":"FLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","55422ddf":"# Process spawner for training on TPUs","e5fca9d3":"# Main training function","b0badc82":"# Load model and datasets","d29767ad":"# Multi-processing wrapper","2c996f1c":"# Install torch_xla","e7a15402":"# Define config","d8b7a034":"# Import libraries and utility scripts"}}