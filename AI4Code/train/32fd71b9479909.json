{"cell_type":{"d212654c":"code","5257cef4":"code","56b172d0":"code","ba774820":"code","3344943d":"code","d244c88f":"code","5d28ce18":"code","efa250f0":"code","6a6e4ca9":"code","7e87834b":"code","55410648":"code","cbde9097":"code","b5e761fc":"code","5b56bbb8":"code","3cc1ed41":"code","da94cfc0":"code","087fca17":"code","5726f48a":"code","d7b47ac6":"code","e65ffee3":"code","01825df9":"code","c53fb65e":"code","f3289926":"code","f2d03304":"code","476f9a19":"code","941ccbce":"code","bcf2e819":"code","f91954ad":"code","ab1a0165":"code","b3283d46":"code","f189ae14":"code","36110a82":"code","6ffa40ba":"code","c42b8510":"code","f42c1bbc":"code","919cf1ed":"code","f3fe4170":"code","6cbab032":"code","64d2a574":"code","c5725d0e":"code","9cb4c078":"code","2a01239e":"code","183e1fad":"code","a0dc8856":"code","043bd21d":"code","b39ea1d8":"code","920c2e03":"code","ca5dea34":"code","1b4d1955":"code","ab62ac4f":"code","5c6d01d7":"code","5a11cece":"code","d5149591":"code","0cd61dfb":"code","ce92ab47":"code","e2f08e57":"code","55ba214b":"code","8655b860":"code","06534e6b":"code","4b4f07b2":"code","07537ca3":"code","3010f9ec":"code","fe4243ad":"code","a5aaa338":"code","c8b0bf0b":"code","b05ebf76":"code","f919c365":"code","9c82caf1":"code","b8f89e79":"code","93a30906":"markdown","6e20f571":"markdown","75173a70":"markdown","fe0dbbca":"markdown","ba12cc78":"markdown","afb9f0d4":"markdown","c3bb558e":"markdown","d28bdd25":"markdown","7a945460":"markdown","ea8aca81":"markdown","ad2d83a8":"markdown","b0fd92b3":"markdown","ce7e4940":"markdown","3ba08e25":"markdown","c097f3c1":"markdown","81d0944b":"markdown","3c6c4a89":"markdown","1f59a7f8":"markdown","3b8d02f6":"markdown","9dcd3e16":"markdown","37e8b2f5":"markdown","173b8a6e":"markdown","e4c7a028":"markdown","b7d804ed":"markdown","3dda3b37":"markdown","f32898d0":"markdown"},"source":{"d212654c":"import warnings\n\nwarnings.filterwarnings('ignore', 'SettingWithCopyWarning')\nwarnings.filterwarnings(\"ignore\", 'Creating legend with loc=\"best\" can be slow with large amounts of data.')","5257cef4":"import random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datatable as dt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import metrics, model_selection\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\n\nfrom IPython.display import display, Markdown, Latex","56b172d0":"# matplotlib\nplt.rc('font', size=15)\nplt.rc('axes', titlesize=18)  \nplt.rc('xtick', labelsize=10)  \nplt.rc('ytick', labelsize=10)\n\n# seaborn\nsns.set(font_scale = 1.2)\nsns.set_style(\"whitegrid\")","ba774820":"class Cfg:\n    RANDOM_STATE = 2021\n    TRAIN_DATA = '..\/input\/tabular-playground-series-oct-2021\/train.csv'\n    TEST_DATA = '..\/input\/tabular-playground-series-oct-2021\/test.csv'\n    SUBMISSION = '..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv'    \n    SUBMISSION_FILE = 'submission.csv'\n    TEST_SIZE = 0.4\n    SAMPLE_FRAC = 0.03\n    N_FEATURE = 5 # 285\n    \n    INDEX = 'id'\n    TARGET = 'target'\n    FEATURES = ['f{}'.format(i) for i in range(1, 285)]\n    \n    @staticmethod\n    def set_seed():\n        random.seed(Cfg.RANDOM_STATE)\n        np.random.seed(Cfg.RANDOM_STATE)\n\nCfg.set_seed()","3344943d":"def read_data(\n    train_file:str=Cfg.TRAIN_DATA, \n    test_file:str=Cfg.TEST_DATA\n) -> (pd.DataFrame, pd.DataFrame):\n    \"\"\"\n    \"\"\"\n    # read csv files\n    train_df = dt.fread(train_file).to_pandas().set_index(Cfg.INDEX)\n    test_df = dt.fread(test_file).to_pandas().set_index(Cfg.INDEX)\n    \n    # determine data types\n    num_cols = train_df.dtypes[train_df.dtypes == 'float64'].index.to_list()\n    binary_cols = train_df.dtypes[train_df.dtypes == 'bool'].index.drop(Cfg.TARGET).to_list()\n    \n    # reduce memory usage\n    train_df[num_cols] = train_df[num_cols].astype(np.float32, copy=False)\n    train_df[binary_cols] = train_df[binary_cols].astype(np.short, copy=False)\n    train_df[Cfg.TARGET] = train_df[Cfg.TARGET].astype(np.short, copy=False)\n    \n    test_df[num_cols] = test_df[num_cols].astype(np.float32, copy=False)\n    test_df[binary_cols] = test_df[binary_cols].astype(np.short, copy=False)\n    \n    return train_df, test_df","d244c88f":"%%time\ntrain_data, test_data = read_data()","5d28ce18":"Cfg.FEATURES = train_data.dtypes.index.drop(Cfg.TARGET).to_list()\n\nCfg.NUM_FEATURES = train_data.dtypes[train_data.dtypes == 'float32'].index.to_list()\nCfg.BINARY_FEATURES = train_data.dtypes[train_data.dtypes == 'short'].index.drop(Cfg.TARGET).to_list()","efa250f0":"# memory usage\nmemory_usage = train_data.memory_usage(deep=True) \/ 1024 ** 2\nprint('Memory (train): {:.2f} MB'.format(memory_usage.sum()))\n\nmemory_usage = test_data.memory_usage(deep=True) \/ 1024 ** 2\nprint('Memory (test) : {:.2f} MB'.format(memory_usage.sum()))","6a6e4ca9":"print('Features: {}'.format(len(Cfg.FEATURES)))\nprint('Numerical features: {}'.format(len(Cfg.NUM_FEATURES)))\nprint('Categorical features: {}'.format(len(Cfg.BINARY_FEATURES)))","7e87834b":"train_data.head()","55410648":"test_data.head()","cbde9097":"pd.DataFrame({\n    'data_set': ['train', 'test'],\n    'missing_values': [\n        train_data.isna().sum().sum(), \n        test_data.isna().sum().sum()\n    ]\n}).set_index('data_set')","b5e761fc":"def get_sample_data(\n    data,\n    split_target=True,\n    frac=Cfg.SAMPLE_FRAC, \n    random_state=Cfg.RANDOM_STATE):\n    \"\"\"Select a sample subset from the data\n    \"\"\"\n    idx = train_data.sample(frac=frac, random_state=random_state).index\n\n    if split_target:\n        X_data = train_data.iloc[idx][Cfg.FEATURES]\n        y_data = train_data.iloc[idx][Cfg.TARGET]\n    \n        return X_data, y_data\n    \n    return train_data.iloc[idx]","5b56bbb8":"stat_data = train_data.describe().drop('count')\nstat_data.loc['var'] = stat_data.T['std']**2\n\nstat_data.T.style.bar(\n    subset=['mean'], \n    color='Bules'\n).background_gradient(subset=['50%'], cmap='Blues')","3cc1ed41":"def plot_count(\n    data:pd.DataFrame, \n    feature:str, \n    title='Countplot',\n    ax=None):\n    \"\"\"\n    \"\"\"\n    if ax == None:\n        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n    \n    sns.countplot(\n        data=train_data,\n        x=feature, \n        palette='Blues_r',\n        ax=ax\n    )\n    \n    ax.set_title(title)\n\n    ax.set_xlabel('Feature {}'.format(feature))\n    ax.set_ylabel('Count')\n\n    return ax","da94cfc0":"plot_count(train_data, Cfg.TARGET);","087fca17":"def plot_pdf(\n    data:pd.DataFrame, \n    feature:str, \n    title='Histplot',\n    bins=70,\n    ax=None):\n    \"\"\" Plots the estimated pdf. \n    \"\"\"\n    if ax == None:\n        fig, ax = plt.subplots(1, 1)\n    \n    # plot pdf\n    sns.histplot(\n        data=data[[feature, Cfg.TARGET]],\n        x=feature,\n        hue=Cfg.TARGET,\n        bins=bins,\n        palette='Blues_r',\n        legend=True,\n        kde=False,\n        ax=ax\n    )\n    mean = np.mean(data[feature])\n    ax.vlines(\n        mean, 0, 1, \n        transform=ax.get_xaxis_transform(), \n        color='red', ls=':')\n    \n    ax.set_title(title)\n    \n    ax.set_xlabel('Feature {}'.format(feature))\n    ax.set_ylabel('Count')\n    \n    return ax","5726f48a":"def plot_boxplot(\n    data:pd.DataFrame, \n    feature:str, \n    title='Boxplot',\n    ax=None):\n    \"\"\"\n    \"\"\"\n    if ax == None:\n        fig, ax = plt.subplots(1, 1)\n    \n    ax = sns.boxplot(\n        x=Cfg.TARGET, \n        y=feature,\n        palette='Blues_r',\n        data=data\n    )\n    \n    ax.set_title(title)\n    \n    ax.set_xlabel('Target {}'.format(Cfg.TARGET))\n    ax.set_ylabel('Feature {}'.format(feature))\n    \n    return ax","d7b47ac6":"X_data = get_sample_data(train_data, split_target=False)\n\nfor feature in Cfg.NUM_FEATURES[0:Cfg.N_FEATURE]:\n    display(Markdown('### Feature `{}`'.format(feature)))\n \n    info = np.round(train_data[feature].describe(), 4)\n    \n    format_str = '* mean: {}\\n* std: {}\\n* min: {}\\n* 25%: {}\\n* 50%: {}\\n* 75%: {}\\n* max: {}'\n    display(Markdown(format_str.format(info['mean'], info['std'], info['min'], info['25%'], info['50%'], info['75%'], info['max'])))\n    \n    fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n\n    plot_pdf(X_data, feature, ax=ax[0])\n    plot_boxplot(X_data, feature, ax=ax[1])\n    \n    plt.show()","e65ffee3":"def plot_heatmap(\n    data:pd.DataFrame, \n    feature:str, \n    title='Heatmap',\n    ax=None):\n    \"\"\"\n    \"\"\"\n    if ax == None:\n        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n\n    N, _ = data.shape\n    sns.heatmap(\n        pd.crosstab(data[feature], data['target']) \/ N,\n        cmap='Blues_r',\n        annot=True, \n        ax=ax\n    )\n\n    ax.set_title(title)\n    return ax","01825df9":"for feature in Cfg.BINARY_FEATURES[0:Cfg.N_FEATURE]:\n    display(Markdown('### Feature `{}`'.format(feature)))\n            \n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n    plot_count(train_data, feature, ax=ax[0])\n    plot_heatmap(train_data, feature, ax=ax[1])\n    \n    plt.show()","c53fb65e":"from sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectPercentile, SelectKBest\nfrom sklearn.feature_selection import f_classif, chi2\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.compose import make_column_selector, make_column_transformer","f3289926":"def plot_feature_importances(\n    feature_imp, \n    feature_names,\n    title='Feature importance',\n    num=20, \n    ax=None\n):\n    if ax == None:\n        fig, ax = plt.subplots(1, 1)\n    \n    df = pd.DataFrame({\n        'feature': feature_names,\n        'value': feature_imp\n    }).sort_values('value', ascending=False).head(num)\n    \n    sns.barplot(\n        x='value', \n        y='feature', \n        palette='Blues_r',\n        data=df,\n        ax=ax\n    ) \n\n    ax.set_title(title)\n    ax.set_ylabel(\"Features\")\n\n    return ax","f2d03304":" def plot_features_high_variance(\n     stat_data, \n     features, \n     threshold, \n     title='Features hight variance'\n ):\n    \"\"\"\n    \"\"\"\n    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n\n    # plot pdf\n    sns.histplot(\n        data=stat_data.loc['var'],\n        bins=50,\n        palette='Blues_r',\n        legend=True,\n        ax=ax[0]\n    )\n\n    # plot threshold line\n    ax[0].vlines(\n        threshold, 0, 1, \n        transform=ax[0].get_xaxis_transform(), \n        color='red', ls=':')\n\n    plot_feature_importances(\n        stat_data.T.loc[features]['var'], \n        feature_names=features,\n        title=title,\n        num=20, \n        ax=ax[1])\n\n    plt.show()","476f9a19":"num_threshold = 0.0025\n\nplot_features_high_variance(\n    stat_data.T.loc[Cfg.NUM_FEATURES].sort_values('var').T, \n    features=Cfg.NUM_FEATURES, \n    threshold=num_threshold, \n    title='Numerical Features hight variance'\n)","941ccbce":"bin_threshold = 0.175\n\nplot_features_high_variance(\n    stat_data.T.loc[Cfg.BINARY_FEATURES].sort_values('var').T, \n    features=Cfg.BINARY_FEATURES, \n    threshold=bin_threshold, \n    title='Binary Features hight variance'\n)","bcf2e819":"X_data, y_data = get_sample_data(train_data, frac=0.001)","f91954ad":"rf = RandomForestClassifier(random_state=Cfg.RANDOM_STATE)\nrfe = RFECV(\n    estimator=rf, \n    cv=StratifiedKFold(2),\n    scoring='accuracy',\n    min_features_to_select=1,\n    step=3, \n    verbose=0\n)","ab1a0165":"%%time\n\nrfe.fit(X_data, y_data);\nprint('Optimal number of features: {}'.format(rfe.n_features_))","b3283d46":"fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n\nsns.lineplot(\n    x=range(1, len(rfe.grid_scores_) + 1),\n    y=rfe.grid_scores_,\n    ax=ax\n)\n\nax.set_title('Recursive feature elimination')\nax.set_xlabel('Number of features selected')\nax.set_ylabel('Cross validation score (accuracy)')\n\nplt.show()","f189ae14":"from sklearn.compose import make_column_selector, ColumnTransformer\n\nfeature_selector = make_column_transformer(\n    ('drop', X_data.columns[~rfe.support_]), \n    remainder='passthrough'\n)","36110a82":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis","6ffa40ba":"idx = train_data.sample(frac=0.2, random_state=Cfg.RANDOM_STATE).index\n\nX_data = train_data.iloc[idx][Cfg.FEATURES]\ny_data = train_data.iloc[idx][Cfg.TARGET]","c42b8510":"lda = LinearDiscriminantAnalysis()\n\nlda_data = lda.fit_transform(X_data[Cfg.FEATURES], y_data)\nnum_lda_data = lda.fit_transform(X_data[Cfg.NUM_FEATURES], y_data)\nbin_lda_data = lda.fit_transform(X_data[Cfg.BINARY_FEATURES], y_data)","f42c1bbc":"fig, ax = plt.subplots(3, 1, figsize=(20, 9))\n\nsns.scatterplot(\n    x=lda_data.reshape(-1),\n    y=0,\n    hue=y_data,\n    ax=ax[0],\n    alpha=0.4\n)\nax[0].set_title('All features')\nax[0].get_yaxis().set_visible(False)\n\nsns.scatterplot(\n    x=num_lda_data.reshape(-1),\n    y=0,\n    hue=y_data,\n    ax=ax[1],\n    alpha=0.4\n)\nax[1].set_title('Numerical features')\nax[1].get_yaxis().set_visible(False)\n\nsns.scatterplot(\n    x=bin_lda_data.reshape(-1),\n    y=0,\n    hue=y_data,\n    ax=ax[2],\n    alpha=0.4\n)\nax[2].set_title('Binary features')\nax[2].get_yaxis().set_visible(False)\n\nplt.tight_layout()\nplt.show()","919cf1ed":"from sklearn.decomposition import PCA, KernelPCA","f3fe4170":"X_data, y_data = get_sample_data(train_data, frac=0.01)","6cbab032":"%%time\n\nn_components=rfe.n_features_\nkpca = make_pipeline(\n    feature_selector,\n    KernelPCA(\n        n_components=n_components,\n        kernel='poly',\n        gamma=15,\n        random_state=Cfg.RANDOM_STATE)\n)\n\ncomponents = kpca.fit_transform(X_data, y_data)\npca_data = pd.DataFrame(\n    components, \n    columns=['pc{}'.format(i) for i in range(1, n_components + 1)]\n)\n\npca_data[Cfg.TARGET] = y_data.values","64d2a574":"def plot_pca(data, x, y, ax=None):\n    if ax == None:\n        fig, ax = plt.subplots(1, 1)\n        \n    sns.scatterplot(\n        data=data,\n        x=x, \n        y=y,\n        hue=Cfg.TARGET,\n        legend='brief',\n        alpha=0.2,\n        ax=ax\n    )","c5725d0e":"fig, ax = plt.subplots(1, 5, figsize=(28, 5))\n\nplot_pca(pca_data, 'pc1', 'pc2', ax=ax[0])\nplot_pca(pca_data, 'pc2', 'pc3', ax=ax[1])\nplot_pca(pca_data, 'pc3', 'pc4', ax=ax[2])\nplot_pca(pca_data, 'pc4', 'pc5', ax=ax[3])\nplot_pca(pca_data, 'pc5', 'pc6', ax=ax[4])\n\nplt.tight_layout()\nplt.show()","9cb4c078":"X_data, y_data = get_sample_data(train_data, frac=0.01)","2a01239e":"rf = RandomForestClassifier(random_state=0).fit(X_data, y_data)\nfeature_imp = rf.estimators_[0].feature_importances_","183e1fad":"fig, ax = plt.subplots(figsize=(20, 8))\nplot_feature_importances(feature_imp, feature_names=Cfg.FEATURES, num=35, ax=ax)\n\nfig.tight_layout()\nplt.show()","a0dc8856":"import lightgbm as lgb\n\nfrom xgboost import XGBClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import classification_report, roc_auc_score\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import StackingClassifier","043bd21d":"def plot_model_proba(proba, ax=None):\n    \"\"\"\n    \"\"\"\n    if ax == None:\n        fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n\n    sns.histplot(\n        data=proba,\n        palette='Blues_r',\n        legend=True,\n        bins=100,\n        kde=True,\n        ax=ax\n    )\n\n    ax.set_xlabel('Prediction probapility')\n    ax.set_ylabel('Probabitity')","b39ea1d8":"def plot_roc(model, X_val, y_val, ax=None):\n    \"\"\"\n    \"\"\"\n    if ax == None:\n        fig, ax = plt.subplots(1, 1)\n    \n    metrics.plot_roc_curve(model, X_val, y_val, ax=ax)","920c2e03":"from sklearn import metrics\n\ndef plot_confusion_matrix(model, X_val, y_val, ax=None):\n    if ax == None:\n        fig, ax = plt.subplots(1, 1)\n\n    metrics.plot_confusion_matrix(\n        model, \n        X_val, \n        y_val, \n        cmap=plt.cm.Blues,\n        normalize='true', \n        ax=ax\n    ) ","ca5dea34":"def display_model_result(model, X_val, y_val, y_pred, y_pred_proba):\n    \"\"\"\n    \"\"\"\n    fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n\n    plot_roc(model, X_val, y_val, ax=ax[0])\n    plot_model_proba(y_pred_proba, ax=ax[1])\n    plot_confusion_matrix(model, X_val, y_val, ax=ax[2])\n\n    plt.show()\n\n    print(classification_report(y_val, y_pred))","1b4d1955":"X_data, y_data = get_sample_data(train_data, frac=0.1)","ab62ac4f":"# spit data into train and validation data sets\nX_train, X_val, y_train, y_val = train_test_split(\n    X_data,\n    y_data,\n    test_size=Cfg.TEST_SIZE, \n    random_state=Cfg.RANDOM_STATE\n)","5c6d01d7":"print(f'train size: {X_train.shape[0]} rows')\nprint(f'val size  : {X_val.shape[0]} rows')","5a11cece":"preprocess = make_pipeline(\n    feature_selector,\n    #PCA(n_components=200)\n)","d5149591":"lda_model = make_pipeline(\n    feature_selector,\n    #preprocess,\n    LinearDiscriminantAnalysis()\n)","0cd61dfb":"%%time\n\ny_lda_pred = lda_model.fit(X_train, y_train).predict(X_val)\ny_lda_pred_proba = lda_model.predict_proba(X_val)[:, 1]\n\ndisplay_model_result(lda_model, X_val, y_val, y_lda_pred, y_lda_pred_proba)","ce92ab47":"from sklearn.tree import DecisionTreeClassifier\n\ndt_model = make_pipeline(\n    feature_selector,\n    #preprocess,\n    DecisionTreeClassifier(max_depth=5)\n)","e2f08e57":"%%time\n\ny_dt_pred = dt_model.fit(X_train, y_train).predict(X_val)\ny_dt_pred_proba = dt_model.predict_proba(X_val)[:, 1]\n\ndisplay_model_result(dt_model, X_val, y_val, y_dt_pred, y_dt_pred_proba)","55ba214b":"rf_model = make_pipeline(\n    feature_selector,\n    #preprocess,\n    RandomForestClassifier(\n        max_depth=5, \n        n_estimators=20\n    )\n)","8655b860":" %%time\n\ny_rf_pred = rf_model.fit(X_train, y_train).predict(X_val)\ny_rf_pred_proba = rf_model.predict_proba(X_val)[:, 1]\n\ndisplay_model_result(rf_model, X_val, y_val, y_rf_pred, y_rf_pred_proba)","06534e6b":"ada_model = make_pipeline(\n    feature_selector,\n    #preprocess,\n    AdaBoostClassifier()\n)","4b4f07b2":"%%time\n\ny_ada_pred = ada_model.fit(X_train, y_train).predict(X_val)\ny_ada_pred_proba = ada_model.predict_proba(X_val)[:, 1]\n\ndisplay_model_result(ada_model, X_val, y_val, y_ada_pred, y_ada_pred_proba)","07537ca3":"lgbm_model = make_pipeline(\n    feature_selector,\n    #preprocess,\n    lgb.LGBMClassifier(\n        learning_rate=0.05,\n        n_estimators=1000,\n        reg_lambda = 1\n    )\n)","3010f9ec":"%%time\n\ny_lgbm_pred = lgbm_model.fit(X_train, y_train).predict(X_val)\ny_lgbm_pred_proba = lgbm_model.predict_proba(X_val)[:, 1]\n\ndisplay_model_result(lgbm_model, X_val, y_val, y_lgbm_pred, y_lgbm_pred_proba)","fe4243ad":"xgb_model = make_pipeline(\n    feature_selector,\n    #preprocess,\n    XGBClassifier(\n        n_estimators=100,\n        use_label_encoder=False,\n        eval_metric='rmse',\n        random_state=Cfg.RANDOM_STATE\n    )\n)","a5aaa338":"%%time\n\ny_xgb_pred = xgb_model.fit(X_train, y_train).predict(X_val)\ny_xgb_pred_proba = xgb_model.predict_proba(X_val)[:, 1]\n\ndisplay_model_result(xgb_model, X_val, y_val, y_xgb_pred, y_xgb_pred_proba)","c8b0bf0b":"estimators = [\n    ('dt', DecisionTreeClassifier(max_depth=5)),\n    ('lda', LinearDiscriminantAnalysis()),\n    ('rf', RandomForestClassifier(\n            max_depth=5, \n            n_estimators=20)),\n    ('ada', AdaBoostClassifier()),\n    ('lgbm', lgb.LGBMClassifier(\n        learning_rate=0.05,\n        n_estimators=100,\n        reg_lambda = 1)),\n    ('xgb', XGBClassifier(\n        n_estimators=100,\n        use_label_encoder=False,\n        eval_metric='rmse',\n        random_state=Cfg.RANDOM_STATE))\n]\n    \nmodel = make_pipeline(\n    feature_selector,\n    #preprocess,\n    StackingClassifier(\n        estimators=estimators, \n        final_estimator=LinearDiscriminantAnalysis(),\n        cv=3,\n        n_jobs=-1,\n        stack_method='predict_proba',\n        verbose=0\n    )\n)\n","b05ebf76":"%%time\n\ny_pred = model.fit(X_train, y_train).predict(X_val)\ny_pred_proba = model.predict_proba(X_val)[:, 1]\n\ndisplay_model_result(model, X_val, y_val, y_pred, y_pred_proba)","f919c365":"y_pred_submission = model.predict_proba(test_data)[:, 1]","9c82caf1":"submission_data = pd.DataFrame({\n    Cfg.INDEX: test_data.index,\n    Cfg.TARGET: y_pred_submission,\n}).set_index(Cfg.INDEX)\n\nsubmission_data","b8f89e79":"# save submission file\nsubmission_data.to_csv(Cfg.SUBMISSION_FILE)","93a30906":"### Model `RandomForest` ","6e20f571":"## Recursive feature elimination (RFE)","75173a70":"## Binary features","fe0dbbca":"# Feature selection (FS) ","ba12cc78":"# Feature Engineering\n\nwork in process","afb9f0d4":"### Model `XGB`","c3bb558e":"## Numerical features","d28bdd25":"### Model `AdaBoost` ","7a945460":"# Import data","ea8aca81":"# Modeling","ad2d83a8":"### Model `DecisionTree`","b0fd92b3":"## Missing values","ce7e4940":"# Submission","3ba08e25":"## Target `target`","c097f3c1":"# Feature importance","81d0944b":"# Principal component analysis (PCA)","3c6c4a89":"### Model `LGBM` ","1f59a7f8":"# Tabular Playground Series - Oct 2021\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/28009\/logos\/header.png?)","3b8d02f6":"# Configuration","9dcd3e16":"## Variance Thresholding","37e8b2f5":"# Imports","173b8a6e":"# Linear discriminant analysis (LDA)","e4c7a028":"## Correlation\n\nwork in process","b7d804ed":"## Data overview\n\n* The training data contains 1000000 rows.\n\n* The test data contains 500000 rows.\n\n* There are 285 features `f0` - `f284`\n    * 240 numerical features\n    * 45 categorical features (All binary - 1\/0).\n\n\n* There are no missing values in both data sets.\n\n* The target variable `target` is binary (1\/0)\n\n* The distribution of `target` is balanced.","3dda3b37":"### Model `LinearDiscriminant`","f32898d0":"# Exploratory data analysis (EDA)"}}