{"cell_type":{"2d73ecd9":"code","a03da59a":"code","64524eaa":"code","38d4b84e":"code","7967f25c":"code","225626d1":"code","4486818d":"code","3bcfc44c":"code","5f4397f3":"code","76f6cde7":"code","4877ec96":"code","cf7010fd":"code","6ed9b249":"code","4ada6749":"code","540db5ee":"code","f6ab41b4":"code","58f11b60":"code","68594751":"code","f592e117":"code","4a761e9f":"code","b10876a7":"code","4da524c0":"code","8d1f30bb":"code","eee9c674":"code","fe2fd162":"code","8ac68a5b":"code","fcafc27e":"code","2cac5baf":"code","c9ae6a34":"code","988869c3":"code","4fbf0d0d":"code","acd771e0":"code","1fc417f3":"code","f39e68d5":"code","e27504fd":"code","014f2295":"code","e5504e04":"code","113fe194":"code","a014d0c0":"code","177b9482":"code","151211b8":"code","cbedcab1":"code","b4eca005":"code","b7a1194e":"markdown","e2d12cb0":"markdown","c4a5cf23":"markdown","629ce978":"markdown","9ad1d10d":"markdown","ddc697e4":"markdown","99100a1c":"markdown","b44f6001":"markdown","5f11f930":"markdown","06e2fcfd":"markdown","da196850":"markdown","10ebff4e":"markdown","9c1782f8":"markdown","a2b214be":"markdown","20666153":"markdown","31062785":"markdown","a3dc3c5f":"markdown","7f06946e":"markdown","cca70aee":"markdown","bede7ed1":"markdown","aaaf6b76":"markdown","b538fe05":"markdown"},"source":{"2d73ecd9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a03da59a":"#Data analysis\nimport pandas as pd\nimport numpy as np\n#Data visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes=True)\nsns.set(font_scale=1)\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n#Modeling\nfrom sklearn.model_selection import cross_val_predict, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n# from sklearn_crfsuite import CRF, scorers, metrics\n# import sklearn_crfsuite\n# from sklearn_crfsuite import scorers\n# from sklearn_crfsuite import metrics\n# from sklearn_crfsuite.metrics import flat_classification_report\nfrom sklearn.metrics import classification_report, make_scorer\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nimport scipy.stats\nimport eli5","64524eaa":"df = pd.read_csv('\/kaggle\/input\/entity-annotated-corpus\/ner_dataset.csv', encoding=\"latin1\")","38d4b84e":"#The dataset does not have any header currently. We can use the first row as a header as it has the relevant headings.\n#We will make the first row as the heading, remove the first row and re-index the dataset\n\ndf.columns = df.iloc[0]\n\ndf = df[1:]\n\ndf.columns = ['Sentence #','Word','POS','Tag']\n\ndf = df.reset_index(drop=True)\n\ndf.head()\n","7967f25c":"df = df.rename(columns={\"Sentence #\": \"sentence#\"})","225626d1":"df.head()","4486818d":"df.shape","3bcfc44c":"df.info()","5f4397f3":"#so we are basically having only those rows where sentence column is not null\ndata = df[df['sentence#'].notnull()]","76f6cde7":"data.info()","4877ec96":"data.head()","cf7010fd":"# A class to retrieve the sentences from the dataset\nclass getsentence(object):\n    \n    def __init__(self, data):\n        self.n_sent = 1.0\n        self.data = data\n        self.empty = False\n        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n                                                           s[\"POS\"].values.tolist(),\n                                                           s[\"Tag\"].values.tolist())]\n        self.grouped = self.data.groupby(\"sentence#\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]","6ed9b249":"data.head()","4ada6749":"getter = getsentence(data)","540db5ee":"sentences = getter.sentences\n#ths is how a sentence will look like.\nprint(sentences[1])","f6ab41b4":"#Lets find the number of words in the dataset\nwords = list(set(data[\"Word\"].values))\nn_words = len(words)\nprint(n_words)","58f11b60":"#Lets visualize how the sentences are distributed by their length\nplt.style.use(\"ggplot\")\nplt.hist([len(s) for s in sentences], bins=50)\nplt.show()","68594751":"#Lets find out the longest sentence length in the dataset\nmaxlen = max([len(s) for s in sentences])\nprint ('Maximum sentence length:', maxlen)","f592e117":"#Words tagged as B-org\ndata.loc[data['Tag'] == 'B-org', 'Word'].head()","4a761e9f":"#Words tagged as I-org\ndata.loc[data['Tag'] == 'I-org', 'Word'].head()","b10876a7":"#Words tagged as B-per\ndata.loc[data['Tag'] == 'B-per', 'Word'].head()","4da524c0":"#Words tagged as I-per\ndata.loc[data['Tag'] == 'I-per', 'Word'].head()","8d1f30bb":"#Words tagged as B-geo\ndata.loc[data['Tag'] == 'B-geo', 'Word'].head()\n","eee9c674":"#Words tagged as I-geo\ndata.loc[data['Tag'] == 'I-geo', 'Word'].head()","fe2fd162":"#Words tagged as I-geo\ndata.loc[data['Tag'] == 'O', 'Word'].head()","8ac68a5b":"#Words distribution across Tags\nplt.figure(figsize=(15, 5))\nax = sns.countplot('Tag', data=data)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\nplt.tight_layout()\nplt.show()","fcafc27e":"#Words distribution across Tags without O tag\nplt.figure(figsize=(15, 5))\nax = sns.countplot('Tag', data=data.loc[data['Tag'] != 'O'])\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\nplt.tight_layout()\nplt.show()","2cac5baf":"#Words distribution across POS\nplt.figure(figsize=(15, 5))\nax = sns.countplot('POS', data=data, orient='h')\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\nplt.tight_layout()\nplt.show()","c9ae6a34":"#Simple feature map to feed arrays into the classifier. \ndef feature_map(word):\n    return np.array([word.istitle(), word.islower(), word.isupper(), len(word),\n                     word.isdigit(),  word.isalpha()])","988869c3":"#We divide the dataset into train and test sets\nwords = [feature_map(w) for w in data[\"Word\"].values.tolist()]\ntags = data[\"Tag\"].values.tolist()","4fbf0d0d":"#Lets see how the input array looks like\nprint(words[:5])","acd771e0":"#Random Forest classifier\npred = cross_val_predict(RandomForestClassifier(n_estimators=20),X=words, y=tags, cv=5)","1fc417f3":"#Lets check the performance \nfrom sklearn.metrics import classification_report\nreport = classification_report(y_pred=pred, y_true=tags)\nprint(report)","f39e68d5":"!pip install sklearn-crfsuite","e27504fd":"from itertools import chain\n\nimport nltk\nimport sklearn\nimport scipy.stats\n\nimport sklearn_crfsuite\nfrom sklearn_crfsuite import scorers,CRF\nfrom sklearn_crfsuite.metrics import flat_classification_report\nfrom sklearn_crfsuite import metrics","014f2295":"# Feature set\ndef word2features(sent, i):\n    word = sent[i][0]\n    postag = sent[i][1]\n\n    features = {\n        'bias': 1.0,\n        'word.lower()': word.lower(),\n        'word[-3:]': word[-3:],\n        'word[-2:]': word[-2:],\n        'word.isupper()': word.isupper(),\n        'word.istitle()': word.istitle(),\n        'word.isdigit()': word.isdigit(),\n        'postag': postag,\n        'postag[:2]': postag[:2],\n    }\n    if i > 0:\n        word1 = sent[i-1][0]\n        postag1 = sent[i-1][1]\n        features.update({\n            '-1:word.lower()': word1.lower(),\n            '-1:word.istitle()': word1.istitle(),\n            '-1:word.isupper()': word1.isupper(),\n            '-1:postag': postag1,\n            '-1:postag[:2]': postag1[:2],\n        })\n    else:\n        features['BOS'] = True\n\n    if i < len(sent)-1:\n        word1 = sent[i+1][0]\n        postag1 = sent[i+1][1]\n        features.update({\n            '+1:word.lower()': word1.lower(),\n            '+1:word.istitle()': word1.istitle(),\n            '+1:word.isupper()': word1.isupper(),\n            '+1:postag': postag1,\n            '+1:postag[:2]': postag1[:2],\n        })\n    else:\n        features['EOS'] = True\n\n    return features","e5504e04":"def sent2features(sent):\n    return [word2features(sent, i) for i in range(len(sent))]\n\ndef sent2labels(sent):\n    return [label for token, postag, label in sent]","113fe194":"#Creating the train and test set\nX = [sent2features(s) for s in sentences]\ny = [sent2labels(s) for s in sentences]","a014d0c0":"#Creating the CRF model\ncrf = CRF(algorithm='lbfgs',\n          c1=0.1,\n          c2=0.1,\n          max_iterations=100,\n          all_possible_transitions=False)","177b9482":"#We predcit using the same 5 fold cross validation\npred = cross_val_predict(estimator=crf, X=X, y=y, cv=5)","151211b8":"#Lets evaluate the mode\nreport = flat_classification_report(y_pred=pred, y_true=y)\nprint(report)","cbedcab1":"#Tuning the parameters manually, setting c1 = 10\ncrf2 = CRF(algorithm='lbfgs',\n          c1=10,\n          c2=0.1,\n          max_iterations=100,\n          all_possible_transitions=False)","b4eca005":"pred = cross_val_predict(estimator=crf2, X=X, y=y, cv=5)\nreport = flat_classification_report(y_pred=pred, y_true=y)\nprint(report)","b7a1194e":"> Libraries","e2d12cb0":"Before going further, we will try to understand what the dataset is all about and what all the features mean. This is important in order to understand how the classifiers will perform and help us interpret the results.","c4a5cf23":"Future Scope - we can do the hyperparameter tuning","629ce978":"# NER USING CRF","9ad1d10d":"With the basic EDA done and understanding the dataset, we can move to the modeling stage.\nSince the problem statement is a simple classification problem, we will start with a simple tree based model, Random Forest using a simple feature map.\nSimple tree based models have been proven to provide decent performance in building NERC systems. Random Forest being one of the most popular tree based models can learn the underlying rules according to which terms are tagged. It is important that the classifier has proper features fed in to improve the performance.","ddc697e4":"## **Modeling the Data**","99100a1c":"Now that we know the words and sentences, lets try to understand what sort of words each tag contains. This will help us in understanding what each tag type and sub-type represents.","b44f6001":"A Conditional Random Field (CRF) is a standard model for predicting the most likely sequence of labels that correspond to a sequence of inputs. It is a supervised learning method which has been proven to be better than the tree based models when it comes to NER. Whereas a discrete classifier predicts a label for a single sample without considering \"neighboring\" samples, a CRF can take context into account; e.g., the linear chain CRF (which is popular in natural language processing) predicts sequences of labels for sequences of input samples.\n\nIn order to use CRF, we will enhance the feature set and create more features which can be used by the model to predict the tags correctly. Since we need to take into account the context as well, we create features which will provide consecutive POS tags for each word. Also, we add new features such as upper, lower, digit, title etc. for each word and also consider the consecutive words in the list. In short, we try to provide a sequence of features to the model for each word - the sequence containing POS tags, capitalisations, type of word(title) etc.","5f11f930":"> [reference link](https:\/\/www.kaggle.com\/shoumikgoswami\/ner-using-random-forest-and-crf)","06e2fcfd":"> So our dataset mostly contains words related to `geopolitical entities`, `geographical locations` and `person names`.","da196850":"## Exploring \/ Visualizing our data","10ebff4e":"The goal of a named entity recognition (NER) system is to identify all textual mentions of the named entities. This can be broken down into two sub-tasks: identifying the boundaries of the NE, and identifying its type.\n\nNamed entity recognition is a task that is well-suited to the type of classifier-based approach. In particular, a tagger can be built that labels each word in a sentence using the IOB format, where chunks are labelled by their appropriate type.\n\n The IOB Tagging system contains tags of the form:\n* **B** - {CHUNK_TYPE} \u2013 for the word in the Beginning chunk\n* **I** - {CHUNK_TYPE} \u2013 for words Inside the chunk\n* **O** \u2013 Outside any chunk\n\nThe IOB tags are further classified into the following classes \u2013\n* **geo** = Geographical Entity\n* **org** = Organization\n* **per** = Person\n* **gpe** = Geopolitical Entity\n* **tim** = Time indicator\n* **art** = Artifact\n* **eve** = Event\n* **nat** = Natural Phenomenon\n\n[Penn Treebank tagset](https:\/\/www.sketchengine.eu\/tagsets\/penn-treebank-tagset\/#:~:text=The%20English%20Penn%20Treebank%20tagset,Sketch%20Engine%20(earlier%20version).)","9c1782f8":"Although we have a good average score, the model performed quite badly. The precision and recall values of most of the classes were 0. It seems the features which require the model to take proper decisions are missing. The model is basically memorizing words and tags, which will not suffice. The context information behind each word needs to be fed to the model as well so that the predictions are more accurate.\nWe can either work on this model alone by improving the features or ensembling it with a more contextual model, or use a different model altogether.","a2b214be":"### Conditional Random Fields classifier","20666153":"so this is dataset is for learning purpose there we are having a same length of sentence","31062785":"We will use 5 fold cross validation as an input parameter to the classifier, i.e. we will divide the dataset into 5 subsets and train-test on them. Some models like decision trees and neural networks are often be able to get 100% accuracy on the training data, but perform much worse on new data. Therefore, we will train on one subset and test on the other, and repeat for every subset so that the classifier classifies correctly on average and the performance estimate is not overly optimistic","a3dc3c5f":"The dataset has the following columns or features -\n\n* **Index -** Index numbers for each word [Numeric type]\n* **Sentence #** - The number of sentences in the dataset (We will find the number of sentences below) [Numeric type]\n* **Word -** The words in the sentence [Character type]\n* **POS -** Parts Of Speech tags, these are tags given to the type of words as per the Penn TreeBank Tagset [Categorical type]\n* **Tag -**The tags given to each word based on the IOB tagging system described above (Target variable) [Categorical type]","7f06946e":"Compared to the Random Forest classifier, the CRF classifier did better as the scores have improved. \nHowever, the precision and recall metrics of the classes individually have improved but not much. Maybe the model is again remembering words and not taking into the context information completely. We will try tuning the model manually to see if we can improve it.","cca70aee":"### Random Forest Classifier","bede7ed1":"Quite surprising, most of the words are tagged as outside of any chunk. These words can be considered as fillers and their presence might impact the classifier performance as well. Lets check the dataset again without the O tags.","aaaf6b76":"Study material - [CRF](http:\/\/homepages.inf.ed.ac.uk\/csutton\/publications\/crftut-fnt.pdf)","b538fe05":"Performance metrics\nBefore we move to the modeling part, it is important to understand the performance metrics on the basis of which the models will be evaluated. Since we are dealing with Information Extraction, we will use the following metrics to evaluate the models -\n\n* Precision\n* Recall\n* F1 score\n\nThe metrics mentioned above are calculated using True\/False positives and True\/False negatives respectively.\n* True Positives (TP) - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes.\n* True Negatives (TN) - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no.\n* False Positives (FP) \u2013 When actual class is no and predicted class is yes.\n* False Negatives (FN) \u2013 When actual class is yes but predicted class in no.\n* Precision - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.\n\nPrecision=TP\/TP+FP \n\n* Recall (Sensitivity) - Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes.\n\nRecall=TP\/TP+FN \n\n* F1 score - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. It is the harmonic mean of the both Precision and Recall\n\nF1\u2212Score=2\u2217(Recall\u2217Precision)\/(Recall+Precision) \n\nFor a decent classifier, we would prefer high precision and recall values. Classification reports are used to obtain the values of these metrics in a text format per class. It is essential that the model is evaluated by these metrics per class to make sure we have a good model."}}