{"cell_type":{"17204e8d":"code","77d9b5e2":"code","fd727724":"code","d4e8afbb":"code","fff6ce41":"code","c84ec502":"code","a6ec4874":"code","c84a5656":"code","6cd23b47":"code","bab2d324":"code","84a09850":"code","ca30a78d":"code","f985f63e":"code","3877f1ba":"code","af88bf1c":"code","d7790e6f":"code","2822b195":"code","38a199e0":"code","18c6503a":"code","dc4633b2":"code","7c653781":"code","fc884978":"code","67fb3707":"code","299a30d9":"code","7ba1f8ae":"code","32b46f1a":"code","8103e975":"code","44da5328":"code","7c9dc20b":"code","fdd78844":"code","0d067942":"code","a7ec98d6":"markdown","0ab89e11":"markdown","99bc08e6":"markdown","081fc215":"markdown","4c997b8b":"markdown","8ac4bc70":"markdown","f8d6ccf1":"markdown","9633f094":"markdown","07fb4779":"markdown"},"source":{"17204e8d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as plt\nimport seaborn as sns\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","77d9b5e2":"train_df = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","fd727724":"train_df.head()","d4e8afbb":"# Set multiple plots distribution\nfigs, axs = plt.pyplot.subplots(nrows = 3, ncols = 3, figsize=(30,15))\n\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train_df, ax = axs[0][0])\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train_df, ax = axs[0][1])\nsns.barplot(x=\"Parch\", y=\"Survived\", data=train_df, ax = axs[0][2])\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=train_df, ax = axs[1][0])\nsns.boxplot(x=\"Survived\", y=\"Fare\", data=train_df[(train_df[\"Sex\"] == \"female\") & (train_df[\"Fare\"] < 100)], ax = axs[1][1])\nsns.scatterplot(x=\"Age\", y=\"Fare\", hue=\"Survived\", data=train_df[(train_df[\"Fare\"] < 60) & (train_df[\"Age\"] < 60)], ax = axs[1][2])\nsns.barplot(x=\"Embarked\", y=\"Survived\", data=train_df, ax=axs[2][1])\n","fff6ce41":"def cab_to_deck(cab):\n    if type(cab) is float:\n        return \"N\"\n    else:\n        return cab[0]\n    \ntrain_df[\"Deck\"] = train_df[\"Cabin\"].apply(cab_to_deck)\ntest[\"Deck\"] = test[\"Cabin\"].apply(cab_to_deck)","c84ec502":"cabin_is_nan = train_df[\"Cabin\"].isna().sum() \/ len(train_df[\"Cabin\"])\nprint(\"Percentage of NaN: \", cabin_is_nan*100)\n\nfigs, axs = plt.pyplot.subplots(ncols = 2, figsize = (30,5))\n\nsns.barplot(x=\"Deck\", y=\"Survived\", data=train_df, ax = axs[0])\nsns.countplot(x=\"Deck\", data=train_df, ax = axs[1])","a6ec4874":"# Criar a Feature\ntrain_df[\"AgeIsNaN\"] = train_df[\"Age\"].isna()\ntest[\"AgeIsNaN\"] = test[\"Age\"].isna()","c84a5656":"sns.barplot(x=\"AgeIsNaN\", y=\"Survived\", data=train_df)","6cd23b47":"# Substituir valores NaN em Age\ntrain_df[\"Age\"].fillna(train_df[\"Age\"].mean(), inplace=True)\ntest[\"Age\"].fillna(test[\"Age\"].mean(), inplace=True)","bab2d324":"from sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split","84a09850":"# Apagar antigas features convertidas, ticket, Cabin e Name\ntrain_df = train_df.drop(columns = ['Ticket', 'Cabin', 'Name'])\ntest = test.drop(columns = ['Ticket', 'Cabin', 'Name'])","ca30a78d":"# Estabelecer um m\u00e1ximo para Fare, 150, para ter melhores resultados ao usar MinMaxScaler\ntrain_df.loc[train_df[\"Fare\"] > 150, \"Fare\"] = train_df.loc[train_df[\"Fare\"] > 150, \"Fare\"].apply(lambda x: 100)\ntest.loc[test[\"Fare\"] > 150, \"Fare\"] = test.loc[train_df[\"Fare\"] > 150, \"Fare\"].apply(lambda x: 100)\n\ntest[\"Fare\"].fillna(test[\"Fare\"].mean(), inplace = True)\n\nsns.distplot(train_df[\"Fare\"])","f985f63e":"scaler = MinMaxScaler()\ntrain_df[[\"Age\", \"Fare\"]] = scaler.fit_transform(train_df[[\"Age\", \"Fare\"]])\ntest[[\"Age\", \"Fare\"]] = scaler.fit_transform(test[[\"Age\", \"Fare\"]])","3877f1ba":"train_df['Embarked'].fillna('C', inplace=True)\ntest['Embarked'].fillna('C', inplace=True)","af88bf1c":"encoder = LabelEncoder()\n\ntrain_df[['Sex', 'Embarked', 'Deck']] = train_df[['Sex', 'Embarked', 'Deck']].apply(lambda feat: encoder.fit_transform(feat))\ntest[['Sex', 'Embarked', 'Deck']] = test[['Sex', 'Embarked', 'Deck']].apply(lambda feat: encoder.fit_transform(feat))","d7790e6f":"# Separar as labels dos dados e retirar os ids\nids = test[\"PassengerId\"]\ny = train_df[\"Survived\"]\nX = train_df.drop(columns = [\"Survived\", \"PassengerId\"])\ntest = test.drop(columns = [\"PassengerId\"])","2822b195":"# Separar os dados em casos de teste e de treino\nX_train, X_test, y_train, y_test = train_test_split(X, y)","38a199e0":"from sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score","18c6503a":"decision_tree = tree.DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)","dc4633b2":"print(decision_tree.score(X_train, y_train))\nprint(cross_val_score(decision_tree, X, y).mean())","7c653781":"parameters = {'criterion' : ['gini', 'entropy'],\n              'splitter' : ['best', 'random'],\n              'max_depth': range(1, 15),\n              'min_samples_leaf' : range(1, 100, 10),\n             }\n\ncv = GridSearchCV(decision_tree, parameters)\n\ncv.fit(X, y)\n\nprint(cv.best_score_)","fc884978":"cv.best_params_","67fb3707":"tree_labels = cv.best_estimator_.predict(test)\nsubmission = pd.DataFrame(data= {'PassengerId' : ids, \"Survived\" : tree_labels})\nsubmission.to_csv('DecisionTree.csv', index=False)","299a30d9":"random_forest = RandomForestClassifier(n_estimators = 1000, oob_score = True)\nrandom_forest.fit(X_train, y_train)","7ba1f8ae":"print(random_forest.score(X_train, y_train))\nprint(cross_val_score(random_forest, X, y).mean())","32b46f1a":"parameters = {\n    'n_estimators' : [200],\n    'max_depth' : [8],\n    'max_features' : range(1, 10),\n} \n\ncv = GridSearchCV(random_forest, parameters)\n\ncv.fit(X, y)\n\nprint(cv.best_score_)","8103e975":"best_forest = RandomForestClassifier(n_estimators = 1400, max_depth = 8, max_features = 7)\nbest_forest.fit(X_train, y_train)","44da5328":"forest_labels = cv.best_estimator_.predict(test)\nsubmission = pd.DataFrame(data= {'PassengerId' : ids, \"Survived\" : forest_labels})\nsubmission.to_csv('RandomForest.csv', index=False)","7c9dc20b":"grad_boost = GradientBoostingClassifier()\ngrad_boost.fit(X_train, y_train)","fdd78844":"print(grad_boost.score(X_train, y_train))\nprint(grad_boost.score(X_test, y_test))","0d067942":"gdboost_labels = cv.best_estimator_.predict(test)\nsubmission = pd.DataFrame(data= {'PassengerId' : ids, \"Survived\" : gdboost_labels})\nsubmission.to_csv('GradientBoosting.csv', index=False)","a7ec98d6":"# An\u00e1lise dos Dados #\nFazer uns gr\u00e1ficos bonitinhos, s\u00f3 para tentarmos \"perceber\" os dados.","0ab89e11":"### Decision Tree","99bc08e6":"Ou seja, n\u00e3o temos dados para a Cabin, e consequentemente para o Deck, em 77% dos dados. Mas parece o facto de sabermos ou n\u00e3o em que Deck alguem estava \u00e9 bastante relevante.","081fc215":"### Random Forest","4c997b8b":"### Gradient Boosted Trees","8ac4bc70":"# Feature Creation\nVamos criar:\n* Deck - Extra\u00edda de Cabin. A primeira letra representa o Deck em que ficava a Cabin.\n* AgeIsNaN - Exactamente o que diz. Indica se Age \u00e9 ou n\u00e3o NaN.","f8d6ccf1":"# Model making\nAgora que j\u00e1 temos todas as features prontas, podemos come\u00e7ar a criar Models. \nVamos experimentar:\n* Decision Tree\n* Random Forest\n* Gradient Boosted Decision Trees\n","9633f094":"# Processamento dos Dados #\nAgora vamos s\u00f3 tentar processar um bocado os dados, de modo a poderem ser usados melhores pelos modelos. \n* Vamos aplicar feature scaling a \"Fare\" e \"Age\".\n* Vamos retirar Cabin e Ticket\n* Vamos converter as categorical features","07fb4779":"Agora podiamos fazer mais gr\u00e1ficos, com outros fatores. Por exemplo, fazer gr\u00e1ficos s\u00f3 para homens, ou s\u00f3 para adultos. Se tiver paci\u00eancia acrescento-os depois. "}}