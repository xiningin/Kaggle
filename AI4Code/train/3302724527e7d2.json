{"cell_type":{"e1f47da7":"code","aa01db6f":"code","ebfec2a2":"code","417a5029":"code","77fa10b2":"code","5bd978cd":"code","fcafc864":"code","312f553b":"code","ad84f5c3":"code","f1e56f13":"code","99165dd4":"code","38214f7e":"code","807bca12":"code","59f169e8":"code","79c0df48":"code","41f35061":"code","e96454d4":"code","6e1c6654":"markdown","5b22ac14":"markdown","6210533b":"markdown","ff5b6f20":"markdown","33e34bf9":"markdown","7ca7b99d":"markdown","c6b64fbe":"markdown","e4512936":"markdown","a9422d7a":"markdown","bc7cd8f6":"markdown","505f3a33":"markdown","3081cbaf":"markdown","55fb5ca9":"markdown","49781b75":"markdown","26f4b970":"markdown"},"source":{"e1f47da7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nimport os\nfrom os import listdir\n# common visualizing tools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nimport cv2\nplt.style.use('ggplot')\n# CNN layers and the Deep Learning model\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense , Flatten, Dropout\nfrom keras.optimizers import Adam,RMSprop\nfrom keras.utils import to_categorical\nfrom sklearn.metrics import precision_recall_curve\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nimport tensorflow as tf\n# post processing and model evaluation\nfrom scipy.misc import imread, imresize\nfrom sklearn.metrics import confusion_matrix ,classification_report\nfrom sklearn import metrics\n# splitting tool for the validation set\nfrom sklearn.model_selection import train_test_split\n\n# to block unnecesarry warnings for updates etc.\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/examples\/examples\/Examples\/\"))\n\n# Any results you write to the current directory are saved as output.","aa01db6f":"img_size = 64 #the size of the image to be resized with\ngrayscale_images = False #set true for RGB\nnum_class = 10 #num of classes to be classified\ntest_size = 0.2 #percent of test size \ndataset_path = \"..\/input\/turkish-digit\/dataset\/Dataset\/\" #training data directory\nos.listdir(dataset_path)\nchannels = 3 #initialize with 1 if gray scale set True\n","ebfec2a2":"def get_img(data_path):\n    # Getting image array from path:\n    img = imread(data_path)\n    img = imresize(img, (img_size, img_size,  channels))\n    return img","417a5029":"def get_dataset(dataset_path):\n    # Getting all data from data path:\n    try:\n        X = np.load('npy_dataset\/X.npy')\n        Y = np.load('npy_dataset\/Y.npy')\n    except:\n        labels = listdir(dataset_path) # Geting labels\n        X = []\n        Y = []\n        for i, label in enumerate(labels):\n            datas_path = dataset_path+'\/'+label\n            for data in listdir(datas_path):\n                img = get_img(datas_path+'\/'+data)\n                X.append(img)\n                Y.append(label)\n        # Create dateset:\n        X = 1-np.array(X).astype('float32')\/255. #here we normalize the images\n        Y = np.array(Y).astype('float32')\n        #Y = to_categorical(Y, num_class) \n        if not os.path.exists('npy_dataset\/'):\n            os.makedirs('npy_dataset\/')\n        np.save('npy_dataset\/X.npy', X)\n        np.save('npy_dataset\/Y.npy', Y)\n    X, X_test, Y, Y_test = train_test_split(X, Y, test_size=test_size, random_state=42)\n    return X, X_test, Y, Y_test\n","77fa10b2":"X, X_test, Y, Y_test = get_dataset(dataset_path) #calling the function","5bd978cd":"X = X.reshape(X.shape[0], img_size, img_size,channels)\nX_test = X_test.reshape(X_test.shape[0], img_size, img_size,channels)\nY = Y.reshape(Y.shape[0],1)\nY_test = Y_test.reshape(Y_test.shape[0], 1)","fcafc864":"plt.figure(figsize=(10,8))\nsns.countplot(x=Y[:,0])","312f553b":"model = tf.keras.models.Sequential([\n  tf.keras.layers.Conv2D(32, (5,5), activation='relu',padding='same', input_shape=(img_size, img_size, 3)),\n  tf.keras.layers.Conv2D(32,(5,5), activation='relu',padding='same'),\n  tf.keras.layers.MaxPooling2D(2, 2),\n  tf.keras.layers.Dropout((0.25)),\n  tf.keras.layers.Conv2D(64, (3,3), activation='relu',padding='same'),\n  tf.keras.layers.Conv2D(64, (3,3), activation='relu',padding='same'),\n  tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(2,2)),\n  tf.keras.layers.Dropout((0.25)),\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(512, activation='relu'),\n  tf.keras.layers.Dropout((0.5)),\n  tf.keras.layers.Dense(10, activation='softmax')\n])\n","ad84f5c3":"model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n#model.compile(optimizer = tf.keras.optimizers.Adam( epsilon=1e-08), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","f1e56f13":"train_datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly shift images horizontally (fraction of total width)\ntrain_datagen.fit(X)","99165dd4":"batch_size = 128\nhistory = model.fit_generator(train_datagen.flow(X,Y,batch_size = batch_size),\n                              epochs=70,\n                              verbose=2,\n                              steps_per_epoch=X.shape[0] \/\/ batch_size,\n                              validation_data=(X_test,Y_test),\n                              callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3,\n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)])","38214f7e":"%matplotlib inline\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nacc=history.history['acc']\nval_acc=history.history['val_acc']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.figure(figsize=(10,7))\nplt.plot(epochs, acc, 'r')\nplt.plot(epochs, val_acc, 'b')\nplt.title('Training and validation accuracy')\nplt.legend(['Training accuracy','Validation accuracy'])\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.figure(figsize=(10,7))\nplt.plot(epochs, loss, 'r')\nplt.plot(epochs, val_loss, 'b')\nplt.title('Training and validation loss')\nplt.legend(['Training loss','Validation loss'])\nplt.figure()\nmax(val_acc) #the best validation accuracy the model have got","807bca12":"print(\"Classification report  \\n%s\\n\"\n      % ( metrics.classification_report(Y_test, model.predict_classes(X_test))))\nprint(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(Y_test,model.predict_classes(X_test)))","59f169e8":"R = 3\nC = 4\nfig, axes = plt.subplots(R,C, figsize=(16,10))\n\nfor i in range(R):\n    for j in range(C):\n        r = np.random.randint(X_test.shape[0], size=1)[0]\n        axes[i, j].imshow(X_test[r][:,:, :])\n        axes[i, j].plot()\n        #print('this is a', name[y_testo[r][0]], '-------- prediction is:', name[predicto[r]])\n        axes[i, j].text(0, 0, 'Prediction: %s' % model.predict_classes(X_test)[r], color='w', backgroundcolor='k', alpha=0.8)\n        axes[i, j].text(1,8, 'LABEL: %s' % Y_test[r][0], color='k', backgroundcolor='w', alpha=0.8)\n        axes[i, j].axis('off')","79c0df48":"X_val = X_test\nY_val = Y_test\nY_pred_classes = model.predict_classes(X_val)\nY_pred_classes = Y_pred_classes.reshape(413,1)\nerrors = (Y_pred_classes - Y_val != 0)\nerrors_indicies = [i for i, x in enumerate(errors) if x]\nY_pred_classes_errors = Y_pred_classes[errors]\nY_true_errors = Y_val[errors]\nX_val_errors = X_val[errors_indicies,:,:,:]\n\nR = 3\nC = 4\nfig, axes = plt.subplots(R,C, figsize=(16,10))\n\nfor i in range(R):\n    for j in range(C):\n        r = np.random.randint(len(errors_indicies), size=1)[0]\n        axes[i, j].imshow(X_val_errors[r][:,:, :])\n        axes[i, j].plot()\n        #print('this is a', name[y_testo[r][0]], '-------- prediction is:', name[predicto[r]])\n        axes[i, j].text(0, 0, 'Prediction: %s' % Y_pred_classes_errors[r], color='w', backgroundcolor='k', alpha=0.8)\n        axes[i, j].text(1,8, 'LABEL: %s' % Y_true_errors[r], color='k', backgroundcolor='w', alpha=0.8)\n        axes[i, j].axis('off')","41f35061":"examples_path = \"..\/input\/examples\/examples\/Examples\/\" \nexamples = []\nfor data in listdir(examples_path):\n    img = get_img(examples_path+'\/'+data)\n    examples.append(img)\nexamples = 1-np.array(examples).astype('float32')\/255.","e96454d4":"examples_pred = model.predict_classes(examples)\n\nR = 2\nC = 5\nr = 0\nfig, axes = plt.subplots(R,C, figsize=(16,10))\nfor i in range(R):\n    for j in range(C):\n        axes[i, j].imshow(examples[r][:,:, :])\n        axes[i, j].plot()\n        #print('this is a', name[y_testo[r][0]], '-------- prediction is:', name[predicto[r]])\n        axes[i, j].text(0, 0, 'Prediction: %s' % examples_pred[r], color='w', backgroundcolor='k', alpha=0.8)\n        #axes[i, j].text(1,8, 'LABEL: %s' % Y_true_errors[r], color='k', backgroundcolor='w', alpha=0.8)\n        axes[i, j].axis('off')\n        r+=1\n        ","6e1c6654":"**visualization for some images which wrongly classified**","5b22ac14":"**fitting and trainig the model**\nto train faster i used batch_size , not to train all the data all at once every epoch ,then i used learning rate decay and it is very effective at late epochs ","6210533b":"the above curves shows that there is no over fitting at all as the validation and the training curve almost stucking together ,even the model is doing better on the validation rather than the training .","ff5b6f20":"**data augmantation**","33e34bf9":"**Confusion matrix and calssification report**\ni conclude from precision that there are some images predicted to be one and three while they aren't.\nalso recall shows that there are some twos and threes images predicted as another numbers and it is clear that they are either ones or threes.","7ca7b99d":"here the count plot shows the frequancy of each label so we can determine the balancing of the data , it is clear that the data is almost balanced all labels ranged from 150 to 175 .","c6b64fbe":"**testing the model while visualizing the output photos**","e4512936":"**variables initialization**","a9422d7a":"**visualiziation for random sample from the validation data**","bc7cd8f6":"**structuring the model**\nmy CNN architechture is In -> [Conv2D->relu*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out(softmax)","505f3a33":"here to set the optimizer and the loss function i have tried Adam and RMSprop optimizers and they are almost gave the same accuracy in our case ,also i used sparse_categorical_crossentropy loss as the data is categorical and the labels are labeled not one hot encoded","3081cbaf":"**reshaping training arrays **","55fb5ca9":"**loading the examples images to test the model **","49781b75":"i have played with data augmentation so as to get the best accuracy here is what i have got","26f4b970":"****functions to get the images from the given path,resize it and label it****"}}