{"cell_type":{"ea7127e5":"code","4aed860f":"code","2217ab9b":"code","106250af":"code","f525f6bd":"code","2add789e":"code","19a281b2":"code","2013691d":"code","90f32564":"code","d7bd0e02":"code","d1126433":"code","29c4e77e":"code","a388728b":"code","08e57f1e":"code","4dac49d5":"code","ea02f402":"code","d8c3a116":"code","9a4a38b1":"code","c5d59d8f":"code","9718dcaf":"code","4147f4fa":"code","39f64b6a":"code","ada4510e":"code","68814698":"code","30f15e5c":"code","02db5ec1":"code","95ce4034":"code","6701c6c7":"markdown","a5d114dd":"markdown","6c62248b":"markdown"},"source":{"ea7127e5":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score#\nfrom sklearn.preprocessing import StandardScaler\n\nimport optuna\nfrom sklearn.model_selection import train_test_split\nfrom optuna.pruners import SuccessiveHalvingPruner","4aed860f":"train_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv\")\nsample_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-apr-2021\/sample_submission.csv\")","2217ab9b":"train_df.tail()","106250af":"train_df.drop(\"PassengerId\", axis=1, inplace=True)\ntest_df.drop(\"PassengerId\", axis=1, inplace=True)","f525f6bd":"def cabin_feat(df):\n    df[\"Cabin\"] = df[\"Cabin\"].fillna(\"None\")\n    df[\"has_Cabin\"] = df[\"Cabin\"].apply(lambda x: 1 if x != \"None\" else 0)\n    df[\"Deck\"] = df[\"Cabin\"].apply(lambda x: x[0])\n    df.drop(\"Cabin\", axis=1, inplace=True)\n    \ncabin_feat(train_df)\ncabin_feat(test_df)","2add789e":"def fill_nan(df, group_col, col):\n    \"\"\"\n    This function fill nan values in given column \n    based on groupby column.\n    \"\"\"\n    mask_dict = df.groupby(group_col).mean()[col].to_dict()\n    missing_mask = df[col].isna()\n    df.loc[missing_mask, col] = df.loc[missing_mask, group_col].map(mask_dict)\n    \nfill_nan(train_df, \"Pclass\", \"Age\")\nfill_nan(test_df, \"Pclass\", \"Age\")\nfill_nan(train_df, \"Deck\", \"Fare\")\nfill_nan(test_df, \"Deck\", \"Fare\")","19a281b2":"train_df[\"Fare\"] = train_df[\"Fare\"].apply(lambda x: np.log(x) if x != 0 else 0)\ntest_df[\"Fare\"] = test_df[\"Fare\"].apply(lambda x: np.log(x) if x != 0 else 0)","2013691d":"train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(train_df[\"Embarked\"].mode()[0])\ntest_df[\"Embarked\"] = test_df[\"Embarked\"].fillna(test_df[\"Embarked\"].mode()[0])","90f32564":"train_df[\"Ticket\"] = train_df[\"Ticket\"].fillna(\"NAN\")\ntest_df[\"Ticket\"] = test_df[\"Ticket\"].fillna(\"NAN\")\ntrain_df[\"Ticket\"] = train_df[\"Ticket\"].apply(lambda x: str(x)[:2])\ntest_df[\"Ticket\"] = test_df[\"Ticket\"].apply(lambda x: str(x)[:2])","d7bd0e02":"train_df[\"FamilySize\"] = train_df[\"SibSp\"] + train_df[\"Parch\"]\ntest_df[\"FamilySize\"] = test_df[\"SibSp\"] + test_df[\"Parch\"]","d1126433":"train_df[\"Name_length\"] = train_df[\"Name\"].apply(lambda x: len(x.split(\",\")[0] + x.split(\",\")[1].strip()))\ntest_df[\"Name_length\"] = test_df[\"Name\"].apply(lambda x: len(x.split(\",\")[0] + x.split(\",\")[1].strip()))\ntrain_df[\"Last_name\"] = train_df[\"Name\"].apply(lambda x: x.split(\",\")[0])\ntrain_df[\"First_name\"] = train_df[\"Name\"].apply(lambda x: x.split(\",\")[1].strip())\ntest_df[\"Last_name\"] = test_df[\"Name\"].apply(lambda x: x.split(\",\")[0])\ntest_df[\"First_name\"] = test_df[\"Name\"].apply(lambda x: x.split(\",\")[1].strip())\ntrain_df.drop(\"Name\", axis=1, inplace=True)\ntest_df.drop(\"Name\", axis=1, inplace=True)","29c4e77e":"def age_feat(x):\n    if x <= 5:\n        return \"baby\"\n    elif 5 < x <= 16:\n        return \"teen\"\n    elif 16 < x <= 30:\n        return \"yound_adult\"\n    elif 30 < x <= 50:\n        return \"adult\"\n    else:\n        return \"elder\"","a388728b":"train_df[\"age_range\"] = train_df[\"Age\"].apply(age_feat)\ntest_df[\"age_range\"] = test_df[\"Age\"].apply(age_feat)","08e57f1e":"enc_cols = [col for col in train_df.select_dtypes(\"object\").columns]\n\ndef label_encoder():\n    for col in enc_cols:\n        le = LabelEncoder()\n        le.fit(train_df[col].values.tolist() + test_df[col].values.tolist())\n        train_df.loc[:, col] = le.transform(train_df[col].values)\n        test_df.loc[:, col] = le.transform(test_df[col].values)\n\nlabel_encoder()","4dac49d5":"def run_training(algo, df, test_df, fold, oof):\n    train_df = df[df.kfold != fold].reset_index(drop=True)\n    valid_df = df[df.kfold == fold].reset_index(drop=True)\n    \n    xtrain = train_df.drop([\"Survived\", \"kfold\"], axis=1)\n    xvalid = valid_df.drop([\"Survived\", \"kfold\"], axis=1)\n    \n    sc = StandardScaler()\n    xtrain = sc.fit_transform(xtrain)\n    xvalid = sc.transform(xvalid)\n    test_df = sc.transform(test_df)\n    \n    ytrain = train_df[\"Survived\"].values\n    yvalid = valid_df[\"Survived\"].values\n    \n    algo.fit(xtrain, ytrain)\n    preds = algo.predict(xvalid)\n    sub_proba = algo.predict_proba(test_df)[:, 1]\n    train_proba = algo.predict_proba(xvalid)[:, 1]\n    \n    fold_acc = accuracy_score(yvalid, preds)\n    \n    print(f\"fold={fold+1}, accuracy={fold_acc}\")\n    print(\"\\n\")\n    oof[valid_idx] += fold_acc\n    \n    return oof, sub_proba, algo, train_proba","ea02f402":"from sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=5)\ntrain_df[\"kfold\"] = -1\n\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\n\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(X=train_df, y=train_df[\"Survived\"])):\n    train_df.loc[valid_idx, \"kfold\"] = fold","d8c3a116":"rfc = RandomForestClassifier(n_estimators=150)\n\nlevel2_df = pd.DataFrame()\ndf_proba = pd.DataFrame()\n\ntest_proba = np.zeros(len(test_df))\noof = np.zeros(len(train_df))\ntrain_pred = []\nfor fold in range(5):\n    oof, proba, rfc_model, tt_pred = run_training(rfc,train_df, test_df, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n    \nlevel2_df[\"randomforest\"] = np.hstack(train_pred)  \ndf_proba[\"randomforest\"] = test_proba \/ 5\nprint(f\"Mean accuracy after 5 folds {np.mean(oof)}\")","9a4a38b1":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(use_label_encoder=False)\n\ntest_proba = np.zeros(len(test_df))\noof = np.zeros(len(train_df))\ntrain_pred = []\nfor fold in range(5):\n    oof, proba, xgb_model, tt_pred = run_training(xgb,train_df, test_df, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n    \nlevel2_df[\"xgboost\"] = np.hstack(train_pred)\ndf_proba[\"xgboost\"] = test_proba \/ 5\nprint(f\"Mean accuracy after 5 folds {np.mean(oof)}\")","c5d59d8f":"from lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier()\n\ntest_proba = np.zeros(len(test_df))\noof = np.zeros(len(train_df))\ntrain_pred = []\nfor fold in range(5):\n    oof, proba, lgbm_model, tt_pred = run_training(lgbm,train_df, test_df, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n\nlevel2_df[\"lgbm\"] = np.hstack(train_pred)\ndf_proba[\"lgbm\"] = test_proba \/ 5\nprint(f\"Mean accuracy after 5 folds {np.mean(oof)}\")","9718dcaf":"df_proba[\"wavg\"] = 0.1 * df_proba[\"randomforest\"] + 0.2 * df_proba[\"xgboost\"] + 0.7 * df_proba[\"lgbm\"]\ndf_proba[\"binary_wavg\"] = np.where(df_proba[\"wavg\"] > 0.5, 1, 0)","4147f4fa":"submission = sample_df.copy()","39f64b6a":"submission[\"Survived\"] = np.where(df_proba[\"lgbm\"] > 0.5, 1, 0)\nsubmission.to_csv(\"new_5fold_lgbm.csv\", index=False)","ada4510e":"# Submit weight average of 3 \nsubmission[\"Survived\"] = df_proba[\"binary_wavg\"]\nsubmission.to_csv(\"new_sub_wavg5.csv\", index=False)","68814698":"import lightgbm as lgbm\n\nfold_params_dict = {}\n\nfor fold in range(5):\n    def objective(trial):\n        train = train_df[train_df.kfold != fold].reset_index(drop=True)\n        valid = train_df[train_df.kfold == fold].reset_index(drop=True)\n        \n        xtrain = train.drop([\"Survived\", \"kfold\"], axis=1)\n        ytrain = train[\"Survived\"].values\n        xvalid = valid.drop([\"Survived\", \"kfold\"], axis=1)\n        yvalid = valid[\"Survived\"].values\n        \n        sc = StandardScaler()\n        xtrain = sc.fit_transform(xtrain)\n        xvalid = sc.transform(xvalid)\n        \n        dtrain = lgbm.Dataset(xtrain, label=ytrain)\n        dvalid = lgbm.Dataset(xvalid, label=yvalid)\n        params = {\n                \"objective\": \"binary\",\n                \"metric\": \"binary_logloss\",\n                \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1),\n                \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n                \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n                \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n                \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n                \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n                \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n                \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100)\n        }\n    \n        gbm = lgbm.train(params, dtrain, valid_sets=[dtrain, dvalid], early_stopping_rounds=100)\n        preds = gbm.predict(xvalid)\n        pred_labels = np.rint(preds)\n        accuracy = accuracy_score(yvalid, pred_labels)\n        return accuracy\n        \n    study = optuna.create_study(direction=\"maximize\", pruner=SuccessiveHalvingPruner())\n    study.optimize(objective, n_trials=200)\n    fold_params_dict[fold] = study.best_trial.params\n        \n    print(\"Number of finished trials:\", len(study.trials))\n    print(\"Best trial:\", study.best_trial.params)","30f15e5c":"test_proba = np.zeros(len(test_df))\noof = np.zeros(len(train_df))\ntrain_pred = []\nfor fold in range(5):\n    lgbm = LGBMClassifier(**fold_params_dict[fold])\n    oof, proba, lgbm_model, tt_pred = run_training(lgbm,train_df, test_df, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n\nlevel2_df[\"lgbm_optuna\"] = np.hstack(train_pred)\ndf_proba[\"lgbm_optuna\"] = test_proba \/ 5\nprint(f\"Mean accuracy after 5 folds {np.mean(oof)}\")","02db5ec1":"submission[\"Survived\"] = np.where(df_proba[\"lgbm\"] > 0.5, 1, 0)\nsubmission.to_csv(\"new_optuna_lgbm_5fold.csv\", index=False)","95ce4034":"import torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.optim as optim\nimport torch.utils.data\nfrom torch.autograd import Variable","6701c6c7":"Mean accuracy after 5 folds 0.7857200000000003 to beat","a5d114dd":"## Optuna","6c62248b":"## Torch model"}}