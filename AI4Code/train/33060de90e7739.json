{"cell_type":{"b54b96df":"code","fedd0fd7":"code","ec32a483":"code","33e350aa":"code","b4e3b24b":"code","34ae07f7":"code","70047736":"code","7e5b0f3b":"code","bb5a8a6f":"code","de14b3d1":"code","fe0c46f4":"code","a276f2c1":"code","fb086ffc":"code","014e0b44":"code","196ff49c":"code","f1b00c26":"code","62f2fb34":"code","68ab230d":"code","b5c9af84":"code","e25aa88d":"code","967542e1":"code","9dc04fbe":"code","661be2eb":"code","8b5a4c83":"code","5ca9322f":"code","23312d80":"code","5a051d1c":"code","aa6fc5ae":"code","914f24cb":"code","317a018a":"code","459df6da":"code","cadb09f4":"code","cadf74ce":"code","968223a1":"code","3da02d63":"code","7d44f385":"code","3adf337e":"code","751b9b6d":"code","13c0b4a5":"code","b9ad9841":"code","0da5ebf2":"code","33b4c6a9":"code","6db991af":"code","0b80b405":"code","ee31456c":"code","732af267":"code","1ade1877":"code","bc198853":"code","70e97764":"code","3ab1eb34":"code","1433edc0":"code","dfe99044":"code","886b1a28":"code","0d0d6c86":"code","238156f8":"code","6c668ac7":"code","ac8f8f33":"code","fa4f96ac":"code","b39485be":"markdown","40e2c454":"markdown","486aaf9e":"markdown","42a222f6":"markdown","cc19d16a":"markdown"},"source":{"b54b96df":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","fedd0fd7":"import os\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nfrom IPython import display","ec32a483":"from pathlib import Path\ngif_path = '..\/input\/gifs-for-titanic-challenge\/Overall Screen.gif'\nwith open(gif_path,'rb') as f:\n  display.Image(data=f.read(), format='png')\n\nprint(\"Overall Gif Screen\")","33e350aa":"gif_path1 = '..\/input\/gifs-for-titanic-challenge\/Dividing Based On Male and Female.gif'\nwith open(gif_path1,'rb') as f:\n  display.Image(data=f.read(), format='png')","b4e3b24b":"gif_path2 = '..\/input\/gifs-for-titanic-challenge\/Total Survived.gif'\nwith open(gif_path2,'rb') as f:\n  display.Image(data=f.read(), format='png')","34ae07f7":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns","70047736":"train_set = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_set = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain_set.head(3)\ntest_set.head(3)\n\n##Train Set Info and Descriptive Statistics\ntrain_set.info()\ntrain_set.describe()\n\n##Test Set Info and Descriptive Statistics\ntest_set.info()\ntest_set.describe()","7e5b0f3b":"t1 = train_set[['Sex', 'Survived']].groupby(['Sex'], as_index=True).mean()\nt2 = train_set[['Pclass', 'Survived']].groupby(['Pclass'], as_index = True).mean()\nt3 = train_set[['SibSp', 'Survived']].groupby(['SibSp'], as_index = True).mean()\nt4 = train_set[['Embarked', 'Survived']].groupby(['Embarked'], as_index = True).mean()\nt5 = train_set[['Parch', 'Survived']].groupby(['Parch'], as_index = True).mean()\nt6 = train_set[['Fare', 'Survived']].groupby(['Fare'], as_index = True).mean()\n\n## Displaying Certain Columns and How they are related to survival\nimport pandas as pd   \nfrom IPython.display import display_html \n\ndf1_styler = t1.style.set_table_attributes(\"style='display:inline'\").set_caption('% of Survived based on Sex')\ndf2_styler = t2.style.set_table_attributes(\"style='display:inline'\").set_caption('% of Survived based on Class')\ndf3_styler = t3.style.set_table_attributes(\"style='display:inline'\").set_caption('% of Survived based on Sibling and Spouses')\ndf4_styler = t4.style.set_table_attributes(\"style='display:inline'\").set_caption('% of Survived based on Embark Location')\ndf5_styler = t5.style.set_table_attributes(\"style='display:inline'\").set_caption('% of Survived based on Parents and Children')\ndf6_styler = t6.style.set_table_attributes(\"style='display:inline'\").set_caption('% of Survived based on Fare')\n\nspace = \"\\xa0\" * 13\ndisplay_html(df1_styler._repr_html_()+ space + df2_styler._repr_html_() + space + df3_styler._repr_html_() + space + df4_styler._repr_html_() + space + df5_styler._repr_html_(), raw=True)","bb5a8a6f":"# duplicate the train set \nDTrain = train_set\nDTest = test_set\n#Replace Sex First- male:0 and female:1\nDTrain['Sex'].replace(to_replace = 'male', value = 0, inplace = True)\nDTrain['Sex'].replace(to_replace = 'female', value = 1, inplace = True)\nDTest['Sex'].replace(to_replace = 'male', value = 0, inplace = True)\nDTest['Sex'].replace(to_replace = 'female', value = 1, inplace = True)\nDTrain.head(3)\nDTest.head(3)","de14b3d1":"# Drop Passenger ID and Ticket and name\nDTrain.drop(['PassengerId','Ticket'], axis = 1, inplace = True)\nDTest.drop(['PassengerId','Ticket'], axis = 1, inplace = True)\nDTrain.head(3)\nDTest.head(3)","fe0c46f4":"#Replacing the Values of Embarked in Training and Test Set\nDTrain['Embarked'].replace(to_replace = 'C', value = 1, inplace = True)\nDTrain['Embarked'].replace(to_replace = 'Q', value = 2, inplace = True)\nDTrain['Embarked'].replace(to_replace = 'S', value = 3, inplace = True)\nDTest['Embarked'].replace(to_replace = 'C', value = 1, inplace = True)\nDTest['Embarked'].replace(to_replace = 'Q', value = 2, inplace = True)\nDTest['Embarked'].replace(to_replace = 'S', value = 3, inplace = True)\n\nDTrain['Embarked'].fillna(\"UnK\", inplace = True)\nDTest['Embarked'].fillna(\"UnK\", inplace = True)\nDTest['Fare'].fillna((DTest['Fare'].median()), inplace = True)\nDTrain['Embarked'].replace(to_replace = 'UnK', value = 4, inplace = True)\nDTest['Embarked'].replace(to_replace = 'UnK', value = 4, inplace = True)","a276f2c1":"#Making another Duplicate Dataframe\nDTS2 = pd.read_csv('..\/input\/titanic\/train.csv')\nDTeS2 = pd.read_csv('..\/input\/titanic\/test.csv')","fb086ffc":"DTrain['Cabin'].fillna(\"NS\", inplace = True)\nDTrain['Cabin'].isna().sum()\nDTest['Cabin'].fillna(\"NS\", inplace = True)\nDTest['Cabin'].isna().sum()\nDTest['Fare'].fillna((DTest['Fare'].median()), inplace = True)\n\n## Filling the Missing Age Values using Simple Imputer\n\nfrom sklearn.impute import SimpleImputer \nimr = SimpleImputer(missing_values=np.nan, strategy='median')\nimr = imr.fit(DTrain[['Age']])\nDTrain['Age'] = imr.transform(DTrain[['Age']])\n\nimr1 = SimpleImputer(missing_values=np.nan, strategy='median')\nimr1 = imr.fit(DTest[['Age']])\nDTest['Age'] = imr1.transform(DTest[['Age']])","014e0b44":"DTrain.info()\nprint('\\n')\nDTest.info()\nprint(\"All Data should be completely Filled\")","196ff49c":"DTrain['Age_Divide'] = pd.cut(DTrain['Age'],6)\nt8 = DTrain.groupby('Age_Divide', as_index=False)['Survived'].mean().sort_values(by = 'Survived', ascending = False)\nDTrain['Age_Divide_Q'] = pd.qcut(DTrain['Age'],5, duplicates = 'drop')\nt9 = DTrain.groupby('Age_Divide_Q', as_index=False)['Survived'].mean().sort_values(by = 'Survived', ascending = False)\nDTrain['Fare_Divide'] = pd.cut(DTrain['Fare'],7)\nt10 = DTrain.groupby('Fare_Divide', as_index=False)['Survived'].mean().sort_values(by = 'Survived', ascending = False)\nDTrain['Fare_Divide_Q'] = pd.qcut(DTrain['Fare'],4)\nt11 = DTrain.groupby('Fare_Divide_Q', as_index=False)['Survived'].mean().sort_values(by = 'Survived', ascending = False)\n\ndf7_styler = t8.style.set_table_attributes(\"style='display:inline'\").set_caption('Age Band Based Survival %')\ndf8_styler = t9.style.set_table_attributes(\"style='display:inline'\").set_caption('Age Band Based Survival %(Quartile Based Cut)')\ndf9_styler = t10.style.set_table_attributes(\"style='display:inline'\").set_caption('Fare Band Based Survival %')\ndf10_styler = t11.style.set_table_attributes(\"style='display:inline'\").set_caption('Fare Band Based Survival % (Quartile Based Cut)')\nspace = \"\\xa0\" * 7\ndisplay_html(df7_styler._repr_html_()+ space + df8_styler._repr_html_() + space + df9_styler._repr_html_() + space + df10_styler._repr_html_(), raw=True)","f1b00c26":"# Dividing Age into 5 Bands \n# 0:20 - 0, 20:25 - 1, 25:30 - 2, 30:40 - 3, 40:80 - 4\nr = [0, 20, 25, 30, 40, 80]\ng = [0,1,2,3,4]\nDTrain['Age'] = pd.cut(DTrain['Age'], bins=r, labels=g)\nDTest['Age'] = pd.cut(DTest['Age'], bins=r, labels=g)","62f2fb34":"#Dividing Fare into 4 Bands\n# -.001:8 - 0, 8:15 - 1, 15:31 - 2, 31:513-3\nr1 = [-0.001,8,15,31,513]\ng1 = [0,1,2,3]\nDTrain['Fare'] = pd.cut(DTrain['Fare'], bins = r1, labels = g1)\nDTest['Fare'] = pd.cut(DTest['Fare'], bins = r1, labels = g1)","68ab230d":"#Reducing the Cabin to a Single Character to see Survival%\nDTrain['Cabin'] = DTrain['Cabin'].astype(str).str[0]\nDTest['Cabin'] = DTest['Cabin'].astype(str).str[0]\nt12 = DTrain[['Cabin', 'Survived']].groupby(['Cabin'], as_index = True).mean().sort_values(by = 'Survived', ascending = False)\nprint(t12)\nDTrain.head(5)\nDTrain.head(5)","b5c9af84":"# Now we are Putting the Cabin Survival Percentage into 4 Categories \n# 0: Survival% between 0-49%\n# 1: Survival% between 50-69%\n# 2: Survival% >70%\n# 3: Those Whose Cabin Hasn't been given ('NS' Value)\n\nDTrain['Cabin'].replace(to_replace = 'T', value = 0, inplace = True)\nDTrain['Cabin'].replace(to_replace = 'A', value = 0, inplace = True)\nDTrain['Cabin'].replace(to_replace = 'C', value = 1, inplace = True)\nDTrain['Cabin'].replace(to_replace = 'F', value = 1, inplace = True)\nDTrain['Cabin'].replace(to_replace = 'G', value = 1, inplace = True)\nDTrain['Cabin'].replace(to_replace = 'B', value = 2, inplace = True)\nDTrain['Cabin'].replace(to_replace = 'D', value = 2, inplace = True)\nDTrain['Cabin'].replace(to_replace = 'E', value = 2, inplace = True)\nDTrain['Cabin'].replace(to_replace = 'N', value = 3, inplace = True)\n\nDTest['Cabin'].replace(to_replace = 'T', value = 0, inplace = True)\nDTest['Cabin'].replace(to_replace = 'A', value = 0, inplace = True)\nDTest['Cabin'].replace(to_replace = 'C', value = 1, inplace = True)\nDTest['Cabin'].replace(to_replace = 'F', value = 1, inplace = True)\nDTest['Cabin'].replace(to_replace = 'G', value = 1, inplace = True)\nDTest['Cabin'].replace(to_replace = 'B', value = 2, inplace = True)\nDTest['Cabin'].replace(to_replace = 'D', value = 2, inplace = True)\nDTest['Cabin'].replace(to_replace = 'E', value = 2, inplace = True)\nDTest['Cabin'].replace(to_replace = 'N', value = 3, inplace = True)","e25aa88d":"t13 = DTrain[['Cabin', 'Survived']].groupby(['Cabin'], as_index = True).mean().sort_values(by = 'Survived', ascending = False)\nprint(t13)","967542e1":"#Dropping Additional Columns and Checking if there is any Missing data before model building\nDTrain.drop(['Age_Divide','Age_Divide_Q','Fare_Divide','Fare_Divide_Q'], axis = 1, inplace = True)\nDTrain.info()\nprint('\\n')\nDTest.info()","9dc04fbe":"#Model Builder# Starting off with Random Forest \nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nY_Train = DTrain['Survived']\nfeats = ['Pclass','Sex','Age','SibSp','Parch','Fare','Cabin','Embarked']\nX_Train = DTrain[feats]\nX_Test = DTest[feats]\n\nXTrain, XVal, yTrain, yVal = train_test_split(X_Train, Y_Train, test_size=0.1, random_state=27)","661be2eb":"Rand_Forest = RandomForestClassifier(n_estimators=130, max_depth=13, random_state=1)\nRand_Forest.fit(XTrain, yTrain)\n# predict\nPredictions = Rand_Forest.predict(X_Train)\nPredictions2 = Rand_Forest.predict(XVal)\n#accuracy = accuracy_score(X_Test, Predictions2)\n# evaluate\nRF_Acc = ((Rand_Forest.score(XTrain, yTrain))*100)\nprint('\\n')\nprint('Accuracy Score: ', RF_Acc)\nprint('\\n')\nprint('\\nConfusion Matrix: ', confusion_matrix(Y_Train, Predictions))\nprint('\\n')\nprint('\\nClassification Report: ', classification_report(Y_Train, Predictions))\nprint('\\n')\nprint(Predictions2)","8b5a4c83":"importances = Rand_Forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in Rand_Forest.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\nfeature_df = pd.DataFrame(X_Train.columns)\nfeature_df.columns = ['Importance']\n\n# print feature ranking\nfeature_df['Feature'] = pd.Series(importances)\nfeature_df.sort_values(by='Feature', ascending=False, inplace=True)\nprint(feature_df)","5ca9322f":"# Using SVM \nfrom sklearn import svm\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score, KFold\nkf = KFold(n_splits=10,random_state=1, shuffle = True)\n\nsvc = SVC(C = 200, kernel = 'rbf',  gamma = 0.1)\nsvc.fit(X_Train, Y_Train) \nacc_SVM = cross_val_score(svc,X_Train,Y_Train,cv=kf)\nprint('Predicted Accuracy Score is', (acc_SVM.mean()*100))","23312d80":"#Finding Optimal Hyperparameter tuning using Gridsearch\nfrom sklearn.model_selection import GridSearchCV \nparam_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'linear', 'sigmoid']}\nsvc_gs = GridSearchCV(SVC(), param_grid, refit = True, verbose = 2) \n# fitting the model for grid search \nsvc_gs.fit(X_Train, Y_Train)\nacc_SVM_gs = cross_val_score(svc_gs,X_Train,Y_Train)\nprint('The Best Params which we have found are \\n', svc_gs.best_params_)\nprint('The Optimal Estimators we have obtained from GridSearch Are \\n', svc_gs.best_estimator_)","5a051d1c":"print('Predicted Accuracy Score is', (acc_SVM_gs.mean()*100))\n","aa6fc5ae":"# Using the K-Nearest Neighbors Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nKnn = KNeighborsClassifier(n_neighbors = 3) \nKnn.fit(X_Train, Y_Train)  \nY_pred = Knn.predict(X_Test)  \nacc_knn = round(Knn.score(X_Train, Y_Train) * 100, 2)\nprint('KNN Acuuracy is : ', acc_knn)","914f24cb":"#Building a Voting Classifier Hard and Soft\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier \n\nLR = LogisticRegression()\nDT = DecisionTreeClassifier()\nGNB = GaussianNB()\nClassifiers = [('Logistic Regression', LR), ('Decision Trees', DT), ('Naive Bayes Classifier', GNB)]\nVoting_Clf = VotingClassifier(estimators = Classifiers, voting = 'hard')\nVoting_Clf_Soft = VotingClassifier(estimators = Classifiers, voting = 'soft')","317a018a":"#Splitting Our Data to Improve Accuracy\nfrom sklearn.metrics import accuracy_score, f1_score, log_loss\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\n# Make the train\/test data from validation\n\nX_tr, X_val, y_tr, y_val = train_test_split(X_Train, Y_Train, test_size=0.1, random_state=27)","459df6da":"# Hard Classifier Fit\nVoting_Clf.fit(X_tr, y_tr)\npreds = Voting_Clf.predict(X_val)\nacc = accuracy_score(y_val, preds)\nl_loss = log_loss(y_val, preds)\nf1 = f1_score(y_val, preds)\nprint(\"Accuracy is: \" + str(acc))\nprint(\"Log Loss is: \" + str(l_loss))\nprint(\"F1 Score is: \" + str(f1))","cadb09f4":"preds = Voting_Clf.predict(X_Test)\nprint(preds)","cadf74ce":"#Voting Classifier (Soft Voting Fit)\nVoting_Clf_Soft.fit(X_tr, y_tr)\npreds_s = Voting_Clf_Soft.predict(X_val)\nacc_s = accuracy_score(y_val, preds_s)\nl_loss_s = log_loss(y_val, preds_s)\nf1_s = f1_score(y_val, preds_s)\n\nprint(\"Accuracy is: \" + str(acc_s))\nprint(\"Log Loss is: \" + str(l_loss_s))\nprint(\"F1 Score is: \" + str(f1_s))","968223a1":"Table1 = {'Name of Classifier':['Random Forest', 'SVM', 'SVM w GridSearch', 'K Nearest Neighbors', 'Voting Clf(Hard Voting)','Voting Clf(Soft Voting)' ], \n        'Accuracy %':[RF_Acc, acc_SVM.mean()*100, acc_SVM_gs.mean()*100, acc_knn, (acc*100), (acc_s*100)]} \n# Create DataFrame \npd.set_option('display.max_columns', None)\nAcc_Table = pd.DataFrame(Table1) \nprint(Acc_Table)","3da02d63":"# Now that we have seen all The Accuracies \n#we get with these features, lets add another \n#feature to try and increase the accuracy. \n#For this we will be adding the family size parameter = the siblingsp + the Parch Column\nDTrain[\"Family_Size\"] = DTrain['SibSp'] + DTrain['Parch'] + 1\nDTest[\"Family_Size\"] = DTest['SibSp'] + DTest['Parch'] + 1","7d44f385":"t13 = DTrain[['Family_Size', 'Survived']].groupby(['Family_Size'], as_index = True).mean().sort_values(by = 'Survived', ascending = False)\nprint(t13)","3adf337e":"r2 = [0,1,6,11]\ng2 = [0,1,2]\nDTrain['Family_Size'] = pd.cut(DTrain['Family_Size'], bins=r2, labels=g2)\nDTest['Family_Size'] = pd.cut(DTest['Family_Size'], bins=r2, labels=g2)\nt14 = DTrain[['Family_Size', 'Survived']].groupby(['Family_Size'], as_index = True).mean().sort_values(by = 'Survived', ascending = False)\nprint(t14)","751b9b6d":"DTrain['Title'] = DTrain['Name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())\nDTest['Title'] = DTest['Name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())\nDTrain['Title'].value_counts()\nDTest['Title'].value_counts()","13c0b4a5":"DTrain['Title'] = DTrain['Title'].replace(['Dr','Rev', 'Mlle', 'Major', 'Col', 'Lady', 'Mme','Sir', 'Capt', 'Jonkheer', 'Don', 'Ms', 'the Countess'], 'Sparse')\nDTest['Title'] = DTest['Title'].replace(['Dr', 'Rev', 'Dona', 'Mme', 'Col','Ms'], 'Sparse')\nDTrain['Title'].value_counts()\nDTest['Title'].value_counts()","b9ad9841":"t15 = DTrain[['Title', 'Survived']].groupby(['Title'], as_index = True).mean().sort_values(by = 'Survived', ascending = False)\nprint(t15)\nDTrain['Title_Replace_Sex'] = DTrain['Title']\nDTrain['Title_Replace_Ind'] = DTrain['Title']\nDTest['Title_Replace_Sex'] = DTest['Title']\nDTest['Title_Replace_Ind'] = DTest['Title']\nDTrain.head(3)\nDTest.head(3)","0da5ebf2":"# I feel there are 2 ways to replace the values in the Title Dataframe \n# The first way is to change it based on sex i.e. Mr and Master as 0 and \n# Miss and Mrs. as 1 and Sparse as 2 just to maintain the uniformity \nDTrain['Title_Replace_Sex'].replace(to_replace = 'Mr', value = 0, inplace = True)\nDTrain['Title_Replace_Sex'].replace(to_replace = 'Miss', value = 1, inplace = True)\nDTrain['Title_Replace_Sex'].replace(to_replace = 'Mrs', value = 1, inplace = True)\nDTrain['Title_Replace_Sex'].replace(to_replace = 'Master', value = 0, inplace = True)\nDTrain['Title_Replace_Sex'].replace(to_replace = 'Sparse', value = 2, inplace = True)\n\nDTest['Title_Replace_Sex'].replace(to_replace = 'Mr', value = 0, inplace = True)\nDTest['Title_Replace_Sex'].replace(to_replace = 'Miss', value = 1, inplace = True)\nDTest['Title_Replace_Sex'].replace(to_replace = 'Mrs', value = 1, inplace = True)\nDTest['Title_Replace_Sex'].replace(to_replace = 'Master', value = 0, inplace = True)\nDTest['Title_Replace_Sex'].replace(to_replace = 'Sparse', value = 2, inplace = True)","33b4c6a9":"# the second way to replace the vlaues would be to replace all the values independently \n# Hence the Names Title_replace_Sex, and Title_Replace_ind  and we will calculate all the values of accuracy independently\n\nDTrain['Title_Replace_Ind'].replace(to_replace = 'Mr', value = 1, inplace = True)\nDTrain['Title_Replace_Ind'].replace(to_replace = 'Miss', value = 2, inplace = True)\nDTrain['Title_Replace_Ind'].replace(to_replace = 'Mrs', value = 3, inplace = True)\nDTrain['Title_Replace_Ind'].replace(to_replace = 'Master', value = 4, inplace = True)\nDTrain['Title_Replace_Ind'].replace(to_replace = 'Sparse', value = 5, inplace = True)\n\nDTest['Title_Replace_Ind'].replace(to_replace = 'Mr', value = 1, inplace = True)\nDTest['Title_Replace_Ind'].replace(to_replace = 'Miss', value = 2, inplace = True)\nDTest['Title_Replace_Ind'].replace(to_replace = 'Mrs', value = 3, inplace = True)\nDTest['Title_Replace_Ind'].replace(to_replace = 'Master', value = 4, inplace = True)\nDTest['Title_Replace_Ind'].replace(to_replace = 'Sparse', value = 5, inplace = True)","6db991af":"DTrain.head(10)\nDTest.head(10)","0b80b405":"DTrain['Title_Replace_Sex'].value_counts()\nDTest['Title_Replace_Sex'].value_counts()\nDTrain['Title_Replace_Ind'].value_counts()\nDTest['Title_Replace_Ind'].value_counts()","ee31456c":"DTrain.drop(['Name','Title'], axis = 1, inplace = True)\nDTest.drop(['Name','Title'], axis = 1, inplace = True)","732af267":"# Trying with the Title Replaced as Sex Column\nY_Train1 = DTrain['Survived']\nfeats1 = ['Pclass','Sex','Age','SibSp','Parch','Fare','Cabin','Embarked','Family_Size', 'Title_Replace_Sex']\nX_Train1 = DTrain[feats1]\nX_Test1 = DTest[feats1]\n\nXTrain1, XVal1, yTrain1, yVal1 = train_test_split(X_Train1, Y_Train1, test_size=0.1, random_state=43)","1ade1877":"Rand_Forest1 = RandomForestClassifier(n_estimators=130, max_depth=13, random_state=1)\nRand_Forest1.fit(XTrain1, yTrain1)\n# evaluate\nRF_Acc1 = ((Rand_Forest1.score(XTrain1, yTrain1))*100)\n\n#Normal SVM Accuracy\nsvc1 = SVC(C = 200, kernel = 'rbf',  gamma = 0.1)\nsvc1.fit(XTrain1, yTrain1) \nacc_SVM1 = cross_val_score(svc1,XTrain1,yTrain1,cv=kf)\n##SVM w GridSearch \nparam_grid1 = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'linear', 'sigmoid']}\nsvc_gs1 = GridSearchCV(SVC(), param_grid1, refit = True, verbose = 2)  \nsvc_gs1.fit(XTrain1, yTrain1)\nacc_SVM_gs1 = cross_val_score(svc_gs1,XTrain1,yTrain1,cv=kf)","bc198853":"# K Nearest Neighbors\nKnn1 = KNeighborsClassifier(n_neighbors = 3) \nKnn1.fit(XTrain1, yTrain1)  \nacc_knn1 = round(Knn1.score(XTrain1, yTrain1) * 100, 2)\n\n# Voting Classifier\n\nLR1 = LogisticRegression()\nDT1 = DecisionTreeClassifier()\nGNB1 = GaussianNB()\nClassifiers1 = [('Logistic Regression', LR1), ('Decision Trees', DT1), ('Naive Bayes Classifier', GNB1)]\nVoting_Clf_Hard1 = VotingClassifier(estimators = Classifiers1, voting = 'hard')\nVoting_Clf_Soft1 = VotingClassifier(estimators = Classifiers1, voting = 'soft')\n\n# VC (Hard Voting Fit)\nVoting_Clf_Hard1.fit(XTrain1, yTrain1)\npreds1 = Voting_Clf_Hard1.predict(XTrain1)\nacc1 = accuracy_score(yTrain1, preds1)\nacc_VC_H1 = round((acc1*100),2)\n\n#VC (Soft Voting Fit)\nVoting_Clf_Soft1.fit(XTrain1, yTrain1)\npreds_s1 = Voting_Clf_Soft1.predict(XTrain1)\nacc_s1 = accuracy_score(yTrain1, preds_s1)\nacc_VC_S1 = round((acc_s1*100),2)","70e97764":"print('Accuracy Score for Random Forest is : ', RF_Acc1)\nprint('Predicted Accuracy Score for SVM without GridSearch is : ', (acc_SVM1.mean()*100))\nprint('Predicted Accuracy Score for SVM with GridSearch is : ', (acc_SVM_gs1.mean()*100))\nprint('KNN Accuracy is : ', acc_knn1)\nprint((acc_VC_H1.mean()), 'Percent Accuracy Achieved in Voting Classifier (Hard Voting)')\nprint((acc_VC_S1.mean()), 'Percent Accuracy Achieved in Voting Classifier (Soft Voting)')","3ab1eb34":"#Trying with the Title Changed Independently Column\nY_Train2 = DTrain['Survived']\nfeats2 = ['Pclass','Sex','Age','SibSp','Parch','Fare','Cabin','Embarked','Family_Size', 'Title_Replace_Ind']\nX_Train2 = DTrain[feats2]\nX_Test2 = DTest[feats2]\n\nXTrain2, XVal2, yTrain2, yVal2 = train_test_split(X_Train2, Y_Train2, test_size=0.1, random_state=44)","1433edc0":"Rand_Forest2 = RandomForestClassifier(n_estimators=130, max_depth=13, random_state=1)\nRand_Forest2.fit(XTrain2, yTrain2)\n# predict\n#Predictions = Rand_Forest.predict(X_Train)\n#accuracy = accuracy_score(X_Test, Predictions2)\n# evaluate\nRF_Acc2 = ((Rand_Forest2.score(XTrain2, yTrain2))*100)\n\n#Normal SVM Accuracy\nsvc2 = SVC(C = 200, kernel = 'rbf',  gamma = 0.1)\nsvc2.fit(XTrain2, yTrain2) \nacc_SVM2 = cross_val_score(svc2,XTrain2,yTrain2,cv=kf)\n\n##SVM w GridSearch \nparam_grid2 = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'linear', 'sigmoid']}\nsvc_gs2 = GridSearchCV(SVC(), param_grid2, refit = True, verbose = 2)  \nsvc_gs2.fit(XTrain2, yTrain2)\nacc_SVM_gs2 = cross_val_score(svc_gs2,XTrain2,yTrain2,cv=kf)","dfe99044":"# K Nearest Neighbors\nKnn2 = KNeighborsClassifier(n_neighbors = 3) \nKnn2.fit(XTrain2, yTrain2)  \nacc_knn2 = round(Knn2.score(XTrain2, yTrain2) * 100, 2)\n\n# Voting Classifier\n\nLR2 = LogisticRegression()\nDT2 = DecisionTreeClassifier()\nGNB2 = GaussianNB()\nClassifiers2 = [('Logistic Regression', LR2), ('Decision Trees', DT2), ('Naive Bayes Classifier', GNB2)]\nVoting_Clf_Hard2 = VotingClassifier(estimators = Classifiers2, voting = 'hard')\nVoting_Clf_Soft2 = VotingClassifier(estimators = Classifiers2, voting = 'soft')\n\n# VC (Hard Voting Fit)\nVoting_Clf_Hard2.fit(XTrain2, yTrain2)\npreds2 = Voting_Clf_Hard2.predict(XTrain2)\nacc2 = accuracy_score(yTrain2, preds2)\nacc_VC_H2 = round((acc2*100),2)\n\n#VC (Soft Voting Fit)\nVoting_Clf_Soft2.fit(XTrain2, yTrain2)\npreds_s2 = Voting_Clf_Soft2.predict(XTrain2)\nacc_s2 = accuracy_score(yTrain2, preds_s2)\nacc_VC_S2 = round((acc_s2*100),2)","886b1a28":"print('Accuracy Score for Random Forest is : ', RF_Acc2)\nprint('\\n')\nprint('Predicted Accuracy Score for SVM without GridSearch is : ', (acc_SVM2.mean()*100))\nprint('\\n')\nprint('Predicted Accuracy Score for SVM with GridSearch is : ', (acc_SVM_gs2.mean()*100))\nprint('\\n')\nprint('KNN Acuuracy is : ', acc_knn2)\nprint('\\n')\nprint((acc_VC_H2.mean()), 'Percent Accuracy Achieved in Voting Classifier (Hard Voting)')\nprint('\\n')\nprint((acc_VC_S2.mean()), 'Percent Accuracy Achieved in Voting Classifier (Soft Voting)')","0d0d6c86":"pd.set_option('display.max_columns', None)\nAcc_Table.insert(2, \"Accuracy %(Sex Based)\", [RF_Acc1, (acc_SVM1.mean()*100) , (acc_SVM_gs1.mean()*100), acc_knn1, (acc_VC_H1.mean()), (acc_VC_S1.mean())], True) \nAcc_Table.insert(3, \"Accuracy %(Independent)\", [RF_Acc2, (acc_SVM2.mean()*100) , (acc_SVM_gs2.mean()*100), acc_knn2, (acc_VC_H2.mean()), (acc_VC_S2.mean())], True) \nprint(Acc_Table)","238156f8":"# Now We Have the highest Accuracies on the training sets,\n# We will Calculate the Validation Accuracies on each of the highest Values and \n# Predict the Values for the Submission File for each accuracy as well\n\n#Random Forest \nRF_Acc_Highest = ((Rand_Forest1.score(XVal1, yVal1))*100)\n\n#SVM \nacc_SVM_Highest = cross_val_score(svc2,XVal2,yVal2)\n\n#SVM w GridSearch\nacc_SVM_gs_Highest = cross_val_score(svc_gs2,XTrain2,yTrain2)\n\n#KNN\nacc_knn_Highest = round(Knn1.score(XVal1, yVal1) * 100, 2)\n\n#Voting Classifier (Hard Voting)\npreds_VC_Hard_Highest = Voting_Clf_Hard2.predict(XVal2)\nacc_VC_H_Highest = accuracy_score(yVal2, preds_VC_Hard_Highest)\n\n#Voting Classifier (Soft Voting)\npreds_VC_Soft_Highest = Voting_Clf_Soft2.predict(XVal2)\nacc_VC_S_Highest = accuracy_score(yVal2, preds_VC_Soft_Highest)\n","6c668ac7":"print('The Highest Accuracy for the Random Forest Clf for Val Set is: ', round(RF_Acc_Highest),'%')\nprint('The Highest Accuracy for SVM for Val Set is: ', round(acc_SVM_Highest.mean()*100),'%')\nprint('The Highest Accuracy for the SVM with GridSearch for Val Set is: ', round(acc_SVM_gs_Highest.mean()*100),'%')\nprint('The Highest Accuracy for the K-Nearest Neighbors for Val Set is: ', round(acc_knn_Highest),'%')\nprint('The Highest Accuracy for the Voting Classifier(Hard Voting) for Val Set is: ', round(acc_VC_H_Highest*100),'%')\nprint('The Highest Accuracy for the Voting Classifier(Soft Voting) for Val Set is: ', round(acc_VC_S_Highest*100),'%')","ac8f8f33":"#Predictions and Submission \n\nPredictions_RF = Rand_Forest1.predict(X_Test1)\nprint(Predictions_RF)\nprint('\\n')\nPredictions_SVM = svc2.predict(X_Test2)\nprint(Predictions_SVM)\nprint('\\n')\nPredictions_SVM_gs = svc_gs2.predict(X_Test2)\nprint(Predictions_SVM_gs)\nprint('\\n')\nPredictions_KNN = Knn1.predict(X_Test2)\nprint(Predictions_KNN)\nprint('\\n')\nPredictions_VC_H = Voting_Clf_Hard2.predict(X_Test2)\nprint(Predictions_VC_H)\nprint('\\n')\nPredictions_VC_S = Voting_Clf_Soft2.predict(X_Test2)\nprint(Predictions_VC_S)\n","fa4f96ac":"Final_Report = pd.DataFrame({'PassengerId': DTeS2.PassengerId, 'Survived_RF': Predictions_RF, 'Survived_SVM': Predictions_SVM, 'Survived_SVM_GS': Predictions_SVM_gs, 'Survived_KNN': Predictions_KNN, 'Survived_VC_H': Predictions_VC_H, 'Survived_VC_Soft': Predictions_VC_S })\nFinal_Report_Sub = pd.DataFrame({'PassengerId': DTeS2.PassengerId,'Survived': Predictions_SVM_gs})\nprint(Final_Report)\n\nFinal_Report_Sub.to_csv('Final_Submission.csv', index=False)\nprint(\"File Successfully Submitted\")","b39485be":"For the Submission, I have submitted the classifier with the highest Validation set Accuracy (SVM with Gridsearch) However, I have displayed the dataframe with all the 6 classifiers with their submissions","40e2c454":"From this cell henceforth, I have come to a number of of conclusions which are related to cleaning data. In the PowerBI Visualizations, I could easily clean these values using the replace values command, but here I will use Pandas and Numpy where necessary. The Changes I Will be making are as Follows.\n1:Encoding male as 0 and female as 1\n2:Removing the name column, Maybe i will utilize it later \n3:Encoding the Embarked points as numbers in alphabetical order i.e. C:1, Q:2 and S:3 \n4:Replaced all missing age values as 99 (I did this in Power BI as to avoid a skew in my graph, but i Have used the Simple Imputer here to fill in the data as median)\n5:Removed Passenger ID from Train and test Data as it is just count \n6:Ticket ID Removed from Train and Test data\n7:2 absent Embarked Values in train set were encoded as UnK and later as 4*\n8:All the Absent Fare Values were filled with median values\n9:Empty Values in Cabin Column have been replaced as 'NS'","486aaf9e":"**** Well This is my submission for the Titanic challenge. I have used a variety of techniques and a number of classifiers to achieve what i Think is a 'Decent' Accuracy on my validation Set. I have used a lot of variables which might be considered to be 'messy' but that comes with the fact i wanted whoever might read this notebook to get an undestanding of what each variable is used for. Some Advice if anybody does use my notebook, running GridSearch with the Polynomial kernel took me 3 Hours to process that cell in my Google Colab :) so please use with caution. Another one is Cross Validation in the SVM is personally not required i feel due to the small amount of validation data itself. If you do like my work, please do upvote it :D, If you do have comments on it, please do leave them or you can mail me @ the_kg@outlook.com for any issues. And If any help with the power BI Visualizations is needed or if you do want additional Gifs, I can paste them here in the notebook or i can send it personally as well and I will do by best to help You visualize the data. Thanks Again and i hope you like my work . A Question i Had, If anybody knows how to resize gifs and display them side by side over here and in Google Colab can paste the solution in the comments, it would be really helpful for my future work. And if any intuition is required as to why i chose these classifiers, I can send you the links personally :) Take care!","42a222f6":"Hello Everyone, this is my first submission for any kaggle challenge ever. So i have been learning Power BI just as a new skill and I thought, why not try it out as an initial data-viz tool to get better insights to start off rather than just directly using Seaborn or MatplotLib as trial and error. I have a few gifs below on what I have done in Power BI, which yielded pretty good results from the purview of gathering initial insights. So I have come up with a number of Conclusions from these visalizations which i shall list after the gifs have been uploaded. If anyone would like a bit of help with the power BI aspect of it, please feel free to contact me, I shall leave a Mail ID at the end of the Notebook :)","cc19d16a":"**** So from the Power BI Visualizations, I could see which values were predominant, values which were missing, values which would have high or low correlation to the survival percentage and I have made a number of changes in the PowerBI DataFrame itself, but now i will be using Pandas for changes within the notebook"}}