{"cell_type":{"af292ce5":"code","1f91ca10":"code","36889eb4":"code","25b49cc1":"code","0c919e5f":"code","2cb02b3d":"code","4a97e16e":"code","880f2725":"code","31f4b1bb":"code","8082492a":"code","a6c15505":"code","6a8f1d47":"code","5043161b":"code","66fbe121":"code","7711ed4c":"code","8877192b":"code","382d2f59":"code","9dcd9d3d":"code","330c7261":"code","317d3c1e":"code","330ffc53":"code","6fab11f2":"code","48a8ae6e":"code","f899da18":"code","44a19163":"code","e0636c4c":"code","997c1db3":"code","ee6aff71":"markdown","4601bc62":"markdown","b99e2966":"markdown","316b1b7f":"markdown","dea3c40b":"markdown","563b99c9":"markdown","194e6fcd":"markdown","fb0745b3":"markdown","06ca099d":"markdown","77454619":"markdown","971dfef0":"markdown","8ce77b26":"markdown","5f386e7a":"markdown","37c247eb":"markdown","eb455dac":"markdown","1e90923b":"markdown","260957a8":"markdown","c3b33a02":"markdown","b833b6b6":"markdown","e17c3a57":"markdown","ad106757":"markdown","9d98e281":"markdown","281fb1e1":"markdown","6c6502b7":"markdown"},"source":{"af292ce5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.offline as py\nimport plotly.express as px\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1f91ca10":"df_apes = pd.read_csv(\"\/kaggle\/input\/ask-reddit\/ask_reddit.csv\")\ndf_apes.head(2)","36889eb4":"df_apes_1 = df_apes.drop(['created', 'id'], axis=1)","25b49cc1":"df_apes_1.head(4)","0c919e5f":"#Code by Yung Chimp https:\/\/www.kaggle.com\/yungchimp\/apes-together-strong-an-analysis-of-strength\/data\n\n# I will look for occurences of the following: [\"Citadel\", \"fucker\", \"fuck\", \"Melvin\", \"apes\", \"moon\", \"short\", \"poor\", \"hype\", \"tendies\", \"diamond hands\", \"paper hands\"]\n\nkeywords = [\"Comment\", \"what\", \"superhero\", \"moment\", \"touched\", \"look\", \"moon\", \"title\", \"yesterday\", \"heart\", \"you\", \"people\", \"most\", \"sex\", \"thing\", \"Reddit\", \"first\", \"time\",\"funniest\"]\n# len(keywords)\n#Change all strings in title to lower-case to search easier\ndf_apes_1['title'].str.lower()","2cb02b3d":"Comment = df_apes_1['title'].str.contains('Comment')\nComment1 = Comment.sum()\nprint('Comment got:',Comment1,'occurences')\n\nwhat = df_apes_1['title'].str.contains('what')\nwhat1 = what.sum()\nprint('what got:',what1,'occurences')\n\nsuperhero = df_apes_1['title'].str.contains('superhero')\nsuperhero1 = superhero.sum()\nprint('superhero got:',superhero1,'occurences')\n\nmoment = df_apes_1['title'].str.contains('moment')\nmoment1 = moment.sum()\nprint('moment got:',moment1,'occurences')\n\ntouched = df_apes_1['title'].str.contains('touched')\ntouched1 = touched.sum()\nprint('touched got:',touched1,'occurences')\n\nlook = df_apes_1['title'].str.contains('look')\nlook1 = look.sum()\nprint('look:',look1,'occurences')\n\nmoon = df_apes_1['title'].str.contains('moon')\nmoon1 = moon.sum()\nprint('Moon got:',moon1,'occurences')\n\ntitle = df_apes_1['title'].str.contains('title')\ntitle1 = title.sum()\nprint('title got:',title1,'occurences')\n\nyesterday = df_apes_1['title'].str.contains('yesterday')\nyesterday1 = yesterday.sum()\nprint('yesterday got:',yesterday1,'occurences')\n\nheart = df_apes_1['title'].str.contains('heart')\nheart1 = heart.sum()\nprint('heart got:',heart1,'occurences')\n\nyou = df_apes_1['title'].str.contains('you')\nyou1 = you.sum()\nprint('you got:',you1,'occurences')\n\npeople = df_apes_1['title'].str.contains('people')\npeople1 = people.sum()\nprint('people got:',people1,'occurences')\n\nmost = df_apes_1['title'].str.contains('most')\nmost1 = most.sum()\nprint('most got:',most1,'occurences')\n\nsex = df_apes_1['title'].str.contains('sex')\nsex1 = sex.sum()\nprint('sex got:',sex1,'occurences')\n\nthing = df_apes_1['title'].str.contains('thing')\nthing1 = thing.sum()\nprint('thing got:',thing1,'occurences')\n\nReddit = df_apes_1['title'].str.contains('Reddit')\nReddit1 = Reddit.sum()\nprint('Reddit got:',Reddit1,'occurences')\n\nfirst = df_apes_1['title'].str.contains('first')\nfirst1 = first.sum()\nprint('first got:',first1,'occurences')\n\ntime = df_apes_1['title'].str.contains('time')\ntime1 = time.sum()\nprint('time got:',time1,'occurences')\n\nfunniest = df_apes_1['title'].str.contains('funniest')\nfunniest1 = funniest.sum()\nprint('funniest got:',funniest1,'occurences')","4a97e16e":"title_keyword_sum_list = [Comment1,what1,superhero1,moment1,touched1,look1,moon1,title1,yesterday1,heart1,you1,people1,most1,sex1,thing1,Reddit1,first1,time1,funniest1]\n# sum(title_keyword_sum_list)\n#?????? is sum of the total keywords in titles  I didn't count here\n# title_keyword_sum_list","880f2725":"data = title_keyword_sum_list\nlabellz = [\"Comment\", \"what\", \"superhero\", \"moment\", \"touched\", \"look\", \"moon\", \"title\", \"yesterday\", \"heart\", \"you\", \"people\", \"most\", \"sex\", \"thing\", \"Reddit\", \"first\", \"time\", \"funniest\"]\nfig = plt.figure(figsize =(12, 12)) \nplt.pie(data, labels = labellz, autopct='%1.0f%%');","31f4b1bb":"title_keyword_sum_list1 = title_keyword_sum_list\ntitle_keyword_sum_list1.sort()\ntitle_keyword_sum_list1","8082492a":"df_apes_1['body'].str.lower()","a6c15505":"last = df_apes_1['body'].str.contains('last')\nlast1 = last.sum()\nprint('last got:',last1,'occurences')\n\npat = df_apes_1['body'].str.contains('pat')\npat1 = pat.sum()\nprint('pat got:',pat1,'occurences')\n\nmorita = df_apes_1['body'].str.contains('morita')\nmorita1 = morita.sum()\nprint('morita got:',morita1,'occurences')\n\nundergraduate = df_apes_1['body'].str.contains('undergraduate')\nundergraduate1 = undergraduate.sum()\nprint('undergraduate got:',undergraduate1,'occurences')\n\nmr = df_apes_1['body'].str.contains('mr')\nmr1 = mr.sum()\nprint('mr got:',mr1,'occurences')\n\nmiyagi = df_apes_1['body'].str.contains('miyagi')\nmiyagi1 = miyagi.sum()\nprint('miyagi got:',miyagi1,'occurences')\n\nyear = df_apes_1['body'].str.contains('year')\nyear1 = year.sum()\nprint('year:',year1,'occurences')","6a8f1d47":"body_keyword_sum_list = [last1,pat1,morita1,undergraduate1,mr1,miyagi1,year1]\nsum(body_keyword_sum_list)\n#11214 is sum of the total keywords in titles\nbody_keyword_sum_list","5043161b":"data = body_keyword_sum_list\nlabellz = ['last','pat','morita', 'undergraduate','mr','miyagi','year']\nfig = plt.figure(figsize =(15, 15)) \nplt.pie(data, labels = labellz, autopct='%1.0f%%');","66fbe121":"fourtwenty = df_apes_1['title'].str.contains('420')\nfourtwenty1 = fourtwenty.sum()\nprint('IN TITLE COLUMN - fourtwenty got:',fourtwenty1,'occurences')\n\nsixtynine = df_apes_1['title'].str.contains('69')\nsixtynine1 = sixtynine.sum()\nprint('IN TITLE COLUMN - sixtynine got:',sixtynine1,'occurences')\n\n\n\nfourtwenty2 = df_apes_1['body'].str.contains('420')\nfourtwenty3 = fourtwenty2.sum()\nprint('IN BODY COLUMN - fourtwenty got:',fourtwenty3,'occurences')\n\nsixtynine2 = df_apes_1['body'].str.contains('69')\nsixtynine3 = sixtynine2.sum()\nprint('IN BODY COLUMN - sixtynine got:',sixtynine3,'occurences')\n\n\ntotal420 = fourtwenty1 + fourtwenty3\ntotal69 = sixtynine1 + sixtynine3\nprint('Total ocurrences of 420 are: ', total420)\nprint('Total occurences of  69 are: ', total69)","7711ed4c":"data = [273, 594]\nlabellz = [420, 69]\nfig = plt.figure(figsize =(7, 7)) \nplt.pie(data, labels = labellz, autopct='%1.0f%%');","8877192b":"yolo2 = df_apes_1['title'].str.contains('YOLO')\nyolo3 = yolo2.sum()\nprint('IN TITLE COLUMN - yolo got:',yolo3,'occurences')\n\nyolo = df_apes_1['body'].str.contains('YOLO')\nyolo1 = yolo.sum()\nprint('IN BODY COLUMN - yolo got:',yolo1,'occurences')\n\ntotalyolo = yolo3 + yolo1\nprint('Total ocurrences of YOLO are: ', totalyolo)","382d2f59":"time = df_apes_1['title'].str.contains('time')\ntime1 = time.sum()\nprint('time :',time1,'occurences')\n\ntime2 = df_apes_1['body'].str.contains('time')\ntime3 = time2.sum()\nprint('time:',time3,'occurences')\n\nTOTAL_BAG_TIMERS = time1 + time3\nprint('Total bag timers: ', TOTAL_BAG_TIMERS)","9dcd9d3d":"num14 = (df_apes_1['score'] == 14)\ndf_apes_1[num14]","330c7261":"pd.set_option('display.max_rows', None)\nnum20 = (df_apes_1['score'] == 20)\ndf_apes_1[num20]","317d3c1e":"#Code by Matias Eiletz  https:\/\/towardsdatascience.com\/mastering-dates-and-timestamps-in-pandas-and-python-in-general-5b8c6edcc50c\n\ndf_apes_1['timestamp'] = df_apes_1['timestamp'].apply(lambda x: pd.Timestamp(x).strftime('%B-%d-%Y %I:%M %p'))","330ffc53":"df_apes_1.head(2)","6fab11f2":"#Code by Matias Eiletz  https:\/\/towardsdatascience.com\/mastering-dates-and-timestamps-in-pandas-and-python-in-general-5b8c6edcc50c\n\ndf_apes_1['timestamp'] = df_apes_1['timestamp'].apply(lambda x: pd.Timestamp(x).strftime('%Y-%m-%d'))","48a8ae6e":"df_apes_1.head(2)","f899da18":"df = pd.read_csv(\"\/kaggle\/input\/ask-reddit\/ask_reddit.csv\")\ndf.head(2)","44a19163":"#Code by SHREYAS P J https:\/\/www.kaggle.com\/shreyaspj\/covid-forecasting-using-dl-and-statistical-models\/notebook\n\nplotsize = (12,5)\n\nfigure, axes = plt.subplots(3,sharex=True)\ndf['score'].plot(ax=axes[0],title='Scores',figsize=plotsize)\ndf['comms_num'].plot(ax=axes[1],title='Number of Comments',figsize=plotsize)\ndf['created'].plot(ax=axes[2],title='Created',figsize=plotsize);","e0636c4c":"y = df.loc[~df.score.isnull()][['score', 'comms_num', 'created']]\n\ny_log=np.log1p(y)","997c1db3":"#Code by Oniel Gracious https:\/\/www.kaggle.com\/onielg\/simplecatboostwithclassifierchains\/notebook\n\n#histogram and normal probability plot\nfrom scipy import stats\n\nfrom scipy.stats import norm\ntarget=['score','comms_num','created']\nfor t in target:\n    fig = plt.figure()\n    sns.distplot(y_log[t], fit=norm);\n    res = stats.probplot(y_log[t], plot=plt)","ee6aff71":"#I didn't realize yet why to sort this list below in such order. I'm still trying to get it.","4601bc62":"#The original code was num420 because that was the number written in score column","b99e2966":"#We don't even have YOLO word, though the programm didn't returned any error.","316b1b7f":"#Ew! Now my CODE is Gone! \n\n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQNeyEu5UWtuLHcMtD5QqW7WzloiLQXn5cakw&usqp=CAU)sites.google.com\n\n#Don't ask me if you didn't get it. Nor Pat Morita\/Mr Miyagi since he's gone too. Ask Reddit.","dea3c40b":"#Codes by Yung Chimp https:\/\/www.kaggle.com\/yungchimp\/apes-together-strong-an-analysis-of-strength\/notebook\n\n#I decide to leave df_apes_1 (instead of just df) to remind me of the Author Yung Chimp.\n\n#Ok Who am I kidding?  There were too many to adapt. After more than 3 hours (3:53) it's about time to publish the Notebook.","563b99c9":"#We don't have scores 420 and 69 in this Dataset. Only in the original code.","194e6fcd":"#Since I dropped created and id, in the beginning. I had to read the file again. Those lines below were already there when I started the \"Keywords\" script.  ","fb0745b3":"#Remove \"created\" column. It serves no purpose. Probably the creation time of the scraper.\n\n#\"id\" column serves no purpose because len(df_apes_1.id.unique()) = 32,204. Removing \"id\" column\n\nhttps:\/\/www.kaggle.com\/yungchimp\/apes-together-strong-an-analysis-of-strength\/notebook","06ca099d":"#Again I made a WordCloud to get the most frequent words. Then I build the list below. \n\nThis time a little shorter than the initial one.","77454619":"![](http:\/\/www.loc.gov\/exhibits\/oliphant\/vc007257.jpg)\nCartoonist: Pat Oliphant - Date: July 10, 1993 \n\nMEME EXPLANATIONS\n\nTitle of Cartoon:\n\n\u2018I won't ask! Don't ask!\n\nI mustn't ask! Don't ask!\n\nI can't ask! Don't ask!\n\nSo don't ask! Don't ask!'\n\nThe soldier in question is putting on lip stick. The officer associates that with being homosexual, but he can't ask about it because that would violate the \"Don't ask, Don't tell\" policy.\n\nThis cartoon is neutral, because it is just portraying the DADT (Don't ask. Don't tell) policy in the military without really taking a side on the issue.\n\nAlthough this cartoon doesn't necessarily take a stand on the policy, it does contain a stereotype about gay men wearing make-up.\n\nhttp:\/\/www.loc.gov\/exhibits\/oliphant\/\n\nhttp:\/\/akasgays.blogspot.com\/2009\/10\/dont-ask.html","971dfef0":"#Changing Date Format","8ce77b26":"#Time analysis.  That was the only word that I found in both columns. Fine, there were more, but I was already tired.","5f386e7a":"#Summary of observations:\n\nThe top 5 occurences of keywords were NOT basically the same either from the \"title\" column or the \"body\" column.  In the original code they were almost the same.\n\nThese keywords were: comment, you, what.","37c247eb":"#Closing thoughts:  NOT yet.\n\nI made those lines below to learn how to deal with timestamp. Then I changed the project and followed Plan B.\n\nThe next part has nothing to do with the initial analysis of the keywords.","eb455dac":"#Recommendations\n\nThere needs to be more than 20 score posts. The posts have to get danker.\n\nAs the original Author(Yung Chimp), I don't have all the time or energy to find all combinations but these are some that come to mind:\n\n#\"Seriously, what the hell are you doing still reading this? Fix your life you degen.\" Quote by the original Author (Yung Chimp).","1e90923b":"#Closing thoughts:  Now it's Closing.\n\n\"I don't have thoughts anymore. My brain was losing its wrinkles ever since I exposed myself to this data. I lost IQ most definetly from start to end of this analysis. I feel dumb, angry, and I want my time back.\"   I spent 3:53 coping\/editing this Notebook. Chimp words were mine too.\n\n\"With that being said, like an ancient Roman that partied with sugar of lead, I had fun.\"\n\n\n#Vote for Yung Chimp and Read his work. I didn't code here again.\n\nhttps:\/\/www.kaggle.com\/yungchimp\/apes-together-strong-an-analysis-of-strength\/notebook","260957a8":"#Future Work - Words by the Author Yung Chimp\n\n\"Re-run all this analysis on 1 yr of scraped ask reddit data.\"   \n\n\"Create a ML bot that posts statements after going through every ask reddit post\/body content\"\n\n\"Scrape and analyze all memes to see which ones survived time.\"\n\n\"Compare scrotum sizes of \"diamond hands\" vs. \"paper hands\"  What???? Don't ask. Don't tell.\n\nhttps:\/\/www.kaggle.com\/yungchimp\/apes-together-strong-an-analysis-of-strength\/notebook","c3b33a02":"#Since my list in title is different than body. The snippet below doesn't make any sense. I just copied for the next work. And to allow me to make the next pie.","b833b6b6":"#Removing time, the higher value, so that Morita\/Mr\/Miyagi words could show in the Piechart clearer.","e17c3a57":"#Changing Date Format in another way.","ad106757":"#Keyword analysis in \"body\" column","9d98e281":"![](https:\/\/pics.me.me\/yagi-i-dont-feel-so-good-mr-miyagi-44398498.png)me.me","281fb1e1":"#Another snippet with score 20.","6c6502b7":"#Keyword Analysis in title column \n\nTo choose the words below, I made a WordCloud. Then I made a search in the Dataset to read what words repeated with more frequency."}}