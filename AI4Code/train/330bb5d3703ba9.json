{"cell_type":{"f8ae7647":"code","e0ce1fbb":"code","7497e796":"code","7b2f4f8a":"code","c646de7e":"code","baf07b4b":"code","a5c57fcb":"code","c8c77ace":"code","d01c4365":"code","ec9a779b":"code","5fafdd28":"code","d9b80cbf":"code","3bf4a6f8":"code","69e7b50b":"code","25b5fb86":"code","1439801b":"code","51bd5ffa":"code","5657b2b8":"code","54205445":"code","3cbaae05":"code","edb59da1":"code","f4a5a70e":"code","e3a6c38b":"code","3507f9a7":"code","4ed05e8f":"code","62dcc4e1":"code","6d5d8a6f":"code","69e4b770":"code","b228682a":"code","6046f9fb":"code","a159e115":"code","6f270506":"code","5ebbeeb5":"code","e09a8f01":"code","067f9c96":"code","28109604":"code","9db4ddee":"code","eb6778a0":"code","34ece113":"code","d22cd079":"code","922cf6b3":"code","d311e9ea":"code","37ae3337":"code","dd127532":"code","2c64b754":"code","478f3322":"code","2f3b3d77":"code","26ee9f1c":"code","3d4dcc16":"code","de064119":"code","c30e5258":"code","59076db3":"markdown","7f08a07c":"markdown","27a246f4":"markdown","e8ec28d0":"markdown","f72cd4d1":"markdown","d8eecb94":"markdown","3b00d78a":"markdown","13fe7ddf":"markdown","66071b1e":"markdown","41088091":"markdown","0b40b3b1":"markdown","2f9c8c4d":"markdown"},"source":{"f8ae7647":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e0ce1fbb":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom nltk import word_tokenize\nfrom imblearn.over_sampling import SMOTE\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nimport math","7497e796":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nimport spacy\nimport string\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer","7b2f4f8a":"data = pd.read_csv('..\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv')","c646de7e":"sns.countplot(data=data,x='Rating')","baf07b4b":"def  clean_text(text):\n    text = text.str.lower()\n    text = text.apply(lambda T: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)|^rt|http.+?\", \"\", T))          \n    return text\n\ndata['Review']= clean_text(data['Review'])\ndata['Review'] = data['Review'].str.replace('#','')\ndata['Review'] = data['Review'].str.lower()\n\npunk_Remove = string.punctuation\n\ndef del_punk(text):\n    return text.translate(str.maketrans('','',punk_Remove))\n\ndata['Review'] = data['Review'].apply(lambda T: del_punk(T))","a5c57fcb":"stop_word = set(stopwords.words('english'))\ndef del_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in stop_word])\n\ndata['Review'] = data['Review'].apply(lambda T: del_stopwords(T))\nlemmatizer = WordNetLemmatizer()\nwordnet_m = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n\n\ndef lemmat_pos_word(text):\n    \n    pos_tagger_text = nltk.pos_tag(text.split())\n    \n    return \" \".join([lemmatizer.lemmatize(word, wordnet_m.get(pos[0],wordnet.NOUN)) for word,pos in pos_tagger_text])\n\n\n\ndata['Review'] = data['Review'].apply(lambda T : lemmat_pos_word(T))","c8c77ace":"data['totalwords'] = data['Review'].str.count(' ') + 1","d01c4365":"plt.figure(figsize=(16,16))\nfor i, rating in enumerate([1,2,3,4,5]):\n    plt.subplot(4,4,i+1)\n    plt.title(\"Rating \"+str(rating))\n    sns.distplot(data[data['Rating']==rating]['totalwords'],kde=False)\n    plt.tight_layout()","ec9a779b":"def multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 \/ rows * vsota","5fafdd28":"data.head()","d9b80cbf":"lbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(data.Rating.values)","3bf4a6f8":"data.columns","69e7b50b":"xtrain, xtest, ytrain, ytest = train_test_split(data.Review.values, y, \n                                                  stratify=y, \n                                                  random_state=42, \n                                                  test_size=0.1, shuffle=True)\nxtrain, xvalid, ytrain, yvalid = train_test_split(pd.DataFrame(xtrain)[0], ytrain, \n                                                  stratify=ytrain, \n                                                  random_state=42, \n                                                  test_size=0.1, shuffle=True)","25b5fb86":"tfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\ntfv.fit(list(xtrain) + list(xvalid)+ list(xtest))\nxtrain_tfv =  tfv.transform(xtrain) \nxvalid_tfv = tfv.transform(xvalid)\nxtest_tfv = tfv.transform(xtest)","1439801b":"LR_tfv_clf = LogisticRegression()\nLR_tfv_clf.fit(xtrain_tfv, ytrain)\npredictions = LR_tfv_clf.predict_proba(xvalid_tfv)\n\n# print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions));\nprint(classification_report(yvalid,np.argmax(predictions,axis=1)))\nprint(confusion_matrix(yvalid, np.argmax(predictions,axis=1)))","51bd5ffa":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 3), stop_words = 'english')\n\nctv.fit(list(xtrain) + list(xvalid))\nxtrain_ctv =  ctv.transform(xtrain) \nxvalid_ctv = ctv.transform(xvalid)","5657b2b8":"LR_ctv_clf = LogisticRegression()\nLR_ctv_clf.fit(xtrain_ctv, ytrain)\npredictions = LR_ctv_clf.predict_proba(xvalid_ctv)\n\nprint(classification_report(yvalid,np.argmax(predictions,axis=1)))\nprint(confusion_matrix(yvalid, np.argmax(predictions,axis=1)))","54205445":"NMB_ctv_clf = MultinomialNB()\nNMB_ctv_clf.fit(xtrain_ctv, ytrain)\npredictions = NMB_ctv_clf.predict_proba(xvalid_ctv)\n\n# print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\nprint(classification_report(yvalid,np.argmax(predictions,axis=1)))\nprint(confusion_matrix(yvalid, np.argmax(predictions,axis=1)))","3cbaae05":"NMB_tfv_clf = MultinomialNB()\nNMB_tfv_clf.fit(xtrain_tfv, ytrain)\npredictions = NMB_tfv_clf.predict_proba(xvalid_tfv)\n\n# print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\nprint(classification_report(yvalid,np.argmax(predictions,axis=1)))\nprint(confusion_matrix(yvalid, np.argmax(predictions,axis=1)))","edb59da1":"XGB_ctv_clf = xgb.XGBClassifier()\nXGB_ctv_clf.fit(xtrain_ctv.tocsc(), ytrain)\npredictions = XGB_ctv_clf.predict_proba(xvalid_ctv.tocsc())\n\n# print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\nprint(classification_report(yvalid,np.argmax(predictions,axis=1)))\nprint(confusion_matrix(yvalid, np.argmax(predictions,axis=1)))","f4a5a70e":"XGB_tfv_clf = xgb.XGBClassifier()\nXGB_tfv_clf.fit(xtrain_tfv.tocsc(), ytrain)\npredictions = XGB_tfv_clf.predict_proba(xvalid_tfv.tocsc())\n\n# print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\nprint(classification_report(yvalid,np.argmax(predictions,axis=1)))\nprint(confusion_matrix(yvalid, np.argmax(predictions,axis=1)))","e3a6c38b":"0","3507f9a7":"C = [10**x for x in range(-5,5)]\ntuned_parameters = [{'C': C}]\nmodel = GridSearchCV(LogisticRegression(class_weight='balanced'), tuned_parameters, scoring = 'accuracy', cv=3,return_train_score = True)\nmodel.fit(xtrain_tfv, ytrain);","4ed05e8f":"print(model.best_estimator_)\nresults = pd.DataFrame.from_dict(model.cv_results_)\nresults = results.sort_values(['param_C'])\n\ntrain_score= results['mean_train_score']\ntrain_acc_std= results['std_train_score']\ncv_score = results['mean_test_score'] \ncv_acc_std= results['std_test_score']\nC =  results['param_C']\n\nC = [math.log(x,10) for x in C]\nplt.plot(C, train_score, label='Train')\n\nplt.plot(C, cv_acc_std, label='CV')\n\nplt.scatter(C, train_score, label='Train points')\nplt.scatter(C, cv_acc_std, label='CV points')\n\nplt.legend()\nplt.xlabel(\"C: hyperparameter\")\nplt.ylabel(\"Acc\")\nplt.grid()\nplt.show()","62dcc4e1":"LR_ctv_clf = LogisticRegression(C=1, class_weight='balanced')\nLR_ctv_clf.fit(xtrain_ctv, ytrain)\npredictions = LR_ctv_clf.predict_proba(xvalid_ctv)\n\n# print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\nprint(classification_report(yvalid,np.argmax(predictions,axis=1)))\nprint(confusion_matrix(yvalid, np.argmax(predictions,axis=1)))","6d5d8a6f":"params = {\n        'n_estimators': [ 500, 1000],\n        'max_depth': [4, 5, 8],\n        'tree_method':['gpu_hist']\n        }\nmodel = GridSearchCV(xgb.XGBClassifier(), params, scoring = 'accuracy', cv=3,return_train_score = True)\nmodel.fit(xtrain_tfv.tocsc(), ytrain)\n\nprint(model.best_estimator_)\nresults = pd.DataFrame.from_dict(model.cv_results_)\nresults = results.sort_values(['param_C'])\n\ntrain_score= results['mean_train_score']\ntrain_acc_std= results['std_train_score']\ncv_score = results['mean_test_score'] \ncv_acc_std= results['std_test_score']\nC =  results['param_C']\n\nC = [math.log(x,10) for x in C]\nplt.plot(C, train_acc, label='Train')\n\nplt.plot(C, cv_acc, label='CV')\n\nplt.scatter(C, train_acc, label='Train points')\nplt.scatter(C, cv_acc, label='CV points')\n\nplt.legend()\nplt.grid()\nplt.show()","69e4b770":"XGB_tfv_clf = xgb.XGBClassifier()\nXGB_tfv_clf.fit(xtrain_tfv.tocsc(), ytrain_tfv)\npredictions = XGB_tfv_clf.predict_proba(xvalid_tfv.tocsc())\n\nprint(classification_report(yvalid,np.argmax(predictions,axis=1)))\nprint(confusion_matrix(yvalid, np.argmax(predictions,axis=1)))","b228682a":"tfidf = TfidfVectorizer(binary=True)\nX_train = tfidf.fit_transform(xtrain).astype('float16')\nX_test = tfidf.transform(xtest).astype('float16')\nX_valid = tfidf.transform(xvalid).astype('float16')","6046f9fb":"lb = preprocessing.LabelEncoder()\ny = lb.fit_transform(ytrain)\ny_train = np_utils.to_categorical(ytrain)\ny_test= np_utils.to_categorical(ytest)\ny_valid = np_utils.to_categorical(yvalid)","a159e115":"X_train.shape","6f270506":"model = Sequential()\nmodel.add(Dense(32, input_dim=65413, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n# model.add(Dense(216, activation='relu'))\n# model.add(Dropout(0.8))\n# model.add(BatchNormalization())\n# model.add(Dense(128, activation='relu'))\n# model.add(Dropout(0.8))\n# model.add(BatchNormalization())\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(5, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","5ebbeeb5":"model.fit(X_train, y=y_train, batch_size=128,epochs=10, verbose=1,validation_data=(X_valid, y_valid))","e09a8f01":"pred=model.predict_classes(X_test)\nprint(classification_report(ytest,pred))\nprint(confusion_matrix(ytest, pred))","067f9c96":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.optimizers import RMSprop, SGD, Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint","28109604":"def create_tokenizer(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\ndef encode_docs(tokenizer, max_length, docs):\n    encoded = tokenizer.texts_to_sequences(docs)\n    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n    return padded","9db4ddee":"tokenizer = create_tokenizer(xtrain)\nvocab_size = len(tokenizer.word_index) + 1\nmax_length = max([len(s.split()) for s in xtrain])","eb6778a0":"Xtrain = encode_docs(tokenizer, max_length, xtrain)","34ece113":"lb = preprocessing.LabelEncoder()\ny = lb.fit_transform(ytrain)\ny_train = np_utils.to_categorical(ytrain)\ny_test= np_utils.to_categorical(ytest)\ny_valid = np_utils.to_categorical(yvalid)","d22cd079":"Xvalid = encode_docs(tokenizer, max_length, xvalid)\nXtest = encode_docs(tokenizer, max_length, xtest)","922cf6b3":"model = Sequential()\nmodel.add(Embedding(vocab_size, 128, input_length=max_length))\nmodel.add(Conv1D(128, 8, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Conv1D(64, 8, activation='relu'))\nmodel.add(Dropout(0.6))\nmodel.add(BatchNormalization())\nmodel.add(Conv1D(32, 8, activation='relu'))\nmodel.add(Dropout(0.7))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling1D())\nmodel.add(Flatten())\nmodel.add(Dense(5, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])","d311e9ea":"checkpoint = ModelCheckpoint(\".\/weights.h5\",monitor=\"val_loss\",mode=\"min\",save_best_only = True,verbose=1)\nmodel.fit(Xtrain, y=y_train, batch_size=128,epochs=25, verbose=1,validation_data=(Xvalid, y_valid),callbacks=[checkpoint])","37ae3337":"model.load_weights(\"weights.h5\")\npred=model.predict_classes(Xvalid)\nprint(classification_report(yvalid,pred))\nprint(confusion_matrix(yvalid, pred))","dd127532":"!wget \"http:\/\/www-nlp.stanford.edu\/data\/glove.840B.300d.zip\"","2c64b754":"!unzip 'glove.840B.300d.zip'","478f3322":"embeddings_index = {}\nf = open('..\/input\/glove840b300dtxt\/glove.840B.300d.txt')\nfor line in tqdm(f):\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","2f3b3d77":"def sent2vec(s):\n    words = str(s).lower().decode('utf-8')\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v \/ np.sqrt((v ** 2).sum())","26ee9f1c":"xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\nxtest_glove = [sent2vec(x) for x in tqdm(xtest)]\nxvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]\n\n","3d4dcc16":"xtrain_glove = np.array(xtrain_glove)\nxtest_glove = np.array(xtest_glove)\nxvalid_glove = np.array(xvalid_glove)","de064119":"\nXGB_glove_clf = xgb.XGBClassifier(nthread=10, silent=False)\nXGB_glove_clf.fit(xtrain_glove, ytrain)\npredictions = XGB_glove_clf.predict_proba(xvalid_glove)\n\n# print (\"logloss: %0.3f \" % multiclass_logloss(xvalid_glove, predictions))\nprint(classification_report(yvalid,np.argmax(predictions,axis=1)))\nprint(confusion_matrix(yvalid, np.argmax(predictions,axis=1)))","c30e5258":"LR_glove_clf = LogisticRegression(C=1.0)\nLR_glove_clf.fit(xtrain_glove, ytrain)\npredictions = LR_glove_clf.predict_proba(xvalid_glove)\n\n# print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\nprint(classification_report(yvalid,np.argmax(predictions,axis=1)))\nprint(confusion_matrix(yvalid, np.argmax(predictions,axis=1)))","59076db3":"Trying CNN","7f08a07c":"Usually these comments are power law, I will check for it later ","27a246f4":"**Using count Vectorizer**","e8ec28d0":"* Data is highly imbalanced.","f72cd4d1":"**Using td Idf**","d8eecb94":"I have tried using both TF-IDF and Count Vectorizer  <br>\n* Logistic regression\n* Naive bayes\n* Xgboost\n* ANN\n* CNN\n\nModels are getting confused for rating 3, few of them are predicted as 4 or 2","3b00d78a":"Logistic Regression gave a better results, Lets tune it.<br>\n","13fe7ddf":"Trying ANN","66071b1e":"Kindly upvote if you like this notebook.<br>\nAny issues or mistake kindly let me know in comments, happy to correct.","41088091":"Lets Tune XGBoost","0b40b3b1":"I also wanted to try GloVe, but I was getting memory issues, I kept the code, later I will try.","2f9c8c4d":"**Cleaning of data**"}}