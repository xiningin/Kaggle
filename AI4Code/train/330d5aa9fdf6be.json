{"cell_type":{"50fa90cd":"code","96a3c0fb":"code","269bad2b":"code","72d36369":"code","61b547e3":"code","17e99493":"code","b7435f6c":"code","6876c7f2":"code","b879b81c":"code","a82d5c6b":"code","9fb0c2c6":"code","9ae612f9":"code","75272c5f":"code","3a0f24f8":"code","005635c5":"code","1f714c2e":"code","a64c7cd4":"code","5c8884d4":"code","03176cc5":"code","88941f30":"markdown","1ee89a6e":"markdown","f01be52c":"markdown","fbc4ae7e":"markdown","140cb671":"markdown","c4954895":"markdown","24ea418a":"markdown","dd1fd6f2":"markdown","a6d007f9":"markdown","b1ed3f9e":"markdown","1116db3a":"markdown","9d9fb38f":"markdown","cd360da6":"markdown","2e19c7de":"markdown","55c0145f":"markdown"},"source":{"50fa90cd":"\n\nfrom tensorflow import keras\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport plotly.graph_objects as go\n\nnp.random.seed(1)\ntf.random.set_seed(1)\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed\n\n","96a3c0fb":"df = pd.read_csv('..\/input\/hdfc-stock-price-data-19962020\/HDFCBANK.NS.csv')\ndf = df[['Date', 'Close']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['Date'].min(), df['Date'].max()","269bad2b":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=df['Date'], y=df['Close'], name='Close price'))\nfig.update_layout(showlegend=True, title='HDFC Bannk NSE Stock Price 1996-2020')\nfig.show()","72d36369":"train, test = df.loc[df['Date'] <= '2018-01-01'], df.loc[df['Date'] > '2018-01-01']\ntrain.shape, test.shape","61b547e3":"train=train.replace(np.nan,0)\ntest=test.replace(np.nan,0)\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\n\nscaler = scaler.fit(train[['Close']])\n\ntrain['Close'] = scaler.transform(train[['Close']])\ntest['Close'] = scaler.transform(test[['Close']])\n","17e99493":"sns.lineplot(x=train.Date,y=train.Close)","b7435f6c":"TIME_STEPS=30\n\ndef create_sequences(X, y, time_steps=TIME_STEPS):\n    Xs, ys = [], []\n    for i in range(len(X)-time_steps):\n        Xs.append(X.iloc[i:(i+time_steps)].values)\n        ys.append(y.iloc[i+time_steps])\n    \n    return np.array(Xs), np.array(ys)\n\nX_train, y_train = create_sequences(train[['Close']], train['Close'])\nX_test, y_test = create_sequences(test[['Close']], test['Close'])\n\nprint(f'Training shape: {X_train.shape}')\nprint(f'Testing shape: {X_test.shape}')","6876c7f2":"model = Sequential()\nmodel.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))\nmodel.add(Dropout(rate=0.2))\nmodel.add(RepeatVector(X_train.shape[1]))\nmodel.add(LSTM(128, return_sequences=True))\nmodel.add(Dropout(rate=0.2))\nmodel.add(TimeDistributed(Dense(X_train.shape[2])))\nmodel.compile(optimizer='adam', loss='mae')\nmodel.summary()","b879b81c":"history = model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.1,\n                    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min')], shuffle=False)\n","a82d5c6b":"plt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.legend();","9fb0c2c6":"model.evaluate(X_test, y_test)","9ae612f9":"X_train_pred = model.predict(X_train, verbose=0)\ntrain_mae_loss = np.mean(np.abs(X_train_pred - X_train), axis=1)\n\nplt.hist(train_mae_loss, bins=50)\nplt.xlabel('Train MAE loss')\nplt.ylabel('Number of Samples');\n\nthreshold = np.max(train_mae_loss)\nprint(f'Reconstruction error threshold: {threshold}')","75272c5f":"\nX_test_pred = model.predict(X_test, verbose=0)\ntest_mae_loss = np.mean(np.abs(X_test_pred-X_test), axis=1)\n\nplt.hist(test_mae_loss, bins=50)\nplt.xlabel('Test MAE loss')\nplt.ylabel('Number of samples')","3a0f24f8":"test_score_df = pd.DataFrame(test[TIME_STEPS:])\ntest_score_df['loss'] = test_mae_loss\ntest_score_df['threshold'] = threshold\ntest_score_df['anomaly'] = test_score_df['loss'] > test_score_df['threshold']\ntest_score_df['Close'] = test[TIME_STEPS:]['Close']\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=test_score_df['Date'], y=test_score_df['loss'], name='Test loss'))\nfig.add_trace(go.Scatter(x=test_score_df['Date'], y=test_score_df['threshold'], name='Threshold'))\nfig.update_layout(showlegend=True, title='Test loss vs. Threshold')\nfig.show()","005635c5":"anomalies = test_score_df.loc[test_score_df['anomaly'] == True]\nanomalies.shape","1f714c2e":"\nanomalies = test_score_df.loc[test_score_df['anomaly'] == True]\nanomalies\n","a64c7cd4":"anomalies.shape","5c8884d4":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=test_score_df['Date'], y=scaler.inverse_transform(test_score_df['Close']), name='Close price'))\nfig.add_trace(go.Scatter(x=anomalies['Date'], y=scaler.inverse_transform(anomalies['Close']), mode='markers', name='Anomaly'))\nfig.update_layout(showlegend=True, title='Detected anomalies')\nfig.show()","03176cc5":"# From the above it can be observed that the model has detected sudden price drops and sudden price hike in the HDFC Banks Stock \n# that is traded at NSE.\n# Another interesting observation would be that the anomalies detected are perfectly correlated with real life events. \n# On the 24th March the Indian stock prices crashed, and that was picked up by the model.\n# On the very next day, the stock price suddenly shot up on hopes of a coronavirus vaccine. Even this was successfully picked up \n # by the model\n# Before 24th March there were anomalies detected which are directly correlated with oil prices declining rapidly during the same time.\n# Finally I would like to add that the model somehow shows a stock price of 0 on the 27th October, 2019. I tried to resolve this issue\n# but couldn't. If you happen to figure it out. Feel free to share. Kudos.","88941f30":"**The Image below shows how the LSTM-Autoencoder model works**\n![image.png](attachment:image.png)","1ee89a6e":"**The threshold that we have found out is 1.33559\n**\n**Values above this threshold will treated as anomalies**","f01be52c":"**Now that we have detected the anomalies how do we validate our model. ( NOTE: IGNORE the 27th October,2019 data-point)**\nOn the 24th on March, 2020 the Indian stock market saw the biggest ever drop in it's entire history. Our model was successfully to detect that drop.\nIn the days that followed, the stock price rose sharply in the hope of a coronavirus vaccine. The model was able to successfully detect this sharp rise as well.\nWhile creating this notebbok, i also tried playing around with the hyperparameters and other values. I found that the model was also able to detect the drop in the stock price when the oil prices had started decliing in the March of 2020.\n","fbc4ae7e":"**UPVOTE IF YOU LIKED THE KERNEL. THANK YOU.**\nCheck out my other notebook on implementing a simple LSTM model\n\n> ****https:\/\/www.kaggle.com\/aayushkandpal\/stock-prediction-lstm-model-tesla-stock\n\n\n","140cb671":"**It can be observed from the above graph that there are few points that have breached the threshold value and will be considered to be anomalies**\n**From the above graph it can also be seen that thhere is very loss value for the data point that corresponds to 27th october 2019. Please treat this as a glitch.**","c4954895":"The biggest rise in the Dow Jones Industrial Average on 24th March. Similar pattern was followed in India and the stock price of HDFC Bank .\n![image.png](attachment:image.png)","24ea418a":"**So the model has predicted 5 anomalies based on the trend that followed from 1996 to 2018 with a Memory shape of 30 days**","dd1fd6f2":"How do LSTM models actually work\n![image.png](attachment:image.png)","a6d007f9":"NOTICE THE RED DOTS.","b1ed3f9e":"![image.png](attachment:image.png)\n\nHDFC Bank Limited is an Indian banking and financial services company headquartered in Mumbai, Maharashtra. It has a base of 1,04,154 permanent employees as of 30 June 2019. HDFC Bank is India\u2019s largest private sector bank by assets. It is the largest bank in India by market capitalisation as of March 2020.\nIn this notebook we will try and detect anomalies ( sudden highs and lows that occurred from 2018-2020 based on the trend that followed since it's inception in 1996)\n\nHere is mygithub profile here https:\/\/github.com\/Aayushk26\n","1116db3a":"**What are autoencoders and how do they work?**\n\nAn autoencoder is a neural network model that seeks to learn a compressed representation of an input.\n\nThey are an unsupervised learning method, although technically, they are trained using supervised learning methods, referred to as self-supervised. They are typically trained as part of a broader model that attempts to recreate the input.\n\n![image.png](attachment:image.png)","9d9fb38f":"![image.png](attachment:image.png)","cd360da6":"Some sources related to LSTM, Auto-Encoders and LSTM-Autoencoders\nhttps:\/\/towardsdatascience.com\/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352\nhttps:\/\/towardsdatascience.com\/lstm-autoencoder-for-extreme-rare-event-classification-in-keras-ce209a224cfb\nhttp:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/\nhttps:\/\/medium.com\/ai%C2%B3-theory-practice-business\/understanding-autoencoders-part-i-116ed2272d35\nhttp:\/\/ufldl.stanford.edu\/tutorial\/unsupervised\/Autoencoders\/","2e19c7de":"![image.png](attachment:image.png)\nThe key to LSTMs is the cell state, the horizontal line running through the top of the diagram.\n\nThe cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It\u2019s very easy for information to just flow along it unchanged.\n\n\nThe LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.\n\nGates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.\n\n\nThe sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means \u201clet nothing through,\u201d while a value of one means \u201clet everything through!\u201d\n\nAn LSTM has three of these gates, to protect and control the cell state.\nCheck out the complete article here http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/","55c0145f":"**This model uses a LSTM-Autoecnoder architecture**\nFor more details check out this article that will help you build your first LSTM model in no time.\nhttps:\/\/machinelearningmastery.com\/use-timesteps-lstm-networks-time-series-forecasting\/"}}