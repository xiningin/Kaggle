{"cell_type":{"f046d058":"code","eb3950b9":"code","1b454ec5":"code","2122b303":"code","023aa51e":"code","4822610f":"code","27b7a164":"code","8edac982":"code","db8da2dc":"code","9086eae0":"code","1c78beef":"code","279d8bbc":"code","124c74fa":"code","3bd2ae36":"code","69f06843":"code","d0964af9":"code","56bd6c0c":"code","dc0203ff":"code","e8696e6c":"code","57e31d13":"code","dd23c221":"code","30e185a5":"code","b66c7912":"code","36df2167":"code","5f63fb59":"code","79ad9e9d":"code","922c66c7":"code","0bd800b2":"code","1b5f4bd4":"code","9e2e36c7":"code","3cda0bac":"code","b4d47452":"code","89c58275":"code","6ad2b1bd":"code","3bfa2019":"code","60a35a4a":"code","739e0132":"code","088b416b":"code","d52f8b05":"code","fd838933":"code","fd5e6135":"code","115e6016":"code","0f58348b":"code","257326aa":"code","01e91dfc":"code","ba755b48":"code","06dd3777":"code","f968ee52":"code","7ea9d38e":"markdown","12d64e8d":"markdown","5a9ee19e":"markdown","87bb2508":"markdown","81be452c":"markdown","829381a6":"markdown","66354b3b":"markdown","cb920d41":"markdown","e7bbce6c":"markdown","09e550f1":"markdown","a2c65559":"markdown","ef6b793a":"markdown","aec0661a":"markdown","21c77a7e":"markdown","c8c35772":"markdown","52bb7cdf":"markdown","5d913cf1":"markdown","beb580d4":"markdown","0d9e0844":"markdown","1b8e7235":"markdown","9e7f77cb":"markdown","18be0247":"markdown","4dd56370":"markdown","5bca43b8":"markdown","e7d9d99e":"markdown","495a8ae7":"markdown","8dc9f5dc":"markdown","6d2b6826":"markdown","ed55eb90":"markdown","aa44ffe1":"markdown","d2623158":"markdown","060f8493":"markdown","b83093ca":"markdown","2a7b8a60":"markdown","97137e7d":"markdown","ab6b5b82":"markdown","f0465d22":"markdown","ff38379c":"markdown","6a81319d":"markdown","d8d63085":"markdown","25b4d9a3":"markdown","9a30d955":"markdown","5c77b864":"markdown","c81db9e5":"markdown","d450a566":"markdown"},"source":{"f046d058":"import pandas as pd\nimport numpy as np\nimport scipy.stats as ss\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport folium\nfrom branca.element import Figure","eb3950b9":"housing_df = pd.read_csv(\"..\/input\/housesalesprediction\/kc_house_data.csv\")\nhousing_df.head()","1b454ec5":"housing_df.info()","2122b303":"housing_df.describe().drop(columns = 'id') #id is dropped temporarily as it does not provide useful information","023aa51e":"housing_df.yr_renovated.replace(0, np.nan, inplace = True)\nhousing_df.sqft_basement.replace(0, np.nan, inplace = True)","4822610f":"sns.pairplot(housing_df[\n    ['price', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement','sqft_living15', 'sqft_lot15']\n])\nplt.show()","27b7a164":"columns = ['price', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement','sqft_living15', 'sqft_lot15']\nnumber_of_columns=4\nnumber_of_rows = len(columns)-1\/number_of_columns\nplt.figure(figsize=(3*number_of_columns,5*number_of_rows))\nfor i in range(0,len(columns)):\n    plt.subplot(number_of_rows + 1,number_of_columns,i+1)\n    sns.set_style('whitegrid')\n    sns.boxplot(y=columns[i], data=housing_df ,color='green')\n    plt.tight_layout()","8edac982":"housing_df['date'] = pd.to_datetime(housing_df['date'])\nfig, ax = plt.subplots(figsize=(9,5))\nsns.lineplot(x=\"date\", y=\"price\", data = housing_df, estimator = np.median)\nplt.show()","db8da2dc":"prices = housing_df.groupby(pd.Grouper(key='date', freq='M'))['price']\nprice_month = {}\nfor name, price in prices:\n    price_month[name] = price\n\nfig, ax = plt.subplots(figsize=(9,5))\nsns.boxplot(data = pd.DataFrame(price_month))\nplt.xticks(rotation=90)\nplt.xlabel(\"Date\")\nplt.ylabel(\"Price\")\nplt.show()","9086eae0":"fig, ax = plt.subplots(figsize=(9,7))\nhousing_df.yr_renovated.replace(0, np.nan, inplace= True) # Replace 0 as null values for year_renovated\nsns.lineplot(x=\"yr_built\", y=\"price\", data = housing_df)\nsns.lineplot(x=\"yr_renovated\", y=\"price\", data = housing_df)\nplt.show()","1c78beef":"seattlemap_fig=Figure(width=750,height=450)\nzipcode_median = housing_df.groupby(\"zipcode\")[[\"price\"]].median().reset_index()\n\nseattle_map = folium.Map(location=[47.6062, -122.3321 ], zoom_start=7)\nseattlemap_fig.add_child(seattle_map)\n\nfolium.Choropleth(\n    geo_data='dataset\/Zip_Codes.geojson',\n    data=zipcode_median,\n    columns=['zipcode', 'price'],\n    key_on='feature.properties.ZIP',\n    fill_color='YlOrRd', \n    nan_fill_color = \"#ffffff\",\n    fill_opacity=1, \n    line_opacity=1,\n    smooth_factor=0\n).add_to(seattle_map)\n\nseattle_map","279d8bbc":"seattlemap_fig=Figure(width=750,height=450)\nzipcode_waterfront = housing_df.groupby(\"zipcode\")[[\"waterfront\"]].mean().reset_index()\n\nseattle_map = folium.Map(location=[47.6062, -122.3321 ], zoom_start=7)\nseattlemap_fig.add_child(seattle_map)\n\nfolium.Choropleth(\n    geo_data='dataset\/Zip_Codes.geojson',\n    data=zipcode_waterfront,\n    columns=['zipcode', 'waterfront'],\n    key_on='feature.properties.ZIP',\n#     fill_color='YlOrRd', \n    nan_fill_color = \"#ffffff\",\n    fill_opacity=1, \n    line_opacity=1,\n    smooth_factor=0\n).add_to(seattle_map)\n\nseattle_map","124c74fa":"cat_col = [\"bedrooms\", \"bathrooms\", \"floors\", \"waterfront\", \"view\", \"grade\", \"condition\"]\nplt.figure(figsize=(15,10))\nfor i, col in enumerate(cat_col):\n    sns.set_palette(sns.color_palette(\"Paired\"))\n    ax = plt.subplot(3,3,i+1)\n    sns.barplot(x=col, y ='price', estimator = np.median, data = housing_df, ax = ax)\n    sns.set_style('whitegrid')\n    plt.xticks(rotation=90)\n    plt.ylabel(\"Median Price\")\n    plt.tight_layout()\nplt.show()","3bd2ae36":"plt.figure(figsize=(12,12))\nsns.heatmap(\n    housing_df.drop(columns = ['id','date','zipcode','long','lat']).corr().abs(),\n    cmap=\"cubehelix_r\",\n    annot = True)\nplt.show()","69f06843":"housing_df_dropped = housing_df.drop(columns=['id', 'date', 'zipcode'])","d0964af9":"print(\"Percentage of Missing Values in yr_renovated column:\\n{:.2f}%\".format(housing_df_dropped.yr_renovated.isnull().sum()*100 \/ housing_df_dropped.shape[0]))\nprint(\"Percentage of Missing Values in sqft_basement column:\\n{:.2f}%\".format(housing_df_dropped.sqft_basement.isnull().sum()*100 \/ housing_df_dropped.shape[0]))","56bd6c0c":"housing_df_dropped.drop(columns = \"yr_renovated\", inplace = True) # Drop yr_renovated column which has 95% of null values\nhousing_df_dropped.drop(columns = \"sqft_basement\", inplace = True) # Drop yr_renovated column which has 60% of null values\nprint(\"yr_renovated\" in housing_df_dropped.columns)\nprint(\"sqft_basement\" in housing_df_dropped.columns)","dc0203ff":"highly_skewed_columns = ['sqft_living', 'sqft_lot', 'sqft_above','sqft_living15', 'sqft_lot15']\nhousing_df_dropped[highly_skewed_columns] = np.log(housing_df_dropped[highly_skewed_columns])","e8696e6c":"number_of_columns=4\nnumber_of_rows = len(highly_skewed_columns)-1\/number_of_columns\nplt.figure(figsize=(3*number_of_columns,5*number_of_rows))\nfor i in range(0,len(highly_skewed_columns)):\n    plt.subplot(number_of_rows + 1,number_of_columns,i+1)\n    sns.set_style('whitegrid')\n    sns.boxplot(y=highly_skewed_columns[i], data=housing_df_dropped ,color='green')\n    plt.tight_layout()","57e31d13":"from sklearn.feature_selection import RFECV, SelectFromModel\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\nfrom sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, RandomizedSearchCV\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor, AdaBoostRegressor, ExtraTreesRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error","dd23c221":"X = housing_df_dropped.drop(columns = 'price')\ny = housing_df_dropped['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 12)","30e185a5":"def model_evaluation(clf, X_train, X_test, y_train, y_test, scoring=[mean_squared_error, mean_absolute_error, r2_score],\n                     columns = [\"mse\", \"mae\", \"r2\"], trainvstest = True ,graph = True, rmse = True, return_table = True, log=False):\n        hist = []\n        pred_train = clf.predict(X_train)\n        pred_test = clf.predict(X_test)\n        \n        \n        if log: # if y label is log scaled\n            y_train = np.exp(y_train) # Inverse Log Scale for training data\n            pred_train = np.exp(pred_train) # Inverse of Log Scale for training set\n            pred_test = np.exp(pred_test) # Inverse of Log Scale for training set\n        \n        for score in scoring:\n            if rmse and score == mean_squared_error:\n                if trainvstest: # if compare both train and test set\n                   hist.append(round(score(y_train, pred_train, squared=False),2))\n                hist.append(round(score(y_test, pred_test, squared=False),2))\n                continue\n            if trainvstest:\n                hist.append(round(score(y_train, pred_train),2))\n            hist.append(round(score(y_test, pred_test),2))\n        cols = columns.copy()\n        if rmse:\n            try:\n                cols[cols.index(\"mse\")] = 'rmse'\n            except:\n                assert False,\"Mean Squared Error is missing from columns\"\n            \n        if trainvstest:\n            cols = np.array([[\"train_\"+col, \"test_\"+col] for col in cols]).flatten()\n        \n        if graph:\n            fig, ax = plt.subplots(figsize=(7,7))\n            plt.scatter(y_test, pred_test, c='crimson')\n            p1 = max(max(pred_test), max(y_test))\n            p2 = min(min(pred_test), min(y_test))\n            plt.plot([p1, p2], [p1, p2], 'b-')\n            plt.xlabel('True Values', fontsize=15)\n            plt.ylabel('Predictions', fontsize=15)\n            ax.set_aspect('equal')\n            plt.show()\n            \n        if return_table:\n            return pd.DataFrame([hist], columns=cols, index = [type(clf).__name__])\n        else:\n            return type(clf).__name__, hist, cols","b66c7912":"dummy = DummyRegressor()\ndummy.fit(X_train, y_train)\nmodel_evaluation(dummy, X_train, X_test, y_train, y_test)","36df2167":"def models_selection(clfs:list, X_train, X_test, y_train, y_test, \n                     scoring=[mean_squared_error, mean_absolute_error, r2_score],\n                     columns = [\"mse\", \"mae\", \"r2\"], trainvstest = True,\n                     graph = False, rmse = True, return_table = False):\n    hists = []\n    model_names = []\n    for clf in tqdm(clfs):\n        clf.fit(X_train, y_train)\n        model_name, hist, col_name = model_evaluation(clf, X_train, X_test, y_train, y_test, \n                         scoring=scoring,columns = columns, trainvstest = trainvstest,\n                         graph = graph, rmse = rmse, return_table = return_table)\n        model_names.append(model_name)\n        hists.append(hist)\n    \n    return pd.DataFrame(hists, columns = col_name, index = model_names)","5f63fb59":"models = [\n    LinearRegression(), Lasso(), Ridge(), #linear_model\n    KNeighborsRegressor(), #distance_based_model\n    SVR(kernel = 'linear'), SVR(kernel = 'rbf'), SVR(kernel = 'poly', degree = 2), SVR(kernel = 'sigmoid'), #SVMs\n    DecisionTreeRegressor(random_state = 12), #tree model\n    RandomForestRegressor(random_state = 12), #ensemble tree models\n    GradientBoostingRegressor(random_state = 12),\n]\nmodels_selection(models, X_train, X_test, y_train, y_test)","79ad9e9d":"tree = DecisionTreeRegressor(random_state = 12)\n\ntree.fit(X_train, y_train)\nmodel_evaluation(tree, X_train, X_test, y_train, y_test)","922c66c7":"pd.DataFrame(list(zip(X_train.columns, tree.feature_importances_))).sort_values(1, ascending = False)","0bd800b2":"X= housing_df.drop(columns = [\n    'id', # No meaning\n    'date', # Not useful\n    'price', # Target Variable\n    'yr_renovated', # High Null Values\n    'sqft_basement' # High Null Values\n])\ny = housing_df['price']\nhighly_skewed_columns = ['sqft_living', 'sqft_lot', 'sqft_above','sqft_living15', 'sqft_lot15']\nX[highly_skewed_columns] = np.log(X[highly_skewed_columns]) # Log transform columns with high number of outliers\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 42)\nX_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size = .25, random_state = 42)","1b5f4bd4":"#To understand which features has higher absolute linear correlation towards price\nhousing_df.corr().abs()['price'].sort_values(ascending=False) ","9e2e36c7":"# Extract the mean of features based on the zipcode\nzipcode_df = X_train.groupby(\"zipcode\").mean()[['grade', 'sqft_living', 'sqft_living15', 'bathrooms', 'sqft_above']]\nzipcode_df","3cda0bac":"from scipy.cluster.hierarchy import dendrogram\nfrom sklearn.cluster import AgglomerativeClustering\n\n\ndef plot_dendrogram(model, **kwargs):\n    # Create linkage matrix and then plot the dendrogram\n\n    # create the counts of samples under each node\n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx < n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack([model.children_, model.distances_,\n                                      counts]).astype(float)\n\n    # Plot the corresponding dendrogram\n    dendrogram(linkage_matrix, **kwargs)\n\n# setting distance_threshold=0 ensures we compute the full tree.\nmodel = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n\nmodel = model.fit(zipcode_df)\nplt.title('Hierarchical Clustering Dendrogram')\n# plot the top three levels of the dendrogram\nplot_dendrogram(model, truncate_mode='level', p=3)\nplt.hlines(2, 0, 300, colors = 'r')\nplt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\nplt.show()","b4d47452":"cluster_model = AgglomerativeClustering(n_clusters=4) # Setting Number of Cluster to 4\nzipcode_df['zipcode_cluster'] = cluster_model.fit_predict(zipcode_df)\nX_train = pd.merge(X_train, zipcode_df[['zipcode_cluster']].reset_index(), on='zipcode', how = 'left')\nX_dev = pd.merge(X_dev, zipcode_df[['zipcode_cluster']].reset_index(), on='zipcode', how = 'left')\nX_test = pd.merge(X_test, zipcode_df[['zipcode_cluster']].reset_index(), on='zipcode', how = 'left')\nX_train","89c58275":"sns.barplot(x = 'zipcode_cluster', y = 'price',data = pd.concat([X_train,y_train], axis = 1))\nplt.title(\"Mean Price against Zipcode Cluster\")\nplt.show()","6ad2b1bd":"cat_features = ['zipcode_cluster']\nnum_features = [col for col in X_train.columns if col not in cat_features]\n\nnum_features_pipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"poly\", PolynomialFeatures()),\n])\n\ncat_features_pipe = Pipeline([\n    (\"onehot\", OneHotEncoder(drop='first'))\n])\n\nprep_pipe = ColumnTransformer([\n    (\"num\", num_features_pipe, num_features),\n    (\"cat\", cat_features_pipe, cat_features)\n])\n\npipe = Pipeline([\n    (\"preprocess\", prep_pipe),\n    (\"estimator\", GradientBoostingRegressor(random_state = 12))\n])","3bfa2019":"pipe.fit(X_train,y_train)","60a35a4a":"model_evaluation(pipe, X_train, X_dev, y_train, y_dev)","739e0132":"def models_selection_pipeline(pipe, clfs:list, X_train, X_test, y_train, y_test, \n                     scoring=[mean_squared_error, mean_absolute_error, r2_score],\n                     columns = [\"mse\", \"mae\", \"r2\"], trainvstest = True,\n                     graph = False, rmse = True, return_table = False, log = False):\n    hists = []\n    model_names = []\n    for clf in tqdm(clfs):\n        pipe.steps.pop(-1) #Remove Final Estimator\n        pipe.steps.append(['estimator',clf]) #Append Estimator into pipeline\n        \n        pipe.fit(X_train, y_train)\n        model_name, hist, col_name = model_evaluation(pipe, X_train, X_test, y_train, y_test, \n                         scoring=scoring,columns = columns, trainvstest = trainvstest,\n                         graph = graph, rmse = rmse, return_table = return_table, log = log)\n        model_names.append(type(clf).__name__)\n        hists.append(hist)\n    \n    return pd.DataFrame(hists, columns = col_name, index = model_names)","088b416b":"tree_models = [\n    DecisionTreeRegressor(random_state = 12), \n    RandomForestRegressor(random_state = 12), \n    GradientBoostingRegressor(random_state = 12),\n    BaggingRegressor(random_state = 12),\n    AdaBoostRegressor(random_state = 12),\n    ExtraTreesRegressor(random_state = 12)\n]\nmodels_selection_pipeline(pipe, tree_models, X_train, X_dev, y_train, y_dev)","d52f8b05":"cv_hist = cross_validate(pipe, X_train, y_train, scoring = 'neg_root_mean_squared_error', n_jobs = -1)\npd.DataFrame(cv_hist).describe().rename(columns = {'test_score':'Root Mean Square'})[['Root Mean Square']].abs().T[['mean','std']]","fd838933":"# pipe.steps.pop() # Remove estimator\n# pipe.steps.append(('estimator', GradientBoostingRegressor())) # Replace estimator with GradientBoostingRegressor\n# params = {\n#     \"estimator__n_estimators\" : np.linspace(100,500,5, dtype = int),\n#     \"estimator__subsample\" : [0.2, 0.5, 0.8, 1.0],\n#     \"estimator__min_samples_leaf\" : [0.01],\n#     \"estimator__random_state\" : [12],\n#     \"estimator__ccp_alpha\" : [0, 0.1,0.2,0.3]\n# }\n# random_search_cv = RandomizedSearchCV(pipe, params,scoring = 'neg_root_mean_squared_error', n_iter = 20)\n# random_search_cv.fit(X_train,y_train)","fd5e6135":"# random_search_cv.best_params_","115e6016":"randomsearch_pipe = Pipeline([\n    (\"preprocess\", prep_pipe),\n    (\"estimator\", GradientBoostingRegressor(\n        subsample= 1,\n        random_state = 12,\n        n_estimators = 500,\n        min_samples_leaf= 0.01,\n        ccp_alpha= 0.3\n        )\n    )\n])","0f58348b":"randomsearch_pipe.fit(X_train, y_train)","257326aa":"model_evaluation(randomsearch_pipe, X_train, X_dev, y_train, y_dev)","01e91dfc":"cv_hist = cross_validate(randomsearch_pipe, X_train, y_train, scoring = 'neg_root_mean_squared_error', n_jobs = -1)\npd.DataFrame(cv_hist).describe().rename(columns = {'test_score':'Root Mean Square'})[['Root Mean Square']].abs().T[['mean','std']]","ba755b48":"model_evaluation(randomsearch_pipe, X_train, X_test, y_train, y_test)","06dd3777":"# import pickle\n# pickle.dump(randomsearch_pipe, open( \"model\/king_county_pipeline.p\", \"wb\" )) #Dumping model into pickle file","f968ee52":"# loaded_model = pickle.load( open( \"model\/king_county_pipeline.p\", \"rb\" ) ) #Loading model\n# loaded_model.predict(X_test) #Perform prediction with loaded model","7ea9d38e":"# Exploratory Data Analysis","12d64e8d":"## Feature Engineering\nTo improve the performance of model, we can perform some feature engineerig to bring out more information about the house that can help the model in its prediction.","5a9ee19e":"### Geo-Spatial Data\nThere are a few features that reviews the pricing information through geo-spatial features which includes [\"long\", \"lat\", \"zipcode\", \"waterfront\"].\n\nAlthough we have zipcode that reviews the region that the house is located in, the high cardinality in zipcode columns makes pure one-hot encoding expensive as it will subject to the Curse of Dimentionality.\n\nHence, it will be great if we can make use of the zipcode to identify a few cluster that has higher average house prizes and cluster with lower house prizes. With that aim in mind, we can make use of Unsupervised clustering to generate the clustering for us.","87bb2508":"## Personal Learning Reflection\n\nThrough the seattle housing price prediction problem, I've learned more of **Geo-location Feature Engineering** without leaking the actual housing prices as well as grasp a better understanding of the **Bias-Variance trade-off** through countless iterations of redefining the params grid, hyperparameter tuning, model evaluation and again! Initially due to the poor design of parameter searching grid, which results in the resulting model being more overfitted than the default parameter, despite the drop in test error. By doing more research and read-up on the Gradient Boosting, I've identified several key hyperparameters that can improve the performance without increasing the variance as much. I've also decided to make use of **AWS Sagemaker** to host and run the entire experiment to speed up the experimenting iteration for this project.\n\nWritten By : Wong Zhao Wu\n\nLast Modified : 25 May 2021\n\n![seattle.jpg](https:\/\/images.unsplash.com\/photo-1502175353174-a7a70e73b362?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=970&q=80)\n\nImage retrieved from [Unsplash](https:\/\/unsplash.com\/photos\/skUTVJi8-jc).\n","81be452c":"After evaluating the model with hold-out dev set, 5-Fold cross-validation, we noticed that RandomizedCV hyperparameters tuning have reduced the dev set error as well as overall cross-validation error of a small margin at the cost of slight increase in variance of our model which can be noticed by the 2% increase of gap between train and test for the final tuned pipeline.\nDue to the time constraint, I did not pursue further to try reduce the high variance caused after hyperparameters tuning.(As this is an assignment with deadline) \n\nLastly, before wrapping up the model, we perform final evaluation with out hold-out test set as the final scrutiny of the ability to generalize of our model. ","829381a6":"## Train-Test-Split\nWe split our dataset into train and test set to better evaluate the performance of the model by examining its performance against the train set and test set.","66354b3b":"After running hyperparams tuning for several hours, we make use of the best params generated from RandomizedSearchCV and build our final pipeline and perform final evaluation with hold-out dev set and test set along with 5-Fold Cross Validation","cb920d41":"# Modelling","e7bbce6c":"# Data Preprocessing\n\nAfter EDA, we have identified the following flaws in our dataset.\n1. Dropping Columns that provides no information to target variable\n2. Missing Values in yr_renovated, sqft_basement\n3. Highly skewed columns with extreme outliers\n4. Weak linear correlation between certain columns (Evaluate Further After Modelling)","09e550f1":"# Conclusion\nBy using GradientBoostingRegressor along with several preprocessing techinuqes, we have managed to predict the housing prices with its attribute down to accuracy of around 129k of root mean squared error on average by trying out various preprocessing techniques and geo-location feature engineering.","a2c65559":"From the result of model selection with pipeline, we noticed that GradientBoostingRegressor looks like hte best candidate with low biasses and low variance. We perform 5-Fold cross validation towards the GradientBoostingRegressor pipeline and evaluate the mean and std of root mean square error.","ef6b793a":"# Hyperparameter Tuning\nFrom the model selection results with the pipeline, we've decided to explore hyperparamter tuning on GradientBoostingRegressor Pipeline since it looks like a promissing candidate with low bias and relatively low variance.","aec0661a":"### Housing Price with Time\n\nSince date column is included in the dataset, we can visualise the general trend of the housing prices to discover any interesting insights.\n\n**Observations**\n1. Housing prices seems to be fructuating throughout the months without any visable trend that we can observe.\n2. The sudden spike around october 2014 is due to presense of outliers than general trend of the data.","21c77a7e":"# Pipeline\nWe decided to make use of pipeline to wrap several preprocessing modules to preprocess the data before parsing it into the final model.\n\nPipeline Members:\n1. ColumnTransformer : Split between different pipeline for categorical features(to be one-hot encoded) and numerical features(to be normalised).\n2. OneHotEncoder : Convert Categorical Features into dummy variables (drop ='first').\n3. PolynomialFeatures : Generate a new feature consisting of all polynomial combinations of the features.\n4. StandardScaler : Ensures all features are at the same scale of mean = 0, standard deviation = 1\n5. SelectFromModel : Make use of LASSO regression to make the features sparse and select the best features to be parsed to the final model\n6. Final Predictor : Tree-based Regressor\n","c8c35772":"## Descriptive Summaries\nBy running `.info()` on our dataframe, the following are the initial observation\nof the dataset.\n\n**Observations**\n\n1. The shape of dataset is `(21612, 21)` whereby there is 21612 observations and 21 columns. \n(21 Features + 1 Target Variable: `\"price\"`)\n2. Datatype for all columns are accurate except for `\"date\"` which can be converted to DateTime64 for better visualisation.\n3. No Missing Values are observed at a glance.","52bb7cdf":"## Train-Test-Dev Split\nWe re-split our data into Train(60%), Dev(20%), Test(20%) set for us to make decision based on the Dev set and finally evaluate the performance based on the test set.","5d913cf1":"By visualising the waterfront label from the dataset, we observed that only certain area is flagged as waterfront and it sort of miss out the hotspot visualise from the color pricing map above.\n\nHence, perhaps feature engineering is needed to better help the model to understand the relation of its geographic location to the pricing","beb580d4":"## Correlation Matrix\nFrom pearson correlation matrix, we observed that [\"sqft_lot\", \"condition\", \"yr_built\", \"sqft_lot15\"] has correlation $p<0.1$ which indicates weak positive linear relationship against target variable, price. The result shown is consistent with the observation made through the pairplots.\n\nBesides, we should also be awared of the high linear correlation between variables sqft_living-sqft_above, grade-sqft_living which can cause potential unstableness for linear model.","0d9e0844":"## RandomSearchCV Hyperparameter Tuning Evaluation","1b8e7235":"## Model Selection\nAfter building our baseline model and evaluation function, we continue to test out different model in sklearn and decide which model fits better to the dataset.","9e7f77cb":"## Categorical Columns\nSince all categorical columns are ordinal categorical data, there isin't much data preprocessing needed to be done as the data are ready to be parse into a machine learning model.","18be0247":"## Graphical Summaries\n","4dd56370":"After log transformation, the columns appear to be more normalised and unskewed. However, there are still alot of outliers observed in the columns. For now, we can leave it as it is and evaluate further during model improvement later.","5bca43b8":"### Pairplots\nWe plot out the pairplots to study the general distribution of numerical columns as well as its interaction and relationships with other columns.\n\n**Observations**\n\n1. Positively Skewed Features\n    \n    `[\"sqft_living15\", \"sqft_above\", \"sqft_living\", \"price\"]`\n    > Log Transformation might be required to unskew the data.\n\n2. Positive Linear Relationship with target variable, `\"price\"`\n\n    `[\"sqft_living\", \"sqft_above\", \"sqft_living15\"]`\n    > This indicates that the features mentioned might be useful in prediction of target variable\n\n3. Extreme Outliers\n    From the histogram of sqft_lot and its scatter plots,  we noticed that most of its datapoints are at lower-end region with multiple extreme outliers.\n    `[\"sqft_lot\"]`\n    > We might need to consider dropping the outliers","e7d9d99e":"## Log Transformation\nWe perform log transformation to unskewed the feature columns that are highly skewed with extreme outliers. Histograms are subsequently plotted to evaluate the effectiveness of the approach.","495a8ae7":"From the feature importance, the following are the observations:\n    1. Features related to the housing details like grade, sqft_living, sqft_living15 seems to provide useful information about the price of the house.\n    2. Geo-spatial information of the house like lat,long and waterfront are quite useful in predicting the pricing of house\n    \nHence, further features engineering on the geo-spatial data might help improve the performance of model. Further feature cleaning and preprocessing towards features related to housing information. It might also be helpful for us to revisit the decisions made earlier and evaluate does it help us in getting better performance in our model.","8dc9f5dc":"# King County Housing Regression\n### Buy Low Sell High\n\nAuthor : Wong Zhao Wu, Bryan\n\n## Modelling Objective\nPerform EDA and Modelling to find the optimal solution in estimating the housing prices by minimizing Root Mean Squared Error as the primary metrics.\n\n## Keywords\n- Supervised Learning\n- Regression\n- Feature Engineering\n- Hierarchical Clustering\n- Gradient Boosted Trees","6d2b6826":"From the test error, although it is slightly higher than the average 5-Fold error, it is still within 1 $\\sigma$ from our mean and hence the result is still acceptable. \n\nIn general, from the plot above we can clearly visualize that the prediction error increases as the actual price increase to larger margin. This indicates that the model is not doing as well for housing prices of higher value as compared to average to lower housing price.","ed55eb90":"## Saving Model\nAfter completing model training and tuning, we can save our model into external file and reload it later during deployment.","aa44ffe1":"From the dendrogram, we decided to make the cut at y=2 so we will only have 4 main cluster left using the zipcode.","d2623158":"## Model Selection With Pipeline\nAfter implementing pipeline into our final model, we've build a custom function that can loop through each estimator and replace it with the selected model.","060f8493":"### Statistical Summaries\nWe explore the variables through `.describe()` to get a rough sense of the mean, median, standard deviation of the data to spot for potential outliers and skewness in the data.\n**Observations**\n\n1. Extreme Outliers\n\n    By comparing the Max to 75% and Min to 25%, we notice a few extreme outliers that might need to take care of.\n    - sqft_living : Sudden jump of sqft_living from 2550sqft at 75% to 13450sqft at maximum flags the presense of extreme outliers.\n    - sqft_above : Sudden jump of sqft_above from 2210sqft at 75% to 9410sqft at maximum flags the presense of extreme outliers.\n    - sqft_lot15 : Sudden jump of sqft_lot15 from 10083sqft at 75% to 871200sqft at maximum flags the presense of extreme outliers.\n\n2. Data Skewness\n\n    By comparing mean, median, 25%, 75%, min and max values, we can get rough sense of which columns might suffers from data skewness.\n    - sqft_living\n    - sqft_above\n    - sqft_lot15\n    \n3. Missing Values\n\n    For [\"yr_renovated\",\"sqft_basement\"], the minimum values of 0 is observed. However, it is not logical for a year and basement to be zero which means that null value is presense in [\"yr_renovated\",\"sqft_basement\"] columns\n\nFurther visualisation is needed to better understands the data and spot potential pain point that we might need to work on for model improvement.","b83093ca":"From the training outcome, the following are the observations for different family of models:\n1. Linear Models(Linear Regression, Lasso[L1 Norm] , Ridge[L2 Norm]):\n    - Does not seems to suffers from major overfitting\n    - Might be suffering from underfitting due to inductive biases.\n    > Try out polynomial regression and evaluate the model performance\n2. Distance Based models(KNeighborsRegressor):\n    - Suffer from major overfitting.\n    - Might be suffering from slight underfitting.\n    > Increase the number of neighbours to reduce overfitting\n3. SVMs with (linear, rbf, poly, sigmoid) kernels:\n    - Changing of Kernels does not seems to improve the performance of SVMs\n    - SVMs suffers from high biases as the train and test set errors are relatively high\n    > Either increase the model complexity by increasing regularization term $C$ or invest time on other models\n4. DecisionTree: \n    - Suffers from major overfitting\n    - Low biases and able to obtain relatively lower testing error\n    > Tune model better to reduce overfitting by limiting the model complexity\n5. Ensemble Tree Models (RandomForest, GradientBoosting)\n    - Does not suffer as much overfitting than decision tree, however, overfitting still occurs\n    - Low biases and able to obtain relatively lower testing error\n    > Tune model better to reduce overfitting by increasing regularization\n    \nConclusions:\n1. Tree-based models can produce promising results after reducing overfitting. Hence, further model tuning is required to reduce overfitting.\n2. Linear models suffers from high biases. Hence, we can try to increase the model complexity to reduce overfitting.","2a7b8a60":"\"yr_renovated\" seems to form a positive linear relationship towards \"price\". As for \"yr_built\", the linear relationship is weaker and not as obvious.","97137e7d":"# Reading Dataset\nThe [datasets](https:\/\/geodacenter.github.io\/data-and-lab\/\/KingCounty-HouseSales2015\/) includes the home sales prices and characteristics for Seattle and King County, WA (May 2014 - 2015).\n\n## Data Dictionary\n|Columns| Description |\n|:---|:---|\n|id | Unique ID for each home sold|\n|date | Date of the home sale|\n|price | Price of each home sold|\n|bedrooms | Number of bedrooms|\n|bathrooms | Number of bathrooms, where .5 accounts for a room with a toilet but no shower|\n|sqft_living | Square footage of the apartments interior living space|\n|sqft_lot | Square footage of the land space|\n|floors | Number of floors|\n|waterfront | A dummy variable for whether the apartment was overlooking the waterfront or not|\n|view | An index from 0 to 4 of how good the view of the property was|\n|condition | An index from 1 to 5 on the condition of the apartment|\n|grade | An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design.|\n|sqft_above | The square footage of the interior housing space that is above ground level|\n|sqft_basement | The square footage of the interior housing space that is below ground level|\n|yr_built | The year the house was initially built|\n|yr_renovated | The year of the house\u2019s last renovation|\n|zipcode | What zipcode area the house is in|\n|lat | Lattitude|\n|long | Longitude|\n|sqft_living15 | The square footage of interior housing living space for the nearest 15 neighbors|\n|sqft_lot15 | The square footage of the land lots of the nearest 15 neighbors|\n\nAdditional [geoJSON](https:\/\/gis-kingcounty.opendata.arcgis.com\/datasets\/zipcodes-for-king-county-and-surrounding-area-shorelines-zipcode-shore-area) file is obtained for purpose of insights gaining and data visualisation.","ab6b5b82":"Although at a glance, the relationship of zipcode_cluster with mean price might not be that obvious, the cluster allows us to review the information of zipcode without getting compromises with high number of columns. We can evaluate this decision at later timing. Note that zipcode_cluster is a nominal categorical data that requires one-hot encoding.","f0465d22":"## Baseline Classifier\nWe make use of dummy regressor as baseline model by always predicting the mean of target variable.The baseline predictor serves as reference point for model selection.","ff38379c":"From the mean and std generated from Cross Validation of GradientBoostingRegressor Pipeline, it seems that the result are still within the acceptable range as error from hold-out dev set does not deviate much from result of cross validation.","6a81319d":"## Handling Missing Values\nAs there are more than around 95% and 60.73% of data points are missing from the [\"yr_renovated\",\"sqft_basement\"] columns, the safest approach is to drop both columns entirely. We can evaluate this decision further during model improvement by trying other imputing methods like filling it with central tendency or perform iterative imputation.","d8d63085":"# Model Improvement\nAfter preliminary modelling, we've shortlisted two category of models to work on, linear model as well as tree-based model. Before we start any feature engineering or elimination as well as hyperparameters tuning, we first train a decision tree regressor with default parameters to understand how it is doing as well as the important features flagged by the model.","25b4d9a3":"### Housing Price with Geo-Location\n\nSince we are given the longitude and latitude as well as the zipcode of the house, we can visualise the median housing prizes along with its geo-location to visualise any trends in the housing prices.\n\n**Observations**\n1. Housing near the waterfront area seems to have higher median prices as compared to housing further away from waterfront.\n2. The waterfront label does not ","9a30d955":"## Dropping Columns\n\nWe will drop the following columns as it does not reviews any relationship with the target variable.\n- Id\n- Date\n- Zipcode (Although geolocation seemed to be quite useful in our prediction, zipcode is first dropped as it does not directly provide information to the models)","5c77b864":"### Housing Price with Categorical Features\n\nThere are several categorical columns observed in the dataset, we can visualise the categorical features along with the median price to visualise the impact of them towards the target variable, price.\n\nCategorical Columns : [\"bedrooms\", \"bathrooms\", \"floors\", \"waterfront\", \"view\", \"grade\"]\n\n**Observations**\n1. All categorical features seems to pocess certain positive linear relationship towards median prices except for \"floor\" which seemed to be relatively constant for each floor value.","c81db9e5":"Thank you for your reading!","d450a566":"### Boxplots\nBoxplot is plotted to better visualise the distribution of numerical columns along with its outliers.\n\n**Observations:**\n\n1. Extreme outliers is observed in `[\"sqft_basement\", \"sqft_lot15\", \"sqft_lot\"]` ."}}