{"cell_type":{"eb764bfc":"code","e51d590d":"code","08d0ad7f":"code","e463147f":"code","3393ea5d":"code","c2af451c":"code","6d25b3c6":"code","d9b376c9":"code","4bdc8949":"code","a217f202":"code","f442184f":"code","063d7c27":"code","7f3aeccd":"code","4900a262":"code","a10b81d8":"code","116e24cd":"code","bbc49f60":"code","f8d67cbb":"code","8c41a56e":"code","0ceee194":"code","29889769":"code","3b8d062b":"code","5cb6ea92":"code","11b21821":"code","4660bc34":"code","bf531b1b":"code","8687b7cc":"code","3b21ceeb":"code","d1788ea0":"code","05dab8f0":"code","8d3598d8":"code","020dcb32":"code","1db8d7dc":"markdown","9ca29015":"markdown","14969812":"markdown","7f4acb07":"markdown","9ce97c31":"markdown","c834de68":"markdown","afccb111":"markdown","6560b836":"markdown","79b847f1":"markdown","19bf1d77":"markdown","8d6a93c2":"markdown","a27aeb6b":"markdown","691d69c5":"markdown","d89da5e4":"markdown","9c302e3c":"markdown"},"source":{"eb764bfc":"from sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport numpy as np\nimport pandas as pd\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport seaborn as sns\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression","e51d590d":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_train.head()\ndf_test=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_test.shape","08d0ad7f":"df_train.shape","e463147f":"train_ID= df_train['Id']\ntrain_ID\ntest_ID= df_test['Id']\ntest_ID","3393ea5d":"df_test.drop('Id',axis=1,inplace=True)\ndf_test.drop('Utilities',axis=1,inplace=True)\ndf_test.shape\n\n#for training the data \ny_train=df_train.SalePrice.values\ny_train","c2af451c":"df_train.describe()","6d25b3c6":"#lets check for the outliers detection \n#fig, ax=plt.subplots()\n#ax.scatter(x=df_train['GrLivArea'], y=df_train['SalePrice'])\n#plt.ylabel('SalePrice',fontsize=13)\n#plt.xlabel('GrLivArea',fontsize=13)\n#plt.show()\n\n#Deleting outliers\n#df_train = df_train.drop(df_train[(df_train['GrLivArea']>3000) & (df_train['SalePrice']>11.5)].index)\n\n#again plot it \n#fig,ax=plt.subplots()\n#ax.scatter(x=df_train['GrLivArea'], y=df_train['SalePrice'])\n#plt.ylabel('SalePrice',fontsize=13)\n#plt.xlabel('GrLivArea',fontsize=13)\n#plt.show()","d9b376c9":"#corelation plot\ncorrmat = df_train.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.6]\nplt.figure(figsize=(6,6))\ng = sns.heatmap(\n    df_train[top_corr_features].corr(), \n    annot = True, cmap = \"Blues\", \n    cbar = False, vmin = .5, \n    vmax = .7, square=True\n    )","4bdc8949":"df_train['SalePrice'].describe()\nsns.distplot(df_train['SalePrice']);\n\n#skewness and kurtosis  https:\/\/towardsdatascience.com\/transforming-skewed-data-73da4c2d0d16\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())\n","a217f202":"#checking skewness and kurtosis and applying log function to reduce it.\nfrom scipy import stats\nfrom scipy.stats import norm, skew  #for statastics\n\n#plot histogram and probabaility\nfig=plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(df_train['SalePrice'],fit=norm);\n(mu,sigma)= norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.subplot(1,2,2)\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.suptitle('Before transformation')\n","f442184f":"# Apply log transformation\ndf_train.SalePrice = np.log1p(df_train.SalePrice )","063d7c27":"# New prediction\ny_train = df_train.SalePrice.values","7f3aeccd":"# Plot histogram and probability after transformation\nfig = plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(df_train['SalePrice'], fit=norm);\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.subplot(1,2,2)\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.suptitle('After transformation')","4900a262":"# Missing data in train\ntrain_miss = df_train.isnull().sum()\nprint(train_miss)\n\ntrain_miss.sort_values(ascending=False)\nprint(train_miss)","a10b81d8":"#missing values in test data\ntest_miss = df_test.isnull().sum()\nprint(test_miss)\n\ntest_miss.sort_values(ascending=False)\nprint(test_miss)","116e24cd":"#Fill out values with most common value\ncommonNa = [\n    'MSZoning','Electrical','KitchenQual',\n    'Exterior1st','Exterior2nd','SaleType',\n    'LotFrontage','Functional'\n    ]\n\n#Fill with zero value\ntoZero = [\n    'MasVnrArea','GarageYrBlt','BsmtHalfBath',\n    'BsmtFullBath','GarageArea','GarageCars',\n    'BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n    'TotalBsmtSF'\n    ]\n\n#Fill with No data\ntoNoData = [\n    'PoolQC','MiscFeature','Alley',\n    'Fence','FireplaceQu','GarageType', \n    'GarageFinish','GarageQual', \n    'GarageCond','BsmtQual','BsmtCond', \n    'BsmtExposure','BsmtFinType1','BsmtFinType2',\n    'MasVnrType'\n    ]\n","bbc49f60":"#Function fill missing values\ndef fillNan(df):\n    df['Functional']=df['Functional'].fillna('Typ')\n    \n    for i in commonNa:\n        df[i]=df[i].fillna(df[i].mode()[0])\n    for i in toNoData:\n        df[i]=df[i].fillna('None')\n    for i in toZero:\n        df[i]=df[i].fillna(0)\n        \n        #Removing utilities . No Predictive value\n        #df.drop(['Utilities'], axis=1, inplace=True)\n        \n    return df","f8d67cbb":"df_train.head()","8c41a56e":"df_train =fillNan(df_train) #passing (df_train,df_test in place of df)      \ndf_train.isnull().sum()","0ceee194":"#for test data\ndf_test =fillNan(df_test) #passing (df_test in place of df)       \ndf_test.isnull().sum()","29889769":"def skew_(df):\n    #columns which are skew-candidates\n    colls = [col for col in df.columns if df[col].dtype in ['int64','float']]\n    skews_df = [col for col in df[colls].columns if df[col].skew() > .7]\n\n    #function to correct skew\n    def skewfix(data, data2):\n        for i in data2:\n            data[i] = np.log1p(data[i])\n            return data\n    return skewfix(df, skews_df)\n\ndf_train, df_test = skew_(df_train), skew_(df_test)\n","3b8d062b":"#We need to encode variables with categorical data:\nencoder = LabelEncoder()\n#sc = StandardScaler()\n\ndef encode(df):\n    cat_df = [col for col in df.columns if df[col].dtype not in ['int','float']]\n    for col in cat_df:\n        df[col] = encoder.fit_transform(df[col])\n    #df_ = sc.fit_transform(df)\n    #df = pd.DataFrame(data=df_, columns = df.columns)\n    return df\n\ndf_test,df_train=encode(df_test),encode(df_train)","5cb6ea92":"#X = df_train.copy()\nX=df_train.drop(['Id', 'SalePrice'], axis=1)\n#print(X)\n\n##Below steps are not required, those are for my assumption\n\n#X3=df_train[['OverallQual', 'YearBuilt', 'TotalBsmtSF'  ,'1stFlrSF' , 'GrLivArea' ,'GarageCars' ,'GarageArea']]\n#X6=X3.iloc[:-1,:]\n#Y3=df_test[['OverallQual', 'YearBuilt', 'TotalBsmtSF'  ,'1stFlrSF' , 'GrLivArea' ,'GarageCars' ,'GarageArea']]\ny=df_train.SalePrice.values\n#Y3.shape\n#X6.shape\n","11b21821":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.35,random_state=100)","4660bc34":"from sklearn.svm import SVR\nregressor = SVR()\nregressor.fit(X_train,y_train)\nsvrpredict=regressor.predict(X_test)","bf531b1b":"for i in range(0,433):\n   print(\"Error in value number\",i,(y_test[i]-svrpredict[i]))","8687b7cc":"from sklearn.metrics import mean_squared_error\nmean_squared_error(y_test,svrpredict)","3b21ceeb":"import xgboost\nmodel = xgboost.XGBRegressor(colsample_bytree=0.4,\n                 gamma=0,                 \n                 learning_rate=0.07,\n                 max_depth=3,\n                 min_child_weight=1.5,\n                 n_estimators=10000,                                                                    \n                 reg_alpha=0.75,\n                 reg_lambda=0.45,\n                 subsample=0.6,\n                 seed=42) \n\nmodel.fit(X_train,y_train)\nxgb_prediction = model.predict(X_test)\n\n\ndef inv_y(transformed_y):\n    return np.exp(transformed_y)\n#print(inv_y(xgb_prediction))\n","d1788ea0":"for i in range(0,433):\n   print(\"Error in value number\",i,(y_test[i]-xgb_prediction[i]))","05dab8f0":"from sklearn.metrics import mean_squared_error\nmean_squared_error(y_test,xgb_prediction)","8d3598d8":"#X3=df_train[['OverallQual', 'YearBuilt', 'TotalBsmtSF'  ,'1stFlrSF' , 'GrLivArea' ,'GarageCars' ,'GarageArea']]\n#X6=X3.iloc[:-1,:]\n#Y3=df_test[['OverallQual', 'YearBuilt', 'TotalBsmtSF'  ,'1stFlrSF' , 'GrLivArea' ,'GarageCars' ,'GarageArea']]\n#Y3.shape\n#y=df_train.iloc[:-1,:].SalePrice.values\n#X6.shape\n#y.shape\n\nX=df_train.drop(['Id', 'SalePrice'], axis=1)\nX4=X.iloc[:-1,:]\nX5=X4.drop('Utilities',axis=1)\ny=df_train.iloc[:-1,:].SalePrice.values\nX5.shape\ndf_test.shape\ny.shape\n\nimport xgboost\nmodel = xgboost.XGBRegressor(colsample_bytree=0.4,\n                 gamma=0,                 \n                 learning_rate=0.07,\n                 max_depth=3,\n                 min_child_weight=1.5,\n                 n_estimators=10000,                                                                    \n                 reg_alpha=0.75,\n                 reg_lambda=0.45,\n                 subsample=0.6,\n                 seed=42) \n\nmodel.fit(X5,y)\nxgb_prediction = model.predict(df_test)\n\n#for inverse, to get original values\ndef inv_y(transformed_y):\n    return np.exp(transformed_y)\n#print(inv_y(xgb_prediction))","020dcb32":"submission=pd.DataFrame({ \"Id\": test_ID, \"SalePrice\": inv_y(xgb_prediction)})\nsubmission.to_csv('Submissionfile.csv',index=False)","1db8d7dc":"view the data frame","9ca29015":"## Label Encoder \nUsing Scikit learn method for converting categorical to numerical.","14969812":"## Corelation Plot\nBy this corelation plot we can know the features which are highly related to target variable.so we can train model with only the features highly corelated, It depends upon the model,\n\nHere in this kernel didn't used this corelated features for training.","7f4acb07":"## SUBMISSION FILE","9ce97c31":"drop test id which is not required for the predictions.And assign the target values to the Y_train variable.","c834de68":"Import the packages required.","afccb111":"read the path of train and test data sets","6560b836":"## Outliers Detection\nOutliers or Anomalies detection is very important in while preprocessing it helps us to remove the unnecessary data point which is not required.\n","79b847f1":"## Kurtosis\n\nKurtosis is nothing but the tail shown in the diagram. if more skew tail diastance will be more.\n\nThe exact interpretation of the measure of Kurtosis used to be disputed, but is now settled. Its about existence of outliers. Kurtosis is a measure of whether the data are heavy-tailed (profusion of outliers) or light-tailed (lack of outliers) relative to a normal distribution.\n![image.png](attachment:image.png)\n","19bf1d77":"## XGBOOST","8d6a93c2":"## Skewness and Kurtosis\nChecking the skewness and kurtosis with target label (SalePrice) is very important. \n\n## What is skewness and Kurtosis?\n**Skewness**\n\nSkewness is a sudden jump in the target value for instance (10,20,30,80) from 30 it jumps to 80 so if that kind of jumps are present in target label.the model will not predict correctly.So we need to reduce the skewness and kurtosis.\n\nSkewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive or negative, or undefined.\n\nIn a perfect normal distribution, the tails on either side of the curve are exact mirror images of each other.\nWhen a distribution is skewed to the left, the tail on the curve\u2019s left-hand side is longer than the tail on the right-hand side, and the mean is less than the mode. This situation is also called negative skewness.\n\nWhen a distribution is skewed to the right, the tail on the curve\u2019s right-hand side is longer than the tail on the left-hand side, and the mean is greater than the mode. This situation is also called positive skewness.\n\n![image.png](attachment:image.png)\n","a27aeb6b":"## checking the missing values","691d69c5":"## SVR ","d89da5e4":"save the ids for submissions","9c302e3c":"## HOW TO REDUCE THE SKEWNESS AND KURTOSIS?\nThere are many methods, but here applied **log transformation** \n\nfor more go through this  https:\/\/towardsdatascience.com\/transforming-skewed-data-73da4c2d0d16"}}