{"cell_type":{"0f96bd32":"code","66f8b8d9":"code","a68d6bfe":"code","358c69e2":"code","815d44cd":"code","1688eafb":"code","5a6c45dd":"code","7548f154":"code","edc3c194":"code","796b37bb":"code","6ec1e37e":"code","6c45c723":"code","2483a7f5":"code","1433f6fb":"code","12dccb04":"code","fbff4eb7":"code","79239686":"code","98737ad2":"code","58e76744":"code","5013896f":"code","612d067b":"code","25f6a5fe":"code","c7b081a7":"code","0758e835":"code","60beb3d4":"code","554d2950":"code","091444d6":"code","2a59b4d8":"markdown","b4d25355":"markdown","6ca71892":"markdown","68f95fa8":"markdown","b280c79d":"markdown","ab084c38":"markdown","89515c08":"markdown","c28f0cc7":"markdown","a90837fb":"markdown","0cb22370":"markdown"},"source":{"0f96bd32":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","66f8b8d9":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime","a68d6bfe":"# Pulling invoice data\ndf_retail = pd.read_csv(\"\/kaggle\/input\/online-retail-customer-clustering\/OnlineRetail.csv\")","358c69e2":"df_retail.head()","815d44cd":"df_retail.info()","1688eafb":"print('Number of na in customer id =' ,df_retail['CustomerID'].isna().sum())\nprint('Number of null in customer id =' ,df_retail['CustomerID'].isnull().sum())","5a6c45dd":"df_retail.dropna(inplace=True)\ndf_retail.info()","7548f154":"# First attribute : Total amount paid\ndf_retail['Amount'] = df_retail['Quantity'] * df_retail['UnitPrice']\ndf_amount = df_retail.groupby(['CustomerID'],as_index=False)[\"Amount\"].sum()\ndf_amount.head()","edc3c194":"# Second attribute : Purchase recency\n\n# UDF to convert string to datetime\ndef convertDate(x):\n    conv_date = datetime.datetime.strptime(x, '%d-%m-%Y %H:%M')\n    return conv_date","796b37bb":"# UDF to split date and pick out recent purchase days \ndef splitDate(y):\n    y = str(y)\n    num_days = y.split()[0]\n    num_days = int(num_days)\n    return num_days","6ec1e37e":"df_retail['InvoiceDate'] = df_retail.loc[:,'InvoiceDate'].apply(convertDate)","6c45c723":"max_date = df_retail['InvoiceDate'].max()\ndf_retail['Recent_purchase_days'] = max_date - df_retail['InvoiceDate']","2483a7f5":"df_retail['Recent_purchase_days'] = df_retail['Recent_purchase_days'].apply(splitDate)\ndf_rec_purch = df_retail.groupby(['CustomerID'],as_index=False)[\"Recent_purchase_days\"].min()\ndf_rec_purch.head()","1433f6fb":"# Attribute : Frequency of purchase\ndf_purchase_freq = df_retail.groupby(['CustomerID'],as_index=False)[\"InvoiceNo\"].count()\ndf_purchase_freq.rename(columns={'CustomerID':'CustomerID','InvoiceNo':'PurchaseCount'},inplace=True)\ndf_purchase_freq.head()","12dccb04":"df_amt_purch = df_amount.merge(df_rec_purch,how='left',on=['CustomerID'])\ndf_model_inp = df_amt_purch.merge(df_purchase_freq,how='left',on=['CustomerID'])\ndf_model_inp.head()","fbff4eb7":"# UDF to determine outliers in data for all the columns\ndef viewDistribution(df):\n    sns.boxplot(data=df)\n    plt.xticks(rotation=90)\n    plt.show","79239686":"# INSIGHTS: There are outliers in amount and frequency column\ndf_anomaly = df_model_inp.iloc[:,1:4]\nviewDistribution(df_anomaly)","98737ad2":"# UDF to remove outlier\ndef remove_outlier_IQR(df):\n    Q1=df.quantile(0.25)\n    Q3=df.quantile(0.75)\n    IQR=Q3-Q1\n    df_final=df[~((df<(Q1-1.5*IQR)) | (df>(Q3+1.5*IQR)))]\n    return df_final","58e76744":"# Removed the anomalies in all the columns\ndf_IQR = remove_outlier_IQR(df_anomaly)\ndf_IQR.fillna(0,inplace=True)\nviewDistribution(df_IQR)","5013896f":"from sklearn.preprocessing import StandardScaler\n# define standard scaler\nscaler = StandardScaler()\n# transform data\ndf_scalar = scaler.fit_transform(df_IQR)\n\ndf_scalar1 = pd.DataFrame(df_scalar)\ndf_scalar1.columns = ['Amount' , 'Frequency' , 'Recency']\ndf_scalar1.head()","612d067b":"# Defining centroids\nk = 5\n\n# Filtering the columns required for clustering\ndata = df_IQR.iloc[:,0:3]\ndata.head()","25f6a5fe":"# Storing the sample dataframe to determine the number of centroids\nk_means = (data.sample(k, replace=False))    # store current means\nk_means2 = pd.DataFrame()                    # store previous means\nclusters = pd.DataFrame()    ","c7b081a7":"while not k_means2.equals(k_means):\n    # distance matrix (euclidean distance)\n    cluster_count = 0\n    for idx, k_mean in k_means.iterrows():\n        clusters[cluster_count] = (data[k_means.columns] - np.array(k_mean)).pow(2).sum(1).pow(0.5)\n        cluster_count += 1\n\n    # update cluster\n    data['MDCluster'] = clusters.idxmin(axis=1)\n\n    # store previous cluster\n    k_means2 = k_means\n    k_means = pd.DataFrame()\n    k_means_frame = data.groupby('MDCluster').agg(np.mean)\n    k_means[k_means_frame.columns] = k_means_frame[k_means_frame.columns]","0758e835":"data.head()","60beb3d4":"plt.figure(figsize=(15,5))\nsns.scatterplot(x=data['Amount'] ,y=data['Recent_purchase_days'] , hue = data['MDCluster'], palette = sns.color_palette('hls',5))\nplt.show()","554d2950":"plt.figure(figsize=(15,5))\nsns.scatterplot(x=data['Amount'] ,y=data['PurchaseCount'] , hue = data['MDCluster'], palette = sns.color_palette('hls',5))\nplt.show()","091444d6":"plt.figure(figsize=(15,5))\nsns.scatterplot(x=data['Recent_purchase_days'] ,y=data['PurchaseCount'] , hue = data['MDCluster'], palette = sns.color_palette('hls',5))\nplt.show()","2a59b4d8":"## Mathematical implementation of K means clustering techinique\n1.Pick K points as the initial centroids from the data set, either randomly or the first K.<br>\n2.Find the Euclidean distance of each point in the data set with the identified K points \u2014 cluster centroids.<br>\n3.Assign each data point to the closest centroid using the distance found in the previous step.<br>\n4.Find the new centroid by taking the average of the points in each cluster group.<br>\n5.Repeat iteration till the centroids don\u2019t change.<br>\n","b4d25355":"## Reading dataframe","6ca71892":"# <center>Online Retail Clustering<\/center>\n\n### Agenda : \nWe are required to cluster the customers using different attributes shared in the data\n### Data used : \nWe have used the data to define the frequency of purchase, recency of purchase and amount of purchase to cluster the customers\n\n### This notebook has been created after multiple iteration:<br>\n1. Iteration 1 : Implemented the K means algorithm using python\n2. Iteration 2 : Created other data attributes which may help in Clustering\n3. Iteration 3 : Scaled the data to improve the clustering technique\n4. Iteration 4 : Lastly, realised there are anomalies and so implemented IQR to eliminate iteration\n","68f95fa8":"## Creating attributes for clustering","b280c79d":"## Null value treatment","ab084c38":"## Create Analytical data set for the clustering model","89515c08":"## Performing scaling on the analytical data set","c28f0cc7":"## Anomaly detection","a90837fb":"## Plotting clusters ","0cb22370":"## Importing libraries"}}