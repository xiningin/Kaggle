{"cell_type":{"c4789cd9":"code","ff7b569e":"code","c22bdd93":"code","2a5da6e6":"code","2d418507":"code","47aea415":"code","63f3e57f":"code","5cdc9a85":"code","26ff77e9":"code","92a48a7f":"code","47a90ebb":"code","9495001c":"code","19e4e5b5":"code","1ef9dbfa":"code","6e725f3b":"code","e390d6d9":"code","997c16d2":"code","4cc07e83":"code","92fc869a":"code","9ffee31b":"code","b8554a27":"code","83cc53e9":"code","37d988b0":"code","583bc163":"code","49eb4af7":"code","62bc3b88":"code","6a22501e":"code","8de828bf":"code","25aa70a9":"code","fc00fa43":"code","dd7357f9":"code","cda04fa5":"code","3381c33a":"code","c2fadc75":"code","ee1f97f5":"code","47e82b70":"code","7aa9aba9":"code","797ce225":"code","ef7d2f94":"code","e43e1d69":"code","dd33896d":"code","1ed79a00":"code","07efff42":"code","589cbb8c":"code","05fdc543":"code","919279a8":"code","2229b82a":"code","0b44b4ba":"code","1c3c8365":"code","2c3508ce":"code","6aec21e5":"code","30ad2925":"code","7342a569":"code","f0814970":"code","10707542":"code","5ea268e9":"code","2c8e6e0b":"code","23484fc1":"code","3724f1ae":"code","91d4bcf0":"code","0b254a05":"code","e9f282b6":"code","ce8e696d":"code","0f6a718f":"code","40cd78cb":"code","5830f8fa":"code","63846820":"code","aceb1b2c":"markdown","3c3a1685":"markdown","8d6f0c82":"markdown","66ab6eb5":"markdown","e32504a2":"markdown","72868fd4":"markdown","a65051b7":"markdown","031dbb0d":"markdown","48f04b28":"markdown","eb563c74":"markdown","3022bb90":"markdown","a202607c":"markdown","bf7b022e":"markdown","089b9167":"markdown","96aba05b":"markdown","87ebe289":"markdown"},"source":{"c4789cd9":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\n\nplt.style.use('fivethirtyeight')\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# Lines below are just to ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","ff7b569e":"# loading training dataset\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","c22bdd93":"# take a look at the head of the training data set\ntrain.head()","2a5da6e6":"# shape of training dataset\ntrain.shape","2d418507":"# loadinig testing dataset\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","47aea415":"# take a look at the head \ntest.head()","63f3e57f":"# the shape of the test data set\ntest.shape","5cdc9a85":"# merging both dataset to do data cleaning on both at once, also getting more accurate filling resualt\ndf = train.merge(test , how='outer')\ndf.head()","26ff77e9":"# checking for nulls in all the columns\ndf.info()","92a48a7f":"# based on the discretion of the data Nan refers to inapplicability or availability\n#of that feature hence it was filled with 'None' \ndf[[ 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', \n       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n        'SaleCondition']] = df[[ 'Street', 'Alley', 'LotShape',\n                                             'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl'\n                                 , 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir',\n       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature'\n                                 , 'SaleCondition']].fillna(\"None\")","47a90ebb":"# checking missing values for categorical variables \nprint(df.MSZoning.value_counts(),\ndf.Electrical.value_counts(),\ndf.KitchenQual.value_counts(),\ndf.Exterior1st.value_counts(),\ndf.Exterior2nd.value_counts(),\ndf.SaleType.value_counts())","9495001c":"# using fill Mode technique to most data due to having high frequency of accuring\ndf.MSZoning.fillna(df['MSZoning'].mode()[0] , inplace = True)\ndf.Electrical.fillna(df['Electrical'].mode()[0] , inplace = True)\ndf.KitchenQual.fillna(df['KitchenQual'].mode()[0] , inplace = True)\ndf.Exterior1st.fillna(df['Exterior1st'].mode()[0] , inplace = True)\ndf.Exterior2nd.fillna(df['Exterior2nd'].mode()[0], inplace = True)\ndf.SaleType.fillna(df['SaleType'].mode()[0] , inplace = True)","19e4e5b5":"# Fill null with 0 for some numeric columns, these columns should be zero because they dont exist in the house\ndf['GarageYrBlt'].fillna(0 , inplace = True)\ndf['GarageArea'].fillna(0, inplace = True)\ndf['GarageCars'].fillna(0, inplace = True)\ndf['BsmtFinSF1'].fillna(0, inplace = True)\ndf['BsmtFinSF2'].fillna(0, inplace = True)\ndf['BsmtUnfSF'].fillna(0, inplace = True)\ndf['TotalBsmtSF'].fillna(0, inplace = True)\ndf['BsmtFullBath'].fillna(0, inplace = True)\ndf['BsmtHalfBath'].fillna(0, inplace = True)\ndf['MasVnrArea'].fillna(0, inplace = True)","1ef9dbfa":"# Applying median on lot frontage based on Nieghborhood to get more accurate fill\ndf[\"LotFrontage\"] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","6e725f3b":"# some numeric varibles referes to date or categories so changed type to str, so that the model doesnot treat them as numeric \n# MSSubClass=The building class\ndf['MSSubClass'] = df['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\ndf['OverallCond'] = df['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\ndf['YrSold'] = df['YrSold'].astype(str)\ndf['MoSold'] = df['MoSold'].astype(str)","e390d6d9":"# No Null\/Missing values, note that the nulls on the sale price due to the testing data not having a label\ndf.info()","997c16d2":"# take  a look at the head\ndf.head()","4cc07e83":"# Plot Scatter plots of Sale Price and 4 correlated variables according to Neighborhood\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'LotArea', 'YearBuilt','Neighborhood']\nplt.figure(figsize=(14,8), dpi= 80)\nsns.pairplot(df[cols], kind=\"scatter\", hue=\"Neighborhood\",palette=\"RdBu\")\nplt.show()\n","92fc869a":"# to get the train data from the meged data set we can use iloc and get all columns, while rows equal to the shape of the train[0]\ndf.iloc[:train.shape[0],:].head()","9ffee31b":"#saleprice correlation matrix\ncorrmat = df.iloc[:train.shape[0],:].corr()\nplt.figure(figsize=(17, 8))\nk = 15 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df.iloc[:train.shape[0],:][cols].values.T)\nsns.set(font_scale=1.50)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True,\n                 fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values,\n                 cmap = 'RdBu', linecolor = 'white', linewidth = 1)\nplt.title(\"Correlations between Sales Price and 15 features\", fontsize =15)\nplt.show()","b8554a27":"# creating dummy variables for all categorical variables in the cleaned and merged dataset\ndf_d = pd.get_dummies(df , drop_first=True)\ndf_d.shape","83cc53e9":"#saleprice correlation matrix\ncorrmat = df_d.iloc[:train.shape[0],:].corr()\nplt.figure(figsize=(17, 8))\nk = 15 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_d.iloc[:train.shape[0],:][cols].values.T)\nsns.set(font_scale=1.50)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True,\n                 fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values,\n                 cmap = 'RdBu', linecolor = 'white', linewidth = 1)\nplt.title(\"Correlations between Sales Price and 15 features\", fontsize =15)\nplt.show()","37d988b0":"# getting the target (sale price) column as y\ny=pd.DataFrame(df_d.pop('SalePrice'))","583bc163":"# checking the shape of the train data set, to know from where to cut data set to get training data useing iloc\ntrain.shape[0]","49eb4af7":"# using iloc on both the target and training data we can get a nice seperation between training and testing datasets\nX_train = df_d.iloc[:train.shape[0] , :]\ny_train = y.iloc[:train.shape[0]]","62bc3b88":"# checking train dataset shape to make sure we did correct seperation\nprint(X_train.shape , y_train.shape)","6a22501e":" # importing test\/train split, and use it on training dataset to train the models and score them \nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3 , random_state = 101)\n","8de828bf":"# importing scaler, then scale training and test dataframes\nfrom sklearn.preprocessing import StandardScaler \n\ns = StandardScaler()\n\nX_train_d_s = pd.DataFrame(s.fit_transform(X_train) , columns=X_train.columns)\nX_test_d_s = pd.DataFrame(s.transform(X_test) , columns=X_test.columns)","25aa70a9":"#importing models \nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor","fc00fa43":"# using grid search on GradientBoostingRegressor model to get the best hyperparameters\nnum_estimators = [3000 , 4000]\nlearn_rates = [0.02, 0.05]\nmax_depths = [ 3 , 4]\nmin_samples_leaf = [10 , 15]\nmin_samples_split = [10 , 15]\nmax_features=['sqrt']\n\nparam_grid = {'n_estimators': num_estimators,\n              'learning_rate': learn_rates,\n              'max_depth': max_depths,\n              'min_samples_leaf': min_samples_leaf,\n              'min_samples_split': min_samples_split , \n             'max_features' : max_features }\n\ngrad = GridSearchCV(GradientBoostingRegressor(loss='huber'),\n                           param_grid, cv=3, verbose= 1 , n_jobs=-1)\ngrad.fit(X_train_d_s , y_train)","dd7357f9":"# getting train score\ngrad.score(X_train_d_s , y_train)","cda04fa5":"# getting test scores\ngrad.score(X_test_d_s , y_test)","3381c33a":"# creating dataframe containing model feature important\ncoef_df = pd.DataFrame({'feature': X_train_d_s.columns,\n                        'importance': abs(grad.best_estimator_.feature_importances_), \n                        })\n\ncoef_df.head()","c2fadc75":"# sort by absolute value of coefficient (magnitude)\ncoef_df.sort_values('importance', ascending=False, inplace=True)\ncoef_df[:10]","ee1f97f5":"# get e barplot for features\nplt.figure(figsize=(7,6))\nplt.xticks(rotation=60)\nsns.barplot(coef_df.feature[:7] , coef_df.importance[:7],palette='RdBu') # top features","47e82b70":"# using RandomGridSerach  to fide best hyperparametrs for RandomForestRegressor\npar = {'bootstrap': [True, False],\n 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n\nra = RandomizedSearchCV(RandomForestRegressor(),\n                   par , cv = 5 , verbose= 1  , n_jobs= -1)\nra.fit(X_train_d_s , y_train)","7aa9aba9":"ra.score(X_train_d_s , y_train)","797ce225":"ra.score(X_test_d_s , y_test)","ef7d2f94":"# creating dataframe containing model feature important\ncoef_df = pd.DataFrame({'feature': X_train_d_s.columns,\n                        'importance': abs(ra.best_estimator_.feature_importances_), \n                        })\n\ncoef_df.head()","e43e1d69":"# sort by absolute value of coefficient (magnitude)\ncoef_df.sort_values('importance', ascending=False, inplace=True)\ncoef_df[:10]","dd33896d":"# get e barplot for features\nplt.figure(figsize=(7,6))\nplt.xticks(rotation=60)\nsns.barplot(coef_df.feature[:7] , coef_df.importance[:7],palette='RdBu') # top  features","1ed79a00":"# lasso with a range values for alpha\nla = LassoCV(alphas=np.logspace(0, 5, 200) , n_jobs=-1)\n\nla.fit(X_train_d_s , y_train)","07efff42":"la.score(X_train_d_s , y_train)","589cbb8c":"la.score(X_test_d_s , y_test)","05fdc543":"# creating dataframe containing model feature important\ncoef_df = pd.DataFrame({'feature': X_train_d_s.columns,\n                        'importance': abs(la.coef_),\n                        })\n\ncoef_df.head()","919279a8":"# sort by absolute value of coefficient (magnitude)\ncoef_df.sort_values('importance', ascending=False, inplace=True)\ncoef_df[:10]","2229b82a":"# get e barplot for features\nplt.figure(figsize=(7,6))\nplt.xticks(rotation=60)\nsns.barplot(coef_df.feature[:7] , coef_df.importance[:7],palette='RdBu') # top features","0b44b4ba":"# Ridge with a range values for alpha\nri = RidgeCV(alphas=np.logspace(0, 5, 200))\n\nri.fit(X_train_d_s , y_train)","1c3c8365":"ri.score(X_train_d_s , y_train)","2c3508ce":"ri.score(X_test_d_s , y_test)","6aec21e5":"# creating dataframe containing model feature important\ncoef_df = pd.DataFrame({'feature': X_train_d_s.columns,\n                        'importance': abs(ri.coef_[0]),\n                        })\n\ncoef_df.head()","30ad2925":"# sort by absolute value of coefficient (magnitude)\ncoef_df.sort_values('importance', ascending=False, inplace=True)\ncoef_df[:10]","7342a569":"# get e barplot for features\nplt.figure(figsize=(7,6))\nplt.xticks(rotation=60)\nsns.barplot(coef_df.feature[:7] , coef_df.importance[:7],palette='RdBu') # top features","f0814970":"# ElasticNet with a range values for alpha and l1 ratio\nela = ElasticNetCV(l1_ratio=np.arange(0.01, 1.0, 0.05) , alphas=np.logspace(0, 5, 200))\n\nela.fit(X_train_d_s , y_train)","10707542":"ela.score(X_train_d_s , y_train)","5ea268e9":"ela.score(X_test_d_s , y_test)","2c8e6e0b":"# creating dataframe containing model feature important\ncoef_df = pd.DataFrame({'feature': X_train_d_s.columns,\n                        'importance': abs(ela.coef_), # its logistic regression, we can get coef_, right?\n                        })\n\ncoef_df.head()","23484fc1":"# sort by absolute value of coefficient (magnitude)\ncoef_df.sort_values('importance', ascending=False, inplace=True)\ncoef_df[:10]","3724f1ae":"# get e barplot for features\nplt.figure(figsize=(7,6))\nplt.xticks(rotation=60)\nsns.barplot(coef_df.feature[:7] , coef_df.importance[:7],palette='RdBu') # top features","91d4bcf0":"# recreating the training and testing dataset to do the prediction on the testing data\ndf_d = pd.get_dummies(df , drop_first=True)\n","0b254a05":"y=pd.DataFrame(df_d.pop('SalePrice'))","e9f282b6":"y_test = y.iloc[train.shape[0]:]\nX_test= df_d.iloc[train.shape[0]:,:]\n","ce8e696d":"y_train = y.iloc[:train.shape[0]]\nX_train = df_d.iloc[:train.shape[0] , :]","0f6a718f":"from sklearn.preprocessing import StandardScaler\n\ns = StandardScaler()\n\nX_train_d_s = pd.DataFrame(s.fit_transform(X_train) , columns=X_train.columns)\nX_test_d_s = pd.DataFrame(s.transform(X_test) , columns=X_test.columns)","40cd78cb":"# creating the dataframe then save it as csv file before submiting.\nsub = pd.DataFrame({\n        \"Id\": test.Id,\n        \"SalePrice\": grad.predict(X_test_d_s)\n})\n\nsub.head()","5830f8fa":"sub.to_csv('sub9GS.csv' , index=False)","63846820":"pd.read_csv('sub9GS.csv').head()","aceb1b2c":"# Predicting then submitting on Kaggle","3c3a1685":"#### Models usedf with Score:\n\n|Model|Train Score|Test Score|Comments|\n|---|---|---|---|\n|GradientBoostingRegressor|0.952621 |0.857017|We can see the model have a little differents in testing and training score, this indicate that there is a little bit of overfit| \n|RandomForestRegressor|0.979259 |0.830038 |We can see the huge different on training and testing scores, random forest tend to overfit| \n|Lasso CV |0.905332 |0.750314 |We can see the overfit| \n|Ridge CV |0.916387 |0.772796 |A lot of overfitting| \n|ElasticNetCV|0.914943|0.773132|The overfitting is close to that in ridge and lasso| \n","8d6f0c82":"We can see the model have a little differents in testing and training score, this indicate that there is a little bit of overfit.","66ab6eb5":"# Feature Engneering","e32504a2":"# Recommendations","72868fd4":"# Problem Statement","a65051b7":"A lot of overfitting.","031dbb0d":"We can see the overfit.","48f04b28":" # Data Exploration","eb563c74":"The overfitting is close to that in ridge and lasso","3022bb90":"# EDA\n### Data Exploration, Feature engineering and visualization.","a202607c":"#### The above plots shows:\n- There is a clear relationship with **sale price** and  many vsriables for suchs as *'OverallQual' and 'YearBuilt'*  The relationship seems to be stronger with *'OverallQual'*, where the  sales prices increases.\n- It is clear that there are mltiple outliers such as LotArea >200000. However attempting to remove them has affected the model and prediction scores in Kaggle, therefore opt to keep them.","bf7b022e":"To **predict the sales price for each house.** For each Id in the test set provided in Kaggle House Prices Competition. prediction will be the value of the SalePrice variable. \n*Applying EDA and Modeling with a regression model in 79 explanatory variables describing (almost) every aspect of residential homes","089b9167":"We can see the huge different on training and testing scores, random forest tend to overfit.","96aba05b":"# Machine Learning","87ebe289":"# Kaggle Competition on House prices\n###### Team:  Ibrahim Hakami - Mohammed Alqahtani - Sumaiah Alsadhan"}}