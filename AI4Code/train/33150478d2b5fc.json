{"cell_type":{"589ac021":"code","4885921d":"code","0bb66175":"code","379f938e":"code","d8be5834":"code","cbbb7aa6":"code","0e687e46":"code","29ab3d27":"code","e72d8ec5":"code","36ff4565":"code","21c90c20":"code","59bfeb8f":"code","bf638187":"code","a280c1a3":"code","17379892":"code","897bc83c":"code","188d0d1d":"code","53304a1e":"code","1c8c472b":"code","9367bc23":"code","c000a058":"code","0740c046":"code","f49ce170":"code","a764c578":"code","710cb0fb":"code","32414948":"code","692b125a":"code","afb8ba67":"markdown","50f64f18":"markdown","301d0fe7":"markdown","204782d8":"markdown","6868f3da":"markdown","e7eb52f5":"markdown","e758ba9a":"markdown","28284ced":"markdown","6847e063":"markdown","99d02850":"markdown","5e905dd2":"markdown","a96cdb92":"markdown","61da418a":"markdown","34787702":"markdown","61c4f934":"markdown","7b5e21df":"markdown","515e2d74":"markdown","b0b1fb21":"markdown","b8fca287":"markdown"},"source":{"589ac021":"!pip install -q imblearn","4885921d":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom imblearn.pipeline import Pipeline\n\nfrom tqdm import tqdm_notebook as tqdm\ntqdm().pandas()","0bb66175":"X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n# summarize class distribution\ncounter = Counter(y)\nprint(counter)\n\nplt.figure(figsize=(10,5))\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n\nplt.legend()\nplt.show()","379f938e":"from imblearn.over_sampling import SMOTE\n\ncounter = Counter(y)\nprint(counter)\n\noversample = SMOTE()\nX, y = oversample.fit_resample(X, y)\n\ncounter = Counter(y)\nprint(counter)\n\nplt.figure(figsize=(10,5))\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n\nplt.legend()\nplt.show()","d8be5834":"X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n\n# define model\nmodel = DecisionTreeClassifier()\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nscores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n\nprint('Mean ROC AUC: %.3f' % np.mean(scores))","cbbb7aa6":"steps = [('over', SMOTE()), ('model', DecisionTreeClassifier())]\npipeline = Pipeline(steps=steps)\nscores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n\nprint('Mean ROC AUC: %.3f' % np.mean(scores))","0e687e46":"k_values = [1, 2, 3, 4, 5, 6, 7]\nfor k in tqdm(k_values):\n    over = SMOTE(sampling_strategy=0.1, k_neighbors=k)\n    steps = [('over', over), ('model', DecisionTreeClassifier())]\n    pipeline = Pipeline(steps=steps)\n    scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n\n    print('k_neighbours :%d, Mean ROC AUC: %.3f' % (k,np.mean(scores)))","29ab3d27":"from imblearn.over_sampling import BorderlineSMOTE\n\nX, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n\ncounter = Counter(y)\nprint(counter)\n\noversample = BorderlineSMOTE()\nX, y = oversample.fit_resample(X, y)\n\ncounter = Counter(y)\nprint(counter)\n\nplt.figure(figsize=(10,5))\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n\nplt.legend()\nplt.show()","e72d8ec5":"from imblearn.over_sampling import SVMSMOTE\n\nX, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n\ncounter = Counter(y)\nprint(counter)\n\noversample = SVMSMOTE()\nX, y = oversample.fit_resample(X, y)\n\ncounter = Counter(y)\nprint(counter)\n\nplt.figure(figsize=(10,5))\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n\nplt.legend()\nplt.show()","36ff4565":"from imblearn.over_sampling import ADASYN\n\nX, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n\ncounter = Counter(y)\nprint(counter)\n\noversample = ADASYN()\nX, y = oversample.fit_resample(X, y)\n\ncounter = Counter(y)\nprint(counter)\n\nplt.figure(figsize=(10,5))\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n\nplt.legend()\nplt.show()","21c90c20":"from imblearn.under_sampling import NearMiss\n\nX, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n\ncounter = Counter(y)\nprint(counter)\n\nundersample = NearMiss(version=1, n_neighbors=3)\nX, y = undersample.fit_resample(X, y)\n\ncounter = Counter(y)\nprint(counter)","59bfeb8f":"plt.figure(figsize=(10,5))\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n\nplt.title('NearMiss Version=1, Neighbours=3', fontsize=18)\nplt.legend()\nplt.show()","bf638187":"X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n\ncounter = Counter(y)\nprint(counter)\n\nundersample = NearMiss(version=2, n_neighbors=3)\nX, y = undersample.fit_resample(X, y)\n\ncounter = Counter(y)\nprint(counter)","a280c1a3":"plt.figure(figsize=(10,5))\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n\nplt.title('NearMiss Version=2, Neighbours=3', fontsize=18)\nplt.legend()\nplt.show()","17379892":"X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n\ncounter = Counter(y)\nprint(counter)\n\nundersample = NearMiss(version=3, n_neighbors=3)\nX, y = undersample.fit_resample(X, y)\n\ncounter = Counter(y)\nprint(counter)","897bc83c":"plt.figure(figsize=(10,5))\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n\nplt.title('NearMiss Version=3, Neighbours=3', fontsize=18)\nplt.legend()\nplt.show()","188d0d1d":"from imblearn.under_sampling import CondensedNearestNeighbour\n\nX, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n\ncounter = Counter(y)\nprint(counter)\n\nundersample = CondensedNearestNeighbour(n_neighbors=1)\nX, y = undersample.fit_resample(X, y)\n\ncounter = Counter(y)\nprint(counter)\n\nplt.figure(figsize=(10,5))\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n\nplt.title('CondensedNearestNeighbour Neighbours=1', fontsize=18)\nplt.legend()\nplt.show()","53304a1e":"from imblearn.under_sampling import TomekLinks\n\nX, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n\ncounter = Counter(y)\nprint(counter)\n\nundersample = TomekLinks()\nX, y = undersample.fit_resample(X, y)\n\ncounter = Counter(y)\nprint(counter)\n\nplt.figure(figsize=(10,5))\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n\nplt.title('TomekLinks', fontsize=18)\nplt.legend()\nplt.show()","1c8c472b":"from imblearn.under_sampling import EditedNearestNeighbours\n\nX, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n\ncounter = Counter(y)\nprint(counter)\n\nundersample = EditedNearestNeighbours(n_neighbors=3)\nX, y = undersample.fit_resample(X, y)\n\ncounter = Counter(y)\nprint(counter)\n\nplt.figure(figsize=(10,5))\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n\nplt.title('EditedNearestNeighbours Neighbors=3', fontsize=18)\nplt.legend()\nplt.show()","9367bc23":"from imblearn.under_sampling import OneSidedSelection\n\nundersample = OneSidedSelection(n_neighbors=1, n_seeds_S=200)\n\nX, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n\ncounter = Counter(y)\nprint(counter)\n\nX, y = undersample.fit_resample(X, y)\n\ncounter = Counter(y)\nprint(counter)\n\nplt.figure(figsize=(10,5))\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n\nplt.title('OneSidedSelection Neighbors=1', fontsize=18)\nplt.legend()\nplt.show()","c000a058":"from imblearn.under_sampling import NeighbourhoodCleaningRule\n\nundersample = NeighbourhoodCleaningRule(n_neighbors=1, threshold_cleaning=0.5)\n\nX, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n\ncounter = Counter(y)\nprint(counter)\n\nX, y = undersample.fit_resample(X, y)\n\ncounter = Counter(y)\nprint(counter)\n\nplt.figure(figsize=(10,5))\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n    row_ix = np.where(y == label)[0]\n    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n\nplt.title('NeighbourhoodCleaningRule Neighbors=1', fontsize=18)\nplt.legend()\nplt.show()","0740c046":"X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n\n# define model\nmodel = DecisionTreeClassifier()\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nscores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n\nprint('Mean ROC AUC: %.3f' % np.mean(scores))","f49ce170":"from imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\ndef run_model(pipeline):\n    X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n\n    model = DecisionTreeClassifier()\n\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n\n    print('Mean ROC AUC: %.3f' % np.mean(scores))","a764c578":"over = RandomOverSampler(sampling_strategy=0.1)\nunder = RandomUnderSampler(sampling_strategy=0.5)    \n\nsteps = [('over',over), ('under', under), ('model', model)]\npipeline = Pipeline(steps=steps)\n\nrun_model(pipeline)","710cb0fb":"from imblearn.over_sampling import SMOTE\n\nover = SMOTE(sampling_strategy=0.1)\nunder = RandomUnderSampler(sampling_strategy=0.5)\n\nsteps = [('over',over), ('under', under), ('model', model)]\npipeline = Pipeline(steps=steps)\n\nrun_model(pipeline)","32414948":"from imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import TomekLinks\n\nresample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority')) \n\n# define pipeline\npipeline = Pipeline(steps=[('r', resample), ('m', model)])\n\nrun_model(pipeline)","692b125a":"from imblearn.combine import SMOTEENN\n\nresample = SMOTEENN()\n# define pipeline\npipeline = Pipeline(steps=[('r', resample), ('m', model)])\n\nrun_model(pipeline)","afb8ba67":"<div style=\"font-family:verdana; word-spacing:1.9px;\">\n<h3>2.2.2. Edited Nearest Neighbors Rule for Undersampling<\/h3>Another rule for finding ambiguous and noisy examples in a dataset is called Edited Nearest Neighbors, or sometimes ENN for short.<br> This rule involves using k = 3 nearest neighbors to locate those examples in a dataset that are misclassified and that are then removed before a k = 1 classification rule is applied.<\/div>","50f64f18":"**<blockquote>\nLike Borderline-SMOTE, we can see that synthetic sample generation is focused around the decision boundary as this region has the lowest density. Unlike Borderline-SMOTE, we can see that the examples that have the most class overlap have the most focus. On problems where these low density examples might be outliers, the ADASYN approach may put too much attention on these areas of the feature space, which may result in worse model performance.<\/blockquote>**","301d0fe7":"**<blockquote>Scatter plot shows many more examples in the minority class created along the lines between the original examples in the minority class.<\/blockquote>**\n","204782d8":"<div style=\"font-family:verdana; word-spacing:1.9px; font-size:14px\">\n<h3><center>3.3 Standard Combined Data Sampling Methods <\/center><\/h3>\n    <h3>3.3.1. SMOTE and Tomek Links Undersampling<\/h3>\n    The SMOTE configuration can be set via the smote argument and takes a configured SMOTE instance. The Tomek Links configuration can be set via the tomek argument and takes a configured TomekLinks object.<br> Both arguments are set to instances of each class with default configurations. The default is to balance the dataset with SMOTE then remove Tomek links from all classes.","6868f3da":"<h3>1.2. Border Line SMOTE<\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nA popular extension to SMOTE involves selecting those instances of the minority class that are misclassified, such as with a k-nearest neighbor classification model. We can then oversample just those difficult instances, providing more resolution only where it may be required.<br><br>\nThe examples on the borderline and the ones nearby are more apt to be misclassified than the ones far from the borderline, and thus more important for classification.\n    <\/div>","e7eb52f5":"<h3>After Sampling<\/h3>","e758ba9a":"<h3>1.1. SMOTE for Balancing Data<\/h3>","28284ced":"<div style=\"font-family:verdana; word-spacing:1.9px; font-size:14px\">\n<h3>3.2. SMOTE Oversampling and Undersampling<\/h3>We can combine SMOTE with RandomUnderSampler. <br>\n    Again, the order in which these procedures are applied does not matter as they are performed on different subsets of the training dataset.","6847e063":"<div style=\"font-family:verdana; word-spacing:1.9px; font-size:15px\">\n<h3>3.3.2. SMOTE and Edited Nearest Neighbors Undersampling<\/h3>\n","99d02850":"<div style=\"font-family:verdana; word-spacing:1.9px;\">\n<h3><center>2.2. Combinations of Keep and Delete Methods <\/center><\/h3>\n    <h3>2.2.1. One-Sided Selection for Undersampling<\/h3>\n    One-Sided Selection, or OSS for short, is an undersampling technique that combines Tomek Links and the Condensed Nearest Neighbor (CNN) Rule. <br>Specifically, Tomek Links are ambiguous points on the class boundary and are identified and removed in the majority class. The CNN method is then used to remove redundant examples from the majority class that are far from the decision boundary.","5e905dd2":"<h2><center>Imbalanced Classification<\/center><\/h2>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nImbalanced classification involves developing predictive models on classification datasets that have a severe class imbalance. The challenge of working with imbalanced datasets is that most machine learning techniques will ignore, and in turn have poor performance on, the minority class, although typically it is performance on the minority class that is most important.\n<\/div>    \n    \n![image.png](attachment:image.png)\n\n<h3><center>1. Over Sampling Methods <\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    A problem with imbalanced classification is that there are too few examples of the minority class for a model to effectively learn the decision boundary. One way to solve this problem is to oversample the examples in the minority class.<br><br>\n    The most widely used approach to synthesizing new examples is called the Synthetic Minority Oversampling TEchnique, or SMOTE for short.<br><br>\n    SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.<br>\nSpecifically, a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k = 5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space.","a96cdb92":"<h3>1.3. Adaptive Synthetic Sampling (ADASYN)<\/h3>\n<div style=\"font-family:verdana; word-spacing:1.9px;\">\nAnother approach involves generating synthetic samples inversely proportional to the density of the examples in the minority class. That is, generate more synthetic examples in regions of the feature space where the density of minority examples is low, and fewer or none where the density is high.","61da418a":"<h3>1.3. Border Line SMOTE SVM<\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">","34787702":"<h3><center>2. Under Sampling Methods <\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    There are many different types of undersampling techniques, although most can be grouped into ,\n<ul>\n  <li>those that select examples to keep in the transformed dataset, \n  <li>those that select examples to delete, \n  <li>and hybrids that combine both types of methods.\n<\/ul>\n    \n<h3><center>2.1. Methods that Select Examples to Keep <\/center><\/h3>\n    <h3>2.1.1 Near Miss Undersampling<\/h3>\n    \n<ul>\n    <li>NearMiss-1: Majority class examples with minimum average distance to three closest minority class examples.\n    <li>NearMiss-2: Majority class examples with minimum average distance to three furthest minority class examples.\n    <li>NearMiss-3: Majority class examples with minimum distance to each minority class example.\n    <\/ul>\n    <\/div>","61c4f934":"<div style=\"font-family:verdana; word-spacing:1.9px;\">\n<h3>2.1.2. Condensed Nearest Neighbor Rule Undersampling<\/h3>\n    Condensed Nearest Neighbors, or CNN for short, is an undersampling technique that seeks a subset of a collection of samples that results in no loss in model performance.<br><br>\n   It is achieved by enumerating the examples in the dataset and adding them to the store only if they cannot be classified correctly by the current contents of the store. This approach was proposed to reduce the memory requirements for the k-Nearest Neighbors (KNN) algorithm<\/div> ","7b5e21df":"<div style=\"font-family:verdana; word-spacing:1.9px;\">\n<h3><center>2.2. Methods that Select Examples to Delete <\/center><\/h3>\n    <h3>2.2.1. Tomek Links for Undersampling<\/h3>\n    The condensed nearest-neighbor (CNN) method chooses samples randomly. <br>This results in <ul>\n    <li> retention of unnecessary samples \n    <li>occasional retention of internal rather than boundary samples.<\/ul>\n    The procedure for finding Tomek Links can be used to locate all cross-class nearest neighbors.\n    It can be used to find all of those examples in the majority class that are closest to the minority class, then removed. These would be the ambiguous examples.<br>\n    Instances that are in Tomek Links are either boundary instances or noisy instances.\n    <\/div>","515e2d74":"<div style=\"font-family:verdana; word-spacing:1.9px; font-size:15px\">\n<h3><center>3. Oversampling and Undersampling <\/center><\/h3>\n    Oversampling methods duplicate or create new synthetic examples in the minority class, whereas undersampling methods delete examples in the majority class.<br> Both types of sampling can be effective when used in isolation, although can be more effective when both types of methods are used together.<br><br>\n    <h3>3.1. Random Oversampling and Undersampling<\/h3>\n    Random oversampling involves randomly duplicating examples in the minority class, whereas random undersampling involves randomly deleting examples from the majority class.<br>\nAs these two transforms are performed on separate classes, the order in which they are applied to the training dataset does not matter.\n","b0b1fb21":"<blockquote>We can see that the focus of the algorithm is those examples in the minority class along the decision boundary between the two classes, specifically, those majority examples around the minority class examples.<\/blockquote>","b8fca287":"<div style=\"font-family:verdana; word-spacing:1.9px;\">\n<h3>2.2.1. Neighborhood Cleaning Rule for Undersampling<\/h3>\n\nThe Neighborhood Cleaning Rule, or NCR for short, is an undersampling technique that combines both the Condensed Nearest Neighbor (CNN) Rule to remove redundant examples and the Edited Nearest Neighbors (ENN) Rule to remove noisy or ambiguous examples.<br>\n    "}}