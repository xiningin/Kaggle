{"cell_type":{"9a2738e3":"code","d9897e33":"code","5bc7df61":"code","04c921c9":"code","f2c03532":"code","153188c2":"code","3e1c4206":"code","4c7c031a":"code","0fd566a3":"code","59919ab9":"code","79e4912e":"code","7d5d60d7":"code","9510605c":"code","72b70dd0":"markdown","6b5fe89c":"markdown","3854eea4":"markdown","03478a2e":"markdown","7ae01da2":"markdown","7e63188c":"markdown","7e271d39":"markdown","9c3b9734":"markdown","a9a6a858":"markdown"},"source":{"9a2738e3":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport lightgbm as lgb\nimport optuna.integration.lightgbm as lgb_optuna\n#import dask_xgboost as xgb\n#import dask.dataframe as dd6\nfrom sklearn import preprocessing, metrics\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nimport os\nfrom tqdm import tqdm_notebook as tqdm\nfrom scipy.sparse import csr_matrix\nimport pickle\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d9897e33":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns: #columns\u6bce\u306b\u51e6\u7406\n        col_type = df[col].dtypes\n        if col_type in numerics: #numerics\u306e\u30c7\u30fc\u30bf\u578b\u306e\u7bc4\u56f2\u5185\u306e\u3068\u304d\u306b\u51e6\u7406\u3092\u5b9f\u884c. \u30c7\u30fc\u30bf\u306e\u6700\u5927\u6700\u5c0f\u5024\u3092\u5143\u306b\u30c7\u30fc\u30bf\u578b\u3092\u52b9\u7387\u7684\u306a\u3082\u306e\u306b\u5909\u66f4\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","5bc7df61":"PATHS = {}\nfor store_id in ['CA_1','CA_2','CA_3','CA_4','TX_1','TX_2','TX_3','WI_1','WI_2','WI_3']:\n    PATHS[store_id] = '\/kaggle\/input\/m5-all-data\/df_' + store_id + '.pkl'","04c921c9":"TARGET = 'sales_binary'\n\nbasic_features = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', \n        'release', 'sell_price', 'price_max', 'price_min', 'price_std',\n       'price_mean', 'price_norm', 'price_nunique', 'item_nunique',\n       'price_momentum', 'price_momentum_m', 'price_momentum_y',\n       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n       'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y',\n       'tm_wm', 'tm_dw', 'tm_w_end']\n\nencoding_features = [\n       'te_id_60', 'te_item_id_60',\n       'te_dept_id_60', 'te_cat_id_60', 'te_store_id_60', 'te_state_id_60',\n       'te_id_tm_dw_60', 'te_item_id_tm_dw_60', 'te_dept_id_tm_dw_60',\n       'te_cat_id_tm_dw_60', 'te_store_id_tm_dw_60', 'te_state_id_tm_dw_60'\n]\n\nlag_features = [\n        'sales_lag_28','sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32',\n        'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36','sales_lag_37',\n        'sales_lag_38', 'sales_lag_39', 'sales_lag_40','sales_lag_41', 'sales_lag_42', \n        'rolling_mean_7', 'rolling_std_7','rolling_mean_14', 'rolling_std_14',\n        'rolling_mean_30','rolling_std_30', 'rolling_mean_60', 'rolling_std_60',\n        'rolling_mean_180', 'rolling_std_180'\n]\n\nrecursive_features =[\n    'rolling_mean_tmp_1_7','rolling_mean_tmp_1_14',\n    'rolling_mean_tmp_1_30','rolling_mean_tmp_1_60',\n    'rolling_mean_tmp_7_7','rolling_mean_tmp_7_14',\n    'rolling_mean_tmp_7_30','rolling_mean_tmp_7_60',\n    'rolling_mean_tmp_14_7','rolling_mean_tmp_14_14',\n    'rolling_mean_tmp_14_30','rolling_mean_tmp_14_60'\n]\n\nadditional_features = ['binary_pred']\n\nremove_features = ['store_id', 'state_id', 'te_store_id_60', 'te_state_id_60', 'te_store_id_tm_dw_60', 'te_state_id_tm_dw_60','snap_CA', 'snap_TX', 'snap_WI']\n\n\nuse_enc_feat = True\nuse_lag_feat = True\nuse_rec_feat = False\nuse_add_feat = False\n\nfeature = basic_features\nif use_enc_feat:\n    feature += encoding_features\nif use_lag_feat:\n    feature += lag_features\nif use_rec_feat:\n    feature += recursive_features\nif use_add_feat:\n    feature += additional_features\n    \nfeature = [i for i in feature if i not in remove_features]\n    \nlen(feature)","f2c03532":"# define lgbm simple model using custom loss and eval metric for early stopping\ndef run_lgb_binary(train, val, features, no_early_stopping = False):\n\n    train_set = lgb.Dataset(train[features], train[TARGET].values)\n    del train\n    gc.collect()\n    \n    val_set = lgb.Dataset(val[features], val[TARGET].values)\n    del val\n    gc.collect()\n\n    params = {\n            'boosting_type': 'gbdt',\n            'first_metric_only': True,\n            'objective': 'binary',\n            'metric': 'auc',\n            'n_jobs': -1,\n            'seed': 42,\n            'learning_rate': 0.1,\n            'bagging_fraction': 0.75,\n            'bagging_freq': 10, \n            'colsample_bytree': 0.75}\n    if no_early_stopping:\n        model = lgb.train(params, train_set, num_boost_round = 200, \n                      valid_sets = [train_set], valid_names=['Train'], \n                      verbose_eval = 10)        \n    else: \n        model = lgb.train(params, train_set, num_boost_round = 1500, early_stopping_rounds = 100, \n                      valid_sets = [train_set, val_set], valid_names=['Train','Val'], \n                      verbose_eval = 10)\n    return model\n","153188c2":"test_start_date = 1970\ntest_end_date = 1970\n\ntrain_width_date = 365\nval_width_date = 27\nshift_width_date = 28\nmin_train_date = 0\n\nslide_list = []\nfor i in range(test_start_date-1,1,-shift_width_date):\n    end_date = i\n    split_date = end_date - val_width_date\n    start_date = split_date - train_width_date\n    if start_date < min_train_date:\n        break\n    slide_list.append([start_date,split_date,end_date])","3e1c4206":"slide_list","4c7c031a":"product = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\nproduct = product[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\nSTORE_IDS = list(product.store_id.unique())\ndel product\ngc.collect()","0fd566a3":"te2 = pd.read_pickle('..\/input\/m5-target-encoding2\/te_60.pkl')","59919ab9":"te2.columns","79e4912e":"add_feat = ['te_id_28', 'te_item_id_28', 'te_dept_id_28', 'te_cat_id_28',\n       'te_store_id_28', 'te_state_id_28', 'te_id_tm_dw_28',\n       'te_item_id_tm_dw_28', 'te_dept_id_tm_dw_28', 'te_cat_id_tm_dw_28',\n       'te_store_id_tm_dw_28', 'te_state_id_tm_dw_28']","7d5d60d7":"gc.collect()","9510605c":"for store_id in STORE_IDS[0:2]:\n    df_binary = pd.DataFrame([],columns = ['id','d','binary_pred'])\n    first = True\n    for start_date, split_date, end_date in tqdm(slide_list):\n        print('start_date, split_date, end_date:',start_date, split_date, end_date)\n        print('store_id:',store_id)\n        print('load dataset')\n        df = pd.read_pickle(PATHS[store_id])\n        df['sales_binary'] = np.where(df.sales==0,0,1)\n        df = df.merge(te2,on=['id', 'd'],how='left')\n        day_mask = (df.d>=start_date)&(df.d<split_date)\n        train = df[day_mask]\n        train_ids = train.id.unique()\n        NUM_ITEMS = len(train_ids)\n\n        day_mask = (df.d>=split_date)&(df.d<=end_date)\n        val = df[day_mask]\n        val = val[val.id.isin(train_ids)]\n        del df\n        gc.collect()\n\n        model = run_lgb_binary(train, val, feature, first)\n        pred = model.predict(val[feature])\n        tmp = val[['id','d']]\n        tmp['binary_pred'] = pred\n        df_binary = pd.concat([df_binary,tmp])\n        first = False\n\n    df_binary = reduce_mem_usage(df_binary)\n    df_binary.to_pickle('binary_pred_' + store_id + '.pkl')\n    \n    \n    ","72b70dd0":"## Preparation","6b5fe89c":"# \u4e8c\u5024\u5206\u985e\u3084\u308b\n    \n## Base Model\n\nLightGBM : https:\/\/www.kaggle.com\/girmdshinsei\/for-japanese-beginner-with-wrmsse-in-lgbm\n\n## Custom Objective\n\nLightGBM : https:\/\/www.kaggle.com\/girmdshinsei\/for-japanese-beginner-with-wrmsse-in-lgbm\n\n## Features\n\nhttps:\/\/www.kaggle.com\/kyakovlev\/m5-simple-fe  \nhttps:\/\/www.kaggle.com\/kyakovlev\/m5-custom-features  \nhttps:\/\/www.kaggle.com\/kyakovlev\/m5-lags-features\n\n## Consideration of Evaluation Function\n\n### RMSSE\n$$\nRMSSE = \\frac{\\sqrt{\\frac{1}{h}\\sum_{t=n+1}^{n+h}(Y_t - \\hat{Y}_t)^2}}{\\sqrt{\\frac{1}{n-1}\\sum_{t=2}^{n}(Y_t - Y_{t-1})^2}} = \\frac{RMSE(\\mathrm{model})}{RMSE(\\mathrm{naive\\ model})}\n$$\n\u3053\u308c\u306f\u30ca\u30a4\u30fc\u30d6\u306a\u4e88\u6e2c\u30e2\u30c7\u30eb$\\hat{Y}_t = Y_{t-1}$\u306eRMSE\u3068\u3001\u5b9f\u969b\u306e\u4e88\u6e2c\u30e2\u30c7\u30eb\u306eRMSE\u306e\u6bd4\u3068\u307f\u306a\u305b\u308b\u3002\nThis can be regarded as the ratio of the RMSE of the naive prediction model $ \\hat{Y}_t = Y_{t-1} $ and the RMSE of our prediction model.\n\n\nRMSSE\u3092\u63a1\u7528\u3059\u308b\u7406\u7531\u306f\u4ee5\u4e0b\u3067\u3042\u308b\u3002\n- \u30bc\u30ed\u304c\u591a\u3044\u305f\u3081\u3001\u5358\u306bRMSE\u3092\u6307\u6a19\u3068\u3059\u308b\u3068\u30bc\u30ed\u3092\u4e88\u6e2c\u3059\u308b\u30e2\u30c7\u30eb\u306e\u30b9\u30b3\u30a2\u304c\u4f4e\u304f\u306a\u308b\u3002\u3057\u305f\u304c\u3063\u3066\u4e88\u6e2c\u30e2\u30c7\u30eb\u304c\u30ca\u30a4\u30fc\u30d6\u306a\u30e2\u30c7\u30eb\u3068\u6bd4\u8f03\u3057\u3001\u3069\u308c\u3060\u3051\u512a\u308c\u3066\u3044\u308b\u304b\u3068\u3044\u3046\u6307\u6a19\u3092\u7528\u3044\u308b\u3002\n- scale independent\n- \u30bc\u30ed\u5272\u306e\u5fc3\u914d\u304c\u306a\u3044\n- \u6b63\u65b9\u5411\u306e\u8aa4\u5dee\u3068\u8ca0\u65b9\u5411\u306e\u8aa4\u5dee\u304c\u5bfe\u79f0\u306b\u898b\u7a4d\u3082\u3089\u308c\u308b\n\n\nRMSSE\u306e\u4ee3\u308f\u308a\u306b\u4e0a\u8a18\u306e\u6761\u4ef6\u3092\u6e80\u305f\u3059\u4ee5\u4e0b\u3092\u5b9a\u7fa9\u3057\u7528\u3044\u308b\uff1a\n we define a new RMSSE that extends above:\n$$\nRMSSE\\_AllDAY = \\frac{\\sqrt{\\frac{1}{h}\\sum_{day_in_validation}(Y_t - \\hat{Y}_t)^2}}{\\sqrt{\\frac{1}{n-1}\\sum_{fixed\\_section}(Y_t - Y_{t-1})^2}} = \\frac{\\sqrt{\\frac{1}{h}\\sum_{day_in_validation}(Y_t - \\hat{Y}_t)^2}}{\\sqrt{\\frac{1}{n-1}\\sum_{t = 2}^{1913}(Y_t - Y_{t-1})^2}}\n$$\nRMSSE\\_B\u306f\u3042\u308b\u533a\u9593\u306e\u30c7\u30fc\u30bf\u304b\u3089\u30ca\u30a4\u30fc\u30d6\u30e2\u30c7\u30eb\u306eRMSE\u3092\u8a08\u7b97\u3059\u308b\u3002","3854eea4":"## Make Parameter Grid","03478a2e":"### Training Parameters","7ae01da2":"## Define Features","7e63188c":"## Use Files","7e271d39":"## Model","9c3b9734":"## import","a9a6a858":"## Training"}}