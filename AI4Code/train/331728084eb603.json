{"cell_type":{"d6dd6aa1":"code","b16b0f93":"code","ed2567c5":"code","bcb85d75":"code","6ee3dd9e":"code","6556b566":"code","4a8ef879":"code","4dcbb873":"code","d42f89c7":"code","0ea2b521":"code","3d5a085f":"code","b69917ec":"code","292f890e":"code","b9ec3f58":"code","0b22e4d2":"code","916dc50e":"code","042e9bfa":"code","64033ca9":"code","b23e0725":"code","1dad9a48":"code","e2392735":"markdown","0d0704c0":"markdown","11633d81":"markdown","111cb08f":"markdown","0b2f4227":"markdown","0521cc01":"markdown","1b911242":"markdown","3fc8a2f9":"markdown","e05f9b63":"markdown","22480874":"markdown","3ba1837a":"markdown","0b371c36":"markdown"},"source":{"d6dd6aa1":"!pip install alibi","b16b0f93":"import tensorflow as tf\ntf.get_logger().setLevel(40)\ntf.compat.v1.disable_v2_behavior() \nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.utils import to_categorical\n\nimport matplotlib\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\nfrom alibi.explainers import CEM\n\nprint('TF version: ', tf.__version__)\nprint('Eager execution enabled: ', tf.executing_eagerly()) # False","ed2567c5":"dataset = load_iris()\nfeature_names = dataset.feature_names\nclass_names = list(dataset.target_names)\nclass_names","bcb85d75":"dataset.data = (dataset.data - dataset.data.mean(axis=0)) \/ dataset.data.std(axis=0)","6ee3dd9e":"idx = 145\nx_train,y_train = dataset.data[:idx,:], dataset.target[:idx]\nx_test, y_test = dataset.data[idx+1:,:], dataset.target[idx+1:]\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)","6556b566":"def lr_model():\n    x_in = Input(shape=(4,))\n    x_out = Dense(3, activation='softmax')(x_in)\n    lr = Model(inputs=x_in, outputs=x_out)\n    lr.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n    return lr","4a8ef879":"lr = lr_model()\nlr.summary()\nlr.fit(x_train, y_train, batch_size=16, epochs=500, verbose=0)\nlr.save('iris_lr.h5', save_format='h5')","4dcbb873":"idx = 0\nX = x_test[idx].reshape((1,) + x_test[idx].shape)\nprint('Prediction on instance to be explained: {}'.format(class_names[np.argmax(lr.predict(X))]))\nprint('Prediction probabilities for each class on the instance: {}'.format(lr.predict(X)))","d42f89c7":"mode = 'PN' \nshape = (1,) + x_train.shape[1:]  \nkappa = .2 \n            \n            \nbeta = .1  \nc_init = 10. \nc_steps = 10\nmax_iterations = 1000  \nfeature_range = (x_train.min(axis=0).reshape(shape)-.1,  \n                 x_train.max(axis=0).reshape(shape)+.1)  \nclip = (-1000.,1000.)  \nlr_init = 1e-2  ","0ea2b521":"lr = load_model('iris_lr.h5')\n\n# initialize CEM explainer and explain instance\ncem = CEM(lr, mode, shape, kappa=kappa, beta=beta, feature_range=feature_range,\n          max_iterations=max_iterations, c_init=c_init, c_steps=c_steps,\n          learning_rate_init=lr_init, clip=clip)\ncem.fit(x_train, no_info_type='median')  \nexplanation = cem.explain(X, verbose=False)","3d5a085f":"print('Original instance: {}'.format(explanation.X))\nprint('Predicted class: {}'.format(class_names[explanation.X_pred]))\nprint('Pertinent negative: {}'.format(explanation.PN))\nprint('Predicted class: {}'.format(class_names[explanation.PN_pred]))","b69917ec":"print('Pertinent negative: {}'.format(explanation.PN))\nprint('Predicted class: {}'.format(class_names[explanation.PN_pred]))","292f890e":"expl = {}\nexpl['PN'] = explanation.PN\nexpl['PN_pred'] = explanation.PN_pred","b9ec3f58":"mode = 'PP'","0b22e4d2":"# define model\nlr = load_model('iris_lr.h5')\n\n# initialize CEM explainer and explain instance\ncem = CEM(lr, mode, shape, kappa=kappa, beta=beta, feature_range=feature_range,\n          max_iterations=max_iterations, c_init=c_init, c_steps=c_steps,\n          learning_rate_init=lr_init, clip=clip)\ncem.fit(x_train, no_info_type='median')\nexplanation = cem.explain(X, verbose=False)","916dc50e":"print('Original instance: {}'.format(explanation.X))\nprint('Predicted class: {}'.format(class_names[explanation.X_pred]))\nprint('Pertinent positive: {}'.format(explanation.PP))\nprint('Predicted class: {}'.format(class_names[explanation.PP_pred]))","042e9bfa":"expl['PP'] = explanation.PP\nexpl['PP_pred'] = explanation.PP_pred","64033ca9":"df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\ndf['species'] = np.array([dataset.target_names[i] for i in dataset.target])","b23e0725":"pn = pd.DataFrame(expl['PN'], columns=dataset.feature_names)\npn['species'] = 'PN_' + class_names[expl['PN_pred']]\npp = pd.DataFrame(expl['PP'], columns=dataset.feature_names)\npp['species'] = 'PP_' + class_names[expl['PP_pred']]\norig_inst = pd.DataFrame(explanation.X, columns=dataset.feature_names)\norig_inst['species'] = 'orig_' + class_names[explanation.X_pred]\ndf = df.append([pn, pp, orig_inst], ignore_index=True)","1dad9a48":"fig = sns.pairplot(df, hue='species', diag_kind='hist',palette=\"husl\");","e2392735":"Now, we are training our regression model:","0d0704c0":"Visualize PN and PP","11633d81":"# Contrastive Explanations Method(CEM) applied to Iris dataset","111cb08f":"Generating contrastive explaination for pertinent negative:","0b2f4227":"CEM generates instance based local black box explanations for classification models in terms of Pertinent Positives (PP) and Pertinent Negatives (PN). \n\nFor a PP, the method finds the features that should be minimally and sufficiently present (e.g. important pixels in an image) to predict the same class as on the original instance. \n\nPN\u2019s on the other hand identify what features should be minimally and necessarily absent from the instance to be explained in order to maintain the original prediction class. The aim of PN\u2019s is not to provide a full set of characteristics that should be absent in the explained instance, but to provide a minimal set that differentiates it from the closest different class.","0521cc01":"Here, \n\n*   mode : 'PN' (Pertinent Negative) or 'PP' (Pertinent Positive)\n*   shape : Shape of the current instance. As CEM is applicable for single explanations, we take 1.\n*   kappa, beta, gamma, c_init, c_steps are all mathematical terms for calculating loss\n*   max_iterations : the total no. of loss optimization steps for each value of c\n*   feature_range : global or feature wise minimum and maximum values for the changed instance\n*   clip : minimum and maximum gradient values\n*   lr_init : initial learning rate \n\n","1b911242":"Here, we have used the iris dataset:","3fc8a2f9":"Generating pertinent positive:","e05f9b63":"The original prediction of the instance is shown in purple (label: orig_virginica). Now, when CEM with PN is applied, the instance is moved away from the mean of instances of virginica (original prediction) and when CEM with PP is applied, it is moved towards the mean of virginica. This is clearly shown in the above graphs as the blue dot showing PP-virginica is near the cluster of virginica(green dots) and the bluish green dot showing PN-versicolor is away from instances of virginica.","22480874":"The above result clearly shows that the pertinent negative method pushes the prediction to get a prediction different from the original prediction which is virginica to versicolor in this case.\n\nThe 3rd component in array which is petal length is different from the original one. Thus, it can be concluded that this change in petal length should necessarily be absent to retain the original prediction as virginica.","3ba1837a":"The above result shows the feature values that should be compulsorily and minimally present in order to get the same original class virginica as predicted class.","0b371c36":"Thus, using CEM we can easily find out what features need to be minimally present and absent for the same class to be predicted. "}}