{"cell_type":{"2b7a26bd":"code","1d78c885":"code","72d47174":"code","118058d5":"code","4874326f":"code","ffc3fd3c":"code","a694402d":"code","96a9d21d":"code","2a023a1d":"code","92cfe092":"code","ba4cf448":"code","07a80349":"code","567e9432":"code","2ac2fc7f":"code","6e786e9f":"code","a1b2421a":"code","ad6b2324":"code","d63a471b":"code","4ed33dd6":"markdown","4240019f":"markdown","fdb37316":"markdown","f260f518":"markdown","ae37cfbf":"markdown"},"source":{"2b7a26bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1d78c885":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nimport spacy\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#plotly\nimport plotly.offline as py\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\ninit_notebook_mode(connected=True)\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\nimport warnings\nwarnings.filterwarnings('ignore')","72d47174":"import pandas as pd\nimport numpy as np\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","118058d5":"train=pd.read_csv('..\/input\/word2vec-nlp-tutorial\/labeledTrainData.tsv.zip',compression='zip',\n                 header=0,delimiter='\\t',quoting=0, doublequote=False, escapechar='\\\\')\ntrain.head()","4874326f":"test=pd.read_csv('..\/input\/word2vec-nlp-tutorial\/testData.tsv.zip', compression='zip',\n                header=0, delimiter='\\t', quoting=0)\ntest.head()","ffc3fd3c":"unlabeled=pd.read_csv('..\/input\/word2vec-nlp-tutorial\/unlabeledTrainData.tsv.zip', compression='zip',\n                 header=0,delimiter='\\t',quoting=0, doublequote=False, escapechar='\\\\')\nunlabeled.head()","a694402d":"from nltk.corpus import stopwords\n\",\".join(stopwords.words(\"english\"))","96a9d21d":"from nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nimport re\nfrom collections import Counter\ncnt=Counter()","2a023a1d":"cList = {\n  \"ain't\": \"am not\",\n  \"aren't\": \"are not\",\n  \"can't\": \"cannot\",\n  \"can't've\": \"cannot have\",\n  \"'cause\": \"because\",\n  \"could've\": \"could have\",\n  \"couldn't\": \"could not\",\n  \"couldn't've\": \"could not have\",\n  \"didn't\": \"did not\",\n  \"doesn't\": \"does not\",\n  \"don't\": \"do not\",\n  \"hadn't\": \"had not\",\n  \"hadn't've\": \"had not have\",\n  \"hasn't\": \"has not\",\n  \"haven't\": \"have not\",\n  \"he'd\": \"he would\",\n  \"he'd've\": \"he would have\",\n  \"he'll\": \"he will\",\n  \"he'll've\": \"he will have\",\n  \"he's\": \"he is\",\n  \"how'd\": \"how did\",\n  \"how'd'y\": \"how do you\",\n  \"how'll\": \"how will\",\n  \"how's\": \"how is\",\n  \"I'd\": \"I would\",\n  \"I'd've\": \"I would have\",\n  \"I'll\": \"I will\",\n  \"I'll've\": \"I will have\",\n  \"I'm\": \"I am\",\n  \"I've\": \"I have\",\n   \"isn't\": \"is not\",\n  \"it'd\": \"it had\",\n  \"it'd've\": \"it would have\",\n  \"it'll\": \"it will\",\n  \"it'll've\": \"it will have\",\n  \"it's\": \"it is\",\n  \"let's\": \"let us\",\n  \"ma'am\": \"madam\",\n  \"mayn't\": \"may not\",\n  \"might've\": \"might have\",\n  \"mightn't\": \"might not\",\n  \"mightn't've\": \"might not have\",\n  \"must've\": \"must have\",\n  \"mustn't\": \"must not\",\n  \"mustn't've\": \"must not have\",\n  \"needn't\": \"need not\",\n  \"needn't've\": \"need not have\",\n  \"o'clock\": \"of the clock\",\n  \"oughtn't\": \"ought not\",\n  \"oughtn't've\": \"ought not have\",\n  \"shan't\": \"shall not\",\n  \"sha'n't\": \"shall not\",\n  \"shan't've\": \"shall not have\",\n  \"she'd\": \"she would\",\n  \"she'd've\": \"she would have\",\n  \"she'll\": \"she will\",\n  \"she'll've\": \"she will have\",\n   \"she's\": \"she is\",\n  \"should've\": \"should have\",\n  \"shouldn't\": \"should not\",\n  \"shouldn't've\": \"should not have\",\n  \"so've\": \"so have\",\n  \"so's\": \"so is\",\n  \"that'd\": \"that would\",\n  \"that'd've\": \"that would have\",\n  \"that's\": \"that is\",\n  \"there'd\": \"there had\",\n  \"there'd've\": \"there would have\",\n  \"there's\": \"there is\",\n  \"they'd\": \"they would\",\n  \"they'd've\": \"they would have\",\n  \"they'll\": \"they will\",\n  \"they'll've\": \"they will have\",\n  \"they're\": \"they are\",\n  \"they've\": \"they have\",\n  \"to've\": \"to have\",\n  \"wasn't\": \"was not\",\n  \"we'd\": \"we had\",\n  \"we'd've\": \"we would have\",\n  \"we'll\": \"we will\",\n  \"we'll've\": \"we will have\",\n  \"we're\": \"we are\",\n  \"we've\": \"we have\",\n  \"weren't\": \"were not\",\n  \"what'll\": \"what will\",\n  \"what'll've\": \"what will have\",\n  \"what're\": \"what are\",\n    \"what's\": \"what is\",\n  \"what've\": \"what have\",\n  \"when's\": \"when is\",\n  \"when've\": \"when have\",\n  \"where'd\": \"where did\",\n  \"where's\": \"where is\",\n  \"where've\": \"where have\",\n  \"who'll\": \"who will\",\n  \"who'll've\": \"who will have\",\n  \"who's\": \"who is\",\n  \"who've\": \"who have\",\n  \"why's\": \"why is\",\n  \"why've\": \"why have\",\n  \"will've\": \"will have\",\n  \"won't\": \"will not\",\n  \"won't've\": \"will not have\",\n  \"would've\": \"would have\",\n  \"wouldn't\": \"would not\",\n  \"wouldn't've\": \"would not have\",\n  \"y'all\": \"you all\",\n  \"y'alls\": \"you alls\",\n  \"y'all'd\": \"you all would\",\n  \"y'all'd've\": \"you all would have\",\n  \"y'all're\": \"you all are\",\n  \"y'all've\": \"you all have\",\n  \"you'd\": \"you had\",\n  \"you'd've\": \"you would have\",\n  \"you'll\": \"you you will\",\n  \"you'll've\": \"you you will have\",\n  \"you're\": \"you are\",\n  \"you've\": \"you have\"\n}\n","92cfe092":"def clean_text(df, col):\n    # lowering the case \n    def lower_case(df,col):\n        df[col]=df[col].str.lower()\n    # remove punctuation \n    \n    punc_to_remove=string.punctuation\n    def remove_punctuation(text):\n        return text.translate(str.maketrans('','',punc_to_remove))\n    df[col]=df[col].apply(lambda text: remove_punctuation(text))\n    \n    #remove_frequent_words\n    FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n    def remove_freqwords(text):\n        \"\"\"custom function to remove the frequent words\"\"\"\n        return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n    df[col]=df[col].apply(lambda text: remove_freqwords(text))\n    \n    # remove_rare words\n    n_rare_word=10\n    RAREWORDS=set([w for (w ,wc) in cnt.most_common()[:-n_rare_word-1:-1]])\n\n    def remove_rareword(text):\n        return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n    df[col]=df[col].apply(lambda text: remove_rareword(text))\n    \n    #Porter Stemming\n    stemmer=PorterStemmer()\n\n    def stem_porter(text):\n        return \" \".join([stemmer.stem(word) for word in text.split()])\n    df[col]=df[col].apply(lambda text: stem_porter(text))\n    \n    # Lemmatization\n    \n    lematizer=WordNetLemmatizer()\n    def lemmatizer_words(text):\n        return \" \".join([lematizer.lemmatize(word) for word in text.split()])\n    df[col]=df[col].apply(lambda text: lemmatizer_words(text))\n    \n    lemmmatizer=WordNetLemmatizer()\n\n    wordnet_map={\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n    def lemmatized_words(text):\n        pos_tagged_text = nltk.pos_tag(text.split())\n        return \" \".join([lematizer.lemmatize(word , wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n    df[col]=df[col].apply(lambda text: lemmatized_words(text))\n    \n    # Remove HTML tag from the text\n    \n    TAG_RE = re.compile(r'<[^>]+>')\n\n    def remove_tags(text):\n        return TAG_RE.sub('', text)\n    df[col]=df[col].apply(lambda text: remove_tags(text))\n    \n    # remove urls\n    \n    def remove_urld(text):\n        url_pattern=re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n        return url_pattern.sub(r'', text)\n\n    df[col]=df[col].apply(lambda text: remove_urld(text))\n    \n    # expamnds the decontracted words\n    \n    c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n    def expandContractions(text, c_re=c_re):\n        def replace(match):\n            return cList[match.group(0)]\n        return c_re.sub(replace, text)\n    df[col]=df[col].apply(lambda text: expandContractions(text, c_re=c_re))\n    \n    ","ba4cf448":"clean_text(train,\"review\")\n","07a80349":"train.head()","567e9432":"train['review'][0]","2ac2fc7f":"clean_text(test,\"review\")\ntest.head()","6e786e9f":"cnt=Counter()\n\nfor text in train['review'].values:\n    for word in text.split():\n        cnt[word]+=1\n        \ncnt.most_common(50)","a1b2421a":"most_common_word=pd.DataFrame(cnt.most_common(50), columns=['words','count'])\nmost_common_word.head()","ad6b2324":"fig, ax = plt.subplots(figsize=(8, 8))\n\n# Plot horizontal bar graph\nmost_common_word.sort_values(by='count').plot.barh(x='words',\n                      y='count',\n                      ax=ax,\n                      color=\"purple\", figsize=(20,20))\n\nax.set_title(\"Common Words Found in Review (Clearned Words)\")\nplt.grid(b=True)\nplt.show()","d63a471b":"fig = px.bar(most_common_word, x=\"words\", y=\"count\", title=\"Bar Chart(including the Stop words)\", color=\"words\")\nfig.show()","4ed33dd6":"# Visualize the Most Common words in review","4240019f":"# Text Preprocesing","fdb37316":"# Text cleanings\n* Lower case\n* Remove Puncuation\n* Remove Frequents Words\n* Remove Rare Words\n* Stemmer\n* Lemmatization\n* Removing stop words\n* Remove URLS\n* Expanding the deontracted words ","f260f518":"# counting the most common words","ae37cfbf":"# Conclusion \n* More Update will be done Soon\n* This is the 2nd step of text cleaning\n* 3rd part will come soon\n* For the 1st part clivk here is the link \n* https:\/\/www.kaggle.com\/jurk06\/sentiment-analysis-part-i\/#Stop-Words"}}