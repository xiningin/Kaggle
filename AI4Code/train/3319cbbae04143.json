{"cell_type":{"36c80da6":"code","3161e70b":"code","18172cee":"code","e595ccb1":"code","15559871":"code","f99733df":"code","cfc80c84":"code","4ffa11a9":"code","eb987990":"code","5da29eba":"code","9d027795":"code","1e50b4de":"code","70282426":"code","532b0892":"code","2846b943":"code","2e2c51d5":"code","9e332402":"code","92263002":"code","7afc47c9":"code","433d1750":"code","c7f7bac6":"code","e8842832":"code","d9dc3338":"code","e960c13b":"code","b025b5d5":"code","7a8b1794":"code","d6205ac8":"code","0416e103":"code","9607cb54":"code","5114964a":"code","dbc047e7":"code","d5612b7f":"code","91f6612b":"code","e1e022a4":"code","171cfab8":"code","e29a4617":"code","bc9b8772":"markdown","85f7f3ce":"markdown","f854af19":"markdown","0f820f2e":"markdown","0faf8497":"markdown","d6ae1a40":"markdown","a87bdff8":"markdown","7627172b":"markdown","49f4174b":"markdown","f1cae319":"markdown","f8c2504a":"markdown","6aeda58d":"markdown","dd9ea608":"markdown","75386bc3":"markdown","32b88923":"markdown","2ff83b8e":"markdown","81486b69":"markdown","8b56550b":"markdown","b5ade4d3":"markdown","63ce2d2e":"markdown"},"source":{"36c80da6":"# Import the packages and libraries needed for this project\nimport matplotlib as mpl, matplotlib.pyplot as plt, \\\npandas as pd, seaborn as sns, sklearn as sk\nfrom sklearn.metrics import accuracy_score, \\\nconfusion_matrix, multilabel_confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split","3161e70b":"# Checking for package version\nprint(\"Matplotlib Version\", mpl.__version__)\nprint(\"Pandas Version\", pd.__version__)\nprint(\"Seaborn Version\", sns.__version__)\nprint(\"Sci-kit learn Version\", sk.__version__)","18172cee":"# Import the three datasets\ndf_data_1 = pd.read_csv('\/kaggle\/input\/hr-analytics\/HR_comma_sep.csv')\ndf_data_2 = pd.read_csv('\/kaggle\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndf_data_3 = pd.read_csv('\/kaggle\/input\/employee-turnover\/turnover.csv', encoding = 'ISO-8859-1')\ndf_model_1 = df_data_1.copy()\ndf_model_2 = df_data_2.copy()\ndf_model_3 = df_data_3.copy()","e595ccb1":"# Explore dataset 1\ndf_data_1.head(5)","15559871":"# Explore dataset 2\ndf_data_2.head(5)","f99733df":"# Explore dataset 3\ndf_data_3.head(5)","cfc80c84":"# Check for null.\ndisplay(df_data_1.isnull().values.any())\ndisplay(df_data_2.isnull().values.any())\ndisplay(df_data_3.isnull().values.any())","4ffa11a9":"# Declare unwanted features (i.e. columns) that will be dropped.\ndrop_2 = ['EmployeeCount', 'EmployeeNumber', 'StandardHours', 'Over18']\n# drop the features.\ndf_model_2 = df_model_2.drop(drop_2,axis=1)","eb987990":"# Rename all target features.\ndf_model_1 = df_model_1.rename(columns={'left': 'churn'})\ndf_model_2 = df_model_2.rename(columns={'Attrition': 'churn'})\ndf_model_3 = df_model_3.rename(columns={'event': 'churn'})","5da29eba":"# Rename all treatment features.\ndf_model_1 = df_model_1.rename(columns={'promotion_last_5years': 'treatment'})\ndf_model_2 = df_model_2.rename(columns={'OverTime': 'treatment'})\ndf_model_3 = df_model_3.rename(columns={'coach': 'treatment'})","9d027795":"# Declare features for label encoding.\nstring1, string2, string3 = ['salary'], ['churn',\n                                         'treatment',\n                                         'BusinessTravel'], ['treatment']\n# Explore the unique data for label encoding\nfor col in string1:\n    display(col, df_model_1[col].unique())\nfor col in string2:\n    display(col, df_model_2[col].unique())\nfor col in string3:\n    display(col, df_model_3[col].unique())","1e50b4de":"# Manually label encode the target (i.e. dependent variable) for dataset 1\ndf_model_1.salary = df_model_1.salary.map({'low': 0, 'medium': 1, 'high':2})","70282426":"# Manually label encode the target and identified treatment (i.e. employee retention program) for dataset 2\ndf_model_2.churn = df_model_2.churn.map({'Yes': 1, 'No': 0})\ndf_model_2.treatment = df_model_2.treatment.map({'Yes': 0, 'No': 1})\n# Declaration BusinessTravel\ndf_model_2.BusinessTravel = df_model_2.BusinessTravel.map({'Non-Travel': 0,\n                                                           'Travel_Rarely': 1,\n                                                           'Travel_Frequently':2})","532b0892":"# Manually label encode the target and treatment for dataset 3\ndf_model_3.treatment = df_model_3.treatment.map({'yes': 0, 'no': 1, 'my head':2})\ndf_model_3 = df_model_3.loc[df_model_3.treatment <=1].reset_index(drop=True)","2846b943":"# One-Hot Encoding:\ndf_model_1, df_model_inverse_1 = pd.get_dummies(df_model_1), pd.get_dummies(df_model_1)\ndf_model_2, df_model_inverse_2 = pd.get_dummies(df_model_2), pd.get_dummies(df_model_2)\ndf_model_3, df_model_inverse_3 = pd.get_dummies(df_model_3), pd.get_dummies(df_model_3)","2e2c51d5":"def correlation_treatment(df:pd.DataFrame):\n    \"\"\"Function to calculate the treatment's correlation\n    \"\"\"\n    correlation = df[['treatment','churn']].corr(method ='pearson') \n    return(pd.DataFrame(round(correlation.loc['churn'] * 100,2)))","9e332402":"print(\"Dataset 1:\", correlation_treatment(df_model_1).iloc[0,0])\nprint(\"Dataset 2:\", correlation_treatment(df_model_2).iloc[0,0])\nprint(\"Dataset 3:\", correlation_treatment(df_model_3).iloc[0,0])","92263002":"def declare_target_class(df:pd.DataFrame):\n    \"\"\"Function for declare the target class\n    \"\"\"\n    #CN:\n    df['target_class'] = 0 \n    #CR:\n    df.loc[(df.treatment == 0) & (df.churn == 0),'target_class'] = 1 \n    #TN:\n    df.loc[(df.treatment == 1) & (df.churn == 1),'target_class'] = 2 \n    #TR:\n    df.loc[(df.treatment == 1) & (df.churn == 0),'target_class'] = 3 \n    return df","7afc47c9":"# Add the four target classes\ndf_model_1, df_model_2, df_model_3 = declare_target_class(df_model_1), \\\ndeclare_target_class(df_model_2), declare_target_class(df_model_3)","433d1750":"def split_data(df_model:pd.DataFrame):\n    \"\"\"Split data into training data and testing data\n    \"\"\"\n    X = df_model.drop(['churn','target_class'],axis=1)\n    y = df_model.churn\n    z = df_model.target_class\n    X_train, X_test, \\\n    y_train, y_test, \\\n    z_train, z_test = train_test_split(X,\n                                       y,\n                                       z,\n                                       test_size=0.3,\n                                       random_state=42,\n                                       stratify=df_model['treatment'])\n    return X_train,X_test, y_train, y_test, z_train, z_test\n\n\ndef machine_learning(X_train:pd.DataFrame,\n                     X_test:pd.DataFrame,\n                     y_train:pd.DataFrame,\n                     y_test:pd.DataFrame,\n                     z_train:pd.DataFrame,\n                     z_test:pd.DataFrame):\n    \"\"\"Machine learning process consists of \n    data training, and data testing process (i.e. prediction) with RandomForestClassifier Algorithm\n    \"\"\"\n    # prepare a new DataFrame\n    prediction_results = pd.DataFrame(X_test).copy()\n    \n    \n    # train the ETP model\n    model_tp \\\n    = RandomForestClassifier().fit(X_train.drop('treatment', axis=1), y_train)  \n    # prediction Process for ETP model \n    prediction_tp \\\n    = model_tp.predict(X_test.drop('treatment',axis=1))\n    probability__tp \\\n    = model_tp.predict_proba(X_test.drop('treatment', axis=1))\n    prediction_results['prediction_churn'] = prediction_tp\n    prediction_results['proba_churn'] = probability__tp[:,1]\n    \n    \n    # train the ETU model\n    model_etu \\\n    = RandomForestClassifier().fit(X_train.drop('treatment', axis=1), z_train)\n    # prediction Process for ETU model \n    prediction_etu \\\n    = model_etu.predict(X_test.drop('treatment', axis=1))\n    probability__etu \\\n    = model_etu.predict_proba(X_test.drop('treatment', axis=1))\n    prediction_results['prediction_target_class'] = prediction_etu\n    prediction_results['proba_CN'] = probability__etu[:,0] \n    prediction_results['proba_CR'] = probability__etu[:,1] \n    prediction_results['proba_TN'] = probability__etu[:,2] \n    prediction_results['proba_TR'] = probability__etu[:,3]\n    prediction_results['score_etu'] = prediction_results.eval('\\\n    proba_CN\/(proba_CN+proba_CR) \\\n    + proba_TR\/(proba_TN+proba_TR) \\\n    - proba_TN\/(proba_TN+proba_TR) \\\n    - proba_CR\/(proba_CN+proba_CR)')  \n    \n    # add the churn and target class into dataframe as validation data\n    prediction_results['churn'] = y_test\n    prediction_results['target_class'] = z_test\n    return prediction_results\n\n\ndef predict(df_model:pd.DataFrame):\n    \"\"\"Combining data split and machine learning process with Random Forest Classifier\n    \"\"\"\n    X_train, X_test, y_train, y_test, z_train, z_test = split_data(df_model)\n    prediction_results = machine_learning(X_train,\n                                          X_test,\n                                          y_train,\n                                          y_test,\n                                          z_train,\n                                          z_test)\n    print(\"Prediction has succeeded\")\n    return prediction_results","c7f7bac6":"# Machine Learning Modelling Process\nprint(\"predicting dataset 1 ...\")\nprediction_results_1 = predict(df_model_1)\nprint(\"predicting dataset 2 ...\")\nprediction_results_2 = predict(df_model_2)\nprint(\"predicting dataset 3 ...\")\nprediction_results_3 = predict(df_model_3)","e8842832":"def cm_evaluation(df:pd.DataFrame):\n    \"\"\"Confusion matrix evaluation\n    \"\"\"  \n    print(\"===================================\")\n    print(\"1. ETP's confusion matrix result:\")\n    confusion_etp = confusion_matrix(df['churn'], df['prediction_churn'])\n    df_confusion_etp = pd.DataFrame(confusion_etp, columns = ['True','False'], index = ['Positive','Negative'])\n    print(df_confusion_etp)\n    \n    print(\"-----------------------------------\")\n    \n    print(\"2. ETU's confusion matrix result:\")   \n    confusion_etu = multilabel_confusion_matrix(df['target_class'], df['prediction_target_class'])\n    print(\"a. CN's confusion matrix:\")  \n    df_cn = pd.DataFrame(confusion_etu[0], columns = ['True','False'], index = ['Positive','Negative'])\n    print(df_cn)\n    print(\"b. CR's confusion matrix:\") \n    df_cr = pd.DataFrame(confusion_etu[1], columns = ['True','False'], index = ['Positive','Negative'])\n    print(df_cr) \n    print(\"c. TN's confusion matrix:\")\n    df_tn = pd.DataFrame(confusion_etu[2], columns = ['True','False'], index = ['Positive','Negative'])\n    print(df_tn) \n    print(\"d. TR's confusion matrix:\") \n    df_tr = pd.DataFrame(confusion_etu[3], columns = ['True','False'], index = ['Positive','Negative'])\n    print(df_tr)\n    \n    print(\"===================================\")","d9dc3338":"# Confusion Matrix Evaluation\nprint(\"Dataset 1\")\ncm_evaluation(prediction_results_1)\nprint(\"Dataset 2\")\ncm_evaluation(prediction_results_2)\nprint(\"Dataset 3\")\ncm_evaluation(prediction_results_3)","e960c13b":"def accuracy_evaluation(df:pd.DataFrame):\n    \"\"\"Accuracy evaluation\n    \"\"\"\n    akurasi_cp = accuracy_score(df['churn'],\n                                df['prediction_churn'])\n    print('ETP model accuracy: %.2f%%' % (akurasi_cp * 100.0))\n    \n    \n    akurasi_uplift = accuracy_score(df['target_class'],\n                                    df['prediction_target_class'])\n    print('ETU model accuracy: %.2f%%' % (akurasi_uplift * 100.0))","b025b5d5":"# Accuracy Evaluation Process.\nprint(\"Dataset 1\")\naccuracy_evaluation(prediction_results_1)\nprint(\"Dataset 2\")\naccuracy_evaluation(prediction_results_2)\nprint(\"Dataset 3\")\naccuracy_evaluation(prediction_results_3)","7a8b1794":"def sorting_data(df:pd.DataFrame):\n    \"\"\"Function to sort data\n    \"\"\"\n    # Set up new DataFrames for ETP model and ETU model\n    df_c = pd.DataFrame({'n':[], 'target_class':[]})\n    df_u = df_c.copy()\n    df_c['target_class'] = df['target_class']\n    df_u['target_class'] = df['target_class']\n    \n    \n    # Add quantiles\n    df_c['n'] = df.proba_churn.rank(pct=True, ascending=False)\n    df_u['n'] = df.score_etu.rank(pct=True, ascending=False)\n    df_c['score'] = df['proba_churn']\n    df_u['score'] = df['score_etu']\n    \n    \n    # Ranking the data by deciles\n    df_c = df_c.sort_values(by='n').reset_index(drop=True)\n    df_u = df_u.sort_values(by='n').reset_index(drop=True)\n    df_c['model'], df_u['model'] = 'CP', 'Uplift'\n    return df_c, df_u\n\n\ndef calculating_qini(df:pd.DataFrame):\n    \"\"\"Function to measure the Qini value\n    \"\"\"\n    # Calculate the C, T, CR, and TR\n    C, T = sum(df['target_class'] <= 1), sum(df['target_class'] >= 2)\n    df['cr'] = 0\n    df['tr'] = 0\n    df.loc[df.target_class  == 1,'cr'] = 1\n    df.loc[df.target_class  == 3,'tr'] = 1\n    df['cr\/c'] = df.cr.cumsum() \/ C\n    df['tr\/t'] = df.tr.cumsum() \/ T\n    \n    \n    # Calculate & add the qini value into the Dataframe\n    df['uplift'] = df['tr\/t'] - df['cr\/c']\n    df['random'] = df['n'] * df['uplift'].iloc[-1]\n    # Add q0 into the Dataframe\n    q0 = pd.DataFrame({'n':0, 'uplift':0, 'target_class': None}, index =[0])\n    qini = pd.concat([q0, df]).reset_index(drop = True)\n    return qini\n\n\ndef merging_data(df_c:pd.DataFrame, df_u:pd.DataFrame):\n    \"\"\"Function to add the 'Model' column and merge the dataframe into one\n    \"\"\"\n    df_u['model'] = 'ETU'\n    df_c['model'] = 'ETP'\n    df = pd.concat([df_u, df_c]).sort_values(by='n').reset_index(drop = True)\n    return df\n\n\ndef plot_qini(df:pd.DataFrame):\n    \"\"\"Function to plot qini\n    \"\"\"\n    # Define the data that will be plotted\n    order = ['ETU','ETP']\n    ax = sns.lineplot(x='n', y=df.uplift, hue='model', data=df,\n                      style='model', palette=['red','deepskyblue'],\n                      style_order=order, hue_order = order)\n    \n    \n    # Additional plot display settings\n    handles, labels = ax.get_legend_handles_labels()\n    plt.xlabel('Proportion targeted',fontsize=30)\n    plt.ylabel('Uplift',fontsize=30)\n    plt.subplots_adjust(right=1)\n    plt.subplots_adjust(top=1)\n    plt.legend(fontsize=30)\n    ax.tick_params(labelsize=24)\n    ax.legend(handles=handles[1:], labels=labels[1:])\n    ax.plot([0,1], [0,df.loc[len(df) - 1,'uplift']],'--', color='grey')\n    return ax\n\n\ndef evaluation_qini(prediction_results:pd.DataFrame):\n    \"\"\"Function to combine all qini evaluation processes\n    \"\"\"\n    df_c, df_u = sorting_data(prediction_results)\n    qini_c, qini_u = calculating_qini(df_c), calculating_qini(df_u)\n    qini = merging_data(qini_c, qini_u)\n    ax = plot_qini(qini)\n    return ax, qini","d6205ac8":"# Qini evaluation results for DataSet 1 with negative treatment correlation\nax, qini_1 = evaluation_qini(prediction_results_1)\nplt.title('Qini Curve - Dataset 1',fontsize=20)\n\n\n# save into pdf:\n# plt.savefig('qini_1_n.pdf', bbox_inches='tight')","0416e103":"# Qini evaluation results for DataSet 2 with negative treatment correlation\nax, qini_2 = evaluation_qini(prediction_results_2)\nplt.title('Qini Curve - Dataset 2',fontsize=20)\n\n\n# save into pdf:\n# plt.savefig('qini_2_n.pdf', bbox_inches='tight'","9607cb54":"# Qini evaluation results for DataSet 3 with negative treatment correlation\nax, qini_3 = evaluation_qini(prediction_results_3)\nplt.title('Qini Curve - Dataset 3',fontsize=20)\n\n\n# save into pdf:\n# plt.savefig('qini_3_n.pdf', bbox_inches='tight')","5114964a":"# The process to inverse treatment's parameter\n# Thus also inverse the treatment's correlation from negative to positive\ndf_model_inverse_1.treatment = df_model_inverse_1.treatment.replace({0: 1, 1: 0})\ndf_model_inverse_2.treatment = df_model_inverse_2.treatment.replace({0: 1, 1: 0})\ndf_model_inverse_3.treatment = df_model_inverse_3.treatment.replace({0: 1, 1: 0})","dbc047e7":"# Recalculate the treatment correlation\ndisplay(correlation_treatment(df_model_inverse_1).iloc[0,0])\ndisplay(correlation_treatment(df_model_inverse_2).iloc[0,0])\ndisplay(correlation_treatment(df_model_inverse_3).iloc[0,0])","d5612b7f":"# Add the target class feature to all three datasets\ndf_model_inverse_1, df_model_inverse_2, df_model_inverse_3 = declare_target_class(df_model_inverse_1), \\\ndeclare_target_class(df_model_inverse_2), declare_target_class(df_model_inverse_3)","91f6612b":"# Do the prediction process once more time\nprediction_results_inverse_1 = predict(df_model_inverse_1)\nprediction_results_inverse_2 = predict(df_model_inverse_2)\nprediction_results_inverse_3 = predict(df_model_inverse_3)","e1e022a4":"# Qini evaluation results for DataSet 1 with positive treatment correlation\nax, qini_inverse_1 = evaluation_qini(prediction_results_inverse_1)\nplt.title('Qini Curve - Dataset 1',fontsize=20)\n\n\n# save into pdf:\n# plt.savefig('qini_1_p.pdf', bbox_inches='tight')","171cfab8":"# qini evaluation results for DataSet 2 with positive treatment correlation\nax, qini_inverse_2 = evaluation_qini(prediction_results_inverse_2)\nplt.title('Qini Curve - Dataset 2',fontsize=20)\n\n\n# save into pdf:\n# plt.savefig('qini_2_p.pdf', bbox_inches='tight')","e29a4617":"# Qini evaluation results for DataSet 3 with positive treatment correlation\nax, qini_inverse_3 = evaluation_qini(prediction_results_inverse_3)\nplt.title('Qini Curve - Dataset 3',fontsize=20)\n\n\n# save into pdf:\n# plt.savefig('qini_3_p.pdf', bbox_inches='tight')","bc9b8772":"# 1. Setup\nFirst let's set up the environment and datasets","85f7f3ce":"# 5. Evaluating predictive performance","f854af19":"Finally we're ready to start the machine learning process:","0f820f2e":"# 2. Data Exploration","0faf8497":"Now we've the datasets ready, let's check for null data.","d6ae1a40":"In [Confusion Matrix](https:\/\/towardsdatascience.com\/understanding-confusion-matrix-a9ad42dcfd62), the True Positive and False Negative are the amount of successful predictions and the True Negative and False Positive are the amount of failed predictions. Therefore, let's generate the confusion matrices:","a87bdff8":"# 4. Machine Learning Modeling","7627172b":"Good, now all of the treatment features are negatively correlated. We will use the positive ones later at the end of this project. Next let's add the four uplift category for each datasets:","49f4174b":"Now let's evaluate the predictive performance:","f1cae319":"Now let's use the prediction results to solve the problem. As explained before, for ETP model employees are ranked by their turnover probability. Employees with the highest turnover probability will be targeted with a retention campaign (the treatment features declared before). On the other side, the ETU models are ranked by its uplift score with LGWUM's formulation.","f8c2504a":"Good, there is no null data.","6aeda58d":"Prediction results are stored in prediction_results_1, prediction_results_2, and prediction_results_3 for dataset 1, dataset 2, and dataset 3, respectively.","dd9ea608":"Good, now the treatment features are positively correlated with employee turnover. This means, if we target the employees with this treatment, it's more likely that the employee turnover rate will be increased. So it'll be wise to use this treatment carefully. Okay, now let's repeat the prediction procedure once again:","75386bc3":"Now, let's calculate the accuracy result:","32b88923":"# 6. Evaluating prescriptive performance","2ff83b8e":"This is an alternative version of Uplift Modeling VS Churn Prediction from my [previous experiment](https:\/\/www.kaggle.com\/davinwijaya\/why-you-should-start-using-uplift-modeling)\n. The difference is in this notebook I use the Logistic Regression as the Machine Learning algorithm to compare Churn prediction Model with the Uplift Modeling.","81486b69":"# 3. Data preprocessing","8b56550b":"Let's check the treatment's correlation to employee turnover:","b5ade4d3":"Secondly, let's turn the rest of the string\/object data into integer with the magical get_dummies function (One hot encoding) from Pandas package, so we can feed the data into the Random Forest Algorithm. Moreover, I add another dataframe df_model_inverse that will be useful for later:","63ce2d2e":"Wow, seems like ETP models are much better than ETU models in terms of prediction accuracy. That makes sense anyway, because ETP models only predict two possible outcomes (The employee is turnover or stay), where ETU models predict four possible outcomes (Persuadables, Sure Things, Lost Causes, and Sleeping Dogs\/Do-not-disturbs). But will ETP will also have a better performance in solving the employee turnover (prescriptive performance)? Let's find out."}}