{"cell_type":{"258af241":"code","ce55864a":"code","1c860c10":"code","2f8cdd9e":"code","e3c90275":"code","06431ad9":"code","136e52df":"code","88ecba94":"code","0aca3086":"code","bf730493":"code","6b99e202":"code","88fde245":"code","36ed6c30":"code","1868ee17":"code","8ee53429":"code","043b7187":"code","698cec87":"code","a85cf718":"code","36dd77cd":"code","baad3bca":"code","a808245a":"code","021d4c06":"code","28aff1bc":"code","62d57cc8":"code","09660039":"code","062dff25":"code","f1bed01c":"code","4f849911":"code","190ef641":"code","ce76c072":"code","fccccbf8":"code","0fc48753":"code","eb9a5e7a":"code","8e800533":"code","b04b2dd2":"code","7647a079":"code","16d5e628":"markdown","8becd6fb":"markdown","a403b705":"markdown","9af2d11f":"markdown","d6cbe26c":"markdown","5fa996f2":"markdown","562bbc00":"markdown","ccc50729":"markdown","29dab90a":"markdown","957b890e":"markdown","dc19e388":"markdown","c5ce018b":"markdown"},"source":{"258af241":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats import chisquare\nimport scipy.stats as ss\n\nimport seaborn as sns\n\nimport math\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport warnings\nwarnings.filterwarnings('ignore')","ce55864a":"df = pd.read_csv('..\/input\/mercedes-benz-greener-manufacturing\/train.csv.zip')\nprint(df)\nprint(df.dtypes)\n\n# select the float columns\ndf_float = df.select_dtypes(include=[np.float])\nprint(df_float.columns)\n# select int columns\ndf_int = df.select_dtypes(include=[np.int])\nprint(df_int.columns)\n# select object columns\ndf_int = df.select_dtypes(include=[object])\nprint(df_int.columns)","1c860c10":"df[df.duplicated(['ID'], keep=False)]","2f8cdd9e":"df.isnull().sum().sum()","e3c90275":"boxplot = df.boxplot(column=['y'])","06431ad9":"# For numerical values\ncolumnLowStd = []\nfor col in df.columns:\n    \n    if df[col].dtype=='int64':\n        \n        if df[col].std() < 0.1:\n            columnLowStd.append(col)\n            \nprint(columnLowStd)","136e52df":"# Check frequency of variables in categorical columns\ndf[\"X0\"].describe()","88ecba94":"df[\"X1\"].describe()","0aca3086":"df[\"X2\"].describe()","bf730493":"df[\"X3\"].describe()","6b99e202":"df[\"X4\"].describe()","88fde245":"df[\"X5\"].describe()","36ed6c30":"df[\"X6\"].describe()","1868ee17":"df[\"X8\"].describe()","8ee53429":"DuplicateColumns = []\n    \nfor col1 in range(df.shape[1]):\n       \n     \n    for col2 in range(col1 + 1, df.shape[1]):            \n                  \n        if df.iloc[:,col1].equals(df.iloc[:,col2]):\n            \n            DuplicateColumns.append(df.columns.values[col2])\n\nprint(DuplicateColumns)","043b7187":"data = df.copy()\ndata = data.drop(columns = ['ID'])\ndata = data.drop(columns = [\"y\"])\n\ncolumn_to_drop = DuplicateColumns + columnLowStd\ndata = data.drop(columns=column_to_drop)\n\ndata = data.applymap(str)\n\n# Cramers V for categorical correlations\ndef cramers_v(x, y):\n    x = np.array(x)\n    y = np.array(y)\n    confusion_matrix = pd.crosstab(x, y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))\n\n\ncramersv = pd.DataFrame(index=data.columns,columns=data.columns)\ncolumns = data.columns\n\nfor i in range(0,len(columns)):\n    for j in range(0,len(columns)):\n        #print(data[columns[i]].tolist())\n        u = cramers_v(data[columns[i]].tolist(),data[columns[j]].tolist())\n        cramersv.loc[columns[i],columns[j]] = u\n        \ncramersv.fillna(value=np.nan,inplace=True)","698cec87":"plt.figure(figsize=(50,50))\nsns.heatmap(cramersv)\nplt.show()","a85cf718":"upper = cramersv.where(np.triu(np.ones(cramersv.shape),k=1).astype(np.bool))\n\nprint(\"X0\",cramersv[cramersv['X0']>0.95]['X0'])\nprint(\"X1\",cramersv[cramersv['X1']>0.95]['X1'])\nprint(\"X2\",cramersv[cramersv['X2']>0.95]['X2'])\nprint(\"X3\",cramersv[cramersv['X3']>0.9]['X3'])\nprint(\"X4\",cramersv[cramersv['X4']>0.9]['X4'])\nprint(\"X5\",cramersv[cramersv['X5']>0.9]['X5'])\nprint(\"X6\",cramersv[cramersv['X6']>0.9]['X6'])\nprint(\"X8\",cramersv[cramersv['X8']>0.9]['X8'])\n\n","36dd77cd":"# Lets check for ID\ndata = df.copy()\ndata = data.drop(columns = [\"y\"])\n\n# From low Std and equal column values\ndata = data.drop(columns=column_to_drop)\ndata = data.applymap(str)\n\ncolumn_to_drop = DuplicateColumns + columnLowStd\n\ndata['ID'] = data['ID'].astype(int)\n\n\n# For correlation between categorical and numerical values\ndef correlation_ratio(categories, measurements):\n    fcat, _ = pd.factorize(categories)\n    #print(fcat)\n    #print(type(fcat[0]))\n    cat_num = np.max(fcat)+1\n    y_avg_array = np.zeros(cat_num)\n    n_array = np.zeros(cat_num)\n    measurements = np.array(measurements)\n    \n    for i in range(0,cat_num):\n        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n        #print(cat_measures)\n        n_array[i] = len(cat_measures)\n        y_avg_array[i] = np.average(cat_measures)\n        \n    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))\/np.sum(n_array)\n    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n    if numerator == 0:\n        eta = 0.0\n    else:\n        eta = np.sqrt(numerator\/denominator)\n    return eta\n\ncorrRatio = pd.DataFrame(index=['ID'],columns=data.columns)\ncolumns = data.columns\n\nfor j in range(0,len(columns)):\n    #print(df[columns[j]].tolist())\n    u = correlation_ratio(data[columns[j]].tolist(),data['ID'].tolist())\n    corrRatio.loc[:,columns[j]] = u\n    \ncorrRatio.fillna(value=np.nan,inplace=True)","baad3bca":"plt.figure(figsize=(70,20))\nsns.heatmap(corrRatio)\nplt.show()","a808245a":"plt.figure(figsize=(20,20))\nboxplot = sns.boxplot(x=\"X5\",y='ID',data=df)\nplt.show()","021d4c06":"data = df.drop(columns=['ID'])\n\n# From low Std and equal column values\ndata = data.drop(columns=column_to_drop)\n\ndata = data.applymap(str)\ndata['y'] = data['y'].astype(float)\n\n# For correlation between categorical and numerical values\ndef correlation_ratio(categories, measurements):\n    \n    fcat, _ = pd.factorize(categories)\n    cat_num = np.max(fcat)+1\n    y_avg_array = np.zeros(cat_num)\n    n_array = np.zeros(cat_num)\n    measurements = np.array(measurements)\n    \n    for i in range(0,cat_num):\n        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n        n_array[i] = len(cat_measures)\n        y_avg_array[i] = np.average(cat_measures)\n        \n    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))\/np.sum(n_array)\n    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n    if numerator == 0:\n        eta = 0.0\n    else:\n        eta = np.sqrt(numerator\/denominator)\n    return eta\n\ncorrRatio = pd.DataFrame(index=['y'],columns=data.columns)\ncolumns = data.columns\n\nfor j in range(0,len(columns)):\n    u = correlation_ratio(data[columns[j]].tolist(),data['y'].tolist())\n    corrRatio.loc[:,columns[j]] = u\n    \ncorrRatio.fillna(value=np.nan,inplace=True)\n\n","28aff1bc":"plt.figure(figsize=(70,20))\nsns.heatmap(corrRatio)\nplt.show()","62d57cc8":"upper = corrRatio.where(np.triu(np.ones(corrRatio.shape),k=1).astype(np.bool))\n# Find index of feature columns with correlation smaller than 0.1\nindex_with_low_coor = [column for column in upper.columns if any(upper[column] < 0.1)]\nprint(index_with_low_coor)","09660039":"# Between numerical values ID and y\nprint(df['ID'].corr(df['y']))","062dff25":"# Lets first train and test within the training set\n\n# Dropping X4 and X3 due to low frequency of values\n# X2('aj') and X5('t') contain variables in the test set which are not present in the training set. X5 is highly correlated with the ID\n# which can be a suitable way to represent it in the model.Similar problem with X0('av')\n\ncolumns_to_drop = columnLowStd + DuplicateColumns + ['X4','X2','X5','X0','X3'] + index_with_low_coor\n\ndata = df.copy()\n\n# Remove only the extreme outlier\ndata = data[data['y']<250]\n\ndata = data.drop(columns=columns_to_drop)\n\nX = data\nX = X.drop(columns='y')\n\ny = data['y']\ny = y.values\ny = y.reshape((len(y), 1))\n\n# split into train and test sets\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1,random_state=32)\n\ncolumns  = X_train.columns\n\n# Label encoding values\nfor c in columns:\n    if X_train[c].dtype == 'object':\n        le = LabelEncoder() \n        X_train[c] = le.fit_transform(X_train[c])\n        X_test[c] = le.transform(X_test[c])\n        \nparam_grid={\n        'n_estimators': range(200,500,100), \n        'min_samples_leaf': range(10,20,10), \n        'min_samples_split': range(10,20,10),  \n        'max_depth': range(40,60,10)       \n    }\ncv = KFold(n_splits=5, shuffle=True, random_state=40)\ngridSearch = GridSearchCV(estimator=RandomForestRegressor(), scoring='r2',cv=cv,param_grid=param_grid)\nresult = gridSearch.fit(X_train, y_train)\nprint(\"Best Score \" + str(result.best_score_)+\" with paramter \"+ str(result.best_params_))\n","f1bed01c":"# Taking values as per grid score\nmodel = RandomForestRegressor(n_estimators=result.best_params_['n_estimators'],\n                              min_samples_split=result.best_params_['min_samples_split'],\n                              min_samples_leaf=result.best_params_['min_samples_leaf'], \n                              max_depth=result.best_params_['max_depth'])\n\nmodel.fit(X_train, y_train)\n\npred = model.predict(X_test)\nprint('r2_score:',r2_score(pred,y_test))\n\n","4f849911":"plt.figure(figsize=(20,20))\nfeat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(40).plot(kind='barh')\nprint(feat_importances.nsmallest(90).index.to_list())\n\nLow_importance_columns = feat_importances.nsmallest(90).index.to_list()\n\n# Remove columns with least importance and train again\n","190ef641":"columns_to_drop = columnLowStd + DuplicateColumns + ['X4','X2','X5','X0','X3'] + index_with_low_coor+ Low_importance_columns\n\ndata = df.copy()\n\n# Remove only the extreme outlier\ndata = data[data['y']<250]\n\ndata = data.drop(columns=columns_to_drop)\n\nX = data\nX = X.drop(columns='y')\n\ny = data['y']\ny = y.values\ny = y.reshape((len(y), 1))\n\n# split into train and test sets\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1,random_state=32)\n\ncolumns  = X_train.columns\n\n# Label encoding values\nfor c in columns:\n    if X_train[c].dtype == 'object':\n        le = LabelEncoder() \n        X_train[c] = le.fit_transform(X_train[c])\n        X_test[c] = le.transform(X_test[c])\n        \nparam_grid={\n        'n_estimators': range(200,500,100), \n        'min_samples_leaf': range(10,20,10), \n        'min_samples_split': range(10,20,10),  \n        'max_depth': range(40,60,10)       \n    }\ncv = KFold(n_splits=5, shuffle=True, random_state=40)\ngridSearch = GridSearchCV(estimator=RandomForestRegressor(), scoring='r2',cv=cv,param_grid=param_grid)\nresult = gridSearch.fit(X_train, y_train)\nprint(\"Best Score \" + str(result.best_score_)+\" with paramter \"+ str(result.best_params_))","ce76c072":"# Taking values as per grid score\nmodel = RandomForestRegressor(n_estimators=result.best_params_['n_estimators'],\n                              min_samples_split=result.best_params_['min_samples_split'],\n                              min_samples_leaf=result.best_params_['min_samples_leaf'], \n                              max_depth=result.best_params_['max_depth'])\n\nmodel.fit(X_train, y_train)\n\npred = model.predict(X_test)\nprint('r2_score:',r2_score(pred,y_test))","fccccbf8":"plt.figure(figsize=(20,20))\nfeat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(40).plot(kind='barh')\n","0fc48753":"data = df.sample(frac =1,random_state=32)\n\ncolumns_to_drop = columnLowStd + DuplicateColumns + ['X4','X2','X5','X0','X3'] + index_with_low_coor+ Low_importance_columns\n\n# Remove only the extreme outlier\ndata = data[data['y']<250]\n\ndata = data.drop(columns=columns_to_drop)\n\nX = data\nX = X.drop(columns='y')\n\ny = data['y']\ny = y.values\ny = y.reshape((len(y), 1))\n\nX_train = X\ny_train = y\n\n# Label Encoding values\ncolumns = X_train.columns\nfor c in columns:\n        \n    if X_train[c].dtype == 'object':\n        le = LabelEncoder() \n        X_train[c] = le.fit_transform(X_train[c])\n       \ntrain_sizes = [100, 500, 2000, 3000, 3366]\n\ntrain_sizes, train_scores, validation_scores = learning_curve(\nestimator = RandomForestRegressor(n_estimators=result.best_params_['n_estimators'],\n                              min_samples_split=result.best_params_['min_samples_split'],\n                              min_samples_leaf=result.best_params_['min_samples_leaf'], \n                              max_depth=result.best_params_['max_depth']),\nX = X_train,y = y_train, train_sizes = train_sizes, cv = 5)\n\ntrain_scores_mean = train_scores.mean(axis = 1)\nvalidation_scores_mean = validation_scores.mean(axis =1)\n\nplt.figure(figsize=(20,20))\nplt.plot(train_sizes, train_scores_mean, label = 'Training error')\nplt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\nplt.ylabel('R2', fontsize = 14)\nplt.xlabel('Training set size', fontsize = 14)\nplt.title('Learning curves for a Random Forest Regression model', fontsize = 18, y = 1.03)\nplt.legend()\nplt.ylim(0,1)\nplt.show()","eb9a5e7a":"columns_to_drop = columnLowStd + DuplicateColumns + ['X4','X2','X5','X0','X3'] + index_with_low_coor+ Low_importance_columns\n\ndata = df.copy()\n\n# Remove only the extreme outlier\ndata = data[data['y']<250]\n\ndata = data.drop(columns=columns_to_drop)\n\nX = data\nX = X.drop(columns='y')\n\ny = data['y']\ny = y.values\ny = y.reshape((len(y), 1))\n\nX_train = X\ny_train = y\n\ndf_test = pd.read_csv('..\/input\/mercedes-benz-greener-manufacturing\/test.csv.zip')\ndata = df_test.copy()\ndata = data.drop(columns=columns_to_drop)\n\nX_test = data\n\ncolumns  = X_train.columns\n\nfor c in columns:\n    if X_train[c].dtype == 'object':\n        le = LabelEncoder() \n        X_train[c] = le.fit_transform(X_train[c])\n        X_test[c] = le.transform(X_test[c])\n        \n\nmodel = RandomForestRegressor(n_estimators=result.best_params_['n_estimators'],\n                              min_samples_split=result.best_params_['min_samples_split'],\n                              min_samples_leaf=result.best_params_['min_samples_leaf'], \n                              max_depth=result.best_params_['max_depth'])\nmodel.fit(X_train, y_train)\n\npred = model.predict(X_test)\n\n","8e800533":"output = pd.DataFrame()\n\noutput['ID'] = X_test['ID']\noutput['y']  = pred\n\noutput.to_csv(\"submissionsRandomForest.csv\",index=False)","b04b2dd2":"plt.figure(figsize=(20,20))\nboxplot = sns.boxplot(x='X0',y='y',data=df,hue=\"X314\")\nplt.show()","7647a079":"# Let me know if further improvement is needed. It's my first notebook in Kaggle. \n# I would like to receive suggestions if there is anything I missed or made a mistake","16d5e628":"- Strong correlation with ID and X5. Lets check with boxplot","8becd6fb":"# Look for mathematical correlations between categorical values and also mixed columns\nhttps:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9\n- Look for categorical features correlated with each other (CramerV)\n- Look for features correlated with ID (CorrelationRatio)\n- Look for features correlated with y  (CorrelatioRatio)","a403b705":"If we observe closely, cells X0,X1,X2 have a lot of correlations with the integer columns. Lets see individually","9af2d11f":"- Lets look at boxplot of target variable, variable with highest importance and it's associated categorical variable","d6cbe26c":"It can be assumed that the binary features are just encodings of categorical columns or the features are just strongly correlated or dependant on each other. In the notebook \"My frustrated approach\"(https:\/\/www.kaggle.com\/robertoruiz\/my-frustrated-approach) by Cro - Magnon where the suggestion was variables X0 to X8 are processes or configurations and the integer binary columns are car models. It could be that variables X0 and X8 are custom features of the cars and the remaining binary columns show whether the car has the feature or not. So we can deduce our results in two ways -:\n- 1) Which car takes the most inspection time (Binary column) or\n- 2) Which sub feature in a feature columns takes the most time(Categorical columns)\n","5fa996f2":"- It could mean the testing for feature X5 in an ordered fashion. Now let's check for y","562bbc00":"-  Check for inconsistencies in the data\n - 1) Check for Duplicates rows\n - 2) Check for missing data\n - 3) Check for outliers in 'y'\n - 4) Check for categorical columns with low variance\n - 5) Check for columns with duplicate values","ccc50729":"- Among the categorical variables,we can see from the above a high correlation with X0 and its dependant features. This shows a strong correlation between X0 and its dependant features. Hence, we can assume that the dependant features would be enough to show impact of X0\n- Now lets implement the features in a Random Forest Regressor\n","29dab90a":"- A high r2 score score during grid search did not correspond to a similar value in the test set. Lets plot the training curve and check for overfitting\n- https:\/\/www.dataquest.io\/blog\/learning-curves-machine-learning\/","957b890e":"Eliminate columns with low feature importance to check if score improves","dc19e388":"Above graph shows that the validation error decreases with increasing training size. The mean of the validation error seems to be close to 0.5 but decreases with increasing sample size. It could be due to the freqency of the categorical variables or any eliminations of columns done above. Let's try on the test set.","c5ce018b":"Rather than finding out which feature is important, another way of looking at it is, which sub feature is important. So from feature importance, we know X314 is important but at the same time we know that X314 is highly correlated with X0. So we can deduce the following\n\n- 1) Car or feature X314 plays a major role on time spent on test bench\n- 2) Or car or feature not having subfeature y could play a major role on the time spent on the test bench"}}