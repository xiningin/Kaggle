{"cell_type":{"978ac727":"code","dd497d05":"code","8b5173ec":"code","5f69743a":"code","339a0ce8":"code","04e895e8":"code","3a269b62":"code","2c3b97f4":"code","ebe1abe0":"code","303d6d55":"code","38ac33e1":"code","1bef9970":"code","755cb997":"code","e99b3180":"code","b749f7da":"markdown","146edaea":"markdown","1afb9b7e":"markdown","cb06357e":"markdown","361c4eec":"markdown","113942fa":"markdown","7518482a":"markdown","2b88be0e":"markdown","356edcd8":"markdown","9e8d9c49":"markdown"},"source":{"978ac727":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n","dd497d05":"df=pd.read_csv(\"..\/input\/classified-data\/Classified Data\",index_col=0)\ndf.head()\n#The columns in the data mean nothing so it is better to use K mearest Neighbors algorithm","8b5173ec":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler() #here we create instance of Standart Scaler","5f69743a":"scaler.fit(df.drop(\"TARGET CLASS\",axis=1)) \n# here we standartize our features in our dataset apart from the target column","339a0ce8":"#After fitting with our data we will transform our data according to Standart Scaler\nscaled_features=scaler.transform(df.drop(\"TARGET CLASS\",axis=1))\nscaled_features \n# here we get standartized features of our dataset as numpy arrays","04e895e8":"df_features=pd.DataFrame(scaled_features,columns=df.columns[:-1])\ndf_features.head()\n# Now all of the features has been standartized and is ready to be put into machine learning algorithm","3a269b62":"#We will split our data before training the algorith:\nX=df_features\ny=df[\"TARGET CLASS\"]\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=101)","2c3b97f4":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train,y_train) # the algorithm fits with our data","ebe1abe0":"predictions=knn.predict(X_test)\npredictions","303d6d55":"from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test,predictions))\n\n#TP=151 : true positive\n#FN=8   : false negative\n#FP=15  :false positive \n#TN=126 : true negative\n#The errors are not too high and absorable\n","38ac33e1":"print(classification_report(y_test,predictions))\n\n#The precision and accuracy precentages are over %90, it is very good with k=1","1bef9970":"#Although k=1 is very good for our predcitions, we will check whether there is better k value or not\nerror_rate=list()\n#here we iterate meny different k values and plot their error rates \n#and discover which one is better than others and has the lowest error rate\nfor i in range(1,40):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    prediction_i=knn.predict(X_test)\n    error_rate.append(np.mean(prediction_i != y_test))","755cb997":"# Now we will plot the prediction error rates of different k values\nplt.figure(figsize=(15,10))\nplt.plot(range(1,40),error_rate, color=\"blue\", linestyle=\"--\",marker=\"o\",markerfacecolor=\"red\",markersize=10)\nplt.title(\"Error Rate vs K Value\")\nplt.xlabel=\"K Value\"\nplt.ylabel(\"Error Rate\")\n","e99b3180":"knn=KNeighborsClassifier(n_neighbors=34)\nknn.fit(X_train, y_train)\npredictions=knn.predict(X_test)\nprint(classification_report(y_test,predictions))\nprint(\"\\n\")\nprint(confusion_matrix(y_test,predictions))","b749f7da":"According to figure, we start with higher error rates with lower k values\nIt seems 34 or 36 or 38 as k value give the lowest error rate\nNow we will choose one of them and evaluate its performance again","146edaea":"The next step is to transform numpy arrays into pandas dataframe","1afb9b7e":"classification_report(y_true, y_pred, *, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False, zero_division='warn')\n    Build a text report showing the main classification metrics\n    \n\nconfusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)\n    Compute confusion matrix to evaluate the accuracy of a classification.\n","cb06357e":"The next step is to evaluate how good our model performs","361c4eec":"*K Nearest Neighbors is a classification algorithm\n\n*K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). \n\n*Firstly we store all the data\n*Secondly we calculate the data from x to all points in our data set, x indicationg particular new data point\n*Then we sort the points near data by increasing distance from x\n*Finally we predict the majority label of K, which is number and represent closest points\n\n*Choosing a K will effect what class a new point is assigned to:\nif we choose k=3, then the algorithm looks at the three nearest neighbors to this new point\nif we set k=6, then the algorithm looks at the six nearest neighbors to this new point and decide according to the majority of these 6 neighbors.\nIf we set larger k values,we get a cleaner cutoff at the expense of mislabelling some points\n","113942fa":"Now, with new k value, the algorithm has % 96 accuracy and the confusion matrix results are also better than before","7518482a":"Pros of K Nearest Neighbors:\n\n    1. It is versy simple to implement\n    2. It works with any number of classes\n    3. It is easy to add new data\n    4. It requires fewer parameters as k,which is how many nearest points we want to look at, as distance metric,whatever distance metric we will use, which means how we are defining matematically the distance between our new test point and the old training point","2b88be0e":"First of all we have to standartize all of the columns in our data set\nbecause the scale of the observation does matter when we use KNearest Neighbors Algorithm.\n\nAny column with large scale will make a larger effect than others","356edcd8":"KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n\nParameters\n |  ----------\n |  n_neighbors : int, default=5\n |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n |  \n |  weights : {'uniform', 'distance'} or callable, default='uniform'\n |      weight function used in prediction.  Possible values:\n |  \n |      - 'uniform' : uniform weights.  All points in each neighborhood\n |        are weighted equally.\n |      - 'distance' : weight points by the inverse of their distance.\n |        in this case, closer neighbors of a query point will have a\n |        greater influence than neighbors which are further away.\n |      - [callable] : a user-defined function which accepts an\n |        array of distances, and returns an array of the same shape\n |        containing the weights.\n |  \n |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n |      Algorithm used to compute the nearest neighbors:\n |  \n |      - 'ball_tree' will use :class:`BallTree`\n |      - 'kd_tree' will use :class:`KDTree`\n |      - 'brute' will use a brute-force search.\n |      - 'auto' will attempt to decide the most appropriate algorithm\n |        based on the values passed to :meth:`fit` method.","9e8d9c49":"Cons of K Nearest Neighbors:\n\n    1. There is high prediction cost which is worse for large datasets\n    2. It is not good for high dimensional data, as our features increase we get higher dimensions for our data, so it will be very difficult to measure distance between various dimensions\n    3. Categorical Variables do not work very well with this algorithm"}}