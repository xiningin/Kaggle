{"cell_type":{"4ec2c9b9":"code","6bd6e47b":"code","8248dbc3":"code","d7332568":"code","9f675044":"code","fe96d453":"code","d1165db7":"code","c24a30c5":"code","d29437d3":"code","b4c3be85":"code","afeec3fb":"code","5ef2a0f8":"code","96b6bd21":"code","1b10ad41":"code","8262129c":"code","1faa4f3e":"code","91a4ba84":"code","d1321899":"code","b99d09d9":"code","5b0abc03":"code","70210a8c":"code","8c2f24d4":"code","2495ecf2":"code","c1c2dcbf":"code","5ff5973d":"code","a7a6bebd":"code","f41f5207":"code","1532f0fc":"code","7062e215":"code","73f139d7":"code","d630689a":"code","ea84d02e":"code","decb4984":"code","31358562":"code","e2b4865c":"code","9ca8ead2":"code","83fb6f2f":"code","e3ed69de":"code","61410393":"code","cda6552b":"code","5861ff5d":"code","79f04f1a":"code","677bdfe3":"code","baaf017b":"code","1fd74ab3":"code","bdf31f16":"code","8657e899":"code","dce9e3d2":"markdown","d799ea80":"markdown","e09d1ccc":"markdown","0a069a2e":"markdown","1f32d69b":"markdown","1a39b703":"markdown"},"source":{"4ec2c9b9":"import pandas as pd\nimport numpy as np\nimport re","6bd6e47b":"# Glossary of Comic Terms\n# https:\/\/comics.ha.com\/c\/ref\/glossary.zx","8248dbc3":"data = pd.read_parquet('comic_data.parquet') # change to your particular location","d7332568":"data.info(memory_usage='deep')","9f675044":"# extract titles' and issues' unique ids\ndata['title_id'] = data.copy().title_link.str.extract(pat=r'\/series.(\\d+)\/\\d.+')\ndata['issue_id'] = data.copy().issue_link.str.extract(pat=r'\/series\/\\d+\/(\\d+)\/.+')","fe96d453":"# keep relevant columns for analysis\ndf = data[['pub_name', 'title', 'title_id', 'issue', 'variant_of', 'volume', 'cover_date', \n           'years', 'cover_price', 'current_value', 'searched', \n          'owned', 'issues_total', 'est_print_run',  \n           'pub_titles_total', 'pub_issues_total',\n           'contributors_names', 'contributors_roles', 'characters', 'synopsis']]","d1165db7":"# turn prices into numerical values\ndf.loc[:, 'cover_price'] = df.copy().loc[:, 'cover_price'].str.replace('[$,]', '', regex=True)\ndf.loc[:, 'cover_price'] = df.copy().loc[:, 'cover_price'].str.replace('Free', '0', regex=False)\ndf.loc[:, 'cover_price'] = df.copy().loc[:, 'cover_price'].apply(eval)","c24a30c5":"df.loc[:, 'current_value'] = df.copy().loc[:, 'current_value'].str.replace('[$,]', '', regex=True)\ndf.loc[:, 'current_value'] = df.copy().loc[:, 'current_value'].str.replace('Free', '0', regex=False)\ndf.loc[:, 'current_value'] = df.copy().loc[:, 'current_value'].apply(eval)","d29437d3":"# Many issues with cover price 0 have high current value so we keep them\ndf[df.cover_price == 0].sort_values('current_value', ascending=False).head(2)","b4c3be85":"# make a copy before doing heavy changes\ndf1 = df.copy()","afeec3fb":"# get the print number\nprints = df1.issue.str.extract('.*(\\d{1,2})[a-zA-Z]{2}[\\s\\-][pP]rint')\ndf1['print_nr'] = prints.fillna(1).astype(int)\n# extract volume and issue information from issue of the type e.g. 'V5-14' or 'V9 6' or 'V12#6'\nvols = (df1.issue.str.extractall('.*[Vv][oO]?[lL]?(\\d+)[\\s\\-#]+(\\d+)')\n            .droplevel(1).rename({0: 'vol', 1: 'issue'}, axis=1))\n# extract volume and issue information from issue of the type e.g. 'B-14' or 'C 6'\nlett = df1.issue.str.extractall('([A-Z])[#\\s\\-](\\d+)\\s?.*').droplevel(1).rename({0: 'vol', 1: 'issue'}, axis=1)","5ef2a0f8":"# Find all ashcan issues\nidx_ash = df1.issue.str.contains('[Aa]shcan')\n# Find all non-numbered and ashcan issues\nidx_nn = df1.issue.str.contains('(?:\\s+|^)nn-?(?:\\s*|$)')\ndf1.loc[idx_nn, 'volume'] = 'one-shot'\n# Find all previews\nidx_prev = df1.issue.str.contains('[Pp]review')\n# find all issues that contain also the volume information\nvols_idx = df1.issue.str.contains('.*[Vv][oO]?[lL]?#?\\d+[\\s\\-#]+\\d+')\n# find issues that are properly numbered\nnumb_idx = df1.issue.str.contains('.*#\\s*-?\\d+\\D*.*')\n# find issues that are numbered as e.g. B-5 etc.\nlett_idx = df1.issue.str.contains('^[A-Z][#\\s\\-]\\d+\\s?.*')\n# find issues that are numbered by single capital letters\ncaplet_idx = df1.issue.str.contains('^[A-Z]$')\n# find issues that are numbered by single lowercase letters\nlowlet_idx = df1.issue.str.contains('^[a-z]$')\n# find issues from free comic book day\nfcbd_idx = df1.issue.str.contains('fcbd|free comic')\ndf1.loc[fcbd_idx, 'volume'] = 'fcbd'\n\n# Extract issue numbers by the ones that are numbered in the conventional way\nnumbered = df1.issue.str.extract(r'#\\s*(\\d+)\\D*')\n\n# give numberical values to non-numbered special-type issues. This is just a convention\n# ashcan --> -1\nnumbered.loc[idx_ash] = str(0)\n# nn --> 1\nnumbered.loc[idx_nn] = str(1)\n# preview --> 0.5\nnumbered.loc[idx_prev] = str(0.5)\n# free comic book day (fcbd) --> 1\nnumbered.loc[fcbd_idx] = str(1)\n# single capital letter numbering to int numbers\nnumbered.loc[caplet_idx] = df1[caplet_idx].issue.apply(lambda x: str(ord(x) - 64)).values.reshape(-1, 1)\n# single lower letter numbering to int numbers\nnumbered.loc[lowlet_idx] = df1[lowlet_idx].issue.apply(lambda x: str(ord(x) - 96)).values.reshape(-1, 1)\n\n# add volume and issue info extracted from issue column\nnumbered.loc[vols_idx, 0] = vols.issue \ndf1.loc[vols_idx, 'volume'] = vols.vol\nnumbered.loc[lett_idx, 0] = lett.issue\ndf1.loc[lett_idx, 'volume'] = lett.vol.apply(lambda x: str(ord(x) - 64))\n\n# create new column with the issues numbers\ndf1.insert(4, 'issue_nr', pd.to_numeric(numbered[0]))","96b6bd21":"# find all the above\nregex = r'(?:\\s+|^)nn-?(?:\\s*|$)|.*#\\s*-?\\d+\\D*.*|[Aa]shcan|[Pp]review|.*[Vv]\\d+\\s?-?\\s?\\d+|[A-Z][\\s-]\\d+\\s?.*|^[A-Z]$|fcbd|free comic|^[a-z]$'\nidx_good = (df1.issue.str.contains(regex))","1b10ad41":"# All that remained that we will drop since issue number is considered essential\nprint(df1[~idx_good].shape)\nprint(df1[~idx_good].current_value.sum())","8262129c":"df2 = df1.dropna(subset=['issue_nr'])\n# drop issues with issue_nr > 10000 - considered outliers\ndf2 = df2[~(df2.issue_nr > 10000)]","1faa4f3e":"# All none values for volume can be considered volume 1\ndf2.volume.isna().sum()","91a4ba84":"# take care of volume numbers formatted as e.g. 'Vol. 5'\nvol_nr = df2.volume.str.extract('.*[vV][oO][lL][.\\s\\-]+(\\d+)')\nvol_nr_idx = vol_nr.notna()[0]\n\ndf2.loc[vol_nr_idx, 'volume'] =  vol_nr","d1321899":"df2.volume.fillna('0', inplace=True)","b99d09d9":"# Volume of type 'one-shot'\noneshot_idx = df2.volume.str.contains('one[\\s\\-=]+shot', flags=re.IGNORECASE)\ndf2.loc[oneshot_idx, 'volume'] = 'one-shot'\n# Volume of type trade paper-back (TPB) or soft-cover (SC)\ntpb_idx = df2.volume.str.contains('TRPB|TPB|TP|paperback|sc|soft', flags=re.IGNORECASE)\ndf2.loc[tpb_idx, 'volume'] = 'TPB\/SC'\n# Volume of type Hard Cover (HC)\nhc_idx = df2.volume.str.contains('HC|hard', flags=re.IGNORECASE)\ndf2.loc[hc_idx, 'volume'] = 'HC'\n# Volume of type graphic novel (GN)\ngn_idx = df2.volume.str.contains('^gn$|^ogn$', flags=re.IGNORECASE)\ndf2.loc[gn_idx, 'volume'] = 'GN'\n# Volume of type mini-series\/ maxi-series \/limited-series which are ofter used interchangeably (https:\/\/en.wikipedia.org\/wiki\/Limited_series_(comics))\nmini_idx = df2.volume.str.contains('mini|maxi|limit', flags=re.IGNORECASE)\ndf2.loc[mini_idx, 'volume'] = 'limited-series'\n# Volume of type magazine\nmag_idx = df2.volume.str.contains('magazine', flags=re.IGNORECASE)\ndf2.loc[mag_idx, 'volume'] = 'magazine'\n# Volume of type fanzine\nfan_idx = df2.volume.str.contains('fanzine', flags=re.IGNORECASE)\ndf2.loc[fan_idx, 'volume'] = 'fanzine'\n# Volume for fcbd comics\nFCBD_idx = df2.volume.str.contains('fcbd', flags=re.IGNORECASE)\ndf2.loc[FCBD_idx, 'volume'] = 'fcbd'\n# Volume for four-colour comics\nfour_idx = df2.volume.str.contains('(?=.*[fF]our)(?=.*[cC]olor|.*[cC]olour)', flags=re.IGNORECASE)\ndf2.loc[four_idx, 'volume'] = 'four-color'","5b0abc03":"volume_cats = ['limited-series', 'one-shot', 'magazine', 'TPB\/SC', 'HC', 'GN', 'four-color', \n               'Prepack', 'fcbd', 'fanzine']\nvolume_nums = list(range(21))\nvolume_lst = volume_cats + [str(n) for n in volume_nums]","70210a8c":"other_idx = ~df2.volume.isin(volume_lst)","8c2f24d4":"# get rid of those, in most cases these are not comic books\ndf3 = df2[~other_idx]","2495ecf2":"# use info in variant to fill volume information\nvariant = df3[df3.variant_of.notna()]","c1c2dcbf":"# find variant number in the form 'Soft Cover or TPB # 213'\ntpb_var_idx = variant.variant_of.str.contains('TRPB|TPB|TP|paperback|sc|soft', flags=re.IGNORECASE)\n# extract issue number\ntpb_var_num = variant[tpb_var_idx].variant_of.str.extract(r'.*#?\\s*(\\d+)\\D*')\n\n# find variant number in the form 'Hard Cover # 213'\nhc_var_idx = variant.variant_of.str.contains('hard|HC', flags=re.IGNORECASE)\n# extract issue number\nhc_var_num = variant[hc_var_idx].variant_of.str.extract(r'.*#?\\s*(\\d+)\\D*')","5ff5973d":"var_tpb_index = tpb_var_idx[(tpb_var_idx == True)].index\ndf3.loc[var_tpb_index, 'volume'] = 'TPB\/SC'\n\nvar_hc_index = tpb_var_idx[(hc_var_idx == True)].index\ndf3.loc[var_hc_index, 'volume'] = 'HC'","a7a6bebd":"# Make new column where variant is bool True or False\nis_variant = df3.variant_of.notna()","f41f5207":"df3.loc[:, 'variant'] = is_variant.copy()","1532f0fc":"# pd.options.display.max_colwidth = 1000\n# pd.options.display.max_rows = 500","7062e215":"df3.synopsis.notna().value_counts()","73f139d7":"df3[df3.synopsis.notna()].synopsis.sample(1)","d630689a":"incentive_idx = df3.synopsis.str.contains('incentive', flags=re.IGNORECASE).fillna(False)","ea84d02e":"limited_idx = df3.synopsis.str.contains('(?=.*limited)(?=.*edition|.*cover|.*variant|.*issue|.*copies|.*copy)', flags=re.IGNORECASE).fillna(False)","decb4984":"limit_copies = df3[limited_idx].synopsis.str.extract(r'.*\\s(\\d,?\\d+) copies.*|.*limited to (\\d,?\\d+).*', flags=re.IGNORECASE)","31358562":"limit_copies = limit_copies[0].fillna(limit_copies[1].fillna(limit_copies[2]))","e2b4865c":"# deluxe edition (< limited)\ndeluxe_idx = df3.synopsis.str.contains('(?=.*deluxe)(?=.*edition)', flags=re.IGNORECASE).fillna(False)","9ca8ead2":"# create a column 'special' to indicate limited\/deluxe edition etc\ndf3.loc[:, 'special'] = 'not'","83fb6f2f":"df3.loc[deluxe_idx, 'special'] = 'deluxe'\ndf3.loc[limited_idx, 'special'] = 'limited'\ndf3.loc[incentive_idx, 'special'] = 'incentive'","e3ed69de":"df3.special.value_counts()","61410393":"# The number of copies for the limited editions wherever it could be obtained\nlimit_copies = limit_copies.str.replace(',', '').astype(float).dropna()","cda6552b":"def get_events(char_list):\n    \n    if char_list == 'no characters assigned':\n        return 'nothing'\n    else:\n        chars = eval(char_list)\n        events = []\n        for char in chars:\n            if char[-1] != '':\n                events.append(char[-1])\n            else:\n                continue\n        if events == []:\n            return 'nothing'\n        else:\n            return str(events)\n            ","5861ff5d":"events = df3.characters.apply(get_event)","79f04f1a":"df3.loc[:, 'first_appear_event'] = events.str.contains('First appearance', flags=re.IGNORECASE)\ndf3.loc[:, 'death_event'] = events.str.contains('Death', flags=re.IGNORECASE)\ndf3.loc[:, 'origin_event'] = events.str.contains('origin', flags=re.IGNORECASE)","677bdfe3":"df4 = df3.copy()","baaf017b":"df4.info()","1fd74ab3":"df4 = df4[['pub_name', 'title', 'title_id', 'issue_nr', 'variant', 'volume', 'print_nr', \n          'date', 'cover_price', 'current_value', 'special', 'searched', 'owned',\n         'first_appear_event', 'death_event', 'origin_event', 'issues_total',\n         'pub_titles_total', 'pub_issues_total']]","bdf31f16":"# take a final look before saving\ndf4.info(memory_usage='deep')","8657e899":"df4.to_csv('data_clean.csv') # change to your particular location","dce9e3d2":"### Prepare final dataset","d799ea80":"### `volume` column\n\nLet's extract important info and clean it","e09d1ccc":"#### Get first appearance and maybe other special events from `characters` column ","0a069a2e":"### `synopsis` column\n\nTry to find other information from the synopsis of each issue e.g. if it is an incentive cover variant. Additional ideas for information extraction from this column are very welcome!","1f32d69b":"### `issue` column\n\nThere are several types of issues as indicated by their numbering and many unconventional numberings:\n\n* Simple ascending numbering # 1-...\n\n* Issues with numbering of the format: # (number)(variant-cover\/ special ed.\/ convention ed. etc.): Normally these issues should be indicated as \"variant of\" (issue number) in the `variant_of` column\n\n* 'Ashcan' Issues which are traditionally used to promote a new series (see [wiki](https:\/\/en.wikipedia.org\/wiki\/Ashcan_comic)). Ashcan comics can be quite rare and valuable, especially from the [Golden Age](https:\/\/en.wikipedia.org\/wiki\/Golden_Age_of_Comic_Books) (30s-50s)\n\n* Issues with `nn` meaning 'non-numbered'. Typically 0th issues or one-shots\n\n* Of \"Vol ** # **\" format (where ** indicated a number). i.e. two enumerations are indicated, one for volume, one for issue number\n\n* Instead of issue number a date is given e.g. \"Spring 2007\" or simply \"1989\"\n\n* Numbering which includes letters e.g. \"C-2\" or \"x\"\n\n* Just the name of the protagonist or in general something explanatory e.g. \"Batwoman\"\n\n* The print run is indicated e.g. '# 2 - 2nd print'\n","1a39b703":"### `variant_of` column\n\nTurn column into numerical containing the number of the issue this issue is a variant of"}}