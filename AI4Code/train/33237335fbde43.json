{"cell_type":{"92570758":"code","1590208b":"code","ce5de9d5":"code","906728ba":"code","73ad38f8":"code","d92fd58b":"code","d9b01ebc":"code","d94630c8":"code","5fe5d891":"code","65dd6cdc":"code","8f04ea03":"code","016a887c":"code","169e6896":"code","8e4b244c":"code","1992c4b9":"code","391824ac":"code","33897cb7":"code","600f1195":"code","d2382076":"code","1c8f169e":"code","209bae04":"code","93cd8d75":"code","48a6cc4c":"code","c99b938a":"code","ac66aa74":"code","8e0ad302":"code","52ddb301":"code","f20c59dc":"code","33a66171":"code","d0062d4f":"code","bd1d8089":"code","cbdf10c7":"code","63bf463b":"code","df86ab1d":"code","5f412c7b":"code","d3178472":"code","61c3bd8a":"code","d51805af":"code","312e50fd":"code","37e239af":"code","e0fd6673":"code","cb6c4473":"code","d3c2361c":"code","f90966eb":"code","b28881f7":"code","78adec45":"code","cf326095":"code","e3988742":"code","774c181a":"code","47bc4c74":"code","462657d9":"code","d1649cb8":"code","0e353156":"code","22bfd547":"code","58573a6b":"code","f59b6bf8":"code","ff78249d":"code","1a1873fb":"code","9e37f828":"code","81c92934":"code","36129fc2":"markdown","6726f60f":"markdown","e56de940":"markdown","92247e8a":"markdown","6230ffca":"markdown","ca496988":"markdown","a6f0c9c7":"markdown","daa0219a":"markdown","4a7eed9d":"markdown","016a6a0f":"markdown","35ad9482":"markdown","a806b1a8":"markdown","f6f1febc":"markdown","b3f0a6ca":"markdown","42da4a41":"markdown","152639f2":"markdown","1a699c40":"markdown","22674706":"markdown","58501f92":"markdown","fda7ff40":"markdown","76dabbe4":"markdown","abd114aa":"markdown","d0a692d0":"markdown","7d1de024":"markdown","0ff9c12e":"markdown","f01e896f":"markdown","97c649b7":"markdown","19952334":"markdown","bf4a3d04":"markdown","9b9aed43":"markdown","1be4f13e":"markdown","b6311ff6":"markdown"},"source":{"92570758":"import numpy as np\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport matplotlib.pyplot as plt\n\nimport xml.etree.ElementTree as ET\nfrom os import listdir, mkdir, getcwd, chdir\nfrom os.path import join\n\nimport time\nimport json\nimport pickle","1590208b":"if 'dataset' not in listdir():\n  mkdir('dataset')\n\nworking_directory = getcwd()\nchdir(join(working_directory, 'dataset'))\n!wget https:\/\/www.govinfo.gov\/bulkdata\/ECFR\/title-14\/ECFR-title14.xml\nchdir(working_directory)","ce5de9d5":"# set the random seed for the train\/valid\/test\/trash split\nnp.random.seed(20210228)\n\n# access the unprocessed xml file\nfilepath = join('dataset', 'ECFR-title14.xml') # file already fetched from https:\/\/www.govinfo.gov\/bulkdata\/ECFR\/title-14\/ECFR-title14.xml\ntree = ET.parse(filepath) # set up a xml tree by parsing the file\nroot = tree.getroot() # get a reference to the tree root\n\n# create new files for each partition\ntrain = open(join('dataset','ECFR-title14-train.txt'), 'w', encoding='utf-8')\nvalid = open(join('dataset','ECFR-title14-valid.txt'), 'w', encoding='utf-8')\ntest = open(join('dataset','ECFR-title14-test.txt'), 'w', encoding='utf-8')\ntrash = open(join('dataset', 'trash.txt'), 'w', encoding='utf-8')\n\n# set up train\/valid\/test split scheme\npartitions = np.array(['train', 'valid', 'test', 'trash'])\nnext_partition = partitions[0]\nsplit_proportions = np.array([.98, .01, .01, 0]) # train, valid, test, trash\n\n# iterate over each element of the xml tree\nfor element in root.iter():\n    # stop when DoT part is reached\n    if element.tag == 'DIV1':\n        if element.attrib['N'] == '4':\n            break \n        # if reached a new section\n    if element.tag == 'DIV8':\n        if next_partition == 'train':\n            [train.write(line) for line in element.itertext() if line[0] != '\\n']\n        elif next_partition == 'valid':\n            [valid.write(line) for line in element.itertext() if line[0] != '\\n']\n        elif next_partition == 'test':\n            [test.write(line) for line in element.itertext() if line[0] != '\\n']\n        elif next_partition == 'trash':\n            [trash.write(line) for line in element.itertext() if line[0] != '\\n']\n        next_partition = np.random.choice(partitions, p = split_proportions)\n\n# close the newly created files\ntrain.close()\nvalid.close()\ntest.close()\ntrash.close()","906728ba":"def make_dataset(partition, tokenizer, sequence_length = None, batch_size = 16, dummy = False, batching_mode = 'regular'):\n    \n    assert batching_mode in ['regular', 'recurrent_encoding', 'recurrent_embedding', 'stateful_encoding']\n\n    # get dataset as list of sequences of tokens\n    with open(join('dataset', 'ECFR-title14-' + partition + '.txt'), encoding='utf-8') as file:\n        sequences = tokenizer.texts_to_sequences(file)\n    \n    # make single stream of tokens for the entire dataset\n    dataset = [] \n    for paragraph in sequences:\n        dataset += paragraph\n    \n    # get x and y from the full sequence\n    dataset = np.array(dataset)\n    y = tf.data.Dataset.from_tensor_slices(dataset[1:])\n    \n    # in case of a dummy dataset (x is all null)\n    if dummy == True:  # empty inputs in order to test setup\n        x = tf.data.Dataset.from_tensor_slices(np.zeros_like(dataset[:-1]))\n    else:\n        x = tf.data.Dataset.from_tensor_slices(dataset[:-1])\n    \n    # zip inputs and labels into tuples\n    if batching_mode == 'regular': # dataset will not be batched into sequences (for simple NN)\n        dataset = tf.data.Dataset.zip((x,y))\n        dataset = dataset.batch(batch_size)\n    elif batching_mode == 'recurrent_encoding': # will be batched in individual entries, then sequences, then batches\n        dataset = tf.data.Dataset.zip((x,y))\n        dataset = dataset.batch(1)\n        dataset = dataset.batch(sequence_length, drop_remainder = True)\n        dataset = dataset.batch(batch_size, drop_remainder = True)\n    elif batching_mode == 'recurrent_embedding': # will be batched into sequences, then batches\n        dataset = tf.data.Dataset.zip((x,y))\n        dataset = dataset.batch(sequence_length, drop_remainder = True)\n        dataset = dataset.batch(batch_size, drop_remainder = True)\n    elif batching_mode == 'stateful_encoding': # will be batch so there is a stream of characters from a few longer sequences, compatible with stateful RNN\n        \n        partition_length = len(dataset)\n\n        # batch_size = number of RNN instances, each seeing a piece from a different paragraph\n        num_paragraphs = batch_size # different name for the same thing\n        paragraph_length = partition_length \/\/ num_paragraphs # characters in each of the batch_size paragraphs\n        # sequence_length = number of characters in a sequence = number of repetitions of the RNN stack per instance\n        sequences_per_paragraph = paragraph_length \/\/ sequence_length # number of steps to finish paragraph = steps per epoch\n        \n        def map_fn(paragraph):\n            return tf.data.Dataset.from_tensor_slices(paragraph).batch(sequence_length, drop_remainder=True)\n        \n        x = x.batch(1)\n        x = x.batch(paragraph_length, drop_remainder = True)\n        x = x.interleave(map_fn, cycle_length = num_paragraphs)\n        x = x.batch(batch_size, drop_remainder = True)\n        \n        y = y.batch(1)\n        y = y.batch(paragraph_length, drop_remainder = True)\n        y = y.interleave(map_fn, cycle_length = num_paragraphs)\n        y = y.batch(batch_size, drop_remainder = True)\n        \n        dataset = tf.data.Dataset.zip((x, y))\n      \n    dataset = dataset.prefetch(1)\n    \n    return dataset","73ad38f8":"tokenizer = keras.preprocessing.text.Tokenizer(char_level = True, lower = False, filters = '')\n\nfor partition in ['train', 'valid', 'test']:\n    file = open(join('dataset', 'ECFR-title14-'+ partition + '.txt'), encoding = 'utf-8')\n    tokenizer.fit_on_texts(file)\n    file.close()","d92fd58b":"sequence_length = 48\nbatch_size = 4\n\ntrain = make_dataset('train', tokenizer, sequence_length = sequence_length, \n                     batch_size = batch_size, batching_mode = 'stateful_encoding')\n\nfor item in train.take(6):\n    for sequence in range(len(item[0].numpy())):\n        text = tokenizer.sequences_to_texts([item[0].numpy()[sequence].squeeze()])[0][::2]\n        text = text.replace('\\n', '')\n        print(text)\n    print()","d9b01ebc":"def print_performance(model, log_dir=None):\n    print(\"Performance on train set:\")\n    perf_train = model.evaluate(train)\n\n    print(\"Performance on valid set:\")\n    perf_valid = model.evaluate(valid)\n\n    print(\"Performance on test set:\")\n    perf_test = model.evaluate(test)\n    \n    if log_dir == None:\n        directory = \"-\"\n    else:\n        directory = log_dir.rpartition('\\\\')[2]\n\n    print(\"Performance log:\")\n    log = '{} | {} | {} | {} {} {} | {}% {}% {}%'.format(directory, model.name, model.count_params(),\n                                               round(perf_train[0],3), round(perf_valid[0],3), round(perf_test[0],3),\n                                               round(perf_train[1]*100,1),round(perf_valid[1]*100,1),round(perf_test[1]*100,1))\n    print(log)\n    \n    return None\n    \ndef print_history(history):    \n    plt.figure(figsize=(15,3))\n\n    plt.subplot(1,2,1)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('loss: categorical cross-entropy')\n\n    plt.subplot(1,2,2)\n    plt.plot(history.history['sparse_categorical_accuracy'])\n    plt.plot(history.history['val_sparse_categorical_accuracy'])\n    plt.title('metric: sparse categorical accuracy')\n    \n    return None\n\ndef get_tensorboard_logdir():\n    run_id = time.strftime(\"run__%Y-%m-%d__%H-%M-%S\")\n    return join(getcwd(), 'tb_logs', run_id)","d94630c8":"class LN_GRU_Cell(keras.layers.Layer):\n    def __init__(self, units, activation=\"tanh\", dropout=0, recurrent_dropout=0, **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.dropout = dropout\n        self.recurrent_dropout = recurrent_dropout\n        self.state_size = units\n        self.output_size = units\n        self.GRU_cell = keras.layers.GRUCell(units, dropout=dropout, recurrent_dropout=recurrent_dropout, activation=None)\n        self.layer_norm = keras.layers.LayerNormalization()\n        self.activation = keras.activations.get(activation)\n    def call(self, inputs, states):\n        outputs, new_states = self.GRU_cell(inputs, states)\n        norm_outputs = self.activation(self.layer_norm(outputs))\n        return norm_outputs, [new_states]\n    def get_config(self):\n        base_config = super().get_config()\n        custom_config = {'units':self.units,\n                         'dropout':self.dropout,\n                         'recurrent_dropout':self.recurrent_dropout,\n                         'activation':self.activation}\n        return {**base_config, **custom_config}\n    \nclass LN_LSTM_Cell(keras.layers.Layer):\n    def __init__(self, units, activation=\"tanh\", dropout=0, recurrent_dropout=0, **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.dropout = dropout\n        self.recurrent_dropout = recurrent_dropout\n        self.state_size = [units, units]\n        self.output_size = units\n        self.LSTM_cell = keras.layers.LSTMCell(units, dropout=dropout, recurrent_dropout=recurrent_dropout, activation=None)\n        self.layer_norm = keras.layers.LayerNormalization()\n        self.activation = keras.activations.get(activation)\n    def call(self, inputs, states):\n        memory_states, carry_states = states\n        outputs, new_states = self.LSTM_cell(inputs, [memory_states, carry_states])\n        norm_outputs = self.activation(self.layer_norm(outputs))\n        return norm_outputs, [new_states]\n    def get_config(self):\n        base_config = super().get_config()\n        custom_config = {'units':self.units,\n                         'dropout':self.dropout,\n                         'recurrent_dropout':self.recurrent_dropout,\n                         'activation':self.activation}\n        return {**base_config, **custom_config}","5fe5d891":" with open(join('dataset','ECFR-title14-train.txt'), 'r', encoding = 'UTF-8') as file:\n    corpus = file.read()\n\nexcerpt_length = 128\ncorrect_letter = []\nuser_guess = []\nend_of_excerpt = []\n\nprint('HUMAN PERFORMANCE EVALUATION')\nprint('How many attemps are you interested in taking?')\nuser_input = input()\nprint()\n\ntry:\n    num_attempts = int(user_input)\nexcept:\n    num_attempts = 0\n\nfor attempt in range(num_attempts):\n    print('Attempt # {}\/{}: '.format(str(attempt+1).rjust(len(str(num_attempts)),'0'), num_attempts))\n    start = np.random.randint(low = 0, high = len(corpus) - excerpt_length)\n    excerpt = corpus[start:start+excerpt_length]\n    print(excerpt[:-1].replace(' ', '_'))\n    end_of_excerpt.append(excerpt[-16:-1])\n    correct_letter.append(excerpt[-1])\n    user_guess.append(input())\n    print()\n\nfor attempt in range(num_attempts):\n    print('Attempt # {}\/{}: '.format(str(attempt+1).rjust(len(str(num_attempts)),'0'), num_attempts), end='')\n    print('{}({})[{}]'.format(end_of_excerpt[attempt].replace('\\n',' '), correct_letter[attempt], user_guess[attempt]), end='')\n    if correct_letter[attempt] == user_guess[attempt]:\n        print(' - CORRECT')\n    else:\n        print(' - WRONG')\n\nprint('\\nUser accuracy: {}'.format((np.array(correct_letter) == np.array(user_guess)).mean()))","65dd6cdc":"batch_size = 128\ndict_size = len(tokenizer.index_word.keys()) + 1\nlr_0 = .5\nepochs = 1","8f04ea03":"train_dummy = make_dataset('train', tokenizer, batch_size = batch_size, dummy = True)\ntrain = make_dataset('train', tokenizer, batch_size = batch_size)\nvalid = make_dataset('valid', tokenizer)\ntest = make_dataset('test', tokenizer)","016a887c":"model = keras.models.Sequential(name = \"Dummy_Classifier\")\n\nmodel.add(keras.layers.Input(shape = (1), name = 'Input')) # add None to first dimension when using RNN\nmodel.add(keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens = dict_size, name = 'Encoding'))\nmodel.add(keras.layers.Dense(units = dict_size, activation = 'softmax', name = 'Output'))\n\nmodel.compile(optimizer = keras.optimizers.SGD(learning_rate = lr_0), \n              loss = 'sparse_categorical_crossentropy', metrics = 'sparse_categorical_accuracy')\n\nmodel.summary()","169e6896":"history = model.fit(train_dummy, epochs = 1, verbose = 0)","8e4b244c":"print_performance(model)","1992c4b9":"bias_output_layer = model.layers[-1].get_weights()[1]","391824ac":"batch_size = 128\ndict_size = len(tokenizer.index_word.keys()) + 1\nlr_0 = 1\nepochs = 50","33897cb7":"train = make_dataset('train', tokenizer, batch_size = batch_size)\nvalid = make_dataset('valid', tokenizer)\ntest = make_dataset('test', tokenizer)","600f1195":"model = keras.models.Sequential(name = \"Softmax_Classifier_with_Encoding\")\n\nmodel.add(keras.layers.Input(shape = (1), name = 'Input')) # add None to first dimension when using RNN\nmodel.add(keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens = dict_size, name = 'Encoding'))\nmodel.add(keras.layers.Dense(units = dict_size, activation = 'softmax', name = 'Output',\n                             bias_initializer = keras.initializers.Constant(bias_output_layer)))\n\nmodel.compile(optimizer = keras.optimizers.SGD(learning_rate = lr_0), \n              loss = 'sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy'])\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience = 1, restore_best_weights = True, verbose = True)\n\nmodel.summary()","d2382076":"history = model.fit(train, epochs = epochs, verbose = 0, validation_data = valid,\n                   callbacks = [early_stopping_cb])","1c8f169e":"print_performance(model)\nprint_history(history)","209bae04":"batch_size = 128\ndict_size = len(tokenizer.index_word.keys()) + 1\nembedding_dim = 10\nlr_0 = 1\nepochs = 50","93cd8d75":"train = make_dataset('train', tokenizer, batch_size = batch_size)\nvalid = make_dataset('valid', tokenizer)\ntest = make_dataset('test', tokenizer)","48a6cc4c":"model = keras.models.Sequential(name = \"Softmax_Classifier_with_Embedding\")\n\nmodel.add(keras.layers.Embedding(input_dim = dict_size, output_dim = embedding_dim, input_length = 1, name = 'Embedding'))\nmodel.add(keras.layers.Dense(units = dict_size, activation = 'softmax', name = 'Output',\n                             bias_initializer = keras.initializers.Constant(bias_output_layer)))\n\nmodel.compile(optimizer = keras.optimizers.SGD(learning_rate = lr_0), \n              loss = 'sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy'])\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience = 1, restore_best_weights = True, verbose = True)\n\nmodel.summary()","c99b938a":"history = model.fit(train, epochs = epochs, verbose = 0, validation_data = valid,\n                   callbacks = [early_stopping_cb])","ac66aa74":"print_performance(model)\nprint_history(history)","8e0ad302":"sequence_length = 32\nbatch_size = 128\ndict_size = len(tokenizer.index_word.keys()) + 1\nlr_0 = .01\nepochs = 100","52ddb301":"train = make_dataset('train', tokenizer, sequence_length = sequence_length, batch_size = batch_size, batching_mode = 'recurrent_encoding')\nvalid = make_dataset('valid', tokenizer, sequence_length = sequence_length, batching_mode = 'recurrent_encoding')\ntest = make_dataset('test', tokenizer, sequence_length = sequence_length, batching_mode = 'recurrent_encoding')","f20c59dc":"encoding = keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens = dict_size, name = \"Encoding\")\ntd_encoding = keras.layers.TimeDistributed(encoding, input_shape = [None, 1], name = \"TD_Encoding\")\n\nsimple_rnn = keras.layers.SimpleRNN(units = dict_size, return_sequences = True, activation = 'softmax', \n                                    bias_initializer = keras.initializers.Constant(bias_output_layer),\n                                    name = \"Recurrent\")\ngru = keras.layers.GRU(units = dict_size, return_sequences = True, activation = 'softmax', \n                       name = \"Recurrent\")\nlstm = keras.layers.LSTM(units = dict_size, return_sequences = True, activation = 'softmax', \n                         name = \"Recurrent\")\n\n#model = keras.models.Sequential([td_encoding, simple_rnn], name = \"Simple_RNN_with_Encoding\")\n#model = keras.models.Sequential([td_encoding, gru], name = \"GRU_with_Encoding\")\nmodel = keras.models.Sequential([td_encoding, lstm], name = \"LSTM_with_Encoding\")\n\nmodel.compile(optimizer = keras.optimizers.Adam(learning_rate = lr_0), \n              loss = 'sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy'])\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience = 5, restore_best_weights = True, verbose = True)\nlog_dir = get_tensorboard_logdir()\ntensorboard_cb = keras.callbacks.TensorBoard(log_dir = log_dir)\n\nmodel.summary()","33a66171":"history = model.fit(train, epochs = epochs, verbose = 0, validation_data = valid,\n                   callbacks = [early_stopping_cb, tensorboard_cb])","d0062d4f":"print_performance(model, log_dir)\nprint_history(history)","bd1d8089":"sequence_length = 32\nbatch_size = 128\ndict_size = len(tokenizer.index_word.keys()) + 1\nembedding_dim = 20\nlr_0 = .01\nepochs = 100","cbdf10c7":"train = make_dataset('train', tokenizer, sequence_length = sequence_length, batch_size = batch_size, batching_mode = 'recurrent_embedding')\nvalid = make_dataset('valid', tokenizer, sequence_length = sequence_length, batching_mode = 'recurrent_embedding')\ntest = make_dataset('test', tokenizer, sequence_length = sequence_length, batching_mode = 'recurrent_embedding')","63bf463b":"embedding = keras.layers.Embedding(input_dim = dict_size, output_dim = embedding_dim, name = \"Embedding\", input_shape=[None])\nrecurrent = keras.layers.SimpleRNN(units = dict_size, return_sequences = True, activation = 'softmax', \n                                   bias_initializer = keras.initializers.Constant(bias_output_layer),\n                                   name = \"Recurrent\")\n\nsimple_rnn = keras.layers.SimpleRNN(units = dict_size, return_sequences = True, activation = 'softmax', \n                                   bias_initializer = keras.initializers.Constant(bias_output_layer),\n                                   name = \"Recurrent\")\n\ngru = keras.layers.GRU(units = dict_size, return_sequences = True, activation = 'softmax', \n                       name = \"Recurrent\")\n\nlstm = keras.layers.LSTM(units = dict_size, return_sequences = True, activation = 'softmax', \n                         name = \"Recurrent\")\n\n#model = keras.models.Sequential([embedding, simple_rnn], name = \"Simple_RNN_with_Embedding\")\nmodel = keras.models.Sequential([embedding, gru], name = \"GRU_with_Embedding\")\n#model = keras.models.Sequential([embedding, lstm], name = \"LSTM_with_Embedding\")\n\n\nmodel.compile(optimizer = keras.optimizers.Adam(learning_rate = lr_0), \n              loss = 'sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy'])\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience = 5, restore_best_weights = True, verbose = True)\nlog_dir = get_tensorboard_logdir()\ntensorboard_cb = keras.callbacks.TensorBoard(log_dir = log_dir)\n\nmodel.summary()","df86ab1d":"history = model.fit(train, epochs = epochs, verbose = 0, validation_data = valid,\n                   callbacks = [early_stopping_cb, tensorboard_cb])","5f412c7b":"print_performance(model, log_dir)\nprint_history(history)","d3178472":"sequence_length = 32\nbatch_size = 128\ndict_size = len(tokenizer.index_word.keys()) + 1\nrecurrent_dim = 2 * dict_size\nlr_0 = .001\nepochs = 100","61c3bd8a":"train = make_dataset('train', tokenizer, sequence_length = sequence_length, batch_size = batch_size, batching_mode = 'recurrent_encoding')\nvalid = make_dataset('valid', tokenizer, sequence_length = sequence_length, batching_mode = 'recurrent_encoding')\ntest = make_dataset('test', tokenizer, sequence_length = sequence_length, batching_mode = 'recurrent_encoding')","d51805af":"encoding = keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens = dict_size, name = \"Encoding\")\ntd_encoding = keras.layers.TimeDistributed(encoding, input_shape = [None, 1], name = \"TD_Encoding\")\n\nsimple_rnn = keras.layers.SimpleRNN(units = recurrent_dim, return_sequences = True, name = \"Recurrent\")\ngru = keras.layers.GRU(units = recurrent_dim, return_sequences = True, name = \"Recurrent\")\nlstm = keras.layers.LSTM(units = recurrent_dim, return_sequences = True, name = \"Recurrent\")\n\ndense = keras.layers.Dense(units = dict_size, activation = 'softmax', name = \"Dense\",\n                           bias_initializer = keras.initializers.Constant(bias_output_layer))\n\n#model = keras.models.Sequential([td_encoding, simple_rnn, dense], name = \"Simple_RNN_Dense_output_with_Encoding\")\n#model = keras.models.Sequential([td_encoding, gru, dense], name = \"GRU_Dense_output_with_Encoding\")\nmodel = keras.models.Sequential([td_encoding, lstm, dense], name = \"LSTM_Dense_output_with_Encoding\")\n\nmodel.compile(optimizer = keras.optimizers.Adam(learning_rate = lr_0), \n              loss = 'sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy'])\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience = 5, min_delta = .01, restore_best_weights = True, verbose = True)\nlog_dir = get_tensorboard_logdir()\ntensorboard_cb = keras.callbacks.TensorBoard(log_dir = log_dir)\n\nmodel.summary()","312e50fd":"history = model.fit(train, epochs = epochs, verbose = 0, validation_data = valid,\n                    callbacks = [early_stopping_cb, tensorboard_cb])","37e239af":"print_performance(model, log_dir)\nprint_history(history)","e0fd6673":"sequence_length = 32\nbatch_size = 128\ndict_size = len(tokenizer.index_word.keys()) + 1\nrecurrent_dim = 2 * dict_size\nrecurrent_stack_size = 3\ntype_of_recurrent_unit = \"lstm\" # \"simple_rnn\", \"gru\", \"lstm\"\nmodel_name = \"LSTM_stack_encoding_dense\"\nlr_0 = .001\nepochs = 100","cb6c4473":"train = make_dataset('train', tokenizer, sequence_length = sequence_length, batch_size = batch_size, batching_mode = 'recurrent_encoding')\nvalid = make_dataset('valid', tokenizer, sequence_length = sequence_length, batching_mode = 'recurrent_encoding')\ntest = make_dataset('test', tokenizer, sequence_length = sequence_length, batching_mode = 'recurrent_encoding')","d3c2361c":"model = keras.models.Sequential(name = model_name)\n\nencoding = keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens = dict_size, name = \"Encoding\")\ntd_encoding = keras.layers.TimeDistributed(encoding, input_shape = [None, 1], name = \"TD_Encoding\")\n\nmodel.add(td_encoding)\n\nfor unit in range(recurrent_stack_size):\n    if type_of_recurrent_unit == \"simple_rnn\":    \n        model.add(keras.layers.SimpleRNN(units = recurrent_dim, return_sequences = True, name = \"Recurrent_\"+str(unit)))\n    elif type_of_recurrent_unit == \"gru\":\n        model.add(keras.layers.GRU(units = recurrent_dim, return_sequences = True, name = \"Recurrent_\"+str(unit)))\n    elif type_of_recurrent_unit == \"lstm\":\n        model.add(keras.layers.LSTM(units = recurrent_dim, return_sequences = True, name = \"Recurrent_\"+str(unit)))\n\ndense = keras.layers.Dense(units = dict_size, activation = 'softmax', name = \"Dense\",\n                           bias_initializer = keras.initializers.Constant(bias_output_layer))              \n              \nmodel.add(dense)\n\nmodel.compile(optimizer = keras.optimizers.Adam(learning_rate = lr_0), \n              loss = 'sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy'])\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience = 5, min_delta = .01, restore_best_weights = True, verbose = True)\nlog_dir = get_tensorboard_logdir()\ntensorboard_cb = keras.callbacks.TensorBoard(log_dir = log_dir)\n\nmodel.summary()","f90966eb":"history = model.fit(train, epochs = epochs, verbose = 0, validation_data = valid,\n                    callbacks = [early_stopping_cb, tensorboard_cb])","b28881f7":"print_performance(model, log_dir)\nprint_history(history)","78adec45":"sequence_length = 128\nbatch_size = 32\ndict_size = len(tokenizer.index_word.keys()) + 1\nrecurrent_dim = 4 * dict_size\nrecurrent_stack_size = 3\ntype_of_recurrent_unit = \"gru\" # \"simple_rnn\", \"gru\", \"lstm\"\nmodel_name = \"GRU_LN_encoding_dense\"\nlr_0 = .001\nepochs = 100\nclip_norm = 1","cf326095":"train = make_dataset('train', tokenizer, sequence_length = sequence_length, batch_size = batch_size, batching_mode = 'recurrent_encoding')\nvalid = make_dataset('valid', tokenizer, sequence_length = sequence_length, batching_mode = 'recurrent_encoding')\ntest = make_dataset('test', tokenizer, sequence_length = sequence_length, batching_mode = 'recurrent_encoding')","e3988742":"model = keras.models.Sequential(name = model_name)\n\nencoding = keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens = dict_size, name = \"Encoding\")\ntd_encoding = keras.layers.TimeDistributed(encoding, input_shape = [None, 1], name = \"TD_Encoding\")\n\nmodel.add(td_encoding)\n\nfor unit in range(recurrent_stack_size):\n    if type_of_recurrent_unit == \"simple_rnn\":    \n        # NOT YET IMPLEMENTED WITH LN\n        model.add(keras.layers.SimpleRNN(units = recurrent_dim, return_sequences = True, name = \"Recurrent_\"+str(unit)))\n    elif type_of_recurrent_unit == \"gru\":\n        model.add(keras.layers.RNN(LN_GRU_Cell(units = recurrent_dim), return_sequences = True, name = \"Recurrent_\"+str(unit)))\n    elif type_of_recurrent_unit == \"lstm\":\n        model.add(keras.layers.RNN(LN_LSTM_Cell(units = recurrent_dim), return_sequences = True, name = \"Recurrent_\"+str(unit)))\n\ndense = keras.layers.Dense(units = dict_size, activation = 'softmax', name = \"Dense\",\n                           bias_initializer = keras.initializers.Constant(bias_output_layer))              \n              \nmodel.add(dense)\n\nmodel.compile(optimizer = keras.optimizers.Adam(learning_rate = lr_0, clipnorm = clip_norm), \n              loss = 'sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy'])\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience = 2, min_delta = .01, restore_best_weights = True, verbose = True)\nlog_dir = get_tensorboard_logdir()\ntensorboard_cb = keras.callbacks.TensorBoard(log_dir = log_dir)\n\nmodel.summary()","774c181a":"history = model.fit(train, epochs = epochs, verbose = 1, validation_data = valid,\n                    callbacks = [early_stopping_cb, tensorboard_cb])","47bc4c74":"print_performance(model, log_dir)\nprint_history(history)","462657d9":"tf.random.set_seed(20210228)\nsequence_length = 512\nbatch_size = 64\ndict_size = len(tokenizer.index_word.keys()) + 1\nrecurrent_dim = 5 * dict_size\nrecurrent_stack_size = 3\ndropout = .10\nrecurrent_dropout = .05\ntype_of_recurrent_unit = \"gru\" # \"simple_rnn\", \"gru\", \"lstm\"\nmodel_name = \"Stateful_GRU_LN_encoding_dense\"\nlr_0 = .001\nepochs = 100\nclip_norm = None","d1649cb8":"train = make_dataset('train', tokenizer, sequence_length = sequence_length, batch_size = batch_size, batching_mode = 'stateful_encoding')\nvalid = make_dataset('valid', tokenizer, sequence_length = sequence_length, batching_mode = 'stateful_encoding')\ntest = make_dataset('test', tokenizer, sequence_length = sequence_length, batching_mode = 'stateful_encoding')","0e353156":"model = keras.models.Sequential(name = model_name)\n\nencoding = keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens = dict_size, name = \"Encoding\")\ntd_encoding = keras.layers.TimeDistributed(encoding, input_shape = [None, 1], name = \"TD_Encoding\")\n\nmodel.add(td_encoding)\n\nfor unit in range(recurrent_stack_size):\n    if type_of_recurrent_unit == \"simple_rnn\":    \n        # NOT YET IMPLEMENTED WITH LN\n        model.add(keras.layers.SimpleRNN(units = recurrent_dim, return_sequences = True, name = \"Recurrent_\"+str(unit)))\n    elif type_of_recurrent_unit == \"gru\":\n        model.add(keras.layers.RNN(LN_GRU_Cell(units = recurrent_dim, dropout = dropout, recurrent_dropout = recurrent_dropout), \n                                   return_sequences = True, name = \"Recurrent_\"+str(unit)))\n    elif type_of_recurrent_unit == \"lstm\":\n        model.add(keras.layers.RNN(LN_LSTM_Cell(units = recurrent_dim, dropout = dropout, recurrent_dropout = recurrent_dropout), \n                                   return_sequences = True, name = \"Recurrent_\"+str(unit)))\n\ndense = keras.layers.Dense(units = dict_size, activation = 'softmax', name = \"Dense\",\n                           bias_initializer = keras.initializers.Constant(bias_output_layer))              \n              \nmodel.add(dense)\n\nmodel.compile(optimizer = keras.optimizers.Nadam(learning_rate = lr_0, clipnorm = clip_norm), \n              loss = 'sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy'])\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience = 100, min_delta = 0, restore_best_weights = True, verbose = True)\nplateau_cb = keras.callbacks.ReduceLROnPlateau(monitor = 'loss', min_delta = .01, patience = 3, factor = .3, cooldown = 30, verbose = 1)\nterminate_on_nan_cb = keras.callbacks.TerminateOnNaN()\nlog_dir = get_tensorboard_logdir()\ntensorboard_cb = keras.callbacks.TensorBoard(log_dir = log_dir)\n\nmodel.summary()","22bfd547":"history = model.fit(train, epochs = epochs, verbose = 1, validation_data = valid,\n                    callbacks = [early_stopping_cb, tensorboard_cb, terminate_on_nan_cb, plateau_cb])","58573a6b":"print_performance(model, log_dir)\nprint_history(history)","f59b6bf8":"file_name = 'model___' + log_dir.replace(join(getcwd(),'tb_logs',''), '') + '___' + model_name + '.h5'\nif 'models' not in listdir():\n    mkdir('models')\nmodel.save(join(getcwd(), 'models', file_name))","ff78249d":"file_name = 'tokenizer___' + log_dir.replace(join(getcwd(),'tb_logs',''), '') + '___' + model_name + '.json'\n\nwith open(join(getcwd(), 'models', file_name), 'wb') as file:\n    pickle.dump(tokenizer, file)","1a1873fb":"def convert_to_inference_model(original_model, custom_objects=None):\n    original_model_json = original_model.to_json()\n    inference_model_dict = json.loads(original_model_json)\n\n    layers = inference_model_dict['config']['layers']\n    for layer in layers:\n        if 'stateful' in layer['config']:\n            layer['config']['stateful'] = True\n\n        if 'batch_input_shape' in layer['config']:\n            layer['config']['batch_input_shape'][0] = 1\n            layer['config']['batch_input_shape'][1] = None\n\n    inference_model = keras.models.model_from_json(json.dumps(inference_model_dict), custom_objects = custom_objects)\n    inference_model.set_weights(original_model.get_weights())\n\n    return inference_model","9e37f828":"inference_model = convert_to_inference_model(model, custom_objects = {'LN_GRU_Cell':LN_GRU_Cell, 'LN_LSTM_Cell':LN_LSTM_Cell})","81c92934":"length_generated_text = 10000\nfile_name = ('generated_text___' + log_dir.replace(join(getcwd(),'tb_logs',''), '') + \n             '___' + model_name + '___' + time.strftime(\"%Y-%m-%d__%H-%M-%S\") + '.txt')\ndirectory = 'generated_text'\n\ntext = '\u00a7'\nsequence = tokenizer.texts_to_sequences([text])\n\nfor _ in range(length_generated_text):\n    probs = inference_model.predict(np.array(sequence[0][-1]).reshape(1,-1,1))\n    token = np.random.choice(np.arange(dict_size), p = probs[0,-1,:])\n    sequence[0].append(token)\n\ntext = tokenizer.sequences_to_texts(sequence)[0]    \nprint(text[0:np.min([2000, length_generated_text]):2])\n\nif directory not in listdir():\n    mkdir(directory)\n\nwith open(join(getcwd(), 'generated_text', file_name), 'w', encoding='utf-8') as file:\n    file.write(text[0::2])","36129fc2":"The results with an embedding layer seam to be marginally worse than those with one-hot encoding.","6726f60f":"A detailed discussion of the results and the properties of the generated text is available at [this blog post](https:\/\/fabio-a-oliveira.github.io\/2021-04-01_14-CFR-FAA.html). A file with 1 million characters generated with the model is found at [this link](https:\/\/raw.githubusercontent.com\/fabio-a-oliveira\/14-CFR-FAA\/main\/generated_text\/generated_text___1000000_chars__2021-03-27__21-15-26.txt).","e56de940":"## RNN followed by dense layer, with character encoding\n\n* All built-in RNN units implemented: Simple RNN & GRU & LSTM.\n* With a dense layer for the output, we can experiment with the number of units in the recurrent layers.\n* Bias initialization on dense layer considering class imbalance.\n* Minor tweaking of learning_rate.","92247e8a":"## Test the dataset for stateful RNN\n\nThis piece of code displays an excerpt of the dataset when prepared for training a stateful RNN. In this setup, the dataset is split into multiple batches of 4 sequences of length 48 each.\n\nHere we see 6 of this batches. Notice that the first sequence of the first, second, third etc batches are pieces of the same sentence. The same goes for the second sequence and so on. This means that each instance of the RNN will be trained with a continuation of the sentence it just saw in the previous gradient descent step.","6230ffca":"## Create a tokenizer instance and fit to train set\n\nA _tokenizer_ object is then fit to the corpus of text. If will be provided as input to the _make_dataset_ function and will be used to translate the characters into integer numbers according the a dictionary constructed via the _fit_on_texts()_ method.","ca496988":"## RNN with single recurrent unit and embedding\n\n* Same setup as above, but with and embedding instead of encoding.\n* The embedding layer does not require wrapping with a time distributed layer.","a6f0c9c7":"## Import packages\n\nBesides the usual suspects, we use the _xml.etree.ElementTree_ Python model to process the .xml file obtained from the e-CFR.","daa0219a":"## Save model\n\nThe code below saves the model in the .h5 file format.","4a7eed9d":"Here, the biases in the output layer are saved to the _bias_output_layer_ variable. This vector will be used as the initialization for the output layer for the next models.","016a6a0f":"## Final model: Stateful RNN, one-hot encoding, layer normalization\n\nThe final performance improvement comes from using a _stateful_ RNN architecture. This allows the network to identify patterns that are longer than the sequence size fed to the network at each optimization step. Here are the features of this model:\n\n* Recurrent layers are created with the _stateful=True_ argument, so the memory cells are not reset after each gradient descent step.\n* In order to used a _stateful_ network, the dataset needs to be split in such a way that each network instance is fed with the sequence that corresponds to that which it processed in the previous step. The dataset processing function defined in the beginning of the notebook is setup as such.\n* A _ReduceLROnPlateau_ callback is carefully tuned to reduce the learning rate at approapriate moments during the training process so that learning is more precise in the intermediate and final stages of training.\n* Dropout rates are selected to provide a regularization effect and reduce overfitting.\n* After a lot of experimentation, GRU show better performance and robustness to larger learning rates.\n* The batch size and sequence length are selected to allow for faster training, ability to identify longer patterns, and capacity to fit into the available memory.","35ad9482":"## Stack of recurrent units followed by dense layer, with one-hot character encoding\n\n* Application of a stack of 3 recurrent units.\n* Implementation for all 3 built-in recurrent units (choose which one using the _type_of_recurrent_unit_ variable in the first cell).","a806b1a8":"This code applies the function above to the model, and creates a stateful version for inference.","f6f1febc":"The improvements in this model provide a major performance boost, with over 82% accuracy in the validation and test sets.","b3f0a6ca":"# Models\n\nA healthy practice when developing complex models is to start with a rudimentary version and to incrementaly add more elements. This stepwise approach provides transparency as to what each piece of the model provides in terms of performance and allows for more control when deciding which pieces to add. Ideally, no complexity should be included in the model unless it improves performance.","42da4a41":"## Simple model with embedded input\n\n* Given a single character, apply embedding and predict the next character\n* Embedding layer followed by Dense layer with softmax activation","152639f2":"## Custom recurrent layers with Layer Normalization\n\nLayer normalization is a game-changing technique for stabilizing the training of RNNs with long sequences of inputs.\n\nWhile both the layer normalization layer and the GRU and LSTM layers are built-in to _keras_, there is currently no straighforward way to apply layer normalization to a recurrent unit. In order to do so, custom RNN cells are required.\n\nThe classes defined below implement both GRU and LSTM cells with layer normalization.","1a699c40":"## Recurrent units with layer normalization\n\n* Model with a stack of 3 recurrent units followed by a dense layer.\n* This model uses custom GRU and LSTM recurrent units (the classes are declared in a section above).\n* Layer normalization stabilizes training and allows for faster learning rates as well as processing of longer text sequences with decreased risk of exploding gradients.","22674706":"# Inference and model persistence\n\nIn this section, we save the model and use it to create new text sequences.","58501f92":"# Evaluate human performance\n\nIt is usually interesting to assess how well humans perform on a certain task before trying to build a model for that same task. In order to do so, the code below challenges the user to try and guess what is the next character after a text excerpt has been presented.\n\nTo avoid confusion with excerpts ending in spaces, all spaces are substituted with a '_' character.\n\nAfter a few hundred guesses, I was able to reach around 65% accuracy.","fda7ff40":"# Introduction\n\nThis notebook contains a variety of models that tackle the problem of creating a character-based language-model out of the full set of FAA regulations. The final and best performing model is then used to generate new text in the style of the corpus of regulations it has been trained on - **a new regulations generator!**\n\nThe FAA is the US federal agency responsible for regulating the entire civil aviation sector in the country. It oversees everything from aircraft certification to air traffic control, airmen qualifications, flight operations, aircraft maintenance and much more.\n\nIn this project, we download the full set of regulations under the FAA responsibility directly from the e-CFR (electronic Code of Federal Regulations). The .xml file is processed and split into a _train_, a _validation_ and a _test_ .txt files, which are then used to train and evaluate a Recurrent Neural Network model that receives a sequence of characters as input and outputs a prediction for the next character in the sequence.\n\nHere are a few features that are explored in the sections below:\n\n* Human performance is evaluated as a benchmark by presenting a series of short sequences of text and requesting a guess as to what would be the next character. Upon trying it for a few hundred attemps, I was able to correctly guess the next character around 65% of the times.\n\n* A _tokenizer_ object is used to create a dictionary with every character found in the text corpus. This dictionary is then used to feed into two different encoding approaches:   \n    1. One-hot encoding, in which a vector of the same size as the dictionary is created for each input and the position corresponding to the character in the input is set to 1, while all others are set to 0;\n    2. Character embedding, in which a trainable dense representation of the characters is created, with a dimension smaller than the size of the dictionary.\n\n* A function is created to prepare the dataset according to parameters such as the chosen encoding, batch size, sequence length and the statefulness status of the RNN.\n\n* A model with _null_ input (every input character set to zero) is created in order to set the performance benchmark and determine the prevalence of each of the characters in the dictionary.\n\n* A simple one-to-one (non-recurrent) model is also trained as a benchmark.\n\n* Different types of recurrent units are experimented with: the SimpleRNN, GRU and LSTM built-in units from _keras_, as well as custom GRU and LSTM units paired with a layer normalization layer.\n\n* The length of the sequences to which each instance of the RNN is exposed is carefully chosen (together with the batch size) to maximize the sequence length (so the network can try and learn longer relationships and patterns), while respecting memory restrictions and a reasonable training time.\n\n* The final and best performing model is composed of a stack of 3 GRUs with 580 units, with layer normalization and a dense sofmax layer on top, regularized with dropout (both vertical and horizontal), with a sequence length of 512 characters. The RNN is configured as _stateful_, meaning that the memory cells are not erased after each gradient descent iteration.\n\n* With the _stateful_ configuration, the RNN can propagate information indefinetely. However, because it was trained with a sequence length of 512, backpropagation is not able to_teach_ the network to actively learn to preserve longer patterns. In other words, the network _stores_ patterns in longer sequences, but it was not taught to _actively memorize_ patterns that might be useful more than 512 characters in the future.\n\n* Every one of the models used in the work are character-based. At this time, there were no experiments with word embeddings, which could potentially have a dramatic improvement in performance. Nevertheless, the results are quite remarkable considering that the model has no prior knowledge of the _semantics_ of the text whatsoever.\n\nFinally, you are encouraged to use this code any way you want. You _**are not**_ encouraged to try to train any of these models without a GPU!","76dabbe4":"## Download and split data\n* The .xml file is downloaded directly from the e-CFR website using the _wget_ command and is saved to the _dataset_ directory.\n\nThe file is then processed with the _xml.etree.ElementTree_ module. Not all volumes of Title 14 of the Code of Federal Regulations are related to civil aviation (there are also DOT and NASA volumes), so they are not saved in the dataset.\n\nThe full text is split into a _train_, a _validation_ and a _test_ sets in a proportion of 98\/1\/1%. The code also allows for a _trash_ file to be created to allow for some experimentation with smaller datasets.\n\nThe division is done section by section, so that paragraphs are not split during the partitioning of the dataset.","abd114aa":"## Set up the datasets\n\nThe function _make_dataset_ sets up each of the partitions in a _Dataset_ object from the _TensorFlow_ _data_ API. The datasets are batched according the the selection of the sequence length and batch size.\n\nA few different modes are also implemented, depending on the type of model that will be used. The shape of the inputs is dependent on whether they will be fed into a dense non-recurrent network, a recurrent network with one-hot encoding or character embedding, or a stateful recurrent network.\n\nThe preparation of the dataset for use with the stateful model is particularly demanding. It is fundamental to make that each instance of the network is fed always fed with a sequence that is the continuation of the previous sequence. When doing training with multiple batches, this requires batching and interleaving the pieces in a very particular order.","d0a692d0":"# Setup","7d1de024":"The use of a stack of 3 recurrent units gives us another performance boost.","0ff9c12e":"With the use of a dense layer as the output layer and the increase in the number of units in the recurrent units, we get a significant performance improvement.","f01e896f":"## RNNs with single recurrent unit and one-hot encoding\n\n* Many-to-many setup: the network tries to predict the entire sequence of characters.\n* Single recurrent unit.\n* Code available for all three built-in recurrent units: Simple recurrent unit & GRU & LSTM (uncomment appropriate line to chose which one to use).\n* One-hot encoding layer is wrapped by a time distributed layer.\n* Default GRU and LSTM implementations use the same bias_initializer for the kernel and recurrent channels. Bias initialization with the biases obtained from the dummy model requires accessing the right part of the bias vector (not implemented here).","97c649b7":"## Text generator\n\nThis section uses the recently trained model to create original text.\n\nThe _convert_to_inference_model_ function receives a model as argument and returns a version with _stateful_ set to _True_. The application of this function is not necessary for the final model (which was created and trained in this setup), but is important for the use of all the previous versions.","19952334":"This section of code generates a new sequence of characters. Because the model is set to _stateful=True_, we can feed the model with a seed text (in this case, the section character) and use it to generate probabilities for the following character. We then randomly select a new character according to the predicted probabilities, add it to the text, and feed the text back into the inference model.","bf4a3d04":"## Helper functions\n\nSome helper functions are defined to print the model performance in a standardized manner and plot a figure with the evolution of its loss and performance metric during training. Another function is defined to create an unique name for a directory for performance logging for usage with _TensorBoard_.","9b9aed43":"## Simple model with one-hot encoded input\n\n* Given a single character, one-hot encode and predict the next one\n* Encoding layer followed by Dense layer with softmax activation","1be4f13e":"## Dummy model with null input\n\nWe begin modeling by constructing a dummy model. In this setup, the dataset features in the dataset are replaced with a stream of zeros. A densely connected layer with softmax activation is used to learn the biases corresponding the frequency of each of the characters in the dataset.\n\nHere are some of the features of this model:\n\n* Given a null input, predict next character\n* Gradients for the weights will always be null\n* This setup will only learn the bias, which can then be used as initialization for the bias on the last layer of the subsequent models\n* Avoids hockey-stick learning curves\n* Learning rate can be set up on next models according to a phase when the network is actually learning patterns in the data\n* Training for a single epoch is sufficient for dense layer to learn the biases.\n\nAdditionaly, the value for the loss and metrics obtained in this manner serve as an important performance benchmark - any model that is effectively able to extract information from the inputs should be able to beat these values.","b6311ff6":"Here we see a significant improvement in performance in comparison with the simpler models that only took one character into account when predicting the next one."}}