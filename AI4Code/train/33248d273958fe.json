{"cell_type":{"f3553e58":"code","3adf6740":"code","884c52f8":"code","85ee503f":"code","980282c4":"code","499a17b3":"code","cf3ff8cf":"code","a6d11a73":"code","18ac3ce3":"code","be660bfa":"code","ef9ded52":"code","e4c652ad":"code","8790d869":"code","fbaebea0":"code","b3455eb5":"code","11e6936b":"code","9604999c":"code","2ece3907":"code","713765b1":"code","1f49fed6":"code","7c0b11fe":"code","c04eba16":"code","50008551":"code","7c2f944f":"code","13a7791f":"code","19888971":"code","a9f67c3d":"code","ddcb9742":"code","4abb07e7":"code","6ae19adc":"code","5d401422":"markdown","a8d2ee9f":"markdown","e2cd25d0":"markdown","521adcc1":"markdown","b3667c51":"markdown","ad589f14":"markdown","c8871977":"markdown","2470d5e7":"markdown","a316a920":"markdown","0345975b":"markdown","49e5f5f9":"markdown"},"source":{"f3553e58":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3adf6740":"data = pd.read_csv(\"\/kaggle\/input\/world-happiness-report-2021\/world-happiness-report.csv\")\ndata.head()","884c52f8":"test = pd.read_csv(\"\/kaggle\/input\/world-happiness-report-2021\/world-happiness-report-2021.csv\")\ntest.head()","85ee503f":"test_cols = ['Ladder score','Logged GDP per capita','Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption']\ndata_test = test[test_cols]\ndata_test.head()","980282c4":"data = data.drop([\"Country name\", \"year\", \"Positive affect\", \"Negative affect\"], axis=1)\ndata.head()","499a17b3":"data.describe()","cf3ff8cf":"data.info()","a6d11a73":"sns.heatmap(data.isnull(), yticklabels=False)","18ac3ce3":"sns.set_style('darkgrid')\ndata.hist(figsize=(15, 12))","be660bfa":"sns.heatmap(data.corr(), annot=True, vmin=-1, vmax=1)","ef9ded52":"sns.pairplot(data)","e4c652ad":"data_red = data.drop([\"Social support\", \"Healthy life expectancy at birth\"], axis=1)\ndata_test_red = data_test.drop(['Social support', 'Healthy life expectancy'], axis=1)","8790d869":"sns.heatmap(data_red.corr(), annot=True, vmin=-1)","fbaebea0":"data_predictors = data.drop(\"Life Ladder\", axis=1)\ndata_red_predictors = data_red.drop(\"Life Ladder\", axis=1)\ndata_labels = data[\"Life Ladder\"].copy()","b3455eb5":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler","11e6936b":"pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('std_scaler', StandardScaler()),\n    ])","9604999c":"pipeline_red = Pipeline([\n        ('imputer_red', SimpleImputer(strategy=\"median\")),\n        ('std_scaler_red', StandardScaler()),\n    ])","2ece3907":"data_train = pipeline.fit_transform(data_predictors)","713765b1":"data_red_train = pipeline_red.fit_transform(data_red_predictors)","1f49fed6":"data_train","7c0b11fe":"data_red_train","c04eba16":"from sklearn.linear_model import LinearRegression\n\nlin = LinearRegression()\nlin.fit(data_train, data_labels)","50008551":"lin_red = LinearRegression()\nlin_red.fit(data_red_train, data_labels)","7c2f944f":"some_data = data_test.drop(\"Ladder score\", axis=1)\nsome_labels = data_test[\"Ladder score\"]\nsome_data_prepared = pipeline.transform(some_data)\npredictions = lin.predict(some_data_prepared)","13a7791f":"some_data_red = data_test_red.drop(\"Ladder score\", axis=1)\nsome_labels_red = data_test_red[\"Ladder score\"]\nsome_data_prepRed = pipeline_red.transform(some_data_red)\npredictions_red = lin_red.predict(some_data_prepRed)","19888971":"from sklearn.metrics import mean_squared_error\n\nlin_mse = mean_squared_error(some_labels, predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","a9f67c3d":"lin_red_mse = mean_squared_error(some_labels_red, predictions_red)\nlin_red_rmse = np.sqrt(lin_red_mse)\nlin_red_rmse","ddcb9742":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(lin, some_data_prepared, some_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-scores)","4abb07e7":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\ndisplay_scores(lin_rmse_scores)","6ae19adc":"scores_red = cross_val_score(lin_red, some_data_prepRed, some_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\nlinRed_rmse_scores = np.sqrt(-scores_red)\ndisplay_scores(linRed_rmse_scores)","5d401422":"Even though both models are far from being perfect for predicting happiness, the most straightforward model is slightly better and could have been used directly without any reductions. That is an interesting result for me because I've thought that all strong relationships between predictors are generally bad for the model performance.","a8d2ee9f":"I create a pipeline to fill missing values with median values and to standardize variables in one step. That is also useful to apply to test data.","e2cd25d0":"Now our train data is properly scaled and can be used to train the model. I choose linear regression to start with something simple.","521adcc1":"This is my first project on Kaggle, I want to practice some useful techniques, that I've learned from the first two chapters of the \"Hands-on Machine Learning\" by Aurelien Geron. \n\nFirst I compare columns in train and test data.","b3667c51":"As we can see, almost all attributes miss some values, even though it's not much. We can keep all attributes and fill them with mean or median values calculated for every attribute. ","ad589f14":"I drop some columns to make my train data and test data consistent. Also I don't think that country names and years can provide us with useful information.","c8871977":"I use two separate models. One for data and another one for data_red. After that I'm going to compare their performance in order to find out whether reduction was necessary or not.","2470d5e7":"Now it's time to prepare our datasets for machine learning algorithms.\nFirst of all i'll separate predictors from responses.","a316a920":"After RMSE I've applied cross validation and according to this method the reduced model shows worse results as well. ","0345975b":"According to RMSE (root mean squared error) the reduced model is slightly worse than the \"full\" one.","49e5f5f9":"I've noticed that some features have strong relationships not only with our target variable but also with other features. For example, \"Healthy life expectancy at birth\" is strongly correlated with \"Life ladder\", but even stronger it is correlated with \"Log GDP per capita\". I wonder if such relationships between features would reduce performance of our prediction model. To test this idea i would create an alternative dataset - data_red. "}}