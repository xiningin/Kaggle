{"cell_type":{"e73d1d31":"code","72e550c2":"code","3db7f014":"code","2f710104":"code","19bb9403":"code","286bc4e3":"code","1f712ba3":"code","be934eba":"code","f7642756":"code","9da1a644":"code","c558ca51":"code","adfefa0e":"code","5b988312":"code","c6bdf1b9":"markdown","c7fca02f":"markdown","5f9cc977":"markdown","693a9233":"markdown","aba1d582":"markdown","e61d9af7":"markdown","e15d530c":"markdown","838312c4":"markdown","4bcdb93b":"markdown","6f5f788f":"markdown","335b2ee7":"markdown"},"source":{"e73d1d31":"!\/opt\/conda\/bin\/python3.7 -m pip install --upgrade pip\n! pip install -q efficientnet","72e550c2":"#-------------------\n# importing libraries\n#-------------------\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport efficientnet.tfkeras as efn\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nfrom kaggle_datasets import KaggleDatasets\n\n\nimport pandas as pd\nimport numpy as np\n\nimport os\nimport shutil\nimport csv\n\nimport matplotlib.pyplot as plt\nimport PIL","3db7f014":"AUTO = tf.data.experimental.AUTOTUNE\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","2f710104":"GCS_DS_PATH = KaggleDatasets().get_gcs_path()\nprint(GCS_DS_PATH)\n\nTRAIN_PATH = GCS_DS_PATH + \"\/train_images\/\"\n\ntrain_df = pd.read_csv(\"..\/input\/plant-pathology-2021-fgvc8\/train.csv\")\n\ncount_dict = train_df.labels.value_counts()\nclasses = list(count_dict.index)\nclasses_count = list(count_dict.values)\nprint(\"Number of unique labels: \",len(classes))\n\nlabel2id = {\n    'scab': 0,\n    'frog_eye_leaf_spot' : 1,\n    'rust' : 2,\n    'complex' : 3,\n    'powdery_mildew' : 4,\n}\nNUM_CLASSES = len(label2id)    \nid2label = dict([(value, key) for key, value in label2id.items()])\ntrain_df[\"labels\"] = train_df[\"labels\"].map(lambda x : [i for i in x.split(\" \") if i != \"healthy\"])\n#train_df[\"labels\"] = train_df[\"labels\"].map(lambda x : x.split(\" \"))\ntrain_df[\"labels\"] = train_df[\"labels\"].map(lambda x : [label2id[i] for i in x])\n\ntrain_df.head()","19bb9403":"plt.figure(figsize=(35,15))\nplt.bar(classes,classes_count)\nplt.title(\"Number of instances per class\",fontweight=\"bold\",fontsize=40)\nplt.xlabel(\"Classes\",fontsize = 30)\nplt.xticks(rotation=20,fontsize = 20,fontweight = \"bold\")\nplt.xticks(fontsize = 20,fontweight = \"bold\")\nplt.ylabel(\"Count\",fontsize=30)\nplt.show()","286bc4e3":"#--------------\n#initialize constants\n#--------------\nHEIGHT,WIDTH = 512,512\nCHANNELS = 3\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nSEED = 143\nAUTO = tf.data.experimental.AUTOTUNE","1f712ba3":"def process_img(filepath,label):\n    image = tf.io.read_file(filepath)\n    image = tf.image.decode_jpeg(image, channels=CHANNELS)\n    image = tf.image.convert_image_dtype(image, tf.float32) \n    image = tf.image.resize(image, [HEIGHT,WIDTH])\n    return image,label\n\n\ndef data_augment(image, label):\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n            \n    # Flips\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    if p_spatial > .75:\n        image = tf.image.transpose(image)\n        \n    # Rotates\n    if p_rotate > .75:\n        image = tf.image.rot90(image, k=3) \n    elif p_rotate > .5:\n        image = tf.image.rot90(image, k=2) \n    elif p_rotate > .25:\n        image = tf.image.rot90(image, k=1) \n        \n    \n    if p_pixel_1 >= .4:\n        image = tf.image.random_saturation(image, lower=.7, upper=1.3)\n    if p_pixel_2 >= .4:\n        image = tf.image.random_contrast(image, lower=.8, upper=1.2)\n    if p_pixel_3 >= .4:\n        image = tf.image.random_brightness(image, max_delta=.1)\n        \n    \n    if p_crop > .7:\n        if p_crop > .9:\n            image = tf.image.central_crop(image, central_fraction=.7)\n        elif p_crop > .8:\n            image = tf.image.central_crop(image, central_fraction=.8)\n        else:\n            image = tf.image.central_crop(image, central_fraction=.9)\n    elif p_crop > .4:\n        crop_size = tf.random.uniform([], int(HEIGHT*.8), HEIGHT, dtype=tf.int32)\n        image = tf.image.random_crop(image, size=[crop_size, crop_size, CHANNELS])\n    \n    image = tf.image.resize(image, [HEIGHT,WIDTH])\n    return image,label\n\ndef get_dataset(filenames,labels, training=True):\n    dataset = tf.data.Dataset.from_tensor_slices((filenames,labels))\n    dataset = dataset.map(process_img,num_parallel_calls=AUTO)\n    dataset = dataset.map(data_augment,num_parallel_calls=AUTO)\n    dataset = dataset.cache()\n    dataset = dataset.repeat()\n    if training:\n        dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset","be934eba":"def create_model():\n    \n    pretrained = efn.EfficientNetB7(include_top=False, weights='noisy-student',input_shape=[HEIGHT,WIDTH, 3])\n            \n    x = pretrained.output\n    x = tf.keras.layers.GlobalAveragePooling2D() (x)\n    x = tf.keras.layers.Dense(512, activation = \"relu\") (x)\n    x = tf.keras.layers.Dropout(0.3) (x)\n    x = tf.keras.layers.Dense(256, activation = \"relu\") (x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.2) (x)\n    x = tf.keras.layers.GaussianDropout(0.4) (x)\n    outputs = tf.keras.layers.Dense(NUM_CLASSES,activation=\"sigmoid\", dtype='float32')(x)\n        \n    model = tf.keras.Model(pretrained.input, outputs)\n    return model\n\nmodel = create_model()\n#model.summary()","f7642756":"import tensorflow_addons as tfa\n\ndef compile_model(model, lr=0.001):\n    \n    optimizer = tf.keras.optimizers.Adam(lr=lr)\n    \n    loss = tf.keras.losses.BinaryCrossentropy()\n    #loss =  tfa.losses.SigmoidFocalCrossEntropy(name=\"loss\",alpha=1.0,gamma=2.0)\n    metrics = [\n       tfa.metrics.F1Score(num_classes = NUM_CLASSES,average = \"macro\", name = \"f1_score\"),\n       tf.keras.metrics.BinaryAccuracy(name='acc')\n    ]\n\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n    return model","9da1a644":"METRIC = \"val_f1_score\"\n\ndef create_callbacks(kfold,metric = METRIC):\n    \n    cpk_path = f'.\/best_model_{kfold}.h5'\n    \n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        filepath=cpk_path,\n        monitor= metric,\n        mode='max',\n        save_best_only=True,\n        verbose=1,\n    )\n\n    reducelr = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor= metric,\n        mode='max',\n        factor=0.1,\n        patience=3,\n        verbose=0\n    )\n\n    earlystop = tf.keras.callbacks.EarlyStopping(\n        monitor= metric,\n        mode='max',\n        patience=10, \n        verbose=1\n    )\n    \n    callbacks = [checkpoint, reducelr, earlystop]         \n    \n    return callbacks","c558ca51":"files_ls = tf.io.gfile.glob(TRAIN_PATH + '*.jpg')\nfiles_df = pd.DataFrame(files_ls, columns = [\"filepath\"])\n\nlabels = np.zeros((len(files_ls),NUM_CLASSES))\n\nfor i in range(len(files_ls)):\n    labels[i][train_df.iloc[i][\"labels\"]] = 1\n    \nprint(\"Total number of Images: \",len(files_ls))","adfefa0e":"EPOCHS = 20\nVERBOSE = 1\nN_SPLITS = 5\n\nkfold = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nhistory = {}\n\n\nfor fold,(tID,vID) in enumerate(kfold.split(files_ls,labels)):\n    tFiles, tLabels = list(files_df.iloc[tID][\"filepath\"]) , labels[tID]\n    vFiles, vLabels = list(files_df.iloc[vID][\"filepath\"]) , labels[vID]\n    print(\"Number of Training Images: \",len(tID))\n    print(\"Number of Validation Images: \",len(vID))\n    \n    STEPS_PER_EPOCH  = len(tID)\/\/BATCH_SIZE\n    VALID_STEPS = len(vID)\/\/BATCH_SIZE\n    \n    tf.keras.backend.clear_session()\n    \n    train_ds = get_dataset(tFiles,tLabels, training = True)\n    val_ds = get_dataset(vFiles, vLabels, training = False)\n    \n    with strategy.scope():\n        model = create_model()\n        model = compile_model(model, lr=0.0001)\n        callbacks = create_callbacks(kfold = fold)\n    \n        print(\"------------------Fold - \",fold+1,\" --------------------------\")\n        history[fold] = model.fit(\n                            train_ds,\n                            epochs=EPOCHS,\n                            callbacks=callbacks,\n                            validation_data = val_ds,\n                            verbose=VERBOSE,\n                            steps_per_epoch = STEPS_PER_EPOCH,\n                            validation_steps=VALID_STEPS\n                           )","5b988312":"plt.figure(figsize=(8*N_SPLITS,24))\n\nfor i in range(N_SPLITS):\n    acc = history[i].history['acc']\n    val_acc = history[i].history['val_acc']\n    f1 = history[i].history['f1_score']\n    val_f1 = history[i].history['val_f1_score']\n    loss = history[i].history['loss']\n    val_loss = history[i].history['val_loss']\n    epochs_range = range(len(history[i].history['val_loss'])) \n    \n    plt.subplot(N_SPLITS, 3,i*3+1)\n    plt.plot(epochs_range, acc, label='Training Accuracy')\n    plt.plot(epochs_range, val_acc, label='Validation  Accuracy')\n    plt.legend(loc='lower right')\n    plt.title(f'FOLD:{str(i)} Training and Validation  Accuracy')\n    \n    plt.subplot(N_SPLITS, 3,i*3+2)\n    plt.plot(epochs_range, f1, label='Training F1 score')\n    plt.plot(epochs_range, val_f1, label='Validation  F1 score')\n    plt.legend(loc='lower right')\n    plt.title(f'FOLD:{str(i)} Training and Validation  F1 score')\n    \n    plt.subplot(N_SPLITS, 3, i*3+3)\n    plt.plot(epochs_range, loss, label='Training Loss')\n    plt.plot(epochs_range, val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.title(f'FOLD:{str(i)} Training and Validation Loss')\n\nplt.show()","c6bdf1b9":"### It's a biased data\n\nAlso the image size are varying as follows:\n## height = [1728,4032], width = [2592,5184]","c7fca02f":"### If you have any doubts or suggestions feel free to contact me.\n## Happy coding\u2764 ","5f9cc977":"# Data Preprocessing Functions","693a9233":"# Compiling the Model","aba1d582":"# Plant Pathology 2021 - FGVC8\n<img src=\"https:\/\/www.researchgate.net\/profile\/Seung-Yeol-Lee\/publication\/282210822\/figure\/fig1\/AS:502574476660736@1496834500292\/Comparison-of-leaves-with-apple-blotch-disease-and-apple-blotch-like-symptom-A-H-apple.png\" width=\"400\" height=\"400\" \/>\n\n## Description\nApples are one of the most important temperate fruit crops in the world. Foliar (leaf) diseases pose a major threat to the overall productivity and quality of apple orchards. The current process for disease diagnosis in apple orchards is based on manual scouting by humans, which is time-consuming and expensive.\n\nAlthough computer vision-based models have shown promise for plant disease identification, there are some limitations that need to be addressed. Large variations in visual symptoms of a single disease across different apple cultivars, or new varieties that originated under cultivation, are major challenges for computer vision-based disease identification. These variations arise from differences in natural and image capturing environments, for example, leaf color and leaf morphology, the age of infected tissues, non-uniform image background, and different light illumination during imaging etc.\n\n## About CVPR\nThis competition is part of the Fine-Grained Visual Categorization FGVC8 workshop at the Computer Vision and Pattern Recognition Conference CVPR 2021. A panel will review the top submissions for the competition based on the description of the methods provided. From this, a subset may be invited to present their results at the workshop. Attending the workshop is not required to participate in the competition, however only teams that are attending the workshop will be considered to present their work.","e61d9af7":"# History plotting","e15d530c":"# Callbacks Function","838312c4":"# Model","4bcdb93b":"# Training","6f5f788f":"# Data Visualization","335b2ee7":"# Which type of competition is this ? (Multi class or Multi label)\n### Many have doubt whether the follwing competition is multi-class or multi-label . Let me clarify this by giving more details.\n\nIn the details of data it is mentioned as follows:\n> ## **Unhealthy leaves with too many diseases to classify visually will have the complex class, and may also have a subset of the diseases identified.**\n\nLet me show you some labels from `train.csv` file\n\n> **`8002cb321f8bfcdf.jpg     scab frog_eye_leaf_spot complex`**\n\n> **`801f78399a44e7af.jpg     complex`**\n\n> **`80769797ce42f658.jpg      scab frog_eye_leaf_spot`**\n\nYou can see that the image with complex label may have some other diseases mentioned. But from data I got **number of unique labels as 12**. So I considered it as **`Multi-Class classification`** and proceded. \n\n### **In this version I am using MULTI-LABEL CLASSIFICATION**"}}