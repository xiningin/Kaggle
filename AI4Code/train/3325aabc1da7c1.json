{"cell_type":{"764a7562":"code","96d0ec4b":"code","6b850902":"code","5a43e560":"code","4d53f281":"code","f0ee5013":"code","a32f860b":"code","36ca3947":"code","5e7dc045":"code","76274103":"code","6f53c1a5":"code","cb355fe1":"code","62e91513":"code","0376e136":"code","9e085ca5":"code","7cc2ee3b":"code","17c9d6d2":"code","8fa7bd9b":"code","9a8ee8b4":"code","a0a0eefb":"code","872dc4c1":"code","87ecd7c8":"code","68e28cb3":"markdown","435c7e47":"markdown","9621e3f2":"markdown","f769aa52":"markdown","cabdf33c":"markdown","4b29dddd":"markdown","f8f1cc28":"markdown"},"source":{"764a7562":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","96d0ec4b":"train = pd.read_csv(\"..\/input\/learn-together\/train.csv\")\ntest = pd.read_csv(\"..\/input\/learn-together\/test.csv\")","6b850902":"train.head()\n","5a43e560":"train.shape","4d53f281":"train.nunique()","f0ee5013":"test.head()","a32f860b":"test.shape","36ca3947":"#nulls columns?\nmissing_val_count_by_column_test = (test.isnull().sum())\nprint(missing_val_count_by_column_test[missing_val_count_by_column_test > 0])","5e7dc045":"train.describe()","76274103":"train = train.drop([\"Id\"], axis = 1)\n\ntest_ids = test[\"Id\"]\ntest = test.drop([\"Id\"], axis = 1)","6f53c1a5":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier","cb355fe1":"X_train, X_valid, y_train, y_valid = train_test_split(train.drop(['Cover_Type'], axis=1), train['Cover_Type'], test_size=0.2)","62e91513":"X_train.shape, X_valid.shape, y_train.shape, y_valid.shape","0376e136":"#nodistancetoroad\nX_train_no_road = X_train.drop(['Horizontal_Distance_To_Roadways'],axis=1)\nX_valid_no_road = X_valid.drop(['Horizontal_Distance_To_Roadways'],axis=1)","9e085ca5":"model = RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train_no_road, y_train)","7cc2ee3b":"from sklearn.metrics import classification_report, accuracy_score","17c9d6d2":"model.score(X_train_no_road, y_train)","8fa7bd9b":"predictions = model.predict(X_valid_no_road)\naccuracy_score(y_valid, predictions)","9a8ee8b4":"X_test_no_road = test.drop(['Horizontal_Distance_To_Roadways'],axis=1)\nX_test_no_road.head()","a0a0eefb":"test_pred = model.predict(X_test_no_road)","872dc4c1":"# Save test predictions to file\noutput = pd.DataFrame({'ID': test_ids,\n                       'TARGET': test_pred})\noutput.to_csv('submission.csv', index=False)","87ecd7c8":"output.head()","68e28cb3":"Let's delete the Id column in the training set but store it for the test set before deleting","435c7e47":"# Model Training","9621e3f2":"# Future work\n\n* Explorative Data Analysis to extract the most relevant features\n* Feature engineering\n* Cross-validation so we can use the entire training data\n* Grid-Search to find the optimal parameters for our classifier so we can fight overfitting\n* Try a different classifer. XgBoost for example (I suspect the winning solution will use an xgboost. highly recommended\n* Deep-learning ? hummm probably not. Overkill","f769aa52":"Our Model has a 100% accuracy on the training set and 86% on the test set. A clear example of overfitting. But we won't get into that cause this notebook is to get started.","cabdf33c":"Let's use 80% of the Data for training, and 20% for validation. We'll then train a simple Random Forest Classifier with 100 trees","4b29dddd":"# Predictions","f8f1cc28":"# Import Data"}}