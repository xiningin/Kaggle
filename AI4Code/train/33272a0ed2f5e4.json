{"cell_type":{"dff9ad71":"code","6547e4c7":"code","c923b99b":"code","0e8e6964":"code","c9fe2de6":"code","cdf266a6":"code","0dc61140":"code","bb3072f4":"code","48f3c299":"code","c558a58f":"code","fd8f0867":"code","2b6d5b3b":"code","1565898a":"code","0de417e0":"code","7916e7ca":"code","bf48b693":"code","25984060":"code","eb0509a5":"code","043163e4":"code","3e6161e6":"code","d46deb75":"code","c56e1adc":"code","bb9eb417":"code","398faf37":"code","01e2a5dc":"code","1b402cfb":"code","dc0acf7c":"code","eccfa67d":"code","eea5a886":"code","48468c44":"code","0f32e70a":"code","eeb029e8":"code","c026dd82":"code","12a4e9d0":"code","79be4dab":"code","3cafb08d":"code","9904876b":"code","6947d2fc":"code","5cb888dd":"code","321c8da4":"code","c8ce5069":"code","e1126b3c":"code","05a9ef97":"code","2b4f0f34":"markdown","9088a5d9":"markdown","1dbce170":"markdown","2b9ed3cf":"markdown","cff5498e":"markdown","d38fe112":"markdown","c379b0d4":"markdown","c40d7d08":"markdown","4d43dee7":"markdown","9ff7b8e5":"markdown","5e106494":"markdown","18ee3864":"markdown","c369bcb4":"markdown","55aa848d":"markdown","faa68e63":"markdown","df349f61":"markdown","026e3e88":"markdown","7fb5de97":"markdown","ba37499f":"markdown","26ba1f1f":"markdown","3e314583":"markdown","d897223c":"markdown","9b51a0b7":"markdown","26110ac6":"markdown","55ff95df":"markdown","764ed7df":"markdown","fc6c994e":"markdown","a0561fc3":"markdown","2ebca4e5":"markdown","c43ab706":"markdown","f124664c":"markdown","bdef6e03":"markdown","52bd6e49":"markdown","64fed8b9":"markdown","72db85d1":"markdown"},"source":{"dff9ad71":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams['figure.figsize'] = (16, 10)","6547e4c7":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_train['dataset'] = 'train'\ndf_test = pd.read_csv('..\/input\/test.csv')\ndf_test['dataset'] = 'test'\ndf = pd.concat([df_train, df_test], sort=True, copy=False)","c923b99b":"df_train.info()","0e8e6964":"df_train.nunique().sort_values()","c9fe2de6":"num_features = df_train.select_dtypes(['float64', 'int64']).columns.tolist()\ncat_features = df_train.select_dtypes(['object']).columns.tolist()\nprint('{} numerical features:\\n{} \\nand {} categorical features:\\n{}'.format(len(num_features), num_features, len(cat_features), cat_features))","cdf266a6":"num_features.remove('PassengerId')\nnum_features = sorted(num_features)\nnum_features","0dc61140":"df_train[num_features].describe()","bb3072f4":"print('{:.2f}% survival rate, {} out of {} survived'.format(df_train.Survived.sum()\/len(df_train)*100, df_train.Survived.sum(), len(df_train)))","48f3c299":"corrplot = sns.heatmap(df_train[num_features].corr(), cmap=plt.cm.Reds, annot=True)","c558a58f":"abs(df_train[num_features].corr()['Survived']).sort_values(ascending=False)","fd8f0867":"g = sns.FacetGrid(df_train, col='Survived')\ng.map(sns.distplot, 'Pclass')","2b6d5b3b":"df_train.groupby('Pclass').agg(['mean', 'count'])['Survived']","1565898a":"sns.boxplot(data=df, x='Fare', y='Pclass', orient='h')","0de417e0":"age_plot = sns.distplot(df_train[df_train.Age.notnull()].Age)","7916e7ca":"df_train.Age.isnull().sum()","bf48b693":"# remember to use df instead of df_train to transform dataset\ndf['Age'] = df[['Age']].applymap(lambda x: df.Age.mean() if pd.isnull(x) else x)","25984060":"print(df_train.groupby('SibSp').agg(['mean', 'count'])['Survived'])\nprint(df_train.groupby('Parch').agg(['mean', 'count'])['Survived'])","eb0509a5":"df['Family'] = df.SibSp + df.Parch\nprint(df[df.dataset == 'train'].groupby('Family').agg(['mean', 'count'])['Survived'])","043163e4":"df.drop(['SibSp', 'Parch'], axis=1, inplace=True)","3e6161e6":"fare_plot = sns.distplot(df_train.Fare)","d46deb75":"df_train['Fare_std'] = df_train[['Fare']].apply(lambda x: abs(x-x.mean())\/x.std())\ndf_train[['Fare', 'Fare_std']].sort_values('Fare_std', ascending=False).head(25)","c56e1adc":"# condition we want to remove is dataset == 'train' and Fare > 200\n# so we use negation\ndf = df[(df.dataset != 'train') | (df.Fare < 200)]","bb9eb417":"df_train[['Name', 'Ticket']].head(20)","398faf37":"df.drop(['Name', 'Ticket'], axis=1, inplace=True)","01e2a5dc":"g = sns.FacetGrid(df_train, col='Survived').map(sns.countplot, 'Sex')\ndf_train.groupby('Sex').agg(['mean', 'count'])['Survived']","1b402cfb":"df['Sex'] = df['Sex'].map({'female': 0, 'male': 1})","dc0acf7c":"print(df_train.Cabin.isnull().sum())\nprint(df_train.Cabin.nunique())\ndf_train.Cabin[df_train.Cabin.notnull()].head(10)","eccfa67d":"df['Cabin'] = df[['Cabin']].applymap(lambda x: 'Z' if pd.isnull(x) else x[0])","eea5a886":"# Lets create stacked plot\npivoted = df.groupby(['Cabin', 'Survived']).size().reset_index().pivot(index='Cabin', columns='Survived')\nstackedplot = pivoted.plot.bar(stacked=True)","48468c44":"# without Z\nstackedplot_withoutZ = pivoted.drop('Z').plot.bar(stacked=True)","0f32e70a":"df = pd.get_dummies(df, columns=['Cabin'], prefix='Cabin')","eeb029e8":"df.head()","c026dd82":"pivoted = df.groupby(['Embarked', 'Survived']).size().reset_index().pivot(index='Embarked', columns='Survived')\nstackedplot = pivoted.plot.bar(stacked=True)","12a4e9d0":"df = pd.get_dummies(df, columns=['Embarked'], prefix='Embarked')","79be4dab":"df.head()","3cafb08d":"# for splitting train and validate dataset\nfrom sklearn.model_selection import train_test_split\n\n# machine learning classifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier","9904876b":"X = df[df.dataset == 'train'].drop(['PassengerId', 'dataset', 'Survived'], axis=1)\ny = df[df.dataset == 'train']['Survived']\nX_test = df[df.dataset == 'test'].drop(['PassengerId', 'dataset', 'Survived'], axis=1)","6947d2fc":"# Split train and tes set from train dataset\nX_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size = 0.2)","5cb888dd":"# Train model\nmodel = RandomForestClassifier(n_estimators=1000)\nmodel.fit(X_train, y_train)","321c8da4":"model.score(X_validate, y_validate)","c8ce5069":"X_test.isnull().sum()","e1126b3c":"X_test['Fare'] = X_test[['Fare']].applymap(lambda x: df.Fare.median() if pd.isnull(x) else x)","05a9ef97":"# Create submission file\ny_test = model.predict(X_test)\nsubmission = pd.DataFrame(np.c_[df[df.dataset == 'test'].PassengerId, y_test.astype(int)], columns=['PassengerId','Survived'])\nsubmission.to_csv('submission.csv', index=False)","2b4f0f34":"For training model, we use scikit-learn package","9088a5d9":"We concate train and test dataset so every transformation on Feature Engineering will change both of them. But for EDA (Exploratory Data Analysis) we will using train dataset.","1dbce170":"## Embarked","2b9ed3cf":"the scale goes to high because too many Z value. So we must exclude Z value to see proportion of other values.","cff5498e":"we will split training dataset to 80% for training model and 20% for validate model","d38fe112":"We will remove Z-score > 3 thats mean Fare > 200","c379b0d4":"## Name & Ticket","c40d7d08":"No constant feature (feature with 1 unique value except dataset, it wont affect model so we want remove it first)","4d43dee7":"X contain all predictor feature from training dataset, \ny only contain dependent feature, thats what we want to predict,\nX_test same as X except it from test dataset which we want to submit","9ff7b8e5":"Seems only Pclass and Fare that has promising correlation to Survived, the other has low correlation.","5e106494":"11 predictor features (minus Survived and dataset). Age, Cabin and Embarked seems contain null value, we will take care of them later.","18ee3864":"Cabin has 687 NaN value and lot of 147 category. We will simplify with take first letter on Cabin and change NaN value to 'Z'","c369bcb4":"## Pclass","55aa848d":"We must fill Fare NaN value with median of Fare. We dont use mean because it contain outlier, and mean is sensitive to outlier value.","faa68e63":"## Sex","df349f61":"177 null value. There is some way to handle missing data, like delete it, replace with central value (mean \/ median), and interpolate it. But since we use title *Simple ML*, we just replace all missing data with mean of Age on all dataset.","026e3e88":"This is survival rate each SibSp and Parch, how about join them together","7fb5de97":"## Age","ba37499f":"Family feature makes difference more obvious. We will use Family and remove SubSp and Parch feature","26ba1f1f":"## Cabin","3e314583":"Normal distribution, outlier not detected. Nothing suspicious here.","d897223c":"Same as Cabin, we will create dummies values.","9b51a0b7":"Yep! safety proportional to cost we pay.","26110ac6":"From above we see that Survived rate more likely related to Pclass. Pclass 3 has the worst survival rate, the second 2 and the best is Pclass 1. So for safety purpose just buy class 1 ticket okay. But it must cost more money i think.","55ff95df":"Thats accuracy of our model. If you not satisfied with the result, you just need to re-run training model with different classifier or tuning some parameter. You can see all available parameter each classifier on scikit-learn documentation.\n\nOkay, now create submission file. But first lets check if any value NaN on test feature","764ed7df":"## Survived","fc6c994e":"Now we create convert Cabin to numerical feature.\nFor unique member k > 2 we can create feature with values 1, 2, 3,... but we cant assume Cabin A > Cabin B > Cabin C ...\nSo its better to use dummy values.","a0561fc3":"Actually, we can construct useful feature from Name and Ticket. But again, for simple ML we just remove this two feature and see the model performance.","2ebca4e5":"We will remove PassengerId because it doesnt mean anything. Now lets analyse 6 numerical features.","c43ab706":"Very clearly there is outlier > 200. Lets confirm with Z-score.","f124664c":"## SibSp & Parch","bdef6e03":"Around 74% female survived but male just 18%. This big difference make it key feature to predict Survived.\nSome ML algorithm wont do better if there is some non-numerical data, so we must convert it to numerical data.","52bd6e49":"Here is the public score so far:\n\nsvm.SVC() => 0.66028\n\nlinear_moel.LogisticRegression() => 0.74641\n\nensemble.RandomForestClassifier() => 0.75119\n\nensemble.RandomForestClassifier(n_estimators=1000) => 0.77033","64fed8b9":"# Training Model","72db85d1":"## Fare"}}