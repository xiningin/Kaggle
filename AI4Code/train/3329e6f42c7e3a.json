{"cell_type":{"73c3cba5":"code","c410135f":"code","69bf3ef3":"code","2d527e58":"code","6787ecf1":"code","29adb228":"code","daafc7f3":"code","661c3508":"code","9c8fb062":"code","54b9b34d":"code","6489882d":"code","96b30b09":"code","20df31e0":"code","75c48f97":"code","6dd343b2":"code","752772e9":"code","d7668b1b":"code","51feefee":"code","0cc69bf1":"code","a7690ada":"code","1693fb59":"code","87358d2d":"code","51f8784a":"code","865b0846":"code","5a72e850":"code","945e49ee":"code","17d39893":"code","c9a4d53f":"code","195eeea3":"code","c0ccebb9":"code","3173eaba":"code","ef29caca":"code","d0fd43cb":"code","91553405":"code","2f2ef6e0":"code","8ff1a835":"code","615fe4ff":"code","52335475":"code","6cdaa0ff":"code","925ec9af":"code","a0db471b":"code","4822e34e":"code","99fea8fd":"code","dcd03dba":"markdown","29e8a6a5":"markdown","15bf6059":"markdown","b5001f03":"markdown","675cc531":"markdown","db3042a9":"markdown","3294ebcd":"markdown","900df869":"markdown","e4b42301":"markdown","5fc26a04":"markdown","062e8498":"markdown","a75f38b5":"markdown","3b999c4f":"markdown","f26a8fb6":"markdown","b3c18518":"markdown","73986ddf":"markdown","5ad5b05f":"markdown","6867fb20":"markdown"},"source":{"73c3cba5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c410135f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n\n\n","69bf3ef3":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","2d527e58":"train.head()","6787ecf1":"train.info()","29adb228":"test.info()","daafc7f3":"frames = [train, test]\nall_data = pd.concat(frames, axis=0)\nall_data.shape","661c3508":"all_data.tail()","9c8fb062":"plt.figure(figsize=(15,8))\n\nax = sns.countplot(x='Pclass', data=all_data, palette=\"Blues_d\")\nfor p in ax.patches:\n    ax.annotate(int(p.get_height()), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.title('# of passengers in each class')\nplt.xticks( np.arange(3), ['1st class', '2nd class', '3rd class'])\nplt.show()","54b9b34d":"plt.figure(figsize=(20,8))\n\nplt.subplot(1,2,1)\nax = sns.countplot(x='Pclass', hue='Sex', data=all_data, palette=\"Reds_d\")\nfor p in ax.patches:\n    ax.annotate(int(p.get_height()), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.title('Proportion of male\/female in each class')\nplt.xticks( np.arange(3), ['1st class', '2nd class', '3rd class'])\n\nplt.subplot(1,2,2)\nax = sns.countplot(x='Pclass', hue='Survived', data=all_data, palette=\"Blues_d\")\nfor p in ax.patches:\n    ax.annotate(int(p.get_height()), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.title('Proportion of survived\/not survived in each class')\nplt.xticks( np.arange(3), ['1st class', '2nd class', '3rd class'])\nplt.legend(['Not Survived','Survived'], title='Survived')\n\nplt.show()","6489882d":"plt.figure(figsize=(20,8))\nax = sns.catplot(x = 'Pclass', hue = 'Survived', col = 'Sex', kind = 'count', data = all_data, palette='Blues_d')\nplt.xticks( np.arange(3), ['1st class', '2nd class', '3rd class'])\nplt.show()","96b30b09":"plt.figure(figsize=(15,8))\n\nax = sns.countplot(x='Sex', data=all_data, palette=\"Blues_d\")\nfor p in ax.patches:\n    ax.annotate(int(p.get_height()), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.title('# of Males and Females')\nplt.show()","20df31e0":"plt.figure(figsize=(18,5))\nbinsize=2\nbins = np.arange(0,all_data['Age'].max()+binsize,binsize)\nsns.distplot(all_data['Age'], bins=bins);\nplt.title('Distrubution of passengers age')\nplt.xlabel('Age')\nplt.ylabel('Frequency');","75c48f97":"all_data['Age'].describe()","6dd343b2":"plt.figure(figsize=(18,5))\npalette = 'Blues_d'\n\nplt.subplot(1,3,1)\nsns.boxplot(x='Sex', y='Age', data=all_data, palette=palette)\n\nplt.subplot(1,3,2)\nsns.boxplot(x='Survived', y='Age', data=all_data, palette=palette)\nplt.xticks( np.arange(2), ['not survived', 'survived']);\n\nplt.subplot(1,3,3)\nsns.boxplot(x='Pclass', y='Age', data=all_data, palette=palette)\nplt.xticks( np.arange(3), ['1st class', '2nd class', '3rd class']);","752772e9":"age_1_class_stat = pd.DataFrame(all_data.groupby('Sex')['Age'].describe())\nage_2_class_stat = pd.DataFrame(all_data.groupby('Sex')['Age'].describe())\nage_3_class_stat = pd.DataFrame(all_data.groupby('Sex')['Age'].describe())\n\npd.concat([age_1_class_stat, age_2_class_stat, age_3_class_stat], axis=0, sort = False, keys = ['1st', '2nd', '3rd'])","d7668b1b":"all_data.isna().sum()","51feefee":"all_data.drop('Cabin', axis=1, inplace=True)","0cc69bf1":"all_data['Age'] = all_data['Age'].fillna(all_data['Age'].mean())","a7690ada":"all_data['Fare'] = all_data['Fare'].fillna(all_data['Fare'].mean())\nall_data['Embarked'] = all_data['Embarked'].fillna('S')","1693fb59":"all_data['Rich_Woman'] = 0\nall_data['Rich_Man'] = 0\nall_data['Medium_Woman'] = 0\nall_data['Medium_Man'] = 0\nall_data['Poor_Woman'] = 0\nall_data['Poor_Man'] = 0\nall_data.loc[(all_data['Pclass']==1) & (all_data['Sex']=='female'), 'Rich_Woman'] = 1\nall_data.loc[(all_data['Pclass']==1) & (all_data['Sex']=='male'), 'Rich_Man'] = 1\nall_data.loc[(all_data['Pclass']==2) & (all_data['Sex']=='female'), 'Medium_Woman'] = 1\nall_data.loc[(all_data['Pclass']==2) & (all_data['Sex']=='female'), 'Medium_Man'] = 1\nall_data.loc[(all_data['Pclass']==3) & (all_data['Sex']=='female'), 'Poor_Woman'] = 1\nall_data.loc[(all_data['Pclass']==3) & (all_data['Sex']=='male'), 'Poor_Man'] = 1\n\nall_data = pd.concat([all_data, \n                           pd.get_dummies(all_data['Embarked'], \n                           prefix = 'embarked')], axis=1)","87358d2d":"all_data['Sex'] = all_data['Sex'].apply(lambda x: 1 if x=='male' else 0)\nall_data.head()","51f8784a":"all_data['Log_Age'] =  np.log1p(all_data['Age'])\nall_data['Log_Fare'] = np.log1p(all_data['Fare'])\nall_data.drop(['Age','Fare'], axis=1, inplace=True)","865b0846":"title_dict = {  'Mr':     'Mr',\n                'Mrs':    'Mrs',\n                'Miss':   'Miss',\n                'Master': 'Master',              \n                'Ms':     'Miss',\n                'Mme':    'Mrs',\n                'Mlle':   'Miss',\n                'Capt':   'military',\n                'Col':    'military',\n                'Major':  'military',\n                'Dr':     'Dr',\n                'Rev':    'Rev',                  \n                'Sir':    'honor',\n                'the Countess': 'honor',\n                'Lady':   'honor',\n                'Jonkheer': 'honor',\n                'Don':    'honor',\n                'Dona':   'honor' }\n\n\nall_data['Title'] = all_data['Name'].str.split(',', expand = True)[1].str.split('.', expand = True)[0].str.strip(' ')\n\nall_data['Title_category'] = all_data['Title'].map(title_dict)\n","5a72e850":"all_data = pd.concat([all_data, \n                           pd.get_dummies(all_data['Title_category'],\n                           prefix = 'title')], axis=1)\n","945e49ee":"print(all_data[(all_data['SibSp']==0)]['Survived'].mean())\nprint(all_data[(all_data['SibSp']==1)]['Survived'].mean())\nprint(all_data[(all_data['SibSp']==2)]['Survived'].mean())\nprint(all_data[(all_data['SibSp']==3)]['Survived'].mean())\nprint(all_data[(all_data['SibSp']==4)]['Survived'].mean())\nprint(all_data[(all_data['SibSp']==5)]['Survived'].mean())\nprint(all_data[(all_data['SibSp']==8)]['Survived'].mean())","17d39893":"all_data['Family_size'] = all_data['SibSp'] + all_data['Parch'] + 1\nall_data['Family_size_group'] = all_data['Family_size'].map(lambda x: 'f_single' if x == 1 \n                                                                        else ('f_usual' if 5 > x >= 2 \n                                                                              else ('f_big' if 8 > x >= 5 \n                                                                                   else 'f_large' )))\nall_data = pd.concat([all_data, \n                      pd.get_dummies(all_data['Family_size_group'], \n                      prefix = 'family')], axis=1)","c9a4d53f":"del all_data['PassengerId']\ndel all_data['Ticket']  \ndel all_data['Title_category']\ndel all_data['Name']\ndel all_data['Family_size']\ndel all_data['Family_size_group'] \ndel all_data['Embarked']    ","195eeea3":"del all_data['Title']","c0ccebb9":"all_data.info()","3173eaba":"test_id = test['PassengerId']\ntrain_1 = all_data[(all_data['Survived']==1) | (all_data['Survived']==0)]\ntest_1 = all_data[(all_data['Survived']!=1) & (all_data['Survived']!=0)]\n\nprint(train_1.shape)\nprint(test_1.shape)","ef29caca":"X = train_1.drop('Survived', axis=1)\ny = train_1['Survived']","d0fd43cb":"corr=X.corr(method='pearson')\nplt.figure(figsize=(25, 25))\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\npalette = sns.diverging_palette(50, 200, n=256)\nsns.heatmap(corr, annot=True, fmt=\".2f\", cmap='RdBu', mask= mask, vmin=-1, vmax=1, center= 0,\n            square=True, linewidths=2, cbar_kws={\"shrink\": .5})","91553405":"# Train and validaton errors initialized as empty list\ntrain_errs = list()\nvalid_errs = list()\nC_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n\n# Loop over values of C_value\nfor C_value in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n    # Create LogisticRegression object and fit\n    lr = LogisticRegression(C=C_value)\n    lr.fit(X, y)\n    \n    # Evaluate error rates and append to lists\n    train_errs.append( 1.0 - lr.score(X, y) )\n    valid_errs.append( 1.0 - lr.score(X, y) )\n    \n# Plot results\nplt.semilogx(C_values, train_errs, C_values, valid_errs)\nplt.legend((\"train\", \"validation\"))\nplt.xlabel('C inverse regularization strength')\nplt.ylabel('calssifaication errors')\nplt.show()","2f2ef6e0":"LR = LogisticRegression()\nparameters = {'C':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n              'penalty':['l1','l2']}\nsearcher_LR = GridSearchCV(LR, parameters, cv=10)\nsearcher_LR.fit(X, y)\n\n# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher_LR.best_params_)\nprint(\"Best CV accuracy\", searcher_LR.best_score_)","8ff1a835":"# Instantiate an RBF SVM\nsvm = SVC()\n\n# Instantiate the GridSearchCV object and run the search\nparameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\nsearcher_svm = GridSearchCV(svm, parameters)\nsearcher_svm.fit(X, y)\n\n# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher_svm.best_params_)\nprint(\"Best CV accuracy\", searcher_svm.best_score_)","615fe4ff":"# Instantiate an KNN\nknn = KNeighborsClassifier()\n\n# Instantiate the GridSearchCV object and run the search\nparameters = {'n_neighbors':[1, 3, 5, 7, 9]}\nsearcher_knn = GridSearchCV(knn, parameters)\nsearcher_knn.fit(X, y)\n\n# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher_knn.best_params_)\nprint(\"Best CV accuracy\", searcher_knn.best_score_)","52335475":"# We set random_state=0 for reproducibility \nlinear_classifier = SGDClassifier(random_state=0)\n\n# Instantiate the GridSearchCV object and run the search\nparameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n             'loss':['hinge','log'], 'penalty':['l1','l2']}\nsearcher_SGD = GridSearchCV(linear_classifier, parameters, cv=10)\nsearcher_SGD.fit(X, y)\n\n# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher_SGD.best_params_)\nprint(\"Best CV accuracy\", searcher_SGD.best_score_)","6cdaa0ff":"# Instantiate an Decision Tree\nDT = DecisionTreeClassifier()\n\n# Instantiate the GridSearchCV object and run the search\nparameters = {'criterion':['gini','entropy'],    \n              'max_depth':[2, 4, 6, 8, 10],\n              'min_samples_leaf':[1, 0.1, 0.01, 0.001]\n             }\nsearcher_dt = GridSearchCV(DT, parameters, cv=10)\nsearcher_dt.fit(X, y)\n\n# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher_dt.best_params_)\nprint(\"Best CV accuracy\", searcher_dt.best_score_)\n#print(\"Test accuracy of best grid search hypers:\", searcher_dt.score(X_test, y_test))","925ec9af":"# Instantiate an AdaBoostClassifier\nadb_clf = AdaBoostClassifier(base_estimator=DT)\n\n# Instantiate the GridSearchCV object and run the search\nparameters = {'n_estimators':[100, 200, 300, 400, 500, 600, 700, 800],\n              'learning_rate':[1, 0.1, 0.01, 0.001, 0.0001]\n             }\nsearcher_ada = GridSearchCV(adb_clf, parameters, cv=10)\nsearcher_ada.fit(X, y)\n\n# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher_ada.best_params_)\nprint(\"Best CV accuracy\", searcher_ada.best_score_)","a0db471b":"# Instantiate an Random Forest Classifier\nRF = RandomForestClassifier()\n\n# Instantiate the GridSearchCV object and run the search\nparameters = {'n_estimators':[100, 200, 300, 400, 500, 600, 700],\n              'min_samples_leaf':[1, 0.1, 0.01, 0.001, 0.0001]\n             }\nsearcher_rf = GridSearchCV(RF, parameters, cv=10)\nsearcher_rf.fit(X, y)\n\n# Report the best parameters and the corresponding score\nprint(\"Best CV params\", searcher_rf.best_params_)\nprint(\"Best CV accuracy\", searcher_rf.best_score_)","4822e34e":"test_1.drop('Survived',axis=1, inplace=True)\ny_pred = searcher_dt.predict(test_1)\ny_pred = y_pred.astype('int64')\n\nsubmission = pd.DataFrame({'PassengerId': test_id, 'Survived': y_pred})\nsubmission.to_csv(\"Titanic_submission.csv\", index = False)","99fea8fd":"submission.head()","dcd03dba":"1. the range of ages is nearly the same between males and females\n2. the range of ages in 1st class is higher than the two other classes\n","29e8a6a5":">I have 263 missing value in age column, 1 in fare column, 1014 in cabin column and 2 in embarked column.\n I will drop cabin column and impute the others.","15bf6059":"## 6. AdaBoostClassifier","b5001f03":"## 7. Random Forest Classifier","675cc531":"# Titanic Dataset\n#### Titanic problem is considered a classification problem. It's a good start for anyone to practise on solving this kind of problems. For me in this notebook, I will not train a model only but I will try to make some EDA (Exploratory Data Analysis) to get some useful insights from the data. Also, I will do some feature engineering. Feature engineering is about creating new input features from your existing ones. It will help a lot to improve any model I will choose.<br>\n#### But before doing anything I have to know What do I need to get from this data? What's the one million dollar question I need to answer?<br>\n#### And the question is: \n  > **What sorts of people were more likely to survive?**  \n  ","db3042a9":"# 1. Introduction","3294ebcd":"> 1. In each class, the number of males is greater than females<br>\n> 2. In the first class, the survived passengers are more than drowned passengers. On the contrary, In the 3rd class, the drowned passengers are greater than survived passengers<br>\n> 3. We can conclude that the more class higher the more chance to survive.\n","900df869":"## Feature Engineering","e4b42301":"> The females in 1st and 2nd classes have a high chance to survive but in 3rd class, the chance for survival is equal to drowning. For men, In the three classes the drowning is more common","5fc26a04":"**Our training set contains 12 columns but I will ignore PassengerId so that, It contains 11 feature including the 'Survived' (label feature) and the same for test set but without the label feature. How we will deal with this dataset?<br> \nMy strategy for this problem is to dig deeper into each feature trying to explore the data in addition to finding some relations between them. Then use these relations to impute the missingness. Also, doing some EDA (Exploratory Data Analysis) and data visualizations to extract other features (Feature Engineering).<br>\nAnd now let start our journey**","062e8498":"## 4. Stochastic Gradient Descent","a75f38b5":"## Machine Learning","3b999c4f":"## 5. Decision Tree","f26a8fb6":"# 2. Data Exploring","b3c18518":"# 3. Data Cleaning","73986ddf":"## 3. KNeighborsClassifier","5ad5b05f":"## 2. Support Vector Machine","6867fb20":"## 1. Logistic Regression"}}