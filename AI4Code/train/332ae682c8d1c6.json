{"cell_type":{"afb2088c":"code","3bd8b8a5":"code","0cd4b150":"code","fb0d28ac":"code","0b63ce80":"code","7c9381de":"code","023c52e4":"code","26feed5b":"code","28683a97":"code","b2689ef8":"code","ad61afd2":"code","c6ddab19":"code","e6afbd40":"code","92d46ff6":"code","658aba8c":"code","b33bef86":"code","41cf205b":"code","ac6d232f":"code","382fe9d4":"code","c71d22ba":"code","f6031638":"code","d86311a2":"code","dfc17300":"code","0bca6a45":"code","97e712a3":"code","cb527a7a":"code","363159f9":"code","cbf84a30":"code","a43e5b5a":"code","95da2b93":"code","7f19224e":"code","7a40ac13":"code","e9aa8914":"code","d675e50f":"code","382b68d5":"code","b1d7bf02":"code","45cb8cfd":"code","28da79ad":"code","ca117330":"code","3b1c163c":"code","34606f4f":"code","261b4407":"code","9fd1e4b5":"code","562bcb14":"code","daecc73e":"code","5a28d0b9":"code","73d2996a":"code","fca99fde":"code","90b2cc18":"code","3845711b":"code","2bafc2c8":"code","55d64f59":"code","d1c3e3d5":"code","4dab0c00":"code","e45b791b":"code","12eaf9d0":"code","3cd0859d":"code","c8a87c1d":"code","4e8150bb":"code","5d28b91b":"code","fd65ceda":"code","a86714ee":"code","7af5653b":"code","523cf26b":"code","f9cf7358":"code","d11d7c03":"code","feaf67fc":"code","cdfd1448":"code","a9bdcff6":"code","7138c999":"code","3ea0ee6e":"code","1bddde4c":"code","9c9940be":"code","d8496b41":"code","bee8897a":"code","d46a0f67":"code","7f87c3e7":"code","edac7e5f":"code","ed33f115":"code","4dadce26":"code","56f17a49":"code","c21c7c57":"code","1abb98a8":"code","afdc9dbd":"markdown","975c04e3":"markdown","66989c03":"markdown","01ddf70d":"markdown"},"source":{"afb2088c":"import pandas as pd\nfrom matplotlib import pyplot\nimport statistics\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","3bd8b8a5":"Data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nTest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nData1 = Data\ntarget = Data['SalePrice']\nData = Data.drop(['SalePrice'],axis=1)","0cd4b150":"target","fb0d28ac":"Data.info()\nData.shape","0b63ce80":"#number of unique values in Data \nData.apply(pd.Series.nunique)","7c9381de":"\nTest.apply(pd.Series.nunique)","023c52e4":"Test.info()\nTest.shape","26feed5b":"##lot of dataFeatures are given, segregating as per the type\n\nquantitative_data = [i for i in Data if Data[i].dtype == np.int64]\n\nprint(\"A total of {} quantitative columns are in data\".format(len(quantitative_data)))\nprint('\\n',quantitative_data,'\\n')\n\nqualitative_data = [i for i in Data if Data[i].dtype == np.object]\n\nprint(\"A total of {} object columns are in data\".format(len(qualitative_data)))\n\nprint('\\n',qualitative_data,'\\n')\n\ncontinuous_data = [i for i in Data if Data[i].dtype == np.float64]\n\nprint(\"A total of {} continuous columns are in data\".format(len(continuous_data)))\n\nprint('\\n',continuous_data,'\\n')\n\n","28683a97":"##lot of dataFeatures are given, segregating as per the type\n\nquantitative_data = [i for i in Test if Test[i].dtype == np.int64]\n\nprint(\"A total of {} quantitative columns are in data\".format(len(quantitative_data)))\nprint('\\n',quantitative_data,'\\n')\n\nqualitative_data = [i for i in Test if Test[i].dtype == np.object]\n\nprint(\"A total of {} object columns are in data\".format(len(qualitative_data)))\n\nprint('\\n',qualitative_data,'\\n')\n\ncontinuous_data = [i for i in Test if Test[i].dtype == np.float64]\n\nprint(\"A total of {} continuous columns are in data\".format(len(continuous_data)))\n\nprint('\\n',continuous_data,'\\n')\n\n","b2689ef8":"#NullValues with the help of Heatmap\nsns.heatmap(Data.isnull(), cmap=\"inferno\")\nplt.figure(figsize=(16,9))\n","ad61afd2":"#NullValues with the help of Heatmap in Test case\nsns.heatmap(Test.isnull(), cmap=\"viridis\")","c6ddab19":"null = Test.isnull().sum().sort_values(ascending=False)\n#First sum and order all null values for each variable\npercent = (Test.isnull().sum()\/Test.isnull().count()).sort_values(ascending=False) \ndataType = Test.dtypes\nmissing_variables = pd.concat([null, percent,dataType], axis=1, keys=['Total', 'percent','Data Type'])\nmissing_variables_test = missing_variables.head(19)","e6afbd40":"missing_variables_test","92d46ff6":"null = Data.isnull().sum().sort_values(ascending=False)\n#First sum and order all null values for each variable\npercent = (Data.isnull().sum()\/Data.isnull().count()).sort_values(ascending=False) \ndataType = Data.dtypes\nmissing_variables = pd.concat([null, percent,dataType], axis=1, keys=['Total', 'percent','Data Type'])\nmissing_variables = missing_variables.head(19)","658aba8c":"missing_variables","b33bef86":"#too many missing values ,dropping them \n\nData = Data.drop(['PoolQC','MiscFeature','Alley','Fence'],axis=1)\n","41cf205b":"#too many missing values ,dropping them \n\nTest = Test.drop(['PoolQC','MiscFeature','Alley','Fence'],axis=1)\n","ac6d232f":"Test.shape","382fe9d4":"m = np.ones_like(Data1.drop(columns = 'SalePrice').corr())\nm[np.tril_indices_from(m)]=0","c71d22ba":"plt.figure(figsize = (15,10))\nsns.heatmap(Data1.drop(columns = 'SalePrice').corr(), annot= True, annot_kws= {'size' : 12},\n           cmap = 'Blues', fmt = '.2f', linewidths= 2, linecolor='white', mask = m,vmin=-1)\nplt.show();","f6031638":"missing_variables_test","d86311a2":"#sperating the categorical var for impuring and encoding\n\nCategorical_test = Test.select_dtypes(include='object') \n\nCategorical_test = Categorical_test.drop(['FireplaceQu', 'GarageType',\n 'GarageQual', 'GarageCond', 'GarageFinish', 'BsmtFinType2', 'BsmtExposure',\n 'BsmtCond', 'BsmtFinType1','BsmtQual'],axis=1)","dfc17300":"Categorical_test.shape","0bca6a45":"Demo_Test = Test[['FireplaceQu', 'GarageType',\n 'GarageQual', 'GarageCond', 'GarageFinish', 'BsmtFinType2', 'BsmtExposure',\n 'BsmtCond', 'BsmtFinType1','BsmtQual']]","97e712a3":"print(\"missing value before Imputing the Train Data set\",Demo_Test.isnull().sum())","cb527a7a":"from sklearn.impute import SimpleImputer\n\nimp = SimpleImputer(missing_values=np.nan,strategy='most_frequent')\n\nDemo_Test = imp.fit_transform(Demo_Test)\nDemo_Test=pd.DataFrame(Demo_Test)","363159f9":"print(\"missing value before Imputing in the Train Data set\",Demo_Test.isnull().sum())","cbf84a30":"Data.shape","a43e5b5a":"Demo_Test.shape","95da2b93":"Categorical_test.shape","7f19224e":"#sperating the categorical var for impuring and encoding\n\nCategorical = Data.select_dtypes(include='object') \n\nCategorical = Categorical.drop(['FireplaceQu', 'GarageType',\n 'GarageQual', 'GarageCond', 'GarageFinish', 'BsmtFinType2', 'BsmtExposure',\n 'BsmtCond', 'BsmtFinType1','BsmtQual'],axis=1)","7a40ac13":"Demo = Data[['FireplaceQu', 'GarageType',\n 'GarageQual', 'GarageCond', 'GarageFinish', 'BsmtFinType2', 'BsmtExposure',\n 'BsmtCond', 'BsmtFinType1','BsmtQual']]","e9aa8914":"Demo.shape","d675e50f":"print(\"missing value before Imputing in the test data set\",Demo.isnull().sum())","382b68d5":"from sklearn.impute import SimpleImputer\n\nimp = SimpleImputer(missing_values=np.nan,strategy='most_frequent')\n\nDemo = imp.fit_transform(Demo)\nDemo=pd.DataFrame(Demo)","b1d7bf02":"print(\"missing value after Imputing in the Train data set\",Demo.isnull().sum())","45cb8cfd":"#putting them in the frameowrk on features to do encoding after imputing , finind var and passing float and integer for imputing - TRAIN DATA\n\nCategorical = pd.concat([Categorical,Demo],axis=1)\nIntegerData = Data.select_dtypes(include='int64') \nFloatData = Data.select_dtypes(include='float64') ","28da79ad":"#putting them in the frameowrk on features to do encoding after imputing , finind var and passing float and integer for imputing - TEST DATA\n\nCategorical_test = pd.concat([Categorical_test,Demo_Test],axis=1)\nIntegerData_test = Test.select_dtypes(include='int64') \nFloatData_test = Test.select_dtypes(include='float64') ","ca117330":"#Ealier we have listed all the datatypes that are there, now we select only the features with datatype Object in LabelEn and do Label encoding - TRAIN CASE\n\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nCategorical = Categorical.apply(label_encoder.fit_transform)","3b1c163c":"#Ealier we have listed all the datatypes that are there, now we select only the features with datatype Object in LabelEn and do Label encoding - TEST CASE\n\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nCategorical_test = Categorical_test.apply(label_encoder.fit_transform)","34606f4f":"sns.heatmap(IntegerData.isnull(), cmap=\"inferno\")\n","261b4407":"sns.heatmap(FloatData.isnull(), cmap=\"inferno\")","9fd1e4b5":"sns.heatmap(IntegerData_test.isnull(), cmap=\"viridis\")\n","562bcb14":"sns.heatmap(FloatData_test.isnull(), cmap=\"viridis\")\n","daecc73e":"#imputing for the null values in the test data set\nfrom sklearn.impute import SimpleImputer\n\nimp = SimpleImputer(missing_values=np.nan,strategy='mean')\n\nFloatData_test = imp.fit_transform(FloatData_test)\nFloatData_test=pd.DataFrame(FloatData_test)","5a28d0b9":"#imputing for the train case dataset \n\nfrom sklearn.impute import SimpleImputer\n\nimp = SimpleImputer(missing_values=np.nan,strategy='mean')\n\nFloatData.iloc[:,0:3] = imp.fit_transform(FloatData.iloc[:,0:3])\nFloatData.iloc[:,0:3]=pd.DataFrame(FloatData.iloc[:,0:3])","73d2996a":"sns.heatmap(FloatData_test.isnull(), cmap=\"RdYlGn\")\n\n","fca99fde":"features = pd.concat([FloatData,Categorical,IntegerData],axis=1)\n","90b2cc18":"features_Test = pd.concat([FloatData_test,Categorical_test,IntegerData_test],axis=1)\n","3845711b":"print(\"missing value before Imputing in the Train data set\",features_Test.isnull().sum())","2bafc2c8":"#Features with no null values and with encoding done\n\nsns.heatmap(features_Test.isnull(), cmap=\"RdYlGn\")\nfeatures_Test.head()","55d64f59":"print(\"missing value before Imputing in the Train data set\",features.isnull().sum())","d1c3e3d5":"#Features with no null values and with encoding done\n\nsns.heatmap(features.isnull(), cmap=\"RdYlGn\")\nfeatures.head()","4dab0c00":"#a zero variance column will always have exactly one distinct value\n\ntemp = []\nfor i in features.columns:\n    if features[i].var()==0:\n        temp.append(i)\nprint(len(temp))\nprint(temp)","e45b791b":"sns.boxplot(target)","12eaf9d0":"#no null values in target\n\ntarget.isnull().sum()\/target.shape[0] *100\n","3cd0859d":"features.shape\n\nmean = float(target.mean())\n\n\n\ntarget = target.mask(target > 400000, mean)\n                           \n#target = np.log1p(target)\n\nprint(\"number of null values\",target.isnull().sum())\n","c8a87c1d":"sns.boxplot(target)","4e8150bb":"features_Test.shape,features.shape\n","5d28b91b":"target.shape,features.shape","fd65ceda":"from statsmodels.formula.api import ols # ordinary least square \nimport statsmodels.api as sm","a86714ee":"    \nols_model = sm.OLS(target,features)\nols_results = ols_model.fit()\nprint(ols_results.summary())","7af5653b":"pred_val = ols_results.fittedvalues.copy()\ntrue_val= target.values.copy()\nresidual = true_val - pred_val","523cf26b":"# Histogram for distribution\nplt.figure(figsize=(8, 4), dpi=120, facecolor='w', edgecolor='b')\nplt.hist(residual, bins = 150)\nplt.xlabel('Error')\nplt.ylabel('Frequency')\nplt.title('Distribution of Error Terms')\nplt.show()","f9cf7358":"#residuals vs. predictions\n\nfig, ax = plt.subplots(figsize=(6,2.5))\n_ = ax.scatter(residual, pred_val)","d11d7c03":"# importing the QQ-plot from the from the statsmodels\n\n#The QQ-plot clearly verifies our findings from the the histogram of the residuals, the data is not exactly normal in nature, and there are some outliers on the higher end of the Residues.\n\nfrom statsmodels.graphics.gofplots import qqplot\n\n## Plotting the QQ plot\nfig, ax = plt.subplots(figsize=(5,5) , dpi = 120)\nqqplot(residual, line = 's' , ax = ax)\nplt.ylabel('Residual Quantiles')\nplt.xlabel('Ideal Scaled Quantiles')\nplt.title('Checking distribution of Residual Errors')\nplt.show()","feaf67fc":"\n\nplt.figure(figsize=(8, 4), dpi=120, facecolor='w', edgecolor='b')\nf = range(0,len(features))\nk = [0 for i in range(0,len(features))]\nplt.scatter( f,residual, label = 'Residuals')\nplt.plot( f, k , color = 'red', label = 'Mean' )\nplt.xlabel('fitted points ')\nplt.ylabel('residuals')\nplt.title('Residual plot')\n#plt.ylim(-4000, 4000)\nplt.legend();","cdfd1448":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=1)","a9bdcff6":"#standardizing the data\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n\nTEST = sc.fit_transform(features_Test)\nTEST=pd.DataFrame(TEST)\n\n","7138c999":"X_train.shape","3ea0ee6e":"y_test.shape","1bddde4c":"from sklearn.linear_model import Ridge, Lasso, LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import r2_score \nfrom sklearn import metrics\n","9c9940be":"algos = [LinearRegression(),  Ridge(), Lasso(),\n          KNeighborsRegressor(), DecisionTreeRegressor(),RandomForestRegressor(),AdaBoostRegressor()]\n\nnames = ['Linear Regression', 'Ridge Regression', 'Lasso Regression',\n         'K Neighbors Regressor', 'Decision Tree Regressor','RandomForestRegressor','AdaBoostRegressor']\n\nrmse_list = []\n","d8496b41":"for name in algos:\n    model = name\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    MSE= metrics.mean_squared_error(y_test,y_pred)\n    rmse = np.sqrt(MSE)\n    rmse_list.append(rmse)\n   \n","bee8897a":"evaluation = pd.DataFrame({'Model': names,\n                           'RMSE': rmse_list})","d46a0f67":"evaluation","7f87c3e7":"import xgboost as xgb\nD_train  = xgb.DMatrix(X_train, label = y_train)\nD_test = xgb.DMatrix(X_test)\nparam = {\n       'eta' : 0.3,\n    'max_depth' : 3,\n    'objective': 'reg:squarederror',\n}","edac7e5f":"model = xgb.train(param, D_train, 20)\ny_pred = model.predict(D_test)\n\nr2_score(y_test, y_pred)\n","ed33f115":"plt.scatter(y_test, y_pred)\nplt.colorbar()","4dadce26":"model = RandomForestRegressor()\nmodel.fit(features, target)\ny_predict_Test = model.predict(TEST)","56f17a49":"submission=pd.DataFrame()\n\nsample_submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n\nsubmission['Id']= sample_submission.Id\n\nsubmission['SalePrice']=y_predict_Test\n\n\n","c21c7c57":"submission","1abb98a8":"\n\nsubmission.to_csv('sales_price.csv',index=False)","afdc9dbd":">  Community Please help me with your valuable feedback for me to hone my skills ****","975c04e3":"# testing on the TEST DATA SET","66989c03":"**** Homoscedasticity ","01ddf70d":"https:\/\/www.datarobot.com\/blog\/ordinary-least-squares-in-python\/#:~:text=OLS%20class%20and%20and%20its,is%20the%20number%20of%20predictors.  Refer this blog for Details on OLS test"}}