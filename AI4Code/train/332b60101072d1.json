{"cell_type":{"6a0dbff3":"code","8f6a5724":"code","97102c16":"code","274510f2":"code","aaa881eb":"code","27a04a75":"code","c7420685":"code","70d05176":"code","e01cbb20":"code","f8d2a9ff":"code","b2e2fe8c":"code","d0575fde":"code","ebd9ca16":"code","afce86e4":"code","e18b669e":"markdown","d9e6da7b":"markdown","45e737be":"markdown","0cb7c26e":"markdown","cf18a8ea":"markdown","ec7824c9":"markdown"},"source":{"6a0dbff3":"!apt install aptitude -y\n!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n!pip install mecab-python3==0.996.6rc2\n\n!pip install unidic-lite","8f6a5724":"import gc\nimport os\nimport pandas as pd\nimport numpy as np\n\nimport MeCab\n\nimport tqdm\nimport pickle\n\nfrom transformers import *","97102c16":"class config:\n    OUTPUT = \"\/kaggle\/working\"\n    MAX_LEN = 512\n    TOKENIZER = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku\/bert-base-japanese\")\n\nlen(config.TOKENIZER)","274510f2":"!wget https:\/\/jaqket.s3-ap-northeast-1.amazonaws.com\/data\/train_questions.json 2> \/dev\/null\n!wget https:\/\/jaqket.s3-ap-northeast-1.amazonaws.com\/data\/dev1_questions.json 2> \/dev\/null\n!wget https:\/\/jaqket.s3-ap-northeast-1.amazonaws.com\/data\/dev2_questions.json 2> \/dev\/null\n!wget https:\/\/www.nlp.ecei.tohoku.ac.jp\/projects\/aio\/labeled_entities.txt 2> \/dev\/null\n!wget https:\/\/jaqket.s3-ap-northeast-1.amazonaws.com\/data\/candidate_entities.json.gz 2> \/dev\/null\n!wget https:\/\/www.nlp.ecei.tohoku.ac.jp\/projects\/AIP-LB\/static\/aio_leaderboard.json 2> \/dev\/null\n!gzip -d candidate_entities.json.gz\n!ls","aaa881eb":"df_train_questions = pd.read_json('train_questions.json', orient='records', lines=True)\ndf_train_questions.head(1)","27a04a75":"df_dev1_questions = pd.read_json('dev1_questions.json', orient='records', lines=True)\ndf_dev1_questions.head(1)","c7420685":"df_dev2_questions = pd.read_json('dev2_questions.json', orient='records', lines=True)\ndf_dev2_questions.head(1)","70d05176":"df_candidate_entities = pd.read_json('candidate_entities.json', orient='records', lines=True)\ndf_candidate_entities.head(1)","e01cbb20":"df_aio_leaderboard = pd.read_json(\"aio_leaderboard.json\", orient='records', lines=True)\ndf_aio_leaderboard.head(1)","f8d2a9ff":"def tokenize_dataframe(df):\n    inputs = []\n    for idx in tqdm.notebook.tqdm(range(len(df))):\n        question = df.loc[idx, \"question\"]\n        answer_candidates = df.loc[idx, \"answer_candidates\"]\n\n        _ids, _mask, _id_type = [], [], [] \n        for c in answer_candidates:\n          context = df_candidate_entities[df_candidate_entities[\"title\"]==c][\"text\"].iloc[0]\n      \n          tok = config.TOKENIZER.encode_plus(context,\n                                             question,\n                                             add_special_tokens=True,\n                                             max_length=config.MAX_LEN,\n                                             truncation_strategy=\"only_first\",\n                                             pad_to_max_length=True)\n          _ids.append(tok[\"input_ids\"])\n          _mask.append(tok[\"attention_mask\"])\n          _id_type.append(tok[\"token_type_ids\"])\n\n        answer_entity = df.loc[idx, \"answer_entity\"]\n        \n        try:\n            label = answer_candidates.index(answer_entity)\n        except ValueError:\n            label = 999  # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u30e9\u30d9\u30eb\u306f\u3068\u308a\u3042\u3048\u305a999\u3068\u3057\u3066\u304a\u304f\n\n        d = {\n            \"input_ids\": _ids,\n            \"attention_mask\": _mask,\n            \"token_type_ids\": _id_type, \n            \"label\": label\n        }\n        \n        inputs.append(d)\n    return inputs","b2e2fe8c":"dev1 = tokenize_dataframe(df_dev1_questions)\n\ndel df_dev1_questions\ngc.collect()\n\nwith open(f\"{config.OUTPUT}\/dev1.pkl\", \"wb\") as f:\n    pickle.dump(dev1, f)","d0575fde":"dev2 = tokenize_dataframe(df_dev2_questions)\n\ndel df_dev2_questions \ngc.collect()\n\nwith open(f\"{config.OUTPUT}\/dev2.pkl\", \"wb\") as f:\n    pickle.dump(dev2, f)","ebd9ca16":"test = tokenize_dataframe(df_aio_leaderboard)\n\ndel df_aio_leaderboard \ngc.collect()\n\nwith open(f\"{config.OUTPUT}\/test.pkl\", \"wb\") as f:\n    pickle.dump(test, f)","afce86e4":"train = tokenize_dataframe(df_train_questions)\n\ndel df_train_questions, df_candidate_entities\ngc.collect()\n\nwith open(f\"{config.OUTPUT}\/train.pkl\", \"wb\") as f:\n  pickle.dump(train, f)","e18b669e":"\u5fc5\u8981\u306a\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","d9e6da7b":"# \u30af\u30a4\u30baAI\u738b \u5b66\u7fd2\u7528TokenID\u4f5c\u6210notebook","45e737be":"BertTokenizer\u3092\u4f7f\u3063\u3066\u5404\u6587\u7ae0\u3092TokenID\u306b\u5909\u63db\u3059\u308b","0cb7c26e":"\u5404\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f","cf18a8ea":"MeCab\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","ec7824c9":"\u5404\u30d5\u30a1\u30a4\u30eb\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9"}}