{"cell_type":{"7dd59985":"code","29daa890":"code","38bdb93e":"code","ad500067":"code","c14ddf18":"code","dba53ecb":"code","56ddb7b5":"code","f8aab47b":"code","e4b85a14":"code","9933cb14":"code","5eb798b1":"code","6790b7b2":"code","8cbf4d19":"code","bd4fb7a3":"code","fef60f52":"code","b0912b86":"code","7ae37fae":"code","e06c6e4c":"code","b3ed2aa8":"code","6dd770f2":"code","cd92891e":"code","32144946":"code","87b49044":"code","b54a66ee":"code","44e840f9":"code","02814c49":"code","ebc0084f":"code","ebae4328":"code","af0e2333":"code","aca0b02c":"code","5f9fa797":"code","b543262f":"code","8ffc8fd6":"code","24972437":"code","d755da25":"code","d3d51f9c":"code","d0cbc604":"code","ef190a32":"code","617914a5":"code","d2500c36":"code","9db9543f":"code","6cbb57fb":"markdown","cd2392c3":"markdown","2bcec109":"markdown","8e775d0b":"markdown","27bffafb":"markdown","e205371d":"markdown","6ce23b8c":"markdown","8dfcf1eb":"markdown","9b915063":"markdown","73bf7a0b":"markdown","45b1dced":"markdown","aa405a92":"markdown","358bf8d4":"markdown","464d92cd":"markdown","9fe3e377":"markdown","ec8bb6e9":"markdown","776e5b3c":"markdown","13e80db3":"markdown","2e3fe06d":"markdown","94a47ccf":"markdown","72207d02":"markdown","40782f35":"markdown"},"source":{"7dd59985":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nimport sklearn","29daa890":"import matplotlib as mpl\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nsns.set(style='white', context='notebook', palette='deep')\npylab.rcParams['figure.figsize'] = 12,8\nwarnings.filterwarnings('ignore')\nmpl.style.use('ggplot')\nsns.set_style('dark')\n%matplotlib inline\n","38bdb93e":"df_train=pd.read_csv(\"..\/input\/train.csv\")\ndf_test=pd.read_csv(\"..\/input\/test.csv\")  \n\n# NOTE:    0= Not Survived, 1= Survived","ad500067":"type(df_train)","c14ddf18":"type(df_test)","dba53ecb":"# Modify the graph above by assigning each species an individual color.\n\ng = sns.FacetGrid(df_train, hue=\"Survived\", col=\"Pclass\", margin_titles=True,\n                  palette={1:\"blue\", 0:\"orange\"})\ng=g.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend();","56ddb7b5":"df_train.plot(kind='scatter', x='Age', y='Fare',alpha = 0.5,color = 'seagreen')  #here alpha is color darkness of points","f8aab47b":"ax= sns.boxplot(x=\"Pclass\", y=\"Age\", data=df_train)\nax= sns.stripplot(x=\"Pclass\", y=\"Age\", data=df_train, jitter=True, edgecolor=\"black\")\nplt.show()","e4b85a14":"df_train.hist(figsize=(15,20));\nplt.figure();","9933cb14":"df_train[\"Age\"].hist();","5eb798b1":"df_train.Age.plot(kind = 'hist',bins = 5);","6790b7b2":"f,ax=plt.subplots(1,2,figsize=(20,10))\ndf_train[df_train['Survived']==0].Age.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='seagreen')\nax[0].set_title('Survived= 0')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\ndf_train[df_train['Survived']==1].Age.plot.hist(ax=ax[1],color='brown',bins=20,edgecolor='black')\nax[1].set_title('Survived= 1')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()","8cbf4d19":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndf_train['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=df_train,ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","bd4fb7a3":"sns.countplot('Pclass', hue='Survived', data=df_train)\nplt.title('Pclass: Sruvived vs Dead')\nplt.show()","fef60f52":"df_train.shape","b0912b86":"def check_missing_data(df):\n    flag=df.isna().sum().any()\n    if flag==True:\n        total = df.isnull().sum()\n        percent = (df.isnull().sum())\/(df.isnull().count()*100)\n        output = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n        data_type = []\n        for col in df.columns:\n            dtype = str(df[col].dtype)\n            data_type.append(dtype)\n        output['Types'] = data_type\n        return(np.transpose(output))\n    else:\n        return(False)\n    ","7ae37fae":" check_missing_data(df_train)","e06c6e4c":" check_missing_data(df_test)","b3ed2aa8":"print(df_train.info())\ndf_train.isnull().sum()","6dd770f2":"df_train['Age'].unique() #here lot of unique values are there, If we train the model with this ages we can get the model biased","cd92891e":"df_train[\"Pclass\"].value_counts()","32144946":"df_train.head(5)","87b49044":"df_test.head(5)","b54a66ee":"df_train.sample(5)","44e840f9":"df_train.groupby('Survived').count()","02814c49":"X = df_train.iloc[:, :-1].values  #ALL ROWS AND COLUMNS(-1 SAYS THAT\"EXCEPT LAST COLUMN\")\ny = df_train.iloc[:, -1].values   #ONLY LAST COLUMN WILL PRINTS","ebc0084f":"def simplify_ages(df):\n    df.Age = df.Age.fillna(-0.5)\n    bins = (-1, 0, 5, 12, 18, 25, 35, 60, 120)\n    group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\n    categories = pd.cut(df.Age, bins, labels=group_names)\n    df.Age = categories\n    return df\n\ndef simplify_cabins(df):\n    df.Cabin = df.Cabin.fillna('N')\n    df.Cabin = df.Cabin.apply(lambda x: x[0])\n    return df\n\ndef simplify_fares(df):\n    df.Fare = df.Fare.fillna(-0.5)\n    bins = (-1, 0, 8, 15, 31, 1000)\n    group_names = ['Unknown', '1_quartile', '2_quartile', '3_quartile', '4_quartile']\n    categories = pd.cut(df.Fare, bins, labels=group_names)\n    df.Fare = categories\n    return df\n\ndef format_name(df):\n    df['Lname'] = df.Name.apply(lambda x: x.split(' ')[0])\n    df['NamePrefix'] = df.Name.apply(lambda x: x.split(' ')[1])\n    return df    \n    \ndef drop_features(df):\n    return df.drop(['Ticket', 'Name', 'Embarked'], axis=1)\n\ndef transform_features(df):\n    df = simplify_ages(df)\n    df = simplify_cabins(df)\n    df = simplify_fares(df)\n    df = format_name(df)\n    df = drop_features(df)\n    return df\n\ndf_train = transform_features(df_train)\ndf_test = transform_features(df_test)\ndf_train.head()","ebae4328":"from sklearn import preprocessing\ndef encode_features(df_train, df_test):\n    features = ['Fare', 'Cabin', 'Age', 'Sex', 'Lname', 'NamePrefix']\n    df_combined = pd.concat([df_train[features], df_test[features]])\n    \n    for feature in features:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(df_combined[feature])\n        df_train[feature] = le.transform(df_train[feature])\n        df_test[feature] = le.transform(df_test[feature])\n    return df_train, df_test\n        \n        ","af0e2333":"df_train,df_test = encode_features(df_train,df_test)\ndf_train.head()\n","aca0b02c":"x_all = df_train.drop(['Survived', 'PassengerId'], axis=1)\ny_all = df_train['Survived']","5f9fa797":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test= train_test_split(x_all, y_all, test_size=0.3, random_state=100)","b543262f":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, accuracy_score\nrfc = RandomForestClassifier()\n#help(RandomForestClassifier())\n\n# Choose some parameter combinations to try\nparameters = { \n              'n_estimators': [4,6,9],\n              'max_features': [ 'sqrt'], #['sqrt','log2','auto']\n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2,3,5,10], \n              'min_samples_split': [2, 3, 5],\n              'min_samples_leaf': [1,5,8],\n             }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(rfc, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nrfc = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nrfc.fit(X_train, y_train)","8ffc8fd6":"rfc_prediction = rfc.predict(X_test)\nrfc_score=accuracy_score(y_test, rfc_prediction)\nprint(rfc_score)","24972437":"import xgboost as xgb\nxgboost = xgb.XGBClassifier(learning_rate=0.1, n_estimators=100, min_samples_split=200,min_samples_leaf=40,max_depth=2,max_features='sqrt',subsample=0.8,random_state=10).fit(X_train, y_train)\nxgb_prediction = xgboost.predict(X_test)\nxgb_score=accuracy_score(y_test, xgb_prediction)\nprint(xgb_score)","d755da25":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n","d3d51f9c":"logreg_prediction = logreg.predict(X_test)\nlogreg_score=accuracy_score(y_test, logreg_prediction)\nprint(logreg_score)","d0cbc604":"from sklearn.svm import SVC\nclf = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)\nclf.fit(X_train,y_train)\n","ef190a32":"svc_prediction= clf.predict(X_test)\nsvc_score=accuracy_score(y_test,svc_prediction)\nprint(svc_score)","617914a5":"X_train = df_train.drop(\"Survived\",axis=1)\nX_train = X_train.drop(\"PassengerId\",axis=1)\nX_test  = df_test.drop(\"PassengerId\",axis=1)\ny_train = df_train[\"Survived\"]\n\n\nX_train.head()","d2500c36":"import xgboost as xgb\nxgboost = xgb.XGBClassifier(learning_rate=0.1, n_estimators=100, min_samples_split=200,min_samples_leaf=40,max_depth=8,max_features='sqrt',subsample=0.8,random_state=10).fit(X_train, y_train)\nY_pred = xgboost.predict(X_test)","9db9543f":"submission_file = pd.DataFrame({\n        \"PassengerId\": df_test[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission_file.to_csv('submission_file.csv', index=False)","6cbb57fb":"Import the basic packages. I will setup the packages where ever it is required in the code cell.","cd2392c3":"**Box Plot**\n\nIn descriptive statistics, a box plot or boxplot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram.[wikipedia]","2bcec109":"**Histograms for the train data**","8e775d0b":"**SCATTER PLOT **\n","27bffafb":"# Metrics Evaluation\n## What is precision, recall, prediction, specificity, and F1-score?\n\n**PRECISION: **        Choose precision when you want to know how much correctly the alogorithm is classified the true positives(correctly showed positive   results whether he is Diabetic or not)\n\n**RECALL:**                Choose recall when you want to know how much it is showing wrongly classified as false positive (for ex: prediction (+ve)= Healthy, Prediction (\u2013ve)= Diabetic)\n\n**SPECIFICITY: **      If you don\u2019t want any false positives. Only the correct true positive will be choosen.\n\n**ACCURACY: **       How much percentage of your algorithm correctly classified.\n\nYes, accuracy is a great measure but only when you have symmetric datasets (false negatives & false positives counts are close), also, false negatives & false positives have similar costs.\n\nIf the cost of false positives and false negatives are different then F1 is your savior. F1 is best if you have an uneven class distribution.\n\n\n\n**Bottom Line is**\n\n\u2014 Accuracy value of 90% means that 1 of every 10 labels is incorrect, and 9 is correct.\n\u2014 Precision value of 80% means that on average, 2 of every 10 diabetic labeled student by our program is healthy, and 8 is diabetic.\n\u2014 Recall value is 70% means that 3 of every 10 diabetic people in reality are missed by our program and 7 labeled as diabetic.\n\u2014 Specificity value is 60% means that 4 of every 10 healthy people in reality are miss-labeled as diabetic and 6 are correctly labeled as healthy.\n\nIf you want to know more about go throgh this link https:\/\/towardsdatascience.com\/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124\n\n","e205371d":"# Random Forest","6ce23b8c":"Setting up the basic changes to the notebook i.e (graph sizes, colours of plots, etc,.","8dfcf1eb":"Import the data.\n\n**Each row is an observation (also known as : sample, example, instance, record) **\n\n**Each column is a feature (also known as: Prameter,Predictor, attribute, Independent Variable, input, regressor, Covariate)**","9b915063":"## Conclusion\nIn this I had explained about some of the machine learning models along with reference links. \n\nMore yet to come!!\n\n### I hope you find this kernel helpful and some UPVOTES would be very much appreciated\n","73bf7a0b":"# Logistic Regression ","45b1dced":" for more about matplotlib plots refer this link \n\n https:\/\/www.southampton.ac.uk\/~fangohr\/training\/python\/notebooks\/Matplotlib.html ","aa405a92":"Here we can plot using bins or binnin method, I will discuss about that in below comming cells. (main idea about binning is: it will show [0-10],[20-30], etc, age grops","358bf8d4":"**Transforming Features**\n\nData transformation is the process of converting data from one format or structure into another format or structure[wiki]\n\nAge\nCabin\nFare\nName\n\n","464d92cd":"Here lot of unique values are there in age column, If we train the model with this ages we can get the model will get biased.So like what I had said previously\n\n**\"Binning Concept\"**\n\nLearn more about Binning Concept from the below link.\n\nhttps:\/\/towardsdatascience.com\/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b","9fe3e377":"### References\n\n1. https:\/\/towardsdatascience.com\/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124\n2. https:\/\/www.kaggle.com\/mjbahmani\/a-comprehensive-ml-workflow-with-python (Thanks to mjbahmani for your detailed kernel)\n3. https:\/\/towardsdatascience.com\/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b\n ","ec8bb6e9":"# XGBOOST","776e5b3c":"**Data Preprocessing**\n\nData preprocessing refers to the transformations applied to our data before feeding it to the algorithm.\n\nData Preprocessing is a technique that is used to convert the **raw data** into a **clean data set**. In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis. there are plenty of steps for data preprocessing and we just listed some of them :\n\n**removing Target column (id)**\n\n**Sampling (without replacement)**\n\n**Dealing with Imbalanced Data**\n\n**Introducing missing values and treating them (replacing by average values)**\n\n**Noise filtering**\n\n**Data discretization**\n\n**Normalization and standardization**\n\n**PCA analysis**\n\n**Feature selection (filter, embedded, wrapper)****","13e80db3":"After adding at data, Check whether the data is in data frame or not","2e3fe06d":"**Exploratory Data Analysis**\n\n* Understanding the data in deeper way and removing of unwanted columns.\n* Visualization of data for better understanding.","94a47ccf":"# SVM(Support Vector Machine)\n","72207d02":"**Data Cleaning And Data Preparation For The Model**\n\nThe primary goal of data cleaning is to detect and remove errors and anomalies to increase the value of data in analytics and decision making.\nIf data cleaning and preparation has been done correctly 70% of the problem will get finished.\n\n","40782f35":"**Feature Encoding: Converting Categorical to Numerical**\n\nIn machine learning projects, one important part is feature engineering. It is very common to see categorical features in a dataset. However, our machine learning algorithm can only read numerical values. It is essential to encoding categorical features into numerical values.\n\nEncode labels with value between 0 and n_classes-1\n\nLabelEncoder can be used to normalize labels.\nIt can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels."}}