{"cell_type":{"5802f40e":"code","1e54ec00":"code","1b6db9d9":"code","6c3eb319":"code","2577bcf8":"code","36eb0523":"code","d0d9cf30":"code","850de814":"code","84526f1d":"code","eaee15da":"code","668cded3":"code","6ea2c118":"code","1febd382":"code","2b2829a2":"code","b366eb46":"code","27360e25":"code","7b90722a":"code","753728ab":"code","e49fbb73":"code","f1518ae1":"code","6cf54acd":"code","370a2add":"code","fad2a0a6":"code","0458dcfe":"code","47b2d15e":"code","3fbea95c":"code","e0b95f17":"code","28d176bc":"code","dd5f8f90":"code","4846e12c":"code","4d0f739c":"code","5385651c":"code","0a90ff98":"code","70e88ec4":"code","4adf3a5d":"code","67c8f93b":"code","e06e2a4d":"code","8077df12":"code","0bae3d5c":"code","0ccdc9bf":"code","74754474":"code","7d109342":"code","33157653":"code","4c497f4a":"code","ed01fb3f":"code","0c55f909":"code","0821c0ee":"code","6a501610":"code","29952050":"code","6ee5b82b":"code","61fb411e":"code","74c20a8f":"code","9f9bd282":"code","e25f5a01":"code","bcc0375c":"code","b48bf4ae":"code","84a0064b":"code","415bd48c":"code","390fe58e":"code","5e6bbbfc":"code","450ee2ab":"code","5261f078":"code","a3abd750":"code","fc5b1e83":"code","02ff200b":"code","8bc0e5ba":"code","72adde59":"code","c52d79fa":"code","0be45ed5":"code","1d9b74b6":"code","34b9c79f":"code","18d887aa":"code","cc2a2b6b":"code","ca66aa3b":"code","21850313":"code","9fb580a1":"code","acea2190":"code","9b8b736b":"code","18144b0c":"code","a1b388ab":"code","dae4371e":"code","2424c301":"code","441408d6":"markdown","a8f67efe":"markdown","a5df001a":"markdown","3599ee4a":"markdown","3a5069b1":"markdown","7eec2bf8":"markdown","d4470a0f":"markdown","2644b22a":"markdown","45850a5e":"markdown","ed26805c":"markdown","518b13a1":"markdown","449e6fd1":"markdown"},"source":{"5802f40e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","1e54ec00":"data_train = pd.read_csv('..\/input\/train.csv')\ndata_test = pd.read_csv('..\/input\/test.csv')","1b6db9d9":"data_train.head()","6c3eb319":"print(data_train.dtypes)\nprint(data_train.describe())\nprint(data_train.info())","2577bcf8":"data_train.label.value_counts()","36eb0523":"print(data_train.shape, data_test.shape)","d0d9cf30":"from sklearn.feature_extraction.text import CountVectorizer","850de814":"vectorizer = CountVectorizer()","84526f1d":"train_vector = vectorizer.fit_transform(data_train.sentence)\ntest_vector = vectorizer.transform(data_test.sentence)","eaee15da":"WordFrequency = pd.DataFrame({'Word': vectorizer.get_feature_names(), 'Count': train_vector.toarray().sum(axis=0)})","668cded3":"WordFrequency['Frequency'] = WordFrequency['Count'] \/ WordFrequency['Count'].sum()","6ea2c118":"plt.plot(WordFrequency.Frequency)\nplt.xlabel('Word Index')\nplt.ylabel('Word Frequency')\nplt.show()","1febd382":"WordFrequency_sort = WordFrequency.sort_values(by='Frequency', ascending=False)\nWordFrequency_sort.head()","2b2829a2":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.cross_validation import cross_val_score","b366eb46":"clf1 = MultinomialNB()","27360e25":"cross_val_acc = cross_val_score(clf1, train_vector, data_train.label.values, cv=10, scoring='accuracy')\nprint(cross_val_acc)\nprint(cross_val_acc.mean())","7b90722a":"clf1.fit(train_vector, data_train.label.values)\npredictions = clf1.predict(test_vector)","753728ab":"solution1 = pd.DataFrame(list(zip(data_test.sentence, predictions)), columns=['sentence', 'label'])","e49fbb73":"solution1.to_csv('.\/solution1_naive_bayes.csv', index=False)\n# Accuracy in testing data: 0.97461","f1518ae1":"from sklearn.ensemble import RandomForestClassifier\nclf2 = RandomForestClassifier(n_jobs=-1)","6cf54acd":"cross_val_acc2 = cross_val_score(clf2, train_vector, data_train.label.values, cv=10, scoring='accuracy')\nprint(cross_val_acc2)\nprint(cross_val_acc2.mean())","370a2add":"clf2.fit(train_vector, data_train.label.values)\nprediction2 = clf2.predict(test_vector)","fad2a0a6":"solution2 = pd.DataFrame(list(zip(data_test.sentence, prediction2)), columns=['sentence','label'])","0458dcfe":"solution2.to_csv('.\/solution2_random_forest.csv', index=False)\n# Accuracy in testing data: 0.97884","47b2d15e":"from pprint import pprint\nfrom sklearn.model_selection import GridSearchCV","3fbea95c":"pprint(clf2.get_params())","e0b95f17":"param_grid = {\n             'class_weight': ['balanced', None],\n             'criterion': ['gini', 'entropy'],\n             'max_depth': [None, 1, 5, 10],\n             'max_features': ['auto', 'log2', None],\n             'n_estimators': [5, 10, 20]}\ncv_clf2 = GridSearchCV(estimator=clf2, param_grid=param_grid, scoring='accuracy', verbose=0, n_jobs=-1)\ncv_clf2.fit(train_vector, data_train.label.values)\nbest_parameters = cv_clf2.best_params_\nprint('The best parameters for using RF model is: ', best_parameters)","28d176bc":"clf2_balanced_gini = RandomForestClassifier(class_weight='balanced', n_estimators=20)\nclf2_entropy = RandomForestClassifier(criterion='entropy', n_estimators=20)\nclf2_gini = RandomForestClassifier(n_estimators=20)","dd5f8f90":"RF_score1 = cross_val_score(clf2_balanced_gini, train_vector, data_train.label.values, cv=10, scoring='accuracy')\nprint(RF_score1)\nprint(RF_score1.mean())","4846e12c":"RF_score2 = cross_val_score(clf2_entropy, train_vector, data_train.label.values, cv=10, scoring='accuracy')\nprint(RF_score2)\nprint(RF_score2.mean())","4d0f739c":"RF_score3 = cross_val_score(clf2_gini, train_vector, data_train.label.values, cv=10, scoring='accuracy')\nprint(RF_score3)\nprint(RF_score3.mean())","5385651c":"clf2_balanced_gini.fit(train_vector, data_train.label.values)\nprediction2_tuned = clf2_balanced_gini.predict(test_vector)\nsolution2_tuned = pd.DataFrame(list(zip(data_test.sentence, prediction2_tuned)), columns=['sentence', 'label'])","0a90ff98":"solution2_tuned.to_csv('.\/solution2_RF_tuned.csv', index=False)","70e88ec4":"# Use Logistic Regression directly\nfrom sklearn.linear_model import LogisticRegression\nclf3_1 = LogisticRegression()","4adf3a5d":"cross_val_acc3_1 = cross_val_score(clf3_1, train_vector, data_train.label.values, cv=10, scoring='accuracy')\nprint(cross_val_acc3_1)\nprint(cross_val_acc3_1.mean())","67c8f93b":"pprint(clf3_1.get_params())","e06e2a4d":"param_grid = {'penalty': ['l1', 'l2'],\n             'class_weight': ['balanced', None],\n             'C': [0.1, 1, 10]\n             }\nclf3_2 = GridSearchCV(estimator=clf3_1, param_grid=param_grid, scoring='accuracy', verbose=1, n_jobs=-1)\nclf3_2.fit(train_vector, data_train.label.values)\nbest_param = clf3_2.best_params_\nprint('The best parameters for using LR model is: ', best_param)","8077df12":"clf3_2 = LogisticRegression(C=9.4)\ncross_val_acc3_2 = cross_val_score(clf3_2, train_vector, data_train.label.values, cv=10, scoring='accuracy')\nprint(cross_val_acc3_2)\nprint(cross_val_acc3_2.mean())","0bae3d5c":"clf3_1.fit(train_vector, data_train.label.values)\nclf3_2.fit(train_vector, data_train.label.values)\nprediction3_1 = clf3_1.predict(test_vector)\nprediction3_2 = clf3_2.predict(test_vector)","0ccdc9bf":"solution3_origin_LR = pd.DataFrame(list(zip(data_test.sentence, prediction3_1)), columns=['sentence', 'label'])\nsolution3_CV_LR = pd.DataFrame(list(zip(data_test.sentence, prediction3_2)), columns=['sentence', 'label'])","74754474":"solution3_origin_LR.to_csv('.\/solution3_origin_LR.csv', index=False)\n# Accuracy in testing data: 0.99083","7d109342":"solution3_CV_LR.to_csv('.\/solution3_CV_LR.csv', index=False)\n# Accuracy in testing data:0.99083","33157653":"import collections\nimport tensorflow as tf\nimport os\nimport nltk\nfrom keras.preprocessing import sequence","4c497f4a":"data_train.head()","ed01fb3f":"num_sentences = len(data_train)\nprint(num_sentences)","0c55f909":"maxLength = 0\nword_frequency = collections.Counter()","0821c0ee":"for idx, row in data_train.iterrows():\n    words = nltk.word_tokenize(row['sentence'].lower())\n    if len(words) > maxLength:\n        maxLength = len(words)\n    for word in words:\n        word_frequency[word] += 1\nprint(len(word_frequency))\nprint(maxLength)","6a501610":"maxFeatures = 2074\nvocab_size = maxFeatures + 2\nword2index = {x[0]: i+2 for i, x in enumerate(word_frequency.most_common(maxLength))}\n\nword2index['PAD'] = 0\nword2index['UNK'] = 1\nindex2word = {i:w for w, i in word2index.items()}","29952050":"data_X_in = np.empty((num_sentences, ), dtype=list)\ndata_y = np.zeros((num_sentences, ))\ni = 0\n\nfor index, row in data_train.iterrows():\n    words = nltk.word_tokenize(row['sentence'].lower())\n    seqs = []\n    for word in words:\n        if word in word2index:\n            seqs.append(word2index[word])\n        else:\n            seqs.append(word2index[\"UNK\"])\n    data_X_in[i] = seqs\n    data_y[i] = int(row['label'])\n    i += 1","6ee5b82b":"data_X_in = sequence.pad_sequences(data_X_in, padding='post', value=word2index['PAD'], maxlen=maxLength)","61fb411e":"print(data_X_in[:5])\nprint(data_X_in.shape)","74c20a8f":"print(data_train.sentence.head())","9f9bd282":"print(data_y[:5])\nprint(data_y.shape)","e25f5a01":"def data_generator(batch_size):\n    while True:\n        for i in range(0,len(data_X_in),batch_size):\n            if i + batch_size < len(data_X_in):\n                yield data_X_in[i:i + batch_size], data_y[i:i + batch_size]","bcc0375c":"batch_size = 24\nembedding_size = 100\nvocab_size = maxFeatures + 2\nnum_units = 64\nNUM_EPOCHS = 10","b48bf4ae":"import tflearn\ntf.reset_default_graph()\nconfig = tf.ConfigProto(log_device_placement=True,allow_soft_placement = True)\nconfig.gpu_options.allow_growth = True\nsession = tf.Session(config=config)","84a0064b":"with tf.device('\/gpu:1'):\n    initializer = tf.random_uniform_initializer(\n        -0.08, 0.08)\n    tf.get_variable_scope().set_initializer(initializer)\n    x = tf.placeholder(\"int32\", [None, None])\n    y = tf.placeholder(\"int32\", [None])\n    x_len = tf.placeholder(\"int32\",[None])\n    \n    learning_rate = tf.placeholder(tf.float32, shape=[])\n    \n    # embedding\n    embedding_encoder = tf.get_variable(\n        \"embedding_encoder\", [vocab_size, embedding_size],dtype=tf.float32)\n    encoder_emb_inp = tf.nn.embedding_lookup(\n        embedding_encoder, x)\n    \n    # Build RNN cell\n    encoder_cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n    \n    encoder_cell = tf.contrib.rnn.DropoutWrapper(cell=encoder_cell, output_keep_prob=0.75)\n    # Run Dynamic RNN\n    #   encoder_outputs: [max_time, batch_size, num_units]\n    #   encoder_state: [batch_size, num_units]\n    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n        encoder_cell, encoder_emb_inp,\n        sequence_length=x_len, time_major=False,dtype=tf.float32)\n    \n    model_logistic = tf.layers.dense(encoder_state[0],1)\n    model_pred = tf.nn.sigmoid(model_logistic)\n    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(y,tf.float32),logits=tf.reshape(model_logistic,(-1,)))\n    loss = tf.reduce_mean(loss)\n    optimizer = tf.train.AdamOptimizer().minimize(loss)","415bd48c":"session.run(tf.global_variables_initializer())","390fe58e":"import os\nimport sys\nimport time\n\nclass Dataset():\n    def __init__(self,data,label):\n        self._index_in_epoch = 0\n        self._epochs_completed = 0\n        self._data = data\n        self._label = label\n        assert(data.shape[0] == label.shape[0])\n        self._num_examples = data.shape[0]\n        pass\n\n    @property\n    def data(self):\n        return self._data\n    \n    @property\n    def label(self):\n        return self._label\n\n    def next_batch(self,batch_size,shuffle = True):\n        start = self._index_in_epoch\n        if start == 0 and self._epochs_completed == 0:\n            idx = np.arange(0, self._num_examples)  # get all possible indexes\n            np.random.shuffle(idx)  # shuffle indexe\n            self._data = self.data[idx]  # get list of `num` random samples\n            self._label = self.label[idx]\n\n        # go to the next batch\n        if start + batch_size > self._num_examples:\n            self._epochs_completed += 1\n            rest_num_examples = self._num_examples - start\n            data_rest_part = self.data[start:self._num_examples]\n            label_rest_part = self.label[start:self._num_examples]\n            idx0 = np.arange(0, self._num_examples)  # get all possible indexes\n            np.random.shuffle(idx0)  # shuffle indexes\n            self._data = self.data[idx0]  # get list of `num` random samples\n            self._label = self.label[idx0]\n\n            start = 0\n            self._index_in_epoch = batch_size - rest_num_examples #avoid the case where the #sample != integar times of batch_size\n            end =  self._index_in_epoch  \n            data_new_part =  self._data[start:end]  \n            label_new_part = self._label[start:end]\n            return np.concatenate((data_rest_part, data_new_part), axis=0),np.concatenate((label_rest_part, label_new_part), axis=0)\n        else:\n            self._index_in_epoch += batch_size\n            end = self._index_in_epoch\n            return self._data[start:end],self._label[start:end]\n\nclass ProgressBar():\n    def __init__(self,worksum,info=\"\",auto_display=True):\n        self.worksum = worksum\n        self.info = info\n        self.finishsum = 0\n        self.auto_display = auto_display\n    def startjob(self):\n        self.begin_time = time.time()\n    def complete(self,num):\n        self.gaptime = time.time() - self.begin_time\n        self.finishsum += num\n        if self.auto_display == True:\n            self.display_progress_bar()\n    def display_progress_bar(self):\n        percent = self.finishsum * 100 \/ self.worksum\n        eta_time = self.gaptime * 100 \/ (percent + 0.001) - self.gaptime\n        strprogress = \"[\" + \"=\" * int(percent \/\/ 2) + \">\" + \"-\" * int(50 - percent \/\/ 2) + \"]\"\n        str_log = (\"%s %.2f %% %s %s\/%s \\t used:%ds eta:%d s\" % (self.info,percent,strprogress,self.finishsum,self.worksum,self.gaptime,eta_time))\n        sys.stdout.write('\\r' + str_log)\n\ndef get_dataset(paths):\n    dataset = []\n    for path in paths.split(':'):\n        path_exp = os.path.expanduser(path)\n        classes = os.listdir(path_exp)\n        classes.sort()\n        nrof_classes = len(classes)\n        for i in range(nrof_classes):\n            class_name = classes[i]\n            facedir = os.path.join(path_exp, class_name)\n            if os.path.isdir(facedir):\n                images = os.listdir(facedir)\n                image_paths = [os.path.join(facedir,img) for img in images]\n                dataset.append(ImageClass(class_name, image_paths))\n  \n    return dataset\n\nclass ImageClass():\n    \"Stores the paths to images for a given class\"\n    def __init__(self, name, image_paths):\n        self.name = name\n        self.image_paths = image_paths\n  \n    def __str__(self):\n        return self.name + ', ' + str(len(self.image_paths)) + ' images'\n  \n    def __len__(self):\n        return len(self.image_paths)\n\ndef split_dataset(dataset, split_ratio, mode):\n    if mode=='SPLIT_CLASSES':\n        nrof_classes = len(dataset)\n        class_indices = np.arange(nrof_classes)\n        np.random.shuffle(class_indices)\n        split = int(round(nrof_classes*split_ratio))\n        train_set = [dataset[i] for i in class_indices[0:split]]\n        test_set = [dataset[i] for i in class_indices[split:-1]]\n    elif mode=='SPLIT_IMAGES':\n        train_set = []\n        test_set = []\n        min_nrof_images = 2\n        for cls in dataset:\n            paths = cls.image_paths\n            np.random.shuffle(paths)\n            split = int(round(len(paths)*split_ratio))\n            if split<min_nrof_images:\n                continue  # Not enough images for test set. Skip class...\n            train_set.append(ImageClass(cls.name, paths[0:split]))\n            test_set.append(ImageClass(cls.name, paths[split:-1]))\n    else:\n        raise ValueError('Invalid train\/test split mode \"%s\"' % mode)\n    return train_set, test_set","5e6bbbfc":"losses = []\nbeginning_lr = 0.1\ngen = data_generator(batch_size)\nfor one_epoch in range(0,1):\n    pb = ProgressBar(worksum=len(data_X_in))\n    pb.startjob()\n    for one_batch in range(0,len(data_X_in),batch_size):\n        batch_x,batch_y = gen.__next__()\n        batch_x_len = np.asarray([len(i) for i in batch_x])\n        batch_lr = beginning_lr \n        \n        _,batch_loss = session.run([optimizer,loss],feed_dict={\n            x:batch_x,\n            y:batch_y,\n            x_len:batch_x_len,\n            learning_rate:batch_lr,\n        })\n        pb.info = \"EPOCH {} batch {} lr {} loss {}\".format(one_epoch,one_batch,batch_lr,batch_loss)\n        pb.complete(batch_size)\n        losses.append(batch_loss)","450ee2ab":"%matplotlib inline\npd.DataFrame(losses).plot()","5261f078":"def predict_result(sent):\n    words = nltk.word_tokenize(row['sentence'].lower())\n    senttoken = [word2index.get(word,word2index['UNK']) for word in words]\n    inputx = np.asarray([senttoken])\n    inputx_len = np.asarray([len(senttoken)])\n    batch_predict = session.run(model_pred,feed_dict={\n            x:inputx,\n            x_len:inputx_len,\n        })[0]\n    return 1 if batch_predict > 0.5 else 0","a3abd750":"labels = []\nfor index, row in data_test.iterrows():\n    label = predict_result(row['sentence'])\n    labels.append(label)","fc5b1e83":"print(len(labels))","02ff200b":"solution_RNN1 = pd.DataFrame(list(zip(data_test.sentence, labels)), columns=['sentence', 'label'])","8bc0e5ba":"solution_RNN1.to_csv('.\/solution_RNN1.csv', index=False)","72adde59":"max_len = 0\nword_freq = collections.Counter()\nfor i in data_train.sentence.values:\n    words = [j.lower() for j in i.strip('\\n').split()]\n    if len(words) > max_len:\n        max_len = len(words)\n    for word in words:\n        word_freq[word] += 1\nprint(len(word_freq))\nprint(max_len)","c52d79fa":"maxFeatures = 2673\nvocab_size = maxFeatures + 2\nword2index = {x[0]: i+2 for i, x in enumerate(word_freq.most_common(maxLength))}\n\nword2index['PAD'] = 0\nword2index['UNK'] = 1\nindex2word = {i:w for w, i in word2index.items()}","0be45ed5":"data_X_in = np.empty((num_sentences, ), dtype=list)\ndata_y = np.zeros((num_sentences, ))\ni = 0\n\nfor index, row in data_train.iterrows():\n    words = [j.lower() for j in row['sentence'].split()]\n    seqs = []\n    for word in words:\n        if word in word2index:\n            seqs.append(word2index[word])\n        else:\n            seqs.append(word2index[\"UNK\"])\n    data_X_in[i] = seqs\n    data_y[i] = int(row['label'])\n    i += 1","1d9b74b6":"data_X_in = sequence.pad_sequences(data_X_in, padding='post', value=word2index['PAD'], maxlen=max_len)","34b9c79f":"print(data_X_in[:5])\nprint(data_X_in.shape)","18d887aa":"print(data_train.head())","cc2a2b6b":"print(data_y[:5])\nprint(data_y.shape)","ca66aa3b":"tf.reset_default_graph()\nconfig = tf.ConfigProto(log_device_placement=True,allow_soft_placement = True)\nconfig.gpu_options.allow_growth = True\n#config.gpu_options.per_process_gpu_memory_fraction = 0.4\nsession = tf.Session(config=config)\n\nwith tf.device('\/gpu:1'):\n    initializer = tf.random_uniform_initializer(\n        -0.08, 0.08)\n    tf.get_variable_scope().set_initializer(initializer)\n    x = tf.placeholder(\"int32\", [None, None])\n    y = tf.placeholder(\"int32\", [None])\n    x_len = tf.placeholder(\"int32\",[None])\n    \n    learning_rate = tf.placeholder(tf.float32, shape=[])\n    \n    # embedding\n    embedding_encoder = tf.get_variable(\n        \"embedding_encoder\", [vocab_size, embedding_size],dtype=tf.float32)\n    encoder_emb_inp = tf.nn.embedding_lookup(\n        embedding_encoder, x)\n    \n    # Build RNN cell\n    encoder_cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n    \n    encoder_cell = tf.contrib.rnn.DropoutWrapper(cell=encoder_cell, output_keep_prob=0.75)\n    # Run Dynamic RNN\n    #   encoder_outputs: [max_time, batch_size, num_units]\n    #   encoder_state: [batch_size, num_units]\n    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n        encoder_cell, encoder_emb_inp,\n        sequence_length=x_len, time_major=False,dtype=tf.float32)\n    \n    model_logistic = tf.layers.dense(encoder_state[0],1)\n    model_pred = tf.nn.sigmoid(model_logistic)\n    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(y,tf.float32),logits=tf.reshape(model_logistic,(-1,)))\n    loss = tf.reduce_mean(loss)\n    optimizer = tf.train.AdamOptimizer().minimize(loss)","21850313":"session.run(tf.global_variables_initializer())","9fb580a1":"losses = []\nbeginning_lr = 0.1\ngen = data_generator(batch_size)\nfor one_epoch in range(0,1):\n    pb = ProgressBar(worksum=len(data_X_in))\n    pb.startjob()\n    for one_batch in range(0,len(data_X_in),batch_size):\n        batch_x,batch_y = gen.__next__()\n        batch_x_len = np.asarray([len(i) for i in batch_x])\n        batch_lr = beginning_lr \n        \n        _,batch_loss = session.run([optimizer,loss],feed_dict={\n            x:batch_x,\n            y:batch_y,\n            x_len:batch_x_len,\n            learning_rate:batch_lr,\n        })\n        pb.info = \"EPOCH {} batch {} lr {} loss {}\".format(one_epoch,one_batch,batch_lr,batch_loss)\n        pb.complete(batch_size)\n        losses.append(batch_loss)","acea2190":"pd.DataFrame(losses).plot()","9b8b736b":"def predict_result(sent):\n    words = [j.lower() for j in row['sentence'].split()]\n    senttoken = [word2index.get(word,word2index['UNK']) for word in words]\n    inputx = np.asarray([senttoken])\n    inputx_len = np.asarray([len(senttoken)])\n    batch_predict = session.run(model_pred,feed_dict={\n            x:inputx,\n            x_len:inputx_len,\n        })[0]\n    return 1 if batch_predict > 0.5 else 0","18144b0c":"labels = []\nfor index, row in data_test.iterrows():\n    label = predict_result(row['sentence'])\n    labels.append(label)","a1b388ab":"print(len(labels))","dae4371e":"COLUMN_NAMES = ['sentence', 'label']\nsolution_RNN2 = pd.DataFrame(columns=COLUMN_NAMES)\n\nsolution_RNN2['sentence'] = data_test['sentence']\nll = pd.Series(labels)\nsolution_RNN2['label'] = ll.values\nprint(solution_RNN2.shape)","2424c301":"solution_RNN2.to_csv('.\/solution_RNN2.csv', index=False)","441408d6":"### Applied nltk.word_tokenize to do words spliting","a8f67efe":"## Model 3: Logistic Regression (Use GridSearchCV to tune hyper-parameters)","a5df001a":"### Visualize Word Frequency","3599ee4a":"## Model 2: Random Forest","3a5069b1":"## Read data","7eec2bf8":"### Use GridSearchCV","d4470a0f":"### Sort WordFrequency in descending order","2644b22a":"# Sentiment Classification","45850a5e":"### Use GridSearchCV","ed26805c":"## Model 4: RNN","518b13a1":"### Applied sentence.split() to do words spliting","449e6fd1":"## Model 1: Naive Bayes"}}