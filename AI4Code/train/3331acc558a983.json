{"cell_type":{"5b63449e":"code","3d123ed7":"code","636f051c":"code","becc8921":"code","81c2dcf3":"code","d2d35662":"code","ae787e54":"code","5d0dc5d7":"code","a51c4132":"code","f2ae0a07":"code","bed183b9":"code","e3cae47b":"code","2e4a565e":"code","72675b72":"code","37e4b167":"code","fcd28719":"code","a4c3fe8f":"code","e91e5b89":"code","79808b12":"code","9d29a889":"code","291fb83c":"code","10b3ad7d":"code","df9bdd19":"code","d8925bc7":"code","c50337c0":"code","eaab78d4":"code","da9f3495":"code","094436eb":"code","a24de946":"code","53294990":"code","c30b9377":"code","505c10b0":"code","aac4a8e5":"markdown","09f27629":"markdown","f609a014":"markdown","0ac01bb8":"markdown"},"source":{"5b63449e":"import numpy as np\nimport pandas as pd\n\nimport torch\n\nimport transformers","3d123ed7":"train = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")","636f051c":"train.head()","becc8921":"#from transformers import AutoTokenizer\n#from transformers import AutoModel\n\npath_model = '..\/input\/pretrained-transformers\/roberta_large_model'\npath_tokenizer = '..\/input\/pretrained-transformers\/roberta_large_tokenizer'\n\n#path_model = '..\/input\/clrp-roberta-base\/clrp_roberta_base'\n#path_tokenizer = '..\/input\/clrp-roberta-base\/clrp_roberta_base'\n\n#path_model = '..\/input\/pretrained-transformers\/roberta_base_model'\n#path_tokenizer = '..\/input\/pretrained-transformers\/roberta_base_tokenizer'\n\n\ntokenizer = transformers.RobertaTokenizer.from_pretrained(path_tokenizer)","81c2dcf3":"tokenizer","d2d35662":"from pprint import pprint\n\nprint(tokenizer(train['excerpt'][0]))","ae787e54":"from transformers import AutoModel\n\n\nmodel = AutoModel.from_pretrained(path_model, num_labels=1)","5d0dc5d7":"#model","a51c4132":"from torch.utils.data import Dataset, DataLoader","f2ae0a07":"class TextData(Dataset):\n    def __init__(self, text, labels, is_train = True, max_len=250):\n        self.text = text\n        self.labels = labels if is_train else None ###\n        self.is_train = is_train ###\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, item):\n        tokenized_text = tokenizer(\n            self.text[item].replace('\\n', ''), max_length=self.max_len, truncation=True, \n            return_attention_mask=True, return_token_type_ids=False)\n                           \n        padding_length = self.max_len - len(tokenized_text['input_ids'])\n        \n           \n        input_ids = torch.tensor(tokenized_text['input_ids'] + ([0] * padding_length), dtype=torch.long)\n        #token_type_ids = torch.tensor(tokenized_text['token_type_ids'] + ([0] * padding_length), dtype=torch.long)\n        attention_mask = torch.tensor(tokenized_text['attention_mask'] + ([0] * padding_length), dtype=torch.long)\n        \n        \n        if self.is_train:\n            label = torch.tensor(self.labels[item], dtype=torch.double)\n            \n        if self.is_train:\n            return {\n                'input_ids': input_ids, \n                #'token_type_ids': token_type_ids,\n                'attention_mask': attention_mask,                                   \n                'label': label\n            }\n        else:\n            return {\n                'input_ids': input_ids,\n                #'token_type_ids': token_type_ids,\n                'attention_mask': attention_mask                \n            }","bed183b9":"train_dataset = TextData(train.loc[:2000, 'excerpt'].values, train.loc[:2000, 'target'].values)\nvalid_dataset = TextData(train.loc[2000:, 'excerpt'].values, train.loc[2000:, 'target'].values)","e3cae47b":"train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=16)","2e4a565e":"next(iter(train_dataloader))","72675b72":"#next(iter(valid_dataloader))","37e4b167":"#from transformers import AutoModel\n\n#AutoModel.from_pretrained('bert-base-cased', output_hidden_states=False);","fcd28719":"from transformers import AutoConfig\nimport torch.nn as nn\n\nclass RegressionModel(torch.nn.Module):\n    \n    def __init__(self):\n        super(RegressionModel, self).__init__()\n        config = AutoConfig.from_pretrained(path_model)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.bert = AutoModel.from_pretrained(path_model, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(1024, 512),            \n            nn.Tanh(),                       \n            nn.Linear(1024, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 1)                        \n        )   \n\n    def forward(self, input_ids, attention_mask, label=None): #token_type_ids,\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n            #token_type_ids=token_type_ids,\n        )\n        \n        sequence_output = self.regressor(outputs.last_hidden_state[:,0])\n        #sequence_output = torch(self.regressor(self.dropout(outputs.last_hidden_state[:,-1,:])), 1)\n        \n        logits = (sequence_output)\n        \n        loss = None\n        if label is not None:\n            loss_fn = torch.nn.MSELoss()\n            logits = logits.view(-1).to(label.dtype)\n            loss = torch.sqrt(loss_fn(logits, label.view(-1)))\n        \n        output = (logits,) + outputs[1:]\n        return ((loss,) + output) if loss is not None else output","a4c3fe8f":"#RegressionModel()\nmodel = RegressionModel()","e91e5b89":"for param in model.bert.embeddings.parameters():\n    param.requires_grad = False\n\nfor param in model.bert.encoder.layer[0].parameters():\n    param.requires_grad = False \n\nfor param in model.bert.encoder.layer[1].parameters():\n    param.requires_grad = False\n\nfor param in model.bert.encoder.layer[2].parameters():\n    param.requires_grad = False\n\nfor param in model.bert.encoder.layer[3].parameters():\n    param.requires_grad = False\n    \nfor param in model.bert.encoder.layer[4].parameters():\n    param.requires_grad = False\n\nfor param in model.bert.encoder.layer[5].parameters():\n    param.requires_grad = False\n    \nfor param in model.bert.encoder.layer[6].parameters():\n    param.requires_grad = False\n    \nfor param in model.bert.encoder.layer[7].parameters():\n    param.requires_grad = False\n    \nfor param in model.bert.encoder.layer[8].parameters():\n    param.requires_grad = False\n    \nfor param in model.bert.encoder.layer[9].parameters():\n    param.requires_grad = False\n    \nfor param in model.bert.encoder.layer[10].parameters():\n    param.requires_grad = False\n    \nfor param in model.bert.encoder.layer[11].parameters():\n    param.requires_grad = False\n    \n#for param in model.bert.encoder.layer[12].parameters():\n#    param.requires_grad = False\n#    \n#for param in model.bert.encoder.layer[13].parameters():\n#    param.requires_grad = False\n#    \n#for param in model.bert.encoder.layer[14].parameters():\n#    param.requires_grad = False\n#    \n#for param in model.bert.encoder.layer[15].parameters():\n#    param.requires_grad = False","79808b12":"device = \"cuda\"\n\nmodel.to(device);","9d29a889":"from transformers import AdamW\n\noptimizer = torch.optim.AdamW([\n    {'params': model.bert.parameters(), 'lr': 0.00001},\n    {'params': model.regressor.parameters(), 'lr': 0.001}\n])","291fb83c":"from transformers import get_scheduler\n\nnum_epochs = 3\nnum_training_steps = num_epochs * len(train_dataloader)\n\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=10,\n    num_training_steps=num_training_steps\n)","10b3ad7d":"from tqdm.auto import tqdm\n\nprogress_bar = tqdm(range(num_training_steps))\n\nfor epoch in range(num_epochs):\n    \n    model.train()\n    \n    losses = []\n    \n    for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs[0]\n        loss.backward()\n        losses.append(loss.item())\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n        \n    print(f\"train RMSE: {np.array(losses).mean()}\")\n        \n    model.eval()\n    \n    losses = []\n    \n    for batch in valid_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = model(**batch)\n\n        loss = outputs[0].item()\n        \n        losses.append(loss)\n     \n    \n    print(f\"valid RMSE: {np.array(losses).mean()}\")","df9bdd19":"def get_preds(dataloader, model):\n    preds = []\n    with torch.no_grad():\n        for data, batch in enumerate(dataloader):\n            ids = batch[\"input_ids\"].to(device)\n            mask = batch[\"attention_mask\"].to(device)\n            #tok_types = batch[\"token_type_ids\"].to(device)\n            \n                        \n            output = model(ids, mask) #, tok_types)\n            output = output[0].squeeze(-1)\n\n            #output = output['logits'].squeeze(-1)\n            #output = output.view(-1).float() ##\n            #preds.append(output.detach().cpu().numpy())\n            preds.extend(output.float().detach().to('cpu').tolist())##\n    return preds   ","d8925bc7":"test_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ntest_dataset = TextData(test_df['excerpt'], None, False, 250)\ntest_loader = DataLoader(test_dataset, batch_size=16, num_workers=8) #DataLoader","c50337c0":"all_preds = get_preds(test_loader, model)\n#predicts = all_preds[0].detach().to('cpu').tolist()\npredicts = all_preds","eaab78d4":"predicts","da9f3495":"pd.DataFrame(all_preds).T.mean(axis=1)","094436eb":"predicts = pd.DataFrame(predicts)","a24de946":"submit = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsubmit['target'] = predicts","53294990":"submit.to_csv(\"submission.csv\",index = False)","c30b9377":"#torch.cuda.empty_cache()","505c10b0":"submit","aac4a8e5":"train RMSE: 0.70108438117762\nvalid RMSE: 0.591509985725852\n\ntrain RMSE: 0.4599159102574394\nvalid RMSE: 0.5314388177489617\n\ntrain RMSE: 0.3041276327755984\nvalid RMSE: 0.508451227175764\n\ntrain RMSE: 0.2022837107640159\nvalid RMSE: 0.5050663417667759","09f27629":"train RMSE: 0.8249859021885411\nvalid RMSE: 0.5797919076732253\ntrain RMSE: 0.5543886480447727\nvalid RMSE: 0.6473528378036738\ntrain RMSE: 0.4553731330876249\nvalid RMSE: 0.573552249816162\ntrain RMSE: 0.39186169261522136\nvalid RMSE: 0.5580758739581811\ntrain RMSE: 0.32023625471194966\nvalid RMSE: 0.5731837957088508\ntrain RMSE: 0.26528761870212597\nvalid RMSE: 0.5293163088017251","f609a014":"train RMSE: 0.8144331196217753\nvalid RMSE: 0.6743270671867236\n\ntrain RMSE: 0.5551262840222608\nvalid RMSE: 0.569206174302016\n\ntrain RMSE: 0.47716535342445876\nvalid RMSE: 0.5417254085514731","0ac01bb8":"Large without 11 layers and 4 epoch\n\ntrain RMSE: 0.8796342564623829\nvalid RMSE: 0.6481774119879732\n\ntrain RMSE: 0.5957295629519985\nvalid RMSE: 0.594007074209979\n\ntrain RMSE: 0.5168589246785117\nvalid RMSE: 0.5644268208866577\n\ntrain RMSE: 0.4681257860342965\nvalid RMSE: 0.6382700901139396"}}