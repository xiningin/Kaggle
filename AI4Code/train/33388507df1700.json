{"cell_type":{"b3f19fe9":"code","9a12f7bf":"code","707bced6":"code","e9e41ff3":"code","043843dd":"code","3bc058d9":"code","0d9966f7":"code","41549911":"code","2afa8ddf":"code","2b72e666":"code","0b9fbbee":"code","6ca16567":"code","7ed24b00":"code","feb050ee":"code","0a7aa0db":"code","24920259":"code","de4f0496":"code","2dfd91e7":"code","958b4bb3":"code","a2944dae":"code","cd76835c":"code","a67c5100":"code","efba9b2a":"code","e09cb1a1":"code","a5c7d817":"code","a51a71c6":"code","6a4a4eb0":"code","f15fca76":"code","db536040":"code","69f3eacf":"code","5d52d69d":"code","29339968":"code","7de89039":"code","97031a89":"code","7be5f06d":"code","37f20749":"code","6586c9bc":"code","3027a8aa":"code","4a17cb62":"code","93ee2e12":"code","454c330c":"markdown","f7614353":"markdown","ce779519":"markdown","7e6856c2":"markdown","6b441777":"markdown","0eb6e7a1":"markdown","97eaafe3":"markdown"},"source":{"b3f19fe9":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime \nimport category_encoders as ce\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm import tqdm_notebook as tqdm\nimport random\nrandom.seed(20)\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\nfrom statistics import mean, median,variance,stdev\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier","9a12f7bf":"#df = pd.read_csv('..\/input\/train.csv')\n#df = df.sample(n=10000, random_state=20)\n#df_train, df_test = train_test_split(df, test_size=0.2, random_state=20)","707bced6":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","e9e41ff3":"df_gdp = pd.read_csv(\"..\/input\/US_GDP_by_State.csv\")\ndf_state = pd.read_csv(\"..\/input\/statelatlong.csv\")\ndf_spi = pd.read_csv(\"..\/input\/spi.csv\")","043843dd":"year = []\nfor y in list(df_train[\"issue_d\"]):\n    year.append(int(y[4:8]))\ndf_train[\"year\"] = year\n\nyear_test = []\nfor y in list(df_test[\"issue_d\"]):\n    year_test.append(int(y[4:8]))\ndf_test[\"year\"] = year_test","3bc058d9":"years = [2007, 2008, 2009, 2010, 2011, 2012]\nadd_list = []\nfor y in years:\n    for state in set(df_gdp[\"State\"]):\n        add = []\n        add.append(state)\n        add.append(int(df_gdp[\"State & Local Spending\"][df_gdp.year == 2013][df_gdp.State == state]))\n        add.append(float(df_gdp[\"Gross State Product\"][df_gdp.year == 2013][df_gdp.State == state]))\n        add.append(float(df_gdp[\"Real State Growth %\"][df_gdp.year == 2013][df_gdp.State == state]))\n        add.append(float(df_gdp[\"Population (million)\"][df_gdp.year == 2013][df_gdp.State == state]))\n        add.append(y)\n        add_list.append(add)\n        df_gdp.append(pd.DataFrame(add_list, columns=[\"State\", \"State & Local Spending\", \"Gross State Product\", \"Real State Growth %\", \"Population (million)\", \"year\"]))\n\ndf_gdp = pd.merge(df_gdp, df_state, left_on = \"State\", right_on = \"City\", how='left')\n\ndate_spi = []\nfor d in list(df_spi['date']):\n    date_temp = datetime.strptime(d, '%d-%b-%y')\n    date_spi.append(date_temp.strftime('%b-%Y'))\ndf_spi[\"date\"] = date_spi\ndf_spi = df_spi.groupby(\"date\").mean()\n\ndf_train = pd.merge(df_train, df_gdp, left_on=['addr_state','year'], right_on = ['State_y','year'], how ='left')\ndf_test = pd.merge(df_test, df_gdp, left_on=['addr_state','year'], right_on = ['State_y','year'], how ='left')","0d9966f7":"df_train = pd.merge(df_train, df_spi, left_on='issue_d', right_on ='date', how = 'left')\ndf_test = pd.merge(df_test, df_spi, left_on='issue_d', right_on ='date', how = 'left')","41549911":"#y_train = df_train.loan_condition\n#y_test = df_test.loan_condition\n#df_train = df_train.drop([\"loan_condition\", \"issue_d\"], axis=1)\n#df_test = df_test.drop([\"issue_d\"], axis=1)","2afa8ddf":"columns_raw = df_train.columns\ncolumns_raw_test = df_test.columns","2b72e666":"df_train['fold'] = df_train.loan_amnt.apply( lambda x: random.randint(0,5))","0b9fbbee":"columns = df_train.columns\ncolumns_hasnull = []\nfor col in columns:\n    if df_train[col].isnull().sum() > 0:\n        columns_hasnull.append(col)","6ca16567":"def MissingColumns(train, test, columns):\n    trainout = train.copy()\n    testout = test.copy()\n    for col in columns:\n        name = col + \"_Miss\"\n        trainout[name] = list(train[col].isnull())\n        trainout = trainout.drop(col,axis=1)\n        testout[name] = list(testout[col].isnull())\n        testout = testout.drop(col, axis=1)\n    return trainout, testout","7ed24b00":"col_miss = [\"emp_length\", \"title\", \"emp_title\"]\ntrain_Miss, test_Miss = MissingColumns(df_train[col_miss], df_test[col_miss],col_miss)","feb050ee":"for i in columns_hasnull:\n    if df_train[i].dtype == \"float64\":\n        df_train[i].fillna(df_train[i].median(), inplace=True)\n        df_test[i].fillna(df_train[i].median(), inplace=True)\n    if df_train[i].dtype == \"object\":\n        df_train[i].fillna(\"Kuhaku\", inplace=True)\n        df_test[i].fillna(\"Kuhaku\", inplace=True)","0a7aa0db":"df_train = df_train.join(train_Miss)\ndf_test = df_test.join(test_Miss)","24920259":"df_train[\"loan_sal_ratio\"] = df_train[\"loan_amnt\"]\/df_train[\"annual_inc\"]\ndf_test[\"loan_sal_ratio\"] = df_test[\"loan_amnt\"]\/df_test[\"annual_inc\"]\ndf_train[\"loan_sal_ratio\"] = df_train[\"loan_sal_ratio\"].replace(np.inf, 1)\ndf_test[\"loan_sal_ratio\"] = df_test[\"loan_sal_ratio\"].replace(np.inf, 1)","de4f0496":"def OrdinalEncoder(traindf, testdf, category):\n    oe = ce.OrdinalEncoder(cols=category, return_df=True)\n    names = list(map(lambda x: x + \"_OrdinalEncoder\", category))\n    trainout = oe.fit_transform(traindf[category])\n    testout = oe.transform(testdf[category])\n    trainout.columns = names\n    testout.columns = names\n    for name in names:\n        trainout[name].fillna(trainout[name].median(), inplace=True)\n        testout[name].fillna(trainout[name].median(), inplace=True)\n    return trainout, testout\n\ndef OneHotEncoder(traindf, testdf, category):\n    ohe = ce.OneHotEncoder(cols=category, handle_unknown='impute')\n    trainout = ohe.fit_transform(traindf[category])\n    testout = ohe.transform(testdf[category])\n    return trainout, testout\n\ndef CountEncoder(traindf, testdf, category):\n    trainout = pd.DataFrame()\n    testout = pd.DataFrame()\n    for val in category:\n        newname = val + \"_CountEncoder\"\n        count = traindf.groupby(val)[val].count()\n        trainout[newname] = traindf.groupby(val)[val].transform('count')\n        testout[newname] = traindf.groupby(val)[val].transform('count')\n        trainout[newname].fillna(trainout[newname].median(), inplace=True)\n        testout[newname].fillna(trainout[newname].median(), inplace=True)\n    return trainout, testout\n\ndef TargetEncoder(traindf, testdf, category, target):\n    trainout = pd.DataFrame()\n    testout = pd.DataFrame()\n    for col in category:\n        trainout_temp = pd.DataFrame()\n        testout_temp = pd.DataFrame()\n        n= 0\n        name = col + \"_TargetEncoder\"\n        # training\n        for i in set(traindf[\"fold\"]):\n            label_mean = traindf[traindf[\"fold\"] != i].groupby(col)[target].mean()\n            if n == 0:\n                trainout_temp = traindf[traindf[\"fold\"] == i][col].map(label_mean)\n            else:\n                trainout_temp = trainout_temp.append(traindf[traindf[\"fold\"] == i][col].map(label_mean))\n            n = n + 1\n        trainout[name] = trainout_temp\n        trainout[name].fillna(trainout[name].median(), inplace=True)\n        \n        # test\n        label_mean = traindf.groupby(col)[target].mean()\n        testout[name] = testdf[col].map(label_mean)\n        testout[name].fillna(trainout[name].median(), inplace=True)\n    return trainout, testout","2dfd91e7":"col_CE = [\n#\"earliest_cr_line\",\n]\ntrain_CE, test_CE = CountEncoder(df_train, df_test, col_CE)","958b4bb3":"col_OE = [\n#    \"initial_list_status\",\n#    \"zip_code\",\n#    \"earliest_cr_line\",\n]\ntrain_OE, test_OE = OrdinalEncoder(df_train, df_test, col_OE)","a2944dae":"col_TE = [\n        \"initial_list_status\",\n    \"zip_code\",\n    \"earliest_cr_line\",\n\"title\",\n\"grade\",\n\"sub_grade\",\n\"emp_length\",\n\"home_ownership\",\n\"purpose\",\n\"addr_state\"\n]\ntrain_TE, test_TE = TargetEncoder(df_train, df_test, col_TE, \"loan_condition\")","cd76835c":"for i in list(df_train.columns):\n    if df_train[i].dtype!=\"object\":\n        print(i)","a67c5100":"def StandardScale(train, test, columns):\n    scaler = StandardScaler()\n    scaler.fit(train[columns])\n    trainout = train[columns].copy()\n    testout = test[columns].copy()\n    trainout[columns] = scaler.transform(train[columns])\n    testout[columns] = scaler.transform(test[columns])\n    names = list(map(lambda x: x + \"_StandardScale\", columns))\n    trainout.columns = names\n    testout.columns = names\n    for name in names:\n        trainout[name].fillna(trainout[name].median(), inplace=True)\n        testout[name].fillna(trainout[name].median(), inplace=True)\n    return trainout, testout\n\ndef MinMaxScale(train, test, columns):\n    scaler = MinMaxScaler()\n    scaler.fit(train[columns])\n    trainout = train[columns].copy()\n    testout = test[columns].copy()\n    trainout[columns] = scaler.transform(trainout[columns])\n    testout[columns] = scaler.transform(testout[columns])\n    names = list(map(lambda x: x + \"_MinMaxScale\", columns))\n    trainout.columns = names\n    testout.columns = names\n    for name in names:\n        trainout[name].fillna(trainout[name].median(), inplace=True)\n        testout[name].fillna(trainout[name].median(), inplace=True)\n    return trainout, testout\n\ndef LogTransform(train, test, columns):\n    trainout = train[columns].copy()\n    testout = test[columns].copy()\n    for col in columns:\n        logval = []\n        for i in list(train[col]):\n            tes = np.sign(i)*np.log(abs(i))\n            logval.append(tes)\n        trainout[col] = logval\n        \n        logval_test = []\n        for i in list(test[col]):\n            tes = np.sign(i)*np.log(abs(i))\n            logval_test.append(tes)\n        testout[col] = logval_test\n    names = list(map(lambda x: x + \"_Log\", columns))\n    trainout.columns = names\n    testout.columns = names\n    for name in names:\n        trainout[name].fillna(trainout[name].median(), inplace=True)\n        testout[name].fillna(trainout[name].median(), inplace=True)\n    return trainout, testout\n\ndef BoxCoxTransform(train, test, columns):\n    pt = PowerTransformer(method='yeo-johnson')\n    pt.fit(train[columns])\n    trainout = train[columns].copy()\n    testout = test[columns].copy()\n    trainout[columns] = pt.transform(train[columns])\n    testout[columns] = pt.transform(test[columns])\n    names = list(map(lambda x: x + \"_BoxCox\", columns))\n    trainout.columns = names\n    testout.columns = names\n    for name in names:\n        trainout[name].fillna(trainout[name].median(), inplace=True)\n        testout[name].fillna(trainout[name].median(), inplace=True)\n    return trainout, testout","efba9b2a":"col_SS = [\n    \"loan_amnt\",\n    \"installment\",\n    \"dti\",\n    \"delinq_2yrs\",\n    \"inq_last_6mths\",\n    \"pub_rec\",\n    \"revol_util\",\n    \"State & Local Spending\",\n    \"Gross State Product\",\n    \"Population (million)\",\n    \"close\",\n    \"tot_coll_amt\",\n    \"loan_sal_ratio\",\n    \"mths_since_last_major_derog\",\n    \n]\ntrain_SS, test_SS = StandardScale(df_train, df_test, col_SS)","e09cb1a1":"col_log = [\n    \"annual_inc\",\n    \"open_acc\",\n    \"revol_bal\",\n    \"total_acc\",\n    \"tot_cur_bal\",\n    \"Real State Growth %\"\n]\ntrain_log, test_log = LogTransform(df_train, df_test, col_log)","a5c7d817":"col_MinMax = [\n    \"mths_since_last_record\"\n]\ntrain_MinMax, test_MinMax = MinMaxScale(df_train, df_test, col_MinMax)","a51a71c6":"col_BoxCox = [\n \"mths_since_last_delinq\"\n]\ntrain_BoxCox, test_BoxCox = BoxCoxTransform(df_train, df_test, col_BoxCox)","6a4a4eb0":"col_text = [\n    \"emp_title\"\n]","f15fca76":"def TFIDF(trainlist, testlist):\n    features = 30\n    vec = TfidfVectorizer(min_df = 1, max_features = features)\n    alltxt = trainlist + testlist\n    vec.fit(alltxt)\n    trainout = pd.DataFrame(vec.transform(trainlist).toarray())\n    testout = pd.DataFrame(vec.transform(testlist).toarray())\n    names = []\n    for f in range(features):\n        txt = \"txt_\" + str(f)\n        names.append(txt)\n    trainout.columns = names\n    testout.columns = names\n    return trainout, testout","db536040":"train_TXT, test_TXT = TFIDF(list(df_train[\"emp_title\"]), list(df_test[\"emp_title\"]))","69f3eacf":"df_train.columns","5d52d69d":"df_train = df_train.join([\n    train_CE,\n    train_OE,\n    train_TE,\n    train_SS,\n    train_log,\n    train_MinMax,\n    train_BoxCox,\n    train_TXT\n])\ndf_test = df_test.join([\n    test_CE,\n    test_OE,\n    test_TE,\n    test_SS,\n    test_log,\n    test_MinMax,\n    test_BoxCox,\n    test_TXT\n])","29339968":"X_train = df_train.drop(columns_raw, axis = 1).drop(\"fold\", axis = 1)\ny_train = df_train.loan_condition\nX_test = df_test.drop(columns_raw_test, axis = 1)","7de89039":"clf = LGBMClassifier(boosting_type='gbdt', \n                         class_weight=None,\n                         colsample_bytree=0.71,\n                        importance_type=\"split\",\n                        learning_rate=0.05,\n                        max_depth=-1,\n                        min_child_samples=20,\n                        min_child_weight=0.001,\n                        min_split_gain=0.0,\n                        n_estimators=9999,\n                        n_jobs=-1,\n                        num_leaves=31,\n                        objective=None,\n                        random_state=71, \n                         reg_alpha=1.0,\n                        reg_lambda=1.0,\n                        silent=True,\n                        subsample=0.9, subsample_for_bin=200000,\n                        subsample_freq=0)\nX_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, random_state=42)\nclf.fit(X_train_, y_train_, early_stopping_rounds=200,eval_metric='auc',eval_set=[(X_val,y_val)])","97031a89":"clf2 = LGBMClassifier(boosting_type='gbdt', \n                         class_weight=None,\n                         colsample_bytree=0.71,\n                        importance_type=\"split\",\n                        learning_rate=0.05,\n                        max_depth=-1,\n                        min_child_samples=20,\n                        min_child_weight=0.001,\n                        min_split_gain=0.0,\n                        n_estimators=9999,\n                        n_jobs=-1,\n                        num_leaves=31,\n                        objective=None,\n                        random_state=72, \n                         reg_alpha=1.0,\n                        reg_lambda=1.0,\n                        silent=True,\n                        subsample=0.9, subsample_for_bin=200000,\n                        subsample_freq=0)\nX_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, random_state=43)\nclf2.fit(X_train_, y_train_, early_stopping_rounds=200,eval_metric='auc',eval_set=[(X_val,y_val)])","7be5f06d":"clf3 = LGBMClassifier(boosting_type='gbdt', \n                         class_weight=None,\n                         colsample_bytree=0.71,\n                        importance_type=\"split\",\n                        learning_rate=0.05,\n                        max_depth=-1,\n                        min_child_samples=20,\n                        min_child_weight=0.001,\n                        min_split_gain=0.0,\n                        n_estimators=9999,\n                        n_jobs=-1,\n                        num_leaves=31,\n                        objective=None,\n                        random_state=73, \n                         reg_alpha=1.0,\n                        reg_lambda=1.0,\n                        silent=True,\n                        subsample=0.9, subsample_for_bin=200000,\n                        subsample_freq=0)\nX_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, random_state=44)\nclf3.fit(X_train_, y_train_, early_stopping_rounds=200,eval_metric='auc',eval_set=[(X_val,y_val)])","37f20749":"clf4 = LGBMClassifier(boosting_type='gbdt', \n                         class_weight=None,\n                         colsample_bytree=0.71,\n                        importance_type=\"split\",\n                        learning_rate=0.05,\n                        max_depth=-1,\n                        min_child_samples=20,\n                        min_child_weight=0.001,\n                        min_split_gain=0.0,\n                        n_estimators=9999,\n                        n_jobs=-1,\n                        num_leaves=31,\n                        objective=None,\n                        random_state=74, \n                         reg_alpha=1.0,\n                        reg_lambda=1.0,\n                        silent=True,\n                        subsample=0.9, subsample_for_bin=200000,\n                        subsample_freq=0)\nX_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, random_state=45)\nclf4.fit(X_train_, y_train_, early_stopping_rounds=200,eval_metric='auc',eval_set=[(X_val,y_val)])","6586c9bc":"clf5 = LGBMClassifier(boosting_type='gbdt', \n                         class_weight=None,\n                         colsample_bytree=0.71,\n                        importance_type=\"split\",\n                        learning_rate=0.05,\n                        max_depth=-1,\n                        min_child_samples=20,\n                        min_child_weight=0.001,\n                        min_split_gain=0.0,\n                        n_estimators=9999,\n                        n_jobs=-1,\n                        num_leaves=31,\n                        objective=None,\n                        random_state=75, \n                         reg_alpha=1.0,\n                        reg_lambda=1.0,\n                        silent=True,\n                        subsample=0.9, subsample_for_bin=200000,\n                        subsample_freq=0)\nX_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, random_state=46)\nclf5.fit(X_train_, y_train_, early_stopping_rounds=200,eval_metric='auc',eval_set=[(X_val,y_val)])","3027a8aa":"y_pred = clf.predict_proba(X_test)[:,1]\ny_pred2 = clf2.predict_proba(X_test)[:,1]\ny_pred3 = clf3.predict_proba(X_test)[:,1]\ny_pred4 = clf4.predict_proba(X_test)[:,1]\ny_pred5 = clf5.predict_proba(X_test)[:,1]","4a17cb62":"pred_fin = []\nfor i in range(len(y_pred)):\n    pred = (y_pred[i] + y_pred2[i] + y_pred3[i] + y_pred4[i] + y_pred5[i])\/5\n    pred_fin.append(pred)","93ee2e12":"submission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmission.loan_condition = pred_fin\nsubmission.to_csv(\"submission.csv\", index=False)","454c330c":"# \u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f","f7614353":"# \u30c6\u30ad\u30b9\u30c8","ce779519":"# \u30ab\u30c6\u30b4\u30ea\u30fc\u51e6\u7406","7e6856c2":"# \u6570\u5024\u51e6\u7406","6b441777":"# \u30c7\u30fc\u30bf\u6e96\u5099","0eb6e7a1":"# NULL\u51e6\u7406","97eaafe3":"# \u8ffd\u52a0\u7279\u5fb4\u91cf\u751f\u6210"}}