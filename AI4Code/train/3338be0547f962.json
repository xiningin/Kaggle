{"cell_type":{"96cf2120":"code","ae09e0d3":"code","f719ae94":"code","d812bf52":"code","615ad41b":"code","eb00f24a":"code","b4f3139f":"code","c1ddf27d":"code","1ce54c6a":"code","6337d2eb":"code","9bbd9ab5":"code","af1ca3fb":"code","8d965e12":"code","57b61313":"code","f2d1f86b":"code","c8d60161":"code","9c116f3d":"code","baa90523":"code","ae6a44c9":"code","632aa1d4":"code","ddd89148":"code","1a483c48":"code","6ebc48e1":"code","dc84f73d":"code","3c8ec15e":"code","8952d5b8":"code","0d1c42d6":"code","7d48f7b8":"code","4904f5e5":"code","c451fc4c":"code","cab66e35":"code","a9e918c2":"code","3a4af5c8":"code","b4e39686":"code","18f2fd82":"code","07c61552":"code","e75dbd41":"code","fcdcab95":"code","9d752591":"code","059e2d35":"code","1e15e0b0":"code","a1fa0cb0":"code","d2e6fe4b":"code","685af4d1":"code","1937a94f":"code","10bbfcad":"code","7dc072fb":"code","762188d3":"code","2b3b020e":"code","62a3e24a":"code","7aebff1d":"code","528d964b":"code","c99746d6":"code","8b1db53a":"code","06ff044e":"code","7a9870c7":"code","14a810f4":"code","499b328d":"markdown","3903a989":"markdown","675fc79a":"markdown","047cfef6":"markdown","34f5fb0a":"markdown","c7608aa1":"markdown","93247fd5":"markdown","551ef235":"markdown","d2d29df9":"markdown","978d0e96":"markdown","0ef2c758":"markdown","030a971e":"markdown","060ec176":"markdown","9d3e29ec":"markdown","608220b2":"markdown","05d265a6":"markdown","300c47a4":"markdown","62a33c6f":"markdown","4447fd9e":"markdown","e93921d2":"markdown","d24dfd78":"markdown","91368c4a":"markdown","a2afb2f4":"markdown","822678d1":"markdown","8685c001":"markdown","baa841d1":"markdown","ef4269a4":"markdown","c96b1128":"markdown","a7183535":"markdown","4293b71b":"markdown","cf6b53ef":"markdown","232cbd1f":"markdown"},"source":{"96cf2120":"### importing the libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport json\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","ae09e0d3":"## reading the matadata information\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nmetadata_path = f'{root_path}\/metadata.csv'\ncovid_data = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\ncovid_data.head()","f719ae94":"### loading  all the json files\nall_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\nlen(all_json)","d812bf52":"## helper class to read the json files\nclass FileReader:\n    def __init__(self,filepath):\n        with open(filepath) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            \n            # extract abstract\n            for i in content['abstract']:\n                self.abstract.append(i['text'])\n                \n            # extract body text\n            for i in content['body_text']:\n                self.body_text.append(i['text'])\n                \n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n            \n            def __repr__(self):\n                return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n\n","615ad41b":"def get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data","eb00f24a":"### conversion to a dataframe\ndict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) \/\/ 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    try:\n        content = FileReader(entry)\n    except Exception as e:\n        continue  # invalid paper format, skip\n    \n    \n    # get metadata information\n    meta_data = covid_data.loc[covid_data['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    \n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 0: \n        # no abstract provided\n        dict_['abstract_summary'].append(\"Not provided.\")\n    elif len(content.abstract.split(' ')) > 100:\n        # abstract provided is too long for plot, take first 300 words append with ...\n        info = content.abstract.split(' ')[:100]\n        summary = get_breaks(' '.join(info), 40)\n        dict_['abstract_summary'].append(summary + \"...\")\n    else:\n        # abstract is short enough\n        summary = get_breaks(content.abstract, 40)\n        dict_['abstract_summary'].append(summary)\n        \n    # get metadata information\n    meta_data = covid_data.loc[covid_data['sha'] == content.paper_id]\n    \n    try:\n        # if more than one author\n        authors = meta_data['authors'].values[0].split(';')\n        if len(authors) > 2:\n            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n            dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n        else:\n            # authors will fit in plot\n            dict_['authors'].append(\". \".join(authors))\n    except Exception as e:\n        # if only one author - or Null valie\n        dict_['authors'].append(meta_data['authors'].values[0])\n    \n    # add the title information, add breaks when needed\n    try:\n        title = get_breaks(meta_data['title'].values[0], 40)\n        dict_['title'].append(title)\n    # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\ndf_covid.head()","b4f3139f":"## information of the dataset\ndf_covid.info()","c1ddf27d":"# one in all approach - removal of special characters\n\ndf_covid['title'].replace('[!\"#%\\()''*+,-.\/:;<=>?@\\[\\]^_`{|}~1234567890\u2019\u201d\u201c\u2032\u2018\\\\\\]','',inplace=True,regex=True)\ndf_covid['abstract'].replace('[!\"#%\\()''*+,-.\/:;<=>?@\\[\\]^_`{|}~1234567890\u2019\u201d\u201c\u2032\u2018\\\\\\]','',inplace=True,regex=True)\ndf_covid['abstract_summary'].replace('[!\"#%\\()''*+,-.\/:;<=>?@\\[\\]^_`{|}~1234567890\u2019\u201d\u201c\u2032\u2018\\\\\\]','',inplace=True,regex=True)","1ce54c6a":"## conversion to lower case\n\ndef lower_case(input_str):\n    input_str = input_str.lower()\n    return input_str\n\n \n\ndf_covid['body_text'] = df_covid['body_text'].apply(lambda x: lower_case(x))\ndf_covid['abstract'] = df_covid['abstract'].apply(lambda x: lower_case(x))\ndf_covid['abstract_summary'] = df_covid['abstract_summary'].apply(lambda x: lower_case(x))","6337d2eb":"import re\ndf_covid['abstract'] = df_covid['abstract'].map(lambda x: re.sub('[,\\n.!?]', '', x))","9bbd9ab5":"## importing the nltk stopwords\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nsw = stopwords.words('english')","af1ca3fb":"### importing the lemmatization and tokenization function\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize","8d965e12":"### function to apply tokenization, remove stopwords and apply lemmatizer \ndef process_message(message,stem = False, stop_words = True,lemmi = True):\n    words = word_tokenize(message)\n    if stop_words:\n        sw = stopwords.words('english')\n        words = [word for word in words if word not in sw]\n    if stem:\n        stemmer = PorterStemmer()\n        words = [stemmer.stem(word) for word in words]\n    if lemmi:\n        lemmatizer = WordNetLemmatizer()\n        words = [lemmatizer.lemmatize(word) for word in words]\n    return words","57b61313":"## aplying the function to apply tokenization, remove stopwords and apply lemmatizer on the abstract column of the dataframe\ndf_covid['abstract']=df_covid['abstract'].apply(process_message)","f2d1f86b":"### importing the library\nimport matplotlib.pyplot as plt","c8d60161":"df_covid['journal'].value_counts()[:10].plot.barh(figsize = (10,5))","9c116f3d":"df_covid['title'].value_counts()[:10].plot.barh(figsize = (10,5))","baa90523":"df_covid['authors'].value_counts()[:10].plot.barh(figsize = (10,5))","ae6a44c9":"## importing the word cloud library\nimport wordcloud\nfrom wordcloud import WordCloud","632aa1d4":"df_covid['abstract'] = df_covid['abstract'].astype('str')","ddd89148":"wordcloud = WordCloud().generate(' '.join(df_covid['abstract']))\nplt.figure(figsize=[30,10])\nplt.imshow(wordcloud)\nplt.axis(\"off\")","1a483c48":"# checking the number of CPU core processor used for this process\nimport multiprocessing\ncores = multiprocessing.cpu_count()\nprint(cores)","6ebc48e1":"## importing necessary modules\nfrom tqdm import tqdm\ntqdm.pandas(desc=\"progress-bar\")\nfrom gensim.models import Doc2Vec\nfrom sklearn import utils\nfrom gensim.models.doc2vec import TaggedDocument","dc84f73d":"from gensim.models.doc2vec import Doc2Vec, TaggedDocument\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(list(df_covid['abstract']))]","3c8ec15e":"model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\nmodel_dbow.build_vocab([x for x in tqdm(documents)])","8952d5b8":"#appending all the vectors in a list for training\nX=[]\nfor i in range(36009):\n    X.append(model_dbow.docvecs[i])\n    print(model_dbow.docvecs[i])","0d1c42d6":"#import the modules\nfrom sklearn.cluster import KMeans\nimport numpy as np\n#create the kmeans object withe vectors created previously\nkmeans = KMeans(n_clusters=10, random_state=0).fit(X)\n\n \n\n#print all the labels\nprint(kmeans.labels_)","7d48f7b8":"from sklearn import metrics\nlabels = kmeans.labels_\ncentroids = kmeans.cluster_centers_\n\nprint (\"Cluster id labels for inputted data\")\nprint (labels)\nprint (\"Centroids data\")\nprint (centroids)\n\nprint (\"Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):\")\nprint (kmeans.score(X))","4904f5e5":"## calcualting the silhouette score\nsilhouette_score = metrics.silhouette_score(X, labels, metric='euclidean')\n\nprint (\"Silhouette_score: \")\nprint (silhouette_score)","c451fc4c":"df_covid[\"clust\"] = labels","cab66e35":"## Pie chart visualization showing the percentage of articles belonging to a particular cluster\ndf_covid.groupby(\"clust\")[\"abstract\"].count().plot(kind = \"pie\", autopct='%.2f', figsize = (10, 10))","a9e918c2":"from gensim.models.doc2vec import Doc2Vec, TaggedDocument\nimport gensim\ndef read_corpus(df, column, tokens_only=False):\n    \"\"\"\n    Arguments\n    ---------\n        df: pd.DataFrame\n        column: str \n            text column name\n        tokens_only: bool\n            wether to add tags or not\n    \"\"\"\n    for i, line in enumerate(df[column]):\n\n \n\n        tokens = gensim.parsing.preprocess_string(line)\n        if tokens_only:\n            yield tokens\n        else:\n            # For training data, add tags\n            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])","3a4af5c8":"## resetting the index of the dataframe\ndf_covid.reset_index(inplace=True, drop=True)","b4e39686":"import random\nfrac_of_articles = 1\ntrain_df  = df_covid.sample(frac=frac_of_articles, random_state=42)\ntrain_corpus = read_corpus(train_df, 'abstract')","18f2fd82":"# using distributed memory model\nmodel = gensim.models.doc2vec.Doc2Vec(dm=1, vector_size=60, min_count=5, epochs=20, seed=42, workers=6)\nmodel.build_vocab(train_corpus)\nmodel.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)","07c61552":"task_1 = \"\"\"What is known about transmission, incubation, and environmental stability of COVID-19? What do we know about natural history, transmission, and diagnostics for the virus? What have we learned about infection prevention and control?\nRange of incubation periods for the disease in humans (and how this varies across age and health status)\nand how long individuals are contagious, even after recovery.\nPrevalence of asymptomatic shedding and transmission (e.g., particularly children).\nSeasonality of transmission.\nPhysical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\nPersistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\nPersistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\nNatural history of the virus and shedding of it from an infected person\nImplementation of diagnostics and products to improve clinical processes\nDisease models, including animal models for infection, disease and transmissionTools and studies to monitor phenotypic change and potential adaptation of the virus\nImmune response and immunity\nEffectiveness of movement control strategies to prevent secondary transmission in health care and community settings\nEffectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\nRole of the environment in transmission.\"\"\"\n\n\n\ntask_2 = \"\"\"What do we know about COVID-19 risk factors? What have we learned from epidemiological studies?\nData on potential risks factors\nSmoking, pre-existing pulmonary disease\nCo-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities\nNeonates and pregnant women\nSocio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\nTransmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\nSeverity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\nSusceptibility of populations\nPublic health mitigation measures that could be effective for control\"\"\"\n\n\n\ntask_3 = \"\"\"What do we know about virus genetics, origin, and evolution? What do we know about the virus origin and management measures at the human-animal interface?\nReal-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time.\nAccess to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged.\nEvidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.\nEvidence of whether farmers are infected, and whether farmers could have played a role in the origin.\nSurveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.\nExperimental infections to test host range for this pathogen.\nAnimal host(s) and any evidence of continued spill-over to humans\nSocioeconomic and behavioral risk factors for this spill-over\nSustainable risk reduction strategies\"\"\"\n\n\n\ntask_4 = \"\"\"What do we know about vaccines and therapeutics? What has been published concerning research and development and evaluation efforts of vaccines and therapeutics?\nEffectiveness of drugs being developed and tried to treat COVID-19 patients.\nClinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.\nMethods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\nExploration of use of best animal models and their predictive value for a human vaccine.\nCapabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\nAlternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.\nEfforts targeted at a universal coronavirus vaccine.\nEfforts to develop animal models and standardize challenge studies\nEfforts to develop prophylaxis clinical studies and prioritize in healthcare workers\nApproaches to evaluate risk for enhanced disease after vaccinationAssays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics]\"\"\"\n\n\n\ntask_5 = \"\"\"What do we know about the effectiveness of non-pharmaceutical interventions? What is known about equity and barriers to compliance for non-pharmaceutical interventions?\nGuidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases.\nRapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments.\nRapid assessment of the likely efficacy of school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches.\nMethods to control the spread in communities, barriers to compliance and how these vary among different populations..\nModels of potential interventions to predict costs and benefits that take account of such factors as race, income, disability, age, geographic location, immigration status, housing status, employment status, and health insurance status.\nPolicy changes necessary to enable the compliance of individuals with limited resources and the underserved with NPIs.\nResearch on why people fail to comply with public health advice, even if they want to do so (e.g., social or financial costs may be too high).\nResearch on the economic impact of this or any pandemic. This would include identifying policy and programmatic alternatives that lessen\/mitigate risks to critical government services, food distribution and supplies, access to critical household supplies, and access to health diagnoses, treatment, and needed care, regardless of ability to pay.\"\"\"\n\n\n\ntask_6 = \"\"\"What do we know about diagnostics and surveillance? What has been published concerning systematic, holistic approach to diagnostics (from the public health surveillance perspective to being able to predict clinical outcomes)?\nHow widespread current exposure is to be able to make immediate policy recommendations on mitigation measures. Denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible. Sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as convalescent samples) and early detection of disease (e.g., use of screening of neutralizing antibodies such as ELISAs).\nEfforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms.\nRecruitment, support, and coordination of local expertise and capacity (public, private\u2014commercial, and non-profit, including academic), including legal, ethical, communications, and operational issues.\nNational guidance and guidelines about best practices to states (e.g., how states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public).Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy.\nRapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity. These experiments could aid in collecting longitudinal samples, which are critical to understanding the impact of ad hoc local interventions (which also need to be recorded).\nSeparation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices.\nEfforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance\/detection schemes.\nLatency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling.\nUse of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression, which would be important to understanding best clinical practice and efficacy of therapeutic interventions.\nPolicies and protocols for screening and testing.Policies to mitigate the effects on supplies associated with mass testing, including swabs and reagents.\nTechnology roadmap for diagnostics.\nBarriers to developing and scaling up new diagnostic tests (e.g., market forces), how future coalition and accelerator models (e.g., Coalition for Epidemic Preparedness Innovations) could provide critical funding for diagnostics, and opportunities for a streamlined regulatory environment.\nNew platforms and technology (e.g., CRISPR) to improve response times and employ more holistic approaches to COVID-19 and future diseases.\nCoupling genomics and diagnostic testing on a large scale.\nEnhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant.\nEnhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and explore capabilities for distinguishing naturally-occurring pathogens from intentional.\nOne Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors.\"\"\"\n\n\n\ntask_7 = \"\"\"What has been published about medical care? What has been published concerning surge capacity and nursing homes? What has been published concerning efforts to inform allocation of scarce resources? What do we know about personal protective equipment? What has been published concerning alternative methods to advise on disease management? What has been published concerning processes of care? What do we know about the clinical characterization and management of the virus?\nResources to support skilled nursing facilities and long term care facilities.\nMobilization of surge medical staff to address shortages in overwhelmed communities\nAge-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with\/without other organ failure \u2013 particularly for viral etiologies\nExtracorporeal membrane oxygenation (ECMO) outcomes data of COVID-19 patients\nOutcomes data for COVID-19 after mechanical ventilation adjusted for age.\nKnowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest.\nApplication of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level.\nApproaches for encouraging and facilitating the production of elastomeric respirators, which can save thousands of N95 masks.\nBest telemedicine practices, barriers and faciitators, and specific actions to remove\/expand them within and across state boundaries.\nGuidance on the simple things people can do at home to take care of sick people and manage disease.\nOral medications that might potentially work.\nUse of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually.\nBest practices and critical challenges and innovative solutions and technologies in hospital flow and organization, workforce protection, workforce allocation, community-based support resources, payment, and supply chain management to enhance capacity, efficiency, and outcomes.\nEfforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials\nEfforts to develop a core clinical outcome set to maximize usability of data across a range of trials\nEfforts to determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (e.g. steroids, high flow oxygen)\n\"\"\"\n\n\ntask_8 = \"\"\"What has been published concerning ethical considerations for research? What has been published concerning social sciences at the outbreak response?\nEfforts to articulate and translate existing ethical principles and standards to salient issues in COVID-2019\nEfforts to embed ethics across all thematic areas, engage with novel ethical issues that arise and coordinate to minimize duplication of oversight\nEfforts to support sustained education, access, and capacity building in the area of ethics\nEfforts to establish a team at WHO that will be integrated within multidisciplinary research and operational platforms and that will connect with existing and expanded global networks of social sciences.\nEfforts to develop qualitative assessment frameworks to systematically collect information related to local barriers and enablers for the uptake and adherence to public health measures for prevention and control. This includes the rapid identification of the secondary impacts of these measures. (e.g. use of surgical masks, modification of health seeking behaviors for SRH, school closures)\nEfforts to identify how the burden of responding to the outbreak and implementing public health measures affects the physical and psychological health of those providing care for Covid-19 patients and identify the immediate needs that must be addressed.\nEfforts to identify the underlying drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media.\n\"\"\"\n\n\n\ntask_9 = \"\"\"What has been published about information sharing and inter-sectoral collaboration? What has been published about data standards and nomenclature? What has been published about governmental public health? What do we know about risk communication? What has been published about communicating with high-risk populations? What has been published to clarify community measures? What has been published about equity considerations and problems of inequity?\nMethods for coordinating data-gathering with standardized nomenclature.\nSharing response information among planners, providers, and others.\nUnderstanding and mitigating barriers to information-sharing.\nHow to recruit, support, and coordinate local (non-Federal) expertise and capacity relevant to public health emergency response (public, private, commercial and non-profit, including academic).\nIntegration of federal\/state\/local public health surveillance systems.\nValue of investments in baseline public health response infrastructure preparedness\nModes of communicating with target high-risk populations (elderly, health care workers).\nRisk communication and guidelines that are easy to understand and follow (include targeting at risk populations\u2019 families too).\nCommunication that indicates potential risk of disease to all population groups.\nMisunderstanding around containment and mitigation.\nAction plan to mitigate gaps and problems of inequity in the Nation\u2019s public health capability, capacity, and funding to ensure all citizens in need are supported and can access information, surveillance, and treatment.\nMeasures to reach marginalized and disadvantaged populations.\nData systems and research priorities and agendas incorporate attention to the needs and circumstances of disadvantaged populations and underrepresented minorities.\nMitigating threats to incarcerated people from COVID-19, assuring access to information, prevention, diagnosis, and treatment.\nUnderstanding coverage policies (barriers and opportunities) related to testing, treatment, and care\n\"\"\"","e75dbd41":"list_of_tasks = [task_1, task_2, task_3, task_4, task_5, task_6, task_7, task_8, task_9]","fcdcab95":"def get_doc_vector(doc):\n    tokens = gensim.parsing.preprocess_string(doc)\n    vector = model.infer_vector(tokens)\n    return vector","9d752591":"abstract_vectors = model.docvecs.vectors_docs\narray_of_tasks = [get_doc_vector(task) for task in list_of_tasks]","059e2d35":"train_df['abstract_vector'] = [vec for vec in abstract_vectors]","1e15e0b0":"train_array = train_df['abstract_vector'].values.tolist()","a1fa0cb0":"from sklearn.neighbors import NearestNeighbors\nkd_tree = NearestNeighbors(algorithm='kd_tree', leaf_size=30).fit(train_array)","d2e6fe4b":"distances, indices = kd_tree.kneighbors(array_of_tasks, n_neighbors=10)","685af4d1":"for i, info in enumerate(list_of_tasks):\n    print(\"=\"*80, f\"\\n\\nTask = {info[:500]}\\n\", )\n    df =  train_df.iloc[indices[i]]\n    abstracts = df['abstract']\n    titles = df['title']\n    dist = distances[i]\n    for l in range(len(dist)):\n        print(f\" Text index = {indices[i][l]} \\n Distance = {distances[i][l]} \\n Title: {titles.iloc[l]} \\n Abstract extract: {abstracts.iloc[l][:200]}\\n\\n\")","1937a94f":"## installing the library\n!pip install rank_bm25","10bbfcad":"## importing the library\nfrom rank_bm25 import BM25Okapi\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import preprocess_documents, preprocess_string","7dc072fb":"train_array = df_covid.abstract.fillna('').apply(preprocess_string)","762188d3":"bm25_index = BM25Okapi(train_array)","2b3b020e":"def search(search_string, num_results=10):\n    search_tokens = preprocess_string(search_string)\n    scores = bm25_index.get_scores(search_tokens)\n    top_indexes = np.argsort(scores)[::-1][:num_results]\n    return top_indexes","62a3e24a":"indexes = search(task_1)\nindexes","7aebff1d":"df_covid.loc[indexes,['title','body_text','abstract_summary']]","528d964b":"indexes_task2 = search(task_2)\nindexes_task2","c99746d6":"df_covid.loc[indexes_task2,['title','body_text','abstract_summary']]","8b1db53a":"indexes_task3 = search(task_3)\nindexes_task3","06ff044e":"df_covid.loc[indexes_task3,['title','body_text','abstract_summary']]","7a9870c7":"indexes_task4 = search(task_4)\nindexes_task4","14a810f4":"df_covid.loc[indexes_task4,['title','body_text','abstract_summary']]","499b328d":"### Step3: Creation of the dataframe from json files with important columns","3903a989":"### Step7 : Using doc2vec DM architecture with nearesr neighbors technique","675fc79a":"Helper function adds break after every words when character length reach to certain amount. This is for the interactive plot so that hover tool fits the screen.","047cfef6":"### Step 6 : We are choosing here the \"abstract\" column for further analysis as abstract gives us a consize information about the articles. Also, it takes less computation time for the algorithms to work on abstract columns compared to body text. The abstract contains enough data to work with which can give us beautiful insights and also accurate relevant articles.","34f5fb0a":"The most article titles are on Vaccines for the viruses.","c7608aa1":"### Step 4 : Text pre-processing such as converting the text into lower case, removal of special characters, stopwords. Also, applying lemmatization process to normalize the text.","93247fd5":"## 1.Dataset Description\n\nWe are  working on Covid 19 which is an unsupervised dataset.In response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open dataset.This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up.\n\n\n## 2. Our Goal\n\nThe main aim is to help the medical researchers find the related articles for covid 19 for each set of task mentioned below:\n\n1. What is known about transmission, incubation, and environmental stability?\n2. What do we know about COVID-19 risk factors?\n3. What do we know about virus genetics, origin, and evolution?\n4. What do we know about vaccines and therapeutics?\n5. What do we know about non-pharmaceutical interventions?\n6. What do we know about diagnostics and surveillance?\n7. What has been published about medical care?\n8. What has been published about information sharing and inter-sectoral collaboration?\n9. What has been published about ethical and social science considerations?\n\n\n## 3. Our approach\n\nWe have taken three basic approaches to find the related articles.\n\n1. Text preprocessing of the dataset using NLP, regular expressions and gensim\n2. Word embedding of the texts using doc2vec and finding the similarity scores between the tasks and the articles using nearest neighbouring KD tree algorithm.\n3. Document clustering using K means model and labelled the cluster to the respective tasks.\n4. Using BM25 ranking algorithm to find the most relevant articles for each tasks.\n\n\n## 4. Table of Contents\n\n1. Reading the matadata\n2. Loading the json files\n3. Dataframe creation of the json files\n4. Text Preprocessing \n5. Data Visualization\n6. Doc2vec with K means clustering and DBow method\n7. Doc2vec with nearest neighbor kd tree algorithm and DM method\n8. BM25 ranking algorithm","551ef235":"making a list of all the tasks","d2d29df9":"Displaying the top 10 journals","978d0e96":"Applying nearest neighbors kd_tree algorithm on the abstract column","0ef2c758":"Process 1:\n\nWe are using the Distributed Bag Of Words(DBOW) technqiue by selecting dm = 0 in the hyperparameter  for building the vocabualry of the tagged documents","030a971e":"Calcualting the scores of each of the clusters ","060ec176":"Using random function to the select a sample of the randomly chosen articles and assigining the variable frac_of_articles to 1 which implies that we are considering the entire document for training purpose.","9d3e29ec":"## Conclusion : We have successfully mapped each task with its relevant articles which will make easy for medical researchers to find the relevant articlesfor each task.\n\n\n### Stay Home.Stay safe.","608220b2":"Now we are assigning each taks to a partcular variable for finding the relevant artciles","05d265a6":" The below code is the pre- processing step of doc2vec which generates a tag for abstracts using TaggedDocument function of doc2vec","300c47a4":"Creating a separate column and assigning the cluster labels to it","62a33c6f":"Displaying the top 10 active titles in the dataset","4447fd9e":"Displaying the nearest article to a corresponding tasks along with the article index and title and the similarity score","e93921d2":"As we can see  most of the journals are related to virological methods and on visrus researches. ","d24dfd78":"1. Applying doc2vec technique on the abstract column to get the word embedding and the document ID for each abstract","91368c4a":"Generating word cloud for the abstract column and finding the most frequent words appreaing in the abstract.","a2afb2f4":"### Step 1 : Reading the matadata and converting the json files into a dataframe for analysis","822678d1":"Now we are using Distributed Memory(DM) technique of doc2vec by applying dm = 1 in the hyperparameter to create a vocabualry of the documents and the train the corpus ","8685c001":"### step 5 : Clustering of the documents using K means clustering","baa841d1":"Using wordcloud to find the most frequent words appreparing in the dataset","ef4269a4":"### Step8 : BM25 rank algorithm to find the related articles for each tasks","c96b1128":"### Step 5 :  Data Visualization of the dataset to gather some insights","a7183535":"Process 2:\n\nCreating doc2vec and tagged documents to find related articles using nearest neighbour KD tree algorithm","4293b71b":"Displaying the top 10 most active authors","cf6b53ef":"Generating the tokens and respective vectors and vocabulary for each tasks","232cbd1f":"###  Step2 : Loading the json files using glob function and a Filereader class"}}