{"cell_type":{"1bcd5532":"code","2a382c2e":"code","c08b6ac0":"code","e61c2978":"code","73c3bc4e":"code","64d5203b":"code","1a22a4ae":"code","056ca926":"code","ffeaa21c":"code","6df2fd0f":"code","da5339ee":"code","1c38c99d":"code","2beb88fe":"code","9068c35e":"code","f065d737":"code","feb6d90a":"code","bd16ecc2":"code","68b6a4bc":"code","bb0fbc4a":"code","88839ea6":"code","ecdafcea":"code","d5ef34d0":"code","8ddab7d3":"code","3e497522":"code","b65a70c3":"code","91402ba7":"code","22a54b84":"code","7c5f2aab":"code","eaeedb58":"code","7227e11e":"code","c832a7b8":"code","6d0de451":"code","0c3a000b":"code","8453b325":"code","08ad038a":"code","944947c8":"code","e4cda36d":"code","4c1f14ce":"code","a8ed25ac":"code","03cd146f":"code","d7843a74":"code","4810864d":"code","3dfdcede":"code","7a79816b":"code","3e8ab550":"code","a0c5b930":"code","f74537c9":"code","b149ccf2":"code","fa9ba49f":"code","3eed1f43":"code","78382936":"code","3619bc31":"code","f70575be":"code","35ff5a03":"code","16a9290c":"code","84bc5d3f":"code","7a0b4440":"code","4c9446ff":"code","2512876f":"code","89e9f54d":"code","c85c4ccb":"markdown","54c39900":"markdown","3865ce27":"markdown","1ce6fa98":"markdown","584e36cb":"markdown","0a1c8781":"markdown","b88d3a51":"markdown","ec11e70d":"markdown","211bae4e":"markdown","42cbd061":"markdown","695eafbe":"markdown","c9f3f048":"markdown","50972096":"markdown","dfc892a7":"markdown"},"source":{"1bcd5532":"import numpy as np\nimport pandas as pd\nimport plotly.express as px\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils import column_or_1d\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\n\n\nimport tensorflow as tf\nfrom keras.models import Model\nfrom keras.layers import Dense, Flatten, Dropout, BatchNormalization, Embedding, Input\nfrom keras.layers.merge import concatenate\nfrom keras.utils import to_categorical\n\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2a382c2e":"train_df = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/train.csv\", index_col = 'id')\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/test.csv\", index_col = 'id')\n\nX = train_df.drop('target', axis = 1)\n\nlencoder = LabelEncoder()\ny_df = pd.DataFrame(lencoder.fit_transform(train_df['target']), columns=['target'])\n\ndf_all = pd.concat([X, test_df], axis = 0)\ndf_all = df_all.astype(\"category\")","c08b6ac0":"class __LabelEncoder__(LabelEncoder):\n\n    def transform(self, y):\n\n        check_is_fitted(self, 'classes_')\n        y = column_or_1d(y, warn=True)\n\n        unseen = len(self.classes_)\n\n        e = np.array([\n                     np.searchsorted(self.classes_, x)\n                     if x in self.classes_ else unseen\n                     for x in y\n                     ])\n\n        if unseen in e:\n            self.classes_ = np.array(self.classes_.tolist() + ['unseen'])\n\n        return e\n\ndef get_encoded_data(data, categorical_variables=None):\n   \n    encoders = {}\n\n    df = data.copy()\n\n    if categorical_variables is None:\n        categorical_variables = [col for col in df.columns if df[col].dtype == 'category'] \n\n    for var in categorical_variables:\n        encoders[var] = __LabelEncoder__()\n        df.loc[:, var] = encoders[var].fit_transform(df[var])\n\n    return df.astype(\"category\"), encoders","e61c2978":"df_all, encoders = get_encoded_data(df_all)\ntrain, test, y = df_all[:len(train_df)].to_numpy(), df_all[len(train_df):].to_numpy(), y_df.to_numpy()\n\nX_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.2, random_state=1)","73c3bc4e":"X_train_enc = [X_train[:, i] for i in range(X_train.shape[1])]\nX_test_enc = [X_test[:, i] for i in range(X_test.shape[1])]\ntest_enc = [test[:, i] for i in range(test.shape[1])]\n\ny_train_enc = to_categorical(y_train)\ny_test_enc = to_categorical(y_test)\n\nX_train_enc[0:5]","64d5203b":"categorical_variables = df_all.select_dtypes(include='category').columns\ninfo = {col:(df_all[col].nunique(),min(50,(df_all[col].nunique()+ 1) \/\/2)) for col in categorical_variables}","1a22a4ae":"input_layers = list()\nembedding_layers = list()\n\nfor feature in categorical_variables:\n    n_labels = df_all[feature].nunique()\n    input_layer = Input(shape=(1,))\n    embedding_layer = Embedding(n_labels, 3)(input_layer)\n    input_layers.append(input_layer)\n    embedding_layers.append(embedding_layer)\n    \nmerge = concatenate(embedding_layers)\n\ndense_1 = Dense(128, kernel_initializer='normal', activation='relu')(merge)\nx = BatchNormalization()(dense_1)\nx = Dropout(0.5)(x)\ndense_2 = Dense(32, kernel_initializer='normal', activation='relu')(x)\nx = BatchNormalization()(dense_2)\nx = Dropout(0.25)(x)\nflatten = Flatten()(x)\noutput = Dense(4, activation='softmax')(flatten)\nmodel = Model(inputs=input_layers, outputs=output)","056ca926":"model.compile(loss = \"categorical_crossentropy\", optimizer = tf.keras.optimizers.Adam(), metrics=['accuracy'])","ffeaa21c":"model.fit(X_train_enc, y_train_enc, validation_data=(X_test_enc, y_test_enc), epochs=20, batch_size=64, verbose=2)","6df2fd0f":"embs = list(map(lambda x: x.get_weights()[0], [x for x in model.layers if 'Embedding' in str(x)]))\nembeddings = {var: emb for var, emb in zip(info.keys(), embs)}","da5339ee":"embeddings_df = {}\nfor cat_var in tqdm(embeddings.keys()):\n    df = pd.DataFrame(embeddings[cat_var])\n    df.index = encoders[cat_var].classes_\n    df.columns = [cat_var +  '_embedding_' + str(num) for num in df.columns]\n    embeddings_df[cat_var] = df","1c38c99d":"embeddings_df['feature_0'].head(5)","2beb88fe":"embeddings_df['feature_1'].head(5)","9068c35e":"fig = px.scatter_3d(embeddings_df['feature_3'], x='feature_3_embedding_0', y='feature_3_embedding_1', z='feature_3_embedding_2', color =embeddings_df['feature_3'].index)\nfig.show()","f065d737":"fig = px.scatter_3d(embeddings_df['feature_35'], x='feature_35_embedding_0', y='feature_35_embedding_1', z='feature_35_embedding_2', color =embeddings_df['feature_35'].index)\nfig.show()","feb6d90a":"fig = px.scatter_3d(embeddings_df['feature_18'], x='feature_18_embedding_0', y='feature_18_embedding_1', z='feature_18_embedding_2', color =embeddings_df['feature_18'].index)\nfig.show()","bd16ecc2":"def fit_transform(data, embeddings, encoders, drop_categorical_vars=False):\n\n    dfs={}\n    for cat_var in tqdm(embeddings.keys()):\n        df = pd.DataFrame(embeddings[cat_var])\n        df.index = encoders[cat_var].classes_\n        df.columns = [cat_var +  '_embedding_' + str(num) for num in df.columns]\n        data = data.merge(df, how='left', left_on=cat_var, right_index=True)\n\n    if drop_categorical_vars:\n        return data.drop(list(embeddings.keys()), axis=1)\n    else:\n        return data\n\ndf_categorical_coded = fit_transform(df_all, embeddings, encoders, True)\ntrain_categorical_coded, test_categorical_coded= df_categorical_coded[:len(train_df)], df_categorical_coded[len(train_df):]\ntrain_categorical_coded['target'] = y","68b6a4bc":"train_categorical_coded.head(5)","bb0fbc4a":"test_categorical_coded.head(5)","88839ea6":"train_categorical_coded.to_csv(\"tps-05-train_categorical_coded.csv\")\ntest_categorical_coded.to_csv(\"tps-05-test_categorical_coded.csv\")","ecdafcea":"from sklearn.preprocessing import MinMaxScaler \n\nscaler = MinMaxScaler()\n\nX_train = scaler.fit_transform(df_all[:len(train_df)])\nX_validation = scaler.transform(df_all[len(train_df):])","d5ef34d0":"# Let's define simple AutoEncoder\n\nencoding_dim = 40\n\ninput_size = len(df_all.columns)\n\ninput_df = Input(shape=(input_size,))\nx = Dense(32, kernel_initializer='normal', activation='relu')(input_df)\nencoded = Dense(encoding_dim, activation='relu')(x)\nx = Dense(32, kernel_initializer='normal', activation='relu')(encoded)\ndecoded = Dense(input_size, activation='sigmoid')(x)\n\nautoencoder = Model(input_df, decoded)\n\nautoencoder.compile(optimizer='adadelta', loss='mean_squared_error')\n\nautoencoder.fit(X_train, X_train,\n                epochs=250,\n                batch_size=256,\n                shuffle=True,\n                validation_data=(X_validation, X_validation))","8ddab7d3":"encoder = Model(input_df, encoded)\n\nauto_enc_df_all = pd.DataFrame(encoder.predict(df_all))\nauto_enc_df_all.columns = ['f_' + str(num) for num in auto_enc_df_all.columns]","3e497522":"auto_df_all = pd.concat([df_all, auto_enc_df_all], axis=1)\n\n\nauto_enc_X_train = auto_df_all[:len(train_df)]\nauto_enc_X_train['target'] = y\n\nauto_enc_test = auto_df_all[len(train_df):]","b65a70c3":"auto_enc_X_train.head(5)","91402ba7":"auto_enc_test.head(5)","22a54b84":"auto_enc_X_train.to_csv(\"tps-05-train_autoencoder-40_coded.csv\")\nauto_enc_test.to_csv(\"tps-05-test_autoencoder-40_coded.csv\")","7c5f2aab":"!pip install -U kaggler -q","eaeedb58":"import kaggler\nfrom kaggler.preprocessing import DAE, TargetEncoder, LabelEncoder\nprint(kaggler.__version__)","7227e11e":"encoding_dim = 128\nseed = 42\nn_fold = 5\nn_class = 4","c832a7b8":"df_all = df_all.astype(\"int64\")\ndae = DAE(cat_cols=df_all.columns.to_list(), num_cols=[], encoding_dim=encoding_dim, random_state=seed, \n          swap_prob=.3, n_layer=3)\nX = dae.fit_transform(df_all)\ndf_dae = pd.DataFrame(X, columns=[f'dae1_{x}' for x in range(X.shape[1])])\nprint(df_dae.shape)\ndf_dae.head()","6d0de451":"cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\nte = TargetEncoder(cv=cv)\n\nfeature_cols = df_all.columns.tolist()\ny = y_df.squeeze()\n\nte.fit(train_df[feature_cols], y)\ndf_te = te.transform(df_all[feature_cols])\ndf_te.columns = [f'te_{x}' for x in df_all.columns]\ndf_te.head(5)","0c3a000b":"dae_df_all = pd.concat([df_all, df_te, df_dae], axis=1)\ndae_train = dae_df_all[:len(train_df)]\ndae_train['target'] = y\n\ndae_test = dae_df_all[len(train_df):]","8453b325":"dae_train.to_csv(\"tps-05-train_dae_coded.csv\")\ndae_test.to_csv(\"tps-05-test_dae_coded.csv\")","08ad038a":"# Train LightAutoML on AutoEncoder or Categorical Encoding\n\nDATA = \"DAE\" # Denoise Autoencoder\n#DATA = \"EMB\" # Categorical Encoding (Embedding)\n#DATA = \"AUE\" # AutoEncoder","944947c8":"if DATA == \"AUE\":\n    train_AutoML = auto_enc_X_train\n    test_AutoML = auto_enc_test\nelif DATA == \"EMB\":\n    train_AutoML = train_categorical_coded\n    test_AutoML = test_categorical_coded\nelse: #DAE\n    train_AutoML = dae_train\n    test_AutoML = dae_test","e4cda36d":"pip install -U lightautoml -q","4c1f14ce":"from lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\n\nimport pandas as pd","a8ed25ac":"N_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 3 * 3600 # Time in seconds for automl run\nTARGET_NAME = 'target'","03cd146f":"task = Task('multiclass',)\n\nroles = {\n    'target': TARGET_NAME,\n    'drop': ['id'],\n}","d7843a74":"automl = TabularUtilizedAutoML(task = task, \n                               timeout = TIMEOUT,\n                               cpu_limit = N_THREADS,\n                               reader_params = {'n_jobs': N_THREADS})\noof_pred = automl.fit_predict(train_AutoML, roles = roles)\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred[:10], oof_pred.shape))","4810864d":"laml_test_pred = automl.predict(test_AutoML)\nprint('Prediction for test data:\\n{}\\nShape = {}'.format(laml_test_pred[:10], laml_test_pred.shape))\n\nprint('Check scores...')\nprint('OOF score: {}'.format(log_loss(train_AutoML[TARGET_NAME].values, oof_pred.data)))","3dfdcede":"submission = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')\n\nlaml_submission = submission.copy()\n\nlaml_submission.iloc[:, 1:] = laml_test_pred.data","7a79816b":"laml_submission.drop(\"id\", axis=1).describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","3e8ab550":"laml_submission.to_csv(\"sub-tps-05-laml_submission.csv\",  index = False)","a0c5b930":"!pip install -q -U git+https:\/\/github.com\/mljar\/mljar-supervised.git@dev -q","f74537c9":"from supervised.automl import AutoML # mljar-supervised","b149ccf2":"x_cols = train_AutoML.columns[1:-1].tolist()\ny_col = train_AutoML.columns[-1]\n\nautoml = AutoML(\n    mode=\"Compete\", \n    total_time_limit=4*3600\n)\nautoml.fit(train_AutoML[x_cols], train_AutoML[y_col])","fa9ba49f":"mljar_preds = automl.predict_proba(test_AutoML)","3eed1f43":"mljar_submission = submission.copy()\n\nmljar_submission[mljar_submission.columns[1:]] = mljar_preds","78382936":"mljar_submission.drop(\"id\", axis=1).describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","3619bc31":"mljar_submission.to_csv(\"sub-tps-05-mljar_submission.csv\", index = False)","f70575be":"blended_submission = pd.read_csv(\"..\/input\/tps05blender-v2\/tps05-remek-blender_v2.csv\")","35ff5a03":"def ensemble(a, b, c = 0):\n    if (not isinstance(c, pd.DataFrame)):\n        output = a.copy()\n        output[\"Class_1\"] = (a.Class_1 * 0.5 + b.Class_1 * 0.5) \n        output[\"Class_2\"] = (a.Class_2 * 0.5 + b.Class_2 * 0.5) \n        output[\"Class_3\"] = (a.Class_3 * 0.5 + b.Class_3 * 0.5) \n        output[\"Class_4\"] = (a.Class_4 * 0.5 + b.Class_4 * 0.5)\n        return output \n    else: \n        output = a.copy() \n        output[\"Class_1\"] = a.Class_1 * 0.6 + b.Class_1 * 0.2 + c.Class_1 * 0.2\n        output[\"Class_2\"] = a.Class_2 * 0.6 + b.Class_2 * 0.2 + c.Class_2 * 0.2\n        output[\"Class_3\"] = a.Class_3 * 0.6 + b.Class_3 * 0.2 + c.Class_3 * 0.2\n        output[\"Class_4\"] = a.Class_4 * 0.6 + b.Class_4 * 0.2 + c.Class_4 * 0.2\n        return output  \n    \ndef generate(a, b, c):\n    ab = ensemble(a, b)\n    ab.to_csv('sub-tps-05-blend-ab.csv',index=False)   \n    ac = ensemble(a, c)\n    ac.to_csv('sub-tps-05-blend-ac.csv',index=False)\n    bc = ensemble(b, c)\n    bc.to_csv('sub-tps-05-blend-bc.csv',index=False)  \n    abc = ensemble(a, b, c)\n    abc.to_csv('sub-tps-05-blend-abc.csv',index=False)","16a9290c":"generate(blended_submission, laml_submission, mljar_submission)","84bc5d3f":"bl_laml = pd.read_csv(\"sub-tps-05-blend-ab.csv\")\nbl_mljar = pd.read_csv(\"sub-tps-05-blend-ac.csv\")\nlaml_mljar = pd.read_csv(\"sub-tps-05-blend-bc.csv\")\nbl_laml_mljar = pd.read_csv(\"sub-tps-05-blend-abc.csv\")","7a0b4440":"bl_laml.drop(\"id\", axis=1).describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","4c9446ff":"bl_mljar.drop(\"id\", axis=1).describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","2512876f":"laml_mljar.drop(\"id\", axis=1).describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","89e9f54d":"bl_laml_mljar.drop(\"id\", axis=1).describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","c85c4ccb":"# PART 1. CATEGORICAL ENCODING using Embedding\n\nThis part will be improved. Now Categorical Encoding is not perfect (I am going to use cross validation for building Embedding).","54c39900":"# PART 2. Autoencoder on Tabular Data\n\n\nFrom post in comments - Alexander Ryzhkov:\n> What are the fix variants which can help:\n> 1) Try not to use target while you create embeddings - you can use autoencoder for that\n> 2) If you want to use the target, you can do it based on cross-validation, but in this situation you can use only OOF predictions instead of categorical embeddings because for 2 different runs on k-1 folds embeddings for sure do not have same columns to concat them vertically.","3865ce27":"### Let's look into feature_0 coded ","1ce6fa98":"### Let's encode input data into embedding values","584e36cb":"# This notebook is the result of my research related to tps-05 competition data.\n\n","0a1c8781":"## 3. BLENDING","b88d3a51":"In this notebook, I conducted three data encoding experiments that I think could have benefited from a better result in the competition. Finally, I built the models with two leading autoML solutions - LightAutoML and MLJAR.\n\n<div class=\"alert alert-success\">\n  <strong>This notebook provides three way to encode TPS-05 data using:<\/strong>\n    <ul>\n        <li>NN (Embedding) - Categorical Encoding<\/li>\n        <li>AutoEncoder<\/li>\n        <li>Denoise Autoencoder<\/li>\n    <\/ul>\n<\/div>\n\n#### I know that asking directly in notebook for votes is not the best idea but please appreciate my work. I put a lot of effort into it to get the best result. At the same time, I do not hide my research. I share with you.","ec11e70d":"## 2. MLJAR","211bae4e":"# PART 2. Denoise AutoEncoder (DAE) on Tabular Data\nI will use Kaggler API","42cbd061":"# AUTO ML PREDICTION","695eafbe":"Dirty Kaggle code :) This is not the best ML practice ... but ... we can call this .... creativity :)","c9f3f048":"### For coding we will use Embedding leayer (3 embeddings for each categorical feature)","50972096":"### Let's look into graphical representation of feature embedding","dfc892a7":"## 1. LightAutoML"}}