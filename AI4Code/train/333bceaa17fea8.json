{"cell_type":{"2563e7a4":"code","20e6926b":"code","ac0ff10b":"code","d319b258":"code","9c31fc0e":"code","b1760b07":"code","4d5c9a27":"code","6e1c3c37":"code","f9210c39":"code","daa08c7a":"code","a19905be":"code","0e4eb2e3":"code","fa14cd42":"code","dd9f4646":"code","1b01d9f3":"code","3f60745d":"code","7254d68e":"code","0c66497d":"code","a3c05b4a":"code","6760ed7d":"code","2394a892":"code","f26338c8":"code","d01c2c68":"code","c9e9af91":"code","9bd25b18":"code","9ca53461":"code","87be6de1":"markdown","0fa54cd6":"markdown","032d2942":"markdown","f891fa85":"markdown","6f6dbae6":"markdown","9dd8ac85":"markdown","c2a2fba4":"markdown","21139ce4":"markdown","0937f29d":"markdown","4c994ba7":"markdown","573e95ea":"markdown","7c82de86":"markdown","2c3b43bf":"markdown","d9f00d98":"markdown","de7ec192":"markdown","ec30f06f":"markdown","b42ffdcc":"markdown","37f94e85":"markdown","59bc9521":"markdown","d5f89efe":"markdown","bb3d6fb9":"markdown","19882444":"markdown"},"source":{"2563e7a4":"!pip install kneed","20e6926b":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport dask_xgboost as xgb\nimport dask.dataframe as dd\nfrom sklearn import preprocessing, metrics\nfrom kneed import KneeLocator\nimport gc\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ac0ff10b":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\n\n# function to read the data and merge it (ignoring some columns, this is a very fst model)\n\n\ndef read_data():\n    print('Reading files...')\n    calendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\n    calendar = reduce_mem_usage(calendar)\n    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n    sell_prices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\n    sell_prices = reduce_mem_usage(sell_prices)\n    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n    sales_train_validation = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\n    print('Sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n    submission = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')\n    print('Submission has {} rows and {} columns'.format(submission.shape[0], submission.shape[1]))\n    return calendar, sell_prices, sales_train_validation, submission\n\n\ndef melt_and_merge(calendar, sell_prices, sales_train_validation, submission, nrows = 55000000, merge = False):\n        \n    # melt sales data, get it ready for training\n    sales_train_validation = pd.melt(sales_train_validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n\n    print('Melted sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n    sales_train_validation = reduce_mem_usage(sales_train_validation)\n    \n    # seperate test dataframes\n    test1_rows = [row for row in submission['id'] if 'validation' in row]\n    test2_rows = [row for row in submission['id'] if 'evaluation' in row]\n    test1 = submission[submission['id'].isin(test1_rows)]\n    test2 = submission[submission['id'].isin(test2_rows)]\n    \n    # change column names\n    test1.columns = ['id', 'd_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919', 'd_1920', 'd_1921', 'd_1922', 'd_1923', \n                     'd_1924', 'd_1925', 'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', 'd_1931', \n                      'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937', 'd_1938', 'd_1939', 'd_1940', 'd_1941']\n    test2.columns = ['id', 'd_1942', 'd_1943', 'd_1944', 'd_1945', 'd_1946', 'd_1947', 'd_1948', 'd_1949', 'd_1950', 'd_1951', \n                     'd_1952', 'd_1953', 'd_1954', 'd_1955', 'd_1956', 'd_1957', 'd_1958', 'd_1959', \n                      'd_1960', 'd_1961', 'd_1962', 'd_1963', 'd_1964', 'd_1965', 'd_1966', 'd_1967', 'd_1968', 'd_1969']\n    \n    # get product table\n    product = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n    \n    # merge with product table\n    test2['id'] = test2['id'].str.replace('_evaluation','_validation')\n    test1 = test1.merge(product, how = 'inner', on = 'id')\n    test2 = test2.merge(product, how = 'inner', on = 'id')\n    test2['id'] = test2['id'].str.replace('_validation','_evaluation')\n    \n    # \n    test1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    test2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    \n    sales_train_validation['part'] = 'train'\n    test1['part'] = 'test1'\n    test2['part'] = 'test2'\n    \n    data = pd.concat([sales_train_validation, test1, test2], axis = 0, ignore_index=True)\n    \n    del sales_train_validation, test1, test2\n    \n    # get only a sample for fast training\n    data = data.loc[nrows:]\n    \n    # drop some calendar features\n    calendar.drop(['weekday', 'wday', 'month', 'year'], inplace = True, axis = 1)\n    \n    # delete test2 for now\n    data = data[data['part'] != 'test2']\n    \n    if merge:\n        # notebook crash with the entire dataset (maybee use tensorflow, dask, pyspark xD)\n        data = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n        data.drop(['d', 'day'], inplace = True, axis = 1)\n        # get the sell price data (this feature should be very important)\n        data = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n        print('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n    else: \n        pass\n    \n    gc.collect()\n    \n    return data\n  \ncalendar, sell_prices, sales_train_validation, submission = read_data()\n\n# sales_train_validation.head()\n# data = melt_and_merge(calendar, sell_prices, sales_train_validation, submission, nrows = 27500000, merge = True)","d319b258":"def transform(data):\n    \n    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in nan_features:\n        data[feature].fillna('unknown', inplace = True)\n        \n    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in cat:\n        encoder = preprocessing.LabelEncoder()\n        data[feature] = encoder.fit_transform(data[feature])\n    \n    return data\n\ndef simple_fe(data):\n    \n    # rolling demand features\n    data['lag_t28'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n    data['lag_t29'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(29))\n    data['lag_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(30))\n    data['rolling_mean_t7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n    data['rolling_std_t7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n    data['rolling_mean_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n    data['rolling_mean_t90'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(90).mean())\n    data['rolling_mean_t180'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(180).mean())\n    data['rolling_std_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n    data['rolling_skew_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).skew())\n    data['rolling_kurt_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).kurt())\n    \n    \n    # price features\n    data['lag_price_t1'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n    data['price_change_t1'] = (data['lag_price_t1'] - data['sell_price']) \/ (data['lag_price_t1'])\n    data['rolling_price_max_t365'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max())\n    data['price_change_t365'] = (data['rolling_price_max_t365'] - data['sell_price']) \/ (data['rolling_price_max_t365'])\n    data['rolling_price_std_t7'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(7).std())\n    data['rolling_price_std_t30'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(30).std())\n    data.drop(['rolling_price_max_t365', 'lag_price_t1'], inplace = True, axis = 1)\n    \n    # time features\n    data['date'] = pd.to_datetime(data['date'])\n    data['year'] = data['date'].dt.year\n    data['month'] = data['date'].dt.month\n    data['week'] = data['date'].dt.week\n    data['day'] = data['date'].dt.day\n    data['dayofweek'] = data['date'].dt.dayofweek\n    \n    \n    return data\n\ndef run_lgb(data):\n    \n    # going to evaluate with the last 28 days\n    # Modified this to include different date range (See Cell: X for more detailed reason)\n    x_train = data[data['date'] <= '2016-03-27']\n    y_train = x_train['demand']\n    x_val = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n    y_val = x_val['demand']\n    test = data[(data['date'] > '2016-04-24')]\n    del data\n    gc.collect()\n\n    # define random hyperparammeters\n    params = {\n        'boosting_type': 'gbdt',\n        'metric': 'rmse',\n        'objective': 'regression',\n        'n_jobs': -1,\n        'seed': 236,\n        'learning_rate': 0.1,\n        'bagging_fraction': 0.75,\n        'bagging_freq': 10, \n        'colsample_bytree': 0.75}\n    train_set = lgb.Dataset(x_train[features], y_train)\n    val_set = lgb.Dataset(x_val[features], y_val)\n    \n    del x_train, y_train\n\n    model = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, valid_sets = [train_set, val_set], verbose_eval = 100)\n    val_pred = model.predict(x_val[features])\n    val_score = np.sqrt(metrics.mean_squared_error(val_pred, y_val))\n    print(f'Our val rmse score is {val_score}')\n    y_pred = model.predict(test[features])\n    test['demand'] = y_pred\n    return test\n\ndef predict(test, submission, filename):\n    predictions = test[['id', 'date', 'demand']]\n    predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\n    predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\n    evaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \n    evaluation = submission[submission['id'].isin(evaluation_rows)]\n\n    validation = submission[['id']].merge(predictions, on = 'id')\n    final = pd.concat([validation, evaluation])\n    \n    final.to_csv(filename, index = False)\n    \n\n# define list of features\nfeatures = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'year', 'month', 'week', 'day', 'dayofweek', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', \n            'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_t28', 'lag_t29', 'lag_t30', 'rolling_mean_t7', 'rolling_std_t7', 'rolling_mean_t30', 'rolling_mean_t90', \n            'rolling_mean_t180', 'rolling_std_t30', 'price_change_t1', 'price_change_t365', 'rolling_price_std_t7', 'rolling_price_std_t30', 'rolling_skew_t30', 'rolling_kurt_t30']\n\n\ndef transform_train_and_eval(data, filename):\n    data = transform(data)\n    data = simple_fe(data)\n    # reduce memory for new features so we can train\n    data = reduce_mem_usage(data)\n    test = run_lgb(data)\n    predict(test, submission, filename)","9c31fc0e":"# for all CA_1 store data\nstore_sales_train_validation = sales_train_validation.loc[sales_train_validation['store_id'] == 'CA_1']\nstore_sell_prices =  sell_prices.loc[sell_prices['store_id'] == 'CA_1']\ndata = melt_and_merge(calendar, store_sell_prices, store_sales_train_validation, submission, nrows = 50000, merge = True)\ntransform_train_and_eval(data,'submission_ca.csv')","b1760b07":"# for clustering CA_1 store data by monthly demand \ncalendar, sell_prices, sales_train_validation, submission = read_data()\nstore_sales_train_validation = sales_train_validation.loc[sales_train_validation['store_id'] == 'CA_1']\nstore_sell_prices =  sell_prices.loc[sell_prices['store_id'] == 'CA_1']\ndata_ca = melt_and_merge(calendar, store_sell_prices, store_sales_train_validation, submission, nrows = 50000, merge = True)\ndata_ca['date'] = pd.to_datetime(data_ca['date'])","4d5c9a27":"data_ca.head()","6e1c3c37":"len(data_ca['item_id'].unique())","f9210c39":"# define function to get aggregated data for clustering \ndef getDataForClusters(df,items):\n    data_demand = []\n    data_sellPrice = []\n    for item in items:\n        df_item_data = df[df['item_id'] == item]\n        data_demand.append(df_item_data.resample('M', on='date')['demand'].mean().values)  \n        data_sellPrice.append(df_item_data.resample('M', on='date')['sell_price'].median().values)  \n    return data_demand, data_sellPrice\n\n# aggregate data with average monthly demand for each cluster\ndef getDataForClustersPlot(df,clusters):\n    data_demand = []\n    for cluster in clusters:\n        df_cluster_data = df[df['cluster'] == cluster]\n        data_demand.append(df_cluster_data.resample('M', on='date')['demand'].mean().values)  \n    return data_demand","daa08c7a":"# Get data for clustering\ntotal_items = data_ca['item_id'].unique()\ndata_demand, data_sellPrice = getDataForClusters(data_ca,total_items)","a19905be":"data_aggregated = [avgDemand * medianSellPrice for avgDemand, medianSellPrice in zip(data_demand, data_sellPrice)]","0e4eb2e3":"# Removing the last month sales as the data indicates 0 data\ndata_for_clustering = pd.DataFrame(data_aggregated, index=total_items).drop(columns=[63]).fillna(0)\ndata_for_clustering.head()","fa14cd42":"# K Means clustering\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (5,5)\n\n# normalizing demand across all departments\nscaler = MinMaxScaler()\nfeatures = scaler.fit_transform(data_for_clustering)\n\n# fit kmeans\nsse = []\nfor k in range(1, 10):\n    kmeans = KMeans(k)\n    kmeans.fit(features)\n    sse.append(kmeans.inertia_)\n\n# elbow curve to find the number of clusters\nplt.plot(range(1, 10), sse)\nplt.Circle((5, 5), 0.5, color='b', fill=False)\nplt.xticks(range(1, 10))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"SSE\")\nplt.show()\n\n# determine number of clusters\nknee = KneeLocator(range(1, 10), sse, curve=\"convex\", direction=\"decreasing\")\nprint(\"Optimal number of clusters:\",knee.elbow)\n\n# fit kmeans with the optimal number of clusters from above plot\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(features)\n\n# create a map of items and corresponding cluster it belongs to\nclusterMap = pd.DataFrame()\nclusterMap['item_id'] = data_for_clustering.index.values\nclusterMap['cluster'] = kmeans.labels_","dd9f4646":"clusterMap.head()","1b01d9f3":"data_with_clusters = data_ca.merge(clusterMap, how='left', on='item_id')\ndata_with_clusters.head()","3f60745d":"unique_clusters = clusterMap['cluster'].unique()\navg_monthly_demand = getDataForClustersPlot(data_with_clusters,unique_clusters)\ndf_avg_monthly_demand = pd.DataFrame(avg_monthly_demand,unique_clusters).drop(columns=[63])\ndf_avg_monthly_demand.head()","7254d68e":"plt.rcParams[\"figure.figsize\"] = (20,5)\nfor i in unique_clusters:\n    plt.plot(df_avg_monthly_demand.columns, df_avg_monthly_demand.loc[i].values, lw=3)\n    plt.xticks(df_avg_monthly_demand.columns)\n    plt.xticks(np.arange(0, 63, 1))\n    plt.legend(['Slow demand Cluster(2)', 'Medium demand Cluster(0)', 'High demand Cluster(1)'])","0c66497d":"plt.plot(np.linspace(0, 63, 63), df_avg_monthly_demand.loc[1].values,c='green', lw=3)\nplt.xticks(df_avg_monthly_demand.columns)\nplt.xticks(np.arange(0, 63, 1))\nplt.legend(['High demand Cluster(1)'])\nplt.show()\n\nplt.plot(np.linspace(0, 63, 63), df_avg_monthly_demand.loc[0].values,c='orange', lw=3)\nplt.xticks(df_avg_monthly_demand.columns)\nplt.xticks(np.arange(0, 63, 1))\nplt.legend(['Medium demand Cluster(0)'])\nplt.show()\n\nplt.plot(np.linspace(0, 63, 63), df_avg_monthly_demand.loc[2].values, lw=3)\nplt.xticks(df_avg_monthly_demand.columns)\nplt.xticks(np.arange(0, 63, 1))\nplt.legend(['Slow demand Cluster(2)'])\nplt.show()","a3c05b4a":"def transform(df_data):\n    \n    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in nan_features:\n        df_data[feature].fillna('unknown', inplace = True)\n        \n    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in cat:\n        encoder = preprocessing.LabelEncoder()\n        df_data[feature] = encoder.fit_transform(df_data[feature])\n    \n    return df_data\n\ndef simple_fe(df_data):\n    \n    # rolling demand features\n    df_data['lag_t28'] = df_data.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n    df_data['lag_t29'] = df_data.groupby(['id'])['demand'].transform(lambda x: x.shift(29))\n    df_data['lag_t30'] = df_data.groupby(['id'])['demand'].transform(lambda x: x.shift(30))\n    df_data['rolling_mean_t7'] = df_data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n    df_data['rolling_std_t7'] = df_data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n    df_data['rolling_mean_t30'] = df_data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n    df_data['rolling_mean_t90'] = df_data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(90).mean())\n    df_data['rolling_mean_t180'] = df_data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(180).mean())\n    df_data['rolling_std_t30'] = df_data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n    df_data['rolling_skew_t30'] = df_data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).skew())\n    df_data['rolling_kurt_t30'] = df_data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).kurt())\n    \n    \n    # price features\n    df_data['lag_price_t1'] = df_data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n    df_data['price_change_t1'] = (df_data['lag_price_t1'] - df_data['sell_price']) \/ (df_data['lag_price_t1'])\n    df_data['rolling_price_max_t365'] = df_data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max())\n    df_data['price_change_t365'] = (df_data['rolling_price_max_t365'] - df_data['sell_price']) \/ (df_data['rolling_price_max_t365'])\n    df_data['rolling_price_std_t7'] = df_data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(7).std())\n    df_data['rolling_price_std_t30'] = df_data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(30).std())\n    df_data.drop(['rolling_price_max_t365', 'lag_price_t1'], inplace = True, axis = 1)\n    \n    # time features\n    df_data['date'] = pd.to_datetime(df_data['date'])\n    df_data['year'] = df_data['date'].dt.year\n    df_data['month'] = df_data['date'].dt.month\n    df_data['week'] = df_data['date'].dt.week\n    df_data['day'] = df_data['date'].dt.day\n    df_data['dayofweek'] = df_data['date'].dt.dayofweek\n    \n    \n    return df_data\n\ndef run_lgb(df_data):\n    \n    # going to evaluate with the last 28 days\n    # Modified this to include different date range (See Cell: X for more detailed reason)\n    x_train = df_data[df_data['date'] <= '2016-03-27']\n    y_train = x_train['demand']\n    x_val = df_data[(df_data['date'] > '2016-03-27') & (df_data['date'] <= '2016-04-24')]\n    y_val = x_val['demand']\n    test = df_data[(df_data['date'] > '2016-04-24')]\n    del df_data\n    gc.collect()\n\n    # define random hyperparammeters\n    params = {\n        'boosting_type': 'gbdt',\n        'metric': 'rmse',\n        'objective': 'regression',\n        'n_jobs': -1,\n        'seed': 236,\n        'learning_rate': 0.1,\n        'bagging_fraction': 0.75,\n        'bagging_freq': 10, \n        'colsample_bytree': 0.75}\n    train_set = lgb.Dataset(x_train[features], y_train)\n    val_set = lgb.Dataset(x_val[features], y_val)\n    \n    del x_train, y_train\n\n    model = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, valid_sets = [train_set, val_set], verbose_eval = 100)\n    val_pred = model.predict(x_val[features])\n    val_score = np.sqrt(metrics.mean_squared_error(val_pred, y_val))\n    print(f'Our val rmse score is {val_score}')\n    y_pred = model.predict(test[features])\n    test['demand'] = y_pred\n    return test\n\ndef predict(test, submission, filename):\n    predictions = test[['id', 'date', 'demand']]\n    predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\n    predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\n#     evaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \n#     evaluation = submission[submission['id'].isin(evaluation_rows)]\n\n    validation = submission[['id']].merge(predictions, on = 'id')\n#     final = pd.concat([validation, evaluation])\n    \n    validation.to_csv(filename, index = False)\n    \n\n# define list of features\nfeatures = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'year', 'month', 'week', 'day', 'dayofweek', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', \n            'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_t28', 'lag_t29', 'lag_t30', 'rolling_mean_t7', 'rolling_std_t7', 'rolling_mean_t30', 'rolling_mean_t90', \n            'rolling_mean_t180', 'rolling_std_t30', 'price_change_t1', 'price_change_t365', 'rolling_price_std_t7', 'rolling_price_std_t30', 'rolling_skew_t30', 'rolling_kurt_t30']\n\n\ndef transform_train_and_eval(df_data, filename):\n    df_data = transform(df_data)\n    df_data = simple_fe(df_data)\n    # reduce memory for new features so we can train\n    df_data = reduce_mem_usage(df_data)\n    test = run_lgb(df_data)\n    predict(test, submission, filename)","6760ed7d":"# Slow demand items cluster\ngroup_2 = data_with_clusters[data_with_clusters['cluster'] == 2]\n\n# Medium demand items cluster\ngroup_0 = data_with_clusters[data_with_clusters['cluster'] == 0]\n\n# High demand items cluster\ngroup_1 = data_with_clusters[data_with_clusters['cluster'] == 1]\n\n# Validate model on each individual groups created above\nprint(\"\\nGroup 0\")\ntransform_train_and_eval(group_0,'submission0.csv')\n\nprint(\"\\nGroup 1\")\ntransform_train_and_eval(group_1,'submission1.csv')\n\nprint(\"\\nGroup 2\")\ntransform_train_and_eval(group_2,'submission2.csv')","2394a892":"submission0 = pd.read_csv('.\/submission0.csv')\nsubmission1 = pd.read_csv('.\/submission1.csv')\nsubmission2 = pd.read_csv('.\/submission2.csv')\nsubmission_ca = pd.read_csv('.\/submission_ca.csv')","f26338c8":"submission2.head()","d01c2c68":"full_submission = pd.read_csv('\/kaggle\/input\/output\/submission.csv')","c9e9af91":"full_submission.head()","9bd25b18":"full_submission.merge(submission2,how='left',on='id')\nleft = full_submission.set_index('id')\nright = submission2.set_index('id')\nfinal = left.reindex(columns=left.columns.union(right.columns))\nfinal.update(right)\nfinal.reset_index(inplace=True)\nfinal","9ca53461":"final.to_csv(\"submission.csv\",index=False)","87be6de1":"**The below code illustrates the total number of items present in the CA_1 store data**","0fa54cd6":"#### **2.6 Data for Clustering**\n\nNow that we have all the data required for clustering such as items, monthly product data of average demand and median sell price - we can now feed this data to a dataframe and visualize the values. The 64th month sales data is not available and every value is defaulted to 0, so explicitly removed this month to avoid some unintended noise during grouping. ","032d2942":"**Get the full submission file containing the submission for all the stores**\n\n**Reference:** https:\/\/www.kaggle.com\/kneroma\/m5-first-public-notebook-under-0-50\/notebook","f891fa85":"#### **2.12 Make Submission to the Competition**\n\nNow we have the predictions from all the three clusters and will use the group that has the lowest rmse score and will replace these items prediction in the full submission dataset having predictions for all the stores and make a final submission to the competition.\n\n**Load the final submission files for each cluster after modeling**","6f6dbae6":"**Viewing the cluster map**","9dd8ac85":"#### **2.8 Data for Plots**\n\nSteps below:\n\n1. Get all the distinct clusters for CA_1 store items using .unique() \n2. Once we identify the unique clusters, we feed in the dataset with clusters attribute and clusters as arguments and make ***getDataForClustersPlot()*** call.\n3. Capture the monthly average demand by each cluster to a list variable returned from the above function call.\n4. We can now feed this data to a dataframe and view the values.\n5. The 64th month sales data is not available and every value is defaulted to 0, so explicitly removed this month to avoid some unintended noise in plots\n","c2a2fba4":"**Write the final submission file to a csv and use this file to make a submission to the competition**","21139ce4":"#### **2.11 Modeling on Clusters**\n","0937f29d":"# Objective\n\n* Make a baseline model that predict the validation (28 days). \n* This competition has 2 stages, so the main objective is to make a model that can predict the demand for the next 28 days","4c994ba7":"#### **2.7 Clustering**\n\nBelow are the steps:\n\n1. **Normalization** : Inorder to transform all of the attributes which may be different orders of magnitude into similar scale so that it can be compared and can contribute equally to the distance computations while using it for clustering. \n2. **K-means**: Using K-means for clustering as the data contains numerical features. \n3. **Plot SSE**: It is ideal to find out the optimal value for K. This can be acheived by fitting multiple values of K and capturing the corresponding SSE values and visualizing elbow curve for all SSE obtained to find the number of clusters.\n4. **Find Optimal K**: As we may or may not find the right value just by eye balling the elbow plot, it is best to use Kneelocator to find the optimal K value from the *elbow* attribute.\n5. **Fit K-means**:  Fit the data with the optimal number of K obtained above and determine the clusters.\n6. **Create Cluster Map**: Create a map containing the item id and the cluster it belongs to.","573e95ea":"**Joining the all stores submission file with the cluster 2 predicted values**","7c82de86":"#### **2.9 Plot Clusters**\n\nThe below code plots average monthly demand for items in each cluster\n\n**X-axis**: Month numbers (0-62): total 63 months\n\n**Y-axis**: Average demand for each month of all items in each cluster","2c3b43bf":"#### **2.4 Data Aggregation**\n\nAfter defining the data aggregation function in earlier step, now its time to put these into play and make the dataset ready for clustering. Below are required steps:\n\n1. Get all the distinct items available in CA_1 store using .unique() \n2. Once we identify the unique items, we feed in the dataset and items as arguments and make ***getDataForClusters()*** call.\n3. Capture the monthly average demand and median sell price lists into its own seperate list variables by deconstructing the tuple returned from the above function call.","d9f00d98":"#### **2.10 Defining Clusters**\n\n1. **Cluster 1**: **High** demand\n\nThe items belonging to this cluster have a high average demand than the rest of the other clusters. The average demand for this group ranges from 10 to 25 reaching it's peak sale during the months 30 to 33.\n\n2. **Cluster 0**: **Medium** demand\n\nThe items belonging to this cluster have a medium average demand than the rest of the other clusters. The average demand for this group ranges from 2 to 5 reaching it's peak sale during the months 30 to 33. The items in this group tend to have similar trend as with the fast selling items.\n\n3. **Cluster 2**: **Low** demand\n\nThe items belonging to this cluster have a very slow selling rate than the rest of the other clusters. The average demand for this group ranges from 0 to 1. There is an increasing trend for these items over the years.","de7ec192":"#### **2.3 Define functions**\n\n***1. getDataForClusters()*** function is defined to get the average monthly demand and the median sell price for each item. The dataset includes 64 months of historical data and so we expect each item to have value for each of these months. \n\nThis function takes in two input parameters of type DataFrame and List and returns two Lists \n\n**Parameters:** \n1. ***df***: Dataframe which includes date, demand, item_id, sell_price features at the least.\n2. ***items***: List which includes unique items in the df parameter defined above.\n\n**Returns:**\n1. ***data_demand***: Includes list of all the average demand aggregated values for each item for all the 64 months.\n2. ***data_sellPrice***: Includes list of all the median sell price aggregated values for each item for all the 64 months.\n\n***2. getDataForClustersPlot()*** function is defined to get the average monthly demand for each cluster for all items. \n\nThis function takes in two input parameters of type DataFrame and List and returns a List\n\n**Parameters:** \n1. ***df***: Dataframe which includes date, demand, item_id features at the least.\n2. ***clusters***: List which includes unique clusters obtained from k-means clustering.\n\n**Returns:**\n1. ***data_demand***: Includes list of all the average demand aggregated values for each cluster for all the 64 months.\n","ec30f06f":"# **2. Cluster and Evaluate CA_1 store data** \n#### Cluster the CA_1 store items based on aggregated monthly demand and sell price of each item in the store\n\n#### **2.1 Data Collection**\n\nLoad the walmart dataset with items and features and filter the data to include records only present in store CA_1. Both the sales_train_validation and sell_prices datasets have been restricted to include only CA_1 store data. The submission dataset is left intact with no filtering at the store level. Type coercions for the date column has been included to facilitate resampling based on month attribute.(see *2.* more more details)","b42ffdcc":"# **1. Evaluate CA_1 store data** \n\nBelow code illustrates the results and metrics after applying the lgb model on CA_1 one store data. ","37f94e85":"#### **2.2 Data Exploration**\n\nOnce all the necessary cleanup steps have been completed, viewing the dataset to understand the features available and their respective values.","59bc9521":"**Join the assigned clusters back to the initial data set based on item_id**","d5f89efe":"* We have the data to build our first model, let's build a baseline and predict the validation data (in our case is test1)","bb3d6fb9":"**View the data for cluster 2 having the lowest RMSE score**","19882444":"#### **2.5 Derived Feature**\n\nIn order to take both demand and sell prices into consideration for grouping items in the CA_1 store - crafted a new derived feature which is the product of average demand and median sell price obtained from the aggregated data above for each month.\n\n**Output**: Is a list of product values of mean demand and median sell price\n\n**Technical details**: Used list comprehensions to do the element wise product after zipping both the mean demand and median sell price lists. Alternatively, the same operation can be done using numpy."}}