{"cell_type":{"a3caf010":"code","ca7c076a":"code","486edbec":"code","11a9200e":"code","bcf76c44":"code","a731bda5":"code","3bdd8b6d":"code","f5101614":"code","96b5d50d":"code","f81c7403":"code","378cec2a":"code","856fe13a":"code","767e4bce":"code","6e5d647c":"code","551fccbb":"code","ed80c897":"code","24c920df":"markdown","b5255881":"markdown","7ddf0593":"markdown","9188d8a8":"markdown","686b0e80":"markdown","bcc9df92":"markdown","1ace4fc2":"markdown","390a062f":"markdown","56a5d84e":"markdown","52af34aa":"markdown","284a9502":"markdown","36afacbe":"markdown","3c4ee147":"markdown","3c1ee3b1":"markdown","602bc03e":"markdown","8e88b80a":"markdown","8a67d85e":"markdown","140454d8":"markdown","88c5b50d":"markdown","e4191002":"markdown","f4beb140":"markdown","f66199cb":"markdown"},"source":{"a3caf010":"import numpy as np # Linear algebra\nimport pandas as pd # Data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import ensemble, model_selection, svm, naive_bayes, neighbors, tree # Machine learning models\nfrom sklearn.metrics import accuracy_score # Score\nimport seaborn as sns # Plotting\n\n# Remove annoying warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\ndata = pd.concat([train_data, test_data])","ca7c076a":"data.head()","486edbec":"data.describe()","11a9200e":"data.describe(include=\"O\")","bcf76c44":"sns.barplot(data=train_data, x='Sex', y='Survived')","a731bda5":"sns.barplot(data=train_data, x='Pclass', y='Survived')","3bdd8b6d":"sns.barplot(data=train_data, x='Embarked', y='Survived')","f5101614":"sns.histplot(data=train_data, x=\"Age\", hue=\"Survived\", stat=\"density\")","96b5d50d":"sns.histplot(data=train_data, x=\"Fare\", hue=\"Survived\", stat=\"density\")","f81c7403":"data = data.fillna({\"Fare\" : data[\"Fare\"].mean(), \"Embarked\": data[\"Embarked\"].mode()})\n# Get all NaN indexes\nindex = data[\"Age\"].index[data[\"Age\"].apply(np.isnan)]\n# Sample values for all NaN indexes\nvalues = np.random.normal(loc=data[\"Age\"].mean(), scale=data[\"Age\"].std(), size=len(data[data[\"Age\"].isna()]))\n# Fill NaN indexes with values\ndata = data.fillna({\"Age\" : {k: v for k,v in zip(index, values)}})\ndata = data.drop(columns=[\"Cabin\"])","378cec2a":"data['Title'] = data['Name'].str.findall('([A-Za-z]*\\.)')\ndata['Fancy Title'] = data['Title'].apply(lambda x: True if x not in [['Mr.'], ['Miss.'], ['Mrs.']] else False)\ndata = data.drop(columns=[\"Name\", \"Title\"])","856fe13a":"data = data.drop(columns=[\"Ticket\"])","767e4bce":"data = pd.get_dummies(data, columns=[\"Pclass\", \"Sex\", \"Embarked\"])","6e5d647c":"train_data = data[data[\"PassengerId\"] < 792]\ntrain_data, val_data = model_selection.train_test_split(train_data)\ntest_data = data[data[\"PassengerId\"] >= 892]\n\ntrain_X = train_data.drop(columns=[\"Survived\", \"PassengerId\"])\ntrain_y = train_data[[\"Survived\"]]\n\nval_X = val_data.drop(columns=[\"Survived\", \"PassengerId\"])\nval_y = val_data[[\"Survived\"]]","551fccbb":"# SVC\nsvc_model = svm.SVC()\nsvc_model.fit(train_X, train_y)\nprint(f'SVC accuracy is: {accuracy_score(val_y, svc_model.predict(val_X))}!')\n\n# Linear SVC\nlsvc_model = svm.LinearSVC()\nlsvc_model.fit(train_X, train_y)\nprint(f'Linear SVC accuracy is: {accuracy_score(val_y, lsvc_model.predict(val_X))}!')\n\n# Naive Bayes\nnb_model = naive_bayes.GaussianNB()\nnb_model.fit(train_X, train_y)\nprint(f'Naive Bayes accuracy is: {accuracy_score(val_y, nb_model.predict(val_X))}!')\n\n# K Neighbors\nkn_model = neighbors.KNeighborsClassifier()\nkn_model.fit(train_X, train_y)\nprint(f'K Neighbors accuracy is: {accuracy_score(val_y, kn_model.predict(val_X))}!')\n\n# Decision Tree\ndt_model = tree.DecisionTreeClassifier()\ndt_model.fit(train_X, train_y)\nprint(f'Decision Tree accuracy is: {accuracy_score(val_y, dt_model.predict(val_X))}!')\n\n# Random Forest\nrf_model = ensemble.RandomForestClassifier()\nrf_model.fit(train_X, train_y)\nprint(f'Random Forest accuracy is: {accuracy_score(val_y, rf_model.predict(val_X))}!')","ed80c897":"clf = naive_bayes.GaussianNB()\nclf.fit(train_X, train_y)\nprint(f\"Train accuracy is: {accuracy_score(train_y, clf.predict(train_X))}!\")\n\n# Predicting\ntest_X = test_data.drop(columns=[\"Survived\", \"PassengerId\"])\npred_y = clf.predict(test_X)\n\n# Saving to file\ndf_to_save = pd.concat([test_data[[\"PassengerId\"]], pd.Series(pred_y)], axis=1)\ndf_to_save = df_to_save.rename(columns={0: \"Survived\"})\ndf_to_save.Survived = df_to_save.Survived.astype(int)\ndf_to_save.to_csv(\"naive_bayes_predict.csv\", index=False)\nprint(\"Predictions Saved!\")","24c920df":"Now we will create \"one hot\" features from our categorical features.","b5255881":"Now we will test (on the validation dataset) a few models and choose the best one. ","7ddf0593":"## Fine Tuning The Model","9188d8a8":"From the previous sections we know the following features have missing values: \"Age\",\"Fare\",\"Cabin\",\"Embarked\".<br>\nOut of those \"Fare\" and \"Embarked\" are missing only 1\/2 rows, so we can safely fill them with the mean\/mode without worrying too much about if we use the proper data completion method.<br>\nWe will drop \"Cabin\" as a feature and we will use normal distribution to fill the missing values in \"Age\".","686b0e80":"Only 'Age' is missing ('Fare' is only missing one row, so we don't need to worry about how to fill its values).<br>\nHow about categorical columns?","bcc9df92":"# Titanic Predictions","1ace4fc2":"## Cleaning The Data & Feature Engineering","390a062f":"We would further drop \"Ticket\" as it is a unique identifier that has no useful data in it.","56a5d84e":"## Choosing A Model","52af34aa":"And so is the port you embarked in.","284a9502":"Naive bayes has no parameters we can tune, so we will skip this part.","36afacbe":"Let's look at the data:","3c4ee147":"By now we can see a few features we would like to drop, such as: 'Name' (we might try to extract some useful data from it before dropping it) and 'Ticket' (just an identifier).<br>\nHow about missing values (numerical)?","3c1ee3b1":"## Exploratory Data Analysis","602bc03e":"We can see that sex greatly affects your survival chances.","8e88b80a":"## Imports & Loading Data","8a67d85e":"Next we would like to extract any special titles from passengers names and keep those that are \"fancy\" (not Mr.\/Miss.\/Mrs.)","140454d8":"Next we will look at the distribution of age and fare as a function of survival.","88c5b50d":"And so is your passenger class.","e4191002":"We can see that naive bayes gives the best results, so we will use it as our model.","f4beb140":"Same story as the numerical data, a single column ('Cabin') is missing data.<br>\nNow let's look at how the different features affect the survival of a passenger.","f66199cb":"## Saving The Results"}}