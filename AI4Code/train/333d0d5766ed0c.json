{"cell_type":{"6c6cd6d6":"code","13167e9b":"code","5388a7a2":"code","dceefa6e":"code","192ec74e":"code","95d5c471":"code","1eb3f635":"code","19062244":"code","b32971e4":"code","3c3f0f37":"code","83a6e923":"code","ee98fa2d":"code","306a3bcc":"code","652c30db":"code","136039b7":"code","867cf949":"code","ae092f9c":"code","f44c1503":"code","d38c86fe":"code","895485ad":"code","b383de37":"markdown","020f8e5c":"markdown","ca72ea58":"markdown","ee08b7b2":"markdown","1a45f42d":"markdown","6e5ece5e":"markdown","e6ad1d76":"markdown","da70f860":"markdown","480cd87c":"markdown","e2fdb2f5":"markdown","9f9b7dfc":"markdown","31e7cba2":"markdown","d49b6e94":"markdown","7d38d910":"markdown","0157e68b":"markdown"},"source":{"6c6cd6d6":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport random\nfrom pprint import pprint","13167e9b":"df = pd.read_csv(\"..\/input\/Iris.csv\")\ndf = df.drop(\"Id\", axis=1)\nprint(df.shape)\ndf.head()","5388a7a2":"# function to split dataframe to train and test data \ndef train_test_split(df, test_size):\n    if isinstance(test_size, float):\n        test_size = round(test_size * len(df))\n    indices = df.index.tolist()\n    test_indices = random.sample(population=indices, k=test_size)\n    test_df = df.loc[test_indices]\n    train_df = df.drop(test_indices)\n    return train_df, test_df","dceefa6e":"random.seed(0) # setting random seed to always have same split\ntrain_df, test_df = train_test_split(df, test_size=20) # 20 rows compose a test dataframe, rest 130 - train","192ec74e":"# checks number of classes in dataframe. If n=1 the data is pure, meaning it will overfit\ndef check_purity(data):\n    label_column = data[:, -1]\n    unique_classes = np.unique(label_column)\n    if len(unique_classes) == 1:\n        return True\n    else:\n        return False","95d5c471":"def classify_data(data):\n    label_column = data[:, -1]\n    # returns unique classes and number of instances\n    unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n    # returns index of the class with the biggest number of instances\n    index = counts_unique_classes.argmax()\n    # returns name of the class with the biggest number fo instances\n    classification = unique_classes[index]\n    return classification","1eb3f635":"def get_potential_splits(data):\n    potential_splits = {}\n    _, n_columns = data.shape\n    # excluding the last column which is the label\n    for column_index in range(n_columns - 1):\n        # initialization of empty array for particular column index\n        potential_splits[column_index] = [] \n        # takes all values from dataframe corresponding to column index\n        values = data[:, column_index] \n        # takes unique values from values variable\n        unique_values = np.unique(values) \n        for index in range(len(unique_values)):\n            if index != 0:\n                current_value = unique_values[index]\n                previous_value = unique_values[index - 1]\n                # takes mid point of two close unique values \n                potential_split = (current_value + previous_value) \/ 2 \n                # add to potential splits dict\n                potential_splits[column_index].append(potential_split) \n    return potential_splits","19062244":"def split_data(data, split_column, split_value):\n    # gets split column values\n    split_column_values = data[:, split_column]\n    # data below split\n    data_below = data[split_column_values <= split_value]\n    # data above split\n    data_above = data[split_column_values >  split_value]\n    return data_below, data_above","b32971e4":"def calculate_entropy(data):\n    # takes label of the row\n    label_column = data[:, -1]\n    # takes number of instances of each class\n    _, counts = np.unique(label_column, return_counts=True)\n    # array of probabilities of the classes (each number of one instance is divided by sum of the instances) )\n    probabilities = counts \/ counts.sum()\n    # calculates entropy\n    entropy = sum(probabilities * -np.log2(probabilities))\n    return entropy","3c3f0f37":"def calculate_overall_entropy(data_below, data_above):\n    # number of data points\n    n = len(data_below) + len(data_above)\n    # p for the data below divided by number of data points\n    p_data_below = len(data_below) \/ n\n    # p for the data above divided by number of data points\n    p_data_above = len(data_above) \/ n\n    # overall entropy = p_below * entropy + p_above * entropy\n    overall_entropy =  (p_data_below * calculate_entropy(data_below) \n                      + p_data_above * calculate_entropy(data_above))\n    return overall_entropy","83a6e923":"def determine_best_split(data, potential_splits):\n    overall_entropy = 9999\n    for column_index in potential_splits: # compares entropy one vs all\n        for value in potential_splits[column_index]:\n            # splits data to below and above\n            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n            # current entropy of below and above splits\n            current_overall_entropy = calculate_overall_entropy(data_below, data_above)\n            # if current entropy  \n            if current_overall_entropy <= overall_entropy:\n                # update overall entropy \n                overall_entropy = current_overall_entropy\n                # then best split columns is column_index\n                best_split_column = column_index\n                # best split value is value\n                best_split_value = value\n    \n    return best_split_column, best_split_value","ee98fa2d":"def decision_tree_algorithm(df, counter=0, min_samples=2, max_depth=5):\n    # data preparations\n    if counter == 0:\n        global COLUMN_HEADERS\n        COLUMN_HEADERS = df.columns\n        data = df.values\n    else:\n        data = df           \n    \n    # base cases\n    if (check_purity(data)) or (len(data) < min_samples) or (counter == max_depth):\n        # if data is pure => simply return classification\n        classification = classify_data(data)  \n        return classification\n    \n    # if data is not pure\n    else:    \n        counter += 1\n        # helper functions \n        potential_splits = get_potential_splits(data)\n        # takes split column and value\n        split_column, split_value = determine_best_split(data, potential_splits)\n        # \n        data_below, data_above = split_data(data, split_column, split_value)\n        # instantiate sub-tree\n        feature_name = COLUMN_HEADERS[split_column]\n        question = \"{} <= {}\".format(feature_name, split_value)\n        sub_tree = {question: []}\n        \n        # find answers (recursion)\n        yes_answer = decision_tree_algorithm(data_below, counter, min_samples, max_depth)\n        no_answer = decision_tree_algorithm(data_above, counter, min_samples, max_depth)\n        \n        # If the answers are the same, then there is no point in asking the qestion.\n        # This could happen when the data is classified even though it is not pure\n        # yet (min_samples or max_depth base case).\n        if yes_answer == no_answer:\n            sub_tree = yes_answer\n        else:\n            sub_tree[question].append(yes_answer)\n            sub_tree[question].append(no_answer)\n        \n        return sub_tree","306a3bcc":"dtree = decision_tree_algorithm(train_df, max_depth=3)\npprint(dtree)","652c30db":"from sklearn import tree\nfrom sklearn.tree import export_graphviz\ndecision_tree = tree.DecisionTreeClassifier(criterion='entropy',max_depth=3)\ndecision_tree.fit(train_df[['SepalLengthCm', 'SepalWidthCm','PetalLengthCm', 'PetalWidthCm']], train_df['Species'])\nexport_graphviz(decision_tree, out_file='tree.dot',feature_names = ['SepalLengthCm', 'SepalWidthCm','PetalLengthCm', 'PetalWidthCm'],\n                class_names = df['Species'], filled=True, rounded=True, special_characters=True) \n\n!dot -Tpng tree.dot -o tree.png -Gdpi=600\nfrom IPython.display import Image\nImage(filename = 'tree.png')","136039b7":"test_df.head(20)","867cf949":"test_df.loc[0] = (5.1, 2.6, 3.1, 1.2, 'Iris-setose')\nexample = test_df.iloc[-1]\nprint(example)","ae092f9c":"def classify_example(example, dtree):\n    question = list(dtree.keys())[0]\n    feature_name, comparison_operator, value = question.split(\" \")\n\n    # ask question\n    if example[feature_name] <= float(value):\n        answer = dtree[question][0]\n    else:\n        answer = dtree[question][1]\n\n    # base case\n    if not isinstance(answer, dict):\n        return answer\n    \n    # recursive part\n    else:\n        residual_tree = answer\n        return classify_example(example, residual_tree)","f44c1503":"classify_example(example, dtree)","d38c86fe":"def calculate_accuracy(df, tree):\n\n    df[\"classification\"] = df.apply(classify_example, axis=1, args=(tree,))\n    df[\"classification_correct\"] = df[\"classification\"] == df[\"Species\"]\n    \n    accuracy = df[\"classification_correct\"].mean()\n    \n    return accuracy","895485ad":"accuracy = calculate_accuracy(test_df, dtree)\naccuracy","b383de37":"### Lowest Overall Entropy","020f8e5c":"### Classify","ca72ea58":"### Algorithm","ee08b7b2":"# Helper Functions","1a45f42d":"# Decision Tree Algorithm","6e5ece5e":"### Potential splits","e6ad1d76":"sub_tree = {question: [yes_answer, no_answer]}","da70f860":"# Load and Prepare Data","480cd87c":"# Calculate Accuracy","e2fdb2f5":"# Classification","9f9b7dfc":"### Split Data","31e7cba2":"# Train-Test-Split","d49b6e94":"# Import Statements","7d38d910":"### Data pure","0157e68b":"### Representation of the Decision Tree"}}