{"cell_type":{"68a11275":"code","9e6225a5":"code","123862cc":"code","7e553d77":"code","a5ec7a09":"code","c59dbfc6":"code","7127cfa1":"code","45ba19bf":"code","9fd291ab":"code","f94a2124":"code","634633cf":"code","026acdd8":"code","f846e6c6":"code","360f5caa":"code","609ffbd7":"code","69eef38f":"code","36289cb1":"code","b860a26d":"code","fb49508b":"code","4eb563a7":"code","071a2ac2":"code","8388af58":"code","6860ab3c":"code","4dbd1afd":"code","c20cfb87":"code","8795ef27":"code","ef2dd5b5":"code","085076f2":"code","ecc1f592":"code","fe9f401c":"code","e741c1f6":"code","5023f76b":"code","03b0579e":"code","a18d4bcb":"markdown","68ad5994":"markdown","ac59500c":"markdown","7be40d47":"markdown","b743cba3":"markdown","eeccf0b9":"markdown","86979646":"markdown","d94eef87":"markdown","b475f787":"markdown","0f9d8d93":"markdown","4200abf5":"markdown","d27369ee":"markdown","ff6827eb":"markdown","eaa9036a":"markdown","7bad8eec":"markdown","bcc073a3":"markdown","1736d199":"markdown","332645a9":"markdown","d8a89f2b":"markdown","419e51c1":"markdown","c896da1e":"markdown","2f2e362b":"markdown"},"source":{"68a11275":"import os\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nfrom matplotlib.ticker import FormatStrFormatter\n\nimport gc\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9e6225a5":"train = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv', index_col='Id')\ntest = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-dec-2021\/test.csv', index_col='Id')\nsubmission= pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-dec-2021\/sample_submission.csv', index_col='Id')","123862cc":"# this code snippet is taken form https:\/\/www.kaggle.com\/c\/tabular-playground-series-oct-2021\/discussion\/275854\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)\ngc.collect()","7e553d77":"print('shape')\nprint(train.shape)\nprint(test.shape)\n\nprint('Nullvalues')\ndisplay(train.isna().sum().sum())\ndisplay(test.isna().sum().sum())","a5ec7a09":"train.head()","c59dbfc6":"numerical_features = train.columns[:10]\ncategorical_features = train.columns[10:]","7127cfa1":"train[numerical_features].describe().T.sort_values(by='mean' , ascending = False)\\\n.style.background_gradient(cmap='Greys')\\\n.bar(subset=[\"mean\",], color='#6495ED')\\\n.bar(subset=[\"max\"], color='#ff355d')","45ba19bf":"test[numerical_features].describe().T.sort_values(by='mean' , ascending = False)\\\n.style.background_gradient(cmap='Greys')\\\n.bar(subset=[\"mean\",], color='#6495ED')\\\n.bar(subset=[\"max\"], color='#ff355d')","9fd291ab":"target = train['Cover_Type']","f94a2124":"pal = ['#6495ED','#ff355d', '#8fdab0', 'red', 'red', '#000000', '#ccc000']\nplt.figure(figsize=(8, 6))\nax = sns.countplot(x=target, palette=pal)\nax.set_title('Target variable distribution', fontsize=20, y=1.05)\n\nsns.despine(right=True)\nsns.despine(offset=10, trim=True)","634633cf":"for i in range(1, 8):\n    print('Number of samples, Cover_Type{} = {}'.format(i, len(train[train['Cover_Type'] == i])))","026acdd8":"train_ = train.sample(40000, random_state=1221)\ntest_ = test.sample(10000, random_state=1221)\n\nfeatures = train.columns\nnumerical_features = features[:-1]","f846e6c6":"def density_plotter(a, b, title):    \n    L = len(numerical_features[a:b])\n    nrow= int(np.ceil(L\/5))\n    ncol= 5\n    \n    fig, ax = plt.subplots(nrow, ncol,figsize=(20, 6), sharey=False, facecolor='#dddddd')\n\n    fig.subplots_adjust(top=0.90)\n    i = 1\n    for feature in numerical_features[a:b]:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.kdeplot(train_[feature], shade=True,  color='#6495ED',  alpha=0.85, label='train')\n        ax = sns.kdeplot(test_[feature], shade=True, color='#ff355d',  alpha=0.85, label='test')\n        ax.yaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n        ax.xaxis.set_label_position('top')\n        ax.set_ylabel('')\n        ax.set_yticks([])        \n        ax.set_xticks([])        \n           \n        i += 1\n\n    lines, labels = fig.axes[-1].get_legend_handles_labels()    \n    fig.legend(lines, labels, loc = 'upper right',borderaxespad= 4.0, title='data set') \n\n    plt.suptitle(title, fontsize=20)\n    plt.show()","360f5caa":"density_plotter(a=0, b=10, title='Density plot: train & test data (numerical features)')","609ffbd7":"## Noticablly different features \nff = ['Wilderness_Area1', 'Wilderness_Area3', 'Soil_Type7', 'Soil_Type15']","69eef38f":"def count_plot_testTrain(data1, data2, features, titleText):\n    L = len(features)\n    nrow= int(np.ceil(L\/9))\n    ncol= 9    \n    remove_last= (nrow * ncol) - L    \n\n    fig, ax = plt.subplots(nrow, ncol,figsize=(22, 14), sharey=True, facecolor='#dddddd')\n    \n    fig.subplots_adjust(top=0.92)\n    i = 1\n    for feature in features[:-1]:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.countplot(x=feature, color='#6495ED', data=data1, label='train')\n        ax = sns.countplot(x=feature, color='#ff355d', data=data2, label='test')\n        ax.set_xlabel(feature)\n        ax.set_ylabel('')\n        ax.set_yticks([])        \n        ax.xaxis.set_label_position('top')         \n        \n        \n        if feature in ff:\n            ax = sns.countplot(x=feature, color='#6495ED', data=data1, label='train')\n            ax = sns.countplot(x=feature, color='#ff355d', data=data2, label='test')            \n            ax.set_facecolor('cyan')\n        \n        i += 1\n        \n    lines, labels = fig.axes[-1].get_legend_handles_labels()    \n    fig.legend(lines, labels, loc = 'upper right',borderaxespad= 4.0, title='data set') \n\n    plt.suptitle(titleText, fontsize=20)\n    plt.show()\n\ncount_plot_testTrain(train_, test_, categorical_features, titleText='Train & test data categorical features count plots ')","36289cb1":"def count_plot(data, features, titleText, hue=None):\n    \n    L = len(features)\n    nrow= int(np.ceil(L\/9))\n    ncol= 9\n    \n    fig, ax = plt.subplots(nrow, ncol,figsize=(22, 14), sharey=True, facecolor='#dddddd')\n    fig.subplots_adjust(top=0.92)\n    \n    i = 1\n    for feature in features[:-1]:\n        total = float(len(data)) \n        plt.subplot(nrow, ncol, i)\n        ax = sns.countplot(x=feature, palette=pal, data=data, hue=hue)\n        ax.set_xlabel(feature)\n        ax.set_ylabel('')\n        ax.xaxis.set_label_position('top')\n        ax.set_yticks([]) \n        ax.get_legend().remove()\n        \n        if feature in ff:\n            ax.set_facecolor('cyan')\n        \n        i += 1\n        \n    lines, labels = fig.axes[-1].get_legend_handles_labels()    \n    fig.legend(lines, labels, loc = 'upper right',borderaxespad= 3.0,title='data set' ) \n    \n    plt.suptitle(titleText ,fontsize = 20)\n    plt.show() \n    \ncount_plot(train_, categorical_features, 'Train data cat_feats: target distribution (count plot)', hue='Cover_Type')","b860a26d":"fig, ax = plt.subplots(1, 1, figsize=(16 , 16), facecolor='#dddddd')\ncorr = train.sample(40000, random_state=2021).corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, ax=ax, square=True, center=0, linewidth=1, #vmax=0.2, vmin=-0.2,\n        cmap=sns.diverging_palette(240, 10, as_cmap=True),\n        cbar_kws={\"shrink\": .85}, mask=mask ) \n\nax.set_title('Correlation heatmap: All features', fontsize=24, y= 1.05);","fb49508b":"cols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n       'Horizontal_Distance_To_Fire_Points','Cover_Type']\ndf = train_[cols]\n\nsns.pairplot(df, hue='Cover_Type', palette='coolwarm', corner=True)\nplt.show()","4eb563a7":"from catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler","071a2ac2":"SEED =2021\n\nX = train\ny = train.pop('Cover_Type')\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=2021, shuffle=True)","8388af58":"clf_xgb = XGBClassifier(\n    seed=SEED,\n    objective=\"multi:softmax\",\n    tree_method = 'gpu_hist',\n    predictor = 'gpu_predictor',)\n\nclf_xgb.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid)],\n            early_stopping_rounds=40,\n            verbose=500);","6860ab3c":"preds_valid = np.array(clf_xgb.predict(X_valid, ))\nvalid_acc = accuracy_score(y_pred=preds_valid, y_true=y_valid)\nprint('Xgboost validation accuracy {}'.format(valid_acc))","4dbd1afd":"preds_test = np.array(clf_xgb.predict(test))\nsubmission['Cover_Type'] = preds_test\nsubmission.head()","c20cfb87":"submission.to_csv(\"submission_xgb.csv\", index=False)","8795ef27":"catb_params = {\n              'loss_function': 'MultiClass',                \n              'task_type':'GPU',\n              'bootstrap_type':'Bernoulli'\n             } \nclf_catboost = CatBoostClassifier(**catb_params)\nclf_catboost.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid)],\n            early_stopping_rounds=40,\n            verbose=500);","ef2dd5b5":"preds_valid = np.array(clf_catboost.predict(X_valid, ))\nvalid_acc = accuracy_score(y_pred=preds_valid, y_true=y_valid)\nprint('Catboost validation accuracy: {}'.format(valid_acc))","085076f2":"preds_test = np.array(clf_catboost.predict(test))\nsubmission['Cover_Type'] = preds_test\nsubmission.head() ","ecc1f592":"submission.to_csv(\"submission_catboost.csv\", index=False)","fe9f401c":"lgbm_params = {'objective': 'multiclass',  \n          'random_state': SEED,\n          'device': 'gpu'\n          }\nclf_lgbm= LGBMClassifier(**lgbm_params)\nclf_lgbm.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid)],\n            early_stopping_rounds=40,\n            verbose=500);","e741c1f6":"preds_valid = np.array(clf_lgbm.predict(X_valid, ))\nvalid_acc = accuracy_score(y_pred=preds_valid, y_true=y_valid)\nprint('LGBM validation accuracy: {}'.format(valid_acc))","5023f76b":"preds_test = np.array(clf_lgbm.predict(test))\nsubmission['Cover_Type'] = preds_test\nsubmission.head()","03b0579e":"submission.to_csv(\"submission_lgbm.csv\", index=False)","a18d4bcb":"### Load Data","68ad5994":"<!-- clf = TabNetClassifier(\n    n_d=64, \n    n_a=64, \n    n_steps=5,\n    gamma=1.5, \n    n_independent=2, \n    n_shared=2,\n    cat_emb_dim=1,\n    lambda_sparse=1e-4,\n    momentum=0.3, \n    clip_value=2.,\n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=2e-2),\n    scheduler_params = {\"gamma\": 0.95,\n                     \"step_size\": 20},\n    scheduler_fn=torch.optim.lr_scheduler.StepLR, \n    epsilon=1e-15\n) -->","ac59500c":"## Models\n\n#### XGBoost, Catboost, LGBM \n#### 1. Base model with no feature engineering no hyperparameteres","7be40d47":"---\n# DECEMBER TPS\n\n---\n\n## 1. Introduction\n\nFor this competition, we will be predicting a categorical target based on a number of feature columns given in the data. The data is synthetically generated by a GAN that was trained on a the data from the [Forest Cover Type Prediction](https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction\/overview) (past comp). This dataset is (a) much larger, and (b) may or may not have the same relationship to the target as the original data.\n\nPlease refer to this [data page](https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction\/data) for a detailed explanation of the features.\n\n#### Files\n\n- train.csv - the training data with the target Cover_Type column\n- test.csv - the test set; you will be predicting the Cover_Type for each row in this file (the target integer class)\n- sample_submission.csv - a sample submission file in the correct format\n\n#### Evaluation metrics\n\n* Submissions are evaluated on `multi-class classification accuracy`.\n\n---\n#### About the project\nThe study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. You are asked to predict an integer classification for the forest cover type. The seven types are:\n\n- 1 - Spruce\/Fir\n- 2 - Lodgepole Pine\n- 3 - Ponderosa Pine\n- 4 - Cottonwood\/Willow\n- 5 - Aspen\n- 6 - Douglas-fir\n- 7 - Krummholz\n\nThe training set (15120 observations) contains both features and the Cover_Type. The test set contains only the features. You must predict the Cover_Type for every row in the test set (565892 observations).\n\n#### Data Fields\n- Elevation - Elevation in meters\n- Aspect - Aspect in degrees azimuth\n- Slope - Slope in degrees\n- Horizontal_Distance_To_Hydrology - Horz Dist to nearest surface water features\n- Vertical_Distance_To_Hydrology - Vert Dist to nearest surface water features\n- Horizontal_Distance_To_Roadways - Horz Dist to nearest roadway\n- Hillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice\n- Hillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice\n- Hillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice\n- Horizontal_Distance_To_Fire_Points - Horz Dist to nearest wildfire ignition points\n- Wilderness_Area (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation\n- Soil_Type (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation\n- Cover_Type (7 types, integers 1 to 7) - Forest Cover Type designation\n\n#### The wilderness areas are:\n\n- 1 - Rawah Wilderness Area\n- 2 - Neota Wilderness Area\n- 3 - Comanche Peak Wilderness Area\n- 4 - Cache la Poudre Wilderness Area\n\n#### The soil types are:\n\n1 Cathedral family - Rock outcrop complex, extremely stony.\n2 Vanet - Ratake families complex, very stony.\n3 Haploborolis - Rock outcrop complex, rubbly.\n4 Ratake family - Rock outcrop complex, rubbly.\n5 Vanet family - Rock outcrop complex complex, rubbly.\n6 Vanet - Wetmore families - Rock outcrop complex, stony.\n7 Gothic family.\n8 Supervisor - Limber families complex.\n9 Troutville family, very stony.\n10 Bullwark - Catamount families - Rock outcrop complex, rubbly.\n11 Bullwark - Catamount families - Rock land complex, rubbly.\n12 Legault family - Rock land complex, stony.\n13 Catamount family - Rock land - Bullwark family complex, rubbly.\n14 Pachic Argiborolis - Aquolis complex.\n15 unspecified in the USFS Soil and ELU Survey.\n16 Cryaquolis - Cryoborolis complex.\n17 Gateview family - Cryaquolis complex.\n18 Rogert family, very stony.\n19 Typic Cryaquolis - Borohemists complex.\n20 Typic Cryaquepts - Typic Cryaquolls complex.\n21 Typic Cryaquolls - Leighcan family, till substratum complex.\n22 Leighcan family, till substratum, extremely bouldery.\n23 Leighcan family, till substratum - Typic Cryaquolls complex.\n24 Leighcan family, extremely stony.\n25 Leighcan family, warm, extremely stony.\n26 Granile - Catamount families complex, very stony.\n27 Leighcan family, warm - Rock outcrop complex, extremely stony.\n28 Leighcan family - Rock outcrop complex, extremely stony.\n29 Como - Legault families complex, extremely stony.\n30 Como family - Rock land - Legault family complex, extremely stony.\n31 Leighcan - Catamount families complex, extremely stony.\n32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.\n33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.\n34 Cryorthents - Rock land complex, extremely stony.\n35 Cryumbrepts - Rock outcrop - Cryaquepts complex.\n36 Bross family - Rock land - Cryumbrepts complex, extremely stony.\n37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.\n38 Leighcan - Moran families - Cryaquolls complex, extremely stony.\n39 Moran family - Cryorthents - Leighcan family complex, extremely stony.\n40 Moran family - Cryorthents - Rock land complex, extremely stony.\n\n---","b743cba3":"#### Summary base models \n\n- xgboost : 0.958579\n- catboost: 0.959965\n- lgbm: 0.943972\n\n> Catboost gives the `best` score\n","eeccf0b9":"#### 2. Catboost","86979646":"<!-- max_epochs=15\n\nclf.fit(\n    X_train=X_train, y_train=y_train,\n    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n    eval_name=['train', 'valid'],\n    max_epochs=max_epochs, \n    patience=100,\n    batch_size=20480, \n    virtual_batch_size=256\n) -->","d94eef87":"#### Pairplots of the numerical features","b475f787":"### Features Data Visualization\n- The numerical features distribution seems to be similar in both train and test datasets\n- Most of the categorical features are zero's\n- Features `Wilderness_Area1`, `Wilderness_Area3`, `Soil_Type7`, `Soil_Type15` are different from the others.\n - In features `Wilderness_Area1` and `Wilderness_Area3` we see *not insignificant* cat-1 presence.\n - In features `Soil_Type7` and `Soil_Type15` we see n0 cat-1 at all. They are all zeros.\n > These could be important features for the modeling!","0f9d8d93":"### Are `Cover_Type4` & `Cover_Type5` really zeros?","4200abf5":"#### 1. XGBoost","d27369ee":"#### 3. LGBM","ff6827eb":"### Features ","eaa9036a":"<!-- X = StandardScaler().fit_transform(train)\nY = y.astype(dtype=int) \nX_test = StandardScaler().fit_transform(test) -->","7bad8eec":"### Statistical Description (numerical features)\n","bcc073a3":"<!-- X_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.33, random_state=2021, shuffle=True) -->","1736d199":"## 2. EDA and Data Visualizations\n### Imports","332645a9":"### Correlation Heatmap \n- Notice the correlation coeff. of the features we have identified above as 'different' (`Wilderness_Area1`, `Wilderness_Area3`, `Soil_Type7` and `Soil_Type15`)","d8a89f2b":"<!-- preds = clf.predict(X_test)\nsubmission['Cover_Type'] = preds\nsubmission.head()\nsubmission.to_csv(\"submission_tabnet.csv\", index=False) -->","419e51c1":"<!-- #### 4. TabNet\n\n[I used this reference to build the TabNet model](https:\/\/github.com\/dreamquark-ai\/tabnet\/blob\/develop\/forest_example.ipynb) (most of the params are also from the same ref)\n\n!pip install pytorch_tabnet -q\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nimport torch -->","c896da1e":"### Dataset Overview\n\n#### Data size\n- Train data has 4,000,000 rows and 55 features including the target variable\n- Test dataset has 1,000,000 rows and 54 features.\n\n#### Missing Values\n- No missing values in both train and test datasets!\n\n#### Features\n - 10 features area numerical features.\n - The rest (44) are categorical features.\n \n#### Target\n- Multiclass target (1 to 7)\n- Target distribution is NOT balanced. \n- Target 1 and 2 are by far the dominating classes. Class-2 being the most.\n- Note also that  Cover_Type4 and  Cover_Type5 are almost non-existent (377 instances of Cover_Type4 and ONLY 1 for Cover_Type 5 out of 4milion rows!).","2f2e362b":"### Target Distribution"}}