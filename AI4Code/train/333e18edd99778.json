{"cell_type":{"3094516c":"code","7cbf2ea0":"code","b1ff9b07":"code","eb4f8f5a":"code","8dc7604d":"code","493d22a0":"code","9680c0bc":"code","83563936":"code","8de29b96":"code","dabe10c4":"code","d10897ca":"code","15cd7257":"code","945e62cd":"code","cfb2ad24":"code","e3eee4b9":"code","cd202dbf":"code","4c1e7c24":"code","ac2a2820":"code","4530a6ad":"code","be9fa7b8":"code","7c499c1c":"code","afc2aed1":"code","4f55f3d2":"code","3edad3e2":"markdown"},"source":{"3094516c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7cbf2ea0":"#Databricks works on spark engine. Installing pyspark in your notebook\n!pip install pyspark","b1ff9b07":"import os\nimport pandas as pd\nimport numpy as np\n\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext","eb4f8f5a":"#Pandas reading file\n#https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.read_csv.html#pandas.read_csv\ndf = pd.read_csv(\"\/kaggle\/input\/onlineretail\/OnlineRetail.csv\",sep=',',encoding = 'unicode_escape')","8dc7604d":"inv_df = df[df.InvoiceNo =='536365']","493d22a0":"inv_df","9680c0bc":"#Pandas to_csv\n#https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.to_csv.html\ncompression_opts = dict(method='zip',archive_name='clean_out.csv')  \ninv_df.to_csv('\/kaggle\/working\/inv_out.zip', index=False,compression=compression_opts)","83563936":"#Start spark session\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Python Spark SQL basic example\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n","8de29b96":"#Pyspark read csv file\nsrk_df = spark.read.csv(\"\/kaggle\/input\/onlineretail\/OnlineRetail.csv\",sep=',',header = True)","dabe10c4":"srk_df.show()","d10897ca":"sprk_fil_df = srk_df.filter(srk_df.InvoiceNo=='536365')","15cd7257":"sprk_fil_df.show()","945e62cd":"#Spark to Csv file\n#sprk_fil_df.write.csv('\/kaggle\/working\/spark_write.csv',sep = ',',header=True)\nsprk_fil_df.write.mode('overwrite').option('header', 'true').csv('\/kaggle\/working\/spark_write_2.csv')","cfb2ad24":"#spark read file\nsprk_7_df = spark.read.csv(\"..\/input\/demo-spark\/part-00000-0e499c3f-9c1b-4b9b-bbe5-7d063617a0e4-c000.csv\",sep=',',header = True)","e3eee4b9":"sprk_7_df.show()","cd202dbf":"#Cleaning file \n#https:\/\/spark.apache.org\/docs\/2.2.0\/api\/R\/regexp_replace.html\nclean_df = sprk_7_df.withColumn('StockCode', regexp_replace('StockCode', \"'\", '')) \\\n                 .withColumn('Description', regexp_replace('Description', \"@\", ''))\\\n                 .withColumn('Country', regexp_replace('Country', ' ', ''))","4c1e7c24":"clean_df.show()","ac2a2820":"clean_df = clean_df.withColumn(col, func.ltrim(func.rtrim(clean_df[col])))","4530a6ad":"clean_df.show()","be9fa7b8":"subtr_df= clean_df.withColumn('InvoiceDate', substring('InvoiceDate', 1, 9))","7c499c1c":"subtr_df.show()","afc2aed1":"new_df = subtr_df.withColumn('StockCode', regexp_replace('StockCode', \"\\D\", ''))","4f55f3d2":"new_df.show()","3edad3e2":"**Databricks Topics for Friday 7\/24**\n1. Loading Data from DataFrame to csv file as output.\n2. Cleaning files using lambda functions (LTRIM, SUBSTR, INSTR etc.,)\n3. Working with JSON files\n4. Regular expressions"}}