{"cell_type":{"200c63ab":"code","28736064":"code","4278fe79":"code","28fc3453":"code","6c33bb24":"code","83e252e0":"code","72c4b0b1":"code","a23df113":"code","731078cf":"code","11307522":"code","31e8d3ea":"code","3c199f8f":"code","78782426":"code","f7e66019":"code","15764b89":"code","8f5234d8":"code","d86677a8":"code","452072e2":"code","ae4f615b":"code","be2bf22c":"code","248bab6c":"code","7f246594":"code","39dcf0d2":"code","3f69bd60":"code","d009ef88":"code","32d8b15f":"code","7bb6eb1d":"code","466632cc":"code","7c1da8f6":"code","a4a63213":"code","d724e7e0":"code","f729d88d":"code","b1c2af44":"code","9fd1caff":"code","d8aa7cf0":"code","3297da21":"code","bd8b72a5":"code","f6dd37d9":"code","cc61c4d6":"code","5a1ab8f0":"code","d1869234":"code","9a2fb373":"code","b4ec3bb4":"code","23591ace":"code","6a0c77ad":"code","9f9a7eb0":"code","7a00979b":"code","036344cf":"code","fa5ed51f":"code","1c0cbdae":"code","c23bc1c0":"code","2daf0525":"code","b1a8cda5":"code","170b3e5a":"code","1206b97f":"code","de4d82ac":"code","6d93cb25":"markdown","bbfe3e5d":"markdown","7e902c10":"markdown","678e43cb":"markdown","5216ba58":"markdown","04e11f49":"markdown","39ea74ed":"markdown","f3f7f11f":"markdown","0967053e":"markdown","e13a0278":"markdown","a37efd99":"markdown","6d927dc2":"markdown","d9a6457c":"markdown","4a09b048":"markdown","6ae11a47":"markdown","1e2ce1dc":"markdown","925e260e":"markdown","eebcbaab":"markdown","8a46ca8a":"markdown","51029e43":"markdown","00ef7faa":"markdown","790b1127":"markdown","d7c63367":"markdown","6696db19":"markdown","f1069036":"markdown","b7f73b31":"markdown","09e305cf":"markdown"},"source":{"200c63ab":"import numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\nimport glob \nimport json \n\nimport matplotlib.pyplot as plt \nfrom matplotlib.pyplot import figure \nimport seaborn as sns \n\nimport sys \nif not sys.warnoptions: \n    import warnings \n    warnings.simplefilter(\"ignore\")\n\nplt.style.use('ggplot')","28736064":"# Display setting to show more characters in column\npd.options.display.max_colwidth = 100\n\n# The path\npath = '\/kaggle\/input\/CORD-19-research-challenge\/'","4278fe79":"metadata_df = pd.read_csv(f'{path}metadata.csv',\n                          parse_dates=['publish_time'],\n                          dtype={'pubmed_id':str, 'Microsoft Academic Paper ID':str},\n                          low_memory=False)\n\nmetadata_df.info()","28fc3453":"metadata_df = metadata_df[['sha', 'title', 'authors', 'journal', 'publish_time', 'abstract']]\nmetadata_df = metadata_df[metadata_df.sha.isna()==False]\nmetadata_df.rename(columns={'sha':'paper_id'}, inplace=True)\n","6c33bb24":"# If the number of authors is greater than two, we keep the first one and replace the other by \"et al.\" \nauthors = []\nfor author in metadata_df.authors.astype(str):\n    if len(author.split('; ')) > 2:\n        authors.append(author.split('; ')[0]+' et al.')\n    else:\n        authors.append(author)\n\nmetadata_df['authors'] = authors\nmetadata_df['abstract'] = [str(a).replace('Abstract ','') for a in metadata_df.abstract]\n\nmetadata_df.head()","83e252e0":"# Path for reading all the JSON files\npdf_json = glob.glob(path+'**\/pdf_json\/*.json', recursive=True)\npmc_json = glob.glob(path+'**\/pmc_json\/*.json', recursive=True)\n\nprint(f'pdf_json contains {len(pdf_json)} JSON files')\nprint(f'pmc_json contains {len(pmc_json)} JSON files')","72c4b0b1":"with open(pdf_json[0], mode='r') as file:\n     contents = json.loads(file.read())\n        \ncontents.keys()","a23df113":"%%time\npapers_df = pd.DataFrame(columns=['paper_id','body_text'])\n\nfor j in range(len(pdf_json)):\n    with open(pdf_json[j], mode='r') as file:\n        contents = json.loads(file.read())\n\n    papers_df.loc[j,'paper_id'] = contents['paper_id']\n\n    texts = []\n    for text in contents['body_text']:\n        texts.append(text['text'])\n    body_text = '\\n '.join(texts)\n    papers_df.loc[j,'body_text'] = body_text\n","731078cf":"%%time\ncord19_df = pd.merge(metadata_df, papers_df, on=['paper_id'], how='inner')\ncord19_df.drop_duplicates(['paper_id', 'body_text'], inplace=True)\n\ncord19_df.info()","11307522":"cord19_df.head()","31e8d3ea":"cord19_df.isnull().sum()","3c199f8f":"nlp = spacy.load(\"en_core_web_sm\")\nnlp = en_core_sci_sm.load()\nnlp.max_length = 1_000_000\nnlp.add_pipe(LanguageDetector(), name='language_detector', last=True)","78782426":"%%time\ncord19_df['paper_language'] = cord19_df.abstract.apply(lambda x: nlp(str(x[:1000]))._.language['language'])\n\ncord19_df.paper_language.value_counts()","f7e66019":"# Keep only the documents with english language \ncord19_df = cord19_df[cord19_df.paper_language=='en'].drop(['paper_language'], axis=1)\ncord19_df.isnull().sum()","15764b89":"print(f'The CORD19 dataset contains {cord19_df.shape[0]} papers written in English')","8f5234d8":"import numpy as np\nimport pandas as pd \nfrom pathlib import Path, PurePath\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nimport string\nimport torch\n\n# Search engine\nfrom rank_bm25 import BM25Okapi \n\n#Display setting to show more characters in column\npd.options.display.max_colwidth = 100 ","d86677a8":"# Stop words and extension words\nenglish_stopwords = stopwords.words('english') \nenglish_stopwords.extend(['_url_','_mention_','_hashtag_','figure','unmanned',\n                          'also','use','say','subject','edu','would','say','know',\n                          'good','go','get','done','try','many','nice','thank','think',\n                          'see','rather','easy','easily','lot','lack','make','want','seem',\n                          'run','need','even','right','line','even','also','may','take','come',\n                          'year','time','hour','first','last','second','high','new','low'])\n\n# Replace contractions with their longer forms\ncontraction_mapping = {\"u.s.\":\"america\", \"u.s\":\"america\", \"usa\":\"america\", \"u.k.\":\"england\", \"u.k\":\"england\", \"e-mail\":\"email\",\n                       \"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\"he'd\": \"he would\",\"he'll\": \"he will\", \n                       \"he's\": \"he is\", \n                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \n                       \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n                       \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\",\n                       \"i've\": \"i have\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n                       \"it'll've\": \"it will have\",\"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n                       \"might've\": \"might have\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n                       \"mustn't've\": \"must not have\", \"needn't've\": \"need not have\",\n                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n                       \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n                       \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n                       \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n                       \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"we'd\":\"we would\", \n                       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\":\"we are\", \"we've\":\"we have\",\n                       \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n                       \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n                       \"where's\": \"where is\", \"where've\":\"where have\", \"who'll\":\"who will\", \"who'll've\":\"who will have\", \"who's\":\"who is\", \n                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n                       \"would've\": \"would have\", \"wouldn't've\":\"would not have\", \n                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n                       \"y'all've\": \"you all have\",\"you'd've\": \"you would have\"}\n\n# URL, MENTION, HASHTAG \ngiant_url_regex= 'http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\nmention_regex  = '@[\\w\\-]+'\nhashtag_regex  = '#[\\w\\-]+'\nspace_pattern  = '\\s+'","452072e2":"class CovidSearchEngine:\n    \"\"\"\n    Simple CovidSearchEngine.\n    \"\"\"\n    \n    def text_process(self, text):\n        \n        # Deal with URL, MENTION, HASHTAG\n        text = re.sub(space_pattern, ' ', text)\n        text = re.sub(giant_url_regex, '_URL_', text)\n        text = re.sub(mention_regex, '_MENTION_', text)\n        text = re.sub(hashtag_regex, '_HASHTAG_', text)\n        \n        # Special_character\n        text = re.sub(r\"\\\/\",\" \", text)\n        text = re.sub(r\"\\^\",\" ^ \", text)\n        text = re.sub(r\"\\+\",\" + \", text)\n        text = re.sub(r\"\\-\",\" - \", text)\n        text = re.sub(r\"\\=\",\" = \", text)\n        text = re.sub(r\"\\\/\",\" \", text)\n        text = re.sub(r\"\\^\",\" ^ \", text)\n        text = re.sub(r\"\\+\",\" + \", text)\n        text = re.sub(r\"\\-\",\" - \", text)\n        text = re.sub(r\"\\=\",\" = \", text)\n        \n        # contraction and punctuation\n        text = text.lower()\n        text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in nltk.word_tokenize(text)])\n        text = text.replace(' .', '.').replace('( ', '(').replace(' )', ')')\n        text = text.translate(str.maketrans('', '', string.punctuation))\n        return text\n\n    def text_tokenize(self, text):\n        \n        # tokenize text\n        words = nltk.word_tokenize(text)\n        return list(set([word for word in words \n                         if len(word) > 2\n                         and not word in english_stopwords\n                         and not word.isnumeric() # if word.isalpha()\n                        ]))\n    \n    def preprocess(self, text):\n        # Clean and tokenize text input\n        return self.text_tokenize(self.text_process(text.lower()))\n\n    def __init__(self, corpus: pd.DataFrame):\n        self.corpus = corpus\n        self.columns = corpus.columns\n        \n        raw_search_str = self.corpus.abstract.fillna('') + ' ' \\\n                            + self.corpus.title.fillna('')\n        \n        self.index = raw_search_str.apply(self.preprocess).to_frame()\n        self.index.columns = ['terms']\n        self.index.index = self.corpus.index\n        self.bm25 = BM25Okapi(self.index.terms.tolist())\n    \n    def search(self, query, num):\n        \"\"\"\n        Return top `num` results that better match the query\n        \"\"\"\n        # obtain scores\n        search_terms = self.preprocess(query) \n        doc_scores = self.bm25.get_scores(search_terms)\n        \n        # sort by scores\n        ind = np.argsort(doc_scores)[::-1][:num] \n        \n        # select top results and returns\n        results = self.corpus.iloc[ind][self.columns]\n        results['score'] = doc_scores[ind]\n        results = results[results.score > 0]\n        return results.reset_index()","ae4f615b":"%%time\ncse = CovidSearchEngine(cord19_df)","be2bf22c":"%%time\n\nimport torch\nfrom transformers import BertTokenizer\nfrom transformers import BertForQuestionAnswering\n\n# Use GPU for computation, if available \ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Use the pre-trained model to get answer to a question \nBERT_SQUAD = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n\nmodel = BertForQuestionAnswering.from_pretrained(BERT_SQUAD, proxies=proxies)\ntokenizer = BertTokenizer.from_pretrained(BERT_SQUAD)\n\nmodel = model.to(torch_device)\nmodel.eval()\n\nprint()","248bab6c":"!pip install rank_bm25\n\nimport numpy as np\nimport pandas as pd \nfrom pathlib import Path, PurePath\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nimport string\nimport torch\n\nfrom rank_bm25 import BM25Okapi # Search engine","7f246594":"!pip install rank-bm25","39dcf0d2":"def answer_question(question, context):\n    \n    # anser question given question and context\n    encoded_dict = tokenizer.encode_plus(\n                        question, context,\n                        add_special_tokens = True,\n                        max_length = 256,\n                        pad_to_max_length = True,\n                        return_tensors = 'pt')\n    \n    input_ids = encoded_dict['input_ids'].to(torch_device)\n    token_type_ids = encoded_dict['token_type_ids'].to(torch_device)\n    \n    start_scores, end_scores = model(input_ids, token_type_ids=token_type_ids)\n\n    all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n    start_index = torch.argmax(start_scores)\n    end_index = torch.argmax(end_scores)\n    \n    answer = tokenizer.convert_tokens_to_string(all_tokens[start_index:end_index+1])\n    answer = answer.replace('[CLS]', '')\n    return answer","3f69bd60":"cord19_kaggle_questions = {\n    \"data\":[\n        {\n            \"task\": \"What is known about transmission, incubation, and environmental stability?\",\n            \"questions\": [\n                \"Is the virus transmitted by aerisol, droplets, food, close contact, fecal matter, or water?\",\n                \"How long is the incubation period for the virus?\",\n                \"Can the virus be transmitted asymptomatically or during the incubation period?\",\n                \"How does weather, heat, and humidity affect the tramsmission of 2019-nCoV?\",\n                \"How long can the 2019-nCoV virus remain viable on common surfaces?\"\n            ]\n        },\n        {\n            \"task\": \"What do we know about COVID-19 risk factors?\",\n            \"questions\": [\n                \"What risk factors contribute to the severity of 2019-nCoV?\",\n                \"How does hypertension affect patients?\",\n                \"How does heart disease affect patients?\",\n                \"How does copd affect patients?\",\n                \"How does smoking affect patients?\",\n                \"How does pregnancy affect patients?\",\n                \"What is the fatality rate of 2019-nCoV?\",\n                \"What public health policies prevent or control the spread of 2019-nCoV?\"\n            ]\n        },\n        {\n            \"task\": \"What do we know about virus genetics, origin, and evolution?\",\n            \"questions\": [\n                \"Can animals transmit 2019-nCoV?\",\n                \"What animal did 2019-nCoV come from?\",\n                \"What real-time genomic tracking tools exist?\",\n                \"What geographic variations are there in the genome of 2019-nCoV?\",\n                \"What effors are being done in asia to prevent further outbreaks?\"\n            ]\n        },\n        {\n            \"task\": \"What do we know about vaccines and therapeutics?\",\n            \"questions\": [\n                \"What drugs or therapies are being investigated?\",\n                \"Are anti-inflammatory drugs recommended?\"\n            ]\n        },\n        {\n            \"task\": \"What do we know about non-pharmaceutical interventions?\",\n            \"questions\": [\n                \"Which non-pharmaceutical interventions limit tramsission?\",\n                \"What are most important barriers to compliance?\"\n            ]\n        },\n        {\n            \"task\": \"What has been published about medical care?\",\n            \"questions\": [\n                \"How does extracorporeal membrane oxygenation affect 2019-nCoV patients?\",\n                \"What telemedicine and cybercare methods are most effective?\",\n                \"How is artificial intelligence being used in real time health delivery?\",\n                \"What adjunctive or supportive methods can help patients?\"\n            ]\n        },\n        {\n            \"task\": \"What do we know about diagnostics and surveillance?\",\n            \"questions\": [\n                \"What diagnostic tests (tools) exist or are being developed to detect 2019-nCoV?\"\n            ]\n        },\n        {\n            \"task\": \"Other interesting questions\",\n            \"questions\": [\n                \"What is the immune system response to 2019-nCoV?\",\n                \"Can personal protective equipment prevent the transmission of 2019-nCoV?\",\n                \"Can 2019-nCoV infect patients a second time?\"\n            ]\n        }\n    ]\n}\n","d009ef88":"NUM_CONTEXT_FOR_EACH_QUESTION = 5\n\n\ndef get_all_context(query, num_results):\n    \"\"\"\n    Return ^num_results' papers that better match the query\n    \"\"\"\n    papers_df = cse.search(query, num_results)\n    return papers_df['abstract'].tolist()\n\ndef get_all_answers(question, all_contexts):\n    \"\"\"\n    Ask the same question to all contexts (all papers)\n    \"\"\"\n    all_answers = []\n    \n    for context in all_contexts:\n        all_answers.append(answer_question(question, context))\n    return all_answers\n\ndef create_output_results(question, \n                          all_contexts, \n                          all_answers, \n                          summary_answer='', \n                          summary_context=''):\n    \"\"\"\n    Return results in json format\n    \"\"\"\n    \n    def find_start_end_index_substring(context, answer):\n        search_re = re.search(re.escape(answer.lower()), context.lower())\n        if search_re:\n            return search_re.start(), search_re.end()\n        else:\n            return 0, len(context)\n        \n    output = {}\n    output['question'] = question\n    output['summary_answer'] = summary_answer\n    output['summary_context'] = summary_context\n    \n    results = []\n    for c, a in zip(all_contexts, all_answers):\n        span = {}\n        span['context'] = c\n        span['answer'] = a\n        span['start_index'], span['end_index'] = find_start_end_index_substring(c,a)\n        \n        results.append(span)\n    \n    output['results'] = results\n    \n    return output\n    \ndef get_results(question, \n                summarize=False, \n                num_results=NUM_CONTEXT_FOR_EACH_QUESTION,\n                verbose=True):\n    \"\"\"\n    Get results\n    \"\"\"\n    all_contexts = get_all_context(question, num_results)\n    all_answers = get_all_answers(question, all_contexts)\n    \n    if summarize:\n        # NotImplementedYet\n        summary_answer = get_summary(all_answers)\n        summary_context = get_summary(all_contexts)\n    \n    return create_output_results(question, \n                                 all_contexts, \n                                 all_answers)","32d8b15f":"%%time\nall_tasks = []\n\nfor i, t in enumerate(cord19_kaggle_questions['data']):\n    print(\"Answering questions to task {}. ...\".format(i+1))\n    answers_to_question = []\n    for q in t['questions']:\n            answers_to_question.append(get_results(q, verbose=False))\n    task = {}\n    task['task'] = t['task']\n    task['questions'] = answers_to_question\n    \n    all_tasks.append(task)\n\nprint(\"Hey, we're okay! All the questions of the tasks are answered.\")\ncord19_answers_json = {} \ncord19_answers_json['data'] = all_tasks","7bb6eb1d":"# Save to JSON format\n#import json\n#with open('..\/data_processed\/cord19_answers.json', 'w') as outfile:\n#    json.dump(cord19_answers_json, outfile)","466632cc":"authors_year = []\nfor j in range(cord19_df.shape[0]):\n    authors_year.append(str(cord19_df.authors[j])+' ('+str(cord19_df.publish_time[j].year)+')')","7c1da8f6":"j = 0\nresults_df = pd.DataFrame(columns=['task','question','context','answer','reference'])\nfor tasks in cord19_answers_json['data']:\n    for questions in tasks['questions']:\n        for question in questions['results']:\n            results_df.loc[j,'task']    = tasks['task']\n            results_df.loc[j,'question']= questions['question']\n            results_df.loc[j,'context'] = question['context']\n            results_df.loc[j,'answer']  = question['answer']\n            j+=1\n\nfor abstract, ref in zip(cord19_df.abstract, authors_year):\n    for j, context in enumerate(results_df.context):\n        if context in abstract:\n            results_df.loc[j,'reference'] = ref\n\nresults_df.head()","a4a63213":"#results_df.to_csv('..\/data_processed\/cord19_answers.csv', index=False)","d724e7e0":"#import json\n\n#with open('..\/data_processed\/cord19_answers.json', mode='r') as file:\n#     cord19_answers_json = json.loads(file.read())\n        ","f729d88d":"from IPython.display import display, Markdown, Latex, HTML\n\ndef layout_style():\n    style = \"\"\"\n        div {\n            color: black;\n            }\n        .single_answer {\n            border-left: 3px solid green;\n            padding-left: 10px;\n            font-family: Arial;\n            font-size: 16px;\n            color: #777777;\n            margin-left: 5px;\n            }\n        .answer{\n            color: #dc7b15;\n            }\n        .ay{\n            color: red;\n            }\n        .question_title {\n            color: darkblue;\n            display: block;\n            text-transform: none;\n            }\n        .task_title {\n            color: darkgreen;\n            }\n        div.output_scroll {\n            height: auto;\n            }\n    \"\"\"\n    return \"<style>\" + style + \"<\/style>\"\n\ndef dm(x): display(Markdown(x))\ndef dh(x): display(HTML(layout_style() + x))","b1c2af44":"def display_single_context(context, start_index, end_index):\n    \n    before_answer = context[:start_index]\n    answer = context[start_index:end_index]\n    after_answer = context[end_index:]\n    \n    content = before_answer + \"<span class='answer'>\" + answer + \"<\/span>\" + after_answer\n    \n    return dh(\"\"\"<div class=\"single_answer\">{}<\/div>\"\"\".format(content))\n\ndef display_question_title(question):\n    return dh(\"<h2 class='question_title'>{}<\/h2>\".format(question)) #.capitalize()\n\ndef display_all_contexts(index, question):\n    \n    def answer_not_found(context, start_index, end_index):\n        return (start_index == 0 and len(context) == end_index) or (start_index == 0 and end_index == 0)\n\n    display_question_title(str(index + 1) + \". \" + question['question'].capitalize())\n    \n    # display context\n    for i in question['results']:\n        for a, ay in zip(cord19_df.abstract, authors_year):\n            if i['context'] in a:\n                \n                if answer_not_found(i['context'], i['start_index'], i['end_index']):\n                    continue # skip not found questions\n                display_single_context(i['context']+'<br>'+'<strong>'+'<font color=black>'+ay,\n                                       i['start_index'], i['end_index'])\n\ndef display_task_title(index, task):\n    task_title = \"Task \" + str(index) + \": \" + task\n    return dh(\"<h1 class='task_title'>{}<\/h1>\".format(task_title))\n\ndef display_single_task(index, task):\n    \n    display_task_title(index, task['task'])\n    \n    for i, question in enumerate(task['questions']):\n        display_all_contexts(i, question)","9fd1caff":"# Show the reponse of the given task\ntask = int(input('Enter a task number'))\n\ndisplay_single_task(task, cord19_answers_json['data'][task-1])","d8aa7cf0":"task = int(input('Enter a task number'))\n\ndisplay_single_task(task, cord19_answers_json['data'][task-1])","3297da21":"import numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\nimport glob\nimport re\nimport sys\n#!{sys.executable} -m spacy download en\n\n# Gensim\nimport gensim, spacy, logging, warnings \nimport gensim.corpora as corpora \nfrom gensim.utils import lemmatize, simple_preprocess \nfrom gensim.models import CoherenceModel \n\n# NLTK \nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import sent_tokenize, word_tokenize \n\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)","bd8b72a5":"def process_to_words(papers):\n    \n    for texts in papers:\n        # Deal with URL, MENTION, HASHTAG\n        texts = re.sub(space_pattern, ' ', texts)\n        texts = re.sub(giant_url_regex, '_URL_', texts)\n        texts = re.sub(mention_regex, '_MENTION_', texts)\n        texts = re.sub(hashtag_regex, '_HASHTAG_', texts)\n\n        # Special_character\n        texts = re.sub(r\"\\\/\",\" \", texts)\n        texts = re.sub(r\"\\^\",\" ^ \", texts)\n        texts = re.sub(r\"\\+\",\" + \", texts)\n        texts = re.sub(r\"\\-\",\" - \", texts)\n        texts = re.sub(r\"\\=\",\" = \", texts)\n        texts = re.sub(r\"\\\/\",\" \", texts)\n        texts = re.sub(r\"\\^\",\" ^ \", texts)\n        texts = re.sub(r\"\\+\",\" + \", texts)\n        texts = re.sub(r\"\\-\",\" - \", texts)\n        texts = re.sub(r\"\\=\",\" = \", texts)\n\n        # contraction and punctuation\n        texts = texts.lower()\n        texts = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in nltk.word_tokenize(texts)])\n        texts = texts.replace(' .', '.').replace('( ', '(').replace(' )', ')')\n        texts = texts.translate(str.maketrans('', '', string.punctuation))\n        \n        texts = gensim.utils.simple_preprocess(str(texts), deacc=True)\n        yield(texts)","f6dd37d9":"%%time \n# Convert to list\nwords_list = list(process_to_words(cord19_df.abstract))","cc61c4d6":"%%time\n# Build the bigram and trigram models\nbigram  = gensim.models.Phrases(words_list, min_count=5, threshold=50) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[words_list], threshold=70)  \n\n# Faster way to get a sentence clubbed as a trigram\/bigram\nbigram_mod  = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)","5a1ab8f0":"def tokenize_Ngram(texts,\n                  stop_words=english_stopwords,\n                  allowed_postags=['NOUN','ADJ','VERB','ADV']):\n    \"\"\"\n    Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\n    \"\"\"\n    \n    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words \n              and len(word)>2 and word.isalpha()] for doc in texts]\n    texts = [bigram_mod[doc] for doc in texts] \n    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n    \n    nlp = spacy.load('en', disable=['parser', 'ner'])\n    \n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n        \n    # remove stopwords once more after lemmatization\n    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words and len(word)>2] for doc in texts_out]    \n    return texts_out","d1869234":"%%time\ndata_ready = tokenize_Ngram(words_list)","9a2fb373":"print(trigram_mod[bigram_mod[data_ready[0][:100]]])","b4ec3bb4":"%%time\n# Create Dictionary\nid2word = corpora.Dictionary(data_ready)\n\n# Create Corpus: Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in data_ready]\n\n# Build LDA model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, \n                                            alpha='symmetric', num_topics=4, \n                                            random_state=42, chunksize=100, \n                                            passes=5, iterations=1000, \n                                            per_word_topics=True)","23591ace":"coherence_model = CoherenceModel(model=lda_model, texts=data_ready, dictionary=id2word, coherence='c_v').get_coherence()\ncoherence_model","6a0c77ad":"from matplotlib import pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.colors as mcolors\n\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n\ncloud = WordCloud(stopwords=english_stopwords,\n                  background_color='white',\n                  width=2500,\n                  height=1800,\n                  max_words=10,\n                  colormap='tab10',\n                  color_func=lambda *args, **kwargs: cols[i],\n                  prefer_horizontal=1.0)\n\ntopics = lda_model.show_topics(formatted=False)\n\nfig, axes = plt.subplots(2,2, figsize=(17,13), sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    fig.add_subplot(ax)\n    topic_words = dict(topics[i][1])\n    cloud.generate_from_frequencies(topic_words, max_font_size=400)\n    plt.gca().imshow(cloud)\n    plt.gca().set_title('Topic_' + str(i+1), fontdict=dict(size=36))\n    plt.gca().axis('off')\n\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.tight_layout()\nplt.show()\n","9f9a7eb0":"from collections import Counter\ntopics = lda_model.show_topics(formatted=False)\ndata_flat = [w for w_list in data_ready for w in w_list]\ncounter = Counter(data_flat)\n\nout = []\nfor i, topic in topics:\n    for word, weight in topic:\n        out.append([word, i , weight, counter[word]])\n\ndf = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])\ndf.sort_values('importance', ascending=False, inplace=True)\nprint(df.shape)\ndf.head()","7a00979b":"# Plot Word Count and Weights of Topic Keywords \nfig, axes = plt.subplots(2, 2, figsize=(12,7), sharey=True, dpi=120)\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n\nfor i, ax in enumerate(axes.flatten()):\n    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n    ax_twin = ax.twinx()\n    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n    ax.set_ylabel('Word Count', color=cols[i])\n    #ax_twin.set_ylim(0, 0.040); ax.set_ylim(0, 40000)\n    ax.set_title('Topic_' + str(i), color=cols[i], fontsize=16)\n    ax.tick_params(axis='y', left=False)\n    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n    ax.legend(loc='upper center'); ax_twin.legend(loc='upper right')\n\nfig.tight_layout(w_pad=2)\nfig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \nplt.show()","036344cf":"doc_lda = lda_model[corpus]\ndef format_topics_sentences(ldamodel=None, corpus=corpus, texts=list(cord19_df.abstract.values)):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row_list in enumerate(ldamodel[corpus]):\n        row = row_list[0] if ldamodel.per_word_topics else row_list \n        row = sorted(row, key=lambda x: (x[1]), reverse=True) \n        \n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n                \n    sent_topics_df.columns = ['dominant_topic', 'perc_contribution', 'topic_keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    \n    return(sent_topics_df) \n","fa5ed51f":"%%time\n# Display setting to show more characters in column\npd.options.display.max_colwidth = 100\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['doc_id', 'dominant_topic', 'topic_perc_contrib', 'keywords', 'text']\n\ndf_dominant_topic.sort_values(by='topic_perc_contrib',  ascending=False, inplace=True)\ndf_dominant_topic.head(5)","1c0cbdae":"print('The percentage of documents that dominated the topic has less than 80% contribution is: {}%'.format(\n      round(len(df_dominant_topic.loc[df_dominant_topic.topic_perc_contrib<=0.50])\/len(df_dominant_topic)*100,2)))","c23bc1c0":"# Display setting to show more characters in column \npd.options.display.max_colwidth = 100\n\nsent_topics_sorteddf_mallet = pd.DataFrame()\nsent_topics_outdf_grpd = df_topic_sents_keywords.groupby('dominant_topic')\n\nfor i, grp in sent_topics_outdf_grpd:\n    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n                                             grp.sort_values(['perc_contribution'], ascending=False).head(1)], \n                                            axis=0)\n\n# Reset Index \nsent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n\n# Format \nsent_topics_sorteddf_mallet.columns = ['no_topic', \"topic_perc_contrib\", \"keywords\", \"representative text\"]\n\n# Show \nsent_topics_sorteddf_mallet","2daf0525":"import seaborn as sns\nimport matplotlib.colors as mcolors\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n\nfig, axes = plt.subplots(2,2, figsize=(10,7), dpi=120, sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.dominant_topic == i,:]\n    doc_lens = [len(d) for d in df_dominant_topic_sub.text]\n    ax.hist(doc_lens, bins = 120, color=cols[i])\n    ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n    sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n    ax.set(xlim=(0, 300), ylim=(0, max(doc_lens)+300), xlabel='Document Word Count')\n    ax.set_ylabel('Number of Documents', color=cols[i])\n    ax.set_title('Topic: '+str(i), fontdict=dict(size=12, color=cols[i]))\n\nfig.tight_layout()\nfig.subplots_adjust(top=0.90)\nplt.xticks(np.linspace(0,300,9))\nfig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=16)\nplt.show()","b1a8cda5":"## Sentence Coloring of N Sentences","170b3e5a":"from matplotlib.patches import Rectangle\n\ndef sentences_chart(lda_model=lda_model, corpus=corpus, start=0, end=11):\n    \n    corp = corpus[start:end]\n    mycolors = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n\n    fig, axes = plt.subplots(end-start, 1, figsize=(20, (end-start)*0.95), dpi=120)       \n    axes[0].axis('off')\n    \n    for i, ax in enumerate(axes):\n        if i > 0:\n            corp_cur = corp[i-1] \n            topic_percs, wordid_topics, wordid_phivalues = lda_model[corp_cur]\n            word_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics] \n            \n            ax.text(0.01, 0.5, \"Doc \" + str(i-1) + \": \", verticalalignment='center',\n                    fontsize=16, color='black', transform=ax.transAxes, fontweight=700)\n            \n            # Draw Rectange\n            topic_percs_sorted = sorted(topic_percs, key=lambda x: (x[1]), reverse=True)\n            ax.add_patch(Rectangle((0.0, 0.05), 0.99, 0.90, fill=None, alpha=1, \n                                   color=mycolors[topic_percs_sorted[0][0]], linewidth=2))\n            \n            word_pos = 0.06 \n            for j, (word, topics) in enumerate(word_dominanttopic):\n                if j < 13:\n                    ax.text(word_pos, 0.5, word,\n                            horizontalalignment='left',\n                            verticalalignment='center',\n                            fontsize=16, color=mycolors[topics],\n                            transform=ax.transAxes, fontweight=700)\n                    word_pos += .009 * len(word)  # to move the word for the next iter\n                    ax.axis('off')\n                    \n            ax.text(word_pos, 0.5, '. . .',\n                    horizontalalignment='left',\n                    verticalalignment='center',\n                    fontsize=16, color='black',\n                    transform=ax.transAxes)\n    plt.subplots_adjust(wspace=0, hspace=0)\n    plt.suptitle('Sentence Topic Coloring for Documents: '+str(start)+' to '+str(end-2),\n                 fontsize=22, y=0.95, fontweight=700)\n    plt.tight_layout()\n    plt.show()\n\nsentences_chart()","1206b97f":"# Get topic weights and dominant topics ------------\nfrom sklearn.manifold import TSNE\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\n\n# Get topic weights\ntopic_weights = []\nfor i, row_list in enumerate(lda_model[corpus]):\n    topic_weights.append([w for i, w in row_list[0]])\n\n# Array of topic weights    \narr = pd.DataFrame(topic_weights).fillna(0).values\n\n# Keep the well separated points (optional)\narr = arr[np.amax(arr, axis=1) > 0.35]\n\n# Dominant topic number in each doc\ntopic_num = np.argmax(arr, axis=1)\n\n# tSNE Dimension Reduction\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\ntsne_lda = tsne_model.fit_transform(arr)\n\n# Plot the Topic Clusters using Bokeh\noutput_notebook()\nn_topics = 4\nmycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\nplot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n              plot_width=900, plot_height=700)\nplot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\nshow(plot)\n","de4d82ac":"%%time\nimport pyLDAvis.gensim\npyLDAvis.enable_notebook(local=False)\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\nvis ","6d93cb25":"## Answer questions\n### Get the answers","bbfe3e5d":"## Merging the metadata and the body text of the papers","7e902c10":"# Preprocessing\nThe function `process_document` deletes all the deletes all the specific characters by locating internet links and correct the spelling of certain words such as contractions, abbreviations.\n","678e43cb":"## QA model\nThis model is adapted in the following notebook: https:\/\/www.kaggle.com\/jonathanbesomi\/a-qa-model-to-answer-them-all\n","5216ba58":"### Read the processed data","04e11f49":"## Metadata ","39ea74ed":"Next step: invoking the function display_single_task for all eight tasks. ","f3f7f11f":"### Get the contexts","0967053e":"## Extract the body text of the papers from JSON files","e13a0278":"### To CSV format\n#### Answer reference (authors and year of publication)","a37efd99":"### Get the results","6d927dc2":"## Preprocessing and Search Engine","d9a6457c":"# Data Processing\n## Necessary Libraries","4a09b048":"## Download pre-trained QA model  \nWe use a pre-trained question answering model. Here we are installing the dependencies and downloading the models.","6ae11a47":"### Tasks and Questions dictionary","1e2ce1dc":"## CORD-19: Question-Answer (QA) model and Topic modelling  \n@authors  \n     Djiby BALDE [GitHub](https:\/\/github.com\/djibybalde)  \n     Tatiana MAIA [GitHub](https:\/\/github.com\/tatianamaia)  \n     Students at Aix-Marseille School of Economics, Marseille, France\n     \n### AIM\nThe aim of the exercise is to study the literature provided by the Kaggle in the COVID-19 Open Research Dataset (CORD-19) in order to answer a total of 8 taks. \n\nOur aim is to try to built a QA model that can answer all the CORD-19 tasks. \n\nThis notebook is decomposed in 3 parts: \n\n   * 1st: Data Processing\n    \n   * 2nd: QA model: This model is based in the following notebook:  https:\/\/www.kaggle.com\/jonathanbesomi\/a-qa-model-to-answer-them-all\n    \n   * 3rd: Topic modelling \n","925e260e":"We we  **iterate** over all **tasks** and all **questions** and store the results into `all_tasks`:","eebcbaab":"## Language detection \n>Available pretrained statistical models for English: [SpaCy](https:\/\/spacy.io\/models\/en#en_core_web_sm)","8a46ca8a":"### Display the results","51029e43":"This function will return the span of text that better represent the question.\n","00ef7faa":"#### As we can see some results are not that accurate. We are working on having more accurate results. \n#### We are also working on a Dash application in order to be able to select multiple questions at the same time. ","790b1127":"### HTML Layout Style","d7c63367":"## Building the Topic Model with 4 topics","6696db19":"# Creating Bigram and Trigram Models\n\nThe two most important arguments to Phrases are:  \n- `min_count` ignore all words and bigrams with total collected count lower than this value.  \n- `threshold` represent a score threshold for forming the phrases (higher means fewer phrases). A phrase of words `a` followed by `b` is accepted if the score of the phrase is greater than threshold. ","f1069036":"## Visualisation\u00b6","b7f73b31":"# Topic modeling using LDA model\nThe following shema are provided by CHRIS MOODY in his article entitled Presentation of our hybrid lda2with algorithm available [here](https:\/\/multithreaded.stitchfix.com\/blog\/2016\/05\/27\/lda2vec\/#topic=38&lambda=1&term=).\n\n# <center> ![](https:\/\/multithreaded.stitchfix.com\/assets\/posts\/2016-05-27-lda2vec\/anim01.gif)\n# <center> ![](https:\/\/multithreaded.stitchfix.com\/assets\/posts\/2016-05-27-lda2vec\/anim02.gif)","09e305cf":"In this section, we store in a `dict` object a list of `tasks` and their `questions`. In the next parts, we will ask our model to answers them all. "}}