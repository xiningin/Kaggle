{"cell_type":{"5858bd24":"code","7149607f":"code","fa59ba7e":"code","3de84453":"code","31f9f88a":"code","fe365c28":"code","6c77b33a":"code","b1f51927":"code","e0e89a72":"code","9716b700":"code","42283bc7":"code","e8206b85":"code","ace61883":"code","c0265a09":"code","56b1ef75":"code","4ff40229":"code","43f0916f":"code","1f8765bf":"code","c0067b46":"code","e02ff770":"code","3e2c781d":"code","218638de":"code","f8d1192c":"code","d679f18d":"code","e5abfeaa":"code","8dcea833":"code","86b7893e":"code","0c852531":"code","94155af3":"code","00474f1d":"code","776b3a91":"code","efbf3808":"code","eaf0010c":"code","16c5dc10":"code","a7ae45be":"code","3b5c9e23":"code","26c6aff0":"code","0c4fc719":"code","8916b919":"code","10a20965":"code","b30989b3":"code","662a2b2b":"code","b799cae9":"code","2e75b389":"code","a0a694e0":"code","0f830688":"code","68193262":"code","2332aa06":"code","a2307bbc":"code","3e2f9fb3":"code","a0e0dbbc":"code","95e8343c":"code","f117baaf":"code","034bd3a2":"code","8d0ae452":"code","1fe131f6":"code","8f7b0bb2":"code","6ff8d99b":"code","10f166bd":"code","640c628c":"code","8495cf78":"code","a68e1e7c":"code","63720a0f":"code","6bb2be0d":"code","7c9c5900":"code","d0ba5a56":"code","5424fafc":"code","e53c4028":"code","6185d1f0":"code","1bbdc8d4":"code","29851163":"code","5dce7d86":"code","607c772f":"code","8330ec7c":"code","bfb17b4a":"code","265340f0":"code","cdda8cb0":"code","2fb7c588":"code","4a501be9":"code","cdb8002f":"code","10e33cb7":"code","b8fbe3a5":"code","0169675e":"code","6bd198c0":"code","8694f8b4":"code","2b311e84":"code","5f5bbba1":"code","c0ca237b":"code","d0a04344":"code","73aae572":"code","8ddf79fb":"code","4ae4e6bb":"code","cfff6228":"markdown","22512770":"markdown","611f0f87":"markdown","9d79d841":"markdown","62a563b2":"markdown","2fc5ab71":"markdown","12612721":"markdown","bbbe144c":"markdown","f7800985":"markdown","8c4554c0":"markdown","b8a4f6ba":"markdown","71a08bba":"markdown","13784039":"markdown","ba29ac13":"markdown","e7d235c0":"markdown","a67af97a":"markdown","affdac45":"markdown","842314f8":"markdown","fcfb1ec0":"markdown","113d9552":"markdown","9d6cb273":"markdown","bcf1c686":"markdown","a8f0960f":"markdown","3ca58107":"markdown","70e05902":"markdown","083441bf":"markdown","1e68f913":"markdown","1e0221d6":"markdown","b20b289b":"markdown","d6f4872e":"markdown","f7af0d95":"markdown","1d0c00e3":"markdown","258b7d2e":"markdown","e57f78e0":"markdown","a8a9c689":"markdown","d0fb8d4b":"markdown","7c152cd3":"markdown","9f640365":"markdown","cf1e8430":"markdown","6d75531f":"markdown","1eeee37a":"markdown","ba7dc92b":"markdown","66bc37ae":"markdown","2b11cafe":"markdown","522b19fa":"markdown","89a1d647":"markdown","ff397a80":"markdown","282d3195":"markdown","52b2888a":"markdown","8f1d0547":"markdown","b6b2c6f7":"markdown","3c87f51b":"markdown","78ea5dde":"markdown","435be619":"markdown","e3bfd675":"markdown","12c1ceb1":"markdown","aaedd3b8":"markdown","e8e0c20d":"markdown"},"source":{"5858bd24":"# Import neccessary basic libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","7149607f":"# Load & read the source file to understand it in detail\n\nLoan_df = pd.read_csv('..\/input\/bank-sl-project\/Bank_SL_Project.csv')","fa59ba7e":"Loan_df.head() # displays the top 5 rows of dataframe.","3de84453":"# getting total number of rows and column in the dataframe\nprint(f\" Shape of the dataframe = {Loan_df.shape}\")\ntotalrows=Loan_df.shape[0]\nprint(f\" Total number of rows in the dataset =  {totalrows}\")","31f9f88a":"Loan_df.info() \nLoan_df.dtypes # Checking data type of each colunm to check if any type needs to be changed","fe365c28":"# Checking Missing values in dataset\n\nLoan_df.isna().sum()","6c77b33a":"Loan_df.isnull().sum()","b1f51927":"Loan_df['CreditCard'].value_counts() # It is categorical Data","e0e89a72":"Loan_df['Family'].value_counts() # It is categorical Data","9716b700":"Loan_df['Personal Loan'].value_counts() # It is categorical Data","42283bc7":"Loan_df['Securities Account'].value_counts() # It is categorical Data","e8206b85":"Loan_df['CD Account'].value_counts() # It is categorical Data","ace61883":"Loan_df['Online'].value_counts() # It is categorical Data","c0265a09":"Loan_df['CreditCard'].value_counts() # It is categorical Data","56b1ef75":"#Convert variables to a categorical variable as relevant\n\nLoan_df['Family'] = Loan_df['Family'].astype('category')\nLoan_df['Education'] = Loan_df['Education'].astype('category')\nLoan_df['Personal Loan']=Loan_df['Personal Loan'].astype('category')\nLoan_df['Securities Account']=Loan_df['Securities Account'].astype('category')\nLoan_df['CD Account']=Loan_df['CD Account'].astype('category')\nLoan_df['Online']=Loan_df['Online'].astype('category')\nLoan_df['CreditCard']=Loan_df['CreditCard'].astype('category')","4ff40229":"# Now Recheck the Data type of all attributes in data set\nLoan_df.dtypes","43f0916f":"# Ratio of Yes to No to identify data imbalance in Dependent Variable 'Personal Loan'\n\nLoan_df['Personal Loan'].value_counts(normalize=True)","1f8765bf":"# Check data distribution using summary statistics\nLoan_df.describe(include='all').T.round(2)","c0067b46":"# Corelation analysis between the variables using heat map\n\ncorelation = plt.cm.viridis_r # Color range used in heat map\nplt.figure(figsize=(15,10))\nplt.title('Corelation between Attributes', y=1.02, size=20)\nsns.heatmap(data=Loan_df.corr().round(2), linewidths=0.1, vmax=1, square=True, cmap=corelation, linecolor='black', annot=True);","e02ff770":"Loan_df.columns","3e2c781d":"plt.figure(figsize=(15,6))\nplt.subplot(1,3,1)\nplt.title('Distribution of Age')\nsns.distplot(Loan_df['Age'], color='r')\n\n# Subplot 2\nplt.subplot(1,3,2)\nplt.title('Distribution of Exp.')\nsns.distplot(Loan_df['Experience'], color='g')\n\n# Subplot 3\nplt.subplot(1,3,3)\nplt.title('Distribution of Income')\nsns.distplot(Loan_df['Income'], color='b')\n\n\n# Box plot distribution of Data\nplt.figure(figsize=(20,8))\nplt.subplot(1,4,1)\nplt.title('Distribution of Age')\nsns.boxplot(Loan_df['Age'],orient='vertical',color='r')\n\n# Box Subplot 2\nplt.subplot(1,4,2)\nplt.title('Distribution of Exp.')\nsns.boxplot(Loan_df['Experience'],orient='vertical',color='g')\n\n# Box Subplot 3\nplt.subplot(1,4,3)\nplt.title('Distribution of Income')\nsns.boxplot(Loan_df['Income'],orient='vertical',color='b')\n","218638de":"plt.figure(figsize=(15,6))\nplt.subplot(1,3,1)\nplt.title('Distribution of ZIP Code')\nsns.distplot(Loan_df['ZIP Code'],color='r')\n\n# Subplot 2\nplt.subplot(1,3,2)\nplt.title('Distribution of Mortgage')\nsns.distplot(Loan_df['Mortgage'], color='g')\n\n# Subplot 3\nplt.subplot(1,3,3)\nplt.title('Distribution of CC Avg.')\nsns.distplot(Loan_df['CCAvg'], color='y')\n\n\n\n# Box plot distribution of Data\nplt.figure(figsize=(20,6))\nplt.subplot(1,4,1)\nplt.title('Distribution of ZIP Code')\nsns.boxplot(Loan_df['ZIP Code'],orient='vertical',color='r')\n\n# Box Subplot 2\nplt.subplot(1,4,2)\nplt.title('Distribution of Mortgage')\nsns.boxplot(Loan_df['Mortgage'],orient='vertical',color='g')\n\n# Box Subplot 3\nplt.subplot(1,4,3)\nplt.title('Distribution of CC Avg.')\nsns.boxplot(Loan_df['CCAvg'],orient='vertical',color='y')\n\n","f8d1192c":"plt.figure(figsize=(16,6))\nplt.subplot(1,3,1)\nplt.title('Distribution of Family Members')\nsns.countplot(Loan_df['Family'], palette='Greens')\n\n# Subplot 2\nplt.subplot(1,3,2)\nplt.title('Distribution of Education')\nsns.countplot(Loan_df['Education'], palette='Reds')\n\n# Subplot 3\nplt.subplot(1,3,3)\nplt.title('Distribution of Account Security (1=Yes, 0=N0)')\nsns.countplot(Loan_df['Securities Account'], palette='Greys')\n\n# Subplot 4\nplt.figure(figsize=(22,6))\nplt.subplot(1,4,1)\nplt.title('Distribution of CD Account (1=Yes, 0=N0)')\nsns.countplot(Loan_df['CD Account'], palette='viridis')\n\n# Subplot 5\nplt.subplot(1,4,2)\nplt.title('Distribution of Online Usage (1=Yes, 0=N0)')\nsns.countplot(Loan_df['Online'], palette='RdYlGn')\n\n# Subplot 6\nplt.subplot(1,4,3)\nplt.title('Distribution of Credit Card (1=Yes, 0=N0)')\nsns.countplot(Loan_df['CreditCard'], palette='Accent')","d679f18d":"# Univariate distribution of Target Variable\n\nplt.title('Distribution of Personal Loan')\nsns.countplot(Loan_df['Personal Loan'], palette='Accent')","e5abfeaa":"# Relationship of Dependent Variable on Independent Attributes\n\nplt.figure(figsize=(15,6))\nplt.subplot(1,3,1)\nplt.title('Personal Loan Vs Age')\nsns.boxplot(Loan_df['Age'], Loan_df['Personal Loan'], palette='Greens')\n\n# Subplot 2\nplt.subplot(1,3,2)\nplt.title('Personal Loan Vs Exp.')\nsns.boxplot(Loan_df['Experience'], Loan_df['Personal Loan'], palette='Oranges')\n\n# Subplot 3\nplt.subplot(1,3,3)\nplt.title('Personal Loan Vs Income')\nsns.boxplot(Loan_df['Income'], Loan_df['Personal Loan'], palette='Blues')\n","8dcea833":"# Relationship of Dependent Variable on Independent Attributes\n\nplt.figure(figsize=(15,6))\nplt.subplot(1,3,1)\nplt.title('Personal Loan Vs ZIP Code')\nsns.boxplot(Loan_df['ZIP Code'], Loan_df['Personal Loan'], palette='Greens')\n\n# Subplot 2\nplt.subplot(1,3,2)\nplt.title('Personal Loan Vs Mortgage')\nsns.boxplot(Loan_df['Mortgage'], Loan_df['Personal Loan'], palette='Oranges')\n\n# Subplot 3\nplt.subplot(1,3,3)\nplt.title('Personal Loan Vs CC Avg.')\nsns.boxplot(Loan_df['CCAvg'], Loan_df['Personal Loan'], palette='Blues')","86b7893e":"# Relationship of Dependent Variable on Independent Attributes\n\nplt.figure(figsize=(15,6))\nplt.subplot(1,3,1)\nplt.title('Personal Loan Vs Income')\nsns.barplot(Loan_df['Personal Loan'], Loan_df['Income'], hue=Loan_df['Family'],palette='Greens')\n\n# Subplot 2\nplt.subplot(1,3,2)\nplt.title('Personal Loan Vs Age')\nsns.barplot(Loan_df['Personal Loan'],Loan_df['Age'], hue=Loan_df['Education'], palette='Oranges')\n\n# Subplot 3\nplt.subplot(1,3,3)\nplt.title('Personal Loan Vs Experience')\nsns.barplot(Loan_df['Personal Loan'], Loan_df['Experience'], hue=Loan_df['Education'], palette='Blues')","0c852531":"# Continued study of Dependent Variable.\n\nplt.figure(figsize=(15,6))\nplt.subplot(1,3,1)\nplt.title('Personal Loan Vs Mortgage')\nsns.barplot(Loan_df['Personal Loan'], Loan_df['Mortgage'], hue=Loan_df['Family'],palette='Greys')\n\n# Subplot 2\nplt.subplot(1,3,2)\nplt.title('Personal Loan Vs CC Usage')\nsns.barplot(Loan_df['Personal Loan'],Loan_df['CCAvg'], hue=Loan_df['Education'], palette='YlOrRd')\n\n# Subplot 3\nplt.subplot(1,3,3)\nplt.title('Personal Loan Vs Income')\nsns.barplot(Loan_df['CD Account'], Loan_df['Income'], hue=Loan_df['Personal Loan'], palette='Greys')","94155af3":"# Understand the Data distribution through Pair Plot.\n\nsns.pairplot(Loan_df)","00474f1d":"Loan_df.head()","776b3a91":"# Defining Dependent & Independent Variables for model inputs\n\nX = Loan_df.drop(['ID', 'Personal Loan'], axis=1) # ID is not having any influence on data set hence droped from Independent Variables\ny = Loan_df['Personal Loan'] # Dependent Variable","efbf3808":"X.head()","eaf0010c":"y.head()","16c5dc10":"X = pd.get_dummies(X, drop_first=True ) # #Convert categorical vriables to dummy variables","a7ae45be":"X.head()","3b5c9e23":"# Import train test model to spilt the data in 70:30 ration\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=101)","26c6aff0":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","0c4fc719":"print(\"{0:0.2f}% data is in training set\".format((len(X_train)\/len(Loan_df.index)) * 100))\nprint(\"{0:0.2f}% data is in test set\".format((len(X_test)\/len(Loan_df.index)) * 100))","8916b919":"print(\"Original Personal Loan True Values    :{0}({1:0.2f}%)\".format(len(Loan_df.loc[Loan_df['Personal Loan']==1]), (len(Loan_df.loc[Loan_df['Personal Loan']==1])\/len(Loan_df.index))*100))\nprint(\"Orinical Personal Loan False Values   :{0}({1:0.2f}%)\".format(len(Loan_df.loc[Loan_df['Personal Loan']==0]), (len(Loan_df.loc[Loan_df['Personal Loan']==0])\/len(Loan_df.index))*100))\nprint(\"\")\nprint (\"Training Personal Loan True Values   :{0}({1:0.2f}%)\".format(len(y_train.loc[y_train[:]==1]),(len(y_train.loc[y_train[:]==1])\/len(y_train.index))*100))\nprint (\"Training Personal Loan False Values  :{0}({1:0.2f}%)\".format(len(y_train.loc[y_train[:]==0]),(len(y_train.loc[y_train[:]==0])\/len(y_train.index))*100))\nprint(\"\")\nprint (\"Testing Personal Loan True Values    :{0}({1:0.2f}%)\".format(len(y_test.loc[y_test[:]==1]),(len(y_test.loc[y_test[:]==1])\/len(y_test.index))*100))\nprint (\"Testing Personal Loan False Values   :{0}({1:0.2f}%)\".format(len(y_test.loc[y_test[:]==0]),(len(y_test.loc[y_test[:]==0])\/len(y_test.index))*100))","10a20965":"# To model the Navie Bayes classifier imoprt Bernoulli, Multinomial & Gaussian classifiers\n\nfrom sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB","b30989b3":"# To calculate the accuracy score of the model, report & build confusion metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report, recall_score\nfrom sklearn.metrics import confusion_matrix","662a2b2b":"# Implement Classifiers to build the Model using Bernoulli Classifier\nBer = BernoulliNB()\nBer.fit(X_train, y_train) # Model Trained using training Data","b799cae9":"y_pred = Ber.predict(X_test) # Model is ready for predictions based on test Data\ny_pred_Train1 = Ber.predict(X_train) # Prediction of Training Data","2e75b389":"confusion_matrix(y_test, y_pred)","a0a694e0":"print(\"Accuracy of the Bernoulli NB is  :({:0.2f}%)\".format(accuracy_score(y_pred, y_test)*100)) # Accuracy of our Bernoulli Naive Bayes model\n","0f830688":"cm1 = confusion_matrix(y_train,y_pred_Train1, labels=[0,1]) # Confusion metrix of Bernoulli NB Classifier on Test Data\nplt.figure(figsize=(20,5))\nplt.subplot(1,3,1)\nplt.title(\"CM of Training Data\")\ncm1_df = pd.DataFrame(cm1, columns=[i for i in [\"Actual 1\", \"Actual 0\"]], index=[i for i in [\"Predict 1\",\"Predict 0\"]])\nsns.heatmap(data=cm1_df, annot=True, fmt='.4g')\n\ncm2 = confusion_matrix(y_test,y_pred, labels=[0,1]) # Confusion metrix of Bernoulli NB Classifier on Test Data\nplt.subplot(1,3,2)\nplt.title(\"CM of Test Data\")\ncm2_df = pd.DataFrame(cm2, columns=[i for i in [\"Actual 1\", \"Actual 0\"]], index=[i for i in [\"Predict 1\",\"Predict 0\"]])\nsns.heatmap(data=cm2_df, annot=True, fmt='.4g')","68193262":"print(classification_report(y_test, y_pred)) # Classification Report of Bernoulli NB Model\nprint(\"\")\nprint(classification_report(y_train,y_pred_Train1))","2332aa06":"# Implement Classifiers to build the Model using Gaussian Classifier\nGau = GaussianNB()\nGau.fit(X_train, y_train) # Model Trained using training Data.","a2307bbc":"y_pred1 = Gau.predict(X_test) # Model is ready for predictions based on test Data\ny_pred_Train2 = Gau.predict(X_train)","3e2f9fb3":"print(\"Accuracy of the Gaussian NB is  :({:0.2f}%)\".format(accuracy_score(y_pred1, y_test)*100)) # Accuracy of our Gaussian Naive Bayes model","a0e0dbbc":"cm3 = confusion_matrix(y_train,y_pred_Train2, labels=[0,1]) # Confusion metrix of Gaussian NB Classifier on Test Data\nplt.figure(figsize=(20,5))\nplt.subplot(1,3,1)\nplt.title(\"CM of Training Data\")\ncm3_df = pd.DataFrame(cm3, columns=[i for i in [\"Actual 1\", \"Actual 0\"]], index=[i for i in [\"Predict 1\",\"Predict 0\"]])\nsns.heatmap(data=cm3_df, annot=True, fmt='.5g')\n\ncm4 = confusion_matrix(y_test,y_pred1, labels=[0,1]) # Confusion metrix of Gaussian NB Classifier on Test Data\nplt.subplot(1,3,2)\nplt.title(\"CM of Test Data\")\ncm4_df = pd.DataFrame(cm4, columns=[i for i in [\"Actual 1\", \"Actual 0\"]], index=[i for i in [\"Predict 1\",\"Predict 0\"]])\nsns.heatmap(data=cm4_df, annot=True, fmt='.5g')","95e8343c":"print (classification_report(y_test,y_pred1)) # Print Classification Report to study other important parameters of the model\nprint(\"\")\nprint(classification_report(y_train,y_pred_Train2)) # Classification Report of Training Data","f117baaf":"# Import supporting Libraries\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve","034bd3a2":"#Build the logistic regression model\nlogisticRegr = LogisticRegression(solver='liblinear')\nlogisticRegr.fit(X_train,y_train)","8d0ae452":"y_pred2 = logisticRegr.predict(X_test) # Model is ready to predict based on test Data\ny_pred_Train3 = logisticRegr.predict(X_train) # Predictions for Training Data","1fe131f6":"print(logisticRegr.score(X_test, y_test))\nprint(\"Accuracy of the Logistic Regression Model is  :({:0.2f}%)\".format(accuracy_score(y_pred2, y_test)*100))","8f7b0bb2":"import statsmodels.api as sm\n\nlogit = sm.Logit(y_train, sm.add_constant(X_train))\nlg = logit.fit()\nlg.summary2()","6ff8d99b":"#Calculate Odds Ratio, probability\n##create a data frame to collate Odds ratio, probability and p-value of the coef\nlgcoef = pd.DataFrame(lg.params, columns=['coef'])\nlgcoef.loc[:, \"Odds_ratio\"] = np.exp(lgcoef.coef)\nlgcoef['probability'] = lgcoef['Odds_ratio']\/(1+lgcoef['Odds_ratio'])\nlgcoef['pval']=lg.pvalues\npd.options.display.float_format = '{:.2f}'.format","10f166bd":"# FIlter by significant p-value (pval <0.1) and sort descending by Odds ratio\nlgcoef = lgcoef.sort_values(by=\"Odds_ratio\", ascending=False)\npval_filter = lgcoef['pval']<=0.1\nlgcoef[pval_filter]","640c628c":"cm_LR1 = plt.cm.Greens_r # Color Scheme for confusion metrics\ncm5 = confusion_matrix(y_train,y_pred_Train3, labels=[0,1]) # Confusion metrix for logistic Regression Classifier on Training Data\nplt.figure(figsize=(20,5))\nplt.subplot(1,3,1)\nplt.title(\"CM of Training Data\")\ncm5_df = pd.DataFrame(cm5, columns=[i for i in [\"Actual 1\", \"Actual 0\"]], index=[i for i in [\"Predict 1\",\"Predict 0\"]])\nsns.heatmap(data=cm5_df, annot=True, fmt='.5g', cmap=cm_LR1, linecolor='Black', square=True)\n\ncm_LR2 = plt.cm.Reds_r # Color Scheme for confusion metrics\ncm6 = confusion_matrix(y_test,y_pred2, labels=[0,1]) # Confusion metrix of Logistic Regression Classifier on Test Data\nplt.subplot(1,3,2)\nplt.title(\"CM of Test Data\")\ncm6_df = pd.DataFrame(cm6, columns=[i for i in [\"Actual 1\", \"Actual 0\"]], index=[i for i in [\"Predict 1\",\"Predict 0\"]])\nsns.heatmap(data=cm6_df, annot=True, fmt='.5g', cmap=cm_LR2, linecolor='Black', square=True)","8495cf78":"print(classification_report(y_pred2, y_test))\nprint(\"\")\nprint(classification_report(y_pred_Train3, y_train))","a68e1e7c":"# ROC AUC Curves Logistic Regression.\n\nlogit_roc_auc = roc_auc_score(y_test, y_pred2)\nfpr, tpr, thresholds = roc_curve(y_test, y_pred2)\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('AUC ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","63720a0f":"# Import libraries to build KNN model\nfrom sklearn.neighbors import KNeighborsClassifier","6bb2be0d":"knn = KNeighborsClassifier(n_neighbors=5, weights='distance')","7c9c5900":"# Call KNN algorithm\n\nknn.fit(X_train,y_train)","d0ba5a56":"# For every test data point, predict it's label based on 5 nearest neighbours in this model.\n\ny_pred3 = knn.predict(X_test)\ny_pred_Train4 = knn.predict(X_train)","5424fafc":"print(\"Accuracy of the KNN Test Model is      :({:0.2f}%)\".format(accuracy_score(y_pred3, y_test)*100))\nprint(\"Accuracy of the KNN Training Model is  :({:0.2f}%)\".format(accuracy_score(y_pred_Train4, y_train)*100))","e53c4028":"print(confusion_matrix(y_pred3, y_test)) # Confusion metrics for Test Data with K=5 Value\nprint(\"\")\nprint(confusion_matrix(y_pred_Train4,y_train)) # Confusion metrics for Training Data with K=5 value\nprint(\"\")\nprint(\"\")\nprint(classification_report(y_pred3, y_test))\nprint(\"\")\nprint(classification_report(y_pred_Train4,y_train))","6185d1f0":"# Building graphical metrics to find optimal value of K.\nscores = []\nfor k in range (1,100):\n    knn = KNeighborsClassifier(n_neighbors=k, weights='distance')\n    knn.fit(X_train, y_train)\n    scores.append(knn.score(X_test,y_test))\nplt.plot(range(1,100), scores)\nplt.xlabel(\"Number of K Neighbors\")\nplt.ylabel(\"Accuracy Score\")","1bbdc8d4":"# creating odd list of K for KNN\nmyList = list(range(1,50))\n\n# subsetting just the odd ones\nneighbors = list(filter(lambda x: x % 2 != 0, myList))","29851163":"# empty list that will hold accuracy scores\nac_scores = []\n\n# perform accuracy metrics for values from 1,3,5....49\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    # predict the response\n    y_pred3 = knn.predict(X_test)\n    # evaluate accuracy\n    scores = accuracy_score(y_test, y_pred3)\n    ac_scores.append(scores)\n\n# changing to misclassification error\nMSE = [1 - x for x in ac_scores]\n\n# determining best k\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","5dce7d86":"#Plot misclassification error vs k (with k value on X-axis) using matplotlib.\n# plot misclassification error vs k\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","607c772f":"#Use K=23 as the final model for prediction\nknnfinal = KNeighborsClassifier(n_neighbors = 23)\n\n# fitting the model\nknnfinal.fit(X_train, y_train)\n\n# predict the response\ny_pred4 = knnfinal.predict(X_test)\ny_pred_Train5 = knnfinal.predict(X_train)\n\n# evaluate accuracy\n\nprint(\"Accuracy of the KNN Test Model is      :({:0.2f}%)\".format(accuracy_score(y_test, y_pred4)*100))\nprint(\"Recall of the KNN Test Model is        :({:0.2f}%)\".format(recall_score(y_test, y_pred4)*100))","8330ec7c":"cm_knn1 = plt.cm.Greys_r # Color Scheme for confusion metrics\ncm9 = confusion_matrix(y_train,y_pred_Train5, labels=[0,1]) # Confusion metrix for KNN Classifier on Training Data\nplt.figure(figsize=(20,5))\nplt.subplot(1,3,1)\nplt.title(\"CM of Training Data\")\ncm9_df = pd.DataFrame(cm9, columns=[i for i in [\"Actual 1\", \"Actual 0\"]], index=[i for i in [\"Predict 1\",\"Predict 0\"]])\nsns.heatmap(data=cm9_df, annot=True, fmt='.5g', cmap=cm_knn1, linecolor='Black', square=True)\n\ncm_knn2 = plt.cm.Oranges_r # Color Scheme for confusion metrics\ncm10 = confusion_matrix(y_test,y_pred4, labels=[0,1]) # Confusion metrix of KNN Classifier on Test Data\nplt.subplot(1,3,2)\nplt.title(\"CM of Test Data\")\ncm10_df = pd.DataFrame(cm10, columns=[i for i in [\"Actual 1\", \"Actual 0\"]], index=[i for i in [\"Predict 1\",\"Predict 0\"]])\nsns.heatmap(data=cm10_df, annot=True, fmt='.5g', cmap=cm_knn2, linecolor='Black', square=True)","bfb17b4a":"print(classification_report(y_test, y_pred4))\nprint(\"\")\nprint(classification_report(y_train, y_pred_Train5))","265340f0":"# Import SVM library for the model building\n\nfrom sklearn.svm import SVC","cdda8cb0":"svc = SVC()\nsvc.fit(X_train,y_train) # Model training\n\nprint(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test, y_test)))","2fb7c588":"# Scale the data points using MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)","4a501be9":"# Now fit the data on scaled points\nsvc.fit(X_train_scaled,y_train)\nprint('Accuracy on training set: {:.2f}'.format(svc.score(X_train_scaled,y_train)))\nprint('Accuracy on testting set: {:.2f}'.format(svc.score(X_test_scaled,y_test)))","cdb8002f":"# predict the response\ny_pred5 = svc.predict(X_test_scaled)\ny_pred_Train6 = svc.predict(X_train_scaled)","10e33cb7":"confusion_matrix(y_test,y_pred5)","b8fbe3a5":"cm_svm1 = plt.cm.Blues_r # Color Scheme for confusion metrics\ncm11 = confusion_matrix(y_train,y_pred_Train6, labels=[0,1]) # Confusion metrix for SVM on Training Data\nplt.figure(figsize=(20,5))\nplt.subplot(1,3,1)\nplt.title(\"CM of Training Data\")\ncm11_df = pd.DataFrame(cm11, columns=[i for i in [\"Actual 1\", \"Actual 0\"]], index=[i for i in [\"Predict 1\",\"Predict 0\"]])\nsns.heatmap(data=cm11_df, annot=True, fmt='.5g', cmap=cm_svm1, linecolor='Black', square=True)\n\ncm_svm2 = plt.cm.Oranges_r # Color Scheme for confusion metrics\ncm12 = confusion_matrix(y_test,y_pred5, labels=[0,1]) # Confusion metrix for SVM on Test Data\nplt.subplot(1,3,2)\nplt.title(\"CM of Test Data\")\ncm12_df = pd.DataFrame(cm12, columns=[i for i in [\"Actual 1\", \"Actual 0\"]], index=[i for i in [\"Predict 1\",\"Predict 0\"]])\nsns.heatmap(data=cm12_df, annot=True, fmt='.5g', cmap=cm_svm2, linecolor='Black', square=True)","0169675e":"print(classification_report(y_test,y_pred5))\nprint(\"\")\nprint(classification_report(y_train,y_pred_Train6))","6bd198c0":"# Try improving the SVM model accuracy using C & gamma\nsvc1 = SVC(gamma=0.1, C=1000)\nsvc1.fit(X_train_scaled, y_train)\n\nprint(\"Accuracy on training set (g=0.1, C=1000): {:.3f}\".format(svc1.score(X_train_scaled, y_train)*100))\nprint(\"Accuracy on test set (g=0.1, C=1000): {:.3f}\".format(svc1.score(X_test_scaled, y_test)*100))\nprint(\"\")\n\nsvc2 = SVC(gamma=0.1, C=100)\nsvc2.fit(X_train_scaled, y_train)\n\nprint(\"Accuracy on training set (g=0.1, C=100): {:.3f}\".format(svc2.score(X_train_scaled, y_train)*100))\nprint(\"Accuracy on test set (g=0.1, C=100): {:.3f}\".format(svc2.score(X_test_scaled, y_test)*100))\nprint(\"\")\n\nsvc3 = SVC(gamma=0.01, C=1000)\nsvc3.fit(X_train_scaled, y_train)\n\nprint(\"Accuracy on training set (g=0.01, C=100): {:.3f}\".format(svc3.score(X_train_scaled, y_train)*100))\nprint(\"Accuracy on test set (g=0.01, C=100): {:.3f}\".format(svc3.score(X_test_scaled, y_test)*100))\nprint(\"\")\n\nsvc4 = SVC(gamma=0.01, C=100)\nsvc4.fit(X_train_scaled, y_train)\n\nprint(\"Accuracy on training set (g=0.01, C=100): {:.3f}\".format(svc4.score(X_train_scaled, y_train)*100))\nprint(\"Accuracy on test set (g=0.01, C=100): {:.3f}\".format(svc4.score(X_test_scaled, y_test)*100))\nprint(\"\")\n\nsvc5 = SVC(gamma=0.01, C=10)\nsvc5.fit(X_train_scaled, y_train)\n\nprint(\"Accuracy on training set (g=0.01, C=10): {:.3f}\".format(svc5.score(X_train_scaled, y_train)*100))\nprint(\"Accuracy on test set (g=0.01, C=10): {:.3f}\".format(svc5.score(X_test_scaled, y_test)*100))\nprint(\"\")\n\nsvc6 = SVC(gamma=0.1, C=10)\nsvc6.fit(X_train_scaled, y_train)\n\nprint(\"Accuracy on training set (g=0.1, C=10): {:.3f}\".format(svc6.score(X_train_scaled, y_train)*100))\nprint(\"Accuracy on test set (g=0.1, C=10): {:.3f}\".format(svc6.score(X_test_scaled, y_test)*100))\nprint(\"\")\n\nsvc7 = SVC(gamma=1, C=10)\nsvc7.fit(X_train_scaled, y_train)\n\nprint(\"Accuracy on training set (g=1, C=10): {:.3f}\".format(svc7.score(X_train_scaled, y_train)*100))\nprint(\"Accuracy on test set (g=1, C=10): {:.3f}\".format(svc7.score(X_test_scaled, y_test)*100))\nprint(\"\")\n\nsvc8 = SVC(gamma=10, C=100)\nsvc8.fit(X_train_scaled, y_train)\n\nprint(\"Accuracy on training set (g=10, C=100): {:.3f}\".format(svc8.score(X_train_scaled, y_train)*100))\nprint(\"Accuracy on test set (g=10, C=100): {:.3f}\".format(svc8.score(X_test_scaled, y_test)*100))\nprint(\"\")\n\nsvc9 = SVC(gamma=100, C=1000)\nsvc9.fit(X_train_scaled, y_train)\n\nprint(\"Accuracy on training set (g=100, C=1000): {:.3f}\".format(svc9.score(X_train_scaled, y_train)*100))\nprint(\"Accuracy on test set (g=100, C=1000): {:.3f}\".format(svc9.score(X_test_scaled, y_test)*100))\nprint(\"\")\n\n","8694f8b4":"# predict the response with svm2 Model\ny_pred6 = svc2.predict(X_test_scaled)\ny_pred_Train7 = svc2.predict(X_train_scaled)\n\n# predict the response with svm6 Model\ny_pred7=svc6.predict(X_test_scaled)\ny_pred_Train8 =svc6.predict(X_train_scaled)\n","2b311e84":"print(confusion_matrix(y_test,y_pred6))\nprint(\"\")\nprint(confusion_matrix(y_test,y_pred7))","5f5bbba1":"cm_svm3 = plt.cm.Greys_r # Color Scheme for confusion metrics\ncm13 = confusion_matrix(y_train,y_pred_Train7, labels=[0,1]) # Confusion metrix for SVM on Training Data\nplt.figure(figsize=(20,5))\nplt.subplot(1,3,1)\nplt.title(\"CM of Training Data\")\ncm13_df = pd.DataFrame(cm13, columns=[i for i in [\"Actual 1\", \"Actual 0\"]], index=[i for i in [\"Predict 1\",\"Predict 0\"]])\nsns.heatmap(data=cm13_df, annot=True, fmt='.5g', cmap=cm_svm3, linecolor='Black', square=True)\n\ncm_svm4 = plt.cm.Blues_r # Color Scheme for confusion metrics\ncm14 = confusion_matrix(y_test,y_pred6, labels=[0,1]) # Confusion metrix for SVM on Test Data\nplt.subplot(1,3,2)\nplt.title(\"CM of Test Data\")\ncm14_df = pd.DataFrame(cm14, columns=[i for i in [\"Actual 1\", \"Actual 0\"]], index=[i for i in [\"Predict 1\",\"Predict 0\"]])\nsns.heatmap(data=cm14_df, annot=True, fmt='.5g', cmap=cm_svm4, linecolor='Black', square=True)","c0ca237b":"print(classification_report(y_test,y_pred6))\nprint(\"\")\nprint(classification_report(y_train,y_pred_Train7))","d0a04344":"# ROC AUC Curves SMV.\n\nsvm_roc_auc = roc_auc_score(y_test, y_pred6)\nfpr1, tpr1, thresholds1 = roc_curve(y_test, y_pred6)\nplt.figure()\nplt.plot(fpr1, tpr1, label='SVM (area = %0.2f)' % svm_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('AUC ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","73aae572":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score","8ddf79fb":"def generate_model_report (y_actual, y_predicted):\n    print (\"Accuracy =\", round(accuracy_score(y_actual,y_predicted)*100,2))\n    print (\"Precision =\", round(precision_score(y_actual,y_predicted)*100,2))\n    print (\"Recall =\", round(recall_score(y_actual,y_predicted)*100,2))\n    print (\"F1 Score =\", round(f1_score(y_actual,y_predicted)*100,2))\n    pass","4ae4e6bb":"print(\"Bernoulli NB Model Report\")\nprint(\"\")\ngenerate_model_report(y_test,y_pred)\nprint(\"\")\n\nprint(\"Gaussian NB Model Report\")\nprint(\"\")\ngenerate_model_report(y_test,y_pred1)\nprint(\"\")\n\nprint(\"Logistic Regression Model Report\")\nprint(\"\")\ngenerate_model_report(y_test,y_pred2)\nprint(\"\")\n\nprint(\"KNN Model Report K=5\")\nprint(\"\")\ngenerate_model_report(y_test,y_pred3)\nprint(\"\")\n\nprint(\"KNN Model Report K=23\")\nprint(\"\")\ngenerate_model_report(y_test,y_pred4)\nprint(\"\")\n\nprint(\"SVM Model Report with Default gamma & C \")\nprint(\"\")\ngenerate_model_report(y_test,y_pred5)\nprint(\"\")\n\nprint(\"SVM Model Report with Optimized gamma & C \")\nprint(\"\")\ngenerate_model_report(y_test,y_pred6)\nprint(\"\")","cfff6228":"* From the above combinations of gamma & C, we have observed that SVM models (svm2 & svm6) with C=100, C=10 & gamma=0.1 are having better accuracy than other values for both training & testing Data, hence we will choose these two for further study.","22512770":"#### * for K=5 algorithm is doing extreamly good for training data but testing data still have a lot of misclassification & poor precision and recall values, need to find the optimal value of K for the Model.","611f0f87":"##### *  From the classification report it is clear that accuracy of Gaussian NB model is dropped to 88% from 90% (Bernoulli NB) for test Data.\n##### * Precision decreased to 42% from 48% but Recall improved to 55% from 14% for test data. However still it's not a good model to predict the likelihood of customers buying personal loans.","9d79d841":"# 3. Model Building","62a563b2":"### 3.2.2 Gaussian NB Classification\n\n* Generally used in cases where independent features in data set are contineous in nature. In Gaussian classification contineous values associated with each attribute are assumed to be distributed normally.","2fc5ab71":"#### * Much better classification of the given data using support vector machine model for both training & testing Data.","12612721":"##### *  So the above results shows that accuracy of the model is almost ~90% based on bernoulli classifyer\n##### * But the precision & recall are really bad 48% and 14% respectively for test Data, Also these parameters are poor for training data. hence not a good model to predict the values.","bbbe144c":"### Confusion Metrics (KNN)","f7800985":"#### Univariate Analysis of Categorical Variables","8c4554c0":"## 1. Reading Source File and Checking Data","b8a4f6ba":"#### * AUC is 0.65 for the test Data but still it is not up to expectations, hence not a good model to use as of now","71a08bba":"#### Customers accepted Personal loan have no relation with ZIP Code.\n\n#### Customers with high avg. spending on credit card have opted for Personal Loan, though CCAvg. & Mortgage both are having many outliers.","13784039":"### 2.2.  Study of data distribution in each attribute.","ba29ac13":"# Final Inference of the Project","e7d235c0":"##### Most of the people are in the Age range of 35 to 60\n##### Most of the people are having experience in the range 10 yrs. to 35 yrs.\n##### Avg. income of people is somewhere between 50 dollar to 180 dollar but here we are having large outliers","a67af97a":"### Confusion Metrics (SVM with C=100 & gamma=0.1)","affdac45":"### 3.2.1 Bernoulli NB Classification\n\n* Usually used in cases where categorical attributes have only two values (0 & 1).","842314f8":"# 3.5 Support Vector Machine","fcfb1ec0":"#### From the above graph we can say that customers having high Mortgage have opted for Personal loan in past as compared to low Mortgage customers.\n\n#### Also customer who's credit card usage is higher is potential candidate for Personal loan. ","113d9552":"### 2.3.  Study of distribution of Target Attribute (Personal Loan).","9d6cb273":"# Data Description:\n\nThe file contains data on 5000 customers. The data include customer demographic information (age, income, etc.), the customer's relationship with the bank (mortgage, securities account, etc.), and the customer response to the last personal loan campaign (Personal Loan). Among these 5000 customers, only 480 (= 9.6%) accepted the personal loan that was offered to them in the earlier campaign.\n\n### Domain:\nBanking\n\n### Context:\nThis case is about a bank (Thera Bank) whose management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors). A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio with minimal budget.\n\n### Attribute Information:\n* ID : Customer ID\n* Age : Customer's age in completed years\n* Experience : #years of professional experience\n* Income : Annual income of the customer.\n* ZIP Code : Home Address ZIP code.\n* Family : Family size of the customer.\n* CCAvg : Avg. spending on credit cards per month.\n* Education : Education Level. 1: Undergrad; 2: Graduate; 3: Advanced\/Professional\n* Mortgage : Value of house mortgage if any.\n* Personal Loan : Did this customer accept the personal loan offered in the last campaign?\n* Securities Account : Does the customer have a securities account with the bank?\n* CD Account : Does the customer have a certificate of deposit (CD) account with the bank?\n* Online : Does the customer use internet banking facilities?\n* Credit card : Does the customer use a credit card issued by UniversalBank?\n\n### Learning Outcomes:\n\n* Exploratory Data Analysis\n* Preparing the data to train a model\n* Training and making predictions using a classification model\n* Model evaluation\n\n### Objective:\n\nThe classification goal is to predict the likelihood of a liability customer buying personalloans.\n\n### Steps and tasks:\n1. Read the column description and ensure you understand each attribute well\n2. Study the data distribution in each attribute, share your findings (15 marks)\n3. Get the target column distribution. Your comments (5 marks)\n4. Split the data into training and test set in the ratio of 70:30 respectively (5 marks)\n5. Use different classification models (Logistic, K-NN and Na\u00efve Bayes) to predict the likelihood of a customer buying personal loans (15 marks)\n6. Print the confusion matrix for all the above models (5 marks)\n7. Give your reasoning on which is the best model in this case and why it performsbetter? (5 marks)","bcf1c686":" #### Univariate Analysis of Contineous Variables","a8f0960f":"* Customers with Education_2\/3 & CD Account_1 have a 98% probability of getting the Personal loan as per training Data\n* Also R^2 value is healty 0.644 for training Data.","3ca58107":"##### ZIP Code is having most of the values in 90K to 1L with some outliers in lower range.\n##### Mortgage attribute is having lot of outliers in given data \n##### Avg. spending on credit card (CCAvg.) per month is between 1K to 5K dollar with right skewed distribution, also here we are having large outliers.","70e05902":"#### * Above report shows better balance in training & testing data with K=23 compared to biased results with K=5, however still the results of accuracy along with precision & recall are not satisfactory to choose the model for using Personal loan predictions. Model is overfitting as training data is performing very well, however testing data is performing very bad.","083441bf":"* We will compare all the parameters of all models we have build to conclude which is better for predicting the customers will buy personal loan from the bank","1e68f913":"* SVM model has better classification of given data, specially false negatives are much controlled, however false negative is still on higher side (63) for test data.","1e0221d6":"#### 1. There is very strong corelation between independent variabes \"Experience\" & \"Age\"\n#### 2. There is corelation between attributes \"CCAvg\" & \"Income\"\n#### 3. Other attributes are not having any strong corelation","b20b289b":"#### Confusion Metrics (Bernoulli NB)","d6f4872e":"## 2. Data Cleaning & Exploratory Data Analysis","f7af0d95":"#### From the above two steps its observed that we have 13 int and 1 float attribute in given data, but attributes e.g. 'Education' , 'Personal Loan', 'Credit Card' and 'Family' seems categorical as per description given in problem statement, need to understand them further.","1d0c00e3":"* Confution metrics is balanced after choosing optimal value of K (23), but misclassifications are still observed in testing data.","258b7d2e":"#### Customers accepted Personal Loan are equally distributed over Age & Experience, hoewever as far as income is concerned customers with higher income has accepted more personal loans as compared to lower earners","e57f78e0":"* Customers having higher income are more likely to opt for Personal Loan\n* Customers Age & Experience is not having much impact on Personal Loans.","a8a9c689":"#### SVM report is much improved as compared to other model in terms of accuracy, precision & recall, however recall is still a concern for this model as well. We will try different values of C & gamma to optimize the model results. ","d0fb8d4b":"* Categorical Data is already in number format, hence label encoding is not required here.","7c152cd3":"#### Above results of all models are showing that support vector machine Model with optimized value of gamma & C is much better than other models, Hence we will select SVM model for the project rollout.","9f640365":"## 3.2 Naive Bayes Classification Model\n\n* Naive bayes classification is based on bayes theorem which works on probabilities, It assumes that presence of a particular feature in class is unrelated to the presence of any other feature","cf1e8430":"###    2.1 Read the column description and to understand each attribute well","6d75531f":"### Insights based on above 5 point summary\n    1. Age & Experience attributes seems normally distributed\n    2. Mean and Median values of Income & Mortgage are very different, hence these are skewed in nature\n    3. Standard deviation of 'Income' and 'Mortgage' attribute is very high.\n    4. CCAvg variable is also right skewed as mean & median are different\n    5. Other attributes are categorical in nature and having different 'unique' categories e.g Family=4, Education=3 & rest      other are having 2.\n    6. ZIP Code is address of people, it is looking normally distributed, but deviation in minimum value & mean is huge.","1eeee37a":"## 3.1 Model Slicing (70:30 ratio)","ba7dc92b":"* Very few custmers have accepted the Personal Loan from the Bank, Its clear that we have class imbalance in the dependent variable.","66bc37ae":"No null values found in any attribute","2b11cafe":"### Confusion Metrics (Logistic Regression)","522b19fa":"* Logistic Regression is also having misclassification of Data in Training (FP=94, FN=233) as well as in testing Data (FP=32, FN=99)","89a1d647":"#### Confusion Metrics (Gaussian NB)","ff397a80":"#### From the above outcomes it is clear that these attributes are categorical in nature hence data type of these attributes need to be changed from contineous (int) to categorical to make them relevant to data set","282d3195":"* In the above confusion metrix we have huge misclassification (FP = 23, FN = 125) for the test Data\n* Training Data also have misclassification (FP=41, FN=274) will lead to lower Precision & recall parameters","52b2888a":"* Age & Expereince, Income & Avg. Credit Card Usage (CCAvg.) are having co-relation other attributes are not much influenced.","8f1d0547":"# 3.4 KNN Classification Model\n\nKNN is popular algorithms used in machine learning & data mining, this algorithm works based on how similar is a data from other. In short each data point is labelled based on distance from its nearest neighbors.","b6b2c6f7":"# 3.3 Logistic Regression\n\nIn statistics, the logistic model is usually taken to apply to a categorical dependent variable. In regression analysis, logit regression estimates the parameters of a logistic model.\n\nIn logistic regression, dependent variable is a logit, which is the natural log of the odds, that is,\n\nlog(Odds) = logit(P)=log(P\/1-P)\n\nSo a logit is a log of \"Odds\" and \"Odds\" are fuction of \"Probability\". In short logistic regression predicts the likelihood or probability of certain event in data set using \"Sigmoid\" fuction.","3c87f51b":"##### from the above metrics it is very clear that the model (svc2) with gamma=0.1 & C=100 is better that svc6 model, hence we will further develop on svc2 model.","78ea5dde":"#### * In the given data set singles and people with undergraduate Education level are in high samples as compared to family size>=2 and graduates\/advanced people.\n\n#### * Customers with not having security account\/certificate of deposits\/uses Credit cards are in high numbers as compared to others in these attributes\n\n#### * Good amount of customers uses Online Internet banking facility.","435be619":"* This method of classification is also not wisely classifying the samples & having high missclassification of data (FP=110 &  FN=66) which will severly impact on Recall & Precision Parameters","e3bfd675":"#### From above Classification table  & ROC curve it is veryclear that SVM model is having much better accuracy, precision, recall, F1 score & AUC parameters than other classification models i.e. (logistic regression, KNN, Naive Bayes).","12c1ceb1":"#### Precision & Recall are very poor for customers opting personal loan (1) from above report for both training as well as testing data, inspite of having accuracy of 91%. ","aaedd3b8":"### Interpretation of Pseudo R^2","e8e0c20d":"* The model overfits substantially with a perfect score on the training set but 90% accuracy on the test set.\n\n* SVM requires all the features to be on a similar scale. We will need to rescale our data that all the features are approximately on the same scale and than see the performance"}}