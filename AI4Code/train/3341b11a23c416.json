{"cell_type":{"1c0d781b":"code","56f65340":"code","80b388e8":"code","8f976034":"code","a47d93c9":"code","4af07377":"code","ebcfa084":"code","470e72ac":"code","c2a85d3f":"code","001f50e0":"code","4088def4":"code","135307d3":"code","ad069d43":"code","2477ee64":"code","eb9e91d2":"code","7098d79c":"code","b662f48d":"code","0a210fcb":"code","c05d9c2c":"code","0e2e628b":"code","b505f94f":"code","df0e4933":"code","88953f14":"code","8572e9f1":"code","ac82e789":"code","7bc0a033":"code","5d5cdfed":"code","2814a366":"code","b0077c57":"code","25e594c3":"code","b26b38c8":"code","5a89300a":"code","673ea461":"code","642f40b0":"markdown","67afccd7":"markdown","a477f7e6":"markdown","e44312d9":"markdown","b3cb3809":"markdown"},"source":{"1c0d781b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport random\nimport datetime\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\nimport functools\nfrom sklearn.model_selection import train_test_split\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# Any results you write to the current directory are saved as output.","56f65340":"tf.enable_eager_execution()","80b388e8":"TRAIN_CSV_PATH = '\/kaggle\/input\/street-view-getting-started-with-julia\/trainLabels.csv'\nTRAIN_IMGS_BASE_PATH = '\/kaggle\/input\/street-view-getting-started-with-julia\/trainresized\/trainResized\/'\nTEST_IMGS_BASE_PATH = '\/kaggle\/input\/street-view-getting-started-with-julia\/testresized\/testResized\/'\nBATH_SIZE = 256","8f976034":"train_data = pd.read_csv(TRAIN_CSV_PATH)","a47d93c9":"train_data.head()","4af07377":"LABELS = train_data['Class']\nUNIQUE_LABELS = list(set(LABELS))","ebcfa084":"LABEL_IDX = [UNIQUE_LABELS.index(l) for l in LABELS]\nLABEL_IDX = np.array(LABEL_IDX, dtype=np.float32)\ntrain_data['label'] = LABEL_IDX","470e72ac":"train_data.drop([283, 2289, 3135], inplace=True)\ntrain_data.reset_index(inplace=True)","c2a85d3f":"random_id = random.choice(train_data['ID'].values)\nsample_img = TRAIN_IMGS_BASE_PATH + str(random_id) + '.Bmp'","001f50e0":"!find {sample_img}","4088def4":"img_cnt = tf.read_file(filename=sample_img)\nimg = tf.io.decode_bmp(img_cnt, channels=3)\nprint(img.shape)\nplt.imshow(img)\nplt.title(LABELS[random_id-1])\nprint(LABEL_IDX[random_id - 1])\nprint(UNIQUE_LABELS.index(LABELS[random_id-1]))","135307d3":"train_data['img'] = [TRAIN_IMGS_BASE_PATH + str(id) + '.Bmp' for id in train_data['ID'].values]","ad069d43":"train_data.head()","2477ee64":"def transform_img(img, label=None):\n    img_cnt = tf.read_file(img)\n    img_cnt = tf.io.decode_bmp(img_cnt, channels=3)\n#     img_cnt = tf.keras.applications.resnet50.preprocess_input(img_cnt)\n    img_cnt \/= 255\n#     mean = tf.math.reduce_mean(img_cnt)\n#     std = tf.math.reduce_std(img_cnt)\n#     img_cnt = (img_cnt - std) \/ mean\n    return img_cnt, label","eb9e91d2":"def get_dataset(imgs, labels=None):\n    dataset = (\n        tf.data.Dataset.from_tensor_slices((imgs, labels))\n        .shuffle(len(imgs))\n        .map(transform_img)\n        .batch(BATH_SIZE)\n        .repeat()\n        .prefetch(1)\n    )\n    iterator = dataset.make_one_shot_iterator()\n    return iterator","7098d79c":"X_train, X_test, y_train, y_test = train_test_split(train_data['img'], train_data['label'], test_size=0.2, random_state=42)","b662f48d":"steps_per_epoch, validation_steps = X_train.shape[0]\/BATH_SIZE, X_test.shape[0]\/BATH_SIZE","0a210fcb":"validation_steps","c05d9c2c":"train_iter = get_dataset(X_train, y_train)","0e2e628b":"validation_iter = get_dataset(X_test, y_test)","b505f94f":"fig = plt.figure()\nfor i in range(1, 5):\n    plt.subplot(5, 5, i)\n    imgs, lbs = train_iter.get_next()\n#     print(imgs.numpy().shape)\n#     print(lbs.numpy().shape)\n    plt.imshow(imgs[3])\n    plt.title(UNIQUE_LABELS[int(lbs[3])])\n    \nplt.show()","df0e4933":"Activation = 'elu'\nInput = tf.keras.layers.Input\nConv2D = functools.partial(\n        tf.keras.layers.Conv2D,\n        activation=Activation,\n        padding='same'\n        )\nDense = functools.partial(\n        tf.keras.layers.Dense\n        )\nDropout = tf.keras.layers.Dropout\nAvgpool = tf.keras.layers.AveragePooling2D\nMaxPool2D = tf.keras.layers.MaxPool2D\nBatchNorm = tf.keras.layers.BatchNormalization\nFlatten = tf.keras.layers.Flatten","88953f14":"def get_model(outputs_shape):\n    input = Input(shape=(20, 20, 3,))\n    conv_1 = Conv2D(16, (2, 2))(input)\n    conv_2 = Conv2D(16, (2, 2))(conv_1)\n    conv_3 = Conv2D(32, (3, 3))(conv_2)\n    avg_1 = Avgpool((2, 2))(conv_2)\n    batch_norm_2 = BatchNorm()(conv_2)\n    \n    conv_3 = Conv2D(64, (3, 3))(batch_norm_2)\n    conv_4 = Conv2D(64, (3, 3))(conv_3)\n#     avg_2 = Avgpool((2, 2))(conv_4)\n    batch_norm_4 =  BatchNorm()(conv_4)\n    \n    conv_5 = Conv2D(32, (3, 3))(batch_norm_4)\n    conv_6 = Conv2D(32, (5, 5))(conv_5)\n    dropout_1 = Dropout(0.3)(conv_6)\n    batch_norm_6 =  BatchNorm()(dropout_1)\n    \n    conv_7 = Conv2D(16, (5, 5))(batch_norm_6)\n    conv_8 = Conv2D(16, (5, 5))(conv_7)\n    batch_norm_7 =  BatchNorm()(conv_8)\n    \n    flat_1 = Flatten()(batch_norm_7)\n    dense_1 = Dense(512, activation=Activation)(flat_1)\n    outputs = Dense(outputs_shape, activation='softmax')(dense_1)\n    \n    model = tf.keras.Model(input, outputs)\n    model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy' ,metrics=['accuracy'])\n    model.summary()\n    return model","8572e9f1":"unique_labels_count = len(list(set(LABELS)))\nprint(unique_labels_count)\nmodel = get_model(unique_labels_count)","ac82e789":"logdir = os.path.join(\"\/tmp\/logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(filepath='.\/weights.hdf5', verbose=1, save_best_only=True),\n    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n    tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)\n]","7bc0a033":"history = model.fit(train_iter, steps_per_epoch=20, epochs=50, validation_data=validation_iter, validation_steps=5, callbacks=callbacks)","5d5cdfed":"test_imgs = []\nfor dirname, _, filenames in os.walk(TEST_IMGS_BASE_PATH):\n    for filename in filenames:\n        test_imgs.append(os.path.join(dirname, filename))\nprint(test_imgs[:5])\ntest_imgs = np.array(test_imgs)","2814a366":"len(test_imgs)\/256","b0077c57":"grey_imgs = []\nfor i, img in enumerate(test_imgs):\n    try:\n        img_cnt = tf.read_file(img)\n        img_cnt = tf.image.decode_bmp(img_cnt, channels=3)\n    except:\n        print(i)\n        grey_imgs.append(i)","25e594c3":"test_imgs = np.delete(test_imgs, grey_imgs)","b26b38c8":"test_pipeline = get_dataset(test_imgs, tf.zeros(len(test_imgs)))","5a89300a":"predictions = model.predict(test_pipeline, steps=25)","673ea461":"rand_img = random.choice(range(len(test_imgs)))\nimg_cnt = tf.read_file(test_imgs[rand_img])\nimg_cnt = tf.image.decode_bmp(img_cnt)\nplt.imshow(img_cnt)\nplt.title(UNIQUE_LABELS[np.argmax(predictions[rand_img])])","642f40b0":"## Inferance","67afccd7":"## Tf Model","a477f7e6":"## Remove grey images","e44312d9":"## Training","b3cb3809":"## Data pipeline"}}