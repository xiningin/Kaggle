{"cell_type":{"090a98a0":"code","a7653e7a":"code","52e69fce":"code","6d38edd2":"code","b7b7add9":"code","56c554eb":"code","6ae80c8a":"code","3c2481af":"code","bed22848":"code","7969d3d8":"code","470eece5":"code","5038f146":"code","2a25b1e1":"code","ae987464":"code","e318287b":"markdown","51ab4f22":"markdown","e8694625":"markdown","a1b7e74b":"markdown","36a50bba":"markdown","1c9ae5bf":"markdown","d2a32c82":"markdown","490c8f6c":"markdown","4e0cfe0c":"markdown","1d7f58fd":"markdown","e126304b":"markdown","2890d7a8":"markdown","fb6c8ed2":"markdown","a0a3dfcb":"markdown","e604d5fc":"markdown","778e9ed4":"markdown","044210b4":"markdown","3a09b729":"markdown","70b89a24":"markdown","d4795cee":"markdown"},"source":{"090a98a0":"import os\nimport numpy as np\nimport pandas as pd\nimport librosa as lb\nimport IPython.display as ipd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential, layers\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom tqdm import tqdm","a7653e7a":"metadata = pd.read_csv('..\/input\/urbansound8k\/UrbanSound8K.csv')\nprint(metadata.shape)\nmetadata.head()","52e69fce":"classes = metadata.groupby('classID')['class'].unique()\nclasses","6d38edd2":"def feature_extractor(path):\n    data, simple_rate = lb.load(path)\n    data = lb.feature.mfcc(data,n_mfcc=128)\n    data = np.mean(data,axis=1)\n    return data","b7b7add9":"x, y = [], []\nfor i,rows in tqdm(metadata.iterrows()):\n    path = '..\/input\/urbansound8k\/' + 'fold' + str(rows['fold']) + '\/' + str(rows['slice_file_name'])\n    x.append(feature_extractor(path))\n    y.append(rows['classID'])\nx = np.array(x)\ny = np.array(y)\nx.shape, y.shape","56c554eb":"y = to_categorical(y)\ny.shape","6ae80c8a":"xtrainval, xtest, ytrainval, ytest = train_test_split(x,y,test_size=0.1,stratify=y,random_state=387)\nxtrain, xvalid, ytrain, yvalid = train_test_split(xtrainval,ytrainval,test_size=0.2,stratify=ytrainval,random_state=387)\nprint('\\nNumber of samples for Train set :',xtrain.shape[0])\nprint('Number of samples for Validation set :',xvalid.shape[0])\nprint('Number of samples for Test set :',xtest.shape[0])","3c2481af":"model = Sequential(\n                        [\n                            layers.Dense(1000,activation='relu',input_shape=(128,)),\n                            layers.Dense(750,activation='relu'),\n                            layers.Dense(500,activation='relu'),\n                            layers.Dense(250,activation='relu'),\n                            layers.Dense(100,activation='relu'),\n                            layers.Dense(50,activation='relu'),\n                            layers.Dense(10,activation='softmax')\n                        ]\n                   )\nmodel.summary()","bed22848":"model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\ntraining = model.fit(xtrain,ytrain,validation_data=(xvalid,yvalid),epochs=20)","7969d3d8":"train_hist = pd.DataFrame(training.history)\ntrain_hist","470eece5":"plt.figure(figsize=(20,8))\nplt.plot(train_hist[['loss','val_loss']])\nplt.legend(['loss','val_loss'])\nplt.title('Loss Per Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()\n\nplt.figure(figsize=(20,8))\nplt.plot(train_hist[['accuracy','val_accuracy']])\nplt.legend(['accuracy','val_accuracy'])\nplt.title('Accuracy Per Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.show()","5038f146":"ytrue = np.argmax(ytest,axis=1)\nypred = np.argmax(model.predict(xtest),axis=1)\nprint('\\nConfusion Matrix :\\n\\n')\nprint(confusion_matrix(ytrue,ypred))\nprint('\\n\\nClassification Report : \\n\\n',classification_report(ytrue,ypred))","2a25b1e1":"def predict(path):\n    audio = np.array([feature_extractor(path)])\n    classid = np.argmax(model.predict(audio)[0])\n    print('Class predicted :',classes[classid][0],'\\n\\n')\n    return ipd.Audio(path)","ae987464":"predict('..\/input\/urbansound8k\/fold6\/104327-2-0-26.wav')","e318287b":"#### One Hot Transformation","51ab4f22":"#### Train, Test and validation Split","e8694625":"#### Visualizing Training History","a1b7e74b":"#### Training and Compilation of the model","36a50bba":"#### A function that extract and returns numeric features from audio file","1c9ae5bf":"#### Artificial Neural Network Model Building","d2a32c82":"#### Audio Classes","490c8f6c":"#### The final Prediction function that takes the audio path and returns the predicted class along with audio","4e0cfe0c":"#### Testing the Prediction Function on a Audio file","1d7f58fd":"# <center> **Audio Classification**","e126304b":">","2890d7a8":"#### Training History","fb6c8ed2":"#### Importing all necessary libraries","a0a3dfcb":"#### Importing the metadata file contains the information about the audio dataset.","e604d5fc":"***","778e9ed4":"#### Model Performance Analysis on Test Data","044210b4":"# <center> **Thank You**","3a09b729":"### **AIM : Aiming to develop a Neural Network Model that can predict or classify a sound or audio that belongs to which class.**","70b89a24":"#### Extracting Features from Audio files and preparing the dataset","d4795cee":"<div align='center'><img src='https:\/\/production-media.paperswithcode.com\/datasets\/UrbanSound8K-0000003722-02faef06.jpg'><\/div>"}}