{"cell_type":{"95ac2d0b":"code","61594d40":"code","04e08451":"code","5eb53705":"code","87178124":"code","bb29c946":"markdown","b6a8afa9":"markdown","f3e75805":"markdown","1c38c05a":"markdown","652efdc9":"markdown","1fc9d402":"markdown","f79ca951":"markdown"},"source":{"95ac2d0b":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import special\n\nplt.figure(figsize=(8,5))\ncolor = ['r', 'g', 'b']\ncolor_index = 0\nmu = [0]\nstd = [0.25, 0.5, 1]\n\nfor i in range(len(mu)):\n    for j in range(len(std)):\n        s = np.random.lognormal(mu[i], std[j], 100000)\n        count, bins, ignored = plt.hist(s, 100, ec='k', density=True, alpha=0.0)\n        x = np.linspace(min(bins), max(bins), 10000)\n        pdf = (np.exp(-(np.log(x) - mu[i])**2 \/ (2 * std[j]**2)) \/ (x * std[j] * np.sqrt(2 * np.pi)))\n        plt.plot(x, pdf, linewidth=2, color=color[color_index], label=\"(mu = \"+ str(mu[i]) + \", std = \"+ str(std[j]) + \")\")\n        color_index = color_index + 1\n        \nplt.legend(loc=\"upper right\")\nplt.axis([0, 4.5, 0, 2])\nplt.show()","61594d40":"import numpy as np\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\nfrom IPython.display import display\n\nmu = widgets.BoundedFloatText(\n    value=1,\n    min=0,\n    max=30.0,\n    step=0.01,\n    description='Mean',\n    disabled=False\n)\nsigma = widgets.BoundedFloatText(\n    value=0.5,\n    min=0,\n    max=1.0,\n    step=0.01,\n    description='Sigma',\n    disabled=False\n)\n\ndef update_plot(mu, sigma):\n    \"\"\"\n    This function is linked to the sliders and \n    it replots the log-normal when the sliders \n    are changed.\n    \"\"\"\n    if sigma == 0:\n        print(\"Divide by zero error. Look at the equation for the PDF in the next section to better understand this error.\")\n        return\n    # Graph settings\n    plt.figure(figsize=(8,5))\n    plt.title('Log-normal graph:')\n    \n    s = np.random.lognormal(mu, sigma, 100000)\n    count, bins, ignored = plt.hist(s, 100, ec='k', density=True, alpha=0.0)\n    x = np.linspace(min(bins), max(bins), 1000)\n    pdf_ca = (np.exp(-(np.log(x) - mu)**2 \/ (2 * sigma**2)) \/ (x * sigma * np.sqrt(2 * np.pi)))\n    plt.plot(x, pdf_ca, linewidth=2, color='m', label=\"log-normal\")\n    plt.ylabel('PDF',fontsize=9)\n    plt.xlabel('x',fontsize=9)\n    plt.legend(loc=\"upper right\")\n    plt.show()\n\nwidgets.interactive(update_plot, mu=mu, sigma=sigma)","04e08451":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import special\n\nplt.figure(figsize=(8,5))\n\n### PDF ###\n\n# Normal: \nmu, sigma = 37055.25472, 19450.17473\ns = np.random.normal(mu, sigma, 100000)\ncount, bins, ignored = plt.hist(s, 100, ec='k', density=True, alpha=0.0)\nx = np.linspace(min(bins), max(bins), 10000)\npdf_ca = (np.exp(-0.5*(x - mu)**2 \/ (sigma**2)) \/ (sigma * np.sqrt(2 * np.pi)))\nplt.plot(x, pdf_ca, linewidth=2, color='g', label=\"normal\")\n\n# Log Normal:\nln_mu, ln_sigma = 10.3984899, 0.493306287\ns = np.random.lognormal(ln_mu, ln_sigma, 100000)\ncount, bins, ignored = plt.hist(s, 100, ec='k', density=True, alpha=0.0)\nx = np.linspace(min(bins), max(bins), 1000)\npdf_ca = (np.exp(-(np.log(x) - ln_mu)**2 \/ (2 * ln_sigma**2)) \/ (x * ln_sigma * np.sqrt(2 * np.pi)))\nplt.plot(x, pdf_ca, linewidth=2, color='m', label=\"log-normal\")\n\n# Graph settings\nplt.title('Probablity Density Function')\nplt.ylabel('PDF',fontsize=9)\nplt.xlabel('x',fontsize=9)\nplt.legend(loc=\"upper right\")\nplt.show()","5eb53705":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import special\n\nplt.figure(figsize=(8,5))\n\n### CDF ###\n\n\n# Normal: \nmu, sigma = 37055.25472, 19450.17473\ns = np.random.normal(mu, sigma, 1000)\ncount, bins, ignored = plt.hist(s, 100, ec='k', density=True)\nx = np.linspace(min(bins), 200000, 1000)\ncdf = (1+special.erf((x-mu)\/np.sqrt(2*sigma**2)))\/2\nplt.plot(x, cdf, linewidth=2, color='g', label=\"normal\")\n\n# Log Normal:\nln_mu, ln_sigma = 10.3984899, 0.493306287\ns = np.random.lognormal(ln_mu, ln_sigma, 1000)\ncount, bins, ignored = plt.hist(s, 100, ec='k', density=True)\nx = np.linspace(min(bins), 200000, 1000)\ncdf = (1+special.erf((np.log(x)-ln_mu)\/(np.sqrt(2)*ln_sigma)))\/2\nplt.plot(x, cdf, linewidth=2, color='m', label=\"log-normal\")\n\n# Graph settings\nplt.title('Cummulative Density Function')\nplt.ylabel('CDF',fontsize=9)\nplt.xlabel('x',fontsize=9)\nplt.legend(loc=\"upper left\")\nplt.show()","87178124":"import numpy as np\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(13,8))\n\n# Data collected from World Bank.\ncolor = ['r', 'k', 'gray', 'b', 'c', 'g', 'm', 'hotpink', 'lime', 'y']\ncountries = ['Canada', 'Chile', 'Finland', 'South Korea', 'United States', 'United Kingdom', 'Mexico', 'Italy', 'Germany']\nmu_countries = [10.3984899, 9.14558636, 10.24047662, 10.04203869, 10.48117837, 10.17196315, 8.781295311, 9.843113525, 10.25328085]\nsigma_countries = [0.493306287, 0.773320443, 0.472658046, 0.4850775, 0.611671832, 0.505692298, 0.702435424, 0.540310432, 0.472658046]\n\nfor i in range(9):\n    s = np.random.lognormal(mu_countries[i], sigma_countries[i], 10000)\n    count, bins, ignored = plt.hist(s, 100, ec='k', density=True, alpha=0.0)\n    x = np.linspace(min(bins), 80000, 100)\n    pdf = (np.exp(-(np.log(x) - mu_countries[i])**2 \/ (2 * sigma_countries[i]**2)) \/ (x * sigma_countries[i] * np.sqrt(2 * np.pi)))\n    plt.plot(x, pdf, linewidth=2.5, color=color[i], label=countries[i])\n\n# Graph settings\nplt.title('Wealth distribution graph')\nplt.ylabel('Probability Density',fontsize=9)\nplt.xlabel('Household Income',fontsize=9)\nplt.tick_params(axis='both',labelsize=9)\nplt.axis([0, 80000, 0, 0.00012])\nplt.legend(loc='upper right')\nplt.show()","bb29c946":"<h2>Section 3: Applications <\/h2>\n\n<h3> Wealth Distribution <\/h3>\n<p>To better explain the log-normal we will start with the example of wealth distrbution in 9 industrilized countries. \n    \nBefore you go further, from the knowledge you have gained so far and the graph below, which country do you think has the greatest standard deviation? The graph of Mexico has a high amplitude but a shorter width of curve compared to Italy. What does that tell us about the wealth distribution in the respective countries?\n<\/p>","b6a8afa9":"To answer the questions above, the higher the peak of the graph the more population with that particular wealth. So for the case of Mexico, since the graph peaks much earlier and higher than the graph of Italy, the average wealth of most people in Mexico is lower than that of Italy. Italy also has a wider curve which tells us that the wealth in Italy is more distributed rather than few rich people controlling the wealth of the entire country.\n\nIn your opinion, which country seems to be best off?\n\nThe short analysis above shows us how powerful the visualizations of log-normal can be. Just by looking at the graphs above we can make assumptions about the mean wealth of the population and how distributed the wealth of the country is. \n\nDid you know that economic inequality leads to higher health and social problems and can greatly hinder economic growth and also results in a lower happiness and satisfaction rate in the over-all population? (Source:[Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Effects_of_economic_inequality))","f3e75805":"<h3>Cumulative Density Function (CDF) <\/h3>\n\nThe CDF of normal distribution,\n\n\\begin{equation}\n    F_Y(x) = \\frac{1}{2} + \\frac{1}{2}\\text{erf}\\Big(\\frac{x-\\mu}{\\sigma \\sqrt{2}}\\Big)\n\\end{equation}\n\nis simple compared to that of the log-normal\n\n\\begin{equation}\n    F_X(x) = \\frac{1}{2} + \\frac{1}{2}\\text{erf}\\Big(\\frac{\\ln x-\\mu}{\\sigma \\sqrt{2}}\\Big)\n\\end{equation}\n\nwith $x$ being replaced with $\\ln x$ and $erf$ refers to the error function.\n\nThe following is a graphical comparison of the CDF of the two distributions using the equations provided above:","1c38c05a":"You may use the interactive graph below to answer the questions provided in the section above (To use the interactive activity you will need to move to the \"copy and edit\" mode in Kaggle).\n\nWhat happens when you set the value of $\\sigma$ to 0. Can you figure out why you get that result? From the properties you have explored so far, can you think of any applications of the log-normal distribution?","652efdc9":"<h3>Answers: <\/h3>\n\nAns1: When $\\mu$ is constant and $\\sigma$ is increased the peak of the graph decreases. This is because the $\\sigma$ is inversely propotional to the probability density of $x$.\n<br>\n<br>\nAns2: $\\mu$ $>= 0$ and $\\sigma > 0$ (See PDF).\n<br>\n<br>\nAns3: When $\\sigma = 0$ the PDF = $\\infty$(See PDF equation). The applications of log-normal are mentioned in section 2.","1fc9d402":"<h1>MATH 3311 - The Log-Normal Distribution<\/h1>\n<p>\n    To use the interactive widgets, please click the \"Copy and Edit\" button on the top-right of the activity. Due to the limitations of Kaggle, the ipython widgets don't work in the Viewer mode. \n<\/p>\n<br>\n<h2>Section 1: Introduction <\/h2>\n<p>Log-normal distribution, also known as the Galton distribution, is a continuous distribution in which instead of the original variable $X$, the logarithm of the variable $X$ i.e., $Y$ has a normal distribution. The relationship between random variables $X$ and $Y$ can be described as follows:\n<p style=\"text-align:center;\">$\\ln X = Y$<\/p>\nJust like normal distribution, the log-normal is described using two parameters, $\\mu$ and $\\sigma$. \n<\/p>\n<p>\nBefore you move on further try answering the questions below:<br>\n<br>\n1. What happens when we keep the $\\sigma$ constant and change the $\\mu$?\n<br>\n2. What are the domains of the $\\sigma$ and $\\mu$?\n<br>\n<\/p>\n\nThe following is the plot of the lognormal probability density function for four values of $\\sigma$:","f79ca951":"<h2>Section 2: Normal vs Log-Normal <\/h2>\n<p>\n    To better understand the log-normal and it's properties, this section compares the log-normal distribution with the more familiar normal distribution. \n    <br>\n    <br>\n    The log-normal distribution has two important properties i.e., it is lower bounded by zero and it is skewed right, unlike a normal distribution graph which has zero skew and has both negative and positive values (See figure below). The skewness is important in detemining which distribution is more appropriate in determining the probability of the event occurring. Some popular applications of the log-normal are in areas like stock analysis, income distribution, reliability analysis and even in certain physiological measurements like weight and blood pressure, etc.\n    <br>\n    Following is a comparison of the $PDF$ and $CDF$ of normal and log-normal graphs to help visualize how they differ.\n<\/p>\n<br>\n<h3> Probability Density Function (PDF) <\/h3>\n\nThe PDF of log-normal is slightly dfferent from the normal distribution,\n\n\\begin{equation}\n    f_Y(x)= \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\n\\end{equation}\n\nwhere $\\sigma$ is the standard derivation and $\\mu$ is the mean of the normal distribution. Simply by taking the natural logarithm of $x$, the equation becomes log-normal:\n\n\\begin{equation}\n    f_X(x) = \\frac{1}{x\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{\\ln x-\\mu}{\\sigma})^2}\n\\end{equation}\n<br>\nThe $x$ in the denominator is a consequence of requiring the total cumulative probability to remain the same between the two functions. It is because of this equation that when you set $\\sigma$ to zero that the interactive graph above returns a \"divide by zero\" error.\n\nBelow is a comparison of the shapes of the normal vs log-normal using the PDF equations described above:   "}}