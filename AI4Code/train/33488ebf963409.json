{"cell_type":{"1a60098b":"code","09617a88":"code","6ef5735c":"code","ce75002c":"code","dbf476ba":"code","6bd5bbd2":"code","8c9d957e":"code","2db9ce0c":"code","9a252254":"code","8949750b":"code","5cb37c35":"code","4e88e92c":"markdown","9ad7241f":"markdown","47651cdb":"markdown","bd2f263c":"markdown","f9d054e6":"markdown","2e0c8a85":"markdown","b472f85b":"markdown","dd9a9d7e":"markdown","b509fb8c":"markdown","113d5406":"markdown","9a6bff31":"markdown","259d693b":"markdown"},"source":{"1a60098b":"import jax\nimport jax.numpy as np\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split","09617a88":"def get_data():\n    \"\"\"Create dataset for linear regression using 5 features. Set fix bias = 2.0\n    \n    Returns:\n        X: training data\n        X_test: testing data\n        y: train labels\n        y_test: test labels\n        coef: true weight matrix (coefficients) for the dataset\n    \"\"\"\n    # create our dataset. Set fix bias of 2.0 and return weights (coef=True)\n    X, y, coef = make_regression(n_features=5, coef=True, bias=2.0)\n    X, X_test, y, y_test = train_test_split(X, y)\n    return (X, X_test, y, y_test, coef)\n\ndef J(X, w, b, y):\n    \"\"\"Cost function for a linear regression. A forward pass of our model.\n\n    Args:\n        X: a features matrix.\n        w: weights (a column vector).\n        b: a bias.\n        y: a target vector.\n\n    Returns:\n        scalar: a cost of this solution.    \n    \"\"\"\n    y_hat = np.dot(X, w) + b # Predict values.\n    return ((y_hat - y)**2).mean() # Return cost.","6ef5735c":"def batch_gradient_descent(J, X, y):\n    \"\"\"Performs linear regression using batch gradient descent optimizer\n    \n    Args:\n        J: cost function\n        X: training data\n        y: training labels\n        \n    Returns:\n        params: the weights and bias after performing the optimization\n    \"\"\"\n    # Some configurations\n    LOG = False  # To print loss after every n epochs\n    lr = 0.05    # Learning rate\n    \n    # The weights and bias terms we will be computing\n    params = {\n        'w': np.zeros(X.shape[1:]),\n        'b': 0.\n    }\n    \n    # Define the gradient function w.r.t w and b\n    grad_W = jax.jit(jax.grad(J, argnums=1))    # argnums indicates which variable to differentiate with from the parameters list passed to the function\n    grad_B = jax.jit(jax.grad(J, argnums=2))\n\n    # Run once to compile JIT (Just In Time). The next uses of grad_W and grad_B will now be fast\n    grad_W(X, params['w'], params['b'], y)\n    grad_B(X, params['w'], params['b'], y)\n\n\n    for i in range(1000):\n        # Gradient w.r.t. argumnet index 1 i.e., w and parameter update\n        params['w'] -= lr * grad_W(X, params['w'], params['b'], y)\n        # Gradient w.r.t. argumnet index 2 i.e., b and parameter update\n        params['b'] -= lr * grad_B(X, params['w'], params['b'], y)\n\n        if LOG and i % 20 == 0:\n            print(J(X, params['w'], params['b'], y))\n    \n    return params\n\n            \nX, X_test, y, y_test, coef = get_data()\nparams = batch_gradient_descent(J, X, y)\n\nprint(\"True weights =\", coef)\nprint(\"Calculated weights =\", params['w'])\nprint(\"True bias = 2.0\\tCalculated bias = {:.6f}\".format(params['b']))\nprint(\"Test loss: {:.9f}\".format(J(X_test, params['w'], params['b'], y_test)))","ce75002c":"def momentum(J, X, y, gamma=0.95):\n    \"\"\"Performs linear regression using batch gradient descent + momentum\n    \n    Args:\n        J: cost function\n        X: training data\n        y: training labels\n        gamma: Decay parameter for vecloity sum\n        \n    Returns:\n        params: the weights and bias after performing the optimization\n    \"\"\"\n    # Some configurations\n    LOG = False\n    lr = 0.05    # Learning rate\n    \n    # The weights and bias terms we will be computing\n    params = {\n        'w': np.zeros(X.shape[1:]),\n        'b': 0.\n    }\n\n    # To keep track of velocity parameters\n    params_v = {\n        'wv': np.zeros(X.shape[1:]),\n        'bv': 0.\n    }\n\n    # Define the gradient function w.r.t w and b\n    grad_W = jax.jit(jax.grad(J, argnums=1))   # argnums indicates which variable to differentiate with from the parameters list passed to the function\n    grad_B = jax.jit(jax.grad(J, argnums=2))\n\n    # Run once to compile JIT (Just In Time). The next uses of grad_W and grad_B will now be fast\n    grad_W(X, params['w'], params['b'], y)\n    grad_B(X, params['w'], params['b'], y)\n    \n    for i in range(1000):\n        # Gradient w.r.t. argumnet index 1 i.e., w\n        grad_w = grad_W(X, params['w'], params['b'], y)\n        # Gradient w.r.t. argumnet index 2 i.e., b\n        grad_b = grad_B(X, params['w'], params['b'], y)\n\n        # Update velocity\n        params_v['wv'] = gamma * params_v['wv'] + grad_w\n        params_v['bv'] = gamma * params_v['bv'] + grad_b\n\n        # Parameter update\n        params['w'] -= lr * params_v['wv']\n        params['b'] -= lr * params_v['bv']\n\n        if LOG and i % 20 == 0:\n            print(J(X, params['w'], params['b'], y))\n            \n    return params\n\n\nX, X_test, y, y_test, coef = get_data()\nparams = momentum(J, X, y)\n\nprint(\"True weights =\", coef)\nprint(\"Calculated weights =\", params['w'])\nprint(\"True bias = 2.0\\tCalculated bias = {:.6f}\".format(params['b']))\nprint(\"Test loss: {:.9f}\".format(J(X_test, params['w'], params['b'], y_test)))","dbf476ba":"def nesterov_momentum(J, X, y, gamma=0.95):\n    \"\"\"Performs linear regression using batch nesterov momentum\n    \n    Args:\n        J: cost function\n        X: training data\n        y: training labels\n        gamm: Decay parameter for vecloity sum\n        \n    Returns:\n        params: the weights and bias after performing the optimization\n    \"\"\"\n    # Some configurations\n    LOG = False\n    lr = 0.05    # Learning rate\n    \n    # The weights and bias terms we will be computing\n    params = {\n        'w': np.zeros(X.shape[1:]),\n        'b': 0.\n    }\n\n    # To keep track of velocity parameters\n    params_v = {\n        'wv': np.zeros(X.shape[1:]),\n        'bv': 0.\n    }\n\n    # Define the gradient function w.r.t w and b\n    grad_W = jax.jit(jax.grad(J, argnums=1))   # argnums indicates which variable to differentiate with from the parameters list passed to the function\n    grad_B = jax.jit(jax.grad(J, argnums=2))\n\n    # Run once to compile JIT (Just In Time). The next uses of grad_W and grad_B will now be fast\n    grad_W(X, params['w'], params['b'], y)\n    grad_B(X, params['w'], params['b'], y)\n    \n    for i in range(1000):\n        # Gradient w.r.t. argumnet index 1 i.e., w\n        grad_w = grad_W(X, params['w'], params['b'], y)\n        # Gradient w.r.t. argumnet index 2 i.e., b\n        grad_b = grad_B(X, params['w'], params['b'], y)\n\n        # Update velocity\n        params_v['wv'] = gamma * params_v['wv'] - lr * grad_W(X, params['w'] + gamma * params_v['wv'], params['b'] + gamma * params_v['bv'], y)\n        params_v['bv'] = gamma * params_v['bv'] - lr * grad_B(X, params['w'] + gamma * params_v['wv'], params['b'] + gamma * params_v['bv'], y)\n\n        # Parameter update\n        params['w'] += params_v['wv']\n        params['b'] += params_v['bv']\n\n        if LOG and i % 20 == 0:\n            print(J(X, params['w'], params['b'], y))\n            \n    return params\n\n\nX, X_test, y, y_test, coef = get_data()\nparams = nesterov_momentum(J, X, y)\n\nprint(\"True weights =\", coef)\nprint(\"Calculated weights =\", params['w'])\nprint(\"True bias = 2.0\\tCalculated bias = {:.6f}\".format(params['b']))\nprint(\"Test loss: {:.9f}\".format(J(X_test, params['w'], params['b'], y_test)))","6bd5bbd2":"def adagrad(J, X, y):\n    \"\"\"Performs linear regression using adagrad\n    \n    Args:\n        J: cost function\n        X: training data\n        y: training labels\n        \n    Returns:\n        params: the weights and bias after performing the optimization\n    \"\"\"\n    # Some configurations\n    LOG = False\n    lr = 2    # Learning rate\n    e = 1e-7  # Epsilon value to prevent the fractions going to infinity when denominator is zero\n    \n    # The weights and bias terms we will be computing\n    params = {\n        'w': np.zeros(X.shape[1:]),\n        'b': 0.\n    }\n\n    # To keep track of running sum of squares of gradients\n    squared_grad = {\n        'w': np.zeros(X.shape[1:]),\n        'b': 0.\n    }\n\n    # Define the gradient function w.r.t w and b\n    grad_W = jax.jit(jax.grad(J, argnums=1))   # argnums indicates which variable to differentiate with from the parameters list passed to the function\n    grad_B = jax.jit(jax.grad(J, argnums=2))\n\n    # Run once to compile JIT (Just In Time). The next uses of grad_W and grad_B will now be fast\n    grad_W(X, params['w'], params['b'], y)\n    grad_B(X, params['w'], params['b'], y)\n    \n    for i in range(5000):\n        # Gradient w.r.t. argumnet index 1 i.e., w\n        grad_w = grad_W(X, params['w'], params['b'], y)\n        # Gradient w.r.t. argumnet index 2 i.e., b\n        grad_b = grad_B(X, params['w'], params['b'], y)\n\n        # Running sum of squares\n        squared_grad['w'] += grad_w * grad_w\n        squared_grad['b'] += grad_b * grad_b\n\n        # Parameter update\n        params['w'] -= (lr \/ (np.sqrt(squared_grad['w']) + e) * grad_w)\n        params['b'] -= (lr \/ (np.sqrt(squared_grad['b']) + e) * grad_b)\n\n        if LOG and i % 20 == 0:\n            print(J(X, params['w'], params['b'], y))\n            \n    return params\n\n\nX, X_test, y, y_test, coef = get_data()\nparams = adagrad(J, X, y)\n\nprint(\"True weights =\", coef)\nprint(\"Calculated weights =\", params['w'])\nprint(\"True bias = 2.0\\tCalculated bias = {:.6f}\".format(params['b']))\nprint(\"Test loss: {:.9f}\".format(J(X_test, params['w'], params['b'], y_test)))","8c9d957e":"def rmsprop(J, X, y, delta=0.95):\n    \"\"\"Performs linear regression using rmsprop\n    \n    Args:\n        J: cost function\n        X: training data\n        y: training labels\n        delta: decay parameter\n        \n    Returns:\n        params: the weights and bias after performing the optimization\n    \"\"\"\n    # Some configurations\n    LOG = False\n    lr = 0.1  # Learning rate\n    e = 1e-7  # Epsilon value to prevent the fractions going to infinity when denominator is zero\n    \n    # The weights and bias terms we will be computing\n    params = {\n        'w': np.zeros(X.shape[1:]),\n        'b': 0.\n    }\n\n    # To keep running sum of squares of gradients with decay\n    squared_grad = {\n        'w': np.zeros(X.shape[1:]),\n        'b': 0.\n    }\n\n    # Define the gradient function w.r.t w and b\n    grad_W = jax.jit(jax.grad(J, argnums=1))   # argnums indicates which variable to differentiate with from the parameters list passed to the function\n    grad_B = jax.jit(jax.grad(J, argnums=2))\n\n    # Run once to compile JIT (Just In Time). The next uses of grad_W and grad_B will now be fast\n    grad_W(X, params['w'], params['b'], y)\n    grad_B(X, params['w'], params['b'], y)\n    \n    for i in range(5000):\n        # Gradient w.r.t. argumnet index 1 i.e., w\n        grad_w = grad_W(X, params['w'], params['b'], y)\n        # Gradient w.r.t. argumnet index 2 i.e., b\n        grad_b = grad_B(X, params['w'], params['b'], y)\n\n        # Running decaying sum of squares of gradients\n        squared_grad['w'] = delta * squared_grad['w'] + (1 - delta) * grad_w * grad_w\n        squared_grad['b'] = delta * squared_grad['b'] + (1 - delta) * grad_b * grad_b\n\n        # Parameter update\n        params['w'] -= (lr \/ (np.sqrt(squared_grad['w']) + e)) * grad_w\n        params['b'] -= (lr \/ (np.sqrt(squared_grad['b']) + e)) * grad_b\n\n        if LOG and i % 20 == 0:\n            print(J(X, params['w'], params['b'], y))\n            \n    return params\n\n\nX, X_test, y, y_test, coef = get_data()\nparams = rmsprop(J, X, y)\n\nprint(\"True weights =\", coef)\nprint(\"Calculated weights =\", params['w'])\nprint(\"True bias = 2.0\\tCalculated bias = {:.6f}\".format(params['b']))\nprint(\"Test loss: {:.9f}\".format(J(X_test, params['w'], params['b'], y_test)))","2db9ce0c":"def adam(J, X, y, delta1=0.9, delta2=0.999):\n    \"\"\"Performs linear regression using adam\n    \n    Args:\n        J: cost function\n        X: training data\n        y: training labels\n        delta1: decay parameter 1\n        delta2: decay parameter 2\n        \n    Returns:\n        params: the weights and bias after performing the optimization\n    \"\"\"\n    # Some configurations\n    LOG = False\n    lr = 0.5  # Learning rate\n    e = 1e-7  # Epsilon value to prevent the fractions going to infinity when denominator is zero\n    \n    # The weights and bias terms we will be computing\n    params = {\n        'w': np.zeros(X.shape[1:]),\n        'b': 0.\n    }\n\n    # To keep track of velocity parameters\n    params_v = {\n        'wv': np.zeros(X.shape[1:]),\n        'bv': 0.\n    }\n\n    # To keep track of running sum of squares of gradients with decay\n    squared_grad = {\n        'w': np.zeros(X.shape[1:]),\n        'b': 0.\n    }\n\n    # Define the gradient function w.r.t w and b\n    grad_W = jax.jit(jax.grad(J, argnums=1))   # argnums indicates which variable to differentiate with from the parameters list passed to the function\n    grad_B = jax.jit(jax.grad(J, argnums=2))\n\n    # Run once to compile JIT (Just In Time). The next uses of grad_W and grad_B will now be fast\n    grad_W(X, params['w'], params['b'], y)\n    grad_B(X, params['w'], params['b'], y)\n    \n    for i in range(1000):\n        # Gradient w.r.t. argumnet index 1 i.e., w\n        grad_w = grad_W(X, params['w'], params['b'], y)\n        # Gradient w.r.t. argumnet index 2 i.e., b\n        grad_b = grad_B(X, params['w'], params['b'], y)\n\n        # Momements update\n        # Velocity\n        params_v['wv'] = delta1 * params_v['wv'] + (1 - delta1) * grad_w\n        params_v['bv'] = delta1 * params_v['bv'] + (1 - delta1) * grad_b\n        \n        # Square of gradients\n        squared_grad['w'] = delta2 * squared_grad['w'] + (1 - delta2) * (grad_w * grad_w)\n        squared_grad['b'] = delta2 * squared_grad['b'] + (1 - delta2) * (grad_b * grad_b)\n\n        # Bias correction\n        moment_w = params_v['wv'] \/ (1. - delta1**(i+1))\n        moment_b = params_v['bv'] \/ (1. - delta1**(i+1))\n\n        moment_squared_w = squared_grad['w'] \/ (1. - delta2**(i+1))\n        moment_squared_b = squared_grad['b'] \/ (1. - delta2**(i+1))\n\n        # Parameter update\n        params['w'] -= (lr \/ (np.sqrt(moment_squared_w) + e)) * moment_w\n        params['b'] -= (lr \/ (np.sqrt(moment_squared_b) + e)) * moment_b\n\n        if LOG and i % 20 == 0:\n            print(J(X, params['w'], params['b'], y))\n            \n    return params\n\n\nX, X_test, y, y_test, coef = get_data()\nparams = adam(J, X, y)\n\nprint(\"True weights =\", coef)\nprint(\"Calculated weights =\", params['w'])\nprint(\"True bias = 2.0\\tCalculated bias = {:.6f}\".format(params['b']))\nprint(\"Test loss: {:.9f}\".format(J(X_test, params['w'], params['b'], y_test)))","9a252254":"def adamax(J, X, y, delta1=0.9, delta2=0.999):\n    \"\"\"Performs linear regression using adamax\n    \n    Args:\n        J: cost function\n        X: training data\n        y: training labels\n        delta1: decay parameter 1\n        delta2: decay parameter 2\n        \n    Returns:\n        params: the weights and bias after performing the optimization\n    \"\"\"\n    # Some configurations\n    LOG = False\n    lr = 0.1  # Learning rate\n    e = 1e-7  # Epsilon value to prevent the fractions going to infinity when denominator is zero\n    \n    # The weights and bias terms we will be computing\n    params = {\n        'w': np.zeros(X.shape[1:]),\n        'b': 0.\n    }\n\n    # To keep track of velocity parameters\n    params_v = {\n        'wv': np.zeros(X.shape[1:]),\n        'bv': 0.\n    }\n\n    # Instead of sum of squares (similar to l2 norm), adamax uses l-infinity norm\n    infinity_grad = {\n        'w': np.zeros(X.shape[1:]),\n        'b': 0.\n    }\n\n    # Define the gradient function w.r.t w and b\n    grad_W = jax.jit(jax.grad(J, argnums=1))   # argnums indicates which variable to differentiate with from the parameters list passed to the function\n    grad_B = jax.jit(jax.grad(J, argnums=2))\n\n    # Run once to compile JIT (Just In Time). The next uses of grad_W and grad_B will now be fast\n    grad_W(X, params['w'], params['b'], y)\n    grad_B(X, params['w'], params['b'], y)\n    \n    for i in range(1000):\n        # Gradient w.r.t. argumnet index 1 i.e., w\n        grad_w = grad_W(X, params['w'], params['b'], y)\n        # Gradient w.r.t. argumnet index 2 i.e., b\n        grad_b = grad_B(X, params['w'], params['b'], y)\n\n        # Momements update\n        params_v['wv'] = delta1 * params_v['wv'] + (1 - delta1) * grad_w\n        params_v['bv'] = delta1 * params_v['bv'] + (1 - delta1) * grad_b\n\n        infinity_grad['w'] = np.maximum(delta2 * infinity_grad['w'], np.abs(grad_w))\n        infinity_grad['b'] = np.maximum(delta2 * infinity_grad['b'], np.abs(grad_b))\n\n        # Bias correction\n        moment_w = params_v['wv'] \/ (1. - delta1**(i+1))\n        moment_b = params_v['bv'] \/ (1. - delta1**(i+1))\n\n\n        params['w'] -= (lr \/ (np.sqrt(infinity_grad['w']) + e)) * moment_w\n        params['b'] -= (lr \/ (np.sqrt(infinity_grad['b']) + e)) * moment_b\n\n        if LOG and i % 20 == 0:\n            print(J(X, params['w'], params['b'], y))\n            \n    return params\n\n\nX, X_test, y, y_test, coef = get_data()\nparams = adamax(J, X, y)\n\nprint(\"True weights =\", coef)\nprint(\"Calculated weights =\", params['w'])\nprint(\"True bias = 2.0\\tCalculated bias = {:.6f}\".format(params['b']))\nprint(\"Test loss: {:.9f}\".format(J(X_test, params['w'], params['b'], y_test)))","8949750b":"def nadam(J, X, y, delta1=0.9, delta2=0.999):\n    \"\"\"Performs linear regression using nadam\n    \n    Args:\n        J: cost function\n        X: training data\n        y: training labels\n        delta1: decay parameter 1\n        delta2: decay parameter 2\n        \n    Returns:\n        params: the weights and bias after performing the optimization\n    \"\"\"\n    # Some configurations\n    LOG = False\n    lr = 0.5  # Learning rate\n    e = 1e-7  # Epsilon value to prevent the fractions going to infinity when denominator is zero\n    \n    # The weights and bias terms we will be computing\n    params = {\n        'w': np.zeros(X.shape[1:]),\n        'b': 0.\n    }\n\n    # To keep track of velocity parameters\n    params_v = {\n        'wv': np.zeros(X.shape[1:]),\n        'bv': 0.\n    }\n\n    # To keep running sum of squares of gradients with decay\n    squared_grad = {\n        'w': np.zeros(X.shape[1:]),\n        'b': 0.\n    }\n\n    # Define the gradient function w.r.t w and b\n    grad_W = jax.jit(jax.grad(J, argnums=1))   # argnums indicates which variable to differentiate with from the parameters list passed to the function\n    grad_B = jax.jit(jax.grad(J, argnums=2))\n\n    # Run once to compile JIT (Just In Time). The next uses of grad_W and grad_B will now be fast\n    grad_W(X, params['w'], params['b'], y)\n    grad_B(X, params['w'], params['b'], y)\n    \n    for i in range(1000):\n        # Gradient w.r.t. argumnet index 1 i.e., w\n        grad_w = grad_W(X, params['w'], params['b'], y)\n        # Gradient w.r.t. argumnet index 2 i.e., b\n        grad_b = grad_B(X, params['w'], params['b'], y)\n\n        # Momements update\n        params_v['wv'] = delta1 * params_v['wv'] + (1 - delta1) * grad_w\n        params_v['bv'] = delta1 * params_v['bv'] + (1 - delta1) * grad_b\n\n        squared_grad['w'] = delta2 * squared_grad['w'] + (1 - delta2) * (grad_w * grad_w)\n        squared_grad['b'] = delta2 * squared_grad['b'] + (1 - delta2) * (grad_b * grad_b)\n\n        # Bias correction\n        moment_w = params_v['wv'] \/ (1. - delta1**(i+1))\n        moment_b = params_v['bv'] \/ (1. - delta1**(i+1))\n\n        moment_squared_w = squared_grad['w'] \/ (1. - delta2**(i+1))\n        moment_squared_b = squared_grad['b'] \/ (1. - delta2**(i+1))\n\n        # Parameter update\n        params['w'] -= (lr \/ (np.sqrt(moment_squared_w) + e)) * (delta1 * moment_w + (1 - delta1) * grad_w \/ (1 - delta1**(i+1)))\n        params['b'] -= (lr \/ (np.sqrt(moment_squared_b) + e)) * (delta1 * moment_b + (1 - delta1) * grad_b \/ (1 - delta1**(i+1)))\n\n        if LOG and i % 20 == 0:\n            print(J(X, params['w'], params['b'], y))\n            \n    return params\n\n\nX, X_test, y, y_test, coef = get_data()\nparams = nadam(J, X, y)\n\nprint(\"True weights =\", coef)\nprint(\"Calculated weights =\", params['w'])\nprint(\"True bias = 2.0\\tCalculated bias = {:.6f}\".format(params['b']))\nprint(\"Test loss: {:.9f}\".format(J(X_test, params['w'], params['b'], y_test)))","5cb37c35":"def adabelief(J, X, y, delta1=0.9, delta2=0.999):\n    \"\"\"Performs linear regression using adabelief\n    \n    Args:\n        J: cost function\n        X: training data\n        y: training labels\n        delta1: decay parameter 1\n        delta2: decay parameter 2\n        \n    Returns:\n        params: the weights and bias after performing the optimization\n    \"\"\"\n    # Some configurations\n    LOG = False\n    lr = 0.5  # Learning rate\n    e = 1e-7  # Epsilon value to prevent the fractions going to infinity when denominator is zero\n    \n    # The weights and bias terms we will be computing\n    params = {\n        'w': np.zeros(X.shape[1:]),\n        'b': 0.\n    }\n\n    # To keep track of velocity parameters\n    params_v = {\n        'wv': np.zeros(X.shape[1:]),\n        'bv': 0.\n    }\n\n    # To keep track of running variance of gradients\n    var_grad = {\n        'w': np.zeros(X.shape[1:]),\n        'b': 0.\n    }\n\n    # Define the gradient function w.r.t w and b\n    grad_W = jax.jit(jax.grad(J, argnums=1))   # argnums indicates which variable to differentiate with from the parameters list passed to the function\n    grad_B = jax.jit(jax.grad(J, argnums=2))\n\n    # Run once to compile JIT (Just In Time). The next uses of grad_W and grad_B will now be fast\n    grad_W(X, params['w'], params['b'], y)\n    grad_B(X, params['w'], params['b'], y)\n    \n    for i in range(1000):\n        # Gradient w.r.t. argumnet index 1 i.e., w\n        grad_w = grad_W(X, params['w'], params['b'], y)\n        # Gradient w.r.t. argumnet index 2 i.e., b\n        grad_b = grad_B(X, params['w'], params['b'], y)\n\n        # Momements update\n        params_v['wv'] = delta1 * params_v['wv'] + (1 - delta1) * grad_w\n        params_v['bv'] = delta1 * params_v['bv'] + (1 - delta1) * grad_b\n\n        var_grad['w'] = delta2 * var_grad['w'] + (1 - delta2) * (grad_w - params_v['wv']) * (grad_w - params_v['wv'])\n        var_grad['b'] = delta2 * var_grad['b'] + (1 - delta2) * (grad_b - params_v['bv']) * (grad_b - params_v['bv'])\n\n        # Bias correction\n        moment_w = params_v['wv'] \/ (1. - delta1**(i+1))\n        moment_b = params_v['bv'] \/ (1. - delta1**(i+1))\n\n        moment_var_w = var_grad['w'] \/ (1. - delta2**(i+1))\n        moment_var_b = var_grad['b'] \/ (1. - delta2**(i+1))\n\n        # Parameter update\n        params['w'] -= (lr \/ (np.sqrt(moment_var_w) + e)) * moment_w\n        params['b'] -= (lr \/ (np.sqrt(moment_var_b) + e)) * moment_b\n\n        if LOG and i % 20 == 0:\n            print(J(X, params['w'], params['b'], y))\n            \n    return params\n\n\nX, X_test, y, y_test, coef = get_data()\nparams = adabelief(J, X, y)\n\nprint(\"True weights =\", coef)\nprint(\"Calculated weights =\", params['w'])\nprint(\"True bias = 2.0\\tCalculated bias = {:.6f}\".format(params['b']))\nprint(\"Test loss: {:.9f}\".format(J(X_test, params['w'], params['b'], y_test)))","4e88e92c":"# Nadam","9ad7241f":"# Libraries","47651cdb":"# RMSprop","bd2f263c":"# Helper Functions","f9d054e6":"# Batch Gradient Descent","2e0c8a85":"# Adam","b472f85b":"# Adamax","dd9a9d7e":"# Adagrad","b509fb8c":"# Gradient Descent + Momentum","113d5406":"# Adabelief","9a6bff31":"# Nesterov accelerated momentum","259d693b":"# Optimizers in ML using JAX\n\n> Optimization refers to the process of minimizing the loss function by systematically updating the network weights.\n\nIn this notebook, I implement a few popular optimizers from scratch for a simple model i.e., Linear Regression on a dataset of 5 features. The goal of this notebook was to understand how these optimizers work under the hood and try to do a toy implementation myself. I also use a bit of JAX magic to perform the differentiation of the loss function w.r.t to the Weights and the Bias without explicitly writing their derivatives as a separate function. This can help to generalize this notebook for other types of loss functions as well.\n\nThe optimizers I have implemented are - \n* Batch Gradient Descent\n* Batch Gradient Descent + Momentum\n* Nesterov Accelerated Momentum\n* Adagrad\n* RMSprop\n* Adam\n* Adamax\n* Nadam\n* Adabelief\n\nReferences -\n* https:\/\/ruder.io\/optimizing-gradient-descent\/\n* https:\/\/theaisummer.com\/optimization\/"}}