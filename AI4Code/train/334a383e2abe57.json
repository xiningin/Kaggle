{"cell_type":{"a9e8cf12":"code","59e31de2":"code","35b6c7f5":"code","7eefdf4d":"code","1af5b16a":"code","5f5370e8":"code","45cffaf9":"code","fea3b26f":"code","69b7866f":"code","61c66c1e":"code","b92ec964":"code","497f2de9":"code","8bc9bd7a":"code","aaf08fbe":"code","f59aba51":"code","5659ce31":"code","581793fa":"code","5d881713":"code","0ab75e57":"code","af4c2582":"code","c802e7e8":"code","a68d5a7c":"code","ad748f9f":"code","5255da4f":"code","56410ae8":"code","35c6710d":"code","ef6652a2":"code","a3626e81":"code","d7202cc6":"code","c729eeed":"code","42e39880":"code","dc22ba78":"markdown"},"source":{"a9e8cf12":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","59e31de2":"# Loading libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","35b6c7f5":"# Reading data\ndata = pd.read_csv(r'..\/input\/customer-churn\/churn_modelling.csv',index_col=0)","7eefdf4d":"data.head()","1af5b16a":"data.shape","5f5370e8":"data.info()","45cffaf9":"data.nunique()","fea3b26f":"data.dtypes","69b7866f":"plt.figure(figsize=(14,7))\nplt.subplot(121)\nsns.countplot(x='Geography',hue='Exited',data=data)\n\nplt.subplot(122)\nsns.countplot(x='Gender',hue='Exited',data=data)\n\nplt.show()","61c66c1e":"sns.countplot(x='Exited',data=data)\nplt.show()","b92ec964":"data.drop(['RowNumber','CustomerId','Surname'],axis=1,inplace=True)","497f2de9":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nvar_mod = ['Gender','Geography']\nfor i in var_mod:\n    data[i] = le.fit_transform(data[i])","8bc9bd7a":"data.dtypes","aaf08fbe":"X = data.drop('Exited',axis=1)\ny=data['Exited']","f59aba51":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,stratify=data['Exited'],test_size=0.3)","5659ce31":"X_train.shape, X_test.shape","581793fa":"from sklearn.feature_selection import mutual_info_classif\n# determine the mutual information\nmutual_info = mutual_info_classif(X_train, y_train)\nmutual_info","5d881713":"mutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train.columns\nmutual_info.sort_values(ascending=False)","0ab75e57":"#let's plot the ordered mutual_info values per feature\nmutual_info.sort_values(ascending=False).plot.bar(figsize=(14, 7))\nplt.show()","af4c2582":"from sklearn.feature_selection import SelectKBest\n#we Will select the  top 5 important features\nsel_five_cols = SelectKBest(mutual_info_classif, k=5)\nsel_five_cols.fit(X_train, y_train)\nX_train.columns[sel_five_cols.get_support()]","c802e7e8":"X_train.drop(['Gender','CreditScore','EstimatedSalary','Balance','Tenure'],axis=1)\nX_test.drop(['Gender','CreditScore','EstimatedSalary','Balance','Tenure'],axis=1)","a68d5a7c":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","ad748f9f":"from keras.models import Sequential\nfrom keras.layers import Dense\n","5255da4f":"model = Sequential()\n# Adding the input layer and the first hidden layer\nmodel.add(Dense(units = 6, kernel_initializer='he_uniform', activation='relu', input_dim=X_train.shape[1]))\n# Adding the second hidden layer\nmodel.add(Dense(units = 6, kernel_initializer='he_uniform',activation='relu'))\n# Adding the output layer\nmodel.add(Dense(units=1, kernel_initializer='glorot_uniform',activation='sigmoid'))","56410ae8":"model.compile(optimizer='Adamax',loss='binary_crossentropy',metrics = ['accuracy'])","35c6710d":"model.summary()","ef6652a2":"model_history = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100,batch_size=10)","a3626e81":"pred = model.predict(X_test)","d7202cc6":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,pred.round())","c729eeed":"# summarize history for loss\nplt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","42e39880":"# summarize history for accuracy\nplt.plot(model_history.history['accuracy'])\nplt.plot(model_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","dc22ba78":"### Feature Selection"}}