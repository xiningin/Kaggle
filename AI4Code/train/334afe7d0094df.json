{"cell_type":{"40c8f4ff":"code","59a826ca":"code","e5d563b5":"code","c60cc2bd":"code","edd26117":"code","42baaf1d":"code","52254d80":"code","1c65749c":"code","2a1e15f9":"code","4e73c37f":"code","7a0adbeb":"code","3155fb4c":"markdown","9eabc831":"markdown"},"source":{"40c8f4ff":"import os\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n\nfrom scipy.stats import norm","59a826ca":"train = pd.read_csv('..\/input\/ghost-drift-and-outliers\/train_clean.csv')\ntest  = pd.read_csv('..\/input\/ghost-drift-and-outliers\/test_clean.csv')\norig_train = pd.read_csv('..\/input\/liverpool-ion-switching\/train.csv')","e5d563b5":"class ViterbiClassifier:\n    def __init__(self):\n        self._p_trans = None\n        self._p_signal = None\n    \n    def fit(self, x, y):\n        self._states = np.unique(y)\n        self._n_states = len(self._states)\n        \n        self._p_trans = self.markov_p_trans(y)\n        \n        self._dists = []\n        for s in np.arange(y.min(), y.max() + 1):\n            self._dists.append((np.mean(x[y == s]), np.std(x[y == s])))\n        \n        return self\n        \n    def predict(self, x, p_signal=None, proba=False):\n        if p_signal is None:\n            p_signal = self.markov_p_signal(x)\n\n        preds, probs = self.viterbi(self._p_trans, p_signal[self._states], x)\n        \n        if proba:\n            return probs\n        else:\n            return preds\n    \n    def markov_p_signal(self, signal):\n        p_signal = np.zeros((self._n_states, len(signal)))\n        for k, dist in enumerate(self._dists):\n            p_signal[k, :] = norm.pdf(signal, *dist)\n            \n        return p_signal\n    \n    def markov_p_trans(self, states):\n        # https:\/\/www.kaggle.com\/friedchips\/the-viterbi-algorithm-a-complete-solution\n        max_state = np.max(states)\n        states_next = np.roll(states, -1)\n        matrix = []\n        for i in range(max_state + 1):\n            current_row = np.histogram(states_next[states == i], bins=np.arange(max_state + 2))[0]\n            if np.sum(current_row) == 0: # if a state doesn't appear in states...\n                current_row = np.ones(max_state + 1) \/ (max_state + 1) # ...use uniform probability\n            else:\n                current_row = current_row \/ np.sum(current_row) # normalize to 1\n            matrix.append(current_row)\n        return np.array(matrix)\n    \n    def viterbi(self, p_trans, p_signal, signal):\n        # https:\/\/www.kaggle.com\/friedchips\/the-viterbi-algorithm-a-complete-solution\n        offset = 10**(-20) # added to values to avoid problems with log2(0)\n\n        p_trans_tlog  = np.transpose(np.log2(p_trans  + offset)) # p_trans, logarithm + transposed\n        p_signal_tlog = np.transpose(np.log2(p_signal + offset)) # p_signal, logarithm + transposed\n        \n        T1 = np.zeros(p_signal.shape)\n        T2 = np.zeros(p_signal.shape)\n\n        T1[:, 0] = p_signal_tlog[0, :]\n        T2[:, 0] = 0\n\n        for j in range(1, p_signal.shape[1]):\n            for i in range(len(p_trans)):\n                T1[i, j] = np.max(T1[:, j - 1] + p_trans_tlog[:, i] + p_signal_tlog[j, i])\n                T2[i, j] = np.argmax(T1[:, j - 1] + p_trans_tlog[:, i] + p_signal_tlog[j, i])\n        \n        x = np.empty(p_signal.shape[1], 'B')\n        x[-1] = np.argmax(T1[:, p_signal.shape[1] - 1])\n        for i in reversed(range(1, p_signal.shape[1])):\n            x[i - 1] = T2[x[i], i]\n    \n        return x, T1\n    \nclass PosteriorDecoder:\n    def __init__(self):\n        self._p_trans = None\n        self._p_signal = None\n    \n    def fit(self, x, y):\n        self._states = np.unique(y)\n        self._n_states = len(self._states)\n        \n        self._dists = []\n        for s in np.arange(y.min(), y.max() + 1):\n            self._dists.append((np.mean(x[y == s]), np.std(x[y == s])))\n        \n        self._p_trans = self.markov_p_trans(y)\n        \n        return self\n        \n    def predict(self, x, p_signal=None, proba=False):\n        if p_signal is None:\n            p_signal = self.markov_p_signal(x)\n        preds = self.posterior_decoding(self._p_trans, p_signal[self._states])\n        \n        if proba:\n            return probs\n        else:\n            return preds\n    \n    def markov_p_signal(self, signal):\n        p_signal = np.zeros((self._n_states, len(signal)))\n        for k, dist in enumerate(self._dists):\n            p_signal[k, :] = norm.pdf(signal, *dist)\n            \n        return p_signal\n    \n    def markov_p_trans(self, states):\n        # https:\/\/www.kaggle.com\/friedchips\/the-viterbi-algorithm-a-complete-solution\n        max_state = np.max(states)\n        states_next = np.roll(states, -1)\n        matrix = []\n        for i in range(max_state + 1):\n            current_row = np.histogram(states_next[states == i], bins=np.arange(max_state + 2))[0]\n            if np.sum(current_row) == 0: # if a state doesn't appear in states...\n                current_row = np.ones(max_state + 1) \/ (max_state + 1) # ...use uniform probability\n            else:\n                current_row = current_row \/ np.sum(current_row) # normalize to 1\n            matrix.append(current_row)\n        return np.array(matrix)\n    \n    def forward(self, p_trans, p_signal):\n        \"\"\"Calculate the probability of being in state `k` at time `t`, \n           given all previous observations `x_1 ... x_t`\"\"\"\n        T1 = np.zeros(p_signal.shape)\n        T1[:, 0] = p_signal[:, 0]\n        T1[:, 0] \/= np.sum(T1[:, 0])\n\n        for j in range(1, p_signal.shape[1]):\n            for i in range(len(p_trans)):\n                T1[i, j] = p_signal[i, j] * np.sum(T1[:, j - 1] * p_trans[i, :])\n            T1[:, j] \/= np.sum(T1[:, j])\n\n        return T1\n\n    def backward(self, p_trans, p_signal):\n        \"\"\"Calculate the probability of observing `x_{t + 1} ... x_n` if we \n           start in state `k` at time `t`.\"\"\"\n        T1 = np.zeros(p_signal.shape)\n        T1[:, -1] = p_signal[:, -1]\n        T1[:, -1] \/= np.sum(T1[:, -1])\n\n        for j in range(p_signal.shape[1] - 2, -1, -1):\n            for i in range(len(p_trans)):\n                T1[i, j] = np.sum(T1[:, j + 1] * p_trans[:, i] * p_signal[:, j + 1])\n            T1[:, j] \/= np.sum(T1[:, j])\n\n        return T1\n    \n    def posterior_decoding(self, p_trans, p_signal):\n        fwd = self.forward(p_trans, p_signal)\n        bwd = self.backward(p_trans, p_signal)\n\n        x = np.empty(p_signal.shape[1], 'B')\n        for i in range(p_signal.shape[1]):\n            x[i] = np.argmax(fwd[:, i] * bwd[:, i])\n\n        return x\n    ","c60cc2bd":"train['batch'] = (train['time'] - 0.0001) \/\/ 50\ncounts = train.groupby('batch').count()['time'].values\nmodels = [0, 0, 1, 2, 4, 3, 1, 2, 3, 4]\nblocks = [[], [], [], [], []]\ntotal = 0\nfor model, count in zip(models, counts):\n    blocks[model].extend(list(range(total, total + count)))\n    total += count\nprint([len(x) for x in blocks])","edd26117":"# train['block'] = np.NaN\n# for model, ix in enumerate(blocks):\n#     train.loc[ix, 'block'] = model\n# distributions = train.groupby(['block', 'open_channels'])['signal'].agg(['mean', 'std'])\n# distributions","42baaf1d":"true_state = train.open_channels.values\nsignal = train.signal.values","52254d80":"models = []\nviterbi_predictions = np.zeros(len(signal))\npos_dec_predictions = np.zeros(len(signal))\nfor i, ix in enumerate(blocks):\n    sub_signal = signal[ix]\n    \n    viterbi = ViterbiClassifier().fit(sub_signal, true_state[ix])\n    viterbi_predictions[ix] = viterbi.predict(sub_signal)\n    \n    decoder = PosteriorDecoder().fit(sub_signal, true_state[ix])\n    pos_dec_predictions[ix] = decoder.predict(sub_signal)\n    models.append(decoder)\n    \n    print('[Model #{}] || Pos. Dec.: F1 = {}'.format(\n        i, \n        f1_score(y_pred=viterbi_predictions[ix], y_true=true_state[ix], average='macro'),\n        f1_score(y_pred=pos_dec_predictions[ix], y_true=true_state[ix], average='macro')\n    ))\n    \n    print(classification_report(y_pred=pos_dec_predictions[ix], y_true=true_state[ix]))","1c65749c":"print(\"[Viterbi] Total Accuracy =\", accuracy_score(y_pred=viterbi_predictions, y_true=true_state))\nprint(\"[Viterbi] Total F1 (macro) =\", f1_score(y_pred=viterbi_predictions, y_true=true_state, average='macro'))\n\nprint(\"[Posterior Decoding] Total Accuracy =\", accuracy_score(y_pred=pos_dec_predictions, y_true=true_state))\nprint(\"[Posterior Decoding] Total F1 (macro) =\", f1_score(y_pred=pos_dec_predictions, y_true=true_state, average='macro'))\n\nprint(classification_report(y_pred=pos_dec_predictions, y_true=true_state))","2a1e15f9":"test_blocks = [\n    list(range(0, 100000)) + list(range(300000, 400000)) + list(range(800000, 900000)) + list(range(1000000, 2000000)),\n    list(range(400000, 500000)),\n    list(range(100000, 200000)) + list(range(900000, 1000000)),\n    list(range(200000, 300000)) + list(range(600000, 700000)),\n    list(range(500000, 600000)) + list(range(700000, 800000))\n]\n\n# Sanity check\nassert sum([len(x) for x in test_blocks]) == 2000000","4e73c37f":"df_subm = pd.read_csv(\"..\/input\/liverpool-ion-switching\/sample_submission.csv\")\nfor i, ix in enumerate(test_blocks):\n    df_subm.loc[ix, 'open_channels'] = models[i].predict(test.signal.values[ix])\ndf_subm.to_csv(\"forward_backward.csv\", float_format='%.4f', index=False)","7a0adbeb":"# Sanity check \n# https:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930\nplt.figure(figsize=(20,5))\nres = 1000; let = ['A','B','C','D','E','F','G','H','I','J']\nplt.plot(range(0,test.shape[0],res),df_subm.open_channels[0::res])\nfor i in range(5): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor i in range(21): plt.plot([i*100000,i*100000],[-5,12.5],'r:')\nfor k in range(4): plt.text(k*500000+250000,10,str(k+1),size=20)\nfor k in range(10): plt.text(k*100000+40000,7.5,let[k],size=16)\nplt.title('Test Data Predictions',size=16)\nplt.show()","3155fb4c":"# Viterbi & Posterior Decoding (collapsed)","9eabc831":"# Posterior Decoding (Forward-Backward Algorithm).\n\nIn this notebook, I will implement the [Forward-Backward Algorithm](https:\/\/en.wikipedia.org\/wiki\/Forward%E2%80%93backward_algorithm) (which is sometimes called Posterior Decoding as well), which is very similar to the [Viterbi Algorithm](https:\/\/en.wikipedia.org\/wiki\/Viterbi_algorithm), but it performs two passes. In a first, forward pass we try to estimate the probability of ending up in any particular state given the first t observations, $F_t = P(X_t\\ |\\ o_{1:t})$. In a second, backward pass we try to estimate the probability of observing the remaining observations, given a starting state, $B_t = P(o_{t+1:T}\\ |\\ X_t)$. We then calculate $P_t = F_tB_t,\\ \\forall t \\in \\{ 1, \\ldots, T \\}$. While Viterbi gives you the most likely sequence, posterior (Forward-Backward) gives most likely state at each position ([source](https:\/\/stats.stackexchange.com\/questions\/31119\/posterior-probability-vs-viterbi-algorithm)), which is closer related to the competition objective. I got the implementation details from this [source](https:\/\/cran.r-project.org\/web\/packages\/seqHMM\/vignettes\/seqHMM_algorithms.pdf).\n\nThe difference in performance with Viterbi is rather marginal, and the algorithm is a bit slower. I just share it here for completeness. Moreover, now you have two matrices of probabilities (`fwd` and `bwd`, as opposed to only `T1` of Viterbi). This could be even more features for your feature set.\n\n\n**Another very interesting idea that I have been experimenten with is not estimating `p_signal`, but instead using out-of-fold predictions of strong public notebooks. Unfortunately, I've had no luck with that approach so far. Let me know if you manage to get it working! Moreover, if you manage to improve my implementation either in terms of predictive performance or speed, I would love to hear how!**"}}