{"cell_type":{"b3d74a09":"code","24df85ae":"code","250183b7":"code","5f7a00b3":"code","2b794403":"code","c35cb480":"code","97b96f7e":"code","dace9f4a":"code","2f18a19f":"markdown","1cf8f1a0":"markdown","b73e07d2":"markdown","4e702859":"markdown","0a63ac4c":"markdown","862394cb":"markdown","bdf7dbca":"markdown","8b4290e8":"markdown","3bdd4f01":"markdown","99697406":"markdown","805719c0":"markdown"},"source":{"b3d74a09":"from numpy import dot, exp, array, append\n\ndef ReLU(x):\n    return x * (x > 0)\n\ndef Sigmoid(x):\n    return 1 \/ (1 + exp(-x))\n\n\ni =  array([[0.1, 0.4, 0.5]])\n\nw_ij = array([[0.1, 0.2, 0.4, 0.3],\n              [0.5, 0.4, 0.7, 0.9],\n              [0.2, 0.6, 0.3, 0.8]])\n\nw_jk = array([[0.2],\n              [0.3],\n              [0.6],\n              [0.1]])\n\nz_ij = dot(i, w_ij) #Calcular as entradas e pesos da camada oculta\na_ij = ReLU(z_ij) # Aplicando a ReLU\n\nz_jk = dot(a_ij, w_jk) #Ap\u00f3s a fun\u00e7\u00e3o de ativa\u00e7\u00e3o da camada de sa\u00edda\na_jk = Sigmoid(z_jk) #Aplicando a Sigmoidn \n\nprint (\"Sa\u00edda da RNA Feed Forward: \",a_jk)\n","24df85ae":"from numpy import log10\ndef crossentropyerror(a, y):\n    return - sum(y * log10(a) + (1 - y) * log10(1 - a))\n\nprint(\"Saida de nossa RNA:  \",a_jk)\ny = array([[1]])\nprint(\"Nosso Ground Truth:  \",y)\nprint(\"Cross Entropy Error: \",crossentropyerror(a_jk, y))","250183b7":"import matplotlib.pyplot as plt\ndef SigmoidD(x):\n    return Sigmoid(x) * (1 - Sigmoid(x))\nx = array(range(-10, 10))\nplt.plot(x, SigmoidD(x))\nplt.title(\"Derivada - sigm\u00f3ide\")\nplt.xlabel(\"x\")\nplt.ylabel(\"sigm\u00f3ide\")","5f7a00b3":"def ReLUD(x):\n    return 1. * (x > 0)\nx = array(range(-10, 10))\nplt.plot(x, ReLUD(x))\nplt.title(\"Derivada - ReLU\")\nplt.xlabel(\"x\")\nplt.ylabel(\"ReLU\")","2b794403":"print(\"Backpropagation da camada de sa\u00edda para a camada oculta.\\n\")\n\ndl_jk = -y\/a_jk + (1 - y)\/(1 - a_jk)\nprint(\"Derivada da Cross Entropy Loss em rela\u00e7\u00e3o a nossa saida: {0}\\n\".format(dl_jk))\n\nda_jk = SigmoidD(z_jk) \nprint(\"Derivada da Sign\u00f3ide de entrada (antes da fun\u00e7\u00e3o de ativa\u00e7\u00e3o) da camada de sa\u00edda: {0}\\n\".format(da_jk))\n\ndz_jk = a_ij \nprint(\"Derivada das entradas da camada oculta (antes da fun\u00e7\u00e3o de ativa\u00e7\u00e3o) \\nem rela\u00e7\u00e3o aos pesos da camada de sa\u00edda: {0}\\n\".format(dz_jk))\n\ngradient_jk = dot(dz_jk.T , dl_jk * da_jk) \nprint(\"Chain Rule: {0}\\n\".format(gradient_jk))\n\n\n","c35cb480":"print(\"Backpropagation da camada oculta para a camada de sa\u00edda.\\n\")\n\ndl_ij = dot(da_jk * dl_jk, w_jk.T) \nprint(\"Derivada da Cross Entropy Loss em rela\u00e7\u00e3o a entrada da camada oculta (ap\u00f3s a fun\u00e7\u00e3o de ativa\u00e7\u00e3o):\\n{0}\\n\".format(dl_ij))\n\nda_ij = ReLUD(z_ij)\nprint(\"Derivada da ReLU de entrada (antes da fun\u00e7\u00e3o de ativa\u00e7\u00e3o) da camada oculta:\\n{0}\\n\".format(da_ij))\n\ndz_ij = i\nprint(\"Derivada das entradas da camada oculta (antes da fun\u00e7\u00e3o de ativa\u00e7\u00e3o)\\ncom os pesos da camada oculta: {0}\\n\".format(dz_ij))\n\ngradient_ij = dot(dz_ij.T , dl_ij * da_ij)\nprint(\"Chain Rule:\\n{0}\".format(gradient_ij))\n\n","97b96f7e":"print(\"Novos pesos obtidos ap\u00f3s o processo do Backpropagation.\\n\")\nw_ij = w_ij - gradient_ij \nw_jk = w_jk - gradient_jk\nprint(\"w_ij:\\n{0}\\n\".format(w_ij))\nprint(\"w_jk:\\n{0}\\n\".format(w_jk))","dace9f4a":"print(\"Agora \u00e9 s\u00f3 prever a sa\u00edda com os novos pesos.\\n\")\n\n\ni = array([[0.1, 0.4, 0.5]])\nprint(\"i: {0}\\n\".format(i))\n\n\nw_ij = array([[ 0.10723859,  0.21085788,  0.42171576,  0.30361929],\n              [ 0.52895435,  0.44343152,  0.78686304,  0.91447717],\n              [ 0.23619293,  0.6542894,   0.4085788,   0.81809647]])\nprint(\"Novos pesos da camada oculta:\\n{0}\\n\".format(w_ij))\n\n \nw_jk = array([[ 0.3121981 ],\n              [ 0.47372609],\n              [ 0.77010679],\n              [ 0.38592418]])\nprint(\"Novos pessoas da camada de sa\u00edda:\\n{0}\\n\".format(w_jk))\n\n\nz_ij = dot(i, w_ij) \nprint(\"Calcular o produto escalar das entradas e dos pesos da camada oculta:\\n{0}\\n\".format(z_ij))\n\na_ij = ReLU(z_ij)\nprint(\"ReLU: {0}\\n\".format(a_ij))\n\nz_jk = dot(a_ij, w_jk) \nprint(\"Produto escalar ap\u00f3s ativa\u00e7\u00e3o: {0}\\n\".format(z_jk))\n\na_jk = Sigmoid(z_jk) \nprint(\"Sigmoid: {0}\\n\".format(a_jk))","2f18a19f":"\ud83d\udccc Em resumo, \u00e9 assim que um <strong>Forward Propagation<\/strong> trabalha e como nossa RNA gera suas previs\u00f5es. A etapa de propaga\u00e7\u00e3o direta \u00e9 menos tensa matematicamente, frente ao <strong>Back-Propagation<\/strong>.  ","1cf8f1a0":"Uma ilustra\u00e7\u00e3o de como uma rede neural retropropaga seu erro:\n![](https:\/\/matthewmazur.files.wordpress.com\/2018\/03\/output_1_backprop-4.png?w=525)","b73e07d2":"## Cross entropy error\nA Cross-entropy loss ou  perda logar\u00edtimica, mede o desempenho de um modelo cuja sa\u00edda \u00e9 um valor de probabilidade entre <strong>0<\/strong> e <strong>1<\/strong>. A Cross-entropy loss aumenta \u00e0 mediada que sua probabilidade prevista diverge da label real. \n<br \/>\n\ud83d\udccd<i> Um modelo perfeito teria uma perda logar\u00edtimica de 0.<\/i>\n\n<strong>Equa\u00e7\u00e3o:<\/strong>\n$$L = - \\sum_{i}(y\\prime_{i} log(y_{i}) + (1 - y\\prime_{i}) log(1 - y_{i}))$$\n\n<hr \/>\n\ud83e\uddee Onde <strong>$y_{i}$<\/strong> \u00e9 o valor previsto para a classe <strong>$i$I<\/strong> e <strong>$y\\prime_{i}$<\/strong>  \u00e9 a probabilidade verdadeira da classe. ","4e702859":"**Refer\u00eancias:**\n* [Deep Learning Brasil (Em portugu\u00eas) \u2014 Perceptron e Adaline](https:\/\/www.youtube.com\/watch?v=6yYUc6nU3Cw)\n* [Deep Learning Brasil (Em portugu\u00eas) \u2014 Parametriza\u00e7\u00e3o de redes neurais profundas](https:\/\/www.youtube.com\/watch?v=qDmKwmkc4vs&t=1s)\n* [Deep Learning Brasil (Em portugu\u00eas) \u2014 Algoritmo da retropropaga\u00e7\u00e3o de erros (Backpropagation) para redes multi-layer perceptron](https:\/\/www.youtube.com\/watch?v=DGNbd2FGw2s)\n* https:\/\/medium.com\/@franckepeixoto\/mlp-multilayer-perceptron-conceito-de-b%C3%A1sico-93afa91dd03e\n* https:\/\/medium.com\/@franckepeixoto\/machine-learning-indaga%C3%A7%C3%B5es-f0fce290451c\n* https:\/\/en.wikipedia.org\/wiki\/Artificial_neural_network\n* https:\/\/en.wikipedia.org\/wiki\/Activation_function\n* https:\/\/en.wikipedia.org\/wiki\/Sigmoid_function\n* https:\/\/theclevermachine.wordpress.com\/tag\/tanh-function\/\n* http:\/\/dataaspirant.com\/2017\/03\/07\/difference-between-softmax-function-and-sigmoid-function\/\n* https:\/\/github.com\/Kulbear\/deep-learning-nano-foundation\/wiki\/ReLU-and-Softmax-Activation-Functions\n* http:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/loss_functions.html\n* https:\/\/en.wikipedia.org\/wiki\/Backpropagation\n* https:\/\/towardsdatascience.com\/understanding-backpropagation-algorithm-7bb3aa2f95fd","0a63ac4c":"\ud83d\udd18 Demonstra\u00e7\u00e3o de como \u00e9 o c\u00e1lculo da sa\u00edda de um <strong>forward pass<\/strong> em uma RNA 3-4-1:\n\n*Os pesos iniciais da primeira e segunda camadas*:\n\n$W_{ij} (peso)=                      \n\\begin{bmatrix}\n    0.1 & 0.2 & 0.4 & 0.3 \\\\\n    0.5 & 0.4 & 0.7 & 0.9 \\\\\n    0.2 & 0.6 & 0.3 & 0.8 \\\\\n\\end{bmatrix}$\n$>$\n$W_{jk} = \n\\begin{bmatrix}\n    0.2 \\\\\n    0.3 \\\\\n    0.6 \\\\\n    0.1 \\\\\n\\end{bmatrix}$\n\n<hr \/> \n<strong>Input:<\/strong>\n\n$i = \n\\begin{bmatrix}\n    0.1 \\\\\n    0.4 \\\\\n    0.5 \\\\\n\\end{bmatrix}$\n\n<hr \/>\n<strong>Output:<\/strong>\n\n$o = \n\\begin{bmatrix}\n    1 \\\\\n\\end{bmatrix}$","862394cb":"Em nosso exemplo, usaremos a fun\u00e7\u00e3o de ativa\u00e7\u00e3o  ReLU e na camada de sa\u00edda \u00e1 ser\u00e1 usado a Sigm\u00f3ide.\n\nblz, mas agora  <i>vamos codificar um pouco! \ud83e\udd71<\/i>","bdf7dbca":"\ud83c\udfc6 E \u00e9 isso, <strong>Acabou! \ud83e\udd23 <\/strong>\n<br \/>\nVoc\u00ea acabou de fazer os propagations mais famosos da Deep Learning! ","8b4290e8":"ReLU:\n\\begin{equation}\n   \\frac{\\partial ReLU(x)}{\\partial x}=\\begin{cases}\n    1, & \\text{if $x>0$}.\\\\\n    0, & \\text{otherwise}.\n  \\end{cases}\n\\end{equation}\n<hr \/>","3bdd4f01":"# Back-propagation\n\n[Back-propagation](https:\/\/towardsdatascience.com\/understanding-backpropagation-algorithm-7bb3aa2f95fd) \u00e9 um m\u00e9todo usado em RNAs para calcular um gradiente (nossa b\u00fassola \ud83d\udd2d) que \u00e9 necess\u00e1rio no c\u00e1clulo dos pesos que s\u00e3o usados na rede.\n<br \/>\n<img src='https:\/\/www.guru99.com\/images\/1\/030819_0937_BackPropaga1.png' style='height:300px;' \/>\n","99697406":"## Alguns derivadas importantes\n\n<strong> Equa\u00e7\u00f5es:<\/strong>\n\nCross entropy error:\n$$\\frac{\\partial L}{\\partial y_{i}} = \\frac{-y\\prime_{i}}{y_{i}} + \\frac{(1 - y\\prime_{i})}{(1 - y_{i})}$$\n<hr \/>\nSigmoid: \n$$\\frac{\\partial Sigmoid(x)}{\\partial x} = Sigmoid(x) \\times (1 - Sigmoid(x))$$\n<hr \/>\n","805719c0":"##### [Artigo Redes Neurais Artificiais | Parte #1](https:\/\/www.kaggle.com\/franckepeixoto\/redes-neurais-artificiais-conceito-b-sico)\n\n# Forward Propagation\n\u27a1\ufe0f Forward propagation \u00e9 o processo de \"alimentar\" um conjunto de entradas em uma rede neural para obter resultado com seus pesos e, em seguida, na \u00faltima entrada sua fun\u00e7\u00e3o de ativa\u00e7\u00e3o compara seus valores com a s\u00e1ida real (gabarito \ud83e\udd13).\n<img src='https:\/\/franckepeixoto.files.wordpress.com\/2020\/02\/forward-propagation.png'  style='height:340px'\/>\n<a href='https:\/\/www.youtube.com\/watch?v=YdrHNywoe7U' taget='_blank'>\ud83d\udca1<\/a>"}}