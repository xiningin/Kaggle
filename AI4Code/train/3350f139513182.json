{"cell_type":{"8dd71313":"code","fc950f9e":"code","35f792f9":"code","a9fcba7f":"code","095a652a":"code","c6ab49d1":"code","5051ef51":"code","57f57963":"code","c516dbb4":"code","79d883b5":"code","bfc10354":"code","d1f46bbf":"code","b967083a":"code","e36003b0":"code","0ca5c617":"code","2a658c75":"code","3ae29186":"code","a305d33b":"code","33bab449":"code","48bb599c":"code","c6d5a858":"code","1b6690dc":"code","5b569063":"code","fc7b8f2a":"code","e938887b":"code","6d7cffb1":"code","a01bc3e3":"code","3612a209":"code","dcd22762":"code","cc7e28de":"code","63cc75b1":"markdown","9dd684cc":"markdown","f4d86d85":"markdown","7d61367c":"markdown","959c51eb":"markdown","27bd82db":"markdown","6cbd8460":"markdown","94714b65":"markdown","89dbdbf1":"markdown","aee5bc8d":"markdown","191d3d4b":"markdown","98f4a666":"markdown","9ccca8c2":"markdown","ba71a798":"markdown","d5b33869":"markdown","8c69c940":"markdown","02431c1a":"markdown","d9dc3894":"markdown","456d0f95":"markdown","bdac1ca8":"markdown","25a85421":"markdown","2e111b3e":"markdown","6ef54bfb":"markdown","2a777128":"markdown","6b10150f":"markdown","211e5584":"markdown","12425ba7":"markdown","c509f96e":"markdown","a4fdcf9c":"markdown","6571874f":"markdown","d798a0a8":"markdown","e7db992b":"markdown","bb92c5c9":"markdown","3b574464":"markdown","8a1206e7":"markdown","c348b59d":"markdown","44d0d6fd":"markdown","8b4d0cc4":"markdown","cb6e3392":"markdown","cab8934a":"markdown","b159f7c6":"markdown","ccefd126":"markdown","a35d9c9b":"markdown"},"source":{"8dd71313":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # graphing capabilities\nfrom beautifultext import BeautifulText as bt # utility script\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport seaborn as sns# for data viz.\nimport plotly.express as px\n\nfrom plotly.subplots import make_subplots# for subplots using plotly\n\nimport plotly.graph_objects as go\n\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# Any results you write to the current directory are saved as output.","fc950f9e":"pd.read_csv('\/kaggle\/input\/kaggle-survey-2018\/SurveySchema.csv').head(20)","35f792f9":"g1=bt(font_family='Comic Sans MS',color='Dark Black',font_size=19)\ng1.printbeautiful('Reading Files')","a9fcba7f":"multiple_choice_responses = pd.read_csv(\"..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv\",low_memory=False)# this warning shows when pandas finds \n# difficult to guess datatype for each column in large dataset\nother_text_responses = pd.read_csv(\"..\/input\/kaggle-survey-2019\/other_text_responses.csv\",low_memory=False)\nquestions_only = pd.read_csv(\"..\/input\/kaggle-survey-2019\/questions_only.csv\",low_memory=False)\nsurvey_schema = pd.read_csv(\"..\/input\/kaggle-survey-2019\/survey_schema.csv\",low_memory=False)","095a652a":"multiple_2018=pd.read_csv('\/kaggle\/input\/kaggle-survey-2018\/multipleChoiceResponses.csv',low_memory=False)","c6ab49d1":"multiple_2018.head(3)","5051ef51":"g1=bt(font_family='Comic Sans MS',color='bLUE',font_size=19)\n\ng1.printbeautiful('MULTIPLE CHOICE QUESTION OVERVIEW')","57f57963":"multiple_choice_responses.head(3)","c516dbb4":"g1=bt(font_family='Comic Sans MS',color='bLUE',font_size=19)\ng1.printbeautiful('MUTIPLE CHOICE RESPONSES MISSING VALUES')","79d883b5":"total = multiple_choice_responses.isnull().sum().sort_values(ascending=False)\npercent_1 = multiple_choice_responses.isnull().sum()\/multiple_choice_responses.isnull().count()*100\npercent_1 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_multiple_choice_responses = pd.concat([total, percent_1], axis=1, keys=[\"Total\", \"%\"], sort=False)\n\ng1=bt(font_family='Comic Sans MS',color='bLUE',font_size=19)\ng1.printbeautiful('PERCENTAGE OF MISSING VALUES FROM DATA OF 2019 ')\nmissing_multiple_choice_responses.head(10)","bfc10354":"age_groups=multiple_choice_responses.groupby('Q1').count().Q2\nage_groups.drop(age_groups.tail(1).index,inplace=True)\n\n\nage_groups_2018=multiple_2018.groupby('Q2').count().Q1\nage_groups_2018.drop(age_groups_2018.tail(1).index,inplace=True)\n\nfig=make_subplots(rows=1,cols=2,\n                  specs=[[{\"type\": \"bar\"},{\"type\": \"bar\"}]] # always make list of list for making subplots\n                 ,subplot_titles=('2019','2018'))\nfig.add_trace(go.Bar(x=age_groups.index,y=age_groups.values,name='2019'),row=1, col=1)\n\nfig.add_trace(go.Bar(x=age_groups_2018.index,y=age_groups_2018.values,name='2018'),row=1,col=2)\n\nfig.update_layout(title_text='AGE GROUPS OF KAGGLERS ')","d1f46bbf":"age_25_29_dict={}\nage_25_29=multiple_choice_responses.loc[multiple_choice_responses.Q1=='25-29']\nage_25_29_dict['Twitter']=age_25_29.Q12_Part_1.value_counts().sum()\nage_25_29_dict['Hacker News']=age_25_29.Q12_Part_2.value_counts().sum()\nage_25_29_dict['Reddit']=age_25_29.Q12_Part_3.value_counts().sum()\nage_25_29_dict['Kaggle']=age_25_29.Q12_Part_4.value_counts().sum()\nage_25_29_dict['Course Forums']=age_25_29.Q12_Part_5.value_counts().sum()\nage_25_29_dict['YouTube']=age_25_29.Q12_Part_6.value_counts().sum()\nage_25_29_dict['Podcasts']=age_25_29.Q12_Part_7.value_counts().sum()\nage_25_29_dict['Blogs']=age_25_29.Q12_Part_8.value_counts().sum()\nage_25_29_dict['Journal Publications']=age_25_29.Q12_Part_9.value_counts().sum()\nage_25_29_dict['Slack']=age_25_29.Q12_Part_10.value_counts().sum()\nage_25_29_dict['Other']=age_25_29.Q12_Part_11.value_counts().sum()\n\nage_25_29_dict=pd.DataFrame(list(age_25_29_dict.items()),columns=['Learning Resources','Numbers'])\n\npercentage_25_29=[]\nfor i in range(len(age_25_29_dict)):\n    percentage_25_29.append(round(age_25_29_dict.at[i,'Numbers']\/age_25_29_dict['Numbers'].sum(),2)*100)\n\nage_25_29_dict['Percentage']=percentage_25_29\n\nage_25_29_dict=age_25_29_dict.sort_values(['Percentage'],ascending=False)\nfig=px.bar(age_25_29_dict,y='Learning Resources',x='Percentage', height=500,width=1000,orientation='h',color=\"Percentage\",title='AGE GROUP 25-29')\nfig.show()\n#######################################################\nage_22_24_dict={}\nage_22_24=multiple_choice_responses.loc[multiple_choice_responses.Q1=='22-24']\nage_22_24_dict['Twitter']=age_22_24.Q12_Part_1.value_counts().sum()\nage_22_24_dict['Hacker News']=age_22_24.Q12_Part_2.value_counts().sum()\nage_22_24_dict['Reddit']=age_22_24.Q12_Part_3.value_counts().sum()\nage_22_24_dict['Kaggle']=age_22_24.Q12_Part_4.value_counts().sum()\nage_22_24_dict['Course Forums']=age_22_24.Q12_Part_5.value_counts().sum()\nage_22_24_dict['YouTube']=age_22_24.Q12_Part_6.value_counts().sum()\nage_22_24_dict['Podcasts']=age_22_24.Q12_Part_7.value_counts().sum()\nage_22_24_dict['Blogs']=age_22_24.Q12_Part_8.value_counts().sum()\nage_22_24_dict['Journal Publications']=age_22_24.Q12_Part_9.value_counts().sum()\nage_22_24_dict['Slack']=age_22_24.Q12_Part_10.value_counts().sum()\nage_22_24_dict['Other']=age_22_24.Q12_Part_11.value_counts().sum()\n\nage_22_24_dict=pd.DataFrame(list(age_22_24_dict.items()),columns=['Learning Resources','Numbers'])\n\npercentage_22_24=[]\nfor i in range(len(age_22_24_dict)):\n    percentage_22_24.append(round(age_22_24_dict.at[i,'Numbers']\/age_22_24_dict['Numbers'].sum(),2)*100)\n\nage_22_24_dict['Percentage']=percentage_22_24\n\nage_22_24_dict=age_22_24_dict.sort_values(['Percentage'],ascending=False)\nfig=px.bar(age_22_24_dict,y='Learning Resources',x='Percentage', height=400,width=1000,color='Percentage',title='AGE GROUP 22-24',orientation='h')\nfig.show()","b967083a":"gender_dist_2019=multiple_choice_responses.Q2.iloc[1:].value_counts()\ngender_dist_2018=multiple_2018.Q1.iloc[1:].value_counts()\n\n\nfig=make_subplots(rows=1,cols=2,\n                  specs=[[{\"type\": \"pie\"},{\"type\": \"pie\"}]] # always make list of list for making subplots\n                 ,subplot_titles=('2019','2018'))\nfig.add_trace(go.Pie(labels=gender_dist_2019.index[:2],values=gender_dist_2019.values[:2],hole=0.2,name='2019',pull=[0,0.3]),row=1,col=1)\nfig.add_trace(go.Pie(labels=gender_dist_2018.index[:2], values=gender_dist_2018.values[:2],name='2018', hole=.2,pull=[0,0.3]),row=1,col=2)\n\nfig.update_layout(title_text='AGE GROUPS OF KAGGLERS ')","e36003b0":"key1 = \"University Courses (resulting in a university degree)\"\ndf=multiple_choice_responses.copy()\n# df1 = df[df['Q13_Part_10'] == key1]\n# df2 = df[df['Q13_Part_10'] != key1]\n\nnations = [\"United States of America\", \"Canada\", \"United Kingdom of Great Britain and Northern Ireland\", \"Brazil\", \"Russia\", \"Germany\", \"Spain\", \"France\",  \"India\", \"Japan\", \"China\", \"Other\"]\nnation_map = {\"United States of America\" : \"USA\", \"United Kingdom of Great Britain and Northern Ireland\" : \"UK\"}\nplt.figure(figsize=(12,12))\n\nvals = []\nfor j in range(len(nations)):\n    country = nations[j]\n    country_df = df[df['Q3'] == country]\n    ddf1 = country_df[country_df['Q13_Part_10'] == key1]\n    ddf2 = country_df[country_df['Q13_Part_10'] != key1]\n    plt.subplot(4, 4, j+1)\n    \n    if j < 4:\n        colors = [\"#ff8ce0\",'#89e8a2']\n    elif j < 8:\n        colors = [\"#60cfe6\",\"#827ec4\" ]\n    else:\n        colors = [\"#ff8ce0\",\"#89e8a2\"]\n    \n    vals.append(len(ddf1) \/ (len(ddf1) + len(ddf2)))    \n    plt.pie([len(ddf1), len(ddf2)],\n            labels=[\"With Degree\", \"No Degree\"],\n            autopct=\"%1.0f%%\", \n            colors=colors,\n            wedgeprops={\"linewidth\":5,\"edgecolor\":\"white\"})\n    if country in nation_map:\n        country = nation_map[country]\n    plt.title(r\"$\\bf{\" + country + \"}$\")","0ca5c617":"country=multiple_choice_responses.Q3.value_counts()\n\n\nfig = go.Figure(go.Treemap(\n    labels = country.index,\n    parents=['World']*len(country),\n    values = country\n))\n\nfig.update_layout(title = 'Country of Survey Participants')\nfig.show()\n\n## credits https:\/\/www.kaggle.com\/subinium\/the-hitchhiker-s-guide-to-the-kaggle thanks for this wonderful plot type","2a658c75":"lang_recom_2019=multiple_choice_responses.Q19[1:].value_counts()\n\nlang_recom_2018=multiple_2018.Q18[1:].value_counts()\n\nprint(lang_recom_2019)","3ae29186":"lang_recom_2019=multiple_choice_responses.Q19[1:].value_counts()\n\nlang_recom_2018=multiple_2018.Q18[1:].value_counts()\n\n\nfig=go.Figure(data=[\n    go.Bar(x=lang_recom_2019.index,y=lang_recom_2019.values,name='2019',marker_color='rgb(55, 83, 109)'),\n    go.Bar(x=lang_recom_2018.index,y=lang_recom_2018.values,name='2018',marker_color='rgb(26, 118, 255)')\n])\nfig.update_layout(barmode='group')\nfig.show()","a305d33b":"multiple_choice_responses.head(2)","33bab449":"india_df=multiple_choice_responses.loc[multiple_choice_responses['Q3']=='India']\n\nusa_df=multiple_choice_responses.loc[multiple_choice_responses['Q3']=='United States of America']\n\n","48bb599c":"age_groups_india=india_df.groupby('Q1').count().Q2\n\nage_groups_usa=usa_df.groupby('Q1').count().Q2\n\nfig=make_subplots(rows=1,cols=2,\n                  specs=[[{\"type\": \"bar\"},{\"type\": \"bar\"}]] # always make list of list for making subplots\n                 ,subplot_titles=('INDIA',\"USA\"))\n\n\nfig.add_trace(go.Bar(x=age_groups_india.index,y=age_groups_india.values,name='INDIA'),row=1, col=1)\n\nfig.add_trace(go.Bar(x=age_groups_usa.index,y=age_groups_usa.values,name='USA'),row=1,col=2)\n\nfig.update_layout(title_text='AGE GROUPS OF KAGGLERS ')","c6d5a858":"fig=make_subplots(rows=1,cols=2,\n                 specs=[[{\"type\": \"pie\"},{\"type\": \"pie\"}]]\n                 ,subplot_titles=('INDIA',\"USA\"),\n            )\n\ngender_india=india_df.Q2.value_counts()\n\ngender_usa=usa_df.Q2.value_counts()\n\nfig.add_trace(go.Pie(labels=gender_india[:2].index,values=gender_india.values[:2],name='INDIA',pull=[0.4,0]),row=1,col=1)\nfig.add_trace(go.Pie(labels=gender_usa[:2].index,values=gender_usa.values[:2],name='USA',pull=[0.4,0],),row=1,col=2)\nfig.update_layout(height=500, showlegend=True)\n","1b6690dc":"india_formal_edu=india_df.Q4.value_counts().sort_values(ascending=False)# to make groups of eduaction with their frequencies\n\nusa_formal_edu=usa_df.Q4.value_counts().sort_values(ascending=False)# to make groups of eduaction with their frequencies\n\nfig=make_subplots(rows=1,cols=2,\n                 specs=[[{'type':'bar'},{'type':'bar'}]],\n                 subplot_titles=('INDIA',\"USA\"))\nfig.add_trace(go.Bar(x=india_formal_edu.index[:3],y=india_formal_edu.values[:3],name='INDIA'),row=1,col=1)\nfig.add_trace(go.Bar(x=usa_formal_edu.index[:3],y=usa_formal_edu.values[:3],name=\"USA\"),row=1,col=2)","5b569063":"usa_comm=usa_df.Q6.value_counts()\nindia_comm=india_df.Q6.value_counts()\nfig=go.Figure(data=[\n    go.Bar(x=usa_comm.index,y=usa_comm.values,name='USA',marker_color='rgb(55, 83, 109)'),\n    go.Bar(x=india_comm.index,y=india_comm.values,name='INDIA',marker_color='rgb(26, 118, 255)')\n])\nfig.update_layout(barmode='group')\nfig.show()","fc7b8f2a":"usa_data_media={}\n\nindia_data_media={}\n\nusa_data_media['Twitter']=usa_df.Q12_Part_1.value_counts().sum()\nusa_data_media['Hacker News']=usa_df.Q12_Part_2.value_counts().sum()\nusa_data_media['Reddit']=usa_df.Q12_Part_3.value_counts().sum()\nusa_data_media['Kaggle']=usa_df.Q12_Part_4.value_counts().sum()\nusa_data_media['Course Forums']=usa_df.Q12_Part_5.value_counts().sum()\nusa_data_media['YouTube']=usa_df.Q12_Part_6.value_counts().sum()\nusa_data_media['Podcasts']=usa_df.Q12_Part_7.value_counts().sum()\nusa_data_media['Blogs']=usa_df.Q12_Part_8.value_counts().sum()\nusa_data_media['Journal Publications']=usa_df.Q12_Part_9.value_counts().sum()\nusa_data_media['Slack']=usa_df.Q12_Part_10.value_counts().sum()\nusa_data_media['Other']=usa_df.Q12_Part_11.value_counts().sum()\n\nusa_media_df=pd.DataFrame(list(usa_data_media.items()),columns=['Sources','Numbers'])\n\nindia_data_media['Twitter']=india_df.Q12_Part_1.value_counts().sum()\nindia_data_media['Hacker News']=india_df.Q12_Part_2.value_counts().sum()\nindia_data_media['Reddit']=india_df.Q12_Part_3.value_counts().sum()\nindia_data_media['Kaggle']=india_df.Q12_Part_4.value_counts().sum()\nindia_data_media['Course Forums']=india_df.Q12_Part_5.value_counts().sum()\nindia_data_media['YouTube']=india_df.Q12_Part_6.value_counts().sum()\nindia_data_media['Podcasts']=india_df.Q12_Part_7.value_counts().sum()\nindia_data_media['Blogs']=india_df.Q12_Part_8.value_counts().sum()\nindia_data_media['Journal Publications']=india_df.Q12_Part_9.value_counts().sum()\nindia_data_media['Slack']=india_df.Q12_Part_10.value_counts().sum()\nindia_data_media['Other']=india_df.Q12_Part_11.value_counts().sum()\n\nindia_media_df=pd.DataFrame(list(india_data_media.items()),columns=['Sources','Numbers'])# dataframe for Indian Data science community about how\n#they update themselves about Data Science\n\nfig=go.Figure(data=[\n    go.Bar(x=india_media_df['Sources'],y=india_media_df['Numbers'],name='INDIA'),\n    go.Bar(x=usa_media_df['Sources'],y=usa_media_df['Numbers'],name=\"USA\")\n])\n\n# Change the bar mode\nfig.update_layout(barmode='group')\nfig.show()","e938887b":"usa_comm_tools=usa_df.Q14.value_counts()\nindia_comm_tools=india_df.Q14.value_counts()\nfig=go.Figure(data=[\n    go.Bar(x=usa_comm_tools.index[:2],y=usa_comm_tools.values[:2],name='USA'),\n    go.Bar(x=india_comm_tools.index[:2],y=india_comm_tools.values[:2],name='INDIA')\n])\nfig.update_layout(barmode='group')\nfig.show()","6d7cffb1":"usa_lang={}\nindia_lang={}\n\nusa_lang['Python']=usa_df.Q18_Part_1.value_counts().sum()\nusa_lang['R']=usa_df.Q18_Part_2.value_counts().sum()\nusa_lang['SQL']=usa_df.Q18_Part_3.value_counts().sum()\nusa_lang['C']=usa_df.Q18_Part_4.value_counts().sum()\nusa_lang['C++']=usa_df.Q18_Part_5.value_counts().sum()\nusa_lang['Java']=usa_df.Q18_Part_6.value_counts().sum()\nusa_lang['Javascript']=usa_df.Q18_Part_7.value_counts().sum()\nusa_lang['TypeScript']=usa_df.Q18_Part_8.value_counts().sum()\nusa_lang['Bash']=usa_df.Q18_Part_9.value_counts().sum()\nusa_lang['MATLAB']=usa_df.Q18_Part_10.value_counts().sum()\nusa_lang['None']=usa_df.Q18_Part_11.value_counts().sum()\nusa_lang['Other']=usa_df.Q18_Part_12.value_counts().sum()\n\n\nusa_lang_df=pd.DataFrame(list(usa_lang.items()),columns=['Languages','Numbers'])\n\n\nindia_lang['Python']=india_df.Q18_Part_1.value_counts().sum()\nindia_lang['R']=india_df.Q18_Part_2.value_counts().sum()\nindia_lang['SQL']=india_df.Q18_Part_3.value_counts().sum()\nindia_lang['C']=india_df.Q18_Part_4.value_counts().sum()\nindia_lang['C++']=india_df.Q18_Part_5.value_counts().sum()\nindia_lang['Java']=india_df.Q18_Part_6.value_counts().sum()\nindia_lang['Javascript']=india_df.Q18_Part_7.value_counts().sum()\nindia_lang['TypeScript']=india_df.Q18_Part_8.value_counts().sum()\nindia_lang['Bash']=india_df.Q18_Part_9.value_counts().sum()\nindia_lang['MATLAB']=india_df.Q18_Part_10.value_counts().sum()\nindia_lang['None']=india_df.Q18_Part_11.value_counts().sum()\nindia_lang['Other']=india_df.Q18_Part_12.value_counts().sum()\n\nindia_lang_df=pd.DataFrame(list(india_lang.items()),columns=['Languages','Numbers'])\n\nfig=make_subplots(rows=1,cols=2,\n                 specs=[[{\"type\": \"pie\"},{\"type\": \"pie\"}]]\n                 ,subplot_titles=('INDIA',\"USA\"),\n            )\n\nfig.add_trace(go.Pie(labels=india_lang_df['Languages'],values=india_lang_df['Numbers'],name='INDIA'),row=1,col=1)\n\nfig.add_trace(go.Pie(labels=usa_lang_df['Languages'],values=usa_lang_df['Numbers'],name='USA'),row=1,col=2)\n\nfig.update_layout(height=800, showlegend=True)\n\nfig.update_traces(hole=.4)# to create donut like pie chart\n\nfig.show()","a01bc3e3":"india_lang_combined_dict={}\n\nusa_lang_combined_dict={}\n\nindia_lang_combined=india_df.loc[:,['Q18_Part_1','Q18_Part_2','Q18_Part_3','Q18_Part_4'\n               ,'Q18_Part_5','Q18_Part_6','Q18_Part_7','Q18_Part_8',\n               'Q18_Part_9','Q18_Part_10','Q18_Part_11','Q18_Part_12']]\n\nindia_lang_combined_dict['Python and R']=india_lang_combined.loc[(india_lang_combined['Q18_Part_1']=='Python') & \n                         (india_lang_combined['Q18_Part_2']=='R') ].shape[0]\n\nindia_lang_combined_dict['Python and SQL']=india_lang_combined.loc[(india_lang_combined['Q18_Part_1']=='Python') & \n                         (india_lang_combined['Q18_Part_3']=='SQL') ].shape[0]\n\nindia_lang_combined_dict['R and SQL']=india_lang_combined.loc[(india_lang_combined['Q18_Part_2']=='R') & \n                         (india_lang_combined['Q18_Part_3']=='SQL') ].shape[0]\n\nindia_lang_combined_dict['Python ,R and SQL']=india_lang_combined.loc[(india_lang_combined['Q18_Part_1']=='Python') & (india_lang_combined['Q18_Part_2']=='R') &\n                         (india_lang_combined['Q18_Part_3']=='SQL') ].shape[0]\n\nindia_lang_combined_dict=pd.DataFrame(list(india_lang_combined_dict.items()),columns=['Languages',\"Numbers\"])\n#######################################################################################################3\nusa_lang_combined=usa_df.loc[:,['Q18_Part_1','Q18_Part_2','Q18_Part_3','Q18_Part_4'\n               ,'Q18_Part_5','Q18_Part_6','Q18_Part_7','Q18_Part_8',\n               'Q18_Part_9','Q18_Part_10','Q18_Part_11','Q18_Part_12']]\n\nusa_lang_combined_dict['Python and R']=usa_lang_combined.loc[(usa_lang_combined['Q18_Part_1']=='Python') & \n                         (usa_lang_combined['Q18_Part_2']=='R') ].shape[0]\n\nusa_lang_combined_dict['Python and SQL']=usa_lang_combined.loc[(usa_lang_combined['Q18_Part_1']=='Python') & \n                         (usa_lang_combined['Q18_Part_3']=='SQL') ].shape[0]\n\nusa_lang_combined_dict['R and SQL']=usa_lang_combined.loc[(usa_lang_combined['Q18_Part_2']=='R') & \n                         (usa_lang_combined['Q18_Part_3']=='SQL') ].shape[0]\n\nusa_lang_combined_dict['Python ,R and SQL']=usa_lang_combined.loc[(usa_lang_combined['Q18_Part_1']=='Python') & (usa_lang_combined['Q18_Part_2']=='R') &\n                         (usa_lang_combined['Q18_Part_3']=='SQL') ].shape[0]\n\nusa_lang_combined_dict=pd.DataFrame(list(usa_lang_combined_dict.items()),columns=['Languages',\"Numbers\"])","3612a209":"fig=make_subplots(1,2,specs=[[{'type':'bar'},{'type':'bar'}]], subplot_titles=('INDIA',\"USA\"),)\n\nfig.add_trace(go.Bar(x=india_lang_combined_dict['Languages'],y=india_lang_combined_dict['Numbers'],name='INDIA'),row=1,col=1)\n\nfig.add_trace(go.Bar(x=usa_lang_combined_dict['Languages'],y=usa_lang_combined_dict['Numbers'],name='USA'),row=1,col=2)","dcd22762":"usa_hardware={}\nindia_hardware={}\n\n\nusa_hardware['CPU']=usa_df.Q21_Part_1.value_counts().sum()\nusa_hardware['GPU']=usa_df.Q21_Part_2.value_counts().sum()\nusa_hardware['TPU']=usa_df.Q21_Part_3.value_counts().sum()\nusa_hardware['other']=usa_df.Q21_Part_5.value_counts().sum()\nusa_hardware=pd.DataFrame(list(usa_hardware.items()),columns=['Hardware',\"Numbers\"])\n\nindia_hardware['CPU']=india_df.Q21_Part_1.value_counts().sum()\nindia_hardware['GPU']=india_df.Q21_Part_2.value_counts().sum()\nindia_hardware['TPU']=india_df.Q21_Part_3.value_counts().sum()\nindia_hardware['other']=india_df.Q21_Part_5.value_counts().sum()\nindia_hardware=pd.DataFrame(list(india_hardware.items()),columns=['Hardware',\"Numbers\"])\n\nfig=make_subplots(rows=1,cols=2,\n                 specs=[[{\"type\": \"bar\"},{\"type\": \"bar\"}]]\n                 ,subplot_titles=('INDIA',\"USA\"),\n            )\nfig.add_trace(go.Bar(y=india_hardware['Hardware'],x=india_hardware['Numbers'],name='INDIA', orientation='h'),row=1,col=1)\nfig.add_trace(go.Bar(y=usa_hardware['Hardware'],x=usa_hardware['Numbers'],name=\"USA\", orientation='h'),row=1,col=2)","cc7e28de":"country_list={}\ncountries=multiple_choice_responses.loc[1:].Q3.unique()\ncountries=list(countries)\nfor i in countries[:]:\n    f=multiple_choice_responses.loc[multiple_choice_responses['Q3']==i]\n    country_list[i]=f.Q2.value_counts()[:2]\n#     print(f.Q2.unique())\ncountry_list=pd.DataFrame(list(country_list.items()),columns=['Name','Male'])\n# country_list","63cc75b1":"1.  **Recommendations** have dropped significantly for **R** and **Python** the two most loved languages by Data Scientists from 2018 to 2019\n2. But still **Python** is dominating the Data Science Field","9dd684cc":"## So that's it for comparisons between 2018 and 2019  we are going to start with a new story between \n## <font color='Darkblue'>Giants in Data Science Community<\/font>","f4d86d85":"## Special Mention to Refrences :\nhttps:\/\/www.kaggle.com\/shivamb\/spending-for-ms-in-data-science-worth-it\n","7d61367c":"## The MOST <font color='red'>LETHAL<\/font> Combination","959c51eb":"So Looking at the numbers we see **Indians** use more **Hardware(CPU,GPU and TPU)** but percentages reveal the real **Truth** :\n1. 52.31% Americans use **CPU**  as compared  to 50.05% Indian Users\n2. 31.11% Americans use **GPU** as comapred to 33.4% Indian Users\n\nSo we can CPU's are still very Popular in **USA**\n\nVery Strange thing to notice Although **USA** and **India** are biggest communities of **Data Scientist**\nbut a very few people use **TPU**(Tensor processing Units) which are way more faster even than **GPU's**","27bd82db":"1. **Python** and **SQL** are used most in combination by Indian and American Data Scientist.\n2. **Python** and **R** is the **second most dominant combination** of languages used in USA and INDIA","6cbd8460":"<a id=\"8\"><\/a> <br>\n**3.<font color='blue'>FORMAL EDUCATION<\/font>** and <font color='blue'>**SIZE OF ORGANIZATION**<\/font>\n","94714b65":"<a id=\"11\"><\/a> <br>\n**6.<font color='Green'>Which Programming Language is More Popular?<\/font>**","89dbdbf1":"<a id=\"10\"><\/a> <br>\n**5.<font color='blue'>Interactive Development Environments(IDE's)<\/font>**","aee5bc8d":"<a id=\"4.1\"><\/a> <br>\n## Q5 Most <font color='red'>recommended<\/font> language from <font color='red'>2018<\/font> to <font color='red'>2019<\/font>","191d3d4b":"<font color='blue'>SIZE OF ORGANIZATION<\/font>","98f4a666":"![f](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcTDCm2_LLSNP9vc8929zXK-1K3e5rkAF-rScrI0qMCKWWyDIYTL)\n\n## And Very <font color='red'>SAD<\/font> change is shown by this although marginal <font color='red'>(0.5%)<\/font> but the number of <font color='red'>FEMALE<\/font> kagglers have dropped from last year","9ccca8c2":"<a id=\"4\"><\/a> <br>\n## Q4 Country Wise Distributions of <font color='red'>Kagglers<\/font>","ba71a798":"![d](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcTgnkmvlA47WDMH0nI1pBgvwbgcvzFtlnxzXoqcoy3VHMbmPEJT)","d5b33869":"This is my first **Kaggle competition** in a Team \nI am a **Kaggle Lover** and I was overwhelmed when I saw this survey will serve as Kaggle competition\nIncase you enjoy our kernel and Love Our Visualisation work\n<font color='red'>PLS UPVOTE<\/font>\n![d](https:\/\/media2.giphy.com\/media\/xUOxf4YpQNlEtssg8g\/giphy.gif)","8c69c940":"<a id=\"3\"><\/a> <br>\n## Q2 Comapring <font color='red'>Female<\/font> vs <font color='red'>Male <\/font> from 2018 to 2019","02431c1a":"<a id=\"2\"><\/a> <br>\n## Q1 Age Groups of <font color='red'>Kagglers<\/font>  In YEAR <font color='red'>2019<\/font> vs <font color='red'>2018<\/font> ?","d9dc3894":"<a id=\"6\"><\/a> <br>\n**1. <font color='red'>AGE GROUPS<\/font>**","456d0f95":"This clearly shows that there is large **GENDER IMBALANCE** in both **USA AND INDIA** although **USA** is little better but still very less women","bdac1ca8":"## **Analyzing Top3 Languages in USA and INDIA**","25a85421":"\n*Note 1: We have many columns missing almost 100% of its values, perhaps we should consider dropping such columns. The reason why many columns have very little values can be found in the overview of the competition:*\n> Not every question was shown to every respondent. You can learn more\nabout the different segments we used in the survey_schema.csv file. In general, respondents with more experience were asked more questions and respondents with less experience were asked less questions.\n","2e111b3e":"1. In **INDIA** more children are into Data Science field at the very starting of their careers and this Number is almost **9 times** as compared to that in **USA** but this number starts to decrease as the age grows because **INDIA** is a country where typical **SOFTWARE ENGINEERING** (most of the colleges) is taught in colleges so we can  safely guess that after learning that software engineeringstuff  more and more INDIAN graduates tend to move towards **SAAS**where they can work as typical software engineer hence this number goes down (number of people into DATA SCIENCE) as the age grows but as compared to in **USA** once a graduate passes from US **UNIVERSITY\/COLLEGE** they are more interested in learning DATA SCIENCE hence their number increase till the age of 25","6ef54bfb":"<a id=\"7\"><\/a> <br>\n**2. <font color='blue'>SEX RATIO<\/font>**","2a777128":"<a id=\"5\"><\/a> <br>\n## <font color='Red'>India <\/font> and <font color='red'>USA<\/font> are way ahead than other countries in terms of number of **kaggle** **users**","6b10150f":"So this is It for **INDIA** and **USA**\nIn next version we will discover new Trends\nIf you like it \n## PLEASE UPVOTE","211e5584":"## <font color='parrotgreen'>Highlights<\/font>\n* **Kaggle Learn,You Tube** and **Blogs** are most **trusted** and **used platforms** of learning  for almost over **50%**(56%) of people (because age group of 22-24 and 25-29) combine to more than 50% of total population involved in survey.\n* This shows 25-29 is age group where People are incilned towards Data science \n  And also upto the age of 29 this Number of Participants were increasing then it starts to decrease.\n* **Kaggle** started their Online learning from early 2017 in form of Notebooks which focus on both Theoratical learning as well as Programming aspect which is essential for Data Scientists","12425ba7":"![s](https:\/\/media.giphy.com\/media\/Jlt69Ka6SwtH2\/giphy.gif)\n\n","c509f96e":"* This clearly shows **HUGE** GENDER IMBALANCE between **Kaggle** Users\n","a4fdcf9c":"<a id=\"3.1\"><\/a> <br>\n## <font color='green'>Q3. Proportion of Individuals with University Degrees ?<\/font>\nLet's look at what per cent of individuals completed their university degrees to become a data scientist across different countries. Respondents were asked about their country in one of the questions.","6571874f":"## NOTEBOOK CONTENT\n* [Work Starts](#1)\n\n\n1. [Age Group of Kagglers in 2019 vs 2018](#2)\n\n    1.1 [Which Learning resources prefferd by most number of people ?](#2.1)  \n    \n    \n2  [Have Number of Female Kagglers increased or not from 2018 to 2019?](#3)\n\n\n3 [Proportion of Individuals with University Degrees ?](#3.1)\n\n\n4 [Country Wise Distribution for Kagglers](#4)\n\n5 [Have recommendations for Programming language in Data Science Arena changed from 2018 to 2019](#4.1)\n\n\n [So we found our Rivals](#5)\n \n ## New Story Begins \u2694\ufe0f\n\n**USA vs INDIA**\n\n\n  4.1 [Age Groups](#6)\n\n\n4.2 [Female vs Male percentage](#7)\n\n\n4.3 [Formal education and Size of Organisation](#8)\n\n\n4.4 [Learning Platforms](#9)\n\n\n4.5 [Interactive Development Environments](#10)\n\n\n4.6 [Which Programming Language is More Popular?](#11)\n\n\n4.7 [Hardware Trends](#12)","d798a0a8":"* In most of the countries, more than 1\/4th of the respondents completed their university degrees to becoming a data scientist. In the United States, about 27% of the individuals who were part of this survey completed their university degrees.\n* Countries with the highest proportion of data scientists with university degrees are 'Tunisia', 'Austria', 'New Zealand' and 'Greece' with over 40% of the individuals completing university degrees.\n* On the other hand, countries like 'Japan', 'Nigeria', 'Belarus', and 'Algeria' shows a lower number (less than 10%) of individuals completing university degrees.\n* Among the genders, female respondents have a higher number for completing university degrees than male respondents. 23% of the female respondents completed their university degrees and only 20% of the male respondents completed those degrees.\n\n","e7db992b":"**WALLA** I also like **Jupyter** very much \n\n1.Most preferred tools for Development in both **USA** and **India** are **LOCAL ENVIRONMENTS** like *JUPYTER LAB and R studio*","bb92c5c9":"**Python,SQL** are most popular programming languages both in USA and INDIA\n\n**C++,C** are relatively popular languages in INDIA as compared to USA","3b574464":"## <font color='purple'>INTRODUCTION<\/font>: <font color='blue'>KAGGLE SURVEY ANALYSIS<\/font> BY \u26a1LIGHTNING BOY","8a1206e7":"**VERY INTERESTING RESULTS**\n\n1.IN **USA** exactly **50%** have pursued **Master's Degree** whereas in **INDIA** the case is very different the highest education which is pursued by more than **50%** people is **Bachelor's Degree**\n\n2.More than **95%** people in **USA** and **INDIA** have formal education higher than **Bachelor's** (i.e either they have **Bachelor's**,**Master's** or **Doctoral degree**)\n\n**3.More Data scientists in both *USA* and *INDIA* prefer to work either in Very large organisations or either very small organisations.**","c348b59d":"## The <font color='red'>BOSS<\/font>\n\n![fgg](https:\/\/dimensionless.in\/wp-content\/uploads\/2019\/03\/Python-Programming-1024x640.png)\n\n","44d0d6fd":"# SO WE GOT OUR 2 DATA SCIENCE GIANTS <font color='red'>INDIA<\/font> AND <font color='red'>USA<\/font> \n# Let the battle begins","8b4d0cc4":"* Looking at the numbers we can clearly see that for all the age groups from 18 to 39 years numbers of **Data Scientists** have decreased have from year **2018** to **2019** but a different thing was observed from age group of **40 to 69** that number of Dats Scientists increased in year **2019** as comapered to **2018**.","cb6e3392":"<a id=\"9\"><\/a> <br>\n**4.<font color='red'>Learning Platforms for data Scienctists<\/font>**","cab8934a":"<a id=\"2.1\"><\/a> <br>\n## 1.1 <font color='parrotgreen'>Learning Resources preffered by Top 2 largest group of Data Scentists<\/font>\n### AGE GROUP OF <font color='parrotgreen'>25-29 and 22-24 <\/font> year 2019","b159f7c6":"<a id=\"12\"><\/a> <br>\n7. **<font color='red'>Hardware Trends<\/font>**","ccefd126":"<a id=\"1\"><\/a> <br>\n## Let's <font color='red'>START<\/font> the GREAT WORK","a35d9c9b":"1.**KAGGLE** and **YOU TUBE** are way more popular for learning new data science concepts in INDIA then in USA\n\n2.People of United States prefer **PODCASTS** more as compared to people in India\n\nWe have to keep in mind that population of INDIA is way more than USA and also there are more INDIANS using KAGGLE than AMERICANS\nso while making any predictions we have to keep this fact in mind."}}