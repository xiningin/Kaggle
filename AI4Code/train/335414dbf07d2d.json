{"cell_type":{"b37bb82f":"code","e4bd0545":"code","1f80b985":"code","ab82b102":"code","427c3afe":"code","87592934":"code","8c877ab1":"code","50e2c601":"code","1997751b":"code","e21cb7af":"code","10f29047":"code","a1ea4706":"code","90737eee":"code","0db52fb7":"code","6bb0d6ca":"code","64249fc9":"code","e23092db":"code","6f021017":"code","3c992c4a":"code","dae6a455":"code","121294e5":"code","c11ca3a1":"code","1e7a0707":"code","b4d17fa6":"code","4e0b509e":"code","45ac310c":"code","4c268e40":"code","00e4d27e":"markdown","d26faf66":"markdown","dad2c065":"markdown","5a201825":"markdown","42170277":"markdown","754572a5":"markdown","b52d9f8c":"markdown","9473872b":"markdown","ad4899e0":"markdown","23bb0595":"markdown","66de3710":"markdown","66eb6085":"markdown"},"source":{"b37bb82f":"!pip install whiteboxlayer==0.1.2","e4bd0545":"import os, glob, random\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport whiteboxlayer.layers as lay\n\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph","1f80b985":"def sorted_list(path):\n    \n    tmplist = glob.glob(path)\n    tmplist.sort()\n    \n    return tmplist\n\ndef read_csv(path):\n    \n    return pd.read_csv(path)\n\ndef min_max_norm(data):\n    \n    return (data - data.min()) \/ (data.max() - data.min())\n\ndef plot_cadenece(cadence):\n    # 6 positions of the cadence\n    \n    plt.figure(figsize=(12, 6))\n    \n    for idx_c in range(cadence.shape[0]):\n        try: cadence_tot = np.append(cadence_tot, cadence[idx_c], axis=0)\n        except: cadence_tot = cadence[idx_c]\n    plt.imshow(min_max_norm(cadence_tot).astype(np.float32).T)\n    for idx_c in range(cadence.shape[0]):\n        plt.axvline(idx_c*cadence[idx_c].shape[0], color='white', linestyle='--', linewidth=0.5, alpha=0.5)\n    \n    plt.tight_layout()    \n    plt.show()","ab82b102":"\"\"\" Step 1\nFind out given dataset\"\"\"\n\nsorted_list(os.path.join('..\/input\/seti-breakthrough-listen', '*'))","427c3afe":"\"\"\" Step 2\nDefine the path to call the each file\"\"\"\n\npath_df = '..\/input\/seti-breakthrough-listen\/train_labels.csv'\npath_tr = '..\/input\/seti-breakthrough-listen\/train'\npath_te = '..\/input\/seti-breakthrough-listen\/test'\npath_sb = '..\/input\/seti-breakthrough-listen\/sample_submission.csv'","87592934":"\"\"\" Step 1\nLoad training set as a dataframe.\nThen, confirm the shape of training set.\"\"\"\n\ndf_tr = read_csv(path=path_df)\nprint(\"Shape of Training Set:\", df_tr.shape)","8c877ab1":"\"\"\" Step 2\nConfirm head of the training set.\"\"\"\n\ndf_tr.head(10)","50e2c601":"\"\"\" Step 3\nConfirm tail of the training set.\"\"\"\n\ndf_tr.tail(10)","1997751b":"\"\"\" Step 4\nConfirm unique label (target column) of the training set.\"\"\"\n\nlist_class = list(set(list(df_tr['target'])))\nprint(\"Class:\", list_class)","e21cb7af":"\"\"\" Step 5\nConfirm number of sample for each unique label.\nWe can confirm that the training set has a highly imbalanced form.\"\"\"\n\ndf_tr0 = df_tr[df_tr['target'] == 0]\ndf_tr1 = df_tr[df_tr['target'] != 0]\nnum_cls0, num_cls1 = df_tr0.shape[0], df_tr1.shape[0]\nprint(\"Class 0: %d\\nClass 1: %d\" %(num_cls0, num_cls1))\nhist = df_tr.hist()","10f29047":"\"\"\" Step 6\nConfirm number sub-ID of given dataset.\nGiven dataset inclues 16 sub-IDs.\"\"\"\n\nsorted_list(os.path.join(path_tr, '*'))","a1ea4706":"\"\"\" Step 7\nConfirm number of sample for each sub-ID.\nThe training set includes 16 of unique sub-ID as the shown list 'list_subid'.\nMoreover, we confirm the number of sample for each label for each sub-ID.\"\"\"\n\nlist_subid = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f']\nfor subid in list_subid:\n    df_sub = df_tr[df_tr['id'].str.startswith(subid)]\n    num_cls0, num_cls1 = df_sub[df_sub['target'] == 0].shape[0], df_sub[df_sub['target'] != 0].shape[0]\n    num_tot = df_sub.shape[0]\n    print(\"Sub-ID: %s | Class 0:%d (%.3f%%)  Class 1: %d (%.3f%%)\" %(subid, num_cls0, num_cls0\/num_tot*100, num_cls1, num_cls1\/num_tot*100))","90737eee":"\"\"\" Step 8\nListing the numpy file of the training set.\"\"\"\n\nlist_npy = sorted_list(os.path.join(path_tr, '*', '*.npy'))\nprint(\"Number of npy file: %d\" %(len(list_npy)))","0db52fb7":"\"\"\" Step 9.1\nConfirm the numpy file (includes cadence information) as a image.\nFirstly, this code block presents random samples with label 0.\"\"\"\n\nfor subid in list_subid:\n    terminator = False\n    while(True):\n        idx = random.randint(0, df_tr.shape[0])\n        if(df_tr.iloc[idx]['target'] == 0 and df_tr.iloc[idx]['id'].startswith(subid)): terminator = True \n        else: continue\n        print(\"Index:%d | ID: %s, Target: %d\" %(idx, df_tr.iloc[idx]['id'], df_tr.iloc[idx]['target']))\n        tmp_npy = np.load(list_npy[idx])\n        plot_cadenece(cadence=tmp_npy)\n        if(terminator): break","6bb0d6ca":"\"\"\" Step 9.2\nAs 'Step 9.1', this code block presents random samples with label 1.\"\"\"\n\nfor subid in list_subid:\n    terminator = False\n    while(True):\n        idx = random.randint(0, df_tr.shape[0])\n        if(df_tr.iloc[idx]['target'] == 1 and df_tr.iloc[idx]['id'].startswith(subid)): terminator = True \n        else: continue\n        print(\"Index:%d | ID: %s, Target: %d\" %(idx, df_tr.iloc[idx]['id'], df_tr.iloc[idx]['target']))\n        tmp_npy = np.load(list_npy[idx])\n        plot_cadenece(cadence=tmp_npy)\n        if(terminator): break","64249fc9":"\"\"\" Step 1\nLoad and confirm the submission file.\"\"\"\n\ndf_sb = read_csv(path=path_sb)\nprint(\"Shape of Submission Set:\", df_sb.shape)","e23092db":"\"\"\" Step 2\nConfirm head of the submission file.\nAll the 'target' values have assigned with a value of 0.5.\nWe will replace each 'target' value in the test procedure after training the model.\"\"\"\n\ndf_sb.head(10)","6f021017":"\"\"\" Step 3\nConfirm number of the sample for each sub-ID.\nAlso, the percentage of each sub-ID is shown.\"\"\"\n\nfor subid in list_subid:\n    df_sub = df_sb[df_sb['id'].str.startswith(subid)]\n    num_sub, num_tot = df_sub.shape[0], df_sb.shape[0]\n    print(\"Sub-ID: %s | %d (%.3f%%)\" %(subid, num_sub, num_sub\/num_tot*100))","3c992c4a":"\"\"\" Step 4\nPresents random samples of test set.\"\"\"\n\nfor _ in range(5):\n    idx = random.randint(0, df_sb.shape[0])\n    print(\"Index:%d | ID: %s, Target: %d\" %(idx, df_sb.iloc[idx]['id'], df_sb.iloc[idx]['target']))\n    tmp_npy = np.load(list_npy[idx])\n    plot_cadenece(cadence=tmp_npy)","dae6a455":"\"\"\" Step 1\nConvert list of npy files to list of id.\"\"\"\n\nlist_npy = sorted_list(os.path.join(path_tr, '*', '*.npy'))\nlist_npy.extend(sorted_list(os.path.join(path_te, '*', '*.npy')))\n\nlist_id = [] # list_npy\nfor path_npy in list_npy:\n    list_id.append(path_npy.split('\/')[-1].replace('.npy', ''))\n    \npath_npy = list_npy[list_id.index('%s' %(df_tr.iloc[idx]['id']))]\nprint(df_tr.iloc[idx]['id'], path_npy)\n\nsample = np.load(path_npy)\nprint(sample.shape)\n[h, w, c] = np.transpose(sample, [1, 2, 0]).shape\nprint(h, w, c)","121294e5":"class Agent(object):\n\n    def __init__(self, **kwargs):\n\n        print(\"\\nInitializing Neural Network...\")\n\n        self.dim_h = kwargs['dim_h']\n        self.dim_w = kwargs['dim_w']\n        self.dim_c = kwargs['dim_c']\n        self.num_class = kwargs['num_class']\n        self.learning_rate = kwargs['learning_rate']\n        self.path_ckpt = kwargs['path_ckpt']\n\n        self.variables = {}\n\n        dummy = tf.zeros((1, self.dim_h, self.dim_w, self.dim_c), dtype=tf.float32)\n        self.__model = Neuralnet(**kwargs)\n        self.__model.forward(x=dummy, verbose=True)\n        print(\"\\nNum Parameter: %d\" %(self.__model.layer.num_params))\n\n        self.__init_propagation(path=self.path_ckpt)\n\n    def __init_propagation(self, path):\n\n        self.summary_writer = tf.summary.create_file_writer(self.path_ckpt)\n\n        self.variables['trainable'] = []\n        ftxt = open(\"list_parameters.txt\", \"w\")\n        for key in list(self.__model.layer.parameters.keys()):\n            trainable = self.__model.layer.parameters[key].trainable\n            text = \"T: \" + str(key) + str(self.__model.layer.parameters[key].shape)\n            if(trainable):\n                self.variables['trainable'].append(self.__model.layer.parameters[key])\n            ftxt.write(\"%s\\n\" %(text))\n        ftxt.close()\n\n        self.optimizer = tf.optimizers.Adam(learning_rate=self.learning_rate)\n        self.save_params()\n\n        conc_func = self.__model.__call__.get_concrete_function(\\\n            tf.TensorSpec(shape=(1, self.dim_h, self.dim_w, self.dim_c), dtype=tf.float32))\n        self.__get_flops(conc_func)\n\n    def __loss(self, y, y_hat):\n\n        entropy_b = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_hat)\n        entropy = tf.math.reduce_mean(entropy_b)\n\n        return {'entropy_b': entropy_b, 'entropy': entropy}\n\n    @tf.autograph.experimental.do_not_convert\n    def step(self, minibatch, iteration=0, training=False):\n\n        x = minibatch['x']\n        y = minibatch['y']\n\n        with tf.GradientTape() as tape:\n            logit, y_hat = self.__model.forward(x=x, verbose=False)\n            losses = self.__loss(y=y, y_hat=logit)\n\n        if(training):\n            gradients = tape.gradient(losses['entropy'], self.variables['trainable'])\n            self.optimizer.apply_gradients(zip(gradients, self.variables['trainable']))\n\n            with self.summary_writer.as_default():\n                tf.summary.scalar('%s\/entropy' %(self.__model.who_am_i), losses['entropy'], step=iteration)\n\n        return {'y_hat':y_hat, 'losses':losses}\n\n    def __get_flops(self, conc_func):\n\n        frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(conc_func)\n\n        with tf.Graph().as_default() as graph:\n            tf.compat.v1.graph_util.import_graph_def(graph_def, name='')\n\n            run_meta = tf.compat.v1.RunMetadata()\n            opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n            flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\n\n            flop_tot = flops.total_float_ops\n            ftxt = open(\"flops.txt\", \"w\")\n            for idx, name in enumerate(['', 'K', 'M', 'G', 'T']):\n                text = '%.3f [%sFLOPS]' %(flop_tot\/10**(3*idx), name)\n                print(text)\n                ftxt.write(\"%s\\n\" %(text))\n            ftxt.close()\n\n    def save_params(self, model='base', tflite=False):\n\n        if(tflite):\n            # https:\/\/github.com\/tensorflow\/tensorflow\/issues\/42818\n            conc_func = self.__model.__call__.get_concrete_function(\\\n                tf.TensorSpec(shape=(1, self.dim_h, self.dim_w, self.dim_c), dtype=tf.float32))\n            converter = tf.lite.TFLiteConverter.from_concrete_functions([conc_func])\n\n            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n            converter.experimental_new_converter = True\n            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n\n            tflite_model = converter.convert()\n\n            with open('model.tflite', 'wb') as f:\n                f.write(tflite_model)\n        else:\n            vars_to_save = self.__model.layer.parameters.copy()\n            vars_to_save[\"optimizer\"] = self.optimizer\n\n            ckpt = tf.train.Checkpoint(**vars_to_save)\n            ckptman = tf.train.CheckpointManager(ckpt, directory=os.path.join(self.path_ckpt, model), max_to_keep=1)\n            ckptman.save()\n\n    def load_params(self, model):\n\n        vars_to_load = self.__model.layer.parameters.copy()\n        vars_to_load[\"optimizer\"] = self.optimizer\n\n        ckpt = tf.train.Checkpoint(**vars_to_load)\n        latest_ckpt = tf.train.latest_checkpoint(os.path.join(self.path_ckpt, model))\n        status = ckpt.restore(latest_ckpt)\n        status.expect_partial()\n\nclass Neuralnet(tf.Module):\n\n    def __init__(self, **kwargs):\n        super(Neuralnet, self).__init__()\n\n        self.who_am_i = \"NN\"\n        self.dim_h = kwargs['dim_h']\n        self.dim_w = kwargs['dim_w']\n        self.dim_c = kwargs['dim_c']\n        self.num_class = kwargs['num_class']\n        self.filters = [self.dim_c, 8, 16, 32, 64, 64]\n\n        self.layer = lay.Layers()\n\n        self.forward = tf.function(self.__call__)\n\n    @tf.function\n    def __call__(self, x, verbose=False):\n\n        # origin deco: @tf.function\n        # @tf.autograph.experimental.do_not_convert\n        logit = self.__nn(x=x, name='nn', verbose=verbose)\n        y_hat = tf.nn.softmax(logit, name=\"y_hat\") \n\n        return logit, y_hat\n\n    def __nn(self, x, name='nn', verbose=True):\n\n        for idx, _ in enumerate(self.filters):\n            if(idx == 0): continue\n            x = self.layer.conv2d(x=x, stride=1, \\\n                filter_size=[3, 3, self.filters[idx-1], self.filters[idx]], dilations=[1, 1, 1, 1], \\\n                padding='VALID', batch_norm=False, activation='relu', name='%s-%d_1' %(name, idx), verbose=verbose)\n            x = self.layer.conv2d(x=x, stride=2, \\\n                filter_size=[3, 3, self.filters[idx], self.filters[idx]], dilations=[1, 1, 1, 1], \\\n                padding='VALID', batch_norm=False, activation='relu', name='%s-%d_2' %(name, idx), verbose=verbose)\n        \n        x = tf.reshape(x, shape=[x.shape[0], -1], name=\"flat\")\n        x = self.layer.fully_connected(x=x, c_out=128, \\\n                batch_norm=False, activation='relu', name=\"%s-clf0\" %(name), verbose=verbose)\n        x = self.layer.fully_connected(x=x, c_out=self.num_class, \\\n                batch_norm=False, activation=None, name=\"%s-clf1\" %(name), verbose=verbose)\n        \n        return x","c11ca3a1":"\"\"\" Step 1\nGPU setting.\"\"\"\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        print(e)\n\n\"\"\" Step 2\nInitializing Neural Network.\"\"\"\nagent = Agent(\\\n    dim_h = h, \\\n    dim_w = w, \\\n    dim_c = c, \\\n    num_class = 2, \\\n    learning_rate = 5e-4, \\\n    path_ckpt = 'Checkpoint')","1e7a0707":"def next_batch(df, idx, batch_size):\n    \n    batch_x, batch_y, batch_id, terminate = [], [], [], False\n    while(True):\n        \n        try: path_npy = list_npy[list_id.index('%s' %(df.iloc[idx]['id']))]\n        except: \n            idx = 0\n            terminate = True\n            break\n        else:\n            tmp_x = np.load(path_npy)\n            batch_x.append(np.transpose(sample, [1, 2, 0]))\n\n            try:\n                if(df.iloc[idx]['target'] == 0): batch_y.append(np.diag(np.ones(2))[0])\n                else: batch_y.append(np.diag(np.ones(2))[1])\n            except: \n                batch_y.append(np.diag(np.ones(2))[0])\n            \n            batch_id.append(df.iloc[idx]['id'])\n            \n            idx += 1\n            if(len(batch_x) == batch_size): break\n    \n    batch_x = np.asarray(batch_x)\n    batch_y = np.asarray(batch_y)\n    \n    return {'x':batch_x.astype(np.float32), 'y':batch_y.astype(np.float32), 'id':batch_id, 'terminate':terminate, 'idx':idx}","b4d17fa6":"\"\"\" Note that, This example does not fully complete the learning. \nLearning is up to you. Good luck! \"\"\"\nepochs, batch_size = 10, 32\niteration = 0\nfor epoch in range(epochs):\n    \n    idx, list_loss = 0, []\n    while(True):\n        minibatch = next_batch(df=df_tr, idx=idx, batch_size=batch_size)\n        idx, terminate = minibatch['idx'], minibatch['terminate']\n        \n        step_dict = agent.step(minibatch=minibatch, iteration=iteration, training=True)\n        list_loss.append(step_dict['losses']['entropy'])\n        if(iteration % 100 == 0): print(\" Iteration %10d | Loss: %.5f\" %(iteration, step_dict['losses']['entropy']))\n        iteration += 1\n        \n        del minibatch\n        if(terminate): break\n        \n    loss = np.average(np.asarray(list_loss))\n    print(\"Epoch [%d \/ %d] | Loss: %.5f\" %(epoch, epochs, loss))\n    \n    agent.save_params(model='model_0')\nagent.save_params(tflite=True)","4e0b509e":"idx = 0\nwhile(True):\n    minibatch = next_batch(df=df_sb, idx=idx, batch_size=batch_size)\n    idx = minibatch['idx']\n    if(idx % (batch_size*20) == 0): print(\"Progress [%d\/%d] (%.2f%%)\" %(idx, df_sb.shape[0], idx\/df_sb.shape[0]))\n\n    step_dict = agent.step(minibatch=minibatch, training=False)\n\n    for idx_id, tmp_id in enumerate(minibatch['id']):\n        df_sb.loc[df_sb['id'] == tmp_id, 'target'] = np.argmax(step_dict['y_hat'][idx_id])\n\n    if(minibatch['terminate']): break","45ac310c":"\"\"\" Confirm the histogram of answer after test process. \"\"\"\nhist = df_sb['target'].hist()","4c268e40":"df_sb.to_csv('submission.csv', index=False)","00e4d27e":"# 4. Test","d26faf66":"## 2.3 Confirm Submission File (for Test)","dad2c065":"## 2.1 Confirm Given Dataset","5a201825":"# 5. Make Submission","42170277":"# 2. EDA","754572a5":"## 3.1 Preparing for Training","b52d9f8c":"# 0. Introduction\n\nWelcome to the competition <a href=\"https:\/\/www.kaggle.com\/c\/seti-breakthrough-listen\/overview\">'SETI Breakthrough Listen - E.T. Signal Search'<\/a>!  \n\nAlso, welcome to this source code.  \nThis source code is constructed for the following goals.  \n* Exploratory Data Analysis (EDA)  \n* Make it possible for users to simply and easily submit results.  \n\nTry this source code and upvote if you like it!<\/br>\nHave a nice day and good luck to you.  \n","9473872b":"## 3.2 Preparing the neural network","ad4899e0":"## 2.2 Confirm Training Set","23bb0595":"# 1. Preparation  \nIn this section, we will prepare some of the python packages and define some of the python custom functions.","66de3710":"# 3. Training","66eb6085":"## 3.3 Training Iteration"}}