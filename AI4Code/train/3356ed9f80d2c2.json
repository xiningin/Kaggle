{"cell_type":{"d9bd81a2":"code","0026f99f":"code","321e226d":"code","9c5c6041":"code","72b5e3f8":"code","fc7f2671":"code","2c0abd10":"code","0505943a":"code","bcf74a82":"code","32088dcd":"code","a63b735e":"code","60d0f473":"code","282fbed6":"code","b5db7683":"code","54407c2e":"code","2ff13d93":"code","d9a1b3ed":"code","63c0f328":"code","d28afcf5":"code","094a0e72":"code","34d98d9a":"code","e3397824":"code","641a0209":"code","272670eb":"code","2a263921":"code","6cc5bbe6":"code","3c486fc9":"code","fe834ae0":"code","d4d1cb9c":"code","acef6fa2":"code","99cd6ece":"code","9bed3dc7":"code","ca966980":"code","e8586292":"code","7705d398":"code","6b9c8d82":"code","e76b24e1":"code","7b86868d":"code","c26581a3":"code","513b1699":"code","d26ce602":"code","f978b1f0":"code","7696bb2d":"code","c1426382":"code","ae639999":"code","a5fb683e":"code","43e509da":"code","b47f2f68":"code","bc904f15":"code","0156b7ca":"code","7d2a5c95":"code","02939335":"code","eeb2a2c2":"code","8d586715":"code","9f2a9620":"code","634bb46b":"code","3559d3c4":"code","aa7bf8fe":"code","6f76b78e":"code","33499b4c":"code","932d63af":"code","3eae2aad":"code","3b18d14a":"code","70934203":"code","15cf1723":"code","0bf318e1":"code","5eec5704":"code","5c73711a":"markdown","40cf9d59":"markdown","2a524656":"markdown","b1a0bd80":"markdown","6f0f29c6":"markdown","7078d091":"markdown","380faa72":"markdown","7854fc2e":"markdown","a986e76c":"markdown","dcc5e3a9":"markdown","78caa1f9":"markdown","3df73e5e":"markdown","9000e94a":"markdown"},"source":{"d9bd81a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0026f99f":"#We are importing our libraries.\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","321e226d":"#We get the training and test datasets in two dataframes named train_df and test_df.\ntrain_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ncombine = [train_df, test_df]","9c5c6041":"# There are 12 columns in the dataset.\n# Survived, Sex, Embarked, Pclass categorical, Age, Fare, SibSp, Parch continuous variables.\nprint(train_df.columns.values)\nlen(train_df.columns.values)","72b5e3f8":"# If we take a look at the data,\n#Ticket column is a mix of numeric and alphanumeric data types. The cabin is alphanumeric.\n#Name column may contain errors or typos.\ntrain_df.head()","fc7f2671":"#Let's look at the general information of the data with the info function.\n#In the Train dataset, the Cabin , Age , Embarked columns contain null values.\n#Test dataset has missing data in Cabin , Age columns.\n# 7 columns of integers or floats in the Train dataset. 6 in the test dataset.\n# 5 columns of both strings (object).\ntrain_df.info()\nprint('_'*40)\ntest_df.info()","2c0abd10":"#Let's look missing datat with bar plot\nimport missingno as msno","0505943a":"msno.bar(train_df)","bcf74a82":"#Check missing value distrubition as matrix\nmsno.matrix(train_df)","32088dcd":"msno.heatmap(train_df)","a63b735e":"#msno.dendrogram(train_df)","60d0f473":"#Let's look at the statistics of the data.\n#Names are unique. (count=unique=891)\n#Sex variable takes two values and is 65% male.. (top=male, freq=577\/count=891).\n#Cabin values have duplicate data, several passengers may have shared the same cabin.\n#Embarked takes three different values. Port S is the most used.(top=S)\n#The ticket feature has a high rate (22%) of duplicate values. (unique=681)\n\n#Statistics of numerical data\nprint(train_df.describe())\n\n#Categorical data\nprint(train_df.describe(include=['O']))","282fbed6":"#When we look at the survival rates according to PClass, the highest survival rate is pclass=1 while the lowest pclass=3.\n#So passenger class has to do with survival.\ntrain_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","b5db7683":"#If we look at the survival rate by sex, we really see that the survival rate of women is higher than that of men.\ntrain_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","54407c2e":"#The survival rate does not appear to be directly related to SipSp.\ntrain_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","2ff13d93":"#Again, we can say that survival is not directly related to the number of parents\/children on board.\ntrain_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","d9a1b3ed":"#We can combine multiple features to find correlations using a single plot.\n\ngrid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\n#grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();\n\n\n#Pclass=3 had many passengers, but most did not survive.\n#Infant passengers in Pclass=2 and Pclass=3 mostly survived.\n#Most passengers in Pclass=1 survived. Our assumptions are correct\n#We can use Pclass for model training.","63c0f328":"#Let's examine the survival of males and females according to pclass and embarked feautures.\n\n#Survival rates for female passengers are much higher than for males.\n#Men with Embarked=C have a higher survival rate.\n#There may be a correlation between Pclass and Embarked and Pclass with Survived and not a direct correlation with Embarked.\n\ngrid = sns.FacetGrid(train_df, col='Embarked')\n#grid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","d28afcf5":"#Ticket information is useless when analyzing, Cabin information contains too many null values.\n#So we're dropping these two columns.\nprint(\"Before\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\n\n\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape","094a0e72":"#We create a new column named Title by taking only the title part from the Name attribute.\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","34d98d9a":"#Many titles occur only once. We can classify them as 'Rare'.\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","e3397824":"#Let's change the Title variable from categorical to ordinal.\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()","641a0209":"#We no longer need the Name and PassengerId attributes.\ntrain_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","272670eb":"#Let's change the Sex property to female=1 and male=0.\nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","2a263921":"grid = sns.FacetGrid(train_df, col='Pclass', hue='Sex')\n#grid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","6cc5bbe6":"#Let's create an empty array to contain estimated Age values based on Pclass x Gender combinations.\nguess_ages = np.zeros((2,3))\nguess_ages","3c486fc9":"#Let's calculate the Age values \u200b\u200bfor the six combinations.\nfor dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            #round\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()","fe834ae0":"#Let's create AgeBand determine correlations with Survived.\ntrain_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","d4d1cb9c":"#Let's update the Age values according to these ranges.\nfor dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_df.head()","acef6fa2":"#We have nothing to do with Ageband, let's drop it.\ntrain_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","99cd6ece":"#Let's combine Parch and SibSp to create a new FamilySize property. This will allow us to exclude Parch and SibSp from our datasets.\nfor dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","9bed3dc7":"#Let's create another property called IsAlone using FamilySize.\nfor dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","ca966980":"#Let's drop Parch, SibSp and FamilySize from our dataset\ntrain_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","e8586292":"#Embarked feature takes S, Q, C values based on embarkation port. Our training dataset has two missing values. Let's fill them with the most common values.\nfreq_port = train_df.Embarked.dropna().mode()[0]","7705d398":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","6b9c8d82":"#Let's change the Embarked property to numeric.\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()","e76b24e1":"#Let's complete the Fare feature in the test data with the median.\ntest_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()","7b86868d":"#Let's assign the Fare as the range to a new attribute called FareBand.\ntrain_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","c26581a3":"#Let's convert the Fare feature to ordinal values based on FareBand.\nfor dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head(10)","513b1699":"#E\u011fitim setimin de\u011fi\u015fkenlerini X_train, \u00e7\u0131kt\u0131y\u0131 Y_train de\u011fi\u015fkenlerine at\u0131yorum. \nX_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","d26ce602":"# Logistic Regression\n#We create an object named logreg from the LogisticRegression class.\nlogreg = LogisticRegression()\n#We train the model with our variable and output datasets.\nlogreg.fit(X_train, Y_train)\n#We predict the data in the test dataset.\nY_pred = logreg.predict(X_test)\n\n#Model score is %78\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","f978b1f0":"#Let's look at the coefficients of each variable in the logistic regression.\n#The most impacting variable on survival is Sex, next Pclass\ncoeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","7696bb2d":"# Support Vector Machines\n#First we create an object from the SVC class.\n#We train the model with our training set and make predictions with the test data.\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\n#Model score is %83\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","c1426382":"svc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025\n    }","ae639999":"# Support Vector Machines\n#First we create an object from the SVC class.\n#We train the model with our training set and make predictions with the test data.\nsvc_params = SVC(**svc_params)\nsvc_params.fit(X_train, Y_train)\nY_pred = svc_params.predict(X_test)\n#Model score is %83\nsvc_params = round(svc_params.score(X_train, Y_train) * 100, 2)\nsvc_params","a5fb683e":"acc_knn = []\nfor i in range(1,10):\n #We create a new object by selecting neighbor parameter i, distance function, minkowski from class KNeighborsClassifier.\n #Between the number of neighbors 1 and 10, this model will work and we will find the number of clusters that give the highest prediction score.\n    knn = KNeighborsClassifier(n_neighbors=i ,metric=\"minkowski\")\n    #We did the training and predict process.\n    knn.fit(X_train,Y_train)\n    Y_pred = knn.predict(X_test)\n    acc_knn.append(knn.score(X_train,Y_train))","43e509da":"#We calculated the highest score as 85 for cluster number 5.\n#KNN score is higher than Logistic regression.\nmax_acc_knn = max(acc_knn)\ntrain_scores_ind = [i for i, v in enumerate(acc_knn) if v == max_acc_knn]\nprint('Max train score {} % and k = {}'.format(max_acc_knn*100,list(map(lambda x: x+1, train_scores_ind))))","b47f2f68":"# Gaussian Naive Bayes\n#We did the training and predict in Naive Bayes again.\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\n#This model's score is 76%\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","bc904f15":"# Perceptron\n#Perceptron is also used in the same way.\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\n#This model's score is %78\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","0156b7ca":"# Decision Tree\n#The model's score is the highest among the models we've ever written.\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","7d2a5c95":"#When training a machine learning algorithm, we need to specify some parameters.\n#These parameters that change according to the problem are called hyperparameters. These parameters direct the behavior of the algorithm during the learning process.\n#Determining the hyperparameters is a work of art. It requires mastering the theory of algorithms and parameters, and experiencing a lot of the effects of the changed parameters.\n#But python has some libraries to set these parameters, one of them is GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n#For this algorithm, we need to create a dictionary that determines which values to try for which parameters.\nparams = {\n    \"criterion\":(\"gini\", \"entropy\"), \n    \"splitter\":(\"best\", \"random\"), \n    \"max_depth\":(list(range(1, 20))), \n    \"min_samples_split\":[2, 3, 4], \n    \"min_samples_leaf\":list(range(1, 20)), \n}\n\n#GridSearchCv uses Cross Validation to evaluate parameters more reliably.\n#Runs the algorithm for each probability.\n\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_cv = GridSearchCV(tree_clf, params, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=3)\ntree_cv.fit(X_train, Y_train)\nbest_params = tree_cv.best_params_\nprint(f\"Best paramters: {best_params})\")\n# Algoritm says that DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6, min_samples_leaf=1, min_samples_split=2, presort=False, random_state=None,splitter='random')\n# is the best parameteres.","02939335":"decisionTree_best = DecisionTreeClassifier(**best_params)\ndecisionTree_best.fit(X_train, Y_train)\nY_pred = decisionTree_best.predict(X_test)\ndecisionTree_best.score(X_train, Y_train)\n#The model's score is the same as the decision tree and is the highest among the models we've written so far.\ndecisionTree_best = round(decisionTree_best.score(X_train, Y_train) * 100, 2)\ndecisionTree_best","eeb2a2c2":"# Random Forest\n#We are generating a RandomForest object that does the splits according to the gini index.\nrandom_forest = RandomForestClassifier(criterion='gini',n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\n#The model's score is the same as the decision tree and is the highest among the models we've written so far.\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","8d586715":"rf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n","9f2a9620":"random_forest = RandomForestClassifier(**rf_params)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\n#The model's score is the same as the decision tree and is the highest among the models we've written so far.\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","634bb46b":"#One of the libraries used for hypermetparameter is RandomizedSearchCV.\n#This algorithm is very useful when we have many parameters to try and the training time is very long.\n#While GridSearch tries all the possibilities, a hyperparameter set is randomly selected in this model and tested by establishing the model with cross-validation.\n#These steps continue until the specified calculation time limit or the number of iterations is reached.\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num=11)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n\nrandom_grid = {'n_estimators': n_estimators, 'max_features': max_features,\n               'max_depth': max_depth, 'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}\n\nrf_clf = RandomForestClassifier(random_state=42)\n\nrf_cv = RandomizedSearchCV(estimator=rf_clf, scoring='f1',param_distributions=random_grid, n_iter=100, cv=3, \n                               verbose=2, random_state=42, n_jobs=-1)\n\nrf_cv.fit(X_train, Y_train)\nrf_best_params = rf_cv.best_params_\nprint(f\"Best paramters: {rf_best_params})\")","3559d3c4":"random_forest_best = RandomForestClassifier(**rf_best_params)\nrandom_forest_best.fit(X_train, Y_train)\nY_pred = random_forest_best.predict(X_test)\nrandom_forest_best.score(X_train, Y_train)\n#The model's score is the same as the decision tree and is the highest among the models we've written so far.\nacc_random_forest_best = round(random_forest_best.score(X_train, Y_train) * 100, 2)\nacc_random_forest_best","aa7bf8fe":"from sklearn.ensemble import ExtraTreesClassifier\nextraTree = ExtraTreesClassifier()\nextraTree.fit(X_train, Y_train)\nY_pred = extraTree.predict(X_test)\nextraTree.score(X_train, Y_train)\n#The model's score is the same as the decision tree and is the highest among the models we've written so far.\nextraTree = round(extraTree.score(X_train, Y_train) * 100, 2)\nextraTree","6f76b78e":"et_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\nextraTree_params = ExtraTreesClassifier(**et_params)\nextraTree_params.fit(X_train, Y_train)\nY_pred = extraTree_params.predict(X_test)\nextraTree_params.score(X_train, Y_train)\n#The model's score is the same as the decision tree and is the highest among the models we've written so far.\nextraTree_params = round(extraTree_params.score(X_train, Y_train) * 100, 2)\nextraTree_params","33499b4c":"from sklearn.ensemble import AdaBoostClassifier\nadaBoost = AdaBoostClassifier()\nadaBoost.fit(X_train, Y_train)\nY_pred = adaBoost.predict(X_test)\nadaBoost.score(X_train, Y_train)\n#The model's score is the same as the decision tree and is the highest among the models we've written so far.\nadaBoost = round(adaBoost.score(X_train, Y_train) * 100, 2)\nadaBoost","932d63af":"ada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}","3eae2aad":"adaBoost_params = AdaBoostClassifier(**ada_params)\nadaBoost_params.fit(X_train, Y_train)\nY_pred = adaBoost_params.predict(X_test)\nadaBoost_params.score(X_train, Y_train)\n#The model's score is the same as the decision tree and is the highest among the models we've written so far.\nadaBoost_params = round(adaBoost_params.score(X_train, Y_train) * 100, 2)\nadaBoost_params","3b18d14a":"from sklearn.ensemble import GradientBoostingClassifier\ngradientBoost = GradientBoostingClassifier()\ngradientBoost.fit(X_train, Y_train)\nY_pred = gradientBoost.predict(X_test)\ngradientBoost.score(X_train, Y_train)\n#The model's score is the same as the decision tree and is the highest among the models we've written so far.\ngradientBoost = round(gradientBoost.score(X_train, Y_train) * 100, 2)\ngradientBoost","70934203":"gb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n","15cf1723":"from sklearn.ensemble import GradientBoostingClassifier\ngradientBoost_params = GradientBoostingClassifier(**gb_params)\ngradientBoost_params.fit(X_train, Y_train)\nY_pred = gradientBoost_params.predict(X_test)\ngradientBoost_params.score(X_train, Y_train)\n#The model's score is the same as the decision tree and is the highest among the models we've written so far.\ngradientBoost_params = round(gradientBoost_params.score(X_train, Y_train) * 100, 2)\ngradientBoost_params","0bf318e1":"import xgboost as xgb\ngbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1)\ngbm.fit(X_train, Y_train)\nY_pred = gbm.predict(X_test)\ngbm.score(X_train, Y_train)\n#The model's score is the same as the decision tree and is the highest among the models we've written so far.\ngbm = round(gbm.score(X_train, Y_train) * 100, 2)\ngbm","5eec5704":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines','Support Vector Machines Params', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Decision Tree','Decision Tree Best','Random Forest Best','ExtraTreesClassifier','ExtraTreesClassifier Param','AdaBoostClassifier','AdaBoostClassifier Params',\n             'GradientBoostingClassifier','GradientBoostingClassifier Params','XGBClassifier'],\n    'Score': [acc_svc,svc_params, max_acc_knn*100, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, acc_decision_tree,decisionTree_best,acc_random_forest_best,extraTree,extraTree_params,adaBoost,adaBoost_params,\n             gradientBoost,gradientBoost_params,gbm]})\nmodels.sort_values(by='Score', ascending=False)","5c73711a":" Our assumption is correct, age has something to do with survival, we should consider the Age feauture in model training.\n- We have to complete the Age feature for missing values.\n- We should label the Age feature as range.","40cf9d59":"# Titanic Dataset Analysis\n\n\n\n>Based on training data listing passengers who survived or died from the Titanic disaster, we will try to predict whether passengers in a test dataset that does not contain survival information survive.","2a524656":"## KNN or k-Nearest Neighbors ##\n\nThe KNN algorithm is an algorithm that classifies a data to be classified according to its proximity with previous data.\nDifferent distance functions such as Euclidian, Manhattan, Minkowski are used to calculate distance.","b1a0bd80":"Let's complete the properties with missing values.\nHere we will use other correlated properties to estimate missing values.\nWe can take advantage of the correlation between Age, Gender and PClass class.\nLet's estimate the Age values \u200b\u200bfor the Pclass and Gender feature combinations using the median values of the Age property.\nFor example, Pclass=1 and Gender=0, median Age for Pclass=1 and Gender=1, etc...","6f0f29c6":"We want to know how much each feauture relates to survival. The following inferences can be made from the data we have examined so far:\n\u200b\n\u200b\n1. Survived may be related to age feature. So we have to fill in the missing data in the Age feature.\n\u200b\n\u200b\n2. It can be dropped from our data as the Ticket feature has a duplicate rate (22%) and there may be no correlation between Ticket and survival. \n\n3. The Cabin property can be dropped because it contains a large number of null values in both the training and test dataset.\n4. PassengerId can be dropped from the training dataset as it is not related to survival.\n5. The Name attribute does not directly contribute to survivability, so it can be dropped.\n\u200b\n\u200b\n6. We can create a new feature called Family based on Parch and SibSp to find the total number of family members on board.\n7. We can get the titles of the passengers from the Name feature and create a new feature.\n8. We can create a new feature for age groups. This converts a continuous numeric feature into an ordered categorical feature.\n9. We can also create a fare band property from the Fare property.\n\u200b\n\u200b\nWe can make assumptions like\n* Women are more likely to survive,\n* Children are more likely to survive,\n* Upper class passengers (Pclass=1) are more likely to survive.","7078d091":"### Random Forest ##\n\nRandom Forest, as the name suggests, is simply that the algorithm randomly creates a forest. There is a direct relationship between the number of trees in the algorithm and the result it can achieve. As the number of trees increases, we get a precise result.\nThe difference between the Random Forest algorithm and the Decision Tree algorithm is that finding the root node and splitting the nodes work randomly in the Random Forest.","380faa72":"## Model, prediction and solution\n\nWe are now ready to train a model and predict the required solution. There are more than 60 predictive modeling algorithms to choose from. We need to understand the type of problem and the need for a solution to narrow it down to a few models we can consider. Our problem is a classification and regression problem. We want to determine the relationship between the output (Survive or not) and the variables. When we train our model with a specific dataset, we are doing supervised learning. With these two criteria, we can narrow our model selection down to a few with Classification and Regression, one of the Supervised Learning methods. These:\n\n- Logistic Regression\n- KNN or k-Nearest Neighbors\n- Support Vector Machines\n- Naive Bayes Classifier\n- Decision Tree\n- Random Forrest\n- Perceptron\n- Artificial Neural Network\n- RVM or Relevance Vector Machine","7854fc2e":"## Perceptron ##\n\nPerceptron is an artificial neural network unit that performs calculations to detect features in the input data.\nThe algorithm helps us to make optimal use of the weighting coefficients in the neural network. Once the optimized weight coefficients are found, the input properties are multiplied by these weights and inferred to determine whether a nerve should be stimulated.","a986e76c":"## Gaussian Naive Bayes ##\n\nNaive Bayes classifier is based on Bayes theorem. It can also work on unstable datasets. The way the algorithm works is it calculates the probability of each state for an element and classifies it according to the highest probability value. It can produce very successful works with a little training data.","dcc5e3a9":"## Support Vector Machines ##\n\nA support vector machine is a vector space based machine learning method that finds a decision boundary between the two classes that are furthest from any point in the training data.","78caa1f9":"## Logistic Regression ##\n\nLogistic regression measures the relationship between a categorical dependent variable and one or more independent variables (features) by estimating probabilities using a logistic function that is a cumulative logistic distribution.\n* Estimation can be made by calculating the coefficient of features with logistic regression.","3df73e5e":"## Decision Tree ##\n\nA decision tree uses a large number of records to divide a dataset into smaller clusters by applying a set of decision rules. In other words, it is a structure used by dividing large amounts of records into very small record groups by applying simple decision-making steps","9000e94a":"## Model evaluation\n\nWe can now rank our evaluation of all models to choose the best one for our problem. While both Desicion Tree and Random Forest score the same, we choose to use Random Forest as they correct the overfitting of the decision trees to the training sets."}}