{"cell_type":{"3acb2bdc":"code","12600b7c":"code","7c99ca7b":"code","a838c611":"code","a5845e3a":"code","f9f807dc":"code","598a7f9d":"code","dd05bce3":"code","7de53e03":"code","84995cc3":"code","802df223":"code","7c87259c":"code","e4155510":"code","e2bd1615":"code","a4da2836":"code","ec4bf808":"code","8657c3bf":"code","b6c887c0":"code","cfdf8481":"code","238bf82b":"code","f437b62e":"code","bb63191f":"code","970e3e06":"code","bf0a3a6e":"code","0ead99bf":"code","1f3d8fb9":"code","3f040632":"code","9022755d":"code","d5d2a618":"code","a3f1b6b1":"code","b7f1053e":"code","c9d89f5d":"code","8e856342":"code","d4208e22":"code","3266af2a":"code","ebc6c86f":"code","474171b7":"code","b11b2fd7":"code","c3dda660":"code","af33fb11":"code","8a8a9f1a":"code","871bc209":"code","43e7427e":"code","9b6d73a8":"code","3cd328bd":"code","e1c227e4":"code","d83c4d86":"code","7dd1c5bc":"code","b82e40e2":"code","cb008fcc":"code","4e0f5f6e":"code","aed3042c":"code","d9b72b17":"code","c3138581":"code","9036b3fd":"code","7675d9e8":"code","c5041ff4":"code","442086d4":"code","df8e8753":"code","750042e6":"code","2e0b03a9":"code","48b0414c":"code","daf41c00":"code","1f9bc16a":"code","1b85089b":"code","9e5bbb76":"code","c00fee46":"code","7466414d":"code","fd1276c8":"code","a7428a58":"code","6fc93959":"code","e3eb44da":"code","2234ff15":"code","6d14854e":"code","6cf782f1":"code","36d2254a":"markdown","f51e710b":"markdown","6c8d0fec":"markdown","79dc107a":"markdown","ff91daa0":"markdown","03f1872e":"markdown","a7250f4d":"markdown","27c93248":"markdown","8298f454":"markdown","d80adeac":"markdown","82fb33b6":"markdown","67de3317":"markdown","1880b693":"markdown","3c6ce293":"markdown","714178b1":"markdown","196dc43a":"markdown","8c11890c":"markdown","7a63fe69":"markdown","609113cb":"markdown","335d333e":"markdown","c933a4a9":"markdown","221af896":"markdown","dcd07135":"markdown","9d370c3e":"markdown","f631f846":"markdown","53e01b0e":"markdown","35322282":"markdown","0993bdf7":"markdown","e13e69cc":"markdown"},"source":{"3acb2bdc":"import numpy as np \nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","12600b7c":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport gc\nfrom tqdm import tqdm_notebook\nfrom itertools import product\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.svm import LinearSVR\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import AdaBoostRegressor,GradientBoostingRegressor,RandomForestRegressor,VotingRegressor,StackingRegressor\nfrom sklearn.model_selection import KFold,train_test_split\nimport lightgbm as lgb\nimport tensorflow as tf\nimport keras","7c99ca7b":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    return df","a838c611":"train_data=pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntrain_data.head()","a5845e3a":"test_data=pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\ntest_data.head()","f9f807dc":"print(train_data.shape,test_data.shape)","598a7f9d":"print(train_data['shop_id'].nunique(),test_data['shop_id'].nunique())\nl=train_data['shop_id'].unique()\nfor i in test_data['shop_id'].unique():\n    if i not in l:\n        print(i)","dd05bce3":"print(train_data['item_id'].nunique(),test_data['item_id'].nunique())\nl=train_data['item_id'].unique()\nc=0\nfor i in test_data['item_id'].unique():\n    if i not in l:\n        c+=1\nprint('test item id not in train',c)\nl=test_data['item_id'].unique()\nc=0\nfor i in train_data['item_id'].unique():\n    if i not in l:\n        c+=1\nprint('train item id not in test',c)","7de53e03":"items=pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nprint(items.shape)","84995cc3":"items.head()","802df223":"items['item_id'].nunique()","7c87259c":"item_dict={}\nfor i in range(items.shape[0]):\n    itid=items.loc[i,'item_id']\n    itcat=items.loc[i,'item_category_id']\n    item_dict[itid]=itcat","e4155510":"# Downcasting to 16 bit integer\ntrain_data['item_cnt_day'] = train_data['item_cnt_day'].astype('int16')","e2bd1615":"test_data.isnull().sum()","a4da2836":"train_data.isnull().sum()","ec4bf808":"train_data['date_block_num'].value_counts()","8657c3bf":"train_data['item_cnt_day'].value_counts()","b6c887c0":"sns.boxplot(train_data['item_cnt_day'])","cfdf8481":"train_data['item_price'].value_counts()","238bf82b":"sns.boxplot(train_data['item_price'])","f437b62e":"train_data.describe()","bb63191f":"fig = plt.figure(figsize=(18,9))\nplt.subplots_adjust(hspace=.5)\n\nplt.subplot2grid((3,3), (0,0), colspan = 3)\ntrain_data['shop_id'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('Shop ID Values in the Training Set (Normalized)')\n\nplt.subplot2grid((3,3), (1,0))\ntrain_data['item_id'].plot(kind='hist', alpha=0.7)\nplt.title('Item ID Histogram')\n\nplt.subplot2grid((3,3), (1,1))\ntrain_data['item_price'].plot(kind='hist', alpha=0.7, color='orange')\nplt.title('Item Price Histogram')\n\nplt.subplot2grid((3,3), (1,2))\ntrain_data['item_cnt_day'].plot(kind='hist', alpha=0.7, color='green')\nplt.title('Item Count Day Histogram')\n\nplt.subplot2grid((3,3), (2,0), colspan = 3)\ntrain_data['date_block_num'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('Month (date_block_num) Values in the Training Set (Normalized)')\n\nplt.show()","970e3e06":"print(np.percentile(train_data['item_cnt_day'],99.99))\nprint(np.percentile(train_data['item_price'],99.99))","bf0a3a6e":"train_data = train_data[train_data['item_cnt_day'] < 100]\ntrain_data = train_data[train_data['item_price'] < 30000]","0ead99bf":"train_data[train_data['item_price']<0]","1f3d8fb9":"median = train_data[(train_data.shop_id==32)&(train_data.item_id==2973)&(train_data.date_block_num==4)&(train_data.item_price>0)].item_price.median()\ntrain_data.loc[train_data.item_price<0, 'item_price'] = median","3f040632":"train_data[train_data['item_cnt_day']<0]","9022755d":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ntrain_data.loc[train_data.shop_id == 0, 'shop_id'] = 57\ntest_data.loc[test_data.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ntrain_data.loc[train_data.shop_id == 1, 'shop_id'] = 58\ntest_data.loc[test_data.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ntrain_data.loc[train_data.shop_id == 10, 'shop_id'] = 11\ntest_data.loc[test_data.shop_id == 10, 'shop_id'] = 11","d5d2a618":"item_categories=pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nshops=pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')","a3f1b6b1":"shops.head()","b7f1053e":"item_categories.head()","c9d89f5d":"shops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]\n\nitem_categories['split'] = item_categories['item_category_name'].str.split('-')\nitem_categories['type'] = item_categories['split'].map(lambda x: x[0].strip())\nitem_categories['type_code'] = LabelEncoder().fit_transform(item_categories['type'])\n# if subtype is nan then type\nitem_categories['subtype'] = item_categories['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\nitem_categories['subtype_code'] = LabelEncoder().fit_transform(item_categories['subtype'])\nitem_categories = item_categories[['item_category_id','type_code', 'subtype_code']]\n\nitems.drop(['item_name'], axis=1, inplace=True)","8e856342":"import time","d4208e22":"ts = time.time()\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train_data[train_data.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)\ntime.time()-ts","3266af2a":"train_data['revenue'] = train_data['item_price'] *  train_data['item_cnt_day']","ebc6c86f":"ts = time.time()\ngroup = train_data.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))\ntime.time() - ts","474171b7":"test_data['date_block_num'] = 34\ntest_data['date_block_num'] = test_data['date_block_num'].astype(np.int8)\ntest_data['shop_id'] = test_data['shop_id'].astype(np.int8)\ntest_data['item_id'] = test_data['item_id'].astype(np.int16)","b11b2fd7":"ts = time.time()\nmatrix = pd.concat([matrix, test_data], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month\ntime.time() - ts","c3dda660":"ts = time.time()\nmatrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, item_categories, on=['item_category_id'], how='left')\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\ntime.time() - ts","af33fb11":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","8a8a9f1a":"ts = time.time()\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')\ntime.time() - ts","871bc209":"ts = time.time()\ngroup = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_avg_item_cnt')\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","43e7427e":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_item_avg_item_cnt')\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","9b6d73a8":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","3cd328bd":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\nmatrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","e1c227e4":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_cat_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmatrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_cat_avg_item_cnt')\nmatrix.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","d83c4d86":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_type_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'type_code'], how='left')\nmatrix['date_shop_type_avg_item_cnt'] = matrix['date_shop_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_type_avg_item_cnt')\nmatrix.drop(['date_shop_type_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","7dd1c5bc":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nmatrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_subtype_avg_item_cnt')\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","b82e40e2":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'city_code'], how='left')\nmatrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_city_avg_item_cnt')\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","cb008fcc":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'city_code'], how='left')\nmatrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_item_city_avg_item_cnt')\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","4e0f5f6e":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_type_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'type_code'], how='left')\nmatrix['date_type_avg_item_cnt'] = matrix['date_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_type_avg_item_cnt')\nmatrix.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","aed3042c":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_subtype_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'subtype_code'], how='left')\nmatrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_subtype_avg_item_cnt')\nmatrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","d9b72b17":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_subtype_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'subtype_code'], how='left')\nmatrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_subtype_avg_item_cnt')\nmatrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","c3138581":"matrix['month'] = matrix['date_block_num'] % 12","9036b3fd":"ts = time.time()\nmatrix = matrix[matrix.date_block_num > 11]\ntime.time() - ts","7675d9e8":"ts = time.time()\ndef fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = fill_na(matrix)\ntime.time() - ts","c5041ff4":"matrix.to_pickle('data.pkl')\ndel matrix\ndel group\ndel items\ndel shops\ndel item_categories\ndel train_data\ngc.collect();","442086d4":"data = pd.read_pickle('data.pkl')","df8e8753":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\ny_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\ny_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","750042e6":"del data\ngc.collect();","2e0b03a9":"lr = LinearRegression()\nlr.fit(X_train.values, y_train)\npred_lr = lr.predict(X_valid.values).clip(0,20)\ntest_lr = lr.predict(X_test.values).clip(0,20)\nprint('Test R-squared for linreg is %f' % r2_score(y_valid, pred_lr))\nprint('Test rmse for linreg is %f' % mean_squared_error(y_valid, pred_lr,squared=False))\nnp.save('lr.npy',pred_lr)\nnp.save('lr_test.npy',test_lr)","48b0414c":"lgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.03, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':0 \n              }\n\nmodel = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)\npred_lgb = model.predict(X_valid).clip(0,20)\ntest_lgb = model.predict(X_test).clip(0,20)\nprint('Test R-squared for LightGBM is %f' % r2_score(y_valid, pred_lgb))\nprint('Test rmse for LightGBM is %f' % mean_squared_error(y_valid, pred_lgb,squared=False))\nnp.save('lgb.npy',pred_lgb)\nnp.save('lgb_test.npy',test_lgb)","daf41c00":"svc = LinearSVR(verbose=1)\nsvc.fit(X_train, y_train)\npred_svc = svc.predict(X_valid).clip(0,20)\ntest_svc = svc.predict(X_test).clip(0,20)\nprint('Test R-squared for xgboost is %f' % r2_score(y_valid, pred_svc))\nprint('Test rmse for xgboost is %f' % mean_squared_error(y_valid, pred_svc,squared=False))\nnp.save('svc.npy',pred_svc)\nnp.save('svc_test.npy',test_svc)","1f9bc16a":"ridge = RidgeCV()\nridge.fit(X_train, y_train)\npred_ridge = ridge.predict(X_valid).clip(0,20)\ntest_ridge = ridge.predict(X_test).clip(0,20)\nprint('Test R-squared for xgboost is %f' % r2_score(y_valid, pred_ridge))\nprint('Test rmse for xgboost is %f' % mean_squared_error(y_valid, pred_ridge,squared=False))\nnp.save('ridge.npy',pred_ridge)\nnp.save('ridge_test.npy',test_ridge)","1b85089b":"xgb = XGBRegressor(max_depth=8,n_estimators=20,min_child_weight=300, colsample_bytree=0.8, subsample=0.8, eta=0.3)\nxgb.fit(X_train, y_train, eval_metric=\"rmse\", eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=True)\npred_xgb = xgb.predict(X_valid).clip(0,20)\ntest_xgb = xgb.predict(X_test).clip(0,20)\nprint('Test R-squared for xgboost is %f' % r2_score(y_valid, pred_xgb))\nprint('Test rmse for xgboost is %f' % mean_squared_error(y_valid, pred_xgb,squared=False))\nnp.save('xgb.npy',pred_xgb)\nnp.save('xgb_test.npy',test_xgb)","9e5bbb76":"grad = GradientBoostingRegressor(criterion='mse',n_estimators=20,subsample=0.8,min_samples_split=100,max_depth=8,verbose=2)\ngrad.fit(X_train, y_train)\npred_grad = grad.predict(X_valid).clip(0,20)\ntest_grad = grad.predict(X_test).clip(0,20)\nprint('Test R-squared for gardient boost is %f' % r2_score(y_valid, pred_grad))\nprint('Test rmse for gradient boost is %f' % mean_squared_error(y_valid, pred_grad,squared=False))\nnp.save('grad.npy',pred_grad)\nnp.save('grad_test.npy',test_grad)","c00fee46":"rand = RandomForestRegressor(n_estimators=50,min_samples_split=20,max_depth=8,verbose=2)\nrand.fit(X_train, y_train)\npred_rand = rand.predict(X_valid).clip(0,20)\ntest_rand = rand.predict(X_test).clip(0,20)\nprint('Test R-squared for random forest is %f' % r2_score(y_valid, pred_rand))\nprint('Test rmse for random forest is %f' % mean_squared_error(y_valid, pred_rand,squared=False))\nnp.save('rand.npy',pred_rand)\nnp.save('rand_test.npy',test_rand)","7466414d":"import tensorflow as tf\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam","fd1276c8":"nn_model=Sequential()\nnn_model.add(Dense(1024,activation='relu'))\nnn_model.add(Dense(1024,activation='relu'))\nnn_model.add(Dense(256,activation='relu'))\nnn_model.add(Dense(256,activation='relu'))\nnn_model.add(Dense(1,activation='relu'))\nnn_model.compile(optimizer=Adam(lr=0.0000001),loss='mean_squared_error',metrics=['mean_squared_error'])","a7428a58":"nn_model.fit(X_train,y_train,epochs=2,batch_size=512,validation_data=(X_valid,y_valid))\npred_nn=nn_model.predict(X_valid).clip(0,20)\ntest_nn=nn_model.predict(X_test).clip(0,20)\nprint('Test R-squared for neural network is %f' % r2_score(y_valid, pred_nn))\nprint('Test rmse for neural network is %f' % mean_squared_error(y_valid, pred_nn,squared=False))\nnp.save('nn.npy',pred_nn)\nnp.save('nn_test.npy',test_nn)","6fc93959":"pred_lr=np.load('\/kaggle\/input\/predictions\/lr.npy')\npred_lgb=np.load('\/kaggle\/input\/predictions\/lgb.npy')\npred_svc=np.load('\/kaggle\/input\/predictions\/svc.npy')\npred_ridge=np.load('\/kaggle\/input\/predictions\/ridge.npy')\npred_xgb=np.load('\/kaggle\/input\/predictions\/xgb.npy')\npred_grad=np.load('\/kaggle\/input\/predictions\/grad.npy')\npred_rand=np.load('\/kaggle\/input\/predictions\/rand.npy')\npred_nn=np.load('\/kaggle\/input\/predictions\/nn.npy')","e3eb44da":"test_lr=np.load('\/kaggle\/input\/predictions\/lr_test.npy')\ntest_lgb=np.load('\/kaggle\/input\/predictions\/lgb_test.npy')\ntest_svc=np.load('\/kaggle\/input\/predictions\/svc_test.npy')\ntest_ridge=np.load('\/kaggle\/input\/predictions\/ridge_test.npy')\ntest_xgb=np.load('\/kaggle\/input\/predictions\/xgb_test.npy')\ntest_grad=np.load('\/kaggle\/input\/predictions\/grad_test.npy')\ntest_rand=np.load('\/kaggle\/input\/predictions\/rand_test.npy')\ntest_nn=np.load('\/kaggle\/input\/predictions\/nn_test.npy')","2234ff15":"from sklearn.model_selection import KFold\ndata_level2=np.c_[pred_lr,pred_lgb,pred_svc,pred_ridge,pred_xgb,pred_grad,pred_rand,pred_nn]\ntest_level2=np.c_[test_lr,test_lgb,test_svc,test_ridge,test_xgb,test_grad,test_rand,test_nn]\ny_valid=np.array(y_valid)\nprint(data_level2.shape,y_valid.shape)\nprint(test_level2.shape,X_test.shape)","6d14854e":"kf = KFold(n_splits=5)\ni=0\ntest=np.zeros(test_lgb.shape)\nfor train_index, val_index in kf.split(data_level2,y_valid):\n    X_train, X_val = data_level2[train_index], data_level2[val_index]\n    y_train, y_val = y_valid[train_index], y_valid[val_index]\n    lr = XGBRegressor(max_depth=5,n_estimators=50,min_child_weight=50, colsample_bytree=0.8, subsample=0.8, eta=0.3)\n    lr.fit(X_train, y_train,eval_metric=\"rmse\",eval_set=[(X_train, y_train), (X_val, y_val)], verbose=True)\n    val = lr.predict(X_val).clip(0,20)\n    test_lr = lr.predict(test_level2).clip(0,20)\n    i+=1\n    print('Iteration = %f' % i)\n    print('Test R-squared for linreg is %f' % r2_score(y_val, val))\n    print('Test rmse for linreg is %f' % mean_squared_error(y_val, val,squared=False))\ntest=test\/5\nnp.save('test.npy',test)","6cf782f1":"submission = pd.DataFrame({\n    \"ID\": test_data.index, \n    \"item_cnt_month\": test\n})\nsubmission.to_csv('submission.csv', index=False)","36d2254a":"Creating level 2 features. We will ensemble them using xgboost on validation set from above data with K-Fold validation.","f51e710b":"# Ensembling","6c8d0fec":"Out of 5100 item_id in test data, 363 are not present in training data. We need to look at items.csv and item_categories.csv files.","79dc107a":"We will also try to make test data similar to train data.","ff91daa0":"Saving the final data for training and freeing up the memory","03f1872e":"Some shops are duplicates (according to name), so fixing them","a7250f4d":"Loading data","27c93248":"# Models","8298f454":"# Separating data into train and test sets and clipping target variable","d80adeac":"# Aggregating data and getting monthly features","82fb33b6":"### Since, maximum lag value is 12, we cannot use features for month<12. So, we will drop them.","67de3317":"**Some Observations**\n\n* Each shop_name starts with the city name.\n* Each category contains type and subtype in its name.","1880b693":"Month feature","3c6ce293":"Monthly item count and revenue","714178b1":"# Feature Generation","196dc43a":"We do not have null values in data","8c11890c":"We can see that one item has -9 as item count which is clearly wrong. We will deal with it later.","7a63fe69":"Adding generated features from shops, items and categories.","609113cb":"We can see one item has negative price. We will replace it with its median value.","335d333e":"## Lag feature generation","c933a4a9":"Getting rid of item_cnt_day values above 100 and item_price above 30000 as they are outliers","221af896":"We will replace it with median.","dcd07135":"All shop_id for test data are present in training data.","9d370c3e":"# Mean Encoded feature generation along with lag","f631f846":"Appending test data to matrix to make test set similar to training set.","53e01b0e":"# EDA","35322282":"### Replacement of null values produced during lag feature generation\nWe simply replace them with 0.","0993bdf7":"These denote item returned, so we do not need to change these.","e13e69cc":"# Training"}}