{"cell_type":{"4165f869":"code","42389197":"code","902de68b":"code","1379d2c8":"code","63eab445":"code","73e24625":"code","dc7fbfc8":"code","9ae3f6b0":"code","837f21dc":"code","0d9b9258":"code","4d52ca1c":"code","cb907d29":"code","47c7c5e8":"code","9e60246c":"code","47a2b480":"code","99408f0f":"code","8ede1286":"code","5a4b2627":"code","a40ccbcd":"code","31c833d7":"code","9b9b2cb5":"code","45217ead":"code","182e9bb8":"code","ce81c1fd":"markdown","d91cb31a":"markdown","93b78093":"markdown","559fcfe3":"markdown"},"source":{"4165f869":"import numpy as np\nimport pandas as pd\nimport os\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n\n# keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom keras.optimizers import Adam\nfrom keras.losses import SparseCategoricalCrossentropy\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# sklearn library\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import classification_report ","42389197":"file = Path(\"..\/input\/a-large-scale-fish-dataset\/Fish_Dataset\/Fish_Dataset\") \nFile_Path = list(file.glob(r\"**\/*.png\"))  #getting a list of all image names\nLabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],File_Path))     # splitting image name into head and tail\n# using File_Path 'tail' as labels","902de68b":"# Creating Data Frame\n\nFile_Path = pd.Series(File_Path).astype(str)\nLabels = pd.Series(Labels)\nLabels.unique()","1379d2c8":"# Creating Dataframe\n\ndf = pd.concat([File_Path,Labels],axis=1)\ndf.columns = ['image', 'label']\n# removing images with GT label\ndf = df[df[\"label\"].apply(lambda x: x[-2:] != \"GT\")].reset_index(drop=True)","63eab445":"df.head()","73e24625":"# Viewing dataframe images\n\nfig, axes = plt.subplots(nrows=3,ncols=3, figsize=(7, 7),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\n# iterating over images\nfor name, ax, in enumerate(axes.flatten()):\n    ax.imshow(plt.imread(df.image[name]))   # reading images\n    ax.set_title(df.label[name])            #labelling images\n\nplt.show()","dc7fbfc8":"# splitting the dataset\n# specified random state for split to give same output\n\ntrain_set, test_data = train_test_split(df, test_size= 0.3, random_state = 42)\ntrain_data, val_data = train_test_split(train_set, test_size= 0.2, random_state = 42)\n\n\nprint(train_data.shape)\nprint(test_data.shape)\nprint(val_data.shape)","9ae3f6b0":"# resetting index\n\ntrain_data = train_data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)\nval_data = val_data.reset_index(drop=True)","837f21dc":"img_size = (224, 224)\ninput_shape = (224, 224, 3)","0d9b9258":"# image Generator\n\nimg_gen = ImageDataGenerator(    \n    rotation_range=10,\n    rescale=1.\/255,\n    horizontal_flip=True,\n)\n","4d52ca1c":"# example generator to check data augmentation parameterexample_data = train_data.sample(n=1).reset_index(drop=True)\n\nexample_data = train_data.sample(n=1).reset_index(drop=True)\n\nexample_gen = img_gen.flow_from_dataframe(\n    dataframe = example_data,\n    x_col = 'image', #name of the column containing the image in the test set\n    y_col ='label', #name of column containing the target in the test set\n    target_size = img_size,\n    class_mode ='categorical',\n    batch_size = 32,\n    shuffle = False # not to shuffle the given data\n)\n\nfor i in range(1, 10):\n    plt.subplot(3, 3, i)\n    for X_batch, Y_batch in example_gen:\n        image = X_batch[0]\n        plt.imshow(image)\n        plt.axis('Off')\n        break\n\nplt.tight_layout()  # for auto padding \nplt.show()","cb907d29":"# training generator\ntrain_gen = img_gen.flow_from_dataframe(\n    train_data, \n    x_col='image',\n    y_col='label',\n    target_size=img_size,\n    class_mode='categorical',\n    batch_size=32,\n    shuffle = False\n)\n\n# validation generator\nvalidation_gen = img_gen.flow_from_dataframe(\n    val_data, \n    x_col='image',\n    y_col='label',\n    target_size=img_size,\n    class_mode='categorical',\n    batch_size=32,\n    shuffle = False\n)","47c7c5e8":"# creating model\nmodel = Sequential()\n\n# Adding layers\n\n# input layer\nmodel.add(Conv2D(32, (3, 3), activation='relu', strides=(1, 1),input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n#flatten layer\nmodel.add(Flatten())\n\n# dense layers with dropout\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(rate = 0.3))\nmodel.add(Dense(64, activation = 'relu'))\n\n#output layer\nmodel.add(Dense(9, activation='softmax'))","9e60246c":"model.summary()","47a2b480":"# compile model\nmodel.compile(optimizer='adam',\n              loss=\"categorical_crossentropy\", \n              metrics=['accuracy'])","99408f0f":"# preventing overfitting\n\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n# reducing learning rate if model doesn't improve with patience=5\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=5, \n                                            verbose=1, \n                                            factor=0.2, \n                                            min_lr=0.0001)\n\nearlystop = EarlyStopping(patience=10)","8ede1286":"# fitting model\n\nhistory = model.fit(train_gen,\n                    validation_data = validation_gen,\n                    epochs = 20,\n                    verbose=1, #for animated bar during epoch\n                    callbacks = [earlystop, learning_rate_reduction])","5a4b2627":"# Loss vs Accuracy\n\n#accuracy\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\n#loss\nloss = history.history['loss'] \nval_loss = history.history['val_loss']","a40ccbcd":"plt.figure(figsize=(10, 5))\n\n# visualising Accuracy \nplt.subplot(2, 1, 1)\n\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\n\nplt.ylabel('Accuracy') \nplt.title('Training and Validation Accuracy') \n\n# Visualising Loss\nplt.subplot(2, 1, 2)\n\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\n\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\n\nplt.show()","31c833d7":"# test generator\n\ntest_gen = img_gen.flow_from_dataframe(\n    test_data, \n    x_col='image',\n    y_col='label',\n    target_size=img_size,\n    class_mode='categorical',\n    batch_size=32,\n    shuffle = False # not to shuffle the given data\n)","9b9b2cb5":"# Predict the label of the test_images\npred = model.predict(test_gen)\npred = np.argmax(pred,axis = 1) # pick the class with highest probability\n\n# labelling data\nlabels = (train_gen.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred2 = [labels[k] for k in pred]","45217ead":"from sklearn.metrics import classification_report\ny_test = test_data.label # expected output\nprint(classification_report(y_test, pred2)) ","182e9bb8":"# Display 15 picture of the dataset with their labels\nfig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15, 10),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\ncolor = \"blue\" if pred2[i] == test_data.label.iloc[i] else \"red\"\nfor i, ax ,in enumerate(axes.flat):\n    ax.imshow(plt.imread(test_data.image.iloc[i]))\n    ax.set_title(f\"True: {test_data.label.iloc[i]}\\nPredicted: {pred2[i]}\",color=color)\n    \nplt.subplots_adjust(hspace = 0.3)\nplt.suptitle(\"Model predictions (blue: correct, red: incorrect)\",y=0.98)\nplt.tight_layout()\nplt.show()","ce81c1fd":"# CNN model","d91cb31a":"# Pre-processing Data","93b78093":"# Data Augmentation and Image Generators","559fcfe3":"# Model Evaluation"}}