{"cell_type":{"45950953":"code","2ca119be":"code","cb4e16d5":"code","564ae46e":"code","bb94ac10":"code","a46e374f":"code","2ca60798":"code","1bbf0aa1":"code","1e47d8b7":"code","52157266":"code","40748906":"code","dda6f593":"code","f83ae979":"code","66a113d9":"code","abda6e7c":"code","53f2d34b":"code","409feb9c":"code","0b47abbc":"markdown","abe97231":"markdown","4a0801a4":"markdown","245a8681":"markdown","fe9d1abe":"markdown","cc5c9f23":"markdown","b1f426cb":"markdown"},"source":{"45950953":"## Imports (code & data)\nimport re\nimport pandas as pd\nimport yake_helper_funcs as yhf\nfrom datetime import datetime, timedelta\nfrom math import sqrt, floor\nfrom sklearn.cluster import SpectralClustering\nimport numpy as np\nimport itertools\nfrom matplotlib import pyplot as plt\nimport removing_polite_posts as rpp\nfrom flashtext.keyword import KeywordProcessor\nimport string\n\nforum_posts = pd.read_csv(\"..\/input\/meta-kaggle\/ForumMessages.csv\")\n\n# read in pre-tuned vectors\nvectors = pd.read_csv(\"..\/input\/fine-tuning-word2vec-2-0\/kaggle_word2vec.model\", \n                      delim_whitespace=True,\n                      skiprows=[0], \n                      header=None\n                     )\n\n# set words as index rather than first column\nvectors.index = vectors[0]\nvectors.drop(0, axis=1, inplace=True)","2ca119be":"## Utility functions\n\n# get vectors for each word in post\n# TODO: can we vectorize this?\ndef vectors_from_post(post):\n    all_words = [] \n\n    for words in post:\n        all_words.append(words) \n        \n    return(vectors[vectors.index.isin(all_words)])\n\n\n# create document embeddings from post\ndef doc_embed_from_post(post):\n    test_vectors = vectors_from_post(post)\n\n    return(test_vectors.mean())\n\n# explore our posts by cluster\ndef get_keyword_set_by_cluster(number):\n    cluster_index = list(clustering.labels_ == number)\n    return(list(itertools.compress(keyword_sets, cluster_index)))\n\n# get sample post info by #\ndef get_post_info_by_cluster(number, \n                             data,\n                             cluster):\n    return(data[cluster.labels_ == number])\n\n# remove HTML stuff\n# https:\/\/medium.com\/@jorlugaqui\/how-to-strip-html-tags-from-a-string-in-python-7cb81a2bbf44\ndef remove_html_tags(text):\n    clean = re.compile('<.*?>')\n    return(re.sub(clean, '', text))\n\n# remove \"good\", \"nice\", \"thanks\", etc\ndef remove_thanks(text):\n    text = text.lower()\n    \n    text = re.sub(\"nice\", \"\", text)\n    text = re.sub(\"thank.*\\s\", \" \", text)\n    text = re.sub(\"good\",\"\", text)\n    text = re.sub(\"hi\", \"\", text)\n    text = re.sub(\"hello\", \"\", text)\n    \n    return(text)\n\ndef polite_post_index(forum_posts):\n    '''Pass in a list of fourm posts, get\n    back the indexes of short, polite ones.'''\n    \n    polite_indexes = []\n    \n    # create  custom stop word list to identify polite forum posts\n    stop_word_list = [\"no problem\", \"thanks\", \"thx\", \"thank\", \"great\",\n                      \"nice\", \"interesting\", \"awesome\", \"perfect\", \n                      \"amazing\", \"well done\", \"good job\"]\n\n    # create a KeywordProcess\n    keyword_processor = KeywordProcessor()\n    keyword_processor.add_keywords_from_list(stop_word_list)\n\n    # test our keyword processor\n    for i,post in enumerate(forum_posts):\n        post = post.lower().translate(str.maketrans({a:None for a in string.punctuation}))\n        \n        if len(post) < 100:\n            keywords_found = keyword_processor.extract_keywords(post.lower(), span_info=True)\n            if keywords_found:\n                polite_indexes.append(i)\n\n    return(polite_indexes)","cb4e16d5":"## Hyperprameters\n\n# number of clusters currently based on the square root of the # of posts\ndays_of_posts = 1","564ae46e":"# For sample posts, get forum title and topic title\n# based on queries from https:\/\/www.kaggle.com\/pavlofesenko\/strategies-to-earn-discussion-medals\ntopics = pd.read_csv('..\/input\/meta-kaggle\/\/ForumTopics.csv').rename(columns={'Title': 'TopicTitle'})\nforums = pd.read_csv('..\/input\/meta-kaggle\/Forums.csv').rename(columns={'Title': 'ForumTitle'})\n\ndf1 = pd.merge(forum_posts[['ForumTopicId', 'PostDate', 'Message']], topics[['Id', 'ForumId', 'TopicTitle']], left_on='ForumTopicId', right_on='Id')\ndf1 = df1.drop(['ForumTopicId', 'Id'], axis=1)\n\nforum_posts = pd.merge(df1, forums[['Id', 'ForumTitle']], left_on='ForumId', right_on='Id')\nforum_posts = forum_posts.drop(['ForumId', 'Id'], axis=1)\nforum_posts.head()","bb94ac10":"# parse dates\nforum_posts['Date'] = pd.to_datetime(forum_posts.PostDate, format=\"%m\/%d\/%Y %H:%M:%S\")\n\n# posts from the last X days\nstart_time = datetime.now() + timedelta(days=-days_of_posts)  \n\n# forum posts from last week (remember to convert to str)\nsample_post_info = forum_posts.loc[forum_posts.Date > start_time]\nsample_posts = sample_post_info.Message.astype(str)\n\n# reindex from 0\nsample_posts.reset_index(drop=True)\nsample_post_info.reset_index(drop=True)\n\n# remove html tags\nsample_post_info.Message = sample_post_info.Message\\\n    .astype(str)\\\n    .apply(remove_html_tags)\nsample_posts = sample_posts.apply(remove_html_tags)\n\n# remove polite posts (make sure you remove HTML tags first)\npolite_posts = sample_posts.index[polite_post_index(sample_posts)]\n# posts aren't being dropped \nsample_posts = sample_posts.drop(polite_posts)\nsample_post_info = sample_post_info.drop(polite_posts)\n\n# number of posts\nnum_of_posts = sample_posts.shape[0]\n\n# Number of clusters is square root of the # of posts (rounded down)\nnumber_clusters = floor(sqrt(num_of_posts))","a46e374f":"# extact keywords & tokenize\n#keywords = yhf.keywords_yake(sample_posts, )\nkeywords_tokenized = yhf.tokenizing_after_YAKE(sample_posts)\nkeyword_sets = [set(post) for post in keywords_tokenized]","2ca60798":"# create empty array for document embeddings\ndoc_embeddings = np.zeros([num_of_posts, 300])\n\n# get document embeddings for posts\nfor i in range(num_of_posts):\n    embeddings = np.array(doc_embed_from_post(keyword_sets[i]))\n    if np.isnan(embeddings).any():\n        doc_embeddings[i,:] = np.zeros([1,300])\n    else:\n        doc_embeddings[i,:] = embeddings","1bbf0aa1":"# the default k-means label assignment didn't work well\nclustering = SpectralClustering(n_clusters=number_clusters, \n                                assign_labels=\"discretize\",\n                                n_neighbors=number_clusters).fit(doc_embeddings)","1e47d8b7":"# look at distrobution of cluster labels\npd.Series(clustering.labels_).value_counts()","52157266":"for i in range(number_clusters):\n    \n    print(f\"Cluster {i}:\\n\")\n    print(get_post_info_by_cluster(i, \n                                   data = sample_post_info,\n                                   cluster = clustering))\n    print(\"\\n\")\n    ","40748906":"for i in range(number_clusters):\n    \n    print(f\"Cluster {i}:\\n\")\n    print(get_keyword_set_by_cluster(i))\n    print(\"\\n\")","dda6f593":"# count of posts\/cluster\ncluster_counts = pd.Series(clustering.labels_).value_counts()\n\n# get clusters bigger than expected\nmax_cluster_size = number_clusters * 2\nbig_clusters = cluster_counts[cluster_counts > max_cluster_size]","f83ae979":"# sub-cluster first (biggest) cluster\ncluster_label = big_clusters.index[0]\n\nsub_sample = sample_post_info[clustering.labels_ == cluster_label]\nsub_cluster_embeddings = doc_embeddings[clustering.labels_ == cluster_label]\n\nnumber_sub_clusters = floor(sqrt(sub_sample.shape[0]))\n\nsub_cluster = SpectralClustering(n_clusters=number_sub_clusters, \n                                 assign_labels=\"discretize\", \n                                 n_neighbors=number_sub_clusters).fit(sub_cluster_embeddings)","66a113d9":"# see how it looks\nfor i in range(number_sub_clusters):\n\n    print(f\"Cluster {i}:\\n\")\n    print(get_post_info_by_cluster(i, data = sub_sample, \n                                   cluster = sub_cluster))\n    print(\"\\n\")","abda6e7c":"pd.Series(sub_cluster.labels_).value_counts()","53f2d34b":"from os import path\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt","409feb9c":"# TODO why do I see thank you?\nposts_as_string = sample_post_info\\\n    .Message\\\n    .to_string(index=False)\n\n# shouldn't have to do this b\/c I removed polite posts earlier\nposts_as_string = remove_thanks(posts_as_string)\n\n# Generate a word cloud image\nwordcloud = WordCloud().generate(posts_as_string)\n\n# Display the generated image:\n# the matplotlib way:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","0b47abbc":"# Going forward\n\nBiggest problem: redundent clusters\n\nPossible solutions: \n\n* Remove very short posts\n* Don't include posts on kernels\n* Build filter for removing short \"thanks!\" type posts\n* Start w\/ sentiment analys & put all very high sentiment posts in a single bin","abe97231":"# Get word vectors for keywords in post","4a0801a4":"# Word clouds","245a8681":"# Visualization brain storming\n\nSlides on text visualizatoin: https:\/\/courses.cs.washington.edu\/courses\/cse512\/15sp\/lectures\/CSE512-Text.pdf\n\n* Bigram based method, reporting the two terms with the median freuquency\n* term saliency, normalize by freq of most common term log(tf_w) \/ log(tf_the) (and then some sort of regression?)\n* Termite-based model: Topics as columns, terms as rows and weight visualiation of term distinctivenes as KL divergence p(T|term)\/p(T|any_term)","fe9d1abe":"# Preprocessing posts","cc5c9f23":"# Clustering!","b1f426cb":"# Refining clustering\n\nSteps:\n\n1. Drop empty clusters\n2. Identify large clusters (2 times more than expected)\n3. Recluster those clusters (# clusters = sqrt # posts)\n\n"}}