{"cell_type":{"54cd809b":"code","6c3e6446":"code","04e9402f":"code","2df8b62a":"code","e99bc4dc":"code","d802025b":"code","151d2390":"code","2fc803fc":"code","eea58982":"code","2864f64c":"code","ea0529af":"code","b2e2dec4":"code","39efa34c":"code","10dc048c":"markdown","04fcad8f":"markdown","8fafc5cb":"markdown","fe62a33c":"markdown","84ba27c6":"markdown","d3ec3717":"markdown","f66845bb":"markdown","0f99b284":"markdown","dbf9425c":"markdown","ef1bc5c8":"markdown","f135067f":"markdown","b0d73112":"markdown"},"source":{"54cd809b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow import keras\nimport cv2\nfrom tqdm import tqdm, tqdm_notebook\nimport matplotlib.pyplot as plt\n\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D,DepthwiseConv2D, Dense, Flatten, Dropout, BatchNormalization, LeakyReLU,GlobalAveragePooling2D, Activation, Average,AveragePooling2D\n\nfrom keras.optimizers import Adam,Adamax\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nseed = 42\nnp.random.seed(seed)","6c3e6446":"base_dir = os.path.join(\"..\", \"input\") # set base directory\ntrain_df = pd.read_csv(os.path.join(base_dir, \"train.csv\"))\ntrain_dir = os.path.join(base_dir, \"train\/train\")\ntest_dir = os.path.join(base_dir, \"test\/test\")\n\n# print(os.listdir(train_dir))\n","04e9402f":"train_df['has_cactus'] = train_df['has_cactus'].astype(str)\n\nbatch_size = 64\ntrain_size = 15750\nvalidation_size = 1750\n\ndatagen = ImageDataGenerator(\n    rescale=1.\/255,\n    horizontal_flip=True,\n    vertical_flip=True,\n    validation_split=0.1,\n    #zoom_range =0.3,\n    zca_whitening = False\n    )\n\ndata_args = {\n    \"dataframe\": train_df,\n    \"directory\": train_dir,\n    \"x_col\": 'id',\n    \"y_col\": 'has_cactus',\n    \"featurewise_center\" : True,\n    \"featurewise_std_normalization\" : True,\n    \"samplewise_std_normalization\" : False,\n    \"samplewise_center\" : False,\n    \"shuffle\": True,\n    \"target_size\": (32, 32),\n    \"batch_size\": batch_size,\n    \"class_mode\": 'binary'\n}\n\ntrain_generator = datagen.flow_from_dataframe(**data_args, subset='training')\nvalidation_generator = datagen.flow_from_dataframe(**data_args, subset='validation')\n","2df8b62a":"train_df.head(1)","e99bc4dc":"def show_image():\n    f, ax = plt.subplots(1,10,figsize=(15,15))\n    for i in range(10):\n        image1 = next(train_generator)\n        img1 = array_to_img(image1[0][0])\n        ax[i].imshow(img1)\n        ax[i].axis(\"off\")\n        \n    plt.show()\n    plt.axis(\"off\")\n    plt.title(\"Preprocessed\", fontsize=18)\n\n\n#image2 = next(train_generator)\nshow_image()","d802025b":"def Le_Conv():\n    model1 = Sequential()\n    model1.add(Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=(32, 32, 3), padding=\"same\"))\n    model1.add(AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n    model1.add(Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='valid'))\n    model1.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n    model1.add(Conv2D(120, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='valid'))\n    model1.add(Flatten())\n    model1.add(Dense(84, activation='relu'))\n    model1.add(Dense(1, activation='sigmoid'))\n    model1.compile(optimizer =  Adamax(lr =0.001) , loss = \"binary_crossentropy\", metrics=[\"acc\"])\n    return model1\n\ndef VggNet_Model():\n    model = Sequential()\n    model.add(Conv2D(16, (2,2), activation=\"relu\", input_shape=(32, 32, 3)))\n    #model.add(LeakyReLU(alpha =0.3))\n    model.add(Conv2D(16, (2,2), activation=\"relu\"))\n    model.add(BatchNormalization())\n    #model.add(LeakyReLU(alpha =0.3))\n    model.add(MaxPooling2D(2,2))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(32, (3,3), activation=\"relu\"))\n    #model.add(LeakyReLU(alpha =0.3))\n    model.add(Conv2D(32, (3,3), activation=\"relu\"))\n    model.add(BatchNormalization())\n    #model.add(LeakyReLU(alpha =0.3))\n    #model.add(MaxPooling2D(2,2))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(64, (3,3), activation=\"relu\"))\n    #model.add(LeakyReLU(alpha =0.3))\n    model.add(Conv2D(64, (3,3), activation=\"relu\"))\n    model.add(BatchNormalization())\n    #model.add(LeakyReLU(alpha =0.3))\n    model.add(MaxPooling2D(2,2))\n    model.add(Dropout(0.3))\n    #model.add(Conv2D(128, (3,3), activation=\"relu\"))\n    #model.add(BatchNormalization())\n    #model.add(LeakyReLU(alpha =0.3))\n    #model.add(Conv2D(128, (3,3), activation=\"relu\"))\n    #model.add(BatchNormalization())\n    #model.add(LeakyReLU(alpha =0.3))\n    #model.add(MaxPooling2D(2,2))\n    #model.add(Dropout(0.3))\n    model.add(Flatten())\n    model.add(Dense(units=1024, activation='relu'))\n    model.add(Dropout(0.4))\n    model.add(Dense(units=64, activation='relu'))\n    model.add(Dropout(0.7))\n    model.add(Dense(units=1, activation='sigmoid'))\n    model.summary()\n    adam = Adam(lr=0.001)\n    model.compile(optimizer=adam, \n                 loss='binary_crossentropy',\n                 metrics=['acc'])\n    \n    return model\n\ndef Custom_Conv():\n    model = Sequential()\n    model.add(Conv2D(32, (5,5), activation=\"relu\", input_shape=(32, 32, 3)))\n    model.add(BatchNormalization())\n#model.add(LeakyReLU(alpha =0.3))\n    model.add(Conv2D(32, (5,5), activation=\"relu\"))\n    model.add(BatchNormalization())\n#model.add(LeakyReLU(alpha =0.3))\n    model.add(MaxPooling2D(2,2))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(64, (3,3), activation=\"relu\"))\n    model.add(BatchNormalization())\n#model.add(LeakyReLU(alpha =0.3))\n    model.add(Conv2D(128, (3,3), activation=\"relu\"))\n    model.add(BatchNormalization())\n#model.add(LeakyReLU(alpha =0.3))\n    model.add(MaxPooling2D(2,2))\n    model.add(Dropout(0.3))\n    model.add(Flatten())\n    model.add(Dense(units=128, activation='relu'))\n    model.add(Dropout(0.4))\n    model.add(Dense(units=64, activation='relu'))\n    model.add(Dropout(0.4))\n    model.add(Dense(units=1, activation='sigmoid'))\n    adam = Adam(lr=0.001)\n    model.compile(optimizer=adam, \n                 loss='binary_crossentropy',\n                 metrics=['acc'])\n    return model\n\ndef Complex_model():\n    model = Sequential()\n        \n    model.add(Conv2D(3, kernel_size = 3, activation = 'relu', input_shape = (32, 32, 3)))\n    \n    model.add(Conv2D(filters = 16, kernel_size = 3, activation = 'relu'))\n    model.add(Conv2D(filters = 16, kernel_size = 3, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    \n    model.add(DepthwiseConv2D(kernel_size = 3, strides = 1, padding = 'Same', use_bias = True))\n    model.add(Conv2D(filters = 32, kernel_size = 1, activation = 'relu'))\n    model.add(Conv2D(filters = 64, kernel_size = 1, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    \n    model.add(DepthwiseConv2D(kernel_size = 3, strides = 2, padding = 'Same', use_bias = True))\n    model.add(Conv2D(filters = 128, kernel_size = 1, activation = 'relu'))\n    model.add(Conv2D(filters = 256, kernel_size = 1, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    \n    model.add(DepthwiseConv2D(kernel_size = 3, strides = 1, padding = 'Same', use_bias = True))\n    model.add(Conv2D(filters = 256, kernel_size = 1, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters = 512, kernel_size = 1, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    \n    model.add(DepthwiseConv2D(kernel_size = 3, strides = 2, padding = 'Same', use_bias = True))\n    model.add(Conv2D(filters = 512, kernel_size = 1, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters = 1024, kernel_size = 1, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    \n    model.add(DepthwiseConv2D(kernel_size = 3, strides = 1, padding = 'Same', use_bias = True))\n    model.add(Conv2D(filters = 1024, kernel_size = 1, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters = 2048, kernel_size = 1, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    \n    #model.add(GlobalAveragePooling2D())\n    model.add(Flatten())\n    \n    model.add(Dense(470, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(256, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(128, activation = 'tanh'))\n\n    model.add(Dense(1, activation = 'sigmoid'))\n    \n    opt=Adam(0.001)\n    model.compile(optimizer = opt, loss = 'mean_squared_error', metrics = ['accuracy'])\n    \n    return model\n","151d2390":"model = []\n#model.append(VggNet_Model())\nmodel.append(Custom_Conv())\nmodel.append(Le_Conv())\nmodel.append(Complex_model())","2fc803fc":"models = []\nhistories =[]\n\nfor i in range(len(model)):\n    ckpt_path = 'aerial_cactus_detection_'+str(i)+'.hdf5'\n    earlystop = EarlyStopping(monitor='val_acc', patience=25, verbose=1, restore_best_weights=False)\n    reducelr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=3, verbose=1, min_lr=1e-6)\n    modelckpt_cb = ModelCheckpoint(ckpt_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n    tb = TensorBoard()\n    callbacks = [earlystop, reducelr, modelckpt_cb, tb]\n    history = model[i].fit_generator(train_generator,\n              validation_data=validation_generator,\n              steps_per_epoch=train_size\/\/batch_size,\n              validation_steps=validation_size\/\/batch_size,\n              epochs=80, verbose=1, \n              shuffle=True,\n              callbacks=callbacks)\n    models.append(model[i])\n    histories.append(history)","eea58982":"# Training plots\nfor history in histories :\n    epochs = [i for i in range(1, len(history.history['loss'])+1)]\n    plt.plot(epochs, history.history['loss'], color='blue', label=\"training_loss\")\n    plt.plot(epochs, history.history['val_loss'], color='red', label=\"validation_loss\")\n    plt.legend(loc='best')\n    plt.title('loss')\n    plt.xlabel('epoch')\n    plt.show()\n    plt.plot(epochs, history.history['acc'], color='blue', label=\"training_accuracy\")\n    plt.plot(epochs, history.history['val_acc'], color='red',label=\"validation_accuracy\")\n    plt.legend(loc='best')\n    plt.title('accuracy')\n    plt.xlabel('epoch')\n    plt.show()","2864f64c":"test_df = pd.read_csv(os.path.join(base_dir, \"sample_submission.csv\"))\nprint(test_df.head())\ntest_images = []\nimages = test_df['id'].values\n\nfor image_id in images:\n    test_images.append(cv2.imread(os.path.join(test_dir, image_id)))\n    \ntest_images = np.asarray(test_images)\ntest_images = test_images \/ 255.0\nprint(\"Number of Test set images: \" + str(len(test_images)))","ea0529af":"final_pred =[]\npredictcustom = models[0].predict(test_images)\npredictle = models[1].predict(test_images)\npredictcomplex = models[2].predict(test_images)\n#predict4 = models[3].predict(test_images)\nstacked_arrays = np.hstack((predictcustom, predictle, predictcomplex)) #,predict4))\npred =np.mean(stacked_arrays, axis=1)\npredweighted = 0.3*predictcustom +0.2*predictle + 0.5*predictcomplex","b2e2dec4":"test_df['has_cactus'] = predweighted\n#test_df['has_cactus1'] = labels\n#test_df['has_cactus2'] = final_predict\ntest_df.to_csv('aerial-cactus-submission.csv', index = False)","39efa34c":"test_df.head(5)","10dc048c":"### Set callbacks for training","04fcad8f":"### Get Test Set images for prediction","8fafc5cb":"### Train Models in a loop and store the trained models","fe62a33c":"### Make predictions on test set","84ba27c6":"This Kernel deals with the Aerial Cactus (32 x 32) images . It is a good candidate for initial learning . With CNN and basic image preprocessing I was getting good score . Now I have added simple three model ensemble to boost the score . \n\nWe can achieve better result by directly using fast.ai or transfer learning , however I wanted to start with simple Keras CNN.\n\nThanks to the kernel \"CNN using Keras\" from user @Anirudh Chakravarthy to get me started\n","d3ec3717":"Image Generators have been used to augment the existing data. Training set is split in a 80:20 into train and validation set. Generators are created for each split. ","f66845bb":"### Build the model\nI have used three models Le_Conv , Custom_Conv and Complex_model  and trained the models individually before using the predictions from the model . All models use 50 Epochs and early stopping . Training the model happens in a simple for loop","0f99b284":"### Using Image Generators for preprocessing input images","dbf9425c":"### Train vs Validation Visualization\n\nThese plots can help realize cases of overfitting.","ef1bc5c8":"These are some standard callbacks which keras provides. \n1. EarlyStopping: Stops the training process if the monitored parameter stops improving with 'patience' number of epochs.\n2. ReduceLROnPlateau: Reduces learning rate by a factor if monitored parameter stops improving with 'patience' number of epochs. This helps fit the training data better.\n3. TensorBoard: Helps in visualization.\n4. ModelCheckpoint: Stores the best weights after each epoch in the path provided.\n\nFor further details, refer [this link.](https:\/\/keras.io\/callbacks)","f135067f":"### Set train and test directories","b0d73112":"##### Creating a list of models for looping through"}}