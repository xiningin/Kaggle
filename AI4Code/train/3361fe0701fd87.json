{"cell_type":{"3f8f0999":"code","73b7c2f3":"code","4cedcd87":"code","41d924ae":"code","ff9dc675":"code","d99bd67e":"code","23584607":"code","66a640d2":"code","02d0b91f":"code","5b95cb26":"code","28973c7e":"code","3116e0bd":"code","79518960":"code","524a3f6f":"code","bab814f1":"code","b1959450":"code","36fb43d8":"code","2eb76e86":"code","caec38ba":"code","382e624e":"code","060bf070":"code","94c82672":"code","bd766b14":"code","d058aa11":"code","618d0e8b":"code","1a50bf47":"code","d045031e":"code","7a2a21b4":"code","d0dc1f0b":"code","726efaa9":"code","2772ee9c":"code","e31c7826":"code","66d723c4":"code","e1ece40a":"code","3e2cb48c":"code","5766ab21":"code","3b1f9f69":"code","996ff222":"code","f7655c12":"code","d5efdc05":"code","7f96859e":"code","4292a412":"code","2c1c9a48":"code","b4020fe8":"code","3edadf6a":"code","bb6bb552":"code","df0b4e1b":"markdown","64566b20":"markdown","d6603c78":"markdown","8e72fe78":"markdown","17c2b642":"markdown","5257a1e0":"markdown","9573e9c8":"markdown","7d1b5123":"markdown","9c88c30b":"markdown","13c39e21":"markdown","c407e2f6":"markdown","c1412ecf":"markdown","f0871f38":"markdown","6233303f":"markdown","b8060ee6":"markdown","637d2ee3":"markdown","bc9683e2":"markdown","37d31b03":"markdown","61e2411e":"markdown","22640fb4":"markdown","d7d16bc0":"markdown","321faa8c":"markdown","a9b1df2f":"markdown","6f7c1b54":"markdown","0f6fa5c6":"markdown","0a403175":"markdown","9377bbdf":"markdown"},"source":{"3f8f0999":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","73b7c2f3":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom keras.preprocessing import text, sequence\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout\nfrom keras.callbacks import ReduceLROnPlateau\nimport tensorflow as tf","4cedcd87":"true = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\")\nfalse = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\")","41d924ae":"true.head()","ff9dc675":"false.head()","d99bd67e":"true['category'] = 1\nfalse['category'] = 0","23584607":"df = pd.concat([true,false]) #Merging the 2 datasets","66a640d2":"sns.set_style(\"darkgrid\")\nsns.countplot(df.category)","02d0b91f":"df.head()","5b95cb26":"df.isna().sum() # Checking for nan Values","28973c7e":"df.title.count()","3116e0bd":"df.subject.value_counts()","79518960":"plt.figure(figsize = (12,8))\nsns.set(style = \"whitegrid\",font_scale = 1.2)\nchart = sns.countplot(x = \"subject\", hue = \"category\" , data = df)\nchart.set_xticklabels(chart.get_xticklabels(),rotation=90)","524a3f6f":"df['text'] = df['text'] + \" \" + df['title']\ndel df['title']\ndel df['subject']\ndel df['date']","bab814f1":"stop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","b1959450":"def strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n# Removing URL's\ndef remove_between_square_brackets(text):\n    return re.sub(r'http\\S+', '', text)\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n#Removing the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = remove_stopwords(text)\n    return text\n#Apply function on review column\ndf['text']=df['text'].apply(denoise_text)","36fb43d8":"plt.figure(figsize = (20,20)) # Text that is not Fake\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(df[df.category == 1].text))\nplt.imshow(wc , interpolation = 'bilinear')","2eb76e86":"plt.figure(figsize = (20,20)) # Text that is Fake\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(df[df.category == 0].text))\nplt.imshow(wc , interpolation = 'bilinear')","caec38ba":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\ntext_len=df[df['category']==1]['text'].str.len()\nax1.hist(text_len,color='red')\nax1.set_title('Original text')\ntext_len=df[df['category']==0]['text'].str.len()\nax2.hist(text_len,color='green')\nax2.set_title('Fake text')\nfig.suptitle('Characters in texts')\nplt.show()","382e624e":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\ntext_len=df[df['category']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(text_len,color='red')\nax1.set_title('Original text')\ntext_len=df[df['category']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(text_len,color='green')\nax2.set_title('Fake text')\nfig.suptitle('Words in texts')\nplt.show()","060bf070":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\nword=df[df['category']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('Original text')\nword=df[df['category']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Fake text')\nfig.suptitle('Average word length in each text')","94c82672":"def get_corpus(text):\n    words = []\n    for i in text:\n        for j in i.split():\n            words.append(j.strip())\n    return words\ncorpus = get_corpus(df.text)\ncorpus[:5]","bd766b14":"from collections import Counter\ncounter = Counter(corpus)\nmost_common = counter.most_common(10)\nmost_common = dict(most_common)\nmost_common","d058aa11":"from sklearn.feature_extraction.text import CountVectorizer\ndef get_top_text_ngrams(corpus, n, g):\n    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","618d0e8b":"plt.figure(figsize = (16,9))\nmost_common_uni = get_top_text_ngrams(df.text,10,1)\nmost_common_uni = dict(most_common_uni)\nsns.barplot(x=list(most_common_uni.values()),y=list(most_common_uni.keys()))","1a50bf47":"plt.figure(figsize = (16,9))\nmost_common_bi = get_top_text_ngrams(df.text,10,2)\nmost_common_bi = dict(most_common_bi)\nsns.barplot(x=list(most_common_bi.values()),y=list(most_common_bi.keys()))","d045031e":"plt.figure(figsize = (16,9))\nmost_common_tri = get_top_text_ngrams(df.text,10,3)\nmost_common_tri = dict(most_common_tri)\nsns.barplot(x=list(most_common_tri.values()),y=list(most_common_tri.keys()))","7a2a21b4":"x_train,x_test,y_train,y_test = train_test_split(df.text,df.category,random_state = 0)","d0dc1f0b":"max_features = 10000\nmaxlen = 300","726efaa9":"tokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(x_train)\ntokenized_train = tokenizer.texts_to_sequences(x_train)\nx_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)","2772ee9c":"tokenized_test = tokenizer.texts_to_sequences(x_test)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)","e31c7826":"EMBEDDING_FILE = '..\/input\/glove-twitter\/glove.twitter.27B.100d.txt'","66d723c4":"def get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","e1ece40a":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n#change below line if computing normal stats is too slow\nembedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","3e2cb48c":"batch_size = 256\nepochs = 10\nembed_size = 100","5766ab21":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)","3b1f9f69":"#Defining Neural Network\nmodel = Sequential()\n#Non-trainable embeddidng layer\nmodel.add(Embedding(max_features, output_dim=embed_size, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n#LSTM \nmodel.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\nmodel.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\nmodel.add(Dense(units = 32 , activation = 'relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy'])","996ff222":"model.summary()","f7655c12":"history = model.fit(x_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = epochs , callbacks = [learning_rate_reduction])","d5efdc05":"print(\"Accuracy of the model on Training Data is - \" , model.evaluate(x_train,y_train)[1]*100 , \"%\")\nprint(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")","7f96859e":"epochs = [i for i in range(10)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Testing Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\nax[1].set_title('Training & Testing Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","4292a412":"pred = model.predict_classes(X_test)\npred[:5]","2c1c9a48":"print(classification_report(y_test, pred, target_names = ['Fake','Not Fake']))","b4020fe8":"cm = confusion_matrix(y_test,pred)\ncm","3edadf6a":"cm = pd.DataFrame(cm , index = ['Fake','Original'] , columns = ['Fake','Original'])","bb6bb552":"plt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Fake','Original'] , yticklabels = ['Fake','Original'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")","df0b4e1b":"# WHY FAKE NEWS IS A PROBLEM?\n**Fake news refers to misinformation, disinformation or mal-information which is spread through word of mouth and traditional media and more recently through digital forms of communication such as edited videos, memes, unverified advertisements and social media propagated rumours.Fake news spread through social media has become a serious problem, with the potential of it resulting in mob violence, suicides etc as a result of misinformation circulated on social media.**\n![image.png](attachment:image.png)","64566b20":"# LOADING THE NECESSARY LIBRARIES","d6603c78":"# TRAINING THE MODEL","8e72fe78":"**Trigram Analysis**","17c2b642":"**![image.png](attachment:image.png)\nThe behavior of P_ik\/P_jk for various words\nConsider the entity\nP_ik\/P_jk where P_ik = X_ik\/X_i\nHere P_ik denotes the probability of seeing word i and k together, which is computed by dividing the number of times i and k appeared together (X_ik) by the total number of times word i appeared in the corpus (X_i).\nYou can see that given two words, i.e. ice and steam, if the third word k (also called the \u201cprobe word\u201d),\nis very similar to ice but irrelevant to steam (e.g. k=solid), P_ik\/P_jk will be very high (>1),\nis very similar to steam but irrelevant to ice (e.g. k=gas), P_ik\/P_jk will be very small (<1),\nis related or unrelated to either words, then P_ik\/P_jk will be close to 1\nSo, if we can find a way to incorporate P_ik\/P_jk to computing word vectors we will be achieving the goal of using global statistics when learning word vectors.**\n\n**Source Credits - https:\/\/towardsdatascience.com\/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010**","5257a1e0":"**DATA CLEANING**","9573e9c8":"**PLS UPVOTE THIS NOTEBOOK IF YOU LIKE IT! THANKS FOR YOUR TIME !**","7d1b5123":"**Bigram Analysis**","9c88c30b":"**SO, WE CAN SEE THAT THE DATASET IS BALANCED**","13c39e21":"**Splitting the data into 2 parts - training and testing data**","c407e2f6":"**WORDCLOUD FOR REAL TEXT (LABEL - 1)**","c1412ecf":"**WHAT ARE STOPWORDS?**\n\n**Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc. Such words are already captured this in corpus named corpus. We first download it to our python environment.**","f0871f38":"**Tokenizing Text -> Repsesenting each word by a number**\n\n**Mapping of orginal word to number is preserved in word_index property of tokenizer**\n\n**Tokenized applies basic processing like changing it to lower case, explicitely setting that as False**\n\n**Lets keep all news to 300, add padding to news with less than 300 words and truncating long ones**","6233303f":"**WORDCLOUD FOR FAKE TEXT (LABEL - 0)**","b8060ee6":"**The distribution of both seems to be a bit different. 2500 characters in text is the most common in original text category while around 5000 characters in text are most common in fake text category.**","637d2ee3":"**Unigram Analysis**","bc9683e2":"**Number of words in each text**","37d31b03":"# ANALYSIS AFTER TRAINING OF MODEL","61e2411e":"**Number of characters in texts**","22640fb4":"**Some Model Parameters**","d7d16bc0":"**MERGING ALL THE TEXT DATA INTO 1 COLUMN i.e. 'text'**","321faa8c":"# DATA VISUALIZATION AND PREPROCESSING","a9b1df2f":"# BRIEF DESCRIPTION OF DATASET\n**This dataset consists of about 40000 articles consisting of fake as well as real news. Our aim is train our model so that it can correctly predict whether a given piece of news is real or fake.The fake and real news data is given in two separate datasets with each dataset consisting around 20000 articles each.**","6f7c1b54":"# IMPORTING THE DATASET","0f6fa5c6":"**SINCE THE TOPICS IN SUBJECT COLUMN ARE DIFFERENT FOR BOTH CATEGORIES, HENCE WE HAVE TO EXCLUDE IT FROM FINAL TEXT COLUMN**","0a403175":"# Introduction to GloVe\n**GloVe method is built on an important idea,\nYou can derive semantic relationships between words from the co-occurrence matrix.\nGiven a corpus having V words, the co-occurrence matrix X will be a V x V matrix, where the i th row and j th column of X, X_ij denotes how many times word i has co-occurred with word j. An example co-occurrence matrix might look as follows.**\n![image.png](attachment:image.png)\n\n**The co-occurrence matrix for the sentence \u201cthe cat sat on the mat\u201d with a window size of 1. As you probably noticed it is a symmetric matrix.\nHow do we get a metric that measures semantic similarity between words from this? For that, you will need three words at a time. Let me concretely lay down this statement.**","9377bbdf":"**Average word length in a text**"}}