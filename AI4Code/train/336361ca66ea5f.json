{"cell_type":{"008f5fbc":"code","7937fc1e":"code","a4baab43":"code","ffe66ce6":"code","7fe7a8a3":"code","5920930f":"code","7d5d4569":"code","4b49f2f3":"code","cb24f326":"code","d6169859":"code","ebf85d9f":"code","0b88969c":"code","1adeb2fe":"code","f130fc30":"code","40cdf0c4":"code","2355777f":"code","a2cdfe96":"code","64c368b8":"code","477c593f":"code","84b15d80":"code","4dd63463":"code","dbfd1ee2":"code","2a747c1b":"code","34dfef8e":"code","72c8603e":"code","0546c5bd":"code","eeb9b8c5":"code","4d6a8316":"code","6c17c55c":"code","d0ce8451":"code","3c3d0e2f":"code","8b3e6d26":"code","9802f1f6":"code","07913ac2":"code","f5ec6c7a":"code","5b8ee9ac":"code","0597e8e9":"code","4055d047":"code","8d44cd60":"code","bc9acffe":"code","320eb8a7":"code","2b08af8a":"code","aed55334":"code","0d7ff320":"code","6f02c4e8":"code","54e6a05f":"code","790f50c4":"code","796e35cb":"markdown","681ee656":"markdown","3e66a859":"markdown","3f1653d7":"markdown","0e46519e":"markdown","0f57881f":"markdown","84a6d844":"markdown","da7cccc7":"markdown","ce7152ba":"markdown","ba4a1768":"markdown","97488627":"markdown","94e7ca71":"markdown","eee3cadb":"markdown","2e32529b":"markdown","106a058a":"markdown","572ef7a8":"markdown"},"source":{"008f5fbc":"# Basic packages\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport time\nimport random\nimport glob\nimport sys\nimport os\nimport gc\n\n# ML packages\nfrom scipy.stats import spearmanr\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom IPython.display import display\nimport matplotlib.patches as patch\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import NuSVR\nfrom scipy.stats import norm\nimport lightgbm as lgb\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom scipy.stats import kurtosis, skew\n\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\n\n# visualization packages\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# execution progress bar\nfrom tqdm import tqdm_notebook, tnrange\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n","7937fc1e":"# System Setup\n%matplotlib inline\n%precision 4\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')\nnp.set_printoptions(suppress=True)\npd.set_option(\"display.precision\", 15)","a4baab43":"print(os.listdir(\"..\/input\/\"))","ffe66ce6":"# import Dataset to play with it\ntrain= pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv('..\/input\/test.csv')\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')","7fe7a8a3":"train.shape, test.shape, sample_submission.shape","5920930f":"train.head(5)","7d5d4569":"#Temporary tiny sample df for code development\n#random.seed(4444)\n#train = train.sample(n=5000)\n#train.shape","4b49f2f3":"train.columns","cb24f326":"print(len(train.columns))","d6169859":"print(train.info())","ebf85d9f":"train.describe()","0b88969c":"# distribution of targets\ncolors = ['darkseagreen','lightcoral']\nplt.figure(figsize=(6,6))\nplt.pie(train[\"target\"].value_counts(), explode=(0, 0.25), labels= [\"0\", \"1\"], startangle=45, autopct='%1.1f%%', colors=colors)\nplt.axis('equal')\nplt.show()","1adeb2fe":"# correlation with target\nlabels = []\nvalues = []\n\nfor col in train.columns:\n    if col not in ['ID_code', 'target']:\n        labels.append(col)\n        values.append(spearmanr(train[col].values, train['target'].values)[0])\n\ncorr_df = pd.DataFrame({'col_labels': labels, 'corr_values' : values})\ncorr_df = corr_df.sort_values(by='corr_values')\n\ncorr_df = corr_df[(corr_df['corr_values']>0.03) | (corr_df['corr_values']<-0.03)]\n\nind = np.arange(corr_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,12))\nrects = ax.barh(ind, np.array(corr_df.corr_values.values), color='darkseagreen')\nax.set_yticks(ind)\nax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Variable correlation to Target\")\nplt.show()","f130fc30":"# check covariance among importance variables\ncols_to_use = corr_df[(corr_df['corr_values']>0.05) | (corr_df['corr_values']<-0.05)].col_labels.tolist()\n\ntemp_df = train[cols_to_use]\ncorrmat = temp_df.corr(method='spearman')\nf, ax = plt.subplots(figsize=(18, 18))\n\n#Draw the heatmap using seaborn\nsns.heatmap(corrmat, vmax=1., square=True, cmap=\"Blues\", annot=True)\nplt.title(\"Important variables correlation map\", fontsize=15)\nplt.show()","40cdf0c4":"# Check missing data for test & train\ndef check_missing_data(df):\n    flag=df.isna().sum().any()\n    if flag==True:\n        total = df.isnull().sum()\n        percent = (df.isnull().sum())\/(df.isnull().count()*100)\n        output = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n        data_type = []\n        # written by MJ Bahmani\n        for col in df.columns:\n            dtype = str(df[col].dtype)\n            data_type.append(dtype)\n        output['Types'] = data_type\n        return(np.transpose(output))\n    else:\n        return(False)","2355777f":"print('missing in train: ',check_missing_data(train))\nprint('missing in test: ',check_missing_data(test))","a2cdfe96":"train.head()","64c368b8":"'''pca_df = preprocessing.normalize(train.drop(['ID_code','target'],axis=1))\npca_test_df = preprocessing.normalize(test.drop(['ID_code'],axis=1))\n\ndef _get_number_components(model, threshold):\n    component_variance = model.explained_variance_ratio_\n    explained_variance = 0.0\n    components = 0\n\n    for var in component_variance:\n        explained_variance += var\n        components += 1\n        if(explained_variance >= threshold):\n            break\n    return components\n\n### Get the optimal number of components\npca = PCA()\ntrain_pca = pca.fit_transform(pca_df)\ntest_pca = pca.fit_transform(pca_test_df)\ncomponents = _get_number_components(pca, threshold=0.9)\ncomponents'''","477c593f":"# Implement PCA \n'''obj_pca = model = PCA(n_components = components)\nX_pca = obj_pca.fit_transform(pca_df)\nX_t_pca = obj_pca.fit_transform(pca_test_df)'''","84b15d80":"# add the decomposed features in the train dataset\n'''def _add_decomposition(df, decomp, ncomp, flag):\n    for i in range(1, ncomp+1):\n        df[flag+\"_\"+str(i)] = decomp[:, i - 1]'''","4dd63463":"#pca_train = train[['ID_code','target']]\n#pca_test = test[['ID_code']]\n\n'''_add_decomposition(train, X_pca, 90, 'pca')\n_add_decomposition(test, X_t_pca, 90, 'pca')'''","dbfd1ee2":"idx = features = train.columns.values[2:202]\nfor df in [train, test]:\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)\n","2a747c1b":"train.head()","34dfef8e":"test.head()","72c8603e":"train.shape","0546c5bd":"cols=[\"target\",\"ID_code\"]\nX = train.drop(cols,axis=1)\ny = train[\"target\"]\ntest_ID = test[\"ID_code\"]\n#cols=[\"target\",\"ID_code\"]\n#X = pca_train.drop(cols,axis=1)\n#y = pca_train[\"target\"]","eeb9b8c5":"X_test  = test.drop(\"ID_code\",axis=1)\n#X_test  = pca_test.drop(\"ID_code\",axis=1)","4d6a8316":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)","6c17c55c":"# rfc_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)","d0ce8451":"'''perm = PermutationImportance(rfc_model, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())'''","3c3d0e2f":"# features = [c for c in train.columns if c not in ['ID_code', 'target']]","8b3e6d26":"# for get better result chage fold_n to 5\nfold_n=5\nfolds = StratifiedKFold(n_splits=fold_n, shuffle=True, random_state=10)","9802f1f6":"params = {\n        'bagging_freq': 5,\n        'bagging_fraction': 0.4,\n        'boost_from_average':'false',\n        'boost': 'gbdt',\n        'feature_fraction': 0.05,\n        'learning_rate': 0.01,\n        'max_depth': -1,\n        'metric':'auc',\n        'min_data_in_leaf': 80,\n        'min_sum_hessian_in_leaf': 10.0,\n        'num_leaves': 13,\n        'num_threads': 8,\n        'tree_learner': 'serial',\n        'objective': 'binary', \n        'verbosity': -1,\n        #'is_unbalance': True,\n        'reg_alpha': 0.1,\n        'reg_lambda': 8\n}\n","07913ac2":"%%time\ny_pred_lgb = np.zeros(len(X_test))\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    train_data = lgb.Dataset(X_train, label=y_train)\n    valid_data = lgb.Dataset(X_valid, label=y_valid)\n        \n    lgb_model = lgb.train(params,train_data,num_boost_round=100000,\n                    valid_sets = [train_data, valid_data],verbose_eval=1000,early_stopping_rounds = 3000)\n            \n    y_pred_lgb += lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\/5\n","f5ec6c7a":"train_preds_lgb = np.zeros(len(X))\ntrain_preds_lgb = lgb_model.predict(X, num_iteration=lgb_model.best_iteration)\/5\nauc_lgb  = round(roc_auc_score(train['target'], train_preds_lgb), 4) ","5b8ee9ac":"train_features = train.drop(['target','ID_code'], axis = 1)\ntest_features = test.drop(['ID_code'],axis = 1)\ntrain_target = train['target']\n\nsc = StandardScaler()\ntrain_features = sc.fit_transform(train_features)\ntest_features = sc.transform(test_features)\n\nn_splits = 5 # Number of K-fold Splits\nsplits = list(StratifiedKFold(n_splits=n_splits, shuffle=True).split(train_features, train_target))","0597e8e9":"class CyclicLR(object):\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n                 step_size=2000, mode='triangular', gamma=1.,\n                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n            if len(base_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} base_lr, got {}\".format(\n                    len(optimizer.param_groups), len(base_lr)))\n            self.base_lrs = list(base_lr)\n        else:\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n            if len(max_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} max_lr, got {}\".format(\n                    len(optimizer.param_groups), len(max_lr)))\n            self.max_lrs = list(max_lr)\n        else:\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n\n        self.step_size = step_size\n\n        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n                and scale_fn is None:\n            raise ValueError('mode is invalid and scale_fn is None')\n\n        self.mode = mode\n        self.gamma = gamma\n\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = self._triangular_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = self._triangular2_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = self._exp_range_scale_fn\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.batch_step(last_batch_iteration + 1)\n        self.last_batch_iteration = last_batch_iteration\n\n    def batch_step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group['lr'] = lr\n\n    def _triangular_scale_fn(self, x):\n        return 1.\n\n    def _triangular2_scale_fn(self, x):\n        return 1 \/ (2. ** (x - 1))\n\n    def _exp_range_scale_fn(self, x):\n        return self.gamma**(x)\n\n    def get_lr(self):\n        step_size = float(self.step_size)\n        cycle = np.floor(1 + self.last_batch_iteration \/ (2 * step_size))\n        x = np.abs(self.last_batch_iteration \/ step_size - 2 * cycle + 1)\n\n        lrs = []\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n        for param_group, base_lr, max_lr in param_lrs:\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n            if self.scale_mode == 'cycle':\n                lr = base_lr + base_height * self.scale_fn(cycle)\n            else:\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n            lrs.append(lr)\n        return lrs","4055d047":"class Simple_NN(nn.Module):\n    def __init__(self ,input_dim ,hidden_dim, dropout = 0.1):\n        super(Simple_NN, self).__init__()\n        \n        self.inpt_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, int(hidden_dim\/2))\n        self.fc3 = nn.Linear(int(hidden_dim\/2), int(hidden_dim\/4))\n        self.fc4 = nn.Linear(int(hidden_dim\/4), int(hidden_dim\/8))\n        self.fc5 = nn.Linear(int(hidden_dim\/8), 1)\n        self.bn1 = nn.BatchNorm1d(hidden_dim)\n        self.bn2 = nn.BatchNorm1d(int(hidden_dim\/2))\n        self.bn3 = nn.BatchNorm1d(int(hidden_dim\/4))\n        self.bn4 = nn.BatchNorm1d(int(hidden_dim\/8))\n    \n    def forward(self, x):\n        y = self.fc1(x)\n        y = self.relu(y)\n        #y = self.bn1(y)\n        y = self.dropout(y)\n        \n        y = self.fc2(y)\n        y = self.relu(y)\n        #y = self.bn2(y)\n        y = self.dropout(y)\n        \n        y = self.fc3(y)\n        y = self.relu(y)\n        #y = self.bn3(y)\n        y = self.dropout(y)\n        \n        y = self.fc4(y)\n        y = self.relu(y)\n        #y = self.bn4(y)\n        y = self.dropout(y)\n        \n        out= self.fc5(y)\n        \n        return out","8d44cd60":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))","bc9acffe":"model = Simple_NN(208,512)\nmodel.cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.0002) # Using Adam optimizer","320eb8a7":"from torch.optim.optimizer import Optimizer\nn_epochs = 40\nbatch_size = 512\n\ntrain_preds = np.zeros((len(train_features)))\ntest_preds = np.zeros((len(test_features)))\n\nx_test = np.array(test_features)\nx_test_cuda = torch.tensor(x_test, dtype=torch.float).cuda()\ntest = torch.utils.data.TensorDataset(x_test_cuda)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n\navg_losses_f = []\navg_val_losses_f = []\n\nfor i, (train_idx, valid_idx) in enumerate(splits):  \n    x_train = np.array(train_features)\n    y_train = np.array(train_target)\n    \n    x_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.float).cuda()\n    y_train_fold = torch.tensor(y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n    \n    x_val_fold = torch.tensor(x_train[valid_idx.astype(int)], dtype=torch.float).cuda()\n    y_val_fold = torch.tensor(y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n    \n    loss_fn = torch.nn.BCEWithLogitsLoss()\n    \n    step_size = 2000\n    base_lr, max_lr = 0.0001, 0.005  \n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n                             lr=max_lr)\n    \n    ################################################################################################\n    scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,\n               step_size=step_size, mode='exp_range',\n               gamma=0.99994)\n    ###############################################################################################\n\n    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n    \n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n    \n    print(f'Fold {i + 1}')\n    for epoch in range(n_epochs):\n        start_time = time.time()\n        model.train()\n        avg_loss = 0.\n        #avg_auc = 0.\n        for i, (x_batch, y_batch) in enumerate(train_loader):\n            y_pred = model(x_batch)\n            #########################\n            if scheduler:\n                #print('cycle_LR')\n                scheduler.batch_step()\n            ########################\n            loss = loss_fn(y_pred, y_batch)\n\n            optimizer.zero_grad()\n            loss.backward()\n\n            optimizer.step()\n            avg_loss += loss.item()\/len(train_loader)\n            #avg_auc += round(roc_auc_score(y_batch.cpu(),y_pred.detach().cpu()),4) \/ len(train_loader)\n        model.eval()\n        \n        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n        test_preds_fold = np.zeros((len(test_features)))\n        \n        avg_val_loss = 0.\n        #avg_val_auc = 0.\n        for i, (x_batch, y_batch) in enumerate(valid_loader):\n            y_pred = model(x_batch).detach()\n            \n            #avg_val_auc += round(roc_auc_score(y_batch.cpu(),sigmoid(y_pred.cpu().numpy())[:, 0]),4) \/ len(valid_loader)\n            avg_val_loss += loss_fn(y_pred, y_batch).item() \/ len(valid_loader)\n            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n            \n        elapsed_time = time.time() - start_time \n        print('Epoch {}\/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n            epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n        \n    avg_losses_f.append(avg_loss)\n    avg_val_losses_f.append(avg_val_loss) \n    \n    for i, (x_batch,) in enumerate(test_loader):\n        y_pred = model(x_batch).detach()\n\n        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n        \n    train_preds[valid_idx] = valid_preds_fold\n    test_preds += test_preds_fold \/ len(splits)\n\nauc_nn  =  round(roc_auc_score(train_target,train_preds),4)      \nprint('All \\t loss={:.4f} \\t val_loss={:.4f} \\t auc={:.4f}'.format(np.average(avg_losses_f),np.average(avg_val_losses_f),auc_nn))","2b08af8a":"ensemble = 0.5*train_preds_lgb + 0.5* train_preds\nensemble_test = 0.5* y_pred_lgb + 0.5* test_preds","aed55334":"print('LightBGM auc = {:<8.5f}'.format(auc_lgb))\nprint('NN auc = {:<8.5f}'.format(auc_nn))\nprint('NN+LightBGM auc = {:<8.5f}'.format(roc_auc_score(train_target, ensemble)))","0d7ff320":"submission_lgb = pd.DataFrame({\n        \"ID_code\": test_ID,\n        \"target\": y_pred_lgb\n    })\nsubmission_lgb.to_csv('submission_lgb.csv', index=False)","6f02c4e8":"submission_nn = pd.DataFrame({\n        \"ID_code\": test_ID,\n        \"target\": test_preds\n    })\nsubmission_nn.to_csv('submission_nn.csv', index=False)","54e6a05f":"submission_ens = pd.DataFrame({\n        \"ID_code\": test_ID,\n        \"target\": ensemble_test\n    })\nsubmission_ens.to_csv('submission_ens.csv', index=False)","790f50c4":"'''submission_rfc_cat = pd.DataFrame({\n        \"ID_code\": test[\"ID_code\"],\n        \"target\": (y_pred_rfc +y_pred_cat)\/2\n    })\nsubmission_rfc_cat.to_csv('submission_rfc_cat.csv', index=False)'''","796e35cb":"1. [https:\/\/www.kaggle.com\/mjbahmani\/santander-ml-explainability](https:\/\/www.kaggle.com\/mjbahmani\/santander-ml-explainability)  \n1. [https:\/\/www.kaggle.com\/super13579\/pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899](https:\/\/www.kaggle.com\/super13579\/pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899)  \n1. [https:\/\/www.kaggle.com\/dansbecker\/permutation-importance](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance)\n1. [https:\/\/www.kaggle.com\/dansbecker\/partial-plots](https:\/\/www.kaggle.com\/dansbecker\/partial-plots)\n1. [https:\/\/www.kaggle.com\/miklgr500\/catboost-with-gridsearch-cv](https:\/\/www.kaggle.com\/miklgr500\/catboost-with-gridsearch-cv)\n1. [https:\/\/www.kaggle.com\/dromosys\/sctp-working-lgb](https:\/\/www.kaggle.com\/dromosys\/sctp-working-lgb)\n1. [https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction](https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction)\n1. [permutation-importance](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance)\n1. [partial-plots](https:\/\/www.kaggle.com\/dansbecker\/partial-plots)\n1. [https:\/\/www.kaggle.com\/dansbecker\/shap-values](https:\/\/www.kaggle.com\/dansbecker\/shap-values)\n1. [algorithm-choice](https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/algorithm-choice)","681ee656":"## Feature importance","3e66a859":" ## Model Development","3f1653d7":" <a id=\"55\"><\/a> <br>\n## Stacking","0e46519e":"## Submission Files","0f57881f":"### lightgbm","84a6d844":"#### Summary Stats","da7cccc7":"### Neural Net","ce7152ba":"## Data Preprocessing","ba4a1768":"##   Data Exploration","97488627":"#### PCA","94e7ca71":"# References & credits\nThanks fo following kernels that help me to create this kernel.","eee3cadb":"## Load Data","2e32529b":"## Variable Engineering","106a058a":"### Permutation Importance","572ef7a8":"# Not Completed yet!!!"}}