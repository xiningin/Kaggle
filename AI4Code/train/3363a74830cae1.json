{"cell_type":{"bfb5630d":"code","1c04c562":"code","398bc97d":"code","63267ca7":"code","886c0f74":"code","122152c5":"code","b7629982":"code","99ab251e":"code","adf0ea5f":"code","9cac3983":"code","f12f7aab":"code","37b58f21":"code","aabf47d4":"code","f0a1bbe1":"code","f12b8898":"code","225c0f64":"code","6dad81c4":"code","02df5a68":"code","d24270a9":"code","2428600c":"code","85ace556":"code","b3e8af4b":"code","eccfe9f9":"code","30960bdc":"markdown","f13a8aa1":"markdown","caca11a3":"markdown","920370d9":"markdown","0301b367":"markdown","92448017":"markdown","dc69784c":"markdown","a24306d5":"markdown","e49a45f7":"markdown","04f253c9":"markdown","68ec9e29":"markdown","59269221":"markdown","6032f390":"markdown","980e1e6f":"markdown","e749c4e7":"markdown","850b70b6":"markdown","8a768bd9":"markdown"},"source":{"bfb5630d":"import torch\nimport torchvision\nimport torchvision.transforms as transforms","1c04c562":"trainset = torchvision.datasets.CIFAR10(root='.\/datasets',\n                                        train=True,\n                                        download=True,\n                                        transform=transforms.ToTensor())\n#train_set = torchvision.datasets.FashionMNIST(root='.\/datasets', \n#                                              download=True, \n#                                              transform=transforms.Compose([transforms.ToTensor()]))\n#test_set = torchvision.datasets.FashionMNIST(root='.\/datasets', \n#                                             download=True, \n#                                             train=False, \n#                                             transform=transforms.Compose([transforms.ToTensor()]))  ","398bc97d":"trainset","63267ca7":"trainloader = torch.utils.data.DataLoader(trainset,\n                                          batch_size=8,\n                                          shuffle=True,\n                                          num_workers=2)","886c0f74":"testset = torchvision.datasets.CIFAR10(root='.\/datasets',\n                                       train=False,\n                                       download=True,\n                                       transform=transforms.ToTensor())","122152c5":"testset","b7629982":"testloader = torch.utils.data.DataLoader(testset,\n                                         batch_size=8,\n                                         shuffle=False,\n                                         num_workers=2)","99ab251e":"labels = ('plane', 'car', 'bird', 'cat','deer', \n          'dog', 'frog', 'horse', 'ship', 'truck')","adf0ea5f":"import matplotlib.pyplot as plt\nimport numpy as np","9cac3983":"images_batch, labels_batch = iter(trainloader).next()","f12f7aab":"images_batch.shape","37b58f21":"img = torchvision.utils.make_grid(images_batch)","aabf47d4":"img.shape","f0a1bbe1":"np.transpose(img, (1,2,0)).shape","f12b8898":"plt.imshow(np.transpose(img, (1,2,0)))\nplt.axis('off')\nplt.show()","225c0f64":"import torch.nn as nn","6dad81c4":"in_size = 3\nhid1_size = 16\nhid2_size = 32\nout_size = len(labels)\nk_conv_size = 5 ","02df5a68":"class ConvNet(nn.Module):\n    \n    def __init__(self):\n        super(ConvNet, self).__init__()\n        \n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_size, hid1_size, k_conv_size ),\n            nn.BatchNorm2d(hid1_size),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2))\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(hid1_size, hid2_size, k_conv_size),\n            nn.BatchNorm2d(hid2_size),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2))\n        \n        self.fc = nn.Linear(hid2_size *  k_conv_size * k_conv_size, out_size)\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc(out)\n        \n        return out","d24270a9":"model = ConvNet()","2428600c":"learning_rate = 0.001\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = torch.optim.Adam(model.parameters(), \n                             lr=learning_rate)","85ace556":"total_step = len(trainloader)\nnum_epochs = 20\n\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(trainloader):\n        \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 2000 == 0:\n            print ('Epoch [{}\/{}], Step [{}\/{}], Loss: {:.4f}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","b3e8af4b":"model.eval()  \nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in testloader:\n\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Accuracy of the model on the 10000 test images: {}%'\\\n          .format(100 * correct \/ total))\n","eccfe9f9":"#End of Code","30960bdc":"#### Create a tuple containing the unique labels in our dataset","f13a8aa1":"#### Get a batch from the training data\nThe DataLoader object divides the dataset into batches. Here we examine the first batch of the training data set","caca11a3":"#### Evaluate the model\nmodel.eval() sets our model in evaluation(test) mode <br>\nWe will use test data to check accuracy of our model","920370d9":"#### Download the CIFAR dataset from the Pytorch library\nThe torchvision module contains a number of datasets which are meant to help Pytorch users practice building models with them. We use one of these datasets here. The datasets are available in torchvision.datasets.\n\nBy specifying <b>train = True<\/b> we will load the training data from the dataset","0301b367":"### Data Loaders\nData loader combines a dataset and provides single or multi-process iterators over the dataset. Iterators feed in your training data to your neural network in batches.\n\n* <b>batch_size<\/b> = how many samples per batch to load (default: 1).<br>\n* <b>shuffle = True<\/b>, to have the data reshuffled at every epoch <br>\n* <b>num_workers= 2<\/b>, 2 subprocesses will be used for data loading.\n0 means that the data will be loaded in the main process. (default: 0)","92448017":"#### Configuring the neural network\n* The input size will be the 3 channels of the images\n* The first convolution will produce 16 channels\n* The second convolution produces 32 channels\n* The final output will have a size equal to the number of classes for the prediction\n* The convolving kernel will have a size of 5","dc69784c":"<b>Summary:<\/b> Using cifar dataset from torchvission of pytorch to create CNN classifier from scratch\n\nwe'll use the Torchvision module from PyTorch. Which contains datasets to help PyTorch users practice building models using these datasets. And it's available directly from Torchvision. \n\nThe CIFAR-10 is a popular dataset used for image classification.","a24306d5":"### Define the Convolutional Neural Network\n\n<b>Conv2d: <\/b>Applies a 2D convolution over an input signal composed of several input planes.<br>\nParameters<br>\nin_channels (int) \u2013 Number of channels in the input image<br>\nout_channels (int) \u2013 Number of channels produced by the convolution<br>\nkernel_size (int or tuple) \u2013 Size of the convolving kernel<br>\n\n<b>BatchNorm2d: <\/b>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .\nParameters<br>\nnum_features \u2013 C from an expected input of size (N,C,H,W)\n\n<b>ReLU: <\/b>Activation function\n\n<b>Maxpool2d: <\/b>\nParameters:<br>\nkernel_size \u2013 the size of the window to take a max over\n\n<b>Linear: <\/b>\nParameter:<br>\n\nin_features: \nAll the operations above used 4D Tensors of shape => torch.Size([8, 32, 5, 5]), where 8 are number of images per batch<br>\nNow for fully connected layers(linear layers) we need to transform them in 1D Tensors<br>\nSo to the in_features of fully connected layer we will give **32\\*5\\*5** (hid2_size X k_conv_size X k_conv_size)\n\nout_features:<br>\nnum_classes = since we are using Cifar10 we have 10 labels  ","e49a45f7":"#### Check out the shape of this batch of images\nThe first dimension gives the number of images. The next dimension represents the number of channels. The last two give the image size","04f253c9":"#### Define the Learning Rate, Loss Function and Optimizer","68ec9e29":"#### Use a DataLoader to prepare our test data","59269221":"#### Create a grid to display the batch of images\ntorchvision is a module which contains datasets, model architectures and image transformation tools. Here, we use one of its utilities to create a grid of images from our batch ","6032f390":"## Examining the datasets\nWe will view some of the images in the datasets for which we first need to import Matplotlib and Numpy","980e1e6f":"#### Training the model\n\n<b>Forward Pass: <\/b> \nPredict Output from train data and then compute loss using CrossEntropyLoss() defined above\n\n<b>Backward Pass: <\/b>\nFirstly zero all the gradient variables and then back propogate<br>\nWe are using Adam optimizer to optimize our model at every step\n","e749c4e7":"#### Examine the shape\nThe grid effectively contains all the 8 images placed side by side with padding of 2 pixels between the images and at the edges of the grid","850b70b6":"#### Load the CIFAR test dataset\nWe specify <b>train=False<\/b> in this case","8a768bd9":"#### Transposing the image\nWe will use np.transpose to change the shape of image tensor<br>\n<b>.imshow()<\/b> needs a 2D array, or a 3D array with the third dimension being of size 3 or 4 only (For RGB or RGBA), so we will shift first axis to last<br>\nRef - https:\/\/matplotlib.org\/api\/pyplot_api.html#matplotlib.pyplot.imshow"}}