{"cell_type":{"50019138":"code","f7866215":"code","6b1977bb":"code","7f4655f2":"code","186bf199":"code","a8973769":"code","b92fa108":"code","9cede0a1":"code","bd882580":"code","0e9b87ba":"code","14505e2b":"code","7923b6e5":"code","53c42775":"code","8fc79a05":"code","11fed4d8":"code","c39988b9":"code","1f0c8529":"code","c013627e":"code","e26d19ff":"code","f2fdec8f":"code","5d32cc22":"code","02cd8170":"code","a52abd52":"code","7e03002c":"code","638e7557":"code","4bfb48fb":"code","353a2f61":"code","06d8051b":"code","03b60dd4":"code","60d17603":"code","2f687d1b":"code","50e10243":"code","94c69d8d":"code","20f98173":"code","76186453":"markdown","0867def5":"markdown","180c749b":"markdown","c93783e4":"markdown","eccd846e":"markdown","d185a164":"markdown","6034a7de":"markdown","57e299b3":"markdown","526cfaf0":"markdown","d60d89f5":"markdown","67ac2bd9":"markdown","f5adc0a6":"markdown","224e6ebd":"markdown","55f0a3f1":"markdown","0d32c1d0":"markdown","394547ff":"markdown"},"source":{"50019138":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f7866215":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\")\nimport matplotlib.image as implt \nfrom PIL import Image #(PIL)Python Image Library (https:\/\/www.slideshare.net\/caglardursun\/python-image-ktphanesi)\nimport seaborn as sns #(https:\/\/teknoloji.org\/seaborn-kutuphanesi-nedir-nasil-kullanilir\/)\nimport cv2 as cs2 #(https:\/\/medium.com\/operations-management-t%C3%BCrkiye\/opencv-ile-temel-resim-kamera-i%CC%87%C5%9Flemleri-14294a688965)\nimport os\n\nimport warnings #Bir mod\u00fcl\u00fc i\u00e7e aktarma i\u015flemi s\u0131ras\u0131nda tetiklenen uyar\u0131lar i\u00e7in temel kategori (varsay\u0131lan olarak yok say\u0131l\u0131r).\nwarnings.filterwarnings('ignore') #(\"ignore\": e\u015fle\u015fen uyar\u0131lar\u0131 asla yazd\u0131rma)\n#yukar\u0131daki import i\u015flemlerinde herhangi bir uyar\u0131 \u00e7\u0131karsa yok say ve yazd\u0131rma dedik.","6b1977bb":"import os\nimport seaborn as sns","7f4655f2":"train_path = \"\/kaggle\/input\/horses-or-humans-dataset\/horse-or-human\/train\/\"\ntest_path = \"\/kaggle\/input\/horses-or-humans-dataset\/horse-or-human\/validation\/\"\n\ntrain_horses = \"\/kaggle\/input\/horses-or-humans-dataset\/horse-or-human\/train\/horses\/\"\ntest_horses = \"\/kaggle\/input\/horses-or-humans-dataset\/horse-or-human\/validation\/horses\/\"\n\ntrain_humans = \"\/kaggle\/input\/horses-or-humans-dataset\/horse-or-human\/train\/humans\/\"\ntest_humans = \"\/kaggle\/input\/horses-or-humans-dataset\/horse-or-human\/validation\/humans\/\"","186bf199":"# VISUALIZATION\ncategory_names = os.listdir(train_path) # output: ['humans', 'horses']\nnb_categories = len(category_names) # output: 2\ntrain_images = []\n\nfor category in category_names:\n    folder = train_path + \"\/\" + category\n    train_images.append(len(os.listdir(folder)))\n\nsns.barplot(y=category_names, x=train_images).set_title(\"Number Of Training Images Per Category\");","a8973769":"test_images = []\nfor category in category_names:\n    folder = test_path +\"\/\" + category\n    test_images.append(len(os.listdir(folder)))\n    \nsns.barplot(y=category_names, x=test_images).set_title(\"Numer of Testing Images Per Category\");\n    ","b92fa108":"img1 = implt.imread(train_horses + \"\/horse01-0.png\")\nimg2 = implt.imread(train_humans + \"\/human01-00.png\")\n\nplt.subplot(1,2,1) #subplot(x,y,z): Grafiklerin d\u00fczlemini ve ka\u00e7\u0131nc\u0131 grafik oldu\u011funu belirtir. \u0130lk say\u0131 sat\u0131r\u0131, ikinci say\u0131 s\u00fctunu, \u00fc\u00e7\u00fcnc\u00fc say\u0131 ise ka\u00e7\u0131nc\u0131 grafik oldu\u011funu ifade eder.\nplt.title('horse') #ba\u015fl\u0131k\nplt.imshow(img1) \nplt.subplot(1,2,2)\nplt.title(\"human\")\nplt.imshow(img2)\nplt.show()","9cede0a1":"img_size = 50\nhumans_train = []\nhorses_train = []\nlabel = []\n\nfor i in os.listdir(train_humans): # all train human images\n    if os.path.isfile(train_path + \"\/humans\/\" + i): # check image in file\n        humans = Image.open(train_path + \"\/humans\/\" + i).convert(\"L\") # converting grey scale \n        humans = humans.resize((img_size,img_size), Image.ANTIALIAS) # resizing to 50,50\n        humans = np.asarray(humans)\/255 # bit format\n        humans_train.append(humans)\n        label.append(1)\n        \nfor i in os.listdir(train_horses): # all train horse images\n    if os.path.isfile(train_path + \"\/horses\/\" + i): # check image in file\n        horses = Image.open(train_path + \"\/horses\/\" + i).convert(\"L\") # converting grey scale \n        horses = horses.resize((img_size,img_size), Image.ANTIALIAS) # resizing to 50,50\n        horses = np.asarray(horses)\/255 # bit format\n        horses_train.append(horses)\n        label.append(0)","bd882580":"x_train = np.concatenate((humans_train,horses_train),axis=0)\nx_train_label = np.asarray(label)\nx_train_label = x_train_label.reshape(x_train_label.shape[0],1)\n\nprint(\"humans:\",np.shape(humans_train) , \"horses:\",np.shape(horses_train))\nprint(\"train_dataset:\",np.shape(x_train), \"train_values:\",np.shape(x_train_label))","0e9b87ba":"x_train = np.concatenate((humans_train,horses_train),axis=0) # training dataset\nx_train_label = np.asarray(label) # label array containing 0 and 1\nx_train_label = x_train_label.reshape(x_train_label.shape[0],1)\n\nprint(\"humans:\",np.shape(humans_train) , \"horses:\",np.shape(horses_train))\nprint(\"train_dataset:\",np.shape(x_train), \"train_values:\",np.shape(x_train_label))\n","14505e2b":"img_size = 50\nhumans_test = []\nhorses_test = []\nlabel = []\n\nfor i in os.listdir(test_humans): # all train human images\n    if os.path.isfile(test_path + \"\/humans\/\" + i): # check image in file\n        humans = Image.open(test_path + \"\/humans\/\" + i).convert(\"L\") # converting grey scale \n        humans = humans.resize((img_size,img_size), Image.ANTIALIAS) # resizing to 50,50\n        humans = np.asarray(humans)\/255 # bit format\n        humans_test.append(humans)\n        label.append(1)\n        \nfor i in os.listdir(test_horses): # all train horse images\n    if os.path.isfile(test_path + \"\/horses\/\" + i): # check image in file\n        horses = Image.open(test_path + \"\/horses\/\" + i).convert(\"L\") # converting grey scale \n        horses = horses.resize((img_size,img_size), Image.ANTIALIAS) # resizing to 50,50\n        horses = np.asarray(horses)\/255 # bit format\n        horses_test.append(horses)\n        label.append(0)","7923b6e5":"x_test = np.concatenate((humans_test,horses_test),axis=0) # training dataset\nx_test_label = np.asarray(label) # label array containing 0 and 1\nx_test_label = x_test_label.reshape(x_test_label.shape[0],1)\n\nprint(\"humans:\",np.shape(humans_test) , \"horses:\",np.shape(horses_test))\nprint(\"test_dataset:\",np.shape(x_test), \"test_values:\",np.shape(x_test_label))","53c42775":"x = np.concatenate((x_train,x_test),axis=0) # count: 1027+256= 1283  | train_data\n# x.shape: \n#   output = (1283,50,50)\ny = np.concatenate((x_train_label,x_test_label),axis=0) # count: 1027+256= 1283 | test_data\nx = x.reshape(x.shape[0],x.shape[1]*x.shape[2]) # flatten 3D image array to 2D, count: 50*50 = 2500\nprint(\"images:\",np.shape(x), \"labels:\",np.shape(y))","8fc79a05":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=42)\nnumber_of_train = X_train.shape[0]\nnumber_of_test = X_test.shape[0]\n\nprint(\"Train Number: \", number_of_train)\nprint(\"Test Number: \", number_of_test)","11fed4d8":"x_train = X_train.T\nx_test = X_test.T\ny_train = y_train.T\ny_test = y_test.T\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","c39988b9":"def initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","1f0c8529":"def forward_backward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = -(1-y_train)*np.log(1-y_head)-y_train*np.log(y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\n        \"derivative_weight\":derivative_weight,\n        \"derivative_bias\": derivative_bias\n    }\n    return cost,gradients","c013627e":"def update(w, b, x_train, y_train, learning_rate,number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iteration times\n    for i in range(number_of_iteration):\n        \n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        \n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 50 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n        # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation=\"vertical\")\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","e26d19ff":"def predict(w,b,x_test):\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    y_prediction = np.zeros((1,x_test.shape[1]))\n\n    # if z is bigger than 0.5, our prediction is human (y_head=1)\n    # if z is smaller than 0.5, our prediction is horse (y_head=0)\n    \n\n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n    return y_prediction","f2fdec8f":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):\n    # initialize\n    dimension = x_train.shape[0] # 2500\n    w,b = initialize_weights_and_bias(dimension)\n    parameters, gradients, cost_list = update(w,b,x_train,y_train,learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n    \n    test_acc_lr = round((100 - np.mean(np.abs(y_prediction_test - y_test)) * 100),2)\n    train_acc_lr = round((100 - np.mean(np.abs(y_prediction_train - y_train))*100),2)\n    \n    # Print train\/test Errors\n    print(\"train accuracy: %\", train_acc_lr)\n    print(\"test accuracy: %\", test_acc_lr)\n    return train_acc_lr, test_acc_lr","5d32cc22":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 1000)","02cd8170":"#you can adjust learning_rate and num_iteration to check how the result is affected\n#(for learning rate, try exponentially lower values:0.001 etc.) \ntrain_acc_lr, test_acc_lr = logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.001, num_iterations = 2000)","a52abd52":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nlogreg = LogisticRegression()\ntest_acc_logregsk = round(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)* 100, 2)\ntrain_acc_logregsk = round(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)* 100, 2)","7e03002c":"from sklearn.model_selection import GridSearchCV\n\ngrid = {\n    \"C\": np.logspace(-4, 4, 20),\n    \"penalty\": [\"l1\",\"l2\"]\n}\nlg=LogisticRegression(random_state=42)\nlog_reg_cv=GridSearchCV(lg,grid,cv=10,n_jobs=-1,verbose=2)\nlog_reg_cv.fit(x_train.T,y_train.T)\nprint(\"accuracy:\", log_reg_cv.best_score_)","638e7557":"models = pd.DataFrame({\n    'Model': ['LR without sklearn','LR with sklearn','LR with GridSearchCV' ],\n    'Train Score': [train_acc_lr, train_acc_logregsk, \"-\"],\n    'Test Score': [test_acc_lr, test_acc_logregsk, log_reg_cv.best_score_*100]\n})\nmodels.sort_values(by='Test Score', ascending=False)","4bfb48fb":"x_train.shape[0]","353a2f61":"def initialize_parameters_and_layer_sizes_NN(x_train,y_train):\n    parameters = {\n        \"weight1\": np.random.randn(3,x_train.shape[0]) * 0.1,\n        \"bias1\": np.zeros((3,1)),\n        \"weight2\": np.random.randn(y_train.shape[0],3) * 0.1,\n        \"bias2\": np.zeros((y_train.shape[0],1))\n    }\n    return parameters","06d8051b":"def forward_propagation_NN(x_train,parameters):\n    Z1 = np.dot(parameters[\"weight1\"],x_train) + parameters[\"bias1\"]\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n    \n    cache = {\n        \"Z1\": Z1,\n        \"A1\": A1,\n        \"Z2\": Z2,\n        \"A2\": A2\n    }\n    return A2, cache","03b60dd4":"def compute_cost_NN(A2,Y,parameters):\n    logprobs = np.multiply(np.log(A2),Y)\n    cost = -np.sum(logprobs)\/Y.shape[1]\n    return cost","60d17603":"def backward_propagation_NN(parameters, cache, X, Y):\n\n    dZ2 = cache[\"A2\"]-Y\n    dW2 = np.dot(dZ2,cache[\"A1\"].T)\/X.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)\/X.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1,X.T)\/X.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)\/X.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads","2f687d1b":"def update_parameters_NN(parameters, grads, learning_rate = 0.08):\n    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n                  \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n                  \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n                  \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n    \n    return parameters","50e10243":"def predict_NN(parameters,x_test):\n    # x_test is a input for forward propagation\n    A2, cache = forward_propagation_NN(x_test,parameters)\n    Y_prediction = np.zeros((1,x_test.shape[1])) # x_test.shape[1]: 193\n    # if z is bigger than 0.5 our prediction is human\n    # if z is smaller than 0.5 our prediction is horse\n    for i in range(A2.shape[1]):\n        if A2[0,i] <= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    return Y_prediction","94c69d8d":"def two_layer_neural_network(x_train,y_train,x_test,y_test,num_iterations):\n    cost_list = []\n    index_list = []\n    # initialize parameters and layer sizes\n    parameters = initialize_parameters_and_layer_sizes_NN(x_train,y_train)\n    \n    for i in range(0,num_iterations):\n        # forward propagation\n        A2,cache = forward_propagation_NN(x_train,parameters)\n        # compute cost\n        cost = compute_cost_NN(A2,y_train,parameters)\n        # backward propagation\n        grads = backward_propagation_NN(parameters,cache,x_train,y_train)\n        # update parameters\n        parameters = update_parameters_NN(parameters,grads)\n        \n        if i % 100 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    # predict\n    y_prediction_test = predict_NN(parameters,x_test)\n    y_prediction_train = predict_NN(parameters,x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters","20f98173":"parameters = two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=2500)","76186453":"# Importing Libraries","0867def5":"1. Forward propagation is almost same with logistic regression.\n2. The only difference is we use tanh function and we make all process twice.\n3. Also numpy has tanh function. So we do not need to implement it.","180c749b":"# Backward Propagation","c93783e4":"# Create Model","eccd846e":"Denetimli \u00f6\u011frenme : bir modelin etiketli (labeled) bir veri k\u00fcmesi \u00fczerine e\u011fitildi\u011fi yani hem input hem de output parametrelerine sahip bir t\u00fcrd\u00fcr.T\u00fcm veriler etiketlidir ve algoritmalar giri\u015f verilerinden \u00e7\u0131kt\u0131y\u0131 tahmin etmeyi \u00f6\u011frenir.\n\nDenetimsiz \u00f6\u011frenme : Denetimsiz: T\u00fcm veriler etiketsizdir ve algoritmalar, giri\u015f verilerinden do\u011fal yap\u0131y\u0131 \u00f6\u011frenir.\nDenetimsiz \u00f6\u011frenmenin amac\u0131, veriler hakk\u0131nda daha fazla bilgi edinmek i\u00e7in verilerin temelini olu\u015fturan yap\u0131y\u0131 veya da\u011f\u0131l\u0131m\u0131 modellemektir. Bunlara denetimsiz \u00f6\u011frenme denir, \u00e7\u00fcnk\u00fc yukar\u0131daki denetimli \u00f6\u011frenmenin aksine do\u011fru cevaplar yoktur ve \u00f6\u011fretmen yoktur. Bu algoritmalar \u00f6zellikle bizlerin veride ne arayaca\u011f\u0131m\u0131z\u0131 bilmedi\u011fimiz durumlarda yararl\u0131d\u0131r.Model g\u00f6zlem yoluyla \u00f6\u011frenir ve verilerdeki yap\u0131lar\u0131 bulur. Modele bir veri k\u00fcmesi verdi\u011fimizde, i\u00e7inde k\u00fcmeler olu\u015fturarak veri k\u00fcmesindeki desenleri ve ili\u015fkileri otomatik olarak bulur. Yapamayaca\u011f\u0131 \u015fey, k\u00fcmeye etiket eklemektir, bunlar \u015feker veya bunlar \u00e7ikolata diyemez, ancak t\u00fcm \u015fekerleri \u00e7ikolatadan ay\u0131racakt\u0131r.","d185a164":"# Update Parameters","6034a7de":"# Exploring the Dataset","57e299b3":"# Prediction with learnt parameters Weight and Bias","526cfaf0":"* Forward Propagation ","d60d89f5":"# Processing Dataset","67ac2bd9":"# Visualization","f5adc0a6":"# Logistic Regression with sklearn","224e6ebd":"# Layer Neural Network","55f0a3f1":"# Supervised Learning and Unsupervised Learning","0d32c1d0":"# Logistic Regression without sklearn","394547ff":"# Loss Function and Cost Function"}}