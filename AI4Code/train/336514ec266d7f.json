{"cell_type":{"90426718":"code","fde01568":"code","7819a078":"code","9735cfae":"code","3e0882bd":"code","1a64a46a":"code","6daeb68e":"code","8f43b895":"code","eb3036f4":"code","c00520a8":"code","4fec1a07":"code","77f5dd1e":"code","b22b5a43":"code","4c30e17f":"code","3e5244f8":"code","f6e8aaf5":"markdown","096e92be":"markdown","0001e86a":"markdown","108a457d":"markdown","35a80050":"markdown","8450f92b":"markdown","5acdedbd":"markdown","e2d710e0":"markdown","f649f99a":"markdown","00ed0eef":"markdown","b44b8520":"markdown","b4993b10":"markdown"},"source":{"90426718":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode,iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fde01568":"dataframe = pd.read_csv('..\/input\/voice.csv') #read to file","7819a078":"newDataframe = dataframe.loc[:,[\"label\"]]\nimport missingno as msno\nmsno.matrix(newDataframe)\nplt.show()","9735cfae":"msno.bar(newDataframe)\nplt.show()","3e0882bd":"dataframe.head() #TOP10 data","1a64a46a":"dataframe.label = [1 if(each == 'male') else 0 for each in dataframe.label] #1:Male 0: Female\ny = dataframe.label.values\nx_data = dataframe.drop([\"label\"],axis=1) \nx = (x_data - np.min(x_data))\/(np.max(x_data) - np.min(x_data)).values #Normalization\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n\nx_train = x_train.T\nx_test  = x_test.T\ny_train = y_train.T\ny_test  = y_test.T","6daeb68e":"def init_weights_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w, b\ndef sigmoid(z):\n    y_head = 1 \/ (1 + np.exp(-z))\n    return y_head","8f43b895":"def forward_backward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]    \n    \n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] \n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 \n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost, gradients","eb3036f4":"def update_weights_bias(w,b,x_train,y_train,learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    for i in range(number_of_iterarion):\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        w = w - learning_rate * gradients[\"derivative_weight\"] \n        b = b - learning_rate * gradients[\"derivative_bias\"]   \n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","c00520a8":"def predict(w,b,x_test):\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","4fec1a07":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):\n    dimension = x_train.shape[0]\n    w, b = init_weights_bias(dimension)\n\n    parameters,gradients,cost_list = update_weights_bias(w,b,x_train,y_train,learning_rate,num_iterations)\n\n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return y_prediction_test #Estimates for Complex Matrix\n    \ny_predict = logistic_regression(x_train, y_train, x_test, y_test, learning_rate=1, num_iterations=300)","77f5dd1e":"predict = []\nfor i in range(0,1):\n    for each in y_predict[i]:\n        predict.append(int(each))","b22b5a43":"truePredict = 0\nfalsePredict = 0\nfor p in range(len(predict)):\n    for y in range(p,len(y_test)):\n        if (predict[p] == y_test[y]):\n            truePredict = truePredict +1\n            break\n        else:\n            falsePredict = falsePredict +1\n            break\nprint(\"True Predict: \",truePredict)\nprint(\"False Predict\",falsePredict)\nprint(\"-------------------------------------------------------------------------------------\")\nprint(\"Predict: \",predict)\nprint(\"-------------------------------------------------------------------------------------\")\nprint(\"y_test: \",y_test)","4c30e17f":"x_Axis = [\"True\",\"False\"]\ny_Axis = [truePredict,falsePredict]\n\nplt.figure(figsize=(15,15))\nsns.barplot(x=x_Axis,y=y_Axis,palette = sns.cubehelix_palette(len(x_Axis)))\nplt.xlabel(\"Gender Class\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Male and Female\")\nplt.show()","3e5244f8":"conf_matrix = confusion_matrix(y_test,predict)\nf,ax = plt.subplots(figsize=(15,15))\nsns.heatmap(conf_matrix,annot=True,linewidths=0.5,linecolor=\"white\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"predict\")\nplt.show()","f6e8aaf5":"**Please see below**\n\n> The* Logistic Regression* algorithm returns data for the values 0 or 1. Conclusion We want you to come back as a man or woman. For this reason dataframe.label = [1 if(each == 'male') else 0 for each in dataframe.label]  the code line is converted to Male 0 Female 1.\n","096e92be":"**Content**\n\n* Working to Learn Male and Female Voices Using *Logistic Regression* Algorithm\n* I also plotted some graphs using *Data Visualization*\n\nKaan Can(https:\/\/www.kaggle.com\/kanncaa1) thanks for a Logistic Regression Algorithm","0001e86a":"**Please see below**\n\n> *The **forward_backward_propagation function **can be explained by the logical regression algorithm*\n\n*NOTE: This algorithm is described in detail* **https:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners**","108a457d":"**Please see below**\n> *The update function allows us to create our model (the w and b parameters are our model here)*","35a80050":"**Please see below**\n\n***Notes on Confusion Matrix***\n\n> It specifies which values are incorrectly estimated and which values are estimated correctly.\n> The complexity matrix is used to show us what our estimates are\n\n*NOTE: The above code is an example of a confusion matrix without using the sklearn library.*\n\nNOTE: **Number 1: Male** , **Number 0: Female**","8450f92b":"**Please see below**\n\n*The following code has **616** correct guess and **18** wrong guess*\n\n*A list of estimated values is available below **(Predict:)** The list of actual values is available below **(y_test:)***","5acdedbd":"**Please see below**\n\n> *The first 10 data in the data set*","e2d710e0":"**Notes On Logistic Regression**\n\nIn the logistic_regression function, learning_rate and num_iterations are very important\n\n> *learning_rate is the number of skips in our tilt calculations. It must be neither too big nor too small. If the cause is too large, the minimum cost value can never be found. If it is too small, it moves very slowly. In the above function we have chosen 1*\n> \n> *The num_iteration parameter allows you to specify the number of repetitions. As the parameter value increases, the learning fields are updated again*\n> \n> When **learning_rate = 1** and **num_iterations = 100** are selected, test validity is **92.74447949526814%** when **learning_rate = 1** and **num_iterations = 300** are selected, and test validity is **97.1608832807571%.***\n\n**NOTE**: We have chosen Learning_rate = 1, num_iterations = 300 as the default value. The test accuracy is 97.1608832807571  However, as we explained above, when we increase the num_iteration parameter, the test accuracy will also increase","f649f99a":"**Please see below**\n> *The update function helps you update weight and bias values*\n*We update these values to predict the test values correctly*","00ed0eef":"**Please see below**\n\n> *Y_predict values are of float type. We converted these values int.*","b44b8520":"**Please see below**\n\nThe label column is very important to us. In the chart below we show that there is no NaN(missing data) data\n\n**NOTE**: No white band because there is no *NaN* value","b4993b10":"**Please see below**\n\n> ***init_weights_bias** is where the initial value of weights and prime values are assigned.*\n> The values of 0.01 and 0.0 are the subject of the deep learning lesson. For now, it's nice to know these values by default"}}