{"cell_type":{"550b5e9c":"code","3380f37b":"code","17e1c334":"code","771da469":"code","f777cace":"code","2a6c4116":"code","112ea1d8":"code","236bbb7c":"code","33bdd5c9":"code","7d0d4a85":"code","7f0462eb":"code","73a1b599":"code","7fc08703":"code","9f500780":"code","8ddd7c8f":"code","8ea9f568":"code","dc548d1c":"code","de53c573":"code","e34a44da":"code","42591b6e":"code","6ff501ba":"code","2f824a26":"code","b09321e2":"markdown","f3050696":"markdown"},"source":{"550b5e9c":"from fastai.vision.all import *\nimport pandas as pd\nimport numpy as np\nimport albumentations\nimport cv2\nimport spacy\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nimport random","3380f37b":"random.seed(42) # consistent color palette\ndf = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","17e1c334":"# Override node attributes to customise the plot\nfrom spacy.tokens import Token\nToken.set_extension('plot', default={})  # Create a token underscore extension","771da469":"# https:\/\/github.com\/cyclecycle\/visualise-spacy-tree\/blob\/master\/visualise_spacy_tree\/visualise_spacy_tree.py\n\nimport pydot\n\nDEFAULT_NODE_ATTRS = {\n    'color': 'cyan',\n    'shape': 'box',\n    'style': 'filled',\n}\n\ndef node_label(token):\n    try:\n        label = token._.plot['label']\n    except:\n        label = '{0} [{1}]\\n({2} \/ {3})'.format(\n            token.orth_,\n            token.i,\n            token.pos_,\n            token.tag_\n        )\n    return label\n\n\ndef get_edge_label(from_token, to_token):\n    label = from_token.dep_\n    return label\n\n\ndef to_pydot(tokens, get_edge_label=get_edge_label):\n    graph = pydot.Dot(graph_type='graph')\n\n    # Add nodes to graph\n    idx2node = {}\n    for token in tokens:\n        try:\n            plot_attrs = token._.plot\n        except AttributeError:\n            plot_attrs = {}\n        for attr, val in DEFAULT_NODE_ATTRS.items():\n            if attr not in plot_attrs:\n                plot_attrs[attr] = val\n        label = node_label(token)\n        plot_attrs['name'] = token.i\n        plot_attrs['label'] = label\n        node = pydot.Node(**plot_attrs)\n        idx2node[token.i] = node\n        graph.add_node(node)\n\n    '''Add edges'''\n    for token in tokens:\n        if token.dep_ == 'ROOT':\n            continue\n        if token.head not in tokens:\n            continue\n        from_token = token\n        to_token = token.head\n        from_node = idx2node[from_token.i]\n        to_node = idx2node[to_token.i]\n        label = get_edge_label(from_token, to_token)\n        edge_color = dep2color[label]\n        edge = pydot.Edge(\n            to_node, from_node,\n            fontsize=12,\n            color=edge_color\n        )\n        graph.add_edge(edge)\n    return graph\n\ndef create_png(tokens, prog=None):\n    graph = to_pydot(tokens)\n    png = graph.create_png(prog=prog)\n    return png","f777cace":"deps = [\"ROOT\", \"acl\", \"acomp\", \"advcl\", \"advmod\", \"agent\", \"amod\", \"appos\", \"attr\", \"aux\", \"auxpass\", \n        \"case\", \"cc\", \"ccomp\", \"compound\", \"conj\", \"csubj\", \"csubjpass\", \"dative\", \"dep\", \"det\", \"dobj\", \n        \"expl\", \"intj\", \"mark\", \"meta\", \"neg\", \"nmod\", 'npadvmod', \"nsubj\", \"nsubjpass\", \"nummod\", \n        \"oprd\", \"parataxis\", \"pcomp\", \"pobj\", \"poss\", \"preconj\", \"predet\", \"prep\", \"prt\", \"punct\", \n        \"quantmod\", \"relcl\", \"xcomp\", \"\"]","2a6c4116":"tags = [\"$\", \"''\", \",\", \"-LRB-\", \"-RRB-\", \".\", \":\", \"ADD\", \"AFX\", \"CC\", \"CD\", \"DT\", \"EX\", \n        \"FW\", \"HYPH\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NFP\", \"NN\", \"NNP\", \"NNPS\", \n        \"NNS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"SYM\", \"TO\", \"UH\", \"VB\", \"VBD\", \n        \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"WDT\", \"WP\", \"WP$\", \"WRB\", \"XX\", \"``\", \"_SP\"]","112ea1d8":"cols = [\"aliceblue\", \"antiquewhite\", \"antiquewhite1\", \"antiquewhite2\", \"antiquewhite3\",\n\"antiquewhite4\", \"aqua\", \"aquamarine\", \"aquamarine1\", \"aquamarine2\",\n\"aquamarine3\", \"aquamarine4\", \"azure\", \"azure1\", \"azure2\",\n\"azure3\", \"azure4\", \"beige\", \"bisque\", \"bisque1\",\n\"bisque2\", \"bisque3\", \"bisque4\", \"black\", \"blanchedalmond\",\n\"blue\", \"blue1\", \"blue2\", \"blue3\", \"blue4\",\n\"blueviolet\", \"brown\", \"brown1\", \"brown2\", \"brown3\",\n\"brown4\", \"burlywood\", \"burlywood1\", \"burlywood2\", \"burlywood3\",\n\"burlywood4\", \"cadetblue\", \"cadetblue1\", \"cadetblue2\", \"cadetblue3\",\n\"cadetblue4\", \"chartreuse\", \"chartreuse1\", \"chartreuse2\", \"chartreuse3\",\n\"chartreuse4\", \"chocolate\", \"chocolate1\", \"chocolate2\", \"chocolate3\",\n\"chocolate4\", \"coral\", \"coral1\", \"coral2\", \"coral3\",\n\"coral4\", \"cornflowerblue\", \"cornsilk\", \"cornsilk1\", \"cornsilk2\",\n\"cornsilk3\", \"cornsilk4\", \"crimson\", \"cyan\", \"cyan1\",\n\"cyan2\", \"cyan3\", \"cyan4\", \"darkblue\", \"darkcyan\",\n\"darkgoldenrod\", \"darkgoldenrod1\", \"darkgoldenrod2\", \"darkgoldenrod3\", \"darkgoldenrod4\",\n\"darkgray\", \"darkgreen\", \"darkgrey\", \"darkkhaki\", \"darkmagenta\",\n\"darkolivegreen\", \"darkolivegreen1\", \"darkolivegreen2\", \"darkolivegreen3\", \"darkolivegreen4\",\n\"darkorange\", \"darkorange1\", \"darkorange2\", \"darkorange3\", \"darkorange4\",\n\"darkorchid\", \"darkorchid1\", \"darkorchid2\", \"darkorchid3\", \"darkorchid4\",\n\"darkred\", \"darksalmon\", \"darkseagreen\", \"darkseagreen1\", \"darkseagreen2\",\n\"darkseagreen3\", \"darkseagreen4\", \"darkslateblue\", \"darkslategray\", \"darkslategray1\",\n\"darkslategray2\", \"darkslategray3\", \"darkslategray4\", \"darkslategrey\", \"darkturquoise\",\n\"darkviolet\", \"deeppink\", \"deeppink1\", \"deeppink2\", \"deeppink3\",\n\"deeppink4\", \"deepskyblue\", \"deepskyblue1\", \"deepskyblue2\", \"deepskyblue3\",\n\"deepskyblue4\", \"dimgray\", \"dimgrey\", \"dodgerblue\", \"dodgerblue1\",\n\"dodgerblue2\", \"dodgerblue3\", \"dodgerblue4\", \"firebrick\", \"firebrick1\",\n\"firebrick2\", \"firebrick3\", \"firebrick4\", \"floralwhite\", \"forestgreen\",\n\"fuchsia\", \"gainsboro\", \"ghostwhite\", \"gold\", \"gold1\",\n\"gold2\", \"gold3\", \"gold4\", \"goldenrod\", \"goldenrod1\",\n\"goldenrod2\", \"goldenrod3\", \"goldenrod4\", \"gray\"]","236bbb7c":"colors = random.choices(cols, k=len(deps))\ndep2color = {}\nfor i,dep in enumerate(deps):\n    dep2color[dep] = colors[i]","33bdd5c9":"colors = random.choices(cols, k=len(tags))\ntag2color = {}\nfor i,tag in enumerate(tags):\n    tag2color[tag] = colors[i]","7d0d4a85":"# Override node attributes to customise the plot\n# https:\/\/graphviz.gitlab.io\/_pages\/doc\/info\/attrs.html\ndef customize_plot(doc):\n    for token in doc:\n        token._.plot['label'] = \"  \" * len(token.orth_)\n        token._.plot['color'] = tag2color[token.tag_]\n    return doc","7f0462eb":"nlp = spacy.load(\"en_core_web_sm\")","73a1b599":"out_folder = '\/tmp\/images'\nos.makedirs(out_folder, exist_ok=True)\npath = Path(out_folder)","7fc08703":"def convert_text_to_image(text, path, identifier, scale=0.1, size=512):\n    doc_folder = path\/identifier\n    os.makedirs(str(doc_folder), exist_ok=True)\n    doc = nlp(text)\n    sentence_spans = list(doc.sents)\n    for i, sent in enumerate(sentence_spans):\n        doc = nlp(str(sent))\n        doc = customize_plot(doc)\n        png = create_png(doc)\n        fname = f'{str(i)}.png'\n        save_path = doc_folder\/fname\n        with open(str(save_path), 'wb') as f:\n            f.write(png)","9f500780":"for i in tqdm(range(len(df))):\n    text = df.excerpt.loc[i]\n    identifier = df['id'].loc[i]\n    convert_text_to_image(text, path, identifier)","8ddd7c8f":"IMG_SIZE = 448","8ea9f568":"class BagOfImagesModel(Module):\n    def __init__(self, encoder):\n        self.encoder = encoder\n        self.bn1 = nn.BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.d1 = nn.Dropout(p=0.25, inplace=False)\n        self.l1 = nn.Linear(in_features=4096, out_features=512, bias=False)\n        self.bn2 = nn.BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        self.d2 = nn.Dropout(p=0.5, inplace=False)\n        self.l2 = nn.Linear(in_features=512, out_features=1, bias=False)\n\n    def forward(self, imgs):\n        b,n,ch,h,w = imgs.shape\n        unrolled = imgs.reshape(-1,ch,h,w)\n        ftrs = self.encoder(unrolled).squeeze()\n        num_ftrs = ftrs.shape[-1]\n        ftrs = ftrs.reshape(b,n,num_ftrs)        \n        ftrs_max = torch.max(ftrs, 1, keepdim=True)[0].squeeze()\n        ftrs_mean = torch.mean(ftrs, 1, keepdim=True).squeeze()\n        if b == 1: # error with batch size 1 being squeezed out above\n            ftrs_max = ftrs_max[None, ...]\n            ftrs_mean = ftrs_mean[None, ...]\n        ftrs_cat = torch.cat([ftrs_max, ftrs_mean], 1)\n        x = self.bn1(ftrs_cat)\n        x = self.d1(x)\n        x = self.l1(x)\n        x = F.relu(x)\n        x = self.bn2(x)\n        x = self.d2(x)\n        out = self.l2(x) \n        return out","dc548d1c":"aug = albumentations.Compose([\n        albumentations.LongestMaxSize(max_size=IMG_SIZE, p=1.0),\n        albumentations.PadIfNeeded(min_height=IMG_SIZE, min_width=IMG_SIZE, border_mode=0, value=0., p=1.0),\n        albumentations.Normalize(p=1.0)],\n    p=1.)","de53c573":"# for test, we will use all sentences \/ images for each example with batch size = 1\n\nclass TestImageBagDataset(torch.utils.data.Dataset):\n    def __init__(self, df, path, aug):\n        self.df = df\n        self.path = path\n        self.aug = aug\n        \n    def __getitem__(self, i):\n        image_id = self.df['id'].loc[i]\n        target = torch.tensor(0, dtype=torch.float)\n        img_folder = self.path\/image_id\n        num_imgs = len(img_folder.ls())\n        img_paths = [self.path\/f'{image_id}\/{i}.png' for i in range(num_imgs)]\n        imgs = [self._open_img(x) for x in img_paths]\n        imgs = torch.stack(imgs)\n        return (imgs, target)\n    \n    def __len__(self): \n        return len(self.df)\n    \n    def _open_img(self, x):\n        img = cv2.imread(str(x), cv2.IMREAD_UNCHANGED)[...,:3]\n        img = self.aug(image=img)['image']\n        img = torch.tensor(img, dtype=torch.float)\n        img = img.permute(2,0,1)\n        return img","e34a44da":"from matplotlib import pyplot as plt\ndef visualize(image):\n    plt.figure(figsize=(10, 10))\n    plt.axis('off')\n    plt.imshow(image)\n\ndataset = TestImageBagDataset(df, path, aug)\nvisualize(dataset[1][0][0].permute(1,2,0))","42591b6e":"all_preds = []\nfor k in range(5):\n    test_ds = TestImageBagDataset(df, path, aug)\n    test_dls = DataLoaders.from_dsets(test_ds, test_ds, bs=1).cuda() # a hack to get dataloaders, there is probably a better way\n    encoder = create_body(resnet50, pretrained=False, cut=-1)\n    net = BagOfImagesModel(encoder)\n    net = net.cuda()\n    learn = Learner(test_dls, net, loss_func=MSELossFlat(), metrics=rmse, model_dir=\".\").to_fp16()\n    learn = learn.load(f'..\/input\/commonlit-cv-models-resnet50\/model\/model_{k}')\n    preds, _ = learn.get_preds()\n    all_preds.append(preds)","6ff501ba":"preds = torch.stack(all_preds).mean(0).cpu().numpy()","2f824a26":"sub = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsub.target = preds\nsub.to_csv('submission.csv', index=False)","b09321e2":"# Model Inference","f3050696":"# Generate Images"}}