{"cell_type":{"fab70ff6":"code","9c96da97":"code","d2d91958":"code","b227a00e":"code","74b5345b":"code","a849bef7":"code","28cdffd9":"code","8bc9cd2f":"code","aeeb7674":"code","d8f8f386":"code","14385761":"code","c5c8c5f4":"code","5b885918":"code","edd2f40d":"code","8fc6aa9e":"code","b41ae573":"code","0f4b5fc4":"code","5993879d":"code","ecfacd4f":"code","968fd977":"code","3002c162":"markdown","5de7f148":"markdown","de331021":"markdown","8e91cc0a":"markdown","4c317bd2":"markdown","dc968f6d":"markdown","4c33cfa5":"markdown","64e3d958":"markdown","0a38873e":"markdown","455692b8":"markdown","fe373ea3":"markdown","af184c02":"markdown","8d55ef42":"markdown","9da5be08":"markdown"},"source":{"fab70ff6":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import classification_report\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n","9c96da97":"data_full = pd.read_csv('..\/input\/student-alcohol-consumption\/student-mat.csv')","d2d91958":"data_full.head()","b227a00e":"data_full.shape","74b5345b":"data_full.dtypes","a849bef7":"corr = data_full.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True, annot=True)\n\n","28cdffd9":"data_full.isnull().sum()","8bc9cd2f":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.histplot(data_full['G3'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"final grade\")\nax.set(title=\"Final grade distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","aeeb7674":"y = data_full['G3']\n\ndata_full['G3'] = [1 if x >= 10 else 0 for x in data_full['G3']]\n\ny_bool = data_full['G3']\n\ndata = data_full.drop(['G3'], axis=1)\n\n","d8f8f386":"# One-hot encode the data \ndata = pd.get_dummies(data).reset_index(drop=True)\n","14385761":"# Select features with lasso\n\nfeature_sel_model = SelectFromModel(Lasso(alpha=0.03, random_state=0)) \nfeature_sel_model.fit(data, y)\nfeature_sel_model.get_support()\n\nselected_feat = data.columns[(feature_sel_model.get_support())]\n\nprint('total features: {}'.format((data.shape[1])))\nprint('selected features: {}'.format(len(selected_feat)))\nprint('features with coefficients shrank to zero: {}'.format(\n    np.sum(feature_sel_model.estimator_.coef_ == 0)))\ndata = data[selected_feat]","c5c8c5f4":"data.head()","5b885918":"# Regression\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(data, y, test_size=0.2) \n\nDT_model = tree.DecisionTreeRegressor()\nRF_model = RandomForestRegressor(n_estimators=500, random_state=0) # value of n_estimators based on Cortez & Silva \nLinR_model = LinearRegression()\nNN_model = MLPRegressor(random_state=0, hidden_layer_sizes=(393, 395, 395), max_iter=1500, early_stopping=True)\n# Cortez & Silva use, 1 layer, 100 epochs of the BFGS algorithm, but this does not perform well in my case\nXGB_model = XGBRegressor(n_estimators=100, learning_rate=0.08)\n\nDT_model.fit(X_train, Y_train)\nRF_model.fit(X_train, Y_train)\nLinR_model.fit(X_train, Y_train)\nNN_model.fit(X_train, Y_train)\nXGB_model.fit(X_train, Y_train)\n\nDT_prediction = DT_model.predict(X_valid)\nprint(\"Decision Tree score: \", DT_model.score(X_train,Y_train))\nmae_DT_prediction = mean_absolute_error(DT_prediction, Y_valid)\nprint(\"Mean Absolute Error: \", mae_DT_prediction)\n\nRF_prediction = RF_model.predict(X_valid)\nprint(\"Random Forest score: \", RF_model.score(X_train,Y_train))\nmae_RF_prediction = mean_absolute_error(RF_prediction, Y_valid)\nprint(\"Mean Absolute Error: \", mae_RF_prediction)\n\nLinR_prediction = LinR_model.predict(X_valid)\nprint(\"Linear Regression score: \", LinR_model.score(X_train,Y_train))\nmae_LinR_prediction = mean_absolute_error(LinR_prediction, Y_valid)\nprint(\"Mean Absolute Error: \", mae_LinR_prediction)\n\nNN_prediction = NN_model.predict(X_valid)\nprint(\"Neural Network score: \", NN_model.score(X_train,Y_train))\nmae_NN_prediction = mean_absolute_error(NN_prediction, Y_valid)\nprint(\"Mean Absolute Error: \", mae_NN_prediction)\n\nXGB_prediction = XGB_model.predict(X_valid)\nprint(\"XGBoost score: \", XGB_model.score(X_train,Y_train))\nmae_XGB_prediction = mean_absolute_error(XGB_prediction, Y_valid)\nprint(\"Mean Absolute Error: \", mae_XGB_prediction)\n","edd2f40d":"# Classification\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(data, y_bool, test_size=0.2) \n\nLR_model = LogisticRegression(max_iter=500)\nSVC_model = SVC()\nKNN_model = KNeighborsClassifier(n_neighbors=5)\n\nLR_model.fit(X_train, Y_train)\nSVC_model.fit(X_train, Y_train)\nKNN_model.fit(X_train, Y_train)\n\nLR_prediction = LR_model.predict(X_valid)\nSVC_prediction = SVC_model.predict(X_valid)\nKNN_prediction = KNN_model.predict(X_valid)\n\n# print classification report\nprint('LR_prediction \\n', classification_report(LR_prediction, Y_valid))\nprint('SVC_prediction \\n',classification_report(SVC_prediction, Y_valid))\nprint('KNN_prediction \\n',classification_report(KNN_prediction, Y_valid))","8fc6aa9e":"data_clean = data_full.drop(['G3', 'G2', 'G1'], axis=1)","b41ae573":"# One-hot encode the data \ndata_clean = pd.get_dummies(data_clean).reset_index(drop=True)","0f4b5fc4":"# Select features with lasso\n\nfeature_sel_model = SelectFromModel(Lasso(alpha=0.01, random_state=0)) \nfeature_sel_model.fit(data_clean, y)\nfeature_sel_model.get_support()\n\nselected_feat = data_clean.columns[(feature_sel_model.get_support())]\n\nprint('total features: {}'.format((data_clean.shape[1])))\nprint('selected features: {}'.format(len(selected_feat)))\nprint('features with coefficients shrank to zero: {}'.format(\n    np.sum(feature_sel_model.estimator_.coef_ == 0)))\ndata = data_clean[selected_feat]","5993879d":"data.head()","ecfacd4f":"# Regression\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(data_clean, y, test_size=0.2) \n\nDT_model = tree.DecisionTreeRegressor()\nRF_model = RandomForestRegressor(n_estimators=500, random_state=0) \nLinR_model = LinearRegression()\nNN_model = MLPRegressor(random_state=0, hidden_layer_sizes=(395, 395, 395), max_iter=1500, early_stopping=True)\nXGB_model = XGBRegressor(n_estimators=100, learning_rate=0.05)\n\nDT_model.fit(X_train, Y_train)\nRF_model.fit(X_train, Y_train)\nLinR_model.fit(X_train, Y_train)\nNN_model.fit(X_train, Y_train)\nXGB_model.fit(X_train, Y_train)\n\nDT_prediction = DT_model.predict(X_valid)\nprint(\"Decision Tree score: \", DT_model.score(X_train,Y_train))\nmae_DT_prediction = mean_absolute_error(DT_prediction, Y_valid)\nprint(\"Mean Absolute Error: \", mae_DT_prediction)\n\nRF_prediction = RF_model.predict(X_valid)\nprint(\"Random Forest score: \", RF_model.score(X_train,Y_train))\nmae_RF_prediction = mean_absolute_error(RF_prediction, Y_valid)\nprint(\"Mean Absolute Error: \", mae_RF_prediction)\n\nLinR_prediction = LinR_model.predict(X_valid)\nprint(\"Linear Regression score: \", LinR_model.score(X_train,Y_train))\nmae_LinR_prediction = mean_absolute_error(LinR_prediction, Y_valid)\nprint(\"Mean Absolute Error: \", mae_LinR_prediction)\n\nNN_prediction = NN_model.predict(X_valid)\nprint(\"Neural Network score: \", NN_model.score(X_train,Y_train))\nmae_NN_prediction = mean_absolute_error(NN_prediction, Y_valid)\nprint(\"Mean Absolute Error: \", mae_NN_prediction)\n\nXGB_prediction = XGB_model.predict(X_valid)\nprint(\"XGBoost score: \", XGB_model.score(X_train,Y_train))\nmae_XGB_prediction = mean_absolute_error(XGB_prediction, Y_valid)\nprint(\"Mean Absolute Error: \", mae_XGB_prediction)\n","968fd977":"# Classification\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(data_clean, y_bool, test_size=0.2) \n\nLR_model = LogisticRegression(max_iter=500)\nSVC_model = SVC()\nKNN_model = KNeighborsClassifier(n_neighbors=5)\n\nLR_model.fit(X_train, Y_train)\nSVC_model.fit(X_train, Y_train)\nKNN_model.fit(X_train, Y_train)\n\nLR_prediction = LR_model.predict(X_valid)\nSVC_prediction = SVC_model.predict(X_valid)\nKNN_prediction = KNN_model.predict(X_valid)\n\n# print classification report\nprint('LR_prediction \\n', classification_report(LR_prediction, Y_valid))\nprint('SVC_prediction \\n',classification_report(SVC_prediction, Y_valid))\nprint('KNN_prediction \\n',classification_report(KNN_prediction, Y_valid))\n","3002c162":"The heatmap below shows the correlation between the various features. The variables with the strongest correlation are G1 - first period grade and G2 - second period grade. To a lesser extent other features like 'failures' and the level of eduction of the student's mother contribute to the exam results. Alcohol consumption does not seem to influence much in this dataset.\n\n","5de7f148":"As we have seen in the heatmap, not all features seem to be relevant, especially since G1 and G2 are much stronger correlated to G3 than the other variables. \nWe can use Lasso to select the most relevant variables. ","de331021":"Next, we will analyse the distribution of the dependent variable, G3 - final grade. It seems like an almost normal distribution. However, there are almost 40 students with 0 marks, which could be meaningful.","8e91cc0a":"This dataset is  obtained in a survey of students math and portuguese language courses in a secondary school. It contains social, gender and study information about the students. It vcan be used for some EDA or to predict students final grade.I used the publication by Paolo Cortez and Alice Silva as input to try out some regression and classification methods.\n\nI will perform some basic EDA, prepare the data, perform some feature selection and try out some regression and classification methods. Finally I will reflect on the central question of Cortez & Silva: can we predict a student's final grade?  \n\nI am beginning my data science journey. So there will be mistakes in this notebook. Constructive critisism and suggestions for improvement are very much welcome. I used some code snippets of others and mention them in the sources at the end.\n\n","4c317bd2":"The algorithms seem to work well. The neural network is outperformed by others. Cortez & Silva explain that this is due to the presenc of irrelevant data. In my case it could also be a lack of understanding how to tweak MLPRegressor correctly. As pointed out by Cortez & Silva, student achievement is highly influenced by previous performances, in particular by the variables G2 and G1. However, I would argue that this does not explain well what makes a student successful or not. It is obivious that a successful student will pass all the exams, but what makes this person to stand out? I would argue that exams are results not inputs. Passing the G1, G2 and G3 exams is the result of other factors. Otherwise, no additional study effort would be necessary after the first exam. Does this dataset provide some insight in other variables that could predict a student's final mark? Therefore, let's drop the exam results and focus on the other features, perform the same pipeline and see whether the remaining data can predict the final exam outcomes.\n","dc968f6d":"Let's check if there is any missing data.","4c33cfa5":"Now let's split the dataset in dependent (G3) and independent variables. I think we can try to use this dataset to test both regression and classification algorithms. I will use some methods to predict student's scores and also a few algorithms to predict whether a student will pass or fail the final exam. Therefore, I will prepare a binary version of the y-values for classification. ","64e3d958":"Now let's see if we can use some classification algorithms to predict failure or success.","0a38873e":"Let's select a bit larger set of features","455692b8":"Without considering the previous exam results, the algorithms struggle to predict well. In the case of classification, the models present strong class imbalances, as they prodominantly predict that students will pass the exam (especially in the case of SVM). The same situation occurs for the regression algorithms. Here, the MAE is much worse, making this dataset less useable for predictions. \nIt seems to me that the dataset does not help very much to explain what makes a student successful or not. To predict student performance better, probably other variables might be considered, like the quality of the school, the teachers and learning materials used, motivation and attitude of the student, their IQ, the composition of the class and socio-economical factors.\n\n","fe373ea3":"Let's see whether our data is categorical or numerical","af184c02":"I will use the four regressions methods that are mentioned in the paper by Paulo Cortez and Alice Silva. These are Decision Trees, Random Forest, Neural Network and Linear Regresson. I add XGBoost, as it was used in Kaggle's Intermediate Machine Learning course.","8d55ef42":"For this notebook I will only use the math dataset. Let start with getting some insights in the data.","9da5be08":"Sources:\n\nUSING DATA MINING TO PREDICT SECONDARY SCHOOL STUDENT PERFORMANCE, Paulo Cortez and Alice Silva\nhttp:\/\/www3.dsi.uminho.pt\/pcortez\/student.pdf\n\nLasso: https:\/\/github.com\/krishnaik06\/Advanced-House-Price-Prediction-\n\nKaggle Intermediate Machine Learing Course: https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning\n\nClassification; https:\/\/stackabuse.com\/overview-of-classification-methods-in-python-with-scikit-learn\/\n\nImage: https:\/\/odinland.vn\/a-comprehensive-guide-about-the-education-system-in-portugal\/?lang=en\nhttps:\/\/www.youtube.com\/watch?v=1JXrxCJoHuw&list=PLZoTAELRMXVMcRQwR5_J8k9S7cffVFq_U&index=6"}}