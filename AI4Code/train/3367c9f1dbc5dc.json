{"cell_type":{"9b49ffc4":"code","b259b6fd":"code","042461fe":"code","07aa4037":"code","467700f4":"code","67991997":"code","5f953f4a":"code","6c600e83":"code","95824a99":"code","d0e5e20c":"code","f4905983":"code","11649220":"markdown","f0bfab85":"markdown","64f35333":"markdown","b6932c19":"markdown","9087fe69":"markdown","141ed8e2":"markdown","3becd6b4":"markdown"},"source":{"9b49ffc4":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier","b259b6fd":"df_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","042461fe":"df_train.head()\n","07aa4037":"df_test.head()","467700f4":"df_train.tail()","67991997":"df_test.tail()","5f953f4a":"women = df_train.loc[df_train['Sex'] == \"female\", \"Survived\"]\nwomen","6c600e83":"womenRate = sum(women)\/ len(women)\nwomenRate\nwomenRate = round(womenRate * 100)\nprint(f\"Rate of women who survived: {womenRate}%\")","95824a99":"men = df_train.loc[df_train['Sex'] == 'male', 'Survived']\nmenRate = sum(men)\/len(men)\nmenRate = round(100* menRate)\n\n\nprint(f\"Rate of men who survived: {menRate}%\")","d0e5e20c":"y = df_train[\"Survived\"]\n\nfeatures =[\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nx = pd.get_dummies(df_train[features])\nx_test = pd.get_dummies(df_test[features])\n\nmodel = RandomForestClassifier(n_estimators=1000, max_depth=5, random_state=1)\nmodel.fit(x, y)\npredictions = model.predict(x_test)\n\noutput = pd.DataFrame({'PassengerId': df_test.PassengerId, 'Survived': predictions})","f4905983":"output.to_csv('sp_submission.csv', index=False)","11649220":"Since '1' is used to indicate 'surived', we can sum up all the '1s' and divide by total number of female passengers in order to get the rate of survival. \nWhat I do here is calculate that rate, and multiply it by 100 in order be able to present this in a percentage form folks are familiar with (i.e. 74%). I also use the python function **round()** since I just want to round to the nearest whole number.","f0bfab85":"Let's take a look to see if our data loaded correctly by using **.head()** and **.tail()** to look at the beginning and end of the file. Both functions can take a number argument, but by default show you 5 rows of data.","64f35333":"I do the same thing for males. I also save the percentage number as a variable, 'menRate', and use an f-string to print the result.","b6932c19":"The following is the standard random forest modeling. There are a lot of good reasons why random forests seems to be the go-to for this dataset, but I would love to try using a logistic regression model in the future to see how it compares.\n\nThe features variable contains a list of the fields that are likely important in being able to classify survival rate. I will make a post in the future changing a few things to see how performance is affected, and going into a bit more detail as to why they're important.\n\nIf you're familiar with decision trees, you might be interesting in changing a few other parameters, such as 'n_estimators' (i.e. the number of 'trees' in the 'forest'). The final step is then to save the predictions, and see how you fare!","9087fe69":"Load the titanic **training** dataset from kaggle; the **read_csv()** function is imported from the pandas library.\nI'll also go ahead and load the **testing** dataset as well.","141ed8e2":"Welcome! This is my first attempt with the (iconic) **Titanic dataset** on kaggle. My goal is to use a few different machine learning algorithms, using both the Python and R environment. Here's the initial attempt using **random forests**.","3becd6b4":"Now we'll create a 'women' variable that'll be used to calculate the rate of survival for women on the titanic.\nI can use the pandas function .**loc** to filter by this criteria, creating a new dataframe that contains a \"Survive\" column for women. "}}