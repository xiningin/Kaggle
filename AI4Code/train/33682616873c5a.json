{"cell_type":{"5fefba15":"code","84ccc042":"code","05515984":"code","7843c7d2":"code","0def9835":"code","fc9f9ede":"code","0a242dcf":"code","c3c623b8":"code","7e700a23":"code","22c7544d":"code","457f1b1e":"code","4a1c73cd":"code","7e1e7284":"code","c809dd9c":"code","bbfc81e2":"code","7835c0d5":"code","a008885d":"code","af46721e":"code","a368550e":"code","aa1520a1":"code","396b35bd":"code","c95cc190":"code","3677603b":"code","77b995fd":"code","206791f2":"code","57d87ccf":"code","2fb8eed0":"code","33ee9d10":"code","472ffa8c":"code","54ff774d":"code","f9f37db4":"code","5bbb4729":"code","3a8618dd":"code","ea93f461":"code","c8114da6":"code","9852f710":"code","cc06f997":"code","00fab743":"code","c6523adc":"code","a02bd55a":"code","0371c4a2":"code","bdceaef2":"code","fd809f29":"code","ebac8452":"code","416d7fba":"code","7c3027dd":"code","15be5ed1":"code","5b8cd778":"markdown","2f65e9a4":"markdown","6b1dbdf7":"markdown","f17dfbc1":"markdown","d650c21e":"markdown","96955212":"markdown","0d428e6d":"markdown","39088b2e":"markdown","51239ab6":"markdown","b002871c":"markdown","66120fe7":"markdown","b6e40d94":"markdown","a19e3536":"markdown","43f368b2":"markdown","004236b1":"markdown","a44d28f5":"markdown","70be90bb":"markdown","d73002ff":"markdown","bfa9cd20":"markdown","f3284699":"markdown","c4e14eca":"markdown","c39b10e0":"markdown","4ef5ec3a":"markdown","ad0df1d5":"markdown","7c509853":"markdown","99cecfd7":"markdown","99701c4f":"markdown","fabeeb8e":"markdown","4acef302":"markdown","097714c8":"markdown"},"source":{"5fefba15":"# Import Numpy & PyTorch\nimport numpy as np\nimport torch","84ccc042":"# Create tensors.\nx = torch.tensor(3.)\nw = torch.tensor(4., requires_grad=True)\nb = torch.tensor(5., requires_grad=True)","05515984":"# Print tensors\nprint(x)\nprint(w)\nprint(b)","7843c7d2":"# Arithmetic operations\ny = w * x + b\nprint(y)","0def9835":"# Compute gradients\ny.backward()","fc9f9ede":"# Display gradients\nprint('dy\/dw:', w.grad)\nprint('dy\/db:', b.grad)","0a242dcf":"# Input (temp, rainfall, humidity)\ninputs = np.array([[73, 67, 43], \n                   [91, 88, 64], \n                   [87, 134, 58], \n                   [102, 43, 37], \n                   [69, 96, 70]], dtype='float32')","c3c623b8":"# Targets (apples, oranges)\ntargets = np.array([[56, 70], \n                    [81, 101], \n                    [119, 133], \n                    [22, 37], \n                    [103, 119]], dtype='float32')","7e700a23":"# Convert inputs and targets to tensors\ninputs = torch.from_numpy(inputs)\ntargets = torch.from_numpy(targets)\nprint(inputs)\nprint(targets)","22c7544d":"# Weights and biases\nw = torch.randn(2, 3, requires_grad=True)\nb = torch.randn(2, requires_grad=True)\nprint(w)\nprint(b)","457f1b1e":"# Define the model\ndef model(x):\n    return x @ w.t() + b","4a1c73cd":"# Generate predictions\npreds = model(inputs)\nprint(preds)","7e1e7284":"# Compare with targets\nprint(targets)","c809dd9c":"# MSE loss\ndef mse(t1, t2):\n    diff = t1 - t2\n    return torch.sum(diff * diff) \/ diff.numel()","bbfc81e2":"# Compute loss\nloss = mse(preds, targets)\nprint(loss)","7835c0d5":"# Compute gradients\nloss.backward()","a008885d":"# Gradients for weights\nprint(w)\nprint(w.grad)","af46721e":"# Gradients for bias\nprint(b)\nprint(b.grad)","a368550e":"w.grad.zero_()\nb.grad.zero_()\nprint(w.grad)\nprint(b.grad)","aa1520a1":"# Generate predictions\npreds = model(inputs)\nprint(preds)","396b35bd":"# Calculate the loss\nloss = mse(preds, targets)\nprint(loss)","c95cc190":"# Compute gradients\nloss.backward()","3677603b":"# Adjust weights & reset gradients\nwith torch.no_grad():\n    w -= w.grad * 1e-5\n    b -= b.grad * 1e-5\n    w.grad.zero_()\n    b.grad.zero_()","77b995fd":"print(w)","206791f2":"# Calculate loss\npreds = model(inputs)\nloss = mse(preds, targets)\nprint(loss)","57d87ccf":"# Train for 100 epochs\nfor i in range(100):\n    preds = model(inputs)\n    loss = mse(preds, targets)\n    loss.backward()\n    with torch.no_grad():\n        w -= w.grad * 1e-5\n        b -= b.grad * 1e-5\n        w.grad.zero_()\n        b.grad.zero_()","2fb8eed0":"# Calculate loss\npreds = model(inputs)\nloss = mse(preds, targets)\nprint(loss)","33ee9d10":"# Print predictions\npreds","472ffa8c":"# Print targets\ntargets","54ff774d":"# Imports\nimport torch.nn as nn","f9f37db4":"# Input (temp, rainfall, humidity)\ninputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70], [73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70], [73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70]], dtype='float32')\n# Targets (apples, oranges)\ntargets = np.array([[56, 70], [81, 101], [119, 133], [22, 37], [103, 119], \n                    [56, 70], [81, 101], [119, 133], [22, 37], [103, 119], \n                    [56, 70], [81, 101], [119, 133], [22, 37], [103, 119]], dtype='float32')","5bbb4729":"inputs = torch.from_numpy(inputs)\ntargets = torch.from_numpy(targets)","3a8618dd":"# Import tensor dataset & data loader\nfrom torch.utils.data import TensorDataset, DataLoader","ea93f461":"# Define dataset\ntrain_ds = TensorDataset(inputs, targets)\ntrain_ds[0:3]","c8114da6":"# Define data loader\nbatch_size = 5\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True)\nnext(iter(train_dl))","9852f710":"# Define model\nmodel = nn.Linear(3, 2)\nprint(model.weight)\nprint(model.bias)","cc06f997":"# Define optimizer\nopt = torch.optim.SGD(model.parameters(), lr=1e-5)","00fab743":"# Import nn.functional\nimport torch.nn.functional as F","c6523adc":"# Define loss function\nloss_fn = F.mse_loss","a02bd55a":"loss = loss_fn(model(inputs), targets)\nprint(loss)","0371c4a2":"# Define a utility function to train the model\ndef fit(num_epochs, model, loss_fn, opt):\n    for epoch in range(num_epochs):\n        for xb,yb in train_dl:\n            # Generate predictions\n            pred = model(xb)\n            loss = loss_fn(pred, yb)\n            # Perform gradient descent\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n    print('Training loss: ', loss_fn(model(inputs), targets))","bdceaef2":"# Train the model for 100 epochs\nfit(100, model, loss_fn, opt)","fd809f29":"# Generate predictions\npreds = model(inputs)\npreds","ebac8452":"# Compare with targets\ntargets","416d7fba":"class SimpleNet(nn.Module):\n    # Initialize the layers\n    def __init__(self):\n        super().__init__()\n        self.linear1 = nn.Linear(3, 3)\n        self.act1 = nn.ReLU() # Activation function\n        self.linear2 = nn.Linear(3, 2)\n    \n    # Perform the computation\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.act1(x)\n        x = self.linear2(x)\n        return x","7c3027dd":"model = SimpleNet()\nopt = torch.optim.SGD(model.parameters(), 1e-5)\nloss_fn = F.mse_loss","15be5ed1":"fit(100, model, loss_fn, opt)","5b8cd778":"The matrix obtained by passing the input data to the model is a set of predictions for the target variables.","2f65e9a4":"With the new weights and biases, the model should have a lower loss.","6b1dbdf7":"The gradients are stored in the `.grad` property of the respective tensors.","f17dfbc1":"Before we build a model, we need to convert inputs and targets to PyTorch tensors.","d650c21e":"## Linear Regression Model using PyTorch built-ins\n\nLet's re-implement the same model using some built-in functions and classes from PyTorch.","96955212":"### nn.Linear\nInstead of initializing the weights & biases manually, we can define the model using `nn.Linear`.","0d428e6d":"### Optimizer\nInstead of manually manipulating the weights & biases using gradients, we can use the optimizer `optim.SGD`.","39088b2e":"### Train the model\n\nWe are ready to train the model now. We can define a utility function `fit` which trains the model for a given number of epochs.","51239ab6":"## Adjust weights and biases using gradient descent\n\nWe'll reduce the loss and improve our model using the gradient descent algorithm, which has the following steps:\n\n1. Generate predictions\n2. Calculate the loss\n3. Compute gradients w.r.t the weights and biases\n4. Adjust the weights by subtracting a small quantity proportional to the gradient\n5. Reset the gradients to zero","b002871c":"# PyTorch basics - Linear Regression from scratch\n\n<!-- <iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/ECHX1s0Kk-o?controls=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe> -->\n\nTutorial inspired from [FastAI development notebooks](https:\/\/github.com\/fastai\/fastai_v1\/tree\/master\/dev_nb)\n\n## Machine Learning\n\n<img src=\"https:\/\/i.imgur.com\/oJEQe7k.png\" width=\"500\">\n\n\n## Tensors & Gradients","66120fe7":"We'll create a model that predicts crop yeilds for apples and oranges (*target variables*) by looking at the average temperature, rainfall and humidity (*input variables or features*) in a region. Here's the training data:\n\n<img src=\"https:\/\/i.imgur.com\/lBguUV9.png\" width=\"500\" \/>\n\nIn a **linear regression** model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :\n\n```\nyeild_apple  = w11 * temp + w12 * rainfall + w13 * humidity + b1\nyeild_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2\n```\n\nVisually, it means that the yield of apples is a linear or planar function of the temperature, rainfall & humidity.\n\n<img src=\"https:\/\/i.imgur.com\/mtkR2lB.png\" width=\"540\" >\n\n\n**Our objective**: Find a suitable set of *weights* and *biases* using the training data, to make accurate predictions.","b6e40d94":"The *model* is simply a function that performs a matrix multiplication of the input `x` and the weights `w` (transposed) and adds the bias `b` (replicated for each observation).\n\n$$\n\\hspace{2.5cm} X \\hspace{1.1cm} \\times \\hspace{1.2cm} W^T \\hspace{1.2cm}  + \\hspace{1cm} b \\hspace{2cm}\n$$\n\n$$\n\\left[ \\begin{array}{cc}\n73 & 67 & 43 \\\\\n91 & 88 & 64 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n69 & 96 & 70\n\\end{array} \\right]\n%\n\\times\n%\n\\left[ \\begin{array}{cc}\nw_{11} & w_{21} \\\\\nw_{12} & w_{22} \\\\\nw_{13} & w_{23}\n\\end{array} \\right]\n%\n+\n%\n\\left[ \\begin{array}{cc}\nb_{1} & b_{2} \\\\\nb_{1} & b_{2} \\\\\n\\vdots & \\vdots \\\\\nb_{1} & b_{2} \\\\\n\\end{array} \\right]\n$$","a19e3536":"## Linear Regression Model (from scratch)\n\nThe *weights* and *biases* can also be represented as matrices, initialized with random values. The first row of `w` and the first element of `b` are use to predict the first target variable i.e. yield for apples, and similarly the second for oranges.","43f368b2":"## Loss Function\n\nWe can compare the predictions with the actual targets, using the following method: \n* Calculate the difference between the two matrices (`preds` and `targets`).\n* Square all elements of the difference matrix to remove negative values.\n* Calculate the average of the elements in the resulting matrix.\n\nThe result is a single number, known as the **mean squared error** (MSE).","004236b1":"What makes PyTorch special, is that we can automatically compute the derivative of `y` w.r.t. the tensors that have `requires_grad` set to `True` i.e. `w` and `b`.","a44d28f5":"We can combine tensors with the usual arithmetic operations.","70be90bb":"## Train for multiple epochs\n\nTo reduce the loss further, we repeat the process of adjusting the weights and biases using the gradients multiple times. Each iteration is called an epoch.","d73002ff":"A key insight from calculus is that the gradient indicates the rate of change of the loss, or the slope of the loss function w.r.t. the weights and biases. \n\n* If a gradient element is **postive**, \n    * **increasing** the element's value slightly will **increase** the loss.\n    * **decreasing** the element's value slightly will **decrease** the loss.\n\n<img src=\"https:\/\/i.imgur.com\/2H4INoV.png\" width=\"400\" \/>\n\n\n\n* If a gradient element is **negative**,\n    * **increasing** the element's value slightly will **decrease** the loss.\n    * **decreasing** the element's value slightly will **increase** the loss.\n    \n<img src=\"https:\/\/i.imgur.com\/h7E2uAv.png\" width=\"400\" \/>    \n\nThe increase or decrease is proportional to the value of the gradient.","bfa9cd20":"Because we've started with random weights and biases, the model does not a very good job of predicting the target varaibles.","f3284699":"## Training Data\nThe training data can be represented using 2 matrices (inputs and targets), each with one row per observation and one column per variable.","c4e14eca":"A tensor is a number, vector, matrix or any n-dimensional array.","c39b10e0":"## Compute Gradients\n\nWith PyTorch, we can automatically compute the gradient or derivative of the `loss` w.r.t. to the weights and biases, because they have `requires_grad` set to `True`.","4ef5ec3a":"### Loss Function\nInstead of defining a loss function manually, we can use the built-in loss function `mse_loss`.","ad0df1d5":"Finally, we'll reset the gradients to zero before moving forward, because PyTorch accumulates gradients.","7c509853":"Finally, we can apply gradient descent to train the model using the same `fit` function defined earlier for linear regression.\n\n<img src=\"https:\/\/i.imgur.com\/g7Rl0r8.png\" width=\"500\">","99cecfd7":"# Bonus: Feedfoward Neural Network\n\n![ffnn](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/0\/00\/Multi-Layer_Neural_Network-Vector-Blank.svg\/400px-Multi-Layer_Neural_Network-Vector-Blank.svg.png)\n\nConceptually, you think of feedforward neural networks as two or more linear regression models stacked on top of one another with a non-linear activation function applied between them.\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*XxxiA0jJvPrHEJHD4z893g.png\" width=\"640\">\n\nTo use a feedforward neural network instead of linear regression, we can extend the `nn.Module` class from PyTorch.","99701c4f":"### Dataset and DataLoader\n\nWe'll create a `TensorDataset`, which allows access to rows from `inputs` and `targets` as tuples. We'll also create a DataLoader, to split the data into batches while training. It also provides other utilities like shuffling and sampling.","fabeeb8e":"## Problem Statement","4acef302":"The resulting number is called the **loss**, because it indicates how bad the model is at predicting the target variables. Lower the loss, better the model. ","097714c8":"Now we can define the model, optimizer and loss function exactly as before."}}