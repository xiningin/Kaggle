{"cell_type":{"cb3dda45":"code","baa47c57":"code","f6afdec6":"code","8f1d0a99":"code","f00ec572":"code","5c317bd7":"code","65a6ab99":"code","93cdfee1":"code","4a2787a7":"code","56668777":"code","eaa6e1d4":"code","138d435e":"code","41f9094d":"code","1b37f304":"code","72db1178":"markdown","ce78ee28":"markdown","67525d6e":"markdown","cbbd77c6":"markdown","a29686d6":"markdown","ff7fa0b1":"markdown","de5e272e":"markdown","7b6ee462":"markdown","e6ad37f7":"markdown","8a456098":"markdown"},"source":{"cb3dda45":"import pandas as pd\n\nmillion = pd.read_csv('..\/input\/million-headlines\/abcnews-date-text.csv', delimiter=',', nrows=200000)\ndata = million.drop(['publish_date'], axis=1).rename(columns={'headline_text': 'headline'})","baa47c57":"START = '\u00f7'\nEND = '\u25a0'","f6afdec6":"import numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\ndef format_data(data, max_features, maxlen, shuffle=False):\n    if shuffle:\n        data = data.sample(frac=1).reset_index(drop=True)\n    \n    # Add start and end tokens\n    data['headline'] = START + ' ' + data['headline'].str.lower() + ' ' + END\n\n    text = data['headline']\n    \n    # Tokenize text\n    filters = \"!\\\"#$%&()*+,-.\/;<=>?@[\\\\]^_`{|}~\\t\\n\"\n    tokenizer = Tokenizer(num_words=max_features, filters=filters)\n    tokenizer.fit_on_texts(list(text))\n    corpus = tokenizer.texts_to_sequences(text)\n    \n    # Build training sequences of (context, next_word) pairs.\n    # Note that context sequences have variable length. An alternative\n    # to this approach is to parse the training data into n-grams.\n    X, Y = [], []\n    for line in corpus:\n        for i in range(1, len(line)-1):\n            X.append(line[:i+1])\n            Y.append(line[i+1])\n    \n    # Pad X and convert Y to categorical (Y consisted of integers)\n    X = pad_sequences(X, maxlen=maxlen)\n    Y = to_categorical(Y, num_classes=max_features)\n\n    return X, Y, tokenizer","8f1d0a99":"max_features, max_len = 3500, 20\nX, Y, tokenizer = format_data(data, max_features, max_len)","f00ec572":"tokenizer.word_index['trump'], tokenizer.word_index[START], tokenizer.word_index[END]","5c317bd7":"from keras.layers import Input, Dense, Bidirectional, GRU, Embedding, Dropout, LSTM\nfrom keras.layers import concatenate, SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.models import Model, Sequential\n\nepochs = 3\n\nmodel = Sequential()\n\n# Embedding and GRU\nmodel.add(Embedding(max_features, 300))\nmodel.add(SpatialDropout1D(0.33))\nmodel.add(Bidirectional(LSTM(30)))\n\n# Output layer\nmodel.add(Dense(max_features, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X, Y, epochs=epochs, batch_size=128, verbose=1)\n\nmodel.save_weights('model{}.h5'.format(epochs))","65a6ab99":"model.evaluate(X, Y)","93cdfee1":"def sample(preds, temp=1.0):\n    \"\"\"\n    Sample next word given softmax probabilities, using temperature.\n    \n    Taken and modified from:\n    https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/lstm_text_generation.py\n    \"\"\"\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) \/ temp\n    preds = np.exp(preds) \/ np.sum(np.exp(preds))\n    probs = np.random.multinomial(1, preds, 1)\n    return np.argmax(probs)","4a2787a7":"\"\"\"When sampling from the distribution, we do not know which word is being\nsampled, only its index. We need a way to go from index to word. Unfortunately,\nthe tokenizer class only contains a dictionary of {word: index} items. We will\nreverse that dictionary to get {index: word} items. That way, going from\nindices to words is much faster.\"\"\"\nidx_to_words = {value: key for key, value in tokenizer.word_index.items()}\n\n\ndef process_input(text):\n    \"\"\"Tokenize and pad input text\"\"\"\n    tokenized_input = tokenizer.texts_to_sequences([text])[0]\n    return pad_sequences([tokenized_input], maxlen=max_len-1)\n\n\ndef generate_text(input_text, model, n=7, temp=1.0):\n    \"\"\"Takes some input text and feeds it to the model (after processing it).\n    Then, samples a next word and feeds it back into the model until the end\n    token is produced.\n    \n    :input_text: A string or list of strings to be used as a generation seed.\n    :model:      The model to be used during generation.\n    :temp:       A float that adjusts how 'volatile' predictions are. A higher\n                 value increases the chance of low-probability predictions to\n                 be picked.\"\"\"\n    if type(input_text) is str:\n        sent = input_text\n    else:\n        sent = ' '.join(input_text)\n    \n    tokenized_input = process_input(input_text)\n    \n    while True:\n        preds = model.predict(tokenized_input, verbose=0)[0]\n        pred_idx = sample(preds, temp=temp)\n        pred_word = idx_to_words[pred_idx]\n        \n        if pred_word == END:\n            return sent\n        \n        sent += ' ' + pred_word\n#         print(sent)\n#         tokenized_input = process_input(sent[-n:])\n        tokenized_input = process_input(sent)","56668777":"text = generate_text(START, model, temp=0.01)\ntext[2:] # the first two elements are '\u00f7 '","eaa6e1d4":"text = generate_text(START, model, temp=0.25)\ntext[2:] # the first two elements are '\u00f7 '","138d435e":"text = generate_text(START, model, temp=0.5)\ntext[2:] # the first two elements are '\u00f7 '","41f9094d":"text = generate_text(START, model, temp=0.75)\ntext[2:] # the first two elements are '\u00f7 '","1b37f304":"text = generate_text(START, model, temp=1.0)\ntext[2:] # the first two elements are '\u00f7 '","72db1178":"## Data Processing\n\nFirst we will read data from [A Million News Headlines](https:\/\/www.kaggle.com\/therohk\/million-headlines).","ce78ee28":"## Model Building\n\nNow we are ready to build our model. As usual, we will first pass the input through an embedding layer. On these embeddings, a bidirectional GRU layer will operate and will produce an output for the Dense classifier. The output of this final Dense layer is the probability distribution of all words.","67525d6e":"We now need to convert the textual data into a format appropriate for training (ie. a vector).\n\nFirst though, we need to take a small step back and think about the generation process as a whole. We want to be able to generate text over and over again until... when? Maybe when we generate 10 words, we call it quits. Would that be desirable though? No, since most sentences\/titles are not just ten words; some have more words, some fewer. Also, stopping generation after a fixed number of words may result in text that is abruptly cut. To avoid these problems, we are going to ask the model when to stop generation. How? We will add a stop token and the model will learn to predict it like any other word. So, we are going to add a special end character at the end of each training item. We will also add another special character at the start of each item, to denote the beginning of each sentence.\n\nThese special characters are simply '\u00f7' and '\u25a0' (start\/end respectively).","cbbd77c6":"## Text Generation\n\nWith the model at hand, we are ready to start generating text. We are going to feed the starting character, '\u00f7', to the model and then continuously sample and input generated text until we reach the end character, '\u25a0'.\n\nIn our sampling function, we are going to adjust the softmax probabilities by temperature.","a29686d6":"After appending the start and end tokens, we are going to tokenize our text using the `keras` built-in libraries. Next, after the tokenization, we need to format our data for training. When generating text, we are given a sequence of words and want to predict the next one in line. This is what we are going to do here: we will create `(context, next_word)` pairs. In our case `context` is a variable-length sequence of words, but it can be n-grams. Given a list of words, we want to predict `next_word`.","ff7fa0b1":"In the generation function, we are given an input text. We will tokenize this input and pad it so that it is in the correct form. Then, we will feed the tokenized input into the model to compute the probability distribution of next words. We will sample from this distribution and add the selected word to the generated sentence. Then, we will feed the generated sentence in its whole to the network and generate the next word (alternatively, we can only feed part of the generated sentence). We repeat this proess until we generate the end token ('\u25a0').","de5e272e":"Let's tokenize our data!","7b6ee462":"The tokenizer is basically a large dictionary of `{word: index}` items. We are going to print indices for a random word and the two special tokens (to make sure they are included in the tokenizer).","e6ad37f7":"## Headline Generation\n\nThis is a starter kernel which uses a dataset of a million headlines to model language for (real) news headlines. Then, using this language model we will generate headlines.\n\n* Language Model: Learn to predict the next word given some context.\n* Text Generation: Continuously sample from the model until the end of headline character is reached.","8a456098":"We can now start generating text, using the starting symbol as input."}}