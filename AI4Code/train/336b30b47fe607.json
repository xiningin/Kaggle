{"cell_type":{"f1e0ee9e":"code","6353487a":"code","0264a45b":"code","dc2db6b5":"code","e372dc84":"code","fe5596d2":"code","68ee2e6a":"code","2cfa8c13":"code","2f10fc16":"code","a89a09b4":"code","9cb58090":"code","d935a8ce":"markdown","6e4ef9d9":"markdown","fc6b0a61":"markdown","1d8a48fc":"markdown"},"source":{"f1e0ee9e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport random\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, scale\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6353487a":"music_df = pd.read_csv('..\/input\/data.csv')\nmusic_df.head(3)","0264a45b":"genres = music_df.groupby('label')\n","dc2db6b5":"features = list(music_df.columns)\nfeatures.remove('filename')\nfeatures.remove('label')\nprint(features)\n\nfor feat in features:\n    fig, ax = plt.subplots(figsize=(20,10))\n    ax.tick_params(axis = 'both', which = 'major', labelsize = 16)\n    sns.violinplot(data=music_df, x='label', y=feat, figsize=(20,10))","e372dc84":"music_features_df = music_df[features]\nmusic_features_norm_df = scale(music_features_df)\n","fe5596d2":"def lower_diag_matrix_plot(matrix, title=None):\n    \"\"\" Args:\n        matrix - the full size symmetric matrix of any type that is lower diagonalized\n        title - title of the plot\n    \"\"\"\n    plt.style.use('default')\n    \n    # Create lower triangular matrix to mask the input matrix\n    triu = np.tri(len(matrix), k=0, dtype=bool) == False\n    matrix = matrix.mask(triu)\n    fig, ax = plt.subplots(figsize=(20,20))\n    if title:\n        fig.suptitle(title, fontsize=32, verticalalignment='bottom')\n        fig.tight_layout()\n    plot = ax.matshow(matrix)\n    \n    # Add grid lines to separate the points\n    # Adjust the ticks to create visually appealing grid\/labels\n    # Puts minor ticks every half step and bases the grid off this\n    ax.set_xticks(np.arange(-0.5, len(matrix.columns), 1), minor=True)\n    ax.set_yticks(np.arange(-0.5, len(matrix.columns), 1), minor=True)\n    ax.grid(which='minor', color='w', linestyle='-', linewidth=3)\n    # Puts major ticks every full step and bases the labels off this\n    ax.set_xticks(np.arange(0, len(matrix.columns), 1))\n    ax.set_yticks(np.arange(0, len(matrix.columns), 1))\n    plt.yticks(range(len(matrix.columns)), matrix.columns)\n    # Must put this here for x axis grid to show\n    plt.xticks(range(len(matrix.columns)))\n    ax.tick_params(axis='both', which='major', labelsize=24)\n    # Whitens (transparent) x labels\n    ax.tick_params(axis='x', colors=(0,0,0,0))\n    \n    # Add a colorbar for reference\n    cax = make_axes_locatable(ax)\n    cax = cax.append_axes(\"right\", size=\"5%\", pad=0.05)\n    cax.tick_params(axis='both', which='major', labelsize=24)\n    fig.colorbar(plot, cax=cax, cmap='hot')\n    \n    # Get rid of borders of plot\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n","68ee2e6a":"cov_matrix = music_features_df.cov()\nlower_diag_matrix_plot(cov_matrix, 'Covariance Matrix')\n\ncorr_matrix = music_features_df.corr()\nlower_diag_matrix_plot(corr_matrix, 'Correlation Matrix')","2cfa8c13":"music_df_no_categories = music_features_df.copy()\nmusic_df_no_categories['label'] = music_df['label']\n# sns.pairplot(music_df, hue='label')","2f10fc16":"enc = OneHotEncoder()\nsolvers = ['svd', 'eigen']\nfor solver in solvers:\n    clf = LinearDiscriminantAnalysis(solver=solver, n_components=2)\n    le = LabelEncoder()\n\n    new_labels = pd.DataFrame(le.fit_transform(music_df['label']))\n    music_df['label'] = new_labels\n\n    params = clf.fit_transform(music_features_norm_df, new_labels,)\n    fig, ax = plt.subplots()\n    labels_list = list(set(list(new_labels)))\n    ax.scatter(params[:,0], params[:,1], c=new_labels.as_matrix().reshape(params[:,0].shape))\n    ax.legend()\n","a89a09b4":"def param_search(param_names, params_list, model, data, plot=True, seed=None, verbose=False):\n    params_accuracies = []\n    train_data, train_labels, test_data, test_labels = data\n    for param_name, param_list in zip(param_names, params_list):\n        accuracies = []\n        kwargs = {}\n        if verbose:\n            print(\"--------------------------------\")\n            print(\"Parameter Under Test: {}\\n\".format(param_name))\n        if type(param_list) != list:\n            kwargs[param_name] = param_list\n            continue\n        for param_val in param_list:\n            if verbose: print(\"Parameter search ({0} -> {1})\".format(param_name, param_val))\n            kwargs[param_name] = param_val\n            classifier = model(**kwargs)\n            classifier.fit(train_data, train_labels)\n            accuracy = classifier.score(test_data, test_labels)\n            accuracies.append(accuracy)\n        params_accuracies.append(accuracies)\n        if plot:\n            fig, ax = plt.subplots()\n            ax.plot(param_list, accuracies)\n        if verbose: print(\"--------------------------------\")\n    if type(param_list) != list:\n        kwargs[param_name] = param_list\n        classifier = model(**kwargs)\n        classifier.fit(train_data, train_labels)\n        accuracy = classifier.score(test_data, test_labels)\n        params_accuracies.append([accuracy])\n    return params_accuracies","9cb58090":"param_names = ['n_neighbors']\nparams_list = [[i+3 for i in range(20)]]\ndummy_param_names = ['strategy']\ndummy_params_list = [['stratified', 'most_frequent', 'uniform', 'prior']]\ntree_param_names = ['min_samples_split', 'max_depth']\ntree_params_list = [10, 8]\ntree_params_lists_only = [x for x in tree_params_list if type(x) == list]\ndummy_params_lists_only = [x for x in dummy_params_list if type(x) == list]\nparams_lists_only = [x for x in params_list if type(x) == list]\nfolds = 10\nrandom_state = random.randint(1, 65536)\nprint(\"Random State: {}\".format(random_state))\n\ncv = StratifiedKFold(n_splits=folds,\n                     shuffle=True,\n                     random_state=random_state,\n                     )\n\ndata = list(cv.split(music_features_df, music_df['label']))\nfig, ax = plt.subplots()\ndummy_fig, dummy_ax = plt.subplots()\ntree_fig, tree_ax = plt.subplots()\n\nfor i, indices in enumerate(data):\n    train_index, test_index = indices\n    title = \"Training on fold {}\/{}...\\n\".format(i+1, len(data))\n    print(title)\n    train_data = music_features_df.iloc[train_index]\n    train_labels = music_df['label'].iloc[train_index]\n    test_data = music_features_df.iloc[test_index]\n    test_labels = music_df['label'].iloc[test_index]\n    full_data = (train_data, train_labels, test_data, test_labels)\n    \n    dummy_accuracies = param_search(dummy_param_names, \n                                    dummy_params_list, \n                                    DummyClassifier, \n                                    data=full_data,\n                                    plot=False)\n    for X, y in zip(dummy_params_list, dummy_accuracies):\n        dummy_ax.scatter(X, y, label='K Fold {}'.format(i+1))\n        dummy_ax.legend()\n    \n    accuracies = param_search(param_names, \n                              params_list, \n                              KNeighborsClassifier, \n                              data=full_data,\n                              plot=False)\n    for X, y in zip(params_list, accuracies):\n        ax.plot(X, y, label='K Fold {}'.format(i+1))\n        ax.legend()\n    \n    tree_accuracies = param_search(tree_param_names, \n                                   tree_params_list, \n                                   DecisionTreeClassifier,\n                                   data=full_data,\n                                   plot=False)\n    print(tree_params_lists_only, tree_accuracies)\n    if not tree_params_lists_only:\n        tree_params_lists_only = [['K Fold Results']]\n    for X, y in zip(tree_params_lists_only, tree_accuracies):\n        tree_ax.scatter(X, y, label='K Fold {}'.format(i+1))\n        tree_ax.legend()\n\n    ","d935a8ce":"##### As one can see from above, the Correlation Matrix is much easier to garner information from. This is because the data in it's original form has large values for 'rolloff', 'spectral_centroid' and 'spectral_bandwidth'. Once standardized, correlations can be deduced as in the second plot.","6e4ef9d9":"###### Let's try to do just this, and run a kNN classifier on the data","fc6b0a61":"# Covariance vs. Correlation\n## Covariance\n$$\n\n$$","1d8a48fc":"##### As can be seen above, LDA to two variables doesn't give us a ton of separation in the data. Maybe we should keep most of our features?"}}