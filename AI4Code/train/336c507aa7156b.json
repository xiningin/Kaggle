{"cell_type":{"324e776f":"code","ec0049b8":"code","a9dcc590":"code","488bfd2b":"code","5b95a195":"code","2dcf69ee":"code","5783717f":"code","3a5d3677":"code","fc5e4868":"code","ab549f88":"code","f8a588e9":"code","8b994940":"code","210c569c":"code","f9a01e7d":"code","3691a20f":"code","ccb1ec23":"code","6d7e0cdd":"code","a46022d8":"code","396a6d57":"code","6a521bd5":"code","437b9024":"code","b646038b":"code","90a7c132":"code","12cd65bf":"code","8fa3911d":"code","d0935e23":"code","6be79b92":"code","776f6be1":"code","da33e61b":"code","e984fe1f":"code","81aa8f63":"code","910858d9":"code","db5a800c":"code","ffed4989":"code","3fa65cee":"code","04106d2a":"code","2e41ab1d":"code","b960e236":"code","cd9c247f":"code","64bbbf0b":"code","1e7050a8":"code","0e586761":"code","32a32b0e":"code","81f05406":"code","3ff2568d":"code","26949783":"code","137ce3a6":"code","ca2a698d":"code","716c01c4":"code","30786eac":"code","a6f4e23d":"code","78f96249":"code","35e3b656":"code","525f6c71":"code","2adb90c3":"code","0daa9416":"code","23b4d6e1":"code","8d3504cf":"markdown","e70fe93c":"markdown","6c1d9cf7":"markdown","7539456e":"markdown","8faca883":"markdown","38b5f5fe":"markdown","fda66f80":"markdown","7aa57a80":"markdown","24300952":"markdown","d42dd04a":"markdown","07922817":"markdown","1c711e7f":"markdown","2b70c3d2":"markdown","573cc81f":"markdown","4aaedffc":"markdown"},"source":{"324e776f":"#import some necessary librairies\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\nfrom IPython.display import display\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\/house-prices-advanced-regression-techniques\"]).decode(\"utf8\")) #check the files available in the directory","ec0049b8":"#Now let's import and put the train and test datasets in  pandas dataframe\ndef get_data():\n    train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n    test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n    display(train.head(5))\n    display(test.head(5))\n    return train, test","a9dcc590":"def split_id_column(train, test):\n    #check the numbers of samples and features\n    print(\"The train data size before dropping Id feature is : {} \".format(train.shape))\n    print(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n    #Save the 'Id' column\n    train_ID = train['Id']\n    test_ID = test['Id']\n\n    #Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n    train.drop(\"Id\", axis = 1, inplace = True)\n    test.drop(\"Id\", axis = 1, inplace = True)\n\n    #check again the data size after dropping the 'Id' variable\n    print(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \n    print(\"The test data size after dropping Id feature is : {} \".format(test.shape))\n    \n    return train_ID, test_ID, train, test","488bfd2b":"#descriptive statistics summary\ndef describe_SalePrice():\n    display(train['SalePrice'].describe())","5b95a195":"#histogram\ndef plot_histogram_SalePrice():\n    sns.distplot(train['SalePrice']);","2dcf69ee":"def plot_scatter_relation_between_feature_and_target_variable(feature, targetVariable, preferences=None):\n    data = pd.concat([train[targetVariable], train[feature]], axis=1)\n    data.plot.scatter(x=feature, y=targetVariable, ylim=(0,800000));","5783717f":"def plot_box_relation_between_feature_and_target_variable(feature, targetVariable, preferences=None):\n    data = pd.concat([train[targetVariable], train[feature]], axis=1)\n    \n    if preferences.get('subplots', {}).get('figsize'):\n        height = preferences.get('subplots', {}).get('figsize')[0]\n        width = preferences.get('subplots', {}).get('figsize')[1]\n        f, ax = plt.subplots(figsize=(height, width))\n    \n    fig = sns.boxplot(x=feature, y=targetVariable, data=data)\n    fig.axis(ymin=0, ymax=800000);\n    \n    if preferences.get('xticks', {}).get('rotation'):\n        plt.xticks(rotation=90);","3a5d3677":"#correlation matrix\ndef plot_correlation_matrix():\n    corrmat = train.corr()\n    f, ax = plt.subplots(figsize=(12, 9))\n    sns.heatmap(corrmat, vmax=.8, square=True);","fc5e4868":"#saleprice correlation matrix\ndef plot_SalePrice_zoom_correlation_matrix(k = 10):\n    corrmat = train.corr()\n    cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n    cm = np.corrcoef(train[cols].values.T)\n    sns.set(font_scale=1.25)\n    hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n    plt.show()","ab549f88":"#scatterplot\ndef plot_scatter_relations(cols):\n    sns.set()\n#cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n    sns.pairplot(train[cols], size = 2.5)\n    plt.show();","f8a588e9":"def delete_outliers(train):\n    train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n    return train","8b994940":"def plot_feature_distribution(train, feature='SalePrice'):\n    sns.distplot(train[feature] , fit=norm);\n\n    # Get the fitted parameters used by the function\n    (mu, sigma) = norm.fit(train[feature])\n    print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n    #Now plot the distribution\n    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n                loc='best')\n    plt.ylabel('Frequency')\n    plt.title(feature + ' distribution')\n\n    #Get also the QQ-plot\n    fig = plt.figure()\n    res = stats.probplot(train[feature], plot=plt)\n    plt.show()","210c569c":"def apply_log_transformation_on_feature(train, feature='SalePrice', visualize = True):\n\n    #We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n    train[feature] = np.log1p(train[feature])\n\n    if visualize == True:\n        plot_feature_distribution(train, feature)\n    \n    return train","f9a01e7d":"def transform_totalbasement(all_data):\n    #create column for new variable (one is enough because it's a binary categorical feature)\n    #if area>0 it gets 1, for area==0 it gets 0\n    all_data['HasBsmt'] = pd.Series(len(all_data['TotalBsmtSF']), index=all_data.index)\n    all_data['HasBsmt'] = 0 \n    all_data.loc[all_data['TotalBsmtSF']>0,'HasBsmt'] = 1\n    \n    all_data.loc[all_data['HasBsmt']==1,'TotalBsmtSF'] = np.log(all_data['TotalBsmtSF'])\n    \n    all_data.drop(['HasBsmt'], axis=1, inplace=True)\n\n    #histogram and normal probability plot\n    sns.distplot(all_data[all_data['TotalBsmtSF']>0]['TotalBsmtSF'], fit=norm);\n    fig = plt.figure()\n    res = stats.probplot(all_data[all_data['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)\n    \n    return all_data","3691a20f":"def concatenate_data(train, test):\n    ntrain = train.shape[0]\n    ntest = test.shape[0]\n    y_train = train.SalePrice.values\n    all_data = pd.concat((train, test)).reset_index(drop=True)\n    all_data.drop(['SalePrice'], axis=1, inplace=True)\n    print(\"all_data size is : {}\".format(all_data.shape))\n    \n    return ntrain, ntest, y_train, all_data","ccb1ec23":"def print_missing_data(all_data):\n    #missing data\n    all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\n    all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\n    missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n    display(missing_data.head(25))","6d7e0cdd":"def transform_missing_data(all_data, transform_type='mean'):\n    if transform_type == 'none':\n        all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\n        all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\n        all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\n        all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n        all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n    \n        #Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\n        all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n            lambda x: x.fillna(x.median()))\n    \n        for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n            all_data[col] = all_data[col].fillna('None')\n        \n        for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n            all_data[col] = all_data[col].fillna(0)\n    \n        for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n            all_data[col] = all_data[col].fillna(0)\n        \n        for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n            all_data[col] = all_data[col].fillna('None')\n        \n        all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\n        all_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n    \n        all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n\n        all_data = all_data.drop(['Utilities'], axis=1)\n    \n        all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n    \n        all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n    \n        all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n    \n        all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\n        all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n    \n        all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n    \n        all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n    \n    if transform_type=='mean':\n        print('transform_type = mean')\n        all_data = all_data.fillna(all_data.mean())\n    \n    return all_data","a46022d8":"def transform_numerical_variables_to_categorical(all_data):\n    #MSSubClass=The building class\n    all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n    #Changing OverallCond into a categorical variable\n    all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n    #Year and month sold are transformed into categorical features.\n    all_data['YrSold'] = all_data['YrSold'].astype(str)\n    all_data['MoSold'] = all_data['MoSold'].astype(str)\n\n    return all_data","396a6d57":"def encode_label_to_categorical_features(all_data):\n\n    from sklearn.preprocessing import LabelEncoder\n    cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n            'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n            'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n            'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n            'YrSold', 'MoSold')\n    # process columns, apply LabelEncoder to categorical features\n    for c in cols:\n        lbl = LabelEncoder() \n        lbl.fit(list(all_data[c].values)) \n        all_data[c] = lbl.transform(list(all_data[c].values))\n\n    # shape        \n    print('Shape all_data: {}'.format(all_data.shape))\n    \n    return all_data","6a521bd5":"def create_square_feet_feature(all_data):\n    # Adding total sqfootage feature \n    all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n    return all_data","437b9024":"def get_skewness_in_numerical_features(all_data):\n    numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n    # Check the skew of all numerical features\n    skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n    print(\"\\nSkew in numerical features: \\n\")\n    skewness = pd.DataFrame({'Skew' :skewed_feats})\n    display(skewness.head(10))\n    \n    return skewness","b646038b":"def apply_box_cox_transformation_for_skew_features(all_data, skewness):\n    print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\n    from scipy.special import boxcox1p\n    skewed_features = skewness.index\n    lam = 0.15\n    for feat in skewed_features:\n        #all_data[feat] += 1\n        all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n    return all_data \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","90a7c132":"def end_processing(all_data, ntrain):\n    # Convert categorical variable into dummy\/indicator variables.\n    all_data = pd.get_dummies(all_data)\n    print(all_data.shape)\n    \n    all_data = transform_missing_data(all_data, transform_type='mean')\n    #all_data = all_data.fillna(all_data.mean())\n    print_missing_data(all_data)\n    \n    # Split data\n    train = all_data[:ntrain]\n    test = all_data[ntrain:]\n    \n    return train, test","12cd65bf":"from sklearn.metrics import mean_squared_error, accuracy_score\nfrom sklearn import linear_model, svm\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import KFold, cross_val_score\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras.optimizers import Adam","8fa3911d":"#Validation functions\n\ndef rmse_cv(model, train, y_train, n_folds = 5):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\ndef mae_cv(model, train, y_train, n_folds = 5):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    mae= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_absolute_error\", cv = kf))\n    return(mae)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","d0935e23":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","6be79b92":"def create_models(model_list):\n    models = {}\n    if 'linear_regression' in model_list:\n        LinRegr = linear_model.LinearRegression()\n        models['linear_regression'] = LinRegr\n        \n    if 'random_forest' in model_list:\n        RFRegr = RandomForestRegressor(max_depth=10, random_state=0)\n        models['random_forest'] = RFRegr\n        \n    if 'svmr' in model_list:\n        SVMR = svm.SVR()\n        models['svmr'] = SVMR\n    \n    if 'lasso' in model_list:\n        lasso = make_pipeline(RobustScaler(), linear_model.LassoCV(alphas =[1, 0.1, 0.001, 0.0005], random_state=1))\n        models['lasso'] = lasso\n\n    if 'enet' in model_list:\n        enet = make_pipeline(RobustScaler(), linear_model.ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n        models['enet'] = enet\n\n    if 'krr' in model_list:\n        krr = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n        models['krr'] = krr\n    \n    if 'gboost' in model_list:\n        gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n        models['gboost'] = gboost\n    \n    if 'xgboost' in model_list:\n        xgboost = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1)\n        models['xgboost'] = xgboost\n        \n    if 'lightgbm' in model_list:\n        lightgbm = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n        models['lightgbm'] = lightgbm\n        \n    if 'neural_network' in model_list:\n        input_layer = Input(shape=(220,))\n        first_hidden_layer = Dense(128)(input_layer)\n        second_hidden_layer = Dense(32)(first_hidden_layer)\n        output_layer = Dense(1, activation='linear')(second_hidden_layer)\n\n        model = Model(inputs=input_layer, outputs=output_layer)\n        adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n        model.compile(loss='mean_squared_error', metrics=['mse', 'mae'], optimizer=adam)\n        model.fit(x=train, y=y_train, epochs=20, validation_split=0.2)\n        \n        models['neural_network'] = model\n    \n    return models","776f6be1":"def tune_hyperparameters_for_lasso(alphas):\n    models = {}\n    \n    if not alphas:\n        alphas = [0.1]\n    \n    for a in alphas:\n        lasso = linear_model.Lasso(alpha=a)\n        models[str(a)] = lasso\n    \n    return models","da33e61b":"def tune_hyperparameters_for_random_forest(max_depths):\n    models = {}\n    \n    if not max_depths:\n        max_depths = [2]\n    \n    for m in max_depths:\n        RFRegr = RandomForestRegressor(max_depth=m, random_state=0)\n        models[str(m)] = RFRegr\n    \n    return models","e984fe1f":"def get_predictions_by_model(train, y_train, model, test):\n    print(model)\n    model.fit(train, y_train)\n    train_pred = model.predict(train)\n    pred = np.expm1(model.predict(test.values))\n    print(rmsle(y_train, train_pred))\n    if model == models['neural_network']:\n        pred = pred[:,0]\n    return pred","81aa8f63":"train, test = get_data()","910858d9":"train_ID, test_ID, train, test = split_id_column(train, test)","db5a800c":"describe_SalePrice()","ffed4989":"plot_histogram_SalePrice()","3fa65cee":"#scatter plot grlivarea\/saleprice\n\nplot_scatter_relation_between_feature_and_target_variable('GrLivArea', 'SalePrice')","04106d2a":"#scatter plot totalbsmtsf\/saleprice\n\nplot_scatter_relation_between_feature_and_target_variable('TotalBsmtSF', 'SalePrice')","2e41ab1d":"preferences = {'xticks':{'rotation':90}, 'subplots':{'figsize':[8, 6]}}\nplot_box_relation_between_feature_and_target_variable('OverallQual', 'SalePrice', preferences)","b960e236":"preferences = {'xticks':{'rotation':90}, 'subplots':{'figsize':[16, 8]}}\nplot_box_relation_between_feature_and_target_variable('YearBuilt', 'SalePrice', preferences)","cd9c247f":"plot_correlation_matrix()","64bbbf0b":"plot_SalePrice_zoom_correlation_matrix(k = 10)","1e7050a8":"cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nplot_scatter_relations(cols)","0e586761":"plot_scatter_relation_between_feature_and_target_variable('GrLivArea', 'SalePrice', preferences)","32a32b0e":"train = delete_outliers(train)\nplot_scatter_relation_between_feature_and_target_variable('GrLivArea', 'SalePrice', preferences)","81f05406":"train = apply_log_transformation_on_feature(train, feature='SalePrice', visualize = True)","3ff2568d":"ntrain, ntest, y_train, all_data = concatenate_data(train, test)","26949783":"#all_data = apply_log_transformation_on_feature(all_data, feature='GrLivArea', visualize = True)\nall_data = transform_totalbasement(all_data)","137ce3a6":"skewness = get_skewness_in_numerical_features(all_data)\nall_data = apply_box_cox_transformation_for_skew_features(all_data, skewness)\nskewness = get_skewness_in_numerical_features(all_data)","ca2a698d":"print_missing_data(all_data)","716c01c4":"#all_data = transform_missing_data(all_data, transform_type='mean')\n#all_data = all_data.fillna(all_data.mean())\n#print_missing_data(all_data)","30786eac":"all_data = transform_numerical_variables_to_categorical(all_data)","a6f4e23d":"all_data = encode_label_to_categorical_features(all_data)","78f96249":"all_data = create_square_feet_feature(all_data)","35e3b656":"train, test = end_processing(all_data, ntrain)","525f6c71":"#models = create_models(['linear_regression', 'random_forest', 'svmr', 'lasso'])\nmodels = create_models(['lasso', 'neural_network'])\n\nif ('enet' in models) and ('gboost' in models) and ('krr' in models) and ('lasso' in models):\n    stacked_averaged_models = StackingAveragedModels(base_models = (models['enet'], models['gboost'], models['krr']),\n                                                 meta_model = models['lasso'])\n    #models['stacked'] = stacked_averaged_models\n\n#models = tune_hyperparameters_for_lasso([0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009])\n\n#models = tune_hyperparameters_for_random_forest([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n\n","2adb90c3":"#for key, value in models.items():\n#    rmse = rmse_cv(value, train, y_train, 5)\n#    mae = mae_cv(value, train, y_train, 5)\n#    \n#    print(\"\\n\" + key +\" rmse score: {:.4f} ({:.4f})\\n\".format(rmse.mean(), rmse.std()))\n#    print(\"\\n\" + key +\" mae score: {:.4f} ({:.4f})\\n\".format(mae.mean(), mae.std()))","0daa9416":"\n#my_model = models[model_to_use]\n#my_model.fit(train, y_train)\n#predicted_prices = my_model.predict(test)\n#print(predicted_prices)\n\n#lasso_preds = get_predictions_by_model(train, y_train, models['lasso'], test)\n#xgb_preds = get_predictions_by_model(train, y_train, models['xgboost'])\nneural_network_preds = get_predictions_by_model(train, y_train, models['neural_network'], test)\n\n#predicted_prices = 0.7*lasso_preds + 0.3*xgb_preds\n\nmy_submission = pd.DataFrame({'Id': test_ID, 'SalePrice': neural_network_preds})\n# you could use any filename. We choose submission here\nfile_name = 'submission_' + 'neural_network' + '.csv'\nmy_submission.to_csv(file_name, index=False)","23b4d6e1":"#ensemble = get_predictions_by_model(train, y_train, stacked_averaged_models)*0.70 \n#+ get_predictions_by_model(train, y_train, models['xgboost'])*0.15 \n#+ get_predictions_by_model(train, y_train, models['lightgbm'])*0.15\n#        \n#my_submission = pd.DataFrame({'Id': test_ID, 'SalePrice': ensemble})\n#my_submission.to_csv('stack_submission.csv',index=False)","8d3504cf":"### GrLivArea","e70fe93c":"## Relation entre SalePrice et OverallQual","6c1d9cf7":"## Relation entre SalePrice et YearBuilt","7539456e":"# Mod\u00e8le","8faca883":"# Pipeline","38b5f5fe":"# Data Processing","fda66f80":"# Data visualization","7aa57a80":"### Target Variable","24300952":"## Sales price","d42dd04a":"### Transform variables","07922817":"## Relation entre SalePrice et TotalBsmtSF","1c711e7f":"## Correlation matrix","2b70c3d2":"## Outliers","573cc81f":"## Relation entre SalePrice et GrLiveArea","4aaedffc":"## Features engineering"}}