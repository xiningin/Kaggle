{"cell_type":{"ba74333c":"code","dd2733fa":"code","208495d3":"code","3ef768dc":"code","81df6681":"code","7985834f":"code","b64ed5a8":"code","ebdda418":"code","5fb71d4f":"code","388f5357":"code","58ecde7c":"code","1ab1b0d5":"markdown","8f589d51":"markdown","daa6aa34":"markdown","51abb26a":"markdown","63f48a3e":"markdown","b633b649":"markdown","ac05089b":"markdown","5d23fdcf":"markdown","6022ef2c":"markdown","0c62d530":"markdown","048c3b5f":"markdown","0819d8fd":"markdown","b37d48db":"markdown","98d8a975":"markdown","4fa5d68c":"markdown"},"source":{"ba74333c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno as msno\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dd2733fa":"train_data = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/train.csv')\n\ntest_data = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/eval.csv')\n\ntrain_data.head()","208495d3":"msno.matrix(train_data)","3ef768dc":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\ntrain_data['esrb_rating'] = le.fit_transform(train_data['esrb_rating'])\n\n#test_data['Start Date'] = le.fit_transform(test_data['Start Date'])\n\ntrain_data","81df6681":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nX = train_data.iloc[:,range(2,34)]\n\nx_test = test_data.iloc[:,range(1,33)]\n\n\n\nY = train_data.iloc[:,34]\n\n\nparam_grid = {'C': [0.1, 1, 10, 100],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'gamma':['scale', 'auto'],\n              'kernel': ['linear']} \nparam_grid2 = {'C': [0.1, 0.5, 1, 5, 10, 50, 100]}\n\nparam_grid3 = {'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\n\nparam_grid4 = { \n    'n_estimators': [200, 700],\n    'max_features': ['auto', 'sqrt', 'log2']\n}\n\nparam_grid5 = {\n    'n_neighbors': (1,10, 1),\n    'leaf_size': (20,40,1),\n    'p': (1,2),\n    'weights': ('uniform', 'distance'),\n    'metric': ('minkowski', 'chebyshev')}\n\ngridLR = GridSearchCV(LogisticRegression(solver='liblinear'), param_grid2, cv=3, scoring=\"accuracy\")\ngridSVC = GridSearchCV(SVC(), param_grid, verbose = 0,n_jobs=-1)\ngridDT = GridSearchCV(DecisionTreeClassifier(), param_grid3, verbose = 0)\ngridForest = GridSearchCV(RandomForestClassifier(), param_grid4,  verbose = 0)\ngridK = GridSearchCV(KNeighborsClassifier(), param_grid5, verbose = 0)\n\ngridLR.fit(X, Y) \ngridSVC.fit(X,Y)\ngridDT.fit(X,Y)\ngridForest.fit(X,Y)\ngridK.fit(X,Y)\n  ","7985834f":"pred = gridLR.predict(X)\ndf = pd.DataFrame({'Actual': Y, 'Predicted': pred})\n\nprint(df.describe())\n\n\nprint(\" \")\nprint(cross_val_score(gridLR, X, Y, cv=10))\nprint(\" \")\nprint(cross_val_score(gridLR, X, Y, cv=10).mean())","b64ed5a8":"pred = gridSVC.predict(X)\ndf = pd.DataFrame({'Actual': Y, 'Predicted': pred})\n\nprint(df.describe())\nprint(\" \")\nprint(cross_val_score(gridSVC, X, Y, cv=10))\nprint(\" \")\nprint(cross_val_score(gridSVC, X, Y, cv=10).mean())","ebdda418":"pred = gridDT.predict(X)\ndf = pd.DataFrame({'Actual': Y, 'Predicted': pred})\n\nprint(df.describe())\n\nprint(df.describe())\nprint(\" \")\nprint(cross_val_score(gridDT, X, Y, cv=10))\nprint(\" \")\nprint(cross_val_score(gridDT, X, Y, cv=10).mean())","5fb71d4f":"pred = gridForest.predict(X)\ndf = pd.DataFrame({'Actual': Y, 'Predicted': pred})\n\nprint(df.describe())\n\nprint(df.describe())\nprint(\" \")\nprint(cross_val_score(gridForest, X, Y, cv=10))\nprint(\" \")\nprint(cross_val_score(gridForest, X, Y, cv=10).mean())","388f5357":"pred = gridK.predict(X)\ndf = pd.DataFrame({'Actual': Y, 'Predicted': pred})\n\nprint(df.describe())\n\n\n\nprint(df.describe())\nprint(\" \")\nprint(cross_val_score(gridK, X, Y, cv=10))\nprint(\" \")\nprint(cross_val_score(gridK, X, Y, cv=10).mean())","58ecde7c":"predictions = gridSVC.predict(x_test)\n\nDictionary = {1 : 'ET', 0 : 'E', 3 : 'T', 2: 'M' }\n\noutput = pd.DataFrame({'id': test_data.id, 'esrb_rating': predictions})\n\noutput = output.replace({'esrb_rating': Dictionary})\n\n\n\n\nprint(output)\noutput.to_csv('submission.csv', index=False)","1ab1b0d5":"**Decision Tree**","8f589d51":"# Final submission","daa6aa34":"# Building Models","51abb26a":"SVC model scored the best, so it will be used","63f48a3e":"**SVC Model**","b633b649":"**K neighbors**","ac05089b":"**Logistic Regression**","5d23fdcf":"# Outliers","6022ef2c":"# Checking for Missing Data","0c62d530":"**Using labelEncoding to turn the string in the last column into integers, as the models will not work well with strings.**","048c3b5f":"# Evaluating Models","0819d8fd":"No need for outlier handling in this form of categorical data.","b37d48db":"# Loading Data","98d8a975":"All data seems to be present","4fa5d68c":"**Random Forest Model**"}}