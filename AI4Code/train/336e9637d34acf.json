{"cell_type":{"43c1f03b":"code","cfd4f4db":"code","2e9c8c4c":"code","d6b788f2":"code","0bdc4690":"code","b641ade0":"code","47de2048":"code","4ae581f9":"code","c86c9b7f":"code","d2cef615":"code","de52a5b2":"code","d1a834de":"code","3c560764":"code","2f8c94ff":"code","ad70fd68":"code","81362497":"code","2f7f90df":"code","bb6a0e1a":"code","e7af67f5":"code","e49d7d8e":"code","a43fcafc":"code","52b045a8":"code","7041fad4":"code","84160677":"code","d854301d":"markdown","c60a84f4":"markdown","85e59799":"markdown","bb55fb9e":"markdown","7faaf728":"markdown","b39b85f4":"markdown"},"source":{"43c1f03b":"# Import libraries and set seed for reproducability\nfrom fastai.tabular.all import *\nimport pandas as pd\nimport numpy as np\nimport itertools\n\nset_seed(42)","cfd4f4db":"path = Path('..\/input\/tabular-playground-series-jan-2022')\npath.ls()","2e9c8c4c":"train_df = pd.read_csv(path\/'train.csv', index_col='row_id')\ntest_df = pd.read_csv(path\/'test.csv', index_col='row_id')\ntrain_df.head()","d6b788f2":"# No need to do any imputation or taking care of missing values otherwise\ntrain_df.isnull().sum(), test_df.isnull().sum()","0bdc4690":"dropped = ['Elapsed', 'Dayofyear', 'Day']\n\n# Add columns relevant to a 'date' column in order to process it \ntrain_df = add_datepart(train_df, 'date', drop=False)\n# Drop some columns that don't seem important\ntrain_df = train_df.drop(columns=dropped)\ntest_df = add_datepart(test_df, 'date', drop=False).drop(columns=dropped)\n                                                         \ntrain_df.head()","b641ade0":"# Adding info about GDP per capita\ngdp_per_capita = pd.read_csv('..\/input\/gdp-per-capita-finland-norway-sweden-201519\/GDP_per_capita_2015_to_2019_Finland_Norway_Sweden.csv')\ngdp_per_capita = gdp_per_capita.rename(columns={'year': 'Year'})\ngdp_per_capita.head()","47de2048":"# Convert dataframe from wide to long format\ngdp_per_capita = gdp_per_capita.melt(id_vars='Year', value_vars=['Finland', 'Norway', 'Sweden'],\n                    var_name='country', value_name='gdp')\ngdp_per_capita","4ae581f9":"# Merge the training df with gdp dataset\nuntil_2019 = gdp_per_capita['Year'] < 2019\nsince_2019 = gdp_per_capita['Year'] >= 2019\n\ntrain_df = train_df.merge(gdp_per_capita[until_2019], on=['Year', 'country'], how='left')\ntest_df = test_df.merge(gdp_per_capita[since_2019], on=['Year', 'country'], how='left')\ntrain_df","c86c9b7f":"festives_df = pd.read_csv('..\/input\/festivities-in-finland-norway-sweden-tsp-0122\/nordic_holidays.csv').drop(columns='Unnamed: 0')\nmake_date(festives_df, 'date')\nfestives_df.head()","d2cef615":"train_df = train_df.merge(festives_df, on=['date', 'country'], how='left')\ntest_df = test_df.merge(festives_df, on=['date', 'country'], how='left')\ntrain_df.head(5)","de52a5b2":"# All days that do not have a holiday will now have a NaN entry in the 'holiday' column, so we need to fix that\nprint('Missing values before: ', train_df['holiday'].isna().sum())\ntrain_df['holiday'].fillna('no_holiday', inplace=True)\nprint('Missing values after: ', train_df['holiday'].isna().sum())\ntest_df['holiday'].fillna('no_holiday', inplace=True)","d1a834de":"cpi_df = pd.read_csv('..\/input\/consumer-price-index-20152019-nordic-countries\/Best_CPI.csv', index_col='Unnamed: 0').rename(columns={'GDP': 'CPI'})\ncpi_df.head()","3c560764":"train_df = train_df.merge(cpi_df, left_on=['Year', 'country'], right_on=['year', 'country'], how='left')\ntest_df = test_df.merge(cpi_df, left_on=['Year', 'country'], right_on=['year', 'country'], how='left')\ntrain_df.head()","2f8c94ff":"# Since we don't need the 'date' column anymore, we can drop it\ntrain_df = train_df.drop(columns='date')\ntest_df = test_df.drop(columns='date')\n\n# Useful function that splits values into continuous and categorical columns - However not necessarily correct here\ncont_names, cat_names = cont_cat_split(train_df, dep_var='num_sold')\ncont_names, cat_names","ad70fd68":"# 80-20 Train-Validation split, EndSplitter to avoid look-ahead bias\nsplits = EndSplitter(valid_pct=0.2)(range_of(train_df))\n\n# Continuous and categorical variables\ncont_names = ['gdp',\n              'Year',\n              'CPI']\ncat_names = ['country',\n  'store',\n  'product',\n  'Month',\n  'Dayofweek',\n  'Is_month_end',\n  'Is_month_start',\n  'Is_quarter_end',\n  'Is_quarter_start',\n  'Is_year_end',\n  'Is_year_start',\n  'Week',\n  'holiday']\n\n# Create dataloader\nto = TabularPandas(train_df,\n                   y_names='num_sold', \n                   y_block=RegressionBlock,\n                   cat_names=cat_names,\n                   cont_names=cont_names,\n                   procs=[Categorify, Normalize],\n                   splits=splits)\n\ndls = to.dataloaders(bs=128)","81362497":"dls.show_batch()","2f7f90df":"def SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\n\nsmape = AccumMetric(SMAPE, to_np=True, invert_arg=True)","bb6a0e1a":"# Creater a learner object\nlearn = tabular_learner(dls, metrics=smape)\n\n# Find a learning rate\nlearn.lr_find()","e7af67f5":"# Run 50 epochs, and saving the model with best SMAPE validation score\nlearn.fit_one_cycle(50, cbs=[SaveModelCallback(monitor='SMAPE', comp=np.less)])","e49d7d8e":"learn.show_results()","a43fcafc":"submission_df = pd.read_csv(path\/'sample_submission.csv')\nsubmission_df.head()","52b045a8":"dl = learn.dls.test_dl(test_df)\ny, _ = learn.get_preds(dl=dl)","7041fad4":"submission_df['num_sold'] = np.ceil(y)\nsubmission_df.to_csv('submission.csv', index=False)\n\nsubmission_df.head()","84160677":"submission_df['num_sold'].mean()","d854301d":"Adding informatin about GDP per capita is benefitial according to [this](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/300148) discussion.\n","c60a84f4":"Adding information about the Consumer Price index, as it can improve CV\/LB according to [this](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/300963) discussion.","85e59799":"\"It may be worth rounding up ones submission.csv to the nearest integer (for example with something like np.ceil)\" - [discussion](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298201)","bb55fb9e":"Adding information about festivities in the nordic countries from [this](https:\/\/www.kaggle.com\/lucamassaron\/festivities-in-finland-norway-sweden-tsp-0122) dataset.","7faaf728":"Evaluation Metric: SMAPE as presented in [this](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298201) discussion.","b39b85f4":"# TPS - Jan22 - fastai approach\n\nThis notebook is basically me getting familiar with the [fastai](https:\/\/docs.fast.ai\/) framework for machine learning.\nWhile learning myself, I want to output something for the community, therefore I've tried to document all steps that are not clear by just looking at the code, hoping that others can benefit from this notebook.\nThis is also a challenge to see how far \"basic\" neural networks can get (me) in such a competiton which deals with time-series data.\nI will try to keep updating the notebook, using more advanced techniques and trying to improve the model performance.\n\nIf there are any mistakes in the notebook that you observe, please let me know in the comments. Also consider liking the notebook if you find it useful."}}