{"cell_type":{"14c4bf87":"code","69ba79c8":"code","5bc4be8e":"code","eb33d92f":"code","24095b37":"code","5544de29":"code","71a3b440":"code","26964e01":"code","e13ef647":"markdown","c7516fc2":"markdown"},"source":{"14c4bf87":"import os\nimport gc\nimport itertools\nimport multiprocessing\n\nfrom itertools import combinations\nimport datetime\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\n\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\n\nfrom pprint import pprint\nfrom tqdm import tqdm\n\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt","69ba79c8":"# better handle NAs (use timeseries to understand)\n\n# integrate time splits in modelling\n\n# create one global model and one local model","5bc4be8e":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ndef time_processing(df):\n    startdate = datetime.datetime.strptime('2017-12-01', \"%Y-%m-%d\")\n    df[\"Date\"] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n\n    df['Weekdays'] = df['Date'].dt.dayofweek\n    df['Hours'] = df['Date'].dt.hour\n    df['Days'] = df['Date'].dt.day\n    df = df.drop(['Date', 'TransactionDT'], axis=1)\n    \n    return(df)\n\ndef intersection(lst1, lst2): \n    return list(set(lst1) & set(lst2)) \n\ndef mult_feat(df, feat1, feat2):\n    feat_name = feat1+'x'+feat2\n    df[feat_name] = -999\n    booll = (df[feat1] != -999) & (df[feat2] != -999)\n    df[feat_name][booll] = df[feat1][booll]*df[feat2][booll]\n    \n    return(df)","eb33d92f":"%%time\n##############################\n########## Get Data ##########\n##############################\nprint('\\nGet Data...')\nfiles = ['..\/input\/test_identity.csv', '..\/input\/test_transaction.csv',\n         '..\/input\/train_identity.csv','..\/input\/train_transaction.csv', \n        '..\/input\/sample_submission.csv']\n\ndef load_data(file):\n    return pd.read_csv(file, index_col='TransactionID')\n\nwith multiprocessing.Pool() as pool:\n    test_identity, test_transaction, train_identity, train_transaction, sample_submission = pool.map(load_data, files)\n\n##############################\n######### Merge Data #########\n##############################\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ndel train_transaction, train_identity\ngc.collect()\n\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\ndel test_transaction, test_identity\ngc.collect()\n\n##############################\n######### Define Data ########\n##############################\nprint('\\nData Definition...')\n# Target\ny_train = train['isFraud'].copy()\nX_train = train.drop('isFraud', axis=1)\nX_test = test.copy()\n\ndel train, test\ngc.collect()\n\n# Features\ncat_features = ['ProductCD','addr1', 'addr2', 'P_emaildomain', 'R_emaildomain']+ ['card' + str(i) for i in range(1,7)] + ['M' + str(i) for i in range(1,10)] + ['DeviceType','DeviceInfo'] + ['id_' + str(i) for i in range(12,39)] \nnum_features = [i for i in X_train.columns if i not in cat_features]        \n\n##############################\n####### Missing Values ######\n##############################\nprint('\\nMissing Values...')\nX_train[cat_features] = X_train[cat_features].fillna('missing')\nX_test[cat_features] = X_test[cat_features].fillna('missing')\n\nX_train[num_features] = X_train[num_features].fillna(-999)\nX_test[num_features] = X_test[num_features].fillna(-999)\n\n##############################\n####### Time Features ########\n##############################\nprint('\\nTime Features...')\nX_train, X_test = time_processing(X_train), time_processing(X_test)\n\ncat_features += ['Weekdays', 'Hours', 'Days']\n\n##############################\n###### Features Encoding #####\n##############################\n\ncat_features_dummy = ['ProductCD', 'DeviceType', 'card4', 'card6', 'Hours']+['M' + str(i) for i in range(1,10)]\n\nif len(cat_features_dummy) != 0:\n    print('\\nOne Hot Encoding...')\n    X_train = pd.get_dummies(X_train, prefix = cat_features_dummy, columns = cat_features_dummy, sparse=True)\n    X_test = pd.get_dummies(X_test, prefix = cat_features_dummy, columns = cat_features_dummy, sparse=True)\n\nprint('\\nLabel Encoding...')\nfor f in cat_features:\n    if f not in cat_features_dummy:\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values))  \n  \ncols = intersection(X_train.columns, X_test.columns)\nX_train, X_test = X_train[cols], X_test[cols]\n\n##############################\n######## Feature Eng #########\n##############################\nprint('\\nFeature Engineering...')\n\npd.set_option('mode.chained_assignment', None)\nfor feat1, feat2 in combinations(['V201', 'V258', 'V257', 'V244', 'V189', 'V246'], 2):\n    X_train, X_test = mult_feat(X_train.copy(), feat1, feat2), mult_feat(X_test.copy(), feat1, feat2)\n\n##############################\n######## Reduce Memory #######\n##############################\nprint('\\nReducing Memory... \\n')\nX_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)","24095b37":"def augment(x,y,feat,t=2):\n    xs = []\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xs.append(x1)\n\n    xs = np.vstack(xs)\n    ys = np.ones(xs.shape[0])\n    x = np.vstack([x,xs])\n    x = pd.DataFrame(x, columns = feat)\n    y = np.concatenate([y,ys])\n    return x,y","5544de29":"class XGBGridSearch:\n\n    def __init__(self, param_grid, cv=3, verbose=0, shuffle=False, random_state=2019, augment = False):\n        self.param_grid = param_grid\n        self.cv = cv\n        self.random_state = random_state\n        self.verbose = verbose\n        self.shuffle = shuffle\n        self.augment = augment\n        \n        self.average_scores = []\n        self.scores = []\n        self.feature_importance_df = pd.DataFrame()\n    \n    def fit(self, X, y):\n        self._expand_params()\n        self._split_data(X, y)\n            \n        for params in tqdm(self.param_list, disable=not self.verbose):\n            avg_score, score = self._run_cv(X, y, params)\n            self.average_scores.append(avg_score)\n            self.scores.append(score)\n        \n        self._compute_best()\n        \n    def _run_cv(self, X, y, params):\n        scores = []\n        \n        for fold_, (train_idx, val_idx) in enumerate(self.splits):\n            clf = xgb.XGBClassifier(**params)\n\n            X_train, X_val = X.iloc[train_idx, :], X.iloc[val_idx, :]\n            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n            \n            if self.augment:\n                X_train, y_train = augment(X_train.values, y_train.values, X_train.columns)\n                \n            clf.fit(X_train, y_train)\n            \n            self.avg_feat_importance(features = [c for c in X_train.columns], clf = clf, fold_ = fold_)\n            \n            y_val_pred = clf.predict_proba(X_val)[:, 1]\n            \n            score = roc_auc_score(y_val, y_val_pred)\n            scores.append(score)\n            \n            gc.collect()\n        \n        avg_score = sum(scores) \/ len(scores)\n        return avg_score, scores\n            \n    def _split_data(self, X, y):\n        kf = KFold(n_splits=self.cv, shuffle=self.shuffle, random_state=self.random_state)\n        self.splits = list(kf.split(X, y))\n            \n    def _compute_best(self):\n        idx_best = np.argmax(self.average_scores)\n        self.best_score_ = self.average_scores[idx_best]\n        self.best_params_ = self.param_list[idx_best]\n\n    def _expand_params(self):\n        keys, values = zip(*self.param_grid.items())\n        self.param_list = [dict(zip(keys, v)) for v in itertools.product(*values)]\n        \n    def avg_feat_importance(self, features, clf, fold_):\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = features\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"fold\"] = fold_ + 1\n        self.feature_importance_df = pd.concat([self.feature_importance_df, fold_importance_df], axis=0)","71a3b440":"%%time\n\nparam_grid = {\n    'n_estimators': [500],\n    'missing': [-999],\n    'random_state': [2019],\n    'n_jobs': [1],\n    'tree_method': ['gpu_hist'],\n    'max_depth': [9],\n    'learning_rate': [0.05],\n    'subsample': [0.9],\n    'colsample_bytree': [0.9],\n    'reg_alpha': [0],\n    'reg_lambda': [1]\n}\n\ngrid = XGBGridSearch(param_grid, cv=4, verbose=1, augment = True)\ngrid.fit(X_train, y_train)\n\nprint(\"Best Score:\", grid.best_score_)\nprint(\"Best Params:\", grid.best_params_)\n\nclf = xgb.XGBClassifier(**grid.best_params_)\nclf.fit(X_train, y_train)\n\nsample_submission['isFraud'] = clf.predict_proba(X_test)[:,1]\nsample_submission.to_csv('simple_no_time_xgboost.csv')","26964e01":"# inspired from link below\n# https:\/\/www.kaggle.com\/jesucristo\/santander-magic-lgb-0-901\n\nfeature_importance_df = grid.feature_importance_df\n\ncols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:100].index)\n\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,26))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('XGBoost Features (averaged over folds)')\nplt.tight_layout()","e13ef647":"## Modelling","c7516fc2":"# Preprocessing"}}