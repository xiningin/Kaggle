{"cell_type":{"b6b2c95f":"code","a2f05533":"code","eaaa416c":"code","b0af613b":"code","b7d8e2ad":"code","1ff38cad":"code","3ce83b09":"code","9bf7e721":"code","5127502e":"code","da3de30d":"code","4f8a1a11":"code","1858c627":"code","561629d7":"code","335958df":"code","00954427":"code","e32f80ce":"code","7a5731c9":"code","037854ce":"code","e04b4392":"code","0178ebc6":"code","393ee83b":"code","d1bf195d":"code","224b7644":"code","65f04f94":"code","28ef2379":"code","70a86d3c":"code","0720d649":"code","6d3bcea6":"code","f695bd48":"code","00c73eef":"code","08eac845":"code","59571470":"code","98eef41d":"code","db701539":"code","086d3dec":"code","08e03488":"code","d9bd1661":"code","20651543":"markdown","8c093001":"markdown","3d8b549c":"markdown","87963e1a":"markdown","f7918747":"markdown","8bd376c0":"markdown","955c9878":"markdown","3b89dca1":"markdown","d3e70caf":"markdown","6c856691":"markdown","ddfa4346":"markdown","6f6b6ca1":"markdown","5eac239f":"markdown","2a990bd4":"markdown","c018f89b":"markdown","85b1877e":"markdown","13a350b4":"markdown","2bddb13a":"markdown","456bb484":"markdown","edd663e3":"markdown","8258da50":"markdown"},"source":{"b6b2c95f":"# Import necessary libraries\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Libraries for Data manipulation\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\n\n# Importing plotting libraries\nimport matplotlib.pyplot as plt\n%matplotlib inline","a2f05533":"# Loading the Dataset\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\n\ntrain_data.head()","eaaa416c":"# Print the datatypes of all columns\nprint(train_data.dtypes)","b0af613b":"# Printing the no. of NaN values in all columns\n\nprint(\"Training Data Missing Values:\\n\",train_data.isna().sum())\nprint(\"\\nTest Data Missing Values:\\n\",test_data.isna().sum())","b7d8e2ad":"# Cabin column seems to have a lot of missing values.\n# We will replace all values with the first alphabet of the Cabin\n\ntrain_data['Cabin'] = train_data['Cabin'].apply(lambda x: str(x)[0])\ntest_data['Cabin'] = test_data['Cabin'].apply(lambda x: str(x)[0])\n\n# Null values will be set to 'n'\nprint(\"Cabin:\", train_data['Cabin'].unique())","1ff38cad":"# First, let's replace NaN embarkations with the modal value of embarkation\n\nmode_embarked = train_data['Embarked'].mode()[0]\ntrain_data['Embarked'].fillna(value=mode_embarked, inplace=True)\n\nprint(\"Number of missing values in the Embarked column: \", train_data['Embarked'].isna().sum())","3ce83b09":"# Next, let's replace NaN Ages with the mean age in the dataset\n\nmean_training_age = train_data['Age'].mean()\nmean_testing_age = test_data['Age'].mean()\n\nprint(\"Mean training data is: \", mean_training_age)\nprint(\"Mean testing data is: \",mean_testing_age)\n\n# Replace NaN values of Age column with Mean\ntrain_data['Age'].fillna(value=mean_training_age, inplace=True)\ntest_data['Age'].fillna(value=mean_testing_age, inplace=True)\n\nprint(\"\\nNumber of missing training values in the Age column: \", train_data['Age'].isna().sum())\nprint(\"Number of missing testing values in the Age column: \", test_data['Age'].isna().sum())","9bf7e721":"# First, we'll fix 1 NaN value in fare column of test data\n\nmean_testing_fare = test_data['Fare'].mean()\ntest_data['Fare'].fillna(value=mean_testing_fare, inplace=True)\n\nprint(\"Mean Fare in the testing data is: \", mean_testing_fare)\nprint(\"Number of missing testing values in the fare column: \", test_data['Fare'].isna().sum())","5127502e":"# A Final check to ensure no null values remain\n\nprint(\"Training Data Missing Values:\\n\",train_data.isna().sum())\nprint(\"\\nTest Data Missing Values:\\n\",test_data.isna().sum())","da3de30d":"# Class wise Survival\n\npclasses = np.sort(train_data.Pclass.unique())\n\nlabels = ['Survived', 'Died']\nfig, axes = plt.subplots(1, 3, figsize=(20, 35))\naxes_flat = axes.flatten()\nexplode = (0, 0.1)\n\nfor i in range(len(pclasses)):\n    axis = axes_flat[i]\n    \n    pclass_data = train_data[train_data.Pclass == pclasses[i]]\n    survived = pclass_data[pclass_data.Survived == 1].shape[0]\n    died = pclass_data.Survived.shape[0] - survived\n    \n    axis.pie([survived, died], labels=labels, explode=explode, \n             shadow=True, startangle=90, autopct='%1.1f%%',\n             textprops={'fontsize': 16})\n    axis.set_title(\"Class {} Passesngers\".format(pclasses[i]), fontdict={'size':16})\n\nplt.text(x=-4, y=2, s=\"Passenger Class Wise Survival\", fontdict={'size':18})\nplt.show()","4f8a1a11":"# Gender wise Survival\n\ngender = ['male', 'female']\nlabels = ['Survived', 'Died']\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 20))\naxes_flat = axes.flatten()\nexplode = (0, 0.1)\n\nfor i in range(len(gender)):\n    axis = axes_flat[i]\n    \n    gender_data = train_data[train_data.Sex == gender[i]]\n    survived = gender_data[gender_data.Survived == 1].shape[0]\n    died = gender_data.Survived.shape[0] - survived\n    \n    axis.pie([survived, died], labels=labels, explode=explode, \n             shadow=True, startangle=90, autopct='%1.1f%%', \n             textprops={'fontsize': 16})\n    axis.set_title(\"Gender: {}\".format(gender[i]), fontdict={'fontsize':16})\n\nplt.text(x=-2, y=2, s=\"Gender Wise Survival\", fontdict={'size':18})\nplt.show()","1858c627":"# Place of Embarkment and Survival Rate\n\nplt.figure(figsize=(12,6))\nsns.catplot(x='Embarked', y='Survived', ci=None, kind='bar', hue='Pclass', data=train_data)\n\nplt.show()","561629d7":"# Finding Embarkment wise survival\ngrouped_survived = train_data.groupby(['Embarked'])['Survived'].sum()\ngrouped_survived","335958df":"# Finding the number of class 1 passengers from queenstown\nnum_queenstown = train_data[train_data['Embarked'] == 'Q']\nnum_class = num_queenstown[num_queenstown['Pclass'] == 2]\nnum_survived = num_class[num_class['Survived'] == 1]\n\nprint(\"Total number of passengers survived from Queenstown are\/is:\", grouped_survived['Q'])\nprint(\"Total number of passengers of class 2 from Queenstown are\/is:\",num_class['PassengerId'].count())\nprint(\"Number of class 2 passengers that survived from Queenstown are\/is:\", num_survived['PassengerId'].count())","00954427":"# Cabin, class and Fare prices\nplt.figure(figsize=(12,8))\nsns.boxplot(y='Cabin', x='Fare', data=train_data)\nplt.title(\"Cabin vs Fare\")\n\nplt.show()","e32f80ce":"# Age wise Survival\n\ngender = ['male', 'female']\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\naxes_flat = axes.flatten()\nx_ticks = [i for i in range(0, 90, 10)]\ny_ticks = [i for i in range(0, 150, 10)]\n\nfor i in range(len(gender)):                                        # For male and female\n    axis = axes_flat[i]\n    \n    gender_data = train_data[train_data.Sex == gender[i]]\n    sns.histplot(data=gender_data, x=gender_data.Age, hue='Survived', ax=axis)\n    \n    axis.set_xticks(x_ticks)\n    axis.set_xticklabels(x_ticks)\n    axis.set_yticks(y_ticks)\n    axis.set_yticklabels(y_ticks)\n    axis.set_title(\"Gender: {}\".format(gender[i]), fontdict={'fontsize':16})\n\nplt.text(s=\"Gender Wise Survival\", y=160, x=-30, fontdict={'size':18})\nplt.show()","7a5731c9":"# Analysing ticket data\ntrain_data.Ticket.describe()","037854ce":"# Since the number of unique values in ticket is too large, it cannot be generalised and used.\n# Thus we drop the column entirely\n\ntrain_data.drop(labels=\"Ticket\", axis=1 ,inplace=True)\ntest_data.drop(labels=\"Ticket\", axis=1 ,inplace=True)","e04b4392":"# Used to encode categorical values\n\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()","0178ebc6":"# Encoding Sex with 0 and 1\ntrain_data['Sex'] = encoder.fit_transform(train_data['Sex'])\ntest_data['Sex'] = encoder.transform(test_data['Sex'])","393ee83b":"# Similarly, we encode all the ports\n\ntrain_data['Embarked'] = encoder.fit_transform(train_data['Embarked'])\ntest_data['Embarked'] = encoder.transform(test_data['Embarked'])","d1bf195d":"# And then we also encode cabin data\n\ntrain_data['Cabin'] = encoder.fit_transform(train_data['Cabin'])\ntest_data['Cabin'] = encoder.transform(test_data['Cabin'])","224b7644":"# Making a new Feature called Title\n\ndata_all = [train_data, test_data]\n\nfor data in data_all:\n    data['Title'] = data['Name'].apply(lambda x: x.split(\", \")\n    [1].split(\".\")[0])\n\n    # Grouping similar titles\n    data['Title'].replace(['Miss', 'Mrs', 'Mme', 'Mlle', 'Ms'], 'Mrs\/Ms\/Miss', inplace=True)\n    data['Title'].replace(['Lady', 'the Countess'], 'Noble Females', inplace=True)\n    data['Title'].replace(['Dr', 'Don', 'Rev', 'Col', 'Capt', 'Jonkheer', 'Major', 'Sir'], \n                          'Nobility\/Doctor\/Clergy\/Navy', inplace=True)\n    \nprint(\"Training data titles: \\n\", train_data['Title'].unique())\nprint(\"\\nTesting data titles: \\n\", test_data['Title'].unique())","65f04f94":"# We observe that we get an exception where title is set as Dona\n# Let us replace it with Mrs\/Ms\/Miss title\n\ntest_data['Title'].replace('Dona', 'Mrs\/Ms\/Miss', inplace=True)\nprint('Testing data titles: \\n', test_data['Title'].unique())","28ef2379":"# Plotting and analysing data with the help of the new feature\n\nplt.figure(figsize=(10,6))\nplt.xlabel('Titles', fontsize=16)\nplt.xticks(fontsize=14, rotation=45)\nplt.ylabel('Count', fontsize=16)\n\nsns.histplot(data=train_data, x=train_data['Title'], hue='Survived', multiple='stack')\n\nplt.title('Survival Based On Title Held By The Passenger', fontsize=18)\nplt.show()","70a86d3c":"# Finally, we encode the new feature created\n\ntrain_data['Title'] = encoder.fit_transform(train_data['Title'])\ntest_data['Title'] = encoder.transform(test_data['Title'])","0720d649":"# Drop the unecessary columns for the training and testing dataset\n\nX_train = train_data.drop([\"Survived\", \"Name\", \"PassengerId\"], axis=1)\ny_train = train_data[\"Survived\"]\nX_test  = test_data.drop([\"PassengerId\", \"Name\"], axis=1).copy()","6d3bcea6":"# Cross Validation for Analysing Models\n\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\n\n# We will append cross validation scores of each model used\ncv_results = {'Model':[], 'Score':[]}\n\n# Generating KFold validation sets\nkfold = StratifiedKFold(n_splits=10)","f695bd48":"from sklearn.linear_model import LogisticRegression\n\nlog_model = LogisticRegression(solver='liblinear')\nlog_model.fit(X_train, y_train)\ny_pred_log = log_model.predict(X_test)\n\ncv_score = cross_val_score(log_model, X_train, y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4).mean()\ncv_results['Model'].append('Logistic Regression')\ncv_results['Score'].append(cv_score)","00c73eef":"from sklearn.naive_bayes import GaussianNB\n\nnb_model = GaussianNB()\nnb_model.fit(X_train, y_train)\ny_pred_nb = nb_model.predict(X_test)\n\ncv_score = cross_val_score(nb_model, X_train, y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4).mean()\ncv_results['Model'].append('Naive Bayes')\ncv_results['Score'].append(cv_score)","08eac845":"from sklearn.ensemble import RandomForestClassifier\n\nrf_model = RandomForestClassifier(max_depth=10)\n# Note: Max depth set to 10 to prevent overfitting \n\nrf_model.fit(X_train, y_train)\ny_pred_rf = rf_model.predict(X_test)\n\ncv_score = cross_val_score(rf_model, X_train, y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4).mean()\ncv_results['Model'].append('Random Forest')\ncv_results['Score'].append(cv_score)","59571470":"from sklearn.svm import SVC\n\nsvc_model = SVC()\nsvc_model.fit(X_train, y_train)\ny_pred_svc = svc_model.predict(X_test)\n\ncv_score = cross_val_score(svc_model, X_train, y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4).mean()\ncv_results['Model'].append('Support Vectors')\ncv_results['Score'].append(cv_score)","98eef41d":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier()\nknn_model.fit(X_train, y_train)\ny_pred_knn = knn_model.predict(X_test)\n\ncv_score = cross_val_score(knn_model, X_train, y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4).mean()\ncv_results['Model'].append('K-Nearest Neighbors')\ncv_results['Score'].append(cv_score)","db701539":"result_df = pd.DataFrame.from_dict(cv_results)\n\n#Note that this is corss-validation score and not the final score \nresult_df","086d3dec":"# So we will be using random forest as the model of choice.\npassenger_id = test_data['PassengerId'] # This will come in useful at the time of submission\ntest_data.drop([\"PassengerId\", \"Name\"], axis=1, inplace=True)","08e03488":"y_preds = rf_model.predict(test_data)\nsubmission_df = pd.DataFrame(np.stack([passenger_id, y_preds], axis=1), columns=['PassengerId', 'Survived'])\nsubmission_df","d9bd1661":"# Save the result as csv \n#submission_df.to_csv(\"Submission_1.csv\", index=False)","20651543":"Well, it seems that the number of passengers in class 2 from Quenstown were only 3\n\nHence it will be wrong to assume anything for the passengers who boarded from Queenstown solely based on the Port of Embarkment.","8c093001":"### Feature Engineering","3d8b549c":"### Random Forest","87963e1a":"# Titanic - A Statistical Survival Guide","f7918747":"The above graph shows the chance of survival according to the port of embarkment. \n\nIt is interesting to note that class 2 passengers who boarded at Queenstown (Q) had a better survival rate than class 1 passengers from Queenstown and almost comparable to class 1 passengers from Cherbourg. \n\nWe will look more into why this was the case.","8bd376c0":"It is observed that, in general Cabins B and C have a higher fare\/cost. ","955c9878":"### Logistic Regression","3b89dca1":"* While I have tried to cover and analyse many columns, certain columns have been left out. For example, the columns SibSp and Parch can be utilized to Feature Engineer another column describing the Family size of a PassegerId. Family size may prove an important parameter for survival.\n\n* Similarly, Ticket columns can be put to better use by clubbing ticket numbers of passengers from same Class, Cabin or Family.\n\n* As far as accuracy of the model is concerned, it gave me a 0.7703 score on submission. This score not good as far as i am concerned. But, it can be easily improved using hyperparameter tuning of the models. GridSearchCV can be used for finding out the best possible parameters for each model.\n\n**This concludes the analysis for the titanic dataset.**","d3e70caf":"The above picture shows the cabin layout for the ship. \n\nThe Cabins A, B and C are closer to the deck, and thus the passengers in these decks will have a better survival chance.","6c856691":"### K-Nearest Neighbors","ddfa4346":"Both the graphs exhibit a normal distribution, the only difference being the lower percentage of survival in each age category for males.\n\nWhile females have a 50percent survival chance in almost every age group, no ages of males who survived dominates the ones who did not.\n\nApart from the Gender bias, it seems that the percent of passengers (children below 20) seem to have a better survival rate. Age group 15-30 have a mediocre survival chance.","6f6b6ca1":"## Data Preprocessing","5eac239f":"### Final Predictions","2a990bd4":"### Naive Bayes","c018f89b":"## Building The Machine Learning Model","85b1877e":"### SVC","13a350b4":"**The Unspoken *Women First* Rule**\n\nThe above mentioned statistics highlight the common notion about any emergency evacuation. \nWomen and children are given priority, their lives are saved first in a life-threatening situation.\n\nIt must be stated here that there is no maritime law\/protocol stated related to this. The most vulnerable people, those in need of help must be prioritised in any emergency situation.","2bddb13a":"**Class Matters**\n\n![Image](http:\/\/rpmarchildon.com\/wp-content\/uploads\/2018\/06\/titanic_class_cabin_locations.png)&nbsp;\n\n\nThe above image shows where different classes of passengers stayed.&nbsp;\n\nThe image clearly shows that Class 1 and Class 2 were closer to the deck and hence to the lifeboats.\n\nThus they had an obvious survival advantage, with more than 60 and 45 percent chance of survival.","456bb484":"## Visualizing The Data","edd663e3":"<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/0\/0d\/Olympic_%26_Titanic_cutaway_diagram.png\" \n     alt=\"diagram\" width=\"500\" style=\"  display: block; margin-left: auto; margin-right: auto;\"\/>","8258da50":"## Future Scope And Conclusion"}}