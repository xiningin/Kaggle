{"cell_type":{"fc73c925":"code","7d9c706d":"code","e1c5e416":"code","e54bba40":"code","ddaf7732":"code","d9109cfd":"code","ac6983d8":"code","c854552e":"code","7cc02848":"code","c45b4b71":"code","f1c95c6d":"code","5f453ecb":"code","e14c9207":"code","374150ff":"code","7e04c43f":"code","0d06ae7a":"code","aa6b90ab":"code","41b53772":"code","d36b48d1":"code","f25102e4":"code","61b0f969":"code","d924ced6":"code","bc83d0a4":"code","93828361":"code","ec4c9f74":"code","149e10b9":"code","798176f6":"code","614baeef":"code","d3bd9753":"code","5f3016a9":"code","3acb0375":"code","65f0cced":"code","d51c587e":"code","6fabaab3":"code","90fa9328":"code","85181dbd":"code","577a1b01":"code","45f898b6":"code","1757f3d7":"code","0145df28":"code","e1bfb9d5":"code","fae9ec7f":"code","8e22d1fc":"code","592608d3":"code","cbba49fa":"code","e6e63890":"code","4fa86390":"code","d63bb3b4":"code","59e59731":"code","8a9033d4":"code","87c9e883":"code","2738d3e4":"code","c1aaa786":"code","dbfb984e":"code","77a1b7e7":"code","95acfd22":"code","231a58fc":"code","7fff667a":"code","c5c0c553":"code","45f49634":"code","49cec01f":"code","748bfbdc":"code","72015611":"code","54c43c92":"code","d5783a56":"code","62d1653e":"code","5ad1d147":"code","5d0883a0":"code","6b9b684c":"code","2a1ba121":"code","3c434577":"code","a97ef990":"code","315c0455":"code","b90fabe5":"code","5aff799a":"code","884c53bc":"code","9a219e37":"code","53645b8c":"code","2952d3f8":"code","f77c323a":"code","38c5345b":"code","09b977c9":"code","6000d283":"code","7d5ea52d":"code","d4385db0":"code","3cb86a84":"code","0bedcba7":"code","b89be91f":"code","e6971067":"code","8febf1e9":"code","57d3a058":"code","82c1aaf4":"code","43ae3727":"code","74ecefcf":"code","f0a9f4eb":"code","38af8fd4":"code","678a6f2a":"code","bcf52af8":"code","5303acf1":"code","d2a7ea8f":"code","40dae9f0":"code","fd952143":"markdown","344232b0":"markdown","7ac588b1":"markdown","aaaf7d47":"markdown","637eca14":"markdown","eb3b9e1e":"markdown","2cea5ca5":"markdown","f672d970":"markdown","72f14023":"markdown","bded2b7b":"markdown","c16af7f3":"markdown","630ddd1d":"markdown","31d0aecf":"markdown","c6b126af":"markdown","1054f99b":"markdown","47f1a8f7":"markdown","ffb0e9d7":"markdown","8f31a76a":"markdown","080cbd62":"markdown","4af27335":"markdown","da9d9023":"markdown","5c6b313b":"markdown","5b25bc46":"markdown","4eeedf6c":"markdown","bd18aed6":"markdown","8d2169aa":"markdown","c44172f5":"markdown","3f0e00fe":"markdown","5e420070":"markdown","09d2cf3d":"markdown","2bd89755":"markdown","ee28d558":"markdown","014dfaa5":"markdown","2353718a":"markdown","c5b15f5e":"markdown","44c070ef":"markdown","7d80814a":"markdown","00544d87":"markdown","684deda3":"markdown","acb16032":"markdown","cfcf9e2f":"markdown","d66a5914":"markdown","7864174a":"markdown","0c3bb697":"markdown","16f86b7a":"markdown","766dbbfc":"markdown","1fe508f8":"markdown","6d4dfcfd":"markdown","e117ff33":"markdown","7b5ff776":"markdown","33ee8a84":"markdown","8d9af4ea":"markdown","1c73b0e5":"markdown","5971609b":"markdown","f833c494":"markdown","eda5b280":"markdown","7dafc901":"markdown","0b737fac":"markdown","e5e3b6cb":"markdown","9ae00400":"markdown","6d028a13":"markdown","b4103d01":"markdown","9168b24c":"markdown"},"source":{"fc73c925":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n\nimport pandas_profiling as pdp\n\nimport gc\ngc.enable()\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import RegexpTokenizer\n\nimport spacy\n\n!python -m spacy download en_core_web_md\n!pip install wordcloud\n!pip install pyspellchecker\n!pip install contractions\n!pip install imblearn\n!pip install -U textblob\n!python -m textblob.download_corpora\n!pip install empath\n\nimport contractions\nimport en_core_web_md\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom collections import Counter\nfrom wordcloud import WordCloud \n\nimport scattertext as st\n\nfrom spellchecker import SpellChecker\nimport re\n\nfrom textblob import TextBlob\n\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import GridSearchCV\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\n\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","7d9c706d":"from IPython.display import IFrame\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:98% !important; }<\/style>\"))","e1c5e416":"data = pd.read_csv(\"..\/input\/sexualpredatorspan\/train_def.csv\")","e54bba40":"data.describe().T","ddaf7732":"data.info()","d9109cfd":"data.head()","ac6983d8":"data.tail()","c854552e":"reporte = pdp.ProfileReport(data, title=\"Pandas Profiling Reporte\",minimal=True)\nreporte","7cc02848":"sns.countplot(data=data,x='predator',orient=\"h\")\nplt.title(\"No predador\/Predador\")\nplt.xlabel(\"Predador\")\nplt.ylabel(\"Cantidad\")\nplt.show()","c45b4b71":"plt.figure(figsize=(10,10))\ndata.predator.value_counts().plot(kind=\"pie\",autopct='%1.0f%%')\nplt.ylabel(\"Usuarios\")\n\nplt.title(\"Numero de Predadores en porcentajes\")\nplt.show","f1c95c6d":"data['msg']=data['msg'].apply(str)","5f453ecb":"data['contara'] = data['msg'].apply(lambda x: len(str(x).split()))","e14c9207":"data.head()","374150ff":"plt.figure(figsize=(16,8))\nsns.barplot(data = data, x = \"predator\", y = \"contara\")\nplt.title('cantidad de palabras segun predador\/No predador', fontsize= 25)\nplt.xlabel('predator')\nplt.ylabel('contara')","7e04c43f":"pred= data[data[\"predator\"]== 1]","0d06ae7a":"pred_user=pred[\"author id\"].value_counts()\npred_user","aa6b90ab":"pred_user.shape","41b53772":"user_unique=data[data[\"author id\"] == \"a03edc2f70bbebc73ef3ba3f06968360\" ]\nuser_unique.head(10)","d36b48d1":"user_unique.tail(10)","f25102e4":"pred_user_conv=pred[\"conversation id\"].value_counts()\npred_user_conv","61b0f969":"pred_user_conv.shape","d924ced6":"conv_unique=data[data[\"conversation id\"] == \"629a3f3a95f4d4253f0def5f6e4434f6\" ]\nconv_unique.head(10)","bc83d0a4":"conv_unique.tail(10)","93828361":"for i in range(data.shape[1]):\n    print(i,len(pd.unique(data.iloc[:,i])))","ec4c9f74":"for i in range(data.shape[1]):\n    num=len(pd.unique(data.iloc[:,i]))\n    porcentaje=float(num)\/data.shape[0]*100\n    print(\"%d, %d, %.1f%%\"%(i,num,porcentaje))","149e10b9":"duplicado = data.duplicated()\nprint(duplicado.any())\nprint(data[duplicado])","798176f6":"data.drop([\"Unnamed: 0\",\"conversation id\",\"author id\",\"time\",\"contara\"],axis=1,inplace=True)","614baeef":"data.head()","d3bd9753":"nlp = en_core_web_md.load()","5f3016a9":"stopwords_spacy = list(STOP_WORDS)\nprint(stopwords_spacy)\nlen(stopwords_spacy)","3acb0375":"en_stop_words=STOP_WORDS","65f0cced":"spell = SpellChecker(language='en',distance=1)","d51c587e":"def spell_check(x):\n    correct_word = []\n    mispelled_word = x\n    for word in mispelled_word:\n        correct_word.append(spell.correction(word))\n    return ' '.join(correct_word)","6fabaab3":"pred1=data[data[\"predator\"]== 1]","90fa9328":"pred1.shape","85181dbd":"pred0=data[data[\"predator\"]== 0]","577a1b01":"pred0.shape","45f898b6":"pred0.head()","1757f3d7":"pred01=pred0[0:250000]","0145df28":"pred01.shape","e1bfb9d5":"pred02=pred0[250000:500000]","fae9ec7f":"pred02.shape","8e22d1fc":"pred03=pred0[500000:750000]","592608d3":"pred03.shape","cbba49fa":"pred04=pred0[750000:862626]","e6e63890":"pred04.shape","4fa86390":"pred01 = pd.read_csv(\"..\/input\/sexualpredator1part\/pred01.csv\")","d63bb3b4":"#pred02 = pd.read_csv(\"pred02.csv\")","59e59731":"#pred03 = pd.read_csv(\"pred03.csv\")","8a9033d4":"#pred04 = pd.read_csv(\"pred04.csv\")","87c9e883":"pred1 = pd.read_csv(\"..\/input\/sexualpredator1part\/pred1.csv\")","2738d3e4":"data=pd.concat([pred01,pred1])\ndata.head()","c1aaa786":"data = data.dropna()","dbfb984e":"data.shape","77a1b7e7":"data=data.drop(data[data.msg_lemm.str.contains(\"PRON\")| data.msg_lemm.str.contains(\"http\")].index)","95acfd22":"data.shape","231a58fc":"data.info()","7fff667a":"data_pred = data[data.predator == 1]\ndata_nopred = data[data.predator == 0]","c5c0c553":"data_pred.shape","45f49634":"data_nopred.shape","49cec01f":"predators = []\nregexp=RegexpTokenizer(r\"\\w+\")\n\nfor i in range(data_pred.shape[0]):\n    x = data_pred.iloc[i].msg_lemm\n    x = regexp.tokenize(x) \n    x = [t for t in x if len(t)>1]\n    predators.append(x) ","748bfbdc":"pred=[line for line in predators for line in set(line)]\npred = Counter(pred)\npred = pred.most_common(20)\npred=pd.DataFrame(pred,columns = ['Words', 'Frequency'])\npred.head(10)","72015611":"no_predators = []\nregexp=RegexpTokenizer(r\"\\w+\")\n\nfor i in range(data_nopred.shape[0]):\n    x = data_nopred.iloc[i].msg_lemm\n    x = regexp.tokenize(x) \n    x = [t for t in x if len(t)>1] \n    no_predators.append(x) ","54c43c92":"nopred=[line for line in no_predators for line in set(line)]\nnopred = Counter(nopred)\nnopred = nopred.most_common(20)\nnopred=pd.DataFrame(nopred,columns = ['Words', 'Frequency'])\nnopred.head(10)","d5783a56":"plt.figure(figsize=(12,12))\n\nplt.subplot(321)\npreds=(\" \").join(pred[\"Words\"])\nwc = WordCloud(max_words = 20, width = 1800 , height = 800 ).generate(preds)\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.title('predators', fontsize=25)\n\nplt.subplot(322)\nnopreds=(\" \").join(nopred[\"Words\"])\nwc = WordCloud(max_words = 20, width = 1800 , height = 800).generate(nopreds)\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.title('no predators', fontsize=25)\n\nplt.tight_layout()\nplt.show()","62d1653e":"data2 = data.copy()","5ad1d147":"data2['cat']=data['predator'].astype(\"category\").cat.rename_categories({0:'nopred',1:'pred'})","5d0883a0":"%%time\n\nscatter_corpus_nlp = st.CorpusFromPandas(data2,\n                             category_col='cat',\n                             text_col='msg_lemm',nlp=nlp).build()","6b9b684c":"html = st.produce_scattertext_explorer(scatter_corpus_nlp,\n         category='pred',category_name='predator',         \n        not_category_name='nopred',width_in_pixels=1000,\n          metadata=data2['cat'])\nopen(\"preds-Visualization.html\", 'wb').write(html.encode('utf-8'))\nIFrame(src='preds-Visualization.html', width = 1300, height=700)","2a1ba121":"%%time\n\nfeat_builder = st.FeatsFromOnlyEmpath()\nempath_corpus = st.CorpusFromParsedDocuments(data2,\n                                              category_col='cat',\n                                              feats_from_spacy_doc=feat_builder,\n                                              parsed_col='msg_lemm').build()\nhtml = st.produce_scattertext_explorer(empath_corpus,\n                                        category='pred',\n                                        category_name='predator',\n                                        not_category_name='nopred',\n                                        width_in_pixels=1000,\n                                        metadata=data2['cat'],\n                                        use_non_text_features=True,\n                                        use_full_doc=True,\n                                        topic_model_term_lists=feat_builder.get_top_model_term_lists())\nopen(\"Visualization-Empath.html\", 'wb').write(html.encode('utf-8'))\nIFrame(src='Visualization-Empath.html', width = 1300, height=700)","3c434577":"blob = TextBlob(str(data['msg_lemm']))\npos_df = pd.DataFrame(blob.tags, columns = ['word' , 'pos'])\npos_df = pos_df.pos.value_counts()[:20]\npos_df.iplot(\n    kind='bar',\n    xTitle='POS',\n    yTitle='count', \n    title='Top 6 Part-of-speech tagging para msg_lemm')","a97ef990":"X_train, X_test, y_train, y_test = train_test_split(np.array(data['msg_lemm']), \n                                                    np.array(data['predator']), stratify = np.array(data['predator']),\n                                                    test_size = 0.20, random_state = 42)\n\ntfidf = TfidfVectorizer(min_df = 0.00015, norm = 'l2', use_idf = True, smooth_idf = True)\n\nX_train = tfidf.fit_transform(X_train)\nX_test = tfidf.transform(X_test)\n\nprint(X_train.shape)\nprint(X_test.shape)","315c0455":"all_words = tfidf.get_feature_names()","b90fabe5":"over = SMOTE(random_state=42)\nunder = RandomUnderSampler(random_state=42)\n\nsteps = [('o', over), ('u', under)]\npipeline = Pipeline(steps=steps)","5aff799a":"X_train, y_train = pipeline.fit_resample(X_train, y_train)\n","884c53bc":"X_train = X_train.astype(np.uint8)\nX_test = X_test.astype(np.uint8)","9a219e37":"svc = LinearSVC(C = 0.1)\nsvc.fit(X_train.todense(),y_train)","53645b8c":"y_train_pred = svc.predict(X_train.todense())\ny_test_pred = svc.predict(X_test.todense())\n\nprint(\"Accuracy train LinearSVC: \", accuracy_score(y_train, y_train_pred))\nprint(\"Accuracy test LinearSVC: \", accuracy_score(y_test, y_test_pred))","2952d3f8":"print(classification_report(y_test, y_test_pred)) ","f77c323a":"plot_confusion_matrix(svc, X_test.todense(), y_test)  ","38c5345b":"coeff = list(svc.coef_[0])\nlabels = list(all_words)\nimportancia = pd.DataFrame()\nimportancia['words'] = labels\nimportancia['orden'] = coeff\nimportancia = importancia.reset_index(drop=True)\norden_imp = importancia.sort_values(by=['orden'], ascending=False)\norden_imp = orden_imp.reset_index(drop=True)\nplot = pd.concat([orden_imp.head(10), orden_imp.tail(10)])\nplot.sort_values(by=['orden'], ascending=False, inplace = True)\nplot['vpos'] = plot['orden'] > 0\nplot.set_index('words', inplace = True)\nplot.orden.plot(kind='bar', figsize = (14,8),color = plot.vpos.map({True: 'pink', False: 'violet'}),fontsize=12,orientation=u'vertical')\nplt.xlabel ('words', fontsize=12)\nplt.xticks(rotation=45)\nplt.ylabel ('orden', rotation = 90, fontsize=12)\nplt.title ('Word importance for predator\/no predator ',fontsize=20)\nplt.show()","09b977c9":"import tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers.core import SpatialDropout1D\nimport keras\nfrom keras.layers.convolutional import Conv1D   \nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros","6000d283":"from keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","7d5ea52d":"custom_early_stopping = EarlyStopping(\n    monitor='val_accuracy', \n    patience=20,\n    restore_best_weights=True\n)","d4385db0":"x = {0: 0,1:1}\n\ndata['binario'] = data['predator'].map(x)","3cb86a84":"X = data['msg_lemm']\ny = data[\"binario\"].values","0bedcba7":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42,stratify=y)","b89be91f":"max_words = 5000\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(X_train)\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nX_train = tokenizer.sequences_to_matrix(X_train, mode='count')\nX_test = tokenizer.sequences_to_matrix(X_test, mode='count')\n\n\nnum_classes = max(y_train) + 1\n\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)","e6971067":"model = Sequential()\nmodel.add(Dense(128, input_shape=(max_words,)))\nmodel.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(64))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32))\nmodel.add(Dropout(0.5))\nmodel.add(Activation('relu'))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))","8febf1e9":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',f1_m,precision_m, recall_m])\nprint(model.metrics_names)","57d3a058":"model.summary()","82c1aaf4":"batch_size = 32\nepochs = 50","43ae3727":"history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.3,callbacks=[custom_early_stopping])","74ecefcf":"score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","f0a9f4eb":"X = data['msg_lemm']\ny = data[\"binario\"].values\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42,stratify=y)\n\nMAX_NB_WORDS = 5000\n\nMAX_SEQUENCE_LENGTH = 20\n\nEMBEDDING_DIM = 100\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nX_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\nX_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n\n\n\nnum_classes = max(y_train) + 1\n\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)","38af8fd4":"EMBEDDING_DIM=100","678a6f2a":"model = Sequential()\nembedding_layer= (Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\nmodel.add(embedding_layer)\nmodel.add(Conv1D(300, 5, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(150, activation='sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])","bcf52af8":"model.summary()","5303acf1":"epochs = 300\nbatch_size = 512","d2a7ea8f":"history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.3,callbacks=[custom_early_stopping])","40dae9f0":"score= model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","fd952143":"#### pred03= 500000 a 750000","344232b0":"### Cantidad de predadores Sexuales","7ac588b1":"### El dataset cuenta con 900.000 filas por lo que se dificulta su analisis sin sufrir un agotamiento de la RAM en poco tiempo. Para evitar este problema se lo procesara por partes y finalmente se volvera a unir.","aaaf7d47":"### Visualizacion de palabras","637eca14":"#### pred1 (Aprox 6 min)","eb3b9e1e":"# C.1)Metrica","2cea5ca5":"#### pred02= 250000 a 500000","f672d970":"### Activacion de Spellcheck","72f14023":"### Analisis Individual de conversacion","bded2b7b":"### Para el desarrollo del presente trabajo utilizaremos 2 metricas:\n\n    Accuracy: Debido a que se trabajara con un dataset balanceado, esto es posible despues de procesar los datos con SMOTE Y RandomUnderSampler\n\n    F1: Esta metrica se utilizara como complemento y para mostrar el poder de los modelos, ya que nos da una vision mas integral al tener en cuenta tanto el accuracy como el recall del modelo.\n    \n#### Vale aclarar que F1 es una metrica nula para las redes neuronales, esto se debe a que F1 trabaja con la globalidad y las redes neuronales funcionan analizando los batchs. Aun asi fue incluida esta medida como forma demostrativa. Pero en este caso se recomienda guiarse con accuracy o en su caso val_accuracy","c16af7f3":"### Metricas globales","630ddd1d":"pred04.to_csv('\/kaggle\/working\/pred04.csv', index = False, encoding = 'utf-8')","31d0aecf":"### Union de los preds","c6b126af":"### Organizacion:\n    A) Realizar una corecta exploracion y entendimiento de los datos.\n    B) Realizar un preprocesamiento de los datos.\n    C) Aplicar Machine Learning.\n            C.1) Eval\u00faa de forma apropiada sus resultados. Justifica la m\u00e9trica elegida.\n            C.2) Elige un modelo benchmark.\n    D)Aplicar Deep learning.\n            D.1) Compara el benchmark con las redes neuronales.","1054f99b":"### Busqueda de duplicados","47f1a8f7":"### Visualizacion de terminos asociados a predadores\/no predadores","ffb0e9d7":"### Resultados Obtenidos:\n       \n       \u2022LinearSVC:\n           Accuracy en train:0.6196\n           Accuracy en Test:0.7875\n           F1 score: 0.79\n           \n       \u2022Red neuronal simple:\n           Accuracy en train: 0.9096\n           Accuracy en test:0.8743\n           F1 score:0.8743\n           \n       \u2022Convolucional:\n           Accuracy en train:0.9188\n           Accuracy en test:0.8603\n           F1 score:0.8604\n           ","8f31a76a":"# B)Preprocesamiento de datos","080cbd62":"### Contar palabras","4af27335":"# C)Machine Learning","da9d9023":"### Predadores","5c6b313b":"### SMOTE: para dataset imbalanceado","5b25bc46":"%%time\n\nregexp=RegexpTokenizer(r\"\\w+\")\npred01['msg_lemm'] = ''\n\nfor i in range(pred01.shape[0]):\n    msgs = pred01.iloc[i].msg\n    msgs = contractions.fix(msgs)\n    msgs = regexp.tokenize(msgs)\n    msgs = [word.lower() for word in msgs if word.lower() not in en_stop_words]\n    msgs = spell_check(msgs)\n    msgs = nlp(msgs)\n    msgs = [word.lemma_ for word in msgs]\n    pred01['msg_lemm'].iloc[i] = ' '.join(msgs)","4eeedf6c":"pred01.to_csv('\/kaggle\/working\/pred01.csv', index = False, encoding = 'utf-8')","bd18aed6":"pred02.to_csv('\/kaggle\/working\/pred02.csv', index = False, encoding = 'utf-8')","8d2169aa":"%%time\n\nregexp=RegexpTokenizer(r\"\\w+\")\npred1['msg_lemm'] = ''\n\nfor i in range(pred1.shape[0]):\n    msgs = pred1.iloc[i].msg\n    msgs = contractions.fix(msgs)\n    msgs = regexp.tokenize(msgs)\n    msgs = [word.lower() for word in msgs if word.lower() not in en_stop_words]\n    msgs = spell_check(msgs)\n    msgs = nlp(msgs)\n    msgs = [word.lemma_ for word in msgs]\n    pred1['msg_lemm'].iloc[i] = ' '.join(msgs)","c44172f5":"#### pred04 (Aprox 20 min)","3f0e00fe":"%%time\n\nregexp=RegexpTokenizer(r\"\\w+\")\npred02['msg_lemm'] = \"\"\n\nfor i in range(pred02.shape[0]):\n    msgs = pred02.iloc[i].msg\n    msgs = contractions.fix(msgs)\n    msgs = regexp.tokenize(msgs)\n    msgs = [word.lower() for word in msgs if word.lower() not in en_stop_words]\n    msgs = spell_check(msgs)\n    msgs = nlp(msgs)\n    msgs = [word.lemma_ for word in msgs]\n    pred02['msg_lemm'].iloc[i] = ' '.join(msgs)","5e420070":"%%time\n\nregexp=RegexpTokenizer(r\"\\w+\")\npred03['msg_lemm'] = \"\"\n\nfor i in range(pred03.shape[0]):\n    msgs = pred03.iloc[i].msg\n    msgs = contractions.fix(msgs)\n    msgs = regexp.tokenize(msgs)\n    msgs = [word.lower() for word in msgs if word.lower() not in en_stop_words]\n    msgs = spell_check(msgs)\n    msgs = nlp(msgs)\n    msgs = [word.lemma_ for word in msgs]\n    pred03['msg_lemm'].iloc[i] = ' '.join(msgs)","09d2cf3d":"### Eliminacion de PRON y HTTPS","2bd89755":"# C.2) Benchmark","ee28d558":"### Earlystopping","014dfaa5":"# A)EDA","2353718a":"### NLP","c5b15f5e":"pred03.to_csv('\/kaggle\/working\/pred03.csv', index = False, encoding = 'utf-8')","44c070ef":"# D.1) Compara el benchmark con las redes neuronales.","7d80814a":"### TextBlob","00544d87":"%%time\n\nregexp=RegexpTokenizer(r\"\\w+\")\npred04['msg_lemm'] = \"\"\n\nfor i in range(pred04.shape[0]):\n    msgs = pred04.iloc[i].msg\n    msgs = contractions.fix(msgs)\n    msgs = regexp.tokenize(msgs)\n    msgs = [word.lower() for word in msgs if word.lower() not in en_stop_words]\n    msgs = spell_check(msgs)\n    msgs = nlp(msgs)\n    msgs = [word.lemma_ for word in msgs]\n    pred04['msg_lemm'].iloc[i] = ' '.join(msgs)","684deda3":"pred1.to_csv('\/kaggle\/working\/pred1.csv', index = False, encoding = 'utf-8')","acb16032":"# Proyecto Final Acamica: Deteccion de predadores sexuales a traves de chats(En Ingles)","cfcf9e2f":"#### pred02 (Aprox 55 min)","d66a5914":"### Analisis individual del usuario con mas mensajes","7864174a":"### Msg_lemm","0c3bb697":"### Convolucional","16f86b7a":"#### Aqui puede utilizar todo el dataset o solo trabajar con partes.","766dbbfc":"### No predadores","1fe508f8":"### Scattertext (25 min aprox)","6d4dfcfd":"#### Al momento de comparar los modelos vemos una mejora significativa al momento de utilizar redes neuronales, pasando de un acc de 0.79 en LinearSVC a un acc de 0.87 con una red neuronal personalizada. Sin duda podemos ver el gran poder que tiene el deep learning en esta area, donde ha demostrado una gran ventaja en este caso en concreto.\n\n#### Personalmente esto era algo de esperarse, a pesar de no tener grandes conocimientos en Deep Learning y no poder garantizar el mejor modelo para este caso. Si puedo se\u00f1alar que el modelo hecho personalmente ha superado a un LinearSVC, algo que considero un peque\u00f1o logro.\n\n#### El unico punto que puedo destacar como positivo para el LinearSVC, es que el mismo no cae en overfitting, los resultados logrados en este sentido son excelentes, y si consideramos la simplicididad de su utilizacion, deberia ser el primer paso para cualquiera que intente realizar este desafio. Por otro lado al ver los resultados de las redes neuronales, podemos ver un peque\u00f1o overfitting en los resultados, problema que puede ser tratado con un aumento de dropouts o noise. Pero al momento de intentar reducirlo he de destacar que no lo he logrado, esto puede deberse a mi desconocimiento en la arquitectura de redes o a los datos mismos.","e117ff33":"### Columnas con pocos valores","7b5ff776":"#### pred03 (Aprox 50 min)","33ee8a84":"### POS msg_lemm","8d9af4ea":"#### pred01= 0 a 250000","1c73b0e5":"### Dataset:https:\/\/zenodo.org\/record\/3713280\n    El dataset es privado debido a lo delicado de su contenido, para tener acceso al mismo se debe solicitar acceso a su autor aclarando el uso que tendra.","5971609b":"### Visualizacion de terminos especificos pero que tienen baja ocurrencia en comparacion con terminos generales (31 min aprox)","f833c494":"### Aqui nuestro modelo de LinearSVC sera nuestro benchmark. A partir de este punto implementaremos Redes Neuronales.","eda5b280":"### Red Neuronal Simple","7dafc901":"# D) Deep Learning","0b737fac":"### Predator=1","e5e3b6cb":"#### Pred01 (Aprox 57 min)","9ae00400":"### Columnas con solo un valor","6d028a13":"### LinearSVC","b4103d01":"### Eliminacion de columnas con poco valor","9168b24c":"#### pred04= 750000 a 862626"}}