{"cell_type":{"4849b9a3":"code","11d41aad":"code","82ea64af":"code","28572aea":"code","e6d58806":"code","7c64f306":"code","3741d0be":"code","5cc91933":"code","fe645acf":"code","a3abdaf5":"code","96105627":"code","41b51bc9":"code","00dd8a8b":"code","0b166aa2":"code","1b6e1747":"code","dab2d163":"code","e4a333cc":"markdown","bade321e":"markdown","352031fd":"markdown","42762519":"markdown","a46cd098":"markdown","93e50aa7":"markdown","eaee4e31":"markdown","bffe6488":"markdown","08e814b6":"markdown","5c097075":"markdown","1107ffd0":"markdown","628a662e":"markdown","96b92d03":"markdown","7c37e81d":"markdown","4d3ed593":"markdown","0045654c":"markdown","e0945f97":"markdown","ecaf0db9":"markdown","7a6e3120":"markdown"},"source":{"4849b9a3":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nfrom scipy.optimize import minimize\n        \n#Load the data and notes. \n#I take the US data from the covidtracking.com website, which has daily updates.\ndata_live = pd.read_csv('http:\/\/covidtracking.com\/api\/states\/daily.csv')\ndata_live['date'] = pd.to_datetime(data_live['date'],format='%Y%m%d')\ndata_live['elapsed'] = (data_live['date'] - data_live['date'].iloc[-1])\/timedelta(days=1)\n#These are notes on data quality, and other relevant info for each state\ninfo = pd.read_csv('https:\/\/covidtracking.com\/api\/states\/info.csv',index_col=0)\n\n#This is the 'training' data from the Kaggle project, which I use for all other countries\ndata_global = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-2\/train.csv')\ndata_global.columns = ['Id','subregion','state','date','positive','death']\ndata_global['date'] = pd.to_datetime(data_global['date'],format='%Y-%m-%d')\ndata_global['elapsed'] = (data_global['date'] - data_live['date'].iloc[-1])\/timedelta(days=1)\ndata_global = data_global.fillna(value='NaN')\n\n#Make lookup table for predictions\nid_list = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-2\/test.csv',index_col=0)\nid_list['Date'] = pd.to_datetime(id_list['Date'],format='%Y-%m-%d')\nid_list['elapsed'] = (id_list['Date'] - data_live['date'].iloc[-1])\/timedelta(days=1)\nid_list = id_list.fillna(value='NaN')\nid_list = id_list.reset_index().set_index(['Country_Region','Province_State','elapsed'])\nid_list = id_list.sort_index()\n\nsubmission = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-2\/submission.csv',index_col=0)\n\n\n#Make script for basic plotting\ndef BasicPlots(data,info=info,metric='positive',regions=['MA']):\n    #Make table of states from data, using the column in 'metric', with date as index\n    table = data.pivot_table(index='date',values=metric,columns='state')\n    #Pick out a few states we are interested in and plot\n    table[regions].plot()\n    plt.gca().set_yscale('log')\n    plt.gca().set_ylabel(metric)\n    plt.show()\n    \n    #Print notes if using US data\n    for region in regions:\n        if region in info.index.values:\n            print(region)\n            print(info['notes'].loc[region])\n            print('------------------')\n\n    #Make another table with purely numeric data in index (instead of datetime) so we can regress\n    table = data.pivot_table(index='elapsed',values=metric,columns='state')\n    plt.plot(np.log10(table.sum(axis=1)).index.values,np.log10(table.sum(axis=1)),'o',label='Data')\n    sns.regplot(np.log10(table.sum(axis=1)).index.values[:-4],np.log10(table.sum(axis=1)).iloc[:-4],label='Regression')\n    plt.legend()\n    plt.gca().set_xlabel('Days Elapsed since March 4')\n    plt.gca().set_ylabel('log10 '+metric)\n    plt.gca().set_title('Total over all regions')\n    plt.show()\n    \n    return table\n\ndef PowerLaw(data,metric='positive',regions=['NY'],subregion=None,\n             start_cutoff=4,start_shift=2,params=[4,0.1],t0=None,ymin=100,xmin=10):\n    end_time = 0\n    if subregion is None:\n        table = data.pivot_table(index='elapsed',values=metric,columns='state',aggfunc=np.sum)\n        for region in regions:\n            if t0 is None:\n                start_time = table.loc[table[region]>=start_cutoff].index.values[0]\n            else:\n                start_time = t0\n                start_shift = 0\n            table.index = table.index.values-start_time+start_shift\n            table[region].plot(marker='o',label=region)\n            end_time = np.max([end_time,np.max(table.index.values)])\n    else:\n        data_region = data.copy()\n        data_region = data_region.loc[data['state']==regions[0]]\n        table = data.pivot_table(index='elapsed',values=metric,columns='subregion',aggfunc=np.sum)\n        if t0 is None:\n            start_time = table.loc[table[subregion]>=start_cutoff].index.values[0]\n        else:\n            start_time = t0\n            start_shift = 0\n        table.index = table.index.values-start_time+start_shift\n        table[subregion].plot(marker='o',label=subregion)\n        end_time = np.max([end_time,np.max(table.index.values)])\n\n    plt.legend()\n    plt.gca().set_xscale('log')\n    plt.gca().set_yscale('log')\n    plt.gca().set_xlabel('Days since beginning of spread')\n    plt.gca().set_ylabel(metric)\n\n    z = params[0]\n    c = params[1]\n    days = np.linspace(1,end_time*1.01,100)\n    plt.plot(days,c*days**z,'k',linewidth='2')\n    plt.gca().set_xlim((xmin,end_time*1.01))\n    plt.gca().set_ylim((ymin,np.nanmax(table.values.reshape(-1))*1.2))\n    plt.show()\n    \n    return table","11d41aad":"table = PowerLaw(data_live,regions=['NY'],params=[4,0.11])\ntable = PowerLaw(data_live,regions=['WA','MA','IL','CA'],params=[4,0.01])","82ea64af":"table = PowerLaw(data_global,regions=['Italy'],params=[3.6,0.22])\ntable = PowerLaw(data_global,regions=['Spain'],params=[4.5,0.009])","28572aea":"def cost_p(params,data):\n    t0,C,z = params\n    prediction = np.log(C)+z*np.log(data.index.values-t0)\n    \n    return 0.5*((np.log(data.values)-prediction)**2).sum()\n\ndef cost_f(params,data,p_params):\n    t0,C,z = p_params\n    Delta,r = params\n    prediction = np.log(r*C)+z*np.log(data.index.values-t0-Delta)\n    \n    return 0.5*((np.log(data.values)-prediction)**2).sum()\n\ndef jac_p(params,data):\n    t0,C,z = params\n    prediction = np.log(C)+z*np.log(data.index.values-t0)\n    \n    return np.asarray([((z\/(data.index.values-t0))*(np.log(data.values)-prediction)).sum(),\n                       -((1\/C)*(np.log(data.values)-prediction)).sum(),\n                      -(np.log(data.index.values-t0)*(np.log(data.values)-prediction)).sum()])\n\ndef jac_f(params,data,p_params):\n    t0,C,z = p_params\n    Delta,r = params\n    prediction = np.log(r*C)+z*np.log(data.index.values-t0-Delta)\n    \n    return np.asarray([((z\/(data.index.values-t0-Delta))*(np.log(data.values)-prediction)).sum(),\n                       -((1\/r)*(np.log(data.values)-prediction)).sum()])","e6d58806":"region = 'Italy'\nstart_cutoff=4\np0=5e2\nz0 = 4\nDelta0 = 5\nr0 = 0.1\nf0 = 50\n\ntable = data_global.pivot_table(index='elapsed',values='positive',columns='state',aggfunc=np.sum)\nt00 = table.loc[table[region]>=start_cutoff].index.values[0]\nC0 = table[region].max()\/(np.max(table.index.values)-t00)**z0\ntrain = table[region].loc[table[region]>p0]\nout = minimize(cost_p,[t00,C0,z0],args=(train,),jac=jac_p,bounds=((None,int(train.index.values[0])-1),(1e-6,None),(0,10)))\nt0,C,z = out.x\ntable = PowerLaw(data_global,metric='positive',regions=[region],params=[z,C],t0=t0,xmin=1)\n\ntable = data_global.pivot_table(index='elapsed',values='death',columns='state',aggfunc=np.sum)\ntrain = table[region].loc[table[region]>f0]\nout = minimize(cost_f,[Delta0,r0],args=(train,[t0,C,z]),jac=jac_f,bounds=((0,12),(1e-6,1)))\nDelta,r = out.x\ntable = PowerLaw(data_global,metric='death',regions=[region],params=[z,C*r],t0=t0+Delta,ymin=1,xmin=1)","7c64f306":"start_cutoff=4 #Initial estimate of t0 is when the number of cases exceeds this number\np0=5e2 #For confirmed cases model, only include times after the number of cases crosses this point\nf0=50 #For fatality model, only include times after the number of fatalities crosses this point\nDelta0 = 5 #Initial estimate of time lag between t0 and first fatality\nr0 = 0.1 #Initial estimate of death rate\n\n#Set up the variables\np_valid = True\nf_valid = True\nfailed_regions = []\np = data_global.pivot_table(index='elapsed',values='positive',columns=['state','subregion'],aggfunc=np.sum)\nf = data_global.pivot_table(index='elapsed',values='death',columns=['state','subregion'],aggfunc=np.sum)\nparams_table = pd.DataFrame(columns=['Country_Region','State_Province','t0','t0_abs','C','z','Delta t','r'])\nparam_id = 0\n\n#Loop through regions\nfor region in set(id_list.reset_index()['Country_Region']):\n    #These regions need different initial conditions for optimizer to converge nicely\n    if region in ['Australia','Japan','Germany','Canada','United Kingdom','France','Iceland']:\n        z0 = 9\n        f0 = 10\n    #These initial conditions work well everywhere else\n    else:\n        z0 = 4.5\n        f0 = 50\n    p_region = p.T.loc[region].T\n    f_region = f.T.loc[region].T\n    \n    #Now loop through the \"subregions\" (states or provinces)\n    for subregion in p_region.keys():\n        p_train = p_region[subregion]\n        f_train = f_region[subregion]\n        \n        #Only use places where the number of cases eventually exceeds twice the minimum threshold, and where there are at least three data points\n        #South Korea and China have already saturated, so I'm going to keep the estimate at the current number of cases\n        #South Africa and Ecuador aren't working, and I haven't tracked down the problem yet.\n        if np.max(p_train)>2*p0 and np.sum(p_train>p0)>3 and region not in ['Japan','Holy See','Diamond Princess','Greenland','Korea, South','China','South Africa','Ecuador','Syria'] and subregion not in ['Missouri','Wisconsin','North Carolina','Quebec','New South Wales']:\n            t00 = p_train.loc[p_train>=start_cutoff].index.values[0]\n            C0 = p_train.max()\/(np.max(p_train.index.values)-t00)**z0\n            p_train = p_train.loc[p_train>p0]\n            out = minimize(cost_p,[t00,C0,z0],args=(p_train,),jac=jac_p,bounds=((None,int(p_train.index.values[0])-1),(1e-6,None),(0,10)))\n            t0,C,z = out.x\n            p_valid = True #out.success\n        else:\n            p_valid = False\n\n        #If the spreading model was successfully trained, now try to learn the fatality rate and time delay\n        if np.max(f_train)>2*f0 and p_valid and np.sum(f_train>f0)>2:\n            f_train = f_train.loc[f_train>f0]\n            out = minimize(cost_f,[Delta0,r0],args=(f_train,[t0,C,z]),jac=jac_f,bounds=((1,12),(1e-6,1)))\n            Delta,r = out.x\n            f_valid = True #out.success\n        else:\n            f_valid = False\n\n        #Save the data\n        if p_valid:\n            params_table.loc[param_id,'Country_Region']=region\n            params_table.loc[param_id,'State_Province']=subregion\n            params_table.loc[param_id,'t0'] = t0\n            params_table.loc[param_id,'t0_abs'] = (timedelta(days=t0)+data_live['date'].iloc[-1]).isoformat()[:10]\n            params_table.loc[param_id,'C'] = C\n            params_table.loc[param_id,'z'] = z\n            if subregion is not 'NaN':\n                table = PowerLaw(data_global,metric='positive',regions=[region],params=[z,C],t0=t0,subregion=subregion,ymin=1,xmin=1)\n            else:\n                table = PowerLaw(data_global,metric='positive',regions=[region],params=[z,C],t0=t0)\n            for t in id_list.loc[region].reset_index()['elapsed']:\n                pred_id = id_list['ForecastId'].loc[region,subregion,t]\n                submission.loc[pred_id,'ConfirmedCases'] = C*((t-t0)**z)\n        else:\n            failed_regions.append([region,subregion])\n            for t in id_list.loc[region].reset_index()['elapsed']:\n                pred_id = id_list['ForecastId'].loc[region,subregion,t]\n                if t in p_train.index.values:\n                    submission.loc[pred_id,'ConfirmedCases'] = p_train.loc[t]\n                else:\n                    submission.loc[pred_id,'ConfirmedCases'] = np.max(p_train)\n                \n        if f_valid:\n            params_table.loc[param_id,'Delta t'] = Delta\n            params_table.loc[param_id,'r'] = r\n            if subregion is not 'NaN':\n                table = PowerLaw(data_global,metric='death',regions=[region],params=[z,C*r],t0=t0+Delta,ymin=1,subregion=subregion)\n            else:\n                table = PowerLaw(data_global,metric='death',regions=[region],params=[z,C*r],t0=t0+Delta,ymin=1)\n            for t in id_list.loc[region].reset_index()['elapsed']:\n                pred_id = id_list['ForecastId'].loc[region,subregion,t]\n                submission.loc[pred_id,'Fatalities'] = r*C*((t-t0-Delta)**z)\n        else:\n            for t in id_list.loc[region].reset_index()['elapsed']:\n                pred_id = id_list['ForecastId'].loc[region,subregion,t]\n                if t in f_train.index.values:\n                    submission.loc[pred_id,'Fatalities'] = f_train.loc[t]\n                else:\n                    submission.loc[pred_id,'Fatalities'] = np.max(f_train)\n                \n        param_id += 1","3741d0be":"submission.to_csv('submission.csv')\nparams_table.to_csv('params.csv',index=False)","5cc91933":"params_table = pd.read_csv('params.csv',index_col=[0,1])\nzlist = params_table['z']\nzlist = zlist.loc[~np.isnan(zlist)]\nplt.hist(zlist)\nplt.show()\n\nrlist = params_table['r']\nrlist = rlist.loc[~np.isnan(rlist)]\nplt.hist(rlist)\nplt.show()\n\nDellist = params_table['Delta t']\nDellist = Dellist.loc[~np.isnan(Dellist)]\nplt.hist(Dellist)\nplt.show()","fe645acf":"params_table = pd.read_csv('params.csv')\nparams_table['State_Province'] = params_table['State_Province'].fillna(value='NaN')\nparams_table = params_table.set_index(['Country_Region','State_Province']).sort_index()\nzlist = params_table['z']\nrlist = params_table['r']\nDellist = params_table['Delta t']\n\nz = zlist.mean()\nr0 = rlist.mean()\nDelta = Dellist.mean()\n\nstart_cutoff=4\np = data_global.pivot_table(index='elapsed',values='positive',columns=['state','subregion'],aggfunc=np.sum)\nf = data_global.pivot_table(index='elapsed',values='death',columns=['state','subregion'],aggfunc=np.sum)\n\n#Make predictions for regions with insufficient data, based on global averages\nfor region in set(id_list.reset_index()['Country_Region']):\n    p_region = p.T.loc[region].T\n    f_region = f.T.loc[region].T\n    \n    #Now loop through the \"subregions\" (states or provinces)\n    for subregion in p_region.keys():\n        p_train = p_region[subregion]\n        f_train = f_region[subregion]\n        z = zlist.mean()\n        \n        #Find all the regions that did not meet our criteria before (and that are not on the excluded list)\n        if not(np.max(p_train)>2*p0 and np.sum(p_train>p0)>3) and region not in ['Japan','Holy See','Diamond Princess','Greenland','Korea, South','China','South Africa','Ecuador','Syria'] and subregion not in ['Missouri','Wisconsin','North Carolina','Quebec','New South Wales']:\n            #Estimate start time if possible\n            if (p_train>start_cutoff).sum()>1:\n                t0 = p_train.loc[p_train>=start_cutoff].index.values[0]\n                C = p_train.max()\/(np.max(p_train.index.values)-t0)**z\n            #If not enough cases to estimate start time, assume infection is contained\n            else:\n                t0 = -80\n                z = 0\n                C = p_train.max()\n            p_valid = False \n        else:\n            p_valid = True\n\n        #Find all the regions that did not meet our criteria before (and that are not on the excluded list)\n        if not(np.max(f_train)>2*f0 and p_valid and np.sum(f_train>f0)>2) and region not in ['Japan','Holy See','Diamond Princess','Greenland','Korea, South','China','South Africa','Ecuador','Syria'] and subregion not in ['Missouri','Wisconsin','North Carolina','Quebec','New South Wales']:\n            #If there is spread and there are fatalities, estimate fatality rate from data\n            if (region, subregion) in params_table.index.tolist():\n                z = float(params_table.loc[[region,subregion],'z'])\n                t0 = float(params_table.loc[[region,subregion],'t0'])\n                C = float(params_table.loc[[region,subregion],'C'])\n                t0_abs = (timedelta(days=t0)+data_live['date'].iloc[-1]).isoformat()\n            if f_train.max() > 1 and z>0 and np.max(f_train.index.values)-t0-Delta > 0:\n                r = f_train.max()\/(C*(np.max(f_train.index.values)-t0-Delta)**z)\n            else:\n                r = r0\n            f_valid = False \n        else:\n            f_valid = True\n\n        #Save the data and plot\n        if not p_valid:\n            t0_abs = (timedelta(days=t0)+data_live['date'].iloc[-1]).isoformat()\n            new_params = pd.DataFrame(np.asarray([t0,t0_abs,C,z,np.nan,np.nan])[np.newaxis,:],index=pd.MultiIndex.from_tuples([(region,subregion)]),columns=['t0','t0_abs','C','z','Delta t','r'])\n            params_table = params_table.append(new_params)\n            if subregion is not 'NaN':\n                table = PowerLaw(data_global,metric='positive',regions=[region],params=[z,C],t0=t0,subregion=subregion,ymin=1,xmin=1)\n            else:\n                table = PowerLaw(data_global,metric='positive',regions=[region],params=[z,C],t0=t0,ymin=1,xmin=1)\n            for t in id_list.loc[region].reset_index()['elapsed']:\n                pred_id = id_list['ForecastId'].loc[region,subregion,t]\n                submission.loc[pred_id,'ConfirmedCases'] = C*((t-t0)**z)\n                \n        if not f_valid:\n            params_table.loc[region,subregion] = np.asarray([t0,t0_abs,C,z,Delta,r])\n            if subregion is not 'NaN':\n                table = PowerLaw(data_global,metric='death',regions=[region],params=[z,C*r],t0=t0+Delta,subregion=subregion,ymin=1,xmin=1)\n            else:\n                table = PowerLaw(data_global,metric='death',regions=[region],params=[z,C*r],t0=t0+Delta,ymin=1,xmin=1)\n            for t in id_list.loc[region].reset_index()['elapsed']:\n                pred_id = id_list['ForecastId'].loc[region,subregion,t]\n                submission.loc[pred_id,'Fatalities'] = r*C*((t-t0-Delta)**z)","a3abdaf5":"submission.to_csv('submission.csv')\nparams_table.to_csv('params.csv',index=False)","96105627":"params_table=pd.read_csv('params.csv')\nregion = 'US'\nparams_region = params_table.set_index(['Country_Region','State_Province']).loc[region]\nparams_region['t0'] = pd.to_datetime(params_region['t0'],format='%Y-%m-%d')\nDelta_region = timedelta(days=params_region['Delta t'].mean())\nr_region = params_region['r'].mean()\nparams_region['Delta t'] = params_region['Delta t'].fillna(value=0)\nfor item in params_region.index:\n    params_region.loc[item,'Delta t'] = timedelta(days=params_region.loc[item,'Delta t'])\n\ndates = [datetime.today()+timedelta(days=k) for k in range(40)]\ncase_predictions = pd.DataFrame(index=dates,columns=params_region.reset_index()['State_Province'])\ndeath_predictions = pd.DataFrame(index=dates,columns=params_region.reset_index()['State_Province'])\nfor subregion in case_predictions.keys():\n    params = params_region.loc[subregion]\n    for t in case_predictions.index:\n        case_predictions.loc[t,subregion] = params['C']*((t-params['t0'])\/timedelta(days=1))**params['z']\n        if params['r'] is not np.nan:\n            death_predictions.loc[t] = params['r']*params['C']*((t-params['t0']-params['Delta t'])\/timedelta(days=1))**params['z']\n        else:\n            death_predictions.loc[t] = r_region*params['C']*((t-params['t0']-Delta_region)\/timedelta(days=1))**params['z']\ncase_predictions = case_predictions.fillna(value=0)\ndeath_predictions = death_predictions.fillna(value=0)\n\ncase_predictions.sum(axis=1).plot(label='Cases')\ndeath_predictions.sum(axis=1).plot(label='Fatalities')\nplt.gca().set_yscale('log')\nplt.gca().set_title('US Total Predictions')\nplt.legend()\nplt.show()","41b51bc9":"table = BasicPlots(data_live,metric='positive',regions=['MA','WI','MI','TX','IL'])","00dd8a8b":"table = BasicPlots(data_global,metric='positive',regions=['Italy','Spain','France'])","0b166aa2":"table = BasicPlots(data_live,metric='hospitalized',regions=['MA','NY'])","1b6e1747":"table = BasicPlots(data_live,metric='death',regions=['MA','WI','MI','TX','IL'])","dab2d163":"states=['MA','WI','NY','CA']\ntable = BasicPlots(data_live,metric='totalTestResults',regions=states)\n\ntable1 = data_live.pivot_table(index='date',values='totalTestResults',columns='state')\ntable2 = data_live.pivot_table(index='date',values='positive',columns='state')\ntable = table2\/table1\ntable[states].plot()\nplt.gca().set_ylabel('Positive fraction')\nplt.show()\n\n(table2.sum(axis=1)\/table1.sum(axis=1)).plot(marker='o')\nplt.gca().set_ylabel('Positive fraction')\nplt.gca().set_ylim((0,.2))\nplt.show()","e4a333cc":"## Fatalities","bade321e":"In the US, the exponent seems to be about 4, and is the same at late times for many states.","352031fd":"## Testing","42762519":"# Prediction","a46cd098":"### US","93e50aa7":"In Europe, Italy has an exponent of about 3.6, while Spain has 4.5. Note that Italy's curve appears to be flattening in these last few days, departing from the power law.","eaee4e31":"In this notebook, I provide preliminary evidence that the spread of COVID-19 at late times (once self-averaging comesinto play) follows a power law. Specifically:\n\nnumber of cases $\\approx$ C x (time elapsed since first infection)$^z$\n\nwhere C is a constant that depends on the size of the region (among other things?), and $z$ ranges from around 2 to 9, depending strongly on latitude.\n\nThe notebook also facilitates plotting of hospitalization, deaths, and total amount of testing for US states.","bffe6488":"# Summary plots","08e814b6":"Test on Italy","5c097075":"## Hospitalization","1107ffd0":"# Exploration","628a662e":"# Imports and data loading","96b92d03":"## Cases","7c37e81d":"Now try on everything","4d3ed593":"To make predictions based on this model, we have to fit three parameters: the time of origin $t_0$, the exponent $z$, and the coefficient $C$. We also need to predict the fatalities. \n\nLet $p$ be the number of positive tests, and $f$ the number of fatalities. Let $\\Delta t$ be the delay between infection and death, and $r$ the death rate. Then our model is:\n\n\\begin{align}\np &= C (t-t_0)^z\\\\\nf &= rC (t-t_0-\\Delta t)^z.\n\\end{align}\n\nBecause we are dealing with a self-replicating entity, noise is multiplicative, so we should define the cost function in terms of the difference between the log of the data and the log of the prediction. We expect the model to perform best when the number of infections is high enough to self-average, so we will set a cutoff $p_0$ and only keep data with $p>p_0$ for training. If the number of infections in a country has not yet reached $p_0$, we decline to make a prediction, and instead just fill the corresponding entries in the submission spreadsheet with the last observed value. Finally, since the number of fatalities in most countries is still low and thus very noisy, we want to fit the model for the total number of cases first, and keep the same exponent for fitting the fatalities.","0045654c":"# Make predictions for recently infected countries","e0945f97":"# Plot Predictions","ecaf0db9":"### World","7a6e3120":"Now that we have the exponent for a lot of countries, we can estimate how the epidemic is going to unfold in new countries with only a few cases by using the average exponent and a rough estimate of the (effective) time of the first case."}}