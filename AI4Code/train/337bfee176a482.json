{"cell_type":{"6761b31a":"code","0e2581ca":"code","c2f3a74f":"code","811b498f":"code","b6fae84d":"code","1f9aadeb":"code","45188816":"code","b2f388cf":"code","d4ef96ee":"code","63d98904":"code","643385c7":"code","46c10d57":"code","7b806329":"code","f9249103":"code","68f5d99e":"markdown","538c0dca":"markdown","e740f3a9":"markdown","dca5a2a9":"markdown","6c54c11c":"markdown","4d5b9f55":"markdown","1178f205":"markdown","406623b7":"markdown","eef5b21a":"markdown","1bb2bb70":"markdown","f426760d":"markdown","192b617e":"markdown","d2555936":"markdown"},"source":{"6761b31a":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom IPython.display import display\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split,GridSearchCV, StratifiedKFold,cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nimport warnings\n\nwarnings.filterwarnings('ignore')","0e2581ca":"#import data into pandas dataframe\ndata = pd.read_csv('..\/input\/student-mat.csv')\n\n#display first 5 lines\ndisplay(data.head())\n\n#print data properties\nprint('Data Shape: {}'.format(data.shape))\ndisplay(data.describe())\ndisplay(data.info())","c2f3a74f":"plt.figure()\nsns.countplot(data['G1'],label='Count')\nplt.figure()\nsns.countplot(data['G2'],label=\"Count\")\nplt.figure()\nsns.countplot(data['G3'],label=\"Count\")","811b498f":"#Feature correlations\nplt.figure(figsize=(15,10))\ncorr_mat=sns.heatmap(data.corr(),annot=True,cbar=True,\n            cmap='viridis', vmax=1,vmin=-1)\ncorr_mat.set_xticklabels(corr_mat.get_xticklabels(),rotation=90)","b6fae84d":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import FeatureUnion\n\nclass FeatureSelector(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, feature_names):\n        self.feature_names = feature_names\n        \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        return X[self.feature_names]\n    \n\nclass CategoricalTranformer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, ):\n        self.binary_features = ['school','sex','address','famsize',\n                                   'Pstatus','schoolsup','famsup','paid',\n                                   'activities','nursery','higher','internet',\n                                   'romantic','G1','G2']\n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        \n        le = LabelEncoder()\n        \n        for col in np.intersect1d(self.binary_features,X.columns):\n            le.fit(X[col])\n            X[col] = le.transform(X[col])\n\n        X = pd.get_dummies(X)\n        \n        return X       ","1f9aadeb":"X = data.iloc[:,0:32]\ny = data['G3']\n\nbins = (-1, 9.5, 21)\nperformance_level = ['fail', 'pass']\nX['G1'] = pd.cut(X['G1'], bins = bins, labels = performance_level)\nX['G2'] = pd.cut(X['G2'], bins = bins, labels = performance_level)\ny = pd.cut(y, bins = bins, labels = performance_level)\n\nplt.figure()\nsns.countplot(X['G1'],label=\"Count\")\nplt.figure()\nsns.countplot(X['G2'],label=\"Count\")\nplt.figure()\nsns.countplot(y,label=\"Count\")\n\n#encode target\nle_target = LabelEncoder()\nle_target.fit(y)\ny = le_target.transform(y)","45188816":"X_train,X_test,y_train,y_test = train_test_split(X, y, random_state=0,stratify=y)\n\ncategorical_features = X.select_dtypes(exclude=['int64']).columns\nnumerical_features = X.select_dtypes(include=['int64']).columns\n\n\ncategorical_preproc = Pipeline(steps = [('cat_selector', FeatureSelector(categorical_features)),\n                                         ('cat_transformer', CategoricalTranformer())])\n    \nnumerical_preproc = Pipeline(steps = [('num_selector', FeatureSelector(numerical_features)),         \n                                       ('scaler', StandardScaler())])\n\nfull_preproc = FeatureUnion(transformer_list = [('categorical_pipeline', categorical_preproc),\n                                                 ('numerical_pipeline', numerical_preproc)])\n# Temporary variable to extract column names\nX_temp = categorical_preproc.transform(X)\ncol = X_temp.columns.tolist()+numerical_features.tolist()","b2f388cf":"kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\nparam_grid = {'model__C': [0.001, 0.01, 0.1, 1, 10],\n              'model__gamma': [0.001, 0.01, 0.1, 1, 10],\n              'model__kernel': ['linear', 'rbf']}\n\npipe = Pipeline(steps = [('proproc', full_preproc),\n                         ('model', SVC(random_state=0))])\n                \ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=kfold, scoring='accuracy', n_jobs=-1)\ngrid.fit(X_train, y_train)\n                \nprint(\"Best cross-validation accuracy: {:.3f}\".format(grid.best_score_))\nprint(\"Test set score: {:.3f}\".format(grid.score(X_test,y_test)))\nprint(\"Best parameters: {}\".format(grid.best_params_))\n\nconf_mat = confusion_matrix(y_test, grid.predict(X_test))\nsns.heatmap(conf_mat, annot=True, cbar=False, cmap=\"viridis_r\",\n            yticklabels=le_target.classes_, xticklabels=le_target.classes_)\n\nsvm = SVC(random_state=0, C = grid.best_params_['model__C'],\n          gamma = grid.best_params_['model__gamma'],\n          kernel = grid.best_params_['model__kernel'],\n          probability=True)","d4ef96ee":"X = data.iloc[:,0:32]\ny = data['G3']\n\nbins = (-1, 9.5, 16.5, 21)\nperformance_level = ['fail', 'average', 'good']\nX['G1'] = pd.cut(X['G1'], bins = bins, labels = performance_level)\nX['G2'] = pd.cut(X['G2'], bins = bins, labels = performance_level)\ny = pd.cut(y, bins = bins, labels = performance_level)\n\nplt.figure()\nsns.countplot(X['G1'],label=\"Count\")\nplt.figure()\nsns.countplot(X['G2'],label=\"Count\")\nplt.figure()\nsns.countplot(y,label=\"Count\")\n\n#encode target\nle_target = LabelEncoder()\nle_target.fit(y)\ny = le_target.transform(y)\n\nX_train,X_test,y_train,y_test = train_test_split(X, y, random_state=0,stratify=y)\n\ncategorical_features = X.select_dtypes(exclude=['int64']).columns\nnumerical_features = X.select_dtypes(include=['int64']).columns\n\n\ncategorical_preproc = Pipeline(steps = [('cat_selector', FeatureSelector(categorical_features)),\n                                         ('cat_transformer', CategoricalTranformer())])\n    \nnumerical_preproc = Pipeline(steps = [('num_selector', FeatureSelector(numerical_features)),         \n                                       ('scaler', StandardScaler())])\n\nfull_preproc = FeatureUnion(transformer_list = [('categorical_pipeline', categorical_preproc),\n                                                 ('numerical_pipeline', numerical_preproc)])\n\n\nX_temp = categorical_preproc.transform(X)\ncol = X_temp.columns.tolist()+numerical_features.tolist()","63d98904":"kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\nparam_grid = {'model__C': [0.001, 0.01, 0.1, 1, 10],\n              'model__gamma': [0.001, 0.01, 0.1, 1, 10],\n              'model__kernel': ['linear', 'rbf']}\n\npipe = Pipeline(steps = [('proproc', full_preproc),\n                         ('model', SVC(random_state=0))])\n                \ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=kfold, scoring='accuracy', n_jobs=-1)\ngrid.fit(X_train, y_train)\n                \nprint(\"Best cross-validation accuracy: {:.3f}\".format(grid.best_score_))\nprint(\"Test set score: {:.3f}\".format(grid.score(X_test,y_test)))\nprint(\"Best parameters: {}\".format(grid.best_params_))\n\nconf_mat = confusion_matrix(y_test, grid.predict(X_test))\nsns.heatmap(conf_mat, annot=True, cbar=False, cmap=\"viridis_r\",\n            yticklabels=le_target.classes_, xticklabels=le_target.classes_)\n\nsvm = SVC(random_state=0, C = grid.best_params_['model__C'],\n          gamma = grid.best_params_['model__gamma'],\n          kernel = grid.best_params_['model__kernel'],\n          probability=True)","643385c7":"X = data.iloc[:,0:30]\ny = data['G3']\n\nbins = (-1, 9.5, 21)\nperformance_level = ['fail', 'pass']\ny = pd.cut(y, bins = bins, labels = performance_level)\n\n#encode target  and preprocessing\nle = LabelEncoder()\nle_target.fit(y)\ny = le_target.transform(y)\n\nX_train,X_test,y_train,y_test = train_test_split(X, y, random_state=0,stratify=y)\n\ncategorical_features = X.select_dtypes(exclude=['int64']).columns\nnumerical_features = X.select_dtypes(include=['int64']).columns\ncategorical_preproc = Pipeline(steps = [('cat_selector', FeatureSelector(categorical_features)),\n                                         ('cat_transformer', CategoricalTranformer())])\n    \nnumerical_preproc = Pipeline(steps = [('num_selector', FeatureSelector(numerical_features)),         \n                                       ('scaler', StandardScaler())])\n\nfull_preproc = FeatureUnion(transformer_list = [('categorical_pipeline', categorical_preproc),\n                                                 ('numerical_pipeline', numerical_preproc)])\n# Temporary variable to extract column names\nX_temp = categorical_preproc.transform(X)\ncol = X_temp.columns.tolist()+numerical_features.tolist()","46c10d57":"kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\nparam_grid = {'model__C': [0.001, 0.01, 0.1, 1, 10],\n              'model__gamma': [0.001, 0.01, 0.1, 1, 10],\n              'model__kernel': ['linear', 'rbf']}\n\npipe = Pipeline(steps = [('proproc', full_preproc),\n                         ('model', SVC(random_state=0))])\n                \ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=kfold, scoring='accuracy', n_jobs=-1)\ngrid.fit(X_train, y_train)\n                \nprint(\"Best cross-validation accuracy: {:.3f}\".format(grid.best_score_))\nprint(\"Test set score: {:.3f}\".format(grid.score(X_test,y_test)))\nprint(\"Best parameters: {}\".format(grid.best_params_))\n\nconf_mat = confusion_matrix(y_test, grid.predict(X_test))\nsns.heatmap(conf_mat, annot=True, cbar=False, cmap=\"viridis_r\",\n            yticklabels=le_target.classes_, xticklabels=le_target.classes_)\n\nsvm = SVC(random_state=0, C = grid.best_params_['model__C'],\n          gamma = grid.best_params_['model__gamma'],\n          kernel = grid.best_params_['model__kernel'],\n          probability=True)","7b806329":"X = data.iloc[:,0:30]\ny = data['G3']\n\nbins = (-1, 9.5, 16.5, 21)\nperformance_level = ['fail', 'average', 'good']\ny = pd.cut(y, bins = bins, labels = performance_level)\n\n#encode target\nle_target = LabelEncoder()\nle_target.fit(y)\ny = le_target.transform(y)\n\nX_train,X_test,y_train,y_test = train_test_split(X, y, random_state=0,stratify=y)\n\ncategorical_features = X.select_dtypes(exclude=['int64']).columns\nnumerical_features = X.select_dtypes(include=['int64']).columns\n\n\ncategorical_preproc = Pipeline(steps = [('cat_selector', FeatureSelector(categorical_features)),\n                                         ('cat_transformer', CategoricalTranformer())])\n    \nnumerical_preproc = Pipeline(steps = [('num_selector', FeatureSelector(numerical_features)),         \n                                       ('scaler', StandardScaler())])\n\nfull_preproc = FeatureUnion(transformer_list = [('categorical_pipeline', categorical_preproc),\n                                                 ('numerical_pipeline', numerical_preproc)])\n\n\nX_temp = categorical_preproc.transform(X)\ncol = X_temp.columns.tolist()+numerical_features.tolist()","f9249103":"kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\nparam_grid = {'model__C': [0.001, 0.01, 0.1, 1, 10],\n              'model__gamma': [0.001, 0.01, 0.1, 1, 10],\n              'model__kernel': ['linear', 'rbf']}\n\npipe = Pipeline(steps = [('proproc', full_preproc),\n                         ('model', SVC(random_state=0))])\n                \ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=kfold, scoring='accuracy', n_jobs=-1)\ngrid.fit(X_train, y_train)\n                \nprint(\"Best cross-validation accuracy: {:.3f}\".format(grid.best_score_))\nprint(\"Test set score: {:.3f}\".format(grid.score(X_test,y_test)))\nprint(\"Best parameters: {}\".format(grid.best_params_))\n\nconf_mat = confusion_matrix(y_test, grid.predict(X_test))\nsns.heatmap(conf_mat, annot=True, cbar=False, cmap=\"viridis_r\",\n            yticklabels=le_target.classes_, xticklabels=le_target.classes_)\n\nsvm = SVC(random_state=0, C = grid.best_params_['model__C'],\n          gamma = grid.best_params_['model__gamma'],\n          kernel = grid.best_params_['model__kernel'],\n          probability=True)","68f5d99e":"## Correlation Matrix","538c0dca":"## Summary\n\nAs expected, the high correlation between G1, G2 & G3 played a big role in increasing the model's performance when taken into consideration. As well, the model performed better with binary classification than multi-class classification. This may be attributed to the low number of targets labeled 'good' in the dataset when considering multi-class classification.","e740f3a9":"Train and predict using SVC model","dca5a2a9":"The above histograms indicate a well balanced grade distribution. ","6c54c11c":"Split data into training and test sets and prepare pipeline:","4d5b9f55":"Taking a first look at the data","1178f205":"## 1.2 Multi-Class Classification","406623b7":"Notice how the G1, G2 (midterm grades) and G3 (Final grade) curves are quite similiar and very highly correlated. This is tells us that students who perform well year long are also likey to perform similairly on the final.\n\nSome correlations to note:\n-  Mother education and G1,G2,G3\n-  Father education and mother education\n-  Weekend alcohol consumption and going out\n-  Weekend alcohol consumption and weekday alcohol consumption\n\nBefore training our models, we will need to preprocess the data. Each sample contains both numerical and categorical features. Each would need to be processed differently. Numerical features will need to be scaled. Binary categorical features will need to be encoded into 0 or 1 while multi-category features will need to be one-hot encoded.\n\nTo be able to perform proper cross validation and grid search later, we'll need to create a cusotm transformer to handle the above preprocessing step and feed it into a pipeline.","eef5b21a":"# 2. Classification without G1 & G2\n\n## 2.1 Binary Classification","1bb2bb70":"# Predicting Student Success\n\nIn this notebook, we attempt to predict the final grade of students using different classification criteria.\n-  Binary classification (pass\/fail)\n-  Multi-class classification (fail\/average\/good)\n\nThis notebook was inspired by the following paper published by Paulo Cortez and Alice Silva from the University of Minho:\n[Usind Data Mining to Predict Secondary School Student Performance](http:\/\/www3.dsi.uminho.pt\/pcortez\/student.pdf)\n\nComments and criticism welcome.\n\n**Please upvote if you found this notebook useful :)**","f426760d":"## 2.2 Multi-Class Classification","192b617e":"# 1. Classification using all Features\n## 1.1 Binary Classificaton","d2555936":"We see that we have no NaN or missing values."}}