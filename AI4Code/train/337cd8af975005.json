{"cell_type":{"bcd8b5a5":"code","f09de665":"code","d3b747ec":"code","8df965bd":"code","8eef615e":"code","a22b24d6":"code","10854498":"code","7484f95d":"code","3dcd0028":"code","f3cb0762":"code","dd2f9273":"code","c8f0683f":"code","8ca895ec":"code","f32de824":"code","3639c2d2":"code","ee935dea":"code","56598746":"code","d278efee":"code","afa61629":"code","07d4a5b7":"code","51ff0261":"code","a90956a6":"code","41107b61":"code","b01d99fe":"code","66b4fea4":"code","ce8ae36c":"code","99a6ee5c":"code","a184019e":"code","e81188aa":"code","109d65d3":"code","2260cc3a":"code","c40b77f5":"code","6f390727":"code","d6788496":"code","969335a9":"code","a0f05d79":"markdown","6442685a":"markdown","6220bd8d":"markdown"},"source":{"bcd8b5a5":"#Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport keras","f09de665":"train = pd.read_csv('..\/input\/train.csv')","d3b747ec":"train.head()","8df965bd":"test = pd.read_csv('..\/input\/test.csv')","8eef615e":"train.shape, test.shape","a22b24d6":"#Randomly shuffling the training set (incase consecutive digits are same)\nindices = np.arange(len(train))\nnp.random.shuffle(indices)\ntrainShuffled = train.loc[indices,:]","10854498":"#A visual plot of the images\nfor i in range(25):\n  plt.subplot(5,5,i+1)\n  plt.imshow(np.array(trainShuffled.iloc[i,1:]).reshape(28,28), cmap = 'gray')\n  plt.axis('off')","7484f95d":"\n#Creating Keras Model, consiting of Conv2d + Pooling blocks followed by one Dense layer with 10 hidden units\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Conv2D(32, (3,3), input_shape = (28,28,1,), activation = 'relu', name = 'Conv2D1'))\nmodel.add(keras.layers.MaxPooling2D((2,2)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Conv2D(64, (3,3), activation = 'relu', name = 'Conv2D2'))\nmodel.add(keras.layers.MaxPooling2D(2,2))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Conv2D(128, (2,2),activation = 'relu', name = 'Conv2D3'))\nmodel.add(keras.layers.Dropout(0.5))\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(10, activation = 'softmax'))","3dcd0028":"model.summary()","f3cb0762":"from keras.preprocessing.image import ImageDataGenerator","dd2f9273":"generator = ImageDataGenerator(rescale = 1.\/255, rotation_range = 15, width_shift_range = 0.1, height_shift_range = 0.1,\n                              shear_range = 0.1, zoom_range = 0.1)","c8f0683f":"#Validation set and train set split\ntrain_X = trainShuffled.iloc[:35000, 1:]\ntrain_y = trainShuffled.iloc[:35000, 0]\ntest_X = trainShuffled.iloc[35000:, 1:]\ntest_y = trainShuffled.iloc[35000:, 0]\ntrain_X.shape, train_y.shape, test_X.shape, test_y.shape","8ca895ec":"train_X = np.array(train_X).reshape(-1,28,28,1) #Networks expects a 4D input\n                                        #(samples, height ,width, channel), since not RGB channel = 1\ntest_X = np.array(test_X).reshape(-1,28,28,1)","f32de824":"train_X.shape, test_X.shape","3639c2d2":"model.compile(optimizer = keras.optimizers.Adam(.001), loss = 'sparse_categorical_crossentropy',\n              metrics = ['accuracy']) #Our y_train is numerical, so instead of categorical_crossentropy\n                                      #we use sparse_categorical_cross_entropy, and a learning rate 0.001","ee935dea":"myDataGen = generator.flow(train_X,train_y, batch_size = 100) #We get data augmented images","56598746":"test_X = test_X\/255. #Also normalize test data, since we did it with train data","d278efee":"hist  = model.fit_generator(myDataGen, steps_per_epoch = 350, epochs = 40, validation_data = (test_X, test_y))","afa61629":"hist.history.keys()","07d4a5b7":"val_loss = np.array(hist.history['val_loss'])\nval_acc = np.array(hist.history['val_acc'])\ntrain_loss = np.array(hist.history['loss'])\ntrain_acc = np.array(hist.history['acc'])\nepochs = len(train_acc)","51ff0261":"accuracies = pd.DataFrame(train_acc, columns = ['train_acc'])\naccuracies['val_acc'] = pd.DataFrame(val_acc)\nlosses = pd.DataFrame(train_loss, columns = ['train_loss'])\nlosses['val_loss'] = pd.DataFrame(val_loss)\nlosses['epochs'] = pd.DataFrame(list(range(epochs)))\naccuracies['epochs'] = pd.DataFrame(list(range(epochs)))","a90956a6":"accuracies.shape","41107b61":"\nplt.scatter(x = 'epochs', y = 'train_acc', data = accuracies, marker = 'x', label = 'train_acc')\nplt.scatter(x = 'epochs', y = 'val_acc', s = 5,data = accuracies, label = 'val_acc', color = 'r')\nplt.plot(accuracies.iloc[:,0:2])\nplt.legend()\nplt.show()\n","b01d99fe":"\nplt.scatter(x = 'epochs', y = 'train_loss', data = losses, marker = 'x', label = 'train_loss')\nplt.scatter(x = 'epochs', y = 'val_loss', data = losses, s = 5,label = 'val_loss', color = 'r')\nplt.plot(losses.iloc[:,0:2])\nplt.legend()\nplt.show()\n","66b4fea4":"test = np.array(test)\/255. #Need also to normalize data that we need to predict\ntest = test.reshape(-1,28,28,1)","ce8ae36c":"#Combine train and validation set for final model training, from scratch\ntotalData_X = np.concatenate((train_X, test_X))\ntotalData_y = np.concatenate((train_y, test_y))\nmyDataGen = generator.flow(totalData_X,totalData_y, batch_size = 100)","99a6ee5c":"#Redefining model\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Conv2D(32, (3,3), input_shape = (28,28,1,), activation = 'relu', name = 'Conv2D1'))\nmodel.add(keras.layers.MaxPooling2D((2,2)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Conv2D(64, (3,3), activation = 'relu', name = 'Conv2D2'))\nmodel.add(keras.layers.MaxPooling2D(2,2))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Conv2D(128, (2,2),activation = 'relu', name = 'Conv2D3'))\nmodel.add(keras.layers.Dropout(0.5))\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(10, activation = 'softmax'))","a184019e":"model.compile(optimizer = keras.optimizers.Adam(.001), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])","e81188aa":"hist  = model.fit_generator(myDataGen, steps_per_epoch = 450, epochs = 40)","109d65d3":"preds = model.predict(test)","2260cc3a":"predict = pd.DataFrame([preds[x].argmax() for x in range(len(preds))], columns = ['Label'])\npredict.head(2)\n","c40b77f5":"sample = pd.read_csv('..\/input\/sample_submission.csv')","6f390727":"predict['ImageId'] = sample['ImageId']\npredict = predict[['ImageId','Label']]","d6788496":"predict.head()","969335a9":"predict.to_csv('predictionsMy.csv', index = False)","a0f05d79":"**Data Augmentation**\nIt is another method like Dropout, to prevent overfitting. One way to deal with overfitting is more data. This is not always possible, so what we do here is we create new data from the given data. How? We have images, so we randomly rotate the images by some degrees (not too extreme) and we shear it, or scale the height and width. We can also zoom it. There are many more parameters that can be experiemented with, like flipping the image (which I found not to be useful), but there are more.","6442685a":"**Convolution Neural Network**\nIt has the following architecture:\nConv2D(32) -> MaxPool -> BN -> Conv2D(64) -> MaxPool -> BN -> Conv2D(128) -> Dropout(50%) -> Dense 10 (output layer)\n\n**Batch Normalization:** it improves the convergence rate in optimization problem, when we are trying to minimize our loss. It has same idea like when we divide the features (input) by 255 (because images are from 0 to 255) or when we use:\nx = (x-mean)\/standard deviation\nSo, doing this for all hidden units, sometimes is benefitial.\n\n**Dropout:** it involves randomly turning off certain neurons in the hidden layer it is specified for (0.5 means 50% after every batch, because we are using mini-batch gradient descent). This prevents overfitting, as the network would not rely on certain neurons always to get the class.","6220bd8d":"**Validation or Hold-out Set**\nTo know if our model is overfitting or not, and to have a rough idea about the number of epochs it would require to get our desired result, we use a validation data that is seaparate from the train data and the model hasn't seen this data before.\nIt is somewhat equivalent to test data, but since we sometimes tune the hyperparameters of our network based on results obtained on validation data, we are optimizing our model for validation data and it might not perform that well on test data, but comparison should be somewhat similar."}}