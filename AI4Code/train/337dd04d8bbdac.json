{"cell_type":{"f84d7a35":"code","18ac885d":"code","6d82f031":"code","4e64a505":"code","c8e3478e":"code","01836aff":"code","d5e81321":"code","b1dd4a79":"code","da333c3c":"code","5178bc86":"code","685d8f6a":"code","8e79c8a6":"code","50ae34ac":"code","b31d3165":"code","8f469297":"code","3a35c2dd":"code","f4801b73":"code","3e1a0d8d":"code","d79d75d2":"code","9048c3f7":"code","1b1fbce5":"code","a35b779e":"code","c31999ff":"code","9c9f26f2":"code","f518f460":"code","d0647143":"code","0a695a49":"code","001207f1":"code","12d464b0":"code","23b75719":"code","47f1ed60":"code","cc40d5b1":"code","d55b1262":"code","09e20de4":"code","2b934d7c":"code","1b100e7f":"markdown","8f77d85e":"markdown","c5f51779":"markdown","bf79188a":"markdown","36dc8173":"markdown","dd0a1425":"markdown","2e2ca529":"markdown","53148ef4":"markdown"},"source":{"f84d7a35":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport warnings\nfrom sklearn.exceptions import DataConversionWarning\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)    \n    \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n#!pip install sweetviz\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","18ac885d":"training_og = pd.read_csv('\/kaggle\/input\/ghouls-goblins-and-ghosts-boo\/train.csv.zip')\ntest_og = pd.read_csv('\/kaggle\/input\/ghouls-goblins-and-ghosts-boo\/test.csv.zip')\nsample = pd.read_csv('\/kaggle\/input\/ghouls-goblins-and-ghosts-boo\/sample_submission.csv.zip')\n\ntraining_og['train_test'] = 1 # ---> to later on be able to identify the two sets, and split them from the all_data set\ntest_og['train_test'] = 0 # ---> to later on be able to identify the two sets, and split them from the all_data set\nall_data = pd.concat([training_og, test_og])\n\n%matplotlib inline\nall_data.columns","6d82f031":"#import sweetviz as sv\n#data_report = sv.analyze(training)\n#data_report.show_html('test.html')\n#from IPython.display import IFrame\n#IFrame(src = 'test.html',width=1000,height=600)","4e64a505":"num_cols = ['bone_length', 'rotting_flesh', 'hair_length', 'has_soul']\ncat_cols = ['color', 'type']","c8e3478e":"# distributions for all numeric variables \nfor i in num_cols:\n    plt.hist(training_og[i])\n    plt.title(i)\n    plt.show()","01836aff":"# corrplot numeric features\nprint(training_og[num_cols].corr())\nsns.heatmap(training_og[num_cols].corr())","d5e81321":"# Comparing target and each of the categorical variables \n\nprint(pd.pivot_table(training_og, index = 'type', columns = 'color', aggfunc ='count'))","b1dd4a79":"# Let's drop the 'id' column from training and test. Since this might misguide the prediction model (it does!)\ntraining = training_og.drop('id', axis=1)\ntest = test_og.drop('id', axis=1)","da333c3c":"# log transform numerical columns\nimport numpy as np \n\nlog_training_num_cols = np.log(training[num_cols])\nprint(log_training_num_cols.hist())\n\nlog_test_num_cols = np.log(test[num_cols])\n#print(log_test_num_cols.hist().hist()) -> error message for some reason","5178bc86":"# create a training set with log transformed numeric values\nlog_training = training.copy()\nfor i in num_cols:\n    log_training[i] = log_training_num_cols[i].values\n    \n# create a test set with log transformed numeric values\nlog_test = test.copy()\nfor i in num_cols:\n    log_test[i] = log_test_num_cols[i].values","685d8f6a":"# TRAINING DF\n# instantiate target and transform to numerical \ny = log_training.type\n\nlabel_enc = LabelEncoder()\nlabel_enc.fit(y)\ny = label_enc.transform(y)\n\n# drop the unnecessary columns from log transformed data\nlog_training.pop('train_test')\nlog_training.pop('type')\n\n# drop the unnecessary columns from untouched data\ntraining.pop('train_test')\ntraining.pop('type')\n\n# rename log X-set\nX_log = log_training\n\n# rename untouched X-set\nX = training","8e79c8a6":"# TEST DF\n# drop the unnecessary columns from log transformed data\nlog_test.pop('train_test')\n\n# drop the unnecessary columns from untouched data\ntest.pop('train_test')\n","50ae34ac":"from sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nimport xgboost as xgb","b31d3165":"# create pipeline with OH encoder, scaler and model\n\noh = OneHotEncoder(handle_unknown='ignore')\nsc = StandardScaler()\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('oh', oh, ['color']),\n        ('scal', sc, ['bone_length', 'rotting_flesh', 'hair_length', 'has_soul'])\n    ], remainder='passthrough')","8f469297":"# Get cross val score per model automatically (non-logged datasets)\n\nsvc = SVC()\nrf = RandomForestClassifier()\nknn = KNeighborsClassifier()\nnb = GaussianNB()\nxgb = xgb.XGBClassifier()\n\nmodels = [svc, rf, knn, nb, xgb]\nscores = {}","3a35c2dd":"# Get cross val score per model automatically (non-logged datasets)\nfor i in models:\n    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', i)\n                             ])\n    model_score = cross_val_score(pipe, X, y, cv=10).mean()\n    scores[i] = model_score\n\nscores","f4801b73":"# Get cross val score per model automatically  (logged datasets)\n\nscores_log = {}\n\n# Get cross val score per model automatically  (logged datasets)\n\nfor i in models:\n    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', i)\n                             ])\n    model_score = cross_val_score(pipe, X_log, y, cv=10).mean()\n    scores_log[i] = model_score\n\nscores_log","3e1a0d8d":"X.head()","d79d75d2":"X_log.head()","9048c3f7":"X = preprocessor.fit_transform(X)\nX_log = preprocessor.fit_transform(X_log)","1b1fbce5":"# parameter tuning for svc\n\nsvc = SVC()\nparameter_grid = {'C': [0.001, 0.01, 0.1, 1, 10],\n    'kernel' :  ['linear', 'rbf', 'sigmoid'],\n    'gamma' : [0.001, 0.01, 0.1, 1]}\n\nfolds = 3\nparam_comb = 5\n\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\nrandom_search = RandomizedSearchCV(svc, param_distributions=parameter_grid, n_iter=param_comb, scoring='roc_auc', n_jobs=-1, cv=skf.split(X_log,y), verbose=3, random_state=1001 )\n\n# Here we go\ngrid_result = random_search.fit(X_log, y)\nprint(grid_result.best_estimator_)\nprint(grid_result.best_score_)\n\n","a35b779e":"# parameter tuning for randomforest\nparams = {\n        'n_estimators': [10, 100, 300, 700, 2000],\n        'max_depth':[None, 5, 10, 20],\n        'min_samples_leaf': [1, 2, 3, 4, 5, 6],\n        'max_features': [5, 8, 10, 12, None]\n        }\n\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\nrandom_search = RandomizedSearchCV(rf, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=-1, cv=skf.split(X,y), verbose=3, random_state=1001 )\n\n# Here we go\ngrid_result = random_search.fit(X, y)\nprint(grid_result.best_estimator_)\nprint(grid_result.best_score_)\n","c31999ff":"# Evaluate random forest on non-logged training df\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=42)\n\nmetrics = [mean_absolute_error, mean_squared_error, r2_score, accuracy_score]\n\nrf = RandomForestClassifier(max_depth=5, max_features=8, min_samples_leaf=2, n_estimators=2000)\nrf.fit(X_train, y_train)\npreds = rf.predict(X_valid)\nfor i in metrics:\n    a = i(y_valid, preds)\n    print(f'random forest score (non-log) {i}: {a}')","9c9f26f2":"# Evaluate random forest on logged training df\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nX_train_log, X_valid_log, y_train_log, y_valid_log = train_test_split(X_log, y, test_size=0.33, random_state=42)\n\nmetrics = [mean_absolute_error, mean_squared_error, r2_score, accuracy_score]\n\nrf = RandomForestClassifier(max_depth=5, max_features=8, min_samples_leaf=2, n_estimators=2000)\nrf.fit(X_train_log, y_train_log)\npreds = rf.predict(X_valid_log)\nfor i in metrics:\n    a = i(y_valid_log, preds)\n    print(f'random forest score (log) {i}: {a}')","f518f460":"# Evaluate svc on logged training df\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nX_train_log, X_valid_log, y_train_log, y_valid_log = train_test_split(X_log, y, test_size=0.33, random_state=42)\n\nmetrics = [mean_absolute_error, mean_squared_error, r2_score, accuracy_score]\n\nsvc = SVC(C=0.01, gamma=1)\nsvc.fit(X_train_log, y_train_log)\npreds = svc.predict(X_valid_log)\nfor i in metrics:\n    a = i(y_valid_log, preds)\n    print(f'svc score (log) {i}: {a}')\n","d0647143":"# Evaluate svc on non-logged training df\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=42)\n\nmetrics = [mean_absolute_error, mean_squared_error, r2_score, accuracy_score]\n\nsvc = SVC(C=0.01, gamma=1)\nsvc.fit(X_train, y_train)\npreds = svc.predict(X_valid)\nfor i in metrics:\n    a = i(y_valid, preds)\n    print(f'svc score (non-log) {i}: {a}')","0a695a49":"# From an error encountered later, we find out there are -inf values in the log_test dataset, because of the log transform.\n# Instantiate negative infinity value\nnegative_infinity = float('-inf')\nnegative_infinity\n# We localize these values:\nlog_test.iloc[log_test.values==negative_infinity]","001207f1":"# Replace the negative infinity values with the median of their columns\ncolumns = ['bone_length', 'rotting_flesh', 'hair_length', 'has_soul']\nfor i in columns:\n    log_test[i] = log_test[i].replace([negative_infinity], log_test[i].median())","12d464b0":"log_test.iloc[log_test.values==negative_infinity]","23b75719":"test = preprocessor.fit_transform(test)\nlog_test = preprocessor.fit_transform(log_test)","47f1ed60":"svc = SVC(C=0.01, gamma=1)\n\nsvc.fit(X_log, y)\nsvc_preds = svc.predict(log_test)","cc40d5b1":"# the submission needs to be in string-form ('ghoul','ghost', 'goblin')\n# the target variable y was label encoded before, so we need to reverse that encoding procedure\n\nnew_svc_preds = label_enc.inverse_transform(svc_preds)","d55b1262":"rf = RandomForestClassifier(max_depth=5, max_features=8, min_samples_leaf=2, n_estimators=2000)\n\nrf.fit(X_log, y)\nrf_preds = rf.predict(log_test)\n\nnew_rf_preds = label_enc.inverse_transform(rf_preds)","09e20de4":"submission_rf_log = pd.DataFrame({\"id\": sample.id, \"type\": new_rf_preds})\n\nsubmission_rf_log.to_csv(\"submission_rf_log.csv\", index=False)\n\nsubmission_rf_log.head()\n\n","2b934d7c":"submission_csv_log = pd.DataFrame({\"id\": sample.id, \"type\": new_svc_preds})\n\nsubmission_csv_log.to_csv(\"submission_csv_log.csv\", index=False)\n\n# submission_csv['type'].nunique()\nsubmission_csv_log.head()\n","1b100e7f":"Output of transformer is numpy.ndarray, not DataFrame! \n\nbest estimator for both log and non-log datasets: \n","8f77d85e":"WHY IS CSV ONLY PRODUCING 'ghoul' PREDICTIONS???","c5f51779":"best estimator for both log and non-log datasets: \n\nRandomForestClassifier(max_depth=5, max_features=8, min_samples_leaf=2,\n                       n_estimators=2000)","bf79188a":"# DATASETS ARE READY, we can start modeling","36dc8173":"Transforming columns (e.g. scaler, imputer, etc) outputs a *numpy array*, rather than a dataframe, where the column names have been removed and the columns have changed place!\n\nTo create 'normal' view: create pd.dataframe with the array","dd0a1425":"# Feature Engineering","2e2ca529":"# Light Data Exploration\n \n1) For numeric data\n\n    Make histograms to understand distributions\n    Corrplot\n    Pivot table comparing survival rate across numeric variables\n\n2) For Categorical Data\n\n    Make bar charts to understand balance of classes\n    Make pivot tables to understand relationship with target","53148ef4":"* SVC (non-logged) and RandomForestClassifier (logged) are performing best.\n* Approx scores: 0.71. "}}