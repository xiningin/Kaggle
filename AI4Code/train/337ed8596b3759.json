{"cell_type":{"96bf3acf":"code","e7bb095e":"code","bc0232b4":"code","65876d67":"code","77df5353":"code","974fa8d2":"code","9ae7e5f2":"code","292bbf00":"code","cb08fcc0":"code","33dde2df":"code","590e1d25":"code","4fc4ab10":"code","ebc179ba":"code","44d9d91b":"code","3603a2fd":"markdown","98d4b2e4":"markdown","c2605ce1":"markdown","eb9de1e5":"markdown","81fbb86e":"markdown","1b265c60":"markdown","ee226c12":"markdown","8cb8c7ec":"markdown","e5b4cef4":"markdown","1f8526e2":"markdown"},"source":{"96bf3acf":"import pandas as pd\nimport numpy as np\nimport time\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\nimport gc\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nfrom sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.neighbors import RadiusNeighborsRegressor, KNeighborsRegressor\nfrom sklearn.linear_model import GammaRegressor, HuberRegressor, PassiveAggressiveRegressor, PoissonRegressor\nfrom sklearn.linear_model import RANSACRegressor, SGDRegressor, TheilSenRegressor, TweedieRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor as ens_ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, GradientBoostingRegressor, RandomForestRegressor\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor \n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","e7bb095e":"path= '..\/input\/tabular-playground-series-feb-2021\/'\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\nss = pd.read_csv(path + 'sample_submission.csv')","bc0232b4":"train","65876d67":"test","77df5353":"def get_validation_df(X_train, y_train):\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n    return X_train, X_val, y_train, y_val\n\ndef get_preprocessed_df(train_df, test_df):\n    \n    train = train_df.copy()\n    test = test_df.copy()\n    \n    cat_cols = ['cat{}'.format(i) for i in range(10)]\n    cont_cols = ['cont{}'.format(i) for i in range(14)]\n\n    X_train = train[cat_cols+cont_cols]\n    y_train = train['target']\n    X_test = test.drop('id',axis=1)\n    del train, test\n    \n    # continuous - standard scaling\n    st_scaler = StandardScaler()\n    st_scaler.fit(X_train[cont_cols])\n    X_train[cont_cols] = st_scaler.transform(X_train[cont_cols])\n    X_test[cont_cols] = st_scaler.transform(X_test[cont_cols])\n    \n    # categorical -> one-hot encoding\n    oh_enc = OneHotEncoder()\n    oh_enc.fit(X_train[cat_cols])\n    oh_cols = oh_enc.get_feature_names(cat_cols)\n\n    X_train[oh_cols] = oh_enc.transform(X_train[cat_cols]).toarray()\n    X_test[oh_cols] = oh_enc.transform(X_test[cat_cols]).toarray()    \n    \n    X_train.drop(cat_cols, axis=1, inplace=True)\n    X_test.drop(cat_cols, axis=1, inplace=True)\n    \n    X_train, X_val, y_train, y_val = get_validation_df(X_train, y_train)\n    \n    return X_train, X_val, y_train, y_val, X_test\n\ndef eval_score(y_true, pred):\n\n    print('rmse:', rmse)\n    print('mae:', mae)\n    print('r2:', r2)\n    \n    return rmse, mae, r2\n    \ndef eval_model(model, X_train, X_val, y_train, y_val, model_name, scores_dict):\n    start = time.time()\n    \n    model.fit(X_train, y_train)\n    pred = model.predict(X_val)\n    \n    rmse = round(mean_squared_error(y_val, pred, squared=False),4)\n    mae = round(mean_absolute_error(y_val, pred),4)\n    r2 = round(r2_score(y_val,pred),4)\n    print('rmse:', rmse)\n    print('mae:', mae)\n    print('r2:', r2)\n    \n    scores_dict[model_name] = [rmse, mae, r2]\n    \n    print('execution time: {:.4f}s'.format(time.time()-start))\n    \n    return scores_dict","974fa8d2":"X_train, X_val, y_train, y_val, X_test = get_preprocessed_df(train, test)","9ae7e5f2":"print('Train set:', X_train.shape, y_train.shape)\nprint('Validation set:', X_val.shape, y_val.shape)\nprint('Test set:', X_test.shape)","292bbf00":"# making models\n\ndt_reg = DecisionTreeRegressor()\net_reg = ExtraTreeRegressor()\nmlp_reg = MLPRegressor()\n#rn_reg = RadiusNeighborsRegressor(n_jobs=-1) it raises an error\n#kn_reg = KNeighborsRegressor(n_jobs=-1) it takes so long\n#gm_reg = GammaRegressor() Some value(s) of y are out of the valid range for family GammaDistribution\nhn_reg = HuberRegressor()\npa_reg = PassiveAggressiveRegressor()\nps_reg = PoissonRegressor()\nrs_reg = RANSACRegressor() \nsgd_reg = SGDRegressor()\n#ts_reg = TheilSenRegressor() over memory \ntwd_reg = TweedieRegressor()\n#gp_reg = GaussianProcessRegressor() over memory\neet_reg = ens_ExtraTreesRegressor()\nab_reg = AdaBoostRegressor()\nbg_reg = BaggingRegressor()\ngb_reg = GradientBoostingRegressor()\nrf_reg = RandomForestRegressor()\nhgb_reg = HistGradientBoostingRegressor()\nxgb_reg = XGBRegressor()\nlgbm_reg = LGBMRegressor()\ncat_reg = CatBoostRegressor()\n\nmodels = [\n    ('DecisionTreeRegressor', dt_reg),\n    ('ExtraTreeRegressor', et_reg),\n    ('MLPRegressor', mlp_reg),\n    #('RadiusNeighborsRegressor', rn_reg)\n    #('KNeighborsRegressor', kn_reg),\n    #('GammaRegressor', gm_reg),\n    ('HuberRegressor', hn_reg),\n    ('PassiveAggressiveRegressor', pa_reg),\n    ('PoissonRegressor', ps_reg),\n    ('RANSACRegressor', rs_reg),\n    ('SGDRegressor', sgd_reg),\n    #('TheilSenRegressor', ts_reg),\n    ('TweedieRegressor', twd_reg),\n    #('GaussianProcessRegressor', gp_reg),\n    ('EnsembleExtraTreesRegressor', eet_reg),\n    ('AdaBoostRegressor', ab_reg),\n    ('BaggingRegressor', bg_reg),\n    ('GradientBoostingRegressor', gb_reg),\n    ('RandomForestRegressor', rf_reg),\n    ('HistGradientBoostingRegressor', hgb_reg),\n    ('XGBRegressor', xgb_reg),\n    ('LGBMRegressor', lgbm_reg),\n    ('CatBoostRegressor', cat_reg)\n]","cb08fcc0":"scores_dict = dict()\nfor model_name, model in models:\n    print('#',model_name)\n    scores_dict = eval_model(model, X_train, X_val, y_train, y_val, model_name, scores_dict)\n    print()\n    gc.collect()","33dde2df":"result = pd.DataFrame.from_dict(scores_dict, orient='index', columns=['rmse','mae','r2'])\nresult.drop('RANSACRegressor',axis=0, inplace=True) # result outlier \nresult","590e1d25":"import plotly.express as px\ndata = result.sort_values('rmse')\nfig = px.bar(data, x=data.index, y='rmse', color=data['rmse'])\nfig.show()","4fc4ab10":"data = result.sort_values('mae')\nfig = px.bar(data, x=data.index, y='mae', color=data['mae'])\nfig.show()","ebc179ba":"data = result.sort_values('r2')\nfig = px.bar(data, x=data.index, y='r2', color=data['r2'])\nfig.show()","44d9d91b":"ss['target'] = cat_reg.predict(X_test)\nss.to_csv('submission.csv', index=False)","3603a2fd":"# results","98d4b2e4":"# functions","c2605ce1":"# model fitting","eb9de1e5":"# data preprocessing","81fbb86e":"# making models","1b265c60":"# packages","ee226c12":"# loading files","8cb8c7ec":"With basic params, __Catboost__ made best score","e5b4cef4":"Sometimes, there are more effective model than lightgbm or xgboost. So I tried __all regressors in scikit-learn including lightgbm, xgboost and catboost__. I didn't do any parameter tuning (too many params...). I don't know exactly about some of regressors, but I tried as many as I can. ","1f8526e2":"actually, __r2_score__ was meaningless in this experiments"}}