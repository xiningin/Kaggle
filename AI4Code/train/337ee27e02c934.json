{"cell_type":{"11dc28cf":"code","67ec0862":"code","7f943887":"code","51b71641":"code","a9d06670":"code","f43d1b30":"code","585c7ba4":"code","fd81f035":"markdown","b8ae5e22":"markdown","e76e9f6e":"markdown","db6cbc5a":"markdown"},"source":{"11dc28cf":"import gym\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt ","67ec0862":"env = gym.make(\"Taxi-v2\").env","7f943887":"q_table = np.zeros([env.observation_space.n,env.action_space.n])","51b71641":"alpha = 0.1\ngamma = 0.9\nepsilon = 0.1","a9d06670":"reward_list = []\ndroputs_list = []","f43d1b30":"episode_number = 10000\nfor i in range(1,episode_number):\n    \n    # initialize enviroment\n    state = env.reset()\n    \n    reward_count = 0\n    dropouts = 0\n    \n    while True:\n        \n        # exploit vs explore to find action\n        # %10 = explore, %90 exploit\n        if random.uniform(0,1) < epsilon:\n            action = env.action_space.sample()\n        else:\n            action = np.argmax(q_table[state])\n        \n        # action process and take reward\/ observation\n        next_state, reward, done, _ = env.step(action)\n        \n        # Q learning function\n        old_value = q_table[state,action] # old_value\n        next_max = np.max(q_table[next_state]) # next_max\n        \n        next_value = (1-alpha)*old_value + alpha*(reward + gamma*next_max)\n        \n        # Q table update \n        q_table[state,action] = next_value\n        \n        # update state\n        state = next_state\n        \n        # find wrong dropouts\n        if reward == -10:\n            dropouts += 1\n            \n        \n        if done:\n            break\n        \n        reward_count += reward \n        \n    if i%10 == 0:\n        droputs_list.append(dropouts)\n        reward_list.append(reward_count)\n        print(\"Episode: {}, reward {}, wrong dropout {}\".format(i,reward_count,dropouts))\n        ","585c7ba4":"env.s = env.encode(0,0,3,4) \nenv.render()   \n\nenv.s = env.encode(4,4,4,3) \nenv.render() ","fd81f035":"* Plotting Metrix","b8ae5e22":"Actions: \n    There are 6 discrete deterministic actions:\n    - 0: move south\n    - 1: move north\n    - 2: move east \n    - 3: move west \n    - 4: pickup passenger\n    - 5: dropoff passenger\n    \ntaxi row, taxi column, passenger index, destination ","e76e9f6e":"* Hyperparameters","db6cbc5a":"* Q table"}}