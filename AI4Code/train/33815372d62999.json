{"cell_type":{"5c147745":"code","def4814c":"code","300936f8":"code","267154a5":"code","7a7cf4b7":"code","3e969c83":"code","614c5b0f":"code","5dd82b21":"markdown","a7fdd516":"markdown","e9358633":"markdown","73f2f55c":"markdown","d53a8d09":"markdown","cb8f3aef":"markdown","fcbcfe1b":"markdown"},"source":{"5c147745":"from keras.preprocessing import sequence\nfrom keras.datasets import imdb # IMDB is public data from keras \nfrom keras import layers, models\n","def4814c":"(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=20000)","300936f8":"x_train.shape\n\n","267154a5":"class Data:\n    def __init__(self, max_features=20000, maxlen=80):\n        \n        (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n        # load_data() = bring data from imdb\n        # max_features = words's maximum frequency\n        \n        x_train = sequence.pad_sequences(x_train, maxlen=maxlen) \n        x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n        # pad_sequences() = make every sequence have same length\n\n        self.x_train, self.y_train = x_train, y_train\n        self.x_test, self.y_test = x_test, y_test\n","7a7cf4b7":"class RNN_LSTM(models.Model):\n    def __init__(self, max_features, maxlen):\n        \n        # input (80 element)\n        x = layers.Input((maxlen,))\n        \n        # embedding = every element change to be words(128 length) \n        # 128 is output vector size\n        h = layers.Embedding(max_features, 128)(x)\n        \n        # 128 nodes, dropout and recurrent_dropout set 20%\n        h = layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2)(h)\n        \n        # output (activation fuction = sigmoid)\n        y = layers.Dense(1, activation='sigmoid')(h)\n        super().__init__(x, y)\n\n        # model compile (set oprimizer function and loss function)\n        self.compile(loss='binary_crossentropy',\n                     optimizer='adam', metrics=['accuracy'])\n","3e969c83":"class Machine:\n    def __init__(self,\n                 max_features=20000,\n                 maxlen=80):\n        self.data = Data(max_features, maxlen)\n        self.model = RNN_LSTM(max_features, maxlen)\n\n    def run(self, epochs=3, batch_size=32):\n        data = self.data\n        model = self.model\n        print('Training stage')\n        print('==============')\n        \n        # training LSTM\n        model.fit(data.x_train, data.y_train,\n                  batch_size=batch_size,\n                  epochs=epochs,\n                  validation_data=(data.x_test, data.y_test))\n\n        score, acc = model.evaluate(data.x_test, data.y_test,\n                                    batch_size=batch_size)\n        print('Test performance: accuracy={0}, loss={1}'.format(acc, score))\n        \n  ","614c5b0f":"def main():\n    m = Machine()\n    m.run()\n\nif __name__ == '__main__':\n    main()","5dd82b21":"* 25000 binary movie reputation\n* 1 = recommend, 2 = not recommend","a7fdd516":"<a id=\"two\"><\/a>\n# 2. Prepare Data","e9358633":"# Simple LSTM \n\nLSTM (Long term Short Term memory)\n* **RNN (Recurrent Neural Network)** has problem about long term memory, **LSTM** has improvement about that\n* Our **LSTM** is focusing on predicting some movie is good or not, when **LSTM** gets people's reputation\n* We will use Keras module\n<hr>\n\nHow to use this notebook :\n\nThere is only minimum explanation\n\nThis notebook could be helpful for who want to see how code works right away\n\nPlease upvote if it was helpful !\n\n<hr>\n\n## Content\n1. [Libraries import](#one)\n2. [Prepare Data](#two)\n3. [Modeling](#three)\n4. [Training & Evaluation](#four)\n\n<hr>","73f2f55c":"<a id=\"one\"><\/a>\n# 1. Libraries import","d53a8d09":"<a id=\"four\"><\/a>\n# 4. Training & Evaluation","cb8f3aef":"## Reference\n* Coding chef 3 minute deep learning  - [ex5_1_lstm_imdb](https:\/\/github.com\/jskDr\/keraspp\/blob\/master\/ex5_1_lstm_imdb_cl.py)","fcbcfe1b":"<a id=\"three\"><\/a>\n# 3. Modeliing"}}