{"cell_type":{"d6130917":"code","e06bfa96":"code","50119039":"code","9e3cec64":"code","32aaf268":"code","2f9691ec":"code","0f032497":"code","b70945e3":"code","72fc43dd":"code","4dcbd46c":"code","d27d3d38":"markdown","760ec608":"markdown","d5c51df2":"markdown","09a5aed9":"markdown"},"source":{"d6130917":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e06bfa96":"import pandas as pd\nimport numpy as np\n\ndataset = pd.read_csv(\"\/kaggle\/input\/breast-cancer-prediction-dataset\/Breast_cancer_data.csv\")\ndataset.head()","50119039":"dataset.info()","9e3cec64":"dataset.describe()","32aaf268":"from sklearn.preprocessing import StandardScaler, RobustScaler    \nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer, make_column_selector","2f9691ec":"# Select numerical features on datatset\nnumerical_features = make_column_selector(dtype_include=np.number)\n\n# Build pipeline for numerical features\nnumerical_pipeline = make_pipeline(StandardScaler(), RobustScaler())\n\n# Associate numerical piepeline for numerical features\nprepro = make_column_transformer((numerical_pipeline,numerical_features))","0f032497":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","b70945e3":"# Build train and test dataset\ny = dataset['diagnosis']\nX = dataset.drop(columns=['diagnosis'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Build Model with pipeline\nLogisticRegression = LogisticRegression(C=0.1, random_state=4)\nmodel = make_pipeline(prepro,LogisticRegression)\n\n# Fit model\nmodel.fit(X_train, y_train)","72fc43dd":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, plot_confusion_matrix, plot_roc_curve\nfrom sklearn.model_selection import learning_curve","4dcbd46c":"y_pred = model.predict(X_test)\n\n# Confusion matrix\nplot_confusion_matrix(model, X_test, y_test)\n\n# Values Report\nprint(classification_report(y_test, y_pred))\n\n# ROC curve\nplot_roc_curve(model, X_test, y_test,name=\"Logisitc Regression\")\nplt.show()\n\n# Learning curve\ntrain_size, train_score, val_score = learning_curve(model, X_train, y_train,cv=4, scoring='f1',train_sizes=np.linspace(0.1, 1, 10))\nplt.plot(train_size, train_score.mean(axis=1), label='train score')\nplt.plot(train_size, val_score.mean(axis=1), label='validation score')\nplt.legend()","d27d3d38":"# Model","760ec608":"# Preprocessing","d5c51df2":"# **Load dataset from csv file**[](http:\/\/)","09a5aed9":"# Model evaluation"}}