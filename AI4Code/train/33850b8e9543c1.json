{"cell_type":{"cfc98d0f":"code","44c4daab":"code","cc7e935b":"code","8beed929":"code","88d881bd":"code","35d2a974":"code","a67ca9ee":"code","d90301f3":"code","c2a7a46c":"code","6db7dd20":"code","ecb00d43":"code","595edf4e":"code","5d2b41b3":"code","f7ce5388":"code","f71006ee":"code","ee540838":"code","12e428bb":"code","176e8497":"code","2d5c30e6":"code","6a15fd28":"code","bef47dcc":"code","4b9c62ae":"markdown","f7ca0ee4":"markdown","f716dfe0":"markdown","ab1159e2":"markdown","d6ecc55f":"markdown","c663c13c":"markdown","8f6fbee5":"markdown","227a8402":"markdown","6cc66e06":"markdown","a78cd73d":"markdown","932c0ba0":"markdown","acbc0776":"markdown","1d0b64bf":"markdown","3a11c20f":"markdown","ceb919f8":"markdown","0df1c502":"markdown","05c3e975":"markdown","8d72c69f":"markdown","f9e70101":"markdown","cfcc289a":"markdown","97264621":"markdown","12ab8555":"markdown","1cd8945c":"markdown"},"source":{"cfc98d0f":"# First, we need to import the needed packages.\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestClassifier as rfc\nfrom sklearn.svm import SVC as svc\nimport sklearn.linear_model as sk\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport scipy.stats as sp\nfrom scipy.special import boxcox1p\nfrom sklearn.feature_selection import RFECV\nimport sklearn.metrics as mt\nimport warnings\nwarnings.filterwarnings('ignore')","44c4daab":"train = pd.read_csv('..\/input\/train.csv', index_col='Id')","cc7e935b":"# Set an array with 'No' feature names.\nno_features = ['Alley', 'BsmtQual', 'BsmtCond',\n               'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n               'FireplaceQu', 'GarageType', 'GarageFinish',\n               'GarageQual', 'GarageYrBlt', 'GarageCond', 'PoolQC',\n               'Fence', 'MiscFeature']\n# Fill NaNs with 'No' value.\ntrain[no_features] = train[no_features].fillna('No')\ntrain.loc[:, \"LotFrontage_NA\"] = train.LotFrontage.isnull() * 1\ntrain.loc[:, \"LotFrontage_NA\"] = train.loc[:, \"LotFrontage_NA\"].astype(\"object\")","8beed929":"def HasFeat(var):\n    hasf = var\n    hasf[hasf != 'No'] = 1\n    hasf[hasf == 'No'] = 0\n    return hasf\ndef feat_eng (tr):\n    tr.GarageQual[tr['GarageQual'] == 'No'] = 0\n    tr.GarageQual[tr['GarageQual'] == 'Po'] = 1\n    tr.GarageQual[tr['GarageQual'] == 'Fa'] = 2\n    tr.GarageQual[tr['GarageQual'] == 'TA'] = 3\n    tr.GarageQual[tr['GarageQual'] == 'Gd'] = 4\n    tr.GarageQual[tr['GarageQual'] == 'Ex'] = 5\n    tr.GarageQual = tr.GarageQual.astype('float32')\n    tr.GarageCond[tr['GarageCond'] == 'No'] = 0\n    tr.GarageCond[tr['GarageCond'] == 'Po'] = 1\n    tr.GarageCond[tr['GarageCond'] == 'Fa'] = 2\n    tr.GarageCond[tr['GarageCond'] == 'TA'] = 3\n    tr.GarageCond[tr['GarageCond'] == 'Gd'] = 4\n    tr.GarageCond[tr['GarageCond'] == 'Ex'] = 5\n    tr.GarageCond = tr.GarageCond.astype('float32')\n    tr.GarageYrBlt[tr['GarageYrBlt'] == 'No'] = 0\n    tr.GarageYrBlt = tr.GarageYrBlt.astype('float32')\n    tr.Functional[tr['Functional'] == 'Typ'] = 7\n    tr.Functional[tr['Functional'] == 'Min1'] = 6\n    tr.Functional[tr['Functional'] == 'Min2'] = 5\n    tr.Functional[tr['Functional'] == 'Mod'] = 4\n    tr.Functional[tr['Functional'] == 'Maj1'] = 3\n    tr.Functional[tr['Functional'] == 'Maj2'] = 2\n    tr.Functional[tr['Functional'] == 'Sev'] = 1\n    tr.Functional[tr['Functional'] == 'Sal'] = 0\n    tr.Functional = tr.Functional.astype('float32')\n    tr.Fence[tr['Fence'] == 'No'] = 0\n    tr.Fence[tr['Fence'] == 'MnWw'] = 1\n    tr.Fence[tr['Fence'] == 'GdWo'] = 2\n    tr.Fence[tr['Fence'] == 'MnPrv'] = 3\n    tr.Fence[tr['Fence'] == 'GdPrv'] = 4\n    tr.Fence = tr.Fence.astype('float32')\n    tr.KitchenQual[tr['KitchenQual'] == 'Po'] = 1\n    tr.KitchenQual[tr['KitchenQual'] == 'Fa'] = 2\n    tr.KitchenQual[tr['KitchenQual'] == 'TA'] = 3\n    tr.KitchenQual[tr['KitchenQual'] == 'Gd'] = 4\n    tr.KitchenQual[tr['KitchenQual'] == 'Ex'] = 5\n    tr.KitchenQual = tr.KitchenQual.astype('float32')\n    tr.HeatingQC[tr['HeatingQC'] == 'Po'] = 1\n    tr.HeatingQC[tr['HeatingQC'] == 'Fa'] = 2\n    tr.HeatingQC[tr['HeatingQC'] == 'TA'] = 3\n    tr.HeatingQC[tr['HeatingQC'] == 'Gd'] = 4\n    tr.HeatingQC[tr['HeatingQC'] == 'Ex'] = 5\n    tr.HeatingQC = tr.HeatingQC.astype('float32')\n    tr.ExterQual[tr['ExterQual'] == 'Po'] = 1\n    tr.ExterQual[tr['ExterQual'] == 'Fa'] = 2\n    tr.ExterQual[tr['ExterQual'] == 'TA'] = 3\n    tr.ExterQual[tr['ExterQual'] == 'Gd'] = 4\n    tr.ExterQual[tr['ExterQual'] == 'Ex'] = 5\n    tr.ExterQual = tr.ExterQual.astype('float32')\n    tr.BsmtQual[tr['BsmtQual'] == 'No'] = 0\n    tr.BsmtQual[tr['BsmtQual'] == 'Po'] = 1\n    tr.BsmtQual[tr['BsmtQual'] == 'Fa'] = 2\n    tr.BsmtQual[tr['BsmtQual'] == 'TA'] = 3\n    tr.BsmtQual[tr['BsmtQual'] == 'Gd'] = 4\n    tr.BsmtQual[tr['BsmtQual'] == 'Ex'] = 5\n    tr.BsmtQual = tr.BsmtQual.astype('float32')\n    tr.BsmtFinType1[tr['BsmtFinType1'] == 'No'] = 0\n    tr.BsmtFinType1[tr['BsmtFinType1'] == 'Unf'] = 1\n    tr.BsmtFinType1[tr['BsmtFinType1'] == 'LwQ'] = 2\n    tr.BsmtFinType1[tr['BsmtFinType1'] == 'Rec'] = 3\n    tr.BsmtFinType1[tr['BsmtFinType1'] == 'BLQ'] = 4\n    tr.BsmtFinType1[tr['BsmtFinType1'] == 'ALQ'] = 5\n    tr.BsmtFinType1[tr['BsmtFinType1'] == 'GLQ'] = 6\n    tr.BsmtFinType1 = tr.BsmtFinType1.astype('float32')\n    tr.BsmtFinType2[tr['BsmtFinType2'] == 'No'] = 0\n    tr.BsmtFinType2[tr['BsmtFinType2'] == 'Unf'] = 1\n    tr.BsmtFinType2[tr['BsmtFinType2'] == 'LwQ'] = 2\n    tr.BsmtFinType2[tr['BsmtFinType2'] == 'Rec'] = 3\n    tr.BsmtFinType2[tr['BsmtFinType2'] == 'BLQ'] = 4\n    tr.BsmtFinType2[tr['BsmtFinType2'] == 'ALQ'] = 5\n    tr.BsmtFinType2[tr['BsmtFinType2'] == 'GLQ'] = 6\n    tr.BsmtFinType2 = tr.BsmtFinType2.astype('float32')\n    tr['TotalArea'] = tr['TotalBsmtSF'] + tr['1stFlrSF'] + tr['2ndFlrSF']\n    tr['BsmtFinArea'] = (tr['BsmtFinSF1'] + tr['BsmtFinSF2'])\/(tr['BsmtFinSF1'] + tr['BsmtFinSF2'] + tr['BsmtUnfSF'])\n    tr['MSSubClass'] = tr['MSSubClass'].astype('object')\n    return tr\n\ntrain = feat_eng(train)","88d881bd":"# Plotting GrLivArea.\nplt.scatter(train['GrLivArea'], train['SalePrice'])\n# Plot only the two outliers.\nplt.scatter([4676, 5642], [184750,160000], color=\"red\")\n# Show plot.\nplt.show()","35d2a974":"# Removing the outliers.\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n# Plotting again.\nplt.scatter(train['GrLivArea'], train['SalePrice'])\nplt.show()","a67ca9ee":"# We need to get all the numerical features from the train dataset.\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnum = train.select_dtypes(include=numerics)\n\n# Now we create a Series with the skewness and plot in an ordered line graph.\nskew = pd.DataFrame(sp.skew(num), num.keys())\n# Sorting values.\nskew=skew.sort_values(by=0, ascending=False)\n# Setting the size.\nplt.figure(figsize=(15,7))\n# Plotting the skew.\nplt.plot(skew)\n# Rotating the features labels.\nplt.xticks(rotation=45, fontsize=8)\n# Adding labels.\nplt.xlabel('Numerical Features', fontsize=10)\nplt.ylabel('Skew', fontsize=10)\n# Adding limit lines.\nplt.axhline(y=1, c='red')\nplt.axhline(y=-1, c='red', )\nplt.show()\n","d90301f3":"# Fist we define a function to indicate what variables will be transformed.\ndef boxcox(a, f):\n    if sp.skew(a) not in range(-1,1) or sp.kurtosis(a) not in range(-1,1):\n        a = boxcox1p(a, 0.05)\n        print(f, 'transformed.')\n        return a\n# Here we create an array of feature keys for transforming later.\ntransformed_feats = []\n\n# For loop with substitutions, transformations and new features.\nfor f in train.keys():\n    # Categorical features are recognised as objects by read_csv.\n    if train[f].dtype == object:\n        train[f].fillna(value=train[f].value_counts().idxmax())\n        train[f] = train[f].astype('category')\n    else:\n        # Numerical features.\n        train[f] = train[f].fillna(value=np.mean(train[f]))\n        # The target feature will be transformed later so we must exclude from this loop for now.\n        if f != 'SalePrice':\n            train[f] = boxcox(train[f], f)\n            train[f + 'Sq'] = (train[f] ** 2)\n            train[f + 'Cub'] = (train[f] ** 3)\n            transformed_feats.append(f)","c2a7a46c":"# Getting the numerical features.\nnum = train[skew.index].select_dtypes(include=numerics)\n# Creating a Series with skewness values for each feature.\nskew = pd.DataFrame(sp.skew(num), num.keys())\nskew=skew.sort_values(by=0, ascending=False)\n# Plotting in a line graph.\nplt.figure(figsize=(15,7))\nplt.plot(skew)\n# Rotating the features names.\nplt.xticks(rotation=45, fontsize=8)\n# Adding labels.\nplt.xlabel('Numerical Features', fontsize=10)\nplt.ylabel('Skew', fontsize=10)\n# Adding limits.\nplt.axhline(y=1, c='red')\nplt.axhline(y=-1, c='red', )\n# Showing the graph.\nplt.show()","6db7dd20":"cols = train.columns\ntrain = pd.get_dummies(train, drop_first=True)","ecb00d43":"# Saving all features for future comparison.\nall_features = train.keys()\n# Removing features.\ntrain = train.drop(train.loc[:,(train==0).sum()>=1444],axis=1)\ntrain = train.drop(train.loc[:,(train==1).sum()>=1444],axis=1) \n# Getting and printing the remaining features.\nremain_features = train.keys()\nremov_features = [st for st in all_features if st not in remain_features]\nprint(len(remov_features), 'features were removed:', remov_features)","595edf4e":"# Plot histogram of SalePrice.\nplt.hist(train['SalePrice'])\nplt.show()\n#Plot the QQ-plot\nsp.probplot(train['SalePrice'], plot=plt)\nplt.show()","5d2b41b3":"# Transformation\ny = np.log(train['SalePrice'].values)\n\n# Let's plot again to see the result.\nplt.hist(y)\nplt.show()\nsp.probplot(y, plot=plt)\nplt.show()","f7ce5388":"X = train.drop('SalePrice', axis=1)","f71006ee":"def scorer(estimator, X, y):\n    y_new = estimator.predict(X)   \n    return np.sqrt(mt.mean_squared_error(y, y_new))\nest = sk.ElasticNet(l1_ratio=0, alpha=0.017, random_state=1)\nfsel = RFECV(est, step=1, cv=15, n_jobs=-1, scoring=scorer)\nfsel = fsel.fit(X, y)\nimportant_feat = list(X.loc[:, fsel.ranking_<=130].columns)\nX = train.loc[:, important_feat].values\nprint(\"Most important features:\", important_feat)","ee540838":"def cv_train(X, y, k):\n    param = {'learning_rate' : [0.1],\n             'n_estimators' : [600],\n             'max_depth' : [2]}\n    gbm = GradientBoostingRegressor(loss='huber', random_state=1)\n    cv1 = GridSearchCV(gbm, param, cv=k, scoring='neg_mean_squared_error')\n    cv1.fit(X, y)\n    lasso = sk.ElasticNet(random_state=1)\n    param = {'l1_ratio' : [0],\n             'alpha' : [0.017]}\n    cv2 = GridSearchCV(lasso, param, cv=k, scoring='neg_mean_squared_error')\n    cv2.fit(X, y)\n    print('GBM:', np.sqrt(cv1.best_score_*-1),\n          'Lasso:', np.sqrt(cv2.best_score_*-1),\n          'Mean:', np.sqrt(((cv1.best_score_+cv2.best_score_)\/2)*-1),      \n          'Pond Mean (0.8):', np.sqrt(((cv1.best_score_*0.2)+(cv2.best_score_*(0.8)))*-1))\n    return cv1, cv2","12e428bb":"cv1, cv2 = cv_train(X, y, 20)","176e8497":"loss1 = (y-cv1.predict(X))**2\nloss2 = (y-cv2.predict(X))**2\nplt.scatter(y, loss1)\nplt.scatter(y, loss2)\nplt.legend(['GBM', 'Lasso'])\nplt.show()\n\n#Plot the QQ-plot\nsp.probplot(loss1, plot=plt)\nplt.show()\nsp.probplot(loss2, plot=plt)\nplt.show()","2d5c30e6":"test = pd.read_csv('..\/input\/test.csv')\nId = test.loc[:, \"Id\"]","6a15fd28":"test[no_features] = test[no_features].fillna('No')\ntest.loc[:, \"LotFrontage_NA\"] = test.LotFrontage.isnull() * 1\ntest.loc[:, \"LotFrontage_NA\"] = test.loc[:, \"LotFrontage_NA\"].astype(\"object\")\n\ntest = feat_eng(test)\n\nfor f in test.keys():\n    # Categorical features are recognised as objects by read_csv.\n    if test[f].dtype == object:\n        test[f].fillna(value=test[f].value_counts().idxmax())\n        test[f] = test[f].astype('category')\n    else:\n        # Numerical features.\n        test[f] = test[f].fillna(value=np.mean(test[f]))\n        # The target feature will be transformed later so we must exclude from this loop for now.\n        if f in transformed_feats and f != 'SalePrice':\n            test[f] = boxcox(test[f], f)\n            test[f + 'Sq'] = (test[f] ** 2)\n            test[f + 'Cub'] = (test[f] ** 3)            \n\ntest = pd.get_dummies(test, drop_first=True)\ntest = test.loc[:, important_feat]\nprint(test.loc[:, np.sum(test.isnull()) == len(test)].columns)\nfor c in list(test.loc[:, np.sum(test.isnull()) == len(test)].columns):\n    test.loc[:, c] = 0\nprint(\"NAs:\", np.sum(np.sum(test.isnull())))","bef47dcc":"pred = ((math.e**cv2.predict(test))*(0.8)) + ((math.e**cv1.predict(test))*0.2)\nsub = pd.read_csv('..\/input\/sample_submission.csv')\npd.DataFrame({\"Id\": Id.values, \"SalePrice\": pred}).to_csv(\"submission.csv\", index=False)","4b9c62ae":"Some features are very highly skewed and this can negatively impact the model.\nWe will try to reduce skewness by boxcox transformation of all features with more than 1 or less than -1 skewness.\nFor that we will use a for loop that will substitute NaNs values on categorical features by the most common value and  by the mean on numerical features.\nWe will also use this for loop to create exponential versions (Square and Cubic) of numerical features.\n\nNote that the transformed features keys are saved to transform the test set later.","f7ca0ee4":"Way better! Let's check the skewness of data.\nWhen modelling regressions, it is important (although not necessary) that the features are normally distributed.\nSkewness and kurtosis are frequently used to measure normality.\nIn the present work, we will only account for skewness on the data.\nFell free to work on improving kurtosis and see how it affects this model!","f716dfe0":"Then, we need to import the data.","ab1159e2":"Let's check the numerical variables once again.","d6ecc55f":"Features that have too low variance can negatively impact the model, so we need to remove them by the number of repetitive equal values.\nIn this case, we used a threshold of 1444 (1458-1444 = 14 not 0 or 1 values) which corresponds to approximately 90% of the sample we have. \nTherefore, if any feature has more than 1444 reps of 1 or 0 it will be excluded.\nWhen doing this, ","c663c13c":"When checking for outliers, GrLivArea showed important insights. \nThey are shown in the plot as red dots.","8f6fbee5":"It is clear that the skewness really improved and that will be enough for now. \nSome features that still show high skew as PoolArea and, 3SsnPorch, LowQualFinSF and MiscVal probably have many zero values and will be investigated later.\n\nNext, we create a column for each category of categorical features and revome the dummy ones.","227a8402":"Now lets check the loss plots.","6cc66e06":"We can see that the model have room to improve but the RSME is acceptable for now. ","a78cd73d":"# Modeling","932c0ba0":"# Modelling with Lasso and Gradient Boosting Machine in Houses Prices Competition\n\n**In case of using this notebook, any credit is appretiated!**\n**This kernel is a work in progress, so if you have any feedback to give, I encourage you to leave a comment!**\n\nThis competition really helped me understand the regression principles in machine learning. \nI have read many great notebooks here, and would like to name specifically 2 of them:\n\n- [Comprehensive data exploration with Python][1] by Pedro Marcelino\n[1]: https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n- [Stacked Regressions to predict House Prices][2] from Serigne\n[2]: https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/notebook\n\nThe main steps made in EDA and Wrangling here were:\n\n- Imputing missing values\n- Setting feature types to most adequate options.\n- Transforming the data with Box Cox Transformation.\n- Removing dummy features.\n- Removing features with little variance.\n- Removing two ouliers that really improved the scoring.\n\nThe main steps mande in Modelling were:\n\n- Using Lasso for regularization of features.\n- Filtering Feature Selection with correlation.\n- Wrapping Feature Selection with sequential removal of less important features after modelling with Lasso (Sklearn REFCV).\n- Weighted Mean of Lasso and GBM predictions.\n\nThe average of these two models showed a RMSE of 0.11754 wich corresponds to Top 19% by 29\/8\/2019.\n\nLet's get to it!","acbc0776":"Is is clear by the graphs that SalePrice feature is skewed to the left.\nWe will try solve this by using natural log transformation.","1d0b64bf":"## Feature Engineering\n\nHere, we define the functions that will be used to prepare and add features to train and test datasets.\n\nSome features that are categorical and ordered are set to numerical here.\n\nTwo important features are added as well 'TotalArea' (the total 'inside' area) and BsmtFinArea (the proportion of finalyzed area of Basement).","3a11c20f":"As expected, features with still high variance after the transformations were removed for having too much 0s or 1s.\n\n### Target Variable\n\nLet's check the distribution of SalePrice.","ceb919f8":"# Data Wrangling\n\nFirst, as the documentation states, some features have missing values that means absence of that feature.\nTherefore, we need to fill these NaNs.","0df1c502":"Way better!\nThe log transformation really improved the SalePrice distribution.\n\nLast but not least, we need to set the indepentent features dataset.","05c3e975":"We can note that there are two outliers that can be really bad for the model, as they have very large GrLivArea and low SalePrice.\nAlthough it is not always recommended to exclude outliers, is this case it looks like an good solution.\nSo let's remove tham and plot the data again to see the differences.","8d72c69f":"### Prepare Test table","f9e70101":"This model got a RMSE of 0.11754 on Submission which corresponded to Top 19% by 29\/8\/2019.","cfcc289a":"## Prediction","97264621":"### Predict and save","12ab8555":"## Feature Selection","1cd8945c":"## Making Predictions"}}