{"cell_type":{"4f24c613":"code","288eedcc":"code","98c88537":"code","10336ad1":"code","832b2852":"code","a6e6cba1":"code","a7111410":"code","626813fb":"code","47b1098c":"code","ce356744":"code","f9018c5a":"code","ae875142":"code","ed07a866":"code","e49191b6":"code","f49b8abe":"code","fbe17acf":"code","5acd4e72":"code","6d15047e":"code","85cb8fbd":"code","4c2b5ded":"code","198d813f":"code","36b7a080":"code","21d67512":"code","155659a8":"code","85be3977":"code","380132c5":"code","eae5e60e":"code","2ac68070":"code","49c011e1":"code","cf2482b4":"code","9e6f5d21":"code","5a4dec8f":"code","52c540fd":"code","fbf0c74f":"code","f0ce9d8a":"code","b5f7e23f":"markdown","82109d1e":"markdown","bc35ee05":"markdown","744735ec":"markdown","57bb1e34":"markdown","bd85eafa":"markdown","3774c93d":"markdown","d67a4e52":"markdown","506023e0":"markdown","2dff7f93":"markdown","fe11400b":"markdown","721c3523":"markdown","099ec36c":"markdown","c4239de0":"markdown","64574908":"markdown","4311597f":"markdown","7d9db13e":"markdown","893b3c6c":"markdown"},"source":{"4f24c613":"import os\nimport random\nimport time\nimport IPython.display\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nfrom math import ceil\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_percentage_error","288eedcc":"def cleandata(data):\n    for col in data.columns:\n        data = data[data[col] != -9999.0]\n        data = data[data[col] != -9999.99]\n    data = data[data['wv (m\/s)'] >= 0]\n    return data\n\ndef findnanrows(df):\n    is_NaN = df.isnull() \n    row_has_NaN = is_NaN.any(axis=1) \n    rows_with_NaN = df[row_has_NaN] \n    return rows_with_NaN\n\ndef interpolatedata(df):\n    filldf = df.groupby(pd.Grouper(freq='10T')).mean()\n    dfnan = findnanrows(filldf)\n    print(\"==> %s rows have been filled <==\" %len(dfnan))\n    filldf = filldf.interpolate().round(2)\n    return filldf","98c88537":"class VizData():\n    def __init__(self, data):\n        self.data = data\n        \n    def plotfeatures(self, mode, title):\n        features = self.data.columns.tolist()\n        plot_features = self.data.groupby(pd.Grouper(freq=str(mode)+'T')).mean()\n        plot_features = plot_features[features]\n        plot_features.index = plot_features.index\n        ncols = 2\n        nrows = ceil(len(features)\/ncols)\n        fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 10), facecolor=\"w\", edgecolor=\"k\")\n        for i, feature in enumerate(features):\n            axes[i \/\/ ncols, i % ncols].plot(plot_features[feature])\n            axes[i \/\/ ncols, i % ncols].set_title(f'{feature} - {title}')     \n        plt.tight_layout()\n        plt.show()\n\n    def plothist2d(self, feature):\n        f = self.data.columns.tolist()\n        features = [x for x in f if x != feature]\n        ncols = 2\n        nrows = ceil(len(features)\/ncols)\n        fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 10))\n        for i, feature_x in enumerate(features):\n            if i == nrows * ncols - 2:\n                ax = plt.subplot(nrows,1,ncols)\n                counts, xedges, yedges, img = plt.hist2d(self.data[feature_x], self.data[feature], bins=(50, 50))\n                ax.set_xlabel(f'{feature_x}')\n                ax.set_ylabel(f'{feature}')\n                ax.axis('tight')  \n                plt.colorbar(img, ax=ax)\n                plt.gca()\n            else:\n                ax = plt.subplot(nrows,ncols,i+1)\n                counts, xedges, yedges, img = ax.hist2d(self.data[feature_x], self.data[feature], bins=(50, 50))\n                ax.set_xlabel(f'{feature_x}')\n                ax.set_ylabel(f'{feature}')\n                ax.axis('tight')  \n                plt.colorbar(img, ax=ax)\n                plt.gca()\n        plt.show()\n\n    def plotbox(self, mode):\n        plot_features = self.data.groupby(pd.Grouper(freq=str(60)+'T')).mean().copy()\n        plot_features[mode] = [eval('x.%s'%mode) for x in plot_features.index] \n        plot_features.boxplot('wv (m\/s)', by=mode, figsize=(12, 8), grid=False)\n        plt.show()","10336ad1":"weather = pd.read_csv(\"..\/input\/jena-weather-dataset\/jena_weather_2004_2020.csv\", parse_dates=True, index_col=\"Date Time\")\nweather.index.name = 'datetime'\nweather = weather[[x for x in weather.columns if x != 'CO2 (ppm)']]\n\nprint(\"Size before cleaning data:\", weather.shape)\nweather = cleandata(data=weather)\nprint(\"Size after cleaning data:\", weather.shape)\n\nprint(\"Size before filling data:\", weather.shape)\n#weather = filldata(weather)\nweather = interpolatedata(weather)\nprint(\"Size after filling data:\", weather.shape)\n\nweather.head()","832b2852":"weather.describe().transpose()","a6e6cba1":"usecols = [\"T (degC)\", \"rh (%)\", \"p (mbar)\", \"wv (m\/s)\"]\nweather = weather[usecols]\nprint(\"weather dataset shape:\", weather.shape)","a7111410":"vd = VizData(data=weather)\nvd.plotfeatures(mode=60, title=\"hourly\")\nvd.plotfeatures(mode=60*24, title=\"daily\")\nvd.plothist2d(feature=\"wv (m\/s)\")\nvd.plotbox(mode=\"hour\")\nvd.plotbox(mode=\"month\")","626813fb":"class DataPreprocessing():\n    def __init__(self, data, sampling_window, **kwargs):\n        self.data = data\n        self.sampling_window = sampling_window\n        self.trainsize = kwargs.get('trainsize', 0.7)\n        self.valsize = kwargs.get('valsize', 0.2)\n\n    def datasplit(self, **kwargs):\n        self.addcyclics = kwargs.get('addcyclics', False)\n        self.normalize = kwargs.get('normalize', None)\n        self.features = kwargs.get('features', None)\n        \n        self.resample_data = self.data.resample(str(self.sampling_window)+'T').mean()\n        \n        if self.resample_data.isna().sum().sum() > 0:\n            raise Exception(f\"Oops! there are some NaN values in resampled data.\")\n\n        if self.features is not None:\n            self.resample_data = self.resample_data[self.features]\n        if self.addcyclics:\n            self.resample_data = self.__class__.cyclical(self.resample_data)   \n            self.features = self.resample_data.columns\n        \n        self.train_df = self.resample_data[0:int(len(self.resample_data)*self.trainsize)]\n        self.val_df = self.resample_data[int(len(self.resample_data)*self.trainsize):int(len(self.resample_data)*(self.trainsize+self.valsize))]\n        self.test_df = self.resample_data[int(len(self.resample_data)*(self.trainsize+self.valsize)):]\n                \n        if self.normalize is not None:\n            if self.normalize == 'MinMaxScaler':\n                self.scaler = MinMaxScaler(feature_range=(0, 1))\n            elif self.normalize == 'StandardScaler':\n                self.scaler = StandardScaler()\n            # normalizing input features\n            self.train_df = pd.DataFrame(self.scaler.fit_transform(self.train_df), columns=self.train_df.columns, index=self.train_df.index)\n            self.val_df = pd.DataFrame(self.scaler.transform(self.val_df), columns=self.val_df.columns, index=self.val_df.index)\n            self.test_df = pd.DataFrame(self.scaler.transform(self.test_df), columns=self.test_df.columns, index=self.test_df.index)\n            \n        return self.train_df, self.val_df, self.test_df\n    \n    @staticmethod\n    def cyclical(data):\n        data = data.copy()\n        # Extracting the hour of day\n        data[\"hour\"] = [x.hour for x in data.index]\n        # Creating the cyclical daily feature \n        data[\"day_cos\"] = [np.cos(x * (2 * np.pi \/ 24)) for x in data[\"hour\"]]\n        data[\"day_sin\"] = [np.sin(x * (2 * np.pi \/ 24)) for x in data[\"hour\"]]\n        # Extracting the timestamp from the datetime object \n        data[\"timestamp\"] = [x.timestamp() for x in data.index]\n        # Seconds in day \n        s = 24 * 60 * 60\n        # Seconds in year \n        year = (365.25) * s\n        data[\"month_cos\"] = [np.cos((x) * (2 * np.pi \/ year)) for x in data[\"timestamp\"]]\n        data[\"month_sin\"] = [np.sin((x) * (2 * np.pi \/ year)) for x in data[\"timestamp\"]]\n        data = data.drop(['hour', 'timestamp'], axis=1)\n        return data","47b1098c":"class WindowGenerator():\n    def __init__(self, input_width, label_width, shift,\n                 batch_size, sequence_stride,\n                 train_df, val_df, test_df,\n                 label_columns=None):\n        # Store the raw data.\n        self.train_df = train_df\n        self.val_df = val_df\n        self.test_df = test_df\n        self.batch_size = batch_size\n        self.sequence_stride = sequence_stride\n\n        # Work out the label column indices.\n        self.label_columns = label_columns\n        if label_columns is not None:\n            self.label_columns_indices = {name: i for i, name in\n                                        enumerate(label_columns)}\n            self.column_indices = {name: i for i, name in\n                               enumerate(train_df.columns)}\n\n        # Work out the window parameters.\n        self.input_width = input_width\n        self.label_width = label_width\n        self.shift = shift\n\n        self.total_window_size = input_width + shift + label_width\n\n        self.input_slice = slice(0, input_width)\n        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n        self.label_start = self.input_width + self.shift\n        self.labels_slice = slice(self.label_start, None)\n        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n    def __repr__(self):\n        return '\\n'.join([\n            f'Total window size: {self.total_window_size}',\n            f'Input indices: {self.input_indices}',\n            f'Label indices: {self.label_indices}',\n            f'Offset: {self.shift}',\n            f'Label column name(s): {self.label_columns}',\n        ])\n\n    def split_window(self, features):\n        inputs = features[:, self.input_slice, :]\n        labels = features[:, self.labels_slice, :]\n        if self.label_columns is not None:\n            labels = tf.stack(\n                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n                axis=-1)\n\n        # Slicing doesn't preserve static shape information, so set the shapes\n        # manually. This way the `tf.data.Datasets` are easier to inspect.\n        inputs.set_shape([None, self.input_width, None])\n        labels.set_shape([None, self.label_width, None])\n        return inputs, labels\n    \n    def make_dataset(self, data, sequence_stride):\n        data = np.array(data, dtype=np.float32)\n        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n            data=data,\n            targets=None,\n            sequence_length=self.total_window_size,\n            sequence_stride=sequence_stride, \n            shuffle=False,\n            batch_size=self.batch_size,\n        )\n        ds = ds.map(self.split_window)\n        return ds\n    \n    @property\n    def train(self):\n        return self.make_dataset(self.train_df, self.sequence_stride)\n    @property\n    def val(self):\n        return self.make_dataset(self.val_df, self.sequence_stride)\n    @property\n    def test(self):\n        return self.make_dataset(self.test_df, self.sequence_stride)\n    \n    @property\n    def flattrain(self):\n        return self.make_dataset(self.train_df, self.label_width)\n    @property\n    def flatval(self):\n        return self.make_dataset(self.val_df, self.label_width)\n    @property\n    def flattest(self):\n        return self.make_dataset(self.test_df, self.label_width)\n \n    def randomplots(self, mc=None, max_subplots=3, **kwargs):   \n        plot_col = kwargs.get('plot_col', self.label_columns[0])\n\n        real_x = np.concatenate(list(map(lambda x: x[0].numpy(), self.flattest)))\n        real_y = np.concatenate(list(map(lambda x: x[1].numpy(), self.flattest)))  \n\n        plot_col_index = self.column_indices[plot_col]\n        indexes = random.sample(range(len(real_x)), max_subplots)\n\n        if mc is not None:\n            reshape_real_x = mc.reshape(real_x)\n            max_subplots = max_subplots + 1\n\n        plt.figure(figsize=(12, 8))\n        for n in range(max_subplots):\n            plt.subplot(max_subplots, 1, n+1)\n            if n == max_subplots - 1 and mc is not None: \n                plt.plot(real_y[:, :, label_col_index].reshape(-1, 1))\n                plt.plot(predictions.reshape(-1, 1))\n\n            else: \n                plt.plot(self.input_indices, real_x[indexes[n], :, plot_col_index], \n                         label='Inputs', marker='.', zorder=-10)\n\n                if self.label_columns:\n                    label_col_index = self.label_columns_indices.get(plot_col, None)\n                else:\n                    label_col_index = plot_col_index\n\n                if label_col_index is None:\n                    continue\n\n                plt.scatter(self.label_indices, real_y[indexes[n], :, label_col_index], \n                            marker='p', edgecolors='k', label='Labels', \n                            c='#2ca02c', s=64)\n                if mc is not None:\n                    predictions = mc.model.predict(reshape_real_x)\n                    plt.scatter(self.label_indices, predictions[indexes[n], :], \n                                marker='*', edgecolors='k', label='Predictions',\n                                c='#ff7f0e', s=64)\n            if n == 0:\n                plt.legend()\n                plt.title(f'{plot_col} [scaled]')\n        plt.xlabel('Timesteps')\n        \n    def plotforecast(self, mc, title, **kwargs):\n        withinputs = kwargs.get('withinputs', False)\n        windows = kwargs.get('windows', None)\n        labels = [\"Actual values\", \"Predicted values\"]\n\n        real_x = np.concatenate(list(map(lambda x: x[0].numpy(), self.flattest)))\n        real_y = np.concatenate(list(map(lambda x: x[1].numpy(), self.flattest)))\n        pred_y = mc.model.predict(mc.reshape(real_x))\n\n        real_y = real_y.reshape(-1, 1)\n        pred_y = pred_y.reshape(-1, 1)\n\n        plt.figure(figsize=(15,5))\n        if withinputs:\n            plt.plot(self.test_df[self.label_columns].values[:self.input_width+len(pred_y[:windows])], label=labels[0])\n            plt.plot(np.arange(self.input_width, self.input_width+len(pred_y[:windows])), pred_y[:windows], label=labels[1])\n        else:\n            plt.plot(real_y[:windows], label=labels[0])\n            plt.plot(pred_y[:windows], label=labels[1])\n        plt.title(f'{title}: {self.input_width} previous timesteps-based {self.label_width} future timesteps forecasting')\n        plt.xlabel(\"Test dataset points\")\n        plt.legend(frameon=False)\n        plt.show()","ce356744":"class ModelingClass():\n    def __init__(self, mapdataset, epochs, patience, learning_rate, loss, metrics, verbose=1):\n        self.mapdataset = mapdataset\n        self.epochs = epochs\n        self.patience = patience\n        self.learning_rate = learning_rate\n        self.verbose = verbose\n        self.loss = loss \n        self.metrics = metrics\n        self.label_feature_number = list(map(lambda x: x[1].numpy(), mapdataset.train.take(1)))[0].shape[2]\n \n    def reshape(self, data):\n        return data\n    \n    # build the model\n    def modelbuild(self):\n        # define model\n        model = None\n        return model\n        \n    def traincallback(self):\n        # simple early stopping\n        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=self.patience, verbose=1)\n        return early_stopping \n    \n    # train the model\n    def train(self):\n        # Building the model\n        self.model = self.modelbuild()\n        # Initiating the optimizer\n        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n        # Instantiate an optimizer before passing it to model.compile() or pass it by its string identifier.\n        self.model.compile(loss=self.loss, optimizer=optimizer, metrics=self.metrics)\n        # fit network\n        if (self.mapdataset.val is not None):\n            self.history = self.model.fit(\n                self.mapdataset.train,\n                validation_data=self.mapdataset.val,    \n                epochs=self.epochs, \n                verbose=self.verbose, \n                shuffle=False,\n                callbacks=[self.traincallback()],\n            )\n        else:\n            self.history = self.model.fit(\n                self.mapdataset.train,    \n                epochs=self.epochs, \n                verbose=self.verbose, \n                shuffle=False,\n                callbacks=[self.traincallback()],\n            )\n        return self.history\n    \n    def computemetrics(self, mapdata): \n        real_y = np.concatenate(list(map(lambda x: x[1].numpy(), mapdata)))\n        real_y = real_y.reshape(real_y.shape[0], real_y.shape[1])\n        pred_y = self.model.predict(mapdata)\n        pred_y = pred_y.reshape(pred_y.shape[0], pred_y.shape[1])\n\n        RMSE = mean_squared_error(real_y, pred_y, squared=False)\n        MSE = mean_squared_error(real_y, pred_y)\n        MAE = mean_absolute_error(real_y, pred_y)\n        MAPE = 100*mean_absolute_percentage_error(real_y, pred_y)\n        R2 = r2_score(real_y, pred_y)   \n        # calculate the metric score for each timestep\n        rmse_scores = mean_squared_error(real_y, pred_y, multioutput='raw_values', squared=False)\n        mse_scores = mean_squared_error(real_y, pred_y, multioutput='raw_values')\n        mae_scores = mean_absolute_error(real_y, pred_y, multioutput='raw_values')\n        mape_scores = 100*mean_absolute_percentage_error(real_y, pred_y, multioutput='raw_values')\n        r2_scores = r2_score(real_y, pred_y, multioutput='raw_values')\n        print('rmse: %.4f [%s]' %(RMSE, ', '.join(['%.4f' % s for s in rmse_scores])))\n        print('mse : %.4f [%s]' %(MSE, ', '.join(['%.4f' % s for s in mse_scores])))\n        print('mae : %.4f [%s]' %(MAE, ', '.join(['%.4f' % s for s in mae_scores])))\n        print('mape: %.4f [%s]' %(MAPE, ', '.join(['%.4f' % s for s in mape_scores])))\n        print('r2  : %.4f [%s]' %(R2, ', '.join(['%.4f' % s for s in r2_scores])))\n    \n    @property\n    def trainmetrics(self):\n        return self.computemetrics(self.mapdataset.train)\n    @property\n    def valmetrics(self):\n        return self.computemetrics(self.mapdataset.val)\n    @property\n    def testmetrics(self):\n        return self.computemetrics(self.mapdataset.test)\n    \n    def lossplot(self, **kwargs):\n        if len(self.model.metrics_names) <= 2:\n            l, c = 1, 2\n        else:\n            l, c = (len(self.model.metrics_names)\/\/2)+1, 2\n        title = kwargs.get('title', \"\")\n        epochs = range(len(self.history.history[self.model.metrics_names[0]]))\n        plt.figure(figsize=(15,3.5*l))\n        for i in range(len(self.model.metrics_names)):\n            plt.subplot(l,c,i+1)\n            if self.model.metrics_names[i] == \"loss\":\n                plt.plot(epochs, self.history.history[self.model.metrics_names[i]], \"b\", label=\"Training \"+self.loss[0]+'*')\n                plt.plot(epochs, self.history.history[\"val_\"+self.model.metrics_names[i]], \"green\", label=\"Validation \"+\"val_\"+self.loss[0]+'*')\n                plt.xlabel(\"Epochs\")\n                plt.ylabel(self.loss[0]+'*')\n                plt.legend(frameon=False)\n            else:\n                plt.plot(epochs, self.history.history[self.model.metrics_names[i]], \"b\", label=\"Training \"+self.model.metrics_names[i])\n                plt.plot(epochs, self.history.history[\"val_\"+self.model.metrics_names[i]], \"green\", label=\"Validation \"+\"val_\"+self.model.metrics_names[i])\n                plt.xlabel(\"Epochs\")\n                plt.ylabel(self.model.metrics_names[i])\n                plt.legend(frameon=False)\n        plt.suptitle(\"Training and validation losses: {}\".format(title))\n        plt.subplots_adjust(hspace=0.4)\n        plt.show()\n    \n    def modelevaluate(self, train_performance, val_performance, test_performance):\n        for x in [\"train\", \"val\", \"test\"]:\n            print(\"\\nModel evaluation on %s dataset:\\n\"%x)\n            eval(\"%s_performance\"%x)[self.__class__.__name__] = self.model.evaluate(eval(\"self.mapdataset.%s\"%x))\n            print(list(map(lambda x: round(x, 4), eval(\"%s_performance\"%x)[self.__class__.__name__])))\n        return train_performance, val_performance, test_performance","f9018c5a":"sampling_window = 60\naddcyclics = False\nnormalize = True\n#normalize_type = \"StandardScaler\"\nnormalize_type = \"MinMaxScaler\"\n\nn_input, n_output = 48, 7 #48, 7\nsequence_stride = 1 \nepochs, batch_size, learning_rate, patience = 200, 2**7, 0.001, 10 #150, 2**7, 0.001, 10\nloss, metrics = ['mse'], ['mae', 'mape', tfa.metrics.RSquare(name='r2', dtype=tf.float32, y_shape=(n_output, 1))] \n\nfeatures = [\"p (mbar)\", \"T (degC)\", \"rh (%)\", \"wv (m\/s)\"]\n\ntrain_performance = {}\nval_performance = {}\ntest_performance = {}","ae875142":"def run(data, modelingclassname, title, train_performance, val_performance, test_performance):\n    \n    start = time.time()\n    \n    dp = DataPreprocessing(data=data, sampling_window=sampling_window, trainsize=0.7, valsize=0.2)\n    train_df, val_df, test_df = dp.datasplit(features=features, addcyclics=addcyclics, normalize=normalize_type)\n\n    mw = WindowGenerator(\n        train_df=train_df, \n        val_df=val_df, \n        test_df=test_df,\n        input_width=n_input,\n        label_width=n_output,\n        shift=0,\n        label_columns=['wv (m\/s)'],\n        batch_size=batch_size,\n        sequence_stride=sequence_stride,\n    )       \n        \n    mc = modelingclassname(\n        mapdataset=mw,\n        epochs=epochs,\n        patience=patience,\n        learning_rate=learning_rate,\n        loss=loss, \n        metrics=metrics)\n\n    history = mc.train()\n    \n    IPython.display.clear_output()\n    \n    print(mw)\n    \n    for example_inputs, example_labels in mw.train.take(1):\n    #for example_inputs, example_labels in mw.train:\n        print(f'\\nInput shape (batch size, timesteps, features): {example_inputs.shape}')\n        print(f'Label shape (batch size, timesteps, features): {example_labels.shape}\\n')\n\n    display(tf.keras.utils.plot_model(mc.model, mc.__class__.__name__+\"_model_with_shape_info.png\", show_shapes=True, show_layer_names=True))\n        \n    train_performance, val_performance, test_performance = mc.modelevaluate(train_performance, val_performance, test_performance)\n        \n    print(\"\\nModeling metrics on train data set:\\n\")\n    mc.trainmetrics\n    print(\"\\nModeling metrics on validation data set:\\n\")\n    mc.valmetrics\n    print(\"\\nModeling metrics on test data set:\\n\")\n    mc.testmetrics\n\n    mc.lossplot(title=title)\n \n    mw.randomplots(mc)\n\n    mw.plotforecast(mc, title=title, windows=500, withinputs=True)\n    \n    print(\"Time taken with the %s model: %.2f sec\" %(mc.__class__.__name__, time.time()-start))\n    \n    return dp, mw, mc, train_performance, val_performance, test_performance","ed07a866":"class Linear(ModelingClass):\n    # build the model\n    def modelbuild(self):\n        # define model\n        model = tf.keras.Sequential([\n            # Take the last time-step.\n            # Shape [batch, time, features] => [batch, 1, features]\n            tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n            # Shape => [batch, 1, out_steps*features]\n            tf.keras.layers.Dense(self.mapdataset.label_width*self.label_feature_number, kernel_initializer=tf.initializers.zeros),\n            # Shape => [batch, out_steps, features]\n            tf.keras.layers.Reshape([self.mapdataset.label_width, self.label_feature_number])\n        ])\n        return model","e49191b6":"dp_linear, mw_linear, mc_linear, train_performance, val_performance, test_performance = run(\n    data=weather, modelingclassname=Linear, title=\"Linear\", \n    train_performance=train_performance, val_performance=val_performance, test_performance=test_performance)","f49b8abe":"class CNN(ModelingClass):\n    \n    def __init__(self, mapdataset, epochs, patience, learning_rate, loss, metrics, verbose=1, conv_width=3):\n        super().__init__(mapdataset, epochs, patience, learning_rate, loss, metrics, verbose=1)\n        self.conv_width = conv_width\n        \n    # build the model\n    def modelbuild(self):\n        # define model\n        model = tf.keras.Sequential([\n            # Shape [batch, time, features] => [batch, conv_width, features]\n            tf.keras.layers.Lambda(lambda x: x[:, -self.conv_width:, :]),\n            # Shape => [batch, 1, conv_units]\n            tf.keras.layers.Conv1D(filters=256, kernel_size=self.conv_width, activation='relu'),\n            # Shape => [batch, 1,  out_steps*features]\n            tf.keras.layers.Dense(self.mapdataset.label_width*self.label_feature_number, kernel_initializer=tf.initializers.zeros),\n            ###\n            # Shape => [batch, out_steps, features]\n            tf.keras.layers.Reshape([self.mapdataset.label_width, self.label_feature_number])\n            ###\n        ])\n        return model","fbe17acf":"dp_cnn, mw_cnn, mc_cnn, train_performance, val_performance, test_performance = run(\n    data=weather, modelingclassname=CNN, title=\"CNN\", \n    train_performance=train_performance, val_performance=val_performance, test_performance=test_performance)","5acd4e72":"class LSTM(ModelingClass):\n    # build the model\n    def modelbuild(self):\n        # define model        \n        model = tf.keras.Sequential([\n            # Shape [batch, time, features] => [batch, lstm_units]\n            # Adding more `lstm_units` just overfits more quickly.\n            tf.keras.layers.LSTM(32, return_sequences=False),\n            # Shape => [batch, out_steps*features]\n            tf.keras.layers.Dense(self.mapdataset.label_width*self.label_feature_number, kernel_initializer=tf.initializers.zeros),\n            # Shape => [batch, out_steps, features]\n            tf.keras.layers.Reshape([self.mapdataset.label_width, self.label_feature_number])\n        ])\n        return model","6d15047e":"dp_lstm, mw_lstm, mc_lstm, train_performance, val_performance, test_performance = run(\n    data=weather, modelingclassname=LSTM, title=\"LSTM\", \n    train_performance=train_performance, val_performance=val_performance, test_performance=test_performance)","85cb8fbd":"class StackedLSTM(ModelingClass):\n    # build the model\n    def modelbuild(self):\n        # define model\n        model = tf.keras.Sequential([\n            # Shape [batch, time, features] => [batch, lstm_units]\n            # Adding more `lstm_units` just overfits more quickly.\n            tf.keras.layers.LSTM(10, return_sequences=True),\n            tf.keras.layers.Dropout(0.2),\n            #tf.keras.layers.LSTM(10, return_sequences=True),\n            #tf.keras.layers.Dropout(0.2),\n            #tf.keras.layers.LSTM(5, return_sequences=True),\n            #tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.LSTM(5),\n            tf.keras.layers.Dropout(0.2),\n            # Shape => [batch, out_steps*features]\n            tf.keras.layers.Dense(self.mapdataset.label_width*self.label_feature_number, kernel_initializer=tf.initializers.zeros),\n            ###\n            # Shape => [batch, out_steps, features]\n            tf.keras.layers.Reshape([self.mapdataset.label_width, self.label_feature_number])\n            ###\n        ])\n        return model","4c2b5ded":"dp_slstm, mw_slstm, mc_slstm, train_performance, val_performance, test_performance = run(\n    data=weather, modelingclassname=StackedLSTM, title=\"Stacked LSTM\", \n    train_performance=train_performance, val_performance=val_performance, test_performance=test_performance)","198d813f":"class StackedGRU(ModelingClass):\n    # build the model\n    def modelbuild(self):\n        # define model\n        model = tf.keras.Sequential([\n            # Shape [batch, time, features] => [batch, lstm_units]\n            # Adding more `gru_units` just overfits more quickly.\n            tf.keras.layers.GRU(10, return_sequences=True),\n            tf.keras.layers.Dropout(0.2),\n            #tf.keras.layers.GRU(10, return_sequences=True),\n            #tf.keras.layers.Dropout(0.2),\n            #tf.keras.layers.GRU(5, return_sequences=True),\n            #tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.GRU(5),\n            tf.keras.layers.Dropout(0.2),\n            # Shape => [batch, out_steps*features]\n            tf.keras.layers.Dense(self.mapdataset.label_width*self.label_feature_number, kernel_initializer=tf.initializers.zeros),\n            ###\n            # Shape => [batch, out_steps, features]\n            tf.keras.layers.Reshape([self.mapdataset.label_width, self.label_feature_number])\n            ###\n        ])\n        return model","36b7a080":"dp_sgru, mw_sgru, mc_sgru, train_performance, val_performance, test_performance = run(\n    data=weather, modelingclassname=StackedGRU, title=\"Stacked GRU\", \n    train_performance=train_performance, val_performance=val_performance, test_performance=test_performance)","21d67512":"class StackedBiLSTM(ModelingClass):\n    # build the model\n    def modelbuild(self):\n        # define model\n        model = tf.keras.Sequential([\n            # Shape [batch, time, features] => [batch, lstm_units]\n            # Adding more `gru_units` just overfits more quickly.\n            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(10, return_sequences=True)),\n            tf.keras.layers.Dropout(0.2),\n            #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(10, return_sequences=True)),\n            #tf.keras.layers.Dropout(0.2),\n            #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(5, return_sequences=True)),\n            #tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(5)),\n            tf.keras.layers.Dropout(0.2),\n            # Shape => [batch, out_steps*features]\n            tf.keras.layers.Dense(self.mapdataset.label_width*self.label_feature_number, kernel_initializer=tf.initializers.zeros),\n            ###\n            # Shape => [batch, out_steps, features]\n            tf.keras.layers.Reshape([self.mapdataset.label_width, self.label_feature_number])\n            ###\n        ])\n        return model","155659a8":"dp_sblstm, mw_sblstm, mc_sblstm, train_performance, val_performance, test_performance = run(\n    data=weather, modelingclassname=StackedBiLSTM, title=\"Stacked Bid LSTM\", \n    train_performance=train_performance, val_performance=val_performance, test_performance=test_performance)","85be3977":"class EnDeLSTM(ModelingClass):\n    # build the model \n    def modelbuild(self):\n        # define model\n        model = tf.keras.Sequential([\n            tf.keras.layers.LSTM(10, return_sequences=False),\n            tf.keras.layers.RepeatVector(self.mapdataset.label_width),   \n            tf.keras.layers.LSTM(10, activation='relu', return_sequences=True),\n            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(5, activation='relu')),\n            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(self.label_feature_number, kernel_initializer=tf.initializers.zeros)),\n        ])\n        return model","380132c5":"dp_edlstm, mw_edlstm, mc_edlstm , train_performance, val_performance, test_performance = run(\n    data=weather, modelingclassname=EnDeLSTM, title=\"Encoder Decoder LSTM\", \n    train_performance=train_performance, val_performance=val_performance, test_performance=test_performance)","eae5e60e":"class CNNLSTMEnDe(ModelingClass):\n    \n    def __init__(self, mapdataset, epochs, patience, learning_rate, loss, metrics, verbose=1, filters=64, conv_width=3):\n        super().__init__(mapdataset, epochs, patience, learning_rate, loss, metrics, verbose=1)\n        self.filters = filters\n        self.conv_width = conv_width\n        \n    # build the model \n    def modelbuild(self):\n        # define model\n        model = tf.keras.Sequential([            \n            tf.keras.layers.Conv1D(filters=self.filters, kernel_size=self.conv_width, activation='relu'),\n            tf.keras.layers.MaxPooling1D(pool_size=2),\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.RepeatVector(self.mapdataset.label_width),\n            tf.keras.layers.LSTM(10, activation='relu', return_sequences=True),\n            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(5, activation='relu')),\n            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(self.label_feature_number, kernel_initializer=tf.initializers.zeros)),\n        ])\n        return model","2ac68070":"dp_cnnlstmed, mw_cnnlstmed, mc_cnnlstmed , train_performance, val_performance, test_performance = run(\n    data=weather, modelingclassname=CNNLSTMEnDe, title=\"CNN-LSTM Encoder-Decoder\", \n    train_performance=train_performance, val_performance=val_performance, test_performance=test_performance)","49c011e1":"class ConvLSTMEnDe(ModelingClass):\n    \n    def __init__(self, mapdataset, epochs, patience, learning_rate, loss, metrics, verbose=1, filters=64):\n        super().__init__(mapdataset, epochs, patience, learning_rate, loss, metrics, verbose=1)\n        \n        self.filters = filters\n        \n        self.train_x = np.concatenate(list(map(lambda x: x[0].numpy(), mapdataset.train)))\n        #self.train_x = np.vstack(np.array(list(map(lambda x: x[0].numpy(), mapdataset.train)), dtype=object))\n        self.train_y = np.concatenate(list(map(lambda x: x[1].numpy(), mapdataset.train)))\n        self.val_x = np.concatenate(list(map(lambda x: x[0].numpy(), mapdataset.val)))\n        self.val_y = np.concatenate(list(map(lambda x: x[1].numpy(), mapdataset.val)))\n        self.test_x = np.concatenate(list(map(lambda x: x[0].numpy(), mapdataset.test)))\n        self.test_y = np.concatenate(list(map(lambda x: x[1].numpy(), mapdataset.test)))\n    \n    def getFactors(self, n):\n        factors = []\n        for i in range(1, n + 1):\n            if n % i == 0:\n                factors.append(i)\n        if len(factors) == 2:\n            return (factors[0], n)\n        else:\n            return (factors[1], int(n \/ factors[1]))\n    @property\n    def reshapeparams(self):\n        return self.getFactors(self.mapdataset.input_width)\n    \n    def reshape(self, data):\n        data = data.reshape(data.shape[0], self.reshapeparams[0], 1, self.reshapeparams[1], data.shape[2])\n        return data\n        \n    @property\n    def trainreshape(self):\n        return self.reshape(self.train_x)\n    @property\n    def valreshape(self):\n        return self.reshape(self.val_x)\n    @property\n    def testreshape(self):\n        return self.reshape(self.test_x)\n        \n    # build the model \n    def modelbuild(self):\n        # define model\n        model = tf.keras.Sequential([\n            tf.keras.Input(shape=(self.trainreshape.shape[1], self.trainreshape.shape[2], self.trainreshape.shape[3], self.trainreshape.shape[4])), \n            tf.keras.layers.ConvLSTM2D(filters=self.filters, kernel_size=(1, self.mapdataset.label_width), activation='relu'),\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.RepeatVector(self.train_y.shape[1]),\n            tf.keras.layers.LSTM(10, activation='relu', return_sequences=True),\n            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(5, activation='relu')),\n            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(self.label_feature_number)),\n            ])\n        return model\n\n    # train the model\n    def train(self):\n        # Building the model\n        self.model = self.modelbuild()\n        # Initiating the optimizer\n        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n        # Instantiate an optimizer before passing it to model.compile() or pass it by its string identifier.\n        self.model.compile(loss=self.loss, optimizer=optimizer, metrics=self.metrics)\n        # fit network\n        if (self.val_x is not None) & (self.val_y is not None):\n            self.history = self.model.fit(\n                self.trainreshape,\n                self.train_y,\n                validation_data=(self.valreshape, self.val_y),\n                epochs=self.epochs, \n                batch_size=self.mapdataset.batch_size, \n                verbose=self.verbose, \n                shuffle=False,\n                callbacks=[self.traincallback()],\n            )\n        else:           \n            self.history = self.model.fit(\n                self.trainreshape,\n                self.train_y,\n                epochs=self.epochs, \n                batch_size=self.mapdataset.batch_size, \n                verbose=self.verbose, \n                shuffle=False,\n                callbacks=[self.traincallback()],\n            )\n        return self.history\n    \n    def modelevaluate(self, train_performance, val_performance, test_performance):\n        for x in [\"train\", \"val\", \"test\"]:\n            print(\"\\nModel evaluation on %s dataset:\\n\"%x)\n            eval(\"%s_performance\"%x)[self.__class__.__name__] = self.model.evaluate(eval(\"self.%sreshape\"%x), eval(\"self.%s_y\"%x), batch_size=self.mapdataset.batch_size)  \n            print(list(map(lambda x: round(x, 4), eval(\"%s_performance\"%x)[self.__class__.__name__])))\n        return train_performance, val_performance, test_performance\n\n    def computemetrics(self, reshape_real_x, real_y): \n        real_y = real_y.reshape(real_y.shape[0], real_y.shape[1])\n        pred_y = self.model.predict(reshape_real_x)\n        pred_y = pred_y.reshape(pred_y.shape[0], pred_y.shape[1])\n\n        RMSE = mean_squared_error(real_y, pred_y, squared=False)\n        MSE = mean_squared_error(real_y, pred_y)\n        MAE = mean_absolute_error(real_y, pred_y)\n        MAPE = 100*mean_absolute_percentage_error(real_y, pred_y)\n        R2 = r2_score(real_y, pred_y)   \n        # calculate the metric score for each timestep\n        rmse_scores = mean_squared_error(real_y, pred_y, multioutput='raw_values', squared=False)\n        mse_scores = mean_squared_error(real_y, pred_y, multioutput='raw_values')\n        mae_scores = mean_absolute_error(real_y, pred_y, multioutput='raw_values')\n        mape_scores = 100*mean_absolute_percentage_error(real_y, pred_y, multioutput='raw_values')\n        r2_scores = r2_score(real_y, pred_y, multioutput='raw_values')\n        print('rmse: %.4f [%s]' %(RMSE, ', '.join(['%.4f' % s for s in rmse_scores])))\n        print('mse : %.4f [%s]' %(MSE, ', '.join(['%.4f' % s for s in mse_scores])))\n        print('mae : %.4f [%s]' %(MAE, ', '.join(['%.4f' % s for s in mae_scores])))\n        print('mape: %.4f [%s]' %(MAPE, ', '.join(['%.4f' % s for s in mape_scores])))\n        print('r2  : %.4f [%s]' %(R2, ', '.join(['%.4f' % s for s in r2_scores]))) \n    \n    @property\n    def trainmetrics(self):\n        return self.computemetrics(self.trainreshape, self.train_y)\n    @property\n    def valmetrics(self):\n        return self.computemetrics(self.valreshape, self.val_y)\n    @property\n    def testmetrics(self):\n        return self.computemetrics(self.testreshape, self.test_y)","cf2482b4":"dp_convlstmed, mw_convlstmed, mc_convlstmed , train_performance, val_performance, test_performance = run(\n    data=weather, modelingclassname=ConvLSTMEnDe, title=\"ConvLSTM Encoder-Decoder\", \n    train_performance=train_performance, val_performance=val_performance, test_performance=test_performance)","9e6f5d21":"def perfplot(train_performance, val_performance, test_performance, metric_name, anymodel, ylabel):\n    # set width of bar\n    barWidth = 0.25\n    # set height of bar      \n    metric_index = anymodel.metrics_names.index(metric_name)\n    train_metric = [v[metric_index] for v in train_performance.values()]\n    val_metric = [v[metric_index] for v in val_performance.values()]\n    test_metric = [v[metric_index] for v in test_performance.values()]\n    # Set position of bar on X axis\n    r1 = np.arange(len(test_performance))\n    r2 = [x + barWidth for x in r1]\n    r3 = [x + barWidth for x in r2]\n    # Make the plot\n    plt.figure(figsize=(14, 8))\n    plt.bar(r1, train_metric, width=barWidth, label='Train')\n    plt.bar(r2, val_metric, width=barWidth, label='Validation')\n    plt.bar(r3, test_metric, width=barWidth, label='Test')        \n    # Set axis label\n    plt.ylabel(ylabel)\n    # Add xticks on the middle of the group bars\n    plt.xticks([r + barWidth for r in range(len(test_performance))], \n               labels=test_performance.keys(), rotation=45)\n    plt.legend()\n    plt.show()\n    \n    print(pd.DataFrame(np.array([train_metric, val_metric, test_metric]),\n                 columns=test_performance.keys(), \n                 index=[\"Train\", \"Val\", \"Test\"]).round(4).transpose())","5a4dec8f":"perfplot(train_performance, val_performance, test_performance, metric_name='loss', anymodel=mc_slstm.model, ylabel='mse [wv (m\/s), scaled]')","52c540fd":"perfplot(train_performance, val_performance, test_performance, metric_name='mae', anymodel=mc_slstm.model, ylabel='mae [wv (m\/s), scaled]')","fbf0c74f":"perfplot(train_performance, val_performance, test_performance, metric_name='r2', anymodel=mc_slstm.model, ylabel='r2 [wv (m\/s), scaled]')","f0ce9d8a":"perfplot(train_performance, val_performance, test_performance, metric_name='mape', anymodel=mc_slstm.model, ylabel='mape [wv (m\/s), scaled]')","b5f7e23f":"<a id=\"[CNN-LSTMEncoder-Decodermodelresults]\"><\/a>\n### CNN-LSTM Encoder-Decoder model results","82109d1e":"<a id=\"[StackedBidirectionalLSTMmodelresults]\"><\/a>\n### Stacked Bidirectional LSTM model results","bc35ee05":"<a id=\"[StackedLSTMmodelresults]\"><\/a>\n### Stacked LSTM model results","744735ec":"<a id=\"[BuildingClasses]\"><\/a>\n## Building Classes","57bb1e34":"<a id=\"[Linearmodelresults]\"><\/a>\n### Linear model results","bd85eafa":"<a id=\"[StackedGRUmodelresults]\"><\/a>\n### Stacked GRU model results","3774c93d":"<a id=\"[Parametersetting]\"><\/a>\n## Parameter setting","d67a4e52":"<a id=\"[Performanceresults]\"><\/a>\n## Performance results","506023e0":"<a id=\"[Modelresults]\"><\/a>\n## Model results","2dff7f93":"<a id=\"[ConvLSTMEncoder-Decodermodelresults]\"><\/a>\n### ConvLSTM Encoder-Decoder model results","fe11400b":"<a id=\"[ConvolutionNeuralNetworkmodelresults]\"><\/a>\n### Convolution Neural Network (CNN) model results","721c3523":"```\ndef loaddata(sartyear, endyear):\n    urlpath = 'https:\/\/www.bgc-jena.mpg.de\/wetter\/'\n    urllist = []\n    df = pd.DataFrame()\n    for year in np.arange(sartyear, endyear, 1):\n        urllist.append(urlpath+\"mpi_roof_\"+str(year)+\"a.zip\")\n        urllist.append(urlpath+\"mpi_roof_\"+str(year)+\"b.zip\")\n    for url in urllist:\n        df = df.append(pd.read_csv(url, encoding='unicode_escape', parse_dates=True, index_col=\"Date Time\"))\n    df.index.name = 'datetime'\n    return df\n\ndf = loaddata(sartyear=2004, endyear=2021)\n```","099ec36c":"<a id=\"[Encoder-DecoderLSTMmodelresults]\"><\/a>\n### Encoder-Decoder LSTM model results","c4239de0":"<a id=\"[Someusefuldefinitions]\"><\/a>\n## Some useful definitions\n\n**What is Time-Series Forecasting ?**\n\nTime-Series forecasting basically means predicting future dependent variable (``y``) based on past independent variable (``x``).\n\n**What is Multivariate Forecasting ?**\n\nIf the model predicts dependent variable (``y``) based on one independent variable (``x``), it is called univariate forecasting. For Multivariate forecasting, it simply means predicting dependent variable (``y``) based on more than one independent variable (``x``).\n\n**What is Multi-step Forecasting ?**\n\nIf the model predicts a single value for next time-step, it is called one-step forecast. For Multi-step forecast, it means predicting few times-steps ahead.\n\n**What is Multivariate Multi-step Time-Series Forecasting ?**\n\nWith all methods combined, the model in this article will predict multi-step ahead of dependent variable (``y``) based on the past two independent variables (``x``).","64574908":"<a id=\"[Requiredpackages]\"><\/a>\n## Required packages","4311597f":"<a id=\"[Loadandpreparedataset]\"><\/a>\n## Load and prepare datasets","7d9db13e":"# Time series forecasting models for weather features\n\nThis report is an introduction to time series forecasting using Deep learning. It builds a few different styles of models including Convolutional and Recurrent Neural Networks (CNNs and RNNs). Unlike other machine learning algorithms, long short-term memory recurrent neural networks are capable of automatically learning features from sequence data, support multi-variate data, and can output a variable length sequences that can be used for multi-step forecasting. In this tutorial, you will discover how to develop long short-term memory recurrent neural networks for multi-step time series forecasting of weather features.\n\nThis document uses a <a href=\"https:\/\/www.bgc-jena.mpg.de\/wetter\/\" class=\"external\">weather time series dataset<\/a> recorded by the <a href=\"https:\/\/www.bgc-jena.mpg.de\" class=\"external\">Max Planck Institute for Biogeochemistry<\/a>. The dataset contains many different features such as air temperature, atmospheric pressure, and humidity... These ones were collected every 10 minutes, beginning in 2003. However, only the data collected between 2004 and 2020 will be used. \n\nThe table of contents is the following:\n\n* [Some useful definitions](#[Someusefuldefinitions])    \n* [Required packages](#[Requiredpackages])    \n* [Load and prepare dataset](#[Loadandpreparedataset])\n* [Building Classes](#[BuildingClasses])\n* [Parameter setting](#[Parametersetting])\n* [Model results](#[Modelresults])\n    * [Linear model results](#[Linearmodelresults])\n    * [Convolution Neural Network (CNN) model results](#[ConvolutionNeuralNetworkmodelresults])\n    * [Recurrent Neural Network (RNN\/LSTM) model results](#[RecurrentNeuralNetworkmodelresults])\n    * [Stacked LSTM model results](#[StackedLSTMmodelresults])\n    * [Stacked GRU model results](#[StackedGRUmodelresults])\n    * [Stacked Bidirectional LSTM model results](#[StackedBidirectionalLSTMmodelresults])\n    * [Encoder-Decoder LSTM model results](#[Encoder-DecoderLSTMmodelresults]) \n    * [CNN-LSTM Encoder-Decoder model results](#[CNN-LSTMEncoder-Decodermodelresults]) \n    * [ConvLSTM Encoder-Decoder model results](#[ConvLSTMEncoder-Decodermodelresults]) \n* [Performance results](#[Performanceresults])","893b3c6c":"<a id=\"[RecurrentNeuralNetworkmodelresults]\"><\/a>\n### Recurrent Neural Network (RNN\/LSTM) model results"}}