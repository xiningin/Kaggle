{"cell_type":{"7db02340":"code","32003274":"code","a67010b9":"code","6254010c":"code","7acd48a1":"code","42788c53":"code","285071bf":"code","812aec24":"code","99470c56":"code","882e412c":"code","fb0efcc9":"code","736a64fb":"code","614868eb":"code","5c9a4c9a":"code","145bc72d":"code","f7ec55bf":"code","47845bfd":"code","45898a99":"code","9ad2368a":"code","0b80ae43":"code","8dc5f233":"code","1bfdd1c1":"markdown","95379834":"markdown","5f6b0738":"markdown","fc49df16":"markdown","65ca3d07":"markdown","c752efd4":"markdown","d4df7048":"markdown","8a3d47de":"markdown","03435e6e":"markdown","9bb30572":"markdown","b7f0cbdd":"markdown","734df0c7":"markdown","b6d8a01c":"markdown","99710e7f":"markdown","dc98f7e7":"markdown","ca6f8639":"markdown","b06027d0":"markdown","4cd026fb":"markdown"},"source":{"7db02340":"!pip install --upgrade scikit-learn","32003274":"# Essentials \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Rand Index\nfrom sklearn.metrics.cluster import adjusted_rand_score\nfrom sklearn.metrics.cluster import rand_score\n\n# Encode labels\nfrom sklearn import preprocessing\n\n# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix","a67010b9":"# To make the code reproducable\nnp.random.seed(42)","6254010c":"mushrooms = pd.read_csv(\"..\/input\/mushroom-classification\/mushrooms.csv\")\nmushrooms.head()","7acd48a1":"target = mushrooms[['class']]\nsee_no_evil = mushrooms.drop(['class'], axis=1)\nsee_no_evil.head()","42788c53":"see_no_evil.info()","285071bf":"see_no_evil.nunique()","812aec24":"data = see_no_evil.drop(['veil-type'],axis=1)","99470c56":"encoder = preprocessing.LabelEncoder()\nencoded_data = data.apply(encoder.fit_transform)\nencoded_data.head()","882e412c":"!pip install kmodes","fb0efcc9":"from kmodes.kmodes import KModes\n\nkm = KModes(n_clusters=2, init='Cao', verbose=1)\nclusters = km.fit_predict(encoded_data)\npredicted_labels = pd.DataFrame(clusters, columns=['predicted-label'])","736a64fb":"predicted_labels.value_counts().plot.pie(autopct='%1.0f%%', pctdistance=0.7, labeldistance=1.1)","614868eb":"data['predicted-labels'] = clusters","5c9a4c9a":"encoded_target = target.apply(encoder.fit_transform)\nprint(f'in this encoding, {encoded_target.iloc[0].values} represents {target.iloc[0].values}')","145bc72d":"labels = pd.DataFrame()\nlabels['target'] = encoded_target.values.reshape(1, -1).tolist()[0]\nlabels['prediction'] = clusters\nlabels.value_counts([\"target\", \"prediction\"])","f7ec55bf":"ri = rand_score(encoded_target.values.reshape(1, -1)[0], clusters)\nari = adjusted_rand_score(encoded_target.values.reshape(1, -1)[0], clusters)\n\nprint(f'Rand Index: {ri}')\nprint(f'Adjusted Rand Index: {ari}')","47845bfd":"cf_matrix = confusion_matrix(encoded_target.values.reshape(1, -1)[0], clusters)\nlabels = ['True Neg','False Pos','False Neg','True Pos']\nlabels = np.asarray(labels).reshape(2,2)\nfig, ax = plt.subplots(1, 1)\nsns.heatmap(cf_matrix\/np.sum(cf_matrix), annot=labels, fmt='', cmap='Blues')\nax.set_ylabel('Target Labels')    \nax.set_xlabel('Predicted Labels')","45898a99":"from IPython.display import YouTubeVideo\n\nYouTubeVideo('-ORE0pp9QNk')","9ad2368a":"True_neg = cf_matrix[0,0]\nFalse_pos = cf_matrix[0,1]\nTrue_pos = cf_matrix[1,1]\nFalse_neg = cf_matrix[1,0]\n\naccuracy = (True_neg + True_pos)\/(True_neg + False_neg + True_pos + False_pos)\nrecall = (True_pos)\/(False_neg+True_pos)\nprecision = (True_pos)\/(False_pos + True_pos)\nF1_score = 2 * ((precision*recall)\/(precision+recall))","0b80ae43":"print(f'Accuracy: {accuracy}')\nprint(f'Recall: {recall}')\nprint(f'Precision: {precision}')\nprint(f'F1_score: {F1_score}')","8dc5f233":"mushrooms['clusters'] = clusters\nmushrooms.to_csv('.\/results.csv')","1bfdd1c1":"rand score is **0.78**, which is good. \ud83d\udc4d\n\n## Confusion Matrix\n\nNow, let's create a confusion matrix to compare our predicted labels with the actual target:","95379834":"after 5 different initializations, k-mode gives us the best results. let's add these labels to our original dataframe (`data`)","5f6b0738":"Yup, the `Dtype` of every column is `object`; so every one of them contain categorical data. the good news is, there are no _missing values_ so that's a relief. Let's check the number of unique values in each column. this helps us understand these categorical features better.","fc49df16":"In this notebook, I'll try to cluster mushroom data in two classes so that we can figure out which ones are poisonous and which ones are edible. \n\nWe'll go through data, we will perform pre-processing on it and then we will use our powerful clustering methods to help us detect delicious, friendly mushrooms from the killer ones.\n\nFirst, let's start by Importing libraries:","65ca3d07":"Now we can use it to cluster our data into two classes:","c752efd4":"# Libraries\n\nfirst we should add every library we want to use in the future. I've added comments so you'll know why I used every library. ","d4df7048":"# Load the Data","8a3d47de":"# Take a look at our data\nwith target values gone, it is now safe to investigate the data. first, let's take a look at the description of each column:\n* cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n* cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n* cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n* bruises: bruises=t,no=f\n* odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n* gill-attachment: attached=a,descending=d,free=f,notched=n\n* gill-spacing: close=c,crowded=w,distant=d\n* gill-size: broad=b,narrow=n\n* gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n* stalk-shape: enlarging=e,tapering=t\n* stalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n* stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n* stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n* stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n* stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n* veil-type: partial=p,universal=u\n* veil-color: brown=n,orange=o,white=w,yellow=y\n* ring-number: none=n,one=o,two=t\n* ring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n* spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n* population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n* habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d\n\n**Uh-oh, It looks like every column in this dataset contains _Categorical Features_! these kinds of data can be a pain in the A$$! \ud83e\udd26\u200d\u2642\ufe0f**\n\nLet's check again:","03435e6e":"# Train the Model\n\nNow it's time to train our clustering model. we know that our model should find 2 classes. so, the number of clusters is known. \n\nIt would be really convenient if we could just use the k-means clustering method; because it could simply give us two classes. but it's not possible because k-means works with Euclidean distance which is not meaningful on discrete data like ours. \n\n<div class=\"alert alert-warning\" role=\"alert\">\n  \u26a0 If you want to learn more about k-means clustering, read <a href=\"https:\/\/github.com\/HalflingWizard\/MachineLearning\/blob\/main\/3-%20Clustering\/K-Means.md\">my notes on this method<\/a>.\n<\/div>\n\nfortunately, there is an extention for this algorithm called **K-modes** that works on categorical data. there is a python implementation of it [here](https:\/\/github.com\/nicodv\/kmodes). \n\n> k-modes is used for clustering categorical variables. It defines clusters based on the number of matching categories between data points. (This is in contrast to the more well-known k-means algorithm, which clusters numerical data based on Euclidean distance.) The k-prototypes algorithm combines k-modes and k-means and is able to cluster mixed numerical \/ categorical data.\n\nLet's first install this library:","9bb30572":"Finally, I add the clusters to the dataset and save it as output.","b7f0cbdd":"## See no evil! \ud83d\ude48\nWe want to _Cluster_ our data. clustering is an _unsupervised_ task. so, before we do anything, we mush remove the target (`class`) column. we won't look at our targets until we have finished clustering; so we can evaluate our model.","734df0c7":"<center><img src=\"https:\/\/www.pngix.com\/pngfile\/big\/323-3231328_mario-mushroom-png.png\" alt=\"drawing\" width=\"200\"\/>\n    <h1>\ud83c\udf44 Shrooming!<\/h1>\n    <h3>K-mode Clustering on Mushrooms Dataset<\/h3>\n<\/center>","b6d8a01c":"hmmm... it seems that our predicted labels are aligned with the target labels. which means, Class 0 = `e` and Class 1 = `p`. \n\nThis is how we calculate rand index using sklearn:","99710e7f":"using the confusion matrix, we could calculate other evaluation metrics such as **accuracy**, **percision**, **recall** and **F1 Score**\n\n<div class=\"alert alert-danger\" role=\"alert\">\n  If you are not familiar with Confusion matrix and Classification Evaluation and Metrics, I recommend you watch my video on this subject \ud83d\udc47\n<\/div>","dc98f7e7":"# Evaluate our predictions\n\nNow we can finally look at our targets. I want to compare our predicted labels with the target classes and figure out wherher we've done a good job or not. \n\n## Rand Index\n\nFirst, I'm going to Calculate the **Rand Index**. The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings. Perfect labeling is scored 1.0.\n\nTo do so, I have to encode target labels:","ca6f8639":"# Preprocessing\n\nNow we should pre-process the data so that it is ready for an ML model. since all our data is categorical, there is no need to scale the data. also, there are no outliers. so, the only thing we should do is to encode our labels. ","b06027d0":"notice that `veil-type` has only one unique value, which means every data in this column is the same. since it doesn't help us classify our model, let's just get rid of it.","4cd026fb":"# Everything Looks good!\n\nCongrats! we did it! \ud83c\udf89\n\nWe successfully used K-Mods algorithm to cluster categorical data and the results were totally acceptable. \n\n<div class=\"alert alert-danger\" role=\"alert\" style=\"text-align:center;\">\n    I hope you enjoyed this tutorial. If you did, please consider subscribing to <b><a href=\"https:\/\/www.youtube.com\/channel\/UC34Gj0-vHuBiTNEYlP7wczg\">my YouTube Channel \u25b6<\/a><\/b>\n<\/div>\n\n<center><h2><span style=\"font-family:cursive;\"> Also, please Upvode! \ud83d\ude1c <\/span><\/h2><\/center>"}}