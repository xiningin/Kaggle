{"cell_type":{"b3668907":"code","865f775e":"code","80e85382":"code","b4b0864e":"code","bdd9db3d":"code","4815af20":"code","e6edd66e":"code","dff7d2ca":"code","eb16505a":"code","b54a3ac9":"code","a82a466b":"code","b7e19a40":"code","ab558303":"code","a5bb0f2a":"code","e0e533b1":"code","5d2a8fa4":"code","e1758332":"code","c62282ae":"code","af61af85":"code","e132d15b":"code","2bf76bfa":"code","429331e1":"code","56a15a6b":"code","58b09607":"code","17847863":"code","6883e73b":"code","8c3bcaf8":"code","c0082f71":"code","eba84a84":"code","6ec57c3f":"code","5bd28407":"code","9cdc8ee9":"code","01360de7":"code","eba4fa5e":"code","1f935635":"code","1fb4790d":"code","4f595bf0":"code","378788a0":"code","9dbbb0d7":"code","01cfdebf":"code","198eb773":"code","2cddcd59":"code","9fe510b3":"code","6f9c40c8":"code","040c6f4d":"code","2761d59f":"code","6b10b39a":"code","9c8de0d9":"code","4cf62b10":"code","441da1ca":"code","926f753e":"code","38f2cf5f":"code","783e1a42":"code","df0af9b1":"code","cb4cf3c3":"code","54fd022f":"code","1b4d2bab":"code","02e7e37b":"code","0baefc93":"code","f83d0dcb":"code","8ad29718":"code","c0c8eb25":"code","d904a653":"code","8e6fc175":"code","cc6c217e":"code","ad21cf3f":"code","83ca93cb":"code","8c552c03":"code","7af7b842":"code","4859ffa3":"code","14867f66":"code","043bbf83":"code","49366870":"code","433a5585":"code","125f6cb1":"code","1e6094b6":"code","697e81a5":"code","8b150097":"code","917b024a":"code","68325a0a":"code","8c8a6423":"code","030f526b":"code","c8c47ab7":"code","346f2ec6":"code","b2547ed0":"code","6232f77a":"code","84cd9c0a":"code","f6479872":"code","8ecba3d3":"code","ba05e7c1":"code","3e51ba4b":"code","b30adf4e":"code","d7bf43b4":"code","a85f7590":"markdown","1fad5e69":"markdown","b6c893ab":"markdown","d4d8067e":"markdown","18253f81":"markdown","db94aa03":"markdown","bd4e44b2":"markdown","73b0b91d":"markdown","cb9dbd54":"markdown","8925d87f":"markdown","ba21199f":"markdown","db0d3cd7":"markdown","7a6e5f24":"markdown","230fe1eb":"markdown","70a0c3df":"markdown","8261cdc4":"markdown","e613bac1":"markdown","308df8c5":"markdown","c83610f6":"markdown","8ba3da9d":"markdown","4f8a33ea":"markdown","d3f490c8":"markdown","54086a98":"markdown","d59bf2ff":"markdown","97ae9f68":"markdown"},"source":{"b3668907":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns","865f775e":"path = '..\/input\/sentimental-analysis-for-tweets\/sentiment_tweets3.csv'","80e85382":"df = pd.read_csv(path)","b4b0864e":"df.head()","bdd9db3d":"df['label (depression result)'].value_counts()","4815af20":"df.tail()","e6edd66e":"## Let's see some of the text\n\ndf.loc[10311]['message to examine']","dff7d2ca":"df.loc[3]['message to examine']","eb16505a":"df.loc[1234]['message to examine']","b54a3ac9":"# Checking if there is any null values.\n\ndf.isnull().sum()","a82a466b":"# Check if there's any number\n\nfor i in df['message to examine']:\n  for j in i.split():\n    if j.isdigit():\n      s = \"yes\"\n    else:\n      s = \"no\"\nprint(s)","b7e19a40":"import re","ab558303":"# First Remove all the numbers\n\ndef remove_numbers(text):\n  return ' '.join([i for i in str(text).split() if not i.isdigit()])\n\ndf['clean_tweets'] = df['message to examine'].apply(lambda x: remove_numbers(x))","a5bb0f2a":"df","e0e533b1":"# Lowercasing all the tweets\n\ndf['clean_tweets'] = df['clean_tweets'].str.lower()","5d2a8fa4":"# Removal of Weblinks\n\ndef remove_weblinks(text):\n  return re.sub(r\"http\\S+\", \"\", text)\n\ndf['clean_tweets2'] = df['clean_tweets'].apply(lambda x: remove_weblinks(x))","e1758332":"df","c62282ae":"def remove_twitter(text):\n  return re.sub('@[\\w]+','',text)","af61af85":"df['clean_tweets3'] = df['clean_tweets2'].apply(lambda x: remove_twitter(x))","e132d15b":"df","2bf76bfa":"import string","429331e1":"PUNCT_TO_REMOVE = string.punctuation","56a15a6b":"def remove_punctuation(text):\n  return text.translate(str.maketrans('','', PUNCT_TO_REMOVE))","58b09607":"df['clean_tweets4'] = df['clean_tweets3'].apply(lambda x: remove_punctuation(x))","17847863":"df","6883e73b":"import nltk\nnltk.download('stopwords')","8c3bcaf8":"from nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))","c0082f71":"STOPWORDS = set(stopwords.words('english'))\n\ndef remove_stopwords(text):\n  return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n","eba84a84":"df['clean_tweets5'] = df['clean_tweets4'].apply(lambda x: remove_stopwords(x))","6ec57c3f":"df","5bd28407":"from collections import Counter\ncnt = Counter()\n\nfor text in df['clean_tweets5'].values:\n  for word in text.split():\n    cnt[word] += 1\n\ncnt.most_common(10)","9cdc8ee9":"n_rare_words = 10\nRAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n\n# Let's see what are the Rarewords\n\nRAREWORDS","01360de7":"# Let's remove these\n\ndef remove_stopwords(text):\n  return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n","eba4fa5e":"df['clean_tweets6'] = df['clean_tweets5'].apply(lambda x: remove_stopwords(x))","1f935635":"nltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')","1fb4790d":"from nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\nwordnet_map = {\"n\": wordnet.NOUN, \"v\": wordnet.VERB, \"j\": wordnet.ADJ, \"r\": wordnet.ADV}\n\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.VERB)) for word, pos in pos_tagged_text])\n\ndf[\"text_lemmatized\"] = df['clean_tweets6'].apply(lambda text: lemmatize_words(text))\n","4f595bf0":"df","378788a0":"short_words = {\n\"aint\": \"am not\",\n\"arent\": \"are not\",\n\"cant\": \"cannot\",\n\"'cause\": \"because\",\n\"couldve\": \"could have\",\n\"couldnt\": \"could not\",\n\"didnt\": \"did not\",\n\"doesnt\": \"does not\",\n\"dont\": \"do not\",\n\"hadnt\": \"had not\",\n\"hasnt\": \"has not\",\n\"havent\": \"have not\",\n\"im\": \"I am\",\n\"em\": \"them\",\n\"ive\": \"I have\",\n\"isnt\": \"is not\",\n\"lets\": \"let us\",\n\"theyre\": \"they are\",\n\"theyve\": \"they have\",\n\"wasnt\": \"was not\",\n\"well\": \"we will\",\n\"were\": \"we are\",\n\"werent\": \"were not\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","9dbbb0d7":"def replace_short_words(text):\n  for word in text.split():\n    if word in short_words:\n      text = text.replace(word, short_words[word])\n  \n  return text","01cfdebf":"df[\"clean_tweets7\"] = df['text_lemmatized'].apply(lambda text: replace_short_words(text))","198eb773":"df","2cddcd59":"xdf = df[['Index','label (depression result)','clean_tweets7']]","9fe510b3":"xdf","6f9c40c8":"xdf.columns = ['Index','Labels','Tweets']","040c6f4d":"xdf","2761d59f":"# Plot the word cloud\n\nfrom wordcloud import WordCloud\n\nimport matplotlib.pyplot as plt","6b10b39a":"sentences = xdf['Tweets'].tolist()","9c8de0d9":"len(sentences)","4cf62b10":"# Joining sentences (combining all the sentences that we have)\n\njoined_sentences = \" \".join(sentences)","441da1ca":"plt.figure(figsize = (12,8))\nplt.imshow(WordCloud().generate(joined_sentences));","926f753e":"xdf['Labels'].value_counts()","38f2cf5f":"# Let's visualize postive and negative tweets","783e1a42":"positive_tweets = xdf[xdf['Labels'] == 0]\npositive_sentences = positive_tweets['Tweets'].tolist()\npositive_string = \" \".join(positive_sentences)","df0af9b1":"plt.figure(figsize = (12,8))\nplt.imshow(WordCloud().generate(positive_string));","cb4cf3c3":"# Let's visualize negative tweets","54fd022f":"negative_tweets = xdf[xdf['Labels'] == 1]\nnegative_sentences = negative_tweets['Tweets'].tolist()\nnegative_string = \" \".join(negative_sentences)","1b4d2bab":"plt.figure(figsize = (12,8))\nplt.imshow(WordCloud().generate(negative_string));","02e7e37b":"from sklearn.feature_extraction.text import TfidfVectorizer","0baefc93":"cv = TfidfVectorizer()","f83d0dcb":"tfidf = cv.fit_transform(xdf['Tweets'])","8ad29718":"from sklearn.model_selection import train_test_split","c0c8eb25":"tfX_train, tfX_test, tfy_train, tfy_test = train_test_split(tfidf, xdf['Labels'], test_size = 0.2)","d904a653":"tfX_train","8e6fc175":"tfX_train.shape","cc6c217e":"## NaiveBayes\n\nfrom sklearn.naive_bayes import MultinomialNB","ad21cf3f":"mnb = MultinomialNB()","83ca93cb":"mnb.fit(tfX_train, tfy_train)","8c552c03":"from sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred_mnb = mnb.predict(tfX_test)\n\nprint(f'Accuracy score is : {accuracy_score(tfy_test, y_pred_mnb)}')","7af7b842":"cf = confusion_matrix(tfy_test, y_pred_mnb, labels = [1,0])\ncf","4859ffa3":"x_axis_labels = [\"Positive(1)\",\"Negative(0)\"]\ny_axis_labels = [\"Positive(1)\",\"Negative(0)\"]\n\nplt.figure(figsize = (8,6))\nsns.set(font_scale=1)\nsns.heatmap(cf, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, fmt='g',annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\", fontsize = 20)\nplt.ylabel(\"Predicted Class\", fontsize = 20)\nplt.show()","14867f66":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, LSTM\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Sequential\nimport tensorflow_hub as hub\n","043bbf83":"# Load Pretrained Word2Vec\n\nembed = hub.load(\"https:\/\/tfhub.dev\/google\/Wiki-words-250\/2\")","49366870":"def get_max_length(df):\n  ## get max token counts from train data,\n  ## so we use this number as fixed length input to RNN cell\n\n  max_length = 0\n  for row in xdf['Tweets']:\n    if len(row.split(\" \")) > max_length:\n      max_length = len(row.split(\" \"))\n  \n  return max_length","433a5585":"get_max_length(xdf['Tweets'])","125f6cb1":"def get_word2vec_enc(tweets):\n  ## get word2vec value for each word in sentence\n  # concatenate word in numpy array, so we can use it as RNN input\n\n  encoded_tweets = []\n  for tweet in tweets:\n    tokens = tweet.split(\" \")\n    word2vec_embedding = embed(tokens)\n    encoded_tweets.append(word2vec_embedding)\n  return encoded_tweets\n","1e6094b6":"def get_padded_encoded_tweets(encoded_tweets):\n  # for short sentences, we prepend zero padding so all input to RNN\n  # has same length\n\n  padded_tweets_encoding = []\n  for enc_tweet in encoded_tweets:\n    zero_padding_cnt = max_length - enc_tweet.shape[0]\n    pad = np.zeros((1, 250))\n    for i in range(zero_padding_cnt):\n      enc_tweet = np.concatenate((pad, enc_tweet), axis = 0)\n    padded_tweets_encoding.append(enc_tweet)\n  return padded_tweets_encoding","697e81a5":"def sentiment_encode(sentiment):\n    if sentiment == 0:\n        return [0,1]\n    else:\n        return [1,0]","8b150097":"def preprocess(df):\n  # encode text value to numeric value\n\n  tweets = df['Tweets'].tolist()\n\n  encoded_tweets = get_word2vec_enc(tweets)\n  padded_encoded_tweets = get_padded_encoded_tweets(encoded_tweets)\n\n  #encoded sentiment\n  sentiments = df['Labels'].tolist()\n  encoded_sentiment = [sentiment_encode(sentiment) for sentiment in sentiments]\n\n  X = np.array(padded_encoded_tweets)\n  Y = np.array(encoded_sentiment)\n\n  return X, Y","917b024a":"# Preprocess \n\nmax_length = get_max_length(xdf)\nmax_length","68325a0a":"tdf = xdf.sample(frac = 1)\ntrain = tdf[:8000]\ntest = tdf[8000:]","8c8a6423":"train.shape, test.shape","030f526b":"train_X, train_Y = preprocess(train)\ntest_X, test_Y = preprocess(test)","c8c47ab7":"## Build Model","346f2ec6":"# LSTM\n\nmodel = Sequential()\nmodel.add(LSTM(32))\nmodel.add(Dense(2, activation = 'softmax'))","b2547ed0":"model.compile(loss = 'categorical_crossentropy',\n              optimizer = 'adam',\n              metrics = ['accuracy'])","6232f77a":"\nmodel.fit(train_X, train_Y, epochs = 10)","84cd9c0a":"model.summary()","f6479872":"score, acc = model.evaluate(test_X, test_Y, verbose = 2)\nprint(\"Test Score:\", score)\nprint(\"Test Accuracy:\", acc)","8ecba3d3":"# Confusion Matrix\n\ny_pred = model.predict(test_X)","ba05e7c1":"import sklearn.metrics as metrics","3e51ba4b":"matrix = metrics.confusion_matrix(test_Y.argmax(axis = 1), y_pred.argmax(axis = 1), labels = [1,0])","b30adf4e":"matrix","d7bf43b4":"x_axis_labels = [\"Positive(1)\",\"Negative(0)\"]\ny_axis_labels = [\"Positive(1)\",\"Negative(0)\"]\n\nplt.figure(figsize = (8,6))\nsns.set(font_scale=1)\nsns.heatmap(matrix, xticklabels = x_axis_labels, yticklabels = y_axis_labels, annot = True, fmt='g',annot_kws = {'size': 16})\nplt.xlabel(\"Actual Class\", fontsize = 20)\nplt.ylabel(\"Predicted Class\", fontsize = 20)\nplt.show()","a85f7590":"## Remove Numbers","1fad5e69":"## Replace some short words\n\nI see some of the short words like **dont**,  **im**. I've just used most frequent words used based on my knowledge, you can add up.\n","b6c893ab":"# Observe Dataset","d4d8067e":"Our text is clean and ready for training. But let's delete all the previous columns.","18253f81":"## Removal of Punctuations","db94aa03":"## Train","bd4e44b2":"## Removal of Stopwords","73b0b91d":"## Models and Evaluation ","cb9dbd54":"First, we will build our model with Basic Sentiment Analysis technique with **tf-idf** and **NaiveBayes Classifier**","8925d87f":"# Text Preprocessing : Part II\n\n- tf-idf \n- word2vec","ba21199f":"## Test","db0d3cd7":"## Lemmatization\n","7a6e5f24":"There are 802312 tweets, altogether.","230fe1eb":"Since, this is a sentiment analysis, **depression** word is important for sentiment analysis, let's keep this as it is.","70a0c3df":"## Remove Weblinks","8261cdc4":"### Basic Sentiment Analaysis","e613bac1":"# Sentiment Analysis for Tweets\n\nIn this notebook we will be doing sentiment analysis based on the given tweets. \n\n- Part One: \nWe will using cleaning the tweets (cleaning as: removing mentions @, removing links, removing stopwords, correcting short words (isn't : is not) and removing rare words)\n\n- Part Two:\nFirst, we will perform basic sentiment analysis with **Tf-idf** and using **Naive Bayes** algorithm\nAlso, we will perform advanced and better sentiment analysis with **Pre-trained WordEmbeddings** and **LSTM**\n\n\n# Outlines\n\n- **Observe Dataset**.\n\n\n- **Preprocessing Step I**\n    - Lowercasing all the tweets.\n    - Romoval of punctuations\n    - Removal of Stopwords\n    - Lemmatization\n    \n    \n- **Preprocessing Step II**\n    - Basic Tf-Idf\n    - Word Embedding\n    \n    \n- **Preparing for Models**\n    - Splitting Dataset\n    \n    \n- **Models**\n    - Naive Bayes\n    - LSTM","308df8c5":"## Remove Twitter Mentions","c83610f6":"## Rare words","8ba3da9d":"![1_fxiTNIgOyvAombPJx5KGeA.png](attachment:3509b09c-17ef-4731-a9f5-a0d057e6a7db.png)","4f8a33ea":"Footnotes:\n\nhttps:\/\/github.com\/minsuk-heo\/tf2\/blob\/master\/jupyter_notebooks\/10.Word2Vec_LSTM.ipynb\n\n\nhttps:\/\/www.kaggle.com\/sudalairajkumar\/getting-started-with-text-preprocessing","d3f490c8":"# Text Preprocessing\n\n- Lowercasing all the tweets.\n- Romoval of punctuations\n- Removal of Stopwords\n- Lemmatization\n\n","54086a98":"## Frequent Words","d59bf2ff":"## Splitting Dataset","97ae9f68":"### Better Sentiment Analaysis\n\nWe will improve our model with **LSTIM** and **Pre-trained Word Embeddings**\n\nThe use of pre-trained word embeddings. \n\nThe theory is that these pre-trained vectors already have words with similar semantic meaning close together in vector space, e.g. \"sad\", \"depressed\", \"bad\" are nearby. This gives our embedding layer a good initialization as it does not have to learn these relations from scratch."}}