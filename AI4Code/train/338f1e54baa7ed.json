{"cell_type":{"c4d9467f":"code","8ce1e008":"code","6a28c0c6":"code","9467a912":"code","33083ec7":"code","5fa3b472":"code","d5478211":"code","be59e4d8":"code","809fed29":"code","5172cbae":"code","b190270a":"code","60b5194c":"code","f6de0a46":"code","5a68f99d":"code","0ef5438e":"code","39cdaee7":"code","e67ff007":"code","4512f8f9":"code","17453e32":"code","dc396f03":"code","c4e362cd":"code","9b993dbc":"code","bbd8dc33":"code","db9645ac":"markdown","5c181832":"markdown","24a39b43":"markdown","b5787977":"markdown","93404b10":"markdown","e5bf18db":"markdown","664591ac":"markdown","69c001e2":"markdown","dc56ecd8":"markdown","552d4fee":"markdown","619501d0":"markdown","29e5dd67":"markdown","4e05a11c":"markdown","b1bc18e1":"markdown","4b018f55":"markdown","3059f0ab":"markdown","d9ec0cf4":"markdown","c5a6c100":"markdown"},"source":{"c4d9467f":"from tensorflow.keras.applications import VGG16","8ce1e008":"!pip install split-folders\n","6a28c0c6":"import splitfolders\nsplitfolders.ratio('\/kaggle\/input\/flowers-recognition\/flowers',output=\"\/kaggle\/output\/data\",seed=1337,ratio=(0.5,0.25,0.25))","9467a912":"conv_base = VGG16(\n    weights='imagenet',        # checkpoints from when to initialize the model\n    include_top=False,         # when True, includes the fully connected classifier network\n    input_shape=(150, 150, 3)  # shape of the image tensors to feed the network with\n)\nconv_base.summary()","33083ec7":"import os\nimport numpy as np\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","5fa3b472":"import os\n\nbase_dir = '\/kaggle\/output\/data'\n\nTRAIN_DIR = os.path.join(base_dir, 'train')\nVALIDATION_DIR = os.path.join(base_dir, 'val')\nTEST_DIR = os.path.join(base_dir, 'test')\nprint(TRAIN_DIR)\ndatagen = ImageDataGenerator(rescale=1.\/255)\nbatch_size=20","d5478211":"def extract_features(directory, sample_count):\n    features = np.zeros(shape=(sample_count, 4, 4, 512)) # shape of the output of the last layer of the conv_base\n    labels = np.zeros(shape=(sample_count,5))\n    generator = datagen.flow_from_directory(\n        directory,\n        target_size=(150, 150),\n        batch_size=batch_size,\n        class_mode='categorical'\n    )\n    \n    i = 0\n    for inputs_batch, labels_batch in generator:\n        features_batch = conv_base.predict(inputs_batch)\n        features[i * batch_size : (i + 1) * batch_size] = features_batch \n        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n        i += 1\n        if i * batch_size >= sample_count:\n        # Since generators yield data indefinitely in a loop, we must `break` after every image has been seen once.\n            break\n    return features, labels\n    \ntrain_features, train_labels = extract_features(TRAIN_DIR, 2158)\nvalidation_features, validation_labels = extract_features(VALIDATION_DIR, 1079)\ntest_features, test_labels = extract_features(TEST_DIR, 1080)","be59e4d8":"print(train_features.shape, validation_features.shape, test_features.shape,train_labels.shape, validation_labels.shape, test_labels.shape)","809fed29":"train_features = np.reshape(train_features, (2158, 4*4*512))\nvalidation_features = np.reshape(validation_features, (1079, 4*4*512))\ntest_features = np.reshape(test_features, (1080, 4*4*512))","5172cbae":"print(train_features.shape, validation_features.shape, test_features.shape,train_labels.shape, validation_labels.shape, test_labels.shape)","b190270a":"from tensorflow.keras import models, layers, optimizers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(256, activation='relu', input_dim=4*4*512))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(5, activation='softmax'))\n\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer=optimizers.RMSprop(lr=2e-5),\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    train_features,\n    train_labels,\n    epochs=30,\n    batch_size=20,\n    validation_data=(validation_features, validation_labels)\n)","60b5194c":"import matplotlib.pyplot as plt\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","f6de0a46":"from tensorflow.keras import models, layers, optimizers\nconv_base = VGG16(\n    weights='imagenet',        # checkpoints from when to initialize the model\n    include_top=False,         # when True, includes the fully connected classifier network\n    input_shape=(150, 150, 3)  # shape of the image tensors to feed the network with\n)\n\nconv_base.trainable = False\n\nmodel = models.Sequential()\nmodel.add(conv_base)\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(5, activation='softmax'))\n          \nmodel.summary()","5a68f99d":"train_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n# Do not apply data augmentation on validation\nvalidation_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    TRAIN_DIR, \n    target_size=(150, 150),\n    batch_size=20,\n    class_mode='categorical'\n)\nvalidation_generator = validation_datagen.flow_from_directory(\n    VALIDATION_DIR,\n    target_size=(150, 150),\n    batch_size=20,\n    class_mode='categorical'\n)","0ef5438e":"model.compile(\n    optimizer=optimizers.RMSprop(lr=2e-5),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n      train_generator,\n      steps_per_epoch=100,\n      epochs=30,\n      validation_data=validation_generator,\n      validation_steps=50\n)","39cdaee7":"model.save('\/kaggle\/output\/models\/PretrainedFlowerRecognition.h5')","e67ff007":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","4512f8f9":"# Unfreeze the last 3 layers\nconv_base.trainable = True \nset_trainable = False\n\nfor layer in conv_base.layers:\n    if layer.name == \"block4_conv1\":\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False\n\n        \nconv_base.summary()","17453e32":"model.compile(\n    optimizer=optimizers.RMSprop(lr=1e-5),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n    \n)\n\nhistory = model.fit(\n      train_generator,\n      steps_per_epoch=100,\n      epochs=20,\n      validation_data=validation_generator,\n     validation_steps=50\n)","dc396f03":"model.save('\/kaggle\/output\/models\/PretrainedFlowerRecognitionFineTuned.h5')","c4e362cd":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","9b993dbc":"def smooth_curve(points, factor=0.8):\n    smoothed_points = []\n    for point in points:\n        if smoothed_points:\n            previous = smoothed_points[-1]\n            smoothed_points.append(previous * factor + point * (1 - factor))\n        else:\n            smoothed_points.append(point)\n    return smoothed_points\n\nplt.plot(epochs, smooth_curve(acc), 'bo', label='Smoothed training acc')\nplt.plot(epochs, smooth_curve(val_acc), 'b', label='Smoothed validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, smooth_curve(loss), 'bo', label='Smoothed training loss')\nplt.plot(epochs, smooth_curve(val_loss), 'b', label='Smoothed validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","bbd8dc33":"test_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntest_generator = test_datagen.flow_from_directory(\n    TEST_DIR,\n    target_size=(150, 150),\n    batch_size=20,\n    class_mode='categorical'\n)\n\ntest_loss, test_accuracy = model.evaluate(test_generator, steps=50)\nprint('test accuracy:', test_accuracy)","db9645ac":"#### Extract features from convolution base","5c181832":"We obtain a good 90% validation accuracy, which outperforms the CNN from scratch we built in the previous notebook. On the other hand, due to the impossibility to add data augmentation we start also to overfit quite early, despite of using `Dropout` with a fairly large rate.","24a39b43":"#### Define and train the densely connected classifier\n\nThe training should be very fast, since we have to do with a small network.","b5787977":"#### Add a densely connected classifier on top of the convolutional base\n\nBy _freezing_ the `conv_base` we will reduce the amount of weights that will be trained: only those who belong the classifier on top.","93404b10":"#### Unfreeze selected layers","e5bf18db":"### Fast feature extraction _without_ data augmentation","664591ac":"#### Setup generators with data augmentation","69c001e2":"#### Plot metrics","dc56ecd8":"### Feature extraction _with_ data augmentation\n\nThis technique allows data augmentation during training, but is also expensive. Absolutely not recommended without at least one GPU, since we'll have to do with more than 16 millions parameters.","552d4fee":"#### Define convolutional base","619501d0":"These curves look much cleaner and more stable. We are seeing a nice 4% absolute improvement.\n\nNote that **the loss curve does not show any real improvement** (in fact, it is deteriorating). How could accuracy improve if the loss isn't decreasing? The answer is simple: what we display is an average of pointwise loss values, but what actually matters for accuracy is the distribution of the loss values, not their average, since accuracy is the result of a binary thresholding of the class probability prediced by the model. The model may still be improving even if this isn't reflected in the average loss.","29e5dd67":"#### Compile and train the model","4e05a11c":"#### Evaluate the model on test data","b1bc18e1":"At this point we need to flatten the feature array in order to feed the classifier, exactly as a `Flatted` layer would do. Currently, the features have shape (`samples, 4, 4, 512`), so new array will have a shape equal to (`samples, (4 * 4 * 512)`).","4b018f55":"#### Plot loss and accuracy","3059f0ab":"These curves look very noisy. To make them more readable, we can smooth them by replacing every loss and accuracy with exponential moving averages of these quantities. Here's a trivial utility function to do this:","d9ec0cf4":"#### Compile and fine-tune the network","c5a6c100":"Now we can start fine-tuning our network. We will do this with the RMSprop optimizer, using a very low learning rate. The reason for using a low learning rate is that we want to limit the magnitude of the modifications we make to the representations of the 3 layers that we are fine-tuning. Updates that are too large may harm these representations."}}