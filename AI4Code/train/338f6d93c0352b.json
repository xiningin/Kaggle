{"cell_type":{"365aed42":"code","e104d54f":"code","b3e73bb1":"code","12d4c24b":"code","98c94943":"code","5a7366e6":"code","fe074064":"code","06467b93":"code","4052c352":"code","ce5fdaf3":"code","8a1dc286":"code","93ed0589":"code","c538c39d":"code","9bd34a95":"code","3e22955b":"code","0bf068f1":"code","f908139d":"code","e135cce9":"code","7266d232":"code","e1132814":"code","4b316d83":"code","7368b81e":"code","560f8b40":"code","50155818":"code","612813d2":"code","ed6ae868":"code","16ddaa0d":"code","f5429bd2":"markdown","3d29224d":"markdown","cf1a3fb8":"markdown","7f1098ed":"markdown","562c0847":"markdown","0a803b8b":"markdown","8739a194":"markdown","75145f33":"markdown","2c649229":"markdown","f69bae75":"markdown","02a339b1":"markdown","d3ed3d8e":"markdown","9de72a56":"markdown","7430ee03":"markdown","2c35a365":"markdown"},"source":{"365aed42":"import os\nimport pickle\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport sklearn.metrics as metrics\nimport sklearn.impute as impute\nimport sklearn.model_selection as ms\nfrom scipy.stats import mode","e104d54f":"train_csv = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntest_csv = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")","b3e73bb1":"train_csv.head()","12d4c24b":"print(\"Number of training samples: \", len(train_csv))","98c94943":"print(\"Training Data shape: \", train_csv.shape)","5a7366e6":"print(\"Number of test samples: \", len(test_csv))","fe074064":"print(\"Test Data shape: \", test_csv.shape)","06467b93":"print(\"Data columns\")\ncols = train_csv.columns.to_list()\nprint(cols)","4052c352":"print(\"number of null samples in train csv columns:\")\nfor col, null in zip(cols, train_csv.isnull().sum()):\n    print(f\"{col} - {null}\")","ce5fdaf3":"print(\"number of null samples in test csv columns:\")\nfor col, null in zip(cols, test_csv.isnull().sum()):\n    print(f\"{col} - {null}\")","8a1dc286":"target = \"claim\"\nfeatures = [f for f in train_csv.columns if f not in [\"id\", target]]\nprint(features)","93ed0589":"train_csv[\"claim\"].value_counts()","c538c39d":"%%time\nimputer = impute.SimpleImputer(strategy=\"mean\")\nimputer.fit(train_csv[features])\ntrain_csv[features] = imputer.transform(train_csv[features])\ntest_csv[features] = imputer.transform(test_csv[features])","9bd34a95":"%%time\ntrain_csv.to_csv(\"train_mean_filling.csv\", index=False)\ntest_csv.to_csv(\"test_mean_filling.csv\", index=False)","3e22955b":"class CrossValidation:\n    def __init__(self, df, shuffle,random_state=None):\n        self.df = df\n        self.random_state = random_state\n        self.shuffle = shuffle\n        if shuffle is True:\n            self.df = df.sample(frac=1,\n                random_state=self.random_state).reset_index(drop=True)\n\n    def hold_out_split(self,percent,stratify=None):\n        if stratify is not None:\n            y = self.df[stratify]\n            train,val = ms.train_test_split(self.df, test_size=percent\/100,\n                stratify=y, random_state=self.random_state)\n            return train,val\n        size = len(self.df) - int(len(self.df)*(percent\/100))\n        train = self.df.iloc[:size,:]\n        val = self.df.iloc[size:,:]\n        return train,val\n\n    def kfold_split(self, splits, stratify=None):\n        if stratify is not None:\n            kf = ms.StratifiedKFold(n_splits=splits,\n                shuffle=self.shuffle,\n                random_state=self.random_state)\n            y = self.df[stratify]\n            for train, val in kf.split(X=self.df,y=y):\n                t = self.df.iloc[train,:]\n                v = self.df.iloc[val, :]\n                yield t,v\n        else:\n            kf = ms.KFold(n_splits=splits, shuffle=self.shuffle,\n                random_state=self.random_state)\n            for train, val in kf.split(X=self.df):\n                t = self.df.iloc[train,:]\n                v = self.df.iloc[val, :]\n                yield t,v","0bf068f1":"seed = 42\nfolds = 5","f908139d":"cv = CrossValidation(train_csv,\n                     shuffle=True,\n                     random_state=seed\n                    )","e135cce9":"total_val_fold_auc = []\ntest_predictions = []","7266d232":"def xgb_train_and_predict(xgb_params, seed_mul=1):\n    valid_preds = {}\n    test_preds = []\n    val_fold_auc = []\n    for fold, (train_, val_) in enumerate(cv.kfold_split(splits=folds, stratify=\"claim\")):\n        print(\"Training fold: \", fold+1)\n        model = xgb.XGBClassifier(**xgb_params,\n                                  seed=fold*seed_mul,\n                                  tree_method=\"gpu_hist\",\n                                  gpu_id=0,\n                                  predictor=\"gpu_predictor\",\n                                  use_label_encoder=False\n                                )\n        trainX = train_[features]\n        trainY = train_[target]\n        valX = val_[features]\n        valY = val_[target]\n\n        val_ids = val_.id.values.tolist()\n\n        model.fit(trainX, trainY, \n                  early_stopping_rounds=300, \n                  eval_set=[(valX, valY)],\n                  eval_metric=\"auc\",\n                  verbose=1000)\n\n        predY = model.predict(valX)\n        val_auc = metrics.roc_auc_score(valY, predY)\n        print(val_auc)\n        val_fold_auc.append(val_auc)\n\n        valid_preds.update(dict(zip(val_ids, predY)))\n\n        predY = model.predict(test_csv[features])\n        test_preds.append(predY)\n    return val_fold_auc, valid_preds, np.column_stack(test_preds)","e1132814":"model_count = 1","4b316d83":"xgb_params = {\n    'booster': 'gbtree',\n    'n_estimators': 10000\n}\n\nval_fold_auc, valid_preds, test_preds = xgb_train_and_predict(xgb_params, seed_mul=1)\n\nfold_auc = np.mean(val_fold_auc)\nprint(\"Fold Validation: \", fold_auc)\n\ntotal_val_fold_auc.append(fold_auc)\n\npred_df = pd.DataFrame.from_dict(valid_preds, orient=\"index\").reset_index()\npred_df.columns = [\"id\", f\"pred_{model_count}\"]\npred_df.to_csv(f\"train_pred_{model_count}.csv\", index=False)\n\ntest_df = pd.DataFrame(columns=[\"id\", f\"pred_{model_count}\"]) \ntest_df[\"id\"] = test_csv[\"id\"]\ntest_preds = np.mean(test_preds, axis=1)\ntest_df[f\"pred_{model_count}\"] = test_preds\ntest_df.to_csv(f\"test_pred_{model_count}.csv\", index=False)\ntest_predictions.append(test_preds)\n\nmodel_count += 1","7368b81e":"xgb_params = {\n    'booster': 'gbtree',\n    'n_estimators': 5000\n}\n\nval_fold_auc, valid_preds, test_preds = xgb_train_and_predict(xgb_params, seed_mul=11)\n\nfold_auc = np.mean(val_fold_auc)\nprint(\"Fold Validation: \", fold_auc)\n\ntotal_val_fold_auc.append(fold_auc)\n\npred_df = pd.DataFrame.from_dict(valid_preds, orient=\"index\").reset_index()\npred_df.columns = [\"id\", f\"pred_{model_count}\"]\npred_df.to_csv(f\"train_pred_{model_count}.csv\", index=False)\n\ntest_df = pd.DataFrame(columns=[\"id\", f\"pred_{model_count}\"]) \ntest_df[\"id\"] = test_csv[\"id\"]\ntest_preds = np.mean(test_preds, axis=1)\ntest_df[f\"pred_{model_count}\"] = test_preds\ntest_df.to_csv(f\"test_pred_{model_count}.csv\", index=False)\ntest_predictions.append(test_preds)\n\nmodel_count += 1","560f8b40":"print(\"All Models Validation AUC: \", np.mean(total_val_fold_auc))","50155818":"def create_submission(sub_name,\n                      predictions, \n                      template_path=\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\"):\n    template = pd.read_csv(template_path)\n    template[target] = predictions\n    template.to_csv(sub_name+\".csv\", index=False)","612813d2":"def voting_ensembling(predictions, axis):\n    predictions, _ = mode(predictions, axis=axis)\n    return predictions","ed6ae868":"predictions = voting_ensembling(np.column_stack(test_predictions), axis=1)\npredictions.shape","16ddaa0d":"create_submission(\"submission\", predictions)","f5429bd2":"# Basic EDA","3d29224d":"### check for null or nan values in dataset","cf1a3fb8":"# Creating Submission File","7f1098ed":"### checking number of columns and dataset length, shape","562c0847":"# Cross Validation","0a803b8b":"### Prediction with XGB model 1","8739a194":"# Folds Predictions","75145f33":"# Loading Data","2c649229":"# Data Preprocessing","f69bae75":"Not much data imbalance","02a339b1":"# Installing Dependencies","d3ed3d8e":"### excluding id and target from feature set","9de72a56":"### Handling Missing Values\nHere we use mean strategy to fill missing values. In this method we fill missing data with mean of feature column. There are other methods that can be expermented like KNNImputer, Using a model to regress missing values but currently this will work fine for a baseling","7430ee03":"### Prediction with XGB model 2","2c35a365":"### checking class distribution of dataset"}}