{"cell_type":{"a3632f74":"code","0f2b0b64":"code","932d3937":"code","2b6976a7":"code","cc2d3908":"code","fbe28b6e":"code","849c359a":"code","4e8e5fcc":"code","dd83e205":"code","61c69470":"code","b8239321":"code","7e5fe625":"code","3a098ded":"code","1bb9f9c8":"code","5f97a5ba":"code","383a2b63":"code","994e75a5":"code","b8ee3ad7":"code","b0a0aa3f":"code","574266c2":"code","fb2911bd":"code","0537b688":"code","3e53b9d9":"code","828eaef0":"code","5f8c4ff9":"code","7145845f":"code","232e05f3":"code","5ed79ce7":"code","3ff571f6":"code","df4fca32":"code","855e92ae":"code","da21f583":"code","8f246f9b":"code","575fe48c":"code","10b43368":"code","800397f4":"code","79a815dd":"code","29c4b6d5":"code","d212da89":"code","7b4bf5dd":"code","81bcda23":"code","859d57bf":"code","8b47b34e":"code","42cf9d13":"code","b80f591d":"code","7ebe899b":"code","cb232ae1":"code","d4766588":"code","1bd9c7c0":"code","4c2a532c":"markdown","36996d8d":"markdown","b9d67ea4":"markdown","ad25f36d":"markdown","38a463e8":"markdown","7250b2ca":"markdown","f342e53a":"markdown","ca3e6f1b":"markdown","2d12337a":"markdown","32804d04":"markdown","f97d0f65":"markdown","01119d01":"markdown","1fefbfcb":"markdown","6de7e537":"markdown","6c57f313":"markdown","138235d0":"markdown","36f19404":"markdown","8586f90d":"markdown","cc3aa911":"markdown","6b2ed8c1":"markdown","33172751":"markdown","907d46d4":"markdown","76d3b7af":"markdown","bfaeec02":"markdown","b493bf24":"markdown","7c1194a8":"markdown","5a931238":"markdown","1c7a0e87":"markdown","5c2c9fc3":"markdown","63cf809a":"markdown","14718df2":"markdown","6b55bfa0":"markdown","9700209f":"markdown","f22a1610":"markdown","82f5ae13":"markdown","cf46ab33":"markdown","a75284ae":"markdown","28814bdf":"markdown","779217f2":"markdown","bc30c233":"markdown"},"source":{"a3632f74":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0f2b0b64":"#Data analysis and wrangling\n\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nimport plotly.express as px\n\n#Data Visualisation\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#machine learning\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\nfrom scipy import stats\n\nimport warnings\nwarnings.filterwarnings('ignore')","932d3937":"#import data\ntrain= pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest= pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nall_data=[train, test]","2b6976a7":"train.head()","cc2d3908":"test.head()","fbe28b6e":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {}  \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n","849c359a":"train['SalePrice'].describe()","4e8e5fcc":"#histogram\nsns.distplot(train['SalePrice']);","dd83e205":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","61c69470":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","b8239321":"fig, ax = plt.subplots()\nax.scatter(x = train['TotalBsmtSF'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","7e5fe625":"fig, ax = plt.subplots()\nsns.boxplot(x = train['OverallQual'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('OverallQual', fontsize=13)\nplt.show()","3a098ded":"fig, ax = plt.subplots()\nsns.boxplot(x = train['YearBuilt'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('OverallQual', fontsize=13)\nplt.xticks(rotation=90)\nplt.show()","1bb9f9c8":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","5f97a5ba":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","383a2b63":"corr=train.corr()","994e75a5":"corr[corr['SalePrice']>0.3].index","b8ee3ad7":"train = train[['Id','LotFrontage', 'OverallQual', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea',\n       'BsmtFinSF1', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n       'FullBath', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars',\n       'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'SalePrice']]\ntest=test[['Id','LotFrontage', 'OverallQual', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea',\n       'BsmtFinSF1', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n       'FullBath', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars',\n       'GarageArea', 'WoodDeckSF', 'OpenPorchSF']]","b0a0aa3f":"total = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)","574266c2":"#dealing with missing data\ntrain = train.drop((missing_data[missing_data['Total'] > 81]).index,1)","fb2911bd":"train.isnull().sum().sort_values(ascending=False).head(20)","0537b688":"#missing data\ntotal_test = test.isnull().sum().sort_values(ascending=False)\npercent_test = (test.isnull().sum()\/test.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total_test, percent_test], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)","3e53b9d9":"#dealing with missing data\ntest = test.drop((missing_data[missing_data['Total'] > 78]).index,1)","828eaef0":"test.isnull().sum().sort_values(ascending=False).head(20)","5f8c4ff9":"train.head()","7145845f":"test.head()","232e05f3":"print(train.shape, test.shape)","5ed79ce7":"train.info()\ntest.info()","3ff571f6":"train['GarageYrBlt'] = train['GarageYrBlt'].fillna(train['GarageYrBlt'].mean())\ntrain['MasVnrArea'] = train['MasVnrArea'].fillna(train['MasVnrArea'].mean())","df4fca32":"test['GarageYrBlt'] = test['GarageYrBlt'].fillna(test['GarageYrBlt'].mean())\ntest['MasVnrArea'] = test['MasVnrArea'].fillna(test['MasVnrArea'].mean())\ntest['GarageCars'] = test['GarageCars'].fillna(test['GarageCars'].mean())\ntest['GarageArea'] = test['GarageArea'].fillna(test['GarageArea'].mean())\ntest['BsmtFinSF1'] = test['BsmtFinSF1'].fillna(test['BsmtFinSF1'].mean())\ntest['TotalBsmtSF'] = test['TotalBsmtSF'].fillna(test['TotalBsmtSF'].mean())","855e92ae":"train.isnull().sum()","da21f583":"test.isnull().sum()","8f246f9b":"train.shape","575fe48c":"# Categorical boolean mask\ncategorical_feature_mask = train.dtypes==object\n# filter categorical columns using mask and turn it into alist\ncategorical_cols = train.columns[categorical_feature_mask].tolist()","10b43368":"categorical_cols","800397f4":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ntrain[categorical_cols] = train[categorical_cols].apply(lambda col: labelencoder.fit_transform(col.astype(str)))","79a815dd":"# Categorical boolean mask\ncategorical_feature_mask_test = test.dtypes==object\n# filter categorical columns using mask and turn it into alist\ncategorical_cols_test = test.columns[categorical_feature_mask_test].tolist()","29c4b6d5":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ntest[categorical_cols_test] = test[categorical_cols_test].apply(lambda col: labelencoder.fit_transform(col.astype(str)))","d212da89":"print(\"Train Data\")\ncols = ['SalePrice', 'BsmtFinSF1', 'GrLivArea','TotalBsmtSF']\nsns_plot = sns.pairplot(train[cols], size = 2.5);\nsns_plot.savefig(\"output.png\")\nsns_plot","7b4bf5dd":"outliers = train[\n    (train['GrLivArea'] > 4000) &\n    (train['SalePrice'] < 300000)&\n    (train['TotalBsmtSF'] > 3000)\n]\n\ntrain.drop(outliers.index, inplace=True)","81bcda23":"outliers = test[\n    (test['GrLivArea'] > 4000) &\n    (test['TotalBsmtSF'] > 3000)\n]\n\ntest.drop(outliers.index, inplace=True)","859d57bf":"print(train.shape, test.shape)","8b47b34e":"from sklearn.model_selection import train_test_split , KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score","42cf9d13":"X_train, X_test, y_train, y_test = train_test_split(train.drop('SalePrice', axis=1), train['SalePrice'], test_size=0.2, random_state=42)","b80f591d":"print(X_train.shape, y_train.shape, train.shape, test.shape)","7ebe899b":"from sklearn.linear_model import LinearRegression\nregr = LinearRegression()\n# Train the model using the training sets\nregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_test_pred = regr.predict(X_test)\ny_train_pred = regr.predict(X_train)\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nprint('MSE train: %.3f, test: %.3f' % (\n        mean_squared_error(y_train, y_train_pred),\n        mean_squared_error(y_test, y_test_pred)))\nprint('R^2 train: %.3f, test: %.3f' % (\n        r2_score(y_train, y_train_pred),\n        r2_score(y_test, y_test_pred)))","cb232ae1":"from sklearn.linear_model import Lasso\n\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_train, y_train)\ny_train_pred_1 = lasso.predict(X_train)\ny_test_pred_1= lasso.predict(X_test)\n#print(lasso.coef_)\nprint('MSE train: %.3f, test: %.3f' % (\n        mean_squared_error(y_train, y_train_pred_1),\n        mean_squared_error(y_test, y_test_pred_1)))\nprint('R^2 train: %.3f, test: %.3f' % (\n        r2_score(y_train, y_train_pred_1),\n        r2_score(y_test, y_test_pred_1)))","d4766588":"import xgboost\n\nxgb = xgboost.XGBRegressor(colsample_bytree=0.4,\n                 gamma=0,                 \n                 learning_rate=0.07,\n                 max_depth=3,\n                 min_child_weight=1.5,\n                 n_estimators=10000,                                                                    \n                 reg_alpha=0.75,\n                 reg_lambda=0.45,\n                 subsample=0.6,\n                 seed=42)\nxgb.fit(X_train,y_train)\ny_test_pred_2 = xgb.predict(X_test)\ny_train_pred_2= xgb.predict(X_train)\n\nprint('MSE train: %.3f, test: %.3f' % (\n        mean_squared_error(y_train, y_train_pred_2),\n        mean_squared_error(y_test, y_test_pred_2)))\nprint('R^2 train: %.3f, test: %.3f' % (\n        r2_score(y_train, y_train_pred_2),\n        r2_score(y_test, y_test_pred_2)))","1bd9c7c0":"y_pred = xgb.predict(test)\nfinal_predictions = y_pred\n\nsubmission = pd.DataFrame({\n        \"Id\": test[\"Id\"],\n        \"SalePrice\": final_predictions\n    })\n\nsubmission.to_csv(\"xgb.csv\", encoding='utf-8', index=False)\n\nprint(submission.head())","4c2a532c":"**Sample Test Dataset**","36996d8d":"Let's now begin to train out regression model! We will need to first split up our data into an X array that contains the features to train on, and a y array with the target variable(Salesprice)","b9d67ea4":"**ll(c) Analysing SalesPrice with other variables**","ad25f36d":"**Relationship with categorical features**","38a463e8":"# I. Import Packages and dataset","7250b2ca":"# ThankYou!","f342e53a":"**Observation**\n\n* Deviate from the normal distribution.\n* Have appreciable positive skewness.\n* Show peakedness","ca3e6f1b":"**ll.(a) Analysing 'SalePrice'**","2d12337a":"**lV(a) Linear Regression**","32804d04":"**Relationship with numerical variables**","f97d0f65":"# lll. Feature Engineering","01119d01":"Skewness\n\n* Is the degree of distortion from the symmetrical bell curve or the normal curve.\n* So, a symmetrical distribution will have a skewness of \"0\".\n* There are two types of Skewness: Positive and Negative.\n* Positive Skewness(similar to our target variable distribution) means the tail on the right side of the distribution is longer and fatter.\n* In positive Skewness the mean and median will be greater than the mode similar to this dataset. Which means more houses were sold by less than the average price.\n* Negative Skewness means the tail on the left side of the distribution is longer and fatter.\n* In negative Skewness the mean and median will be less than the mode.\n","1fefbfcb":"It seems that 'SalePrice' and 'GrLivArea' have good relationship, with a linear relationship.We can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers. Therefore, we can safely delete them.","6de7e537":"We will be using 3 models to predict the sale price amount of the houses.  :\n* Linear Regression\n* Lasso Regression \n* XG Boost ","6c57f313":"We can say the best working model by loking error rates.\nThe best working model is XGBoost","138235d0":"**lll(b) Encoding the categorical features**","36f19404":"* Correlation matrix (heatmap style).\n* SalePrice correlation matrix (zoomed heatmap style).\n* Scatter plots between the most correlated variables (move like Jagger style).","8586f90d":"As we can see some of paremeters have a lot of missing values. That's why we should drop these from data. And we are going to drop parematers which total value is larger than 81.","cc3aa911":"![image.png](attachment:72bda209-cf83-4527-89e4-2452b64634de.png)!","6b2ed8c1":"# II. DataSet Exploration","33172751":"# V. Submission","907d46d4":"**'SalePrice' correlation matrix (zoomed heatmap style)**","76d3b7af":"We are going to do same thing to the test data","bfaeec02":"**ll(b) SalesPrice Analysis with other variables**","b493bf24":"**Correlation matrix (heatmap style)**","7c1194a8":"**Fill Missing Values**","5a931238":"**lll(c) Handling Outliers**","1c7a0e87":"**lll(a) Missing data and Handling Missing data**","5c2c9fc3":"\n**Train Test Split**","63cf809a":"Conclution-\n\n* GrLivArea and TotalBsmtSF are linearly related with 'SalePrice'. \n* Both relationships are positive, which means that as one variable increases, the other also increases.\n* OverallQual and YearBuilt also are related with 'SalePrice'.We observe that with increase in overall material and finish quality, salesprice also increases \n\nWe just analysed four variables, but there are many other that we should analyse. \n","14718df2":"we need to handle missing data.","6b55bfa0":"TotalBsmtSF' closes in itself and gives zero credit to 'SalePrice'","9700209f":"We droped some columns that less than 0.3 of correlation of Sale Prices.","f22a1610":" **lV(b) Lasso Regression**","82f5ae13":" The minimum sale price is larger than zero.","cf46ab33":"**lV(c) XGBoost**","a75284ae":"![image.png](attachment:30528b90-110f-4ed2-b388-dd3cd4f4b5ec.png)!","28814bdf":"**Sample Train Dataset**","779217f2":"# lV. Linear Regression ","bc30c233":"**Kurtosis According to Wikipedia,**\n\nIn probability theory and statistics, Kurtosis is the measure of the \"tailedness\" of the probability. distribution of a real-valued random variable. So, In other words, it is the measure of the extreme values(outliers) present in the distribution.\n\n* There are three types of Kurtosis: Mesokurtic, Leptokurtic, and Platykurtic.\n* Mesokurtic is similar to the normal curve with **the standard value of 3**. This means that the extreme values of this distribution are similar to that of a normal distribution.\n* Leptokurtic Example of leptokurtic distributions are the T-distributions with small degrees of freedom.\n* Platykurtic: Platykurtic describes a particular statistical distribution with thinner tails than a normal distribution. Because this distribution has thin tails, it has fewer outliers (e.g., extreme values three or more standard deviations from the mean) than do mesokurtic and leptokurtic distributions."}}