{"cell_type":{"9b98f1c5":"code","1c8737be":"code","094f77e5":"code","473da7bb":"code","1f71a25c":"code","6c666039":"code","8626709a":"code","e1aae891":"code","2f5a7e91":"code","45cddc1a":"code","39a5fbdf":"code","00a2b02d":"code","ef927619":"code","9ea3b58b":"code","00e2c20e":"code","0c15db9c":"code","eb0ce329":"code","a37fc0d9":"code","027eb19b":"code","6428990b":"code","87de3b4d":"code","a2c82ba3":"code","8a71eed4":"code","15f92d0b":"code","39dc43bd":"code","ce5af3f8":"code","ee72fc84":"code","e7d7b95e":"code","85f6e88a":"code","640d3f34":"code","7e72dec9":"code","a00dac90":"code","4d71a9f9":"code","8582df18":"code","7ca934bd":"code","583aa6f1":"code","e292e81a":"code","5401577b":"code","fd6e23ba":"code","8f56c546":"code","a2252e8e":"code","9c6180ee":"code","a8e3e2b0":"code","69200cce":"code","77f5ef47":"code","47a565f4":"code","452aaeea":"code","e3678b35":"code","3e0b17e1":"code","2e771183":"code","0dee6167":"code","0642784e":"code","04e9e522":"code","630ed9f9":"code","3aaed018":"code","f2ce59b8":"code","6cb64c3f":"code","9cd8ac3e":"code","901573d9":"code","edbe7003":"code","038dfee1":"code","20b44963":"code","3abe258c":"code","36f15cb3":"code","0916458a":"code","1740415d":"code","1e88b5e2":"code","83a5624b":"code","394f4b1d":"code","a1cd2c25":"code","acee33e0":"markdown","9eace6b0":"markdown","72ab1585":"markdown","bfaf0cd1":"markdown","30b68fcb":"markdown","cde851dc":"markdown","a5bfe1ac":"markdown","246e873f":"markdown","a36938a8":"markdown","2d34f0f4":"markdown","7b462a22":"markdown","71828ee0":"markdown","893e38a3":"markdown","656abdcc":"markdown","25e86948":"markdown","c6f07603":"markdown","3c7b398e":"markdown","bc0b1d06":"markdown","a3ecf11c":"markdown","48a1120c":"markdown","179c83f9":"markdown","dec4edd2":"markdown","a4a2dd36":"markdown","0865ac71":"markdown","484bab3e":"markdown"},"source":{"9b98f1c5":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.optimize import minimize\n\n# Note that there are no NANs in these data; '?' is\n# used when there is missing information\naccepts = pd.read_csv('..\/input\/chefmozaccepts.csv')\ncuisine = pd.read_csv('..\/input\/chefmozcuisine.csv')\nhours = pd.read_csv('..\/input\/chefmozhours4.csv')\nparking = pd.read_csv('..\/input\/chefmozparking.csv')\ngeo = pd.read_csv('..\/input\/geoplaces2.csv') \nusercuisine = pd.read_csv('..\/input\/usercuisine.csv')\npayment = pd.read_csv('..\/input\/userpayment.csv')\nprofile = pd.read_csv('..\/input\/userprofile.csv')\nrating = pd.read_csv('..\/input\/rating_final.csv')","1c8737be":"accepts.head()","094f77e5":"print(\"There are {} unique placeID's.\".format(len(accepts.placeID.unique())))\nprint(\"There are {} unique Rpayment categories:\".format(len(accepts.Rpayment.unique())))\nprint(accepts.Rpayment.unique())","473da7bb":"cuisine.head()","1f71a25c":"print(\"There are {} unique placeID's.\".format(len(cuisine.placeID.unique())))\nprint(\"There are {} unique Rcuisine categories:\".format(len(cuisine.Rcuisine.unique())))\nprint(cuisine.Rcuisine.unique())","6c666039":"hours.head()","8626709a":"print(\"There are {} unique placeID's.\".format(len(hours.placeID.unique())))","e1aae891":"parking.head()","2f5a7e91":"print(\"There are {} unique placeID's.\".format(len(parking.placeID.unique())))\nprint(\"There are {} unique parking_lot categories:\".format(len(parking.parking_lot.unique())))\nprint(parking.parking_lot.unique())","45cddc1a":"geo.head()","39a5fbdf":"print(\"There are {} unique placeID's.\".format(len(geo.placeID.unique())))","00a2b02d":"usercuisine.head()","ef927619":"print(\"There are {} unique userID's.\".format(len(usercuisine.userID.unique())))\nprint(\"There are {} unique Rcuisine categories:\".format(len(usercuisine.Rcuisine.unique())))\nprint(usercuisine.Rcuisine.unique())","9ea3b58b":"payment.head()","00e2c20e":"print(\"There are {} unique userID's.\".format(len(payment.userID.unique())))\nprint(\"There are {} unique Upayment categories:\".format(len(payment.Upayment.unique())))\nprint(payment.Upayment.unique())","0c15db9c":"profile.head()","eb0ce329":"print(\"There are {} unique userID's.\".format(len(profile.userID.unique())))","a37fc0d9":"rating.head()","027eb19b":"print(\"There are {} unique userID's.\".format(len(rating.userID.unique())))\nprint(\"There are {} unique placeID's.\".format(len(rating.placeID.unique())))\nprint(\"There are {} * 3 ratings.\".format(len(rating)))","6428990b":"rating.iloc[:,2:].describe()","87de3b4d":"res_all = np.concatenate((accepts.placeID.unique(), cuisine.placeID.unique(), \n                          hours.placeID.unique(), parking.placeID.unique(), geo.placeID.unique()))\nres_all = np.sort( np.unique(res_all) ) # All the placeID's\n\nprint(\"There are {} restaurants.\".format(len(res_all)))","a2c82ba3":"user_all = np.concatenate((usercuisine.userID.unique(), payment.userID.unique(), \n                           profile.userID.unique()))\nuser_all = np.sort( np.unique(user_all) ) # All the userID's\n\nprint(\"There are {} users.\".format(len(user_all)))","8a71eed4":"overall_rating = pd.DataFrame( np.zeros((len(res_all),len(user_all)))-1.0, \n                              columns=user_all, index=res_all )\nfood_rating = overall_rating.copy()\nservice_rating = overall_rating.copy() \n\nfor r, u, o, f, s in zip(rating.placeID, rating.userID, rating.rating, rating.food_rating, \n                         rating.service_rating):\n    overall_rating.loc[r,u] = o\n    food_rating.loc[r,u] = f\n    service_rating.loc[r,u] = s","15f92d0b":"review = pd.DataFrame( np.zeros(overall_rating.shape), columns=user_all, index=res_all)\nreview[overall_rating >= 0] = 1","39dc43bd":"# use dummy variables for different cuisine categories of the restaurants\nres_cuisine = pd.get_dummies(cuisine,columns=['Rcuisine'])\n\n# remove duplicate restaurant ID's. \n# A restaurant with multiple cuisine categories would have multiple columns equal 1\nres_cuisine = res_cuisine.groupby('placeID',as_index=False).sum()\n\nres_cuisine.head()","ce5af3f8":"parking.parking_lot.value_counts()","ee72fc84":"res_parking = parking.copy()\nres_parking.parking_lot = res_parking.parking_lot.map({'fee':1, 'none':0, 'public':1, 'yes':2,\n                                        'street':1, 'valet parking':1, 'validated parking':1})\n\n# remove duplicate restaurant ID's. \n# A restaurant with multiple parking options may have a value > 2\nres_parking = res_parking.groupby('placeID',as_index=False).sum()","e7d7b95e":"res_info = geo[['latitude','longitude','placeID','name','address','city','state']]\n\n# These features should be relevant for rating prediction since they are about services \n# and price. Especially, 'alcohol','smoking_area', and 'price' relate to 'drink_level', \n# 'smoker', and 'budget' in the user profiles \nres_service_price = geo[['placeID','alcohol','smoking_area','other_services','price']]\nprint(res_service_price.alcohol.value_counts())\nprint('\\n')\nprint(res_service_price.smoking_area.value_counts())\nprint('\\n')\nprint(res_service_price.other_services.value_counts())\nprint('\\n')\nprint(res_service_price.price.value_counts())","85f6e88a":"# 1 if alcohol is available, 0 otherwise\nres_service_price.alcohol = res_service_price.alcohol.map(lambda x: 0 if x == 'No_Alcohol_Served' else 1)\n# 1 if there is smoking area, 0 otherwise\nres_service_price.smoking_area = res_service_price.smoking_area.map(lambda x: 0 if (x == 'none') | (x == 'not permitted') else 1)\n# 1 if other services are available, 0 otherwise\nres_service_price.other_services = res_service_price.other_services.map(lambda x: 0 if x == 'none'  else 1)\n# map price levels to numbers\nres_service_price.price = res_service_price.price.map({'low': 1, 'medium': 2, 'high': 3})","640d3f34":"# Whether these features are relevant is not so clear\nres_environment = geo[['placeID','dress_code','accessibility','Rambience','area']]\nprint(res_environment.dress_code.value_counts())\nprint('\\n')\nprint(res_environment.accessibility.value_counts())\nprint('\\n')\nprint(res_environment.Rambience.value_counts())\nprint('\\n')\nprint(res_environment.area.value_counts())","7e72dec9":"# 1 if formal dress is required, 0 otherwise\nres_environment.dress_code = res_environment.dress_code.map({'informal':0, 'casual':0, 'formal': 1})\n# map accessibility levels to numbers\nres_environment.accessibility = res_environment.accessibility.map({'no_accessibility':0, 'partially':1, 'completely': 2})\nres_environment.Rambience = res_environment.Rambience.map({'familiar':0, 'quiet': 1})\nres_environment.area = res_environment.area.map({'open':0, 'closed':1})","a00dac90":"df_res = pd.DataFrame({'placeID': res_all})\ndf_res = pd.merge(left=df_res, right=res_cuisine, how=\"left\", on=\"placeID\")\ndf_res = pd.merge(left=df_res, right=res_parking, how=\"left\", on=\"placeID\")\ndf_res = pd.merge(left=df_res, right=res_service_price, how=\"left\", on=\"placeID\")\ndf_res = pd.merge(left=df_res, right=res_environment, how=\"left\", on=\"placeID\")\n\nprint(df_res.shape)\ndf_res.head()","4d71a9f9":"# The placeID's for the 130 restaurants with ratings\nres_rated = res_all[np.sum(review,axis=1) > 0] \n\n# tells us whether a restaurant-user pair has a rating. 0 means No and 1 means Yes.\nR = review.loc[res_rated].values  # shape = (130,138)\n\nY_service = service_rating.loc[res_rated].values\nY_overall = overall_rating.loc[res_rated].values\nY_food  = food_rating.loc[res_rated].values","8582df18":"# select the indices of \"df_res\" where a restaurant has ratings\nindex = np.array([x in res_rated for x in df_res['placeID'].values])\nindex = np.where(index == True)[0]\n# restaurant features for the 130 restaurants with ratings\nX = df_res.loc[index, :].reset_index(drop=True)\nprint(X.isnull().sum()) # all the NANs are from cuisine ","7ca934bd":"X = X.fillna(0) # fill all NANs with 0\n# drop a feature if the entire column are 0\nfeatures_to_drop = X.columns.values[np.sum(X,axis=0) == 0] \nX = X.drop(features_to_drop, axis=1)\nX = X.drop(['placeID'], axis=1)\nX.head()","583aa6f1":"X = X[['parking_lot','alcohol','smoking_area','other_services','price','dress_code','accessibility']]\nX['x0'] = 1 # add a bias term for linear regressions\nX.head()","e292e81a":"num_rating = np.round(np.sum(R,axis=0)) # number of ratings from each user (minimum = 3)\n\n# 25% of the existing ratings will be used as the validation set\n# So during the training, they will be flagged \ng = lambda x: int(round(x*0.25)) \nflag = np.array( [g(x) for x in num_rating] )\n\nrandom.seed(0)\ncond = True\n\nwhile cond:\n\n    R_train = R.copy()\n\n    # loop over each user\n    for i in range(R_train.shape[1]):\n        # the restaurants that are rated\n        index = list( np.where(R_train[:,i] == 1)[0] )  \n        # randomly select about 25% of them to be flagged\n        index_flag = random.sample(index,flag[i])\n        R_train[index_flag,i] = 0  \n    \n    # make sure in the traning set, each restaurant and each user receives\/gives at least \n    # 2 ratings\n    if (np.sum(R_train,axis=0).min() > 1) & (np.sum(R_train,axis=1).min() > 1): \n        cond = False\n        \nR_valid = R - R_train \n# Now \"R_train\" contains 876 ones, and \"R_valid\" contains 285 ones (\"R\" contains 1161 ones)\n# The shape of \"R\", \"R_train\" and \"R_valid\" are all (130,138)  \nprint(R_train.sum())\nprint(R_valid.sum())","5401577b":"# FCP\ndef FCP(Y,Y_pred,R):\n    \n    # list of true ratings from each user (we only select users with at least two ratings)\n    Y_fcp = []  \n    Y_pred_fcp = [] # list of predicted ratings from each user \n    n_user = R.shape[1]\n    \n    for i in range(n_user):\n        \n        cond = (R.sum(axis=0) >= 2)[i] # there should be at least two ratings from a user\n        index = np.where( R[:,i] == 1)[0] # the indices (restaurants) with ratings\n    \n        if cond:\n            \n            Y_fcp.append( (Y*R)[:,i][index] )\n            Y_pred_fcp.append( (Y_pred*R)[:,i][index] )\n\n        \n    n_fcp = len(Y_fcp) # number of users with at least two ratings\n    TP = 0. # Total number of pairs\n    DP = 0. # number of discordant pairs\n    CP = 0. # number of concordant pairs (excluding ties)\n    \n    for i in range(n_fcp):\n        \n        num_Y = len(Y_fcp[i])   # number of ratings from a user\n        TP += num_Y*(num_Y-1)\/2 # number of rating pairs = n*(n+1)\/2 \n\n        greater = np.array([])\n        greater_pred = np.array([])\n\n        # this loop is to go over all the rating pairs\n        for j in range(num_Y-1):\n            \n            not_equal = Y_fcp[i][j] != Y_fcp[i][j+1:]\n            greater = Y_fcp[i][j] > Y_fcp[i][j+1:]\n            greater_pred = Y_pred_fcp[i][j] > Y_pred_fcp[i][j+1:]\n\n            # filter the ones that are not ties\n            greater = greater[not_equal]\n            greater_pred = greater_pred[not_equal]\n\n            DP += (greater != greater_pred).sum()\n            CP += (greater == greater_pred).sum()\n            \n    print(\"Total number of rating pairs: {}\".format(int(TP)))\n    print(\"Total number of discordant pairs: {}\".format(int(DP)))\n    print(\"Total number of concordant pairs: {}\".format(int(CP)))\n    print(\"Total number of ties: {}\".format(int(TP-DP-CP)))\n    print(\"FCP: {}\".format(CP\/(CP+DP)))\n","fd6e23ba":"def GetMean(Y,R):\n\n    Y = Y*R\n    mean =  (np.sum(Y, axis=1)\/np.sum((R == 1.0), axis=1)).reshape(Y.shape[0],1) * np.ones(Y.shape)\n    return mean","8f56c546":"Y = Y_overall \nY_mean = GetMean(Y,R_train) # get the average ratings based on the training set\nY_pred = np.zeros(Y.shape) + Y_mean # prediction \n\n# RMSE\nprint(\"RMSE of the training set: {}\".format(np.sqrt(mean_squared_error(Y[R_train == 1], Y_pred[R_train == 1]))))\nprint(\"RMSE of the validation set: {}\".format(np.sqrt(mean_squared_error(Y[R_valid == 1], Y_pred[R_valid == 1]))))","a2252e8e":"# FCP\nprint(\"Training Set:\")\nFCP(Y,Y_pred,R_train)\nprint(\"\\n\")\nprint(\"Validation Set:\")\nFCP(Y,Y_pred,R_valid)","9c6180ee":"def MakeBoxplot(Y_pred, Y_true, R, title):\n    \n    data1 = Y_pred[R == 1][Y_true[R == 1] == 0]\n    data2 = Y_pred[R == 1][Y_true[R == 1] == 1]\n    data3 = Y_pred[R == 1][Y_true[R == 1] == 2]\n    data = [data1,data2,data3]\n\n    fig = plt.figure()\n    plt.boxplot(data)\n    plt.xticks([1, 2, 3],[0,1,2])\n    plt.xlabel('True Rating')\n    plt.ylabel('Predicted Rating')\n    plt.title(title)\n    plt.show()","a8e3e2b0":"MakeBoxplot(Y_pred, Y, R_train, 'Training set')","69200cce":"MakeBoxplot(Y_pred, Y, R_valid, 'Validation set')","77f5ef47":"# The parameters of the cost function are the weights of all the users, with a shape = \n# (n_user, n_feature), where n_user = 138 = number of users, and n_feature = 8 = number \n# of restaurant features (including the bias term). However, to feed the cost function \n# to SciPy's minimize(), the parameters of the function cannot be a matrix and has to be \n# a 1D vector\n\ndef CostFunction(params, X, Y, R, lambd): # lambd is the L2 regularization coefficient\n    \n    num_user = R.shape[1]\n    num_feature = X.shape[1]\n\n    # reshape the parameters to a 2D matrix so we can perform matrix factorization\n    Theta = params.reshape(num_user, num_feature)\n    J = 0.5 * np.sum( (np.dot(X, Theta.T) * R - Y)**2 )\n\n    # regularization\n    J = J + lambd\/2. * np.sum(Theta[:,:-1]**2) \n\n    return J\n\n\ndef Gradient(params, X, Y, R, lambd):\n    \n    num_user = R.shape[1]\n    num_feature = X.shape[1]\n\n    Theta = params.reshape(num_user, num_feature)\n    Theta_grad = np.dot((np.dot(Theta, X.T) * R.T - Y.T), X)\n\n    # regularization\n    Theta_grad[:,:-1] = Theta_grad[:,:-1] + lambd*Theta[:,:-1]\n\n    return Theta_grad.reshape(-1)","47a565f4":"def MeanNorm(Y,R):\n    \n    Y_norm = Y*R\n    mean =  (np.sum(Y_norm, axis=1)\/np.sum((R == 1.0), axis=1)).reshape(Y.shape[0],1) * np.ones(Y.shape)\n    Y_norm = (Y_norm - mean)*R\n\n    return Y_norm, mean","452aaeea":"Y_norm, Y_mean = MeanNorm(Y,R_train)\n\nn_user = R.shape[1]\nn_feature = X.shape[1]\nlambd = 64. # L2 regularization; I ran the optimization multiple times with different values \n            # (1, 2, 4, 8...) and 64 results in the best validation FCP\n    \nTheta = np.random.normal(0,1,(n_user, n_feature)).reshape(-1) # initialize the weights\n\nresult = minimize(CostFunction, Theta, jac=Gradient, args=(X, Y_norm, R_train, lambd),\n                  options={'disp': True, 'maxiter': 500})","e3678b35":"Theta_opt = result.x.reshape(n_user, n_feature) # reshape the optimial parameters to a 2D matrix \nY_pred = np.dot(X, Theta_opt.T) + Y_mean\nprint(\"RMSE of the training set: {}\".format(np.sqrt(mean_squared_error(Y[R_train == 1], Y_pred[R_train == 1]))))\nprint(\"RMSE of the validation set: {}\".format(np.sqrt(mean_squared_error(Y[R_valid == 1], Y_pred[R_valid == 1]))))","3e0b17e1":"print(\"Training Set:\")\nFCP(Y,Y_pred,R_train)\nprint(\"\\n\")\nprint(\"Validation Set:\")\nFCP(Y,Y_pred,R_valid)","2e771183":"MakeBoxplot(Y_pred, Y, R_train, 'Training set')","0dee6167":"MakeBoxplot(Y_pred, Y, R_valid, 'Validation set')","0642784e":"user_info = profile[['smoker','drink_level','transport','budget']]\n\nprint(user_info.smoker.value_counts())\nprint('\\n')\nprint(user_info.drink_level.value_counts())\nprint('\\n')\nprint(user_info.transport.value_counts())\nprint('\\n')\nprint(user_info.budget.value_counts())","04e9e522":"# 1 for smokers, 0 otherwise; assume '?' to be non-smokers\nuser_info.smoker = user_info.smoker.map({'false': 0, 'true': 1, '?': 0})\n# map drink levels to numbers\nuser_info.drink_level = user_info.drink_level.map({'abstemious': 1, 'casual drinker': 2, 'social drinker': 3})\n# 1 for car owners, 0 otherwise; assume '?' to be not car owners\nuser_info.transport = user_info.transport.map({'public':0, 'car owner':1, 'on foot':0, '?':0})\n# map budget levels to numbers; assume '?' to be medium\nuser_info.budget = user_info.budget.map({'medium':2, 'low':1, 'high':3, '?':2})","630ed9f9":"user_info.head()","3aaed018":"Theta_modifier = pd.DataFrame(np.ones((n_user, n_feature)), columns=X.columns.values) \nTheta_modifier['parking_lot'] = user_info['transport'] # 0 or 1\nTheta_modifier['alcohol'] = user_info['drink_level'] # 1, 2 or 3\nTheta_modifier['smoking_area'] = user_info['smoker'] # 0 or 1\nTheta_modifier['price'] = 1.0\/user_info['budget'] # 1 (low), 1\/2 (medium) or 1\/3 (high)\nTheta_modifier = Theta_modifier.values","f2ce59b8":"def CostFunction2(params, X, Y, R, Theta_modifier, lambd):\n\n    num_user = R.shape[1]\n    num_feature = X.shape[1]\n\n    # reshape the Theta_modifier to 1-D\n    Theta_temp = Theta_modifier.reshape(-1)\n    # elements with non-zero values in Theta_modifier are multiplied to the input weights\n    Theta_temp[Theta_temp > 0] = Theta_temp[Theta_temp > 0] * params\n    \n    # reshape the parameters to a 2D matrix so we can perform matrix factorization.\n    # Elements with zero values in Theta_modifier always remain 0 in this matrix (for those \n    # users who don't have a car and those who don't smoke)\n    Theta = Theta_temp.reshape(num_user, num_feature)\n    J = 0.5 * np.sum( (np.dot(X, Theta.T) * R - Y)**2 )\n\n    # regularization\n    J = J + lambd\/2. * np.sum(Theta[:,:-1]**2) \n\n    return J\n\n\ndef Gradient2(params, X, Y, R, Theta_modifier, lambd):\n\n    num_user = R.shape[1]\n    num_feature = X.shape[1]\n\n    Theta_temp = Theta_modifier.reshape(-1)\n    Theta_temp[Theta_temp > 0] = Theta_temp[Theta_temp > 0] * params\n\n    Theta = Theta_temp.reshape(num_user, num_feature)\n    Theta_grad = np.dot((np.dot(Theta, X.T) * R.T - Y.T), X) \n\n    # regularization\n    Theta_grad[:,:-1] = Theta_grad[:,:-1] + lambd*Theta[:,:-1]\n    Theta_grad = Theta_grad * Theta_modifier\n\n    Theta_grad = Theta_grad[Theta_modifier > 0]\n    \n    return Theta_grad","6cb64c3f":"lambd = 64. # L2 regularization; I ran the optimization multiple times with different values \n            # (1, 2, 4, 8...) and 64 results in the best validation FCP\n    \nTheta = np.random.normal(0,1,(n_user, n_feature))[Theta_modifier > 0] # initialize the weights\n\nresult = minimize(CostFunction2, Theta, jac=Gradient2, args=(X, Y_norm, R_train, Theta_modifier, \n                  lambd), options={'disp': True, 'maxiter': 500})","9cd8ac3e":"# elements with non-zero values in Theta_modifier are multiplied to the optimal weights\nTheta_transformer = Theta_modifier.reshape(-1)\nTheta_transformer[Theta_transformer > 0] = Theta_transformer[Theta_transformer > 0] * result.x\n# reshape the parameters to a 2D matrix \nTheta_opt = Theta_transformer.reshape(n_user, n_feature)\n\nY_pred = np.dot(X, Theta_opt.T) + Y_mean\nprint(\"RMSE of the training set: {}\".format(np.sqrt(mean_squared_error(Y[R_train == 1], Y_pred[R_train == 1]))))\nprint(\"RMSE of the validation set: {}\".format(np.sqrt(mean_squared_error(Y[R_valid == 1], Y_pred[R_valid == 1]))))","901573d9":"print(\"Training Set:\")\nFCP(Y,Y_pred,R_train)\nprint(\"\\n\")\nprint(\"Validation Set:\")\nFCP(Y,Y_pred,R_valid)","edbe7003":"MakeBoxplot(Y_pred, Y, R_train, 'Training set')","038dfee1":"MakeBoxplot(Y_pred, Y, R_valid, 'Validation set')","20b44963":"R_test =  R*(-1) + 1.0 # 1 for those without existing ratings, 0 otherwise\n\n# predicted ratings for the restaurants that each user hasn't visited\nY_final = Y_pred * R_test\nY_final = pd.DataFrame( Y_final, columns=user_all, index=res_rated )\nY_final.head()","3abe258c":"from surprise import SVD \nfrom surprise.dataset import Reader, Dataset","36f15cb3":"uID = []\npID = []\nr = []\n\nfor i in range(R.shape[1]):\n    for j in range(R.shape[0]):\n\n        if R_train[j,i] == 1:\n            \n            pID.append(res_rated[j])\n            uID.append(user_all[i])\n            r.append(Y[j,i])\n\n\nrating_train = pd.DataFrame({'userID':uID, 'placeID':pID, 'rating':r})\nrating_train = rating_train[['userID','placeID','rating']]\n\nuID = []\npID = []\nr = []\n\nfor i in range(R.shape[1]):\n    for j in range(R.shape[0]):\n\n        if R_valid[j,i] == 1:\n            \n            pID.append(res_rated[j])\n            uID.append(user_all[i])\n            r.append(Y[j,i])\n\n\nrating_valid = pd.DataFrame({'userID':uID, 'placeID':pID, 'rating':r})\nrating_valid = rating_valid[['userID','placeID','rating']]","0916458a":"# A reader is needed to specify the rating_scale\nreader = Reader(rating_scale=(0, 2))\ndata_train = Dataset.load_from_df(rating_train, reader)\ntrainset = data_train.build_full_trainset() # use the whole training set ","1740415d":"model = SVD() # better performance can be done by playing with the parameters\nmodel.train(trainset)\n\n# my data form of ratings: shape=(130,138)\nY_pred = pd.DataFrame( np.zeros((len(res_rated),len(user_all)))-1.0, columns=user_all, index=res_rated )\n\nsize_valid = len(rating_valid)\nsize_train = len(rating_train)\nr_pred_valid = np.zeros(size_valid)\nr_pred_train = np.zeros(size_train)\n\n# obtain predictions for the validation set\nfor i in range(size_valid):\n    pred = model.predict(rating_valid.userID[i], rating_valid.placeID[i], verbose=False)\n    r_pred_valid[i] = pred.est\n\n# fill the predictions into my 'Y_pred' data frame\nfor r, u, o in zip(rating_valid.placeID, rating_valid.userID, r_pred_valid):\n    Y_pred.loc[r,u] = o\n\n# obtain predictions for the training set    \nfor i in range(size_train):\n    pred = model.predict(rating_train.userID[i], rating_train.placeID[i], verbose=False)\n    r_pred_train[i] = pred.est\n\n# fill the predictions into my 'Y_pred' data frame    \nfor r, u, o in zip(rating_train.placeID, rating_train.userID, r_pred_train):\n    Y_pred.loc[r,u] = o\n    \nY_pred = Y_pred.values    ","1e88b5e2":"print(\"RMSE of the training set: {}\".format(np.sqrt(mean_squared_error(Y[R_train == 1], Y_pred[R_train == 1]))))\nprint(\"RMSE of the validation set: {}\".format(np.sqrt(mean_squared_error(Y[R_valid == 1], Y_pred[R_valid == 1]))))","83a5624b":"print(\"Training Set:\")\nFCP(Y,Y_pred,R_train)\nprint(\"\\n\")\nprint(\"Validation Set:\")\nFCP(Y,Y_pred,R_valid)","394f4b1d":"MakeBoxplot(Y_pred, Y, R_train, 'Training set')","a1cd2c25":"MakeBoxplot(Y_pred, Y, R_valid, 'Validation set')","acee33e0":"Secondly, we need to perform \"mean normalization\" on the ratings. For each restaurant, we will subtract the average rating from all the ratings (in the training set). So after the normalization, the average rating is 0 for all the restaurant. The model will be trained on these normalized ratings instead of the original values. When we make the prediction, we will add those average ratings back.  \n<br>\nAccording to Andrew Ng's course, the reason for doing this is that if there is a user who never rated anything (so there is no data we can learn from this user), the predicted ratings for him\/her will be the average values.\n<br>\n<br>\nIn our dataset, all the users have rated some restaurants. However, (I think) mean normalization is still required to account for the overall food & service quality of a restaurant, which is not in the data. As I mentioned earlier, the ratings just don't depend that much on the restaurant features because they are not the main factors. When mean normalization is performed, what the model learns and predicts is the **deviation** from the average rating (as a function of user preference) instead of the absolute value of a rating. The average rating of each restaurant is the best we can use as a proxy of the overall food & service quality. If mean normalization is not performed, the model would perform poorly.","9eace6b0":"Now we can try the three models. I will only predict the overall ratings here (otherwise this notebook would be too long...)","72ab1585":"Unfortunately this doesn't really improve the result. Also, the two models perform better than the benchmark model by only a little (in terms of FCP). Later let's try the Surprise package and see if it can do a better job.","bfaf0cd1":"To load a data frame or csv file, the columns must correspond to user id, item id and ratings (in this order). In order to compare the result of SVD with my models, we have to use the same training\/validation split that are used above and transform them to the form that can be fed to Surprise.","30b68fcb":"Now I will combine the restaurant information to one data frame","cde851dc":"## Content-Based Model 1##\nLet's first implement the cost function and the gradient of its parameters using matrix factorization. For the mathematical detail, please refer to Andrew Ng's ML course: http:\/\/www.holehouse.org\/mlclass\/16_Recommender_Systems.html","a5bfe1ac":"## Evaluation Metrics ##\nI will use two evaluation metrics to quantify the model performances. The first one is root-mean-squre error (RMSE), which is very standard for regression problems because it tells you how close your predictions are to the labels. However, for recommender systems, the most important thing is the ranks of the products, and RMSE sometimes doesn't work will for this purpose. For example, a perfectly ranked prediction can have a terrible RMSE by predicting the ratings on the wrong scale.\n<br>\n<br>\nThus, the second metric I will use is **Fraction of Concordant Pairs (FCP)** Please refer to Section 5.2 of http:\/\/www.ijcai.org\/Proceedings\/13\/Papers\/449.pdf. The concept is the following. Suppose a user has rated n products, then there are n*(n-1)\/2 unique pairs of ratings. If product A receives a higher rating than product B from a user and the model predicts the same, A and B are a concordant pair, otherwise a discordant pair. FCP is simply the fraction of concordant pairs among all the pairs (sum over all users). Note that because the ratings are discrete integers, many rating pairs would actually be ties. For such cases I will not include them in the calculation of FCP.","246e873f":"For a person who has a car, whether the restaurant has parking is relevant and may influence what rating he\/she gives. Here I group the original parking options into only three categories (from best to worse): restaurant's free parking lot (2 points), other types of parking (1 point), and no parking (0 point)","a36938a8":"## Content-Based Model 2##\nFor this model, I will take into account the user information that is related to some of the restaurant features, namely, 'smoker', 'drink_level', 'transport', 'budget'. Let's first process the user data.","2d34f0f4":"## Benchmark Model ##\nThis model just computes the average rating of each restaurant.","7b462a22":"The parameters (weights) of the cost function correspond to those in \"Theta_modifier\" which has a value greater than 0","71828ee0":"## SVD with the Surprise Package##\nSurprise is an easy-to-use Python package for recommender systems. Please refer to their [project page](http:\/\/surpriselib.com\/) and [document page](http:\/\/surprise.readthedocs.io\/en\/stable\/index.html#) for details of installation and tutorials. Here I will use the famous [SVD algorithm](http:\/\/sifter.org\/simon\/journal\/20061211.html).","893e38a3":"Here I create a matrix, \"Theta_modifier\" with a shape = (n_user, n_feature), to **adjust the restaurant features the users see.** Most of the elements in this matrix are one, except for those corresponding to 'parking_lot', 'smoking_area', 'alcohol', and 'price':\n<br>\n(1) If a user doesn't have a car, he\/she wouldn't care if there is parking and a corresponding weight for 'parking_lot' is not needed. <br>\n(2) If a user doesn't smoke, he\/she wouldn't care if there is a smoking area and a corresponding weight for 'smoking_area' is not needed. <br>\n(3) Users with different 'drink_level' care differently about whether alcohol is provided in the restaurant.<br>\n(4) Users with different 'budget' care differently about the price. People with higher budget cares less.","656abdcc":"Only 130 out of 938 restaurants have ratings. I will later work on these 130 only because it's not wise to recommend a restaurant without ratings. I will now create three data frames for the three types of rating, each of which will have a shape of (938, 138). A restaurant-user pair without a rating will be recorded as -1 (to be different from 0, the lowest rating).","25e86948":"In the data frame \"geo\", some of the columns might be useful for rating prediction, while some are just names, locations, or websites. Locations are useful when recommendations are made, so I store them in a data frame \"res_info\". ","c6f07603":"Now I will combine the information of the restaurants and create a data frame for their features. I will skip \"accepts\" and \"hours\" for now because they shouldn't be revelant for modeling. However, their information will be required when recommendations are made.","3c7b398e":"# Content-based Recommenders #\n\nadded another evaluation metric, Fraction of Concordant Pairs (FCP) and included a [SVD model](http:\/\/sifter.org\/simon\/journal\/20061211.html) using [Surprise](http:\/\/surpriselib.com\/).**\n\nThis work is largely inspired by the Recommender Systems section in [Andrew Ng's ML course](https:\/\/www.coursera.org\/learn\/machine-learning) (course notes: http:\/\/www.holehouse.org\/mlclass\/).\n\nOur goal here is to predict the ratings that would be given by each consumer for the restaurants he\/she has not rated. A list of restaurants with the highest predicted ratings can then be recommended to each consumer. Because the restaurant ratings are numerical, predicting their values can be treated as a regression problem. Using classification techniques is also reasonable since the possible ratings are three discrete integers in this dataset. However, a three-class classification will likely predict too many ties and therefore prevent us from generating a top-n list of recommendations for a consumer. Rather than predicting the exact values of ratings that a consumer would give to certain restaurants, what is more important is predicting the ranks of these restaurants for the consumer. Thus, I will attempt to predict continuous values for the consumer ratings.\n\nI will use linear models to predict the ratings. The fundamental concepts are that each restaurant is characterized by a vector of features; each consumer preference is described by a vector of weights which has the same dimension as the product features; and the predicted rating of a restaurant-consumer pair equals the inner product of the two vectors. The mathematics is therefore equivalent to a simple linear regression. For a content-based approach, the features are already determined based on the product information, leaving the weights to be optimized. Instead of using an existing library for recommender systems, I will implement the cost functions of the problem and perform optimization using \"minimize()\" from the SciPy package. But I will also run a [SVD algorithm](http:\/\/sifter.org\/simon\/journal\/20061211.html) using [Surprise](http:\/\/surpriselib.com\/) (which is a collaborative filtering approach) at the end and compare its performance with mine.","bc0b1d06":"The performance of Surprise is kind of unstable. It's sometimes better than my models and sometime worse, which is possibly due to how the parameters are initialized.","a3ecf11c":"## Data Preprocessing ##\nLet's first inspect each of these data frames","48a1120c":"I will try three models. The first one is a benchmark model which simply predicts the average of the ratings that a restaurant already received. Therefore, we can compare other models with this benchmark to see if we are doing better than just using average ratings. The second one is a content-based predictor which uses the restaurant features above ('X'). The third one is another content-based predictor that also takes some user information into account (details and user data processing will be shown later).\n<br>\n<br>\nBefore we implement the models, let's split the existing ratings into a training set and a validation set. I will use a 75\/25 split. Note that what are being split are the rating instances, not the examples of restaurants or users. Rather than just randomly splitting the data, I will ensure that every restaurant and every consumer receives\/gives some ratings in the training set. This constraint makes sure that the characteristic of every restaurant and consumer is learned.","179c83f9":"Now we have 32 features for each of the restaurants. Removing irrelevant features can not only avoid overfitting, but also reduce dimensionality and therefore the computation time. One thing I tried was to compute the average ratings of the 138 restaurants, and then inspect the correlation coefficients between the features and average ratings. This can rank the importance of individual features and allows us to remove those with low correlation coefficients. However, at the end of the day I found that the performance of my model changes very little as I changes the number of features. This is probably because the ratings just don't depend a lot on these features. The main factors that affect the ratings are (1) whether the food is delicious and (2) whether a customer had a satisfying experience about the service provided. These two are definitely not something that are in the data. \n<br>\n<br>\nIn the following, I will only use 'parking_lot','alcohol','smoking_area','other_services','price','dress_code', and 'accessibility'. After some tests (which I skip here), I found the performance based on these 7 features is as good as a model that includes all the 32 features, and of course, this runs much faster.","dec4edd2":"With the predicted ratings for the restaurants each user hasn't visited, we can recommend the top-n list for each user along with the restaurant information. To avoid recommending a restaurant that is too far away from the user, we can use a search radius to pre-select all the places that are within this range. These should all be straightforward, so I will just stop here.","a4a2dd36":"The performance is better than the benchmark model.","0865ac71":"The following data frame tells us whether a restaurant-user pair has a rating. 0 means No and 1 means Yes.","484bab3e":"Now let's see how many restaurants and users in total"}}