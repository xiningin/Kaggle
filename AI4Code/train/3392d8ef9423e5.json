{"cell_type":{"fb80f011":"code","2572a989":"code","5f8003ab":"code","d482c5b2":"code","b8f366ad":"code","0049e17f":"code","7fb0706f":"code","cc47663e":"code","15cc83f4":"code","7176fb03":"code","8fd62a45":"code","4448268a":"code","1f44eb86":"code","6b5123a4":"code","3b242811":"code","93c1c593":"code","b2f45c1d":"code","01134bc4":"code","5d807914":"code","da65d6e6":"code","d6c9f78a":"code","bc247a84":"code","1088f856":"code","dd5e09c3":"code","3979fac0":"code","ea72fe9d":"code","00056fa9":"code","6e10cd3a":"code","7352e9ad":"code","298b262f":"code","66ceb7e7":"code","1df5c2af":"code","e7e73c51":"code","da57c73a":"code","53cc2bd6":"code","52d2bba4":"code","fcedd7ba":"markdown","f8cbaec9":"markdown","78056798":"markdown","083d2bdb":"markdown","2035c87f":"markdown","0a6b3b61":"markdown","8430da3f":"markdown","cd616c72":"markdown","b84040b3":"markdown","2cbb4e96":"markdown"},"source":{"fb80f011":"import os\nimport cv2\nimport math\nimport numpy as np\nfrom collections import OrderedDict\nimport scikitplot\nimport seaborn as sns\nfrom matplotlib import pyplot\n\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import plot_model\n\nfrom keras import backend as K\nfrom keras.utils import np_utils\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","2572a989":"np.random.seed(42)","5f8003ab":"INPUT_PATH = \"..\/input\/ck48-5-emotions\/CK+48\/\"\n\ntotal_images = 0\nfor dir_ in os.listdir(INPUT_PATH):\n    count = 0\n    for f in os.listdir(INPUT_PATH + dir_ + \"\/\"):\n        count += 1\n    total_images += count\n    print(f\"{dir_} has {count} number of images\")\n    \nprint(f\"\\ntotal images: {total_images}\")","d482c5b2":"TOP_EMOTIONS = [\"happy\", \"surprise\", \"anger\", \"sadness\", \"fear\"]","b8f366ad":"img_arr = np.empty(shape=(total_images, 48, 48, 1))\nimg_label = np.empty(shape=(total_images))\nlabel_to_text = {}\n\nidx = 0\nlabel = 0\nfor dir_ in os.listdir(INPUT_PATH):\n    if dir_ in  TOP_EMOTIONS:\n        for f in os.listdir(INPUT_PATH + dir_ + \"\/\"):\n            img_arr[idx] = np.expand_dims(cv2.imread(INPUT_PATH + dir_ + \"\/\" + f, 0), axis=2)\n            img_label[idx] = label\n            idx += 1\n        label_to_text[label] = dir_\n        label += 1\n\nimg_label = np_utils.to_categorical(img_label)\n\nimg_arr.shape, img_label.shape, label_to_text","0049e17f":"X_train, X_test, y_train, y_test = train_test_split(img_arr, img_label, train_size=0.7, stratify=img_label, shuffle=True, random_state=42)\nX_train.shape, X_test.shape","7fb0706f":"fig = pyplot.figure(1, (10,10))\n\nidx = 0\nfor k in label_to_text:\n    sample_indices = np.random.choice(np.where(y_train[:,k]==1)[0], size=5, replace=False)\n    sample_images = X_train[sample_indices]\n    for img in sample_images:\n        idx += 1\n        ax = pyplot.subplot(5,5,idx)\n        ax.imshow(img.reshape(48,48), cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(label_to_text[k])\n        pyplot.tight_layout()","cc47663e":"# data normalization\nX_train = X_train \/ 255.\nX_test = X_test \/ 255.","15cc83f4":"def build_dcnn(input_shape, num_classes):\n    model_in = Input(shape=input_shape, name=\"input\")\n    \n    conv2d_1 = Conv2D(\n        filters=64,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_1'\n    )(model_in)\n    batchnorm_1 = BatchNormalization(name='batchnorm_1')(conv2d_1)\n    conv2d_2 = Conv2D(\n        filters=64,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_2'\n    )(batchnorm_1)\n    batchnorm_2 = BatchNormalization(name='batchnorm_2')(conv2d_2)\n    \n    maxpool2d_1 = MaxPooling2D(pool_size=(2,2), name='maxpool2d_1')(batchnorm_2)\n    dropout_1 = Dropout(0.3, name='dropout_1')(maxpool2d_1)\n\n    conv2d_3 = Conv2D(\n        filters=128,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_3'\n    )(dropout_1)\n    batchnorm_3 = BatchNormalization(name='batchnorm_3')(conv2d_3)\n    conv2d_4 = Conv2D(\n        filters=128,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_4'\n    )(batchnorm_3)\n    batchnorm_4 = BatchNormalization(name='batchnorm_4')(conv2d_4)\n    \n    maxpool2d_2 = MaxPooling2D(pool_size=(2,2), name='maxpool2d_2')(batchnorm_4)\n    dropout_2 = Dropout(0.3, name='dropout_2')(maxpool2d_2)\n\n    conv2d_5 = Conv2D(\n        filters=256,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_5'\n    )(dropout_2)\n    batchnorm_5 = BatchNormalization(name='batchnorm_5')(conv2d_5)\n    conv2d_6 = Conv2D(\n        filters=256,\n        kernel_size=(3,3),\n        activation='elu',\n        padding='same',\n        kernel_initializer='he_normal',\n        name='conv2d_6'\n    )(batchnorm_5)\n    batchnorm_6 = BatchNormalization(name='batchnorm_6')(conv2d_6)\n    \n    maxpool2d_3 = MaxPooling2D(pool_size=(2,2), name='maxpool2d_3')(batchnorm_6)\n    dropout_3 = Dropout(0.3, name='dropout_3')(maxpool2d_3)\n\n    flatten = Flatten(name='flatten')(dropout_3)\n    \n    dense_1 = Dense(\n        128,\n        activation='elu',\n        kernel_initializer='he_normal',\n        name='dense1'\n    )(flatten)\n    batchnorm_7 = BatchNormalization(name='batchnorm_7')(dense_1)\n    dropout_4 = Dropout(0.4, name='dropout_4')(batchnorm_7)\n\n    model_out = Dense(\n        num_classes,\n        activation='softmax',\n        name='out_layer'\n    )(dropout_4)\n\n    model = Model(inputs=model_in, outputs=model_out, name=\"DCNN\")\n    \n    return model","7176fb03":"INPUT_SHAPE = (48, 48, 1)\noptim = optimizers.Adam(0.001)\n\nmodel = build_dcnn(input_shape=(48,48,1), num_classes=len(label_to_text))\nmodel.compile(\n        loss='categorical_crossentropy',\n        optimizer=optim,\n        metrics=['accuracy']\n)\n\nplot_model(model, show_shapes=True, show_layer_names=True, expand_nested=True, dpi=50, to_file='model.png')","8fd62a45":"early_stopping = EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.00008,\n    patience=12,\n    verbose=1,\n    restore_best_weights=True,\n)\n\nlr_scheduler = ReduceLROnPlateau(\n    monitor='val_accuracy',\n    min_delta=0.0001,\n    factor=0.4,\n    patience=6,\n    min_lr=1e-7,\n    verbose=1,\n)\n\ncallbacks = [\n    early_stopping,\n    lr_scheduler,\n]\n\nbatch_size = 10\nepochs = 60","4448268a":"train_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.15,\n    height_shift_range=0.15,\n    shear_range=0.15,\n    zoom_range=0.15,\n    horizontal_flip=True,\n)\ntrain_datagen.fit(X_train)","1f44eb86":"history = model.fit_generator(\n    train_datagen.flow(X_train, y_train, batch_size=batch_size),\n    validation_data=(X_test, y_test),\n    steps_per_epoch=len(X_train) \/ batch_size,\n    epochs=epochs,\n    callbacks=callbacks,\n    use_multiprocessing=True\n)","6b5123a4":"sns.set()\nfig = pyplot.figure(0, (12, 4))\n\nax = pyplot.subplot(1, 2, 1)\nsns.lineplot(history.epoch, history.history['accuracy'], label='train')\nsns.lineplot(history.epoch, history.history['val_accuracy'], label='valid')\npyplot.title('Accuracy')\npyplot.tight_layout()\n\nax = pyplot.subplot(1, 2, 2)\nsns.lineplot(history.epoch, history.history['loss'], label='train')\nsns.lineplot(history.epoch, history.history['val_loss'], label='valid')\npyplot.title('Loss')\npyplot.tight_layout()\n\npyplot.savefig('epoch_history.png')\npyplot.show()","3b242811":"label_to_text","93c1c593":"text_to_label = dict((v,k) for k,v in label_to_text.items())\ntext_to_label","b2f45c1d":"yhat_test = model.predict(X_test)\nyhat_test = np.argmax(yhat_test, axis=1)\nytest_ = np.argmax(y_test, axis=1)\n\nscikitplot.metrics.plot_confusion_matrix(ytest_, yhat_test, figsize=(7,7))\npyplot.savefig(\"confusion_matrix_model3pipes.png\")\n\ntest_accu = np.sum(ytest_ == yhat_test) \/ len(ytest_) * 100\nprint(f\"test accuracy: {round(test_accu, 4)} %\\n\\n\")\n\nprint(classification_report(ytest_, yhat_test))","01134bc4":"layer_list = [(layer.name, layer) for layer in model.layers if \"conv\" in layer.name]\nlayer_list","5d807914":"INTERESTED_CONV_LAYERS = [\"conv2d_1\", \"conv2d_4\"]","da65d6e6":"%%time\n\nidx = 1\nfor layer in layer_list:\n    if layer[0] in INTERESTED_CONV_LAYERS:        \n        layer_output = layer[1].output\n        filters, bias = layer[1].get_weights()     \n        filters = (filters - filters.min()) \/ (filters.max() - filters.min())\n    \n        cols = 20\n        rows = math.ceil(filters.shape[-1] \/ cols)\n        fig = pyplot.figure(i, (20, rows))\n\n        idx += 1\n        for i,f in enumerate(np.rollaxis(filters, 3)):\n            ax = pyplot.subplot(rows, cols, i+1)\n            f = np.mean(f, axis=2)\n            ax.imshow(f, cmap=\"viridis\")\n            ax.set_xticks([])\n            ax.set_yticks([])\n            pyplot.suptitle(f\"layer name: {layer[0]}, {filters.shape[3]} filters of shape {filters.shape[:-1]}\", fontsize=20, y=1.1)\n            pyplot.tight_layout()","d6c9f78a":"sns.reset_orig()\nsample_img = X_test[0] # select a random image\npyplot.imshow(sample_img.reshape(48,48), cmap=\"gray\")\npyplot.show()","bc247a84":"sample_img = np.expand_dims(sample_img, axis=0)\nsample_img.shape","1088f856":"INTERESTED_CONV_LAYERS = [\"conv2d_1\", \"conv2d_4\"]","dd5e09c3":"%%time\n\ni = 1\nfor layer in layer_list:\n    if layer[0] in INTERESTED_CONV_LAYERS:    \n        model_conv2d = Model(inputs=model.inputs, outputs=layer[1].output)\n        featuremaps_conv2d = model_conv2d.predict(sample_img)\n\n        cols = 20\n        rows = math.ceil(featuremaps_conv2d.shape[-1] \/ cols)\n        fig = pyplot.figure(i, (20, rows))        \n        i += 1\n        \n        for idx, feature_map in enumerate(np.rollaxis(featuremaps_conv2d, axis=3)):\n            ax = pyplot.subplot(rows, cols ,idx+1)\n            ax.imshow(feature_map[0], cmap=\"viridis\")\n            ax.set_xticks([])\n            ax.set_yticks([])\n            pyplot.suptitle(f\"layer name: {layer[0]}, feature map shape: {featuremaps_conv2d.shape}\", fontsize=20, y=1.1)\n            pyplot.tight_layout()","3979fac0":"INTERESTED_CONV_LAYERS = [\"conv2d_1\", \"conv2d_2\", \"conv2d_3\", \"conv2d_4\", \"conv2d_5\", \"conv2d_6\"]","ea72fe9d":"preds = model.predict(sample_img)\nlabel_to_text[np.argmax(preds[0])]","00056fa9":"pred_vector_output = model.output[:, np.argmax(preds[0])]\npred_vector_output","6e10cd3a":"heatmaps = []\n\nfor layer in layer_list:\n    if layer[0] in INTERESTED_CONV_LAYERS:\n        some_conv_layer = model.get_layer(layer[0])\n        grads = K.gradients(pred_vector_output, some_conv_layer.output)[0]\n        pooled_grads = K.mean(grads, axis=(0, 1, 2))\n        iterate = K.function([model.input], [pooled_grads, some_conv_layer.output[0]])\n        pooled_grads_value, conv_layer_output_value = iterate([sample_img])\n\n        for i in range(model.get_layer(layer[0]).output_shape[-1]):\n            conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n\n        heatmaps.append(np.mean(conv_layer_output_value, axis=-1))","7352e9ad":"fig = pyplot.figure(figsize=(14, 3))\n\nfor i, (name,hm) in enumerate(zip(INTERESTED_CONV_LAYERS, heatmaps)):\n    ax = pyplot.subplot(1, 6, i+1)\n    img_heatmap = np.maximum(hm, 0)\n    img_heatmap \/= np.max(img_heatmap)\n    ax.imshow(img_heatmap, cmap=\"viridis\")\n    ax.set_xticks([])\n    ax.set_yticks([])\n    pyplot.title(name)\n    pyplot.tight_layout()","298b262f":"fig = pyplot.figure(figsize=(14, 3))\n\nfor i, (name,hm) in enumerate(zip(INTERESTED_CONV_LAYERS, heatmaps)):\n    img_heatmap = np.maximum(hm, 0)\n    img_heatmap \/= np.max(img_heatmap)\n    \n    img_hm = cv2.resize(img_heatmap, (48,48))\n    img_hm = np.uint8(255 * img_hm)\n\n    img_hm = cv2.applyColorMap(img_hm, cv2.COLORMAP_JET)\n\n    # 0.4 here is a heatmap intensity factor\n    superimposed_img = img_hm * 0.4 + sample_img\n\n    ax = pyplot.subplot(1, 6, i+1)\n    ax.imshow(superimposed_img[0,:,:])\n    ax.set_xticks([])\n    ax.set_yticks([])\n    pyplot.title(name)\n    pyplot.tight_layout()","66ceb7e7":"! pip install keract","1df5c2af":"from keract import get_activations, display_activations, display_heatmaps","e7e73c51":"activations = get_activations(model, sample_img)\nactivations.keys()","da57c73a":"INTERESTED_CONV_LAYERS = [\"conv2d_1\", \"conv2d_2\", \"conv2d_3\", \"conv2d_4\", \"conv2d_5\", \"conv2d_6\", \"dense1\", \"out_layer\"]\n\nactivations_convs = OrderedDict()\n\nfor k,v in activations.items():\n    if k in INTERESTED_CONV_LAYERS:\n        activations_convs[k] = v\n        \nactivations_convs.keys()","53cc2bd6":"display_activations(activations_convs, save=False)","52d2bba4":"display_heatmaps(activations_convs, sample_img, save=False)","fcedd7ba":"#### Plot Feature Maps","f8cbaec9":"#### Plot Filters","78056798":"## Let's Visualize What our CNN Learned","083d2bdb":"#### Plot Class Activation Map (CAM) \nThe following CAM is taken from [here](https:\/\/github.com\/himanshurawlani\/convnet-interpretability-keras\/blob\/master\/Visualizing%20heatmaps\/heatmap_visualization_using_gradcam.ipynb).","2035c87f":"`sadness` and `fear` has very low number of images as compared to other classes","0a6b3b61":"We can see that our model focuses on important aspects of the image i.e., `lips`, `eyes` and `eyebrows`.","8430da3f":"As there are many layers in the model and visualizing all of them is not very informative, so I just plotted the interested ones i.e., conv layers","cd616c72":"**If you like this notebook then please upvote and share with others and also see my other kernels on [EDA](https:\/\/www.kaggle.com\/gauravsharma99\/eda-on-mpg-data), [Statistical Analysis](https:\/\/www.kaggle.com\/gauravsharma99\/statistical-analysis-on-mpg-data), [NSL](https:\/\/www.kaggle.com\/gauravsharma99\/neural-structured-learning), [etc.](https:\/\/www.kaggle.com\/notebooks)**","b84040b3":"We can see that the first convolutional layer has detected many edges in the image and indeed this is what we expected.","2cbb4e96":"Infact this all can be done just using few lines of code using some interesting libraries like `keract`, `keras-viz`, `lucid`. Below I showed keract"}}