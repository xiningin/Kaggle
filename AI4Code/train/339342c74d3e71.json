{"cell_type":{"9587fa92":"code","128b372c":"code","c43a37f6":"code","06d931be":"code","49e21432":"code","5c4ffa12":"code","9666ad76":"code","bbc7b850":"code","f2d926bb":"code","d878fb3a":"code","a4c28b4c":"code","2d5568e7":"code","c1e0ee71":"code","abdaf424":"code","e4f4a705":"code","7db458c8":"code","c11b678a":"code","f9ffa9c0":"code","5b075e52":"code","a5522fc8":"code","ddb0f93a":"code","c34972ba":"code","055a57de":"code","c4cf5e86":"code","1fe11de6":"code","284bee4c":"code","518cd7ee":"code","e0715b19":"code","8bd025ec":"code","b9ecc23b":"code","7f0accfe":"code","a6a221d7":"markdown","bfa3f962":"markdown","fa861969":"markdown","8666a3e2":"markdown","d5f7461f":"markdown","7d38580c":"markdown","0be87d92":"markdown","501cc379":"markdown","23ecba39":"markdown","69bb8dd8":"markdown","cf1a38ff":"markdown","d49d9e4c":"markdown","746e668d":"markdown","c9606c77":"markdown","3cd1d203":"markdown","136e3948":"markdown","0d514438":"markdown","15b9f2a9":"markdown","3fc25caa":"markdown","b69ff835":"markdown","2bad115d":"markdown","5dd9d39b":"markdown","b6b8e780":"markdown","e2b19ef8":"markdown","472cb99e":"markdown","d2858edf":"markdown","50b6ffc6":"markdown","ff5d2fbf":"markdown","c092268e":"markdown","842bb570":"markdown","6ab6e3a9":"markdown","6c865cd0":"markdown","90a1b3ab":"markdown","4ec1a344":"markdown"},"source":{"9587fa92":"import pandas as pd\nimport numpy as np\nfrom wordcloud import WordCloud, STOPWORDS\nimport seaborn as sns\nfrom os import path\nsns.set()\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score\nimport datetime as dt\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nimport calendar\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n%matplotlib inline\nimport time\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM, Bidirectional\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing import text\nfrom keras.layers import Conv1D, Flatten,MaxPooling1D\nfrom keras.preprocessing import text\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.metrics import auc\n","128b372c":"# Importing the csv data files \nsarcasm_df = pd.read_csv('..\/input\/sarcasm\/train-balanced-sarcasm.csv')\nsarcasm_df.head()","c43a37f6":"# Data Pre-Processing\n# Removing the null comments\nsarcasm_df.dropna(subset=['comment'], inplace=True)\nsarcasm_df['comment'] = sarcasm_df['comment'].str.lower()\nsarcasm_df['comment'] = sarcasm_df['comment'].str.replace('[^\\w\\s]','')","06d931be":"# Converting the timestamp into DateTime object\nsarcasm_df.created_utc = pd.to_datetime(sarcasm_df.created_utc)\nsarcasm_df.info()","49e21432":"plt.figure(figsize=(5,5))\nax = sns.countplot(x='label',  data= sarcasm_df)\nax.set(title = \"Distribution of Classes\", xlabel=\"Sarcasm Status\", ylabel = \"Total Count\")\ntotal = float(len(sarcasm_df ))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.1f}%'.format((height\/total)*100),\n            ha=\"center\") \nplt.show()","5c4ffa12":"# Distribution of the lenth of Sarcastic comments\nsns.boxplot(x= sarcasm_df.loc[sarcasm_df['label'] == 1, 'comment'].str.len()).set(title = 'Length of Sarcastic Comments', xlabel = 'Length')\nsns.despine(offset=10, trim=True)\nplt.show()","9666ad76":"# Distribution of the lenth of Neutral comments\nsns.boxplot(x= sarcasm_df.loc[sarcasm_df['label'] == 0, 'comment'].str.len()).set(title = 'Length of Neutral Comments', xlabel = 'Length')\nsns.despine(offset=10, trim=True)\nplt.show()","bbc7b850":"sarcasm_df['log_comment'] = sarcasm_df['comment'].apply(lambda text: np.log1p(len(text)))\nsarcasm_df[sarcasm_df['label']==1]['log_comment'].hist(alpha=0.6,label='Sarcastic', color = 'blue')\nsarcasm_df[sarcasm_df['label']==0]['log_comment'].hist(alpha=0.6,label='Non-Sarcastic', color = 'red')\nplt.legend()\nplt.title('Natural Log Length of Comments')\nplt.show()","f2d926bb":"wordcloud = WordCloud(background_color='black', stopwords = STOPWORDS,\n                max_words = 200, max_font_size = 100, \n                random_state = 17, width=800, height=400)\n\nplt.figure(figsize=(12, 12))\nwordcloud.generate(str(sarcasm_df.loc[sarcasm_df['label'] == 1, 'comment']))\nplt.grid(b= False)\nplt.imshow(wordcloud);","d878fb3a":"# Converting the scores into numpy array\nsarcasm_score = np.array(sarcasm_df.loc[sarcasm_df['label'] == 1]['score'])\nneutral_score = np.array(sarcasm_df.loc[sarcasm_df['label'] == 0]['score'])","a4c28b4c":"# Displaying the distribution of Marital Status in a Pie chart\nlabels = ['Sarcastic Score', 'Neutral Score']\nsizes = [3235069, 3725113]\n#colors\ncolors = ['#F21F3B', '#1FF257']\n \nplt.rcParams.update({'font.size': 14})\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, colors = colors, labels=labels, autopct='%1.1f%%', startangle=30)\nax1.set_title(\"Scores of Subreddits\")\n#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n# Equal aspect ratio ensures that pie is drawn as a circle\nax1.axis('equal') \nplt.tight_layout()\nplt.show()","2d5568e7":"sarcasm_comm_len = np.array(sarcasm_df.loc[sarcasm_df['label'] == 1]['comment'].str.len())\nparent_comm_len = np.array(sarcasm_df.loc[sarcasm_df['label'] == 1]['parent_comment'].str.len())\nratio_len = np.array((sarcasm_df.loc[sarcasm_df['label'] == 1]['comment'].str.len())\/(sarcasm_df.loc[sarcasm_df['label'] == 1]['parent_comment'].str.len()))","c1e0ee71":"dataset = pd.DataFrame({'Comment Length': sarcasm_comm_len, 'Parent Comment Length': parent_comm_len, 'Ratio Length': ratio_len}, columns=['Comment Length', 'Parent Comment Length', 'Ratio Length'])","abdaf424":"ax = plt.axes()\nsns.scatterplot(data=dataset, x=\"Comment Length\", y=\"Parent Comment Length\",  size=ratio_len)\nax.set_title(\"Comparing Sarcastic Comment Length with Parent Comment\")\n# control x and y limits\nplt.ylim(0, 12000)\nplt.xlim(0, 800)\nplt.show()","e4f4a705":"# Getting the top 5 popular subreddits\nsarcasm_df['subreddit'].value_counts()[:5]","7db458c8":"top_reddits =['AskReddit', 'politics', 'worldnews', 'leagueoflegends', 'pcmasterrace']","c11b678a":"subreddit = pd.DataFrame()\nsubreddit['subreddit'] = top_reddits\nsubreddit['sarcastic'] = np.nan\nsubreddit['natural'] = np.nan\nsubreddit['total'] = np.nan","f9ffa9c0":"# Calculating the count of Sarcastic and Natural comments for the top 5 subreddits \nfor i in range(len(top_reddits)):\n    temp = sarcasm_df.loc[sarcasm_df['subreddit'] == subreddit.subreddit.iloc[i]]\n    length = len(temp)\n    count_sarcastic = len(temp.loc[temp['label'] == 1])\n    subreddit.sarcastic.iloc[i] = count_sarcastic\n    subreddit.natural.iloc[i] = length - count_sarcastic\n    subreddit.total.iloc[i] = length","5b075e52":"# Initialize the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 5))\n# Plot the total comments for the subreddits\nsns.barplot(x=\"total\", y=\"subreddit\", data=subreddit,\n            label=\"Total\", color=\"b\")\n# Plot the total sarcastic comments for the subreddits\nsns.barplot(x=\"sarcastic\", y=\"subreddit\", data=subreddit,\n            label=\"Sarcastic Comments\", color=\"r\")\nax.legend(ncol=2, loc=\"lower right\", frameon=True)\nax.set( ylabel=\"Subreddits\",\n       xlabel=\"Total number of comments\")\nsns.despine(left=True, bottom=True)","a5522fc8":"# Feature Engineering- Extracting the day of a week\nsarcasm_df['created_utc'] = pd.to_datetime(sarcasm_df['created_utc'], format = '%d\/%m\/%Y %H:%M:%S')\nsarcasm_df['Day of Week'] = sarcasm_df['created_utc'].dt.day_name()","ddb0f93a":"# Visualization of Column- label\nplt.figure(figsize=(10,5))\nax = sns.countplot(x='Day of Week',  data= sarcasm_df.loc[sarcasm_df['label']==1])\nax.set(title = \"Count of sarcastic comments per day\", xlabel=\"Days of the week\", ylabel = \"Total Count\")\ntotal = float(len(sarcasm_df ))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 7,\n            '{:1.1f}%'.format((height\/total)*100*2),\n            ha=\"center\") \nplt.show()","c34972ba":"\nlogit = LogisticRegression(random_state= 42)\n\nvectorizer = TfidfVectorizer(use_idf=True, lowercase=True)\nX_tf_idf= vectorizer.fit_transform(sarcasm_df['comment'])\nx_train, x_test, y_train, y_test = train_test_split(X_tf_idf, sarcasm_df['label'], random_state=42)\n\n\n\nlogit.fit(x_train, y_train)\n\n","055a57de":"predicted_class=logit.predict(x_test)\npredicted_class_train=logit.predict(x_train)\ntest_probs = logit.predict_proba(x_test)\ntest_probs = test_probs[:, 1]\nyhat = logit.predict(x_test)\ntrain_accuracy = accuracy_score(y_train,predicted_class_train)\ntest_accuracy = accuracy_score(y_test,predicted_class)\nprint(\"Train accuracy score: \", train_accuracy)\nprint(\"Test accuracy score: \",test_accuracy )\nprint()","c4cf5e86":"sarcasm_df.reset_index(drop = True, inplace = True)\ncorpus = [sarcasm_df['comment'][i] for i in range( len(sarcasm_df))]\n\nvoc_size=5000\n\nonehot_=[one_hot(words,voc_size)for words in corpus] \n\nmax_sent_length = 20\n\nembedded_docs=pad_sequences(onehot_,padding='pre',maxlen=max_sent_length)\n    \nembedding_vector_features=40\n\nX_final=np.array(embedded_docs)\ny_final=np.array(sarcasm_df['label'])\n\nX_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X_final, y_final, test_size=0.33, random_state=42)","1fe11de6":"lstm_model=Sequential()\nlstm_model.add(Embedding(voc_size,embedding_vector_features,input_length=max_sent_length))\nlstm_model.add(Dropout(0.3))\nlstm_model.add(Bidirectional(LSTM(100)))\nlstm_model.add(Dropout(0.3))\nlstm_model.add(Flatten())\nlstm_model.add(Dense(1,activation='sigmoid'))\nlstm_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","284bee4c":"\nlstm_model.fit(X_train_lstm,y_train_lstm,validation_data=(X_test_lstm,y_test_lstm),epochs=10,batch_size=64)\n","518cd7ee":"y_test_pred=lstm_model.predict_classes(X_test_lstm)\ny_train_pred=lstm_model.predict_classes(X_train_lstm)\ntest_acc_lstm = accuracy_score(y_test_lstm,y_test_pred)\ntrain_acc_lstm = accuracy_score(y_train_lstm,y_train_pred)\n\n\nprint('Train accuracy of lstm: ',train_acc_lstm)\n\nprint('Test accuracy of lstm: ', test_acc_lstm)","e0715b19":"\nvocab_size = 1000\nmaxlen = 1000\nembedding_dims = 50\nfilters = 32\nkernel_size = 3\nhidden_dims = 250\nepochs = 10","8bd025ec":"X_train, X_test, y_train, y_test = train_test_split(sarcasm_df['comment'], sarcasm_df['label'], test_size=0.33, random_state=42)\n\ntokenizer = text.Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(X_train)\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n","b9ecc23b":"cnn_model = Sequential()\ncnn_model.add(Embedding(vocab_size,embedding_dims,input_length=maxlen))\ncnn_model.add(Dropout(.5))\ncnn_model.add(Conv1D(filters, kernel_size,padding = 'valid', activation = 'relu'))\ncnn_model.add(MaxPooling1D())\ncnn_model.add(Conv1D(filters, kernel_size,padding = 'valid', activation = 'relu'))\ncnn_model.add(MaxPooling1D())\ncnn_model.add(Flatten())\ncnn_model.add(Dense(hidden_dims, activation ='relu'))\ncnn_model.add(Dropout(.5))\ncnn_model.add(Dense(1, activation='sigmoid'))\n\n\ncnn_model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n\ncnn_model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)\n","7f0accfe":"logit = LogisticRegression(random_state= 42)\n\nvectorizer = TfidfVectorizer(use_idf=True, lowercase=True)\nX_tf_idf= vectorizer.fit_transform(sarcasm_df['comment'])\n\nlogit.fit(X_tf_idf,  sarcasm_df['label'])\n\npickle.dump(vectorizer, open('vectorizer.pkl', 'wb'))\npickle.dump(logit, open('logit.pkl', 'wb'))\n\n# Saving the model and the tf idf object so as to transform the new data which is to be predicted if that is sarcastic or not\nlogit = pickle.load(open('logit.pkl','rb'))\nvectorizer=pickle.load(open('vectorizer.pkl','rb'))\n","a6a221d7":"\n\n\n\n## <p style=\"background-color:#0000FF;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Popularity of the comments according to being sarcastic<\/p>","bfa3f962":"Since the dataset is skewed log transformations are being made\n\nNatural Log Length of Comments for Sarcastic and Non-Sarcastic Comments","fa861969":"Lets see if there is any relation between the length of the comment and the comment being sarcastic","8666a3e2":"> **Bidirectional LSTM is performing better here, although not by a great margin..**\n\n> **Created the app using Flask.**\n\n> **Deployed the model on heroku. Checkout the app, if you want to go through the code, checkout my github.**\n\n> **App link: https:\/\/check-if-its-sarcastic-or-not.herokuapp.com\/**","d5f7461f":"\n\n## <p style=\"background-color:#0000FF;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Using a CNN<\/p>","7d38580c":"\n<a id=\"4\"><\/a>\n# <p style=\"background-color:#0000FF;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">4. Modelling<\/p>","0be87d92":"> Using a bidirectional LSTM, because it can help the model detect the context of the statement in a better way which can be missed out otherwise if using an LSTM.","501cc379":"\n## <p style=\"background-color:#0000FF;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Distribution of the classes in the dataset<\/p>","23ecba39":"According to the dataset sarcastic comments tend to be less popular due to having lower overall scores.","69bb8dd8":"In this EDA we will analyse the proportion of sarcastic comments for top 5 Subreddits in the dataset. ","cf1a38ff":"\n\n\n## <p style=\"background-color:#0000FF;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Wordcloud of  Sarcastic comments<\/p>","d49d9e4c":"\n\n## <p style=\"background-color:#0000FF;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Using Logistic Regression<\/p>","746e668d":"According to the above visual, we can conclude that in most of the cases; the length of the sarcastic comments is longer than its corresponding parent comment.","c9606c77":"According to the graph above the lenght of the sarcastic comments is notmally distributed where as the non-sarcastic comments is slightly negatively skewed.","3cd1d203":"The figure above ensures that the dataset is balanced as the proportion of the sarcastic and non-sarcastic comments are same i.e.- 50%","136e3948":"## <p style=\"background-color:#0000FF;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Preparing the model for deployment using Logistic Regression<\/p>","0d514438":"\n<a id=\"1\"><\/a>\n# <p style=\"background-color:#0000FF;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">1. Importing Libraries<\/p>","15b9f2a9":"> The model here certainly is not overfitting; however, I feel the accuracies can be improved. Working on tuning the model in next version.","3fc25caa":"\n\n## <p style=\"background-color:#0000FF;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Being sarcastic on a specific day of the week<\/p>","b69ff835":"According to the visual above we can see that the the count of the sarcastic comments decreases during the weekends. One of the reason for this issue could be due to the reduced number of traffic in Reddit during the weekends ","2bad115d":"Now, lets determine whether the length of the sarcastic comments is more than its parent comment.","5dd9d39b":"> **I am using logitic regression model for deployment. Also using the entire dataset to learn when deploying the model to expand the model's learning.**","b6b8e780":"\n<a id=\"2\"><\/a>\n# <p style=\"background-color:#0000FF;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">2. Loading Data<\/p>","e2b19ef8":"\n<a id=\"3\"><\/a>\n# <p style=\"background-color:#0000FF;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">3. Data PreProcessing And Visualizations<\/p>","472cb99e":"\n## <p style=\"background-color:#0000FF;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Top Five popular subreddits & Sarcastic comments<\/p>","d2858edf":"\n\n\n\n\n## <p style=\"background-color:#0000FF;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Length of Sarcastic comment compared to the parent comment<\/p>","50b6ffc6":"# Credits for EDA goes to @ASIF RANA, I am building on his work here.\n","ff5d2fbf":"In this EDA we are tying to figure out wehther the user of Reddit tend to be more sarcastic on a specific day of the week.","c092268e":"> Using a CNN here because each convolution will fire when a special pattern is detected. By varying the size of the kernels and concatenating the outputs,we can detect patterns of multiples size.","842bb570":"With the help of the score of the comments, we can determine whether the sarcastic comments are more popular in Reddit discussions.","6ab6e3a9":"\n<a id=\"5\"><\/a>\n# <p style=\"background-color:#0000FF;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">5. Results & Conclusions<\/p>","6c865cd0":"\n\n## <p style=\"background-color:#0000FF;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Length of the comments<\/p>","90a1b3ab":"\n\n# <p style=\"background-color:#0000FF;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">EDA\/ Classification\/Deployment if the text is sarcasmic or not with Bidirectional LSTM\/CNN\/Logistic Regression<\/p>\n\n<img src=\"https:\/\/thumbs.dreamstime.com\/z\/sarcasm-seal-print-scratced-style-blue-vector-rubber-print-sarcasm-title-corroded-texture-text-title-placed-138701372.jpg\"  width=\"1000\" height=\"600\">\n\n\n<p style=\"background-color:#0000FF;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Table Of Contents<\/p>   \n    \n* [1. Importing Modules](#1)\n    \n* [2. Loading Data](#2)\n    \n* [3. Data PreProcessing And Visualizations](#3)\n      \n* [4. Modelling](#4)\n    \n* [5. Results And Conclusion](#5)\n \n","4ec1a344":"\n\n## <p style=\"background-color:#0000FF;font-family:newtimeroman;color:#000000;font-size:150%;text-align:center;border-radius:40px 40px;\">Using a Bidirectional LSTM<\/p>"}}