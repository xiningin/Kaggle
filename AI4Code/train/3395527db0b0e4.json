{"cell_type":{"eb4f9d1d":"code","6e59dc8f":"code","41170e4e":"code","cf769c50":"code","80bb9c4c":"code","3726e472":"code","dd06b367":"code","bdac17f1":"code","74057f97":"code","3dd4e450":"code","58ff7df0":"code","00b48854":"code","4f7ac875":"code","81309828":"code","400a58f4":"code","877c8f84":"code","9877c18d":"code","90ae8299":"code","490ea777":"code","abc5309d":"code","4715ebfd":"code","d8ccfa77":"code","126746e8":"code","01d7e6b3":"code","2805211b":"code","ed8de989":"code","499ec405":"code","a8d0d936":"code","9403e4a2":"code","b0471da1":"code","f4479ed4":"code","491faf9c":"code","bd02c16f":"code","095cc38a":"code","e593d613":"code","53013230":"code","5e6b2dc9":"code","fba6cc2e":"code","1f613035":"code","66fb9a78":"code","43a652f4":"code","02026c0c":"code","d4066e31":"code","de95e5b0":"code","550b64f9":"code","96840ecd":"code","e2c512b2":"code","48693c46":"code","1effbc9b":"code","6943fd2e":"code","1c998c89":"code","7723b6a3":"code","dd830e2f":"code","5467fb59":"code","b8735aec":"code","06c983af":"code","ab6a5dc6":"code","041be3bc":"code","72b9ad1c":"code","a2f051ba":"code","f07ff325":"code","c8195075":"code","99a52142":"code","c5479bf1":"code","6173ff5e":"code","9f6e4559":"code","ed64c309":"code","71cc14bf":"code","88dbdc60":"code","e90c7461":"code","bff11049":"code","6e189fe3":"code","56838a15":"code","f9ad0ba4":"code","97b62539":"code","f8ed841b":"code","931ef8a0":"code","f7ab65d5":"code","c95a5be1":"code","d3ffe5b4":"code","4b460425":"code","60aa66d2":"code","1da4f59b":"code","963eb836":"code","68815ffb":"code","212c6971":"code","9ff899d7":"code","3d3c22b8":"code","f5083251":"code","c612850d":"markdown","ade40708":"markdown","fe654fea":"markdown","d8135360":"markdown","05d00618":"markdown","f32d4b8c":"markdown","3480c5a8":"markdown","67efab48":"markdown","576cf895":"markdown","6dfd9455":"markdown","4d9b5375":"markdown","ce53c035":"markdown","2a9545fe":"markdown","61910785":"markdown","409d567b":"markdown","bde719f4":"markdown","9577a709":"markdown","1050b83f":"markdown","3f84a17b":"markdown","b8d57d04":"markdown","bac25ed7":"markdown","a6171931":"markdown","0221738a":"markdown","2884d7b6":"markdown","4c772dbd":"markdown","5e7eed7d":"markdown","f5632ba2":"markdown","d60bae0a":"markdown","4990a062":"markdown","1621f890":"markdown","efc81501":"markdown","ac87eb41":"markdown","4829bf09":"markdown","2bb8ca82":"markdown","f115f4cb":"markdown","df416252":"markdown","6ca67de6":"markdown","f10cb2e0":"markdown","ea2f42ed":"markdown","b318a1ac":"markdown","ba38d71e":"markdown","d1039117":"markdown","2eaa1e42":"markdown","558f3440":"markdown","0595b978":"markdown","336f9fee":"markdown","bcd342dd":"markdown","68042525":"markdown","44989ce1":"markdown"},"source":{"eb4f9d1d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6e59dc8f":"# \uae30\ubcf8 \ub370\uc774\ud130 \uc815\ub9ac \ubc0f \ucc98\ub9ac\nimport numpy as np\nimport pandas as pd\n\n# \uc2dc\uac01\ud654\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\nimport missingno\n\n# \uc804\ucc98\ub9ac \ubc0f \uba38\uc2e0\ub7ec\ub2dd \uc54c\uace0\ub9ac\uc998\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# \ubaa8\ub378 \ud29c\ub2dd \ubc0f \ud3c9\uac00\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn import model_selection\n\n# \uacbd\uace0 \uc81c\uac70\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')","41170e4e":"test = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')","cf769c50":"# \ub370\uc774\ud130\ud504\ub808\uc784 \ud655\uc778\ntrain.head()","80bb9c4c":"train.describe(include='all')","3726e472":"# \uac01 \uc5f4\uc758 \ub370\uc774\ud130 \uc885\ub958\ntrain.dtypes","dd06b367":"# \ubcd1\ud569 \uc900\ube44\nntrain = train.shape[0]\nntest = test.shape[0]\n\n# y_train \uacb0\uacfc, test\uc758 \uc2b9\uac1dID(\ucd5c\uc885 \uacb0\uacfc\uc5d0 \ub123\uc5b4\uc57c \ub428) \ub530\ub85c \ube7c\ub193\uae30\ny_train = train['Survived'].values\npassId = test['PassengerId']\n\n# \ubcd1\ud569 \ud30c\uc77c \ub9cc\ub4e4\uae30\ndata = pd.concat((train, test))\n\n# \ub370\uc774\ud130 \ud06c\uae30 \ud655\uc778\nprint(\"data size is: {}\".format(data.shape))","bdac17f1":"train['Survived'].value_counts()","74057f97":"missingno.matrix(data, figsize=(15, 8))","3dd4e450":"# \uacb0\uce21\uac12 \uccb4\ud06c\ndata.isnull().sum()\n# isna()\ub97c \uc368\ub3c4 \ub611\uac19\uc74c","58ff7df0":"# \uac01 \uc5f4 \uc774\ub984 \ud655\uc778\ndata.columns","00b48854":"# Co-relation \ub9e4\ud2b8\ub9ad\uc2a4\ncorr = data.corr()\n\n# \ub9c8\uc2a4\ud06c \uc14b\uc5c5\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# \uadf8\ub798\ud504 \uc14b\uc5c5\nplt.figure(figsize=(14, 8))\n\n# \uadf8\ub798\ud504 \ud0c0\uc774\ud2c0\nplt.title('Overall Correlation of Titanic Features', fontsize=18)\n\n# Co-relation \ub9e4\ud2b8\ub9ad\uc2a4 \ub7f0\uce6d\nsns.heatmap(corr, mask=mask, annot=False,cmap='RdYlGn', linewidths=0.2, annot_kws={'size':20})\n\nplt.show()","4f7ac875":"# \uc0dd\uc874 \uc5ec\ubd80 0\uacfc 1\uc758 \uc22b\uc790\ub97c \uc138\uc5b4\ubcf8 \ud6c4, \uadf8\ub9bc\uc744 \uadf8\ub824\ubcf4\ub294 countplot \uc0dd\uc131\nfig = plt.figure(figsize=(10, 2))\nsns.countplot(y='Survived', data=train)\nprint(train.Survived.value_counts())","81309828":"def piecount(col):\n    f, ax = plt.subplots(1, 2, figsize=(15, 6))\n    train[col].value_counts().plot.pie(explode=[0.1 for i in range(train[col].nunique())], autopct='%1.1f%%', ax=ax[0], shadow=True)\n    ax[0].set_title(col)\n    ax[0].set_ylabel('')\n    sns.countplot(col, data=train, ax=ax[1])\n    ax[1].set_title(col)\n    plt.show()\n    \npiecount('Survived')  ","400a58f4":"# \uac01 \ud074\ub798\uc2a4 \ub2f9 \uc0dd\uc874\uc790 \ud655\uc778\npd.crosstab(train.Pclass, train.Survived, margins=True).style.background_gradient(cmap='summer_r')\n\n# train.groupby(['Pclass', 'Survived'])['Survived'].count()","877c8f84":"piecount(\"Pclass\")","9877c18d":"data.Name.value_counts()","90ae8299":"# ['Initial'] \uc5f4\uc744 \uc0c8\ub85c \ub9cc\ub4e4\uace0, Name\uc5d0\uc11c \ucd94\ucd9c\ud55c Regular Expression \ub123\uc5b4\uc90c\n# \ub300.\uc18c\ubb38\uc790 \uc911 .\uc73c\ub85c \ub05d\ub098\ub294 \ubd80\ubd84 \ucd94\ucd9c\ntemp = data.copy()\ntemp['Initial'] = 0\ntemp['Initial'] = data.Name.str.extract('([A-Za-z]+)\\.')\n","490ea777":"temp['Initial'].value_counts()","abc5309d":"# \uc0dd\uc874\ub960 \ud568\uc218\ndef survpct(col):\n    return temp.groupby(col)['Survived'].mean()\n\nsurvpct('Initial')","4715ebfd":"# Dona \uacb0\uce21\uac12\uc744 \ucc44\uc6b0\uae30 \uc704\ud574 \ub098\uc774 \ud655\uc778\ntemp.loc[temp['Initial'] == 'Dona']","d8ccfa77":"# \ub098\uc774\ub85c \ucd94\uce21\ud558\uace0 Mrs.\ub85c \ub123\uc5b4\uc90c\ntemp.loc[temp['Initial'] == 'Dona', 'Initial'] = 'Mrs'","126746e8":"# \uc131\ubcc4\uacfc \ud568\uaed8 \ud655\uc778\npd.crosstab(temp.Initial, temp.Sex).T.style.background_gradient(cmap='summer_r')","01d7e6b3":"# Last name\uc740 \uc804\ubd80 \uc22b\uc790\ub85c \ubc14\uafd4\uc90c\ntemp['LastName'] = data.Name.str.extract('([A-Za-z]+)')\ntemp['NumName'] = temp['LastName'].factorize()[0]\npd.crosstab(temp.NumName, temp.Survived).T.style.background_gradient(cmap='summer_r')","2805211b":"def bag(col, target, title, title1):\n    f,ax = plt.subplots(1, 2, figsize=(12,5))\n    train.groupby([col])[target].mean().plot(kind='bar', ax=ax[0])\n    ax[0].set_title(title)\n    sns.countplot(col, hue=target, data=train, ax=ax[1])\n    ax[1].set_title(title1)\n    plt.show()\n\nbag('Sex', 'Survived', 'Survived per Sex', 'Sex Survived vs Not Survived')","ed8de989":"pd.crosstab([train.Sex, train.Survived], train.Pclass, margins=True).style.background_gradient(cmap='summer_r')","499ec405":"print('Oldest Passenger was', data['Age'].max(), 'Years')\nprint('Youngest Passenger was', data['Age'].min(), 'Years')\nprint('Average Age on the ship was', int(data['Age'].mean()), 'Years')","a8d0d936":"f, ax = plt.subplots(1, 2, figsize=(18, 8))\nsns.violinplot(\"Pclass\", \"Age\", hue=\"Survived\", data=train, split=True, ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0, 110, 10))\nsns.violinplot(\"Sex\", \"Age\", hue=\"Survived\", data=train, split=True, ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0, 110, 10))\nplt.show()","9403e4a2":"# Initial\ubcc4 \ud3c9\uade0 \uc5f0\ub839 \ud655\uc778\ntemp.groupby('Initial').agg({'Age' : ['mean', 'count']})","b0471da1":"# \uc774\ub2c8\uc15c \ubcc4 \ud3c9\uade0 \uc5f0\ub839\uc744 \ube48 \uac12\uc5d0 \ub123\uc5b4\uc90c\ntemp = temp.reset_index(drop=True)\ntemp['Age'] = temp.groupby('Initial')['Age'].apply(lambda x: x.fillna(x.mean()))\ntemp[31:50]","f4479ed4":"# Initial \uc815\ub9ac\ntemp['Initial'].replace(['Capt', 'Col', 'Countess', 'Don', 'Dona' , 'Dr', 'Jonkheer', 'Lady', 'Major', 'Master',  'Miss'  ,'Mlle', 'Mme', 'Mr', 'Mrs', 'Ms', 'Rev', 'Sir'], ['Sacrificed', 'Respected', 'Nobles', 'Mr', 'Mrs', 'Respected', 'Mr', 'Nobles', 'Respected', 'Kids', 'Miss', 'Nobles', 'Nobles', 'Mr', 'Mrs', 'Nobles', 'Sacrificed', 'Nobles'],inplace=True)\ntemp['Initial'].replace(['Kids', 'Miss', 'Mr', 'Mrs', 'Nobles', 'Respected', 'Sacrificed'], [4, 4, 2, 5, 6, 3, 1], inplace=True)","491faf9c":"temp['Age_Range'] = pd.qcut(temp['Age'], 10) # 10\uac1c \uac12\uc529 \ub098\ub204\uc5b4\uc11c \ubc94\uc8fc \uc0dd\uc131\nsurvpct('Age_Range')\n","bd02c16f":"temp['Agroup'] = 0\ntemp.loc[temp['Age'] < 1.0, 'Agroup'] = 1\ntemp.loc[(temp['Age'] >= 1.0) & (temp['Age'] <= 3.0), 'Agroup'] = 2\ntemp.loc[(temp['Age'] > 3.0) & (temp['Age'] < 11.0), 'Agroup'] = 7\ntemp.loc[(temp['Age'] >= 11.0) & (temp['Age'] < 15.0), 'Agroup'] = 13\ntemp.loc[(temp['Age'] >= 15.0) & (temp['Age'] < 18.0), 'Agroup'] = 16\ntemp.loc[(temp['Age'] >= 18.0) & (temp['Age'] <=  20.0), 'Agroup'] = 18\ntemp.loc[(temp['Age'] > 20.0) & (temp['Age'] <= 22.0), 'Agroup'] = 21\ntemp.loc[(temp['Age'] > 22.0) & (temp['Age'] <= 26.0), 'Agroup'] = 24\ntemp.loc[(temp['Age'] > 26.0) & (temp['Age'] <= 30.0), 'Agroup'] = 28\ntemp.loc[(temp['Age'] > 30.0) & (temp['Age'] <= 32.0), 'Agroup'] = 31\ntemp.loc[(temp['Age'] > 32.0) & (temp['Age'] <= 34.0), 'Agroup'] = 33\ntemp.loc[(temp['Age'] > 34.0) & (temp['Age'] <= 38.0), 'Agroup'] = 36\ntemp.loc[(temp['Age'] > 38.0) & (temp['Age'] <= 52.0), 'Agroup'] = 45\ntemp.loc[(temp['Age'] > 52.0) & (temp['Age'] <= 75.0), 'Agroup'] = 60\ntemp.loc[temp['Age'] > 75.0, 'Agroup'] = 78\n\ntemp.head()","095cc38a":"# \uc704\ub97c \ubcf4\uace0 sex\ub97c \ub0a8, \ub140, 1\uc138 \uc774\ud558 Baby\ub85c \ub098\ub214\ntemp.loc[(temp['Sex'] == 'male'), 'Sex'] = 1\ntemp.loc[(temp['Sex'] == 'female'), 'Sex'] = 2\ntemp.loc[(temp['Age'] < 1), 'Sex'] = 3\n\nsurvpct('Sex')","e593d613":"temp.loc[(temp['SibSp'] == 0) & (temp['Parch'] == 0), 'Alone'] = 1\ntemp['Family'] = temp['Parch'] + temp['SibSp'] + 1\n\ntemp.head(n=10)","53013230":"bag('Parch', 'Survived', 'Survived per Parch', 'Parch Survived vs Not Survived')","5e6b2dc9":"# \uc2b9\uc120 \uc7a5\uc18c\ubcc4 \uc0dd\uc874 \ud655\ub960\nsns.factorplot('Embarked', 'Survived', data=temp)\nfig = plt.gcf()\nfig.set_size_inches(5, 3)\nplt.show()","fba6cc2e":"f,ax = plt.subplots(2, 2, figsize=(20,15))\nsns.countplot('Embarked', data=temp, ax=ax[0,0])\nax[0,0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked', hue='Sex', data=temp, ax=ax[1,0])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked', hue='Survived', data=temp, ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked', hue='Pclass', data=temp, ax=ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\n\nplt.show()","1f613035":"# \uacb0\uce21\uac12 2\uac1c \ud655\uc778\n\ntemp.loc[(temp.Embarked.isnull())]","66fb9a78":"# \ub450 \uc0ac\ub78c\uc758 \ud2f0\ucf13 \ubc88\ud638\uac00 \uac19\uc73c\ubbc0\ub85c \uac19\uc740 \ud2f0\ucf13 \ubc88\ud638\ub97c \uac00\uc9c4 \ub2e4\ub978 \uc0ac\ub78c\uc744 \ucc3e\uc74c\ntemp.loc[(temp.Ticket == '113572')]","43a652f4":"# \uc5c6\uc73c\ubbc0\ub85c \uac00\uc7a5 \ube44\uc2b7\ud55c \ubc88\ud638 \ucc3e\uae30\ntemp.sort_values(['Ticket'], ascending=True)[55:70]","02026c0c":"# \uc55e \ub4a4\ub85c \ubaa8\ub450 S\uc774\uace0 Pclass\ub3c4 \ubaa8\ub450 1\uc774\ubbc0\ub85c S\uc77c \uac00\ub2a5\uc131\uc774 \ud07c\ntemp.loc[(temp.Embarked.isnull()), 'Embarked'] = 'S'\ntemp.loc[(temp.Embarked.isnull())]","d4066e31":"temp['Embarked'] = temp['Embarked'].factorize()[0]\ntemp[11:20]","de95e5b0":"temp['Priority'] = 0\ntemp.loc[(temp['Initial'] == 6), 'Priority'] = 1\ntemp.loc[(temp['Pclass'] == 1) & (temp['Sex'] == 2), 'Priority'] = 2\ntemp.loc[(temp['Age'] < 1), 'Priority'] = 3\ntemp.loc[(temp['Pclass'] == 1) & (temp['Age'] <= 17), 'Priority'] = 4\ntemp.loc[(temp['Pclass'] == 2) & (temp['Age'] <= 17), 'Priority'] = 5","550b64f9":"survpct('Priority')","96840ecd":"temp.Priority.value_counts()","e2c512b2":"temp['F1'] = temp['Priority']\ntemp['F2'] = temp['Initial']\ntemp['F3'] = temp['NumName']\ntemp['F4'] = temp['Family']\ntemp['F5'] = temp['Embarked']\ntemp['F6'] = temp['Sex']\ntemp['F7'] = temp['Pclass']","48693c46":"temp","1effbc9b":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\n# DataFrame \uc0dd\uc131\ndfl = pd.DataFrame() # for label encoding\ngood_columns = ['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7']\ndfl[good_columns] = temp[good_columns]\n\ndfh = dfl.copy() # for One Hot Encoding\n\ndfl_enc = dfl.apply(LabelEncoder().fit_transform)\ndfl_enc.head()","6943fd2e":"one_hot_cols = dfh.columns.tolist()\ndfh_enc = pd.get_dummies(dfh, columns=one_hot_cols)\n\ndfh_enc.head()","1c998c89":"# \uc778\ucf54\ub529\ud55c \ud30c\uc77c\uc744 train\uacfc test\ub85c \ucabc\uac2c\ntrain = dfh_enc[:ntrain]\ntest = dfh_enc[ntrain:]","7723b6a3":"X_test = test\nX_train = train","dd830e2f":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","5467fb59":"ran = RandomForestClassifier(random_state=1)\nknn = KNeighborsClassifier()\nlog = LogisticRegression()\nxgb = XGBClassifier()\ngbc = GradientBoostingClassifier()\nsvc = SVC(probability=True)\next = ExtraTreesClassifier()\nada = AdaBoostClassifier()\ngnb = GaussianNB()\ngpc = GaussianProcessClassifier()\nbag = BaggingClassifier()\n\n# \ub9ac\uc2a4\ud2b8 \uc900\ube44\nmodels = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]\nmodel_names = ['Random Forest', 'K Nearest Neighbor', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier']\nscores = {}\n\n# \uc5f0\uc18d\uc801\uc73c\ub85c \ubaa8\ub378\uc744 \ud559\uc2b5 \uc2dc\ud0a4\uace0, \uad50\ucc28 \uac80\uc99d\nfor ind, mod in enumerate(models):\n    mod.fit(X_train, y_train)\n    acc = cross_val_score(mod, X_train, y_train, scoring=\"accuracy\", cv=10)\n    scores[model_names[ind]] = acc","b8735aec":"# \uacb0\uacfc \ud14c\uc774\ube14 \uc0dd\uc131\nresults = pd.DataFrame(scores).T\nresults['mean'] = results.mean(1)\n\nresult_df = results.sort_values(by='mean', ascending=False)\nresult_df.head(11)","06c983af":"result_df = result_df.drop(['mean'], axis=1)\nsns.boxplot(data=result_df.T, orient='h')\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')","ab6a5dc6":"# \uc911\uc694\ub3c4 \ud655\uc778 \ud568\uc218\ndef importance_plotting(data, xlabel, ylabel, title, n=20):\n    sns.set(style=\"whitegrid\")\n    ax = data.tail(n).plot(kind='barh')\n    \n    ax.set(title=title, xlabel=xlabel, ylabel=ylabel)\n    ax.xaxis.grid(False)\n    ax.yaxis.grid(True)\n    plt.show()","041be3bc":"# \ub370\uc774\ud130 \ud504\ub808\uc784\uc5d0 \ud56d\ubaa9 \uc911\uc694\ub3c4 \uc0bd\uc785\nfi = {'Features' :train.columns.tolist(), 'Importance':xgb.feature_importances_}\nimportance = pd.DataFrame(fi, index=fi['Features']).sort_values('Importance', ascending=True)","72b9ad1c":"# \uadf8\ub798\ud504 \uc81c\ubaa9\ntitle = 'Top 20 most important features in predicting survival on the Titanic: XGB'\n\n# \uadf8\ub798\ud504 \uadf8\ub9ac\uae30\nimportance_plotting(importance, 'Importance', 'Features', title, 20)","a2f051ba":"# \uc911\uc694\ub3c4\ub97c \ub370\uc774\ud130 \ud504\ub808\uc784\uc5d0 \ub123\uc74c. Logistic regression\uc5d0\uc11c\ub294 \uc911\uc694\ub3c4\ubcf4\ub2e4 coefficients\ub97c \uc0ac\uc6a9\n\n# Features\ub77c\ub294 \uc5f4\uc5d0 train\uc758 \uc5f4\ub4e4\uc758 \uc774\ub984\uc744 \ub9ac\uc2a4\ud2b8\ub85c \ub9cc\ub4e4\uc5b4 \ub123\uace0,\n# Importance\uc5d0\ub294 Logistic regression\uc758 coefficient\ub97c \ubc14\uafb8\uc5b4 \ub123\uc73c\ub77c\ub294 \ub118\ud30c\uc774 \uba85\ub839. (\uc989, \uac00\ub85c\ub97c \uc138\ub85c\ub85c)\nfi = {'Features':train.columns.tolist(), 'Importance':np.transpose(log.coef_[0])}\nimportance = pd.DataFrame(fi, index=fi['Features']).sort_values('Importance', ascending=True)\n\n# \uadf8\ub798\ud504 \uc81c\ubaa9\ntitle = 'Top 20 important features in predicting survival on the Titanic: Logistic Regression'\n\n# \uadf8\ub798\ud504 \uadf8\ub9ac\uae30\nimportance_plotting(importance, 'Importance', 'Features', title, 20)","f07ff325":"# 5\uac00\uc9c0 \ubaa8\ub378\uc5d0 \ub300\ud55c \ud56d\ubaa9 \uc911\uc694\ub3c4 \uc5bb\uae30\ngbc_imp = pd.DataFrame({'Feature':train.columns, 'gbc importance':gbc.feature_importances_})\nxgb_imp = pd.DataFrame({'Feature':train.columns, 'xgb importance':xgb.feature_importances_})\nran_imp = pd.DataFrame({'Feature':train.columns, 'ran importance':ran.feature_importances_})\next_imp = pd.DataFrame({'Feature':train.columns, 'ext importance':ext.feature_importances_})\nada_imp = pd.DataFrame({'Feature':train.columns, 'ada importance':ada.feature_importances_})\n\n# \uc774\ub97c \ud558\ub098\uc758 \ub370\uc774\ud130 \ud504\ub808\uc784\uc73c\ub85c\nimportances = gbc_imp.merge(xgb_imp, on='Feature').merge(ran_imp, on='Feature').merge(ext_imp, on='Feature').merge(ada_imp, on='Feature')\n\n# \ud56d\ubaa9 \ub2f9 \ud3c9\uade0 \uc911\uc694\ub3c4\nimportances['Average'] = importances.mean(axis=1)\n\n#\ub7ad\ud0b9 \uc815\ud558\uae30\nimportances = importances.sort_values(by='Average', ascending=False).reset_index(drop=True)","c8195075":"# \uc911\uc694\ub3c4\ub97c \ub2e4\uc2dc \ub370\uc774\ud130\ud504\ub808\uc784\uc5d0 \ub123\uc74c\nfi = {'Features':importances['Feature'], 'Importance':importances['Average']}\nimportance = pd.DataFrame(fi).set_index('Features').sort_values('Importance', ascending=True)\n\n# \uadf8\ub798\ud504 \uc81c\ubaa9\ntitle = 'Top 20 important features in predicting survival on the Titanic: 5 model average'\n\n# \uadf8\ub798\ud504 \uadf8\ub9ac\uae30\nimportance_plotting(importance, 'Importance', 'Features', title, 20)","99a52142":"importance1 = importance[-381:]\n\nimportance1[371:381]","c5479bf1":"# \uc601\uc591\uac00 \uc788\ub294 380\uac1c\ub9cc \ub123\uc74c\nmylist = list(importance1.index)","6173ff5e":"train1 = pd.DataFrame()\ntest1 = pd.DataFrame()\n\nfor i in mylist:\n    train1[i] = train[i]\n    test1[i] = test[i]\n    \ntrain1.head()","9f6e4559":"train = train1\ntest = test1\n\n# \ubaa8\ub378\uc758 \ubcc0\uc218 \ub2e4\uc2dc \uc815\uc758\nX_train = train\nX_test = test\n\n# \ubc14\uafd4\uc90c\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","ed64c309":"ran = RandomForestClassifier(random_state=1)\nknn = KNeighborsClassifier()\nlog = LogisticRegression()\nxgb = XGBClassifier(random_state=1)\ngbc = GradientBoostingClassifier(random_state=1)\nsvc = SVC(probability=True)\next = ExtraTreesClassifier(random_state=1)\nada = AdaBoostClassifier(random_state=1)\ngnb = GaussianNB()\ngpc = GaussianProcessClassifier()\nbag = BaggingClassifier(random_state=1)\n\n# \ub9ac\uc2a4\ud2b8 \uc900\ube44\nmodels = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \nmodel_names = ['Random Forest', 'K Nearest Neighbor', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier']\nscores2 = {}\n\n# \ud559\uc2b5 \ubc0f \uad50\ucc28 \uac80\uc99d\nfor ind, mod in enumerate(models):\n    mod.fit(X_train, y_train)\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv = 10)\n    scores2[model_names[ind]] = acc","71cc14bf":"# \uacb0\uacfc \ud14c\uc774\ube14 \uc0dd\uc131\nresults = pd.DataFrame(scores2).T\nresults['mean'] = results.mean(1)\n\nresult_df = results.sort_values(by='mean', ascending=False)\nresult_df.head(11)","88dbdc60":"result_df = result_df.drop(['mean'], axis=1)\nsns.boxplot(data=result_df.T, orient='h')\nplt.title('Machine Learning Algorithm Acuuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')","e90c7461":"# \ud30c\ub77c\ubbf8\ud130 \uc11c\uce58\nCs = [0.01, 0.1, 1, 5, 10, 15, 20, 50]\ngammas = [0.001, 0.01, 0.1]\n\n# \ud30c\ub77c\ubbf8\ud130 \uadf8\ub9ac\ub4dc \uc14b\ud305\nhyperparams = {'C': Cs, 'gamma': gammas}\n\n# \uad50\ucc28\uac80\uc99d\ngd = GridSearchCV(estimator=SVC(probability=True), param_grid=hyperparams, verbose=True, cv=5, scoring=\"accuracy\", n_jobs=-1)\n\n# \ubaa8\ub378 fitting\ngd.fit(X_train, y_train)\n\n# \uacb0\uacfc\nprint(gd.best_score_)\nprint(gd.best_params_)","bff11049":"learning_rate = [0.01, 0.05, 0.1, 0.2, 0.5]\nn_estimators = [100, 1000, 2000]\nmax_depth = [3, 5, 10, 15]\n\nhyperparams = {'learning_rate': learning_rate, 'n_estimators': n_estimators}\n\ngd = GridSearchCV(estimator=GradientBoostingClassifier(), param_grid = hyperparams, verbose=True, cv=5, scoring=\"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","6e189fe3":"penalty = ['l1', 'l2']\nC = np.logspace(0, 4, 10)\n\nhyperparams = {'penalty': penalty, 'C': C}\n\ngd = GridSearchCV(estimator=LogisticRegression(), param_grid=hyperparams, verbose=True, cv=5, scoring=\"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","56838a15":"learning_rate = [0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\nn_estimators = [10, 50, 100, 250, 500, 1000]\n\nhyperparams = {'learning_rate': learning_rate, 'n_estimators': n_estimators}\n\ngd = GridSearchCV(estimator = XGBClassifier(), param_grid=hyperparams, verbose=True, cv=5, scoring=\"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","f9ad0ba4":"max_depth = [3, 4, 5, 6, 7, 8, 9, 10]\nmin_child_weight = [1, 2, 3, 4, 5, 6]\n\nhyperparams = {'max_depth': max_depth, 'min_child_weight': min_child_weight}\n\ngd = GridSearchCV(estimator=XGBClassifier(learning_rate=0.2, n_estimators=50),\n                  param_grid=hyperparams, verbose=True, cv=5, scoring=\"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","97b62539":"gamma = [i*0.1 for i in range(0,5)]\n\nhyperparams = {'gamma': gamma}\n\ngd = GridSearchCV(estimator= XGBClassifier(learning_rate=0.2, n_estimators=50, max_depth=8, min_child_weight=2), \n                  param_grid=hyperparams, verbose=True, cv=5, scoring=\"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","f8ed841b":"subsample = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\ncolsample_bytree = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n\nhyperparams = {'subsample': subsample, 'colsample_bytree': colsample_bytree}\n\ngd = GridSearchCV(estimator=XGBClassifier(learning_rate=0.2, n_estimators=50, max_depth=8, min_child_weight=2, gamma=0), \n                  param_grid=hyperparams, verbose=True, cv=5, scoring=\"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\n\nprint(gd.best_score_)\nprint(gd.best_params_)","931ef8a0":"reg_alpha = [1e-5, 1e-2, 0.1, 1, 100]\n\nhyperparams = {'reg_alpha': reg_alpha}\n\ngd = GridSearchCV(estimator=XGBClassifier(learning_rate=0.2, n_estimator=50, max_depth=8, min_child_weight=2, gamma=0, subsample=0.7, colsample_bytree=0.65),\n                 param_grid=hyperparams, verbose=True, cv=5, scoring=\"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","f7ab65d5":"n_restarts_optimizer = [0, 1, 2, 3]\nmax_iter_predict = [1, 2, 5, 10, 20, 35, 50, 100]\nwarm_start = [True, False]\n\nhyperparams = {'n_restarts_optimizer': n_restarts_optimizer, 'max_iter_predict': max_iter_predict, 'warm_start': warm_start}\n\ngd = GridSearchCV(estimator=GaussianProcessClassifier(), param_grid=hyperparams, verbose=True, cv=5, scoring=\"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","c95a5be1":"n_estimators = [10, 100, 200, 500]\nlearning_rate = [0.001, 0.01, 0.1, 0.5, 1, 1.5, 2]\n\nhyperparams = {'n_estimators': n_estimators, 'learning_rate': learning_rate}\n\ngd = GridSearchCV(estimator=AdaBoostClassifier(), param_grid=hyperparams, verbose=True, cv=5, scoring=\"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","d3ffe5b4":"n_neighbors = [1, 2, 3, 4, 5]\nalgorithm = ['auto']\nweights = ['uniform', 'distance']\nleaf_size = [1, 2, 3, 4, 5, 10]\n\nhyperparams = {'algorithm':algorithm, 'weights': weights, 'leaf_size': leaf_size, 'n_neighbors': n_neighbors}\n\ngd=GridSearchCV(estimator=KNeighborsClassifier(), param_grid=hyperparams, verbose=True, cv=5, scoring=\"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","4b460425":"n_estimators = [10, 50, 100, 200]\nmax_depth = [3, None]\nmax_features = [0.1, 0.2, 0.5, 0.8]\nmin_samples_split = [2, 6]\nmin_samples_leaf = [2, 6]\n\nhyperparams = {'n_estimators': n_estimators, 'max_depth': max_depth, 'max_features': max_features,\n              'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n\ngd = GridSearchCV(estimator=RandomForestClassifier(), param_grid=hyperparams, verbose=True, cv=5, scoring=\"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","60aa66d2":"n_estimators = [10, 25, 50, 75, 100]\nmax_depth = [3, None]\nmax_features = [0.1, 0.2, 0.5, 0.8]\nmin_samples_split = [2, 10]\nmin_samples_leaf = [2, 10]\n\nhyperparams = {'n_estimators': n_estimators, 'max_depth': max_depth, 'max_features': max_features, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n\ngd = GridSearchCV(estimator=ExtraTreesClassifier(), param_grid=hyperparams, verbose=True, cv=5, scoring=\"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","1da4f59b":"n_estimators = [10, 50, 75, 100, 200]\nmax_samples = [0.1, 0.2, 0.5, 0.8, 1.0]\nmax_features = [0.1, 0.2, 0.5, 0.8, 1.0]\n\nhyperparams = {'n_estimators': n_estimators, 'max_samples': max_samples, 'max_features': max_features}\n\ngd = GridSearchCV(estimator=BaggingClassifier(), param_grid=hyperparams, verbose=True, cv=5, scoring=\"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","963eb836":"# \ud29c\ub2dd \ubaa8\ub378 \uc2dc\uc791\n# sample\uc744 split\ud558\ub294 \uac83\uc740 \uc804\uccb4 \ub370\uc774\ud130\uc758 80%\ub97c train \uc14b\uc5d0 20%\ub294 test \uc14b\uc5d0 \uc90c\n\nran = RandomForestClassifier(max_depth=None, max_features=0.1, min_samples_leaf=2, min_samples_split=6, n_estimators=100, random_state=1)\n\nknn = KNeighborsClassifier(leaf_size=1, n_neighbors=5, weights='distance')\n\nlog = LogisticRegression(C=1.0, penalty='l2')\n\nxgb = XGBClassifier(learning_rate=0.2, n_estimators=50, max_depth=8, min_child_weight=2, gamma=0, subsample=0.7, colsample_bytree=0.65, reg_alpha=1)\n\ngbc = GradientBoostingClassifier(learning_rate=0.2, max_depth=3, n_estimators=1000)\n\nsvc = SVC(probability=True, gamma=0.001, C=20)\n\next = ExtraTreesClassifier(max_depth=None, max_features=0.1, min_samples_leaf=2, min_samples_split=2, n_estimators=50, random_state=1)\n\nada = AdaBoostClassifier(learning_rate=0.5, n_estimators=200, random_state=1)\n\ngpc = GaussianProcessClassifier(max_iter_predict=1, n_restarts_optimizer=0, warm_start=True)\n\nbag = BaggingClassifier(max_features=0.5, max_samples=1.0, n_estimators=75, random_state=1)\n\n# \ub9ac\uc2a4\ud2b8\nmodels = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \nmodel_names = ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier']\nscores3 = {}\n\n# Sequentially fit and cross validate all models\nfor ind, mod in enumerate(models):\n    mod.fit(X_train, y_train)\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv = 10)\n    scores3[model_names[ind]] = acc","68815ffb":"results = pd.DataFrame(scores).T\nresults['mean'] = results.mean(1)\nresult_df = results.sort_values(by='mean', ascending=False)\nresult_df.head(11)\n\nresult_df = result_df.drop(['mean'], axis=1)\nsns.boxplot(data=result_df.T, orient='h')\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')","212c6971":"# \ud29c\ub2dd\ud55c \ud30c\ub77c\ubbf8\ud130\ub85c \ud558\ub4dc\ubcf4\ud305\ngrid_hard = VotingClassifier(estimators = [('Random Forest', ran),\n                                           ('Logistic Regression', log),\n                                           ('XGBoost', xgb),\n                                           ('Gradient Boosting', gbc),\n                                           ('Extra Trees', ext),\n                                           ('AdaBoost', ada),\n                                           ('Gaussian Process', gpc),\n                                           ('SVC', svc),\n                                           ('K Nearest Neighbor', knn),\n                                           ('Bagging Classifier', bag)], voting='hard')\n\ngrid_hard_cv = model_selection.cross_validate(grid_hard, X_train, y_train, cv=10)\ngrid_hard.fit(X_train, y_train)\n\nprint(\"Hard voting on test set score mean: {:.2f}\".format(grid_hard_cv['test_score'].mean()*100))","9ff899d7":"# \ud29c\ub2dd\ud55c \ud30c\ub77c\ubbf8\ud130\ub85c \uc18c\ud504\ud2b8\ubcf4\ud305\ngrid_soft = VotingClassifier(estimators = [('Random Forest', ran),\n                                           ('Logistic Regression', log),\n                                           ('XGBoost', xgb),\n                                           ('Gradient Boosting', gbc),\n                                           ('Extra Trees', ext),\n                                           ('AdaBoost', ada),\n                                           ('Gaussian Process', gpc),\n                                           ('SVC', svc),\n                                           ('K Nearest Neighbor', knn),\n                                           ('Bagging Classifier', bag)], voting='soft')\n\ngrid_soft_cv = model_selection.cross_validate(grid_soft, X_train, y_train, cv=10)\ngrid_soft.fit(X_train, y_train)\n\nprint(\"Soft voting on test set score mean: {:.2f}\".format(grid_soft_cv['test_score'].mean()*100))","3d3c22b8":"# \ub9c8\uc9c0\ub9c9 \uc608\uce21 hard\npredictions = grid_hard.predict(X_test)\n\nsubmission = pd.concat([pd.DataFrame(passId), pd.DataFrame(predictions)], axis='columns')\n\nsubmission.columns = [\"PassengerId\", \"Survived\"]\nsubmission.to_csv('titanic_submission1.csv', header=True, index=False)","f5083251":"# \ub9c8\uc9c0\ub9c9 \uc608\uce21 soft\npredictions = grid_soft.predict(X_test)\n\nsubmission = pd.concat([pd.DataFrame(passId), pd.DataFrame(predictions)], axis='columns')\n\nsubmission.columns = [\"PassengerId\", \"Survived\"]\nsubmission.to_csv('titanic_submission2.csv', header=True, index=False)","c612850d":"\ud0d1\uc2b9\uac1d\uc740 \ub0a8\uc790\uac00 \ud6e8\uc52c \ub354 \ub9ce\uc9c0\ub9cc, \uc0dd\uc874 \uc5ec\uc131 \uc218\ub294 \ub0a8\uc131 \uc218\uc758 \uac70\uc758 \ub450 \ubc30","ade40708":"- \"Ticket\" \ubd84\uc11d","fe654fea":"- \"Survived\" \ubd84\uc11d\n(0 - Not Survived, 1 - Survived)","d8135360":"XGB Step 5.","05d00618":"## 5. Feature Engineering","f32d4b8c":"- \"Embarked\" \ubd84\uc11d","3480c5a8":"Gaussian Process","67efab48":"Random Forest","576cf895":"\uadf8 \uacb0\uacfc\ub85c csv \ud30c\uc77c \uc0dd\uc131","6dfd9455":"- \"Age\" \ubd84\uc11d (\ube48 \uac12\uc774 \ub9ce\uc544\uc11c \uacb0\uce21\uac12 \ucc98\ub9ac\uac00 \uc911\uc694)","4d9b5375":"## 10. \ubaa8\ub378 \uc7ac \ud2b8\ub808\uc774\ub2dd","ce53c035":"Bagging Classifier","2a9545fe":"- \"Name\" \ubd84\uc11d","61910785":"train \ub370\uc774\ud130 \uc778\uc6d0 \uc911 342\uba85\uc758 \uc0dd\uc874\uc790, 549\uba85\uc758 \uc0ac\ub9dd\uc790","409d567b":"SVC","bde719f4":"Gradient Boosting Classifier","9577a709":"KNN","1050b83f":"177\uac1c\uc758 \uacb0\uce21\uac12 \ucc98\ub9ac\uac00 \ud544\uc694. but, \ud3c9\uade0\uc5f0\ub839 29\uc138\ub97c 4\uc138 \uc544\uc774\uc5d0\uac8c \ud560\ub2f9\ud560 \uc218 \uc5c6\uae30\uc5d0 Initial\ubcc4 \ud3c9\uade0 \uc5f0\ub839\uc744 \ubcf4\uace0, Age\uc5d0 \uc801\uc6a9","3f84a17b":"- \"Cabin\" \ubd84\uc11d","b8d57d04":"### \uad00\ucc30\n1. Pclass\uc5d0 \ub530\ub77c \uc5b4\ub9b0\uc774 \uc218 \uc99d\uac00, \uc5b4\ub9b0\uc774\uc758 \uc0dd\uc874\ub960\uc740 Pclass\uc5d0 \uc0c1\uad00\uc5c6\uc774 \uc591\ud638\n2. 1\ub4f1\uae09 20~50\uc138 \uc2b9\uac1d\uc758 \uc0dd\uc874 \uac00\ub2a5\uc131\uc774 \ub192\uc73c\uba70, \uc5ec\uc131\uc774 \ub354 \uc88b\uc74c\n3. \ub0a8\uc131 \uc0dd\uc874 \ud655\ub960\uc740 \ub098\uc774 \uc99d\uac00\uc5d0 \ub530\ub77c \uac10\uc18c\ud568","bac25ed7":"XGB Step 4.","a6171931":"Extra Trees","0221738a":"XGB Step 2.","2884d7b6":"XGBoost Step 1.","4c772dbd":"### CSV\ub97c Dataframe\uc73c\ub85c \ubcc0\ud658","5e7eed7d":"## 1. \ub370\uc774\ud130 \uc900\ube44 \ubc0f \ubaa8\ub4c8 \uc784\ud3ec\ud2b8","f5632ba2":"XGB Step 3.","d60bae0a":"## 7. \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378 \ub9cc\ub4e4\uae30","4990a062":"\uacb0\uce21\uac12\uc744 \uc9c1\uad00\uc801\uc73c\ub85c \ud655\uc778 \uac00\ub2a5","1621f890":"## 6. \ub9c8\uc9c0\ub9c9 \ud56d\ubaa9 \uacb0\uc815","efc81501":"hard voting\uacfc soft voting \ubaa8\ub450\ub97c \ucd5c\uc885 \uc81c\ucd9c\uc5d0 \ub300\ud55c \ucd5c\uc885 \uc608\uce21\uc744 \ud558\ub294\ub370 \uc0ac\uc6a9","ac87eb41":"1\ub4f1\uae09 \uac1d\uc2e4\uc740 \uc0dd\uc874\uc790\uac00 \ub354 \ub9ce\uace0, 2\ub4f1\uae09\uc740 \uc0dd\uc874\uc790\uc5d0 \uc0ac\ub9dd\uc790\uac00 \uc870\uae08 \ub354 \ub9ce\uc73c\ub098, 3\ub4f1\uae09\uc740 \uc0ac\ub9dd\uc790\uac00 3\ubc30 \uc774\uc0c1 \ub9ce\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc74c","4829bf09":"## 12. \ub9c8\uc9c0\ub9c9 \ubaa8\ub378 \uc608\uce21","2bb8ca82":"## 8. \uc911\uc694\ub3c4\uc5d0 \ub530\ub77c \ubaa8\ub378 \uc7ac\uc124\uc815","f115f4cb":"## 2. \ud30c\uc77c \ubcd1\ud569","df416252":"- \"Fare\" \ubd84\uc11d","6ca67de6":"## 3. \ud30c\uc77c \ud0d0\uc0c9","f10cb2e0":"Adaptive Boost","ea2f42ed":"- \"Sex\" \ubd84\uc11d","b318a1ac":"## 11. \ub9c8\uc9c0\ub9c9 \ubcf4\ud305","ba38d71e":"Age, Cabin, Embarked\uc5d0 \uacb0\uce21\uac12\uc774 \uc788\uc74c\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc74c","d1039117":"- \"Pclass\" \ubd84\uc11d\n(1=1st, 2=2nd, 3=3rd)","2eaa1e42":"- \"SibSp\"(\ud63c\uc790\uc778\uc9c0 \uac00\uc871\uacfc \ud568\uaed8 \uc788\ub294\uc9c0) + \"Parch\"(\ubd80\ubaa8\uc640 \ud568\uaed8 \ud0d4\ub294\uc9c0) \ubd84\uc11d ('Alone'\uadf8\ub8f9\uacfc 'Family'\uadf8\ub8f9\uc73c\ub85c \ub098\ub220\ubcf4\ub824 \ud568)","558f3440":"1~3\uba85\uc758 \ubd80\ubaa8\uc640 \ud568\uaed8 \ud0d1\uc2b9\ud55c \uacbd\uc6b0\uc758 \uc0dd\uc874 \uac00\ub2a5\uc131\uc774 \uc81c\uc77c \ub192\uc74c","0595b978":"## 4. \ub370\uc774\ud130 \ud0d0\uad6c","336f9fee":"Priority - Nobles, Women in Pclass 1&2, Babies under 1, Kids under 17 in Pclass 1&2","bcd342dd":"## 9. \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd","68042525":"\uc0dd\uc874\uc790\ubcf4\ub2e4 \uc0ac\ub9dd\uc790\uac00 \ub354 \ub9ce\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc74c","44989ce1":"Logistic Regression"}}