{"cell_type":{"6aedbfae":"code","550496fe":"code","0a63af1d":"code","3e9ca8c4":"code","5f5c3e57":"code","5cff086b":"code","28cf5735":"code","58d320ec":"code","59398846":"code","49400ab6":"code","882f9cc7":"code","92ccda71":"code","14380295":"code","4233c096":"code","aeed8e1c":"code","ac9c3eda":"code","5180cfff":"code","c404eb4f":"code","18c7d9fe":"code","35035ebc":"code","7cc9cd6b":"code","53d99878":"code","1a29eb34":"code","40f7c034":"code","658c93cb":"code","ff883980":"code","6d255291":"code","67786a58":"code","4503667f":"code","2ac08a87":"code","be7b466e":"code","6a8fdda3":"code","2f9d46d9":"code","85a32c2b":"code","a7140a14":"code","d21ceb11":"code","7c24b6b5":"code","0f60048b":"code","dcc586c5":"code","b9250a54":"code","259e1e40":"code","3c7a2693":"code","28094b98":"code","15f0e159":"code","ac88af7d":"code","b98f4d6b":"code","03ae0d80":"code","86a7b113":"code","68658ff2":"code","3fbede74":"code","4ec95f8d":"code","3b387ec3":"code","54e62042":"code","98a51083":"code","4e6fe58b":"code","64ad7db2":"code","c51164d1":"code","69b88759":"code","5f30437d":"code","12a4a365":"code","45fcacd6":"code","df8e1373":"code","0ce9de73":"code","66c9637f":"code","e03dcfa6":"code","776dc5ae":"code","679b22e0":"code","01282201":"code","2d9293af":"code","3f157457":"code","bf59caed":"code","a801a097":"code","dbc11c01":"markdown","26e1c463":"markdown","03be1aed":"markdown","cc8865ae":"markdown","ad8e0dfe":"markdown","b06d3d5d":"markdown","e07e82e2":"markdown","523db7b4":"markdown","3a5e5bf6":"markdown","5f125797":"markdown","7c22d6de":"markdown","f176bd6c":"markdown","3e7225d4":"markdown","9ca12cf9":"markdown","3261d7b8":"markdown","75fe3538":"markdown","dfbe6d19":"markdown","e343e24b":"markdown","1eed14a7":"markdown","71e1de02":"markdown","5a9c2952":"markdown","15cb9fa1":"markdown","0947c9e8":"markdown","76135a76":"markdown","d0d41050":"markdown","3c44ad06":"markdown","84d1be42":"markdown","a14db505":"markdown","2a6e2c13":"markdown","232d8b9d":"markdown","f48aed1c":"markdown","8ab13b8f":"markdown","24f78fd1":"markdown","c80a3782":"markdown","074d6f6e":"markdown","8beb1a95":"markdown","981d8276":"markdown","f133dc67":"markdown","77c7df87":"markdown","8188d462":"markdown","9320ff1a":"markdown","8d8050c8":"markdown","d248484a":"markdown","5f81f2ff":"markdown","72a61333":"markdown","2cb81222":"markdown","48297be7":"markdown","6bbc6b74":"markdown","78dfd3ad":"markdown","18bf1a4b":"markdown","c8a99a47":"markdown","a35f0144":"markdown","2e671d31":"markdown","365e77b2":"markdown","d86871eb":"markdown","3f3e949b":"markdown","e3d4d7f4":"markdown","6c6d0a30":"markdown","526705e0":"markdown","08ec8f2b":"markdown","d914025c":"markdown","c2f777b5":"markdown","3094b27d":"markdown","ecbf8ece":"markdown","2bdb7305":"markdown","cfdfa439":"markdown","48f799cd":"markdown","5ad359a2":"markdown","9cc5acb0":"markdown","a2bc48a6":"markdown","4290a889":"markdown","b6500c24":"markdown","781fbc26":"markdown","3d7c45de":"markdown","f0adff2c":"markdown","5bec3713":"markdown","ac369e3e":"markdown","039c0c87":"markdown","68a113c3":"markdown","3d5b73ea":"markdown","f36ab090":"markdown","6d9bf817":"markdown","71f2795a":"markdown","af6c1b68":"markdown","03b53978":"markdown","5458e0e1":"markdown","09b1d2e6":"markdown","9e1fa47b":"markdown"},"source":{"6aedbfae":"%%capture\n!pip install --upgrade -q comet_ml contractions emoji unidecode langdetect","550496fe":"# # import comet_ml in the top of your file\n# from comet_ml import Experiment","0a63af1d":"%%capture\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport emoji\nimport plotly.graph_objects as go\nimport nltk\nimport os\nimport string\nimport contractions\nimport xgboost\nimport time\nimport gc\nimport warnings\n\nfrom langdetect import detect\nfrom wordcloud import WordCloud\nfrom urllib import request\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom nltk import bigrams\nfrom nltk import bigrams\nfrom wordcloud import STOPWORDS\nfrom nltk import word_tokenize, sent_tokenize, FreqDist\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import ComplementNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm, tree\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.linear_model import RidgeClassifier\n\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\n#warnings.filterwarnings('ignore')","3e9ca8c4":"#Import of Data for use in a Notebook\n# train_df = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/Maddy-Muir\/Classification_Predict\/master\/train.csv\")\n# test_df = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/Maddy-Muir\/Classification_Predict\/master\/test.csv\")\n\n#Import of Data for use in Kaggle\ntrain_df =pd.read_csv('..\/input\/climate-change-belief-analysis\/train.csv')\ntest_df =pd.read_csv('..\/input\/climate-change-belief-analysis\/test.csv')","5f5c3e57":"train_df.info()","5cff086b":"np.where(train_df.applymap(lambda x: x == ''))","28cf5735":"train_df.head()","58d320ec":"# Determining number of rows for each sentiment\nrows = train_df['sentiment'].value_counts()\nrows_df = pd.DataFrame({'Sentiment':rows.index, 'Rows':rows.values})\n\n# Determining percentage distribution for each sentiment\npercentage = round(train_df['sentiment'].value_counts(normalize=True)*100,2)\npercentage_df = pd.DataFrame({'Sentiment':percentage.index,\n                              'Percentage':percentage.values})\n\n# Joining row and percentage information\nsentiment_df = pd.merge(rows_df, percentage_df, on='Sentiment', how='outer')\nsentiment_df.set_index('Sentiment', inplace=True)\nsentiment_df.sort_index(axis = 0)","59398846":"sns.countplot(x = 'sentiment', data = train_df, palette=\"hls\")\nplt.title(\"Sentiment Distribution\");","49400ab6":"# Separate joined words based on capitals\ndef camel_case_split(identifier):\n    matches = re.finditer(\n        r'.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)',\n        identifier\n    )\n    return \" \".join([m.group(0) for m in matches])\n\n# Extract Mentions\ndef extract_mentions(tweet):\n  \"\"\"Helper function to extract mentions\"\"\"\n  mentions = re.findall(r'@([a-zA-Z0-9_]{1}[a-zA-Z0-9_]{0,14})', tweet)\n  return mentions\n\n# Extract Hashtags\ndef extract_hash_tags(tweet):\n  \"\"\"Helper function to extract hashtags\"\"\"\n  hash_tags = re.findall(r'(?:^|\\s)#(\\w+)', tweet)\n  return [camel_case_split(tag) for tag in hash_tags]\n\n# Identifying Retweets\ndef is_retweet(tweet):\n  \"\"\"Helper Function to determine if a tweet is a re-tweet\"\"\"\n  match = re.search(r'^\\s?RT\\s@[a-zA-Z0-9_]{1}[a-zA-Z0-9_]{0,14}[\\s:]', tweet)\n  if match:\n    return 1\n  return 0\n\n# Extract Emojis\ndef extract_emojis(tweet):\n  \"\"\"Helper to extract all emoji's from a tweet\"\"\"\n  emojis = ''.join(c for c in tweet if c in emoji.UNICODE_EMOJI)\n  if emojis:\n    return emojis\n  return None\n\n# Language Detection of tweet\ndef detect_language(tweet):\n  \"\"\"Helpler function to detect language\"\"\"\n  return detect(tweet)","882f9cc7":"# Extracting Mentions\ntrain_df['mentions'] = train_df['message'].apply(extract_mentions)\n\n# Extracting Hashtags\ntrain_df['hashtags'] = train_df['message'].apply(extract_hash_tags)\n\n# Identifying Retweets\ntrain_df['is_retweet'] = train_df['message'].apply(is_retweet)\n\n# Language Detection\ntrain_df['language'] = train_df['message'].apply(detect_language)\n\n# Extracting Emojis\ntrain_df['emojis'] = train_df['message'].apply(extract_emojis)\ntrain_df.emojis.fillna(value=np.nan, inplace=True)\n\n# Finding Number of Words per Tweet\ntrain_df[\"num_words\"] = train_df[\"message\"].apply(lambda x: len(str(x).split()))\n\n# Finding Number of Characters per Tweet\ntrain_df[\"num_chars\"] = train_df[\"message\"].apply(lambda x: len(str(x)))","92ccda71":"train_df.head()","14380295":"# Create a list of all the mentions\nmentions_list = [item for sublist in train_df['mentions'] for item in sublist]\n\n# Grouping mentions by sentiment\n# News Mentions\nnews_mentions = train_df[train_df['sentiment'] == 2]['mentions']\nnews_mentions = [x for x in news_mentions if x != []]\nnews_mentions = [item for sublist in news_mentions for item in sublist]\n\n# Positive Mentions\npos_mentions = train_df[train_df['sentiment'] == 1]['mentions']\npos_mentions = [x for x in pos_mentions if x != []]\npos_mentions = [item for sublist in pos_mentions for item in sublist]\n\n# Neutral Mentions\nneutral_mentions =train_df[train_df['sentiment'] == 0]['mentions']\nneutral_mentions = [x for x in neutral_mentions if x != []]\nneutral_mentions = [item for sublist in neutral_mentions for item in sublist]\n\n# Negative Mentions\nneg_mentions = train_df[train_df['sentiment'] ==-1]['mentions']\nneg_mentions = [x for x in neg_mentions if x != []]\nneg_mentions = [item for sublist in neg_mentions for item in sublist]","4233c096":"# Get count of mentions and unique mentions\nprint(\"Total number of mentions: \\t\\t\\t\"+ str(len(mentions_list)))\nprint(\"Total number of unique mentions: \\t\\t\"+ str(len(set(mentions_list))))\n\n# Get count of mentions and unique mentions per sentiment\nprint(\"Total number of News mentions: \\t\\t\\t\"+ str(len(news_mentions)))\nprint(\"Total number of unique News mentions: \\t\\t\"+ str(len(set(news_mentions))))\n\nprint(\"Total number of Positve mentions: \\t\\t\"+ str(len(pos_mentions)))\nprint(\"Total number of unique Positive mentions: \\t\"+ str(len(set(pos_mentions))))\n\nprint(\"Total number of Neutral mentions: \\t\\t\"+ str(len(neutral_mentions)))\nprint(\"Total number of unique Neutral mentions: \\t\"+ str(len(set(neutral_mentions))))\n\nprint(\"Total number of Negative mentions: \\t\\t\"+ str(len(neg_mentions)))\nprint(\"Total number of unique Negative mentions: \\t\"+ str(len(set(neg_mentions))))\n\n# Count of common mentions\ncommon_mentions = set(pos_mentions) & set(news_mentions) & set(neg_mentions) & set(neutral_mentions)\nprint(\"Total number of Common mentions: \\t\\t\"+ str(len(common_mentions)))","aeed8e1c":"mentions =['All', 'Postive', 'Neutral', 'Negative', 'News']\n\nfig = go.Figure(data=[\n    go.Bar(name='Total Mentions', x=mentions, y=[14799, 8497, 2198, 1386, 2718],marker_color='lightblue'),\n    go.Bar(name='Unique Mentions', x=mentions, y=[7640, 4495, 1880, 919, 1302], marker_color ='purple')\n])\n# Change the bar mode\nfig.update_layout(barmode='group', title = \"Distribution of Mentions\")\n\nfig.show()","ac9c3eda":"# Extract rows based on sentiment\nHT_neg = train_df[train_df['sentiment'] == -1]['hashtags']\nHT_neutral = train_df[train_df['sentiment'] == 0]['hashtags']\nHT_pos = train_df[train_df['sentiment'] == 1]['hashtags']\nHT_news = train_df[train_df['sentiment'] == 2]['hashtags']\n\n# # List of sentiment hashtags\nHT_neg = sum(HT_neg,[])\nHT_neutral = sum(HT_neutral,[])\nHT_pos = sum(HT_pos, [])\nHT_news = sum(HT_news,[])","5180cfff":"# Graph for Negative Sentiment\na = nltk.FreqDist(HT_neg)\nd = pd.DataFrame({'Negative': list(a.keys()),\n                  'Count': list(a.values())})\n    \nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Negative\", y = \"Count\",palette=\"hls\")\nax.set(ylabel = 'Count')\nplt.title(\"Top 10 Negative Hashtags\")\nplt.show()\n\n# Graph for Neutral Sentiment\na = nltk.FreqDist(HT_neutral)\nd = pd.DataFrame({'Neutral': list(a.keys()),\n                  'Count': list(a.values())})\n\nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Neutral\", y = \"Count\",palette=\"hls\")\nax.set(ylabel = 'Count')\nplt.title(\"Top 10 Neutral Hashtags\")\nplt.show()\n\n# Graph for Positive Sentiment\na = nltk.FreqDist(HT_pos)\nd = pd.DataFrame({'Positive': list(a.keys()),\n                  'Count': list(a.values())})\n    \nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Positive\", y = \"Count\",palette=\"hls\")\nax.set(ylabel = 'Count')\nplt.title(\"Top 10 Positive Hashtags\")\nplt.show()\n\n# Graph for News Sentiment\na = nltk.FreqDist(HT_news)\nd = pd.DataFrame({'News': list(a.keys()),\n                  'Count': list(a.values())})\n     \nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"News\", y = \"Count\",palette=\"hls\")\nax.set(ylabel = 'Count')\nplt.title(\"Top 10 News Hashtags\")\nplt.show()","c404eb4f":"# Determining number of rows which are retweets\nrows_RT = train_df['is_retweet'].value_counts()\nrows_RT_df = pd.DataFrame({'Is_Retweet':rows_RT.index, 'Rows':rows_RT.values})\n\n# Determining percentage of rows which are retweets\npercentage_RT = round(train_df['is_retweet'].value_counts(normalize=True)*100,2)\npercentage_RT_df = pd.DataFrame({'Is_Retweet':percentage_RT.index, \n                                 'Percentage':percentage_RT.values})\n\n# Joining row and percentage information\nRT_df = pd.merge(rows_RT_df, percentage_RT_df, on='Is_Retweet', how='outer')\nRT_df.set_index('Is_Retweet', inplace=True)\nRT_df.sort_index(axis = 0)","18c7d9fe":"# Extracting dataframe with duplicate messages\nduplicates_df = train_df[train_df['message'].duplicated()]\n\n# Checking how many duplicatas are not retweets\nx = len(duplicates_df[duplicates_df['is_retweet'] == 0])\nprint(\"Total number of duplicate tweets which are NOT retweets: \\t\"+ str(x))","35035ebc":"# Creating a dataframe with the count for each language\nlanguages = train_df['language'].value_counts()\nlanguage_df = pd.DataFrame({'ISO Code':languages.index, 'Rows':languages.values})\nlanguage_df.set_index('ISO Code', inplace=True)\nlanguage_df.head()","7cc9cd6b":"# Extracting the rows that contains emojis\nemoji_df = train_df[train_df['emojis'].notnull()]","53d99878":"print(\"Total number of tweets containing emojis: \"+ str(len(emoji_df)))\nprint(\"Total number of emojis in tweets: \\t  \"+ str(len(''.join(emoji_df['emojis'].tolist()))))","1a29eb34":"def emoji_cloud(df):\n  \"\"\" Create an emoji cloud\n  Taken from: https:\/\/github.com\/amueller\/word_cloud\/blob\/d1ec087a7f86e6dc14ed3771a9f8e84a5d384e0a\/examples\/emoji.py\n  \"\"\"\n\n  # Create a string with all the emojis\n  emoji_list = ''.join(df['emojis'].tolist())\n\n  # Get data directory (using getcwd() is needed to support running example in generated IPython notebook)\n  d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n\n  # font_path = os.path.join(d, 'fonts', 'EmojiOneColor', 'EmojiOneColor.ttf')\n  font_path = '..\/input\/emoji-fonts\/EmojiOneColor.otf'\n\n  # Checking if font has been downloaded, if not download font\n  if not os.path.isfile(font_path):\n    if not os.path.exists(os.path.join(d, 'fonts', 'EmojiOneColor')):\n      os.makedirs(os.path.join(d, 'fonts', 'EmojiOneColor'))\n      \n    url = 'https:\/\/github.com\/adobe-fonts\/emojione-color\/blob\/master\/EmojiOneColor.otf?raw=true'\n    request.urlretrieve(url, font_path)\n\n  # Two consecutive punctuations :)\n  ascii_art = r\"(?:[{punctuation}][{punctuation}]+)\".format(punctuation=string.punctuation)\n\n  # A single character that is not alpha_numeric or other ascii printable\n  emoji = r\"(?:[^\\s])(?<![\\w{ascii_printable}])\".format(ascii_printable=string.printable)\n\n  # Regular expression matching ascii_art or emoji character\n  regexp = r\"{ascii_art}|{emoji}\".format(ascii_art=ascii_art, emoji=emoji)\n\n  # Create the Emoji-Cloud\n  emojis = WordCloud(\n    background_color='black',\n    max_words=150,\n    max_font_size=70, \n    scale=20,\n    random_state=42,\n    collocations=False,\n    font_path=font_path,\n    regexp=regexp,\n    normalize_plurals=False\n  ).generate(emoji_list)\n\n  # Plot the Emoji-Cloud\n  plt.figure(figsize=(12,8))\n  plt.tight_layout(pad = 0) \n  plt.imshow(emojis)\n  plt.axis('off')\n  plt.title(\"Emoji Cloud\", fontsize = 20)\n  plt.show()\n  plt.savefig('emoji_cloud.png')\n\nemoji_cloud(emoji_df)","40f7c034":"# Boxplot for Number of words in each class\nf, axes = plt.subplots(2, 1, figsize=(9,12))\nsns.boxplot(x='sentiment', y=\"num_chars\", data=train_df, ax=axes[0], palette=\"hls\")\naxes[0].set_xlabel('sentiment', fontsize=12)\naxes[0].set_title(\"Number of Words in each Class\", fontsize=15)\n\n# Boxplot for Number of characters in each class\nsns.boxplot(x='sentiment', y='num_words', data=train_df, ax=axes[1], palette=\"hls\")\naxes[1].set_xlabel('sentiment', fontsize=12)\naxes[1].set_title(\"Number of Characters in each Class\", fontsize=15);","658c93cb":"# Removing words that has no relevance to the context (https, RT, CO)\ntrain_df['word_cloud'] = train_df['message'].str.replace('http\\S+|www.\\S+', '', case=False)\n\n# Removing common words which appear in all sentiments\nremove_words = ['climate', 'change', 'RT', 'global', 'warming', 'Donald', 'Trump']\n\n# Function to remove common words listed above\ndef remove_common_words(message):\n  pattern = re.compile(r'\\b(' + r'|'.join(remove_words) + r')\\b\\s*')\n  message = pattern.sub('', message)\n  return message\n\ntrain_df['word_cloud'] = train_df['word_cloud'].apply(remove_common_words)\n\n# Extracing rows per sentiment\nnews = train_df[train_df['sentiment'] == 2]['word_cloud']\npos = train_df[train_df['sentiment'] == 1]['word_cloud']\nneutral = train_df[train_df['sentiment'] == 0]['word_cloud']\nneg = train_df[train_df['sentiment'] ==-1]['word_cloud']\n\n# Splitting strings into lists\nnews = [word for line in news for word in line.split()]\npos = [word for line in pos for word in line.split()]\nneutral = [word for line in neutral for word in line.split()]\nneg = [word for line in neg for word in line.split()]\n\nnews = WordCloud(\n    background_color='black',\n    max_words=100,\n    max_font_size=60, \n    scale=20,\n    random_state=42,\n    collocations=False,\n    normalize_plurals=False\n).generate(' '.join(news))\n\npos = WordCloud(\n    background_color='black',\n    max_words=100,\n    max_font_size=60, \n    scale=20,\n    random_state=42,\n    collocations=False,\n    normalize_plurals=False\n).generate(' '.join(pos))\n\nneutral = WordCloud(\n    background_color='black',\n    max_words=100,\n    max_font_size=60, \n    scale=20,\n    random_state=42,\n    collocations=False,\n    normalize_plurals=False\n).generate(' '.join(neutral))\n\nneg = WordCloud(\n    background_color='black',\n    max_words=100,\n    max_font_size=60, \n    scale=20,\n    random_state=42,\n    collocations=False,\n    normalize_plurals=False\n).generate(' '.join(neg))\n\n##Creating individual wordclouds per sentiment\nfig, axs = plt.subplots(2, 2, figsize = (20, 12))\nfig.tight_layout(pad = 0)\n\naxs[0, 0].imshow(news)\naxs[0, 0].set_title('News', fontsize = 20)\naxs[0, 0].axis('off')\n\naxs[0, 1].imshow(pos)\naxs[0, 1].set_title('Positive ', fontsize = 20)\naxs[0, 1].axis('off')\n\naxs[1, 0].imshow(neg)\naxs[1, 0].set_title('Negative ', fontsize = 20)\naxs[1, 0].axis('off')\n\naxs[1, 1].imshow(neutral)\naxs[1, 1].set_title('Neutral  ', fontsize = 20)\naxs[1, 1].axis('off')\n\nplt.savefig('joint_cloud.png')","ff883980":"# Adding select words to stop words for better analysis on important word frequency\nstop = set(stopwords.words('english')) \nstop_words = [\"via\", \"co\", \"I\",'We','The','going'] + list(stop)\n\n# Removing stop words from the tweets\ntrain_df['word'] = train_df['word_cloud'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop_words]))\ntrain_df['word'] = train_df['word'].str.replace(r'[^\\w\\s]+', '')\n\n# Separating the strings to a list of words\nword_list = [word for line in train_df['word'] for word in line.split()]\n\n# Creating a word frequency counter\nsns.set(style=\"darkgrid\")\ncounts = Counter(word_list).most_common(15)\ncounts_df = pd.DataFrame(counts)\ncounts_df\ncounts_df.columns = ['word', 'frequency']\n\n# Creating a word frequency plot\nfig, ax = plt.subplots(figsize = (9, 9))\nax = sns.barplot(y=\"word\", x='frequency', ax = ax, data=counts_df, palette=\"hls\")\nplt.title('WORD FREQUENCY')\nplt.savefig('wordcount_bar.png')","6d255291":"# Make a set of stop words\nfig.suptitle('Bigrams in Tweets')\nstopwords = set(STOPWORDS)\nmore_stopwords = {'https', 'amp','https rt'}\nstopwords = stopwords.union(more_stopwords)\n\n# Plot for each sentiment of bigrams\nplt.figure(figsize=(16,12))\n\nplt.subplot(2,2,1)\nbigram_d = list(\n    bigrams(\n        [w for w in word_tokenize(' '.join(train_df.loc[train_df.sentiment==1, 'word']).lower()) \n        if (w not in stopwords) & (w.isalpha())]\n    )\n)\n\nd_fq = FreqDist(bg for bg in bigram_d)\nbgdf_d = pd.DataFrame.from_dict(d_fq, orient='index', columns=['count'])\nbgdf_d.index = bgdf_d.index.map(lambda x: ' '.join(x))\nbgdf_d = bgdf_d.sort_values('count',ascending=False)\nsns.barplot(bgdf_d.head(10)['count'], bgdf_d.index[:10], color='pink')\nplt.title('Positive Tweets')\n\nplt.subplot(2,2,2)\nbigram_nd = list(bigrams([w for w in word_tokenize(' '.join(train_df.loc[train_df.sentiment==2, 'word']).lower()) if \n              (w not in stopwords) & (w.isalpha())]))\nnd_fq = FreqDist(bg for bg in bigram_nd)\nbgdf_nd = pd.DataFrame.from_dict(nd_fq, orient='index', columns=['count'])\nbgdf_nd.index = bgdf_nd.index.map(lambda x: ' '.join(x))\nbgdf_nd = bgdf_nd.sort_values('count',ascending=False)\nsns.barplot(bgdf_nd.head(10)['count'], bgdf_nd.index[:10], color='b')\nplt.title('News Tweets')\n\nplt.subplot(2,2,3)\nbigram_nd = list(bigrams([w for w in word_tokenize(' '.join(train_df.loc[train_df.sentiment==-1, 'word']).lower()) if \n              (w not in stopwords) & (w.isalpha())]))\nnd_fq = FreqDist(bg for bg in bigram_nd)\nbgdf_nd = pd.DataFrame.from_dict(nd_fq, orient='index', columns=['count'])\nbgdf_nd.index = bgdf_nd.index.map(lambda x: ' '.join(x))\nbgdf_nd = bgdf_nd.sort_values('count',ascending=False)\nsns.barplot(bgdf_nd.head(10)['count'], bgdf_nd.index[:10], color='c')\nplt.title('Negative Tweets')\n\nplt.subplot(2,2,4)\nbigram_nd = list(bigrams([w for w in word_tokenize(' '.join(train_df.loc[train_df.sentiment==0, 'word']).lower()) if \n              (w not in stopwords) & (w.isalpha())]))\nnd_fq = FreqDist(bg for bg in bigram_nd)\nbgdf_nd = pd.DataFrame.from_dict(nd_fq, orient='index', columns=['count'])\nbgdf_nd.index = bgdf_nd.index.map(lambda x: ' '.join(x))\nbgdf_nd = bgdf_nd.sort_values('count',ascending=False)\nsns.barplot(bgdf_nd.head(10)['count'], bgdf_nd.index[:10], color='g')\nplt.title('Neutral Tweets')\nplt.show()","67786a58":"def clean_tweets(message):\n    \"\"\"\n    Cleaning all tweets by removing contractions, url-links, punctuation, digits,\n    stopwords and Lemmatizing all the words.\n\n    Returns\n      A clean tweet as string\n    \"\"\"\n\n    # change all words into lower case\n    message = message.lower()\n\n    #removing contractions\n    message = contractions.fix(message)\n\n    # replace all url-links with url-web\n    url = r'http[s]?:\/\/(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n    web = 'url-web'\n    message = re.sub(url, web, message)\n\n    # removing all punctuation and digits\n    message = re.sub(r'[-]',' ',message)\n    message = re.sub(r'[^\\w\\s]','',message)\n    message = re.sub('[0-9]+', '', message)\n    \n    # removing stopwords\n    nltk_stopword = nltk.corpus.stopwords.words('english')\n    message = ' '.join([item for item in message.split() if item not in nltk_stopword])\n\n    # lemmatizing all words\n    message = message.lower()\n    lemmatizer = WordNetLemmatizer()\n    message = [lemmatizer.lemmatize(token) for token in message.split(\" \")]\n    message = [lemmatizer.lemmatize(token, \"v\") for token in message]\n    message = \" \".join(message)\n\n    return message","4503667f":"train_df['message_clean']=train_df['message'].apply(clean_tweets)\ntest_df['message_clean']=test_df['message'].apply(clean_tweets)","2ac08a87":"duplicates_df2 = train_df[train_df['message_clean'].duplicated()]\nduplicates_df2['sentiment'].value_counts()","be7b466e":"sns.countplot(x = 'sentiment', data = duplicates_df2, palette=\"hls\")\nplt.title('Duplicated Tweets per Sentiment');","6a8fdda3":"# Grouping duplicates dataframe and doing count of unique tweets\nduplicates_df2= (\n    duplicates_df2.groupby(['message_clean', 'sentiment'])\n      .message.agg('count')\n      .to_frame('Unique RT')\n      .sort_values('Unique RT', ascending = False)\n)\nduplicates_df2 = duplicates_df2.reset_index(level=['message_clean','sentiment'])\nduplicates_df2.head()","2f9d46d9":"# Defining Features & Labels\nX=train_df['message_clean']\ny=train_df['sentiment']\n","85a32c2b":"#Having a split of 95%\/5% yielded the best results.\nX_train ,X_test ,y_train ,y_test = train_test_split(X,y,test_size =0.05, random_state =42)","a7140a14":"#List of all models\nclassifiers = [\n               LinearSVC(),\n               svm.SVC(),\n               tree.DecisionTreeClassifier(),\n               RandomForestClassifier(n_estimators=100, \n                               max_depth=2, \n                               random_state=0, \n                               class_weight=\"balanced\"),\n               MLPClassifier(alpha=1e-5, \n                             hidden_layer_sizes=(5, 2), \n                             random_state=42),\n               LogisticRegression(random_state=123, \n                                  multi_class='ovr',\n                                  n_jobs=1, \n                                  C=1e5,\n                                  max_iter = 4000),\n               KNeighborsClassifier(n_neighbors=3),\n               MultinomialNB(),\n               ComplementNB(),\n               SGDClassifier(loss='hinge', \n                             penalty='l2',\n                             alpha=1e-3, \n                             random_state=42, \n                             max_iter=5, \n                             tol=None),\n               GradientBoostingClassifier(),\n               xgboost.XGBClassifier(learning_rate =0.1,\n                                     n_estimators=1000,\n                                     max_depth=5, \n                                     min_child_weight=1,\n                                     gamma=0,\n                                     subsample=0.8,\n                                     colsample_bytree=0.8,\n                                     nthread=4,\n                                     seed=27)\n    ]","d21ceb11":"def model_building(classifiers, X_train, y_train, X_test,y_test):\n    \"\"\"Function to build a variety of classifiers and return a summary of F1-score\n    and processing time as a dataframe\n    \"\"\" \n    model_summary = {}\n    \n    # Pipeline to balance the classses and then to build the model\n    for clf in classifiers:\n      text_clf = Pipeline([\n            ('tfidf', TfidfVectorizer(stop_words='english', \n                             min_df=1, \n                             max_df=0.9, \n                             ngram_range=(1, 2))),\n            ('clf',clf)\n      ])\n\n      # Logging the Execution Time for each model\n      start_time = time.time()\n      text_clf.fit(X_train, y_train)\n      predictions = text_clf.predict(X_test)\n      run_time = time.time()-start_time\n      \n      # Output for each model: F1_Macro, F1_Accuracy, F1_Weighted & Execution TIme\n      model_summary[clf.__class__.__name__] = {\n          'F1-Macro':metrics.f1_score(y_test,predictions,average='macro'),\n          'F1-Accuracy':metrics.f1_score(y_test,predictions,average='micro'),\n          'F1-Weighted':metrics.f1_score(y_test,predictions,average='weighted'),\n          'Execution Time': run_time\n      }\n        \n    return pd.DataFrame.from_dict(model_summary, orient='index')","7c24b6b5":"classifiers_df = model_building(classifiers,X_train, y_train, X_test, y_test)\nordered_df = classifiers_df.sort_values('F1-Macro',ascending=False)\nordered_df","0f60048b":"ordered_df = ordered_df.rename_axis(index='Model')\nordered_df = ordered_df.reset_index(level='Model')","dcc586c5":"ax = plt.gca()\n\nordered_df.plot(kind='line',x='Model',y='F1-Macro',ax=ax)\nordered_df.plot(kind='line',x='Model',y='F1-Accuracy', color='red', ax=ax)\nplt.xticks(rotation=90)\nplt.show()","b9250a54":"ax = plt.gca()\nordered_df.plot(kind='bar',x='Model',y='Execution Time',ax=ax)\nplt.xticks(rotation=90)\nplt.show()","259e1e40":"def cross_val_models(X,y):\n    \"\"\"Function to build a variety of classifiers and return a summary of F1-score\n    and processing time as a dataframe\n    \"\"\" \n    model_summary = []\n    \n    for clf in classifiers:\n      if clf.__class__.__name__ == 'XGBClassifier':\n        continue\n      text_clf = Pipeline([\n        ('tfidf', TfidfVectorizer(\n          stop_words='english', \n          min_df=1, \n          max_df=0.9, \n          ngram_range=(1, 2))\n        ),\n        ('clf',clf)\n      ])\n\n      start_time = time.time()\n      scores = cross_val_score(text_clf, X=X, y=y, cv=10)\n      run_time = time.time()-start_time\n      model_summary.append([clf.__class__.__name__, scores.mean(), scores.std(), run_time ])\n    \n    cv = pd.DataFrame(model_summary, columns=['Model', 'CV_Mean', 'CV_Std_Dev', 'Execution Time'])\n    cv.set_index('Model', inplace=True)\n      \n    return cv","3c7a2693":"cross_val_df = cross_val_models(X,y)\ncross_val_df = cross_val_df.sort_values('CV_Mean',ascending=False)\ncross_val_df","28094b98":"cross_val = cross_val_df.reset_index()","15f0e159":"ax = plt.gca()\ncross_val.plot(kind='line',x='Model',y='CV_Mean',ax=ax)\nplt.xticks(rotation=90)\nplt.show()","ac88af7d":"ax = plt.gca()\ncross_val.plot(kind='bar',x='Model',y='Execution Time', color='blue', ax=ax)\nplt.xticks(rotation=90)\nplt.show()","b98f4d6b":"tfidf =TfidfVectorizer(stop_words='english',min_df=1, \n                             max_df=0.9, \n                             ngram_range=(1, 2))\n\nX_Tfidf =tfidf.fit_transform(train_df['message_clean'])","03ae0d80":"# Redefining X and y variables after Vectorization\nX=X_Tfidf\ny=train_df['sentiment']\nX_train ,X_test ,y_train ,y_test = train_test_split(X,y,test_size =0.05, random_state =42)","86a7b113":"# API key to run experiment in Comet\n# experiment = Experiment(api_key=\"h9aq14TfOuTPJxNhr12fk20kk\",\n#                         project_name=\"tweet-classification\", \n#                         workspace=\"maddy-muir\")\n\nparam_grid = {'C': [0.1, 1, 10],  \n              'gamma': [1, 0.1, 0.01], \n              'kernel': ['linear']}\n\nSVC_grid = GridSearchCV(SVC(), \n                        param_grid, \n                        refit = True, \n                        verbose = 3, \n                        scoring = 'f1_macro')\n  \n# fitting the model for grid search \nSVC_grid.fit(X_train, y_train) \ny_pred = SVC_grid.predict(X_test)\n\nf1_macro = metrics.f1_score(y_test, y_pred, average='macro')\naccuracy = metrics.f1_score(y_test, y_pred, average='micro')\n\n# experiment.log_dataset_hash(X_train)\n# experiment.log_parameters({\"model_type\": \"Linear SVC\", \"param_grid\": param_grid})\n# experiment.log_metrics({'F1 Macro': f1_macro, \"Accuracy\": accuracy})\n\n# experiment.end()","68658ff2":"print(SVC_grid.best_score_)\nprint(SVC_grid.best_params_) \nprint(SVC_grid.best_estimator_) ","3fbede74":"print(metrics.classification_report(y_test, y_pred))\nprint(metrics.confusion_matrix(y_test, y_pred))","4ec95f8d":"#  API key to run experiment in Comet\n# experiment = Experiment(api_key=\"h9aq14TfOuTPJxNhr12fk20kk\",\n#                         project_name=\"tweet-classification\", \n#                         workspace=\"maddy-muir\")\n\n# define models and parameters\nmodel = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n\n# define grid search\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nLR_model = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1_macro',error_score=0)\nLR_model.fit(X_train, y_train)\n\ny_pred = LR_model.predict(X_test)\nf1_macro = metrics.f1_score(y_test, y_pred, average='macro')\naccuracy = metrics.f1_score(y_test, y_pred, average='micro')\n\n# experiment.log_dataset_hash(X_train)\n# experiment.log_parameters({\"model_type\": \"Logistic Regression\", \"param_grid\": grid})\n# experiment.log_metrics({'F1 Macro': f1_macro, \"Accuracy\": accuracy})\n\n# experiment.end()","3b387ec3":"print(LR_model.best_score_)\nprint(LR_model.best_params_) \nprint(LR_model.best_estimator_) ","54e62042":"print(metrics.classification_report(y_test, y_pred))\nprint(metrics.confusion_matrix(y_test, y_pred))","98a51083":"# API key to run experiment in Comet\n# experiment = Experiment(api_key=\"h9aq14TfOuTPJxNhr12fk20kk\",\n#                         project_name=\"tweet-classification\", \n#                         workspace=\"maddy-muir\")\n\nmodel = RidgeClassifier()\nalpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n\n# define grid search\ngrid = dict(alpha=alpha)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nRC_model = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\nRC_model.fit(X_train, y_train)\n\ny_pred = RC_model.predict(X_test)\nf1_macro = metrics.f1_score(y_test, y_pred, average='macro')\naccuracy = metrics.f1_score(y_test, y_pred, average='micro')\n\n# experiment.log_dataset_hash(X_train)\n# experiment.log_parameters({\"model_type\": \"Ridge Classifier\", \"param_grid\": grid})\n# experiment.log_metrics({'F1 Macro': f1_macro, \"Accuracy\": accuracy})\n\n# experiment.end()","4e6fe58b":"print(RC_model.best_score_)\nprint(RC_model.best_params_) \nprint(RC_model.best_estimator_) ","64ad7db2":"print(metrics.classification_report(y_test, y_pred))\nprint(metrics.confusion_matrix(y_test, y_pred))","c51164d1":"# API key to run experiment in Comet\n# experiment = Experiment(api_key=\"h9aq14TfOuTPJxNhr12fk20kk\",\n#                         project_name=\"tweet-classification\", \n#                         workspace=\"maddy-muir\")\n\nmodel = KNeighborsClassifier()\nn_neighbors = range(1, 21, 2)\nweights = ['uniform', 'distance']\nmetric = ['euclidean', 'manhattan', 'minkowski']\n\n# define grid search\ngrid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nKN_model = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\nKN_model.fit(X_train, y_train)\n\ny_pred = KN_model.predict(X_test)\nf1_macro = metrics.f1_score(y_test, y_pred, average='macro')\naccuracy = metrics.f1_score(y_test, y_pred, average='micro')\n\n# experiment.log_dataset_hash(X_train)\n# experiment.log_parameters({\"model_type\": \"KNeighbours\", \"param_grid\": grid})\n# experiment.log_metrics({'F1 Macro': f1_macro, \"Accuracy\": accuracy})\n\n# experiment.end()","69b88759":"print(KN_model.best_score_)\nprint(KN_model.best_params_) \nprint(KN_model.best_estimator_) ","5f30437d":"print(metrics.classification_report(y_test, y_pred))\nprint(metrics.confusion_matrix(y_test, y_pred))","12a4a365":"# API key to run experiment in Comet\n# experiment = Experiment(api_key=\"h9aq14TfOuTPJxNhr12fk20kk\",\n#                         project_name=\"tweet-classification\", \n#                         workspace=\"maddy-muir\")\n\nmodel = ComplementNB()\nalpha =[0.01, 0.1, 0.5, 1, 10]\nfit_prior = [True, False]\nnorm = [True, False]\n\ngrid = dict(alpha=alpha, fit_prior=fit_prior, norm=norm)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nNB_model = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy', error_score=0)\nNB_model.fit(X_train,y_train)\n\ny_pred = NB_model.predict(X_test)\nf1_macro = metrics.f1_score(y_test, y_pred, average='macro')\naccuracy = metrics.f1_score(y_test, y_pred, average='micro')\n\n# experiment.log_dataset_hash(X_train)\n# experiment.log_parameters({\"model_type\": \"Complement NB\", \"param_grid\": grid})\n# experiment.log_metrics({'F1 Macro': f1_macro, \"Accuracy\": accuracy})\n\n# experiment.end()","45fcacd6":"print(NB_model.best_score_)\nprint(NB_model.best_params_) \nprint(NB_model.best_estimator_) ","df8e1373":"print(metrics.classification_report(y_test, y_pred))\nprint(metrics.confusion_matrix(y_test, y_pred))","0ce9de73":"#List of all models\nfinal_models = [\n               SVC(C=10, gamma=1, kernel='linear'),\n               LinearSVC(),\n               LogisticRegression(C=100, \n                                  class_weight=None, \n                                  dual=False,\n                                  fit_intercept=True,\n                                  intercept_scaling=1, \n                                  l1_ratio=None,\n                                  max_iter=100, \n                                  multi_class='auto', \n                                  n_jobs=None,\n                                  penalty='l2',\n                                  random_state=None, \n                                  solver='lbfgs',\n                                  tol=0.0001,\n                                  warm_start=False),\n               KNeighborsClassifier(metric='euclidean', \n                                  n_neighbors=15, \n                                  weights='distance'),\n               ComplementNB(alpha=1, \n                            class_prior=None, \n                            fit_prior=True, \n                            norm=True),\n               RidgeClassifier(alpha=0.5)    \n    ]","66c9637f":"X=train_df['message_clean']\ny=train_df['sentiment']\nX_train ,X_test ,y_train ,y_test = train_test_split(X,y,test_size =0.05, random_state =42)","e03dcfa6":"final_check = model_building(final_models, X_train, y_train, X_test, y_test)\nfinal_ordered = final_check.sort_values('F1-Macro',ascending=False)\nfinal_ordered","776dc5ae":"def final_model_fitting(classifiers, X, y):\n    \"\"\"Function to build all the final classifiers \n    \"\"\" \n    model_summary = {}\n    \n    for clf in classifiers:\n        text_clf = Pipeline([\n            ('tfidf', TfidfVectorizer(stop_words='english', \n                             min_df=1, \n                             max_df=0.9, \n                             ngram_range=(1, 2))),\n            ('clf',clf)\n        ])\n    \n        text_clf.fit(X,y)\n\n        model_summary[clf.__class__.__name__] = text_clf\n      \n    return model_summary","679b22e0":"dict_final = final_model_fitting(final_models,X,y);","01282201":"final_ordered = final_ordered.rename_axis(index='Model')\nfinal_ordered = final_ordered.reset_index(level='Model')\n\nax = plt.gca()\n\nfinal_ordered.plot(kind='line',x='Model',y='F1-Macro',ax=ax)\nfinal_ordered.plot(kind='line',x='Model',y='F1-Accuracy', color='red', ax=ax)\nplt.xticks(rotation=90)\nplt.show()","2d9293af":"ax = plt.gca()\nfinal_ordered.plot(kind='bar',x='Model',y='Execution Time', color='blue', ax=ax)\nplt.show()","3f157457":"# Download CSV file for each one of the final models\nfor key, model in dict_final.items():\n  test_df['sentiment'] = model.predict(test_df['message_clean'])\n  submission = test_df[['tweetid', 'sentiment']]\n  submission.to_csv(f'{key}.csv',index=False)","bf59caed":"# Function to Pickle model for use within  API\ndef save_pickle(filename, model):\n    \"\"\"Pickle model for use within our API\"\"\"\n    save_path = f'{filename}.pkl'\n    print (f\"Training completed. Saving model to: {save_path}\")\n    pickle.dump(model, open(save_path,'wb'))","a801a097":"import pickle\nfor key, model in dict_final.items():\n  save_pickle(key,model)","dbc11c01":"The Data consists out of the following:\n\n\n\n*   15 819 Rows\n*   3 Columns - sentiment, message, tweetid\n*   No null values in any of the columns\n*   There are no empty strings\n\n\n*   Feature: message\n*   Label: sentiment\n\n\n\n\n\n\n\n\n\n\n\n","26e1c463":"#### Logist Regression","03be1aed":"## Label Analyis - Sentiment","cc8865ae":"## Analyse Possible Models\n\n> Models were trained in two ways to determine which models should be selected for hyper parameter tuning.\n\n\n*   Test Train Split\n*   KFold - Cross Validation\n\n\n\n**All outputs from this point onwards have been recorded in markdown cells as execution can take a few hours.**\n\n\n\n\n\n","ad8e0dfe":"With the hashtag analyses it is clear that there are definite commonalities\n\n*   climate\n*   climatechange\n*   Trump\n*   COP22 (Positive, Neutral & News)\n\nThere are also unique hashtags which give us insight into the opinions of the various sentiment and can be analysed further.","b06d3d5d":"# Environment Setup\n\nStart by installing and importing required packages\/libraries\n\n","e07e82e2":"#### Number of Words & Characters","523db7b4":"## Comet Initialization\n\nDue to the way Comet ties into other Maching Learning packages automatically to track certain features, it is required to be one of the first packages imported at the top of the notebook.","3a5e5bf6":"## Hyper Parameter Tuning","5f125797":"Extraction of:\n\n*   Mentions\n*   Hashtags\n*   Identifying Retweets\n*   Emoji's\n*   Language of Tweet\n*   Number of Words & Characters\n\n","7c22d6de":"The 3 Best Models to continue working with will be:\n\n* Linear SVC\n* Ridge Classifier\n* Logistic Regression\n\n","f176bd6c":"### Pickle Files","3e7225d4":"Total Rows & Percentage Distribution of each Sentiment","9ca12cf9":"|Value|Sentiment|Description|\n|-----|---------|-----------|\n|-1 |NEGATIVE |The tweet does not believe in man-made Climate Change|\n|0 |NEUTRAL |The tweet does not believe in man-made Climate Change |\n|1 |POSITIVE |The tweet supports the belief of man-made climate change |\n|2 |NEWS |The tweet links to factual news about climate change |\n ","3261d7b8":"#### Mentions","75fe3538":"## Package Imports","dfbe6d19":"**Ridge Classifier**:\n\n|Category |Outcome |\n|---|---|\n|Best Score |0.7465844609050872 |\n|Best Parameter |{'alpha': 0.4} |\n|Best Estimator |RidgeClassifier(alpha=0.4) |\n","e343e24b":"Output for Kfold Training 12 models:\n\n\n|Model|CV_Mean\t|CV_Std_Dev\t|Execution Time |\n|---|---|---|---|\t\n|LinearSVC\t|0.756432\t|0.009096\t|10.344280 |\n|LogisticRegression\t|0.747393\t|0.011581\t|629.997570 |\n|SVC\t|0.731968\t|0.006319\t|956.538003 |\n|ComplementNB\t|0.725900\t|0.008746\t|6.461605 |\n|GradientBoostingClassifier\t|0.674632\t|0.009818\t|582.413202 |\n|MultinomialNB\t|0.642835\t|0.007042\t|6.423173 |\n|SGDClassifier\t|0.630761\t|0.006763\t|7.825076 |\n|KNeighborsClassifier\t|0.629179\t|0.012365\t|14.777497 |\n|MLPClassifier\t|0.626404\t|0.070268\t|1157.865175 |\n|DecisionTreeClassifier\t|0.596877\t|0.018486\t|111.890710 |\n|RandomForestClassifier\t|0.496115\t|0.043125\t|10.143518 |\n","1eed14a7":"## Download the dataset","71e1de02":"After removing words that are common in all sentiments the words most frequently used are believe, real and world. \n\nThis is in line with most of the tweets having a postive sentiment about man-mad climate change.","5a9c2952":"Display of DataFrame after Information Extraction","15cb9fa1":"#### Common Words - Frequency","0947c9e8":"# Climate Change Sentiment Prediction\n\n**Problem statement**\n\nBuild a robust Machine Learning Model that will be able to predict a person\u2019s belief in Climate Change based on their Tweet Data,  allowing  companies to gain access into customer sentiment","76135a76":"**KNeighbours Classifier:**\n\n|Category |Outcome |\n|---|---|\n|Best Score |0.6768224141878656 |\n|Best Paramater |{'metric': 'euclidean', 'n_neighbors': 15, 'weights': 'distance'} |\n|Best Estimator |KNeighborsClassifier(metric='euclidean', n_neighbors=15, weights='distance') |\n","d0d41050":"**Logistic Regression**\n\n|Category |Outcome |\n|---|---|\n|Best Score |0.6538534933430172 |\n|Best Parameter |{'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'} |\n|Best Estimator |LogisticRegression(C=100, class_weight=None, dual=False, <br> fit_intercept=True,intercept_scaling=1, l1_ratio=None, <br> max_iter=100, multi_class='auto', n_jobs=None,<br> penalty='l2',random_state=None, solver='lbfgs',<br> tol=0.0001, verbose=0,warm_start=False) |","3c44ad06":"Create a visualization of the top 15 words being used in the tweets","84d1be42":"## Function for cleaning the data:\n\n* Remove punctuations \n* Remove stop words\n* Lemmitazation ","a14db505":"Visual display of the DataFrame","2a6e2c13":"## Basic Data Analysis","232d8b9d":"Output for Train-Test-Split Model Training:\n\n| |F1-Macro |F1-Accuracy |F1-Weighted |Execution Time |\n|---|---|---|---|---|\n|LinearSVC |0.715826\t|0.786346\t|0.775680\t|1.451867 |\n|LogisticRegression |0.698115 |0.774968 |0.764190\t|96.207174 |\n|MLPClassifier |0.660432\t|0.738306\t|0.730358\t|172.958856 |\n|SVC\t|0.645543\t|0.761062\t|0.738366\t|111.151640 |\n|XGBClassifier\t|0.639972\t|0.723135\t|0.719677\t|657.077793 |\n|ComplementNB\t|0.625275\t|0.748420\t|0.713261\t|0.847110 |\n|KNeighborsClassifier\t|0.572804\t|0.653603\t|0.654594\t|1.352911 |\n|GradientBoostingClassifier |0.563331\t|0.682680\t|0.659418\t|80.984350 |\n|DecisionTreeClassifier\t|0.497822\t|0.577750\t|0.582857\t|13.826243 |\n|RandomForestClassifier\t|0.409121\t|0.432364\t|0.426874\t|1.280030 |\n|MultinomialNB |0.399523\t|0.663717\t|0.581961\t|0.844753 |\n|SGDClassifier |0.389509 |0.639697\t|0.566300 |0.929380 |","f48aed1c":"The number of words for the negative tweets are more dense with 16 to 23 words. \nNeutral tweets have density span of 15 to almost 21 words. \nNeutral tweets have the lowest minimum number of characters and words in comparison to other sentiments.","8ab13b8f":"Six models are built for sentiment analyis. \n\nThree of these models have an F1-Macro score that is higher than 0.7.\n\nThe LinearSVC model is still the best model with the highest F1-Macro score of 0.715826.\n\nThe dataset was quite unbalanced in this may lead to a score not much higher than 0.7.\n\nHyper Parameter tuning did assist in increase the accuracy of some of the models.","24f78fd1":"#### Linear SVC","c80a3782":"# Exploratory Data Analysis","074d6f6e":"#### Ridge Classifier","8beb1a95":"#### Complement NB","981d8276":"## Feature Analysis","f133dc67":"## Final Models\n\nCheck if F1 Macro Score have improved with new defined hyper parameters","77c7df87":"Twitter has become one of the most cost-effective maketing strategy platforms used by companies as way of engaging with  consumers. \n\nThis notebook describes the process to classifying tweets by sentiment using Natural Language Processing techniques. \n\nIt describes the initial data exploration, as well as implementation of different machine model classifiers used for predictions .\n\nThe notebook will be divided into the sections listed below.","8188d462":"## Test-Train Split","9320ff1a":"#### KNeighbours Classifier","8d8050c8":"#### Bar Graph to compare Execution Time for the final 6 Models","d248484a":"### Functions to Extract Information from Feature","5f81f2ff":"#### Word Cloud\n\nCreating a word cloud to visualizate tweet keywords and text data.\n\nThis is to highlight popular or trending terms based on frequency of use and prominence. \n\nThe larger the word in the visual the more common the word is on tweet messages.\n\nCommon words that appear in all sentiments have been removed from these word clouds to give a clear picture of unique words being used for each sentiment.\n","72a61333":"## Analysis on Data Post Cleaning","2cb81222":"A total of 1965 tweets are duplicated, which are over 10% of all given tweets.\n\nThe majority of these tweets belongs to the Postive Sentiment Class, with the most popular tweet being retweeted 306 times. \n\nFurther analysis on these tweets might be useful.","48297be7":"It is clear that a positive sentiment towards man-made climate change is in a majority with over 50% of the given date falling in this category.","6bbc6b74":"Comparing inforamtion regarding retweets and duplicate messages","78dfd3ad":"# Model Building","18bf1a4b":"# Introduction","c8a99a47":"### Models with KFold\n\nTraining all models with Cross Validation to see how it compares to the Test Train Split training.\n\nExcluding XGBoost from this training as the execution time is excessive.","a35f0144":"Based on the output the best models for the data are:\n\n\n\n1.   Linear SVC\n2.   Logistic Regression\n\n1.   Multi-layer Perceptron classifier (MLP)\n2.   SVC\n\n1.   XGB Classifier\n2.   Complement Naive Bayes\n\nTraining time has to be taken into consideration and for this reason the SVC, MLP & XGB models do not seem suitable\n\n\n\n\n\n\n","2e671d31":"## List of Models","365e77b2":"#### Language Detection","d86871eb":"### Analysis of Each New Feature","3f3e949b":"The following common words have been excluded form the wordcloud to get a better undertanding of each sentiment:\n\n*   climate, change, RT, global, warming\n\nValualble information can be gathered from these wordclouds as the words clearly display the sentiment.\n\nAs an example, negative sentiment contains words like fake, hoax & scam.\n\n","e3d4d7f4":"#### Emojis","6c6d0a30":"Based on the output above the best models based on Kfold training are:\n\n\n\n1.   Linear SVC\n2.   Logisttic Regression\n\n1.   SVC\n2.   Complement Naive Bayes\n\n2.   Gradient Boosting Classifier\n\n\nThe XGB Classifiers's execution time far exceeds any of the other models and for this reason have not been included in the Cross Validation training.\n\nThe SVC and Gradient Boosting Classifier also have higher execution times but these models can possibly considered for further training with GridSearch.\n\nThe Gradient Boosting Classifier have out-performed the Multi-layer Perceptron classifier (MLP) - listed as number 3 under the Train Test Split Model Training\n\n\n\n\n\n","526705e0":"### Application of Extraction Functions","08ec8f2b":"#### Hashtags","d914025c":"# Table of Contents\n\n\n\n1.   Environment Setup\n\n    1.1 Python Package Setup\n\n    1.2 Comet Initialization\n\n    1.3 Package Imports\n\n2.   Data\n\n    2.1 Download of dataset\n\n    2.2 Basic Data Analysis\n\n\n3.   Exploratory Data Analysis\n\n    3.1 Label Analysis - Sentiment\n\n    3.2 Feature Analysis\n\n4.   Cleaning of Data\n\n    4.1 Function for cleaning the data\n\n    4.2 Analysis of Data Post Cleaning\n\n5.  Preparing Data for Training\n\n    5.1 Train Test Split\n\n6.  Model Building \n\n    6.1 List of Models\n\n    6.2 Analyse Possible Models\n\n    6.3 Hyper Parameter Tuning\n\n    6.4 Final Models\n\n7.  Conclusion\n\n\n\n","c2f777b5":"# Conclusion","3094b27d":"### CSV Files","ecbf8ece":"### TfidfVectorizer","2bdb7305":"<center><img src=\"https:\/\/www.azocleantech.com\/images\/Article_Images\/ImageForArticle_898(1).jpg\" width=\"90%\" \/><\/center>\n\n\n","cfdfa439":"#### Visualization of Metrics","48f799cd":"**Complement Naive Bayes**\n\n|Category |Outcome |\n|---|---|\n|Best Score |0.7169281794452226 |\n|Best Paramater |{'alpha': 1, 'fit_prior': True, 'norm': True} |\n|Best Estimator |ComplementNB(alpha=1, class_prior=None, fit_prior=True, norm=True) |\n","5ad359a2":"172 of the rows are indentified as not being English.\n\nThis can cause an inaccuracy when we build the model\/s.\n\nTwo approaches can be followed:\n\n\n1.   Exclude these rows when training and fitting the model\n2.   Translate these rows \n\n","9cc5acb0":"# Data\n\nTwo dataframes are provided.\n\nThe first dataframe will be for training the model and the second for testing the dataframe and submission in Kaggle.\n\n","a2bc48a6":"Almost 61% of the messages are retweets and only 5 of all the duplicate messages are not retweets.","4290a889":"Analysing duplicate messages","b6500c24":"## Python Package Installation","781fbc26":"Currently no further useful insights can be gathered from the mentions","3d7c45de":"### GridSearch","f0adff2c":"#### Visualization of Metrics","5bec3713":"# Cleaning of Data","ac369e3e":"### Building of final models","039c0c87":"# Preparing Data for Training","68a113c3":"Hyper Parameter Tuning will be performed on the following models:\n\n\nFurther Tuning on Current Models \n*   Linear SVC\n*   Logistic Regression\n*   Complement NB\n*   KNeighbours Classifier\n\nNew Models to Test\n*   Ridge Classifier\n\n\n\n\n\n\n\n\n\n","3d5b73ea":"The data only contains 305 emojis belonging to 182 rows.\n\nTwo approaches can be followed:\n\n\n1.   Exclude these emojis with model building\n2.   Convert the emojis to English words","f36ab090":"**Linear SVC**\n\n|Category |Outcome |\n|---|---|\n|Best Score |0.6452938881166734 |\n|Best Parameter |{'C': 10, 'gamma': 1, 'kernel': 'linear'} |\n|Best Estimator | SVC(C=10, gamma=1, kernel='linear') |\n","6d9bf817":"### Visualizations of Metrics\n\n#### Line Graph to compare F1-Macro & F1-Accuracy Scores for the Final 6 Models","71f2795a":" Final Check of Models \n\n|Model |F1-Macro |F1-Accuracy\t|F1-Weighted\t|Execution Time |\n|---|---|---|---|---|\n|LinearSVC\t|0.715826\t|0.786346\t|0.775680\t|1.115070 |\n|RidgeClassifier\t|0.704771\t|0.782554\t|0.769334\t|1.236814 |\n|SVC\t|0.703910\t|0.772440\t|0.762630\t|109.217182 |\n|LogisticRegression\t|0.695375\t|0.772440\t|0.761290\t|8.824065 |\n|KNeighborsClassifier\t|0.617822\t|0.702908\t|0.687766\t|1.210301 |\n|ComplementNB\t|0.582987\t|0.728192\t|0.686366\t|0.703257 |","af6c1b68":"#### Bigram Analyis per Sentiment","03b53978":"### Analysis of Main Feature","5458e0e1":"### Models with Test-Train Split\nFunction for model building with a pipeline to first vectorize the feature with the TfidfVectorizer","09b1d2e6":"#### Identifying Retweets","9e1fa47b":"Visualization of Sentiment Distribution"}}