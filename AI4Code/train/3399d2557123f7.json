{"cell_type":{"6b330440":"code","69f8a1e1":"code","930280c0":"code","a8ca8a8a":"code","9fa1e8fd":"code","286468ab":"code","42d4531b":"code","d196959d":"code","90b43477":"code","08bcd484":"code","79f06d77":"code","cf6e998f":"code","85845533":"code","b5335297":"code","eec3e9e6":"code","e298200f":"code","0e845777":"code","4b21d91f":"markdown","5d4bbcf4":"markdown","79115616":"markdown","3d5b0c48":"markdown","6cb0ae6a":"markdown","d340a503":"markdown","6b7e1e19":"markdown","2e539b6c":"markdown","b6b203f1":"markdown","222fcb0b":"markdown","7609aa97":"markdown","70c03495":"markdown","effea397":"markdown","e5b6f7bc":"markdown","c6af88e4":"markdown"},"source":{"6b330440":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nimport warnings\nimport time\nimport sys\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')","69f8a1e1":"new_transactions = pd.read_csv('..\/input\/new_merchant_transactions.csv')\n\nnew_transactions['authorized_flag'] = \\\n    new_transactions['authorized_flag'].map({'Y':1, 'N':0})\n\ndef aggregate_new_transactions(new_trans):    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'month_lag': ['min', 'max']\n        }\n    agg_new_trans = new_trans.groupby(['card_id']).agg(agg_func)\n    agg_new_trans.columns = ['new_' + '_'.join(col).strip() \n                           for col in agg_new_trans.columns.values]\n    agg_new_trans.reset_index(inplace=True)\n    \n    df = (new_trans.groupby('card_id')\n          .size()\n          .reset_index(name='new_transactions_count'))\n    \n    agg_new_trans = pd.merge(df, agg_new_trans, on='card_id', how='left')\n    \n    return agg_new_trans\n\nnew_trans = aggregate_new_transactions(new_transactions)\n\n# historical_transactions\n\nhistorical_transactions = pd.read_csv('..\/input\/historical_transactions.csv')\n\nhistorical_transactions['authorized_flag'] = \\\n    historical_transactions['authorized_flag'].map({'Y':1, 'N':0})\n\ndef aggregate_historical_transactions(history):\n    \n    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max']\n        }\n    agg_history = history.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = ['hist_' + '_'.join(col).strip() \n                           for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n    \n    df = (history.groupby('card_id')\n          .size()\n          .reset_index(name='hist_transactions_count'))\n    \n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history\n\nhistory = aggregate_historical_transactions(historical_transactions)\n\ndef read_data(input_file):\n    df = pd.read_csv(input_file)\n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    df['year'] = df['first_active_month'].dt.year\n    df['month'] = df['first_active_month'].dt.month\n    df['elapsed_time'] = (datetime.date(2018, 2, 1) - df['first_active_month'].dt.date).dt.days\n    return df\ntrain = read_data('..\/input\/train.csv')\ntest = read_data('..\/input\/test.csv')\n\ntarget = train['target']\ndel train['target']\n\n# Merge historical transactions and new transactions\n\ntrain = pd.merge(train, history, on='card_id', how='left')\ntest = pd.merge(test, history, on='card_id', how='left')\n\ntrain = pd.merge(train, new_trans, on='card_id', how='left')\ntest = pd.merge(test, new_trans, on='card_id', how='left')\n\n\n# Feature preparation\n\nuse_cols = [col for col in train.columns if col not in ['card_id', 'first_active_month']]\n\ntrain = train[use_cols]\ntest = test[use_cols]\n\nfeatures = list(train[use_cols].columns)\ncategorical_feats = [col for col in features if 'feature_' in col]\n\nfor col in categorical_feats:\n    print(col, 'have', train[col].value_counts().shape[0], 'categories.')\n\n\nfrom sklearn.preprocessing import LabelEncoder\nfor col in categorical_feats:\n    print(col)\n    lbl = LabelEncoder()\n    lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n    train[col] = lbl.transform(list(train[col].values.astype('str')))\n    test[col] = lbl.transform(list(test[col].values.astype('str')))\n\ndf_all = pd.concat([train, test])\ndf_all = pd.get_dummies(df_all, columns=categorical_feats)\n\nlen_train = train.shape[0]\n\ntrain = df_all[:len_train]\ntest = df_all[len_train:]","930280c0":"from scipy.stats import ks_2samp","a8ca8a8a":"FOLDs = KFold(n_splits=5, shuffle=True, random_state=1989)\ntrn_idxs = []\nval_idxs = []\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_idxs.append(trn_idx)\n    val_idxs.append(val_idx)","9fa1e8fd":"print(ks_2samp(target, target[trn_idxs[0]]))\nprint(ks_2samp(target, target[trn_idxs[1]]))\nprint(ks_2samp(target, target[trn_idxs[2]]))\nprint(ks_2samp(target, target[trn_idxs[3]]))\nprint(ks_2samp(target, target[trn_idxs[4]]))","286468ab":"lgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\", \n               \"max_depth\": 7, \"min_child_samples\": 20, \n               \"reg_alpha\": 1, \"reg_lambda\": 1,\n               \"num_leaves\" : 64, \"learning_rate\" : 0.001, \n               \"subsample\" : 0.8, \"colsample_bytree\" : 0.8, \n               \"verbosity\": -1}\n\nFOLDs = KFold(n_splits=5, shuffle=True, random_state=1989)\n\noof_lgb = np.zeros(len(train))\npredictions_lgb = np.zeros(len(test))\n\nfeatures_lgb = list(train.columns)\nfeature_importance_df_lgb = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx], label=target.iloc[val_idx])\n\n    print(\"LGB \" + str(fold_) + \"-\" * 50)\n    num_round = 2000\n    clf = lgb.train(lgb_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 2000)\n    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n\n    fold_importance_df_lgb = pd.DataFrame()\n    fold_importance_df_lgb[\"feature\"] = features_lgb\n    fold_importance_df_lgb[\"importance\"] = clf.feature_importance()\n    fold_importance_df_lgb[\"fold\"] = fold_ + 1\n    feature_importance_df_lgb = pd.concat([feature_importance_df_lgb, fold_importance_df_lgb], axis=0)\n    predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) \/ FOLDs.n_splits\n    \n\nprint(np.sqrt(mean_squared_error(oof_lgb, target)))","42d4531b":"corr_result = pd.concat([train, target], axis=1)\nabs(corr_result.corr()['target']).sort_values(ascending=False)","d196959d":"from sklearn.model_selection import StratifiedKFold","90b43477":"new_cat = np.ceil(train['new_purchase_amount_max'].fillna(0)\/20).values\n\nFOLDs = StratifiedKFold(n_splits=5, shuffle=True, random_state=1989)\ntrn_idxs = []\nval_idxs = []\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train,new_cat)):\n    trn_idxs.append(trn_idx)\n    val_idxs.append(val_idx)\n\nprint(ks_2samp(target, target[trn_idxs[0]]))\nprint(ks_2samp(target, target[trn_idxs[1]]))\nprint(ks_2samp(target, target[trn_idxs[2]]))\nprint(ks_2samp(target, target[trn_idxs[3]]))\nprint(ks_2samp(target, target[trn_idxs[4]]))","08bcd484":"lgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\", \n               \"max_depth\": 7, \"min_child_samples\": 20, \n               \"reg_alpha\": 1, \"reg_lambda\": 1,\n               \"num_leaves\" : 64, \"learning_rate\" : 0.001, \n               \"subsample\" : 0.8, \"colsample_bytree\" : 0.8, \n               \"verbosity\": -1}\n\nFOLDs = StratifiedKFold(n_splits=5, shuffle=True, random_state=1989)\n\noof_lgb = np.zeros(len(train))\npredictions_lgb = np.zeros(len(test))\n\nfeatures_lgb = list(train.columns)\nfeature_importance_df_lgb = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train, new_cat)):\n    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx], label=target.iloc[val_idx])\n\n    print(\"LGB \" + str(fold_) + \"-\" * 50)\n    num_round = 2000\n    clf = lgb.train(lgb_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 2000)\n    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n\n    fold_importance_df_lgb = pd.DataFrame()\n    fold_importance_df_lgb[\"feature\"] = features_lgb\n    fold_importance_df_lgb[\"importance\"] = clf.feature_importance()\n    fold_importance_df_lgb[\"fold\"] = fold_ + 1\n    feature_importance_df_lgb = pd.concat([feature_importance_df_lgb, fold_importance_df_lgb], axis=0)\n    predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) \/ FOLDs.n_splits\n    \n\nprint(np.sqrt(mean_squared_error(oof_lgb, target)))","79f06d77":"new_cat = np.ceil(train['elapsed_time'] \/ 10).values\n\nFOLDs = StratifiedKFold(n_splits=5, shuffle=True, random_state=1989)\ntrn_idxs = []\nval_idxs = []\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train,new_cat)):\n    trn_idxs.append(trn_idx)\n    val_idxs.append(val_idx)\n\nprint(ks_2samp(target, target[trn_idxs[0]]))\nprint(ks_2samp(target, target[trn_idxs[1]]))\nprint(ks_2samp(target, target[trn_idxs[2]]))\nprint(ks_2samp(target, target[trn_idxs[3]]))\nprint(ks_2samp(target, target[trn_idxs[4]]))","cf6e998f":"lgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\", \n               \"max_depth\": 7, \"min_child_samples\": 20, \n               \"reg_alpha\": 1, \"reg_lambda\": 1,\n               \"num_leaves\" : 64, \"learning_rate\" : 0.001, \n               \"subsample\" : 0.8, \"colsample_bytree\" : 0.8, \n               \"verbosity\": -1}\n\nFOLDs = StratifiedKFold(n_splits=5, shuffle=True, random_state=1989)\n\noof_lgb = np.zeros(len(train))\npredictions_lgb = np.zeros(len(test))\n\nfeatures_lgb = list(train.columns)\nfeature_importance_df_lgb = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train, new_cat)):\n    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx], label=target.iloc[val_idx])\n\n    print(\"LGB \" + str(fold_) + \"-\" * 50)\n    num_round = 2000\n    clf = lgb.train(lgb_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 2000)\n    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n\n    fold_importance_df_lgb = pd.DataFrame()\n    fold_importance_df_lgb[\"feature\"] = features_lgb\n    fold_importance_df_lgb[\"importance\"] = clf.feature_importance()\n    fold_importance_df_lgb[\"fold\"] = fold_ + 1\n    feature_importance_df_lgb = pd.concat([feature_importance_df_lgb, fold_importance_df_lgb], axis=0)\n    predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) \/ FOLDs.n_splits\n    \n\nprint(np.sqrt(mean_squared_error(oof_lgb, target)))","85845533":"import xgboost as xgb\n\nxgb_params = {'eta': 0.001, 'max_depth': 7, 'subsample': 0.8, 'colsample_bytree': 0.8, \n          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True}\n\n\n\nFOLDs = StratifiedKFold(n_splits=5, shuffle=True, random_state=1989)\n\noof_xgb = np.zeros(len(train))\npredictions_xgb = np.zeros(len(test))\n\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train, new_cat)):\n    trn_data = xgb.DMatrix(data=train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = xgb.DMatrix(data=train.iloc[val_idx], label=target.iloc[val_idx])\n    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n    print(\"xgb \" + str(fold_) + \"-\" * 50)\n    num_round = 2000\n    xgb_model = xgb.train(xgb_params, trn_data, num_round, watchlist, early_stopping_rounds=100, verbose_eval=200)\n    oof_xgb[val_idx] = xgb_model.predict(xgb.DMatrix(train.iloc[val_idx]), ntree_limit=xgb_model.best_ntree_limit+50)\n\n    predictions_xgb += xgb_model.predict(xgb.DMatrix(test), ntree_limit=xgb_model.best_ntree_limit+50) \/ FOLDs.n_splits\n\nnp.sqrt(mean_squared_error(oof_xgb, target))","b5335297":"print('lgb', np.sqrt(mean_squared_error(oof_lgb, target)))\nprint('xgb', np.sqrt(mean_squared_error(oof_xgb, target)))","eec3e9e6":"total_sum = 0.5 * oof_lgb + 0.5 * oof_xgb\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(total_sum, target)**0.5))","e298200f":"cols = (feature_importance_df_lgb[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df_lgb.loc[feature_importance_df_lgb.feature.isin(cols)]\n\nplt.figure(figsize=(14,14))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","0e845777":"sub_df = pd.read_csv('..\/input\/sample_submission.csv')\nsub_df[\"target\"] = 0.5 * predictions_lgb + 0.5 * predictions_xgb\nsub_df.to_csv(\"submission.csv\", index=False)","4b21d91f":"# 2. new_purchase_amount_max","5d4bbcf4":"# 1.  Random sampling\n- At first, we use random-sampling to get a control case.","79115616":"- Statistics should go to zero and P-value should go to 1.","3d5b0c48":"# Background\n- Delicate sampling is needed to avoid unwanted bias.\n- Let's find the good features which give well-distributed folds.\n- To experiment, I forked my kernel and use same features, random number,  parameters and model.","6cb0ae6a":"# 3. elapsed_time ","d340a503":"- This kernel shows \n- how to use 'new_merchant_transactions.csv' \n- how to use lgb and xgb\n- how to ensemble them.","6b7e1e19":"# XGB","2e539b6c":"- Compared to my previous kernel, the cv score is improved.\n- 3.73413  -> 3.73340 ","b6b203f1":"- To get more stable sampling, we can compare the distribution of target with the distribution of target for each fold.\n- Simply, I used the Kolmogorov\u2013Smirnov test.\n","222fcb0b":"# I referred a very helpful kernel. Thanks!\n- https:\/\/www.kaggle.com\/fabiendaniel\/elo-world\n- https:\/\/www.kaggle.com\/youhanlee\/stratified-sampling-for-regression-lb-1-4627","7609aa97":"- Statistics should go to zero and P-value should go to 1.\n- K-Fold using random sampling shows well-distributed sampling.","70c03495":"- After checking the correlation with target, we know that 'new_purchase_amount_max' and 'elapsed_time' are good candidate.","effea397":"# New transactions","e5b6f7bc":"# ===================== Let's start =====================","c6af88e4":"## After manual categorization, 20 is selected as denominator. "}}