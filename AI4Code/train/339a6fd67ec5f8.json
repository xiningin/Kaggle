{"cell_type":{"1f37f1a7":"code","30998b8a":"code","2458b617":"code","bad928e5":"code","e113788d":"code","558c5e7c":"code","a409245d":"code","5a4ef36b":"code","25fd961e":"code","871ee04b":"code","baa59b8e":"code","289cd87d":"code","84bdaebb":"code","42f27262":"code","b829dfe4":"code","5df9ea23":"code","001731d4":"code","307948b8":"code","a33bb025":"code","df2d976c":"code","0db599da":"code","165a8715":"code","71a055cb":"code","1eec7c87":"code","dd02ab6a":"markdown","0504faae":"markdown","17b697af":"markdown","1c70d3d8":"markdown","27f20689":"markdown","ba127a78":"markdown","44ac6781":"markdown","88f0b958":"markdown","d5e3fedb":"markdown","cc80c662":"markdown","f436b19c":"markdown","ec06af4a":"markdown","63cca0cf":"markdown","0a0af37c":"markdown","ce2fc210":"markdown"},"source":{"1f37f1a7":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer,WordNetLemmatizer\nfrom string import punctuation\nfrom wordcloud import WordCloud\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.decomposition import TruncatedSVD\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split,GridSearchCV,StratifiedKFold\nfrom sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score,confusion_matrix\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense,Embedding,Bidirectional,Dropout,SpatialDropout1D,GlobalMaxPool1D,LSTM,BatchNormalization,Conv1D,MaxPool1D\nfrom keras.models import Sequential,load_model\nfrom keras.optimizers import Adam,RMSprop\nfrom keras import regularizers","30998b8a":"train = pd.read_csv(\"..\/input\/movie-review-sentiment-analysis-kernels-only\/train.tsv.zip\",sep=\"\\t\")\ntest = pd.read_csv(\"..\/input\/movie-review-sentiment-analysis-kernels-only\/test.tsv.zip\",sep=\"\\t\")\nsub = pd.read_csv(\"..\/input\/movie-review-sentiment-analysis-kernels-only\/sampleSubmission.csv\")","2458b617":"train.head()","bad928e5":"#Count Of Tweets\nsns.countplot(train['Sentiment'],palette='rocket_r')\nplt.title(\"No of Tweet Sentiments\")","e113788d":"#Phrases Per Sentence\nfig,ax = plt.subplots(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(train.groupby('SentenceId')['PhraseId'].count())\nplt.title(\"Avg Phrases Per Sentence in Train\")\nplt.subplot(1,2,2)\nsns.distplot(test.groupby('SentenceId')['PhraseId'].count())\nplt.title(\"Avg Phrases Per Sentence in Test\")\n\nprint(\"Avg Phrases Per Sentence in Train: \",round(train.groupby('SentenceId')['PhraseId'].count().mean()))\nprint(\"Avg Phrases Per Sentence in Test: \",round(test.groupby('SentenceId')['PhraseId'].count().mean()))","558c5e7c":"#No of Words in Phrases\nfig,ax = plt.subplots(figsize=(22,5))\nfig.suptitle(\"Avg Words In Phrases\",fontsize=16)\n\nplt.subplot(1,5,1)\nsns.distplot(train[train['Sentiment']==0]['Phrase'].str.split().apply(lambda x:len(x)))\nplt.title(\"Sentiment 0\")\nprint(\"Avg Words in Phrases with Sentiment 0: \",round(train[train['Sentiment']==0]['Phrase'].str.split().apply(lambda x:len(x)).mean()))\n\nplt.subplot(1,5,2)\nsns.distplot(train[train['Sentiment']==1]['Phrase'].str.split().apply(lambda x:len(x)))\nplt.title(\"Sentiment 1\")\nprint(\"Avg Words in Phrases with Sentiment 1: \",round(train[train['Sentiment']==1]['Phrase'].str.split().apply(lambda x:len(x)).mean()))\n\nplt.subplot(1,5,3)\nsns.distplot(train[train['Sentiment']==2]['Phrase'].str.split().apply(lambda x:len(x)))\nplt.title(\"Sentiment 2\")\nprint(\"Avg Words in Phrases with Sentiment 2: \",round(train[train['Sentiment']==2]['Phrase'].str.split().apply(lambda x:len(x)).mean()))\n\nplt.subplot(1,5,4)\nsns.distplot(train[train['Sentiment']==3]['Phrase'].str.split().apply(lambda x:len(x)))\nplt.title(\"Sentiment 3\")\nprint(\"Avg Words in Phrases with Sentiment 3: \",round(train[train['Sentiment']==3]['Phrase'].str.split().apply(lambda x:len(x)).mean()))\n\nplt.subplot(1,5,5)\nsns.distplot(train[train['Sentiment']==4]['Phrase'].str.split().apply(lambda x:len(x)))\nplt.title(\"Sentiment 4\")\nprint(\"Avg Words in Phrases with Sentiment 4: \",round(train[train['Sentiment']==4]['Phrase'].str.split().apply(lambda x:len(x)).mean()))","a409245d":"#Avg Characters in Phrases\nfig,ax = plt.subplots(figsize=(22,5))\nfig.suptitle(\"Avg Characters In Phrases\",fontsize=16)\n\nplt.subplot(1,5,1)\nsns.distplot(train[train['Sentiment']==0]['Phrase'].str.len())\nplt.title(\"Sentiment 0\")\nprint(\"Avg Characters in Phrases with Sentiment 0: \",round(train[train['Sentiment']==0]['Phrase'].str.len().mean()))\n\nplt.subplot(1,5,2)\nsns.distplot(train[train['Sentiment']==1]['Phrase'].str.len())\nplt.title(\"Sentiment 1\")\nprint(\"Avg Characters in Phrases with Sentiment 1: \",round(train[train['Sentiment']==1]['Phrase'].str.len().mean()))\n\nplt.subplot(1,5,3)\nsns.distplot(train[train['Sentiment']==2]['Phrase'].str.len())\nplt.title(\"Sentiment 2\")\nprint(\"Avg Characters in Phrases with Sentiment 2: \",round(train[train['Sentiment']==2]['Phrase'].str.len().mean()))\n\nplt.subplot(1,5,4)\nsns.distplot(train[train['Sentiment']==3]['Phrase'].str.len())\nplt.title(\"Sentiment 3\")\nprint(\"Avg Characters in Phrases with Sentiment 3: \",round(train[train['Sentiment']==3]['Phrase'].str.len().mean()))\n\nplt.subplot(1,5,5)\nsns.distplot(train[train['Sentiment']==4]['Phrase'].str.len())\nplt.title(\"Sentiment 4\")\nprint(\"Avg Characters in Phrases with Sentiment 4: \",round(train[train['Sentiment']==4]['Phrase'].str.len().mean()))\n","5a4ef36b":"#WordCloud\nfig,ax = plt.subplots(figsize=(20,40))\nplt.axis('off')\n\nplt.subplot(5,1,1)\ntext = \" \".join(train[train['Sentiment']==0]['Phrase'])\nwordcloud = WordCloud(max_font_size = None,background_color='white',width=1000,height=1000).generate(text)\nplt.title(\"WordCloud for Sentiment 0\")\nplt.axis('off')\nplt.imshow(wordcloud)\n\nplt.subplot(5,1,2)\ntext = \" \".join(train[train['Sentiment']==1]['Phrase'])\nwordcloud = WordCloud(max_font_size = None,background_color='white',width=1000,height=1000).generate(text)\nplt.title(\"WordCloud for Sentiment 1\")\nplt.axis('off')\nplt.imshow(wordcloud)\n\nplt.subplot(5,1,3)\ntext = \" \".join(train[train['Sentiment']==2]['Phrase'])\nwordcloud = WordCloud(max_font_size = None,background_color='white',width=1000,height=1000).generate(text)\nplt.title(\"WordCloud for Sentiment 2\")\nplt.axis('off')\nplt.imshow(wordcloud)\n\nplt.subplot(5,1,4)\ntext = \" \".join(train[train['Sentiment']==3]['Phrase'])\nwordcloud = WordCloud(max_font_size = None,background_color='white',width=1000,height=1000).generate(text)\nplt.title(\"WordCloud for Sentiment 3\")\nplt.axis('off')\nplt.imshow(wordcloud)\n\nplt.subplot(5,1,5)\ntext = \" \".join(train[train['Sentiment']==4]['Phrase'])\nwordcloud = WordCloud(max_font_size = None,background_color='white',width=1000,height=1000).generate(text)\nplt.title(\"WordCloud for Sentiment 4\")\nplt.axis('off')\nplt.imshow(wordcloud)\n","25fd961e":"stemmer = SnowballStemmer('english',ignore_stopwords=True)\nlemmatizer = WordNetLemmatizer()\ndef clean(text):\n    sentence=[]\n    for word in text.split():\n        word = re.sub('[^a-zA-Z]','',word)\n        word = word.lower()\n        word = lemmatizer.lemmatize(word)\n        word = word.strip()\n        sentence.append(word)\n    return \" \".join(sentence)\n\ntrain['Phrase'] = train['Phrase'].apply(lambda x:clean(x))\ntest['Phrase'] = test['Phrase'].apply(lambda x:clean(x))","871ee04b":"x_train,x_valid,y_train,y_valid = train_test_split(train['Phrase'],train['Sentiment'],test_size=0.2,random_state=42)","baa59b8e":"#Tokenize\nvocab_size=20000\nembedding_dim=200\nmax_length=50\ntrunc_type=\"post\"\npad_type=\"post\"\noov_tok=\"<OOV>\"\nepochs=10\nbatch_size=128\n\ntokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_tok)\ntokenizer.fit_on_texts(list(x_train)+list(x_valid))\nword_index = tokenizer.word_index\n\ntrain_seq = tokenizer.texts_to_sequences(x_train)\ntrain_pad = pad_sequences(train_seq,maxlen=max_length,truncating = trunc_type,padding=pad_type)\n\nval_seq = tokenizer.texts_to_sequences(x_valid)\nval_pad = pad_sequences(val_seq,maxlen=max_length,truncating = trunc_type,padding=pad_type)","289cd87d":"len(word_index)","84bdaebb":"#Glove Embeddings\nembeddings_index={}\nwith open(\"..\/input\/glove6b\/glove.6B.200d.txt\",'r',encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n        \nembeddings_matrix = np.zeros((len(word_index)+1, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector","42f27262":"#Model\n\nmodel = Sequential()\nmodel.add(Embedding(len(word_index)+1,embedding_dim,input_length=max_length,weights=[embeddings_matrix]))\nmodel.add(SpatialDropout1D(0.4))\n\nmodel.add(Conv1D(128,3,activation='relu',padding='same'))\nmodel.add(MaxPool1D(2))\n\nmodel.add(Conv1D(64,3,activation='relu',padding='same'))\nmodel.add(MaxPool1D(2))\n\nmodel.add(Bidirectional(LSTM(64,recurrent_dropout=0.5,dropout=0.5,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(64,recurrent_dropout=0.5,dropout=0.5,return_sequences=True)))\n\nmodel.add(GlobalMaxPool1D())\n\nmodel.add(Dense(64,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(32,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(5,activation='softmax'))\n\nmodel.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc'])\nmodel.summary()","b829dfe4":"#Callbacks\nearlystop = EarlyStopping(monitor='val_loss',patience=2,verbose=1)\nlearning_reduce = ReduceLROnPlateau(patience=1,monitor=\"val_acc\",verbose=1,min_lr=0.00001,factor=0.5,cooldown=1)\ncallbacks = [earlystop,learning_reduce]","5df9ea23":"history = model.fit(train_pad,y_train,epochs=epochs,validation_data=(val_pad,y_valid),callbacks=callbacks,\n                    batch_size=batch_size)","001731d4":"#Plot\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history[\"val_\"+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string,\"val_\"+string])\n    plt.show()\nplot_graphs(history,'acc')\nplot_graphs(history,'loss')","307948b8":"tweet_tokenizer = TweetTokenizer()\ntfidf = TfidfVectorizer(strip_accents=None,lowercase=False,preprocessor=None,ngram_range=(1,3),\n                        tokenizer=tweet_tokenizer.tokenize,use_idf=True,norm='l2',smooth_idf=True)\ntfidf.fit(list(x_train.values) + list(x_valid.values))\nxtrain_tfv = tfidf.transform(x_train)\nxvalid_tfv = tfidf.transform(x_valid)\n\nscl = preprocessing.StandardScaler(with_mean=False)\nxtrain_tfv_std = scl.fit_transform(xtrain_tfv)\nxvalid_tfv_std = scl.transform(xvalid_tfv)","a33bb025":"logistic = LogisticRegressionCV(cv=3,scoring='accuracy',random_state=42,n_jobs=-1,verbose=3)\nlogistic.fit(xtrain_tfv_std,y_train)\nlogistic_accuracy = logistic.score(xvalid_tfv_std,y_valid)\nprint(\"Accuracy:\",logistic_accuracy)","df2d976c":"svc = SVC(C=0.1,random_state=42,verbose=2)\nsvc.fit(xtrain_tfv_std,y_train)\nsvc_accuracy = svc.score(xvalid_tfv_std,y_valid)\nprint(\"Accuracy:\",svc_accuracy)","0db599da":"naive = MultinomialNB()\nnaive.fit(xtrain_tfv_std,y_train)\nnaive_accuracy = naive.score(xvalid_tfv_std,y_valid)\nprint(\"Accuracy:\",naive_accuracy)","165a8715":"xgboost = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                            subsample=0.8, nthread=10, learning_rate=0.1,verbose=2)\nxgboost.fit(xtrain_tfv_std,y_train)\nxg_accuracy = xgboost.score(xvalid_tfv_std,y_valid)\nprint(\"Accuracy:\",xg_accuracy)","71a055cb":"test_sequences = tokenizer.texts_to_sequences(test['Phrase'])\ntest_pad = pad_sequences(test_sequences,maxlen=max_length,truncating=trunc_type,padding=pad_type)","1eec7c87":"# CNN+LSTM\nypred = model.predict_classes(test_pad,verbose=1)\nsub['Sentiment'] = ypred\nsub.to_csv(\"submission.csv\", index=False, header=True)","dd02ab6a":"## Logistic Regression","0504faae":"# Preparing Data","17b697af":"# Exploratory Data Analysis","1c70d3d8":"# XGBoost","27f20689":"## Neural Network (Conv + LSTM)","ba127a78":"# Movie Review Sentiment Analysis","44ac6781":"# Importing Libraries","88f0b958":"We can't remove stopwords and punctuations as we have phrases as our data and phrases can contain a single word.","d5e3fedb":"The sentiment labels are:\n\n0 - negative <br>\n1 - somewhat negative <br>\n2 - neutral <br>\n3 - somewhat positive <br>\n4 - positive","cc80c662":"## SVM ","f436b19c":"## Naive Bayes","ec06af4a":"# Submission","63cca0cf":"# Cleaning","0a0af37c":"# Classification Model","ce2fc210":"<img src=\"https:\/\/www.rottentomatoes.com\/assets\/pizza-pie\/head-assets\/images\/RT_TwitterCard_2018.jpg\" alt=\"drawing\" width=\"500\" height=\"600\"\/>\n<br><br>\n\"There's a thin line between likably old-fashioned and fuddy-duddy, and The Count of Monte Cristo ... never quite settles on either side.\" \n<br><br>\nThe <b>Rotten Tomatoes<b> movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee.In their work on sentiment treebanks, Socher et al. used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. This competition presents a chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoes dataset. You are asked to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging."}}