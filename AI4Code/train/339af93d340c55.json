{"cell_type":{"eaf4c37b":"code","6e706d80":"code","ccb78404":"code","db6a6e49":"code","e9e05864":"code","55ace69b":"code","9617db96":"code","ad03a13d":"code","95da05c8":"code","406f001c":"code","b5e38ba8":"code","8506f976":"code","aed54f2a":"code","2d9ada6f":"code","7a4f12a5":"code","a0811ca5":"code","515c09e9":"code","66b7048f":"code","035df284":"code","f310a6ee":"code","b7bb35ac":"code","4faa72f7":"code","f519140d":"code","eb59c400":"code","e3b43f3e":"code","dd063463":"code","c232e7ee":"code","68f32c83":"code","7df362cd":"code","c6dc3102":"code","53847bd9":"code","262083f3":"code","e11617d3":"code","d4ed65a6":"code","58860a8a":"code","6a2d5e14":"code","1e4ee38e":"code","3d9e0a4e":"markdown","afbc3de1":"markdown","e1e29ca4":"markdown","ef79540c":"markdown","6401f13f":"markdown","cc8b8d3d":"markdown","06518186":"markdown","5b69fe2c":"markdown","0a17f343":"markdown","43cfc64d":"markdown","e21a33ff":"markdown","4d67f9ac":"markdown"},"source":{"eaf4c37b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport time\nimport warnings\nwarnings.simplefilter(action = 'ignore', category = FutureWarning)","6e706d80":"from sklearn.metrics import roc_auc_score, precision_score, recall_score\nfrom sklearn.model_selection import KFold, StratifiedKFold","ccb78404":"from lightgbm import LGBMClassifier","db6a6e49":"from scipy.stats import ranksums","e9e05864":"from bayes_opt import BayesianOptimization","55ace69b":"def reduce_mem_usage(data, verbose = True):\n    start_mem = data.memory_usage().sum() \/ 1024**2\n    if verbose:\n        print('Memory usage of dataframe: {:.2f} MB'.format(start_mem))\n    \n    for col in data.columns:\n        col_type = data[col].dtype\n        \n        if col_type != object:\n            c_min = data[col].min()\n            c_max = data[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    data[col] = data[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    data[col] = data[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    data[col] = data[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    data[col] = data[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    data[col] = data[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    data[col] = data[col].astype(np.float32)\n                else:\n                    data[col] = data[col].astype(np.float64)\n\n    end_mem = data.memory_usage().sum() \/ 1024**2\n    if verbose:\n        print('Memory usage after optimization: {:.2f} MB'.format(end_mem))\n        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return data","9617db96":"def one_hot_encoder(data, nan_as_category = True):\n    original_columns = list(data.columns)\n    categorical_columns = [col for col in data.columns \\\n                           if not pd.api.types.is_numeric_dtype(data[col].dtype)]\n    for c in categorical_columns:\n        if nan_as_category:\n            data[c].fillna('NaN', inplace = True)\n        values = list(data[c].unique())\n        for v in values:\n            data[str(c) + '_' + str(v)] = (data[c] == v).astype(np.uint8)\n    data.drop(categorical_columns, axis = 1, inplace = True)\n    return data, [c for c in data.columns if c not in original_columns]","ad03a13d":"file_path = '..\/input\/'","95da05c8":"def application_train_test(file_path = file_path, nan_as_category = True):\n    # Read data and merge\n    df_train = pd.read_csv(file_path + 'application_train.csv')\n    df_test = pd.read_csv(file_path + 'application_test.csv')\n    df = pd.concat([df_train, df_test], axis = 0, ignore_index = True)\n    del df_train, df_test\n    gc.collect()\n    \n    # Remove some rows with values not present in test set\n    df.drop(df[df['CODE_GENDER'] == 'XNA'].index, inplace = True)\n    df.drop(df[df['NAME_INCOME_TYPE'] == 'Maternity leave'].index, inplace = True)\n    df.drop(df[df['NAME_FAMILY_STATUS'] == 'Unknown'].index, inplace = True)\n    \n    # Remove some empty features\n    df.drop(['FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', \n            'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', \n            'FLAG_DOCUMENT_21'], axis = 1, inplace = True)\n    \n    # Replace some outliers\n    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace = True)\n    df.loc[df['OWN_CAR_AGE'] > 80, 'OWN_CAR_AGE'] = np.nan\n    df.loc[df['REGION_RATING_CLIENT_W_CITY'] < 0, 'REGION_RATING_CLIENT_W_CITY'] = np.nan\n    df.loc[df['AMT_INCOME_TOTAL'] > 1e8, 'AMT_INCOME_TOTAL'] = np.nan\n    df.loc[df['AMT_REQ_CREDIT_BUREAU_QRT'] > 10, 'AMT_REQ_CREDIT_BUREAU_QRT'] = np.nan\n    df.loc[df['OBS_30_CNT_SOCIAL_CIRCLE'] > 40, 'OBS_30_CNT_SOCIAL_CIRCLE'] = np.nan\n    \n    # Categorical features with Binary encode (0 or 1; two categories)\n    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n        df[bin_feature], _ = pd.factorize(df[bin_feature])\n        \n    # Categorical features with One-Hot encode\n    df, _ = one_hot_encoder(df, nan_as_category)\n    \n    # Some new features\n    df['app missing'] = df.isnull().sum(axis = 1).values\n    \n    df['app EXT_SOURCE mean'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis = 1)\n    df['app EXT_SOURCE std'] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis = 1)\n    df['app EXT_SOURCE prod'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n    df['app EXT_SOURCE_1 * EXT_SOURCE_2'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2']\n    df['app EXT_SOURCE_1 * EXT_SOURCE_3'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_3']\n    df['app EXT_SOURCE_2 * EXT_SOURCE_3'] = df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n    df['app EXT_SOURCE_1 * DAYS_EMPLOYED'] = df['EXT_SOURCE_1'] * df['DAYS_EMPLOYED']\n    df['app EXT_SOURCE_2 * DAYS_EMPLOYED'] = df['EXT_SOURCE_2'] * df['DAYS_EMPLOYED']\n    df['app EXT_SOURCE_3 * DAYS_EMPLOYED'] = df['EXT_SOURCE_3'] * df['DAYS_EMPLOYED']\n    df['app EXT_SOURCE_1 \/ DAYS_BIRTH'] = df['EXT_SOURCE_1'] \/ df['DAYS_BIRTH']\n    df['app EXT_SOURCE_2 \/ DAYS_BIRTH'] = df['EXT_SOURCE_2'] \/ df['DAYS_BIRTH']\n    df['app EXT_SOURCE_3 \/ DAYS_BIRTH'] = df['EXT_SOURCE_3'] \/ df['DAYS_BIRTH']\n    \n    df['app AMT_CREDIT - AMT_GOODS_PRICE'] = df['AMT_CREDIT'] - df['AMT_GOODS_PRICE']\n    df['app AMT_CREDIT \/ AMT_GOODS_PRICE'] = df['AMT_CREDIT'] \/ df['AMT_GOODS_PRICE']\n    df['app AMT_CREDIT \/ AMT_ANNUITY'] = df['AMT_CREDIT'] \/ df['AMT_ANNUITY']\n    df['app AMT_CREDIT \/ AMT_INCOME_TOTAL'] = df['AMT_CREDIT'] \/ df['AMT_INCOME_TOTAL']\n    \n    df['app AMT_INCOME_TOTAL \/ 12 - AMT_ANNUITY'] = df['AMT_INCOME_TOTAL'] \/ 12. - df['AMT_ANNUITY']\n    df['app AMT_INCOME_TOTAL \/ AMT_ANNUITY'] = df['AMT_INCOME_TOTAL'] \/ df['AMT_ANNUITY']\n    df['app AMT_INCOME_TOTAL - AMT_GOODS_PRICE'] = df['AMT_INCOME_TOTAL'] - df['AMT_GOODS_PRICE']\n    df['app AMT_INCOME_TOTAL \/ CNT_FAM_MEMBERS'] = df['AMT_INCOME_TOTAL'] \/ df['CNT_FAM_MEMBERS']\n    df['app AMT_INCOME_TOTAL \/ CNT_CHILDREN'] = df['AMT_INCOME_TOTAL'] \/ (1 + df['CNT_CHILDREN'])\n    \n    df['app most popular AMT_GOODS_PRICE'] = df['AMT_GOODS_PRICE'] \\\n                        .isin([225000, 450000, 675000, 900000]).map({True: 1, False: 0})\n    df['app popular AMT_GOODS_PRICE'] = df['AMT_GOODS_PRICE'] \\\n                        .isin([1125000, 1350000, 1575000, 1800000, 2250000]).map({True: 1, False: 0})\n    \n    df['app OWN_CAR_AGE \/ DAYS_BIRTH'] = df['OWN_CAR_AGE'] \/ df['DAYS_BIRTH']\n    df['app OWN_CAR_AGE \/ DAYS_EMPLOYED'] = df['OWN_CAR_AGE'] \/ df['DAYS_EMPLOYED']\n    \n    df['app DAYS_LAST_PHONE_CHANGE \/ DAYS_BIRTH'] = df['DAYS_LAST_PHONE_CHANGE'] \/ df['DAYS_BIRTH']\n    df['app DAYS_LAST_PHONE_CHANGE \/ DAYS_EMPLOYED'] = df['DAYS_LAST_PHONE_CHANGE'] \/ df['DAYS_EMPLOYED']\n    df['app DAYS_EMPLOYED - DAYS_BIRTH'] = df['DAYS_EMPLOYED'] - df['DAYS_BIRTH']\n    df['app DAYS_EMPLOYED \/ DAYS_BIRTH'] = df['DAYS_EMPLOYED'] \/ df['DAYS_BIRTH']\n    \n    df['app CNT_CHILDREN \/ CNT_FAM_MEMBERS'] = df['CNT_CHILDREN'] \/ df['CNT_FAM_MEMBERS']\n    \n    return reduce_mem_usage(df)","406f001c":"def bureau_and_balance(file_path = file_path, nan_as_category = True):\n    df_bureau_b = reduce_mem_usage(pd.read_csv(file_path + 'bureau_balance.csv'), verbose = False)\n    \n    # Some new features in bureau_balance set\n    tmp = df_bureau_b[['SK_ID_BUREAU', 'STATUS']].groupby('SK_ID_BUREAU')\n    tmp_last = tmp.last()\n    tmp_last.columns = ['First_status']\n    df_bureau_b = df_bureau_b.join(tmp_last, how = 'left', on = 'SK_ID_BUREAU')\n    tmp_first = tmp.first()\n    tmp_first.columns = ['Last_status']\n    df_bureau_b = df_bureau_b.join(tmp_first, how = 'left', on = 'SK_ID_BUREAU')\n    del tmp, tmp_first, tmp_last\n    gc.collect()\n    \n    tmp = df_bureau_b[['SK_ID_BUREAU', 'MONTHS_BALANCE']].groupby('SK_ID_BUREAU').last()\n    tmp = tmp.apply(abs)\n    tmp.columns = ['Month']\n    df_bureau_b = df_bureau_b.join(tmp, how = 'left', on = 'SK_ID_BUREAU')\n    del tmp\n    gc.collect()\n    \n    tmp = df_bureau_b.loc[df_bureau_b['STATUS'] == 'C', ['SK_ID_BUREAU', 'MONTHS_BALANCE']] \\\n                .groupby('SK_ID_BUREAU').last()\n    tmp = tmp.apply(abs)\n    tmp.columns = ['When_closed']\n    df_bureau_b = df_bureau_b.join(tmp, how = 'left', on = 'SK_ID_BUREAU')\n    del tmp\n    gc.collect()\n    \n    df_bureau_b['Month_closed_to_end'] = df_bureau_b['Month'] - df_bureau_b['When_closed']\n\n    for c in range(6):\n        tmp = df_bureau_b.loc[df_bureau_b['STATUS'] == str(c), ['SK_ID_BUREAU', 'MONTHS_BALANCE']] \\\n                         .groupby('SK_ID_BUREAU').count()\n        tmp.columns = ['DPD_' + str(c) + '_cnt']\n        df_bureau_b = df_bureau_b.join(tmp, how = 'left', on = 'SK_ID_BUREAU')\n        df_bureau_b['DPD_' + str(c) + ' \/ Month'] = df_bureau_b['DPD_' + str(c) + '_cnt'] \/ df_bureau_b['Month']\n        del tmp\n        gc.collect()\n    df_bureau_b['Non_zero_DPD_cnt'] = df_bureau_b[['DPD_1_cnt', 'DPD_2_cnt', 'DPD_3_cnt', 'DPD_4_cnt', 'DPD_5_cnt']].sum(axis = 1)\n    \n    df_bureau_b, bureau_b_cat = one_hot_encoder(df_bureau_b, nan_as_category)\n\n    # Bureau balance: Perform aggregations \n    aggregations = {}\n    for col in df_bureau_b.columns:\n        aggregations[col] = ['mean'] if col in bureau_b_cat else ['min', 'max', 'size']\n    df_bureau_b_agg = df_bureau_b.groupby('SK_ID_BUREAU').agg(aggregations)\n    df_bureau_b_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in df_bureau_b_agg.columns.tolist()])\n    del df_bureau_b\n    gc.collect()\n\n    df_bureau = reduce_mem_usage(pd.read_csv(file_path + 'bureau.csv'), verbose = False)\n                  \n    # Replace\\remove some outliers in bureau set\n    df_bureau.loc[df_bureau['AMT_ANNUITY'] > .8e8, 'AMT_ANNUITY'] = np.nan\n    df_bureau.loc[df_bureau['AMT_CREDIT_SUM'] > 3e8, 'AMT_CREDIT_SUM'] = np.nan\n    df_bureau.loc[df_bureau['AMT_CREDIT_SUM_DEBT'] > 1e8, 'AMT_CREDIT_SUM_DEBT'] = np.nan\n    df_bureau.loc[df_bureau['AMT_CREDIT_MAX_OVERDUE'] > .8e8, 'AMT_CREDIT_MAX_OVERDUE'] = np.nan\n    df_bureau.loc[df_bureau['DAYS_ENDDATE_FACT'] < -10000, 'DAYS_ENDDATE_FACT'] = np.nan\n    df_bureau.loc[(df_bureau['DAYS_CREDIT_UPDATE'] > 0) | (df_bureau['DAYS_CREDIT_UPDATE'] < -40000), 'DAYS_CREDIT_UPDATE'] = np.nan\n    df_bureau.loc[df_bureau['DAYS_CREDIT_ENDDATE'] < -10000, 'DAYS_CREDIT_ENDDATE'] = np.nan\n    \n    df_bureau.drop(df_bureau[df_bureau['DAYS_ENDDATE_FACT'] < df_bureau['DAYS_CREDIT']].index, inplace = True)\n    \n    # Some new features in bureau set\n    df_bureau['bureau AMT_CREDIT_SUM - AMT_CREDIT_SUM_DEBT'] = df_bureau['AMT_CREDIT_SUM'] - df_bureau['AMT_CREDIT_SUM_DEBT']\n    df_bureau['bureau AMT_CREDIT_SUM - AMT_CREDIT_SUM_LIMIT'] = df_bureau['AMT_CREDIT_SUM'] - df_bureau['AMT_CREDIT_SUM_LIMIT']\n    df_bureau['bureau AMT_CREDIT_SUM - AMT_CREDIT_SUM_OVERDUE'] = df_bureau['AMT_CREDIT_SUM'] - df_bureau['AMT_CREDIT_SUM_OVERDUE']\n\n    df_bureau['bureau DAYS_CREDIT - CREDIT_DAY_OVERDUE'] = df_bureau['DAYS_CREDIT'] - df_bureau['CREDIT_DAY_OVERDUE']\n    df_bureau['bureau DAYS_CREDIT - DAYS_CREDIT_ENDDATE'] = df_bureau['DAYS_CREDIT'] - df_bureau['DAYS_CREDIT_ENDDATE']\n    df_bureau['bureau DAYS_CREDIT - DAYS_ENDDATE_FACT'] = df_bureau['DAYS_CREDIT'] - df_bureau['DAYS_ENDDATE_FACT']\n    df_bureau['bureau DAYS_CREDIT_ENDDATE - DAYS_ENDDATE_FACT'] = df_bureau['DAYS_CREDIT_ENDDATE'] - df_bureau['DAYS_ENDDATE_FACT']\n    df_bureau['bureau DAYS_CREDIT_UPDATE - DAYS_CREDIT_ENDDATE'] = df_bureau['DAYS_CREDIT_UPDATE'] - df_bureau['DAYS_CREDIT_ENDDATE']\n    \n    # Categorical features with One-Hot encode\n    df_bureau, bureau_cat = one_hot_encoder(df_bureau, nan_as_category)\n    \n    # Bureau balance: merge with bureau.csv\n    df_bureau = df_bureau.join(df_bureau_b_agg, how = 'left', on = 'SK_ID_BUREAU')\n    df_bureau.drop('SK_ID_BUREAU', axis = 1, inplace = True)\n    del df_bureau_b_agg\n    gc.collect()\n    \n    # Bureau and bureau_balance aggregations for application set\n    categorical = bureau_cat + bureau_b_cat\n    aggregations = {}\n    for col in df_bureau.columns:\n        aggregations[col] = ['mean'] if col in categorical else ['min', 'max', 'size', 'mean', 'var', 'sum']\n    df_bureau_agg = df_bureau.groupby('SK_ID_CURR').agg(aggregations)\n    df_bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in df_bureau_agg.columns.tolist()])\n    \n    # Bureau: Active credits\n    active_agg = df_bureau[df_bureau['CREDIT_ACTIVE_Active'] == 1].groupby('SK_ID_CURR').agg(aggregations)\n    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n    df_bureau_agg = df_bureau_agg.join(active_agg, how = 'left')\n    del active_agg\n    gc.collect()\n    \n    # Bureau: Closed credits\n    closed_agg = df_bureau[df_bureau['CREDIT_ACTIVE_Closed'] == 1].groupby('SK_ID_CURR').agg(aggregations)\n    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n    df_bureau_agg = df_bureau_agg.join(closed_agg, how = 'left')\n    del closed_agg, df_bureau\n    gc.collect()\n    \n    return reduce_mem_usage(df_bureau_agg)","b5e38ba8":"def previous_application(file_path = file_path, nan_as_category = True):\n    df_prev = pd.read_csv(file_path + 'previous_application.csv')\n    \n    # Replace some outliers\n    df_prev.loc[df_prev['AMT_CREDIT'] > 6000000, 'AMT_CREDIT'] = np.nan\n    df_prev.loc[df_prev['SELLERPLACE_AREA'] > 3500000, 'SELLERPLACE_AREA'] = np.nan\n    df_prev[['DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', \n             'DAYS_LAST_DUE', 'DAYS_TERMINATION']].replace(365243, np.nan, inplace = True)\n    \n    # Some new features\n    df_prev['prev missing'] = df_prev.isnull().sum(axis = 1).values\n    df_prev['prev AMT_APPLICATION \/ AMT_CREDIT'] = df_prev['AMT_APPLICATION'] \/ df_prev['AMT_CREDIT']\n    df_prev['prev AMT_APPLICATION - AMT_CREDIT'] = df_prev['AMT_APPLICATION'] - df_prev['AMT_CREDIT']\n    df_prev['prev AMT_APPLICATION - AMT_GOODS_PRICE'] = df_prev['AMT_APPLICATION'] - df_prev['AMT_GOODS_PRICE']\n    df_prev['prev AMT_GOODS_PRICE - AMT_CREDIT'] = df_prev['AMT_GOODS_PRICE'] - df_prev['AMT_CREDIT']\n    df_prev['prev DAYS_FIRST_DRAWING - DAYS_FIRST_DUE'] = df_prev['DAYS_FIRST_DRAWING'] - df_prev['DAYS_FIRST_DUE']\n    df_prev['prev DAYS_TERMINATION less -500'] = (df_prev['DAYS_TERMINATION'] < -500).astype(int)\n    \n    # Categorical features with One-Hot encode\n    df_prev, categorical = one_hot_encoder(df_prev, nan_as_category)\n\n    # Aggregations for application set\n    aggregations = {}\n    for col in df_prev.columns:\n        aggregations[col] = ['mean'] if col in categorical else ['min', 'max', 'size', 'mean', 'var', 'sum']\n    df_prev_agg = df_prev.groupby('SK_ID_CURR').agg(aggregations)\n    df_prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in df_prev_agg.columns.tolist()])\n    \n    # Previous Applications: Approved Applications\n    approved_agg = df_prev[df_prev['NAME_CONTRACT_STATUS_Approved'] == 1].groupby('SK_ID_CURR').agg(aggregations)\n    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n    df_prev_agg = df_prev_agg.join(approved_agg, how = 'left')\n    del approved_agg\n    gc.collect()\n    \n    # Previous Applications: Refused Applications\n    refused_agg = df_prev[df_prev['NAME_CONTRACT_STATUS_Refused'] == 1].groupby('SK_ID_CURR').agg(aggregations)\n    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n    df_prev_agg = df_prev_agg.join(refused_agg, how = 'left')\n    del refused_agg, df_prev\n    gc.collect()\n    \n    return reduce_mem_usage(df_prev_agg)","8506f976":"def pos_cash(file_path = file_path, nan_as_category = True):\n    df_pos = pd.read_csv(file_path + 'POS_CASH_balance.csv')\n    \n    # Replace some outliers\n    df_pos.loc[df_pos['CNT_INSTALMENT_FUTURE'] > 60, 'CNT_INSTALMENT_FUTURE'] = np.nan\n    \n    # Some new features\n    df_pos['pos CNT_INSTALMENT more CNT_INSTALMENT_FUTURE'] = \\\n                    (df_pos['CNT_INSTALMENT'] > df_pos['CNT_INSTALMENT_FUTURE']).astype(int)\n    \n    # Categorical features with One-Hot encode\n    df_pos, categorical = one_hot_encoder(df_pos, nan_as_category)\n    \n    # Aggregations for application set\n    aggregations = {}\n    for col in df_pos.columns:\n        aggregations[col] = ['mean'] if col in categorical else ['min', 'max', 'size', 'mean', 'var', 'sum']\n    df_pos_agg = df_pos.groupby('SK_ID_CURR').agg(aggregations)\n    df_pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in df_pos_agg.columns.tolist()])\n\n    # Count POS lines\n    df_pos_agg['POS_COUNT'] = df_pos.groupby('SK_ID_CURR').size()\n    del df_pos\n    gc.collect()\n    \n    return reduce_mem_usage(df_pos_agg)","aed54f2a":"def installments_payments(file_path = file_path, nan_as_category = True):\n    df_ins = pd.read_csv(file_path + 'installments_payments.csv')\n    \n    # Replace some outliers\n    df_ins.loc[df_ins['NUM_INSTALMENT_VERSION'] > 70, 'NUM_INSTALMENT_VERSION'] = np.nan\n    df_ins.loc[df_ins['DAYS_ENTRY_PAYMENT'] < -4000, 'DAYS_ENTRY_PAYMENT'] = np.nan\n    \n    # Some new features\n    df_ins['ins DAYS_ENTRY_PAYMENT - DAYS_INSTALMENT'] = df_ins['DAYS_ENTRY_PAYMENT'] - df_ins['DAYS_INSTALMENT']\n    df_ins['ins NUM_INSTALMENT_NUMBER_100'] = (df_ins['NUM_INSTALMENT_NUMBER'] == 100).astype(int)\n    df_ins['ins DAYS_INSTALMENT more NUM_INSTALMENT_NUMBER'] = (df_ins['DAYS_INSTALMENT'] > df_ins['NUM_INSTALMENT_NUMBER'] * 50 \/ 3 - 11500 \/ 3).astype(int)\n    df_ins['ins AMT_INSTALMENT - AMT_PAYMENT'] = df_ins['AMT_INSTALMENT'] - df_ins['AMT_PAYMENT']\n    df_ins['ins AMT_PAYMENT \/ AMT_INSTALMENT'] = df_ins['AMT_PAYMENT'] \/ df_ins['AMT_INSTALMENT']\n    \n    # Categorical features with One-Hot encode\n    df_ins, categorical = one_hot_encoder(df_ins, nan_as_category)\n\n    # Aggregations for application set\n    aggregations = {}\n    for col in df_ins.columns:\n        aggregations[col] = ['mean'] if col in categorical else ['min', 'max', 'size', 'mean', 'var', 'sum']\n    df_ins_agg = df_ins.groupby('SK_ID_CURR').agg(aggregations)\n    df_ins_agg.columns = pd.Index(['INS_' + e[0] + \"_\" + e[1].upper() for e in df_ins_agg.columns.tolist()])\n    \n    # Count installments lines\n    df_ins_agg['INSTAL_COUNT'] = df_ins.groupby('SK_ID_CURR').size()\n    del df_ins\n    gc.collect()\n    \n    return reduce_mem_usage(df_ins_agg)","2d9ada6f":"def credit_card_balance(file_path = file_path, nan_as_category = True):\n    df_card = pd.read_csv(file_path + 'credit_card_balance.csv')\n    \n    # Replace some outliers\n    df_card.loc[df_card['AMT_PAYMENT_CURRENT'] > 4000000, 'AMT_PAYMENT_CURRENT'] = np.nan\n    df_card.loc[df_card['AMT_CREDIT_LIMIT_ACTUAL'] > 1000000, 'AMT_CREDIT_LIMIT_ACTUAL'] = np.nan\n\n    # Some new features\n    df_card['card missing'] = df_card.isnull().sum(axis = 1).values\n    df_card['card SK_DPD - MONTHS_BALANCE'] = df_card['SK_DPD'] - df_card['MONTHS_BALANCE']\n    df_card['card SK_DPD_DEF - MONTHS_BALANCE'] = df_card['SK_DPD_DEF'] - df_card['MONTHS_BALANCE']\n    df_card['card SK_DPD - SK_DPD_DEF'] = df_card['SK_DPD'] - df_card['SK_DPD_DEF']\n    \n    df_card['card AMT_TOTAL_RECEIVABLE - AMT_RECIVABLE'] = df_card['AMT_TOTAL_RECEIVABLE'] - df_card['AMT_RECIVABLE']\n    df_card['card AMT_TOTAL_RECEIVABLE - AMT_RECEIVABLE_PRINCIPAL'] = df_card['AMT_TOTAL_RECEIVABLE'] - df_card['AMT_RECEIVABLE_PRINCIPAL']\n    df_card['card AMT_RECIVABLE - AMT_RECEIVABLE_PRINCIPAL'] = df_card['AMT_RECIVABLE'] - df_card['AMT_RECEIVABLE_PRINCIPAL']\n\n    df_card['card AMT_BALANCE - AMT_RECIVABLE'] = df_card['AMT_BALANCE'] - df_card['AMT_RECIVABLE']\n    df_card['card AMT_BALANCE - AMT_RECEIVABLE_PRINCIPAL'] = df_card['AMT_BALANCE'] - df_card['AMT_RECEIVABLE_PRINCIPAL']\n    df_card['card AMT_BALANCE - AMT_TOTAL_RECEIVABLE'] = df_card['AMT_BALANCE'] - df_card['AMT_TOTAL_RECEIVABLE']\n\n    df_card['card AMT_DRAWINGS_CURRENT - AMT_DRAWINGS_ATM_CURRENT'] = df_card['AMT_DRAWINGS_CURRENT'] - df_card['AMT_DRAWINGS_ATM_CURRENT']\n    df_card['card AMT_DRAWINGS_CURRENT - AMT_DRAWINGS_OTHER_CURRENT'] = df_card['AMT_DRAWINGS_CURRENT'] - df_card['AMT_DRAWINGS_OTHER_CURRENT']\n    df_card['card AMT_DRAWINGS_CURRENT - AMT_DRAWINGS_POS_CURRENT'] = df_card['AMT_DRAWINGS_CURRENT'] - df_card['AMT_DRAWINGS_POS_CURRENT']\n    \n    # Categorical features with One-Hot encode\n    df_card, categorical = one_hot_encoder(df_card, nan_as_category)\n    \n    # Aggregations for application set\n    aggregations = {}\n    for col in df_card.columns:\n        aggregations[col] = ['mean'] if col in categorical else ['min', 'max', 'size', 'mean', 'var', 'sum']\n    df_card_agg = df_card.groupby('SK_ID_CURR').agg(aggregations)\n    df_card_agg.columns = pd.Index(['CARD_' + e[0] + \"_\" + e[1].upper() for e in df_card_agg.columns.tolist()])\n\n    # Count credit card lines\n    df_card_agg['CARD_COUNT'] = df_card.groupby('SK_ID_CURR').size()\n    del df_card\n    gc.collect()\n    \n    return reduce_mem_usage(df_card_agg)","7a4f12a5":"def aggregate(file_path = file_path):\n    warnings.simplefilter(action = 'ignore')\n    \n    print('-' * 20)\n    print('1: application train & test (', time.ctime(), ')')\n    print('-' * 20)\n    df = application_train_test(file_path)\n    print('     DF shape:', df.shape)\n    \n    print('-' * 20)\n    print('2: bureau & balance (', time.ctime(), ')')\n    print('-' * 20)\n    bureau = bureau_and_balance(file_path)\n    df = df.join(bureau, how = 'left', on = 'SK_ID_CURR')\n    print('     DF shape:', df.shape)\n    del bureau\n    gc.collect()\n    \n    print('-' * 20)\n    print('3: previous_application (', time.ctime(), ')')\n    print('-' * 20)\n    prev = previous_application(file_path)\n    df = df.join(prev, how = 'left', on = 'SK_ID_CURR')\n    print('     DF shape:', df.shape)\n    del prev\n    gc.collect()\n    \n    print('-' * 20)\n    print('4: POS_CASH_balance (', time.ctime(), ')')\n    print('-' * 20)\n    pos = pos_cash(file_path)\n    df = df.join(pos, how = 'left', on = 'SK_ID_CURR')\n    print('     DF shape:', df.shape)\n    del pos\n    gc.collect()\n    \n    print('-' * 20)\n    print('5: installments_payments (', time.ctime(), ')')\n    print('-' * 20)\n    ins = installments_payments(file_path)\n    df = df.join(ins, how = 'left', on = 'SK_ID_CURR')\n    print('     DF shape:', df.shape)\n    del ins\n    gc.collect()\n    \n    print('-' * 20)\n    print('6: credit_card_balance (', time.ctime(), ')')\n    print('-' * 20)\n    cc = credit_card_balance(file_path)\n    df = df.join(cc, how = 'left', on = 'SK_ID_CURR')\n    print('     DF shape:', df.shape)\n    del cc\n    gc.collect()\n    \n    print('-' * 20)\n    print('7: final dataset (', time.ctime(), ')')\n    print('-' * 20)\n    return reduce_mem_usage(df)","a0811ca5":"# Kaggle has not ehough memory to clean this dataset\n# Aggregated dataset has 3411 features\n\n#df = aggregate()","515c09e9":"def corr_feature_with_target(feature, target):\n    c0 = feature[target == 0].dropna()\n    c1 = feature[target == 1].dropna()\n        \n    if set(feature.unique()) == set([0, 1]):\n        diff = abs(c0.mean(axis = 0) - c1.mean(axis = 0))\n    else:\n        diff = abs(c0.median(axis = 0) - c1.median(axis = 0))\n        \n    p = ranksums(c0, c1)[1] if ((len(c0) >= 20) & (len(c1) >= 20)) else 2\n        \n    return [diff, p]","66b7048f":"def clean_data(data):\n    warnings.simplefilter(action = 'ignore')\n    \n    # Removing empty features\n    nun = data.nunique()\n    empty = list(nun[nun <= 1].index)\n    \n    data.drop(empty, axis = 1, inplace = True)\n    print('After removing empty features there are {0:d} features'.format(data.shape[1]))\n    \n    # Removing features with the same distribution on 0 and 1 classes\n    corr = pd.DataFrame(index = ['diff', 'p'])\n    ind = data[data['TARGET'].notnull()].index\n    \n    for c in data.columns.drop('TARGET'):\n        corr[c] = corr_feature_with_target(data.loc[ind, c], data.loc[ind, 'TARGET'])\n\n    corr = corr.T\n    corr['diff_norm'] = abs(corr['diff'] \/ data.mean(axis = 0))\n    \n    to_del_1 = corr[((corr['diff'] == 0) & (corr['p'] > .05))].index\n    to_del_2 = corr[((corr['diff_norm'] < .5) & (corr['p'] > .05))].drop(to_del_1).index\n    to_del = list(to_del_1) + list(to_del_2)\n    if 'SK_ID_CURR' in to_del:\n        to_del.remove('SK_ID_CURR')\n        \n    data.drop(to_del, axis = 1, inplace = True)\n    print('After removing features with the same distribution on 0 and 1 classes there are {0:d} features'.format(data.shape[1]))\n    \n    # Removing features with not the same distribution on train and test datasets\n    corr_test = pd.DataFrame(index = ['diff', 'p'])\n    target = data['TARGET'].notnull().astype(int)\n    \n    for c in data.columns.drop('TARGET'):\n        corr_test[c] = corr_feature_with_target(data[c], target)\n\n    corr_test = corr_test.T\n    corr_test['diff_norm'] = abs(corr_test['diff'] \/ data.mean(axis = 0))\n    \n    bad_features = corr_test[((corr_test['p'] < .05) & (corr_test['diff_norm'] > 1))].index\n    bad_features = corr.loc[bad_features][corr['diff_norm'] == 0].index\n    \n    data.drop(bad_features, axis = 1, inplace = True)\n    print('After removing features with not the same distribution on train and test datasets there are {0:d} features'.format(data.shape[1]))\n    \n    del corr, corr_test\n    gc.collect()\n    \n    # Removing features not interesting for classifier\n    clf = LGBMClassifier(random_state = 0)\n    train_index = data[data['TARGET'].notnull()].index\n    train_columns = data.drop('TARGET', axis = 1).columns\n\n    score = 1\n    new_columns = []\n    while score > .7:\n        train_columns = train_columns.drop(new_columns)\n        clf.fit(data.loc[train_index, train_columns], data.loc[train_index, 'TARGET'])\n        f_imp = pd.Series(clf.feature_importances_, index = train_columns)\n        score = roc_auc_score(data.loc[train_index, 'TARGET'], \n                              clf.predict_proba(data.loc[train_index, train_columns])[:, 1])\n        new_columns = f_imp[f_imp > 0].index\n\n    data.drop(train_columns, axis = 1, inplace = True)\n    print('After removing features not interesting for classifier there are {0:d} features'.format(data.shape[1]))\n\n    return data","035df284":"# Kaggle has not ehough memory to run this code - more than 14 Gb RAM\n\n# Dataset for cleaning has 3411 features\n# After removing empty features there are 3289 features\n# After removing features with the same distribution on 0 and 1 classes there are 2171 features\n# After removing features with not the same distribution on train and test datasets there are 2115 features\n# After removing features not interesting for classifier there are 1505 features\n\n#df = clean_data(df)","f310a6ee":"def cv_scores(df, num_folds, params, stratified = False, verbose = -1, \n              save_train_prediction = False, train_prediction_file_name = 'train_prediction.csv',\n              save_test_prediction = True, test_prediction_file_name = 'test_prediction.csv'):\n    warnings.simplefilter('ignore')\n    \n    clf = LGBMClassifier(**params)\n\n    # Divide in training\/validation and test data\n    train_df = df[df['TARGET'].notnull()]\n    test_df = df[df['TARGET'].isnull()]\n    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n\n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = 1001)\n    else:\n        folds = KFold(n_splits = num_folds, shuffle = True, random_state = 1001)\n        \n    # Create arrays and dataframes to store results\n    train_pred = np.zeros(train_df.shape[0])\n    train_pred_proba = np.zeros(train_df.shape[0])\n\n    test_pred = np.zeros(train_df.shape[0])\n    test_pred_proba = np.zeros(train_df.shape[0])\n    \n    prediction = np.zeros(test_df.shape[0])\n    \n    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n    \n    df_feature_importance = pd.DataFrame(index = feats)\n    \n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n        print('Fold', n_fold, 'started at', time.ctime())\n        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n\n        clf.fit(train_x, train_y, \n                eval_set = [(train_x, train_y), (valid_x, valid_y)], eval_metric = 'auc', \n                verbose = verbose, early_stopping_rounds = 200)\n\n        train_pred[train_idx] = clf.predict(train_x, num_iteration = clf.best_iteration_)\n        train_pred_proba[train_idx] = clf.predict_proba(train_x, num_iteration = clf.best_iteration_)[:, 1]\n        test_pred[valid_idx] = clf.predict(valid_x, num_iteration = clf.best_iteration_)\n        test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration = clf.best_iteration_)[:, 1]\n        \n        prediction += \\\n                clf.predict_proba(test_df[feats], num_iteration = clf.best_iteration_)[:, 1] \/ folds.n_splits\n\n        df_feature_importance[n_fold] = pd.Series(clf.feature_importances_, index = feats)\n        \n        print('Fold %2d AUC : %.6f' % (n_fold, roc_auc_score(valid_y, test_pred_proba[valid_idx])))\n        del train_x, train_y, valid_x, valid_y\n        gc.collect()\n\n    roc_auc_train = roc_auc_score(train_df['TARGET'], train_pred_proba)\n    precision_train = precision_score(train_df['TARGET'], train_pred, average = None)\n    recall_train = recall_score(train_df['TARGET'], train_pred, average = None)\n    \n    roc_auc_test = roc_auc_score(train_df['TARGET'], test_pred_proba)\n    precision_test = precision_score(train_df['TARGET'], test_pred, average = None)\n    recall_test = recall_score(train_df['TARGET'], test_pred, average = None)\n\n    print('Full AUC score %.6f' % roc_auc_test)\n    \n    df_feature_importance.fillna(0, inplace = True)\n    df_feature_importance['mean'] = df_feature_importance.mean(axis = 1)\n    \n    # Write prediction files\n    if save_train_prediction:\n        df_prediction = train_df[['SK_ID_CURR', 'TARGET']]\n        df_prediction['Prediction'] = test_pred_proba\n        df_prediction.to_csv(train_prediction_file_name, index = False)\n        del df_prediction\n        gc.collect()\n\n    if save_test_prediction:\n        df_prediction = test_df[['SK_ID_CURR']]\n        df_prediction['TARGET'] = prediction\n        df_prediction.to_csv(test_prediction_file_name, index = False)\n        del df_prediction\n        gc.collect()\n    \n    return df_feature_importance, \\\n           [roc_auc_train, roc_auc_test,\n            precision_train[0], precision_test[0], precision_train[1], precision_test[1],\n            recall_train[0], recall_test[0], recall_train[1], recall_test[1], 0]","b7bb35ac":"def display_folds_importances(feature_importance_df_, n_folds = 5):\n    n_columns = 3\n    n_rows = (n_folds + 1) \/\/ n_columns\n    _, axes = plt.subplots(n_rows, n_columns, figsize=(8 * n_columns, 8 * n_rows))\n    for i in range(n_folds):\n        sns.barplot(x = i, y = 'index', data = feature_importance_df_.reset_index().sort_values(i, ascending = False).head(20), \n                    ax = axes[i \/\/ n_columns, i % n_columns])\n    sns.barplot(x = 'mean', y = 'index', data = feature_importance_df_.reset_index().sort_values('mean', ascending = False).head(20), \n                    ax = axes[n_rows - 1, n_columns - 1])\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.show()","4faa72f7":"scores_index = [\n    'roc_auc_train', 'roc_auc_test', \n    'precision_train_0', 'precision_test_0', \n    'precision_train_1', 'precision_test_1', \n    'recall_train_0', 'recall_test_0', \n    'recall_train_1', 'recall_test_1', \n    'LB'\n]\n\nscores = pd.DataFrame(index = scores_index)","f519140d":"# Parameters from Tilii kernel: https:\/\/www.kaggle.com\/tilii7\/olivier-lightgbm-parameters-by-bayesian-opt\/code\nlgbm_params = {\n            'nthread': 4,\n            'n_estimators': 10000,\n            'learning_rate': .02,\n            'num_leaves': 34,\n            'colsample_bytree': .9497036,\n            'subsample': .8715623,\n            'max_depth': 8,\n            'reg_alpha': .041545473,\n            'reg_lambda': .0735294,\n            'min_split_gain': .0222415,\n            'min_child_weight': 39.3259775,\n            'silent': -1,\n            'verbose': -1\n}","eb59c400":"#feature_importance, scor = cv_scores(df, 5, lgbm_params, test_prediction_file_name = 'prediction_0.csv')","e3b43f3e":"#step = 'Tilii`s Bayesian optimization'\n#scores[step] = scor\n#scores.loc['LB', step] = .797\n#scores.T","dd063463":"#display_folds_importances(feature_importance)","c232e7ee":"#feature_importance[feature_importance['mean'] == 0].shape","68f32c83":"#feature_importance.sort_values('mean', ascending = False).head(20)","7df362cd":"def lgbm_evaluate(**params):\n    warnings.simplefilter('ignore')\n    \n    params['num_leaves'] = int(params['num_leaves'])\n    params['max_depth'] = int(params['max_depth'])\n        \n    clf = LGBMClassifier(**params, n_estimators = 10000, nthread = 4)\n\n    train_df = df[df['TARGET'].notnull()]\n    test_df = df[df['TARGET'].isnull()]\n\n    folds = KFold(n_splits = 2, shuffle = True, random_state = 1001)\n        \n    test_pred_proba = np.zeros(train_df.shape[0])\n    \n    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n    \n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n\n        clf.fit(train_x, train_y, \n                eval_set = [(train_x, train_y), (valid_x, valid_y)], eval_metric = 'auc', \n                verbose = False, early_stopping_rounds = 100)\n\n        test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration = clf.best_iteration_)[:, 1]\n        \n        del train_x, train_y, valid_x, valid_y\n        gc.collect()\n\n    return roc_auc_score(train_df['TARGET'], test_pred_proba)","c6dc3102":"params = {'colsample_bytree': (0.8, 1),\n          'learning_rate': (.01, .02), \n          'num_leaves': (33, 35), \n          'subsample': (0.8, 1), \n          'max_depth': (7, 9), \n          'reg_alpha': (.03, .05), \n          'reg_lambda': (.06, .08), \n          'min_split_gain': (.01, .03),\n          'min_child_weight': (38, 40)}\n#bo = BayesianOptimization(lgbm_evaluate, params)\n#bo.maximize(init_points = 5, n_iter = 5)","53847bd9":"#best_params = bo.res['max']['max_params']\n#best_params['num_leaves'] = int(best_params['num_leaves'])\n#best_params['max_depth'] = int(best_params['max_depth'])\n\n#best_params","262083f3":"#bo.res['max']['max_val']","e11617d3":"#feature_importance, scor = cv_scores(df, 5, best_params, test_prediction_file_name = 'prediction_1.csv')","d4ed65a6":"#step = 'Bayesian optimization for new set'\n#scores[step] = scor\n#scores.loc['LB', step] = .797\n#scores.T","58860a8a":"#display_folds_importances(feature_importance)","6a2d5e14":"#feature_importance[feature_importance['mean'] == 0].shape","1e4ee38e":"#feature_importance.sort_values('mean', ascending = False).head(20)","3d9e0a4e":"## Aggregating datasets","afbc3de1":"__Warning!__ This kernel cannot run on Kaggle: not enough memory. But the code works fine and quickly on the local computer with the same amount of memory.","e1e29ca4":"## Optimization LGBM parameters","ef79540c":"### Aggregating functions","6401f13f":"### Optimization and visualisation functions","cc8b8d3d":"## Cleaning dataset","06518186":"Based on kernels: \n\n- https:\/\/www.kaggle.com\/jsaguiar\/updated-0-792-lb-lightgbm-with-simple-features\n\n- https:\/\/www.kaggle.com\/poohtls\/fork-of-fork-lightgbm-with-simple-features","5b69fe2c":"### First scores with parameters from Tilii kernel","0a17f343":"### New Bayesian Optimization","43cfc64d":"# Home Credit Default Risk 2018","e21a33ff":"### Service functions","4d67f9ac":"### Table for scores"}}