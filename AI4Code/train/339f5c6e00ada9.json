{"cell_type":{"25f7fa62":"code","af9429e8":"code","66d46f35":"code","51828762":"code","efc16097":"code","44560dfd":"code","6f3f109f":"code","c91b1e14":"code","c3b3f7e8":"code","57fd2039":"code","59e5b9a6":"code","e883a5da":"code","8dafa941":"code","03a23024":"code","d10cb2b3":"code","3c55bf72":"code","36ccc97d":"code","590db886":"code","34a60337":"code","22106dbd":"code","1ede4d0c":"code","3e5b088b":"code","dd19b7af":"code","0829605b":"code","056684a1":"code","ff7e845a":"code","e8a8469c":"code","c1e7d324":"code","5a4c8618":"code","f0043316":"code","9434eed7":"code","af0ddc73":"code","d26c591f":"code","d1a10cd3":"code","53cd3288":"code","51a7a03a":"code","cdba7302":"code","56230a5d":"code","9c5cc7db":"code","fc06af93":"code","557581a9":"code","abee8de7":"code","59b44815":"code","50e368b5":"code","cbdb9332":"code","829e84bc":"code","b102f4e4":"code","c175b12b":"code","790ce9f5":"code","43c7ca9d":"code","d4247fb7":"code","96aa3090":"code","3d6d39b4":"code","f5ad249d":"code","23c1dc2d":"code","18c60682":"code","a3b87c95":"code","3069c8ac":"code","94caa738":"code","88ef6289":"markdown","83a3e0e7":"markdown","486fd462":"markdown","d52d7632":"markdown","0197b045":"markdown","8d06c5b7":"markdown","339c67ec":"markdown","9aa5bd85":"markdown","e3c05f3c":"markdown","76af410a":"markdown","af67c62e":"markdown","241e1bc4":"markdown","703d8c84":"markdown","76e1c95a":"markdown","3a5b1a30":"markdown","6e7c096c":"markdown","8ef7bece":"markdown","1ae7bb9d":"markdown","d12e548f":"markdown","28695d7f":"markdown","a8e1e971":"markdown","6c6f479b":"markdown","462acaae":"markdown","ed98d34b":"markdown","f641beb1":"markdown","4f72fbfa":"markdown","8afdb043":"markdown","0af42328":"markdown","34389bd5":"markdown","12022e96":"markdown","4c938dcd":"markdown","dd4a10c5":"markdown","ade68ff0":"markdown","cd09f993":"markdown","72c4e190":"markdown","63e6d9fb":"markdown","562f0c21":"markdown","8fedb056":"markdown","531d2bff":"markdown","60147341":"markdown"},"source":{"25f7fa62":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import datasets\nfrom sklearn.naive_bayes import GaussianNB\n#model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\n#preprocess.\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder,OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","af9429e8":"df_train= pd.read_csv('\/kaggle\/input\/voicegender\/voice.csv')\ndf_train.head()","66d46f35":"df_train.shape","51828762":"df_train.columns","efc16097":"df_train.isnull().sum()","44560dfd":"df_train.describe()","6f3f109f":"def check_outliers(col):\n    q1,q3=df[col].quantile([0.25,0.75])\n    iqr=q3-q1\n    rang=1.5*iqr\n    return(q1-rang,q3+rang)","c91b1e14":"def plot(col):\n    fig,axes=plt.subplots(1,2)\n    sns.boxplot(data=df,x=col,ax=axes[0])\n    sns.distplot(a=df[col],ax=axes[1],color='#ff4125')\n    fig.set_size_inches(15,5)\n    lower,upper = check_outliers(col)\n    l=[df[col] for i in df[col] if i>lower and i<upper] \n    print(\"Number of data points remaining if outliers removed : \",len(l))","c3b3f7e8":"df=df_train","57fd2039":"del df_train","59e5b9a6":"df.columns","e883a5da":"plot('meanfreq')","8dafa941":"plot('sd')","03a23024":"plot('median')","d10cb2b3":"plot('Q25')","3c55bf72":"plot('Q75')","36ccc97d":"plot('skew')","590db886":"plot('kurt')","34a60337":"plot('sp.ent')","22106dbd":"plot('sfm')","1ede4d0c":"plot('meanfun')","3e5b088b":"sns.countplot(data=df,x='label');","dd19b7af":"df['label']=df['label'].replace({'male':1,'female':0})","0829605b":"df.head()","056684a1":"# correlation heatmap \ncor_mat= df[:].corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)","ff7e845a":"def plot_against_target(feature):\n    sns.factorplot(data=df,y=feature,x='label',kind='box')\n    fig=plt.gcf()\n    fig.set_size_inches(7,7)","e8a8469c":"plot_against_target('meanfreq')","c1e7d324":"plot_against_target('sd')","5a4c8618":"plot_against_target('median')","f0043316":"plot_against_target('Q25')","9434eed7":"plot_against_target('IQR')","af0ddc73":"plot_against_target('sp.ent')","d26c591f":"plot_against_target('meanfun')  ","d1a10cd3":"g = sns.PairGrid(df[['meanfreq','sd','median','Q25','IQR','sp.ent','sfm','meanfun','label']], hue = \"label\")\ng = g.map(plt.scatter).add_legend()","53cd3288":"for col in df.columns:\n    l,u=check_outliers(col)\n    df=df[(df[col]>l)&(df[col]<u)]","51a7a03a":"df.shape","cdba7302":"temp_df=df.copy()\ntemp_df.drop(['skew','kurt','mindom','maxdom','centroid'],axis=1,inplace=True)","56230a5d":"temp_df.head()","9c5cc7db":"## skewness with pearson coefficient\ntemp_df['pear_skew']=temp_df['meanfreq']-temp_df['mode']\ntemp_df['pear_skew']=temp_df['pear_skew']\/temp_df['sd']\ntemp_df.head(10)","fc06af93":"sns.boxplot(data=temp_df,y='pear_skew',x='label');","557581a9":"temp_df['meanfreq']=temp_df['meanfreq'].apply(lambda x:x*2)\ntemp_df['median']=temp_df['meanfreq']+temp_df['mode']\ntemp_df['median']=temp_df['median'].apply(lambda x:x\/3)","abee8de7":"sns.boxplot(data=temp_df,y='median',x='label');","59b44815":"scaler=StandardScaler()\nscaled_df=scaler.fit_transform(temp_df.drop('label',axis=1))\nX=scaled_df\nY=df['label'].values","50e368b5":"x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.20,random_state=42)","cbdb9332":"clf_lr=LogisticRegression()\nclf_lr.fit(x_train,y_train)\npred=clf_lr.predict(x_test)\nprint(accuracy_score(pred,y_test))","829e84bc":"clf_knn=KNeighborsClassifier()\nclf_knn.fit(x_train,y_train)\npred=clf_knn.predict(x_test)\nprint(accuracy_score(pred,y_test))","b102f4e4":"clf_svm=SVC()\nclf_svm.fit(x_train,y_train)\npred=clf_svm.predict(x_test)\nprint(accuracy_score(pred,y_test))","c175b12b":"clf_dt=DecisionTreeClassifier()\nclf_dt.fit(x_train,y_train)\npred=clf_dt.predict(x_test)\nprint(accuracy_score(pred,y_test))","790ce9f5":"clf_rf=RandomForestClassifier()\nclf_rf.fit(x_train,y_train)\npred=clf_rf.predict(x_test)\nprint(accuracy_score(pred,y_test))","43c7ca9d":"clf_gb=GradientBoostingClassifier()\nclf_gb.fit(x_train,y_train)\npred=clf_gb.predict(x_test)\nprint(accuracy_score(pred,y_test))","d4247fb7":"models=[LogisticRegression(),LinearSVC(),SVC(kernel='rbf'),KNeighborsClassifier(),RandomForestClassifier(),\n        DecisionTreeClassifier(),GradientBoostingClassifier(),GaussianNB()]\nmodel_names=['LogisticRegression','LinearSVM','rbfSVM','KNearestNeighbors','RandomForestClassifier','DecisionTree',\n             'GradientBoostingClassifier','GaussianNB']\n\nacc=[]\nd={}\n\nfor model in range(len(models)):\n    clf=models[model]\n    clf.fit(x_train,y_train)\n    pred=clf.predict(x_test)\n    acc.append(accuracy_score(pred,y_test))\n     \nd={'Modelling Algo':model_names,'Accuracy':acc}","96aa3090":"acc=pd.DataFrame(d)\nacc","3d6d39b4":"sns.barplot(y='Modelling Algo',x='Accuracy',data=acc);","f5ad249d":"params_dict={'C':[0.001,0.01,0.1,1,10,100],'gamma':[0.001,0.01,0.1,1,10,100],'kernel':['linear','rbf']}\nclf=GridSearchCV(estimator=SVC(),param_grid=params_dict,scoring='accuracy',cv=10)\nclf.fit(x_train,y_train)","23c1dc2d":"clf.best_params_","18c60682":"# best score\nclf.best_score_","a3b87c95":"# train on these paramaters\nclf_svm=SVC(C=100,gamma=0.01,kernel='rbf')\nclf_svm.fit(x_train,y_train)\npred=clf_svm.predict(x_test)\nprint(accuracy_score(pred,y_test))","3069c8ac":"print(accuracy_score(clf_svm.predict(x_test),y_test))","94caa738":"print(precision_score(clf_svm.predict(x_test),y_test))","88ef6289":"### from data description we know that :\n\nmeanfreq: mean frequency (in kHz)\n\nsd: standard deviation of frequency\n\nmedian: median frequency (in kHz)\n\nQ25: first quantile (in kHz)\n\nQ75: third quantile (in kHz)\n\nIQR: interquantile range (in kHz)\n\nskew: skewness (see note in specprop description)\n\nkurt: kurtosis (see note in specprop description)\n\nsp.ent: spectral entropy\n\nsfm: spectral flatness\n\nmode: mode frequency\n\ncentroid: frequency centroid (see specprop)\n\npeakf: peak frequency (frequency with highest energy)\n\nmeanfun: average of fundamental frequency measured across acoustic signal\n\nminfun: minimum fundamental frequency measured across acoustic signal\n\nmaxfun: maximum fundamental frequency measured across acoustic signal\n\nmeandom: average of dominant frequency measured across acoustic signal\n\nmindom: minimum of dominant frequency measured across acoustic signal\n\nmaxdom: maximum of dominant frequency measured across acoustic signal\n\ndfrange: range of dominant frequency measured across acoustic signal\n\nmodindx: modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range\n\nlabel: male or female","83a3e0e7":"## Tuning SVM with grid search cross validation","486fd462":"## There's not a remarkable difference between the original and tuned model, but it's a good practice.","d52d7632":"### performing EDA with plots.","0197b045":"### show best parameter values to train on","8d06c5b7":"## Today I'll work on classifying genders by their voice using classical ML and some data analysis and visualizations","339c67ec":"### let's check if we have outliers in our data by calculating the 1.5 IQR range","9aa5bd85":"### SVM classifier","e3c05f3c":"### plot features against target label to compare distributions.","76af410a":"1. from box plot: we have some outliers according to 1.5 IQR rule\n2. from distplot: the distribution is not perfect;y normal, we have very little -ve skewness >> we can normalize that.\n3. more outliers are on the left of the distribution.","af67c62e":"### Importing libraries ","241e1bc4":"### DECISION TREE classifier","703d8c84":"### make class labels 0,1 instead of male, female","76e1c95a":"### since all features are numeric, I'll use histogram and box plot ","3a5b1a30":"### gradient boosting ","6e7c096c":"so according to previous plots, we'll delete \"skew,kurt,mindom,maxdom,centroid\"","8ef7bece":"### KNN","1ae7bb9d":"### so the number of observations reduced when we deleted the outliers.","d12e548f":"change median column to be 1\/3(2mean+mode)","28695d7f":"1. Correlation between data features","a8e1e971":"### Reading our dataset.","6c6f479b":"## Feature engineering, My fav part. ^^","462acaae":"### let's remove the outliers","ed98d34b":"## Univariate analysis","f641beb1":"## ML","4f72fbfa":"### let's plot a pairplot grid with scatter plots to compare features","8afdb043":"### no missing values","0af42328":"### females have higher mean frequency than males.","34389bd5":"we know the strong relation between the target label and IQR and this is ","12022e96":"I'll comment on this section in the feature engineering","4c938dcd":"## compare the results","dd4a10c5":"### luckily our data is balanced \"equal number of classes\"","ade68ff0":"### scaling","cd09f993":"1. IQR is the most correlated feature with the target label\n2. median and centroid () , mean freq and centroid >> delete centroid\n3. other obseravtions, there are weak correlations and high ones, i'll delete some columns in feature engineering.","72c4e190":"### logistic regression","63e6d9fb":"## to me this notebook is about visuals more than modelling, but I had fun doing it with classic ML and it gave good results without the need for ANNs.","562f0c21":"### creating new features","8fedb056":"### random forest classifer","531d2bff":"### let's check missing values","60147341":"##  Bivariate Analysis"}}