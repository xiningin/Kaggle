{"cell_type":{"0b0a3ed1":"code","068168de":"code","8be0049d":"code","df6c8591":"code","62157eac":"code","446c5acd":"code","146f235c":"code","7767d8ef":"code","6e686943":"code","8ce3137e":"code","9a0aa5f6":"code","b0fa5c12":"code","9d436404":"code","5ca9ef74":"code","bf602436":"code","b6202e74":"code","a4be98fb":"code","638653d7":"code","c13c8a42":"code","20f6c38f":"code","46c67525":"markdown","70821d91":"markdown","4a938120":"markdown","4a53f5d4":"markdown","24584695":"markdown","75372562":"markdown","5e5ac2bf":"markdown","57893691":"markdown","45fc75d2":"markdown","5ca817e0":"markdown","36cf1d07":"markdown","00dbeb0a":"markdown","cbb846e9":"markdown","bf9eecdd":"markdown","cd97065a":"markdown","a451aed5":"markdown","addc7ede":"markdown","912ab022":"markdown","7277d61e":"markdown","ce0008c0":"markdown","aca8052c":"markdown","e8ada31d":"markdown","5846b003":"markdown","ca385463":"markdown","8ed939b2":"markdown","74f786fa":"markdown"},"source":{"0b0a3ed1":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport imblearn\nfrom sklearn.metrics import accuracy_score,classification_report,plot_confusion_matrix,plot_roc_curve","068168de":"df=pd.read_csv('..\/input\/swarm-behaviour-classification\/Swarm_Behaviour.csv')","8be0049d":"df.head()","df6c8591":"df.info()","62157eac":"df.isna().sum()","446c5acd":"df.describe()","146f235c":"\nsns.countplot(df[\"Swarm_Behaviour\"])","7767d8ef":"# class count\nclass_count_0, class_count_1 = df['Swarm_Behaviour'].value_counts()\n\n# Separate class\nclass_0 = df[df['Swarm_Behaviour'] == 0]\nclass_1 = df[df['Swarm_Behaviour'] == 1]\n\n# print the shape of the class\nprint('class 0:', class_0.shape)\nprint('class 1:', class_1.shape)","6e686943":"class_0_under = class_0.sample(class_count_1)\n\ntest_under = pd.concat([class_0_under, class_1], axis=0)\n\nprint(\"total class of 1 and 0:\",test_under['Swarm_Behaviour'].value_counts())# plot the count after under-sampeling\ntest_under['Swarm_Behaviour'].value_counts().plot(kind='bar', title='count (target)')","8ce3137e":"class_1_over = class_1.sample(class_count_0, replace=True)\n\ntest_over = pd.concat([class_1_over, class_0], axis=0)\n\nprint(\"total class of 1 and 0:\",test_under['Swarm_Behaviour'].value_counts())\n# plot the count after under-sampeling\ntest_over['Swarm_Behaviour'].value_counts().plot(kind='bar', title='count (target)')\nprint(\"total class of 1 and 0 after under-sampeling:\",test_over['Swarm_Behaviour'].value_counts())","9a0aa5f6":"from sklearn.model_selection import train_test_split\nX = df.drop(columns = 'Swarm_Behaviour', axis=1)\nY = df['Swarm_Behaviour']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, stratify=Y, random_state=2)","b0fa5c12":"# import library\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter\n\nRandomUnderSampler = RandomUnderSampler(random_state=42, replacement=True)# fit predictor and target variable\nx_RandomUnderSampler, y_RandomUnderSampler = RandomUnderSampler.fit_resample(X, Y)\n\nprint('original dataset shape:', Counter(X))\nprint('Resample dataset shape', Counter(y_RandomUnderSampler))","9d436404":"# import library\nfrom imblearn.over_sampling import RandomOverSampler\n\nRandomOverSampler = RandomOverSampler(random_state=42)\n\n# fit predictor and target variable \nx_RandomOverSampler, y_RandomOverSampler = RandomOverSampler.fit_resample(X, Y)\n\nprint('Original dataset shape', Counter(Y))\nprint('Resample dataset shape', Counter(y_RandomOverSampler))","5ca9ef74":"# import library\nfrom imblearn.under_sampling import TomekLinks\n\ntl = TomekLinks()\n# fit predictor and target variable\nx_tl, y_tl = tl.fit_resample(X, Y)\n\nprint('Original dataset shape', Counter(Y))\nprint('Resample dataset shape', Counter(y_tl))","bf602436":"# import library\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\n\n# fit predictor and target variable\nx_smote, y_smote = smote.fit_resample(X, Y)\n\nprint('Original dataset shape', Counter(Y))\nprint('Resample dataset shape', Counter(y_smote))","b6202e74":"from imblearn.under_sampling import NearMiss\n\nnm = NearMiss()\n\nx_nm, y_nm = nm.fit_resample(X, Y)\n\nprint('Original dataset shape:', Counter(Y))\nprint('Resample dataset shape:', Counter(y_nm))","a4be98fb":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score","638653d7":"# load library\nfrom sklearn.svm import SVC\n\n# we can add class_weight='balanced' to add panalize mistake\nsvc_model = SVC(class_weight='balanced', probability=True)\n\nsvc_model.fit(X_train, Y_train)\n\nsvc_predict = svc_model.predict(X_test)# check performance\n","c13c8a42":"print('ROC-AUC score:',roc_auc_score(Y_test, svc_predict))\nprint('Accuracy score:',accuracy_score(Y_test, svc_predict))\nprint('F1 score:',f1_score(Y_test, svc_predict))","20f6c38f":"# load library\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nrfc = RandomForestClassifier()\n\n# fit the predictor and target\nrfc.fit(X_train, Y_train)\n\n# predict\nrfc_predict = rfc.predict(X_test)# check performance\nprint('ROCAUC score:',roc_auc_score(Y_test, rfc_predict))\nprint('Accuracy score:',accuracy_score(Y_test, rfc_predict))\nprint('F1 score:',f1_score(Y_test, rfc_predict))","46c67525":"# **3. Random under-sampling with imblearn**","70821d91":"# **7. NearMiss**","4a938120":"**The next tactic is to use penalized learning algorithms that increase the cost of classification mistakes on the minority class.**\n\n**A popular algorithm for this technique is Penalized-SVM.**\n\n**During training, we can use the argument class_weight=\u2019balanced\u2019 to penalize mistakes on the minority class by an amount proportional to how under-represented it is.**\n\n**We also want to include the argument probability=True if we want to enable probability estimates for SVM algorithms.**\n\n**Let\u2019s train a model using Penalized-SVM on the original imbalanced dataset:**","4a53f5d4":"**RandomUnderSampler is a fast and easy way to balance the data by randomly selecting a subset of data for the targeted classes. Under-sample the majority class(es) by randomly picking samples with or without replacement.**","24584695":"# **1. Random Under-Sampling**","75372562":"**Tomek links are pairs of very close instances but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process.**","5e5ac2bf":"# **2. Random Over-Sampling**","57893691":"**First of all, we need to import all the necessary libraries.**","45fc75d2":"**NearMiss is an under-sampling technique. Instead of resampling the Minority class, using a distance, this will make the majority class equal to the minority class.**","5ca817e0":"**Undersampling can be defined as removing some observations of the majority class. This is done until the majority and minority class is balanced out.But we can remove pertinent information.**\n","36cf1d07":"# **8. Change the performance metric**","00dbeb0a":"# **Resampling Technique**","cbb846e9":"# **10. Change the algorithm**","bf9eecdd":"**SMOTE (Synthetic Minority Oversampling Technique) works by randomly picking a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.(NB: this method take a lots of time)** ","cd97065a":"**A widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and\/or adding more examples from the minority class (over-sampling).**","a451aed5":"**Oversampling can be defined as adding more copies to the minority class.**","addc7ede":"# **Exploratory Data Analysis**","912ab022":"# **5. Under-sampling: Tomek links**","7277d61e":"**One other way to fight imbalance data is to generate new samples in the minority classes. The most naive strategy is to generate new samples by randomly sampling with replacement of the currently available samples.**","ce0008c0":"# **6. Synthetic Minority Oversampling Technique (SMOTE)**","aca8052c":"**the  observation in the first class is higher than the observation in the second classe then there exists an imbalance.Class Imbalance is a common problem in machine learning, especially in classification problems.Imbalance data can hamper our model accuracy big time. so we need to deal with this problem.**","e8ada31d":"# **9. Penalize Algorithms (Cost-Sensitive Training)**","5846b003":"**While in every machine learning problem, it\u2019s a good rule of thumb to try a variety of algorithms, it can be especially beneficial with imbalanced datasets.**\n\n**Decision trees frequently perform well on imbalanced data. In modern machine learning, tree ensembles (Random Forests, Gradient Boosted Trees, etc.) almost always outperform singular decision trees, so we\u2019ll jump right into those:**\n\n**Tree base algorithm work by learning a hierarchy of if\/else questions. This can force both classes to be addressed.**","ca385463":"**Accuracy is not the best metric to use when evaluating imbalanced datasets as it can be misleading.**\n\n**Metrics that can provide better insight are:**\n\n**Confusion Matrix: a table showing correct predictions and types of incorrect predictions.**\n\n**Precision: the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier\u2019s exactness. Low precision indicates a high number of false positives.**\n\n**Recall: the number of true positives divided by the number of positive values in the test data. The recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier\u2019s completeness. Low recall indicates a high number of false negatives.**\n\n**F1: Score: the weighted average of precision and recall.**\n\n**Area Under ROC Curve (AUROC): AUROC represents the likelihood of your model distinguishing observations from two classes.**\n\n**In other words, if you randomly select one observation from each class, what\u2019s the probability that your model will be able to \u201crank\u201d them correctly?**","8ed939b2":"# **4. Random over-sampling with imblearn**","74f786fa":"**SMOTE algorithm works in 4 simple steps:**\n\n**1. Choose a minority class as the input vector**\n\n**2. Find its k nearest neighbors (k_neighbors is specified as an argument in the SMOTE() function)**\n\n**3. Choose one of these neighbors and place a synthetic point anywhere on the line joining the point under consideration and its chosen neighbor**\n\n**4. Repeat the steps until data is balanced**"}}