{"cell_type":{"8db0e5fa":"code","eeb003bc":"code","49ae820c":"code","ef0fd5aa":"code","643bdab2":"code","a15f584b":"code","801f4755":"code","fd3de08e":"code","f4843742":"code","f87563bd":"code","006d40ec":"code","03ca2afc":"code","26a8e54b":"code","00a447dc":"code","6408924c":"code","87d4c4fd":"code","928264c2":"code","58ab5d74":"code","efb61509":"code","59dd2a1c":"code","f89b7251":"code","f54fe011":"code","1c1a839d":"code","607e6ee1":"code","c2ac1c0b":"code","25883b47":"code","fcc20858":"code","bb06d885":"code","dbd09e8c":"code","cb4b4008":"code","f1fd0e17":"code","5ee40fcf":"code","c252ce20":"code","3f70936a":"code","89e8f7d0":"code","0dc42c45":"code","7002689b":"code","7ed4bd8c":"code","5c9d79a7":"code","f9576834":"code","c4eb91af":"code","7d4dea31":"code","b670c6c9":"code","3761d1d3":"code","bf75f973":"code","c2d0be42":"code","c8d9b5c0":"code","b4029c53":"code","54ac1b5c":"code","729f8e25":"code","1673486b":"code","48a76c11":"code","29611077":"code","9934279f":"code","c7a3aa89":"code","384a42ef":"code","701e4c30":"code","bc6bc18a":"code","6e6664f1":"code","91d7ebd4":"code","293e6dc0":"code","83ae4c11":"code","45666723":"code","89d2a1a1":"code","86d17d70":"code","a4f4a626":"code","62a8ebc3":"code","27cfd988":"code","aa10a624":"code","89095f78":"code","1a60c7b7":"markdown","cdc9b6d9":"markdown","c677b655":"markdown","e2954302":"markdown","677c98fe":"markdown","011ee02a":"markdown","a435a9d3":"markdown","bebb01f9":"markdown","9c701960":"markdown","bfa320a1":"markdown","f78ad226":"markdown","01a0662f":"markdown","e25d3bd6":"markdown","15b08010":"markdown","495a183a":"markdown","2c2a3fe2":"markdown","29fbc7b2":"markdown","c1c2fab0":"markdown","4e64bc23":"markdown","259c0643":"markdown","e31047fa":"markdown","2e684dbc":"markdown","57b2a5e9":"markdown","0868a474":"markdown","c10d4d0d":"markdown","209b802b":"markdown"},"source":{"8db0e5fa":"from IPython.display import Image\nImage(url= \"https:\/\/static1.squarespace.com\/static\/5006453fe4b09ef2252ba068\/5095eabce4b06cb305058603\/5095eabce4b02d37bef4c24c\/1352002236895\/100_anniversary_titanic_sinking_by_esai8mellows-d4xbme8.jpg\")","eeb003bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set() # setting seaborn default for plots\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","49ae820c":"train.head(3)","ef0fd5aa":"train.shape","643bdab2":"test.shape","a15f584b":"train.info()","801f4755":"test.info","fd3de08e":"train.isnull().sum()\n#age\uc5d0 177\uac1c\uc758 null \uac12 \uc788\uc74c\n#cabin\uc5d0 687\uac1c\uc758 null \uac12 \uc788\uc74c\n#embarked\uc5d0 2\uac1c\uc758 null \uac12 \uc788\uc74c","f4843742":"test.isnull().sum()\n#age\uc5d0 86\uac1c\uc758 null \uac12 \uc788\uc74c\n#cabin\uc5d0 327\uac1c\uc758 null \uac12 \uc788\uc74c","f87563bd":"#\ub9c9\ub300 \uadf8\ub798\ud504 \uadf8\ub9ac\ub294 \ud568\uc218 \uc815\uc758\ndef bar_chart(feature):\n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))","006d40ec":"#\ubaa8\ub4e0 attributes\ub4e4\uac04\uc758 pair graph\ndata1 = train.copy(deep = True)\n\npp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.8, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\npp.set(xticklabels=[])","03ca2afc":"bar_chart('Sex')\n#\uc5ec\uc131\uc758 \ub300\ubd80\ubd84\uc774 \uc0dd\uc874\n#\ub0a8\uc131\uc758 \ub300\ubd80\ubd84\uc774 \uc0ac\ub9dd","26a8e54b":"bar_chart('Pclass')\n#1\ub4f1\uc11d \ud0d1\uc2b9\uac1d\uc740 \ub2e4\uc218\uac00 \uc0dd\uc874\n#3\ub4f1\uc11d \ud0d1\uc2b9\uac1d\uc758 \ub300\ubd80\ubd84\uc774 \uc0ac\ub9dd\n#\uadf8 \uc774\uc720\ub294 \ud0c0\uc774\ud0c0\ub2c9\ud638\uac00 \ud6c4\ubbf8\ubd80\ud130 \uce68\ubab0\ud588\uc9c0\ub54c\ubb38(\uc774\ubbf8\uc9c0)","00a447dc":"bar_chart('SibSp')\n#\ud63c\uc790\uc628 \uc0ac\ub78c\ub4e4\uc740 \uc0ac\ub9dd\ud65c \ud655\ub960\uc774 \ub354 \ub192\uc74c","6408924c":"bar_chart('Parch')\n#\ud63c\uc790\uc628 \uc0ac\ub78c\ub4e4\uc740 \uc0ac\ub9dd\ud65c \ud655\ub960\uc774 \ub354 \ub192\uc74c","87d4c4fd":"bar_chart('Embarked')\n#\ud0d1\uc2b9\uac1d \uc218: S > C > Q\n#\ud558\uc9c0\ub9cc S\uc5d0\uc11c \ud0d1\uc2b9\uac1d\uc740 \uacfc\ubc18\uc218\uac00 \uc0ac\ub9dd","928264c2":"train.head(3)","58ab5d74":"#\ud55c\ubc88\uc5d0 \uc5f0\uc0b0\ud558\uae30\uc704\ud574\uc11c train\uc774\ub791 test \ud569\uce68\ntrain_test_data = [train, test]\n\n#Title\uc774\ub77c\ub294 attribute\ub97c \ub9cc\ub4e4\uace0 Name\uc5d0\uc11c \ud638\uce6d\ub9cc \ube7c\ub0b4\uc11c \uc800\uc7a5\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)","efb61509":"train['Title'].value_counts()","59dd2a1c":"test['Title'].value_counts()\n#\uc8fc\uc758: test set\uc5d0\ub294 \uc0c8\ub85c\uc6b4 \ud638\uce6d\uc778 Dona\uac00 \uc788\uc74c","f89b7251":"#Domain Knowledge: 0: \ub0a8\uc131, 1 : \uc80a\uc740 \uc5ec\uc131, 2 : \uc911\uc7a5\ub144 \uc5ec\uc131, 3: \uc80a\uc740 \ub0a8\uc131, 4: \uc911\uc7a5\ub144 \ub0a8\uc131\ntitle_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \n                 \"Master\": 3, \"Dr\": 4, \"Rev\": 4, \"Col\": 4, \"Major\": 4, \"Mlle\": 1,\"Countess\": 3,\n                 \"Ms\": 1, \"Lady\": 2, \"Jonkheer\": 1, \"Don\": 1, \"Dona\" : 2, \"Mme\": 1,\"Capt\": 4,\"Sir\": 4 }\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)","f54fe011":"bar_chart('Title')\n#\ub0a8\uc131\uacfc \uc911\uc7a5\ub144 \ub0a8\uc131\uc740 \ub300\ubd80\ubd84\uc774 \uc0ac\ub9dd","1c1a839d":"#Name\uc740 \ub354\uc774\uc0c1 \ud544\uc694\uc5c6\uc73c\ub2c8 \uc0ad\uc81c\ntrain.drop('Name', axis=1, inplace=True)\ntest.drop('Name', axis=1, inplace=True)","607e6ee1":"#\ub0a8\uc131: 0, \uc5ec\uc131: 1\nsex_mapping = {\"male\": 0, \"female\": 1}\nfor dataset in train_test_data:\n    dataset['Sex'] = dataset['Sex'].map(sex_mapping)","c2ac1c0b":"# \ud574\ub2f9 Title \ud3c9\uade0\ub098\uc774\ub97c \ube48 age\uc5d0 \ub123\uc74c (Mr, Mrs, Miss \ub4f1..)\ntrain[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\ntest[\"Age\"].fillna(test.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)","25883b47":"train.groupby(\"Title\")[\"Age\"].transform(\"median\")","fcc20858":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()\n \nplt.show()","bb06d885":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()\nplt.xlim(0, 20)","dbd09e8c":"#\uc601\uc720\uc544: 0, \uccad\uc18c\ub144: 1, \uc131\uc778: 2, \uc911\uc7a5\ub144: 3, \ub178\uc778: 4\nfor dataset in train_test_data:\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0,\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 26), 'Age'] = 1,\n    dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 36), 'Age'] = 2,\n    dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 62), 'Age'] = 3,\n    dataset.loc[ dataset['Age'] > 62, 'Age'] = 4","cb4b4008":"train.head(3)","f1fd0e17":"bar_chart('Age')","5ee40fcf":"Pclass1 = train[train['Pclass']==1]['Embarked'].value_counts()\nPclass2 = train[train['Pclass']==2]['Embarked'].value_counts()\nPclass3 = train[train['Pclass']==3]['Embarked'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5))\n#Q\uc5d0\uc11c \ud0c4 \uc2b9\uac1d\uc740 \uac70\uc758 \ubaa8\ub450 3\ub4f1\uc11d\uc744 \ud0d0\n#C\uc5d0\uc11c \ud0c4 \uc2b9\uac1d\uc740 1\ub4f1\uc11d\uc744 \ub9ce\uc774 \ud0d0(\ubd80\ucd0c\uc778\uac00\ubcf4\ub2e4)","c252ce20":"#S\uc5d0\uc11c \ud0d1\uc2b9\ud55c \uc778\uc6d0\uc774 \uc81c\uc77c \ub9ce\uc73c\ub2c8 \ube48\uacf3\uc740 S\ub85c \ucc44\uc6c0\nfor dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')","3f70936a":"#S : 0, C : 1, Q : 2\nembarked_mapping = {\"S\": 0, \"C\": 1, \"Q\": 2}\nfor dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)","89e8f7d0":"#Fare\uc5d0 null\uac12\uc740 Pclass \uadf8\ub8f9\uc758 \ud3c9\uade0\uac12\uc73c\ub85c \ub123\uc74c\n#train\uc5d0\ub294 \ubbf8\uc2f1\uac12\uc774 \uc5c6\ub294\ub370 test\uc5d0 1\uac1c\uc788\uc73c\ub2c8\uae4c \ub300\ucda9 \uc774\ub807\uac8c\ud558\uc790\ntrain[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntest[\"Fare\"].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntrain.head(5)","0dc42c45":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Fare',shade= True)\nfacet.set(xlim=(0, train['Fare'].max()))\nfacet.add_legend()\n \nplt.show()","7002689b":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Fare',shade= True)\nfacet.set(xlim=(0, train['Fare'].max()))\nfacet.add_legend()\nplt.xlim(0, 20)","7ed4bd8c":"for dataset in train_test_data:\n    dataset.loc[ dataset['Fare'] <= 17, 'Fare'] = 0,\n    dataset.loc[(dataset['Fare'] > 17) & (dataset['Fare'] <= 30), 'Fare'] = 1,\n    dataset.loc[(dataset['Fare'] > 30) & (dataset['Fare'] <= 100), 'Fare'] = 2,\n    dataset.loc[ dataset['Fare'] > 100, 'Fare'] = 3","5c9d79a7":"#cabin\uc740 \uac12\ub3c4 \uc81c\uac01\uac01\uc774\uace0 missing values\uac00 \ub108\ubb34 \ub9ce\uc74c\ntrain.Cabin.value_counts()","f9576834":"for dataset in train_test_data:\n    dataset['Cabin'] = dataset['Cabin'].str[:1]\n#\uc55e\uc5d0 \uc54c\ud30c\ubcb3\ub9cc \uac74\uc838\ub0b4\uae30","c4eb91af":"Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts()\nPclass2 = train[train['Pclass']==2]['Cabin'].value_counts()\nPclass3 = train[train['Pclass']==3]['Cabin'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5))\n#A,B,C\ub294 1\ub4f1\uc11d\uc5d0\ub9cc \uc788\ub2e4","7d4dea31":"#\ud53c\uc154 \uc2a4\ucf00\uc77c\ub9c1: \uc720\ud074\ub9ac\ub514\uc5b8 \ub514\uc2a4\ud134\uc2a4\ub85c \uc22b\uc790\ubc94\uc704\ub97c \uc881\uac8c \ud574\uc11c \uc911\uc694\ub3c4 \ub0ae\ucda4\ncabin_mapping = {\"A\": 0, \"B\": 0.4, \"C\": 0.8, \"D\": 1.2, \"E\": 1.6, \"F\": 2, \"G\": 2.4, \"T\": 2.8}\nfor dataset in train_test_data:\n    dataset['Cabin'] = dataset['Cabin'].map(cabin_mapping)","b670c6c9":"# \ube48\uac12\uc740 Pclass\uc758 \ud3c9\uade0\uac12\uc73c\ub85c \ub123\uc74c\ntrain[\"Cabin\"].fillna(train.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)\ntest[\"Cabin\"].fillna(test.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)","3761d1d3":"train[\"FamilySize\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\ntest[\"FamilySize\"] = test[\"SibSp\"] + test[\"Parch\"] + 1","bf75f973":"facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'FamilySize',shade= True)\nfacet.set(xlim=(0, train['FamilySize'].max()))\nfacet.add_legend()\nplt.xlim(0)","c2d0be42":"#familysize\ub294 \uc911\uc694\ud558\uc9c0 \uc54a\ub2e4\uace0 \ud310\ub2e8 -> \ub0ae\uc740 weight\ub97c \uc90c\nfamily_mapping = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4}\nfor dataset in train_test_data:\n    dataset['FamilySize'] = dataset['FamilySize'].map(family_mapping)","c8d9b5c0":"#\ud2f0\ucf13, \ud615\uc81c, \ubd80\ubaa8 \ubc84\ub9ac\uae30\nfeatures_drop = ['Ticket', 'SibSp', 'Parch']\ntrain = train.drop(features_drop, axis=1)\ntest = test.drop(features_drop, axis=1)\ntrain = train.drop(['PassengerId'], axis=1)","b4029c53":"#train_data\uc640 target \uc0dd\uc131\ntrain_data = train.drop('Survived', axis=1) \ntarget = train['Survived']\n\ntrain_data.shape, target.shape","54ac1b5c":"#preprocessing \uacb0\uacfc\ntrain.head(5)","729f8e25":"#\uc6d0\ubcf8 \uc800\uc7a5\ntrain_copy = train_data","1673486b":"train.info()","48a76c11":"# \ud544\uc694\ud55c \ubaa8\ub4c8 Import\nfrom sklearn.neighbors import KNeighborsClassifier #for KNN\nfrom sklearn.tree import DecisionTreeClassifier #for Decision Tree\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.naive_bayes import GaussianNB #for Naive Bayes\nfrom sklearn.svm import SVC #for SVM\nfrom sklearn.model_selection import KFold #for kfold\nfrom sklearn.model_selection import cross_val_score\nimport math\nimport traceback","29611077":"Image(url = \"https:\/\/static.oschina.net\/uploads\/img\/201609\/26155106_OfXx.png\", width=500)","9934279f":"#kfold \ub77c\uc774\ube0c\ub7ec\ub9ac\ub85c \uad6c\ud604\n#891\uac1c\uc774\ubbc0\ub85c 10\ubc88 \ub098\ub220\uc11c \uacc4\uc0b0\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","c7a3aa89":"Image(url =\"https:\/\/helloacm.com\/wp-content\/uploads\/2016\/03\/2012-10-26-knn-concept.png\", width=500)","384a42ef":"#KNN \ub77c\uc774\ube0c\ub7ec\ub9ac\ub85c \uad6c\ud604\nclf = KNeighborsClassifier(n_neighbors = 13)\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring='accuracy')\nprint(score)\nprint(round(np.mean(score)*100, 2))\ntrain_data = train_copy","701e4c30":"#KNN \uc9c1\uc811 \uad6c\ud604\n\n# MyKnn \ud074\ub798\uc2a4 \uc0dd\uc131\nclass MyKNeighborsClassifier:\n    def __init__(self, para):\n        self.k = para.get('k')  # k \uac12 : 13\n        self.method = para.get('method')  # \uac70\ub9ac\uacc4\uc0b0 \uac12: L2(\uc720\ud074\ub9ac\uc548)\n\n    def train(self, x, y):  # train function\n        self.x = x\n        self.y = y\n        # get matrix information(row, col)\n        self.trow, self.tcol = self.x.shape\n\n    def predict(self, arr):\n        self.prow, self.pcol = arr.shape\n        dist = self.distance(arr)  # \ub514\uc2a4\ud134\uc2a4 \uad6c\ud558\uae30\n        result = np.zeros(self.prow)\n        num = int(max(self.y)) + 1\n        for i in range(0, len(result)):  # voting\uc744 \ud1b5\ud574 \uc608\uc0c1\uac12 \uacb0\uc815\n            table = np.zeros(num)\n            for j in range(0, self.k):\n                tmp = np.argmin(dist[i])\n                table[int(self.y[tmp])] += 1\n                dist[i][tmp] = math.inf\n            result[i] = np.argmax(table)\n        return result\n\n    def distance(self, arr):\n        # call function by method\n        ords = {\n            'Euclidean': 2,\n            'L2': 2,\n            'L1': 1,\n            'Manhattan': 1,\n            'Maximum': math.inf\n        }.get(self.method, 1)\n        dist = self.norm(arr, ords)\n        return dist\n\n    # \uac70\ub9ac\uacc4\uc0b0 (Manhattan is L1, Euclidean is L2, Maximum is L(inf))\n    def norm(self, arr, ords):\n            dist = np.zeros((self.prow, self.trow))\n            for i in range(0, self.prow):\n                tmp = self.x - arr[i]\n                if self.prow == 1:\n                    tmp = self.x - arr\n                if self.trow == 1:\n                    dist[i] = np.linalg.norm(tmp, ord=ords)\n                    continue\n                if self.prow == 1:\n                    tmp = self.x - arr\n                dist[i] = np.linalg.norm(tmp, axis=1, ord=ords)\n            return dist\n        \n    # Kfold \uad6c\ud604\n    def fold(self, n):\n        originx = self.x\n        originy = self.y\n        each = int(self.trow\/n)\n        tmp = np.zeros(n)\n        for i in range(0, n):\n            arr = originx[i*each: (i+1)*each]  # slicing\n            target = originy[i*each: (i+1)*each]\n            if i == 0:\n                self.x = originx[(i+1)*each:]\n                self.y = originy[(i+1)*each:]\n            elif i != (n-1):\n                self.x = np.concatenate((originx[(i + 1) * each:], originx[:i * each]), axis=0)\n                self.y = np.concatenate((originy[(i + 1) * each:], originy[:i * each]), axis=0)\n            else:  # i == (n-1):\n                arr = originx[i * each:]\n                target = originy[i * each:]\n                self.x = originx[:i*each]\n                self.y = originy[:i*each]\n            self.train(self.x, self.y)  # reset train data\n            pre = self.predict(arr)\n            tmp[i] = (sum(target[a] == pre[a] for a in range(0, len(target))))\/len(target)\n        print(tmp)\n        print('fold cv : %.3f' % (sum(tmp)*100\/n))\n        self.train(originx, originy)  # recover train data","bc6bc18a":"knn = MyKNeighborsClassifier({'k': 13, 'method': 'L2'})\ntx = np.array(train_data.values)\nty = np.array(target.values)\nt = np.array(test.values)\nknn.train(tx, ty)\n\nknn.fold(10)\ntrain_data = train_copy","6e6664f1":"#Decision Tree \ub77c\uc774\ube0c\ub7ec\ub9ac\ub85c \uad6c\ud604\nclf = DecisionTreeClassifier(criterion = \"gini\", random_state = 90, max_depth=4, min_samples_leaf=20)\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nround(np.mean(score)*100, 2)","91d7ebd4":"#\uad6c\ud604\ub41c \ud2b8\ub9ac\ub97c \uc2dc\uac01\ud654\nfrom sklearn import tree\n\nclf.fit(train_data, target)\nwith open(\"decision_tree.txt\", \"w\") as f:\n    f = tree.export_graphviz(clf, out_file=f, class_names=list(train_data.columns.values))\ntrain_data = train_copy","293e6dc0":"import sys\nimport math\n\nclass Node(object):\n    def __init__(self, attribute, threshold):\n        self.attr = attribute\n        self.thres = threshold\n        self.left = None\n        self.right = None\n        self.leaf = False\n        self.predict = None\n\n# threshold \uace0\ub974\uae30\ndef select_threshold(df, attribute, predict_attr):\n    #dataframe -> list\ub85c convert\n    values = df[attribute].tolist()\n    values = [ float(x) for x in values]\n    #list -> set\uc73c\ub85c convert: \uc911\ubcf5\ub41c value \uc81c\uac70\n    values = set(values)\n    values = list(values)\n    values.sort() #sorting\n    max_ig = float(\"-inf\")\n    thres_val = 0\n    for i in range(0, len(values) - 1):\n        thres = (values[i] + values[i+1])\/2\n        ig = info_gain(df, attribute, predict_attr, thres)\n        if ig > max_ig:\n            max_ig = ig\n            thres_val = thres\n    # information gain\uc774 \uac00\uc7a5 \ub192\uc740 threshold\ub97c return\n    return thres_val\n\n# \uc5d4\ud2b8\ub85c\ud53c \uacc4\uc0b0\ndef info_entropy(df, predict_attr):\n    p_df = df[df[predict_attr] == 1] #p_df: \uc0dd\uc874\n    n_df = df[df[predict_attr] == 0] #n_df: \uc0ac\ub9dd\n    p = float(p_df.shape[0])\n    n = float(n_df.shape[0])\n    # \uc5d4\ud2b8\ub85c\ud53c \uacc4\uc0b0\n    if p  == 0 or n == 0:\n        I = 0\n    else:\n        I = ((-1*p)\/(p + n))*math.log(p\/(p+n), 2) + ((-1*n)\/(p + n))*math.log(n\/(p+n), 2)\n    return I\n\ndef remainder(df, df_subsets, predict_attr):\n    # test data \uac1c\uc218\n    num_data = df.shape[0]\n    remainder = float(0)\n    for df_sub in df_subsets:\n        if df_sub.shape[0] > 1:\n            remainder += float(df_sub.shape[0]\/num_data)*info_entropy(df_sub, predict_attr)\n    return remainder\n\n# threshold\ub85c informaion gain \uacc4\uc0b0 \ndef info_gain(df, attribute, predict_attr, threshold):\n    sub_1 = df[df[attribute] < threshold]\n    sub_2 = df[df[attribute] > threshold]\n    # information gain = entropy - remainder\n    ig = info_entropy(df, predict_attr) - remainder(df, [sub_1, sub_2], predict_attr)\n    return ig #\uacc4\uc0b0\uac12\uc740 \uacc4\uc18d \ub2ec\ub77c\uc9c8\uc218O\n\n# \uc0dd\uc874\uc790&\uc0ac\ub9dd\uc790 \uba85\uc218 return\ndef num_class(df, predict_attr):\n    p_df = df[df[predict_attr] == 1]\n    n_df = df[df[predict_attr] == 0]\n    return p_df.shape[0], n_df.shape[0]\n\n# info gain \uc81c\uc77c \ub192\uc740 attribute\ub791 threshold \uace0\ub974\uae30\ndef choose_attr(df, attributes, predict_attr):\n    max_info_gain = float(\"-inf\")\n    best_attr = None\n    threshold = 0\n    # Testing (attributes \uc911\ubcf5 \uc120\ud0dd \uac00\ub2a5)\n    for attr in attributes:\n        thres = select_threshold(df, attr, predict_attr)\n        ig = info_gain(df, attr, predict_attr, thres) #information gain \uacc4\uc0b0\n        if ig > max_info_gain:\n            max_info_gain = ig\n            best_attr = attr\n            threshold = thres\n    return best_attr, threshold\n\n#decision tree \uad6c\ucd95\ndef build_tree(df, cols, predict_attr):\n    p, n = num_class(df, predict_attr) #p: \uc0dd\uc874\uc790(p_df) ,n: \uc0ac\ub9dd\uc790(n_df)\n    if p == 0 or n == 0: #\uc804\ubd80\ub2e4 \uc0dd\uc874\ud588\uac70\ub098 \uc0ac\ub9dd\ud588\uc73c\uba74 -> leaf\uc5d0 \ub3c4\ub2ec\ud55c\uac70\n        # leaf node \uc0dd\uc131\n        leaf = Node(None,None)\n        leaf.leaf = True\n        if p > n:\n            leaf.predict = 1\n        else:\n            leaf.predict = 0\n        return leaf\n    else:\n        #informaion gain\uc744 \uae30\uc900\uc73c\ub85c attribute, threshold \uacb0\uc815\n        best_attr, threshold = choose_attr(df, cols, predict_attr)\n        #internal tree \uc0dd\uc131\n        tree = Node(best_attr, threshold)\n        sub_1 = df[df[best_attr] < threshold]\n        sub_2 = df[df[best_attr] > threshold]\n        \n        # \uc7ac\uadc0\uc801\uc73c\ub85c subtree\uc0dd\uc131  \n        tree.left = build_tree(sub_1, cols, predict_attr)\n        tree.right = build_tree(sub_2, cols, predict_attr)\n        return tree\n\ndef predict(node, row_df):\n    if node.leaf: #leaf node\uc774\uba74,\n        return node.predict #leaf\uc758 prediciton\n    if row_df[node.attr] <= node.thres: #threshold\ubcf4\ub2e4 \uc791\uc73c\uba74,\n        return predict(node.left, row_df) #\uc67c\ucabd \uac00\uc9c0\ub85c\n    elif row_df[node.attr] > node.thres: #threshold\ubcf4\ub2e4 \ud06c\uba74,\n        return predict(node.right, row_df) #\uc624\ub978\ucabd \uac00\uc9c0\ub85c\n\n# \ub9cc\ub4e4\uc5b4\uc9c4 \ub514\uc528\uc83c\ud2b8\ub9ac\ub97c \uc774\uc6a9\ud574\uc11c test\uac12 \uc608\uce21\ud55c \uacb0\uacfc \uc815\ud655\ub3c4 \uacc4\uc0b0\ndef test_predictions(root, df):\n    num_data = df.shape[0]\n    num_correct = 0\n    for index,row in df.iterrows():\n        prediction = predict(root, row)\n        if prediction == row['Survived']: #\ub2f5\uacfc \uc608\uce21\uac12\uc774 \uc77c\uce58\ud560\uacbd\uc6b0\n            num_correct += 1 #num_correct 1 \uc99d\uac00\n    return round(num_correct\/num_data, 2) #\ubc31\ubd84\uc728 \uacc4\uc0b0\n\ndef main():\n    #df_train: 91~891\uae4c\uc9c0 df_test: 0~90\uae4c\uc9c0\n    df_train = train.loc[91:]\n    df_test = train.loc[0:90]\n\n    attributes = train.columns.values\n    \nif __name__ == '__main__':\n    main()","83ae4c11":"#Random Forest \ub77c\uc774\ube0c\ub7ec\ub9ac\ub85c \uad6c\ud604\nclf = RandomForestClassifier(n_estimators=13)\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\ntrain_data = train_copy","45666723":"round(np.mean(score)*100, 2)","89d2a1a1":"#Gaussian Naive Bayes \ub77c\uc774\ube0c\ub7ec\ub9ac\ub85c \uad6c\ud604\nclf = GaussianNB()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\ntrain_data = train_copy","86d17d70":"round(np.mean(score)*100, 2)","a4f4a626":"#SVM \ub77c\uc774\ube0c\ub7ec\ub9ac\ub85c \uad6c\ud604\nclf = SVC()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)","62a8ebc3":"round(np.mean(score)*100,2)","27cfd988":"#SVM\uc774 \uac00\uc7a5 \ub192\uac8c \ub098\uc654\uae30\ub54c\ubb38\uc5d0 SVM\uc73c\ub85c \uc81c\ucd9c\nclf.fit(train_data, target)\n\ntest_data = test.drop(\"PassengerId\", axis=1).copy()\nprediction = clf.predict(test_data)","aa10a624":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": prediction\n    })\n\nsubmission.to_csv('submission.csv', index=False)","89095f78":"submission = pd.read_csv('submission.csv')\nsubmission.head(5)","1a60c7b7":"Pridict survival on the Titanic by Data Science\n===================================\n\ubaa9\ud45c : \uc815\ub2f5\uc728 0.8% \uc774\uc0c1\n* 11\/1 0.7% \ub0a8\/\uc5ec\ub85c\ub9cc \uad6c\ubd84\n* 11\/19 0.78947% #SVM \uc0ac\uc6a9\n* 11\/21 0.75% #desicion tree\uc5d0 \ud30c\ub77c\ubbf8\ud130 \ucd94\uac00\ud558\uc5ec accuracy \ub192\uc600\uc73c\ub098 \uc2e4\uc81c \uacb0\uacfc\ub294 \ub354 \ub0ae\uac8c \ub098\uc634(\uc65c?)\n* 11\/22 0.77%  \uc624\ubc84\ud53c\ud305\ub3fc\uc788\uc5b4\uc11c\ub77c\uace0\ud55c\ub2e4 -> \ud504\ub8e8\ub2dd\ud558\uc790","cdc9b6d9":"## 0. Data Introduction\n-------------------------------","c677b655":"### 2.5. SVM","e2954302":"### 1.5 Fare","677c98fe":"- - - \n## 1. Preprocessing\n* NULL \uac12\uc5d0 \ub370\uc774\ud130 \ucc44\uc6b0\uae30\n* \uc758\ubbf8\uc5c6\ub294 Attribute \uc9c0\uc6b0\uae30\n* Numeric\uc73c\ub85c \ubc14\uafb8\uae30","011ee02a":"* Survived : 0 = Dead, 1 = Survived\n* Pclass : 1\ub4f1\uc11d > 2\ub4f1\uc11d > 3\ub4f1\uc11d \n* Name : \uc2b9\uac1d \uc774\ub984 (\ud638\uce6d \ud3ec\ud568)\n* Sex :  \uc2b9\uac1d \uc131\ubcc4 (female = \uc5ec, male : \ub0a8)\n* Age : \uc2b9\uac1d \ub098\uc774\n* SibSp : \uc2b9\uac1d\uacfc \uac19\uc774 \ud0d1\uc2b9\ud55c \ud615\uc7ac \ub610\ub294 \ub3d9\ub8cc(Sibling and Spouses)\uac00 \uba87\uba85 \uc788\ub294\uc9c0\n* Parch : \uc2b9\uac1d\uc774 \uac19\uc774 \ud0d1\uc2b9\ud55c \ubd80\ubaa8 \ub610\ub294 \uc790\ub140\uac00 \uba87\uba85 \uc788\ub294\uc9c0\n* Ticket : \ud2f0\ucf13 \ubc88\ud638\n* Fare : \ud2f0\ucf13 \uac00\uaca9\n* Cabin : \uc218\ud654\ubb3c \ubc88\ud638\n* Embarked : \ud0d1\uc2b9\ud55c \ud56d\uad6c \uba85 (C = Cherborug, Q = Queentown, S = Southampton)","a435a9d3":"\ud504\ub85c\uc81d\ud2b8 \uc8fc\uc81c","bebb01f9":"### 2.1. KNN","9c701960":"\uc601\ubb38\ud638\uce6d|\uc131\ubcc4|\uacb0\ud63c|\ube44\uace0\n--|-|-|---------\nMr|\ub0a8|\ubaa8\ub984|\nMiss|\uc5ec|\uc548\ud568|\nMrs|\uc5ec|\ud568|\nMaster|\ub0a8|\ubbf8\ud63c|\uccad\uc18c\ub144\nDr|\ub0a8|\uae30\ud63c|\ubc15\uc0ac\nRev|\ub0a8|\uae30\ud63c|\ubaa9\uc0ac\nCol|\ub0a8|\uae30\ud63c|\ub300\ub839\nMlle|\uc5ec|\ubbf8\ud63c|\ub9c8\ub4dc\ubb34\uc544\uc824\nMajor|\ub0a8|\uae30\ud63c|\nCapt|\ub0a8|\uae30\ud63c|\uad70\uc778\nDon|\ub0a8|\uae30\ud63c|\ub178\uc778\nDona|\uc5ec|\uae30\ud63c|\ub178\uc778\nMme|\uc5ec|\uae30\ud63c|\ub9c8\ub2f4\nCountess|\uc5ec|\uae30\ud63c|\ubc31\uc791\ubd80\uc778\nJonkheer|\ub0a8|\ubaa8\ub984|\uadc0\uc871\nLady|\uc5ec|\uae30\ud63c|\uc5ec\uc0ac\nMs|\uc5ec|\ubaa8\ub984|","bfa320a1":"## 3. Submission","f78ad226":"### 2.0. K-fold\nkfold\ub97c \uc774\uc6a9\ud558\uc5ec accuracy\ub97c \uac80\uc99d","01a0662f":"### 2.2 Decision Tree","e25d3bd6":"### 1.4 Embarked","15b08010":"## 2. Modeling","495a183a":"### 1.3 Age","2c2a3fe2":"### 1.1 Name","29fbc7b2":"### 2.4 Naive Bayes","c1c2fab0":"### 1.2 Sex","4e64bc23":"### 1.7 FamilySize\n* FamilySize = Parch + SibSp","259c0643":"\ud0c0\uc774\ud0c0\ub2c9\ud638\uc5d0 \ud0c0\uace0\uc788\ub358 \uc2b9\uac1d \uc815\ubcf4\ub97c \uc774\uc6a9\ud558\uc5ec \uc0dd\uc874\uc790\ub97c \uc608\uce21\ud558\uc790\n* training set : 891\n* test set : 418","e31047fa":"### 1.6 Cabin","2e684dbc":"### [\ucc38\uace0\ubb38\ud5cc]\n* [01] https:\/\/www.kaggle.com\/jamesleslie\/titanic-random-forrest-use-title-to-impute-age\n* [02] https:\/\/github.com\/minsuk-heo\/kaggle-titanic\/blob\/master\/titanic-solution.ipynb\n* [03] https:\/\/wikidocs.net\/7646\n* [04] https:\/\/github.com\/dpkravi\/DecisionTreeClassifier\/blob\/master\/tree.py \n* [05] https:\/\/m.blog.naver.com\/PostView.nhn?blogId=yeop9657&logNo=220830690968&categoryNo=116&proxyReferer=&proxyReferer=https%3A%2F%2Fwww.google.com%2F\n* [06] http:\/\/hamait.tistory.com\/843\n* [07] http:\/\/blog.naver.com\/PostView.nhnblogId=nonamed0000&logNo=220976767692&categoryNo=26&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=search\n* [08] https:\/\/github.com\/dpkravi\/DecisionTreeClassifier\/blob\/master\/tree.py\n* [09] http:\/\/dni-institute.in\/blogs\/step-by-step-tutorial-on-decision-tree-using-python\/\n* [10] https:\/\/www.youtube.com\/watch?v=tNa99PG8hR8\n* [11] http:\/\/www.webgraphviz.com","57b2a5e9":"### 2.3 Random Forest","0868a474":"\uc54c\uc218\uc788\ub294 \uc815\ubcf4:\n* \uc131\ubcc4-\uc0dd\uc874 \uc0c1\uad00\uad00\uacc4 \ub192\uc74c: \uc5ec\uc131\uc774\uba74 \uc0b4 \ud655\ub960\uc774 \ub0a8\uc131\ubcf4\ub2e4 \ud6e8\uc52c \ub192\uc74c\n* \ud63c\uc790\ud0c4 \uc0ac\ub78c\uc740 1\/3\ub9cc \uc0dd\uc874\n* fare\ub97c \uc801\uac8c\ub0b8\uc0ac\ub78c\uc740 \uac70\uc758 \uc0ac\ub9dd\ud558\uc600\uc74c","c10d4d0d":"- - -","209b802b":"- - -"}}