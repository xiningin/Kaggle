{"cell_type":{"7f0660e9":"code","65863086":"code","d311380c":"code","0fc3cbbc":"code","789137d3":"code","f3079b76":"code","febe3651":"code","b11897a3":"code","c5ce4e55":"code","808fbf70":"code","b97ec990":"code","18be507e":"code","1366fdb8":"code","ed4274ab":"code","38dd99d5":"code","cd6bfd7b":"code","c51a11dd":"code","67760519":"code","9e33ec28":"code","7da700e8":"code","1c54e0c8":"code","ec8a7797":"code","a8c2bfc9":"code","03e699f0":"code","69631d10":"code","83309a2d":"code","effb346d":"code","189dae09":"code","b8be6ae4":"code","00c220d5":"code","ec733e62":"code","6a7a5593":"code","f62f88cd":"code","de029fbc":"code","3bb7a3a7":"code","8e5bf59c":"code","16d273f5":"code","dc475c54":"code","334dae4e":"code","b5940722":"code","f330aaee":"code","febed1ba":"code","a3d17689":"code","8bc7814f":"code","d56523a2":"code","49107e65":"code","9abfdcfc":"code","fcd5dcf5":"code","e8712811":"code","e8e5c272":"code","98adba9e":"code","85223ea6":"code","ddac2cd5":"code","307427a8":"code","aa21641e":"code","316d42b2":"code","114653b3":"code","c45e19b3":"code","853aed69":"code","1a7da85b":"code","d648989a":"code","3f951c6c":"code","a716df09":"code","f56bf562":"code","ce8ab61d":"code","f3066958":"code","7a5514cb":"code","bd3d08c9":"code","465b0d7f":"code","674aa1cf":"code","8f5968c8":"code","d0dd2ead":"code","0f595283":"code","ff983602":"code","7c75ca1d":"code","f6f8f5a3":"code","5cf4bd27":"code","fde59b30":"code","d08cfaf8":"code","e8405564":"code","92aadd9d":"code","53bb80dc":"code","ba2e66d2":"code","53e6bf29":"code","74c91d20":"code","e1f3da3c":"code","d61a5816":"code","10e3c750":"code","d0c56701":"code","10e24cea":"code","40f7ff01":"code","fdb9bff2":"code","473f22e0":"code","862e9801":"code","04603704":"code","7c6c224e":"code","38437677":"code","fa821c27":"code","c9c3902b":"code","f2371e95":"code","afeba29e":"code","75fe8b60":"code","e3d1c207":"code","dd603d81":"code","46a9ea36":"markdown","2a28dd15":"markdown","07736e1d":"markdown","33ca3056":"markdown","8e5137a1":"markdown","2a098bae":"markdown","6b1fd886":"markdown","394262e1":"markdown","99ae7e9d":"markdown","bb38526e":"markdown","586f9147":"markdown","88eb0eb6":"markdown","4660ba77":"markdown","41594e20":"markdown","edf60d2c":"markdown","bc10a37b":"markdown","c4a87ec7":"markdown","a996fb31":"markdown","bd0e4499":"markdown","6318d660":"markdown","2efed665":"markdown","427710c8":"markdown","9bf88f6c":"markdown","e62c228b":"markdown","cc7fbf8a":"markdown","e6e8ce57":"markdown","dfcbb5bd":"markdown","ae68b8ef":"markdown","a9806f7b":"markdown","a5d9da93":"markdown","ddda06fb":"markdown","ece21ae4":"markdown","7c9e583b":"markdown","de0f0d8a":"markdown","4102fd7a":"markdown","8e09aaac":"markdown","d55a7520":"markdown","5f26c34a":"markdown","0575d0a3":"markdown","805372bf":"markdown","200c403c":"markdown","bec727a1":"markdown","75f574b4":"markdown","b079567b":"markdown","785fa646":"markdown","7ead1ca6":"markdown","0b377c77":"markdown","c3250c66":"markdown","f0b98b24":"markdown","0a9948c0":"markdown","94a5ac6f":"markdown","554123b1":"markdown","45c21aa7":"markdown","22943709":"markdown","4a0551ab":"markdown","aaacbdd6":"markdown","bc9060d3":"markdown","cc28d05d":"markdown","6636d9c8":"markdown","dad12c80":"markdown","0570959c":"markdown","93fac003":"markdown","fad64d5b":"markdown","83e3e9a2":"markdown","40ac8e63":"markdown","beede44c":"markdown","21400d55":"markdown","9e0db332":"markdown","00ba841d":"markdown","7f32ae7f":"markdown","0b03bf01":"markdown","2877e7c2":"markdown","7a0e3fb4":"markdown"},"source":{"7f0660e9":"## Data Analysis and Munging\/ Wrangling\nimport pandas as pd\nimport numpy as np\n\n## Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n## Machine Learning Models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom collections import OrderedDict\nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\n","65863086":"## Location\/ Path where documents are stored\npath = \"\/kaggle\/input\/titanic\/\"\n\n## Dataframes that correspond to each spreadsheet\ndf_train = pd.read_csv(path + 'train.csv')\ndf_test = pd.read_csv(path + 'test.csv')\ndf_gender_submission = pd.read_csv(path + 'gender_submission.csv')","d311380c":"## Dataframe 'df_train'\ndf_train.head(3)","0fc3cbbc":"## Dataframe 'df_test'\ndf_test.head(3)","789137d3":"## Dataframe 'df_gender_submission'\ndf_gender_submission.head(3)","f3079b76":"## Set the passenger id as the index of the dataframes\ndf_train.set_index(['PassengerId'], inplace = True)\ndf_test.set_index(['PassengerId'], inplace = True)\ndf_gender_submission.set_index(['PassengerId'], inplace = True)","febe3651":"dict_columns = {'Pclass': 'Pass_Class', \n                'Ticket': 'Ticket_Id', \n                'Fare': 'Pass_Fare',\n                'Cabin': 'Cabin_Id', \n                'Embarked' : 'Port_Embark',\n                'Parch' : 'Par_Child_Aboard', \n                'SibSp': 'Sibli_Aboard'}\n\ndf_train.rename(columns = dict_columns, inplace = True)\n\ndf_test.rename(columns = dict_columns, inplace = True)","b11897a3":"## Checking the dataframes structures like number of rows and columns\nprint(\"df_train: \", df_train.shape, \n      \"\\ndf_test: \", df_test.shape, \n      \"\\ndf_gender_submission: \", df_gender_submission.shape)","c5ce4e55":"## Percentagem of null values in dataframe df_train\ndf_train.isnull().sum() \/ len(df_train) * 100","808fbf70":"## Percentagem of null values in dataframe df_train\ndf_test.isnull().sum() \/ len(df_test) * 100","b97ec990":"## Percentage of null values in dataframe df_train\ndf_gender_submission.isnull().sum() \/ len(df_gender_submission) * 100","18be507e":"## Check if there is any relationship in the unnamed cabins, with the types of accommodation\n\n# Creating a dataframe with only the information from the columns 'Pclass' and 'Cabin'\ndf_train_null_values_Cabin =  df_train.loc[:, ['Pass_Class', 'Cabin_Id']]\n\n# Creating a new column to identify whether the field, which has any string, is null or not\ndf_train_null_values_Cabin['is_null'] = np.where(df_train_null_values_Cabin['Cabin_Id'].isnull(),1,0 )\n\n# Creating a crosstab with the information of the classes of the cabins, and the number \n# of cabins with unknown name \/ number\npd.crosstab(df_train_null_values_Cabin[\"Pass_Class\"],df_train_null_values_Cabin[\"is_null\"],margins=True)","1366fdb8":"## Replacing the null values in the column 'Cabin' with the value 'Unknown'\ndf_train['Cabin_Id'].fillna('Unknown', inplace = True)\ndf_train.head(3)","ed4274ab":"## Creating the column Deck\ndf_train['Deck'] = df_train['Cabin_Id'].apply(lambda nm : nm[0])","38dd99d5":"## Since the field only contains some specific types of values, more precisely three, \n## it is possible to verify the distribution of these data\ndf_train['Port_Embark'].value_counts(dropna = False)","cd6bfd7b":"## Replacing the null values in the column 'Embarked' with the value 'Not Informed'\/ 'Not Info' \n\n## Applying the changes\ndf_train['Port_Embark'].fillna(value = 'Not Info', inplace = True)\n","c51a11dd":"df_train[df_train['Age'].isnull()]","67760519":"## Since there are many passengers without age information, the median will be adopted to replace these null values.\ndf_train['Age'].fillna(df_train['Age'].median(), inplace = True)\ndf_train.head(3)","9e33ec28":"df_test.isnull().sum()","7da700e8":"## It seems that there is only one passenger of the third class, \n## so we apply the median based on the values of the third class.\ndf_test[df_test['Pass_Fare'].isnull()]","1c54e0c8":"median = df_test.loc[df_test['Pass_Class'] == 3, 'Pass_Fare'].mean()\n## Assign the passenger Thomas Storey, the fare value equal to the average of his class\ndf_test['Pass_Fare'].fillna(median, inplace = True)\n\n## Replacing the null values in the column 'Cabin' with the value 'Unknown'\ndf_test['Cabin_Id'].fillna('Unknown', inplace = True)\n\n## Since there are many passengers without age information, the median will be adopted to replace these null values.\ndf_test['Age'].fillna(df_test['Age'].median(), inplace = True)\n\n## Applying the treatment related to column 'Deck'\ndf_test['Deck'] = df_test['Cabin_Id'].apply(lambda nm : nm[0])","ec8a7797":"## Check if there are still null values in the dataframes.\nprint(\"Are there null values in the dataframe df_train ?: \", df_train.isnull().values.any())\nprint(\"Are there null values in the dataframe df_test ?: \", df_test.isnull().values.any())","a8c2bfc9":"# The previous code showed the boxplot of all variables \/ columns.\nplt.figure(figsize = (18,13))\nsns.boxplot(data = df_train)\n\nplt.show()","03e699f0":"def fn_validating_dataframe(p_df_dataframe):\n    \"\"\"\n        Description:\n            Validates information related to the \n            dataframe.\n\n        Keyword arguments:\n            p_df_dataframe -- the dataframe \n\n        Return:\n            None\n\n        Exception:\n            Validates whether the object passed is a pandas dataframe;\n            Validates that the dataframe is empty.\n    \"\"\"\n    \n    if not (isinstance(p_df_dataframe, pd.DataFrame)):\n            raise Exception(\"The past object is not a Pandas Dataframe\")\n            \n    if p_df_dataframe.empty:\n            raise Exception(\"The dataframe is empty\")","69631d10":"def fn_number_of_outliers_per_dataframe(p_df_dataframe):\n    \"\"\"\n        Description:\n            Validates the number of outliers on a dataframe\n\n        Keyword arguments:\n            p_df_dataframe -- the dataframe \n\n        Return:\n            Object with the number of outliers per column\n\n        Exception:\n            Validates whether the object passed is a pandas dataframe;\n            Validates that the dataframe is empty.\n    \"\"\"\n    \n    fn_validating_dataframe(p_df_dataframe)\n        \n    Q1 = p_df_dataframe.quantile(0.25)\n    Q3 = p_df_dataframe.quantile(0.75)\n    IQR = Q3 - Q1\n    sr_out = ((p_df_dataframe < (Q1 - 1.5 * IQR)) | (p_df_dataframe > (Q3 + 1.5 * IQR))).sum()\n    return sr_out","83309a2d":"fn_number_of_outliers_per_dataframe(df_train)","effb346d":"fn_number_of_outliers_per_dataframe(df_test)","189dae09":"  \ndef fn_catching_outliers(p_df_dataframe, p_column):\n    \"\"\"\n    Description:\n        Function that locates outliers in an informed dataframe.\n\n    Keyword arguments:\n        p_df_dataframe -- the dataframe \n        p_column -- the dataframe column\n\n    Return:\n        df_with_outliers -- Dataframe with the outliers located\n        df_without_outliers -- Dataframe without the outilers\n    \n    Exception:\n        None\n    \"\"\"\n    # Check if the information passed is valid.\n    fn_number_of_outliers_per_dataframe(p_df_dataframe)\n    \n    # Calculate the first and the third qurtile of the dataframe  \n    Q1 = p_df_dataframe[p_column].quantile(0.25)\n    Q3 = p_df_dataframe[p_column].quantile(0.75)    \n  \n    \n    # Calculate the interquartile value\n    IQR = Q3 - Q1\n    \n    #sr_out = ((p_df_dataframe < (Q1 - 1.5 * IQR)) | (p_df_dataframe > (Q3 + 1.5 * IQR))).sum()\n    \n    # Generating the fence hig and low values\n    fence_high = Q3 + (1.5 * IQR)\n    fence_low = Q1 - (1.5 * IQR)\n    \n    # And Finally we are generating two dataframes, onde with the outliers values and the second with the values within values\n    df_with_outliers = p_df_dataframe[((p_df_dataframe[p_column] < fence_low) | (p_df_dataframe[p_column] > fence_high))]\n    \n    if df_with_outliers.empty:\n        print(\"No outliers were detected.\")\n    \n    return df_with_outliers","b8be6ae4":"# Column 'Age'\ndf_out = fn_catching_outliers(df_train, 'Age')\ndf_out.head(5)","00c220d5":"## Creating a function to assist in classification.\ndef age_definition(p_age):\n    \"\"\"\n        Description:\n            Function that classifies someone's age\n\n        Keyword arguments:\n            p_age -- Number that matches someone's age\n\n        Return:\n            Returns the age classification as Young, Child and etc.\n\n        Exception:\n            None\n    \"\"\"\n    age = int(p_age)\n    if age <= 2:\n        return 'Infant'\n    elif age <= 12:\n        return 'Childreen'\n    elif age <= 17:\n        return 'Young'\n    elif age <= 24:\n        return 'Young Adult'\n    elif age <= 44:\n        return 'Adult'\n    elif age <= 59:\n        return 'Middle-Age'\n    elif age <= 74:\n        return 'Senior'\n    elif age <= 90:\n        return \"Elder\"\n    else:\n        return 'Extreme Old Age'\n\n## Creating the new column\ndf_train['Age_Gr'] = df_train['Age'].apply(age_definition)\n\n## Catching the outlier again\ndf_out = fn_catching_outliers(df_train, 'Age')\n\n## Age classification based on outliers \ndf_out['Age_Gr'].value_counts()","ec733e62":"## Catching the outlier from dataframe 'Test'\ndf_out = fn_catching_outliers(df_test, 'Age')\ndf_out.head(3)","6a7a5593":"## Creating the new column\ndf_test['Age_Gr'] = df_test['Age'].apply(age_definition)\n\n## Catching the outlier again\ndf_out = fn_catching_outliers(df_test, 'Age')\n\n## Age classification based on outliers \ndf_out['Age_Gr'].value_counts()","f62f88cd":"# Column 'Parents_Or_Childreeens_Aboard'\ndf_out = fn_catching_outliers(df_train, 'Par_Child_Aboard')\ndf_out.head(5)","de029fbc":"df_out['Par_Child_Aboard'].value_counts()","3bb7a3a7":"## Column 'Siblings_Aboard'\ndf_out = fn_catching_outliers(df_train, 'Sibli_Aboard')\ndf_out.head(5)                   ","8e5bf59c":"df_out['Sibli_Aboard'].value_counts()","16d273f5":"##Passenger_Fare   \ndf_out = fn_catching_outliers(df_train, 'Pass_Fare')\ndf_out.head(5)","dc475c54":"## Main statistical metrics related to passenger tariffs\ndf_train['Pass_Fare'].describe()","334dae4e":"df_train['Crew'] = np.where(df_train['Pass_Fare'] == 0, 1, 0)\n\ndf_test['Crew'] = np.where(df_test['Pass_Fare'] == 0, 1, 0)","b5940722":"## Check the quartiles\nquantiles = [0.10, 0.25, 0.50, 0.70 ,0.75, 0.80, 0.90, 0.95, 0.99]\nfor q in quantiles:\n    print(\"Quantile[\", q*100,\"%] :\", df_train['Pass_Fare'].quantile(q))","f330aaee":"## Main statistical metrics related to outliers, more specifically those related to passenger fares\ndf_out['Pass_Fare'].describe()","febed1ba":"## Group the amount of outliers by social class\ndf_out['Pass_Class'].value_counts()","a3d17689":"df_out[df_out['Pass_Class'] == 3 ]","8bc7814f":"## Number of people with the same ticket, more precisely the ticket 'CA. 2343 '\nlen(df_out[df_out['Ticket_Id'] == 'CA. 2343'])","d56523a2":"# Create dictionary with the number of tickets on the ship\ndt_ticket_identification = df_train['Ticket_Id'].value_counts().to_dict()\n# Create column showing the number of passengers using the same ticket\ndf_train['Num_Same_Ticket'] = df_train['Ticket_Id'].map(lambda x: dt_ticket_identification[x])\n# Update column value\ndf_train['Pass_Fare'] = np.where((df_train['Pass_Fare'] != 0)\n                                      ,df_train['Pass_Fare']\/df_train['Num_Same_Ticket']\n                                      ,df_train['Pass_Fare'])","49107e65":"# Create dictionary with the number of tickets on the ship\ndt_ticket_identification = df_test['Ticket_Id'].value_counts().to_dict()\n# Create column showing the number of passengers using the same ticket\ndf_test['Num_Same_Ticket'] = df_test['Ticket_Id'].map(lambda x: dt_ticket_identification[x])\n# Update column value\ndf_test['Pass_Fare'] = np.where((df_test['Pass_Fare'] != 0)\n                                      ,df_test['Pass_Fare']\/df_test['Num_Same_Ticket']\n                                      ,df_test['Pass_Fare'])","9abfdcfc":"## Number of people with the same ticket, more precisely the ticket 'CA. 2343 '\ndf_train[df_train['Ticket_Id'] == 'CA. 2343']","fcd5dcf5":"## Identify whether outliers still exist\ndf_out = fn_catching_outliers(df_train, 'Pass_Fare')\nlen(df_out)","e8712811":"## Show the first three rows of the dataframe\ndf_out.head(3)","e8e5c272":"## Show the most commom statistics about the dataframe of outliers\ndf_out['Pass_Fare'].describe()","98adba9e":"## Group the amount of outliers by social class\ndf_out['Pass_Class'].value_counts()","85223ea6":"df_out['Port_Embark'].value_counts()","ddac2cd5":"df_out['Cabin_Id'].value_counts()","307427a8":"df_train['Is_Alone'] = np.where((df_train['Sibli_Aboard'] > 0) | \n                                (df_train['Par_Child_Aboard'] > 0), 0, 1)\n\ndf_train.head(3)","aa21641e":"df_survivors = df_train['Survived'].value_counts().reset_index()\n\ndf_survivors.rename(columns = {'index': 'Survived', \n                               'Survived': 'Number'}, \n                    inplace = True)\n\ndf_survivors['Survived'] = np.where(df_survivors['Survived'] == 1, 'Yes', 'No')\n\ndf_survivors","316d42b2":"ax = sns.barplot(x = \"Survived\", \n                 y = \"Number\", \n                 data = df_survivors)\n\nplt.xlabel('Survivors') # add to x-label to the plot\nplt.ylabel('Number of Passengers') # add y-label to the plot\nplt.title('RMS Titanic passenger information') # add title to the plot\n\nfor p in ax.patches:\n        ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.4, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')\nplt.show()","114653b3":"df_gender_survivor = df_train.pivot_table( values = 'Survived',index = 'Sex', aggfunc = 'sum')\ndf_gender_survivor","c45e19b3":"ax = df_gender_survivor['Survived'].plot(kind='bar', figsize=(10, 6), color = ['orange', 'turquoise'])\n\nax.set_xticklabels(ax.get_xticklabels(), \n                   rotation = 0)\n\nplt.xlabel('Survivors')\n\nplt.ylabel('Number\\n of\\n Passengers', \n           labelpad = 50, \n           rotation = 0)\n\nplt.title('Survivors by Gender')\n\nfor p in ax.patches:\n        ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')\n\nplt.show()","853aed69":"df_gender_survivor = df_train.groupby(['Sex', 'Survived'])[['Survived']].count()\n\ndf_gender_survivor.rename(columns = {'Survived': 'Number'}, \n                          index={0: 'No', \n                                 1:'' 'Yes',\n                                 'male' : 'Male',\n                                 'female': 'Female'},\n                    inplace = True)\n\ndf_gender_survivor","1a7da85b":"df = df_gender_survivor.reset_index()\n\nplt.figure(figsize=(10, 10))\n\nax = sns.barplot(x = \"Sex\", y = \"Number\", hue = \"Survived\", data = df, palette=\"vlag\")\n\nax.tick_params(axis = 'both', \n               which = 'major', \n               labelsize = 14)\n\nfor p in ax.patches:\n        ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.2, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black', fontsize=14)\n\nplt.xlabel('Gender',\n           fontsize = 16)\n\nplt.ylabel('Number\\n of\\n Passengers', \n           labelpad = 50, \n           rotation = 0, \n           fontsize = 16)\n\nplt.title('List of Survivors and Deaths by Gender', \n          fontsize = 20)\n\nplt.show()","d648989a":"df_gender_survivor = df_train.groupby(['Sex', 'Survived', 'Is_Alone'])[['Survived']].count()\n\ndf_gender_survivor.rename(columns = {'Survived': 'Number'}, \n                          index={0: 'No', \n                                 1:'' 'Yes',\n                                 'male' : 'Male',\n                                 'female': 'Female'},\n                    inplace = True)\n\ndf_gender_survivor","3f951c6c":"df = df_gender_survivor.reset_index()\n\nax = sns.factorplot(x='Is_Alone', \n                    y='Number', \n                    hue='Survived', \n                    col='Sex', \n                    data = df, \n                    kind='bar',\n                   palette = 'hls')\n\n\nax.set_xlabels('Passenger is Alone', fontsize = 14)\n\nax.set_ylabels('Number\\n of\\n Passengers', \n               labelpad = 60, \n               rotation = 0, \n               fontsize = 14)\n\naxes = ax.axes.flatten()\naxes[0].set_title(\"Survivor information for MALE passengers\")\naxes[1].set_title(\"Survivor information for FEMALE passengers\")\n\nplt.show()","a716df09":"df = df_train.loc[df_train['Survived'] == 1]\n\ndf_gender_survivor = df.groupby(['Sex', 'Age_Gr'])[['Survived']].count()\n\ndf_gender_survivor.rename(columns = {'Survived': 'Number'}, \n                          index={0: 'No', \n                                 1:'' 'Yes',\n                                 'male' : 'Male',\n                                 'female': 'Female'},\n                    inplace = True)\n\ndf_gender_survivor","f56bf562":"df = df_gender_survivor.reset_index()\n\nplt.figure(figsize=(10, 10))\n\nax = sns.barplot(x = \"Sex\", y = \"Number\", hue = \"Age_Gr\", data = df, palette=\"hls\")\n\nax.tick_params(axis = 'both', \n               which = 'major', \n               labelsize = 14)\n\nfor p in ax.patches:\n        ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.05, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black', fontsize=14)\n\nplt.xlabel('Gender',\n           fontsize = 16)\n\nplt.ylabel('Number\\n of\\n Passengers', \n           labelpad = 50, \n           rotation = 0, \n           fontsize = 16)\n\n\n\nplt.title('Survivors according to their Age Group', \n          fontsize = 20)\n\n\nplt.show()","ce8ab61d":"df_gender_survivor = df_train.groupby(['Age_Gr'])[['Survived']].count()\n\ndf_gender_survivor.rename(columns = {'Survived': 'Number'}, \n                           inplace = True)\n\ndf_gender_survivor","f3066958":"df = df_gender_survivor.reset_index()\n\nplt.figure(figsize=(15, 10))\n\nax = sns.barplot(x = \"Age_Gr\", y = \"Number\", data = df, palette=\"hls\")\n\nax.tick_params(axis = 'both', \n               which = 'major', \n               labelsize = 14)\n\nfor p in ax.patches:\n        ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.4, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black', fontsize=14)\n\nplt.xlabel('Age Group',\n           fontsize = 16)\n\nplt.ylabel('Number\\n of\\n Passengers', \n           labelpad = 50, \n           rotation = 0, \n           fontsize = 16)\n\nplt.title('Passengers according to their Age Group', \n          fontsize = 20)\n\nplt.show()","7a5514cb":"df_gender_survivor = df_train.groupby(['Deck', 'Survived'])[['Survived']].count()\n\ndf_gender_survivor.rename(columns = {'Survived': 'Number'}, \n                          index={0: 'No', \n                                 1:'' 'Yes'},\n                           inplace = True)\n\ndf_gender_survivor","bd3d08c9":"df = df_gender_survivor.reset_index()\n\nax = sns.factorplot(x='Deck', \n                    y='Number', \n                    col='Survived', \n                    data = df, \n                    kind='bar',\n                   palette = 'hls')\n\n\nax.set_xlabels('Deck', fontsize = 14)\n\nax.set_ylabels('Number\\n of\\n Passengers', \n               labelpad = 60, \n               rotation = 0, \n               fontsize = 14)\n\naxes = ax.axes.flatten()\naxes[0].set_title(\"Number of DEATHS per deck\")\naxes[1].set_title(\"Number of SURVIVORS per deck\")\n\nplt.show()","465b0d7f":"df_gender_survivor = df_train.groupby(['Pass_Class', 'Survived'])[['Survived']].count()\n\ndf_gender_survivor.rename(columns = {'Survived': 'Number'}, \n                           inplace = True)\n\ndf_gender_survivor","674aa1cf":"df = df_gender_survivor.reset_index()\ndf['Survived'] = np.where(df['Survived'] == 1, 'Yes', 'No')\n\nplt.figure(figsize=(10, 10))\n\nax = sns.barplot(x = \"Pass_Class\", y = \"Number\", hue = \"Survived\", data = df, palette=\"vlag\")\n\nax.tick_params(axis = 'both', \n               which = 'major', \n               labelsize = 14)\n\nfor p in ax.patches:\n        ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.2, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black', fontsize=14)\n\nplt.xlabel('Passenger Class',\n           fontsize = 16)\n\nplt.ylabel('Number\\n of\\n Passengers', \n           labelpad = 50, \n           rotation = 0, \n           fontsize = 16)\n\nplt.title('List of Survivors and Deaths by Passenger\\'s Class' , \n          fontsize = 20)\n\nplt.show()","8f5968c8":"## Show columns data types\ndf_train.dtypes","d0dd2ead":"## Column Sex\ndf_train['Sex'].unique()","0f595283":"## Column Age_Group\ndf_train['Age_Gr'].unique()","ff983602":"df_train_treat = pd.get_dummies(df_train, \n                                columns=['Sex', 'Age_Gr', 'Port_Embark'], \n                                drop_first = True, \n                                prefix = ['Sex', 'Age_Gr', 'Port_Embark'],\n                                prefix_sep='_')\n\ndf_train_treat.head(3)","7c75ca1d":"## Show columns data types\ndf_train_treat.dtypes","f6f8f5a3":"def fn_generate_histogram(p_df_dataframe, p_colum, p_num_desc_bar_adjust = 0):\n    \n    plt.title(\"Histograma of column [{}]\".format(p_colum), \n              fontsize = 16)\n    count, bin_edges = np.histogram(p_df_dataframe[p_colum])   \n    ax =  p_df_dataframe[p_colum].plot(kind = 'hist', \n                         xticks = bin_edges, \n                         figsize=(10, 10))\n    plt.ylabel('Frequency', \n           labelpad = 50, \n           rotation = 0, \n           fontsize = 16)\n    \n    plt.xlabel('{}'.format(p_colum), \n           fontsize = 16)\n    \n    for p in ax.patches:\n        ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+p_num_desc_bar_adjust, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black', fontsize=14)\n    \n    \n    plt.show()","5cf4bd27":"fn_generate_histogram(df_train_treat, 'Age', 4)","fde59b30":"fn_generate_histogram(df_train_treat, 'Pass_Fare', 11)","d08cfaf8":"df_train_treat_corr = df_train_treat.corr()\ndf_train_treat_corr","e8405564":"## Size of figure\nplt.figure(figsize = (30, 18))\n\n## Creating the heatmap\nax = sns.heatmap(df_train_treat_corr, \n                       vmin = -1, \n                       cmap = 'coolwarm',\n                       annot = True)\n\n## Configuring some characteristics of the chart, such as axis rotation and font size\nax.set_xticklabels(ax.get_xticklabels(), \n                         rotation = 35)\n\nax.tick_params(axis = 'both', \n               which = 'major', \n               labelsize = 14)\n\n## Title of Plot\nplt.title('Heat Map of Correlation' , \n          fontsize = 26)\n\n## Show Figure\nplt.show()","92aadd9d":"## Dependent Variable\ntarget = df_train_treat[\"Survived\"]\n\n## Independents Variables\nexpl = df_train_treat.drop(columns = ['Survived','Name', 'Ticket_Id', 'Cabin_Id', 'Deck'], \n                           axis=1)","53bb80dc":"def fn_split_bases_training_teste(p_df_x_var, p_df_y_var, p_test_size ,p_random_state):\n    \"\"\"\n        Description:\n            Function that separates a database into two bases, more precisely the training and test bases.\n            The bases are separated in a similar proportion in relation to the target variable\n\n        Keyword arguments:\n            p_df_x_var -- Object containing only the target variables\n            p_df_y_var -- Object containing only the explanatory variables\n            p_test_size -- What percentage of data should be assigned to the test base\n            p_random_state -- Seed to the random generator\n\n        Return:\n            x_train -- Training base that corresponds to the independent variables.\n            x_test -- Test base that corresponds to the independent variables.\n            y_train -- Training base that corresponds to the dependent \/ target variable\n            y_test -- Test base that corresponds to the independent\/ target variable\n\n        Exception:\n            None\n    \"\"\"\n    y_all = p_df_x_var\n    x_all = p_df_y_var\n    \n    x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size = p_test_size ,random_state = p_random_state)\n    \n    print('Number of observations in the Training base: {} \\nNumber of observations in the Test base: {}'.format(len(x_train),len(x_test)))\n    \n    return  x_train, x_test, y_train, y_test","ba2e66d2":"x_train, x_test, y_train, y_test = fn_split_bases_training_teste(target, expl, 0.3, 123)","53e6bf29":"## Distribution of the target variable in the training and test bases\nprint(\"\\n Training\")\nprint(y_train.value_counts() \/ len(y_train))\nprint(\"\\n Test\")\nprint(y_test.value_counts() \/ len(y_test))","74c91d20":"x_train","e1f3da3c":"def fn_calc_model_accuracy(p_y_train, \n                           p_y_test, \n                           p_y_pred_train, \n                           p_y_score_train, \n                           p_y_pred_test, \n                           p_y_score_test, \n                           model = 'Not Informed',\n                           p_first_index = False):\n    \"\"\"\n        Description:\n            Function that calculates the accuracy of a model\n\n        Keyword arguments:\n            p_y_train -- Training base that corresponds to the dependent \/ target variable\n            p_y_test -- Test base that corresponds to the dependent \/ target variable\n            p_y_pred_train -- Object with the predicted value of the target variable, from the training base\n            p_y_score_train -- Object with the estimated probabilities of the training base\n            p_y_pred_test -- Object with the predicted value of the target variable, from the test base\n            p_y_score_test -- Object with the estimated probabilities of the test base\n            model(Defaul Value) -- Name of the model used in predictions\n\n        Return:\n            acc_train -- Accuracy of training base \n            gini_train -- Gini coefficient from training base \n            roc_auc_train -- Roc Cruve value from training base \n            acc_test --  Accuracy of test base \n            gini_test -- Gini coefficient from test base \n            roc_auc_test -- Roc Cruve value from test base \n\n        Exception:\n            None\n    \"\"\"\n    acc_train = round(accuracy_score(p_y_pred_train, p_y_train) * 100, 2)\n    \n    acc_test = round(accuracy_score(p_y_pred_test, p_y_test) * 100, 2)\n    \n    y_score_train = p_y_score_train[:, 1] if (p_first_index == False) else p_y_score_train\n    y_score_teste = p_y_score_test[:, 1] if (p_first_index == False) else p_y_score_test\n    \n    fpr_train, tpr_train, thresholds = roc_curve(p_y_train, y_score_train)\n    roc_auc_train = 100 * round(auc(fpr_train, tpr_train), 2)\n    gini_train = 100 * round((2 * roc_auc_train\/ 100 - 1), 2)\n\n    # \n    fpr_test, tpr_test, thresholds = roc_curve(p_y_test, y_score_teste)\n    roc_auc_test = 100 * round(auc(fpr_test, tpr_test), 2)\n    gini_test = 100 * round((2 * roc_auc_test\/100 - 1), 2)\n    \n    print('Model - ', model)\n    print('----Taining Base----\\nAccuracy: {} \\nGini: {} \\nROC Curve: {}'.format(acc_train, \\\n                                                                                     gini_train, \\\n                                                                                     roc_auc_train))\n\n    print('\\n----Test Base----\\nAccuracy: {} \\nGini: {} \\nROC Curve: {}'.format(acc_test, \\\n                                                                                      gini_test, \\\n                                                                                      roc_auc_test))\n    \n    return acc_train, gini_train, roc_auc_train, acc_test, gini_test, roc_auc_test","d61a5816":"def fn_model_gaussian_naive(p_x_train, p_y_train, p_x_test):\n    \"\"\"\n        Description:\n            Function that executes the gaussian model according to the passed information\n\n        Keyword arguments:\n            p_x_train -- Training base that corresponds to the independent variables\n            p_y_train -- Training base that corresponds to the dependent \/ target variable\n            p_x_test -- Test base that corresponds to the independent variables\n           \n\n        Return:\n            y_pred_gaussian_train -- Object with the predicted value of the target variable, from the training base\n            y_score_gaussian_train -- Object with the estimated probabilities of the training base \n            y_pred_gaussian_test -- Object with the predicted value of the target variable, from the test base\n            y_score_gaussian_test --  Object with the estimated probabilities of the test base\n            \n\n        Exception:\n            None\n    \"\"\"\n    gaussian = GaussianNB()\n    gaussian.fit(p_x_train, p_y_train)\n\n    # \n    y_pred_gaussian_train = gaussian.predict(p_x_train)\n    y_score_gaussian_train = gaussian.predict_proba(p_x_train)\n\n    # \n    y_pred_gaussian_test = gaussian.predict(p_x_test)\n    y_score_gaussian_test = gaussian.predict_proba(p_x_test)\n    \n    return y_pred_gaussian_train, y_score_gaussian_train, y_pred_gaussian_test, y_score_gaussian_test","10e3c750":"def fn_model_logistic_regression(p_x_train, p_y_train, p_x_test):\n    \"\"\"\n        Description:\n            Function that executes the logistic regression model according to the passed information\n\n        Keyword arguments:\n            p_x_train -- Training base that corresponds to the independent variables\n            p_y_train -- Training base that corresponds to the dependent \/ target variable\n            p_x_test -- Test base that corresponds to the independent variables\n           \n\n        Return:\n            y_pred_logreg_train -- Object with the predicted value of the target variable, from the training base\n            y_score_logreg_train -- Object with the estimated probabilities of the training base \n            y_pred_logreg_test -- Object with the predicted value of the target variable, from the test base\n            y_score_logreg_test --  Object with the estimated probabilities of the test base\n            \n\n        Exception:\n            None\n    \"\"\"\n    logreg = LogisticRegression(solver = 'liblinear')\n    logreg.fit(p_x_train, p_y_train)\n\n    # \n    y_pred_logreg_train = logreg.predict(p_x_train)\n    y_score_logreg_train = logreg.predict_proba(p_x_train)\n\n    # \n    y_pred_logreg_test = logreg.predict(p_x_test)\n    y_score_logreg_test = logreg.predict_proba(p_x_test)\n    \n    return y_pred_logreg_train, y_score_logreg_train, y_pred_logreg_test, y_score_logreg_test","d0c56701":"def fn_model_SVM(p_x_train, p_y_train, p_x_test):\n    \"\"\"\n        Description:\n            Function that executes the SVM model according to the passed information\n\n        Keyword arguments:\n            p_x_train -- Training base that corresponds to the independent variables\n            p_y_train -- Training base that corresponds to the dependent \/ target variable\n            p_x_test -- Test base that corresponds to the independent variables\n           \n\n        Return:\n            y_pred_svc_train -- Object with the predicted value of the target variable, from the training base\n            y_score_svc_train -- Object with the estimated probabilities of the training base \n            y_pred_svc_test -- Object with the predicted value of the target variable, from the test base\n            y_score_svc_test --  Object with the estimated probabilities of the test base\n            \n\n        Exception:\n            None\n    \"\"\"\n    svc = SVC()\n\n    svc.fit(p_x_train, p_y_train)\n\n    y_pred_svc_train = svc.predict(p_x_train)\n    y_score_svc_train = 1\/(1+np.exp(-svc.decision_function(p_x_train)))\n\n    y_pred_svc_test = svc.predict(p_x_test)\n    y_score_svc_test = 1\/(1+np.exp(-svc.decision_function(p_x_test)))\n    \n    return y_pred_svc_train, y_score_svc_train, y_pred_svc_test, y_score_svc_test   \n    ","10e24cea":"def fn_model_descicion_tree(p_x_train, p_y_train, p_x_test, p_max_depth, p_random_state ):\n    \"\"\"\n        Description:\n            Function that executes the Decision Tree model according to the passed information\n\n        Keyword arguments:\n            p_x_train -- Training base that corresponds to the independent variables\n            p_y_train -- Training base that corresponds to the dependent \/ target variable\n            p_x_test -- Test base that corresponds to the independent variables\n            p_max_depth -- The maximum depth of the tree\n            p_random_state -- Seed to the random generator\n           \n\n        Return:\n            y_pred_dectree_train -- Object with the predicted value of the target variable, from the training base\n            y_score_dectree_train -- Object with the estimated probabilities of the training base \n            y_pred_dectree_test -- Object with the predicted value of the target variable, from the test base\n            y_score_dectree_test --  Object with the estimated probabilities of the test base\n            \n\n        Exception:\n            None\n    \"\"\"\n    dectree = DecisionTreeClassifier(criterion = 'entropy',\n                                     max_depth = p_max_depth,\n                                     random_state = p_random_state)\n    \n    dectree.fit(p_x_train, p_y_train)\n\n\n    # Treino\n    y_pred_dectree_train = dectree.predict(p_x_train)\n    y_score_dectree_train = dectree.predict_proba(p_x_train)[:,1]\n\n    # Teste\n    y_pred_dectree_test = dectree.predict(p_x_test)\n    y_score_dectree_test = dectree.predict_proba(p_x_test)[:,1]\n    \n    return y_pred_dectree_train, y_score_dectree_train, y_pred_dectree_test, y_score_dectree_test   \n    ","40f7ff01":"def fn_model_random_forest(p_x_train, p_y_train, p_x_test, p_max_depth, p_random_state ):\n    \"\"\"\n        Description:\n            Function that executes the Random Forest model according to the passed information\n\n        Keyword arguments:\n            p_x_train -- Training base that corresponds to the independent variables\n            p_y_train -- Training base that corresponds to the dependent \/ target variable\n            p_x_test -- Test base that corresponds to the independent variables\n            p_max_depth -- The maximum depth \n            p_random_state -- Seed to the random generator\n           \n\n        Return:\n            y_pred_rndforest_train -- Object with the predicted value of the target variable, from the training base\n            y_score_rndforest_train -- Object with the estimated probabilities of the training base \n            y_pred_rndforest_test -- Object with the predicted value of the target variable, from the test base\n            y_score_rndforest_test --  Object with the estimated probabilities of the test base\n            \n\n        Exception:\n            None\n    \"\"\"\n    rndforest = RandomForestClassifier(criterion = 'entropy',\n                                       max_depth = p_max_depth,\n                                       random_state = p_random_state)\n    \n    rndforest.fit(p_x_train, p_y_train)\n\n    # Treino\n    y_pred_rndforest_train = rndforest.predict(p_x_train)\n    y_score_rndforest_train = rndforest.predict_proba(p_x_train)[:,1]\n\n    # Teste\n    y_pred_rndforest_test = rndforest.predict(p_x_test)\n    y_score_rndforest_test = rndforest.predict_proba(p_x_test)[:,1]\n    \n    return y_pred_rndforest_train, y_score_rndforest_train, y_pred_rndforest_test, y_score_rndforest_test   \n    ","fdb9bff2":"def fn_model_gradient_boosting(p_x_train, p_y_train, p_x_test, p_min_samples_leaf ):\n    \"\"\"\n        Description:\n            Function that executes the Random Forest model according to the passed information\n\n        Keyword arguments:\n            p_x_train -- Training base that corresponds to the independent variables\n            p_y_train -- Training base that corresponds to the dependent \/ target variable\n            p_x_test -- Test base that corresponds to the independent variables\n            p_min_samples_leaf -- Number of samples required to be at a leaf node\n           \n\n        Return:\n            y_pred_rndforest_train -- Object with the predicted value of the target variable, from the training base\n            y_score_rndforest_train -- Object with the estimated probabilities of the training base \n            y_pred_rndforest_test -- Object with the predicted value of the target variable, from the test base\n            y_score_rndforest_test --  Object with the estimated probabilities of the test base\n            \n\n        Exception:\n            None\n    \"\"\"\n    \n\n    gbc = GradientBoostingClassifier(min_samples_leaf = p_min_samples_leaf)\n\n    gbc.fit(p_x_train, p_y_train)\n\n    # Treino\n    y_pred_gbc_train = gbc.predict(p_x_train)\n    y_score_gbc_train = gbc.predict_proba(p_x_train)[:,1]\n\n    # Teste\n    y_pred_gbc_test = gbc.predict(p_x_test)\n    y_score_gbc_test = gbc.predict_proba(p_x_test)[:,1]\n    \n    return y_pred_gbc_train, y_score_gbc_train, y_pred_gbc_test, y_score_gbc_test   \n    ","473f22e0":"y_pred_gaussian_train, y_score_gaussian_train, y_pred_gaussian_test, y_score_gaussian_test = fn_model_gaussian_naive(x_train, \n                                                                                                                     y_train, \n                                                                                                                     x_test)","862e9801":"acc_gau_train, gini_gau_train, roc_gau_train, acc_gau_test, gini_gau_test, roc_auc_gau_test = fn_calc_model_accuracy(y_train, \n                                                                                                 y_test, \n                                                                                                 y_pred_gaussian_train,\n                                                                                                 y_score_gaussian_train,\n                                                                                                 y_pred_gaussian_test,\n                                                                                                 y_score_gaussian_test,\n                                                                                                 'GAUSSIAN NAIVE BAYES')","04603704":"columns = ['Pass_Class', 'Age', 'Is_Alone', 'Pass_Fare', 'Sex_male']\nx_train_treat = x_train.loc[:,columns]\nx_test_treat = x_test.loc[:, columns]\n\ny_pred_gaussian_train, y_score_gaussian_train, y_pred_gaussian_test, y_score_gaussian_test = fn_model_gaussian_naive(x_train_treat, \n                                                                                                                     y_train, \n                                                                                                                     x_test_treat)\n\nacc_gau_train, gini_gau_train, roc_gau_train, acc_gau_test, gini_gau_test, roc_auc_gau_test = fn_calc_model_accuracy(y_train, \n                                                                                                 y_test, \n                                                                                                 y_pred_gaussian_train,\n                                                                                                 y_score_gaussian_train,\n                                                                                                 y_pred_gaussian_test,\n                                                                                                 y_score_gaussian_test,\n                                                                                                 'GAUSSIAN NAIVE BAYES')","7c6c224e":"y_pred_log_train, y_score_log_train, y_pred_log_test, y_score_log_test = fn_model_logistic_regression(x_train,\n                                                                                                      y_train, \n                                                                                                      x_test)\n\nacc_log_train, gini_log_train, roc_log_train, acc_log_test, gini_log_test, roc_auc_log_test = fn_calc_model_accuracy(y_train, \n                                                                                                 y_test, \n                                                                                                 y_pred_log_train,\n                                                                                                 y_score_log_train,\n                                                                                                 y_pred_log_test,\n                                                                                                 y_score_log_test,\n                                                                                                 'LOGISTIC REGRESSION')","38437677":"y_pred_svm_train, y_score_svm_train, y_pred_svm_test, y_score_svm_test = fn_model_SVM(x_train,\n                                                                                      y_train, \n                                                                                      x_test)\n\nacc_svm_train, gini_svm_train, roc_svm_train, acc_svm_test, gini_svm_test, roc_auc_svm_test = fn_calc_model_accuracy(y_train, \n                                                                                                                     y_test, \n                                                                                                                     y_pred_svm_train,\n                                                                                                                     y_score_svm_train,\n                                                                                                                     y_pred_svm_test,\n                                                                                                                     y_score_svm_test,\n                                                                                                                     'SUPPORT VECTOR MACHINE',\n                                                                                                                     True)","fa821c27":"y_pred_tree_train, y_score_tree_train, y_pred_tree_test, y_score_tree_test = fn_model_descicion_tree(x_train,\n                                                                                                     y_train,\n                                                                                                     x_test,\n                                                                                                     5,\n                                                                                                     42)\n\nacc_tree_train, gini_tree_train, roc_tree_train, acc_tree_test, gini_tree_test, roc_auc_tree_test = fn_calc_model_accuracy(y_train, \n                                                                                                                           y_test, \n                                                                                                                           y_pred_tree_train,\n                                                                                                                           y_score_tree_train,\n                                                                                                                           y_pred_tree_test,\n                                                                                                                           y_score_tree_test,\n                                                                                                                           'DECISION TREE',\n                                                                                                                           True)","c9c3902b":"y_pred_rndfor_train, y_score_rndfor_train, y_pred_rndfor_test, y_score_rndfor_test = fn_model_random_forest(x_train,\n                                                                                                            y_train,\n                                                                                                            x_test,\n                                                                                                            4,\n                                                                                                            42)\n\nacc_rndfor_train, gini_rndfor_train, roc_rndfor_train, acc_rndfor_test, gini_rndfor_test, roc_auc_rndfor_test = fn_calc_model_accuracy(y_train, \n                                                                                                                            y_test, \n                                                                                                                           y_pred_rndfor_train,\n                                                                                                                           y_score_rndfor_train,\n                                                                                                                           y_pred_rndfor_test,\n                                                                                                                           y_score_rndfor_test,\n                                                                                                                           'RANDOM FOREST',\n                                                                                                                           True)","f2371e95":"y_pred_gbc_train, y_score_gbc_train, y_pred_gbc_test, y_score_gbc_test = fn_model_gradient_boosting(x_train,\n                                                                                                    y_train,\n                                                                                                    x_test,\n                                                                                                    6)\n\nacc_gbc_train, gini_gbc_train, roc_gbc_train, acc_gbc_test, gini_gbc_test, roc_auc_gbc_test = fn_calc_model_accuracy(y_train, \n                                                                                                                            y_test, \n                                                                                                                           y_pred_gbc_train,\n                                                                                                                           y_score_gbc_train,\n                                                                                                                           y_pred_gbc_test,\n                                                                                                                           y_score_gbc_test,\n                                                                                                                           'GRADIENT BOOSTING',\n                                                                                                                           True)","afeba29e":"\n\nmodels = pd.DataFrame({\n    'Model': ['Decision Tree', \n               'Random Forest', \n               'Gradient Boosting',\n               'Support Vector Machine(SVM)',\n               'Logistic Regression',\n               'Gaussian Naive Bayes'],\n    \n     'Train Accuracy': [acc_tree_train,\n                        acc_rndfor_train,\n                        acc_gbc_train,\n                        acc_svm_train,\n                        acc_log_train, \n                        acc_gau_train],   \n    \n    \n    'Test Accuracy': [acc_tree_test,\n                      acc_rndfor_test,\n                      acc_gbc_test,\n                      acc_svm_test,\n                      acc_log_test,\n                      acc_gau_test]\n\n})\nmodel_comp = models.sort_values(by = 'Test Accuracy', \n                                ascending = False)\nmodel_comp = model_comp[['Model','Train Accuracy','Test Accuracy']]\nmodel_comp\n\n","75fe8b60":"x_train = x_train.drop(columns = ['Port_Embark_Not Info'], \n                           axis=1)\n\ndf_test['Is_Alone'] = np.where((df_test['Sibli_Aboard'] > 0) | \n                                (df_test['Par_Child_Aboard'] > 0), 0, 1)\n\ndf_test_treat = pd.get_dummies(df_test, \n                                columns=['Sex', 'Age_Gr', 'Port_Embark'], \n                                drop_first = True, \n                                prefix = ['Sex', 'Age_Gr', 'Port_Embark'],\n                                prefix_sep='_')\n\ndf_test_treat = df_test_treat.drop(columns = ['Name', 'Ticket_Id', 'Cabin_Id', 'Deck'], \n                           axis=1)\n\ndf_test_treat.head(3)","e3d1c207":"y_pred_gbc_train, y_score_gbc_train, y_pred_gbc_test, y_score_gbc_test = fn_model_gradient_boosting(x_train,\n                                                                                                    y_train,\n                                                                                                    df_test_treat,\n                                                                                                    6)","dd603d81":"df_test_treat = df_test_treat.reset_index()\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": df_test_treat[\"PassengerId\"],\n        \"Survived\": y_pred_gbc_test\n    })\n\n#submission.to_csv('submission.csv', index = False)","46a9ea36":"<p>The column <i>Fare<\/i> also has null values in the  dataframe <b>df_test<\/b> , so it will be necessary to observe the column in greater detail.<\/p>","2a28dd15":"<p>With the insights gathered and treatments carried out along the base, we will now begin the treatment of some categorical variables, the famous process of creating <b>Dummy variables<\/b>. Then we will work with the models.<\/p>","07736e1d":"<h2>Conclusion<\/h2>\n<p>What I can express is that I was very happy with the challenge, it helped me to improve a lot both in the analysis process and in the use of the tool and its libraries.<\/p>","33ca3056":"<h2>Ph-6: Generate Submission File<\/h2>","8e5137a1":"<p>Perform the treatments on the dataframe that corresponds to the file 'test.csv'<\/p>","2a098bae":"<p>Next we will show the correlation between the dataframe variables. For that we will use both a table and a heat map<\/p>","6b1fd886":"<p>Of the 342 survivors, most were women, more precisely 233 passengers.<\/p>","394262e1":"<p>For a better understanding of the data, the columns of the dataframes will be renamed.<\/p>","99ae7e9d":"<h4>Support Vector Machine (SVM)<\/h4>","bb38526e":"<p><strong>Number of survivors<\/strong><\/p>","586f9147":"<p>Up to that point we have already dealt with some null values and also outliers.<br><br>\n\nNow some plots will be made to capture some insights about the data, but first we will create a column that will determine if the passenger is accompanying a family member.\n<\/p>","88eb0eb6":"<h2><font color=\"red\">Ph-0: Understanding the problem<\/font><\/h2>\n<p>The main objective of this competition is to identify the passengers of the RMS Titanic who survived or not, thus highlighting the variables responsible for these scenarios, through a prediction model.<\/p>","4660ba77":"<p>The accuracy of the training and test bases was less than 70%, indicating that we need more information to increase the score, or that we have a lot of information.<br><br>\n\nWe will use some specific columns of the dataframe to apply to the model and thus check if the accuracy increases. In this case we will use the variables \/ columns that have a stronger correlation, in this case the variables: <b>'Pass_Class', 'Age', 'Is_Alone', 'Pass_Fare', 'Sex_male'<\/b><\/p>","41594e20":"<p>We obtained a significant increase in the model's score on both bases.<br><br>\n\nIn the future we will be able to use other columns or even capture new insights to insert in this model.<\/p>","edf60d2c":"<p>We identified the problem, basically there are passengers with the same ticket.<br><br>\n\nMost likely it is a family, friends or maybe people who are in the same cabin, where a single ticket was generated with the value of all members.<br><br>\n\nWe can solve this problem by dividing the total ticket value by the number of individuals associated<\/p>","bc10a37b":"<p>Show the <font color=\"blue\">first three rows<\/font> of each dataframe<\/p>","c4a87ec7":"<h3>Models<\/h3>\n<p>In the following lines, with the database treated and correctly divided, we apply the functions to compute the models and at the same time to show us the accuracy score that them produces.<\/p>","a996fb31":"<p>Now, we will evaluate column by column and decide what to do.<\/p>","bd0e4499":"<p>Coincidentally the dataframes df_test and dt_train, have outliers in the same columns.\n\nWe must investigate these outliers<\/p>","6318d660":"<h2>Ph-2: Load the Data<\/h2>\n<p>In this phase the data is captured and used in memory through the pandas library, where we build a tabular structure known as <b>DataFrame<\/b> that corresponds to each of the spreadsheets \/ tables provided.<\/p>","2efed665":"<p>The previous graph is a summary of some of the information previously collected, such as number of survivors and their distribution in relation to gender.<\/p>","427710c8":"<p>Next, we will use a histogram to check the distribution of some values throughout the dataset, for example the distribution of the ages and tariffs of passengers.<br><br>\n\nAs the same code will be replicated twice, or more depending on the situation, we will put the plotting process in a function<\/p>","9bf88f6c":"<h2>Ph-5: Select Variables<\/h2>\n<p>In this phase we establish which is the <b>target variable \/ dependent variable<\/b>, and which <b>explanatory \/ independent variables<\/b> we will use throughout the model.<\/p>","e62c228b":"<p>According to the information, shown in the chart above, most survivors were over 17 years of age.\n\nTo find out if this number really makes sense, we will make an age distribution over the entire dataset.<\/p>","cc7fbf8a":"<h3>Checking Nulls<\/h3>\n<p>Identifying the percentage of null values in each dataframe, and for each column.<\/p>","e6e8ce57":"<h2>Ph-3: Data Checking<\/h2>\n<p>In this phase we will only understand the data, more precisely the most basic structure of the tables \/ dataframes.<\/p>\n\n","dfcbb5bd":"<p>According to the cross table, there is not much to say about the null values of the cabins, which implies that it was just <b>Not Informed<\/b>, or even <b>Unknown<\/b> <\/p>","ae68b8ef":"<p>Check unique values by categorical column<\/p>","a9806f7b":"<h3>Data Dictionary<\/h3>\n\n\n|Variable\t|Definition                                |Key                                           |\n|-----------|------------------------------------------|----------------------------------------------|\n|survival \t|Survival \t                               |0 = No, 1 = Yes                               |\n|pclass \t|Ticket class \t                           |1 = 1st, 2 = 2nd, 3 = 3rd                     |\n|sex \t    |Sex                                       |\t                                          |\n|Age \t    |Age in years \t                           |                                              |\n|sibsp \t    |# of siblings \/ spouses aboard the Titanic|                                              | \t\n|parch \t    |# of parents \/ children aboard the Titanic| \t                                          |\n|ticket \t|Ticket number                             |                                              |\t\n|fare \t    |Passenger fare                            |                                              |\t\n|cabin \t    |Cabin number                              |                                              |\t\n|embarked \t|Port of Embarkation \t                   |C = Cherbourg, Q = Queenstown, S = Southampton|\n\n<b>Variable Notes<\/b>\n\n<b>pclass<\/b>: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\n<b>age<\/b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n<b>sibsp<\/b>: The dataset defines family relations in this way...\n<b>Sibling<\/b> = brother, sister, stepbrother, stepsister\n<b>Spouse<\/b> = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n<b>parch:<\/b> The dataset defines family relations in this way...\n<b>Parent<\/b> = mother, father\n<b>Child<\/b> = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","a5d9da93":"<p>Many outliers were detected in the field corresponding to the passenger's fare, what remains for us to know is to check if all these outliers correspond to first class passengers.<\/p>","ddda06fb":"<p>With the graph and tables above, we can say that most of the men who survived were alone, and most of the surviving women were accompanied.<\/p>","ece21ae4":"<p>An important piece of information that we were able to gather in this data set is that, as always, many cabins have more than one passenger. This could suggest the creation of a column that contained the number of people who already occupy the cabin.<br><br>\n\nHowever, there are approximately 687 unmarked cabins, which, given the number of passengers, makes the creation of this column irrelevant.<\/p>","7c9e583b":"<p>Now we will check if there are still null values, and after that we will apply the same treatment to the dataframe df_test.<\/p>","de0f0d8a":"<h2>Ph-4: Performing Some Analysis<\/h2>\n<p>In this phase, we will verify some basic information about the data, in order to capture some insights that can help to improve the model that will be applied.<\/p>","4102fd7a":"<h2>Ph-7: Applying Machine Learning Models<\/h2>\n<p>In this phase we apply some of the main models used in machine learning, and in the end we will choose the one with the highest score.<br><br>\n\nThe main models to be used are: <b>Gaussian Naive Bayes, Logistic Regression, Decision Tree, Random Forest, Gradient Boosting<\/b> and <b>Support Vector Machine (SVM)<\/b><\/p>","8e09aaac":"![capa_titanic_sink_wreck_widelg.jpeg](attachment:capa_titanic_sink_wreck_widelg.jpeg)\n\nImage source: https:\/\/aventurasnahistoria.uol.com.br\/noticias\/reportagem\/historia-naufragio-do-titan-o-livro-que-previu-o-naufragio-do-titanic-14-anos-antes.phtml","d55a7520":"<p>With the information passed by the graph we have that of the 891 passengers, 549 did not survive and 342 survived<\/p>","5f26c34a":"<h4>Gradient Boosting<\/h4>","0575d0a3":"<p>As was done in the process with the decision tree, some processes related to the depth of the algorithm were made, where the best score was obtained with a <i>'max_depth'<\/i> <b>equal to 4<\/b>.<\/p>","805372bf":"<p>An interesting point about the cabins is that the first letter of your identification can identify the deck where you are. Therefore, a column will be created that will correspond to the ship's deck.<\/p>","200c403c":"<h1 style=\"text-align:center\" >Titanic Data Science Competition<\/h1>\n<p>First of all, I would like to thank those who are viewing this notebook, which is nothing more than an initial project that I did in order to learn more about the \"World of Data\".<br><br>\n\nThis notebook is an analysis on dataset provided by the famous challenge <a href=\"https:\/\/www.kaggle.com\/c\/titanic\">Titanic: Machine Learning from Disaster<\/a><br><br>\n\nHere I list the main techniques I performed to complete this challenge, I hope you enjoy it.<\/p>","bec727a1":"<p><strong>The information about siblings and parents can be very useful in determining whether the person is alone.<\/strong><\/p>","75f574b4":"<h4>Model Gaussian Naive Bayes<\/h4>\n","b079567b":"<p>All outliers are first class passengers, so some of them paid more for tickets.\n\nWhat could impact the value of the ticket is the clutch location and the characteristics of the cabins.<\/p>","785fa646":"<p>Passenger class is a strong variable in relation to whether or not the shipwreck survives.<\/p>","7ead1ca6":"<p>Now we will check the null values of the column *Embarked* from the dataset *df_train* <\/p>","0b377c77":"<p>There are still outliers associated with passenger fares, it will be necessary to re-identify the situation of these outliers to take action.\n\nThe first thing that can be done is to check the social class of these outliers<\/p>","c3250c66":"<p>Although not represented in the notebook, some pruning processes were carried out on the tree, where the best score was obtained with a <i>'max_depth'<\/i> <b>equal to 5<\/b>.<\/p>","f0b98b24":"<p>Now we will check the null values of the column *Age* from the dataset *df_train* <\/p>","0a9948c0":"<p>In this scenario, there is not much to do, because:\n    <ol>\n        <li>There is no clear list for each passenger;<\/li>\n        <li>It was common for people to have many children, it is quite natural that there are many brothers on board<\/li>\n    <\/ol>\n<\/p>","94a5ac6f":"<p>The model's accuracy in the training and test bases was very good, around 80%.\n\nIn the future we can apply the same treatments made on the Gaussian model, choosing specific features<\/p>","554123b1":"<p>Unfortunately, the Deck does not have a high relationship with the survivors of the wreck, and this is because much of the information related to the cabins is unknown.<br><br>\n\nBased on the analyzes already done, gender and age have a somewhat strong relationship with the survivors. Finally, we will see a list of survivors or not related to the passenger class.<\/p>","45c21aa7":"<p>Generate the file with the data that will be compared by Kaggle's Result<\/p>","22943709":"<h2>Understanding Everything<\/h2>\n<p>Each method adopted to analyze the data and the problems presented, is divided into phases. Such phases range from the import of data, to exploratory analysis of them and finally the elaboration of the model.<\/p>","4a0551ab":"<p><strong>Number of survivors by gender<\/strong><\/p>","aaacbdd6":"<h4>Model Logistic Regression<\/h4>","bc9060d3":"<h2>Ph-4: EDA - Exploratory Data Analysis<\/h2>\n<p>In this phase we will carry out the treatment of the data, how to rename the columns to be easier to understand and work, to treat null values, identify outliers, <b>understanding de data<\/b><\/p>","cc28d05d":"<h4>Decision Tree<\/h4>","6636d9c8":"<h4>Random forest<\/h4>","dad12c80":"<p>In this scenario there is not much to do, because:<br>\n<ol>\n    <li>There is no clear relationship for each passenger;<\/li>\n    <li>A son can be accompanied only by his mother or father;<\/li>\n    <li>A parent may be in the company of only one child;<\/li>\n    <li>Some families may be composed of several children.<\/li>\n<\/ol>    \n    <\/p>","0570959c":"<p>Considering the value of what was charged for the tickets, it is surprising that there are third class passengers who pay for more expensive tickets.\nWe must identify the reason why some third class passengers paid more than the average of their tickets.<\/p>","93fac003":"<h2>Ph-1: Imports<\/h2>\n<p>Python has a <b>large number of libraries<\/b> to assist in the most varied situations \/ problems. Therefore, the main libraries that will be used in this analysis are listed below.<\/p>","fad64d5b":"<p>The datasets\/ dataframes <b>df_train<\/b> and <b>df_test<\/b> have some null information, so we will understand the context of this information and see what technique we can use to treat it<\/p>","83e3e9a2":"<h3>Identifying Outliers<\/h3>\n<p>In this phase the outliers will be identified and at the same time it will be seen whether they can be maintained or treated<\/p>","40ac8e63":"<h2>Ph-6: Test and training approach<\/h2>\n<p>In this phase we will divide the dataset into two bases: <b>Training<\/b> and <b>test<\/b>. Thus we will be able to attribute greater accuracy to the model and reduce the overfiting that may arise.<\/p>","beede44c":"<p>Something that drew attention is that there is a passenger fare with zero value, which may indicate that the passenger is part of the crew.\nWith this information, we can create a new column indicating that the passenger is part of the crew.<\/p>","21400d55":"<h3>The Best Model<\/h3>\n<p>Based on the many models applied to the problem, a table will be made illustrating the score of each one and then we will select the model with the best score on the test base.<\/p>","9e0db332":"<p>Once the outliers are listed, it will be necessary to study some of them, in order to determine what should be done.<\/p>\n<p>Next, we'll see <b>how many outliers there are per column<\/b> of each dataframe.<\/p>\n<p>Obs.: And in order not to replicate codes, some help functions will be created.<\/p>","00ba841d":"<h3>Functions<\/h3>","7f32ae7f":"<h2>Next Steps<\/h2>\n<p>\n    <ol>\n        <ul>Better understand the data and capture more insights;<\/ul>\n        <ul>Study and apply the many existing algorithms and their processes, in order to improve accuracy;<\/ul>\n        <ul>Use the train.csv file as the training base, and the test.csv file as the test base;<\/ul>\n        <ul>Many other processes that I can't remember. \ud83d\ude02<\/ul>\n    <\/ol>\n<\/p>","0b03bf01":"<p>Apparently the outliers are the elderly, and the younger children.<\/p>\n<p>In this case, we can create a column that identifies whether they are children, senior, youth and adults.<\/p>\n\n|Age Range(Years)\t|Definition                                |\n|-------------------|------------------------------------------|\n|0 - 2 |Infant |\n|3 - 12 | Childreen |\n|13 - 17 | Young |\n|18 - 24 | Young Adult|\n|25 - 44 | Adult|\n|45 - 59 | Middle-Age|\n|60 - 74 | Senior|\n|75 - 90 | Elder|\n|> 90 | Extreme Old Age|","2877e7c2":"<p>Use the Gradient Boosting model, which had the best score, to perform the prediction of the test base<\/p>","7a0e3fb4":"<h3>Understanding the data provided<\/h3>\n<p>Understanding what each dataframe, table, spreadsheet means in the general context of the project.<\/p>\n<p>\n    <ul>    \n        <li>The datasets <i>train.csv<\/i> and <i>test.csv<\/i> contain passenger information, such as socio-economic information;<\/li>\n        <li>The dataset <i>Train.csv<\/i> has more details about passengers, which is the main object of study;<\/li>\n        <li>The dataset <i>test.csv<\/i> will be used to predict the information requested by the problem;<\/li>\n        <li>The dataset <i>df_gender_submission<i\/> contains information about who survived and who did not.<\/li> \n    <\/ul>\n<\/p>"}}