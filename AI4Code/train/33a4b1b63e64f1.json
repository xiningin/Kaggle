{"cell_type":{"666363e5":"code","ac01cf3b":"code","621f3629":"code","52094315":"code","cb049acb":"code","d7a66d7e":"code","fe83fc6a":"code","b5df123d":"code","943d0a7a":"code","ff065d42":"code","e66b1dc6":"code","4b18384b":"code","6b2676b5":"code","84d657dd":"code","8b58f157":"markdown","42f8c0a1":"markdown","b592a6e6":"markdown","8c4d881f":"markdown","6678e6e7":"markdown","d24fe6c6":"markdown","32d4fcc9":"markdown","2d04f935":"markdown","64566c3c":"markdown","2d42f06d":"markdown"},"source":{"666363e5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder # For one-hot encoding\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\n# Load the data\ntrain_dataframe = pd.read_csv(\"..\/input\/train.csv\", sep=\",\")\nXsubm_dataframe = pd.read_csv(\"..\/input\/test.csv\", sep = \",\")","ac01cf3b":"print(train_dataframe.isnull().sum())","621f3629":"sns.countplot(x = \"Sex\", hue = \"Survived\", data = train_dataframe)\nplt.show()\nsns.countplot(x=\"Pclass\", hue = \"Survived\", data = train_dataframe)\nplt.show()\nsns.countplot(x=\"SibSp\", hue = \"Survived\", data = train_dataframe)\nplt.show()\nsns.countplot(x=\"Parch\", hue = \"Survived\", data = train_dataframe)\nplt.show()\nsns.countplot(x=\"Embarked\", hue = \"Survived\", data = train_dataframe)\nplt.show()\nsns.catplot(x = \"Pclass\", hue = \"Survived\", col = \"Sex\", kind = \"count\", data = train_dataframe)\nplt.show()\nsns.catplot(x = \"Parch\", hue = \"Survived\", col = \"Sex\", kind = \"count\", data = train_dataframe)\nplt.show()\nsns.catplot(x = \"SibSp\", hue = \"Survived\", col = \"Sex\", kind = \"count\", data = train_dataframe)\nplt.show()\nsns.catplot(x = 'Pclass',y = 'Survived', hue='Sex', data=train_dataframe, kind = \"point\") # equivalent to sns.pointplot\nplt.show()\nsns.catplot(x = 'Embarked',hue = 'Survived', col='Pclass', data=train_dataframe, kind = \"count\")\nplt.show()","52094315":"train_dataframe[\"Title\"] = 0\ntrain_dataframe[\"Title\"] = train_dataframe.Name.str.extract(\"([a-zA-Z]+)\\.\")\nXsubm_dataframe[\"Title\"] = 0\nXsubm_dataframe.Title = Xsubm_dataframe.Name.str.extract(\"([a-zA-Z]+)\\.\")\n\ndef replace_titles(x):\n    title=x['Title']\n    if title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col', \"Sir\"]:\n        return 'Mr'\n    elif title in ['Countess', 'Mme',\"Dona\"]:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms', \"Lady\"]:\n        return 'Miss'\n    elif title =='Dr':\n        if x['Sex']=='Male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title\n\ntrain_dataframe['Title']=train_dataframe.apply(replace_titles, axis=1)\nXsubm_dataframe['Title']=Xsubm_dataframe.apply(replace_titles, axis=1)\n\n# here I use the compact title to infer the age. To have more \"randomness\", I estimate the\n# unknown age with a normal distribution having mean and std drawn from the same \"Title\" category\nfor thistitle in list(set(train_dataframe.Title)):    \n    row = (train_dataframe.Age.isnull())&(train_dataframe.Title == thistitle)    \n    df = train_dataframe[train_dataframe.Title == thistitle][\"Age\"]    \n    mu = df.mean()\n    st = df.std() if len(df)>1 else 0        \n    value = int(mu + st*np.random.randn(1))    \n    train_dataframe.loc[row, \"Age\"] = value    \n\nfor thistitle in list(set(Xsubm_dataframe.Title)):\n    row = (Xsubm_dataframe.Age.isnull())&(Xsubm_dataframe.Title == thistitle)\n    mu = Xsubm_dataframe[Xsubm_dataframe.Title == thistitle][\"Age\"].mean()\n    st = Xsubm_dataframe[Xsubm_dataframe.Title == thistitle][\"Age\"].std() if len(Xsubm_dataframe[Xsubm_dataframe.Title == thistitle][\"Age\"])>1 else 0    \n    value = int(mu + st*np.random.randn(1))    \n    Xsubm_dataframe.loc[row, \"Age\"] = value\n\ntitle_reduced = list(set(train_dataframe.Title))\nle_title = LabelEncoder()\nle_title.fit(title_reduced)\ntrain_dataframe[\"Title_LE\"] = le_title.transform(train_dataframe.Title)\nXsubm_dataframe[\"Title_LE\"] = le_title.transform(Xsubm_dataframe.Title)","cb049acb":"age_max = max([train_dataframe.Age.max(), Xsubm_dataframe.Age.max()])\nbins = np.linspace(0, age_max, 20)\ntrain_dataframe.loc[:,\"Age_bin\"] = np.digitize(train_dataframe.Age, bins)\nXsubm_dataframe.loc[:,\"Age_bin\"] = np.digitize(Xsubm_dataframe.Age, bins)\n\n\n# ### Add Fare category\n\n# In[7]:\n\n\nfare_max = max([train_dataframe.Fare.max(), Xsubm_dataframe.Fare.max()])\nfare_bins = np.linspace(0, fare_max,10)\ntrain_dataframe.loc[:,\"Fare_bin\"] = np.digitize(train_dataframe.Fare, fare_bins)\nXsubm_dataframe.loc[:, \"Fare_bin\"] = np.digitize(Xsubm_dataframe.Fare, fare_bins)","d7a66d7e":"train_dataframe[\"Surname\"]= 0\ntrain_dataframe[\"Surname\"]= train_dataframe.Name.str.extract(\"^(.+?),\")\n\n## Correcting the SibSp when possible\n\n# Allison: looks like there is 1 Mrs. with 2 children\ntrain_dataframe.loc[297,\"SibSp\"] = 1\ntrain_dataframe.loc[297,\"Parch\"] = 1\ntrain_dataframe.loc[305,\"SibSp\"] = 1\ntrain_dataframe.loc[305,\"Parch\"] = 1\ntrain_dataframe.loc[498,\"SibSp\"] = 0\ntrain_dataframe.loc[498,\"Parch\"] = 2\n\n# Andersson: one misidentified family members (they are not)\ntrain_dataframe.loc[68, [\"SibSp\", \"Parch\"]] = [0,0]\n\n# Backstrom\ntrain_dataframe.loc[85,\"SibSp\"] = 1","fe83fc6a":"# One-hot encoding for Sex\na = train_dataframe.loc[:,\"Sex\"] == \"male\"\ntrain_dataframe.loc[:,\"Male\"] = a.astype(int)\ntrain_dataframe.loc[:,\"Female\"] = (~a).astype(int)\na = Xsubm_dataframe.loc[:,\"Sex\"] == \"male\"\nXsubm_dataframe.loc[:,\"Male\"] = a.astype(int)\nXsubm_dataframe.loc[:,\"Female\"] = (~a).astype(int)\n\n# One-hot encoding for Embark\nembark = []\ncounter = 0\nfor i in train_dataframe.loc[:,\"Embarked\"]:\n    if i not in embark:\n        embark.append(i)\n\nfor p_embarked in embark:\n    train_dataframe.loc[:,\"Embarked_\"+str(p_embarked)] = (train_dataframe.loc[:,\"Embarked\"] == p_embarked).astype(int)\n    Xsubm_dataframe.loc[:,\"Embarked_\"+str(p_embarked)] = (Xsubm_dataframe.loc[:,\"Embarked\"] == p_embarked).astype(int)\n\ntrain_dataframe.head(1)\n","b5df123d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import GridSearchCV, train_test_split","943d0a7a":"features = [\"Pclass\", \"SibSp\", \"Parch\", \"Title_LE\", \"Age_bin\", \"Fare_bin\", \n                \"Male\",\"Female\", \"Embarked_S\", \"Embarked_C\",\"Embarked_Q\",\"Embarked_nan\"]\n\nX = train_dataframe[features].values\nY  = train_dataframe.loc[:, \"Survived\"].values\nXsubm = Xsubm_dataframe[features].values\n\nXtrain, Xtest,Ytrain, Ytest = train_test_split(X, Y, test_size = 0.33)","ff065d42":"C = np.logspace(-4,1,10)\ntuned_parameters = [{\"solver\":[\"liblinear\"],\n                    \"C\": C,\n                    \"penalty\":[\"l2\",\"l1\"]},\n                   {\"solver\":[\"newton-cg\"],\n                    \"C\": C,\n                    \"penalty\":[\"l2\"]}]\n\nscores = [\"precision\"]\nfor score in scores:\n    print(\"Tuning hyperparameters for %s\" % score)\n    clf = GridSearchCV(LogisticRegression(), tuned_parameters, cv = 3,\n                      scoring = \"%s_macro\" % score)\n    clf.fit(Xtrain, Ytrain)   \n    print(\"Best parameters found: {0}. Score {1}\\n\".format(clf.best_params_, clf.best_score_))        \n    print(\"Grid scores on development set: \\n\")\n    means = clf.cv_results_[\"mean_test_score\"]\n    stds = clf.cv_results_[\"std_test_score\"]\n    for mean, std, params in zip(means, stds, clf.cv_results_[\"params\"]):\n        print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))\n    print(\"Detailed classification report:\")\n    print()\n    print(\"The model is trained on the full development set.\")\n    print(\"The scores are computed on the full evaluation set.\")\n    print()\nprint(clf.best_estimator_)","e66b1dc6":"print(\"Scores on the train dataset\")\nprint(classification_report(Ytrain, clf.best_estimator_.predict(Xtrain)))\n\nprint(\"Scores on the test dataset\")\nprint(classification_report(Ytest, clf.best_estimator_.predict(Xtest)))\nYpred = clf.best_estimator_.predict(Xtest)\naccuracy_dict = {}\naccuracy_dict[\"logReg\"] = [accuracy_score(Ytest, Ypred), clf.best_estimator_, ]\nprint(\"Accuracy: {0:.4g}%\".format(accuracy_score(Ytest, Ypred)*100))","4b18384b":"from sklearn.svm import SVC\n\nparameters = [{\"C\":[0.01,0.03,0.1,0.3,1.,3.,10],\n              \"kernel\": [\"linear\", \"rbf\", \"sigmoid\"],\n              \"gamma\": [0.01,0.03,0.1,0.3,1.,3.,10]}]\n\n\nclf = GridSearchCV(estimator=SVC(),\n                   param_grid=parameters, scoring = \"accuracy\", cv = 5)\nclf.fit(Xtrain, Ytrain)\nprint(\"Best parameters found: {0}. Score {1}\\n\".format(clf.best_params_, clf.best_score_))        \n\nYpred = clf.predict(Xtest)\naccuracy_dict[\"svm\"] = [accuracy_score(Ytest, Ypred), clf.best_estimator_]\nprint(\"Accuracy on the test set: {0}%\".format(accuracy_score(Ytest, Ypred)*100))","6b2676b5":"print(\"Model\\tAccuracy\")\nbestAccuracy = 0\nfor key in accuracy_dict.keys():    \n    print(\"{0}\\t{1}%\".format(key, accuracy_dict[key][0]))\n    if accuracy_dict[key][0]> bestAccuracy:\n        bestModel = key","84d657dd":"print(\"Best model is \" + bestModel)\n\npid = Xsubm_dataframe.loc[:,\"PassengerId\"].values\nres = np.zeros((len(pid),2), dtype = int)\nres[:,0] = pid\nres[:,1] = accuracy_dict[bestModel][1].predict(Xsubm)\nnp.savetxt(\"submission.csv\", res, header = \"PassengerId,Survived\", delimiter = \",\", fmt = \"%d\", comments = '')","8b58f157":"# My personal attempt at solving the Titanic database\n\n\nThe data elaboration has been adapted from and inspired by the contents of [this page](https:\/\/mlguy.org\/2018\/06\/11\/tutorial-kaggle-titanic-competition-exploratory-data-analysis-and-classification\/).","42f8c0a1":"## Logistic regression\n\nHere is where the fun starts...\n\nAs a first model, I tried to train a logistic regression model on the following features\n\n* Pclass\n* Sibsp\n* Parch\n* Title_LE\n* Age_bin\n* Fare_bin\n* Female\n* Male\n* Embarked_S, _C, _Q, _nan","b592a6e6":"## Exploration, integration and completion of the dataset\n\n1. Analyse the empty cells (NaNs) and try to infer their value\n2. Bin the age and converting categorical features into one-hot encoding\n3. Find correlations (or lack thereof) between the data","8c4d881f":"### Create some other new features.\n\nHere, I create two new features, the \"age range\" and the \"fare bin\" features. \nI will assign each individuals to a binned age and fare, to have more data homogeneity. \nThe idea is that small differences in age or ticket price shouldnt affect directly the survival rate.","6678e6e7":"### Analysis of the families onboard\n\nThis is the most tedious part. Using internet and cross-references in the database, I tried to check when the info in the database are partial, missing or wrong. \n\nThis section is still not complete.\n\nI don't know how to deal with the possibility that part of the family is in the train dataset and part is in the CV or test dataset.","d24fe6c6":"# Experimenting with random stuff.\n## SVM","32d4fcc9":"### Introducing one hot encoding for *Sex* and *Embarked*.","2d04f935":"### Analysis of the individuals' titles\nHere, I identifiy the mostly used titles and, with the help of Wiki and Google, replace the least common ones to have more homogeneous data. This will help me also to infer the age, wehre is missing.","64566c3c":"### Exploratory analysis\nUse \n* sns.countplot to \"count\" the number of elements, divided according to the \"x\" label\n* sns.catplot (with kind = \"count\") to count over two subgroups (using the 'col' kwargs).\n","2d42f06d":"From this first look, it seems that the survival rate depends mostly on the sex and on the class.\nMoreover, there is an increased death rate for people embarked in \"S\" (probably correlated with the fact that they are mostly belonging to the 3rd class), and for people with no siblings\/spouse and 0 parents\/children."}}