{"cell_type":{"6ce44d56":"code","dda56951":"code","766d85b8":"code","0c09920a":"code","b5cc5090":"code","a79d199d":"code","ecc212df":"code","a786fd9e":"code","2cc1ff8f":"code","85c6e663":"code","a11ad542":"code","8aeca729":"code","d8674577":"code","487cf961":"code","bbf3f498":"code","98d84141":"code","f2b845bf":"code","7b4e3d85":"code","63059fec":"code","026a7451":"code","6523b134":"code","7e5f30ac":"code","8d473923":"code","43d08ef8":"code","cff45633":"code","a8a51ced":"code","32725647":"code","f6c7f873":"code","7196be4a":"code","5f8c0fe7":"code","0aa7b619":"markdown","38e9a103":"markdown"},"source":{"6ce44d56":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","dda56951":"#!pip install tensorflow==2.0.0-beta1","766d85b8":"import pandas as pd, numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras","0c09920a":"train = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv')\ntest = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv')\n##test_labels = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv') }Can not be loaded \nsubm = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv')\n","b5cc5090":"test.head()","a79d199d":"subm.head()","ecc212df":"train.head()","a786fd9e":"text = train['comment_text']","2cc1ff8f":"text[0]","85c6e663":"train['comment_text'][0]","a11ad542":"# for train\nlens = train.comment_text.str.len()\nlens.mean(), lens.std(), lens.max()","8aeca729":"# for test\nlens = test.comment_text.str.len()\nlens.mean(), lens.std(), lens.max()","d8674577":"lens.hist();","487cf961":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ntrain['none'] = 1-train[label_cols].max(axis=1) ## each colum may have the value of one ( Labled ) . 1- calc the max # if has no lable max = 0 then col = 1 -0 = 0\ntrain.describe()","bbf3f498":"len(train),len(test)","98d84141":"## deal with nulls \nCOMMENT = 'comment_text'\ntrain[COMMENT].fillna(\"unknown\", inplace=True)\ntest[COMMENT].fillna(\"unknown\", inplace=True)","f2b845bf":"import re, string\nre_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()\ndef clean(s): return re_tok.sub(r' \\1 ', s)","7b4e3d85":"## decide vocab size \nwords = []\nfor t in text:\n    words.extend(tokenize(t))\nprint(words[:100])\nvocab = list(set(words))\nprint(len(words), len(vocab))","63059fec":"train['comment_text'][0]","026a7451":"clean(train['comment_text'][0])","6523b134":"def one_hot_word_embedding(vtrain_data,vtest_data):\n    # switch data back to text \n    train_labels = vtrain_data[label_cols]\n    txt_train_data = [clean(txt) for txt in train['comment_text']]\n    txt_test_data = [clean(txt) for txt in test['comment_text']]\n    \n    # integer encode the documents\n    vocab_size = 10000\n    encoded_txt_train_data = [keras.preprocessing.text.one_hot(d, vocab_size) for d in txt_train_data]\n    encoded_txt_test_data = [keras.preprocessing.text.one_hot(d, vocab_size) for d in txt_test_data]\n    #print(encoded_txt_train_data)\n\n    ptxt_train_data = keras.preprocessing.sequence.pad_sequences(encoded_txt_train_data,\n                                                            padding='post',\n                                                            maxlen=5000)\n\n    ptxt_test_data = keras.preprocessing.sequence.pad_sequences(encoded_txt_test_data,\n                                                           padding='post',\n                                                           maxlen=5000)\n    x_val = ptxt_train_data[:100000] \n    partial_x_train = ptxt_train_data[100000:]\n\n    y_val = train_labels[:100000]\n    partial_y_train = train_labels[100000:]\n    return (x_val,partial_x_train,y_val,partial_y_train,ptxt_test_data)\n\ndef full_one_hot_word_embedding(vtrain_data,vtest_data):\n    # switch data back to text \n    train_labels = vtrain_data[label_cols]\n    txt_train_data = [clean(txt) for txt in train['comment_text']]\n    txt_test_data = [clean(txt) for txt in test['comment_text']]\n    \n    # integer encode the documents\n    vocab_size = 10000\n    encoded_txt_train_data = [keras.preprocessing.text.one_hot(d, vocab_size) for d in txt_train_data]\n    encoded_txt_test_data = [keras.preprocessing.text.one_hot(d, vocab_size) for d in txt_test_data]\n    #print(encoded_txt_train_data)\n\n    ptxt_train_data = keras.preprocessing.sequence.pad_sequences(encoded_txt_train_data,\n                                                            padding='post',\n                                                            maxlen=5000)\n\n    ptxt_test_data = keras.preprocessing.sequence.pad_sequences(encoded_txt_test_data,\n                                                           padding='post',\n                                                           maxlen=5000)\n    partial_x_train = ptxt_train_data\n    partial_y_train = train_labels\n    return (partial_x_train,partial_y_train,ptxt_test_data)","7e5f30ac":"def model_with_emb_acc(vtrain_data,vtest_data,vocab_size = 10000):\n    model1 = keras.Sequential()\n    model1.add(keras.layers.Embedding(vocab_size, 16))\n    model1.add(keras.layers.GlobalAveragePooling1D())\n    model1.add(keras.layers.Dense(512, activation=tf.nn.relu))\n    #model.add(keras.layers.Dense(16, activation=tf.nn.relu,activity_regularizer=keras.regularizers.l1(0.001)))\n    #model.add(keras.layers.Dropout(0.2))\n    model1.add(keras.layers.Dense(6, activation=tf.nn.sigmoid))\n    model1.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])\n    x_val,partial_x_train,y_val,partial_y_train,test_data = one_hot_word_embedding(vtrain_data,vtest_data)\n    earlystopper = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n    history = model1.fit(x_val,\n                     y_val,\n                     epochs=20,\n                     callbacks=[earlystopper],\n                     batch_size=512,\n                     validation_data=(x_val, y_val),\n                     verbose=1)\n#     results1 = model1.evaluate(x_val, y_val)\n    return (model1,test_data,history)\n\ndef full_model_with_emb_acc(vtrain_data,vtest_data,vocab_size = 10000):\n    model1 = keras.Sequential()\n    model1.add(keras.layers.Embedding(vocab_size, 16))\n    model1.add(keras.layers.GlobalAveragePooling1D())\n    model1.add(keras.layers.Dense(512, activation=tf.nn.relu))\n    #model.add(keras.layers.Dense(16, activation=tf.nn.relu,activity_regularizer=keras.regularizers.l1(0.001)))\n    #model.add(keras.layers.Dropout(0.2))\n    model1.add(keras.layers.Dense(6, activation=tf.nn.sigmoid))\n    model1.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])\n    partial_x_train,partial_y_train,test_data = full_one_hot_word_embedding(vtrain_data,vtest_data)\n    earlystopper = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n    history = model1.fit(partial_x_train,\n                     partial_y_train,\n                     epochs=20,\n                     callbacks=[earlystopper],\n                     batch_size=1024,\n                     verbose=1)\n#     results1 = model1.evaluate(x_val, y_val)\n    return (model1,test_data,history)","8d473923":"model1,test_data,his1 = full_model_with_emb_acc(train,test)","43d08ef8":"# serialize model to JSON\nmodel_json = model1.to_json()\nwith open(\"my_model1.json\", \"w\") as json_file:\n    json_file.write(model_json)\n    \nmodel1.save_weights('my_model1_weights.h5')","cff45633":"### load the model 1 \n# load json and create model\njson_file = open('my_model1.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nmodel1 = tf.keras.models.model_from_json(loaded_model_json)\n# load weights into new model\nmodel1.load_weights(\"my_model1_weights.h5\")\nprint(\"Loaded model from disk\")","a8a51ced":"def column(matrix, i):\n    return [row[i] for row in matrix]","32725647":"y_pred = model1.predict(test_data, batch_size=1024)","f6c7f873":"submission = pd.DataFrame()\nsubmission['id'] = test['id']\nsubmission['toxic'] = column(y_pred, 0)\nsubmission['severe_toxic'] = column(y_pred, 1)\nsubmission['obscene'] = column(y_pred, 2)\nsubmission['threat'] = column(y_pred, 3)\nsubmission['insult'] = column(y_pred, 4)\nsubmission['identity_hate'] = column(y_pred, 5)","7196be4a":"submission.to_csv('submission.csv', index=False)","5f8c0fe7":"submission","0aa7b619":"### Data Exploration","38e9a103":"### Building the model\u00b6\n"}}