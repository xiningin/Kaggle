{"cell_type":{"29bbd94e":"code","63099e81":"code","a9b726a2":"code","5f3ef908":"code","2fd87edb":"code","b9dfef20":"code","9c41d2b4":"code","6793903c":"code","4b19c69a":"code","30fe4817":"code","5d57c5f0":"code","cd2e529f":"code","81439ad8":"code","6e8b319e":"code","9016338f":"code","60525814":"code","3157be58":"code","c193c750":"code","93b12d71":"code","6d5cb98e":"code","f78319d0":"code","77f70e2a":"code","3b195155":"code","c2fd386b":"code","0497c6d4":"code","710c826d":"code","a53d893b":"code","a65399b3":"code","b9af9f52":"code","aeed3f56":"code","b3736fd2":"code","5d04decc":"code","f35c9458":"code","a8125138":"code","afed8523":"code","277e40d7":"markdown","e5e8a216":"markdown","17e39867":"markdown","e7d93f96":"markdown","ba21e702":"markdown","2ff70b63":"markdown","038214c3":"markdown","a7d81745":"markdown","103974db":"markdown","2452c6bd":"markdown"},"source":{"29bbd94e":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline","63099e81":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a9b726a2":"milk = pd.read_csv('\/kaggle\/input\/monthy-milk\/monthly-milk-production.csv',index_col='Month')","5f3ef908":"milk.head()","2fd87edb":"milk.index = pd.to_datetime(milk.index)","b9dfef20":"milk.describe()","9c41d2b4":"milk.plot()","6793903c":"milk.info()","4b19c69a":"train_set = milk.head(156)","30fe4817":"train_set.head()","5d57c5f0":"test_set = milk.tail(12)","cd2e529f":"from sklearn.preprocessing import MinMaxScaler","81439ad8":"scaler = MinMaxScaler()","6e8b319e":"train_scaled = scaler.fit_transform(train_set)","9016338f":"test_scaled = scaler.transform(test_set)","60525814":"def next_batch(training_data,batch_size,steps):\n    \n    \n    # Grab a random starting point for each batch\n    rand_start = np.random.randint(0,len(training_data)-steps) \n\n    # Create Y data for time series in the batches\n    y_batch = np.array(training_data[rand_start:rand_start+steps+1]).reshape(1,steps+1)\n\n    return y_batch[:, :-1].reshape(-1, steps, 1), y_batch[:, 1:].reshape(-1, steps, 1)","3157be58":"# y_batch[:<all rows>, :-1<from last>].reshape(-1, steps, 1), y_batch[:, 1:<from start>].reshape(-1, steps, 1) \n# we need 2 y_batch because of back propogating","c193c750":"# putting values to parameter\nnum_inputs = 1\nnum_time_steps = 12\nnum_neurons = 100\nnum_outputs = 1\nlearning_rate = 0.03\nnum_train_iterations = 4000\nbatch_size = 1","93b12d71":"X = tf.placeholder(tf.float32, [None, num_time_steps, num_inputs])\ny = tf.placeholder(tf.float32, [None, num_time_steps, num_outputs])","6d5cb98e":"cell = tf.contrib.rnn.OutputProjectionWrapper(\n    tf.contrib.rnn.BasicLSTMCell(num_units=num_neurons, activation=tf.nn.relu),\n    output_size=num_outputs) ","f78319d0":"# tf.nn.rnn_cell.LSTMCell above\n","77f70e2a":"outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)","3b195155":"# above is deprecated use this one ===tf.keras.layers.RNN","c2fd386b":"loss = tf.reduce_mean(tf.square(outputs - y)) # MSE\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\ntrain = optimizer.minimize(loss)","0497c6d4":"init = tf.global_variables_initializer()","710c826d":"saver = tf.train.Saver()","a53d893b":"gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)","a65399b3":"with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n    sess.run(init)\n    \n    for iteration in range(num_train_iterations):\n        \n        X_batch, y_batch = next_batch(train_scaled,batch_size,num_time_steps)\n        sess.run(train, feed_dict={X: X_batch, y: y_batch})\n        \n        if iteration % 100 == 0:\n            \n            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})\n            print(iteration, \"\\tMSE:\", mse)\n    \n    # Save Model for Later\n    saver.save(sess, \".\/ex_time_series_model\")","b9af9f52":"with tf.Session() as sess:\n    \n    # Use your Saver instance to restore your saved rnn time series model\n    saver.restore(sess, \".\/ex_time_series_model\")\n\n    # Create a numpy array for your genreative seed from the last 12 months of the \n    # training set data. Hint: Just use tail(12) and then pass it to an np.array\n    train_seed = list(train_scaled[-12:])\n    \n    ## Now create a for loop that \n    for iteration in range(12):\n        X_batch = np.array(train_seed[-num_time_steps:]).reshape(1, num_time_steps, 1)\n        y_pred = sess.run(outputs, feed_dict={X: X_batch})\n        train_seed.append(y_pred[0, -1, 0])","aeed3f56":"# inverse transform data into normal form from scaling","b3736fd2":"results = scaler.inverse_transform(np.array(train_seed[12:]).reshape(12,1))","5d04decc":"#new column on the test_set = \"Generated\" ","f35c9458":"test_set['Generated'] = results","a8125138":"test_set.plot()","afed8523":"test_set","277e40d7":"# Generative Session\n\nNOTE: generating new values based off some previous pattern, rather than trying to directly predict the future.  its limits due to the smaller size of our data set.","e5e8a216":"# Time series data for predicting monthly  milk production","17e39867":"Monthly milk production: pounds per cow. Jan 62 - Dec 75","e7d93f96":"** Now create the RNN Layer, you have complete freedom over this, use tf.contrib.rnn and choose anything you want, OutputProjectionWrappers, BasicRNNCells, BasicLSTMCells, MultiRNNCell, GRUCell etc... Keep in mind not every combination will work well! (If in doubt, the solutions used an Outputprojection Wrapper around a basic LSTM cell with relu activation.**","ba21e702":"## Batch Function","2ff70b63":"<img src='https:\/\/isaacchanghau.github.io\/img\/deeplearning\/lstmgru\/lstmandgru.png' \/>","038214c3":"### importing libraries and time series data","a7d81745":"## MODEL LSTM","103974db":"## Scale the Data","2452c6bd":"<img src=\"http:\/\/slideplayer.com\/slide\/9823679\/32\/images\/1\/The+Process+of+Milk+Production.jpg\" \/>"}}