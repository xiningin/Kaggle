{"cell_type":{"ce61e0bd":"code","c201e25a":"code","f56840c9":"code","7f5f96af":"code","c3c54853":"code","aac90e7c":"code","8a1bd9c0":"code","f7d57e86":"code","d138c2d6":"code","d7b7f422":"code","a49650dc":"code","ac0e29de":"code","69f0c97e":"code","588988bf":"code","ee95c1c6":"code","8ce7351f":"code","882968e6":"code","5a308398":"code","e16e8a2f":"code","d8115eec":"code","ba9fd3c6":"code","1cdc33ee":"code","4ac29425":"code","a4de373b":"code","d437ca13":"code","8336a493":"code","5437cb90":"code","bd28c7ec":"code","faf4630e":"code","391e18a6":"code","09072557":"code","cb2d86ae":"code","9b8e1505":"code","af7f85ba":"code","c026a72a":"code","8985eaf7":"code","7d8a4e68":"code","139d700e":"code","df02e712":"code","139137ce":"code","40477b05":"code","2a3a4f36":"markdown","cd27405e":"markdown","990e42ed":"markdown","dcbc1a6c":"markdown","a3f0066c":"markdown","2bc640ae":"markdown","7e402bc4":"markdown","f7450ef1":"markdown","ff82ebed":"markdown","7be9dc4f":"markdown","a530f94f":"markdown","03dbe52c":"markdown","0e93eeb8":"markdown","6a662060":"markdown","f2a2c0cc":"markdown","a23a091f":"markdown","a8909b4e":"markdown","d81ec71d":"markdown","f6bdf929":"markdown","c44bf10e":"markdown","4e87b597":"markdown"},"source":{"ce61e0bd":"# loading basic libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","c201e25a":"# load train and test data\ntrain_data = pd.read_csv(\"..\/input\/titanic\/train.csv\", index_col='PassengerId')\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\", index_col='PassengerId')","f56840c9":"# see columns names, types and missing values and head of test_data\nprint(train_data.info())\nprint(\"Shape of train and test DataFrames:\", train_data.shape, test_data.shape)\ntrain_data.head()","7f5f96af":"# print description of numerical data\ntrain_data.describe()","c3c54853":"# print number of missing values in train and test dataFrame\nprint(pd.isnull(train_data).sum())\nprint(pd.isnull(test_data).sum())","aac90e7c":"# plot heatmap with numeric features\nsns.heatmap(data=train_data.corr(), vmin=-1, vmax=1, cmap='bwr', annot=True, fmt = \".2f\")\nplt.show()","8a1bd9c0":"# plot barplot\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train_data)\nplt.show()\n\n# print values\ntrain_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(\n    by='Survived', ascending=False)","f7d57e86":"# plot barplot\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train_data)\nplt.show()\n\n# print values\ntrain_data[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(\n    by='Survived', ascending=False)","d138c2d6":"# plot barplot\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=train_data)\nplt.show()\n\n# print values\ntrain_data[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(\n    by='Survived', ascending=False)","d7b7f422":"# plot barplot\nsns.barplot(x=\"Parch\", y=\"Survived\", data=train_data)\nplt.show()\n\n# print values\ntrain_data[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean().sort_values(\n    by='Survived', ascending=False)","a49650dc":"# plot barplot\nsns.barplot(x=\"Embarked\", y=\"Survived\", data=train_data)\n\n# print values\ntrain_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(\n    by='Survived', ascending=False)","ac0e29de":"#sort the ages into logical categories\nbins = [-1, 0, 5, 12, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager-Adult', 'Senior']\nfor df in [train_data, test_data]:\n    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n    df['AgeGroup'] = pd.cut(df[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train_data)\nplt.xticks(np.linspace(0,6,7), labels, rotation=45, ha=\"right\")\nplt.xlim(-0.6,4.6)\nplt.show()\n\n# print values\ntrain_data[['AgeGroup', 'Survived']].groupby(['AgeGroup'], as_index=False).mean().sort_values(\n    by='Survived', ascending=False)","69f0c97e":"#sort the ages into logical categories\nbins = [-1, 8, 15, 30, np.inf]\nlabels = ['<8', '8-15', '15-31', '>31']\nfor df in [train_data, test_data]:\n    df[\"Fare\"] = df[\"Fare\"].fillna(-0.5)\n    df['FareGroup'] = pd.cut(df[\"Fare\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"FareGroup\", y=\"Survived\", data=train_data)\nplt.xticks(np.linspace(0,5,6), labels, rotation=45, ha=\"right\")\nplt.xlim(-0.6,3.6)\nplt.show()\n\n# print values\ntrain_data[['FareGroup', 'Survived']].groupby(['FareGroup'], as_index=False).mean().sort_values(\n    by='Survived', ascending=False)","588988bf":"# create Title\nfor df in [train_data, test_data]:\n    df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n                                             'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n    df['Title'] = df['Title'].replace('Ms', 'Miss')\n    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n\ntrain_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","ee95c1c6":"# join SibSp and Parch as FamilySize\nfor df in [train_data, test_data]:\n    df['FamilySize'] = (df['SibSp'] + df['Parch'] + 1)\n    df.loc[df['FamilySize'] > 4, 'FamilySize'] = 5\n\ntrain_data[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(\n        by='Survived', ascending=False)","8ce7351f":"# fill missing Embarked with mode\nfor df in [train_data, test_data]:\n    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])","882968e6":"# map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\nfor df in [train_data, test_data]:\n    df['Sex'] = df['Sex'].map(sex_mapping).astype(int)","5a308398":"# map each Age value to a numerical value and fill missing values\nage_group_mapping = {'Baby' : 0, 'Child' : 1, 'Teenager-Adult' : 2, 'Senior' : 3}\nfor df in [train_data, test_data]:\n    # Fill missing values based on Title    \n    df['AgeGroup'] = df['AgeGroup'].replace(['Unknown'], [None])\n    mr_age = df[df[\"Title\"] == \"Mr\"][\"AgeGroup\"].mode()[0] \n    miss_age = df[df[\"Title\"] == \"Miss\"][\"AgeGroup\"].mode()[0]\n    mrs_age = df[df[\"Title\"] == \"Mrs\"][\"AgeGroup\"].mode()[0] \n    master_age = df[df[\"Title\"] == \"Master\"][\"AgeGroup\"].mode()[0]\n    rare_age = df[df[\"Title\"] == \"Rare\"][\"AgeGroup\"].mode()[0]\n    title_age_mapping = {\"Mr\": mr_age, \"Miss\": miss_age, \"Mrs\": mrs_age, \"Master\": master_age, \"Rare\": rare_age}\n    df['AgeGroup'].fillna(df['Title'].map(title_age_mapping), inplace=True)\n    \n    # map strings to int\n    df['AgeGroup'] = df['AgeGroup'].map(age_group_mapping).astype('int')","e16e8a2f":"# map each FareGroup value to a numerical value\nfare_mapping = {'<8' : 0, '8-15' : 1, '15-31' : 2, '>31' : 3}\nfor df in [train_data, test_data]:\n    df['FareGroup'] = df['FareGroup'].map(fare_mapping).astype('int')","d8115eec":"# create CabinBool feature, that show if the passanger have or not a Cabin\nfor df in [train_data, test_data]:\n    df[\"CabinBool\"] = df[\"Cabin\"].notnull().astype('bool')","ba9fd3c6":"# drop unused data\nfor df in [train_data, test_data]:\n    df.drop(['Name'], axis = 1, inplace=True)\n    df.drop(['SibSp'], axis = 1, inplace=True)\n    df.drop(['Parch'], axis = 1, inplace=True)\n    df.drop(['Age'], axis = 1, inplace=True)\n    df.drop(['Cabin'], axis = 1, inplace=True)\n    df.drop(['Fare'], axis = 1, inplace=True)\n    df.drop(['Ticket'], axis = 1, inplace=True)","1cdc33ee":"train_data.info()\ntest_data.info()","4ac29425":"print(train_data.describe(), test_data.describe())","a4de373b":"# prepare data to be used in the models\nfrom sklearn.model_selection import cross_val_score\ntrain_data = pd.get_dummies(train_data, columns=['Embarked', 'Title'], drop_first=True)\ny = train_data[\"Survived\"]\nX = train_data.drop(['Survived'], axis = 1)","d437ca13":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\nscores = cross_val_score(gaussian, X, y, cv=5)\nacc_gaussian = round(scores.mean() * 100, 2)\nprint(acc_gaussian)","8336a493":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(solver='lbfgs')\nscores = cross_val_score(logreg, X, y, cv=5)\nacc_logreg = round(scores.mean() * 100, 2)\nprint(acc_logreg)","5437cb90":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC(gamma='auto')\nscores = cross_val_score(svc, X, y, cv=5)\nacc_svc = round(scores.mean() * 100, 2)\nprint(acc_svc)","bd28c7ec":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC(max_iter=3000)\nscores = cross_val_score(linear_svc, X, y, cv=5)\nacc_linear_svc = round(scores.mean() * 100, 2)\nprint(acc_linear_svc)","faf4630e":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nscores = cross_val_score(perceptron, X, y, cv=5)\nacc_perceptron = round(scores.mean() * 100, 2)\nprint(acc_perceptron)","391e18a6":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\nscores = cross_val_score(decisiontree, X, y, cv=5)\nacc_decisiontree = round(scores.mean() * 100, 2)\nprint(acc_decisiontree)","09072557":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier(max_depth=4, n_estimators=600)\nscores = cross_val_score(randomforest, X, y, cv=5)\nacc_randomforest = round(scores.mean() * 100, 2)\nprint(acc_randomforest)","cb2d86ae":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 5)\nscores = cross_val_score(knn, X, y, cv=5)\nacc_knn = round(scores.mean() * 100, 2)\nprint(acc_knn)","9b8e1505":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nscores = cross_val_score(sgd, X, y, cv=5)\nacc_sgd = round(scores.mean() * 100, 2)\nprint(acc_sgd)","af7f85ba":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier(n_estimators=500, learning_rate=0.11)\nscores = cross_val_score(gbk, X, y, cv=5)\nacc_gbk = round(scores.mean() * 100, 2)\nprint(acc_gbk)","c026a72a":"# Comparison of Models\nmodels = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, acc_randomforest, acc_gaussian, acc_perceptron, \n              acc_linear_svc, acc_decisiontree, acc_sgd, acc_gbk]})\nmodels = models.sort_values(by='Score', ascending=False)\n\nprint(models)","8985eaf7":"randomforest.fit(X, y)\ndfFit = pd.DataFrame(randomforest.feature_importances_, train_data.drop(['Survived'], axis = 1).columns, \n                     columns=['Coefficient']).sort_values('Coefficient') \ndfFit.sort_values(by='Coefficient', ascending=False)","7d8a4e68":"# print classification report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.2)\nrandomforest = RandomForestClassifier(max_depth=4, n_estimators=600)\nrandomforest.fit(X_train, y_train)\ny_pred = randomforest.predict(X_test)\n\nprint(classification_report(y_test,y_pred))","139d700e":"# plot confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test,y_pred)\ncmNorm = [[cm[0,0]\/(cm[0,0]+cm[0,1]), cm[0,1]\/(cm[0,0]+cm[0,1])],\n         [cm[1,0]\/(cm[1,0]+cm[1,1]), cm[1,1]\/(cm[1,0]+cm[1,1])]]\ndf_cm = pd.DataFrame(cmNorm, index=['Real True', 'Real False'], columns=['Predict True', 'Predict False'])\nplt.figure(figsize = (6,3))\nplt.title(\"Normalized Confusion Matrix\")\nsns.heatmap(df_cm, annot=True, vmin=0, vmax=1, cmap='binary', fmt = \".3f\")\nplt.show()","df02e712":"# plot roc_curve and auc\nfrom sklearn.metrics import roc_curve, auc\n\ny_pred_proba = randomforest.predict_proba(X_test)\ny_pred_proba = y_pred_proba[:, 1] \nFPR, TPR, _ = roc_curve(y_test, y_pred_proba)\nROC_AUC = auc(FPR, TPR)\nprint (\"Area Under ROC Curve (AUC):\", ROC_AUC)\n\nplt.figure(figsize =[8,7])\nplt.plot(FPR, TPR, label= 'ROC curve(area = %0.2f)'%ROC_AUC, linewidth= 4)\nplt.plot([0,1],[0,1], 'k--', linewidth = 4)\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nplt.xlabel('False Positive Rate', fontsize = 18)\nplt.ylabel('True Positive Rate', fontsize = 18)\nplt.title('ROC for Titanic survivors', fontsize= 18)\nplt.show()","139137ce":"# plot precision_recall_curve\nfrom sklearn.metrics import precision_recall_curve\n\nprecision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\nPR_AUC = auc(recall, precision)\n\nplt.figure(figsize=[8,7])\nplt.plot(recall, precision, label='PR curve (area = %0.2f)' % PR_AUC, linewidth=4)\nplt.xlabel('Recall', fontsize=18)\nplt.ylabel('Precision', fontsize=18)\nplt.title('Precision Recall Curve for Titanic survivors', fontsize=18)\nplt.legend(loc=\"lower right\")\nplt.show()","40477b05":"X_test = pd.get_dummies(test_data, columns=['Embarked', 'Title'], drop_first=True)\n\nrandomforest = RandomForestClassifier(max_depth=4, n_estimators=600)\nrandomforest.fit(X, y)\npredictions = randomforest.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': X_test.index.values, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","2a3a4f36":"## Parch\n\nPassagers alone or with large families have smaller survival rate.","cd27405e":"Here we fill Age missing values based on the title, because they have a high correlation","990e42ed":"## 2) Read and Explore Data\n\nLoad data from csv and make initial exploration of train and test DataFrames. In order to make the importation the data have to be downloaded from Kaggle ([kaggle titanic data](https:\/\/www.kaggle.com\/c\/titanic\/data)) extracted and saved as 'data\/titanic_train.csv' and 'data\/titanic_test.csv'. ","dcbc1a6c":"## Embarked\n\nPeople that embarked in Cherbourg (C) have higher survival probability than people that embarked in Queenstown (Q) and Southampton (S).","a3f0066c":"## SibSp\n\nPassagers alone or with large families have smaller survival rate.","2bc640ae":"Checking the data in train and test DataFrames.","7e402bc4":"## 8) Creating Submission File\n\nCreate and save submission file. The file is saved in the data folder with the name 'titanic_submission.csv'","f7450ef1":"## 3) Data Analysis and Visualization\n\nMake correlation analysis and visual plot of the relation between all features and the target variable.","ff82ebed":"Primary analysis:\nTest DataFrame have 891 passagers, and Train Datafram have 418 passagers.\n\nFrom the 11 columns data we have that:\n* 6 columns are numerical: Survived, Pclass, Age, SibSp, Parch, Fare.\n  * 2 are continous: Age, Fare\n  * 4 are discrite: Survived, Pclass, SibSp, Parch\n        \n* 5 are strings: Name, Sex, Ticket, Cabin, Embarked\n  * 2 are categorical strings: Sex, Embarked\n  \nAlso Cabin, Age and Embarked have missing values that have to be handled.","7be9dc4f":"## 4) Clean and arrange data\n\nCreate new features, fill missing values, mapp values to int or bool.","a530f94f":"## 6) Fitting and comparing Models\n\nHere it is trained 9 models, all optimized and checked with cross validation score. We choose the model with best accuracy. The models trained here are:\n* Gaussian Naive Bayes\n* Logistic Regression\n* Support Vector Machines\n* Perceptron\n* Decision Tree Classifier\n* Random Forest Classifier\n* KNN or k-Nearest Neighbors\n* Stochastic Gradient Descent\n* Gradient Boosting Classifier","03dbe52c":"From the previous graphic we got that Pclass has the higher correlation with Survived, followed by Fare. This gives hypothesis on what to expect from the model results.","0e93eeb8":"## 1) Import Libraries\n\nImport numpy, pandas and plotting libraries (matplotlib and seaborn).","6a662060":"## Sex\n\nFemale passagers have a higher survivel rate tha male.","f2a2c0cc":"### PClass\n\nAs expected passangers from higher class (First and Second Class) have higher survival rate.","a23a091f":"Create Title data, base on the Name column, as can be seen, some titles have higher survival rate.","a8909b4e":"As Random Forest was one of the best models (82.8%) it will be utilized in submission file.\n\nBellow we see the importance of each feature in Random Forest model:","d81ec71d":"# Titanic Introductory Solution\n\n* This notebook contain a solution to [kaggle titanic survival challenge](https:\/\/www.kaggle.com\/c\/titanic\/).\n\n### Contents:\n1. Import Libraries\n2. Read and Explore Data\n3. Data Analysis and Visualization\n5. Clean and arrange data\n6. Fitting and comparing Models\n7. Validating the Model\n7. Creating Submission File","f6bdf929":"## Fare\n\nCreate new column FareGroup. People that paid a greater Fare have higher survival probability.","c44bf10e":"## Age\n\nCreate new column AgeGroup. Babies have higher survival rate and seniors have lower survival rate.","4e87b597":"## 7) Validating the Model\n\nVerify quality of the trained model (random forest). Use classification report, confusion matrix, roc curve and precision recall curve."}}