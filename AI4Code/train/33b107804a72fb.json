{"cell_type":{"30ee41e2":"code","8f3668b2":"code","81f3831e":"code","a9099c0b":"code","b56713af":"code","4df4fba8":"code","d8006531":"code","d299aa63":"code","ac4b969a":"code","b07a5fbf":"code","b4f46e0b":"code","9a37287e":"code","4c6090b6":"code","1aed6201":"code","052d5837":"code","200c3420":"code","f12bd1fe":"code","4ef63f03":"code","6f0cd39d":"code","1cbd6967":"code","b8955fc0":"code","79e6dded":"code","e2eab1e4":"code","cace0088":"code","14a13179":"code","9719dd43":"markdown","f3dd9107":"markdown","ea485d76":"markdown","c472414c":"markdown","3fc53bb3":"markdown","a182bd5f":"markdown","ae4015fc":"markdown","39d03f5b":"markdown","820504a7":"markdown","f2fc6e3d":"markdown","fb486eff":"markdown","9db10395":"markdown","b88053eb":"markdown","c7f237dc":"markdown"},"source":{"30ee41e2":"import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix","8f3668b2":"df_final = pd.read_csv(\"..\/input\/df-rock\/df_base_rock.csv\")\ndf_final","81f3831e":"#Data Scaling using sklearn\n\n\ndata = df_final.drop(columns={\"artist_name\",\"artist_id\",\"genre\",\"album_name\",\"album_id\",\"track_name\"})\n\n\nscaler = MinMaxScaler()\nscaler.fit(data)\ndf_scaled = scaler.transform(data)\n\n\nX = df_scaled\ny = df_final[\"genre\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666, train_size = 0.80)","a9099c0b":"from sklearn.metrics import make_scorer \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import balanced_accuracy_score\n\nbalanced_score = make_scorer(balanced_accuracy_score, greater_is_better=False, needs_proba=False, needs_threshold=False)","b56713af":"#Import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nparam_grid_knn={\"n_neighbors\":range(5,35,5),\n                \"weights\":[\"uniform\", \"distance\"],\n                \"algorithm\":[\"ball_tree\", \"kd_tree\", \"brute\"]}\n\ngrid_knn_a= GridSearchCV(KNeighborsClassifier(),param_grid_knn,cv=5,scoring = balanced_score)\n\ngrid_knn_a.fit(X_train, y_train)\n\nparam_knn=grid_knn_a.best_params_\n\npredictions_train = grid_knn_a.predict(X_train)\naccuracy_train = accuracy_score(y_train,predictions_train)\n\n\npredictions_test = grid_knn_a.predict(X_test)\naccuracy_test = accuracy_score(y_test,predictions_test)\n\n\nprint(\"accuracy train\",accuracy_train )\nprint(\"accuracy test\",accuracy_test )","4df4fba8":"#Import DecisionTree\nfrom sklearn.tree import DecisionTreeClassifier\n\nparam_grid_DTC = {\"criterion\":[\"gini\",\"entropy\"],\n             \"splitter\":[\"best\", \"random\"],\n            \"max_depth\":range(5,35,5)\n            }\n\ngrid_DTC_a = GridSearchCV(DecisionTreeClassifier(),param_grid_DTC,cv=5,scoring = balanced_score)\n\ngrid_DTC_a.fit(X_train, y_train)\n\nparam_DTC=grid_DTC_a.best_params_\n\npredictions_train = grid_DTC_a.predict(X_train)\naccuracy_train = accuracy_score(y_train,predictions_train)\n\n\npredictions_test = grid_DTC_a.predict(X_test)\naccuracy_test = accuracy_score(y_test,predictions_test)\n\n\nprint(\"accuracy train\",accuracy_train )\nprint(\"accuracy test\",accuracy_test )","d8006531":"#Import Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nparam_grid_lr = {\"penalty\":[\"l2\",\"none\"],\n            \"max_iter\":range(500,1500,150),\n            \"solver\":[\"newton-cg\", \"lbfgs\", \"sag\", \"saga\"],\n           }\n\ngrid_lr_a = GridSearchCV(LogisticRegression(),param_grid_lr,cv=5,scoring = balanced_score)\n\n\ngrid_lr_a.fit(X_train, y_train)\n\nparam_lr=grid_lr_a.best_params_\n\n\npredictions_train = grid_lr_a.predict(X_train)\naccuracy_train = accuracy_score(y_train,predictions_train)\n\n\npredictions_test = grid_lr_a.predict(X_test)\naccuracy_test = accuracy_score(y_test,predictions_test)\n\n\nprint(\"accuracy train\",accuracy_train )\nprint(\"accuracy test\",accuracy_test )","d299aa63":"#Import Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nparam_grid_forest = {\"n_estimators\":range(50,250,50),\n                    \"criterion\":[\"gini\",\"entropy\"],\n                    \"max_depth\":range(1,45,4)\n                    }\n\ngrid_forest_a = GridSearchCV(RandomForestClassifier(),param_grid_forest,cv=5,scoring = balanced_score)\n\ngrid_forest_a.fit(X_train, y_train)\n\nparam_forest_a = grid_forest_a.best_params_\n\npredictions_train = grid_forest_a.predict(X_train)\naccuracy_train = accuracy_score(y_train,predictions_train)\n    \n\npredictions_test = grid_forest_a.predict(X_test)\naccuracy_test = accuracy_score(y_test,predictions_test)\n\n\nprint(\"accuracy train\",accuracy_train )\nprint(\"accuracy test\",accuracy_test )","ac4b969a":"df_country = pd.read_csv(\"..\/input\/df-rock\/df_final_country_rock.csv\")\ndf_country.drop(columns=\"Unnamed: 0\",inplace=True)","b07a5fbf":"df_country","b4f46e0b":"#ScalingDesData\ndata = df_country.drop(columns={\"artist_name\",\"artist_id\",\"genre\",\"album_name\",\"album_id\",\"track_name\",'country'})\n\n\nscaler = MinMaxScaler()\nscaler.fit(data)\ndf_scaled = scaler.transform(data)\n\n\n\nX = df_scaled\ny = df_country[\"genre\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666, train_size = 0.80)","9a37287e":"#Import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nparam_grid_knn={\"n_neighbors\":range(5,35,5),\n                \"weights\":[\"uniform\", \"distance\"],\n                \"algorithm\":[\"ball_tree\", \"kd_tree\", \"brute\"]}\n\ngrid_knn_b= GridSearchCV(KNeighborsClassifier(),param_grid_knn,cv=5,scoring = balanced_score)\n\ngrid_knn_b.fit(X_train, y_train)\n\nparam_knn_b=grid_knn_b.best_params_\n\npredictions_train = grid_knn_b.predict(X_train)\naccuracy_train = accuracy_score(y_train,predictions_train)\n\n\npredictions_test = grid_knn_b.predict(X_test)\naccuracy_test = accuracy_score(y_test,predictions_test)\n\n\nprint(\"accuracy train\",accuracy_train )\nprint(\"accuracy test\",accuracy_test )","4c6090b6":"#Import DecisionTree\nfrom sklearn.tree import DecisionTreeClassifier\n\nparam_grid_DTC = {\"criterion\":[\"gini\",\"entropy\"],\n             \"splitter\":[\"best\", \"random\"],\n            \"max_depth\":range(15,55,5)\n            }\n\ngrid_DTC_b = GridSearchCV(DecisionTreeClassifier(),param_grid_DTC,cv=5,scoring = balanced_score)\n\ngrid_DTC_b.fit(X_train, y_train)\n\nparam_DTC_b=grid_DTC_b.best_params_\n\npredictions_train = grid_DTC_b.predict(X_train)\naccuracy_train = accuracy_score(y_train,predictions_train)\n\n\npredictions_test = grid_DTC_b.predict(X_test)\naccuracy_test = accuracy_score(y_test,predictions_test)\n\n\nprint(\"accuracy train\",accuracy_train )\nprint(\"accuracy test\",accuracy_test )","1aed6201":"#Import Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nparam_grid_lr = {\"penalty\":[\"l2\",\"none\"],\n            \"max_iter\":range(1500,1700,100),\n            \"solver\":[\"newton-cg\", \"lbfgs\", \"sag\", \"saga\"],\n           }\n\ngrid_lr_b = GridSearchCV(LogisticRegression(),param_grid_lr,cv=5,scoring = balanced_score)\n\n\ngrid_lr_b.fit(X_train, y_train)\n\nparam_lr=grid_lr_b.best_params_\n\n\npredictions_train = grid_lr_b.predict(X_train)\naccuracy_train = accuracy_score(y_train,predictions_train)\n\n\npredictions_test = grid_lr_b.predict(X_test)\naccuracy_test = accuracy_score(y_test,predictions_test)\n\n\nprint(\"accuracy train\",accuracy_train )\nprint(\"accuracy test\",accuracy_test )","052d5837":"#Import Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nparam_grid_forest = {\"n_estimators\":range(50,250,50),\n                    \"criterion\":[\"gini\",\"entropy\"],\n                    \"max_depth\":range(1,100,5)\n                    }\n\ngrid_forest_b = GridSearchCV(RandomForestClassifier(),param_grid_forest,cv=5,scoring = balanced_score)\n\ngrid_forest_b.fit(X_train, y_train)\n\nparam_forest_b = grid_forest_b.best_params_\n\npredictions_train = grid_forest_b.predict(X_train)\naccuracy_train = accuracy_score(y_train,predictions_train)\n    \n\npredictions_test = grid_forest_b.predict(X_test)\naccuracy_test = accuracy_score(y_test,predictions_test)\n\n\nprint(\"accuracy train\",accuracy_train )\nprint(\"accuracy test\",accuracy_test )","200c3420":"df_country_genre = pd.read_csv(\"..\/input\/df-rock\/df_final_country_genre (1).csv\")\ndf_country_genre = df_country_genre.drop(columns=\"Unnamed: 0\")","f12bd1fe":"from sklearn.metrics import make_scorer \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import balanced_accuracy_score\n\nbalanced_score = make_scorer(balanced_accuracy_score, greater_is_better=False, needs_proba=False, needs_threshold=False)","4ef63f03":"#ScalingDesData\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\ndata = df_country_genre.drop(columns={\"artist_name\",\"artist_id\",\"genre\",\"album_name\",\"album_id\",\"track_name\",\"country_x\",\"genre_sp\"})\n\n\n\n\nscaler = MinMaxScaler()\nscaler.fit(data)\ndf_scaled = scaler.transform(data)\n\n\n\nX = df_scaled\ny = df_country_genre[\"genre\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666, train_size = 0.80)","6f0cd39d":"#Import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nparam_grid_knn={\"n_neighbors\":range(5,35,5),\n                \"weights\":[\"uniform\", \"distance\"],\n                \"algorithm\":[\"ball_tree\", \"kd_tree\", \"brute\"]}\n\ngrid_knn_b= GridSearchCV(KNeighborsClassifier(),param_grid_knn,cv=5)#scoring = balanced_score)\n\ngrid_knn_b.fit(X_train, y_train)\n\nparam_knn=grid_knn_b.best_params_\n\npredictions_train = grid_knn_b.predict(X_train)\naccuracy_train = accuracy_score(y_train,predictions_train)\n\n\npredictions_test = grid_knn_b.predict(X_test)\naccuracy_test = accuracy_score(y_test,predictions_test)\n\n\nprint(\"accuracy train\",accuracy_train )\nprint(\"accuracy test\",accuracy_test )","1cbd6967":"param_knn","b8955fc0":"#Import DecisionTree\nfrom sklearn.tree import DecisionTreeClassifier\n\nparam_grid_DTC = {\"criterion\":[\"gini\",\"entropy\"],\n             \"splitter\":[\"best\", \"random\"],\n            \"max_depth\":range(5,175,5)\n            }\n\ngrid_DTC_a = GridSearchCV(DecisionTreeClassifier(),param_grid_DTC,cv=5)#scoring = balanced_score)\n\ngrid_DTC_a.fit(X_train, y_train)\n\nparam_DTC=grid_DTC_a.best_params_\n\npredictions_train = grid_DTC_a.predict(X_train)\naccuracy_train = accuracy_score(y_train,predictions_train)\n\n\npredictions_test = grid_DTC_a.predict(X_test)\naccuracy_test = accuracy_score(y_test,predictions_test)\n\n\nprint(\"accuracy train\",accuracy_train )\nprint(\"accuracy test\",accuracy_test )","79e6dded":"param_DTC","e2eab1e4":"#Import Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nparam_grid_forest = {\"n_estimators\":range(50,250,50),\n                    \"criterion\":[\"gini\",\"entropy\"],\n                    \"max_depth\":range(1,74,4)\n                    }\n\ngrid_forest_b = GridSearchCV(RandomForestClassifier(),param_grid_forest,cv=5)#scoring = balanced_score)\n\ngrid_forest_b.fit(X_train, y_train)\n\nparam_forest_b = grid_forest_b.best_params_\n\npredictions_train = grid_forest_b.predict(X_train)\naccuracy_train = accuracy_score(y_train,predictions_train)\n    \n\npredictions_test = grid_forest_b.predict(X_test)\naccuracy_test = accuracy_score(y_test,predictions_test)\n\n\nprint(\"accuracy train\",accuracy_train )\nprint(\"accuracy test\",accuracy_test )","cace0088":"param_forest_b","14a13179":"#Import Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\n\n\nlr =LogisticRegression(max_iter=2200)\n\n\nlr.fit(X_train, y_train)\n\n\n\npredictions_train = lr.predict(X_train)\naccuracy_train = accuracy_score(y_train,predictions_train)\n\n\npredictions_test = lr.predict(X_test)\naccuracy_test = accuracy_score(y_test,predictions_test)\n\nprint(\"accuracy train\",accuracy_train )\nprint(\"accuracy test\",accuracy_test)","9719dd43":"To reduce overfitting we're going to use a balanced score to have the lower difference between the training set accuracy and the testing set accuracy y using the [balanced_accuracy_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.balanced_accuracy_score.html)","f3dd9107":"For the random forest, n_estimators is the max allowed in the gridsearch, for a latter study with this data set it would be pertinent to increase this value.","ea485d76":"For this study we're going to create our own database by using spotify's API via the librairie called [Spotipy](https:\/\/spotipy.readthedocs.io\/en\/2.16.1\/)\n\n[**Click Here to Access The Notebook**](https:\/\/www.kaggle.com\/victorandouard\/musical-dataframe-creation)","c472414c":"Logisctic regressions takes a lot of times to fit so we're only going to use this one for now:","3fc53bb3":"## Part 2 : Algorythm fitting with enhanced data","a182bd5f":"Scores are a better now, with 0.10 point gained on the test & train setscore","ae4015fc":"Overall scores are not great, so let's try to add more data to improve the scores","39d03f5b":"## Part 3: More enginnering","820504a7":"Again, we're going to run the same ML algorithms as before and check the differents scores\n\n\nAs the dataset is getting bigger & bigger, training time starts to be very long so i'm only going to use one algorithm","f2fc6e3d":"## Part 1 : Classifications using basic data information","fb486eff":"Overall, we managed to increase to our score to an average of 10\/20 up to 70+ !","9db10395":"For this last part we're going to add an extra layer of data.\n\nSpotify\/Spotipy provides a X number of genres for each artist(between 0 for the unfortunate artists to 17 for others).\n\nI'm also going to remove the scoring from those algorythms so the differences between the test set & train set does not lower scores.\n\nSo we're going to uses those to improve our algorythms.\n\n\n[**Click Here to Access The Notebook**](https:\/\/www.kaggle.com\/victorandouard\/features-enginering-2)","b88053eb":"I decided to add a band's coutry origin to enhance the data, by using Discogs & Wikipedia librairies to extract this information from a band desciption text.\n\n\n[**Click Here to Access The Notebook**](https:\/\/www.kaggle.com\/victorandouard\/features-enginering-1)","c7f237dc":"We're going to run the same ML algorithms as before and check the differents scores"}}