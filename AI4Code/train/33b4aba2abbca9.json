{"cell_type":{"34973e29":"code","d3c22ccc":"code","c5ee1825":"code","336f9f05":"code","6618f91f":"code","4650a335":"code","c2c9f153":"code","480ef765":"code","5c9ad84a":"code","b19aacc4":"code","d8fc85ad":"markdown","cb9b6c45":"markdown","9cd8622c":"markdown","ec871b63":"markdown","8b096e2b":"markdown","30622730":"markdown","5768b3e8":"markdown"},"source":{"34973e29":"# Loading the packages\nimport numpy as np\nimport pandas as pd \nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import StratifiedKFold\nimport matplotlib.pyplot as plt","d3c22ccc":"# Loading the training dataset\ndf_train = pd.read_csv(\"..\/input\/train.csv\")","c5ee1825":"y = df_train[\"target\"]\n# We exclude the target and id columns from the training dataset\ndf_train.pop(\"target\");\ndf_train.pop(\"id\")\ncolnames = df_train.columns\nX = df_train \ndel df_train\nX = X.values # Converting pandas dataframe to numpy array \ny = y.values # Converting pandas series to numpy array ","336f9f05":"from sklearn.metrics import make_scorer \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\n# We are going to perform a gridsearch \n\n# TODO: Initialize the classifier\nclf = RandomForestClassifier(class_weight = 'balanced', random_state=0)\n\n# Create the parameters list you wish to tune, using a dictionary if needed.\n# parameters = {'parameter_1': [value1, value2], 'parameter_2': [value1, value2]}\n\nparameters = {'n_estimators':[ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \n              'max_depth':[ 1, 2, 3, 4, 5, 6 ],\n              'criterion':['gini', 'entropy'], \n              'max_features': ['auto', 'sqrt', 'log2'], \n              \n             }\n\n# Make an roc_auc scoring object using make_scorer()\nscorer = make_scorer(roc_auc_score)\n\ngrid_obj = GridSearchCV(clf, parameters, scoring=scorer, verbose=1, cv=10)\n\ngrid_fit = grid_obj.fit(X, y)\n","6618f91f":"# Get the estimator\nbest_clf = grid_fit.best_estimator_\nprint(best_clf)","4650a335":"model = RandomForestClassifier(bootstrap=True, class_weight='balanced',\n            criterion='entropy', max_depth=2, max_features='log2',\n            max_leaf_nodes=None, min_impurity_decrease=0.0,\n            min_impurity_split=None, min_samples_leaf=1,\n            min_samples_split=2, min_weight_fraction_leaf=0.0,\n            n_estimators=6, n_jobs=None, oob_score=False, random_state=0,\n            verbose=0, warm_start=False)","c2c9f153":"model.fit(X, y)","480ef765":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model, random_state=0).fit(X, y)\n\neli5.show_weights(perm, feature_names = colnames.tolist())","5c9ad84a":"df_test = pd.read_csv(\"..\/input\/test.csv\")\ndf_test.pop(\"id\");\nX = df_test \ndel df_test\ny_pred = model.predict_proba(X)\ny_pred = y_pred[:,1]\n","b19aacc4":"# submit prediction\nsmpsb_df = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsmpsb_df[\"target\"] = y_pred\nsmpsb_df.to_csv(\"random_forests.csv\", index=None)","d8fc85ad":"Random Forest can also be used to identify variables that are  better predictors. We will use permutation importance in order to measure the importance of the explanatory variables, I suggest you to read the following web page in order to understand this:  \n\nhttps:\/\/www.kaggle.com\/dansbecker\/permutation-importance\n\n","cb9b6c45":"We fit the model with the whole dataset. ","9cd8622c":"We are going to copy the parameters calculated above in the next cell of code. ","ec871b63":"## References\n\n[1] https:\/\/towardsdatascience.com\/random-forests-and-the-bias-variance-tradeoff-3b77fee339b4\n\n[2] https:\/\/www.kaggle.com\/dansbecker\/permutation-importance","8b096e2b":"The basic principle of decision trees is to split each parent node in as distinct node as possible using some kind of optimization criteria such as: gini impurity or entropy. In doing so, we can impose certain conditions, for example: limit the depth of the decision tree in order to reduce overfitting. \n\nIn the above process we do not make any assumption on the other hand there are statistical and machine learning models that are sensitive to skewed distributions. Because of that we do not apply any transformation in the explanatory variables to correct any problem such as: skewed distributions.\n\nIn this kernel we are going to use random forest the main idea of this machine learning model is: \"While an individual tree is overfit to the training data and is likely to have large error, Random Forests uses the insight that a suitably large number of uncorrelated errors average out to zero to solve this problem. Random Forest chooses multiple random samples of observations from the training data, with replacement, constructing a tree from each one. Since each tree learns from different data, they are fairly uncorrelated from one another\". [1]","30622730":"Random Forests have many parameters, you can see them in the following web page: \n\n[Random Forest Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html)\n\nIn order to find the best set of parameters we are going to use a GridSearchCV which is an exhaustive search over the parameters of the model. ","5768b3e8":"We can interpret the numbers in the table above as follows: \n\n\"The first number in each row shows how much model performance decreased with a random shuffling (in this case, using \"accuracy\" as the performance metric)\"[2].\n\n\"Like most things in data science, there is some randomness to the exact performance change from a shuffling a column. We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. The number after the \u00b1 measures how performance varied from one-reshuffling to the next\"[2].\n\nAccording to the table above the best predictors are: 33, 279, 272, 83, 237, 241, 91, 199, 216, 19, 65, 141, 70, 243, 137, 26, 90. Finally, we will send the submission.\n"}}