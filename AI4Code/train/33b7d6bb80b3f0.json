{"cell_type":{"d6770aac":"code","fb6ec9ad":"code","46cb02dc":"code","bc9e50cb":"code","966d8d16":"code","d0083d1f":"code","70d2c047":"code","7ec709cd":"code","2e622a55":"code","1e78fd41":"code","3610bb15":"code","de7d6b14":"code","b7b446da":"code","d8fdf08a":"code","36f41458":"code","a25fb4f8":"code","ebad875c":"code","455ae0f1":"code","89cf0eb0":"code","2fd8f2e0":"code","2aa6f602":"code","b9495538":"code","1ac891d9":"code","e52ed689":"code","710bebbd":"code","328d952b":"code","98328083":"code","b338b7b6":"code","75ee48f1":"code","231dcb4c":"code","e9dcaa85":"code","6601d6b7":"code","e4298b22":"code","241659f7":"code","8405fbdc":"code","d33fc476":"markdown","1b9d4003":"markdown","178f3cbf":"markdown","f1f05c9e":"markdown","d0da86a4":"markdown","d0fcd970":"markdown","66da3387":"markdown","d0cdf48f":"markdown","137a2d80":"markdown","9fe80d2b":"markdown","a1bc6ce0":"markdown","8c885b76":"markdown","ddf619cd":"markdown","effcfa32":"markdown","18281219":"markdown","c205b99a":"markdown","70343d59":"markdown","b938b214":"markdown","bc329210":"markdown","8aa89fe4":"markdown"},"source":{"d6770aac":"# Library of Functions for the OpenClassrooms Multivariate Exploratory Data Analysis Course\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import LineCollection\nimport numpy as np\nimport pandas as pd\nfrom scipy.cluster.hierarchy import dendrogram\nfrom pandas.plotting import parallel_coordinates\nimport seaborn as sns\n\n\npalette = sns.color_palette(\"bright\", 10)\n\ndef display_circles(pcs, n_comp, pca, axis_ranks, labels=None, label_rotation=0, lims=None):\n    \"\"\"Display correlation circles, one for each factorial plane\"\"\"\n\n    # For each factorial plane\n    for d1, d2 in axis_ranks: \n        if d2 < n_comp:\n\n            # Initialise the matplotlib figure\n            fig, ax = plt.subplots(figsize=(20,20))\n\n            # Determine the limits of the chart\n            if lims is not None :\n                xmin, xmax, ymin, ymax = lims\n            elif pcs.shape[1] < 30 :\n                xmin, xmax, ymin, ymax = -1, 1, -1, 1\n            else :\n                xmin, xmax, ymin, ymax = min(pcs[d1,:]), max(pcs[d1,:]), min(pcs[d2,:]), max(pcs[d2,:])\n\n            # Add arrows\n            # If there are more than 30 arrows, we do not display the triangle at the end\n            if pcs.shape[1] < 30 :\n                plt.quiver(np.zeros(pcs.shape[1]), np.zeros(pcs.shape[1]),\n                   pcs[d1,:], pcs[d2,:], \n                   angles='xy', scale_units='xy', scale=1, color=\"grey\")\n                # (see the doc : https:\/\/matplotlib.org\/api\/_as_gen\/matplotlib.pyplot.quiver.html)\n            else:\n                lines = [[[0,0],[x,y]] for x,y in pcs[[d1,d2]].T]\n                ax.add_collection(LineCollection(lines, axes=ax, alpha=.1, color='black'))\n            \n            # Display variable names\n            if labels is not None:  \n                for i,(x, y) in enumerate(pcs[[d1,d2]].T):\n                    if x >= xmin and x <= xmax and y >= ymin and y <= ymax :\n                        plt.text(x, y, labels[i], fontsize='14', ha='center', va='center', rotation=label_rotation, color=\"blue\", alpha=0.5)\n            \n            # Display circle\n            circle = plt.Circle((0,0), 1, facecolor='none', edgecolor='b')\n            plt.gca().add_artist(circle)\n\n            # Define the limits of the chart\n            plt.xlim(xmin, xmax)\n            plt.ylim(ymin, ymax)\n        \n            # Display grid lines\n            plt.plot([-1, 1], [0, 0], color='grey', ls='--')\n            plt.plot([0, 0], [-1, 1], color='grey', ls='--')\n\n            # Label the axes, with the percentage of variance explained\n            plt.xlabel('PC{} ({}%)'.format(d1+1, round(100*pca.explained_variance_ratio_[d1],1)))\n            plt.ylabel('PC{} ({}%)'.format(d2+1, round(100*pca.explained_variance_ratio_[d2],1)))\n            nr=d1+1\n            plt.title(\"Correlation Circle (PC{} and PC{})\".format(d1+1, d2+1))\n            plt.show(block=False)\n            d = {'values': pca.components_[d1], 'factors': labels}\n            df1= pd.DataFrame(d)\n            df1.set_index('factors')\n            df2=df1.sort_values(by='values', ascending=False)\n            df3=df1.sort_values(by='values', ascending=True)\n            print(\"Principal Component\" + str(nr)+ \" Presenting Values\")\n            print(df2.head(3))\n            print(df3.head(3))\n            \n            nr=d2+1\n            \n            d = {'values': pca.components_[d2], 'factors': labels}\n            df1= pd.DataFrame(d)\n            df1.set_index('factors')\n            df2=df1.sort_values(by='values', ascending=False)\n            df3=df1.sort_values(by='values', ascending=True)\n            print(\"Principal Component\" + str(nr)+ \" Presenting Values\")\n            print(df2.head(3))\n            print(df3.head(3))\n        \ndef display_factorial_planes(X_projected, n_comp, pca, axis_ranks, labels=None, alpha=1, illustrative_var=None):\n    '''Display a scatter plot on a factorial plane, one for each factorial plane'''\n\n    # For each factorial plane\n    for d1,d2 in axis_ranks:\n        if d2 < n_comp:\n \n            # Initialise the matplotlib figure      \n            fig = plt.figure(figsize=(7,6))\n        \n            # Display the points\n            if illustrative_var is None:\n                plt.scatter(X_projected[:, d1], X_projected[:, d2], alpha=alpha)\n            else:\n                illustrative_var = np.array(illustrative_var)\n                for value in np.unique(illustrative_var):\n                    selected = np.where(illustrative_var == value)\n                    plt.scatter(X_projected[selected, d1], X_projected[selected, d2], alpha=alpha, label=value)\n                plt.legend()\n\n            # Display the labels on the points\n            if labels is not None:\n                for i,(x,y) in enumerate(X_projected[:,[d1,d2]]):\n                    plt.text(x, y, labels[i],\n                              fontsize='14', ha='center',va='center') \n                \n            # Define the limits of the chart\n            boundary = np.max(np.abs(X_projected[:, [d1,d2]])) * 1.1\n            plt.xlim([-boundary,boundary])\n            plt.ylim([-boundary,boundary])\n        \n            # Display grid lines\n            plt.plot([-100, 100], [0, 0], color='grey', ls='--')\n            plt.plot([0, 0], [-100, 100], color='grey', ls='--')\n\n            # Label the axes, with the percentage of variance explained\n            plt.xlabel('PC{} ({}%)'.format(d1+1, round(100*pca.explained_variance_ratio_[d1],1)))\n            plt.ylabel('PC{} ({}%)'.format(d2+1, round(100*pca.explained_variance_ratio_[d2],1)))\n\n            plt.title(\"Projection of points (on PC{} and PC{})\".format(d1+1, d2+1))\n            #plt.show(block=False)\n   \ndef display_scree_plot(pca):\n    '''Display a scree plot for the pca'''\n\n    scree = pca.explained_variance_ratio_*100\n    plt.bar(np.arange(len(scree))+1, scree)\n    plt.plot(np.arange(len(scree))+1, scree.cumsum(),c=\"red\",marker='o')\n    plt.xlabel(\"Number of principal components\")\n    plt.ylabel(\"Percentage explained variance\")\n    plt.title(\"Scree plot\")\n    plt.show(block=False)\n\ndef append_class(df, class_name, feature, thresholds, names):\n    '''Append a new class feature named 'class_name' based on a threshold split of 'feature'.  Threshold values are in 'thresholds' and class names are in 'names'.'''\n    \n    n = pd.cut(df[feature], bins = thresholds, labels=names)\n    df[class_name] = n\n\ndef plot_dendrogram(Z, names, figsize=(10,25)):\n    '''Plot a dendrogram to illustrate hierarchical clustering'''\n\n    plt.figure(figsize=figsize)\n    plt.title('Hierarchical Clustering Dendrogram')\n    plt.xlabel('distance')\n    dendrogram(\n        Z,\n        labels = names,\n        orientation = \"left\",\n    )\n    #plt.show()\n\ndef addAlpha(colour, alpha):\n    '''Add an alpha to the RGB colour'''\n    \n    return (colour[0],colour[1],colour[2],alpha)\n\ndef display_parallel_coordinates(df, num_clusters):\n    '''Display a parallel coordinates plot for the clusters in df'''\n\n    # Select data points for individual clusters\n    cluster_points = []\n    for i in range(num_clusters):\n        cluster_points.append(df[df.cluster==i])\n    \n    # Create the plot\n    fig = plt.figure(figsize=(12, 15))\n    title = fig.suptitle(\"Parallel Coordinates Plot for the Clusters\", fontsize=18)\n    fig.subplots_adjust(top=0.95, wspace=0)\n\n    # Display one plot for each cluster, with the lines for the main cluster appearing over the lines for the other clusters\n    for i in range(num_clusters):    \n        plt.subplot(num_clusters, 1, i+1)\n        for j,c in enumerate(cluster_points): \n            if i!= j:\n                pc = parallel_coordinates(c, 'cluster', color=[addAlpha(palette[j],0.2)])\n        pc = parallel_coordinates(cluster_points[i], 'cluster', color=[addAlpha(palette[i],0.5)])\n\n        # Stagger the axes\n        ax=plt.gca()\n        for tick in ax.xaxis.get_major_ticks()[1::2]:\n            tick.set_pad(20)        \n\n\ndef display_parallel_coordinates_centroids(df, num_clusters):\n    '''Display a parallel coordinates plot for the centroids in df'''\n\n    # Create the plot\n    fig = plt.figure(figsize=(12, 5))\n    title = fig.suptitle(\"Parallel Coordinates plot for the Centroids\", fontsize=18)\n    fig.subplots_adjust(top=0.9, wspace=0)\n\n    # Draw the chart\n    parallel_coordinates(df, 'cluster', color=palette)\n\n    # Stagger the axes\n    ax=plt.gca()\n    for tick in ax.xaxis.get_major_ticks()[1::2]:\n        tick.set_pad(5)    ","fb6ec9ad":"%config IPCompleter.greedy=True\nimport pandas as pd\nimport numpy as nm\nimport warnings\n\nfrom bokeh.io import output_file, output_notebook\nfrom bokeh.plotting import figure, show\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.layouts import row, column, gridplot\nfrom bokeh.models.widgets import Tabs, Panel\nimport scipy.stats as sst\n\nwarnings.filterwarnings(\"ignore\")\ndata = pd.read_csv(\"..\/input\/world-food-facts\/en.openfoodfacts.org.products.tsv\", delimiter='\\t',dtype='unicode')\ndata.describe()","46cb02dc":"def filterSet(data,term):\n    ##selecting right columns\n    data1= data.filter(regex=term)\n    ##removing all rows without values\n    data1.dropna(axis = 0, how = 'all', inplace = True)\n    ##if column contains less than 1000 observations remove it\n    dropColumns(data1,1000)\n    return data1","bc9e50cb":"def dropColumns(data,count):\n    for column in data:\n        if data[column].count()<=1000:\n            del data[column]\n    return data","966d8d16":"def testDropColumn(data,count):\n    print(\"Dataset has \"+str(len(data.columns))+\" columns before removal\")\n    data1=dropColumns(data,count)\n    data2=data.dropna(thresh=count,how='all',axis=1)\n    if(len(data1.columns)==len(data2.columns)):\n        print( \"Test is ok\")\n    else:\n        print(\"Test did not succeed length by dropna= \"+str(len(data2.columns))+\" length by function \"+ str(len(data2.columns)))\n    ","d0083d1f":"def removeDuplicates(data, term):\n    data1=data\n    ##remove duplicates rows depending on selected column. I use it to delete duplicates for product name\n    data1['add']= term\n    data1=data1.drop_duplicates(subset='add', keep=\"first\")\n    del data1['add']\n    return data1","70d2c047":"def removeDuplicatesTest(data,term,result):\n    res=removeDuplicates(data,term)\n    if res.shape[0]==result.shape[0]:\n        return \"test is OK\"\n    else: \n        return res.shape[0]","7ec709cd":"duplicated=data.drop_duplicates( subset='product_name',keep=\"first\")\nprepared=data.drop('product_name', axis=1)\nremoveDuplicatesTest(prepared,data['product_name'],duplicated)","2e622a55":"def showData(column,data2):\n    import scipy.stats as sst\n    ##preprocessing dataset\n    data2=filterSet(data,column)\n    data2=removeDuplicates(data2,data['product_name'])\n    \n    ##convert object to float and keep positive values\n    data2[data2.columns]=data2[data2.columns].apply(pd.to_numeric, errors='coerce')\n    data2=data2.abs()\n    \n    ##add column with qualitative data\n    data2['main_category']=data['main_category']\n    data2 = data2[pd.notnull(data2['main_category'])]\n    \n    ##fill NAN with 0\n    data2.fillna(0, inplace=True)\n    ##format qualitative column\n    data3=data2[data2[\"main_category\"].str.contains('en')]\n    data3[\"main_category\"]=data3[\"main_category\"].str[3:]\n    \n    ##group by product category having count >1600, we want to have groups with 1600 observations\n    data4=data3.groupby(\"main_category\").filter(lambda x: len(x) > 1600)\n    categories = data4['main_category'].unique()\n    groups = []\n    ##create list that contains lists observations for each category\n    for m in categories:\n        groups.append(data4[data4['main_category']==m][column])\n        \n    ##drawing boxplots\n    plt.boxplot(groups,labels=categories,vert=False,showfliers=False )\n    plt.title(column)\n    plt.xlabel('Measure units')\n    plt.ylabel('Food categories')\n    plt.show()\n    \n    \n    ##F statistic calculation\n    count=0\n    sum=0\n    ##list with mean for each column\n    means=[]\n    for i in groups:\n        mn=i.mean()\n        means.append(mn)\n        count+=len(i)\n        for u in i:\n            sum+=u\n    ##Grand mean\n    MMean=(sum\/count)\n    \n    ##Estimated sum of squares\n    ESS=0\n    ##Residual sum of squares\n    RSS=0\n    ##Degree of freedom TSS\n    DfT=count-1\n    ##Degree of freedom ESS\n    DfE=len(groups)-1\n    ##Degree of freedom RSS\n    DfR=DfT-DfE\n    ccount=0\n    for y in means:\n        ESS+=(y-MMean)**2*len(groups[ccount])\n        for ii in groups[ccount]:\n            RSS+=(ii-y)**2\n        ccount+=1\n    ##Total sum of squares\n    TSS=0\n    for h in groups:\n        for j in h:\n            TSS+=(j-MMean)**2\n    print(\"TTS: \"+str(TSS)+ \" ESS: \"+ str(ESS)+ \" Cooficient of Determination \" +str(ESS\/TSS)+ ' RSS '+ str(RSS))\n    print(\"F value : \"+str((ESS\/DfE)\/(RSS\/DfR)))\n    import scipy.stats as stats\n    print(\"Critical Value: \"+str(stats.f.ppf(q=0.99, dfn=ESS\/DfE, dfd=RSS\/DfR)))\n    print(\"-----------------\")\n    ##direct calculation using scipy library\n    if len(groups)==5:\n        print(stats.f_oneway(groups[0],groups[1],groups[2] ,groups[3],groups[4]  ))\n        print(\"----------Testing Assumptions----------------\")\n        ##print(str(sst.shapiro(groups[0])[1])+\" \"+str(sst.shapiro(groups[1])[1])+\" \"+str(sst.shapiro(groups[2])[1])+\" \"+str(sst.shapiro(groups[3])[1])+\" \"+str(sst.shapiro(groups[4])[1]))\n        ##testing fro normality\n        checko=True\n        for u in groups:\n            if sst.shapiro(u)[1]<0.05:\n                checko=False\n        if checko ==True:\n            print(\"Normality check with Shapiro test is satisfied\")\n        else:\n            print(\"Normality check with Shapiro test is not satisfied\")\n            ##testing for homoskedasticity\n        \n    print(\"Homoskedasticity check with Bartlett test: p=\"+ str(sst.bartlett(groups[0],groups[1],groups[2] ,groups[3],groups[4] )[1]))\n    ","1e78fd41":"def show2Variab(data):\n    ##preproces data\n    data2=data\n    data2.dropna(axis = 0, how = 'all', inplace = True)\n    data2[(data2.T != 0).any()]\n    data2[data2.columns]=data2[data2.columns].apply(pd.to_numeric, errors='coerce')\n    data2=data2.abs()\n    data2.fillna(0, inplace=True)\n    ##remove outliers, we remove 1 percent of largest observations \n    return removeOutliers(data2)","3610bb15":"def removeOutliers(data2):\n    for column in data2:\n        q = data2[column].quantile(0.99)\n        data2=data2[data2[column] < q]\n    return data2","de7d6b14":"dataa=pd.DataFrame(data, columns = ['energy_100g','sugars_100g'])\ndataa=show2Variab(dataa)\ndataa.describe()","b7b446da":"import plotly.express as px\nfig = px.scatter(dataa, x=\"energy_100g\", y=\"sugars_100g\", title=\"Energy\/Sugars\")\nfig.show()\n","d8fdf08a":"dataa=pd.DataFrame(data, columns = ['proteins_100g','sugars_100g'])\ndataa=show2Variab(dataa)\ndataa.describe()","36f41458":"fig = px.scatter(dataa, x=\"proteins_100g\", y=\"sugars_100g\",title='Proteins\/Sugars')\nfig.show()\n\n","a25fb4f8":"dataa=pd.DataFrame(data, columns = ['proteins_100g','energy_100g'])\ndataa=show2Variab(dataa)\ndataa.describe()","ebad875c":"fig = px.scatter(dataa, x=\"proteins_100g\", y=\"energy_100g\",title='Proteins\/Energy')\nfig.show()\n","455ae0f1":"import matplotlib.pyplot as plt\nshowData('proteins_100g',data)\nshowData('fat_100g',data)\nshowData('energy_100g',data)\nshowData('sugars_100g',data)","89cf0eb0":"data1= filterSet(data,'100g')\ndata1=data1.drop([\"nutrition-score-uk_100g\",\"nutrition-score-fr_100g\"], axis=1)\nprint(data1.columns)","2fd8f2e0":"data1=removeDuplicates(data1,data['product_name'])","2aa6f602":"data1[data1.columns]=data1[data1.columns].apply(pd.to_numeric, errors='coerce')\n","b9495538":"data1.dropna(axis=0, thresh=19, inplace=True)","1ac891d9":"data1= data1.abs()","e52ed689":"data1.fillna(0, inplace=True)\n","710bebbd":"from sklearn.preprocessing import StandardScaler\n# Standardize the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(data1)","328d952b":"from sklearn.decomposition import PCA\n\nnum_components = 10\n# Create the PCA model\npca = PCA(n_components=num_components)\n\n# Fit the model with the standardised data\npca.fit(X_scaled)\nnwD=pca.transform(X_scaled)\npcs = pca.components_ \npca.explained_variance_ratio_.cumsum()","98328083":"\ndisplay_scree_plot(pca) ","b338b7b6":"%run functions\n# Generate a correlation circle\npca.components_=np.around(pca.components_, decimals=4)\npcs=pca.components_\ndisplay_circles(pcs, num_components, pca, [(0,1)], labels = np.array(data1.columns),)","75ee48f1":"display_circles(pcs, num_components, pca, [(2,3)], labels = np.array(data1.columns),)","231dcb4c":"num_components=4\npca = PCA(n_components=num_components)\n# Fit the model with the standardised data\npca.fit(X_scaled)\nnwD=pca.transform(X_scaled)\npcs = pca.components_ ","e9dcaa85":"principalDf = pd.DataFrame(nwD, columns = range(1,num_components+1))\nfrom sklearn.cluster import KMeans\n# Run a number of tests, for 1, 2, ... num_clusters\nnum_clusters = 9\nkmeans_tests = [KMeans(n_clusters=i, init='random', n_init=10) for i in range(1, num_clusters)]\nscore = [kmeans_tests[i].fit(principalDf).score(principalDf) for i in range(len(kmeans_tests))]","6601d6b7":"# Plot the curve\nplt.plot(range(1, num_clusters),score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Curve')\nplt.show()","e4298b22":"# Create a k-means clustering model\nkmeans = KMeans(init='random', n_clusters=4, n_init=10)\n\n# Fit the data to the model\nkmeans.fit(principalDf)\n\n# Determine which clusters each data point belongs to:\nclusters =  kmeans.predict(principalDf)","241659f7":"# Add cluster number to the original data\nX_clustered = pd.DataFrame(principalDf, columns=principalDf.columns, index=principalDf.index)\nX_clustered['cluster'] = clusters","8405fbdc":"%run functions\n# Display parallel coordinates plots, one for each cluster\ndisplay_parallel_coordinates(X_clustered, 4)","d33fc476":"## Analysing quantitative data\n_nutrition facts energy and sugars per 100g of various products_","1b9d4003":"## Run Principal Component Analysis with 10 components\n\n##### PCA reduces dimentions but saves important information about the data. It allows to understand what factors influence the variance of sample. It combines correlated variables together into principal components. Different components are not correlated but the correlation exists between variables in one component . Because the method studies variation, variables with different scales should be standartised so they can contribute equally to analysis.\n","178f3cbf":"# No one scatter plot shows a correlation between the reviewed variables\n\n\n\n# Let's review the variation of those nutrition factors related to food category\n\n* present multiple boxplot for each factor according the food category\n* group products into the categories that have at least 1600 observations\n* run ANOVA test \n* The null hypothesis is simply that the means of the nutrition factor groups are the same\n* calculate total sum of squares, residual sum of squares, estimate sum of squares, cooficient of determination\n* Calculate F value, critical value\n\n_box plot allows easy snapshot of major descriptive statistics over a chosen data_\n\n#### ANOVA\n_the test conducted between one categorical and one quantitative variable\\\nthe purpose to test if the difference between the means within different categories belonging to the categorical variable is statistically significant\\\nWe study if mean of nutition factor quantity per 100g of product is dependent on product category_\n##### ANOVA assumptions\n* The samples are independent and they do not influence one another.\n* The dependent variable should be approximately normally distributed for each category.\n* * Testing every group with Shapiro test: h0= samples are coming from normal distribution\n* The population standard deviations of the groups are all equal.\n* * Testing it with Bartlett test: h0= samples have the same variance. Chose barlett test over levene since it is desined for normally distributed data\n","f1f05c9e":"_viewing data on a scatter plot_","d0da86a4":"_nutrition facts proteins and sugars per 100g of various products_","d0fcd970":"# PRODUCT DATASET\n- 162 columns\n- 320749 rows\n- qualitative and quantitative data","66da3387":"_plot the cumulative sum of 10 PCs into a scree plot to investigate optimal number of components_\n \n","d0cdf48f":"## Even F statistic and cooficients of determination shows that we could reject null hypotese, the assumptions of homoskedasticity and normality are not satisfied\n\n# Exploratory Analysis\n\n* Principal Component Analysis to reduce the dimentions and but keep variance of the data\n* K-mean clustering to group the data and find any patterns\n\n## Available columns that include nutrition facts information\n","137a2d80":"### Variables have different scales so we standardize them to be presented on one scale","9fe80d2b":"## Let's review how our 4 reviewed PCs are presented in the different clusters\n### clusters created by K-means are spherical and have similar size\n\n_parallel coordinates plots displays presence of different PC in various clusters_","a1bc6ce0":"## PC1: Vitamins\n### Vitamis C, E and copper are strongly presented and positively correlated in the first principal component that explians 11% of data variation. No significant negative correlation is found in PC1\n\n## PC2: Fat\/Energy\n### Fat, monosaturated fat and energy are strongly presented in PC2 that explains additional 10% of data variation. \n\n# Correlation circle of  PC3 and PC4","8c885b76":"#### 4 clusters looks like optimal number ir gives score of -3000 as within cluster sum\n\n## Running k-mean clustering for 4 n of clusters","ddf619cd":"_viewing data on the scatter plot_","effcfa32":"## Data preprocessing cont\n* remove duplicates \n* convert set to float\n* drop row if it misses half of the values\n* convert negative valuet to positive\n* replace NANs with median","18281219":"_viewing data on the scatter plot to be able to investigate relationship\/ correlation between the variables_","c205b99a":"_nutrition facts proteins and energy per 100g of various products_","70343d59":"## PC3: Minerals\n### Sodium and salt are positively correlated and highly repserented in PC3, vitamin b9 and zinc are negatively correlated with the above.\n\n## PC4: Diet important elements\n\n### Carbohydrates are negatively correlated with cholesterol in PC4.","b938b214":"## Review 4 first components in the correlation circle\n\n### Correlation circle of  PC1 and PC2","bc329210":"### The first cluster includes minerals and diet important elements\n### The second equaly includes all the different dimentions, however it looks like vitamins are presented more in this cluster\n### The third has more vitamins\n### The forth sligthly has also more vitamins","8aa89fe4":"## For our further analysis we will keep 4 dimentions(PC) that we have reviewed previously to perform clustering and to find some pattern in the data. It explains 33 % of the data variation.\n\n#  K-means algorithm\n* used to orginize data in k number of clusters\n* places randomly k number of centroid in multidimentional space \n* assignes individals to the closest centroid\n* moving centroids to the point that represents mean of individual distances to a centroid\n* repeat the process to find clusters with sum of distances to the cluster center\n\n### finding optimal number of clusters with help of elbow curve\n#### y axis :\n<b>Score<\/b> - negative sum of distances of samples to their closest cluster center."}}