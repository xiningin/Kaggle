{"cell_type":{"4eb2159b":"code","9140bcf3":"code","cfb5d94a":"code","293dca4e":"code","3a20a5be":"code","d294c584":"code","af23659b":"code","4d1d0436":"code","773da83b":"code","833824cb":"code","8fe9fa00":"code","7cefcc5a":"code","df1355ba":"code","32202024":"code","1c25e772":"code","95222ad6":"code","e35efa79":"code","b11fca1f":"code","7ba80bfc":"code","3141a999":"code","23a62aea":"code","76ef1fc7":"code","d9e03880":"code","83fd818b":"code","7d818aba":"code","67030dde":"code","70914b00":"code","a72c5b3e":"code","1b215552":"code","1bd61ede":"code","ebcd9991":"code","ff61fb7a":"code","a2c7866a":"code","01c40e86":"code","d19f2949":"code","56f525bc":"code","74907212":"code","d137b363":"code","5b0ed7bd":"code","43da1cb5":"code","dea238b2":"code","ebca708b":"code","fa822bea":"code","fb882b0f":"code","0a8db63d":"code","901c937b":"code","ffe00a1c":"code","3f0b60ed":"code","d5d72479":"code","8c2fa3a8":"code","d23ad92f":"code","11a25ac1":"code","d3a7142b":"code","e5b7e9e5":"code","b778051c":"code","d7a8f6ac":"code","eec96a51":"code","f7b4e6c0":"code","10271efd":"code","206103ee":"code","62c1b442":"markdown","be55624b":"markdown","c2497587":"markdown","46d0927e":"markdown","1d7adc6d":"markdown","6d9f07f6":"markdown","f65aa62b":"markdown","df098d8f":"markdown","1997ade6":"markdown","e3f12659":"markdown"},"source":{"4eb2159b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9140bcf3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pandas import plotting\n\n#plotly \nimport plotly.offline as py\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\ninit_notebook_mode(connected=True)\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn import neighbors\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score, plot_roc_curve, plot_precision_recall_curve, balanced_accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nsns.set(style=\"whitegrid\")\n\nplt.style.use('fivethirtyeight')","cfb5d94a":"df=pd.read_csv('\/kaggle\/input\/heart-disease-prediction-using-logistic-regression\/framingham.csv')","293dca4e":"df.head()","3a20a5be":"df.isnull().sum()","d294c584":"df.shape","af23659b":"sns.kdeplot(df['education'], fill=True, hue=df['diabetes'])","4d1d0436":"sns.kdeplot(df['cigsPerDay'], fill=True, hue=df['diabetes'])","773da83b":"sns.kdeplot(df['BPMeds'], fill=True, hue=df['diabetes'])","833824cb":"sns.kdeplot(df['totChol'], fill=True, hue=df['diabetes'])","8fe9fa00":"sns.kdeplot(df['BMI'], fill=True, hue=df['diabetes'])","7cefcc5a":"sns.kdeplot(df['glucose'], fill=True, hue=df['diabetes'])","df1355ba":"df['education'].value_counts()","32202024":"df['cigsPerDay'].value_counts()","1c25e772":"df['BPMeds'].value_counts()","95222ad6":"df['totChol'].value_counts()","e35efa79":"df['BMI'].value_counts()","b11fca1f":"df['glucose'].value_counts()","7ba80bfc":"df['heartRate'].value_counts()","3141a999":"df.info()","23a62aea":"df['diabetes'].value_counts()","76ef1fc7":"df['education'].fillna(1.0, inplace=True)","d9e03880":"df['cigsPerDay'].fillna(0, inplace=True)","83fd818b":"df['BPMeds'].fillna(0, inplace=True)","7d818aba":"df['totChol'].fillna(240.0, inplace=True)","67030dde":"df['BMI'].fillna(df['BMI'].mean(), inplace=True)","70914b00":"df['glucose'].fillna(df['glucose'].mean(), inplace=True)\ndf['heartRate'].fillna(df['heartRate'].mean(), inplace=True)","a72c5b3e":"df.isnull().sum()","1b215552":"corr = df.corr().round(2)\n\n# Mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set figure size\nf, ax = plt.subplots(figsize=(20, 20))\n\n# Define custom colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.tight_layout()","1bd61ede":"df.columns","ebcd9991":"from statsmodels.stats.outliers_influence import variance_inflation_factor\ncols=['diabetes']\nX=df.drop(cols, axis=1)","ff61fb7a":"X.columns","a2c7866a":"vif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n                          for i in range(len(X.columns))]\nprint(vif_data)","01c40e86":"x=df[['male', 'age', 'education', 'currentSmoker', 'cigsPerDay', 'BPMeds',\n       'prevalentStroke', 'prevalentHyp','totChol', 'sysBP',\n       'diaBP', 'BMI', 'heartRate', 'glucose', 'TenYearCHD']]\ny=df['diabetes']\nX_train, X_test, y_train, y_test=train_test_split(x, y, test_size=0.3, random_state=10)","d19f2949":"from sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix,ConfusionMatrixDisplay","56f525bc":"import statsmodels.api as sm\nimport statsmodels.formula.api as smf","74907212":"lr=LogisticRegression(solver='liblinear')\nlr.fit(X_train, y_train)","d137b363":"y_pred=lr.predict(X_train)","5b0ed7bd":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred))","43da1cb5":"\ny_test_pred=lr.predict(X_test)","dea238b2":"lr_acc=metrics.accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy Score:-\",lr_acc)\nprint(\"F1 Score:-\", metrics.f1_score(y_test, y_test_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_test, y_test_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_test, y_test_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_test, y_test_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_test, y_test_pred))","ebca708b":"cfm=confusion_matrix(y_test, y_test_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cfm,\n                               display_labels=lr.classes_)\ndisp.plot() ","fa822bea":"disp = ConfusionMatrixDisplay(confusion_matrix=cfm,\n                               display_labels=lr.classes_)\ndisp.plot() ","fb882b0f":"y_test_pred_prob=lr.predict_proba(X_test)[:,1]\ny_test_pred_prob\n\nfrom sklearn.metrics import roc_curve","0a8db63d":"metrics.roc_auc_score(y_test, y_test_pred_prob)","901c937b":"metrics.roc_auc_score(y_test, y_test_pred_prob)","ffe00a1c":"fpr, tpr,thresholds=roc_curve(y_test,y_test_pred_prob)\nplt.figure(figsize=(10,10))\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel(\"fpr (False Possitive rate)\")\nplt.ylabel(\"tpr-(True Positive rate)\")\nplt.title(\"ROC_AUC\")\nplt.show()","3f0b60ed":"from sklearn.metrics import precision_recall_curve\nno_skill=len(y==1)\/len(y)\ny_test_prob=lr.predict_proba(X_test)[:,1]\nplt.figure(figsize=(10,8))\nplt.plot([0,1],[no_skill, no_skill], label=\"No Skill\")\nprecision, recall,_ =precision_recall_curve(y_test, y_test_prob)\nplt.plot(recall, precision, marker='',label=\"Logistic Regression\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Recall-Precision Curve\")\nplt.legend()\nplt.show()","d5d72479":"from sklearn.linear_model import LogisticRegression\nmodel= LogisticRegression(solver='liblinear')\nmodel.fit(X_train, y_train)","8c2fa3a8":"threshold = []\naccuracy = []\n\nfor p in np.unique(model.predict_proba(X_train)[:,1]):\n    threshold.append(p)\n    y_pred = (model.predict_proba(X_train)[:,1] >= p).astype(int)\n    accuracy.append(balanced_accuracy_score(y_train,y_pred))","d23ad92f":"plt.figure(figsize=(10,6))\nplt.scatter(threshold,accuracy)\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Balanced accuracy\")\nplt.show()","11a25ac1":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score, plot_roc_curve, plot_precision_recall_curve, balanced_accuracy_score\n\ndef clf_scores(clf, y_predicted):\n    # Accuracy\n    acc_train = clf.score(X_train, y_train)*100\n    acc_test = clf.score(X_test, y_test)*100\n    \n    roc = roc_auc_score(y_test, y_predicted)*100 \n    tn, fp, fn, tp = confusion_matrix(y_test, y_predicted).ravel()\n    cm = confusion_matrix(y_test, y_predicted)\n    correct = tp + tn\n    incorrect = fp + fn\n    d=[acc_train, acc_test,  roc, correct, incorrect,  cm]\n    index=[\"acc_train\",'Test Accuracy',\"Roc Score\",\"COrrect\",\"Incorrect\",\"Confusion\"  ]\n    output=pd.DataFrame(data=d, index=index)\n    \n    d=sns.heatmap(cm, annot=True)\n    dd=plot_roc_curve(clf, X_train, y_train)\n    ddd=plot_precision_recall_curve(clf, X_train, y_train)\n\n    return output,d, dd, ddd","d3a7142b":"#1. Logistic regression\n\nfrom sklearn.linear_model import LogisticRegression\nclf_lr = LogisticRegression(solver='liblinear')\nclf_lr.fit(X_train, y_train)\n\nY_pred_lr = clf_lr.predict(X_test)\nprint(clf_scores(clf_lr, Y_pred_lr))","e5b7e9e5":"# 2 Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nclf_rf = RandomForestClassifier()\nclf_rf.fit(X_train, y_train)\n\nY_pred_rf = clf_rf.predict(X_test)\nprint(clf_scores(clf_rf, Y_pred_rf))","b778051c":"# 3 XGboost\nfrom sklearn.ensemble import GradientBoostingClassifier\nclf_xg = GradientBoostingClassifier()\nclf_xg.fit(X_train, y_train)\n\nY_pred_xg = clf_xg.predict(X_test)\nprint(clf_scores(clf_xg, Y_pred_xg))","d7a8f6ac":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n{'n_estimators': [10, 25], 'max_features': [5, 10], \n 'max_depth': [10, 50, None], 'bootstrap': [True, False]}\n]\n\ngrid_search_forest = GridSearchCV(clf_rf, param_grid, cv=10, scoring='neg_mean_squared_error')\ngrid_search_forest.fit(X_train, y_train)","eec96a51":"#now let's how the RMSE changes for each parameter configuration\ncvres = grid_search_forest.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","f7b4e6c0":"#find the best model of grid search\ngrid_search_forest.best_estimator_","10271efd":"# Performance metrics\ngrid_best= grid_search_forest.best_estimator_.predict(X_train)\nerrors = abs(grid_best - y_train)\n# Calculate mean absolute percentage error (MAPE)\nmape = np.mean(100 * (errors \/ y_train))\n# Calculate and display accuracy\naccuracy = 100 - mape    \n#print result\nprint('The best model from grid-search has an accuracy of', round(accuracy, 2),'%')","206103ee":"# Tuned Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nclf_rf = RandomForestClassifier(max_depth=50, max_features=5, n_estimators=10)\nclf_rf.fit(X_train, y_train)\n\nY_pred_rf = clf_rf.predict(X_test)\nprint(clf_scores(clf_rf, Y_pred_rf))","62c1b442":"# Multicollinearity\u00b6\n","be55624b":"# Response variable","c2497587":"# Logistic Regresion\u00b6\n","46d0927e":"# Grid Search\nIn this grid search I will try different combinations of RF hyperparameters.\n\nMost important hyperparameters of Random Forest:\n\n* n_estimators = n of trees\n* max_features = max number of features considered for splitting a node\n* max_depth = max number of levels in each decision tree\n* min_samples_split = min number of data points placed in a node before the node is split\n* min_samples_leaf = min number of data points allowed in a leaf node\n* bootstrap = method for sampling data points (with or without replacement)","1d7adc6d":"# Fine-tune Random Forest","6d9f07f6":"# Threshold","f65aa62b":"# Check the distribution of the missing values","df098d8f":"# Other Models","1997ade6":"# Correlation Matrix","e3f12659":"# Filling the missing values"}}