{"cell_type":{"2db0d7da":"code","47553e8c":"code","2960f157":"code","826e6c3e":"code","1b2831c6":"code","6774cd13":"code","600687d7":"code","8b413830":"code","5419825d":"code","04d7ab5f":"code","a1744b98":"code","05514687":"code","0f152b0b":"code","5ad97893":"code","c14158a2":"code","5413c88e":"code","8780ed5d":"code","8086c6c0":"code","e61b109b":"code","751899b1":"code","6e0e705d":"code","0d7f607e":"code","196a5e7e":"markdown","8f7a118a":"markdown","fc31edd2":"markdown","f3d359ae":"markdown","b1594250":"markdown","cb02a4a4":"markdown","a32665cb":"markdown","477ffb86":"markdown","7068f8d3":"markdown","14e93793":"markdown","42cc2edc":"markdown","e4b27228":"markdown"},"source":{"2db0d7da":"!mkdir squad\n!mkdir results\n!mkdir logs\n!mkdir models\n!wget https:\/\/rajpurkar.github.io\/SQuAD-explorer\/dataset\/train-v2.0.json -O squad\/train-v2.0.json\n!wget https:\/\/rajpurkar.github.io\/SQuAD-explorer\/dataset\/dev-v2.0.json -O squad\/dev-v2.0.json","47553e8c":"import json\nfrom pathlib import Path\n\ndef read_squad(path):\n    path = Path(path)\n    with open(path, 'rb') as f:\n        squad_dict = json.load(f)\n\n    contexts = []\n    questions = []\n    answers = []\n    for group in squad_dict['data']:\n        for passage in group['paragraphs']:\n            context = passage['context']\n            for qa in passage['qas']:\n                question = qa['question']\n                for answer in qa['answers']:\n                    contexts.append(context)\n                    questions.append(question)\n                    answers.append(answer)\n\n    return contexts, questions, answers\n\ntrain_contexts, train_questions, train_answers = read_squad('squad\/train-v2.0.json')\nval_contexts, val_questions, val_answers = read_squad('squad\/dev-v2.0.json')","2960f157":"def add_end_idx(answers, contexts):\n    for answer, context in zip(answers, contexts):\n        gold_text = answer['text']\n        start_idx = answer['answer_start']\n        end_idx = start_idx + len(gold_text)\n\n        \n        if context[start_idx:end_idx] == gold_text:\n            answer['answer_end'] = end_idx\n        elif context[start_idx-1:end_idx-1] == gold_text:\n            answer['answer_start'] = start_idx - 1\n            answer['answer_end'] = end_idx - 1     \n        elif context[start_idx-2:end_idx-2] == gold_text:\n            answer['answer_start'] = start_idx - 2\n            answer['answer_end'] = end_idx - 2     \n\nadd_end_idx(train_answers, train_contexts)\nadd_end_idx(val_answers, val_contexts)","826e6c3e":"from transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\ntrain_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\nval_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)","1b2831c6":"print(tokenizer.model_max_length)","6774cd13":"def add_token_positions(encodings, answers):\n    start_positions = []\n    end_positions = []\n    for i in range(len(answers)):\n        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n        # if None, the answer passage has been truncated\n        if start_positions[-1] is None:\n            start_positions[-1] = tokenizer.model_max_length -1\n        if end_positions[-1] is None:\n            end_positions[-1] = tokenizer.model_max_length -1\n    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n\nadd_token_positions(train_encodings, train_answers)\nadd_token_positions(val_encodings, val_answers)","600687d7":"import tensorflow as tf\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    {key: train_encodings[key] for key in ['input_ids', 'attention_mask']},\n    {key: train_encodings[key] for key in ['start_positions', 'end_positions']}\n))\nprint('train_dataset')\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    {key: val_encodings[key] for key in ['input_ids', 'attention_mask']},\n    {key: val_encodings[key] for key in ['start_positions', 'end_positions']}\n))","8b413830":"from transformers import TFDistilBertForQuestionAnswering\nmodel = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")","5419825d":"# Keras will expect a tuple when dealing with labels\ntrain_dataset = train_dataset.map(lambda x, y: (x, (y['start_positions'], y['end_positions'])))\n\n# Keras will assign a separate loss for each output and add them together. So we'll just use the standard CE loss\n# instead of using the built-in model.compute_loss, which expects a dict of outputs and averages the two terms.\n# Note that this means the loss will be 2x of when using TFTrainer since we're adding instead of averaging them.\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.distilbert.return_dict = False # if using \ud83e\udd17 Transformers >3.02, make sure outputs are tuples\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\nmodel.compile(optimizer=optimizer, loss=loss,metrics=[\"sparse_categorical_accuracy\"]) # can also use any keras loss fn\nmodel.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)","04d7ab5f":"model.summary()","a1744b98":"model.save('.\/models')","05514687":"list(val_dataset)","0f152b0b":"print(\"predict\")\npredictions = model.predict(val_dataset)","5ad97893":"print(predictions[:3])","c14158a2":"val_dataset = val_dataset.map(lambda x, y: (x, (y['start_positions'], y['end_positions'])))","5413c88e":"print(\"Evaluate\")\nresult = model.evaluate(val_dataset.shuffle(1000).batch(16))\ndict(zip(model.metrics_names, result))","8780ed5d":"loss, acc = model.evaluate(val_dataset.batch(16), verbose=2)\nprint(\"Model, accuracy: {:5.2f}%\".format(100 * acc))","8086c6c0":"features, label = iter(val_dataset).next()","e61b109b":"label","751899b1":"features","6e0e705d":"print(train_dataset)\nprint(val_dataset)","0d7f607e":"from transformers import pipeline\nnlp = pipeline(task=\"question-answering\", model=model, tokenizer=tokenizer)\n\ncontext = \"\"\"\nExtractive Question Answering is the task of extracting an answer from a text given a question. An example of a\nquestion answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\na model on a SQuAD task, you may leverage the examples\/question-answering\/run_squad.py script.\n\"\"\"\n\nresult = nlp(question=\"What is extractive question answering?\", context=context)\nprint(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")","196a5e7e":"The contexts and questions are just strings. The answers are dicts containing the subsequence of the passage with the correct answer as well as an integer indicating the character at which the answer begins. In order to train a model on this data we need (1) the tokenized context\/question pairs, and (2) integers indicating at which token positions the answer begins and ends.\n\nFirst, let\u2019s get the character position at which the answer ends in the passage (we are given the starting position). Sometimes SQuAD answers are off by one or two characters, so we will also adjust for that.","8f7a118a":"Now train_answers and val_answers include the character end positions and the corrected start positions. Next, let\u2019s tokenize our context\/question pairs. \ud83e\udd17 Tokenizers can accept parallel lists of sequences and encode them together as sequence pairs.","fc31edd2":"Each split is in a structured json file with a number of questions and answers for each passage (or context). We\u2019ll take this apart into parallel lists of contexts, questions, and answers (note that the contexts here are repeated since there are multiple questions per context):","f3d359ae":"**Question answering** comes in many forms. In this example, we\u2019ll look at the particular type of extractive QA that involves answering a question about a passage by highlighting the segment of the passage that answers the question. This involves fine-tuning a model which predicts a start position and an end position in the passage. We will use the Stanford Question Answering Dataset (SQuAD) 2.0.\n\nWe will start by downloading the data:","b1594250":"Our data is ready. Let\u2019s just put it in a PyTorch\/TensorFlow dataset so that we can easily use it for training. In PyTorch, we define a custom Dataset class. In TensorFlow, we pass a tuple of (inputs_dict, labels_dict) to the from_tensor_slices method.","cb02a4a4":"## **Download SQuAD 2.0 Data**","a32665cb":"## **Note :**","477ffb86":"Please write your code in the cells with the \"**Your code here**\" placeholder.","7068f8d3":"Next we need to convert our character start\/end positions to token start\/end positions. When using \ud83e\udd17 Fast Tokenizers, we can use the <b>built in char_to_token()<\/b> method.","14e93793":"Note : This dataset can be explored in the Hugging Face model hub (SQuAD V2), and can be alternatively downloaded with the \ud83e\udd17 NLP library with load_dataset(\"squad_v2\").","42cc2edc":"Now we can use a DistilBert model with a QA head for training:","e4b27228":"The data and model are both ready to go. You can train the model with Trainer\/TFTrainer exactly as in the sequence classification example above. If using native PyTorch, replace labels with start_positions and end_positions in the training example. If using Keras\u2019s fit, we need to make a minor modification to handle this example since it involves multiple model outputs."}}