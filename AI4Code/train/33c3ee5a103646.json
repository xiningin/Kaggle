{"cell_type":{"87efdf5a":"code","83d4d2a7":"code","f582dd8e":"code","a2c53eea":"code","55a38136":"code","a46226ed":"code","d5bb42c4":"code","b5b17aac":"code","bdff5956":"code","5ca66108":"code","55bd563e":"code","a42bcc5a":"code","d5a6e693":"code","9f5321bb":"code","ceec6cbd":"code","9be7b816":"code","26a1ff2a":"code","d0c106b2":"code","89006ca0":"code","9eb4a558":"code","12b04119":"code","c2db9176":"code","0601d0cc":"code","70b75e2d":"code","1d46bc6c":"code","1c30cbfe":"code","b452996a":"code","fdda7b67":"code","70b8aa96":"code","35cbde96":"code","303aa865":"code","5c7e582f":"code","ef00bd3c":"code","c090122e":"code","7e68b330":"code","69d59c80":"code","b0305182":"code","d5423112":"code","23362d23":"code","39956e59":"code","eacd4073":"code","40467126":"code","65437d94":"code","8bd704f1":"code","7faacd45":"code","e0e960dd":"code","9bbd12db":"code","5c31f327":"code","3069c696":"code","c3881c56":"code","c699de4b":"code","7852c5ca":"code","c4881356":"code","44e95c5f":"code","98a929a7":"code","55da7da0":"code","c87b55b3":"code","4f557c16":"code","cd114f84":"code","d1032448":"code","b3407619":"code","1f4fa63f":"code","873a1fd9":"code","99528e0b":"code","615de677":"code","c2191c1c":"code","bdb391f8":"code","86932a5a":"code","48834f36":"code","4450c4b8":"code","12a49344":"code","cfb6362a":"code","5e408b89":"code","4aa20210":"code","e085e683":"code","60078e0f":"code","aad1d61c":"code","71adfa52":"code","0f61a72c":"code","71cd7289":"code","44a92de2":"code","c78eaa50":"code","b86d93e3":"code","6d35de2a":"code","db7279df":"code","d6f8a6ae":"code","49fac349":"code","4fd2804a":"code","3cdc0878":"code","0d9aff55":"code","5bee8c27":"code","c0f16f7b":"code","6bec6bfd":"code","f8bd8a27":"code","dd0784e4":"code","de96b949":"code","8c6ab698":"code","ca6215f8":"code","c8e8ef9a":"code","a9f848b2":"code","8aa7e441":"code","cf65ae16":"code","89a0afed":"code","01fd4373":"code","964fade2":"code","5dc849a9":"code","abac3802":"code","1a98c6a0":"code","68d0f626":"code","e35acc81":"code","423363a1":"code","344627c2":"code","fad36ae4":"code","82869002":"code","2c5a993e":"code","2d9b3a3b":"code","673e4c45":"code","583ed0bc":"code","8724cc37":"code","4c094d15":"code","4702b23a":"code","900b6554":"code","5154c185":"code","02005537":"code","882502c1":"code","f1f6c5ee":"code","a9ec82e9":"code","388b584e":"code","5761a086":"code","9c0848d6":"code","4b4663ea":"code","f7bece25":"code","0587eb95":"code","c6afea3a":"code","e263126a":"code","bc83c77e":"code","07e5e92c":"code","61c2e156":"code","2a64e36d":"code","bba91d07":"code","07b4be68":"code","75e4ae59":"code","18239c9c":"code","7b495047":"code","6cbd3e5c":"code","038a287b":"code","e450d7de":"code","728adcc2":"code","95665505":"code","221c1af9":"code","d5a71986":"code","3dccd800":"code","d21b7837":"code","0a85c352":"code","c75c4652":"code","1e350f01":"code","a30fab84":"code","8c61f65b":"code","ccc28a4d":"code","9cb51a41":"code","8bff1514":"code","c4b49429":"code","e5bdeac3":"code","8149b8d6":"code","0a587bcf":"code","3449d364":"code","68993b49":"code","00ad53d1":"code","d93f0921":"code","92fcc2bf":"code","3e034635":"code","9f40aaa0":"code","6380e154":"code","6fa704f1":"code","5f9f62a6":"code","270d0e0a":"code","0de7b7a4":"code","ac9efea7":"code","dc75606d":"code","da1fcc58":"code","6ab441b6":"code","c05a9702":"code","47591554":"code","231380b3":"code","9b539903":"code","c4dcefc5":"code","94e704db":"code","c262dcdf":"code","6fe462d5":"code","b21bc725":"code","fb7e2056":"code","6127985f":"code","8ecabba5":"code","2a3c441d":"code","45904808":"code","56a7590d":"code","e1def456":"code","5d430a76":"code","238fbebe":"code","20a43b46":"code","b49371d7":"code","dac72168":"code","b5e480ba":"code","bff4109d":"code","56d7ae91":"code","bc81c925":"code","bb91aa87":"code","8a1940f9":"markdown","5eca1365":"markdown","da8ed439":"markdown","d717f75d":"markdown","158c3f42":"markdown","03bb8c26":"markdown","e30b77a4":"markdown","fd3205bc":"markdown","27e8956b":"markdown","e7564901":"markdown","e97ae492":"markdown","54bc3cc7":"markdown","ffe6e1fa":"markdown","d95649f7":"markdown","c4d4e4a0":"markdown","92d22714":"markdown","40ce29f8":"markdown","eedaf1fa":"markdown","5a73cbdb":"markdown","6b790cb3":"markdown","3e6a3f14":"markdown","12866a50":"markdown","faf80748":"markdown","9aa44c7f":"markdown","84a3e25a":"markdown","32be6b68":"markdown","e6da9a97":"markdown","cfed9f15":"markdown","5904451b":"markdown","84085c65":"markdown","9cd4f04c":"markdown","bfed7961":"markdown","872fbaef":"markdown"},"source":{"87efdf5a":"import numpy as np\nimport os\nimport pandas as pd \nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nfrom sklearn.preprocessing import scale \nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","83d4d2a7":"test = pd.read_csv(\"test.csv\") #importing test data set \ndf_test = test.copy()\ntrain = pd.read_csv(\"train.csv\") #importing test data set \ndf_train = train.copy()\n","f582dd8e":"df_test.head()","a2c53eea":"df_train.head()","55a38136":"df_train.columns","a46226ed":"df_train.isnull().values.any()","d5bb42c4":"df_test.info()","b5b17aac":"df_train.info()","bdff5956":"df_train.describe().T","5ca66108":" ## handling the null values","55bd563e":"df_train.isnull().sum() # I'll delete the variable cabin because of it's over 78 percent, ","a42bcc5a":"df_train[\"Cabin\"]","d5a6e693":"df_train = df_train.drop([\"Cabin\",\"Name\",\"Ticket\"], axis=1) # I'll delete the variable cabin because of it's over 78 percent, \ndf_test = df_test.drop([\"Cabin\",\"Name\", \"Ticket\",], axis =1) # same processes on the our test dataset\n# we can understand that the name, and ticket, variables can't effect our independent variable which is survived","9f5321bb":"df_test.isnull().sum()","ceec6cbd":"df_train.isnull().sum()","9be7b816":"# Filling missing Embarked values with most common value\ndf_train['Embarked'] = df_train['Embarked'].fillna(df_train['Embarked'].mode()[0])","26a1ff2a":"df_train.isnull().sum()","d0c106b2":"# Filling missing Fare value with mean value\ndf_test['Fare'] = df_test['Fare'].fillna(df_test['Fare'].mean())","89006ca0":"df_test.isnull().sum()","9eb4a558":"#Pclass' is a categorical feature so we convert its values to strings\ndf_train['Pclass'] = train['Pclass'].apply(str)\ndf_test['Pclass'] = test['Pclass'].apply(str)","12b04119":"from ycimpute.imputer import knnimput","c2db9176":"n_dftrain = df_train.select_dtypes(include = ['float64', 'int64']) #because the imputer method don't except the string value \nn_dftest = df_train.select_dtypes(include = ['float64', 'int64'])","0601d0cc":"var_namestrain = list(n_dftrain)\nvar_namestest = list(n_dftest)","70b75e2d":"var_namestrain","1d46bc6c":"var_namestest","1c30cbfe":"#we have changed the our dataframe to numpy array because our knninmput want it \nn_dftrain = np.array(n_dftrain) \nn_dftest = np.array(n_dftest) ","b452996a":"n_dftrain = knnimput.KNN(k = 4).complete(n_dftrain)\nn_dftest = knnimput.KNN(k = 4).complete(n_dftest)","fdda7b67":"type(n_dftrain) # we have change this to pandas DataFrame","70b8aa96":"n_dftrain = pd.DataFrame(n_dftrain, columns = var_namestrain)\nn_dftest = pd.DataFrame(n_dftest, columns = var_namestest)","35cbde96":"n_dftrain.head()","303aa865":"n_dftest.head()","5c7e582f":"#We replaced our age variable with the one with null values filled in by ML model\ndf_train[\"Age\"] = n_dftrain[\"Age\"] \ndf_test[\"Age\"] = n_dftest[\"Age\"]","ef00bd3c":"df_train.isnull().sum()","c090122e":"df_test.isnull().sum() # we handle the missing value issue by using KNN imputation","7e68b330":"# Let\u2019s perform a basic one hot encoding of categorical features","69d59c80":"for col in df_train.dtypes[df_train.dtypes == 'object'].index:\n   for_dummy = df_train.pop(col)\n   df_train = pd.concat([df_train, pd.get_dummies(for_dummy, prefix=col)], axis=1)","b0305182":"for col in df_test.dtypes[df_test.dtypes == 'object'].index:\n   for_dummy = df_test.pop(col)\n   df_test = pd.concat([df_test, pd.get_dummies(for_dummy, prefix=col)], axis=1)","d5423112":"df_train.head()","23362d23":"df_test.head()","39956e59":"# we convert the some  float values of Age to integer\ndf_train['Age'] = df_train['Age'].apply(int)\ndf_test['Age'] = df_test['Age'].apply(int)","eacd4073":"df_train.head()","40467126":"df_test.head()","65437d94":"# we have finished the our data preparation now we can start creating our model ","8bd704f1":"df_train[\"Survived\"].value_counts().plot.barh(); # we can see that distribution of variable Survived by bar plot","7faacd45":"df_train.describe().T","e0e960dd":"y = df_train[\"Survived\"] # this is our dependent variable","9bbd12db":"X = df_train.drop([\"Survived\"], axis=1) # these are our independent variables","5c31f327":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.30, \n                                                    random_state = 42)\n# we splitted our dataset to test and train","3069c696":"from sklearn.linear_model import LogisticRegression","c3881c56":"# we created our model then have fitted it to our train data\nloj = LogisticRegression(solver = \"liblinear\") \nloj_model = loj.fit(X_train,y_train)\nloj_model","c699de4b":"loj_model.intercept_ # interception to y axis","7852c5ca":"loj_model.coef_ #coefficent of our independent variables","c4881356":"y_pred = loj_model.predict(X_train) # now our model is predicting the y","44e95c5f":"accuracy_score(y_train ,y_pred) # this our train accuracy score it is bad predictor of our test score","98a929a7":"accuracy_score(y_test ,loj_model.predict(X_test)) #  accuracy score","55da7da0":"cross_val_score(loj_model, X_test, y_test, cv=10)","c87b55b3":"cross_val_score(loj_model, X_test, y_test, cv=10).mean() # verified accuracy score","4f557c16":"y_pred2 = loj_model.predict(df_test) # we predicted our test data set","cd114f84":"y_pred2[:5]","d1032448":"df = pd.DataFrame(y_pred2, columns = ['Survived']) # we change te numpy array's type to pandas dataframe","b3407619":"test1 = pd.concat([df_test[\"PassengerId\"], df], axis=1) # concatenating the passenger\u0131d and","1f4fa63f":"test1.head() ","873a1fd9":"test1.to_csv('test1.csv',index=False) # to csv file in your current workig directory\n#now yuo can upload your predicts yo kaggle competition","99528e0b":"y = df_train[\"Survived\"] # this is our dependent variable","615de677":"X = df_train.drop([\"Survived\"], axis=1) # these are our independent variables","c2191c1c":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.30, \n                                                    random_state = 42)\n# we splitted our dataset to test and train","bdb391f8":"from sklearn.naive_bayes import GaussianNB","86932a5a":"# we created our model then have fitted it to our train data\nnb = GaussianNB()\nnb_model = nb.fit(X_train, y_train) \nnb_model","48834f36":"nb_model.predict(X_test)[0:10] # we can see predicted values like this","4450c4b8":"nb_model.predict_proba(X_test)[0:10] # these are probablity of predicted values","12a49344":"y_pred = nb_model.predict(X_test) #  now our model is predicting the y values","cfb6362a":"accuracy_score(y_test ,y_pred) # test do\u011fruluk oran\u0131","5e408b89":"cross_val_score(nb_model, X_test, y_test, cv=10).mean() # verified accuracy score","4aa20210":"y = df_train[\"Survived\"] # this is our dependent variable","e085e683":"X = df_train.drop([\"Survived\"], axis=1) # these are our independent variables","60078e0f":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.30, \n                                                    random_state = 42)\n# we splitted our dataset to test and train","aad1d61c":"knn = KNeighborsClassifier()\nknn_model = knn.fit(X_train, y_train)\nknn_model","71adfa52":"y_pred = knn_model.predict(X_test) #  now our model is predicting the y values","0f61a72c":"accuracy_score(y_test, y_pred) # primitive score","71cd7289":"knn_params = {\"n_neighbors\": np.arange(1,50)}","44a92de2":"knn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, knn_params, cv=10) # we implement algoritm \nknn_cv.fit(X_train, y_train)","c78eaa50":"print(\"Best Score:\" + str(knn_cv.best_score_)) ","b86d93e3":"print(\"Best Parameters: \" + str(knn_cv.best_params_))","6d35de2a":"knn = KNeighborsClassifier(25) #  We created our model with best parameters \nknn_tuned = knn.fit(X_train, y_train)    ","db7279df":"knn_tuned.score(X_test, y_test)","d6f8a6ae":"y = df_train[\"Survived\"] # this is our dependent variable","49fac349":"X = df_train.drop([\"Survived\"], axis=1) # these are our independent variables","4fd2804a":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.30, \n                                                    random_state = 42)\n# we splitted our dataset to test and train","3cdc0878":"svm_model = SVC(kernel = 'linear').fit(X_train, y_train)","0d9aff55":"y_pred = svm_model.predict(X_test)","5bee8c27":"accuracy_score(y_test , y_pred)","c0f16f7b":"svc_params = {\"C\": np.arange(1,10)} # 0 value gives us an exception \n\nsvc = SVC(kernel = \"linear\")\n\nsvc_cv_model = GridSearchCV(svc,svc_params, \n                            cv = 10, \n                            n_jobs = -1, \n                            verbose = 2 )","6bec6bfd":"svc_cv_model.fit(X_train, y_train)","f8bd8a27":"print(\"Best Params: \" + str(svc_cv_model.best_params_))","dd0784e4":"svc_tuned = SVC(kernel = \"linear\", C = 3).fit(X_train, y_train) # we created new model with best params that we have found","de96b949":"y_pred = svc_tuned.predict(X_test)\naccuracy_score(y_test, y_pred)","8c6ab698":"y = df_train[\"Survived\"] # this is our dependent variable","ca6215f8":"X = df_train.drop([\"Survived\"], axis=1) # these are our independent variables","c8e8ef9a":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.30, \n                                                    random_state = 42)\n# we splitted our dataset to test and train","a9f848b2":"svc_model = SVC(kernel = \"rbf\").fit(X_train, y_train) # we using kernel = rbf (radial basis function)","8aa7e441":"y_pred = svc_model.predict(X_test)\naccuracy_score(y_test, y_pred) # primitive score","cf65ae16":"svc_params = {\"C\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100],\n             \"gamma\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100]}","89a0afed":"svc = SVC()\nsvc_cv_model = GridSearchCV(svc, svc_params, \n                         cv = 10, \n                         n_jobs = -1,\n                         verbose = 2)\n\nsvc_cv_model.fit(X_train, y_train)","01fd4373":"print(\"Best Params: \" + str(svc_cv_model.best_params_))","964fade2":"svc_tuned = SVC(C = 100, gamma = 0.0001).fit(X_train, y_train)","5dc849a9":"y_pred = svc_tuned.predict(X_test)\naccuracy_score(y_test, y_pred)","abac3802":"y = df_train[\"Survived\"] # this is our dependent variable","1a98c6a0":"X = df_train.drop([\"Survived\"], axis=1) # these are our independent variables","68d0f626":"# import our scaler object\nfrom sklearn.preprocessing import StandardScaler ","e35acc81":"scaler = StandardScaler() # OOP sampling","423363a1":"# we transform our data\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","344627c2":"X_train_scaled[0:2]","fad36ae4":"X_test_scaled[0:2]","82869002":"from sklearn.neural_network import MLPClassifier","2c5a993e":"mlpc = MLPClassifier().fit(X_train_scaled, y_train)","2d9b3a3b":"mlpc.coefs_","673e4c45":"y_pred = mlpc.predict(X_test_scaled)\naccuracy_score(y_test, y_pred) # primitive score","583ed0bc":"mlpc_params = {\"alpha\": [0.1, 0.01, 0.02, 0.005, 0.0001,0.00001],\n              \"hidden_layer_sizes\": [(10,10,10),\n                                     (100,100,100),\n                                     (100,100),\n                                     (3,5), \n                                     (5, 3)],\n              \"solver\" : [\"lbfgs\",\"adam\",\"sgd\"],\n              \"activation\": [\"relu\",\"logistic\"]}\n","8724cc37":"from sklearn.model_selection import RandomizedSearchCV","4c094d15":"# we use RandomizedSearchCV because its faster than GridSearchcv\nmlpc = MLPClassifier()\nmlpc_cv_model = RandomizedSearchCV(mlpc, mlpc_params, \n                         cv = 10, \n                         n_jobs = -1,\n                         verbose = 2)\n\nmlpc_cv_model.fit(X_train_scaled, y_train) ","4702b23a":"print(\"Best Params: \" + str(mlpc_cv_model.best_params_))","900b6554":"mlpc_tuned = MLPClassifier(activation = \"logistic\", \n                           alpha = 0.1, \n                           hidden_layer_sizes = (100, 100),\n                          solver = \"adam\")","5154c185":"mlpc_tuned.fit(X_train_scaled, y_train)","02005537":"y_pred = mlpc_tuned.predict(X_test_scaled)\naccuracy_score(y_test, y_pred) # our validated score","882502c1":"y = df_train[\"Survived\"] # this is our dependent variable","f1f6c5ee":"X = df_train.drop([\"Survived\"], axis=1) # these are our independent variables","a9ec82e9":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.30, \n                                                    random_state = 42)\n# we splitted our dataset to test and train","388b584e":"from sklearn.tree import DecisionTreeClassifier","5761a086":"cart = DecisionTreeClassifier()\ncart_model = cart.fit(X_train, y_train)","9c0848d6":"from skompiler import skompile\nprint(skompile(cart_model.predict).to(\"python\/code\")) # this is our decision tree it has turned into the ython code","4b4663ea":"y_pred = cart_model.predict(X_test)\naccuracy_score(y_test, y_pred) # this is primitive score","f7bece25":"cart_grid = {\"max_depth\": range(1,10),\n            \"min_samples_split\" : list(range(2,50)) }","0587eb95":"cart = tree.DecisionTreeClassifier()\ncart_cv = GridSearchCV(cart, cart_grid, cv = 10, n_jobs = -1, verbose = 2)\ncart_cv_model = cart_cv.fit(X_train, y_train)","c6afea3a":"print(\"Best Params: \" + str(cart_cv_model.best_params_))","e263126a":"cart = tree.DecisionTreeClassifier(max_depth = 5, min_samples_split = 19) # we have created our model again with best params\ncart_tuned = cart.fit(X_train, y_train)","bc83c77e":"y_pred = cart_tuned.predict(X_test)\naccuracy_score(y_test, y_pred) # this is our accuracy score","07e5e92c":"y = df_train[\"Survived\"] # this is our dependent variable","61c2e156":"X = df_train.drop([\"Survived\"], axis=1) # these are our independent variables","2a64e36d":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.30, \n                                                    random_state = 42)\n# we splitted our dataset to test and train","bba91d07":"from sklearn.ensemble import RandomForestClassifier","07b4be68":"rf_model = RandomForestClassifier().fit(X_train, y_train)","75e4ae59":"y_pred = rf_model.predict(X_test)\naccuracy_score(y_test, y_pred) # primitive score","18239c9c":"rf_params = {\"max_depth\": [2,5,8,10],\n            \"max_features\": [2,5,8],\n            \"n_estimators\": [10,500,1000],\n            \"min_samples_split\": [2,5,10]}","7b495047":"rf_model = RandomForestClassifier()\n\nrf_cv_model = RandomizedSearchCV(rf_model, \n                           rf_params, \n                           cv = 10, \n                           n_jobs = -1, \n                           verbose = 2) ","6cbd3e5c":"rf_cv_model.fit(X_train, y_train)","038a287b":"print(\"Best Params: \" + str(rf_cv_model.best_params_))","e450d7de":"#final model with best params that we have found\nrf_tuned = RandomForestClassifier(max_depth = 10,\n                                  max_features = 5, \n                                  min_samples_split = 10,\n                                  n_estimators = 500)\n\nrf_tuned.fit(X_train, y_train)","728adcc2":"y_pred = rf_tuned.predict(X_test)\naccuracy_score(y_test, y_pred)","95665505":"Importance = pd.DataFrame({\"Importance\": rf_tuned.feature_importances_*100},\n                         index = X_train.columns)","221c1af9":"Importance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"r\")\n\nplt.xlabel(\"Variable Significance Levels\")","d5a71986":"y = df_train[\"Survived\"] # this is our dependent variable","3dccd800":"X = df_train.drop([\"Survived\"], axis=1) # these are our independent variables","d21b7837":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.30, \n                                                    random_state = 42)\n# we splitted our dataset to test and train","0a85c352":"from sklearn.ensemble import GradientBoostingClassifier","c75c4652":"gbm_model = GradientBoostingClassifier().fit(X_train, y_train)","1e350f01":"y_pred = gbm_model.predict(X_test)\naccuracy_score(y_test, y_pred) # primitive accuracy score","a30fab84":"gbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n             \"n_estimators\": [100,500,100],\n             \"max_depth\": [3,5,10],\n             \"min_samples_split\": [2,5,10]}","8c61f65b":"gbm = GradientBoostingClassifier()\n\ngbm_cv_model = RandomizedSearchCV(gbm, gbm_params, cv = 10, n_jobs = -1, verbose = 2)","ccc28a4d":"gbm_cv_model.fit(X_train, y_train)","9cb51a41":"print(\"Best Params: \" + str(gbm_cv_model.best_params_))","8bff1514":"gbm = GradientBoostingClassifier(learning_rate = 0.05, \n                                 max_depth = 3,\n                                min_samples_split = 2,\n                                n_estimators = 100)","c4b49429":"gbm_tuned =  gbm.fit(X_train,y_train)","e5bdeac3":"y_pred = gbm_tuned.predict(X_test)\naccuracy_score(y_test, y_pred) # our tuned accuracy score","8149b8d6":"y = df_train[\"Survived\"] # this is our dependent variable","0a587bcf":"X = df_train.drop([\"Survived\"], axis=1) # these are our independent variables","3449d364":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.30, \n                                                    random_state = 42)\n# we splitted our dataset to test and train","68993b49":"from xgboost import XGBClassifier","00ad53d1":"xgb_model = XGBClassifier().fit(X_train, y_train)","d93f0921":"y_pred = xgb_model.predict(X_test)\naccuracy_score(y_test, y_pred)","92fcc2bf":"xgb_params = {\n        'n_estimators': [100, 500, 1000, 2000],\n        'subsample': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5,6],\n        'learning_rate': [0.1,0.01,0.02,0.05],\n        \"min_samples_split\": [2,5,10]}","3e034635":"from sklearn.model_selection import RandomizedSearchCV","9f40aaa0":"xgb = XGBClassifier()\n\nxgb_cv_model = RandomizedSearchCV(xgb, xgb_params, cv = 10, n_jobs = -1, verbose = 2)","6380e154":"xgb_cv_model.fit(X_train, y_train)","6fa704f1":"xgb_cv_model.best_params_","5f9f62a6":"xgb = XGBClassifier(learning_rate = 0.05, \n                    max_depth = 5,\n                    min_samples_split = 2,\n                    n_estimators = 100,\n                    subsample = 0.8)","270d0e0a":"xgb_tuned =  xgb.fit(X_train,y_train)","0de7b7a4":"y_pred = xgb_tuned.predict(X_test)\naccuracy_score(y_test, y_pred)","ac9efea7":"y_pred2 = xgb_tuned.predict(df_test) # we predicted our test data set","dc75606d":"df = pd.DataFrame(y_pred2, columns = ['Survived']) # we change te numpy array's type to pandas dataframe","da1fcc58":"test3 = pd.concat([df_test[\"PassengerId\"], df], axis=1) # concatenating the passenger\u0131d and","6ab441b6":"test3 = pd.concat([df_test[\"PassengerId\"], df], axis=1) # concatenating the passenger\u0131d and","c05a9702":"test3.head() ","47591554":"test1.to_csv('test3.csv',index=False) # to csv file in your current workig directory\n#now yuo can upload your predicts yo kaggle competition","231380b3":"y = df_train[\"Survived\"] # this is our dependent variable","9b539903":"X = df_train.drop([\"Survived\"], axis=1) # these are our independent variables","c4dcefc5":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.30, \n                                                    random_state = 42)\n# we splitted our dataset to test and train","94e704db":"from lightgbm import LGBMClassifier","c262dcdf":"lgbm_model = LGBMClassifier().fit(X_train, y_train)","6fe462d5":"y_pred = lgbm_model.predict(X_test)\naccuracy_score(y_test, y_pred) # primitive accuracy score","b21bc725":"lgbm_params = {\n        'n_estimators': [100, 500, 1000, 2000],\n        'subsample': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5,6],\n        'learning_rate': [0.1,0.01,0.02,0.05],\n        \"min_child_samples\": [5,10,20]}","fb7e2056":"lgbm = LGBMClassifier()\n\nlgbm_cv_model = RandomizedSearchCV(lgbm, lgbm_params, \n                             cv = 10, \n                             n_jobs = -1, \n                             verbose = 2)\n","6127985f":"lgbm_cv_model.fit(X_train, y_train) ","8ecabba5":"lgbm_cv_model.best_params_","2a3c441d":"lgbm = LGBMClassifier(learning_rate = 0.01, \n                       max_depth = 6,\n                       subsample = 0.6,\n                       n_estimators = 100,\n                       min_child_samples = 10)","45904808":"lgbm_tuned = lgbm.fit(X_train,y_train)","56a7590d":"y_pred = lgbm_tuned.predict(X_test)\naccuracy_score(y_test, y_pred) #our accuracy score","e1def456":"y = df_train[\"Survived\"] # this is our dependent variable","5d430a76":"X = df_train.drop([\"Survived\"], axis=1) # these are our independent variables","238fbebe":"from catboost import CatBoostClassifier","20a43b46":"cat_model = CatBoostClassifier().fit(X_train, y_train)","b49371d7":"y_pred = cat_model.predict(X_test)\naccuracy_score(y_test, y_pred) # primitive accuracy","dac72168":"catb_params = {\n    'iterations': [200,500],\n    'learning_rate': [0.01,0.05, 0.1],\n    'depth': [3,5,8] }","b5e480ba":"catb = CatBoostClassifier()\ncatb_cv_model = GridSearchCV(catb, catb_params, cv=5, n_jobs = -1, verbose = 2)\ncatb_cv_model.fit(X_train, y_train)\ncatb_cv_model.best_params_","bff4109d":"catb_cv_model.best_params_","56d7ae91":"catb = CatBoostClassifier(iterations = 200, \n                          learning_rate = 0.01, \n                          depth = 8)\n\ncatb_tuned = catb.fit(X_train, y_train)\ny_pred = catb_tuned.predict(X_test)","bc81c925":"y_pred = catb_tuned.predict(X_test)\naccuracy_score(y_test, y_pred)","bb91aa87":"modeller = [\n    knn_tuned,\n    loj_model,\n    svc_tuned,\n    nb_model,\n    mlpc_tuned,\n    cart_tuned,\n    rf_tuned,\n    gbm_tuned,\n    catb_tuned,\n    lgbm_tuned,\n    xgb_tuned\n    \n]\n\n\nfor model in modeller:\n    isimler = model.__class__.__name__\n    y_pred = model.predict(X_test)\n    dogruluk = accuracy_score(y_test, y_pred)\n    print(\"-\"*28)\n    print(isimler + \":\" )\n    print(\"Accuracy: {:.4%}\".format(dogruluk))","8a1940f9":"- You can see the variable significance levels with this code fragments","5eca1365":"# Logistic Regression","da8ed439":"# RBF(Radial Basis Function) SVC (Non Linear)","d717f75d":"### Model","158c3f42":"# Artificial neural networks","03bb8c26":"## Model Tuning","e30b77a4":"# Light GBM","fd3205bc":"## Model Tuning","27e8956b":"## Model Tuning","e7564901":"# Extreme Gradient Boosting (XGBoost)","e97ae492":"## Model Tuning","54bc3cc7":"## Model Tuning","ffe6e1fa":"- 'Pclass' is a categorical feature so we convert its values to strings","d95649f7":"### Model & Predict","c4d4e4a0":"## Model Tuning","92d22714":"- Artificial neural networks are affected by the scales of variables and the situations between them.So we have to standarized them ","40ce29f8":"# Classification and Regression Trees (CART)","eedaf1fa":"### I did these operations to upload my model's predicts to kaggle","5a73cbdb":"# Preparing Data","6b790cb3":"### Model Tuning","3e6a3f14":"## Model & Predict","12866a50":"# SVC ( Support Vector Classification)","faf80748":"# Naive Bayes \n\nIt does not have an external model hyperparameter. Therefore, there is no situation that we can tune, there is a situation we can verify.","9aa44c7f":"## Model Tuning","84a3e25a":"# Gradient Boosting Machines","32be6b68":" Handling the missing values\n- Fill the missing \u2018Embarked\u2019 values by the most frequent value\n- For now the only age variable has missing value we are going to handle them by knnimput method(nearest neighbor imputation.)","e6da9a97":"## Model Tuning","cfed9f15":"# Comparison of All Models","5904451b":"# Category Boosting (CatBoost)","84085c65":"## Model tuning\n\nIt does not have an external model hyperparameter. Therefore, there is no situation that we can tune, there is a situation we can verify.","9cd4f04c":"# Random Forest Classifier","bfed7961":"- I did these operations to upload my model's predicts to kaggle","872fbaef":"# K Nearest Neigbors Algoritm"}}